{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0227457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import centralized as CP\n",
    "import davisyin as DY\n",
    "import admm as admm\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from numpy import linalg as LA\n",
    "from numpy.linalg import inv\n",
    "import matplotlib as plt\n",
    "from matplotlib import rc\n",
    "# Configura el tipo de letra globalmente\n",
    "rc('font', **{'family': 'serif', 'serif': ['Computer Modern Roman']})\n",
    "rc('text', usetex=True)\n",
    "#plt.rcParams['text.usetex'] = True\n",
    "import matplotlib.pyplot as plt\n",
    "import proyecciones as pro\n",
    "import time\n",
    "import briceno as BA\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.1f}\".format(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa1b9f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.26.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96d1d966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2, 0.2, 0.2, 0.2, 0.2]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caso 1: Caso base\n",
    "    \n",
    "# Cambiar criterio de parada por errores relativos.\n",
    "\n",
    "# Seteamos los parámetros:\n",
    "N, M = 3, 5  # Son 3 tecnologías, 5 escenarios\n",
    "\n",
    "# Probabilidades:\n",
    "inv_, mc_, voll_, d_ = [50.0,  1000.0, 10000.0, 1000.0]\n",
    "                       #[10.0, 2000.0, 10000.0, 1000.0]\n",
    "                       #[50.0,  1000.0, 10000.0, 1000.0]\n",
    "Sigma = np.ones((1,M))\n",
    "#Sigma = np.random.rand(1,M)\n",
    "Sigma /= Sigma.sum()\n",
    "Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1895a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      " [[5.0]\n",
      " [50.0]\n",
      " [95.0]]\n",
      "MC\n",
      " [[[1000.0 0.0 0.0]\n",
      "  [0.0 21000.0 0.0]\n",
      "  [0.0 0.0 41000.0]]\n",
      "\n",
      " [[900.0 0.0 0.0]\n",
      "  [0.0 20500.0 0.0]\n",
      "  [0.0 0.0 42000.0]]\n",
      "\n",
      " [[1200.0 0.0 0.0]\n",
      "  [0.0 22000.0 0.0]\n",
      "  [0.0 0.0 39000.0]]\n",
      "\n",
      " [[700.0 0.0 0.0]\n",
      "  [0.0 19500.0 0.0]\n",
      "  [0.0 0.0 44000.0]]\n",
      "\n",
      " [[1400.0 0.0 0.0]\n",
      "  [0.0 23000.0 0.0]\n",
      "  [0.0 0.0 37000.0]]]\n",
      "VOLL\n",
      " 10000.0\n",
      "D\n",
      " [[200.0 750.0 1000.0 1250.0 1500.0]]\n",
      "frobenius_norm: 458.6700171031756\n"
     ]
    }
   ],
   "source": [
    "Times = {}\n",
    "r_ = 1\n",
    "\n",
    "\n",
    "# Parámetros funciones:\n",
    "I    = inv_ * np.ones((N, 1)) + r_*np.array([[-45], [0], [45]])\n",
    "print(\"I\\n\",I)\n",
    "aux  = np.array([1 + r_*20*i for i in range(N)])\n",
    "\n",
    "mc_11 = 100\n",
    "mc_22 = 500\n",
    "mc_33 = 1000\n",
    "MC   = np.array([np.diag(mc_*aux + r_*np.array([((-1)**m)*mc_11*m, ((-1)**m)*mc_22*m, ((-1)**(m+1))*mc_33*m])) for m in range(M)])\n",
    "print(\"MC\\n\",MC)\n",
    "VOLL = voll_\n",
    "print(\"VOLL\\n\",VOLL)\n",
    "D    = d_*np.ones((1,M)) + r_*np.array([-800, -250, 0, 250, 500])[np.newaxis]\n",
    "print(\"D\\n\",D)\n",
    "\n",
    "e_ = 0\n",
    "\n",
    "e1  = e_\n",
    "e2  = e_\n",
    "e31 = e_*1e2/2\n",
    "e32 = e_\n",
    "\n",
    "Q1, B1 = np.zeros((N,N)), I\n",
    "Q2, B2 = 0.01*MC, np.zeros((N,M))\n",
    "Q3, B3 = np.zeros((1,M)), VOLL*np.ones((1,M))\n",
    "\n",
    "\n",
    "frobenius_norm = (e1+e2)*np.sqrt(N)+e31+e32+np.array([LA.norm(np.einsum('i,ikl->ikl',Sigma[0],0.01*MC)[xi], 'fro') for xi in range(M)]).sum()\n",
    "#frobenius_norm = max([e1*np.sqrt(N),e31+e32,e2*np.sqrt(N)+np.array([LA.norm(np.einsum('i,ikl->ikl',Sigma[0],0.01*MC)[xi], 'fro') for xi in range(M)]).sum()])\n",
    "print(\"frobenius_norm:\",frobenius_norm)\n",
    "\n",
    "def Grad_Phi_1(x1, Q_1 = np.zeros((N,N)), B_1 = I, e1 = e1, N = N):\n",
    "       return np.dot(Q_1,x1)+B_1# - e1*np.dot(np.identity(N),np.maximum(-x1,0))\n",
    "\n",
    "\n",
    "def Grad_Phi_2(x2, Q_2 = 0.01*MC, B_2 = np.zeros((N,M)), e2 = e2, N = N, M = M):\n",
    "\n",
    "    return np.einsum('ijk,ki->ji', Q_2, x2)+B_2# - e2*np.einsum('ijk,ki->ji', np.array([np.diag(np.ones(N)) for m in range(M)]), np.maximum(-x2,0))\n",
    "\n",
    "\n",
    "def Grad_Phi_3(x3, Q_3 = np.zeros((1,M)), B_3 = VOLL*np.ones((1,M)), D=D, e31=e31, e32= e32, M = M):\n",
    "    return Q_3*x3+B_3 #- e31*np.dot(np.maximum(-x3,0),np.identity(M)) - e32*np.dot(np.maximum(-D+x3,0),np.identity(M))\n",
    "\n",
    "\n",
    "def Grad_Phi(x1,x2,x3, P = Sigma):\n",
    "    return Grad_Phi_1(x1), P*Grad_Phi_2(x2), P*Grad_Phi_3(x3)\n",
    "\n",
    "def Grad_Phi_NA(x1,x2,x3, P = Sigma):\n",
    "    return P*Grad_Phi_1(x1), P*Grad_Phi_2(x2), P*Grad_Phi_3(x3)\n",
    "\n",
    "def Phi_1(x1, Q_1 = np.zeros((N,N)), B_1 = I, C_1 = 0.0, e1 = e1):\n",
    "    return 0.5*np.einsum('ij,ji -> i', x1.T,np.dot(Q_1,x1))[:,np.newaxis]+np.dot(x1.T, B_1)+C_1 + e1/2*LA.norm(np.maximum(-x1.flatten(),0))**2\n",
    "\n",
    "def Phi_2_xi(x2, Q_2 = 0.01*MC, B_2 = np.zeros((N,M)), C_2 = np.zeros((M, 1))):\n",
    "    return 0.5*np.einsum('ij,ji -> i', x2.T, np.einsum('ijk,ki -> ji', Q_2, x2))[:,np.newaxis]+np.einsum('ij,ji->i',x2.T,B_2)[:,np.newaxis]+C_2 + e2/2*LA.norm(np.maximum(-x2.flatten(),0))**2\n",
    "\n",
    "def Phi_3_xi(x3, Q_3 = np.zeros((1,M)), B_3 = VOLL*np.ones((1,M)), C_3 = -VOLL*D ):\n",
    "    return (0.5*x3*Q_3*x3+B_3*x3+C_3).T + e31/2*LA.norm(np.maximum(-x3.flatten(),0))**2 + e32/2*LA.norm(np.maximum((D-x3).flatten(),0))**2\n",
    "\n",
    "\n",
    "def objective_function(x1, x2, x3, P = Sigma, NA = True):\n",
    "\n",
    "# NA = True, cumple la funcion que si se impuso \n",
    "#      la condición de no anticipatividad para x1\n",
    "#      entonces, Phi_1(x1).shape == (M,1)\n",
    "    if NA:\n",
    "        return np.dot(P, Phi_1(x1) +Phi_2_xi(x2)+Phi_3_xi(x3))\n",
    "    else:\n",
    "        return Phi_1(x1)+ np.dot(P, Phi_2_xi(x2)+Phi_3_xi(x3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4f57c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = time.time()\n",
    "x1, x2, x3, rho, mu = map(np.array, CP.modelo(Sigma, N, M, \\\n",
    "                                              parametros = [I.T[0].tolist(),\\\n",
    "                                                            np.array([mc_*aux + r_*np.array([((-1)**m)*mc_11*m, ((-1)**m)*mc_22*m, ((-1)**(m+1))*mc_33*m]) for m in range(M)]).T.tolist(),\\\n",
    "                                                            VOLL,\\\n",
    "                                                            D[0]] , show = 0))\n",
    "fin = time.time()\n",
    "\n",
    "\n",
    "Times[\"CP\"] = fin - cp\n",
    "\n",
    "x1 = x1[:,np.newaxis]\n",
    "x2 = x2.T\n",
    "x3 = x3[np.newaxis,:][0]\n",
    "rho = rho[np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1265a4e1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primal:\n",
      "[[1188.3]\n",
      " [44.3]\n",
      " [25.7]]\n",
      "[[186.6 704.0 833.3 1188.3 714.3]\n",
      " [8.9 30.9 44.3 42.8 43.5]\n",
      " [4.6 15.1 25.6 19.0 25.7]]\n",
      "[[0.0 0.0 96.7 0.0 716.5]]\n",
      "Dual:\n",
      "[[0.0 0.0 0.0 25.0 0.0]\n",
      " [0.0 0.0 250.0 0.0 0.0]\n",
      " [0.0 0.0 0.0 0.0 475.0]]\n",
      "[[1865.7 6336.1 10000.0 8342.8 10000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"primal:\\n{x1}\\n{x2}\\n{x3}\")\n",
    "print(f\"Dual:\\n{mu}\\n{rho}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c84c5f51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta: 0.002180216632244036\n",
      "Gamma: 0.004013225500591509\n",
      "Lambda_k: 1\n",
      "Iteration: 1 lambda_k: 1 Loss: 0.9458180938529576\n",
      "Iteration: 2 lambda_k: 1 Loss: 0.8660472506762181\n",
      "Iteration: 3 lambda_k: 1 Loss: 0.8301219872743254\n",
      "Iteration: 4 lambda_k: 1 Loss: 0.8399072147388666\n",
      "Iteration: 5 lambda_k: 1 Loss: 0.8315818810768336\n",
      "Iteration: 6 lambda_k: 1 Loss: 0.8207532590858538\n",
      "Iteration: 7 lambda_k: 1 Loss: 0.8136064251562316\n",
      "Iteration: 8 lambda_k: 1 Loss: 0.8085830631187111\n",
      "Iteration: 9 lambda_k: 1 Loss: 0.8045489670980089\n",
      "Iteration: 10 lambda_k: 1 Loss: 0.8013920832043254\n",
      "Iteration: 11 lambda_k: 1 Loss: 0.7983607508912482\n",
      "Iteration: 12 lambda_k: 1 Loss: 0.7953903846603333\n",
      "Iteration: 13 lambda_k: 1 Loss: 0.7925009314650064\n",
      "Iteration: 14 lambda_k: 1 Loss: 0.7897015871482305\n",
      "Iteration: 15 lambda_k: 1 Loss: 0.7869810970028334\n",
      "Iteration: 16 lambda_k: 1 Loss: 0.7843155071203574\n",
      "Iteration: 17 lambda_k: 1 Loss: 0.7816984115104484\n",
      "Iteration: 18 lambda_k: 1 Loss: 0.7791149007539367\n",
      "Iteration: 19 lambda_k: 1 Loss: 0.7765630156042729\n",
      "Iteration: 20 lambda_k: 1 Loss: 0.7740417039710596\n",
      "Iteration: 21 lambda_k: 1 Loss: 0.7715493121304571\n",
      "Iteration: 22 lambda_k: 1 Loss: 0.7690843971020693\n",
      "Iteration: 23 lambda_k: 1 Loss: 0.7668370876948445\n",
      "Iteration: 24 lambda_k: 1 Loss: 0.7647144274529198\n",
      "Iteration: 25 lambda_k: 1 Loss: 0.7626244934577782\n",
      "Iteration: 26 lambda_k: 1 Loss: 0.7605642409415093\n",
      "Iteration: 27 lambda_k: 1 Loss: 0.7585310616196628\n",
      "Iteration: 28 lambda_k: 1 Loss: 0.756522690592862\n",
      "Iteration: 29 lambda_k: 1 Loss: 0.75453714280454\n",
      "Iteration: 30 lambda_k: 1 Loss: 0.752572666208377\n",
      "Iteration: 31 lambda_k: 1 Loss: 0.7506277055432995\n",
      "Iteration: 32 lambda_k: 1 Loss: 0.7487008737048143\n",
      "Iteration: 33 lambda_k: 1 Loss: 0.7467909287789151\n",
      "Iteration: 34 lambda_k: 1 Loss: 0.744896755282555\n",
      "Iteration: 35 lambda_k: 1 Loss: 0.7430173485484542\n",
      "Iteration: 36 lambda_k: 1 Loss: 0.7411518015274439\n",
      "Iteration: 37 lambda_k: 1 Loss: 0.7392992935146315\n",
      "Iteration: 38 lambda_k: 1 Loss: 0.737459080446522\n",
      "Iteration: 39 lambda_k: 1 Loss: 0.7356304865010889\n",
      "Iteration: 40 lambda_k: 1 Loss: 0.7338128967888904\n",
      "Iteration: 41 lambda_k: 1 Loss: 0.7320057509699855\n",
      "Iteration: 42 lambda_k: 1 Loss: 0.7302085376494689\n",
      "Iteration: 43 lambda_k: 1 Loss: 0.7284207894514512\n",
      "Iteration: 44 lambda_k: 1 Loss: 0.7266420786684943\n",
      "Iteration: 45 lambda_k: 1 Loss: 0.72487201341107\n",
      "Iteration: 46 lambda_k: 1 Loss: 0.7231102341901772\n",
      "Iteration: 47 lambda_k: 1 Loss: 0.7213564108765086\n",
      "Iteration: 48 lambda_k: 1 Loss: 0.7196102399879636\n",
      "Iteration: 49 lambda_k: 1 Loss: 0.7178714422643173\n",
      "Iteration: 50 lambda_k: 1 Loss: 0.7161397604937169\n",
      "Iteration: 51 lambda_k: 1 Loss: 0.714414957560609\n",
      "Iteration: 52 lambda_k: 1 Loss: 0.7126968146888902\n",
      "Iteration: 53 lambda_k: 1 Loss: 0.7109851298576129\n",
      "Iteration: 54 lambda_k: 1 Loss: 0.7092797163696167\n",
      "Iteration: 55 lambda_k: 1 Loss: 0.7075804015560178\n",
      "Iteration: 56 lambda_k: 1 Loss: 0.7058870256017649\n",
      "Iteration: 57 lambda_k: 1 Loss: 0.7041994404788774\n",
      "Iteration: 58 lambda_k: 1 Loss: 0.7025175089770581\n",
      "Iteration: 59 lambda_k: 1 Loss: 0.7008411038202379\n",
      "Iteration: 60 lambda_k: 1 Loss: 0.6991701068614462\n",
      "Iteration: 61 lambda_k: 1 Loss: 0.6975044083477685\n",
      "Iteration: 62 lambda_k: 1 Loss: 0.6958439062486143\n",
      "Iteration: 63 lambda_k: 1 Loss: 0.6941885056412311\n",
      "Iteration: 64 lambda_k: 1 Loss: 0.6925381181477224\n",
      "Iteration: 65 lambda_k: 1 Loss: 0.6908926614208684\n",
      "Iteration: 66 lambda_k: 1 Loss: 0.6892520586689627\n",
      "Iteration: 67 lambda_k: 1 Loss: 0.6876162382234949\n",
      "Iteration: 68 lambda_k: 1 Loss: 0.6859851331400726\n",
      "Iteration: 69 lambda_k: 1 Loss: 0.6843586808321139\n",
      "Iteration: 70 lambda_k: 1 Loss: 0.682736822734094\n",
      "Iteration: 71 lambda_k: 1 Loss: 0.6811195039917605\n",
      "Iteration: 72 lambda_k: 1 Loss: 0.6795066731770217\n",
      "Iteration: 73 lambda_k: 1 Loss: 0.6778982820254423\n",
      "Iteration: 74 lambda_k: 1 Loss: 0.6762942851935683\n",
      "Iteration: 75 lambda_k: 1 Loss: 0.6746946400395963\n",
      "Iteration: 76 lambda_k: 1 Loss: 0.6730993064131184\n",
      "Iteration: 77 lambda_k: 1 Loss: 0.671508246469106\n",
      "Iteration: 78 lambda_k: 1 Loss: 0.6699214244925381\n",
      "Iteration: 79 lambda_k: 1 Loss: 0.668338806737417\n",
      "Iteration: 80 lambda_k: 1 Loss: 0.6667603612782727\n",
      "Iteration: 81 lambda_k: 1 Loss: 0.6651860578740503\n",
      "Iteration: 82 lambda_k: 1 Loss: 0.663615867838266\n",
      "Iteration: 83 lambda_k: 1 Loss: 0.6620497639271636\n",
      "Iteration: 84 lambda_k: 1 Loss: 0.6604877202287895\n",
      "Iteration: 85 lambda_k: 1 Loss: 0.658929712064338\n",
      "Iteration: 86 lambda_k: 1 Loss: 0.6573757158963138\n",
      "Iteration: 87 lambda_k: 1 Loss: 0.6558257092438481\n",
      "Iteration: 88 lambda_k: 1 Loss: 0.6542796706037528\n",
      "Iteration: 89 lambda_k: 1 Loss: 0.6527375793791593\n",
      "Iteration: 90 lambda_k: 1 Loss: 0.6511994158119407\n",
      "Iteration: 91 lambda_k: 1 Loss: 0.6496651609212152\n",
      "Iteration: 92 lambda_k: 1 Loss: 0.6481347964463323\n",
      "Iteration: 93 lambda_k: 1 Loss: 0.6466083047941796\n",
      "Iteration: 94 lambda_k: 1 Loss: 0.6450856689905017\n",
      "Iteration: 95 lambda_k: 1 Loss: 0.6435668726349274\n",
      "Iteration: 96 lambda_k: 1 Loss: 0.6420518998594167\n",
      "Iteration: 97 lambda_k: 1 Loss: 0.6405407352898634\n",
      "Iteration: 98 lambda_k: 1 Loss: 0.639033364010614\n",
      "Iteration: 99 lambda_k: 1 Loss: 0.6375297715316771\n",
      "Iteration: 100 lambda_k: 1 Loss: 0.6360299437584235\n",
      "Iteration: 101 lambda_k: 1 Loss: 0.6345338669635824\n",
      "Iteration: 102 lambda_k: 1 Loss: 0.6330415277613608\n",
      "Iteration: 103 lambda_k: 1 Loss: 0.6315529130835259\n",
      "Iteration: 104 lambda_k: 1 Loss: 0.630068010157299\n",
      "Iteration: 105 lambda_k: 1 Loss: 0.6285868064849232\n",
      "Iteration: 106 lambda_k: 1 Loss: 0.6271092898247818\n",
      "Iteration: 107 lambda_k: 1 Loss: 0.6256354481689637\n",
      "Iteration: 108 lambda_k: 1 Loss: 0.6241652697471783\n",
      "Iteration: 109 lambda_k: 1 Loss: 0.6226987429804677\n",
      "Iteration: 110 lambda_k: 1 Loss: 0.621235856490347\n",
      "Iteration: 111 lambda_k: 1 Loss: 0.6197765990793725\n",
      "Iteration: 112 lambda_k: 1 Loss: 0.6183209597205653\n",
      "Iteration: 113 lambda_k: 1 Loss: 0.6168689275405439\n",
      "Iteration: 114 lambda_k: 1 Loss: 0.6154204918191485\n",
      "Iteration: 115 lambda_k: 1 Loss: 0.6139756419734355\n",
      "Iteration: 116 lambda_k: 1 Loss: 0.6125343675509219\n",
      "Iteration: 117 lambda_k: 1 Loss: 0.6110966582216786\n",
      "Iteration: 118 lambda_k: 1 Loss: 0.6096625037709197\n",
      "Iteration: 119 lambda_k: 1 Loss: 0.6082318940921613\n",
      "Iteration: 120 lambda_k: 1 Loss: 0.6068048191809461\n",
      "Iteration: 121 lambda_k: 1 Loss: 0.605381269129075\n",
      "Iteration: 122 lambda_k: 1 Loss: 0.6039612341193282\n",
      "Iteration: 123 lambda_k: 1 Loss: 0.6025447044136506\n",
      "Iteration: 124 lambda_k: 1 Loss: 0.6011316703760299\n",
      "Iteration: 125 lambda_k: 1 Loss: 0.5997221224205731\n",
      "Iteration: 126 lambda_k: 1 Loss: 0.598316051050828\n",
      "Iteration: 127 lambda_k: 1 Loss: 0.5969134468361046\n",
      "Iteration: 128 lambda_k: 1 Loss: 0.595514300411751\n",
      "Iteration: 129 lambda_k: 1 Loss: 0.5941186024778513\n",
      "Iteration: 130 lambda_k: 1 Loss: 0.5927263437733001\n",
      "Iteration: 131 lambda_k: 1 Loss: 0.591337515151087\n",
      "Iteration: 132 lambda_k: 1 Loss: 0.5899521074537059\n",
      "Iteration: 133 lambda_k: 1 Loss: 0.5885701115939861\n",
      "Iteration: 134 lambda_k: 1 Loss: 0.587191518649597\n",
      "Iteration: 135 lambda_k: 1 Loss: 0.5858163225777054\n",
      "Iteration: 136 lambda_k: 1 Loss: 0.5844445372213557\n",
      "Iteration: 137 lambda_k: 1 Loss: 0.5830761311861988\n",
      "Iteration: 138 lambda_k: 1 Loss: 0.5817111013817872\n",
      "Iteration: 139 lambda_k: 1 Loss: 0.5803494372629706\n",
      "Iteration: 140 lambda_k: 1 Loss: 0.5789911274189868\n",
      "Iteration: 141 lambda_k: 1 Loss: 0.5776361603537766\n",
      "Iteration: 142 lambda_k: 1 Loss: 0.5762845247535109\n",
      "Iteration: 143 lambda_k: 1 Loss: 0.5749362094918068\n",
      "Iteration: 144 lambda_k: 1 Loss: 0.5735912035820669\n",
      "Iteration: 145 lambda_k: 1 Loss: 0.5722494961297802\n",
      "Iteration: 146 lambda_k: 1 Loss: 0.5709110763575447\n",
      "Iteration: 147 lambda_k: 1 Loss: 0.5695759335741746\n",
      "Iteration: 148 lambda_k: 1 Loss: 0.5682440571835501\n",
      "Iteration: 149 lambda_k: 1 Loss: 0.5669154366631343\n",
      "Iteration: 150 lambda_k: 1 Loss: 0.5655900615770405\n",
      "Iteration: 151 lambda_k: 1 Loss: 0.5642679215587867\n",
      "Iteration: 152 lambda_k: 1 Loss: 0.5629490063127982\n",
      "Iteration: 153 lambda_k: 1 Loss: 0.5616333056102127\n",
      "Iteration: 154 lambda_k: 1 Loss: 0.5603208092860955\n",
      "Iteration: 155 lambda_k: 1 Loss: 0.5590115072369328\n",
      "Iteration: 156 lambda_k: 1 Loss: 0.5577053894184261\n",
      "Iteration: 157 lambda_k: 1 Loss: 0.5564024458435332\n",
      "Iteration: 158 lambda_k: 1 Loss: 0.555102666580723\n",
      "Iteration: 159 lambda_k: 1 Loss: 0.5538060417524114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 160 lambda_k: 1 Loss: 0.5525125615335608\n",
      "Iteration: 161 lambda_k: 1 Loss: 0.5512222161504191\n",
      "Iteration: 162 lambda_k: 1 Loss: 0.5499349958793874\n",
      "Iteration: 163 lambda_k: 1 Loss: 0.5486508910459943\n",
      "Iteration: 164 lambda_k: 1 Loss: 0.5473698920239709\n",
      "Iteration: 165 lambda_k: 1 Loss: 0.5460919892344103\n",
      "Iteration: 166 lambda_k: 1 Loss: 0.5448171731450056\n",
      "Iteration: 167 lambda_k: 1 Loss: 0.543545434269356\n",
      "Iteration: 168 lambda_k: 1 Loss: 0.5422767631663351\n",
      "Iteration: 169 lambda_k: 1 Loss: 0.5410111504395132\n",
      "Iteration: 170 lambda_k: 1 Loss: 0.5397485867366303\n",
      "Iteration: 171 lambda_k: 1 Loss: 0.5384890627491122\n",
      "Iteration: 172 lambda_k: 1 Loss: 0.5372325692116282\n",
      "Iteration: 173 lambda_k: 1 Loss: 0.5359790969016829\n",
      "Iteration: 174 lambda_k: 1 Loss: 0.534728636639243\n",
      "Iteration: 175 lambda_k: 1 Loss: 0.5334811792863919\n",
      "Iteration: 176 lambda_k: 1 Loss: 0.53223671574696\n",
      "Iteration: 177 lambda_k: 1 Loss: 0.5309952369664445\n",
      "Iteration: 178 lambda_k: 1 Loss: 0.5297567339313933\n",
      "Iteration: 179 lambda_k: 1 Loss: 0.5285211976694429\n",
      "Iteration: 180 lambda_k: 1 Loss: 0.5272886192489434\n",
      "Iteration: 181 lambda_k: 1 Loss: 0.526058989778797\n",
      "Iteration: 182 lambda_k: 1 Loss: 0.524832300408245\n",
      "Iteration: 183 lambda_k: 1 Loss: 0.5236085423266864\n",
      "Iteration: 184 lambda_k: 1 Loss: 0.5223877067635088\n",
      "Iteration: 185 lambda_k: 1 Loss: 0.5211697849879319\n",
      "Iteration: 186 lambda_k: 1 Loss: 0.5199547683088649\n",
      "Iteration: 187 lambda_k: 1 Loss: 0.5187426480747719\n",
      "Iteration: 188 lambda_k: 1 Loss: 0.5175334156735518\n",
      "Iteration: 189 lambda_k: 1 Loss: 0.5163270625324268\n",
      "Iteration: 190 lambda_k: 1 Loss: 0.5151235801178412\n",
      "Iteration: 191 lambda_k: 1 Loss: 0.5139229599353703\n",
      "Iteration: 192 lambda_k: 1 Loss: 0.5127251935296381\n",
      "Iteration: 193 lambda_k: 1 Loss: 0.5115302724842444\n",
      "Iteration: 194 lambda_k: 1 Loss: 0.5103381884217014\n",
      "Iteration: 195 lambda_k: 1 Loss: 0.5091489330033777\n",
      "Iteration: 196 lambda_k: 1 Loss: 0.5079624979294524\n",
      "Iteration: 197 lambda_k: 1 Loss: 0.5067788749388765\n",
      "Iteration: 198 lambda_k: 1 Loss: 0.5055980558093451\n",
      "Iteration: 199 lambda_k: 1 Loss: 0.5044200323572757\n",
      "Iteration: 200 lambda_k: 1 Loss: 0.5032447964377964\n",
      "Iteration: 201 lambda_k: 1 Loss: 0.5020723399447432\n",
      "Iteration: 202 lambda_k: 1 Loss: 0.5009026548106665\n",
      "Iteration: 203 lambda_k: 1 Loss: 0.4997357330068454\n",
      "Iteration: 204 lambda_k: 1 Loss: 0.49857156654331203\n",
      "Iteration: 205 lambda_k: 1 Loss: 0.4974101474688867\n",
      "Iteration: 206 lambda_k: 1 Loss: 0.49625146787122026\n",
      "Iteration: 207 lambda_k: 1 Loss: 0.49509551987684847\n",
      "Iteration: 208 lambda_k: 1 Loss: 0.49394229565125686\n",
      "Iteration: 209 lambda_k: 1 Loss: 0.49279178739895496\n",
      "Iteration: 210 lambda_k: 1 Loss: 0.4916439873635635\n",
      "Iteration: 211 lambda_k: 1 Loss: 0.4904988878279115\n",
      "Iteration: 212 lambda_k: 1 Loss: 0.48935648111414665\n",
      "Iteration: 213 lambda_k: 1 Loss: 0.48821675958385763\n",
      "Iteration: 214 lambda_k: 1 Loss: 0.48707971563820995\n",
      "Iteration: 215 lambda_k: 1 Loss: 0.4859453417180949\n",
      "Iteration: 216 lambda_k: 1 Loss: 0.4848136303042932\n",
      "Iteration: 217 lambda_k: 1 Loss: 0.48368457391765346\n",
      "Iteration: 218 lambda_k: 1 Loss: 0.4825581651192866\n",
      "Iteration: 219 lambda_k: 1 Loss: 0.4814343965107759\n",
      "Iteration: 220 lambda_k: 1 Loss: 0.480313260734405\n",
      "Iteration: 221 lambda_k: 1 Loss: 0.4791947504734033\n",
      "Iteration: 222 lambda_k: 1 Loss: 0.4780788584498681\n",
      "Iteration: 223 lambda_k: 1 Loss: 0.476965577434252\n",
      "Iteration: 224 lambda_k: 1 Loss: 0.47585490023203525\n",
      "Iteration: 225 lambda_k: 1 Loss: 0.4747468196929633\n",
      "Iteration: 226 lambda_k: 1 Loss: 0.47364132870959785\n",
      "Iteration: 227 lambda_k: 1 Loss: 0.4725384202172163\n",
      "Iteration: 228 lambda_k: 1 Loss: 0.4714380871942316\n",
      "Iteration: 229 lambda_k: 1 Loss: 0.47034032266274595\n",
      "Iteration: 230 lambda_k: 1 Loss: 0.4692451196912706\n",
      "Iteration: 231 lambda_k: 1 Loss: 0.46815247138641775\n",
      "Iteration: 232 lambda_k: 1 Loss: 0.46706237090639624\n",
      "Iteration: 233 lambda_k: 1 Loss: 0.4659748114531053\n",
      "Iteration: 234 lambda_k: 1 Loss: 0.46488978627439437\n",
      "Iteration: 235 lambda_k: 1 Loss: 0.46380728866517557\n",
      "Iteration: 236 lambda_k: 1 Loss: 0.4627273119680766\n",
      "Iteration: 237 lambda_k: 1 Loss: 0.46164984957402766\n",
      "Iteration: 238 lambda_k: 1 Loss: 0.4605748949229923\n",
      "Iteration: 239 lambda_k: 1 Loss: 0.4595024415047939\n",
      "Iteration: 240 lambda_k: 1 Loss: 0.45843248285998744\n",
      "Iteration: 241 lambda_k: 1 Loss: 0.4573650125807734\n",
      "Iteration: 242 lambda_k: 1 Loss: 0.45630002431196576\n",
      "Iteration: 243 lambda_k: 1 Loss: 0.45523751175202376\n",
      "Iteration: 244 lambda_k: 1 Loss: 0.4541774686471067\n",
      "Iteration: 245 lambda_k: 1 Loss: 0.4531198888212112\n",
      "Iteration: 246 lambda_k: 1 Loss: 0.4520647661316678\n",
      "Iteration: 247 lambda_k: 1 Loss: 0.4510120945031604\n",
      "Iteration: 248 lambda_k: 1 Loss: 0.44996186792140397\n",
      "Iteration: 249 lambda_k: 1 Loss: 0.4489140804326887\n",
      "Iteration: 250 lambda_k: 1 Loss: 0.44786872614576034\n",
      "Iteration: 251 lambda_k: 1 Loss: 0.44682579923611687\n",
      "Iteration: 252 lambda_k: 1 Loss: 0.4457852939399805\n",
      "Iteration: 253 lambda_k: 1 Loss: 0.44474720456778005\n",
      "Iteration: 254 lambda_k: 1 Loss: 0.44371152549870396\n",
      "Iteration: 255 lambda_k: 1 Loss: 0.44267825118420023\n",
      "Iteration: 256 lambda_k: 1 Loss: 0.4416473761508383\n",
      "Iteration: 257 lambda_k: 1 Loss: 0.44061889500280677\n",
      "Iteration: 258 lambda_k: 1 Loss: 0.4395928024244499\n",
      "Iteration: 259 lambda_k: 1 Loss: 0.4385690931830799\n",
      "Iteration: 260 lambda_k: 1 Loss: 0.437547762132034\n",
      "Iteration: 261 lambda_k: 1 Loss: 0.43652880421393675\n",
      "Iteration: 262 lambda_k: 1 Loss: 0.4355122144641737\n",
      "Iteration: 263 lambda_k: 1 Loss: 0.434497988014605\n",
      "Iteration: 264 lambda_k: 1 Loss: 0.43348612009754267\n",
      "Iteration: 265 lambda_k: 1 Loss: 0.4324766060500139\n",
      "Iteration: 266 lambda_k: 1 Loss: 0.4314694413183328\n",
      "Iteration: 267 lambda_k: 1 Loss: 0.43046462146300807\n",
      "Iteration: 268 lambda_k: 1 Loss: 0.4294621421640163\n",
      "Iteration: 269 lambda_k: 1 Loss: 0.42846199922648\n",
      "Iteration: 270 lambda_k: 1 Loss: 0.4274641885760489\n",
      "Iteration: 271 lambda_k: 1 Loss: 0.4264687063064412\n",
      "Iteration: 272 lambda_k: 1 Loss: 0.4254755486234112\n",
      "Iteration: 273 lambda_k: 1 Loss: 0.42448471189689985\n",
      "Iteration: 274 lambda_k: 1 Loss: 0.4234961926589677\n",
      "Iteration: 275 lambda_k: 1 Loss: 0.4225099876094051\n",
      "Iteration: 276 lambda_k: 1 Loss: 0.4215260936255094\n",
      "Iteration: 277 lambda_k: 1 Loss: 0.4205445077732058\n",
      "Iteration: 278 lambda_k: 1 Loss: 0.41956522731849094\n",
      "Iteration: 279 lambda_k: 1 Loss: 0.41858824973987513\n",
      "Iteration: 280 lambda_k: 1 Loss: 0.41761357274294236\n",
      "Iteration: 281 lambda_k: 1 Loss: 0.416641194194128\n",
      "Iteration: 282 lambda_k: 1 Loss: 0.41567111242419674\n",
      "Iteration: 283 lambda_k: 1 Loss: 0.4147033257973276\n",
      "Iteration: 284 lambda_k: 1 Loss: 0.4137378330796169\n",
      "Iteration: 285 lambda_k: 1 Loss: 0.41277463327905384\n",
      "Iteration: 286 lambda_k: 1 Loss: 0.41181372590355636\n",
      "Iteration: 287 lambda_k: 1 Loss: 0.41085511078907994\n",
      "Iteration: 288 lambda_k: 1 Loss: 0.4098987879440098\n",
      "Iteration: 289 lambda_k: 1 Loss: 0.4089447797365885\n",
      "Iteration: 290 lambda_k: 1 Loss: 0.4079930666753175\n",
      "Iteration: 291 lambda_k: 1 Loss: 0.40704365213264754\n",
      "Iteration: 292 lambda_k: 1 Loss: 0.40609654025297537\n",
      "Iteration: 293 lambda_k: 1 Loss: 0.4051517331576468\n",
      "Iteration: 294 lambda_k: 1 Loss: 0.4042092323323947\n",
      "Iteration: 295 lambda_k: 1 Loss: 0.4032690399779242\n",
      "Iteration: 296 lambda_k: 1 Loss: 0.4023311591574884\n",
      "Iteration: 297 lambda_k: 1 Loss: 0.4013955938577479\n",
      "Iteration: 298 lambda_k: 1 Loss: 0.4004623488999038\n",
      "Iteration: 299 lambda_k: 1 Loss: 0.3995314300383811\n",
      "Iteration: 300 lambda_k: 1 Loss: 0.3986028440484202\n",
      "Iteration: 301 lambda_k: 1 Loss: 0.3976765987798102\n",
      "Iteration: 302 lambda_k: 1 Loss: 0.3967527032945677\n",
      "Iteration: 303 lambda_k: 1 Loss: 0.39583116793853657\n",
      "Iteration: 304 lambda_k: 1 Loss: 0.39491200446394825\n",
      "Iteration: 305 lambda_k: 1 Loss: 0.39399522616651633\n",
      "Iteration: 306 lambda_k: 1 Loss: 0.3930808480065298\n",
      "Iteration: 307 lambda_k: 1 Loss: 0.3921688867849237\n",
      "Iteration: 308 lambda_k: 1 Loss: 0.3912593613012303\n",
      "Iteration: 309 lambda_k: 1 Loss: 0.390352292546959\n",
      "Iteration: 310 lambda_k: 1 Loss: 0.38944770391338723\n",
      "Iteration: 311 lambda_k: 1 Loss: 0.38854562142042604\n",
      "Iteration: 312 lambda_k: 1 Loss: 0.3876460739677091\n",
      "Iteration: 313 lambda_k: 1 Loss: 0.38674909360955173\n",
      "Iteration: 314 lambda_k: 1 Loss: 0.38585471585403774\n",
      "Iteration: 315 lambda_k: 1 Loss: 0.3849629799919068\n",
      "Iteration: 316 lambda_k: 1 Loss: 0.3840739294507374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 317 lambda_k: 1 Loss: 0.3831876121651565\n",
      "Iteration: 318 lambda_k: 1 Loss: 0.38232995302809164\n",
      "Iteration: 319 lambda_k: 1 Loss: 0.38152682059616444\n",
      "Iteration: 320 lambda_k: 1 Loss: 0.38073245392455063\n",
      "Iteration: 321 lambda_k: 1 Loss: 0.37994737852875565\n",
      "Iteration: 322 lambda_k: 1 Loss: 0.37917112799422087\n",
      "Iteration: 323 lambda_k: 1 Loss: 0.3784032618019073\n",
      "Iteration: 324 lambda_k: 1 Loss: 0.37764356609718386\n",
      "Iteration: 325 lambda_k: 1 Loss: 0.37689193892274675\n",
      "Iteration: 326 lambda_k: 1 Loss: 0.3761482926138226\n",
      "Iteration: 327 lambda_k: 1 Loss: 0.3754125266420608\n",
      "Iteration: 328 lambda_k: 1 Loss: 0.37468453345453845\n",
      "Iteration: 329 lambda_k: 1 Loss: 0.3739642067882024\n",
      "Iteration: 330 lambda_k: 1 Loss: 0.37325144439856084\n",
      "Iteration: 331 lambda_k: 1 Loss: 0.37254614742560316\n",
      "Iteration: 332 lambda_k: 1 Loss: 0.371848219345301\n",
      "Iteration: 333 lambda_k: 1 Loss: 0.37115756543778056\n",
      "Iteration: 334 lambda_k: 1 Loss: 0.37047409267831616\n",
      "Iteration: 335 lambda_k: 1 Loss: 0.3697977098095406\n",
      "Iteration: 336 lambda_k: 1 Loss: 0.3691283273800766\n",
      "Iteration: 337 lambda_k: 1 Loss: 0.36846575772142187\n",
      "Iteration: 338 lambda_k: 1 Loss: 0.36780926754783544\n",
      "Iteration: 339 lambda_k: 1 Loss: 0.36715865845323836\n",
      "Iteration: 340 lambda_k: 1 Loss: 0.366513346044252\n",
      "Iteration: 341 lambda_k: 1 Loss: 0.36587302766151486\n",
      "Iteration: 342 lambda_k: 1 Loss: 0.36523742094484296\n",
      "Iteration: 343 lambda_k: 1 Loss: 0.3646062632001232\n",
      "Iteration: 344 lambda_k: 1 Loss: 0.363979311198845\n",
      "Iteration: 345 lambda_k: 1 Loss: 0.3633563399400312\n",
      "Iteration: 346 lambda_k: 1 Loss: 0.36273714110254623\n",
      "Iteration: 347 lambda_k: 1 Loss: 0.36212152162238914\n",
      "Iteration: 348 lambda_k: 1 Loss: 0.36150930252016383\n",
      "Iteration: 349 lambda_k: 1 Loss: 0.36090031787460397\n",
      "Iteration: 350 lambda_k: 1 Loss: 0.3602945288477893\n",
      "Iteration: 351 lambda_k: 1 Loss: 0.35969175451529956\n",
      "Iteration: 352 lambda_k: 1 Loss: 0.3590917547049191\n",
      "Iteration: 353 lambda_k: 1 Loss: 0.35849441038246976\n",
      "Iteration: 354 lambda_k: 1 Loss: 0.357899608915934\n",
      "Iteration: 355 lambda_k: 1 Loss: 0.35730724328073926\n",
      "Iteration: 356 lambda_k: 1 Loss: 0.356717214491266\n",
      "Iteration: 357 lambda_k: 1 Loss: 0.35612943192113866\n",
      "Iteration: 358 lambda_k: 1 Loss: 0.35554381205792324\n",
      "Iteration: 359 lambda_k: 1 Loss: 0.35496027736382985\n",
      "Iteration: 360 lambda_k: 1 Loss: 0.35437875572448946\n",
      "Iteration: 361 lambda_k: 1 Loss: 0.35379918014804584\n",
      "Iteration: 362 lambda_k: 1 Loss: 0.3532214884556559\n",
      "Iteration: 363 lambda_k: 1 Loss: 0.35264562293232204\n",
      "Iteration: 364 lambda_k: 1 Loss: 0.3520715299839588\n",
      "Iteration: 365 lambda_k: 1 Loss: 0.35149915974258894\n",
      "Iteration: 366 lambda_k: 1 Loss: 0.3509284660433951\n",
      "Iteration: 367 lambda_k: 1 Loss: 0.35035940584396585\n",
      "Iteration: 368 lambda_k: 1 Loss: 0.34979193911933354\n",
      "Iteration: 369 lambda_k: 1 Loss: 0.3492260287329888\n",
      "Iteration: 370 lambda_k: 1 Loss: 0.34866163983048565\n",
      "Iteration: 371 lambda_k: 1 Loss: 0.34809874087821363\n",
      "Iteration: 372 lambda_k: 1 Loss: 0.34753730415895207\n",
      "Iteration: 373 lambda_k: 1 Loss: 0.3469773127968167\n",
      "Iteration: 374 lambda_k: 1 Loss: 0.3464187349749787\n",
      "Iteration: 375 lambda_k: 1 Loss: 0.3458615471606794\n",
      "Iteration: 376 lambda_k: 1 Loss: 0.34530572574005763\n",
      "Iteration: 377 lambda_k: 1 Loss: 0.3447512481349557\n",
      "Iteration: 378 lambda_k: 1 Loss: 0.34419809324375805\n",
      "Iteration: 379 lambda_k: 1 Loss: 0.3436462411568035\n",
      "Iteration: 380 lambda_k: 1 Loss: 0.3430956732980084\n",
      "Iteration: 381 lambda_k: 1 Loss: 0.3425463721745273\n",
      "Iteration: 382 lambda_k: 1 Loss: 0.3419983213549743\n",
      "Iteration: 383 lambda_k: 1 Loss: 0.34145150539418717\n",
      "Iteration: 384 lambda_k: 1 Loss: 0.34090590970486256\n",
      "Iteration: 385 lambda_k: 1 Loss: 0.3403615205785641\n",
      "Iteration: 386 lambda_k: 1 Loss: 0.33981832505309845\n",
      "Iteration: 387 lambda_k: 1 Loss: 0.33927631088900784\n",
      "Iteration: 388 lambda_k: 1 Loss: 0.3387354665192178\n",
      "Iteration: 389 lambda_k: 1 Loss: 0.3381957809747404\n",
      "Iteration: 390 lambda_k: 1 Loss: 0.3376572438732301\n",
      "Iteration: 391 lambda_k: 1 Loss: 0.3371198453556176\n",
      "Iteration: 392 lambda_k: 1 Loss: 0.33658357605607425\n",
      "Iteration: 393 lambda_k: 1 Loss: 0.3360484270651101\n",
      "Iteration: 394 lambda_k: 1 Loss: 0.3355143898968291\n",
      "Iteration: 395 lambda_k: 1 Loss: 0.33498145645773936\n",
      "Iteration: 396 lambda_k: 1 Loss: 0.3344496190229088\n",
      "Iteration: 397 lambda_k: 1 Loss: 0.33391887020715844\n",
      "Iteration: 398 lambda_k: 1 Loss: 0.3333892029344098\n",
      "Iteration: 399 lambda_k: 1 Loss: 0.3328606104286988\n",
      "Iteration: 400 lambda_k: 1 Loss: 0.3323330861841328\n",
      "Iteration: 401 lambda_k: 1 Loss: 0.3318066239493345\n",
      "Iteration: 402 lambda_k: 1 Loss: 0.33128121770911717\n",
      "Iteration: 403 lambda_k: 1 Loss: 0.3307568616684265\n",
      "Iteration: 404 lambda_k: 1 Loss: 0.3302335502369695\n",
      "Iteration: 405 lambda_k: 1 Loss: 0.32971127801546757\n",
      "Iteration: 406 lambda_k: 1 Loss: 0.3291900397826614\n",
      "Iteration: 407 lambda_k: 1 Loss: 0.3286698304833638\n",
      "Iteration: 408 lambda_k: 1 Loss: 0.32815064521738424\n",
      "Iteration: 409 lambda_k: 1 Loss: 0.3276324792292747\n",
      "Iteration: 410 lambda_k: 1 Loss: 0.3271153278988332\n",
      "Iteration: 411 lambda_k: 1 Loss: 0.3265991867328009\n",
      "Iteration: 412 lambda_k: 1 Loss: 0.3260840513548425\n",
      "Iteration: 413 lambda_k: 1 Loss: 0.32556991750074304\n",
      "Iteration: 414 lambda_k: 1 Loss: 0.32505678100951113\n",
      "Iteration: 415 lambda_k: 1 Loss: 0.3245446378175909\n",
      "Iteration: 416 lambda_k: 1 Loss: 0.32403348395274884\n",
      "Iteration: 417 lambda_k: 1 Loss: 0.32352331552852437\n",
      "Iteration: 418 lambda_k: 1 Loss: 0.32301412873908686\n",
      "Iteration: 419 lambda_k: 1 Loss: 0.3225059198542404\n",
      "Iteration: 420 lambda_k: 1 Loss: 0.32199868521577807\n",
      "Iteration: 421 lambda_k: 1 Loss: 0.3214924212323942\n",
      "Iteration: 422 lambda_k: 1 Loss: 0.32098712437652993\n",
      "Iteration: 423 lambda_k: 1 Loss: 0.32048279118064404\n",
      "Iteration: 424 lambda_k: 1 Loss: 0.3199794182339938\n",
      "Iteration: 425 lambda_k: 1 Loss: 0.3194770021796095\n",
      "Iteration: 426 lambda_k: 1 Loss: 0.3189755397115035\n",
      "Iteration: 427 lambda_k: 1 Loss: 0.3184750275720694\n",
      "Iteration: 428 lambda_k: 1 Loss: 0.3179754625496758\n",
      "Iteration: 429 lambda_k: 1 Loss: 0.3174768414764317\n",
      "Iteration: 430 lambda_k: 1 Loss: 0.31697916122611586\n",
      "Iteration: 431 lambda_k: 1 Loss: 0.31648241871225286\n",
      "Iteration: 432 lambda_k: 1 Loss: 0.3159866108863287\n",
      "Iteration: 433 lambda_k: 1 Loss: 0.3154917347361363\n",
      "Iteration: 434 lambda_k: 1 Loss: 0.3149977872842354\n",
      "Iteration: 435 lambda_k: 1 Loss: 0.31450476558652557\n",
      "Iteration: 436 lambda_k: 1 Loss: 0.31401266673091954\n",
      "Iteration: 437 lambda_k: 1 Loss: 0.3135214878361104\n",
      "Iteration: 438 lambda_k: 1 Loss: 0.3130312260504263\n",
      "Iteration: 439 lambda_k: 1 Loss: 0.31254187855076554\n",
      "Iteration: 440 lambda_k: 1 Loss: 0.312053442541606\n",
      "Iteration: 441 lambda_k: 1 Loss: 0.31156591525408445\n",
      "Iteration: 442 lambda_k: 1 Loss: 0.311079293945139\n",
      "Iteration: 443 lambda_k: 1 Loss: 0.310593575896711\n",
      "Iteration: 444 lambda_k: 1 Loss: 0.310108758415002\n",
      "Iteration: 445 lambda_k: 1 Loss: 0.3096248388297812\n",
      "Iteration: 446 lambda_k: 1 Loss: 0.30914181449374056\n",
      "Iteration: 447 lambda_k: 1 Loss: 0.3086596827818925\n",
      "Iteration: 448 lambda_k: 1 Loss: 0.3081784410910088\n",
      "Iteration: 449 lambda_k: 1 Loss: 0.30769808683910704\n",
      "Iteration: 450 lambda_k: 1 Loss: 0.30721861746491386\n",
      "Iteration: 451 lambda_k: 1 Loss: 0.3067400304274897\n",
      "Iteration: 452 lambda_k: 1 Loss: 0.3062623232057365\n",
      "Iteration: 453 lambda_k: 1 Loss: 0.30578549329800836\n",
      "Iteration: 454 lambda_k: 1 Loss: 0.30530953822175305\n",
      "Iteration: 455 lambda_k: 1 Loss: 0.3048344555131545\n",
      "Iteration: 456 lambda_k: 1 Loss: 0.30436024272680035\n",
      "Iteration: 457 lambda_k: 1 Loss: 0.30388689743537384\n",
      "Iteration: 458 lambda_k: 1 Loss: 0.3034144172293641\n",
      "Iteration: 459 lambda_k: 1 Loss: 0.3029427997167939\n",
      "Iteration: 460 lambda_k: 1 Loss: 0.30247204252296284\n",
      "Iteration: 461 lambda_k: 1 Loss: 0.30200214329020547\n",
      "Iteration: 462 lambda_k: 1 Loss: 0.30153309967766395\n",
      "Iteration: 463 lambda_k: 1 Loss: 0.3010649093610726\n",
      "Iteration: 464 lambda_k: 1 Loss: 0.3005975700325557\n",
      "Iteration: 465 lambda_k: 1 Loss: 0.30013107940043465\n",
      "Iteration: 466 lambda_k: 1 Loss: 0.29966543518904726\n",
      "Iteration: 467 lambda_k: 1 Loss: 0.2992006351385752\n",
      "Iteration: 468 lambda_k: 1 Loss: 0.29873667700488077\n",
      "Iteration: 469 lambda_k: 1 Loss: 0.29827355855935195\n",
      "Iteration: 470 lambda_k: 1 Loss: 0.2978112775887551\n",
      "Iteration: 471 lambda_k: 1 Loss: 0.29734983189509423\n",
      "Iteration: 472 lambda_k: 1 Loss: 0.29688921929547796\n",
      "Iteration: 473 lambda_k: 1 Loss: 0.29642943762199103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 474 lambda_k: 1 Loss: 0.2959704847215729\n",
      "Iteration: 475 lambda_k: 1 Loss: 0.29551235845590074\n",
      "Iteration: 476 lambda_k: 1 Loss: 0.29505505670127785\n",
      "Iteration: 477 lambda_k: 1 Loss: 0.2945985773485262\n",
      "Iteration: 478 lambda_k: 1 Loss: 0.29414291830288347\n",
      "Iteration: 479 lambda_k: 1 Loss: 0.29368807748390424\n",
      "Iteration: 480 lambda_k: 1 Loss: 0.29323405282536436\n",
      "Iteration: 481 lambda_k: 1 Loss: 0.29278084227516904\n",
      "Iteration: 482 lambda_k: 1 Loss: 0.2923284437952639\n",
      "Iteration: 483 lambda_k: 1 Loss: 0.2918768553615499\n",
      "Iteration: 484 lambda_k: 1 Loss: 0.29142607496379924\n",
      "Iteration: 485 lambda_k: 1 Loss: 0.2909761006055761\n",
      "Iteration: 486 lambda_k: 1 Loss: 0.2905269303041576\n",
      "Iteration: 487 lambda_k: 1 Loss: 0.29007856209045885\n",
      "Iteration: 488 lambda_k: 1 Loss: 0.28963099400895886\n",
      "Iteration: 489 lambda_k: 1 Loss: 0.28918422411762906\n",
      "Iteration: 490 lambda_k: 1 Loss: 0.28873825048786333\n",
      "Iteration: 491 lambda_k: 1 Loss: 0.2882930712044101\n",
      "Iteration: 492 lambda_k: 1 Loss: 0.2878486843653061\n",
      "Iteration: 493 lambda_k: 1 Loss: 0.28740508808181064\n",
      "Iteration: 494 lambda_k: 1 Loss: 0.2869622804783431\n",
      "Iteration: 495 lambda_k: 1 Loss: 0.28652025969242\n",
      "Iteration: 496 lambda_k: 1 Loss: 0.28607902387459444\n",
      "Iteration: 497 lambda_k: 1 Loss: 0.2856385711883963\n",
      "Iteration: 498 lambda_k: 1 Loss: 0.28519889981027374\n",
      "Iteration: 499 lambda_k: 1 Loss: 0.2847600079295353\n",
      "Iteration: 500 lambda_k: 1 Loss: 0.28432189374829403\n",
      "Iteration: 501 lambda_k: 1 Loss: 0.2838845554814111\n",
      "Iteration: 502 lambda_k: 1 Loss: 0.28344799135644166\n",
      "Iteration: 503 lambda_k: 1 Loss: 0.2830121996135806\n",
      "Iteration: 504 lambda_k: 1 Loss: 0.2825771785056095\n",
      "Iteration: 505 lambda_k: 1 Loss: 0.28214292629784465\n",
      "Iteration: 506 lambda_k: 1 Loss: 0.2817094412680852\n",
      "Iteration: 507 lambda_k: 1 Loss: 0.28127672170656187\n",
      "Iteration: 508 lambda_k: 1 Loss: 0.28084476591588714\n",
      "Iteration: 509 lambda_k: 1 Loss: 0.28041357221100566\n",
      "Iteration: 510 lambda_k: 1 Loss: 0.2799831389191447\n",
      "Iteration: 511 lambda_k: 1 Loss: 0.2795534643797662\n",
      "Iteration: 512 lambda_k: 1 Loss: 0.27912454694451827\n",
      "Iteration: 513 lambda_k: 1 Loss: 0.2786963849771883\n",
      "Iteration: 514 lambda_k: 1 Loss: 0.2782689768536556\n",
      "Iteration: 515 lambda_k: 1 Loss: 0.2778423209618448\n",
      "Iteration: 516 lambda_k: 1 Loss: 0.2774164157016806\n",
      "Iteration: 517 lambda_k: 1 Loss: 0.2769912594850412\n",
      "Iteration: 518 lambda_k: 1 Loss: 0.27656685073571413\n",
      "Iteration: 519 lambda_k: 1 Loss: 0.27614318788935066\n",
      "Iteration: 520 lambda_k: 1 Loss: 0.275720269393422\n",
      "Iteration: 521 lambda_k: 1 Loss: 0.2752980937071751\n",
      "Iteration: 522 lambda_k: 1 Loss: 0.27487665930158944\n",
      "Iteration: 523 lambda_k: 1 Loss: 0.2744559646593331\n",
      "Iteration: 524 lambda_k: 1 Loss: 0.2740360082747206\n",
      "Iteration: 525 lambda_k: 1 Loss: 0.27361678865366995\n",
      "Iteration: 526 lambda_k: 1 Loss: 0.27319830431366066\n",
      "Iteration: 527 lambda_k: 1 Loss: 0.27278055378369176\n",
      "Iteration: 528 lambda_k: 1 Loss: 0.27236353560424026\n",
      "Iteration: 529 lambda_k: 1 Loss: 0.27194724832721984\n",
      "Iteration: 530 lambda_k: 1 Loss: 0.2715316905159399\n",
      "Iteration: 531 lambda_k: 1 Loss: 0.2711168607450649\n",
      "Iteration: 532 lambda_k: 1 Loss: 0.2707027576005741\n",
      "Iteration: 533 lambda_k: 1 Loss: 0.27028937967972105\n",
      "Iteration: 534 lambda_k: 1 Loss: 0.2698767255909942\n",
      "Iteration: 535 lambda_k: 1 Loss: 0.2694647939540771\n",
      "Iteration: 536 lambda_k: 1 Loss: 0.2690535833998091\n",
      "Iteration: 537 lambda_k: 1 Loss: 0.2686430925701458\n",
      "Iteration: 538 lambda_k: 1 Loss: 0.2682333201181202\n",
      "Iteration: 539 lambda_k: 1 Loss: 0.26782426470780724\n",
      "Iteration: 540 lambda_k: 1 Loss: 0.2674159250142823\n",
      "Iteration: 541 lambda_k: 1 Loss: 0.26700829972358286\n",
      "Iteration: 542 lambda_k: 1 Loss: 0.26660138753267126\n",
      "Iteration: 543 lambda_k: 1 Loss: 0.26619518714939794\n",
      "Iteration: 544 lambda_k: 1 Loss: 0.265789697292463\n",
      "Iteration: 545 lambda_k: 1 Loss: 0.2653849166913802\n",
      "Iteration: 546 lambda_k: 1 Loss: 0.26498084408643907\n",
      "Iteration: 547 lambda_k: 1 Loss: 0.2645774782286685\n",
      "Iteration: 548 lambda_k: 1 Loss: 0.26417481787980035\n",
      "Iteration: 549 lambda_k: 1 Loss: 0.263772861812233\n",
      "Iteration: 550 lambda_k: 1 Loss: 0.2633716088089951\n",
      "Iteration: 551 lambda_k: 1 Loss: 0.2629710576637103\n",
      "Iteration: 552 lambda_k: 1 Loss: 0.2625712071805608\n",
      "Iteration: 553 lambda_k: 1 Loss: 0.2621720561742525\n",
      "Iteration: 554 lambda_k: 1 Loss: 0.26177360346997913\n",
      "Iteration: 555 lambda_k: 1 Loss: 0.261375847903388\n",
      "Iteration: 556 lambda_k: 1 Loss: 0.26097878832054433\n",
      "Iteration: 557 lambda_k: 1 Loss: 0.2605824235778967\n",
      "Iteration: 558 lambda_k: 1 Loss: 0.2601867525422428\n",
      "Iteration: 559 lambda_k: 1 Loss: 0.2597917740906953\n",
      "Iteration: 560 lambda_k: 1 Loss: 0.25939748711064664\n",
      "Iteration: 561 lambda_k: 1 Loss: 0.2590038904997364\n",
      "Iteration: 562 lambda_k: 1 Loss: 0.2586109831658168\n",
      "Iteration: 563 lambda_k: 1 Loss: 0.2582187640269192\n",
      "Iteration: 564 lambda_k: 1 Loss: 0.2578272320112206\n",
      "Iteration: 565 lambda_k: 1 Loss: 0.2574363860570106\n",
      "Iteration: 566 lambda_k: 1 Loss: 0.257046225112658\n",
      "Iteration: 567 lambda_k: 1 Loss: 0.2566567481365783\n",
      "Iteration: 568 lambda_k: 1 Loss: 0.2562679540972007\n",
      "Iteration: 569 lambda_k: 1 Loss: 0.25587984197293534\n",
      "Iteration: 570 lambda_k: 1 Loss: 0.25549241075214113\n",
      "Iteration: 571 lambda_k: 1 Loss: 0.25510565943309355\n",
      "Iteration: 572 lambda_k: 1 Loss: 0.2547195870239521\n",
      "Iteration: 573 lambda_k: 1 Loss: 0.25433419254272904\n",
      "Iteration: 574 lambda_k: 1 Loss: 0.253949475017257\n",
      "Iteration: 575 lambda_k: 1 Loss: 0.2535654334851576\n",
      "Iteration: 576 lambda_k: 1 Loss: 0.25318206699380974\n",
      "Iteration: 577 lambda_k: 1 Loss: 0.25279937460031865\n",
      "Iteration: 578 lambda_k: 1 Loss: 0.25241735537148446\n",
      "Iteration: 579 lambda_k: 1 Loss: 0.2520360083837707\n",
      "Iteration: 580 lambda_k: 1 Loss: 0.25165533272327445\n",
      "Iteration: 581 lambda_k: 1 Loss: 0.2512753274856947\n",
      "Iteration: 582 lambda_k: 1 Loss: 0.250895991776302\n",
      "Iteration: 583 lambda_k: 1 Loss: 0.25051732470990806\n",
      "Iteration: 584 lambda_k: 1 Loss: 0.25013932541083506\n",
      "Iteration: 585 lambda_k: 1 Loss: 0.24976199301289428\n",
      "Iteration: 586 lambda_k: 1 Loss: 0.24938532665930205\n",
      "Iteration: 587 lambda_k: 1 Loss: 0.24900932550276605\n",
      "Iteration: 588 lambda_k: 1 Loss: 0.24863398870535933\n",
      "Iteration: 589 lambda_k: 1 Loss: 0.24825931543849475\n",
      "Iteration: 590 lambda_k: 1 Loss: 0.24788530488292435\n",
      "Iteration: 591 lambda_k: 1 Loss: 0.2475119562287072\n",
      "Iteration: 592 lambda_k: 1 Loss: 0.2471392686751748\n",
      "Iteration: 593 lambda_k: 1 Loss: 0.24676724143090115\n",
      "Iteration: 594 lambda_k: 1 Loss: 0.2463958737136743\n",
      "Iteration: 595 lambda_k: 1 Loss: 0.2460251647504674\n",
      "Iteration: 596 lambda_k: 1 Loss: 0.2456551137774098\n",
      "Iteration: 597 lambda_k: 1 Loss: 0.24528572003975863\n",
      "Iteration: 598 lambda_k: 1 Loss: 0.24491698279186946\n",
      "Iteration: 599 lambda_k: 1 Loss: 0.24454890129716797\n",
      "Iteration: 600 lambda_k: 1 Loss: 0.24418147482812247\n",
      "Iteration: 601 lambda_k: 1 Loss: 0.2438147026662143\n",
      "Iteration: 602 lambda_k: 1 Loss: 0.24344858410190934\n",
      "Iteration: 603 lambda_k: 1 Loss: 0.24308311843463065\n",
      "Iteration: 604 lambda_k: 1 Loss: 0.24271830497272986\n",
      "Iteration: 605 lambda_k: 1 Loss: 0.24235414303350641\n",
      "Iteration: 606 lambda_k: 1 Loss: 0.2419906319429995\n",
      "Iteration: 607 lambda_k: 1 Loss: 0.2416277710361593\n",
      "Iteration: 608 lambda_k: 1 Loss: 0.24126555965637647\n",
      "Iteration: 609 lambda_k: 1 Loss: 0.24090399715737693\n",
      "Iteration: 610 lambda_k: 1 Loss: 0.24054308289891707\n",
      "Iteration: 611 lambda_k: 1 Loss: 0.24018281625256624\n",
      "Iteration: 612 lambda_k: 1 Loss: 0.23982319659196233\n",
      "Iteration: 613 lambda_k: 1 Loss: 0.23946363952916092\n",
      "Iteration: 614 lambda_k: 1 Loss: 0.23910381542470313\n",
      "Iteration: 615 lambda_k: 1 Loss: 0.23874508499169722\n",
      "Iteration: 616 lambda_k: 1 Loss: 0.23838765536203305\n",
      "Iteration: 617 lambda_k: 1 Loss: 0.23803141997840635\n",
      "Iteration: 618 lambda_k: 1 Loss: 0.23767625072747167\n",
      "Iteration: 619 lambda_k: 1 Loss: 0.2373220632483352\n",
      "Iteration: 620 lambda_k: 1 Loss: 0.23696889359521092\n",
      "Iteration: 621 lambda_k: 1 Loss: 0.23661644154903072\n",
      "Iteration: 622 lambda_k: 1 Loss: 0.23626474749535242\n",
      "Iteration: 623 lambda_k: 1 Loss: 0.23591375908936052\n",
      "Iteration: 624 lambda_k: 1 Loss: 0.2355634063427452\n",
      "Iteration: 625 lambda_k: 1 Loss: 0.23521364104168974\n",
      "Iteration: 626 lambda_k: 1 Loss: 0.23486443162494589\n",
      "Iteration: 627 lambda_k: 1 Loss: 0.23451575325813587\n",
      "Iteration: 628 lambda_k: 1 Loss: 0.23416758411720492\n",
      "Iteration: 629 lambda_k: 1 Loss: 0.2338199053660541\n",
      "Iteration: 630 lambda_k: 1 Loss: 0.23347270132528086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 631 lambda_k: 1 Loss: 0.2331259590491161\n",
      "Iteration: 632 lambda_k: 1 Loss: 0.23277966775097442\n",
      "Iteration: 633 lambda_k: 1 Loss: 0.23243381837514354\n",
      "Iteration: 634 lambda_k: 1 Loss: 0.2320884033089699\n",
      "Iteration: 635 lambda_k: 1 Loss: 0.2317434161587077\n",
      "Iteration: 636 lambda_k: 1 Loss: 0.23139885155434936\n",
      "Iteration: 637 lambda_k: 1 Loss: 0.23105470498089362\n",
      "Iteration: 638 lambda_k: 1 Loss: 0.2307109726372428\n",
      "Iteration: 639 lambda_k: 1 Loss: 0.23036765131933631\n",
      "Iteration: 640 lambda_k: 1 Loss: 0.23002473832250314\n",
      "Iteration: 641 lambda_k: 1 Loss: 0.22968223135911925\n",
      "Iteration: 642 lambda_k: 1 Loss: 0.22934012848896596\n",
      "Iteration: 643 lambda_k: 1 Loss: 0.22899842806035628\n",
      "Iteration: 644 lambda_k: 1 Loss: 0.2286571286603861\n",
      "Iteration: 645 lambda_k: 1 Loss: 0.22831622907288504\n",
      "Iteration: 646 lambda_k: 1 Loss: 0.22797572824286952\n",
      "Iteration: 647 lambda_k: 1 Loss: 0.22763562524650505\n",
      "Iteration: 648 lambda_k: 1 Loss: 0.22729591926575224\n",
      "Iteration: 649 lambda_k: 1 Loss: 0.22695660956700028\n",
      "Iteration: 650 lambda_k: 1 Loss: 0.22661769548309763\n",
      "Iteration: 651 lambda_k: 1 Loss: 0.2262791763982781\n",
      "Iteration: 652 lambda_k: 1 Loss: 0.22594105173555504\n",
      "Iteration: 653 lambda_k: 1 Loss: 0.2256033209462187\n",
      "Iteration: 654 lambda_k: 1 Loss: 0.22526598350112606\n",
      "Iteration: 655 lambda_k: 1 Loss: 0.22492903888351523\n",
      "Iteration: 656 lambda_k: 1 Loss: 0.22459248658311762\n",
      "Iteration: 657 lambda_k: 1 Loss: 0.2242563260913705\n",
      "Iteration: 658 lambda_k: 1 Loss: 0.22392055689756396\n",
      "Iteration: 659 lambda_k: 1 Loss: 0.2235851784857764\n",
      "Iteration: 660 lambda_k: 1 Loss: 0.22325019033247642\n",
      "Iteration: 661 lambda_k: 1 Loss: 0.2229155919046852\n",
      "Iteration: 662 lambda_k: 1 Loss: 0.22258138265860866\n",
      "Iteration: 663 lambda_k: 1 Loss: 0.22224756203866164\n",
      "Iteration: 664 lambda_k: 1 Loss: 0.22191412947681916\n",
      "Iteration: 665 lambda_k: 1 Loss: 0.2215810843922366\n",
      "Iteration: 666 lambda_k: 1 Loss: 0.2212484261910919\n",
      "Iteration: 667 lambda_k: 1 Loss: 0.2209161542666088\n",
      "Iteration: 668 lambda_k: 1 Loss: 0.22058426799922543\n",
      "Iteration: 669 lambda_k: 1 Loss: 0.2202527667568799\n",
      "Iteration: 670 lambda_k: 1 Loss: 0.21992164989538765\n",
      "Iteration: 671 lambda_k: 1 Loss: 0.21959091675888903\n",
      "Iteration: 672 lambda_k: 1 Loss: 0.21926056668035063\n",
      "Iteration: 673 lambda_k: 1 Loss: 0.21893059898275377\n",
      "Iteration: 674 lambda_k: 1 Loss: 0.21860101297649126\n",
      "Iteration: 675 lambda_k: 1 Loss: 0.21827180796593418\n",
      "Iteration: 676 lambda_k: 1 Loss: 0.21794298324463632\n",
      "Iteration: 677 lambda_k: 1 Loss: 0.21761453809809125\n",
      "Iteration: 678 lambda_k: 1 Loss: 0.21728647180418018\n",
      "Iteration: 679 lambda_k: 1 Loss: 0.21695878363363463\n",
      "Iteration: 680 lambda_k: 1 Loss: 0.2166314728505855\n",
      "Iteration: 681 lambda_k: 1 Loss: 0.21630453871312705\n",
      "Iteration: 682 lambda_k: 1 Loss: 0.2159779804738581\n",
      "Iteration: 683 lambda_k: 1 Loss: 0.21565179738039353\n",
      "Iteration: 684 lambda_k: 1 Loss: 0.21532598867584238\n",
      "Iteration: 685 lambda_k: 1 Loss: 0.21500055359930578\n",
      "Iteration: 686 lambda_k: 1 Loss: 0.21467549138631187\n",
      "Iteration: 687 lambda_k: 1 Loss: 0.21435080126924794\n",
      "Iteration: 688 lambda_k: 1 Loss: 0.21402648247707506\n",
      "Iteration: 689 lambda_k: 1 Loss: 0.21370253423909988\n",
      "Iteration: 690 lambda_k: 1 Loss: 0.21337895577899318\n",
      "Iteration: 691 lambda_k: 1 Loss: 0.21305574632079505\n",
      "Iteration: 692 lambda_k: 1 Loss: 0.2127329050870049\n",
      "Iteration: 693 lambda_k: 1 Loss: 0.21241043129902598\n",
      "Iteration: 694 lambda_k: 1 Loss: 0.21208832417757156\n",
      "Iteration: 695 lambda_k: 1 Loss: 0.21176658294295686\n",
      "Iteration: 696 lambda_k: 1 Loss: 0.21144520681534323\n",
      "Iteration: 697 lambda_k: 1 Loss: 0.21112419501497337\n",
      "Iteration: 698 lambda_k: 1 Loss: 0.21080354676239937\n",
      "Iteration: 699 lambda_k: 1 Loss: 0.2104832612786977\n",
      "Iteration: 700 lambda_k: 1 Loss: 0.21016333778566998\n",
      "Iteration: 701 lambda_k: 1 Loss: 0.2098437755060303\n",
      "Iteration: 702 lambda_k: 1 Loss: 0.20952457366358043\n",
      "Iteration: 703 lambda_k: 1 Loss: 0.2092057314833731\n",
      "Iteration: 704 lambda_k: 1 Loss: 0.20888724819186516\n",
      "Iteration: 705 lambda_k: 1 Loss: 0.20856912301705988\n",
      "Iteration: 706 lambda_k: 1 Loss: 0.20825135518863963\n",
      "Iteration: 707 lambda_k: 1 Loss: 0.20793394393808984\n",
      "Iteration: 708 lambda_k: 1 Loss: 0.20761688849881368\n",
      "Iteration: 709 lambda_k: 1 Loss: 0.20730018810623915\n",
      "Iteration: 710 lambda_k: 1 Loss: 0.20698384199791853\n",
      "Iteration: 711 lambda_k: 1 Loss: 0.2066678494136201\n",
      "Iteration: 712 lambda_k: 1 Loss: 0.20635220959541392\n",
      "Iteration: 713 lambda_k: 1 Loss: 0.20603692178775093\n",
      "Iteration: 714 lambda_k: 1 Loss: 0.20572198523753582\n",
      "Iteration: 715 lambda_k: 1 Loss: 0.20540739919419493\n",
      "Iteration: 716 lambda_k: 1 Loss: 0.2050931629097386\n",
      "Iteration: 717 lambda_k: 1 Loss: 0.20477927563881856\n",
      "Iteration: 718 lambda_k: 1 Loss: 0.20446573663878104\n",
      "Iteration: 719 lambda_k: 1 Loss: 0.20415254516971526\n",
      "Iteration: 720 lambda_k: 1 Loss: 0.20383970049449815\n",
      "Iteration: 721 lambda_k: 1 Loss: 0.20352720187883536\n",
      "Iteration: 722 lambda_k: 1 Loss: 0.20321504859129838\n",
      "Iteration: 723 lambda_k: 1 Loss: 0.2029032399033587\n",
      "Iteration: 724 lambda_k: 1 Loss: 0.20259177508941908\n",
      "Iteration: 725 lambda_k: 1 Loss: 0.20228065342684132\n",
      "Iteration: 726 lambda_k: 1 Loss: 0.20196987419597187\n",
      "Iteration: 727 lambda_k: 1 Loss: 0.20165943668016484\n",
      "Iteration: 728 lambda_k: 1 Loss: 0.20134934016580247\n",
      "Iteration: 729 lambda_k: 1 Loss: 0.20103958394231383\n",
      "Iteration: 730 lambda_k: 1 Loss: 0.20073016730219073\n",
      "Iteration: 731 lambda_k: 1 Loss: 0.2004210895410026\n",
      "Iteration: 732 lambda_k: 1 Loss: 0.20011234995740898\n",
      "Iteration: 733 lambda_k: 1 Loss: 0.19980394785317054\n",
      "Iteration: 734 lambda_k: 1 Loss: 0.19949588253315878\n",
      "Iteration: 735 lambda_k: 1 Loss: 0.19918815330536382\n",
      "Iteration: 736 lambda_k: 1 Loss: 0.19888075948090123\n",
      "Iteration: 737 lambda_k: 1 Loss: 0.19857370037401748\n",
      "Iteration: 738 lambda_k: 1 Loss: 0.1982669753020942\n",
      "Iteration: 739 lambda_k: 1 Loss: 0.1979605835856514\n",
      "Iteration: 740 lambda_k: 1 Loss: 0.1976545245483499\n",
      "Iteration: 741 lambda_k: 1 Loss: 0.19734879751699197\n",
      "Iteration: 742 lambda_k: 1 Loss: 0.19704340182152255\n",
      "Iteration: 743 lambda_k: 1 Loss: 0.19673833679502817\n",
      "Iteration: 744 lambda_k: 1 Loss: 0.19643360177373614\n",
      "Iteration: 745 lambda_k: 1 Loss: 0.1961291960970125\n",
      "Iteration: 746 lambda_k: 1 Loss: 0.19582511910735978\n",
      "Iteration: 747 lambda_k: 1 Loss: 0.19552137015041351\n",
      "Iteration: 748 lambda_k: 1 Loss: 0.19521794857493877\n",
      "Iteration: 749 lambda_k: 1 Loss: 0.19491485373282624\n",
      "Iteration: 750 lambda_k: 1 Loss: 0.19461208497908694\n",
      "Iteration: 751 lambda_k: 1 Loss: 0.19430964167184792\n",
      "Iteration: 752 lambda_k: 1 Loss: 0.1940075231723461\n",
      "Iteration: 753 lambda_k: 1 Loss: 0.19370572884492268\n",
      "Iteration: 754 lambda_k: 1 Loss: 0.19340425805701703\n",
      "Iteration: 755 lambda_k: 1 Loss: 0.1931031101791601\n",
      "Iteration: 756 lambda_k: 1 Loss: 0.1928022845849674\n",
      "Iteration: 757 lambda_k: 1 Loss: 0.19250178065113216\n",
      "Iteration: 758 lambda_k: 1 Loss: 0.19220159775741807\n",
      "Iteration: 759 lambda_k: 1 Loss: 0.1919017352866513\n",
      "Iteration: 760 lambda_k: 1 Loss: 0.19160219262471334\n",
      "Iteration: 761 lambda_k: 1 Loss: 0.19130296916053263\n",
      "Iteration: 762 lambda_k: 1 Loss: 0.19100406428607672\n",
      "Iteration: 763 lambda_k: 1 Loss: 0.19070547739634378\n",
      "Iteration: 764 lambda_k: 1 Loss: 0.19040720788935442\n",
      "Iteration: 765 lambda_k: 1 Loss: 0.19010925516614322\n",
      "Iteration: 766 lambda_k: 1 Loss: 0.18981161863074986\n",
      "Iteration: 767 lambda_k: 1 Loss: 0.18951429769021055\n",
      "Iteration: 768 lambda_k: 1 Loss: 0.18921729175454946\n",
      "Iteration: 769 lambda_k: 1 Loss: 0.18892060023676935\n",
      "Iteration: 770 lambda_k: 1 Loss: 0.1886242225528432\n",
      "Iteration: 771 lambda_k: 1 Loss: 0.18832815812170473\n",
      "Iteration: 772 lambda_k: 1 Loss: 0.18803240636523963\n",
      "Iteration: 773 lambda_k: 1 Loss: 0.18773696670827666\n",
      "Iteration: 774 lambda_k: 1 Loss: 0.18744183857857802\n",
      "Iteration: 775 lambda_k: 1 Loss: 0.18714702140683068\n",
      "Iteration: 776 lambda_k: 1 Loss: 0.1868525146266371\n",
      "Iteration: 777 lambda_k: 1 Loss: 0.18655831767450615\n",
      "Iteration: 778 lambda_k: 1 Loss: 0.1862644299898436\n",
      "Iteration: 779 lambda_k: 1 Loss: 0.1859708510149436\n",
      "Iteration: 780 lambda_k: 1 Loss: 0.1856775801949789\n",
      "Iteration: 781 lambda_k: 1 Loss: 0.18538461697799216\n",
      "Iteration: 782 lambda_k: 1 Loss: 0.18509196081488655\n",
      "Iteration: 783 lambda_k: 1 Loss: 0.18479961115941676\n",
      "Iteration: 784 lambda_k: 1 Loss: 0.18450756746817987\n",
      "Iteration: 785 lambda_k: 1 Loss: 0.18421582920060642\n",
      "Iteration: 786 lambda_k: 1 Loss: 0.18392439581895134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 787 lambda_k: 1 Loss: 0.18363326678828482\n",
      "Iteration: 788 lambda_k: 1 Loss: 0.1833424415764834\n",
      "Iteration: 789 lambda_k: 1 Loss: 0.18305191965422127\n",
      "Iteration: 790 lambda_k: 1 Loss: 0.1827617004949612\n",
      "Iteration: 791 lambda_k: 1 Loss: 0.18247178357494565\n",
      "Iteration: 792 lambda_k: 1 Loss: 0.18218216837318815\n",
      "Iteration: 793 lambda_k: 1 Loss: 0.18189285437146457\n",
      "Iteration: 794 lambda_k: 1 Loss: 0.1816038410543043\n",
      "Iteration: 795 lambda_k: 1 Loss: 0.18131512790898174\n",
      "Iteration: 796 lambda_k: 1 Loss: 0.1810267144255077\n",
      "Iteration: 797 lambda_k: 1 Loss: 0.18073860009662068\n",
      "Iteration: 798 lambda_k: 1 Loss: 0.1804507844177788\n",
      "Iteration: 799 lambda_k: 1 Loss: 0.180163266887151\n",
      "Iteration: 800 lambda_k: 1 Loss: 0.17987604700560908\n",
      "Iteration: 801 lambda_k: 1 Loss: 0.17958912427671908\n",
      "Iteration: 802 lambda_k: 1 Loss: 0.1793024982067332\n",
      "Iteration: 803 lambda_k: 1 Loss: 0.17901616830458147\n",
      "Iteration: 804 lambda_k: 1 Loss: 0.17873013408186417\n",
      "Iteration: 805 lambda_k: 1 Loss: 0.17844439505284312\n",
      "Iteration: 806 lambda_k: 1 Loss: 0.17815895073443425\n",
      "Iteration: 807 lambda_k: 1 Loss: 0.17787380064619945\n",
      "Iteration: 808 lambda_k: 1 Loss: 0.17758894431033873\n",
      "Iteration: 809 lambda_k: 1 Loss: 0.17730438125168255\n",
      "Iteration: 810 lambda_k: 1 Loss: 0.17702011099768422\n",
      "Iteration: 811 lambda_k: 1 Loss: 0.17673613307841213\n",
      "Iteration: 812 lambda_k: 1 Loss: 0.17645244702654214\n",
      "Iteration: 813 lambda_k: 1 Loss: 0.1761690523773504\n",
      "Iteration: 814 lambda_k: 1 Loss: 0.17588594866870552\n",
      "Iteration: 815 lambda_k: 1 Loss: 0.1756031354410619\n",
      "Iteration: 816 lambda_k: 1 Loss: 0.17532061223745143\n",
      "Iteration: 817 lambda_k: 1 Loss: 0.17503837860347748\n",
      "Iteration: 818 lambda_k: 1 Loss: 0.17475643408730693\n",
      "Iteration: 819 lambda_k: 1 Loss: 0.1744747782396638\n",
      "Iteration: 820 lambda_k: 1 Loss: 0.1741934106138216\n",
      "Iteration: 821 lambda_k: 1 Loss: 0.17391233076559678\n",
      "Iteration: 822 lambda_k: 1 Loss: 0.17363153825334207\n",
      "Iteration: 823 lambda_k: 1 Loss: 0.17335103263793947\n",
      "Iteration: 824 lambda_k: 1 Loss: 0.173070813482794\n",
      "Iteration: 825 lambda_k: 1 Loss: 0.17279088035382645\n",
      "Iteration: 826 lambda_k: 1 Loss: 0.17251123281946754\n",
      "Iteration: 827 lambda_k: 1 Loss: 0.172231870450651\n",
      "Iteration: 828 lambda_k: 1 Loss: 0.1719527928208076\n",
      "Iteration: 829 lambda_k: 1 Loss: 0.17167399950585874\n",
      "Iteration: 830 lambda_k: 1 Loss: 0.17139549008420998\n",
      "Iteration: 831 lambda_k: 1 Loss: 0.17111726413674547\n",
      "Iteration: 832 lambda_k: 1 Loss: 0.17083932124682133\n",
      "Iteration: 833 lambda_k: 1 Loss: 0.17056166100026005\n",
      "Iteration: 834 lambda_k: 1 Loss: 0.1702842829853447\n",
      "Iteration: 835 lambda_k: 1 Loss: 0.1700071867928127\n",
      "Iteration: 836 lambda_k: 1 Loss: 0.1697303720158504\n",
      "Iteration: 837 lambda_k: 1 Loss: 0.16945383825008742\n",
      "Iteration: 838 lambda_k: 1 Loss: 0.16917758509359102\n",
      "Iteration: 839 lambda_k: 1 Loss: 0.16890161214686056\n",
      "Iteration: 840 lambda_k: 1 Loss: 0.1686259190128223\n",
      "Iteration: 841 lambda_k: 1 Loss: 0.16835050529682366\n",
      "Iteration: 842 lambda_k: 1 Loss: 0.16807537060662842\n",
      "Iteration: 843 lambda_k: 1 Loss: 0.16780051455241143\n",
      "Iteration: 844 lambda_k: 1 Loss: 0.16752593674675328\n",
      "Iteration: 845 lambda_k: 1 Loss: 0.16725163680463548\n",
      "Iteration: 846 lambda_k: 1 Loss: 0.16697761434343564\n",
      "Iteration: 847 lambda_k: 1 Loss: 0.16670386898292233\n",
      "Iteration: 848 lambda_k: 1 Loss: 0.16643040034525047\n",
      "Iteration: 849 lambda_k: 1 Loss: 0.1661572080549569\n",
      "Iteration: 850 lambda_k: 1 Loss: 0.16588429173895497\n",
      "Iteration: 851 lambda_k: 1 Loss: 0.16561165102653078\n",
      "Iteration: 852 lambda_k: 1 Loss: 0.16533928554933858\n",
      "Iteration: 853 lambda_k: 1 Loss: 0.16506719494139627\n",
      "Iteration: 854 lambda_k: 1 Loss: 0.1647953788390811\n",
      "Iteration: 855 lambda_k: 1 Loss: 0.16452383688112546\n",
      "Iteration: 856 lambda_k: 1 Loss: 0.1642525687086129\n",
      "Iteration: 857 lambda_k: 1 Loss: 0.16398157396497387\n",
      "Iteration: 858 lambda_k: 1 Loss: 0.1637108522959819\n",
      "Iteration: 859 lambda_k: 1 Loss: 0.16344040334974963\n",
      "Iteration: 860 lambda_k: 1 Loss: 0.16317022677672507\n",
      "Iteration: 861 lambda_k: 1 Loss: 0.16290032222968767\n",
      "Iteration: 862 lambda_k: 1 Loss: 0.16263068936374503\n",
      "Iteration: 863 lambda_k: 1 Loss: 0.16236132783632903\n",
      "Iteration: 864 lambda_k: 1 Loss: 0.1620922373071925\n",
      "Iteration: 865 lambda_k: 1 Loss: 0.161823417438497\n",
      "Iteration: 866 lambda_k: 1 Loss: 0.1615548678943845\n",
      "Iteration: 867 lambda_k: 1 Loss: 0.1612865883417208\n",
      "Iteration: 868 lambda_k: 1 Loss: 0.16101857844949805\n",
      "Iteration: 869 lambda_k: 1 Loss: 0.16075083788901862\n",
      "Iteration: 870 lambda_k: 1 Loss: 0.16048336633388965\n",
      "Iteration: 871 lambda_k: 1 Loss: 0.1602161634600175\n",
      "Iteration: 872 lambda_k: 1 Loss: 0.15994922894560737\n",
      "Iteration: 873 lambda_k: 1 Loss: 0.15968256247183735\n",
      "Iteration: 874 lambda_k: 1 Loss: 0.15941616371968662\n",
      "Iteration: 875 lambda_k: 1 Loss: 0.15915003237547784\n",
      "Iteration: 876 lambda_k: 1 Loss: 0.15888416812650816\n",
      "Iteration: 877 lambda_k: 1 Loss: 0.1586185706623783\n",
      "Iteration: 878 lambda_k: 1 Loss: 0.15835323967500353\n",
      "Iteration: 879 lambda_k: 1 Loss: 0.15808817485859125\n",
      "Iteration: 880 lambda_k: 1 Loss: 0.1578233759096236\n",
      "Iteration: 881 lambda_k: 1 Loss: 0.15755884252685226\n",
      "Iteration: 882 lambda_k: 1 Loss: 0.1572945744112987\n",
      "Iteration: 883 lambda_k: 1 Loss: 0.15703057126625436\n",
      "Iteration: 884 lambda_k: 1 Loss: 0.15676683279727924\n",
      "Iteration: 885 lambda_k: 1 Loss: 0.15650335871219975\n",
      "Iteration: 886 lambda_k: 1 Loss: 0.15624014872110661\n",
      "Iteration: 887 lambda_k: 1 Loss: 0.15597720253635328\n",
      "Iteration: 888 lambda_k: 1 Loss: 0.15571451987255427\n",
      "Iteration: 889 lambda_k: 1 Loss: 0.15545210044658378\n",
      "Iteration: 890 lambda_k: 1 Loss: 0.1551899439775741\n",
      "Iteration: 891 lambda_k: 1 Loss: 0.1549280501869144\n",
      "Iteration: 892 lambda_k: 1 Loss: 0.15466641879824944\n",
      "Iteration: 893 lambda_k: 1 Loss: 0.15440504953747838\n",
      "Iteration: 894 lambda_k: 1 Loss: 0.1541439421327534\n",
      "Iteration: 895 lambda_k: 1 Loss: 0.15388309631447913\n",
      "Iteration: 896 lambda_k: 1 Loss: 0.15362251181531117\n",
      "Iteration: 897 lambda_k: 1 Loss: 0.1533621883701556\n",
      "Iteration: 898 lambda_k: 1 Loss: 0.1531021257161681\n",
      "Iteration: 899 lambda_k: 1 Loss: 0.15284232359275288\n",
      "Iteration: 900 lambda_k: 1 Loss: 0.15258278174156242\n",
      "Iteration: 901 lambda_k: 1 Loss: 0.1523234999064966\n",
      "Iteration: 902 lambda_k: 1 Loss: 0.1520644778337025\n",
      "Iteration: 903 lambda_k: 1 Loss: 0.15180571527157363\n",
      "Iteration: 904 lambda_k: 1 Loss: 0.15154721197074977\n",
      "Iteration: 905 lambda_k: 1 Loss: 0.15128896768411665\n",
      "Iteration: 906 lambda_k: 1 Loss: 0.15103098216680572\n",
      "Iteration: 907 lambda_k: 1 Loss: 0.150773255176194\n",
      "Iteration: 908 lambda_k: 1 Loss: 0.15051578647190403\n",
      "Iteration: 909 lambda_k: 1 Loss: 0.15025857581580393\n",
      "Iteration: 910 lambda_k: 1 Loss: 0.15000162297200756\n",
      "Iteration: 911 lambda_k: 1 Loss: 0.14974492770687436\n",
      "Iteration: 912 lambda_k: 1 Loss: 0.14948848978901\n",
      "Iteration: 913 lambda_k: 1 Loss: 0.14923230898926648\n",
      "Iteration: 914 lambda_k: 1 Loss: 0.14897638508074248\n",
      "Iteration: 915 lambda_k: 1 Loss: 0.14872071783878385\n",
      "Iteration: 916 lambda_k: 1 Loss: 0.1484653070409843\n",
      "Iteration: 917 lambda_k: 1 Loss: 0.14821015246718575\n",
      "Iteration: 918 lambda_k: 1 Loss: 0.14795525389947914\n",
      "Iteration: 919 lambda_k: 1 Loss: 0.1477006111222053\n",
      "Iteration: 920 lambda_k: 1 Loss: 0.14744622392195547\n",
      "Iteration: 921 lambda_k: 1 Loss: 0.1471920920875724\n",
      "Iteration: 922 lambda_k: 1 Loss: 0.14693821541015126\n",
      "Iteration: 923 lambda_k: 1 Loss: 0.14668459368304082\n",
      "Iteration: 924 lambda_k: 1 Loss: 0.14643122670184433\n",
      "Iteration: 925 lambda_k: 1 Loss: 0.14617811426442076\n",
      "Iteration: 926 lambda_k: 1 Loss: 0.14592525617088617\n",
      "Iteration: 927 lambda_k: 1 Loss: 0.14567265222361495\n",
      "Iteration: 928 lambda_k: 1 Loss: 0.1454203022272412\n",
      "Iteration: 929 lambda_k: 1 Loss: 0.14516820598866018\n",
      "Iteration: 930 lambda_k: 1 Loss: 0.14491636331702987\n",
      "Iteration: 931 lambda_k: 1 Loss: 0.14466477402377262\n",
      "Iteration: 932 lambda_k: 1 Loss: 0.1444134379225767\n",
      "Iteration: 933 lambda_k: 1 Loss: 0.14416235482939818\n",
      "Iteration: 934 lambda_k: 1 Loss: 0.14391152456246267\n",
      "Iteration: 935 lambda_k: 1 Loss: 0.14366094694226725\n",
      "Iteration: 936 lambda_k: 1 Loss: 0.1434106217915822\n",
      "Iteration: 937 lambda_k: 1 Loss: 0.1431605489354535\n",
      "Iteration: 938 lambda_k: 1 Loss: 0.14291072820120435\n",
      "Iteration: 939 lambda_k: 1 Loss: 0.14266115941843774\n",
      "Iteration: 940 lambda_k: 1 Loss: 0.14241184241903848\n",
      "Iteration: 941 lambda_k: 1 Loss: 0.14216277703717559\n",
      "Iteration: 942 lambda_k: 1 Loss: 0.14191396310930462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 943 lambda_k: 1 Loss: 0.14166540047417234\n",
      "Iteration: 944 lambda_k: 1 Loss: 0.14141708897281322\n",
      "Iteration: 945 lambda_k: 1 Loss: 0.14116902844855533\n",
      "Iteration: 946 lambda_k: 1 Loss: 0.14092121874702557\n",
      "Iteration: 947 lambda_k: 1 Loss: 0.14067365971615012\n",
      "Iteration: 948 lambda_k: 1 Loss: 0.14042635120615732\n",
      "Iteration: 949 lambda_k: 1 Loss: 0.14017929306958035\n",
      "Iteration: 950 lambda_k: 1 Loss: 0.13993248516126033\n",
      "Iteration: 951 lambda_k: 1 Loss: 0.13968592733834942\n",
      "Iteration: 952 lambda_k: 1 Loss: 0.13943961946031344\n",
      "Iteration: 953 lambda_k: 1 Loss: 0.1391935613889354\n",
      "Iteration: 954 lambda_k: 1 Loss: 0.13894775298831838\n",
      "Iteration: 955 lambda_k: 1 Loss: 0.138702194124889\n",
      "Iteration: 956 lambda_k: 1 Loss: 0.1384568846674006\n",
      "Iteration: 957 lambda_k: 1 Loss: 0.13821182448693667\n",
      "Iteration: 958 lambda_k: 1 Loss: 0.13796701345691423\n",
      "Iteration: 959 lambda_k: 1 Loss: 0.1377224514530892\n",
      "Iteration: 960 lambda_k: 1 Loss: 0.13747813835355455\n",
      "Iteration: 961 lambda_k: 1 Loss: 0.13723407403874868\n",
      "Iteration: 962 lambda_k: 1 Loss: 0.136990258391459\n",
      "Iteration: 963 lambda_k: 1 Loss: 0.13674669129682446\n",
      "Iteration: 964 lambda_k: 1 Loss: 0.13650337264233972\n",
      "Iteration: 965 lambda_k: 1 Loss: 0.13626030231785904\n",
      "Iteration: 966 lambda_k: 1 Loss: 0.1360174802156\n",
      "Iteration: 967 lambda_k: 1 Loss: 0.13577490623014818\n",
      "Iteration: 968 lambda_k: 1 Loss: 0.1355325802584606\n",
      "Iteration: 969 lambda_k: 1 Loss: 0.13529050219987018\n",
      "Iteration: 970 lambda_k: 1 Loss: 0.1350486719560901\n",
      "Iteration: 971 lambda_k: 1 Loss: 0.1348070894312179\n",
      "Iteration: 972 lambda_k: 1 Loss: 0.13456575453174016\n",
      "Iteration: 973 lambda_k: 1 Loss: 0.1343246671665365\n",
      "Iteration: 974 lambda_k: 1 Loss: 0.13408382724688458\n",
      "Iteration: 975 lambda_k: 1 Loss: 0.1338432346864644\n",
      "Iteration: 976 lambda_k: 1 Loss: 0.133602889401363\n",
      "Iteration: 977 lambda_k: 1 Loss: 0.1333627913100793\n",
      "Iteration: 978 lambda_k: 1 Loss: 0.13312294033352876\n",
      "Iteration: 979 lambda_k: 1 Loss: 0.1328833363950513\n",
      "Iteration: 980 lambda_k: 1 Loss: 0.13264397942041245\n",
      "Iteration: 981 lambda_k: 1 Loss: 0.1324048693377993\n",
      "Iteration: 982 lambda_k: 1 Loss: 0.13216600607784285\n",
      "Iteration: 983 lambda_k: 1 Loss: 0.13192738957361605\n",
      "Iteration: 984 lambda_k: 1 Loss: 0.1316890197606385\n",
      "Iteration: 985 lambda_k: 1 Loss: 0.13145089657688164\n",
      "Iteration: 986 lambda_k: 1 Loss: 0.1312130199627743\n",
      "Iteration: 987 lambda_k: 1 Loss: 0.1309753898612081\n",
      "Iteration: 988 lambda_k: 1 Loss: 0.13073800621754306\n",
      "Iteration: 989 lambda_k: 1 Loss: 0.13050086897961274\n",
      "Iteration: 990 lambda_k: 1 Loss: 0.13026397809773038\n",
      "Iteration: 991 lambda_k: 1 Loss: 0.13002733352469423\n",
      "Iteration: 992 lambda_k: 1 Loss: 0.12979093521579335\n",
      "Iteration: 993 lambda_k: 1 Loss: 0.12955478312881352\n",
      "Iteration: 994 lambda_k: 1 Loss: 0.12931887722404328\n",
      "Iteration: 995 lambda_k: 1 Loss: 0.12908321746427953\n",
      "Iteration: 996 lambda_k: 1 Loss: 0.1288478038148339\n",
      "Iteration: 997 lambda_k: 1 Loss: 0.12861263624355612\n",
      "Iteration: 998 lambda_k: 1 Loss: 0.12837771472078402\n",
      "Iteration: 999 lambda_k: 1 Loss: 0.12814303921940767\n",
      "Iteration: 1000 lambda_k: 1 Loss: 0.12790860971486026\n",
      "Iteration: 1001 lambda_k: 1 Loss: 0.12767442618512087\n",
      "Iteration: 1002 lambda_k: 1 Loss: 0.12744048861072038\n",
      "Iteration: 1003 lambda_k: 1 Loss: 0.12720679697474838\n",
      "Iteration: 1004 lambda_k: 1 Loss: 0.12697335126285997\n",
      "Iteration: 1005 lambda_k: 1 Loss: 0.12674015146328266\n",
      "Iteration: 1006 lambda_k: 1 Loss: 0.12650719756682274\n",
      "Iteration: 1007 lambda_k: 1 Loss: 0.12627448956687257\n",
      "Iteration: 1008 lambda_k: 1 Loss: 0.12604202745941698\n",
      "Iteration: 1009 lambda_k: 1 Loss: 0.1258098112430407\n",
      "Iteration: 1010 lambda_k: 1 Loss: 0.12557784091893534\n",
      "Iteration: 1011 lambda_k: 1 Loss: 0.12534611649090632\n",
      "Iteration: 1012 lambda_k: 1 Loss: 0.12511463796538042\n",
      "Iteration: 1013 lambda_k: 1 Loss: 0.12488340535141287\n",
      "Iteration: 1014 lambda_k: 1 Loss: 0.1246524186606948\n",
      "Iteration: 1015 lambda_k: 1 Loss: 0.12442167790756074\n",
      "Iteration: 1016 lambda_k: 1 Loss: 0.12419118310899596\n",
      "Iteration: 1017 lambda_k: 1 Loss: 0.12396093428464439\n",
      "Iteration: 1018 lambda_k: 1 Loss: 0.12373093145681599\n",
      "Iteration: 1019 lambda_k: 1 Loss: 0.12350117465049476\n",
      "Iteration: 1020 lambda_k: 1 Loss: 0.12327166389334647\n",
      "Iteration: 1021 lambda_k: 1 Loss: 0.12304239921572621\n",
      "Iteration: 1022 lambda_k: 1 Loss: 0.1228133806506861\n",
      "Iteration: 1023 lambda_k: 1 Loss: 0.122584608233987\n",
      "Iteration: 1024 lambda_k: 1 Loss: 0.122356082004102\n",
      "Iteration: 1025 lambda_k: 1 Loss: 0.1221278020022263\n",
      "Iteration: 1026 lambda_k: 1 Loss: 0.12189976827228562\n",
      "Iteration: 1027 lambda_k: 1 Loss: 0.12167198086094465\n",
      "Iteration: 1028 lambda_k: 1 Loss: 0.12144443981761556\n",
      "Iteration: 1029 lambda_k: 1 Loss: 0.12121714519446655\n",
      "Iteration: 1030 lambda_k: 1 Loss: 0.12099009704643057\n",
      "Iteration: 1031 lambda_k: 1 Loss: 0.12076329543121388\n",
      "Iteration: 1032 lambda_k: 1 Loss: 0.12053674040930505\n",
      "Iteration: 1033 lambda_k: 1 Loss: 0.12031043204398378\n",
      "Iteration: 1034 lambda_k: 1 Loss: 0.12008437040133\n",
      "Iteration: 1035 lambda_k: 1 Loss: 0.11985855555023281\n",
      "Iteration: 1036 lambda_k: 1 Loss: 0.11963298756239972\n",
      "Iteration: 1037 lambda_k: 1 Loss: 0.11940766651236588\n",
      "Iteration: 1038 lambda_k: 1 Loss: 0.11918259247750342\n",
      "Iteration: 1039 lambda_k: 1 Loss: 0.1189577655380308\n",
      "Iteration: 1040 lambda_k: 1 Loss: 0.11873318577702241\n",
      "Iteration: 1041 lambda_k: 1 Loss: 0.11850885328041824\n",
      "Iteration: 1042 lambda_k: 1 Loss: 0.11828476813703336\n",
      "Iteration: 1043 lambda_k: 1 Loss: 0.11806093043856782\n",
      "Iteration: 1044 lambda_k: 1 Loss: 0.11783734027961643\n",
      "Iteration: 1045 lambda_k: 1 Loss: 0.11761399775768014\n",
      "Iteration: 1046 lambda_k: 1 Loss: 0.11739090297319245\n",
      "Iteration: 1047 lambda_k: 1 Loss: 0.11716805602947022\n",
      "Iteration: 1048 lambda_k: 1 Loss: 0.11694545703278537\n",
      "Iteration: 1049 lambda_k: 1 Loss: 0.11672310609235818\n",
      "Iteration: 1050 lambda_k: 1 Loss: 0.11650100332036528\n",
      "Iteration: 1051 lambda_k: 1 Loss: 0.11627914883194881\n",
      "Iteration: 1052 lambda_k: 1 Loss: 0.11605754274522785\n",
      "Iteration: 1053 lambda_k: 1 Loss: 0.11583618518130895\n",
      "Iteration: 1054 lambda_k: 1 Loss: 0.11561507626429704\n",
      "Iteration: 1055 lambda_k: 1 Loss: 0.11539421612130622\n",
      "Iteration: 1056 lambda_k: 1 Loss: 0.1151736048824707\n",
      "Iteration: 1057 lambda_k: 1 Loss: 0.11495324268095584\n",
      "Iteration: 1058 lambda_k: 1 Loss: 0.1147331296529693\n",
      "Iteration: 1059 lambda_k: 1 Loss: 0.1145132659377722\n",
      "Iteration: 1060 lambda_k: 1 Loss: 0.11429365167769054\n",
      "Iteration: 1061 lambda_k: 1 Loss: 0.11407428701812654\n",
      "Iteration: 1062 lambda_k: 1 Loss: 0.1138551721075702\n",
      "Iteration: 1063 lambda_k: 1 Loss: 0.11363630709761084\n",
      "Iteration: 1064 lambda_k: 1 Loss: 0.11341769214295337\n",
      "Iteration: 1065 lambda_k: 1 Loss: 0.11319932740170364\n",
      "Iteration: 1066 lambda_k: 1 Loss: 0.11298121303446661\n",
      "Iteration: 1067 lambda_k: 1 Loss: 0.11276334920531203\n",
      "Iteration: 1068 lambda_k: 1 Loss: 0.11254573608152292\n",
      "Iteration: 1069 lambda_k: 1 Loss: 0.11232837383356095\n",
      "Iteration: 1070 lambda_k: 1 Loss: 0.11211126263506918\n",
      "Iteration: 1071 lambda_k: 1 Loss: 0.111894402662847\n",
      "Iteration: 1072 lambda_k: 1 Loss: 0.11167779409692093\n",
      "Iteration: 1073 lambda_k: 1 Loss: 0.11146143712054109\n",
      "Iteration: 1074 lambda_k: 1 Loss: 0.11124533192018994\n",
      "Iteration: 1075 lambda_k: 1 Loss: 0.11102947868559412\n",
      "Iteration: 1076 lambda_k: 1 Loss: 0.11081387760973772\n",
      "Iteration: 1077 lambda_k: 1 Loss: 0.11059852888887602\n",
      "Iteration: 1078 lambda_k: 1 Loss: 0.11038343272254854\n",
      "Iteration: 1079 lambda_k: 1 Loss: 0.11016858931359239\n",
      "Iteration: 1080 lambda_k: 1 Loss: 0.10995399886815561\n",
      "Iteration: 1081 lambda_k: 1 Loss: 0.10973966159571023\n",
      "Iteration: 1082 lambda_k: 1 Loss: 0.10952557770906562\n",
      "Iteration: 1083 lambda_k: 1 Loss: 0.10931174742438274\n",
      "Iteration: 1084 lambda_k: 1 Loss: 0.10909817096118749\n",
      "Iteration: 1085 lambda_k: 1 Loss: 0.10888484854238445\n",
      "Iteration: 1086 lambda_k: 1 Loss: 0.10867178039427078\n",
      "Iteration: 1087 lambda_k: 1 Loss: 0.10845896674654998\n",
      "Iteration: 1088 lambda_k: 1 Loss: 0.10824640783234617\n",
      "Iteration: 1089 lambda_k: 1 Loss: 0.10803410388821798\n",
      "Iteration: 1090 lambda_k: 1 Loss: 0.1078220551541729\n",
      "Iteration: 1091 lambda_k: 1 Loss: 0.10761026187368157\n",
      "Iteration: 1092 lambda_k: 1 Loss: 0.10739872429369211\n",
      "Iteration: 1093 lambda_k: 1 Loss: 0.10718744266464483\n",
      "Iteration: 1094 lambda_k: 1 Loss: 0.10697641724048655\n",
      "Iteration: 1095 lambda_k: 1 Loss: 0.1067656482786855\n",
      "Iteration: 1096 lambda_k: 1 Loss: 0.10655513604024598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1097 lambda_k: 1 Loss: 0.10634488078972329\n",
      "Iteration: 1098 lambda_k: 1 Loss: 0.10613488279523872\n",
      "Iteration: 1099 lambda_k: 1 Loss: 0.10592514232849466\n",
      "Iteration: 1100 lambda_k: 1 Loss: 0.10571565966478949\n",
      "Iteration: 1101 lambda_k: 1 Loss: 0.10550643508303315\n",
      "Iteration: 1102 lambda_k: 1 Loss: 0.10529746886576238\n",
      "Iteration: 1103 lambda_k: 1 Loss: 0.10508876129915629\n",
      "Iteration: 1104 lambda_k: 1 Loss: 0.10488031267305138\n",
      "Iteration: 1105 lambda_k: 1 Loss: 0.1046721232809578\n",
      "Iteration: 1106 lambda_k: 1 Loss: 0.10446419342007478\n",
      "Iteration: 1107 lambda_k: 1 Loss: 0.10425652339130626\n",
      "Iteration: 1108 lambda_k: 1 Loss: 0.1040491134992771\n",
      "Iteration: 1109 lambda_k: 1 Loss: 0.10384196405235964\n",
      "Iteration: 1110 lambda_k: 1 Loss: 0.10363507536275361\n",
      "Iteration: 1111 lambda_k: 1 Loss: 0.10342844774624127\n",
      "Iteration: 1112 lambda_k: 1 Loss: 0.10322208152247128\n",
      "Iteration: 1113 lambda_k: 1 Loss: 0.10301597701491531\n",
      "Iteration: 1114 lambda_k: 1 Loss: 0.10281013455086681\n",
      "Iteration: 1115 lambda_k: 1 Loss: 0.10260455446145118\n",
      "Iteration: 1116 lambda_k: 1 Loss: 0.10239923708164296\n",
      "Iteration: 1117 lambda_k: 1 Loss: 0.10219418275028436\n",
      "Iteration: 1118 lambda_k: 1 Loss: 0.10198939181010215\n",
      "Iteration: 1119 lambda_k: 1 Loss: 0.1017848646077246\n",
      "Iteration: 1120 lambda_k: 1 Loss: 0.10158060149413396\n",
      "Iteration: 1121 lambda_k: 1 Loss: 0.1013766028236355\n",
      "Iteration: 1122 lambda_k: 1 Loss: 0.10117286895409813\n",
      "Iteration: 1123 lambda_k: 1 Loss: 0.10096940024788813\n",
      "Iteration: 1124 lambda_k: 1 Loss: 0.1007661970714865\n",
      "Iteration: 1125 lambda_k: 1 Loss: 0.10056325979537493\n",
      "Iteration: 1126 lambda_k: 1 Loss: 0.10036058879402475\n",
      "Iteration: 1127 lambda_k: 1 Loss: 0.10015818444596915\n",
      "Iteration: 1128 lambda_k: 1 Loss: 0.09995604713394185\n",
      "Iteration: 1129 lambda_k: 1 Loss: 0.09975417724426051\n",
      "Iteration: 1130 lambda_k: 1 Loss: 0.09955257516758137\n",
      "Iteration: 1131 lambda_k: 1 Loss: 0.09935124129873232\n",
      "Iteration: 1132 lambda_k: 1 Loss: 0.09915017603667187\n",
      "Iteration: 1133 lambda_k: 1 Loss: 0.0989493797844872\n",
      "Iteration: 1134 lambda_k: 1 Loss: 0.09874885294962613\n",
      "Iteration: 1135 lambda_k: 1 Loss: 0.09854859594360557\n",
      "Iteration: 1136 lambda_k: 1 Loss: 0.09834860918168704\n",
      "Iteration: 1137 lambda_k: 1 Loss: 0.09814889308358933\n",
      "Iteration: 1138 lambda_k: 1 Loss: 0.09794944807336399\n",
      "Iteration: 1139 lambda_k: 1 Loss: 0.09775027457933258\n",
      "Iteration: 1140 lambda_k: 1 Loss: 0.09755137303407728\n",
      "Iteration: 1141 lambda_k: 1 Loss: 0.09735274387446347\n",
      "Iteration: 1142 lambda_k: 1 Loss: 0.09715438754166439\n",
      "Iteration: 1143 lambda_k: 1 Loss: 0.09695630448118095\n",
      "Iteration: 1144 lambda_k: 1 Loss: 0.0967584951428596\n",
      "Iteration: 1145 lambda_k: 1 Loss: 0.09656095998096947\n",
      "Iteration: 1146 lambda_k: 1 Loss: 0.0963636994540935\n",
      "Iteration: 1147 lambda_k: 1 Loss: 0.0961667140251726\n",
      "Iteration: 1148 lambda_k: 1 Loss: 0.0959700041616116\n",
      "Iteration: 1149 lambda_k: 1 Loss: 0.09577357032169379\n",
      "Iteration: 1150 lambda_k: 1 Loss: 0.09557741301666305\n",
      "Iteration: 1151 lambda_k: 1 Loss: 0.09538153269431253\n",
      "Iteration: 1152 lambda_k: 1 Loss: 0.09518592986394842\n",
      "Iteration: 1153 lambda_k: 1 Loss: 0.09499055269039652\n",
      "Iteration: 1154 lambda_k: 1 Loss: 0.09479548235909585\n",
      "Iteration: 1155 lambda_k: 1 Loss: 0.09460104124440671\n",
      "Iteration: 1156 lambda_k: 1 Loss: 0.09440698132700255\n",
      "Iteration: 1157 lambda_k: 1 Loss: 0.09421316953635929\n",
      "Iteration: 1158 lambda_k: 1 Loss: 0.0940195833087285\n",
      "Iteration: 1159 lambda_k: 1 Loss: 0.09382624692714987\n",
      "Iteration: 1160 lambda_k: 1 Loss: 0.09363318303192858\n",
      "Iteration: 1161 lambda_k: 1 Loss: 0.09344039753845146\n",
      "Iteration: 1162 lambda_k: 1 Loss: 0.09324788441463683\n",
      "Iteration: 1163 lambda_k: 1 Loss: 0.0930556342936674\n",
      "Iteration: 1164 lambda_k: 1 Loss: 0.09286363944212746\n",
      "Iteration: 1165 lambda_k: 1 Loss: 0.09267189466039205\n",
      "Iteration: 1166 lambda_k: 1 Loss: 0.09248039627995269\n",
      "Iteration: 1167 lambda_k: 1 Loss: 0.09228914107139777\n",
      "Iteration: 1168 lambda_k: 1 Loss: 0.0920981257295447\n",
      "Iteration: 1169 lambda_k: 1 Loss: 0.09190734683565782\n",
      "Iteration: 1170 lambda_k: 1 Loss: 0.0917168009945277\n",
      "Iteration: 1171 lambda_k: 1 Loss: 0.0915264849469731\n",
      "Iteration: 1172 lambda_k: 1 Loss: 0.09133639560548566\n",
      "Iteration: 1173 lambda_k: 1 Loss: 0.0911465300395996\n",
      "Iteration: 1174 lambda_k: 1 Loss: 0.09095688544891373\n",
      "Iteration: 1175 lambda_k: 1 Loss: 0.09076745914392971\n",
      "Iteration: 1176 lambda_k: 1 Loss: 0.09057824853715267\n",
      "Iteration: 1177 lambda_k: 1 Loss: 0.0903892511412945\n",
      "Iteration: 1178 lambda_k: 1 Loss: 0.09020046456555149\n",
      "Iteration: 1179 lambda_k: 1 Loss: 0.09001188651369366\n",
      "Iteration: 1180 lambda_k: 1 Loss: 0.08982351477490426\n",
      "Iteration: 1181 lambda_k: 1 Loss: 0.08963534724179253\n",
      "Iteration: 1182 lambda_k: 1 Loss: 0.08944738187471762\n",
      "Iteration: 1183 lambda_k: 1 Loss: 0.08925961671990687\n",
      "Iteration: 1184 lambda_k: 1 Loss: 0.0890720498839186\n",
      "Iteration: 1185 lambda_k: 1 Loss: 0.08888467956219431\n",
      "Iteration: 1186 lambda_k: 1 Loss: 0.0886975040170738\n",
      "Iteration: 1187 lambda_k: 1 Loss: 0.08851052157667871\n",
      "Iteration: 1188 lambda_k: 1 Loss: 0.08832373062874702\n",
      "Iteration: 1189 lambda_k: 1 Loss: 0.08813712964898363\n",
      "Iteration: 1190 lambda_k: 1 Loss: 0.08795071712659563\n",
      "Iteration: 1191 lambda_k: 1 Loss: 0.08776449163440189\n",
      "Iteration: 1192 lambda_k: 1 Loss: 0.08757845179956306\n",
      "Iteration: 1193 lambda_k: 1 Loss: 0.08739259627712136\n",
      "Iteration: 1194 lambda_k: 1 Loss: 0.08720692382032348\n",
      "Iteration: 1195 lambda_k: 1 Loss: 0.08702143154899178\n",
      "Iteration: 1196 lambda_k: 1 Loss: 0.08683500775441078\n",
      "Iteration: 1197 lambda_k: 1 Loss: 0.08664899941122903\n",
      "Iteration: 1198 lambda_k: 1 Loss: 0.08646334953121886\n",
      "Iteration: 1199 lambda_k: 1 Loss: 0.08627796925670637\n",
      "Iteration: 1200 lambda_k: 1 Loss: 0.08609284081740913\n",
      "Iteration: 1201 lambda_k: 1 Loss: 0.08590796910021517\n",
      "Iteration: 1202 lambda_k: 1 Loss: 0.08572335805811392\n",
      "Iteration: 1203 lambda_k: 1 Loss: 0.08553900542348343\n",
      "Iteration: 1204 lambda_k: 1 Loss: 0.08535490439216271\n",
      "Iteration: 1205 lambda_k: 1 Loss: 0.08517104626128866\n",
      "Iteration: 1206 lambda_k: 1 Loss: 0.08498742239093501\n",
      "Iteration: 1207 lambda_k: 1 Loss: 0.08480402528786139\n",
      "Iteration: 1208 lambda_k: 1 Loss: 0.08462084881094141\n",
      "Iteration: 1209 lambda_k: 1 Loss: 0.08443788778355336\n",
      "Iteration: 1210 lambda_k: 1 Loss: 0.08425513748631266\n",
      "Iteration: 1211 lambda_k: 1 Loss: 0.08407259335506936\n",
      "Iteration: 1212 lambda_k: 1 Loss: 0.08389025093191738\n",
      "Iteration: 1213 lambda_k: 1 Loss: 0.08370810595062769\n",
      "Iteration: 1214 lambda_k: 1 Loss: 0.08352615442795945\n",
      "Iteration: 1215 lambda_k: 1 Loss: 0.08334439271165968\n",
      "Iteration: 1216 lambda_k: 1 Loss: 0.08316281744004649\n",
      "Iteration: 1217 lambda_k: 1 Loss: 0.082981425508147\n",
      "Iteration: 1218 lambda_k: 1 Loss: 0.0828002140292898\n",
      "Iteration: 1219 lambda_k: 1 Loss: 0.08261918031052558\n",
      "Iteration: 1220 lambda_k: 1 Loss: 0.0824383218409737\n",
      "Iteration: 1221 lambda_k: 1 Loss: 0.08225763628548842\n",
      "Iteration: 1222 lambda_k: 1 Loss: 0.08207712147751048\n",
      "Iteration: 1223 lambda_k: 1 Loss: 0.08189677540912987\n",
      "Iteration: 1224 lambda_k: 1 Loss: 0.08171659621937256\n",
      "Iteration: 1225 lambda_k: 1 Loss: 0.08153658218251927\n",
      "Iteration: 1226 lambda_k: 1 Loss: 0.08135673169761828\n",
      "Iteration: 1227 lambda_k: 1 Loss: 0.0811770432829475\n",
      "Iteration: 1228 lambda_k: 1 Loss: 0.08099751556012635\n",
      "Iteration: 1229 lambda_k: 1 Loss: 0.08081814724409872\n",
      "Iteration: 1230 lambda_k: 1 Loss: 0.08063893715273991\n",
      "Iteration: 1231 lambda_k: 1 Loss: 0.08045988419050536\n",
      "Iteration: 1232 lambda_k: 1 Loss: 0.08028098734025788\n",
      "Iteration: 1233 lambda_k: 1 Loss: 0.08010224565790798\n",
      "Iteration: 1234 lambda_k: 1 Loss: 0.07992365826748785\n",
      "Iteration: 1235 lambda_k: 1 Loss: 0.0797452243562896\n",
      "Iteration: 1236 lambda_k: 1 Loss: 0.07956694317016981\n",
      "Iteration: 1237 lambda_k: 1 Loss: 0.07938881400909524\n",
      "Iteration: 1238 lambda_k: 1 Loss: 0.0792108362229559\n",
      "Iteration: 1239 lambda_k: 1 Loss: 0.07903300920766336\n",
      "Iteration: 1240 lambda_k: 1 Loss: 0.07885533240154347\n",
      "Iteration: 1241 lambda_k: 1 Loss: 0.07867780528201034\n",
      "Iteration: 1242 lambda_k: 1 Loss: 0.07850042736249371\n",
      "Iteration: 1243 lambda_k: 1 Loss: 0.07832319818958972\n",
      "Iteration: 1244 lambda_k: 1 Loss: 0.07814611734041423\n",
      "Iteration: 1245 lambda_k: 1 Loss: 0.07796918442014525\n",
      "Iteration: 1246 lambda_k: 1 Loss: 0.07779239905974415\n",
      "Iteration: 1247 lambda_k: 1 Loss: 0.07761576091384649\n",
      "Iteration: 1248 lambda_k: 1 Loss: 0.07743926965881152\n",
      "Iteration: 1249 lambda_k: 1 Loss: 0.07726292499091988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1250 lambda_k: 1 Loss: 0.07708672662470739\n",
      "Iteration: 1251 lambda_k: 1 Loss: 0.07691067429142512\n",
      "Iteration: 1252 lambda_k: 1 Loss: 0.07673476773761777\n",
      "Iteration: 1253 lambda_k: 1 Loss: 0.0765590067238105\n",
      "Iteration: 1254 lambda_k: 1 Loss: 0.07638339102329766\n",
      "Iteration: 1255 lambda_k: 1 Loss: 0.0762079204210256\n",
      "Iteration: 1256 lambda_k: 1 Loss: 0.076032594712563\n",
      "Iteration: 1257 lambda_k: 1 Loss: 0.07585741370315284\n",
      "Iteration: 1258 lambda_k: 1 Loss: 0.0756823772068386\n",
      "Iteration: 1259 lambda_k: 1 Loss: 0.07550748504566093\n",
      "Iteration: 1260 lambda_k: 1 Loss: 0.0753327370489184\n",
      "Iteration: 1261 lambda_k: 1 Loss: 0.07515813305248786\n",
      "Iteration: 1262 lambda_k: 1 Loss: 0.07498367289820025\n",
      "Iteration: 1263 lambda_k: 1 Loss: 0.07480935643326736\n",
      "Iteration: 1264 lambda_k: 1 Loss: 0.07463518350975624\n",
      "Iteration: 1265 lambda_k: 1 Loss: 0.07446115398410665\n",
      "Iteration: 1266 lambda_k: 1 Loss: 0.07428726771668913\n",
      "Iteration: 1267 lambda_k: 1 Loss: 0.0741135245714007\n",
      "Iteration: 1268 lambda_k: 1 Loss: 0.07393992441529461\n",
      "Iteration: 1269 lambda_k: 1 Loss: 0.07376646711824164\n",
      "Iteration: 1270 lambda_k: 1 Loss: 0.07359315255262115\n",
      "Iteration: 1271 lambda_k: 1 Loss: 0.07341998059303892\n",
      "Iteration: 1272 lambda_k: 1 Loss: 0.07324695111606987\n",
      "Iteration: 1273 lambda_k: 1 Loss: 0.07307406400002389\n",
      "Iteration: 1274 lambda_k: 1 Loss: 0.07290131912473259\n",
      "Iteration: 1275 lambda_k: 1 Loss: 0.07272871637135542\n",
      "Iteration: 1276 lambda_k: 1 Loss: 0.072556255622204\n",
      "Iteration: 1277 lambda_k: 1 Loss: 0.07238393676058254\n",
      "Iteration: 1278 lambda_k: 1 Loss: 0.0722117596706439\n",
      "Iteration: 1279 lambda_k: 1 Loss: 0.07203972423725882\n",
      "Iteration: 1280 lambda_k: 1 Loss: 0.07186783034589865\n",
      "Iteration: 1281 lambda_k: 1 Loss: 0.07169607788252917\n",
      "Iteration: 1282 lambda_k: 1 Loss: 0.07152446673351538\n",
      "Iteration: 1283 lambda_k: 1 Loss: 0.07135299678553654\n",
      "Iteration: 1284 lambda_k: 1 Loss: 0.07118166792550952\n",
      "Iteration: 1285 lambda_k: 1 Loss: 0.07101048004052063\n",
      "Iteration: 1286 lambda_k: 1 Loss: 0.07083943301776557\n",
      "Iteration: 1287 lambda_k: 1 Loss: 0.07066852674449521\n",
      "Iteration: 1288 lambda_k: 1 Loss: 0.0704977611079688\n",
      "Iteration: 1289 lambda_k: 1 Loss: 0.07032713599541197\n",
      "Iteration: 1290 lambda_k: 1 Loss: 0.07015665129398084\n",
      "Iteration: 1291 lambda_k: 1 Loss: 0.0699863068907299\n",
      "Iteration: 1292 lambda_k: 1 Loss: 0.06981610267258534\n",
      "Iteration: 1293 lambda_k: 1 Loss: 0.06964603852632154\n",
      "Iteration: 1294 lambda_k: 1 Loss: 0.06947611433854135\n",
      "Iteration: 1295 lambda_k: 1 Loss: 0.06930632999565975\n",
      "Iteration: 1296 lambda_k: 1 Loss: 0.06913668538389038\n",
      "Iteration: 1297 lambda_k: 1 Loss: 0.06896718038923466\n",
      "Iteration: 1298 lambda_k: 1 Loss: 0.06879781489747337\n",
      "Iteration: 1299 lambda_k: 1 Loss: 0.06862858879416035\n",
      "Iteration: 1300 lambda_k: 1 Loss: 0.06845950196461832\n",
      "Iteration: 1301 lambda_k: 1 Loss: 0.06829055429393598\n",
      "Iteration: 1302 lambda_k: 1 Loss: 0.0681217456669674\n",
      "Iteration: 1303 lambda_k: 1 Loss: 0.06795307596833221\n",
      "Iteration: 1304 lambda_k: 1 Loss: 0.06778454508241712\n",
      "Iteration: 1305 lambda_k: 1 Loss: 0.06761615289337881\n",
      "Iteration: 1306 lambda_k: 1 Loss: 0.06744789928514743\n",
      "Iteration: 1307 lambda_k: 1 Loss: 0.06727978414143135\n",
      "Iteration: 1308 lambda_k: 1 Loss: 0.06711180734572215\n",
      "Iteration: 1309 lambda_k: 1 Loss: 0.06694396878130097\n",
      "Iteration: 1310 lambda_k: 1 Loss: 0.06677626833124467\n",
      "Iteration: 1311 lambda_k: 1 Loss: 0.0666087058784329\n",
      "Iteration: 1312 lambda_k: 1 Loss: 0.06644128130555554\n",
      "Iteration: 1313 lambda_k: 1 Loss: 0.06627399449512048\n",
      "Iteration: 1314 lambda_k: 1 Loss: 0.06610684532946137\n",
      "Iteration: 1315 lambda_k: 1 Loss: 0.06593983369074624\n",
      "Iteration: 1316 lambda_k: 1 Loss: 0.06577295946098564\n",
      "Iteration: 1317 lambda_k: 1 Loss: 0.0656062225220413\n",
      "Iteration: 1318 lambda_k: 1 Loss: 0.06543962275563489\n",
      "Iteration: 1319 lambda_k: 1 Loss: 0.06527316004335644\n",
      "Iteration: 1320 lambda_k: 1 Loss: 0.06510683426667344\n",
      "Iteration: 1321 lambda_k: 1 Loss: 0.06494064530693926\n",
      "Iteration: 1322 lambda_k: 1 Loss: 0.06477459304540206\n",
      "Iteration: 1323 lambda_k: 1 Loss: 0.06460867736321341\n",
      "Iteration: 1324 lambda_k: 1 Loss: 0.06444289814143692\n",
      "Iteration: 1325 lambda_k: 1 Loss: 0.0642772552610565\n",
      "Iteration: 1326 lambda_k: 1 Loss: 0.06411174860298532\n",
      "Iteration: 1327 lambda_k: 1 Loss: 0.06394637804807347\n",
      "Iteration: 1328 lambda_k: 1 Loss: 0.06378114347711664\n",
      "Iteration: 1329 lambda_k: 1 Loss: 0.06361604477086372\n",
      "Iteration: 1330 lambda_k: 1 Loss: 0.06345108181002494\n",
      "Iteration: 1331 lambda_k: 1 Loss: 0.06328625447527957\n",
      "Iteration: 1332 lambda_k: 1 Loss: 0.0631215626472834\n",
      "Iteration: 1333 lambda_k: 1 Loss: 0.06295700620667634\n",
      "Iteration: 1334 lambda_k: 1 Loss: 0.06279258503408942\n",
      "Iteration: 1335 lambda_k: 1 Loss: 0.06262829901015189\n",
      "Iteration: 1336 lambda_k: 1 Loss: 0.06246414801549816\n",
      "Iteration: 1337 lambda_k: 1 Loss: 0.0623001319307747\n",
      "Iteration: 1338 lambda_k: 1 Loss: 0.06213625063664638\n",
      "Iteration: 1339 lambda_k: 1 Loss: 0.061972504013802665\n",
      "Iteration: 1340 lambda_k: 1 Loss: 0.06180889194296425\n",
      "Iteration: 1341 lambda_k: 1 Loss: 0.0616454143048886\n",
      "Iteration: 1342 lambda_k: 1 Loss: 0.061482070980376204\n",
      "Iteration: 1343 lambda_k: 1 Loss: 0.061318861850275956\n",
      "Iteration: 1344 lambda_k: 1 Loss: 0.06115578679549086\n",
      "Iteration: 1345 lambda_k: 1 Loss: 0.06099284569698328\n",
      "Iteration: 1346 lambda_k: 1 Loss: 0.06083003843577985\n",
      "Iteration: 1347 lambda_k: 1 Loss: 0.0606673648929769\n",
      "Iteration: 1348 lambda_k: 1 Loss: 0.060504824949745115\n",
      "Iteration: 1349 lambda_k: 1 Loss: 0.06034241848733411\n",
      "Iteration: 1350 lambda_k: 1 Loss: 0.060180145387077125\n",
      "Iteration: 1351 lambda_k: 1 Loss: 0.06001800553039525\n",
      "Iteration: 1352 lambda_k: 1 Loss: 0.059855998798801935\n",
      "Iteration: 1353 lambda_k: 1 Loss: 0.05969412507390655\n",
      "Iteration: 1354 lambda_k: 1 Loss: 0.059532384237419515\n",
      "Iteration: 1355 lambda_k: 1 Loss: 0.059370776171154126\n",
      "Iteration: 1356 lambda_k: 1 Loss: 0.059209300757031874\n",
      "Iteration: 1357 lambda_k: 1 Loss: 0.05904795787708548\n",
      "Iteration: 1358 lambda_k: 1 Loss: 0.05888674741346233\n",
      "Iteration: 1359 lambda_k: 1 Loss: 0.05872566924842772\n",
      "Iteration: 1360 lambda_k: 1 Loss: 0.05856472326436807\n",
      "Iteration: 1361 lambda_k: 1 Loss: 0.05840390934379391\n",
      "Iteration: 1362 lambda_k: 1 Loss: 0.05824322736934295\n",
      "Iteration: 1363 lambda_k: 1 Loss: 0.05808267722378286\n",
      "Iteration: 1364 lambda_k: 1 Loss: 0.05792225879001384\n",
      "Iteration: 1365 lambda_k: 1 Loss: 0.057761971951071654\n",
      "Iteration: 1366 lambda_k: 1 Loss: 0.05760181659012957\n",
      "Iteration: 1367 lambda_k: 1 Loss: 0.05744179259050135\n",
      "Iteration: 1368 lambda_k: 1 Loss: 0.05728189983564321\n",
      "Iteration: 1369 lambda_k: 1 Loss: 0.057122138209156195\n",
      "Iteration: 1370 lambda_k: 1 Loss: 0.05696250759478844\n",
      "Iteration: 1371 lambda_k: 1 Loss: 0.056803007876437044\n",
      "Iteration: 1372 lambda_k: 1 Loss: 0.05664363893815005\n",
      "Iteration: 1373 lambda_k: 1 Loss: 0.05648440066412858\n",
      "Iteration: 1374 lambda_k: 1 Loss: 0.05632529293872858\n",
      "Iteration: 1375 lambda_k: 1 Loss: 0.05616631564646252\n",
      "Iteration: 1376 lambda_k: 1 Loss: 0.05600746867200101\n",
      "Iteration: 1377 lambda_k: 1 Loss: 0.055848751900174666\n",
      "Iteration: 1378 lambda_k: 1 Loss: 0.05569016521597533\n",
      "Iteration: 1379 lambda_k: 1 Loss: 0.055531708504558006\n",
      "Iteration: 1380 lambda_k: 1 Loss: 0.0553733816512419\n",
      "Iteration: 1381 lambda_k: 1 Loss: 0.05521518454151197\n",
      "Iteration: 1382 lambda_k: 1 Loss: 0.055057117061020164\n",
      "Iteration: 1383 lambda_k: 1 Loss: 0.05489917909558686\n",
      "Iteration: 1384 lambda_k: 1 Loss: 0.05474137053120199\n",
      "Iteration: 1385 lambda_k: 1 Loss: 0.05458369125402599\n",
      "Iteration: 1386 lambda_k: 1 Loss: 0.0544261411503913\n",
      "Iteration: 1387 lambda_k: 1 Loss: 0.05426872010680315\n",
      "Iteration: 1388 lambda_k: 1 Loss: 0.054111428009940725\n",
      "Iteration: 1389 lambda_k: 1 Loss: 0.053954264746658064\n",
      "Iteration: 1390 lambda_k: 1 Loss: 0.053797230203985066\n",
      "Iteration: 1391 lambda_k: 1 Loss: 0.05364032426912834\n",
      "Iteration: 1392 lambda_k: 1 Loss: 0.05348354682947227\n",
      "Iteration: 1393 lambda_k: 1 Loss: 0.0533268977725795\n",
      "Iteration: 1394 lambda_k: 1 Loss: 0.053170376986192126\n",
      "Iteration: 1395 lambda_k: 1 Loss: 0.05301398435823216\n",
      "Iteration: 1396 lambda_k: 1 Loss: 0.052857719776802536\n",
      "Iteration: 1397 lambda_k: 1 Loss: 0.05270158313018746\n",
      "Iteration: 1398 lambda_k: 1 Loss: 0.05254557430685355\n",
      "Iteration: 1399 lambda_k: 1 Loss: 0.052389693195450136\n",
      "Iteration: 1400 lambda_k: 1 Loss: 0.05223393968481002\n",
      "Iteration: 1401 lambda_k: 1 Loss: 0.05207831366395018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1402 lambda_k: 1 Loss: 0.051922815022072244\n",
      "Iteration: 1403 lambda_k: 1 Loss: 0.0517674436485631\n",
      "Iteration: 1404 lambda_k: 1 Loss: 0.05161219943299544\n",
      "Iteration: 1405 lambda_k: 1 Loss: 0.051457082265128275\n",
      "Iteration: 1406 lambda_k: 1 Loss: 0.05130209203490742\n",
      "Iteration: 1407 lambda_k: 1 Loss: 0.05114722863246619\n",
      "Iteration: 1408 lambda_k: 1 Loss: 0.05099249194812568\n",
      "Iteration: 1409 lambda_k: 1 Loss: 0.05083788187239523\n",
      "Iteration: 1410 lambda_k: 1 Loss: 0.05068339829597302\n",
      "Iteration: 1411 lambda_k: 1 Loss: 0.050529041109746424\n",
      "Iteration: 1412 lambda_k: 1 Loss: 0.050374810204792425\n",
      "Iteration: 1413 lambda_k: 1 Loss: 0.050220705472378085\n",
      "Iteration: 1414 lambda_k: 1 Loss: 0.05006672680396109\n",
      "Iteration: 1415 lambda_k: 1 Loss: 0.04991287409118984\n",
      "Iteration: 1416 lambda_k: 1 Loss: 0.04975914722590416\n",
      "Iteration: 1417 lambda_k: 1 Loss: 0.0496055461001356\n",
      "Iteration: 1418 lambda_k: 1 Loss: 0.04945207060610766\n",
      "Iteration: 1419 lambda_k: 1 Loss: 0.04929872063623635\n",
      "Iteration: 1420 lambda_k: 1 Loss: 0.049145496083130624\n",
      "Iteration: 1421 lambda_k: 1 Loss: 0.04899239683959254\n",
      "Iteration: 1422 lambda_k: 1 Loss: 0.04883942279861781\n",
      "Iteration: 1423 lambda_k: 1 Loss: 0.04868657385339611\n",
      "Iteration: 1424 lambda_k: 1 Loss: 0.04853384989731133\n",
      "Iteration: 1425 lambda_k: 1 Loss: 0.048381250823942276\n",
      "Iteration: 1426 lambda_k: 1 Loss: 0.048228776527062714\n",
      "Iteration: 1427 lambda_k: 1 Loss: 0.048076426900641754\n",
      "Iteration: 1428 lambda_k: 1 Loss: 0.04792420183884457\n",
      "Iteration: 1429 lambda_k: 1 Loss: 0.04777210123603231\n",
      "Iteration: 1430 lambda_k: 1 Loss: 0.04762012498676278\n",
      "Iteration: 1431 lambda_k: 1 Loss: 0.04746827298579065\n",
      "Iteration: 1432 lambda_k: 1 Loss: 0.04731654512806804\n",
      "Iteration: 1433 lambda_k: 1 Loss: 0.047164941308744705\n",
      "Iteration: 1434 lambda_k: 1 Loss: 0.04701346142316854\n",
      "Iteration: 1435 lambda_k: 1 Loss: 0.046862105366885995\n",
      "Iteration: 1436 lambda_k: 1 Loss: 0.046710873035642364\n",
      "Iteration: 1437 lambda_k: 1 Loss: 0.04655976432538238\n",
      "Iteration: 1438 lambda_k: 1 Loss: 0.04640877913225039\n",
      "Iteration: 1439 lambda_k: 1 Loss: 0.04625791735259098\n",
      "Iteration: 1440 lambda_k: 1 Loss: 0.04610717888294928\n",
      "Iteration: 1441 lambda_k: 1 Loss: 0.04595656362007165\n",
      "Iteration: 1442 lambda_k: 1 Loss: 0.04580607146090583\n",
      "Iteration: 1443 lambda_k: 1 Loss: 0.04565570230260156\n",
      "Iteration: 1444 lambda_k: 1 Loss: 0.04550545604251096\n",
      "Iteration: 1445 lambda_k: 1 Loss: 0.04535533257818929\n",
      "Iteration: 1446 lambda_k: 1 Loss: 0.04520533180739509\n",
      "Iteration: 1447 lambda_k: 1 Loss: 0.04505545362809092\n",
      "Iteration: 1448 lambda_k: 1 Loss: 0.04490569793844373\n",
      "Iteration: 1449 lambda_k: 1 Loss: 0.04475606463682555\n",
      "Iteration: 1450 lambda_k: 1 Loss: 0.04460655362181409\n",
      "Iteration: 1451 lambda_k: 1 Loss: 0.04445716479219299\n",
      "Iteration: 1452 lambda_k: 1 Loss: 0.04430789804695258\n",
      "Iteration: 1453 lambda_k: 1 Loss: 0.04415875328529071\n",
      "Iteration: 1454 lambda_k: 1 Loss: 0.04400973040661297\n",
      "Iteration: 1455 lambda_k: 1 Loss: 0.0438608293105335\n",
      "Iteration: 1456 lambda_k: 1 Loss: 0.043712049896875646\n",
      "Iteration: 1457 lambda_k: 1 Loss: 0.04356339206567261\n",
      "Iteration: 1458 lambda_k: 1 Loss: 0.043414855717168084\n",
      "Iteration: 1459 lambda_k: 1 Loss: 0.043266440751817035\n",
      "Iteration: 1460 lambda_k: 1 Loss: 0.043118147070286356\n",
      "Iteration: 1461 lambda_k: 1 Loss: 0.042969974573455624\n",
      "Iteration: 1462 lambda_k: 1 Loss: 0.04282192316241775\n",
      "Iteration: 1463 lambda_k: 1 Loss: 0.042673992738479966\n",
      "Iteration: 1464 lambda_k: 1 Loss: 0.04252618320316442\n",
      "Iteration: 1465 lambda_k: 1 Loss: 0.04237849445820922\n",
      "Iteration: 1466 lambda_k: 1 Loss: 0.04223092640556897\n",
      "Iteration: 1467 lambda_k: 1 Loss: 0.04208347894741598\n",
      "Iteration: 1468 lambda_k: 1 Loss: 0.041936151986140924\n",
      "Iteration: 1469 lambda_k: 1 Loss: 0.04178894542435379\n",
      "Iteration: 1470 lambda_k: 1 Loss: 0.04164185916488481\n",
      "Iteration: 1471 lambda_k: 1 Loss: 0.04149489311078558\n",
      "Iteration: 1472 lambda_k: 1 Loss: 0.041348047165329874\n",
      "Iteration: 1473 lambda_k: 1 Loss: 0.041201321232014615\n",
      "Iteration: 1474 lambda_k: 1 Loss: 0.041054715214561055\n",
      "Iteration: 1475 lambda_k: 1 Loss: 0.04090822901691606\n",
      "Iteration: 1476 lambda_k: 1 Loss: 0.04076186254325257\n",
      "Iteration: 1477 lambda_k: 1 Loss: 0.04061561569797132\n",
      "Iteration: 1478 lambda_k: 1 Loss: 0.040469488385701864\n",
      "Iteration: 1479 lambda_k: 1 Loss: 0.040323480511303696\n",
      "Iteration: 1480 lambda_k: 1 Loss: 0.040177591979867276\n",
      "Iteration: 1481 lambda_k: 1 Loss: 0.040031822696715694\n",
      "Iteration: 1482 lambda_k: 1 Loss: 0.03988617256740567\n",
      "Iteration: 1483 lambda_k: 1 Loss: 0.03974064149772904\n",
      "Iteration: 1484 lambda_k: 1 Loss: 0.0395952293937139\n",
      "Iteration: 1485 lambda_k: 1 Loss: 0.03944993616162623\n",
      "Iteration: 1486 lambda_k: 1 Loss: 0.039304761707971196\n",
      "Iteration: 1487 lambda_k: 1 Loss: 0.03915970593949467\n",
      "Iteration: 1488 lambda_k: 1 Loss: 0.03901476876318458\n",
      "Iteration: 1489 lambda_k: 1 Loss: 0.038869950086272746\n",
      "Iteration: 1490 lambda_k: 1 Loss: 0.03872524981623622\n",
      "Iteration: 1491 lambda_k: 1 Loss: 0.03858066786079888\n",
      "Iteration: 1492 lambda_k: 1 Loss: 0.03843620412793349\n",
      "Iteration: 1493 lambda_k: 1 Loss: 0.03829185852586274\n",
      "Iteration: 1494 lambda_k: 1 Loss: 0.038147630963061584\n",
      "Iteration: 1495 lambda_k: 1 Loss: 0.03800352134825889\n",
      "Iteration: 1496 lambda_k: 1 Loss: 0.03785952959043906\n",
      "Iteration: 1497 lambda_k: 1 Loss: 0.03771565559884411\n",
      "Iteration: 1498 lambda_k: 1 Loss: 0.03757189928297559\n",
      "Iteration: 1499 lambda_k: 1 Loss: 0.037428260552596515\n",
      "Iteration: 1500 lambda_k: 1 Loss: 0.03728473931773353\n",
      "Iteration: 1501 lambda_k: 1 Loss: 0.03714133548867877\n",
      "Iteration: 1502 lambda_k: 1 Loss: 0.036998048975992334\n",
      "Iteration: 1503 lambda_k: 1 Loss: 0.036854879690504085\n",
      "Iteration: 1504 lambda_k: 1 Loss: 0.0367118275433161\n",
      "Iteration: 1505 lambda_k: 1 Loss: 0.03656889244580517\n",
      "Iteration: 1506 lambda_k: 1 Loss: 0.036426074309624724\n",
      "Iteration: 1507 lambda_k: 1 Loss: 0.03628337304670755\n",
      "Iteration: 1508 lambda_k: 1 Loss: 0.03614078856926835\n",
      "Iteration: 1509 lambda_k: 1 Loss: 0.03599832078980612\n",
      "Iteration: 1510 lambda_k: 1 Loss: 0.0358559696211067\n",
      "Iteration: 1511 lambda_k: 1 Loss: 0.03571373497624558\n",
      "Iteration: 1512 lambda_k: 1 Loss: 0.03557161676859067\n",
      "Iteration: 1513 lambda_k: 1 Loss: 0.035429614911805095\n",
      "Iteration: 1514 lambda_k: 1 Loss: 0.035287729319849935\n",
      "Iteration: 1515 lambda_k: 1 Loss: 0.03514595990698741\n",
      "Iteration: 1516 lambda_k: 1 Loss: 0.035004306587783696\n",
      "Iteration: 1517 lambda_k: 1 Loss: 0.03486276927711207\n",
      "Iteration: 1518 lambda_k: 1 Loss: 0.0347213478901562\n",
      "Iteration: 1519 lambda_k: 1 Loss: 0.03458004234241347\n",
      "Iteration: 1520 lambda_k: 1 Loss: 0.034438852549698085\n",
      "Iteration: 1521 lambda_k: 1 Loss: 0.034297778428144535\n",
      "Iteration: 1522 lambda_k: 1 Loss: 0.034156819894211425\n",
      "Iteration: 1523 lambda_k: 1 Loss: 0.034015976864684674\n",
      "Iteration: 1524 lambda_k: 1 Loss: 0.03387524925668153\n",
      "Iteration: 1525 lambda_k: 1 Loss: 0.033734636987654165\n",
      "Iteration: 1526 lambda_k: 1 Loss: 0.03359413997539348\n",
      "Iteration: 1527 lambda_k: 1 Loss: 0.03345375813803332\n",
      "Iteration: 1528 lambda_k: 1 Loss: 0.03331349139405452\n",
      "Iteration: 1529 lambda_k: 1 Loss: 0.033173339662288845\n",
      "Iteration: 1530 lambda_k: 1 Loss: 0.03303330286192364\n",
      "Iteration: 1531 lambda_k: 1 Loss: 0.032893380912505835\n",
      "Iteration: 1532 lambda_k: 1 Loss: 0.032753573733946904\n",
      "Iteration: 1533 lambda_k: 1 Loss: 0.03261388124652712\n",
      "Iteration: 1534 lambda_k: 1 Loss: 0.032474303370900535\n",
      "Iteration: 1535 lambda_k: 1 Loss: 0.03233484002809956\n",
      "Iteration: 1536 lambda_k: 1 Loss: 0.032195491139540185\n",
      "Iteration: 1537 lambda_k: 1 Loss: 0.03205625662702726\n",
      "Iteration: 1538 lambda_k: 1 Loss: 0.03191713641275916\n",
      "Iteration: 1539 lambda_k: 1 Loss: 0.031778130419333916\n",
      "Iteration: 1540 lambda_k: 1 Loss: 0.03163923856975404\n",
      "Iteration: 1541 lambda_k: 1 Loss: 0.03150046078743264\n",
      "Iteration: 1542 lambda_k: 1 Loss: 0.03136179699619919\n",
      "Iteration: 1543 lambda_k: 1 Loss: 0.031223247120305286\n",
      "Iteration: 1544 lambda_k: 1 Loss: 0.03108481108443079\n",
      "Iteration: 1545 lambda_k: 1 Loss: 0.03094648881369036\n",
      "Iteration: 1546 lambda_k: 1 Loss: 0.03080828023363955\n",
      "Iteration: 1547 lambda_k: 1 Loss: 0.030670185270281744\n",
      "Iteration: 1548 lambda_k: 1 Loss: 0.030532203850074564\n",
      "Iteration: 1549 lambda_k: 1 Loss: 0.030394335899937244\n",
      "Iteration: 1550 lambda_k: 1 Loss: 0.030256581347257387\n",
      "Iteration: 1551 lambda_k: 1 Loss: 0.03011894011989851\n",
      "Iteration: 1552 lambda_k: 1 Loss: 0.02998141214620765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1553 lambda_k: 1 Loss: 0.029843997355022838\n",
      "Iteration: 1554 lambda_k: 1 Loss: 0.029706695675681093\n",
      "Iteration: 1555 lambda_k: 1 Loss: 0.029569507038026726\n",
      "Iteration: 1556 lambda_k: 1 Loss: 0.029432431372419614\n",
      "Iteration: 1557 lambda_k: 1 Loss: 0.029295468609743774\n",
      "Iteration: 1558 lambda_k: 1 Loss: 0.02915861868141598\n",
      "Iteration: 1559 lambda_k: 1 Loss: 0.02902188151939519\n",
      "Iteration: 1560 lambda_k: 1 Loss: 0.028885257056191604\n",
      "Iteration: 1561 lambda_k: 1 Loss: 0.028748745224876193\n",
      "Iteration: 1562 lambda_k: 1 Loss: 0.02861234595909068\n",
      "Iteration: 1563 lambda_k: 1 Loss: 0.02847605919305753\n",
      "Iteration: 1564 lambda_k: 1 Loss: 0.02833988486159023\n",
      "Iteration: 1565 lambda_k: 1 Loss: 0.02820382290010418\n",
      "Iteration: 1566 lambda_k: 1 Loss: 0.028067873244627242\n",
      "Iteration: 1567 lambda_k: 1 Loss: 0.027932035831811315\n",
      "Iteration: 1568 lambda_k: 1 Loss: 0.027796310598943743\n",
      "Iteration: 1569 lambda_k: 1 Loss: 0.027660697483959207\n",
      "Iteration: 1570 lambda_k: 1 Loss: 0.02752519642545202\n",
      "Iteration: 1571 lambda_k: 1 Loss: 0.027389807362688538\n",
      "Iteration: 1572 lambda_k: 1 Loss: 0.02725453023562024\n",
      "Iteration: 1573 lambda_k: 1 Loss: 0.027119364984896796\n",
      "Iteration: 1574 lambda_k: 1 Loss: 0.02698431155187991\n",
      "Iteration: 1575 lambda_k: 1 Loss: 0.02684936987865744\n",
      "Iteration: 1576 lambda_k: 1 Loss: 0.026714539908057403\n",
      "Iteration: 1577 lambda_k: 1 Loss: 0.02657982158366337\n",
      "Iteration: 1578 lambda_k: 1 Loss: 0.026445214849829507\n",
      "Iteration: 1579 lambda_k: 1 Loss: 0.026310719647518275\n",
      "Iteration: 1580 lambda_k: 1 Loss: 0.026176335912779895\n",
      "Iteration: 1581 lambda_k: 1 Loss: 0.026042063620142466\n",
      "Iteration: 1582 lambda_k: 1 Loss: 0.02590790271237154\n",
      "Iteration: 1583 lambda_k: 1 Loss: 0.025773853130967035\n",
      "Iteration: 1584 lambda_k: 1 Loss: 0.025639914821624147\n",
      "Iteration: 1585 lambda_k: 1 Loss: 0.025506087734057343\n",
      "Iteration: 1586 lambda_k: 1 Loss: 0.02537237182033053\n",
      "Iteration: 1587 lambda_k: 1 Loss: 0.025238767033522666\n",
      "Iteration: 1588 lambda_k: 1 Loss: 0.025105273327165202\n",
      "Iteration: 1589 lambda_k: 1 Loss: 0.024971890655274644\n",
      "Iteration: 1590 lambda_k: 1 Loss: 0.024838618972634806\n",
      "Iteration: 1591 lambda_k: 1 Loss: 0.024705458235053157\n",
      "Iteration: 1592 lambda_k: 1 Loss: 0.02457240839948156\n",
      "Iteration: 1593 lambda_k: 1 Loss: 0.0244394694240246\n",
      "Iteration: 1594 lambda_k: 1 Loss: 0.024306641267907533\n",
      "Iteration: 1595 lambda_k: 1 Loss: 0.024173923891458622\n",
      "Iteration: 1596 lambda_k: 1 Loss: 0.0240413172561218\n",
      "Iteration: 1597 lambda_k: 1 Loss: 0.023908821324490057\n",
      "Iteration: 1598 lambda_k: 1 Loss: 0.02377643606034549\n",
      "Iteration: 1599 lambda_k: 1 Loss: 0.02364416142869519\n",
      "Iteration: 1600 lambda_k: 1 Loss: 0.02351199739580235\n",
      "Iteration: 1601 lambda_k: 1 Loss: 0.023379943929215063\n",
      "Iteration: 1602 lambda_k: 1 Loss: 0.023248000997795407\n",
      "Iteration: 1603 lambda_k: 1 Loss: 0.023116168571750733\n",
      "Iteration: 1604 lambda_k: 1 Loss: 0.0229844466226672\n",
      "Iteration: 1605 lambda_k: 1 Loss: 0.022852835123544985\n",
      "Iteration: 1606 lambda_k: 1 Loss: 0.022721334048834373\n",
      "Iteration: 1607 lambda_k: 1 Loss: 0.022589943374473277\n",
      "Iteration: 1608 lambda_k: 1 Loss: 0.022458663077925177\n",
      "Iteration: 1609 lambda_k: 1 Loss: 0.022327493138218853\n",
      "Iteration: 1610 lambda_k: 1 Loss: 0.0221964335359887\n",
      "Iteration: 1611 lambda_k: 1 Loss: 0.02206548425351729\n",
      "Iteration: 1612 lambda_k: 1 Loss: 0.021934645274779056\n",
      "Iteration: 1613 lambda_k: 1 Loss: 0.021803916585485195\n",
      "Iteration: 1614 lambda_k: 1 Loss: 0.021673298173130597\n",
      "Iteration: 1615 lambda_k: 1 Loss: 0.021542790027041972\n",
      "Iteration: 1616 lambda_k: 1 Loss: 0.02141239213842808\n",
      "Iteration: 1617 lambda_k: 1 Loss: 0.021282104500431248\n",
      "Iteration: 1618 lambda_k: 1 Loss: 0.02115192710818087\n",
      "Iteration: 1619 lambda_k: 1 Loss: 0.02102185995884884\n",
      "Iteration: 1620 lambda_k: 1 Loss: 0.02089190305170724\n",
      "Iteration: 1621 lambda_k: 1 Loss: 0.02076205638818737\n",
      "Iteration: 1622 lambda_k: 1 Loss: 0.020632319971941326\n",
      "Iteration: 1623 lambda_k: 1 Loss: 0.020502693808906135\n",
      "Iteration: 1624 lambda_k: 1 Loss: 0.0203731779073695\n",
      "Iteration: 1625 lambda_k: 1 Loss: 0.020243772278038356\n",
      "Iteration: 1626 lambda_k: 1 Loss: 0.020114476934109934\n",
      "Iteration: 1627 lambda_k: 1 Loss: 0.019985291891345427\n",
      "Iteration: 1628 lambda_k: 1 Loss: 0.019856217168146225\n",
      "Iteration: 1629 lambda_k: 1 Loss: 0.019727252785633313\n",
      "Iteration: 1630 lambda_k: 1 Loss: 0.01959839876772924\n",
      "Iteration: 1631 lambda_k: 1 Loss: 0.01946965514124357\n",
      "Iteration: 1632 lambda_k: 1 Loss: 0.019341021935961256\n",
      "Iteration: 1633 lambda_k: 1 Loss: 0.01921249918473481\n",
      "Iteration: 1634 lambda_k: 1 Loss: 0.019084086923579584\n",
      "Iteration: 1635 lambda_k: 1 Loss: 0.018955785191772948\n",
      "Iteration: 1636 lambda_k: 1 Loss: 0.01882759403195728\n",
      "Iteration: 1637 lambda_k: 1 Loss: 0.018699513490247014\n",
      "Iteration: 1638 lambda_k: 1 Loss: 0.018571543616340152\n",
      "Iteration: 1639 lambda_k: 1 Loss: 0.018443684463633786\n",
      "Iteration: 1640 lambda_k: 1 Loss: 0.018315936089344287\n",
      "Iteration: 1641 lambda_k: 1 Loss: 0.01818829855463267\n",
      "Iteration: 1642 lambda_k: 1 Loss: 0.01806077192473479\n",
      "Iteration: 1643 lambda_k: 1 Loss: 0.017933356269096857\n",
      "Iteration: 1644 lambda_k: 1 Loss: 0.017806051661516623\n",
      "Iteration: 1645 lambda_k: 1 Loss: 0.017678858180290316\n",
      "Iteration: 1646 lambda_k: 1 Loss: 0.017551775908365527\n",
      "Iteration: 1647 lambda_k: 1 Loss: 0.017424804933500706\n",
      "Iteration: 1648 lambda_k: 1 Loss: 0.017297945348431494\n",
      "Iteration: 1649 lambda_k: 1 Loss: 0.01717119725104378\n",
      "Iteration: 1650 lambda_k: 1 Loss: 0.017044560744554453\n",
      "Iteration: 1651 lambda_k: 1 Loss: 0.016918035937699513\n",
      "Iteration: 1652 lambda_k: 1 Loss: 0.016791622944930997\n",
      "Iteration: 1653 lambda_k: 1 Loss: 0.016665321886621846\n",
      "Iteration: 1654 lambda_k: 1 Loss: 0.01653913288927996\n",
      "Iteration: 1655 lambda_k: 1 Loss: 0.01641305608577226\n",
      "Iteration: 1656 lambda_k: 1 Loss: 0.01628709161555755\n",
      "Iteration: 1657 lambda_k: 1 Loss: 0.016161239624931236\n",
      "Iteration: 1658 lambda_k: 1 Loss: 0.01603550026728013\n",
      "Iteration: 1659 lambda_k: 1 Loss: 0.015909873703349103\n",
      "Iteration: 1660 lambda_k: 1 Loss: 0.015784360101519918\n",
      "Iteration: 1661 lambda_k: 1 Loss: 0.015658959638103404\n",
      "Iteration: 1662 lambda_k: 1 Loss: 0.015533672497644762\n",
      "Iteration: 1663 lambda_k: 1 Loss: 0.015408498873243134\n",
      "Iteration: 1664 lambda_k: 1 Loss: 0.015283438966886434\n",
      "Iteration: 1665 lambda_k: 1 Loss: 0.015158492989802496\n",
      "Iteration: 1666 lambda_k: 1 Loss: 0.015033661162826579\n",
      "Iteration: 1667 lambda_k: 1 Loss: 0.014908943716786489\n",
      "Iteration: 1668 lambda_k: 1 Loss: 0.014784340892907345\n",
      "Iteration: 1669 lambda_k: 1 Loss: 0.014659852943235267\n",
      "Iteration: 1670 lambda_k: 1 Loss: 0.014535480131082735\n",
      "Iteration: 1671 lambda_k: 1 Loss: 0.014411222731495709\n",
      "Iteration: 1672 lambda_k: 1 Loss: 0.01428708103174468\n",
      "Iteration: 1673 lambda_k: 1 Loss: 0.014163055331840091\n",
      "Iteration: 1674 lambda_k: 1 Loss: 0.014039145945074877\n",
      "Iteration: 1675 lambda_k: 1 Loss: 0.013915353198594325\n",
      "Iteration: 1676 lambda_k: 1 Loss: 0.013791677433995734\n",
      "Iteration: 1677 lambda_k: 1 Loss: 0.013668119007959903\n",
      "Iteration: 1678 lambda_k: 1 Loss: 0.013544678292915467\n",
      "Iteration: 1679 lambda_k: 1 Loss: 0.013421355677738863\n",
      "Iteration: 1680 lambda_k: 1 Loss: 0.013298151568492352\n",
      "Iteration: 1681 lambda_k: 1 Loss: 0.013175066389201327\n",
      "Iteration: 1682 lambda_k: 1 Loss: 0.013052100582674994\n",
      "Iteration: 1683 lambda_k: 1 Loss: 0.012929254611371494\n",
      "Iteration: 1684 lambda_k: 1 Loss: 0.012806528958311999\n",
      "Iteration: 1685 lambda_k: 1 Loss: 0.012683924128045622\n",
      "Iteration: 1686 lambda_k: 1 Loss: 0.01256144064766974\n",
      "Iteration: 1687 lambda_k: 1 Loss: 0.012439079067907722\n",
      "Iteration: 1688 lambda_k: 1 Loss: 0.012316839964249678\n",
      "Iteration: 1689 lambda_k: 1 Loss: 0.012194723938159345\n",
      "Iteration: 1690 lambda_k: 1 Loss: 0.012072731618352294\n",
      "Iteration: 1691 lambda_k: 1 Loss: 0.011950863662149465\n",
      "Iteration: 1692 lambda_k: 1 Loss: 0.011829120756912379\n",
      "Iteration: 1693 lambda_k: 1 Loss: 0.011707503621564903\n",
      "Iteration: 1694 lambda_k: 1 Loss: 0.011586013008208223\n",
      "Iteration: 1695 lambda_k: 1 Loss: 0.011464649703835253\n",
      "Iteration: 1696 lambda_k: 1 Loss: 0.011343414532151562\n",
      "Iteration: 1697 lambda_k: 1 Loss: 0.011222308355511818\n",
      "Iteration: 1698 lambda_k: 1 Loss: 0.011101332076978023\n",
      "Iteration: 1699 lambda_k: 1 Loss: 0.010980486642510893\n",
      "Iteration: 1700 lambda_k: 1 Loss: 0.01085977304330259\n",
      "Iteration: 1701 lambda_k: 1 Loss: 0.010739192318262546\n",
      "Iteration: 1702 lambda_k: 1 Loss: 0.01061874555666775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1703 lambda_k: 1 Loss: 0.010498433900990116\n",
      "Iteration: 1704 lambda_k: 1 Loss: 0.01037825854991512\n",
      "Iteration: 1705 lambda_k: 1 Loss: 0.010258220761566093\n",
      "Iteration: 1706 lambda_k: 1 Loss: 0.010138321856951245\n",
      "Iteration: 1707 lambda_k: 1 Loss: 0.010018563223651343\n",
      "Iteration: 1708 lambda_k: 1 Loss: 0.00989894631976697\n",
      "Iteration: 1709 lambda_k: 1 Loss: 0.009779472678147381\n",
      "Iteration: 1710 lambda_k: 1 Loss: 0.009660143910924046\n",
      "Iteration: 1711 lambda_k: 1 Loss: 0.009540961714374714\n",
      "Iteration: 1712 lambda_k: 1 Loss: 0.009421927874145737\n",
      "Iteration: 1713 lambda_k: 1 Loss: 0.009303044270863967\n",
      "Iteration: 1714 lambda_k: 1 Loss: 0.009184312886171263\n",
      "Iteration: 1715 lambda_k: 1 Loss: 0.009065735809219879\n",
      "Iteration: 1716 lambda_k: 1 Loss: 0.008947315243668302\n",
      "Iteration: 1717 lambda_k: 1 Loss: 0.008829053515223818\n",
      "Iteration: 1718 lambda_k: 1 Loss: 0.008710953079780457\n",
      "Iteration: 1719 lambda_k: 1 Loss: 0.008593016532207757\n",
      "Iteration: 1720 lambda_k: 1 Loss: 0.008475246615850801\n",
      "Iteration: 1721 lambda_k: 1 Loss: 0.008357646232807697\n",
      "Iteration: 1722 lambda_k: 1 Loss: 0.008240218455059662\n",
      "Iteration: 1723 lambda_k: 1 Loss: 0.008122966536535214\n",
      "Iteration: 1724 lambda_k: 1 Loss: 0.008005893926199322\n",
      "Iteration: 1725 lambda_k: 1 Loss: 0.007889004282269383\n",
      "Iteration: 1726 lambda_k: 1 Loss: 0.0077723014876697775\n",
      "Iteration: 1727 lambda_k: 1 Loss: 0.007655789666851183\n",
      "Iteration: 1728 lambda_k: 1 Loss: 0.00753947320411298\n",
      "Iteration: 1729 lambda_k: 1 Loss: 0.007423356763585696\n",
      "Iteration: 1730 lambda_k: 1 Loss: 0.007307445311046992\n",
      "Iteration: 1731 lambda_k: 1 Loss: 0.007191744137765848\n",
      "Iteration: 1732 lambda_k: 1 Loss: 0.007076258886593004\n",
      "Iteration: 1733 lambda_k: 1 Loss: 0.006960995580541947\n",
      "Iteration: 1734 lambda_k: 1 Loss: 0.006845960654135931\n",
      "Iteration: 1735 lambda_k: 1 Loss: 0.006731160987827848\n",
      "Iteration: 1736 lambda_k: 1 Loss: 0.006616603945841202\n",
      "Iteration: 1737 lambda_k: 1 Loss: 0.006502297417823309\n",
      "Iteration: 1738 lambda_k: 1 Loss: 0.006388249864751855\n",
      "Iteration: 1739 lambda_k: 1 Loss: 0.006274470369593277\n",
      "Iteration: 1740 lambda_k: 1 Loss: 0.006160968693276736\n",
      "Iteration: 1741 lambda_k: 1 Loss: 0.006047755336622343\n",
      "Iteration: 1742 lambda_k: 1 Loss: 0.005934841608946117\n",
      "Iteration: 1743 lambda_k: 1 Loss: 0.005822239704163174\n",
      "Iteration: 1744 lambda_k: 1 Loss: 0.005709962785321651\n",
      "Iteration: 1745 lambda_k: 1 Loss: 0.005598025078626703\n",
      "Iteration: 1746 lambda_k: 1 Loss: 0.005486441978160547\n",
      "Iteration: 1747 lambda_k: 1 Loss: 0.005375230162673541\n",
      "Iteration: 1748 lambda_k: 1 Loss: 0.005264407726009582\n",
      "Iteration: 1749 lambda_k: 1 Loss: 0.005153994322953019\n",
      "Iteration: 1750 lambda_k: 1 Loss: 0.005044011332536226\n",
      "Iteration: 1751 lambda_k: 1 Loss: 0.004934482041163158\n",
      "Iteration: 1752 lambda_k: 1 Loss: 0.004825431848495825\n",
      "Iteration: 1753 lambda_k: 1 Loss: 0.004716888496007495\n",
      "Iteration: 1754 lambda_k: 1 Loss: 0.004608882331043383\n",
      "Iteration: 1755 lambda_k: 1 Loss: 0.004501446596281218\n",
      "Iteration: 1756 lambda_k: 1 Loss: 0.004394617764046562\n",
      "Iteration: 1757 lambda_k: 1 Loss: 0.004288435892888051\n",
      "Iteration: 1758 lambda_k: 1 Loss: 0.0041829450735057176\n",
      "Iteration: 1759 lambda_k: 1 Loss: 0.004078193907834742\n",
      "Iteration: 1760 lambda_k: 1 Loss: 0.00399051901952347\n",
      "Iteration: 1761 lambda_k: 1 Loss: 0.003907940334290246\n",
      "Iteration: 1762 lambda_k: 1 Loss: 0.0038296195104711183\n",
      "Iteration: 1763 lambda_k: 1 Loss: 0.0037556635480761697\n",
      "Iteration: 1764 lambda_k: 1 Loss: 0.003685704444604224\n",
      "Iteration: 1765 lambda_k: 1 Loss: 0.0036192517138824857\n",
      "Iteration: 1766 lambda_k: 1 Loss: 0.003555912090263617\n",
      "Iteration: 1767 lambda_k: 1 Loss: 0.0034954418113600145\n",
      "Iteration: 1768 lambda_k: 1 Loss: 0.0034377054113155056\n",
      "Iteration: 1769 lambda_k: 1 Loss: 0.003382616371948697\n",
      "Iteration: 1770 lambda_k: 1 Loss: 0.0033300969074308013\n",
      "Iteration: 1771 lambda_k: 1 Loss: 0.003280062331427512\n",
      "Iteration: 1772 lambda_k: 1 Loss: 0.0032324212415598243\n",
      "Iteration: 1773 lambda_k: 1 Loss: 0.0031870813232340893\n",
      "Iteration: 1774 lambda_k: 1 Loss: 0.003143954365837773\n",
      "Iteration: 1775 lambda_k: 1 Loss: 0.0031029585109017964\n",
      "Iteration: 1776 lambda_k: 1 Loss: 0.003064018097064637\n",
      "Iteration: 1777 lambda_k: 1 Loss: 0.0030270625202401\n",
      "Iteration: 1778 lambda_k: 1 Loss: 0.002992024885230611\n",
      "Iteration: 1779 lambda_k: 1 Loss: 0.0029588410474327654\n",
      "Iteration: 1780 lambda_k: 1 Loss: 0.0029274489845545638\n",
      "Iteration: 1781 lambda_k: 1 Loss: 0.002897788497488799\n",
      "Iteration: 1782 lambda_k: 1 Loss: 0.0028698010485377237\n",
      "Iteration: 1783 lambda_k: 1 Loss: 0.0028434296454555876\n",
      "Iteration: 1784 lambda_k: 1 Loss: 0.002818618750672684\n",
      "Iteration: 1785 lambda_k: 1 Loss: 0.002795314120546675\n",
      "Iteration: 1786 lambda_k: 1 Loss: 0.002773462751967021\n",
      "Iteration: 1787 lambda_k: 1 Loss: 0.0027530127750064635\n",
      "Iteration: 1788 lambda_k: 1 Loss: 0.002733912529974011\n",
      "Iteration: 1789 lambda_k: 1 Loss: 0.002715661244685204\n",
      "Iteration: 1790 lambda_k: 1 Loss: 0.002698449161458468\n",
      "Iteration: 1791 lambda_k: 1 Loss: 0.0026824522591897084\n",
      "Iteration: 1792 lambda_k: 1 Loss: 0.0026677182868002623\n",
      "Iteration: 1793 lambda_k: 1 Loss: 0.002654218725122958\n",
      "Iteration: 1794 lambda_k: 1 Loss: 0.0026418709521831775\n",
      "Iteration: 1795 lambda_k: 1 Loss: 0.002630578556904833\n",
      "Iteration: 1796 lambda_k: 1 Loss: 0.0026202580056581427\n",
      "Iteration: 1797 lambda_k: 1 Loss: 0.0026108458881674824\n",
      "Iteration: 1798 lambda_k: 1 Loss: 0.002602293786606807\n",
      "Iteration: 1799 lambda_k: 1 Loss: 0.0025945598416362214\n",
      "Iteration: 1800 lambda_k: 1 Loss: 0.0025876028345875263\n",
      "Iteration: 1801 lambda_k: 1 Loss: 0.0025813802199870705\n",
      "Iteration: 1802 lambda_k: 1 Loss: 0.002575848788801561\n",
      "Iteration: 1803 lambda_k: 1 Loss: 0.002570966124523147\n",
      "Iteration: 1804 lambda_k: 1 Loss: 0.002566691724333795\n",
      "Iteration: 1805 lambda_k: 1 Loss: 0.002562987472220482\n",
      "Iteration: 1806 lambda_k: 1 Loss: 0.002559817608321329\n",
      "Iteration: 1807 lambda_k: 1 Loss: 0.0025571484649483615\n",
      "Iteration: 1808 lambda_k: 1 Loss: 0.0025549481866081005\n",
      "Iteration: 1809 lambda_k: 1 Loss: 0.002553186541044891\n",
      "Iteration: 1810 lambda_k: 1 Loss: 0.0025518348473791094\n",
      "Iteration: 1811 lambda_k: 1 Loss: 0.0025508659128759673\n",
      "Iteration: 1812 lambda_k: 1 Loss: 0.002550254054762772\n",
      "Iteration: 1813 lambda_k: 1 Loss: 0.002549975076711959\n",
      "Iteration: 1814 lambda_k: 1 Loss: 0.0025500062148428554\n",
      "Iteration: 1815 lambda_k: 1 Loss: 0.0025503260706872467\n",
      "Iteration: 1816 lambda_k: 1 Loss: 0.00255091454010359\n",
      "Iteration: 1817 lambda_k: 1 Loss: 0.0025517527456411803\n",
      "Iteration: 1818 lambda_k: 1 Loss: 0.002552822957346528\n",
      "Iteration: 1819 lambda_k: 1 Loss: 0.002554108581999445\n",
      "Iteration: 1820 lambda_k: 1 Loss: 0.002555594069978419\n",
      "Iteration: 1821 lambda_k: 1 Loss: 0.002557264855012582\n",
      "Iteration: 1822 lambda_k: 1 Loss: 0.0025591073067876475\n",
      "Iteration: 1823 lambda_k: 1 Loss: 0.002561108676211725\n",
      "Iteration: 1824 lambda_k: 1 Loss: 0.002563257039451196\n",
      "Iteration: 1825 lambda_k: 1 Loss: 0.002565541242598033\n",
      "Iteration: 1826 lambda_k: 1 Loss: 0.0025679508483924334\n",
      "Iteration: 1827 lambda_k: 1 Loss: 0.0025704760852176137\n",
      "Iteration: 1828 lambda_k: 1 Loss: 0.0025731077980090775\n",
      "Iteration: 1829 lambda_k: 1 Loss: 0.0025758374007977167\n",
      "Iteration: 1830 lambda_k: 1 Loss: 0.0025786568309140835\n",
      "Iteration: 1831 lambda_k: 1 Loss: 0.002581558505100845\n",
      "Iteration: 1832 lambda_k: 1 Loss: 0.002584535277836117\n",
      "Iteration: 1833 lambda_k: 1 Loss: 0.002587580402126021\n",
      "Iteration: 1834 lambda_k: 1 Loss: 0.0025906874929644405\n",
      "Iteration: 1835 lambda_k: 1 Loss: 0.002593850493617825\n",
      "Iteration: 1836 lambda_k: 1 Loss: 0.0025970636448720674\n",
      "Iteration: 1837 lambda_k: 1 Loss: 0.002600321457359774\n",
      "Iteration: 1838 lambda_k: 1 Loss: 0.0026036186870559835\n",
      "Iteration: 1839 lambda_k: 1 Loss: 0.002606950313985137\n",
      "Iteration: 1840 lambda_k: 1 Loss: 0.002610311524125337\n",
      "Iteration: 1841 lambda_k: 1 Loss: 0.002613697694432397\n",
      "Iteration: 1842 lambda_k: 1 Loss: 0.002617104380841889\n",
      "Iteration: 1843 lambda_k: 1 Loss: 0.0026205273090460987\n",
      "Iteration: 1844 lambda_k: 1 Loss: 0.0026239623677879295\n",
      "Iteration: 1845 lambda_k: 1 Loss: 0.0026274056043688425\n",
      "Iteration: 1846 lambda_k: 1 Loss: 0.002630853222034678\n",
      "Iteration: 1847 lambda_k: 1 Loss: 0.0026343015788828903\n",
      "Iteration: 1848 lambda_k: 1 Loss: 0.0026377471879296204\n",
      "Iteration: 1849 lambda_k: 1 Loss: 0.0026411867179825643\n",
      "Iteration: 1850 lambda_k: 1 Loss: 0.0026446169949860565\n",
      "Iteration: 1851 lambda_k: 1 Loss: 0.0026480350035363114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1852 lambda_k: 1 Loss: 0.002651437888302902\n",
      "Iteration: 1853 lambda_k: 1 Loss: 0.00265482295513739\n",
      "Iteration: 1854 lambda_k: 1 Loss: 0.0026581876716962162\n",
      "Iteration: 1855 lambda_k: 1 Loss: 0.002661529667451411\n",
      "Iteration: 1856 lambda_k: 1 Loss: 0.0026648467330069006\n",
      "Iteration: 1857 lambda_k: 1 Loss: 0.002668136818678295\n",
      "Iteration: 1858 lambda_k: 1 Loss: 0.002671398032329008\n",
      "Iteration: 1859 lambda_k: 1 Loss: 0.0026746286364849035\n",
      "Iteration: 1860 lambda_k: 1 Loss: 0.002677827044772586\n",
      "Iteration: 1861 lambda_k: 1 Loss: 0.002680991817744057\n",
      "Iteration: 1862 lambda_k: 1 Loss: 0.002684121658162196\n",
      "Iteration: 1863 lambda_k: 1 Loss: 0.0026872154058283915\n",
      "Iteration: 1864 lambda_k: 1 Loss: 0.0026902720320365406\n",
      "Iteration: 1865 lambda_k: 1 Loss: 0.002693290633737416\n",
      "Iteration: 1866 lambda_k: 1 Loss: 0.0026962704274936748\n",
      "Iteration: 1867 lambda_k: 1 Loss: 0.002699210743301172\n",
      "Iteration: 1868 lambda_k: 1 Loss: 0.0027021110183454503\n",
      "Iteration: 1869 lambda_k: 1 Loss: 0.002704970790755283\n",
      "Iteration: 1870 lambda_k: 1 Loss: 0.002707789693407154\n",
      "Iteration: 1871 lambda_k: 1 Loss: 0.002710567447827372\n",
      "Iteration: 1872 lambda_k: 1 Loss: 0.0027133038582304748\n",
      "Iteration: 1873 lambda_k: 1 Loss: 0.0027159988057260855\n",
      "Iteration: 1874 lambda_k: 1 Loss: 0.002718652242719568\n",
      "Iteration: 1875 lambda_k: 1 Loss: 0.0027212641875258665\n",
      "Iteration: 1876 lambda_k: 1 Loss: 0.0027238347192110003\n",
      "Iteration: 1877 lambda_k: 1 Loss: 0.0027263639726705638\n",
      "Iteration: 1878 lambda_k: 1 Loss: 0.002728852133951244\n",
      "Iteration: 1879 lambda_k: 1 Loss: 0.0027312994358173236\n",
      "Iteration: 1880 lambda_k: 1 Loss: 0.002733706153561727\n",
      "Iteration: 1881 lambda_k: 1 Loss: 0.002736072601058708\n",
      "Iteration: 1882 lambda_k: 1 Loss: 0.0027383991270533556\n",
      "Iteration: 1883 lambda_k: 1 Loss: 0.002740686111681664\n",
      "Iteration: 1884 lambda_k: 1 Loss: 0.0027429339632134924\n",
      "Iteration: 1885 lambda_k: 1 Loss: 0.002745143115010379\n",
      "Iteration: 1886 lambda_k: 1 Loss: 0.002747314022689189\n",
      "Iteration: 1887 lambda_k: 1 Loss: 0.0027494471614820408\n",
      "Iteration: 1888 lambda_k: 1 Loss: 0.0027515430237833646\n",
      "Iteration: 1889 lambda_k: 1 Loss: 0.002753602116874092\n",
      "Iteration: 1890 lambda_k: 1 Loss: 0.002755624960813616\n",
      "Iteration: 1891 lambda_k: 1 Loss: 0.0027576120864900694\n",
      "Iteration: 1892 lambda_k: 1 Loss: 0.002759564033819489\n",
      "Iteration: 1893 lambda_k: 1 Loss: 0.002761481350085292\n",
      "Iteration: 1894 lambda_k: 1 Loss: 0.002763364588408943\n",
      "Iteration: 1895 lambda_k: 1 Loss: 0.0027652143063439387\n",
      "Iteration: 1896 lambda_k: 1 Loss: 0.002767031064584863\n",
      "Iteration: 1897 lambda_k: 1 Loss: 0.0027688154257841497\n",
      "Iteration: 1898 lambda_k: 1 Loss: 0.0027705679534691646\n",
      "Iteration: 1899 lambda_k: 1 Loss: 0.002772289211053012\n",
      "Iteration: 1900 lambda_k: 1 Loss: 0.0027739797609324063\n",
      "Iteration: 1901 lambda_k: 1 Loss: 0.0027756401636668472\n",
      "Iteration: 1902 lambda_k: 1 Loss: 0.002777270977232953\n",
      "Iteration: 1903 lambda_k: 1 Loss: 0.002778872756349039\n",
      "Iteration: 1904 lambda_k: 1 Loss: 0.0027804460518645563\n",
      "Iteration: 1905 lambda_k: 1 Loss: 0.002781991410209917\n",
      "Iteration: 1906 lambda_k: 1 Loss: 0.002783509372902114\n",
      "Iteration: 1907 lambda_k: 1 Loss: 0.002785000543089288\n",
      "Iteration: 1908 lambda_k: 1 Loss: 0.002786465331456264\n",
      "Iteration: 1909 lambda_k: 1 Loss: 0.0027879043238481518\n",
      "Iteration: 1910 lambda_k: 1 Loss: 0.0027893180325944678\n",
      "Iteration: 1911 lambda_k: 1 Loss: 0.0027907069651216896\n",
      "Iteration: 1912 lambda_k: 1 Loss: 0.0027920716230081\n",
      "Iteration: 1913 lambda_k: 1 Loss: 0.0027934125022331187\n",
      "Iteration: 1914 lambda_k: 1 Loss: 0.0027947300930664764\n",
      "Iteration: 1915 lambda_k: 1 Loss: 0.0027960248792160748\n",
      "Iteration: 1916 lambda_k: 1 Loss: 0.0027972973368053967\n",
      "Iteration: 1917 lambda_k: 1 Loss: 0.00279854793375628\n",
      "Iteration: 1918 lambda_k: 1 Loss: 0.002799777129692983\n",
      "Iteration: 1919 lambda_k: 1 Loss: 0.002800985376138708\n",
      "Iteration: 1920 lambda_k: 1 Loss: 0.002802173116733023\n",
      "Iteration: 1921 lambda_k: 1 Loss: 0.002803340787335621\n",
      "Iteration: 1922 lambda_k: 1 Loss: 0.0028044888160221972\n",
      "Iteration: 1923 lambda_k: 1 Loss: 0.002805617623038744\n",
      "Iteration: 1924 lambda_k: 1 Loss: 0.0028067276207701633\n",
      "Iteration: 1925 lambda_k: 1 Loss: 0.0028078192137449116\n",
      "Iteration: 1926 lambda_k: 1 Loss: 0.0028088927986708342\n",
      "Iteration: 1927 lambda_k: 1 Loss: 0.002809948764489343\n",
      "Iteration: 1928 lambda_k: 1 Loss: 0.002810987492437977\n",
      "Iteration: 1929 lambda_k: 1 Loss: 0.00281200935611703\n",
      "Iteration: 1930 lambda_k: 1 Loss: 0.002813014721560095\n",
      "Iteration: 1931 lambda_k: 1 Loss: 0.002814003947309356\n",
      "Iteration: 1932 lambda_k: 1 Loss: 0.0028149773844959244\n",
      "Iteration: 1933 lambda_k: 1 Loss: 0.0028159353769253467\n",
      "Iteration: 1934 lambda_k: 1 Loss: 0.0028168782611675664\n",
      "Iteration: 1935 lambda_k: 1 Loss: 0.00281780636665074\n",
      "Iteration: 1936 lambda_k: 1 Loss: 0.002818720015758485\n",
      "Iteration: 1937 lambda_k: 1 Loss: 0.002819619523929862\n",
      "Iteration: 1938 lambda_k: 1 Loss: 0.002820505199761751\n",
      "Iteration: 1939 lambda_k: 1 Loss: 0.0028213773451132994\n",
      "Iteration: 1940 lambda_k: 1 Loss: 0.0028222362552120417\n",
      "Iteration: 1941 lambda_k: 1 Loss: 0.002823082218761289\n",
      "Iteration: 1942 lambda_k: 1 Loss: 0.002823915518048701\n",
      "Iteration: 1943 lambda_k: 1 Loss: 0.0028247364290557167\n",
      "Iteration: 1944 lambda_k: 1 Loss: 0.0028255452215674133\n",
      "Iteration: 1945 lambda_k: 1 Loss: 0.0028263421592828945\n",
      "Iteration: 1946 lambda_k: 1 Loss: 0.0028271274999257224\n",
      "Iteration: 1947 lambda_k: 1 Loss: 0.002827901495354372\n",
      "Iteration: 1948 lambda_k: 1 Loss: 0.0028286643916724504\n",
      "Iteration: 1949 lambda_k: 1 Loss: 0.0028294164293385703\n",
      "Iteration: 1950 lambda_k: 1 Loss: 0.0028301578432757503\n",
      "Iteration: 1951 lambda_k: 1 Loss: 0.002830888862980076\n",
      "Iteration: 1952 lambda_k: 1 Loss: 0.002831609712628761\n",
      "Iteration: 1953 lambda_k: 1 Loss: 0.0028323206111872806\n",
      "Iteration: 1954 lambda_k: 1 Loss: 0.0028330217725155643\n",
      "Iteration: 1955 lambda_k: 1 Loss: 0.002833713405473103\n",
      "Iteration: 1956 lambda_k: 1 Loss: 0.0028343957140230785\n",
      "Iteration: 1957 lambda_k: 1 Loss: 0.002835068897335175\n",
      "Iteration: 1958 lambda_k: 1 Loss: 0.002835733149887295\n",
      "Iteration: 1959 lambda_k: 1 Loss: 0.0028363886615658316\n",
      "Iteration: 1960 lambda_k: 1 Loss: 0.002837035617764738\n",
      "Iteration: 1961 lambda_k: 1 Loss: 0.002837674199483126\n",
      "Iteration: 1962 lambda_k: 1 Loss: 0.002838304583421508\n",
      "Iteration: 1963 lambda_k: 1 Loss: 0.0028389269420766037\n",
      "Iteration: 1964 lambda_k: 1 Loss: 0.0028395414438346055\n",
      "Iteration: 1965 lambda_k: 1 Loss: 0.0028401482530630343\n",
      "Iteration: 1966 lambda_k: 1 Loss: 0.002840747530201053\n",
      "Iteration: 1967 lambda_k: 1 Loss: 0.0028413394318482824\n",
      "Iteration: 1968 lambda_k: 1 Loss: 0.0028419241108520307\n",
      "Iteration: 1969 lambda_k: 1 Loss: 0.002842501716393146\n",
      "Iteration: 1970 lambda_k: 1 Loss: 0.002843072394070072\n",
      "Iteration: 1971 lambda_k: 1 Loss: 0.0028436362859816304\n",
      "Iteration: 1972 lambda_k: 1 Loss: 0.0028441935308081033\n",
      "Iteration: 1973 lambda_k: 1 Loss: 0.0028447442638908364\n",
      "Iteration: 1974 lambda_k: 1 Loss: 0.002845288617310307\n",
      "Iteration: 1975 lambda_k: 1 Loss: 0.002845826719962733\n",
      "Iteration: 1976 lambda_k: 1 Loss: 0.0028463586976350073\n",
      "Iteration: 1977 lambda_k: 1 Loss: 0.002846884673078331\n",
      "Iteration: 1978 lambda_k: 1 Loss: 0.0028474047660802635\n",
      "Iteration: 1979 lambda_k: 1 Loss: 0.0028479190935352745\n",
      "Iteration: 1980 lambda_k: 1 Loss: 0.0028484277695138955\n",
      "Iteration: 1981 lambda_k: 1 Loss: 0.0028489309053303827\n",
      "Iteration: 1982 lambda_k: 1 Loss: 0.002849428609609029\n",
      "Iteration: 1983 lambda_k: 1 Loss: 0.002849920988348938\n",
      "Iteration: 1984 lambda_k: 1 Loss: 0.0028504081449875424\n",
      "Iteration: 1985 lambda_k: 1 Loss: 0.0028508901804627034\n",
      "Iteration: 1986 lambda_k: 1 Loss: 0.0028513671932734086\n",
      "Iteration: 1987 lambda_k: 1 Loss: 0.0028518392795391797\n",
      "Iteration: 1988 lambda_k: 1 Loss: 0.002852306533058201\n",
      "Iteration: 1989 lambda_k: 1 Loss: 0.0028527690453640678\n",
      "Iteration: 1990 lambda_k: 1 Loss: 0.002853226905781327\n",
      "Iteration: 1991 lambda_k: 1 Loss: 0.0028536802014797876\n",
      "Iteration: 1992 lambda_k: 1 Loss: 0.002854129017527536\n",
      "Iteration: 1993 lambda_k: 1 Loss: 0.002854573436942771\n",
      "Iteration: 1994 lambda_k: 1 Loss: 0.0028550135407444666\n",
      "Iteration: 1995 lambda_k: 1 Loss: 0.002855449408001837\n",
      "Iteration: 1996 lambda_k: 1 Loss: 0.002855881115882686\n",
      "Iteration: 1997 lambda_k: 1 Loss: 0.0028563087397006155\n",
      "Iteration: 1998 lambda_k: 1 Loss: 0.0028567323529611432\n",
      "Iteration: 1999 lambda_k: 1 Loss: 0.002857152027406723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2000 lambda_k: 1 Loss: 0.002857567833060738\n",
      "Iteration: 2001 lambda_k: 1 Loss: 0.00285797983827038\n",
      "Iteration: 2002 lambda_k: 1 Loss: 0.0028583881097486107\n",
      "Iteration: 2003 lambda_k: 1 Loss: 0.0028587927126150436\n",
      "Iteration: 2004 lambda_k: 1 Loss: 0.0028591937104358897\n",
      "Iteration: 2005 lambda_k: 1 Loss: 0.002859591165262899\n",
      "Iteration: 2006 lambda_k: 1 Loss: 0.002859985137671456\n",
      "Iteration: 2007 lambda_k: 1 Loss: 0.0028603756867976587\n",
      "Iteration: 2008 lambda_k: 1 Loss: 0.0028607628703745573\n",
      "Iteration: 2009 lambda_k: 1 Loss: 0.0028611467447674835\n",
      "Iteration: 2010 lambda_k: 1 Loss: 0.002861527365008498\n",
      "Iteration: 2011 lambda_k: 1 Loss: 0.0028619047848300857\n",
      "Iteration: 2012 lambda_k: 1 Loss: 0.002862279056697939\n",
      "Iteration: 2013 lambda_k: 1 Loss: 0.0028626502318429897\n",
      "Iteration: 2014 lambda_k: 1 Loss: 0.002863018360292564\n",
      "Iteration: 2015 lambda_k: 1 Loss: 0.0028633834909009195\n",
      "Iteration: 2016 lambda_k: 1 Loss: 0.002863745671378891\n",
      "Iteration: 2017 lambda_k: 1 Loss: 0.0028641049483228613\n",
      "Iteration: 2018 lambda_k: 1 Loss: 0.0028644613672430133\n",
      "Iteration: 2019 lambda_k: 1 Loss: 0.0028648149725908644\n",
      "Iteration: 2020 lambda_k: 1 Loss: 0.002865165807786152\n",
      "Iteration: 2021 lambda_k: 1 Loss: 0.0028655139152429336\n",
      "Iteration: 2022 lambda_k: 1 Loss: 0.0028658593363951987\n",
      "Iteration: 2023 lambda_k: 1 Loss: 0.002866202111721721\n",
      "Iteration: 2024 lambda_k: 1 Loss: 0.0028665422807703602\n",
      "Iteration: 2025 lambda_k: 1 Loss: 0.0028668798821816707\n",
      "Iteration: 2026 lambda_k: 1 Loss: 0.0028672149537119786\n",
      "Iteration: 2027 lambda_k: 1 Loss: 0.002867547532255838\n",
      "Iteration: 2028 lambda_k: 1 Loss: 0.002867877653868005\n",
      "Iteration: 2029 lambda_k: 1 Loss: 0.0028682053537847072\n",
      "Iteration: 2030 lambda_k: 1 Loss: 0.0028685306664445677\n",
      "Iteration: 2031 lambda_k: 1 Loss: 0.0028688536255087717\n",
      "Iteration: 2032 lambda_k: 1 Loss: 0.002869174263880907\n",
      "Iteration: 2033 lambda_k: 1 Loss: 0.002869492613726229\n",
      "Iteration: 2034 lambda_k: 1 Loss: 0.0028698087064904366\n",
      "Iteration: 2035 lambda_k: 1 Loss: 0.0028701225729179662\n",
      "Iteration: 2036 lambda_k: 1 Loss: 0.002870434243069831\n",
      "Iteration: 2037 lambda_k: 1 Loss: 0.0028707437463409845\n",
      "Iteration: 2038 lambda_k: 1 Loss: 0.0028710511114772895\n",
      "Iteration: 2039 lambda_k: 1 Loss: 0.0028713563665919554\n",
      "Iteration: 2040 lambda_k: 1 Loss: 0.0028716595391817027\n",
      "Iteration: 2041 lambda_k: 1 Loss: 0.002871960656142353\n",
      "Iteration: 2042 lambda_k: 1 Loss: 0.002872259743784148\n",
      "Iteration: 2043 lambda_k: 1 Loss: 0.002872556827846586\n",
      "Iteration: 2044 lambda_k: 1 Loss: 0.002872851933512904\n",
      "Iteration: 2045 lambda_k: 1 Loss: 0.0028731450854242637\n",
      "Iteration: 2046 lambda_k: 1 Loss: 0.0028734363076933717\n",
      "Iteration: 2047 lambda_k: 1 Loss: 0.0028737256239180363\n",
      "Iteration: 2048 lambda_k: 1 Loss: 0.0028740130571941183\n",
      "Iteration: 2049 lambda_k: 1 Loss: 0.0028742986301282885\n",
      "Iteration: 2050 lambda_k: 1 Loss: 0.0028745823648504026\n",
      "Iteration: 2051 lambda_k: 1 Loss: 0.002874864283025552\n",
      "Iteration: 2052 lambda_k: 1 Loss: 0.002875144405865816\n",
      "Iteration: 2053 lambda_k: 1 Loss: 0.002875422754141814\n",
      "Iteration: 2054 lambda_k: 1 Loss: 0.0028756993481937262\n",
      "Iteration: 2055 lambda_k: 1 Loss: 0.002875974207942222\n",
      "Iteration: 2056 lambda_k: 1 Loss: 0.0028762473528990582\n",
      "Iteration: 2057 lambda_k: 1 Loss: 0.0028765188021773764\n",
      "Iteration: 2058 lambda_k: 1 Loss: 0.002876788574501748\n",
      "Iteration: 2059 lambda_k: 1 Loss: 0.002877056688217928\n",
      "Iteration: 2060 lambda_k: 1 Loss: 0.0028773231613024365\n",
      "Iteration: 2061 lambda_k: 1 Loss: 0.0028775880113717798\n",
      "Iteration: 2062 lambda_k: 1 Loss: 0.00287785125569159\n",
      "Iteration: 2063 lambda_k: 1 Loss: 0.0028781129111853138\n",
      "Iteration: 2064 lambda_k: 1 Loss: 0.0028783729944428437\n",
      "Iteration: 2065 lambda_k: 1 Loss: 0.002878631521728883\n",
      "Iteration: 2066 lambda_k: 1 Loss: 0.002878888508991061\n",
      "Iteration: 2067 lambda_k: 1 Loss: 0.002879143971867862\n",
      "Iteration: 2068 lambda_k: 1 Loss: 0.0028793979256963816\n",
      "Iteration: 2069 lambda_k: 1 Loss: 0.0028796503855197334\n",
      "Iteration: 2070 lambda_k: 1 Loss: 0.0028799013660945148\n",
      "Iteration: 2071 lambda_k: 1 Loss: 0.002880150881897848\n",
      "Iteration: 2072 lambda_k: 1 Loss: 0.0028803989471343243\n",
      "Iteration: 2073 lambda_k: 1 Loss: 0.0028806455757428052\n",
      "Iteration: 2074 lambda_k: 1 Loss: 0.002880890781402973\n",
      "Iteration: 2075 lambda_k: 1 Loss: 0.0028811345775417965\n",
      "Iteration: 2076 lambda_k: 1 Loss: 0.002881376977339712\n",
      "Iteration: 2077 lambda_k: 1 Loss: 0.0028816179937367855\n",
      "Iteration: 2078 lambda_k: 1 Loss: 0.0028818576394385492\n",
      "Iteration: 2079 lambda_k: 1 Loss: 0.0028820959269218495\n",
      "Iteration: 2080 lambda_k: 1 Loss: 0.002882332868440444\n",
      "Iteration: 2081 lambda_k: 1 Loss: 0.0028825684760304744\n",
      "Iteration: 2082 lambda_k: 1 Loss: 0.0028828027615157843\n",
      "Iteration: 2083 lambda_k: 1 Loss: 0.0028830357365131343\n",
      "Iteration: 2084 lambda_k: 1 Loss: 0.0028832674124372216\n",
      "Iteration: 2085 lambda_k: 1 Loss: 0.002883497800505659\n",
      "Iteration: 2086 lambda_k: 1 Loss: 0.002883726911743757\n",
      "Iteration: 2087 lambda_k: 1 Loss: 0.002883954756989133\n",
      "Iteration: 2088 lambda_k: 1 Loss: 0.002884181346896354\n",
      "Iteration: 2089 lambda_k: 1 Loss: 0.0028844066919412824\n",
      "Iteration: 2090 lambda_k: 1 Loss: 0.0028846308024254555\n",
      "Iteration: 2091 lambda_k: 1 Loss: 0.0028848536884802456\n",
      "Iteration: 2092 lambda_k: 1 Loss: 0.002885075360070992\n",
      "Iteration: 2093 lambda_k: 1 Loss: 0.002885295827000957\n",
      "Iteration: 2094 lambda_k: 1 Loss: 0.002885515098915184\n",
      "Iteration: 2095 lambda_k: 1 Loss: 0.002885733185304344\n",
      "Iteration: 2096 lambda_k: 1 Loss: 0.0028859500955083503\n",
      "Iteration: 2097 lambda_k: 1 Loss: 0.0028861658387199993\n",
      "Iteration: 2098 lambda_k: 1 Loss: 0.0028863804239884303\n",
      "Iteration: 2099 lambda_k: 1 Loss: 0.002886593860222518\n",
      "Iteration: 2100 lambda_k: 1 Loss: 0.002886806156194225\n",
      "Iteration: 2101 lambda_k: 1 Loss: 0.0028870173205417676\n",
      "Iteration: 2102 lambda_k: 1 Loss: 0.0028872273617728257\n",
      "Iteration: 2103 lambda_k: 1 Loss: 0.0028874362882675476\n",
      "Iteration: 2104 lambda_k: 1 Loss: 0.0028876441082815577\n",
      "Iteration: 2105 lambda_k: 1 Loss: 0.002887850829948864\n",
      "Iteration: 2106 lambda_k: 1 Loss: 0.0028880564612846597\n",
      "Iteration: 2107 lambda_k: 1 Loss: 0.0028882610101881073\n",
      "Iteration: 2108 lambda_k: 1 Loss: 0.002888464484444992\n",
      "Iteration: 2109 lambda_k: 1 Loss: 0.0028886668917303578\n",
      "Iteration: 2110 lambda_k: 1 Loss: 0.0028888682396110213\n",
      "Iteration: 2111 lambda_k: 1 Loss: 0.0028890685355480623\n",
      "Iteration: 2112 lambda_k: 1 Loss: 0.0028892677868992654\n",
      "Iteration: 2113 lambda_k: 1 Loss: 0.0028894660009214537\n",
      "Iteration: 2114 lambda_k: 1 Loss: 0.0028896631847727386\n",
      "Iteration: 2115 lambda_k: 1 Loss: 0.0028898593455148006\n",
      "Iteration: 2116 lambda_k: 1 Loss: 0.0028900544901150544\n",
      "Iteration: 2117 lambda_k: 1 Loss: 0.002890248625448731\n",
      "Iteration: 2118 lambda_k: 1 Loss: 0.0028904417583009915\n",
      "Iteration: 2119 lambda_k: 1 Loss: 0.0028906338953688902\n",
      "Iteration: 2120 lambda_k: 1 Loss: 0.002890825043263344\n",
      "Iteration: 2121 lambda_k: 1 Loss: 0.002891015208511049\n",
      "Iteration: 2122 lambda_k: 1 Loss: 0.0028912043975562953\n",
      "Iteration: 2123 lambda_k: 1 Loss: 0.002891392616762843\n",
      "Iteration: 2124 lambda_k: 1 Loss: 0.00289157987241564\n",
      "Iteration: 2125 lambda_k: 1 Loss: 0.0028917661707225377\n",
      "Iteration: 2126 lambda_k: 1 Loss: 0.002891951517815955\n",
      "Iteration: 2127 lambda_k: 1 Loss: 0.0028921359197545313\n",
      "Iteration: 2128 lambda_k: 1 Loss: 0.0028923193825246924\n",
      "Iteration: 2129 lambda_k: 1 Loss: 0.0028925019120422425\n",
      "Iteration: 2130 lambda_k: 1 Loss: 0.0028926835141537717\n",
      "Iteration: 2131 lambda_k: 1 Loss: 0.0028928641946382003\n",
      "Iteration: 2132 lambda_k: 1 Loss: 0.0028930439592082073\n",
      "Iteration: 2133 lambda_k: 1 Loss: 0.002893222813511602\n",
      "Iteration: 2134 lambda_k: 1 Loss: 0.002893400763132648\n",
      "Iteration: 2135 lambda_k: 1 Loss: 0.0028935778135934574\n",
      "Iteration: 2136 lambda_k: 1 Loss: 0.002893753970355229\n",
      "Iteration: 2137 lambda_k: 1 Loss: 0.002893929238819544\n",
      "Iteration: 2138 lambda_k: 1 Loss: 0.0028941036243295038\n",
      "Iteration: 2139 lambda_k: 1 Loss: 0.0028942771321709957\n",
      "Iteration: 2140 lambda_k: 1 Loss: 0.002894449767573859\n",
      "Iteration: 2141 lambda_k: 1 Loss: 0.002894621535712962\n",
      "Iteration: 2142 lambda_k: 1 Loss: 0.0028947924417093624\n",
      "Iteration: 2143 lambda_k: 1 Loss: 0.002894962490631349\n",
      "Iteration: 2144 lambda_k: 1 Loss: 0.0028951316874954694\n",
      "Iteration: 2145 lambda_k: 1 Loss: 0.0028953000372675857\n",
      "Iteration: 2146 lambda_k: 1 Loss: 0.002895467544863897\n",
      "Iteration: 2147 lambda_k: 1 Loss: 0.002895634215151803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2148 lambda_k: 1 Loss: 0.002895800052950956\n",
      "Iteration: 2149 lambda_k: 1 Loss: 0.0028959650630341486\n",
      "Iteration: 2150 lambda_k: 1 Loss: 0.0028961292501281885\n",
      "Iteration: 2151 lambda_k: 1 Loss: 0.0028962926189147887\n",
      "Iteration: 2152 lambda_k: 1 Loss: 0.002896455174031428\n",
      "Iteration: 2153 lambda_k: 1 Loss: 0.0028966169200721633\n",
      "Iteration: 2154 lambda_k: 1 Loss: 0.0028967778615884643\n",
      "Iteration: 2155 lambda_k: 1 Loss: 0.002896938003090003\n",
      "Iteration: 2156 lambda_k: 1 Loss: 0.0028970973490453837\n",
      "Iteration: 2157 lambda_k: 1 Loss: 0.002897255903882919\n",
      "Iteration: 2158 lambda_k: 1 Loss: 0.0028974136719914024\n",
      "Iteration: 2159 lambda_k: 1 Loss: 0.0028975706577207654\n",
      "Iteration: 2160 lambda_k: 1 Loss: 0.0028977268653828043\n",
      "Iteration: 2161 lambda_k: 1 Loss: 0.002897882299251802\n",
      "Iteration: 2162 lambda_k: 1 Loss: 0.0028980369635652876\n",
      "Iteration: 2163 lambda_k: 1 Loss: 0.0028981908625246365\n",
      "Iteration: 2164 lambda_k: 1 Loss: 0.0028983440002956643\n",
      "Iteration: 2165 lambda_k: 1 Loss: 0.0028984963810092692\n",
      "Iteration: 2166 lambda_k: 1 Loss: 0.002898648008762039\n",
      "Iteration: 2167 lambda_k: 1 Loss: 0.0028987988876168417\n",
      "Iteration: 2168 lambda_k: 1 Loss: 0.002898949021603363\n",
      "Iteration: 2169 lambda_k: 1 Loss: 0.0028990984147187035\n",
      "Iteration: 2170 lambda_k: 1 Loss: 0.00289924707092787\n",
      "Iteration: 2171 lambda_k: 1 Loss: 0.0028993949941643485\n",
      "Iteration: 2172 lambda_k: 1 Loss: 0.0028995421883306234\n",
      "Iteration: 2173 lambda_k: 1 Loss: 0.00289968865729863\n",
      "Iteration: 2174 lambda_k: 1 Loss: 0.0028998344049103023\n",
      "Iteration: 2175 lambda_k: 1 Loss: 0.0028999794349779786\n",
      "Iteration: 2176 lambda_k: 1 Loss: 0.0029001237512849844\n",
      "Iteration: 2177 lambda_k: 1 Loss: 0.0029002673575859923\n",
      "Iteration: 2178 lambda_k: 1 Loss: 0.0029004102576074857\n",
      "Iteration: 2179 lambda_k: 1 Loss: 0.0029005524550482268\n",
      "Iteration: 2180 lambda_k: 1 Loss: 0.002900693953579665\n",
      "Iteration: 2181 lambda_k: 1 Loss: 0.0029008347568463123\n",
      "Iteration: 2182 lambda_k: 1 Loss: 0.002900974868466197\n",
      "Iteration: 2183 lambda_k: 1 Loss: 0.0029011142920312735\n",
      "Iteration: 2184 lambda_k: 1 Loss: 0.002901253031107729\n",
      "Iteration: 2185 lambda_k: 1 Loss: 0.0029013910892364183\n",
      "Iteration: 2186 lambda_k: 1 Loss: 0.00290152846993325\n",
      "Iteration: 2187 lambda_k: 1 Loss: 0.0029016651766894893\n",
      "Iteration: 2188 lambda_k: 1 Loss: 0.0029018012129721255\n",
      "Iteration: 2189 lambda_k: 1 Loss: 0.0029019365822242557\n",
      "Iteration: 2190 lambda_k: 1 Loss: 0.002902071287865349\n",
      "Iteration: 2191 lambda_k: 1 Loss: 0.002902205333291654\n",
      "Iteration: 2192 lambda_k: 1 Loss: 0.0029023387218764206\n",
      "Iteration: 2193 lambda_k: 1 Loss: 0.0029024714569702978\n",
      "Iteration: 2194 lambda_k: 1 Loss: 0.002902603541901606\n",
      "Iteration: 2195 lambda_k: 1 Loss: 0.0029027349799766212\n",
      "Iteration: 2196 lambda_k: 1 Loss: 0.002902865774479883\n",
      "Iteration: 2197 lambda_k: 1 Loss: 0.0029029959286744696\n",
      "Iteration: 2198 lambda_k: 1 Loss: 0.002903125445802272\n",
      "Iteration: 2199 lambda_k: 1 Loss: 0.002903254329084297\n",
      "Iteration: 2200 lambda_k: 1 Loss: 0.002903382581720888\n",
      "Iteration: 2201 lambda_k: 1 Loss: 0.0029035102068920055\n",
      "Iteration: 2202 lambda_k: 1 Loss: 0.00290363720775746\n",
      "Iteration: 2203 lambda_k: 1 Loss: 0.00290376358745721\n",
      "Iteration: 2204 lambda_k: 1 Loss: 0.0029038893491115676\n",
      "Iteration: 2205 lambda_k: 1 Loss: 0.0029040144958214172\n",
      "Iteration: 2206 lambda_k: 1 Loss: 0.0029041390306684843\n",
      "Iteration: 2207 lambda_k: 1 Loss: 0.0029042629567155474\n",
      "Iteration: 2208 lambda_k: 1 Loss: 0.0029043862770066543\n",
      "Iteration: 2209 lambda_k: 1 Loss: 0.0029045089945673613\n",
      "Iteration: 2210 lambda_k: 1 Loss: 0.0029046311124049018\n",
      "Iteration: 2211 lambda_k: 1 Loss: 0.0029047526335084406\n",
      "Iteration: 2212 lambda_k: 1 Loss: 0.002904873560849251\n",
      "Iteration: 2213 lambda_k: 1 Loss: 0.002904993897380933\n",
      "Iteration: 2214 lambda_k: 1 Loss: 0.002905113646039565\n",
      "Iteration: 2215 lambda_k: 1 Loss: 0.002905232809743944\n",
      "Iteration: 2216 lambda_k: 1 Loss: 0.002905351391395754\n",
      "Iteration: 2217 lambda_k: 1 Loss: 0.0029054693938797426\n",
      "Iteration: 2218 lambda_k: 1 Loss: 0.002905586820063915\n",
      "Iteration: 2219 lambda_k: 1 Loss: 0.0029057036727996714\n",
      "Iteration: 2220 lambda_k: 1 Loss: 0.0029058199549220187\n",
      "Iteration: 2221 lambda_k: 1 Loss: 0.0029059356692497217\n",
      "Iteration: 2222 lambda_k: 1 Loss: 0.0029060508185854944\n",
      "Iteration: 2223 lambda_k: 1 Loss: 0.0029061654057161157\n",
      "Iteration: 2224 lambda_k: 1 Loss: 0.0029062794334126004\n",
      "Iteration: 2225 lambda_k: 1 Loss: 0.0029063929044303833\n",
      "Iteration: 2226 lambda_k: 1 Loss: 0.002906505821509445\n",
      "Iteration: 2227 lambda_k: 1 Loss: 0.0029066181873744715\n",
      "Iteration: 2228 lambda_k: 1 Loss: 0.002906730004734983\n",
      "Iteration: 2229 lambda_k: 1 Loss: 0.002906841276285503\n",
      "Iteration: 2230 lambda_k: 1 Loss: 0.002906952004705679\n",
      "Iteration: 2231 lambda_k: 1 Loss: 0.0029070621926604243\n",
      "Iteration: 2232 lambda_k: 1 Loss: 0.002907171842800065\n",
      "Iteration: 2233 lambda_k: 1 Loss: 0.0029072809577604286\n",
      "Iteration: 2234 lambda_k: 1 Loss: 0.002907389540163035\n",
      "Iteration: 2235 lambda_k: 1 Loss: 0.002907497592615223\n",
      "Iteration: 2236 lambda_k: 1 Loss: 0.002907605117710194\n",
      "Iteration: 2237 lambda_k: 1 Loss: 0.0029077121180272137\n",
      "Iteration: 2238 lambda_k: 1 Loss: 0.0029078185961317012\n",
      "Iteration: 2239 lambda_k: 1 Loss: 0.002907924554575397\n",
      "Iteration: 2240 lambda_k: 1 Loss: 0.002908029995896369\n",
      "Iteration: 2241 lambda_k: 1 Loss: 0.0029081349226192468\n",
      "Iteration: 2242 lambda_k: 1 Loss: 0.0029082393372552445\n",
      "Iteration: 2243 lambda_k: 1 Loss: 0.0029083432423023334\n",
      "Iteration: 2244 lambda_k: 1 Loss: 0.002908446640245323\n",
      "Iteration: 2245 lambda_k: 1 Loss: 0.0029085495335559406\n",
      "Iteration: 2246 lambda_k: 1 Loss: 0.0029086519246930196\n",
      "Iteration: 2247 lambda_k: 1 Loss: 0.0029087538161024734\n",
      "Iteration: 2248 lambda_k: 1 Loss: 0.0029088552102175275\n",
      "Iteration: 2249 lambda_k: 1 Loss: 0.00290895610945873\n",
      "Iteration: 2250 lambda_k: 1 Loss: 0.0029090565162340812\n",
      "Iteration: 2251 lambda_k: 1 Loss: 0.0029091564329391537\n",
      "Iteration: 2252 lambda_k: 1 Loss: 0.002909255861957139\n",
      "Iteration: 2253 lambda_k: 1 Loss: 0.0029093548056589634\n",
      "Iteration: 2254 lambda_k: 1 Loss: 0.0029094532664033826\n",
      "Iteration: 2255 lambda_k: 1 Loss: 0.002909551246537064\n",
      "Iteration: 2256 lambda_k: 1 Loss: 0.0029096487483946947\n",
      "Iteration: 2257 lambda_k: 1 Loss: 0.0029097457742990056\n",
      "Iteration: 2258 lambda_k: 1 Loss: 0.002909842326560951\n",
      "Iteration: 2259 lambda_k: 1 Loss: 0.0029099384074797526\n",
      "Iteration: 2260 lambda_k: 1 Loss: 0.0029100340193429185\n",
      "Iteration: 2261 lambda_k: 1 Loss: 0.002910129164426444\n",
      "Iteration: 2262 lambda_k: 1 Loss: 0.002910223844994776\n",
      "Iteration: 2263 lambda_k: 1 Loss: 0.002910318063300972\n",
      "Iteration: 2264 lambda_k: 1 Loss: 0.002910411821586783\n",
      "Iteration: 2265 lambda_k: 1 Loss: 0.002910505122082655\n",
      "Iteration: 2266 lambda_k: 1 Loss: 0.0029105979670078492\n",
      "Iteration: 2267 lambda_k: 1 Loss: 0.00291069035857055\n",
      "Iteration: 2268 lambda_k: 1 Loss: 0.002910782298967907\n",
      "Iteration: 2269 lambda_k: 1 Loss: 0.0029108737903860682\n",
      "Iteration: 2270 lambda_k: 1 Loss: 0.002910964835000332\n",
      "Iteration: 2271 lambda_k: 1 Loss: 0.002911055434975151\n",
      "Iteration: 2272 lambda_k: 1 Loss: 0.002911145592464241\n",
      "Iteration: 2273 lambda_k: 1 Loss: 0.0029112353096106417\n",
      "Iteration: 2274 lambda_k: 1 Loss: 0.002911324588546773\n",
      "Iteration: 2275 lambda_k: 1 Loss: 0.0029114134313945033\n",
      "Iteration: 2276 lambda_k: 1 Loss: 0.002911501840265223\n",
      "Iteration: 2277 lambda_k: 1 Loss: 0.002911589817259914\n",
      "Iteration: 2278 lambda_k: 1 Loss: 0.002911677364469208\n",
      "Iteration: 2279 lambda_k: 1 Loss: 0.0029117644839734506\n",
      "Iteration: 2280 lambda_k: 1 Loss: 0.0029118511778427536\n",
      "Iteration: 2281 lambda_k: 1 Loss: 0.0029119374481370846\n",
      "Iteration: 2282 lambda_k: 1 Loss: 0.002912023296906308\n",
      "Iteration: 2283 lambda_k: 1 Loss: 0.0029121087261902235\n",
      "Iteration: 2284 lambda_k: 1 Loss: 0.0029121937380187\n",
      "Iteration: 2285 lambda_k: 1 Loss: 0.0029122783344116103\n",
      "Iteration: 2286 lambda_k: 1 Loss: 0.002912362517379053\n",
      "Iteration: 2287 lambda_k: 1 Loss: 0.0029124462889212727\n",
      "Iteration: 2288 lambda_k: 1 Loss: 0.002912529651028775\n",
      "Iteration: 2289 lambda_k: 1 Loss: 0.0029126126056823807\n",
      "Iteration: 2290 lambda_k: 1 Loss: 0.0029126951548532672\n",
      "Iteration: 2291 lambda_k: 1 Loss: 0.0029127773005030357\n",
      "Iteration: 2292 lambda_k: 1 Loss: 0.002912859044583765\n",
      "Iteration: 2293 lambda_k: 1 Loss: 0.002912940389038068\n",
      "Iteration: 2294 lambda_k: 1 Loss: 0.0029130213357991155\n",
      "Iteration: 2295 lambda_k: 1 Loss: 0.002913101886790762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2296 lambda_k: 1 Loss: 0.0029131820439275186\n",
      "Iteration: 2297 lambda_k: 1 Loss: 0.0029132618091146224\n",
      "Iteration: 2298 lambda_k: 1 Loss: 0.0029133411842481214\n",
      "Iteration: 2299 lambda_k: 1 Loss: 0.002913420171214914\n",
      "Iteration: 2300 lambda_k: 1 Loss: 0.002913498771892783\n",
      "Iteration: 2301 lambda_k: 1 Loss: 0.0029135769881504293\n",
      "Iteration: 2302 lambda_k: 1 Loss: 0.002913654821847558\n",
      "Iteration: 2303 lambda_k: 1 Loss: 0.002913732274834929\n",
      "Iteration: 2304 lambda_k: 1 Loss: 0.0029138093489544\n",
      "Iteration: 2305 lambda_k: 1 Loss: 0.002913886046038916\n",
      "Iteration: 2306 lambda_k: 1 Loss: 0.002913962367912647\n",
      "Iteration: 2307 lambda_k: 1 Loss: 0.0029140383163909793\n",
      "Iteration: 2308 lambda_k: 1 Loss: 0.0029141138932805726\n",
      "Iteration: 2309 lambda_k: 1 Loss: 0.0029141891003794072\n",
      "Iteration: 2310 lambda_k: 1 Loss: 0.0029142639394768408\n",
      "Iteration: 2311 lambda_k: 1 Loss: 0.002914338412353652\n",
      "Iteration: 2312 lambda_k: 1 Loss: 0.002914412520782047\n",
      "Iteration: 2313 lambda_k: 1 Loss: 0.0029144862665257806\n",
      "Iteration: 2314 lambda_k: 1 Loss: 0.0029145596513400866\n",
      "Iteration: 2315 lambda_k: 1 Loss: 0.0029146326769718548\n",
      "Iteration: 2316 lambda_k: 1 Loss: 0.0029147053451595902\n",
      "Iteration: 2317 lambda_k: 1 Loss: 0.0029147776576334667\n",
      "Iteration: 2318 lambda_k: 1 Loss: 0.002914849616115379\n",
      "Iteration: 2319 lambda_k: 1 Loss: 0.0029149212223189597\n",
      "Iteration: 2320 lambda_k: 1 Loss: 0.0029149924779496854\n",
      "Iteration: 2321 lambda_k: 1 Loss: 0.0029150633847048763\n",
      "Iteration: 2322 lambda_k: 1 Loss: 0.0029151339442737138\n",
      "Iteration: 2323 lambda_k: 1 Loss: 0.0029152041583372992\n",
      "Iteration: 2324 lambda_k: 1 Loss: 0.002915274028568743\n",
      "Iteration: 2325 lambda_k: 1 Loss: 0.0029153435566331384\n",
      "Iteration: 2326 lambda_k: 1 Loss: 0.00291541274418763\n",
      "Iteration: 2327 lambda_k: 1 Loss: 0.0029154815928814403\n",
      "Iteration: 2328 lambda_k: 1 Loss: 0.002915550104355927\n",
      "Iteration: 2329 lambda_k: 1 Loss: 0.002915618280244626\n",
      "Iteration: 2330 lambda_k: 1 Loss: 0.0029156861221732583\n",
      "Iteration: 2331 lambda_k: 1 Loss: 0.0029157536317598004\n",
      "Iteration: 2332 lambda_k: 1 Loss: 0.0029158208106145065\n",
      "Iteration: 2333 lambda_k: 1 Loss: 0.002915887660339972\n",
      "Iteration: 2334 lambda_k: 1 Loss: 0.002915954182531116\n",
      "Iteration: 2335 lambda_k: 1 Loss: 0.0029160203787752807\n",
      "Iteration: 2336 lambda_k: 1 Loss: 0.002916086250652221\n",
      "Iteration: 2337 lambda_k: 1 Loss: 0.00291615179973416\n",
      "Iteration: 2338 lambda_k: 1 Loss: 0.002916217027585848\n",
      "Iteration: 2339 lambda_k: 1 Loss: 0.0029162819357645637\n",
      "Iteration: 2340 lambda_k: 1 Loss: 0.0029163465258201654\n",
      "Iteration: 2341 lambda_k: 1 Loss: 0.002916410799295136\n",
      "Iteration: 2342 lambda_k: 1 Loss: 0.002916474757724604\n",
      "Iteration: 2343 lambda_k: 1 Loss: 0.0029165384026363793\n",
      "Iteration: 2344 lambda_k: 1 Loss: 0.0029166017355509773\n",
      "Iteration: 2345 lambda_k: 1 Loss: 0.002916664757981699\n",
      "Iteration: 2346 lambda_k: 1 Loss: 0.0029167274714346297\n",
      "Iteration: 2347 lambda_k: 1 Loss: 0.002916789877408658\n",
      "Iteration: 2348 lambda_k: 1 Loss: 0.0029168519773955697\n",
      "Iteration: 2349 lambda_k: 1 Loss: 0.002916913772880028\n",
      "Iteration: 2350 lambda_k: 1 Loss: 0.0029169752653396114\n",
      "Iteration: 2351 lambda_k: 1 Loss: 0.002917036456244874\n",
      "Iteration: 2352 lambda_k: 1 Loss: 0.0029170973470593617\n",
      "Iteration: 2353 lambda_k: 1 Loss: 0.0029171579392396458\n",
      "Iteration: 2354 lambda_k: 1 Loss: 0.0029172182342353703\n",
      "Iteration: 2355 lambda_k: 1 Loss: 0.002917278233489283\n",
      "Iteration: 2356 lambda_k: 1 Loss: 0.0029173379384372345\n",
      "Iteration: 2357 lambda_k: 1 Loss: 0.0029173973505082357\n",
      "Iteration: 2358 lambda_k: 1 Loss: 0.0029174564711245245\n",
      "Iteration: 2359 lambda_k: 1 Loss: 0.0029175153017015317\n",
      "Iteration: 2360 lambda_k: 1 Loss: 0.0029175738436479502\n",
      "Iteration: 2361 lambda_k: 1 Loss: 0.0029176320983657672\n",
      "Iteration: 2362 lambda_k: 1 Loss: 0.0029176900672502884\n",
      "Iteration: 2363 lambda_k: 1 Loss: 0.0029177477516901815\n",
      "Iteration: 2364 lambda_k: 1 Loss: 0.0029178051530674904\n",
      "Iteration: 2365 lambda_k: 1 Loss: 0.0029178622727576682\n",
      "Iteration: 2366 lambda_k: 1 Loss: 0.0029179191121296083\n",
      "Iteration: 2367 lambda_k: 1 Loss: 0.0029179756725456903\n",
      "Iteration: 2368 lambda_k: 1 Loss: 0.002918031955361815\n",
      "Iteration: 2369 lambda_k: 1 Loss: 0.0029180879619273986\n",
      "Iteration: 2370 lambda_k: 1 Loss: 0.0029181436935854363\n",
      "Iteration: 2371 lambda_k: 1 Loss: 0.002918199151672512\n",
      "Iteration: 2372 lambda_k: 1 Loss: 0.002918254337518854\n",
      "Iteration: 2373 lambda_k: 1 Loss: 0.0029183092524483136\n",
      "Iteration: 2374 lambda_k: 1 Loss: 0.002918363897778474\n",
      "Iteration: 2375 lambda_k: 1 Loss: 0.0029184182748206123\n",
      "Iteration: 2376 lambda_k: 1 Loss: 0.0029184723848797527\n",
      "Iteration: 2377 lambda_k: 1 Loss: 0.0029185262292546798\n",
      "Iteration: 2378 lambda_k: 1 Loss: 0.0029185798092380144\n",
      "Iteration: 2379 lambda_k: 1 Loss: 0.002918633126116211\n",
      "Iteration: 2380 lambda_k: 1 Loss: 0.0029186861811695353\n",
      "Iteration: 2381 lambda_k: 1 Loss: 0.0029187389756722137\n",
      "Iteration: 2382 lambda_k: 1 Loss: 0.002918791510892335\n",
      "Iteration: 2383 lambda_k: 1 Loss: 0.002918843788091962\n",
      "Iteration: 2384 lambda_k: 1 Loss: 0.002918895808527129\n",
      "Iteration: 2385 lambda_k: 1 Loss: 0.0029189475734478815\n",
      "Iteration: 2386 lambda_k: 1 Loss: 0.002918999084098285\n",
      "Iteration: 2387 lambda_k: 1 Loss: 0.0029190503417164644\n",
      "Iteration: 2388 lambda_k: 1 Loss: 0.0029191013475346464\n",
      "Iteration: 2389 lambda_k: 1 Loss: 0.0029191521027791393\n",
      "Iteration: 2390 lambda_k: 1 Loss: 0.0029192026086704336\n",
      "Iteration: 2391 lambda_k: 1 Loss: 0.002919252866423169\n",
      "Iteration: 2392 lambda_k: 1 Loss: 0.0029193028772461728\n",
      "Iteration: 2393 lambda_k: 1 Loss: 0.0029193526423425095\n",
      "Iteration: 2394 lambda_k: 1 Loss: 0.0029194021629094934\n",
      "Iteration: 2395 lambda_k: 1 Loss: 0.0029194514401387115\n",
      "Iteration: 2396 lambda_k: 1 Loss: 0.002919500475216055\n",
      "Iteration: 2397 lambda_k: 1 Loss: 0.002919549269321741\n",
      "Iteration: 2398 lambda_k: 1 Loss: 0.002919597823630358\n",
      "Iteration: 2399 lambda_k: 1 Loss: 0.002919646139310845\n",
      "Iteration: 2400 lambda_k: 1 Loss: 0.002919694217526578\n",
      "Iteration: 2401 lambda_k: 1 Loss: 0.0029197420594353697\n",
      "Iteration: 2402 lambda_k: 1 Loss: 0.0029197896661894533\n",
      "Iteration: 2403 lambda_k: 1 Loss: 0.002919837038935599\n",
      "Iteration: 2404 lambda_k: 1 Loss: 0.0029198841788150537\n",
      "Iteration: 2405 lambda_k: 1 Loss: 0.0029199310869636227\n",
      "Iteration: 2406 lambda_k: 1 Loss: 0.0029199777645116443\n",
      "Iteration: 2407 lambda_k: 1 Loss: 0.0029200242125840886\n",
      "Iteration: 2408 lambda_k: 1 Loss: 0.002920070432300496\n",
      "Iteration: 2409 lambda_k: 1 Loss: 0.002920116424775058\n",
      "Iteration: 2410 lambda_k: 1 Loss: 0.002920162191116625\n",
      "Iteration: 2411 lambda_k: 1 Loss: 0.00292020773242875\n",
      "Iteration: 2412 lambda_k: 1 Loss: 0.002920253049809684\n",
      "Iteration: 2413 lambda_k: 1 Loss: 0.0029202981443524224\n",
      "Iteration: 2414 lambda_k: 1 Loss: 0.002920343017144718\n",
      "Iteration: 2415 lambda_k: 1 Loss: 0.0029203876692690937\n",
      "Iteration: 2416 lambda_k: 1 Loss: 0.0029204321018029126\n",
      "Iteration: 2417 lambda_k: 1 Loss: 0.002920476315818348\n",
      "Iteration: 2418 lambda_k: 1 Loss: 0.002920520312382416\n",
      "Iteration: 2419 lambda_k: 1 Loss: 0.002920564092557059\n",
      "Iteration: 2420 lambda_k: 1 Loss: 0.0029206076573990813\n",
      "Iteration: 2421 lambda_k: 1 Loss: 0.002920651007960246\n",
      "Iteration: 2422 lambda_k: 1 Loss: 0.0029206941452872602\n",
      "Iteration: 2423 lambda_k: 1 Loss: 0.002920737070421793\n",
      "Iteration: 2424 lambda_k: 1 Loss: 0.002920779784400515\n",
      "Iteration: 2425 lambda_k: 1 Loss: 0.002920822288255127\n",
      "Iteration: 2426 lambda_k: 1 Loss: 0.0029208645830123866\n",
      "Iteration: 2427 lambda_k: 1 Loss: 0.0029209066696940698\n",
      "Iteration: 2428 lambda_k: 1 Loss: 0.002920948549317121\n",
      "Iteration: 2429 lambda_k: 1 Loss: 0.0029209902228935236\n",
      "Iteration: 2430 lambda_k: 1 Loss: 0.002921031691430436\n",
      "Iteration: 2431 lambda_k: 1 Loss: 0.0029210729559301747\n",
      "Iteration: 2432 lambda_k: 1 Loss: 0.002921114017390228\n",
      "Iteration: 2433 lambda_k: 1 Loss: 0.0029211548768032984\n",
      "Iteration: 2434 lambda_k: 1 Loss: 0.0029211955351573114\n",
      "Iteration: 2435 lambda_k: 1 Loss: 0.0029212359934354336\n",
      "Iteration: 2436 lambda_k: 1 Loss: 0.0029212762526161147\n",
      "Iteration: 2437 lambda_k: 1 Loss: 0.0029213163136730864\n",
      "Iteration: 2438 lambda_k: 1 Loss: 0.0029213561775753975\n",
      "Iteration: 2439 lambda_k: 1 Loss: 0.0029213958452874145\n",
      "Iteration: 2440 lambda_k: 1 Loss: 0.0029214353177689054\n",
      "Iteration: 2441 lambda_k: 1 Loss: 0.0029214745959749927\n",
      "Iteration: 2442 lambda_k: 1 Loss: 0.0029215136808562036\n",
      "Iteration: 2443 lambda_k: 1 Loss: 0.002921552573358471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2444 lambda_k: 1 Loss: 0.0029215912744231867\n",
      "Iteration: 2445 lambda_k: 1 Loss: 0.0029216297849871927\n",
      "Iteration: 2446 lambda_k: 1 Loss: 0.0029216681059828524\n",
      "Iteration: 2447 lambda_k: 1 Loss: 0.00292170623833801\n",
      "Iteration: 2448 lambda_k: 1 Loss: 0.0029217441829760523\n",
      "Iteration: 2449 lambda_k: 1 Loss: 0.0029217819408159005\n",
      "Iteration: 2450 lambda_k: 1 Loss: 0.0029218195127720236\n",
      "Iteration: 2451 lambda_k: 1 Loss: 0.0029218568997545365\n",
      "Iteration: 2452 lambda_k: 1 Loss: 0.002921894102669123\n",
      "Iteration: 2453 lambda_k: 1 Loss: 0.0029219311224171155\n",
      "Iteration: 2454 lambda_k: 1 Loss: 0.002921967959895519\n",
      "Iteration: 2455 lambda_k: 1 Loss: 0.002922004615996947\n",
      "Iteration: 2456 lambda_k: 1 Loss: 0.0029220410916097595\n",
      "Iteration: 2457 lambda_k: 1 Loss: 0.0029220773876180266\n",
      "Iteration: 2458 lambda_k: 1 Loss: 0.0029221135049015325\n",
      "Iteration: 2459 lambda_k: 1 Loss: 0.002922149444335843\n",
      "Iteration: 2460 lambda_k: 1 Loss: 0.0029221852067922416\n",
      "Iteration: 2461 lambda_k: 1 Loss: 0.002922220793137842\n",
      "Iteration: 2462 lambda_k: 1 Loss: 0.0029222562042355645\n",
      "Iteration: 2463 lambda_k: 1 Loss: 0.0029222914409441593\n",
      "Iteration: 2464 lambda_k: 1 Loss: 0.0029223265041182366\n",
      "Iteration: 2465 lambda_k: 1 Loss: 0.0029223613946082797\n",
      "Iteration: 2466 lambda_k: 1 Loss: 0.002922396113260629\n",
      "Iteration: 2467 lambda_k: 1 Loss: 0.0029224306609175786\n",
      "Iteration: 2468 lambda_k: 1 Loss: 0.0029224650384172927\n",
      "Iteration: 2469 lambda_k: 1 Loss: 0.0029224992465939543\n",
      "Iteration: 2470 lambda_k: 1 Loss: 0.00292253328627765\n",
      "Iteration: 2471 lambda_k: 1 Loss: 0.0029225671582944895\n",
      "Iteration: 2472 lambda_k: 1 Loss: 0.002922600863466586\n",
      "Iteration: 2473 lambda_k: 1 Loss: 0.002922634402612051\n",
      "Iteration: 2474 lambda_k: 1 Loss: 0.0029226677765450504\n",
      "Iteration: 2475 lambda_k: 1 Loss: 0.0029227009860758137\n",
      "Iteration: 2476 lambda_k: 1 Loss: 0.002922734032010646\n",
      "Iteration: 2477 lambda_k: 1 Loss: 0.0029227669151519536\n",
      "Iteration: 2478 lambda_k: 1 Loss: 0.002922799636298249\n",
      "Iteration: 2479 lambda_k: 1 Loss: 0.0029228321962442043\n",
      "Iteration: 2480 lambda_k: 1 Loss: 0.0029228645957805883\n",
      "Iteration: 2481 lambda_k: 1 Loss: 0.0029228968356943717\n",
      "Iteration: 2482 lambda_k: 1 Loss: 0.0029229289167687343\n",
      "Iteration: 2483 lambda_k: 1 Loss: 0.002922960839783036\n",
      "Iteration: 2484 lambda_k: 1 Loss: 0.0029229926055128717\n",
      "Iteration: 2485 lambda_k: 1 Loss: 0.0029230242147300564\n",
      "Iteration: 2486 lambda_k: 1 Loss: 0.0029230556682026845\n",
      "Iteration: 2487 lambda_k: 1 Loss: 0.0029230869666951316\n",
      "Iteration: 2488 lambda_k: 1 Loss: 0.0029231181109680288\n",
      "Iteration: 2489 lambda_k: 1 Loss: 0.002923149101778396\n",
      "Iteration: 2490 lambda_k: 1 Loss: 0.002923179939879497\n",
      "Iteration: 2491 lambda_k: 1 Loss: 0.0029232106260209975\n",
      "Iteration: 2492 lambda_k: 1 Loss: 0.002923241160948884\n",
      "Iteration: 2493 lambda_k: 1 Loss: 0.002923271545405556\n",
      "Iteration: 2494 lambda_k: 1 Loss: 0.002923301780129799\n",
      "Iteration: 2495 lambda_k: 1 Loss: 0.0029233318658568267\n",
      "Iteration: 2496 lambda_k: 1 Loss: 0.0029233618033182632\n",
      "Iteration: 2497 lambda_k: 1 Loss: 0.002923391593242204\n",
      "Iteration: 2498 lambda_k: 1 Loss: 0.0029234212363532003\n",
      "Iteration: 2499 lambda_k: 1 Loss: 0.0029234507333722633\n",
      "Iteration: 2500 lambda_k: 1 Loss: 0.0029234800850169427\n",
      "Iteration: 2501 lambda_k: 1 Loss: 0.00292350929200126\n",
      "Iteration: 2502 lambda_k: 1 Loss: 0.002923538355035822\n",
      "Iteration: 2503 lambda_k: 1 Loss: 0.0029235672748277393\n",
      "Iteration: 2504 lambda_k: 1 Loss: 0.0029235960520807044\n",
      "Iteration: 2505 lambda_k: 1 Loss: 0.002923624687494966\n",
      "Iteration: 2506 lambda_k: 1 Loss: 0.0029236531817673985\n",
      "Iteration: 2507 lambda_k: 1 Loss: 0.0029236815355915\n",
      "Iteration: 2508 lambda_k: 1 Loss: 0.002923709749657376\n",
      "Iteration: 2509 lambda_k: 1 Loss: 0.002923737824651755\n",
      "Iteration: 2510 lambda_k: 1 Loss: 0.002923765761258043\n",
      "Iteration: 2511 lambda_k: 1 Loss: 0.0029237935601563606\n",
      "Iteration: 2512 lambda_k: 1 Loss: 0.0029238212220234643\n",
      "Iteration: 2513 lambda_k: 1 Loss: 0.002923848747532844\n",
      "Iteration: 2514 lambda_k: 1 Loss: 0.002923876137354751\n",
      "Iteration: 2515 lambda_k: 1 Loss: 0.0029239033921561173\n",
      "Iteration: 2516 lambda_k: 1 Loss: 0.0029239305126006455\n",
      "Iteration: 2517 lambda_k: 1 Loss: 0.002923957499348817\n",
      "Iteration: 2518 lambda_k: 1 Loss: 0.002923984353057898\n",
      "Iteration: 2519 lambda_k: 1 Loss: 0.0029240110743819357\n",
      "Iteration: 2520 lambda_k: 1 Loss: 0.0029240376639718215\n",
      "Iteration: 2521 lambda_k: 1 Loss: 0.0029240641224752653\n",
      "Iteration: 2522 lambda_k: 1 Loss: 0.0029240904505368295\n",
      "Iteration: 2523 lambda_k: 1 Loss: 0.002924116648797929\n",
      "Iteration: 2524 lambda_k: 1 Loss: 0.002924142717896858\n",
      "Iteration: 2525 lambda_k: 1 Loss: 0.0029241686584687994\n",
      "Iteration: 2526 lambda_k: 1 Loss: 0.0029241944711458404\n",
      "Iteration: 2527 lambda_k: 1 Loss: 0.0029242201565569874\n",
      "Iteration: 2528 lambda_k: 1 Loss: 0.0029242457153281897\n",
      "Iteration: 2529 lambda_k: 1 Loss: 0.002924271148082338\n",
      "Iteration: 2530 lambda_k: 1 Loss: 0.002924296455439304\n",
      "Iteration: 2531 lambda_k: 1 Loss: 0.0029243216380159184\n",
      "Iteration: 2532 lambda_k: 1 Loss: 0.002924346696426016\n",
      "Iteration: 2533 lambda_k: 1 Loss: 0.002924371631280432\n",
      "Iteration: 2534 lambda_k: 1 Loss: 0.00292439644318704\n",
      "Iteration: 2535 lambda_k: 1 Loss: 0.0029244211327507166\n",
      "Iteration: 2536 lambda_k: 1 Loss: 0.0029244457005733987\n",
      "Iteration: 2537 lambda_k: 1 Loss: 0.002924470147254117\n",
      "Iteration: 2538 lambda_k: 1 Loss: 0.002924494473388935\n",
      "Iteration: 2539 lambda_k: 1 Loss: 0.0029245186795710445\n",
      "Iteration: 2540 lambda_k: 1 Loss: 0.002924542766390752\n",
      "Iteration: 2541 lambda_k: 1 Loss: 0.0029245667344354174\n",
      "Iteration: 2542 lambda_k: 1 Loss: 0.0029245905842896084\n",
      "Iteration: 2543 lambda_k: 1 Loss: 0.002924614316534999\n",
      "Iteration: 2544 lambda_k: 1 Loss: 0.0029246379317504415\n",
      "Iteration: 2545 lambda_k: 1 Loss: 0.002924661430511935\n",
      "Iteration: 2546 lambda_k: 1 Loss: 0.0029246848133927103\n",
      "Iteration: 2547 lambda_k: 1 Loss: 0.0029247080809631696\n",
      "Iteration: 2548 lambda_k: 1 Loss: 0.002924731233790928\n",
      "Iteration: 2549 lambda_k: 1 Loss: 0.002924754272440844\n",
      "Iteration: 2550 lambda_k: 1 Loss: 0.002924777197475002\n",
      "Iteration: 2551 lambda_k: 1 Loss: 0.0029248000094527537\n",
      "Iteration: 2552 lambda_k: 1 Loss: 0.0029248227089307185\n",
      "Iteration: 2553 lambda_k: 1 Loss: 0.0029248452964627785\n",
      "Iteration: 2554 lambda_k: 1 Loss: 0.0029248677726001196\n",
      "Iteration: 2555 lambda_k: 1 Loss: 0.0029248901378912534\n",
      "Iteration: 2556 lambda_k: 1 Loss: 0.002924912392881974\n",
      "Iteration: 2557 lambda_k: 1 Loss: 0.0029249345381154417\n",
      "Iteration: 2558 lambda_k: 1 Loss: 0.002924956574132135\n",
      "Iteration: 2559 lambda_k: 1 Loss: 0.002924978501469909\n",
      "Iteration: 2560 lambda_k: 1 Loss: 0.0029250003206639933\n",
      "Iteration: 2561 lambda_k: 1 Loss: 0.002925022032246974\n",
      "Iteration: 2562 lambda_k: 1 Loss: 0.0029250436367488394\n",
      "Iteration: 2563 lambda_k: 1 Loss: 0.002925065134697013\n",
      "Iteration: 2564 lambda_k: 1 Loss: 0.002925086526616312\n",
      "Iteration: 2565 lambda_k: 1 Loss: 0.002925107813029024\n",
      "Iteration: 2566 lambda_k: 1 Loss: 0.002925128994454809\n",
      "Iteration: 2567 lambda_k: 1 Loss: 0.002925150071410856\n",
      "Iteration: 2568 lambda_k: 1 Loss: 0.0029251710444118005\n",
      "Iteration: 2569 lambda_k: 1 Loss: 0.002925191913969739\n",
      "Iteration: 2570 lambda_k: 1 Loss: 0.0029252126805942966\n",
      "Iteration: 2571 lambda_k: 1 Loss: 0.0029252333447925744\n",
      "Iteration: 2572 lambda_k: 1 Loss: 0.0029252539070692145\n",
      "Iteration: 2573 lambda_k: 1 Loss: 0.002925274367926373\n",
      "Iteration: 2574 lambda_k: 1 Loss: 0.0029252947278637347\n",
      "Iteration: 2575 lambda_k: 1 Loss: 0.0029253149873785973\n",
      "Iteration: 2576 lambda_k: 1 Loss: 0.0029253351469657515\n",
      "Iteration: 2577 lambda_k: 1 Loss: 0.002925355207117607\n",
      "Iteration: 2578 lambda_k: 1 Loss: 0.00292537516832416\n",
      "Iteration: 2579 lambda_k: 1 Loss: 0.0029253950310729935\n",
      "Iteration: 2580 lambda_k: 1 Loss: 0.0029254147958492995\n",
      "Iteration: 2581 lambda_k: 1 Loss: 0.0029254344631358966\n",
      "Iteration: 2582 lambda_k: 1 Loss: 0.002925454033413255\n",
      "Iteration: 2583 lambda_k: 1 Loss: 0.0029254735071594777\n",
      "Iteration: 2584 lambda_k: 1 Loss: 0.0029254928848503345\n",
      "Iteration: 2585 lambda_k: 1 Loss: 0.002925512166959249\n",
      "Iteration: 2586 lambda_k: 1 Loss: 0.0029255313539573407\n",
      "Iteration: 2587 lambda_k: 1 Loss: 0.0029255504463134114\n",
      "Iteration: 2588 lambda_k: 1 Loss: 0.002925569444493947\n",
      "Iteration: 2589 lambda_k: 1 Loss: 0.002925588348963193\n",
      "Iteration: 2590 lambda_k: 1 Loss: 0.002925607160183073\n",
      "Iteration: 2591 lambda_k: 1 Loss: 0.002925625878613267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2592 lambda_k: 1 Loss: 0.002925644504711217\n",
      "Iteration: 2593 lambda_k: 1 Loss: 0.0029256630389320904\n",
      "Iteration: 2594 lambda_k: 1 Loss: 0.0029256814817288563\n",
      "Iteration: 2595 lambda_k: 1 Loss: 0.002925699833552218\n",
      "Iteration: 2596 lambda_k: 1 Loss: 0.0029257180948507296\n",
      "Iteration: 2597 lambda_k: 1 Loss: 0.002925736266070678\n",
      "Iteration: 2598 lambda_k: 1 Loss: 0.002925754347656215\n",
      "Iteration: 2599 lambda_k: 1 Loss: 0.002925772340049274\n",
      "Iteration: 2600 lambda_k: 1 Loss: 0.0029257902436896598\n",
      "Iteration: 2601 lambda_k: 1 Loss: 0.002925808059014985\n",
      "Iteration: 2602 lambda_k: 1 Loss: 0.0029258257864607506\n",
      "Iteration: 2603 lambda_k: 1 Loss: 0.0029258434264602625\n",
      "Iteration: 2604 lambda_k: 1 Loss: 0.0029258609794447538\n",
      "Iteration: 2605 lambda_k: 1 Loss: 0.002925878445843336\n",
      "Iteration: 2606 lambda_k: 1 Loss: 0.0029258958260829897\n",
      "Iteration: 2607 lambda_k: 1 Loss: 0.0029259131205886272\n",
      "Iteration: 2608 lambda_k: 1 Loss: 0.0029259303297830396\n",
      "Iteration: 2609 lambda_k: 1 Loss: 0.0029259474540869967\n",
      "Iteration: 2610 lambda_k: 1 Loss: 0.00292596449391915\n",
      "Iteration: 2611 lambda_k: 1 Loss: 0.0029259814496961157\n",
      "Iteration: 2612 lambda_k: 1 Loss: 0.0029259983218324816\n",
      "Iteration: 2613 lambda_k: 1 Loss: 0.002926015110740775\n",
      "Iteration: 2614 lambda_k: 1 Loss: 0.0029260318168314917\n",
      "Iteration: 2615 lambda_k: 1 Loss: 0.002926048440513153\n",
      "Iteration: 2616 lambda_k: 1 Loss: 0.0029260649821922294\n",
      "Iteration: 2617 lambda_k: 1 Loss: 0.0029260814422732125\n",
      "Iteration: 2618 lambda_k: 1 Loss: 0.0029260978211586206\n",
      "Iteration: 2619 lambda_k: 1 Loss: 0.0029261141192489805\n",
      "Iteration: 2620 lambda_k: 1 Loss: 0.002926130336942842\n",
      "Iteration: 2621 lambda_k: 1 Loss: 0.0029261464746368113\n",
      "Iteration: 2622 lambda_k: 1 Loss: 0.0029261625327255507\n",
      "Iteration: 2623 lambda_k: 1 Loss: 0.0029261785116017716\n",
      "Iteration: 2624 lambda_k: 1 Loss: 0.0029261944116562534\n",
      "Iteration: 2625 lambda_k: 1 Loss: 0.0029262102332778712\n",
      "Iteration: 2626 lambda_k: 1 Loss: 0.0029262259768535873\n",
      "Iteration: 2627 lambda_k: 1 Loss: 0.0029262416427684387\n",
      "Iteration: 2628 lambda_k: 1 Loss: 0.0029262572314056007\n",
      "Iteration: 2629 lambda_k: 1 Loss: 0.0029262727431463367\n",
      "Iteration: 2630 lambda_k: 1 Loss: 0.002926288178370061\n",
      "Iteration: 2631 lambda_k: 1 Loss: 0.002926303537454312\n",
      "Iteration: 2632 lambda_k: 1 Loss: 0.002926318820774778\n",
      "Iteration: 2633 lambda_k: 1 Loss: 0.0029263340287052635\n",
      "Iteration: 2634 lambda_k: 1 Loss: 0.0029263491616178007\n",
      "Iteration: 2635 lambda_k: 1 Loss: 0.0029263642198825313\n",
      "Iteration: 2636 lambda_k: 1 Loss: 0.0029263792038678373\n",
      "Iteration: 2637 lambda_k: 1 Loss: 0.002926394113940228\n",
      "Iteration: 2638 lambda_k: 1 Loss: 0.0029264089504644508\n",
      "Iteration: 2639 lambda_k: 1 Loss: 0.002926423713803446\n",
      "Iteration: 2640 lambda_k: 1 Loss: 0.002926438404318376\n",
      "Iteration: 2641 lambda_k: 1 Loss: 0.0029264530223685956\n",
      "Iteration: 2642 lambda_k: 1 Loss: 0.0029264675683117257\n",
      "Iteration: 2643 lambda_k: 1 Loss: 0.0029264820425036514\n",
      "Iteration: 2644 lambda_k: 1 Loss: 0.0029264964452984533\n",
      "Iteration: 2645 lambda_k: 1 Loss: 0.002926510777048486\n",
      "Iteration: 2646 lambda_k: 1 Loss: 0.0029265250381043753\n",
      "Iteration: 2647 lambda_k: 1 Loss: 0.0029265392288150265\n",
      "Iteration: 2648 lambda_k: 1 Loss: 0.0029265533495276244\n",
      "Iteration: 2649 lambda_k: 1 Loss: 0.002926567400587662\n",
      "Iteration: 2650 lambda_k: 1 Loss: 0.0029265813823388972\n",
      "Iteration: 2651 lambda_k: 1 Loss: 0.0029265952951234163\n",
      "Iteration: 2652 lambda_k: 1 Loss: 0.002926609139281636\n",
      "Iteration: 2653 lambda_k: 1 Loss: 0.0029266229151522737\n",
      "Iteration: 2654 lambda_k: 1 Loss: 0.002926636623072382\n",
      "Iteration: 2655 lambda_k: 1 Loss: 0.0029266502633773753\n",
      "Iteration: 2656 lambda_k: 1 Loss: 0.0029266638364009822\n",
      "Iteration: 2657 lambda_k: 1 Loss: 0.0029266773424753206\n",
      "Iteration: 2658 lambda_k: 1 Loss: 0.0029266907819308742\n",
      "Iteration: 2659 lambda_k: 1 Loss: 0.0029267041550964734\n",
      "Iteration: 2660 lambda_k: 1 Loss: 0.002926717462299348\n",
      "Iteration: 2661 lambda_k: 1 Loss: 0.0029267307038651127\n",
      "Iteration: 2662 lambda_k: 1 Loss: 0.0029267438801177754\n",
      "Iteration: 2663 lambda_k: 1 Loss: 0.0029267569913797487\n",
      "Iteration: 2664 lambda_k: 1 Loss: 0.0029267700379718726\n",
      "Iteration: 2665 lambda_k: 1 Loss: 0.0029267830202133888\n",
      "Iteration: 2666 lambda_k: 1 Loss: 0.0029267959384219697\n",
      "Iteration: 2667 lambda_k: 1 Loss: 0.002926808792913726\n",
      "Iteration: 2668 lambda_k: 1 Loss: 0.0029268215840032165\n",
      "Iteration: 2669 lambda_k: 1 Loss: 0.0029268343120034744\n",
      "Iteration: 2670 lambda_k: 1 Loss: 0.002926846977225928\n",
      "Iteration: 2671 lambda_k: 1 Loss: 0.0029268595799805024\n",
      "Iteration: 2672 lambda_k: 1 Loss: 0.002926872120575634\n",
      "Iteration: 2673 lambda_k: 1 Loss: 0.002926884599318186\n",
      "Iteration: 2674 lambda_k: 1 Loss: 0.002926897016513516\n",
      "Iteration: 2675 lambda_k: 1 Loss: 0.0029269093724655034\n",
      "Iteration: 2676 lambda_k: 1 Loss: 0.00292692166747653\n",
      "Iteration: 2677 lambda_k: 1 Loss: 0.0029269339018474605\n",
      "Iteration: 2678 lambda_k: 1 Loss: 0.002926946075877694\n",
      "Iteration: 2679 lambda_k: 1 Loss: 0.002926958189865164\n",
      "Iteration: 2680 lambda_k: 1 Loss: 0.002926970244106319\n",
      "Iteration: 2681 lambda_k: 1 Loss: 0.0029269822388961427\n",
      "Iteration: 2682 lambda_k: 1 Loss: 0.0029269941745281945\n",
      "Iteration: 2683 lambda_k: 1 Loss: 0.0029270060512945613\n",
      "Iteration: 2684 lambda_k: 1 Loss: 0.0029270178694858844\n",
      "Iteration: 2685 lambda_k: 1 Loss: 0.0029270296293913978\n",
      "Iteration: 2686 lambda_k: 1 Loss: 0.0029270413312988924\n",
      "Iteration: 2687 lambda_k: 1 Loss: 0.002927052975494735\n",
      "Iteration: 2688 lambda_k: 1 Loss: 0.0029270645622638965\n",
      "Iteration: 2689 lambda_k: 1 Loss: 0.0029270760918899633\n",
      "Iteration: 2690 lambda_k: 1 Loss: 0.002927087564655074\n",
      "Iteration: 2691 lambda_k: 1 Loss: 0.0029270989808399916\n",
      "Iteration: 2692 lambda_k: 1 Loss: 0.002927110340724129\n",
      "Iteration: 2693 lambda_k: 1 Loss: 0.0029271216445854852\n",
      "Iteration: 2694 lambda_k: 1 Loss: 0.0029271328927006875\n",
      "Iteration: 2695 lambda_k: 1 Loss: 0.002927144085345024\n",
      "Iteration: 2696 lambda_k: 1 Loss: 0.0029271552227924016\n",
      "Iteration: 2697 lambda_k: 1 Loss: 0.0029271663053154168\n",
      "Iteration: 2698 lambda_k: 1 Loss: 0.002927177333185258\n",
      "Iteration: 2699 lambda_k: 1 Loss: 0.002927188306671823\n",
      "Iteration: 2700 lambda_k: 1 Loss: 0.0029271992260436605\n",
      "Iteration: 2701 lambda_k: 1 Loss: 0.0029272100915680053\n",
      "Iteration: 2702 lambda_k: 1 Loss: 0.0029272209035107865\n",
      "Iteration: 2703 lambda_k: 1 Loss: 0.002927231662136603\n",
      "Iteration: 2704 lambda_k: 1 Loss: 0.00292724236770873\n",
      "Iteration: 2705 lambda_k: 1 Loss: 0.0029272530204891753\n",
      "Iteration: 2706 lambda_k: 1 Loss: 0.0029272636207386257\n",
      "Iteration: 2707 lambda_k: 1 Loss: 0.0029272741687165257\n",
      "Iteration: 2708 lambda_k: 1 Loss: 0.0029272846646810242\n",
      "Iteration: 2709 lambda_k: 1 Loss: 0.0029272951088889472\n",
      "Iteration: 2710 lambda_k: 1 Loss: 0.0029273055015959244\n",
      "Iteration: 2711 lambda_k: 1 Loss: 0.002927315843056275\n",
      "Iteration: 2712 lambda_k: 1 Loss: 0.0029273261335230898\n",
      "Iteration: 2713 lambda_k: 1 Loss: 0.002927336373248216\n",
      "Iteration: 2714 lambda_k: 1 Loss: 0.0029273465624822296\n",
      "Iteration: 2715 lambda_k: 1 Loss: 0.0029273567014744956\n",
      "Iteration: 2716 lambda_k: 1 Loss: 0.0029273667904731356\n",
      "Iteration: 2717 lambda_k: 1 Loss: 0.0029273768297250462\n",
      "Iteration: 2718 lambda_k: 1 Loss: 0.002927386819475915\n",
      "Iteration: 2719 lambda_k: 1 Loss: 0.0029273967599702123\n",
      "Iteration: 2720 lambda_k: 1 Loss: 0.002927406651451189\n",
      "Iteration: 2721 lambda_k: 1 Loss: 0.0029274164941609413\n",
      "Iteration: 2722 lambda_k: 1 Loss: 0.0029274262883403138\n",
      "Iteration: 2723 lambda_k: 1 Loss: 0.0029274360342289914\n",
      "Iteration: 2724 lambda_k: 1 Loss: 0.002927445732065473\n",
      "Iteration: 2725 lambda_k: 1 Loss: 0.0029274553820870576\n",
      "Iteration: 2726 lambda_k: 1 Loss: 0.00292746498452991\n",
      "Iteration: 2727 lambda_k: 1 Loss: 0.0029274745396290227\n",
      "Iteration: 2728 lambda_k: 1 Loss: 0.002927484047618217\n",
      "Iteration: 2729 lambda_k: 1 Loss: 0.002927493508730145\n",
      "Iteration: 2730 lambda_k: 1 Loss: 0.002927502923196343\n",
      "Iteration: 2731 lambda_k: 1 Loss: 0.0029275122912471754\n",
      "Iteration: 2732 lambda_k: 1 Loss: 0.0029275216131118944\n",
      "Iteration: 2733 lambda_k: 1 Loss: 0.0029275308890186237\n",
      "Iteration: 2734 lambda_k: 1 Loss: 0.002927540119194319\n",
      "Iteration: 2735 lambda_k: 1 Loss: 0.0029275493038648597\n",
      "Iteration: 2736 lambda_k: 1 Loss: 0.0029275584432549924\n",
      "Iteration: 2737 lambda_k: 1 Loss: 0.00292756753758834\n",
      "Iteration: 2738 lambda_k: 1 Loss: 0.0029275765870874428\n",
      "Iteration: 2739 lambda_k: 1 Loss: 0.0029275855919737447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2740 lambda_k: 1 Loss: 0.00292759455246758\n",
      "Iteration: 2741 lambda_k: 1 Loss: 0.0029276034687882174\n",
      "Iteration: 2742 lambda_k: 1 Loss: 0.002927612341153805\n",
      "Iteration: 2743 lambda_k: 1 Loss: 0.0029276211697814502\n",
      "Iteration: 2744 lambda_k: 1 Loss: 0.0029276299548871673\n",
      "Iteration: 2745 lambda_k: 1 Loss: 0.0029276386966859114\n",
      "Iteration: 2746 lambda_k: 1 Loss: 0.002927647395391588\n",
      "Iteration: 2747 lambda_k: 1 Loss: 0.0029276560512170257\n",
      "Iteration: 2748 lambda_k: 1 Loss: 0.0029276646643740167\n",
      "Iteration: 2749 lambda_k: 1 Loss: 0.0029276732350733093\n",
      "Iteration: 2750 lambda_k: 1 Loss: 0.0029276817635245735\n",
      "Iteration: 2751 lambda_k: 1 Loss: 0.00292769024993649\n",
      "Iteration: 2752 lambda_k: 1 Loss: 0.002927698694516699\n",
      "Iteration: 2753 lambda_k: 1 Loss: 0.002927707097471801\n",
      "Iteration: 2754 lambda_k: 1 Loss: 0.0029277154590073877\n",
      "Iteration: 2755 lambda_k: 1 Loss: 0.0029277237793280163\n",
      "Iteration: 2756 lambda_k: 1 Loss: 0.0029277320586372753\n",
      "Iteration: 2757 lambda_k: 1 Loss: 0.0029277402971376954\n",
      "Iteration: 2758 lambda_k: 1 Loss: 0.0029277484950308587\n",
      "Iteration: 2759 lambda_k: 1 Loss: 0.002927756652517291\n",
      "Iteration: 2760 lambda_k: 1 Loss: 0.0029277647697965965\n",
      "Iteration: 2761 lambda_k: 1 Loss: 0.002927772847067353\n",
      "Iteration: 2762 lambda_k: 1 Loss: 0.002927780884527137\n",
      "Iteration: 2763 lambda_k: 1 Loss: 0.00292778888237259\n",
      "Iteration: 2764 lambda_k: 1 Loss: 0.0029277968407993796\n",
      "Iteration: 2765 lambda_k: 1 Loss: 0.0029278047600021836\n",
      "Iteration: 2766 lambda_k: 1 Loss: 0.0029278126401747423\n",
      "Iteration: 2767 lambda_k: 1 Loss: 0.0029278204815098123\n",
      "Iteration: 2768 lambda_k: 1 Loss: 0.0029278282841991928\n",
      "Iteration: 2769 lambda_k: 1 Loss: 0.0029278360484337883\n",
      "Iteration: 2770 lambda_k: 1 Loss: 0.002927843774403508\n",
      "Iteration: 2771 lambda_k: 1 Loss: 0.0029278514622973414\n",
      "Iteration: 2772 lambda_k: 1 Loss: 0.0029278591123033408\n",
      "Iteration: 2773 lambda_k: 1 Loss: 0.002927866724608651\n",
      "Iteration: 2774 lambda_k: 1 Loss: 0.0029278742993994566\n",
      "Iteration: 2775 lambda_k: 1 Loss: 0.0029278818368610503\n",
      "Iteration: 2776 lambda_k: 1 Loss: 0.002927889337177775\n",
      "Iteration: 2777 lambda_k: 1 Loss: 0.0029278968005331005\n",
      "Iteration: 2778 lambda_k: 1 Loss: 0.0029279042271095784\n",
      "Iteration: 2779 lambda_k: 1 Loss: 0.0029279116170888558\n",
      "Iteration: 2780 lambda_k: 1 Loss: 0.002927918970651661\n",
      "Iteration: 2781 lambda_k: 1 Loss: 0.002927926287977862\n",
      "Iteration: 2782 lambda_k: 1 Loss: 0.0029279335692464366\n",
      "Iteration: 2783 lambda_k: 1 Loss: 0.002927940814635464\n",
      "Iteration: 2784 lambda_k: 1 Loss: 0.0029279480243221347\n",
      "Iteration: 2785 lambda_k: 1 Loss: 0.0029279551984827848\n",
      "Iteration: 2786 lambda_k: 1 Loss: 0.0029279623372928543\n",
      "Iteration: 2787 lambda_k: 1 Loss: 0.002927969440926952\n",
      "Iteration: 2788 lambda_k: 1 Loss: 0.002927976509558779\n",
      "Iteration: 2789 lambda_k: 1 Loss: 0.0029279835433612244\n",
      "Iteration: 2790 lambda_k: 1 Loss: 0.0029279905425062987\n",
      "Iteration: 2791 lambda_k: 1 Loss: 0.002927997507165177\n",
      "Iteration: 2792 lambda_k: 1 Loss: 0.0029280044375081245\n",
      "Iteration: 2793 lambda_k: 1 Loss: 0.0029280113337046476\n",
      "Iteration: 2794 lambda_k: 1 Loss: 0.002928018195923394\n",
      "Iteration: 2795 lambda_k: 1 Loss: 0.002928025024332165\n",
      "Iteration: 2796 lambda_k: 1 Loss: 0.002928031819097914\n",
      "Iteration: 2797 lambda_k: 1 Loss: 0.002928038580386802\n",
      "Iteration: 2798 lambda_k: 1 Loss: 0.0029280453083641573\n",
      "Iteration: 2799 lambda_k: 1 Loss: 0.002928052003194493\n",
      "Iteration: 2800 lambda_k: 1 Loss: 0.0029280586650414983\n",
      "Iteration: 2801 lambda_k: 1 Loss: 0.0029280652940680794\n",
      "Iteration: 2802 lambda_k: 1 Loss: 0.0029280718904363093\n",
      "Iteration: 2803 lambda_k: 1 Loss: 0.0029280784543074543\n",
      "Iteration: 2804 lambda_k: 1 Loss: 0.0029280849858420114\n",
      "Iteration: 2805 lambda_k: 1 Loss: 0.002928091485199672\n",
      "Iteration: 2806 lambda_k: 1 Loss: 0.0029280979525393436\n",
      "Iteration: 2807 lambda_k: 1 Loss: 0.002928104388019147\n",
      "Iteration: 2808 lambda_k: 1 Loss: 0.002928110791796413\n",
      "Iteration: 2809 lambda_k: 1 Loss: 0.002928117164027685\n",
      "Iteration: 2810 lambda_k: 1 Loss: 0.002928123504868751\n",
      "Iteration: 2811 lambda_k: 1 Loss: 0.0029281298144746277\n",
      "Iteration: 2812 lambda_k: 1 Loss: 0.002928136092999563\n",
      "Iteration: 2813 lambda_k: 1 Loss: 0.0029281423405970396\n",
      "Iteration: 2814 lambda_k: 1 Loss: 0.002928148557419771\n",
      "Iteration: 2815 lambda_k: 1 Loss: 0.002928154743619745\n",
      "Iteration: 2816 lambda_k: 1 Loss: 0.002928160899348171\n",
      "Iteration: 2817 lambda_k: 1 Loss: 0.002928167024755522\n",
      "Iteration: 2818 lambda_k: 1 Loss: 0.0029281731199915203\n",
      "Iteration: 2819 lambda_k: 1 Loss: 0.002928179185205158\n",
      "Iteration: 2820 lambda_k: 1 Loss: 0.0029281852205447047\n",
      "Iteration: 2821 lambda_k: 1 Loss: 0.0029281912261576544\n",
      "Iteration: 2822 lambda_k: 1 Loss: 0.0029281972021908097\n",
      "Iteration: 2823 lambda_k: 1 Loss: 0.0029282031487902226\n",
      "Iteration: 2824 lambda_k: 1 Loss: 0.0029282090661012458\n",
      "Iteration: 2825 lambda_k: 1 Loss: 0.002928214954268487\n",
      "Iteration: 2826 lambda_k: 1 Loss: 0.0029282208134358777\n",
      "Iteration: 2827 lambda_k: 1 Loss: 0.0029282266437466093\n",
      "Iteration: 2828 lambda_k: 1 Loss: 0.00292823244534314\n",
      "Iteration: 2829 lambda_k: 1 Loss: 0.002928238218367265\n",
      "Iteration: 2830 lambda_k: 1 Loss: 0.002928243962960074\n",
      "Iteration: 2831 lambda_k: 1 Loss: 0.0029282496792619435\n",
      "Iteration: 2832 lambda_k: 1 Loss: 0.0029282553674125867\n",
      "Iteration: 2833 lambda_k: 1 Loss: 0.002928261027550982\n",
      "Iteration: 2834 lambda_k: 1 Loss: 0.0029282666598154598\n",
      "Iteration: 2835 lambda_k: 1 Loss: 0.002928272264343622\n",
      "Iteration: 2836 lambda_k: 1 Loss: 0.002928277841272439\n",
      "Iteration: 2837 lambda_k: 1 Loss: 0.002928283390738164\n",
      "Iteration: 2838 lambda_k: 1 Loss: 0.0029282889128764145\n",
      "Iteration: 2839 lambda_k: 1 Loss: 0.0029282944078220952\n",
      "Iteration: 2840 lambda_k: 1 Loss: 0.002928299875709486\n",
      "Iteration: 2841 lambda_k: 1 Loss: 0.0029283053166721633\n",
      "Iteration: 2842 lambda_k: 1 Loss: 0.002928310730843077\n",
      "Iteration: 2843 lambda_k: 1 Loss: 0.0029283161183544923\n",
      "Iteration: 2844 lambda_k: 1 Loss: 0.002928321479338048\n",
      "Iteration: 2845 lambda_k: 1 Loss: 0.002928326813924708\n",
      "Iteration: 2846 lambda_k: 1 Loss: 0.0029283321222447964\n",
      "Iteration: 2847 lambda_k: 1 Loss: 0.0029283374044280137\n",
      "Iteration: 2848 lambda_k: 1 Loss: 0.0029283426606033754\n",
      "Iteration: 2849 lambda_k: 1 Loss: 0.0029283478908992866\n",
      "Iteration: 2850 lambda_k: 1 Loss: 0.0029283530954435123\n",
      "Iteration: 2851 lambda_k: 1 Loss: 0.0029283582743631995\n",
      "Iteration: 2852 lambda_k: 1 Loss: 0.00292836342778485\n",
      "Iteration: 2853 lambda_k: 1 Loss: 0.0029283685558343427\n",
      "Iteration: 2854 lambda_k: 1 Loss: 0.002928373658636935\n",
      "Iteration: 2855 lambda_k: 1 Loss: 0.0029283787363172616\n",
      "Iteration: 2856 lambda_k: 1 Loss: 0.002928383788999356\n",
      "Iteration: 2857 lambda_k: 1 Loss: 0.0029283888168066103\n",
      "Iteration: 2858 lambda_k: 1 Loss: 0.002928393819861842\n",
      "Iteration: 2859 lambda_k: 1 Loss: 0.0029283987982872406\n",
      "Iteration: 2860 lambda_k: 1 Loss: 0.0029284037522043687\n",
      "Iteration: 2861 lambda_k: 1 Loss: 0.0029284086817342445\n",
      "Iteration: 2862 lambda_k: 1 Loss: 0.0029284135869972333\n",
      "Iteration: 2863 lambda_k: 1 Loss: 0.0029284184681131363\n",
      "Iteration: 2864 lambda_k: 1 Loss: 0.0029284233252011667\n",
      "Iteration: 2865 lambda_k: 1 Loss: 0.0029284281583799032\n",
      "Iteration: 2866 lambda_k: 1 Loss: 0.0029284329677673826\n",
      "Iteration: 2867 lambda_k: 1 Loss: 0.0029284377534810474\n",
      "Iteration: 2868 lambda_k: 1 Loss: 0.002928442515637767\n",
      "Iteration: 2869 lambda_k: 1 Loss: 0.0029284472543538373\n",
      "Iteration: 2870 lambda_k: 1 Loss: 0.002928451969744929\n",
      "Iteration: 2871 lambda_k: 1 Loss: 0.0029284566619261943\n",
      "Iteration: 2872 lambda_k: 1 Loss: 0.0029284613310121914\n",
      "Iteration: 2873 lambda_k: 1 Loss: 0.002928465977116928\n",
      "Iteration: 2874 lambda_k: 1 Loss: 0.0029284706003538277\n",
      "Iteration: 2875 lambda_k: 1 Loss: 0.0029284752008357767\n",
      "Iteration: 2876 lambda_k: 1 Loss: 0.0029284797786750683\n",
      "Iteration: 2877 lambda_k: 1 Loss: 0.0029284843339834944\n",
      "Iteration: 2878 lambda_k: 1 Loss: 0.0029284888668722397\n",
      "Iteration: 2879 lambda_k: 1 Loss: 0.0029284933774519706\n",
      "Iteration: 2880 lambda_k: 1 Loss: 0.002928497865832793\n",
      "Iteration: 2881 lambda_k: 1 Loss: 0.0029285023321242733\n",
      "Iteration: 2882 lambda_k: 1 Loss: 0.0029285067764354363\n",
      "Iteration: 2883 lambda_k: 1 Loss: 0.0029285111988747964\n",
      "Iteration: 2884 lambda_k: 1 Loss: 0.00292851559955026\n",
      "Iteration: 2885 lambda_k: 1 Loss: 0.002928519978569244\n",
      "Iteration: 2886 lambda_k: 1 Loss: 0.0029285243360386492\n",
      "Iteration: 2887 lambda_k: 1 Loss: 0.002928528672064825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2888 lambda_k: 1 Loss: 0.002928532986753601\n",
      "Iteration: 2889 lambda_k: 1 Loss: 0.0029285372802102744\n",
      "Iteration: 2890 lambda_k: 1 Loss: 0.002928541552539648\n",
      "Iteration: 2891 lambda_k: 1 Loss: 0.002928545803845963\n",
      "Iteration: 2892 lambda_k: 1 Loss: 0.002928550034232988\n",
      "Iteration: 2893 lambda_k: 1 Loss: 0.0029285542438039426\n",
      "Iteration: 2894 lambda_k: 1 Loss: 0.0029285584326615723\n",
      "Iteration: 2895 lambda_k: 1 Loss: 0.0029285626009080946\n",
      "Iteration: 2896 lambda_k: 1 Loss: 0.002928566748645223\n",
      "Iteration: 2897 lambda_k: 1 Loss: 0.0029285708759741553\n",
      "Iteration: 2898 lambda_k: 1 Loss: 0.0029285749829956174\n",
      "Iteration: 2899 lambda_k: 1 Loss: 0.0029285790698098065\n",
      "Iteration: 2900 lambda_k: 1 Loss: 0.0029285831365164577\n",
      "Iteration: 2901 lambda_k: 1 Loss: 0.0029285871832147817\n",
      "Iteration: 2902 lambda_k: 1 Loss: 0.0029285912100035175\n",
      "Iteration: 2903 lambda_k: 1 Loss: 0.0029285952169808994\n",
      "Iteration: 2904 lambda_k: 1 Loss: 0.002928599204244684\n",
      "Iteration: 2905 lambda_k: 1 Loss: 0.0029286031718921634\n",
      "Iteration: 2906 lambda_k: 1 Loss: 0.002928607120020122\n",
      "Iteration: 2907 lambda_k: 1 Loss: 0.002928611048724882\n",
      "Iteration: 2908 lambda_k: 1 Loss: 0.0029286149581022806\n",
      "Iteration: 2909 lambda_k: 1 Loss: 0.002928618848247667\n",
      "Iteration: 2910 lambda_k: 1 Loss: 0.0029286227192559474\n",
      "Iteration: 2911 lambda_k: 1 Loss: 0.0029286265712215494\n",
      "Iteration: 2912 lambda_k: 1 Loss: 0.002928630404238431\n",
      "Iteration: 2913 lambda_k: 1 Loss: 0.0029286342184000743\n",
      "Iteration: 2914 lambda_k: 1 Loss: 0.002928638013799513\n",
      "Iteration: 2915 lambda_k: 1 Loss: 0.002928641790529324\n",
      "Iteration: 2916 lambda_k: 1 Loss: 0.0029286455486816254\n",
      "Iteration: 2917 lambda_k: 1 Loss: 0.002928649288348053\n",
      "Iteration: 2918 lambda_k: 1 Loss: 0.0029286530096198195\n",
      "Iteration: 2919 lambda_k: 1 Loss: 0.0029286567125876833\n",
      "Iteration: 2920 lambda_k: 1 Loss: 0.002928660397341936\n",
      "Iteration: 2921 lambda_k: 1 Loss: 0.002928664063972456\n",
      "Iteration: 2922 lambda_k: 1 Loss: 0.002928667712568632\n",
      "Iteration: 2923 lambda_k: 1 Loss: 0.00292867134321946\n",
      "Iteration: 2924 lambda_k: 1 Loss: 0.002928674956013453\n",
      "Iteration: 2925 lambda_k: 1 Loss: 0.0029286785510386983\n",
      "Iteration: 2926 lambda_k: 1 Loss: 0.0029286821283828735\n",
      "Iteration: 2927 lambda_k: 1 Loss: 0.002928685688133195\n",
      "Iteration: 2928 lambda_k: 1 Loss: 0.002928689230376463\n",
      "Iteration: 2929 lambda_k: 1 Loss: 0.002928692755199025\n",
      "Iteration: 2930 lambda_k: 1 Loss: 0.002928696262686822\n",
      "Iteration: 2931 lambda_k: 1 Loss: 0.002928699752925361\n",
      "Iteration: 2932 lambda_k: 1 Loss: 0.00292870322599975\n",
      "Iteration: 2933 lambda_k: 1 Loss: 0.0029287066819946414\n",
      "Iteration: 2934 lambda_k: 1 Loss: 0.0029287101209942968\n",
      "Iteration: 2935 lambda_k: 1 Loss: 0.0029287135430825294\n",
      "Iteration: 2936 lambda_k: 1 Loss: 0.002928716948342767\n",
      "Iteration: 2937 lambda_k: 1 Loss: 0.0029287203368580134\n",
      "Iteration: 2938 lambda_k: 1 Loss: 0.002928723708710879\n",
      "Iteration: 2939 lambda_k: 1 Loss: 0.0029287270639835362\n",
      "Iteration: 2940 lambda_k: 1 Loss: 0.0029287304027577777\n",
      "Iteration: 2941 lambda_k: 1 Loss: 0.0029287337251149605\n",
      "Iteration: 2942 lambda_k: 1 Loss: 0.0029287370311360693\n",
      "Iteration: 2943 lambda_k: 1 Loss: 0.0029287403209016616\n",
      "Iteration: 2944 lambda_k: 1 Loss: 0.002928743594491952\n",
      "Iteration: 2945 lambda_k: 1 Loss: 0.0029287468519866734\n",
      "Iteration: 2946 lambda_k: 1 Loss: 0.0029287500934652264\n",
      "Iteration: 2947 lambda_k: 1 Loss: 0.0029287533190066103\n",
      "Iteration: 2948 lambda_k: 1 Loss: 0.00292875652868941\n",
      "Iteration: 2949 lambda_k: 1 Loss: 0.0029287597225918526\n",
      "Iteration: 2950 lambda_k: 1 Loss: 0.0029287629007917603\n",
      "Iteration: 2951 lambda_k: 1 Loss: 0.002928766063366571\n",
      "Iteration: 2952 lambda_k: 1 Loss: 0.0029287692103933524\n",
      "Iteration: 2953 lambda_k: 1 Loss: 0.002928772341948779\n",
      "Iteration: 2954 lambda_k: 1 Loss: 0.0029287754581091307\n",
      "Iteration: 2955 lambda_k: 1 Loss: 0.002928778558950339\n",
      "Iteration: 2956 lambda_k: 1 Loss: 0.0029287816445479594\n",
      "Iteration: 2957 lambda_k: 1 Loss: 0.002928784714977148\n",
      "Iteration: 2958 lambda_k: 1 Loss: 0.0029287877703127133\n",
      "Iteration: 2959 lambda_k: 1 Loss: 0.002928790810629093\n",
      "Iteration: 2960 lambda_k: 1 Loss: 0.002928793836000342\n",
      "Iteration: 2961 lambda_k: 1 Loss: 0.0029287968465001443\n",
      "Iteration: 2962 lambda_k: 1 Loss: 0.0029287998422018453\n",
      "Iteration: 2963 lambda_k: 1 Loss: 0.002928802823178413\n",
      "Iteration: 2964 lambda_k: 1 Loss: 0.002928805789502451\n",
      "Iteration: 2965 lambda_k: 1 Loss: 0.002928808741246204\n",
      "Iteration: 2966 lambda_k: 1 Loss: 0.0029288116784815716\n",
      "Iteration: 2967 lambda_k: 1 Loss: 0.0029288146012800822\n",
      "Iteration: 2968 lambda_k: 1 Loss: 0.002928817509712934\n",
      "Iteration: 2969 lambda_k: 1 Loss: 0.002928820403850944\n",
      "Iteration: 2970 lambda_k: 1 Loss: 0.002928823283764589\n",
      "Iteration: 2971 lambda_k: 1 Loss: 0.0029288261495240297\n",
      "Iteration: 2972 lambda_k: 1 Loss: 0.002928829001199019\n",
      "Iteration: 2973 lambda_k: 1 Loss: 0.0029288318388590055\n",
      "Iteration: 2974 lambda_k: 1 Loss: 0.0029288346625730957\n",
      "Iteration: 2975 lambda_k: 1 Loss: 0.0029288374724100365\n",
      "Iteration: 2976 lambda_k: 1 Loss: 0.002928840268438259\n",
      "Iteration: 2977 lambda_k: 1 Loss: 0.0029288430507258436\n",
      "Iteration: 2978 lambda_k: 1 Loss: 0.0029288458193405054\n",
      "Iteration: 2979 lambda_k: 1 Loss: 0.0029288485743496524\n",
      "Iteration: 2980 lambda_k: 1 Loss: 0.002928851315820388\n",
      "Iteration: 2981 lambda_k: 1 Loss: 0.002928854043819423\n",
      "Iteration: 2982 lambda_k: 1 Loss: 0.0029288567584131835\n",
      "Iteration: 2983 lambda_k: 1 Loss: 0.002928859459667764\n",
      "Iteration: 2984 lambda_k: 1 Loss: 0.0029288621476489017\n",
      "Iteration: 2985 lambda_k: 1 Loss: 0.002928864822422044\n",
      "Iteration: 2986 lambda_k: 1 Loss: 0.002928867484052279\n",
      "Iteration: 2987 lambda_k: 1 Loss: 0.002928870132604402\n",
      "Iteration: 2988 lambda_k: 1 Loss: 0.0029288727681428895\n",
      "Iteration: 2989 lambda_k: 1 Loss: 0.0029288753907318822\n",
      "Iteration: 2990 lambda_k: 1 Loss: 0.0029288780004351938\n",
      "Iteration: 2991 lambda_k: 1 Loss: 0.0029288805973163566\n",
      "Iteration: 2992 lambda_k: 1 Loss: 0.0029288831814385558\n",
      "Iteration: 2993 lambda_k: 1 Loss: 0.00292888575286469\n",
      "Iteration: 2994 lambda_k: 1 Loss: 0.002928888311657349\n",
      "Iteration: 2995 lambda_k: 1 Loss: 0.0029288908578787884\n",
      "Iteration: 2996 lambda_k: 1 Loss: 0.0029288933915909794\n",
      "Iteration: 2997 lambda_k: 1 Loss: 0.0029288959128555597\n",
      "Iteration: 2998 lambda_k: 1 Loss: 0.0029288984217338923\n",
      "Iteration: 2999 lambda_k: 1 Loss: 0.002928900918287004\n",
      "Iteration: 3000 lambda_k: 1 Loss: 0.0029289034025756695\n",
      "Iteration: 3001 lambda_k: 1 Loss: 0.002928905874660322\n",
      "Iteration: 3002 lambda_k: 1 Loss: 0.002928908334601102\n",
      "Iteration: 3003 lambda_k: 1 Loss: 0.002928910782457858\n",
      "Iteration: 3004 lambda_k: 1 Loss: 0.0029289132182901342\n",
      "Iteration: 3005 lambda_k: 1 Loss: 0.0029289156421571975\n",
      "Iteration: 3006 lambda_k: 1 Loss: 0.002928918054118011\n",
      "Iteration: 3007 lambda_k: 1 Loss: 0.0029289204542312396\n",
      "Iteration: 3008 lambda_k: 1 Loss: 0.002928922842555291\n",
      "Iteration: 3009 lambda_k: 1 Loss: 0.002928925219148245\n",
      "Iteration: 3010 lambda_k: 1 Loss: 0.0029289275840679073\n",
      "Iteration: 3011 lambda_k: 1 Loss: 0.0029289299373717963\n",
      "Iteration: 3012 lambda_k: 1 Loss: 0.0029289322791171755\n",
      "Iteration: 3013 lambda_k: 1 Loss: 0.002928934609360991\n",
      "Iteration: 3014 lambda_k: 1 Loss: 0.0029289369281598845\n",
      "Iteration: 3015 lambda_k: 1 Loss: 0.002928939235570287\n",
      "Iteration: 3016 lambda_k: 1 Loss: 0.0029289415316482957\n",
      "Iteration: 3017 lambda_k: 1 Loss: 0.0029289438164497444\n",
      "Iteration: 3018 lambda_k: 1 Loss: 0.0029289460900302033\n",
      "Iteration: 3019 lambda_k: 1 Loss: 0.0029289483524449628\n",
      "Iteration: 3020 lambda_k: 1 Loss: 0.0029289506037490356\n",
      "Iteration: 3021 lambda_k: 1 Loss: 0.002928952843997159\n",
      "Iteration: 3022 lambda_k: 1 Loss: 0.0029289550732438134\n",
      "Iteration: 3023 lambda_k: 1 Loss: 0.0029289572915432004\n",
      "Iteration: 3024 lambda_k: 1 Loss: 0.002928959498949258\n",
      "Iteration: 3025 lambda_k: 1 Loss: 0.00292896169551563\n",
      "Iteration: 3026 lambda_k: 1 Loss: 0.0029289638812957444\n",
      "Iteration: 3027 lambda_k: 1 Loss: 0.002928966056342744\n",
      "Iteration: 3028 lambda_k: 1 Loss: 0.002928968220709483\n",
      "Iteration: 3029 lambda_k: 1 Loss: 0.0029289703744485892\n",
      "Iteration: 3030 lambda_k: 1 Loss: 0.0029289725176124106\n",
      "Iteration: 3031 lambda_k: 1 Loss: 0.00292897465025306\n",
      "Iteration: 3032 lambda_k: 1 Loss: 0.0029289767724223464\n",
      "Iteration: 3033 lambda_k: 1 Loss: 0.0029289788841718773\n",
      "Iteration: 3034 lambda_k: 1 Loss: 0.0029289809855529737\n",
      "Iteration: 3035 lambda_k: 1 Loss: 0.0029289830766167165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3036 lambda_k: 1 Loss: 0.002928985157413914\n",
      "Iteration: 3037 lambda_k: 1 Loss: 0.00292898722799515\n",
      "Iteration: 3038 lambda_k: 1 Loss: 0.0029289892884107396\n",
      "Iteration: 3039 lambda_k: 1 Loss: 0.002928991338710766\n",
      "Iteration: 3040 lambda_k: 1 Loss: 0.002928993378945042\n",
      "Iteration: 3041 lambda_k: 1 Loss: 0.002928995409163144\n",
      "Iteration: 3042 lambda_k: 1 Loss: 0.0029289974294144164\n",
      "Iteration: 3043 lambda_k: 1 Loss: 0.0029289994397479467\n",
      "Iteration: 3044 lambda_k: 1 Loss: 0.002929001440212582\n",
      "Iteration: 3045 lambda_k: 1 Loss: 0.0029290034308569347\n",
      "Iteration: 3046 lambda_k: 1 Loss: 0.0029290054117293413\n",
      "Iteration: 3047 lambda_k: 1 Loss: 0.002929007382877968\n",
      "Iteration: 3048 lambda_k: 1 Loss: 0.002929009344350693\n",
      "Iteration: 3049 lambda_k: 1 Loss: 0.002929011296195167\n",
      "Iteration: 3050 lambda_k: 1 Loss: 0.002929013238458805\n",
      "Iteration: 3051 lambda_k: 1 Loss: 0.002929015171188788\n",
      "Iteration: 3052 lambda_k: 1 Loss: 0.0029290170944320814\n",
      "Iteration: 3053 lambda_k: 1 Loss: 0.0029290190082353904\n",
      "Iteration: 3054 lambda_k: 1 Loss: 0.0029290209126452095\n",
      "Iteration: 3055 lambda_k: 1 Loss: 0.002929022807707785\n",
      "Iteration: 3056 lambda_k: 1 Loss: 0.00292902469346914\n",
      "Iteration: 3057 lambda_k: 1 Loss: 0.0029290265699750922\n",
      "Iteration: 3058 lambda_k: 1 Loss: 0.0029290284372712173\n",
      "Iteration: 3059 lambda_k: 1 Loss: 0.0029290302954028654\n",
      "Iteration: 3060 lambda_k: 1 Loss: 0.002929032144415157\n",
      "Iteration: 3061 lambda_k: 1 Loss: 0.002929033984353006\n",
      "Iteration: 3062 lambda_k: 1 Loss: 0.002929035815261085\n",
      "Iteration: 3063 lambda_k: 1 Loss: 0.0029290376371838606\n",
      "Iteration: 3064 lambda_k: 1 Loss: 0.0029290394501655637\n",
      "Iteration: 3065 lambda_k: 1 Loss: 0.0029290412542502284\n",
      "Iteration: 3066 lambda_k: 1 Loss: 0.0029290430494816736\n",
      "Iteration: 3067 lambda_k: 1 Loss: 0.0029290448359034623\n",
      "Iteration: 3068 lambda_k: 1 Loss: 0.0029290466135589886\n",
      "Iteration: 3069 lambda_k: 1 Loss: 0.002929048382491399\n",
      "Iteration: 3070 lambda_k: 1 Loss: 0.002929050142743663\n",
      "Iteration: 3071 lambda_k: 1 Loss: 0.002929051894358475\n",
      "Iteration: 3072 lambda_k: 1 Loss: 0.002929053637378399\n",
      "Iteration: 3073 lambda_k: 1 Loss: 0.002929055371845721\n",
      "Iteration: 3074 lambda_k: 1 Loss: 0.00292905709780255\n",
      "Iteration: 3075 lambda_k: 1 Loss: 0.0029290588152907997\n",
      "Iteration: 3076 lambda_k: 1 Loss: 0.0029290605243521488\n",
      "Iteration: 3077 lambda_k: 1 Loss: 0.002929062225028091\n",
      "Iteration: 3078 lambda_k: 1 Loss: 0.0029290639173598873\n",
      "Iteration: 3079 lambda_k: 1 Loss: 0.0029290656013886145\n",
      "Iteration: 3080 lambda_k: 1 Loss: 0.0029290672771551496\n",
      "Iteration: 3081 lambda_k: 1 Loss: 0.0029290689447001674\n",
      "Iteration: 3082 lambda_k: 1 Loss: 0.0029290706040641505\n",
      "Iteration: 3083 lambda_k: 1 Loss: 0.0029290722552873507\n",
      "Iteration: 3084 lambda_k: 1 Loss: 0.002929073898409833\n",
      "Iteration: 3085 lambda_k: 1 Loss: 0.0029290755334714915\n",
      "Iteration: 3086 lambda_k: 1 Loss: 0.002929077160511969\n",
      "Iteration: 3087 lambda_k: 1 Loss: 0.002929078779570765\n",
      "Iteration: 3088 lambda_k: 1 Loss: 0.002929080390687174\n",
      "Iteration: 3089 lambda_k: 1 Loss: 0.002929081993900291\n",
      "Iteration: 3090 lambda_k: 1 Loss: 0.002929083589248994\n",
      "Iteration: 3091 lambda_k: 1 Loss: 0.002929085176771983\n",
      "Iteration: 3092 lambda_k: 1 Loss: 0.0029290867565077944\n",
      "Iteration: 3093 lambda_k: 1 Loss: 0.0029290883284947534\n",
      "Iteration: 3094 lambda_k: 1 Loss: 0.0029290898927709853\n",
      "Iteration: 3095 lambda_k: 1 Loss: 0.0029290914493744174\n",
      "Iteration: 3096 lambda_k: 1 Loss: 0.0029290929983428445\n",
      "Iteration: 3097 lambda_k: 1 Loss: 0.002929094539713844\n",
      "Iteration: 3098 lambda_k: 1 Loss: 0.0029290960735247782\n",
      "Iteration: 3099 lambda_k: 1 Loss: 0.002929097599812865\n",
      "Iteration: 3100 lambda_k: 1 Loss: 0.002929099118615123\n",
      "Iteration: 3101 lambda_k: 1 Loss: 0.0029291006299683956\n",
      "Iteration: 3102 lambda_k: 1 Loss: 0.0029291021339093314\n",
      "Iteration: 3103 lambda_k: 1 Loss: 0.0029291036304743977\n",
      "Iteration: 3104 lambda_k: 1 Loss: 0.002929105119699897\n",
      "Iteration: 3105 lambda_k: 1 Loss: 0.0029291066016219392\n",
      "Iteration: 3106 lambda_k: 1 Loss: 0.0029291080762764675\n",
      "Iteration: 3107 lambda_k: 1 Loss: 0.002929109543699236\n",
      "Iteration: 3108 lambda_k: 1 Loss: 0.0029291110039258252\n",
      "Iteration: 3109 lambda_k: 1 Loss: 0.002929112456991646\n",
      "Iteration: 3110 lambda_k: 1 Loss: 0.0029291139029319364\n",
      "Iteration: 3111 lambda_k: 1 Loss: 0.002929115341781736\n",
      "Iteration: 3112 lambda_k: 1 Loss: 0.0029291167735759596\n",
      "Iteration: 3113 lambda_k: 1 Loss: 0.002929118198349301\n",
      "Iteration: 3114 lambda_k: 1 Loss: 0.002929119616136307\n",
      "Iteration: 3115 lambda_k: 1 Loss: 0.0029291210269713442\n",
      "Iteration: 3116 lambda_k: 1 Loss: 0.002929122430888619\n",
      "Iteration: 3117 lambda_k: 1 Loss: 0.0029291238279221627\n",
      "Iteration: 3118 lambda_k: 1 Loss: 0.002929125218105835\n",
      "Iteration: 3119 lambda_k: 1 Loss: 0.002929126601473342\n",
      "Iteration: 3120 lambda_k: 1 Loss: 0.0029291279780582296\n",
      "Iteration: 3121 lambda_k: 1 Loss: 0.002929129347893829\n",
      "Iteration: 3122 lambda_k: 1 Loss: 0.0029291307110133795\n",
      "Iteration: 3123 lambda_k: 1 Loss: 0.002929132067449895\n",
      "Iteration: 3124 lambda_k: 1 Loss: 0.0029291334172362408\n",
      "Iteration: 3125 lambda_k: 1 Loss: 0.0029291347604051402\n",
      "Iteration: 3126 lambda_k: 1 Loss: 0.0029291360969891525\n",
      "Iteration: 3127 lambda_k: 1 Loss: 0.0029291374270206654\n",
      "Iteration: 3128 lambda_k: 1 Loss: 0.002929138750531904\n",
      "Iteration: 3129 lambda_k: 1 Loss: 0.002929140067554939\n",
      "Iteration: 3130 lambda_k: 1 Loss: 0.0029291413781216804\n",
      "Iteration: 3131 lambda_k: 1 Loss: 0.002929142682263898\n",
      "Iteration: 3132 lambda_k: 1 Loss: 0.0029291439800131743\n",
      "Iteration: 3133 lambda_k: 1 Loss: 0.0029291452714009683\n",
      "Iteration: 3134 lambda_k: 1 Loss: 0.0029291465564585586\n",
      "Iteration: 3135 lambda_k: 1 Loss: 0.002929147835217078\n",
      "Iteration: 3136 lambda_k: 1 Loss: 0.0029291491077074925\n",
      "Iteration: 3137 lambda_k: 1 Loss: 0.0029291503739606512\n",
      "Iteration: 3138 lambda_k: 1 Loss: 0.0029291516340072144\n",
      "Iteration: 3139 lambda_k: 1 Loss: 0.0029291528878777097\n",
      "Iteration: 3140 lambda_k: 1 Loss: 0.0029291541356025134\n",
      "Iteration: 3141 lambda_k: 1 Loss: 0.0029291553772118418\n",
      "Iteration: 3142 lambda_k: 1 Loss: 0.0029291566127357875\n",
      "Iteration: 3143 lambda_k: 1 Loss: 0.0029291578422042663\n",
      "Iteration: 3144 lambda_k: 1 Loss: 0.002929159065647038\n",
      "Iteration: 3145 lambda_k: 1 Loss: 0.002929160283093756\n",
      "Iteration: 3146 lambda_k: 1 Loss: 0.0029291614945738745\n",
      "Iteration: 3147 lambda_k: 1 Loss: 0.0029291627001167657\n",
      "Iteration: 3148 lambda_k: 1 Loss: 0.0029291638997516146\n",
      "Iteration: 3149 lambda_k: 1 Loss: 0.002929165093507461\n",
      "Iteration: 3150 lambda_k: 1 Loss: 0.002929166281413216\n",
      "Iteration: 3151 lambda_k: 1 Loss: 0.002929167463497645\n",
      "Iteration: 3152 lambda_k: 1 Loss: 0.0029291686397893806\n",
      "Iteration: 3153 lambda_k: 1 Loss: 0.0029291698103168996\n",
      "Iteration: 3154 lambda_k: 1 Loss: 0.002929170975108543\n",
      "Iteration: 3155 lambda_k: 1 Loss: 0.0029291721341925007\n",
      "Iteration: 3156 lambda_k: 1 Loss: 0.0029291732875968315\n",
      "Iteration: 3157 lambda_k: 1 Loss: 0.0029291744353494753\n",
      "Iteration: 3158 lambda_k: 1 Loss: 0.0029291755774782224\n",
      "Iteration: 3159 lambda_k: 1 Loss: 0.002929176714010713\n",
      "Iteration: 3160 lambda_k: 1 Loss: 0.0029291778449744505\n",
      "Iteration: 3161 lambda_k: 1 Loss: 0.002929178970396804\n",
      "Iteration: 3162 lambda_k: 1 Loss: 0.0029291800903050425\n",
      "Iteration: 3163 lambda_k: 1 Loss: 0.0029291812047262658\n",
      "Iteration: 3164 lambda_k: 1 Loss: 0.0029291823136874218\n",
      "Iteration: 3165 lambda_k: 1 Loss: 0.0029291834172153947\n",
      "Iteration: 3166 lambda_k: 1 Loss: 0.0029291845153368522\n",
      "Iteration: 3167 lambda_k: 1 Loss: 0.002929185608078387\n",
      "Iteration: 3168 lambda_k: 1 Loss: 0.002929186695466447\n",
      "Iteration: 3169 lambda_k: 1 Loss: 0.0029291877775273504\n",
      "Iteration: 3170 lambda_k: 1 Loss: 0.0029291888542872665\n",
      "Iteration: 3171 lambda_k: 1 Loss: 0.002929189925772257\n",
      "Iteration: 3172 lambda_k: 1 Loss: 0.0029291909920082584\n",
      "Iteration: 3173 lambda_k: 1 Loss: 0.0029291920530210625\n",
      "Iteration: 3174 lambda_k: 1 Loss: 0.0029291931088363354\n",
      "Iteration: 3175 lambda_k: 1 Loss: 0.0029291941594796417\n",
      "Iteration: 3176 lambda_k: 1 Loss: 0.0029291952049763917\n",
      "Iteration: 3177 lambda_k: 1 Loss: 0.0029291962453518723\n",
      "Iteration: 3178 lambda_k: 1 Loss: 0.0029291972806312554\n",
      "Iteration: 3179 lambda_k: 1 Loss: 0.002929198310839596\n",
      "Iteration: 3180 lambda_k: 1 Loss: 0.0029291993360017984\n",
      "Iteration: 3181 lambda_k: 1 Loss: 0.0029292003561427007\n",
      "Iteration: 3182 lambda_k: 1 Loss: 0.0029292013712869363\n",
      "Iteration: 3183 lambda_k: 1 Loss: 0.002929202381459081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3184 lambda_k: 1 Loss: 0.002929203386683562\n",
      "Iteration: 3185 lambda_k: 1 Loss: 0.0029292043869846925\n",
      "Iteration: 3186 lambda_k: 1 Loss: 0.0029292053823866634\n",
      "Iteration: 3187 lambda_k: 1 Loss: 0.00292920637291356\n",
      "Iteration: 3188 lambda_k: 1 Loss: 0.0029292073585893232\n",
      "Iteration: 3189 lambda_k: 1 Loss: 0.002929208339437812\n",
      "Iteration: 3190 lambda_k: 1 Loss: 0.002929209315482723\n",
      "Iteration: 3191 lambda_k: 1 Loss: 0.0029292102867476823\n",
      "Iteration: 3192 lambda_k: 1 Loss: 0.0029292112532561514\n",
      "Iteration: 3193 lambda_k: 1 Loss: 0.0029292122150315182\n",
      "Iteration: 3194 lambda_k: 1 Loss: 0.0029292131720970254\n",
      "Iteration: 3195 lambda_k: 1 Loss: 0.002929214124475836\n",
      "Iteration: 3196 lambda_k: 1 Loss: 0.0029292150721909643\n",
      "Iteration: 3197 lambda_k: 1 Loss: 0.002929216015265305\n",
      "Iteration: 3198 lambda_k: 1 Loss: 0.0029292169537216903\n",
      "Iteration: 3199 lambda_k: 1 Loss: 0.0029292178875827946\n",
      "Iteration: 3200 lambda_k: 1 Loss: 0.0029292188168712004\n",
      "Iteration: 3201 lambda_k: 1 Loss: 0.0029292197416093645\n",
      "Iteration: 3202 lambda_k: 1 Loss: 0.0029292206618196474\n",
      "Iteration: 3203 lambda_k: 1 Loss: 0.0029292215775242733\n",
      "Iteration: 3204 lambda_k: 1 Loss: 0.002929222488745409\n",
      "Iteration: 3205 lambda_k: 1 Loss: 0.0029292233955050487\n",
      "Iteration: 3206 lambda_k: 1 Loss: 0.002929224297825119\n",
      "Iteration: 3207 lambda_k: 1 Loss: 0.002929225195727435\n",
      "Iteration: 3208 lambda_k: 1 Loss: 0.00292922608923369\n",
      "Iteration: 3209 lambda_k: 1 Loss: 0.0029292269783654816\n",
      "Iteration: 3210 lambda_k: 1 Loss: 0.002929227863144304\n",
      "Iteration: 3211 lambda_k: 1 Loss: 0.002929228743591533\n",
      "Iteration: 3212 lambda_k: 1 Loss: 0.002929229619728461\n",
      "Iteration: 3213 lambda_k: 1 Loss: 0.0029292304915762273\n",
      "Iteration: 3214 lambda_k: 1 Loss: 0.0029292313591559238\n",
      "Iteration: 3215 lambda_k: 1 Loss: 0.0029292322224885067\n",
      "Iteration: 3216 lambda_k: 1 Loss: 0.0029292330815948375\n",
      "Iteration: 3217 lambda_k: 1 Loss: 0.0029292339364956747\n",
      "Iteration: 3218 lambda_k: 1 Loss: 0.002929234787211674\n",
      "Iteration: 3219 lambda_k: 1 Loss: 0.0029292356337633696\n",
      "Iteration: 3220 lambda_k: 1 Loss: 0.0029292364761712374\n",
      "Iteration: 3221 lambda_k: 1 Loss: 0.0029292373144556056\n",
      "Iteration: 3222 lambda_k: 1 Loss: 0.0029292381486367425\n",
      "Iteration: 3223 lambda_k: 1 Loss: 0.002929238978734819\n",
      "Iteration: 3224 lambda_k: 1 Loss: 0.0029292398047698637\n",
      "Iteration: 3225 lambda_k: 1 Loss: 0.0029292406267618183\n",
      "Iteration: 3226 lambda_k: 1 Loss: 0.002929241444730547\n",
      "Iteration: 3227 lambda_k: 1 Loss: 0.0029292422586958034\n",
      "Iteration: 3228 lambda_k: 1 Loss: 0.002929243068677238\n",
      "Iteration: 3229 lambda_k: 1 Loss: 0.002929243874694463\n",
      "Iteration: 3230 lambda_k: 1 Loss: 0.0029292446767668944\n",
      "Iteration: 3231 lambda_k: 1 Loss: 0.0029292454749139185\n",
      "Iteration: 3232 lambda_k: 1 Loss: 0.002929246269154809\n",
      "Iteration: 3233 lambda_k: 1 Loss: 0.0029292470595087506\n",
      "Iteration: 3234 lambda_k: 1 Loss: 0.0029292478459948205\n",
      "Iteration: 3235 lambda_k: 1 Loss: 0.0029292486286320108\n",
      "Iteration: 3236 lambda_k: 1 Loss: 0.002929249407439215\n",
      "Iteration: 3237 lambda_k: 1 Loss: 0.0029292501824352366\n",
      "Iteration: 3238 lambda_k: 1 Loss: 0.0029292509536388025\n",
      "Iteration: 3239 lambda_k: 1 Loss: 0.0029292517210685338\n",
      "Iteration: 3240 lambda_k: 1 Loss: 0.0029292524847429457\n",
      "Iteration: 3241 lambda_k: 1 Loss: 0.0029292532446804753\n",
      "Iteration: 3242 lambda_k: 1 Loss: 0.002929254000899479\n",
      "Iteration: 3243 lambda_k: 1 Loss: 0.0029292547534181886\n",
      "Iteration: 3244 lambda_k: 1 Loss: 0.0029292555022547967\n",
      "Iteration: 3245 lambda_k: 1 Loss: 0.0029292562474273745\n",
      "Iteration: 3246 lambda_k: 1 Loss: 0.002929256988953891\n",
      "Iteration: 3247 lambda_k: 1 Loss: 0.002929257726852267\n",
      "Iteration: 3248 lambda_k: 1 Loss: 0.002929258461140309\n",
      "Iteration: 3249 lambda_k: 1 Loss: 0.00292925919183573\n",
      "Iteration: 3250 lambda_k: 1 Loss: 0.002929259918956163\n",
      "Iteration: 3251 lambda_k: 1 Loss: 0.002929260642519164\n",
      "Iteration: 3252 lambda_k: 1 Loss: 0.002929261362542196\n",
      "Iteration: 3253 lambda_k: 1 Loss: 0.0029292620790426307\n",
      "Iteration: 3254 lambda_k: 1 Loss: 0.002929262792037761\n",
      "Iteration: 3255 lambda_k: 1 Loss: 0.002929263501544793\n",
      "Iteration: 3256 lambda_k: 1 Loss: 0.002929264207580829\n",
      "Iteration: 3257 lambda_k: 1 Loss: 0.0029292649101629394\n",
      "Iteration: 3258 lambda_k: 1 Loss: 0.0029292656093080666\n",
      "Iteration: 3259 lambda_k: 1 Loss: 0.002929266305033077\n",
      "Iteration: 3260 lambda_k: 1 Loss: 0.002929266997354746\n",
      "Iteration: 3261 lambda_k: 1 Loss: 0.0029292676862897687\n",
      "Iteration: 3262 lambda_k: 1 Loss: 0.0029292683718547935\n",
      "Iteration: 3263 lambda_k: 1 Loss: 0.0029292690540663486\n",
      "Iteration: 3264 lambda_k: 1 Loss: 0.00292926973294089\n",
      "Iteration: 3265 lambda_k: 1 Loss: 0.002929270408494792\n",
      "Iteration: 3266 lambda_k: 1 Loss: 0.002929271080744348\n",
      "Iteration: 3267 lambda_k: 1 Loss: 0.0029292717497057833\n",
      "Iteration: 3268 lambda_k: 1 Loss: 0.002929272415395217\n",
      "Iteration: 3269 lambda_k: 1 Loss: 0.0029292730778287287\n",
      "Iteration: 3270 lambda_k: 1 Loss: 0.0029292737370222818\n",
      "Iteration: 3271 lambda_k: 1 Loss: 0.0029292743929917584\n",
      "Iteration: 3272 lambda_k: 1 Loss: 0.0029292750457530054\n",
      "Iteration: 3273 lambda_k: 1 Loss: 0.0029292756953217544\n",
      "Iteration: 3274 lambda_k: 1 Loss: 0.002929276341713668\n",
      "Iteration: 3275 lambda_k: 1 Loss: 0.0029292769849443403\n",
      "Iteration: 3276 lambda_k: 1 Loss: 0.0029292776250292836\n",
      "Iteration: 3277 lambda_k: 1 Loss: 0.0029292782619839332\n",
      "Iteration: 3278 lambda_k: 1 Loss: 0.002929278895823652\n",
      "Iteration: 3279 lambda_k: 1 Loss: 0.002929279526563711\n",
      "Iteration: 3280 lambda_k: 1 Loss: 0.0029292801542193047\n",
      "Iteration: 3281 lambda_k: 1 Loss: 0.002929280778805578\n",
      "Iteration: 3282 lambda_k: 1 Loss: 0.0029292814003376065\n",
      "Iteration: 3283 lambda_k: 1 Loss: 0.0029292820188303666\n",
      "Iteration: 3284 lambda_k: 1 Loss: 0.002929282634298759\n",
      "Iteration: 3285 lambda_k: 1 Loss: 0.0029292832467576204\n",
      "Iteration: 3286 lambda_k: 1 Loss: 0.0029292838562217233\n",
      "Iteration: 3287 lambda_k: 1 Loss: 0.0029292844627057655\n",
      "Iteration: 3288 lambda_k: 1 Loss: 0.0029292850662243452\n",
      "Iteration: 3289 lambda_k: 1 Loss: 0.0029292856667920203\n",
      "Iteration: 3290 lambda_k: 1 Loss: 0.0029292862644232785\n",
      "Iteration: 3291 lambda_k: 1 Loss: 0.00292928685913253\n",
      "Iteration: 3292 lambda_k: 1 Loss: 0.0029292874509340688\n",
      "Iteration: 3293 lambda_k: 1 Loss: 0.002929288039842186\n",
      "Iteration: 3294 lambda_k: 1 Loss: 0.002929288625871054\n",
      "Iteration: 3295 lambda_k: 1 Loss: 0.0029292892090348275\n",
      "Iteration: 3296 lambda_k: 1 Loss: 0.0029292897893475525\n",
      "Iteration: 3297 lambda_k: 1 Loss: 0.0029292903668231758\n",
      "Iteration: 3298 lambda_k: 1 Loss: 0.0029292909414756515\n",
      "Iteration: 3299 lambda_k: 1 Loss: 0.0029292915133188177\n",
      "Iteration: 3300 lambda_k: 1 Loss: 0.0029292920823664526\n",
      "Iteration: 3301 lambda_k: 1 Loss: 0.002929292648632269\n",
      "Iteration: 3302 lambda_k: 1 Loss: 0.002929293212129892\n",
      "Iteration: 3303 lambda_k: 1 Loss: 0.0029292937728729236\n",
      "Iteration: 3304 lambda_k: 1 Loss: 0.0029292943308748603\n",
      "Iteration: 3305 lambda_k: 1 Loss: 0.002929294886149157\n",
      "Iteration: 3306 lambda_k: 1 Loss: 0.002929295438709184\n",
      "Iteration: 3307 lambda_k: 1 Loss: 0.002929295988568228\n",
      "Iteration: 3308 lambda_k: 1 Loss: 0.002929296535739566\n",
      "Iteration: 3309 lambda_k: 1 Loss: 0.00292929708023636\n",
      "Iteration: 3310 lambda_k: 1 Loss: 0.002929297622071734\n",
      "Iteration: 3311 lambda_k: 1 Loss: 0.0029292981612587385\n",
      "Iteration: 3312 lambda_k: 1 Loss: 0.0029292986978103424\n",
      "Iteration: 3313 lambda_k: 1 Loss: 0.0029292992317394855\n",
      "Iteration: 3314 lambda_k: 1 Loss: 0.0029292997630590303\n",
      "Iteration: 3315 lambda_k: 1 Loss: 0.0029293002917817634\n",
      "Iteration: 3316 lambda_k: 1 Loss: 0.002929300817920431\n",
      "Iteration: 3317 lambda_k: 1 Loss: 0.0029293013414876863\n",
      "Iteration: 3318 lambda_k: 1 Loss: 0.0029293018624961423\n",
      "Iteration: 3319 lambda_k: 1 Loss: 0.00292930238095835\n",
      "Iteration: 3320 lambda_k: 1 Loss: 0.0029293028968868018\n",
      "Iteration: 3321 lambda_k: 1 Loss: 0.0029293034102938964\n",
      "Iteration: 3322 lambda_k: 1 Loss: 0.0029293039211920184\n",
      "Iteration: 3323 lambda_k: 1 Loss: 0.002929304429593456\n",
      "Iteration: 3324 lambda_k: 1 Loss: 0.0029293049355104423\n",
      "Iteration: 3325 lambda_k: 1 Loss: 0.002929305438955177\n",
      "Iteration: 3326 lambda_k: 1 Loss: 0.0029293059399397676\n",
      "Iteration: 3327 lambda_k: 1 Loss: 0.0029293064384762813\n",
      "Iteration: 3328 lambda_k: 1 Loss: 0.002929306934576731\n",
      "Iteration: 3329 lambda_k: 1 Loss: 0.002929307428253046\n",
      "Iteration: 3330 lambda_k: 1 Loss: 0.002929307919517098\n",
      "Iteration: 3331 lambda_k: 1 Loss: 0.002929308408380715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3332 lambda_k: 1 Loss: 0.00292930889485568\n",
      "Iteration: 3333 lambda_k: 1 Loss: 0.002929309378953693\n",
      "Iteration: 3334 lambda_k: 1 Loss: 0.0029293098606864177\n",
      "Iteration: 3335 lambda_k: 1 Loss: 0.0029293103400654307\n",
      "Iteration: 3336 lambda_k: 1 Loss: 0.0029293108171022666\n",
      "Iteration: 3337 lambda_k: 1 Loss: 0.0029293112918084233\n",
      "Iteration: 3338 lambda_k: 1 Loss: 0.002929311764195308\n",
      "Iteration: 3339 lambda_k: 1 Loss: 0.002929312234274284\n",
      "Iteration: 3340 lambda_k: 1 Loss: 0.00292931270205668\n",
      "Iteration: 3341 lambda_k: 1 Loss: 0.0029293131675537493\n",
      "Iteration: 3342 lambda_k: 1 Loss: 0.0029293136307766924\n",
      "Iteration: 3343 lambda_k: 1 Loss: 0.002929314091736641\n",
      "Iteration: 3344 lambda_k: 1 Loss: 0.0029293145504447\n",
      "Iteration: 3345 lambda_k: 1 Loss: 0.0029293150069119086\n",
      "Iteration: 3346 lambda_k: 1 Loss: 0.00292931546114923\n",
      "Iteration: 3347 lambda_k: 1 Loss: 0.002929315913167617\n",
      "Iteration: 3348 lambda_k: 1 Loss: 0.0029293163629779406\n",
      "Iteration: 3349 lambda_k: 1 Loss: 0.0029293168105910036\n",
      "Iteration: 3350 lambda_k: 1 Loss: 0.0029293172560175915\n",
      "Iteration: 3351 lambda_k: 1 Loss: 0.002929317699268401\n",
      "Iteration: 3352 lambda_k: 1 Loss: 0.002929318140354099\n",
      "Iteration: 3353 lambda_k: 1 Loss: 0.0029293185792853126\n",
      "Iteration: 3354 lambda_k: 1 Loss: 0.0029293190160725882\n",
      "Iteration: 3355 lambda_k: 1 Loss: 0.002929319450726414\n",
      "Iteration: 3356 lambda_k: 1 Loss: 0.002929319883257265\n",
      "Iteration: 3357 lambda_k: 1 Loss: 0.0029293203136755495\n",
      "Iteration: 3358 lambda_k: 1 Loss: 0.002929320741991604\n",
      "Iteration: 3359 lambda_k: 1 Loss: 0.0029293211682157217\n",
      "Iteration: 3360 lambda_k: 1 Loss: 0.002929321592358159\n",
      "Iteration: 3361 lambda_k: 1 Loss: 0.002929322014429135\n",
      "Iteration: 3362 lambda_k: 1 Loss: 0.002929322434438778\n",
      "Iteration: 3363 lambda_k: 1 Loss: 0.002929322852397179\n",
      "Iteration: 3364 lambda_k: 1 Loss: 0.002929323268314405\n",
      "Iteration: 3365 lambda_k: 1 Loss: 0.002929323682200458\n",
      "Iteration: 3366 lambda_k: 1 Loss: 0.0029293240940652886\n",
      "Iteration: 3367 lambda_k: 1 Loss: 0.002929324503918783\n",
      "Iteration: 3368 lambda_k: 1 Loss: 0.002929324911770804\n",
      "Iteration: 3369 lambda_k: 1 Loss: 0.0029293253176311605\n",
      "Iteration: 3370 lambda_k: 1 Loss: 0.002929325721509603\n",
      "Iteration: 3371 lambda_k: 1 Loss: 0.002929326123415864\n",
      "Iteration: 3372 lambda_k: 1 Loss: 0.0029293265233595774\n",
      "Iteration: 3373 lambda_k: 1 Loss: 0.002929326921350358\n",
      "Iteration: 3374 lambda_k: 1 Loss: 0.0029293273173977715\n",
      "Iteration: 3375 lambda_k: 1 Loss: 0.0029293277115113483\n",
      "Iteration: 3376 lambda_k: 1 Loss: 0.0029293281037005713\n",
      "Iteration: 3377 lambda_k: 1 Loss: 0.0029293284939748425\n",
      "Iteration: 3378 lambda_k: 1 Loss: 0.0029293288823435486\n",
      "Iteration: 3379 lambda_k: 1 Loss: 0.0029293292688160337\n",
      "Iteration: 3380 lambda_k: 1 Loss: 0.0029293296534015616\n",
      "Iteration: 3381 lambda_k: 1 Loss: 0.0029293300361094\n",
      "Iteration: 3382 lambda_k: 1 Loss: 0.0029293304169487437\n",
      "Iteration: 3383 lambda_k: 1 Loss: 0.0029293307959287326\n",
      "Iteration: 3384 lambda_k: 1 Loss: 0.0029293311730584815\n",
      "Iteration: 3385 lambda_k: 1 Loss: 0.0029293315483470495\n",
      "Iteration: 3386 lambda_k: 1 Loss: 0.00292933192180346\n",
      "Iteration: 3387 lambda_k: 1 Loss: 0.002929332293436673\n",
      "Iteration: 3388 lambda_k: 1 Loss: 0.002929332663255625\n",
      "Iteration: 3389 lambda_k: 1 Loss: 0.0029293330312691974\n",
      "Iteration: 3390 lambda_k: 1 Loss: 0.0029293333974862432\n",
      "Iteration: 3391 lambda_k: 1 Loss: 0.0029293337619155455\n",
      "Iteration: 3392 lambda_k: 1 Loss: 0.002929334124565861\n",
      "Iteration: 3393 lambda_k: 1 Loss: 0.0029293344854459027\n",
      "Iteration: 3394 lambda_k: 1 Loss: 0.002929334844564345\n",
      "Iteration: 3395 lambda_k: 1 Loss: 0.00292933520192981\n",
      "Iteration: 3396 lambda_k: 1 Loss: 0.0029293355575508856\n",
      "Iteration: 3397 lambda_k: 1 Loss: 0.0029293359114361114\n",
      "Iteration: 3398 lambda_k: 1 Loss: 0.002929336263593973\n",
      "Iteration: 3399 lambda_k: 1 Loss: 0.0029293366140329524\n",
      "Iteration: 3400 lambda_k: 1 Loss: 0.0029293369627614563\n",
      "Iteration: 3401 lambda_k: 1 Loss: 0.0029293373097878496\n",
      "Iteration: 3402 lambda_k: 1 Loss: 0.002929337655120476\n",
      "Iteration: 3403 lambda_k: 1 Loss: 0.0029293379987676326\n",
      "Iteration: 3404 lambda_k: 1 Loss: 0.002929338340737565\n",
      "Iteration: 3405 lambda_k: 1 Loss: 0.0029293386810384796\n",
      "Iteration: 3406 lambda_k: 1 Loss: 0.0029293390196785665\n",
      "Iteration: 3407 lambda_k: 1 Loss: 0.002929339356665927\n",
      "Iteration: 3408 lambda_k: 1 Loss: 0.0029293396920086596\n",
      "Iteration: 3409 lambda_k: 1 Loss: 0.002929340025714828\n",
      "Iteration: 3410 lambda_k: 1 Loss: 0.0029293403577924436\n",
      "Iteration: 3411 lambda_k: 1 Loss: 0.0029293406882494924\n",
      "Iteration: 3412 lambda_k: 1 Loss: 0.0029293410170938778\n",
      "Iteration: 3413 lambda_k: 1 Loss: 0.002929341344333532\n",
      "Iteration: 3414 lambda_k: 1 Loss: 0.002929341669976271\n",
      "Iteration: 3415 lambda_k: 1 Loss: 0.0029293419940299315\n",
      "Iteration: 3416 lambda_k: 1 Loss: 0.002929342316502297\n",
      "Iteration: 3417 lambda_k: 1 Loss: 0.002929342637401093\n",
      "Iteration: 3418 lambda_k: 1 Loss: 0.002929342956734032\n",
      "Iteration: 3419 lambda_k: 1 Loss: 0.0029293432745087717\n",
      "Iteration: 3420 lambda_k: 1 Loss: 0.0029293435907329413\n",
      "Iteration: 3421 lambda_k: 1 Loss: 0.0029293439054141287\n",
      "Iteration: 3422 lambda_k: 1 Loss: 0.0029293442185599067\n",
      "Iteration: 3423 lambda_k: 1 Loss: 0.0029293445301777645\n",
      "Iteration: 3424 lambda_k: 1 Loss: 0.0029293448402751875\n",
      "Iteration: 3425 lambda_k: 1 Loss: 0.002929345148859621\n",
      "Iteration: 3426 lambda_k: 1 Loss: 0.0029293454559384717\n",
      "Iteration: 3427 lambda_k: 1 Loss: 0.0029293457615191015\n",
      "Iteration: 3428 lambda_k: 1 Loss: 0.002929346065608857\n",
      "Iteration: 3429 lambda_k: 1 Loss: 0.0029293463682150147\n",
      "Iteration: 3430 lambda_k: 1 Loss: 0.002929346669344836\n",
      "Iteration: 3431 lambda_k: 1 Loss: 0.002929346969005558\n",
      "Iteration: 3432 lambda_k: 1 Loss: 0.002929347267204376\n",
      "Iteration: 3433 lambda_k: 1 Loss: 0.002929347563948426\n",
      "Iteration: 3434 lambda_k: 1 Loss: 0.002929347859244831\n",
      "Iteration: 3435 lambda_k: 1 Loss: 0.0029293481531006815\n",
      "Iteration: 3436 lambda_k: 1 Loss: 0.002929348445523034\n",
      "Iteration: 3437 lambda_k: 1 Loss: 0.0029293487365188845\n",
      "Iteration: 3438 lambda_k: 1 Loss: 0.002929349026095229\n",
      "Iteration: 3439 lambda_k: 1 Loss: 0.002929349314259009\n",
      "Iteration: 3440 lambda_k: 1 Loss: 0.0029293496010171344\n",
      "Iteration: 3441 lambda_k: 1 Loss: 0.0029293498863764717\n",
      "Iteration: 3442 lambda_k: 1 Loss: 0.0029293501703438972\n",
      "Iteration: 3443 lambda_k: 1 Loss: 0.0029293504529261877\n",
      "Iteration: 3444 lambda_k: 1 Loss: 0.002929350734130139\n",
      "Iteration: 3445 lambda_k: 1 Loss: 0.002929351013962472\n",
      "Iteration: 3446 lambda_k: 1 Loss: 0.002929351292429935\n",
      "Iteration: 3447 lambda_k: 1 Loss: 0.002929351569539183\n",
      "Iteration: 3448 lambda_k: 1 Loss: 0.0029293518452968683\n",
      "Iteration: 3449 lambda_k: 1 Loss: 0.0029293521197096056\n",
      "Iteration: 3450 lambda_k: 1 Loss: 0.0029293523927839686\n",
      "Iteration: 3451 lambda_k: 1 Loss: 0.002929352664526492\n",
      "Iteration: 3452 lambda_k: 1 Loss: 0.002929352934943708\n",
      "Iteration: 3453 lambda_k: 1 Loss: 0.0029293532040421013\n",
      "Iteration: 3454 lambda_k: 1 Loss: 0.0029293534718281273\n",
      "Iteration: 3455 lambda_k: 1 Loss: 0.0029293537383081876\n",
      "Iteration: 3456 lambda_k: 1 Loss: 0.0029293540034886884\n",
      "Iteration: 3457 lambda_k: 1 Loss: 0.002929354267375975\n",
      "Iteration: 3458 lambda_k: 1 Loss: 0.0029293545299763743\n",
      "Iteration: 3459 lambda_k: 1 Loss: 0.0029293547912961813\n",
      "Iteration: 3460 lambda_k: 1 Loss: 0.0029293550513416716\n",
      "Iteration: 3461 lambda_k: 1 Loss: 0.002929355310119057\n",
      "Iteration: 3462 lambda_k: 1 Loss: 0.002929355567634565\n",
      "Iteration: 3463 lambda_k: 1 Loss: 0.002929355823894358\n",
      "Iteration: 3464 lambda_k: 1 Loss: 0.002929356078904568\n",
      "Iteration: 3465 lambda_k: 1 Loss: 0.002929356332671318\n",
      "Iteration: 3466 lambda_k: 1 Loss: 0.0029293565852007114\n",
      "Iteration: 3467 lambda_k: 1 Loss: 0.0029293568364987684\n",
      "Iteration: 3468 lambda_k: 1 Loss: 0.0029293570865715083\n",
      "Iteration: 3469 lambda_k: 1 Loss: 0.0029293573354249344\n",
      "Iteration: 3470 lambda_k: 1 Loss: 0.002929357583065026\n",
      "Iteration: 3471 lambda_k: 1 Loss: 0.0029293578294976854\n",
      "Iteration: 3472 lambda_k: 1 Loss: 0.002929358074728837\n",
      "Iteration: 3473 lambda_k: 1 Loss: 0.002929358318764355\n",
      "Iteration: 3474 lambda_k: 1 Loss: 0.0029293585616100896\n",
      "Iteration: 3475 lambda_k: 1 Loss: 0.002929358803271848\n",
      "Iteration: 3476 lambda_k: 1 Loss: 0.0029293590437554336\n",
      "Iteration: 3477 lambda_k: 1 Loss: 0.0029293592830665995\n",
      "Iteration: 3478 lambda_k: 1 Loss: 0.002929359521211076\n",
      "Iteration: 3479 lambda_k: 1 Loss: 0.0029293597581945695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3480 lambda_k: 1 Loss: 0.0029293599940227654\n",
      "Iteration: 3481 lambda_k: 1 Loss: 0.0029293602287013103\n",
      "Iteration: 3482 lambda_k: 1 Loss: 0.0029293604622358358\n",
      "Iteration: 3483 lambda_k: 1 Loss: 0.0029293606946319164\n",
      "Iteration: 3484 lambda_k: 1 Loss: 0.0029293609258951225\n",
      "Iteration: 3485 lambda_k: 1 Loss: 0.0029293611560310038\n",
      "Iteration: 3486 lambda_k: 1 Loss: 0.0029293613850450467\n",
      "Iteration: 3487 lambda_k: 1 Loss: 0.0029293616129427652\n",
      "Iteration: 3488 lambda_k: 1 Loss: 0.0029293618397296163\n",
      "Iteration: 3489 lambda_k: 1 Loss: 0.0029293620654110247\n",
      "Iteration: 3490 lambda_k: 1 Loss: 0.0029293622899923925\n",
      "Iteration: 3491 lambda_k: 1 Loss: 0.0029293625134790948\n",
      "Iteration: 3492 lambda_k: 1 Loss: 0.0029293627358764848\n",
      "Iteration: 3493 lambda_k: 1 Loss: 0.0029293629571898777\n",
      "Iteration: 3494 lambda_k: 1 Loss: 0.0029293631774246014\n",
      "Iteration: 3495 lambda_k: 1 Loss: 0.0029293633965859116\n",
      "Iteration: 3496 lambda_k: 1 Loss: 0.0029293636146790606\n",
      "Iteration: 3497 lambda_k: 1 Loss: 0.0029293638317092626\n",
      "Iteration: 3498 lambda_k: 1 Loss: 0.0029293640476817234\n",
      "Iteration: 3499 lambda_k: 1 Loss: 0.0029293642626015974\n",
      "Iteration: 3500 lambda_k: 1 Loss: 0.002929364476474032\n",
      "Iteration: 3501 lambda_k: 1 Loss: 0.0029293646893041574\n",
      "Iteration: 3502 lambda_k: 1 Loss: 0.0029293649010970744\n",
      "Iteration: 3503 lambda_k: 1 Loss: 0.002929365111857842\n",
      "Iteration: 3504 lambda_k: 1 Loss: 0.0029293653215915023\n",
      "Iteration: 3505 lambda_k: 1 Loss: 0.0029293655303030924\n",
      "Iteration: 3506 lambda_k: 1 Loss: 0.0029293657379976133\n",
      "Iteration: 3507 lambda_k: 1 Loss: 0.0029293659446800084\n",
      "Iteration: 3508 lambda_k: 1 Loss: 0.0029293661503552314\n",
      "Iteration: 3509 lambda_k: 1 Loss: 0.002929366355028219\n",
      "Iteration: 3510 lambda_k: 1 Loss: 0.002929366558703863\n",
      "Iteration: 3511 lambda_k: 1 Loss: 0.0029293667613870472\n",
      "Iteration: 3512 lambda_k: 1 Loss: 0.0029293669630826077\n",
      "Iteration: 3513 lambda_k: 1 Loss: 0.0029293671637953835\n",
      "Iteration: 3514 lambda_k: 1 Loss: 0.002929367363530162\n",
      "Iteration: 3515 lambda_k: 1 Loss: 0.002929367562291731\n",
      "Iteration: 3516 lambda_k: 1 Loss: 0.0029293677600848466\n",
      "Iteration: 3517 lambda_k: 1 Loss: 0.0029293679569142293\n",
      "Iteration: 3518 lambda_k: 1 Loss: 0.002929368152784599\n",
      "Iteration: 3519 lambda_k: 1 Loss: 0.002929368347700661\n",
      "Iteration: 3520 lambda_k: 1 Loss: 0.00292936854166706\n",
      "Iteration: 3521 lambda_k: 1 Loss: 0.0029293687346884445\n",
      "Iteration: 3522 lambda_k: 1 Loss: 0.002929368926769417\n",
      "Iteration: 3523 lambda_k: 1 Loss: 0.0029293691179145855\n",
      "Iteration: 3524 lambda_k: 1 Loss: 0.002929369308128533\n",
      "Iteration: 3525 lambda_k: 1 Loss: 0.0029293694974157902\n",
      "Iteration: 3526 lambda_k: 1 Loss: 0.002929369685780895\n",
      "Iteration: 3527 lambda_k: 1 Loss: 0.002929369873228335\n",
      "Iteration: 3528 lambda_k: 1 Loss: 0.00292937005976261\n",
      "Iteration: 3529 lambda_k: 1 Loss: 0.0029293702453881847\n",
      "Iteration: 3530 lambda_k: 1 Loss: 0.0029293704301094924\n",
      "Iteration: 3531 lambda_k: 1 Loss: 0.002929370613930957\n",
      "Iteration: 3532 lambda_k: 1 Loss: 0.002929370796856972\n",
      "Iteration: 3533 lambda_k: 1 Loss: 0.00292937097889191\n",
      "Iteration: 3534 lambda_k: 1 Loss: 0.0029293711600401283\n",
      "Iteration: 3535 lambda_k: 1 Loss: 0.0029293713403059645\n",
      "Iteration: 3536 lambda_k: 1 Loss: 0.0029293715196937174\n",
      "Iteration: 3537 lambda_k: 1 Loss: 0.002929371698207688\n",
      "Iteration: 3538 lambda_k: 1 Loss: 0.0029293718758521417\n",
      "Iteration: 3539 lambda_k: 1 Loss: 0.0029293720526313133\n",
      "Iteration: 3540 lambda_k: 1 Loss: 0.002929372228549452\n",
      "Iteration: 3541 lambda_k: 1 Loss: 0.002929372403610756\n",
      "Iteration: 3542 lambda_k: 1 Loss: 0.002929372577819406\n",
      "Iteration: 3543 lambda_k: 1 Loss: 0.0029293727511795695\n",
      "Iteration: 3544 lambda_k: 1 Loss: 0.002929372923695398\n",
      "Iteration: 3545 lambda_k: 1 Loss: 0.0029293730953709987\n",
      "Iteration: 3546 lambda_k: 1 Loss: 0.0029293732662104984\n",
      "Iteration: 3547 lambda_k: 1 Loss: 0.002929373436217958\n",
      "Iteration: 3548 lambda_k: 1 Loss: 0.00292937360539745\n",
      "Iteration: 3549 lambda_k: 1 Loss: 0.00292937377375302\n",
      "Iteration: 3550 lambda_k: 1 Loss: 0.002929373941288696\n",
      "Iteration: 3551 lambda_k: 1 Loss: 0.002929374108008482\n",
      "Iteration: 3552 lambda_k: 1 Loss: 0.0029293742739163655\n",
      "Iteration: 3553 lambda_k: 1 Loss: 0.002929374439016309\n",
      "Iteration: 3554 lambda_k: 1 Loss: 0.0029293746033122498\n",
      "Iteration: 3555 lambda_k: 1 Loss: 0.0029293747668081177\n",
      "Iteration: 3556 lambda_k: 1 Loss: 0.0029293749295078294\n",
      "Iteration: 3557 lambda_k: 1 Loss: 0.002929375091415272\n",
      "Iteration: 3558 lambda_k: 1 Loss: 0.002929375252534314\n",
      "Iteration: 3559 lambda_k: 1 Loss: 0.002929375412868808\n",
      "Iteration: 3560 lambda_k: 1 Loss: 0.002929375572422582\n",
      "Iteration: 3561 lambda_k: 1 Loss: 0.0029293757311994375\n",
      "Iteration: 3562 lambda_k: 1 Loss: 0.0029293758892031713\n",
      "Iteration: 3563 lambda_k: 1 Loss: 0.0029293760464375886\n",
      "Iteration: 3564 lambda_k: 1 Loss: 0.002929376202906408\n",
      "Iteration: 3565 lambda_k: 1 Loss: 0.0029293763586133925\n",
      "Iteration: 3566 lambda_k: 1 Loss: 0.0029293765135622513\n",
      "Iteration: 3567 lambda_k: 1 Loss: 0.002929376667756691\n",
      "Iteration: 3568 lambda_k: 1 Loss: 0.0029293768212003985\n",
      "Iteration: 3569 lambda_k: 1 Loss: 0.002929376973897021\n",
      "Iteration: 3570 lambda_k: 1 Loss: 0.0029293771258502286\n",
      "Iteration: 3571 lambda_k: 1 Loss: 0.0029293772770636497\n",
      "Iteration: 3572 lambda_k: 1 Loss: 0.0029293774275408867\n",
      "Iteration: 3573 lambda_k: 1 Loss: 0.002929377577285538\n",
      "Iteration: 3574 lambda_k: 1 Loss: 0.0029293777263011763\n",
      "Iteration: 3575 lambda_k: 1 Loss: 0.0029293778745913485\n",
      "Iteration: 3576 lambda_k: 1 Loss: 0.002929378022159625\n",
      "Iteration: 3577 lambda_k: 1 Loss: 0.002929378169009511\n",
      "Iteration: 3578 lambda_k: 1 Loss: 0.0029293783151445245\n",
      "Iteration: 3579 lambda_k: 1 Loss: 0.0029293784605681425\n",
      "Iteration: 3580 lambda_k: 1 Loss: 0.002929378605283852\n",
      "Iteration: 3581 lambda_k: 1 Loss: 0.0029293787492951254\n",
      "Iteration: 3582 lambda_k: 1 Loss: 0.002929378892605368\n",
      "Iteration: 3583 lambda_k: 1 Loss: 0.0029293790352180123\n",
      "Iteration: 3584 lambda_k: 1 Loss: 0.0029293791771364797\n",
      "Iteration: 3585 lambda_k: 1 Loss: 0.002929379318364141\n",
      "Iteration: 3586 lambda_k: 1 Loss: 0.002929379458904374\n",
      "Iteration: 3587 lambda_k: 1 Loss: 0.0029293795987605266\n",
      "Iteration: 3588 lambda_k: 1 Loss: 0.0029293797379359443\n",
      "Iteration: 3589 lambda_k: 1 Loss: 0.002929379876433957\n",
      "Iteration: 3590 lambda_k: 1 Loss: 0.002929380014257868\n",
      "Iteration: 3591 lambda_k: 1 Loss: 0.002929380151410963\n",
      "Iteration: 3592 lambda_k: 1 Loss: 0.0029293802878965205\n",
      "Iteration: 3593 lambda_k: 1 Loss: 0.002929380423717789\n",
      "Iteration: 3594 lambda_k: 1 Loss: 0.0029293805588780255\n",
      "Iteration: 3595 lambda_k: 1 Loss: 0.002929380693380447\n",
      "Iteration: 3596 lambda_k: 1 Loss: 0.002929380827228272\n",
      "Iteration: 3597 lambda_k: 1 Loss: 0.002929380960424686\n",
      "Iteration: 3598 lambda_k: 1 Loss: 0.0029293810929728677\n",
      "Iteration: 3599 lambda_k: 1 Loss: 0.0029293812248759993\n",
      "Iteration: 3600 lambda_k: 1 Loss: 0.0029293813561372035\n",
      "Iteration: 3601 lambda_k: 1 Loss: 0.002929381486759633\n",
      "Iteration: 3602 lambda_k: 1 Loss: 0.0029293816167464026\n",
      "Iteration: 3603 lambda_k: 1 Loss: 0.0029293817461006023\n",
      "Iteration: 3604 lambda_k: 1 Loss: 0.00292938187482533\n",
      "Iteration: 3605 lambda_k: 1 Loss: 0.002929382002923677\n",
      "Iteration: 3606 lambda_k: 1 Loss: 0.0029293821303986766\n",
      "Iteration: 3607 lambda_k: 1 Loss: 0.002929382257253352\n",
      "Iteration: 3608 lambda_k: 1 Loss: 0.0029293823834907707\n",
      "Iteration: 3609 lambda_k: 1 Loss: 0.002929382509113912\n",
      "Iteration: 3610 lambda_k: 1 Loss: 0.0029293826341257987\n",
      "Iteration: 3611 lambda_k: 1 Loss: 0.0029293827585293885\n",
      "Iteration: 3612 lambda_k: 1 Loss: 0.0029293828823276544\n",
      "Iteration: 3613 lambda_k: 1 Loss: 0.0029293830055235697\n",
      "Iteration: 3614 lambda_k: 1 Loss: 0.0029293831281200547\n",
      "Iteration: 3615 lambda_k: 1 Loss: 0.002929383250120048\n",
      "Iteration: 3616 lambda_k: 1 Loss: 0.002929383371526452\n",
      "Iteration: 3617 lambda_k: 1 Loss: 0.0029293834923421773\n",
      "Iteration: 3618 lambda_k: 1 Loss: 0.002929383612570082\n",
      "Iteration: 3619 lambda_k: 1 Loss: 0.0029293837322130452\n",
      "Iteration: 3620 lambda_k: 1 Loss: 0.0029293838512739236\n",
      "Iteration: 3621 lambda_k: 1 Loss: 0.0029293839697555526\n",
      "Iteration: 3622 lambda_k: 1 Loss: 0.0029293840876607564\n",
      "Iteration: 3623 lambda_k: 1 Loss: 0.0029293842049923583\n",
      "Iteration: 3624 lambda_k: 1 Loss: 0.0029293843217531555\n",
      "Iteration: 3625 lambda_k: 1 Loss: 0.002929384437945923\n",
      "Iteration: 3626 lambda_k: 1 Loss: 0.0029293845535734364\n",
      "Iteration: 3627 lambda_k: 1 Loss: 0.002929384668638461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3628 lambda_k: 1 Loss: 0.002929384783143733\n",
      "Iteration: 3629 lambda_k: 1 Loss: 0.002929384897091987\n",
      "Iteration: 3630 lambda_k: 1 Loss: 0.00292938501048593\n",
      "Iteration: 3631 lambda_k: 1 Loss: 0.0029293851233282617\n",
      "Iteration: 3632 lambda_k: 1 Loss: 0.0029293852356216937\n",
      "Iteration: 3633 lambda_k: 1 Loss: 0.0029293853473688933\n",
      "Iteration: 3634 lambda_k: 1 Loss: 0.0029293854585725303\n",
      "Iteration: 3635 lambda_k: 1 Loss: 0.002929385569235258\n",
      "Iteration: 3636 lambda_k: 1 Loss: 0.0029293856793597\n",
      "Iteration: 3637 lambda_k: 1 Loss: 0.002929385788948497\n",
      "Iteration: 3638 lambda_k: 1 Loss: 0.0029293858980042565\n",
      "Iteration: 3639 lambda_k: 1 Loss: 0.002929386006529579\n",
      "Iteration: 3640 lambda_k: 1 Loss: 0.0029293861145270584\n",
      "Iteration: 3641 lambda_k: 1 Loss: 0.002929386221999249\n",
      "Iteration: 3642 lambda_k: 1 Loss: 0.002929386328948725\n",
      "Iteration: 3643 lambda_k: 1 Loss: 0.002929386435378035\n",
      "Iteration: 3644 lambda_k: 1 Loss: 0.002929386541289715\n",
      "Iteration: 3645 lambda_k: 1 Loss: 0.0029293866466863095\n",
      "Iteration: 3646 lambda_k: 1 Loss: 0.002929386751570313\n",
      "Iteration: 3647 lambda_k: 1 Loss: 0.0029293868559442126\n",
      "Iteration: 3648 lambda_k: 1 Loss: 0.0029293869598105084\n",
      "Iteration: 3649 lambda_k: 1 Loss: 0.0029293870631716835\n",
      "Iteration: 3650 lambda_k: 1 Loss: 0.002929387166030199\n",
      "Iteration: 3651 lambda_k: 1 Loss: 0.002929387268388498\n",
      "Iteration: 3652 lambda_k: 1 Loss: 0.0029293873702490275\n",
      "Iteration: 3653 lambda_k: 1 Loss: 0.002929387471614204\n",
      "Iteration: 3654 lambda_k: 1 Loss: 0.002929387572486453\n",
      "Iteration: 3655 lambda_k: 1 Loss: 0.002929387672868175\n",
      "Iteration: 3656 lambda_k: 1 Loss: 0.0029293877727617633\n",
      "Iteration: 3657 lambda_k: 1 Loss: 0.0029293878721696072\n",
      "Iteration: 3658 lambda_k: 1 Loss: 0.002929387971094053\n",
      "Iteration: 3659 lambda_k: 1 Loss: 0.002929388069537482\n",
      "Iteration: 3660 lambda_k: 1 Loss: 0.002929388167502224\n",
      "Iteration: 3661 lambda_k: 1 Loss: 0.002929388264990602\n",
      "Iteration: 3662 lambda_k: 1 Loss: 0.0029293883620049688\n",
      "Iteration: 3663 lambda_k: 1 Loss: 0.0029293884585476207\n",
      "Iteration: 3664 lambda_k: 1 Loss: 0.002929388554620857\n",
      "Iteration: 3665 lambda_k: 1 Loss: 0.0029293886502269636\n",
      "Iteration: 3666 lambda_k: 1 Loss: 0.0029293887453682188\n",
      "Iteration: 3667 lambda_k: 1 Loss: 0.0029293888400468973\n",
      "Iteration: 3668 lambda_k: 1 Loss: 0.002929388934265251\n",
      "Iteration: 3669 lambda_k: 1 Loss: 0.002929389028025529\n",
      "Iteration: 3670 lambda_k: 1 Loss: 0.002929389121329957\n",
      "Iteration: 3671 lambda_k: 1 Loss: 0.0029293892141807575\n",
      "Iteration: 3672 lambda_k: 1 Loss: 0.002929389306580139\n",
      "Iteration: 3673 lambda_k: 1 Loss: 0.002929389398530296\n",
      "Iteration: 3674 lambda_k: 1 Loss: 0.0029293894900334397\n",
      "Iteration: 3675 lambda_k: 1 Loss: 0.0029293895810917356\n",
      "Iteration: 3676 lambda_k: 1 Loss: 0.0029293896717073565\n",
      "Iteration: 3677 lambda_k: 1 Loss: 0.002929389761882468\n",
      "Iteration: 3678 lambda_k: 1 Loss: 0.0029293898516191924\n",
      "Iteration: 3679 lambda_k: 1 Loss: 0.0029293899409196895\n",
      "Iteration: 3680 lambda_k: 1 Loss: 0.002929390029786088\n",
      "Iteration: 3681 lambda_k: 1 Loss: 0.0029293901182204877\n",
      "Iteration: 3682 lambda_k: 1 Loss: 0.0029293902062250093\n",
      "Iteration: 3683 lambda_k: 1 Loss: 0.0029293902938017343\n",
      "Iteration: 3684 lambda_k: 1 Loss: 0.002929390380952758\n",
      "Iteration: 3685 lambda_k: 1 Loss: 0.002929390467680157\n",
      "Iteration: 3686 lambda_k: 1 Loss: 0.0029293905539859734\n",
      "Iteration: 3687 lambda_k: 1 Loss: 0.0029293906398722837\n",
      "Iteration: 3688 lambda_k: 1 Loss: 0.002929390725341129\n",
      "Iteration: 3689 lambda_k: 1 Loss: 0.0029293908103945484\n",
      "Iteration: 3690 lambda_k: 1 Loss: 0.002929390895034546\n",
      "Iteration: 3691 lambda_k: 1 Loss: 0.0029293909792631636\n",
      "Iteration: 3692 lambda_k: 1 Loss: 0.0029293910630823947\n",
      "Iteration: 3693 lambda_k: 1 Loss: 0.0029293911464942347\n",
      "Iteration: 3694 lambda_k: 1 Loss: 0.0029293912295006636\n",
      "Iteration: 3695 lambda_k: 1 Loss: 0.0029293913121036614\n",
      "Iteration: 3696 lambda_k: 1 Loss: 0.0029293913943051904\n",
      "Iteration: 3697 lambda_k: 1 Loss: 0.002929391476107227\n",
      "Iteration: 3698 lambda_k: 1 Loss: 0.0029293915575117125\n",
      "Iteration: 3699 lambda_k: 1 Loss: 0.002929391638520576\n",
      "Iteration: 3700 lambda_k: 1 Loss: 0.0029293917191357275\n",
      "Iteration: 3701 lambda_k: 1 Loss: 0.0029293917993591123\n",
      "Iteration: 3702 lambda_k: 1 Loss: 0.0029293918791926383\n",
      "Iteration: 3703 lambda_k: 1 Loss: 0.0029293919586381844\n",
      "Iteration: 3704 lambda_k: 1 Loss: 0.00292939203769766\n",
      "Iteration: 3705 lambda_k: 1 Loss: 0.002929392116372933\n",
      "Iteration: 3706 lambda_k: 1 Loss: 0.0029293921946658944\n",
      "Iteration: 3707 lambda_k: 1 Loss: 0.002929392272578394\n",
      "Iteration: 3708 lambda_k: 1 Loss: 0.0029293923501122992\n",
      "Iteration: 3709 lambda_k: 1 Loss: 0.0029293924272694384\n",
      "Iteration: 3710 lambda_k: 1 Loss: 0.0029293925040516414\n",
      "Iteration: 3711 lambda_k: 1 Loss: 0.0029293925804607505\n",
      "Iteration: 3712 lambda_k: 1 Loss: 0.002929392656498579\n",
      "Iteration: 3713 lambda_k: 1 Loss: 0.0029293927321669276\n",
      "Iteration: 3714 lambda_k: 1 Loss: 0.002929392807467608\n",
      "Iteration: 3715 lambda_k: 1 Loss: 0.0029293928824024153\n",
      "Iteration: 3716 lambda_k: 1 Loss: 0.0029293929569731195\n",
      "Iteration: 3717 lambda_k: 1 Loss: 0.002929393031181507\n",
      "Iteration: 3718 lambda_k: 1 Loss: 0.002929393105029317\n",
      "Iteration: 3719 lambda_k: 1 Loss: 0.002929393178518324\n",
      "Iteration: 3720 lambda_k: 1 Loss: 0.0029293932516502924\n",
      "Iteration: 3721 lambda_k: 1 Loss: 0.002929393324426949\n",
      "Iteration: 3722 lambda_k: 1 Loss: 0.0029293933968500318\n",
      "Iteration: 3723 lambda_k: 1 Loss: 0.0029293934689212232\n",
      "Iteration: 3724 lambda_k: 1 Loss: 0.0029293935406422816\n",
      "Iteration: 3725 lambda_k: 1 Loss: 0.00292939361201488\n",
      "Iteration: 3726 lambda_k: 1 Loss: 0.0029293936830407513\n",
      "Iteration: 3727 lambda_k: 1 Loss: 0.0029293937537215693\n",
      "Iteration: 3728 lambda_k: 1 Loss: 0.0029293938240590126\n",
      "Iteration: 3729 lambda_k: 1 Loss: 0.0029293938940547467\n",
      "Iteration: 3730 lambda_k: 1 Loss: 0.002929393963710459\n",
      "Iteration: 3731 lambda_k: 1 Loss: 0.0029293940330277785\n",
      "Iteration: 3732 lambda_k: 1 Loss: 0.002929394102008376\n",
      "Iteration: 3733 lambda_k: 1 Loss: 0.0029293941706538752\n",
      "Iteration: 3734 lambda_k: 1 Loss: 0.002929394238965909\n",
      "Iteration: 3735 lambda_k: 1 Loss: 0.002929394306946127\n",
      "Iteration: 3736 lambda_k: 1 Loss: 0.002929394374596116\n",
      "Iteration: 3737 lambda_k: 1 Loss: 0.002929394441917486\n",
      "Iteration: 3738 lambda_k: 1 Loss: 0.0029293945089118568\n",
      "Iteration: 3739 lambda_k: 1 Loss: 0.0029293945755808036\n",
      "Iteration: 3740 lambda_k: 1 Loss: 0.0029293946419259097\n",
      "Iteration: 3741 lambda_k: 1 Loss: 0.0029293947079487635\n",
      "Iteration: 3742 lambda_k: 1 Loss: 0.0029293947736509355\n",
      "Iteration: 3743 lambda_k: 1 Loss: 0.0029293948390339717\n",
      "Iteration: 3744 lambda_k: 1 Loss: 0.0029293949040994365\n",
      "Iteration: 3745 lambda_k: 1 Loss: 0.002929394968848875\n",
      "Iteration: 3746 lambda_k: 1 Loss: 0.0029293950332838397\n",
      "Iteration: 3747 lambda_k: 1 Loss: 0.002929395097405845\n",
      "Iteration: 3748 lambda_k: 1 Loss: 0.0029293951612164187\n",
      "Iteration: 3749 lambda_k: 1 Loss: 0.0029293952247170783\n",
      "Iteration: 3750 lambda_k: 1 Loss: 0.0029293952879093426\n",
      "Iteration: 3751 lambda_k: 1 Loss: 0.002929395350794705\n",
      "Iteration: 3752 lambda_k: 1 Loss: 0.002929395413374672\n",
      "Iteration: 3753 lambda_k: 1 Loss: 0.0029293954756506963\n",
      "Iteration: 3754 lambda_k: 1 Loss: 0.002929395537624297\n",
      "Iteration: 3755 lambda_k: 1 Loss: 0.0029293955992969237\n",
      "Iteration: 3756 lambda_k: 1 Loss: 0.0029293956606700486\n",
      "Iteration: 3757 lambda_k: 1 Loss: 0.0029293957217451337\n",
      "Iteration: 3758 lambda_k: 1 Loss: 0.0029293957825236227\n",
      "Iteration: 3759 lambda_k: 1 Loss: 0.0029293958430069607\n",
      "Iteration: 3760 lambda_k: 1 Loss: 0.0029293959031965926\n",
      "Iteration: 3761 lambda_k: 1 Loss: 0.0029293959630939397\n",
      "Iteration: 3762 lambda_k: 1 Loss: 0.002929396022700436\n",
      "Iteration: 3763 lambda_k: 1 Loss: 0.002929396082017487\n",
      "Iteration: 3764 lambda_k: 1 Loss: 0.002929396141046502\n",
      "Iteration: 3765 lambda_k: 1 Loss: 0.002929396199788879\n",
      "Iteration: 3766 lambda_k: 1 Loss: 0.0029293962582460304\n",
      "Iteration: 3767 lambda_k: 1 Loss: 0.0029293963164193438\n",
      "Iteration: 3768 lambda_k: 1 Loss: 0.0029293963743101696\n",
      "Iteration: 3769 lambda_k: 1 Loss: 0.0029293964319199238\n",
      "Iteration: 3770 lambda_k: 1 Loss: 0.0029293964892499413\n",
      "Iteration: 3771 lambda_k: 1 Loss: 0.002929396546301605\n",
      "Iteration: 3772 lambda_k: 1 Loss: 0.0029293966030762638\n",
      "Iteration: 3773 lambda_k: 1 Loss: 0.0029293966595752706\n",
      "Iteration: 3774 lambda_k: 1 Loss: 0.002929396715799961\n",
      "Iteration: 3775 lambda_k: 1 Loss: 0.00292939677175167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3776 lambda_k: 1 Loss: 0.0029293968274317324\n",
      "Iteration: 3777 lambda_k: 1 Loss: 0.0029293968828414817\n",
      "Iteration: 3778 lambda_k: 1 Loss: 0.002929396937982212\n",
      "Iteration: 3779 lambda_k: 1 Loss: 0.002929396992855225\n",
      "Iteration: 3780 lambda_k: 1 Loss: 0.002929397047461847\n",
      "Iteration: 3781 lambda_k: 1 Loss: 0.002929397101803362\n",
      "Iteration: 3782 lambda_k: 1 Loss: 0.002929397155881064\n",
      "Iteration: 3783 lambda_k: 1 Loss: 0.002929397209696249\n",
      "Iteration: 3784 lambda_k: 1 Loss: 0.0029293972632501762\n",
      "Iteration: 3785 lambda_k: 1 Loss: 0.002929397316544129\n",
      "Iteration: 3786 lambda_k: 1 Loss: 0.002929397369579379\n",
      "Iteration: 3787 lambda_k: 1 Loss: 0.002929397422357177\n",
      "Iteration: 3788 lambda_k: 1 Loss: 0.00292939747487877\n",
      "Iteration: 3789 lambda_k: 1 Loss: 0.002929397527145407\n",
      "Iteration: 3790 lambda_k: 1 Loss: 0.0029293975791583188\n",
      "Iteration: 3791 lambda_k: 1 Loss: 0.002929397630918767\n",
      "Iteration: 3792 lambda_k: 1 Loss: 0.002929397682427949\n",
      "Iteration: 3793 lambda_k: 1 Loss: 0.002929397733687101\n",
      "Iteration: 3794 lambda_k: 1 Loss: 0.0029293977846974618\n",
      "Iteration: 3795 lambda_k: 1 Loss: 0.0029293978354602212\n",
      "Iteration: 3796 lambda_k: 1 Loss: 0.0029293978859765927\n",
      "Iteration: 3797 lambda_k: 1 Loss: 0.00292939793624776\n",
      "Iteration: 3798 lambda_k: 1 Loss: 0.002929397986274931\n",
      "Iteration: 3799 lambda_k: 1 Loss: 0.002929398036059269\n",
      "Iteration: 3800 lambda_k: 1 Loss: 0.002929398085601996\n",
      "Iteration: 3801 lambda_k: 1 Loss: 0.0029293981349042438\n",
      "Iteration: 3802 lambda_k: 1 Loss: 0.002929398183967215\n",
      "Iteration: 3803 lambda_k: 1 Loss: 0.0029293982327920774\n",
      "Iteration: 3804 lambda_k: 1 Loss: 0.002929398281379965\n",
      "Iteration: 3805 lambda_k: 1 Loss: 0.0029293983297320365\n",
      "Iteration: 3806 lambda_k: 1 Loss: 0.0029293983778494468\n",
      "Iteration: 3807 lambda_k: 1 Loss: 0.002929398425733337\n",
      "Iteration: 3808 lambda_k: 1 Loss: 0.002929398473384847\n",
      "Iteration: 3809 lambda_k: 1 Loss: 0.002929398520805097\n",
      "Iteration: 3810 lambda_k: 1 Loss: 0.0029293985679952205\n",
      "Iteration: 3811 lambda_k: 1 Loss: 0.0029293986149563213\n",
      "Iteration: 3812 lambda_k: 1 Loss: 0.0029293986616895293\n",
      "Iteration: 3813 lambda_k: 1 Loss: 0.002929398708195952\n",
      "Iteration: 3814 lambda_k: 1 Loss: 0.002929398754476685\n",
      "Iteration: 3815 lambda_k: 1 Loss: 0.002929398800532836\n",
      "Iteration: 3816 lambda_k: 1 Loss: 0.002929398846365489\n",
      "Iteration: 3817 lambda_k: 1 Loss: 0.0029293988919757486\n",
      "Iteration: 3818 lambda_k: 1 Loss: 0.0029293989373646613\n",
      "Iteration: 3819 lambda_k: 1 Loss: 0.0029293989825333426\n",
      "Iteration: 3820 lambda_k: 1 Loss: 0.00292939902748284\n",
      "Iteration: 3821 lambda_k: 1 Loss: 0.002929399072214243\n",
      "Iteration: 3822 lambda_k: 1 Loss: 0.002929399116728596\n",
      "Iteration: 3823 lambda_k: 1 Loss: 0.0029293991610269657\n",
      "Iteration: 3824 lambda_k: 1 Loss: 0.002929399205110374\n",
      "Iteration: 3825 lambda_k: 1 Loss: 0.0029293992489798928\n",
      "Iteration: 3826 lambda_k: 1 Loss: 0.002929399292636561\n",
      "Iteration: 3827 lambda_k: 1 Loss: 0.002929399336081401\n",
      "Iteration: 3828 lambda_k: 1 Loss: 0.0029293993793154597\n",
      "Iteration: 3829 lambda_k: 1 Loss: 0.002929399422339751\n",
      "Iteration: 3830 lambda_k: 1 Loss: 0.002929399465155295\n",
      "Iteration: 3831 lambda_k: 1 Loss: 0.0029293995077631046\n",
      "Iteration: 3832 lambda_k: 1 Loss: 0.002929399550164197\n",
      "Iteration: 3833 lambda_k: 1 Loss: 0.002929399592359575\n",
      "Iteration: 3834 lambda_k: 1 Loss: 0.0029293996343502495\n",
      "Iteration: 3835 lambda_k: 1 Loss: 0.0029293996761372127\n",
      "Iteration: 3836 lambda_k: 1 Loss: 0.0029293997177214357\n",
      "Iteration: 3837 lambda_k: 1 Loss: 0.002929399759103913\n",
      "Iteration: 3838 lambda_k: 1 Loss: 0.0029293998002856494\n",
      "Iteration: 3839 lambda_k: 1 Loss: 0.002929399841267591\n",
      "Iteration: 3840 lambda_k: 1 Loss: 0.002929399882050724\n",
      "Iteration: 3841 lambda_k: 1 Loss: 0.002929399922636022\n",
      "Iteration: 3842 lambda_k: 1 Loss: 0.002929399963024439\n",
      "Iteration: 3843 lambda_k: 1 Loss: 0.0029294000032169362\n",
      "Iteration: 3844 lambda_k: 1 Loss: 0.002929400043214459\n",
      "Iteration: 3845 lambda_k: 1 Loss: 0.002929400083017955\n",
      "Iteration: 3846 lambda_k: 1 Loss: 0.002929400122628364\n",
      "Iteration: 3847 lambda_k: 1 Loss: 0.0029294001620466443\n",
      "Iteration: 3848 lambda_k: 1 Loss: 0.0029294002012737125\n",
      "Iteration: 3849 lambda_k: 1 Loss: 0.0029294002403105032\n",
      "Iteration: 3850 lambda_k: 1 Loss: 0.0029294002791579307\n",
      "Iteration: 3851 lambda_k: 1 Loss: 0.0029294003178169394\n",
      "Iteration: 3852 lambda_k: 1 Loss: 0.0029294003562884176\n",
      "Iteration: 3853 lambda_k: 1 Loss: 0.002929400394573311\n",
      "Iteration: 3854 lambda_k: 1 Loss: 0.0029294004326724963\n",
      "Iteration: 3855 lambda_k: 1 Loss: 0.0029294004705868906\n",
      "Iteration: 3856 lambda_k: 1 Loss: 0.0029294005083173903\n",
      "Iteration: 3857 lambda_k: 1 Loss: 0.0029294005458648954\n",
      "Iteration: 3858 lambda_k: 1 Loss: 0.002929400583230292\n",
      "Iteration: 3859 lambda_k: 1 Loss: 0.0029294006204144685\n",
      "Iteration: 3860 lambda_k: 1 Loss: 0.0029294006574183025\n",
      "Iteration: 3861 lambda_k: 1 Loss: 0.0029294006942426763\n",
      "Iteration: 3862 lambda_k: 1 Loss: 0.0029294007308884535\n",
      "Iteration: 3863 lambda_k: 1 Loss: 0.0029294007673564978\n",
      "Iteration: 3864 lambda_k: 1 Loss: 0.0029294008036476835\n",
      "Iteration: 3865 lambda_k: 1 Loss: 0.002929400839762862\n",
      "Iteration: 3866 lambda_k: 1 Loss: 0.0029294008757028926\n",
      "Iteration: 3867 lambda_k: 1 Loss: 0.0029294009114686345\n",
      "Iteration: 3868 lambda_k: 1 Loss: 0.0029294009470609314\n",
      "Iteration: 3869 lambda_k: 1 Loss: 0.002929400982480629\n",
      "Iteration: 3870 lambda_k: 1 Loss: 0.0029294010177285702\n",
      "Iteration: 3871 lambda_k: 1 Loss: 0.002929401052805577\n",
      "Iteration: 3872 lambda_k: 1 Loss: 0.0029294010877124886\n",
      "Iteration: 3873 lambda_k: 1 Loss: 0.0029294011224501207\n",
      "Iteration: 3874 lambda_k: 1 Loss: 0.0029294011570193076\n",
      "Iteration: 3875 lambda_k: 1 Loss: 0.00292940119142086\n",
      "Iteration: 3876 lambda_k: 1 Loss: 0.0029294012256556035\n",
      "Iteration: 3877 lambda_k: 1 Loss: 0.0029294012597243344\n",
      "Iteration: 3878 lambda_k: 1 Loss: 0.0029294012936278814\n",
      "Iteration: 3879 lambda_k: 1 Loss: 0.0029294013273670227\n",
      "Iteration: 3880 lambda_k: 1 Loss: 0.0029294013609425682\n",
      "Iteration: 3881 lambda_k: 1 Loss: 0.00292940139435531\n",
      "Iteration: 3882 lambda_k: 1 Loss: 0.002929401427606045\n",
      "Iteration: 3883 lambda_k: 1 Loss: 0.0029294014606955627\n",
      "Iteration: 3884 lambda_k: 1 Loss: 0.0029294014936246354\n",
      "Iteration: 3885 lambda_k: 1 Loss: 0.0029294015263940567\n",
      "Iteration: 3886 lambda_k: 1 Loss: 0.002929401559004611\n",
      "Iteration: 3887 lambda_k: 1 Loss: 0.002929401591457039\n",
      "Iteration: 3888 lambda_k: 1 Loss: 0.002929401623752127\n",
      "Iteration: 3889 lambda_k: 1 Loss: 0.002929401655890644\n",
      "Iteration: 3890 lambda_k: 1 Loss: 0.0029294016878733445\n",
      "Iteration: 3891 lambda_k: 1 Loss: 0.0029294017197009853\n",
      "Iteration: 3892 lambda_k: 1 Loss: 0.0029294017513743216\n",
      "Iteration: 3893 lambda_k: 1 Loss: 0.0029294017828941045\n",
      "Iteration: 3894 lambda_k: 1 Loss: 0.002929401814261072\n",
      "Iteration: 3895 lambda_k: 1 Loss: 0.0029294018454759717\n",
      "Iteration: 3896 lambda_k: 1 Loss: 0.002929401876539561\n",
      "Iteration: 3897 lambda_k: 1 Loss: 0.0029294019074525557\n",
      "Iteration: 3898 lambda_k: 1 Loss: 0.0029294019382156956\n",
      "Iteration: 3899 lambda_k: 1 Loss: 0.0029294019688296926\n",
      "Iteration: 3900 lambda_k: 1 Loss: 0.00292940199929528\n",
      "Iteration: 3901 lambda_k: 1 Loss: 0.002929402029613182\n",
      "Iteration: 3902 lambda_k: 1 Loss: 0.0029294020597841162\n",
      "Iteration: 3903 lambda_k: 1 Loss: 0.00292940208980881\n",
      "Iteration: 3904 lambda_k: 1 Loss: 0.002929402119687947\n",
      "Iteration: 3905 lambda_k: 1 Loss: 0.002929402149422264\n",
      "Iteration: 3906 lambda_k: 1 Loss: 0.002929402179012436\n",
      "Iteration: 3907 lambda_k: 1 Loss: 0.0029294022084591894\n",
      "Iteration: 3908 lambda_k: 1 Loss: 0.0029294022377631982\n",
      "Iteration: 3909 lambda_k: 1 Loss: 0.0029294022669251675\n",
      "Iteration: 3910 lambda_k: 1 Loss: 0.002929402295945804\n",
      "Iteration: 3911 lambda_k: 1 Loss: 0.002929402324825773\n",
      "Iteration: 3912 lambda_k: 1 Loss: 0.00292940235356576\n",
      "Iteration: 3913 lambda_k: 1 Loss: 0.0029294023821664398\n",
      "Iteration: 3914 lambda_k: 1 Loss: 0.0029294024106285036\n",
      "Iteration: 3915 lambda_k: 1 Loss: 0.0029294024389526136\n",
      "Iteration: 3916 lambda_k: 1 Loss: 0.002929402467139433\n",
      "Iteration: 3917 lambda_k: 1 Loss: 0.002929402495189649\n",
      "Iteration: 3918 lambda_k: 1 Loss: 0.0029294025231039107\n",
      "Iteration: 3919 lambda_k: 1 Loss: 0.0029294025508828964\n",
      "Iteration: 3920 lambda_k: 1 Loss: 0.0029294025785272324\n",
      "Iteration: 3921 lambda_k: 1 Loss: 0.002929402606037616\n",
      "Iteration: 3922 lambda_k: 1 Loss: 0.002929402633414661\n",
      "Iteration: 3923 lambda_k: 1 Loss: 0.0029294026606590286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3924 lambda_k: 1 Loss: 0.00292940268777136\n",
      "Iteration: 3925 lambda_k: 1 Loss: 0.0029294027147522862\n",
      "Iteration: 3926 lambda_k: 1 Loss: 0.0029294027416024663\n",
      "Iteration: 3927 lambda_k: 1 Loss: 0.002929402768322537\n",
      "Iteration: 3928 lambda_k: 1 Loss: 0.002929402794913104\n",
      "Iteration: 3929 lambda_k: 1 Loss: 0.0029294028213748234\n",
      "Iteration: 3930 lambda_k: 1 Loss: 0.0029294028477083065\n",
      "Iteration: 3931 lambda_k: 1 Loss: 0.0029294028739141886\n",
      "Iteration: 3932 lambda_k: 1 Loss: 0.0029294028999930703\n",
      "Iteration: 3933 lambda_k: 1 Loss: 0.002929402925945595\n",
      "Iteration: 3934 lambda_k: 1 Loss: 0.002929402951772354\n",
      "Iteration: 3935 lambda_k: 1 Loss: 0.0029294029774739644\n",
      "Iteration: 3936 lambda_k: 1 Loss: 0.002929403003051049\n",
      "Iteration: 3937 lambda_k: 1 Loss: 0.002929403028504194\n",
      "Iteration: 3938 lambda_k: 1 Loss: 0.002929403053834002\n",
      "Iteration: 3939 lambda_k: 1 Loss: 0.0029294030790410763\n",
      "Iteration: 3940 lambda_k: 1 Loss: 0.002929403104126009\n",
      "Iteration: 3941 lambda_k: 1 Loss: 0.0029294031290893895\n",
      "Iteration: 3942 lambda_k: 1 Loss: 0.0029294031539318253\n",
      "Iteration: 3943 lambda_k: 1 Loss: 0.002929403178653897\n",
      "Iteration: 3944 lambda_k: 1 Loss: 0.002929403203256189\n",
      "Iteration: 3945 lambda_k: 1 Loss: 0.002929403227739278\n",
      "Iteration: 3946 lambda_k: 1 Loss: 0.0029294032521037325\n",
      "Iteration: 3947 lambda_k: 1 Loss: 0.0029294032763501595\n",
      "Iteration: 3948 lambda_k: 1 Loss: 0.002929403300479102\n",
      "Iteration: 3949 lambda_k: 1 Loss: 0.002929403324491146\n",
      "Iteration: 3950 lambda_k: 1 Loss: 0.002929403348386841\n",
      "Iteration: 3951 lambda_k: 1 Loss: 0.0029294033721667862\n",
      "Iteration: 3952 lambda_k: 1 Loss: 0.002929403395831512\n",
      "Iteration: 3953 lambda_k: 1 Loss: 0.002929403419381594\n",
      "Iteration: 3954 lambda_k: 1 Loss: 0.0029294034428175857\n",
      "Iteration: 3955 lambda_k: 1 Loss: 0.0029294034661400516\n",
      "Iteration: 3956 lambda_k: 1 Loss: 0.002929403489349513\n",
      "Iteration: 3957 lambda_k: 1 Loss: 0.0029294035124465437\n",
      "Iteration: 3958 lambda_k: 1 Loss: 0.00292940353543168\n",
      "Iteration: 3959 lambda_k: 1 Loss: 0.002929403558305468\n",
      "Iteration: 3960 lambda_k: 1 Loss: 0.0029294035810684487\n",
      "Iteration: 3961 lambda_k: 1 Loss: 0.002929403603721156\n",
      "Iteration: 3962 lambda_k: 1 Loss: 0.002929403626264143\n",
      "Iteration: 3963 lambda_k: 1 Loss: 0.0029294036486979236\n",
      "Iteration: 3964 lambda_k: 1 Loss: 0.0029294036710230337\n",
      "Iteration: 3965 lambda_k: 1 Loss: 0.002929403693240005\n",
      "Iteration: 3966 lambda_k: 1 Loss: 0.002929403715349351\n",
      "Iteration: 3967 lambda_k: 1 Loss: 0.00292940373735161\n",
      "Iteration: 3968 lambda_k: 1 Loss: 0.002929403759247292\n",
      "Iteration: 3969 lambda_k: 1 Loss: 0.0029294037810369197\n",
      "Iteration: 3970 lambda_k: 1 Loss: 0.002929403802720995\n",
      "Iteration: 3971 lambda_k: 1 Loss: 0.002929403824300057\n",
      "Iteration: 3972 lambda_k: 1 Loss: 0.0029294038457745837\n",
      "Iteration: 3973 lambda_k: 1 Loss: 0.0029294038671451124\n",
      "Iteration: 3974 lambda_k: 1 Loss: 0.0029294038884121365\n",
      "Iteration: 3975 lambda_k: 1 Loss: 0.002929403909576132\n",
      "Iteration: 3976 lambda_k: 1 Loss: 0.002929403930637634\n",
      "Iteration: 3977 lambda_k: 1 Loss: 0.002929403951597138\n",
      "Iteration: 3978 lambda_k: 1 Loss: 0.0029294039724551218\n",
      "Iteration: 3979 lambda_k: 1 Loss: 0.002929403993212088\n",
      "Iteration: 3980 lambda_k: 1 Loss: 0.002929404013868514\n",
      "Iteration: 3981 lambda_k: 1 Loss: 0.0029294040344249117\n",
      "Iteration: 3982 lambda_k: 1 Loss: 0.0029294040548817476\n",
      "Iteration: 3983 lambda_k: 1 Loss: 0.0029294040752395082\n",
      "Iteration: 3984 lambda_k: 1 Loss: 0.0029294040954986797\n",
      "Iteration: 3985 lambda_k: 1 Loss: 0.002929404115659728\n",
      "Iteration: 3986 lambda_k: 1 Loss: 0.0029294041357231515\n",
      "Iteration: 3987 lambda_k: 1 Loss: 0.002929404155689401\n",
      "Iteration: 3988 lambda_k: 1 Loss: 0.002929404175558968\n",
      "Iteration: 3989 lambda_k: 1 Loss: 0.0029294041953322985\n",
      "Iteration: 3990 lambda_k: 1 Loss: 0.0029294042150098693\n",
      "Iteration: 3991 lambda_k: 1 Loss: 0.002929404234592166\n",
      "Iteration: 3992 lambda_k: 1 Loss: 0.0029294042540796393\n",
      "Iteration: 3993 lambda_k: 1 Loss: 0.0029294042734727367\n",
      "Iteration: 3994 lambda_k: 1 Loss: 0.002929404292771929\n",
      "Iteration: 3995 lambda_k: 1 Loss: 0.0029294043119776655\n",
      "Iteration: 3996 lambda_k: 1 Loss: 0.002929404331090409\n",
      "Iteration: 3997 lambda_k: 1 Loss: 0.002929404350110606\n",
      "Iteration: 3998 lambda_k: 1 Loss: 0.0029294043690387037\n",
      "Iteration: 3999 lambda_k: 1 Loss: 0.0029294043878751452\n",
      "Iteration: 4000 lambda_k: 1 Loss: 0.002929404406620367\n",
      "Iteration: 4001 lambda_k: 1 Loss: 0.002929404425274824\n",
      "Iteration: 4002 lambda_k: 1 Loss: 0.00292940444383896\n",
      "Iteration: 4003 lambda_k: 1 Loss: 0.002929404462313204\n",
      "Iteration: 4004 lambda_k: 1 Loss: 0.00292940448069801\n",
      "Iteration: 4005 lambda_k: 1 Loss: 0.0029294044989937934\n",
      "Iteration: 4006 lambda_k: 1 Loss: 0.002929404517200996\n",
      "Iteration: 4007 lambda_k: 1 Loss: 0.002929404535320039\n",
      "Iteration: 4008 lambda_k: 1 Loss: 0.0029294045533513665\n",
      "Iteration: 4009 lambda_k: 1 Loss: 0.0029294045712954016\n",
      "Iteration: 4010 lambda_k: 1 Loss: 0.0029294045891525493\n",
      "Iteration: 4011 lambda_k: 1 Loss: 0.002929404606923254\n",
      "Iteration: 4012 lambda_k: 1 Loss: 0.002929404624607914\n",
      "Iteration: 4013 lambda_k: 1 Loss: 0.002929404642206954\n",
      "Iteration: 4014 lambda_k: 1 Loss: 0.0029294046597207898\n",
      "Iteration: 4015 lambda_k: 1 Loss: 0.0029294046771498484\n",
      "Iteration: 4016 lambda_k: 1 Loss: 0.0029294046944945092\n",
      "Iteration: 4017 lambda_k: 1 Loss: 0.0029294047117552026\n",
      "Iteration: 4018 lambda_k: 1 Loss: 0.002929404728932337\n",
      "Iteration: 4019 lambda_k: 1 Loss: 0.0029294047460263306\n",
      "Iteration: 4020 lambda_k: 1 Loss: 0.002929404763037566\n",
      "Iteration: 4021 lambda_k: 1 Loss: 0.00292940477996645\n",
      "Iteration: 4022 lambda_k: 1 Loss: 0.002929404796813385\n",
      "Iteration: 4023 lambda_k: 1 Loss: 0.0029294048135787634\n",
      "Iteration: 4024 lambda_k: 1 Loss: 0.002929404830262987\n",
      "Iteration: 4025 lambda_k: 1 Loss: 0.002929404846866442\n",
      "Iteration: 4026 lambda_k: 1 Loss: 0.0029294048633895235\n",
      "Iteration: 4027 lambda_k: 1 Loss: 0.0029294048798326356\n",
      "Iteration: 4028 lambda_k: 1 Loss: 0.0029294048961961607\n",
      "Iteration: 4029 lambda_k: 1 Loss: 0.002929404912480459\n",
      "Iteration: 4030 lambda_k: 1 Loss: 0.0029294049286859368\n",
      "Iteration: 4031 lambda_k: 1 Loss: 0.0029294049448129723\n",
      "Iteration: 4032 lambda_k: 1 Loss: 0.002929404960861939\n",
      "Iteration: 4033 lambda_k: 1 Loss: 0.002929404976833223\n",
      "Iteration: 4034 lambda_k: 1 Loss: 0.002929404992727212\n",
      "Iteration: 4035 lambda_k: 1 Loss: 0.002929405008544273\n",
      "Iteration: 4036 lambda_k: 1 Loss: 0.0029294050242847673\n",
      "Iteration: 4037 lambda_k: 1 Loss: 0.002929405039949083\n",
      "Iteration: 4038 lambda_k: 1 Loss: 0.002929405055537575\n",
      "Iteration: 4039 lambda_k: 1 Loss: 0.0029294050710506245\n",
      "Iteration: 4040 lambda_k: 1 Loss: 0.0029294050864885953\n",
      "Iteration: 4041 lambda_k: 1 Loss: 0.0029294051018518433\n",
      "Iteration: 4042 lambda_k: 1 Loss: 0.002929405117140754\n",
      "Iteration: 4043 lambda_k: 1 Loss: 0.002929405132355665\n",
      "Iteration: 4044 lambda_k: 1 Loss: 0.00292940514749694\n",
      "Iteration: 4045 lambda_k: 1 Loss: 0.0029294051625649176\n",
      "Iteration: 4046 lambda_k: 1 Loss: 0.0029294051775599766\n",
      "Iteration: 4047 lambda_k: 1 Loss: 0.0029294051924824743\n",
      "Iteration: 4048 lambda_k: 1 Loss: 0.002929405207332739\n",
      "Iteration: 4049 lambda_k: 1 Loss: 0.002929405222111145\n",
      "Iteration: 4050 lambda_k: 1 Loss: 0.0029294052368180356\n",
      "Iteration: 4051 lambda_k: 1 Loss: 0.002929405251453746\n",
      "Iteration: 4052 lambda_k: 1 Loss: 0.0029294052660186376\n",
      "Iteration: 4053 lambda_k: 1 Loss: 0.0029294052805130403\n",
      "Iteration: 4054 lambda_k: 1 Loss: 0.002929405294937302\n",
      "Iteration: 4055 lambda_k: 1 Loss: 0.002929405309291779\n",
      "Iteration: 4056 lambda_k: 1 Loss: 0.0029294053235767673\n",
      "Iteration: 4057 lambda_k: 1 Loss: 0.0029294053377926454\n",
      "Iteration: 4058 lambda_k: 1 Loss: 0.0029294053519397165\n",
      "Iteration: 4059 lambda_k: 1 Loss: 0.0029294053660183536\n",
      "Iteration: 4060 lambda_k: 1 Loss: 0.0029294053800288586\n",
      "Iteration: 4061 lambda_k: 1 Loss: 0.0029294053939715645\n",
      "Iteration: 4062 lambda_k: 1 Loss: 0.002929405407846808\n",
      "Iteration: 4063 lambda_k: 1 Loss: 0.0029294054216549093\n",
      "Iteration: 4064 lambda_k: 1 Loss: 0.0029294054353961917\n",
      "Iteration: 4065 lambda_k: 1 Loss: 0.0029294054490709926\n",
      "Iteration: 4066 lambda_k: 1 Loss: 0.0029294054626796103\n",
      "Iteration: 4067 lambda_k: 1 Loss: 0.0029294054762223905\n",
      "Iteration: 4068 lambda_k: 1 Loss: 0.0029294054896996627\n",
      "Iteration: 4069 lambda_k: 1 Loss: 0.0029294055031117102\n",
      "Iteration: 4070 lambda_k: 1 Loss: 0.002929405516458869\n",
      "Iteration: 4071 lambda_k: 1 Loss: 0.002929405529741449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4072 lambda_k: 1 Loss: 0.0029294055429597617\n",
      "Iteration: 4073 lambda_k: 1 Loss: 0.002929405556114129\n",
      "Iteration: 4074 lambda_k: 1 Loss: 0.0029294055692048494\n",
      "Iteration: 4075 lambda_k: 1 Loss: 0.002929405582232229\n",
      "Iteration: 4076 lambda_k: 1 Loss: 0.002929405595196593\n",
      "Iteration: 4077 lambda_k: 1 Loss: 0.002929405608098238\n",
      "Iteration: 4078 lambda_k: 1 Loss: 0.0029294056209374446\n",
      "Iteration: 4079 lambda_k: 1 Loss: 0.002929405633714552\n",
      "Iteration: 4080 lambda_k: 1 Loss: 0.0029294056464298384\n",
      "Iteration: 4081 lambda_k: 1 Loss: 0.002929405659083608\n",
      "Iteration: 4082 lambda_k: 1 Loss: 0.0029294056716761634\n",
      "Iteration: 4083 lambda_k: 1 Loss: 0.002929405684207807\n",
      "Iteration: 4084 lambda_k: 1 Loss: 0.002929405696678812\n",
      "Iteration: 4085 lambda_k: 1 Loss: 0.002929405709089494\n",
      "Iteration: 4086 lambda_k: 1 Loss: 0.0029294057214401432\n",
      "Iteration: 4087 lambda_k: 1 Loss: 0.0029294057337310485\n",
      "Iteration: 4088 lambda_k: 1 Loss: 0.0029294057459624996\n",
      "Iteration: 4089 lambda_k: 1 Loss: 0.0029294057581347884\n",
      "Iteration: 4090 lambda_k: 1 Loss: 0.0029294057702481807\n",
      "Iteration: 4091 lambda_k: 1 Loss: 0.002929405782302991\n",
      "Iteration: 4092 lambda_k: 1 Loss: 0.002929405794299474\n",
      "Iteration: 4093 lambda_k: 1 Loss: 0.002929405806237931\n",
      "Iteration: 4094 lambda_k: 1 Loss: 0.002929405818118635\n",
      "Iteration: 4095 lambda_k: 1 Loss: 0.002929405829941878\n",
      "Iteration: 4096 lambda_k: 1 Loss: 0.002929405841707929\n",
      "Iteration: 4097 lambda_k: 1 Loss: 0.0029294058534170783\n",
      "Iteration: 4098 lambda_k: 1 Loss: 0.0029294058650695828\n",
      "Iteration: 4099 lambda_k: 1 Loss: 0.002929405876665722\n",
      "Iteration: 4100 lambda_k: 1 Loss: 0.002929405888205771\n",
      "Iteration: 4101 lambda_k: 1 Loss: 0.0029294058996900086\n",
      "Iteration: 4102 lambda_k: 1 Loss: 0.002929405911118695\n",
      "Iteration: 4103 lambda_k: 1 Loss: 0.002929405922492112\n",
      "Iteration: 4104 lambda_k: 1 Loss: 0.002929405933810516\n",
      "Iteration: 4105 lambda_k: 1 Loss: 0.002929405945074167\n",
      "Iteration: 4106 lambda_k: 1 Loss: 0.002929405956283356\n",
      "Iteration: 4107 lambda_k: 1 Loss: 0.002929405967438336\n",
      "Iteration: 4108 lambda_k: 1 Loss: 0.0029294059785393667\n",
      "Iteration: 4109 lambda_k: 1 Loss: 0.0029294059895867038\n",
      "Iteration: 4110 lambda_k: 1 Loss: 0.002929406000580611\n",
      "Iteration: 4111 lambda_k: 1 Loss: 0.002929406011521343\n",
      "Iteration: 4112 lambda_k: 1 Loss: 0.002929406022409186\n",
      "Iteration: 4113 lambda_k: 1 Loss: 0.002929406033244363\n",
      "Iteration: 4114 lambda_k: 1 Loss: 0.002929406044027123\n",
      "Iteration: 4115 lambda_k: 1 Loss: 0.0029294060547577355\n",
      "Iteration: 4116 lambda_k: 1 Loss: 0.0029294060654364634\n",
      "Iteration: 4117 lambda_k: 1 Loss: 0.002929406076063546\n",
      "Iteration: 4118 lambda_k: 1 Loss: 0.00292940608663924\n",
      "Iteration: 4119 lambda_k: 1 Loss: 0.0029294060971637908\n",
      "Iteration: 4120 lambda_k: 1 Loss: 0.002929406107637454\n",
      "Iteration: 4121 lambda_k: 1 Loss: 0.002929406118060457\n",
      "Iteration: 4122 lambda_k: 1 Loss: 0.0029294061284330587\n",
      "Iteration: 4123 lambda_k: 1 Loss: 0.0029294061387554952\n",
      "Iteration: 4124 lambda_k: 1 Loss: 0.00292940614902802\n",
      "Iteration: 4125 lambda_k: 1 Loss: 0.0029294061592508797\n",
      "Iteration: 4126 lambda_k: 1 Loss: 0.002929406169424308\n",
      "Iteration: 4127 lambda_k: 1 Loss: 0.002929406179548541\n",
      "Iteration: 4128 lambda_k: 1 Loss: 0.002929406189623817\n",
      "Iteration: 4129 lambda_k: 1 Loss: 0.0029294061996503673\n",
      "Iteration: 4130 lambda_k: 1 Loss: 0.002929406209628445\n",
      "Iteration: 4131 lambda_k: 1 Loss: 0.002929406219558272\n",
      "Iteration: 4132 lambda_k: 1 Loss: 0.002929406229440085\n",
      "Iteration: 4133 lambda_k: 1 Loss: 0.0029294062392741195\n",
      "Iteration: 4134 lambda_k: 1 Loss: 0.002929406249060605\n",
      "Iteration: 4135 lambda_k: 1 Loss: 0.0029294062587997666\n",
      "Iteration: 4136 lambda_k: 1 Loss: 0.002929406268491841\n",
      "Iteration: 4137 lambda_k: 1 Loss: 0.0029294062781370597\n",
      "Iteration: 4138 lambda_k: 1 Loss: 0.0029294062877356394\n",
      "Iteration: 4139 lambda_k: 1 Loss: 0.0029294062972878053\n",
      "Iteration: 4140 lambda_k: 1 Loss: 0.0029294063067937907\n",
      "Iteration: 4141 lambda_k: 1 Loss: 0.00292940631625382\n",
      "Iteration: 4142 lambda_k: 1 Loss: 0.0029294063256681186\n",
      "Iteration: 4143 lambda_k: 1 Loss: 0.0029294063350368844\n",
      "Iteration: 4144 lambda_k: 1 Loss: 0.002929406344360374\n",
      "Iteration: 4145 lambda_k: 1 Loss: 0.00292940635363878\n",
      "Iteration: 4146 lambda_k: 1 Loss: 0.0029294063628723384\n",
      "Iteration: 4147 lambda_k: 1 Loss: 0.0029294063720612554\n",
      "Iteration: 4148 lambda_k: 1 Loss: 0.0029294063812057417\n",
      "Iteration: 4149 lambda_k: 1 Loss: 0.0029294063903060196\n",
      "Iteration: 4150 lambda_k: 1 Loss: 0.0029294063993623156\n",
      "Iteration: 4151 lambda_k: 1 Loss: 0.0029294064083748217\n",
      "Iteration: 4152 lambda_k: 1 Loss: 0.002929406417343756\n",
      "Iteration: 4153 lambda_k: 1 Loss: 0.002929406426269344\n",
      "Iteration: 4154 lambda_k: 1 Loss: 0.00292940643515178\n",
      "Iteration: 4155 lambda_k: 1 Loss: 0.00292940644399128\n",
      "Iteration: 4156 lambda_k: 1 Loss: 0.0029294064527880365\n",
      "Iteration: 4157 lambda_k: 1 Loss: 0.002929406461542278\n",
      "Iteration: 4158 lambda_k: 1 Loss: 0.0029294064702542013\n",
      "Iteration: 4159 lambda_k: 1 Loss: 0.002929406478924019\n",
      "Iteration: 4160 lambda_k: 1 Loss: 0.002929406487551923\n",
      "Iteration: 4161 lambda_k: 1 Loss: 0.0029294064961381274\n",
      "Iteration: 4162 lambda_k: 1 Loss: 0.002929406504682828\n",
      "Iteration: 4163 lambda_k: 1 Loss: 0.002929406513186217\n",
      "Iteration: 4164 lambda_k: 1 Loss: 0.0029294065216485038\n",
      "Iteration: 4165 lambda_k: 1 Loss: 0.0029294065300698928\n",
      "Iteration: 4166 lambda_k: 1 Loss: 0.002929406538450564\n",
      "Iteration: 4167 lambda_k: 1 Loss: 0.0029294065467907436\n",
      "Iteration: 4168 lambda_k: 1 Loss: 0.0029294065550906057\n",
      "Iteration: 4169 lambda_k: 1 Loss: 0.002929406563350348\n",
      "Iteration: 4170 lambda_k: 1 Loss: 0.002929406571570179\n",
      "Iteration: 4171 lambda_k: 1 Loss: 0.0029294065797502884\n",
      "Iteration: 4172 lambda_k: 1 Loss: 0.0029294065878908444\n",
      "Iteration: 4173 lambda_k: 1 Loss: 0.002929406595992044\n",
      "Iteration: 4174 lambda_k: 1 Loss: 0.0029294066040541035\n",
      "Iteration: 4175 lambda_k: 1 Loss: 0.002929406612077187\n",
      "Iteration: 4176 lambda_k: 1 Loss: 0.0029294066200614946\n",
      "Iteration: 4177 lambda_k: 1 Loss: 0.002929406628007227\n",
      "Iteration: 4178 lambda_k: 1 Loss: 0.002929406635914552\n",
      "Iteration: 4179 lambda_k: 1 Loss: 0.002929406643783646\n",
      "Iteration: 4180 lambda_k: 1 Loss: 0.002929406651614725\n",
      "Iteration: 4181 lambda_k: 1 Loss: 0.0029294066594079634\n",
      "Iteration: 4182 lambda_k: 1 Loss: 0.0029294066671635317\n",
      "Iteration: 4183 lambda_k: 1 Loss: 0.002929406674881619\n",
      "Iteration: 4184 lambda_k: 1 Loss: 0.0029294066825624005\n",
      "Iteration: 4185 lambda_k: 1 Loss: 0.002929406690206064\n",
      "Iteration: 4186 lambda_k: 1 Loss: 0.002929406697812785\n",
      "Iteration: 4187 lambda_k: 1 Loss: 0.0029294067053827555\n",
      "Iteration: 4188 lambda_k: 1 Loss: 0.0029294067129161496\n",
      "Iteration: 4189 lambda_k: 1 Loss: 0.0029294067204131383\n",
      "Iteration: 4190 lambda_k: 1 Loss: 0.002929406727873902\n",
      "Iteration: 4191 lambda_k: 1 Loss: 0.0029294067352985996\n",
      "Iteration: 4192 lambda_k: 1 Loss: 0.0029294067426874175\n",
      "Iteration: 4193 lambda_k: 1 Loss: 0.0029294067500405365\n",
      "Iteration: 4194 lambda_k: 1 Loss: 0.0029294067573581196\n",
      "Iteration: 4195 lambda_k: 1 Loss: 0.0029294067646403604\n",
      "Iteration: 4196 lambda_k: 1 Loss: 0.0029294067718873963\n",
      "Iteration: 4197 lambda_k: 1 Loss: 0.002929406779099432\n",
      "Iteration: 4198 lambda_k: 1 Loss: 0.002929406786276618\n",
      "Iteration: 4199 lambda_k: 1 Loss: 0.0029294067934190924\n",
      "Iteration: 4200 lambda_k: 1 Loss: 0.002929406800527068\n",
      "Iteration: 4201 lambda_k: 1 Loss: 0.0029294068076007114\n",
      "Iteration: 4202 lambda_k: 1 Loss: 0.0029294068146401672\n",
      "Iteration: 4203 lambda_k: 1 Loss: 0.0029294068216456103\n",
      "Iteration: 4204 lambda_k: 1 Loss: 0.0029294068286172116\n",
      "Iteration: 4205 lambda_k: 1 Loss: 0.002929406835555131\n",
      "Iteration: 4206 lambda_k: 1 Loss: 0.002929406842459515\n",
      "Iteration: 4207 lambda_k: 1 Loss: 0.0029294068493305463\n",
      "Iteration: 4208 lambda_k: 1 Loss: 0.002929406856168368\n",
      "Iteration: 4209 lambda_k: 1 Loss: 0.002929406862973164\n",
      "Iteration: 4210 lambda_k: 1 Loss: 0.002929406869745081\n",
      "Iteration: 4211 lambda_k: 1 Loss: 0.002929406876484291\n",
      "Iteration: 4212 lambda_k: 1 Loss: 0.002929406883190933\n",
      "Iteration: 4213 lambda_k: 1 Loss: 0.002929406889865172\n",
      "Iteration: 4214 lambda_k: 1 Loss: 0.0029294068965071618\n",
      "Iteration: 4215 lambda_k: 1 Loss: 0.0029294069031170723\n",
      "Iteration: 4216 lambda_k: 1 Loss: 0.0029294069096950388\n",
      "Iteration: 4217 lambda_k: 1 Loss: 0.0029294069162412372\n",
      "Iteration: 4218 lambda_k: 1 Loss: 0.002929406922755802\n",
      "Iteration: 4219 lambda_k: 1 Loss: 0.0029294069292388992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4220 lambda_k: 1 Loss: 0.0029294069356906715\n",
      "Iteration: 4221 lambda_k: 1 Loss: 0.0029294069421112877\n",
      "Iteration: 4222 lambda_k: 1 Loss: 0.0029294069485008736\n",
      "Iteration: 4223 lambda_k: 1 Loss: 0.0029294069548595997\n",
      "Iteration: 4224 lambda_k: 1 Loss: 0.0029294069611876016\n",
      "Iteration: 4225 lambda_k: 1 Loss: 0.002929406967485038\n",
      "Iteration: 4226 lambda_k: 1 Loss: 0.002929406973752035\n",
      "Iteration: 4227 lambda_k: 1 Loss: 0.002929406979988779\n",
      "Iteration: 4228 lambda_k: 1 Loss: 0.0029294069861953895\n",
      "Iteration: 4229 lambda_k: 1 Loss: 0.002929406992372024\n",
      "Iteration: 4230 lambda_k: 1 Loss: 0.002929406998518812\n",
      "Iteration: 4231 lambda_k: 1 Loss: 0.002929407004635916\n",
      "Iteration: 4232 lambda_k: 1 Loss: 0.002929407010723472\n",
      "Iteration: 4233 lambda_k: 1 Loss: 0.0029294070167816258\n",
      "Iteration: 4234 lambda_k: 1 Loss: 0.0029294070228105176\n",
      "Iteration: 4235 lambda_k: 1 Loss: 0.0029294070288102773\n",
      "Iteration: 4236 lambda_k: 1 Loss: 0.002929407034781074\n",
      "Iteration: 4237 lambda_k: 1 Loss: 0.002929407040723034\n",
      "Iteration: 4238 lambda_k: 1 Loss: 0.0029294070466362706\n",
      "Iteration: 4239 lambda_k: 1 Loss: 0.002929407052520962\n",
      "Iteration: 4240 lambda_k: 1 Loss: 0.0029294070583772347\n",
      "Iteration: 4241 lambda_k: 1 Loss: 0.002929407064205224\n",
      "Iteration: 4242 lambda_k: 1 Loss: 0.0029294070700050566\n",
      "Iteration: 4243 lambda_k: 1 Loss: 0.00292940707577687\n",
      "Iteration: 4244 lambda_k: 1 Loss: 0.0029294070815208003\n",
      "Iteration: 4245 lambda_k: 1 Loss: 0.0029294070872369904\n",
      "Iteration: 4246 lambda_k: 1 Loss: 0.002929407092925584\n",
      "Iteration: 4247 lambda_k: 1 Loss: 0.0029294070985866907\n",
      "Iteration: 4248 lambda_k: 1 Loss: 0.0029294071042204645\n",
      "Iteration: 4249 lambda_k: 1 Loss: 0.0029294071098270214\n",
      "Iteration: 4250 lambda_k: 1 Loss: 0.0029294071154065083\n",
      "Iteration: 4251 lambda_k: 1 Loss: 0.002929407120959043\n",
      "Iteration: 4252 lambda_k: 1 Loss: 0.002929407126484771\n",
      "Iteration: 4253 lambda_k: 1 Loss: 0.0029294071319838164\n",
      "Iteration: 4254 lambda_k: 1 Loss: 0.0029294071374562866\n",
      "Iteration: 4255 lambda_k: 1 Loss: 0.0029294071429023386\n",
      "Iteration: 4256 lambda_k: 1 Loss: 0.0029294071483220957\n",
      "Iteration: 4257 lambda_k: 1 Loss: 0.0029294071537156793\n",
      "Iteration: 4258 lambda_k: 1 Loss: 0.00292940715908321\n",
      "Iteration: 4259 lambda_k: 1 Loss: 0.0029294071644248177\n",
      "Iteration: 4260 lambda_k: 1 Loss: 0.0029294071697406276\n",
      "Iteration: 4261 lambda_k: 1 Loss: 0.002929407175030772\n",
      "Iteration: 4262 lambda_k: 1 Loss: 0.002929407180295374\n",
      "Iteration: 4263 lambda_k: 1 Loss: 0.0029294071855345436\n",
      "Iteration: 4264 lambda_k: 1 Loss: 0.002929407190748425\n",
      "Iteration: 4265 lambda_k: 1 Loss: 0.002929407195937123\n",
      "Iteration: 4266 lambda_k: 1 Loss: 0.0029294072011007747\n",
      "Iteration: 4267 lambda_k: 1 Loss: 0.0029294072062394745\n",
      "Iteration: 4268 lambda_k: 1 Loss: 0.0029294072113533695\n",
      "Iteration: 4269 lambda_k: 1 Loss: 0.002929407216442578\n",
      "Iteration: 4270 lambda_k: 1 Loss: 0.0029294072215072056\n",
      "Iteration: 4271 lambda_k: 1 Loss: 0.0029294072265473713\n",
      "Iteration: 4272 lambda_k: 1 Loss: 0.00292940723156319\n",
      "Iteration: 4273 lambda_k: 1 Loss: 0.002929407236554801\n",
      "Iteration: 4274 lambda_k: 1 Loss: 0.002929407241522329\n",
      "Iteration: 4275 lambda_k: 1 Loss: 0.002929407246465849\n",
      "Iteration: 4276 lambda_k: 1 Loss: 0.0029294072513855077\n",
      "Iteration: 4277 lambda_k: 1 Loss: 0.0029294072562814234\n",
      "Iteration: 4278 lambda_k: 1 Loss: 0.002929407261153694\n",
      "Iteration: 4279 lambda_k: 1 Loss: 0.0029294072660024408\n",
      "Iteration: 4280 lambda_k: 1 Loss: 0.002929407270827772\n",
      "Iteration: 4281 lambda_k: 1 Loss: 0.0029294072756297983\n",
      "Iteration: 4282 lambda_k: 1 Loss: 0.0029294072804086474\n",
      "Iteration: 4283 lambda_k: 1 Loss: 0.002929407285164413\n",
      "Iteration: 4284 lambda_k: 1 Loss: 0.002929407289897234\n",
      "Iteration: 4285 lambda_k: 1 Loss: 0.0029294072946072017\n",
      "Iteration: 4286 lambda_k: 1 Loss: 0.002929407299294432\n",
      "Iteration: 4287 lambda_k: 1 Loss: 0.0029294073039590304\n",
      "Iteration: 4288 lambda_k: 1 Loss: 0.0029294073086011153\n",
      "Iteration: 4289 lambda_k: 1 Loss: 0.0029294073132207768\n",
      "Iteration: 4290 lambda_k: 1 Loss: 0.0029294073178181296\n",
      "Iteration: 4291 lambda_k: 1 Loss: 0.002929407322393283\n",
      "Iteration: 4292 lambda_k: 1 Loss: 0.0029294073269463703\n",
      "Iteration: 4293 lambda_k: 1 Loss: 0.002929407331477469\n",
      "Iteration: 4294 lambda_k: 1 Loss: 0.002929407335986697\n",
      "Iteration: 4295 lambda_k: 1 Loss: 0.0029294073404741595\n",
      "Iteration: 4296 lambda_k: 1 Loss: 0.002929407344939949\n",
      "Iteration: 4297 lambda_k: 1 Loss: 0.0029294073493841928\n",
      "Iteration: 4298 lambda_k: 1 Loss: 0.002929407353806975\n",
      "Iteration: 4299 lambda_k: 1 Loss: 0.0029294073582084106\n",
      "Iteration: 4300 lambda_k: 1 Loss: 0.0029294073625885982\n",
      "Iteration: 4301 lambda_k: 1 Loss: 0.002929407366947627\n",
      "Iteration: 4302 lambda_k: 1 Loss: 0.0029294073712856305\n",
      "Iteration: 4303 lambda_k: 1 Loss: 0.0029294073756026785\n",
      "Iteration: 4304 lambda_k: 1 Loss: 0.0029294073798988903\n",
      "Iteration: 4305 lambda_k: 1 Loss: 0.0029294073841743687\n",
      "Iteration: 4306 lambda_k: 1 Loss: 0.0029294073884292087\n",
      "Iteration: 4307 lambda_k: 1 Loss: 0.0029294073926635004\n",
      "Iteration: 4308 lambda_k: 1 Loss: 0.0029294073968773684\n",
      "Iteration: 4309 lambda_k: 1 Loss: 0.002929407401070894\n",
      "Iteration: 4310 lambda_k: 1 Loss: 0.002929407405244178\n",
      "Iteration: 4311 lambda_k: 1 Loss: 0.0029294074093973274\n",
      "Iteration: 4312 lambda_k: 1 Loss: 0.0029294074135304223\n",
      "Iteration: 4313 lambda_k: 1 Loss: 0.002929407417643565\n",
      "Iteration: 4314 lambda_k: 1 Loss: 0.0029294074217368574\n",
      "Iteration: 4315 lambda_k: 1 Loss: 0.0029294074258103936\n",
      "Iteration: 4316 lambda_k: 1 Loss: 0.0029294074298642793\n",
      "Iteration: 4317 lambda_k: 1 Loss: 0.0029294074338985883\n",
      "Iteration: 4318 lambda_k: 1 Loss: 0.0029294074379134284\n",
      "Iteration: 4319 lambda_k: 1 Loss: 0.0029294074419088916\n",
      "Iteration: 4320 lambda_k: 1 Loss: 0.0029294074458850695\n",
      "Iteration: 4321 lambda_k: 1 Loss: 0.002929407449842054\n",
      "Iteration: 4322 lambda_k: 1 Loss: 0.0029294074537799387\n",
      "Iteration: 4323 lambda_k: 1 Loss: 0.0029294074576988248\n",
      "Iteration: 4324 lambda_k: 1 Loss: 0.002929407461598784\n",
      "Iteration: 4325 lambda_k: 1 Loss: 0.00292940746547993\n",
      "Iteration: 4326 lambda_k: 1 Loss: 0.0029294074693423395\n",
      "Iteration: 4327 lambda_k: 1 Loss: 0.002929407473186112\n",
      "Iteration: 4328 lambda_k: 1 Loss: 0.0029294074770113442\n",
      "Iteration: 4329 lambda_k: 1 Loss: 0.0029294074808181086\n",
      "Iteration: 4330 lambda_k: 1 Loss: 0.0029294074846065066\n",
      "Iteration: 4331 lambda_k: 1 Loss: 0.002929407488376627\n",
      "Iteration: 4332 lambda_k: 1 Loss: 0.0029294074921285456\n",
      "Iteration: 4333 lambda_k: 1 Loss: 0.002929407495862366\n",
      "Iteration: 4334 lambda_k: 1 Loss: 0.0029294074995781607\n",
      "Iteration: 4335 lambda_k: 1 Loss: 0.0029294075032760326\n",
      "Iteration: 4336 lambda_k: 1 Loss: 0.0029294075069560502\n",
      "Iteration: 4337 lambda_k: 1 Loss: 0.0029294075106183026\n",
      "Iteration: 4338 lambda_k: 1 Loss: 0.0029294075142628872\n",
      "Iteration: 4339 lambda_k: 1 Loss: 0.0029294075178898905\n",
      "Iteration: 4340 lambda_k: 1 Loss: 0.002929407521499379\n",
      "Iteration: 4341 lambda_k: 1 Loss: 0.0029294075250914484\n",
      "Iteration: 4342 lambda_k: 1 Loss: 0.0029294075286662043\n",
      "Iteration: 4343 lambda_k: 1 Loss: 0.002929407532223687\n",
      "Iteration: 4344 lambda_k: 1 Loss: 0.002929407535764014\n",
      "Iteration: 4345 lambda_k: 1 Loss: 0.0029294075392872487\n",
      "Iteration: 4346 lambda_k: 1 Loss: 0.0029294075427934913\n",
      "Iteration: 4347 lambda_k: 1 Loss: 0.0029294075462828082\n",
      "Iteration: 4348 lambda_k: 1 Loss: 0.0029294075497552923\n",
      "Iteration: 4349 lambda_k: 1 Loss: 0.002929407553211025\n",
      "Iteration: 4350 lambda_k: 1 Loss: 0.0029294075566500648\n",
      "Iteration: 4351 lambda_k: 1 Loss: 0.002929407560072525\n",
      "Iteration: 4352 lambda_k: 1 Loss: 0.0029294075634784767\n",
      "Iteration: 4353 lambda_k: 1 Loss: 0.002929407566867999\n",
      "Iteration: 4354 lambda_k: 1 Loss: 0.0029294075702411535\n",
      "Iteration: 4355 lambda_k: 1 Loss: 0.002929407573598039\n",
      "Iteration: 4356 lambda_k: 1 Loss: 0.002929407576938712\n",
      "Iteration: 4357 lambda_k: 1 Loss: 0.002929407580263276\n",
      "Iteration: 4358 lambda_k: 1 Loss: 0.0029294075835718113\n",
      "Iteration: 4359 lambda_k: 1 Loss: 0.0029294075868643863\n",
      "Iteration: 4360 lambda_k: 1 Loss: 0.0029294075901410747\n",
      "Iteration: 4361 lambda_k: 1 Loss: 0.0029294075934019476\n",
      "Iteration: 4362 lambda_k: 1 Loss: 0.002929407596647082\n",
      "Iteration: 4363 lambda_k: 1 Loss: 0.002929407599876559\n",
      "Iteration: 4364 lambda_k: 1 Loss: 0.002929407603090457\n",
      "Iteration: 4365 lambda_k: 1 Loss: 0.0029294076062888518\n",
      "Iteration: 4366 lambda_k: 1 Loss: 0.0029294076094718174\n",
      "Iteration: 4367 lambda_k: 1 Loss: 0.00292940761263944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4368 lambda_k: 1 Loss: 0.002929407615791773\n",
      "Iteration: 4369 lambda_k: 1 Loss: 0.0029294076189288823\n",
      "Iteration: 4370 lambda_k: 1 Loss: 0.0029294076220508667\n",
      "Iteration: 4371 lambda_k: 1 Loss: 0.00292940762515779\n",
      "Iteration: 4372 lambda_k: 1 Loss: 0.002929407628249715\n",
      "Iteration: 4373 lambda_k: 1 Loss: 0.0029294076313267354\n",
      "Iteration: 4374 lambda_k: 1 Loss: 0.0029294076343889092\n",
      "Iteration: 4375 lambda_k: 1 Loss: 0.0029294076374363175\n",
      "Iteration: 4376 lambda_k: 1 Loss: 0.002929407640469016\n",
      "Iteration: 4377 lambda_k: 1 Loss: 0.0029294076434870763\n",
      "Iteration: 4378 lambda_k: 1 Loss: 0.0029294076464905856\n",
      "Iteration: 4379 lambda_k: 1 Loss: 0.0029294076494796118\n",
      "Iteration: 4380 lambda_k: 1 Loss: 0.0029294076524542124\n",
      "Iteration: 4381 lambda_k: 1 Loss: 0.002929407655414477\n",
      "Iteration: 4382 lambda_k: 1 Loss: 0.002929407658360444\n",
      "Iteration: 4383 lambda_k: 1 Loss: 0.0029294076612922174\n",
      "Iteration: 4384 lambda_k: 1 Loss: 0.0029294076642098375\n",
      "Iteration: 4385 lambda_k: 1 Loss: 0.002929407667113371\n",
      "Iteration: 4386 lambda_k: 1 Loss: 0.002929407670002923\n",
      "Iteration: 4387 lambda_k: 1 Loss: 0.002929407672878536\n",
      "Iteration: 4388 lambda_k: 1 Loss: 0.0029294076757402816\n",
      "Iteration: 4389 lambda_k: 1 Loss: 0.0029294076785882197\n",
      "Iteration: 4390 lambda_k: 1 Loss: 0.0029294076814224187\n",
      "Iteration: 4391 lambda_k: 1 Loss: 0.0029294076842429303\n",
      "Iteration: 4392 lambda_k: 1 Loss: 0.0029294076870498647\n",
      "Iteration: 4393 lambda_k: 1 Loss: 0.002929407689843243\n",
      "Iteration: 4394 lambda_k: 1 Loss: 0.0029294076926231465\n",
      "Iteration: 4395 lambda_k: 1 Loss: 0.002929407695389639\n",
      "Iteration: 4396 lambda_k: 1 Loss: 0.0029294076981427977\n",
      "Iteration: 4397 lambda_k: 1 Loss: 0.002929407700882659\n",
      "Iteration: 4398 lambda_k: 1 Loss: 0.002929407703609324\n",
      "Iteration: 4399 lambda_k: 1 Loss: 0.0029294077063228316\n",
      "Iteration: 4400 lambda_k: 1 Loss: 0.0029294077090232604\n",
      "Iteration: 4401 lambda_k: 1 Loss: 0.00292940771171066\n",
      "Iteration: 4402 lambda_k: 1 Loss: 0.002929407714385098\n",
      "Iteration: 4403 lambda_k: 1 Loss: 0.0029294077170466315\n",
      "Iteration: 4404 lambda_k: 1 Loss: 0.0029294077196953335\n",
      "Iteration: 4405 lambda_k: 1 Loss: 0.0029294077223312675\n",
      "Iteration: 4406 lambda_k: 1 Loss: 0.0029294077249544755\n",
      "Iteration: 4407 lambda_k: 1 Loss: 0.002929407727565035\n",
      "Iteration: 4408 lambda_k: 1 Loss: 0.002929407730163009\n",
      "Iteration: 4409 lambda_k: 1 Loss: 0.0029294077327484606\n",
      "Iteration: 4410 lambda_k: 1 Loss: 0.0029294077353214476\n",
      "Iteration: 4411 lambda_k: 1 Loss: 0.00292940773788201\n",
      "Iteration: 4412 lambda_k: 1 Loss: 0.002929407740430226\n",
      "Iteration: 4413 lambda_k: 1 Loss: 0.00292940774296616\n",
      "Iteration: 4414 lambda_k: 1 Loss: 0.0029294077454898623\n",
      "Iteration: 4415 lambda_k: 1 Loss: 0.002929407748001394\n",
      "Iteration: 4416 lambda_k: 1 Loss: 0.0029294077505008192\n",
      "Iteration: 4417 lambda_k: 1 Loss: 0.002929407752988189\n",
      "Iteration: 4418 lambda_k: 1 Loss: 0.00292940775546356\n",
      "Iteration: 4419 lambda_k: 1 Loss: 0.002929407757926992\n",
      "Iteration: 4420 lambda_k: 1 Loss: 0.002929407760378542\n",
      "Iteration: 4421 lambda_k: 1 Loss: 0.00292940776281826\n",
      "Iteration: 4422 lambda_k: 1 Loss: 0.002929407765246234\n",
      "Iteration: 4423 lambda_k: 1 Loss: 0.0029294077676625028\n",
      "Iteration: 4424 lambda_k: 1 Loss: 0.002929407770067117\n",
      "Iteration: 4425 lambda_k: 1 Loss: 0.0029294077724601234\n",
      "Iteration: 4426 lambda_k: 1 Loss: 0.00292940777484161\n",
      "Iteration: 4427 lambda_k: 1 Loss: 0.0029294077772116078\n",
      "Iteration: 4428 lambda_k: 1 Loss: 0.002929407779570168\n",
      "Iteration: 4429 lambda_k: 1 Loss: 0.0029294077819173453\n",
      "Iteration: 4430 lambda_k: 1 Loss: 0.0029294077842532207\n",
      "Iteration: 4431 lambda_k: 1 Loss: 0.002929407786577818\n",
      "Iteration: 4432 lambda_k: 1 Loss: 0.0029294077888912286\n",
      "Iteration: 4433 lambda_k: 1 Loss: 0.0029294077911934803\n",
      "Iteration: 4434 lambda_k: 1 Loss: 0.0029294077934846324\n",
      "Iteration: 4435 lambda_k: 1 Loss: 0.002929407795764729\n",
      "Iteration: 4436 lambda_k: 1 Loss: 0.002929407798033845\n",
      "Iteration: 4437 lambda_k: 1 Loss: 0.0029294078002920026\n",
      "Iteration: 4438 lambda_k: 1 Loss: 0.002929407802539278\n",
      "Iteration: 4439 lambda_k: 1 Loss: 0.0029294078047757267\n",
      "Iteration: 4440 lambda_k: 1 Loss: 0.0029294078070013843\n",
      "Iteration: 4441 lambda_k: 1 Loss: 0.002929407809216312\n",
      "Iteration: 4442 lambda_k: 1 Loss: 0.0029294078114205685\n",
      "Iteration: 4443 lambda_k: 1 Loss: 0.0029294078136141836\n",
      "Iteration: 4444 lambda_k: 1 Loss: 0.002929407815797227\n",
      "Iteration: 4445 lambda_k: 1 Loss: 0.0029294078179697485\n",
      "Iteration: 4446 lambda_k: 1 Loss: 0.002929407820131807\n",
      "Iteration: 4447 lambda_k: 1 Loss: 0.0029294078222834355\n",
      "Iteration: 4448 lambda_k: 1 Loss: 0.0029294078244246767\n",
      "Iteration: 4449 lambda_k: 1 Loss: 0.002929407826555596\n",
      "Iteration: 4450 lambda_k: 1 Loss: 0.0029294078286762454\n",
      "Iteration: 4451 lambda_k: 1 Loss: 0.0029294078307866736\n",
      "Iteration: 4452 lambda_k: 1 Loss: 0.002929407832886922\n",
      "Iteration: 4453 lambda_k: 1 Loss: 0.0029294078349770525\n",
      "Iteration: 4454 lambda_k: 1 Loss: 0.002929407837057111\n",
      "Iteration: 4455 lambda_k: 1 Loss: 0.0029294078391271404\n",
      "Iteration: 4456 lambda_k: 1 Loss: 0.002929407841187177\n",
      "Iteration: 4457 lambda_k: 1 Loss: 0.0029294078432372844\n",
      "Iteration: 4458 lambda_k: 1 Loss: 0.002929407845277503\n",
      "Iteration: 4459 lambda_k: 1 Loss: 0.0029294078473079107\n",
      "Iteration: 4460 lambda_k: 1 Loss: 0.0029294078493285343\n",
      "Iteration: 4461 lambda_k: 1 Loss: 0.0029294078513394106\n",
      "Iteration: 4462 lambda_k: 1 Loss: 0.002929407853340589\n",
      "Iteration: 4463 lambda_k: 1 Loss: 0.002929407855332121\n",
      "Iteration: 4464 lambda_k: 1 Loss: 0.0029294078573140645\n",
      "Iteration: 4465 lambda_k: 1 Loss: 0.0029294078592864542\n",
      "Iteration: 4466 lambda_k: 1 Loss: 0.0029294078612493307\n",
      "Iteration: 4467 lambda_k: 1 Loss: 0.002929407863202733\n",
      "Iteration: 4468 lambda_k: 1 Loss: 0.002929407865146744\n",
      "Iteration: 4469 lambda_k: 1 Loss: 0.002929407867081363\n",
      "Iteration: 4470 lambda_k: 1 Loss: 0.0029294078690066533\n",
      "Iteration: 4471 lambda_k: 1 Loss: 0.0029294078709226784\n",
      "Iteration: 4472 lambda_k: 1 Loss: 0.002929407872829453\n",
      "Iteration: 4473 lambda_k: 1 Loss: 0.002929407874727054\n",
      "Iteration: 4474 lambda_k: 1 Loss: 0.0029294078766154935\n",
      "Iteration: 4475 lambda_k: 1 Loss: 0.0029294078784948487\n",
      "Iteration: 4476 lambda_k: 1 Loss: 0.002929407880365133\n",
      "Iteration: 4477 lambda_k: 1 Loss: 0.0029294078822264214\n",
      "Iteration: 4478 lambda_k: 1 Loss: 0.002929407884078718\n",
      "Iteration: 4479 lambda_k: 1 Loss: 0.002929407885922096\n",
      "Iteration: 4480 lambda_k: 1 Loss: 0.0029294078877565807\n",
      "Iteration: 4481 lambda_k: 1 Loss: 0.002929407889582224\n",
      "Iteration: 4482 lambda_k: 1 Loss: 0.002929407891399074\n",
      "Iteration: 4483 lambda_k: 1 Loss: 0.0029294078932071603\n",
      "Iteration: 4484 lambda_k: 1 Loss: 0.002929407895006539\n",
      "Iteration: 4485 lambda_k: 1 Loss: 0.0029294078967972543\n",
      "Iteration: 4486 lambda_k: 1 Loss: 0.002929407898579323\n",
      "Iteration: 4487 lambda_k: 1 Loss: 0.002929407900352809\n",
      "Iteration: 4488 lambda_k: 1 Loss: 0.002929407902117748\n",
      "Iteration: 4489 lambda_k: 1 Loss: 0.0029294079038741845\n",
      "Iteration: 4490 lambda_k: 1 Loss: 0.002929407905622136\n",
      "Iteration: 4491 lambda_k: 1 Loss: 0.002929407907361668\n",
      "Iteration: 4492 lambda_k: 1 Loss: 0.002929407909092827\n",
      "Iteration: 4493 lambda_k: 1 Loss: 0.002929407910815637\n",
      "Iteration: 4494 lambda_k: 1 Loss: 0.002929407912530153\n",
      "Iteration: 4495 lambda_k: 1 Loss: 0.0029294079142363986\n",
      "Iteration: 4496 lambda_k: 1 Loss: 0.0029294079159344296\n",
      "Iteration: 4497 lambda_k: 1 Loss: 0.0029294079176242793\n",
      "Iteration: 4498 lambda_k: 1 Loss: 0.0029294079193059944\n",
      "Iteration: 4499 lambda_k: 1 Loss: 0.0029294079209796083\n",
      "Iteration: 4500 lambda_k: 1 Loss: 0.0029294079226451514\n",
      "Iteration: 4501 lambda_k: 1 Loss: 0.002929407924302667\n",
      "Iteration: 4502 lambda_k: 1 Loss: 0.002929407925952198\n",
      "Iteration: 4503 lambda_k: 1 Loss: 0.002929407927593777\n",
      "Iteration: 4504 lambda_k: 1 Loss: 0.0029294079292274443\n",
      "Iteration: 4505 lambda_k: 1 Loss: 0.0029294079308532376\n",
      "Iteration: 4506 lambda_k: 1 Loss: 0.0029294079324712037\n",
      "Iteration: 4507 lambda_k: 1 Loss: 0.0029294079340813676\n",
      "Iteration: 4508 lambda_k: 1 Loss: 0.002929407935683767\n",
      "Iteration: 4509 lambda_k: 1 Loss: 0.002929407937278451\n",
      "Iteration: 4510 lambda_k: 1 Loss: 0.002929407938865452\n",
      "Iteration: 4511 lambda_k: 1 Loss: 0.0029294079404448034\n",
      "Iteration: 4512 lambda_k: 1 Loss: 0.0029294079420165344\n",
      "Iteration: 4513 lambda_k: 1 Loss: 0.00292940794358069\n",
      "Iteration: 4514 lambda_k: 1 Loss: 0.002929407945137324\n",
      "Iteration: 4515 lambda_k: 1 Loss: 0.0029294079466864477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4516 lambda_k: 1 Loss: 0.0029294079482281065\n",
      "Iteration: 4517 lambda_k: 1 Loss: 0.002929407949762347\n",
      "Iteration: 4518 lambda_k: 1 Loss: 0.0029294079512891926\n",
      "Iteration: 4519 lambda_k: 1 Loss: 0.0029294079528086737\n",
      "Iteration: 4520 lambda_k: 1 Loss: 0.00292940795432084\n",
      "Iteration: 4521 lambda_k: 1 Loss: 0.002929407955825714\n",
      "Iteration: 4522 lambda_k: 1 Loss: 0.0029294079573233328\n",
      "Iteration: 4523 lambda_k: 1 Loss: 0.0029294079588137426\n",
      "Iteration: 4524 lambda_k: 1 Loss: 0.002929407960296957\n",
      "Iteration: 4525 lambda_k: 1 Loss: 0.0029294079617730494\n",
      "Iteration: 4526 lambda_k: 1 Loss: 0.002929407963242013\n",
      "Iteration: 4527 lambda_k: 1 Loss: 0.002929407964703912\n",
      "Iteration: 4528 lambda_k: 1 Loss: 0.002929407966158754\n",
      "Iteration: 4529 lambda_k: 1 Loss: 0.0029294079676065905\n",
      "Iteration: 4530 lambda_k: 1 Loss: 0.0029294079690474453\n",
      "Iteration: 4531 lambda_k: 1 Loss: 0.002929407970481366\n",
      "Iteration: 4532 lambda_k: 1 Loss: 0.002929407971908386\n",
      "Iteration: 4533 lambda_k: 1 Loss: 0.002929407973328518\n",
      "Iteration: 4534 lambda_k: 1 Loss: 0.0029294079747418174\n",
      "Iteration: 4535 lambda_k: 1 Loss: 0.0029294079761483095\n",
      "Iteration: 4536 lambda_k: 1 Loss: 0.0029294079775480267\n",
      "Iteration: 4537 lambda_k: 1 Loss: 0.002929407978940999\n",
      "Iteration: 4538 lambda_k: 1 Loss: 0.0029294079803272493\n",
      "Iteration: 4539 lambda_k: 1 Loss: 0.0029294079817068315\n",
      "Iteration: 4540 lambda_k: 1 Loss: 0.0029294079830797745\n",
      "Iteration: 4541 lambda_k: 1 Loss: 0.002929407984446085\n",
      "Iteration: 4542 lambda_k: 1 Loss: 0.0029294079858058126\n",
      "Iteration: 4543 lambda_k: 1 Loss: 0.002929407987158994\n",
      "Iteration: 4544 lambda_k: 1 Loss: 0.0029294079885056694\n",
      "Iteration: 4545 lambda_k: 1 Loss: 0.002929407989845844\n",
      "Iteration: 4546 lambda_k: 1 Loss: 0.0029294079911795614\n",
      "Iteration: 4547 lambda_k: 1 Loss: 0.0029294079925068525\n",
      "Iteration: 4548 lambda_k: 1 Loss: 0.0029294079938277594\n",
      "Iteration: 4549 lambda_k: 1 Loss: 0.002929407995142311\n",
      "Iteration: 4550 lambda_k: 1 Loss: 0.002929407996450535\n",
      "Iteration: 4551 lambda_k: 1 Loss: 0.002929407997752428\n",
      "Iteration: 4552 lambda_k: 1 Loss: 0.002929407999048062\n",
      "Iteration: 4553 lambda_k: 1 Loss: 0.0029294080003374485\n",
      "Iteration: 4554 lambda_k: 1 Loss: 0.002929408001620622\n",
      "Iteration: 4555 lambda_k: 1 Loss: 0.0029294080028976093\n",
      "Iteration: 4556 lambda_k: 1 Loss: 0.002929408004168459\n",
      "Iteration: 4557 lambda_k: 1 Loss: 0.00292940800543318\n",
      "Iteration: 4558 lambda_k: 1 Loss: 0.002929408006691814\n",
      "Iteration: 4559 lambda_k: 1 Loss: 0.0029294080079443814\n",
      "Iteration: 4560 lambda_k: 1 Loss: 0.002929408009190924\n",
      "Iteration: 4561 lambda_k: 1 Loss: 0.002929408010431457\n",
      "Iteration: 4562 lambda_k: 1 Loss: 0.002929408011666008\n",
      "Iteration: 4563 lambda_k: 1 Loss: 0.0029294080128946117\n",
      "Iteration: 4564 lambda_k: 1 Loss: 0.0029294080141172942\n",
      "Iteration: 4565 lambda_k: 1 Loss: 0.0029294080153341134\n",
      "Iteration: 4566 lambda_k: 1 Loss: 0.0029294080165450536\n",
      "Iteration: 4567 lambda_k: 1 Loss: 0.0029294080177501847\n",
      "Iteration: 4568 lambda_k: 1 Loss: 0.002929408018949506\n",
      "Iteration: 4569 lambda_k: 1 Loss: 0.0029294080201430523\n",
      "Iteration: 4570 lambda_k: 1 Loss: 0.002929408021330841\n",
      "Iteration: 4571 lambda_k: 1 Loss: 0.002929408022512905\n",
      "Iteration: 4572 lambda_k: 1 Loss: 0.002929408023689258\n",
      "Iteration: 4573 lambda_k: 1 Loss: 0.002929408024859965\n",
      "Iteration: 4574 lambda_k: 1 Loss: 0.002929408026025046\n",
      "Iteration: 4575 lambda_k: 1 Loss: 0.002929408027184507\n",
      "Iteration: 4576 lambda_k: 1 Loss: 0.0029294080283383747\n",
      "Iteration: 4577 lambda_k: 1 Loss: 0.002929408029486678\n",
      "Iteration: 4578 lambda_k: 1 Loss: 0.0029294080306294682\n",
      "Iteration: 4579 lambda_k: 1 Loss: 0.002929408031766751\n",
      "Iteration: 4580 lambda_k: 1 Loss: 0.002929408032898554\n",
      "Iteration: 4581 lambda_k: 1 Loss: 0.002929408034024917\n",
      "Iteration: 4582 lambda_k: 1 Loss: 0.0029294080351458442\n",
      "Iteration: 4583 lambda_k: 1 Loss: 0.0029294080362613634\n",
      "Iteration: 4584 lambda_k: 1 Loss: 0.0029294080373715335\n",
      "Iteration: 4585 lambda_k: 1 Loss: 0.0029294080384763485\n",
      "Iteration: 4586 lambda_k: 1 Loss: 0.0029294080395758353\n",
      "Iteration: 4587 lambda_k: 1 Loss: 0.0029294080406700425\n",
      "Iteration: 4588 lambda_k: 1 Loss: 0.0029294080417589683\n",
      "Iteration: 4589 lambda_k: 1 Loss: 0.0029294080428426475\n",
      "Iteration: 4590 lambda_k: 1 Loss: 0.0029294080439211125\n",
      "Iteration: 4591 lambda_k: 1 Loss: 0.0029294080449943703\n",
      "Iteration: 4592 lambda_k: 1 Loss: 0.0029294080460624803\n",
      "Iteration: 4593 lambda_k: 1 Loss: 0.002929408047125435\n",
      "Iteration: 4594 lambda_k: 1 Loss: 0.0029294080481832804\n",
      "Iteration: 4595 lambda_k: 1 Loss: 0.0029294080492360316\n",
      "Iteration: 4596 lambda_k: 1 Loss: 0.002929408050283715\n",
      "Iteration: 4597 lambda_k: 1 Loss: 0.0029294080513263387\n",
      "Iteration: 4598 lambda_k: 1 Loss: 0.0029294080523639657\n",
      "Iteration: 4599 lambda_k: 1 Loss: 0.0029294080533965755\n",
      "Iteration: 4600 lambda_k: 1 Loss: 0.002929408054424237\n",
      "Iteration: 4601 lambda_k: 1 Loss: 0.002929408055446937\n",
      "Iteration: 4602 lambda_k: 1 Loss: 0.0029294080564647106\n",
      "Iteration: 4603 lambda_k: 1 Loss: 0.0029294080574775727\n",
      "Iteration: 4604 lambda_k: 1 Loss: 0.002929408058485566\n",
      "Iteration: 4605 lambda_k: 1 Loss: 0.0029294080594887135\n",
      "Iteration: 4606 lambda_k: 1 Loss: 0.0029294080604870282\n",
      "Iteration: 4607 lambda_k: 1 Loss: 0.002929408061480531\n",
      "Iteration: 4608 lambda_k: 1 Loss: 0.002929408062469238\n",
      "Iteration: 4609 lambda_k: 1 Loss: 0.0029294080634531916\n",
      "Iteration: 4610 lambda_k: 1 Loss: 0.002929408064432426\n",
      "Iteration: 4611 lambda_k: 1 Loss: 0.002929408065406934\n",
      "Iteration: 4612 lambda_k: 1 Loss: 0.0029294080663767423\n",
      "Iteration: 4613 lambda_k: 1 Loss: 0.002929408067341883\n",
      "Iteration: 4614 lambda_k: 1 Loss: 0.002929408068302388\n",
      "Iteration: 4615 lambda_k: 1 Loss: 0.002929408069258255\n",
      "Iteration: 4616 lambda_k: 1 Loss: 0.0029294080702095353\n",
      "Iteration: 4617 lambda_k: 1 Loss: 0.002929408071156229\n",
      "Iteration: 4618 lambda_k: 1 Loss: 0.0029294080720983573\n",
      "Iteration: 4619 lambda_k: 1 Loss: 0.002929408073035956\n",
      "Iteration: 4620 lambda_k: 1 Loss: 0.0029294080739690406\n",
      "Iteration: 4621 lambda_k: 1 Loss: 0.0029294080748976476\n",
      "Iteration: 4622 lambda_k: 1 Loss: 0.002929408075821779\n",
      "Iteration: 4623 lambda_k: 1 Loss: 0.002929408076741445\n",
      "Iteration: 4624 lambda_k: 1 Loss: 0.002929408077656687\n",
      "Iteration: 4625 lambda_k: 1 Loss: 0.0029294080785675224\n",
      "Iteration: 4626 lambda_k: 1 Loss: 0.0029294080794739874\n",
      "Iteration: 4627 lambda_k: 1 Loss: 0.0029294080803760818\n",
      "Iteration: 4628 lambda_k: 1 Loss: 0.002929408081273816\n",
      "Iteration: 4629 lambda_k: 1 Loss: 0.0029294080821672323\n",
      "Iteration: 4630 lambda_k: 1 Loss: 0.002929408083056351\n",
      "Iteration: 4631 lambda_k: 1 Loss: 0.0029294080839411952\n",
      "Iteration: 4632 lambda_k: 1 Loss: 0.0029294080848217786\n",
      "Iteration: 4633 lambda_k: 1 Loss: 0.002929408085698117\n",
      "Iteration: 4634 lambda_k: 1 Loss: 0.002929408086570245\n",
      "Iteration: 4635 lambda_k: 1 Loss: 0.002929408087438175\n",
      "Iteration: 4636 lambda_k: 1 Loss: 0.002929408088301929\n",
      "Iteration: 4637 lambda_k: 1 Loss: 0.002929408089161528\n",
      "Iteration: 4638 lambda_k: 1 Loss: 0.0029294080900169775\n",
      "Iteration: 4639 lambda_k: 1 Loss: 0.002929408090868308\n",
      "Iteration: 4640 lambda_k: 1 Loss: 0.0029294080917155606\n",
      "Iteration: 4641 lambda_k: 1 Loss: 0.00292940809255871\n",
      "Iteration: 4642 lambda_k: 1 Loss: 0.0029294080933978087\n",
      "Iteration: 4643 lambda_k: 1 Loss: 0.002929408094232884\n",
      "Iteration: 4644 lambda_k: 1 Loss: 0.0029294080950639165\n",
      "Iteration: 4645 lambda_k: 1 Loss: 0.0029294080958909676\n",
      "Iteration: 4646 lambda_k: 1 Loss: 0.0029294080967140358\n",
      "Iteration: 4647 lambda_k: 1 Loss: 0.0029294080975331253\n",
      "Iteration: 4648 lambda_k: 1 Loss: 0.002929408098348283\n",
      "Iteration: 4649 lambda_k: 1 Loss: 0.0029294080991595327\n",
      "Iteration: 4650 lambda_k: 1 Loss: 0.002929408099966866\n",
      "Iteration: 4651 lambda_k: 1 Loss: 0.0029294081007703106\n",
      "Iteration: 4652 lambda_k: 1 Loss: 0.002929408101569892\n",
      "Iteration: 4653 lambda_k: 1 Loss: 0.00292940810236562\n",
      "Iteration: 4654 lambda_k: 1 Loss: 0.0029294081031575165\n",
      "Iteration: 4655 lambda_k: 1 Loss: 0.0029294081039456044\n",
      "Iteration: 4656 lambda_k: 1 Loss: 0.002929408104729888\n",
      "Iteration: 4657 lambda_k: 1 Loss: 0.0029294081055103987\n",
      "Iteration: 4658 lambda_k: 1 Loss: 0.0029294081062871606\n",
      "Iteration: 4659 lambda_k: 1 Loss: 0.0029294081070601785\n",
      "Iteration: 4660 lambda_k: 1 Loss: 0.0029294081078294737\n",
      "Iteration: 4661 lambda_k: 1 Loss: 0.002929408108595069\n",
      "Iteration: 4662 lambda_k: 1 Loss: 0.0029294081093569732\n",
      "Iteration: 4663 lambda_k: 1 Loss: 0.002929408110115214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4664 lambda_k: 1 Loss: 0.0029294081108698126\n",
      "Iteration: 4665 lambda_k: 1 Loss: 0.002929408111620774\n",
      "Iteration: 4666 lambda_k: 1 Loss: 0.0029294081123681266\n",
      "Iteration: 4667 lambda_k: 1 Loss: 0.002929408113111868\n",
      "Iteration: 4668 lambda_k: 1 Loss: 0.0029294081138520342\n",
      "Iteration: 4669 lambda_k: 1 Loss: 0.002929408114588647\n",
      "Iteration: 4670 lambda_k: 1 Loss: 0.002929408115321708\n",
      "Iteration: 4671 lambda_k: 1 Loss: 0.002929408116051237\n",
      "Iteration: 4672 lambda_k: 1 Loss: 0.0029294081167772833\n",
      "Iteration: 4673 lambda_k: 1 Loss: 0.002929408117499817\n",
      "Iteration: 4674 lambda_k: 1 Loss: 0.0029294081182188797\n",
      "Iteration: 4675 lambda_k: 1 Loss: 0.0029294081189344657\n",
      "Iteration: 4676 lambda_k: 1 Loss: 0.0029294081196466256\n",
      "Iteration: 4677 lambda_k: 1 Loss: 0.0029294081203553464\n",
      "Iteration: 4678 lambda_k: 1 Loss: 0.002929408121060658\n",
      "Iteration: 4679 lambda_k: 1 Loss: 0.0029294081217625845\n",
      "Iteration: 4680 lambda_k: 1 Loss: 0.0029294081224611255\n",
      "Iteration: 4681 lambda_k: 1 Loss: 0.0029294081231562943\n",
      "Iteration: 4682 lambda_k: 1 Loss: 0.002929408123848129\n",
      "Iteration: 4683 lambda_k: 1 Loss: 0.0029294081245366285\n",
      "Iteration: 4684 lambda_k: 1 Loss: 0.0029294081252218135\n",
      "Iteration: 4685 lambda_k: 1 Loss: 0.0029294081259037155\n",
      "Iteration: 4686 lambda_k: 1 Loss: 0.002929408126582324\n",
      "Iteration: 4687 lambda_k: 1 Loss: 0.0029294081272576633\n",
      "Iteration: 4688 lambda_k: 1 Loss: 0.0029294081279297585\n",
      "Iteration: 4689 lambda_k: 1 Loss: 0.0029294081285986076\n",
      "Iteration: 4690 lambda_k: 1 Loss: 0.002929408129264253\n",
      "Iteration: 4691 lambda_k: 1 Loss: 0.002929408129926688\n",
      "Iteration: 4692 lambda_k: 1 Loss: 0.002929408130585931\n",
      "Iteration: 4693 lambda_k: 1 Loss: 0.0029294081312420087\n",
      "Iteration: 4694 lambda_k: 1 Loss: 0.002929408131894927\n",
      "Iteration: 4695 lambda_k: 1 Loss: 0.00292940813254471\n",
      "Iteration: 4696 lambda_k: 1 Loss: 0.002929408133191355\n",
      "Iteration: 4697 lambda_k: 1 Loss: 0.0029294081338349072\n",
      "Iteration: 4698 lambda_k: 1 Loss: 0.0029294081344753476\n",
      "Iteration: 4699 lambda_k: 1 Loss: 0.002929408135112715\n",
      "Iteration: 4700 lambda_k: 1 Loss: 0.0029294081357470144\n",
      "Iteration: 4701 lambda_k: 1 Loss: 0.0029294081363782616\n",
      "Iteration: 4702 lambda_k: 1 Loss: 0.0029294081370064636\n",
      "Iteration: 4703 lambda_k: 1 Loss: 0.0029294081376316536\n",
      "Iteration: 4704 lambda_k: 1 Loss: 0.0029294081382538364\n",
      "Iteration: 4705 lambda_k: 1 Loss: 0.0029294081388730256\n",
      "Iteration: 4706 lambda_k: 1 Loss: 0.002929408139489225\n",
      "Iteration: 4707 lambda_k: 1 Loss: 0.002929408140102475\n",
      "Iteration: 4708 lambda_k: 1 Loss: 0.002929408140712756\n",
      "Iteration: 4709 lambda_k: 1 Loss: 0.0029294081413201094\n",
      "Iteration: 4710 lambda_k: 1 Loss: 0.0029294081419245334\n",
      "Iteration: 4711 lambda_k: 1 Loss: 0.0029294081425260614\n",
      "Iteration: 4712 lambda_k: 1 Loss: 0.002929408143124681\n",
      "Iteration: 4713 lambda_k: 1 Loss: 0.0029294081437204337\n",
      "Iteration: 4714 lambda_k: 1 Loss: 0.0029294081443133175\n",
      "Iteration: 4715 lambda_k: 1 Loss: 0.0029294081449033355\n",
      "Iteration: 4716 lambda_k: 1 Loss: 0.0029294081454905255\n",
      "Iteration: 4717 lambda_k: 1 Loss: 0.0029294081460748784\n",
      "Iteration: 4718 lambda_k: 1 Loss: 0.0029294081466564215\n",
      "Iteration: 4719 lambda_k: 1 Loss: 0.0029294081472351803\n",
      "Iteration: 4720 lambda_k: 1 Loss: 0.0029294081478111384\n",
      "Iteration: 4721 lambda_k: 1 Loss: 0.0029294081483843457\n",
      "Iteration: 4722 lambda_k: 1 Loss: 0.002929408148954803\n",
      "Iteration: 4723 lambda_k: 1 Loss: 0.0029294081495225026\n",
      "Iteration: 4724 lambda_k: 1 Loss: 0.0029294081500874677\n",
      "Iteration: 4725 lambda_k: 1 Loss: 0.0029294081506497055\n",
      "Iteration: 4726 lambda_k: 1 Loss: 0.0029294081512092453\n",
      "Iteration: 4727 lambda_k: 1 Loss: 0.0029294081517661015\n",
      "Iteration: 4728 lambda_k: 1 Loss: 0.0029294081523202815\n",
      "Iteration: 4729 lambda_k: 1 Loss: 0.002929408152871794\n",
      "Iteration: 4730 lambda_k: 1 Loss: 0.002929408153420652\n",
      "Iteration: 4731 lambda_k: 1 Loss: 0.002929408153966867\n",
      "Iteration: 4732 lambda_k: 1 Loss: 0.0029294081545104427\n",
      "Iteration: 4733 lambda_k: 1 Loss: 0.002929408155051416\n",
      "Iteration: 4734 lambda_k: 1 Loss: 0.002929408155589795\n",
      "Iteration: 4735 lambda_k: 1 Loss: 0.002929408156125562\n",
      "Iteration: 4736 lambda_k: 1 Loss: 0.0029294081566587533\n",
      "Iteration: 4737 lambda_k: 1 Loss: 0.0029294081571893944\n",
      "Iteration: 4738 lambda_k: 1 Loss: 0.0029294081577174885\n",
      "Iteration: 4739 lambda_k: 1 Loss: 0.0029294081582430216\n",
      "Iteration: 4740 lambda_k: 1 Loss: 0.002929408158766035\n",
      "Iteration: 4741 lambda_k: 1 Loss: 0.00292940815928653\n",
      "Iteration: 4742 lambda_k: 1 Loss: 0.00292940815980454\n",
      "Iteration: 4743 lambda_k: 1 Loss: 0.002929408160320037\n",
      "Iteration: 4744 lambda_k: 1 Loss: 0.0029294081608330583\n",
      "Iteration: 4745 lambda_k: 1 Loss: 0.0029294081613436117\n",
      "Iteration: 4746 lambda_k: 1 Loss: 0.0029294081618517097\n",
      "Iteration: 4747 lambda_k: 1 Loss: 0.002929408162357371\n",
      "Iteration: 4748 lambda_k: 1 Loss: 0.002929408162860599\n",
      "Iteration: 4749 lambda_k: 1 Loss: 0.0029294081633613996\n",
      "Iteration: 4750 lambda_k: 1 Loss: 0.002929408163859802\n",
      "Iteration: 4751 lambda_k: 1 Loss: 0.0029294081643558034\n",
      "Iteration: 4752 lambda_k: 1 Loss: 0.0029294081648494263\n",
      "Iteration: 4753 lambda_k: 1 Loss: 0.0029294081653406536\n",
      "Iteration: 4754 lambda_k: 1 Loss: 0.0029294081658295143\n",
      "Iteration: 4755 lambda_k: 1 Loss: 0.0029294081663160322\n",
      "Iteration: 4756 lambda_k: 1 Loss: 0.002929408166800214\n",
      "Iteration: 4757 lambda_k: 1 Loss: 0.0029294081672820694\n",
      "Iteration: 4758 lambda_k: 1 Loss: 0.002929408167761625\n",
      "Iteration: 4759 lambda_k: 1 Loss: 0.0029294081682388535\n",
      "Iteration: 4760 lambda_k: 1 Loss: 0.002929408168713777\n",
      "Iteration: 4761 lambda_k: 1 Loss: 0.002929408169186417\n",
      "Iteration: 4762 lambda_k: 1 Loss: 0.002929408169656794\n",
      "Iteration: 4763 lambda_k: 1 Loss: 0.002929408170124901\n",
      "Iteration: 4764 lambda_k: 1 Loss: 0.0029294081705907643\n",
      "Iteration: 4765 lambda_k: 1 Loss: 0.0029294081710543847\n",
      "Iteration: 4766 lambda_k: 1 Loss: 0.0029294081715157947\n",
      "Iteration: 4767 lambda_k: 1 Loss: 0.0029294081719749543\n",
      "Iteration: 4768 lambda_k: 1 Loss: 0.0029294081724319074\n",
      "Iteration: 4769 lambda_k: 1 Loss: 0.0029294081728866565\n",
      "Iteration: 4770 lambda_k: 1 Loss: 0.0029294081733392268\n",
      "Iteration: 4771 lambda_k: 1 Loss: 0.002929408173789618\n",
      "Iteration: 4772 lambda_k: 1 Loss: 0.002929408174237838\n",
      "Iteration: 4773 lambda_k: 1 Loss: 0.002929408174683915\n",
      "Iteration: 4774 lambda_k: 1 Loss: 0.002929408175127844\n",
      "Iteration: 4775 lambda_k: 1 Loss: 0.0029294081755696514\n",
      "Iteration: 4776 lambda_k: 1 Loss: 0.002929408176009324\n",
      "Iteration: 4777 lambda_k: 1 Loss: 0.00292940817644688\n",
      "Iteration: 4778 lambda_k: 1 Loss: 0.0029294081768823335\n",
      "Iteration: 4779 lambda_k: 1 Loss: 0.0029294081773157017\n",
      "Iteration: 4780 lambda_k: 1 Loss: 0.002929408177746979\n",
      "Iteration: 4781 lambda_k: 1 Loss: 0.0029294081781761935\n",
      "Iteration: 4782 lambda_k: 1 Loss: 0.0029294081786033436\n",
      "Iteration: 4783 lambda_k: 1 Loss: 0.0029294081790284423\n",
      "Iteration: 4784 lambda_k: 1 Loss: 0.0029294081794514915\n",
      "Iteration: 4785 lambda_k: 1 Loss: 0.0029294081798725007\n",
      "Iteration: 4786 lambda_k: 1 Loss: 0.0029294081802915027\n",
      "Iteration: 4787 lambda_k: 1 Loss: 0.002929408180708491\n",
      "Iteration: 4788 lambda_k: 1 Loss: 0.0029294081811234622\n",
      "Iteration: 4789 lambda_k: 1 Loss: 0.0029294081815364474\n",
      "Iteration: 4790 lambda_k: 1 Loss: 0.0029294081819474373\n",
      "Iteration: 4791 lambda_k: 1 Loss: 0.002929408182356463\n",
      "Iteration: 4792 lambda_k: 1 Loss: 0.002929408182763518\n",
      "Iteration: 4793 lambda_k: 1 Loss: 0.0029294081831686062\n",
      "Iteration: 4794 lambda_k: 1 Loss: 0.002929408183571744\n",
      "Iteration: 4795 lambda_k: 1 Loss: 0.002929408183972941\n",
      "Iteration: 4796 lambda_k: 1 Loss: 0.0029294081843722107\n",
      "Iteration: 4797 lambda_k: 1 Loss: 0.0029294081847695665\n",
      "Iteration: 4798 lambda_k: 1 Loss: 0.0029294081851650045\n",
      "Iteration: 4799 lambda_k: 1 Loss: 0.0029294081855585417\n",
      "Iteration: 4800 lambda_k: 1 Loss: 0.00292940818595018\n",
      "Iteration: 4801 lambda_k: 1 Loss: 0.002929408186339929\n",
      "Iteration: 4802 lambda_k: 1 Loss: 0.002929408186727816\n",
      "Iteration: 4803 lambda_k: 1 Loss: 0.002929408187113824\n",
      "Iteration: 4804 lambda_k: 1 Loss: 0.002929408187497973\n",
      "Iteration: 4805 lambda_k: 1 Loss: 0.0029294081878802935\n",
      "Iteration: 4806 lambda_k: 1 Loss: 0.0029294081882607808\n",
      "Iteration: 4807 lambda_k: 1 Loss: 0.002929408188639438\n",
      "Iteration: 4808 lambda_k: 1 Loss: 0.00292940818901627\n",
      "Iteration: 4809 lambda_k: 1 Loss: 0.002929408189391283\n",
      "Iteration: 4810 lambda_k: 1 Loss: 0.0029294081897645063\n",
      "Iteration: 4811 lambda_k: 1 Loss: 0.002929408190135927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4812 lambda_k: 1 Loss: 0.0029294081905055515\n",
      "Iteration: 4813 lambda_k: 1 Loss: 0.0029294081908733944\n",
      "Iteration: 4814 lambda_k: 1 Loss: 0.002929408191239479\n",
      "Iteration: 4815 lambda_k: 1 Loss: 0.002929408191603805\n",
      "Iteration: 4816 lambda_k: 1 Loss: 0.0029294081919663834\n",
      "Iteration: 4817 lambda_k: 1 Loss: 0.002929408192327224\n",
      "Iteration: 4818 lambda_k: 1 Loss: 0.0029294081926863145\n",
      "Iteration: 4819 lambda_k: 1 Loss: 0.0029294081930436784\n",
      "Iteration: 4820 lambda_k: 1 Loss: 0.002929408193399319\n",
      "Iteration: 4821 lambda_k: 1 Loss: 0.002929408193753247\n",
      "Iteration: 4822 lambda_k: 1 Loss: 0.0029294081941054913\n",
      "Iteration: 4823 lambda_k: 1 Loss: 0.002929408194456033\n",
      "Iteration: 4824 lambda_k: 1 Loss: 0.002929408194804896\n",
      "Iteration: 4825 lambda_k: 1 Loss: 0.0029294081951520778\n",
      "Iteration: 4826 lambda_k: 1 Loss: 0.0029294081954975835\n",
      "Iteration: 4827 lambda_k: 1 Loss: 0.002929408195841424\n",
      "Iteration: 4828 lambda_k: 1 Loss: 0.002929408196183626\n",
      "Iteration: 4829 lambda_k: 1 Loss: 0.002929408196524186\n",
      "Iteration: 4830 lambda_k: 1 Loss: 0.0029294081968630906\n",
      "Iteration: 4831 lambda_k: 1 Loss: 0.0029294081972003638\n",
      "Iteration: 4832 lambda_k: 1 Loss: 0.0029294081975360427\n",
      "Iteration: 4833 lambda_k: 1 Loss: 0.002929408197870102\n",
      "Iteration: 4834 lambda_k: 1 Loss: 0.002929408198202546\n",
      "Iteration: 4835 lambda_k: 1 Loss: 0.002929408198533399\n",
      "Iteration: 4836 lambda_k: 1 Loss: 0.002929408198862644\n",
      "Iteration: 4837 lambda_k: 1 Loss: 0.002929408199190336\n",
      "Iteration: 4838 lambda_k: 1 Loss: 0.002929408199516444\n",
      "Iteration: 4839 lambda_k: 1 Loss: 0.00292940819984097\n",
      "Iteration: 4840 lambda_k: 1 Loss: 0.002929408200163933\n",
      "Iteration: 4841 lambda_k: 1 Loss: 0.002929408200485346\n",
      "Iteration: 4842 lambda_k: 1 Loss: 0.0029294082008052163\n",
      "Iteration: 4843 lambda_k: 1 Loss: 0.0029294082011235567\n",
      "Iteration: 4844 lambda_k: 1 Loss: 0.0029294082014403523\n",
      "Iteration: 4845 lambda_k: 1 Loss: 0.0029294082017556496\n",
      "Iteration: 4846 lambda_k: 1 Loss: 0.002929408202069425\n",
      "Iteration: 4847 lambda_k: 1 Loss: 0.0029294082023816766\n",
      "Iteration: 4848 lambda_k: 1 Loss: 0.0029294082026924345\n",
      "Iteration: 4849 lambda_k: 1 Loss: 0.002929408203001698\n",
      "Iteration: 4850 lambda_k: 1 Loss: 0.002929408203309484\n",
      "Iteration: 4851 lambda_k: 1 Loss: 0.002929408203615785\n",
      "Iteration: 4852 lambda_k: 1 Loss: 0.0029294082039205928\n",
      "Iteration: 4853 lambda_k: 1 Loss: 0.0029294082042239616\n",
      "Iteration: 4854 lambda_k: 1 Loss: 0.002929408204525873\n",
      "Iteration: 4855 lambda_k: 1 Loss: 0.0029294082048263296\n",
      "Iteration: 4856 lambda_k: 1 Loss: 0.0029294082051253226\n",
      "Iteration: 4857 lambda_k: 1 Loss: 0.0029294082054229045\n",
      "Iteration: 4858 lambda_k: 1 Loss: 0.0029294082057190435\n",
      "Iteration: 4859 lambda_k: 1 Loss: 0.0029294082060137465\n",
      "Iteration: 4860 lambda_k: 1 Loss: 0.002929408206307046\n",
      "Iteration: 4861 lambda_k: 1 Loss: 0.0029294082065989433\n",
      "Iteration: 4862 lambda_k: 1 Loss: 0.0029294082068894267\n",
      "Iteration: 4863 lambda_k: 1 Loss: 0.0029294082071785192\n",
      "Iteration: 4864 lambda_k: 1 Loss: 0.0029294082074662223\n",
      "Iteration: 4865 lambda_k: 1 Loss: 0.0029294082077525444\n",
      "Iteration: 4866 lambda_k: 1 Loss: 0.00292940820803748\n",
      "Iteration: 4867 lambda_k: 1 Loss: 0.002929408208321055\n",
      "Iteration: 4868 lambda_k: 1 Loss: 0.0029294082086032663\n",
      "Iteration: 4869 lambda_k: 1 Loss: 0.002929408208884113\n",
      "Iteration: 4870 lambda_k: 1 Loss: 0.002929408209163619\n",
      "Iteration: 4871 lambda_k: 1 Loss: 0.0029294082094417757\n",
      "Iteration: 4872 lambda_k: 1 Loss: 0.0029294082097185885\n",
      "Iteration: 4873 lambda_k: 1 Loss: 0.002929408209994085\n",
      "Iteration: 4874 lambda_k: 1 Loss: 0.0029294082102682374\n",
      "Iteration: 4875 lambda_k: 1 Loss: 0.00292940821054109\n",
      "Iteration: 4876 lambda_k: 1 Loss: 0.002929408210812628\n",
      "Iteration: 4877 lambda_k: 1 Loss: 0.0029294082110828596\n",
      "Iteration: 4878 lambda_k: 1 Loss: 0.002929408211351808\n",
      "Iteration: 4879 lambda_k: 1 Loss: 0.0029294082116194534\n",
      "Iteration: 4880 lambda_k: 1 Loss: 0.0029294082118857994\n",
      "Iteration: 4881 lambda_k: 1 Loss: 0.002929408212150877\n",
      "Iteration: 4882 lambda_k: 1 Loss: 0.0029294082124146793\n",
      "Iteration: 4883 lambda_k: 1 Loss: 0.0029294082126772028\n",
      "Iteration: 4884 lambda_k: 1 Loss: 0.0029294082129384803\n",
      "Iteration: 4885 lambda_k: 1 Loss: 0.002929408213198496\n",
      "Iteration: 4886 lambda_k: 1 Loss: 0.0029294082134572637\n",
      "Iteration: 4887 lambda_k: 1 Loss: 0.002929408213714784\n",
      "Iteration: 4888 lambda_k: 1 Loss: 0.002929408213971064\n",
      "Iteration: 4889 lambda_k: 1 Loss: 0.00292940821422612\n",
      "Iteration: 4890 lambda_k: 1 Loss: 0.0029294082144799565\n",
      "Iteration: 4891 lambda_k: 1 Loss: 0.002929408214732577\n",
      "Iteration: 4892 lambda_k: 1 Loss: 0.0029294082149839747\n",
      "Iteration: 4893 lambda_k: 1 Loss: 0.002929408215234149\n",
      "Iteration: 4894 lambda_k: 1 Loss: 0.0029294082154831353\n",
      "Iteration: 4895 lambda_k: 1 Loss: 0.0029294082157309237\n",
      "Iteration: 4896 lambda_k: 1 Loss: 0.002929408215977506\n",
      "Iteration: 4897 lambda_k: 1 Loss: 0.002929408216222919\n",
      "Iteration: 4898 lambda_k: 1 Loss: 0.0029294082164671455\n",
      "Iteration: 4899 lambda_k: 1 Loss: 0.002929408216710202\n",
      "Iteration: 4900 lambda_k: 1 Loss: 0.0029294082169520844\n",
      "Iteration: 4901 lambda_k: 1 Loss: 0.0029294082171928246\n",
      "Iteration: 4902 lambda_k: 1 Loss: 0.002929408217432398\n",
      "Iteration: 4903 lambda_k: 1 Loss: 0.0029294082176708363\n",
      "Iteration: 4904 lambda_k: 1 Loss: 0.002929408217908108\n",
      "Iteration: 4905 lambda_k: 1 Loss: 0.0029294082181442315\n",
      "Iteration: 4906 lambda_k: 1 Loss: 0.002929408218379233\n",
      "Iteration: 4907 lambda_k: 1 Loss: 0.0029294082186130986\n",
      "Iteration: 4908 lambda_k: 1 Loss: 0.002929408218845852\n",
      "Iteration: 4909 lambda_k: 1 Loss: 0.0029294082190774763\n",
      "Iteration: 4910 lambda_k: 1 Loss: 0.002929408219307998\n",
      "Iteration: 4911 lambda_k: 1 Loss: 0.0029294082195374088\n",
      "Iteration: 4912 lambda_k: 1 Loss: 0.0029294082197657005\n",
      "Iteration: 4913 lambda_k: 1 Loss: 0.002929408219992901\n",
      "Iteration: 4914 lambda_k: 1 Loss: 0.0029294082202190033\n",
      "Iteration: 4915 lambda_k: 1 Loss: 0.002929408220444034\n",
      "Iteration: 4916 lambda_k: 1 Loss: 0.0029294082206679788\n",
      "Iteration: 4917 lambda_k: 1 Loss: 0.0029294082208908456\n",
      "Iteration: 4918 lambda_k: 1 Loss: 0.002929408221112632\n",
      "Iteration: 4919 lambda_k: 1 Loss: 0.0029294082213333758\n",
      "Iteration: 4920 lambda_k: 1 Loss: 0.0029294082215530507\n",
      "Iteration: 4921 lambda_k: 1 Loss: 0.002929408221771668\n",
      "Iteration: 4922 lambda_k: 1 Loss: 0.0029294082219892383\n",
      "Iteration: 4923 lambda_k: 1 Loss: 0.0029294082222057613\n",
      "Iteration: 4924 lambda_k: 1 Loss: 0.0029294082224212404\n",
      "Iteration: 4925 lambda_k: 1 Loss: 0.0029294082226356934\n",
      "Iteration: 4926 lambda_k: 1 Loss: 0.0029294082228491095\n",
      "Iteration: 4927 lambda_k: 1 Loss: 0.002929408223061511\n",
      "Iteration: 4928 lambda_k: 1 Loss: 0.002929408223272878\n",
      "Iteration: 4929 lambda_k: 1 Loss: 0.0029294082234832385\n",
      "Iteration: 4930 lambda_k: 1 Loss: 0.002929408223692581\n",
      "Iteration: 4931 lambda_k: 1 Loss: 0.002929408223900925\n",
      "Iteration: 4932 lambda_k: 1 Loss: 0.0029294082241082546\n",
      "Iteration: 4933 lambda_k: 1 Loss: 0.0029294082243146056\n",
      "Iteration: 4934 lambda_k: 1 Loss: 0.002929408224519963\n",
      "Iteration: 4935 lambda_k: 1 Loss: 0.002929408224724343\n",
      "Iteration: 4936 lambda_k: 1 Loss: 0.0029294082249277263\n",
      "Iteration: 4937 lambda_k: 1 Loss: 0.0029294082251301442\n",
      "Iteration: 4938 lambda_k: 1 Loss: 0.0029294082253315703\n",
      "Iteration: 4939 lambda_k: 1 Loss: 0.002929408225532035\n",
      "Iteration: 4940 lambda_k: 1 Loss: 0.0029294082257315364\n",
      "Iteration: 4941 lambda_k: 1 Loss: 0.002929408225930083\n",
      "Iteration: 4942 lambda_k: 1 Loss: 0.002929408226127666\n",
      "Iteration: 4943 lambda_k: 1 Loss: 0.0029294082263243153\n",
      "Iteration: 4944 lambda_k: 1 Loss: 0.002929408226519995\n",
      "Iteration: 4945 lambda_k: 1 Loss: 0.002929408226714744\n",
      "Iteration: 4946 lambda_k: 1 Loss: 0.0029294082269085697\n",
      "Iteration: 4947 lambda_k: 1 Loss: 0.002929408227101454\n",
      "Iteration: 4948 lambda_k: 1 Loss: 0.002929408227293426\n",
      "Iteration: 4949 lambda_k: 1 Loss: 0.0029294082274844637\n",
      "Iteration: 4950 lambda_k: 1 Loss: 0.002929408227674595\n",
      "Iteration: 4951 lambda_k: 1 Loss: 0.002929408227863812\n",
      "Iteration: 4952 lambda_k: 1 Loss: 0.002929408228052129\n",
      "Iteration: 4953 lambda_k: 1 Loss: 0.002929408228239518\n",
      "Iteration: 4954 lambda_k: 1 Loss: 0.0029294082284260204\n",
      "Iteration: 4955 lambda_k: 1 Loss: 0.0029294082286116145\n",
      "Iteration: 4956 lambda_k: 1 Loss: 0.002929408228796325\n",
      "Iteration: 4957 lambda_k: 1 Loss: 0.0029294082289801483\n",
      "Iteration: 4958 lambda_k: 1 Loss: 0.0029294082291630823\n",
      "Iteration: 4959 lambda_k: 1 Loss: 0.0029294082293451545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4960 lambda_k: 1 Loss: 0.00292940822952633\n",
      "Iteration: 4961 lambda_k: 1 Loss: 0.0029294082297066557\n",
      "Iteration: 4962 lambda_k: 1 Loss: 0.002929408229886093\n",
      "Iteration: 4963 lambda_k: 1 Loss: 0.002929408230064673\n",
      "Iteration: 4964 lambda_k: 1 Loss: 0.0029294082302424125\n",
      "Iteration: 4965 lambda_k: 1 Loss: 0.002929408230419291\n",
      "Iteration: 4966 lambda_k: 1 Loss: 0.0029294082305953185\n",
      "Iteration: 4967 lambda_k: 1 Loss: 0.0029294082307704914\n",
      "Iteration: 4968 lambda_k: 1 Loss: 0.002929408230944837\n",
      "Iteration: 4969 lambda_k: 1 Loss: 0.002929408231118336\n",
      "Iteration: 4970 lambda_k: 1 Loss: 0.0029294082312909964\n",
      "Iteration: 4971 lambda_k: 1 Loss: 0.002929408231462833\n",
      "Iteration: 4972 lambda_k: 1 Loss: 0.002929408231633844\n",
      "Iteration: 4973 lambda_k: 1 Loss: 0.002929408231804033\n",
      "Iteration: 4974 lambda_k: 1 Loss: 0.0029294082319734007\n",
      "Iteration: 4975 lambda_k: 1 Loss: 0.00292940823214196\n",
      "Iteration: 4976 lambda_k: 1 Loss: 0.0029294082323097145\n",
      "Iteration: 4977 lambda_k: 1 Loss: 0.0029294082324766556\n",
      "Iteration: 4978 lambda_k: 1 Loss: 0.0029294082326427975\n",
      "Iteration: 4979 lambda_k: 1 Loss: 0.002929408232808149\n",
      "Iteration: 4980 lambda_k: 1 Loss: 0.002929408232972698\n",
      "Iteration: 4981 lambda_k: 1 Loss: 0.0029294082331364733\n",
      "Iteration: 4982 lambda_k: 1 Loss: 0.0029294082332994493\n",
      "Iteration: 4983 lambda_k: 1 Loss: 0.0029294082334616394\n",
      "Iteration: 4984 lambda_k: 1 Loss: 0.0029294082336230355\n",
      "Iteration: 4985 lambda_k: 1 Loss: 0.0029294082337836696\n",
      "Iteration: 4986 lambda_k: 1 Loss: 0.0029294082339435413\n",
      "Iteration: 4987 lambda_k: 1 Loss: 0.002929408234102634\n",
      "Iteration: 4988 lambda_k: 1 Loss: 0.0029294082342609567\n",
      "Iteration: 4989 lambda_k: 1 Loss: 0.0029294082344185264\n",
      "Iteration: 4990 lambda_k: 1 Loss: 0.002929408234575337\n",
      "Iteration: 4991 lambda_k: 1 Loss: 0.002929408234731399\n",
      "Iteration: 4992 lambda_k: 1 Loss: 0.002929408234886726\n",
      "Iteration: 4993 lambda_k: 1 Loss: 0.002929408235041297\n",
      "Iteration: 4994 lambda_k: 1 Loss: 0.0029294082351951373\n",
      "Iteration: 4995 lambda_k: 1 Loss: 0.0029294082353482145\n",
      "Iteration: 4996 lambda_k: 1 Loss: 0.002929408235500567\n",
      "Iteration: 4997 lambda_k: 1 Loss: 0.0029294082356521836\n",
      "Iteration: 4998 lambda_k: 1 Loss: 0.0029294082358030716\n",
      "Iteration: 4999 lambda_k: 1 Loss: 0.0029294082359532466\n",
      "Iteration: 5000 lambda_k: 1 Loss: 0.0029294082361027013\n",
      "Iteration: 5001 lambda_k: 1 Loss: 0.0029294082362514282\n",
      "Iteration: 5002 lambda_k: 1 Loss: 0.0029294082363994453\n",
      "Iteration: 5003 lambda_k: 1 Loss: 0.002929408236546755\n",
      "Iteration: 5004 lambda_k: 1 Loss: 0.002929408236693344\n",
      "Iteration: 5005 lambda_k: 1 Loss: 0.0029294082368392397\n",
      "Iteration: 5006 lambda_k: 1 Loss: 0.002929408236984419\n",
      "Iteration: 5007 lambda_k: 1 Loss: 0.002929408237128906\n",
      "Iteration: 5008 lambda_k: 1 Loss: 0.002929408237272705\n",
      "Iteration: 5009 lambda_k: 1 Loss: 0.0029294082374158236\n",
      "Iteration: 5010 lambda_k: 1 Loss: 0.0029294082375582397\n",
      "Iteration: 5011 lambda_k: 1 Loss: 0.0029294082376999787\n",
      "Iteration: 5012 lambda_k: 1 Loss: 0.0029294082378410213\n",
      "Iteration: 5013 lambda_k: 1 Loss: 0.0029294082379813986\n",
      "Iteration: 5014 lambda_k: 1 Loss: 0.0029294082381211093\n",
      "Iteration: 5015 lambda_k: 1 Loss: 0.0029294082382601335\n",
      "Iteration: 5016 lambda_k: 1 Loss: 0.002929408238398491\n",
      "Iteration: 5017 lambda_k: 1 Loss: 0.002929408238536198\n",
      "Iteration: 5018 lambda_k: 1 Loss: 0.002929408238673227\n",
      "Iteration: 5019 lambda_k: 1 Loss: 0.0029294082388096093\n",
      "Iteration: 5020 lambda_k: 1 Loss: 0.002929408238945332\n",
      "Iteration: 5021 lambda_k: 1 Loss: 0.0029294082390804127\n",
      "Iteration: 5022 lambda_k: 1 Loss: 0.0029294082392148277\n",
      "Iteration: 5023 lambda_k: 1 Loss: 0.002929408239348604\n",
      "Iteration: 5024 lambda_k: 1 Loss: 0.0029294082394817466\n",
      "Iteration: 5025 lambda_k: 1 Loss: 0.002929408239614243\n",
      "Iteration: 5026 lambda_k: 1 Loss: 0.0029294082397460968\n",
      "Iteration: 5027 lambda_k: 1 Loss: 0.0029294082398773347\n",
      "Iteration: 5028 lambda_k: 1 Loss: 0.002929408240007933\n",
      "Iteration: 5029 lambda_k: 1 Loss: 0.0029294082401379066\n",
      "Iteration: 5030 lambda_k: 1 Loss: 0.002929408240267261\n",
      "Iteration: 5031 lambda_k: 1 Loss: 0.0029294082403959814\n",
      "Iteration: 5032 lambda_k: 1 Loss: 0.0029294082405240873\n",
      "Iteration: 5033 lambda_k: 1 Loss: 0.0029294082406515843\n",
      "Iteration: 5034 lambda_k: 1 Loss: 0.00292940824077847\n",
      "Iteration: 5035 lambda_k: 1 Loss: 0.002929408240904744\n",
      "Iteration: 5036 lambda_k: 1 Loss: 0.002929408241030422\n",
      "Iteration: 5037 lambda_k: 1 Loss: 0.002929408241155491\n",
      "Iteration: 5038 lambda_k: 1 Loss: 0.002929408241279956\n",
      "Iteration: 5039 lambda_k: 1 Loss: 0.002929408241403817\n",
      "Iteration: 5040 lambda_k: 1 Loss: 0.002929408241527091\n",
      "Iteration: 5041 lambda_k: 1 Loss: 0.0029294082416497632\n",
      "Iteration: 5042 lambda_k: 1 Loss: 0.0029294082417718487\n",
      "Iteration: 5043 lambda_k: 1 Loss: 0.0029294082418933535\n",
      "Iteration: 5044 lambda_k: 1 Loss: 0.0029294082420142763\n",
      "Iteration: 5045 lambda_k: 1 Loss: 0.00292940824213461\n",
      "Iteration: 5046 lambda_k: 1 Loss: 0.0029294082422543664\n",
      "Iteration: 5047 lambda_k: 1 Loss: 0.002929408242373569\n",
      "Iteration: 5048 lambda_k: 1 Loss: 0.002929408242492175\n",
      "Iteration: 5049 lambda_k: 1 Loss: 0.0029294082426102107\n",
      "Iteration: 5050 lambda_k: 1 Loss: 0.0029294082427276835\n",
      "Iteration: 5051 lambda_k: 1 Loss: 0.0029294082428446126\n",
      "Iteration: 5052 lambda_k: 1 Loss: 0.002929408242960957\n",
      "Iteration: 5053 lambda_k: 1 Loss: 0.0029294082430767567\n",
      "Iteration: 5054 lambda_k: 1 Loss: 0.0029294082431919983\n",
      "Iteration: 5055 lambda_k: 1 Loss: 0.0029294082433066753\n",
      "Iteration: 5056 lambda_k: 1 Loss: 0.0029294082434208114\n",
      "Iteration: 5057 lambda_k: 1 Loss: 0.0029294082435343946\n",
      "Iteration: 5058 lambda_k: 1 Loss: 0.0029294082436474352\n",
      "Iteration: 5059 lambda_k: 1 Loss: 0.0029294082437599242\n",
      "Iteration: 5060 lambda_k: 1 Loss: 0.002929408243871873\n",
      "Iteration: 5061 lambda_k: 1 Loss: 0.002929408243983301\n",
      "Iteration: 5062 lambda_k: 1 Loss: 0.002929408244094197\n",
      "Iteration: 5063 lambda_k: 1 Loss: 0.002929408244204547\n",
      "Iteration: 5064 lambda_k: 1 Loss: 0.0029294082443143705\n",
      "Iteration: 5065 lambda_k: 1 Loss: 0.0029294082444236693\n",
      "Iteration: 5066 lambda_k: 1 Loss: 0.002929408244532456\n",
      "Iteration: 5067 lambda_k: 1 Loss: 0.002929408244640693\n",
      "Iteration: 5068 lambda_k: 1 Loss: 0.0029294082447484234\n",
      "Iteration: 5069 lambda_k: 1 Loss: 0.002929408244855646\n",
      "Iteration: 5070 lambda_k: 1 Loss: 0.0029294082449623595\n",
      "Iteration: 5071 lambda_k: 1 Loss: 0.0029294082450685415\n",
      "Iteration: 5072 lambda_k: 1 Loss: 0.002929408245174218\n",
      "Iteration: 5073 lambda_k: 1 Loss: 0.0029294082452793876\n",
      "Iteration: 5074 lambda_k: 1 Loss: 0.00292940824538406\n",
      "Iteration: 5075 lambda_k: 1 Loss: 0.002929408245488207\n",
      "Iteration: 5076 lambda_k: 1 Loss: 0.00292940824559187\n",
      "Iteration: 5077 lambda_k: 1 Loss: 0.0029294082456950395\n",
      "Iteration: 5078 lambda_k: 1 Loss: 0.0029294082457976965\n",
      "Iteration: 5079 lambda_k: 1 Loss: 0.0029294082458998717\n",
      "Iteration: 5080 lambda_k: 1 Loss: 0.0029294082460015526\n",
      "Iteration: 5081 lambda_k: 1 Loss: 0.0029294082461027464\n",
      "Iteration: 5082 lambda_k: 1 Loss: 0.0029294082462034666\n",
      "Iteration: 5083 lambda_k: 1 Loss: 0.0029294082463036876\n",
      "Iteration: 5084 lambda_k: 1 Loss: 0.002929408246403428\n",
      "Iteration: 5085 lambda_k: 1 Loss: 0.0029294082465026942\n",
      "Iteration: 5086 lambda_k: 1 Loss: 0.002929408246601499\n",
      "Iteration: 5087 lambda_k: 1 Loss: 0.0029294082466998043\n",
      "Iteration: 5088 lambda_k: 1 Loss: 0.0029294082467976406\n",
      "Iteration: 5089 lambda_k: 1 Loss: 0.0029294082468950215\n",
      "Iteration: 5090 lambda_k: 1 Loss: 0.002929408246991928\n",
      "Iteration: 5091 lambda_k: 1 Loss: 0.002929408247088366\n",
      "Iteration: 5092 lambda_k: 1 Loss: 0.0029294082471843395\n",
      "Iteration: 5093 lambda_k: 1 Loss: 0.0029294082472798634\n",
      "Iteration: 5094 lambda_k: 1 Loss: 0.0029294082473749193\n",
      "Iteration: 5095 lambda_k: 1 Loss: 0.002929408247469532\n",
      "Iteration: 5096 lambda_k: 1 Loss: 0.002929408247563676\n",
      "Iteration: 5097 lambda_k: 1 Loss: 0.002929408247657374\n",
      "Iteration: 5098 lambda_k: 1 Loss: 0.002929408247750628\n",
      "Iteration: 5099 lambda_k: 1 Loss: 0.002929408247843443\n",
      "Iteration: 5100 lambda_k: 1 Loss: 0.0029294082479357987\n",
      "Iteration: 5101 lambda_k: 1 Loss: 0.002929408248027697\n",
      "Iteration: 5102 lambda_k: 1 Loss: 0.0029294082481191677\n",
      "Iteration: 5103 lambda_k: 1 Loss: 0.002929408248210212\n",
      "Iteration: 5104 lambda_k: 1 Loss: 0.0029294082483008184\n",
      "Iteration: 5105 lambda_k: 1 Loss: 0.0029294082483909733\n",
      "Iteration: 5106 lambda_k: 1 Loss: 0.002929408248480701\n",
      "Iteration: 5107 lambda_k: 1 Loss: 0.0029294082485699963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5108 lambda_k: 1 Loss: 0.002929408248658861\n",
      "Iteration: 5109 lambda_k: 1 Loss: 0.002929408248747319\n",
      "Iteration: 5110 lambda_k: 1 Loss: 0.002929408248835327\n",
      "Iteration: 5111 lambda_k: 1 Loss: 0.002929408248922928\n",
      "Iteration: 5112 lambda_k: 1 Loss: 0.0029294082490101034\n",
      "Iteration: 5113 lambda_k: 1 Loss: 0.0029294082490968648\n",
      "Iteration: 5114 lambda_k: 1 Loss: 0.002929408249183196\n",
      "Iteration: 5115 lambda_k: 1 Loss: 0.0029294082492691094\n",
      "Iteration: 5116 lambda_k: 1 Loss: 0.0029294082493546178\n",
      "Iteration: 5117 lambda_k: 1 Loss: 0.0029294082494397172\n",
      "Iteration: 5118 lambda_k: 1 Loss: 0.0029294082495244103\n",
      "Iteration: 5119 lambda_k: 1 Loss: 0.0029294082496087045\n",
      "Iteration: 5120 lambda_k: 1 Loss: 0.0029294082496925822\n",
      "Iteration: 5121 lambda_k: 1 Loss: 0.0029294082497760545\n",
      "Iteration: 5122 lambda_k: 1 Loss: 0.0029294082498591296\n",
      "Iteration: 5123 lambda_k: 1 Loss: 0.0029294082499418113\n",
      "Iteration: 5124 lambda_k: 1 Loss: 0.0029294082500241144\n",
      "Iteration: 5125 lambda_k: 1 Loss: 0.0029294082501060037\n",
      "Iteration: 5126 lambda_k: 1 Loss: 0.002929408250187496\n",
      "Iteration: 5127 lambda_k: 1 Loss: 0.0029294082502685976\n",
      "Iteration: 5128 lambda_k: 1 Loss: 0.002929408250349315\n",
      "Iteration: 5129 lambda_k: 1 Loss: 0.002929408250429657\n",
      "Iteration: 5130 lambda_k: 1 Loss: 0.0029294082505096066\n",
      "Iteration: 5131 lambda_k: 1 Loss: 0.002929408250589159\n",
      "Iteration: 5132 lambda_k: 1 Loss: 0.0029294082506683356\n",
      "Iteration: 5133 lambda_k: 1 Loss: 0.0029294082507471436\n",
      "Iteration: 5134 lambda_k: 1 Loss: 0.002929408250825579\n",
      "Iteration: 5135 lambda_k: 1 Loss: 0.002929408250903621\n",
      "Iteration: 5136 lambda_k: 1 Loss: 0.0029294082509812892\n",
      "Iteration: 5137 lambda_k: 1 Loss: 0.0029294082510585785\n",
      "Iteration: 5138 lambda_k: 1 Loss: 0.0029294082511354983\n",
      "Iteration: 5139 lambda_k: 1 Loss: 0.002929408251212056\n",
      "Iteration: 5140 lambda_k: 1 Loss: 0.0029294082512882403\n",
      "Iteration: 5141 lambda_k: 1 Loss: 0.0029294082513640542\n",
      "Iteration: 5142 lambda_k: 1 Loss: 0.0029294082514395177\n",
      "Iteration: 5143 lambda_k: 1 Loss: 0.002929408251514622\n",
      "Iteration: 5144 lambda_k: 1 Loss: 0.0029294082515893597\n",
      "Iteration: 5145 lambda_k: 1 Loss: 0.0029294082516637425\n",
      "Iteration: 5146 lambda_k: 1 Loss: 0.002929408251737752\n",
      "Iteration: 5147 lambda_k: 1 Loss: 0.002929408251811406\n",
      "Iteration: 5148 lambda_k: 1 Loss: 0.00292940825188472\n",
      "Iteration: 5149 lambda_k: 1 Loss: 0.002929408251957684\n",
      "Iteration: 5150 lambda_k: 1 Loss: 0.0029294082520302956\n",
      "Iteration: 5151 lambda_k: 1 Loss: 0.0029294082521025637\n",
      "Iteration: 5152 lambda_k: 1 Loss: 0.002929408252174469\n",
      "Iteration: 5153 lambda_k: 1 Loss: 0.0029294082522460354\n",
      "Iteration: 5154 lambda_k: 1 Loss: 0.0029294082523172653\n",
      "Iteration: 5155 lambda_k: 1 Loss: 0.002929408252388158\n",
      "Iteration: 5156 lambda_k: 1 Loss: 0.0029294082524587016\n",
      "Iteration: 5157 lambda_k: 1 Loss: 0.002929408252528913\n",
      "Iteration: 5158 lambda_k: 1 Loss: 0.002929408252598774\n",
      "Iteration: 5159 lambda_k: 1 Loss: 0.0029294082526682983\n",
      "Iteration: 5160 lambda_k: 1 Loss: 0.002929408252737492\n",
      "Iteration: 5161 lambda_k: 1 Loss: 0.002929408252806362\n",
      "Iteration: 5162 lambda_k: 1 Loss: 0.002929408252874912\n",
      "Iteration: 5163 lambda_k: 1 Loss: 0.0029294082529431314\n",
      "Iteration: 5164 lambda_k: 1 Loss: 0.0029294082530110115\n",
      "Iteration: 5165 lambda_k: 1 Loss: 0.002929408253078554\n",
      "Iteration: 5166 lambda_k: 1 Loss: 0.002929408253145779\n",
      "Iteration: 5167 lambda_k: 1 Loss: 0.0029294082532126914\n",
      "Iteration: 5168 lambda_k: 1 Loss: 0.0029294082532792887\n",
      "Iteration: 5169 lambda_k: 1 Loss: 0.002929408253345559\n",
      "Iteration: 5170 lambda_k: 1 Loss: 0.002929408253411506\n",
      "Iteration: 5171 lambda_k: 1 Loss: 0.002929408253477129\n",
      "Iteration: 5172 lambda_k: 1 Loss: 0.0029294082535424484\n",
      "Iteration: 5173 lambda_k: 1 Loss: 0.002929408253607454\n",
      "Iteration: 5174 lambda_k: 1 Loss: 0.0029294082536721502\n",
      "Iteration: 5175 lambda_k: 1 Loss: 0.0029294082537365414\n",
      "Iteration: 5176 lambda_k: 1 Loss: 0.0029294082538006048\n",
      "Iteration: 5177 lambda_k: 1 Loss: 0.002929408253864361\n",
      "Iteration: 5178 lambda_k: 1 Loss: 0.00292940825392781\n",
      "Iteration: 5179 lambda_k: 1 Loss: 0.0029294082539909603\n",
      "Iteration: 5180 lambda_k: 1 Loss: 0.00292940825405382\n",
      "Iteration: 5181 lambda_k: 1 Loss: 0.0029294082541163795\n",
      "Iteration: 5182 lambda_k: 1 Loss: 0.0029294082541786426\n",
      "Iteration: 5183 lambda_k: 1 Loss: 0.0029294082542405905\n",
      "Iteration: 5184 lambda_k: 1 Loss: 0.002929408254302236\n",
      "Iteration: 5185 lambda_k: 1 Loss: 0.0029294082543635793\n",
      "Iteration: 5186 lambda_k: 1 Loss: 0.002929408254424636\n",
      "Iteration: 5187 lambda_k: 1 Loss: 0.002929408254485398\n",
      "Iteration: 5188 lambda_k: 1 Loss: 0.0029294082545458836\n",
      "Iteration: 5189 lambda_k: 1 Loss: 0.0029294082546060815\n",
      "Iteration: 5190 lambda_k: 1 Loss: 0.0029294082546659755\n",
      "Iteration: 5191 lambda_k: 1 Loss: 0.0029294082547255767\n",
      "Iteration: 5192 lambda_k: 1 Loss: 0.0029294082547848925\n",
      "Iteration: 5193 lambda_k: 1 Loss: 0.0029294082548439273\n",
      "Iteration: 5194 lambda_k: 1 Loss: 0.002929408254902685\n",
      "Iteration: 5195 lambda_k: 1 Loss: 0.0029294082549611612\n",
      "Iteration: 5196 lambda_k: 1 Loss: 0.0029294082550193694\n",
      "Iteration: 5197 lambda_k: 1 Loss: 0.002929408255077295\n",
      "Iteration: 5198 lambda_k: 1 Loss: 0.002929408255134929\n",
      "Iteration: 5199 lambda_k: 1 Loss: 0.0029294082551922936\n",
      "Iteration: 5200 lambda_k: 1 Loss: 0.00292940825524937\n",
      "Iteration: 5201 lambda_k: 1 Loss: 0.0029294082553061834\n",
      "Iteration: 5202 lambda_k: 1 Loss: 0.0029294082553627298\n",
      "Iteration: 5203 lambda_k: 1 Loss: 0.0029294082554190103\n",
      "Iteration: 5204 lambda_k: 1 Loss: 0.0029294082554750227\n",
      "Iteration: 5205 lambda_k: 1 Loss: 0.002929408255530755\n",
      "Iteration: 5206 lambda_k: 1 Loss: 0.002929408255586212\n",
      "Iteration: 5207 lambda_k: 1 Loss: 0.002929408255641407\n",
      "Iteration: 5208 lambda_k: 1 Loss: 0.00292940825569634\n",
      "Iteration: 5209 lambda_k: 1 Loss: 0.002929408255751011\n",
      "Iteration: 5210 lambda_k: 1 Loss: 0.002929408255805422\n",
      "Iteration: 5211 lambda_k: 1 Loss: 0.00292940825585957\n",
      "Iteration: 5212 lambda_k: 1 Loss: 0.002929408255913452\n",
      "Iteration: 5213 lambda_k: 1 Loss: 0.00292940825596708\n",
      "Iteration: 5214 lambda_k: 1 Loss: 0.002929408256020453\n",
      "Iteration: 5215 lambda_k: 1 Loss: 0.002929408256073558\n",
      "Iteration: 5216 lambda_k: 1 Loss: 0.0029294082561264114\n",
      "Iteration: 5217 lambda_k: 1 Loss: 0.0029294082561790264\n",
      "Iteration: 5218 lambda_k: 1 Loss: 0.002929408256231388\n",
      "Iteration: 5219 lambda_k: 1 Loss: 0.002929408256283501\n",
      "Iteration: 5220 lambda_k: 1 Loss: 0.0029294082563353454\n",
      "Iteration: 5221 lambda_k: 1 Loss: 0.002929408256386936\n",
      "Iteration: 5222 lambda_k: 1 Loss: 0.0029294082564382796\n",
      "Iteration: 5223 lambda_k: 1 Loss: 0.0029294082564893806\n",
      "Iteration: 5224 lambda_k: 1 Loss: 0.0029294082565402353\n",
      "Iteration: 5225 lambda_k: 1 Loss: 0.0029294082565908593\n",
      "Iteration: 5226 lambda_k: 1 Loss: 0.0029294082566412387\n",
      "Iteration: 5227 lambda_k: 1 Loss: 0.002929408256691392\n",
      "Iteration: 5228 lambda_k: 1 Loss: 0.002929408256741279\n",
      "Iteration: 5229 lambda_k: 1 Loss: 0.00292940825679095\n",
      "Iteration: 5230 lambda_k: 1 Loss: 0.0029294082568403655\n",
      "Iteration: 5231 lambda_k: 1 Loss: 0.00292940825688954\n",
      "Iteration: 5232 lambda_k: 1 Loss: 0.0029294082569384797\n",
      "Iteration: 5233 lambda_k: 1 Loss: 0.002929408256987196\n",
      "Iteration: 5234 lambda_k: 1 Loss: 0.002929408257035677\n",
      "Iteration: 5235 lambda_k: 1 Loss: 0.0029294082570839207\n",
      "Iteration: 5236 lambda_k: 1 Loss: 0.0029294082571319443\n",
      "Iteration: 5237 lambda_k: 1 Loss: 0.0029294082571797277\n",
      "Iteration: 5238 lambda_k: 1 Loss: 0.0029294082572272713\n",
      "Iteration: 5239 lambda_k: 1 Loss: 0.0029294082572745876\n",
      "Iteration: 5240 lambda_k: 1 Loss: 0.0029294082573216858\n",
      "Iteration: 5241 lambda_k: 1 Loss: 0.002929408257368554\n",
      "Iteration: 5242 lambda_k: 1 Loss: 0.002929408257415194\n",
      "Iteration: 5243 lambda_k: 1 Loss: 0.002929408257461619\n",
      "Iteration: 5244 lambda_k: 1 Loss: 0.0029294082575078225\n",
      "Iteration: 5245 lambda_k: 1 Loss: 0.002929408257553813\n",
      "Iteration: 5246 lambda_k: 1 Loss: 0.0029294082575995846\n",
      "Iteration: 5247 lambda_k: 1 Loss: 0.0029294082576451267\n",
      "Iteration: 5248 lambda_k: 1 Loss: 0.00292940825769044\n",
      "Iteration: 5249 lambda_k: 1 Loss: 0.002929408257735548\n",
      "Iteration: 5250 lambda_k: 1 Loss: 0.0029294082577804316\n",
      "Iteration: 5251 lambda_k: 1 Loss: 0.002929408257825106\n",
      "Iteration: 5252 lambda_k: 1 Loss: 0.002929408257869573\n",
      "Iteration: 5253 lambda_k: 1 Loss: 0.0029294082579138245\n",
      "Iteration: 5254 lambda_k: 1 Loss: 0.002929408257957863\n",
      "Iteration: 5255 lambda_k: 1 Loss: 0.0029294082580017034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5256 lambda_k: 1 Loss: 0.0029294082580453066\n",
      "Iteration: 5257 lambda_k: 1 Loss: 0.0029294082580887033\n",
      "Iteration: 5258 lambda_k: 1 Loss: 0.0029294082581318866\n",
      "Iteration: 5259 lambda_k: 1 Loss: 0.0029294082581748705\n",
      "Iteration: 5260 lambda_k: 1 Loss: 0.0029294082582176453\n",
      "Iteration: 5261 lambda_k: 1 Loss: 0.0029294082582602223\n",
      "Iteration: 5262 lambda_k: 1 Loss: 0.0029294082583026034\n",
      "Iteration: 5263 lambda_k: 1 Loss: 0.0029294082583447806\n",
      "Iteration: 5264 lambda_k: 1 Loss: 0.002929408258386746\n",
      "Iteration: 5265 lambda_k: 1 Loss: 0.0029294082584284988\n",
      "Iteration: 5266 lambda_k: 1 Loss: 0.002929408258470057\n",
      "Iteration: 5267 lambda_k: 1 Loss: 0.0029294082585114185\n",
      "Iteration: 5268 lambda_k: 1 Loss: 0.002929408258552572\n",
      "Iteration: 5269 lambda_k: 1 Loss: 0.0029294082585935256\n",
      "Iteration: 5270 lambda_k: 1 Loss: 0.0029294082586342925\n",
      "Iteration: 5271 lambda_k: 1 Loss: 0.0029294082586748633\n",
      "Iteration: 5272 lambda_k: 1 Loss: 0.002929408258715246\n",
      "Iteration: 5273 lambda_k: 1 Loss: 0.002929408258755432\n",
      "Iteration: 5274 lambda_k: 1 Loss: 0.00292940825879543\n",
      "Iteration: 5275 lambda_k: 1 Loss: 0.0029294082588352385\n",
      "Iteration: 5276 lambda_k: 1 Loss: 0.0029294082588748487\n",
      "Iteration: 5277 lambda_k: 1 Loss: 0.002929408258914262\n",
      "Iteration: 5278 lambda_k: 1 Loss: 0.0029294082589534907\n",
      "Iteration: 5279 lambda_k: 1 Loss: 0.002929408258992537\n",
      "Iteration: 5280 lambda_k: 1 Loss: 0.0029294082590313863\n",
      "Iteration: 5281 lambda_k: 1 Loss: 0.002929408259070064\n",
      "Iteration: 5282 lambda_k: 1 Loss: 0.0029294082591085416\n",
      "Iteration: 5283 lambda_k: 1 Loss: 0.0029294082591468395\n",
      "Iteration: 5284 lambda_k: 1 Loss: 0.002929408259184958\n",
      "Iteration: 5285 lambda_k: 1 Loss: 0.0029294082592229054\n",
      "Iteration: 5286 lambda_k: 1 Loss: 0.0029294082592606747\n",
      "Iteration: 5287 lambda_k: 1 Loss: 0.002929408259298243\n",
      "Iteration: 5288 lambda_k: 1 Loss: 0.002929408259335634\n",
      "Iteration: 5289 lambda_k: 1 Loss: 0.0029294082593728493\n",
      "Iteration: 5290 lambda_k: 1 Loss: 0.00292940825940988\n",
      "Iteration: 5291 lambda_k: 1 Loss: 0.002929408259446739\n",
      "Iteration: 5292 lambda_k: 1 Loss: 0.002929408259483409\n",
      "Iteration: 5293 lambda_k: 1 Loss: 0.0029294082595199083\n",
      "Iteration: 5294 lambda_k: 1 Loss: 0.002929408259556244\n",
      "Iteration: 5295 lambda_k: 1 Loss: 0.002929408259592411\n",
      "Iteration: 5296 lambda_k: 1 Loss: 0.002929408259628405\n",
      "Iteration: 5297 lambda_k: 1 Loss: 0.002929408259664214\n",
      "Iteration: 5298 lambda_k: 1 Loss: 0.002929408259699858\n",
      "Iteration: 5299 lambda_k: 1 Loss: 0.0029294082597353262\n",
      "Iteration: 5300 lambda_k: 1 Loss: 0.002929408259770631\n",
      "Iteration: 5301 lambda_k: 1 Loss: 0.0029294082598057495\n",
      "Iteration: 5302 lambda_k: 1 Loss: 0.00292940825984069\n",
      "Iteration: 5303 lambda_k: 1 Loss: 0.0029294082598754828\n",
      "Iteration: 5304 lambda_k: 1 Loss: 0.002929408259910101\n",
      "Iteration: 5305 lambda_k: 1 Loss: 0.002929408259944566\n",
      "Iteration: 5306 lambda_k: 1 Loss: 0.0029294082599788606\n",
      "Iteration: 5307 lambda_k: 1 Loss: 0.0029294082600129943\n",
      "Iteration: 5308 lambda_k: 1 Loss: 0.0029294082600469667\n",
      "Iteration: 5309 lambda_k: 1 Loss: 0.0029294082600807873\n",
      "Iteration: 5310 lambda_k: 1 Loss: 0.002929408260114452\n",
      "Iteration: 5311 lambda_k: 1 Loss: 0.0029294082601479554\n",
      "Iteration: 5312 lambda_k: 1 Loss: 0.0029294082601812703\n",
      "Iteration: 5313 lambda_k: 1 Loss: 0.002929408260214421\n",
      "Iteration: 5314 lambda_k: 1 Loss: 0.002929408260247421\n",
      "Iteration: 5315 lambda_k: 1 Loss: 0.002929408260280267\n",
      "Iteration: 5316 lambda_k: 1 Loss: 0.0029294082603129505\n",
      "Iteration: 5317 lambda_k: 1 Loss: 0.00292940826034548\n",
      "Iteration: 5318 lambda_k: 1 Loss: 0.002929408260377854\n",
      "Iteration: 5319 lambda_k: 1 Loss: 0.0029294082604100786\n",
      "Iteration: 5320 lambda_k: 1 Loss: 0.002929408260442149\n",
      "Iteration: 5321 lambda_k: 1 Loss: 0.0029294082604740634\n",
      "Iteration: 5322 lambda_k: 1 Loss: 0.0029294082605058305\n",
      "Iteration: 5323 lambda_k: 1 Loss: 0.00292940826053745\n",
      "Iteration: 5324 lambda_k: 1 Loss: 0.002929408260568891\n",
      "Iteration: 5325 lambda_k: 1 Loss: 0.0029294082606001947\n",
      "Iteration: 5326 lambda_k: 1 Loss: 0.002929408260631339\n",
      "Iteration: 5327 lambda_k: 1 Loss: 0.00292940826066234\n",
      "Iteration: 5328 lambda_k: 1 Loss: 0.0029294082606931907\n",
      "Iteration: 5329 lambda_k: 1 Loss: 0.002929408260723895\n",
      "Iteration: 5330 lambda_k: 1 Loss: 0.0029294082607544424\n",
      "Iteration: 5331 lambda_k: 1 Loss: 0.002929408260784858\n",
      "Iteration: 5332 lambda_k: 1 Loss: 0.0029294082608151218\n",
      "Iteration: 5333 lambda_k: 1 Loss: 0.002929408260845247\n",
      "Iteration: 5334 lambda_k: 1 Loss: 0.0029294082608752226\n",
      "Iteration: 5335 lambda_k: 1 Loss: 0.0029294082609050615\n",
      "Iteration: 5336 lambda_k: 1 Loss: 0.0029294082609347648\n",
      "Iteration: 5337 lambda_k: 1 Loss: 0.0029294082609643067\n",
      "Iteration: 5338 lambda_k: 1 Loss: 0.0029294082609937068\n",
      "Iteration: 5339 lambda_k: 1 Loss: 0.0029294082610229694\n",
      "Iteration: 5340 lambda_k: 1 Loss: 0.0029294082610520824\n",
      "Iteration: 5341 lambda_k: 1 Loss: 0.0029294082610810627\n",
      "Iteration: 5342 lambda_k: 1 Loss: 0.0029294082611098908\n",
      "Iteration: 5343 lambda_k: 1 Loss: 0.0029294082611385857\n",
      "Iteration: 5344 lambda_k: 1 Loss: 0.0029294082611671466\n",
      "Iteration: 5345 lambda_k: 1 Loss: 0.002929408261195583\n",
      "Iteration: 5346 lambda_k: 1 Loss: 0.002929408261223877\n",
      "Iteration: 5347 lambda_k: 1 Loss: 0.002929408261252028\n",
      "Iteration: 5348 lambda_k: 1 Loss: 0.0029294082612800515\n",
      "Iteration: 5349 lambda_k: 1 Loss: 0.0029294082613079476\n",
      "Iteration: 5350 lambda_k: 1 Loss: 0.0029294082613357123\n",
      "Iteration: 5351 lambda_k: 1 Loss: 0.002929408261363348\n",
      "Iteration: 5352 lambda_k: 1 Loss: 0.0029294082613908327\n",
      "Iteration: 5353 lambda_k: 1 Loss: 0.002929408261418185\n",
      "Iteration: 5354 lambda_k: 1 Loss: 0.002929408261445421\n",
      "Iteration: 5355 lambda_k: 1 Loss: 0.0029294082614725178\n",
      "Iteration: 5356 lambda_k: 1 Loss: 0.0029294082614994763\n",
      "Iteration: 5357 lambda_k: 1 Loss: 0.002929408261526312\n",
      "Iteration: 5358 lambda_k: 1 Loss: 0.002929408261553017\n",
      "Iteration: 5359 lambda_k: 1 Loss: 0.0029294082615795984\n",
      "Iteration: 5360 lambda_k: 1 Loss: 0.0029294082616060516\n",
      "Iteration: 5361 lambda_k: 1 Loss: 0.0029294082616323704\n",
      "Iteration: 5362 lambda_k: 1 Loss: 0.002929408261658579\n",
      "Iteration: 5363 lambda_k: 1 Loss: 0.0029294082616846606\n",
      "Iteration: 5364 lambda_k: 1 Loss: 0.0029294082617106086\n",
      "Iteration: 5365 lambda_k: 1 Loss: 0.0029294082617364377\n",
      "Iteration: 5366 lambda_k: 1 Loss: 0.0029294082617621533\n",
      "Iteration: 5367 lambda_k: 1 Loss: 0.002929408261787745\n",
      "Iteration: 5368 lambda_k: 1 Loss: 0.0029294082618132075\n",
      "Iteration: 5369 lambda_k: 1 Loss: 0.0029294082618385488\n",
      "Iteration: 5370 lambda_k: 1 Loss: 0.0029294082618637556\n",
      "Iteration: 5371 lambda_k: 1 Loss: 0.002929408261888852\n",
      "Iteration: 5372 lambda_k: 1 Loss: 0.0029294082619138132\n",
      "Iteration: 5373 lambda_k: 1 Loss: 0.002929408261938655\n",
      "Iteration: 5374 lambda_k: 1 Loss: 0.002929408261963373\n",
      "Iteration: 5375 lambda_k: 1 Loss: 0.0029294082619879813\n",
      "Iteration: 5376 lambda_k: 1 Loss: 0.002929408262012473\n",
      "Iteration: 5377 lambda_k: 1 Loss: 0.002929408262036853\n",
      "Iteration: 5378 lambda_k: 1 Loss: 0.002929408262061113\n",
      "Iteration: 5379 lambda_k: 1 Loss: 0.0029294082620852716\n",
      "Iteration: 5380 lambda_k: 1 Loss: 0.0029294082621093035\n",
      "Iteration: 5381 lambda_k: 1 Loss: 0.0029294082621332223\n",
      "Iteration: 5382 lambda_k: 1 Loss: 0.0029294082621570293\n",
      "Iteration: 5383 lambda_k: 1 Loss: 0.002929408262180728\n",
      "Iteration: 5384 lambda_k: 1 Loss: 0.002929408262204313\n",
      "Iteration: 5385 lambda_k: 1 Loss: 0.002929408262227767\n",
      "Iteration: 5386 lambda_k: 1 Loss: 0.0029294082622511384\n",
      "Iteration: 5387 lambda_k: 1 Loss: 0.0029294082622743954\n",
      "Iteration: 5388 lambda_k: 1 Loss: 0.0029294082622975115\n",
      "Iteration: 5389 lambda_k: 1 Loss: 0.0029294082623205165\n",
      "Iteration: 5390 lambda_k: 1 Loss: 0.0029294082623434153\n",
      "Iteration: 5391 lambda_k: 1 Loss: 0.002929408262366208\n",
      "Iteration: 5392 lambda_k: 1 Loss: 0.0029294082623888837\n",
      "Iteration: 5393 lambda_k: 1 Loss: 0.002929408262411453\n",
      "Iteration: 5394 lambda_k: 1 Loss: 0.0029294082624339245\n",
      "Iteration: 5395 lambda_k: 1 Loss: 0.0029294082624562777\n",
      "Iteration: 5396 lambda_k: 1 Loss: 0.0029294082624785247\n",
      "Iteration: 5397 lambda_k: 1 Loss: 0.0029294082625006836\n",
      "Iteration: 5398 lambda_k: 1 Loss: 0.002929408262522723\n",
      "Iteration: 5399 lambda_k: 1 Loss: 0.0029294082625446606\n",
      "Iteration: 5400 lambda_k: 1 Loss: 0.0029294082625665042\n",
      "Iteration: 5401 lambda_k: 1 Loss: 0.0029294082625882377\n",
      "Iteration: 5402 lambda_k: 1 Loss: 0.0029294082626098727\n",
      "Iteration: 5403 lambda_k: 1 Loss: 0.0029294082626314085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5404 lambda_k: 1 Loss: 0.0029294082626528176\n",
      "Iteration: 5405 lambda_k: 1 Loss: 0.002929408262674147\n",
      "Iteration: 5406 lambda_k: 1 Loss: 0.0029294082626953565\n",
      "Iteration: 5407 lambda_k: 1 Loss: 0.002929408262716458\n",
      "Iteration: 5408 lambda_k: 1 Loss: 0.002929408262737455\n",
      "Iteration: 5409 lambda_k: 1 Loss: 0.0029294082627583542\n",
      "Iteration: 5410 lambda_k: 1 Loss: 0.0029294082627791627\n",
      "Iteration: 5411 lambda_k: 1 Loss: 0.0029294082627998597\n",
      "Iteration: 5412 lambda_k: 1 Loss: 0.0029294082628204547\n",
      "Iteration: 5413 lambda_k: 1 Loss: 0.002929408262840952\n",
      "Iteration: 5414 lambda_k: 1 Loss: 0.00292940826286136\n",
      "Iteration: 5415 lambda_k: 1 Loss: 0.0029294082628816684\n",
      "Iteration: 5416 lambda_k: 1 Loss: 0.002929408262901877\n",
      "Iteration: 5417 lambda_k: 1 Loss: 0.002929408262921991\n",
      "Iteration: 5418 lambda_k: 1 Loss: 0.0029294082629420155\n",
      "Iteration: 5419 lambda_k: 1 Loss: 0.002929408262961945\n",
      "Iteration: 5420 lambda_k: 1 Loss: 0.002929408262981774\n",
      "Iteration: 5421 lambda_k: 1 Loss: 0.002929408263001514\n",
      "Iteration: 5422 lambda_k: 1 Loss: 0.0029294082630211627\n",
      "Iteration: 5423 lambda_k: 1 Loss: 0.002929408263040724\n",
      "Iteration: 5424 lambda_k: 1 Loss: 0.00292940826306019\n",
      "Iteration: 5425 lambda_k: 1 Loss: 0.002929408263079537\n",
      "Iteration: 5426 lambda_k: 1 Loss: 0.0029294082630987915\n",
      "Iteration: 5427 lambda_k: 1 Loss: 0.0029294082631179524\n",
      "Iteration: 5428 lambda_k: 1 Loss: 0.0029294082631370244\n",
      "Iteration: 5429 lambda_k: 1 Loss: 0.0029294082631560066\n",
      "Iteration: 5430 lambda_k: 1 Loss: 0.002929408263174903\n",
      "Iteration: 5431 lambda_k: 1 Loss: 0.0029294082631936983\n",
      "Iteration: 5432 lambda_k: 1 Loss: 0.0029294082632124025\n",
      "Iteration: 5433 lambda_k: 1 Loss: 0.0029294082632310117\n",
      "Iteration: 5434 lambda_k: 1 Loss: 0.002929408263249538\n",
      "Iteration: 5435 lambda_k: 1 Loss: 0.002929408263267981\n",
      "Iteration: 5436 lambda_k: 1 Loss: 0.0029294082632863364\n",
      "Iteration: 5437 lambda_k: 1 Loss: 0.0029294082633046065\n",
      "Iteration: 5438 lambda_k: 1 Loss: 0.0029294082633227842\n",
      "Iteration: 5439 lambda_k: 1 Loss: 0.00292940826334088\n",
      "Iteration: 5440 lambda_k: 1 Loss: 0.0029294082633589047\n",
      "Iteration: 5441 lambda_k: 1 Loss: 0.0029294082633768304\n",
      "Iteration: 5442 lambda_k: 1 Loss: 0.0029294082633946747\n",
      "Iteration: 5443 lambda_k: 1 Loss: 0.002929408263412433\n",
      "Iteration: 5444 lambda_k: 1 Loss: 0.0029294082634301146\n",
      "Iteration: 5445 lambda_k: 1 Loss: 0.0029294082634477134\n",
      "Iteration: 5446 lambda_k: 1 Loss: 0.002929408263465222\n",
      "Iteration: 5447 lambda_k: 1 Loss: 0.002929408263482625\n",
      "Iteration: 5448 lambda_k: 1 Loss: 0.002929408263499949\n",
      "Iteration: 5449 lambda_k: 1 Loss: 0.0029294082635172194\n",
      "Iteration: 5450 lambda_k: 1 Loss: 0.0029294082635343763\n",
      "Iteration: 5451 lambda_k: 1 Loss: 0.0029294082635514477\n",
      "Iteration: 5452 lambda_k: 1 Loss: 0.002929408263568443\n",
      "Iteration: 5453 lambda_k: 1 Loss: 0.002929408263585352\n",
      "Iteration: 5454 lambda_k: 1 Loss: 0.00292940826360219\n",
      "Iteration: 5455 lambda_k: 1 Loss: 0.0029294082636189475\n",
      "Iteration: 5456 lambda_k: 1 Loss: 0.002929408263635612\n",
      "Iteration: 5457 lambda_k: 1 Loss: 0.0029294082636521983\n",
      "Iteration: 5458 lambda_k: 1 Loss: 0.0029294082636687154\n",
      "Iteration: 5459 lambda_k: 1 Loss: 0.002929408263685149\n",
      "Iteration: 5460 lambda_k: 1 Loss: 0.002929408263701505\n",
      "Iteration: 5461 lambda_k: 1 Loss: 0.0029294082637177856\n",
      "Iteration: 5462 lambda_k: 1 Loss: 0.002929408263733988\n",
      "Iteration: 5463 lambda_k: 1 Loss: 0.0029294082637501156\n",
      "Iteration: 5464 lambda_k: 1 Loss: 0.0029294082637661657\n",
      "Iteration: 5465 lambda_k: 1 Loss: 0.0029294082637821477\n",
      "Iteration: 5466 lambda_k: 1 Loss: 0.0029294082637980365\n",
      "Iteration: 5467 lambda_k: 1 Loss: 0.0029294082638138615\n",
      "Iteration: 5468 lambda_k: 1 Loss: 0.0029294082638296206\n",
      "Iteration: 5469 lambda_k: 1 Loss: 0.002929408263845303\n",
      "Iteration: 5470 lambda_k: 1 Loss: 0.0029294082638609085\n",
      "Iteration: 5471 lambda_k: 1 Loss: 0.0029294082638764343\n",
      "Iteration: 5472 lambda_k: 1 Loss: 0.0029294082638918915\n",
      "Iteration: 5473 lambda_k: 1 Loss: 0.0029294082639072547\n",
      "Iteration: 5474 lambda_k: 1 Loss: 0.002929408263922566\n",
      "Iteration: 5475 lambda_k: 1 Loss: 0.002929408263937814\n",
      "Iteration: 5476 lambda_k: 1 Loss: 0.002929408263952966\n",
      "Iteration: 5477 lambda_k: 1 Loss: 0.0029294082639680333\n",
      "Iteration: 5478 lambda_k: 1 Loss: 0.0029294082639830326\n",
      "Iteration: 5479 lambda_k: 1 Loss: 0.0029294082639979625\n",
      "Iteration: 5480 lambda_k: 1 Loss: 0.0029294082640128134\n",
      "Iteration: 5481 lambda_k: 1 Loss: 0.0029294082640275955\n",
      "Iteration: 5482 lambda_k: 1 Loss: 0.0029294082640423068\n",
      "Iteration: 5483 lambda_k: 1 Loss: 0.0029294082640569435\n",
      "Iteration: 5484 lambda_k: 1 Loss: 0.0029294082640715104\n",
      "Iteration: 5485 lambda_k: 1 Loss: 0.002929408264086012\n",
      "Iteration: 5486 lambda_k: 1 Loss: 0.002929408264100455\n",
      "Iteration: 5487 lambda_k: 1 Loss: 0.0029294082641148204\n",
      "Iteration: 5488 lambda_k: 1 Loss: 0.0029294082641291154\n",
      "Iteration: 5489 lambda_k: 1 Loss: 0.0029294082641433527\n",
      "Iteration: 5490 lambda_k: 1 Loss: 0.0029294082641575223\n",
      "Iteration: 5491 lambda_k: 1 Loss: 0.0029294082641716265\n",
      "Iteration: 5492 lambda_k: 1 Loss: 0.0029294082641856544\n",
      "Iteration: 5493 lambda_k: 1 Loss: 0.002929408264199616\n",
      "Iteration: 5494 lambda_k: 1 Loss: 0.0029294082642135084\n",
      "Iteration: 5495 lambda_k: 1 Loss: 0.002929408264227342\n",
      "Iteration: 5496 lambda_k: 1 Loss: 0.0029294082642411143\n",
      "Iteration: 5497 lambda_k: 1 Loss: 0.0029294082642548234\n",
      "Iteration: 5498 lambda_k: 1 Loss: 0.0029294082642684657\n",
      "Iteration: 5499 lambda_k: 1 Loss: 0.0029294082642820395\n",
      "Iteration: 5500 lambda_k: 1 Loss: 0.0029294082642955612\n",
      "Iteration: 5501 lambda_k: 1 Loss: 0.0029294082643090153\n",
      "Iteration: 5502 lambda_k: 1 Loss: 0.0029294082643224117\n",
      "Iteration: 5503 lambda_k: 1 Loss: 0.0029294082643357396\n",
      "Iteration: 5504 lambda_k: 1 Loss: 0.0029294082643489803\n",
      "Iteration: 5505 lambda_k: 1 Loss: 0.0029294082643621486\n",
      "Iteration: 5506 lambda_k: 1 Loss: 0.002929408264375279\n",
      "Iteration: 5507 lambda_k: 1 Loss: 0.0029294082643883355\n",
      "Iteration: 5508 lambda_k: 1 Loss: 0.0029294082644013212\n",
      "Iteration: 5509 lambda_k: 1 Loss: 0.0029294082644142467\n",
      "Iteration: 5510 lambda_k: 1 Loss: 0.002929408264427119\n",
      "Iteration: 5511 lambda_k: 1 Loss: 0.002929408264439915\n",
      "Iteration: 5512 lambda_k: 1 Loss: 0.002929408264452655\n",
      "Iteration: 5513 lambda_k: 1 Loss: 0.0029294082644653273\n",
      "Iteration: 5514 lambda_k: 1 Loss: 0.0029294082644779427\n",
      "Iteration: 5515 lambda_k: 1 Loss: 0.0029294082644905025\n",
      "Iteration: 5516 lambda_k: 1 Loss: 0.0029294082645029917\n",
      "Iteration: 5517 lambda_k: 1 Loss: 0.0029294082645154253\n",
      "Iteration: 5518 lambda_k: 1 Loss: 0.002929408264527798\n",
      "Iteration: 5519 lambda_k: 1 Loss: 0.002929408264540111\n",
      "Iteration: 5520 lambda_k: 1 Loss: 0.0029294082645523745\n",
      "Iteration: 5521 lambda_k: 1 Loss: 0.0029294082645645713\n",
      "Iteration: 5522 lambda_k: 1 Loss: 0.0029294082645767217\n",
      "Iteration: 5523 lambda_k: 1 Loss: 0.002929408264588805\n",
      "Iteration: 5524 lambda_k: 1 Loss: 0.002929408264600832\n",
      "Iteration: 5525 lambda_k: 1 Loss: 0.0029294082646128105\n",
      "Iteration: 5526 lambda_k: 1 Loss: 0.0029294082646247367\n",
      "Iteration: 5527 lambda_k: 1 Loss: 0.0029294082646366053\n",
      "Iteration: 5528 lambda_k: 1 Loss: 0.0029294082646484226\n",
      "Iteration: 5529 lambda_k: 1 Loss: 0.002929408264660182\n",
      "Iteration: 5530 lambda_k: 1 Loss: 0.002929408264671874\n",
      "Iteration: 5531 lambda_k: 1 Loss: 0.0029294082646835248\n",
      "Iteration: 5532 lambda_k: 1 Loss: 0.0029294082646951214\n",
      "Iteration: 5533 lambda_k: 1 Loss: 0.0029294082647066556\n",
      "Iteration: 5534 lambda_k: 1 Loss: 0.0029294082647181403\n",
      "Iteration: 5535 lambda_k: 1 Loss: 0.0029294082647295734\n",
      "Iteration: 5536 lambda_k: 1 Loss: 0.002929408264740949\n",
      "Iteration: 5537 lambda_k: 1 Loss: 0.002929408264752271\n",
      "Iteration: 5538 lambda_k: 1 Loss: 0.002929408264763512\n",
      "Iteration: 5539 lambda_k: 1 Loss: 0.0029294082647746984\n",
      "Iteration: 5540 lambda_k: 1 Loss: 0.0029294082647858596\n",
      "Iteration: 5541 lambda_k: 1 Loss: 0.002929408264796975\n",
      "Iteration: 5542 lambda_k: 1 Loss: 0.0029294082648080064\n",
      "Iteration: 5543 lambda_k: 1 Loss: 0.002929408264818982\n",
      "Iteration: 5544 lambda_k: 1 Loss: 0.002929408264829914\n",
      "Iteration: 5545 lambda_k: 1 Loss: 0.0029294082648407926\n",
      "Iteration: 5546 lambda_k: 1 Loss: 0.002929408264851617\n",
      "Iteration: 5547 lambda_k: 1 Loss: 0.0029294082648623865\n",
      "Iteration: 5548 lambda_k: 1 Loss: 0.0029294082648730914\n",
      "Iteration: 5549 lambda_k: 1 Loss: 0.0029294082648837452\n",
      "Iteration: 5550 lambda_k: 1 Loss: 0.0029294082648943587\n",
      "Iteration: 5551 lambda_k: 1 Loss: 0.0029294082649049223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5552 lambda_k: 1 Loss: 0.0029294082649154295\n",
      "Iteration: 5553 lambda_k: 1 Loss: 0.002929408264925885\n",
      "Iteration: 5554 lambda_k: 1 Loss: 0.002929408264936299\n",
      "Iteration: 5555 lambda_k: 1 Loss: 0.002929408264946671\n",
      "Iteration: 5556 lambda_k: 1 Loss: 0.00292940826495698\n",
      "Iteration: 5557 lambda_k: 1 Loss: 0.0029294082649672384\n",
      "Iteration: 5558 lambda_k: 1 Loss: 0.0029294082649774455\n",
      "Iteration: 5559 lambda_k: 1 Loss: 0.002929408264987616\n",
      "Iteration: 5560 lambda_k: 1 Loss: 0.002929408264997745\n",
      "Iteration: 5561 lambda_k: 1 Loss: 0.0029294082650078135\n",
      "Iteration: 5562 lambda_k: 1 Loss: 0.002929408265017836\n",
      "Iteration: 5563 lambda_k: 1 Loss: 0.002929408265027817\n",
      "Iteration: 5564 lambda_k: 1 Loss: 0.0029294082650377397\n",
      "Iteration: 5565 lambda_k: 1 Loss: 0.002929408265047633\n",
      "Iteration: 5566 lambda_k: 1 Loss: 0.002929408265057474\n",
      "Iteration: 5567 lambda_k: 1 Loss: 0.002929408265067262\n",
      "Iteration: 5568 lambda_k: 1 Loss: 0.0029294082650770134\n",
      "Iteration: 5569 lambda_k: 1 Loss: 0.002929408265086722\n",
      "Iteration: 5570 lambda_k: 1 Loss: 0.002929408265096371\n",
      "Iteration: 5571 lambda_k: 1 Loss: 0.0029294082651059776\n",
      "Iteration: 5572 lambda_k: 1 Loss: 0.002929408265115552\n",
      "Iteration: 5573 lambda_k: 1 Loss: 0.0029294082651250774\n",
      "Iteration: 5574 lambda_k: 1 Loss: 0.00292940826513455\n",
      "Iteration: 5575 lambda_k: 1 Loss: 0.0029294082651439915\n",
      "Iteration: 5576 lambda_k: 1 Loss: 0.002929408265153385\n",
      "Iteration: 5577 lambda_k: 1 Loss: 0.002929408265162741\n",
      "Iteration: 5578 lambda_k: 1 Loss: 0.00292940826517204\n",
      "Iteration: 5579 lambda_k: 1 Loss: 0.002929408265181298\n",
      "Iteration: 5580 lambda_k: 1 Loss: 0.0029294082651904994\n",
      "Iteration: 5581 lambda_k: 1 Loss: 0.002929408265199644\n",
      "Iteration: 5582 lambda_k: 1 Loss: 0.0029294082652087617\n",
      "Iteration: 5583 lambda_k: 1 Loss: 0.0029294082652178304\n",
      "Iteration: 5584 lambda_k: 1 Loss: 0.00292940826522684\n",
      "Iteration: 5585 lambda_k: 1 Loss: 0.0029294082652358165\n",
      "Iteration: 5586 lambda_k: 1 Loss: 0.0029294082652447403\n",
      "Iteration: 5587 lambda_k: 1 Loss: 0.0029294082652536212\n",
      "Iteration: 5588 lambda_k: 1 Loss: 0.0029294082652624575\n",
      "Iteration: 5589 lambda_k: 1 Loss: 0.0029294082652712577\n",
      "Iteration: 5590 lambda_k: 1 Loss: 0.0029294082652800107\n",
      "Iteration: 5591 lambda_k: 1 Loss: 0.00292940826528872\n",
      "Iteration: 5592 lambda_k: 1 Loss: 0.0029294082652973844\n",
      "Iteration: 5593 lambda_k: 1 Loss: 0.002929408265306007\n",
      "Iteration: 5594 lambda_k: 1 Loss: 0.0029294082653146\n",
      "Iteration: 5595 lambda_k: 1 Loss: 0.002929408265323152\n",
      "Iteration: 5596 lambda_k: 1 Loss: 0.002929408265331651\n",
      "Iteration: 5597 lambda_k: 1 Loss: 0.002929408265340109\n",
      "Iteration: 5598 lambda_k: 1 Loss: 0.0029294082653485323\n",
      "Iteration: 5599 lambda_k: 1 Loss: 0.002929408265356924\n",
      "Iteration: 5600 lambda_k: 1 Loss: 0.002929408265365267\n",
      "Iteration: 5601 lambda_k: 1 Loss: 0.0029294082653735683\n",
      "Iteration: 5602 lambda_k: 1 Loss: 0.0029294082653818286\n",
      "Iteration: 5603 lambda_k: 1 Loss: 0.0029294082653900564\n",
      "Iteration: 5604 lambda_k: 1 Loss: 0.0029294082653982395\n",
      "Iteration: 5605 lambda_k: 1 Loss: 0.0029294082654063823\n",
      "Iteration: 5606 lambda_k: 1 Loss: 0.002929408265414485\n",
      "Iteration: 5607 lambda_k: 1 Loss: 0.0029294082654225586\n",
      "Iteration: 5608 lambda_k: 1 Loss: 0.002929408265430592\n",
      "Iteration: 5609 lambda_k: 1 Loss: 0.0029294082654385896\n",
      "Iteration: 5610 lambda_k: 1 Loss: 0.0029294082654465477\n",
      "Iteration: 5611 lambda_k: 1 Loss: 0.002929408265454472\n",
      "Iteration: 5612 lambda_k: 1 Loss: 0.002929408265462365\n",
      "Iteration: 5613 lambda_k: 1 Loss: 0.0029294082654702154\n",
      "Iteration: 5614 lambda_k: 1 Loss: 0.002929408265478028\n",
      "Iteration: 5615 lambda_k: 1 Loss: 0.0029294082654858144\n",
      "Iteration: 5616 lambda_k: 1 Loss: 0.002929408265493562\n",
      "Iteration: 5617 lambda_k: 1 Loss: 0.002929408265501277\n",
      "Iteration: 5618 lambda_k: 1 Loss: 0.0029294082655089457\n",
      "Iteration: 5619 lambda_k: 1 Loss: 0.002929408265516572\n",
      "Iteration: 5620 lambda_k: 1 Loss: 0.0029294082655241752\n",
      "Iteration: 5621 lambda_k: 1 Loss: 0.002929408265531732\n",
      "Iteration: 5622 lambda_k: 1 Loss: 0.002929408265539264\n",
      "Iteration: 5623 lambda_k: 1 Loss: 0.0029294082655467527\n",
      "Iteration: 5624 lambda_k: 1 Loss: 0.002929408265554208\n",
      "Iteration: 5625 lambda_k: 1 Loss: 0.0029294082655616344\n",
      "Iteration: 5626 lambda_k: 1 Loss: 0.002929408265569028\n",
      "Iteration: 5627 lambda_k: 1 Loss: 0.002929408265576383\n",
      "Iteration: 5628 lambda_k: 1 Loss: 0.002929408265583703\n",
      "Iteration: 5629 lambda_k: 1 Loss: 0.0029294082655909686\n",
      "Iteration: 5630 lambda_k: 1 Loss: 0.0029294082655981924\n",
      "Iteration: 5631 lambda_k: 1 Loss: 0.0029294082656053794\n",
      "Iteration: 5632 lambda_k: 1 Loss: 0.0029294082656125533\n",
      "Iteration: 5633 lambda_k: 1 Loss: 0.0029294082656196783\n",
      "Iteration: 5634 lambda_k: 1 Loss: 0.0029294082656267676\n",
      "Iteration: 5635 lambda_k: 1 Loss: 0.0029294082656338228\n",
      "Iteration: 5636 lambda_k: 1 Loss: 0.0029294082656408584\n",
      "Iteration: 5637 lambda_k: 1 Loss: 0.002929408265647837\n",
      "Iteration: 5638 lambda_k: 1 Loss: 0.002929408265654784\n",
      "Iteration: 5639 lambda_k: 1 Loss: 0.0029294082656616976\n",
      "Iteration: 5640 lambda_k: 1 Loss: 0.002929408265668584\n",
      "Iteration: 5641 lambda_k: 1 Loss: 0.002929408265675432\n",
      "Iteration: 5642 lambda_k: 1 Loss: 0.002929408265682243\n",
      "Iteration: 5643 lambda_k: 1 Loss: 0.002929408265689039\n",
      "Iteration: 5644 lambda_k: 1 Loss: 0.0029294082656957897\n",
      "Iteration: 5645 lambda_k: 1 Loss: 0.002929408265702512\n",
      "Iteration: 5646 lambda_k: 1 Loss: 0.0029294082657092\n",
      "Iteration: 5647 lambda_k: 1 Loss: 0.002929408265715849\n",
      "Iteration: 5648 lambda_k: 1 Loss: 0.0029294082657224684\n",
      "Iteration: 5649 lambda_k: 1 Loss: 0.0029294082657290543\n",
      "Iteration: 5650 lambda_k: 1 Loss: 0.0029294082657356055\n",
      "Iteration: 5651 lambda_k: 1 Loss: 0.002929408265742137\n",
      "Iteration: 5652 lambda_k: 1 Loss: 0.002929408265748639\n",
      "Iteration: 5653 lambda_k: 1 Loss: 0.002929408265755099\n",
      "Iteration: 5654 lambda_k: 1 Loss: 0.0029294082657615335\n",
      "Iteration: 5655 lambda_k: 1 Loss: 0.002929408265767939\n",
      "Iteration: 5656 lambda_k: 1 Loss: 0.002929408265774315\n",
      "Iteration: 5657 lambda_k: 1 Loss: 0.002929408265780656\n",
      "Iteration: 5658 lambda_k: 1 Loss: 0.0029294082657869668\n",
      "Iteration: 5659 lambda_k: 1 Loss: 0.0029294082657932465\n",
      "Iteration: 5660 lambda_k: 1 Loss: 0.002929408265799494\n",
      "Iteration: 5661 lambda_k: 1 Loss: 0.002929408265805713\n",
      "Iteration: 5662 lambda_k: 1 Loss: 0.0029294082658119095\n",
      "Iteration: 5663 lambda_k: 1 Loss: 0.002929408265818081\n",
      "Iteration: 5664 lambda_k: 1 Loss: 0.002929408265824225\n",
      "Iteration: 5665 lambda_k: 1 Loss: 0.002929408265830338\n",
      "Iteration: 5666 lambda_k: 1 Loss: 0.002929408265836413\n",
      "Iteration: 5667 lambda_k: 1 Loss: 0.0029294082658424645\n",
      "Iteration: 5668 lambda_k: 1 Loss: 0.002929408265848482\n",
      "Iteration: 5669 lambda_k: 1 Loss: 0.0029294082658544696\n",
      "Iteration: 5670 lambda_k: 1 Loss: 0.002929408265860439\n",
      "Iteration: 5671 lambda_k: 1 Loss: 0.0029294082658663733\n",
      "Iteration: 5672 lambda_k: 1 Loss: 0.0029294082658722865\n",
      "Iteration: 5673 lambda_k: 1 Loss: 0.0029294082658781763\n",
      "Iteration: 5674 lambda_k: 1 Loss: 0.0029294082658840245\n",
      "Iteration: 5675 lambda_k: 1 Loss: 0.002929408265889851\n",
      "Iteration: 5676 lambda_k: 1 Loss: 0.00292940826589565\n",
      "Iteration: 5677 lambda_k: 1 Loss: 0.0029294082659014294\n",
      "Iteration: 5678 lambda_k: 1 Loss: 0.0029294082659071874\n",
      "Iteration: 5679 lambda_k: 1 Loss: 0.0029294082659129064\n",
      "Iteration: 5680 lambda_k: 1 Loss: 0.002929408265918596\n",
      "Iteration: 5681 lambda_k: 1 Loss: 0.002929408265924267\n",
      "Iteration: 5682 lambda_k: 1 Loss: 0.0029294082659299153\n",
      "Iteration: 5683 lambda_k: 1 Loss: 0.0029294082659355354\n",
      "Iteration: 5684 lambda_k: 1 Loss: 0.002929408265941132\n",
      "Iteration: 5685 lambda_k: 1 Loss: 0.0029294082659467027\n",
      "Iteration: 5686 lambda_k: 1 Loss: 0.0029294082659522443\n",
      "Iteration: 5687 lambda_k: 1 Loss: 0.0029294082659577607\n",
      "Iteration: 5688 lambda_k: 1 Loss: 0.002929408265963243\n",
      "Iteration: 5689 lambda_k: 1 Loss: 0.0029294082659686994\n",
      "Iteration: 5690 lambda_k: 1 Loss: 0.002929408265974149\n",
      "Iteration: 5691 lambda_k: 1 Loss: 0.0029294082659795675\n",
      "Iteration: 5692 lambda_k: 1 Loss: 0.002929408265984963\n",
      "Iteration: 5693 lambda_k: 1 Loss: 0.0029294082659903223\n",
      "Iteration: 5694 lambda_k: 1 Loss: 0.002929408265995648\n",
      "Iteration: 5695 lambda_k: 1 Loss: 0.0029294082660009644\n",
      "Iteration: 5696 lambda_k: 1 Loss: 0.0029294082660062597\n",
      "Iteration: 5697 lambda_k: 1 Loss: 0.002929408266011505\n",
      "Iteration: 5698 lambda_k: 1 Loss: 0.002929408266016735\n",
      "Iteration: 5699 lambda_k: 1 Loss: 0.0029294082660219225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5700 lambda_k: 1 Loss: 0.0029294082660271084\n",
      "Iteration: 5701 lambda_k: 1 Loss: 0.0029294082660322436\n",
      "Iteration: 5702 lambda_k: 1 Loss: 0.002929408266037374\n",
      "Iteration: 5703 lambda_k: 1 Loss: 0.002929408266042453\n",
      "Iteration: 5704 lambda_k: 1 Loss: 0.002929408266047533\n",
      "Iteration: 5705 lambda_k: 1 Loss: 0.0029294082660525733\n",
      "Iteration: 5706 lambda_k: 1 Loss: 0.002929408266057586\n",
      "Iteration: 5707 lambda_k: 1 Loss: 0.002929408266062592\n",
      "Iteration: 5708 lambda_k: 1 Loss: 0.002929408266067566\n",
      "Iteration: 5709 lambda_k: 1 Loss: 0.0029294082660725044\n",
      "Iteration: 5710 lambda_k: 1 Loss: 0.0029294082660774245\n",
      "Iteration: 5711 lambda_k: 1 Loss: 0.002929408266082321\n",
      "Iteration: 5712 lambda_k: 1 Loss: 0.002929408266087216\n",
      "Iteration: 5713 lambda_k: 1 Loss: 0.0029294082660920673\n",
      "Iteration: 5714 lambda_k: 1 Loss: 0.002929408266096886\n",
      "Iteration: 5715 lambda_k: 1 Loss: 0.00292940826610168\n",
      "Iteration: 5716 lambda_k: 1 Loss: 0.0029294082661064738\n",
      "Iteration: 5717 lambda_k: 1 Loss: 0.002929408266111237\n",
      "Iteration: 5718 lambda_k: 1 Loss: 0.002929408266115973\n",
      "Iteration: 5719 lambda_k: 1 Loss: 0.0029294082661206803\n",
      "Iteration: 5720 lambda_k: 1 Loss: 0.002929408266125358\n",
      "Iteration: 5721 lambda_k: 1 Loss: 0.002929408266130014\n",
      "Iteration: 5722 lambda_k: 1 Loss: 0.002929408266134649\n",
      "Iteration: 5723 lambda_k: 1 Loss: 0.0029294082661392674\n",
      "Iteration: 5724 lambda_k: 1 Loss: 0.0029294082661438592\n",
      "Iteration: 5725 lambda_k: 1 Loss: 0.0029294082661484363\n",
      "Iteration: 5726 lambda_k: 1 Loss: 0.0029294082661529817\n",
      "Iteration: 5727 lambda_k: 1 Loss: 0.0029294082661575098\n",
      "Iteration: 5728 lambda_k: 1 Loss: 0.002929408266162022\n",
      "Iteration: 5729 lambda_k: 1 Loss: 0.0029294082661665056\n",
      "Iteration: 5730 lambda_k: 1 Loss: 0.002929408266170975\n",
      "Iteration: 5731 lambda_k: 1 Loss: 0.002929408266175425\n",
      "Iteration: 5732 lambda_k: 1 Loss: 0.002929408266179855\n",
      "Iteration: 5733 lambda_k: 1 Loss: 0.002929408266184251\n",
      "Iteration: 5734 lambda_k: 1 Loss: 0.0029294082661886203\n",
      "Iteration: 5735 lambda_k: 1 Loss: 0.002929408266192973\n",
      "Iteration: 5736 lambda_k: 1 Loss: 0.0029294082661973126\n",
      "Iteration: 5737 lambda_k: 1 Loss: 0.002929408266201628\n",
      "Iteration: 5738 lambda_k: 1 Loss: 0.002929408266205919\n",
      "Iteration: 5739 lambda_k: 1 Loss: 0.0029294082662101946\n",
      "Iteration: 5740 lambda_k: 1 Loss: 0.002929408266214455\n",
      "Iteration: 5741 lambda_k: 1 Loss: 0.002929408266218687\n",
      "Iteration: 5742 lambda_k: 1 Loss: 0.002929408266222899\n",
      "Iteration: 5743 lambda_k: 1 Loss: 0.0029294082662270913\n",
      "Iteration: 5744 lambda_k: 1 Loss: 0.00292940826623127\n",
      "Iteration: 5745 lambda_k: 1 Loss: 0.002929408266235437\n",
      "Iteration: 5746 lambda_k: 1 Loss: 0.00292940826623957\n",
      "Iteration: 5747 lambda_k: 1 Loss: 0.002929408266243697\n",
      "Iteration: 5748 lambda_k: 1 Loss: 0.002929408266247796\n",
      "Iteration: 5749 lambda_k: 1 Loss: 0.0029294082662518696\n",
      "Iteration: 5750 lambda_k: 1 Loss: 0.0029294082662559323\n",
      "Iteration: 5751 lambda_k: 1 Loss: 0.0029294082662599742\n",
      "Iteration: 5752 lambda_k: 1 Loss: 0.0029294082662639966\n",
      "Iteration: 5753 lambda_k: 1 Loss: 0.002929408266267997\n",
      "Iteration: 5754 lambda_k: 1 Loss: 0.0029294082662719738\n",
      "Iteration: 5755 lambda_k: 1 Loss: 0.00292940826627594\n",
      "Iteration: 5756 lambda_k: 1 Loss: 0.002929408266279897\n",
      "Iteration: 5757 lambda_k: 1 Loss: 0.002929408266283829\n",
      "Iteration: 5758 lambda_k: 1 Loss: 0.002929408266287742\n",
      "Iteration: 5759 lambda_k: 1 Loss: 0.002929408266291635\n",
      "Iteration: 5760 lambda_k: 1 Loss: 0.0029294082662955083\n",
      "Iteration: 5761 lambda_k: 1 Loss: 0.0029294082662993694\n",
      "Iteration: 5762 lambda_k: 1 Loss: 0.0029294082663032157\n",
      "Iteration: 5763 lambda_k: 1 Loss: 0.0029294082663070395\n",
      "Iteration: 5764 lambda_k: 1 Loss: 0.0029294082663108424\n",
      "Iteration: 5765 lambda_k: 1 Loss: 0.0029294082663146323\n",
      "Iteration: 5766 lambda_k: 1 Loss: 0.0029294082663184067\n",
      "Iteration: 5767 lambda_k: 1 Loss: 0.002929408266322156\n",
      "Iteration: 5768 lambda_k: 1 Loss: 0.0029294082663258916\n",
      "Iteration: 5769 lambda_k: 1 Loss: 0.002929408266329611\n",
      "Iteration: 5770 lambda_k: 1 Loss: 0.002929408266333319\n",
      "Iteration: 5771 lambda_k: 1 Loss: 0.0029294082663370038\n",
      "Iteration: 5772 lambda_k: 1 Loss: 0.002929408266340674\n",
      "Iteration: 5773 lambda_k: 1 Loss: 0.00292940826634432\n",
      "Iteration: 5774 lambda_k: 1 Loss: 0.0029294082663479538\n",
      "Iteration: 5775 lambda_k: 1 Loss: 0.0029294082663515702\n",
      "Iteration: 5776 lambda_k: 1 Loss: 0.0029294082663551715\n",
      "Iteration: 5777 lambda_k: 1 Loss: 0.0029294082663587607\n",
      "Iteration: 5778 lambda_k: 1 Loss: 0.0029294082663623264\n",
      "Iteration: 5779 lambda_k: 1 Loss: 0.0029294082663658787\n",
      "Iteration: 5780 lambda_k: 1 Loss: 0.002929408266369411\n",
      "Iteration: 5781 lambda_k: 1 Loss: 0.002929408266372925\n",
      "Iteration: 5782 lambda_k: 1 Loss: 0.0029294082663764284\n",
      "Iteration: 5783 lambda_k: 1 Loss: 0.0029294082663799087\n",
      "Iteration: 5784 lambda_k: 1 Loss: 0.002929408266383374\n",
      "Iteration: 5785 lambda_k: 1 Loss: 0.0029294082663868285\n",
      "Iteration: 5786 lambda_k: 1 Loss: 0.00292940826639026\n",
      "Iteration: 5787 lambda_k: 1 Loss: 0.0029294082663936837\n",
      "Iteration: 5788 lambda_k: 1 Loss: 0.002929408266397089\n",
      "Iteration: 5789 lambda_k: 1 Loss: 0.0029294082664004864\n",
      "Iteration: 5790 lambda_k: 1 Loss: 0.0029294082664038687\n",
      "Iteration: 5791 lambda_k: 1 Loss: 0.0029294082664072297\n",
      "Iteration: 5792 lambda_k: 1 Loss: 0.00292940826641057\n",
      "Iteration: 5793 lambda_k: 1 Loss: 0.002929408266413875\n",
      "Iteration: 5794 lambda_k: 1 Loss: 0.0029294082664171775\n",
      "Iteration: 5795 lambda_k: 1 Loss: 0.0029294082664204656\n",
      "Iteration: 5796 lambda_k: 1 Loss: 0.0029294082664237447\n",
      "Iteration: 5797 lambda_k: 1 Loss: 0.002929408266426993\n",
      "Iteration: 5798 lambda_k: 1 Loss: 0.002929408266430236\n",
      "Iteration: 5799 lambda_k: 1 Loss: 0.002929408266433454\n",
      "Iteration: 5800 lambda_k: 1 Loss: 0.002929408266436664\n",
      "Iteration: 5801 lambda_k: 1 Loss: 0.0029294082664398616\n",
      "Iteration: 5802 lambda_k: 1 Loss: 0.002929408266443021\n",
      "Iteration: 5803 lambda_k: 1 Loss: 0.0029294082664461716\n",
      "Iteration: 5804 lambda_k: 1 Loss: 0.002929408266449322\n",
      "Iteration: 5805 lambda_k: 1 Loss: 0.0029294082664524713\n",
      "Iteration: 5806 lambda_k: 1 Loss: 0.002929408266455581\n",
      "Iteration: 5807 lambda_k: 1 Loss: 0.0029294082664586612\n",
      "Iteration: 5808 lambda_k: 1 Loss: 0.0029294082664617477\n",
      "Iteration: 5809 lambda_k: 1 Loss: 0.0029294082664648113\n",
      "Iteration: 5810 lambda_k: 1 Loss: 0.002929408266467871\n",
      "Iteration: 5811 lambda_k: 1 Loss: 0.0029294082664708984\n",
      "Iteration: 5812 lambda_k: 1 Loss: 0.0029294082664739264\n",
      "Iteration: 5813 lambda_k: 1 Loss: 0.0029294082664769313\n",
      "Iteration: 5814 lambda_k: 1 Loss: 0.0029294082664799146\n",
      "Iteration: 5815 lambda_k: 1 Loss: 0.0029294082664828875\n",
      "Iteration: 5816 lambda_k: 1 Loss: 0.002929408266485856\n",
      "Iteration: 5817 lambda_k: 1 Loss: 0.0029294082664888107\n",
      "Iteration: 5818 lambda_k: 1 Loss: 0.002929408266491735\n",
      "Iteration: 5819 lambda_k: 1 Loss: 0.002929408266494657\n",
      "Iteration: 5820 lambda_k: 1 Loss: 0.0029294082664975503\n",
      "Iteration: 5821 lambda_k: 1 Loss: 0.00292940826650045\n",
      "Iteration: 5822 lambda_k: 1 Loss: 0.0029294082665033147\n",
      "Iteration: 5823 lambda_k: 1 Loss: 0.0029294082665061905\n",
      "Iteration: 5824 lambda_k: 1 Loss: 0.0029294082665090276\n",
      "Iteration: 5825 lambda_k: 1 Loss: 0.002929408266511851\n",
      "Iteration: 5826 lambda_k: 1 Loss: 0.002929408266514661\n",
      "Iteration: 5827 lambda_k: 1 Loss: 0.00292940826651748\n",
      "Iteration: 5828 lambda_k: 1 Loss: 0.0029294082665202665\n",
      "Iteration: 5829 lambda_k: 1 Loss: 0.0029294082665230494\n",
      "Iteration: 5830 lambda_k: 1 Loss: 0.0029294082665258085\n",
      "Iteration: 5831 lambda_k: 1 Loss: 0.002929408266528548\n",
      "Iteration: 5832 lambda_k: 1 Loss: 0.0029294082665313054\n",
      "Iteration: 5833 lambda_k: 1 Loss: 0.0029294082665340163\n",
      "Iteration: 5834 lambda_k: 1 Loss: 0.0029294082665367194\n",
      "Iteration: 5835 lambda_k: 1 Loss: 0.0029294082665394057\n",
      "Iteration: 5836 lambda_k: 1 Loss: 0.0029294082665420797\n",
      "Iteration: 5837 lambda_k: 1 Loss: 0.0029294082665447447\n",
      "Iteration: 5838 lambda_k: 1 Loss: 0.0029294082665474253\n",
      "Iteration: 5839 lambda_k: 1 Loss: 0.0029294082665500673\n",
      "Iteration: 5840 lambda_k: 1 Loss: 0.0029294082665527028\n",
      "Iteration: 5841 lambda_k: 1 Loss: 0.0029294082665553257\n",
      "Iteration: 5842 lambda_k: 1 Loss: 0.0029294082665579225\n",
      "Iteration: 5843 lambda_k: 1 Loss: 0.0029294082665605138\n",
      "Iteration: 5844 lambda_k: 1 Loss: 0.002929408266563117\n",
      "Iteration: 5845 lambda_k: 1 Loss: 0.002929408266565688\n",
      "Iteration: 5846 lambda_k: 1 Loss: 0.0029294082665682467\n",
      "Iteration: 5847 lambda_k: 1 Loss: 0.0029294082665707877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5848 lambda_k: 1 Loss: 0.002929408266573326\n",
      "Iteration: 5849 lambda_k: 1 Loss: 0.0029294082665758453\n",
      "Iteration: 5850 lambda_k: 1 Loss: 0.0029294082665783793\n",
      "Iteration: 5851 lambda_k: 1 Loss: 0.0029294082665808794\n",
      "Iteration: 5852 lambda_k: 1 Loss: 0.002929408266583362\n",
      "Iteration: 5853 lambda_k: 1 Loss: 0.0029294082665858373\n",
      "Iteration: 5854 lambda_k: 1 Loss: 0.0029294082665883257\n",
      "Iteration: 5855 lambda_k: 1 Loss: 0.002929408266590778\n",
      "Iteration: 5856 lambda_k: 1 Loss: 0.0029294082665932194\n",
      "Iteration: 5857 lambda_k: 1 Loss: 0.002929408266595637\n",
      "Iteration: 5858 lambda_k: 1 Loss: 0.00292940826659805\n",
      "Iteration: 5859 lambda_k: 1 Loss: 0.0029294082666004514\n",
      "Iteration: 5860 lambda_k: 1 Loss: 0.002929408266602848\n",
      "Iteration: 5861 lambda_k: 1 Loss: 0.002929408266605231\n",
      "Iteration: 5862 lambda_k: 1 Loss: 0.002929408266607597\n",
      "Iteration: 5863 lambda_k: 1 Loss: 0.0029294082666099586\n",
      "Iteration: 5864 lambda_k: 1 Loss: 0.002929408266612307\n",
      "Iteration: 5865 lambda_k: 1 Loss: 0.002929408266614639\n",
      "Iteration: 5866 lambda_k: 1 Loss: 0.002929408266616959\n",
      "Iteration: 5867 lambda_k: 1 Loss: 0.0029294082666192624\n",
      "Iteration: 5868 lambda_k: 1 Loss: 0.002929408266621564\n",
      "Iteration: 5869 lambda_k: 1 Loss: 0.0029294082666238516\n",
      "Iteration: 5870 lambda_k: 1 Loss: 0.0029294082666261284\n",
      "Iteration: 5871 lambda_k: 1 Loss: 0.0029294082666284017\n",
      "Iteration: 5872 lambda_k: 1 Loss: 0.002929408266630655\n",
      "Iteration: 5873 lambda_k: 1 Loss: 0.0029294082666328964\n",
      "Iteration: 5874 lambda_k: 1 Loss: 0.002929408266635128\n",
      "Iteration: 5875 lambda_k: 1 Loss: 0.002929408266637347\n",
      "Iteration: 5876 lambda_k: 1 Loss: 0.00292940826663956\n",
      "Iteration: 5877 lambda_k: 1 Loss: 0.002929408266641769\n",
      "Iteration: 5878 lambda_k: 1 Loss: 0.0029294082666439587\n",
      "Iteration: 5879 lambda_k: 1 Loss: 0.002929408266646139\n",
      "Iteration: 5880 lambda_k: 1 Loss: 0.002929408266648305\n",
      "Iteration: 5881 lambda_k: 1 Loss: 0.0029294082666504622\n",
      "Iteration: 5882 lambda_k: 1 Loss: 0.0029294082666526207\n",
      "Iteration: 5883 lambda_k: 1 Loss: 0.002929408266654765\n",
      "Iteration: 5884 lambda_k: 1 Loss: 0.002929408266656892\n",
      "Iteration: 5885 lambda_k: 1 Loss: 0.0029294082666590235\n",
      "Iteration: 5886 lambda_k: 1 Loss: 0.002929408266661128\n",
      "Iteration: 5887 lambda_k: 1 Loss: 0.0029294082666632354\n",
      "Iteration: 5888 lambda_k: 1 Loss: 0.002929408266665325\n",
      "Iteration: 5889 lambda_k: 1 Loss: 0.0029294082666674083\n",
      "Iteration: 5890 lambda_k: 1 Loss: 0.0029294082666694826\n",
      "Iteration: 5891 lambda_k: 1 Loss: 0.0029294082666715448\n",
      "Iteration: 5892 lambda_k: 1 Loss: 0.0029294082666735943\n",
      "Iteration: 5893 lambda_k: 1 Loss: 0.0029294082666756313\n",
      "Iteration: 5894 lambda_k: 1 Loss: 0.0029294082666776636\n",
      "Iteration: 5895 lambda_k: 1 Loss: 0.002929408266679698\n",
      "Iteration: 5896 lambda_k: 1 Loss: 0.0029294082666817102\n",
      "Iteration: 5897 lambda_k: 1 Loss: 0.002929408266683716\n",
      "Iteration: 5898 lambda_k: 1 Loss: 0.0029294082666857166\n",
      "Iteration: 5899 lambda_k: 1 Loss: 0.002929408266687714\n",
      "Iteration: 5900 lambda_k: 1 Loss: 0.0029294082666896856\n",
      "Iteration: 5901 lambda_k: 1 Loss: 0.002929408266691649\n",
      "Iteration: 5902 lambda_k: 1 Loss: 0.0029294082666936195\n",
      "Iteration: 5903 lambda_k: 1 Loss: 0.0029294082666955824\n",
      "Iteration: 5904 lambda_k: 1 Loss: 0.002929408266697528\n",
      "Iteration: 5905 lambda_k: 1 Loss: 0.0029294082666994625\n",
      "Iteration: 5906 lambda_k: 1 Loss: 0.0029294082667013985\n",
      "Iteration: 5907 lambda_k: 1 Loss: 0.002929408266703314\n",
      "Iteration: 5908 lambda_k: 1 Loss: 0.0029294082667052153\n",
      "Iteration: 5909 lambda_k: 1 Loss: 0.0029294082667071114\n",
      "Iteration: 5910 lambda_k: 1 Loss: 0.0029294082667089935\n",
      "Iteration: 5911 lambda_k: 1 Loss: 0.002929408266710871\n",
      "Iteration: 5912 lambda_k: 1 Loss: 0.0029294082667127436\n",
      "Iteration: 5913 lambda_k: 1 Loss: 0.0029294082667146075\n",
      "Iteration: 5914 lambda_k: 1 Loss: 0.002929408266716459\n",
      "Iteration: 5915 lambda_k: 1 Loss: 0.002929408266718306\n",
      "Iteration: 5916 lambda_k: 1 Loss: 0.0029294082667201448\n",
      "Iteration: 5917 lambda_k: 1 Loss: 0.0029294082667219823\n",
      "Iteration: 5918 lambda_k: 1 Loss: 0.0029294082667238007\n",
      "Iteration: 5919 lambda_k: 1 Loss: 0.002929408266725611\n",
      "Iteration: 5920 lambda_k: 1 Loss: 0.0029294082667274107\n",
      "Iteration: 5921 lambda_k: 1 Loss: 0.0029294082667292087\n",
      "Iteration: 5922 lambda_k: 1 Loss: 0.0029294082667309955\n",
      "Iteration: 5923 lambda_k: 1 Loss: 0.0029294082667327727\n",
      "Iteration: 5924 lambda_k: 1 Loss: 0.0029294082667345443\n",
      "Iteration: 5925 lambda_k: 1 Loss: 0.0029294082667363076\n",
      "Iteration: 5926 lambda_k: 1 Loss: 0.002929408266738057\n",
      "Iteration: 5927 lambda_k: 1 Loss: 0.002929408266739807\n",
      "Iteration: 5928 lambda_k: 1 Loss: 0.0029294082667415443\n",
      "Iteration: 5929 lambda_k: 1 Loss: 0.002929408266743282\n",
      "Iteration: 5930 lambda_k: 1 Loss: 0.002929408266745017\n",
      "Iteration: 5931 lambda_k: 1 Loss: 0.00292940826674673\n",
      "Iteration: 5932 lambda_k: 1 Loss: 0.0029294082667484355\n",
      "Iteration: 5933 lambda_k: 1 Loss: 0.002929408266750134\n",
      "Iteration: 5934 lambda_k: 1 Loss: 0.002929408266751824\n",
      "Iteration: 5935 lambda_k: 1 Loss: 0.0029294082667535057\n",
      "Iteration: 5936 lambda_k: 1 Loss: 0.0029294082667551857\n",
      "Iteration: 5937 lambda_k: 1 Loss: 0.0029294082667568493\n",
      "Iteration: 5938 lambda_k: 1 Loss: 0.0029294082667585047\n",
      "Iteration: 5939 lambda_k: 1 Loss: 0.002929408266760154\n",
      "Iteration: 5940 lambda_k: 1 Loss: 0.0029294082667617924\n",
      "Iteration: 5941 lambda_k: 1 Loss: 0.002929408266763438\n",
      "Iteration: 5942 lambda_k: 1 Loss: 0.002929408266765076\n",
      "Iteration: 5943 lambda_k: 1 Loss: 0.0029294082667667013\n",
      "Iteration: 5944 lambda_k: 1 Loss: 0.0029294082667683176\n",
      "Iteration: 5945 lambda_k: 1 Loss: 0.0029294082667699235\n",
      "Iteration: 5946 lambda_k: 1 Loss: 0.0029294082667715346\n",
      "Iteration: 5947 lambda_k: 1 Loss: 0.0029294082667731232\n",
      "Iteration: 5948 lambda_k: 1 Loss: 0.002929408266774707\n",
      "Iteration: 5949 lambda_k: 1 Loss: 0.002929408266776281\n",
      "Iteration: 5950 lambda_k: 1 Loss: 0.0029294082667778534\n",
      "Iteration: 5951 lambda_k: 1 Loss: 0.0029294082667794177\n",
      "Iteration: 5952 lambda_k: 1 Loss: 0.0029294082667809663\n",
      "Iteration: 5953 lambda_k: 1 Loss: 0.002929408266782512\n",
      "Iteration: 5954 lambda_k: 1 Loss: 0.0029294082667840615\n",
      "Iteration: 5955 lambda_k: 1 Loss: 0.0029294082667855946\n",
      "Iteration: 5956 lambda_k: 1 Loss: 0.0029294082667871133\n",
      "Iteration: 5957 lambda_k: 1 Loss: 0.0029294082667886447\n",
      "Iteration: 5958 lambda_k: 1 Loss: 0.0029294082667901617\n",
      "Iteration: 5959 lambda_k: 1 Loss: 0.0029294082667916774\n",
      "Iteration: 5960 lambda_k: 1 Loss: 0.0029294082667931883\n",
      "Iteration: 5961 lambda_k: 1 Loss: 0.0029294082667946906\n",
      "Iteration: 5962 lambda_k: 1 Loss: 0.0029294082667961747\n",
      "Iteration: 5963 lambda_k: 1 Loss: 0.002929408266797657\n",
      "Iteration: 5964 lambda_k: 1 Loss: 0.00292940826679913\n",
      "Iteration: 5965 lambda_k: 1 Loss: 0.002929408266800594\n",
      "Iteration: 5966 lambda_k: 1 Loss: 0.002929408266802053\n",
      "Iteration: 5967 lambda_k: 1 Loss: 0.0029294082668035056\n",
      "Iteration: 5968 lambda_k: 1 Loss: 0.0029294082668049324\n",
      "Iteration: 5969 lambda_k: 1 Loss: 0.002929408266806376\n",
      "Iteration: 5970 lambda_k: 1 Loss: 0.002929408266807816\n",
      "Iteration: 5971 lambda_k: 1 Loss: 0.0029294082668092393\n",
      "Iteration: 5972 lambda_k: 1 Loss: 0.0029294082668106553\n",
      "Iteration: 5973 lambda_k: 1 Loss: 0.0029294082668120495\n",
      "Iteration: 5974 lambda_k: 1 Loss: 0.0029294082668134443\n",
      "Iteration: 5975 lambda_k: 1 Loss: 0.002929408266814836\n",
      "Iteration: 5976 lambda_k: 1 Loss: 0.0029294082668162224\n",
      "Iteration: 5977 lambda_k: 1 Loss: 0.002929408266817577\n",
      "Iteration: 5978 lambda_k: 1 Loss: 0.0029294082668189572\n",
      "Iteration: 5979 lambda_k: 1 Loss: 0.0029294082668203146\n",
      "Iteration: 5980 lambda_k: 1 Loss: 0.0029294082668216595\n",
      "Iteration: 5981 lambda_k: 1 Loss: 0.0029294082668230113\n",
      "Iteration: 5982 lambda_k: 1 Loss: 0.0029294082668243626\n",
      "Iteration: 5983 lambda_k: 1 Loss: 0.002929408266825706\n",
      "Iteration: 5984 lambda_k: 1 Loss: 0.0029294082668270406\n",
      "Iteration: 5985 lambda_k: 1 Loss: 0.0029294082668283434\n",
      "Iteration: 5986 lambda_k: 1 Loss: 0.0029294082668296665\n",
      "Iteration: 5987 lambda_k: 1 Loss: 0.002929408266830981\n",
      "Iteration: 5988 lambda_k: 1 Loss: 0.002929408266832265\n",
      "Iteration: 5989 lambda_k: 1 Loss: 0.0029294082668335796\n",
      "Iteration: 5990 lambda_k: 1 Loss: 0.002929408266834852\n",
      "Iteration: 5991 lambda_k: 1 Loss: 0.0029294082668361353\n",
      "Iteration: 5992 lambda_k: 1 Loss: 0.0029294082668374142\n",
      "Iteration: 5993 lambda_k: 1 Loss: 0.0029294082668386667\n",
      "Iteration: 5994 lambda_k: 1 Loss: 0.0029294082668399352\n",
      "Iteration: 5995 lambda_k: 1 Loss: 0.002929408266841172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5996 lambda_k: 1 Loss: 0.0029294082668424276\n",
      "Iteration: 5997 lambda_k: 1 Loss: 0.0029294082668436536\n",
      "Iteration: 5998 lambda_k: 1 Loss: 0.0029294082668448944\n",
      "Iteration: 5999 lambda_k: 1 Loss: 0.002929408266846135\n",
      "Iteration: 6000 lambda_k: 1 Loss: 0.002929408266847348\n",
      "Iteration: 6001 lambda_k: 1 Loss: 0.002929408266848577\n",
      "Iteration: 6002 lambda_k: 1 Loss: 0.0029294082668497763\n",
      "Iteration: 6003 lambda_k: 1 Loss: 0.0029294082668509733\n",
      "Iteration: 6004 lambda_k: 1 Loss: 0.0029294082668521832\n",
      "Iteration: 6005 lambda_k: 1 Loss: 0.0029294082668533676\n",
      "Iteration: 6006 lambda_k: 1 Loss: 0.002929408266854567\n",
      "Iteration: 6007 lambda_k: 1 Loss: 0.002929408266855759\n",
      "Iteration: 6008 lambda_k: 1 Loss: 0.0029294082668569216\n",
      "Iteration: 6009 lambda_k: 1 Loss: 0.002929408266858099\n",
      "Iteration: 6010 lambda_k: 1 Loss: 0.0029294082668592744\n",
      "Iteration: 6011 lambda_k: 1 Loss: 0.002929408266860421\n",
      "Iteration: 6012 lambda_k: 1 Loss: 0.0029294082668615707\n",
      "Iteration: 6013 lambda_k: 1 Loss: 0.002929408266862732\n",
      "Iteration: 6014 lambda_k: 1 Loss: 0.002929408266863888\n",
      "Iteration: 6015 lambda_k: 1 Loss: 0.002929408266865013\n",
      "Iteration: 6016 lambda_k: 1 Loss: 0.002929408266866154\n",
      "Iteration: 6017 lambda_k: 1 Loss: 0.0029294082668672758\n",
      "Iteration: 6018 lambda_k: 1 Loss: 0.0029294082668684025\n",
      "Iteration: 6019 lambda_k: 1 Loss: 0.0029294082668695092\n",
      "Iteration: 6020 lambda_k: 1 Loss: 0.00292940826687064\n",
      "Iteration: 6021 lambda_k: 1 Loss: 0.0029294082668717297\n",
      "Iteration: 6022 lambda_k: 1 Loss: 0.0029294082668728377\n",
      "Iteration: 6023 lambda_k: 1 Loss: 0.0029294082668739245\n",
      "Iteration: 6024 lambda_k: 1 Loss: 0.002929408266875006\n",
      "Iteration: 6025 lambda_k: 1 Loss: 0.002929408266876098\n",
      "Iteration: 6026 lambda_k: 1 Loss: 0.0029294082668771823\n",
      "Iteration: 6027 lambda_k: 1 Loss: 0.002929408266878245\n",
      "Iteration: 6028 lambda_k: 1 Loss: 0.002929408266879308\n",
      "Iteration: 6029 lambda_k: 1 Loss: 0.00292940826688038\n",
      "Iteration: 6030 lambda_k: 1 Loss: 0.0029294082668814476\n",
      "Iteration: 6031 lambda_k: 1 Loss: 0.002929408266882498\n",
      "Iteration: 6032 lambda_k: 1 Loss: 0.002929408266883535\n",
      "Iteration: 6033 lambda_k: 1 Loss: 0.002929408266884568\n",
      "Iteration: 6034 lambda_k: 1 Loss: 0.002929408266885622\n",
      "Iteration: 6035 lambda_k: 1 Loss: 0.0029294082668866726\n",
      "Iteration: 6036 lambda_k: 1 Loss: 0.002929408266887688\n",
      "Iteration: 6037 lambda_k: 1 Loss: 0.0029294082668886987\n",
      "Iteration: 6038 lambda_k: 1 Loss: 0.0029294082668897266\n",
      "Iteration: 6039 lambda_k: 1 Loss: 0.002929408266890743\n",
      "Iteration: 6040 lambda_k: 1 Loss: 0.0029294082668917397\n",
      "Iteration: 6041 lambda_k: 1 Loss: 0.0029294082668927367\n",
      "Iteration: 6042 lambda_k: 1 Loss: 0.002929408266893745\n",
      "Iteration: 6043 lambda_k: 1 Loss: 0.00292940826689473\n",
      "Iteration: 6044 lambda_k: 1 Loss: 0.0029294082668957127\n",
      "Iteration: 6045 lambda_k: 1 Loss: 0.002929408266896706\n",
      "Iteration: 6046 lambda_k: 1 Loss: 0.0029294082668976725\n",
      "Iteration: 6047 lambda_k: 1 Loss: 0.002929408266898653\n",
      "Iteration: 6048 lambda_k: 1 Loss: 0.002929408266899617\n",
      "Iteration: 6049 lambda_k: 1 Loss: 0.0029294082669005916\n",
      "Iteration: 6050 lambda_k: 1 Loss: 0.0029294082669015474\n",
      "Iteration: 6051 lambda_k: 1 Loss: 0.002929408266902495\n",
      "Iteration: 6052 lambda_k: 1 Loss: 0.002929408266903437\n",
      "Iteration: 6053 lambda_k: 1 Loss: 0.0029294082669044054\n",
      "Iteration: 6054 lambda_k: 1 Loss: 0.002929408266905365\n",
      "Iteration: 6055 lambda_k: 1 Loss: 0.0029294082669062958\n",
      "Iteration: 6056 lambda_k: 1 Loss: 0.0029294082669072143\n",
      "Iteration: 6057 lambda_k: 1 Loss: 0.002929408266908162\n",
      "Iteration: 6058 lambda_k: 1 Loss: 0.0029294082669090935\n",
      "Iteration: 6059 lambda_k: 1 Loss: 0.002929408266910013\n",
      "Iteration: 6060 lambda_k: 1 Loss: 0.002929408266910915\n",
      "Iteration: 6061 lambda_k: 1 Loss: 0.0029294082669118126\n",
      "Iteration: 6062 lambda_k: 1 Loss: 0.0029294082669127346\n",
      "Iteration: 6063 lambda_k: 1 Loss: 0.0029294082669136324\n",
      "Iteration: 6064 lambda_k: 1 Loss: 0.0029294082669145427\n",
      "Iteration: 6065 lambda_k: 1 Loss: 0.0029294082669154287\n",
      "Iteration: 6066 lambda_k: 1 Loss: 0.0029294082669163385\n",
      "Iteration: 6067 lambda_k: 1 Loss: 0.0029294082669172263\n",
      "Iteration: 6068 lambda_k: 1 Loss: 0.0029294082669181\n",
      "Iteration: 6069 lambda_k: 1 Loss: 0.00292940826691899\n",
      "Iteration: 6070 lambda_k: 1 Loss: 0.002929408266919857\n",
      "Iteration: 6071 lambda_k: 1 Loss: 0.002929408266920717\n",
      "Iteration: 6072 lambda_k: 1 Loss: 0.0029294082669215874\n",
      "Iteration: 6073 lambda_k: 1 Loss: 0.002929408266922457\n",
      "Iteration: 6074 lambda_k: 1 Loss: 0.0029294082669233047\n",
      "Iteration: 6075 lambda_k: 1 Loss: 0.0029294082669241604\n",
      "Iteration: 6076 lambda_k: 1 Loss: 0.0029294082669249983\n",
      "Iteration: 6077 lambda_k: 1 Loss: 0.002929408266925856\n",
      "Iteration: 6078 lambda_k: 1 Loss: 0.0029294082669266927\n",
      "Iteration: 6079 lambda_k: 1 Loss: 0.0029294082669275388\n",
      "Iteration: 6080 lambda_k: 1 Loss: 0.002929408266928359\n",
      "Iteration: 6081 lambda_k: 1 Loss: 0.002929408266929177\n",
      "Iteration: 6082 lambda_k: 1 Loss: 0.0029294082669299977\n",
      "Iteration: 6083 lambda_k: 1 Loss: 0.0029294082669308356\n",
      "Iteration: 6084 lambda_k: 1 Loss: 0.0029294082669316613\n",
      "Iteration: 6085 lambda_k: 1 Loss: 0.00292940826693247\n",
      "Iteration: 6086 lambda_k: 1 Loss: 0.002929408266933277\n",
      "Iteration: 6087 lambda_k: 1 Loss: 0.0029294082669340696\n",
      "Iteration: 6088 lambda_k: 1 Loss: 0.002929408266934887\n",
      "Iteration: 6089 lambda_k: 1 Loss: 0.0029294082669356803\n",
      "Iteration: 6090 lambda_k: 1 Loss: 0.00292940826693646\n",
      "Iteration: 6091 lambda_k: 1 Loss: 0.0029294082669372554\n",
      "Iteration: 6092 lambda_k: 1 Loss: 0.00292940826693803\n",
      "Iteration: 6093 lambda_k: 1 Loss: 0.0029294082669388223\n",
      "Iteration: 6094 lambda_k: 1 Loss: 0.002929408266939597\n",
      "Iteration: 6095 lambda_k: 1 Loss: 0.0029294082669403614\n",
      "Iteration: 6096 lambda_k: 1 Loss: 0.002929408266941152\n",
      "Iteration: 6097 lambda_k: 1 Loss: 0.00292940826694192\n",
      "Iteration: 6098 lambda_k: 1 Loss: 0.00292940826694267\n",
      "Iteration: 6099 lambda_k: 1 Loss: 0.002929408266943426\n",
      "Iteration: 6100 lambda_k: 1 Loss: 0.0029294082669441943\n",
      "Iteration: 6101 lambda_k: 1 Loss: 0.0029294082669449337\n",
      "Iteration: 6102 lambda_k: 1 Loss: 0.002929408266945678\n",
      "Iteration: 6103 lambda_k: 1 Loss: 0.002929408266946437\n",
      "Iteration: 6104 lambda_k: 1 Loss: 0.0029294082669471706\n",
      "Iteration: 6105 lambda_k: 1 Loss: 0.002929408266947903\n",
      "Iteration: 6106 lambda_k: 1 Loss: 0.0029294082669486304\n",
      "Iteration: 6107 lambda_k: 1 Loss: 0.0029294082669493746\n",
      "Iteration: 6108 lambda_k: 1 Loss: 0.0029294082669501097\n",
      "Iteration: 6109 lambda_k: 1 Loss: 0.0029294082669508287\n",
      "Iteration: 6110 lambda_k: 1 Loss: 0.002929408266951545\n",
      "Iteration: 6111 lambda_k: 1 Loss: 0.002929408266952256\n",
      "Iteration: 6112 lambda_k: 1 Loss: 0.0029294082669529676\n",
      "Iteration: 6113 lambda_k: 1 Loss: 0.002929408266953685\n",
      "Iteration: 6114 lambda_k: 1 Loss: 0.002929408266954381\n",
      "Iteration: 6115 lambda_k: 1 Loss: 0.002929408266955076\n",
      "Iteration: 6116 lambda_k: 1 Loss: 0.002929408266955789\n",
      "Iteration: 6117 lambda_k: 1 Loss: 0.002929408266956495\n",
      "Iteration: 6118 lambda_k: 1 Loss: 0.002929408266957184\n",
      "Iteration: 6119 lambda_k: 1 Loss: 0.002929408266957867\n",
      "Iteration: 6120 lambda_k: 1 Loss: 0.00292940826695855\n",
      "Iteration: 6121 lambda_k: 1 Loss: 0.0029294082669592426\n",
      "Iteration: 6122 lambda_k: 1 Loss: 0.0029294082669599217\n",
      "Iteration: 6123 lambda_k: 1 Loss: 0.002929408266960593\n",
      "Iteration: 6124 lambda_k: 1 Loss: 0.002929408266961257\n",
      "Iteration: 6125 lambda_k: 1 Loss: 0.0029294082669619557\n",
      "Iteration: 6126 lambda_k: 1 Loss: 0.00292940826696262\n",
      "Iteration: 6127 lambda_k: 1 Loss: 0.002929408266963277\n",
      "Iteration: 6128 lambda_k: 1 Loss: 0.0029294082669639307\n",
      "Iteration: 6129 lambda_k: 1 Loss: 0.002929408266964607\n",
      "Iteration: 6130 lambda_k: 1 Loss: 0.002929408266965253\n",
      "Iteration: 6131 lambda_k: 1 Loss: 0.0029294082669658987\n",
      "Iteration: 6132 lambda_k: 1 Loss: 0.002929408266966549\n",
      "Iteration: 6133 lambda_k: 1 Loss: 0.0029294082669672145\n",
      "Iteration: 6134 lambda_k: 1 Loss: 0.0029294082669678654\n",
      "Iteration: 6135 lambda_k: 1 Loss: 0.0029294082669684912\n",
      "Iteration: 6136 lambda_k: 1 Loss: 0.0029294082669691227\n",
      "Iteration: 6137 lambda_k: 1 Loss: 0.002929408266969747\n",
      "Iteration: 6138 lambda_k: 1 Loss: 0.0029294082669703713\n",
      "Iteration: 6139 lambda_k: 1 Loss: 0.0029294082669710187\n",
      "Iteration: 6140 lambda_k: 1 Loss: 0.0029294082669716354\n",
      "Iteration: 6141 lambda_k: 1 Loss: 0.0029294082669722612\n",
      "Iteration: 6142 lambda_k: 1 Loss: 0.002929408266972892\n",
      "Iteration: 6143 lambda_k: 1 Loss: 0.0029294082669734946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6144 lambda_k: 1 Loss: 0.0029294082669741052\n",
      "Iteration: 6145 lambda_k: 1 Loss: 0.00292940826697471\n",
      "Iteration: 6146 lambda_k: 1 Loss: 0.002929408266975313\n",
      "Iteration: 6147 lambda_k: 1 Loss: 0.002929408266975936\n",
      "Iteration: 6148 lambda_k: 1 Loss: 0.0029294082669765282\n",
      "Iteration: 6149 lambda_k: 1 Loss: 0.002929408266977127\n",
      "Iteration: 6150 lambda_k: 1 Loss: 0.0029294082669777447\n",
      "Iteration: 6151 lambda_k: 1 Loss: 0.00292940826697833\n",
      "Iteration: 6152 lambda_k: 1 Loss: 0.0029294082669789312\n",
      "Iteration: 6153 lambda_k: 1 Loss: 0.0029294082669795098\n",
      "Iteration: 6154 lambda_k: 1 Loss: 0.0029294082669800883\n",
      "Iteration: 6155 lambda_k: 1 Loss: 0.002929408266980663\n",
      "Iteration: 6156 lambda_k: 1 Loss: 0.0029294082669812332\n",
      "Iteration: 6157 lambda_k: 1 Loss: 0.002929408266981806\n",
      "Iteration: 6158 lambda_k: 1 Loss: 0.0029294082669823708\n",
      "Iteration: 6159 lambda_k: 1 Loss: 0.0029294082669829545\n",
      "Iteration: 6160 lambda_k: 1 Loss: 0.002929408266983512\n",
      "Iteration: 6161 lambda_k: 1 Loss: 0.002929408266984067\n",
      "Iteration: 6162 lambda_k: 1 Loss: 0.002929408266984637\n",
      "Iteration: 6163 lambda_k: 1 Loss: 0.002929408266985184\n",
      "Iteration: 6164 lambda_k: 1 Loss: 0.002929408266985742\n",
      "Iteration: 6165 lambda_k: 1 Loss: 0.0029294082669863186\n",
      "Iteration: 6166 lambda_k: 1 Loss: 0.002929408266986864\n",
      "Iteration: 6167 lambda_k: 1 Loss: 0.002929408266987405\n",
      "Iteration: 6168 lambda_k: 1 Loss: 0.002929408266987953\n",
      "Iteration: 6169 lambda_k: 1 Loss: 0.0029294082669885147\n",
      "Iteration: 6170 lambda_k: 1 Loss: 0.002929408266989062\n",
      "Iteration: 6171 lambda_k: 1 Loss: 0.0029294082669895976\n",
      "Iteration: 6172 lambda_k: 1 Loss: 0.0029294082669901237\n",
      "Iteration: 6173 lambda_k: 1 Loss: 0.002929408266990667\n",
      "Iteration: 6174 lambda_k: 1 Loss: 0.0029294082669911914\n",
      "Iteration: 6175 lambda_k: 1 Loss: 0.002929408266991717\n",
      "Iteration: 6176 lambda_k: 1 Loss: 0.0029294082669922344\n",
      "Iteration: 6177 lambda_k: 1 Loss: 0.002929408266992754\n",
      "Iteration: 6178 lambda_k: 1 Loss: 0.002929408266993299\n",
      "Iteration: 6179 lambda_k: 1 Loss: 0.00292940826699381\n",
      "Iteration: 6180 lambda_k: 1 Loss: 0.0029294082669943126\n",
      "Iteration: 6181 lambda_k: 1 Loss: 0.0029294082669948335\n",
      "Iteration: 6182 lambda_k: 1 Loss: 0.0029294082669953365\n",
      "Iteration: 6183 lambda_k: 1 Loss: 0.0029294082669958452\n",
      "Iteration: 6184 lambda_k: 1 Loss: 0.002929408266996341\n",
      "Iteration: 6185 lambda_k: 1 Loss: 0.002929408266996835\n",
      "Iteration: 6186 lambda_k: 1 Loss: 0.002929408266997331\n",
      "Iteration: 6187 lambda_k: 1 Loss: 0.0029294082669978315\n",
      "Iteration: 6188 lambda_k: 1 Loss: 0.0029294082669983216\n",
      "Iteration: 6189 lambda_k: 1 Loss: 0.0029294082669988316\n",
      "Iteration: 6190 lambda_k: 1 Loss: 0.002929408266999315\n",
      "Iteration: 6191 lambda_k: 1 Loss: 0.0029294082669997965\n",
      "Iteration: 6192 lambda_k: 1 Loss: 0.002929408267000279\n",
      "Iteration: 6193 lambda_k: 1 Loss: 0.00292940826700076\n",
      "Iteration: 6194 lambda_k: 1 Loss: 0.002929408267001254\n",
      "Iteration: 6195 lambda_k: 1 Loss: 0.0029294082670017273\n",
      "Iteration: 6196 lambda_k: 1 Loss: 0.002929408267002195\n",
      "Iteration: 6197 lambda_k: 1 Loss: 0.002929408267002663\n",
      "Iteration: 6198 lambda_k: 1 Loss: 0.0029294082670031536\n",
      "Iteration: 6199 lambda_k: 1 Loss: 0.002929408267003621\n",
      "Iteration: 6200 lambda_k: 1 Loss: 0.0029294082670040843\n",
      "Iteration: 6201 lambda_k: 1 Loss: 0.0029294082670045427\n",
      "Iteration: 6202 lambda_k: 1 Loss: 0.0029294082670049963\n",
      "Iteration: 6203 lambda_k: 1 Loss: 0.0029294082670054443\n",
      "Iteration: 6204 lambda_k: 1 Loss: 0.002929408267005915\n",
      "Iteration: 6205 lambda_k: 1 Loss: 0.0029294082670063763\n",
      "Iteration: 6206 lambda_k: 1 Loss: 0.002929408267006821\n",
      "Iteration: 6207 lambda_k: 1 Loss: 0.0029294082670072875\n",
      "Iteration: 6208 lambda_k: 1 Loss: 0.0029294082670077324\n",
      "Iteration: 6209 lambda_k: 1 Loss: 0.0029294082670081717\n",
      "Iteration: 6210 lambda_k: 1 Loss: 0.0029294082670086206\n",
      "Iteration: 6211 lambda_k: 1 Loss: 0.0029294082670090794\n",
      "Iteration: 6212 lambda_k: 1 Loss: 0.0029294082670095157\n",
      "Iteration: 6213 lambda_k: 1 Loss: 0.0029294082670099507\n",
      "Iteration: 6214 lambda_k: 1 Loss: 0.002929408267010381\n",
      "Iteration: 6215 lambda_k: 1 Loss: 0.0029294082670108137\n",
      "Iteration: 6216 lambda_k: 1 Loss: 0.0029294082670112435\n",
      "Iteration: 6217 lambda_k: 1 Loss: 0.002929408267011672\n",
      "Iteration: 6218 lambda_k: 1 Loss: 0.002929408267012097\n",
      "Iteration: 6219 lambda_k: 1 Loss: 0.002929408267012546\n",
      "Iteration: 6220 lambda_k: 1 Loss: 0.002929408267012971\n",
      "Iteration: 6221 lambda_k: 1 Loss: 0.002929408267013387\n",
      "Iteration: 6222 lambda_k: 1 Loss: 0.00292940826701381\n",
      "Iteration: 6223 lambda_k: 1 Loss: 0.0029294082670142294\n",
      "Iteration: 6224 lambda_k: 1 Loss: 0.0029294082670146726\n",
      "Iteration: 6225 lambda_k: 1 Loss: 0.0029294082670150825\n",
      "Iteration: 6226 lambda_k: 1 Loss: 0.0029294082670154897\n",
      "Iteration: 6227 lambda_k: 1 Loss: 0.0029294082670158947\n",
      "Iteration: 6228 lambda_k: 1 Loss: 0.002929408267016298\n",
      "Iteration: 6229 lambda_k: 1 Loss: 0.0029294082670167218\n",
      "Iteration: 6230 lambda_k: 1 Loss: 0.002929408267017128\n",
      "Iteration: 6231 lambda_k: 1 Loss: 0.0029294082670175267\n",
      "Iteration: 6232 lambda_k: 1 Loss: 0.002929408267017947\n",
      "Iteration: 6233 lambda_k: 1 Loss: 0.002929408267018347\n",
      "Iteration: 6234 lambda_k: 1 Loss: 0.0029294082670187384\n",
      "Iteration: 6235 lambda_k: 1 Loss: 0.002929408267019132\n",
      "Iteration: 6236 lambda_k: 1 Loss: 0.0029294082670195194\n",
      "Iteration: 6237 lambda_k: 1 Loss: 0.002929408267019918\n",
      "Iteration: 6238 lambda_k: 1 Loss: 0.002929408267020303\n",
      "Iteration: 6239 lambda_k: 1 Loss: 0.0029294082670207095\n",
      "Iteration: 6240 lambda_k: 1 Loss: 0.002929408267021088\n",
      "Iteration: 6241 lambda_k: 1 Loss: 0.0029294082670214636\n",
      "Iteration: 6242 lambda_k: 1 Loss: 0.002929408267021846\n",
      "Iteration: 6243 lambda_k: 1 Loss: 0.0029294082670222226\n",
      "Iteration: 6244 lambda_k: 1 Loss: 0.002929408267022599\n",
      "Iteration: 6245 lambda_k: 1 Loss: 0.0029294082670229737\n",
      "Iteration: 6246 lambda_k: 1 Loss: 0.002929408267023345\n",
      "Iteration: 6247 lambda_k: 1 Loss: 0.0029294082670237305\n",
      "Iteration: 6248 lambda_k: 1 Loss: 0.002929408267024104\n",
      "Iteration: 6249 lambda_k: 1 Loss: 0.0029294082670244656\n",
      "Iteration: 6250 lambda_k: 1 Loss: 0.002929408267024832\n",
      "Iteration: 6251 lambda_k: 1 Loss: 0.0029294082670251894\n",
      "Iteration: 6252 lambda_k: 1 Loss: 0.0029294082670255467\n",
      "Iteration: 6253 lambda_k: 1 Loss: 0.002929408267025923\n",
      "Iteration: 6254 lambda_k: 1 Loss: 0.0029294082670262784\n",
      "Iteration: 6255 lambda_k: 1 Loss: 0.0029294082670266283\n",
      "Iteration: 6256 lambda_k: 1 Loss: 0.00292940826702698\n",
      "Iteration: 6257 lambda_k: 1 Loss: 0.0029294082670273404\n",
      "Iteration: 6258 lambda_k: 1 Loss: 0.0029294082670277147\n",
      "Iteration: 6259 lambda_k: 1 Loss: 0.002929408267028062\n",
      "Iteration: 6260 lambda_k: 1 Loss: 0.0029294082670284077\n",
      "Iteration: 6261 lambda_k: 1 Loss: 0.0029294082670287607\n",
      "Iteration: 6262 lambda_k: 1 Loss: 0.0029294082670291064\n",
      "Iteration: 6263 lambda_k: 1 Loss: 0.0029294082670294685\n",
      "Iteration: 6264 lambda_k: 1 Loss: 0.002929408267029814\n",
      "Iteration: 6265 lambda_k: 1 Loss: 0.002929408267030156\n",
      "Iteration: 6266 lambda_k: 1 Loss: 0.0029294082670305\n",
      "Iteration: 6267 lambda_k: 1 Loss: 0.002929408267030834\n",
      "Iteration: 6268 lambda_k: 1 Loss: 0.0029294082670311677\n",
      "Iteration: 6269 lambda_k: 1 Loss: 0.002929408267031498\n",
      "Iteration: 6270 lambda_k: 1 Loss: 0.0029294082670318447\n",
      "Iteration: 6271 lambda_k: 1 Loss: 0.0029294082670321756\n",
      "Iteration: 6272 lambda_k: 1 Loss: 0.0029294082670325086\n",
      "Iteration: 6273 lambda_k: 1 Loss: 0.0029294082670328456\n",
      "Iteration: 6274 lambda_k: 1 Loss: 0.0029294082670331674\n",
      "Iteration: 6275 lambda_k: 1 Loss: 0.002929408267033491\n",
      "Iteration: 6276 lambda_k: 1 Loss: 0.002929408267033808\n",
      "Iteration: 6277 lambda_k: 1 Loss: 0.0029294082670341236\n",
      "Iteration: 6278 lambda_k: 1 Loss: 0.0029294082670344363\n",
      "Iteration: 6279 lambda_k: 1 Loss: 0.002929408267034751\n",
      "Iteration: 6280 lambda_k: 1 Loss: 0.002929408267035064\n",
      "Iteration: 6281 lambda_k: 1 Loss: 0.0029294082670353943\n",
      "Iteration: 6282 lambda_k: 1 Loss: 0.002929408267035711\n",
      "Iteration: 6283 lambda_k: 1 Loss: 0.00292940826703603\n",
      "Iteration: 6284 lambda_k: 1 Loss: 0.0029294082670363623\n",
      "Iteration: 6285 lambda_k: 1 Loss: 0.002929408267036669\n",
      "Iteration: 6286 lambda_k: 1 Loss: 0.002929408267036983\n",
      "Iteration: 6287 lambda_k: 1 Loss: 0.0029294082670372895\n",
      "Iteration: 6288 lambda_k: 1 Loss: 0.0029294082670375875\n",
      "Iteration: 6289 lambda_k: 1 Loss: 0.0029294082670378876\n",
      "Iteration: 6290 lambda_k: 1 Loss: 0.002929408267038196\n",
      "Iteration: 6291 lambda_k: 1 Loss: 0.0029294082670384934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6292 lambda_k: 1 Loss: 0.0029294082670387953\n",
      "Iteration: 6293 lambda_k: 1 Loss: 0.002929408267039109\n",
      "Iteration: 6294 lambda_k: 1 Loss: 0.0029294082670394098\n",
      "Iteration: 6295 lambda_k: 1 Loss: 0.002929408267039707\n",
      "Iteration: 6296 lambda_k: 1 Loss: 0.002929408267039997\n",
      "Iteration: 6297 lambda_k: 1 Loss: 0.0029294082670402806\n",
      "Iteration: 6298 lambda_k: 1 Loss: 0.0029294082670405677\n",
      "Iteration: 6299 lambda_k: 1 Loss: 0.0029294082670408526\n",
      "Iteration: 6300 lambda_k: 1 Loss: 0.0029294082670411428\n",
      "Iteration: 6301 lambda_k: 1 Loss: 0.002929408267041427\n",
      "Iteration: 6302 lambda_k: 1 Loss: 0.0029294082670417144\n",
      "Iteration: 6303 lambda_k: 1 Loss: 0.0029294082670420158\n",
      "Iteration: 6304 lambda_k: 1 Loss: 0.002929408267042302\n",
      "Iteration: 6305 lambda_k: 1 Loss: 0.0029294082670425774\n",
      "Iteration: 6306 lambda_k: 1 Loss: 0.002929408267042855\n",
      "Iteration: 6307 lambda_k: 1 Loss: 0.0029294082670431373\n",
      "Iteration: 6308 lambda_k: 1 Loss: 0.0029294082670434122\n",
      "Iteration: 6309 lambda_k: 1 Loss: 0.002929408267043689\n",
      "Iteration: 6310 lambda_k: 1 Loss: 0.0029294082670439673\n",
      "Iteration: 6311 lambda_k: 1 Loss: 0.0029294082670442527\n",
      "Iteration: 6312 lambda_k: 1 Loss: 0.0029294082670445276\n",
      "Iteration: 6313 lambda_k: 1 Loss: 0.002929408267044827\n",
      "Iteration: 6314 lambda_k: 1 Loss: 0.0029294082670450914\n",
      "Iteration: 6315 lambda_k: 1 Loss: 0.0029294082670453642\n",
      "Iteration: 6316 lambda_k: 1 Loss: 0.002929408267045629\n",
      "Iteration: 6317 lambda_k: 1 Loss: 0.0029294082670458985\n",
      "Iteration: 6318 lambda_k: 1 Loss: 0.002929408267046172\n",
      "Iteration: 6319 lambda_k: 1 Loss: 0.002929408267046432\n",
      "Iteration: 6320 lambda_k: 1 Loss: 0.0029294082670466995\n",
      "Iteration: 6321 lambda_k: 1 Loss: 0.0029294082670469753\n",
      "Iteration: 6322 lambda_k: 1 Loss: 0.002929408267047234\n",
      "Iteration: 6323 lambda_k: 1 Loss: 0.002929408267047495\n",
      "Iteration: 6324 lambda_k: 1 Loss: 0.002929408267047751\n",
      "Iteration: 6325 lambda_k: 1 Loss: 0.0029294082670479997\n",
      "Iteration: 6326 lambda_k: 1 Loss: 0.002929408267048255\n",
      "Iteration: 6327 lambda_k: 1 Loss: 0.0029294082670485145\n",
      "Iteration: 6328 lambda_k: 1 Loss: 0.0029294082670487842\n",
      "Iteration: 6329 lambda_k: 1 Loss: 0.0029294082670490345\n",
      "Iteration: 6330 lambda_k: 1 Loss: 0.0029294082670492804\n",
      "Iteration: 6331 lambda_k: 1 Loss: 0.002929408267049521\n",
      "Iteration: 6332 lambda_k: 1 Loss: 0.00292940826704977\n",
      "Iteration: 6333 lambda_k: 1 Loss: 0.002929408267050015\n",
      "Iteration: 6334 lambda_k: 1 Loss: 0.002929408267050255\n",
      "Iteration: 6335 lambda_k: 1 Loss: 0.0029294082670504977\n",
      "Iteration: 6336 lambda_k: 1 Loss: 0.0029294082670507375\n",
      "Iteration: 6337 lambda_k: 1 Loss: 0.002929408267050965\n",
      "Iteration: 6338 lambda_k: 1 Loss: 0.0029294082670512\n",
      "Iteration: 6339 lambda_k: 1 Loss: 0.002929408267051432\n",
      "Iteration: 6340 lambda_k: 1 Loss: 0.002929408267051661\n",
      "Iteration: 6341 lambda_k: 1 Loss: 0.0029294082670518968\n",
      "Iteration: 6342 lambda_k: 1 Loss: 0.0029294082670521266\n",
      "Iteration: 6343 lambda_k: 1 Loss: 0.002929408267052363\n",
      "Iteration: 6344 lambda_k: 1 Loss: 0.002929408267052587\n",
      "Iteration: 6345 lambda_k: 1 Loss: 0.0029294082670528253\n",
      "Iteration: 6346 lambda_k: 1 Loss: 0.002929408267053057\n",
      "Iteration: 6347 lambda_k: 1 Loss: 0.0029294082670532915\n",
      "Iteration: 6348 lambda_k: 1 Loss: 0.0029294082670535144\n",
      "Iteration: 6349 lambda_k: 1 Loss: 0.0029294082670537377\n",
      "Iteration: 6350 lambda_k: 1 Loss: 0.002929408267053963\n",
      "Iteration: 6351 lambda_k: 1 Loss: 0.00292940826705418\n",
      "Iteration: 6352 lambda_k: 1 Loss: 0.002929408267054391\n",
      "Iteration: 6353 lambda_k: 1 Loss: 0.0029294082670546064\n",
      "Iteration: 6354 lambda_k: 1 Loss: 0.0029294082670548267\n",
      "Iteration: 6355 lambda_k: 1 Loss: 0.002929408267055042\n",
      "Iteration: 6356 lambda_k: 1 Loss: 0.0029294082670552586\n",
      "Iteration: 6357 lambda_k: 1 Loss: 0.002929408267055468\n",
      "Iteration: 6358 lambda_k: 1 Loss: 0.0029294082670556854\n",
      "Iteration: 6359 lambda_k: 1 Loss: 0.0029294082670558923\n",
      "Iteration: 6360 lambda_k: 1 Loss: 0.002929408267056108\n",
      "Iteration: 6361 lambda_k: 1 Loss: 0.0029294082670563238\n",
      "Iteration: 6362 lambda_k: 1 Loss: 0.002929408267056527\n",
      "Iteration: 6363 lambda_k: 1 Loss: 0.0029294082670567358\n",
      "Iteration: 6364 lambda_k: 1 Loss: 0.0029294082670569504\n",
      "Iteration: 6365 lambda_k: 1 Loss: 0.0029294082670571564\n",
      "Iteration: 6366 lambda_k: 1 Loss: 0.0029294082670573603\n",
      "Iteration: 6367 lambda_k: 1 Loss: 0.0029294082670575624\n",
      "Iteration: 6368 lambda_k: 1 Loss: 0.002929408267057762\n",
      "Iteration: 6369 lambda_k: 1 Loss: 0.0029294082670579644\n",
      "Iteration: 6370 lambda_k: 1 Loss: 0.002929408267058169\n",
      "Iteration: 6371 lambda_k: 1 Loss: 0.0029294082670583664\n",
      "Iteration: 6372 lambda_k: 1 Loss: 0.0029294082670585798\n",
      "Iteration: 6373 lambda_k: 1 Loss: 0.0029294082670587754\n",
      "Iteration: 6374 lambda_k: 1 Loss: 0.0029294082670589727\n",
      "Iteration: 6375 lambda_k: 1 Loss: 0.0029294082670591696\n",
      "Iteration: 6376 lambda_k: 1 Loss: 0.002929408267059361\n",
      "Iteration: 6377 lambda_k: 1 Loss: 0.0029294082670595512\n",
      "Iteration: 6378 lambda_k: 1 Loss: 0.0029294082670597425\n",
      "Iteration: 6379 lambda_k: 1 Loss: 0.0029294082670599415\n",
      "Iteration: 6380 lambda_k: 1 Loss: 0.002929408267060136\n",
      "Iteration: 6381 lambda_k: 1 Loss: 0.0029294082670603266\n",
      "Iteration: 6382 lambda_k: 1 Loss: 0.0029294082670605352\n",
      "Iteration: 6383 lambda_k: 1 Loss: 0.0029294082670607256\n",
      "Iteration: 6384 lambda_k: 1 Loss: 0.002929408267060912\n",
      "Iteration: 6385 lambda_k: 1 Loss: 0.002929408267061106\n",
      "Iteration: 6386 lambda_k: 1 Loss: 0.002929408267061296\n",
      "Iteration: 6387 lambda_k: 1 Loss: 0.0029294082670614794\n",
      "Iteration: 6388 lambda_k: 1 Loss: 0.00292940826706166\n",
      "Iteration: 6389 lambda_k: 1 Loss: 0.0029294082670618406\n",
      "Iteration: 6390 lambda_k: 1 Loss: 0.0029294082670620184\n",
      "Iteration: 6391 lambda_k: 1 Loss: 0.002929408267062206\n",
      "Iteration: 6392 lambda_k: 1 Loss: 0.0029294082670624066\n",
      "Iteration: 6393 lambda_k: 1 Loss: 0.0029294082670625913\n",
      "Iteration: 6394 lambda_k: 1 Loss: 0.002929408267062777\n",
      "Iteration: 6395 lambda_k: 1 Loss: 0.00292940826706296\n",
      "Iteration: 6396 lambda_k: 1 Loss: 0.002929408267063142\n",
      "Iteration: 6397 lambda_k: 1 Loss: 0.002929408267063318\n",
      "Iteration: 6398 lambda_k: 1 Loss: 0.0029294082670634903\n",
      "Iteration: 6399 lambda_k: 1 Loss: 0.002929408267063675\n",
      "Iteration: 6400 lambda_k: 1 Loss: 0.0029294082670638572\n",
      "Iteration: 6401 lambda_k: 1 Loss: 0.002929408267064039\n",
      "Iteration: 6402 lambda_k: 1 Loss: 0.002929408267064216\n",
      "Iteration: 6403 lambda_k: 1 Loss: 0.0029294082670643915\n",
      "Iteration: 6404 lambda_k: 1 Loss: 0.0029294082670645585\n",
      "Iteration: 6405 lambda_k: 1 Loss: 0.0029294082670647315\n",
      "Iteration: 6406 lambda_k: 1 Loss: 0.002929408267064903\n",
      "Iteration: 6407 lambda_k: 1 Loss: 0.0029294082670650668\n",
      "Iteration: 6408 lambda_k: 1 Loss: 0.0029294082670652337\n",
      "Iteration: 6409 lambda_k: 1 Loss: 0.0029294082670654055\n",
      "Iteration: 6410 lambda_k: 1 Loss: 0.0029294082670655763\n",
      "Iteration: 6411 lambda_k: 1 Loss: 0.0029294082670657624\n",
      "Iteration: 6412 lambda_k: 1 Loss: 0.002929408267065932\n",
      "Iteration: 6413 lambda_k: 1 Loss: 0.002929408267066095\n",
      "Iteration: 6414 lambda_k: 1 Loss: 0.002929408267066255\n",
      "Iteration: 6415 lambda_k: 1 Loss: 0.0029294082670664272\n",
      "Iteration: 6416 lambda_k: 1 Loss: 0.0029294082670665955\n",
      "Iteration: 6417 lambda_k: 1 Loss: 0.002929408267066763\n",
      "Iteration: 6418 lambda_k: 1 Loss: 0.0029294082670669333\n",
      "Iteration: 6419 lambda_k: 1 Loss: 0.002929408267067091\n",
      "Iteration: 6420 lambda_k: 1 Loss: 0.002929408267067245\n",
      "Iteration: 6421 lambda_k: 1 Loss: 0.002929408267067399\n",
      "Iteration: 6422 lambda_k: 1 Loss: 0.002929408267067573\n",
      "Iteration: 6423 lambda_k: 1 Loss: 0.0029294082670677283\n",
      "Iteration: 6424 lambda_k: 1 Loss: 0.0029294082670678874\n",
      "Iteration: 6425 lambda_k: 1 Loss: 0.0029294082670680436\n",
      "Iteration: 6426 lambda_k: 1 Loss: 0.002929408267068197\n",
      "Iteration: 6427 lambda_k: 1 Loss: 0.0029294082670683567\n",
      "Iteration: 6428 lambda_k: 1 Loss: 0.0029294082670685132\n",
      "Iteration: 6429 lambda_k: 1 Loss: 0.0029294082670686676\n",
      "Iteration: 6430 lambda_k: 1 Loss: 0.002929408267068825\n",
      "Iteration: 6431 lambda_k: 1 Loss: 0.0029294082670689794\n",
      "Iteration: 6432 lambda_k: 1 Loss: 0.002929408267069133\n",
      "Iteration: 6433 lambda_k: 1 Loss: 0.0029294082670692796\n",
      "Iteration: 6434 lambda_k: 1 Loss: 0.0029294082670694283\n",
      "Iteration: 6435 lambda_k: 1 Loss: 0.002929408267069575\n",
      "Iteration: 6436 lambda_k: 1 Loss: 0.002929408267069726\n",
      "Iteration: 6437 lambda_k: 1 Loss: 0.002929408267069871\n",
      "Iteration: 6438 lambda_k: 1 Loss: 0.0029294082670700172\n",
      "Iteration: 6439 lambda_k: 1 Loss: 0.0029294082670701786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6440 lambda_k: 1 Loss: 0.0029294082670703325\n",
      "Iteration: 6441 lambda_k: 1 Loss: 0.0029294082670704774\n",
      "Iteration: 6442 lambda_k: 1 Loss: 0.0029294082670706214\n",
      "Iteration: 6443 lambda_k: 1 Loss: 0.002929408267070766\n",
      "Iteration: 6444 lambda_k: 1 Loss: 0.002929408267070908\n",
      "Iteration: 6445 lambda_k: 1 Loss: 0.0029294082670710537\n",
      "Iteration: 6446 lambda_k: 1 Loss: 0.0029294082670711903\n",
      "Iteration: 6447 lambda_k: 1 Loss: 0.002929408267071327\n",
      "Iteration: 6448 lambda_k: 1 Loss: 0.002929408267071474\n",
      "Iteration: 6449 lambda_k: 1 Loss: 0.0029294082670716115\n",
      "Iteration: 6450 lambda_k: 1 Loss: 0.0029294082670717437\n",
      "Iteration: 6451 lambda_k: 1 Loss: 0.002929408267071881\n",
      "Iteration: 6452 lambda_k: 1 Loss: 0.002929408267072022\n",
      "Iteration: 6453 lambda_k: 1 Loss: 0.0029294082670721614\n",
      "Iteration: 6454 lambda_k: 1 Loss: 0.002929408267072299\n",
      "Iteration: 6455 lambda_k: 1 Loss: 0.0029294082670724324\n",
      "Iteration: 6456 lambda_k: 1 Loss: 0.002929408267072568\n",
      "Iteration: 6457 lambda_k: 1 Loss: 0.0029294082670727074\n",
      "Iteration: 6458 lambda_k: 1 Loss: 0.0029294082670728444\n",
      "Iteration: 6459 lambda_k: 1 Loss: 0.0029294082670729762\n",
      "Iteration: 6460 lambda_k: 1 Loss: 0.002929408267073133\n",
      "Iteration: 6461 lambda_k: 1 Loss: 0.002929408267073271\n",
      "Iteration: 6462 lambda_k: 1 Loss: 0.0029294082670734013\n",
      "Iteration: 6463 lambda_k: 1 Loss: 0.0029294082670735327\n",
      "Iteration: 6464 lambda_k: 1 Loss: 0.0029294082670736697\n",
      "Iteration: 6465 lambda_k: 1 Loss: 0.0029294082670738093\n",
      "Iteration: 6466 lambda_k: 1 Loss: 0.0029294082670739503\n",
      "Iteration: 6467 lambda_k: 1 Loss: 0.0029294082670740847\n",
      "Iteration: 6468 lambda_k: 1 Loss: 0.0029294082670742096\n",
      "Iteration: 6469 lambda_k: 1 Loss: 0.0029294082670743428\n",
      "Iteration: 6470 lambda_k: 1 Loss: 0.002929408267074497\n",
      "Iteration: 6471 lambda_k: 1 Loss: 0.002929408267074625\n",
      "Iteration: 6472 lambda_k: 1 Loss: 0.002929408267074754\n",
      "Iteration: 6473 lambda_k: 1 Loss: 0.002929408267074877\n",
      "Iteration: 6474 lambda_k: 1 Loss: 0.002929408267075004\n",
      "Iteration: 6475 lambda_k: 1 Loss: 0.002929408267075124\n",
      "Iteration: 6476 lambda_k: 1 Loss: 0.002929408267075251\n",
      "Iteration: 6477 lambda_k: 1 Loss: 0.002929408267075378\n",
      "Iteration: 6478 lambda_k: 1 Loss: 0.0029294082670755037\n",
      "Iteration: 6479 lambda_k: 1 Loss: 0.002929408267075626\n",
      "Iteration: 6480 lambda_k: 1 Loss: 0.002929408267075743\n",
      "Iteration: 6481 lambda_k: 1 Loss: 0.0029294082670758633\n",
      "Iteration: 6482 lambda_k: 1 Loss: 0.002929408267075988\n",
      "Iteration: 6483 lambda_k: 1 Loss: 0.002929408267076107\n",
      "Iteration: 6484 lambda_k: 1 Loss: 0.0029294082670762284\n",
      "Iteration: 6485 lambda_k: 1 Loss: 0.00292940826707634\n",
      "Iteration: 6486 lambda_k: 1 Loss: 0.0029294082670764505\n",
      "Iteration: 6487 lambda_k: 1 Loss: 0.0029294082670765654\n",
      "Iteration: 6488 lambda_k: 1 Loss: 0.002929408267076679\n",
      "Iteration: 6489 lambda_k: 1 Loss: 0.002929408267076791\n",
      "Iteration: 6490 lambda_k: 1 Loss: 0.0029294082670769345\n",
      "Iteration: 6491 lambda_k: 1 Loss: 0.002929408267077051\n",
      "Iteration: 6492 lambda_k: 1 Loss: 0.002929408267077161\n",
      "Iteration: 6493 lambda_k: 1 Loss: 0.0029294082670772697\n",
      "Iteration: 6494 lambda_k: 1 Loss: 0.002929408267077383\n",
      "Iteration: 6495 lambda_k: 1 Loss: 0.0029294082670774943\n",
      "Iteration: 6496 lambda_k: 1 Loss: 0.0029294082670776075\n",
      "Iteration: 6497 lambda_k: 1 Loss: 0.0029294082670777264\n",
      "Iteration: 6498 lambda_k: 1 Loss: 0.00292940826707784\n",
      "Iteration: 6499 lambda_k: 1 Loss: 0.0029294082670779545\n",
      "Iteration: 6500 lambda_k: 1 Loss: 0.0029294082670780603\n",
      "Iteration: 6501 lambda_k: 1 Loss: 0.002929408267078174\n",
      "Iteration: 6502 lambda_k: 1 Loss: 0.0029294082670782767\n",
      "Iteration: 6503 lambda_k: 1 Loss: 0.002929408267078381\n",
      "Iteration: 6504 lambda_k: 1 Loss: 0.0029294082670784896\n",
      "Iteration: 6505 lambda_k: 1 Loss: 0.002929408267078602\n",
      "Iteration: 6506 lambda_k: 1 Loss: 0.002929408267078709\n",
      "Iteration: 6507 lambda_k: 1 Loss: 0.0029294082670788145\n",
      "Iteration: 6508 lambda_k: 1 Loss: 0.002929408267078935\n",
      "Iteration: 6509 lambda_k: 1 Loss: 0.0029294082670790374\n",
      "Iteration: 6510 lambda_k: 1 Loss: 0.002929408267079142\n",
      "Iteration: 6511 lambda_k: 1 Loss: 0.002929408267079242\n",
      "Iteration: 6512 lambda_k: 1 Loss: 0.002929408267079351\n",
      "Iteration: 6513 lambda_k: 1 Loss: 0.0029294082670794554\n",
      "Iteration: 6514 lambda_k: 1 Loss: 0.002929408267079553\n",
      "Iteration: 6515 lambda_k: 1 Loss: 0.002929408267079653\n",
      "Iteration: 6516 lambda_k: 1 Loss: 0.002929408267079761\n",
      "Iteration: 6517 lambda_k: 1 Loss: 0.0029294082670798843\n",
      "Iteration: 6518 lambda_k: 1 Loss: 0.0029294082670799824\n",
      "Iteration: 6519 lambda_k: 1 Loss: 0.002929408267080081\n",
      "Iteration: 6520 lambda_k: 1 Loss: 0.002929408267080179\n",
      "Iteration: 6521 lambda_k: 1 Loss: 0.002929408267080277\n",
      "Iteration: 6522 lambda_k: 1 Loss: 0.002929408267080379\n",
      "Iteration: 6523 lambda_k: 1 Loss: 0.0029294082670804785\n",
      "Iteration: 6524 lambda_k: 1 Loss: 0.0029294082670805782\n",
      "Iteration: 6525 lambda_k: 1 Loss: 0.0029294082670806763\n",
      "Iteration: 6526 lambda_k: 1 Loss: 0.002929408267080779\n",
      "Iteration: 6527 lambda_k: 1 Loss: 0.002929408267080875\n",
      "Iteration: 6528 lambda_k: 1 Loss: 0.0029294082670809716\n",
      "Iteration: 6529 lambda_k: 1 Loss: 0.0029294082670810605\n",
      "Iteration: 6530 lambda_k: 1 Loss: 0.0029294082670811507\n",
      "Iteration: 6531 lambda_k: 1 Loss: 0.002929408267081251\n",
      "Iteration: 6532 lambda_k: 1 Loss: 0.0029294082670813406\n",
      "Iteration: 6533 lambda_k: 1 Loss: 0.00292940826708144\n",
      "Iteration: 6534 lambda_k: 1 Loss: 0.002929408267081531\n",
      "Iteration: 6535 lambda_k: 1 Loss: 0.002929408267081619\n",
      "Iteration: 6536 lambda_k: 1 Loss: 0.0029294082670817075\n",
      "Iteration: 6537 lambda_k: 1 Loss: 0.0029294082670818004\n",
      "Iteration: 6538 lambda_k: 1 Loss: 0.0029294082670818862\n",
      "Iteration: 6539 lambda_k: 1 Loss: 0.002929408267081974\n",
      "Iteration: 6540 lambda_k: 1 Loss: 0.0029294082670820606\n",
      "Iteration: 6541 lambda_k: 1 Loss: 0.0029294082670821477\n",
      "Iteration: 6542 lambda_k: 1 Loss: 0.002929408267082231\n",
      "Iteration: 6543 lambda_k: 1 Loss: 0.0029294082670823234\n",
      "Iteration: 6544 lambda_k: 1 Loss: 0.002929408267082409\n",
      "Iteration: 6545 lambda_k: 1 Loss: 0.002929408267082494\n",
      "Iteration: 6546 lambda_k: 1 Loss: 0.0029294082670825805\n",
      "Iteration: 6547 lambda_k: 1 Loss: 0.0029294082670826642\n",
      "Iteration: 6548 lambda_k: 1 Loss: 0.0029294082670827484\n",
      "Iteration: 6549 lambda_k: 1 Loss: 0.0029294082670828247\n",
      "Iteration: 6550 lambda_k: 1 Loss: 0.0029294082670829062\n",
      "Iteration: 6551 lambda_k: 1 Loss: 0.002929408267083\n",
      "Iteration: 6552 lambda_k: 1 Loss: 0.0029294082670830858\n",
      "Iteration: 6553 lambda_k: 1 Loss: 0.0029294082670831686\n",
      "Iteration: 6554 lambda_k: 1 Loss: 0.002929408267083252\n",
      "Iteration: 6555 lambda_k: 1 Loss: 0.002929408267083336\n",
      "Iteration: 6556 lambda_k: 1 Loss: 0.002929408267083426\n",
      "Iteration: 6557 lambda_k: 1 Loss: 0.0029294082670835095\n",
      "Iteration: 6558 lambda_k: 1 Loss: 0.0029294082670836\n",
      "Iteration: 6559 lambda_k: 1 Loss: 0.002929408267083687\n",
      "Iteration: 6560 lambda_k: 1 Loss: 0.002929408267083766\n",
      "Iteration: 6561 lambda_k: 1 Loss: 0.0029294082670838434\n",
      "Iteration: 6562 lambda_k: 1 Loss: 0.0029294082670839458\n",
      "Iteration: 6563 lambda_k: 1 Loss: 0.0029294082670840195\n",
      "Iteration: 6564 lambda_k: 1 Loss: 0.0029294082670841028\n",
      "Iteration: 6565 lambda_k: 1 Loss: 0.0029294082670841895\n",
      "Iteration: 6566 lambda_k: 1 Loss: 0.0029294082670842693\n",
      "Iteration: 6567 lambda_k: 1 Loss: 0.002929408267084354\n",
      "Iteration: 6568 lambda_k: 1 Loss: 0.002929408267084431\n",
      "Iteration: 6569 lambda_k: 1 Loss: 0.002929408267084506\n",
      "Iteration: 6570 lambda_k: 1 Loss: 0.0029294082670845794\n",
      "Iteration: 6571 lambda_k: 1 Loss: 0.002929408267084658\n",
      "Iteration: 6572 lambda_k: 1 Loss: 0.002929408267084737\n",
      "Iteration: 6573 lambda_k: 1 Loss: 0.002929408267084827\n",
      "Iteration: 6574 lambda_k: 1 Loss: 0.002929408267084904\n",
      "Iteration: 6575 lambda_k: 1 Loss: 0.0029294082670849827\n",
      "Iteration: 6576 lambda_k: 1 Loss: 0.0029294082670850547\n",
      "Iteration: 6577 lambda_k: 1 Loss: 0.0029294082670851328\n",
      "Iteration: 6578 lambda_k: 1 Loss: 0.0029294082670852047\n",
      "Iteration: 6579 lambda_k: 1 Loss: 0.0029294082670852832\n",
      "Iteration: 6580 lambda_k: 1 Loss: 0.0029294082670853635\n",
      "Iteration: 6581 lambda_k: 1 Loss: 0.0029294082670854402\n",
      "Iteration: 6582 lambda_k: 1 Loss: 0.0029294082670855196\n",
      "Iteration: 6583 lambda_k: 1 Loss: 0.002929408267085593\n",
      "Iteration: 6584 lambda_k: 1 Loss: 0.0029294082670856935\n",
      "Iteration: 6585 lambda_k: 1 Loss: 0.0029294082670857768\n",
      "Iteration: 6586 lambda_k: 1 Loss: 0.0029294082670858496\n",
      "Iteration: 6587 lambda_k: 1 Loss: 0.0029294082670859208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6588 lambda_k: 1 Loss: 0.0029294082670859936\n",
      "Iteration: 6589 lambda_k: 1 Loss: 0.002929408267086067\n",
      "Iteration: 6590 lambda_k: 1 Loss: 0.002929408267086152\n",
      "Iteration: 6591 lambda_k: 1 Loss: 0.0029294082670862248\n",
      "Iteration: 6592 lambda_k: 1 Loss: 0.0029294082670863033\n",
      "Iteration: 6593 lambda_k: 1 Loss: 0.0029294082670863687\n",
      "Iteration: 6594 lambda_k: 1 Loss: 0.0029294082670864342\n",
      "Iteration: 6595 lambda_k: 1 Loss: 0.002929408267086499\n",
      "Iteration: 6596 lambda_k: 1 Loss: 0.002929408267086561\n",
      "Iteration: 6597 lambda_k: 1 Loss: 0.0029294082670866307\n",
      "Iteration: 6598 lambda_k: 1 Loss: 0.002929408267086698\n",
      "Iteration: 6599 lambda_k: 1 Loss: 0.002929408267086766\n",
      "Iteration: 6600 lambda_k: 1 Loss: 0.0029294082670868393\n",
      "Iteration: 6601 lambda_k: 1 Loss: 0.0029294082670869187\n",
      "Iteration: 6602 lambda_k: 1 Loss: 0.0029294082670870045\n",
      "Iteration: 6603 lambda_k: 1 Loss: 0.0029294082670870782\n",
      "Iteration: 6604 lambda_k: 1 Loss: 0.0029294082670871403\n",
      "Iteration: 6605 lambda_k: 1 Loss: 0.002929408267087211\n",
      "Iteration: 6606 lambda_k: 1 Loss: 0.0029294082670872773\n",
      "Iteration: 6607 lambda_k: 1 Loss: 0.0029294082670873484\n",
      "Iteration: 6608 lambda_k: 1 Loss: 0.0029294082670874165\n",
      "Iteration: 6609 lambda_k: 1 Loss: 0.0029294082670874876\n",
      "Iteration: 6610 lambda_k: 1 Loss: 0.0029294082670875544\n",
      "Iteration: 6611 lambda_k: 1 Loss: 0.002929408267087623\n",
      "Iteration: 6612 lambda_k: 1 Loss: 0.0029294082670876884\n",
      "Iteration: 6613 lambda_k: 1 Loss: 0.002929408267087748\n",
      "Iteration: 6614 lambda_k: 1 Loss: 0.0029294082670878086\n",
      "Iteration: 6615 lambda_k: 1 Loss: 0.0029294082670878823\n",
      "Iteration: 6616 lambda_k: 1 Loss: 0.0029294082670879495\n",
      "Iteration: 6617 lambda_k: 1 Loss: 0.0029294082670880193\n",
      "Iteration: 6618 lambda_k: 1 Loss: 0.00292940826708808\n",
      "Iteration: 6619 lambda_k: 1 Loss: 0.0029294082670881403\n",
      "Iteration: 6620 lambda_k: 1 Loss: 0.0029294082670882115\n",
      "Iteration: 6621 lambda_k: 1 Loss: 0.002929408267088273\n",
      "Iteration: 6622 lambda_k: 1 Loss: 0.002929408267088332\n",
      "Iteration: 6623 lambda_k: 1 Loss: 0.0029294082670883927\n",
      "Iteration: 6624 lambda_k: 1 Loss: 0.002929408267088453\n",
      "Iteration: 6625 lambda_k: 1 Loss: 0.0029294082670885085\n",
      "Iteration: 6626 lambda_k: 1 Loss: 0.002929408267088567\n",
      "Iteration: 6627 lambda_k: 1 Loss: 0.002929408267088622\n",
      "Iteration: 6628 lambda_k: 1 Loss: 0.0029294082670886794\n",
      "Iteration: 6629 lambda_k: 1 Loss: 0.002929408267088737\n",
      "Iteration: 6630 lambda_k: 1 Loss: 0.002929408267088795\n",
      "Iteration: 6631 lambda_k: 1 Loss: 0.002929408267088854\n",
      "Iteration: 6632 lambda_k: 1 Loss: 0.0029294082670889205\n",
      "Iteration: 6633 lambda_k: 1 Loss: 0.002929408267088979\n",
      "Iteration: 6634 lambda_k: 1 Loss: 0.002929408267089033\n",
      "Iteration: 6635 lambda_k: 1 Loss: 0.0029294082670890914\n",
      "Iteration: 6636 lambda_k: 1 Loss: 0.0029294082670891478\n",
      "Iteration: 6637 lambda_k: 1 Loss: 0.0029294082670892085\n",
      "Iteration: 6638 lambda_k: 1 Loss: 0.002929408267089261\n",
      "Iteration: 6639 lambda_k: 1 Loss: 0.0029294082670893104\n",
      "Iteration: 6640 lambda_k: 1 Loss: 0.0029294082670893663\n",
      "Iteration: 6641 lambda_k: 1 Loss: 0.002929408267089418\n",
      "Iteration: 6642 lambda_k: 1 Loss: 0.0029294082670894704\n",
      "Iteration: 6643 lambda_k: 1 Loss: 0.0029294082670895203\n",
      "Iteration: 6644 lambda_k: 1 Loss: 0.002929408267089569\n",
      "Iteration: 6645 lambda_k: 1 Loss: 0.0029294082670896205\n",
      "Iteration: 6646 lambda_k: 1 Loss: 0.0029294082670896717\n",
      "Iteration: 6647 lambda_k: 1 Loss: 0.002929408267089722\n",
      "Iteration: 6648 lambda_k: 1 Loss: 0.0029294082670897684\n",
      "Iteration: 6649 lambda_k: 1 Loss: 0.0029294082670898182\n",
      "Iteration: 6650 lambda_k: 1 Loss: 0.0029294082670898655\n",
      "Iteration: 6651 lambda_k: 1 Loss: 0.002929408267089914\n",
      "Iteration: 6652 lambda_k: 1 Loss: 0.00292940826708996\n",
      "Iteration: 6653 lambda_k: 1 Loss: 0.0029294082670900078\n",
      "Iteration: 6654 lambda_k: 1 Loss: 0.0029294082670900568\n",
      "Iteration: 6655 lambda_k: 1 Loss: 0.002929408267090108\n",
      "Iteration: 6656 lambda_k: 1 Loss: 0.0029294082670901556\n",
      "Iteration: 6657 lambda_k: 1 Loss: 0.002929408267090205\n",
      "Iteration: 6658 lambda_k: 1 Loss: 0.002929408267090275\n",
      "Iteration: 6659 lambda_k: 1 Loss: 0.002929408267090322\n",
      "Iteration: 6660 lambda_k: 1 Loss: 0.002929408267090373\n",
      "Iteration: 6661 lambda_k: 1 Loss: 0.0029294082670904167\n",
      "Iteration: 6662 lambda_k: 1 Loss: 0.0029294082670904623\n",
      "Iteration: 6663 lambda_k: 1 Loss: 0.002929408267090506\n",
      "Iteration: 6664 lambda_k: 1 Loss: 0.00292940826709056\n",
      "Iteration: 6665 lambda_k: 1 Loss: 0.0029294082670906054\n",
      "Iteration: 6666 lambda_k: 1 Loss: 0.0029294082670906618\n",
      "Iteration: 6667 lambda_k: 1 Loss: 0.002929408267090705\n",
      "Iteration: 6668 lambda_k: 1 Loss: 0.0029294082670907507\n",
      "Iteration: 6669 lambda_k: 1 Loss: 0.0029294082670907927\n",
      "Iteration: 6670 lambda_k: 1 Loss: 0.00292940826709084\n",
      "Iteration: 6671 lambda_k: 1 Loss: 0.0029294082670908842\n",
      "Iteration: 6672 lambda_k: 1 Loss: 0.0029294082670909285\n",
      "Iteration: 6673 lambda_k: 1 Loss: 0.0029294082670909736\n",
      "Iteration: 6674 lambda_k: 1 Loss: 0.0029294082670910182\n",
      "Iteration: 6675 lambda_k: 1 Loss: 0.002929408267091065\n",
      "Iteration: 6676 lambda_k: 1 Loss: 0.0029294082670911115\n",
      "Iteration: 6677 lambda_k: 1 Loss: 0.0029294082670911535\n",
      "Iteration: 6678 lambda_k: 1 Loss: 0.002929408267091197\n",
      "Iteration: 6679 lambda_k: 1 Loss: 0.0029294082670912425\n",
      "Iteration: 6680 lambda_k: 1 Loss: 0.0029294082670912884\n",
      "Iteration: 6681 lambda_k: 1 Loss: 0.0029294082670913296\n",
      "Iteration: 6682 lambda_k: 1 Loss: 0.002929408267091373\n",
      "Iteration: 6683 lambda_k: 1 Loss: 0.002929408267091413\n",
      "Iteration: 6684 lambda_k: 1 Loss: 0.002929408267091457\n",
      "Iteration: 6685 lambda_k: 1 Loss: 0.0029294082670914988\n",
      "Iteration: 6686 lambda_k: 1 Loss: 0.002929408267091539\n",
      "Iteration: 6687 lambda_k: 1 Loss: 0.002929408267091576\n",
      "Iteration: 6688 lambda_k: 1 Loss: 0.002929408267091617\n",
      "Iteration: 6689 lambda_k: 1 Loss: 0.0029294082670916523\n",
      "Iteration: 6690 lambda_k: 1 Loss: 0.0029294082670916904\n",
      "Iteration: 6691 lambda_k: 1 Loss: 0.0029294082670917364\n",
      "Iteration: 6692 lambda_k: 1 Loss: 0.0029294082670917815\n",
      "Iteration: 6693 lambda_k: 1 Loss: 0.002929408267091822\n",
      "Iteration: 6694 lambda_k: 1 Loss: 0.002929408267091866\n",
      "Iteration: 6695 lambda_k: 1 Loss: 0.0029294082670919095\n",
      "Iteration: 6696 lambda_k: 1 Loss: 0.0029294082670919515\n",
      "Iteration: 6697 lambda_k: 1 Loss: 0.00292940826709199\n",
      "Iteration: 6698 lambda_k: 1 Loss: 0.002929408267092035\n",
      "Iteration: 6699 lambda_k: 1 Loss: 0.0029294082670920764\n",
      "Iteration: 6700 lambda_k: 1 Loss: 0.002929408267092117\n",
      "Iteration: 6701 lambda_k: 1 Loss: 0.002929408267092158\n",
      "Iteration: 6702 lambda_k: 1 Loss: 0.0029294082670921987\n",
      "Iteration: 6703 lambda_k: 1 Loss: 0.002929408267092241\n",
      "Iteration: 6704 lambda_k: 1 Loss: 0.0029294082670922833\n",
      "Iteration: 6705 lambda_k: 1 Loss: 0.0029294082670923206\n",
      "Iteration: 6706 lambda_k: 1 Loss: 0.0029294082670923587\n",
      "Iteration: 6707 lambda_k: 1 Loss: 0.002929408267092396\n",
      "Iteration: 6708 lambda_k: 1 Loss: 0.002929408267092441\n",
      "Iteration: 6709 lambda_k: 1 Loss: 0.0029294082670924823\n",
      "Iteration: 6710 lambda_k: 1 Loss: 0.002929408267092521\n",
      "Iteration: 6711 lambda_k: 1 Loss: 0.0029294082670925617\n",
      "Iteration: 6712 lambda_k: 1 Loss: 0.0029294082670926172\n",
      "Iteration: 6713 lambda_k: 1 Loss: 0.002929408267092659\n",
      "Iteration: 6714 lambda_k: 1 Loss: 0.0029294082670926966\n",
      "Iteration: 6715 lambda_k: 1 Loss: 0.002929408267092733\n",
      "Iteration: 6716 lambda_k: 1 Loss: 0.0029294082670927647\n",
      "Iteration: 6717 lambda_k: 1 Loss: 0.0029294082670927946\n",
      "Iteration: 6718 lambda_k: 1 Loss: 0.002929408267092834\n",
      "Iteration: 6719 lambda_k: 1 Loss: 0.002929408267092868\n",
      "Iteration: 6720 lambda_k: 1 Loss: 0.0029294082670929026\n",
      "Iteration: 6721 lambda_k: 1 Loss: 0.002929408267092935\n",
      "Iteration: 6722 lambda_k: 1 Loss: 0.002929408267092968\n",
      "Iteration: 6723 lambda_k: 1 Loss: 0.0029294082670930036\n",
      "Iteration: 6724 lambda_k: 1 Loss: 0.002929408267093037\n",
      "Iteration: 6725 lambda_k: 1 Loss: 0.0029294082670930795\n",
      "Iteration: 6726 lambda_k: 1 Loss: 0.0029294082670931186\n",
      "Iteration: 6727 lambda_k: 1 Loss: 0.0029294082670931515\n",
      "Iteration: 6728 lambda_k: 1 Loss: 0.0029294082670931845\n",
      "Iteration: 6729 lambda_k: 1 Loss: 0.002929408267093219\n",
      "Iteration: 6730 lambda_k: 1 Loss: 0.0029294082670932487\n",
      "Iteration: 6731 lambda_k: 1 Loss: 0.002929408267093281\n",
      "Iteration: 6732 lambda_k: 1 Loss: 0.0029294082670933185\n",
      "Iteration: 6733 lambda_k: 1 Loss: 0.0029294082670933506\n",
      "Iteration: 6734 lambda_k: 1 Loss: 0.0029294082670933827\n",
      "Iteration: 6735 lambda_k: 1 Loss: 0.002929408267093417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6736 lambda_k: 1 Loss: 0.0029294082670934495\n",
      "Iteration: 6737 lambda_k: 1 Loss: 0.002929408267093479\n",
      "Iteration: 6738 lambda_k: 1 Loss: 0.0029294082670935093\n",
      "Iteration: 6739 lambda_k: 1 Loss: 0.0029294082670935457\n",
      "Iteration: 6740 lambda_k: 1 Loss: 0.0029294082670935913\n",
      "Iteration: 6741 lambda_k: 1 Loss: 0.002929408267093623\n",
      "Iteration: 6742 lambda_k: 1 Loss: 0.0029294082670936563\n",
      "Iteration: 6743 lambda_k: 1 Loss: 0.002929408267093692\n",
      "Iteration: 6744 lambda_k: 1 Loss: 0.0029294082670937296\n",
      "Iteration: 6745 lambda_k: 1 Loss: 0.0029294082670937634\n",
      "Iteration: 6746 lambda_k: 1 Loss: 0.00292940826709381\n",
      "Iteration: 6747 lambda_k: 1 Loss: 0.00292940826709385\n",
      "Iteration: 6748 lambda_k: 1 Loss: 0.0029294082670938814\n",
      "Iteration: 6749 lambda_k: 1 Loss: 0.002929408267093918\n",
      "Iteration: 6750 lambda_k: 1 Loss: 0.002929408267093946\n",
      "Iteration: 6751 lambda_k: 1 Loss: 0.002929408267093983\n",
      "Iteration: 6752 lambda_k: 1 Loss: 0.0029294082670940098\n",
      "Iteration: 6753 lambda_k: 1 Loss: 0.0029294082670940427\n",
      "Iteration: 6754 lambda_k: 1 Loss: 0.0029294082670940796\n",
      "Iteration: 6755 lambda_k: 1 Loss: 0.0029294082670941073\n",
      "Iteration: 6756 lambda_k: 1 Loss: 0.0029294082670941377\n",
      "Iteration: 6757 lambda_k: 1 Loss: 0.0029294082670941655\n",
      "Iteration: 6758 lambda_k: 1 Loss: 0.0029294082670941963\n",
      "Iteration: 6759 lambda_k: 1 Loss: 0.0029294082670942275\n",
      "Iteration: 6760 lambda_k: 1 Loss: 0.0029294082670942587\n",
      "Iteration: 6761 lambda_k: 1 Loss: 0.0029294082670942865\n",
      "Iteration: 6762 lambda_k: 1 Loss: 0.002929408267094314\n",
      "Iteration: 6763 lambda_k: 1 Loss: 0.0029294082670943415\n",
      "Iteration: 6764 lambda_k: 1 Loss: 0.0029294082670943715\n",
      "Iteration: 6765 lambda_k: 1 Loss: 0.0029294082670944005\n",
      "Iteration: 6766 lambda_k: 1 Loss: 0.002929408267094432\n",
      "Iteration: 6767 lambda_k: 1 Loss: 0.002929408267094457\n",
      "Iteration: 6768 lambda_k: 1 Loss: 0.0029294082670944886\n",
      "Iteration: 6769 lambda_k: 1 Loss: 0.0029294082670945133\n",
      "Iteration: 6770 lambda_k: 1 Loss: 0.0029294082670945376\n",
      "Iteration: 6771 lambda_k: 1 Loss: 0.0029294082670945575\n",
      "Iteration: 6772 lambda_k: 1 Loss: 0.0029294082670945835\n",
      "Iteration: 6773 lambda_k: 1 Loss: 0.0029294082670946126\n",
      "Iteration: 6774 lambda_k: 1 Loss: 0.002929408267094642\n",
      "Iteration: 6775 lambda_k: 1 Loss: 0.002929408267094668\n",
      "Iteration: 6776 lambda_k: 1 Loss: 0.002929408267094695\n",
      "Iteration: 6777 lambda_k: 1 Loss: 0.0029294082670947236\n",
      "Iteration: 6778 lambda_k: 1 Loss: 0.0029294082670947557\n",
      "Iteration: 6779 lambda_k: 1 Loss: 0.0029294082670947817\n",
      "Iteration: 6780 lambda_k: 1 Loss: 0.002929408267094809\n",
      "Iteration: 6781 lambda_k: 1 Loss: 0.00292940826709483\n",
      "Iteration: 6782 lambda_k: 1 Loss: 0.002929408267094856\n",
      "Iteration: 6783 lambda_k: 1 Loss: 0.0029294082670948867\n",
      "Iteration: 6784 lambda_k: 1 Loss: 0.0029294082670949188\n",
      "Iteration: 6785 lambda_k: 1 Loss: 0.002929408267094946\n",
      "Iteration: 6786 lambda_k: 1 Loss: 0.002929408267094973\n",
      "Iteration: 6787 lambda_k: 1 Loss: 0.002929408267094996\n",
      "Iteration: 6788 lambda_k: 1 Loss: 0.0029294082670950207\n",
      "Iteration: 6789 lambda_k: 1 Loss: 0.0029294082670950484\n",
      "Iteration: 6790 lambda_k: 1 Loss: 0.002929408267095077\n",
      "Iteration: 6791 lambda_k: 1 Loss: 0.0029294082670951326\n",
      "Iteration: 6792 lambda_k: 1 Loss: 0.0029294082670951534\n",
      "Iteration: 6793 lambda_k: 1 Loss: 0.0029294082670951733\n",
      "Iteration: 6794 lambda_k: 1 Loss: 0.0029294082670951955\n",
      "Iteration: 6795 lambda_k: 1 Loss: 0.0029294082670952267\n",
      "Iteration: 6796 lambda_k: 1 Loss: 0.0029294082670952523\n",
      "Iteration: 6797 lambda_k: 1 Loss: 0.002929408267095281\n",
      "Iteration: 6798 lambda_k: 1 Loss: 0.0029294082670953078\n",
      "Iteration: 6799 lambda_k: 1 Loss: 0.0029294082670953325\n",
      "Iteration: 6800 lambda_k: 1 Loss: 0.002929408267095361\n",
      "Iteration: 6801 lambda_k: 1 Loss: 0.002929408267095392\n",
      "Iteration: 6802 lambda_k: 1 Loss: 0.002929408267095413\n",
      "Iteration: 6803 lambda_k: 1 Loss: 0.0029294082670954322\n",
      "Iteration: 6804 lambda_k: 1 Loss: 0.0029294082670954483\n",
      "Iteration: 6805 lambda_k: 1 Loss: 0.0029294082670954743\n",
      "Iteration: 6806 lambda_k: 1 Loss: 0.0029294082670954986\n",
      "Iteration: 6807 lambda_k: 1 Loss: 0.0029294082670955346\n",
      "Iteration: 6808 lambda_k: 1 Loss: 0.0029294082670955567\n",
      "Iteration: 6809 lambda_k: 1 Loss: 0.0029294082670955728\n",
      "Iteration: 6810 lambda_k: 1 Loss: 0.0029294082670955905\n",
      "Iteration: 6811 lambda_k: 1 Loss: 0.0029294082670956087\n",
      "Iteration: 6812 lambda_k: 1 Loss: 0.0029294082670956296\n",
      "Iteration: 6813 lambda_k: 1 Loss: 0.0029294082670956473\n",
      "Iteration: 6814 lambda_k: 1 Loss: 0.0029294082670956643\n",
      "Iteration: 6815 lambda_k: 1 Loss: 0.0029294082670956833\n",
      "Iteration: 6816 lambda_k: 1 Loss: 0.0029294082670957046\n",
      "Iteration: 6817 lambda_k: 1 Loss: 0.0029294082670957254\n",
      "Iteration: 6818 lambda_k: 1 Loss: 0.002929408267095742\n",
      "Iteration: 6819 lambda_k: 1 Loss: 0.0029294082670957553\n",
      "Iteration: 6820 lambda_k: 1 Loss: 0.002929408267095785\n",
      "Iteration: 6821 lambda_k: 1 Loss: 0.00292940826709582\n",
      "Iteration: 6822 lambda_k: 1 Loss: 0.00292940826709585\n",
      "Iteration: 6823 lambda_k: 1 Loss: 0.002929408267095878\n",
      "Iteration: 6824 lambda_k: 1 Loss: 0.0029294082670958937\n",
      "Iteration: 6825 lambda_k: 1 Loss: 0.002929408267095914\n",
      "Iteration: 6826 lambda_k: 1 Loss: 0.002929408267095931\n",
      "Iteration: 6827 lambda_k: 1 Loss: 0.0029294082670959496\n",
      "Iteration: 6828 lambda_k: 1 Loss: 0.0029294082670959683\n",
      "Iteration: 6829 lambda_k: 1 Loss: 0.002929408267095986\n",
      "Iteration: 6830 lambda_k: 1 Loss: 0.0029294082670960043\n",
      "Iteration: 6831 lambda_k: 1 Loss: 0.0029294082670960264\n",
      "Iteration: 6832 lambda_k: 1 Loss: 0.002929408267096045\n",
      "Iteration: 6833 lambda_k: 1 Loss: 0.002929408267096065\n",
      "Iteration: 6834 lambda_k: 1 Loss: 0.0029294082670960854\n",
      "Iteration: 6835 lambda_k: 1 Loss: 0.0029294082670961023\n",
      "Iteration: 6836 lambda_k: 1 Loss: 0.0029294082670961227\n",
      "Iteration: 6837 lambda_k: 1 Loss: 0.00292940826709614\n",
      "Iteration: 6838 lambda_k: 1 Loss: 0.0029294082670961604\n",
      "Iteration: 6839 lambda_k: 1 Loss: 0.00292940826709618\n",
      "Iteration: 6840 lambda_k: 1 Loss: 0.0029294082670961986\n",
      "Iteration: 6841 lambda_k: 1 Loss: 0.002929408267096218\n",
      "Iteration: 6842 lambda_k: 1 Loss: 0.0029294082670962367\n",
      "Iteration: 6843 lambda_k: 1 Loss: 0.0029294082670962593\n",
      "Iteration: 6844 lambda_k: 1 Loss: 0.0029294082670962758\n",
      "Iteration: 6845 lambda_k: 1 Loss: 0.002929408267096298\n",
      "Iteration: 6846 lambda_k: 1 Loss: 0.0029294082670963148\n",
      "Iteration: 6847 lambda_k: 1 Loss: 0.0029294082670963304\n",
      "Iteration: 6848 lambda_k: 1 Loss: 0.0029294082670963486\n",
      "Iteration: 6849 lambda_k: 1 Loss: 0.00292940826709637\n",
      "Iteration: 6850 lambda_k: 1 Loss: 0.002929408267096388\n",
      "Iteration: 6851 lambda_k: 1 Loss: 0.0029294082670964054\n",
      "Iteration: 6852 lambda_k: 1 Loss: 0.002929408267096426\n",
      "Iteration: 6853 lambda_k: 1 Loss: 0.0029294082670964414\n",
      "Iteration: 6854 lambda_k: 1 Loss: 0.002929408267096454\n",
      "Iteration: 6855 lambda_k: 1 Loss: 0.0029294082670964718\n",
      "Iteration: 6856 lambda_k: 1 Loss: 0.0029294082670964865\n",
      "Iteration: 6857 lambda_k: 1 Loss: 0.0029294082670965047\n",
      "Iteration: 6858 lambda_k: 1 Loss: 0.002929408267096524\n",
      "Iteration: 6859 lambda_k: 1 Loss: 0.0029294082670965403\n",
      "Iteration: 6860 lambda_k: 1 Loss: 0.0029294082670965563\n",
      "Iteration: 6861 lambda_k: 1 Loss: 0.002929408267096572\n",
      "Iteration: 6862 lambda_k: 1 Loss: 0.0029294082670965928\n",
      "Iteration: 6863 lambda_k: 1 Loss: 0.0029294082670966093\n",
      "Iteration: 6864 lambda_k: 1 Loss: 0.002929408267096624\n",
      "Iteration: 6865 lambda_k: 1 Loss: 0.002929408267096645\n",
      "Iteration: 6866 lambda_k: 1 Loss: 0.0029294082670966617\n",
      "Iteration: 6867 lambda_k: 1 Loss: 0.0029294082670966786\n",
      "Iteration: 6868 lambda_k: 1 Loss: 0.0029294082670966943\n",
      "Iteration: 6869 lambda_k: 1 Loss: 0.0029294082670967055\n",
      "Iteration: 6870 lambda_k: 1 Loss: 0.0029294082670967224\n",
      "Iteration: 6871 lambda_k: 1 Loss: 0.0029294082670967385\n",
      "Iteration: 6872 lambda_k: 1 Loss: 0.0029294082670967532\n",
      "Iteration: 6873 lambda_k: 1 Loss: 0.00292940826709677\n",
      "Iteration: 6874 lambda_k: 1 Loss: 0.0029294082670967853\n",
      "Iteration: 6875 lambda_k: 1 Loss: 0.0029294082670967983\n",
      "Iteration: 6876 lambda_k: 1 Loss: 0.0029294082670968135\n",
      "Iteration: 6877 lambda_k: 1 Loss: 0.0029294082670968304\n",
      "Iteration: 6878 lambda_k: 1 Loss: 0.002929408267096847\n",
      "Iteration: 6879 lambda_k: 1 Loss: 0.002929408267096864\n",
      "Iteration: 6880 lambda_k: 1 Loss: 0.0029294082670968794\n",
      "Iteration: 6881 lambda_k: 1 Loss: 0.0029294082670968955\n",
      "Iteration: 6882 lambda_k: 1 Loss: 0.0029294082670969137\n",
      "Iteration: 6883 lambda_k: 1 Loss: 0.0029294082670969297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6884 lambda_k: 1 Loss: 0.002929408267096946\n",
      "Iteration: 6885 lambda_k: 1 Loss: 0.0029294082670969623\n",
      "Iteration: 6886 lambda_k: 1 Loss: 0.0029294082670969783\n",
      "Iteration: 6887 lambda_k: 1 Loss: 0.002929408267096995\n",
      "Iteration: 6888 lambda_k: 1 Loss: 0.002929408267097007\n",
      "Iteration: 6889 lambda_k: 1 Loss: 0.0029294082670970243\n",
      "Iteration: 6890 lambda_k: 1 Loss: 0.002929408267097038\n",
      "Iteration: 6891 lambda_k: 1 Loss: 0.002929408267097051\n",
      "Iteration: 6892 lambda_k: 1 Loss: 0.002929408267097066\n",
      "Iteration: 6893 lambda_k: 1 Loss: 0.0029294082670970807\n",
      "Iteration: 6894 lambda_k: 1 Loss: 0.002929408267097096\n",
      "Iteration: 6895 lambda_k: 1 Loss: 0.00292940826709711\n",
      "Iteration: 6896 lambda_k: 1 Loss: 0.0029294082670971266\n",
      "Iteration: 6897 lambda_k: 1 Loss: 0.002929408267097142\n",
      "Iteration: 6898 lambda_k: 1 Loss: 0.002929408267097155\n",
      "Iteration: 6899 lambda_k: 1 Loss: 0.002929408267097171\n",
      "Iteration: 6900 lambda_k: 1 Loss: 0.0029294082670971943\n",
      "Iteration: 6901 lambda_k: 1 Loss: 0.002929408267097214\n",
      "Iteration: 6902 lambda_k: 1 Loss: 0.002929408267097236\n",
      "Iteration: 6903 lambda_k: 1 Loss: 0.002929408267097259\n",
      "Iteration: 6904 lambda_k: 1 Loss: 0.0029294082670972806\n",
      "Iteration: 6905 lambda_k: 1 Loss: 0.0029294082670973027\n",
      "Iteration: 6906 lambda_k: 1 Loss: 0.00292940826709732\n",
      "Iteration: 6907 lambda_k: 1 Loss: 0.0029294082670973365\n",
      "Iteration: 6908 lambda_k: 1 Loss: 0.002929408267097355\n",
      "Iteration: 6909 lambda_k: 1 Loss: 0.0029294082670973725\n",
      "Iteration: 6910 lambda_k: 1 Loss: 0.002929408267097389\n",
      "Iteration: 6911 lambda_k: 1 Loss: 0.002929408267097425\n",
      "Iteration: 6912 lambda_k: 1 Loss: 0.0029294082670974406\n",
      "Iteration: 6913 lambda_k: 1 Loss: 0.002929408267097453\n",
      "Iteration: 6914 lambda_k: 1 Loss: 0.0029294082670974705\n",
      "Iteration: 6915 lambda_k: 1 Loss: 0.002929408267097488\n",
      "Iteration: 6916 lambda_k: 1 Loss: 0.0029294082670975005\n",
      "Iteration: 6917 lambda_k: 1 Loss: 0.0029294082670975135\n",
      "Iteration: 6918 lambda_k: 1 Loss: 0.0029294082670975282\n",
      "Iteration: 6919 lambda_k: 1 Loss: 0.002929408267097542\n",
      "Iteration: 6920 lambda_k: 1 Loss: 0.002929408267097561\n",
      "Iteration: 6921 lambda_k: 1 Loss: 0.002929408267097579\n",
      "Iteration: 6922 lambda_k: 1 Loss: 0.0029294082670975933\n",
      "Iteration: 6923 lambda_k: 1 Loss: 0.002929408267097614\n",
      "Iteration: 6924 lambda_k: 1 Loss: 0.0029294082670976284\n",
      "Iteration: 6925 lambda_k: 1 Loss: 0.002929408267097642\n",
      "Iteration: 6926 lambda_k: 1 Loss: 0.002929408267097651\n",
      "Iteration: 6927 lambda_k: 1 Loss: 0.00292940826709766\n",
      "Iteration: 6928 lambda_k: 1 Loss: 0.0029294082670976796\n",
      "Iteration: 6929 lambda_k: 1 Loss: 0.0029294082670976917\n",
      "Iteration: 6930 lambda_k: 1 Loss: 0.0029294082670977047\n",
      "Iteration: 6931 lambda_k: 1 Loss: 0.0029294082670977186\n",
      "Iteration: 6932 lambda_k: 1 Loss: 0.0029294082670977294\n",
      "Iteration: 6933 lambda_k: 1 Loss: 0.00292940826709774\n",
      "Iteration: 6934 lambda_k: 1 Loss: 0.0029294082670977563\n",
      "Iteration: 6935 lambda_k: 1 Loss: 0.002929408267097781\n",
      "Iteration: 6936 lambda_k: 1 Loss: 0.0029294082670977967\n",
      "Iteration: 6937 lambda_k: 1 Loss: 0.0029294082670978153\n",
      "Iteration: 6938 lambda_k: 1 Loss: 0.0029294082670978236\n",
      "Iteration: 6939 lambda_k: 1 Loss: 0.0029294082670978387\n",
      "Iteration: 6940 lambda_k: 1 Loss: 0.002929408267097849\n",
      "Iteration: 6941 lambda_k: 1 Loss: 0.0029294082670978587\n",
      "Iteration: 6942 lambda_k: 1 Loss: 0.0029294082670978704\n",
      "Iteration: 6943 lambda_k: 1 Loss: 0.0029294082670978795\n",
      "Iteration: 6944 lambda_k: 1 Loss: 0.0029294082670978908\n",
      "Iteration: 6945 lambda_k: 1 Loss: 0.0029294082670979016\n",
      "Iteration: 6946 lambda_k: 1 Loss: 0.002929408267097909\n",
      "Iteration: 6947 lambda_k: 1 Loss: 0.0029294082670979185\n",
      "Iteration: 6948 lambda_k: 1 Loss: 0.0029294082670979302\n",
      "Iteration: 6949 lambda_k: 1 Loss: 0.0029294082670979415\n",
      "Iteration: 6950 lambda_k: 1 Loss: 0.00292940826709795\n",
      "Iteration: 6951 lambda_k: 1 Loss: 0.0029294082670979615\n",
      "Iteration: 6952 lambda_k: 1 Loss: 0.002929408267097971\n",
      "Iteration: 6953 lambda_k: 1 Loss: 0.0029294082670979797\n",
      "Iteration: 6954 lambda_k: 1 Loss: 0.0029294082670979884\n",
      "Iteration: 6955 lambda_k: 1 Loss: 0.002929408267097998\n",
      "Iteration: 6956 lambda_k: 1 Loss: 0.002929408267098007\n",
      "Iteration: 6957 lambda_k: 1 Loss: 0.0029294082670980287\n",
      "Iteration: 6958 lambda_k: 1 Loss: 0.002929408267098046\n",
      "Iteration: 6959 lambda_k: 1 Loss: 0.00292940826709806\n",
      "Iteration: 6960 lambda_k: 1 Loss: 0.0029294082670980747\n",
      "Iteration: 6961 lambda_k: 1 Loss: 0.0029294082670980894\n",
      "Iteration: 6962 lambda_k: 1 Loss: 0.002929408267098099\n",
      "Iteration: 6963 lambda_k: 1 Loss: 0.002929408267098106\n",
      "Iteration: 6964 lambda_k: 1 Loss: 0.002929408267098119\n",
      "Iteration: 6965 lambda_k: 1 Loss: 0.0029294082670981232\n",
      "Iteration: 6966 lambda_k: 1 Loss: 0.002929408267098132\n",
      "Iteration: 6967 lambda_k: 1 Loss: 0.002929408267098152\n",
      "Iteration: 6968 lambda_k: 1 Loss: 0.002929408267098168\n",
      "Iteration: 6969 lambda_k: 1 Loss: 0.002929408267098185\n",
      "Iteration: 6970 lambda_k: 1 Loss: 0.002929408267098197\n",
      "Iteration: 6971 lambda_k: 1 Loss: 0.002929408267098212\n",
      "Iteration: 6972 lambda_k: 1 Loss: 0.002929408267098228\n",
      "Iteration: 6973 lambda_k: 1 Loss: 0.00292940826709824\n",
      "Iteration: 6974 lambda_k: 1 Loss: 0.0029294082670982564\n",
      "Iteration: 6975 lambda_k: 1 Loss: 0.002929408267098267\n",
      "Iteration: 6976 lambda_k: 1 Loss: 0.002929408267098279\n",
      "Iteration: 6977 lambda_k: 1 Loss: 0.0029294082670982932\n",
      "Iteration: 6978 lambda_k: 1 Loss: 0.0029294082670983036\n",
      "Iteration: 6979 lambda_k: 1 Loss: 0.0029294082670983167\n",
      "Iteration: 6980 lambda_k: 1 Loss: 0.00292940826709833\n",
      "Iteration: 6981 lambda_k: 1 Loss: 0.0029294082670983422\n",
      "Iteration: 6982 lambda_k: 1 Loss: 0.002929408267098347\n",
      "Iteration: 6983 lambda_k: 1 Loss: 0.0029294082670983513\n",
      "Iteration: 6984 lambda_k: 1 Loss: 0.002929408267098364\n",
      "Iteration: 6985 lambda_k: 1 Loss: 0.002929408267098376\n",
      "Iteration: 6986 lambda_k: 1 Loss: 0.0029294082670983865\n",
      "Iteration: 6987 lambda_k: 1 Loss: 0.002929408267098395\n",
      "Iteration: 6988 lambda_k: 1 Loss: 0.002929408267098411\n",
      "Iteration: 6989 lambda_k: 1 Loss: 0.0029294082670984194\n",
      "Iteration: 6990 lambda_k: 1 Loss: 0.002929408267098429\n",
      "Iteration: 6991 lambda_k: 1 Loss: 0.0029294082670984403\n",
      "Iteration: 6992 lambda_k: 1 Loss: 0.002929408267098451\n",
      "Iteration: 6993 lambda_k: 1 Loss: 0.002929408267098462\n",
      "Iteration: 6994 lambda_k: 1 Loss: 0.0029294082670984693\n",
      "Iteration: 6995 lambda_k: 1 Loss: 0.0029294082670984806\n",
      "Iteration: 6996 lambda_k: 1 Loss: 0.002929408267098497\n",
      "Iteration: 6997 lambda_k: 1 Loss: 0.0029294082670985\n",
      "Iteration: 6998 lambda_k: 1 Loss: 0.0029294082670985057\n",
      "Iteration: 6999 lambda_k: 1 Loss: 0.0029294082670985183\n",
      "Iteration: 7000 lambda_k: 1 Loss: 0.002929408267098528\n",
      "Iteration: 7001 lambda_k: 1 Loss: 0.002929408267098543\n",
      "Iteration: 7002 lambda_k: 1 Loss: 0.0029294082670985556\n",
      "Iteration: 7003 lambda_k: 1 Loss: 0.0029294082670985764\n",
      "Iteration: 7004 lambda_k: 1 Loss: 0.0029294082670985955\n",
      "Iteration: 7005 lambda_k: 1 Loss: 0.002929408267098602\n",
      "Iteration: 7006 lambda_k: 1 Loss: 0.002929408267098609\n",
      "Iteration: 7007 lambda_k: 1 Loss: 0.002929408267098613\n",
      "Iteration: 7008 lambda_k: 1 Loss: 0.002929408267098618\n",
      "Iteration: 7009 lambda_k: 1 Loss: 0.0029294082670986254\n",
      "Iteration: 7010 lambda_k: 1 Loss: 0.0029294082670986332\n",
      "Iteration: 7011 lambda_k: 1 Loss: 0.0029294082670986454\n",
      "Iteration: 7012 lambda_k: 1 Loss: 0.002929408267098651\n",
      "Iteration: 7013 lambda_k: 1 Loss: 0.0029294082670986575\n",
      "Iteration: 7014 lambda_k: 1 Loss: 0.0029294082670986658\n",
      "Iteration: 7015 lambda_k: 1 Loss: 0.0029294082670986727\n",
      "Iteration: 7016 lambda_k: 1 Loss: 0.002929408267098681\n",
      "Iteration: 7017 lambda_k: 1 Loss: 0.0029294082670986883\n",
      "Iteration: 7018 lambda_k: 1 Loss: 0.0029294082670986957\n",
      "Iteration: 7019 lambda_k: 1 Loss: 0.0029294082670987035\n",
      "Iteration: 7020 lambda_k: 1 Loss: 0.002929408267098709\n",
      "Iteration: 7021 lambda_k: 1 Loss: 0.002929408267098719\n",
      "Iteration: 7022 lambda_k: 1 Loss: 0.0029294082670987256\n",
      "Iteration: 7023 lambda_k: 1 Loss: 0.0029294082670987326\n",
      "Iteration: 7024 lambda_k: 1 Loss: 0.002929408267098738\n",
      "Iteration: 7025 lambda_k: 1 Loss: 0.002929408267098747\n",
      "Iteration: 7026 lambda_k: 1 Loss: 0.002929408267098754\n",
      "Iteration: 7027 lambda_k: 1 Loss: 0.0029294082670987577\n",
      "Iteration: 7028 lambda_k: 1 Loss: 0.0029294082670987646\n",
      "Iteration: 7029 lambda_k: 1 Loss: 0.0029294082670987738\n",
      "Iteration: 7030 lambda_k: 1 Loss: 0.002929408267098783\n",
      "Iteration: 7031 lambda_k: 1 Loss: 0.0029294082670987894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7032 lambda_k: 1 Loss: 0.002929408267098794\n",
      "Iteration: 7033 lambda_k: 1 Loss: 0.0029294082670987998\n",
      "Iteration: 7034 lambda_k: 1 Loss: 0.0029294082670988037\n",
      "Iteration: 7035 lambda_k: 1 Loss: 0.00292940826709881\n",
      "Iteration: 7036 lambda_k: 1 Loss: 0.0029294082670988137\n",
      "Iteration: 7037 lambda_k: 1 Loss: 0.0029294082670988184\n",
      "Iteration: 7038 lambda_k: 1 Loss: 0.002929408267098827\n",
      "Iteration: 7039 lambda_k: 1 Loss: 0.002929408267098833\n",
      "Iteration: 7040 lambda_k: 1 Loss: 0.0029294082670988466\n",
      "Iteration: 7041 lambda_k: 1 Loss: 0.002929408267098854\n",
      "Iteration: 7042 lambda_k: 1 Loss: 0.0029294082670988605\n",
      "Iteration: 7043 lambda_k: 1 Loss: 0.0029294082670988666\n",
      "Iteration: 7044 lambda_k: 1 Loss: 0.002929408267098872\n",
      "Iteration: 7045 lambda_k: 1 Loss: 0.0029294082670988774\n",
      "Iteration: 7046 lambda_k: 1 Loss: 0.002929408267098881\n",
      "Iteration: 7047 lambda_k: 1 Loss: 0.0029294082670988874\n",
      "Iteration: 7048 lambda_k: 1 Loss: 0.0029294082670988956\n",
      "Iteration: 7049 lambda_k: 1 Loss: 0.002929408267098903\n",
      "Iteration: 7050 lambda_k: 1 Loss: 0.0029294082670989112\n",
      "Iteration: 7051 lambda_k: 1 Loss: 0.0029294082670989164\n",
      "Iteration: 7052 lambda_k: 1 Loss: 0.002929408267098921\n",
      "Iteration: 7053 lambda_k: 1 Loss: 0.002929408267098925\n",
      "Iteration: 7054 lambda_k: 1 Loss: 0.002929408267098932\n",
      "Iteration: 7055 lambda_k: 1 Loss: 0.0029294082670989364\n",
      "Iteration: 7056 lambda_k: 1 Loss: 0.002929408267098942\n",
      "Iteration: 7057 lambda_k: 1 Loss: 0.0029294082670989477\n",
      "Iteration: 7058 lambda_k: 1 Loss: 0.002929408267098956\n",
      "Iteration: 7059 lambda_k: 1 Loss: 0.0029294082670989615\n",
      "Iteration: 7060 lambda_k: 1 Loss: 0.0029294082670989667\n",
      "Iteration: 7061 lambda_k: 1 Loss: 0.002929408267098971\n",
      "Iteration: 7062 lambda_k: 1 Loss: 0.002929408267098973\n",
      "Iteration: 7063 lambda_k: 1 Loss: 0.002929408267098983\n",
      "Iteration: 7064 lambda_k: 1 Loss: 0.002929408267098986\n",
      "Iteration: 7065 lambda_k: 1 Loss: 0.002929408267098992\n",
      "Iteration: 7066 lambda_k: 1 Loss: 0.0029294082670989962\n",
      "Iteration: 7067 lambda_k: 1 Loss: 0.0029294082670990023\n",
      "Iteration: 7068 lambda_k: 1 Loss: 0.002929408267099012\n",
      "Iteration: 7069 lambda_k: 1 Loss: 0.002929408267099015\n",
      "Iteration: 7070 lambda_k: 1 Loss: 0.00292940826709902\n",
      "Iteration: 7071 lambda_k: 1 Loss: 0.002929408267099026\n",
      "Iteration: 7072 lambda_k: 1 Loss: 0.002929408267099032\n",
      "Iteration: 7073 lambda_k: 1 Loss: 0.002929408267099037\n",
      "Iteration: 7074 lambda_k: 1 Loss: 0.00292940826709904\n",
      "Iteration: 7075 lambda_k: 1 Loss: 0.0029294082670990457\n",
      "Iteration: 7076 lambda_k: 1 Loss: 0.0029294082670990496\n",
      "Iteration: 7077 lambda_k: 1 Loss: 0.0029294082670990556\n",
      "Iteration: 7078 lambda_k: 1 Loss: 0.002929408267099059\n",
      "Iteration: 7079 lambda_k: 1 Loss: 0.0029294082670990648\n",
      "Iteration: 7080 lambda_k: 1 Loss: 0.0029294082670990687\n",
      "Iteration: 7081 lambda_k: 1 Loss: 0.002929408267099077\n",
      "Iteration: 7082 lambda_k: 1 Loss: 0.002929408267099082\n",
      "Iteration: 7083 lambda_k: 1 Loss: 0.002929408267099086\n",
      "Iteration: 7084 lambda_k: 1 Loss: 0.0029294082670990908\n",
      "Iteration: 7085 lambda_k: 1 Loss: 0.0029294082670990942\n",
      "Iteration: 7086 lambda_k: 1 Loss: 0.0029294082670990964\n",
      "Iteration: 7087 lambda_k: 1 Loss: 0.002929408267099104\n",
      "Iteration: 7088 lambda_k: 1 Loss: 0.0029294082670991077\n",
      "Iteration: 7089 lambda_k: 1 Loss: 0.002929408267099113\n",
      "Iteration: 7090 lambda_k: 1 Loss: 0.002929408267099116\n",
      "Iteration: 7091 lambda_k: 1 Loss: 0.0029294082670991216\n",
      "Iteration: 7092 lambda_k: 1 Loss: 0.0029294082670991268\n",
      "Iteration: 7093 lambda_k: 1 Loss: 0.0029294082670991354\n",
      "Iteration: 7094 lambda_k: 1 Loss: 0.0029294082670991385\n",
      "Iteration: 7095 lambda_k: 1 Loss: 0.002929408267099144\n",
      "Iteration: 7096 lambda_k: 1 Loss: 0.002929408267099145\n",
      "Iteration: 7097 lambda_k: 1 Loss: 0.0029294082670991485\n",
      "Iteration: 7098 lambda_k: 1 Loss: 0.002929408267099154\n",
      "Iteration: 7099 lambda_k: 1 Loss: 0.002929408267099159\n",
      "Iteration: 7100 lambda_k: 1 Loss: 0.0029294082670991654\n",
      "Iteration: 7101 lambda_k: 1 Loss: 0.002929408267099171\n",
      "Iteration: 7102 lambda_k: 1 Loss: 0.002929408267099179\n",
      "Iteration: 7103 lambda_k: 1 Loss: 0.0029294082670991844\n",
      "Iteration: 7104 lambda_k: 1 Loss: 0.0029294082670991927\n",
      "Iteration: 7105 lambda_k: 1 Loss: 0.002929408267099199\n",
      "Iteration: 7106 lambda_k: 1 Loss: 0.002929408267099206\n",
      "Iteration: 7107 lambda_k: 1 Loss: 0.0029294082670992126\n",
      "Iteration: 7108 lambda_k: 1 Loss: 0.0029294082670992187\n",
      "Iteration: 7109 lambda_k: 1 Loss: 0.002929408267099225\n",
      "Iteration: 7110 lambda_k: 1 Loss: 0.0029294082670992265\n",
      "Iteration: 7111 lambda_k: 1 Loss: 0.002929408267099233\n",
      "Iteration: 7112 lambda_k: 1 Loss: 0.0029294082670992404\n",
      "Iteration: 7113 lambda_k: 1 Loss: 0.0029294082670992443\n",
      "Iteration: 7114 lambda_k: 1 Loss: 0.0029294082670992534\n",
      "Iteration: 7115 lambda_k: 1 Loss: 0.0029294082670992573\n",
      "Iteration: 7116 lambda_k: 1 Loss: 0.002929408267099258\n",
      "Iteration: 7117 lambda_k: 1 Loss: 0.002929408267099262\n",
      "Iteration: 7118 lambda_k: 1 Loss: 0.002929408267099268\n",
      "Iteration: 7119 lambda_k: 1 Loss: 0.002929408267099272\n",
      "Iteration: 7120 lambda_k: 1 Loss: 0.0029294082670992747\n",
      "Iteration: 7121 lambda_k: 1 Loss: 0.0029294082670992777\n",
      "Iteration: 7122 lambda_k: 1 Loss: 0.002929408267099283\n",
      "Iteration: 7123 lambda_k: 1 Loss: 0.0029294082670992872\n",
      "Iteration: 7124 lambda_k: 1 Loss: 0.0029294082670992907\n",
      "Iteration: 7125 lambda_k: 1 Loss: 0.0029294082670992968\n",
      "Iteration: 7126 lambda_k: 1 Loss: 0.002929408267099303\n",
      "Iteration: 7127 lambda_k: 1 Loss: 0.002929408267099309\n",
      "Iteration: 7128 lambda_k: 1 Loss: 0.0029294082670993124\n",
      "Iteration: 7129 lambda_k: 1 Loss: 0.002929408267099321\n",
      "Iteration: 7130 lambda_k: 1 Loss: 0.0029294082670993267\n",
      "Iteration: 7131 lambda_k: 1 Loss: 0.0029294082670993328\n",
      "Iteration: 7132 lambda_k: 1 Loss: 0.002929408267099339\n",
      "Iteration: 7133 lambda_k: 1 Loss: 0.002929408267099343\n",
      "Iteration: 7134 lambda_k: 1 Loss: 0.0029294082670993458\n",
      "Iteration: 7135 lambda_k: 1 Loss: 0.002929408267099351\n",
      "Iteration: 7136 lambda_k: 1 Loss: 0.0029294082670993592\n",
      "Iteration: 7137 lambda_k: 1 Loss: 0.0029294082670993653\n",
      "Iteration: 7138 lambda_k: 1 Loss: 0.002929408267099369\n",
      "Iteration: 7139 lambda_k: 1 Loss: 0.002929408267099374\n",
      "Iteration: 7140 lambda_k: 1 Loss: 0.0029294082670993783\n",
      "Iteration: 7141 lambda_k: 1 Loss: 0.0029294082670993844\n",
      "Iteration: 7142 lambda_k: 1 Loss: 0.002929408267099386\n",
      "Iteration: 7143 lambda_k: 1 Loss: 0.0029294082670993904\n",
      "Iteration: 7144 lambda_k: 1 Loss: 0.002929408267099397\n",
      "Iteration: 7145 lambda_k: 1 Loss: 0.0029294082670994013\n",
      "Iteration: 7146 lambda_k: 1 Loss: 0.002929408267099406\n",
      "Iteration: 7147 lambda_k: 1 Loss: 0.00292940826709941\n",
      "Iteration: 7148 lambda_k: 1 Loss: 0.002929408267099418\n",
      "Iteration: 7149 lambda_k: 1 Loss: 0.002929408267099426\n",
      "Iteration: 7150 lambda_k: 1 Loss: 0.002929408267099433\n",
      "Iteration: 7151 lambda_k: 1 Loss: 0.0029294082670994377\n",
      "Iteration: 7152 lambda_k: 1 Loss: 0.0029294082670994455\n",
      "Iteration: 7153 lambda_k: 1 Loss: 0.0029294082670994473\n",
      "Iteration: 7154 lambda_k: 1 Loss: 0.002929408267099451\n",
      "Iteration: 7155 lambda_k: 1 Loss: 0.0029294082670994564\n",
      "Iteration: 7156 lambda_k: 1 Loss: 0.002929408267099464\n",
      "Iteration: 7157 lambda_k: 1 Loss: 0.002929408267099469\n",
      "Iteration: 7158 lambda_k: 1 Loss: 0.00292940826709947\n",
      "Iteration: 7159 lambda_k: 1 Loss: 0.0029294082670994767\n",
      "Iteration: 7160 lambda_k: 1 Loss: 0.002929408267099483\n",
      "Iteration: 7161 lambda_k: 1 Loss: 0.0029294082670994924\n",
      "Iteration: 7162 lambda_k: 1 Loss: 0.002929408267099496\n",
      "Iteration: 7163 lambda_k: 1 Loss: 0.0029294082670995015\n",
      "Iteration: 7164 lambda_k: 1 Loss: 0.002929408267099505\n",
      "Iteration: 7165 lambda_k: 1 Loss: 0.002929408267099509\n",
      "Iteration: 7166 lambda_k: 1 Loss: 0.0029294082670995145\n",
      "Iteration: 7167 lambda_k: 1 Loss: 0.0029294082670995214\n",
      "Iteration: 7168 lambda_k: 1 Loss: 0.002929408267099527\n",
      "Iteration: 7169 lambda_k: 1 Loss: 0.0029294082670995297\n",
      "Iteration: 7170 lambda_k: 1 Loss: 0.002929408267099536\n",
      "Iteration: 7171 lambda_k: 1 Loss: 0.0029294082670995444\n",
      "Iteration: 7172 lambda_k: 1 Loss: 0.0029294082670995474\n",
      "Iteration: 7173 lambda_k: 1 Loss: 0.002929408267099553\n",
      "Iteration: 7174 lambda_k: 1 Loss: 0.0029294082670995583\n",
      "Iteration: 7175 lambda_k: 1 Loss: 0.0029294082670995618\n",
      "Iteration: 7176 lambda_k: 1 Loss: 0.0029294082670995644\n",
      "Iteration: 7177 lambda_k: 1 Loss: 0.002929408267099567\n",
      "Iteration: 7178 lambda_k: 1 Loss: 0.0029294082670995735\n",
      "Iteration: 7179 lambda_k: 1 Loss: 0.002929408267099577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7180 lambda_k: 1 Loss: 0.002929408267099582\n",
      "Iteration: 7181 lambda_k: 1 Loss: 0.0029294082670995873\n",
      "Iteration: 7182 lambda_k: 1 Loss: 0.002929408267099594\n",
      "Iteration: 7183 lambda_k: 1 Loss: 0.0029294082670996008\n",
      "Iteration: 7184 lambda_k: 1 Loss: 0.0029294082670996043\n",
      "Iteration: 7185 lambda_k: 1 Loss: 0.00292940826709961\n",
      "Iteration: 7186 lambda_k: 1 Loss: 0.002929408267099614\n",
      "Iteration: 7187 lambda_k: 1 Loss: 0.0029294082670996186\n",
      "Iteration: 7188 lambda_k: 1 Loss: 0.002929408267099622\n",
      "Iteration: 7189 lambda_k: 1 Loss: 0.0029294082670996277\n",
      "Iteration: 7190 lambda_k: 1 Loss: 0.0029294082670996363\n",
      "Iteration: 7191 lambda_k: 1 Loss: 0.002929408267099642\n",
      "Iteration: 7192 lambda_k: 1 Loss: 0.002929408267099651\n",
      "Iteration: 7193 lambda_k: 1 Loss: 0.0029294082670996533\n",
      "Iteration: 7194 lambda_k: 1 Loss: 0.0029294082670996567\n",
      "Iteration: 7195 lambda_k: 1 Loss: 0.0029294082670996598\n",
      "Iteration: 7196 lambda_k: 1 Loss: 0.002929408267099664\n",
      "Iteration: 7197 lambda_k: 1 Loss: 0.00292940826709967\n",
      "Iteration: 7198 lambda_k: 1 Loss: 0.0029294082670996736\n",
      "Iteration: 7199 lambda_k: 1 Loss: 0.00292940826709968\n",
      "Iteration: 7200 lambda_k: 1 Loss: 0.0029294082670996853\n",
      "Iteration: 7201 lambda_k: 1 Loss: 0.0029294082670996923\n",
      "Iteration: 7202 lambda_k: 1 Loss: 0.0029294082670996975\n",
      "Iteration: 7203 lambda_k: 1 Loss: 0.0029294082670997066\n",
      "Iteration: 7204 lambda_k: 1 Loss: 0.002929408267099712\n",
      "Iteration: 7205 lambda_k: 1 Loss: 0.0029294082670997166\n",
      "Iteration: 7206 lambda_k: 1 Loss: 0.0029294082670997226\n",
      "Iteration: 7207 lambda_k: 1 Loss: 0.002929408267099725\n",
      "Iteration: 7208 lambda_k: 1 Loss: 0.002929408267099729\n",
      "Iteration: 7209 lambda_k: 1 Loss: 0.0029294082670997352\n",
      "Iteration: 7210 lambda_k: 1 Loss: 0.0029294082670997383\n",
      "Iteration: 7211 lambda_k: 1 Loss: 0.0029294082670997417\n",
      "Iteration: 7212 lambda_k: 1 Loss: 0.0029294082670997456\n",
      "Iteration: 7213 lambda_k: 1 Loss: 0.0029294082670997513\n",
      "Iteration: 7214 lambda_k: 1 Loss: 0.0029294082670997565\n",
      "Iteration: 7215 lambda_k: 1 Loss: 0.002929408267099761\n",
      "Iteration: 7216 lambda_k: 1 Loss: 0.002929408267099765\n",
      "Iteration: 7217 lambda_k: 1 Loss: 0.0029294082670997712\n",
      "Iteration: 7218 lambda_k: 1 Loss: 0.0029294082670997777\n",
      "Iteration: 7219 lambda_k: 1 Loss: 0.0029294082670997842\n",
      "Iteration: 7220 lambda_k: 1 Loss: 0.0029294082670997903\n",
      "Iteration: 7221 lambda_k: 1 Loss: 0.002929408267099797\n",
      "Iteration: 7222 lambda_k: 1 Loss: 0.002929408267099803\n",
      "Iteration: 7223 lambda_k: 1 Loss: 0.002929408267099809\n",
      "Iteration: 7224 lambda_k: 1 Loss: 0.002929408267099814\n",
      "Iteration: 7225 lambda_k: 1 Loss: 0.002929408267099817\n",
      "Iteration: 7226 lambda_k: 1 Loss: 0.0029294082670998428\n",
      "Iteration: 7227 lambda_k: 1 Loss: 0.002929408267099846\n",
      "Iteration: 7228 lambda_k: 1 Loss: 0.0029294082670998514\n",
      "Iteration: 7229 lambda_k: 1 Loss: 0.002929408267099855\n",
      "Iteration: 7230 lambda_k: 1 Loss: 0.002929408267099865\n",
      "Iteration: 7231 lambda_k: 1 Loss: 0.002929408267099871\n",
      "Iteration: 7232 lambda_k: 1 Loss: 0.0029294082670998723\n",
      "Iteration: 7233 lambda_k: 1 Loss: 0.0029294082670998783\n",
      "Iteration: 7234 lambda_k: 1 Loss: 0.0029294082670998835\n",
      "Iteration: 7235 lambda_k: 1 Loss: 0.0029294082670998866\n",
      "Iteration: 7236 lambda_k: 1 Loss: 0.00292940826709989\n",
      "Iteration: 7237 lambda_k: 1 Loss: 0.0029294082670998944\n",
      "Iteration: 7238 lambda_k: 1 Loss: 0.0029294082670998966\n",
      "Iteration: 7239 lambda_k: 1 Loss: 0.0029294082670999057\n",
      "Iteration: 7240 lambda_k: 1 Loss: 0.002929408267099916\n",
      "Iteration: 7241 lambda_k: 1 Loss: 0.002929408267099923\n",
      "Iteration: 7242 lambda_k: 1 Loss: 0.00292940826709993\n",
      "Iteration: 7243 lambda_k: 1 Loss: 0.002929408267099934\n",
      "Iteration: 7244 lambda_k: 1 Loss: 0.0029294082670999425\n",
      "Iteration: 7245 lambda_k: 1 Loss: 0.0029294082670999508\n",
      "Iteration: 7246 lambda_k: 1 Loss: 0.002929408267099957\n",
      "Iteration: 7247 lambda_k: 1 Loss: 0.0029294082670999625\n",
      "Iteration: 7248 lambda_k: 1 Loss: 0.002929408267099965\n",
      "Iteration: 7249 lambda_k: 1 Loss: 0.0029294082670999664\n",
      "Iteration: 7250 lambda_k: 1 Loss: 0.002929408267099968\n",
      "Iteration: 7251 lambda_k: 1 Loss: 0.0029294082670999755\n",
      "Iteration: 7252 lambda_k: 1 Loss: 0.0029294082670999816\n",
      "Iteration: 7253 lambda_k: 1 Loss: 0.0029294082670999863\n",
      "Iteration: 7254 lambda_k: 1 Loss: 0.002929408267099988\n",
      "Iteration: 7255 lambda_k: 1 Loss: 0.002929408267099988\n",
      "Iteration: 7256 lambda_k: 1 Loss: 0.0029294082670999907\n",
      "Iteration: 7257 lambda_k: 1 Loss: 0.002929408267099992\n",
      "Iteration: 7258 lambda_k: 1 Loss: 0.0029294082670999933\n",
      "Iteration: 7259 lambda_k: 1 Loss: 0.0029294082670999907\n",
      "Iteration: 7260 lambda_k: 1 Loss: 0.0029294082670999967\n",
      "Iteration: 7261 lambda_k: 1 Loss: 0.0029294082670999985\n",
      "Iteration: 7262 lambda_k: 1 Loss: 0.002929408267099997\n",
      "Iteration: 7263 lambda_k: 1 Loss: 0.002929408267099998\n",
      "Iteration: 7264 lambda_k: 1 Loss: 0.0029294082670999963\n",
      "Iteration: 7265 lambda_k: 1 Loss: 0.0029294082671\n",
      "Iteration: 7266 lambda_k: 1 Loss: 0.0029294082671000045\n",
      "Iteration: 7267 lambda_k: 1 Loss: 0.002929408267100011\n",
      "Iteration: 7268 lambda_k: 1 Loss: 0.002929408267100013\n",
      "Iteration: 7269 lambda_k: 1 Loss: 0.0029294082671000228\n",
      "Iteration: 7270 lambda_k: 1 Loss: 0.00292940826710003\n",
      "Iteration: 7271 lambda_k: 1 Loss: 0.002929408267100036\n",
      "Iteration: 7272 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7273 lambda_k: 1 Loss: 0.0029294082671000297\n",
      "Iteration: 7274 lambda_k: 1 Loss: 0.0029294082671000254\n",
      "Iteration: 7275 lambda_k: 1 Loss: 0.002929408267100024\n",
      "Iteration: 7276 lambda_k: 1 Loss: 0.0029294082671000353\n",
      "Iteration: 7277 lambda_k: 1 Loss: 0.0029294082671000427\n",
      "Iteration: 7278 lambda_k: 1 Loss: 0.0029294082671000414\n",
      "Iteration: 7279 lambda_k: 1 Loss: 0.0029294082671000414\n",
      "Iteration: 7280 lambda_k: 1 Loss: 0.00292940826710004\n",
      "Iteration: 7281 lambda_k: 1 Loss: 0.0029294082671000483\n",
      "Iteration: 7282 lambda_k: 1 Loss: 0.0029294082671000527\n",
      "Iteration: 7283 lambda_k: 1 Loss: 0.0029294082671000483\n",
      "Iteration: 7284 lambda_k: 1 Loss: 0.002929408267100052\n",
      "Iteration: 7285 lambda_k: 1 Loss: 0.0029294082671000496\n",
      "Iteration: 7286 lambda_k: 1 Loss: 0.002929408267100049\n",
      "Iteration: 7287 lambda_k: 1 Loss: 0.0029294082671000505\n",
      "Iteration: 7288 lambda_k: 1 Loss: 0.0029294082671000496\n",
      "Iteration: 7289 lambda_k: 1 Loss: 0.002929408267100045\n",
      "Iteration: 7290 lambda_k: 1 Loss: 0.0029294082671000423\n",
      "Iteration: 7291 lambda_k: 1 Loss: 0.0029294082671000405\n",
      "Iteration: 7292 lambda_k: 1 Loss: 0.0029294082671000414\n",
      "Iteration: 7293 lambda_k: 1 Loss: 0.0029294082671000405\n",
      "Iteration: 7294 lambda_k: 1 Loss: 0.00292940826710004\n",
      "Iteration: 7295 lambda_k: 1 Loss: 0.0029294082671000414\n",
      "Iteration: 7296 lambda_k: 1 Loss: 0.002929408267100039\n",
      "Iteration: 7297 lambda_k: 1 Loss: 0.0029294082671000405\n",
      "Iteration: 7298 lambda_k: 1 Loss: 0.002929408267100046\n",
      "Iteration: 7299 lambda_k: 1 Loss: 0.0029294082671000436\n",
      "Iteration: 7300 lambda_k: 1 Loss: 0.002929408267100047\n",
      "Iteration: 7301 lambda_k: 1 Loss: 0.0029294082671000488\n",
      "Iteration: 7302 lambda_k: 1 Loss: 0.0029294082671000527\n",
      "Iteration: 7303 lambda_k: 1 Loss: 0.0029294082671000566\n",
      "Iteration: 7304 lambda_k: 1 Loss: 0.0029294082671000596\n",
      "Iteration: 7305 lambda_k: 1 Loss: 0.002929408267100059\n",
      "Iteration: 7306 lambda_k: 1 Loss: 0.002929408267100061\n",
      "Iteration: 7307 lambda_k: 1 Loss: 0.002929408267100064\n",
      "Iteration: 7308 lambda_k: 1 Loss: 0.002929408267100064\n",
      "Iteration: 7309 lambda_k: 1 Loss: 0.002929408267100069\n",
      "Iteration: 7310 lambda_k: 1 Loss: 0.002929408267100069\n",
      "Iteration: 7311 lambda_k: 1 Loss: 0.0029294082671000683\n",
      "Iteration: 7312 lambda_k: 1 Loss: 0.0029294082671000713\n",
      "Iteration: 7313 lambda_k: 1 Loss: 0.0029294082671000696\n",
      "Iteration: 7314 lambda_k: 1 Loss: 0.0029294082671000696\n",
      "Iteration: 7315 lambda_k: 1 Loss: 0.0029294082671000696\n",
      "Iteration: 7316 lambda_k: 1 Loss: 0.0029294082671000622\n",
      "Iteration: 7317 lambda_k: 1 Loss: 0.002929408267100059\n",
      "Iteration: 7318 lambda_k: 1 Loss: 0.002929408267100059\n",
      "Iteration: 7319 lambda_k: 1 Loss: 0.002929408267100069\n",
      "Iteration: 7320 lambda_k: 1 Loss: 0.0029294082671000635\n",
      "Iteration: 7321 lambda_k: 1 Loss: 0.0029294082671000587\n",
      "Iteration: 7322 lambda_k: 1 Loss: 0.002929408267100055\n",
      "Iteration: 7323 lambda_k: 1 Loss: 0.0029294082671000544\n",
      "Iteration: 7324 lambda_k: 1 Loss: 0.002929408267100052\n",
      "Iteration: 7325 lambda_k: 1 Loss: 0.0029294082671000496\n",
      "Iteration: 7326 lambda_k: 1 Loss: 0.002929408267100047\n",
      "Iteration: 7327 lambda_k: 1 Loss: 0.002929408267100043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7328 lambda_k: 1 Loss: 0.002929408267100043\n",
      "Iteration: 7329 lambda_k: 1 Loss: 0.0029294082671000423\n",
      "Iteration: 7330 lambda_k: 1 Loss: 0.0029294082671000405\n",
      "Iteration: 7331 lambda_k: 1 Loss: 0.0029294082671000384\n",
      "Iteration: 7332 lambda_k: 1 Loss: 0.0029294082671000392\n",
      "Iteration: 7333 lambda_k: 1 Loss: 0.0029294082671000366\n",
      "Iteration: 7334 lambda_k: 1 Loss: 0.0029294082671000358\n",
      "Iteration: 7335 lambda_k: 1 Loss: 0.0029294082671000366\n",
      "Iteration: 7336 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7337 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7338 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7339 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7340 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7341 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7342 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7343 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7344 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7345 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7346 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7347 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7348 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7349 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7350 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7351 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7352 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7353 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7354 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7355 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7356 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7357 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7358 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7359 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7360 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7361 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7362 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7363 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7364 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7365 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7366 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7367 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7368 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7369 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7370 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7371 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7372 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7373 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7374 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7375 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7376 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7377 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7378 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7379 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7380 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7381 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7382 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7383 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7384 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7385 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7386 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7387 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7388 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7389 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7390 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7391 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7392 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7393 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7394 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7395 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7396 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7397 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7398 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7399 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7400 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7401 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7402 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7403 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7404 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7405 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7406 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7407 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7408 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7409 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7410 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7411 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7412 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7413 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7414 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7415 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7416 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7417 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7418 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7419 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7420 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7421 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7422 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7423 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7424 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7425 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7426 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7427 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7428 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7429 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7430 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7431 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7432 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7433 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7434 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7435 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7436 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7437 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7438 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7439 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7440 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7441 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7442 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7443 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7444 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7445 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7446 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7447 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7448 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7449 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7450 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7451 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7452 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7453 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7454 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7455 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7456 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7457 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7458 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7459 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7460 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7461 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7462 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7463 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7464 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7465 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7466 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7467 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7468 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7469 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7470 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7471 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7472 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7473 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7474 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7475 lambda_k: 1 Loss: 0.002929408267100031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7476 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7477 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7478 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7479 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7480 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7481 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7482 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7483 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7484 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7485 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7486 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7487 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7488 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7489 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7490 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7491 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7492 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7493 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7494 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7495 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7496 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7497 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7498 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7499 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7500 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7501 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7502 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7503 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7504 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7505 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7506 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7507 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7508 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7509 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7510 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7511 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7512 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7513 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7514 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7515 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7516 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7517 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7518 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7519 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7520 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7521 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7522 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7523 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7524 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7525 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7526 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7527 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7528 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7529 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7530 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7531 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7532 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7533 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7534 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7535 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7536 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7537 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7538 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7539 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7540 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7541 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7542 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7543 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7544 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7545 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7546 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7547 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7548 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7549 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7550 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7551 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7552 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7553 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7554 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7555 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7556 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7557 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7558 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7559 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7560 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7561 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7562 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7563 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7564 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7565 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7566 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7567 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7568 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7569 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7570 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7571 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7572 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7573 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7574 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7575 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7576 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7577 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7578 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7579 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7580 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7581 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7582 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7583 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7584 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7585 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7586 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7587 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7588 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7589 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7590 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7591 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7592 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7593 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7594 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7595 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7596 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7597 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7598 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7599 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7600 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7601 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7602 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7603 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7604 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7605 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7606 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7607 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7608 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7609 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7610 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7611 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7612 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7613 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7614 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7615 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7616 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7617 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7618 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7619 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7620 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7621 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7622 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7623 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7624 lambda_k: 1 Loss: 0.002929408267100032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7625 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7626 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7627 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7628 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7629 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7630 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7631 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7632 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7633 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7634 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7635 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7636 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7637 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7638 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7639 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7640 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7641 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7642 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7643 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7644 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7645 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7646 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7647 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7648 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7649 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7650 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7651 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7652 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7653 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7654 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7655 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7656 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7657 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7658 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7659 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7660 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7661 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7662 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7663 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7664 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7665 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7666 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7667 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7668 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7669 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7670 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7671 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7672 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7673 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7674 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7675 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7676 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7677 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7678 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7679 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7680 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7681 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7682 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7683 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7684 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7685 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7686 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7687 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7688 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7689 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7690 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7691 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7692 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7693 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7694 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7695 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7696 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7697 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7698 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7699 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7700 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7701 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7702 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7703 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7704 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7705 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7706 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7707 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7708 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7709 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7710 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7711 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7712 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7713 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7714 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7715 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7716 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7717 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7718 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7719 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7720 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7721 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7722 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7723 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7724 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7725 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7726 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7727 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7728 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7729 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7730 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7731 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7732 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7733 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7734 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7735 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7736 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7737 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7738 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7739 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7740 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7741 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7742 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7743 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7744 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7745 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7746 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7747 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7748 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7749 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7750 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7751 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7752 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7753 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7754 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7755 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7756 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7757 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7758 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7759 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7760 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7761 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7762 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7763 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7764 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7765 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7766 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7767 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7768 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7769 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7770 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7771 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7772 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7773 lambda_k: 1 Loss: 0.0029294082671000327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7774 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7775 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7776 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7777 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7778 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7779 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7780 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7781 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7782 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7783 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7784 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7785 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7786 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7787 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7788 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7789 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7790 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7791 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7792 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7793 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7794 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7795 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7796 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7797 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7798 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7799 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7800 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7801 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7802 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7803 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7804 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7805 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7806 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7807 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7808 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7809 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7810 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7811 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7812 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7813 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7814 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7815 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7816 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7817 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7818 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7819 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7820 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7821 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7822 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7823 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7824 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7825 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7826 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7827 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7828 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7829 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7830 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7831 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7832 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7833 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7834 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7835 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7836 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7837 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7838 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7839 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7840 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7841 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7842 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7843 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7844 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7845 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7846 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7847 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7848 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7849 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7850 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7851 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7852 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7853 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7854 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7855 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7856 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7857 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7858 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7859 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7860 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7861 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7862 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7863 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7864 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7865 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7866 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7867 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7868 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7869 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7870 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7871 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7872 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7873 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7874 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7875 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7876 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7877 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7878 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7879 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7880 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7881 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7882 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7883 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7884 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7885 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7886 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7887 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7888 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7889 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7890 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7891 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7892 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7893 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7894 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7895 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7896 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7897 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7898 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7899 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7900 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7901 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7902 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7903 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7904 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7905 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7906 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7907 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7908 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7909 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7910 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7911 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7912 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7913 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7914 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7915 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7916 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7917 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7918 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7919 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7920 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7921 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7922 lambda_k: 1 Loss: 0.002929408267100031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7923 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7924 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7925 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7926 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7927 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7928 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7929 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7930 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7931 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7932 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7933 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7934 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7935 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7936 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7937 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7938 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7939 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7940 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7941 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7942 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7943 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7944 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7945 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7946 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7947 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7948 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7949 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7950 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7951 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7952 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7953 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7954 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7955 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7956 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7957 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7958 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7959 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7960 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7961 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7962 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7963 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7964 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7965 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7966 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7967 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7968 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7969 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7970 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7971 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7972 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7973 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7974 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7975 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7976 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7977 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7978 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7979 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7980 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7981 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7982 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7983 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7984 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7985 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7986 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7987 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7988 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7989 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7990 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7991 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7992 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7993 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7994 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7995 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7996 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 7997 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 7998 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 7999 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8000 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8001 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8002 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8003 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8004 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8005 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8006 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8007 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8008 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8009 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8010 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8011 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8012 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8013 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8014 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8015 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8016 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8017 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8018 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8019 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8020 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8021 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8022 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8023 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8024 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8025 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8026 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8027 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8028 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8029 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8030 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8031 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8032 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8033 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8034 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8035 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8036 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8037 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8038 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8039 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8040 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8041 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8042 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8043 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8044 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8045 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8046 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8047 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8048 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8049 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8050 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8051 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8052 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8053 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8054 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8055 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8056 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8057 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8058 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8059 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8060 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8061 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8062 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8063 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8064 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8065 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8066 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8067 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8068 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8069 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8070 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8071 lambda_k: 1 Loss: 0.002929408267100032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8072 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8073 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8074 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8075 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8076 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8077 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8078 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8079 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8080 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8081 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8082 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8083 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8084 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8085 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8086 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8087 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8088 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8089 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8090 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8091 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8092 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8093 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8094 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8095 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8096 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8097 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8098 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8099 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8100 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8101 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8102 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8103 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8104 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8105 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8106 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8107 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8108 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8109 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8110 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8111 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8112 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8113 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8114 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8115 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8116 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8117 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8118 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8119 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8120 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8121 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8122 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8123 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8124 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8125 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8126 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8127 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8128 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8129 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8130 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8131 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8132 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8133 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8134 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8135 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8136 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8137 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8138 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8139 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8140 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8141 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8142 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8143 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8144 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8145 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8146 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8147 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8148 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8149 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8150 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8151 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8152 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8153 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8154 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8155 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8156 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8157 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8158 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8159 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8160 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8161 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8162 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8163 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8164 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8165 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8166 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8167 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8168 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8169 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8170 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8171 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8172 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8173 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8174 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8175 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8176 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8177 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8178 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8179 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8180 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8181 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8182 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8183 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8184 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8185 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8186 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8187 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8188 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8189 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8190 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8191 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8192 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8193 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8194 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8195 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8196 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8197 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8198 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8199 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8200 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8201 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8202 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8203 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8204 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8205 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8206 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8207 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8208 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8209 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8210 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8211 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8212 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8213 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8214 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8215 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8216 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8217 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8218 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8219 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8220 lambda_k: 1 Loss: 0.0029294082671000327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8221 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8222 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8223 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8224 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8225 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8226 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8227 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8228 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8229 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8230 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8231 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8232 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8233 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8234 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8235 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8236 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8237 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8238 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8239 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8240 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8241 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8242 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8243 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8244 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8245 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8246 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8247 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8248 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8249 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8250 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8251 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8252 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8253 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8254 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8255 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8256 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8257 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8258 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8259 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8260 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8261 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8262 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8263 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8264 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8265 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8266 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8267 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8268 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8269 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8270 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8271 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8272 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8273 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8274 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8275 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8276 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8277 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8278 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8279 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8280 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8281 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8282 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8283 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8284 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8285 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8286 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8287 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8288 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8289 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8290 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8291 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8292 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8293 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8294 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8295 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8296 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8297 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8298 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8299 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8300 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8301 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8302 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8303 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8304 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8305 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8306 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8307 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8308 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8309 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8310 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8311 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8312 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8313 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8314 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8315 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8316 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8317 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8318 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8319 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8320 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8321 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8322 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8323 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8324 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8325 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8326 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8327 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8328 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8329 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8330 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8331 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8332 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8333 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8334 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8335 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8336 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8337 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8338 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8339 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8340 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8341 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8342 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8343 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8344 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8345 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8346 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8347 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8348 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8349 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8350 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8351 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8352 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8353 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8354 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8355 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8356 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8357 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8358 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8359 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8360 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8361 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8362 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8363 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8364 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8365 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8366 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8367 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8368 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8369 lambda_k: 1 Loss: 0.002929408267100031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8370 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8371 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8372 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8373 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8374 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8375 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8376 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8377 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8378 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8379 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8380 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8381 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8382 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8383 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8384 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8385 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8386 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8387 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8388 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8389 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8390 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8391 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8392 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8393 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8394 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8395 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8396 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8397 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8398 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8399 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8400 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8401 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8402 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8403 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8404 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8405 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8406 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8407 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8408 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8409 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8410 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8411 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8412 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8413 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8414 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8415 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8416 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8417 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8418 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8419 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8420 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8421 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8422 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8423 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8424 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8425 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8426 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8427 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8428 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8429 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8430 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8431 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8432 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8433 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8434 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8435 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8436 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8437 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8438 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8439 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8440 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8441 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8442 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8443 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8444 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8445 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8446 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8447 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8448 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8449 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8450 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8451 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8452 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8453 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8454 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8455 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8456 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8457 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8458 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8459 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8460 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8461 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8462 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8463 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8464 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8465 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8466 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8467 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8468 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8469 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8470 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8471 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8472 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8473 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8474 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8475 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8476 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8477 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8478 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8479 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8480 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8481 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8482 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8483 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8484 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8485 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8486 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8487 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8488 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8489 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8490 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8491 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8492 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8493 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8494 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8495 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8496 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8497 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8498 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8499 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8500 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8501 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8502 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8503 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8504 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8505 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8506 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8507 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8508 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8509 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8510 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8511 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8512 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8513 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8514 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8515 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8516 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8517 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8518 lambda_k: 1 Loss: 0.002929408267100032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8519 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8520 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8521 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8522 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8523 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8524 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8525 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8526 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8527 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8528 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8529 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8530 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8531 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8532 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8533 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8534 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8535 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8536 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8537 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8538 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8539 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8540 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8541 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8542 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8543 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8544 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8545 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8546 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8547 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8548 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8549 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8550 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8551 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8552 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8553 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8554 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8555 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8556 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8557 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8558 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8559 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8560 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8561 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8562 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8563 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8564 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8565 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8566 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8567 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8568 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8569 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8570 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8571 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8572 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8573 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8574 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8575 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8576 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8577 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8578 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8579 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8580 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8581 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8582 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8583 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8584 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8585 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8586 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8587 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8588 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8589 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8590 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8591 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8592 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8593 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8594 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8595 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8596 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8597 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8598 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8599 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8600 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8601 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8602 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8603 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8604 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8605 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8606 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8607 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8608 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8609 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8610 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8611 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8612 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8613 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8614 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8615 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8616 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8617 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8618 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8619 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8620 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8621 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8622 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8623 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8624 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8625 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8626 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8627 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8628 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8629 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8630 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8631 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8632 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8633 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8634 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8635 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8636 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8637 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8638 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8639 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8640 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8641 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8642 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8643 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8644 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8645 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8646 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8647 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8648 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8649 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8650 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8651 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8652 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8653 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8654 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8655 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8656 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8657 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8658 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8659 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8660 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8661 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8662 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8663 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8664 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8665 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8666 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8667 lambda_k: 1 Loss: 0.0029294082671000327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8668 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8669 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8670 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8671 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8672 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8673 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8674 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8675 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8676 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8677 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8678 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8679 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8680 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8681 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8682 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8683 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8684 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8685 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8686 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8687 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8688 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8689 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8690 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8691 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8692 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8693 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8694 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8695 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8696 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8697 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8698 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8699 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8700 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8701 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8702 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8703 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8704 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8705 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8706 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8707 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8708 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8709 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8710 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8711 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8712 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8713 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8714 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8715 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8716 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8717 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8718 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8719 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8720 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8721 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8722 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8723 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8724 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8725 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8726 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8727 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8728 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8729 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8730 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8731 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8732 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8733 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8734 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8735 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8736 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8737 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8738 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8739 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8740 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8741 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8742 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8743 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8744 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8745 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8746 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8747 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8748 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8749 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8750 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8751 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8752 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8753 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8754 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8755 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8756 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8757 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8758 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8759 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8760 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8761 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8762 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8763 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8764 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8765 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8766 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8767 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8768 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8769 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8770 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8771 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8772 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8773 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8774 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8775 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8776 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8777 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8778 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8779 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8780 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8781 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8782 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8783 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8784 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8785 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8786 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8787 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8788 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8789 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8790 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8791 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8792 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8793 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8794 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8795 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8796 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8797 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8798 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8799 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8800 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8801 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8802 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8803 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8804 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8805 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8806 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8807 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8808 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8809 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8810 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8811 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8812 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8813 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8814 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8815 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8816 lambda_k: 1 Loss: 0.002929408267100031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8817 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8818 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8819 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8820 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8821 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8822 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8823 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8824 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8825 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8826 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8827 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8828 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8829 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8830 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8831 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8832 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8833 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8834 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8835 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8836 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8837 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8838 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8839 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8840 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8841 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8842 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8843 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8844 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8845 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8846 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8847 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8848 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8849 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8850 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8851 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8852 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8853 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8854 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8855 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8856 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8857 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8858 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8859 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8860 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8861 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8862 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8863 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8864 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8865 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8866 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8867 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8868 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8869 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8870 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8871 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8872 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8873 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8874 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8875 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8876 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8877 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8878 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8879 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8880 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8881 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8882 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8883 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8884 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8885 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8886 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8887 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8888 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8889 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8890 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8891 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8892 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8893 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8894 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8895 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8896 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8897 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8898 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8899 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8900 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8901 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8902 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8903 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8904 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8905 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8906 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8907 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8908 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8909 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8910 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8911 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8912 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8913 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8914 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8915 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8916 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8917 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8918 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8919 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8920 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8921 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8922 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8923 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8924 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8925 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8926 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8927 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8928 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8929 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8930 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8931 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8932 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8933 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8934 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8935 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8936 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8937 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8938 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8939 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8940 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8941 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8942 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8943 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8944 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8945 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8946 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8947 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8948 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8949 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8950 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8951 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8952 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8953 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8954 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8955 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8956 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8957 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8958 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8959 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8960 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8961 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8962 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8963 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8964 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8965 lambda_k: 1 Loss: 0.002929408267100032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8966 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8967 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8968 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8969 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8970 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8971 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8972 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8973 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8974 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8975 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8976 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8977 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8978 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8979 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8980 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8981 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8982 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8983 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8984 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8985 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8986 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8987 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8988 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8989 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8990 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8991 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8992 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8993 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8994 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8995 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8996 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 8997 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 8998 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 8999 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9000 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9001 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9002 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9003 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9004 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9005 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9006 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9007 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9008 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9009 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9010 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9011 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9012 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9013 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9014 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9015 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9016 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9017 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9018 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9019 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9020 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9021 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9022 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9023 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9024 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9025 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9026 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9027 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9028 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9029 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9030 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9031 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9032 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9033 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9034 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9035 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9036 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9037 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9038 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9039 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9040 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9041 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9042 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9043 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9044 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9045 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9046 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9047 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9048 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9049 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9050 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9051 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9052 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9053 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9054 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9055 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9056 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9057 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9058 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9059 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9060 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9061 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9062 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9063 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9064 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9065 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9066 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9067 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9068 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9069 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9070 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9071 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9072 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9073 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9074 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9075 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9076 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9077 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9078 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9079 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9080 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9081 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9082 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9083 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9084 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9085 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9086 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9087 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9088 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9089 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9090 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9091 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9092 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9093 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9094 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9095 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9096 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9097 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9098 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9099 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9100 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9101 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9102 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9103 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9104 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9105 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9106 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9107 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9108 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9109 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9110 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9111 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9112 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9113 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9114 lambda_k: 1 Loss: 0.0029294082671000327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9115 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9116 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9117 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9118 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9119 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9120 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9121 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9122 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9123 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9124 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9125 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9126 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9127 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9128 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9129 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9130 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9131 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9132 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9133 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9134 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9135 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9136 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9137 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9138 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9139 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9140 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9141 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9142 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9143 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9144 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9145 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9146 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9147 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9148 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9149 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9150 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9151 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9152 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9153 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9154 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9155 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9156 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9157 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9158 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9159 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9160 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9161 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9162 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9163 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9164 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9165 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9166 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9167 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9168 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9169 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9170 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9171 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9172 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9173 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9174 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9175 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9176 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9177 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9178 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9179 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9180 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9181 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9182 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9183 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9184 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9185 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9186 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9187 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9188 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9189 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9190 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9191 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9192 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9193 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9194 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9195 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9196 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9197 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9198 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9199 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9200 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9201 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9202 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9203 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9204 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9205 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9206 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9207 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9208 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9209 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9210 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9211 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9212 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9213 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9214 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9215 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9216 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9217 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9218 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9219 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9220 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9221 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9222 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9223 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9224 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9225 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9226 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9227 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9228 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9229 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9230 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9231 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9232 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9233 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9234 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9235 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9236 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9237 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9238 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9239 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9240 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9241 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9242 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9243 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9244 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9245 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9246 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9247 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9248 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9249 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9250 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9251 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9252 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9253 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9254 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9255 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9256 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9257 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9258 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9259 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9260 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9261 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9262 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9263 lambda_k: 1 Loss: 0.002929408267100031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9264 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9265 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9266 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9267 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9268 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9269 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9270 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9271 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9272 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9273 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9274 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9275 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9276 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9277 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9278 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9279 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9280 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9281 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9282 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9283 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9284 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9285 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9286 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9287 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9288 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9289 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9290 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9291 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9292 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9293 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9294 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9295 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9296 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9297 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9298 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9299 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9300 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9301 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9302 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9303 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9304 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9305 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9306 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9307 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9308 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9309 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9310 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9311 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9312 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9313 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9314 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9315 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9316 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9317 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9318 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9319 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9320 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9321 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9322 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9323 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9324 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9325 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9326 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9327 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9328 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9329 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9330 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9331 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9332 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9333 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9334 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9335 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9336 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9337 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9338 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9339 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9340 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9341 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9342 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9343 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9344 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9345 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9346 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9347 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9348 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9349 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9350 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9351 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9352 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9353 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9354 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9355 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9356 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9357 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9358 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9359 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9360 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9361 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9362 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9363 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9364 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9365 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9366 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9367 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9368 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9369 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9370 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9371 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9372 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9373 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9374 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9375 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9376 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9377 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9378 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9379 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9380 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9381 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9382 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9383 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9384 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9385 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9386 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9387 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9388 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9389 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9390 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9391 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9392 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9393 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9394 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9395 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9396 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9397 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9398 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9399 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9400 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9401 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9402 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9403 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9404 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9405 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9406 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9407 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9408 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9409 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9410 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9411 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9412 lambda_k: 1 Loss: 0.002929408267100032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9413 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9414 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9415 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9416 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9417 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9418 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9419 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9420 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9421 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9422 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9423 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9424 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9425 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9426 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9427 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9428 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9429 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9430 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9431 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9432 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9433 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9434 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9435 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9436 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9437 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9438 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9439 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9440 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9441 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9442 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9443 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9444 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9445 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9446 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9447 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9448 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9449 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9450 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9451 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9452 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9453 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9454 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9455 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9456 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9457 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9458 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9459 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9460 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9461 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9462 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9463 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9464 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9465 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9466 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9467 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9468 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9469 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9470 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9471 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9472 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9473 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9474 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9475 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9476 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9477 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9478 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9479 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9480 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9481 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9482 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9483 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9484 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9485 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9486 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9487 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9488 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9489 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9490 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9491 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9492 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9493 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9494 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9495 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9496 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9497 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9498 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9499 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9500 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9501 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9502 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9503 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9504 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9505 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9506 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9507 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9508 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9509 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9510 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9511 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9512 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9513 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9514 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9515 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9516 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9517 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9518 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9519 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9520 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9521 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9522 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9523 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9524 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9525 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9526 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9527 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9528 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9529 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9530 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9531 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9532 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9533 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9534 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9535 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9536 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9537 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9538 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9539 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9540 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9541 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9542 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9543 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9544 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9545 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9546 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9547 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9548 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9549 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9550 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9551 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9552 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9553 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9554 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9555 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9556 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9557 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9558 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9559 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9560 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9561 lambda_k: 1 Loss: 0.0029294082671000327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9562 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9563 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9564 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9565 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9566 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9567 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9568 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9569 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9570 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9571 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9572 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9573 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9574 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9575 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9576 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9577 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9578 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9579 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9580 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9581 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9582 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9583 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9584 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9585 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9586 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9587 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9588 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9589 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9590 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9591 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9592 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9593 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9594 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9595 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9596 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9597 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9598 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9599 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9600 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9601 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9602 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9603 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9604 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9605 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9606 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9607 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9608 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9609 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9610 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9611 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9612 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9613 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9614 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9615 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9616 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9617 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9618 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9619 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9620 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9621 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9622 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9623 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9624 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9625 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9626 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9627 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9628 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9629 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9630 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9631 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9632 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9633 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9634 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9635 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9636 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9637 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9638 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9639 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9640 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9641 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9642 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9643 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9644 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9645 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9646 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9647 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9648 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9649 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9650 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9651 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9652 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9653 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9654 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9655 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9656 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9657 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9658 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9659 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9660 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9661 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9662 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9663 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9664 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9665 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9666 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9667 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9668 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9669 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9670 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9671 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9672 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9673 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9674 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9675 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9676 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9677 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9678 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9679 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9680 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9681 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9682 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9683 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9684 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9685 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9686 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9687 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9688 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9689 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9690 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9691 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9692 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9693 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9694 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9695 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9696 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9697 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9698 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9699 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9700 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9701 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9702 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9703 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9704 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9705 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9706 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9707 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9708 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9709 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9710 lambda_k: 1 Loss: 0.002929408267100031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9711 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9712 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9713 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9714 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9715 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9716 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9717 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9718 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9719 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9720 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9721 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9722 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9723 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9724 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9725 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9726 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9727 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9728 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9729 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9730 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9731 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9732 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9733 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9734 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9735 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9736 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9737 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9738 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9739 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9740 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9741 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9742 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9743 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9744 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9745 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9746 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9747 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9748 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9749 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9750 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9751 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9752 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9753 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9754 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9755 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9756 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9757 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9758 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9759 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9760 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9761 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9762 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9763 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9764 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9765 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9766 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9767 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9768 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9769 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9770 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9771 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9772 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9773 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9774 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9775 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9776 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9777 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9778 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9779 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9780 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9781 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9782 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9783 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9784 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9785 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9786 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9787 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9788 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9789 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9790 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9791 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9792 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9793 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9794 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9795 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9796 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9797 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9798 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9799 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9800 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9801 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9802 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9803 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9804 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9805 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9806 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9807 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9808 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9809 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9810 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9811 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9812 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9813 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9814 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9815 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9816 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9817 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9818 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9819 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9820 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9821 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9822 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9823 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9824 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9825 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9826 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9827 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9828 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9829 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9830 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9831 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9832 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9833 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9834 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9835 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9836 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9837 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9838 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9839 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9840 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9841 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9842 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9843 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9844 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9845 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9846 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9847 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9848 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9849 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9850 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9851 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9852 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9853 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9854 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9855 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9856 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9857 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9858 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9859 lambda_k: 1 Loss: 0.002929408267100032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9860 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9861 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9862 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9863 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9864 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9865 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9866 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9867 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9868 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9869 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9870 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9871 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9872 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9873 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9874 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9875 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9876 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9877 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9878 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9879 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9880 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9881 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9882 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9883 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9884 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9885 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9886 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9887 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9888 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9889 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9890 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9891 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9892 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9893 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9894 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9895 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9896 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9897 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9898 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9899 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9900 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9901 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9902 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9903 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9904 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9905 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9906 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9907 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9908 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9909 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9910 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9911 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9912 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9913 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9914 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9915 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9916 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9917 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9918 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9919 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9920 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9921 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9922 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9923 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9924 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9925 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9926 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9927 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9928 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9929 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9930 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9931 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9932 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9933 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9934 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9935 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9936 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9937 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9938 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9939 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9940 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9941 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9942 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9943 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9944 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9945 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9946 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9947 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9948 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9949 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9950 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9951 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9952 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9953 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9954 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9955 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9956 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9957 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9958 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9959 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9960 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9961 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9962 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9963 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9964 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9965 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9966 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9967 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9968 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9969 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9970 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9971 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9972 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9973 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9974 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9975 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9976 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9977 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9978 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9979 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9980 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9981 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9982 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9983 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9984 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9985 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9986 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9987 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9988 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9989 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9990 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9991 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9992 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9993 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9994 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9995 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9996 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 9997 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Iteration: 9998 lambda_k: 1 Loss: 0.002929408267100031\n",
      "Iteration: 9999 lambda_k: 1 Loss: 0.0029294082671000327\n",
      "Iteration: 10000 lambda_k: 1 Loss: 0.002929408267100032\n",
      "Beta: 0.002180216632244036\n",
      "Gamma: 0.004013225500591509\n",
      "Lambda_k: 1\n"
     ]
    }
   ],
   "source": [
    "# DY\n",
    "    \n",
    "dy = time.time()\n",
    "DY_list, DY_f_list, DY_z_list, Dual_DY_list, iterations_DY   = DY.Davis_Yin(N, M, frobenius_norm, Grad_Phi, D, (x1,x2,x3), Sigma)\n",
    "fin = time.time()\n",
    "\n",
    "Times[\"DY\"] = fin - dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6753c5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:\n",
      "10000\n",
      "Primal: (x1,x2,x3)\n",
      " ((array([[1188.6],\n",
      "       [42.2],\n",
      "       [23.2]]), array([[186.6, 704.0, 833.3, 1188.6, 714.3],\n",
      "       [8.9, 30.9, 42.2, 42.2, 42.2],\n",
      "       [4.6, 15.1, 23.2, 19.2, 23.2]]), array([[0.0, 0.0, 101.3, 0.0, 720.4]])), 'infactible')\n",
      "Primal: (xf1,xf2,xf3)\n",
      " ((array([[1188.6],\n",
      "       [42.2],\n",
      "       [23.2]]), array([[186.6, 704.0, 833.3, 1188.6, 714.3],\n",
      "       [8.9, 30.9, 42.2, 42.2, 42.2],\n",
      "       [4.6, 15.1, 23.2, 19.2, 23.2]]), array([[0.0, 0.0, 101.3, 0.0, 720.4]])), 'infactible')\n",
      "Primal: (z1,z2,z3)\n",
      " ((array([[1188.6],\n",
      "       [42.0],\n",
      "       [22.8]]), array([[186.6, 704.0, 833.3, 1188.7, 714.3],\n",
      "       [8.9, 30.9, 42.7, 42.3, 42.4],\n",
      "       [4.6, 15.1, 24.0, 19.2, 24.3]]), array([[0.0, 0.0, 101.3, 0.0, 720.4]])), 'infactible')\n",
      "Dual: (Capacity, Equilibrium)\n",
      " (array([[0.0, 0.0, 0.0, 25.0, 0.0],\n",
      "       [0.0, 0.0, 144.8, 44.7, 60.5],\n",
      "       [0.0, 0.0, 191.1, 0.0, 283.9]]), array([[1865.7, 6336.1, 10000.0, 8445.5, 10000.0]]))\n"
     ]
    }
   ],
   "source": [
    "iter_ = -1\n",
    "print(f\"Iterations:\\n{iterations_DY}\")\n",
    "print(\"Primal: (x1,x2,x3)\\n\",DY_list[iter_])\n",
    "print(\"Primal: (xf1,xf2,xf3)\\n\",DY_f_list[iter_])\n",
    "print(\"Primal: (z1,z2,z3)\\n\",DY_z_list[iter_])\n",
    "print(\"Dual: (Capacity, Equilibrium)\\n\",Dual_DY_list[iter_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "999e17b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta:  0.002180216632244036\n",
      "gamma: 0.003967180688989444\n",
      "Iteration: 1 lambda_n: 1.0440291188065327 Loss: 0.5857033695247924\n",
      "Iteration: 2 lambda_n: 0.963047714892765 Loss: 0.5267123678021739\n",
      "Iteration: 3 lambda_n: 0.9754053431892143 Loss: 0.5123342880606044\n",
      "Iteration: 4 lambda_n: 0.9031145566521263 Loss: 0.5077932200830988\n",
      "Iteration: 5 lambda_n: 1.0494541794608783 Loss: 0.5052207434187161\n",
      "Iteration: 6 lambda_n: 0.9739230043928029 Loss: 0.5023578842016584\n",
      "Iteration: 7 lambda_n: 0.9146546345374871 Loss: 0.4998399240842123\n",
      "Iteration: 8 lambda_n: 0.9876121702810646 Loss: 0.4976043921156483\n",
      "Iteration: 9 lambda_n: 0.959710769976669 Loss: 0.4953228315637964\n",
      "Iteration: 10 lambda_n: 0.9634539832171126 Loss: 0.4932212282494741\n",
      "Iteration: 11 lambda_n: 0.9776500785495688 Loss: 0.4912038790927915\n",
      "Iteration: 12 lambda_n: 0.9832650407298472 Loss: 0.4892380104148504\n",
      "Iteration: 13 lambda_n: 0.9015474370574881 Loss: 0.48733326242092323\n",
      "Iteration: 14 lambda_n: 0.9861808068066328 Loss: 0.4856445558999308\n",
      "Iteration: 15 lambda_n: 1.036783732795174 Loss: 0.4838502313328168\n",
      "Iteration: 16 lambda_n: 0.9630064303506111 Loss: 0.48201723777392935\n",
      "Iteration: 17 lambda_n: 0.9471693061931102 Loss: 0.4803593250072657\n",
      "Iteration: 18 lambda_n: 0.9420055623338872 Loss: 0.47876472579952856\n",
      "Iteration: 19 lambda_n: 0.9446259907964604 Loss: 0.4772099676934178\n",
      "Iteration: 20 lambda_n: 0.938461201479724 Loss: 0.47567839944943924\n",
      "Iteration: 21 lambda_n: 0.9031588024698219 Loss: 0.4741811085000012\n",
      "Iteration: 22 lambda_n: 1.0303071509060433 Loss: 0.47276071910811623\n",
      "Iteration: 23 lambda_n: 0.9068805389236257 Loss: 0.4711613114085409\n",
      "Iteration: 24 lambda_n: 0.9883032261061926 Loss: 0.4697716322074101\n",
      "Iteration: 25 lambda_n: 0.9124343006808118 Loss: 0.4682735868057902\n",
      "Iteration: 26 lambda_n: 0.9511563785526711 Loss: 0.4669053346254161\n",
      "Iteration: 27 lambda_n: 1.0391154243003031 Loss: 0.46549264328509843\n",
      "Iteration: 28 lambda_n: 0.9698541339687802 Loss: 0.4639643021667679\n",
      "Iteration: 29 lambda_n: 0.9558184038733929 Loss: 0.46255220107448497\n",
      "Iteration: 30 lambda_n: 0.9040182586165358 Loss: 0.4611738307713088\n",
      "Iteration: 31 lambda_n: 0.9310998610264469 Loss: 0.46002676524514086\n",
      "Iteration: 32 lambda_n: 0.9288352300835647 Loss: 0.4588810050965261\n",
      "Iteration: 33 lambda_n: 0.896534752677724 Loss: 0.4577548836315769\n",
      "Iteration: 34 lambda_n: 0.958524081032588 Loss: 0.45668119454423883\n",
      "Iteration: 35 lambda_n: 0.9997874643270528 Loss: 0.45554583935579884\n",
      "Iteration: 36 lambda_n: 0.9921806618363662 Loss: 0.4543746013547056\n",
      "Iteration: 37 lambda_n: 0.9371880500080332 Loss: 0.45322472847009565\n",
      "Iteration: 38 lambda_n: 0.9381757820939702 Loss: 0.4521494141603434\n",
      "Iteration: 39 lambda_n: 0.9710206806411044 Loss: 0.4510825258496144\n",
      "Iteration: 40 lambda_n: 0.9168081940471084 Loss: 0.4499875859433337\n",
      "Iteration: 41 lambda_n: 1.0318035273020978 Loss: 0.44896230453495795\n",
      "Iteration: 42 lambda_n: 0.9017372024557134 Loss: 0.44781697624197564\n",
      "Iteration: 43 lambda_n: 0.9522965922337923 Loss: 0.4468239415422011\n",
      "Iteration: 44 lambda_n: 0.9987142788604755 Loss: 0.44578212946440915\n",
      "Iteration: 45 lambda_n: 0.9965666415343172 Loss: 0.444696796831971\n",
      "Iteration: 46 lambda_n: 0.9613766032171442 Loss: 0.4436210041279137\n",
      "Iteration: 47 lambda_n: 1.045014248626054 Loss: 0.44258977731041615\n",
      "Iteration: 48 lambda_n: 0.9026619779395847 Loss: 0.4414754039509772\n",
      "Iteration: 49 lambda_n: 0.9096653711176949 Loss: 0.4405187003433439\n",
      "Iteration: 50 lambda_n: 1.0426084246789726 Loss: 0.4395602691864133\n",
      "Iteration: 51 lambda_n: 0.930066144770122 Loss: 0.438468685829303\n",
      "Iteration: 52 lambda_n: 0.9725351285520066 Loss: 0.4375005086685536\n",
      "Iteration: 53 lambda_n: 0.9614976872888588 Loss: 0.4364928303782085\n",
      "Iteration: 54 lambda_n: 1.0020431319365308 Loss: 0.4355012418245071\n",
      "Iteration: 55 lambda_n: 0.987759064447233 Loss: 0.43447244451244227\n",
      "Iteration: 56 lambda_n: 1.0170572315968873 Loss: 0.4334628612397297\n",
      "Iteration: 57 lambda_n: 0.9580802700801659 Loss: 0.4324277772266182\n",
      "Iteration: 58 lambda_n: 0.9485611067993878 Loss: 0.4314568681300087\n",
      "Iteration: 59 lambda_n: 0.9813059365180372 Loss: 0.4304993465722083\n",
      "Iteration: 60 lambda_n: 0.9656761412938792 Loss: 0.42951248576423395\n",
      "Iteration: 61 lambda_n: 0.9395699113323909 Loss: 0.42854501277910234\n",
      "Iteration: 62 lambda_n: 0.9780833787737879 Loss: 0.42760710771814864\n",
      "Iteration: 63 lambda_n: 0.9486803884208163 Loss: 0.42663412552275665\n",
      "Iteration: 64 lambda_n: 0.8928335818262491 Loss: 0.4256937079778\n",
      "Iteration: 65 lambda_n: 1.037894176637984 Loss: 0.424811603179866\n",
      "Iteration: 66 lambda_n: 1.036119204245175 Loss: 0.42378934686309905\n",
      "Iteration: 67 lambda_n: 0.9001164792811318 Loss: 0.4227724334658522\n",
      "Iteration: 68 lambda_n: 0.9240236775183649 Loss: 0.42189204699808913\n",
      "Iteration: 69 lambda_n: 0.9629730031173045 Loss: 0.4209909413783804\n",
      "Iteration: 70 lambda_n: 1.0037920862561658 Loss: 0.4200546544080655\n",
      "Iteration: 71 lambda_n: 0.9589229481580986 Loss: 0.4190816726114854\n",
      "Iteration: 72 lambda_n: 0.9469350580623841 Loss: 0.41815511141370315\n",
      "Iteration: 73 lambda_n: 1.0243237728600045 Loss: 0.417242852034675\n",
      "Iteration: 74 lambda_n: 0.9291619478881747 Loss: 0.4162589013623001\n",
      "Iteration: 75 lambda_n: 1.000308017118252 Loss: 0.41536912766857864\n",
      "Iteration: 76 lambda_n: 1.0202310369886416 Loss: 0.41441389079255125\n",
      "Iteration: 77 lambda_n: 1.0388542290356075 Loss: 0.4134425192351351\n",
      "Iteration: 78 lambda_n: 0.9519854760957337 Loss: 0.4124563805406113\n",
      "Iteration: 79 lambda_n: 0.9756125867146538 Loss: 0.41155543198514244\n",
      "Iteration: 80 lambda_n: 1.0312178889428927 Loss: 0.410634658890917\n",
      "Iteration: 81 lambda_n: 0.9134478909692889 Loss: 0.40966412585899414\n",
      "Iteration: 82 lambda_n: 0.9861858601376988 Loss: 0.4088069491440871\n",
      "Iteration: 83 lambda_n: 1.0083912322724513 Loss: 0.40788390278722847\n",
      "Iteration: 84 lambda_n: 0.9113059156385369 Loss: 0.4069426846381821\n",
      "Iteration: 85 lambda_n: 0.961665155425271 Loss: 0.40609447434241\n",
      "Iteration: 86 lambda_n: 0.8978441652857229 Loss: 0.40520165474204806\n",
      "Iteration: 87 lambda_n: 1.0488498261796557 Loss: 0.40437029806100516\n",
      "Iteration: 88 lambda_n: 1.0416718812324153 Loss: 0.40340151729965057\n",
      "Iteration: 89 lambda_n: 0.9875953492705474 Loss: 0.40244212694365666\n",
      "Iteration: 90 lambda_n: 1.0261545289804392 Loss: 0.40153512054136203\n",
      "Iteration: 91 lambda_n: 0.9544207138872531 Loss: 0.40059522657872354\n",
      "Iteration: 92 lambda_n: 0.9309883280198312 Loss: 0.3997234585780361\n",
      "Iteration: 93 lambda_n: 0.9303542812604693 Loss: 0.39887527852409044\n",
      "Iteration: 94 lambda_n: 1.0001536643606195 Loss: 0.39802979442401015\n",
      "Iteration: 95 lambda_n: 0.9572867896553267 Loss: 0.3971231437589301\n",
      "Iteration: 96 lambda_n: 0.9140912608916019 Loss: 0.3962576688364961\n",
      "Iteration: 97 lambda_n: 0.9608129337108031 Loss: 0.39543335256134327\n",
      "Iteration: 98 lambda_n: 0.9562427610398294 Loss: 0.39456900879395435\n",
      "Iteration: 99 lambda_n: 0.9601802156532911 Loss: 0.39371096784126575\n",
      "Iteration: 100 lambda_n: 0.918701283888906 Loss: 0.39285157407251003\n",
      "Iteration: 101 lambda_n: 0.9272745888500945 Loss: 0.3920313898030967\n",
      "Iteration: 102 lambda_n: 0.961994867858051 Loss: 0.39120555709048604\n",
      "Iteration: 103 lambda_n: 0.9912078728828013 Loss: 0.39035089469916406\n",
      "Iteration: 104 lambda_n: 0.9747252854673762 Loss: 0.3894725063096332\n",
      "Iteration: 105 lambda_n: 1.025682719135139 Loss: 0.38861097154546465\n",
      "Iteration: 106 lambda_n: 0.9414258934911446 Loss: 0.38770671451643823\n",
      "Iteration: 107 lambda_n: 0.9157979528317699 Loss: 0.3868789664190513\n",
      "Iteration: 108 lambda_n: 0.9512653405631597 Loss: 0.38607573336385487\n",
      "Iteration: 109 lambda_n: 0.9719942991189383 Loss: 0.3852433891753009\n",
      "Iteration: 110 lambda_n: 0.936658716797643 Loss: 0.3843950193199737\n",
      "Iteration: 111 lambda_n: 0.9190412139069101 Loss: 0.3835795619695037\n",
      "Iteration: 112 lambda_n: 0.9380309126653059 Loss: 0.3827813944631088\n",
      "Iteration: 113 lambda_n: 0.9277276131193235 Loss: 0.38196868420709884\n",
      "Iteration: 114 lambda_n: 1.0021496217395691 Loss: 0.3811668617861658\n",
      "Iteration: 115 lambda_n: 1.0258795274718693 Loss: 0.3803028080353926\n",
      "Iteration: 116 lambda_n: 0.9285439198432296 Loss: 0.3794205977273418\n",
      "Iteration: 117 lambda_n: 0.9754584594447964 Loss: 0.3786242162673967\n",
      "Iteration: 118 lambda_n: 0.9048948384858819 Loss: 0.37778961449401965\n",
      "Iteration: 119 lambda_n: 1.0249697529863848 Loss: 0.37701734398721304\n",
      "Iteration: 120 lambda_n: 0.9298352506737181 Loss: 0.3761446518597787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 121 lambda_n: 0.9928347572364766 Loss: 0.37535506069386065\n",
      "Iteration: 122 lambda_n: 0.9096800890620933 Loss: 0.37451400378872235\n",
      "Iteration: 123 lambda_n: 1.0489879789905696 Loss: 0.37374536836006583\n",
      "Iteration: 124 lambda_n: 0.9523577892584519 Loss: 0.37286111459543914\n",
      "Iteration: 125 lambda_n: 0.9803541542722759 Loss: 0.3720604925743457\n",
      "Iteration: 126 lambda_n: 1.0435680450963587 Loss: 0.3712383652734838\n",
      "Iteration: 127 lambda_n: 0.920275533071056 Loss: 0.37036544637520896\n",
      "Iteration: 128 lambda_n: 0.9955861298296778 Loss: 0.3695977323156758\n",
      "Iteration: 129 lambda_n: 1.0280657199572731 Loss: 0.3687691692936833\n",
      "Iteration: 130 lambda_n: 0.9038155034882471 Loss: 0.3679157769745917\n",
      "Iteration: 131 lambda_n: 1.029721780348674 Loss: 0.3671675136903467\n",
      "Iteration: 132 lambda_n: 1.0375539356714911 Loss: 0.3663170060829769\n",
      "Iteration: 133 lambda_n: 1.0422195893818573 Loss: 0.3654623079688296\n",
      "Iteration: 134 lambda_n: 0.9082094852332421 Loss: 0.36460606600155976\n",
      "Iteration: 135 lambda_n: 1.033530116769434 Loss: 0.3638619250569321\n",
      "Iteration: 136 lambda_n: 1.0195262472345366 Loss: 0.36301709163764584\n",
      "Iteration: 137 lambda_n: 0.9556406616267751 Loss: 0.3621859494026601\n",
      "Iteration: 138 lambda_n: 0.8946287620772055 Loss: 0.36140894319969386\n",
      "Iteration: 139 lambda_n: 0.9767044307678976 Loss: 0.3606833385765409\n",
      "Iteration: 140 lambda_n: 0.9920931026009493 Loss: 0.3598929958624089\n",
      "Iteration: 141 lambda_n: 1.0064566826006465 Loss: 0.3590922233198134\n",
      "Iteration: 142 lambda_n: 0.9405496742432127 Loss: 0.35828193473603215\n",
      "Iteration: 143 lambda_n: 1.0409586187903197 Loss: 0.3575266688696613\n",
      "Iteration: 144 lambda_n: 1.0309119099741726 Loss: 0.3566928004039316\n",
      "Iteration: 145 lambda_n: 0.9742936920971756 Loss: 0.3558691912714078\n",
      "Iteration: 146 lambda_n: 0.944206551159794 Loss: 0.35509287693990543\n",
      "Iteration: 147 lambda_n: 0.9259044249150363 Loss: 0.3543424185927768\n",
      "Iteration: 148 lambda_n: 0.9061586752682327 Loss: 0.35360829063277055\n",
      "Iteration: 149 lambda_n: 0.8967401939151577 Loss: 0.3528915253399004\n",
      "Iteration: 150 lambda_n: 0.8933469919558885 Loss: 0.3521838582975654\n",
      "Iteration: 151 lambda_n: 1.0166648757035324 Loss: 0.3514804894520535\n",
      "Iteration: 152 lambda_n: 0.9729034660694763 Loss: 0.350681862285665\n",
      "Iteration: 153 lambda_n: 0.9663011813592989 Loss: 0.3499195997996474\n",
      "Iteration: 154 lambda_n: 1.0221697619456327 Loss: 0.3491643948719385\n",
      "Iteration: 155 lambda_n: 1.0276426826341125 Loss: 0.3483675018785053\n",
      "Iteration: 156 lambda_n: 0.939995664801313 Loss: 0.3475684354600499\n",
      "Iteration: 157 lambda_n: 0.9568971149948489 Loss: 0.34683943773320736\n",
      "Iteration: 158 lambda_n: 0.9828798142668519 Loss: 0.346099113639674\n",
      "Iteration: 159 lambda_n: 1.0205944961151212 Loss: 0.34534054473279896\n",
      "Iteration: 160 lambda_n: 0.9004890902173415 Loss: 0.34455484356190974\n",
      "Iteration: 161 lambda_n: 0.9046437239829691 Loss: 0.3438634059008453\n",
      "Iteration: 162 lambda_n: 0.9971750907378288 Loss: 0.3431703715672716\n",
      "Iteration: 163 lambda_n: 1.03878488474631 Loss: 0.3424082119831359\n",
      "Iteration: 164 lambda_n: 0.978483902258973 Loss: 0.3416162650262591\n",
      "Iteration: 165 lambda_n: 0.9537316465942707 Loss: 0.34087225973755814\n",
      "Iteration: 166 lambda_n: 0.9225114551281592 Loss: 0.3401488783760665\n",
      "Iteration: 167 lambda_n: 1.0203157251730324 Loss: 0.3394508714816484\n",
      "Iteration: 168 lambda_n: 0.9746846443005559 Loss: 0.33868067299998633\n",
      "Iteration: 169 lambda_n: 1.0495580987156041 Loss: 0.33794682400815484\n",
      "Iteration: 170 lambda_n: 0.9068254015989038 Loss: 0.3371585577552433\n",
      "Iteration: 171 lambda_n: 0.9234803981959538 Loss: 0.3364792996012767\n",
      "Iteration: 172 lambda_n: 0.953789323730729 Loss: 0.3357891558922066\n",
      "Iteration: 173 lambda_n: 1.0077034068642228 Loss: 0.3350780293345953\n",
      "Iteration: 174 lambda_n: 0.9291499463201431 Loss: 0.3343285209187032\n",
      "Iteration: 175 lambda_n: 0.9606678212275936 Loss: 0.33363919931941693\n",
      "Iteration: 176 lambda_n: 0.9059027837862859 Loss: 0.332928170565465\n",
      "Iteration: 177 lambda_n: 1.0139079665898016 Loss: 0.3322593026694051\n",
      "Iteration: 178 lambda_n: 0.9887040657397813 Loss: 0.3315124058423396\n",
      "Iteration: 179 lambda_n: 0.9369164829636436 Loss: 0.33078593985863747\n",
      "Iteration: 180 lambda_n: 0.9353756006534808 Loss: 0.3300992425102829\n",
      "Iteration: 181 lambda_n: 0.9096528882483271 Loss: 0.32941529500364586\n",
      "Iteration: 182 lambda_n: 1.0373291740485941 Loss: 0.3287517243188762\n",
      "Iteration: 183 lambda_n: 0.9790349941942063 Loss: 0.32799675489650565\n",
      "Iteration: 184 lambda_n: 1.045022183483396 Loss: 0.3272860729200441\n",
      "Iteration: 185 lambda_n: 0.969947715439557 Loss: 0.32652936263632953\n",
      "Iteration: 186 lambda_n: 0.9014523307195162 Loss: 0.32582886011588547\n",
      "Iteration: 187 lambda_n: 0.9865050674910161 Loss: 0.3251794125102507\n",
      "Iteration: 188 lambda_n: 1.000870688635663 Loss: 0.3244703020113768\n",
      "Iteration: 189 lambda_n: 0.9614950470688609 Loss: 0.3237526495389302\n",
      "Iteration: 190 lambda_n: 0.9084115092237738 Loss: 0.3230649630997405\n",
      "Iteration: 191 lambda_n: 0.9953546907938994 Loss: 0.3224168108955793\n",
      "Iteration: 192 lambda_n: 0.929645045173512 Loss: 0.32170824598707176\n",
      "Iteration: 193 lambda_n: 0.9227401493616771 Loss: 0.3210481090599597\n",
      "Iteration: 194 lambda_n: 0.9454113868434629 Loss: 0.3203944026201429\n",
      "Iteration: 195 lambda_n: 0.9946964051747739 Loss: 0.3197261845937941\n",
      "Iteration: 196 lambda_n: 0.9031955363304928 Loss: 0.31902479838304865\n",
      "Iteration: 197 lambda_n: 0.9198857469736932 Loss: 0.31838951634250695\n",
      "Iteration: 198 lambda_n: 0.9215354236229268 Loss: 0.31774395813848233\n",
      "Iteration: 199 lambda_n: 0.9923800228484722 Loss: 0.31709873086522705\n",
      "Iteration: 200 lambda_n: 0.8957710412749954 Loss: 0.3164055039135075\n",
      "Iteration: 201 lambda_n: 1.0400497034761498 Loss: 0.31578131360975736\n",
      "Iteration: 202 lambda_n: 1.0355081815196916 Loss: 0.315058213212021\n",
      "Iteration: 203 lambda_n: 0.9994980699856035 Loss: 0.31434014074823907\n",
      "Iteration: 204 lambda_n: 0.9185418798408613 Loss: 0.3136488307008935\n",
      "Iteration: 205 lambda_n: 0.9471543363302494 Loss: 0.3130150976169223\n",
      "Iteration: 206 lambda_n: 0.9889595157005087 Loss: 0.312363121945657\n",
      "Iteration: 207 lambda_n: 0.8983093203336244 Loss: 0.31168397852706775\n",
      "Iteration: 208 lambda_n: 1.013419738685122 Loss: 0.3110686056657072\n",
      "Iteration: 209 lambda_n: 0.9134584456209218 Loss: 0.31037593497248245\n",
      "Iteration: 210 lambda_n: 1.0252895522690804 Loss: 0.3097531613850791\n",
      "Iteration: 211 lambda_n: 1.0092112679238576 Loss: 0.30905573666375985\n",
      "Iteration: 212 lambda_n: 0.9674903125496966 Loss: 0.3083710000842998\n",
      "Iteration: 213 lambda_n: 0.9920112368891568 Loss: 0.3077162176961744\n",
      "Iteration: 214 lambda_n: 0.8992804294647296 Loss: 0.3070464557884583\n",
      "Iteration: 215 lambda_n: 1.037828637879488 Loss: 0.306440796702685\n",
      "Iteration: 216 lambda_n: 0.9109457085657109 Loss: 0.30574339181303406\n",
      "Iteration: 217 lambda_n: 0.9641910341138736 Loss: 0.3051328254281906\n",
      "Iteration: 218 lambda_n: 0.9185468306338644 Loss: 0.3044880340401277\n",
      "Iteration: 219 lambda_n: 0.9318496516458545 Loss: 0.30387523564897095\n",
      "Iteration: 220 lambda_n: 0.9996098565095928 Loss: 0.3032549795391796\n",
      "Iteration: 221 lambda_n: 1.0473730001793395 Loss: 0.30259116051087726\n",
      "Iteration: 222 lambda_n: 0.9178971675873165 Loss: 0.30189734814988234\n",
      "Iteration: 223 lambda_n: 1.026634311049989 Loss: 0.3012908800023365\n",
      "Iteration: 224 lambda_n: 1.0480042243228365 Loss: 0.3006141130540413\n",
      "Iteration: 225 lambda_n: 0.9927522082469458 Loss: 0.2999250160940679\n",
      "Iteration: 226 lambda_n: 0.9987394791572671 Loss: 0.2992739414799755\n",
      "Iteration: 227 lambda_n: 0.9025403335428877 Loss: 0.2986205497602466\n",
      "Iteration: 228 lambda_n: 0.9564075260263063 Loss: 0.29803154987089187\n",
      "Iteration: 229 lambda_n: 0.9008966164938285 Loss: 0.29740879072953574\n",
      "Iteration: 230 lambda_n: 0.9452759917383127 Loss: 0.2968235630974073\n",
      "Iteration: 231 lambda_n: 1.0183557333757773 Loss: 0.29621087486813424\n",
      "Iteration: 232 lambda_n: 1.0080538792475704 Loss: 0.2955523631916209\n",
      "Iteration: 233 lambda_n: 0.9371419327315096 Loss: 0.2949021527566368\n",
      "Iteration: 234 lambda_n: 0.95181865702781 Loss: 0.29429918472312383\n",
      "Iteration: 235 lambda_n: 0.9325584460504178 Loss: 0.29368819082792125\n",
      "Iteration: 236 lambda_n: 0.9511767458748438 Loss: 0.29309096638863474\n",
      "Iteration: 237 lambda_n: 1.0138001386102484 Loss: 0.2924832207358535\n",
      "Iteration: 238 lambda_n: 1.0103977176004124 Loss: 0.2918369841409494\n",
      "Iteration: 239 lambda_n: 0.9988843181279334 Loss: 0.2911945267313816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 240 lambda_n: 1.0041061158408162 Loss: 0.29056097212743487\n",
      "Iteration: 241 lambda_n: 1.0036102055359946 Loss: 0.2899256738890362\n",
      "Iteration: 242 lambda_n: 0.9204559230930157 Loss: 0.2892922607829533\n",
      "Iteration: 243 lambda_n: 0.8985741624655195 Loss: 0.2887127636713051\n",
      "Iteration: 244 lambda_n: 1.0067455047241802 Loss: 0.28814832499932164\n",
      "Iteration: 245 lambda_n: 0.9573526689299104 Loss: 0.28751734121250166\n",
      "Iteration: 246 lambda_n: 1.0037090684439065 Loss: 0.28691880092677274\n",
      "Iteration: 247 lambda_n: 1.0013107779029307 Loss: 0.2862927589211031\n",
      "Iteration: 248 lambda_n: 0.9795718924233687 Loss: 0.2856697559009718\n",
      "Iteration: 249 lambda_n: 0.9071897259043175 Loss: 0.285061780029471\n",
      "Iteration: 250 lambda_n: 0.8925155399043679 Loss: 0.2845000839222028\n",
      "Iteration: 251 lambda_n: 1.0494170091699087 Loss: 0.2839487066643416\n",
      "Iteration: 252 lambda_n: 0.9786300698680206 Loss: 0.2833018274752012\n",
      "Iteration: 253 lambda_n: 0.9674150340743286 Loss: 0.2827001380075516\n",
      "Iteration: 254 lambda_n: 0.9528821822819711 Loss: 0.2821067754339513\n",
      "Iteration: 255 lambda_n: 0.9017679388738283 Loss: 0.28152371691125033\n",
      "Iteration: 256 lambda_n: 0.9559363344102921 Loss: 0.28097322647638007\n",
      "Iteration: 257 lambda_n: 0.9507798681114494 Loss: 0.28039096444009604\n",
      "Iteration: 258 lambda_n: 0.9253818045119453 Loss: 0.2798132046521921\n",
      "Iteration: 259 lambda_n: 1.030194659438656 Loss: 0.2792521927206665\n",
      "Iteration: 260 lambda_n: 0.977894262431291 Loss: 0.27862906299785173\n",
      "Iteration: 261 lambda_n: 1.0088775763822881 Loss: 0.2780390652187728\n",
      "Iteration: 262 lambda_n: 0.9900467888662177 Loss: 0.27743183931846516\n",
      "Iteration: 263 lambda_n: 0.9956620238500441 Loss: 0.2768374256894803\n",
      "Iteration: 264 lambda_n: 0.9111092269052724 Loss: 0.27624109703249033\n",
      "Iteration: 265 lambda_n: 0.9545561865778288 Loss: 0.27569674366444247\n",
      "Iteration: 266 lambda_n: 0.9559641786417463 Loss: 0.27512771254237217\n",
      "Iteration: 267 lambda_n: 1.0030887585880686 Loss: 0.2745591811046717\n",
      "Iteration: 268 lambda_n: 0.8971678749286472 Loss: 0.273964029404897\n",
      "Iteration: 269 lambda_n: 0.9888328907762438 Loss: 0.2734330341798443\n",
      "Iteration: 270 lambda_n: 1.0232824673757508 Loss: 0.2728490829337726\n",
      "Iteration: 271 lambda_n: 0.9482667133257007 Loss: 0.2722462613072653\n",
      "Iteration: 272 lambda_n: 0.9988628921329641 Loss: 0.2716890383673562\n",
      "Iteration: 273 lambda_n: 0.8951065311555778 Loss: 0.2711034583329737\n",
      "Iteration: 274 lambda_n: 0.9299147365482926 Loss: 0.2705799946947991\n",
      "Iteration: 275 lambda_n: 1.0215061065874518 Loss: 0.2700373776015829\n",
      "Iteration: 276 lambda_n: 0.896870535672259 Loss: 0.26944268808288624\n",
      "Iteration: 277 lambda_n: 1.0275346105335836 Loss: 0.2689218709223206\n",
      "Iteration: 278 lambda_n: 1.0273401543701783 Loss: 0.2683265048272155\n",
      "Iteration: 279 lambda_n: 0.9485142062802517 Loss: 0.26773276451804606\n",
      "Iteration: 280 lambda_n: 1.0197910023496766 Loss: 0.2671859719525789\n",
      "Iteration: 281 lambda_n: 0.964206166515676 Loss: 0.2665994748872481\n",
      "Iteration: 282 lambda_n: 0.9112865243053 Loss: 0.26604634549395023\n",
      "Iteration: 283 lambda_n: 1.0254729157028644 Loss: 0.2655248230655346\n",
      "Iteration: 284 lambda_n: 0.95997115042649 Loss: 0.26493928653408894\n",
      "Iteration: 285 lambda_n: 0.98463426042056 Loss: 0.2643925460414165\n",
      "Iteration: 286 lambda_n: 0.8963910623095626 Loss: 0.26383310049296255\n",
      "Iteration: 287 lambda_n: 0.986548227015924 Loss: 0.2633250389308515\n",
      "Iteration: 288 lambda_n: 1.011885982245553 Loss: 0.2627671330128841\n",
      "Iteration: 289 lambda_n: 1.0036740928725472 Loss: 0.2621963106494345\n",
      "Iteration: 290 lambda_n: 1.0174316932146652 Loss: 0.2616315546118177\n",
      "Iteration: 291 lambda_n: 0.9457807789553103 Loss: 0.2610604990030089\n",
      "Iteration: 292 lambda_n: 0.9780080350530644 Loss: 0.2605310121313469\n",
      "Iteration: 293 lambda_n: 1.0229021193841599 Loss: 0.25998478825317894\n",
      "Iteration: 294 lambda_n: 1.0393506173108038 Loss: 0.2594149191876228\n",
      "Iteration: 295 lambda_n: 0.8944300877468981 Loss: 0.2588374007388027\n",
      "Iteration: 296 lambda_n: 1.0328643921925804 Loss: 0.2583417167815523\n",
      "Iteration: 297 lambda_n: 0.9286808643561213 Loss: 0.25777062892766317\n",
      "Iteration: 298 lambda_n: 1.020949339367045 Loss: 0.2572584950709602\n",
      "Iteration: 299 lambda_n: 1.0448079649096327 Loss: 0.2566968244011179\n",
      "Iteration: 300 lambda_n: 0.9004895774441114 Loss: 0.25612353761289836\n",
      "Iteration: 301 lambda_n: 0.918180192697073 Loss: 0.2556307596051793\n",
      "Iteration: 302 lambda_n: 1.030528721601581 Loss: 0.25512947179253537\n",
      "Iteration: 303 lambda_n: 0.978342343370779 Loss: 0.2545681957242344\n",
      "Iteration: 304 lambda_n: 0.9256866729586686 Loss: 0.2540367690513408\n",
      "Iteration: 305 lambda_n: 0.8992656825804809 Loss: 0.25353522694217995\n",
      "Iteration: 306 lambda_n: 1.0236503806912591 Loss: 0.25304918233189794\n",
      "Iteration: 307 lambda_n: 1.044577348524342 Loss: 0.2524972333022718\n",
      "Iteration: 308 lambda_n: 1.0237733405335099 Loss: 0.2519355319149702\n",
      "Iteration: 309 lambda_n: 0.907364785481096 Loss: 0.2513865497297182\n",
      "Iteration: 310 lambda_n: 0.9019913741567465 Loss: 0.25090131604511623\n",
      "Iteration: 311 lambda_n: 1.0385820883855683 Loss: 0.2504201377196868\n",
      "Iteration: 312 lambda_n: 0.9183111249642709 Loss: 0.24986746887889263\n",
      "Iteration: 313 lambda_n: 1.0216495327408066 Loss: 0.24938017835260443\n",
      "Iteration: 314 lambda_n: 0.8955937829690244 Loss: 0.2488394415056359\n",
      "Iteration: 315 lambda_n: 0.9662937894625881 Loss: 0.24836675801864294\n",
      "Iteration: 316 lambda_n: 0.919148956042529 Loss: 0.2478580544644998\n",
      "Iteration: 317 lambda_n: 0.9269022394184527 Loss: 0.24737549227111677\n",
      "Iteration: 318 lambda_n: 1.0166469164561163 Loss: 0.24689014500910128\n",
      "Iteration: 319 lambda_n: 0.9643946645966509 Loss: 0.24635925385227495\n",
      "Iteration: 320 lambda_n: 1.0445427762303934 Loss: 0.2458571465970701\n",
      "Iteration: 321 lambda_n: 0.9565206278914063 Loss: 0.2453148917792376\n",
      "Iteration: 322 lambda_n: 1.027025925215538 Loss: 0.24481988758044182\n",
      "Iteration: 323 lambda_n: 0.892814003355554 Loss: 0.24428997863549398\n",
      "Iteration: 324 lambda_n: 1.006000583725522 Loss: 0.24383077492900124\n",
      "Iteration: 325 lambda_n: 0.9349890944758024 Loss: 0.2433148585310049\n",
      "Iteration: 326 lambda_n: 0.9293059147050392 Loss: 0.24283691402003857\n",
      "Iteration: 327 lambda_n: 1.0045589869819869 Loss: 0.2423633507862427\n",
      "Iteration: 328 lambda_n: 1.0429581728530528 Loss: 0.24189228702752583\n",
      "Iteration: 329 lambda_n: 0.9181746746852322 Loss: 0.2414151005297544\n",
      "Iteration: 330 lambda_n: 1.0013404643319908 Loss: 0.24099992083668034\n",
      "Iteration: 331 lambda_n: 0.9158396508683657 Loss: 0.24055183388761056\n",
      "Iteration: 332 lambda_n: 0.9535608996132906 Loss: 0.2401466546505179\n",
      "Iteration: 333 lambda_n: 1.0329347325947253 Loss: 0.23972916776353403\n",
      "Iteration: 334 lambda_n: 0.9445875020959789 Loss: 0.23928181417439542\n",
      "Iteration: 335 lambda_n: 1.0473183525602796 Loss: 0.23887749673762637\n",
      "Iteration: 336 lambda_n: 0.962534892616862 Loss: 0.23843398155356055\n",
      "Iteration: 337 lambda_n: 0.9241112017402113 Loss: 0.23803116779229744\n",
      "Iteration: 338 lambda_n: 0.9365734194064995 Loss: 0.23764860534078583\n",
      "Iteration: 339 lambda_n: 1.0414677705514495 Loss: 0.23726488817933222\n",
      "Iteration: 340 lambda_n: 0.9543534269646161 Loss: 0.23684265122036186\n",
      "Iteration: 341 lambda_n: 0.9794167755052985 Loss: 0.2364602110480609\n",
      "Iteration: 342 lambda_n: 1.0390100743955257 Loss: 0.23607187978593813\n",
      "Iteration: 343 lambda_n: 0.8964887405168628 Loss: 0.23566438281436158\n",
      "Iteration: 344 lambda_n: 0.9237448605761632 Loss: 0.23531681065913557\n",
      "Iteration: 345 lambda_n: 0.9472403698468824 Loss: 0.2349622047526543\n",
      "Iteration: 346 lambda_n: 0.934362537427433 Loss: 0.2346022687954604\n",
      "Iteration: 347 lambda_n: 0.9610088319099569 Loss: 0.23425080207975832\n",
      "Iteration: 348 lambda_n: 0.9330220564170137 Loss: 0.23389265212949772\n",
      "Iteration: 349 lambda_n: 0.9834098476866668 Loss: 0.23354804863200584\n",
      "Iteration: 350 lambda_n: 0.9628799235498404 Loss: 0.23318782616122294\n",
      "Iteration: 351 lambda_n: 1.0248445186755701 Loss: 0.23283803451717325\n",
      "Iteration: 352 lambda_n: 1.0129536319279686 Loss: 0.23246859272073378\n",
      "Iteration: 353 lambda_n: 0.9312156868179151 Loss: 0.23210628243687767\n",
      "Iteration: 354 lambda_n: 0.9315899206341416 Loss: 0.23177564741503562\n",
      "Iteration: 355 lambda_n: 1.0046959652928291 Loss: 0.2314470024749111\n",
      "Iteration: 356 lambda_n: 0.96331553782979 Loss: 0.23109474727892107\n",
      "Iteration: 357 lambda_n: 0.9076334535405922 Loss: 0.2307591462976493\n",
      "Iteration: 358 lambda_n: 1.020981234320573 Loss: 0.23044478479477107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 359 lambda_n: 1.041439071653497 Loss: 0.23009302558879577\n",
      "Iteration: 360 lambda_n: 1.006542714745663 Loss: 0.22973625911911366\n",
      "Iteration: 361 lambda_n: 0.9875913613820958 Loss: 0.2293933624667124\n",
      "Iteration: 362 lambda_n: 0.9465248281118471 Loss: 0.22905865187034252\n",
      "Iteration: 363 lambda_n: 1.0324806088046072 Loss: 0.22873941307736864\n",
      "Iteration: 364 lambda_n: 0.8972264067633299 Loss: 0.22839273963816684\n",
      "Iteration: 365 lambda_n: 1.0074075871031596 Loss: 0.2280928944380293\n",
      "Iteration: 366 lambda_n: 1.022025329735898 Loss: 0.22775755136831993\n",
      "Iteration: 367 lambda_n: 0.9816382257245012 Loss: 0.22741879550635669\n",
      "Iteration: 368 lambda_n: 0.9158173561462274 Loss: 0.22709478657942958\n",
      "Iteration: 369 lambda_n: 0.9820668304842406 Loss: 0.22679367540792353\n",
      "Iteration: 370 lambda_n: 0.9062440657934621 Loss: 0.22647191459121155\n",
      "Iteration: 371 lambda_n: 1.0002053609414199 Loss: 0.2261760797165035\n",
      "Iteration: 372 lambda_n: 0.9550342303583524 Loss: 0.22585064075584618\n",
      "Iteration: 373 lambda_n: 1.019521116939758 Loss: 0.2255409909465484\n",
      "Iteration: 374 lambda_n: 1.0194672039146193 Loss: 0.22521151152357974\n",
      "Iteration: 375 lambda_n: 0.9285833497945353 Loss: 0.2248831673399693\n",
      "Iteration: 376 lambda_n: 0.9974459808101351 Loss: 0.2245850816160062\n",
      "Iteration: 377 lambda_n: 0.9993925784591794 Loss: 0.22426582996812972\n",
      "Iteration: 378 lambda_n: 0.9272275972580889 Loss: 0.2239469408895583\n",
      "Iteration: 379 lambda_n: 0.9751630966836924 Loss: 0.22365197047042337\n",
      "Iteration: 380 lambda_n: 0.93698260963933 Loss: 0.22334260081965351\n",
      "Iteration: 381 lambda_n: 0.9476305577101144 Loss: 0.22304618328432035\n",
      "Iteration: 382 lambda_n: 0.9131631436101665 Loss: 0.22274719532174017\n",
      "Iteration: 383 lambda_n: 0.9040905152488037 Loss: 0.22245984378276995\n",
      "Iteration: 384 lambda_n: 1.0483686440232056 Loss: 0.22217605964213927\n",
      "Iteration: 385 lambda_n: 1.0085752081256794 Loss: 0.22184779277871378\n",
      "Iteration: 386 lambda_n: 0.9725245180330914 Loss: 0.22153286613878956\n",
      "Iteration: 387 lambda_n: 0.9369082507855888 Loss: 0.22122999695026183\n",
      "Iteration: 388 lambda_n: 0.9047370443157068 Loss: 0.22093895019724638\n",
      "Iteration: 389 lambda_n: 1.0320566939888867 Loss: 0.22065856594548675\n",
      "Iteration: 390 lambda_n: 0.9161160375117018 Loss: 0.22033945157531978\n",
      "Iteration: 391 lambda_n: 0.9913560680412932 Loss: 0.22005691005015682\n",
      "Iteration: 392 lambda_n: 1.0166825065297342 Loss: 0.21975184992336566\n",
      "Iteration: 393 lambda_n: 0.98428775461966 Loss: 0.21943974770496652\n",
      "Iteration: 394 lambda_n: 0.8978390319667775 Loss: 0.21913832554710858\n",
      "Iteration: 395 lambda_n: 0.9884457938855161 Loss: 0.21886401742905018\n",
      "Iteration: 396 lambda_n: 0.9963607287177697 Loss: 0.21856266418816597\n",
      "Iteration: 397 lambda_n: 0.9087517779026424 Loss: 0.21825959650499063\n",
      "Iteration: 398 lambda_n: 0.996898318114275 Loss: 0.21798381149782403\n",
      "Iteration: 399 lambda_n: 0.9669703904776165 Loss: 0.21768190545862043\n",
      "Iteration: 400 lambda_n: 0.9899184352117086 Loss: 0.2173897251321441\n",
      "Iteration: 401 lambda_n: 0.9817842441766015 Loss: 0.21709126230171819\n",
      "Iteration: 402 lambda_n: 0.9364688989941206 Loss: 0.2167959069714936\n",
      "Iteration: 403 lambda_n: 0.9732569606389128 Loss: 0.2165147977362364\n",
      "Iteration: 404 lambda_n: 1.0310601413287943 Loss: 0.21622324924194786\n",
      "Iteration: 405 lambda_n: 0.8941474005786911 Loss: 0.21591504490289437\n",
      "Iteration: 406 lambda_n: 0.9619286011014812 Loss: 0.21564836590488864\n",
      "Iteration: 407 lambda_n: 1.0388122193956426 Loss: 0.21536202761274692\n",
      "Iteration: 408 lambda_n: 0.9055177690868593 Loss: 0.21505344535032872\n",
      "Iteration: 409 lambda_n: 0.9022245982808962 Loss: 0.21478505673221063\n",
      "Iteration: 410 lambda_n: 1.0067193304961317 Loss: 0.21451816080662997\n",
      "Iteration: 411 lambda_n: 0.9255366297751866 Loss: 0.2142209251084852\n",
      "Iteration: 412 lambda_n: 0.9812559169113745 Loss: 0.21394823996828183\n",
      "Iteration: 413 lambda_n: 0.9359541963066201 Loss: 0.21365970266110615\n",
      "Iteration: 414 lambda_n: 0.9756846793258386 Loss: 0.213385052356747\n",
      "Iteration: 415 lambda_n: 1.0252153570022966 Loss: 0.21309930363431442\n",
      "Iteration: 416 lambda_n: 0.9189764475707518 Loss: 0.2127996591900519\n",
      "Iteration: 417 lambda_n: 0.9421581273470225 Loss: 0.2125316357829867\n",
      "Iteration: 418 lambda_n: 0.9420956349707157 Loss: 0.21225737330316538\n",
      "Iteration: 419 lambda_n: 0.995913536227927 Loss: 0.21198366114591494\n",
      "Iteration: 420 lambda_n: 0.9660834854917034 Loss: 0.211694873131211\n",
      "Iteration: 421 lambda_n: 0.8965239386107382 Loss: 0.21141530570914907\n",
      "Iteration: 422 lambda_n: 1.004326969096533 Loss: 0.2111563782764232\n",
      "Iteration: 423 lambda_n: 0.9276083945567171 Loss: 0.21086684600735742\n",
      "Iteration: 424 lambda_n: 0.9042540686172759 Loss: 0.2105999746445679\n",
      "Iteration: 425 lambda_n: 0.9345238501467377 Loss: 0.2103403101797002\n",
      "Iteration: 426 lambda_n: 1.030417369419871 Loss: 0.21007244331900465\n",
      "Iteration: 427 lambda_n: 1.0042022562384547 Loss: 0.209777646521853\n",
      "Iteration: 428 lambda_n: 0.9058429305285757 Loss: 0.20949094372168833\n",
      "Iteration: 429 lambda_n: 1.0253889130349405 Loss: 0.2092328418383051\n",
      "Iteration: 430 lambda_n: 0.9321614454486465 Loss: 0.20894120745369796\n",
      "Iteration: 431 lambda_n: 0.9874711715433242 Loss: 0.2086766289720501\n",
      "Iteration: 432 lambda_n: 0.9795254306718215 Loss: 0.20839687156324596\n",
      "Iteration: 433 lambda_n: 0.9661081364049942 Loss: 0.20811990858742388\n",
      "Iteration: 434 lambda_n: 0.9503000099529214 Loss: 0.20784726866356787\n",
      "Iteration: 435 lambda_n: 0.9908362669774506 Loss: 0.2075796011893747\n",
      "Iteration: 436 lambda_n: 1.032739424114116 Loss: 0.20730103887457257\n",
      "Iteration: 437 lambda_n: 0.9203672772212758 Loss: 0.207011261933674\n",
      "Iteration: 438 lambda_n: 0.9916524647929933 Loss: 0.20675353765344412\n",
      "Iteration: 439 lambda_n: 0.9590870530311505 Loss: 0.20647635281746257\n",
      "Iteration: 440 lambda_n: 0.953727288979976 Loss: 0.20620878961960498\n",
      "Iteration: 441 lambda_n: 1.014481909369952 Loss: 0.20594321911005387\n",
      "Iteration: 442 lambda_n: 0.9531995704480141 Loss: 0.20566125600376456\n",
      "Iteration: 443 lambda_n: 0.960781625158445 Loss: 0.20539684694489949\n",
      "Iteration: 444 lambda_n: 0.9029544082363328 Loss: 0.20513082713143108\n",
      "Iteration: 445 lambda_n: 0.9429637401043148 Loss: 0.20488128257487412\n",
      "Iteration: 446 lambda_n: 0.9124479547196954 Loss: 0.2046211356206218\n",
      "Iteration: 447 lambda_n: 1.0199950388963257 Loss: 0.20436986469766513\n",
      "Iteration: 448 lambda_n: 0.9810443996087534 Loss: 0.20408947163127744\n",
      "Iteration: 449 lambda_n: 0.8971679493372133 Loss: 0.20382031395310266\n",
      "Iteration: 450 lambda_n: 0.9030782812848746 Loss: 0.20357463066317516\n",
      "Iteration: 451 lambda_n: 1.0410917308339083 Loss: 0.20332775352896967\n",
      "Iteration: 452 lambda_n: 0.9280894473228624 Loss: 0.20304363981993706\n",
      "Iteration: 453 lambda_n: 0.8939857098675149 Loss: 0.20279086628673867\n",
      "Iteration: 454 lambda_n: 1.0256563276658246 Loss: 0.2025478111273705\n",
      "Iteration: 455 lambda_n: 0.9612495420607076 Loss: 0.20226943284849327\n",
      "Iteration: 456 lambda_n: 0.9736888812816668 Loss: 0.20200904268357234\n",
      "Iteration: 457 lambda_n: 0.9614645017182544 Loss: 0.20174576325348853\n",
      "Iteration: 458 lambda_n: 0.9160794348584164 Loss: 0.20148626778356096\n",
      "Iteration: 459 lambda_n: 0.9518283803703842 Loss: 0.2012394698968405\n",
      "Iteration: 460 lambda_n: 1.0482125288166413 Loss: 0.2009834840852015\n",
      "Iteration: 461 lambda_n: 0.9344156971433143 Loss: 0.20070208257364947\n",
      "Iteration: 462 lambda_n: 1.0079294206789906 Loss: 0.20045172363119684\n",
      "Iteration: 463 lambda_n: 1.0404137852562763 Loss: 0.2001821420538289\n",
      "Iteration: 464 lambda_n: 1.0326293874802568 Loss: 0.19990439764631718\n",
      "Iteration: 465 lambda_n: 0.9910562694530273 Loss: 0.1996292672814605\n",
      "Iteration: 466 lambda_n: 0.8978375932798764 Loss: 0.19936572180078851\n",
      "Iteration: 467 lambda_n: 1.0204805604121578 Loss: 0.19912740519038785\n",
      "Iteration: 468 lambda_n: 1.0133690421800288 Loss: 0.198856988467051\n",
      "Iteration: 469 lambda_n: 0.9245014970515432 Loss: 0.19858896488776484\n",
      "Iteration: 470 lambda_n: 0.9703052131860059 Loss: 0.19834490403832208\n",
      "Iteration: 471 lambda_n: 0.9192181966420943 Loss: 0.19808919000089797\n",
      "Iteration: 472 lambda_n: 1.0225589215457034 Loss: 0.1978473731571385\n",
      "Iteration: 473 lambda_n: 0.9897707727670956 Loss: 0.19757882786090986\n",
      "Iteration: 474 lambda_n: 0.9623206337177348 Loss: 0.1973193825550669\n",
      "Iteration: 475 lambda_n: 0.941561054368958 Loss: 0.19706759138463814\n",
      "Iteration: 476 lambda_n: 0.9588906375816619 Loss: 0.19682166688915514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 477 lambda_n: 1.0040118810583971 Loss: 0.19657164842805241\n",
      "Iteration: 478 lambda_n: 1.0138191915202217 Loss: 0.19631032495742623\n",
      "Iteration: 479 lambda_n: 0.9603410197837123 Loss: 0.19604693294440564\n",
      "Iteration: 480 lambda_n: 0.9885189951279777 Loss: 0.19579789543680853\n",
      "Iteration: 481 lambda_n: 0.991739761895678 Loss: 0.195541999302655\n",
      "Iteration: 482 lambda_n: 0.9297712926298526 Loss: 0.1952857308152035\n",
      "Iteration: 483 lambda_n: 0.9342369159703806 Loss: 0.19504590701705654\n",
      "Iteration: 484 lambda_n: 1.045899704778693 Loss: 0.19480533741435363\n",
      "Iteration: 485 lambda_n: 0.9079524816940326 Loss: 0.1945364707289364\n",
      "Iteration: 486 lambda_n: 0.9315339640171222 Loss: 0.19430350546501496\n",
      "Iteration: 487 lambda_n: 0.9262151753372071 Loss: 0.1940648812328949\n",
      "Iteration: 488 lambda_n: 1.033575728260878 Loss: 0.19382801746378897\n",
      "Iteration: 489 lambda_n: 1.017096997681409 Loss: 0.19356413949724383\n",
      "Iteration: 490 lambda_n: 0.923629079964373 Loss: 0.19330495038259737\n",
      "Iteration: 491 lambda_n: 0.9557200087632675 Loss: 0.19307000811383507\n",
      "Iteration: 492 lambda_n: 1.0174168087132491 Loss: 0.192827305099807\n",
      "Iteration: 493 lambda_n: 1.0369449952762118 Loss: 0.19256937626629975\n",
      "Iteration: 494 lambda_n: 1.0199862747549822 Loss: 0.19230697421940585\n",
      "Iteration: 495 lambda_n: 1.0374488632585608 Loss: 0.1920493402261071\n",
      "Iteration: 496 lambda_n: 1.0195858062729901 Loss: 0.1917877709771915\n",
      "Iteration: 497 lambda_n: 0.9124262353265489 Loss: 0.1915311788257671\n",
      "Iteration: 498 lambda_n: 1.0333604496008548 Loss: 0.1913019687350393\n",
      "Iteration: 499 lambda_n: 0.9166587150149493 Loss: 0.19104279936351962\n",
      "Iteration: 500 lambda_n: 0.93202881471491 Loss: 0.19081331774607765\n",
      "Iteration: 501 lambda_n: 0.9137310265688214 Loss: 0.19058036579850468\n",
      "Iteration: 502 lambda_n: 0.9865699343050798 Loss: 0.19035236202564462\n",
      "Iteration: 503 lambda_n: 1.0319730155130933 Loss: 0.19010657921118312\n",
      "Iteration: 504 lambda_n: 1.0131616882922385 Loss: 0.18984993125122154\n",
      "Iteration: 505 lambda_n: 0.9429888760793994 Loss: 0.18959841744784706\n",
      "Iteration: 506 lambda_n: 0.9072861181834793 Loss: 0.18936473837263065\n",
      "Iteration: 507 lambda_n: 1.0299349371353288 Loss: 0.18914027697843402\n",
      "Iteration: 508 lambda_n: 1.0271315949120623 Loss: 0.18888587737169446\n",
      "Iteration: 509 lambda_n: 0.9921300408937653 Loss: 0.188632625668615\n",
      "Iteration: 510 lambda_n: 0.9427317001513328 Loss: 0.18838844091963175\n",
      "Iteration: 511 lambda_n: 0.9937641100310561 Loss: 0.18815681364872228\n",
      "Iteration: 512 lambda_n: 1.0142609316021376 Loss: 0.18791304771011408\n",
      "Iteration: 513 lambda_n: 1.016554956871742 Loss: 0.1876646825208414\n",
      "Iteration: 514 lambda_n: 0.9484500630892654 Loss: 0.1874161923029533\n",
      "Iteration: 515 lambda_n: 0.9637809537586751 Loss: 0.18718475616290312\n",
      "Iteration: 516 lambda_n: 1.0111830037913794 Loss: 0.18694996379609233\n",
      "Iteration: 517 lambda_n: 1.0285429487625186 Loss: 0.18670403280929196\n",
      "Iteration: 518 lambda_n: 1.0078961504932393 Loss: 0.18645431472238333\n",
      "Iteration: 519 lambda_n: 0.9222001779667277 Loss: 0.186210041193183\n",
      "Iteration: 520 lambda_n: 0.9033929351072861 Loss: 0.1859869220725378\n",
      "Iteration: 521 lambda_n: 0.960709421247071 Loss: 0.18576869789199757\n",
      "Iteration: 522 lambda_n: 0.9493583392643492 Loss: 0.18553698710329355\n",
      "Iteration: 523 lambda_n: 0.9859244476775917 Loss: 0.18530838923806964\n",
      "Iteration: 524 lambda_n: 1.0124435107708964 Loss: 0.1850713709074598\n",
      "Iteration: 525 lambda_n: 1.0129706622142292 Loss: 0.1848283858510862\n",
      "Iteration: 526 lambda_n: 0.9089571551250399 Loss: 0.18458569235990635\n",
      "Iteration: 527 lambda_n: 0.9616345991383602 Loss: 0.18436829209572073\n",
      "Iteration: 528 lambda_n: 0.9541632772554968 Loss: 0.1841386472371766\n",
      "Iteration: 529 lambda_n: 1.0083776891582934 Loss: 0.18391115701136246\n",
      "Iteration: 530 lambda_n: 0.9352014383234688 Loss: 0.18367112899489657\n",
      "Iteration: 531 lambda_n: 0.9293008244000888 Loss: 0.183448897092927\n",
      "Iteration: 532 lambda_n: 0.8947314048640748 Loss: 0.18322841494205838\n",
      "Iteration: 533 lambda_n: 1.0488404459709924 Loss: 0.18301646585302445\n",
      "Iteration: 534 lambda_n: 0.9240094894457865 Loss: 0.18276838550420105\n",
      "Iteration: 535 lambda_n: 0.9182847431409453 Loss: 0.18255021416212464\n",
      "Iteration: 536 lambda_n: 0.9185496275142153 Loss: 0.18233372975598813\n",
      "Iteration: 537 lambda_n: 0.9101508071994817 Loss: 0.18211751524767225\n",
      "Iteration: 538 lambda_n: 1.0293456258943454 Loss: 0.18190360605987432\n",
      "Iteration: 539 lambda_n: 1.028159098928023 Loss: 0.1816620513432433\n",
      "Iteration: 540 lambda_n: 0.9715812299253164 Loss: 0.1814211884163451\n",
      "Iteration: 541 lambda_n: 0.9714562349021598 Loss: 0.1811939680426708\n",
      "Iteration: 542 lambda_n: 0.9327817979474327 Loss: 0.18096714314914503\n",
      "Iteration: 543 lambda_n: 0.9710571006195582 Loss: 0.18074969846756114\n",
      "Iteration: 544 lambda_n: 0.9836202234575332 Loss: 0.18052368101678284\n",
      "Iteration: 545 lambda_n: 0.9968099924332975 Loss: 0.18029510687123965\n",
      "Iteration: 546 lambda_n: 0.9380443748042001 Loss: 0.18006384370136047\n",
      "Iteration: 547 lambda_n: 0.9996533706921257 Loss: 0.1798465710657671\n",
      "Iteration: 548 lambda_n: 1.018928678537663 Loss: 0.17961538631795662\n",
      "Iteration: 549 lambda_n: 1.0014625541457682 Loss: 0.17938013100172048\n",
      "Iteration: 550 lambda_n: 0.9307157532643499 Loss: 0.17914929454924558\n",
      "Iteration: 551 lambda_n: 0.9643889522105094 Loss: 0.17893511625619235\n",
      "Iteration: 552 lambda_n: 0.9281289947791439 Loss: 0.17871352712472952\n",
      "Iteration: 553 lambda_n: 1.0077884344659545 Loss: 0.17850060493743355\n",
      "Iteration: 554 lambda_n: 1.0223746592294403 Loss: 0.17826975887229535\n",
      "Iteration: 555 lambda_n: 0.9518307031145308 Loss: 0.17803595613051312\n",
      "Iteration: 556 lambda_n: 1.0301830561788483 Loss: 0.17781864685956691\n",
      "Iteration: 557 lambda_n: 0.9773442664867136 Loss: 0.17758381359204353\n",
      "Iteration: 558 lambda_n: 0.9096344875436022 Loss: 0.17736139657012004\n",
      "Iteration: 559 lambda_n: 0.9878831200912725 Loss: 0.1771547152420341\n",
      "Iteration: 560 lambda_n: 0.9475591338593738 Loss: 0.17693058568143213\n",
      "Iteration: 561 lambda_n: 1.0099352279248717 Loss: 0.1767159471575401\n",
      "Iteration: 562 lambda_n: 0.933325400916036 Loss: 0.176487529513709\n",
      "Iteration: 563 lambda_n: 0.9941787399047316 Loss: 0.1762767810986159\n",
      "Iteration: 564 lambda_n: 1.0143975214587355 Loss: 0.17605262923517603\n",
      "Iteration: 565 lambda_n: 0.937730866810508 Loss: 0.17582428396925737\n",
      "Iteration: 566 lambda_n: 0.9259118006903277 Loss: 0.1756135390699304\n",
      "Iteration: 567 lambda_n: 1.0246556755960137 Loss: 0.17540576254797502\n",
      "Iteration: 568 lambda_n: 0.9566017045317406 Loss: 0.1751761691438435\n",
      "Iteration: 569 lambda_n: 0.9246987776440281 Loss: 0.17496217431681585\n",
      "Iteration: 570 lambda_n: 0.9797946289877424 Loss: 0.1747556313086351\n",
      "Iteration: 571 lambda_n: 1.0382120467261664 Loss: 0.1745371046697603\n",
      "Iteration: 572 lambda_n: 1.0209879106364874 Loss: 0.17430591033315243\n",
      "Iteration: 573 lambda_n: 0.9490392170732735 Loss: 0.17407892601028094\n",
      "Iteration: 574 lambda_n: 1.0094533261009628 Loss: 0.17386827780851075\n",
      "Iteration: 575 lambda_n: 1.0461732371461494 Loss: 0.17364455734924944\n",
      "Iteration: 576 lambda_n: 0.9981298208363141 Loss: 0.1734130691695724\n",
      "Iteration: 577 lambda_n: 0.9757169121735209 Loss: 0.17319257569389954\n",
      "Iteration: 578 lambda_n: 0.9780179368051576 Loss: 0.17297737212981792\n",
      "Iteration: 579 lambda_n: 0.9423317412521494 Loss: 0.17276199226359\n",
      "Iteration: 580 lambda_n: 0.9008100751274257 Loss: 0.17255478976176575\n",
      "Iteration: 581 lambda_n: 0.9108885314965397 Loss: 0.17235700960110903\n",
      "Iteration: 582 lambda_n: 0.9183660543402765 Loss: 0.17215729905605462\n",
      "Iteration: 583 lambda_n: 0.9781147269227409 Loss: 0.17195623620405714\n",
      "Iteration: 584 lambda_n: 0.9448677213876306 Loss: 0.17174240033662846\n",
      "Iteration: 585 lambda_n: 0.9745320105960622 Loss: 0.17153614797630523\n",
      "Iteration: 586 lambda_n: 0.9474513078697664 Loss: 0.17132373396540249\n",
      "Iteration: 587 lambda_n: 0.9979507100319442 Loss: 0.17111753561474896\n",
      "Iteration: 588 lambda_n: 0.9746175826001411 Loss: 0.17090066735538048\n",
      "Iteration: 589 lambda_n: 0.9903574356720698 Loss: 0.1706891976051833\n",
      "Iteration: 590 lambda_n: 0.9152090836556708 Loss: 0.1704746375697857\n",
      "Iteration: 591 lambda_n: 1.017692267621845 Loss: 0.17027666158854235\n",
      "Iteration: 592 lambda_n: 0.9058448550589862 Loss: 0.17005682944848638\n",
      "Iteration: 593 lambda_n: 0.9554994937000613 Loss: 0.16986146382676173\n",
      "Iteration: 594 lambda_n: 1.013597321139314 Loss: 0.16965567750553415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 595 lambda_n: 0.923121461137304 Loss: 0.1694377005880631\n",
      "Iteration: 596 lambda_n: 0.9245755885815012 Loss: 0.16923948925959806\n",
      "Iteration: 597 lambda_n: 0.9342295237113712 Loss: 0.16904124720308053\n",
      "Iteration: 598 lambda_n: 0.9134344529381287 Loss: 0.1688412194205734\n",
      "Iteration: 599 lambda_n: 0.9368615816190498 Loss: 0.16864592376919482\n",
      "Iteration: 600 lambda_n: 0.9092149608389842 Loss: 0.16844589953069813\n",
      "Iteration: 601 lambda_n: 1.0236249008207758 Loss: 0.16825205559608244\n",
      "Iteration: 602 lambda_n: 1.0423380455248659 Loss: 0.16803412379928329\n",
      "Iteration: 603 lambda_n: 1.021462166073085 Loss: 0.16781255444423476\n",
      "Iteration: 604 lambda_n: 0.9918205422698884 Loss: 0.16759576693075626\n",
      "Iteration: 605 lambda_n: 1.0025113692878143 Loss: 0.16738559681980159\n",
      "Iteration: 606 lambda_n: 0.8936135333399524 Loss: 0.16717348124995057\n",
      "Iteration: 607 lambda_n: 1.005648824971726 Loss: 0.16698469291058485\n",
      "Iteration: 608 lambda_n: 1.0463792682056994 Loss: 0.16677252432876724\n",
      "Iteration: 609 lambda_n: 0.9041703584580917 Loss: 0.16655209877564772\n",
      "Iteration: 610 lambda_n: 0.9492696905276038 Loss: 0.16636192970429112\n",
      "Iteration: 611 lambda_n: 0.9735691294288349 Loss: 0.16616254786758516\n",
      "Iteration: 612 lambda_n: 0.911946804622403 Loss: 0.16595835487887214\n",
      "Iteration: 613 lambda_n: 1.0249946717298977 Loss: 0.16576736575040388\n",
      "Iteration: 614 lambda_n: 1.0381487086998724 Loss: 0.16555299647599547\n",
      "Iteration: 615 lambda_n: 0.9995977698463473 Loss: 0.16533621021431583\n",
      "Iteration: 616 lambda_n: 0.9597172794116272 Loss: 0.16512779838012656\n",
      "Iteration: 617 lambda_n: 0.9517555689242816 Loss: 0.16492800026733886\n",
      "Iteration: 618 lambda_n: 0.9436263754582952 Loss: 0.1647301437160888\n",
      "Iteration: 619 lambda_n: 1.0480185748874895 Loss: 0.1645342556475759\n",
      "Iteration: 620 lambda_n: 1.0001187821359547 Loss: 0.1643170040618934\n",
      "Iteration: 621 lambda_n: 0.9144201240781222 Loss: 0.16411000485065624\n",
      "Iteration: 622 lambda_n: 0.9940430159938163 Loss: 0.1639210235193128\n",
      "Iteration: 623 lambda_n: 1.0431474219714472 Loss: 0.16371586656185114\n",
      "Iteration: 624 lambda_n: 0.9093190177100344 Loss: 0.16350089310678625\n",
      "Iteration: 625 lambda_n: 0.943316570532742 Loss: 0.16331378722266698\n",
      "Iteration: 626 lambda_n: 0.9281479110897546 Loss: 0.163119947266945\n",
      "Iteration: 627 lambda_n: 0.9692679850683753 Loss: 0.1629294898780568\n",
      "Iteration: 628 lambda_n: 1.0197679324401856 Loss: 0.16273086744329113\n",
      "Iteration: 629 lambda_n: 0.9762178558917635 Loss: 0.1625221956771401\n",
      "Iteration: 630 lambda_n: 0.8989307301532854 Loss: 0.1623227347136661\n",
      "Iteration: 631 lambda_n: 0.9576974852307337 Loss: 0.16213932775362397\n",
      "Iteration: 632 lambda_n: 0.9623676221082645 Loss: 0.16194418928489385\n",
      "Iteration: 633 lambda_n: 0.9930024720117379 Loss: 0.16174837466201855\n",
      "Iteration: 634 lambda_n: 1.0223910721888219 Loss: 0.1615466118592838\n",
      "Iteration: 635 lambda_n: 1.0077412891290132 Loss: 0.16133917985258855\n",
      "Iteration: 636 lambda_n: 0.8950643896514239 Loss: 0.16113502532405183\n",
      "Iteration: 637 lambda_n: 0.9180331125393416 Loss: 0.1609539629313004\n",
      "Iteration: 638 lambda_n: 1.0111496986087005 Loss: 0.16076849666376344\n",
      "Iteration: 639 lambda_n: 0.958802561504642 Loss: 0.1605644925771572\n",
      "Iteration: 640 lambda_n: 1.0125130145750596 Loss: 0.16037133349765914\n",
      "Iteration: 641 lambda_n: 1.0380561389333605 Loss: 0.16016763862412223\n",
      "Iteration: 642 lambda_n: 1.0059364014639027 Loss: 0.15995911201669602\n",
      "Iteration: 643 lambda_n: 0.8947280631503749 Loss: 0.15975734108734788\n",
      "Iteration: 644 lambda_n: 1.0368118691418515 Loss: 0.15957813622944633\n",
      "Iteration: 645 lambda_n: 1.010536748623333 Loss: 0.15937074370643584\n",
      "Iteration: 646 lambda_n: 0.9658474136406703 Loss: 0.15916890906707842\n",
      "Iteration: 647 lambda_n: 1.0074626928575134 Loss: 0.1589762806361713\n",
      "Iteration: 648 lambda_n: 0.9762609146975773 Loss: 0.15877563232179043\n",
      "Iteration: 649 lambda_n: 0.9109890364106461 Loss: 0.15858147944743886\n",
      "Iteration: 650 lambda_n: 0.9168146952734586 Loss: 0.15840056066108665\n",
      "Iteration: 651 lambda_n: 0.9123621135291293 Loss: 0.1582187228661333\n",
      "Iteration: 652 lambda_n: 0.9290751010601842 Loss: 0.1580380057870244\n",
      "Iteration: 653 lambda_n: 0.9506481798972182 Loss: 0.15785421871191424\n",
      "Iteration: 654 lambda_n: 0.9987914243701904 Loss: 0.15766641409641866\n",
      "Iteration: 655 lambda_n: 0.9044089839469635 Loss: 0.157469366993635\n",
      "Iteration: 656 lambda_n: 1.048786922915763 Loss: 0.15729119323740426\n",
      "Iteration: 657 lambda_n: 0.914653408288739 Loss: 0.15708484428338185\n",
      "Iteration: 658 lambda_n: 0.9788252151122998 Loss: 0.15690515308636813\n",
      "Iteration: 659 lambda_n: 1.0340027476815534 Loss: 0.15671310553774004\n",
      "Iteration: 660 lambda_n: 0.9815112957736807 Loss: 0.15651051463390672\n",
      "Iteration: 661 lambda_n: 1.0094275482250792 Loss: 0.15631848957189984\n",
      "Iteration: 662 lambda_n: 1.0122401807493886 Loss: 0.15612127774069257\n",
      "Iteration: 663 lambda_n: 1.036148250460328 Loss: 0.15592379877908719\n",
      "Iteration: 664 lambda_n: 0.9352611595127979 Loss: 0.15572194493461594\n",
      "Iteration: 665 lambda_n: 0.9951089753669747 Loss: 0.1555400102339391\n",
      "Iteration: 666 lambda_n: 0.9137666938793828 Loss: 0.15534668933334014\n",
      "Iteration: 667 lambda_n: 0.9529061524195481 Loss: 0.1551694185890813\n",
      "Iteration: 668 lambda_n: 0.9120671322023575 Loss: 0.15498479276860389\n",
      "Iteration: 669 lambda_n: 1.016234916219253 Loss: 0.15480831553809224\n",
      "Iteration: 670 lambda_n: 1.0133024322247377 Loss: 0.1546119356894711\n",
      "Iteration: 671 lambda_n: 1.0132679643206182 Loss: 0.1544164013526954\n",
      "Iteration: 672 lambda_n: 1.041756303992363 Loss: 0.1542211509964063\n",
      "Iteration: 673 lambda_n: 0.9959883293554733 Loss: 0.1540206958775913\n",
      "Iteration: 674 lambda_n: 1.0095616706583583 Loss: 0.15382932566169408\n",
      "Iteration: 675 lambda_n: 0.9058622969990705 Loss: 0.15363561709560936\n",
      "Iteration: 676 lambda_n: 0.9869963587595373 Loss: 0.15346204897822927\n",
      "Iteration: 677 lambda_n: 0.9277564605532078 Loss: 0.1532731745585163\n",
      "Iteration: 678 lambda_n: 0.9899292166493652 Loss: 0.1530958793224892\n",
      "Iteration: 679 lambda_n: 0.9722435269404119 Loss: 0.1529069473045297\n",
      "Iteration: 680 lambda_n: 0.9355464429773155 Loss: 0.15272164523148912\n",
      "Iteration: 681 lambda_n: 0.8941857954563759 Loss: 0.15254357707402016\n",
      "Iteration: 682 lambda_n: 0.94220132415332 Loss: 0.15237360121170973\n",
      "Iteration: 683 lambda_n: 0.9095522697079381 Loss: 0.152194720067672\n",
      "Iteration: 684 lambda_n: 0.982576806587899 Loss: 0.1520222618073154\n",
      "Iteration: 685 lambda_n: 1.0067558327710422 Loss: 0.15183619223915834\n",
      "Iteration: 686 lambda_n: 0.980455854482835 Loss: 0.1516458024077851\n",
      "Iteration: 687 lambda_n: 0.8927101069744071 Loss: 0.15146064287094543\n",
      "Iteration: 688 lambda_n: 0.9820639506466406 Loss: 0.15129228043050394\n",
      "Iteration: 689 lambda_n: 1.020065289586641 Loss: 0.1511072943960058\n",
      "Iteration: 690 lambda_n: 1.033496487229025 Loss: 0.15091540971219913\n",
      "Iteration: 691 lambda_n: 0.9913192553611407 Loss: 0.15072127051028686\n",
      "Iteration: 692 lambda_n: 0.9441877581922988 Loss: 0.15053531713880547\n",
      "Iteration: 693 lambda_n: 0.9936144269013631 Loss: 0.15035844431574824\n",
      "Iteration: 694 lambda_n: 0.956605646267619 Loss: 0.1501725532387704\n",
      "Iteration: 695 lambda_n: 0.9002428902465758 Loss: 0.1499938281973405\n",
      "Iteration: 696 lambda_n: 1.0133043292199855 Loss: 0.1498258522251446\n",
      "Iteration: 697 lambda_n: 0.9107397178198496 Loss: 0.14963701348507819\n",
      "Iteration: 698 lambda_n: 1.0167828595775683 Loss: 0.14946752136858157\n",
      "Iteration: 699 lambda_n: 0.9316185127207277 Loss: 0.14927852986566922\n",
      "Iteration: 700 lambda_n: 0.9126062344053971 Loss: 0.14910560602692313\n",
      "Iteration: 701 lambda_n: 0.9385042584383213 Loss: 0.14893642497580334\n",
      "Iteration: 702 lambda_n: 1.0468627058174258 Loss: 0.1487626583321952\n",
      "Iteration: 703 lambda_n: 0.9101103460858412 Loss: 0.14856907656026253\n",
      "Iteration: 704 lambda_n: 1.0324613570731802 Loss: 0.148401019068695\n",
      "Iteration: 705 lambda_n: 0.9425778165417017 Loss: 0.14821060475633344\n",
      "Iteration: 706 lambda_n: 1.0097360908034183 Loss: 0.14803700849717882\n",
      "Iteration: 707 lambda_n: 1.0323748010368516 Loss: 0.14785128075423226\n",
      "Iteration: 708 lambda_n: 1.033503810706971 Loss: 0.14766164742259716\n",
      "Iteration: 709 lambda_n: 0.9467071929731418 Loss: 0.14747207035334156\n",
      "Iteration: 710 lambda_n: 0.958848112823743 Loss: 0.14729865456144803\n",
      "Iteration: 711 lambda_n: 1.0154180510194264 Loss: 0.14712323816176448\n",
      "Iteration: 712 lambda_n: 0.9054561392398678 Loss: 0.14693771219581458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 713 lambda_n: 1.0443813103464394 Loss: 0.14677250093989358\n",
      "Iteration: 714 lambda_n: 1.0396023168355828 Loss: 0.14658217398179424\n",
      "Iteration: 715 lambda_n: 0.8963372749241613 Loss: 0.14639298240561052\n",
      "Iteration: 716 lambda_n: 0.9238590486705618 Loss: 0.1462300875870286\n",
      "Iteration: 717 lambda_n: 1.0016092319959464 Loss: 0.14606239223635567\n",
      "Iteration: 718 lambda_n: 0.9443084847622769 Loss: 0.1458808088952317\n",
      "Iteration: 719 lambda_n: 0.9835054015816422 Loss: 0.14570984129005102\n",
      "Iteration: 720 lambda_n: 0.9295000007439195 Loss: 0.14553200125715102\n",
      "Iteration: 721 lambda_n: 0.947598514459112 Loss: 0.1453641456179367\n",
      "Iteration: 722 lambda_n: 1.0283434828243938 Loss: 0.14519323302703765\n",
      "Iteration: 723 lambda_n: 0.9014088527294797 Loss: 0.14500799121178357\n",
      "Iteration: 724 lambda_n: 0.936889060395141 Loss: 0.1448458346165523\n",
      "Iteration: 725 lambda_n: 0.9202800154578178 Loss: 0.14467749696484702\n",
      "Iteration: 726 lambda_n: 1.0169756060882236 Loss: 0.1445123482615582\n",
      "Iteration: 727 lambda_n: 0.925979766879588 Loss: 0.14433007033714276\n",
      "Iteration: 728 lambda_n: 0.8997257331305327 Loss: 0.1441643237467273\n",
      "Iteration: 729 lambda_n: 0.9203957703678136 Loss: 0.14400347286653403\n",
      "Iteration: 730 lambda_n: 1.0138165904207 Loss: 0.14383912197391288\n",
      "Iteration: 731 lambda_n: 0.9889913996519202 Loss: 0.14365831000368606\n",
      "Iteration: 732 lambda_n: 0.955080567210195 Loss: 0.1434821604163546\n",
      "Iteration: 733 lambda_n: 0.9781517732022905 Loss: 0.14331227131344498\n",
      "Iteration: 734 lambda_n: 1.0460147821638734 Loss: 0.143138496775868\n",
      "Iteration: 735 lambda_n: 0.9069820189431156 Loss: 0.1429529053444225\n",
      "Iteration: 736 lambda_n: 0.9237467701250688 Loss: 0.1427922009146572\n",
      "Iteration: 737 lambda_n: 1.0452655084592524 Loss: 0.14262872053502315\n",
      "Iteration: 738 lambda_n: 0.8991992966054594 Loss: 0.14244395958067316\n",
      "Iteration: 739 lambda_n: 0.9414203057189696 Loss: 0.14228523253054903\n",
      "Iteration: 740 lambda_n: 0.9950696705799511 Loss: 0.14211924822055286\n",
      "Iteration: 741 lambda_n: 0.9781966002207975 Loss: 0.14194402101135836\n",
      "Iteration: 742 lambda_n: 1.0159393632046412 Loss: 0.14177198820515116\n",
      "Iteration: 743 lambda_n: 0.9577396691309947 Loss: 0.14159354575392338\n",
      "Iteration: 744 lambda_n: 0.9455951648013619 Loss: 0.14142554717517217\n",
      "Iteration: 745 lambda_n: 0.9318949818204173 Loss: 0.14125988515230406\n",
      "Iteration: 746 lambda_n: 0.8970258951349092 Loss: 0.14109682356311803\n",
      "Iteration: 747 lambda_n: 1.0316889971903989 Loss: 0.1409400526120323\n",
      "Iteration: 748 lambda_n: 0.948573371217512 Loss: 0.14075995849880552\n",
      "Iteration: 749 lambda_n: 1.0071982878010313 Loss: 0.14059459336976826\n",
      "Iteration: 750 lambda_n: 1.0360731011066993 Loss: 0.1404192244503788\n",
      "Iteration: 751 lambda_n: 0.9880228908040256 Loss: 0.14023906326498128\n",
      "Iteration: 752 lambda_n: 0.96321664873843 Loss: 0.14006748667500835\n",
      "Iteration: 753 lambda_n: 0.9773642475785498 Loss: 0.13990043074562672\n",
      "Iteration: 754 lambda_n: 1.0418113092839119 Loss: 0.13973113179107563\n",
      "Iteration: 755 lambda_n: 0.9981239644488863 Loss: 0.1395508974411474\n",
      "Iteration: 756 lambda_n: 0.9502324777289145 Loss: 0.13937845192790893\n",
      "Iteration: 757 lambda_n: 1.023613251648008 Loss: 0.13921449063570246\n",
      "Iteration: 758 lambda_n: 0.9741202833669591 Loss: 0.13903808424037808\n",
      "Iteration: 759 lambda_n: 0.9363987220946679 Loss: 0.13887042717513096\n",
      "Iteration: 760 lambda_n: 0.9005214367860197 Loss: 0.1387094632204598\n",
      "Iteration: 761 lambda_n: 0.9372823526563817 Loss: 0.13855485170218432\n",
      "Iteration: 762 lambda_n: 0.970485501407337 Loss: 0.13839411465739782\n",
      "Iteration: 763 lambda_n: 0.9308979574187528 Loss: 0.1382278835068142\n",
      "Iteration: 764 lambda_n: 0.9801919338659038 Loss: 0.13806863041881845\n",
      "Iteration: 765 lambda_n: 0.9218994008511594 Loss: 0.1379011444048503\n",
      "Iteration: 766 lambda_n: 0.9980769336201075 Loss: 0.1377438151201013\n",
      "Iteration: 767 lambda_n: 0.9632388325326215 Loss: 0.13757368682841528\n",
      "Iteration: 768 lambda_n: 1.0182320235568563 Loss: 0.1374097051804525\n",
      "Iteration: 769 lambda_n: 0.9219526076563026 Loss: 0.1372365748251531\n",
      "Iteration: 770 lambda_n: 0.8939122137058261 Loss: 0.13708001661525976\n",
      "Iteration: 771 lambda_n: 0.9431724510263504 Loss: 0.1369283974545598\n",
      "Iteration: 772 lambda_n: 0.9927721526217284 Loss: 0.13676860535019839\n",
      "Iteration: 773 lambda_n: 0.9583195777080784 Loss: 0.13660061205024435\n",
      "Iteration: 774 lambda_n: 0.9062279874536591 Loss: 0.13643865230739213\n",
      "Iteration: 775 lambda_n: 0.9185205587058153 Loss: 0.1362856814769681\n",
      "Iteration: 776 lambda_n: 0.9212737583648544 Loss: 0.13613081366330745\n",
      "Iteration: 777 lambda_n: 0.9223742675155594 Loss: 0.1359756621247651\n",
      "Iteration: 778 lambda_n: 0.9715598666705858 Loss: 0.13582050611786123\n",
      "Iteration: 779 lambda_n: 0.9593345238060658 Loss: 0.13565726747155368\n",
      "Iteration: 780 lambda_n: 0.92043299914736 Loss: 0.13549628033015998\n",
      "Iteration: 781 lambda_n: 0.9537375690981003 Loss: 0.1353420076143708\n",
      "Iteration: 782 lambda_n: 0.8995863008943092 Loss: 0.13518233859351747\n",
      "Iteration: 783 lambda_n: 0.9419065584779621 Loss: 0.13503191537333603\n",
      "Iteration: 784 lambda_n: 0.9418040861289613 Loss: 0.1348745944403464\n",
      "Iteration: 785 lambda_n: 1.0487513075191244 Loss: 0.13471747687704416\n",
      "Iteration: 786 lambda_n: 0.9333318465493853 Loss: 0.13454272634333903\n",
      "Iteration: 787 lambda_n: 1.0213216469584585 Loss: 0.13438741098555526\n",
      "Iteration: 788 lambda_n: 0.9980996028242826 Loss: 0.13421765354348333\n",
      "Iteration: 789 lambda_n: 0.9200172956792837 Loss: 0.13405196792455468\n",
      "Iteration: 790 lambda_n: 1.0179028452426637 Loss: 0.13389943388277709\n",
      "Iteration: 791 lambda_n: 0.9781694539680093 Loss: 0.1337308666729361\n",
      "Iteration: 792 lambda_n: 0.9373102912968748 Loss: 0.1335690850113473\n",
      "Iteration: 793 lambda_n: 0.9016510825496135 Loss: 0.13341425003229088\n",
      "Iteration: 794 lambda_n: 0.964020760062135 Loss: 0.133265479441138\n",
      "Iteration: 795 lambda_n: 0.9303711905688571 Loss: 0.1331065977885171\n",
      "Iteration: 796 lambda_n: 0.9315996326052506 Loss: 0.13295344579243307\n",
      "Iteration: 797 lambda_n: 1.0135448477821905 Loss: 0.13280026933762953\n",
      "Iteration: 798 lambda_n: 0.9711091467624243 Loss: 0.13263381370514707\n",
      "Iteration: 799 lambda_n: 0.8963075054005669 Loss: 0.13247452782172586\n",
      "Iteration: 800 lambda_n: 0.8991242649470896 Loss: 0.13232768773387985\n",
      "Iteration: 801 lambda_n: 0.9083518260741236 Loss: 0.13218055022120295\n",
      "Iteration: 802 lambda_n: 0.9525838577944431 Loss: 0.13203206869527212\n",
      "Iteration: 803 lambda_n: 1.038229210371481 Loss: 0.13187653297757249\n",
      "Iteration: 804 lambda_n: 1.0152753772735257 Loss: 0.1317072148010791\n",
      "Iteration: 805 lambda_n: 1.0332027417040994 Loss: 0.1315418526384834\n",
      "Iteration: 806 lambda_n: 0.9727899367132096 Loss: 0.1313737823516344\n",
      "Iteration: 807 lambda_n: 1.0323367497906313 Loss: 0.131215740737572\n",
      "Iteration: 808 lambda_n: 0.9990475302228022 Loss: 0.13104822760780044\n",
      "Iteration: 809 lambda_n: 1.0395164000418589 Loss: 0.13088632240728812\n",
      "Iteration: 810 lambda_n: 0.9818623646765849 Loss: 0.1307180672114669\n",
      "Iteration: 811 lambda_n: 1.0021929803056866 Loss: 0.1305593467639448\n",
      "Iteration: 812 lambda_n: 0.8977086408304957 Loss: 0.1303975361858136\n",
      "Iteration: 813 lambda_n: 0.9104146489332426 Loss: 0.13025277278036893\n",
      "Iteration: 814 lambda_n: 1.0189873213963865 Loss: 0.130106122804447\n",
      "Iteration: 815 lambda_n: 1.003501914445982 Loss: 0.12994216937675104\n",
      "Iteration: 816 lambda_n: 1.0386597716462498 Loss: 0.12978090954009838\n",
      "Iteration: 817 lambda_n: 1.0295395273241812 Loss: 0.1296142062421694\n",
      "Iteration: 818 lambda_n: 0.9094655169112655 Loss: 0.12944917727135558\n",
      "Iteration: 819 lambda_n: 0.9624106335163545 Loss: 0.12930357789137878\n",
      "Iteration: 820 lambda_n: 1.0370448425100653 Loss: 0.1291496748918186\n",
      "Iteration: 821 lambda_n: 1.036423105592185 Loss: 0.1289840335361368\n",
      "Iteration: 822 lambda_n: 0.9749617170617231 Loss: 0.12881870165652223\n",
      "Iteration: 823 lambda_n: 0.9558012483758525 Loss: 0.12866337048995635\n",
      "Iteration: 824 lambda_n: 1.0347628861817368 Loss: 0.1285112732046987\n",
      "Iteration: 825 lambda_n: 0.9988437125843659 Loss: 0.1283468042417337\n",
      "Iteration: 826 lambda_n: 0.9469606227280251 Loss: 0.1281882444426\n",
      "Iteration: 827 lambda_n: 0.9771895675372753 Loss: 0.12803810319714343\n",
      "Iteration: 828 lambda_n: 1.0070688190674146 Loss: 0.12788334848937846\n",
      "Iteration: 829 lambda_n: 0.9199352395713515 Loss: 0.12772405227719835\n",
      "Iteration: 830 lambda_n: 0.8966850292638803 Loss: 0.12757871592459843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 831 lambda_n: 1.0220319945849634 Loss: 0.12743721105433575\n",
      "Iteration: 832 lambda_n: 1.0132210883994461 Loss: 0.12727610320675747\n",
      "Iteration: 833 lambda_n: 1.0009917343362496 Loss: 0.12711658257496905\n",
      "Iteration: 834 lambda_n: 0.9851287421436962 Loss: 0.12695918112792168\n",
      "Iteration: 835 lambda_n: 1.025684880513714 Loss: 0.12680446208724078\n",
      "Iteration: 836 lambda_n: 0.9914010679382518 Loss: 0.12664356671506574\n",
      "Iteration: 837 lambda_n: 0.9244064056402281 Loss: 0.12648824221976362\n",
      "Iteration: 838 lambda_n: 0.924718822685819 Loss: 0.12634358693211675\n",
      "Iteration: 839 lambda_n: 1.0298261511248488 Loss: 0.12619904476373497\n",
      "Iteration: 840 lambda_n: 0.9323496239689059 Loss: 0.1260382552014912\n",
      "Iteration: 841 lambda_n: 0.9329932256765268 Loss: 0.12589286485682874\n",
      "Iteration: 842 lambda_n: 0.9229200228470578 Loss: 0.1257475381552932\n",
      "Iteration: 843 lambda_n: 0.9262782374342969 Loss: 0.12560394241464864\n",
      "Iteration: 844 lambda_n: 0.925189763944623 Loss: 0.12545998484994372\n",
      "Iteration: 845 lambda_n: 0.9021589989787223 Loss: 0.12531635718895803\n",
      "Iteration: 846 lambda_n: 0.9083158983852176 Loss: 0.12517646083580267\n",
      "Iteration: 847 lambda_n: 0.896937170060412 Loss: 0.12503576303174097\n",
      "Iteration: 848 lambda_n: 0.9640326282589935 Loss: 0.12489697969756264\n",
      "Iteration: 849 lambda_n: 1.0272464864194393 Loss: 0.12474797681809863\n",
      "Iteration: 850 lambda_n: 0.894640011825284 Loss: 0.1245893887503134\n",
      "Iteration: 851 lambda_n: 1.0470393763105854 Loss: 0.12445144152448326\n",
      "Iteration: 852 lambda_n: 0.9629031465609442 Loss: 0.12429017157085773\n",
      "Iteration: 853 lambda_n: 0.9880347989529377 Loss: 0.1241420460237368\n",
      "Iteration: 854 lambda_n: 0.9748851987254391 Loss: 0.12399023059365565\n",
      "Iteration: 855 lambda_n: 1.012364952015269 Loss: 0.12384061311686483\n",
      "Iteration: 856 lambda_n: 1.021370351570172 Loss: 0.12368542588087168\n",
      "Iteration: 857 lambda_n: 0.8933776314606763 Loss: 0.12352904840585288\n",
      "Iteration: 858 lambda_n: 0.905825195984701 Loss: 0.1233924329406146\n",
      "Iteration: 859 lambda_n: 0.9154498319413938 Loss: 0.12325406239376707\n",
      "Iteration: 860 lambda_n: 0.9777569837847693 Loss: 0.12311437341204501\n",
      "Iteration: 861 lambda_n: 0.9712836622150575 Loss: 0.12296534135508246\n",
      "Iteration: 862 lambda_n: 0.9464816891906305 Loss: 0.12281746900605481\n",
      "Iteration: 863 lambda_n: 0.9223037269852674 Loss: 0.12267353954694889\n",
      "Iteration: 864 lambda_n: 0.9918101816863264 Loss: 0.1225334450257482\n",
      "Iteration: 865 lambda_n: 0.9114468657836324 Loss: 0.12238295976481421\n",
      "Iteration: 866 lambda_n: 0.9432151612726802 Loss: 0.12224483040122683\n",
      "Iteration: 867 lambda_n: 0.9384161737239594 Loss: 0.12210204247684955\n",
      "Iteration: 868 lambda_n: 0.9711058601930571 Loss: 0.12196014075051842\n",
      "Iteration: 869 lambda_n: 0.9208739947080787 Loss: 0.12181346063430708\n",
      "Iteration: 870 lambda_n: 1.0447025349735857 Loss: 0.12167452792744628\n",
      "Iteration: 871 lambda_n: 0.9809096027088431 Loss: 0.12151708805790615\n",
      "Iteration: 872 lambda_n: 1.047067603504689 Loss: 0.12136944477169348\n",
      "Iteration: 873 lambda_n: 0.9587984410188202 Loss: 0.12121202861554567\n",
      "Iteration: 874 lambda_n: 0.9731103707841758 Loss: 0.12106806092782495\n",
      "Iteration: 875 lambda_n: 0.914870272271865 Loss: 0.12092211094966505\n",
      "Iteration: 876 lambda_n: 0.9630301107907661 Loss: 0.12078505372704254\n",
      "Iteration: 877 lambda_n: 0.9579269931837667 Loss: 0.12064093910450889\n",
      "Iteration: 878 lambda_n: 0.9685358479117433 Loss: 0.12049775189257533\n",
      "Iteration: 879 lambda_n: 0.9593409685989474 Loss: 0.12035314357335077\n",
      "Iteration: 880 lambda_n: 0.993719059897432 Loss: 0.12021007243829933\n",
      "Iteration: 881 lambda_n: 0.962533686854079 Loss: 0.120062043372975\n",
      "Iteration: 882 lambda_n: 1.0460860732604582 Loss: 0.119918828111688\n",
      "Iteration: 883 lambda_n: 0.9569163730722244 Loss: 0.11976335991027993\n",
      "Iteration: 884 lambda_n: 0.9545894966151793 Loss: 0.11962131868073905\n",
      "Iteration: 885 lambda_n: 0.9380679238885453 Loss: 0.11947978319414289\n",
      "Iteration: 886 lambda_n: 1.0061149232936284 Loss: 0.11934085406084534\n",
      "Iteration: 887 lambda_n: 0.9405188738870757 Loss: 0.11919201339615806\n",
      "Iteration: 888 lambda_n: 1.003285961762695 Loss: 0.11905304111056988\n",
      "Iteration: 889 lambda_n: 1.0096809547428531 Loss: 0.11890495995580296\n",
      "Iteration: 890 lambda_n: 0.9763748419325818 Loss: 0.11875611150877119\n",
      "Iteration: 891 lambda_n: 0.9013955764834429 Loss: 0.11861234401821796\n",
      "Iteration: 892 lambda_n: 1.0355829399587515 Loss: 0.11847976864579646\n",
      "Iteration: 893 lambda_n: 0.957596146366563 Loss: 0.11832762141508167\n",
      "Iteration: 894 lambda_n: 0.9529767536040675 Loss: 0.11818710239877075\n",
      "Iteration: 895 lambda_n: 0.9170382274923221 Loss: 0.11804741888616618\n",
      "Iteration: 896 lambda_n: 0.9381296865591967 Loss: 0.11791315335925841\n",
      "Iteration: 897 lambda_n: 0.9269674538459006 Loss: 0.117775948348417\n",
      "Iteration: 898 lambda_n: 1.0164434299733984 Loss: 0.11764052530271653\n",
      "Iteration: 899 lambda_n: 1.0149312667612571 Loss: 0.11749219386794536\n",
      "Iteration: 900 lambda_n: 0.949498836808221 Loss: 0.117344260050486\n",
      "Iteration: 901 lambda_n: 1.0105307621322117 Loss: 0.11720602750768748\n",
      "Iteration: 902 lambda_n: 0.9418635366798321 Loss: 0.1170590747485805\n",
      "Iteration: 903 lambda_n: 1.0299328318875778 Loss: 0.11692226909296524\n",
      "Iteration: 904 lambda_n: 0.9698213873731912 Loss: 0.11677283825952843\n",
      "Iteration: 905 lambda_n: 0.9385098447036188 Loss: 0.11663229791380277\n",
      "Iteration: 906 lambda_n: 0.9919202337154239 Loss: 0.11649644922147326\n",
      "Iteration: 907 lambda_n: 0.9180074809027092 Loss: 0.11635302826571287\n",
      "Iteration: 908 lambda_n: 0.9868734781203105 Loss: 0.11622044743922062\n",
      "Iteration: 909 lambda_n: 0.9258888043377217 Loss: 0.11607807523402806\n",
      "Iteration: 910 lambda_n: 0.9621203224282826 Loss: 0.11594465444647697\n",
      "Iteration: 911 lambda_n: 0.9424624029722974 Loss: 0.1158061635389021\n",
      "Iteration: 912 lambda_n: 0.9392388996683859 Loss: 0.11567065467818763\n",
      "Iteration: 913 lambda_n: 1.0194535101207818 Loss: 0.11553575812580817\n",
      "Iteration: 914 lambda_n: 1.015985325620926 Loss: 0.11538950308761102\n",
      "Iteration: 915 lambda_n: 0.9097623397270467 Loss: 0.11524391925108816\n",
      "Iteration: 916 lambda_n: 0.954327005847554 Loss: 0.1151137095747889\n",
      "Iteration: 917 lambda_n: 0.9246020592940676 Loss: 0.11497726745761744\n",
      "Iteration: 918 lambda_n: 1.0381035686896476 Loss: 0.11484522202268944\n",
      "Iteration: 919 lambda_n: 1.024084298038135 Loss: 0.11469712909501209\n",
      "Iteration: 920 lambda_n: 0.9448413912045084 Loss: 0.11455121285838654\n",
      "Iteration: 921 lambda_n: 0.9412469728759001 Loss: 0.1144167470659362\n",
      "Iteration: 922 lambda_n: 0.966553841350659 Loss: 0.11428294035134409\n",
      "Iteration: 923 lambda_n: 0.9251914402510707 Loss: 0.11414568722938642\n",
      "Iteration: 924 lambda_n: 0.9650058472266468 Loss: 0.11401445500757881\n",
      "Iteration: 925 lambda_n: 1.0025643447185764 Loss: 0.11387772357945349\n",
      "Iteration: 926 lambda_n: 0.9800048368266714 Loss: 0.11373583080623095\n",
      "Iteration: 927 lambda_n: 0.936587014584666 Loss: 0.11359729240487132\n",
      "Iteration: 928 lambda_n: 0.9035623201375301 Loss: 0.11346504209103449\n",
      "Iteration: 929 lambda_n: 0.9629663462780238 Loss: 0.1133375935419175\n",
      "Iteration: 930 lambda_n: 1.028350807988867 Loss: 0.11320190973709277\n",
      "Iteration: 931 lambda_n: 0.960787266215943 Loss: 0.1130571765679533\n",
      "Iteration: 932 lambda_n: 0.9289238736051683 Loss: 0.11292211303847342\n",
      "Iteration: 933 lambda_n: 0.9180171980534935 Loss: 0.11279167403732016\n",
      "Iteration: 934 lambda_n: 0.9966332770322738 Loss: 0.1126629055015366\n",
      "Iteration: 935 lambda_n: 0.9377275886322731 Loss: 0.1125232600710331\n",
      "Iteration: 936 lambda_n: 0.9447915939887838 Loss: 0.11239201944388183\n",
      "Iteration: 937 lambda_n: 0.9034548531314291 Loss: 0.11225993423128348\n",
      "Iteration: 938 lambda_n: 1.0152180956992662 Loss: 0.11213376588741887\n",
      "Iteration: 939 lambda_n: 0.9592662559883361 Loss: 0.11199214030339\n",
      "Iteration: 940 lambda_n: 0.9837910403899726 Loss: 0.11185847680509681\n",
      "Iteration: 941 lambda_n: 1.0470430173704393 Loss: 0.11172154895839403\n",
      "Iteration: 942 lambda_n: 1.0131542135587925 Loss: 0.11157598484119068\n",
      "Iteration: 943 lambda_n: 0.9910175689721402 Loss: 0.11143530242098669\n",
      "Iteration: 944 lambda_n: 0.9494794777511452 Loss: 0.11129785497127956\n",
      "Iteration: 945 lambda_n: 0.9904609697641478 Loss: 0.11116631906004816\n",
      "Iteration: 946 lambda_n: 0.988856461364962 Loss: 0.11102925738650171\n",
      "Iteration: 947 lambda_n: 1.0278766474247745 Loss: 0.11089257464938027\n",
      "Iteration: 948 lambda_n: 0.9669785540829391 Loss: 0.11075066176410131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 949 lambda_n: 1.0366504295742844 Loss: 0.11061731448873782\n",
      "Iteration: 950 lambda_n: 1.0259948887342534 Loss: 0.110474520610021\n",
      "Iteration: 951 lambda_n: 0.9690624173721228 Loss: 0.11033336375707398\n",
      "Iteration: 952 lambda_n: 0.9581269965758004 Loss: 0.11020019690688755\n",
      "Iteration: 953 lambda_n: 0.9535003311396995 Loss: 0.11006868010726052\n",
      "Iteration: 954 lambda_n: 0.925787635525023 Loss: 0.10993794325169032\n",
      "Iteration: 955 lambda_n: 0.9822926334801123 Loss: 0.10981114556339472\n",
      "Iteration: 956 lambda_n: 0.9779350762585745 Loss: 0.10967675373136998\n",
      "Iteration: 957 lambda_n: 0.9102866303746063 Loss: 0.10954310981271635\n",
      "Iteration: 958 lambda_n: 0.9211089218065932 Loss: 0.10941885007331156\n",
      "Iteration: 959 lambda_n: 0.9855399966756052 Loss: 0.10929324535263633\n",
      "Iteration: 960 lambda_n: 0.9473064131847377 Loss: 0.10915899868564535\n",
      "Iteration: 961 lambda_n: 1.026714246940769 Loss: 0.10903010618444697\n",
      "Iteration: 962 lambda_n: 0.8957442313460652 Loss: 0.10889056345684014\n",
      "Iteration: 963 lambda_n: 1.0406360241467083 Loss: 0.10876896321153014\n",
      "Iteration: 964 lambda_n: 1.035749331032796 Loss: 0.10862784210053217\n",
      "Iteration: 965 lambda_n: 1.00289475807986 Loss: 0.10848755215339378\n",
      "Iteration: 966 lambda_n: 1.0416792545029319 Loss: 0.1083518739154045\n",
      "Iteration: 967 lambda_n: 0.913850964774725 Loss: 0.1082111123119436\n",
      "Iteration: 968 lambda_n: 1.0277312499343287 Loss: 0.10808777022488593\n",
      "Iteration: 969 lambda_n: 0.905840648197078 Loss: 0.10794920595477998\n",
      "Iteration: 970 lambda_n: 1.0182229056390277 Loss: 0.10782721823954132\n",
      "Iteration: 971 lambda_n: 1.019355955660987 Loss: 0.10769024139201977\n",
      "Iteration: 972 lambda_n: 0.9798160588383463 Loss: 0.10755327295656976\n",
      "Iteration: 973 lambda_n: 0.9902653834192638 Loss: 0.10742177123594564\n",
      "Iteration: 974 lambda_n: 1.000479516942929 Loss: 0.10728901721926422\n",
      "Iteration: 975 lambda_n: 0.9516182070150397 Loss: 0.10715504698197434\n",
      "Iteration: 976 lambda_n: 0.9831100388519673 Loss: 0.10702776544807129\n",
      "Iteration: 977 lambda_n: 0.9162882627803757 Loss: 0.1068964163553643\n",
      "Iteration: 978 lambda_n: 0.9733153453577801 Loss: 0.10677413238232002\n",
      "Iteration: 979 lambda_n: 0.9296888312068523 Loss: 0.10664437575085274\n",
      "Iteration: 980 lambda_n: 0.9257429658291818 Loss: 0.10652057315559964\n",
      "Iteration: 981 lambda_n: 1.0400522651951218 Loss: 0.10639742775023707\n",
      "Iteration: 982 lambda_n: 0.9703127719806354 Loss: 0.10625922600667109\n",
      "Iteration: 983 lambda_n: 0.94742993176475 Loss: 0.10613044415244839\n",
      "Iteration: 984 lambda_n: 1.0117781293630568 Loss: 0.10600483921346325\n",
      "Iteration: 985 lambda_n: 1.0339296848777841 Loss: 0.10587085062673675\n",
      "Iteration: 986 lambda_n: 0.931252819575137 Loss: 0.10573408817466957\n",
      "Iteration: 987 lambda_n: 0.9768465156817844 Loss: 0.1056110518248105\n",
      "Iteration: 988 lambda_n: 1.0415068737684856 Loss: 0.10548213058746173\n",
      "Iteration: 989 lambda_n: 0.9685615689438919 Loss: 0.10534483117861461\n",
      "Iteration: 990 lambda_n: 1.00111657564666 Loss: 0.1052172994200266\n",
      "Iteration: 991 lambda_n: 0.9523808220597877 Loss: 0.1050856283455906\n",
      "Iteration: 992 lambda_n: 1.0040098429525743 Loss: 0.10496051034751644\n",
      "Iteration: 993 lambda_n: 1.0144004703066272 Loss: 0.1048287548542791\n",
      "Iteration: 994 lambda_n: 0.9412698017739667 Loss: 0.10469578945737469\n",
      "Iteration: 995 lambda_n: 0.956030678005255 Loss: 0.10457255226193465\n",
      "Iteration: 996 lambda_n: 0.8928096427392462 Loss: 0.10444751798104399\n",
      "Iteration: 997 lambda_n: 1.0433459898629882 Loss: 0.10433087911754066\n",
      "Iteration: 998 lambda_n: 0.9304152402102935 Loss: 0.10419471642559756\n",
      "Iteration: 999 lambda_n: 0.9908043297688788 Loss: 0.10407343525884746\n",
      "Iteration: 1000 lambda_n: 1.0178663807413872 Loss: 0.10394442128386029\n",
      "Iteration: 1001 lambda_n: 0.9215097287711324 Loss: 0.10381203471281242\n",
      "Iteration: 1002 lambda_n: 0.9152474861574188 Loss: 0.10369231884410655\n",
      "Iteration: 1003 lambda_n: 0.9764714796669665 Loss: 0.10357354210222282\n",
      "Iteration: 1004 lambda_n: 0.9228203483206533 Loss: 0.10344695415076159\n",
      "Iteration: 1005 lambda_n: 0.9188926164655298 Loss: 0.1033274544818074\n",
      "Iteration: 1006 lambda_n: 0.90387840700754 Loss: 0.10320858929645904\n",
      "Iteration: 1007 lambda_n: 0.9633352686651849 Loss: 0.10309178925666408\n",
      "Iteration: 1008 lambda_n: 1.0023257123189784 Loss: 0.10296743625787655\n",
      "Iteration: 1009 lambda_n: 1.0433159552331994 Loss: 0.10283819380988274\n",
      "Iteration: 1010 lambda_n: 0.9015909432961976 Loss: 0.10270382140060713\n",
      "Iteration: 1011 lambda_n: 0.9699119823030152 Loss: 0.10258783872942909\n",
      "Iteration: 1012 lambda_n: 0.9847829094075962 Loss: 0.10246319731765881\n",
      "Iteration: 1013 lambda_n: 1.0367248421870388 Loss: 0.10233678586734879\n",
      "Iteration: 1014 lambda_n: 0.9110246888404497 Loss: 0.10220385816622621\n",
      "Iteration: 1015 lambda_n: 0.9811716439350836 Loss: 0.10208718421984286\n",
      "Iteration: 1016 lambda_n: 1.0487715728382485 Loss: 0.10196165916292856\n",
      "Iteration: 1017 lambda_n: 1.0084716446153117 Loss: 0.10182763804194296\n",
      "Iteration: 1018 lambda_n: 0.9465656439196967 Loss: 0.10169892092411545\n",
      "Iteration: 1019 lambda_n: 0.8999040330592997 Loss: 0.10157824373994516\n",
      "Iteration: 1020 lambda_n: 0.9426579381828674 Loss: 0.10146363906643732\n",
      "Iteration: 1021 lambda_n: 0.907995042968495 Loss: 0.10134371417324596\n",
      "Iteration: 1022 lambda_n: 0.9285516750204463 Loss: 0.10122832326697821\n",
      "Iteration: 1023 lambda_n: 1.0155733443627366 Loss: 0.10111044310731498\n",
      "Iteration: 1024 lambda_n: 1.030745660580775 Loss: 0.10098165439212962\n",
      "Iteration: 1025 lambda_n: 0.8960493957658351 Loss: 0.10085109403941521\n",
      "Iteration: 1026 lambda_n: 0.9242160477196806 Loss: 0.10073772682455544\n",
      "Iteration: 1027 lambda_n: 0.9069308134837886 Loss: 0.10062091655384132\n",
      "Iteration: 1028 lambda_n: 0.9767170823471323 Loss: 0.1005064120176666\n",
      "Iteration: 1029 lambda_n: 0.9963060611932326 Loss: 0.10038322608913831\n",
      "Iteration: 1030 lambda_n: 0.9806088803140519 Loss: 0.10025771055887735\n",
      "Iteration: 1031 lambda_n: 1.007354081533797 Loss: 0.10013431330181774\n",
      "Iteration: 1032 lambda_n: 1.0346600462298443 Loss: 0.10000769345510951\n",
      "Iteration: 1033 lambda_n: 1.015122017800257 Loss: 0.09987779203838547\n",
      "Iteration: 1034 lambda_n: 0.9245275356999982 Loss: 0.0997504943262655\n",
      "Iteration: 1035 lambda_n: 0.9406115399049962 Loss: 0.09963469049659074\n",
      "Iteration: 1036 lambda_n: 0.8996963282103668 Loss: 0.09951699713294433\n",
      "Iteration: 1037 lambda_n: 0.9190313941550003 Loss: 0.09940454386074513\n",
      "Iteration: 1038 lambda_n: 1.0144014793318117 Loss: 0.09928979266140528\n",
      "Iteration: 1039 lambda_n: 0.9377477765731979 Loss: 0.09916326874493027\n",
      "Iteration: 1040 lambda_n: 0.9411615074934732 Loss: 0.09904644020479837\n",
      "Iteration: 1041 lambda_n: 1.0016469998851225 Loss: 0.09892931242809005\n",
      "Iteration: 1042 lambda_n: 0.954098870529852 Loss: 0.09880479285807098\n",
      "Iteration: 1043 lambda_n: 0.8949076455161012 Loss: 0.09868631947449842\n",
      "Iteration: 1044 lambda_n: 0.9421964220290715 Loss: 0.09857531649789583\n",
      "Iteration: 1045 lambda_n: 0.9544422896298872 Loss: 0.09845856863580617\n",
      "Iteration: 1046 lambda_n: 0.9774135050761137 Loss: 0.09834043132241146\n",
      "Iteration: 1047 lambda_n: 0.9702586812693611 Loss: 0.09821958350598002\n",
      "Iteration: 1048 lambda_n: 0.9133202097585637 Loss: 0.09809975457024683\n",
      "Iteration: 1049 lambda_n: 0.8940883696228061 Loss: 0.09798708205981249\n",
      "Iteration: 1050 lambda_n: 0.8921866800673789 Loss: 0.09787689722968063\n",
      "Iteration: 1051 lambda_n: 0.9661242785892907 Loss: 0.09776705941056357\n",
      "Iteration: 1052 lambda_n: 0.9678701843345292 Loss: 0.09764824215104141\n",
      "Iteration: 1053 lambda_n: 1.0464388797800146 Loss: 0.09752934204826991\n",
      "Iteration: 1054 lambda_n: 0.9094007672451127 Loss: 0.09740093430532054\n",
      "Iteration: 1055 lambda_n: 0.896942526775818 Loss: 0.09728947379860088\n",
      "Iteration: 1056 lambda_n: 0.9070006970825547 Loss: 0.09717965464006623\n",
      "Iteration: 1057 lambda_n: 0.9287642187407709 Loss: 0.09706871838693024\n",
      "Iteration: 1058 lambda_n: 0.953355317893866 Loss: 0.09695523877552978\n",
      "Iteration: 1059 lambda_n: 0.9716573858402395 Loss: 0.0968388790850936\n",
      "Iteration: 1060 lambda_n: 1.0405645382563211 Loss: 0.0967204156101461\n",
      "Iteration: 1061 lambda_n: 0.9125128829470249 Loss: 0.09659369389013422\n",
      "Iteration: 1062 lambda_n: 1.0109321759759822 Loss: 0.09648269680358633\n",
      "Iteration: 1063 lambda_n: 0.9654475944585368 Loss: 0.09635985884431507\n",
      "Iteration: 1064 lambda_n: 0.9597156019943313 Loss: 0.09624268287284533\n",
      "Iteration: 1065 lambda_n: 1.0444332343882023 Loss: 0.09612633151172752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1066 lambda_n: 0.9603044357385211 Loss: 0.09599985063483932\n",
      "Iteration: 1067 lambda_n: 0.8958491743986321 Loss: 0.09588369550501592\n",
      "Iteration: 1068 lambda_n: 0.9515338176401087 Loss: 0.09577545491407223\n",
      "Iteration: 1069 lambda_n: 0.9881844044868076 Loss: 0.09566060554921202\n",
      "Iteration: 1070 lambda_n: 0.9413588368092342 Loss: 0.09554146351071724\n",
      "Iteration: 1071 lambda_n: 0.9987476496691894 Loss: 0.09542809493222497\n",
      "Iteration: 1072 lambda_n: 1.027475735474808 Loss: 0.09530794615212328\n",
      "Iteration: 1073 lambda_n: 0.892366638894836 Loss: 0.09518448376548583\n",
      "Iteration: 1074 lambda_n: 0.9252211029032984 Loss: 0.09507738026066073\n",
      "Iteration: 1075 lambda_n: 0.9654353065133328 Loss: 0.09496644793934576\n",
      "Iteration: 1076 lambda_n: 0.960835078951321 Loss: 0.09485081782753352\n",
      "Iteration: 1077 lambda_n: 1.0317877620462543 Loss: 0.09473586622900722\n",
      "Iteration: 1078 lambda_n: 0.9991275977498262 Loss: 0.09461256383492\n",
      "Iteration: 1079 lambda_n: 0.9463273360785857 Loss: 0.09449330534281933\n",
      "Iteration: 1080 lambda_n: 1.000965867717713 Loss: 0.09438047789878493\n",
      "Iteration: 1081 lambda_n: 0.9277777072609744 Loss: 0.09426126702604748\n",
      "Iteration: 1082 lambda_n: 0.9989201753910972 Loss: 0.09415089829491481\n",
      "Iteration: 1083 lambda_n: 1.0318715394280127 Loss: 0.0940321946605251\n",
      "Iteration: 1084 lambda_n: 1.046553176521936 Loss: 0.09390971690301694\n",
      "Iteration: 1085 lambda_n: 0.9979383949061412 Loss: 0.09378564424629157\n",
      "Iteration: 1086 lambda_n: 1.0236321917656872 Loss: 0.09366747645646525\n",
      "Iteration: 1087 lambda_n: 0.9929627167830368 Loss: 0.09354640591606322\n",
      "Iteration: 1088 lambda_n: 1.0199359687625447 Loss: 0.09342910050685746\n",
      "Iteration: 1089 lambda_n: 1.0082418311723615 Loss: 0.09330874682033258\n",
      "Iteration: 1090 lambda_n: 0.9598567338614764 Loss: 0.09318991247180858\n",
      "Iteration: 1091 lambda_n: 1.028439147564731 Loss: 0.09307691126200517\n",
      "Iteration: 1092 lambda_n: 0.909115127721691 Loss: 0.09295597131599923\n",
      "Iteration: 1093 lambda_n: 0.9108826665542804 Loss: 0.09284918768098901\n",
      "Iteration: 1094 lambda_n: 1.0078762811096615 Loss: 0.0927423084836292\n",
      "Iteration: 1095 lambda_n: 0.9008650418604712 Loss: 0.09262417459081024\n",
      "Iteration: 1096 lambda_n: 1.016035207633693 Loss: 0.09251870413494712\n",
      "Iteration: 1097 lambda_n: 1.0437931870916606 Loss: 0.09239987585807145\n",
      "Iteration: 1098 lambda_n: 1.0377612604626856 Loss: 0.09227794475913086\n",
      "Iteration: 1099 lambda_n: 0.9285212575980695 Loss: 0.0921568640059801\n",
      "Iteration: 1100 lambda_n: 0.9774289545112965 Loss: 0.0920486562948267\n",
      "Iteration: 1101 lambda_n: 0.95543192646708 Loss: 0.09193487196230228\n",
      "Iteration: 1102 lambda_n: 0.9345981516059322 Loss: 0.0918237732780805\n",
      "Iteration: 1103 lambda_n: 1.0136697682311917 Loss: 0.09171521652623077\n",
      "Iteration: 1104 lambda_n: 0.9087376484015472 Loss: 0.09159760399759544\n",
      "Iteration: 1105 lambda_n: 0.9580092465244302 Loss: 0.09149228767749472\n",
      "Iteration: 1106 lambda_n: 1.0337726018994455 Loss: 0.09138137857731232\n",
      "Iteration: 1107 lambda_n: 1.0288012233819694 Loss: 0.09126183235257919\n",
      "Iteration: 1108 lambda_n: 1.029587579714034 Loss: 0.09114300292422974\n",
      "Iteration: 1109 lambda_n: 1.0430549963097235 Loss: 0.09102424703706076\n",
      "Iteration: 1110 lambda_n: 0.9427596820627383 Loss: 0.09090487484578899\n",
      "Iteration: 1111 lambda_n: 0.8941634467262949 Loss: 0.09079710565251901\n",
      "Iteration: 1112 lambda_n: 0.9127481733515382 Loss: 0.09069499924709223\n",
      "Iteration: 1113 lambda_n: 0.9719693753354406 Loss: 0.09059087600602637\n",
      "Iteration: 1114 lambda_n: 0.9118339454868704 Loss: 0.09048011233661185\n",
      "Iteration: 1115 lambda_n: 1.0102608467345895 Loss: 0.09037631429345513\n",
      "Iteration: 1116 lambda_n: 0.9631802848489986 Loss: 0.09026143231999649\n",
      "Iteration: 1117 lambda_n: 0.9210900108012481 Loss: 0.09015202800724596\n",
      "Iteration: 1118 lambda_n: 0.9431572361010713 Loss: 0.09004751749818952\n",
      "Iteration: 1119 lambda_n: 1.0044351394610858 Loss: 0.08994061490834632\n",
      "Iteration: 1120 lambda_n: 0.9237807655430385 Loss: 0.08982688936900626\n",
      "Iteration: 1121 lambda_n: 0.9606669894324401 Loss: 0.08972241288772646\n",
      "Iteration: 1122 lambda_n: 0.9456328945381504 Loss: 0.08961387891764173\n",
      "Iteration: 1123 lambda_n: 0.9071206714542841 Loss: 0.08950715920812098\n",
      "Iteration: 1124 lambda_n: 0.9278556123655923 Loss: 0.08940489454963457\n",
      "Iteration: 1125 lambda_n: 1.0237240596729549 Loss: 0.08930040011104474\n",
      "Iteration: 1126 lambda_n: 0.895041709396665 Loss: 0.08918523225825596\n",
      "Iteration: 1127 lambda_n: 0.9485046695149627 Loss: 0.08908465523745861\n",
      "Iteration: 1128 lambda_n: 1.0142348005217288 Loss: 0.08897817967681226\n",
      "Iteration: 1129 lambda_n: 0.9996301053019843 Loss: 0.0888644493228561\n",
      "Iteration: 1130 lambda_n: 0.9562400543133177 Loss: 0.08875248520510358\n",
      "Iteration: 1131 lambda_n: 0.9159891476284877 Loss: 0.0886455015120162\n",
      "Iteration: 1132 lambda_n: 0.9743340551893673 Loss: 0.08854313142630087\n",
      "Iteration: 1133 lambda_n: 0.9425579002261429 Loss: 0.08843435520274076\n",
      "Iteration: 1134 lambda_n: 1.032630402523415 Loss: 0.08832924225145579\n",
      "Iteration: 1135 lambda_n: 0.9717384233894429 Loss: 0.08821420983258148\n",
      "Iteration: 1136 lambda_n: 1.0020027508817757 Loss: 0.08810608631520928\n",
      "Iteration: 1137 lambda_n: 0.9946212820701994 Loss: 0.08799471914236874\n",
      "Iteration: 1138 lambda_n: 1.0253130578012561 Loss: 0.08788429815757126\n",
      "Iteration: 1139 lambda_n: 1.0379386886431725 Loss: 0.08777059930029081\n",
      "Iteration: 1140 lambda_n: 0.989319347785433 Loss: 0.08765563493947043\n",
      "Iteration: 1141 lambda_n: 0.9223681735688878 Loss: 0.08754618417008247\n",
      "Iteration: 1142 lambda_n: 1.0483036613353405 Loss: 0.08744425396547138\n",
      "Iteration: 1143 lambda_n: 0.9098739954745855 Loss: 0.08732853139300403\n",
      "Iteration: 1144 lambda_n: 0.9388992052078182 Loss: 0.0872282073176408\n",
      "Iteration: 1145 lambda_n: 0.9977143645417883 Loss: 0.08712479088109326\n",
      "Iteration: 1146 lambda_n: 0.9601973934577873 Loss: 0.08701501515794403\n",
      "Iteration: 1147 lambda_n: 1.0012535116545789 Loss: 0.0869094867944143\n",
      "Iteration: 1148 lambda_n: 1.0097309095572415 Loss: 0.08679956778352564\n",
      "Iteration: 1149 lambda_n: 1.0269837412700833 Loss: 0.08668884505497663\n",
      "Iteration: 1150 lambda_n: 1.027528413293631 Loss: 0.0865763607681039\n",
      "Iteration: 1151 lambda_n: 0.9256964789604913 Loss: 0.08646394894702883\n",
      "Iteration: 1152 lambda_n: 0.9415191387629144 Loss: 0.08636279443569848\n",
      "Iteration: 1153 lambda_n: 0.917664735062185 Loss: 0.08626002020359402\n",
      "Iteration: 1154 lambda_n: 0.9062994647903597 Loss: 0.08615995730758332\n",
      "Iteration: 1155 lambda_n: 0.9952532600458806 Loss: 0.08606123728256106\n",
      "Iteration: 1156 lambda_n: 0.9760126861069163 Loss: 0.08595294236023401\n",
      "Iteration: 1157 lambda_n: 0.8962129947199484 Loss: 0.08584686172043927\n",
      "Iteration: 1158 lambda_n: 1.022595429544419 Loss: 0.08574956169626709\n",
      "Iteration: 1159 lambda_n: 0.9048821326422165 Loss: 0.08563865761711399\n",
      "Iteration: 1160 lambda_n: 1.009738166664544 Loss: 0.08554063268499\n",
      "Iteration: 1161 lambda_n: 0.9011253819905984 Loss: 0.08543136481728374\n",
      "Iteration: 1162 lambda_n: 1.02265252055938 Loss: 0.08533396122363145\n",
      "Iteration: 1163 lambda_n: 0.9451123179267892 Loss: 0.08522353888842114\n",
      "Iteration: 1164 lambda_n: 0.97712547901239 Loss: 0.08512160734073927\n",
      "Iteration: 1165 lambda_n: 1.0227144400570463 Loss: 0.08501633836702592\n",
      "Iteration: 1166 lambda_n: 0.9644394298939856 Loss: 0.08490628285183197\n",
      "Iteration: 1167 lambda_n: 1.0196253576774197 Loss: 0.08480261925192331\n",
      "Iteration: 1168 lambda_n: 0.9761926398447577 Loss: 0.08469314693200722\n",
      "Iteration: 1169 lambda_n: 0.9933951868268234 Loss: 0.08458845990903702\n",
      "Iteration: 1170 lambda_n: 1.0017434751293097 Loss: 0.08448204832622111\n",
      "Iteration: 1171 lambda_n: 0.9275802795069742 Loss: 0.08437486558224766\n",
      "Iteration: 1172 lambda_n: 0.9009593452668135 Loss: 0.08427573112496718\n",
      "Iteration: 1173 lambda_n: 0.9786022316254195 Loss: 0.08417954428465298\n",
      "Iteration: 1174 lambda_n: 0.9834221692741777 Loss: 0.08407517868030014\n",
      "Iteration: 1175 lambda_n: 0.9656058569399351 Loss: 0.08397041770597478\n",
      "Iteration: 1176 lambda_n: 1.0339603205751824 Loss: 0.08386767115658941\n",
      "Iteration: 1177 lambda_n: 0.9284929192985758 Loss: 0.0837577757256324\n",
      "Iteration: 1178 lambda_n: 0.9205829203940419 Loss: 0.08365920576574609\n",
      "Iteration: 1179 lambda_n: 1.0210219737855954 Loss: 0.08356158038900796\n",
      "Iteration: 1180 lambda_n: 0.9489181279033737 Loss: 0.08345342150592294\n",
      "Iteration: 1181 lambda_n: 0.9732119259180877 Loss: 0.08335301803293856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1182 lambda_n: 0.9664651415314115 Loss: 0.08325015787069251\n",
      "Iteration: 1183 lambda_n: 0.9910406474351997 Loss: 0.08314812590544989\n",
      "Iteration: 1184 lambda_n: 0.9905443373060135 Loss: 0.08304361732235596\n",
      "Iteration: 1185 lambda_n: 1.0104528862523234 Loss: 0.08293928122111198\n",
      "Iteration: 1186 lambda_n: 0.9102038889610955 Loss: 0.08283297101071596\n",
      "Iteration: 1187 lambda_n: 0.9733437239386529 Loss: 0.08273731833832966\n",
      "Iteration: 1188 lambda_n: 0.9586938761675119 Loss: 0.08263514000803746\n",
      "Iteration: 1189 lambda_n: 1.0048101402743324 Loss: 0.082534613165266\n",
      "Iteration: 1190 lambda_n: 0.8973093370485156 Loss: 0.08242936927800054\n",
      "Iteration: 1191 lambda_n: 1.0293128317015443 Loss: 0.08233549270957985\n",
      "Iteration: 1192 lambda_n: 0.9317448291055186 Loss: 0.08222792166541988\n",
      "Iteration: 1193 lambda_n: 0.9284867209356552 Loss: 0.08213066188605515\n",
      "Iteration: 1194 lambda_n: 0.9425638577051582 Loss: 0.08203384742024591\n",
      "Iteration: 1195 lambda_n: 0.9311550637234297 Loss: 0.08193567187469863\n",
      "Iteration: 1196 lambda_n: 0.9508998832850277 Loss: 0.08183879108670793\n",
      "Iteration: 1197 lambda_n: 0.9615003832242249 Loss: 0.08173996398940186\n",
      "Iteration: 1198 lambda_n: 0.9028377432318594 Loss: 0.08164014643410472\n",
      "Iteration: 1199 lambda_n: 0.9401195752208633 Loss: 0.08154652298690586\n",
      "Iteration: 1200 lambda_n: 0.9668462568625813 Loss: 0.08144913720976794\n",
      "Iteration: 1201 lambda_n: 0.9284800393117998 Loss: 0.08134909363129385\n",
      "Iteration: 1202 lambda_n: 1.0371293100178258 Loss: 0.08125312785825078\n",
      "Iteration: 1203 lambda_n: 1.031230443550291 Loss: 0.08114605157695914\n",
      "Iteration: 1204 lambda_n: 1.0172630952552086 Loss: 0.08103971357296848\n",
      "Iteration: 1205 lambda_n: 1.0113277522268755 Loss: 0.0809349423546341\n",
      "Iteration: 1206 lambda_n: 0.9227291179114233 Loss: 0.08083090661024854\n",
      "Iteration: 1207 lambda_n: 0.9404729540409866 Loss: 0.08073609578610878\n",
      "Iteration: 1208 lambda_n: 0.9754165730152609 Loss: 0.08063956690306064\n",
      "Iteration: 1209 lambda_n: 1.0495162844594268 Loss: 0.0805395629175962\n",
      "Iteration: 1210 lambda_n: 0.897556005105797 Loss: 0.08043208717776332\n",
      "Iteration: 1211 lambda_n: 1.0006881188073924 Loss: 0.08034028315043902\n",
      "Iteration: 1212 lambda_n: 1.0442571445386246 Loss: 0.08023804115844592\n",
      "Iteration: 1213 lambda_n: 1.0359010948788974 Loss: 0.08013147448920199\n",
      "Iteration: 1214 lambda_n: 0.9710385371548573 Loss: 0.08002589044503891\n",
      "Iteration: 1215 lambda_n: 0.9212448384798337 Loss: 0.07992703688477593\n",
      "Iteration: 1216 lambda_n: 0.9066881461830426 Loss: 0.0798333587563023\n",
      "Iteration: 1217 lambda_n: 0.9167429094787036 Loss: 0.07974126079337295\n",
      "Iteration: 1218 lambda_n: 0.9826417572518787 Loss: 0.07964824144935938\n",
      "Iteration: 1219 lambda_n: 1.0190801023422076 Loss: 0.07954864511926077\n",
      "Iteration: 1220 lambda_n: 1.0385170222210736 Loss: 0.07944547646987035\n",
      "Iteration: 1221 lambda_n: 1.001226459006843 Loss: 0.07934046734052837\n",
      "Iteration: 1222 lambda_n: 1.0148487589210318 Loss: 0.07923935239075512\n",
      "Iteration: 1223 lambda_n: 1.0411587495043895 Loss: 0.07913698360480852\n",
      "Iteration: 1224 lambda_n: 0.9418026146137446 Loss: 0.07903208790518414\n",
      "Iteration: 1225 lambda_n: 0.9258850776344526 Loss: 0.0789373171237738\n",
      "Iteration: 1226 lambda_n: 0.916023508168353 Loss: 0.07884425184703844\n",
      "Iteration: 1227 lambda_n: 0.9286679529573733 Loss: 0.07875227880853031\n",
      "Iteration: 1228 lambda_n: 1.0345310533907701 Loss: 0.07865913796978775\n",
      "Iteration: 1229 lambda_n: 0.9542378235480272 Loss: 0.07855549680810853\n",
      "Iteration: 1230 lambda_n: 0.9171645174521614 Loss: 0.07846001532238767\n",
      "Iteration: 1231 lambda_n: 0.9303541609482654 Loss: 0.07836834679985512\n",
      "Iteration: 1232 lambda_n: 0.9036316657063738 Loss: 0.07827546186432027\n",
      "Iteration: 1233 lambda_n: 0.9752173141444826 Loss: 0.07818534427138486\n",
      "Iteration: 1234 lambda_n: 0.977219634903232 Loss: 0.07808819406460896\n",
      "Iteration: 1235 lambda_n: 1.0385557411415076 Loss: 0.07799095769083293\n",
      "Iteration: 1236 lambda_n: 0.9519995996551793 Loss: 0.07788774030219132\n",
      "Iteration: 1237 lambda_n: 1.0370808806070535 Loss: 0.077793240671675\n",
      "Iteration: 1238 lambda_n: 1.0491115810537865 Loss: 0.07769041481849412\n",
      "Iteration: 1239 lambda_n: 0.9518307992005072 Loss: 0.07758652545812629\n",
      "Iteration: 1240 lambda_n: 0.9388810787743934 Loss: 0.07749238547091877\n",
      "Iteration: 1241 lambda_n: 0.9650292846327526 Loss: 0.07739963180306904\n",
      "Iteration: 1242 lambda_n: 0.9662607438857004 Loss: 0.07730440278808542\n",
      "Iteration: 1243 lambda_n: 1.0011318845003847 Loss: 0.07720916260504398\n",
      "Iteration: 1244 lambda_n: 0.9583878424207416 Loss: 0.07711060058254905\n",
      "Iteration: 1245 lambda_n: 0.9399847040829207 Loss: 0.07701635911165153\n",
      "Iteration: 1246 lambda_n: 0.9734940043785452 Loss: 0.07692403326169416\n",
      "Iteration: 1247 lambda_n: 0.9793518560734782 Loss: 0.07682852495572484\n",
      "Iteration: 1248 lambda_n: 1.0189286639951685 Loss: 0.07673255458615452\n",
      "Iteration: 1249 lambda_n: 0.9672440005233173 Loss: 0.07663282463562703\n",
      "Iteration: 1250 lambda_n: 0.9602560418874667 Loss: 0.07653826835644456\n",
      "Iteration: 1251 lambda_n: 0.9573637584282084 Loss: 0.07644450446103727\n",
      "Iteration: 1252 lambda_n: 0.8974311713710874 Loss: 0.07635113115780119\n",
      "Iteration: 1253 lambda_n: 1.0242074336727442 Loss: 0.07626370293726545\n",
      "Iteration: 1254 lambda_n: 0.9199830597598314 Loss: 0.07616403549293224\n",
      "Iteration: 1255 lambda_n: 0.9586767400083889 Loss: 0.07607461866899783\n",
      "Iteration: 1256 lambda_n: 0.968228360148551 Loss: 0.07598154562203406\n",
      "Iteration: 1257 lambda_n: 0.9381184495784194 Loss: 0.07588765450675124\n",
      "Iteration: 1258 lambda_n: 0.9335309770062956 Loss: 0.07579678912072507\n",
      "Iteration: 1259 lambda_n: 0.9663586256251802 Loss: 0.07570647071400205\n",
      "Iteration: 1260 lambda_n: 0.9374728912751807 Loss: 0.07561308285234608\n",
      "Iteration: 1261 lambda_n: 1.0322126155164637 Loss: 0.07552259197454242\n",
      "Iteration: 1262 lambda_n: 1.0079137641822071 Loss: 0.07542307206411203\n",
      "Iteration: 1263 lambda_n: 1.0473693194813332 Loss: 0.07532601610976254\n",
      "Iteration: 1264 lambda_n: 1.02658294607685 Loss: 0.07522528542764403\n",
      "Iteration: 1265 lambda_n: 0.9340252709291901 Loss: 0.07512667910130102\n",
      "Iteration: 1266 lambda_n: 0.9475845996805308 Loss: 0.07503707310238665\n",
      "Iteration: 1267 lambda_n: 0.9934381597467921 Loss: 0.07494627002750794\n",
      "Iteration: 1268 lambda_n: 0.9913371947607437 Loss: 0.0748511841041555\n",
      "Iteration: 1269 lambda_n: 0.9909492041308418 Loss: 0.07475641418139728\n",
      "Iteration: 1270 lambda_n: 1.0123398261017564 Loss: 0.07466179595570195\n",
      "Iteration: 1271 lambda_n: 0.9165174154188434 Loss: 0.07456525284331943\n",
      "Iteration: 1272 lambda_n: 0.9777217356443642 Loss: 0.07447795384407052\n",
      "Iteration: 1273 lambda_n: 0.9617186964621692 Loss: 0.07438493101512221\n",
      "Iteration: 1274 lambda_n: 1.031814894544475 Loss: 0.07429353984556288\n",
      "Iteration: 1275 lambda_n: 1.022149652363448 Loss: 0.07419560486024529\n",
      "Iteration: 1276 lambda_n: 0.9031285876470485 Loss: 0.07409870968814676\n",
      "Iteration: 1277 lambda_n: 0.9593786052257277 Loss: 0.07401320172445987\n",
      "Iteration: 1278 lambda_n: 0.9678390267358192 Loss: 0.07392247010613354\n",
      "Iteration: 1279 lambda_n: 1.0147753625563563 Loss: 0.07383104642153861\n",
      "Iteration: 1280 lambda_n: 0.9313018553071399 Loss: 0.07373530431492425\n",
      "Iteration: 1281 lambda_n: 0.9840018876672737 Loss: 0.07364754542208246\n",
      "Iteration: 1282 lambda_n: 0.8949806156587591 Loss: 0.07355492811066541\n",
      "Iteration: 1283 lambda_n: 0.902732254526739 Loss: 0.07347078977842302\n",
      "Iteration: 1284 lambda_n: 0.9433948412715875 Loss: 0.07338601650203379\n",
      "Iteration: 1285 lambda_n: 1.031737653015083 Loss: 0.07329752434453948\n",
      "Iteration: 1286 lambda_n: 0.9380078488883995 Loss: 0.07320086053830448\n",
      "Iteration: 1287 lambda_n: 0.8942442332802705 Loss: 0.07311308799848139\n",
      "Iteration: 1288 lambda_n: 0.9269017829343105 Loss: 0.07302950655803792\n",
      "Iteration: 1289 lambda_n: 1.0045641278665833 Loss: 0.07294296936289497\n",
      "Iteration: 1290 lambda_n: 1.0264990363027955 Loss: 0.07284929112100487\n",
      "Iteration: 1291 lambda_n: 1.020677049120354 Loss: 0.07275368709533495\n",
      "Iteration: 1292 lambda_n: 0.9311707958160766 Loss: 0.07265874611526131\n",
      "Iteration: 1293 lambda_n: 0.9528074085216522 Loss: 0.07257223827715661\n",
      "Iteration: 1294 lambda_n: 0.9245015131101778 Loss: 0.07248382322302103\n",
      "Iteration: 1295 lambda_n: 0.9412124306935976 Loss: 0.07239813565491482\n",
      "Iteration: 1296 lambda_n: 1.031082185297592 Loss: 0.07231099991854553\n",
      "Iteration: 1297 lambda_n: 0.9660155153313512 Loss: 0.07221565843791074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1298 lambda_n: 0.9575274298926965 Loss: 0.0721264465182772\n",
      "Iteration: 1299 lambda_n: 0.9792646716719697 Loss: 0.07203812470613034\n",
      "Iteration: 1300 lambda_n: 0.9790186510559902 Loss: 0.07194790628338733\n",
      "Iteration: 1301 lambda_n: 1.02762468796522 Loss: 0.07185782074780933\n",
      "Iteration: 1302 lambda_n: 1.0190717210101512 Loss: 0.07176337961968735\n",
      "Iteration: 1303 lambda_n: 0.9056872885411631 Loss: 0.0716698445939063\n",
      "Iteration: 1304 lambda_n: 1.011046891670431 Loss: 0.07158681973708697\n",
      "Iteration: 1305 lambda_n: 0.9218379385896905 Loss: 0.07149424443303126\n",
      "Iteration: 1306 lambda_n: 0.9341764181728188 Loss: 0.07140994215737342\n",
      "Iteration: 1307 lambda_n: 0.9109102964504938 Loss: 0.07132461057003658\n",
      "Iteration: 1308 lambda_n: 0.9105830317230209 Loss: 0.0712415011650741\n",
      "Iteration: 1309 lambda_n: 0.9469480721425718 Loss: 0.07115851663130174\n",
      "Iteration: 1310 lambda_n: 1.0138000820000574 Loss: 0.07107231769056852\n",
      "Iteration: 1311 lambda_n: 1.022687464119788 Loss: 0.0709801450436168\n",
      "Iteration: 1312 lambda_n: 0.9482057210081573 Loss: 0.07088728324796706\n",
      "Iteration: 1313 lambda_n: 0.9625620440863858 Loss: 0.07080129352407383\n",
      "Iteration: 1314 lambda_n: 0.9888204040482669 Loss: 0.07071410657502604\n",
      "Iteration: 1315 lambda_n: 0.9598998678002281 Loss: 0.0706246506513017\n",
      "Iteration: 1316 lambda_n: 0.9344449117129112 Loss: 0.07053791874797546\n",
      "Iteration: 1317 lambda_n: 0.9934705973976604 Loss: 0.07045358858683397\n",
      "Iteration: 1318 lambda_n: 0.9182666703925603 Loss: 0.07036403910067432\n",
      "Iteration: 1319 lambda_n: 0.9159815957203811 Loss: 0.07028137053504435\n",
      "Iteration: 1320 lambda_n: 0.9881211907014217 Loss: 0.07019900352726674\n",
      "Iteration: 1321 lambda_n: 0.921632607485567 Loss: 0.07011025466145447\n",
      "Iteration: 1322 lambda_n: 1.0196351743503982 Loss: 0.07002757959959002\n",
      "Iteration: 1323 lambda_n: 0.9933412215392273 Loss: 0.06993622294518125\n",
      "Iteration: 1324 lambda_n: 1.016616020495111 Loss: 0.06984733673961915\n",
      "Iteration: 1325 lambda_n: 1.017723842350731 Loss: 0.06975648340068238\n",
      "Iteration: 1326 lambda_n: 1.0287404013751094 Loss: 0.06966564875608913\n",
      "Iteration: 1327 lambda_n: 0.9180437335992304 Loss: 0.06957395018195955\n",
      "Iteration: 1328 lambda_n: 0.8944443142564026 Loss: 0.06949222327866234\n",
      "Iteration: 1329 lambda_n: 0.9491159862864875 Loss: 0.06941268994268542\n",
      "Iteration: 1330 lambda_n: 0.9098884788776302 Loss: 0.06932839304073547\n",
      "Iteration: 1331 lambda_n: 0.8997131808651277 Loss: 0.0692476772009146\n",
      "Iteration: 1332 lambda_n: 0.96461479494253 Loss: 0.06916795661847747\n",
      "Iteration: 1333 lambda_n: 1.0219046321751446 Loss: 0.0690825854537589\n",
      "Iteration: 1334 lambda_n: 0.9042340917096691 Loss: 0.06899225732410953\n",
      "Iteration: 1335 lambda_n: 0.998397756100044 Loss: 0.06891243208534215\n",
      "Iteration: 1336 lambda_n: 0.9222381007187194 Loss: 0.0688243989743963\n",
      "Iteration: 1337 lambda_n: 0.9751080228176371 Loss: 0.06874318349393699\n",
      "Iteration: 1338 lambda_n: 1.0156378244389046 Loss: 0.06865741529616261\n",
      "Iteration: 1339 lambda_n: 0.9339308005383378 Loss: 0.06856819536323626\n",
      "Iteration: 1340 lambda_n: 1.003993809770032 Loss: 0.06848625822132956\n",
      "Iteration: 1341 lambda_n: 0.8968529795585867 Loss: 0.06839828212848487\n",
      "Iteration: 1342 lambda_n: 0.9883838596953964 Loss: 0.06831979343299996\n",
      "Iteration: 1343 lambda_n: 0.9071459012970906 Loss: 0.06823339697339295\n",
      "Iteration: 1344 lambda_n: 1.0081676768136947 Loss: 0.06815420083977689\n",
      "Iteration: 1345 lambda_n: 1.0295307954477573 Loss: 0.06806629129028101\n",
      "Iteration: 1346 lambda_n: 0.9520788356189498 Loss: 0.06797663663225084\n",
      "Iteration: 1347 lambda_n: 0.951017742375613 Loss: 0.06789383518847691\n",
      "Iteration: 1348 lambda_n: 0.9704028410515917 Loss: 0.06781122804778189\n",
      "Iteration: 1349 lambda_n: 0.9298594988871146 Loss: 0.06772704158142812\n",
      "Iteration: 1350 lambda_n: 1.015076235057139 Loss: 0.06764647298069806\n",
      "Iteration: 1351 lambda_n: 0.903675538914527 Loss: 0.06755862937477082\n",
      "Iteration: 1352 lambda_n: 1.0013183677187503 Loss: 0.06748052666229593\n",
      "Iteration: 1353 lambda_n: 0.9175073479718345 Loss: 0.0673940894244863\n",
      "Iteration: 1354 lambda_n: 0.9951823936354921 Loss: 0.06731498818035962\n",
      "Iteration: 1355 lambda_n: 0.9680248197222493 Loss: 0.06722929512864385\n",
      "Iteration: 1356 lambda_n: 1.0012700136913613 Loss: 0.06714604805703517\n",
      "Iteration: 1357 lambda_n: 0.9717406272485363 Loss: 0.06706005180301115\n",
      "Iteration: 1358 lambda_n: 0.9850752061022683 Loss: 0.0669767001678057\n",
      "Iteration: 1359 lambda_n: 1.0413207455523728 Loss: 0.06689231254685828\n",
      "Iteration: 1360 lambda_n: 0.9106148721800297 Loss: 0.06680322331890923\n",
      "Iteration: 1361 lambda_n: 0.9398884531335665 Loss: 0.06672541946557588\n",
      "Iteration: 1362 lambda_n: 0.9566419419345369 Loss: 0.06664521116035142\n",
      "Iteration: 1363 lambda_n: 0.9413962356181462 Loss: 0.0665636743444718\n",
      "Iteration: 1364 lambda_n: 1.0268640925270038 Loss: 0.0664835374161585\n",
      "Iteration: 1365 lambda_n: 1.0322204057521194 Loss: 0.06639623571893906\n",
      "Iteration: 1366 lambda_n: 1.0189870867234454 Loss: 0.0663085974332936\n",
      "Iteration: 1367 lambda_n: 0.9732785751155292 Loss: 0.06622219998485968\n",
      "Iteration: 1368 lambda_n: 0.9675052969164722 Loss: 0.066139787690415\n",
      "Iteration: 1369 lambda_n: 0.9908575409113654 Loss: 0.06605796935249429\n",
      "Iteration: 1370 lambda_n: 0.9069973292945236 Loss: 0.06597428399012037\n",
      "Iteration: 1371 lambda_n: 0.9665682086937438 Loss: 0.06589777945798019\n",
      "Iteration: 1372 lambda_n: 0.9013759437206158 Loss: 0.0658163497075426\n",
      "Iteration: 1373 lambda_n: 0.994146562729046 Loss: 0.06574050771895576\n",
      "Iteration: 1374 lambda_n: 0.9114388838326426 Loss: 0.06565696262521542\n",
      "Iteration: 1375 lambda_n: 1.0155281754266372 Loss: 0.06558046693175183\n",
      "Iteration: 1376 lambda_n: 1.0218092045442388 Loss: 0.06549534142811692\n",
      "Iteration: 1377 lambda_n: 0.9899835347747288 Loss: 0.06540980527015253\n",
      "Iteration: 1378 lambda_n: 1.0141389371602245 Loss: 0.06532704505136572\n",
      "Iteration: 1379 lambda_n: 0.9042893470286871 Loss: 0.06524237802484274\n",
      "Iteration: 1380 lambda_n: 0.8969555521375988 Loss: 0.06516698118974844\n",
      "Iteration: 1381 lambda_n: 0.9616558794322804 Loss: 0.06509228589477886\n",
      "Iteration: 1382 lambda_n: 0.993350301852501 Loss: 0.06501230034610914\n",
      "Iteration: 1383 lambda_n: 0.9146218732281851 Loss: 0.06492978576972637\n",
      "Iteration: 1384 lambda_n: 0.9693679355389242 Loss: 0.06485390983207412\n",
      "Iteration: 1385 lambda_n: 0.9994623301008114 Loss: 0.06477359229659659\n",
      "Iteration: 1386 lambda_n: 0.9304827301343855 Loss: 0.06469088977973267\n",
      "Iteration: 1387 lambda_n: 1.024046441710177 Loss: 0.06461399652115504\n",
      "Iteration: 1388 lambda_n: 0.9949929911794072 Loss: 0.06452947991512281\n",
      "Iteration: 1389 lambda_n: 0.9253388813533163 Loss: 0.06444747337611645\n",
      "Iteration: 1390 lambda_n: 0.9160143318286155 Loss: 0.06437130791993628\n",
      "Iteration: 1391 lambda_n: 0.896385867516293 Loss: 0.06429600371339327\n",
      "Iteration: 1392 lambda_n: 0.8929311550188931 Loss: 0.06422240363378748\n",
      "Iteration: 1393 lambda_n: 0.9723478931128292 Loss: 0.06414917581281404\n",
      "Iteration: 1394 lambda_n: 0.9509935200873818 Loss: 0.06406953360073157\n",
      "Iteration: 1395 lambda_n: 0.9826812973160933 Loss: 0.06399174226591174\n",
      "Iteration: 1396 lambda_n: 1.0417931733731907 Loss: 0.06391146323618158\n",
      "Iteration: 1397 lambda_n: 1.0493658875660772 Loss: 0.06382647023546084\n",
      "Iteration: 1398 lambda_n: 0.9619483031051883 Loss: 0.06374098060026334\n",
      "Iteration: 1399 lambda_n: 1.028910645076194 Loss: 0.06366272183355261\n",
      "Iteration: 1400 lambda_n: 0.8922803767400939 Loss: 0.06357912682635217\n",
      "Iteration: 1401 lambda_n: 1.0135020939180848 Loss: 0.06350673033731843\n",
      "Iteration: 1402 lambda_n: 0.897091053150654 Loss: 0.06342460187318713\n",
      "Iteration: 1403 lambda_n: 1.0451339833769924 Loss: 0.0633520039980893\n",
      "Iteration: 1404 lambda_n: 0.9318254617956129 Loss: 0.06326753370477135\n",
      "Iteration: 1405 lambda_n: 0.9159436731842786 Loss: 0.06319232561174369\n",
      "Iteration: 1406 lambda_n: 0.9640759237908291 Loss: 0.06311849305543893\n",
      "Iteration: 1407 lambda_n: 1.0012949729125893 Loss: 0.06304087939674263\n",
      "Iteration: 1408 lambda_n: 0.9227329321122135 Loss: 0.06296037691164366\n",
      "Iteration: 1409 lambda_n: 1.0301589886695433 Loss: 0.06288629033453877\n",
      "Iteration: 1410 lambda_n: 0.9802381128264268 Loss: 0.06280368649374483\n",
      "Iteration: 1411 lambda_n: 0.9471098498507293 Loss: 0.06272519532187265\n",
      "Iteration: 1412 lambda_n: 1.0255912728702392 Loss: 0.06264945810538665\n",
      "Iteration: 1413 lambda_n: 0.9043130947156902 Loss: 0.06256755429199055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1414 lambda_n: 0.977504532687837 Loss: 0.06249543449910373\n",
      "Iteration: 1415 lambda_n: 0.9866447778171064 Loss: 0.06241757697431119\n",
      "Iteration: 1416 lambda_n: 0.9149225442230892 Loss: 0.062339097729148286\n",
      "Iteration: 1417 lambda_n: 1.0306781423182954 Loss: 0.062266452796973036\n",
      "Iteration: 1418 lambda_n: 0.9943661622593603 Loss: 0.06218530243881379\n",
      "Iteration: 1419 lambda_n: 0.9156116038877132 Loss: 0.062107119720120174\n",
      "Iteration: 1420 lambda_n: 0.98583433832032 Loss: 0.06203522438860263\n",
      "Iteration: 1421 lambda_n: 0.9955802979348026 Loss: 0.061957913717554096\n",
      "Iteration: 1422 lambda_n: 1.0293943096021827 Loss: 0.06187994403491364\n",
      "Iteration: 1423 lambda_n: 0.922120759939784 Loss: 0.06179943683773006\n",
      "Iteration: 1424 lambda_n: 0.9846838409361865 Loss: 0.061727417816957536\n",
      "Iteration: 1425 lambda_n: 0.9304904638224497 Loss: 0.06165061145195515\n",
      "Iteration: 1426 lambda_n: 0.9766013347548484 Loss: 0.061578128613292316\n",
      "Iteration: 1427 lambda_n: 0.9172496053177602 Loss: 0.06150215231567338\n",
      "Iteration: 1428 lambda_n: 0.8937400956659642 Loss: 0.061430887415237126\n",
      "Iteration: 1429 lambda_n: 1.0103501178333107 Loss: 0.06136153597347757\n",
      "Iteration: 1430 lambda_n: 0.9293955700133758 Loss: 0.061283235966380606\n",
      "Iteration: 1431 lambda_n: 0.9189723561919276 Loss: 0.06121130771313688\n",
      "Iteration: 1432 lambda_n: 0.989198488167866 Loss: 0.06114027699900852\n",
      "Iteration: 1433 lambda_n: 0.9410792784002494 Loss: 0.06106391737551135\n",
      "Iteration: 1434 lambda_n: 0.9880177677886738 Loss: 0.060991370165023415\n",
      "Iteration: 1435 lambda_n: 0.985900714471382 Loss: 0.06091530508808904\n",
      "Iteration: 1436 lambda_n: 1.0016651957807692 Loss: 0.06083950680725986\n",
      "Iteration: 1437 lambda_n: 0.9846086353933082 Loss: 0.060762602286924865\n",
      "Iteration: 1438 lambda_n: 0.8952576914427383 Loss: 0.06068711191919955\n",
      "Iteration: 1439 lambda_n: 0.9945333342474342 Loss: 0.06061856359678229\n",
      "Iteration: 1440 lambda_n: 0.9855999941225291 Loss: 0.06054251180877299\n",
      "Iteration: 1441 lambda_n: 0.9317645613255031 Loss: 0.06046724729066101\n",
      "Iteration: 1442 lambda_n: 1.0184558192957422 Loss: 0.06039619011894511\n",
      "Iteration: 1443 lambda_n: 1.0423161738320665 Loss: 0.060318625559394864\n",
      "Iteration: 1444 lambda_n: 1.045996229409592 Loss: 0.06023935759101204\n",
      "Iteration: 1445 lambda_n: 0.9499005946368536 Loss: 0.06015992590643387\n",
      "Iteration: 1446 lambda_n: 0.9378444322231063 Loss: 0.06008789449549551\n",
      "Iteration: 1447 lambda_n: 0.952100269121834 Loss: 0.06001687171855011\n",
      "Iteration: 1448 lambda_n: 0.8961191145235247 Loss: 0.05994486473059325\n",
      "Iteration: 1449 lambda_n: 0.9610010564279353 Loss: 0.059877180731518806\n",
      "Iteration: 1450 lambda_n: 0.9954409253903947 Loss: 0.059804689673000296\n",
      "Iteration: 1451 lambda_n: 0.9477939278316186 Loss: 0.059729703455723794\n",
      "Iteration: 1452 lambda_n: 0.983745199296564 Loss: 0.05965840531454114\n",
      "Iteration: 1453 lambda_n: 0.9140156769661195 Loss: 0.059584502881204794\n",
      "Iteration: 1454 lambda_n: 0.944288483313685 Loss: 0.05951593228699865\n",
      "Iteration: 1455 lambda_n: 1.0490037910551826 Loss: 0.05944518313148446\n",
      "Iteration: 1456 lambda_n: 0.9933105823732161 Loss: 0.05936669697001064\n",
      "Iteration: 1457 lambda_n: 1.0174193623195698 Loss: 0.05929248662995277\n",
      "Iteration: 1458 lambda_n: 0.918969219776337 Loss: 0.05921658313793972\n",
      "Iteration: 1459 lambda_n: 1.0490177813757124 Loss: 0.059148120771867596\n",
      "Iteration: 1460 lambda_n: 1.0019387413212704 Loss: 0.05907007645001336\n",
      "Iteration: 1461 lambda_n: 0.9925886870405211 Loss: 0.058995644676635144\n",
      "Iteration: 1462 lambda_n: 0.9608153584156893 Loss: 0.05892201265143989\n",
      "Iteration: 1463 lambda_n: 0.9209835138673792 Loss: 0.058850837761393035\n",
      "Iteration: 1464 lambda_n: 1.0032280573705574 Loss: 0.05878270614001633\n",
      "Iteration: 1465 lambda_n: 1.0406708689684587 Loss: 0.05870859081652895\n",
      "Iteration: 1466 lambda_n: 1.0001305722299374 Loss: 0.058631821171134496\n",
      "Iteration: 1467 lambda_n: 0.9583634239851386 Loss: 0.05855815111232086\n",
      "Iteration: 1468 lambda_n: 1.0207465190290144 Loss: 0.05848765786472182\n",
      "Iteration: 1469 lambda_n: 0.9506442800411548 Loss: 0.05841268155662057\n",
      "Iteration: 1470 lambda_n: 0.9133930737913054 Loss: 0.058342955019516266\n",
      "Iteration: 1471 lambda_n: 1.0466744164193436 Loss: 0.05827605153260809\n",
      "Iteration: 1472 lambda_n: 0.9313771721777364 Loss: 0.05819949114169063\n",
      "Iteration: 1473 lambda_n: 1.0222177255198885 Loss: 0.05813146401886324\n",
      "Iteration: 1474 lambda_n: 0.9333256309998822 Loss: 0.058056905609211014\n",
      "Iteration: 1475 lambda_n: 0.974088833984814 Loss: 0.05798892902914956\n",
      "Iteration: 1476 lambda_n: 1.033691912136574 Loss: 0.05791808092608503\n",
      "Iteration: 1477 lambda_n: 0.9943659418081525 Loss: 0.05784300611734068\n",
      "Iteration: 1478 lambda_n: 0.953807677135567 Loss: 0.05777089482836237\n",
      "Iteration: 1479 lambda_n: 1.0235326976390675 Loss: 0.0577018237730111\n",
      "Iteration: 1480 lambda_n: 0.9911744174850002 Loss: 0.05762780889960944\n",
      "Iteration: 1481 lambda_n: 1.0234931425728544 Loss: 0.05755624005500438\n",
      "Iteration: 1482 lambda_n: 0.9803324202278626 Loss: 0.057482445764713216\n",
      "Iteration: 1483 lambda_n: 0.9739408260009912 Loss: 0.057411867927186416\n",
      "Iteration: 1484 lambda_n: 1.0372899993080318 Loss: 0.05734185080008995\n",
      "Iteration: 1485 lambda_n: 0.9956925014905286 Loss: 0.05726738817327428\n",
      "Iteration: 1486 lambda_n: 0.9159245495466078 Loss: 0.05719601926571552\n",
      "Iteration: 1487 lambda_n: 0.9020263738858244 Loss: 0.057130461755896034\n",
      "Iteration: 1488 lambda_n: 0.9227260840500345 Loss: 0.05706598571386993\n",
      "Iteration: 1489 lambda_n: 1.0028101049921714 Loss: 0.057000118430379235\n",
      "Iteration: 1490 lambda_n: 0.9823496550712012 Loss: 0.05692863460549643\n",
      "Iteration: 1491 lambda_n: 1.0367608994723705 Loss: 0.05685871243068151\n",
      "Iteration: 1492 lambda_n: 0.9063323245950801 Loss: 0.05678502647637071\n",
      "Iteration: 1493 lambda_n: 1.0404300142942826 Loss: 0.056720705641198535\n",
      "Iteration: 1494 lambda_n: 0.9078879785506715 Loss: 0.05664697196362552\n",
      "Iteration: 1495 lambda_n: 1.0049831299225838 Loss: 0.05658272687864695\n",
      "Iteration: 1496 lambda_n: 1.0124484922984087 Loss: 0.056511710250054754\n",
      "Iteration: 1497 lambda_n: 0.9914872294147286 Loss: 0.05644027342217064\n",
      "Iteration: 1498 lambda_n: 0.9151130836615093 Loss: 0.05637042054103055\n",
      "Iteration: 1499 lambda_n: 0.9475453298726219 Loss: 0.05630604158893368\n",
      "Iteration: 1500 lambda_n: 1.0283487083365954 Loss: 0.056239473207912706\n",
      "Iteration: 1501 lambda_n: 0.9754397457677977 Loss: 0.056167333296014026\n",
      "Iteration: 1502 lambda_n: 0.9089000549698887 Loss: 0.056099008790149785\n",
      "Iteration: 1503 lambda_n: 1.0463684751791793 Loss: 0.05603543625722765\n",
      "Iteration: 1504 lambda_n: 0.9013060318724997 Loss: 0.0559623532413147\n",
      "Iteration: 1505 lambda_n: 1.04400097715262 Loss: 0.055899496835564146\n",
      "Iteration: 1506 lambda_n: 0.9623897090553561 Loss: 0.05582679274301873\n",
      "Iteration: 1507 lambda_n: 0.9090452572239053 Loss: 0.055759875000484876\n",
      "Iteration: 1508 lambda_n: 0.9483219324544635 Loss: 0.05569675672567248\n",
      "Iteration: 1509 lambda_n: 1.0326913290641908 Loss: 0.05563100306799809\n",
      "Iteration: 1510 lambda_n: 0.97876551196813 Loss: 0.05555950518290355\n",
      "Iteration: 1511 lambda_n: 0.94638238782437 Loss: 0.05549184516927268\n",
      "Iteration: 1512 lambda_n: 1.0422321198181912 Loss: 0.05542651991797686\n",
      "Iteration: 1513 lambda_n: 0.9285577488407342 Loss: 0.05535468530524263\n",
      "Iteration: 1514 lambda_n: 1.0166001695317504 Loss: 0.05529078358088814\n",
      "Iteration: 1515 lambda_n: 1.0414299295462988 Loss: 0.05522092489448881\n",
      "Iteration: 1516 lambda_n: 0.9530396175739553 Loss: 0.05514947182905724\n",
      "Iteration: 1517 lambda_n: 0.9508183856126902 Loss: 0.05508418452089435\n",
      "Iteration: 1518 lambda_n: 0.9805359669588538 Loss: 0.055019144280688756\n",
      "Iteration: 1519 lambda_n: 1.0136098484901133 Loss: 0.05495216989165329\n",
      "Iteration: 1520 lambda_n: 1.024019660895439 Loss: 0.054883041677549524\n",
      "Iteration: 1521 lambda_n: 1.0036195039647258 Loss: 0.0548133125998667\n",
      "Iteration: 1522 lambda_n: 0.9053765842110669 Loss: 0.05474507959386555\n",
      "Iteration: 1523 lambda_n: 1.0331670076188644 Loss: 0.05468361799398191\n",
      "Iteration: 1524 lambda_n: 1.0487416165907368 Loss: 0.05461358366911357\n",
      "Iteration: 1525 lambda_n: 0.9347221151923342 Loss: 0.054542607569084044\n",
      "Iteration: 1526 lambda_n: 1.0179774503655388 Loss: 0.05447944702897938\n",
      "Iteration: 1527 lambda_n: 0.9317237041264953 Loss: 0.05441076320615005\n",
      "Iteration: 1528 lambda_n: 1.0438357552357844 Loss: 0.05434799552191623\n",
      "Iteration: 1529 lambda_n: 1.0058200265918702 Loss: 0.054277780811777726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1530 lambda_n: 0.913173530132584 Loss: 0.05421023177992373\n",
      "Iteration: 1531 lambda_n: 1.0026555047320338 Loss: 0.0541489978985637\n",
      "Iteration: 1532 lambda_n: 0.9283740096671563 Loss: 0.054081862437027915\n",
      "Iteration: 1533 lambda_n: 1.002312289705354 Loss: 0.05401979565117217\n",
      "Iteration: 1534 lambda_n: 0.9447783614527135 Loss: 0.05395288543360562\n",
      "Iteration: 1535 lambda_n: 0.948426075247363 Loss: 0.05388991304375136\n",
      "Iteration: 1536 lambda_n: 1.0360633873806115 Loss: 0.053826791240186654\n",
      "Iteration: 1537 lambda_n: 1.0318577423612199 Loss: 0.0537579425044026\n",
      "Iteration: 1538 lambda_n: 1.038690624415593 Loss: 0.0536894847380089\n",
      "Iteration: 1539 lambda_n: 0.9779659967296437 Loss: 0.053620685794922934\n",
      "Iteration: 1540 lambda_n: 0.933553852771785 Loss: 0.05355601304319549\n",
      "Iteration: 1541 lambda_n: 0.8989896657138342 Loss: 0.05349437113532302\n",
      "Iteration: 1542 lambda_n: 1.0318975609869687 Loss: 0.05343509799193758\n",
      "Iteration: 1543 lambda_n: 1.0237464622889103 Loss: 0.053367163280960896\n",
      "Iteration: 1544 lambda_n: 0.9777380510165453 Loss: 0.05329987511488833\n",
      "Iteration: 1545 lambda_n: 0.9457394729449808 Loss: 0.053235713825038306\n",
      "Iteration: 1546 lambda_n: 0.9889968271157077 Loss: 0.05317374774269636\n",
      "Iteration: 1547 lambda_n: 0.9501132406994461 Loss: 0.053109046364988304\n",
      "Iteration: 1548 lambda_n: 0.9270296066769516 Loss: 0.05304698548984286\n",
      "Iteration: 1549 lambda_n: 0.9829193991849351 Loss: 0.05298652349280281\n",
      "Iteration: 1550 lambda_n: 0.916462155964096 Loss: 0.05292251313621078\n",
      "Iteration: 1551 lambda_n: 0.9115910456670193 Loss: 0.05286292242342298\n",
      "Iteration: 1552 lambda_n: 0.9639709471393421 Loss: 0.05280373532271981\n",
      "Iteration: 1553 lambda_n: 0.9319465124895144 Loss: 0.05274124060741934\n",
      "Iteration: 1554 lambda_n: 0.8952835013055075 Loss: 0.052680914587922276\n",
      "Iteration: 1555 lambda_n: 0.9650626614130344 Loss: 0.05262304755950238\n",
      "Iteration: 1556 lambda_n: 0.9008770211609328 Loss: 0.052560762603498686\n",
      "Iteration: 1557 lambda_n: 0.93721618455533 Loss: 0.05250270868618983\n",
      "Iteration: 1558 lambda_n: 1.0466366706684531 Loss: 0.05244240207327624\n",
      "Iteration: 1559 lambda_n: 0.9819635768332072 Loss: 0.05237516068123317\n",
      "Iteration: 1560 lambda_n: 0.9683595641735807 Loss: 0.05231217897801768\n",
      "Iteration: 1561 lambda_n: 0.9296420935692208 Loss: 0.05225016829351296\n",
      "Iteration: 1562 lambda_n: 0.9235096368131378 Loss: 0.0521907293522848\n",
      "Iteration: 1563 lambda_n: 0.9294286179267086 Loss: 0.05213177160390563\n",
      "Iteration: 1564 lambda_n: 0.9467896485985888 Loss: 0.05207252543648144\n",
      "Iteration: 1565 lambda_n: 1.0442087127339301 Loss: 0.05201226466874173\n",
      "Iteration: 1566 lambda_n: 0.9878179221757951 Loss: 0.05194590975764742\n",
      "Iteration: 1567 lambda_n: 0.93038424448166 Loss: 0.051883243528982395\n",
      "Iteration: 1568 lambda_n: 0.9762743562807674 Loss: 0.05182431450241451\n",
      "Iteration: 1569 lambda_n: 1.027454784012986 Loss: 0.05176257483363561\n",
      "Iteration: 1570 lambda_n: 1.0414691306358117 Loss: 0.05169770458654706\n",
      "Iteration: 1571 lambda_n: 0.9040240231235476 Loss: 0.051632061216069954\n",
      "Iteration: 1572 lambda_n: 1.008664698030146 Loss: 0.051575174416623105\n",
      "Iteration: 1573 lambda_n: 0.9746836447670552 Loss: 0.05151180139521227\n",
      "Iteration: 1574 lambda_n: 1.0023726830616575 Loss: 0.05145066430799978\n",
      "Iteration: 1575 lambda_n: 0.8930116869701687 Loss: 0.05138789286264599\n",
      "Iteration: 1576 lambda_n: 0.9668025330732064 Loss: 0.051332059430905214\n",
      "Iteration: 1577 lambda_n: 0.9460656346935228 Loss: 0.05127170455064382\n",
      "Iteration: 1578 lambda_n: 0.9787276524284134 Loss: 0.05121273848390777\n",
      "Iteration: 1579 lambda_n: 1.021222547655688 Loss: 0.051151833913778796\n",
      "Iteration: 1580 lambda_n: 1.0030277949264859 Loss: 0.05108839023434757\n",
      "Iteration: 1581 lambda_n: 0.9762920037122645 Loss: 0.05102618257359405\n",
      "Iteration: 1582 lambda_n: 1.0089263002644322 Loss: 0.05096573376091881\n",
      "Iteration: 1583 lambda_n: 1.0024184906440181 Loss: 0.050903367712980374\n",
      "Iteration: 1584 lambda_n: 0.9960007730759882 Loss: 0.05084150864373257\n",
      "Iteration: 1585 lambda_n: 0.9777327090612621 Loss: 0.050780148963604804\n",
      "Iteration: 1586 lambda_n: 0.9976594029567115 Loss: 0.05072001509132282\n",
      "Iteration: 1587 lambda_n: 1.03941080747336 Loss: 0.05065875754472267\n",
      "Iteration: 1588 lambda_n: 1.0214420358010279 Loss: 0.0505950455077165\n",
      "Iteration: 1589 lambda_n: 0.9109569749327897 Loss: 0.050532544346195314\n",
      "Iteration: 1590 lambda_n: 0.941746253308572 Loss: 0.05047689657174751\n",
      "Iteration: 1591 lambda_n: 1.0463731061306618 Loss: 0.050419457954995056\n",
      "Iteration: 1592 lambda_n: 0.8977153546812596 Loss: 0.050355744064267696\n",
      "Iteration: 1593 lambda_n: 0.9385839454238606 Loss: 0.05030117459253665\n",
      "Iteration: 1594 lambda_n: 1.0240351335167235 Loss: 0.05024420956037035\n",
      "Iteration: 1595 lambda_n: 1.0040048671765431 Loss: 0.05018216105366242\n",
      "Iteration: 1596 lambda_n: 0.909249169462197 Loss: 0.050121432082089745\n",
      "Iteration: 1597 lambda_n: 1.0419696294371799 Loss: 0.05006652610203595\n",
      "Iteration: 1598 lambda_n: 0.8930606446886827 Loss: 0.05000370882676932\n",
      "Iteration: 1599 lambda_n: 0.9394327467777531 Loss: 0.04994996049416008\n",
      "Iteration: 1600 lambda_n: 0.9134967861824185 Loss: 0.04989350980128578\n",
      "Iteration: 1601 lambda_n: 1.0463640291319678 Loss: 0.04983870568627444\n",
      "Iteration: 1602 lambda_n: 0.9499208393963752 Loss: 0.04977603444457475\n",
      "Iteration: 1603 lambda_n: 1.0456154130445368 Loss: 0.04971923929772627\n",
      "Iteration: 1604 lambda_n: 0.945614239735303 Loss: 0.0496568291887055\n",
      "Iteration: 1605 lambda_n: 0.9385678155706195 Loss: 0.04960048693289391\n",
      "Iteration: 1606 lambda_n: 0.9180154633638772 Loss: 0.049544656251967044\n",
      "Iteration: 1607 lambda_n: 1.0364515054100234 Loss: 0.04949013672650199\n",
      "Iteration: 1608 lambda_n: 1.0080876458250443 Loss: 0.049428686527633374\n",
      "Iteration: 1609 lambda_n: 0.980850724327407 Loss: 0.04936902523204088\n",
      "Iteration: 1610 lambda_n: 0.9645879438669972 Loss: 0.04931107738484512\n",
      "Iteration: 1611 lambda_n: 0.9588722712338729 Loss: 0.049254187804050664\n",
      "Iteration: 1612 lambda_n: 0.984304728682588 Loss: 0.04919773097406123\n",
      "Iteration: 1613 lambda_n: 0.9757315994426092 Loss: 0.04913987545179687\n",
      "Iteration: 1614 lambda_n: 0.9272827833062804 Loss: 0.04908262307349268\n",
      "Iteration: 1615 lambda_n: 0.9801808246014379 Loss: 0.04902830562302126\n",
      "Iteration: 1616 lambda_n: 0.9193125253247004 Loss: 0.04897098567111064\n",
      "Iteration: 1617 lambda_n: 1.021187785239502 Loss: 0.04891731657984122\n",
      "Iteration: 1618 lambda_n: 1.0064388945476852 Loss: 0.048857801192797985\n",
      "Iteration: 1619 lambda_n: 1.0230596618194294 Loss: 0.04879925138263372\n",
      "Iteration: 1620 lambda_n: 0.9537928641995166 Loss: 0.0487398420358838\n",
      "Iteration: 1621 lambda_n: 0.9905895185371272 Loss: 0.04868455376868114\n",
      "Iteration: 1622 lambda_n: 1.00925506431648 Loss: 0.048627231838496675\n",
      "Iteration: 1623 lambda_n: 0.9650642041957277 Loss: 0.04856893417805825\n",
      "Iteration: 1624 lambda_n: 0.8945334940150554 Loss: 0.048513288558096106\n",
      "Iteration: 1625 lambda_n: 1.0462668134116888 Loss: 0.04846179694454259\n",
      "Iteration: 1626 lambda_n: 0.8972967318390855 Loss: 0.04840167416462355\n",
      "Iteration: 1627 lambda_n: 1.0435741442945103 Loss: 0.048350204253327714\n",
      "Iteration: 1628 lambda_n: 0.9009504962454153 Loss: 0.04829044656295643\n",
      "Iteration: 1629 lambda_n: 0.9953820995042287 Loss: 0.04823894865343146\n",
      "Iteration: 1630 lambda_n: 0.9978845127746335 Loss: 0.04818214967187846\n",
      "Iteration: 1631 lambda_n: 0.8991618296482621 Loss: 0.04812531111294295\n",
      "Iteration: 1632 lambda_n: 0.985902572566816 Loss: 0.048074185551339806\n",
      "Iteration: 1633 lambda_n: 0.8983000777076456 Loss: 0.048018223256185426\n",
      "Iteration: 1634 lambda_n: 0.9690013396934402 Loss: 0.04796732254108276\n",
      "Iteration: 1635 lambda_n: 0.9515756012540021 Loss: 0.04791250866349375\n",
      "Iteration: 1636 lambda_n: 0.9507579368255987 Loss: 0.047858775713231626\n",
      "Iteration: 1637 lambda_n: 1.0125633020019897 Loss: 0.047805182944363915\n",
      "Iteration: 1638 lambda_n: 0.9604688057898197 Loss: 0.04774820875552237\n",
      "Iteration: 1639 lambda_n: 1.0350789749041711 Loss: 0.04769426503496033\n",
      "Iteration: 1640 lambda_n: 0.9751053878900515 Loss: 0.047636237254973186\n",
      "Iteration: 1641 lambda_n: 1.0236066302452556 Loss: 0.047581674479403896\n",
      "Iteration: 1642 lambda_n: 0.9623153390829416 Loss: 0.04752450355840887\n",
      "Iteration: 1643 lambda_n: 1.0432262466681188 Loss: 0.04747085622864743\n",
      "Iteration: 1644 lambda_n: 0.9089590999985542 Loss: 0.04741280604191988\n",
      "Iteration: 1645 lambda_n: 0.965712455303728 Loss: 0.04736232128181783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1646 lambda_n: 1.0238309452210395 Loss: 0.04730877794828214\n",
      "Iteration: 1647 lambda_n: 1.0037346211634028 Loss: 0.04725211763952289\n",
      "Iteration: 1648 lambda_n: 0.9828687292143184 Loss: 0.047196675915382326\n",
      "Iteration: 1649 lambda_n: 1.0275175381501314 Loss: 0.04714248889067384\n",
      "Iteration: 1650 lambda_n: 0.8950049367630042 Loss: 0.047085947508393705\n",
      "Iteration: 1651 lambda_n: 1.04615413097518 Loss: 0.047036789404332445\n",
      "Iteration: 1652 lambda_n: 0.961701186332488 Loss: 0.046979433370197624\n",
      "Iteration: 1653 lambda_n: 0.9591443690218803 Loss: 0.0469268095640332\n",
      "Iteration: 1654 lambda_n: 0.9922838361268408 Loss: 0.046874422048080254\n",
      "Iteration: 1655 lambda_n: 1.0292327151653937 Loss: 0.04682032535894277\n",
      "Iteration: 1656 lambda_n: 0.9336049617086766 Loss: 0.0467643227276556\n",
      "Iteration: 1657 lambda_n: 1.0065381059406362 Loss: 0.046713620591329094\n",
      "Iteration: 1658 lambda_n: 0.929440255885048 Loss: 0.04665905897251006\n",
      "Iteration: 1659 lambda_n: 0.9874287302771974 Loss: 0.04660877193880143\n",
      "Iteration: 1660 lambda_n: 1.0480390459227544 Loss: 0.04655544601707339\n",
      "Iteration: 1661 lambda_n: 0.9160765019461229 Loss: 0.04649895801968653\n",
      "Iteration: 1662 lambda_n: 0.961731342751782 Loss: 0.046449678791198504\n",
      "Iteration: 1663 lambda_n: 0.919505357800577 Loss: 0.046398037991985226\n",
      "Iteration: 1664 lambda_n: 0.9795403040265008 Loss: 0.046348756075542105\n",
      "Iteration: 1665 lambda_n: 0.9420518235059798 Loss: 0.04629635369440532\n",
      "Iteration: 1666 lambda_n: 0.9360269886159355 Loss: 0.04624605268734113\n",
      "Iteration: 1667 lambda_n: 0.9199070235642555 Loss: 0.04619616615778509\n",
      "Iteration: 1668 lambda_n: 0.93914835081996 Loss: 0.046147229058411614\n",
      "Iteration: 1669 lambda_n: 0.9751952650531963 Loss: 0.046097360347244445\n",
      "Iteration: 1670 lambda_n: 1.0431109181126232 Loss: 0.046045675786766986\n",
      "Iteration: 1671 lambda_n: 0.9078878194141138 Loss: 0.045990502244274596\n",
      "Iteration: 1672 lambda_n: 1.0149939933555814 Loss: 0.04594257649730875\n",
      "Iteration: 1673 lambda_n: 1.0405773011056199 Loss: 0.04588909889134861\n",
      "Iteration: 1674 lambda_n: 0.9554354036218551 Loss: 0.04583438655157913\n",
      "Iteration: 1675 lambda_n: 0.9769138650776201 Loss: 0.04578425324141515\n",
      "Iteration: 1676 lambda_n: 0.9996070322900287 Loss: 0.04573309306372106\n",
      "Iteration: 1677 lambda_n: 0.896647483929618 Loss: 0.04568084938845086\n",
      "Iteration: 1678 lambda_n: 0.9753612580446955 Loss: 0.045634078625839104\n",
      "Iteration: 1679 lambda_n: 0.9830318592291958 Loss: 0.045583298497255745\n",
      "Iteration: 1680 lambda_n: 0.9390652097060385 Loss: 0.045532221804004055\n",
      "Iteration: 1681 lambda_n: 0.9934636946735733 Loss: 0.04548352664489474\n",
      "Iteration: 1682 lambda_n: 1.045545432236916 Loss: 0.04543211285087786\n",
      "Iteration: 1683 lambda_n: 0.9826673304725275 Loss: 0.04537811743509247\n",
      "Iteration: 1684 lambda_n: 1.0028975622238985 Loss: 0.04532747712134117\n",
      "Iteration: 1685 lambda_n: 1.0013530542323192 Loss: 0.04527590119317596\n",
      "Iteration: 1686 lambda_n: 0.9473301103582 Loss: 0.045224512897153124\n",
      "Iteration: 1687 lambda_n: 0.9589493912245032 Loss: 0.04517599729266399\n",
      "Iteration: 1688 lambda_n: 0.9846308681242248 Loss: 0.04512698534723795\n",
      "Iteration: 1689 lambda_n: 0.9964494649001828 Loss: 0.045076764174031114\n",
      "Iteration: 1690 lambda_n: 0.9858482370110084 Loss: 0.045026047169923956\n",
      "Iteration: 1691 lambda_n: 0.986764874798837 Loss: 0.044975976112293224\n",
      "Iteration: 1692 lambda_n: 0.9597712291570708 Loss: 0.044925964509866884\n",
      "Iteration: 1693 lambda_n: 0.9675117536176188 Loss: 0.044877423277206785\n",
      "Iteration: 1694 lambda_n: 1.0417031610339018 Loss: 0.04482859243504931\n",
      "Iteration: 1695 lambda_n: 0.9461053041628521 Loss: 0.04477613088314995\n",
      "Iteration: 1696 lambda_n: 1.0454377987808952 Loss: 0.044728588124950446\n",
      "Iteration: 1697 lambda_n: 0.9467546902991754 Loss: 0.044676167178163216\n",
      "Iteration: 1698 lambda_n: 0.9600554162117275 Loss: 0.044628799620269936\n",
      "Iteration: 1699 lambda_n: 0.9482739179625962 Loss: 0.04458086769316077\n",
      "Iteration: 1700 lambda_n: 1.035431879645861 Loss: 0.04453362441626607\n",
      "Iteration: 1701 lambda_n: 0.9646248226016442 Loss: 0.044482152002907394\n",
      "Iteration: 1702 lambda_n: 0.906048503905262 Loss: 0.044434307787443256\n",
      "Iteration: 1703 lambda_n: 0.9679022485986103 Loss: 0.04438946419896863\n",
      "Iteration: 1704 lambda_n: 0.9253617924783755 Loss: 0.04434166016779857\n",
      "Iteration: 1705 lambda_n: 0.9440633998041792 Loss: 0.044296056055586834\n",
      "Iteration: 1706 lambda_n: 1.041329802941333 Loss: 0.04424962951053387\n",
      "Iteration: 1707 lambda_n: 0.8960291311197907 Loss: 0.04419853534268958\n",
      "Iteration: 1708 lambda_n: 1.0043055224300932 Loss: 0.04415467043334203\n",
      "Iteration: 1709 lambda_n: 0.8946553025404971 Loss: 0.04410561219300241\n",
      "Iteration: 1710 lambda_n: 0.9101499485095916 Loss: 0.044062008192492945\n",
      "Iteration: 1711 lambda_n: 0.9929738707700846 Loss: 0.04401774289400869\n",
      "Iteration: 1712 lambda_n: 0.9477048138543424 Loss: 0.04396955708553276\n",
      "Iteration: 1713 lambda_n: 0.8963796468479133 Loss: 0.043923674569252134\n",
      "Iteration: 1714 lambda_n: 1.0166688583679133 Loss: 0.04388037310831683\n",
      "Iteration: 1715 lambda_n: 1.0096552439076711 Loss: 0.0438313724752013\n",
      "Iteration: 1716 lambda_n: 0.9479592314527874 Loss: 0.04378282932043521\n",
      "Iteration: 1717 lambda_n: 1.0357010624827132 Loss: 0.04373736182307356\n",
      "Iteration: 1718 lambda_n: 0.9368780365622097 Loss: 0.04368780583850201\n",
      "Iteration: 1719 lambda_n: 0.9983978988900236 Loss: 0.04364308859176102\n",
      "Iteration: 1720 lambda_n: 0.9899367595305976 Loss: 0.043595549108935885\n",
      "Iteration: 1721 lambda_n: 1.0466986834007066 Loss: 0.043548530096704556\n",
      "Iteration: 1722 lambda_n: 0.9354073695289427 Loss: 0.04349894233781798\n",
      "Iteration: 1723 lambda_n: 0.9784204228871987 Loss: 0.043454740004322645\n",
      "Iteration: 1724 lambda_n: 1.0163485737161937 Loss: 0.04340861821268344\n",
      "Iteration: 1725 lambda_n: 0.9069143753909571 Loss: 0.04336083174831667\n",
      "Iteration: 1726 lambda_n: 0.9934546141737338 Loss: 0.04331829865842251\n",
      "Iteration: 1727 lambda_n: 0.9682441929143665 Loss: 0.04327182249576369\n",
      "Iteration: 1728 lambda_n: 0.9454106224807192 Loss: 0.04322664386194509\n",
      "Iteration: 1729 lambda_n: 1.0180088203452848 Loss: 0.043182643846261276\n",
      "Iteration: 1730 lambda_n: 0.9671031319592878 Loss: 0.04313538991463366\n",
      "Iteration: 1731 lambda_n: 0.9735115232643398 Loss: 0.04309062090256575\n",
      "Iteration: 1732 lambda_n: 1.0026014499778484 Loss: 0.043045675535715894\n",
      "Iteration: 1733 lambda_n: 1.0226991411016522 Loss: 0.04299951397531112\n",
      "Iteration: 1734 lambda_n: 1.02775780336103 Loss: 0.04295256079307548\n",
      "Iteration: 1735 lambda_n: 1.0173865287576624 Loss: 0.04290551264959218\n",
      "Iteration: 1736 lambda_n: 0.9092151502170387 Loss: 0.0428590761037431\n",
      "Iteration: 1737 lambda_n: 0.9866703311173876 Loss: 0.04281769385211303\n",
      "Iteration: 1738 lambda_n: 1.0266537442588726 Loss: 0.04277291046865652\n",
      "Iteration: 1739 lambda_n: 1.0091194171174513 Loss: 0.04272645115517031\n",
      "Iteration: 1740 lambda_n: 1.0452713827760882 Loss: 0.0426809251860176\n",
      "Iteration: 1741 lambda_n: 1.0219478732196778 Loss: 0.042633915290732155\n",
      "Iteration: 1742 lambda_n: 0.9494292170284281 Loss: 0.04258810106311438\n",
      "Iteration: 1743 lambda_n: 1.0107922961615214 Loss: 0.04254566962513019\n",
      "Iteration: 1744 lambda_n: 0.9590639089817646 Loss: 0.04250063536728109\n",
      "Iteration: 1745 lambda_n: 0.9263894286281219 Loss: 0.042458041321105235\n",
      "Iteration: 1746 lambda_n: 0.9429552731933057 Loss: 0.042417024888435226\n",
      "Iteration: 1747 lambda_n: 0.926760867155953 Loss: 0.04237540353440587\n",
      "Iteration: 1748 lambda_n: 1.010221795797864 Loss: 0.04233462499843257\n",
      "Iteration: 1749 lambda_n: 0.9701998074711133 Loss: 0.04229031934412982\n",
      "Iteration: 1750 lambda_n: 0.9270428881636446 Loss: 0.042247914434349686\n",
      "Iteration: 1751 lambda_n: 0.9744321739729475 Loss: 0.04221004124542397\n",
      "Iteration: 1752 lambda_n: 0.9170589550521444 Loss: 0.04217634541342226\n",
      "Iteration: 1753 lambda_n: 1.0173154593366114 Loss: 0.042144785834045385\n",
      "Iteration: 1754 lambda_n: 1.020113573146155 Loss: 0.042109670022052466\n",
      "Iteration: 1755 lambda_n: 0.9188750933937044 Loss: 0.042074473396707814\n",
      "Iteration: 1756 lambda_n: 0.8949805790508097 Loss: 0.04204288031250591\n",
      "Iteration: 1757 lambda_n: 0.9370833402766946 Loss: 0.04201226219854812\n",
      "Iteration: 1758 lambda_n: 0.9893895205190555 Loss: 0.04198039213128599\n",
      "Iteration: 1759 lambda_n: 0.9341114266842113 Loss: 0.041946971879607796\n",
      "Iteration: 1760 lambda_n: 0.939426488282043 Loss: 0.04191565699697267\n",
      "Iteration: 1761 lambda_n: 0.977104035882125 Loss: 0.04188439820556718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1762 lambda_n: 0.9061316130722664 Loss: 0.04185213386775686\n",
      "Iteration: 1763 lambda_n: 1.0466666700073934 Loss: 0.04182244986778159\n",
      "Iteration: 1764 lambda_n: 1.0401898027089165 Loss: 0.04178841843869965\n",
      "Iteration: 1765 lambda_n: 1.0035046872149755 Loss: 0.04175488421943706\n",
      "Iteration: 1766 lambda_n: 0.9790390424220901 Loss: 0.04172280199497504\n",
      "Iteration: 1767 lambda_n: 1.0043445478502868 Loss: 0.04169175058518627\n",
      "Iteration: 1768 lambda_n: 0.9831559641022541 Loss: 0.04166014121447263\n",
      "Iteration: 1769 lambda_n: 0.9973793318077586 Loss: 0.04162943825876542\n",
      "Iteration: 1770 lambda_n: 1.0068252819843821 Loss: 0.04159852439847984\n",
      "Iteration: 1771 lambda_n: 0.904348965482489 Loss: 0.04156755131167225\n",
      "Iteration: 1772 lambda_n: 1.0336039215352564 Loss: 0.04153993601862772\n",
      "Iteration: 1773 lambda_n: 1.0068550140211923 Loss: 0.041508583733239166\n",
      "Iteration: 1774 lambda_n: 0.9489283298559429 Loss: 0.04147826888422055\n",
      "Iteration: 1775 lambda_n: 0.9342382946410516 Loss: 0.0414499004779391\n",
      "Iteration: 1776 lambda_n: 0.9442243478020017 Loss: 0.041422155478659475\n",
      "Iteration: 1777 lambda_n: 0.9851976063999808 Loss: 0.04139429382661294\n",
      "Iteration: 1778 lambda_n: 0.9107810533921984 Loss: 0.04136540946988562\n",
      "Iteration: 1779 lambda_n: 1.0194456172050235 Loss: 0.04133888142092915\n",
      "Iteration: 1780 lambda_n: 1.045212436972922 Loss: 0.04130936768874962\n",
      "Iteration: 1781 lambda_n: 0.9814001247363134 Loss: 0.04127930839767488\n",
      "Iteration: 1782 lambda_n: 0.9924256250906057 Loss: 0.04125127189722427\n",
      "Iteration: 1783 lambda_n: 1.0316374348390227 Loss: 0.04122309559004039\n",
      "Iteration: 1784 lambda_n: 0.9837182351459871 Loss: 0.04119398671706187\n",
      "Iteration: 1785 lambda_n: 0.9195569871421949 Loss: 0.04116640429980746\n",
      "Iteration: 1786 lambda_n: 0.912777241662599 Loss: 0.04114077282832366\n",
      "Iteration: 1787 lambda_n: 1.0116235717629287 Loss: 0.041115469040062774\n",
      "Iteration: 1788 lambda_n: 0.9644673022374998 Loss: 0.04108757595971921\n",
      "Iteration: 1789 lambda_n: 0.9679411990747491 Loss: 0.04106113787340453\n",
      "Iteration: 1790 lambda_n: 1.0138864720267036 Loss: 0.041034750117725206\n",
      "Iteration: 1791 lambda_n: 0.9398719397658947 Loss: 0.04100726027218092\n",
      "Iteration: 1792 lambda_n: 1.0217225868861115 Loss: 0.040981919266177634\n",
      "Iteration: 1793 lambda_n: 0.984963071779501 Loss: 0.04095451315419887\n",
      "Iteration: 1794 lambda_n: 0.9412720873680067 Loss: 0.040928237594261216\n",
      "Iteration: 1795 lambda_n: 0.9343373985788184 Loss: 0.04090325793608563\n",
      "Iteration: 1796 lambda_n: 0.9304274376306113 Loss: 0.04087858386321652\n",
      "Iteration: 1797 lambda_n: 0.922149285825258 Loss: 0.04085413097169568\n",
      "Iteration: 1798 lambda_n: 1.035102584651402 Loss: 0.04083000983192598\n",
      "Iteration: 1799 lambda_n: 0.9003708234677332 Loss: 0.04080305987457631\n",
      "Iteration: 1800 lambda_n: 0.9784337689607875 Loss: 0.04077973635029465\n",
      "Iteration: 1801 lambda_n: 0.9564066059337337 Loss: 0.04075450203510816\n",
      "Iteration: 1802 lambda_n: 0.9110318666404084 Loss: 0.04072995119066215\n",
      "Iteration: 1803 lambda_n: 1.011357109614673 Loss: 0.04070667032163312\n",
      "Iteration: 1804 lambda_n: 1.0262771317014245 Loss: 0.04068093599361498\n",
      "Iteration: 1805 lambda_n: 0.9485051126794992 Loss: 0.040654943288283674\n",
      "Iteration: 1806 lambda_n: 0.9876120080777336 Loss: 0.04063103118813161\n",
      "Iteration: 1807 lambda_n: 0.9735539748243555 Loss: 0.04060623855016499\n",
      "Iteration: 1808 lambda_n: 0.8986370525837067 Loss: 0.04058190460121989\n",
      "Iteration: 1809 lambda_n: 0.9504104324938419 Loss: 0.04055953725266928\n",
      "Iteration: 1810 lambda_n: 0.9516226034299282 Loss: 0.04053597213594901\n",
      "Iteration: 1811 lambda_n: 1.0129950196136732 Loss: 0.04051247120866958\n",
      "Iteration: 1812 lambda_n: 0.9433867877772428 Loss: 0.04048755366956631\n",
      "Iteration: 1813 lambda_n: 0.9943804072030181 Loss: 0.04046444391782823\n",
      "Iteration: 1814 lambda_n: 0.9563769299824749 Loss: 0.04044017773431497\n",
      "Iteration: 1815 lambda_n: 0.8939109337490292 Loss: 0.040416930758272365\n",
      "Iteration: 1816 lambda_n: 1.0362761256899704 Loss: 0.04039528300951848\n",
      "Iteration: 1817 lambda_n: 0.979359650405477 Loss: 0.04037027486440456\n",
      "Iteration: 1818 lambda_n: 1.0433177891258214 Loss: 0.04034673297932379\n",
      "Iteration: 1819 lambda_n: 0.9821756817753701 Loss: 0.04032174589979906\n",
      "Iteration: 1820 lambda_n: 0.9248199696724342 Loss: 0.040298313251827686\n",
      "Iteration: 1821 lambda_n: 0.9722134101842959 Loss: 0.040276327321745124\n",
      "Iteration: 1822 lambda_n: 1.035632522761575 Loss: 0.040253291356998656\n",
      "Iteration: 1823 lambda_n: 1.0112626594024903 Loss: 0.040228837161096924\n",
      "Iteration: 1824 lambda_n: 0.9848990745956492 Loss: 0.040205044181336955\n",
      "Iteration: 1825 lambda_n: 1.0340088789609483 Loss: 0.04018195149821336\n",
      "Iteration: 1826 lambda_n: 1.0212259663512624 Loss: 0.04015778803835495\n",
      "Iteration: 1827 lambda_n: 1.031641971273199 Loss: 0.04013400513260667\n",
      "Iteration: 1828 lambda_n: 0.9618299181259466 Loss: 0.040110059881010574\n",
      "Iteration: 1829 lambda_n: 1.0014197124151618 Loss: 0.040087808821707926\n",
      "Iteration: 1830 lambda_n: 0.9635596940560173 Loss: 0.04006471268594837\n",
      "Iteration: 1831 lambda_n: 0.9897083609237112 Loss: 0.04004255907291423\n",
      "Iteration: 1832 lambda_n: 1.0268698401063188 Loss: 0.040019871848544455\n",
      "Iteration: 1833 lambda_n: 1.0318315760314607 Loss: 0.03999640359320167\n",
      "Iteration: 1834 lambda_n: 0.9599832803158231 Loss: 0.039972894322739615\n",
      "Iteration: 1835 lambda_n: 1.0361684435174527 Loss: 0.03995108815633265\n",
      "Iteration: 1836 lambda_n: 0.8958732087521153 Loss: 0.03992761724832053\n",
      "Iteration: 1837 lambda_n: 1.0397710297804803 Loss: 0.039907383787218274\n",
      "Iteration: 1838 lambda_n: 1.0105369913372235 Loss: 0.03988396017665368\n",
      "Iteration: 1839 lambda_n: 0.9998824516677284 Loss: 0.0398612607132046\n",
      "Iteration: 1840 lambda_n: 0.9863653002729094 Loss: 0.03983886256480188\n",
      "Iteration: 1841 lambda_n: 0.9432384311349292 Loss: 0.039816826635957536\n",
      "Iteration: 1842 lambda_n: 0.9205218987062809 Loss: 0.039795809157759134\n",
      "Iteration: 1843 lambda_n: 0.8929474405471356 Loss: 0.03977534835163758\n",
      "Iteration: 1844 lambda_n: 0.9922440705930312 Loss: 0.03975554746185799\n",
      "Iteration: 1845 lambda_n: 0.9116574692256711 Loss: 0.03973359504543614\n",
      "Iteration: 1846 lambda_n: 1.0263579077048903 Loss: 0.039713475461461006\n",
      "Iteration: 1847 lambda_n: 0.9330393787855074 Loss: 0.039690876057692455\n",
      "Iteration: 1848 lambda_n: 1.0013751665121495 Loss: 0.03967038257107095\n",
      "Iteration: 1849 lambda_n: 0.9029584438267056 Loss: 0.039648437732646764\n",
      "Iteration: 1850 lambda_n: 0.9000331094725694 Loss: 0.03962869632700517\n",
      "Iteration: 1851 lambda_n: 1.0421826542287174 Loss: 0.03960906039088606\n",
      "Iteration: 1852 lambda_n: 1.006887211291486 Loss: 0.03958637092452655\n",
      "Iteration: 1853 lambda_n: 0.9350246088087742 Loss: 0.03956450174461719\n",
      "Iteration: 1854 lambda_n: 0.9938857838455845 Loss: 0.03954423899245511\n",
      "Iteration: 1855 lambda_n: 0.9644691922174465 Loss: 0.0395227453645337\n",
      "Iteration: 1856 lambda_n: 0.9043607008009389 Loss: 0.03950193297087933\n",
      "Iteration: 1857 lambda_n: 1.0423388996450456 Loss: 0.039482457907995906\n",
      "Iteration: 1858 lambda_n: 0.9708721642458645 Loss: 0.03946005503861931\n",
      "Iteration: 1859 lambda_n: 1.0347223100233198 Loss: 0.03943923343236508\n",
      "Iteration: 1860 lambda_n: 1.0141581142985845 Loss: 0.03941708707834777\n",
      "Iteration: 1861 lambda_n: 0.897423916247941 Loss: 0.03939542641078374\n",
      "Iteration: 1862 lambda_n: 0.918816448771763 Loss: 0.03937629753282019\n",
      "Iteration: 1863 lambda_n: 1.0292671965137827 Loss: 0.03935674740109142\n",
      "Iteration: 1864 lambda_n: 0.9572380122573803 Loss: 0.03933488671604347\n",
      "Iteration: 1865 lambda_n: 0.9218967169738705 Loss: 0.0393145958548736\n",
      "Iteration: 1866 lambda_n: 1.0436154530941113 Loss: 0.03929508945846281\n",
      "Iteration: 1867 lambda_n: 1.0251374907196054 Loss: 0.03927304607659235\n",
      "Iteration: 1868 lambda_n: 1.0219039988077074 Loss: 0.03925143460257666\n",
      "Iteration: 1869 lambda_n: 0.9506018600407812 Loss: 0.039229931420701136\n",
      "Iteration: 1870 lambda_n: 0.952686131172458 Loss: 0.039209964996609725\n",
      "Iteration: 1871 lambda_n: 0.9822280571299599 Loss: 0.03918998839225873\n",
      "Iteration: 1872 lambda_n: 1.0100000238989735 Loss: 0.03916942661752062\n",
      "Iteration: 1873 lambda_n: 1.0124145382658878 Loss: 0.03914831927079519\n",
      "Iteration: 1874 lambda_n: 1.0432784655746317 Loss: 0.03912719772059582\n",
      "Iteration: 1875 lambda_n: 0.9747714080270996 Loss: 0.03910546923012938\n",
      "Iteration: 1876 lambda_n: 0.9055550576923852 Loss: 0.039085202270184444\n",
      "Iteration: 1877 lambda_n: 0.9199722685341156 Loss: 0.03906640406647516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1878 lambda_n: 0.9841849036394856 Loss: 0.03904733436633105\n",
      "Iteration: 1879 lambda_n: 1.0099799048853304 Loss: 0.03902696356718164\n",
      "Iteration: 1880 lambda_n: 0.9802180303977228 Loss: 0.03900609113458831\n",
      "Iteration: 1881 lambda_n: 1.026829839757119 Loss: 0.03898586526560861\n",
      "Iteration: 1882 lambda_n: 0.9459827310623206 Loss: 0.03896470938415499\n",
      "Iteration: 1883 lambda_n: 1.0344628122422448 Loss: 0.038945249048187365\n",
      "Iteration: 1884 lambda_n: 1.0297853443193334 Loss: 0.038923998661122665\n",
      "Iteration: 1885 lambda_n: 0.9522467895351402 Loss: 0.03890287637550537\n",
      "Iteration: 1886 lambda_n: 1.045414180319889 Loss: 0.03888337332064972\n",
      "Iteration: 1887 lambda_n: 0.9459179897410034 Loss: 0.03886199142431864\n",
      "Iteration: 1888 lambda_n: 1.0158299462841762 Loss: 0.03884267265285569\n",
      "Iteration: 1889 lambda_n: 0.9020558675287509 Loss: 0.0388219534894703\n",
      "Iteration: 1890 lambda_n: 0.908263065667643 Loss: 0.038803580189735516\n",
      "Iteration: 1891 lambda_n: 0.9217485601760403 Loss: 0.038785103035007124\n",
      "Iteration: 1892 lambda_n: 0.9599641739910556 Loss: 0.038766374328776984\n",
      "Iteration: 1893 lambda_n: 0.9475971004789081 Loss: 0.0387468929775777\n",
      "Iteration: 1894 lambda_n: 1.0344230260591323 Loss: 0.03872768664815044\n",
      "Iteration: 1895 lambda_n: 1.0025327927037935 Loss: 0.03870674635857085\n",
      "Iteration: 1896 lambda_n: 0.903135168273321 Loss: 0.0386864782630843\n",
      "Iteration: 1897 lambda_n: 1.0277380483410448 Loss: 0.03866824240374391\n",
      "Iteration: 1898 lambda_n: 0.9813407017190848 Loss: 0.038647514223342484\n",
      "Iteration: 1899 lambda_n: 1.0258598149638833 Loss: 0.03862774662456297\n",
      "Iteration: 1900 lambda_n: 1.0101523056549324 Loss: 0.038607106937442265\n",
      "Iteration: 1901 lambda_n: 0.9951708521328981 Loss: 0.0385868081621281\n",
      "Iteration: 1902 lambda_n: 1.0116239724206735 Loss: 0.038566834232017407\n",
      "Iteration: 1903 lambda_n: 0.996112587566583 Loss: 0.03854655367501833\n",
      "Iteration: 1904 lambda_n: 1.0104958426195711 Loss: 0.03852660729684414\n",
      "Iteration: 1905 lambda_n: 0.9623761590756938 Loss: 0.038506395868912115\n",
      "Iteration: 1906 lambda_n: 0.9253000895899124 Loss: 0.0384871686212295\n",
      "Iteration: 1907 lambda_n: 0.9262871418039852 Loss: 0.03846870175529679\n",
      "Iteration: 1908 lambda_n: 0.9071518444238692 Loss: 0.038450233955910594\n",
      "Iteration: 1909 lambda_n: 0.9255095322629696 Loss: 0.03843216578741368\n",
      "Iteration: 1910 lambda_n: 0.9135747749112103 Loss: 0.038413749970641675\n",
      "Iteration: 1911 lambda_n: 0.9933854214488441 Loss: 0.03839558945573114\n",
      "Iteration: 1912 lambda_n: 1.0440405305406912 Loss: 0.038375861595954804\n",
      "Iteration: 1913 lambda_n: 0.9373350949663295 Loss: 0.038355149312454526\n",
      "Iteration: 1914 lambda_n: 1.0306978492072345 Loss: 0.03833657354778184\n",
      "Iteration: 1915 lambda_n: 0.9646173544504574 Loss: 0.0383161672555987\n",
      "Iteration: 1916 lambda_n: 0.9011697437242288 Loss: 0.03829708881908555\n",
      "Iteration: 1917 lambda_n: 0.9603043018295223 Loss: 0.038279282141213115\n",
      "Iteration: 1918 lambda_n: 1.0418711415787476 Loss: 0.03826032392832485\n",
      "Iteration: 1919 lambda_n: 0.9067191827610981 Loss: 0.038239774855205876\n",
      "Iteration: 1920 lambda_n: 0.8953366363673932 Loss: 0.03822190894397206\n",
      "Iteration: 1921 lambda_n: 0.8946932707911902 Loss: 0.03820428246327397\n",
      "Iteration: 1922 lambda_n: 0.9261341632556425 Loss: 0.038186683462446115\n",
      "Iteration: 1923 lambda_n: 0.9575519514503743 Loss: 0.038168481253541685\n",
      "Iteration: 1924 lambda_n: 1.0264096365077013 Loss: 0.038149677704396395\n",
      "Iteration: 1925 lambda_n: 1.036176064645382 Loss: 0.03812953979810106\n",
      "Iteration: 1926 lambda_n: 0.9294234568469811 Loss: 0.03810922913806561\n",
      "Iteration: 1927 lambda_n: 1.034888464564784 Loss: 0.03809102754952092\n",
      "Iteration: 1928 lambda_n: 0.9240782384217083 Loss: 0.03807077751186224\n",
      "Iteration: 1929 lambda_n: 0.9076189627898112 Loss: 0.038052711789958095\n",
      "Iteration: 1930 lambda_n: 1.0464640779682939 Loss: 0.03803498197800905\n",
      "Iteration: 1931 lambda_n: 0.9578961389673313 Loss: 0.03801455622453748\n",
      "Iteration: 1932 lambda_n: 0.9705628225238075 Loss: 0.0379958755578473\n",
      "Iteration: 1933 lambda_n: 0.9330035127876526 Loss: 0.03797696311877057\n",
      "Iteration: 1934 lambda_n: 0.9973989580782558 Loss: 0.037958797120461824\n",
      "Iteration: 1935 lambda_n: 0.9423509194371852 Loss: 0.03793939241312511\n",
      "Iteration: 1936 lambda_n: 0.9355862337240316 Loss: 0.037921073433498625\n",
      "Iteration: 1937 lambda_n: 0.9809078321292066 Loss: 0.03790289976999024\n",
      "Iteration: 1938 lambda_n: 1.048088867854459 Loss: 0.03788386012511984\n",
      "Iteration: 1939 lambda_n: 0.9837697904875514 Loss: 0.037863532500860206\n",
      "Iteration: 1940 lambda_n: 1.0070905947774222 Loss: 0.03784446784536931\n",
      "Iteration: 1941 lambda_n: 0.9318038282078712 Loss: 0.03782496623668035\n",
      "Iteration: 1942 lambda_n: 0.9902420197070818 Loss: 0.03780693627772001\n",
      "Iteration: 1943 lambda_n: 0.9713803069141583 Loss: 0.03778778934941926\n",
      "Iteration: 1944 lambda_n: 0.9188996582354882 Loss: 0.037769021121786\n",
      "Iteration: 1945 lambda_n: 0.9187477584387376 Loss: 0.03775127964922126\n",
      "Iteration: 1946 lambda_n: 1.0231966123695062 Loss: 0.03773355320242725\n",
      "Iteration: 1947 lambda_n: 1.003874995933954 Loss: 0.03771382517061339\n",
      "Iteration: 1948 lambda_n: 0.9781214598957241 Loss: 0.037694484066938476\n",
      "Iteration: 1949 lambda_n: 1.011412631666423 Loss: 0.037675652743739685\n",
      "Iteration: 1950 lambda_n: 0.9223233573220645 Loss: 0.03765619422984986\n",
      "Iteration: 1951 lambda_n: 1.0003810741541141 Loss: 0.03763846220138371\n",
      "Iteration: 1952 lambda_n: 0.9677943751195153 Loss: 0.03761924220910513\n",
      "Iteration: 1953 lambda_n: 0.9535645556381814 Loss: 0.03760066119326322\n",
      "Iteration: 1954 lambda_n: 1.0462175246888932 Loss: 0.03758236561145797\n",
      "Iteration: 1955 lambda_n: 0.9647391496956231 Loss: 0.03756230578518746\n",
      "Iteration: 1956 lambda_n: 0.8940715882683931 Loss: 0.0375438211491062\n",
      "Iteration: 1957 lambda_n: 0.954467756618396 Loss: 0.03752670149435471\n",
      "Iteration: 1958 lambda_n: 1.0161851439811742 Loss: 0.03750843650329802\n",
      "Iteration: 1959 lambda_n: 0.9399678220405832 Loss: 0.037489003011448196\n",
      "Iteration: 1960 lambda_n: 1.0073146603500165 Loss: 0.03747103894232721\n",
      "Iteration: 1961 lambda_n: 0.9664051342219978 Loss: 0.037451799843830305\n",
      "Iteration: 1962 lambda_n: 0.9377326968463798 Loss: 0.03743335406209924\n",
      "Iteration: 1963 lambda_n: 0.9366974080784606 Loss: 0.037415466629125246\n",
      "Iteration: 1964 lambda_n: 0.9482359753504136 Loss: 0.03739760967163267\n",
      "Iteration: 1965 lambda_n: 0.908927874721072 Loss: 0.037379543546326006\n",
      "Iteration: 1966 lambda_n: 0.892221212977839 Loss: 0.037362236591532066\n",
      "Iteration: 1967 lambda_n: 0.9827766079485925 Loss: 0.03734525735005936\n",
      "Iteration: 1968 lambda_n: 1.0145537784211855 Loss: 0.03732656549101181\n",
      "Iteration: 1969 lambda_n: 0.9213343154818285 Loss: 0.03730728109169828\n",
      "Iteration: 1970 lambda_n: 0.9483422267298002 Loss: 0.03728977924379835\n",
      "Iteration: 1971 lambda_n: 1.043085529040975 Loss: 0.037271774555164006\n",
      "Iteration: 1972 lambda_n: 0.9888020866209369 Loss: 0.03725198281448567\n",
      "Iteration: 1973 lambda_n: 0.9950716322830728 Loss: 0.037233232680415944\n",
      "Iteration: 1974 lambda_n: 0.9587584900347056 Loss: 0.03721437483583279\n",
      "Iteration: 1975 lambda_n: 0.9250755344048076 Loss: 0.03719621580133165\n",
      "Iteration: 1976 lambda_n: 1.019973136925789 Loss: 0.037178704541917296\n",
      "Iteration: 1977 lambda_n: 1.0246947475838784 Loss: 0.03715940768726466\n",
      "Iteration: 1978 lambda_n: 0.9797402603482366 Loss: 0.03714003304470228\n",
      "Iteration: 1979 lambda_n: 0.9783495925383968 Loss: 0.03712151924369995\n",
      "Iteration: 1980 lambda_n: 1.03107079094764 Loss: 0.037103042138286135\n",
      "Iteration: 1981 lambda_n: 1.0198150000820965 Loss: 0.03708358040489322\n",
      "Iteration: 1982 lambda_n: 1.0000811221696437 Loss: 0.03706434237392501\n",
      "Iteration: 1983 lambda_n: 0.9170947022589451 Loss: 0.03704548741452252\n",
      "Iteration: 1984 lambda_n: 0.9204625056851621 Loss: 0.03702820650409358\n",
      "Iteration: 1985 lambda_n: 0.9617981701241378 Loss: 0.037010871022224424\n",
      "Iteration: 1986 lambda_n: 0.9477606204871251 Loss: 0.036992766430177404\n",
      "Iteration: 1987 lambda_n: 0.9900161609647248 Loss: 0.036974935512652116\n",
      "Iteration: 1988 lambda_n: 0.9676564324800719 Loss: 0.036956319440997656\n",
      "Iteration: 1989 lambda_n: 0.9375564808522614 Loss: 0.036938133594406064\n",
      "Iteration: 1990 lambda_n: 0.9878510413643669 Loss: 0.036920522616504726\n",
      "Iteration: 1991 lambda_n: 1.0151190909961894 Loss: 0.03690197647540392\n",
      "Iteration: 1992 lambda_n: 1.0431970568654674 Loss: 0.03688292861684481\n",
      "Iteration: 1993 lambda_n: 1.0252621968923306 Loss: 0.036863364630677194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1994 lambda_n: 1.0466848101400923 Loss: 0.03684414761801064\n",
      "Iteration: 1995 lambda_n: 0.9834683198311047 Loss: 0.036824539799421845\n",
      "Iteration: 1996 lambda_n: 1.045379432035493 Loss: 0.036806126199438356\n",
      "Iteration: 1997 lambda_n: 1.007240265498473 Loss: 0.036786563732901054\n",
      "Iteration: 1998 lambda_n: 0.9874540811800784 Loss: 0.03676772513879064\n",
      "Iteration: 1999 lambda_n: 1.0262383464723257 Loss: 0.03674926621046323\n",
      "Iteration: 2000 lambda_n: 1.02812545791306 Loss: 0.036730092193471835\n",
      "Iteration: 2001 lambda_n: 1.0324560234103144 Loss: 0.036710893075554495\n",
      "Iteration: 2002 lambda_n: 0.907888472116779 Loss: 0.03669162326099272\n",
      "Iteration: 2003 lambda_n: 0.9357562130676922 Loss: 0.03667468694240202\n",
      "Iteration: 2004 lambda_n: 1.0027861371275775 Loss: 0.03665723889460128\n",
      "Iteration: 2005 lambda_n: 0.971304635995947 Loss: 0.036638550081405014\n",
      "Iteration: 2006 lambda_n: 1.0460747947159963 Loss: 0.03662045703642952\n",
      "Iteration: 2007 lambda_n: 0.9476090832396636 Loss: 0.03660098096021824\n",
      "Iteration: 2008 lambda_n: 0.9707944185828655 Loss: 0.036583347069816946\n",
      "Iteration: 2009 lambda_n: 1.037482571465166 Loss: 0.03656529032676994\n",
      "Iteration: 2010 lambda_n: 0.9840523066637273 Loss: 0.03654600270857998\n",
      "Iteration: 2011 lambda_n: 0.9090494635637406 Loss: 0.03652771761538892\n",
      "Iteration: 2012 lambda_n: 0.9958605378462069 Loss: 0.03651083414719505\n",
      "Iteration: 2013 lambda_n: 0.944959907718692 Loss: 0.036492346901709875\n",
      "Iteration: 2014 lambda_n: 1.0305919111550104 Loss: 0.036474812970817644\n",
      "Iteration: 2015 lambda_n: 0.9515736444830846 Loss: 0.03645569922092835\n",
      "Iteration: 2016 lambda_n: 1.0228554545339985 Loss: 0.0364380595684161\n",
      "Iteration: 2017 lambda_n: 1.0336460161142729 Loss: 0.03641910752040023\n",
      "Iteration: 2018 lambda_n: 0.9974019099194805 Loss: 0.03639996503697633\n",
      "Iteration: 2019 lambda_n: 0.9697624706710463 Loss: 0.03638150284451527\n",
      "Iteration: 2020 lambda_n: 0.9763433417071327 Loss: 0.03636356076909316\n",
      "Iteration: 2021 lambda_n: 0.9354198081778723 Loss: 0.03634550533976137\n",
      "Iteration: 2022 lambda_n: 0.9813244002643541 Loss: 0.03632821463617741\n",
      "Iteration: 2023 lambda_n: 0.9770170958108315 Loss: 0.036310083633572715\n",
      "Iteration: 2024 lambda_n: 0.910208024686292 Loss: 0.03629204060426374\n",
      "Iteration: 2025 lambda_n: 0.9414224642105433 Loss: 0.03627523893828495\n",
      "Iteration: 2026 lambda_n: 0.8945903815369955 Loss: 0.03625786864314688\n",
      "Iteration: 2027 lambda_n: 0.8922982774684992 Loss: 0.03624136962886038\n",
      "Iteration: 2028 lambda_n: 1.014743858284508 Loss: 0.03622491979590836\n",
      "Iteration: 2029 lambda_n: 1.0221845998842563 Loss: 0.03620622087412336\n",
      "Iteration: 2030 lambda_n: 0.9246707057419387 Loss: 0.03618739380505127\n",
      "Iteration: 2031 lambda_n: 0.991602022890402 Loss: 0.036170370611898525\n",
      "Iteration: 2032 lambda_n: 1.0380037628112686 Loss: 0.03615212329235586\n",
      "Iteration: 2033 lambda_n: 1.0089192764023729 Loss: 0.03613303105476229\n",
      "Iteration: 2034 lambda_n: 1.0221975203969886 Loss: 0.0361144826043802\n",
      "Iteration: 2035 lambda_n: 0.9663559616912312 Loss: 0.036095698855130365\n",
      "Iteration: 2036 lambda_n: 0.9267234906405212 Loss: 0.036077949437182934\n",
      "Iteration: 2037 lambda_n: 1.002349992548993 Loss: 0.03606093541472763\n",
      "Iteration: 2038 lambda_n: 0.9438165844689099 Loss: 0.036042541036763937\n",
      "Iteration: 2039 lambda_n: 0.9598957750431968 Loss: 0.036025228599757035\n",
      "Iteration: 2040 lambda_n: 0.9961572845731749 Loss: 0.03600762888635923\n",
      "Iteration: 2041 lambda_n: 0.9271086746742222 Loss: 0.035989372457625055\n",
      "Iteration: 2042 lambda_n: 1.01575330998056 Loss: 0.03597238898189001\n",
      "Iteration: 2043 lambda_n: 0.9869205471138176 Loss: 0.03595378981700766\n",
      "Iteration: 2044 lambda_n: 0.9240154917143654 Loss: 0.035935726857758914\n",
      "Iteration: 2045 lambda_n: 0.9880874188024238 Loss: 0.03591882259360547\n",
      "Iteration: 2046 lambda_n: 0.908057675670034 Loss: 0.03590075397052108\n",
      "Iteration: 2047 lambda_n: 0.9170066140694819 Loss: 0.03588415599333215\n",
      "Iteration: 2048 lambda_n: 0.8955119877342537 Loss: 0.03586740135749599\n",
      "Iteration: 2049 lambda_n: 0.9581564416754043 Loss: 0.035851046164429426\n",
      "Iteration: 2050 lambda_n: 0.9258140069212599 Loss: 0.03583355414421545\n",
      "Iteration: 2051 lambda_n: 0.9110422457338052 Loss: 0.035816659763359855\n",
      "Iteration: 2052 lambda_n: 1.0090892679444807 Loss: 0.03580004182492674\n",
      "Iteration: 2053 lambda_n: 0.9418549667002217 Loss: 0.03578164333898945\n",
      "Iteration: 2054 lambda_n: 1.0263459957137984 Loss: 0.03576447828948616\n",
      "Iteration: 2055 lambda_n: 1.048672851347483 Loss: 0.03574578162515327\n",
      "Iteration: 2056 lambda_n: 0.9417542852659636 Loss: 0.03572668711566867\n",
      "Iteration: 2057 lambda_n: 0.8925195421792481 Loss: 0.03570954711538695\n",
      "Iteration: 2058 lambda_n: 0.9825849115724932 Loss: 0.03569330988609922\n",
      "Iteration: 2059 lambda_n: 0.9328487909931434 Loss: 0.03567544158121154\n",
      "Iteration: 2060 lambda_n: 1.0328231313184255 Loss: 0.035658485015940475\n",
      "Iteration: 2061 lambda_n: 0.9262054287109764 Loss: 0.03563971938256209\n",
      "Iteration: 2062 lambda_n: 0.9172816016490298 Loss: 0.03562289832577335\n",
      "Iteration: 2063 lambda_n: 1.042466664644078 Loss: 0.035606246189672536\n",
      "Iteration: 2064 lambda_n: 0.9819721031197249 Loss: 0.035587329667548476\n",
      "Iteration: 2065 lambda_n: 0.9442735642413613 Loss: 0.03556951894204143\n",
      "Iteration: 2066 lambda_n: 0.8944151114693354 Loss: 0.03555239934423929\n",
      "Iteration: 2067 lambda_n: 0.8993580789319509 Loss: 0.03553619033184456\n",
      "Iteration: 2068 lambda_n: 0.9904747978421893 Loss: 0.035519898239960004\n",
      "Iteration: 2069 lambda_n: 0.9182048727575064 Loss: 0.035501963040827055\n",
      "Iteration: 2070 lambda_n: 0.9960440719565659 Loss: 0.03548534356625865\n",
      "Iteration: 2071 lambda_n: 0.8925925005952615 Loss: 0.03546732283524401\n",
      "Iteration: 2072 lambda_n: 1.0190414832605912 Loss: 0.035451180597090334\n",
      "Iteration: 2073 lambda_n: 1.0392381529976615 Loss: 0.03543275933441675\n",
      "Iteration: 2074 lambda_n: 0.9569990412692938 Loss: 0.0354139815499347\n",
      "Iteration: 2075 lambda_n: 0.9000454901503615 Loss: 0.03539669742224268\n",
      "Iteration: 2076 lambda_n: 0.9765229740019886 Loss: 0.035380448638028625\n",
      "Iteration: 2077 lambda_n: 1.0259284996996771 Loss: 0.03536282649085654\n",
      "Iteration: 2078 lambda_n: 1.0100310385234763 Loss: 0.03534432098388707\n",
      "Iteration: 2079 lambda_n: 1.0174589010906896 Loss: 0.03532611046668339\n",
      "Iteration: 2080 lambda_n: 0.9181607779675551 Loss: 0.03530777427428608\n",
      "Iteration: 2081 lambda_n: 0.9289944893446912 Loss: 0.03529123472761599\n",
      "Iteration: 2082 lambda_n: 0.9496493792800393 Loss: 0.03527450687648417\n",
      "Iteration: 2083 lambda_n: 1.019523411601738 Loss: 0.03525741422267192\n",
      "Iteration: 2084 lambda_n: 1.0221901825428643 Loss: 0.03523907190183133\n",
      "Iteration: 2085 lambda_n: 0.9374237331010247 Loss: 0.03522068993255885\n",
      "Iteration: 2086 lambda_n: 1.0206969065982168 Loss: 0.03520383967029557\n",
      "Iteration: 2087 lambda_n: 1.0041007523684953 Loss: 0.03518550050967272\n",
      "Iteration: 2088 lambda_n: 0.9556893080318504 Loss: 0.035167467651768945\n",
      "Iteration: 2089 lambda_n: 0.9302193527480619 Loss: 0.035150311709248\n",
      "Iteration: 2090 lambda_n: 0.9134975391214233 Loss: 0.035133619989219704\n",
      "Iteration: 2091 lambda_n: 0.9384005791673095 Loss: 0.03511723504127666\n",
      "Iteration: 2092 lambda_n: 0.902763227135215 Loss: 0.035100410338820644\n",
      "Iteration: 2093 lambda_n: 0.957068074196923 Loss: 0.03508423121714533\n",
      "Iteration: 2094 lambda_n: 1.0493235258639986 Loss: 0.035067085933567825\n",
      "Iteration: 2095 lambda_n: 0.9243930266958699 Loss: 0.035048296311997906\n",
      "Iteration: 2096 lambda_n: 0.954417000055752 Loss: 0.035031751053308816\n",
      "Iteration: 2097 lambda_n: 0.9620641504235701 Loss: 0.03501467554795988\n",
      "Iteration: 2098 lambda_n: 0.9126861175951296 Loss: 0.03499747057344527\n",
      "Iteration: 2099 lambda_n: 0.9116481951392144 Loss: 0.03498115547825773\n",
      "Iteration: 2100 lambda_n: 0.981080877695644 Loss: 0.03496486556631384\n",
      "Iteration: 2101 lambda_n: 1.0218845094314364 Loss: 0.03494734237109972\n",
      "Iteration: 2102 lambda_n: 0.9053108532365628 Loss: 0.034929098527986174\n",
      "Iteration: 2103 lambda_n: 1.0314355239355137 Loss: 0.034912942875599845\n",
      "Iteration: 2104 lambda_n: 1.0198492622379431 Loss: 0.034894544410030835\n",
      "Iteration: 2105 lambda_n: 0.9067986084430366 Loss: 0.03487636096681127\n",
      "Iteration: 2106 lambda_n: 0.9622861986515093 Loss: 0.03486020016188715\n",
      "Iteration: 2107 lambda_n: 0.9498162742080931 Loss: 0.03484305763290124\n",
      "Iteration: 2108 lambda_n: 1.0427464230279366 Loss: 0.034826144501842224\n",
      "Iteration: 2109 lambda_n: 0.9494741650511307 Loss: 0.034807584863669624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2110 lambda_n: 0.9127123175635669 Loss: 0.03479069293782653\n",
      "Iteration: 2111 lambda_n: 0.9308134567528313 Loss: 0.034774461835512055\n",
      "Iteration: 2112 lambda_n: 0.8939547636990101 Loss: 0.034757915690314854\n",
      "Iteration: 2113 lambda_n: 0.9412034964231107 Loss: 0.03474203127635393\n",
      "Iteration: 2114 lambda_n: 0.9490794203893251 Loss: 0.034725314214707904\n",
      "Iteration: 2115 lambda_n: 0.9824429345393743 Loss: 0.034708464411448196\n",
      "Iteration: 2116 lambda_n: 0.9719825191004008 Loss: 0.03469102988455939\n",
      "Iteration: 2117 lambda_n: 0.9600539609180687 Loss: 0.034673788610834935\n",
      "Iteration: 2118 lambda_n: 1.003972128347746 Loss: 0.03465676637074195\n",
      "Iteration: 2119 lambda_n: 0.9531368844154645 Loss: 0.034638973341207295\n",
      "Iteration: 2120 lambda_n: 1.0448840278473235 Loss: 0.03462208874133549\n",
      "Iteration: 2121 lambda_n: 0.9656789535753546 Loss: 0.03460358722602803\n",
      "Iteration: 2122 lambda_n: 0.9084208608153852 Loss: 0.03458649598962607\n",
      "Iteration: 2123 lambda_n: 0.97829584176976 Loss: 0.034570424996939\n",
      "Iteration: 2124 lambda_n: 1.004559254958536 Loss: 0.03455312524685825\n",
      "Iteration: 2125 lambda_n: 0.986701863017649 Loss: 0.03453536907643599\n",
      "Iteration: 2126 lambda_n: 0.9702658067461069 Loss: 0.03451793645948663\n",
      "Iteration: 2127 lambda_n: 1.0264294613828808 Loss: 0.03450080187997876\n",
      "Iteration: 2128 lambda_n: 0.9293368016231833 Loss: 0.03448268372156607\n",
      "Iteration: 2129 lambda_n: 0.9815904520003205 Loss: 0.03446628675415131\n",
      "Iteration: 2130 lambda_n: 0.9187782334734839 Loss: 0.034448975401504225\n",
      "Iteration: 2131 lambda_n: 0.9677216851343817 Loss: 0.034432778868696744\n",
      "Iteration: 2132 lambda_n: 1.0352508928231647 Loss: 0.034415726916842956\n",
      "Iteration: 2133 lambda_n: 0.8970300902892958 Loss: 0.034397493410577726\n",
      "Iteration: 2134 lambda_n: 0.9478517911414768 Loss: 0.034381701381497584\n",
      "Iteration: 2135 lambda_n: 0.8980333250221822 Loss: 0.0343650217285325\n",
      "Iteration: 2136 lambda_n: 1.0311658641897898 Loss: 0.0343492254764577\n",
      "Iteration: 2137 lambda_n: 0.9503406543809089 Loss: 0.03433109550018916\n",
      "Iteration: 2138 lambda_n: 1.024638742070341 Loss: 0.03431439425226022\n",
      "Iteration: 2139 lambda_n: 0.985284160681676 Loss: 0.03429639550743047\n",
      "Iteration: 2140 lambda_n: 0.9002081575661255 Loss: 0.034279096123241676\n",
      "Iteration: 2141 lambda_n: 0.9975338662866767 Loss: 0.03426329740239602\n",
      "Iteration: 2142 lambda_n: 0.9587068313419673 Loss: 0.034245798308600604\n",
      "Iteration: 2143 lambda_n: 0.9201700358697408 Loss: 0.03422898798444161\n",
      "Iteration: 2144 lambda_n: 0.9954652158866376 Loss: 0.034212860438730036\n",
      "Iteration: 2145 lambda_n: 0.9910335804246387 Loss: 0.034195420997218146\n",
      "Iteration: 2146 lambda_n: 1.0159500736801543 Loss: 0.03417806724216884\n",
      "Iteration: 2147 lambda_n: 1.009797654693073 Loss: 0.034160285521331275\n",
      "Iteration: 2148 lambda_n: 0.9828663302869812 Loss: 0.03414261986310229\n",
      "Iteration: 2149 lambda_n: 0.8924613096988879 Loss: 0.034125433381028525\n",
      "Iteration: 2150 lambda_n: 1.0396782371554103 Loss: 0.0341098346052143\n",
      "Iteration: 2151 lambda_n: 0.9128000514879103 Loss: 0.03409167095179418\n",
      "Iteration: 2152 lambda_n: 0.9931269194497978 Loss: 0.03407573125747941\n",
      "Iteration: 2153 lambda_n: 0.9528941117080668 Loss: 0.034058396643814784\n",
      "Iteration: 2154 lambda_n: 0.968589370299925 Loss: 0.03404177191951426\n",
      "Iteration: 2155 lambda_n: 0.9412433485613323 Loss: 0.03402488103978629\n",
      "Iteration: 2156 lambda_n: 0.9543822761489147 Loss: 0.034008474459512786\n",
      "Iteration: 2157 lambda_n: 0.8994238007653343 Loss: 0.033991846331330004\n",
      "Iteration: 2158 lambda_n: 0.9122778096918027 Loss: 0.033976182640296504\n",
      "Iteration: 2159 lambda_n: 0.9945430831555873 Loss: 0.03396030193419246\n",
      "Iteration: 2160 lambda_n: 0.9422767671567026 Loss: 0.03394299702582623\n",
      "Iteration: 2161 lambda_n: 0.8935219224465181 Loss: 0.03392660911833264\n",
      "Iteration: 2162 lambda_n: 1.0154276132884998 Loss: 0.03391107596082412\n",
      "Iteration: 2163 lambda_n: 0.903585295814934 Loss: 0.03389343161825093\n",
      "Iteration: 2164 lambda_n: 0.9416329519855933 Loss: 0.033877737900697785\n",
      "Iteration: 2165 lambda_n: 0.9147170289007545 Loss: 0.03386139059527697\n",
      "Iteration: 2166 lambda_n: 0.9897969652533362 Loss: 0.03384551764798625\n",
      "Iteration: 2167 lambda_n: 1.0326994033133265 Loss: 0.03382834971251015\n",
      "Iteration: 2168 lambda_n: 0.9834828113261667 Loss: 0.033810446362731374\n",
      "Iteration: 2169 lambda_n: 0.9847580467028695 Loss: 0.03379340455307526\n",
      "Iteration: 2170 lambda_n: 0.9371112180883334 Loss: 0.03377634876379255\n",
      "Iteration: 2171 lambda_n: 0.9241293071441268 Loss: 0.03376012576137259\n",
      "Iteration: 2172 lambda_n: 0.9193574762718195 Loss: 0.03374413471863241\n",
      "Iteration: 2173 lambda_n: 0.8973921710541161 Loss: 0.033728233369116616\n",
      "Iteration: 2174 lambda_n: 1.0039457498281912 Loss: 0.03371271879421213\n",
      "Iteration: 2175 lambda_n: 1.0343270176843795 Loss: 0.033695370098397376\n",
      "Iteration: 2176 lambda_n: 0.9698722454152962 Loss: 0.03367750528569538\n",
      "Iteration: 2177 lambda_n: 1.0376573442283388 Loss: 0.033660761939836394\n",
      "Iteration: 2178 lambda_n: 0.917883316641023 Loss: 0.03364285718910518\n",
      "Iteration: 2179 lambda_n: 0.9575021063015292 Loss: 0.033627026737718224\n",
      "Iteration: 2180 lambda_n: 0.9734012234192753 Loss: 0.03361052059834007\n",
      "Iteration: 2181 lambda_n: 0.9837897527409303 Loss: 0.033593748349029416\n",
      "Iteration: 2182 lambda_n: 1.009369090641038 Loss: 0.03357680527537908\n",
      "Iteration: 2183 lambda_n: 0.8995990874439902 Loss: 0.0335594302202703\n",
      "Iteration: 2184 lambda_n: 0.980277015433229 Loss: 0.033543952036779845\n",
      "Iteration: 2185 lambda_n: 0.9478107163386635 Loss: 0.03352709358716127\n",
      "Iteration: 2186 lambda_n: 0.9916927566672329 Loss: 0.03351080128035696\n",
      "Iteration: 2187 lambda_n: 0.9547651377413328 Loss: 0.03349376288213606\n",
      "Iteration: 2188 lambda_n: 0.916509180628617 Loss: 0.03347736689279913\n",
      "Iteration: 2189 lambda_n: 0.9017383275795922 Loss: 0.03346163521165899\n",
      "Iteration: 2190 lambda_n: 0.8981576627757313 Loss: 0.033446164099454105\n",
      "Iteration: 2191 lambda_n: 0.9550167139461666 Loss: 0.03343076136011845\n",
      "Iteration: 2192 lambda_n: 0.9613208067773582 Loss: 0.033414391133871665\n",
      "Iteration: 2193 lambda_n: 0.9093807337471849 Loss: 0.033397920773939535\n",
      "Iteration: 2194 lambda_n: 0.9875868164834211 Loss: 0.03338234763689681\n",
      "Iteration: 2195 lambda_n: 0.9039284233206212 Loss: 0.03336544329933508\n",
      "Iteration: 2196 lambda_n: 0.9384051608873039 Loss: 0.0333499783139312\n",
      "Iteration: 2197 lambda_n: 1.0456079690630278 Loss: 0.03333393095299758\n",
      "Iteration: 2198 lambda_n: 0.9840530653741656 Loss: 0.03331605932949263\n",
      "Iteration: 2199 lambda_n: 0.9074267868409241 Loss: 0.033299248469002875\n",
      "Iteration: 2200 lambda_n: 0.9322801509951565 Loss: 0.033283754090314255\n",
      "Iteration: 2201 lambda_n: 0.960051999980326 Loss: 0.03326784279232071\n",
      "Iteration: 2202 lambda_n: 0.9561975098162051 Loss: 0.03325146541516075\n",
      "Iteration: 2203 lambda_n: 0.9845362729563198 Loss: 0.03323516177525622\n",
      "Iteration: 2204 lambda_n: 0.9043811214722922 Loss: 0.03321838328103271\n",
      "Iteration: 2205 lambda_n: 0.9782499154512386 Loss: 0.033202978258179604\n",
      "Iteration: 2206 lambda_n: 0.9447762326017131 Loss: 0.03318632302474481\n",
      "Iteration: 2207 lambda_n: 1.008084093563685 Loss: 0.03317024565715181\n",
      "Iteration: 2208 lambda_n: 1.0492731330629255 Loss: 0.03315309960422223\n",
      "Iteration: 2209 lambda_n: 1.048408229299123 Loss: 0.03313526246448497\n",
      "Iteration: 2210 lambda_n: 1.0026392619407256 Loss: 0.03311744969969848\n",
      "Iteration: 2211 lambda_n: 0.9710084785484473 Loss: 0.03310042362126769\n",
      "Iteration: 2212 lambda_n: 0.9944729050963865 Loss: 0.03308394312601412\n",
      "Iteration: 2213 lambda_n: 0.978343750084241 Loss: 0.033067073010887564\n",
      "Iteration: 2214 lambda_n: 0.9844742507279198 Loss: 0.03305048504438994\n",
      "Iteration: 2215 lambda_n: 1.0209902014509697 Loss: 0.03303380169106441\n",
      "Iteration: 2216 lambda_n: 0.9352742858851693 Loss: 0.03301650860043717\n",
      "Iteration: 2217 lambda_n: 1.0336411553601714 Loss: 0.033000675455101956\n",
      "Iteration: 2218 lambda_n: 1.031428060322023 Loss: 0.03298318611622221\n",
      "Iteration: 2219 lambda_n: 0.9354916824418069 Loss: 0.032965743707687144\n",
      "Iteration: 2220 lambda_n: 1.0205823449076177 Loss: 0.03294993187928951\n",
      "Iteration: 2221 lambda_n: 0.9582247230923979 Loss: 0.032932690747592266\n",
      "Iteration: 2222 lambda_n: 1.0405819223926271 Loss: 0.03291651152934401\n",
      "Iteration: 2223 lambda_n: 0.9163260215723433 Loss: 0.03289895105656087\n",
      "Iteration: 2224 lambda_n: 0.8969914627577015 Loss: 0.03288349552255771\n",
      "Iteration: 2225 lambda_n: 1.0329450919827012 Loss: 0.03286837340499961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2226 lambda_n: 0.9514693233069897 Loss: 0.03285096824538163\n",
      "Iteration: 2227 lambda_n: 1.022942288112905 Loss: 0.032834944457114955\n",
      "Iteration: 2228 lambda_n: 1.0025491580459474 Loss: 0.03281772609057862\n",
      "Iteration: 2229 lambda_n: 0.915200231767169 Loss: 0.03280086015401141\n",
      "Iteration: 2230 lambda_n: 0.9237590523847166 Loss: 0.03278547162965937\n",
      "Iteration: 2231 lambda_n: 0.8964975732749609 Loss: 0.03276994688496962\n",
      "Iteration: 2232 lambda_n: 0.9357656281949959 Loss: 0.032754887696332606\n",
      "Iteration: 2233 lambda_n: 1.0178513893167358 Loss: 0.03273917667326291\n",
      "Iteration: 2234 lambda_n: 1.031928895483921 Loss: 0.03272209651036536\n",
      "Iteration: 2235 lambda_n: 1.0371324174972072 Loss: 0.03270478974707069\n",
      "Iteration: 2236 lambda_n: 0.9094656476302181 Loss: 0.03268740549699814\n",
      "Iteration: 2237 lambda_n: 0.9665836926955631 Loss: 0.03267216926107136\n",
      "Iteration: 2238 lambda_n: 0.9699391441135381 Loss: 0.03265598441901563\n",
      "Iteration: 2239 lambda_n: 0.9865213347515531 Loss: 0.032639751991231696\n",
      "Iteration: 2240 lambda_n: 0.90320400285312 Loss: 0.032623250900676325\n",
      "Iteration: 2241 lambda_n: 1.0227021593756433 Loss: 0.03260815125771583\n",
      "Iteration: 2242 lambda_n: 1.0323494020205057 Loss: 0.032591062914933314\n",
      "Iteration: 2243 lambda_n: 0.9702023048952493 Loss: 0.032573823144076225\n",
      "Iteration: 2244 lambda_n: 0.9738678572182808 Loss: 0.032557630160860364\n",
      "Iteration: 2245 lambda_n: 0.9613313185988733 Loss: 0.03254138474082044\n",
      "Iteration: 2246 lambda_n: 0.930412896756623 Loss: 0.03252535704983425\n",
      "Iteration: 2247 lambda_n: 0.965039129518554 Loss: 0.03250985299352245\n",
      "Iteration: 2248 lambda_n: 0.9525446000731351 Loss: 0.03249378042044817\n",
      "Iteration: 2249 lambda_n: 1.0225886605458296 Loss: 0.03247792442383951\n",
      "Iteration: 2250 lambda_n: 0.9161483020491639 Loss: 0.03246091187033561\n",
      "Iteration: 2251 lambda_n: 0.9212837026780079 Loss: 0.03244567841077021\n",
      "Iteration: 2252 lambda_n: 1.0401545765337077 Loss: 0.03243036745549014\n",
      "Iteration: 2253 lambda_n: 0.9052736777220602 Loss: 0.03241309049419126\n",
      "Iteration: 2254 lambda_n: 1.0248321210598084 Loss: 0.03239806214082585\n",
      "Iteration: 2255 lambda_n: 0.9641018021467544 Loss: 0.03238105827394416\n",
      "Iteration: 2256 lambda_n: 0.9364173042485057 Loss: 0.032365071029799845\n",
      "Iteration: 2257 lambda_n: 1.022191321680055 Loss: 0.032349551223937976\n",
      "Iteration: 2258 lambda_n: 0.9206623683835954 Loss: 0.032332619250924524\n",
      "Iteration: 2259 lambda_n: 1.0260839981137522 Loss: 0.03231737746901201\n",
      "Iteration: 2260 lambda_n: 0.9932124658978838 Loss: 0.03230039983019518\n",
      "Iteration: 2261 lambda_n: 0.9253909946122482 Loss: 0.03228397556075527\n",
      "Iteration: 2262 lambda_n: 0.9104556116719272 Loss: 0.03226868122225863\n",
      "Iteration: 2263 lambda_n: 0.9577998923046184 Loss: 0.03225364164661532\n",
      "Iteration: 2264 lambda_n: 0.9741266304440502 Loss: 0.03223782849314407\n",
      "Iteration: 2265 lambda_n: 0.9805566167471822 Loss: 0.032221754728875944\n",
      "Iteration: 2266 lambda_n: 0.9695721695488514 Loss: 0.03220558398522925\n",
      "Iteration: 2267 lambda_n: 0.926958593304617 Loss: 0.03218960340023839\n",
      "Iteration: 2268 lambda_n: 0.9253491690383964 Loss: 0.03217433356560775\n",
      "Iteration: 2269 lambda_n: 0.959085123394179 Loss: 0.03215909843388744\n",
      "Iteration: 2270 lambda_n: 0.9459572934218389 Loss: 0.0321433165155009\n",
      "Iteration: 2271 lambda_n: 1.0336192360720555 Loss: 0.032127759253840794\n",
      "Iteration: 2272 lambda_n: 1.0127164181823565 Loss: 0.03211077011921278\n",
      "Iteration: 2273 lambda_n: 0.9785431373851249 Loss: 0.03209413451545553\n",
      "Iteration: 2274 lambda_n: 1.0182229317275568 Loss: 0.03207806964612346\n",
      "Iteration: 2275 lambda_n: 1.0315442133400317 Loss: 0.032061363150278875\n",
      "Iteration: 2276 lambda_n: 1.0300181801369053 Loss: 0.03204444829459633\n",
      "Iteration: 2277 lambda_n: 0.9697788341382575 Loss: 0.032027568730755386\n",
      "Iteration: 2278 lambda_n: 0.9293572321527424 Loss: 0.03201168574002448\n",
      "Iteration: 2279 lambda_n: 1.0481058952915527 Loss: 0.0319964733329301\n",
      "Iteration: 2280 lambda_n: 0.8965428200661016 Loss: 0.03197932722754357\n",
      "Iteration: 2281 lambda_n: 0.962443581009131 Loss: 0.031964669043814495\n",
      "Iteration: 2282 lambda_n: 1.0105762053004035 Loss: 0.03194894212033539\n",
      "Iteration: 2283 lambda_n: 0.9183978466884531 Loss: 0.03193243840764068\n",
      "Iteration: 2284 lambda_n: 0.9167281462154987 Loss: 0.03191744871442028\n",
      "Iteration: 2285 lambda_n: 0.9491405884687528 Loss: 0.031902494504562506\n",
      "Iteration: 2286 lambda_n: 0.9947048811036431 Loss: 0.03188702024112732\n",
      "Iteration: 2287 lambda_n: 0.9817938578360507 Loss: 0.03187081261027653\n",
      "Iteration: 2288 lambda_n: 0.8945349812247909 Loss: 0.03185482488619469\n",
      "Iteration: 2289 lambda_n: 0.9812018438861657 Loss: 0.03184026636248725\n",
      "Iteration: 2290 lambda_n: 1.0485266000755447 Loss: 0.0318243064094102\n",
      "Iteration: 2291 lambda_n: 0.9183756114195807 Loss: 0.03180726187502314\n",
      "Iteration: 2292 lambda_n: 0.9137433028703872 Loss: 0.0317923419644777\n",
      "Iteration: 2293 lambda_n: 0.8999375647061895 Loss: 0.031777505597275955\n",
      "Iteration: 2294 lambda_n: 0.9974798402618091 Loss: 0.031762901483360244\n",
      "Iteration: 2295 lambda_n: 0.97722668662988 Loss: 0.031746723856797596\n",
      "Iteration: 2296 lambda_n: 1.0047754863605118 Loss: 0.031730884299240356\n",
      "Iteration: 2297 lambda_n: 1.0226438202111907 Loss: 0.031714608127179994\n",
      "Iteration: 2298 lambda_n: 0.9924747539836479 Loss: 0.0316980528478272\n",
      "Iteration: 2299 lambda_n: 0.9422908753246595 Loss: 0.03168199595436123\n",
      "Iteration: 2300 lambda_n: 0.9262805815147713 Loss: 0.031666760086084184\n",
      "Iteration: 2301 lambda_n: 1.0240623422663817 Loss: 0.031651791756991864\n",
      "Iteration: 2302 lambda_n: 1.0186493716380671 Loss: 0.031635253332298184\n",
      "Iteration: 2303 lambda_n: 0.9985956680255481 Loss: 0.03161881277955984\n",
      "Iteration: 2304 lambda_n: 0.9749657786951392 Loss: 0.03160270602024555\n",
      "Iteration: 2305 lambda_n: 0.9024736535576513 Loss: 0.03158699009316254\n",
      "Iteration: 2306 lambda_n: 0.9905942508282402 Loss: 0.03157245125062914\n",
      "Iteration: 2307 lambda_n: 0.9553753985001284 Loss: 0.03155650226388008\n",
      "Iteration: 2308 lambda_n: 0.9712860913011482 Loss: 0.0315411297249035\n",
      "Iteration: 2309 lambda_n: 1.024265612944285 Loss: 0.03152551065917488\n",
      "Iteration: 2310 lambda_n: 1.0390299527398887 Loss: 0.03150905001615316\n",
      "Iteration: 2311 lambda_n: 1.0258423887623525 Loss: 0.031492362999943284\n",
      "Iteration: 2312 lambda_n: 0.9092381253925648 Loss: 0.031475898564934074\n",
      "Iteration: 2313 lambda_n: 0.9060686566902544 Loss: 0.03146131456274255\n",
      "Iteration: 2314 lambda_n: 1.0374346066777687 Loss: 0.031446789798665886\n",
      "Iteration: 2315 lambda_n: 1.0464845053229712 Loss: 0.03143016948170339\n",
      "Iteration: 2316 lambda_n: 1.02310066919317 Loss: 0.03141341535125266\n",
      "Iteration: 2317 lambda_n: 0.9538171949966182 Loss: 0.03139704645758622\n",
      "Iteration: 2318 lambda_n: 0.9143493177440606 Loss: 0.03138179573995403\n",
      "Iteration: 2319 lambda_n: 1.0387585083121067 Loss: 0.031367184869990496\n",
      "Iteration: 2320 lambda_n: 0.9844841317591039 Loss: 0.03135059645674771\n",
      "Iteration: 2321 lambda_n: 0.9655364184026455 Loss: 0.03133488505666611\n",
      "Iteration: 2322 lambda_n: 1.0126314936704925 Loss: 0.031319485776362864\n",
      "Iteration: 2323 lambda_n: 0.8970427898820765 Loss: 0.03130334575064967\n",
      "Iteration: 2324 lambda_n: 0.9050525343214875 Loss: 0.03128905693938874\n",
      "Iteration: 2325 lambda_n: 1.042449419014447 Loss: 0.03127464901108613\n",
      "Iteration: 2326 lambda_n: 1.0177403350676193 Loss: 0.031258064358861096\n",
      "Iteration: 2327 lambda_n: 0.9934238410173608 Loss: 0.031241883730086465\n",
      "Iteration: 2328 lambda_n: 1.0305655787992771 Loss: 0.03122610011975678\n",
      "Iteration: 2329 lambda_n: 0.9475839255989081 Loss: 0.03120973729455007\n",
      "Iteration: 2330 lambda_n: 1.0471479136736088 Loss: 0.031194701818498195\n",
      "Iteration: 2331 lambda_n: 0.9958711733664355 Loss: 0.031178097489307017\n",
      "Iteration: 2332 lambda_n: 0.9374336648371663 Loss: 0.031162316917893704\n",
      "Iteration: 2333 lambda_n: 1.0326172550872206 Loss: 0.031147471872181\n",
      "Iteration: 2334 lambda_n: 0.9788797339629814 Loss: 0.031131130223718034\n",
      "Iteration: 2335 lambda_n: 0.9461065205520286 Loss: 0.031115649376401655\n",
      "Iteration: 2336 lambda_n: 0.9860531993028674 Loss: 0.031100696448416783\n",
      "Iteration: 2337 lambda_n: 0.902866318810274 Loss: 0.03108512224951907\n",
      "Iteration: 2338 lambda_n: 0.9211580674277237 Loss: 0.031070870974516458\n",
      "Iteration: 2339 lambda_n: 0.9223840599501297 Loss: 0.03105633988386071\n",
      "Iteration: 2340 lambda_n: 1.005007967590018 Loss: 0.031041798483363355\n",
      "Iteration: 2341 lambda_n: 1.0326059850038427 Loss: 0.0310259648174176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2342 lambda_n: 1.0200810311702269 Loss: 0.031009707557729256\n",
      "Iteration: 2343 lambda_n: 0.9269631474926804 Loss: 0.03099365865952078\n",
      "Iteration: 2344 lambda_n: 0.9350054524492808 Loss: 0.030979084423440877\n",
      "Iteration: 2345 lambda_n: 1.0146515224559174 Loss: 0.030964393056566817\n",
      "Iteration: 2346 lambda_n: 1.014565355623905 Loss: 0.03094846084326866\n",
      "Iteration: 2347 lambda_n: 0.9494351017861083 Loss: 0.030932541031160214\n",
      "Iteration: 2348 lambda_n: 0.9282154700299875 Loss: 0.030917653216464634\n",
      "Iteration: 2349 lambda_n: 1.04597439497198 Loss: 0.030903107521452543\n",
      "Iteration: 2350 lambda_n: 0.998380250795257 Loss: 0.030886727605328772\n",
      "Iteration: 2351 lambda_n: 1.0270579320369164 Loss: 0.0308711040302611\n",
      "Iteration: 2352 lambda_n: 1.0453539094558317 Loss: 0.030855042928904904\n",
      "Iteration: 2353 lambda_n: 0.9376924532912295 Loss: 0.030838707448319406\n",
      "Iteration: 2354 lambda_n: 0.9815925294509167 Loss: 0.03082406445119099\n",
      "Iteration: 2355 lambda_n: 1.0383873709355864 Loss: 0.03080874614527547\n",
      "Iteration: 2356 lambda_n: 1.0049722896525943 Loss: 0.030792552935668443\n",
      "Iteration: 2357 lambda_n: 0.9970321155244164 Loss: 0.030776892009135626\n",
      "Iteration: 2358 lambda_n: 0.9240957497990493 Loss: 0.030761365710930102\n",
      "Iteration: 2359 lambda_n: 1.0022399814511316 Loss: 0.030746984917811127\n",
      "Iteration: 2360 lambda_n: 0.9571017000335429 Loss: 0.030731398611142564\n",
      "Iteration: 2361 lambda_n: 0.9059881361288106 Loss: 0.030716524550622237\n",
      "Iteration: 2362 lambda_n: 1.0221194284460955 Loss: 0.030702454098225262\n",
      "Iteration: 2363 lambda_n: 1.0134349922729402 Loss: 0.030686590904040936\n",
      "Iteration: 2364 lambda_n: 0.939258586540597 Loss: 0.030670873849863476\n",
      "Iteration: 2365 lambda_n: 0.9384894587849048 Loss: 0.030656317288335254\n",
      "Iteration: 2366 lambda_n: 0.940424101692467 Loss: 0.030641782378805917\n",
      "Iteration: 2367 lambda_n: 0.9631163107163414 Loss: 0.030627227278667228\n",
      "Iteration: 2368 lambda_n: 0.8982416167873847 Loss: 0.03061233112155728\n",
      "Iteration: 2369 lambda_n: 1.0149582843490845 Loss: 0.030598447629837492\n",
      "Iteration: 2370 lambda_n: 1.0050158631340242 Loss: 0.030582770917202788\n",
      "Iteration: 2371 lambda_n: 0.9508951043008294 Loss: 0.030567259065063662\n",
      "Iteration: 2372 lambda_n: 1.036865016498001 Loss: 0.030552592897733415\n",
      "Iteration: 2373 lambda_n: 0.9581035226276092 Loss: 0.03053661227099267\n",
      "Iteration: 2374 lambda_n: 1.0215377493443512 Loss: 0.030521856232175588\n",
      "Iteration: 2375 lambda_n: 1.0049674711098635 Loss: 0.03050613454259292\n",
      "Iteration: 2376 lambda_n: 0.9269181928987413 Loss: 0.030490679288676638\n",
      "Iteration: 2377 lambda_n: 0.9390500648730279 Loss: 0.030476434395639795\n",
      "Iteration: 2378 lambda_n: 0.9858041399640541 Loss: 0.030462012910883116\n",
      "Iteration: 2379 lambda_n: 0.9378257309420308 Loss: 0.030446884082917843\n",
      "Iteration: 2380 lambda_n: 1.0154137640201377 Loss: 0.030432501735977266\n",
      "Iteration: 2381 lambda_n: 0.9156635491598715 Loss: 0.030416940711350763\n",
      "Iteration: 2382 lambda_n: 0.9659969826952856 Loss: 0.03040291833860437\n",
      "Iteration: 2383 lambda_n: 0.9118546514132277 Loss: 0.03038813545964881\n",
      "Iteration: 2384 lambda_n: 0.9621809413021383 Loss: 0.03037419084449522\n",
      "Iteration: 2385 lambda_n: 0.970068940027087 Loss: 0.03035948685191203\n",
      "Iteration: 2386 lambda_n: 0.9864091573558512 Loss: 0.0303446729768594\n",
      "Iteration: 2387 lambda_n: 1.0387510380872222 Loss: 0.03032962056450552\n",
      "Iteration: 2388 lambda_n: 0.9573958891214353 Loss: 0.030313781426844502\n",
      "Iteration: 2389 lambda_n: 0.9860289464483627 Loss: 0.03029919373178497\n",
      "Iteration: 2390 lambda_n: 1.0004306199918573 Loss: 0.030284180724934006\n",
      "Iteration: 2391 lambda_n: 0.9182679370931367 Loss: 0.03026895983132531\n",
      "Iteration: 2392 lambda_n: 0.9650094170244216 Loss: 0.030254999100352776\n",
      "Iteration: 2393 lambda_n: 0.9597997550555911 Loss: 0.030240338189426084\n",
      "Iteration: 2394 lambda_n: 0.9363815866962293 Loss: 0.030225767060230304\n",
      "Iteration: 2395 lambda_n: 0.920898669226673 Loss: 0.03021156168712032\n",
      "Iteration: 2396 lambda_n: 0.9431004476428398 Loss: 0.030197601070868756\n",
      "Iteration: 2397 lambda_n: 1.033769537045078 Loss: 0.0301833140434111\n",
      "Iteration: 2398 lambda_n: 1.0110102934378067 Loss: 0.030167665302230858\n",
      "Iteration: 2399 lambda_n: 0.9409522201267511 Loss: 0.030152373066767286\n",
      "Iteration: 2400 lambda_n: 1.037204536385562 Loss: 0.0301381511737505\n",
      "Iteration: 2401 lambda_n: 1.046418672561857 Loss: 0.030122486421667278\n",
      "Iteration: 2402 lambda_n: 0.9859356001330323 Loss: 0.030106695209776477\n",
      "Iteration: 2403 lambda_n: 1.037073110264954 Loss: 0.03009182842019165\n",
      "Iteration: 2404 lambda_n: 0.9842619230231365 Loss: 0.030076202793814216\n",
      "Iteration: 2405 lambda_n: 0.9867310280303428 Loss: 0.030061384517824353\n",
      "Iteration: 2406 lambda_n: 1.0156487697770649 Loss: 0.0300465404671042\n",
      "Iteration: 2407 lambda_n: 1.0022967436324213 Loss: 0.03003127332486572\n",
      "Iteration: 2408 lambda_n: 0.9747576174756274 Loss: 0.03001621877880129\n",
      "Iteration: 2409 lambda_n: 1.0439603439481842 Loss: 0.030001589217542146\n",
      "Iteration: 2410 lambda_n: 0.8988205164317385 Loss: 0.02998593346052008\n",
      "Iteration: 2411 lambda_n: 0.9775460893969297 Loss: 0.029972464604431818\n",
      "Iteration: 2412 lambda_n: 1.0271763204482314 Loss: 0.029957826890986236\n",
      "Iteration: 2413 lambda_n: 1.0315969185202054 Loss: 0.029942458213777726\n",
      "Iteration: 2414 lambda_n: 1.0278326756495104 Loss: 0.0299270359931953\n",
      "Iteration: 2415 lambda_n: 1.0168678545664012 Loss: 0.0299116826229569\n",
      "Iteration: 2416 lambda_n: 0.9522052463237114 Loss: 0.02989650541251408\n",
      "Iteration: 2417 lambda_n: 0.9463266767111679 Loss: 0.029882304493452645\n",
      "Iteration: 2418 lambda_n: 1.0120107647260925 Loss: 0.029868201967955043\n",
      "Iteration: 2419 lambda_n: 0.9401758706206153 Loss: 0.029853132438409393\n",
      "Iteration: 2420 lambda_n: 0.9761172319431729 Loss: 0.02983914356469764\n",
      "Iteration: 2421 lambda_n: 0.9618487073551373 Loss: 0.029824631132477363\n",
      "Iteration: 2422 lambda_n: 1.0086471083797959 Loss: 0.02981034202751619\n",
      "Iteration: 2423 lambda_n: 1.0251945100654416 Loss: 0.029795369640745632\n",
      "Iteration: 2424 lambda_n: 0.9845168135126842 Loss: 0.029780164178488402\n",
      "Iteration: 2425 lambda_n: 0.9761801520412605 Loss: 0.029765573970168286\n",
      "Iteration: 2426 lambda_n: 0.9306594506200887 Loss: 0.02975111886759446\n",
      "Iteration: 2427 lambda_n: 0.9389088581920216 Loss: 0.029737348560366784\n",
      "Iteration: 2428 lambda_n: 0.9173241138654017 Loss: 0.029723466824453783\n",
      "Iteration: 2429 lambda_n: 0.9939065686187297 Loss: 0.029709914545981368\n",
      "Iteration: 2430 lambda_n: 1.0196404276874778 Loss: 0.029695242399394608\n",
      "Iteration: 2431 lambda_n: 0.9810303260640645 Loss: 0.029680202856223966\n",
      "Iteration: 2432 lambda_n: 0.8939439588878197 Loss: 0.02966574476322926\n",
      "Iteration: 2433 lambda_n: 0.9634932196616421 Loss: 0.029652580345223493\n",
      "Iteration: 2434 lambda_n: 0.9144710155397565 Loss: 0.029638402663984837\n",
      "Iteration: 2435 lambda_n: 1.02301205793745 Loss: 0.029624956845196043\n",
      "Iteration: 2436 lambda_n: 1.0265215940293104 Loss: 0.029609927251070534\n",
      "Iteration: 2437 lambda_n: 0.9414386243849195 Loss: 0.029594859006768153\n",
      "Iteration: 2438 lambda_n: 0.9187214770017986 Loss: 0.029581051075316653\n",
      "Iteration: 2439 lambda_n: 1.0179702562606106 Loss: 0.02956758685034504\n",
      "Iteration: 2440 lambda_n: 0.9490119198076513 Loss: 0.02955268024718406\n",
      "Iteration: 2441 lambda_n: 1.0072975005499991 Loss: 0.02953879495374986\n",
      "Iteration: 2442 lambda_n: 1.044574939635396 Loss: 0.029524069047161598\n",
      "Iteration: 2443 lambda_n: 0.9758646205343827 Loss: 0.02950881144264395\n",
      "Iteration: 2444 lambda_n: 1.0343299229030756 Loss: 0.029494569681126202\n",
      "Iteration: 2445 lambda_n: 0.932698166294023 Loss: 0.029479487589154803\n",
      "Iteration: 2446 lambda_n: 0.9043759741630737 Loss: 0.02946589885047591\n",
      "Iteration: 2447 lambda_n: 0.914392837962888 Loss: 0.029452733092474214\n",
      "Iteration: 2448 lambda_n: 0.9513026743951556 Loss: 0.029439431882249876\n",
      "Iteration: 2449 lambda_n: 0.9418838485349132 Loss: 0.029425604847520104\n",
      "Iteration: 2450 lambda_n: 1.041270542690868 Loss: 0.02941192585345974\n",
      "Iteration: 2451 lambda_n: 0.9065021009899878 Loss: 0.029396816306342388\n",
      "Iteration: 2452 lambda_n: 0.9133706413989016 Loss: 0.02938367345957701\n",
      "Iteration: 2453 lambda_n: 1.0284916850324965 Loss: 0.02937044149174018\n",
      "Iteration: 2454 lambda_n: 0.8990751255221587 Loss: 0.029355554347205912\n",
      "Iteration: 2455 lambda_n: 0.9888322759553714 Loss: 0.029342551397081144\n",
      "Iteration: 2456 lambda_n: 0.9373656157451818 Loss: 0.02932826210759598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2457 lambda_n: 0.9575518472953696 Loss: 0.029314728040685873\n",
      "Iteration: 2458 lambda_n: 1.0263871160142641 Loss: 0.02930091395799632\n",
      "Iteration: 2459 lambda_n: 0.9388126683791714 Loss: 0.029286119702635793\n",
      "Iteration: 2460 lambda_n: 0.9004775817618625 Loss: 0.029272599428460284\n",
      "Iteration: 2461 lambda_n: 1.0163845623578616 Loss: 0.02925964175145073\n",
      "Iteration: 2462 lambda_n: 0.9164455385641486 Loss: 0.02924502859060118\n",
      "Iteration: 2463 lambda_n: 1.0284840397405806 Loss: 0.02923186359509876\n",
      "Iteration: 2464 lambda_n: 0.9243557041223067 Loss: 0.029217101898751215\n",
      "Iteration: 2465 lambda_n: 0.9393229648074715 Loss: 0.029203846276282138\n",
      "Iteration: 2466 lambda_n: 0.9829441554517389 Loss: 0.029190387219851666\n",
      "Iteration: 2467 lambda_n: 1.0068994681240873 Loss: 0.029176315247371035\n",
      "Iteration: 2468 lambda_n: 0.9478620536110739 Loss: 0.029161913185618976\n",
      "Iteration: 2469 lambda_n: 0.9617195364795721 Loss: 0.029148367466294203\n",
      "Iteration: 2470 lambda_n: 0.9365250282512945 Loss: 0.02913463553275469\n",
      "Iteration: 2471 lambda_n: 0.9146179416163249 Loss: 0.029121274797984\n",
      "Iteration: 2472 lambda_n: 1.0376964741429542 Loss: 0.02910823752397285\n",
      "Iteration: 2473 lambda_n: 0.9071434523022551 Loss: 0.029093458942681727\n",
      "Iteration: 2474 lambda_n: 0.9163829012702304 Loss: 0.029080551079966814\n",
      "Iteration: 2475 lambda_n: 0.9665940011874925 Loss: 0.029067522579920946\n",
      "Iteration: 2476 lambda_n: 0.9340471707036087 Loss: 0.029053792027636225\n",
      "Iteration: 2477 lambda_n: 0.9790872991134885 Loss: 0.029040535345360333\n",
      "Iteration: 2478 lambda_n: 0.9073461494138788 Loss: 0.029026651613758666\n",
      "Iteration: 2479 lambda_n: 1.0340820630390077 Loss: 0.02901379634868465\n",
      "Iteration: 2480 lambda_n: 0.9695148114232519 Loss: 0.028999158597494856\n",
      "Iteration: 2481 lambda_n: 0.9152026008523785 Loss: 0.028985447514155518\n",
      "Iteration: 2482 lambda_n: 0.9651216296361023 Loss: 0.028972515817735665\n",
      "Iteration: 2483 lambda_n: 0.905088944964172 Loss: 0.028958890669438696\n",
      "Iteration: 2484 lambda_n: 1.0230137336873892 Loss: 0.028946124147545084\n",
      "Iteration: 2485 lambda_n: 1.042040954296714 Loss: 0.028931707233399953\n",
      "Iteration: 2486 lambda_n: 1.0160304830805407 Loss: 0.028917036344435793\n",
      "Iteration: 2487 lambda_n: 1.0453034827058567 Loss: 0.02890274544506922\n",
      "Iteration: 2488 lambda_n: 1.0076383069576462 Loss: 0.02888805703869674\n",
      "Iteration: 2489 lambda_n: 0.9748030279230703 Loss: 0.028873911578048746\n",
      "Iteration: 2490 lambda_n: 0.9271163626835651 Loss: 0.028860239867199742\n",
      "Iteration: 2491 lambda_n: 1.0221167426350917 Loss: 0.028847248663132805\n",
      "Iteration: 2492 lambda_n: 0.9420898855048846 Loss: 0.02883293950334017\n",
      "Iteration: 2493 lambda_n: 0.9515697637174387 Loss: 0.028819762992012755\n",
      "Iteration: 2494 lambda_n: 0.8986901968799916 Loss: 0.028806465895248146\n",
      "Iteration: 2495 lambda_n: 0.9712762438485747 Loss: 0.02879391882235304\n",
      "Iteration: 2496 lambda_n: 0.9964610727280414 Loss: 0.028780370472217934\n",
      "Iteration: 2497 lambda_n: 0.918133410189754 Loss: 0.028766483935705103\n",
      "Iteration: 2498 lambda_n: 0.9447064035896083 Loss: 0.02875370073798401\n",
      "Iteration: 2499 lambda_n: 0.9134659493317232 Loss: 0.028740559369974657\n",
      "Iteration: 2500 lambda_n: 0.9644978146297511 Loss: 0.02872786397572607\n",
      "Iteration: 2501 lambda_n: 1.004017539229654 Loss: 0.028714471523095333\n",
      "Iteration: 2502 lambda_n: 0.9129737970676065 Loss: 0.028700543637220036\n",
      "Iteration: 2503 lambda_n: 0.9327812392917513 Loss: 0.028687890530428646\n",
      "Iteration: 2504 lambda_n: 0.9768468757975498 Loss: 0.02867497453882545\n",
      "Iteration: 2505 lambda_n: 1.0271444451299319 Loss: 0.028661461000672746\n",
      "Iteration: 2506 lambda_n: 0.9971303804864762 Loss: 0.028647265599177605\n",
      "Iteration: 2507 lambda_n: 0.9179206413362219 Loss: 0.028633498693872793\n",
      "Iteration: 2508 lambda_n: 0.9310080678872105 Loss: 0.028620837342729688\n",
      "Iteration: 2509 lambda_n: 0.8971012009027374 Loss: 0.02860800718166417\n",
      "Iteration: 2510 lambda_n: 0.9683828745935519 Loss: 0.028595655461704797\n",
      "Iteration: 2511 lambda_n: 0.9982348906547747 Loss: 0.02858233462592134\n",
      "Iteration: 2512 lambda_n: 1.0186484365244965 Loss: 0.028568616564773244\n",
      "Iteration: 2513 lambda_n: 0.9502441203499732 Loss: 0.028554632030217904\n",
      "Iteration: 2514 lambda_n: 1.008777642095274 Loss: 0.02854159940382314\n",
      "Iteration: 2515 lambda_n: 1.0457383665509647 Loss: 0.028527777548297353\n",
      "Iteration: 2516 lambda_n: 0.8943575414722147 Loss: 0.02851346403546301\n",
      "Iteration: 2517 lambda_n: 0.9674904656883868 Loss: 0.028501234482401706\n",
      "Iteration: 2518 lambda_n: 1.0356304194162635 Loss: 0.028488017314131506\n",
      "Iteration: 2519 lambda_n: 0.9414104917694013 Loss: 0.02847388358193956\n",
      "Iteration: 2520 lambda_n: 1.0395692948940691 Loss: 0.028461048574621096\n",
      "Iteration: 2521 lambda_n: 0.9390466859092506 Loss: 0.028446889542002304\n",
      "Iteration: 2522 lambda_n: 1.030077110025257 Loss: 0.02843411251328685\n",
      "Iteration: 2523 lambda_n: 1.0118187371793077 Loss: 0.028420110968165355\n",
      "Iteration: 2524 lambda_n: 0.9069596041368457 Loss: 0.028406371963959166\n",
      "Iteration: 2525 lambda_n: 0.9554163774451149 Loss: 0.02839406890453452\n",
      "Iteration: 2526 lambda_n: 0.8923955626197028 Loss: 0.028381120923482068\n",
      "Iteration: 2527 lambda_n: 0.9974984263709 Loss: 0.028369038521044006\n",
      "Iteration: 2528 lambda_n: 1.0165144068715763 Loss: 0.02835554627658701\n",
      "Iteration: 2529 lambda_n: 0.9334756642653501 Loss: 0.028341811149232947\n",
      "Iteration: 2530 lambda_n: 0.8936362399581537 Loss: 0.0283292107977808\n",
      "Iteration: 2531 lambda_n: 0.9245560670994897 Loss: 0.028317159671003643\n",
      "Iteration: 2532 lambda_n: 0.9886444552277316 Loss: 0.02830470338911833\n",
      "Iteration: 2533 lambda_n: 0.9390629320708763 Loss: 0.02829139697380699\n",
      "Iteration: 2534 lambda_n: 1.0182610361214193 Loss: 0.028278770645278582\n",
      "Iteration: 2535 lambda_n: 1.0129243665760248 Loss: 0.028265093510811395\n",
      "Iteration: 2536 lambda_n: 0.9942150080607205 Loss: 0.028251502597800585\n",
      "Iteration: 2537 lambda_n: 0.9972137551949299 Loss: 0.028238176840645012\n",
      "Iteration: 2538 lambda_n: 1.0459328323494514 Loss: 0.028224824965553928\n",
      "Iteration: 2539 lambda_n: 1.014366505053898 Loss: 0.028210835950469467\n",
      "Iteration: 2540 lambda_n: 1.043004067579776 Loss: 0.028197283980490425\n",
      "Iteration: 2541 lambda_n: 1.034656022603275 Loss: 0.028183364688075804\n",
      "Iteration: 2542 lambda_n: 0.9992522360587339 Loss: 0.028169572129585676\n",
      "Iteration: 2543 lambda_n: 1.0414881968215515 Loss: 0.02815626603504889\n",
      "Iteration: 2544 lambda_n: 0.9107225871466309 Loss: 0.028142412722331048\n",
      "Iteration: 2545 lambda_n: 1.0468234894955573 Loss: 0.02813031151358492\n",
      "Iteration: 2546 lambda_n: 1.032575289643589 Loss: 0.02811641656212963\n",
      "Iteration: 2547 lambda_n: 1.0414449075829266 Loss: 0.02810272615227079\n",
      "Iteration: 2548 lambda_n: 0.9016603440921518 Loss: 0.028088933678894618\n",
      "Iteration: 2549 lambda_n: 0.9619468425832922 Loss: 0.028077005073207102\n",
      "Iteration: 2550 lambda_n: 0.9077045337922904 Loss: 0.02806429183062069\n",
      "Iteration: 2551 lambda_n: 1.0305415612567501 Loss: 0.02805230771859519\n",
      "Iteration: 2552 lambda_n: 1.0376209785338897 Loss: 0.02803871627672674\n",
      "Iteration: 2553 lambda_n: 1.0281711670138465 Loss: 0.028025047007867846\n",
      "Iteration: 2554 lambda_n: 1.0046849036829089 Loss: 0.028011517631954055\n",
      "Iteration: 2555 lambda_n: 1.018342371330404 Loss: 0.02799831213830391\n",
      "Iteration: 2556 lambda_n: 0.9823561544248955 Loss: 0.027984942116911132\n",
      "Iteration: 2557 lambda_n: 0.9062413219510552 Loss: 0.027972058881762258\n",
      "Iteration: 2558 lambda_n: 0.9861346144808364 Loss: 0.027960186348104422\n",
      "Iteration: 2559 lambda_n: 1.007620274549911 Loss: 0.027947280775020095\n",
      "Iteration: 2560 lambda_n: 1.0441757953836177 Loss: 0.02793410871228755\n",
      "Iteration: 2561 lambda_n: 0.9101343527196631 Loss: 0.02792047447230987\n",
      "Iteration: 2562 lambda_n: 0.9572844275211202 Loss: 0.02790860351358465\n",
      "Iteration: 2563 lambda_n: 0.969786352953429 Loss: 0.02789613070310318\n",
      "Iteration: 2564 lambda_n: 0.9718388720639495 Loss: 0.027883508745728133\n",
      "Iteration: 2565 lambda_n: 0.9715709456929433 Loss: 0.02787087397179794\n",
      "Iteration: 2566 lambda_n: 0.8988955686878143 Loss: 0.027858256606205414\n",
      "Iteration: 2567 lambda_n: 0.9676497229599287 Loss: 0.027846595455126393\n",
      "Iteration: 2568 lambda_n: 1.015920366088443 Loss: 0.02783405564443345\n",
      "Iteration: 2569 lambda_n: 0.964631686655756 Loss: 0.027820905258609762\n",
      "Iteration: 2570 lambda_n: 0.982542655761675 Loss: 0.027808432960699275\n",
      "Iteration: 2571 lambda_n: 0.9506486164027712 Loss: 0.027795743301217236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2572 lambda_n: 1.036840333701803 Loss: 0.027783479229950083\n",
      "Iteration: 2573 lambda_n: 0.9529724287861691 Loss: 0.027770118570793468\n",
      "Iteration: 2574 lambda_n: 0.9722380182032251 Loss: 0.02775785276938566\n",
      "Iteration: 2575 lambda_n: 1.0011648184745177 Loss: 0.027745352975866375\n",
      "Iteration: 2576 lambda_n: 0.9665895979404449 Loss: 0.02773249605150861\n",
      "Iteration: 2577 lambda_n: 1.0039211819214287 Loss: 0.027720097381934008\n",
      "Iteration: 2578 lambda_n: 0.94869255518927 Loss: 0.027707234683893512\n",
      "Iteration: 2579 lambda_n: 1.0474233327396614 Loss: 0.02769509350803092\n",
      "Iteration: 2580 lambda_n: 1.0034925925861042 Loss: 0.027681704515470203\n",
      "Iteration: 2581 lambda_n: 0.9492527168277173 Loss: 0.027668892575455448\n",
      "Iteration: 2582 lambda_n: 1.005498173538886 Loss: 0.027656787110372526\n",
      "Iteration: 2583 lambda_n: 1.009200037443635 Loss: 0.027643979207750826\n",
      "Iteration: 2584 lambda_n: 0.9523633929661679 Loss: 0.027631139523462574\n",
      "Iteration: 2585 lambda_n: 1.0294342269228511 Loss: 0.027619037094281044\n",
      "Iteration: 2586 lambda_n: 1.0202314054071708 Loss: 0.02760597073305433\n",
      "Iteration: 2587 lambda_n: 0.9365016219349993 Loss: 0.027593037057069118\n",
      "Iteration: 2588 lambda_n: 1.0387679758046506 Loss: 0.02758117877158646\n",
      "Iteration: 2589 lambda_n: 0.9835175194497231 Loss: 0.027568041178179663\n",
      "Iteration: 2590 lambda_n: 0.8945925153747266 Loss: 0.027555617515969837\n",
      "Iteration: 2591 lambda_n: 0.9263927109784272 Loss: 0.027544329968531664\n",
      "Iteration: 2592 lambda_n: 1.020358962528761 Loss: 0.027532654076284902\n",
      "Iteration: 2593 lambda_n: 0.9018235290874197 Loss: 0.027519809074318616\n",
      "Iteration: 2594 lambda_n: 1.0293785488500582 Loss: 0.02750846956557621\n",
      "Iteration: 2595 lambda_n: 0.9017169088450989 Loss: 0.027495541440051636\n",
      "Iteration: 2596 lambda_n: 1.0205329597314627 Loss: 0.027484230017852107\n",
      "Iteration: 2597 lambda_n: 0.9972793252167949 Loss: 0.027471443225720097\n",
      "Iteration: 2598 lambda_n: 0.9957278362760305 Loss: 0.027458963296498565\n",
      "Iteration: 2599 lambda_n: 0.8945464473685942 Loss: 0.02744651809541912\n",
      "Iteration: 2600 lambda_n: 0.9168216783231337 Loss: 0.02743535058450948\n",
      "Iteration: 2601 lambda_n: 0.9944024717325408 Loss: 0.027423917837926946\n",
      "Iteration: 2602 lambda_n: 0.9639831060886054 Loss: 0.027411532383269856\n",
      "Iteration: 2603 lambda_n: 1.0197411252547381 Loss: 0.02739954045058427\n",
      "Iteration: 2604 lambda_n: 1.0144196872724554 Loss: 0.027386870601877833\n",
      "Iteration: 2605 lambda_n: 1.0090559968289878 Loss: 0.02737428291977946\n",
      "Iteration: 2606 lambda_n: 1.0012424441393 Loss: 0.027361777697854042\n",
      "Iteration: 2607 lambda_n: 1.0317915803599653 Loss: 0.02734938500851626\n",
      "Iteration: 2608 lambda_n: 0.9006797886353732 Loss: 0.027336630587236975\n",
      "Iteration: 2609 lambda_n: 0.9074808313443193 Loss: 0.027325510507982004\n",
      "Iteration: 2610 lambda_n: 0.9991363396977644 Loss: 0.027314319311050456\n",
      "Iteration: 2611 lambda_n: 0.9835259494034986 Loss: 0.027302012741760234\n",
      "Iteration: 2612 lambda_n: 0.9889859847316742 Loss: 0.02728991375962777\n",
      "Iteration: 2613 lambda_n: 1.0223924587822748 Loss: 0.027277762947880375\n",
      "Iteration: 2614 lambda_n: 1.0383658967678353 Loss: 0.027265217890272638\n",
      "Iteration: 2615 lambda_n: 0.9550448718250734 Loss: 0.02725249370431998\n",
      "Iteration: 2616 lambda_n: 0.9923812838376556 Loss: 0.027240805568156393\n",
      "Iteration: 2617 lambda_n: 0.9370812294341319 Loss: 0.02722867577572654\n",
      "Iteration: 2618 lambda_n: 1.016223462274521 Loss: 0.02721723622352287\n",
      "Iteration: 2619 lambda_n: 0.9970831140585097 Loss: 0.027204846265658965\n",
      "Iteration: 2620 lambda_n: 0.8949139724722154 Loss: 0.027192705603184597\n",
      "Iteration: 2621 lambda_n: 0.9912060302609117 Loss: 0.027181822426848\n",
      "Iteration: 2622 lambda_n: 0.9312609376541183 Loss: 0.02716978310712337\n",
      "Iteration: 2623 lambda_n: 0.9246830778367466 Loss: 0.027158486153750012\n",
      "Iteration: 2624 lambda_n: 1.0061294488666943 Loss: 0.027147282687359924\n",
      "Iteration: 2625 lambda_n: 0.9390625138912615 Loss: 0.027135107935721\n",
      "Iteration: 2626 lambda_n: 1.0249329940976288 Loss: 0.0271237593443567\n",
      "Iteration: 2627 lambda_n: 0.9607568866617292 Loss: 0.02711138913002803\n",
      "Iteration: 2628 lambda_n: 1.0125532221101363 Loss: 0.02709980877653605\n",
      "Iteration: 2629 lambda_n: 1.0166314552859848 Loss: 0.027087620150017196\n",
      "Iteration: 2630 lambda_n: 0.9965079771756883 Loss: 0.0270753990217755\n",
      "Iteration: 2631 lambda_n: 0.9113911347806378 Loss: 0.027063435956824176\n",
      "Iteration: 2632 lambda_n: 0.957662132020307 Loss: 0.027052508739329087\n",
      "Iteration: 2633 lambda_n: 1.0258601462397292 Loss: 0.027041041202693298\n",
      "Iteration: 2634 lambda_n: 1.0390744929979518 Loss: 0.027028773478499912\n",
      "Iteration: 2635 lambda_n: 1.0287151635960037 Loss: 0.027016365100320325\n",
      "Iteration: 2636 lambda_n: 0.9142874722818717 Loss: 0.02700409767460309\n",
      "Iteration: 2637 lambda_n: 0.9756271324140517 Loss: 0.026993209218582424\n",
      "Iteration: 2638 lambda_n: 0.9656419826346408 Loss: 0.026981605241507375\n",
      "Iteration: 2639 lambda_n: 0.8997248602788878 Loss: 0.026970135282146458\n",
      "Iteration: 2640 lambda_n: 0.9196074319861963 Loss: 0.026959461966406088\n",
      "Iteration: 2641 lambda_n: 0.9343367250493366 Loss: 0.02694856643646728\n",
      "Iteration: 2642 lambda_n: 1.0056753458054413 Loss: 0.026937510543401585\n",
      "Iteration: 2643 lambda_n: 1.0169861303751915 Loss: 0.02692562646683399\n",
      "Iteration: 2644 lambda_n: 0.9811113008945248 Loss: 0.026913625577210123\n",
      "Iteration: 2645 lambda_n: 1.0373847948469055 Loss: 0.026902064102790585\n",
      "Iteration: 2646 lambda_n: 0.9577944562873323 Loss: 0.026889856693393523\n",
      "Iteration: 2647 lambda_n: 0.9412918543312371 Loss: 0.026878601574969498\n",
      "Iteration: 2648 lambda_n: 0.9660861631469464 Loss: 0.02686755509763907\n",
      "Iteration: 2649 lambda_n: 0.9870643845065332 Loss: 0.026856232838779245\n",
      "Iteration: 2650 lambda_n: 0.9744722343964307 Loss: 0.02684468063435243\n",
      "Iteration: 2651 lambda_n: 0.9629017257208529 Loss: 0.026833291688890905\n",
      "Iteration: 2652 lambda_n: 1.041480425666018 Loss: 0.02682205337033517\n",
      "Iteration: 2653 lambda_n: 1.0397569327647833 Loss: 0.026809915263926717\n",
      "Iteration: 2654 lambda_n: 0.9798000586343113 Loss: 0.026797815159124435\n",
      "Iteration: 2655 lambda_n: 0.9525305248339613 Loss: 0.02678642921857169\n",
      "Iteration: 2656 lambda_n: 1.0091863683796056 Loss: 0.02677537547129103\n",
      "Iteration: 2657 lambda_n: 0.9009338248538096 Loss: 0.026763680593715464\n",
      "Iteration: 2658 lambda_n: 0.9197593250849193 Loss: 0.026753254605737378\n",
      "Iteration: 2659 lambda_n: 0.9873625314405089 Loss: 0.02674262477544419\n",
      "Iteration: 2660 lambda_n: 0.9759239093692789 Loss: 0.026731229405902734\n",
      "Iteration: 2661 lambda_n: 0.9357592170467705 Loss: 0.02671998210407272\n",
      "Iteration: 2662 lambda_n: 0.9917560326503629 Loss: 0.02670921269175539\n",
      "Iteration: 2663 lambda_n: 0.9448192214167557 Loss: 0.02669781487122573\n",
      "Iteration: 2664 lambda_n: 0.9817396456106253 Loss: 0.026686809620701177\n",
      "Iteration: 2665 lambda_n: 1.0486863809024325 Loss: 0.02667326486591602\n",
      "Iteration: 2666 lambda_n: 0.9447090967340058 Loss: 0.02665774864907816\n",
      "Iteration: 2667 lambda_n: 1.0174367128766162 Loss: 0.02664339032604595\n",
      "Iteration: 2668 lambda_n: 1.0437974771233387 Loss: 0.026627864989808018\n",
      "Iteration: 2669 lambda_n: 1.0102349870456233 Loss: 0.026611999316308383\n",
      "Iteration: 2670 lambda_n: 0.9946817786711122 Loss: 0.026596772461382947\n",
      "Iteration: 2671 lambda_n: 0.9911211479503428 Loss: 0.026581933214857634\n",
      "Iteration: 2672 lambda_n: 1.0368680130778578 Loss: 0.02656730952810725\n",
      "Iteration: 2673 lambda_n: 0.9656850545893272 Loss: 0.02655218614786638\n",
      "Iteration: 2674 lambda_n: 0.9471777019816753 Loss: 0.026538269670006272\n",
      "Iteration: 2675 lambda_n: 0.9953059684988026 Loss: 0.026524773690576424\n",
      "Iteration: 2676 lambda_n: 1.0263436362003246 Loss: 0.026510749569399434\n",
      "Iteration: 2677 lambda_n: 0.938702614015365 Loss: 0.02649645477820537\n",
      "Iteration: 2678 lambda_n: 1.032287759370256 Loss: 0.02648353052239717\n",
      "Iteration: 2679 lambda_n: 1.0157974145599116 Loss: 0.02646946833634385\n",
      "Iteration: 2680 lambda_n: 1.0109015632911473 Loss: 0.026455785018001606\n",
      "Iteration: 2681 lambda_n: 1.0400168091141295 Loss: 0.026442312761883233\n",
      "Iteration: 2682 lambda_n: 0.958910712667151 Loss: 0.026428595378748556\n",
      "Iteration: 2683 lambda_n: 0.9165809480829427 Loss: 0.026416073998845692\n",
      "Iteration: 2684 lambda_n: 0.9491353932019608 Loss: 0.02640421146084329\n",
      "Iteration: 2685 lambda_n: 1.0403909069278787 Loss: 0.026392028803151748\n",
      "Iteration: 2686 lambda_n: 0.9854021136727051 Loss: 0.02637878493847183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2687 lambda_n: 0.994845556371095 Loss: 0.026366345096748522\n",
      "Iteration: 2688 lambda_n: 0.9950570636196009 Loss: 0.026353880178608045\n",
      "Iteration: 2689 lambda_n: 0.8992394370891255 Loss: 0.026341501058267425\n",
      "Iteration: 2690 lambda_n: 0.9964062149847001 Loss: 0.026330386584547116\n",
      "Iteration: 2691 lambda_n: 1.01490837694694 Loss: 0.026318141941785007\n",
      "Iteration: 2692 lambda_n: 0.9292147981020505 Loss: 0.026305742559996553\n",
      "Iteration: 2693 lambda_n: 0.9305681871281015 Loss: 0.026294450936736963\n",
      "Iteration: 2694 lambda_n: 0.9217884281467914 Loss: 0.02628319523571699\n",
      "Iteration: 2695 lambda_n: 0.9111177521833509 Loss: 0.026272093504725612\n",
      "Iteration: 2696 lambda_n: 0.9343244580149928 Loss: 0.026261163328875153\n",
      "Iteration: 2697 lambda_n: 0.9015309133088794 Loss: 0.026249995253934403\n",
      "Iteration: 2698 lambda_n: 1.0449746291208801 Loss: 0.02623925542100492\n",
      "Iteration: 2699 lambda_n: 1.0032332348285595 Loss: 0.026226845749815847\n",
      "Iteration: 2700 lambda_n: 0.8996200634319048 Loss: 0.026214969483374494\n",
      "Iteration: 2701 lambda_n: 1.0145145846552883 Loss: 0.02620434873288918\n",
      "Iteration: 2702 lambda_n: 1.0468547326641235 Loss: 0.026192399804041033\n",
      "Iteration: 2703 lambda_n: 0.985133232175647 Loss: 0.026180099224025446\n",
      "Iteration: 2704 lambda_n: 0.9904333845399411 Loss: 0.026168548973815008\n",
      "Iteration: 2705 lambda_n: 0.9704318511429784 Loss: 0.026156958412284895\n",
      "Iteration: 2706 lambda_n: 0.9116656537511723 Loss: 0.026145621307466616\n",
      "Iteration: 2707 lambda_n: 0.8939917408941696 Loss: 0.026134986785140694\n",
      "Iteration: 2708 lambda_n: 1.0236085302574187 Loss: 0.026124572043999244\n",
      "Iteration: 2709 lambda_n: 0.9584725026740707 Loss: 0.026112661868570534\n",
      "Iteration: 2710 lambda_n: 1.0483218020804708 Loss: 0.026101523102349627\n",
      "Iteration: 2711 lambda_n: 0.9214922981086588 Loss: 0.026089353231269343\n",
      "Iteration: 2712 lambda_n: 0.9956846873790892 Loss: 0.026078666543746507\n",
      "Iteration: 2713 lambda_n: 0.9480301937720045 Loss: 0.026067129270666387\n",
      "Iteration: 2714 lambda_n: 0.9232687621470689 Loss: 0.0260561531511939\n",
      "Iteration: 2715 lambda_n: 0.9933174053205878 Loss: 0.026045471353841623\n",
      "Iteration: 2716 lambda_n: 0.9701894363104919 Loss: 0.02603398664888774\n",
      "Iteration: 2717 lambda_n: 1.0430359776224136 Loss: 0.026022776434872275\n",
      "Iteration: 2718 lambda_n: 0.8985557893490067 Loss: 0.02601073147766693\n",
      "Iteration: 2719 lambda_n: 0.9708663715498175 Loss: 0.02600036061336138\n",
      "Iteration: 2720 lambda_n: 1.0381174925047658 Loss: 0.025989160238974154\n",
      "Iteration: 2721 lambda_n: 1.011522362338466 Loss: 0.025977189435601945\n",
      "Iteration: 2722 lambda_n: 0.9759484451809968 Loss: 0.025965530399888196\n",
      "Iteration: 2723 lambda_n: 0.9326295971874656 Loss: 0.025954285805654755\n",
      "Iteration: 2724 lambda_n: 0.9791864262154437 Loss: 0.025943544080843525\n",
      "Iteration: 2725 lambda_n: 0.9082073571540826 Loss: 0.025932269715330393\n",
      "Iteration: 2726 lambda_n: 1.049083696828001 Loss: 0.02592181578890365\n",
      "Iteration: 2727 lambda_n: 0.899320126722007 Loss: 0.02590974364280952\n",
      "Iteration: 2728 lambda_n: 0.990804855279436 Loss: 0.025899397808153823\n",
      "Iteration: 2729 lambda_n: 1.036834491635643 Loss: 0.025888002250246525\n",
      "Iteration: 2730 lambda_n: 0.9737511744689051 Loss: 0.025876080226300967\n",
      "Iteration: 2731 lambda_n: 0.9896775150894653 Loss: 0.025864886238721095\n",
      "Iteration: 2732 lambda_n: 0.9378095691944206 Loss: 0.025853511613157694\n",
      "Iteration: 2733 lambda_n: 0.9577806585614617 Loss: 0.025842735337567722\n",
      "Iteration: 2734 lambda_n: 1.032633257633955 Loss: 0.02583173164715255\n",
      "Iteration: 2735 lambda_n: 1.0153251379789638 Loss: 0.02581987019462453\n",
      "Iteration: 2736 lambda_n: 1.0190442765836167 Loss: 0.025808209759807053\n",
      "Iteration: 2737 lambda_n: 0.896067563333973 Loss: 0.02579650870786893\n",
      "Iteration: 2738 lambda_n: 0.9409757296273642 Loss: 0.0257862214766645\n",
      "Iteration: 2739 lambda_n: 0.9981943981340575 Loss: 0.02577542027969002\n",
      "Iteration: 2740 lambda_n: 0.9766262219923518 Loss: 0.025763964015704067\n",
      "Iteration: 2741 lambda_n: 0.8927646240568924 Loss: 0.02575275701929566\n",
      "Iteration: 2742 lambda_n: 0.9650294432680852 Loss: 0.025742513850176348\n",
      "Iteration: 2743 lambda_n: 0.9271008717987214 Loss: 0.02573144301339603\n",
      "Iteration: 2744 lambda_n: 1.0049822540343434 Loss: 0.025720808768176832\n",
      "Iteration: 2745 lambda_n: 0.9589952993523847 Loss: 0.025709282707577653\n",
      "Iteration: 2746 lambda_n: 1.0254001464141977 Loss: 0.025698285593045433\n",
      "Iteration: 2747 lambda_n: 1.0297654748641754 Loss: 0.025686528531954675\n",
      "Iteration: 2748 lambda_n: 0.9341697830619952 Loss: 0.025674723037681473\n",
      "Iteration: 2749 lambda_n: 0.9300785945253219 Loss: 0.025664014921042904\n",
      "Iteration: 2750 lambda_n: 0.9209119417723711 Loss: 0.025653354993167007\n",
      "Iteration: 2751 lambda_n: 0.9299579539437983 Loss: 0.025642801385890108\n",
      "Iteration: 2752 lambda_n: 0.9858855318528638 Loss: 0.025632145357422212\n",
      "Iteration: 2753 lambda_n: 0.9724351505860143 Loss: 0.025620849798920806\n",
      "Iteration: 2754 lambda_n: 1.0374262850563885 Loss: 0.02560970971054682\n",
      "Iteration: 2755 lambda_n: 1.0036207140307587 Loss: 0.025597826519820868\n",
      "Iteration: 2756 lambda_n: 0.9117317760128866 Loss: 0.02558633201051779\n",
      "Iteration: 2757 lambda_n: 0.9299073816665784 Loss: 0.0255758911761917\n",
      "Iteration: 2758 lambda_n: 0.928652946543731 Loss: 0.0255652433706693\n",
      "Iteration: 2759 lambda_n: 0.9718937061084143 Loss: 0.02555461111196062\n",
      "Iteration: 2760 lambda_n: 1.0367026752080466 Loss: 0.025543485014974776\n",
      "Iteration: 2761 lambda_n: 0.9783759033914471 Loss: 0.02553161836043079\n",
      "Iteration: 2762 lambda_n: 0.910002973166696 Loss: 0.025520420709528157\n",
      "Iteration: 2763 lambda_n: 0.9356847777398798 Loss: 0.025510006787429006\n",
      "Iteration: 2764 lambda_n: 1.0228127079869767 Loss: 0.02549930010243934\n",
      "Iteration: 2765 lambda_n: 0.9124140560262506 Loss: 0.02548759771728984\n",
      "Iteration: 2766 lambda_n: 1.047257703645226 Loss: 0.025477159675774165\n",
      "Iteration: 2767 lambda_n: 0.9958933193263865 Loss: 0.025465180278400935\n",
      "Iteration: 2768 lambda_n: 0.9298316150434383 Loss: 0.025453789794759003\n",
      "Iteration: 2769 lambda_n: 0.9656282944880433 Loss: 0.02544315609602253\n",
      "Iteration: 2770 lambda_n: 0.9707451601986778 Loss: 0.025432114188122645\n",
      "Iteration: 2771 lambda_n: 0.9206371231111529 Loss: 0.025421014982883513\n",
      "Iteration: 2772 lambda_n: 1.043223742908689 Loss: 0.025410489850156683\n",
      "Iteration: 2773 lambda_n: 0.9368933774453909 Loss: 0.02539856448842508\n",
      "Iteration: 2774 lambda_n: 0.9624609673809191 Loss: 0.02538785586781435\n",
      "Iteration: 2775 lambda_n: 1.0194575964259092 Loss: 0.02537685616248987\n",
      "Iteration: 2776 lambda_n: 1.0063614640724277 Loss: 0.025365206306391608\n",
      "Iteration: 2777 lambda_n: 0.9344684097291402 Loss: 0.025353707406470718\n",
      "Iteration: 2778 lambda_n: 1.0459520973286385 Loss: 0.025343031158885202\n",
      "Iteration: 2779 lambda_n: 0.9303545071953283 Loss: 0.025331082446933283\n",
      "Iteration: 2780 lambda_n: 0.923517551628496 Loss: 0.025320455514350197\n",
      "Iteration: 2781 lambda_n: 0.9832968164522571 Loss: 0.025309907749814963\n",
      "Iteration: 2782 lambda_n: 1.0038788381446142 Loss: 0.025298678359505566\n",
      "Iteration: 2783 lambda_n: 0.9677135713131748 Loss: 0.02528721514429745\n",
      "Iteration: 2784 lambda_n: 1.0097010492568756 Loss: 0.02527616609824127\n",
      "Iteration: 2785 lambda_n: 0.9110180999000043 Loss: 0.025264638855986458\n",
      "Iteration: 2786 lambda_n: 0.979624426908119 Loss: 0.025254239355143343\n",
      "Iteration: 2787 lambda_n: 0.9803946045744382 Loss: 0.02524305778695674\n",
      "Iteration: 2788 lambda_n: 0.9386284676210371 Loss: 0.025231868597796664\n",
      "Iteration: 2789 lambda_n: 1.008897107591941 Loss: 0.025221157199907408\n",
      "Iteration: 2790 lambda_n: 1.0057204414449703 Loss: 0.025209645058528938\n",
      "Iteration: 2791 lambda_n: 0.9308765230620158 Loss: 0.025198170387024484\n",
      "Iteration: 2792 lambda_n: 0.9933102197584263 Loss: 0.02518755076309486\n",
      "Iteration: 2793 lambda_n: 0.906654694867644 Loss: 0.025176219988104532\n",
      "Iteration: 2794 lambda_n: 0.9214914865673801 Loss: 0.025165878772047855\n",
      "Iteration: 2795 lambda_n: 0.9778826140559848 Loss: 0.025155369319499174\n",
      "Iteration: 2796 lambda_n: 1.0305015019366037 Loss: 0.02514421780019639\n",
      "Iteration: 2797 lambda_n: 0.9874093097323275 Loss: 0.02513246741449489\n",
      "Iteration: 2798 lambda_n: 1.0133895633678225 Loss: 0.025121209583645516\n",
      "Iteration: 2799 lambda_n: 0.9569797299696229 Loss: 0.025109656709434915\n",
      "Iteration: 2800 lambda_n: 0.9918266505026571 Loss: 0.025098748046931362\n",
      "Iteration: 2801 lambda_n: 0.9485113779959718 Loss: 0.025087443260461802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2802 lambda_n: 1.013198826807234 Loss: 0.025076633262753504\n",
      "Iteration: 2803 lambda_n: 0.9080355035645012 Loss: 0.025065087137159513\n",
      "Iteration: 2804 lambda_n: 1.0477598476178283 Loss: 0.0250547404735827\n",
      "Iteration: 2805 lambda_n: 0.9504352478475373 Loss: 0.025042802794601346\n",
      "Iteration: 2806 lambda_n: 0.9057048524731479 Loss: 0.025031975113538156\n",
      "Iteration: 2807 lambda_n: 0.9179667641015841 Loss: 0.02502165798644507\n",
      "Iteration: 2808 lambda_n: 0.9475119982297534 Loss: 0.025011202113582518\n",
      "Iteration: 2809 lambda_n: 0.9304292891149883 Loss: 0.025000410685041566\n",
      "Iteration: 2810 lambda_n: 0.9612327455233876 Loss: 0.024989814796346498\n",
      "Iteration: 2811 lambda_n: 0.9258062294946922 Loss: 0.02497886910346053\n",
      "Iteration: 2812 lambda_n: 0.9769509260269489 Loss: 0.024968327798964993\n",
      "Iteration: 2813 lambda_n: 0.9924508914248625 Loss: 0.024957205150067895\n",
      "Iteration: 2814 lambda_n: 0.9216368245360876 Loss: 0.024945907093163366\n",
      "Iteration: 2815 lambda_n: 0.9648894182711867 Loss: 0.02493541617873895\n",
      "Iteration: 2816 lambda_n: 0.9950997895244991 Loss: 0.02492443388692446\n",
      "Iteration: 2817 lambda_n: 0.9385487711802152 Loss: 0.024913108778593172\n",
      "Iteration: 2818 lambda_n: 1.0171308967942432 Loss: 0.02490242827293746\n",
      "Iteration: 2819 lambda_n: 0.9472931142266072 Loss: 0.02489085453749913\n",
      "Iteration: 2820 lambda_n: 1.0151802240564647 Loss: 0.024880076497244517\n",
      "Iteration: 2821 lambda_n: 0.9587144678110161 Loss: 0.024868527073722096\n",
      "Iteration: 2822 lambda_n: 1.0015322765796026 Loss: 0.02485762107049002\n",
      "Iteration: 2823 lambda_n: 1.0415708001442123 Loss: 0.024846228992971713\n",
      "Iteration: 2824 lambda_n: 0.9624611466189509 Loss: 0.024834382579377545\n",
      "Iteration: 2825 lambda_n: 0.8966631346657213 Loss: 0.02482343696805343\n",
      "Iteration: 2826 lambda_n: 1.0065625730070271 Loss: 0.02481324053715954\n",
      "Iteration: 2827 lambda_n: 1.0030470287403983 Loss: 0.02480179530902358\n",
      "Iteration: 2828 lambda_n: 0.9109499778149949 Loss: 0.024790391087548103\n",
      "Iteration: 2829 lambda_n: 0.9027950290828826 Loss: 0.024780034900641314\n",
      "Iteration: 2830 lambda_n: 0.9931565632110253 Loss: 0.024769772256531954\n",
      "Iteration: 2831 lambda_n: 0.9907784692715692 Loss: 0.02475848331929447\n",
      "Iteration: 2832 lambda_n: 0.9491377494720139 Loss: 0.02474722240052417\n",
      "Iteration: 2833 lambda_n: 0.9510221779158654 Loss: 0.024736435697670756\n",
      "Iteration: 2834 lambda_n: 0.931264063392209 Loss: 0.024725628475386095\n",
      "Iteration: 2835 lambda_n: 1.01739437828759 Loss: 0.024715046655754847\n",
      "Iteration: 2836 lambda_n: 1.026035561387041 Loss: 0.024703487081759533\n",
      "Iteration: 2837 lambda_n: 0.9462400130671297 Loss: 0.02469183034912504\n",
      "Iteration: 2838 lambda_n: 0.9215729207180117 Loss: 0.024681081114779412\n",
      "Iteration: 2839 lambda_n: 1.0432872979819101 Loss: 0.024670612942023468\n",
      "Iteration: 2840 lambda_n: 0.9660771332321084 Loss: 0.024658763139113922\n",
      "Iteration: 2841 lambda_n: 0.9793444321836543 Loss: 0.024647791267059953\n",
      "Iteration: 2842 lambda_n: 0.914527111602857 Loss: 0.02463666961930165\n",
      "Iteration: 2843 lambda_n: 0.9567705533683601 Loss: 0.024626284901485898\n",
      "Iteration: 2844 lambda_n: 0.9020941707980477 Loss: 0.024615421323834992\n",
      "Iteration: 2845 lambda_n: 0.902512735445613 Loss: 0.024605179376416875\n",
      "Iteration: 2846 lambda_n: 0.9977504030389246 Loss: 0.02459493343843748\n",
      "Iteration: 2847 lambda_n: 0.9074969141812952 Loss: 0.02458360713614653\n",
      "Iteration: 2848 lambda_n: 0.9224246224908732 Loss: 0.024573306216035706\n",
      "Iteration: 2849 lambda_n: 0.9087788349029392 Loss: 0.02456283662446066\n",
      "Iteration: 2850 lambda_n: 0.9113963190897524 Loss: 0.024552522682738186\n",
      "Iteration: 2851 lambda_n: 0.9515238323995474 Loss: 0.024542179790806543\n",
      "Iteration: 2852 lambda_n: 1.0284238373070889 Loss: 0.02453138230373488\n",
      "Iteration: 2853 lambda_n: 1.0437509534950373 Loss: 0.024519713073112404\n",
      "Iteration: 2854 lambda_n: 0.8969177701084079 Loss: 0.024507870895949364\n",
      "Iteration: 2855 lambda_n: 1.0440003956514536 Loss: 0.02449769549475819\n",
      "Iteration: 2856 lambda_n: 0.9206339266508058 Loss: 0.024485852296022858\n",
      "Iteration: 2857 lambda_n: 0.9833568017460041 Loss: 0.024475409424752237\n",
      "Iteration: 2858 lambda_n: 0.9794089832753775 Loss: 0.024464255877171392\n",
      "Iteration: 2859 lambda_n: 1.0393614318210926 Loss: 0.02445314795127269\n",
      "Iteration: 2860 lambda_n: 0.894500456694663 Loss: 0.024441360964892418\n",
      "Iteration: 2861 lambda_n: 0.9295147913948932 Loss: 0.024431217595993343\n",
      "Iteration: 2862 lambda_n: 0.9591556377576843 Loss: 0.024420677892278794\n",
      "Iteration: 2863 lambda_n: 0.9187040205685579 Loss: 0.024409802858576943\n",
      "Iteration: 2864 lambda_n: 0.9651619700272721 Loss: 0.02439938722354317\n",
      "Iteration: 2865 lambda_n: 1.0170789330891803 Loss: 0.02438844563392022\n",
      "Iteration: 2866 lambda_n: 0.954157709679764 Loss: 0.024376916316336657\n",
      "Iteration: 2867 lambda_n: 0.905797640225186 Loss: 0.024366101072520876\n",
      "Iteration: 2868 lambda_n: 0.9678835511331859 Loss: 0.024355834706686\n",
      "Iteration: 2869 lambda_n: 0.9810303315314017 Loss: 0.024344865385391802\n",
      "Iteration: 2870 lambda_n: 0.9414849429942053 Loss: 0.024333747854467645\n",
      "Iteration: 2871 lambda_n: 1.001934467196414 Loss: 0.024323079233299974\n",
      "Iteration: 2872 lambda_n: 0.9881985288702521 Loss: 0.02431172639010444\n",
      "Iteration: 2873 lambda_n: 0.9049871370317792 Loss: 0.02430052999568439\n",
      "Iteration: 2874 lambda_n: 1.014500885181855 Loss: 0.024290277121005656\n",
      "Iteration: 2875 lambda_n: 0.9965866150247795 Loss: 0.024278784272952027\n",
      "Iteration: 2876 lambda_n: 0.9941430389103807 Loss: 0.024267495180302658\n",
      "Iteration: 2877 lambda_n: 1.036161894297007 Loss: 0.024256234559546393\n",
      "Iteration: 2878 lambda_n: 0.9097977859364721 Loss: 0.024244498811495388\n",
      "Iteration: 2879 lambda_n: 1.0204011273401103 Loss: 0.024234195030343537\n",
      "Iteration: 2880 lambda_n: 0.9589852597212499 Loss: 0.024222639357135544\n",
      "Iteration: 2881 lambda_n: 0.9164505192321992 Loss: 0.024211779961926746\n",
      "Iteration: 2882 lambda_n: 0.9639909622760618 Loss: 0.024201402907085688\n",
      "Iteration: 2883 lambda_n: 0.9910544829981324 Loss: 0.024190488231191146\n",
      "Iteration: 2884 lambda_n: 1.0082069621535257 Loss: 0.024179267867575082\n",
      "Iteration: 2885 lambda_n: 0.9401881502040734 Loss: 0.024167854075360953\n",
      "Iteration: 2886 lambda_n: 1.0310037322870245 Loss: 0.024157211038562568\n",
      "Iteration: 2887 lambda_n: 0.9496461814194564 Loss: 0.024145540693841148\n",
      "Iteration: 2888 lambda_n: 0.9933659277066502 Loss: 0.024134792006303137\n",
      "Iteration: 2889 lambda_n: 1.025379665265411 Loss: 0.024123549178957097\n",
      "Iteration: 2890 lambda_n: 0.960569283713437 Loss: 0.024111944783035032\n",
      "Iteration: 2891 lambda_n: 1.0346733869666676 Loss: 0.024101074588256607\n",
      "Iteration: 2892 lambda_n: 1.0363423831893757 Loss: 0.024089366534745024\n",
      "Iteration: 2893 lambda_n: 1.0119701165275223 Loss: 0.024077640382750714\n",
      "Iteration: 2894 lambda_n: 1.0356948084427657 Loss: 0.024066190767306372\n",
      "Iteration: 2895 lambda_n: 0.9671609446058613 Loss: 0.024054473487412388\n",
      "Iteration: 2896 lambda_n: 0.9629564518837557 Loss: 0.02404353228526066\n",
      "Iteration: 2897 lambda_n: 1.0130737930796903 Loss: 0.02403263931612141\n",
      "Iteration: 2898 lambda_n: 1.024906515903501 Loss: 0.02402118011626608\n",
      "Iteration: 2899 lambda_n: 1.0283824227996818 Loss: 0.024009587810655573\n",
      "Iteration: 2900 lambda_n: 0.9669875484041842 Loss: 0.023997956935457195\n",
      "Iteration: 2901 lambda_n: 0.9943828884870893 Loss: 0.023987021127541495\n",
      "Iteration: 2902 lambda_n: 0.9038047388742375 Loss: 0.02397577617375414\n",
      "Iteration: 2903 lambda_n: 0.9672928416473756 Loss: 0.02396555614564888\n",
      "Iteration: 2904 lambda_n: 0.9619008496478537 Loss: 0.023954618812516457\n",
      "Iteration: 2905 lambda_n: 0.9630974896382387 Loss: 0.023943743087901412\n",
      "Iteration: 2906 lambda_n: 0.8954789788315493 Loss: 0.02393285446779277\n",
      "Iteration: 2907 lambda_n: 0.9014505063650683 Loss: 0.023922730918769012\n",
      "Iteration: 2908 lambda_n: 1.0325485495627058 Loss: 0.023912540407549854\n",
      "Iteration: 2909 lambda_n: 0.9660219337670711 Loss: 0.023900868516862264\n",
      "Iteration: 2910 lambda_n: 1.0308188274621042 Loss: 0.02388994930993599\n",
      "Iteration: 2911 lambda_n: 1.0287174766732647 Loss: 0.023878298350720775\n",
      "Iteration: 2912 lambda_n: 0.9487362590543046 Loss: 0.023866671846232427\n",
      "Iteration: 2913 lambda_n: 0.9509507798709639 Loss: 0.023855949929053953\n",
      "Iteration: 2914 lambda_n: 0.893117229820393 Loss: 0.023845203577269727\n",
      "Iteration: 2915 lambda_n: 0.9596933335989354 Loss: 0.023835111336273116\n",
      "Iteration: 2916 lambda_n: 0.904616709352227 Loss: 0.023824267341011864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2917 lambda_n: 0.955207130355098 Loss: 0.023814046242021603\n",
      "Iteration: 2918 lambda_n: 1.0473094315926328 Loss: 0.023803254086957742\n",
      "Iteration: 2919 lambda_n: 0.9684499907010694 Loss: 0.023791421978798893\n",
      "Iteration: 2920 lambda_n: 0.9790873833051548 Loss: 0.023780481440870097\n",
      "Iteration: 2921 lambda_n: 0.9676683976324938 Loss: 0.023769421333202256\n",
      "Iteration: 2922 lambda_n: 0.9860933464647093 Loss: 0.023758490815127487\n",
      "Iteration: 2923 lambda_n: 0.9388447131833708 Loss: 0.02374735277175723\n",
      "Iteration: 2924 lambda_n: 1.0292909266040269 Loss: 0.023736748984458145\n",
      "Iteration: 2925 lambda_n: 0.9100456332481689 Loss: 0.02372512425097964\n",
      "Iteration: 2926 lambda_n: 1.0168453783305806 Loss: 0.023714846842581003\n",
      "Iteration: 2927 lambda_n: 0.9659346945395242 Loss: 0.023703363881430316\n",
      "Iteration: 2928 lambda_n: 0.9205447352927906 Loss: 0.023692456440467254\n",
      "Iteration: 2929 lambda_n: 0.9908212115062192 Loss: 0.023682062087701093\n",
      "Iteration: 2930 lambda_n: 1.031971498147802 Loss: 0.02367087475722754\n",
      "Iteration: 2931 lambda_n: 0.9376199247085781 Loss: 0.02365922341458297\n",
      "Iteration: 2932 lambda_n: 1.0402656141379878 Loss: 0.023648637914641443\n",
      "Iteration: 2933 lambda_n: 1.0218900205056023 Loss: 0.02363689414962488\n",
      "Iteration: 2934 lambda_n: 1.0108556898469114 Loss: 0.023625358458919086\n",
      "Iteration: 2935 lambda_n: 0.9730229928317979 Loss: 0.02361394793755622\n",
      "Iteration: 2936 lambda_n: 0.9133143168775403 Loss: 0.023602965046157838\n",
      "Iteration: 2937 lambda_n: 0.8987499823496523 Loss: 0.023592656626761962\n",
      "Iteration: 2938 lambda_n: 1.0484321971594337 Loss: 0.02358251306731482\n",
      "Iteration: 2939 lambda_n: 1.0173044761987409 Loss: 0.023570680692303447\n",
      "Iteration: 2940 lambda_n: 0.9882765673550171 Loss: 0.023559200228869082\n",
      "Iteration: 2941 lambda_n: 1.0241581638049204 Loss: 0.0235480479232412\n",
      "Iteration: 2942 lambda_n: 0.9205977554299757 Loss: 0.023536491281492\n",
      "Iteration: 2943 lambda_n: 1.0415319964415515 Loss: 0.023526103750760335\n",
      "Iteration: 2944 lambda_n: 0.992036898528722 Loss: 0.023514352200233426\n",
      "Iteration: 2945 lambda_n: 0.9793002576363312 Loss: 0.02350315967633305\n",
      "Iteration: 2946 lambda_n: 0.8985527036886771 Loss: 0.023492111390464393\n",
      "Iteration: 2947 lambda_n: 1.0269159371140921 Loss: 0.02348197456889861\n",
      "Iteration: 2948 lambda_n: 0.9329193052911114 Loss: 0.023470390152304014\n",
      "Iteration: 2949 lambda_n: 0.98701182667427 Loss: 0.023459866614297237\n",
      "Iteration: 2950 lambda_n: 0.938097391368221 Loss: 0.02344873340037781\n",
      "Iteration: 2951 lambda_n: 0.8925999128066576 Loss: 0.023438152427479424\n",
      "Iteration: 2952 lambda_n: 0.9484352403442577 Loss: 0.023428085078801023\n",
      "Iteration: 2953 lambda_n: 1.0371936001183406 Loss: 0.02341738843378418\n",
      "Iteration: 2954 lambda_n: 0.982800045076724 Loss: 0.023405691277184337\n",
      "Iteration: 2955 lambda_n: 0.8980501455876259 Loss: 0.023394608094071196\n",
      "Iteration: 2956 lambda_n: 0.8952192836215401 Loss: 0.023384481112650395\n",
      "Iteration: 2957 lambda_n: 0.9687286988687819 Loss: 0.023374386474586235\n",
      "Iteration: 2958 lambda_n: 0.9882052589602158 Loss: 0.02336346338409233\n",
      "Iteration: 2959 lambda_n: 1.005112194052931 Loss: 0.02335232117800338\n",
      "Iteration: 2960 lambda_n: 0.9038848228540666 Loss: 0.02334098885505759\n",
      "Iteration: 2961 lambda_n: 0.9772221846848934 Loss: 0.023330798304708623\n",
      "Iteration: 2962 lambda_n: 0.9239197470589396 Loss: 0.02331978138706642\n",
      "Iteration: 2963 lambda_n: 0.9445279077829536 Loss: 0.023309365843806967\n",
      "Iteration: 2964 lambda_n: 1.000323629475593 Loss: 0.02329871842096916\n",
      "Iteration: 2965 lambda_n: 1.0271321171320529 Loss: 0.023287442501793033\n",
      "Iteration: 2966 lambda_n: 0.9645283732827458 Loss: 0.023275864903587472\n",
      "Iteration: 2967 lambda_n: 0.9291080933098623 Loss: 0.02326499345291605\n",
      "Iteration: 2968 lambda_n: 0.8945976122828583 Loss: 0.02325452167643713\n",
      "Iteration: 2969 lambda_n: 0.8986260801787866 Loss: 0.02324443926885451\n",
      "Iteration: 2970 lambda_n: 1.0330475581110237 Loss: 0.023234311852450278\n",
      "Iteration: 2971 lambda_n: 0.8995309437221729 Loss: 0.023222669972879924\n",
      "Iteration: 2972 lambda_n: 0.9976303289560943 Loss: 0.023212533202607136\n",
      "Iteration: 2973 lambda_n: 0.9003795533198018 Loss: 0.02320129138754619\n",
      "Iteration: 2974 lambda_n: 0.9419305473336819 Loss: 0.02319114587527078\n",
      "Iteration: 2975 lambda_n: 1.0392402061006885 Loss: 0.023180532569212126\n",
      "Iteration: 2976 lambda_n: 0.980439313340877 Loss: 0.02316882328042621\n",
      "Iteration: 2977 lambda_n: 1.0155376103604723 Loss: 0.023157776991968835\n",
      "Iteration: 2978 lambda_n: 1.0097176411305406 Loss: 0.023146335729971487\n",
      "Iteration: 2979 lambda_n: 0.9940313945341145 Loss: 0.023134960515858793\n",
      "Iteration: 2980 lambda_n: 0.9511307138050352 Loss: 0.02312376248504743\n",
      "Iteration: 2981 lambda_n: 0.954421212083017 Loss: 0.02311304817864924\n",
      "Iteration: 2982 lambda_n: 0.9759313888107757 Loss: 0.023102297222472583\n",
      "Iteration: 2983 lambda_n: 0.9490017532257187 Loss: 0.023091304393381216\n",
      "Iteration: 2984 lambda_n: 1.0031875884491221 Loss: 0.023080615319120562\n",
      "Iteration: 2985 lambda_n: 1.0192030230731084 Loss: 0.02306931635370833\n",
      "Iteration: 2986 lambda_n: 0.9551375014169583 Loss: 0.023057837465524474\n",
      "Iteration: 2987 lambda_n: 0.9624193352879092 Loss: 0.02304708055812263\n",
      "Iteration: 2988 lambda_n: 1.0065729297704942 Loss: 0.023036242050701837\n",
      "Iteration: 2989 lambda_n: 0.9200754484048042 Loss: 0.023024906726321886\n",
      "Iteration: 2990 lambda_n: 0.9963372722718558 Loss: 0.023014545884354667\n",
      "Iteration: 2991 lambda_n: 0.8936667807778896 Loss: 0.023003326670149933\n",
      "Iteration: 2992 lambda_n: 0.906622093069395 Loss: 0.02299326396083482\n",
      "Iteration: 2993 lambda_n: 1.0271107854812087 Loss: 0.02298305572565477\n",
      "Iteration: 2994 lambda_n: 1.049150029059919 Loss: 0.02297149123336043\n",
      "Iteration: 2995 lambda_n: 0.9471527007490621 Loss: 0.022959679058456268\n",
      "Iteration: 2996 lambda_n: 0.9687313188008182 Loss: 0.02294901567576932\n",
      "Iteration: 2997 lambda_n: 1.022840749983256 Loss: 0.0229381097428207\n",
      "Iteration: 2998 lambda_n: 1.0198173655103038 Loss: 0.022926595066849235\n",
      "Iteration: 2999 lambda_n: 0.9709545583448488 Loss: 0.022915114865007493\n",
      "Iteration: 3000 lambda_n: 0.9986001344842025 Loss: 0.02290418513113771\n",
      "Iteration: 3001 lambda_n: 0.9879176731495295 Loss: 0.02289294460247194\n",
      "Iteration: 3002 lambda_n: 1.0023480014796071 Loss: 0.02288182472639832\n",
      "Iteration: 3003 lambda_n: 0.919574224074944 Loss: 0.0228705428314507\n",
      "Iteration: 3004 lambda_n: 0.9220545095337328 Loss: 0.02286019297091696\n",
      "Iteration: 3005 lambda_n: 0.9826966164206202 Loss: 0.02284981553944393\n",
      "Iteration: 3006 lambda_n: 0.9958200030710721 Loss: 0.022838755966916074\n",
      "Iteration: 3007 lambda_n: 0.9126481389122714 Loss: 0.02282754909373336\n",
      "Iteration: 3008 lambda_n: 0.9691515840987777 Loss: 0.022817278593577586\n",
      "Iteration: 3009 lambda_n: 1.0176991328044904 Loss: 0.02280637258333686\n",
      "Iteration: 3010 lambda_n: 1.0202129512681244 Loss: 0.022794920651080533\n",
      "Iteration: 3011 lambda_n: 1.0377770484260727 Loss: 0.02278344084078456\n",
      "Iteration: 3012 lambda_n: 1.0173303644152853 Loss: 0.022771763807980204\n",
      "Iteration: 3013 lambda_n: 0.9331900247157808 Loss: 0.02276031725228046\n",
      "Iteration: 3014 lambda_n: 0.928390598517196 Loss: 0.02274981777495955\n",
      "Iteration: 3015 lambda_n: 1.0090655222265166 Loss: 0.022739372630882194\n",
      "Iteration: 3016 lambda_n: 0.9879320887513496 Loss: 0.022728020188189476\n",
      "Iteration: 3017 lambda_n: 0.9511719574253704 Loss: 0.02271690588673161\n",
      "Iteration: 3018 lambda_n: 0.9539598391404219 Loss: 0.022706205495961175\n",
      "Iteration: 3019 lambda_n: 0.9297823308841481 Loss: 0.022695474085047056\n",
      "Iteration: 3020 lambda_n: 1.0453760178060219 Loss: 0.02268501498818473\n",
      "Iteration: 3021 lambda_n: 0.9895904224797427 Loss: 0.022673255944277937\n",
      "Iteration: 3022 lambda_n: 0.9412223679231613 Loss: 0.022662124796379305\n",
      "Iteration: 3023 lambda_n: 0.9350216787225235 Loss: 0.022651538048254913\n",
      "Iteration: 3024 lambda_n: 0.9387098350801801 Loss: 0.022641021368369686\n",
      "Iteration: 3025 lambda_n: 0.9280959724771171 Loss: 0.022630463527029698\n",
      "Iteration: 3026 lambda_n: 0.948097096630585 Loss: 0.02262002537893043\n",
      "Iteration: 3027 lambda_n: 0.9194568258501123 Loss: 0.022609362600092588\n",
      "Iteration: 3028 lambda_n: 0.9350109890004643 Loss: 0.022599022238384246\n",
      "Iteration: 3029 lambda_n: 1.0411648394362212 Loss: 0.02258850726026776\n",
      "Iteration: 3030 lambda_n: 0.8956379966626012 Loss: 0.022576798840918923\n",
      "Iteration: 3031 lambda_n: 0.9420421077350717 Loss: 0.022566727274711814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3032 lambda_n: 0.9166868007567335 Loss: 0.022556134186027626\n",
      "Iteration: 3033 lambda_n: 0.9393638810360716 Loss: 0.022545826516397645\n",
      "Iteration: 3034 lambda_n: 0.995998506274541 Loss: 0.022535264155640704\n",
      "Iteration: 3035 lambda_n: 0.9368122340977472 Loss: 0.022524065311259805\n",
      "Iteration: 3036 lambda_n: 0.9956594455098948 Loss: 0.022513532270458447\n",
      "Iteration: 3037 lambda_n: 0.9526736928076909 Loss: 0.022502337902472178\n",
      "Iteration: 3038 lambda_n: 0.9735696728763729 Loss: 0.02249162715533511\n",
      "Iteration: 3039 lambda_n: 0.9410831019463519 Loss: 0.02248068179410063\n",
      "Iteration: 3040 lambda_n: 1.0089440900335822 Loss: 0.02247010197365619\n",
      "Iteration: 3041 lambda_n: 0.9775783065406254 Loss: 0.02245875956810242\n",
      "Iteration: 3042 lambda_n: 1.0018897819621233 Loss: 0.022447770102888735\n",
      "Iteration: 3043 lambda_n: 0.9533588679063665 Loss: 0.022436507666331037\n",
      "Iteration: 3044 lambda_n: 0.9541394734106968 Loss: 0.022425791091948512\n",
      "Iteration: 3045 lambda_n: 0.9267109062814566 Loss: 0.02241506604295063\n",
      "Iteration: 3046 lambda_n: 0.9927571148568478 Loss: 0.022404649596204418\n",
      "Iteration: 3047 lambda_n: 0.9146034738353378 Loss: 0.022393491075025026\n",
      "Iteration: 3048 lambda_n: 1.0076794276340182 Loss: 0.022383211290301602\n",
      "Iteration: 3049 lambda_n: 1.0349922831116176 Loss: 0.022371885666094632\n",
      "Iteration: 3050 lambda_n: 0.9101759887171657 Loss: 0.02236025339946392\n",
      "Iteration: 3051 lambda_n: 0.9888310454123558 Loss: 0.022350024242967618\n",
      "Iteration: 3052 lambda_n: 0.8936929964448125 Loss: 0.02233891139536318\n",
      "Iteration: 3053 lambda_n: 0.9463907823770985 Loss: 0.022328868023799235\n",
      "Iteration: 3054 lambda_n: 1.044983687215896 Loss: 0.02231823269775091\n",
      "Iteration: 3055 lambda_n: 0.9618528830994166 Loss: 0.02230648971660263\n",
      "Iteration: 3056 lambda_n: 1.0343634646944917 Loss: 0.022295681229197322\n",
      "Iteration: 3057 lambda_n: 0.9736132484398718 Loss: 0.022284058237567378\n",
      "Iteration: 3058 lambda_n: 1.0357182154048932 Loss: 0.02227311819764543\n",
      "Iteration: 3059 lambda_n: 0.9266075502467654 Loss: 0.022261480622093287\n",
      "Iteration: 3060 lambda_n: 1.0295226809518845 Loss: 0.02225106933242916\n",
      "Iteration: 3061 lambda_n: 0.9652346169763913 Loss: 0.022239501985631214\n",
      "Iteration: 3062 lambda_n: 0.9522012085696553 Loss: 0.022228657256150457\n",
      "Iteration: 3063 lambda_n: 0.9677504498675394 Loss: 0.022217959236996616\n",
      "Iteration: 3064 lambda_n: 0.9988100696279085 Loss: 0.02220708679644412\n",
      "Iteration: 3065 lambda_n: 0.9891183294937009 Loss: 0.022195865695560216\n",
      "Iteration: 3066 lambda_n: 1.0147404525311343 Loss: 0.02218475376796178\n",
      "Iteration: 3067 lambda_n: 1.0000648557482037 Loss: 0.022173354291751875\n",
      "Iteration: 3068 lambda_n: 1.0029069617109585 Loss: 0.022162119975938196\n",
      "Iteration: 3069 lambda_n: 1.0229778666146228 Loss: 0.022150854024462697\n",
      "Iteration: 3070 lambda_n: 0.9422278487558718 Loss: 0.02213936290700407\n",
      "Iteration: 3071 lambda_n: 0.926504338811296 Loss: 0.022128779132054906\n",
      "Iteration: 3072 lambda_n: 0.9756944441734455 Loss: 0.022118372224267806\n",
      "Iteration: 3073 lambda_n: 1.038120888482874 Loss: 0.022107413048426533\n",
      "Iteration: 3074 lambda_n: 0.9429448739050375 Loss: 0.022095752974066313\n",
      "Iteration: 3075 lambda_n: 0.9052390717142924 Loss: 0.02208516218305747\n",
      "Iteration: 3076 lambda_n: 0.9889607923286283 Loss: 0.022074995127854612\n",
      "Iteration: 3077 lambda_n: 1.0177911195430474 Loss: 0.02206388801409949\n",
      "Iteration: 3078 lambda_n: 0.9948873375680309 Loss: 0.02205245738304831\n",
      "Iteration: 3079 lambda_n: 1.0487405297822505 Loss: 0.022041284259347628\n",
      "Iteration: 3080 lambda_n: 1.0118039799943772 Loss: 0.022029506621069762\n",
      "Iteration: 3081 lambda_n: 0.9285277843440122 Loss: 0.022018144079411252\n",
      "Iteration: 3082 lambda_n: 0.91609163506683 Loss: 0.02200771698264527\n",
      "Iteration: 3083 lambda_n: 0.9529622762325249 Loss: 0.021997429769462888\n",
      "Iteration: 3084 lambda_n: 1.0367258192332542 Loss: 0.021986728753036394\n",
      "Iteration: 3085 lambda_n: 0.9215580717458182 Loss: 0.02197508740156859\n",
      "Iteration: 3086 lambda_n: 0.9532322645524742 Loss: 0.02196473951762476\n",
      "Iteration: 3087 lambda_n: 0.9872047429444483 Loss: 0.02195403620610093\n",
      "Iteration: 3088 lambda_n: 0.9199581393362363 Loss: 0.021942951683823313\n",
      "Iteration: 3089 lambda_n: 1.0094081927877634 Loss: 0.0219326224565091\n",
      "Iteration: 3090 lambda_n: 0.9397325908468489 Loss: 0.02192128913146129\n",
      "Iteration: 3091 lambda_n: 1.0125235675445785 Loss: 0.02191073834799741\n",
      "Iteration: 3092 lambda_n: 0.974167059457832 Loss: 0.021899370553561476\n",
      "Iteration: 3093 lambda_n: 0.931208855461467 Loss: 0.021888433647351966\n",
      "Iteration: 3094 lambda_n: 1.01159393176937 Loss: 0.021877979260807853\n",
      "Iteration: 3095 lambda_n: 1.0383852804067975 Loss: 0.021866622654987573\n",
      "Iteration: 3096 lambda_n: 1.026710240108218 Loss: 0.02185496554210213\n",
      "Iteration: 3097 lambda_n: 1.031695907120088 Loss: 0.021843439762543247\n",
      "Iteration: 3098 lambda_n: 0.930827924938156 Loss: 0.021831858278091563\n",
      "Iteration: 3099 lambda_n: 1.013843680018814 Loss: 0.02182140934286851\n",
      "Iteration: 3100 lambda_n: 0.934203462272825 Loss: 0.02181002875332988\n",
      "Iteration: 3101 lambda_n: 0.9776295951770705 Loss: 0.021799542372635497\n",
      "Iteration: 3102 lambda_n: 0.9029518965850672 Loss: 0.021788568758693675\n",
      "Iteration: 3103 lambda_n: 0.9243987180898884 Loss: 0.021778433594750873\n",
      "Iteration: 3104 lambda_n: 0.9487238923641147 Loss: 0.021768057902786464\n",
      "Iteration: 3105 lambda_n: 0.9973743804702424 Loss: 0.021757409389264817\n",
      "Iteration: 3106 lambda_n: 0.9007515843340366 Loss: 0.021746215046808636\n",
      "Iteration: 3107 lambda_n: 0.9773671853213243 Loss: 0.021736105393931435\n",
      "Iteration: 3108 lambda_n: 0.9763089086817113 Loss: 0.021725136048194645\n",
      "Iteration: 3109 lambda_n: 0.9171488421520521 Loss: 0.021714178804311614\n",
      "Iteration: 3110 lambda_n: 1.03769964949912 Loss: 0.021703885731162536\n",
      "Iteration: 3111 lambda_n: 0.9069443915117146 Loss: 0.021692239949396883\n",
      "Iteration: 3112 lambda_n: 0.9447634318856682 Loss: 0.021682061811329596\n",
      "Iteration: 3113 lambda_n: 0.982936305799211 Loss: 0.021671459448277326\n",
      "Iteration: 3114 lambda_n: 1.0065281633712266 Loss: 0.02166042891281553\n",
      "Iteration: 3115 lambda_n: 0.9521623625018066 Loss: 0.02164913385448514\n",
      "Iteration: 3116 lambda_n: 0.9643412252854872 Loss: 0.021638449095646443\n",
      "Iteration: 3117 lambda_n: 0.9397230552374432 Loss: 0.02162762787789148\n",
      "Iteration: 3118 lambda_n: 0.894002196576019 Loss: 0.021617083112742016\n",
      "Iteration: 3119 lambda_n: 0.9358269183026603 Loss: 0.02160705157530838\n",
      "Iteration: 3120 lambda_n: 0.9055922792569668 Loss: 0.021596550911311456\n",
      "Iteration: 3121 lambda_n: 1.0023007713291519 Loss: 0.021586389689400378\n",
      "Iteration: 3122 lambda_n: 1.003090652916746 Loss: 0.02157514354687853\n",
      "Iteration: 3123 lambda_n: 0.9635829857427703 Loss: 0.02156388876158787\n",
      "Iteration: 3124 lambda_n: 0.9805613313892558 Loss: 0.021553077466898447\n",
      "Iteration: 3125 lambda_n: 0.9015166776861754 Loss: 0.021542075881517624\n",
      "Iteration: 3126 lambda_n: 1.0316189604433426 Loss: 0.021531961342245096\n",
      "Iteration: 3127 lambda_n: 0.9228278443984996 Loss: 0.02152038732347634\n",
      "Iteration: 3128 lambda_n: 0.9753885292100247 Loss: 0.021510034065232532\n",
      "Iteration: 3129 lambda_n: 0.9266931275638727 Loss: 0.02149909131640945\n",
      "Iteration: 3130 lambda_n: 0.9747561041408168 Loss: 0.021488695065139275\n",
      "Iteration: 3131 lambda_n: 0.9596719243524804 Loss: 0.021477759801333644\n",
      "Iteration: 3132 lambda_n: 0.974861910903432 Loss: 0.02146699395403431\n",
      "Iteration: 3133 lambda_n: 0.9652390560797911 Loss: 0.02145605789575397\n",
      "Iteration: 3134 lambda_n: 0.9974073187703699 Loss: 0.02144522998154587\n",
      "Iteration: 3135 lambda_n: 0.9684903083564989 Loss: 0.021434041406116103\n",
      "Iteration: 3136 lambda_n: 1.0272080515324398 Loss: 0.021423177409251447\n",
      "Iteration: 3137 lambda_n: 0.9420800917821425 Loss: 0.02141165495091677\n",
      "Iteration: 3138 lambda_n: 0.9785682862638165 Loss: 0.021401087590571816\n",
      "Iteration: 3139 lambda_n: 1.0295219280241739 Loss: 0.02139011112564413\n",
      "Iteration: 3140 lambda_n: 1.047157062208892 Loss: 0.021378563322298035\n",
      "Iteration: 3141 lambda_n: 0.9700471969168996 Loss: 0.021366817926053397\n",
      "Iteration: 3142 lambda_n: 0.9942469243073457 Loss: 0.021355937630587164\n",
      "Iteration: 3143 lambda_n: 1.028143187970456 Loss: 0.021344786094591693\n",
      "Iteration: 3144 lambda_n: 0.9209840960454807 Loss: 0.021333254576033977\n",
      "Iteration: 3145 lambda_n: 0.9570874413630703 Loss: 0.021322925124057864\n",
      "Iteration: 3146 lambda_n: 1.0072543267319978 Loss: 0.021312190919571896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3147 lambda_n: 0.893492468981232 Loss: 0.02130089425457522\n",
      "Iteration: 3148 lambda_n: 0.9983923371064437 Loss: 0.021290873636015627\n",
      "Iteration: 3149 lambda_n: 0.9237182993190067 Loss: 0.021279676723632304\n",
      "Iteration: 3150 lambda_n: 1.029449600070327 Loss: 0.021269317451168935\n",
      "Iteration: 3151 lambda_n: 1.0124880299493564 Loss: 0.02125777260766194\n",
      "Iteration: 3152 lambda_n: 0.8987617619191139 Loss: 0.021246418176638443\n",
      "Iteration: 3153 lambda_n: 0.9358382765973925 Loss: 0.021236339285655683\n",
      "Iteration: 3154 lambda_n: 1.023382232252754 Loss: 0.021225844767471632\n",
      "Iteration: 3155 lambda_n: 1.0213701281060352 Loss: 0.021214368705674603\n",
      "Iteration: 3156 lambda_n: 0.9858625831369872 Loss: 0.021202915399417564\n",
      "Iteration: 3157 lambda_n: 0.9814794051456305 Loss: 0.021191860447031974\n",
      "Iteration: 3158 lambda_n: 0.9450204974306253 Loss: 0.021180854821194788\n",
      "Iteration: 3159 lambda_n: 0.9815069142943854 Loss: 0.0211702581878089\n",
      "Iteration: 3160 lambda_n: 0.9854393330675881 Loss: 0.02115925259444428\n",
      "Iteration: 3161 lambda_n: 1.0481961785664873 Loss: 0.02114820308007782\n",
      "Iteration: 3162 lambda_n: 0.8997255818889787 Loss: 0.02113645007082484\n",
      "Iteration: 3163 lambda_n: 1.000458725384582 Loss: 0.021126361970589265\n",
      "Iteration: 3164 lambda_n: 0.947847656934744 Loss: 0.021115144566560662\n",
      "Iteration: 3165 lambda_n: 1.028236154732257 Loss: 0.021104517217628998\n",
      "Iteration: 3166 lambda_n: 1.0488571111206066 Loss: 0.02109298871576026\n",
      "Iteration: 3167 lambda_n: 0.9924756592497352 Loss: 0.02108122920026964\n",
      "Iteration: 3168 lambda_n: 0.9103147285032225 Loss: 0.021070101998474953\n",
      "Iteration: 3169 lambda_n: 0.9090037437633818 Loss: 0.02105989610381572\n",
      "Iteration: 3170 lambda_n: 1.0033689579267009 Loss: 0.02104970504815279\n",
      "Iteration: 3171 lambda_n: 0.9502542756059701 Loss: 0.021038456196470896\n",
      "Iteration: 3172 lambda_n: 0.8958865489939224 Loss: 0.021027802978757684\n",
      "Iteration: 3173 lambda_n: 0.8949220220441001 Loss: 0.021017759415772563\n",
      "Iteration: 3174 lambda_n: 0.9360350426448771 Loss: 0.021007726799740823\n",
      "Iteration: 3175 lambda_n: 0.9019551749308992 Loss: 0.020997233421134385\n",
      "Iteration: 3176 lambda_n: 1.0304110230448111 Loss: 0.02098712223296331\n",
      "Iteration: 3177 lambda_n: 1.0212483651059565 Loss: 0.020975571169102442\n",
      "Iteration: 3178 lambda_n: 0.9769619801175571 Loss: 0.020964122992321397\n",
      "Iteration: 3179 lambda_n: 0.9878499725357374 Loss: 0.02095317142760849\n",
      "Iteration: 3180 lambda_n: 1.0094586488618775 Loss: 0.020942097966762316\n",
      "Iteration: 3181 lambda_n: 0.9465887312594732 Loss: 0.02093078244064408\n",
      "Iteration: 3182 lambda_n: 1.0270991565070597 Loss: 0.020920171807908727\n",
      "Iteration: 3183 lambda_n: 0.9026525219194748 Loss: 0.020908658861533563\n",
      "Iteration: 3184 lambda_n: 1.0091400510225161 Loss: 0.020898541007697655\n",
      "Iteration: 3185 lambda_n: 0.9633007969933802 Loss: 0.020887229676171414\n",
      "Iteration: 3186 lambda_n: 0.9541205886845624 Loss: 0.020876432303908676\n",
      "Iteration: 3187 lambda_n: 0.9915802885070311 Loss: 0.02086573797344607\n",
      "Iteration: 3188 lambda_n: 0.8946960296373794 Loss: 0.020854623920019646\n",
      "Iteration: 3189 lambda_n: 0.8987665440420011 Loss: 0.020844595923523866\n",
      "Iteration: 3190 lambda_n: 0.9358792137238435 Loss: 0.020834522427077336\n",
      "Iteration: 3191 lambda_n: 1.046660226619798 Loss: 0.02082403309537246\n",
      "Iteration: 3192 lambda_n: 0.8969861691222384 Loss: 0.020812302279338172\n",
      "Iteration: 3193 lambda_n: 0.8982069666319205 Loss: 0.020802249130485635\n",
      "Iteration: 3194 lambda_n: 0.9426317013152261 Loss: 0.02079218242051782\n",
      "Iteration: 3195 lambda_n: 0.964923449167439 Loss: 0.020781617944359405\n",
      "Iteration: 3196 lambda_n: 0.9963120437524233 Loss: 0.020770803770576773\n",
      "Iteration: 3197 lambda_n: 0.9295368181298236 Loss: 0.020759637958317655\n",
      "Iteration: 3198 lambda_n: 0.9811805975227066 Loss: 0.020749220642169556\n",
      "Iteration: 3199 lambda_n: 1.0041496023516945 Loss: 0.020738224688112903\n",
      "Iteration: 3200 lambda_n: 0.9850822879199265 Loss: 0.02072697146739609\n",
      "Iteration: 3201 lambda_n: 0.9595510809257138 Loss: 0.020715932072259337\n",
      "Iteration: 3202 lambda_n: 1.0494233143289216 Loss: 0.020705178930875746\n",
      "Iteration: 3203 lambda_n: 1.002869511675114 Loss: 0.02069341878725244\n",
      "Iteration: 3204 lambda_n: 1.0417952737598233 Loss: 0.02068218048945091\n",
      "Iteration: 3205 lambda_n: 0.9022423582469274 Loss: 0.02067050613244194\n",
      "Iteration: 3206 lambda_n: 0.9907937685521327 Loss: 0.020660395738075033\n",
      "Iteration: 3207 lambda_n: 0.8953246166678598 Loss: 0.020649293175276383\n",
      "Iteration: 3208 lambda_n: 1.0421865705213202 Loss: 0.020639260537566956\n",
      "Iteration: 3209 lambda_n: 0.9634330796153118 Loss: 0.02062758235521685\n",
      "Iteration: 3210 lambda_n: 0.9660477718022331 Loss: 0.02061678678106588\n",
      "Iteration: 3211 lambda_n: 0.9830992683928519 Loss: 0.020605962036505665\n",
      "Iteration: 3212 lambda_n: 0.9839536442019118 Loss: 0.020594946356743016\n",
      "Iteration: 3213 lambda_n: 1.0387337195140445 Loss: 0.02058392123535237\n",
      "Iteration: 3214 lambda_n: 0.9112715294197908 Loss: 0.02057228244603812\n",
      "Iteration: 3215 lambda_n: 0.9085728566171511 Loss: 0.02056207197082567\n",
      "Iteration: 3216 lambda_n: 0.9865983510467868 Loss: 0.020551891844188392\n",
      "Iteration: 3217 lambda_n: 0.8952990647295854 Loss: 0.020540837598479886\n",
      "Iteration: 3218 lambda_n: 1.0463046513074636 Loss: 0.02053080642393774\n",
      "Iteration: 3219 lambda_n: 0.9761083050772827 Loss: 0.020519083465257916\n",
      "Iteration: 3220 lambda_n: 0.9794686005222296 Loss: 0.020508147131347153\n",
      "Iteration: 3221 lambda_n: 0.9130359567028505 Loss: 0.020497173273434106\n",
      "Iteration: 3222 lambda_n: 0.9418215086013008 Loss: 0.020486943835641477\n",
      "Iteration: 3223 lambda_n: 0.9296398248153989 Loss: 0.02047639200243746\n",
      "Iteration: 3224 lambda_n: 0.9533024298685641 Loss: 0.020465976761015702\n",
      "Iteration: 3225 lambda_n: 1.0270169113358447 Loss: 0.020455296528368454\n",
      "Iteration: 3226 lambda_n: 0.9849335264351523 Loss: 0.02044379056713596\n",
      "Iteration: 3227 lambda_n: 0.9146518397400085 Loss: 0.020432756206010153\n",
      "Iteration: 3228 lambda_n: 1.042731177099912 Loss: 0.020422509334772562\n",
      "Iteration: 3229 lambda_n: 0.9286842241367098 Loss: 0.02041082770638664\n",
      "Iteration: 3230 lambda_n: 1.0106267619858327 Loss: 0.020400423856917104\n",
      "Iteration: 3231 lambda_n: 0.9929980585265321 Loss: 0.020389102139095446\n",
      "Iteration: 3232 lambda_n: 0.9887509619360363 Loss: 0.020377978033504787\n",
      "Iteration: 3233 lambda_n: 0.9146992677699533 Loss: 0.02036690162655503\n",
      "Iteration: 3234 lambda_n: 1.03480754250966 Loss: 0.020356654888303936\n",
      "Iteration: 3235 lambda_n: 1.0402035415826976 Loss: 0.020345062775562774\n",
      "Iteration: 3236 lambda_n: 0.9950309368794168 Loss: 0.02033341034569049\n",
      "Iteration: 3237 lambda_n: 0.9102157042531073 Loss: 0.020322264066561773\n",
      "Iteration: 3238 lambda_n: 1.0406213101573465 Loss: 0.02031206799083707\n",
      "Iteration: 3239 lambda_n: 0.9949591454164666 Loss: 0.02030041124692912\n",
      "Iteration: 3240 lambda_n: 1.0380846583599046 Loss: 0.020289266119867935\n",
      "Iteration: 3241 lambda_n: 1.0130724731414993 Loss: 0.02027763803965883\n",
      "Iteration: 3242 lambda_n: 0.8966085041314767 Loss: 0.02026629025566243\n",
      "Iteration: 3243 lambda_n: 0.8989785582238489 Loss: 0.020256247131369306\n",
      "Iteration: 3244 lambda_n: 0.948436833459088 Loss: 0.02024617755263476\n",
      "Iteration: 3245 lambda_n: 1.0240055512594963 Loss: 0.020235554083162376\n",
      "Iteration: 3246 lambda_n: 0.981424226427381 Loss: 0.02022408427729731\n",
      "Iteration: 3247 lambda_n: 0.910536481108323 Loss: 0.020213091535872026\n",
      "Iteration: 3248 lambda_n: 1.0391766927085868 Loss: 0.02020289289538659\n",
      "Iteration: 3249 lambda_n: 0.9917830997666277 Loss: 0.020191253501763876\n",
      "Iteration: 3250 lambda_n: 1.0438220024431137 Loss: 0.02018014505985378\n",
      "Iteration: 3251 lambda_n: 0.9595178913122778 Loss: 0.02016845387281309\n",
      "Iteration: 3252 lambda_n: 0.9542685874968139 Loss: 0.02015770703351043\n",
      "Iteration: 3253 lambda_n: 0.9266239165475292 Loss: 0.020147019088578865\n",
      "Iteration: 3254 lambda_n: 1.0061402630721712 Loss: 0.02013664086482144\n",
      "Iteration: 3255 lambda_n: 0.9874486122853499 Loss: 0.020125372156540533\n",
      "Iteration: 3256 lambda_n: 0.9510977066776275 Loss: 0.02011431290135043\n",
      "Iteration: 3257 lambda_n: 0.9013533677288627 Loss: 0.020103660871388027\n",
      "Iteration: 3258 lambda_n: 1.0319201098452444 Loss: 0.02009356605618402\n",
      "Iteration: 3259 lambda_n: 0.987758618914331 Loss: 0.02008200904237545\n",
      "Iteration: 3260 lambda_n: 0.9669558117556339 Loss: 0.020070946724514066\n",
      "Iteration: 3261 lambda_n: 0.8998100003745472 Loss: 0.02006011748680191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3262 lambda_n: 0.9417798674750817 Loss: 0.020050040327255464\n",
      "Iteration: 3263 lambda_n: 1.0463569051152946 Loss: 0.020039493227003942\n",
      "Iteration: 3264 lambda_n: 0.9644584415577322 Loss: 0.020027775059068706\n",
      "Iteration: 3265 lambda_n: 1.0264557847940006 Loss: 0.02001697417778685\n",
      "Iteration: 3266 lambda_n: 1.0253956207346713 Loss: 0.020005479095833417\n",
      "Iteration: 3267 lambda_n: 0.9891250659218668 Loss: 0.019993995994192285\n",
      "Iteration: 3268 lambda_n: 0.9225408370609539 Loss: 0.019982919178987302\n",
      "Iteration: 3269 lambda_n: 0.9634169308506835 Loss: 0.019972588106248253\n",
      "Iteration: 3270 lambda_n: 0.9761518426105052 Loss: 0.019961799372098774\n",
      "Iteration: 3271 lambda_n: 0.9046599970190411 Loss: 0.019950868121451387\n",
      "Iteration: 3272 lambda_n: 0.9031988306505773 Loss: 0.019940737546641822\n",
      "Iteration: 3273 lambda_n: 0.9491393172499502 Loss: 0.01993062341528599\n",
      "Iteration: 3274 lambda_n: 1.0479857704080664 Loss: 0.01991999492129193\n",
      "Iteration: 3275 lambda_n: 1.0495351859930235 Loss: 0.019908259639037546\n",
      "Iteration: 3276 lambda_n: 0.8998782649155742 Loss: 0.019896507113940164\n",
      "Iteration: 3277 lambda_n: 0.928027699237988 Loss: 0.019886430514532337\n",
      "Iteration: 3278 lambda_n: 1.0460374114980147 Loss: 0.0198760387858227\n",
      "Iteration: 3279 lambda_n: 1.0151094460431491 Loss: 0.019864325719020394\n",
      "Iteration: 3280 lambda_n: 1.007789791556866 Loss: 0.019852959071489422\n",
      "Iteration: 3281 lambda_n: 1.0434945883079365 Loss: 0.01984167448275088\n",
      "Iteration: 3282 lambda_n: 0.9943113887056843 Loss: 0.019829990193844182\n",
      "Iteration: 3283 lambda_n: 1.0467652827120875 Loss: 0.019818856719979894\n",
      "Iteration: 3284 lambda_n: 1.0283846679167763 Loss: 0.019807136008248433\n",
      "Iteration: 3285 lambda_n: 0.9112864458113835 Loss: 0.019795621205786366\n",
      "Iteration: 3286 lambda_n: 0.9449558345107266 Loss: 0.019785417636420003\n",
      "Iteration: 3287 lambda_n: 1.0185200775333756 Loss: 0.01977483715396934\n",
      "Iteration: 3288 lambda_n: 0.9098138569430623 Loss: 0.019763433075391464\n",
      "Iteration: 3289 lambda_n: 0.9841752827751957 Loss: 0.019753246233691537\n",
      "Iteration: 3290 lambda_n: 0.9718775881412856 Loss: 0.019742226876289005\n",
      "Iteration: 3291 lambda_n: 0.933312856644929 Loss: 0.019731345296675203\n",
      "Iteration: 3292 lambda_n: 0.9233563962351664 Loss: 0.019720895586457933\n",
      "Iteration: 3293 lambda_n: 1.0058340595220217 Loss: 0.019710557429206233\n",
      "Iteration: 3294 lambda_n: 0.9914744186371133 Loss: 0.019699295911261953\n",
      "Iteration: 3295 lambda_n: 0.9556824291158702 Loss: 0.01968819525470793\n",
      "Iteration: 3296 lambda_n: 0.9633892392235146 Loss: 0.019677495412357724\n",
      "Iteration: 3297 lambda_n: 0.9228655396470598 Loss: 0.01966670936475185\n",
      "Iteration: 3298 lambda_n: 1.0185941064795883 Loss: 0.01965637709523912\n",
      "Iteration: 3299 lambda_n: 0.96127513099942 Loss: 0.019644973143699684\n",
      "Iteration: 3300 lambda_n: 0.9052674950198746 Loss: 0.019634211006736936\n",
      "Iteration: 3301 lambda_n: 0.9635413295006776 Loss: 0.019624075988208013\n",
      "Iteration: 3302 lambda_n: 0.9057301560013827 Loss: 0.019613288632940293\n",
      "Iteration: 3303 lambda_n: 0.9052812333468298 Loss: 0.019603148578212097\n",
      "Iteration: 3304 lambda_n: 0.9004369465697314 Loss: 0.01959301361839501\n",
      "Iteration: 3305 lambda_n: 0.9573137218782771 Loss: 0.019582932960449925\n",
      "Iteration: 3306 lambda_n: 0.9116526632499277 Loss: 0.019572215622034674\n",
      "Iteration: 3307 lambda_n: 0.9053949403209245 Loss: 0.01956200954164196\n",
      "Iteration: 3308 lambda_n: 0.9707081612306182 Loss: 0.019551873585453773\n",
      "Iteration: 3309 lambda_n: 1.0076722914367087 Loss: 0.019541006515607966\n",
      "Iteration: 3310 lambda_n: 0.9396561849803613 Loss: 0.01952972571255521\n",
      "Iteration: 3311 lambda_n: 0.9791321476420201 Loss: 0.019519206420819803\n",
      "Iteration: 3312 lambda_n: 0.9986499935049491 Loss: 0.01950824527674765\n",
      "Iteration: 3313 lambda_n: 0.9684389793564505 Loss: 0.019497065713823914\n",
      "Iteration: 3314 lambda_n: 0.9399399014052825 Loss: 0.019486224430780835\n",
      "Iteration: 3315 lambda_n: 0.9355761839298935 Loss: 0.01947570225582872\n",
      "Iteration: 3316 lambda_n: 0.9134980944192502 Loss: 0.019465229000199053\n",
      "Iteration: 3317 lambda_n: 1.000468404995616 Loss: 0.019455002963819003\n",
      "Iteration: 3318 lambda_n: 0.9753429930666146 Loss: 0.019443803421141746\n",
      "Iteration: 3319 lambda_n: 0.9197951965719693 Loss: 0.01943288521592008\n",
      "Iteration: 3320 lambda_n: 0.9617675492012749 Loss: 0.019422588894583752\n",
      "Iteration: 3321 lambda_n: 0.9123317542452565 Loss: 0.01941182279685501\n",
      "Iteration: 3322 lambda_n: 0.9601917288898929 Loss: 0.019401610154447597\n",
      "Iteration: 3323 lambda_n: 0.982413181668603 Loss: 0.019390861834327356\n",
      "Iteration: 3324 lambda_n: 0.9998557356923118 Loss: 0.019379864840459095\n",
      "Iteration: 3325 lambda_n: 0.892383465945936 Loss: 0.01936867267135163\n",
      "Iteration: 3326 lambda_n: 0.9778109689907849 Loss: 0.019358683590690404\n",
      "Iteration: 3327 lambda_n: 1.0108411650887268 Loss: 0.019347738324678156\n",
      "Iteration: 3328 lambda_n: 0.9491532149958827 Loss: 0.019336423404018557\n",
      "Iteration: 3329 lambda_n: 0.9385809179367504 Loss: 0.019325799062717485\n",
      "Iteration: 3330 lambda_n: 1.011180403183823 Loss: 0.019315293127941082\n",
      "Iteration: 3331 lambda_n: 0.955817488847044 Loss: 0.01930397462579963\n",
      "Iteration: 3332 lambda_n: 0.9405226311607467 Loss: 0.01929327589088164\n",
      "Iteration: 3333 lambda_n: 0.9097912444633582 Loss: 0.019282748420762646\n",
      "Iteration: 3334 lambda_n: 0.95995879310574 Loss: 0.01927256499528275\n",
      "Iteration: 3335 lambda_n: 0.9971991391588777 Loss: 0.0192618200997848\n",
      "Iteration: 3336 lambda_n: 0.9010607037450705 Loss: 0.019250658438433925\n",
      "Iteration: 3337 lambda_n: 0.9337669742109906 Loss: 0.019240572919480725\n",
      "Iteration: 3338 lambda_n: 1.048190950584481 Loss: 0.019230121380596425\n",
      "Iteration: 3339 lambda_n: 0.9688173651559184 Loss: 0.019218389176995723\n",
      "Iteration: 3340 lambda_n: 0.9224816324080439 Loss: 0.019207545457962865\n",
      "Iteration: 3341 lambda_n: 0.9982173762903971 Loss: 0.01919722042477967\n",
      "Iteration: 3342 lambda_n: 1.0134873487147074 Loss: 0.019186047770029573\n",
      "Iteration: 3343 lambda_n: 0.9796107839109927 Loss: 0.019174704274091035\n",
      "Iteration: 3344 lambda_n: 1.029678084394317 Loss: 0.019163740010845268\n",
      "Iteration: 3345 lambda_n: 0.957132890991917 Loss: 0.01915221543954685\n",
      "Iteration: 3346 lambda_n: 1.0143031623271037 Loss: 0.0191415028899544\n",
      "Iteration: 3347 lambda_n: 0.8923639224413252 Loss: 0.019130150537032622\n",
      "Iteration: 3348 lambda_n: 0.9772956404628546 Loss: 0.019120163021394947\n",
      "Iteration: 3349 lambda_n: 1.0352829275804754 Loss: 0.019109224991056734\n",
      "Iteration: 3350 lambda_n: 1.0021912891463254 Loss: 0.01909763802590642\n",
      "Iteration: 3351 lambda_n: 0.9165048039934841 Loss: 0.019086421493296934\n",
      "Iteration: 3352 lambda_n: 0.8957716787983833 Loss: 0.01907616402471012\n",
      "Iteration: 3353 lambda_n: 0.9251837091897865 Loss: 0.019066138653593138\n",
      "Iteration: 3354 lambda_n: 1.0397138822026593 Loss: 0.019055784160271105\n",
      "Iteration: 3355 lambda_n: 0.9558125190909259 Loss: 0.01904414792751321\n",
      "Iteration: 3356 lambda_n: 0.9440784391595887 Loss: 0.019033450762887948\n",
      "Iteration: 3357 lambda_n: 0.9829267829695176 Loss: 0.019022884980185565\n",
      "Iteration: 3358 lambda_n: 0.9940986720489892 Loss: 0.019011884479919337\n",
      "Iteration: 3359 lambda_n: 0.9133057433211691 Loss: 0.019000759010387724\n",
      "Iteration: 3360 lambda_n: 0.8936879611089774 Loss: 0.018990537793179652\n",
      "Iteration: 3361 lambda_n: 0.9132069649488633 Loss: 0.018980536178495728\n",
      "Iteration: 3362 lambda_n: 0.9206392019643411 Loss: 0.018970316169738516\n",
      "Iteration: 3363 lambda_n: 1.0206552659401384 Loss: 0.018960013036378308\n",
      "Iteration: 3364 lambda_n: 1.017301229349805 Loss: 0.018948590652916344\n",
      "Iteration: 3365 lambda_n: 0.9957199096309494 Loss: 0.018937205868919105\n",
      "Iteration: 3366 lambda_n: 1.0486854859011192 Loss: 0.01892606266677638\n",
      "Iteration: 3367 lambda_n: 0.9794895722851263 Loss: 0.018914326784857313\n",
      "Iteration: 3368 lambda_n: 0.9009134042928706 Loss: 0.01890336533919411\n",
      "Iteration: 3369 lambda_n: 0.9672237131568201 Loss: 0.018893283290588668\n",
      "Iteration: 3370 lambda_n: 0.9292916053627258 Loss: 0.018882459220709906\n",
      "Iteration: 3371 lambda_n: 0.9499389892659584 Loss: 0.01887205969729831\n",
      "Iteration: 3372 lambda_n: 0.9025084049396681 Loss: 0.01886142916513553\n",
      "Iteration: 3373 lambda_n: 1.007746884187983 Loss: 0.018851329467293745\n",
      "Iteration: 3374 lambda_n: 1.0022237282695394 Loss: 0.01884005213072832\n",
      "Iteration: 3375 lambda_n: 1.0267405311403845 Loss: 0.018828836660601866\n",
      "Iteration: 3376 lambda_n: 0.9758454927811216 Loss: 0.018817346892650303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3377 lambda_n: 0.9763504212873935 Loss: 0.018806426724682412\n",
      "Iteration: 3378 lambda_n: 1.0418131000489954 Loss: 0.018795500960860398\n",
      "Iteration: 3379 lambda_n: 0.9667885209428966 Loss: 0.018783842700580197\n",
      "Iteration: 3380 lambda_n: 0.9140504483248495 Loss: 0.018773024049148143\n",
      "Iteration: 3381 lambda_n: 0.9737341420370794 Loss: 0.01876279560221652\n",
      "Iteration: 3382 lambda_n: 0.9558789207190891 Loss: 0.01875189933008868\n",
      "Iteration: 3383 lambda_n: 0.9447874759465232 Loss: 0.018741202913200843\n",
      "Iteration: 3384 lambda_n: 0.8978070264703949 Loss: 0.018730630661184366\n",
      "Iteration: 3385 lambda_n: 0.99376134036296 Loss: 0.018720584171173563\n",
      "Iteration: 3386 lambda_n: 1.0015061546163915 Loss: 0.018709463998213605\n",
      "Iteration: 3387 lambda_n: 0.938439921982297 Loss: 0.018698257215264167\n",
      "Iteration: 3388 lambda_n: 0.9303230158460808 Loss: 0.01868775619004408\n",
      "Iteration: 3389 lambda_n: 0.9137048258705867 Loss: 0.01867734603915386\n",
      "Iteration: 3390 lambda_n: 0.9290473254112568 Loss: 0.01866712188857631\n",
      "Iteration: 3391 lambda_n: 1.0201732565102135 Loss: 0.01865672610429291\n",
      "Iteration: 3392 lambda_n: 0.9051045149203408 Loss: 0.018645310696317773\n",
      "Iteration: 3393 lambda_n: 1.0357448398013294 Loss: 0.018635182919143272\n",
      "Iteration: 3394 lambda_n: 0.9808134511641352 Loss: 0.018623593375509527\n",
      "Iteration: 3395 lambda_n: 0.9721184343180924 Loss: 0.018612618543949078\n",
      "Iteration: 3396 lambda_n: 0.9078857592578685 Loss: 0.018601741055131855\n",
      "Iteration: 3397 lambda_n: 0.9838861017166395 Loss: 0.01859158234156517\n",
      "Iteration: 3398 lambda_n: 1.0399235477674422 Loss: 0.01858057327442466\n",
      "Iteration: 3399 lambda_n: 0.9808759850547185 Loss: 0.01856893723597095\n",
      "Iteration: 3400 lambda_n: 0.9491611186931944 Loss: 0.018557961951705566\n",
      "Iteration: 3401 lambda_n: 0.939672928369974 Loss: 0.018547341580820387\n",
      "Iteration: 3402 lambda_n: 1.0415386088048844 Loss: 0.018536827420383754\n",
      "Iteration: 3403 lambda_n: 0.9682955245289236 Loss: 0.018525173516643477\n",
      "Iteration: 3404 lambda_n: 0.9857918597120147 Loss: 0.01851433918917522\n",
      "Iteration: 3405 lambda_n: 1.001577601253574 Loss: 0.01850330914138671\n",
      "Iteration: 3406 lambda_n: 1.0041384295560212 Loss: 0.01849210251536406\n",
      "Iteration: 3407 lambda_n: 0.924293900935596 Loss: 0.01848086728573331\n",
      "Iteration: 3408 lambda_n: 1.0108682032655734 Loss: 0.018470525475921436\n",
      "Iteration: 3409 lambda_n: 0.9825390245785934 Loss: 0.018459215042407377\n",
      "Iteration: 3410 lambda_n: 0.9284158398289404 Loss: 0.018448221627336083\n",
      "Iteration: 3411 lambda_n: 0.9797186739100049 Loss: 0.018437833828675518\n",
      "Iteration: 3412 lambda_n: 1.031122585945483 Loss: 0.018426872059737677\n",
      "Iteration: 3413 lambda_n: 0.9423172197021699 Loss: 0.0184153351964448\n",
      "Iteration: 3414 lambda_n: 0.9841595913368861 Loss: 0.01840479199078062\n",
      "Iteration: 3415 lambda_n: 0.9500362991135005 Loss: 0.018393780671449606\n",
      "Iteration: 3416 lambda_n: 0.9115170737629098 Loss: 0.01838315118611185\n",
      "Iteration: 3417 lambda_n: 1.0385752440905471 Loss: 0.01837295271366086\n",
      "Iteration: 3418 lambda_n: 0.9692548175338033 Loss: 0.018361332699833408\n",
      "Iteration: 3419 lambda_n: 1.0343388302672216 Loss: 0.0183504883183544\n",
      "Iteration: 3420 lambda_n: 0.9664897122326096 Loss: 0.0183389157988621\n",
      "Iteration: 3421 lambda_n: 1.0236514642023105 Loss: 0.018328102443013604\n",
      "Iteration: 3422 lambda_n: 0.9774541293621686 Loss: 0.01831664959048087\n",
      "Iteration: 3423 lambda_n: 0.9937029978961085 Loss: 0.01830571364973461\n",
      "Iteration: 3424 lambda_n: 0.997269666089637 Loss: 0.01829459595720008\n",
      "Iteration: 3425 lambda_n: 0.9337471632976717 Loss: 0.01828343840452663\n",
      "Iteration: 3426 lambda_n: 1.0383970100825772 Loss: 0.018272991589327214\n",
      "Iteration: 3427 lambda_n: 0.9744484828309398 Loss: 0.018261373988660158\n",
      "Iteration: 3428 lambda_n: 1.0163669549437055 Loss: 0.018250471889515178\n",
      "Iteration: 3429 lambda_n: 0.9864739207133957 Loss: 0.01823910085110759\n",
      "Iteration: 3430 lambda_n: 0.9640956797244802 Loss: 0.01822806429735395\n",
      "Iteration: 3431 lambda_n: 0.9161121872975124 Loss: 0.018217278149844196\n",
      "Iteration: 3432 lambda_n: 1.0042860487708727 Loss: 0.01820702887192416\n",
      "Iteration: 3433 lambda_n: 1.00203290342581 Loss: 0.018195793162035966\n",
      "Iteration: 3434 lambda_n: 0.8998027637121822 Loss: 0.018184582702638476\n",
      "Iteration: 3435 lambda_n: 1.0323030968440097 Loss: 0.01817451600317404\n",
      "Iteration: 3436 lambda_n: 0.926211537665804 Loss: 0.018162966972372796\n",
      "Iteration: 3437 lambda_n: 0.9844162549741028 Loss: 0.018152604895338792\n",
      "Iteration: 3438 lambda_n: 0.8945677643965361 Loss: 0.01814159168565803\n",
      "Iteration: 3439 lambda_n: 1.0286302724109542 Loss: 0.01813158369730354\n",
      "Iteration: 3440 lambda_n: 0.9908038692151583 Loss: 0.01812007592087178\n",
      "Iteration: 3441 lambda_n: 1.027811007946407 Loss: 0.018108991368248558\n",
      "Iteration: 3442 lambda_n: 0.9078597572508598 Loss: 0.018097492842279957\n",
      "Iteration: 3443 lambda_n: 0.9078711773091164 Loss: 0.018087336296010652\n",
      "Iteration: 3444 lambda_n: 1.037958953350664 Loss: 0.018077179655257087\n",
      "Iteration: 3445 lambda_n: 0.9606592703900451 Loss: 0.01806556771944511\n",
      "Iteration: 3446 lambda_n: 0.9239028655806368 Loss: 0.018054820596570643\n",
      "Iteration: 3447 lambda_n: 0.9545733888858385 Loss: 0.018044484711588166\n",
      "Iteration: 3448 lambda_n: 1.0411696133994188 Loss: 0.018033805744242532\n",
      "Iteration: 3449 lambda_n: 0.913267264881979 Loss: 0.01802215804999341\n",
      "Iteration: 3450 lambda_n: 0.9692560234194634 Loss: 0.018011941252482205\n",
      "Iteration: 3451 lambda_n: 0.8965771119067908 Loss: 0.01800109813848765\n",
      "Iteration: 3452 lambda_n: 1.0122173712584728 Loss: 0.017991068120648757\n",
      "Iteration: 3453 lambda_n: 0.9270358501066751 Loss: 0.017979744468945298\n",
      "Iteration: 3454 lambda_n: 0.9748773325029111 Loss: 0.017969373776875665\n",
      "Iteration: 3455 lambda_n: 0.985304421686926 Loss: 0.01795846791959671\n",
      "Iteration: 3456 lambda_n: 1.0442183651905155 Loss: 0.01794744545193601\n",
      "Iteration: 3457 lambda_n: 0.9587268783466242 Loss: 0.017935763960766474\n",
      "Iteration: 3458 lambda_n: 0.904231452015256 Loss: 0.01792503888573105\n",
      "Iteration: 3459 lambda_n: 0.9644747883581465 Loss: 0.01791492347187615\n",
      "Iteration: 3460 lambda_n: 0.9731219564025424 Loss: 0.017904134163085936\n",
      "Iteration: 3461 lambda_n: 1.0209048241667813 Loss: 0.01789324815553497\n",
      "Iteration: 3462 lambda_n: 0.9262479682195189 Loss: 0.017881827652673946\n",
      "Iteration: 3463 lambda_n: 0.9891269466500745 Loss: 0.017871466077313648\n",
      "Iteration: 3464 lambda_n: 0.9324532645779823 Loss: 0.017860401132620902\n",
      "Iteration: 3465 lambda_n: 0.9623977404617846 Loss: 0.017849970205803856\n",
      "Iteration: 3466 lambda_n: 1.0142839366623235 Loss: 0.01783920433617021\n",
      "Iteration: 3467 lambda_n: 1.01420764553115 Loss: 0.017827858076160746\n",
      "Iteration: 3468 lambda_n: 0.9188727410525077 Loss: 0.017816512706221876\n",
      "Iteration: 3469 lambda_n: 0.9801567614391838 Loss: 0.017806233827199442\n",
      "Iteration: 3470 lambda_n: 1.0317285370594973 Loss: 0.01779526943227463\n",
      "Iteration: 3471 lambda_n: 1.0160091652482284 Loss: 0.017783728171902175\n",
      "Iteration: 3472 lambda_n: 0.9972135415301003 Loss: 0.017772362790229673\n",
      "Iteration: 3473 lambda_n: 1.008900214755329 Loss: 0.017761207697143358\n",
      "Iteration: 3474 lambda_n: 0.9402906538471447 Loss: 0.017749921908548617\n",
      "Iteration: 3475 lambda_n: 1.04748255290663 Loss: 0.017739403634733763\n",
      "Iteration: 3476 lambda_n: 1.0279784598059154 Loss: 0.017727686325120826\n",
      "Iteration: 3477 lambda_n: 1.043338951459557 Loss: 0.0177161872279973\n",
      "Iteration: 3478 lambda_n: 0.9886211062322002 Loss: 0.017704516342662444\n",
      "Iteration: 3479 lambda_n: 1.0108824285041658 Loss: 0.0176934575707824\n",
      "Iteration: 3480 lambda_n: 0.9182503521871157 Loss: 0.017682149815832864\n",
      "Iteration: 3481 lambda_n: 0.9178353807886495 Loss: 0.01767187827630507\n",
      "Iteration: 3482 lambda_n: 1.0221951697399485 Loss: 0.017661611406461917\n",
      "Iteration: 3483 lambda_n: 0.9680119723951823 Loss: 0.01765017720300919\n",
      "Iteration: 3484 lambda_n: 1.0099306621661313 Loss: 0.017639349121337473\n",
      "Iteration: 3485 lambda_n: 1.0280455629870258 Loss: 0.0176280521733155\n",
      "Iteration: 3486 lambda_n: 0.9193351167134163 Loss: 0.01761655262802482\n",
      "Iteration: 3487 lambda_n: 0.9871130113741534 Loss: 0.017606269129946134\n",
      "Iteration: 3488 lambda_n: 1.030764997418095 Loss: 0.01759522751095846\n",
      "Iteration: 3489 lambda_n: 1.0154755419412704 Loss: 0.01758369764327105\n",
      "Iteration: 3490 lambda_n: 1.0331590235034556 Loss: 0.01757233883255703\n",
      "Iteration: 3491 lambda_n: 0.9630223219749043 Loss: 0.017560782252634587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3492 lambda_n: 0.9835873152704101 Loss: 0.017550010230121194\n",
      "Iteration: 3493 lambda_n: 0.9476239236492324 Loss: 0.017539008204441726\n",
      "Iteration: 3494 lambda_n: 0.8945056486735582 Loss: 0.017528408480147865\n",
      "Iteration: 3495 lambda_n: 0.9852724955387867 Loss: 0.0175184029407313\n",
      "Iteration: 3496 lambda_n: 0.9452650711981033 Loss: 0.01750738215103247\n",
      "Iteration: 3497 lambda_n: 1.0127233303130558 Loss: 0.017496808893755388\n",
      "Iteration: 3498 lambda_n: 0.9303605459675371 Loss: 0.017485481111520044\n",
      "Iteration: 3499 lambda_n: 0.9063954531104345 Loss: 0.017475074623822806\n",
      "Iteration: 3500 lambda_n: 0.970111007783533 Loss: 0.01746493622142318\n",
      "Iteration: 3501 lambda_n: 0.9054846884035267 Loss: 0.01745408516083758\n",
      "Iteration: 3502 lambda_n: 1.0236727978981803 Loss: 0.017443956996308792\n",
      "Iteration: 3503 lambda_n: 0.9471094492274179 Loss: 0.0174325068836221\n",
      "Iteration: 3504 lambda_n: 1.0369156306090173 Loss: 0.017421913185364745\n",
      "Iteration: 3505 lambda_n: 1.0336651425577994 Loss: 0.01741031500722609\n",
      "Iteration: 3506 lambda_n: 0.9153555526701255 Loss: 0.01739875321784705\n",
      "Iteration: 3507 lambda_n: 0.9915951891243265 Loss: 0.017388514776618093\n",
      "Iteration: 3508 lambda_n: 0.8923164033624105 Loss: 0.01737742360539463\n",
      "Iteration: 3509 lambda_n: 0.9702386125772283 Loss: 0.01736744291064344\n",
      "Iteration: 3510 lambda_n: 1.0016082117128022 Loss: 0.01735659066877531\n",
      "Iteration: 3511 lambda_n: 1.0063567669557791 Loss: 0.01734538758150027\n",
      "Iteration: 3512 lambda_n: 0.9173738131327969 Loss: 0.017334131409574325\n",
      "Iteration: 3513 lambda_n: 0.9635312778084862 Loss: 0.01732387054418842\n",
      "Iteration: 3514 lambda_n: 0.9879512721156263 Loss: 0.017313093430227716\n",
      "Iteration: 3515 lambda_n: 1.018265910856391 Loss: 0.017302043204618006\n",
      "Iteration: 3516 lambda_n: 1.0297904911936595 Loss: 0.01729065393782464\n",
      "Iteration: 3517 lambda_n: 1.0433300138327657 Loss: 0.01727913579780796\n",
      "Iteration: 3518 lambda_n: 1.0390926686777018 Loss: 0.017267466248436822\n",
      "Iteration: 3519 lambda_n: 0.9923480292385802 Loss: 0.017255844122808242\n",
      "Iteration: 3520 lambda_n: 0.9174253651079249 Loss: 0.017244744858143545\n",
      "Iteration: 3521 lambda_n: 0.9182574510438215 Loss: 0.017234483616736785\n",
      "Iteration: 3522 lambda_n: 0.9841301577468325 Loss: 0.017224213091086113\n",
      "Iteration: 3523 lambda_n: 1.0030836766116293 Loss: 0.017213205816412627\n",
      "Iteration: 3524 lambda_n: 1.035694688514926 Loss: 0.01720198657698351\n",
      "Iteration: 3525 lambda_n: 0.9859359877250584 Loss: 0.01719040261887734\n",
      "Iteration: 3526 lambda_n: 0.9839736828251158 Loss: 0.017179375224753858\n",
      "Iteration: 3527 lambda_n: 1.001914928633521 Loss: 0.017168369803640338\n",
      "Iteration: 3528 lambda_n: 0.9313240556086279 Loss: 0.017157163741108372\n",
      "Iteration: 3529 lambda_n: 1.0082197021423067 Loss: 0.01714674723641855\n",
      "Iteration: 3530 lambda_n: 0.9261870554816032 Loss: 0.0171354707072485\n",
      "Iteration: 3531 lambda_n: 0.9176152505208509 Loss: 0.01712511170379169\n",
      "Iteration: 3532 lambda_n: 0.9093839339626343 Loss: 0.01711484859380145\n",
      "Iteration: 3533 lambda_n: 0.9250050479167524 Loss: 0.017104677568372275\n",
      "Iteration: 3534 lambda_n: 0.9641441710805947 Loss: 0.01709433184932278\n",
      "Iteration: 3535 lambda_n: 1.0373742441668141 Loss: 0.017083548400992514\n",
      "Iteration: 3536 lambda_n: 0.9064911540120595 Loss: 0.017071945937415952\n",
      "Iteration: 3537 lambda_n: 1.023100151884387 Loss: 0.017061807352839794\n",
      "Iteration: 3538 lambda_n: 1.0395304456734493 Loss: 0.017050364586152587\n",
      "Iteration: 3539 lambda_n: 0.8941285945935419 Loss: 0.01703873808238484\n",
      "Iteration: 3540 lambda_n: 1.0299435821068366 Loss: 0.01702873783078813\n",
      "Iteration: 3541 lambda_n: 0.9226132282185766 Loss: 0.0170172185984079\n",
      "Iteration: 3542 lambda_n: 0.9798598965978279 Loss: 0.01700689980733252\n",
      "Iteration: 3543 lambda_n: 1.0077647699453214 Loss: 0.016995940773464478\n",
      "Iteration: 3544 lambda_n: 0.9516919852020231 Loss: 0.01698466966698715\n",
      "Iteration: 3545 lambda_n: 1.037410591080139 Loss: 0.016974025715989646\n",
      "Iteration: 3546 lambda_n: 0.9229837098620233 Loss: 0.016962423090817507\n",
      "Iteration: 3547 lambda_n: 0.9571115577925715 Loss: 0.016952100263133534\n",
      "Iteration: 3548 lambda_n: 0.9729903701269867 Loss: 0.01694139576358765\n",
      "Iteration: 3549 lambda_n: 0.9232480741510297 Loss: 0.01693051369425804\n",
      "Iteration: 3550 lambda_n: 0.9147998462405323 Loss: 0.016920187970882312\n",
      "Iteration: 3551 lambda_n: 0.9824777090265284 Loss: 0.01690995675291812\n",
      "Iteration: 3552 lambda_n: 0.9139318972638764 Loss: 0.01689896863909848\n",
      "Iteration: 3553 lambda_n: 0.96027106867667 Loss: 0.01688874716781592\n",
      "Iteration: 3554 lambda_n: 1.0251727059342728 Loss: 0.016878007456165633\n",
      "Iteration: 3555 lambda_n: 0.9867689703840372 Loss: 0.016866541904013762\n",
      "Iteration: 3556 lambda_n: 0.9015950389016754 Loss: 0.016855505882618655\n",
      "Iteration: 3557 lambda_n: 0.9282673588011635 Loss: 0.0168454224660052\n",
      "Iteration: 3558 lambda_n: 0.942065164058421 Loss: 0.016835040765325295\n",
      "Iteration: 3559 lambda_n: 0.9955274441319759 Loss: 0.016824504769843335\n",
      "Iteration: 3560 lambda_n: 1.012134367022392 Loss: 0.016813370876226367\n",
      "Iteration: 3561 lambda_n: 1.0246603736671354 Loss: 0.016802051274147106\n",
      "Iteration: 3562 lambda_n: 1.0411101996728551 Loss: 0.01679059160501505\n",
      "Iteration: 3563 lambda_n: 0.923853013577835 Loss: 0.01677894798613288\n",
      "Iteration: 3564 lambda_n: 0.8958771129611598 Loss: 0.016768615774472577\n",
      "Iteration: 3565 lambda_n: 1.0221214858938288 Loss: 0.016758596458054116\n",
      "Iteration: 3566 lambda_n: 1.0385262143256457 Loss: 0.016747165268382862\n",
      "Iteration: 3567 lambda_n: 1.045633742161946 Loss: 0.01673555063409827\n",
      "Iteration: 3568 lambda_n: 1.0093063552044923 Loss: 0.01672385653365926\n",
      "Iteration: 3569 lambda_n: 0.9104360891244161 Loss: 0.01671256873143773\n",
      "Iteration: 3570 lambda_n: 0.9054264605021712 Loss: 0.01670238668589343\n",
      "Iteration: 3571 lambda_n: 1.0355863357676722 Loss: 0.01669226068354143\n",
      "Iteration: 3572 lambda_n: 0.898967670837749 Loss: 0.016680679033670768\n",
      "Iteration: 3573 lambda_n: 1.0483847070842038 Loss: 0.016670625300089108\n",
      "Iteration: 3574 lambda_n: 0.977345551227184 Loss: 0.016658900558840815\n",
      "Iteration: 3575 lambda_n: 1.0342107288686668 Loss: 0.01664797031356633\n",
      "Iteration: 3576 lambda_n: 1.0369293754797906 Loss: 0.0166364041309938\n",
      "Iteration: 3577 lambda_n: 0.9222546052213683 Loss: 0.016624807565642948\n",
      "Iteration: 3578 lambda_n: 1.0338326326559517 Loss: 0.01661449349185606\n",
      "Iteration: 3579 lambda_n: 0.9024435415354167 Loss: 0.016602931599275516\n",
      "Iteration: 3580 lambda_n: 0.8952901833461996 Loss: 0.016592839118162377\n",
      "Iteration: 3581 lambda_n: 0.9668318344381378 Loss: 0.016582826652460095\n",
      "Iteration: 3582 lambda_n: 1.033110833065776 Loss: 0.016572014118511107\n",
      "Iteration: 3583 lambda_n: 1.0233386531959594 Loss: 0.016560460374771924\n",
      "Iteration: 3584 lambda_n: 1.0368467881502827 Loss: 0.016549015938088003\n",
      "Iteration: 3585 lambda_n: 1.0008075233498852 Loss: 0.016537420454441933\n",
      "Iteration: 3586 lambda_n: 1.0412137382214917 Loss: 0.01652622803247016\n",
      "Iteration: 3587 lambda_n: 0.9130768277936198 Loss: 0.016514583751733973\n",
      "Iteration: 3588 lambda_n: 1.02692898682487 Loss: 0.016504372491667293\n",
      "Iteration: 3589 lambda_n: 0.8953361706229822 Loss: 0.0164928880003351\n",
      "Iteration: 3590 lambda_n: 0.9754719345584124 Loss: 0.01648287517274854\n",
      "Iteration: 3591 lambda_n: 0.905312825035245 Loss: 0.016471966177758635\n",
      "Iteration: 3592 lambda_n: 0.9444131449534965 Loss: 0.01646184180940935\n",
      "Iteration: 3593 lambda_n: 0.8976061798042927 Loss: 0.01645128018685825\n",
      "Iteration: 3594 lambda_n: 1.0041065856371363 Loss: 0.01644124203448691\n",
      "Iteration: 3595 lambda_n: 0.9274593195949137 Loss: 0.016430012878057093\n",
      "Iteration: 3596 lambda_n: 0.9206192373651351 Loss: 0.016419640902587136\n",
      "Iteration: 3597 lambda_n: 0.9439284588050089 Loss: 0.016409345436569862\n",
      "Iteration: 3598 lambda_n: 0.9546632986204667 Loss: 0.016398789314492103\n",
      "Iteration: 3599 lambda_n: 1.0326842788293604 Loss: 0.016388113158808407\n",
      "Iteration: 3600 lambda_n: 0.9643590573092697 Loss: 0.016376564499213778\n",
      "Iteration: 3601 lambda_n: 1.030623444707195 Loss: 0.016365779948129634\n",
      "Iteration: 3602 lambda_n: 0.9564549363964621 Loss: 0.016354254371312788\n",
      "Iteration: 3603 lambda_n: 1.0301589006123955 Loss: 0.016343558246370186\n",
      "Iteration: 3604 lambda_n: 0.9897665481182566 Loss: 0.016332037900185736\n",
      "Iteration: 3605 lambda_n: 1.0136163421510493 Loss: 0.016320969282366187\n",
      "Iteration: 3606 lambda_n: 0.954238151785741 Loss: 0.016309633968092877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3607 lambda_n: 0.9910511613950843 Loss: 0.01629896269912145\n",
      "Iteration: 3608 lambda_n: 0.9839873691117927 Loss: 0.016287879765389254\n",
      "Iteration: 3609 lambda_n: 0.9135681038733005 Loss: 0.01627687584256291\n",
      "Iteration: 3610 lambda_n: 0.9872385653738632 Loss: 0.016266659432900378\n",
      "Iteration: 3611 lambda_n: 1.0312634928564741 Loss: 0.01625561918333355\n",
      "Iteration: 3612 lambda_n: 0.9135745685223782 Loss: 0.01624408662166832\n",
      "Iteration: 3613 lambda_n: 0.9667428325944165 Loss: 0.01623387018419479\n",
      "Iteration: 3614 lambda_n: 1.0458753617324537 Loss: 0.016223059184430823\n",
      "Iteration: 3615 lambda_n: 0.8969038400202256 Loss: 0.016211363269109536\n",
      "Iteration: 3616 lambda_n: 1.0355070740954622 Loss: 0.01620133330206809\n",
      "Iteration: 3617 lambda_n: 0.9005531696453315 Loss: 0.016189753366862673\n",
      "Iteration: 3618 lambda_n: 0.927416840247586 Loss: 0.01617968261788022\n",
      "Iteration: 3619 lambda_n: 0.9885049311110036 Loss: 0.01616931146992554\n",
      "Iteration: 3620 lambda_n: 1.0050231168367934 Loss: 0.016158257198631985\n",
      "Iteration: 3621 lambda_n: 1.0263647069426518 Loss: 0.01614701822320666\n",
      "Iteration: 3622 lambda_n: 0.9288295579748659 Loss: 0.016135540605243304\n",
      "Iteration: 3623 lambda_n: 0.9898502858435583 Loss: 0.016125153717025175\n",
      "Iteration: 3624 lambda_n: 0.9543064802134136 Loss: 0.016114084462314576\n",
      "Iteration: 3625 lambda_n: 1.041108952818701 Loss: 0.01610341269997807\n",
      "Iteration: 3626 lambda_n: 1.0407904519517575 Loss: 0.01609177026338718\n",
      "Iteration: 3627 lambda_n: 0.9972426718506965 Loss: 0.016080131405134125\n",
      "Iteration: 3628 lambda_n: 0.9430787909394384 Loss: 0.016068979544935608\n",
      "Iteration: 3629 lambda_n: 0.9520955915268522 Loss: 0.016058433397144863\n",
      "Iteration: 3630 lambda_n: 1.0469791116019491 Loss: 0.016047786430915614\n",
      "Iteration: 3631 lambda_n: 0.9335067077636099 Loss: 0.016036078429017435\n",
      "Iteration: 3632 lambda_n: 0.9326963586989561 Loss: 0.01602563936399777\n",
      "Iteration: 3633 lambda_n: 0.9424272000593825 Loss: 0.016015209373749072\n",
      "Iteration: 3634 lambda_n: 0.9846615164295894 Loss: 0.01600467058017625\n",
      "Iteration: 3635 lambda_n: 1.0166695927995462 Loss: 0.0159936595104612\n",
      "Iteration: 3636 lambda_n: 1.0251939381417907 Loss: 0.01598229052207661\n",
      "Iteration: 3637 lambda_n: 1.007192521170999 Loss: 0.015970826224690565\n",
      "Iteration: 3638 lambda_n: 0.9407748359929139 Loss: 0.01595956324425434\n",
      "Iteration: 3639 lambda_n: 0.9776696464861722 Loss: 0.015949042996520425\n",
      "Iteration: 3640 lambda_n: 0.9679924463121093 Loss: 0.015938110184433148\n",
      "Iteration: 3641 lambda_n: 0.9203715126862454 Loss: 0.015927285601323518\n",
      "Iteration: 3642 lambda_n: 1.0164979456755192 Loss: 0.015916993552296314\n",
      "Iteration: 3643 lambda_n: 0.9982715718165364 Loss: 0.015905626583179962\n",
      "Iteration: 3644 lambda_n: 0.9828849176952512 Loss: 0.015894463444373837\n",
      "Iteration: 3645 lambda_n: 0.8941723654030417 Loss: 0.015883472379999406\n",
      "Iteration: 3646 lambda_n: 0.9954202286326501 Loss: 0.015873473351757813\n",
      "Iteration: 3647 lambda_n: 0.9348652966472092 Loss: 0.01586234213770448\n",
      "Iteration: 3648 lambda_n: 0.9713452629024759 Loss: 0.01585188808753261\n",
      "Iteration: 3649 lambda_n: 0.9814756138098162 Loss: 0.01584102611569726\n",
      "Iteration: 3650 lambda_n: 0.9855913579136067 Loss: 0.015830050875170365\n",
      "Iteration: 3651 lambda_n: 0.9045900966252197 Loss: 0.015819029623869718\n",
      "Iteration: 3652 lambda_n: 0.9398130191391941 Loss: 0.01580891417096083\n",
      "Iteration: 3653 lambda_n: 0.987884566348624 Loss: 0.01579840485398689\n",
      "Iteration: 3654 lambda_n: 1.029140643087072 Loss: 0.015787357996556887\n",
      "Iteration: 3655 lambda_n: 0.9449440936867949 Loss: 0.015775849813259836\n",
      "Iteration: 3656 lambda_n: 0.9076641941317434 Loss: 0.015765283155794174\n",
      "Iteration: 3657 lambda_n: 0.9903048388173334 Loss: 0.01575513338493831\n",
      "Iteration: 3658 lambda_n: 0.995781852413851 Loss: 0.015744059513533328\n",
      "Iteration: 3659 lambda_n: 1.0478168159131362 Loss: 0.01573292440938016\n",
      "Iteration: 3660 lambda_n: 1.0315801448572797 Loss: 0.01572120754632001\n",
      "Iteration: 3661 lambda_n: 0.922499739984526 Loss: 0.0157096722116265\n",
      "Iteration: 3662 lambda_n: 1.0493910897893208 Loss: 0.015699356625331493\n",
      "Iteration: 3663 lambda_n: 0.9337786199478927 Loss: 0.015687622113206488\n",
      "Iteration: 3664 lambda_n: 0.94138977807602 Loss: 0.01567718040922952\n",
      "Iteration: 3665 lambda_n: 0.9041555918483429 Loss: 0.01566665360350791\n",
      "Iteration: 3666 lambda_n: 1.0350490502266172 Loss: 0.01565654316673199\n",
      "Iteration: 3667 lambda_n: 0.9473155816636124 Loss: 0.01564496906556417\n",
      "Iteration: 3668 lambda_n: 0.9483032563927907 Loss: 0.015634376027214618\n",
      "Iteration: 3669 lambda_n: 0.9868692457248605 Loss: 0.015623771955481754\n",
      "Iteration: 3670 lambda_n: 0.9245672805915132 Loss: 0.015612736644464935\n",
      "Iteration: 3671 lambda_n: 1.000133744934029 Loss: 0.015602398014014106\n",
      "Iteration: 3672 lambda_n: 0.9510635290140037 Loss: 0.01559121440094564\n",
      "Iteration: 3673 lambda_n: 0.956995135070601 Loss: 0.0155805795083858\n",
      "Iteration: 3674 lambda_n: 1.045850497054627 Loss: 0.015569878298999461\n",
      "Iteration: 3675 lambda_n: 1.034180715449883 Loss: 0.01555818351265608\n",
      "Iteration: 3676 lambda_n: 1.0141063421392835 Loss: 0.015546619231693974\n",
      "Iteration: 3677 lambda_n: 1.0004571446854866 Loss: 0.015535279436217997\n",
      "Iteration: 3678 lambda_n: 0.9999530631904223 Loss: 0.015524092278808233\n",
      "Iteration: 3679 lambda_n: 0.9064192887336787 Loss: 0.015512910769772717\n",
      "Iteration: 3680 lambda_n: 0.9937459562447619 Loss: 0.01550277516911814\n",
      "Iteration: 3681 lambda_n: 0.9314229331553919 Loss: 0.015491663090141699\n",
      "Iteration: 3682 lambda_n: 1.004176281360604 Loss: 0.015481247918590605\n",
      "Iteration: 3683 lambda_n: 0.91131919666529 Loss: 0.015470019229773492\n",
      "Iteration: 3684 lambda_n: 0.9666080064529208 Loss: 0.01545982887833128\n",
      "Iteration: 3685 lambda_n: 0.9842814444852032 Loss: 0.015449020298586294\n",
      "Iteration: 3686 lambda_n: 1.0493479731192101 Loss: 0.015438014105733536\n",
      "Iteration: 3687 lambda_n: 0.9149891633919512 Loss: 0.015426280353315976\n",
      "Iteration: 3688 lambda_n: 0.9858353043781745 Loss: 0.015416049004552316\n",
      "Iteration: 3689 lambda_n: 1.0354190071693041 Loss: 0.015405025468879044\n",
      "Iteration: 3690 lambda_n: 0.9942692183119864 Loss: 0.015393447503245334\n",
      "Iteration: 3691 lambda_n: 0.9842433387967402 Loss: 0.015382329682268433\n",
      "Iteration: 3692 lambda_n: 0.9939270796360893 Loss: 0.015371323980359121\n",
      "Iteration: 3693 lambda_n: 0.9191849588238297 Loss: 0.015360210006517942\n",
      "Iteration: 3694 lambda_n: 0.8993715735672013 Loss: 0.015349931799991725\n",
      "Iteration: 3695 lambda_n: 0.9017901115444473 Loss: 0.015339875153025612\n",
      "Iteration: 3696 lambda_n: 1.033513636131906 Loss: 0.015329791470961687\n",
      "Iteration: 3697 lambda_n: 0.975678794684505 Loss: 0.01531823488605753\n",
      "Iteration: 3698 lambda_n: 1.0457120176987844 Loss: 0.015307325011797566\n",
      "Iteration: 3699 lambda_n: 0.9337602119863179 Loss: 0.01529563204867702\n",
      "Iteration: 3700 lambda_n: 1.0077820284298342 Loss: 0.015285190920657783\n",
      "Iteration: 3701 lambda_n: 0.9123154428624317 Loss: 0.015273922104632555\n",
      "Iteration: 3702 lambda_n: 1.0451958870410416 Loss: 0.015263720786282227\n",
      "Iteration: 3703 lambda_n: 1.018206252568011 Loss: 0.015252033636714036\n",
      "Iteration: 3704 lambda_n: 0.9616976408551545 Loss: 0.015240648290170088\n",
      "Iteration: 3705 lambda_n: 0.9132961428733859 Loss: 0.01522989481980296\n",
      "Iteration: 3706 lambda_n: 0.9035096185275684 Loss: 0.015219682572166538\n",
      "Iteration: 3707 lambda_n: 0.9749134335986096 Loss: 0.015209579763290682\n",
      "Iteration: 3708 lambda_n: 0.930929806802823 Loss: 0.015198678544404999\n",
      "Iteration: 3709 lambda_n: 1.031125789953121 Loss: 0.015188269147611063\n",
      "Iteration: 3710 lambda_n: 0.993967979459053 Loss: 0.01517673939686943\n",
      "Iteration: 3711 lambda_n: 0.977270298247689 Loss: 0.015165625144144169\n",
      "Iteration: 3712 lambda_n: 1.019719084428435 Loss: 0.015154697609427881\n",
      "Iteration: 3713 lambda_n: 1.0418218135265056 Loss: 0.015143295435216642\n",
      "Iteration: 3714 lambda_n: 0.9066689170854311 Loss: 0.01513164612563651\n",
      "Iteration: 3715 lambda_n: 1.0283279242479832 Loss: 0.015121508060503833\n",
      "Iteration: 3716 lambda_n: 0.9414423898386117 Loss: 0.01511000965445586\n",
      "Iteration: 3717 lambda_n: 0.9346271092531967 Loss: 0.015099482781553599\n",
      "Iteration: 3718 lambda_n: 1.0462702699499415 Loss: 0.015089032123044528\n",
      "Iteration: 3719 lambda_n: 0.9436955716685305 Loss: 0.015077333120784865\n",
      "Iteration: 3720 lambda_n: 1.0062297234818864 Loss: 0.015066781079662962\n",
      "Iteration: 3721 lambda_n: 0.9643458781455978 Loss: 0.015055529814570656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3722 lambda_n: 1.0179966139657581 Loss: 0.015044746887191608\n",
      "Iteration: 3723 lambda_n: 1.0372548526731615 Loss: 0.015033364067963743\n",
      "Iteration: 3724 lambda_n: 0.9288232017865172 Loss: 0.015021765920751013\n",
      "Iteration: 3725 lambda_n: 0.921191621094836 Loss: 0.015011380219438318\n",
      "Iteration: 3726 lambda_n: 0.9233625051119352 Loss: 0.015001079858961566\n",
      "Iteration: 3727 lambda_n: 0.9783331717038857 Loss: 0.014990755232312504\n",
      "Iteration: 3728 lambda_n: 0.9975828232607895 Loss: 0.014979815956386762\n",
      "Iteration: 3729 lambda_n: 1.0192603641957247 Loss: 0.014968661448371777\n",
      "Iteration: 3730 lambda_n: 0.9380369850644089 Loss: 0.014957264561221921\n",
      "Iteration: 3731 lambda_n: 0.9508199496498413 Loss: 0.014946775883945447\n",
      "Iteration: 3732 lambda_n: 1.0083669506670485 Loss: 0.01493614428158662\n",
      "Iteration: 3733 lambda_n: 0.9677307109178444 Loss: 0.01492486922529636\n",
      "Iteration: 3734 lambda_n: 0.9049440590970749 Loss: 0.014914048551686543\n",
      "Iteration: 3735 lambda_n: 0.9997155606117568 Loss: 0.014903929934149013\n",
      "Iteration: 3736 lambda_n: 0.9124668882604914 Loss: 0.014892751638392288\n",
      "Iteration: 3737 lambda_n: 1.023864260617822 Loss: 0.014882548919426664\n",
      "Iteration: 3738 lambda_n: 0.9148530492964109 Loss: 0.014871100622331756\n",
      "Iteration: 3739 lambda_n: 0.9074719709799035 Loss: 0.014860871237680448\n",
      "Iteration: 3740 lambda_n: 0.9377466173542217 Loss: 0.014850724391214891\n",
      "Iteration: 3741 lambda_n: 0.9424283710878324 Loss: 0.014840239037664175\n",
      "Iteration: 3742 lambda_n: 1.0296410335419424 Loss: 0.014829701342783276\n",
      "Iteration: 3743 lambda_n: 0.9039858760101956 Loss: 0.014818188493887352\n",
      "Iteration: 3744 lambda_n: 0.9994313942424816 Loss: 0.014808080655801808\n",
      "Iteration: 3745 lambda_n: 0.9987523673043389 Loss: 0.014796905609543963\n",
      "Iteration: 3746 lambda_n: 0.9806980971558215 Loss: 0.014785738163945282\n",
      "Iteration: 3747 lambda_n: 1.0471236708571203 Loss: 0.014774772598275962\n",
      "Iteration: 3748 lambda_n: 1.0393817784292032 Loss: 0.01476306431084102\n",
      "Iteration: 3749 lambda_n: 0.9958855965248397 Loss: 0.014751442597225946\n",
      "Iteration: 3750 lambda_n: 0.9908752962507168 Loss: 0.014740307238875372\n",
      "Iteration: 3751 lambda_n: 1.0439101799785397 Loss: 0.014729227910382042\n",
      "Iteration: 3752 lambda_n: 0.917946825655415 Loss: 0.014717555588230062\n",
      "Iteration: 3753 lambda_n: 0.9125641128447205 Loss: 0.014707291713636583\n",
      "Iteration: 3754 lambda_n: 0.9462760191392441 Loss: 0.014697088031551956\n",
      "Iteration: 3755 lambda_n: 1.031205282015077 Loss: 0.014686507412200784\n",
      "Iteration: 3756 lambda_n: 1.0221422296236455 Loss: 0.014674977178771816\n",
      "Iteration: 3757 lambda_n: 0.9081390195689318 Loss: 0.014663548290353004\n",
      "Iteration: 3758 lambda_n: 1.027139374441744 Loss: 0.014653394114220713\n",
      "Iteration: 3759 lambda_n: 1.0104042082499451 Loss: 0.014641909366377187\n",
      "Iteration: 3760 lambda_n: 1.0076034457447318 Loss: 0.014630611747247849\n",
      "Iteration: 3761 lambda_n: 0.9822156241926002 Loss: 0.014619345451946169\n",
      "Iteration: 3762 lambda_n: 0.9763081327205043 Loss: 0.014608363032405114\n",
      "Iteration: 3763 lambda_n: 0.9926873638497415 Loss: 0.01459744667330264\n",
      "Iteration: 3764 lambda_n: 0.9103178038427603 Loss: 0.014586347180901198\n",
      "Iteration: 3765 lambda_n: 0.9540137410769839 Loss: 0.014576168690395547\n",
      "Iteration: 3766 lambda_n: 0.9847634846772009 Loss: 0.014565501631105104\n",
      "Iteration: 3767 lambda_n: 0.9231186762270186 Loss: 0.01455449075834585\n",
      "Iteration: 3768 lambda_n: 0.9216730233290806 Loss: 0.014544169157351286\n",
      "Iteration: 3769 lambda_n: 0.9971260233235064 Loss: 0.01453386372669852\n",
      "Iteration: 3770 lambda_n: 0.9852114387547061 Loss: 0.014522714645932097\n",
      "Iteration: 3771 lambda_n: 1.0035255825905227 Loss: 0.014511698791749785\n",
      "Iteration: 3772 lambda_n: 0.9484064672471092 Loss: 0.014500478170368399\n",
      "Iteration: 3773 lambda_n: 0.9010116705651329 Loss: 0.014489873853657584\n",
      "Iteration: 3774 lambda_n: 1.0403816652401916 Loss: 0.014479799473396022\n",
      "Iteration: 3775 lambda_n: 0.9438590273717109 Loss: 0.014468166777701354\n",
      "Iteration: 3776 lambda_n: 1.022962079271244 Loss: 0.014457613325952865\n",
      "Iteration: 3777 lambda_n: 1.0240354529829128 Loss: 0.014446175415949449\n",
      "Iteration: 3778 lambda_n: 0.9284776703038373 Loss: 0.014434725511617662\n",
      "Iteration: 3779 lambda_n: 1.0387505790440306 Loss: 0.014424344060688404\n",
      "Iteration: 3780 lambda_n: 1.0346893381669573 Loss: 0.014412729638084158\n",
      "Iteration: 3781 lambda_n: 1.0486388577487498 Loss: 0.014401160632121452\n",
      "Iteration: 3782 lambda_n: 1.0460461063732294 Loss: 0.014389435661972112\n",
      "Iteration: 3783 lambda_n: 1.0294408147541336 Loss: 0.014377739689098234\n",
      "Iteration: 3784 lambda_n: 1.033832279455605 Loss: 0.014366229389277362\n",
      "Iteration: 3785 lambda_n: 0.9073581008488232 Loss: 0.01435466999504663\n",
      "Iteration: 3786 lambda_n: 0.9650293623811681 Loss: 0.014344524728941873\n",
      "Iteration: 3787 lambda_n: 1.0407465235190876 Loss: 0.0143337346401706\n",
      "Iteration: 3788 lambda_n: 0.955942282644741 Loss: 0.014322097956902122\n",
      "Iteration: 3789 lambda_n: 0.9394823199213185 Loss: 0.014311409484181283\n",
      "Iteration: 3790 lambda_n: 1.0233107885496926 Loss: 0.01430090505752971\n",
      "Iteration: 3791 lambda_n: 1.0459116040251928 Loss: 0.014289463344283746\n",
      "Iteration: 3792 lambda_n: 0.9646497079861566 Loss: 0.014277768936513151\n",
      "Iteration: 3793 lambda_n: 1.015499814327551 Loss: 0.01426698312987837\n",
      "Iteration: 3794 lambda_n: 0.958301825763759 Loss: 0.014255628771339547\n",
      "Iteration: 3795 lambda_n: 1.02039587016874 Loss: 0.014244913952747613\n",
      "Iteration: 3796 lambda_n: 1.0340223464824858 Loss: 0.014233504863800506\n",
      "Iteration: 3797 lambda_n: 1.013409746085398 Loss: 0.01422194342322913\n",
      "Iteration: 3798 lambda_n: 1.0113296431234982 Loss: 0.01421061245937141\n",
      "Iteration: 3799 lambda_n: 0.9777032329132723 Loss: 0.014199304759523964\n",
      "Iteration: 3800 lambda_n: 0.9860586936963955 Loss: 0.014188373043399798\n",
      "Iteration: 3801 lambda_n: 0.9812390887520952 Loss: 0.014177347910608014\n",
      "Iteration: 3802 lambda_n: 1.0447194277801757 Loss: 0.014166376671739115\n",
      "Iteration: 3803 lambda_n: 0.9094326815205032 Loss: 0.014154695665085192\n",
      "Iteration: 3804 lambda_n: 0.915721175843387 Loss: 0.014144527305078298\n",
      "Iteration: 3805 lambda_n: 0.8959871028218469 Loss: 0.014134288638432468\n",
      "Iteration: 3806 lambda_n: 1.0084221792983443 Loss: 0.014124270623073795\n",
      "Iteration: 3807 lambda_n: 0.9409539578241009 Loss: 0.014112995478525731\n",
      "Iteration: 3808 lambda_n: 0.9004468497305863 Loss: 0.014102474700145686\n",
      "Iteration: 3809 lambda_n: 0.8941299867663421 Loss: 0.014092406835497567\n",
      "Iteration: 3810 lambda_n: 0.9837106936826666 Loss: 0.01408240960415627\n",
      "Iteration: 3811 lambda_n: 0.9831486522150852 Loss: 0.014071410779633397\n",
      "Iteration: 3812 lambda_n: 0.9316333350836233 Loss: 0.014060418244842058\n",
      "Iteration: 3813 lambda_n: 0.9181072102803649 Loss: 0.01405000170542482\n",
      "Iteration: 3814 lambda_n: 1.0006009083446779 Loss: 0.014039736405715974\n",
      "Iteration: 3815 lambda_n: 1.0274954165514478 Loss: 0.014028548754300919\n",
      "Iteration: 3816 lambda_n: 0.9468750110174742 Loss: 0.014017060403000773\n",
      "Iteration: 3817 lambda_n: 0.9576185897247282 Loss: 0.01400647346803379\n",
      "Iteration: 3818 lambda_n: 0.9556548290884146 Loss: 0.01399576641501919\n",
      "Iteration: 3819 lambda_n: 0.8945660173522941 Loss: 0.013985081323728261\n",
      "Iteration: 3820 lambda_n: 1.0319082752879158 Loss: 0.01397507926570705\n",
      "Iteration: 3821 lambda_n: 0.9959989627344005 Loss: 0.013963541601963465\n",
      "Iteration: 3822 lambda_n: 0.9392801732717815 Loss: 0.013952405442290147\n",
      "Iteration: 3823 lambda_n: 0.9569117444390202 Loss: 0.013941903454524654\n",
      "Iteration: 3824 lambda_n: 0.892471375251626 Loss: 0.013931204334972169\n",
      "Iteration: 3825 lambda_n: 0.9429128568900922 Loss: 0.01392122572033041\n",
      "Iteration: 3826 lambda_n: 0.9262587931395988 Loss: 0.01391068313007978\n",
      "Iteration: 3827 lambda_n: 0.9511507859140809 Loss: 0.013900326751476951\n",
      "Iteration: 3828 lambda_n: 0.9069084484244847 Loss: 0.013889692063422575\n",
      "Iteration: 3829 lambda_n: 0.9513796433981964 Loss: 0.01387955204749717\n",
      "Iteration: 3830 lambda_n: 0.8999134651571197 Loss: 0.013868914809915811\n",
      "Iteration: 3831 lambda_n: 1.0412207308493846 Loss: 0.013858853012665751\n",
      "Iteration: 3832 lambda_n: 0.9193007902923854 Loss: 0.013847211284974058\n",
      "Iteration: 3833 lambda_n: 0.9773773963336996 Loss: 0.013836932730197081\n",
      "Iteration: 3834 lambda_n: 0.9841198114569685 Loss: 0.013826004834814368\n",
      "Iteration: 3835 lambda_n: 0.9333269896133807 Loss: 0.01381500155851873\n",
      "Iteration: 3836 lambda_n: 0.9609823498737868 Loss: 0.013804566192799928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3837 lambda_n: 0.9611411073743412 Loss: 0.013793821621875592\n",
      "Iteration: 3838 lambda_n: 0.956933123614692 Loss: 0.013783075280561009\n",
      "Iteration: 3839 lambda_n: 0.9189171788401851 Loss: 0.013772375992541316\n",
      "Iteration: 3840 lambda_n: 0.90010431845524 Loss: 0.01376210175798012\n",
      "Iteration: 3841 lambda_n: 0.993084853493508 Loss: 0.013752037870456691\n",
      "Iteration: 3842 lambda_n: 0.8958377933386523 Loss: 0.013740934390454507\n",
      "Iteration: 3843 lambda_n: 1.005043613409961 Loss: 0.013730918214441135\n",
      "Iteration: 3844 lambda_n: 0.9429278932145214 Loss: 0.013719681035264475\n",
      "Iteration: 3845 lambda_n: 0.9075379090042207 Loss: 0.013709138363364536\n",
      "Iteration: 3846 lambda_n: 1.0446087107445037 Loss: 0.013698991383321816\n",
      "Iteration: 3847 lambda_n: 0.9747846934511624 Loss: 0.013687311849606488\n",
      "Iteration: 3848 lambda_n: 0.9449190922087655 Loss: 0.013676413007303937\n",
      "Iteration: 3849 lambda_n: 0.99970967603112 Loss: 0.013665848089763724\n",
      "Iteration: 3850 lambda_n: 1.047227420428257 Loss: 0.013654670576094045\n",
      "Iteration: 3851 lambda_n: 0.9838694124101655 Loss: 0.013642961782868166\n",
      "Iteration: 3852 lambda_n: 0.9800140632430241 Loss: 0.013631961384829396\n",
      "Iteration: 3853 lambda_n: 0.9251648350172914 Loss: 0.013621004096972818\n",
      "Iteration: 3854 lambda_n: 0.8936708643529762 Loss: 0.01361066006857592\n",
      "Iteration: 3855 lambda_n: 1.0075197545274275 Loss: 0.01360066816991425\n",
      "Iteration: 3856 lambda_n: 0.9158549515263389 Loss: 0.013589403360908442\n",
      "Iteration: 3857 lambda_n: 0.9017115758895317 Loss: 0.013579163435763894\n",
      "Iteration: 3858 lambda_n: 1.0221819014339624 Loss: 0.013569081647600052\n",
      "Iteration: 3859 lambda_n: 0.9091115144465931 Loss: 0.013557652918146642\n",
      "Iteration: 3860 lambda_n: 0.9131921785831557 Loss: 0.013547488401182639\n",
      "Iteration: 3861 lambda_n: 1.0399006452639339 Loss: 0.01353727826318114\n",
      "Iteration: 3862 lambda_n: 0.9940615142339124 Loss: 0.01352565143862799\n",
      "Iteration: 3863 lambda_n: 1.0324916417074026 Loss: 0.013514537132543124\n",
      "Iteration: 3864 lambda_n: 0.949615220656731 Loss: 0.013502993155146132\n",
      "Iteration: 3865 lambda_n: 1.0094357888693397 Loss: 0.013492375798283714\n",
      "Iteration: 3866 lambda_n: 0.9776528679186292 Loss: 0.01348108961014166\n",
      "Iteration: 3867 lambda_n: 0.9979536793335501 Loss: 0.013470158781238872\n",
      "Iteration: 3868 lambda_n: 0.9665697505221236 Loss: 0.013459000979551891\n",
      "Iteration: 3869 lambda_n: 1.0375411735122717 Loss: 0.013448194075686768\n",
      "Iteration: 3870 lambda_n: 1.0306657881958499 Loss: 0.013436593667555513\n",
      "Iteration: 3871 lambda_n: 0.9118242055607404 Loss: 0.013425070135382917\n",
      "Iteration: 3872 lambda_n: 1.0454510226764349 Loss: 0.013414875335466495\n",
      "Iteration: 3873 lambda_n: 0.9765314309052459 Loss: 0.01340318650301915\n",
      "Iteration: 3874 lambda_n: 0.9463198583332904 Loss: 0.013392268241348062\n",
      "Iteration: 3875 lambda_n: 0.9920495646689389 Loss: 0.013381687768694146\n",
      "Iteration: 3876 lambda_n: 0.8923470544145222 Loss: 0.013370596011992687\n",
      "Iteration: 3877 lambda_n: 0.8996383663057368 Loss: 0.013360618997563222\n",
      "Iteration: 3878 lambda_n: 0.9765050799102566 Loss: 0.013350560464846341\n",
      "Iteration: 3879 lambda_n: 0.9700222449537421 Loss: 0.01333964251663946\n",
      "Iteration: 3880 lambda_n: 0.9017272988720315 Loss: 0.01332879705447689\n",
      "Iteration: 3881 lambda_n: 1.0483211949635731 Loss: 0.013318715176515294\n",
      "Iteration: 3882 lambda_n: 0.9744195027496693 Loss: 0.013306994290433256\n",
      "Iteration: 3883 lambda_n: 0.9659855031593272 Loss: 0.013296099675513769\n",
      "Iteration: 3884 lambda_n: 0.9556893007376616 Loss: 0.01328529936166561\n",
      "Iteration: 3885 lambda_n: 0.9302723046099125 Loss: 0.013274614169341972\n",
      "Iteration: 3886 lambda_n: 1.0234452679027297 Loss: 0.013264213158095608\n",
      "Iteration: 3887 lambda_n: 0.9026589969264508 Loss: 0.013252770420047168\n",
      "Iteration: 3888 lambda_n: 0.9232791437009019 Loss: 0.013242678149251041\n",
      "Iteration: 3889 lambda_n: 0.9619596984492206 Loss: 0.013232355336013439\n",
      "Iteration: 3890 lambda_n: 1.0126332911428062 Loss: 0.013221600054433173\n",
      "Iteration: 3891 lambda_n: 0.9766512638281223 Loss: 0.013210278215660249\n",
      "Iteration: 3892 lambda_n: 0.9765599414016817 Loss: 0.01319935868097228\n",
      "Iteration: 3893 lambda_n: 1.0042619399654231 Loss: 0.013188440170919464\n",
      "Iteration: 3894 lambda_n: 1.0250886960298082 Loss: 0.013177211940035108\n",
      "Iteration: 3895 lambda_n: 0.9784411144171967 Loss: 0.013165750857781355\n",
      "Iteration: 3896 lambda_n: 1.0022095594909235 Loss: 0.013154811326089477\n",
      "Iteration: 3897 lambda_n: 0.9252069004004874 Loss: 0.013143606053196201\n",
      "Iteration: 3898 lambda_n: 0.9552941402314937 Loss: 0.013133261717231596\n",
      "Iteration: 3899 lambda_n: 1.0022142097547515 Loss: 0.013122580992150875\n",
      "Iteration: 3900 lambda_n: 1.0246276342643588 Loss: 0.013111375677815697\n",
      "Iteration: 3901 lambda_n: 1.0457410286799647 Loss: 0.013099919772593704\n",
      "Iteration: 3902 lambda_n: 0.9956759045684164 Loss: 0.013088227811763739\n",
      "Iteration: 3903 lambda_n: 1.0242195124610864 Loss: 0.01307709561032541\n",
      "Iteration: 3904 lambda_n: 0.9253085150243243 Loss: 0.01306564427935654\n",
      "Iteration: 3905 lambda_n: 0.9139556259243335 Loss: 0.013055298830462395\n",
      "Iteration: 3906 lambda_n: 0.9680013077105043 Loss: 0.013045080315972265\n",
      "Iteration: 3907 lambda_n: 0.9860673401389428 Loss: 0.013034257544843685\n",
      "Iteration: 3908 lambda_n: 0.9235349068674583 Loss: 0.013023232789154996\n",
      "Iteration: 3909 lambda_n: 0.9606351918895832 Loss: 0.013012907182378745\n",
      "Iteration: 3910 lambda_n: 0.9824104882271073 Loss: 0.013002166777926765\n",
      "Iteration: 3911 lambda_n: 0.930870982682044 Loss: 0.012991182917493526\n",
      "Iteration: 3912 lambda_n: 0.9742095391358919 Loss: 0.012980775298666935\n",
      "Iteration: 3913 lambda_n: 0.9040855830205088 Loss: 0.012969883135448869\n",
      "Iteration: 3914 lambda_n: 1.0402933790149096 Loss: 0.012959774997062683\n",
      "Iteration: 3915 lambda_n: 0.9908723083723124 Loss: 0.012948143989134407\n",
      "Iteration: 3916 lambda_n: 0.9558826002544666 Loss: 0.012937065537299279\n",
      "Iteration: 3917 lambda_n: 0.9089406453332889 Loss: 0.012926378291181713\n",
      "Iteration: 3918 lambda_n: 0.9680952436789672 Loss: 0.012916215882495252\n",
      "Iteration: 3919 lambda_n: 0.949031903276069 Loss: 0.012905392098856171\n",
      "Iteration: 3920 lambda_n: 0.9779549854016479 Loss: 0.012894781455795151\n",
      "Iteration: 3921 lambda_n: 0.9525397007426962 Loss: 0.012883847441480765\n",
      "Iteration: 3922 lambda_n: 0.8968844468209124 Loss: 0.012873197585474259\n",
      "Iteration: 3923 lambda_n: 1.012834830980774 Loss: 0.012863169984958896\n",
      "Iteration: 3924 lambda_n: 1.045636341303022 Loss: 0.012851846006155911\n",
      "Iteration: 3925 lambda_n: 0.9565173818990532 Loss: 0.012840155294122097\n",
      "Iteration: 3926 lambda_n: 0.9092885979446362 Loss: 0.012829460977645637\n",
      "Iteration: 3927 lambda_n: 1.0161274175026827 Loss: 0.012819294704039337\n",
      "Iteration: 3928 lambda_n: 0.9799851003055554 Loss: 0.01280793392514569\n",
      "Iteration: 3929 lambda_n: 1.0362084255884458 Loss: 0.012796977237336861\n",
      "Iteration: 3930 lambda_n: 0.89985385057588 Loss: 0.012785391949832333\n",
      "Iteration: 3931 lambda_n: 0.9319514697432871 Loss: 0.012775331172162176\n",
      "Iteration: 3932 lambda_n: 1.038222532154142 Loss: 0.012764911531000307\n",
      "Iteration: 3933 lambda_n: 1.0440618238474837 Loss: 0.012753303733996642\n",
      "Iteration: 3934 lambda_n: 0.9290426194140402 Loss: 0.012741630654366422\n",
      "Iteration: 3935 lambda_n: 0.9564027984561531 Loss: 0.01273124354397753\n",
      "Iteration: 3936 lambda_n: 1.0183109344773202 Loss: 0.012720550537266409\n",
      "Iteration: 3937 lambda_n: 0.9275971786484648 Loss: 0.01270916537309683\n",
      "Iteration: 3938 lambda_n: 0.945485740139115 Loss: 0.01269879443142778\n",
      "Iteration: 3939 lambda_n: 1.0124742845202412 Loss: 0.012688223490419932\n",
      "Iteration: 3940 lambda_n: 0.9744342730591062 Loss: 0.012676903591222526\n",
      "Iteration: 3941 lambda_n: 0.9863524226492063 Loss: 0.01266600899864805\n",
      "Iteration: 3942 lambda_n: 1.0223171708823917 Loss: 0.012654981158853574\n",
      "Iteration: 3943 lambda_n: 1.0294006959412374 Loss: 0.012643551220789879\n",
      "Iteration: 3944 lambda_n: 0.9519593520573049 Loss: 0.012632042088952867\n",
      "Iteration: 3945 lambda_n: 0.9387863972459186 Loss: 0.012621398786650433\n",
      "Iteration: 3946 lambda_n: 0.9176028703888104 Loss: 0.012610902766018114\n",
      "Iteration: 3947 lambda_n: 0.9137627893617537 Loss: 0.012600643588427982\n",
      "Iteration: 3948 lambda_n: 1.020373021695918 Loss: 0.012590427346887147\n",
      "Iteration: 3949 lambda_n: 1.0311032386467218 Loss: 0.0125790191619807\n",
      "Iteration: 3950 lambda_n: 0.9678287729630978 Loss: 0.012567491011826136\n",
      "Iteration: 3951 lambda_n: 0.9659051635420466 Loss: 0.012556670298492705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3952 lambda_n: 1.0041965489464206 Loss: 0.012545871094463371\n",
      "Iteration: 3953 lambda_n: 1.0319010030072746 Loss: 0.012534643780173606\n",
      "Iteration: 3954 lambda_n: 1.0346007213646593 Loss: 0.012523106721968541\n",
      "Iteration: 3955 lambda_n: 0.9889693221297273 Loss: 0.012511539482752831\n",
      "Iteration: 3956 lambda_n: 0.9569725514536385 Loss: 0.01250048242312572\n",
      "Iteration: 3957 lambda_n: 0.9980989309983692 Loss: 0.012489783102315738\n",
      "Iteration: 3958 lambda_n: 0.9796028902554085 Loss: 0.01247862397533304\n",
      "Iteration: 3959 lambda_n: 1.0176705134929591 Loss: 0.012467671643743844\n",
      "Iteration: 3960 lambda_n: 1.0036335286251294 Loss: 0.012456293704334417\n",
      "Iteration: 3961 lambda_n: 1.0314565807930944 Loss: 0.012445072706385353\n",
      "Iteration: 3962 lambda_n: 0.9365006561194482 Loss: 0.012433540639021586\n",
      "Iteration: 3963 lambda_n: 0.8929211043329205 Loss: 0.012423070216647727\n",
      "Iteration: 3964 lambda_n: 0.9031330466993196 Loss: 0.012413087031881799\n",
      "Iteration: 3965 lambda_n: 0.9865525814096513 Loss: 0.01240298967593766\n",
      "Iteration: 3966 lambda_n: 0.9705848485735422 Loss: 0.012391959661536275\n",
      "Iteration: 3967 lambda_n: 0.9747429512511193 Loss: 0.012381108174598828\n",
      "Iteration: 3968 lambda_n: 0.9977882877055102 Loss: 0.01237021020097915\n",
      "Iteration: 3969 lambda_n: 0.92808357333677 Loss: 0.012359054574736192\n",
      "Iteration: 3970 lambda_n: 0.8975023371597112 Loss: 0.012348678274195758\n",
      "Iteration: 3971 lambda_n: 1.0286270599641063 Loss: 0.012338643884701895\n",
      "Iteration: 3972 lambda_n: 0.9467521855028193 Loss: 0.012327143477316017\n",
      "Iteration: 3973 lambda_n: 1.002357387542428 Loss: 0.012316558461836338\n",
      "Iteration: 3974 lambda_n: 0.9459466907833846 Loss: 0.012305351763379224\n",
      "Iteration: 3975 lambda_n: 0.9890538377561703 Loss: 0.012294775758124559\n",
      "Iteration: 3976 lambda_n: 0.9374241134771333 Loss: 0.012283717802600977\n",
      "Iteration: 3977 lambda_n: 0.8962411701699657 Loss: 0.012273237087051813\n",
      "Iteration: 3978 lambda_n: 0.9626828075184227 Loss: 0.012263216812622288\n",
      "Iteration: 3979 lambda_n: 1.0457640842288316 Loss: 0.012252453700648931\n",
      "Iteration: 3980 lambda_n: 0.9375808054503828 Loss: 0.012240761714939161\n",
      "Iteration: 3981 lambda_n: 1.0144365609022246 Loss: 0.012230279256117303\n",
      "Iteration: 3982 lambda_n: 1.0072192685144878 Loss: 0.012218937527153924\n",
      "Iteration: 3983 lambda_n: 0.9454034850415504 Loss: 0.012207676492248878\n",
      "Iteration: 3984 lambda_n: 0.9110927309059786 Loss: 0.012197106579875265\n",
      "Iteration: 3985 lambda_n: 0.9210170317311115 Loss: 0.012186920274668158\n",
      "Iteration: 3986 lambda_n: 0.9438569483519555 Loss: 0.012176623014570532\n",
      "Iteration: 3987 lambda_n: 1.034545931065254 Loss: 0.012166070399016037\n",
      "Iteration: 3988 lambda_n: 0.9902855144338213 Loss: 0.012154503854531888\n",
      "Iteration: 3989 lambda_n: 1.0184296474834755 Loss: 0.01214343215756485\n",
      "Iteration: 3990 lambda_n: 0.9988786851412798 Loss: 0.012132045802802575\n",
      "Iteration: 3991 lambda_n: 0.9002700297013387 Loss: 0.0121208780360609\n",
      "Iteration: 3992 lambda_n: 0.982582958382119 Loss: 0.012110812746012284\n",
      "Iteration: 3993 lambda_n: 1.0216525151107103 Loss: 0.012099827174598154\n",
      "Iteration: 3994 lambda_n: 0.9021400926325699 Loss: 0.012088404796072035\n",
      "Iteration: 3995 lambda_n: 0.952795963284613 Loss: 0.012078318603948918\n",
      "Iteration: 3996 lambda_n: 0.9614434126468001 Loss: 0.012067666066142313\n",
      "Iteration: 3997 lambda_n: 0.9802214609975757 Loss: 0.012056916849314257\n",
      "Iteration: 3998 lambda_n: 0.9118159564747913 Loss: 0.012045957690488007\n",
      "Iteration: 3999 lambda_n: 0.9804823326402341 Loss: 0.012035763326869592\n",
      "Iteration: 4000 lambda_n: 1.047630118506266 Loss: 0.0120248012554092\n",
      "Iteration: 4001 lambda_n: 1.0441407673597332 Loss: 0.012013088454787137\n",
      "Iteration: 4002 lambda_n: 0.9120480858938139 Loss: 0.012001414668416453\n",
      "Iteration: 4003 lambda_n: 0.9525803690186364 Loss: 0.011991217717248188\n",
      "Iteration: 4004 lambda_n: 0.9023930515855194 Loss: 0.011980567605707603\n",
      "Iteration: 4005 lambda_n: 0.9040182841584881 Loss: 0.011970478604026709\n",
      "Iteration: 4006 lambda_n: 1.0142515612895708 Loss: 0.011960371433493558\n",
      "Iteration: 4007 lambda_n: 0.9606232838665004 Loss: 0.011949031826805472\n",
      "Iteration: 4008 lambda_n: 0.8952728583002859 Loss: 0.011938291800761766\n",
      "Iteration: 4009 lambda_n: 1.0199400895275832 Loss: 0.011928282411767218\n",
      "Iteration: 4010 lambda_n: 1.0416366192053834 Loss: 0.011916879211781835\n",
      "Iteration: 4011 lambda_n: 0.972754151321213 Loss: 0.011905233441006038\n",
      "Iteration: 4012 lambda_n: 0.9990305400920734 Loss: 0.01189435779633871\n",
      "Iteration: 4013 lambda_n: 0.9809115257021858 Loss: 0.011883188376748875\n",
      "Iteration: 4014 lambda_n: 0.920328431148316 Loss: 0.011872221534372774\n",
      "Iteration: 4015 lambda_n: 0.9183096177664163 Loss: 0.011861932028315541\n",
      "Iteration: 4016 lambda_n: 0.9503978827152159 Loss: 0.011851665094770097\n",
      "Iteration: 4017 lambda_n: 0.9392272348253163 Loss: 0.011841039408025424\n",
      "Iteration: 4018 lambda_n: 1.0060801880576904 Loss: 0.011830538613666959\n",
      "Iteration: 4019 lambda_n: 1.0228369004964661 Loss: 0.011819290388477639\n",
      "Iteration: 4020 lambda_n: 0.9579054087444512 Loss: 0.011807854821087368\n",
      "Iteration: 4021 lambda_n: 0.8960233704766674 Loss: 0.011797145205579496\n",
      "Iteration: 4022 lambda_n: 1.0079933414243611 Loss: 0.011787127447952468\n",
      "Iteration: 4023 lambda_n: 0.9944990034351465 Loss: 0.011775857840730191\n",
      "Iteration: 4024 lambda_n: 0.9738119742043903 Loss: 0.0117647391053363\n",
      "Iteration: 4025 lambda_n: 0.9558693083985196 Loss: 0.011753851657669701\n",
      "Iteration: 4026 lambda_n: 0.9183296064909915 Loss: 0.011743164814980882\n",
      "Iteration: 4027 lambda_n: 0.9615533143695645 Loss: 0.011732897676581765\n",
      "Iteration: 4028 lambda_n: 1.0116593948629027 Loss: 0.011722147288717878\n",
      "Iteration: 4029 lambda_n: 0.9325435575065231 Loss: 0.011710836705106281\n",
      "Iteration: 4030 lambda_n: 0.9215499740188023 Loss: 0.011700410656394102\n",
      "Iteration: 4031 lambda_n: 0.9465686335119545 Loss: 0.011690107520006498\n",
      "Iteration: 4032 lambda_n: 1.0325203902033744 Loss: 0.01167952467094817\n",
      "Iteration: 4033 lambda_n: 0.9196633397816045 Loss: 0.011667980863785718\n",
      "Iteration: 4034 lambda_n: 1.032868872514622 Loss: 0.0116576988252023\n",
      "Iteration: 4035 lambda_n: 1.024297711295377 Loss: 0.011646151125537338\n",
      "Iteration: 4036 lambda_n: 1.0085829918298566 Loss: 0.011634699255205124\n",
      "Iteration: 4037 lambda_n: 0.9005497268306091 Loss: 0.011623423080664433\n",
      "Iteration: 4038 lambda_n: 0.9360586143960637 Loss: 0.011613354742831213\n",
      "Iteration: 4039 lambda_n: 0.9809764652002017 Loss: 0.011602889409540178\n",
      "Iteration: 4040 lambda_n: 0.9814194048447701 Loss: 0.011591921886771729\n",
      "Iteration: 4041 lambda_n: 1.0394983766336692 Loss: 0.01158094941350885\n",
      "Iteration: 4042 lambda_n: 0.8961774261877936 Loss: 0.011569327607003875\n",
      "Iteration: 4043 lambda_n: 0.9042056994881786 Loss: 0.01155930815991317\n",
      "Iteration: 4044 lambda_n: 0.954473296591477 Loss: 0.011549198956461155\n",
      "Iteration: 4045 lambda_n: 0.9510006394721582 Loss: 0.011538527752519562\n",
      "Iteration: 4046 lambda_n: 1.0380753109666612 Loss: 0.011527895375113154\n",
      "Iteration: 4047 lambda_n: 1.0347767481628878 Loss: 0.011516289487112866\n",
      "Iteration: 4048 lambda_n: 0.944015411504685 Loss: 0.011504720479489331\n",
      "Iteration: 4049 lambda_n: 1.0469939355528939 Loss: 0.011494166203084412\n",
      "Iteration: 4050 lambda_n: 0.9341868459653347 Loss: 0.011482460608326072\n",
      "Iteration: 4051 lambda_n: 0.9433603634905068 Loss: 0.01147201622025716\n",
      "Iteration: 4052 lambda_n: 0.9635227504739937 Loss: 0.011461469271939783\n",
      "Iteration: 4053 lambda_n: 0.9346768107246747 Loss: 0.011450696905774573\n",
      "Iteration: 4054 lambda_n: 0.9301332267292461 Loss: 0.011440247044112022\n",
      "Iteration: 4055 lambda_n: 0.9586189960476592 Loss: 0.011429847981964242\n",
      "Iteration: 4056 lambda_n: 0.9954038630890573 Loss: 0.011419130445051218\n",
      "Iteration: 4057 lambda_n: 1.0294456973201318 Loss: 0.011408001648066605\n",
      "Iteration: 4058 lambda_n: 0.939815613129465 Loss: 0.011396492258777802\n",
      "Iteration: 4059 lambda_n: 0.9913813648331331 Loss: 0.01138598495157982\n",
      "Iteration: 4060 lambda_n: 0.994617376242268 Loss: 0.011374901131481322\n",
      "Iteration: 4061 lambda_n: 0.9006722201822454 Loss: 0.011363781133733078\n",
      "Iteration: 4062 lambda_n: 0.9180582493450654 Loss: 0.011353711460790405\n",
      "Iteration: 4063 lambda_n: 0.9489713677545382 Loss: 0.011343447410273358\n",
      "Iteration: 4064 lambda_n: 0.9394149134390704 Loss: 0.011332837747062905\n",
      "Iteration: 4065 lambda_n: 0.9354617916277043 Loss: 0.011322334928014677\n",
      "Iteration: 4066 lambda_n: 1.0424139049517074 Loss: 0.011311876306875003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4067 lambda_n: 1.003511764652961 Loss: 0.011300221944479362\n",
      "Iteration: 4068 lambda_n: 0.8980512350132615 Loss: 0.011289002516110456\n",
      "Iteration: 4069 lambda_n: 0.9740641708710712 Loss: 0.011278962155340995\n",
      "Iteration: 4070 lambda_n: 0.9557111066619168 Loss: 0.011268071958722899\n",
      "Iteration: 4071 lambda_n: 0.9765038520003158 Loss: 0.01125738695373673\n",
      "Iteration: 4072 lambda_n: 0.9283157136319387 Loss: 0.011246469483860383\n",
      "Iteration: 4073 lambda_n: 1.048389265326543 Loss: 0.01123609076643393\n",
      "Iteration: 4074 lambda_n: 1.0046238677445902 Loss: 0.011224369609003635\n",
      "Iteration: 4075 lambda_n: 0.9139490248497125 Loss: 0.011213137757148125\n",
      "Iteration: 4076 lambda_n: 1.0023181357818443 Loss: 0.011202919665528265\n",
      "Iteration: 4077 lambda_n: 1.0042324184596925 Loss: 0.011191713594874746\n",
      "Iteration: 4078 lambda_n: 0.9830086136639841 Loss: 0.011180486123679487\n",
      "Iteration: 4079 lambda_n: 0.9625516536707734 Loss: 0.011169495939247558\n",
      "Iteration: 4080 lambda_n: 1.0131411423150811 Loss: 0.011158734468043083\n",
      "Iteration: 4081 lambda_n: 0.9246510095200479 Loss: 0.011147407400167682\n",
      "Iteration: 4082 lambda_n: 0.9677399319577119 Loss: 0.011137069666374897\n",
      "Iteration: 4083 lambda_n: 0.9718122843319676 Loss: 0.011126250193354253\n",
      "Iteration: 4084 lambda_n: 0.9511765112500745 Loss: 0.011115385192138831\n",
      "Iteration: 4085 lambda_n: 0.9030526450243369 Loss: 0.01110475090310413\n",
      "Iteration: 4086 lambda_n: 0.9555551836994712 Loss: 0.01109465464692795\n",
      "Iteration: 4087 lambda_n: 0.8954598359836967 Loss: 0.011083971406121506\n",
      "Iteration: 4088 lambda_n: 0.9860192259492817 Loss: 0.01107396004087601\n",
      "Iteration: 4089 lambda_n: 1.0442872741760247 Loss: 0.011062936210268211\n",
      "Iteration: 4090 lambda_n: 0.9969101363651369 Loss: 0.011051260936240941\n",
      "Iteration: 4091 lambda_n: 1.0082194831823164 Loss: 0.01104011534645463\n",
      "Iteration: 4092 lambda_n: 0.9913843175311834 Loss: 0.011028843317974421\n",
      "Iteration: 4093 lambda_n: 0.997858686289115 Loss: 0.011017759510209144\n",
      "Iteration: 4094 lambda_n: 0.9241263110864864 Loss: 0.011006603319438833\n",
      "Iteration: 4095 lambda_n: 1.033503527493272 Loss: 0.010996271467477238\n",
      "Iteration: 4096 lambda_n: 1.0372642709768083 Loss: 0.010984716765292248\n",
      "Iteration: 4097 lambda_n: 0.9441278798628482 Loss: 0.010973120018892153\n",
      "Iteration: 4098 lambda_n: 0.9056488516191553 Loss: 0.010962564550437266\n",
      "Iteration: 4099 lambda_n: 0.9761148020359739 Loss: 0.010952439283446722\n",
      "Iteration: 4100 lambda_n: 0.9266688971979035 Loss: 0.010941526199471136\n",
      "Iteration: 4101 lambda_n: 0.9692736732177898 Loss: 0.010931165927955051\n",
      "Iteration: 4102 lambda_n: 0.9079185755432441 Loss: 0.010920329330962411\n",
      "Iteration: 4103 lambda_n: 1.0073565392350874 Loss: 0.010910178692486214\n",
      "Iteration: 4104 lambda_n: 0.9891653207358291 Loss: 0.010898916326794432\n",
      "Iteration: 4105 lambda_n: 0.8932075130099627 Loss: 0.01088785734231522\n",
      "Iteration: 4106 lambda_n: 0.9602040395893664 Loss: 0.010877871178483078\n",
      "Iteration: 4107 lambda_n: 1.0392848940773427 Loss: 0.010867135986742833\n",
      "Iteration: 4108 lambda_n: 0.9930146459028275 Loss: 0.010855516663155377\n",
      "Iteration: 4109 lambda_n: 0.9433701190084756 Loss: 0.010844414647464846\n",
      "Iteration: 4110 lambda_n: 0.9942205374398043 Loss: 0.010833867664308092\n",
      "Iteration: 4111 lambda_n: 0.9256681046755587 Loss: 0.01082275216892289\n",
      "Iteration: 4112 lambda_n: 0.9342385464419085 Loss: 0.01081240309840017\n",
      "Iteration: 4113 lambda_n: 0.9024121995242625 Loss: 0.0108019582104257\n",
      "Iteration: 4114 lambda_n: 1.0295730383002637 Loss: 0.010791869145444\n",
      "Iteration: 4115 lambda_n: 1.0043739567508871 Loss: 0.010780358409762367\n",
      "Iteration: 4116 lambda_n: 0.9446572165472403 Loss: 0.010769129403688507\n",
      "Iteration: 4117 lambda_n: 0.9395778179784579 Loss: 0.010758568038127088\n",
      "Iteration: 4118 lambda_n: 0.9874240144085092 Loss: 0.010748063461791323\n",
      "Iteration: 4119 lambda_n: 0.9209804085920795 Loss: 0.0107370239611382\n",
      "Iteration: 4120 lambda_n: 0.9757579737462739 Loss: 0.010726727307776502\n",
      "Iteration: 4121 lambda_n: 0.9827574896664236 Loss: 0.01071581823676114\n",
      "Iteration: 4122 lambda_n: 0.999731550606323 Loss: 0.01070483091154599\n",
      "Iteration: 4123 lambda_n: 0.9968163692023021 Loss: 0.010693653815766974\n",
      "Iteration: 4124 lambda_n: 0.9286249927683425 Loss: 0.010682509313112707\n",
      "Iteration: 4125 lambda_n: 0.992215464196702 Loss: 0.010672127197618946\n",
      "Iteration: 4126 lambda_n: 0.9828759822457221 Loss: 0.010661034135642036\n",
      "Iteration: 4127 lambda_n: 0.9968760495830289 Loss: 0.010650045491022223\n",
      "Iteration: 4128 lambda_n: 1.035796205949224 Loss: 0.010638900325423346\n",
      "Iteration: 4129 lambda_n: 0.9705217361871855 Loss: 0.010627320030032868\n",
      "Iteration: 4130 lambda_n: 1.0383683784358169 Loss: 0.010616469510214008\n",
      "Iteration: 4131 lambda_n: 1.034567357669309 Loss: 0.01060486045995091\n",
      "Iteration: 4132 lambda_n: 1.0071738524638698 Loss: 0.010593293906586088\n",
      "Iteration: 4133 lambda_n: 0.9307250886040797 Loss: 0.010582033616109916\n",
      "Iteration: 4134 lambda_n: 0.9155769395830028 Loss: 0.010571628030394988\n",
      "Iteration: 4135 lambda_n: 0.9032942452909225 Loss: 0.010561391803172731\n",
      "Iteration: 4136 lambda_n: 0.9288587069757374 Loss: 0.010551292898372514\n",
      "Iteration: 4137 lambda_n: 0.9364743712609471 Loss: 0.010540908181638749\n",
      "Iteration: 4138 lambda_n: 0.9699061197475285 Loss: 0.010530438322054232\n",
      "Iteration: 4139 lambda_n: 0.9983264130460119 Loss: 0.010519594693741812\n",
      "Iteration: 4140 lambda_n: 0.9246715796785921 Loss: 0.010508433325262138\n",
      "Iteration: 4141 lambda_n: 1.0295330297163592 Loss: 0.010498095424604328\n",
      "Iteration: 4142 lambda_n: 0.925759071392772 Loss: 0.01048658516569213\n",
      "Iteration: 4143 lambda_n: 0.9458589831349609 Loss: 0.01047623510862491\n",
      "Iteration: 4144 lambda_n: 0.9072176809382412 Loss: 0.010465660333890663\n",
      "Iteration: 4145 lambda_n: 1.0477434041861609 Loss: 0.010455517572686179\n",
      "Iteration: 4146 lambda_n: 1.031202437222656 Loss: 0.01044380372438113\n",
      "Iteration: 4147 lambda_n: 0.9205662344060247 Loss: 0.01043227480637291\n",
      "Iteration: 4148 lambda_n: 1.0134796401319877 Loss: 0.010421982810067669\n",
      "Iteration: 4149 lambda_n: 1.044329840232363 Loss: 0.010410652036158161\n",
      "Iteration: 4150 lambda_n: 1.026680299289018 Loss: 0.01039897635586515\n",
      "Iteration: 4151 lambda_n: 1.020811850337327 Loss: 0.010387497999706966\n",
      "Iteration: 4152 lambda_n: 0.9160766611518604 Loss: 0.010376085254220182\n",
      "Iteration: 4153 lambda_n: 0.9862066261172879 Loss: 0.010365843456123751\n",
      "Iteration: 4154 lambda_n: 0.9745442963834385 Loss: 0.010354817601208008\n",
      "Iteration: 4155 lambda_n: 0.9427314446836667 Loss: 0.010343922132816506\n",
      "Iteration: 4156 lambda_n: 1.0321944620434522 Loss: 0.010333382335035038\n",
      "Iteration: 4157 lambda_n: 0.9081591092733687 Loss: 0.010321842335916494\n",
      "Iteration: 4158 lambda_n: 0.9616921584134434 Loss: 0.010311689060731469\n",
      "Iteration: 4159 lambda_n: 0.9121106438447242 Loss: 0.010300937283546235\n",
      "Iteration: 4160 lambda_n: 1.0464664195721196 Loss: 0.010290739831538186\n",
      "Iteration: 4161 lambda_n: 1.0049911347486444 Loss: 0.010279040274731234\n",
      "Iteration: 4162 lambda_n: 0.9092836480088465 Loss: 0.010267804415036552\n",
      "Iteration: 4163 lambda_n: 1.0286398456600248 Loss: 0.010257638571474598\n",
      "Iteration: 4164 lambda_n: 0.997282499157076 Loss: 0.010246138319593133\n",
      "Iteration: 4165 lambda_n: 1.0177136952626071 Loss: 0.01023498864554881\n",
      "Iteration: 4166 lambda_n: 1.0047589148605023 Loss: 0.010223610550498682\n",
      "Iteration: 4167 lambda_n: 0.9259824304192178 Loss: 0.010212377291515203\n",
      "Iteration: 4168 lambda_n: 0.9203417501043362 Loss: 0.010202024758709457\n",
      "Iteration: 4169 lambda_n: 0.9431967726891701 Loss: 0.010191735289759432\n",
      "Iteration: 4170 lambda_n: 0.9871452170112149 Loss: 0.010181190301220383\n",
      "Iteration: 4171 lambda_n: 1.0311280227048756 Loss: 0.010170153967619444\n",
      "Iteration: 4172 lambda_n: 1.0429180395580793 Loss: 0.010158625904901956\n",
      "Iteration: 4173 lambda_n: 1.000396992692577 Loss: 0.010146966030135157\n",
      "Iteration: 4174 lambda_n: 1.0073438702034612 Loss: 0.010135781543649513\n",
      "Iteration: 4175 lambda_n: 0.9485548195100316 Loss: 0.010124519391598289\n",
      "Iteration: 4176 lambda_n: 0.9172589431067273 Loss: 0.010113914504720857\n",
      "Iteration: 4177 lambda_n: 0.9858097911147787 Loss: 0.010103659507926807\n",
      "Iteration: 4178 lambda_n: 0.9219992329008478 Loss: 0.010092638110285934\n",
      "Iteration: 4179 lambda_n: 0.9217064949018875 Loss: 0.010082330118304042\n",
      "Iteration: 4180 lambda_n: 0.9383330575749742 Loss: 0.010072025399851444\n",
      "Iteration: 4181 lambda_n: 1.0327514172069732 Loss: 0.010061534796423965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4182 lambda_n: 0.9848189095715493 Loss: 0.010049988592523158\n",
      "Iteration: 4183 lambda_n: 0.9477045280370446 Loss: 0.010038978276887848\n",
      "Iteration: 4184 lambda_n: 0.9498994378894084 Loss: 0.0100283829023131\n",
      "Iteration: 4185 lambda_n: 0.9047248998893234 Loss: 0.010017762989286506\n",
      "Iteration: 4186 lambda_n: 1.0013975197656757 Loss: 0.010007648130060784\n",
      "Iteration: 4187 lambda_n: 1.0152841501756154 Loss: 0.009996452467946915\n",
      "Iteration: 4188 lambda_n: 0.9309159765123103 Loss: 0.009985101553590302\n",
      "Iteration: 4189 lambda_n: 0.9468294625109435 Loss: 0.009974693879285143\n",
      "Iteration: 4190 lambda_n: 0.9084256838490877 Loss: 0.009964108292325231\n",
      "Iteration: 4191 lambda_n: 0.9043967602763411 Loss: 0.009953952061646731\n",
      "Iteration: 4192 lambda_n: 0.9011366867711258 Loss: 0.009943840875114504\n",
      "Iteration: 4193 lambda_n: 0.9288603212763157 Loss: 0.009933766136947518\n",
      "Iteration: 4194 lambda_n: 0.9075516945962915 Loss: 0.009923381448270081\n",
      "Iteration: 4195 lambda_n: 0.9104494305377998 Loss: 0.009913234991381654\n",
      "Iteration: 4196 lambda_n: 1.0036559484238539 Loss: 0.009903056138344912\n",
      "Iteration: 4197 lambda_n: 0.9565251123398887 Loss: 0.009891835234225843\n",
      "Iteration: 4198 lambda_n: 1.014351752871597 Loss: 0.009881141255018893\n",
      "Iteration: 4199 lambda_n: 1.0033457955128653 Loss: 0.009869800772981578\n",
      "Iteration: 4200 lambda_n: 0.9012434228493157 Loss: 0.009858583338629502\n",
      "Iteration: 4201 lambda_n: 0.9589324981644215 Loss: 0.00984850741236342\n",
      "Iteration: 4202 lambda_n: 1.0425751108166923 Loss: 0.009837786521288418\n",
      "Iteration: 4203 lambda_n: 1.0232384930821852 Loss: 0.00982613050429565\n",
      "Iteration: 4204 lambda_n: 0.9768620390712849 Loss: 0.009814690671977306\n",
      "Iteration: 4205 lambda_n: 0.8942164085448714 Loss: 0.009803769330320854\n",
      "Iteration: 4206 lambda_n: 1.049159899084482 Loss: 0.009793771969482945\n",
      "Iteration: 4207 lambda_n: 0.9312376902030384 Loss: 0.009782042337391458\n",
      "Iteration: 4208 lambda_n: 0.9565755168423675 Loss: 0.009771631079041618\n",
      "Iteration: 4209 lambda_n: 1.041524872012597 Loss: 0.009760936543855396\n",
      "Iteration: 4210 lambda_n: 0.9772570802327657 Loss: 0.009749292273729478\n",
      "Iteration: 4211 lambda_n: 0.9217288828035316 Loss: 0.009738366519596114\n",
      "Iteration: 4212 lambda_n: 0.9121233335826839 Loss: 0.009728061572478146\n",
      "Iteration: 4213 lambda_n: 0.9368101992532365 Loss: 0.009717864016177982\n",
      "Iteration: 4214 lambda_n: 0.926000036402928 Loss: 0.009707390460842243\n",
      "Iteration: 4215 lambda_n: 0.9529401269547666 Loss: 0.009697037763927349\n",
      "Iteration: 4216 lambda_n: 0.9060548211167071 Loss: 0.009686383876927529\n",
      "Iteration: 4217 lambda_n: 1.0190560787871628 Loss: 0.009676254169044538\n",
      "Iteration: 4218 lambda_n: 0.9388011568518833 Loss: 0.009664861105861584\n",
      "Iteration: 4219 lambda_n: 0.9408581007924912 Loss: 0.00965436529463239\n",
      "Iteration: 4220 lambda_n: 0.9669362092063867 Loss: 0.009643846487334474\n",
      "Iteration: 4221 lambda_n: 0.9532045068920397 Loss: 0.009633036127014523\n",
      "Iteration: 4222 lambda_n: 1.0035668852817963 Loss: 0.009622379287933636\n",
      "Iteration: 4223 lambda_n: 0.8922195637289642 Loss: 0.00961115939742711\n",
      "Iteration: 4224 lambda_n: 0.8975143028500973 Loss: 0.009601184371979576\n",
      "Iteration: 4225 lambda_n: 0.9595801535191235 Loss: 0.00959115015181134\n",
      "Iteration: 4226 lambda_n: 1.0495443085692215 Loss: 0.00958042203528627\n",
      "Iteration: 4227 lambda_n: 0.9586812516470149 Loss: 0.009568688119187423\n",
      "Iteration: 4228 lambda_n: 1.0303038348743334 Loss: 0.009557970053645346\n",
      "Iteration: 4229 lambda_n: 0.9933671690900965 Loss: 0.009546451247575229\n",
      "Iteration: 4230 lambda_n: 1.0168995419070386 Loss: 0.009535345394413804\n",
      "Iteration: 4231 lambda_n: 1.0454965154640998 Loss: 0.00952397644977317\n",
      "Iteration: 4232 lambda_n: 0.9317449237145976 Loss: 0.00951228779142341\n",
      "Iteration: 4233 lambda_n: 0.9320696160671207 Loss: 0.009501870877278136\n",
      "Iteration: 4234 lambda_n: 1.0346016709937322 Loss: 0.009491450333614353\n",
      "Iteration: 4235 lambda_n: 0.8998623701723553 Loss: 0.009479883481588342\n",
      "Iteration: 4236 lambda_n: 1.0046394529265628 Loss: 0.009469823016218888\n",
      "Iteration: 4237 lambda_n: 1.0491606947921175 Loss: 0.009458591143147722\n",
      "Iteration: 4238 lambda_n: 1.0383614026458838 Loss: 0.009446861523062641\n",
      "Iteration: 4239 lambda_n: 0.9591830894454151 Loss: 0.009435252639764876\n",
      "Iteration: 4240 lambda_n: 1.0067561112424663 Loss: 0.009424528970827855\n",
      "Iteration: 4241 lambda_n: 0.9236120710209212 Loss: 0.009413273435990861\n",
      "Iteration: 4242 lambda_n: 0.9039709017887374 Loss: 0.009402947452206739\n",
      "Iteration: 4243 lambda_n: 0.923718770721464 Loss: 0.009392841057210772\n",
      "Iteration: 4244 lambda_n: 1.0287831099090015 Loss: 0.009382513881529359\n",
      "Iteration: 4245 lambda_n: 0.9811570314940868 Loss: 0.00937101208711756\n",
      "Iteration: 4246 lambda_n: 0.9845412983722539 Loss: 0.009360042752821126\n",
      "Iteration: 4247 lambda_n: 0.9637634096098374 Loss: 0.009349035582990581\n",
      "Iteration: 4248 lambda_n: 1.0320378740078107 Loss: 0.00933826071046871\n",
      "Iteration: 4249 lambda_n: 0.9695150600906851 Loss: 0.009326722530180341\n",
      "Iteration: 4250 lambda_n: 0.9910157017291301 Loss: 0.00931588335533722\n",
      "Iteration: 4251 lambda_n: 0.9278265185877372 Loss: 0.009304803803948889\n",
      "Iteration: 4252 lambda_n: 1.0165911989652685 Loss: 0.009294430707886669\n",
      "Iteration: 4253 lambda_n: 1.0132522500271142 Loss: 0.009283065223654553\n",
      "Iteration: 4254 lambda_n: 0.9428916284142024 Loss: 0.009271737069434467\n",
      "Iteration: 4255 lambda_n: 0.9777906158045838 Loss: 0.00926119554708736\n",
      "Iteration: 4256 lambda_n: 0.9124639480082167 Loss: 0.009250263854799382\n",
      "Iteration: 4257 lambda_n: 0.9885895980647481 Loss: 0.009240062514698107\n",
      "Iteration: 4258 lambda_n: 1.038974142562649 Loss: 0.009229010090897232\n",
      "Iteration: 4259 lambda_n: 0.9225916460489579 Loss: 0.009217394368855599\n",
      "Iteration: 4260 lambda_n: 1.0268078958293487 Loss: 0.009207079802654491\n",
      "Iteration: 4261 lambda_n: 0.9780859199416261 Loss: 0.00919560010024423\n",
      "Iteration: 4262 lambda_n: 0.9013981490652359 Loss: 0.009184665109598825\n",
      "Iteration: 4263 lambda_n: 0.8981351971325745 Loss: 0.00917458748792613\n",
      "Iteration: 4264 lambda_n: 1.0493566107930619 Loss: 0.00916454634645378\n",
      "Iteration: 4265 lambda_n: 0.9633192296588469 Loss: 0.0091528145517669\n",
      "Iteration: 4266 lambda_n: 0.9393129168120486 Loss: 0.009142044654533957\n",
      "Iteration: 4267 lambda_n: 1.0419949479720525 Loss: 0.009131543148061813\n",
      "Iteration: 4268 lambda_n: 0.9451425412046628 Loss: 0.009119893658298616\n",
      "Iteration: 4269 lambda_n: 0.902983909645903 Loss: 0.009109327066952725\n",
      "Iteration: 4270 lambda_n: 0.9506027869625358 Loss: 0.009099231766074953\n",
      "Iteration: 4271 lambda_n: 1.0429088861062734 Loss: 0.009088604067126095\n",
      "Iteration: 4272 lambda_n: 0.9527072242555821 Loss: 0.009076944374859949\n",
      "Iteration: 4273 lambda_n: 0.9795730957147901 Loss: 0.009066293126327552\n",
      "Iteration: 4274 lambda_n: 1.0415392400077714 Loss: 0.009055341514121048\n",
      "Iteration: 4275 lambda_n: 1.0156922696392956 Loss: 0.009043697119471545\n",
      "Iteration: 4276 lambda_n: 0.9343356207557403 Loss: 0.009032341692967563\n",
      "Iteration: 4277 lambda_n: 0.9341251117929055 Loss: 0.009021895832884803\n",
      "Iteration: 4278 lambda_n: 0.9126058230547385 Loss: 0.009011452326730956\n",
      "Iteration: 4279 lambda_n: 0.9075309173423368 Loss: 0.009001249406501672\n",
      "Iteration: 4280 lambda_n: 0.9800649112844193 Loss: 0.008991103224257226\n",
      "Iteration: 4281 lambda_n: 0.9740859354300825 Loss: 0.00898014611370825\n",
      "Iteration: 4282 lambda_n: 0.945604139300372 Loss: 0.008969255848739005\n",
      "Iteration: 4283 lambda_n: 1.0321067127941546 Loss: 0.00895868401046935\n",
      "Iteration: 4284 lambda_n: 0.9437391726662769 Loss: 0.00894714507558938\n",
      "Iteration: 4285 lambda_n: 1.0106086271545471 Loss: 0.008936594088922617\n",
      "Iteration: 4286 lambda_n: 0.9519499807126235 Loss: 0.008925295503609211\n",
      "Iteration: 4287 lambda_n: 1.0019234049428318 Loss: 0.008914652721465082\n",
      "Iteration: 4288 lambda_n: 1.0148032730651522 Loss: 0.008903451238002801\n",
      "Iteration: 4289 lambda_n: 0.8982673438877804 Loss: 0.00889210575848068\n",
      "Iteration: 4290 lambda_n: 0.952388350338183 Loss: 0.008882063148754749\n",
      "Iteration: 4291 lambda_n: 0.9698521735308595 Loss: 0.008871415467814584\n",
      "Iteration: 4292 lambda_n: 0.9031194705592476 Loss: 0.008860572542223\n",
      "Iteration: 4293 lambda_n: 0.966140649186809 Loss: 0.008850475687231291\n",
      "Iteration: 4294 lambda_n: 0.9760697439389299 Loss: 0.008839674257370336\n",
      "Iteration: 4295 lambda_n: 0.9148593603413513 Loss: 0.008828761820956392\n",
      "Iteration: 4296 lambda_n: 1.0492202786035612 Loss: 0.008818533715629488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4297 lambda_n: 0.9129268278552404 Loss: 0.008806803459003712\n",
      "Iteration: 4298 lambda_n: 0.9676410918511995 Loss: 0.008796596960250662\n",
      "Iteration: 4299 lambda_n: 0.9428998062092208 Loss: 0.008785778757854924\n",
      "Iteration: 4300 lambda_n: 0.9195216752895171 Loss: 0.008775237162842474\n",
      "Iteration: 4301 lambda_n: 0.9871735501533248 Loss: 0.008764956935141083\n",
      "Iteration: 4302 lambda_n: 0.9704810066704839 Loss: 0.008753920361729313\n",
      "Iteration: 4303 lambda_n: 1.030630955813482 Loss: 0.00874307041095816\n",
      "Iteration: 4304 lambda_n: 0.9675557324861663 Loss: 0.008731547985896877\n",
      "Iteration: 4305 lambda_n: 1.013792987544138 Loss: 0.008720730740525763\n",
      "Iteration: 4306 lambda_n: 0.954474507194275 Loss: 0.008709396564425865\n",
      "Iteration: 4307 lambda_n: 1.0363647146497121 Loss: 0.008698725567658647\n",
      "Iteration: 4308 lambda_n: 0.9902716051423623 Loss: 0.008687139041248428\n",
      "Iteration: 4309 lambda_n: 0.912169274411568 Loss: 0.008676067761736142\n",
      "Iteration: 4310 lambda_n: 0.962164965640184 Loss: 0.008665869767580223\n",
      "Iteration: 4311 lambda_n: 1.01981334664034 Loss: 0.008655112819940882\n",
      "Iteration: 4312 lambda_n: 0.97718009930724 Loss: 0.008643711354295055\n",
      "Iteration: 4313 lambda_n: 1.0438727893778843 Loss: 0.008632786519026827\n",
      "Iteration: 4314 lambda_n: 0.9388354754906783 Loss: 0.008621116058390539\n",
      "Iteration: 4315 lambda_n: 0.9895667769426733 Loss: 0.008610619909532154\n",
      "Iteration: 4316 lambda_n: 0.9137121073255565 Loss: 0.008599556585968952\n",
      "Iteration: 4317 lambda_n: 0.9735300873953124 Loss: 0.008589341315096446\n",
      "Iteration: 4318 lambda_n: 0.9147260841005064 Loss: 0.00857845728148635\n",
      "Iteration: 4319 lambda_n: 0.9951224301520059 Loss: 0.008568230675051209\n",
      "Iteration: 4320 lambda_n: 0.9995367783006539 Loss: 0.008557105240672916\n",
      "Iteration: 4321 lambda_n: 0.9766937703631284 Loss: 0.008545930454557978\n",
      "Iteration: 4322 lambda_n: 0.9689235555477245 Loss: 0.00853501105297996\n",
      "Iteration: 4323 lambda_n: 1.0033970636788232 Loss: 0.008524178522606127\n",
      "Iteration: 4324 lambda_n: 0.9274322704513922 Loss: 0.00851296058012934\n",
      "Iteration: 4325 lambda_n: 0.9473161905430671 Loss: 0.008502591921697897\n",
      "Iteration: 4326 lambda_n: 0.94639586866917 Loss: 0.008492000962170926\n",
      "Iteration: 4327 lambda_n: 1.0376177651652654 Loss: 0.008481420292197292\n",
      "Iteration: 4328 lambda_n: 0.9058785907948783 Loss: 0.008469819765288846\n",
      "Iteration: 4329 lambda_n: 0.9066320483065974 Loss: 0.008459692077694368\n",
      "Iteration: 4330 lambda_n: 0.9754929379315255 Loss: 0.008449555966803062\n",
      "Iteration: 4331 lambda_n: 0.9022167425679848 Loss: 0.008438649994203379\n",
      "Iteration: 4332 lambda_n: 0.9554353208118113 Loss: 0.008428563246921438\n",
      "Iteration: 4333 lambda_n: 1.0317162408233616 Loss: 0.008417881518382613\n",
      "Iteration: 4334 lambda_n: 1.0492789363511514 Loss: 0.00840634695545547\n",
      "Iteration: 4335 lambda_n: 1.0146233525626145 Loss: 0.008394616109645741\n",
      "Iteration: 4336 lambda_n: 0.9375536424091818 Loss: 0.008383272685812133\n",
      "Iteration: 4337 lambda_n: 1.0377040003277433 Loss: 0.008372790884551775\n",
      "Iteration: 4338 lambda_n: 0.9997210674601955 Loss: 0.008361189401194178\n",
      "Iteration: 4339 lambda_n: 1.0182937638719198 Loss: 0.008350012561771186\n",
      "Iteration: 4340 lambda_n: 1.0431531835645018 Loss: 0.008338628078960087\n",
      "Iteration: 4341 lambda_n: 0.9194447007655164 Loss: 0.008326965668438265\n",
      "Iteration: 4342 lambda_n: 0.8933695178608778 Loss: 0.008316686313916953\n",
      "Iteration: 4343 lambda_n: 0.912263360235549 Loss: 0.008306698479248493\n",
      "Iteration: 4344 lambda_n: 1.0121573607812213 Loss: 0.008296499412790353\n",
      "Iteration: 4345 lambda_n: 0.9828576604501533 Loss: 0.008285183536306313\n",
      "Iteration: 4346 lambda_n: 0.945351154461078 Loss: 0.008274195229815403\n",
      "Iteration: 4347 lambda_n: 0.9294686830781305 Loss: 0.00826362624493816\n",
      "Iteration: 4348 lambda_n: 1.041976148345903 Loss: 0.008253234825842094\n",
      "Iteration: 4349 lambda_n: 1.0217231858272238 Loss: 0.008241585578693932\n",
      "Iteration: 4350 lambda_n: 0.9638830612933033 Loss: 0.008230162759276372\n",
      "Iteration: 4351 lambda_n: 0.9423810560715644 Loss: 0.008219386590315307\n",
      "Iteration: 4352 lambda_n: 0.935932944975889 Loss: 0.00820885081320803\n",
      "Iteration: 4353 lambda_n: 0.9809404514868139 Loss: 0.00819838712607441\n",
      "Iteration: 4354 lambda_n: 0.8983228996436786 Loss: 0.008187420257495697\n",
      "Iteration: 4355 lambda_n: 0.9388142774121058 Loss: 0.008177377049688624\n",
      "Iteration: 4356 lambda_n: 0.9380391261442553 Loss: 0.008166881150531036\n",
      "Iteration: 4357 lambda_n: 0.910058271427019 Loss: 0.00815639391787963\n",
      "Iteration: 4358 lambda_n: 0.9589891712950865 Loss: 0.00814621951017891\n",
      "Iteration: 4359 lambda_n: 0.9740644390442041 Loss: 0.0081354980577015\n",
      "Iteration: 4360 lambda_n: 0.967477261362611 Loss: 0.00812460806480794\n",
      "Iteration: 4361 lambda_n: 0.9824432590414715 Loss: 0.008113791716583845\n",
      "Iteration: 4362 lambda_n: 0.9025980499937848 Loss: 0.008102808049583935\n",
      "Iteration: 4363 lambda_n: 0.9154526512313061 Loss: 0.008092717048384062\n",
      "Iteration: 4364 lambda_n: 0.9793416325473848 Loss: 0.008082482333674057\n",
      "Iteration: 4365 lambda_n: 0.8955313030955429 Loss: 0.008071533343670957\n",
      "Iteration: 4366 lambda_n: 1.0407843222695152 Loss: 0.008061521349222252\n",
      "Iteration: 4367 lambda_n: 0.9389513083854277 Loss: 0.008049885433721362\n",
      "Iteration: 4368 lambda_n: 0.9049051057480804 Loss: 0.008039388006439921\n",
      "Iteration: 4369 lambda_n: 0.972095195987588 Loss: 0.008029271214236725\n",
      "Iteration: 4370 lambda_n: 1.0214380067184856 Loss: 0.00801840324059902\n",
      "Iteration: 4371 lambda_n: 0.9312233519610073 Loss: 0.008006983617237373\n",
      "Iteration: 4372 lambda_n: 1.0203523364117895 Loss: 0.007996572589298636\n",
      "Iteration: 4373 lambda_n: 0.904343126855634 Loss: 0.00798516510432658\n",
      "Iteration: 4374 lambda_n: 1.0184455998312034 Loss: 0.007975054596459693\n",
      "Iteration: 4375 lambda_n: 0.9219166022020989 Loss: 0.007963668429334006\n",
      "Iteration: 4376 lambda_n: 0.925957156689885 Loss: 0.00795336145151896\n",
      "Iteration: 4377 lambda_n: 1.0330757740295187 Loss: 0.007943009300793651\n",
      "Iteration: 4378 lambda_n: 0.937313267759001 Loss: 0.00793145957004518\n",
      "Iteration: 4379 lambda_n: 0.9116012085180688 Loss: 0.007920980459191268\n",
      "Iteration: 4380 lambda_n: 0.9077097608892798 Loss: 0.007910788808013054\n",
      "Iteration: 4381 lambda_n: 0.9898271954298781 Loss: 0.00790064066326002\n",
      "Iteration: 4382 lambda_n: 0.9309994503320533 Loss: 0.007889574450421754\n",
      "Iteration: 4383 lambda_n: 0.974420738170776 Loss: 0.007879165928773777\n",
      "Iteration: 4384 lambda_n: 0.9710850814979914 Loss: 0.007868271959837617\n",
      "Iteration: 4385 lambda_n: 0.98527851011914 Loss: 0.007857415283640184\n",
      "Iteration: 4386 lambda_n: 1.026576123840501 Loss: 0.007846399926001326\n",
      "Iteration: 4387 lambda_n: 0.9544811423110806 Loss: 0.007834922863693571\n",
      "Iteration: 4388 lambda_n: 0.9323104947248119 Loss: 0.007824251819442645\n",
      "Iteration: 4389 lambda_n: 0.9213462305788284 Loss: 0.007813828642017476\n",
      "Iteration: 4390 lambda_n: 0.9735229690226931 Loss: 0.0078035280446823035\n",
      "Iteration: 4391 lambda_n: 0.8961111719151907 Loss: 0.007792644114702767\n",
      "Iteration: 4392 lambda_n: 0.9883332026316548 Loss: 0.007782625644348995\n",
      "Iteration: 4393 lambda_n: 0.9143726502368638 Loss: 0.007771576137353826\n",
      "Iteration: 4394 lambda_n: 0.9979990570366307 Loss: 0.007761353505236174\n",
      "Iteration: 4395 lambda_n: 0.911515385843667 Loss: 0.007750195935098218\n",
      "Iteration: 4396 lambda_n: 0.9741707180826817 Loss: 0.007740005247520223\n",
      "Iteration: 4397 lambda_n: 0.9843173270991293 Loss: 0.007729114077318448\n",
      "Iteration: 4398 lambda_n: 0.9000925255757387 Loss: 0.007718109468903646\n",
      "Iteration: 4399 lambda_n: 0.9719183566838416 Loss: 0.0077080464889410436\n",
      "Iteration: 4400 lambda_n: 0.9625664703127338 Loss: 0.00769718050077444\n",
      "Iteration: 4401 lambda_n: 0.9555994573619151 Loss: 0.0076864190663854135\n",
      "Iteration: 4402 lambda_n: 0.9023822550256136 Loss: 0.007675735523026415\n",
      "Iteration: 4403 lambda_n: 0.9777692386638828 Loss: 0.007665646944960977\n",
      "Iteration: 4404 lambda_n: 0.9307376466284945 Loss: 0.007654715545296361\n",
      "Iteration: 4405 lambda_n: 0.9420325952689815 Loss: 0.0076443099561678786\n",
      "Iteration: 4406 lambda_n: 0.9289026507302779 Loss: 0.007633778090447903\n",
      "Iteration: 4407 lambda_n: 1.0373793093883872 Loss: 0.007623393016921376\n",
      "Iteration: 4408 lambda_n: 1.015062048830104 Loss: 0.007611795181390531\n",
      "Iteration: 4409 lambda_n: 0.961417199005421 Loss: 0.007600446851706751\n",
      "Iteration: 4410 lambda_n: 1.048595358911498 Loss: 0.007589698268314335\n",
      "Iteration: 4411 lambda_n: 0.8964810555100663 Loss: 0.007577975038876335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4412 lambda_n: 0.9719871058957492 Loss: 0.0075679524379198785\n",
      "Iteration: 4413 lambda_n: 0.963710087353986 Loss: 0.007557085684364099\n",
      "Iteration: 4414 lambda_n: 0.965470684631159 Loss: 0.007546311467583216\n",
      "Iteration: 4415 lambda_n: 1.0371897648496693 Loss: 0.007535517567673104\n",
      "Iteration: 4416 lambda_n: 1.0037506720018685 Loss: 0.007523921853328894\n",
      "Iteration: 4417 lambda_n: 0.9630387721431702 Loss: 0.0075126999861353716\n",
      "Iteration: 4418 lambda_n: 1.0288130645526132 Loss: 0.007501933275572914\n",
      "Iteration: 4419 lambda_n: 1.044767729982008 Loss: 0.007490431212962531\n",
      "Iteration: 4420 lambda_n: 0.9681439278071777 Loss: 0.007478750778502459\n",
      "Iteration: 4421 lambda_n: 0.9476911750188374 Loss: 0.007467926993354455\n",
      "Iteration: 4422 lambda_n: 1.0167749570044178 Loss: 0.007457331868853747\n",
      "Iteration: 4423 lambda_n: 0.997242079701475 Loss: 0.007445964392485614\n",
      "Iteration: 4424 lambda_n: 0.9076203526020776 Loss: 0.007434815292631857\n",
      "Iteration: 4425 lambda_n: 0.9239841483214819 Loss: 0.007424668157918883\n",
      "Iteration: 4426 lambda_n: 0.9235579369165081 Loss: 0.007414338077265636\n",
      "Iteration: 4427 lambda_n: 1.018375041090887 Loss: 0.007404012761828544\n",
      "Iteration: 4428 lambda_n: 0.9655128659359704 Loss: 0.007392627397782999\n",
      "Iteration: 4429 lambda_n: 0.9267582798698659 Loss: 0.007381833029507574\n",
      "Iteration: 4430 lambda_n: 0.9900780006073583 Loss: 0.0073714719350798244\n",
      "Iteration: 4431 lambda_n: 1.0342396878682585 Loss: 0.007360402930699536\n",
      "Iteration: 4432 lambda_n: 0.9410900774616011 Loss: 0.007348840201908354\n",
      "Iteration: 4433 lambda_n: 0.8946233480300142 Loss: 0.0073383188795957\n",
      "Iteration: 4434 lambda_n: 0.9821525384972618 Loss: 0.0073283170523114\n",
      "Iteration: 4435 lambda_n: 0.9537561865376946 Loss: 0.0073173366549167\n",
      "Iteration: 4436 lambda_n: 0.960857318957934 Loss: 0.007306673726981045\n",
      "Iteration: 4437 lambda_n: 0.8933928625303535 Loss: 0.007295931409082086\n",
      "Iteration: 4438 lambda_n: 1.0129079605762925 Loss: 0.007285943339296974\n",
      "Iteration: 4439 lambda_n: 1.0109157569640108 Loss: 0.007274619099265077\n",
      "Iteration: 4440 lambda_n: 1.0384606506017438 Loss: 0.0072633171321543465\n",
      "Iteration: 4441 lambda_n: 0.8940872037394736 Loss: 0.007251707215297419\n",
      "Iteration: 4442 lambda_n: 1.0037628552999946 Loss: 0.007241711383598434\n",
      "Iteration: 4443 lambda_n: 0.9745298834005138 Loss: 0.007230489386065581\n",
      "Iteration: 4444 lambda_n: 1.0046268589103957 Loss: 0.007219594211294813\n",
      "Iteration: 4445 lambda_n: 0.9309633753685109 Loss: 0.007208362554687375\n",
      "Iteration: 4446 lambda_n: 1.012480301086311 Loss: 0.007197954450768468\n",
      "Iteration: 4447 lambda_n: 0.9996956903101931 Loss: 0.0071866349936516956\n",
      "Iteration: 4448 lambda_n: 1.030076644287534 Loss: 0.007175458467778026\n",
      "Iteration: 4449 lambda_n: 0.984301960161851 Loss: 0.00716394228523974\n",
      "Iteration: 4450 lambda_n: 0.9751812384401567 Loss: 0.007152937860576129\n",
      "Iteration: 4451 lambda_n: 0.9556874004279892 Loss: 0.007142035405118716\n",
      "Iteration: 4452 lambda_n: 0.9116242439504347 Loss: 0.007131350889545984\n",
      "Iteration: 4453 lambda_n: 1.045101953812638 Loss: 0.0071211589970326524\n",
      "Iteration: 4454 lambda_n: 0.9600218289182303 Loss: 0.0071094748336576735\n",
      "Iteration: 4455 lambda_n: 0.9054069859418781 Loss: 0.0070987418600558716\n",
      "Iteration: 4456 lambda_n: 0.9068833856583862 Loss: 0.0070886194765652\n",
      "Iteration: 4457 lambda_n: 0.9245559489028555 Loss: 0.0070784805871980366\n",
      "Iteration: 4458 lambda_n: 0.9177396740377494 Loss: 0.007068144120043378\n",
      "Iteration: 4459 lambda_n: 0.9528577152647979 Loss: 0.007057883858505072\n",
      "Iteration: 4460 lambda_n: 1.026241190845209 Loss: 0.007047230980040049\n",
      "Iteration: 4461 lambda_n: 0.9349402098814987 Loss: 0.007035757679960683\n",
      "Iteration: 4462 lambda_n: 0.9055707018597641 Loss: 0.007025305118234503\n",
      "Iteration: 4463 lambda_n: 0.949642316306614 Loss: 0.00701518090557798\n",
      "Iteration: 4464 lambda_n: 0.9159299317167398 Loss: 0.007004563975736046\n",
      "Iteration: 4465 lambda_n: 1.0274242799193005 Loss: 0.006994323947990278\n",
      "Iteration: 4466 lambda_n: 1.0347630898329458 Loss: 0.006982837421996139\n",
      "Iteration: 4467 lambda_n: 0.9092202356599601 Loss: 0.006971268848861175\n",
      "Iteration: 4468 lambda_n: 0.9184512354502399 Loss: 0.006961103835523695\n",
      "Iteration: 4469 lambda_n: 1.0353637422880746 Loss: 0.006950835620463891\n",
      "Iteration: 4470 lambda_n: 1.027444380474604 Loss: 0.006939260332633217\n",
      "Iteration: 4471 lambda_n: 0.8943940007110085 Loss: 0.00692777358286131\n",
      "Iteration: 4472 lambda_n: 1.0150912879897211 Loss: 0.00691777432635112\n",
      "Iteration: 4473 lambda_n: 1.0067002214207945 Loss: 0.006906425683567793\n",
      "Iteration: 4474 lambda_n: 0.9845985617584518 Loss: 0.006895170852451094\n",
      "Iteration: 4475 lambda_n: 0.9704538806131405 Loss: 0.006884163116370048\n",
      "Iteration: 4476 lambda_n: 0.8968676324526631 Loss: 0.006873313516906309\n",
      "Iteration: 4477 lambda_n: 1.004000459239825 Loss: 0.00686328660617139\n",
      "Iteration: 4478 lambda_n: 0.9897186374534882 Loss: 0.006852061958882439\n",
      "Iteration: 4479 lambda_n: 1.016881632408483 Loss: 0.006840996981429309\n",
      "Iteration: 4480 lambda_n: 0.9353287677141261 Loss: 0.006829628323980339\n",
      "Iteration: 4481 lambda_n: 1.019173134543068 Loss: 0.006819171421372992\n",
      "Iteration: 4482 lambda_n: 0.9078982384533834 Loss: 0.006807777145455381\n",
      "Iteration: 4483 lambda_n: 0.9897556862449156 Loss: 0.0067976269143313235\n",
      "Iteration: 4484 lambda_n: 0.9707928618599642 Loss: 0.006786561523513236\n",
      "Iteration: 4485 lambda_n: 1.0160962599663559 Loss: 0.006775708135746385\n",
      "Iteration: 4486 lambda_n: 0.974301361981161 Loss: 0.0067643482597230745\n",
      "Iteration: 4487 lambda_n: 0.9704547228599288 Loss: 0.006753455647529331\n",
      "Iteration: 4488 lambda_n: 0.9950990066575607 Loss: 0.006742606040615823\n",
      "Iteration: 4489 lambda_n: 0.9036641746299492 Loss: 0.00673148091272349\n",
      "Iteration: 4490 lambda_n: 0.97372849332089 Loss: 0.0067213780191436945\n",
      "Iteration: 4491 lambda_n: 1.0488450986196516 Loss: 0.0067104918122005775\n",
      "Iteration: 4492 lambda_n: 0.9734369196592666 Loss: 0.0066987658077655565\n",
      "Iteration: 4493 lambda_n: 0.9809731253155289 Loss: 0.0066878828609158395\n",
      "Iteration: 4494 lambda_n: 1.0069583125524704 Loss: 0.006676915660045451\n",
      "Iteration: 4495 lambda_n: 0.9000960278221891 Loss: 0.006665657947025552\n",
      "Iteration: 4496 lambda_n: 0.9254396794695868 Loss: 0.0066555949459078615\n",
      "Iteration: 4497 lambda_n: 1.0391577376855665 Loss: 0.0066452486049386784\n",
      "Iteration: 4498 lambda_n: 0.9399454812553254 Loss: 0.006633630905404454\n",
      "Iteration: 4499 lambda_n: 1.008015223234853 Loss: 0.006623122391034657\n",
      "Iteration: 4500 lambda_n: 0.9391449083591846 Loss: 0.006611852862610611\n",
      "Iteration: 4501 lambda_n: 1.019891666128099 Loss: 0.006601353298868341\n",
      "Iteration: 4502 lambda_n: 0.9198759236187699 Loss: 0.00658995099308654\n",
      "Iteration: 4503 lambda_n: 0.96117343464733 Loss: 0.00657966685530752\n",
      "Iteration: 4504 lambda_n: 0.9063949997988255 Loss: 0.006568921014860696\n",
      "Iteration: 4505 lambda_n: 0.8942262704161762 Loss: 0.006558787592972271\n",
      "Iteration: 4506 lambda_n: 0.9968006091655229 Loss: 0.006548790216605671\n",
      "Iteration: 4507 lambda_n: 0.8931943701777405 Loss: 0.006537646067679371\n",
      "Iteration: 4508 lambda_n: 1.0336197668539497 Loss: 0.006527660228128443\n",
      "Iteration: 4509 lambda_n: 0.9128646920403071 Loss: 0.0065161044443379935\n",
      "Iteration: 4510 lambda_n: 0.9523135693163255 Loss: 0.006505898692470916\n",
      "Iteration: 4511 lambda_n: 1.0096712334824534 Loss: 0.006495251905542966\n",
      "Iteration: 4512 lambda_n: 1.0193657829688842 Loss: 0.006483963864816503\n",
      "Iteration: 4513 lambda_n: 0.9007918453798994 Loss: 0.006472567439981433\n",
      "Iteration: 4514 lambda_n: 0.9326560053086651 Loss: 0.006462496662059935\n",
      "Iteration: 4515 lambda_n: 0.9825052916110845 Loss: 0.006452069645602393\n",
      "Iteration: 4516 lambda_n: 0.9029583743847517 Loss: 0.006441085318408582\n",
      "Iteration: 4517 lambda_n: 0.9456815632217042 Loss: 0.006430990319240748\n",
      "Iteration: 4518 lambda_n: 0.9940477384948451 Loss: 0.006420417678514217\n",
      "Iteration: 4519 lambda_n: 1.0409557788722985 Loss: 0.006409304308130116\n",
      "Iteration: 4520 lambda_n: 1.0205553944745833 Loss: 0.0063976665099335864\n",
      "Iteration: 4521 lambda_n: 1.0049503231209402 Loss: 0.006386256786471239\n",
      "Iteration: 4522 lambda_n: 0.9030724649237767 Loss: 0.0063750215265376406\n",
      "Iteration: 4523 lambda_n: 0.990284116823824 Loss: 0.0063649252526006925\n",
      "Iteration: 4524 lambda_n: 0.965557027285182 Loss: 0.006353853959880222\n",
      "Iteration: 4525 lambda_n: 0.9594344496074333 Loss: 0.0063430591140625015\n",
      "Iteration: 4526 lambda_n: 1.0201291370126786 Loss: 0.006332332718271861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4527 lambda_n: 0.9872130390375069 Loss: 0.006320927761156165\n",
      "Iteration: 4528 lambda_n: 0.9589740267412795 Loss: 0.0063098908033570045\n",
      "Iteration: 4529 lambda_n: 0.9280177672926606 Loss: 0.006299169555440842\n",
      "Iteration: 4530 lambda_n: 0.9665658557050362 Loss: 0.0062887943959873\n",
      "Iteration: 4531 lambda_n: 1.0006275302062537 Loss: 0.006277988272314186\n",
      "Iteration: 4532 lambda_n: 1.0373870887880001 Loss: 0.006266801342159168\n",
      "Iteration: 4533 lambda_n: 0.9778804969531433 Loss: 0.006255203443422133\n",
      "Iteration: 4534 lambda_n: 1.0318076930855755 Loss: 0.006244270823414673\n",
      "Iteration: 4535 lambda_n: 1.0366485904676646 Loss: 0.006232735302115736\n",
      "Iteration: 4536 lambda_n: 0.9272071447965055 Loss: 0.0062211456601391915\n",
      "Iteration: 4537 lambda_n: 1.0375313782861881 Loss: 0.006210779564225464\n",
      "Iteration: 4538 lambda_n: 1.0376012144359787 Loss: 0.0061991800530195726\n",
      "Iteration: 4539 lambda_n: 0.9389994951667143 Loss: 0.006187579761189097\n",
      "Iteration: 4540 lambda_n: 0.9485868111974044 Loss: 0.006177081828175304\n",
      "Iteration: 4541 lambda_n: 0.898207595478394 Loss: 0.006166476709911319\n",
      "Iteration: 4542 lambda_n: 0.9306071021345491 Loss: 0.00615643482701601\n",
      "Iteration: 4543 lambda_n: 1.0266749539512894 Loss: 0.006146030720555127\n",
      "Iteration: 4544 lambda_n: 0.9137559681682464 Loss: 0.006134552583985473\n",
      "Iteration: 4545 lambda_n: 0.9267526638624605 Loss: 0.006124336871958584\n",
      "Iteration: 4546 lambda_n: 1.0165868671135008 Loss: 0.0061139758581116276\n",
      "Iteration: 4547 lambda_n: 0.9520240577176204 Loss: 0.006102610505833088\n",
      "Iteration: 4548 lambda_n: 0.9813325257013221 Loss: 0.0060919669602357744\n",
      "Iteration: 4549 lambda_n: 1.0477910322135706 Loss: 0.0060809957486461326\n",
      "Iteration: 4550 lambda_n: 1.0458687838474146 Loss: 0.006069281536904993\n",
      "Iteration: 4551 lambda_n: 1.0305405142979251 Loss: 0.006057588815863636\n",
      "Iteration: 4552 lambda_n: 0.896484229726658 Loss: 0.0060460674636576795\n",
      "Iteration: 4553 lambda_n: 0.9583072835133293 Loss: 0.0060360448490189254\n",
      "Iteration: 4554 lambda_n: 1.0282665360040546 Loss: 0.006025331058191683\n",
      "Iteration: 4555 lambda_n: 0.9929295139604517 Loss: 0.006013835129213528\n",
      "Iteration: 4556 lambda_n: 0.897965603342626 Loss: 0.006002734265137299\n",
      "Iteration: 4557 lambda_n: 0.9694004567575684 Loss: 0.005992695089279382\n",
      "Iteration: 4558 lambda_n: 0.9641559403181811 Loss: 0.005981857278192036\n",
      "Iteration: 4559 lambda_n: 0.9999874514119504 Loss: 0.005971078100440347\n",
      "Iteration: 4560 lambda_n: 0.911101641699007 Loss: 0.0059598983296881665\n",
      "Iteration: 4561 lambda_n: 0.9516900214836779 Loss: 0.005949712294485116\n",
      "Iteration: 4562 lambda_n: 0.9787455134293733 Loss: 0.005939072484909167\n",
      "Iteration: 4563 lambda_n: 0.8995697893696328 Loss: 0.005928130197450534\n",
      "Iteration: 4564 lambda_n: 1.0297241339289385 Loss: 0.005918073087616565\n",
      "Iteration: 4565 lambda_n: 0.9601518711623273 Loss: 0.0059065608639485455\n",
      "Iteration: 4566 lambda_n: 1.0077654988709908 Loss: 0.005895826452062196\n",
      "Iteration: 4567 lambda_n: 0.9598309586189719 Loss: 0.005884559724191095\n",
      "Iteration: 4568 lambda_n: 0.9871557311905443 Loss: 0.005873828900285782\n",
      "Iteration: 4569 lambda_n: 0.9543888498607743 Loss: 0.005862792587980569\n",
      "Iteration: 4570 lambda_n: 1.0275423333616842 Loss: 0.005852122606564074\n",
      "Iteration: 4571 lambda_n: 0.8925917595538919 Loss: 0.005840634775893712\n",
      "Iteration: 4572 lambda_n: 0.9349267353592989 Loss: 0.005830655680573064\n",
      "Iteration: 4573 lambda_n: 1.0206245521756605 Loss: 0.005820203284132244\n",
      "Iteration: 4574 lambda_n: 0.9994523574193812 Loss: 0.005808792793942408\n",
      "Iteration: 4575 lambda_n: 0.9483265340677319 Loss: 0.005797619007082732\n",
      "Iteration: 4576 lambda_n: 1.0408913674720395 Loss: 0.005787016802398282\n",
      "Iteration: 4577 lambda_n: 1.0135531196313767 Loss: 0.005775379731370382\n",
      "Iteration: 4578 lambda_n: 1.04718129042199 Loss: 0.005764048299581858\n",
      "Iteration: 4579 lambda_n: 0.9051433505639206 Loss: 0.005752340908008818\n",
      "Iteration: 4580 lambda_n: 1.03980029048472 Loss: 0.005742221487780556\n",
      "Iteration: 4581 lambda_n: 0.9891219363477873 Loss: 0.005730596615323193\n",
      "Iteration: 4582 lambda_n: 1.0482590182374316 Loss: 0.005719538322349575\n",
      "Iteration: 4583 lambda_n: 0.9655753336262812 Loss: 0.00570781888230243\n",
      "Iteration: 4584 lambda_n: 0.932431236880776 Loss: 0.005697023838396315\n",
      "Iteration: 4585 lambda_n: 0.9830773158923473 Loss: 0.005686599342549958\n",
      "Iteration: 4586 lambda_n: 0.9024014883753421 Loss: 0.0056756086282689485\n",
      "Iteration: 4587 lambda_n: 0.9364356663979362 Loss: 0.005665519862432381\n",
      "Iteration: 4588 lambda_n: 1.0293655903441215 Loss: 0.0056550505976915725\n",
      "Iteration: 4589 lambda_n: 0.9425432181222424 Loss: 0.005643542385030824\n",
      "Iteration: 4590 lambda_n: 1.0475435400192061 Loss: 0.005633004838603548\n",
      "Iteration: 4591 lambda_n: 0.9821373885273329 Loss: 0.005621293398337317\n",
      "Iteration: 4592 lambda_n: 0.9065433630419331 Loss: 0.005610313192917956\n",
      "Iteration: 4593 lambda_n: 1.037649646459278 Loss: 0.005600178121816641\n",
      "Iteration: 4594 lambda_n: 0.9182170258801199 Loss: 0.005588577294660881\n",
      "Iteration: 4595 lambda_n: 0.9142989691232148 Loss: 0.005578311713256518\n",
      "Iteration: 4596 lambda_n: 0.9287306310881297 Loss: 0.005568089935443419\n",
      "Iteration: 4597 lambda_n: 0.9088827295026459 Loss: 0.0055577068130655966\n",
      "Iteration: 4598 lambda_n: 0.9689783962530715 Loss: 0.005547545588466765\n",
      "Iteration: 4599 lambda_n: 1.0319167635135043 Loss: 0.005536712499970577\n",
      "Iteration: 4600 lambda_n: 1.0191226935715716 Loss: 0.005525175766465014\n",
      "Iteration: 4601 lambda_n: 1.031596102255634 Loss: 0.005513782069569032\n",
      "Iteration: 4602 lambda_n: 0.9233077997063155 Loss: 0.005502248921220665\n",
      "Iteration: 4603 lambda_n: 1.0088927889940935 Loss: 0.005491926426096321\n",
      "Iteration: 4604 lambda_n: 0.8960918021253815 Loss: 0.005480647098854826\n",
      "Iteration: 4605 lambda_n: 1.0265184445902067 Loss: 0.005470628876202497\n",
      "Iteration: 4606 lambda_n: 0.9406383954528253 Loss: 0.005459152495951733\n",
      "Iteration: 4607 lambda_n: 0.8935326354208479 Loss: 0.005448636246708257\n",
      "Iteration: 4608 lambda_n: 0.9484852565454863 Loss: 0.005438646635528659\n",
      "Iteration: 4609 lambda_n: 0.9982619007363579 Loss: 0.005428042659266284\n",
      "Iteration: 4610 lambda_n: 0.9521777216475357 Loss: 0.005416882184870796\n",
      "Iteration: 4611 lambda_n: 0.9949310437451788 Loss: 0.005406236927357013\n",
      "Iteration: 4612 lambda_n: 1.0237187265342536 Loss: 0.005395113691798779\n",
      "Iteration: 4613 lambda_n: 0.9386380709419022 Loss: 0.005383668612740297\n",
      "Iteration: 4614 lambda_n: 0.9339037840554502 Loss: 0.0053731747274899325\n",
      "Iteration: 4615 lambda_n: 1.011482577431267 Loss: 0.005362733771195997\n",
      "Iteration: 4616 lambda_n: 0.9836822290505395 Loss: 0.005351425491383506\n",
      "Iteration: 4617 lambda_n: 0.9349368022367527 Loss: 0.005340428016927975\n",
      "Iteration: 4618 lambda_n: 0.9399970440718427 Loss: 0.005329975511819791\n",
      "Iteration: 4619 lambda_n: 0.9296393782258193 Loss: 0.00531946643375967\n",
      "Iteration: 4620 lambda_n: 1.0427566969921702 Loss: 0.005309073153497093\n",
      "Iteration: 4621 lambda_n: 1.0117484017451488 Loss: 0.005297415232397229\n",
      "Iteration: 4622 lambda_n: 1.0152393759116558 Loss: 0.005286103981188215\n",
      "Iteration: 4623 lambda_n: 1.0137306594411932 Loss: 0.005274753701303394\n",
      "Iteration: 4624 lambda_n: 1.0484911417692642 Loss: 0.005263420288809694\n",
      "Iteration: 4625 lambda_n: 0.9655248931377388 Loss: 0.0052516982575114\n",
      "Iteration: 4626 lambda_n: 0.9347539859794669 Loss: 0.0052409037810681475\n",
      "Iteration: 4627 lambda_n: 1.005158412545352 Loss: 0.005230453320511937\n",
      "Iteration: 4628 lambda_n: 0.9763902384806251 Loss: 0.005219215745245881\n",
      "Iteration: 4629 lambda_n: 1.0238237699053088 Loss: 0.005208299795502611\n",
      "Iteration: 4630 lambda_n: 1.0176247141354244 Loss: 0.0051968535434812405\n",
      "Iteration: 4631 lambda_n: 1.0012091074791318 Loss: 0.00518547659639383\n",
      "Iteration: 4632 lambda_n: 0.9140580064956442 Loss: 0.00517428317430052\n",
      "Iteration: 4633 lambda_n: 0.9758414414085494 Loss: 0.005164064093254826\n",
      "Iteration: 4634 lambda_n: 0.9266426219545281 Loss: 0.0051531542793881585\n",
      "Iteration: 4635 lambda_n: 1.0044251639183341 Loss: 0.005142794503681434\n",
      "Iteration: 4636 lambda_n: 0.9668890566021664 Loss: 0.005131565126681557\n",
      "Iteration: 4637 lambda_n: 0.8921594381659878 Loss: 0.005120755399835689\n",
      "Iteration: 4638 lambda_n: 0.921016002567653 Loss: 0.005110781143016526\n",
      "Iteration: 4639 lambda_n: 0.9396720883869323 Loss: 0.005100484272641459\n",
      "Iteration: 4640 lambda_n: 1.0306712922442367 Loss: 0.005089978829085414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4641 lambda_n: 0.9033304638951356 Loss: 0.005078456023256482\n",
      "Iteration: 4642 lambda_n: 1.0493561745483515 Loss: 0.005068356875699256\n",
      "Iteration: 4643 lambda_n: 0.9307450968885375 Loss: 0.0050566251748356585\n",
      "Iteration: 4644 lambda_n: 0.9919323837881648 Loss: 0.005046219534457029\n",
      "Iteration: 4645 lambda_n: 0.9850777932863084 Loss: 0.005035129826182267\n",
      "Iteration: 4646 lambda_n: 1.0355005872838818 Loss: 0.005024116751638687\n",
      "Iteration: 4647 lambda_n: 0.9306293536052045 Loss: 0.005012539955204392\n",
      "Iteration: 4648 lambda_n: 0.9515357081397863 Loss: 0.005002135609091142\n",
      "Iteration: 4649 lambda_n: 0.9394654942658576 Loss: 0.0049914975320208416\n",
      "Iteration: 4650 lambda_n: 1.0347638569539204 Loss: 0.0049809943988364915\n",
      "Iteration: 4651 lambda_n: 0.9811505531151032 Loss: 0.004969425839261382\n",
      "Iteration: 4652 lambda_n: 1.0108479128691572 Loss: 0.004958456671295673\n",
      "Iteration: 4653 lambda_n: 0.9591463145087697 Loss: 0.004947155489799525\n",
      "Iteration: 4654 lambda_n: 0.9208349184594098 Loss: 0.004936432327219594\n",
      "Iteration: 4655 lambda_n: 0.9094920897595704 Loss: 0.0049261374823853414\n",
      "Iteration: 4656 lambda_n: 1.032205021770795 Loss: 0.004915969449330595\n",
      "Iteration: 4657 lambda_n: 1.028925760771905 Loss: 0.004904429497702404\n",
      "Iteration: 4658 lambda_n: 0.9474202935304941 Loss: 0.0048929262079653755\n",
      "Iteration: 4659 lambda_n: 1.0206342482088857 Loss: 0.004882334141475565\n",
      "Iteration: 4660 lambda_n: 0.9188777039067808 Loss: 0.004870923550177477\n",
      "Iteration: 4661 lambda_n: 1.0183975879347147 Loss: 0.004860650587177889\n",
      "Iteration: 4662 lambda_n: 0.9039356216269436 Loss: 0.004849265001655653\n",
      "Iteration: 4663 lambda_n: 0.923163364142358 Loss: 0.0048391590897934045\n",
      "Iteration: 4664 lambda_n: 0.9349218362659859 Loss: 0.0048288382137050365\n",
      "Iteration: 4665 lambda_n: 1.0301944839240058 Loss: 0.004818385879105496\n",
      "Iteration: 4666 lambda_n: 0.9403995333501688 Loss: 0.004806868405695408\n",
      "Iteration: 4667 lambda_n: 0.8976244913373785 Loss: 0.004796354831099211\n",
      "Iteration: 4668 lambda_n: 1.0378445033633865 Loss: 0.004786319477334107\n",
      "Iteration: 4669 lambda_n: 1.0443156694445639 Loss: 0.004774716477648682\n",
      "Iteration: 4670 lambda_n: 1.0139177950793938 Loss: 0.004763041131030997\n",
      "Iteration: 4671 lambda_n: 1.009045005678936 Loss: 0.0047517056297306124\n",
      "Iteration: 4672 lambda_n: 0.9191606897706257 Loss: 0.004740424605801585\n",
      "Iteration: 4673 lambda_n: 0.9130794827083507 Loss: 0.0047301484797431185\n",
      "Iteration: 4674 lambda_n: 0.9488678800232966 Loss: 0.004719940341033708\n",
      "Iteration: 4675 lambda_n: 1.040353773164131 Loss: 0.004709332091620236\n",
      "Iteration: 4676 lambda_n: 0.9387594550636446 Loss: 0.0046977010389961235\n",
      "Iteration: 4677 lambda_n: 0.8926586626564254 Loss: 0.004687205800893224\n",
      "Iteration: 4678 lambda_n: 1.020278316101774 Loss: 0.004677225965153192\n",
      "Iteration: 4679 lambda_n: 0.9815070010578919 Loss: 0.0046658193543397545\n",
      "Iteration: 4680 lambda_n: 0.904998011416113 Loss: 0.00465484620306029\n",
      "Iteration: 4681 lambda_n: 0.9718291066849022 Loss: 0.00464472841477808\n",
      "Iteration: 4682 lambda_n: 1.0377641946768674 Loss: 0.004633863461511819\n",
      "Iteration: 4683 lambda_n: 0.9621765716442845 Loss: 0.0046222613605453045\n",
      "Iteration: 4684 lambda_n: 1.0161491538903102 Loss: 0.004611504321781121\n",
      "Iteration: 4685 lambda_n: 0.9107890541419468 Loss: 0.004600143874951458\n",
      "Iteration: 4686 lambda_n: 0.9835263628854831 Loss: 0.004589961343649162\n",
      "Iteration: 4687 lambda_n: 1.0483046117509136 Loss: 0.004578965616500635\n",
      "Iteration: 4688 lambda_n: 0.9116086683189109 Loss: 0.004567245675016763\n",
      "Iteration: 4689 lambda_n: 1.0229897146026878 Loss: 0.004557053980664413\n",
      "Iteration: 4690 lambda_n: 0.9630826762840203 Loss: 0.004545617057332848\n",
      "Iteration: 4691 lambda_n: 0.8991248861362159 Loss: 0.004534849888791823\n",
      "Iteration: 4692 lambda_n: 0.9739605036371528 Loss: 0.004524797762033344\n",
      "Iteration: 4693 lambda_n: 0.9815916871132356 Loss: 0.004513908980567561\n",
      "Iteration: 4694 lambda_n: 0.9468230234597385 Loss: 0.004502934883284289\n",
      "Iteration: 4695 lambda_n: 1.0367991734488748 Loss: 0.004492349496249547\n",
      "Iteration: 4696 lambda_n: 0.9906377618467864 Loss: 0.00448075818488055\n",
      "Iteration: 4697 lambda_n: 0.9631796979771292 Loss: 0.004469682953547205\n",
      "Iteration: 4698 lambda_n: 0.9950562574238612 Loss: 0.004458914700681041\n",
      "Iteration: 4699 lambda_n: 0.9653479535006507 Loss: 0.004447790071115367\n",
      "Iteration: 4700 lambda_n: 1.0402132913243005 Loss: 0.004436997577472744\n",
      "Iteration: 4701 lambda_n: 0.8971281210092882 Loss: 0.004425368096890084\n",
      "Iteration: 4702 lambda_n: 0.9842918038138476 Loss: 0.004415338294254602\n",
      "Iteration: 4703 lambda_n: 0.953148499311671 Loss: 0.00440433401041368\n",
      "Iteration: 4704 lambda_n: 1.0471934208886935 Loss: 0.004393677905650058\n",
      "Iteration: 4705 lambda_n: 0.9687957281401399 Loss: 0.004381970388135225\n",
      "Iteration: 4706 lambda_n: 0.9449462842889266 Loss: 0.004371139349023613\n",
      "Iteration: 4707 lambda_n: 0.9326765099796268 Loss: 0.004360574944351556\n",
      "Iteration: 4708 lambda_n: 1.0166379548964575 Loss: 0.004350147714571852\n",
      "Iteration: 4709 lambda_n: 0.9669129009416941 Loss: 0.004338781804313981\n",
      "Iteration: 4710 lambda_n: 1.0442849594787826 Loss: 0.004327971815217847\n",
      "Iteration: 4711 lambda_n: 0.9760813741990011 Loss: 0.004316296814332447\n",
      "Iteration: 4712 lambda_n: 1.0082850669630032 Loss: 0.004305384322727821\n",
      "Iteration: 4713 lambda_n: 0.8971943137485093 Loss: 0.004294111797127048\n",
      "Iteration: 4714 lambda_n: 0.903940803776358 Loss: 0.004284081255016009\n",
      "Iteration: 4715 lambda_n: 1.0270522753432403 Loss: 0.0042739752878666925\n",
      "Iteration: 4716 lambda_n: 0.9465143758296382 Loss: 0.004262492946910826\n",
      "Iteration: 4717 lambda_n: 0.913778488554683 Loss: 0.004251911011605576\n",
      "Iteration: 4718 lambda_n: 0.9544656042475798 Loss: 0.004241695060263221\n",
      "Iteration: 4719 lambda_n: 0.9148082892118171 Loss: 0.004231024231114276\n",
      "Iteration: 4720 lambda_n: 0.9675764393488386 Loss: 0.004220796766789174\n",
      "Iteration: 4721 lambda_n: 0.9415927337985713 Loss: 0.004209979359915965\n",
      "Iteration: 4722 lambda_n: 1.0260120244364848 Loss: 0.004199452448291272\n",
      "Iteration: 4723 lambda_n: 0.9962508782843318 Loss: 0.004187981737574112\n",
      "Iteration: 4724 lambda_n: 0.8988239697011959 Loss: 0.004176843753512074\n",
      "Iteration: 4725 lambda_n: 1.004022915600887 Loss: 0.004166794992477813\n",
      "Iteration: 4726 lambda_n: 0.9368712944016881 Loss: 0.004155570117916219\n",
      "Iteration: 4727 lambda_n: 0.9853187209635799 Loss: 0.004145095991726971\n",
      "Iteration: 4728 lambda_n: 1.035352591286152 Loss: 0.004134080228259186\n",
      "Iteration: 4729 lambda_n: 1.0363455634651797 Loss: 0.004122505091238913\n",
      "Iteration: 4730 lambda_n: 0.9443025192237514 Loss: 0.004110918852940848\n",
      "Iteration: 4731 lambda_n: 0.9266262022295128 Loss: 0.004100361646588622\n",
      "Iteration: 4732 lambda_n: 0.9702927689208066 Loss: 0.0040900020597076535\n",
      "Iteration: 4733 lambda_n: 0.8948658261447959 Loss: 0.004079154285085252\n",
      "Iteration: 4734 lambda_n: 1.0081137333849244 Loss: 0.004069149776063463\n",
      "Iteration: 4735 lambda_n: 0.9267513522057618 Loss: 0.004057879166985646\n",
      "Iteration: 4736 lambda_n: 0.9549037446468561 Loss: 0.004047518181102686\n",
      "Iteration: 4737 lambda_n: 1.045259521199384 Loss: 0.004036842454375088\n",
      "Iteration: 4738 lambda_n: 0.9070891194042958 Loss: 0.004025156559302203\n",
      "Iteration: 4739 lambda_n: 0.9616705439786815 Loss: 0.004015015395302765\n",
      "Iteration: 4740 lambda_n: 1.0492971093116092 Loss: 0.004004264016572844\n",
      "Iteration: 4741 lambda_n: 1.0241481841153277 Loss: 0.003992532981812791\n",
      "Iteration: 4742 lambda_n: 0.9889769504211141 Loss: 0.0039810831095214395\n",
      "Iteration: 4743 lambda_n: 0.9920298210667168 Loss: 0.0039700264480766086\n",
      "Iteration: 4744 lambda_n: 0.9906502637857405 Loss: 0.003958935655880858\n",
      "Iteration: 4745 lambda_n: 1.0097850713298913 Loss: 0.0039478602870321\n",
      "Iteration: 4746 lambda_n: 1.0455241062350389 Loss: 0.003936570993029076\n",
      "Iteration: 4747 lambda_n: 0.9036851812395523 Loss: 0.003924882140309936\n",
      "Iteration: 4748 lambda_n: 0.9524820141822817 Loss: 0.003914779032326294\n",
      "Iteration: 4749 lambda_n: 1.0083358257023178 Loss: 0.003904130380774532\n",
      "Iteration: 4750 lambda_n: 0.9466651059643495 Loss: 0.003892857289361863\n",
      "Iteration: 4751 lambda_n: 0.9196761379341544 Loss: 0.0038822736703308857\n",
      "Iteration: 4752 lambda_n: 0.9142462040797695 Loss: 0.003871991785239216\n",
      "Iteration: 4753 lambda_n: 0.9191399289268126 Loss: 0.0038617706062887037\n",
      "Iteration: 4754 lambda_n: 1.0205561022961254 Loss: 0.0038514947160316875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4755 lambda_n: 0.8942590295644907 Loss: 0.0038400850033847578\n",
      "Iteration: 4756 lambda_n: 0.9257930105471146 Loss: 0.0038300872791080854\n",
      "Iteration: 4757 lambda_n: 0.9296078985233852 Loss: 0.0038197370081916727\n",
      "Iteration: 4758 lambda_n: 1.0020798899389358 Loss: 0.003809344087255053\n",
      "Iteration: 4759 lambda_n: 1.0319246967912266 Loss: 0.0037981409369282444\n",
      "Iteration: 4760 lambda_n: 0.9668120108486139 Loss: 0.003786604124766685\n",
      "Iteration: 4761 lambda_n: 1.012561731212597 Loss: 0.003775795265789923\n",
      "Iteration: 4762 lambda_n: 0.9700119199511021 Loss: 0.0037644749296786976\n",
      "Iteration: 4763 lambda_n: 0.9675811642693215 Loss: 0.003753630296125237\n",
      "Iteration: 4764 lambda_n: 1.0246488717798312 Loss: 0.0037428128382083867\n",
      "Iteration: 4765 lambda_n: 1.0487058455934564 Loss: 0.003731357369229942\n",
      "Iteration: 4766 lambda_n: 0.9955947816022886 Loss: 0.003719632945802018\n",
      "Iteration: 4767 lambda_n: 1.0065224565322988 Loss: 0.0037085022986451653\n",
      "Iteration: 4768 lambda_n: 0.9734184866045388 Loss: 0.003697249481247223\n",
      "Iteration: 4769 lambda_n: 0.949346358738993 Loss: 0.00368636676286204\n",
      "Iteration: 4770 lambda_n: 1.0215122786046784 Loss: 0.003675753168422205\n",
      "Iteration: 4771 lambda_n: 0.9068188621385465 Loss: 0.0036643327664742245\n",
      "Iteration: 4772 lambda_n: 0.9476233734339945 Loss: 0.003654194625132187\n",
      "Iteration: 4773 lambda_n: 0.9841145497134717 Loss: 0.003643600293595771\n",
      "Iteration: 4774 lambda_n: 1.0426800921885502 Loss: 0.00363259799450945\n",
      "Iteration: 4775 lambda_n: 0.9066360442985999 Loss: 0.003620940938742341\n",
      "Iteration: 4776 lambda_n: 0.8981791792907841 Loss: 0.0036108048414189337\n",
      "Iteration: 4777 lambda_n: 0.9152277183508069 Loss: 0.0036007632910033563\n",
      "Iteration: 4778 lambda_n: 0.9478216001030134 Loss: 0.0035905311397134614\n",
      "Iteration: 4779 lambda_n: 1.0328116749714573 Loss: 0.003579934592226895\n",
      "Iteration: 4780 lambda_n: 0.9595184652678267 Loss: 0.003568387864527349\n",
      "Iteration: 4781 lambda_n: 1.0326950890968805 Loss: 0.0035576605473679065\n",
      "Iteration: 4782 lambda_n: 0.9033222621168803 Loss: 0.0035461151231620464\n",
      "Iteration: 4783 lambda_n: 0.9641204591584293 Loss: 0.003536016073804974\n",
      "Iteration: 4784 lambda_n: 0.9666774246802874 Loss: 0.0035252373069318576\n",
      "Iteration: 4785 lambda_n: 0.906630766235927 Loss: 0.003514429953482716\n",
      "Iteration: 4786 lambda_n: 1.0170554382267432 Loss: 0.0035042939154849777\n",
      "Iteration: 4787 lambda_n: 1.0443237913515395 Loss: 0.0034929233411324844\n",
      "Iteration: 4788 lambda_n: 0.959634580523797 Loss: 0.003481247909457045\n",
      "Iteration: 4789 lambda_n: 1.0344945087479558 Loss: 0.0034705192944106487\n",
      "Iteration: 4790 lambda_n: 0.9757832160039729 Loss: 0.00345895375316857\n",
      "Iteration: 4791 lambda_n: 0.9010827215203172 Loss: 0.0034480445981214048\n",
      "Iteration: 4792 lambda_n: 0.9314617080381689 Loss: 0.003437970586877703\n",
      "Iteration: 4793 lambda_n: 0.9722189778592095 Loss: 0.003427556941748975\n",
      "Iteration: 4794 lambda_n: 0.9202845128442895 Loss: 0.0034166876346091403\n",
      "Iteration: 4795 lambda_n: 0.931820378488001 Loss: 0.0034063989494219796\n",
      "Iteration: 4796 lambda_n: 0.9001110144013543 Loss: 0.0033959812944855377\n",
      "Iteration: 4797 lambda_n: 0.9826053327879727 Loss: 0.003385918146970923\n",
      "Iteration: 4798 lambda_n: 0.9598473574075539 Loss: 0.003374932721587603\n",
      "Iteration: 4799 lambda_n: 1.0330371011606159 Loss: 0.003364201728033042\n",
      "Iteration: 4800 lambda_n: 0.9543093050005513 Loss: 0.0033526524807943733\n",
      "Iteration: 4801 lambda_n: 0.941143588557612 Loss: 0.003341983402153263\n",
      "Iteration: 4802 lambda_n: 0.9190164489597857 Loss: 0.0033314615148807873\n",
      "Iteration: 4803 lambda_n: 0.9934493545528383 Loss: 0.0033211870067525536\n",
      "Iteration: 4804 lambda_n: 0.9877467316540215 Loss: 0.0033100803465267105\n",
      "Iteration: 4805 lambda_n: 1.0163071740642882 Loss: 0.00329903744106275\n",
      "Iteration: 4806 lambda_n: 1.009803265112629 Loss: 0.0032876752328636714\n",
      "Iteration: 4807 lambda_n: 1.0114144530641775 Loss: 0.0032763857377217562\n",
      "Iteration: 4808 lambda_n: 1.0055840146566737 Loss: 0.003265078229699835\n",
      "Iteration: 4809 lambda_n: 0.9555498542878296 Loss: 0.003253835905403689\n",
      "Iteration: 4810 lambda_n: 0.9766663024003885 Loss: 0.0032431529578275534\n",
      "Iteration: 4811 lambda_n: 0.9121833639807339 Loss: 0.003232233930596751\n",
      "Iteration: 4812 lambda_n: 1.0252534093318197 Loss: 0.003222035815909505\n",
      "Iteration: 4813 lambda_n: 0.9283734642837346 Loss: 0.003210573589960621\n",
      "Iteration: 4814 lambda_n: 0.9910303921171778 Loss: 0.0032001944717007934\n",
      "Iteration: 4815 lambda_n: 0.9644301147333488 Loss: 0.0031891148555681526\n",
      "Iteration: 4816 lambda_n: 0.9423296303142197 Loss: 0.003178332627784487\n",
      "Iteration: 4817 lambda_n: 1.0378524276533905 Loss: 0.0031677974811328163\n",
      "Iteration: 4818 lambda_n: 0.9888498448189981 Loss: 0.0031561943996337464\n",
      "Iteration: 4819 lambda_n: 0.9869121680589346 Loss: 0.003145139161912897\n",
      "Iteration: 4820 lambda_n: 0.9488508447664509 Loss: 0.003134105587245336\n",
      "Iteration: 4821 lambda_n: 0.9648329044396797 Loss: 0.0031234975342142274\n",
      "Iteration: 4822 lambda_n: 0.9857897887253363 Loss: 0.0031127108034583284\n",
      "Iteration: 4823 lambda_n: 0.9337824287802767 Loss: 0.003101689776961295\n",
      "Iteration: 4824 lambda_n: 1.0461868980173261 Loss: 0.00309125018732427\n",
      "Iteration: 4825 lambda_n: 0.9890148587459419 Loss: 0.0030795539275373096\n",
      "Iteration: 4826 lambda_n: 0.9932195746719911 Loss: 0.0030684968451836904\n",
      "Iteration: 4827 lambda_n: 0.8937250140856396 Loss: 0.003057392754576424\n",
      "Iteration: 4828 lambda_n: 0.9802163658390795 Loss: 0.0030474010027409498\n",
      "Iteration: 4829 lambda_n: 0.9594391445660904 Loss: 0.0030364422866984444\n",
      "Iteration: 4830 lambda_n: 1.0372010591501462 Loss: 0.0030257158578357165\n",
      "Iteration: 4831 lambda_n: 0.9994236120288649 Loss: 0.0030141200589646846\n",
      "Iteration: 4832 lambda_n: 0.9870611845060928 Loss: 0.003002946608013676\n",
      "Iteration: 4833 lambda_n: 1.0194581255197832 Loss: 0.002991911367731788\n",
      "Iteration: 4834 lambda_n: 0.9307258596504968 Loss: 0.002980513933084025\n",
      "Iteration: 4835 lambda_n: 0.9737909137535246 Loss: 0.002970108515865723\n",
      "Iteration: 4836 lambda_n: 0.9850162086794249 Loss: 0.002959221635898007\n",
      "Iteration: 4837 lambda_n: 0.9455276744717218 Loss: 0.002948209258341275\n",
      "Iteration: 4838 lambda_n: 0.938791844283493 Loss: 0.00293763835846786\n",
      "Iteration: 4839 lambda_n: 0.9410330647162428 Loss: 0.002927142764491972\n",
      "Iteration: 4840 lambda_n: 0.9022596996683887 Loss: 0.002916622113932435\n",
      "Iteration: 4841 lambda_n: 0.9065960066416897 Loss: 0.0029065349455344894\n",
      "Iteration: 4842 lambda_n: 1.0414864995666373 Loss: 0.0028963992977041495\n",
      "Iteration: 4843 lambda_n: 0.9413338686313646 Loss: 0.0028847555884019055\n",
      "Iteration: 4844 lambda_n: 1.0102281401918414 Loss: 0.0028742315749854404\n",
      "Iteration: 4845 lambda_n: 0.9866920823216085 Loss: 0.002862937330901861\n",
      "Iteration: 4846 lambda_n: 0.9825442948120995 Loss: 0.002851906217490809\n",
      "Iteration: 4847 lambda_n: 0.9620381527709264 Loss: 0.0028409214759330384\n",
      "Iteration: 4848 lambda_n: 0.9200419153305938 Loss: 0.002830165990905532\n",
      "Iteration: 4849 lambda_n: 1.0415140243512881 Loss: 0.0028198800194048097\n",
      "Iteration: 4850 lambda_n: 1.0291694175628823 Loss: 0.0028082360025727037\n",
      "Iteration: 4851 lambda_n: 0.8984135772868547 Loss: 0.0027967299971699017\n",
      "Iteration: 4852 lambda_n: 1.0197157147774314 Loss: 0.002786685828277529\n",
      "Iteration: 4853 lambda_n: 0.9702038739032095 Loss: 0.002775285514325818\n",
      "Iteration: 4854 lambda_n: 0.9216885523646999 Loss: 0.0027644387375499705\n",
      "Iteration: 4855 lambda_n: 1.0473566830168424 Loss: 0.002754134356965212\n",
      "Iteration: 4856 lambda_n: 1.0143752060806461 Loss: 0.0027424250199893207\n",
      "Iteration: 4857 lambda_n: 0.9775612544759643 Loss: 0.002731084412466101\n",
      "Iteration: 4858 lambda_n: 0.921943367074365 Loss: 0.0027201553810541276\n",
      "Iteration: 4859 lambda_n: 1.0004153440177281 Loss: 0.0027098481517629927\n",
      "Iteration: 4860 lambda_n: 0.9802261694792869 Loss: 0.0026986636140966724\n",
      "Iteration: 4861 lambda_n: 0.9153279097953539 Loss: 0.002687704789290163\n",
      "Iteration: 4862 lambda_n: 0.9898092314467949 Loss: 0.0026774715201796097\n",
      "Iteration: 4863 lambda_n: 0.9591735599089123 Loss: 0.0026664055578023253\n",
      "Iteration: 4864 lambda_n: 0.9541346392069615 Loss: 0.002655682099012064\n",
      "Iteration: 4865 lambda_n: 0.9598181539800152 Loss: 0.0026450149748445135\n",
      "Iteration: 4866 lambda_n: 0.9031985223935654 Loss: 0.0026342843096071314\n",
      "Iteration: 4867 lambda_n: 0.9840103126357281 Loss: 0.0026241866458720036\n",
      "Iteration: 4868 lambda_n: 0.9408716129581339 Loss: 0.002613185514910912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4869 lambda_n: 1.0329974700151379 Loss: 0.002602666670061439\n",
      "Iteration: 4870 lambda_n: 0.9249614357994229 Loss: 0.0025911178679246544\n",
      "Iteration: 4871 lambda_n: 1.0345510444710295 Loss: 0.0025807768972139437\n",
      "Iteration: 4872 lambda_n: 0.9645352759272567 Loss: 0.002569210726329794\n",
      "Iteration: 4873 lambda_n: 1.0038515259609655 Loss: 0.0025584273243320636\n",
      "Iteration: 4874 lambda_n: 0.918964629909766 Loss: 0.0025472043708562203\n",
      "Iteration: 4875 lambda_n: 0.9711973432061889 Loss: 0.0025369304438878676\n",
      "Iteration: 4876 lambda_n: 1.010028863342701 Loss: 0.002526072560753727\n",
      "Iteration: 4877 lambda_n: 0.9381686713179518 Loss: 0.0025147805453730357\n",
      "Iteration: 4878 lambda_n: 0.8932087329180726 Loss: 0.0025042919193272173\n",
      "Iteration: 4879 lambda_n: 1.0232575469142347 Loss: 0.0024943059406349697\n",
      "Iteration: 4880 lambda_n: 0.9079631652331882 Loss: 0.0024828660300474305\n",
      "Iteration: 4881 lambda_n: 1.0444345785663813 Loss: 0.0024727150984118385\n",
      "Iteration: 4882 lambda_n: 0.9790994520776858 Loss: 0.0024610384309099835\n",
      "Iteration: 4883 lambda_n: 0.9886679341275139 Loss: 0.002450092203197444\n",
      "Iteration: 4884 lambda_n: 0.9039916593480389 Loss: 0.002439039000896388\n",
      "Iteration: 4885 lambda_n: 0.9054717125253108 Loss: 0.0024289324703581448\n",
      "Iteration: 4886 lambda_n: 0.9759570125903432 Loss: 0.002418809393002075\n",
      "Iteration: 4887 lambda_n: 1.0478899596413795 Loss: 0.0024078982975152944\n",
      "Iteration: 4888 lambda_n: 1.013270887056095 Loss: 0.0023961829993632945\n",
      "Iteration: 4889 lambda_n: 0.9371873897761566 Loss: 0.0023848547387806222\n",
      "Iteration: 4890 lambda_n: 0.969132083061939 Loss: 0.0023743770836092415\n",
      "Iteration: 4891 lambda_n: 1.0351167060208173 Loss: 0.0023635422901854466\n",
      "Iteration: 4892 lambda_n: 1.0000405482736185 Loss: 0.002351969795727758\n",
      "Iteration: 4893 lambda_n: 0.9353439398739181 Loss: 0.0023407894489978673\n",
      "Iteration: 4894 lambda_n: 0.9534086040010122 Loss: 0.0023303324034747268\n",
      "Iteration: 4895 lambda_n: 0.9854489417110321 Loss: 0.002319673396953318\n",
      "Iteration: 4896 lambda_n: 1.0478329295395274 Loss: 0.002308656182895029\n",
      "Iteration: 4897 lambda_n: 0.9188860371112751 Loss: 0.0022969415225310343\n",
      "Iteration: 4898 lambda_n: 0.9264291758089427 Loss: 0.002286668474690174\n",
      "Iteration: 4899 lambda_n: 0.9206408016437637 Loss: 0.0022763110953834577\n",
      "Iteration: 4900 lambda_n: 0.9924212736250435 Loss: 0.0022660184295019074\n",
      "Iteration: 4901 lambda_n: 1.0019189278117084 Loss: 0.002254923265627499\n",
      "Iteration: 4902 lambda_n: 0.9846900309759893 Loss: 0.002243721919016283\n",
      "Iteration: 4903 lambda_n: 0.9982514606527594 Loss: 0.002232713189654344\n",
      "Iteration: 4904 lambda_n: 1.007989795898018 Loss: 0.0022215528449796777\n",
      "Iteration: 4905 lambda_n: 0.9797376783348961 Loss: 0.0022102836267803372\n",
      "Iteration: 4906 lambda_n: 0.9701981798514813 Loss: 0.002199330264258527\n",
      "Iteration: 4907 lambda_n: 0.9056174191605849 Loss: 0.0021884835523315565\n",
      "Iteration: 4908 lambda_n: 0.9784032088516558 Loss: 0.0021783588464245264\n",
      "Iteration: 4909 lambda_n: 0.9682334176260294 Loss: 0.0021674204031917194\n",
      "Iteration: 4910 lambda_n: 0.8991381220855887 Loss: 0.002156595657158303\n",
      "Iteration: 4911 lambda_n: 0.9100783260652974 Loss: 0.0021465433891570563\n",
      "Iteration: 4912 lambda_n: 0.9705315871019425 Loss: 0.002136368810864069\n",
      "Iteration: 4913 lambda_n: 0.976120627715228 Loss: 0.002125518371601743\n",
      "Iteration: 4914 lambda_n: 1.0075514453804726 Loss: 0.0021146054474846126\n",
      "Iteration: 4915 lambda_n: 1.023565076223901 Loss: 0.002103341130212674\n",
      "Iteration: 4916 lambda_n: 1.0426125333296852 Loss: 0.002091897782285469\n",
      "Iteration: 4917 lambda_n: 0.9748742245990619 Loss: 0.0020802414858514546\n",
      "Iteration: 4918 lambda_n: 0.9369118268393104 Loss: 0.0020693424964751514\n",
      "Iteration: 4919 lambda_n: 0.8923097991061949 Loss: 0.002058867922658496\n",
      "Iteration: 4920 lambda_n: 0.8965452457555945 Loss: 0.002048891994750513\n",
      "Iteration: 4921 lambda_n: 0.9664333685495969 Loss: 0.0020388687150208118\n",
      "Iteration: 4922 lambda_n: 0.9403712906450424 Loss: 0.0020280640935903442\n",
      "Iteration: 4923 lambda_n: 0.9089686212294218 Loss: 0.0020175508434167595\n",
      "Iteration: 4924 lambda_n: 1.0430415413911855 Loss: 0.0020073886717387546\n",
      "Iteration: 4925 lambda_n: 0.9352707091476421 Loss: 0.0019957275792176444\n",
      "Iteration: 4926 lambda_n: 0.9050934978384974 Loss: 0.001985271353061056\n",
      "Iteration: 4927 lambda_n: 0.9619720180558936 Loss: 0.00197515250490905\n",
      "Iteration: 4928 lambda_n: 0.9559132643786935 Loss: 0.0019643977610287657\n",
      "Iteration: 4929 lambda_n: 1.0225361357914995 Loss: 0.0019537107533896575\n",
      "Iteration: 4930 lambda_n: 0.9661306100880831 Loss: 0.0019422790033929928\n",
      "Iteration: 4931 lambda_n: 0.9157025215413425 Loss: 0.0019314778130879733\n",
      "Iteration: 4932 lambda_n: 0.9307795573568507 Loss: 0.0019212403794454053\n",
      "Iteration: 4933 lambda_n: 1.0072881927893493 Loss: 0.0019108343747964497\n",
      "Iteration: 4934 lambda_n: 1.0000312311008042 Loss: 0.001899573005250464\n",
      "Iteration: 4935 lambda_n: 0.9987439413065242 Loss: 0.0018883927632402442\n",
      "Iteration: 4936 lambda_n: 0.9559124072862625 Loss: 0.0018772268213847916\n",
      "Iteration: 4937 lambda_n: 1.0023105950119666 Loss: 0.0018665397778054073\n",
      "Iteration: 4938 lambda_n: 0.9643756117119727 Loss: 0.0018553340342053841\n",
      "Iteration: 4939 lambda_n: 1.0245943366900363 Loss: 0.0018445524174261155\n",
      "Iteration: 4940 lambda_n: 0.9122955954489309 Loss: 0.0018330975648209041\n",
      "Iteration: 4941 lambda_n: 1.039308060647694 Loss: 0.0018228981984609515\n",
      "Iteration: 4942 lambda_n: 1.0050925072787338 Loss: 0.0018112788453912194\n",
      "Iteration: 4943 lambda_n: 0.9283286682752567 Loss: 0.0018000420178586043\n",
      "Iteration: 4944 lambda_n: 0.9294890202597488 Loss: 0.0017896634017009196\n",
      "Iteration: 4945 lambda_n: 0.9184813505366728 Loss: 0.0017792718129559235\n",
      "Iteration: 4946 lambda_n: 1.0152381361247085 Loss: 0.0017690032889000997\n",
      "Iteration: 4947 lambda_n: 1.0319052567572735 Loss: 0.0017576530344064144\n",
      "Iteration: 4948 lambda_n: 1.0154822370865044 Loss: 0.0017461164434629337\n",
      "Iteration: 4949 lambda_n: 0.9964215708736662 Loss: 0.0017347634603146222\n",
      "Iteration: 4950 lambda_n: 0.9256722422699191 Loss: 0.0017236235735452919\n",
      "Iteration: 4951 lambda_n: 1.0099892444029932 Loss: 0.0017132746568480513\n",
      "Iteration: 4952 lambda_n: 1.0053140485798133 Loss: 0.00170198308520074\n",
      "Iteration: 4953 lambda_n: 0.9930461145466366 Loss: 0.0016907437818561007\n",
      "Iteration: 4954 lambda_n: 0.9831058636213901 Loss: 0.0016796416327969925\n",
      "Iteration: 4955 lambda_n: 1.0108396615042046 Loss: 0.0016686506147604702\n",
      "Iteration: 4956 lambda_n: 0.9374798235400843 Loss: 0.0016573495359147971\n",
      "Iteration: 4957 lambda_n: 0.9416685897296013 Loss: 0.0016468686122410056\n",
      "Iteration: 4958 lambda_n: 0.8956088955955276 Loss: 0.0016363408586632956\n",
      "Iteration: 4959 lambda_n: 1.0065803238620221 Loss: 0.0016263280475580539\n",
      "Iteration: 4960 lambda_n: 0.9233526971061555 Loss: 0.0016150745878664454\n",
      "Iteration: 4961 lambda_n: 1.022773624592077 Loss: 0.0016047516041263075\n",
      "Iteration: 4962 lambda_n: 1.0150217054595758 Loss: 0.0015933171051599091\n",
      "Iteration: 4963 lambda_n: 0.9322700601699848 Loss: 0.0015819692718543634\n",
      "Iteration: 4964 lambda_n: 0.9997326545752275 Loss: 0.0015715465930612356\n",
      "Iteration: 4965 lambda_n: 1.018109742062825 Loss: 0.0015603696897654646\n",
      "Iteration: 4966 lambda_n: 0.9146063572048585 Loss: 0.0015489873326459773\n",
      "Iteration: 4967 lambda_n: 0.8994777149090871 Loss: 0.0015387621322360575\n",
      "Iteration: 4968 lambda_n: 0.9991249646418644 Loss: 0.0015287060684398132\n",
      "Iteration: 4969 lambda_n: 0.980050647555253 Loss: 0.001517535959179187\n",
      "Iteration: 4970 lambda_n: 0.9698930370767159 Loss: 0.0015065790987608546\n",
      "Iteration: 4971 lambda_n: 0.9335168285001075 Loss: 0.0014957357993617788\n",
      "Iteration: 4972 lambda_n: 0.9399992666471073 Loss: 0.0014852991820726792\n",
      "Iteration: 4973 lambda_n: 0.9645559217152504 Loss: 0.0014747900918501838\n",
      "Iteration: 4974 lambda_n: 1.0181684424214967 Loss: 0.001464006460902364\n",
      "Iteration: 4975 lambda_n: 1.0356410999611463 Loss: 0.001452623447794022\n",
      "Iteration: 4976 lambda_n: 1.0395364755869383 Loss: 0.0014410450922909674\n",
      "Iteration: 4977 lambda_n: 0.9615943759525216 Loss: 0.0014294231869374432\n",
      "Iteration: 4978 lambda_n: 1.0404681024795546 Loss: 0.001418672665852638\n",
      "Iteration: 4979 lambda_n: 1.0124791522499528 Loss: 0.001407040345064351\n",
      "Iteration: 4980 lambda_n: 0.9251511971727098 Loss: 0.001395720937737435\n",
      "Iteration: 4981 lambda_n: 0.9514175921886429 Loss: 0.0013853778475191821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4982 lambda_n: 0.9629188668896825 Loss: 0.001374741101870745\n",
      "Iteration: 4983 lambda_n: 1.0400401620811561 Loss: 0.0013639757732397976\n",
      "Iteration: 4984 lambda_n: 1.0177799944692638 Loss: 0.0013523482369110957\n",
      "Iteration: 4985 lambda_n: 0.9081034144822719 Loss: 0.0013409695668734722\n",
      "Iteration: 4986 lambda_n: 0.9434151337160868 Loss: 0.001330817069138415\n",
      "Iteration: 4987 lambda_n: 0.9891832079586282 Loss: 0.0013202697902323093\n",
      "Iteration: 4988 lambda_n: 1.0342425763364718 Loss: 0.001309210829242506\n",
      "Iteration: 4989 lambda_n: 1.0096301092249012 Loss: 0.0012976481094270351\n",
      "Iteration: 4990 lambda_n: 0.9414390411555646 Loss: 0.0012863605543507743\n",
      "Iteration: 4991 lambda_n: 1.0347114860079083 Loss: 0.0012758353680411341\n",
      "Iteration: 4992 lambda_n: 0.978146087585271 Loss: 0.0012642674059455735\n",
      "Iteration: 4993 lambda_n: 0.933365319584636 Loss: 0.0012533318388903434\n",
      "Iteration: 4994 lambda_n: 0.939002238268165 Loss: 0.001242896915982904\n",
      "Iteration: 4995 lambda_n: 0.911176161674612 Loss: 0.0012323989729596198\n",
      "Iteration: 4996 lambda_n: 1.0454310461937548 Loss: 0.0012222121224724628\n",
      "Iteration: 4997 lambda_n: 0.9238237850774011 Loss: 0.0012105243169885193\n",
      "Iteration: 4998 lambda_n: 1.0197717696520772 Loss: 0.0012001960674938108\n",
      "Iteration: 4999 lambda_n: 1.0458057889496695 Loss: 0.0011887951299866324\n",
      "Iteration: 5000 lambda_n: 0.904977309338108 Loss: 0.0011771031350031162\n",
      "Iteration: 5001 lambda_n: 0.951564437006396 Loss: 0.0011669855871315007\n",
      "Iteration: 5002 lambda_n: 0.9473234903571341 Loss: 0.001156347200264106\n",
      "Iteration: 5003 lambda_n: 0.9624946415885076 Loss: 0.0011457562267437234\n",
      "Iteration: 5004 lambda_n: 1.0345237671555603 Loss: 0.0011349956414291374\n",
      "Iteration: 5005 lambda_n: 0.9762147216774499 Loss: 0.0011234297783599741\n",
      "Iteration: 5006 lambda_n: 0.9837629783542264 Loss: 0.0011125158040987556\n",
      "Iteration: 5007 lambda_n: 0.9479098956542414 Loss: 0.0011015174411776218\n",
      "Iteration: 5008 lambda_n: 0.9196747487063335 Loss: 0.0010909199118421095\n",
      "Iteration: 5009 lambda_n: 0.9513889868006274 Loss: 0.0010806380483981564\n",
      "Iteration: 5010 lambda_n: 0.8954450095852625 Loss: 0.001070001623255213\n",
      "Iteration: 5011 lambda_n: 0.9610345866826221 Loss: 0.0010599906457035859\n",
      "Iteration: 5012 lambda_n: 1.0309909035887102 Loss: 0.0010492463838592842\n",
      "Iteration: 5013 lambda_n: 0.9816696041085594 Loss: 0.001037720018053708\n",
      "Iteration: 5014 lambda_n: 0.9726604360247912 Loss: 0.0010267450590262127\n",
      "Iteration: 5015 lambda_n: 0.9958314718793421 Loss: 0.0010158708215448429\n",
      "Iteration: 5016 lambda_n: 1.0108201575744529 Loss: 0.0010047375344435345\n",
      "Iteration: 5017 lambda_n: 1.0281878994090932 Loss: 0.0009934366755059848\n",
      "Iteration: 5018 lambda_n: 0.9817306023695166 Loss: 0.00098194164714654\n",
      "Iteration: 5019 lambda_n: 0.935578698690419 Loss: 0.0009709660063254638\n",
      "Iteration: 5020 lambda_n: 0.9224796756906148 Loss: 0.0009605063387742568\n",
      "Iteration: 5021 lambda_n: 0.9460919730465783 Loss: 0.0009501931168981603\n",
      "Iteration: 5022 lambda_n: 1.035640132070279 Loss: 0.0009396159121514726\n",
      "Iteration: 5023 lambda_n: 1.0441482569851441 Loss: 0.0009280375688200657\n",
      "Iteration: 5024 lambda_n: 0.9143039795184241 Loss: 0.000916364105623461\n",
      "Iteration: 5025 lambda_n: 0.9002437495782551 Loss: 0.0009061422872694413\n",
      "Iteration: 5026 lambda_n: 0.9646417172401083 Loss: 0.0008960776607782822\n",
      "Iteration: 5027 lambda_n: 1.000350263251536 Loss: 0.0008852930720985671\n",
      "Iteration: 5028 lambda_n: 0.9184637408267674 Loss: 0.0008741092658272193\n",
      "Iteration: 5029 lambda_n: 0.9447771373887778 Loss: 0.0008638409419374169\n",
      "Iteration: 5030 lambda_n: 0.9592659089300242 Loss: 0.0008532784371972165\n",
      "Iteration: 5031 lambda_n: 1.0050542308739447 Loss: 0.0008425539496207254\n",
      "Iteration: 5032 lambda_n: 1.0018341288468984 Loss: 0.0008313175536741224\n",
      "Iteration: 5033 lambda_n: 0.9041773096956897 Loss: 0.0008201171581615791\n",
      "Iteration: 5034 lambda_n: 0.965535784296862 Loss: 0.0008100085552034899\n",
      "Iteration: 5035 lambda_n: 0.9386076564222336 Loss: 0.0007992139712858462\n",
      "Iteration: 5036 lambda_n: 0.9299769254734982 Loss: 0.00078872044092396\n",
      "Iteration: 5037 lambda_n: 0.9924327271578004 Loss: 0.0007783234012312966\n",
      "Iteration: 5038 lambda_n: 0.8952808654810958 Loss: 0.0007672281125994233\n",
      "Iteration: 5039 lambda_n: 0.926239214969377 Loss: 0.0007572189711364575\n",
      "Iteration: 5040 lambda_n: 0.9347269544094464 Loss: 0.0007468637187857822\n",
      "Iteration: 5041 lambda_n: 0.9442293556070457 Loss: 0.000736413574497325\n",
      "Iteration: 5042 lambda_n: 1.037415534756108 Loss: 0.0007258571944684764\n",
      "Iteration: 5043 lambda_n: 1.0264510364909736 Loss: 0.0007142590033034726\n",
      "Iteration: 5044 lambda_n: 0.9377535858821713 Loss: 0.0007027833940945843\n",
      "Iteration: 5045 lambda_n: 0.9637645087857691 Loss: 0.0006922994126585825\n",
      "Iteration: 5046 lambda_n: 0.9844342694211148 Loss: 0.0006815246320517033\n",
      "Iteration: 5047 lambda_n: 0.9444236400234258 Loss: 0.0006705187658871653\n",
      "Iteration: 5048 lambda_n: 1.0319808383949975 Loss: 0.0006599602142137402\n",
      "Iteration: 5049 lambda_n: 0.99247088847252 Loss: 0.0006484227828553039\n",
      "Iteration: 5050 lambda_n: 1.0089098053828331 Loss: 0.0006373270684612291\n",
      "Iteration: 5051 lambda_n: 1.0415161335116396 Loss: 0.0006260475689111005\n",
      "Iteration: 5052 lambda_n: 0.9458837299288972 Loss: 0.000614403534360402\n",
      "Iteration: 5053 lambda_n: 0.9957213121699758 Loss: 0.0006038286595755763\n",
      "Iteration: 5054 lambda_n: 0.9286887108652349 Loss: 0.0005926966063059304\n",
      "Iteration: 5055 lambda_n: 0.9807192564475535 Loss: 0.000582313970192566\n",
      "Iteration: 5056 lambda_n: 0.9771368165279806 Loss: 0.000571349638548777\n",
      "Iteration: 5057 lambda_n: 0.9524385399166907 Loss: 0.0005604253583714754\n",
      "Iteration: 5058 lambda_n: 0.9528415982207578 Loss: 0.0005497772023687903\n",
      "Iteration: 5059 lambda_n: 0.9815792941914456 Loss: 0.0005391245404529663\n",
      "Iteration: 5060 lambda_n: 0.9752782085315083 Loss: 0.0005281505946080403\n",
      "Iteration: 5061 lambda_n: 1.0047938412505353 Loss: 0.0005172470945305272\n",
      "Iteration: 5062 lambda_n: 1.0036251887144114 Loss: 0.0005060136134431292\n",
      "Iteration: 5063 lambda_n: 0.984113630961422 Loss: 0.0004947931983121053\n",
      "Iteration: 5064 lambda_n: 1.0313189440307273 Loss: 0.0004837909208955859\n",
      "Iteration: 5065 lambda_n: 1.0493723583860104 Loss: 0.00047226089456884094\n",
      "Iteration: 5066 lambda_n: 1.0330914877935125 Loss: 0.00046052903500374973\n",
      "Iteration: 5067 lambda_n: 1.0465541140305328 Loss: 0.0004489792926564037\n",
      "Iteration: 5068 lambda_n: 1.0011116295482652 Loss: 0.0004372804980023087\n",
      "Iteration: 5069 lambda_n: 0.9490261033249924 Loss: 0.0004262612264994847\n",
      "Iteration: 5070 lambda_n: 0.997168654144831 Loss: 0.00041605486497053304\n",
      "Iteration: 5071 lambda_n: 0.9066257616174567 Loss: 0.00040560138006328256\n",
      "Iteration: 5072 lambda_n: 0.9962229540827148 Loss: 0.0003963639670088585\n",
      "Iteration: 5073 lambda_n: 0.9499136735088791 Loss: 0.00038648143941972873\n",
      "Iteration: 5074 lambda_n: 0.9728423641536206 Loss: 0.0003773337494588914\n",
      "Iteration: 5075 lambda_n: 0.913883289480246 Loss: 0.0003682280414833361\n",
      "Iteration: 5076 lambda_n: 0.9838961310864969 Loss: 0.00035991961064261405\n",
      "Iteration: 5077 lambda_n: 1.0304089912510166 Loss: 0.00035121631303595\n",
      "Iteration: 5078 lambda_n: 1.0149960693972555 Loss: 0.00034236609110073403\n",
      "Iteration: 5079 lambda_n: 0.8922109380671479 Loss: 0.00033391225724687003\n",
      "Iteration: 5080 lambda_n: 1.0103300097191943 Loss: 0.0003267016065045735\n",
      "Iteration: 5081 lambda_n: 1.0474622961239817 Loss: 0.0003187505372639232\n",
      "Iteration: 5082 lambda_n: 1.022360157564987 Loss: 0.00031075112400397246\n",
      "Iteration: 5083 lambda_n: 0.9434862165077651 Loss: 0.0003031821268959835\n",
      "Iteration: 5084 lambda_n: 0.977000307075257 Loss: 0.0002964048959926946\n",
      "Iteration: 5085 lambda_n: 0.9054798036893973 Loss: 0.0002895802217588109\n",
      "Iteration: 5086 lambda_n: 1.0473469007144163 Loss: 0.00028343474676932373\n",
      "Iteration: 5087 lambda_n: 0.9395409340801285 Loss: 0.00027651503836997166\n",
      "Iteration: 5088 lambda_n: 0.9762608828697907 Loss: 0.00027049621657640563\n",
      "Iteration: 5089 lambda_n: 0.9565586541205416 Loss: 0.00026441356012074825\n",
      "Iteration: 5090 lambda_n: 0.9119581596806509 Loss: 0.0002586229777143168\n",
      "Iteration: 5091 lambda_n: 0.9013491902023343 Loss: 0.00025325588750046426\n",
      "Iteration: 5092 lambda_n: 1.0269046605727643 Loss: 0.00024809205949382013\n",
      "Iteration: 5093 lambda_n: 0.928684581224454 Loss: 0.00024236430943231334\n",
      "Iteration: 5094 lambda_n: 0.9128432141851879 Loss: 0.00023733867717627203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5095 lambda_n: 0.9894660959944284 Loss: 0.00023253227695615726\n",
      "Iteration: 5096 lambda_n: 0.901108847658353 Loss: 0.00022746145165100326\n",
      "Iteration: 5097 lambda_n: 1.0338860209494205 Loss: 0.00022297591675947458\n",
      "Iteration: 5098 lambda_n: 0.9765224308909379 Loss: 0.00021796534339884512\n",
      "Iteration: 5099 lambda_n: 0.9430617667575454 Loss: 0.00021337472580198785\n",
      "Iteration: 5100 lambda_n: 1.045127015942958 Loss: 0.0002090670469787868\n",
      "Iteration: 5101 lambda_n: 0.9034039301991066 Loss: 0.00020442461577185945\n",
      "Iteration: 5102 lambda_n: 0.9353116658848064 Loss: 0.00020053261287532515\n",
      "Iteration: 5103 lambda_n: 1.0432942186143406 Loss: 0.00019660884770230855\n",
      "Iteration: 5104 lambda_n: 0.9518185735017747 Loss: 0.0001923512709601275\n",
      "Iteration: 5105 lambda_n: 0.9101934174445919 Loss: 0.00018858363257374082\n",
      "Iteration: 5106 lambda_n: 1.025273090769545 Loss: 0.00018507951335578921\n",
      "Iteration: 5107 lambda_n: 0.9519266064091833 Loss: 0.00018123650608295564\n",
      "Iteration: 5108 lambda_n: 0.9316570206088292 Loss: 0.00017777322510866017\n",
      "Iteration: 5109 lambda_n: 0.9466630932983611 Loss: 0.00017447615475780422\n",
      "Iteration: 5110 lambda_n: 0.9838338783978785 Loss: 0.00017121538768305166\n",
      "Iteration: 5111 lambda_n: 0.9179581166620547 Loss: 0.00016791836172404636\n",
      "Iteration: 5112 lambda_n: 0.9573254361310091 Loss: 0.0001649279994986474\n",
      "Iteration: 5113 lambda_n: 0.9669401445641272 Loss: 0.00016189085831022184\n",
      "Iteration: 5114 lambda_n: 1.0191211381364285 Loss: 0.00015890640636292709\n",
      "Iteration: 5115 lambda_n: 1.0337223033592255 Loss: 0.00015584696866115485\n",
      "Iteration: 5116 lambda_n: 1.0230214657214514 Loss: 0.00015283271871078464\n",
      "Iteration: 5117 lambda_n: 1.0259093852784429 Loss: 0.0001499360508955587\n",
      "Iteration: 5118 lambda_n: 1.003173701606096 Loss: 0.00014711418439810182\n",
      "Iteration: 5119 lambda_n: 0.9251501962790033 Loss: 0.0001444334970734691\n",
      "Iteration: 5120 lambda_n: 0.9183786689684882 Loss: 0.00014202975403336516\n",
      "Iteration: 5121 lambda_n: 1.0378434990909762 Loss: 0.0001397044901463433\n",
      "Iteration: 5122 lambda_n: 0.933670618108931 Loss: 0.0001371434525864956\n",
      "Iteration: 5123 lambda_n: 1.013265338479207 Loss: 0.00013490462687932328\n",
      "Iteration: 5124 lambda_n: 0.9472581124171358 Loss: 0.00013253697851540192\n",
      "Iteration: 5125 lambda_n: 1.0323963137373222 Loss: 0.00013038423889579723\n",
      "Iteration: 5126 lambda_n: 1.0288980205390603 Loss: 0.00012809825480329407\n",
      "Iteration: 5127 lambda_n: 0.9260946651241029 Loss: 0.00012588322854923192\n",
      "Iteration: 5128 lambda_n: 1.0389480192888616 Loss: 0.00012394416988045645\n",
      "Iteration: 5129 lambda_n: 0.9900699776072248 Loss: 0.00012182270552374682\n",
      "Iteration: 5130 lambda_n: 0.9058340061802572 Loss: 0.0001198565932613874\n",
      "Iteration: 5131 lambda_n: 0.9532618809429373 Loss: 0.00011810455768078745\n",
      "Iteration: 5132 lambda_n: 0.910074015951824 Loss: 0.0001163046896855977\n",
      "Iteration: 5133 lambda_n: 0.9159298355299248 Loss: 0.00011462906697325076\n",
      "Iteration: 5134 lambda_n: 1.037970255103618 Loss: 0.00011298255535500089\n",
      "Iteration: 5135 lambda_n: 0.9689293213903681 Loss: 0.00011116106422851884\n",
      "Iteration: 5136 lambda_n: 0.9208211667072497 Loss: 0.00010950604349828858\n",
      "Iteration: 5137 lambda_n: 1.0298418598762547 Loss: 0.00010797214078045209\n",
      "Iteration: 5138 lambda_n: 0.8921922771901974 Loss: 0.00010629701541381834\n",
      "Iteration: 5139 lambda_n: 0.9969567092917218 Loss: 0.00010488349543555722\n",
      "Iteration: 5140 lambda_n: 1.0427223087757624 Loss: 0.00010333964013433009\n",
      "Iteration: 5141 lambda_n: 0.9819938415576773 Loss: 0.00010176533486933663\n",
      "Iteration: 5142 lambda_n: 0.9824694290349721 Loss: 0.0001003212017663956\n",
      "Iteration: 5143 lambda_n: 0.9683314636431738 Loss: 9.891155094108036e-05\n",
      "Iteration: 5144 lambda_n: 0.8999885719996144 Loss: 9.755583474490655e-05\n",
      "Iteration: 5145 lambda_n: 0.9477614729664205 Loss: 9.632567142470229e-05\n",
      "Iteration: 5146 lambda_n: 1.0345938202626463 Loss: 9.505869391013352e-05\n",
      "Iteration: 5147 lambda_n: 0.9488693192211143 Loss: 9.370752730978083e-05\n",
      "Iteration: 5148 lambda_n: 0.9984668860912772 Loss: 9.249919544727906e-05\n",
      "Iteration: 5149 lambda_n: 0.9033668944086185 Loss: 9.12566927473429e-05\n",
      "Iteration: 5150 lambda_n: 1.0205528975137468 Loss: 9.015925819229921e-05\n",
      "Iteration: 5151 lambda_n: 0.9478066484247053 Loss: 8.894612034733732e-05\n",
      "Iteration: 5152 lambda_n: 1.0455052097015134 Loss: 8.784654796722273e-05\n",
      "Iteration: 5153 lambda_n: 0.9225178514112847 Loss: 8.666066572846484e-05\n",
      "Iteration: 5154 lambda_n: 0.9850224070666359 Loss: 8.563973808673062e-05\n",
      "Iteration: 5155 lambda_n: 1.0066345078591548 Loss: 8.457298753838534e-05\n",
      "Iteration: 5156 lambda_n: 0.9955017352423646 Loss: 8.350759967223137e-05\n",
      "Iteration: 5157 lambda_n: 0.9147885006361005 Loss: 8.24782946280378e-05\n",
      "Iteration: 5158 lambda_n: 0.9905449397383763 Loss: 8.155385704662735e-05\n",
      "Iteration: 5159 lambda_n: 0.9584029941751891 Loss: 8.057362876729794e-05\n",
      "Iteration: 5160 lambda_n: 1.0263181289918786 Loss: 7.964634307547661e-05\n",
      "Iteration: 5161 lambda_n: 1.0476665558956562 Loss: 7.867466667191768e-05\n",
      "Iteration: 5162 lambda_n: 0.9061448689355124 Loss: 7.770542351991696e-05\n",
      "Iteration: 5163 lambda_n: 1.0168375778261363 Loss: 7.688646765084747e-05\n",
      "Iteration: 5164 lambda_n: 0.9343492301413706 Loss: 7.598578479178386e-05\n",
      "Iteration: 5165 lambda_n: 0.9771171506740454 Loss: 7.517650548055947e-05\n",
      "Iteration: 5166 lambda_n: 1.039455265326334 Loss: 7.4347337214395e-05\n",
      "Iteration: 5167 lambda_n: 0.9790189873691447 Loss: 7.348385656131565e-05\n",
      "Iteration: 5168 lambda_n: 1.0352657739386726 Loss: 7.26886601846226e-05\n",
      "Iteration: 5169 lambda_n: 0.9851576705477992 Loss: 7.186529377902458e-05\n",
      "Iteration: 5170 lambda_n: 0.968564441036857 Loss: 7.109889836334723e-05\n",
      "Iteration: 5171 lambda_n: 1.0265965700152326 Loss: 7.036097633136378e-05\n",
      "Iteration: 5172 lambda_n: 0.95624675556467 Loss: 6.959463573722244e-05\n",
      "Iteration: 5173 lambda_n: 1.0214533381692519 Loss: 6.88959617405733e-05\n",
      "Iteration: 5174 lambda_n: 0.9047178981534314 Loss: 6.816432224434459e-05\n",
      "Iteration: 5175 lambda_n: 0.9427240897835589 Loss: 6.752978893131063e-05\n",
      "Iteration: 5176 lambda_n: 1.0444067094547786 Loss: 6.688072880197424e-05\n",
      "Iteration: 5177 lambda_n: 1.0067364606090032 Loss: 6.617532741957684e-05\n",
      "Iteration: 5178 lambda_n: 1.0184243864991605 Loss: 6.550956522896247e-05\n",
      "Iteration: 5179 lambda_n: 0.9065735034986859 Loss: 6.484953350826285e-05\n",
      "Iteration: 5180 lambda_n: 0.9423507336135895 Loss: 6.427376779941605e-05\n",
      "Iteration: 5181 lambda_n: 0.9162372372810426 Loss: 6.368589615594855e-05\n",
      "Iteration: 5182 lambda_n: 0.908298510852863 Loss: 6.312477615603327e-05\n",
      "Iteration: 5183 lambda_n: 0.8958437685363451 Loss: 6.257834923052225e-05\n",
      "Iteration: 5184 lambda_n: 0.9204297812135098 Loss: 6.204879231032794e-05\n",
      "Iteration: 5185 lambda_n: 0.9215700982007167 Loss: 6.151397870195473e-05\n",
      "Iteration: 5186 lambda_n: 0.9258145495959165 Loss: 6.098781654906619e-05\n",
      "Iteration: 5187 lambda_n: 0.9204944771373931 Loss: 6.046837187199858e-05\n",
      "Iteration: 5188 lambda_n: 0.9247244586161518 Loss: 5.99608196074417e-05\n",
      "Iteration: 5189 lambda_n: 0.9335892167921079 Loss: 5.945961623332078e-05\n",
      "Iteration: 5190 lambda_n: 0.9883880999481592 Loss: 5.896220074507217e-05\n",
      "Iteration: 5191 lambda_n: 0.9563693569860368 Loss: 5.844455400723053e-05\n",
      "Iteration: 5192 lambda_n: 0.9000790875689662 Loss: 5.79526304749666e-05\n",
      "Iteration: 5193 lambda_n: 1.0061195521619364 Loss: 5.749760407756501e-05\n",
      "Iteration: 5194 lambda_n: 0.9513241365594907 Loss: 5.699713033479872e-05\n",
      "Iteration: 5195 lambda_n: 0.947070401210549 Loss: 5.653232685135239e-05\n",
      "Iteration: 5196 lambda_n: 0.9111598750136097 Loss: 5.607731913442913e-05\n",
      "Iteration: 5197 lambda_n: 1.0140175485502785 Loss: 5.564677369987043e-05\n",
      "Iteration: 5198 lambda_n: 0.9940224100540618 Loss: 5.517516516094505e-05\n",
      "Iteration: 5199 lambda_n: 0.9075208367520855 Loss: 5.472088280593769e-05\n",
      "Iteration: 5200 lambda_n: 0.9665264390506652 Loss: 5.43131285206926e-05\n",
      "Iteration: 5201 lambda_n: 0.9647532786254113 Loss: 5.388549985727884e-05\n",
      "Iteration: 5202 lambda_n: 1.0129195743794661 Loss: 5.346554662628085e-05\n",
      "Iteration: 5203 lambda_n: 1.0035899772594903 Loss: 5.303167470017627e-05\n",
      "Iteration: 5204 lambda_n: 0.9655240893313725 Loss: 5.2608950415686264e-05\n",
      "Iteration: 5205 lambda_n: 1.0175066738091771 Loss: 5.220890297003694e-05\n",
      "Iteration: 5206 lambda_n: 0.9687550207547937 Loss: 5.179388734162355e-05\n",
      "Iteration: 5207 lambda_n: 0.9762354084857044 Loss: 5.140518643427295e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5208 lambda_n: 1.0479488706381155 Loss: 5.101949978758162e-05\n",
      "Iteration: 5209 lambda_n: 1.022468271640623 Loss: 5.061183471635611e-05\n",
      "Iteration: 5210 lambda_n: 1.0412673539755641 Loss: 5.022057361476034e-05\n",
      "Iteration: 5211 lambda_n: 0.989718714562341 Loss: 4.982840475497881e-05\n",
      "Iteration: 5212 lambda_n: 1.0199364790524654 Loss: 4.9461581411512854e-05\n",
      "Iteration: 5213 lambda_n: 0.9113544510547893 Loss: 4.908922274186603e-05\n",
      "Iteration: 5214 lambda_n: 1.020891745377306 Loss: 4.876159313612747e-05\n",
      "Iteration: 5215 lambda_n: 1.014167496565555 Loss: 4.839955725126439e-05\n",
      "Iteration: 5216 lambda_n: 0.9089938405094797 Loss: 4.804531477392133e-05\n",
      "Iteration: 5217 lambda_n: 1.0232595922985384 Loss: 4.773250530331933e-05\n",
      "Iteration: 5218 lambda_n: 0.9852226596898502 Loss: 4.738500195084423e-05\n",
      "Iteration: 5219 lambda_n: 0.9810552460840313 Loss: 4.705532081072569e-05\n",
      "Iteration: 5220 lambda_n: 0.8935508526736676 Loss: 4.67316232986468e-05\n",
      "Iteration: 5221 lambda_n: 1.0193549462242706 Loss: 4.644086199416584e-05\n",
      "Iteration: 5222 lambda_n: 0.959957000726921 Loss: 4.611329298939994e-05\n",
      "Iteration: 5223 lambda_n: 1.0067835907109701 Loss: 4.5809151586164676e-05\n",
      "Iteration: 5224 lambda_n: 0.9311063429001382 Loss: 4.549436048948784e-05\n",
      "Iteration: 5225 lambda_n: 0.9314943595195713 Loss: 4.5207199931425984e-05\n",
      "Iteration: 5226 lambda_n: 0.9851679896408877 Loss: 4.4923506981853034e-05\n",
      "Iteration: 5227 lambda_n: 0.9597719952545204 Loss: 4.4627182262947104e-05\n",
      "Iteration: 5228 lambda_n: 0.9176395573825048 Loss: 4.4342241404517595e-05\n",
      "Iteration: 5229 lambda_n: 0.9565661075412543 Loss: 4.407321889417684e-05\n",
      "Iteration: 5230 lambda_n: 1.006116486063149 Loss: 4.3796109642263956e-05\n",
      "Iteration: 5231 lambda_n: 1.039964049471709 Loss: 4.3508216384583e-05\n",
      "Iteration: 5232 lambda_n: 1.0468946811762267 Loss: 4.321443571580374e-05\n",
      "Iteration: 5233 lambda_n: 1.0471337752376648 Loss: 4.2922559634868876e-05\n",
      "Iteration: 5234 lambda_n: 0.920760945717203 Loss: 4.263441575355046e-05\n",
      "Iteration: 5235 lambda_n: 0.9951926563923817 Loss: 4.2384309105453736e-05\n",
      "Iteration: 5236 lambda_n: 0.9081437305619181 Loss: 4.211701524053534e-05\n",
      "Iteration: 5237 lambda_n: 0.9133635322003002 Loss: 4.1876029094810935e-05\n",
      "Iteration: 5238 lambda_n: 0.9321916301394516 Loss: 4.1636286935382415e-05\n",
      "Iteration: 5239 lambda_n: 1.0331943859558725 Loss: 4.139424828973273e-05\n",
      "Iteration: 5240 lambda_n: 0.9834222189546236 Loss: 4.112891884178715e-05\n",
      "Iteration: 5241 lambda_n: 1.0365762903659468 Loss: 4.0879403354431e-05\n",
      "Iteration: 5242 lambda_n: 1.0425862713974 Loss: 4.0619376968077446e-05\n",
      "Iteration: 5243 lambda_n: 0.9201256968633673 Loss: 4.0360931069368454e-05\n",
      "Iteration: 5244 lambda_n: 1.0098606015185898 Loss: 4.0135522275682196e-05\n",
      "Iteration: 5245 lambda_n: 0.9306729181163101 Loss: 3.989067087232147e-05\n",
      "Iteration: 5246 lambda_n: 0.9517292925563148 Loss: 3.966753853955002e-05\n",
      "Iteration: 5247 lambda_n: 1.0246796026417273 Loss: 3.944168251789147e-05\n",
      "Iteration: 5248 lambda_n: 1.0178882604274673 Loss: 3.92010249329931e-05\n",
      "Iteration: 5249 lambda_n: 1.0155065243158439 Loss: 3.896459428731278e-05\n",
      "Iteration: 5250 lambda_n: 0.9242740983846448 Loss: 3.873127053515534e-05\n",
      "Iteration: 5251 lambda_n: 0.9697579970733748 Loss: 3.852117897140349e-05\n",
      "Iteration: 5252 lambda_n: 0.9505163748401181 Loss: 3.830287288836521e-05\n",
      "Iteration: 5253 lambda_n: 0.9976367945787885 Loss: 3.809104207977189e-05\n",
      "Iteration: 5254 lambda_n: 1.0049748395131202 Loss: 3.78708729058365e-05\n",
      "Iteration: 5255 lambda_n: 0.8960936072734177 Loss: 3.765132776373414e-05\n",
      "Iteration: 5256 lambda_n: 0.9912634076704965 Loss: 3.745754382923329e-05\n",
      "Iteration: 5257 lambda_n: 0.9045239430079673 Loss: 3.724508940005544e-05\n",
      "Iteration: 5258 lambda_n: 0.9592626988989515 Loss: 3.7053119834364083e-05\n",
      "Iteration: 5259 lambda_n: 0.9213444490887726 Loss: 3.6851331159920575e-05\n",
      "Iteration: 5260 lambda_n: 0.918958728750953 Loss: 3.6659318525473225e-05\n",
      "Iteration: 5261 lambda_n: 0.9025207312565554 Loss: 3.646949557221173e-05\n",
      "Iteration: 5262 lambda_n: 1.0338971266574277 Loss: 3.628469698722934e-05\n",
      "Iteration: 5263 lambda_n: 0.98246308357667 Loss: 3.60747991298913e-05\n",
      "Iteration: 5264 lambda_n: 0.9549090510997449 Loss: 3.58772703431324e-05\n",
      "Iteration: 5265 lambda_n: 0.8983692399191835 Loss: 3.5687027095809455e-05\n",
      "Iteration: 5266 lambda_n: 0.9063580791140418 Loss: 3.5509615306532825e-05\n",
      "Iteration: 5267 lambda_n: 0.9347728595937878 Loss: 3.5332087427801936e-05\n",
      "Iteration: 5268 lambda_n: 1.0285778862722599 Loss: 3.515048987255107e-05\n",
      "Iteration: 5269 lambda_n: 0.9179590726690314 Loss: 3.4952338747501905e-05\n",
      "Iteration: 5270 lambda_n: 0.9331044678056503 Loss: 3.477710984613645e-05\n",
      "Iteration: 5271 lambda_n: 1.0173008035786393 Loss: 3.4600425486869046e-05\n",
      "Iteration: 5272 lambda_n: 0.9221837340638497 Loss: 3.4409363665462045e-05\n",
      "Iteration: 5273 lambda_n: 1.0490052408165689 Loss: 3.42376873070927e-05\n",
      "Iteration: 5274 lambda_n: 1.0324424561593595 Loss: 3.4043942564914126e-05\n",
      "Iteration: 5275 lambda_n: 0.9381523150431671 Loss: 3.385495442986523e-05\n",
      "Iteration: 5276 lambda_n: 0.9406506087720269 Loss: 3.368471677128783e-05\n",
      "Iteration: 5277 lambda_n: 0.9655450457465926 Loss: 3.3515360104108906e-05\n",
      "Iteration: 5278 lambda_n: 1.0166339344881254 Loss: 3.334287290299968e-05\n",
      "Iteration: 5279 lambda_n: 1.0013507832488322 Loss: 3.316269669254721e-05\n",
      "Iteration: 5280 lambda_n: 0.969658143643775 Loss: 3.298669597841104e-05\n",
      "Iteration: 5281 lambda_n: 0.9460508417612066 Loss: 3.2817641251036416e-05\n",
      "Iteration: 5282 lambda_n: 0.921338130018378 Loss: 3.265398062115831e-05\n",
      "Iteration: 5283 lambda_n: 1.0412485744476319 Loss: 3.2495790574494324e-05\n",
      "Iteration: 5284 lambda_n: 0.9686541223220716 Loss: 3.231830815220834e-05\n",
      "Iteration: 5285 lambda_n: 0.908154299529463 Loss: 3.2154541666832973e-05\n",
      "Iteration: 5286 lambda_n: 1.0018595655341924 Loss: 3.200215486231785e-05\n",
      "Iteration: 5287 lambda_n: 0.9658126631555 Loss: 3.1835217095204074e-05\n",
      "Iteration: 5288 lambda_n: 0.9863555358891809 Loss: 3.167551508523866e-05\n",
      "Iteration: 5289 lambda_n: 0.9565182275964053 Loss: 3.151360778752314e-05\n",
      "Iteration: 5290 lambda_n: 1.0120453788375723 Loss: 3.135776080258546e-05\n",
      "Iteration: 5291 lambda_n: 0.9732295570212427 Loss: 3.119404173002784e-05\n",
      "Iteration: 5292 lambda_n: 0.9426255056437606 Loss: 3.103778019605734e-05\n",
      "Iteration: 5293 lambda_n: 1.036817052354492 Loss: 3.0887513309528927e-05\n",
      "Iteration: 5294 lambda_n: 0.9305746640570471 Loss: 3.072336599596835e-05\n",
      "Iteration: 5295 lambda_n: 0.939852889635132 Loss: 3.057714367170288e-05\n",
      "Iteration: 5296 lambda_n: 1.0033217152197738 Loss: 3.0430449902727186e-05\n",
      "Iteration: 5297 lambda_n: 0.9464775416195487 Loss: 3.0274899135966387e-05\n",
      "Iteration: 5298 lambda_n: 1.0153295189257623 Loss: 3.0129203732087287e-05\n",
      "Iteration: 5299 lambda_n: 0.9579445874619515 Loss: 2.9973949645228708e-05\n",
      "Iteration: 5300 lambda_n: 0.9473016724726597 Loss: 2.9828508870003474e-05\n",
      "Iteration: 5301 lambda_n: 1.0106310329789097 Loss: 2.9685639282997535e-05\n",
      "Iteration: 5302 lambda_n: 0.9588134195247751 Loss: 2.953421310052689e-05\n",
      "Iteration: 5303 lambda_n: 0.9228747675976271 Loss: 2.939154452957823e-05\n",
      "Iteration: 5304 lambda_n: 0.9665561158180169 Loss: 2.92551184451522e-05\n",
      "Iteration: 5305 lambda_n: 0.962538337673582 Loss: 2.911312563622358e-05\n",
      "Iteration: 5306 lambda_n: 1.0335425843922148 Loss: 2.8972640510890637e-05\n",
      "Iteration: 5307 lambda_n: 0.9763364052885364 Loss: 2.8822760681600137e-05\n",
      "Iteration: 5308 lambda_n: 0.9208444626068376 Loss: 2.8682146725458847e-05\n",
      "Iteration: 5309 lambda_n: 1.0422860032088799 Loss: 2.8550377591586652e-05\n",
      "Iteration: 5310 lambda_n: 0.9699380127430646 Loss: 2.8402129682770717e-05\n",
      "Iteration: 5311 lambda_n: 0.9125504416192297 Loss: 2.8265107936810522e-05\n",
      "Iteration: 5312 lambda_n: 0.9411996976795747 Loss: 2.813700189529383e-05\n",
      "Iteration: 5313 lambda_n: 0.912381002586548 Loss: 2.8005649234064933e-05\n",
      "Iteration: 5314 lambda_n: 0.9538084029587368 Loss: 2.7879084840256435e-05\n",
      "Iteration: 5315 lambda_n: 1.021802260234355 Loss: 2.7747541403402762e-05\n",
      "Iteration: 5316 lambda_n: 0.9897931145719366 Loss: 2.7607470967173845e-05\n",
      "Iteration: 5317 lambda_n: 0.9038031464229179 Loss: 2.7472660672011044e-05\n",
      "Iteration: 5318 lambda_n: 1.0458977551348716 Loss: 2.7350324420524893e-05\n",
      "Iteration: 5319 lambda_n: 1.043116074611401 Loss: 2.7209550663978005e-05\n",
      "Iteration: 5320 lambda_n: 0.9964657804673704 Loss: 2.7070060338492547e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5321 lambda_n: 0.9105017988722571 Loss: 2.6937663908809967e-05\n",
      "Iteration: 5322 lambda_n: 0.9007051971456695 Loss: 2.6817427091291922e-05\n",
      "Iteration: 5323 lambda_n: 1.0366610950846502 Loss: 2.669914346447918e-05\n",
      "Iteration: 5324 lambda_n: 0.9800386208193899 Loss: 2.6563748882996854e-05\n",
      "Iteration: 5325 lambda_n: 0.9596006642645968 Loss: 2.6436550147008257e-05\n",
      "Iteration: 5326 lambda_n: 1.0379415250505486 Loss: 2.6312736668075094e-05\n",
      "Iteration: 5327 lambda_n: 0.9488228992348199 Loss: 2.617958281169745e-05\n",
      "Iteration: 5328 lambda_n: 0.9170915384212271 Loss: 2.605861281104148e-05\n",
      "Iteration: 5329 lambda_n: 0.906543762532539 Loss: 2.5942344719574707e-05\n",
      "Iteration: 5330 lambda_n: 0.9839209222867062 Loss: 2.5828034651782922e-05\n",
      "Iteration: 5331 lambda_n: 0.9009645253042646 Loss: 2.570462738420375e-05\n",
      "Iteration: 5332 lambda_n: 0.9415389771372159 Loss: 2.559227421793005e-05\n",
      "Iteration: 5333 lambda_n: 1.049408298042753 Loss: 2.547547640544778e-05\n",
      "Iteration: 5334 lambda_n: 0.9441486504290196 Loss: 2.534600729817842e-05\n",
      "Iteration: 5335 lambda_n: 0.9550865136689182 Loss: 2.523022949617876e-05\n",
      "Iteration: 5336 lambda_n: 0.9748339341452065 Loss: 2.511374536618153e-05\n",
      "Iteration: 5337 lambda_n: 0.9943522768693223 Loss: 2.499550224479437e-05\n",
      "Iteration: 5338 lambda_n: 1.0452894435874989 Loss: 2.487556142422662e-05\n",
      "Iteration: 5339 lambda_n: 0.990449901654514 Loss: 2.4750187836976454e-05\n",
      "Iteration: 5340 lambda_n: 0.9353672418669701 Loss: 2.4632093568221118e-05\n",
      "Iteration: 5341 lambda_n: 1.0071519353231224 Loss: 2.452118867300734e-05\n",
      "Iteration: 5342 lambda_n: 1.0001168568012575 Loss: 2.4402398654149518e-05\n",
      "Iteration: 5343 lambda_n: 0.916525072884584 Loss: 2.428510213618344e-05\n",
      "Iteration: 5344 lambda_n: 0.9068014247826878 Loss: 2.4178207851299816e-05\n",
      "Iteration: 5345 lambda_n: 0.9605704472052738 Loss: 2.4072985163013934e-05\n",
      "Iteration: 5346 lambda_n: 1.001626269464259 Loss: 2.3962081917298823e-05\n",
      "Iteration: 5347 lambda_n: 0.951680144868866 Loss: 2.384705054858072e-05\n",
      "Iteration: 5348 lambda_n: 0.9076697313351251 Loss: 2.3738356330104883e-05\n",
      "Iteration: 5349 lambda_n: 1.0477895618559747 Loss: 2.3635228538724068e-05\n",
      "Iteration: 5350 lambda_n: 0.946658527654878 Loss: 2.3516769990806087e-05\n",
      "Iteration: 5351 lambda_n: 1.0134634646164729 Loss: 2.341035471864337e-05\n",
      "Iteration: 5352 lambda_n: 0.9407073225512317 Loss: 2.3297014311075644e-05\n",
      "Iteration: 5353 lambda_n: 0.9957631656194146 Loss: 2.319238667152898e-05\n",
      "Iteration: 5354 lambda_n: 0.9097058101694231 Loss: 2.3082196753219962e-05\n",
      "Iteration: 5355 lambda_n: 1.0453551199953302 Loss: 2.2982068188879606e-05\n",
      "Iteration: 5356 lambda_n: 0.9234729183496708 Loss: 2.28675695728694e-05\n",
      "Iteration: 5357 lambda_n: 1.0266564397084321 Loss: 2.2766985453768665e-05\n",
      "Iteration: 5358 lambda_n: 0.9377484123783238 Loss: 2.265571241441237e-05\n",
      "Iteration: 5359 lambda_n: 0.985227703739241 Loss: 2.2554629629237854e-05\n",
      "Iteration: 5360 lambda_n: 0.9739507649082176 Loss: 2.2448956182987995e-05\n",
      "Iteration: 5361 lambda_n: 1.0389110563707895 Loss: 2.2345035786840997e-05\n",
      "Iteration: 5362 lambda_n: 0.8984188350212722 Loss: 2.2234752764716952e-05\n",
      "Iteration: 5363 lambda_n: 0.9193166258427404 Loss: 2.2139903835118222e-05\n",
      "Iteration: 5364 lambda_n: 0.9089981312148467 Loss: 2.2043305483990706e-05\n",
      "Iteration: 5365 lambda_n: 1.0258338004963214 Loss: 2.1948250337812878e-05\n",
      "Iteration: 5366 lambda_n: 1.0457369363802766 Loss: 2.1841486054103558e-05\n",
      "Iteration: 5367 lambda_n: 1.042625605515252 Loss: 2.1733231285181447e-05\n",
      "Iteration: 5368 lambda_n: 0.9937240345225858 Loss: 2.162588445060029e-05\n",
      "Iteration: 5369 lambda_n: 0.9705752682286152 Loss: 2.1524124751186174e-05\n",
      "Iteration: 5370 lambda_n: 1.0169171581178928 Loss: 2.1425245680587147e-05\n",
      "Iteration: 5371 lambda_n: 0.9673703338768319 Loss: 2.1322163645292436e-05\n",
      "Iteration: 5372 lambda_n: 0.9375019560583432 Loss: 2.1224616825439723e-05\n",
      "Iteration: 5373 lambda_n: 0.911630718342085 Loss: 2.113055107838304e-05\n",
      "Iteration: 5374 lambda_n: 1.0384031857246527 Loss: 2.103952025433811e-05\n",
      "Iteration: 5375 lambda_n: 0.9575980476516788 Loss: 2.0936313640119522e-05\n",
      "Iteration: 5376 lambda_n: 0.9154664130831613 Loss: 2.084164233941502e-05\n",
      "Iteration: 5377 lambda_n: 1.0153097241303597 Loss: 2.075157746160544e-05\n",
      "Iteration: 5378 lambda_n: 1.0489506839590628 Loss: 2.065215443634917e-05\n",
      "Iteration: 5379 lambda_n: 0.9110204187991318 Loss: 2.054996606258555e-05\n",
      "Iteration: 5380 lambda_n: 0.9719659216972365 Loss: 2.0461686023041823e-05\n",
      "Iteration: 5381 lambda_n: 0.938899792732113 Loss: 2.036793369106074e-05\n",
      "Iteration: 5382 lambda_n: 0.9641365156535386 Loss: 2.0277814741494427e-05\n",
      "Iteration: 5383 lambda_n: 0.8992382423232107 Loss: 2.0185710928474538e-05\n",
      "Iteration: 5384 lambda_n: 0.9305953788233278 Loss: 2.0100223139556333e-05\n",
      "Iteration: 5385 lambda_n: 0.9079773881326325 Loss: 2.001215353550364e-05\n",
      "Iteration: 5386 lambda_n: 0.946450264590273 Loss: 1.992662511490995e-05\n",
      "Iteration: 5387 lambda_n: 0.9035968711697766 Loss: 1.9837877645792504e-05\n",
      "Iteration: 5388 lambda_n: 1.039310284569881 Loss: 1.9753549079050807e-05\n",
      "Iteration: 5389 lambda_n: 0.9086606230550757 Loss: 1.965699215858342e-05\n",
      "Iteration: 5390 lambda_n: 0.9084083093459742 Loss: 1.9573010230335334e-05\n",
      "Iteration: 5391 lambda_n: 0.9165306161828454 Loss: 1.9489431010036868e-05\n",
      "Iteration: 5392 lambda_n: 0.9492416011901582 Loss: 1.9405484914466282e-05\n",
      "Iteration: 5393 lambda_n: 1.0246904082436636 Loss: 1.9318937989587823e-05\n",
      "Iteration: 5394 lambda_n: 0.9774089256383218 Loss: 1.9225951291726283e-05\n",
      "Iteration: 5395 lambda_n: 0.8956569167893215 Loss: 1.913770476831434e-05\n",
      "Iteration: 5396 lambda_n: 0.904439579090862 Loss: 1.9057229727764868e-05\n",
      "Iteration: 5397 lambda_n: 0.9624850266168161 Loss: 1.8976324602925542e-05\n",
      "Iteration: 5398 lambda_n: 1.0024119097507422 Loss: 1.8890610789706044e-05\n",
      "Iteration: 5399 lambda_n: 1.0218571354374695 Loss: 1.880176413793516e-05\n",
      "Iteration: 5400 lambda_n: 1.0314981854503225 Loss: 1.8711640246679208e-05\n",
      "Iteration: 5401 lambda_n: 0.95182164611399 Loss: 1.8621122412572924e-05\n",
      "Iteration: 5402 lambda_n: 0.945831103066475 Loss: 1.8538018916047606e-05\n",
      "Iteration: 5403 lambda_n: 0.9806952878671416 Loss: 1.8455823359402307e-05\n",
      "Iteration: 5404 lambda_n: 0.9092752540897859 Loss: 1.8370992295334374e-05\n",
      "Iteration: 5405 lambda_n: 1.0049258314733596 Loss: 1.8292716024440213e-05\n",
      "Iteration: 5406 lambda_n: 0.9232673121501174 Loss: 1.8206589467896625e-05\n",
      "Iteration: 5407 lambda_n: 0.9323841656501127 Loss: 1.8127849127585873e-05\n",
      "Iteration: 5408 lambda_n: 0.9237839169471156 Loss: 1.804868884610187e-05\n",
      "Iteration: 5409 lambda_n: 0.9461583959611554 Loss: 1.7970614563324444e-05\n",
      "Iteration: 5410 lambda_n: 1.0311823192298215 Loss: 1.7891008387280266e-05\n",
      "Iteration: 5411 lambda_n: 0.9550985499561057 Loss: 1.780464729979111e-05\n",
      "Iteration: 5412 lambda_n: 0.9497317919068612 Loss: 1.772505841689283e-05\n",
      "Iteration: 5413 lambda_n: 1.040530439666133 Loss: 1.764628314586145e-05\n",
      "Iteration: 5414 lambda_n: 0.8932198868303398 Loss: 1.7560373565868097e-05\n",
      "Iteration: 5415 lambda_n: 0.967236718833251 Loss: 1.7486997721534746e-05\n",
      "Iteration: 5416 lambda_n: 0.9377544497117355 Loss: 1.740788465477554e-05\n",
      "Iteration: 5417 lambda_n: 0.9866492008021064 Loss: 1.733154137338246e-05\n",
      "Iteration: 5418 lambda_n: 0.9288631871772322 Loss: 1.72515810577443e-05\n",
      "Iteration: 5419 lambda_n: 0.9935707972391372 Loss: 1.7176662013563793e-05\n",
      "Iteration: 5420 lambda_n: 1.015322349353757 Loss: 1.709688252767587e-05\n",
      "Iteration: 5421 lambda_n: 0.9419107123479022 Loss: 1.701574647943371e-05\n",
      "Iteration: 5422 lambda_n: 0.9012786539586644 Loss: 1.6940844528810078e-05\n",
      "Iteration: 5423 lambda_n: 0.902024627950291 Loss: 1.6869498198362525e-05\n",
      "Iteration: 5424 lambda_n: 1.0431811625864185 Loss: 1.6798401950597858e-05\n",
      "Iteration: 5425 lambda_n: 0.9700923761048182 Loss: 1.671653597056081e-05\n",
      "Iteration: 5426 lambda_n: 1.013613771143501 Loss: 1.6640786768647343e-05\n",
      "Iteration: 5427 lambda_n: 0.994539739131961 Loss: 1.6562007263670166e-05\n",
      "Iteration: 5428 lambda_n: 1.0145766741596618 Loss: 1.6485085527922112e-05\n",
      "Iteration: 5429 lambda_n: 1.0378078699249538 Loss: 1.6406987634160684e-05\n",
      "Iteration: 5430 lambda_n: 0.9071963107536936 Loss: 1.6327489216330892e-05\n",
      "Iteration: 5431 lambda_n: 0.9731904746172241 Loss: 1.625834070632513e-05\n",
      "Iteration: 5432 lambda_n: 0.9772390333377999 Loss: 1.61844834583279e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5433 lambda_n: 1.012009458730744 Loss: 1.6110663564274847e-05\n",
      "Iteration: 5434 lambda_n: 1.0448499912512725 Loss: 1.6034573612087655e-05\n",
      "Iteration: 5435 lambda_n: 0.9664392512080393 Loss: 1.5956393614303357e-05\n",
      "Iteration: 5436 lambda_n: 0.9994915751152805 Loss: 1.588444073300292e-05\n",
      "Iteration: 5437 lambda_n: 1.0467301897487133 Loss: 1.5810369601803508e-05\n",
      "Iteration: 5438 lambda_n: 0.9748382956680505 Loss: 1.5733166768930486e-05\n",
      "Iteration: 5439 lambda_n: 1.0207456567126538 Loss: 1.5661624489871505e-05\n",
      "Iteration: 5440 lambda_n: 0.98681276205724 Loss: 1.5587060378793103e-05\n",
      "Iteration: 5441 lambda_n: 1.0387919268908066 Loss: 1.551532473425627e-05\n",
      "Iteration: 5442 lambda_n: 0.9550084411441532 Loss: 1.5440164485536e-05\n",
      "Iteration: 5443 lambda_n: 0.9803974463345931 Loss: 1.5371407066634004e-05\n",
      "Iteration: 5444 lambda_n: 1.0483869622387287 Loss: 1.5301141616533206e-05\n",
      "Iteration: 5445 lambda_n: 1.0018600338947612 Loss: 1.5226352751400568e-05\n",
      "Iteration: 5446 lambda_n: 1.0330303947946138 Loss: 1.5155238225401428e-05\n",
      "Iteration: 5447 lambda_n: 0.9929568000486413 Loss: 1.5082259281930003e-05\n",
      "Iteration: 5448 lambda_n: 1.0154320011488456 Loss: 1.5012454603522825e-05\n",
      "Iteration: 5449 lambda_n: 1.0487920857078394 Loss: 1.4941405518157694e-05\n",
      "Iteration: 5450 lambda_n: 0.982068622840501 Loss: 1.4868374900263132e-05\n",
      "Iteration: 5451 lambda_n: 0.9325559482188372 Loss: 1.4800329721824036e-05\n",
      "Iteration: 5452 lambda_n: 0.9671071607796738 Loss: 1.4736015209788418e-05\n",
      "Iteration: 5453 lambda_n: 0.9143087893141761 Loss: 1.4669611837229394e-05\n",
      "Iteration: 5454 lambda_n: 1.0256891990554378 Loss: 1.4607120564339529e-05\n",
      "Iteration: 5455 lambda_n: 0.966196942560583 Loss: 1.4537319391448781e-05\n",
      "Iteration: 5456 lambda_n: 0.9758435757424274 Loss: 1.447188527209277e-05\n",
      "Iteration: 5457 lambda_n: 1.0480292672527367 Loss: 1.4406099224874477e-05\n",
      "Iteration: 5458 lambda_n: 0.9167828302665164 Loss: 1.4335772108740361e-05\n",
      "Iteration: 5459 lambda_n: 0.9896234984878342 Loss: 1.4274556270969121e-05\n",
      "Iteration: 5460 lambda_n: 1.0490607875684876 Loss: 1.4208762311436547e-05\n",
      "Iteration: 5461 lambda_n: 0.976543781818885 Loss: 1.4139342060885116e-05\n",
      "Iteration: 5462 lambda_n: 0.9435538462940618 Loss: 1.407503995674857e-05\n",
      "Iteration: 5463 lambda_n: 0.9290440027203221 Loss: 1.4013195913083412e-05\n",
      "Iteration: 5464 lambda_n: 0.9941584367855393 Loss: 1.395257344953677e-05\n",
      "Iteration: 5465 lambda_n: 0.8932627557559247 Loss: 1.3887985817528229e-05\n",
      "Iteration: 5466 lambda_n: 0.9293230681986124 Loss: 1.3830224606598397e-05\n",
      "Iteration: 5467 lambda_n: 0.9655048948503558 Loss: 1.3770384169806959e-05\n",
      "Iteration: 5468 lambda_n: 0.9222246899734579 Loss: 1.3708485690315503e-05\n",
      "Iteration: 5469 lambda_n: 0.9053050373554601 Loss: 1.3649630335123777e-05\n",
      "Iteration: 5470 lambda_n: 1.0486357451384023 Loss: 1.3592105259382688e-05\n",
      "Iteration: 5471 lambda_n: 0.9165533906751561 Loss: 1.3525756150350604e-05\n",
      "Iteration: 5472 lambda_n: 0.9832441553946439 Loss: 1.3468049883195182e-05\n",
      "Iteration: 5473 lambda_n: 1.0333497308365047 Loss: 1.3406411298386632e-05\n",
      "Iteration: 5474 lambda_n: 1.0146848577093501 Loss: 1.3341930785305332e-05\n",
      "Iteration: 5475 lambda_n: 0.938758303604206 Loss: 1.3278922156339955e-05\n",
      "Iteration: 5476 lambda_n: 0.9910411685302853 Loss: 1.3220905980288517e-05\n",
      "Iteration: 5477 lambda_n: 0.9672350610052493 Loss: 1.3159928507185577e-05\n",
      "Iteration: 5478 lambda_n: 0.8967905915120971 Loss: 1.3100692528507134e-05\n",
      "Iteration: 5479 lambda_n: 1.0301358615990877 Loss: 1.3046019948866652e-05\n",
      "Iteration: 5480 lambda_n: 0.9064567726365943 Loss: 1.2983482156050773e-05\n",
      "Iteration: 5481 lambda_n: 1.0456816880189854 Loss: 1.2928718525625905e-05\n",
      "Iteration: 5482 lambda_n: 0.9505248901857675 Loss: 1.2865812080617241e-05\n",
      "Iteration: 5483 lambda_n: 1.0093037004692744 Loss: 1.280891037653864e-05\n",
      "Iteration: 5484 lambda_n: 0.9863770096631077 Loss: 1.2748759105643047e-05\n",
      "Iteration: 5485 lambda_n: 0.9766441644859626 Loss: 1.2690252183354528e-05\n",
      "Iteration: 5486 lambda_n: 1.029729452546467 Loss: 1.2632590236308834e-05\n",
      "Iteration: 5487 lambda_n: 1.0397513093082513 Loss: 1.257207217987072e-05\n",
      "Iteration: 5488 lambda_n: 0.9776942906687937 Loss: 1.251125978649307e-05\n",
      "Iteration: 5489 lambda_n: 0.9965928124534864 Loss: 1.2454355314926061e-05\n",
      "Iteration: 5490 lambda_n: 0.8942101248076413 Loss: 1.239661636176351e-05\n",
      "Iteration: 5491 lambda_n: 0.9633708702078232 Loss: 1.2345050732733186e-05\n",
      "Iteration: 5492 lambda_n: 0.9860509274553607 Loss: 1.22897293327676e-05\n",
      "Iteration: 5493 lambda_n: 0.9428293234547722 Loss: 1.2233360760764735e-05\n",
      "Iteration: 5494 lambda_n: 1.012363531989884 Loss: 1.2179711614171547e-05\n",
      "Iteration: 5495 lambda_n: 0.9620128013931886 Loss: 1.2122359849518849e-05\n",
      "Iteration: 5496 lambda_n: 0.9034494880980984 Loss: 1.2068118548003744e-05\n",
      "Iteration: 5497 lambda_n: 1.0248008093280732 Loss: 1.2017408371588263e-05\n",
      "Iteration: 5498 lambda_n: 0.999609979715751 Loss: 1.1960129769988582e-05\n",
      "Iteration: 5499 lambda_n: 0.980280245649412 Loss: 1.1904526800218843e-05\n",
      "Iteration: 5500 lambda_n: 0.9706600089019471 Loss: 1.1850253804805198e-05\n",
      "Iteration: 5501 lambda_n: 1.0380983033457445 Loss: 1.1796759628170667e-05\n",
      "Iteration: 5502 lambda_n: 0.8956007171869059 Loss: 1.173980833956514e-05\n",
      "Iteration: 5503 lambda_n: 0.9084268595307389 Loss: 1.1690912942497665e-05\n",
      "Iteration: 5504 lambda_n: 0.9105060758270429 Loss: 1.1641524801677192e-05\n",
      "Iteration: 5505 lambda_n: 1.0397432856995465 Loss: 1.1592233668701642e-05\n",
      "Iteration: 5506 lambda_n: 1.0192054895671134 Loss: 1.1536185517944646e-05\n",
      "Iteration: 5507 lambda_n: 1.039742558902882 Loss: 1.1481511244746023e-05\n",
      "Iteration: 5508 lambda_n: 0.9506842734942201 Loss: 1.1426000724913421e-05\n",
      "Iteration: 5509 lambda_n: 1.0297968076505524 Loss: 1.1375491303856538e-05\n",
      "Iteration: 5510 lambda_n: 1.0438003265627538 Loss: 1.1321021491549922e-05\n",
      "Iteration: 5511 lambda_n: 0.8962561237298344 Loss: 1.126607637493247e-05\n",
      "Iteration: 5512 lambda_n: 0.9659571370368094 Loss: 1.1219127753112507e-05\n",
      "Iteration: 5513 lambda_n: 0.9907343439486673 Loss: 1.1168739623259772e-05\n",
      "Iteration: 5514 lambda_n: 0.924292187039816 Loss: 1.1117291970171626e-05\n",
      "Iteration: 5515 lambda_n: 0.9115128979948875 Loss: 1.1069516458359142e-05\n",
      "Iteration: 5516 lambda_n: 0.9305526782508896 Loss: 1.1022604666128507e-05\n",
      "Iteration: 5517 lambda_n: 0.9907902171330518 Loss: 1.097491662549709e-05\n",
      "Iteration: 5518 lambda_n: 1.0443714826226167 Loss: 1.092436199357141e-05\n",
      "Iteration: 5519 lambda_n: 0.9591342408384627 Loss: 1.0871319667126298e-05\n",
      "Iteration: 5520 lambda_n: 0.8987052556543359 Loss: 1.08228437088209e-05\n",
      "Iteration: 5521 lambda_n: 0.966119730260123 Loss: 1.0777625081541153e-05\n",
      "Iteration: 5522 lambda_n: 1.019753217748264 Loss: 1.0729218189546661e-05\n",
      "Iteration: 5523 lambda_n: 0.9463462447217863 Loss: 1.0678354189257545e-05\n",
      "Iteration: 5524 lambda_n: 0.9652550715775059 Loss: 1.0631376059544186e-05\n",
      "Iteration: 5525 lambda_n: 0.9135010527504611 Loss: 1.0583670668334746e-05\n",
      "Iteration: 5526 lambda_n: 0.9416726960092052 Loss: 1.0538726244199044e-05\n",
      "Iteration: 5527 lambda_n: 1.0178351104416352 Loss: 1.0492593050584836e-05\n",
      "Iteration: 5528 lambda_n: 0.966024635077673 Loss: 1.0442947469762616e-05\n",
      "Iteration: 5529 lambda_n: 1.023823842525711 Loss: 1.0396052500078922e-05\n",
      "Iteration: 5530 lambda_n: 0.9543950324610823 Loss: 1.0346575462806265e-05\n",
      "Iteration: 5531 lambda_n: 1.005468417444174 Loss: 1.0300673673195798e-05\n",
      "Iteration: 5532 lambda_n: 0.9551104290465655 Loss: 1.0252530558932779e-05\n",
      "Iteration: 5533 lambda_n: 1.0004344311821098 Loss: 1.020701289844494e-05\n",
      "Iteration: 5534 lambda_n: 1.031110713178302 Loss: 1.015954739737571e-05\n",
      "Iteration: 5535 lambda_n: 1.0288954557856125 Loss: 1.0110854476365205e-05\n",
      "Iteration: 5536 lambda_n: 0.9319663615106626 Loss: 1.0062499560169684e-05\n",
      "Iteration: 5537 lambda_n: 1.0276523025980155 Loss: 1.0018909936985862e-05\n",
      "Iteration: 5538 lambda_n: 0.9130948867972871 Loss: 9.971053575519977e-06\n",
      "Iteration: 5539 lambda_n: 0.9159077419237596 Loss: 9.92873552566642e-06\n",
      "Iteration: 5540 lambda_n: 0.9668267852154175 Loss: 9.8864676314129e-06\n",
      "Iteration: 5541 lambda_n: 1.0090318994722713 Loss: 9.842040212455148e-06\n",
      "Iteration: 5542 lambda_n: 0.8957228532866207 Loss: 9.795882158692124e-06\n",
      "Iteration: 5543 lambda_n: 1.0343406120209322 Loss: 9.755099946402487e-06\n",
      "Iteration: 5544 lambda_n: 0.91422137787348 Loss: 9.70820289824306e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5545 lambda_n: 0.9996124253367766 Loss: 9.666951695703464e-06\n",
      "Iteration: 5546 lambda_n: 0.9602173672512577 Loss: 9.622039498381583e-06\n",
      "Iteration: 5547 lambda_n: 0.9245784353727179 Loss: 9.57909809045336e-06\n",
      "Iteration: 5548 lambda_n: 0.9458541404367502 Loss: 9.537935313821263e-06\n",
      "Iteration: 5549 lambda_n: 0.9995701134918954 Loss: 9.496006583523884e-06\n",
      "Iteration: 5550 lambda_n: 1.0444230040601263 Loss: 9.451891782944172e-06\n",
      "Iteration: 5551 lambda_n: 0.8996675377127143 Loss: 9.40601193239301e-06\n",
      "Iteration: 5552 lambda_n: 1.0192710410592485 Loss: 9.366683095231477e-06\n",
      "Iteration: 5553 lambda_n: 1.01324343223129 Loss: 9.322312395965102e-06\n",
      "Iteration: 5554 lambda_n: 0.9978369995959275 Loss: 9.278413344643576e-06\n",
      "Iteration: 5555 lambda_n: 1.016449314671245 Loss: 9.235385655934615e-06\n",
      "Iteration: 5556 lambda_n: 0.9822479467583065 Loss: 9.19175893342711e-06\n",
      "Iteration: 5557 lambda_n: 0.9983710990594898 Loss: 9.149799586692808e-06\n",
      "Iteration: 5558 lambda_n: 0.9393412172074068 Loss: 9.107346444231502e-06\n",
      "Iteration: 5559 lambda_n: 0.988107955497606 Loss: 9.067588967655292e-06\n",
      "Iteration: 5560 lambda_n: 0.9398011396067918 Loss: 9.025950251811177e-06\n",
      "Iteration: 5561 lambda_n: 0.968001091891934 Loss: 8.986529267357607e-06\n",
      "Iteration: 5562 lambda_n: 0.9056414027337487 Loss: 8.94610296327322e-06\n",
      "Iteration: 5563 lambda_n: 0.9982808069199784 Loss: 8.908451316587948e-06\n",
      "Iteration: 5564 lambda_n: 0.9244372301344674 Loss: 8.867123109568843e-06\n",
      "Iteration: 5565 lambda_n: 1.040967560084142 Loss: 8.829029735570026e-06\n",
      "Iteration: 5566 lambda_n: 1.0374734366517353 Loss: 8.78631897385081e-06\n",
      "Iteration: 5567 lambda_n: 0.9328614992019277 Loss: 8.743957726525543e-06\n",
      "Iteration: 5568 lambda_n: 0.894667100391021 Loss: 8.706051747689752e-06\n",
      "Iteration: 5569 lambda_n: 0.9384649299830332 Loss: 8.669855529498732e-06\n",
      "Iteration: 5570 lambda_n: 1.0421520649655014 Loss: 8.632045370276473e-06\n",
      "Iteration: 5571 lambda_n: 1.0111248642793484 Loss: 8.590241021373572e-06\n",
      "Iteration: 5572 lambda_n: 0.9958883368669489 Loss: 8.549877905616717e-06\n",
      "Iteration: 5573 lambda_n: 0.9184376300436207 Loss: 8.510309996473136e-06\n",
      "Iteration: 5574 lambda_n: 1.038177915945602 Loss: 8.473988338407253e-06\n",
      "Iteration: 5575 lambda_n: 1.027030949221468 Loss: 8.433106677851759e-06\n",
      "Iteration: 5576 lambda_n: 1.0276785292161985 Loss: 8.392859254619647e-06\n",
      "Iteration: 5577 lambda_n: 1.0045116131654817 Loss: 8.352778829493179e-06\n",
      "Iteration: 5578 lambda_n: 0.9286956910980855 Loss: 8.313789189811943e-06\n",
      "Iteration: 5579 lambda_n: 0.9872846511222583 Loss: 8.277910714442785e-06\n",
      "Iteration: 5580 lambda_n: 0.9973230646924134 Loss: 8.239933501049213e-06\n",
      "Iteration: 5581 lambda_n: 0.9540988190244669 Loss: 8.201746293034337e-06\n",
      "Iteration: 5582 lambda_n: 0.9429578678277435 Loss: 8.165383568911132e-06\n",
      "Iteration: 5583 lambda_n: 0.9894348050158643 Loss: 8.129604907349078e-06\n",
      "Iteration: 5584 lambda_n: 0.9788711174055638 Loss: 8.092227396859782e-06\n",
      "Iteration: 5585 lambda_n: 1.0062287102609886 Loss: 8.055419090133832e-06\n",
      "Iteration: 5586 lambda_n: 1.018729749580012 Loss: 8.017754292597929e-06\n",
      "Iteration: 5587 lambda_n: 0.9750363918349672 Loss: 7.979799985969412e-06\n",
      "Iteration: 5588 lambda_n: 0.9233185721067008 Loss: 7.943645623072248e-06\n",
      "Iteration: 5589 lambda_n: 0.9278287747591066 Loss: 7.909564181247684e-06\n",
      "Iteration: 5590 lambda_n: 0.9214795040042042 Loss: 7.87546329525776e-06\n",
      "Iteration: 5591 lambda_n: 0.9689157560017704 Loss: 7.841741877429065e-06\n",
      "Iteration: 5592 lambda_n: 0.8927962635284682 Loss: 7.806436456579262e-06\n",
      "Iteration: 5593 lambda_n: 1.0410669580313128 Loss: 7.774051241548967e-06\n",
      "Iteration: 5594 lambda_n: 0.9208119607063632 Loss: 7.736444428113021e-06\n",
      "Iteration: 5595 lambda_n: 0.9468209559400861 Loss: 7.703342632202821e-06\n",
      "Iteration: 5596 lambda_n: 1.03794355318229 Loss: 7.669451571145571e-06\n",
      "Iteration: 5597 lambda_n: 0.9741172701272892 Loss: 7.632462364184053e-06\n",
      "Iteration: 5598 lambda_n: 0.9721947866303118 Loss: 7.597915256578431e-06\n",
      "Iteration: 5599 lambda_n: 0.9107892837489512 Loss: 7.563592479990177e-06\n",
      "Iteration: 5600 lambda_n: 0.9030280516446816 Loss: 7.531582924656999e-06\n",
      "Iteration: 5601 lambda_n: 1.0072906299371764 Loss: 7.499980520177959e-06\n",
      "Iteration: 5602 lambda_n: 0.9427300569739087 Loss: 7.4648773292031995e-06\n",
      "Iteration: 5603 lambda_n: 1.0413446935912782 Loss: 7.43217786402406e-06\n",
      "Iteration: 5604 lambda_n: 0.9396742780738372 Loss: 7.396216159780247e-06\n",
      "Iteration: 5605 lambda_n: 1.0170902776754165 Loss: 7.363922626914635e-06\n",
      "Iteration: 5606 lambda_n: 0.8933402856240585 Loss: 7.329121249869722e-06\n",
      "Iteration: 5607 lambda_n: 0.9729886914114718 Loss: 7.2986987033747004e-06\n",
      "Iteration: 5608 lambda_n: 0.9928158565849217 Loss: 7.265701348147883e-06\n",
      "Iteration: 5609 lambda_n: 0.9914098900894492 Loss: 7.2321838758687086e-06\n",
      "Iteration: 5610 lambda_n: 1.007847736478626 Loss: 7.198868338211055e-06\n",
      "Iteration: 5611 lambda_n: 0.9497721465829922 Loss: 7.165156502343538e-06\n",
      "Iteration: 5612 lambda_n: 1.0293121223413064 Loss: 7.133536092815149e-06\n",
      "Iteration: 5613 lambda_n: 1.0104686171513935 Loss: 7.0994188811111455e-06\n",
      "Iteration: 5614 lambda_n: 0.9671100076751585 Loss: 7.0660864983375705e-06\n",
      "Iteration: 5615 lambda_n: 1.0447925779520892 Loss: 7.034334231247883e-06\n",
      "Iteration: 5616 lambda_n: 0.9089088699201785 Loss: 7.000185684586824e-06\n",
      "Iteration: 5617 lambda_n: 0.9110370267207193 Loss: 6.970622702667816e-06\n",
      "Iteration: 5618 lambda_n: 0.9101147291725913 Loss: 6.941115689475518e-06\n",
      "Iteration: 5619 lambda_n: 1.0108616063501121 Loss: 6.911763372189147e-06\n",
      "Iteration: 5620 lambda_n: 1.0048775946880737 Loss: 6.879299758197589e-06\n",
      "Iteration: 5621 lambda_n: 0.9219965761157743 Loss: 6.847179947741385e-06\n",
      "Iteration: 5622 lambda_n: 1.0119940120074327 Loss: 6.817846985675821e-06\n",
      "Iteration: 5623 lambda_n: 0.9434189335411867 Loss: 6.785788763900926e-06\n",
      "Iteration: 5624 lambda_n: 0.8997756998319733 Loss: 6.756043455473285e-06\n",
      "Iteration: 5625 lambda_n: 0.931039055323107 Loss: 6.727798582729427e-06\n",
      "Iteration: 5626 lambda_n: 1.0483071338110557 Loss: 6.6986945463850055e-06\n",
      "Iteration: 5627 lambda_n: 1.0198598463169257 Loss: 6.666066545430223e-06\n",
      "Iteration: 5628 lambda_n: 1.049037521943906 Loss: 6.634478610687446e-06\n",
      "Iteration: 5629 lambda_n: 0.9922111079478501 Loss: 6.602140973110283e-06\n",
      "Iteration: 5630 lambda_n: 0.9969713085561337 Loss: 6.571704192291536e-06\n",
      "Iteration: 5631 lambda_n: 0.9542584057021796 Loss: 6.541262420801757e-06\n",
      "Iteration: 5632 lambda_n: 0.995030671436899 Loss: 6.512259866628046e-06\n",
      "Iteration: 5633 lambda_n: 0.90133473787557 Loss: 6.482152253245272e-06\n",
      "Iteration: 5634 lambda_n: 0.9717259554669543 Loss: 6.45500581068573e-06\n",
      "Iteration: 5635 lambda_n: 0.9989468762489445 Loss: 6.425861919380186e-06\n",
      "Iteration: 5636 lambda_n: 1.0104776061606535 Loss: 6.396036925723526e-06\n",
      "Iteration: 5637 lambda_n: 1.0327790157126961 Loss: 6.366007729436835e-06\n",
      "Iteration: 5638 lambda_n: 0.9748275773363342 Loss: 6.335459918103877e-06\n",
      "Iteration: 5639 lambda_n: 0.9318361276494659 Loss: 6.306764605176167e-06\n",
      "Iteration: 5640 lambda_n: 0.9584472704428578 Loss: 6.279459069937998e-06\n",
      "Iteration: 5641 lambda_n: 0.9521216952421407 Loss: 6.251495376399716e-06\n",
      "Iteration: 5642 lambda_n: 0.9132496706245747 Loss: 6.223839973323266e-06\n",
      "Iteration: 5643 lambda_n: 1.0084495124971042 Loss: 6.197431024678494e-06\n",
      "Iteration: 5644 lambda_n: 1.0342118376097502 Loss: 6.168392896919535e-06\n",
      "Iteration: 5645 lambda_n: 0.9929056853655438 Loss: 6.1387525129797004e-06\n",
      "Iteration: 5646 lambda_n: 0.999120313078336 Loss: 6.110432727843563e-06\n",
      "Iteration: 5647 lambda_n: 1.0407818576344063 Loss: 6.082067181438355e-06\n",
      "Iteration: 5648 lambda_n: 0.931281183673983 Loss: 6.052656038918563e-06\n",
      "Iteration: 5649 lambda_n: 0.9558646842954052 Loss: 6.026466530090493e-06\n",
      "Iteration: 5650 lambda_n: 0.930399231762707 Loss: 5.9997020192115695e-06\n",
      "Iteration: 5651 lambda_n: 1.010751710429026 Loss: 5.97376627098579e-06\n",
      "Iteration: 5652 lambda_n: 0.9388816346873782 Loss: 5.945712445139212e-06\n",
      "Iteration: 5653 lambda_n: 0.9977753464693014 Loss: 5.919775804011661e-06\n",
      "Iteration: 5654 lambda_n: 1.0244087498613346 Loss: 5.892332483802043e-06\n",
      "Iteration: 5655 lambda_n: 0.9787507015114879 Loss: 5.864287269095377e-06\n",
      "Iteration: 5656 lambda_n: 0.9044050306480302 Loss: 5.837619592161786e-06\n",
      "Iteration: 5657 lambda_n: 0.9466275424672398 Loss: 5.813089664759035e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5658 lambda_n: 1.0171011408115904 Loss: 5.787522455020239e-06\n",
      "Iteration: 5659 lambda_n: 0.9558580065369019 Loss: 5.760172685574259e-06\n",
      "Iteration: 5660 lambda_n: 1.005366738041561 Loss: 5.7345912231402985e-06\n",
      "Iteration: 5661 lambda_n: 0.8993457902748553 Loss: 5.707804281469803e-06\n",
      "Iteration: 5662 lambda_n: 1.0290645541488521 Loss: 5.683954105292065e-06\n",
      "Iteration: 5663 lambda_n: 0.9943714084009231 Loss: 5.656777907540477e-06\n",
      "Iteration: 5664 lambda_n: 0.985642049719074 Loss: 5.630643483686741e-06\n",
      "Iteration: 5665 lambda_n: 0.9160445749787373 Loss: 5.604858188632674e-06\n",
      "Iteration: 5666 lambda_n: 0.9259103718057166 Loss: 5.581003388947797e-06\n",
      "Iteration: 5667 lambda_n: 1.0264992539585094 Loss: 5.556994310766883e-06\n",
      "Iteration: 5668 lambda_n: 0.9318591953801039 Loss: 5.5304914622225405e-06\n",
      "Iteration: 5669 lambda_n: 0.9827380684285536 Loss: 5.506546857474714e-06\n",
      "Iteration: 5670 lambda_n: 0.9908012477373379 Loss: 5.481404239868277e-06\n",
      "Iteration: 5671 lambda_n: 0.8926657020688353 Loss: 5.456171090610668e-06\n",
      "Iteration: 5672 lambda_n: 0.9335354584629114 Loss: 5.43354186875269e-06\n",
      "Iteration: 5673 lambda_n: 1.0335312371825824 Loss: 5.409974756471209e-06\n",
      "Iteration: 5674 lambda_n: 1.0175517417086788 Loss: 5.383996433269693e-06\n",
      "Iteration: 5675 lambda_n: 0.9512208836768783 Loss: 5.358542597031422e-06\n",
      "Iteration: 5676 lambda_n: 0.9223190653520016 Loss: 5.3348605214323245e-06\n",
      "Iteration: 5677 lambda_n: 1.0436652148695396 Loss: 5.3119994965871175e-06\n",
      "Iteration: 5678 lambda_n: 0.9407971298206105 Loss: 5.286241598139668e-06\n",
      "Iteration: 5679 lambda_n: 1.0162067970849775 Loss: 5.263135111945464e-06\n",
      "Iteration: 5680 lambda_n: 1.018960302696242 Loss: 5.238285633150599e-06\n",
      "Iteration: 5681 lambda_n: 0.9487048124551971 Loss: 5.213486480169772e-06\n",
      "Iteration: 5682 lambda_n: 0.9867264062232778 Loss: 5.190506507791025e-06\n",
      "Iteration: 5683 lambda_n: 1.0191500604423505 Loss: 5.166710922077973e-06\n",
      "Iteration: 5684 lambda_n: 0.9744914193226435 Loss: 5.142246105656036e-06\n",
      "Iteration: 5685 lambda_n: 0.9197448198813788 Loss: 5.118964105365835e-06\n",
      "Iteration: 5686 lambda_n: 0.9830332132834548 Loss: 5.097089581353764e-06\n",
      "Iteration: 5687 lambda_n: 0.9613377854775845 Loss: 5.073809772277219e-06\n",
      "Iteration: 5688 lambda_n: 1.0003905016259351 Loss: 5.051147736707527e-06\n",
      "Iteration: 5689 lambda_n: 1.0232248858113455 Loss: 5.027670437799952e-06\n",
      "Iteration: 5690 lambda_n: 0.9011659470464749 Loss: 5.003768882443401e-06\n",
      "Iteration: 5691 lambda_n: 0.9649788630615151 Loss: 4.98281859205715e-06\n",
      "Iteration: 5692 lambda_n: 0.9636823848330948 Loss: 4.960478718757076e-06\n",
      "Iteration: 5693 lambda_n: 0.9473562365977816 Loss: 4.938268894411693e-06\n",
      "Iteration: 5694 lambda_n: 0.9050218749711353 Loss: 4.916533103328435e-06\n",
      "Iteration: 5695 lambda_n: 0.9293992203353748 Loss: 4.89586002095802e-06\n",
      "Iteration: 5696 lambda_n: 0.9629380983585946 Loss: 4.8747193731880955e-06\n",
      "Iteration: 5697 lambda_n: 1.0477068330245634 Loss: 4.852910421853839e-06\n",
      "Iteration: 5698 lambda_n: 0.9671450626229107 Loss: 4.8292877705125786e-06\n",
      "Iteration: 5699 lambda_n: 0.9711049539760475 Loss: 4.807587703663405e-06\n",
      "Iteration: 5700 lambda_n: 0.9070681153349617 Loss: 4.785896704849354e-06\n",
      "Iteration: 5701 lambda_n: 0.9928643566538454 Loss: 4.7657274809992775e-06\n",
      "Iteration: 5702 lambda_n: 0.9357462741255625 Loss: 4.743743572771926e-06\n",
      "Iteration: 5703 lambda_n: 0.9450050843330491 Loss: 4.723119953280666e-06\n",
      "Iteration: 5704 lambda_n: 0.9954912545452637 Loss: 4.702382830091252e-06\n",
      "Iteration: 5705 lambda_n: 0.947417566336734 Loss: 4.680633763144667e-06\n",
      "Iteration: 5706 lambda_n: 0.9167710773161769 Loss: 4.6600307331916e-06\n",
      "Iteration: 5707 lambda_n: 0.9497652443450133 Loss: 4.640181922109212e-06\n",
      "Iteration: 5708 lambda_n: 0.9483179744184694 Loss: 4.6197063561036436e-06\n",
      "Iteration: 5709 lambda_n: 0.9516922860033845 Loss: 4.599352213921515e-06\n",
      "Iteration: 5710 lambda_n: 1.016388593149371 Loss: 4.579015654220295e-06\n",
      "Iteration: 5711 lambda_n: 0.9343973285173558 Loss: 4.55739265181232e-06\n",
      "Iteration: 5712 lambda_n: 0.9250538678068753 Loss: 4.537607839744978e-06\n",
      "Iteration: 5713 lambda_n: 1.0207839781657522 Loss: 4.51810590492588e-06\n",
      "Iteration: 5714 lambda_n: 0.9937540243555638 Loss: 4.496678291830464e-06\n",
      "Iteration: 5715 lambda_n: 0.98418392687972 Loss: 4.4759170143605615e-06\n",
      "Iteration: 5716 lambda_n: 0.9188052301486729 Loss: 4.455450614015668e-06\n",
      "Iteration: 5717 lambda_n: 1.0134750657783875 Loss: 4.43643115834144e-06\n",
      "Iteration: 5718 lambda_n: 0.9167515131367401 Loss: 4.41554158169621e-06\n",
      "Iteration: 5719 lambda_n: 0.9330508850872637 Loss: 4.396734636712923e-06\n",
      "Iteration: 5720 lambda_n: 0.9720771458584919 Loss: 4.3776748490387934e-06\n",
      "Iteration: 5721 lambda_n: 0.9408354913876538 Loss: 4.357903944407619e-06\n",
      "Iteration: 5722 lambda_n: 0.9074117523429951 Loss: 4.338854887479278e-06\n",
      "Iteration: 5723 lambda_n: 0.8945556528931722 Loss: 4.320562874848341e-06\n",
      "Iteration: 5724 lambda_n: 0.9409788444414672 Loss: 4.302606051860091e-06\n",
      "Iteration: 5725 lambda_n: 0.8976243147335375 Loss: 4.2837958657110106e-06\n",
      "Iteration: 5726 lambda_n: 0.9316185235495287 Loss: 4.265930789981211e-06\n",
      "Iteration: 5727 lambda_n: 0.9929082126983628 Loss: 4.247466472873152e-06\n",
      "Iteration: 5728 lambda_n: 1.0143899468097168 Loss: 4.227872602485196e-06\n",
      "Iteration: 5729 lambda_n: 1.0102036470702542 Loss: 4.207947166827135e-06\n",
      "Iteration: 5730 lambda_n: 1.0187909214722213 Loss: 4.188197488207615e-06\n",
      "Iteration: 5731 lambda_n: 0.8931874835546183 Loss: 4.168373416136779e-06\n",
      "Iteration: 5732 lambda_n: 0.9994231905740316 Loss: 4.151075661913095e-06\n",
      "Iteration: 5733 lambda_n: 0.9028783366411108 Loss: 4.1318008390134e-06\n",
      "Iteration: 5734 lambda_n: 0.9749199826841025 Loss: 4.114468835528304e-06\n",
      "Iteration: 5735 lambda_n: 0.9860755103841432 Loss: 4.095832404244437e-06\n",
      "Iteration: 5736 lambda_n: 1.0452774125762476 Loss: 4.077068111934308e-06\n",
      "Iteration: 5737 lambda_n: 0.9304823034300689 Loss: 4.057268384737723e-06\n",
      "Iteration: 5738 lambda_n: 0.8937477478504341 Loss: 4.039728717328682e-06\n",
      "Iteration: 5739 lambda_n: 0.9371074232440075 Loss: 4.0229543362117335e-06\n",
      "Iteration: 5740 lambda_n: 0.9764953238082951 Loss: 4.005439193643661e-06\n",
      "Iteration: 5741 lambda_n: 1.0109540750438153 Loss: 3.987267334955324e-06\n",
      "Iteration: 5742 lambda_n: 1.0006771463630038 Loss: 3.968539582485522e-06\n",
      "Iteration: 5743 lambda_n: 0.9632978402003027 Loss: 3.950089283453083e-06\n",
      "Iteration: 5744 lambda_n: 1.0330082717176259 Loss: 3.932410757711716e-06\n",
      "Iteration: 5745 lambda_n: 0.9900580224304513 Loss: 3.913537752372497e-06\n",
      "Iteration: 5746 lambda_n: 0.9088782397439262 Loss: 3.895536264869844e-06\n",
      "Iteration: 5747 lambda_n: 0.9184581752548682 Loss: 3.879086828798882e-06\n",
      "Iteration: 5748 lambda_n: 0.9801650310466526 Loss: 3.8625342069786895e-06\n",
      "Iteration: 5749 lambda_n: 0.9352205314400229 Loss: 3.844944876828956e-06\n",
      "Iteration: 5750 lambda_n: 0.9955196124886813 Loss: 3.828238520194957e-06\n",
      "Iteration: 5751 lambda_n: 1.0138158426113622 Loss: 3.8105322839702857e-06\n",
      "Iteration: 5752 lambda_n: 0.9815611996234981 Loss: 3.792584038461214e-06\n",
      "Iteration: 5753 lambda_n: 0.9832365310526681 Loss: 3.7752886741830255e-06\n",
      "Iteration: 5754 lambda_n: 1.0429755929635334 Loss: 3.7580428033443966e-06\n",
      "Iteration: 5755 lambda_n: 1.038194803460755 Loss: 3.739832689268174e-06\n",
      "Iteration: 5756 lambda_n: 0.9035773184902519 Loss: 3.7217938888801726e-06\n",
      "Iteration: 5757 lambda_n: 0.9704375419474306 Loss: 3.706169821623907e-06\n",
      "Iteration: 5758 lambda_n: 1.002457890841084 Loss: 3.6894600998669355e-06\n",
      "Iteration: 5759 lambda_n: 0.9615895560555401 Loss: 3.672276857614051e-06\n",
      "Iteration: 5760 lambda_n: 0.9895402785577166 Loss: 3.6558709165462634e-06\n",
      "Iteration: 5761 lambda_n: 1.0124879084947338 Loss: 3.6390635308294054e-06\n",
      "Iteration: 5762 lambda_n: 0.9801338547232079 Loss: 3.6219454465747977e-06\n",
      "Iteration: 5763 lambda_n: 0.9584791550809277 Loss: 3.6054523270129444e-06\n",
      "Iteration: 5764 lambda_n: 0.9922302233830751 Loss: 3.5893970507563377e-06\n",
      "Iteration: 5765 lambda_n: 0.9912655819887101 Loss: 3.5728504361956734e-06\n",
      "Iteration: 5766 lambda_n: 0.9959230750098004 Loss: 3.5563961176937998e-06\n",
      "Iteration: 5767 lambda_n: 0.9999677642206142 Loss: 3.53994062851852e-06\n",
      "Iteration: 5768 lambda_n: 0.9458100742843222 Loss: 3.5234947646763303e-06\n",
      "Iteration: 5769 lambda_n: 0.9366918711557751 Loss: 3.5080118715648694e-06\n",
      "Iteration: 5770 lambda_n: 0.9650611525310037 Loss: 3.492745627523719e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5771 lambda_n: 1.0220393694612444 Loss: 3.4770854733370923e-06\n",
      "Iteration: 5772 lambda_n: 1.0013448347480751 Loss: 3.4605750930759773e-06\n",
      "Iteration: 5773 lambda_n: 0.9907639697678983 Loss: 3.4444758351981446e-06\n",
      "Iteration: 5774 lambda_n: 1.012823698069435 Loss: 3.428620804053855e-06\n",
      "Iteration: 5775 lambda_n: 0.958837493331847 Loss: 3.412487366933887e-06\n",
      "Iteration: 5776 lambda_n: 0.9595609353188234 Loss: 3.3972857605343597e-06\n",
      "Iteration: 5777 lambda_n: 1.0271881691018274 Loss: 3.3821404597165574e-06\n",
      "Iteration: 5778 lambda_n: 0.9239107180966066 Loss: 3.3660000425298547e-06\n",
      "Iteration: 5779 lambda_n: 0.9711176107234982 Loss: 3.3515517322296637e-06\n",
      "Iteration: 5780 lambda_n: 1.0079401460144832 Loss: 3.3364303830540277e-06\n",
      "Iteration: 5781 lambda_n: 0.9628569293700561 Loss: 3.320806483626027e-06\n",
      "Iteration: 5782 lambda_n: 1.0477783516637775 Loss: 3.3059513081949677e-06\n",
      "Iteration: 5783 lambda_n: 1.0351591223497378 Loss: 3.289858265256909e-06\n",
      "Iteration: 5784 lambda_n: 0.9909901856807847 Loss: 3.274036445853058e-06\n",
      "Iteration: 5785 lambda_n: 1.0395052185876301 Loss: 3.2589625742773267e-06\n",
      "Iteration: 5786 lambda_n: 0.9027480940495367 Loss: 3.24322354915693e-06\n",
      "Iteration: 5787 lambda_n: 0.9137930391776485 Loss: 3.229621163868946e-06\n",
      "Iteration: 5788 lambda_n: 0.9972033028764672 Loss: 3.2159101085468506e-06\n",
      "Iteration: 5789 lambda_n: 0.92259883163956 Loss: 3.2010110471394427e-06\n",
      "Iteration: 5790 lambda_n: 1.0487872979008532 Loss: 3.1872905070889316e-06\n",
      "Iteration: 5791 lambda_n: 0.9482693865839023 Loss: 3.171760199972957e-06\n",
      "Iteration: 5792 lambda_n: 0.99741299877944 Loss: 3.1577867748128507e-06\n",
      "Iteration: 5793 lambda_n: 0.9041195981345624 Loss: 3.1431539402895985e-06\n",
      "Iteration: 5794 lambda_n: 0.9959845198510073 Loss: 3.1299512632205627e-06\n",
      "Iteration: 5795 lambda_n: 0.9236268257102335 Loss: 3.1154681984119012e-06\n",
      "Iteration: 5796 lambda_n: 0.9849173615931756 Loss: 3.102099473110539e-06\n",
      "Iteration: 5797 lambda_n: 0.9289780235784949 Loss: 3.0879047968027447e-06\n",
      "Iteration: 5798 lambda_n: 0.9544658559311873 Loss: 3.074577589530853e-06\n",
      "Iteration: 5799 lambda_n: 0.9265490827498539 Loss: 3.0609438339634643e-06\n",
      "Iteration: 5800 lambda_n: 0.9074873086384819 Loss: 3.0477675400370635e-06\n",
      "Iteration: 5801 lambda_n: 0.9007368035041623 Loss: 3.034917877526675e-06\n",
      "Iteration: 5802 lambda_n: 0.89950485755068 Loss: 3.0222175764957233e-06\n",
      "Iteration: 5803 lambda_n: 0.9981389967543933 Loss: 3.0095877249616834e-06\n",
      "Iteration: 5804 lambda_n: 0.9061409297562876 Loss: 2.995631534827217e-06\n",
      "Iteration: 5805 lambda_n: 1.0431997447847559 Loss: 2.9830204393949878e-06\n",
      "Iteration: 5806 lambda_n: 1.005496786793701 Loss: 2.9685629723290738e-06\n",
      "Iteration: 5807 lambda_n: 0.9706034066105848 Loss: 2.954695564812333e-06\n",
      "Iteration: 5808 lambda_n: 1.034805811703002 Loss: 2.9413719307061565e-06\n",
      "Iteration: 5809 lambda_n: 0.8934206994991298 Loss: 2.927231039691756e-06\n",
      "Iteration: 5810 lambda_n: 0.8921860406579935 Loss: 2.915080913210642e-06\n",
      "Iteration: 5811 lambda_n: 0.9466274471795371 Loss: 2.902997944134381e-06\n",
      "Iteration: 5812 lambda_n: 1.0448995065982767 Loss: 2.8902308139738987e-06\n",
      "Iteration: 5813 lambda_n: 0.9270995667187915 Loss: 2.8762002753255795e-06\n",
      "Iteration: 5814 lambda_n: 0.9227373559559411 Loss: 2.863811949906878e-06\n",
      "Iteration: 5815 lambda_n: 0.9110752702417905 Loss: 2.8515350268369e-06\n",
      "Iteration: 5816 lambda_n: 0.9570037545376381 Loss: 2.839465236255115e-06\n",
      "Iteration: 5817 lambda_n: 0.957045627483959 Loss: 2.826840660407598e-06\n",
      "Iteration: 5818 lambda_n: 0.8961495801190154 Loss: 2.8142716700102756e-06\n",
      "Iteration: 5819 lambda_n: 0.9532947380936945 Loss: 2.802554768760418e-06\n",
      "Iteration: 5820 lambda_n: 1.023982039712935 Loss: 2.7901426082659567e-06\n",
      "Iteration: 5821 lambda_n: 1.0413686635669037 Loss: 2.776869133111541e-06\n",
      "Iteration: 5822 lambda_n: 1.0435494271881691 Loss: 2.7634345056835346e-06\n",
      "Iteration: 5823 lambda_n: 0.9679400266969592 Loss: 2.7500368839000034e-06\n",
      "Iteration: 5824 lambda_n: 0.9201668213974217 Loss: 2.7376702277702813e-06\n",
      "Iteration: 5825 lambda_n: 0.9203631091660652 Loss: 2.7259668065456365e-06\n",
      "Iteration: 5826 lambda_n: 1.0214676162572962 Loss: 2.714310935763355e-06\n",
      "Iteration: 5827 lambda_n: 0.9949130442961703 Loss: 2.701429953505373e-06\n",
      "Iteration: 5828 lambda_n: 0.9842668310283499 Loss: 2.6889433758325335e-06\n",
      "Iteration: 5829 lambda_n: 1.003495130230572 Loss: 2.676647516028533e-06\n",
      "Iteration: 5830 lambda_n: 0.8922882588020687 Loss: 2.6641687782478887e-06\n",
      "Iteration: 5831 lambda_n: 0.948124760140438 Loss: 2.6531246631462574e-06\n",
      "Iteration: 5832 lambda_n: 0.9046696642202386 Loss: 2.641438095345805e-06\n",
      "Iteration: 5833 lambda_n: 1.0472518664762762 Loss: 2.6303362769825056e-06\n",
      "Iteration: 5834 lambda_n: 0.9905677809222274 Loss: 2.6175387547280266e-06\n",
      "Iteration: 5835 lambda_n: 1.0210065824830967 Loss: 2.6054928177277732e-06\n",
      "Iteration: 5836 lambda_n: 0.8937078725343438 Loss: 2.5931338699959906e-06\n",
      "Iteration: 5837 lambda_n: 0.9670232463464349 Loss: 2.5823671506576115e-06\n",
      "Iteration: 5838 lambda_n: 1.0368647123512986 Loss: 2.5707655584805296e-06\n",
      "Iteration: 5839 lambda_n: 1.027546774663969 Loss: 2.558381954208018e-06\n",
      "Iteration: 5840 lambda_n: 0.9091147505387778 Loss: 2.546168759698884e-06\n",
      "Iteration: 5841 lambda_n: 0.996172638469275 Loss: 2.5354148106947792e-06\n",
      "Iteration: 5842 lambda_n: 0.96163529966069 Loss: 2.52368082564747e-06\n",
      "Iteration: 5843 lambda_n: 0.9359286704085449 Loss: 2.5124060860475187e-06\n",
      "Iteration: 5844 lambda_n: 1.0191740391151873 Loss: 2.501481774248198e-06\n",
      "Iteration: 5845 lambda_n: 1.0128175698229047 Loss: 2.4896375396169836e-06\n",
      "Iteration: 5846 lambda_n: 0.9007154304289502 Loss: 2.4779229131012865e-06\n",
      "Iteration: 5847 lambda_n: 0.9187849417110204 Loss: 2.4675539272647547e-06\n",
      "Iteration: 5848 lambda_n: 0.9325500047538253 Loss: 2.4570211907373764e-06\n",
      "Iteration: 5849 lambda_n: 0.9845485197673315 Loss: 2.446376291909995e-06\n",
      "Iteration: 5850 lambda_n: 0.9592461033856691 Loss: 2.435186533904742e-06\n",
      "Iteration: 5851 lambda_n: 0.8933096934080047 Loss: 2.424334219129861e-06\n",
      "Iteration: 5852 lambda_n: 0.9795071574881625 Loss: 2.414272911234208e-06\n",
      "Iteration: 5853 lambda_n: 0.9896816490772438 Loss: 2.4032865547439563e-06\n",
      "Iteration: 5854 lambda_n: 1.0435611798696929 Loss: 2.3922365980323716e-06\n",
      "Iteration: 5855 lambda_n: 1.0189667423024764 Loss: 2.3806386452665257e-06\n",
      "Iteration: 5856 lambda_n: 0.9547365672000276 Loss: 2.369368940104662e-06\n",
      "Iteration: 5857 lambda_n: 0.9605782015326629 Loss: 2.358859608557598e-06\n",
      "Iteration: 5858 lambda_n: 1.0314827140994143 Loss: 2.3483328791637907e-06\n",
      "Iteration: 5859 lambda_n: 0.9362520006781769 Loss: 2.3370795754198486e-06\n",
      "Iteration: 5860 lambda_n: 0.9154633828152149 Loss: 2.326914175677718e-06\n",
      "Iteration: 5861 lambda_n: 0.9331160808158506 Loss: 2.3170177278483306e-06\n",
      "Iteration: 5862 lambda_n: 0.9652201249269585 Loss: 2.3069733550225096e-06\n",
      "Iteration: 5863 lambda_n: 1.0348388991274766 Loss: 2.29662844938838e-06\n",
      "Iteration: 5864 lambda_n: 0.961940625328648 Loss: 2.285587132942999e-06\n",
      "Iteration: 5865 lambda_n: 0.9717273106850742 Loss: 2.2753729604425866e-06\n",
      "Iteration: 5866 lambda_n: 0.9162083083193713 Loss: 2.265100986123591e-06\n",
      "Iteration: 5867 lambda_n: 0.916235007227552 Loss: 2.2554596217147122e-06\n",
      "Iteration: 5868 lambda_n: 0.9525160156896298 Loss: 2.2458590204585446e-06\n",
      "Iteration: 5869 lambda_n: 1.012773574979351 Loss: 2.235920744238572e-06\n",
      "Iteration: 5870 lambda_n: 0.9111442899588911 Loss: 2.2254005239473007e-06\n",
      "Iteration: 5871 lambda_n: 1.0184027990669844 Loss: 2.215980517969093e-06\n",
      "Iteration: 5872 lambda_n: 1.0065272603356452 Loss: 2.2054961767702545e-06\n",
      "Iteration: 5873 lambda_n: 0.9331415915840598 Loss: 2.1951831240600436e-06\n",
      "Iteration: 5874 lambda_n: 0.9753396646391227 Loss: 2.185666707256733e-06\n",
      "Iteration: 5875 lambda_n: 0.9255631641408685 Loss: 2.175763069284162e-06\n",
      "Iteration: 5876 lambda_n: 0.9229266248472815 Loss: 2.1664074537446906e-06\n",
      "Iteration: 5877 lambda_n: 0.9952681785045128 Loss: 2.157118606929023e-06\n",
      "Iteration: 5878 lambda_n: 1.049161892978064 Loss: 2.1471446288609665e-06\n",
      "Iteration: 5879 lambda_n: 0.9875269940621095 Loss: 2.1366791806224994e-06\n",
      "Iteration: 5880 lambda_n: 0.9069676670440758 Loss: 2.1268765628045367e-06\n",
      "Iteration: 5881 lambda_n: 1.0023222865370316 Loss: 2.117914920027332e-06\n",
      "Iteration: 5882 lambda_n: 0.9873072213964271 Loss: 2.1080528242686557e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5883 lambda_n: 1.0268753422345187 Loss: 2.098383705874681e-06\n",
      "Iteration: 5884 lambda_n: 1.021388826798211 Loss: 2.0883732129319243e-06\n",
      "Iteration: 5885 lambda_n: 0.9072901830000191 Loss: 2.078463711653121e-06\n",
      "Iteration: 5886 lambda_n: 0.917855968664785 Loss: 2.069702967696692e-06\n",
      "Iteration: 5887 lambda_n: 0.9703805901228947 Loss: 2.060877562313749e-06\n",
      "Iteration: 5888 lambda_n: 0.9885862320829922 Loss: 2.051586910854689e-06\n",
      "Iteration: 5889 lambda_n: 0.9709173805593301 Loss: 2.042164628588221e-06\n",
      "Iteration: 5890 lambda_n: 0.9220467137959396 Loss: 2.0329532547259287e-06\n",
      "Iteration: 5891 lambda_n: 0.9969134084229826 Loss: 2.0242449933423395e-06\n",
      "Iteration: 5892 lambda_n: 0.9745115177967003 Loss: 2.0148699903374488e-06\n",
      "Iteration: 5893 lambda_n: 1.0358754412967373 Loss: 2.005748104020808e-06\n",
      "Iteration: 5894 lambda_n: 1.0170966083630417 Loss: 1.9960957258787555e-06\n",
      "Iteration: 5895 lambda_n: 0.9989769653974441 Loss: 1.986663944943304e-06\n",
      "Iteration: 5896 lambda_n: 0.9004924255352537 Loss: 1.9774439696876464e-06\n",
      "Iteration: 5897 lambda_n: 0.9022842674657866 Loss: 1.9691715251799073e-06\n",
      "Iteration: 5898 lambda_n: 0.9424459744116946 Loss: 1.9609173000770895e-06\n",
      "Iteration: 5899 lambda_n: 0.9933555918731563 Loss: 1.952331814139417e-06\n",
      "Iteration: 5900 lambda_n: 0.9623724024148633 Loss: 1.943322177766658e-06\n",
      "Iteration: 5901 lambda_n: 0.9164535449459308 Loss: 1.934633842003276e-06\n",
      "Iteration: 5902 lambda_n: 0.9672218499455779 Loss: 1.926397059309575e-06\n",
      "Iteration: 5903 lambda_n: 0.9271652849337421 Loss: 1.9177410038025955e-06\n",
      "Iteration: 5904 lambda_n: 0.9216601574050055 Loss: 1.9094807197072413e-06\n",
      "Iteration: 5905 lambda_n: 0.9299419989890775 Loss: 1.9013048547559703e-06\n",
      "Iteration: 5906 lambda_n: 0.9709419306320655 Loss: 1.8930908492883852e-06\n",
      "Iteration: 5907 lambda_n: 0.9444001245862067 Loss: 1.8845517546277372e-06\n",
      "Iteration: 5908 lambda_n: 1.0321470083563509 Loss: 1.876283555007479e-06\n",
      "Iteration: 5909 lambda_n: 1.0269892224328556 Loss: 1.8672867849783222e-06\n",
      "Iteration: 5910 lambda_n: 0.9997385299009166 Loss: 1.8583779028361168e-06\n",
      "Iteration: 5911 lambda_n: 1.0319830629425017 Loss: 1.8497467962032373e-06\n",
      "Iteration: 5912 lambda_n: 0.9913938379716402 Loss: 1.8408786959573756e-06\n",
      "Iteration: 5913 lambda_n: 0.9205829678886314 Loss: 1.8324002386397672e-06\n",
      "Iteration: 5914 lambda_n: 0.8928257164535427 Loss: 1.8245636247944346e-06\n",
      "Iteration: 5915 lambda_n: 0.9792761819925165 Loss: 1.8169958079233585e-06\n",
      "Iteration: 5916 lambda_n: 1.0313327739149136 Loss: 1.808729648550041e-06\n",
      "Iteration: 5917 lambda_n: 0.9742171121544286 Loss: 1.8000636850763871e-06\n",
      "Iteration: 5918 lambda_n: 0.9829908501974691 Loss: 1.7919168728777582e-06\n",
      "Iteration: 5919 lambda_n: 1.0361777572235717 Loss: 1.7837338995788145e-06\n",
      "Iteration: 5920 lambda_n: 1.0118866949718477 Loss: 1.7751475642346598e-06\n",
      "Iteration: 5921 lambda_n: 0.9093043269710699 Loss: 1.7668028866894103e-06\n",
      "Iteration: 5922 lambda_n: 1.0491829409101172 Loss: 1.759339425567238e-06\n",
      "Iteration: 5923 lambda_n: 1.0135260259546466 Loss: 1.7507642404591597e-06\n",
      "Iteration: 5924 lambda_n: 0.9388364212891114 Loss: 1.7425208682210825e-06\n",
      "Iteration: 5925 lambda_n: 1.0404593148151282 Loss: 1.7349209318894986e-06\n",
      "Iteration: 5926 lambda_n: 1.0424781800574776 Loss: 1.726535092392409e-06\n",
      "Iteration: 5927 lambda_n: 1.00602301326493 Loss: 1.7181735994526518e-06\n",
      "Iteration: 5928 lambda_n: 0.9001532935099543 Loss: 1.7101435894511086e-06\n",
      "Iteration: 5929 lambda_n: 0.9462628610016792 Loss: 1.702992209121088e-06\n",
      "Iteration: 5930 lambda_n: 1.0448301819301975 Loss: 1.6955059473989785e-06\n",
      "Iteration: 5931 lambda_n: 0.977477106105604 Loss: 1.6872762230926383e-06\n",
      "Iteration: 5932 lambda_n: 0.9195671861354748 Loss: 1.679614389439965e-06\n",
      "Iteration: 5933 lambda_n: 0.910658964489301 Loss: 1.672439211289393e-06\n",
      "Iteration: 5934 lambda_n: 0.9883960134403251 Loss: 1.6653639015119234e-06\n",
      "Iteration: 5935 lambda_n: 1.0274225543028324 Loss: 1.6577171109141133e-06\n",
      "Iteration: 5936 lambda_n: 0.9058246187091126 Loss: 1.6498048924366637e-06\n",
      "Iteration: 5937 lambda_n: 1.0408989785655818 Loss: 1.6428624044167237e-06\n",
      "Iteration: 5938 lambda_n: 1.0322424508443946 Loss: 1.6349182456048908e-06\n",
      "Iteration: 5939 lambda_n: 0.9052276609526883 Loss: 1.6270782545335951e-06\n",
      "Iteration: 5940 lambda_n: 0.9955516143058251 Loss: 1.6202359288831198e-06\n",
      "Iteration: 5941 lambda_n: 1.0193892637912252 Loss: 1.6127425234510614e-06\n",
      "Iteration: 5942 lambda_n: 1.038170218208121 Loss: 1.6051051863082252e-06\n",
      "Iteration: 5943 lambda_n: 0.9775192076816294 Loss: 1.5973639807044745e-06\n",
      "Iteration: 5944 lambda_n: 1.0139573015883494 Loss: 1.5901101838857712e-06\n",
      "Iteration: 5945 lambda_n: 1.0408759274404698 Loss: 1.5826201675601563e-06\n",
      "Iteration: 5946 lambda_n: 1.008217876054926 Loss: 1.574967528973685e-06\n",
      "Iteration: 5947 lambda_n: 0.9535721919819993 Loss: 1.567590844703149e-06\n",
      "Iteration: 5948 lambda_n: 0.957117864538983 Loss: 1.5606466616785965e-06\n",
      "Iteration: 5949 lambda_n: 1.038276354804197 Loss: 1.5537075391993443e-06\n",
      "Iteration: 5950 lambda_n: 1.0313936075427728 Loss: 1.5462134914183718e-06\n",
      "Iteration: 5951 lambda_n: 0.9354860358295553 Loss: 1.5388050344429028e-06\n",
      "Iteration: 5952 lambda_n: 0.9459107848512381 Loss: 1.5321176786873167e-06\n",
      "Iteration: 5953 lambda_n: 0.9642263746512195 Loss: 1.5253851920473732e-06\n",
      "Iteration: 5954 lambda_n: 0.8929486910605849 Loss: 1.5185525069236713e-06\n",
      "Iteration: 5955 lambda_n: 0.9240544839914238 Loss: 1.5122532567132083e-06\n",
      "Iteration: 5956 lambda_n: 0.9551376154306948 Loss: 1.5057616180254903e-06\n",
      "Iteration: 5957 lambda_n: 0.9730889819359972 Loss: 1.499080424047775e-06\n",
      "Iteration: 5958 lambda_n: 1.035794432157351 Loss: 1.4923038675421308e-06\n",
      "Iteration: 5959 lambda_n: 1.0009238424557125 Loss: 1.48512324546818e-06\n",
      "Iteration: 5960 lambda_n: 0.9593021513234142 Loss: 1.4782177571224147e-06\n",
      "Iteration: 5961 lambda_n: 0.9233393140999947 Loss: 1.4716302007527341e-06\n",
      "Iteration: 5962 lambda_n: 0.9520625590076272 Loss: 1.465317863571195e-06\n",
      "Iteration: 5963 lambda_n: 1.0055163640016314 Loss: 1.458837085231644e-06\n",
      "Iteration: 5964 lambda_n: 1.0323941109834027 Loss: 1.452022719617603e-06\n",
      "Iteration: 5965 lambda_n: 0.8941744922303654 Loss: 1.4450588912534227e-06\n",
      "Iteration: 5966 lambda_n: 0.9794176377264887 Loss: 1.4390563303212289e-06\n",
      "Iteration: 5967 lambda_n: 0.9269062103399119 Loss: 1.4325088509277666e-06\n",
      "Iteration: 5968 lambda_n: 0.9387653790679392 Loss: 1.4263406122192114e-06\n",
      "Iteration: 5969 lambda_n: 0.9041959194842564 Loss: 1.4201203594060208e-06\n",
      "Iteration: 5970 lambda_n: 0.99091681885483 Loss: 1.4141552959146595e-06\n",
      "Iteration: 5971 lambda_n: 0.9389485498113389 Loss: 1.4076455904213462e-06\n",
      "Iteration: 5972 lambda_n: 1.039038968908837 Loss: 1.4015056835228938e-06\n",
      "Iteration: 5973 lambda_n: 0.9425706627899272 Loss: 1.3947409139290931e-06\n",
      "Iteration: 5974 lambda_n: 0.9623431383737047 Loss: 1.3886338371848378e-06\n",
      "Iteration: 5975 lambda_n: 0.9791668302836302 Loss: 1.3824259580055564e-06\n",
      "Iteration: 5976 lambda_n: 0.9102382059261801 Loss: 1.3761377954102384e-06\n",
      "Iteration: 5977 lambda_n: 0.9197784049455506 Loss: 1.3703188833002793e-06\n",
      "Iteration: 5978 lambda_n: 1.0209068011894016 Loss: 1.3644638507576394e-06\n",
      "Iteration: 5979 lambda_n: 0.8943524606491873 Loss: 1.3579928382905975e-06\n",
      "Iteration: 5980 lambda_n: 0.9551896449348332 Loss: 1.352350879715642e-06\n",
      "Iteration: 5981 lambda_n: 0.9534154102416403 Loss: 1.346350173709955e-06\n",
      "Iteration: 5982 lambda_n: 1.0072179367521743 Loss: 1.340387196098267e-06\n",
      "Iteration: 5983 lambda_n: 0.9975446562603323 Loss: 1.3341156253783625e-06\n",
      "Iteration: 5984 lambda_n: 0.9601987853524658 Loss: 1.3279333546676488e-06\n",
      "Iteration: 5985 lambda_n: 0.9276318407348129 Loss: 1.3220101159719834e-06\n",
      "Iteration: 5986 lambda_n: 1.0393038697018118 Loss: 1.3163133045526085e-06\n",
      "Iteration: 5987 lambda_n: 0.9220262824473039 Loss: 1.309958197706308e-06\n",
      "Iteration: 5988 lambda_n: 1.0104981261029955 Loss: 1.304347442024568e-06\n",
      "Iteration: 5989 lambda_n: 0.9643555478837023 Loss: 1.2982246565056308e-06\n",
      "Iteration: 5990 lambda_n: 0.9727788991899234 Loss: 1.292408891298123e-06\n",
      "Iteration: 5991 lambda_n: 0.9411922745164093 Loss: 1.2865686134288541e-06\n",
      "Iteration: 5992 lambda_n: 1.0353225891742377 Loss: 1.2809435122939613e-06\n",
      "Iteration: 5993 lambda_n: 1.0444115646752246 Loss: 1.2747828939655063e-06\n",
      "Iteration: 5994 lambda_n: 1.024186903036887 Loss: 1.2685980876863385e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5995 lambda_n: 1.0071037514034786 Loss: 1.2625624795809381e-06\n",
      "Iteration: 5996 lambda_n: 1.0260233946401074 Loss: 1.2566557862673044e-06\n",
      "Iteration: 5997 lambda_n: 1.0163382621234396 Loss: 1.2506662872020192e-06\n",
      "Iteration: 5998 lambda_n: 1.0221559377031675 Loss: 1.244761609719235e-06\n",
      "Iteration: 5999 lambda_n: 0.9905585233823831 Loss: 1.2388511757854445e-06\n",
      "Iteration: 6000 lambda_n: 0.9442672286428974 Loss: 1.2331506507116333e-06\n",
      "Iteration: 6001 lambda_n: 0.9825936457590386 Loss: 1.2277415357976102e-06\n",
      "Iteration: 6002 lambda_n: 0.9913956792243963 Loss: 1.2221375678652585e-06\n",
      "Iteration: 6003 lambda_n: 1.005447968337984 Loss: 1.2165092135776966e-06\n",
      "Iteration: 6004 lambda_n: 1.0453946552530162 Loss: 1.2108273751850214e-06\n",
      "Iteration: 6005 lambda_n: 0.9536237043756713 Loss: 1.2049473940380653e-06\n",
      "Iteration: 6006 lambda_n: 0.970999710964295 Loss: 1.1996096457913031e-06\n",
      "Iteration: 6007 lambda_n: 1.0168424723575296 Loss: 1.194198719915569e-06\n",
      "Iteration: 6008 lambda_n: 0.9217433603654444 Loss: 1.1885578981582953e-06\n",
      "Iteration: 6009 lambda_n: 0.9914576858135136 Loss: 1.183468786279865e-06\n",
      "Iteration: 6010 lambda_n: 1.027239230763271 Loss: 1.1780182126641854e-06\n",
      "Iteration: 6011 lambda_n: 1.0373715361159173 Loss: 1.172396943697336e-06\n",
      "Iteration: 6012 lambda_n: 1.0029555405535528 Loss: 1.1667473229270446e-06\n",
      "Iteration: 6013 lambda_n: 0.9601239670883207 Loss: 1.1613114623577978e-06\n",
      "Iteration: 6014 lambda_n: 0.9421092984668955 Loss: 1.1561319917651203e-06\n",
      "Iteration: 6015 lambda_n: 0.9512619633137521 Loss: 1.1510723751647855e-06\n",
      "Iteration: 6016 lambda_n: 0.9508458307045237 Loss: 1.1459859668563838e-06\n",
      "Iteration: 6017 lambda_n: 1.0191891431994606 Loss: 1.1409242550428403e-06\n",
      "Iteration: 6018 lambda_n: 1.0042780571008811 Loss: 1.1355226956142137e-06\n",
      "Iteration: 6019 lambda_n: 0.96022659202682 Loss: 1.1302253676212601e-06\n",
      "Iteration: 6020 lambda_n: 0.9404491876856693 Loss: 1.1251840347828322e-06\n",
      "Iteration: 6021 lambda_n: 0.9976524876129503 Loss: 1.1202685650474771e-06\n",
      "Iteration: 6022 lambda_n: 0.9809549002648736 Loss: 1.1150768946381298e-06\n",
      "Iteration: 6023 lambda_n: 1.046028087762078 Loss: 1.1099957793864772e-06\n",
      "Iteration: 6024 lambda_n: 1.0441855235192457 Loss: 1.1046022955466293e-06\n",
      "Iteration: 6025 lambda_n: 1.0082113507616655 Loss: 1.0992444794491677e-06\n",
      "Iteration: 6026 lambda_n: 0.9249978572402652 Loss: 1.0940963488516827e-06\n",
      "Iteration: 6027 lambda_n: 0.9265135118550247 Loss: 1.0893952490282588e-06\n",
      "Iteration: 6028 lambda_n: 0.9700994003427114 Loss: 1.0847066839152584e-06\n",
      "Iteration: 6029 lambda_n: 1.0024053427969355 Loss: 1.0798186883502043e-06\n",
      "Iteration: 6030 lambda_n: 1.0070568851164685 Loss: 1.0747906801859927e-06\n",
      "Iteration: 6031 lambda_n: 0.9515509945179625 Loss: 1.069762866820563e-06\n",
      "Iteration: 6032 lambda_n: 0.9283198334359982 Loss: 1.0650344001942725e-06\n",
      "Iteration: 6033 lambda_n: 0.9789375109275582 Loss: 1.0604417695597834e-06\n",
      "Iteration: 6034 lambda_n: 1.0187453747322468 Loss: 1.0556196100707763e-06\n",
      "Iteration: 6035 lambda_n: 0.9787191133011174 Loss: 1.0506241859375154e-06\n",
      "Iteration: 6036 lambda_n: 0.9401630055352244 Loss: 1.0458477472942301e-06\n",
      "Iteration: 6037 lambda_n: 0.9186611862333663 Loss: 1.0412803388422099e-06\n",
      "Iteration: 6038 lambda_n: 0.9842876636073234 Loss: 1.0368368840038698e-06\n",
      "Iteration: 6039 lambda_n: 0.8983788650975685 Loss: 1.0320963231266462e-06\n",
      "Iteration: 6040 lambda_n: 1.0263279981876732 Loss: 1.0277893071410606e-06\n",
      "Iteration: 6041 lambda_n: 1.0337841448370526 Loss: 1.022889415030664e-06\n",
      "Iteration: 6042 lambda_n: 0.9188612294149496 Loss: 1.0179774615339045e-06\n",
      "Iteration: 6043 lambda_n: 1.033988661895965 Loss: 1.0136325270804966e-06\n",
      "Iteration: 6044 lambda_n: 0.9377222368285506 Loss: 1.0087640743898455e-06\n",
      "Iteration: 6045 lambda_n: 0.9448173665472885 Loss: 1.004370096172845e-06\n",
      "Iteration: 6046 lambda_n: 1.0062244878429867 Loss: 9.99962160921745e-07\n",
      "Iteration: 6047 lambda_n: 0.995475032530455 Loss: 9.952883462192465e-07\n",
      "Iteration: 6048 lambda_n: 1.0443745714046755 Loss: 9.90686079569223e-07\n",
      "Iteration: 6049 lambda_n: 1.009318390617789 Loss: 9.858800738644818e-07\n",
      "Iteration: 6050 lambda_n: 0.9710019600065035 Loss: 9.81257928240281e-07\n",
      "Iteration: 6051 lambda_n: 0.8939986964922652 Loss: 9.768321049896349e-07\n",
      "Iteration: 6052 lambda_n: 0.9809374948931776 Loss: 9.727756464210056e-07\n",
      "Iteration: 6053 lambda_n: 0.9439387965972836 Loss: 9.683431973975068e-07\n",
      "Iteration: 6054 lambda_n: 1.0042966300132752 Loss: 9.640973702723647e-07\n",
      "Iteration: 6055 lambda_n: 0.8968663590863388 Loss: 9.595998665963728e-07\n",
      "Iteration: 6056 lambda_n: 0.9803237400211638 Loss: 9.55602205633842e-07\n",
      "Iteration: 6057 lambda_n: 1.0013522237073034 Loss: 9.512507537420441e-07\n",
      "Iteration: 6058 lambda_n: 1.040821180870131 Loss: 9.468262065390768e-07\n",
      "Iteration: 6059 lambda_n: 0.993497577968056 Loss: 9.422486601307501e-07\n",
      "Iteration: 6060 lambda_n: 1.0346408400494367 Loss: 9.379003741995047e-07\n",
      "Iteration: 6061 lambda_n: 0.9718885888977804 Loss: 9.333929182657189e-07\n",
      "Iteration: 6062 lambda_n: 0.9692522371133552 Loss: 9.291791996357963e-07\n",
      "Iteration: 6063 lambda_n: 1.01158715915447 Loss: 9.249958876242543e-07\n",
      "Iteration: 6064 lambda_n: 0.9197997011381577 Loss: 9.206495196529176e-07\n",
      "Iteration: 6065 lambda_n: 0.9342303551463615 Loss: 9.167160992323535e-07\n",
      "Iteration: 6066 lambda_n: 0.9774368540039403 Loss: 9.127380418256963e-07\n",
      "Iteration: 6067 lambda_n: 0.9796121770076436 Loss: 9.085940727755537e-07\n",
      "Iteration: 6068 lambda_n: 1.0209043004589191 Loss: 9.044597430019663e-07\n",
      "Iteration: 6069 lambda_n: 0.9423064012116439 Loss: 9.001707561957851e-07\n",
      "Iteration: 6070 lambda_n: 0.9103165053962017 Loss: 8.96230750554926e-07\n",
      "Iteration: 6071 lambda_n: 0.8972439870480439 Loss: 8.924411671167662e-07\n",
      "Iteration: 6072 lambda_n: 0.9544990302895676 Loss: 8.887218021441258e-07\n",
      "Iteration: 6073 lambda_n: 1.0140515970584787 Loss: 8.8478159182165e-07\n",
      "Iteration: 6074 lambda_n: 1.0172650183348255 Loss: 8.806141110567456e-07\n",
      "Iteration: 6075 lambda_n: 0.9736326801191284 Loss: 8.764531219732691e-07\n",
      "Iteration: 6076 lambda_n: 0.9920402440251835 Loss: 8.724894289497337e-07\n",
      "Iteration: 6077 lambda_n: 0.9403545051409634 Loss: 8.684690682942059e-07\n",
      "Iteration: 6078 lambda_n: 0.8932521879888599 Loss: 8.646757361402899e-07\n",
      "Iteration: 6079 lambda_n: 0.9905552660968764 Loss: 8.610881556102291e-07\n",
      "Iteration: 6080 lambda_n: 0.9423298108538884 Loss: 8.571262872895547e-07\n",
      "Iteration: 6081 lambda_n: 1.0464536657158425 Loss: 8.533746503212773e-07\n",
      "Iteration: 6082 lambda_n: 0.9532792117009632 Loss: 8.4922671306502e-07\n",
      "Iteration: 6083 lambda_n: 0.9540079026142002 Loss: 8.454664735120002e-07\n",
      "Iteration: 6084 lambda_n: 1.013553454523853 Loss: 8.417200275619356e-07\n",
      "Iteration: 6085 lambda_n: 1.017431715350199 Loss: 8.37757386050879e-07\n",
      "Iteration: 6086 lambda_n: 0.9206250273357958 Loss: 8.337983147962799e-07\n",
      "Iteration: 6087 lambda_n: 1.0328348474584486 Loss: 8.30232876834016e-07\n",
      "Iteration: 6088 lambda_n: 0.9596189165775856 Loss: 8.262499780160404e-07\n",
      "Iteration: 6089 lambda_n: 0.9222941877133619 Loss: 8.225671790004857e-07\n",
      "Iteration: 6090 lambda_n: 0.9352696941309953 Loss: 8.190434058287181e-07\n",
      "Iteration: 6091 lambda_n: 0.9008290574020928 Loss: 8.154853706495665e-07\n",
      "Iteration: 6092 lambda_n: 0.9525878209523027 Loss: 8.120732500909332e-07\n",
      "Iteration: 6093 lambda_n: 1.036300738913957 Loss: 8.084801823579061e-07\n",
      "Iteration: 6094 lambda_n: 0.9255059618438474 Loss: 8.045886585757578e-07\n",
      "Iteration: 6095 lambda_n: 1.0280001771327685 Loss: 8.011299266741792e-07\n",
      "Iteration: 6096 lambda_n: 1.0096114642439453 Loss: 7.97304681672509e-07\n",
      "Iteration: 6097 lambda_n: 1.0345455327494557 Loss: 7.93565806514659e-07\n",
      "Iteration: 6098 lambda_n: 0.9425892067862698 Loss: 7.897525659232821e-07\n",
      "Iteration: 6099 lambda_n: 0.9856589450065045 Loss: 7.862949686032033e-07\n",
      "Iteration: 6100 lambda_n: 0.9065061101663892 Loss: 7.82695218241241e-07\n",
      "Iteration: 6101 lambda_n: 0.9508300346411722 Loss: 7.7939970608369e-07\n",
      "Iteration: 6102 lambda_n: 0.9872131386589661 Loss: 7.75957618129186e-07\n",
      "Iteration: 6103 lambda_n: 0.979151224561254 Loss: 7.723996089871437e-07\n",
      "Iteration: 6104 lambda_n: 0.9053782353274966 Loss: 7.68886843017262e-07\n",
      "Iteration: 6105 lambda_n: 1.0198786681358845 Loss: 7.656535195730803e-07\n",
      "Iteration: 6106 lambda_n: 0.9836010081999853 Loss: 7.620266095720992e-07\n",
      "Iteration: 6107 lambda_n: 0.9849554382520396 Loss: 7.585452865557296e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6108 lambda_n: 0.906364060659222 Loss: 7.550751020237606e-07\n",
      "Iteration: 6109 lambda_n: 0.9560790334669418 Loss: 7.518964239103463e-07\n",
      "Iteration: 6110 lambda_n: 0.9222302906310764 Loss: 7.485575129222159e-07\n",
      "Iteration: 6111 lambda_n: 0.9543970705425133 Loss: 7.453511191891695e-07\n",
      "Iteration: 6112 lambda_n: 0.9010284473493969 Loss: 7.420471074192179e-07\n",
      "Iteration: 6113 lambda_n: 1.0299719513514074 Loss: 7.38941684038527e-07\n",
      "Iteration: 6114 lambda_n: 0.9425425842143937 Loss: 7.354067143372702e-07\n",
      "Iteration: 6115 lambda_n: 0.9937060988486125 Loss: 7.321872924261359e-07\n",
      "Iteration: 6116 lambda_n: 0.9279827394436536 Loss: 7.288079770890248e-07\n",
      "Iteration: 6117 lambda_n: 1.026943667509298 Loss: 7.256667393555135e-07\n",
      "Iteration: 6118 lambda_n: 0.9022799183175245 Loss: 7.222055059766184e-07\n",
      "Iteration: 6119 lambda_n: 0.918804057807024 Loss: 7.191789527739161e-07\n",
      "Iteration: 6120 lambda_n: 0.9217514015746159 Loss: 7.161098928575877e-07\n",
      "Iteration: 6121 lambda_n: 0.9319196365811248 Loss: 7.130441323284694e-07\n",
      "Iteration: 6122 lambda_n: 0.9860330371621302 Loss: 7.099578271320493e-07\n",
      "Iteration: 6123 lambda_n: 0.9930437511138791 Loss: 7.067064507315582e-07\n",
      "Iteration: 6124 lambda_n: 0.894575815606057 Loss: 7.034469592113738e-07\n",
      "Iteration: 6125 lambda_n: 0.9475544900712501 Loss: 7.00524219716992e-07\n",
      "Iteration: 6126 lambda_n: 0.912403858326184 Loss: 6.974412575208277e-07\n",
      "Iteration: 6127 lambda_n: 0.983493982952712 Loss: 6.944857313791156e-07\n",
      "Iteration: 6128 lambda_n: 0.9984687038984312 Loss: 6.913134308544948e-07\n",
      "Iteration: 6129 lambda_n: 0.9069464657124854 Loss: 6.881075461071316e-07\n",
      "Iteration: 6130 lambda_n: 0.9736122950644239 Loss: 6.852090309107586e-07\n",
      "Iteration: 6131 lambda_n: 0.9553481516031653 Loss: 6.82110570447661e-07\n",
      "Iteration: 6132 lambda_n: 0.9186889721104703 Loss: 6.790839884737787e-07\n",
      "Iteration: 6133 lambda_n: 0.9918855213106207 Loss: 6.761864636608329e-07\n",
      "Iteration: 6134 lambda_n: 0.9100618476885649 Loss: 6.730714325239306e-07\n",
      "Iteration: 6135 lambda_n: 1.015034226904666 Loss: 6.702265419193439e-07\n",
      "Iteration: 6136 lambda_n: 0.9063536413196606 Loss: 6.670669207857907e-07\n",
      "Iteration: 6137 lambda_n: 1.0427878356702667 Loss: 6.642589091942826e-07\n",
      "Iteration: 6138 lambda_n: 0.9626307348507087 Loss: 6.610418108105329e-07\n",
      "Iteration: 6139 lambda_n: 1.039762537972936 Loss: 6.580863940947539e-07\n",
      "Iteration: 6140 lambda_n: 0.9676102357109777 Loss: 6.549084497727604e-07\n",
      "Iteration: 6141 lambda_n: 0.9587202714597604 Loss: 6.519653206459993e-07\n",
      "Iteration: 6142 lambda_n: 1.0159145477185791 Loss: 6.490623422910541e-07\n",
      "Iteration: 6143 lambda_n: 0.9492732938979415 Loss: 6.459998845335758e-07\n",
      "Iteration: 6144 lambda_n: 1.006790804843365 Loss: 6.431518235308236e-07\n",
      "Iteration: 6145 lambda_n: 0.9493482707485849 Loss: 6.401445186983546e-07\n",
      "Iteration: 6146 lambda_n: 1.0253217979748162 Loss: 6.373220614751221e-07\n",
      "Iteration: 6147 lambda_n: 1.0343702433736497 Loss: 6.342871779524863e-07\n",
      "Iteration: 6148 lambda_n: 0.906360727015734 Loss: 6.312400978391035e-07\n",
      "Iteration: 6149 lambda_n: 0.8956504239989694 Loss: 6.285829445379721e-07\n",
      "Iteration: 6150 lambda_n: 0.9495959242944271 Loss: 6.259682484236709e-07\n",
      "Iteration: 6151 lambda_n: 0.9706468308226929 Loss: 6.232076045576743e-07\n",
      "Iteration: 6152 lambda_n: 0.9508164789417429 Loss: 6.203982127688005e-07\n",
      "Iteration: 6153 lambda_n: 0.9047535623509394 Loss: 6.176586287254556e-07\n",
      "Iteration: 6154 lambda_n: 0.9244749955855344 Loss: 6.150632825860497e-07\n",
      "Iteration: 6155 lambda_n: 0.9503704004851622 Loss: 6.124225127000683e-07\n",
      "Iteration: 6156 lambda_n: 0.9471448957606262 Loss: 6.09719433731341e-07\n",
      "Iteration: 6157 lambda_n: 0.9268720402594341 Loss: 6.07037424855348e-07\n",
      "Iteration: 6158 lambda_n: 0.9769241006644861 Loss: 6.04424372791082e-07\n",
      "Iteration: 6159 lambda_n: 0.996508216842904 Loss: 6.016820745916231e-07\n",
      "Iteration: 6160 lambda_n: 0.9873975981024917 Loss: 5.988974999319047e-07\n",
      "Iteration: 6161 lambda_n: 0.9102627913155902 Loss: 5.96151158843261e-07\n",
      "Iteration: 6162 lambda_n: 0.9301053734994555 Loss: 5.936309757357486e-07\n",
      "Iteration: 6163 lambda_n: 0.9432975519065201 Loss: 5.910667473908258e-07\n",
      "Iteration: 6164 lambda_n: 0.9362343687741902 Loss: 5.884773883832361e-07\n",
      "Iteration: 6165 lambda_n: 1.0469581730276865 Loss: 5.859186821565072e-07\n",
      "Iteration: 6166 lambda_n: 1.0147864288895256 Loss: 5.830698178049643e-07\n",
      "Iteration: 6167 lambda_n: 0.9374631656852651 Loss: 5.803219285993641e-07\n",
      "Iteration: 6168 lambda_n: 0.9065839071709757 Loss: 5.777953887859922e-07\n",
      "Iteration: 6169 lambda_n: 0.9347698653126253 Loss: 5.75362714053308e-07\n",
      "Iteration: 6170 lambda_n: 0.946145264626186 Loss: 5.728649729020739e-07\n",
      "Iteration: 6171 lambda_n: 0.983962458833002 Loss: 5.703478170538948e-07\n",
      "Iteration: 6172 lambda_n: 0.9493962124377864 Loss: 5.677415595733603e-07\n",
      "Iteration: 6173 lambda_n: 1.0494524609148346 Loss: 5.652383562239137e-07\n",
      "Iteration: 6174 lambda_n: 0.9231433285622552 Loss: 5.624835483480365e-07\n",
      "Iteration: 6175 lambda_n: 0.9574185963145371 Loss: 5.600721178638313e-07\n",
      "Iteration: 6176 lambda_n: 0.9892345910066112 Loss: 5.575818813945381e-07\n",
      "Iteration: 6177 lambda_n: 1.0286311795833865 Loss: 5.550203382684984e-07\n",
      "Iteration: 6178 lambda_n: 0.9527196961189122 Loss: 5.523690239838738e-07\n",
      "Iteration: 6179 lambda_n: 0.9341750434246838 Loss: 5.499251097835093e-07\n",
      "Iteration: 6180 lambda_n: 1.016091420549559 Loss: 5.475393745646831e-07\n",
      "Iteration: 6181 lambda_n: 0.9710620083545428 Loss: 5.449557017361288e-07\n",
      "Iteration: 6182 lambda_n: 0.9712567702482654 Loss: 5.424981854929947e-07\n",
      "Iteration: 6183 lambda_n: 0.9509592912424549 Loss: 5.400512671281853e-07\n",
      "Iteration: 6184 lambda_n: 0.9830339225457725 Loss: 5.376662970425472e-07\n",
      "Iteration: 6185 lambda_n: 0.9589775361836493 Loss: 5.35211778892216e-07\n",
      "Iteration: 6186 lambda_n: 0.9881524579465399 Loss: 5.328282638784975e-07\n",
      "Iteration: 6187 lambda_n: 0.8971388954175828 Loss: 5.303831792821095e-07\n",
      "Iteration: 6188 lambda_n: 0.9933442770782676 Loss: 5.281734912565124e-07\n",
      "Iteration: 6189 lambda_n: 0.9376825900513286 Loss: 5.257370447975925e-07\n",
      "Iteration: 6190 lambda_n: 0.9293711190884169 Loss: 5.234477393546075e-07\n",
      "Iteration: 6191 lambda_n: 0.9449097763127962 Loss: 5.211886121036768e-07\n",
      "Iteration: 6192 lambda_n: 0.9980649721598392 Loss: 5.189016322133191e-07\n",
      "Iteration: 6193 lambda_n: 1.0447978819675194 Loss: 5.164966060844741e-07\n",
      "Iteration: 6194 lambda_n: 1.0454939985909553 Loss: 5.139906439918868e-07\n",
      "Iteration: 6195 lambda_n: 1.0123493059051825 Loss: 5.114951861952824e-07\n",
      "Iteration: 6196 lambda_n: 0.9057470922484585 Loss: 5.090905790198145e-07\n",
      "Iteration: 6197 lambda_n: 0.9497354528048024 Loss: 5.069493014185674e-07\n",
      "Iteration: 6198 lambda_n: 1.023853881992474 Loss: 5.047134804709212e-07\n",
      "Iteration: 6199 lambda_n: 0.9986795134505807 Loss: 5.023138103473595e-07\n",
      "Iteration: 6200 lambda_n: 0.9607070592085613 Loss: 4.99984278560223e-07\n",
      "Iteration: 6201 lambda_n: 0.9486305682006789 Loss: 4.977537208595563e-07\n",
      "Iteration: 6202 lambda_n: 0.9971569489990622 Loss: 4.955610343164629e-07\n",
      "Iteration: 6203 lambda_n: 0.9303455122631316 Loss: 4.932663424204918e-07\n",
      "Iteration: 6204 lambda_n: 0.9411250550912883 Loss: 4.911353191505435e-07\n",
      "Iteration: 6205 lambda_n: 0.8937486425837539 Loss: 4.889889236510007e-07\n",
      "Iteration: 6206 lambda_n: 0.9480177656526665 Loss: 4.869594918969514e-07\n",
      "Iteration: 6207 lambda_n: 1.0374887446046088 Loss: 4.848157712938908e-07\n",
      "Iteration: 6208 lambda_n: 0.9746536547542632 Loss: 4.824800675358021e-07\n",
      "Iteration: 6209 lambda_n: 0.9532252896744741 Loss: 4.802964027997118e-07\n",
      "Iteration: 6210 lambda_n: 1.0043315908024242 Loss: 4.781704193823185e-07\n",
      "Iteration: 6211 lambda_n: 0.9014792407815135 Loss: 4.759403748086717e-07\n",
      "Iteration: 6212 lambda_n: 0.9835802550172766 Loss: 4.7394804763373543e-07\n",
      "Iteration: 6213 lambda_n: 0.926067876893219 Loss: 4.717833776150898e-07\n",
      "Iteration: 6214 lambda_n: 1.0110818288824333 Loss: 4.697545960250016e-07\n",
      "Iteration: 6215 lambda_n: 1.0334849684220264 Loss: 4.675491018337137e-07\n",
      "Iteration: 6216 lambda_n: 1.0175471969969125 Loss: 4.653053305699055e-07\n",
      "Iteration: 6217 lambda_n: 0.9387325784128913 Loss: 4.6310677037981233e-07\n",
      "Iteration: 6218 lambda_n: 0.9412670277928926 Loss: 4.610880907855191e-07\n",
      "Iteration: 6219 lambda_n: 0.9526789133264703 Loss: 4.59072790264884e-07\n",
      "Iteration: 6220 lambda_n: 0.9639076285415734 Loss: 4.570419776331399e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6221 lambda_n: 0.9976810119696388 Loss: 4.549963248566531e-07\n",
      "Iteration: 6222 lambda_n: 0.9580657339232044 Loss: 4.5288848000244196e-07\n",
      "Iteration: 6223 lambda_n: 0.9518743233708896 Loss: 4.50873715861663e-07\n",
      "Iteration: 6224 lambda_n: 0.958480486422396 Loss: 4.488808833776577e-07\n",
      "Iteration: 6225 lambda_n: 1.0340550860059794 Loss: 4.4688309595502133e-07\n",
      "Iteration: 6226 lambda_n: 1.0260551231576738 Loss: 4.4473738557718274e-07\n",
      "Iteration: 6227 lambda_n: 0.9389071819706145 Loss: 4.426185057466138e-07\n",
      "Iteration: 6228 lambda_n: 0.992818973219376 Loss: 4.4068883715394827e-07\n",
      "Iteration: 6229 lambda_n: 1.040589029595555 Loss: 4.3865726976634716e-07\n",
      "Iteration: 6230 lambda_n: 0.9015177927049514 Loss: 4.365377756745735e-07\n",
      "Iteration: 6231 lambda_n: 1.0491080872489265 Loss: 4.347104235875375e-07\n",
      "Iteration: 6232 lambda_n: 0.9337360034489196 Loss: 4.325928182272885e-07\n",
      "Iteration: 6233 lambda_n: 0.9919330768505529 Loss: 4.3071727716260545e-07\n",
      "Iteration: 6234 lambda_n: 1.0089973358932203 Loss: 4.287334839127135e-07\n",
      "Iteration: 6235 lambda_n: 0.9999783560791323 Loss: 4.267248645169837e-07\n",
      "Iteration: 6236 lambda_n: 1.0443684613264594 Loss: 4.247435326010443e-07\n",
      "Iteration: 6237 lambda_n: 1.0351278985295946 Loss: 4.2268386250967577e-07\n",
      "Iteration: 6238 lambda_n: 0.9340376455525192 Loss: 4.2065232334675634e-07\n",
      "Iteration: 6239 lambda_n: 0.9064780992113979 Loss: 4.1882800096961005e-07\n",
      "Iteration: 6240 lambda_n: 0.9928629349829519 Loss: 4.1706519109986676e-07\n",
      "Iteration: 6241 lambda_n: 0.9700397834302998 Loss: 4.151425232974522e-07\n",
      "Iteration: 6242 lambda_n: 0.9378453445975855 Loss: 4.132727187914932e-07\n",
      "Iteration: 6243 lambda_n: 0.9729750318576342 Loss: 4.114731193459673e-07\n",
      "Iteration: 6244 lambda_n: 0.9871690142851064 Loss: 4.096142471073893e-07\n",
      "Iteration: 6245 lambda_n: 0.9751234147397404 Loss: 4.077367841753585e-07\n",
      "Iteration: 6246 lambda_n: 1.007259303147768 Loss: 4.058907375020761e-07\n",
      "Iteration: 6247 lambda_n: 0.9847898488404593 Loss: 4.0399249353308595e-07\n",
      "Iteration: 6248 lambda_n: 0.892332251021539 Loss: 4.02145281263643e-07\n",
      "Iteration: 6249 lambda_n: 0.9288520480515542 Loss: 4.00479155059096e-07\n",
      "Iteration: 6250 lambda_n: 0.9848011088688722 Loss: 3.987520319237827e-07\n",
      "Iteration: 6251 lambda_n: 0.9109164421306396 Loss: 3.969287798473429e-07\n",
      "Iteration: 6252 lambda_n: 1.0223071396723045 Loss: 3.9525003474056817e-07\n",
      "Iteration: 6253 lambda_n: 0.8948963930789229 Loss: 3.933739805237409e-07\n",
      "Iteration: 6254 lambda_n: 0.9130328301085968 Loss: 3.9173954144872715e-07\n",
      "Iteration: 6255 lambda_n: 0.9638085172247483 Loss: 3.9007891244322315e-07\n",
      "Iteration: 6256 lambda_n: 1.0269796905370634 Loss: 3.8833336976339525e-07\n",
      "Iteration: 6257 lambda_n: 0.9345325211743957 Loss: 3.864817486224702e-07\n",
      "Iteration: 6258 lambda_n: 1.040573906502134 Loss: 3.848048485055946e-07\n",
      "Iteration: 6259 lambda_n: 0.9978176695855253 Loss: 3.8294577910723195e-07\n",
      "Iteration: 6260 lambda_n: 1.0028508373446516 Loss: 3.811717171433705e-07\n",
      "Iteration: 6261 lambda_n: 0.9045535915875232 Loss: 3.7939697384215436e-07\n",
      "Iteration: 6262 lambda_n: 0.941259819526296 Loss: 3.7780364680217916e-07\n",
      "Iteration: 6263 lambda_n: 1.0045700791037706 Loss: 3.7615263263856e-07\n",
      "Iteration: 6264 lambda_n: 0.9106387127900225 Loss: 3.7439827645423064e-07\n",
      "Iteration: 6265 lambda_n: 1.0455512092433017 Loss: 3.7281538342755037e-07\n",
      "Iteration: 6266 lambda_n: 0.9793426860839739 Loss: 3.710056730883529e-07\n",
      "Iteration: 6267 lambda_n: 1.0012493397416515 Loss: 3.6931879671023025e-07\n",
      "Iteration: 6268 lambda_n: 1.0119053984578663 Loss: 3.676020356102027e-07\n",
      "Iteration: 6269 lambda_n: 1.042307498129735 Loss: 3.658750760468067e-07\n",
      "Iteration: 6270 lambda_n: 0.9201107830065679 Loss: 3.64104595572514e-07\n",
      "Iteration: 6271 lambda_n: 0.901033300606397 Loss: 3.625492504001372e-07\n",
      "Iteration: 6272 lambda_n: 0.9898364011559327 Loss: 3.610326658735164e-07\n",
      "Iteration: 6273 lambda_n: 0.9333704159473508 Loss: 3.593735872811528e-07\n",
      "Iteration: 6274 lambda_n: 1.0079316732635515 Loss: 3.5781634804227234e-07\n",
      "Iteration: 6275 lambda_n: 1.022626934270959 Loss: 3.5614200432116713e-07\n",
      "Iteration: 6276 lambda_n: 1.0233596596650814 Loss: 3.5445120597354386e-07\n",
      "Iteration: 6277 lambda_n: 0.9010295459527184 Loss: 3.527672367690783e-07\n",
      "Iteration: 6278 lambda_n: 0.9310239535950868 Loss: 3.512916162680382e-07\n",
      "Iteration: 6279 lambda_n: 0.9442839128751045 Loss: 3.497732579789382e-07\n",
      "Iteration: 6280 lambda_n: 1.0324855728748963 Loss: 3.4823993739737744e-07\n",
      "Iteration: 6281 lambda_n: 0.8954628730949972 Loss: 3.465707525181465e-07\n",
      "Iteration: 6282 lambda_n: 1.0073584297865874 Loss: 3.4513003340374946e-07\n",
      "Iteration: 6283 lambda_n: 0.9953119029623454 Loss: 3.43516028778986e-07\n",
      "Iteration: 6284 lambda_n: 0.9623895682575064 Loss: 3.419287903875858e-07\n",
      "Iteration: 6285 lambda_n: 1.0190155609981615 Loss: 3.4040115221444095e-07\n",
      "Iteration: 6286 lambda_n: 0.9740356697393787 Loss: 3.3879086340695294e-07\n",
      "Iteration: 6287 lambda_n: 0.8947166801286222 Loss: 3.3725894235036953e-07\n",
      "Iteration: 6288 lambda_n: 0.9856640782657371 Loss: 3.3585814010327245e-07\n",
      "Iteration: 6289 lambda_n: 0.9639287862105933 Loss: 3.343213634878354e-07\n",
      "Iteration: 6290 lambda_n: 0.9704964433533567 Loss: 3.3282535885210916e-07\n",
      "Iteration: 6291 lambda_n: 0.9281603484901244 Loss: 3.3132590821076395e-07\n",
      "Iteration: 6292 lambda_n: 0.94576300513699 Loss: 3.298983357415611e-07\n",
      "Iteration: 6293 lambda_n: 0.9812703617423153 Loss: 3.2844996343472995e-07\n",
      "Iteration: 6294 lambda_n: 0.9725157421612088 Loss: 3.2695381867235726e-07\n",
      "Iteration: 6295 lambda_n: 0.9028389008622458 Loss: 3.254777837022829e-07\n",
      "Iteration: 6296 lambda_n: 1.0396185952853743 Loss: 3.241136934774981e-07\n",
      "Iteration: 6297 lambda_n: 0.9457191217812124 Loss: 3.2254953449311997e-07\n",
      "Iteration: 6298 lambda_n: 0.9251561677367264 Loss: 3.2113352622724745e-07\n",
      "Iteration: 6299 lambda_n: 1.002633351777101 Loss: 3.197543943533279e-07\n",
      "Iteration: 6300 lambda_n: 0.9698663779071613 Loss: 3.182661930273053e-07\n",
      "Iteration: 6301 lambda_n: 0.9422103360711739 Loss: 3.168333349294379e-07\n",
      "Iteration: 6302 lambda_n: 0.9251431023779536 Loss: 3.1544760909357754e-07\n",
      "Iteration: 6303 lambda_n: 1.046652968941135 Loss: 3.1409294195306586e-07\n",
      "Iteration: 6304 lambda_n: 1.0208539247366941 Loss: 3.1256693966210216e-07\n",
      "Iteration: 6305 lambda_n: 1.0159777267810586 Loss: 3.110857913871369e-07\n",
      "Iteration: 6306 lambda_n: 1.0167774623437458 Loss: 3.0961871106724237e-07\n",
      "Iteration: 6307 lambda_n: 0.9202604358572198 Loss: 3.081574081083906e-07\n",
      "Iteration: 6308 lambda_n: 1.0002358013766175 Loss: 3.0684106788805615e-07\n",
      "Iteration: 6309 lambda_n: 1.0186511094060964 Loss: 3.0541644972108643e-07\n",
      "Iteration: 6310 lambda_n: 0.9545793286069141 Loss: 3.039723469318461e-07\n",
      "Iteration: 6311 lambda_n: 1.0003430933248718 Loss: 3.0262548245947167e-07\n",
      "Iteration: 6312 lambda_n: 0.993750772012698 Loss: 3.012203089117809e-07\n",
      "Iteration: 6313 lambda_n: 0.9887459394074553 Loss: 2.998308848949305e-07\n",
      "Iteration: 6314 lambda_n: 0.9501703122232696 Loss: 2.984548427518887e-07\n",
      "Iteration: 6315 lambda_n: 0.90493285552609 Loss: 2.9713856261961847e-07\n",
      "Iteration: 6316 lambda_n: 0.9718653295201675 Loss: 2.958904859046121e-07\n",
      "Iteration: 6317 lambda_n: 0.978188182675198 Loss: 2.945557334187167e-07\n",
      "Iteration: 6318 lambda_n: 0.9575094005704655 Loss: 2.9321836484303053e-07\n",
      "Iteration: 6319 lambda_n: 0.9923399833918949 Loss: 2.9191521911572874e-07\n",
      "Iteration: 6320 lambda_n: 0.9062824718557854 Loss: 2.9057067959443413e-07\n",
      "Iteration: 6321 lambda_n: 0.9201122929344393 Loss: 2.8934840379041304e-07\n",
      "Iteration: 6322 lambda_n: 0.9973229488446286 Loss: 2.881127026479256e-07\n",
      "Iteration: 6323 lambda_n: 1.0473723499537777 Loss: 2.8677903579192633e-07\n",
      "Iteration: 6324 lambda_n: 0.9861855067534506 Loss: 2.853849321687669e-07\n",
      "Iteration: 6325 lambda_n: 0.8980156760336852 Loss: 2.840786605495717e-07\n",
      "Iteration: 6326 lambda_n: 0.9288495595309976 Loss: 2.8289462755647906e-07\n",
      "Iteration: 6327 lambda_n: 0.9491269787277563 Loss: 2.816750512596778e-07\n",
      "Iteration: 6328 lambda_n: 0.9917010004435827 Loss: 2.8043423027341575e-07\n",
      "Iteration: 6329 lambda_n: 0.963276829650828 Loss: 2.791434697983217e-07\n",
      "Iteration: 6330 lambda_n: 0.9231605682657399 Loss: 2.7789548348360154e-07\n",
      "Iteration: 6331 lambda_n: 0.9738779069091672 Loss: 2.7670482453163447e-07\n",
      "Iteration: 6332 lambda_n: 1.0278305426021888 Loss: 2.754541411698925e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6333 lambda_n: 0.8968869058142492 Loss: 2.741401444612388e-07\n",
      "Iteration: 6334 lambda_n: 1.0483367953884934 Loss: 2.7299902533897764e-07\n",
      "Iteration: 6335 lambda_n: 0.9647399583674596 Loss: 2.716707746189132e-07\n",
      "Iteration: 6336 lambda_n: 0.9462266216464725 Loss: 2.70454396975311e-07\n",
      "Iteration: 6337 lambda_n: 1.0117473893468176 Loss: 2.6926671066119197e-07\n",
      "Iteration: 6338 lambda_n: 1.041054053788496 Loss: 2.680023685115456e-07\n",
      "Iteration: 6339 lambda_n: 0.9539930800807112 Loss: 2.667075202136478e-07\n",
      "Iteration: 6340 lambda_n: 0.9096198443937837 Loss: 2.655266979891412e-07\n",
      "Iteration: 6341 lambda_n: 0.9634864688252046 Loss: 2.64405791402311e-07\n",
      "Iteration: 6342 lambda_n: 0.9668944563138095 Loss: 2.6322352530081864e-07\n",
      "Iteration: 6343 lambda_n: 0.9269113636117877 Loss: 2.620423900616744e-07\n",
      "Iteration: 6344 lambda_n: 1.047323842478068 Loss: 2.6091518535285014e-07\n",
      "Iteration: 6345 lambda_n: 0.9863542235327094 Loss: 2.5964703536793466e-07\n",
      "Iteration: 6346 lambda_n: 0.9216047545372359 Loss: 2.584585236565518e-07\n",
      "Iteration: 6347 lambda_n: 0.9654868476988555 Loss: 2.573531227187193e-07\n",
      "Iteration: 6348 lambda_n: 0.8937858157105824 Loss: 2.562000484163758e-07\n",
      "Iteration: 6349 lambda_n: 0.9563572787909094 Loss: 2.5513739592988174e-07\n",
      "Iteration: 6350 lambda_n: 1.0203126534509988 Loss: 2.54005073405533e-07\n",
      "Iteration: 6351 lambda_n: 1.0382698319595969 Loss: 2.5280239757157296e-07\n",
      "Iteration: 6352 lambda_n: 0.9536613746091738 Loss: 2.515843585416486e-07\n",
      "Iteration: 6353 lambda_n: 1.0106495856703033 Loss: 2.504709759634983e-07\n",
      "Iteration: 6354 lambda_n: 0.9054134180877921 Loss: 2.4929629044240873e-07\n",
      "Iteration: 6355 lambda_n: 0.9136689352357757 Loss: 2.482488647736633e-07\n",
      "Iteration: 6356 lambda_n: 0.9252092213828538 Loss: 2.471963365676785e-07\n",
      "Iteration: 6357 lambda_n: 1.0349892350368615 Loss: 2.461350401582424e-07\n",
      "Iteration: 6358 lambda_n: 0.9988057301268743 Loss: 2.4495292169866627e-07\n",
      "Iteration: 6359 lambda_n: 0.99387244094274 Loss: 2.4381761798216176e-07\n",
      "Iteration: 6360 lambda_n: 0.9609053324949475 Loss: 2.4269316600474805e-07\n",
      "Iteration: 6361 lambda_n: 0.8921771055797963 Loss: 2.4161103433102444e-07\n",
      "Iteration: 6362 lambda_n: 1.025360884590858 Loss: 2.406107886756597e-07\n",
      "Iteration: 6363 lambda_n: 1.0007516994439885 Loss: 2.3946599376157334e-07\n",
      "Iteration: 6364 lambda_n: 0.9015126268244685 Loss: 2.383539992404666e-07\n",
      "Iteration: 6365 lambda_n: 0.9049550952410869 Loss: 2.3735693437699896e-07\n",
      "Iteration: 6366 lambda_n: 0.9523286620375963 Loss: 2.3636025589299566e-07\n",
      "Iteration: 6367 lambda_n: 0.9172070283721907 Loss: 2.3531581380402779e-07\n",
      "Iteration: 6368 lambda_n: 0.9440892905080215 Loss: 2.34314342936131e-07\n",
      "Iteration: 6369 lambda_n: 0.9930588525053262 Loss: 2.3328791459149806e-07\n",
      "Iteration: 6370 lambda_n: 0.9256637807788279 Loss: 2.3221298344490724e-07\n",
      "Iteration: 6371 lambda_n: 0.9084246116396395 Loss: 2.3121562841682213e-07\n",
      "Iteration: 6372 lambda_n: 1.0013538158778754 Loss: 2.3024105879046309e-07\n",
      "Iteration: 6373 lambda_n: 0.8997559236047593 Loss: 2.2917132943826558e-07\n",
      "Iteration: 6374 lambda_n: 1.0269299573962682 Loss: 2.2821460891973518e-07\n",
      "Iteration: 6375 lambda_n: 0.9364507863572413 Loss: 2.2712722948657797e-07\n",
      "Iteration: 6376 lambda_n: 1.0445657934214971 Loss: 2.2614038803838252e-07\n",
      "Iteration: 6377 lambda_n: 1.0424782210766197 Loss: 2.2504440522796814e-07\n",
      "Iteration: 6378 lambda_n: 1.0174727396360639 Loss: 2.2395592323163647e-07\n",
      "Iteration: 6379 lambda_n: 0.922107897051326 Loss: 2.2289869783128192e-07\n",
      "Iteration: 6380 lambda_n: 1.0275169233857668 Loss: 2.219450943433057e-07\n",
      "Iteration: 6381 lambda_n: 0.9832536702988963 Loss: 2.2088703591912792e-07\n",
      "Iteration: 6382 lambda_n: 0.8982268491180941 Loss: 2.1987939192297858e-07\n",
      "Iteration: 6383 lambda_n: 0.9512765324678434 Loss: 2.189630907592873e-07\n",
      "Iteration: 6384 lambda_n: 0.9119169990515342 Loss: 2.179967240150024e-07\n",
      "Iteration: 6385 lambda_n: 0.9649967313722175 Loss: 2.1707443724772603e-07\n",
      "Iteration: 6386 lambda_n: 0.9776359034105191 Loss: 2.1610260405460626e-07\n",
      "Iteration: 6387 lambda_n: 1.017119925795325 Loss: 2.1512245833414913e-07\n",
      "Iteration: 6388 lambda_n: 1.0318774273793303 Loss: 2.1410736113366573e-07\n",
      "Iteration: 6389 lambda_n: 0.9407537488908067 Loss: 2.1308240453637118e-07\n",
      "Iteration: 6390 lambda_n: 1.0003868440329131 Loss: 2.1215244230091088e-07\n",
      "Iteration: 6391 lambda_n: 0.9531403073134026 Loss: 2.1116785539860217e-07\n",
      "Iteration: 6392 lambda_n: 0.9271916420302082 Loss: 2.102341309045118e-07\n",
      "Iteration: 6393 lambda_n: 0.9971858696029013 Loss: 2.0932985063712568e-07\n",
      "Iteration: 6394 lambda_n: 0.9657127481597136 Loss: 2.0836149729490643e-07\n",
      "Iteration: 6395 lambda_n: 1.0121945310267317 Loss: 2.0742805386966056e-07\n",
      "Iteration: 6396 lambda_n: 0.9911705260304369 Loss: 2.0645407372785258e-07\n",
      "Iteration: 6397 lambda_n: 1.0244843312088088 Loss: 2.055048112093678e-07\n",
      "Iteration: 6398 lambda_n: 1.0321157789046953 Loss: 2.045281639801119e-07\n",
      "Iteration: 6399 lambda_n: 0.8998406958384325 Loss: 2.0354892724572436e-07\n",
      "Iteration: 6400 lambda_n: 0.9697474877249797 Loss: 2.0269928447926236e-07\n",
      "Iteration: 6401 lambda_n: 0.8959495639895214 Loss: 2.0178746475017602e-07\n",
      "Iteration: 6402 lambda_n: 0.9927244599595916 Loss: 2.0094883205503034e-07\n",
      "Iteration: 6403 lambda_n: 0.9500035947837773 Loss: 2.000234855223226e-07\n",
      "Iteration: 6404 lambda_n: 0.907438931586552 Loss: 1.9914204662614643e-07\n",
      "Iteration: 6405 lambda_n: 0.9581007152292204 Loss: 1.9830381840518073e-07\n",
      "Iteration: 6406 lambda_n: 1.0223114717358124 Loss: 1.9742252567921672e-07\n",
      "Iteration: 6407 lambda_n: 0.9862507802423846 Loss: 1.964863579214181e-07\n",
      "Iteration: 6408 lambda_n: 0.9675326862201034 Loss: 1.955875041694514e-07\n",
      "Iteration: 6409 lambda_n: 0.9058268238022276 Loss: 1.9470975247333077e-07\n",
      "Iteration: 6410 lambda_n: 0.9890270659276136 Loss: 1.9389167669237782e-07\n",
      "Iteration: 6411 lambda_n: 0.9844768515755139 Loss: 1.9300222184945414e-07\n",
      "Iteration: 6412 lambda_n: 0.937718938946137 Loss: 1.9212092964762997e-07\n",
      "Iteration: 6413 lambda_n: 1.0332784738692817 Loss: 1.9128533617480702e-07\n",
      "Iteration: 6414 lambda_n: 0.9446016707216813 Loss: 1.9036860415213834e-07\n",
      "Iteration: 6415 lambda_n: 0.9770609942929845 Loss: 1.8953457220532308e-07\n",
      "Iteration: 6416 lambda_n: 1.0449352793063527 Loss: 1.8867566868821282e-07\n",
      "Iteration: 6417 lambda_n: 1.022829777160942 Loss: 1.8776127129491436e-07\n",
      "Iteration: 6418 lambda_n: 0.9603668936194133 Loss: 1.8687056568501745e-07\n",
      "Iteration: 6419 lambda_n: 0.918064885444147 Loss: 1.8603823080727044e-07\n",
      "Iteration: 6420 lambda_n: 0.9205366250676453 Loss: 1.852461106611947e-07\n",
      "Iteration: 6421 lambda_n: 0.9808617188662853 Loss: 1.8445524764519142e-07\n",
      "Iteration: 6422 lambda_n: 0.9924477911046875 Loss: 1.836161636507107e-07\n",
      "Iteration: 6423 lambda_n: 1.002688039662129 Loss: 1.827710396065629e-07\n",
      "Iteration: 6424 lambda_n: 1.0268091409878903 Loss: 1.819211348524089e-07\n",
      "Iteration: 6425 lambda_n: 0.9631417580638126 Loss: 1.8105484148481205e-07\n",
      "Iteration: 6426 lambda_n: 0.9490677604925116 Loss: 1.8024614150552718e-07\n",
      "Iteration: 6427 lambda_n: 0.900893555780676 Loss: 1.7945282680929037e-07\n",
      "Iteration: 6428 lambda_n: 0.9610940484824253 Loss: 1.7870310290564975e-07\n",
      "Iteration: 6429 lambda_n: 1.006479365018922 Loss: 1.7790663003101664e-07\n",
      "Iteration: 6430 lambda_n: 0.9115665899633933 Loss: 1.7707627252159592e-07\n",
      "Iteration: 6431 lambda_n: 0.8949006311581446 Loss: 1.7632773805303148e-07\n",
      "Iteration: 6432 lambda_n: 0.9055397345069366 Loss: 1.7559600307431133e-07\n",
      "Iteration: 6433 lambda_n: 0.9813406930165652 Loss: 1.7485864935572548e-07\n",
      "Iteration: 6434 lambda_n: 1.0335231664336029 Loss: 1.7406293734383044e-07\n",
      "Iteration: 6435 lambda_n: 0.9142951531012622 Loss: 1.7322873704531483e-07\n",
      "Iteration: 6436 lambda_n: 0.9039176346056048 Loss: 1.724943165037484e-07\n",
      "Iteration: 6437 lambda_n: 0.9079469274903316 Loss: 1.717713182225039e-07\n",
      "Iteration: 6438 lambda_n: 0.8955288064574591 Loss: 1.7104814905287184e-07\n",
      "Iteration: 6439 lambda_n: 0.9203383659283143 Loss: 1.703378816816112e-07\n",
      "Iteration: 6440 lambda_n: 0.9331918168860012 Loss: 1.6961097636600864e-07\n",
      "Iteration: 6441 lambda_n: 1.0113897585409188 Loss: 1.6887707289907725e-07\n",
      "Iteration: 6442 lambda_n: 1.0176442028928259 Loss: 1.6808512218662267e-07\n",
      "Iteration: 6443 lambda_n: 0.9614266037758329 Loss: 1.6729202104883514e-07\n",
      "Iteration: 6444 lambda_n: 1.0432209687591585 Loss: 1.6654627819489444e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6445 lambda_n: 1.0027020394048345 Loss: 1.657407076885135e-07\n",
      "Iteration: 6446 lambda_n: 0.930221446568739 Loss: 1.6497018122065912e-07\n",
      "Iteration: 6447 lambda_n: 0.9360499062091816 Loss: 1.6425868491330749e-07\n",
      "Iteration: 6448 lambda_n: 1.0271589190456287 Loss: 1.635458271273816e-07\n",
      "Iteration: 6449 lambda_n: 0.9158223760353683 Loss: 1.6276698894975396e-07\n",
      "Iteration: 6450 lambda_n: 0.9879388280594217 Loss: 1.6207588742500592e-07\n",
      "Iteration: 6451 lambda_n: 0.9251757458688935 Loss: 1.613335397351368e-07\n",
      "Iteration: 6452 lambda_n: 1.0221133941812068 Loss: 1.6064154614750778e-07\n",
      "Iteration: 6453 lambda_n: 1.032152718025882 Loss: 1.598803359365984e-07\n",
      "Iteration: 6454 lambda_n: 0.9746204458918342 Loss: 1.5911530217548944e-07\n",
      "Iteration: 6455 lambda_n: 0.9553120891949883 Loss: 1.5839637825977006e-07\n",
      "Iteration: 6456 lambda_n: 1.0372843855482377 Loss: 1.5769489042437966e-07\n",
      "Iteration: 6457 lambda_n: 1.0467693759067243 Loss: 1.569365935853425e-07\n",
      "Iteration: 6458 lambda_n: 1.0069114824189107 Loss: 1.5617505365113105e-07\n",
      "Iteration: 6459 lambda_n: 0.9933845168432589 Loss: 1.554460763393372e-07\n",
      "Iteration: 6460 lambda_n: 0.9969894868747831 Loss: 1.5473025935845775e-07\n",
      "Iteration: 6461 lambda_n: 0.9997430050647009 Loss: 1.5401516310626775e-07\n",
      "Iteration: 6462 lambda_n: 0.9284812786308817 Loss: 1.533014161496813e-07\n",
      "Iteration: 6463 lambda_n: 0.9517198880361136 Loss: 1.5264162652154658e-07\n",
      "Iteration: 6464 lambda_n: 0.9173366935761659 Loss: 1.5196824315804077e-07\n",
      "Iteration: 6465 lambda_n: 0.93431346896783 Loss: 1.513220597432895e-07\n",
      "Iteration: 6466 lambda_n: 0.9880925875413346 Loss: 1.5066672509246759e-07\n",
      "Iteration: 6467 lambda_n: 0.9679100473976158 Loss: 1.499766804504636e-07\n",
      "Iteration: 6468 lambda_n: 0.9411535056486655 Loss: 1.493038362676506e-07\n",
      "Iteration: 6469 lambda_n: 1.0270019385888696 Loss: 1.486525365856139e-07\n",
      "Iteration: 6470 lambda_n: 1.0273153382152034 Loss: 1.4794493835665742e-07\n",
      "Iteration: 6471 lambda_n: 1.0436875371804457 Loss: 1.4724050452791936e-07\n",
      "Iteration: 6472 lambda_n: 0.9087535334504188 Loss: 1.465282631320359e-07\n",
      "Iteration: 6473 lambda_n: 1.0141069775440146 Loss: 1.4591111416168287e-07\n",
      "Iteration: 6474 lambda_n: 1.0202766946148296 Loss: 1.4522532855173728e-07\n",
      "Iteration: 6475 lambda_n: 1.0243161436543808 Loss: 1.4453862448064274e-07\n",
      "Iteration: 6476 lambda_n: 0.9049720031411133 Loss: 1.438524727151491e-07\n",
      "Iteration: 6477 lambda_n: 1.0019978627360602 Loss: 1.4324915270150073e-07\n",
      "Iteration: 6478 lambda_n: 1.0217329017857908 Loss: 1.42583959657919e-07\n",
      "Iteration: 6479 lambda_n: 0.9882416591649953 Loss: 1.419088258793455e-07\n",
      "Iteration: 6480 lambda_n: 1.0079339916891321 Loss: 1.41258924970376e-07\n",
      "Iteration: 6481 lambda_n: 0.9206381059740468 Loss: 1.4059912011391403e-07\n",
      "Iteration: 6482 lambda_n: 0.937634488909895 Loss: 1.3999928496778901e-07\n",
      "Iteration: 6483 lambda_n: 0.9309482887326215 Loss: 1.3939099160250555e-07\n",
      "Iteration: 6484 lambda_n: 0.9753792966139954 Loss: 1.3878966952862527e-07\n",
      "Iteration: 6485 lambda_n: 1.0167759077376175 Loss: 1.381623761642336e-07\n",
      "Iteration: 6486 lambda_n: 0.909284246706245 Loss: 1.3751142587509385e-07\n",
      "Iteration: 6487 lambda_n: 1.0412495185175026 Loss: 1.369320454776647e-07\n",
      "Iteration: 6488 lambda_n: 0.9952124269076703 Loss: 1.3627138500125926e-07\n",
      "Iteration: 6489 lambda_n: 0.9669824382689749 Loss: 1.3564299236761237e-07\n",
      "Iteration: 6490 lambda_n: 0.8995695080359224 Loss: 1.3503525063673928e-07\n",
      "Iteration: 6491 lambda_n: 1.0053742021736674 Loss: 1.3447242006643735e-07\n",
      "Iteration: 6492 lambda_n: 0.9820186705871555 Loss: 1.3384602296785594e-07\n",
      "Iteration: 6493 lambda_n: 1.0026118865426397 Loss: 1.3323703846709878e-07\n",
      "Iteration: 6494 lambda_n: 1.0314190137177424 Loss: 1.3261812322726278e-07\n",
      "Iteration: 6495 lambda_n: 0.9303668966673972 Loss: 1.3198439437913496e-07\n",
      "Iteration: 6496 lambda_n: 1.012016288227832 Loss: 1.314154965763849e-07\n",
      "Iteration: 6497 lambda_n: 1.0262142574537818 Loss: 1.3079935002352627e-07\n",
      "Iteration: 6498 lambda_n: 1.0208699049548915 Loss: 1.3017750030527144e-07\n",
      "Iteration: 6499 lambda_n: 1.0220619043093988 Loss: 1.295618418219237e-07\n",
      "Iteration: 6500 lambda_n: 0.9014513735871138 Loss: 1.2894839127613524e-07\n",
      "Iteration: 6501 lambda_n: 0.9023138057351148 Loss: 1.2840990424336518e-07\n",
      "Iteration: 6502 lambda_n: 0.9208810455916432 Loss: 1.2787316209508075e-07\n",
      "Iteration: 6503 lambda_n: 0.9976639053028472 Loss: 1.2732767432006058e-07\n",
      "Iteration: 6504 lambda_n: 0.9158591552885743 Loss: 1.2673923541684616e-07\n",
      "Iteration: 6505 lambda_n: 1.0336074204038899 Loss: 1.2620155303855638e-07\n",
      "Iteration: 6506 lambda_n: 0.9438243561653733 Loss: 1.255973283550399e-07\n",
      "Iteration: 6507 lambda_n: 1.0472948796411872 Loss: 1.250482415338993e-07\n",
      "Iteration: 6508 lambda_n: 1.0325126951948436 Loss: 1.2444163399821701e-07\n",
      "Iteration: 6509 lambda_n: 1.0251202867857188 Loss: 1.2384650199677178e-07\n",
      "Iteration: 6510 lambda_n: 0.9490994536108749 Loss: 1.2325846887022318e-07\n",
      "Iteration: 6511 lambda_n: 0.9525496805134817 Loss: 1.2271663916256975e-07\n",
      "Iteration: 6512 lambda_n: 0.9725377729499823 Loss: 1.221752406908807e-07\n",
      "Iteration: 6513 lambda_n: 0.9922045789011215 Loss: 1.2162493103598203e-07\n",
      "Iteration: 6514 lambda_n: 0.9593616259895168 Loss: 1.2106603303340332e-07\n",
      "Iteration: 6515 lambda_n: 0.9690687540609735 Loss: 1.2052812939871106e-07\n",
      "Iteration: 6516 lambda_n: 0.9599554196452843 Loss: 1.199872080628148e-07\n",
      "Iteration: 6517 lambda_n: 1.0145487950594174 Loss: 1.1945378931034552e-07\n",
      "Iteration: 6518 lambda_n: 0.9662506506276434 Loss: 1.1889255239306693e-07\n",
      "Iteration: 6519 lambda_n: 0.9210475952126806 Loss: 1.1836055626110247e-07\n",
      "Iteration: 6520 lambda_n: 0.9426266937436389 Loss: 1.1785572743319561e-07\n",
      "Iteration: 6521 lambda_n: 1.0385973114271347 Loss: 1.1734128495336953e-07\n",
      "Iteration: 6522 lambda_n: 0.934470430494888 Loss: 1.1677695203278438e-07\n",
      "Iteration: 6523 lambda_n: 0.927123279592904 Loss: 1.1627165088379114e-07\n",
      "Iteration: 6524 lambda_n: 0.9809705924172172 Loss: 1.1577250215343676e-07\n",
      "Iteration: 6525 lambda_n: 0.9395498562152421 Loss: 1.1524664106789358e-07\n",
      "Iteration: 6526 lambda_n: 0.9969458321182456 Loss: 1.1474528270340581e-07\n",
      "Iteration: 6527 lambda_n: 1.0184722601768375 Loss: 1.1421562257423843e-07\n",
      "Iteration: 6528 lambda_n: 1.0021255934221611 Loss: 1.1367703572515286e-07\n",
      "Iteration: 6529 lambda_n: 1.029487705127213 Loss: 1.1314960450072164e-07\n",
      "Iteration: 6530 lambda_n: 0.9977762041843198 Loss: 1.1261029870423664e-07\n",
      "Iteration: 6531 lambda_n: 1.0301213532012294 Loss: 1.1209010892640446e-07\n",
      "Iteration: 6532 lambda_n: 0.9824199039263787 Loss: 1.1155554939638224e-07\n",
      "Iteration: 6533 lambda_n: 1.0343967365104412 Loss: 1.1104818701000186e-07\n",
      "Iteration: 6534 lambda_n: 1.0066907981143027 Loss: 1.1051642370165697e-07\n",
      "Iteration: 6535 lambda_n: 0.9209527823084097 Loss: 1.1000139433821421e-07\n",
      "Iteration: 6536 lambda_n: 0.9095796043104939 Loss: 1.0953243603264573e-07\n",
      "Iteration: 6537 lambda_n: 1.0063373324554725 Loss: 1.0907125389309529e-07\n",
      "Iteration: 6538 lambda_n: 0.8966643599166787 Loss: 1.0856317268508221e-07\n",
      "Iteration: 6539 lambda_n: 0.9444339650845541 Loss: 1.0811258312088123e-07\n",
      "Iteration: 6540 lambda_n: 1.0257161365866412 Loss: 1.0763996883551185e-07\n",
      "Iteration: 6541 lambda_n: 0.9041877065359208 Loss: 1.0712893526772551e-07\n",
      "Iteration: 6542 lambda_n: 1.0293195458787732 Loss: 1.0668059980152524e-07\n",
      "Iteration: 6543 lambda_n: 0.9965082528208614 Loss: 1.0617236632494592e-07\n",
      "Iteration: 6544 lambda_n: 0.9678470510017193 Loss: 1.0568269046088118e-07\n",
      "Iteration: 6545 lambda_n: 0.9211918037218182 Loss: 1.0520930395713519e-07\n",
      "Iteration: 6546 lambda_n: 0.9205147827589605 Loss: 1.0476076647317036e-07\n",
      "Iteration: 6547 lambda_n: 0.9776076857727893 Loss: 1.043144801350114e-07\n",
      "Iteration: 6548 lambda_n: 1.0079372933424247 Loss: 1.0384254445104257e-07\n",
      "Iteration: 6549 lambda_n: 1.0447965471880318 Loss: 1.0335818113139798e-07\n",
      "Iteration: 6550 lambda_n: 0.9548927987561754 Loss: 1.028584604499729e-07\n",
      "Iteration: 6551 lambda_n: 0.9524976258222352 Loss: 1.0240396092136551e-07\n",
      "Iteration: 6552 lambda_n: 0.9183204437261568 Loss: 1.0195261625886094e-07\n",
      "Iteration: 6553 lambda_n: 1.0052870229823645 Loss: 1.0151939560837629e-07\n",
      "Iteration: 6554 lambda_n: 1.0386596679988849 Loss: 1.0104717535423953e-07\n",
      "Iteration: 6555 lambda_n: 1.0345444978553007 Loss: 1.0056156165423309e-07\n",
      "Iteration: 6556 lambda_n: 1.0268112912246776 Loss: 1.0008021029886496e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6557 lambda_n: 1.0120256082531787 Loss: 9.960475755487269e-08\n",
      "Iteration: 6558 lambda_n: 1.0363390746774195 Loss: 9.913839075654849e-08\n",
      "Iteration: 6559 lambda_n: 0.8934351648035722 Loss: 9.866306942271259e-08\n",
      "Iteration: 6560 lambda_n: 0.9217334852846943 Loss: 9.825526805172323e-08\n",
      "Iteration: 6561 lambda_n: 0.9677678570226481 Loss: 9.783629985674477e-08\n",
      "Iteration: 6562 lambda_n: 0.9619847743544206 Loss: 9.739829449683983e-08\n",
      "Iteration: 6563 lambda_n: 1.011990020819579 Loss: 9.696486788584593e-08\n",
      "Iteration: 6564 lambda_n: 0.9931725401050182 Loss: 9.651095308297455e-08\n",
      "Iteration: 6565 lambda_n: 0.9878373751472399 Loss: 9.606757717360745e-08\n",
      "Iteration: 6566 lambda_n: 0.9787092387545904 Loss: 9.562862189047619e-08\n",
      "Iteration: 6567 lambda_n: 0.9683845695505231 Loss: 9.519572270092455e-08\n",
      "Iteration: 6568 lambda_n: 0.9925207360803079 Loss: 9.476934183624052e-08\n",
      "Iteration: 6569 lambda_n: 0.9145739973302428 Loss: 9.433430397178881e-08\n",
      "Iteration: 6570 lambda_n: 0.9097252271692542 Loss: 9.393528356763574e-08\n",
      "Iteration: 6571 lambda_n: 0.945684643807592 Loss: 9.354006859473692e-08\n",
      "Iteration: 6572 lambda_n: 0.9332354306837523 Loss: 9.313097175464962e-08\n",
      "Iteration: 6573 lambda_n: 0.9200726301680636 Loss: 9.27290378217737e-08\n",
      "Iteration: 6574 lambda_n: 1.031509383981178 Loss: 9.233449467533811e-08\n",
      "Iteration: 6575 lambda_n: 0.8970564610323284 Loss: 9.18940605654548e-08\n",
      "Iteration: 6576 lambda_n: 0.9754981298908312 Loss: 9.151287447537264e-08\n",
      "Iteration: 6577 lambda_n: 1.0041339264449152 Loss: 9.110008766112736e-08\n",
      "Iteration: 6578 lambda_n: 1.0242053964835498 Loss: 9.067711346166294e-08\n",
      "Iteration: 6579 lambda_n: 0.918672433751195 Loss: 9.02477016544493e-08\n",
      "Iteration: 6580 lambda_n: 0.958507335873255 Loss: 8.986437260182796e-08\n",
      "Iteration: 6581 lambda_n: 1.0333693049678274 Loss: 8.946613280651303e-08\n",
      "Iteration: 6582 lambda_n: 0.9440650916535274 Loss: 8.903870584986389e-08\n",
      "Iteration: 6583 lambda_n: 0.9780907136489188 Loss: 8.865009613905078e-08\n",
      "Iteration: 6584 lambda_n: 0.9395363384671097 Loss: 8.824925031857499e-08\n",
      "Iteration: 6585 lambda_n: 0.9882523212929041 Loss: 8.786595873197328e-08\n",
      "Iteration: 6586 lambda_n: 0.9840993145672647 Loss: 8.746455710500839e-08\n",
      "Iteration: 6587 lambda_n: 0.9658302339265461 Loss: 8.706668185109238e-08\n",
      "Iteration: 6588 lambda_n: 0.9616965255777001 Loss: 8.667798240150028e-08\n",
      "Iteration: 6589 lambda_n: 1.0290722586883796 Loss: 8.629268741373645e-08\n",
      "Iteration: 6590 lambda_n: 0.9757900672395791 Loss: 8.588224564707593e-08\n",
      "Iteration: 6591 lambda_n: 1.0361935594120042 Loss: 8.54949204334327e-08\n",
      "Iteration: 6592 lambda_n: 0.9670193812147344 Loss: 8.508548828366284e-08\n",
      "Iteration: 6593 lambda_n: 1.0100782199300298 Loss: 8.470523285887379e-08\n",
      "Iteration: 6594 lambda_n: 0.9822660805917018 Loss: 8.430983464645979e-08\n",
      "Iteration: 6595 lambda_n: 1.0006934285969602 Loss: 8.392713247531144e-08\n",
      "Iteration: 6596 lambda_n: 1.0381293687721893 Loss: 8.353903458087134e-08\n",
      "Iteration: 6597 lambda_n: 0.9573177319262778 Loss: 8.313829466295274e-08\n",
      "Iteration: 6598 lambda_n: 0.92114848526214 Loss: 8.277053651168878e-08\n",
      "Iteration: 6599 lambda_n: 0.9387216499590582 Loss: 8.241825081353151e-08\n",
      "Iteration: 6600 lambda_n: 1.040502613407673 Loss: 8.206078486513331e-08\n",
      "Iteration: 6601 lambda_n: 1.0420562780005649 Loss: 8.16662934869377e-08\n",
      "Iteration: 6602 lambda_n: 0.9455652984793137 Loss: 8.127312801346063e-08\n",
      "Iteration: 6603 lambda_n: 0.987815603523994 Loss: 8.091809998543723e-08\n",
      "Iteration: 6604 lambda_n: 1.0333478749928975 Loss: 8.05488422243745e-08\n",
      "Iteration: 6605 lambda_n: 1.0373715013115756 Loss: 8.016434163598571e-08\n",
      "Iteration: 6606 lambda_n: 0.9341045211931172 Loss: 7.978020213097837e-08\n",
      "Iteration: 6607 lambda_n: 0.9981880184428213 Loss: 7.943597395738663e-08\n",
      "Iteration: 6608 lambda_n: 0.9153320128287986 Loss: 7.906973125652431e-08\n",
      "Iteration: 6609 lambda_n: 0.8959025287845184 Loss: 7.87354507493754e-08\n",
      "Iteration: 6610 lambda_n: 0.9167142081156248 Loss: 7.840966121459126e-08\n",
      "Iteration: 6611 lambda_n: 1.0007005102529534 Loss: 7.807769519001698e-08\n",
      "Iteration: 6612 lambda_n: 0.9585849708114137 Loss: 7.771686356269468e-08\n",
      "Iteration: 6613 lambda_n: 1.0260560561357344 Loss: 7.737282945092731e-08\n",
      "Iteration: 6614 lambda_n: 0.9849948699676976 Loss: 7.700622510820985e-08\n",
      "Iteration: 6615 lambda_n: 0.988442839507702 Loss: 7.665597422468086e-08\n",
      "Iteration: 6616 lambda_n: 1.0113202874019112 Loss: 7.63061105344722e-08\n",
      "Iteration: 6617 lambda_n: 0.9390796931557092 Loss: 7.594979809842057e-08\n",
      "Iteration: 6618 lambda_n: 0.9997385444982466 Loss: 7.562049686957465e-08\n",
      "Iteration: 6619 lambda_n: 1.0438430815472415 Loss: 7.527145910607233e-08\n",
      "Iteration: 6620 lambda_n: 1.0015515893870657 Loss: 7.490872120358825e-08\n",
      "Iteration: 6621 lambda_n: 0.9821488979823515 Loss: 7.45623726870003e-08\n",
      "Iteration: 6622 lambda_n: 0.9006255690966081 Loss: 7.422431915834557e-08\n",
      "Iteration: 6623 lambda_n: 0.9072672447184857 Loss: 7.391574454778371e-08\n",
      "Iteration: 6624 lambda_n: 0.9218577936555584 Loss: 7.360619918043092e-08\n",
      "Iteration: 6625 lambda_n: 1.002786455456743 Loss: 7.32930057803093e-08\n",
      "Iteration: 6626 lambda_n: 0.9326464513541693 Loss: 7.295378160037949e-08\n",
      "Iteration: 6627 lambda_n: 1.0023502015652663 Loss: 7.263975896341713e-08\n",
      "Iteration: 6628 lambda_n: 0.9885934415129796 Loss: 7.230373436957462e-08\n",
      "Iteration: 6629 lambda_n: 1.02411257356141 Loss: 7.197386994764443e-08\n",
      "Iteration: 6630 lambda_n: 1.0402620131811078 Loss: 7.163372863804503e-08\n",
      "Iteration: 6631 lambda_n: 0.9434328006866709 Loss: 7.128987304261805e-08\n",
      "Iteration: 6632 lambda_n: 0.9858986046834205 Loss: 7.097953609552097e-08\n",
      "Iteration: 6633 lambda_n: 0.9408591646227764 Loss: 7.06566566994345e-08\n",
      "Iteration: 6634 lambda_n: 0.9871995809944603 Loss: 7.034994372573704e-08\n",
      "Iteration: 6635 lambda_n: 0.8983626200497724 Loss: 7.002953583708724e-08\n",
      "Iteration: 6636 lambda_n: 0.9777650881551113 Loss: 6.97393028608759e-08\n",
      "Iteration: 6637 lambda_n: 0.9407703626034214 Loss: 6.942474068737252e-08\n",
      "Iteration: 6638 lambda_n: 0.9894719230163559 Loss: 6.912345996480555e-08\n",
      "Iteration: 6639 lambda_n: 0.916089658907634 Loss: 6.88079726781975e-08\n",
      "Iteration: 6640 lambda_n: 1.0206768146571081 Loss: 6.851723030922136e-08\n",
      "Iteration: 6641 lambda_n: 1.0189672396409848 Loss: 6.819467876639425e-08\n",
      "Iteration: 6642 lambda_n: 0.9356902307050992 Loss: 6.787420004425248e-08\n",
      "Iteration: 6643 lambda_n: 0.9797545724478149 Loss: 6.758131115749623e-08\n",
      "Iteration: 6644 lambda_n: 1.0188209118758418 Loss: 6.727596755794875e-08\n",
      "Iteration: 6645 lambda_n: 0.9161197371572645 Loss: 6.695989963884255e-08\n",
      "Iteration: 6646 lambda_n: 1.0055664394130852 Loss: 6.66770427050988e-08\n",
      "Iteration: 6647 lambda_n: 0.9288788950756093 Loss: 6.636789535827665e-08\n",
      "Iteration: 6648 lambda_n: 0.9991464450483702 Loss: 6.60836635882954e-08\n",
      "Iteration: 6649 lambda_n: 0.917128061252881 Loss: 6.577925502877615e-08\n",
      "Iteration: 6650 lambda_n: 0.9928836401104496 Loss: 6.550113680292624e-08\n",
      "Iteration: 6651 lambda_n: 1.0043666721049818 Loss: 6.520133394558446e-08\n",
      "Iteration: 6652 lambda_n: 1.0204592873983136 Loss: 6.48994682905358e-08\n",
      "Iteration: 6653 lambda_n: 1.013883208078789 Loss: 6.459420284267093e-08\n",
      "Iteration: 6654 lambda_n: 0.9930933501665071 Loss: 6.429234829080513e-08\n",
      "Iteration: 6655 lambda_n: 0.9637719539565014 Loss: 6.399808163276488e-08\n",
      "Iteration: 6656 lambda_n: 0.9429519120361117 Loss: 6.371382621685503e-08\n",
      "Iteration: 6657 lambda_n: 0.9515624055414292 Loss: 6.343696184927132e-08\n",
      "Iteration: 6658 lambda_n: 0.9027509761304134 Loss: 6.315879840573652e-08\n",
      "Iteration: 6659 lambda_n: 1.0358009298559927 Loss: 6.289607508053296e-08\n",
      "Iteration: 6660 lambda_n: 1.0432911143130643 Loss: 6.259590087653834e-08\n",
      "Iteration: 6661 lambda_n: 0.9402996429645232 Loss: 6.229501717379833e-08\n",
      "Iteration: 6662 lambda_n: 0.9698104546262719 Loss: 6.202515586521949e-08\n",
      "Iteration: 6663 lambda_n: 0.9572356604958407 Loss: 6.174804634055432e-08\n",
      "Iteration: 6664 lambda_n: 0.9440622267627633 Loss: 6.147576759165949e-08\n",
      "Iteration: 6665 lambda_n: 1.0111011565827843 Loss: 6.120843536252526e-08\n",
      "Iteration: 6666 lambda_n: 0.9552308407851029 Loss: 6.09233811065148e-08\n",
      "Iteration: 6667 lambda_n: 0.9962538201199236 Loss: 6.065534859719355e-08\n",
      "Iteration: 6668 lambda_n: 0.9440412531434647 Loss: 6.03770515456896e-08\n",
      "Iteration: 6669 lambda_n: 1.0140179705152839 Loss: 6.011456570212993e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6670 lambda_n: 0.9971605101934066 Loss: 5.983386563370994e-08\n",
      "Iteration: 6671 lambda_n: 0.9501156900617556 Loss: 5.955913837183049e-08\n",
      "Iteration: 6672 lambda_n: 1.0115663228944938 Loss: 5.92985905860739e-08\n",
      "Iteration: 6673 lambda_n: 1.0039060977363903 Loss: 5.902242172808997e-08\n",
      "Iteration: 6674 lambda_n: 0.9390906191795775 Loss: 5.874963830231243e-08\n",
      "Iteration: 6675 lambda_n: 0.979582008506956 Loss: 5.849566226905712e-08\n",
      "Iteration: 6676 lambda_n: 0.9776022926456412 Loss: 5.823189689192318e-08\n",
      "Iteration: 6677 lambda_n: 1.0224546418871339 Loss: 5.7969868315720976e-08\n",
      "Iteration: 6678 lambda_n: 0.9513106001025965 Loss: 5.769706875928475e-08\n",
      "Iteration: 6679 lambda_n: 0.944753162382274 Loss: 5.74444624271024e-08\n",
      "Iteration: 6680 lambda_n: 0.9442492463549591 Loss: 5.719471152321546e-08\n",
      "Iteration: 6681 lambda_n: 1.0386810240932978 Loss: 5.694619488788872e-08\n",
      "Iteration: 6682 lambda_n: 0.9455090866215627 Loss: 5.66740303372068e-08\n",
      "Iteration: 6683 lambda_n: 0.932818345288896 Loss: 5.642748083774662e-08\n",
      "Iteration: 6684 lambda_n: 0.9954905007721264 Loss: 5.6185314446813695e-08\n",
      "Iteration: 6685 lambda_n: 1.0166541577677521 Loss: 5.592800383944418e-08\n",
      "Iteration: 6686 lambda_n: 0.9144540753286372 Loss: 5.566644461209906e-08\n",
      "Iteration: 6687 lambda_n: 1.0295675120575585 Loss: 5.543229556554739e-08\n",
      "Iteration: 6688 lambda_n: 0.9806055939384211 Loss: 5.516979757388853e-08\n",
      "Iteration: 6689 lambda_n: 1.0441995243339068 Loss: 5.492098494273498e-08\n",
      "Iteration: 6690 lambda_n: 1.0106897035657703 Loss: 5.465725005887543e-08\n",
      "Iteration: 6691 lambda_n: 0.9526546223084332 Loss: 5.440322370837496e-08\n",
      "Iteration: 6692 lambda_n: 0.9879961170829532 Loss: 5.4164914076409745e-08\n",
      "Iteration: 6693 lambda_n: 0.9639736239138689 Loss: 5.391886358795145e-08\n",
      "Iteration: 6694 lambda_n: 1.009644733392423 Loss: 5.367990357407929e-08\n",
      "Iteration: 6695 lambda_n: 0.9694090114496178 Loss: 5.3430749356140316e-08\n",
      "Iteration: 6696 lambda_n: 0.9803668773129796 Loss: 5.319265253812007e-08\n",
      "Iteration: 6697 lambda_n: 0.9627869672859551 Loss: 5.295295494011065e-08\n",
      "Iteration: 6698 lambda_n: 0.9053144872654766 Loss: 5.271863375614222e-08\n",
      "Iteration: 6699 lambda_n: 1.0066684722281714 Loss: 5.249929109920119e-08\n",
      "Iteration: 6700 lambda_n: 1.006925935020225 Loss: 5.225642410589963e-08\n",
      "Iteration: 6701 lambda_n: 1.0398964561452189 Loss: 5.201463770985351e-08\n",
      "Iteration: 6702 lambda_n: 0.9098025987705172 Loss: 5.17661093575201e-08\n",
      "Iteration: 6703 lambda_n: 0.9367707899990938 Loss: 5.154972883382198e-08\n",
      "Iteration: 6704 lambda_n: 0.9709711288197701 Loss: 5.132788176615985e-08\n",
      "Iteration: 6705 lambda_n: 1.02977260001342 Loss: 5.109894216546285e-08\n",
      "Iteration: 6706 lambda_n: 0.8938468585337941 Loss: 5.085724016540187e-08\n",
      "Iteration: 6707 lambda_n: 0.9451561759791002 Loss: 5.0648451220401354e-08\n",
      "Iteration: 6708 lambda_n: 1.022218570120107 Loss: 5.042859974590781e-08\n",
      "Iteration: 6709 lambda_n: 0.9613033397618597 Loss: 5.019187365128839e-08\n",
      "Iteration: 6710 lambda_n: 0.9037755482062633 Loss: 4.997031791613166e-08\n",
      "Iteration: 6711 lambda_n: 0.9703736810516308 Loss: 4.976295676008835e-08\n",
      "Iteration: 6712 lambda_n: 1.0390130522994399 Loss: 4.954125630357764e-08\n",
      "Iteration: 6713 lambda_n: 0.9537501566512357 Loss: 4.930495103642359e-08\n",
      "Iteration: 6714 lambda_n: 0.9162479639612984 Loss: 4.908909075994564e-08\n",
      "Iteration: 6715 lambda_n: 1.019731027589097 Loss: 4.8882642928329337e-08\n",
      "Iteration: 6716 lambda_n: 0.9841427322611724 Loss: 4.865386313078216e-08\n",
      "Iteration: 6717 lambda_n: 0.9852836620891103 Loss: 4.843412038183796e-08\n",
      "Iteration: 6718 lambda_n: 1.0286620052162787 Loss: 4.8215135345384975e-08\n",
      "Iteration: 6719 lambda_n: 1.0240378690808112 Loss: 4.798756280382016e-08\n",
      "Iteration: 6720 lambda_n: 0.9128663220769982 Loss: 4.77621031294664e-08\n",
      "Iteration: 6721 lambda_n: 0.9082160404791668 Loss: 4.75620820314141e-08\n",
      "Iteration: 6722 lambda_n: 0.9172263855593842 Loss: 4.736392953316543e-08\n",
      "Iteration: 6723 lambda_n: 0.9847217142942963 Loss: 4.7164661311954115e-08\n",
      "Iteration: 6724 lambda_n: 0.9931199242426062 Loss: 4.6951647774552814e-08\n",
      "Iteration: 6725 lambda_n: 0.9692433514657295 Loss: 4.673780715233206e-08\n",
      "Iteration: 6726 lambda_n: 0.9328754698058651 Loss: 4.653007720023523e-08\n",
      "Iteration: 6727 lambda_n: 1.0441800703299269 Loss: 4.633104814895966e-08\n",
      "Iteration: 6728 lambda_n: 0.9528633837722982 Loss: 4.610924495419162e-08\n",
      "Iteration: 6729 lambda_n: 0.9897735178326014 Loss: 4.590782765398048e-08\n",
      "Iteration: 6730 lambda_n: 1.0216706638330169 Loss: 4.569954116544674e-08\n",
      "Iteration: 6731 lambda_n: 1.0325662274173537 Loss: 4.5485538157683554e-08\n",
      "Iteration: 6732 lambda_n: 0.963921247567318 Loss: 4.527028699572952e-08\n",
      "Iteration: 6733 lambda_n: 0.9949035548335097 Loss: 4.5070316466085895e-08\n",
      "Iteration: 6734 lambda_n: 1.014157747637787 Loss: 4.4864849696237835e-08\n",
      "Iteration: 6735 lambda_n: 1.024073818601512 Loss: 4.4656381876937676e-08\n",
      "Iteration: 6736 lambda_n: 0.9486071841387521 Loss: 4.444687497185133e-08\n",
      "Iteration: 6737 lambda_n: 0.9199870842300676 Loss: 4.425373716366509e-08\n",
      "Iteration: 6738 lambda_n: 0.8929873954377862 Loss: 4.40672580922018e-08\n",
      "Iteration: 6739 lambda_n: 0.9903924429275119 Loss: 4.388703123692636e-08\n",
      "Iteration: 6740 lambda_n: 0.9697990012635554 Loss: 4.3687981596536914e-08\n",
      "Iteration: 6741 lambda_n: 1.0146982300979246 Loss: 4.3493974504804456e-08\n",
      "Iteration: 6742 lambda_n: 1.0158315751047713 Loss: 4.329190722638734e-08\n",
      "Iteration: 6743 lambda_n: 1.0291624574834886 Loss: 4.309057535626452e-08\n",
      "Iteration: 6744 lambda_n: 0.9674683516646402 Loss: 4.288757163773539e-08\n",
      "Iteration: 6745 lambda_n: 0.9272772543562523 Loss: 4.2697656633171236e-08\n",
      "Iteration: 6746 lambda_n: 0.9770650510009591 Loss: 4.251645573491579e-08\n",
      "Iteration: 6747 lambda_n: 0.9129592105656666 Loss: 4.2326355060582346e-08\n",
      "Iteration: 6748 lambda_n: 1.0387390896859185 Loss: 4.214953963215722e-08\n",
      "Iteration: 6749 lambda_n: 0.9370787086080401 Loss: 4.1949224766133584e-08\n",
      "Iteration: 6750 lambda_n: 1.0348361755462068 Loss: 4.1769393391353386e-08\n",
      "Iteration: 6751 lambda_n: 0.9717095043665618 Loss: 4.157167382775263e-08\n",
      "Iteration: 6752 lambda_n: 0.9758067337548468 Loss: 4.1386915261247203e-08\n",
      "Iteration: 6753 lambda_n: 0.9009453479535222 Loss: 4.120222228702866e-08\n",
      "Iteration: 6754 lambda_n: 0.8955075515646043 Loss: 4.1032477806641866e-08\n",
      "Iteration: 6755 lambda_n: 0.900322302509636 Loss: 4.086447003605146e-08\n",
      "Iteration: 6756 lambda_n: 0.9946578373895784 Loss: 4.0696267716997314e-08\n",
      "Iteration: 6757 lambda_n: 1.0443319503845774 Loss: 4.051122550467107e-08\n",
      "Iteration: 6758 lambda_n: 0.9322158094806934 Loss: 4.031784786741509e-08\n",
      "Iteration: 6759 lambda_n: 0.950942694491326 Loss: 4.0146075059222494e-08\n",
      "Iteration: 6760 lambda_n: 1.0008332165601255 Loss: 3.997161717283153e-08\n",
      "Iteration: 6761 lambda_n: 1.043673074102479 Loss: 3.978882502159964e-08\n",
      "Iteration: 6762 lambda_n: 0.9420174649825139 Loss: 3.959910296255012e-08\n",
      "Iteration: 6763 lambda_n: 0.9086215535498652 Loss: 3.942869755609131e-08\n",
      "Iteration: 6764 lambda_n: 0.978469464713329 Loss: 3.926505897970636e-08\n",
      "Iteration: 6765 lambda_n: 0.9094692275410232 Loss: 3.90895919854188e-08\n",
      "Iteration: 6766 lambda_n: 0.9412474707196778 Loss: 3.8927246604928154e-08\n",
      "Iteration: 6767 lambda_n: 1.0386064909833024 Loss: 3.875994518019162e-08\n",
      "Iteration: 6768 lambda_n: 0.9144974349134534 Loss: 3.857615384403176e-08\n",
      "Iteration: 6769 lambda_n: 1.0262424666923748 Loss: 3.8415112502456576e-08\n",
      "Iteration: 6770 lambda_n: 1.0286609486704583 Loss: 3.823516850047933e-08\n",
      "Iteration: 6771 lambda_n: 0.9685081500852899 Loss: 3.805566850082489e-08\n",
      "Iteration: 6772 lambda_n: 1.0258330285411739 Loss: 3.788748020704132e-08\n",
      "Iteration: 6773 lambda_n: 0.9441548506875996 Loss: 3.7710146489964665e-08\n",
      "Iteration: 6774 lambda_n: 0.91842515379145 Loss: 3.754771736283802e-08\n",
      "Iteration: 6775 lambda_n: 1.0383843029064308 Loss: 3.739041438205403e-08\n",
      "Iteration: 6776 lambda_n: 0.9270988458205203 Loss: 3.721333215916451e-08\n",
      "Iteration: 6777 lambda_n: 0.9392806580513409 Loss: 3.705599796101866e-08\n",
      "Iteration: 6778 lambda_n: 1.0051451038795867 Loss: 3.6897289818899005e-08\n",
      "Iteration: 6779 lambda_n: 0.948286055900007 Loss: 3.672820147030526e-08\n",
      "Iteration: 6780 lambda_n: 1.043048418306786 Loss: 3.656943027762916e-08\n",
      "Iteration: 6781 lambda_n: 1.0226491013619077 Loss: 3.639557056637176e-08\n",
      "Iteration: 6782 lambda_n: 1.0042889199386087 Loss: 3.622594540790318e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6783 lambda_n: 0.9393386240497 Loss: 3.6060165055721907e-08\n",
      "Iteration: 6784 lambda_n: 1.0315665972194275 Loss: 3.590583685572364e-08\n",
      "Iteration: 6785 lambda_n: 1.0118764073062494 Loss: 3.573710372960681e-08\n",
      "Iteration: 6786 lambda_n: 1.045219081112891 Loss: 3.557239272522996e-08\n",
      "Iteration: 6787 lambda_n: 0.907174529578238 Loss: 3.540306260253755e-08\n",
      "Iteration: 6788 lambda_n: 0.9364350550456999 Loss: 3.525681699117089e-08\n",
      "Iteration: 6789 lambda_n: 0.9414760962016565 Loss: 3.510649738828757e-08\n",
      "Iteration: 6790 lambda_n: 0.9686827123791426 Loss: 3.495603309417221e-08\n",
      "Iteration: 6791 lambda_n: 1.036619740941716 Loss: 3.4801905234957135e-08\n",
      "Iteration: 6792 lambda_n: 1.035287336913886 Loss: 3.4637718444458746e-08\n",
      "Iteration: 6793 lambda_n: 0.9379615166645563 Loss: 3.447454095328506e-08\n",
      "Iteration: 6794 lambda_n: 1.0096639784614456 Loss: 3.4327422005215495e-08\n",
      "Iteration: 6795 lambda_n: 1.044819894205054 Loss: 3.4169754531588304e-08\n",
      "Iteration: 6796 lambda_n: 0.898325583160416 Loss: 3.400737112394822e-08\n",
      "Iteration: 6797 lambda_n: 0.970235970608682 Loss: 3.3868440238278756e-08\n",
      "Iteration: 6798 lambda_n: 1.0007509883608174 Loss: 3.371902157127501e-08\n",
      "Iteration: 6799 lambda_n: 0.9753572489473513 Loss: 3.356560615637945e-08\n",
      "Iteration: 6800 lambda_n: 0.9739425777196749 Loss: 3.341678658473175e-08\n",
      "Iteration: 6801 lambda_n: 1.0300808119516012 Loss: 3.326886391815087e-08\n",
      "Iteration: 6802 lambda_n: 0.9796469335054497 Loss: 3.311313122719993e-08\n",
      "Iteration: 6803 lambda_n: 0.996833528150764 Loss: 3.2965740145121674e-08\n",
      "Iteration: 6804 lambda_n: 1.0137509567007386 Loss: 3.281645388729786e-08\n",
      "Iteration: 6805 lambda_n: 1.0404499213675364 Loss: 3.266534545343602e-08\n",
      "Iteration: 6806 lambda_n: 0.9162307367151747 Loss: 3.251099644460663e-08\n",
      "Iteration: 6807 lambda_n: 1.0011946833841576 Loss: 3.23757394754611e-08\n",
      "Iteration: 6808 lambda_n: 0.9227744903507249 Loss: 3.2228576832110034e-08\n",
      "Iteration: 6809 lambda_n: 0.9963790019660069 Loss: 3.20935790899821e-08\n",
      "Iteration: 6810 lambda_n: 0.9458578277071161 Loss: 3.194844608179557e-08\n",
      "Iteration: 6811 lambda_n: 0.9661708170533126 Loss: 3.1811317313426885e-08\n",
      "Iteration: 6812 lambda_n: 0.901066651204964 Loss: 3.167186671136817e-08\n",
      "Iteration: 6813 lambda_n: 0.9408067266206598 Loss: 3.1542403498905064e-08\n",
      "Iteration: 6814 lambda_n: 0.953232892106842 Loss: 3.140780351506735e-08\n",
      "Iteration: 6815 lambda_n: 1.0145288160770778 Loss: 3.1272029266164145e-08\n",
      "Iteration: 6816 lambda_n: 1.0042931273295734 Loss: 3.112817248187429e-08\n",
      "Iteration: 6817 lambda_n: 1.0242242360795788 Loss: 3.098644666034714e-08\n",
      "Iteration: 6818 lambda_n: 1.0489564268193192 Loss: 3.0842591145288364e-08\n",
      "Iteration: 6819 lambda_n: 1.0065220856995165 Loss: 3.069597197330507e-08\n",
      "Iteration: 6820 lambda_n: 0.8988318204294702 Loss: 3.055597830355537e-08\n",
      "Iteration: 6821 lambda_n: 0.9095943724853028 Loss: 3.043155459869253e-08\n",
      "Iteration: 6822 lambda_n: 0.9832817514736837 Loss: 3.0306173670079505e-08\n",
      "Iteration: 6823 lambda_n: 0.9864247981221657 Loss: 3.017121597298653e-08\n",
      "Iteration: 6824 lambda_n: 0.9655186122239852 Loss: 3.003645345628003e-08\n",
      "Iteration: 6825 lambda_n: 0.9176527715714834 Loss: 2.990515944251504e-08\n",
      "Iteration: 6826 lambda_n: 0.9934547856453326 Loss: 2.9780941306270532e-08\n",
      "Iteration: 6827 lambda_n: 0.9411714802430343 Loss: 2.964704347358294e-08\n",
      "Iteration: 6828 lambda_n: 0.995087283151046 Loss: 2.9520785464213403e-08\n",
      "Iteration: 6829 lambda_n: 0.9772951862653048 Loss: 2.938788641796979e-08\n",
      "Iteration: 6830 lambda_n: 0.9408058104168356 Loss: 2.9257975061181122e-08\n",
      "Iteration: 6831 lambda_n: 1.019022007035652 Loss: 2.913348960836795e-08\n",
      "Iteration: 6832 lambda_n: 1.0208276953471318 Loss: 2.8999252471062575e-08\n",
      "Iteration: 6833 lambda_n: 0.9128467951026572 Loss: 2.8865422835696738e-08\n",
      "Iteration: 6834 lambda_n: 1.039479298539633 Loss: 2.8746324383338116e-08\n",
      "Iteration: 6835 lambda_n: 0.95634322027314 Loss: 2.8611287957840457e-08\n",
      "Iteration: 6836 lambda_n: 1.0259904854577109 Loss: 2.848765956052779e-08\n",
      "Iteration: 6837 lambda_n: 0.9289038785777827 Loss: 2.8355625570925004e-08\n",
      "Iteration: 6838 lambda_n: 0.9205994033687642 Loss: 2.8236663040178556e-08\n",
      "Iteration: 6839 lambda_n: 1.0175642064129031 Loss: 2.8119280010536393e-08\n",
      "Iteration: 6840 lambda_n: 0.9653659036302549 Loss: 2.7990096524046316e-08\n",
      "Iteration: 6841 lambda_n: 0.9758884912388909 Loss: 2.7868127254534618e-08\n",
      "Iteration: 6842 lambda_n: 0.9495841295778077 Loss: 2.7745389497407207e-08\n",
      "Iteration: 6843 lambda_n: 0.9727120923966796 Loss: 2.7626509246552943e-08\n",
      "Iteration: 6844 lambda_n: 0.9211903190064162 Loss: 2.750527869746673e-08\n",
      "Iteration: 6845 lambda_n: 1.0492852813962512 Loss: 2.7390995596353568e-08\n",
      "Iteration: 6846 lambda_n: 0.9639276196171365 Loss: 2.7261386862427707e-08\n",
      "Iteration: 6847 lambda_n: 1.0026460772972325 Loss: 2.714291023351843e-08\n",
      "Iteration: 6848 lambda_n: 0.9761581034358748 Loss: 2.702023494921674e-08\n",
      "Iteration: 6849 lambda_n: 1.0435098361565314 Loss: 2.6901365030108942e-08\n",
      "Iteration: 6850 lambda_n: 0.9515533385891756 Loss: 2.6774878704578907e-08\n",
      "Iteration: 6851 lambda_n: 1.0332042234082226 Loss: 2.666010587071908e-08\n",
      "Iteration: 6852 lambda_n: 0.9301647967138605 Loss: 2.6536044248207624e-08\n",
      "Iteration: 6853 lambda_n: 1.0441108769665726 Loss: 2.64248989146598e-08\n",
      "Iteration: 6854 lambda_n: 0.8961456791287756 Loss: 2.6300686059576242e-08\n",
      "Iteration: 6855 lambda_n: 0.954410122826459 Loss: 2.6194600469258756e-08\n",
      "Iteration: 6856 lambda_n: 0.9686863318903236 Loss: 2.6082095420516943e-08\n",
      "Iteration: 6857 lambda_n: 1.0217226040508143 Loss: 2.5968421746821098e-08\n",
      "Iteration: 6858 lambda_n: 0.9202755926354529 Loss: 2.5849072589800898e-08\n",
      "Iteration: 6859 lambda_n: 0.9638297689615538 Loss: 2.5742091511586895e-08\n",
      "Iteration: 6860 lambda_n: 1.0415575811374727 Loss: 2.563053408381477e-08\n",
      "Iteration: 6861 lambda_n: 0.9239103383795851 Loss: 2.551052882814496e-08\n",
      "Iteration: 6862 lambda_n: 1.0343359050977472 Loss: 2.5404601387823035e-08\n",
      "Iteration: 6863 lambda_n: 0.9881349757223173 Loss: 2.5286531164231393e-08\n",
      "Iteration: 6864 lambda_n: 0.9866191835663227 Loss: 2.517428535832912e-08\n",
      "Iteration: 6865 lambda_n: 0.910888136975232 Loss: 2.5062734525521494e-08\n",
      "Iteration: 6866 lambda_n: 0.9317024769825838 Loss: 2.496022555932494e-08\n",
      "Iteration: 6867 lambda_n: 1.0401437595493053 Loss: 2.4855825231952996e-08\n",
      "Iteration: 6868 lambda_n: 0.8949218688325744 Loss: 2.4739786926077837e-08\n",
      "Iteration: 6869 lambda_n: 0.902746128332264 Loss: 2.4640439393407803e-08\n",
      "Iteration: 6870 lambda_n: 0.9327259317661015 Loss: 2.4540646842851272e-08\n",
      "Iteration: 6871 lambda_n: 0.9534643590638091 Loss: 2.443797995230478e-08\n",
      "Iteration: 6872 lambda_n: 0.9673211324329268 Loss: 2.4333492784815686e-08\n",
      "Iteration: 6873 lambda_n: 1.0016901464242325 Loss: 2.42279645736396e-08\n",
      "Iteration: 6874 lambda_n: 0.9157905511271183 Loss: 2.411918643693865e-08\n",
      "Iteration: 6875 lambda_n: 1.0242408013696067 Loss: 2.402020681546516e-08\n",
      "Iteration: 6876 lambda_n: 0.8971370870032388 Loss: 2.3909985238875473e-08\n",
      "Iteration: 6877 lambda_n: 0.9558156170717456 Loss: 2.3813908399539798e-08\n",
      "Iteration: 6878 lambda_n: 1.0482151096499657 Loss: 2.3711981697139824e-08\n",
      "Iteration: 6879 lambda_n: 1.0042146938483325 Loss: 2.3600706958963163e-08\n",
      "Iteration: 6880 lambda_n: 0.9745838748070782 Loss: 2.349463106371098e-08\n",
      "Iteration: 6881 lambda_n: 0.9765429050110271 Loss: 2.3392173580463623e-08\n",
      "Iteration: 6882 lambda_n: 0.9852387382267682 Loss: 2.3289983081066913e-08\n",
      "Iteration: 6883 lambda_n: 1.0120955123091189 Loss: 2.318735857391872e-08\n",
      "Iteration: 6884 lambda_n: 0.9597757493092621 Loss: 2.3082427753834552e-08\n",
      "Iteration: 6885 lambda_n: 0.9055353636182781 Loss: 2.2983397197899374e-08\n",
      "Iteration: 6886 lambda_n: 1.0203326639638197 Loss: 2.2890387011514403e-08\n",
      "Iteration: 6887 lambda_n: 0.9331338532732156 Loss: 2.278603488533869e-08\n",
      "Iteration: 6888 lambda_n: 1.0213493454876812 Loss: 2.2691060937715967e-08\n",
      "Iteration: 6889 lambda_n: 1.0086543181770997 Loss: 2.2587567570688135e-08\n",
      "Iteration: 6890 lambda_n: 0.9595686766873773 Loss: 2.248585425118688e-08\n",
      "Iteration: 6891 lambda_n: 0.9037015503004772 Loss: 2.2389552200024875e-08\n",
      "Iteration: 6892 lambda_n: 0.9810844712774791 Loss: 2.229926839628786e-08\n",
      "Iteration: 6893 lambda_n: 0.9073449363653244 Loss: 2.220167303937134e-08\n",
      "Iteration: 6894 lambda_n: 0.9210114169432337 Loss: 2.2111831714055192e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6895 lambda_n: 1.0494002686187534 Loss: 2.202102873485127e-08\n",
      "Iteration: 6896 lambda_n: 1.0381345447621713 Loss: 2.191801926516268e-08\n",
      "Iteration: 6897 lambda_n: 0.9015001150364932 Loss: 2.1816621606160872e-08\n",
      "Iteration: 6898 lambda_n: 1.0468792220275869 Loss: 2.1729001481811256e-08\n",
      "Iteration: 6899 lambda_n: 0.9171314547894317 Loss: 2.162768617810494e-08\n",
      "Iteration: 6900 lambda_n: 0.9946284774036843 Loss: 2.1539366944540187e-08\n",
      "Iteration: 6901 lambda_n: 0.9153371873795091 Loss: 2.1444000896668734e-08\n",
      "Iteration: 6902 lambda_n: 0.9458703613086272 Loss: 2.1356650280381963e-08\n",
      "Iteration: 6903 lambda_n: 1.022393515032386 Loss: 2.1266777141095515e-08\n",
      "Iteration: 6904 lambda_n: 0.9351904429586113 Loss: 2.1170068373746477e-08\n",
      "Iteration: 6905 lambda_n: 0.9294826115797445 Loss: 2.1082036054816986e-08\n",
      "Iteration: 6906 lambda_n: 0.9477814592740413 Loss: 2.099492844117369e-08\n",
      "Iteration: 6907 lambda_n: 1.0179216875014065 Loss: 2.0906496926965453e-08\n",
      "Iteration: 6908 lambda_n: 0.9589555661861118 Loss: 2.0811947645220883e-08\n",
      "Iteration: 6909 lambda_n: 1.028950357708554 Loss: 2.0723304555276082e-08\n",
      "Iteration: 6910 lambda_n: 1.0250847670066934 Loss: 2.0628623650070306e-08\n",
      "Iteration: 6911 lambda_n: 1.0191043649973364 Loss: 2.05347581319736e-08\n",
      "Iteration: 6912 lambda_n: 0.9681629057831204 Loss: 2.0441893317829947e-08\n",
      "Iteration: 6913 lambda_n: 1.0000631296600653 Loss: 2.035409620080839e-08\n",
      "Iteration: 6914 lambda_n: 1.035578627430744 Loss: 2.0263822332982632e-08\n",
      "Iteration: 6915 lambda_n: 0.9105598770837011 Loss: 2.017078562606063e-08\n",
      "Iteration: 6916 lambda_n: 1.0457873469252355 Loss: 2.0089381559545645e-08\n",
      "Iteration: 6917 lambda_n: 0.9948400578300131 Loss: 1.9996292137680934e-08\n",
      "Iteration: 6918 lambda_n: 0.9773778864842478 Loss: 1.9908176371559154e-08\n",
      "Iteration: 6919 lambda_n: 0.9603614367901897 Loss: 1.9822015351731198e-08\n",
      "Iteration: 6920 lambda_n: 0.9749864269348444 Loss: 1.9737746525027714e-08\n",
      "Iteration: 6921 lambda_n: 1.0307052092745157 Loss: 1.9652583879114598e-08\n",
      "Iteration: 6922 lambda_n: 1.0143740844321072 Loss: 1.9562970650541746e-08\n",
      "Iteration: 6923 lambda_n: 1.0185226449648062 Loss: 1.947520814832425e-08\n",
      "Iteration: 6924 lambda_n: 0.9300143157675985 Loss: 1.9387510501991764e-08\n",
      "Iteration: 6925 lambda_n: 0.9788125074819864 Loss: 1.9307820002428144e-08\n",
      "Iteration: 6926 lambda_n: 0.9640587119304866 Loss: 1.9224318136390373e-08\n",
      "Iteration: 6927 lambda_n: 0.9564913700044289 Loss: 1.9142456557184487e-08\n",
      "Iteration: 6928 lambda_n: 1.0028468854261863 Loss: 1.9061608813883548e-08\n",
      "Iteration: 6929 lambda_n: 0.9531952335360485 Loss: 1.8977227526468026e-08\n",
      "Iteration: 6930 lambda_n: 0.9696460026597773 Loss: 1.8897405282091433e-08\n",
      "Iteration: 6931 lambda_n: 1.0347510073394335 Loss: 1.8816572578966844e-08\n",
      "Iteration: 6932 lambda_n: 0.8961329342355199 Loss: 1.873070951346442e-08\n",
      "Iteration: 6933 lambda_n: 1.0379113925534693 Loss: 1.865671337675836e-08\n",
      "Iteration: 6934 lambda_n: 0.9583088856255955 Loss: 1.8571375127130882e-08\n",
      "Iteration: 6935 lambda_n: 0.960470650847041 Loss: 1.8492969540899437e-08\n",
      "Iteration: 6936 lambda_n: 1.0495243371485257 Loss: 1.8414744371742095e-08\n",
      "Iteration: 6937 lambda_n: 0.94787095622095 Loss: 1.8329656158265236e-08\n",
      "Iteration: 6938 lambda_n: 0.9113322880663425 Loss: 1.8253191597274013e-08\n",
      "Iteration: 6939 lambda_n: 0.9527095807686 Loss: 1.8180005130905957e-08\n",
      "Iteration: 6940 lambda_n: 0.9529851572458895 Loss: 1.8103826805900768e-08\n",
      "Iteration: 6941 lambda_n: 0.9060699993647706 Loss: 1.8027970978357307e-08\n",
      "Iteration: 6942 lambda_n: 1.0029409861595533 Loss: 1.7956175532325527e-08\n",
      "Iteration: 6943 lambda_n: 0.9838949717105505 Loss: 1.7877046348014375e-08\n",
      "Iteration: 6944 lambda_n: 0.9830187011843209 Loss: 1.7799789292872732e-08\n",
      "Iteration: 6945 lambda_n: 0.934808137571673 Loss: 1.7722961528374176e-08\n",
      "Iteration: 6946 lambda_n: 0.9134570270496286 Loss: 1.765024241566496e-08\n",
      "Iteration: 6947 lambda_n: 1.0282730260891926 Loss: 1.75794994786407e-08\n",
      "Iteration: 6948 lambda_n: 0.9591062260942431 Loss: 1.7500210408392957e-08\n",
      "Iteration: 6949 lambda_n: 1.046098836153024 Loss: 1.742661550464292e-08\n",
      "Iteration: 6950 lambda_n: 0.9472612942991526 Loss: 1.7346711310147487e-08\n",
      "Iteration: 6951 lambda_n: 1.0428142024932048 Loss: 1.7274715654630038e-08\n",
      "Iteration: 6952 lambda_n: 1.041809193459566 Loss: 1.7195814512184714e-08\n",
      "Iteration: 6953 lambda_n: 0.9289101819188746 Loss: 1.7117379759219608e-08\n",
      "Iteration: 6954 lambda_n: 0.9768618277706604 Loss: 1.7047790453559178e-08\n",
      "Iteration: 6955 lambda_n: 1.0288483998764621 Loss: 1.6974931893009495e-08\n",
      "Iteration: 6956 lambda_n: 0.9945189927444414 Loss: 1.6898552182464903e-08\n",
      "Iteration: 6957 lambda_n: 0.9937984577248428 Loss: 1.6825081691986514e-08\n",
      "Iteration: 6958 lambda_n: 1.0227450273467897 Loss: 1.675201125644952e-08\n",
      "Iteration: 6959 lambda_n: 1.0359080886082699 Loss: 1.6677167612573302e-08\n",
      "Iteration: 6960 lambda_n: 0.9409672073685819 Loss: 1.6601729079978222e-08\n",
      "Iteration: 6961 lambda_n: 0.8968690530372233 Loss: 1.6533541384534367e-08\n",
      "Iteration: 6962 lambda_n: 0.9571764311164433 Loss: 1.6468839684568565e-08\n",
      "Iteration: 6963 lambda_n: 0.9159142492064511 Loss: 1.640008177301683e-08\n",
      "Iteration: 6964 lambda_n: 0.9638732240045308 Loss: 1.6334586981748545e-08\n",
      "Iteration: 6965 lambda_n: 0.9025784516056018 Loss: 1.6265962912355337e-08\n",
      "Iteration: 6966 lambda_n: 0.9118976582558121 Loss: 1.620199692586708e-08\n",
      "Iteration: 6967 lambda_n: 1.008785267526648 Loss: 1.613764771577314e-08\n",
      "Iteration: 6968 lambda_n: 0.9802075692913343 Loss: 1.6066770376511496e-08\n",
      "Iteration: 6969 lambda_n: 0.8937668865840482 Loss: 1.599823098437722e-08\n",
      "Iteration: 6970 lambda_n: 1.0414848361291957 Loss: 1.5936026672270006e-08\n",
      "Iteration: 6971 lambda_n: 0.9830344481629398 Loss: 1.5863850002551472e-08\n",
      "Iteration: 6972 lambda_n: 0.9571267620176913 Loss: 1.5796061079496197e-08\n",
      "Iteration: 6973 lambda_n: 0.9262644969394547 Loss: 1.5730367052778147e-08\n",
      "Iteration: 6974 lambda_n: 0.9561389743457378 Loss: 1.5667080460052273e-08\n",
      "Iteration: 6975 lambda_n: 0.9598528521532412 Loss: 1.5602040472096353e-08\n",
      "Iteration: 6976 lambda_n: 0.925349125558169 Loss: 1.553704465572158e-08\n",
      "Iteration: 6977 lambda_n: 0.9495712276830697 Loss: 1.5474671058675718e-08\n",
      "Iteration: 6978 lambda_n: 0.9516212727253713 Loss: 1.5410946434369025e-08\n",
      "Iteration: 6979 lambda_n: 1.0175961109164893 Loss: 1.5347372573465976e-08\n",
      "Iteration: 6980 lambda_n: 0.9810552667637191 Loss: 1.5279699058339092e-08\n",
      "Iteration: 6981 lambda_n: 0.9205551024650143 Loss: 1.5214771186044878e-08\n",
      "Iteration: 6982 lambda_n: 0.9787126297280762 Loss: 1.5154131339720503e-08\n",
      "Iteration: 6983 lambda_n: 0.9100310300737561 Loss: 1.508994289830551e-08\n",
      "Iteration: 6984 lambda_n: 0.9562725693039552 Loss: 1.5030536473993376e-08\n",
      "Iteration: 6985 lambda_n: 0.9777622396876449 Loss: 1.4968381748063968e-08\n",
      "Iteration: 6986 lambda_n: 0.9263536279726149 Loss: 1.4905119358415e-08\n",
      "Iteration: 6987 lambda_n: 0.9114191083282132 Loss: 1.4845461717526653e-08\n",
      "Iteration: 6988 lambda_n: 1.0420358741803417 Loss: 1.478702442921199e-08\n",
      "Iteration: 6989 lambda_n: 0.9538670372586772 Loss: 1.4720502530485718e-08\n",
      "Iteration: 6990 lambda_n: 0.8931888243651742 Loss: 1.4659910701099386e-08\n",
      "Iteration: 6991 lambda_n: 1.0112424074245305 Loss: 1.4603430543334391e-08\n",
      "Iteration: 6992 lambda_n: 1.01906101268992 Loss: 1.4539757453634886e-08\n",
      "Iteration: 6993 lambda_n: 0.9670800000974771 Loss: 1.4475900759868656e-08\n",
      "Iteration: 6994 lambda_n: 0.984742464718765 Loss: 1.441559490847414e-08\n",
      "Iteration: 6995 lambda_n: 0.9676880090177801 Loss: 1.4354470234381769e-08\n",
      "Iteration: 6996 lambda_n: 1.0076303814114775 Loss: 1.4294685503824002e-08\n",
      "Iteration: 6997 lambda_n: 1.0243459882276875 Loss: 1.4232719844463783e-08\n",
      "Iteration: 6998 lambda_n: 0.9161952357551846 Loss: 1.4170028279515745e-08\n",
      "Iteration: 6999 lambda_n: 1.0402894038334016 Loss: 1.411422865565065e-08\n",
      "Iteration: 7000 lambda_n: 1.0028343499313177 Loss: 1.4051147887183123e-08\n",
      "Iteration: 7001 lambda_n: 0.9911127253753782 Loss: 1.3990639165535025e-08\n",
      "Iteration: 7002 lambda_n: 0.9157842622052513 Loss: 1.393112302879912e-08\n",
      "Iteration: 7003 lambda_n: 0.8981038784625028 Loss: 1.3876389467544826e-08\n",
      "Iteration: 7004 lambda_n: 1.0185243830429842 Loss: 1.3822946484928654e-08\n",
      "Iteration: 7005 lambda_n: 0.9938476331361764 Loss: 1.3762597131918524e-08\n",
      "Iteration: 7006 lambda_n: 0.9739678395976614 Loss: 1.3703995257462576e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7007 lambda_n: 1.010886371644864 Loss: 1.3646837148724401e-08\n",
      "Iteration: 7008 lambda_n: 1.010545361081651 Loss: 1.3587787541834776e-08\n",
      "Iteration: 7009 lambda_n: 1.0141573510617 Loss: 1.3529041855379818e-08\n",
      "Iteration: 7010 lambda_n: 0.9706198949505302 Loss: 1.3470369742177752e-08\n",
      "Iteration: 7011 lambda_n: 0.9658844863248705 Loss: 1.341448730521099e-08\n",
      "Iteration: 7012 lambda_n: 0.901915070192478 Loss: 1.3359134391458826e-08\n",
      "Iteration: 7013 lambda_n: 0.9130495351995802 Loss: 1.3307684865713459e-08\n",
      "Iteration: 7014 lambda_n: 1.043844489810698 Loss: 1.3255823799326662e-08\n",
      "Iteration: 7015 lambda_n: 0.9956645947695445 Loss: 1.3196791710834245e-08\n",
      "Iteration: 7016 lambda_n: 0.9812592306589291 Loss: 1.314076392016056e-08\n",
      "Iteration: 7017 lambda_n: 1.0011223120691748 Loss: 1.308580840665041e-08\n",
      "Iteration: 7018 lambda_n: 0.9685387375045933 Loss: 1.3030002394593963e-08\n",
      "Iteration: 7019 lambda_n: 1.0220630805192938 Loss: 1.2976269897259228e-08\n",
      "Iteration: 7020 lambda_n: 0.9661137610552247 Loss: 1.2919829560688878e-08\n",
      "Iteration: 7021 lambda_n: 1.0005864511075622 Loss: 1.2866738260092881e-08\n",
      "Iteration: 7022 lambda_n: 0.9696227300124718 Loss: 1.2812005553473547e-08\n",
      "Iteration: 7023 lambda_n: 0.9647538031984163 Loss: 1.2759219130806553e-08\n",
      "Iteration: 7024 lambda_n: 0.9292117546839783 Loss: 1.27069401841972e-08\n",
      "Iteration: 7025 lambda_n: 1.0396675875236403 Loss: 1.2656818374373786e-08\n",
      "Iteration: 7026 lambda_n: 0.9982167559476057 Loss: 1.2600987008702998e-08\n",
      "Iteration: 7027 lambda_n: 0.9146784626629177 Loss: 1.2547646787594815e-08\n",
      "Iteration: 7028 lambda_n: 1.0226061725045943 Loss: 1.2499002524195135e-08\n",
      "Iteration: 7029 lambda_n: 0.9198324683462502 Loss: 1.2444855621760212e-08\n",
      "Iteration: 7030 lambda_n: 0.9357098920960825 Loss: 1.239638740292339e-08\n",
      "Iteration: 7031 lambda_n: 0.9648219252504688 Loss: 1.234729852862965e-08\n",
      "Iteration: 7032 lambda_n: 0.9538762228126599 Loss: 1.2296907962145608e-08\n",
      "Iteration: 7033 lambda_n: 1.0205550515817268 Loss: 1.2247317880082266e-08\n",
      "Iteration: 7034 lambda_n: 0.9355479199102601 Loss: 1.2194502456390002e-08\n",
      "Iteration: 7035 lambda_n: 1.0442257343977592 Loss: 1.2146321305687312e-08\n",
      "Iteration: 7036 lambda_n: 0.950999794344413 Loss: 1.2092783078217297e-08\n",
      "Iteration: 7037 lambda_n: 0.9630627157511832 Loss: 1.2044266763197394e-08\n",
      "Iteration: 7038 lambda_n: 0.9999138886093393 Loss: 1.1995357538321037e-08\n",
      "Iteration: 7039 lambda_n: 1.0404162587725694 Loss: 1.1944809771480519e-08\n",
      "Iteration: 7040 lambda_n: 1.0221861808849118 Loss: 1.1892465048474863e-08\n",
      "Iteration: 7041 lambda_n: 1.0267762375138112 Loss: 1.1841292204657505e-08\n",
      "Iteration: 7042 lambda_n: 0.9479507200299735 Loss: 1.179013975882662e-08\n",
      "Iteration: 7043 lambda_n: 0.971732255584548 Loss: 1.1743144935998903e-08\n",
      "Iteration: 7044 lambda_n: 0.9293762433776992 Loss: 1.1695188650386108e-08\n",
      "Iteration: 7045 lambda_n: 0.9046079888143935 Loss: 1.1649534784862139e-08\n",
      "Iteration: 7046 lambda_n: 0.9419646259770051 Loss: 1.1605294185690177e-08\n",
      "Iteration: 7047 lambda_n: 1.0205600258775145 Loss: 1.155942513460628e-08\n",
      "Iteration: 7048 lambda_n: 1.0097371448518102 Loss: 1.1509951992233439e-08\n",
      "Iteration: 7049 lambda_n: 0.8954799139236045 Loss: 1.1461241330335322e-08\n",
      "Iteration: 7050 lambda_n: 0.9745567530193078 Loss: 1.1418249933703413e-08\n",
      "Iteration: 7051 lambda_n: 1.0125331035112548 Loss: 1.1371661825662925e-08\n",
      "Iteration: 7052 lambda_n: 1.031737225492012 Loss: 1.1323482977798365e-08\n",
      "Iteration: 7053 lambda_n: 0.9406954150043642 Loss: 1.127462707771266e-08\n",
      "Iteration: 7054 lambda_n: 0.9747995511361484 Loss: 1.1230300865231463e-08\n",
      "Iteration: 7055 lambda_n: 0.9946319047249773 Loss: 1.1184573474633618e-08\n",
      "Iteration: 7056 lambda_n: 1.0344333186509755 Loss: 1.11381323516619e-08\n",
      "Iteration: 7057 lambda_n: 1.0325885077176642 Loss: 1.109006167203355e-08\n",
      "Iteration: 7058 lambda_n: 0.9704023198097592 Loss: 1.1042313051071207e-08\n",
      "Iteration: 7059 lambda_n: 1.0401302951796891 Loss: 1.0997660467020352e-08\n",
      "Iteration: 7060 lambda_n: 0.9752167417125113 Loss: 1.0950020712894456e-08\n",
      "Iteration: 7061 lambda_n: 0.9247400117760207 Loss: 1.0905575131667179e-08\n",
      "Iteration: 7062 lambda_n: 0.9679841814170566 Loss: 1.0863625590923786e-08\n",
      "Iteration: 7063 lambda_n: 0.9884627881063364 Loss: 1.0819907777072084e-08\n",
      "Iteration: 7064 lambda_n: 0.9297553521449192 Loss: 1.0775470860553292e-08\n",
      "Iteration: 7065 lambda_n: 1.0287604667026031 Loss: 1.0733869729348717e-08\n",
      "Iteration: 7066 lambda_n: 1.0461350577683577 Loss: 1.0688042722384111e-08\n",
      "Iteration: 7067 lambda_n: 0.9107863767364198 Loss: 1.0641670027890546e-08\n",
      "Iteration: 7068 lambda_n: 0.9367305766228616 Loss: 1.0601497783942118e-08\n",
      "Iteration: 7069 lambda_n: 0.8952233661799215 Loss: 1.0560360420821968e-08\n",
      "Iteration: 7070 lambda_n: 1.0445876052273089 Loss: 1.052122111348358e-08\n",
      "Iteration: 7071 lambda_n: 0.9278094612608414 Loss: 1.047574660525834e-08\n",
      "Iteration: 7072 lambda_n: 1.0203116272185913 Loss: 1.043555643384058e-08\n",
      "Iteration: 7073 lambda_n: 0.9650206146547494 Loss: 1.0391554770415202e-08\n",
      "Iteration: 7074 lambda_n: 0.9998140124749555 Loss: 1.0350139622065516e-08\n",
      "Iteration: 7075 lambda_n: 0.9296477575376216 Loss: 1.0307428425460631e-08\n",
      "Iteration: 7076 lambda_n: 0.9044087703479575 Loss: 1.0267903521651506e-08\n",
      "Iteration: 7077 lambda_n: 1.04799777904063 Loss: 1.0229621784876139e-08\n",
      "Iteration: 7078 lambda_n: 0.9366419451634083 Loss: 1.0185453566633397e-08\n",
      "Iteration: 7079 lambda_n: 1.0394514735553153 Loss: 1.0146175142242732e-08\n",
      "Iteration: 7080 lambda_n: 1.0013334378111765 Loss: 1.0102779969969609e-08\n",
      "Iteration: 7081 lambda_n: 0.9787154688756436 Loss: 1.006118288981419e-08\n",
      "Iteration: 7082 lambda_n: 1.0381455126423231 Loss: 1.0020719127680159e-08\n",
      "Iteration: 7083 lambda_n: 0.9597121537470261 Loss: 9.977998399295599e-09\n",
      "Iteration: 7084 lambda_n: 0.959909767243515 Loss: 9.93870023345291e-09\n",
      "Iteration: 7085 lambda_n: 0.9337243196134315 Loss: 9.89957352721298e-09\n",
      "Iteration: 7086 lambda_n: 0.9825639997706607 Loss: 9.861687980485854e-09\n",
      "Iteration: 7087 lambda_n: 1.023688397519643 Loss: 9.821998066194165e-09\n",
      "Iteration: 7088 lambda_n: 1.0400423300649768 Loss: 9.780840422915104e-09\n",
      "Iteration: 7089 lambda_n: 0.9338566435673353 Loss: 9.73922902217729e-09\n",
      "Iteration: 7090 lambda_n: 1.0334959229175757 Loss: 9.702050746605729e-09\n",
      "Iteration: 7091 lambda_n: 1.0325378962241651 Loss: 9.661088768872793e-09\n",
      "Iteration: 7092 lambda_n: 1.0225026861111464 Loss: 9.620366027362945e-09\n",
      "Iteration: 7093 lambda_n: 0.9397233584258696 Loss: 9.580237196464924e-09\n",
      "Iteration: 7094 lambda_n: 0.9564925657925779 Loss: 9.543536369155739e-09\n",
      "Iteration: 7095 lambda_n: 0.9073467619499994 Loss: 9.506347701937619e-09\n",
      "Iteration: 7096 lambda_n: 1.0123698704697244 Loss: 9.471230300955858e-09\n",
      "Iteration: 7097 lambda_n: 0.9510566773188219 Loss: 9.432217539286248e-09\n",
      "Iteration: 7098 lambda_n: 0.8967939042230931 Loss: 9.39574396211324e-09\n",
      "Iteration: 7099 lambda_n: 0.9544348710798858 Loss: 9.361506928766372e-09\n",
      "Iteration: 7100 lambda_n: 0.9883785723617343 Loss: 9.325224915997205e-09\n",
      "Iteration: 7101 lambda_n: 1.0462181177354384 Loss: 9.287823251121304e-09\n",
      "Iteration: 7102 lambda_n: 0.9509374837312298 Loss: 9.248419149149936e-09\n",
      "Iteration: 7103 lambda_n: 1.0083224756944729 Loss: 9.212781712048338e-09\n",
      "Iteration: 7104 lambda_n: 0.9030977085322015 Loss: 9.175164783480744e-09\n",
      "Iteration: 7105 lambda_n: 0.9112081508440096 Loss: 9.141634830219986e-09\n",
      "Iteration: 7106 lambda_n: 1.0160054101001013 Loss: 9.107949114893031e-09\n",
      "Iteration: 7107 lambda_n: 1.0392779286204508 Loss: 9.07055226071136e-09\n",
      "Iteration: 7108 lambda_n: 1.0436084770287057 Loss: 9.032483722203794e-09\n",
      "Iteration: 7109 lambda_n: 0.940094589347822 Loss: 8.994445548359135e-09\n",
      "Iteration: 7110 lambda_n: 0.9889823588421052 Loss: 8.960350207528347e-09\n",
      "Iteration: 7111 lambda_n: 0.964437199394714 Loss: 8.924642295597314e-09\n",
      "Iteration: 7112 lambda_n: 0.9420133611875171 Loss: 8.889984340029285e-09\n",
      "Iteration: 7113 lambda_n: 1.0436982181460188 Loss: 8.856287451513863e-09\n",
      "Iteration: 7114 lambda_n: 0.9664995429771186 Loss: 8.819120647054983e-09\n",
      "Iteration: 7115 lambda_n: 0.9990953282613243 Loss: 8.784873625494939e-09\n",
      "Iteration: 7116 lambda_n: 1.025917496912812 Loss: 8.749634401638499e-09\n",
      "Iteration: 7117 lambda_n: 0.9132814379475898 Loss: 8.713621093134384e-09\n",
      "Iteration: 7118 lambda_n: 1.035695423616887 Loss: 8.681717908994609e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7119 lambda_n: 0.90415055696684 Loss: 8.645695870675007e-09\n",
      "Iteration: 7120 lambda_n: 0.9083188762249425 Loss: 8.614403673607656e-09\n",
      "Iteration: 7121 lambda_n: 0.9865674919723731 Loss: 8.583102363512747e-09\n",
      "Iteration: 7122 lambda_n: 0.9826449775141007 Loss: 8.54925152692827e-09\n",
      "Iteration: 7123 lambda_n: 0.9096070551847765 Loss: 8.515693420668939e-09\n",
      "Iteration: 7124 lambda_n: 1.0281149053305294 Loss: 8.484774615021839e-09\n",
      "Iteration: 7125 lambda_n: 1.0230688592164074 Loss: 8.449978917019833e-09\n",
      "Iteration: 7126 lambda_n: 0.9470871535428231 Loss: 8.415523233389204e-09\n",
      "Iteration: 7127 lambda_n: 1.001809497131203 Loss: 8.383781511075157e-09\n",
      "Iteration: 7128 lambda_n: 1.037872760509886 Loss: 8.35035702169428e-09\n",
      "Iteration: 7129 lambda_n: 0.9700621398921639 Loss: 8.315894282969601e-09\n",
      "Iteration: 7130 lambda_n: 1.0328833982056245 Loss: 8.283841996546143e-09\n",
      "Iteration: 7131 lambda_n: 1.0048188030746146 Loss: 8.249871477294928e-09\n",
      "Iteration: 7132 lambda_n: 0.9788907670996432 Loss: 8.21698616927071e-09\n",
      "Iteration: 7133 lambda_n: 0.977984763313567 Loss: 8.185102362512649e-09\n",
      "Iteration: 7134 lambda_n: 0.9263684861105967 Loss: 8.153396263773828e-09\n",
      "Iteration: 7135 lambda_n: 1.0182267018675941 Loss: 8.12350305492541e-09\n",
      "Iteration: 7136 lambda_n: 1.0137357584174982 Loss: 8.090790465772857e-09\n",
      "Iteration: 7137 lambda_n: 0.9148044940988925 Loss: 8.058379734465074e-09\n",
      "Iteration: 7138 lambda_n: 0.9445198633744027 Loss: 8.02927271988405e-09\n",
      "Iteration: 7139 lambda_n: 0.9710087758670006 Loss: 7.999350905799678e-09\n",
      "Iteration: 7140 lambda_n: 0.9480236915386133 Loss: 7.968728039572851e-09\n",
      "Iteration: 7141 lambda_n: 0.9110187416557473 Loss: 7.938967957217819e-09\n",
      "Iteration: 7142 lambda_n: 1.0414736220303473 Loss: 7.910498274228484e-09\n",
      "Iteration: 7143 lambda_n: 1.004465604080786 Loss: 7.878092910791831e-09\n",
      "Iteration: 7144 lambda_n: 0.9324533737120815 Loss: 7.846993631057982e-09\n",
      "Iteration: 7145 lambda_n: 0.9314039408970061 Loss: 7.81826156201814e-09\n",
      "Iteration: 7146 lambda_n: 1.0359806249046288 Loss: 7.789688964618751e-09\n",
      "Iteration: 7147 lambda_n: 1.0314771188656544 Loss: 7.758049049791376e-09\n",
      "Iteration: 7148 lambda_n: 0.9687844719373946 Loss: 7.726701676442114e-09\n",
      "Iteration: 7149 lambda_n: 0.9791620833648412 Loss: 7.697403739067337e-09\n",
      "Iteration: 7150 lambda_n: 0.9676972700260621 Loss: 7.667928229484719e-09\n",
      "Iteration: 7151 lambda_n: 0.947667761428872 Loss: 7.63893328111112e-09\n",
      "Iteration: 7152 lambda_n: 0.9538642934514151 Loss: 7.61066894356704e-09\n",
      "Iteration: 7153 lambda_n: 0.9579818134797359 Loss: 7.582347828384362e-09\n",
      "Iteration: 7154 lambda_n: 0.9569132585116291 Loss: 7.554033308554455e-09\n",
      "Iteration: 7155 lambda_n: 1.0208978753358424 Loss: 7.525879021423596e-09\n",
      "Iteration: 7156 lambda_n: 0.9859957993325534 Loss: 7.49597875230979e-09\n",
      "Iteration: 7157 lambda_n: 0.9799561303829014 Loss: 7.467240633784477e-09\n",
      "Iteration: 7158 lambda_n: 1.0342383806125457 Loss: 7.438812246379909e-09\n",
      "Iteration: 7159 lambda_n: 0.9694379461848938 Loss: 7.408948809427642e-09\n",
      "Iteration: 7160 lambda_n: 0.9889878174641585 Loss: 7.3810938140311944e-09\n",
      "Iteration: 7161 lambda_n: 1.045471415730655 Loss: 7.352807894351811e-09\n",
      "Iteration: 7162 lambda_n: 1.0373513405633712 Loss: 7.323046959089223e-09\n",
      "Iteration: 7163 lambda_n: 0.9687900861858882 Loss: 7.293663714429502e-09\n",
      "Iteration: 7164 lambda_n: 1.010355581477771 Loss: 7.2663574896738546e-09\n",
      "Iteration: 7165 lambda_n: 0.9931566040681551 Loss: 7.238010711493855e-09\n",
      "Iteration: 7166 lambda_n: 1.001871176560659 Loss: 7.210280067227506e-09\n",
      "Iteration: 7167 lambda_n: 1.0284188809754098 Loss: 7.182437959998011e-09\n",
      "Iteration: 7168 lambda_n: 0.9580134216813079 Loss: 7.153993999711155e-09\n",
      "Iteration: 7169 lambda_n: 0.9895800639248622 Loss: 7.127626536610057e-09\n",
      "Iteration: 7170 lambda_n: 0.9108939757672334 Loss: 7.100514110957115e-09\n",
      "Iteration: 7171 lambda_n: 0.9973783127293515 Loss: 7.075674606646071e-09\n",
      "Iteration: 7172 lambda_n: 0.9178891338438525 Loss: 7.048594393726865e-09\n",
      "Iteration: 7173 lambda_n: 0.9631554801263696 Loss: 7.023790261326474e-09\n",
      "Iteration: 7174 lambda_n: 0.9830301701503089 Loss: 6.997876292715207e-09\n",
      "Iteration: 7175 lambda_n: 0.9557993789353108 Loss: 6.971548459402442e-09\n",
      "Iteration: 7176 lambda_n: 0.9175106689730698 Loss: 6.9460692787078875e-09\n",
      "Iteration: 7177 lambda_n: 0.992406832352944 Loss: 6.9217216165153695e-09\n",
      "Iteration: 7178 lambda_n: 0.9388069381669605 Loss: 6.895501174110035e-09\n",
      "Iteration: 7179 lambda_n: 0.9563939661777867 Loss: 6.870813586421804e-09\n",
      "Iteration: 7180 lambda_n: 0.9264731649263256 Loss: 6.845775531516433e-09\n",
      "Iteration: 7181 lambda_n: 0.9055863646419247 Loss: 6.8216307631003484e-09\n",
      "Iteration: 7182 lambda_n: 1.0262385658459412 Loss: 6.7981339966502515e-09\n",
      "Iteration: 7183 lambda_n: 0.9954865695206214 Loss: 6.771621248527705e-09\n",
      "Iteration: 7184 lambda_n: 1.036339111666058 Loss: 6.7460280847098935e-09\n",
      "Iteration: 7185 lambda_n: 1.0220077142626525 Loss: 6.719510464279164e-09\n",
      "Iteration: 7186 lambda_n: 0.9308052461762428 Loss: 6.693488031642929e-09\n",
      "Iteration: 7187 lambda_n: 1.0115159444659096 Loss: 6.669902541544797e-09\n",
      "Iteration: 7188 lambda_n: 1.0134233415399148 Loss: 6.6443851426307655e-09\n",
      "Iteration: 7189 lambda_n: 1.0297482230897854 Loss: 6.6189422287381506e-09\n",
      "Iteration: 7190 lambda_n: 0.906341901356996 Loss: 6.593213687317298e-09\n",
      "Iteration: 7191 lambda_n: 0.9013673537772879 Loss: 6.5706788939672626e-09\n",
      "Iteration: 7192 lambda_n: 1.0338940400161853 Loss: 6.548364054119991e-09\n",
      "Iteration: 7193 lambda_n: 0.9687524051616685 Loss: 6.522877820322131e-09\n",
      "Iteration: 7194 lambda_n: 0.9085641377954095 Loss: 6.499114313294597e-09\n",
      "Iteration: 7195 lambda_n: 0.9055818942653642 Loss: 6.476929496012736e-09\n",
      "Iteration: 7196 lambda_n: 1.0124052722604358 Loss: 6.4549127000639e-09\n",
      "Iteration: 7197 lambda_n: 0.9145177099908774 Loss: 6.430404531519155e-09\n",
      "Iteration: 7198 lambda_n: 0.9327884128942281 Loss: 6.40837212045072e-09\n",
      "Iteration: 7199 lambda_n: 0.9609835031441233 Loss: 6.385996947027505e-09\n",
      "Iteration: 7200 lambda_n: 0.8990033839706573 Loss: 6.363047360341995e-09\n",
      "Iteration: 7201 lambda_n: 0.9026953029410986 Loss: 6.34167563241954e-09\n",
      "Iteration: 7202 lambda_n: 1.0429455182935292 Loss: 6.320307542014229e-09\n",
      "Iteration: 7203 lambda_n: 1.0144213994572946 Loss: 6.295725275390973e-09\n",
      "Iteration: 7204 lambda_n: 0.9661961693529403 Loss: 6.2719334242070875e-09\n",
      "Iteration: 7205 lambda_n: 0.9485791086053998 Loss: 6.249381467023075e-09\n",
      "Iteration: 7206 lambda_n: 0.8980790688816959 Loss: 6.227342030695597e-09\n",
      "Iteration: 7207 lambda_n: 1.002953229033721 Loss: 6.2065696253700304e-09\n",
      "Iteration: 7208 lambda_n: 0.9640370594312381 Loss: 6.183470295295081e-09\n",
      "Iteration: 7209 lambda_n: 0.9394420542464649 Loss: 6.1613726760850176e-09\n",
      "Iteration: 7210 lambda_n: 0.9896076233859064 Loss: 6.139937118346186e-09\n",
      "Iteration: 7211 lambda_n: 1.0345274880635869 Loss: 6.117457424433002e-09\n",
      "Iteration: 7212 lambda_n: 1.0164788552741038 Loss: 6.0940675136481665e-09\n",
      "Iteration: 7213 lambda_n: 0.9996480584913071 Loss: 6.0711982350023285e-09\n",
      "Iteration: 7214 lambda_n: 1.0108960509404403 Loss: 6.048815848144211e-09\n",
      "Iteration: 7215 lambda_n: 0.9329904043398581 Loss: 6.026288760463901e-09\n",
      "Iteration: 7216 lambda_n: 0.970177560461368 Loss: 6.005597169027824e-09\n",
      "Iteration: 7217 lambda_n: 0.9736537680344683 Loss: 5.984175929580249e-09\n",
      "Iteration: 7218 lambda_n: 0.9456699183244707 Loss: 5.96277668277569e-09\n",
      "Iteration: 7219 lambda_n: 0.9363582152445487 Loss: 5.9420882365651306e-09\n",
      "Iteration: 7220 lambda_n: 0.9180922992076531 Loss: 5.92169518855033e-09\n",
      "Iteration: 7221 lambda_n: 0.9040552235355468 Loss: 5.9017885572816355e-09\n",
      "Iteration: 7222 lambda_n: 0.9453266914851592 Loss: 5.882271459587489e-09\n",
      "Iteration: 7223 lambda_n: 0.8922700122820135 Loss: 5.8619507367076185e-09\n",
      "Iteration: 7224 lambda_n: 0.9355228067726941 Loss: 5.8428562822310515e-09\n",
      "Iteration: 7225 lambda_n: 0.9379393960828555 Loss: 5.8229207975456375e-09\n",
      "Iteration: 7226 lambda_n: 1.0394156650871609 Loss: 5.803022304886341e-09\n",
      "Iteration: 7227 lambda_n: 0.9000753201376855 Loss: 5.78106894949998e-09\n",
      "Iteration: 7228 lambda_n: 1.033221689314369 Loss: 5.762151961629705e-09\n",
      "Iteration: 7229 lambda_n: 1.0479526113278814 Loss: 5.7405292210836025e-09\n",
      "Iteration: 7230 lambda_n: 0.9986290186673332 Loss: 5.718705417027356e-09\n",
      "Iteration: 7231 lambda_n: 0.9526473380245787 Loss: 5.698011852320166e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7232 lambda_n: 0.9037379135792252 Loss: 5.678364343557915e-09\n",
      "Iteration: 7233 lambda_n: 0.9750297977826815 Loss: 5.6598094994077705e-09\n",
      "Iteration: 7234 lambda_n: 0.9332905211835718 Loss: 5.639876581290347e-09\n",
      "Iteration: 7235 lambda_n: 0.9827390219034239 Loss: 5.620884915089661e-09\n",
      "Iteration: 7236 lambda_n: 1.0232281236777323 Loss: 5.600975337165923e-09\n",
      "Iteration: 7237 lambda_n: 0.9737502105483647 Loss: 5.580341864685163e-09\n",
      "Iteration: 7238 lambda_n: 1.0441055607151095 Loss: 5.560801088506242e-09\n",
      "Iteration: 7239 lambda_n: 0.9864540611641388 Loss: 5.539945007886913e-09\n",
      "Iteration: 7240 lambda_n: 0.9048415996608475 Loss: 5.520337750014472e-09\n",
      "Iteration: 7241 lambda_n: 0.9272535809247227 Loss: 5.502436499135122e-09\n",
      "Iteration: 7242 lambda_n: 1.0081421179107053 Loss: 5.4841703518584906e-09\n",
      "Iteration: 7243 lambda_n: 1.0294610398935677 Loss: 5.464397899896665e-09\n",
      "Iteration: 7244 lambda_n: 0.942776756125056 Loss: 5.444303581816363e-09\n",
      "Iteration: 7245 lambda_n: 1.0135386698498974 Loss: 5.425990769299018e-09\n",
      "Iteration: 7246 lambda_n: 0.982701156974878 Loss: 5.4063912576624705e-09\n",
      "Iteration: 7247 lambda_n: 0.988898135075487 Loss: 5.38747909035265e-09\n",
      "Iteration: 7248 lambda_n: 1.0280940333308273 Loss: 5.3685360650666475e-09\n",
      "Iteration: 7249 lambda_n: 1.031649468146275 Loss: 5.348934307616702e-09\n",
      "Iteration: 7250 lambda_n: 0.9295360739461898 Loss: 5.329360342674418e-09\n",
      "Iteration: 7251 lambda_n: 0.9645359974176185 Loss: 5.311809742723633e-09\n",
      "Iteration: 7252 lambda_n: 0.9040396704787032 Loss: 5.2936783302522666e-09\n",
      "Iteration: 7253 lambda_n: 0.9160670624207351 Loss: 5.27676154975872e-09\n",
      "Iteration: 7254 lambda_n: 1.0196386985734667 Loss: 5.259692947550053e-09\n",
      "Iteration: 7255 lambda_n: 1.047147596813046 Loss: 5.2407768697422126e-09\n",
      "Iteration: 7256 lambda_n: 0.9123949379540784 Loss: 5.2214440706679755e-09\n",
      "Iteration: 7257 lambda_n: 1.0281937010834865 Loss: 5.204682365303987e-09\n",
      "Iteration: 7258 lambda_n: 0.9964543839452322 Loss: 5.185874811986222e-09\n",
      "Iteration: 7259 lambda_n: 0.9203286556065977 Loss: 5.167736347776891e-09\n",
      "Iteration: 7260 lambda_n: 0.9422970570596855 Loss: 5.151062419577688e-09\n",
      "Iteration: 7261 lambda_n: 1.0085392494239331 Loss: 5.134064704419853e-09\n",
      "Iteration: 7262 lambda_n: 0.9218183154440536 Loss: 5.115953102598567e-09\n",
      "Iteration: 7263 lambda_n: 1.0457147618478535 Loss: 5.09947764871906e-09\n",
      "Iteration: 7264 lambda_n: 0.99356297745216 Loss: 5.080869302520294e-09\n",
      "Iteration: 7265 lambda_n: 0.9445261698482682 Loss: 5.063276261036282e-09\n",
      "Iteration: 7266 lambda_n: 1.0366961532923271 Loss: 5.046629961623731e-09\n",
      "Iteration: 7267 lambda_n: 1.03620007674972 Loss: 5.028440835332356e-09\n",
      "Iteration: 7268 lambda_n: 0.9478691933551885 Loss: 5.010349404991927e-09\n",
      "Iteration: 7269 lambda_n: 0.9227498949516342 Loss: 4.993881097591966e-09\n",
      "Iteration: 7270 lambda_n: 1.0440668171526235 Loss: 4.9779209430580526e-09\n",
      "Iteration: 7271 lambda_n: 0.9082124835814599 Loss: 4.959941221892981e-09\n",
      "Iteration: 7272 lambda_n: 0.9996004663784379 Loss: 4.944378040422555e-09\n",
      "Iteration: 7273 lambda_n: 1.0119611461718587 Loss: 4.927322336385987e-09\n",
      "Iteration: 7274 lambda_n: 0.9261954895963804 Loss: 4.910137210577476e-09\n",
      "Iteration: 7275 lambda_n: 0.9570514330963446 Loss: 4.8944836427848685e-09\n",
      "Iteration: 7276 lambda_n: 1.0437723197511408 Loss: 4.878379322212686e-09\n",
      "Iteration: 7277 lambda_n: 0.93617132867446 Loss: 4.860895149797722e-09\n",
      "Iteration: 7278 lambda_n: 0.910994649201421 Loss: 4.845290583891447e-09\n",
      "Iteration: 7279 lambda_n: 0.9499273683943642 Loss: 4.830172756849379e-09\n",
      "Iteration: 7280 lambda_n: 1.0343366794536686 Loss: 4.814476639018993e-09\n",
      "Iteration: 7281 lambda_n: 1.026420799153947 Loss: 4.797462453558343e-09\n",
      "Iteration: 7282 lambda_n: 0.9447084431501483 Loss: 4.780660895743468e-09\n",
      "Iteration: 7283 lambda_n: 0.8983161198454411 Loss: 4.765271739488541e-09\n",
      "Iteration: 7284 lambda_n: 0.9719524026487926 Loss: 4.750703517504562e-09\n",
      "Iteration: 7285 lambda_n: 1.0157785364299714 Loss: 4.735007970432563e-09\n",
      "Iteration: 7286 lambda_n: 0.8925058494012806 Loss: 4.718679945847014e-09\n",
      "Iteration: 7287 lambda_n: 1.036603059820215 Loss: 4.704402141958864e-09\n",
      "Iteration: 7288 lambda_n: 0.9197738606426462 Loss: 4.687889056410653e-09\n",
      "Iteration: 7289 lambda_n: 1.0350721461944195 Loss: 4.673308654009866e-09\n",
      "Iteration: 7290 lambda_n: 0.9971124409084094 Loss: 4.656971793652368e-09\n",
      "Iteration: 7291 lambda_n: 1.045491037022139 Loss: 4.641310873700132e-09\n",
      "Iteration: 7292 lambda_n: 0.92060559235731 Loss: 4.6249673713448245e-09\n",
      "Iteration: 7293 lambda_n: 0.9348284152082401 Loss: 4.6106470220532694e-09\n",
      "Iteration: 7294 lambda_n: 0.9632610944389026 Loss: 4.596168585344442e-09\n",
      "Iteration: 7295 lambda_n: 0.9837307705685585 Loss: 4.581315569092631e-09\n",
      "Iteration: 7296 lambda_n: 0.9171138505494691 Loss: 4.56621584394763e-09\n",
      "Iteration: 7297 lambda_n: 0.9458904945358622 Loss: 4.55220392732059e-09\n",
      "Iteration: 7298 lambda_n: 0.9614604673006532 Loss: 4.5378148594440075e-09\n",
      "Iteration: 7299 lambda_n: 1.0441103504849698 Loss: 4.5232541714051026e-09\n",
      "Iteration: 7300 lambda_n: 0.913675967700359 Loss: 4.5075135553194084e-09\n",
      "Iteration: 7301 lambda_n: 1.0391189584086482 Loss: 4.493807049091211e-09\n",
      "Iteration: 7302 lambda_n: 0.987785636867128 Loss: 4.478285940690733e-09\n",
      "Iteration: 7303 lambda_n: 0.9073856073424621 Loss: 4.463603843684093e-09\n",
      "Iteration: 7304 lambda_n: 0.9301196369041069 Loss: 4.450179540579082e-09\n",
      "Iteration: 7305 lambda_n: 1.033936463244998 Loss: 4.436477782423307e-09\n",
      "Iteration: 7306 lambda_n: 0.9055731258187094 Loss: 4.421313517669874e-09\n",
      "Iteration: 7307 lambda_n: 0.9529550763302176 Loss: 4.408096552105857e-09\n",
      "Iteration: 7308 lambda_n: 0.9120862830046065 Loss: 4.394247439438552e-09\n",
      "Iteration: 7309 lambda_n: 1.0385513335580607 Loss: 4.381051786471364e-09\n",
      "Iteration: 7310 lambda_n: 1.0030451623858525 Loss: 4.366091146182606e-09\n",
      "Iteration: 7311 lambda_n: 0.9021995195019991 Loss: 4.3517126917893234e-09\n",
      "Iteration: 7312 lambda_n: 1.0439203458033253 Loss: 4.338840933095913e-09\n",
      "Iteration: 7313 lambda_n: 0.929932429093359 Loss: 4.324010628631609e-09\n",
      "Iteration: 7314 lambda_n: 1.0470563022849282 Loss: 4.31086461185282e-09\n",
      "Iteration: 7315 lambda_n: 1.0140812375855033 Loss: 4.296127788478281e-09\n",
      "Iteration: 7316 lambda_n: 0.9536326702606341 Loss: 4.2819254703320185e-09\n",
      "Iteration: 7317 lambda_n: 0.9145821541197782 Loss: 4.268633526198557e-09\n",
      "Iteration: 7318 lambda_n: 0.9942294383939784 Loss: 4.255943139981391e-09\n",
      "Iteration: 7319 lambda_n: 0.9535492352765845 Loss: 4.2422070778632545e-09\n",
      "Iteration: 7320 lambda_n: 0.9362671163787383 Loss: 4.2290947248783236e-09\n",
      "Iteration: 7321 lambda_n: 1.0275160760991355 Loss: 4.216277850027135e-09\n",
      "Iteration: 7322 lambda_n: 0.9342291950251664 Loss: 4.202273914287157e-09\n",
      "Iteration: 7323 lambda_n: 1.0356627413846284 Loss: 4.189602977426626e-09\n",
      "Iteration: 7324 lambda_n: 1.0003220380642246 Loss: 4.175618150436894e-09\n",
      "Iteration: 7325 lambda_n: 0.9351427917641587 Loss: 4.162176418770117e-09\n",
      "Iteration: 7326 lambda_n: 0.9841154870171948 Loss: 4.149669703135665e-09\n",
      "Iteration: 7327 lambda_n: 1.0317139628114926 Loss: 4.136566001283511e-09\n",
      "Iteration: 7328 lambda_n: 0.9577082829361477 Loss: 4.122892205168729e-09\n",
      "Iteration: 7329 lambda_n: 0.893353314359428 Loss: 4.11026089548218e-09\n",
      "Iteration: 7330 lambda_n: 0.9026118963570603 Loss: 4.098531475403941e-09\n",
      "Iteration: 7331 lambda_n: 0.9284355110965007 Loss: 4.086730349687394e-09\n",
      "Iteration: 7332 lambda_n: 1.0384139312688803 Loss: 4.074643207801576e-09\n",
      "Iteration: 7333 lambda_n: 0.9974676223404643 Loss: 4.061183422393781e-09\n",
      "Iteration: 7334 lambda_n: 0.9968058016997431 Loss: 4.048317569519486e-09\n",
      "Iteration: 7335 lambda_n: 0.9639607478935845 Loss: 4.035520641532133e-09\n",
      "Iteration: 7336 lambda_n: 1.0113536486758787 Loss: 4.023203437533535e-09\n",
      "Iteration: 7337 lambda_n: 1.0316981962103762 Loss: 4.010339326344972e-09\n",
      "Iteration: 7338 lambda_n: 0.8944044185512795 Loss: 3.997278924216591e-09\n",
      "Iteration: 7339 lambda_n: 0.9721511590734582 Loss: 3.986011486269595e-09\n",
      "Iteration: 7340 lambda_n: 0.9316949974493182 Loss: 3.973816213392785e-09\n",
      "Iteration: 7341 lambda_n: 0.9099208134084831 Loss: 3.9621819175538035e-09\n",
      "Iteration: 7342 lambda_n: 0.9979078157545171 Loss: 3.9508693367140895e-09\n",
      "Iteration: 7343 lambda_n: 0.920645348923766 Loss: 3.938516031937206e-09\n",
      "Iteration: 7344 lambda_n: 0.9948525701836192 Loss: 3.927172672940827e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7345 lambda_n: 1.044400269633278 Loss: 3.914968141732154e-09\n",
      "Iteration: 7346 lambda_n: 0.8975011882976894 Loss: 3.902215791799079e-09\n",
      "Iteration: 7347 lambda_n: 1.0319406883253666 Loss: 3.8913109188294446e-09\n",
      "Iteration: 7348 lambda_n: 0.9721206284531889 Loss: 3.878825574185462e-09\n",
      "Iteration: 7349 lambda_n: 0.8933888891909254 Loss: 3.8671210845760335e-09\n",
      "Iteration: 7350 lambda_n: 1.0075571109498913 Loss: 3.8564137188218375e-09\n",
      "Iteration: 7351 lambda_n: 0.9963761837629267 Loss: 3.844388836252312e-09\n",
      "Iteration: 7352 lambda_n: 1.0433522581072021 Loss: 3.832553767375631e-09\n",
      "Iteration: 7353 lambda_n: 0.9895055356384117 Loss: 3.820218831220457e-09\n",
      "Iteration: 7354 lambda_n: 0.9667502468968899 Loss: 3.808577898783051e-09\n",
      "Iteration: 7355 lambda_n: 0.9417682349468232 Loss: 3.797257599850322e-09\n",
      "Iteration: 7356 lambda_n: 0.9013031192901142 Loss: 3.7862799847537964e-09\n",
      "Iteration: 7357 lambda_n: 0.923496432285576 Loss: 3.7758205848778034e-09\n",
      "Iteration: 7358 lambda_n: 0.9725354332626861 Loss: 3.765149081732933e-09\n",
      "Iteration: 7359 lambda_n: 1.0346605781689402 Loss: 3.75395974484851e-09\n",
      "Iteration: 7360 lambda_n: 1.0284433497458207 Loss: 3.74211011945289e-09\n",
      "Iteration: 7361 lambda_n: 0.9975642030528434 Loss: 3.73038907996379e-09\n",
      "Iteration: 7362 lambda_n: 0.9519756018126333 Loss: 3.7190749521894164e-09\n",
      "Iteration: 7363 lambda_n: 0.9856682669256379 Loss: 3.708328478884784e-09\n",
      "Iteration: 7364 lambda_n: 0.9529092264823502 Loss: 3.697251492257498e-09\n",
      "Iteration: 7365 lambda_n: 1.0218030220406555 Loss: 3.6865922816572233e-09\n",
      "Iteration: 7366 lambda_n: 0.9340417857593923 Loss: 3.675213670138735e-09\n",
      "Iteration: 7367 lambda_n: 1.0234566606081117 Loss: 3.6648623041039396e-09\n",
      "Iteration: 7368 lambda_n: 0.9569451157306609 Loss: 3.653569875308783e-09\n",
      "Iteration: 7369 lambda_n: 0.9867781615045997 Loss: 3.643062099757442e-09\n",
      "Iteration: 7370 lambda_n: 0.9480960277807728 Loss: 3.6322755058374653e-09\n",
      "Iteration: 7371 lambda_n: 0.9290247377551871 Loss: 3.621959829206647e-09\n",
      "Iteration: 7372 lambda_n: 0.9233430637808409 Loss: 3.611896709291817e-09\n",
      "Iteration: 7373 lambda_n: 0.9605242887303993 Loss: 3.601938832335991e-09\n",
      "Iteration: 7374 lambda_n: 1.038205848671532 Loss: 3.5916249453501493e-09\n",
      "Iteration: 7375 lambda_n: 1.0035069215219174 Loss: 3.5805273063758207e-09\n",
      "Iteration: 7376 lambda_n: 1.0411811908444675 Loss: 3.569852915494713e-09\n",
      "Iteration: 7377 lambda_n: 0.9285787639683946 Loss: 3.558830044305205e-09\n",
      "Iteration: 7378 lambda_n: 0.8945760704816201 Loss: 3.5490473756834553e-09\n",
      "Iteration: 7379 lambda_n: 0.9696164593104297 Loss: 3.5396640558260227e-09\n",
      "Iteration: 7380 lambda_n: 0.9301996003719321 Loss: 3.5295364255934557e-09\n",
      "Iteration: 7381 lambda_n: 0.9520482414783132 Loss: 3.519864781749899e-09\n",
      "Iteration: 7382 lambda_n: 0.9055990924800634 Loss: 3.510009261290174e-09\n",
      "Iteration: 7383 lambda_n: 0.9826970094475227 Loss: 3.5006765128443854e-09\n",
      "Iteration: 7384 lambda_n: 1.0330796601146692 Loss: 3.490592354258583e-09\n",
      "Iteration: 7385 lambda_n: 1.042779236252763 Loss: 3.4800401607673095e-09\n",
      "Iteration: 7386 lambda_n: 0.9291684289500336 Loss: 3.4694406157074672e-09\n",
      "Iteration: 7387 lambda_n: 1.0063362887824945 Loss: 3.460042149419632e-09\n",
      "Iteration: 7388 lambda_n: 1.036865229773812 Loss: 3.4499076140546173e-09\n",
      "Iteration: 7389 lambda_n: 0.95461565058495 Loss: 3.4395150293577945e-09\n",
      "Iteration: 7390 lambda_n: 1.0116218136882003 Loss: 3.42999344324456e-09\n",
      "Iteration: 7391 lambda_n: 0.9370751500612798 Loss: 3.4199485438957494e-09\n",
      "Iteration: 7392 lambda_n: 1.0308228114074158 Loss: 3.410688067667894e-09\n",
      "Iteration: 7393 lambda_n: 0.9087500568442377 Loss: 3.4005460352140897e-09\n",
      "Iteration: 7394 lambda_n: 1.0400817800019073 Loss: 3.3916483303162963e-09\n",
      "Iteration: 7395 lambda_n: 1.040779739698906 Loss: 3.381508256344167e-09\n",
      "Iteration: 7396 lambda_n: 0.9335735250256748 Loss: 3.3714109721686165e-09\n",
      "Iteration: 7397 lambda_n: 1.0222158549677858 Loss: 3.3623980267224387e-09\n",
      "Iteration: 7398 lambda_n: 0.8973418110547144 Loss: 3.3525726236381407e-09\n",
      "Iteration: 7399 lambda_n: 1.0178901588744742 Loss: 3.3439888967921003e-09\n",
      "Iteration: 7400 lambda_n: 0.9456185431962806 Loss: 3.3342931136370764e-09\n",
      "Iteration: 7401 lambda_n: 0.9898366144071827 Loss: 3.325328803859397e-09\n",
      "Iteration: 7402 lambda_n: 0.9605110534053909 Loss: 3.315987007491285e-09\n",
      "Iteration: 7403 lambda_n: 0.900395201977837 Loss: 3.3069641417541066e-09\n",
      "Iteration: 7404 lambda_n: 1.0274876124989087 Loss: 3.2985441393474222e-09\n",
      "Iteration: 7405 lambda_n: 1.0331968777370235 Loss: 3.2889763143618957e-09\n",
      "Iteration: 7406 lambda_n: 0.9053038906090715 Loss: 3.2794017585579165e-09\n",
      "Iteration: 7407 lambda_n: 0.9917668082717105 Loss: 3.271053077238951e-09\n",
      "Iteration: 7408 lambda_n: 0.9311617591157014 Loss: 3.261945952247944e-09\n",
      "Iteration: 7409 lambda_n: 0.9572476279503567 Loss: 3.2534351616720627e-09\n",
      "Iteration: 7410 lambda_n: 1.0303757859821954 Loss: 3.244724227930769e-09\n",
      "Iteration: 7411 lambda_n: 1.0480735278006217 Loss: 3.2353900157620183e-09\n",
      "Iteration: 7412 lambda_n: 1.0272945617280054 Loss: 3.2259414305266424e-09\n",
      "Iteration: 7413 lambda_n: 0.9668475078995279 Loss: 3.2167257639793195e-09\n",
      "Iteration: 7414 lambda_n: 0.9881645629728099 Loss: 3.208094182542127e-09\n",
      "Iteration: 7415 lambda_n: 0.9466751796583117 Loss: 3.199312369631964e-09\n",
      "Iteration: 7416 lambda_n: 1.0227024265235405 Loss: 3.19093830570242e-09\n",
      "Iteration: 7417 lambda_n: 0.9695203167271491 Loss: 3.1819319557568766e-09\n",
      "Iteration: 7418 lambda_n: 0.9160522493489058 Loss: 3.173434943520736e-09\n",
      "Iteration: 7419 lambda_n: 1.0267961606981189 Loss: 3.1654430925648924e-09\n",
      "Iteration: 7420 lambda_n: 0.9648253040171618 Loss: 3.1565236379550183e-09\n",
      "Iteration: 7421 lambda_n: 0.91781579474143 Loss: 3.1481829100657308e-09\n",
      "Iteration: 7422 lambda_n: 0.9538045689926108 Loss: 3.140284506548724e-09\n",
      "Iteration: 7423 lambda_n: 1.0154350066187936 Loss: 3.132111787487328e-09\n",
      "Iteration: 7424 lambda_n: 0.9693320315225985 Loss: 3.1234499668933835e-09\n",
      "Iteration: 7425 lambda_n: 0.9239524727033991 Loss: 3.115220832053192e-09\n",
      "Iteration: 7426 lambda_n: 0.9928811539075929 Loss: 3.107412644510556e-09\n",
      "Iteration: 7427 lambda_n: 0.986637945168794 Loss: 3.099058367773738e-09\n",
      "Iteration: 7428 lambda_n: 1.0024342962885744 Loss: 3.0907953363280345e-09\n",
      "Iteration: 7429 lambda_n: 1.040731026866418 Loss: 3.0824388935881066e-09\n",
      "Iteration: 7430 lambda_n: 1.0397452802372409 Loss: 3.0738040496232766e-09\n",
      "Iteration: 7431 lambda_n: 0.9099202714683204 Loss: 3.065219543323884e-09\n",
      "Iteration: 7432 lambda_n: 0.9629293376713545 Loss: 3.0577435628406598e-09\n",
      "Iteration: 7433 lambda_n: 0.9343844228057328 Loss: 3.0498658791779316e-09\n",
      "Iteration: 7434 lambda_n: 1.0417428001525995 Loss: 3.0422562709611106e-09\n",
      "Iteration: 7435 lambda_n: 0.9651952147380914 Loss: 3.033809580084671e-09\n",
      "Iteration: 7436 lambda_n: 0.8945516618325217 Loss: 3.0260218162541545e-09\n",
      "Iteration: 7437 lambda_n: 1.0224402382773992 Loss: 3.0188367401567047e-09\n",
      "Iteration: 7438 lambda_n: 0.9747721009809621 Loss: 3.0106589759145934e-09\n",
      "Iteration: 7439 lambda_n: 0.9352436829966696 Loss: 3.0028998912462615e-09\n",
      "Iteration: 7440 lambda_n: 0.9477770292502824 Loss: 2.9954895102730267e-09\n",
      "Iteration: 7441 lambda_n: 1.0041584085156552 Loss: 2.988012795081406e-09\n",
      "Iteration: 7442 lambda_n: 0.8986628431254686 Loss: 2.980126568722213e-09\n",
      "Iteration: 7443 lambda_n: 1.0249525380252442 Loss: 2.9731021125366892e-09\n",
      "Iteration: 7444 lambda_n: 0.9360930314368435 Loss: 2.9651243224287582e-09\n",
      "Iteration: 7445 lambda_n: 0.91723278486423 Loss: 2.9578732225403582e-09\n",
      "Iteration: 7446 lambda_n: 0.9754036160475723 Loss: 2.9507994384749447e-09\n",
      "Iteration: 7447 lambda_n: 1.039451716139436 Loss: 2.943309430610338e-09\n",
      "Iteration: 7448 lambda_n: 0.9378142041490445 Loss: 2.9353641628974966e-09\n",
      "Iteration: 7449 lambda_n: 1.0310823069972037 Loss: 2.92823075260625e-09\n",
      "Iteration: 7450 lambda_n: 1.0364515436944364 Loss: 2.920422446526516e-09\n",
      "Iteration: 7451 lambda_n: 0.9113792954883333 Loss: 2.9126114638985165e-09\n",
      "Iteration: 7452 lambda_n: 0.9805961786077324 Loss: 2.905776448994408e-09\n",
      "Iteration: 7453 lambda_n: 0.9542437989631902 Loss: 2.898453821841876e-09\n",
      "Iteration: 7454 lambda_n: 0.9385118857457149 Loss: 2.891360776101185e-09\n",
      "Iteration: 7455 lambda_n: 1.0349998919203074 Loss: 2.8844159058671526e-09\n",
      "Iteration: 7456 lambda_n: 0.961945987214307 Loss: 2.8767908000074667e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7457 lambda_n: 1.0081735107868615 Loss: 2.8697383280049746e-09\n",
      "Iteration: 7458 lambda_n: 0.9777118764245536 Loss: 2.8623803059956734e-09\n",
      "Iteration: 7459 lambda_n: 1.0069378384668026 Loss: 2.8552783717296906e-09\n",
      "Iteration: 7460 lambda_n: 0.9670342207473989 Loss: 2.8479977182015744e-09\n",
      "Iteration: 7461 lambda_n: 0.9773933708781878 Loss: 2.841038624069511e-09\n",
      "Iteration: 7462 lambda_n: 0.9649149339325265 Loss: 2.8340369244118052e-09\n",
      "Iteration: 7463 lambda_n: 0.9110465568293702 Loss: 2.8271563182342582e-09\n",
      "Iteration: 7464 lambda_n: 0.960471426672425 Loss: 2.8206892511014616e-09\n",
      "Iteration: 7465 lambda_n: 0.9541969353965376 Loss: 2.813900498666768e-09\n",
      "Iteration: 7466 lambda_n: 0.9726526148722857 Loss: 2.8071865049664075e-09\n",
      "Iteration: 7467 lambda_n: 0.9209834365844718 Loss: 2.8003733011398418e-09\n",
      "Iteration: 7468 lambda_n: 0.9446982583586322 Loss: 2.7939514745123625e-09\n",
      "Iteration: 7469 lambda_n: 1.0326835167831439 Loss: 2.7873927667168345e-09\n",
      "Iteration: 7470 lambda_n: 0.9062251719045576 Loss: 2.7802550170040732e-09\n",
      "Iteration: 7471 lambda_n: 0.959315541029874 Loss: 2.7740216600584005e-09\n",
      "Iteration: 7472 lambda_n: 0.9932822412842326 Loss: 2.7674512153883478e-09\n",
      "Iteration: 7473 lambda_n: 0.9514562416565343 Loss: 2.7606787542805956e-09\n",
      "Iteration: 7474 lambda_n: 0.9772310700310689 Loss: 2.7542217164768622e-09\n",
      "Iteration: 7475 lambda_n: 0.9608720220224384 Loss: 2.7476193770855393e-09\n",
      "Iteration: 7476 lambda_n: 0.9188589653289568 Loss: 2.7411573366127452e-09\n",
      "Iteration: 7477 lambda_n: 0.9193402550043329 Loss: 2.7350057103351386e-09\n",
      "Iteration: 7478 lambda_n: 1.042197028696372 Loss: 2.72887740065379e-09\n",
      "Iteration: 7479 lambda_n: 0.985396370502506 Loss: 2.7219601215440163e-09\n",
      "Iteration: 7480 lambda_n: 1.0171104551366013 Loss: 2.7154518362615317e-09\n",
      "Iteration: 7481 lambda_n: 0.9918742881217488 Loss: 2.7087651467296154e-09\n",
      "Iteration: 7482 lambda_n: 0.897299157976241 Loss: 2.702275490282465e-09\n",
      "Iteration: 7483 lambda_n: 0.8948279328153602 Loss: 2.696431942021849e-09\n",
      "Iteration: 7484 lambda_n: 0.9877346953755649 Loss: 2.690629031659859e-09\n",
      "Iteration: 7485 lambda_n: 0.9966431021082846 Loss: 2.6842505358143276e-09\n",
      "Iteration: 7486 lambda_n: 1.0442866580352743 Loss: 2.6778443605799995e-09\n",
      "Iteration: 7487 lambda_n: 0.9191244844027178 Loss: 2.6711633435417943e-09\n",
      "Iteration: 7488 lambda_n: 1.003895817757579 Loss: 2.665311885827815e-09\n",
      "Iteration: 7489 lambda_n: 0.988423584492696 Loss: 2.658948326985314e-09\n",
      "Iteration: 7490 lambda_n: 0.9095918752714927 Loss: 2.6527123642163813e-09\n",
      "Iteration: 7491 lambda_n: 0.9919699591233033 Loss: 2.6470003540778877e-09\n",
      "Iteration: 7492 lambda_n: 0.9772365060640525 Loss: 2.6407976457391673e-09\n",
      "Iteration: 7493 lambda_n: 0.9980215192278119 Loss: 2.634715516397314e-09\n",
      "Iteration: 7494 lambda_n: 0.899339557306374 Loss: 2.6285325168926483e-09\n",
      "Iteration: 7495 lambda_n: 1.0288582471433432 Loss: 2.622986960421951e-09\n",
      "Iteration: 7496 lambda_n: 0.9787728603216675 Loss: 2.6166695662511494e-09\n",
      "Iteration: 7497 lambda_n: 0.950266853905287 Loss: 2.6106887218652558e-09\n",
      "Iteration: 7498 lambda_n: 1.0328454147718602 Loss: 2.604908735886855e-09\n",
      "Iteration: 7499 lambda_n: 0.8993014709200479 Loss: 2.5986544997188187e-09\n",
      "Iteration: 7500 lambda_n: 1.0306826703237282 Loss: 2.593235304045179e-09\n",
      "Iteration: 7501 lambda_n: 1.0017880776321288 Loss: 2.587050628732405e-09\n",
      "Iteration: 7502 lambda_n: 0.8978299777520964 Loss: 2.581068423916909e-09\n",
      "Iteration: 7503 lambda_n: 1.0007475702033524 Loss: 2.57573220296358e-09\n",
      "Iteration: 7504 lambda_n: 1.0032001473290841 Loss: 2.5698093759301274e-09\n",
      "Iteration: 7505 lambda_n: 1.0056811916052508 Loss: 2.563899924843369e-09\n",
      "Iteration: 7506 lambda_n: 0.9976489313787413 Loss: 2.5580037624053054e-09\n",
      "Iteration: 7507 lambda_n: 0.9001020359334143 Loss: 2.5521823014289066e-09\n",
      "Iteration: 7508 lambda_n: 0.9072604374150867 Loss: 2.5469546258579476e-09\n",
      "Iteration: 7509 lambda_n: 0.9038130053592343 Loss: 2.5417076535781426e-09\n",
      "Iteration: 7510 lambda_n: 0.9534601727233554 Loss: 2.5365028681474387e-09\n",
      "Iteration: 7511 lambda_n: 0.9290892347971077 Loss: 2.531035487187846e-09\n",
      "Iteration: 7512 lambda_n: 0.9890999031189283 Loss: 2.5257316964174823e-09\n",
      "Iteration: 7513 lambda_n: 1.0393295841226402 Loss: 2.5201099616871227e-09\n",
      "Iteration: 7514 lambda_n: 0.926183176586039 Loss: 2.5142301684814375e-09\n",
      "Iteration: 7515 lambda_n: 0.9941686121953088 Loss: 2.509016037421505e-09\n",
      "Iteration: 7516 lambda_n: 1.0111851442130302 Loss: 2.5034435058127153e-09\n",
      "Iteration: 7517 lambda_n: 0.9864039008363013 Loss: 2.4978020589852304e-09\n",
      "Iteration: 7518 lambda_n: 0.9029044273522485 Loss: 2.492324986761821e-09\n",
      "Iteration: 7519 lambda_n: 0.9010201106485198 Loss: 2.4873347523783223e-09\n",
      "Iteration: 7520 lambda_n: 0.9602209541807816 Loss: 2.4823760541009003e-09\n",
      "Iteration: 7521 lambda_n: 1.0389892527956832 Loss: 2.4771139047826313e-09\n",
      "Iteration: 7522 lambda_n: 1.0200578250646481 Loss: 2.471445768250806e-09\n",
      "Iteration: 7523 lambda_n: 0.9120564120989251 Loss: 2.465908060357294e-09\n",
      "Iteration: 7524 lambda_n: 0.9644335008304521 Loss: 2.460980369516126e-09\n",
      "Iteration: 7525 lambda_n: 1.033474053396773 Loss: 2.4557920197914133e-09\n",
      "Iteration: 7526 lambda_n: 0.8984283485203169 Loss: 2.4502574378301034e-09\n",
      "Iteration: 7527 lambda_n: 0.8926946979866067 Loss: 2.445469398608234e-09\n",
      "Iteration: 7528 lambda_n: 1.0225133248646128 Loss: 2.4407319822343065e-09\n",
      "Iteration: 7529 lambda_n: 0.9337780209016632 Loss: 2.435328413168224e-09\n",
      "Iteration: 7530 lambda_n: 0.9861578729400504 Loss: 2.430417442913159e-09\n",
      "Iteration: 7531 lambda_n: 1.0015950560725988 Loss: 2.425253752666941e-09\n",
      "Iteration: 7532 lambda_n: 1.033558709698088 Loss: 2.4200335163463924e-09\n",
      "Iteration: 7533 lambda_n: 1.045030657959907 Loss: 2.4146720353391564e-09\n",
      "Iteration: 7534 lambda_n: 0.9258330712373255 Loss: 2.409277360256948e-09\n",
      "Iteration: 7535 lambda_n: 1.0071037332155004 Loss: 2.404521448434175e-09\n",
      "Iteration: 7536 lambda_n: 0.9584093996934823 Loss: 2.3993705657500527e-09\n",
      "Iteration: 7537 lambda_n: 0.9396170569610861 Loss: 2.3944919091329767e-09\n",
      "Iteration: 7538 lambda_n: 1.0083231042244112 Loss: 2.3897304394317216e-09\n",
      "Iteration: 7539 lambda_n: 0.9367858835797277 Loss: 2.38464336202637e-09\n",
      "Iteration: 7540 lambda_n: 1.0044874279616272 Loss: 2.379939570332379e-09\n",
      "Iteration: 7541 lambda_n: 1.038670182200562 Loss: 2.3749180455317094e-09\n",
      "Iteration: 7542 lambda_n: 0.9626955420506114 Loss: 2.3697501388218886e-09\n",
      "Iteration: 7543 lambda_n: 0.9449776828463401 Loss: 2.364983605861296e-09\n",
      "Iteration: 7544 lambda_n: 0.8948895002668048 Loss: 2.360325949469179e-09\n",
      "Iteration: 7545 lambda_n: 0.9719384914199555 Loss: 2.3559347493915925e-09\n",
      "Iteration: 7546 lambda_n: 0.8999949263579685 Loss: 2.3511855207644786e-09\n",
      "Iteration: 7547 lambda_n: 0.931213172590846 Loss: 2.3468079068782267e-09\n",
      "Iteration: 7548 lambda_n: 0.9463607393615284 Loss: 2.3422976095347584e-09\n",
      "Iteration: 7549 lambda_n: 0.9975760629866243 Loss: 2.337733999477085e-09\n",
      "Iteration: 7550 lambda_n: 0.9329812714002991 Loss: 2.33294480344321e-09\n",
      "Iteration: 7551 lambda_n: 0.9652489218687634 Loss: 2.3284867094173862e-09\n",
      "Iteration: 7552 lambda_n: 0.987048826024394 Loss: 2.3238946480795486e-09\n",
      "Iteration: 7553 lambda_n: 1.0384012068302366 Loss: 2.319220172303259e-09\n",
      "Iteration: 7554 lambda_n: 0.9747695093608607 Loss: 2.3143253286782017e-09\n",
      "Iteration: 7555 lambda_n: 1.0008935877882659 Loss: 2.3097528485298787e-09\n",
      "Iteration: 7556 lambda_n: 0.9680701210433057 Loss: 2.305079324210529e-09\n",
      "Iteration: 7557 lambda_n: 1.01325225818634 Loss: 2.3005803257383683e-09\n",
      "Iteration: 7558 lambda_n: 1.031523868920601 Loss: 2.295892773038481e-09\n",
      "Iteration: 7559 lambda_n: 0.9829886342153186 Loss: 2.291143423436964e-09\n",
      "Iteration: 7560 lambda_n: 0.9186629770986622 Loss: 2.2866394787929065e-09\n",
      "Iteration: 7561 lambda_n: 0.9458659489991273 Loss: 2.2824497019630965e-09\n",
      "Iteration: 7562 lambda_n: 0.9041018348520086 Loss: 2.2781544863955733e-09\n",
      "Iteration: 7563 lambda_n: 1.0152433285358866 Loss: 2.274067177977646e-09\n",
      "Iteration: 7564 lambda_n: 0.9249478643421236 Loss: 2.2694969357775342e-09\n",
      "Iteration: 7565 lambda_n: 0.98925852421845 Loss: 2.26535302630932e-09\n",
      "Iteration: 7566 lambda_n: 0.9313666687020183 Loss: 2.2609402801820863e-09\n",
      "Iteration: 7567 lambda_n: 0.9503420349166567 Loss: 2.2568050853991416e-09\n",
      "Iteration: 7568 lambda_n: 0.9643645695494379 Loss: 2.25260412115784e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7569 lambda_n: 0.8992414669151454 Loss: 2.2483602166263654e-09\n",
      "Iteration: 7570 lambda_n: 1.0401992784916954 Loss: 2.244420837138487e-09\n",
      "Iteration: 7571 lambda_n: 0.9756037946652981 Loss: 2.2398832440161284e-09\n",
      "Iteration: 7572 lambda_n: 0.8956709049950704 Loss: 2.235648243055922e-09\n",
      "Iteration: 7573 lambda_n: 1.03802112558163 Loss: 2.2317780427771893e-09\n",
      "Iteration: 7574 lambda_n: 1.0381021808168924 Loss: 2.2273116546190034e-09\n",
      "Iteration: 7575 lambda_n: 1.0042460173513303 Loss: 2.222866725048484e-09\n",
      "Iteration: 7576 lambda_n: 0.9412766140332756 Loss: 2.2185877541822554e-09\n",
      "Iteration: 7577 lambda_n: 0.9876646909719599 Loss: 2.21459602670426e-09\n",
      "Iteration: 7578 lambda_n: 0.9028076096601144 Loss: 2.2104261235155208e-09\n",
      "Iteration: 7579 lambda_n: 0.9903187993256753 Loss: 2.2066321815548837e-09\n",
      "Iteration: 7580 lambda_n: 0.9252430482021179 Loss: 2.20248817550307e-09\n",
      "Iteration: 7581 lambda_n: 0.9408617135457622 Loss: 2.1986345111992613e-09\n",
      "Iteration: 7582 lambda_n: 0.9712560243938129 Loss: 2.194732853169091e-09\n",
      "Iteration: 7583 lambda_n: 1.036301341057114 Loss: 2.1907229820903935e-09\n",
      "Iteration: 7584 lambda_n: 0.9461220541028607 Loss: 2.1864641252792504e-09\n",
      "Iteration: 7585 lambda_n: 0.915442972969787 Loss: 2.1825948271203455e-09\n",
      "Iteration: 7586 lambda_n: 0.9501171184279122 Loss: 2.1788676578107253e-09\n",
      "Iteration: 7587 lambda_n: 0.9281497607455539 Loss: 2.1750159837357446e-09\n",
      "Iteration: 7588 lambda_n: 0.9452080518083404 Loss: 2.1712701843440412e-09\n",
      "Iteration: 7589 lambda_n: 0.9085527368269131 Loss: 2.1674722085131324e-09\n",
      "Iteration: 7590 lambda_n: 0.9807504607572856 Loss: 2.163837756993998e-09\n",
      "Iteration: 7591 lambda_n: 0.9507122915324848 Loss: 2.159931279121711e-09\n",
      "Iteration: 7592 lambda_n: 1.0357406591898524 Loss: 2.1561619229919036e-09\n",
      "Iteration: 7593 lambda_n: 0.9910453829270147 Loss: 2.152073846359356e-09\n",
      "Iteration: 7594 lambda_n: 0.8986603887747183 Loss: 2.1481812413425076e-09\n",
      "Iteration: 7595 lambda_n: 0.909310666848816 Loss: 2.1446679655723843e-09\n",
      "Iteration: 7596 lambda_n: 0.9620090265306223 Loss: 2.1411281010973496e-09\n",
      "Iteration: 7597 lambda_n: 0.9645488546586751 Loss: 2.1373991226295115e-09\n",
      "Iteration: 7598 lambda_n: 1.0219524578690913 Loss: 2.1336772356966546e-09\n",
      "Iteration: 7599 lambda_n: 0.965054745331146 Loss: 2.1297517655276233e-09\n",
      "Iteration: 7600 lambda_n: 0.9136703480399572 Loss: 2.126062672796773e-09\n",
      "Iteration: 7601 lambda_n: 0.9832246475102309 Loss: 2.1225858789657716e-09\n",
      "Iteration: 7602 lambda_n: 0.9371186476294516 Loss: 2.118860525222021e-09\n",
      "Iteration: 7603 lambda_n: 0.9699447375367103 Loss: 2.115326297531234e-09\n",
      "Iteration: 7604 lambda_n: 0.9493703867363461 Loss: 2.1116844237033134e-09\n",
      "Iteration: 7605 lambda_n: 1.0380482410386385 Loss: 2.108136087293686e-09\n",
      "Iteration: 7606 lambda_n: 0.9266767912719692 Loss: 2.104273673045835e-09\n",
      "Iteration: 7607 lambda_n: 0.9840723284487785 Loss: 2.1008425120499767e-09\n",
      "Iteration: 7608 lambda_n: 1.0013997861341302 Loss: 2.0972147457338796e-09\n",
      "Iteration: 7609 lambda_n: 0.9879460122847192 Loss: 2.0935402180438005e-09\n",
      "Iteration: 7610 lambda_n: 0.942579073327071 Loss: 2.089932175267862e-09\n",
      "Iteration: 7611 lambda_n: 0.9983397212270746 Loss: 2.086505833580289e-09\n",
      "Iteration: 7612 lambda_n: 0.9588457255788588 Loss: 2.0828929284642966e-09\n",
      "Iteration: 7613 lambda_n: 0.9276075226286764 Loss: 2.0794392815600454e-09\n",
      "Iteration: 7614 lambda_n: 0.9480571846220601 Loss: 2.0761132442579574e-09\n",
      "Iteration: 7615 lambda_n: 0.9780702824485896 Loss: 2.0727287496549256e-09\n",
      "Iteration: 7616 lambda_n: 0.9385353083517387 Loss: 2.0692527196429373e-09\n",
      "Iteration: 7617 lambda_n: 0.9438648992671298 Loss: 2.065932570438777e-09\n",
      "Iteration: 7618 lambda_n: 0.9142658556770112 Loss: 2.062608348264779e-09\n",
      "Iteration: 7619 lambda_n: 0.955154885951927 Loss: 2.0594027025514884e-09\n",
      "Iteration: 7620 lambda_n: 0.9045663189550253 Loss: 2.056068128497296e-09\n",
      "Iteration: 7621 lambda_n: 1.0122917118387427 Loss: 2.052924395466775e-09\n",
      "Iteration: 7622 lambda_n: 1.029486469182073 Loss: 2.0494212950021157e-09\n",
      "Iteration: 7623 lambda_n: 0.9665492144056536 Loss: 2.0458756990601965e-09\n",
      "Iteration: 7624 lambda_n: 0.9374917374530687 Loss: 2.0425630282798205e-09\n",
      "Iteration: 7625 lambda_n: 1.0225696304543221 Loss: 2.0393645944468384e-09\n",
      "Iteration: 7626 lambda_n: 0.9098591070910275 Loss: 2.0358913426803686e-09\n",
      "Iteration: 7627 lambda_n: 0.9533597556224065 Loss: 2.032815824409399e-09\n",
      "Iteration: 7628 lambda_n: 0.9340455066645797 Loss: 2.0296071108634857e-09\n",
      "Iteration: 7629 lambda_n: 0.9264267084868693 Loss: 2.0264775456424215e-09\n",
      "Iteration: 7630 lambda_n: 0.9671499927985167 Loss: 2.0233871895369795e-09\n",
      "Iteration: 7631 lambda_n: 1.0097351831131671 Loss: 2.0201751071262293e-09\n",
      "Iteration: 7632 lambda_n: 1.0199375336002117 Loss: 2.0168369111313005e-09\n",
      "Iteration: 7633 lambda_n: 0.8998492292821814 Loss: 2.0134810596479235e-09\n",
      "Iteration: 7634 lambda_n: 0.9733723262462944 Loss: 2.0105345775879374e-09\n",
      "Iteration: 7635 lambda_n: 1.0489367941618493 Loss: 2.0073609006182156e-09\n",
      "Iteration: 7636 lambda_n: 1.0101843534211958 Loss: 2.003956582831812e-09\n",
      "Iteration: 7637 lambda_n: 0.9286734410032123 Loss: 2.000694264965367e-09\n",
      "Iteration: 7638 lambda_n: 0.9162291297562908 Loss: 1.9977094820623162e-09\n",
      "Iteration: 7639 lambda_n: 0.9690827376791539 Loss: 1.9947776138583318e-09\n",
      "Iteration: 7640 lambda_n: 0.9719840011164773 Loss: 1.9916900556997668e-09\n",
      "Iteration: 7641 lambda_n: 0.9634254402664024 Loss: 1.9886074316955082e-09\n",
      "Iteration: 7642 lambda_n: 1.0430342659848801 Loss: 1.98556598976026e-09\n",
      "Iteration: 7643 lambda_n: 1.02910325306231 Loss: 1.9822882310174023e-09\n",
      "Iteration: 7644 lambda_n: 0.9522933123565013 Loss: 1.9790701912497476e-09\n",
      "Iteration: 7645 lambda_n: 1.017758708845776 Loss: 1.976106817706145e-09\n",
      "Iteration: 7646 lambda_n: 1.0153883430867576 Loss: 1.9729540000187347e-09\n",
      "Iteration: 7647 lambda_n: 1.0209147841942048 Loss: 1.9698236598395335e-09\n",
      "Iteration: 7648 lambda_n: 1.0487717689578282 Loss: 1.9666913930604477e-09\n",
      "Iteration: 7649 lambda_n: 0.9127694286522573 Loss: 1.963489195492539e-09\n",
      "Iteration: 7650 lambda_n: 0.9691530655019588 Loss: 1.960716065184574e-09\n",
      "Iteration: 7651 lambda_n: 1.041336399534996 Loss: 1.9577843485248793e-09\n",
      "Iteration: 7652 lambda_n: 0.9176488529260763 Loss: 1.954648731765278e-09\n",
      "Iteration: 7653 lambda_n: 1.0429186738299274 Loss: 1.9518991535267242e-09\n",
      "Iteration: 7654 lambda_n: 0.9074717917247503 Loss: 1.9487878144224717e-09\n",
      "Iteration: 7655 lambda_n: 0.988369428760466 Loss: 1.94609390524632e-09\n",
      "Iteration: 7656 lambda_n: 1.044853154324653 Loss: 1.943172455194e-09\n",
      "Iteration: 7657 lambda_n: 0.9414128310490297 Loss: 1.940098499144124e-09\n",
      "Iteration: 7658 lambda_n: 1.028695071373256 Loss: 1.937342556893574e-09\n",
      "Iteration: 7659 lambda_n: 0.9686733406446663 Loss: 1.934344542187995e-09\n",
      "Iteration: 7660 lambda_n: 0.9438035873026042 Loss: 1.9315351898191266e-09\n",
      "Iteration: 7661 lambda_n: 0.9007633984706701 Loss: 1.928810532301493e-09\n",
      "Iteration: 7662 lambda_n: 0.9079048773932644 Loss: 1.9262217430045457e-09\n",
      "Iteration: 7663 lambda_n: 0.8949676037760969 Loss: 1.9236235579922475e-09\n",
      "Iteration: 7664 lambda_n: 1.0394735725991964 Loss: 1.9210734092917456e-09\n",
      "Iteration: 7665 lambda_n: 1.036711290455705 Loss: 1.9181240915583216e-09\n",
      "Iteration: 7666 lambda_n: 0.9726654630492796 Loss: 1.9151970951977633e-09\n",
      "Iteration: 7667 lambda_n: 0.9491124537297919 Loss: 1.9124644111763764e-09\n",
      "Iteration: 7668 lambda_n: 0.9180924429539636 Loss: 1.909810190496094e-09\n",
      "Iteration: 7669 lambda_n: 1.0199357721167597 Loss: 1.907254274203356e-09\n",
      "Iteration: 7670 lambda_n: 1.0101626346484278 Loss: 1.904427197806439e-09\n",
      "Iteration: 7671 lambda_n: 0.9654979550224324 Loss: 1.9016407573084595e-09\n",
      "Iteration: 7672 lambda_n: 0.9799568486772778 Loss: 1.8989902731195575e-09\n",
      "Iteration: 7673 lambda_n: 0.9531759335728206 Loss: 1.8963124198358816e-09\n",
      "Iteration: 7674 lambda_n: 0.9938147882767345 Loss: 1.8937198547800823e-09\n",
      "Iteration: 7675 lambda_n: 1.0067261949012094 Loss: 1.891028983860616e-09\n",
      "Iteration: 7676 lambda_n: 1.0356079224338968 Loss: 1.8883160105190953e-09\n",
      "Iteration: 7677 lambda_n: 0.9265823839013675 Loss: 1.8855385493291317e-09\n",
      "Iteration: 7678 lambda_n: 0.9113991706841424 Loss: 1.8830656930294326e-09\n",
      "Iteration: 7679 lambda_n: 0.950046801462661 Loss: 1.880644052262051e-09\n",
      "Iteration: 7680 lambda_n: 0.9219392279882507 Loss: 1.878130660720532e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7681 lambda_n: 0.9468273727607472 Loss: 1.875702629371377e-09\n",
      "Iteration: 7682 lambda_n: 0.9170553025954888 Loss: 1.873219967562456e-09\n",
      "Iteration: 7683 lambda_n: 0.9304789992100652 Loss: 1.870826185750352e-09\n",
      "Iteration: 7684 lambda_n: 0.9009263285494058 Loss: 1.8684079466219304e-09\n",
      "Iteration: 7685 lambda_n: 0.9284011044910792 Loss: 1.8660768666575523e-09\n",
      "Iteration: 7686 lambda_n: 0.9874316075379248 Loss: 1.863684992022222e-09\n",
      "Iteration: 7687 lambda_n: 0.9322615218725364 Loss: 1.8611522678037078e-09\n",
      "Iteration: 7688 lambda_n: 0.9756831424933962 Loss: 1.858772271814188e-09\n",
      "Iteration: 7689 lambda_n: 0.9721769831505116 Loss: 1.8562924626567333e-09\n",
      "Iteration: 7690 lambda_n: 0.9635392156055522 Loss: 1.8538330368445317e-09\n",
      "Iteration: 7691 lambda_n: 1.0263503853073253 Loss: 1.851406729057744e-09\n",
      "Iteration: 7692 lambda_n: 0.9772800212561649 Loss: 1.8488341124405348e-09\n",
      "Iteration: 7693 lambda_n: 1.0383270069254507 Loss: 1.846396444683112e-09\n",
      "Iteration: 7694 lambda_n: 0.9633402013612051 Loss: 1.8438185621933355e-09\n",
      "Iteration: 7695 lambda_n: 1.0407149801419888 Loss: 1.8414386681626504e-09\n",
      "Iteration: 7696 lambda_n: 0.9115288753719066 Loss: 1.8388794234849192e-09\n",
      "Iteration: 7697 lambda_n: 0.9365063285957299 Loss: 1.836648950619872e-09\n",
      "Iteration: 7698 lambda_n: 0.9608195455453871 Loss: 1.8343673089174567e-09\n",
      "Iteration: 7699 lambda_n: 0.9595534058300844 Loss: 1.8320368832904688e-09\n",
      "Iteration: 7700 lambda_n: 0.9330514427439529 Loss: 1.82972017807121e-09\n",
      "Iteration: 7701 lambda_n: 0.9986997702208623 Loss: 1.8274777501622402e-09\n",
      "Iteration: 7702 lambda_n: 0.9557957897845085 Loss: 1.8250882399356985e-09\n",
      "Iteration: 7703 lambda_n: 0.9571076807345489 Loss: 1.8228122540552612e-09\n",
      "Iteration: 7704 lambda_n: 1.0408176784259664 Loss: 1.8205435434549581e-09\n",
      "Iteration: 7705 lambda_n: 1.0203264419297502 Loss: 1.81808767010719e-09\n",
      "Iteration: 7706 lambda_n: 0.8951810988834663 Loss: 1.8156920966756786e-09\n",
      "Iteration: 7707 lambda_n: 0.9832681349056228 Loss: 1.813600558255815e-09\n",
      "Iteration: 7708 lambda_n: 0.9061841988643807 Loss: 1.811313031515082e-09\n",
      "Iteration: 7709 lambda_n: 0.9178153490666556 Loss: 1.8092147233611617e-09\n",
      "Iteration: 7710 lambda_n: 1.0428868926719916 Loss: 1.8070986758374644e-09\n",
      "Iteration: 7711 lambda_n: 1.0188302576439603 Loss: 1.804704820997036e-09\n",
      "Iteration: 7712 lambda_n: 0.912762541734423 Loss: 1.802377834762655e-09\n",
      "Iteration: 7713 lambda_n: 0.9191167569892326 Loss: 1.8003032361846315e-09\n",
      "Iteration: 7714 lambda_n: 1.0208433643416805 Loss: 1.7982232975447212e-09\n",
      "Iteration: 7715 lambda_n: 0.9864030703068652 Loss: 1.7959233155028842e-09\n",
      "Iteration: 7716 lambda_n: 0.9070827118057407 Loss: 1.7937117659867585e-09\n",
      "Iteration: 7717 lambda_n: 0.8972008170574476 Loss: 1.791687632711616e-09\n",
      "Iteration: 7718 lambda_n: 1.0057744244104736 Loss: 1.7896942332497756e-09\n",
      "Iteration: 7719 lambda_n: 0.9100775147837646 Loss: 1.7874692050764111e-09\n",
      "Iteration: 7720 lambda_n: 0.9546280306674317 Loss: 1.785465551694049e-09\n",
      "Iteration: 7721 lambda_n: 0.9332075877673401 Loss: 1.7833729664880407e-09\n",
      "Iteration: 7722 lambda_n: 1.0047869655332848 Loss: 1.7813366862010263e-09\n",
      "Iteration: 7723 lambda_n: 0.9735252060472012 Loss: 1.7791540205252956e-09\n",
      "Iteration: 7724 lambda_n: 0.9215325581658845 Loss: 1.7770494309184874e-09\n",
      "Iteration: 7725 lambda_n: 0.8972038106770109 Loss: 1.7750665083327154e-09\n",
      "Iteration: 7726 lambda_n: 0.994695801828599 Loss: 1.7731444547832136e-09\n",
      "Iteration: 7727 lambda_n: 0.9735520523859834 Loss: 1.7710227229801633e-09\n",
      "Iteration: 7728 lambda_n: 0.9175947871167268 Loss: 1.7689559806170223e-09\n",
      "Iteration: 7729 lambda_n: 0.986239265097994 Loss: 1.7670171064129202e-09\n",
      "Iteration: 7730 lambda_n: 1.0204310389272495 Loss: 1.7649423690246378e-09\n",
      "Iteration: 7731 lambda_n: 1.0258486078830682 Loss: 1.762805851435202e-09\n",
      "Iteration: 7732 lambda_n: 0.9639222232457166 Loss: 1.7606685032462505e-09\n",
      "Iteration: 7733 lambda_n: 0.9973423490575188 Loss: 1.7586700519075985e-09\n",
      "Iteration: 7734 lambda_n: 0.9303519153594251 Loss: 1.7566118745762939e-09\n",
      "Iteration: 7735 lambda_n: 0.9391562097816384 Loss: 1.7547011203124677e-09\n",
      "Iteration: 7736 lambda_n: 0.9077017896252167 Loss: 1.7527809033217537e-09\n",
      "Iteration: 7737 lambda_n: 1.0102886782368723 Loss: 1.750933359929759e-09\n",
      "Iteration: 7738 lambda_n: 0.922950883612084 Loss: 1.7488859895315536e-09\n",
      "Iteration: 7739 lambda_n: 0.9074443484184086 Loss: 1.7470246790607046e-09\n",
      "Iteration: 7740 lambda_n: 0.9010484226898616 Loss: 1.745202747966567e-09\n",
      "Iteration: 7741 lambda_n: 0.9403784908560889 Loss: 1.7434015505927726e-09\n",
      "Iteration: 7742 lambda_n: 0.9363326078983762 Loss: 1.7415298770637826e-09\n",
      "Iteration: 7743 lambda_n: 0.9928140775839792 Loss: 1.7396746831022607e-09\n",
      "Iteration: 7744 lambda_n: 0.9337934195078996 Loss: 1.7377164434821317e-09\n",
      "Iteration: 7745 lambda_n: 1.0141573802609822 Loss: 1.735883413455305e-09\n",
      "Iteration: 7746 lambda_n: 1.01973630411626 Loss: 1.733901577953701e-09\n",
      "Iteration: 7747 lambda_n: 0.9455152068466565 Loss: 1.7319185703659472e-09\n",
      "Iteration: 7748 lambda_n: 0.9980188139442964 Loss: 1.730088913231338e-09\n",
      "Iteration: 7749 lambda_n: 0.9689022620735377 Loss: 1.728166450003781e-09\n",
      "Iteration: 7750 lambda_n: 1.0440194911150193 Loss: 1.7263090422794437e-09\n",
      "Iteration: 7751 lambda_n: 0.9253623256519663 Loss: 1.724316987196027e-09\n",
      "Iteration: 7752 lambda_n: 1.0128398890749564 Loss: 1.7225602083243027e-09\n",
      "Iteration: 7753 lambda_n: 1.032637519671661 Loss: 1.7206459451298064e-09\n",
      "Iteration: 7754 lambda_n: 1.0444094976588125 Loss: 1.7187037935130938e-09\n",
      "Iteration: 7755 lambda_n: 0.9497043872323913 Loss: 1.7167492851887774e-09\n",
      "Iteration: 7756 lambda_n: 0.9323022818918529 Loss: 1.71498096170179e-09\n",
      "Iteration: 7757 lambda_n: 1.0297211347332331 Loss: 1.7132529897090927e-09\n",
      "Iteration: 7758 lambda_n: 0.9501311391168119 Loss: 1.7113530679982735e-09\n",
      "Iteration: 7759 lambda_n: 0.911861976912587 Loss: 1.7096087009906485e-09\n",
      "Iteration: 7760 lambda_n: 1.0193523159143807 Loss: 1.7079422692898646e-09\n",
      "Iteration: 7761 lambda_n: 0.9509932570755338 Loss: 1.7060876226146077e-09\n",
      "Iteration: 7762 lambda_n: 0.9872144417095153 Loss: 1.7043658617276736e-09\n",
      "Iteration: 7763 lambda_n: 0.894791896073747 Loss: 1.7025867368904494e-09\n",
      "Iteration: 7764 lambda_n: 0.9558766918640196 Loss: 1.7009818675522714e-09\n",
      "Iteration: 7765 lambda_n: 0.9361431353267822 Loss: 1.6992748701951208e-09\n",
      "Iteration: 7766 lambda_n: 1.0421562806056648 Loss: 1.6976108431337782e-09\n",
      "Iteration: 7767 lambda_n: 1.0435577643729907 Loss: 1.6957667830652075e-09\n",
      "Iteration: 7768 lambda_n: 1.0305006599844801 Loss: 1.6939295642542785e-09\n",
      "Iteration: 7769 lambda_n: 0.9685248120701684 Loss: 1.692124497925578e-09\n",
      "Iteration: 7770 lambda_n: 1.0275858090899366 Loss: 1.6904364498600416e-09\n",
      "Iteration: 7771 lambda_n: 0.9239685617833181 Loss: 1.6886538715063073e-09\n",
      "Iteration: 7772 lambda_n: 0.9608989122659868 Loss: 1.6870590178780716e-09\n",
      "Iteration: 7773 lambda_n: 1.022666432979827 Loss: 1.6854078555503401e-09\n",
      "Iteration: 7774 lambda_n: 0.9097681331588985 Loss: 1.6836587591810898e-09\n",
      "Iteration: 7775 lambda_n: 0.9721689063091904 Loss: 1.68211046438966e-09\n",
      "Iteration: 7776 lambda_n: 0.981185836846967 Loss: 1.680463281366164e-09\n",
      "Iteration: 7777 lambda_n: 0.9062691157398705 Loss: 1.6788086743533037e-09\n",
      "Iteration: 7778 lambda_n: 0.9207102610234216 Loss: 1.6772876781588563e-09\n",
      "Iteration: 7779 lambda_n: 0.9052009762182972 Loss: 1.6757492434935852e-09\n",
      "Iteration: 7780 lambda_n: 1.0094991489324834 Loss: 1.6742434935922016e-09\n",
      "Iteration: 7781 lambda_n: 1.0482699466907008 Loss: 1.6725716581335332e-09\n",
      "Iteration: 7782 lambda_n: 0.895915945733384 Loss: 1.6708441449349844e-09\n",
      "Iteration: 7783 lambda_n: 0.9761979728281267 Loss: 1.6693752124815136e-09\n",
      "Iteration: 7784 lambda_n: 1.0262637871630111 Loss: 1.6677816491053201e-09\n",
      "Iteration: 7785 lambda_n: 1.0386496875377382 Loss: 1.6661143371601809e-09\n",
      "Iteration: 7786 lambda_n: 0.9955026973608339 Loss: 1.6644353275861378e-09\n",
      "Iteration: 7787 lambda_n: 0.9324596773397663 Loss: 1.6628342048567484e-09\n",
      "Iteration: 7788 lambda_n: 0.9422320300739809 Loss: 1.661341744075535e-09\n",
      "Iteration: 7789 lambda_n: 1.0364066117602284 Loss: 1.6598405018775098e-09\n",
      "Iteration: 7790 lambda_n: 1.0064264316203808 Loss: 1.6581968190124074e-09\n",
      "Iteration: 7791 lambda_n: 0.9856660488434538 Loss: 1.6566087525712398e-09\n",
      "Iteration: 7792 lambda_n: 0.9486884734914547 Loss: 1.6550610800408645e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7793 lambda_n: 1.0022691422114627 Loss: 1.653578632398709e-09\n",
      "Iteration: 7794 lambda_n: 0.993608525744328 Loss: 1.6520197223551954e-09\n",
      "Iteration: 7795 lambda_n: 0.9692188275283956 Loss: 1.6504818477919562e-09\n",
      "Iteration: 7796 lambda_n: 0.9201228742318188 Loss: 1.648989010925954e-09\n",
      "Iteration: 7797 lambda_n: 0.9585921745943926 Loss: 1.6475785032328619e-09\n",
      "Iteration: 7798 lambda_n: 0.8978568851834758 Loss: 1.6461156458793087e-09\n",
      "Iteration: 7799 lambda_n: 1.0262291431611719 Loss: 1.644751894066663e-09\n",
      "Iteration: 7800 lambda_n: 0.9254557158606224 Loss: 1.6432000355337291e-09\n",
      "Iteration: 7801 lambda_n: 0.9728583020270795 Loss: 1.6418075768049358e-09\n",
      "Iteration: 7802 lambda_n: 0.9753009095227597 Loss: 1.6403504372115189e-09\n",
      "Iteration: 7803 lambda_n: 1.028183835798779 Loss: 1.6388966138202338e-09\n",
      "Iteration: 7804 lambda_n: 0.9499230602673081 Loss: 1.6373712965292315e-09\n",
      "Iteration: 7805 lambda_n: 1.0449713174081539 Loss: 1.6359691734049483e-09\n",
      "Iteration: 7806 lambda_n: 1.0470926587002705 Loss: 1.6344339652794213e-09\n",
      "Iteration: 7807 lambda_n: 0.9039016006068855 Loss: 1.6329035349358744e-09\n",
      "Iteration: 7808 lambda_n: 0.9588325718031391 Loss: 1.6315891593983464e-09\n",
      "Iteration: 7809 lambda_n: 0.9472463749552247 Loss: 1.6302011135499624e-09\n",
      "Iteration: 7810 lambda_n: 0.9728221244086012 Loss: 1.6288363040716218e-09\n",
      "Iteration: 7811 lambda_n: 1.0420298004215118 Loss: 1.6274411785372111e-09\n",
      "Iteration: 7812 lambda_n: 0.9364711925475636 Loss: 1.6259539669406035e-09\n",
      "Iteration: 7813 lambda_n: 0.9452412844567004 Loss: 1.6246242461849636e-09\n",
      "Iteration: 7814 lambda_n: 1.0311347458972628 Loss: 1.6232882594020936e-09\n",
      "Iteration: 7815 lambda_n: 0.9144824227725337 Loss: 1.6218376757022608e-09\n",
      "Iteration: 7816 lambda_n: 0.9421621320511124 Loss: 1.6205577118801259e-09\n",
      "Iteration: 7817 lambda_n: 1.0113291431895832 Loss: 1.6192449593437737e-09\n",
      "Iteration: 7818 lambda_n: 0.948565251096499 Loss: 1.6178423927972719e-09\n",
      "Iteration: 7819 lambda_n: 1.0236393376523234 Loss: 1.6165334284814093e-09\n",
      "Iteration: 7820 lambda_n: 0.8939812345131376 Loss: 1.6151275008829638e-09\n",
      "Iteration: 7821 lambda_n: 0.9494457088054425 Loss: 1.613905839553959e-09\n",
      "Iteration: 7822 lambda_n: 1.0358332397648995 Loss: 1.612614120170135e-09\n",
      "Iteration: 7823 lambda_n: 0.9707500642803679 Loss: 1.6112114956828374e-09\n",
      "Iteration: 7824 lambda_n: 0.9775146342517458 Loss: 1.6099037262095214e-09\n",
      "Iteration: 7825 lambda_n: 0.9975753888448369 Loss: 1.6085931711598298e-09\n",
      "Iteration: 7826 lambda_n: 0.9267351768284182 Loss: 1.6072621920917767e-09\n",
      "Iteration: 7827 lambda_n: 0.9810502331495322 Loss: 1.6060318263085617e-09\n",
      "Iteration: 7828 lambda_n: 1.0476103481653884 Loss: 1.6047353356870765e-09\n",
      "Iteration: 7829 lambda_n: 0.9245617115496355 Loss: 1.6033576386745574e-09\n",
      "Iteration: 7830 lambda_n: 1.0403403428143645 Loss: 1.602148046672895e-09\n",
      "Iteration: 7831 lambda_n: 0.9234542914030274 Loss: 1.6007932503427593e-09\n",
      "Iteration: 7832 lambda_n: 0.9812966758026007 Loss: 1.5995968672101492e-09\n",
      "Iteration: 7833 lambda_n: 0.997763082934424 Loss: 1.5983313830585145e-09\n",
      "Iteration: 7834 lambda_n: 1.0308544612392194 Loss: 1.597050934112031e-09\n",
      "Iteration: 7835 lambda_n: 0.9596666868970372 Loss: 1.5957345935678556e-09\n",
      "Iteration: 7836 lambda_n: 1.0223581079132225 Loss: 1.5945154229690321e-09\n",
      "Iteration: 7837 lambda_n: 1.0170279485316625 Loss: 1.5932228197220824e-09\n",
      "Iteration: 7838 lambda_n: 1.0329804390100026 Loss: 1.5919435010883766e-09\n",
      "Iteration: 7839 lambda_n: 1.02835510514246 Loss: 1.5906507060212308e-09\n",
      "Iteration: 7840 lambda_n: 0.9495350088737796 Loss: 1.5893703244923617e-09\n",
      "Iteration: 7841 lambda_n: 1.0189315594253574 Loss: 1.5881941261564005e-09\n",
      "Iteration: 7842 lambda_n: 1.0144593333979173 Loss: 1.5869379501780708e-09\n",
      "Iteration: 7843 lambda_n: 0.9571167067903006 Loss: 1.5856936539199023e-09\n",
      "Iteration: 7844 lambda_n: 0.9639642745245394 Loss: 1.5845256200417137e-09\n",
      "Iteration: 7845 lambda_n: 0.9443177520434187 Loss: 1.583354863020231e-09\n",
      "Iteration: 7846 lambda_n: 0.9191272092150586 Loss: 1.582213483810741e-09\n",
      "Iteration: 7847 lambda_n: 1.0078562544858345 Loss: 1.581107803094788e-09\n",
      "Iteration: 7848 lambda_n: 1.0154035066588547 Loss: 1.5799009726639489e-09\n",
      "Iteration: 7849 lambda_n: 0.984747831415898 Loss: 1.5786912314721447e-09\n",
      "Iteration: 7850 lambda_n: 0.9033169885099127 Loss: 1.5775239851752577e-09\n",
      "Iteration: 7851 lambda_n: 0.9247307770293357 Loss: 1.57645852141507e-09\n",
      "Iteration: 7852 lambda_n: 0.9356539904554811 Loss: 1.5753727520760413e-09\n",
      "Iteration: 7853 lambda_n: 0.9900648764347362 Loss: 1.5742792500509715e-09\n",
      "Iteration: 7854 lambda_n: 1.0439689956860072 Loss: 1.5731276015716063e-09\n",
      "Iteration: 7855 lambda_n: 1.0415889167312533 Loss: 1.5719193096158816e-09\n",
      "Iteration: 7856 lambda_n: 0.9042842279086073 Loss: 1.5707200871145654e-09\n",
      "Iteration: 7857 lambda_n: 0.9771338560161424 Loss: 1.5696843811560655e-09\n",
      "Iteration: 7858 lambda_n: 0.9746997221462227 Loss: 1.5685703357506228e-09\n",
      "Iteration: 7859 lambda_n: 1.0362258866460055 Loss: 1.5674645298922146e-09\n",
      "Iteration: 7860 lambda_n: 0.9143262199995568 Loss: 1.5662947073452847e-09\n",
      "Iteration: 7861 lambda_n: 0.8921529665386735 Loss: 1.565267865619655e-09\n",
      "Iteration: 7862 lambda_n: 0.9903396041060493 Loss: 1.564270538880492e-09\n",
      "Iteration: 7863 lambda_n: 0.9902688688439154 Loss: 1.5631684586164456e-09\n",
      "Iteration: 7864 lambda_n: 0.9234332450033381 Loss: 1.5620719632999572e-09\n",
      "Iteration: 7865 lambda_n: 0.997724324495161 Loss: 1.5610545732660716e-09\n",
      "Iteration: 7866 lambda_n: 1.0296049048985618 Loss: 1.5599604765048829e-09\n",
      "Iteration: 7867 lambda_n: 0.993054624418177 Loss: 1.5588371140510828e-09\n",
      "Iteration: 7868 lambda_n: 0.9300025786458341 Loss: 1.5577592692458901e-09\n",
      "Iteration: 7869 lambda_n: 0.9064518157693808 Loss: 1.5567549113805766e-09\n",
      "Iteration: 7870 lambda_n: 0.9753969191153905 Loss: 1.5557805929169918e-09\n",
      "Iteration: 7871 lambda_n: 0.9049377298136552 Loss: 1.5547370084466438e-09\n",
      "Iteration: 7872 lambda_n: 0.9962845653700462 Loss: 1.553773572662688e-09\n",
      "Iteration: 7873 lambda_n: 1.0467704401993863 Loss: 1.552717780937268e-09\n",
      "Iteration: 7874 lambda_n: 0.931987302117627 Loss: 1.5516141071998692e-09\n",
      "Iteration: 7875 lambda_n: 1.0294931968877157 Loss: 1.5506366534762171e-09\n",
      "Iteration: 7876 lambda_n: 1.0028619184637608 Loss: 1.5495620737856995e-09\n",
      "Iteration: 7877 lambda_n: 1.0452985767870258 Loss: 1.5485207533321695e-09\n",
      "Iteration: 7878 lambda_n: 0.9427263983282939 Loss: 1.5474409271086419e-09\n",
      "Iteration: 7879 lambda_n: 1.044745078097098 Loss: 1.5464722157082816e-09\n",
      "Iteration: 7880 lambda_n: 0.9725077770376759 Loss: 1.545403848730736e-09\n",
      "Iteration: 7881 lambda_n: 0.9597948634964539 Loss: 1.5444146400562866e-09\n",
      "Iteration: 7882 lambda_n: 0.9158025005430103 Loss: 1.5434431979767083e-09\n",
      "Iteration: 7883 lambda_n: 0.9509776124617476 Loss: 1.5425208060331485e-09\n",
      "Iteration: 7884 lambda_n: 0.9505715211977158 Loss: 1.5415674755783176e-09\n",
      "Iteration: 7885 lambda_n: 0.9979959320343601 Loss: 1.540619181396986e-09\n",
      "Iteration: 7886 lambda_n: 1.0217140882430291 Loss: 1.5396284333545015e-09\n",
      "Iteration: 7887 lambda_n: 0.9334808692891193 Loss: 1.5386193114865445e-09\n",
      "Iteration: 7888 lambda_n: 0.9787792598006164 Loss: 1.5377021362664525e-09\n",
      "Iteration: 7889 lambda_n: 0.921646619953572 Loss: 1.536745067245943e-09\n",
      "Iteration: 7890 lambda_n: 1.0149053280611147 Loss: 1.535848366959252e-09\n",
      "Iteration: 7891 lambda_n: 1.0200319825208048 Loss: 1.53486561855282e-09\n",
      "Iteration: 7892 lambda_n: 1.0181361986459014 Loss: 1.5338830452058432e-09\n",
      "Iteration: 7893 lambda_n: 1.048012019520748 Loss: 1.5329074388780199e-09\n",
      "Iteration: 7894 lambda_n: 0.9277416357980386 Loss: 1.5319084646126456e-09\n",
      "Iteration: 7895 lambda_n: 0.9286680430651092 Loss: 1.5310288708498743e-09\n",
      "Iteration: 7896 lambda_n: 1.040372193140614 Loss: 1.5301525996779012e-09\n",
      "Iteration: 7897 lambda_n: 0.985798943924046 Loss: 1.5291756413872726e-09\n",
      "Iteration: 7898 lambda_n: 0.9357634352012599 Loss: 1.5282548784755928e-09\n",
      "Iteration: 7899 lambda_n: 0.9698616149449036 Loss: 1.5273852818144574e-09\n",
      "Iteration: 7900 lambda_n: 0.8928988674783959 Loss: 1.526488354818555e-09\n",
      "Iteration: 7901 lambda_n: 0.9993063706984411 Loss: 1.5256667251785173e-09\n",
      "Iteration: 7902 lambda_n: 0.9763594620829961 Loss: 1.5247514412359298e-09\n",
      "Iteration: 7903 lambda_n: 0.9517204644286915 Loss: 1.523861785102032e-09\n",
      "Iteration: 7904 lambda_n: 0.9838814066323048 Loss: 1.5229989454109778e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7905 lambda_n: 0.9723984708944542 Loss: 1.522111349429385e-09\n",
      "Iteration: 7906 lambda_n: 0.9079937275302006 Loss: 1.5212385770617896e-09\n",
      "Iteration: 7907 lambda_n: 0.9013700576996622 Loss: 1.5204277027208706e-09\n",
      "Iteration: 7908 lambda_n: 0.9465433623357763 Loss: 1.519626532267598e-09\n",
      "Iteration: 7909 lambda_n: 0.9554688882490864 Loss: 1.5187891474430886e-09\n",
      "Iteration: 7910 lambda_n: 0.9617557719486077 Loss: 1.5179480280432894e-09\n",
      "Iteration: 7911 lambda_n: 0.9654968955290991 Loss: 1.5171055739628081e-09\n",
      "Iteration: 7912 lambda_n: 0.9270297184827345 Loss: 1.5162640738012475e-09\n",
      "Iteration: 7913 lambda_n: 0.8976660905012714 Loss: 1.515460136844584e-09\n",
      "Iteration: 7914 lambda_n: 0.9629911197449218 Loss: 1.5146854120781357e-09\n",
      "Iteration: 7915 lambda_n: 0.9829973031883159 Loss: 1.5138582124677083e-09\n",
      "Iteration: 7916 lambda_n: 1.0424307906592982 Loss: 1.5130180679833472e-09\n",
      "Iteration: 7917 lambda_n: 0.8960578027320667 Loss: 1.5121317085266284e-09\n",
      "Iteration: 7918 lambda_n: 1.013638988895207 Loss: 1.5113739166484484e-09\n",
      "Iteration: 7919 lambda_n: 0.9612760394956233 Loss: 1.5105207234602112e-09\n",
      "Iteration: 7920 lambda_n: 0.9224498342119509 Loss: 1.509715879807212e-09\n",
      "Iteration: 7921 lambda_n: 0.9319445179039569 Loss: 1.5089474130262424e-09\n",
      "Iteration: 7922 lambda_n: 1.0448424893705122 Loss: 1.5081747931597747e-09\n",
      "Iteration: 7923 lambda_n: 0.9272423748588582 Loss: 1.5073128203046289e-09\n",
      "Iteration: 7924 lambda_n: 0.9442621316454979 Loss: 1.5065520265019188e-09\n",
      "Iteration: 7925 lambda_n: 0.9705325304327941 Loss: 1.5057810302708146e-09\n",
      "Iteration: 7926 lambda_n: 0.9423928665964992 Loss: 1.5049925148985712e-09\n",
      "Iteration: 7927 lambda_n: 0.9378700490120161 Loss: 1.5042307611854908e-09\n",
      "Iteration: 7928 lambda_n: 0.9276288248794 Loss: 1.503476407962672e-09\n",
      "Iteration: 7929 lambda_n: 0.9943724834518965 Loss: 1.5027339687749996e-09\n",
      "Iteration: 7930 lambda_n: 0.9637859086730467 Loss: 1.5019420042710353e-09\n",
      "Iteration: 7931 lambda_n: 0.9941203331681105 Loss: 1.5011784116558178e-09\n",
      "Iteration: 7932 lambda_n: 0.9142073935188462 Loss: 1.500394792510695e-09\n",
      "Iteration: 7933 lambda_n: 0.9327178029175905 Loss: 1.4996779159509143e-09\n",
      "Iteration: 7934 lambda_n: 0.9544624692570117 Loss: 1.4989500609932596e-09\n",
      "Iteration: 7935 lambda_n: 0.9932387935674456 Loss: 1.4982089062328473e-09\n",
      "Iteration: 7936 lambda_n: 0.9396162467613498 Loss: 1.497441530890706e-09\n",
      "Iteration: 7937 lambda_n: 0.9882384887681562 Loss: 1.4967193869185884e-09\n",
      "Iteration: 7938 lambda_n: 1.026684777295825 Loss: 1.4959636597210458e-09\n",
      "Iteration: 7939 lambda_n: 0.8931956529915205 Loss: 1.4951826476941247e-09\n",
      "Iteration: 7940 lambda_n: 1.025407122765191 Loss: 1.494506839952348e-09\n",
      "Iteration: 7941 lambda_n: 0.9265895346079601 Loss: 1.4937347071304853e-09\n",
      "Iteration: 7942 lambda_n: 0.9015620148193723 Loss: 1.4930407453961554e-09\n",
      "Iteration: 7943 lambda_n: 0.9135133238089217 Loss: 1.4923688460862017e-09\n",
      "Iteration: 7944 lambda_n: 0.9981741517512642 Loss: 1.4916913044219056e-09\n",
      "Iteration: 7945 lambda_n: 1.0262859652141767 Loss: 1.4909545819558248e-09\n",
      "Iteration: 7946 lambda_n: 0.95240489901544 Loss: 1.4902011381299847e-09\n",
      "Iteration: 7947 lambda_n: 1.0473445904173413 Loss: 1.489505724847015e-09\n",
      "Iteration: 7948 lambda_n: 0.9525513244375745 Loss: 1.488744893818061e-09\n",
      "Iteration: 7949 lambda_n: 1.0142684461066132 Loss: 1.4880567654168394e-09\n",
      "Iteration: 7950 lambda_n: 0.901184863330715 Loss: 1.487327786600188e-09\n",
      "Iteration: 7951 lambda_n: 1.013039062321852 Loss: 1.4866835693214409e-09\n",
      "Iteration: 7952 lambda_n: 0.9682573404330751 Loss: 1.4859628987345913e-09\n",
      "Iteration: 7953 lambda_n: 0.97154440024344 Loss: 1.4852778018808439e-09\n",
      "Iteration: 7954 lambda_n: 0.9111490690255247 Loss: 1.484593933662117e-09\n",
      "Iteration: 7955 lambda_n: 0.9435069717524949 Loss: 1.4839558982406833e-09\n",
      "Iteration: 7956 lambda_n: 0.9492187069252321 Loss: 1.4832984350528728e-09\n",
      "Iteration: 7957 lambda_n: 0.9580099035772693 Loss: 1.4826403348868641e-09\n",
      "Iteration: 7958 lambda_n: 0.9782396736428466 Loss: 1.481979512539069e-09\n",
      "Iteration: 7959 lambda_n: 0.9258433275779439 Loss: 1.4813082163723232e-09\n",
      "Iteration: 7960 lambda_n: 0.9259517897121502 Loss: 1.4806762051518082e-09\n",
      "Iteration: 7961 lambda_n: 0.9287446841356247 Loss: 1.4800472625721442e-09\n",
      "Iteration: 7962 lambda_n: 0.9633421478797218 Loss: 1.4794195566300006e-09\n",
      "Iteration: 7963 lambda_n: 0.9270094338943415 Loss: 1.4787717280859677e-09\n",
      "Iteration: 7964 lambda_n: 0.9518753633575601 Loss: 1.4781515588108582e-09\n",
      "Iteration: 7965 lambda_n: 0.9973322762387612 Loss: 1.4775179370599917e-09\n",
      "Iteration: 7966 lambda_n: 1.0159610202518095 Loss: 1.4768574684546141e-09\n",
      "Iteration: 7967 lambda_n: 0.8938368988027791 Loss: 1.4761882891232982e-09\n",
      "Iteration: 7968 lambda_n: 0.9594767918133688 Loss: 1.4756027548230941e-09\n",
      "Iteration: 7969 lambda_n: 0.9421905748550798 Loss: 1.474977267698849e-09\n",
      "Iteration: 7970 lambda_n: 0.9362803520830606 Loss: 1.4743662281602375e-09\n",
      "Iteration: 7971 lambda_n: 0.9709004813874335 Loss: 1.4737621129400106e-09\n",
      "Iteration: 7972 lambda_n: 1.0413537455696447 Loss: 1.4731388501901303e-09\n",
      "Iteration: 7973 lambda_n: 1.0249530177450639 Loss: 1.4724738833859517e-09\n",
      "Iteration: 7974 lambda_n: 1.0216367874556993 Loss: 1.4718230807346657e-09\n",
      "Iteration: 7975 lambda_n: 0.9601471302431391 Loss: 1.4711779872576665e-09\n",
      "Iteration: 7976 lambda_n: 0.9996521980075244 Loss: 1.4705750695308811e-09\n",
      "Iteration: 7977 lambda_n: 0.9290268162216025 Loss: 1.4699506217596321e-09\n",
      "Iteration: 7978 lambda_n: 0.9910961168986502 Loss: 1.4693734336767999e-09\n",
      "Iteration: 7979 lambda_n: 0.9237506472784593 Loss: 1.4687608031108657e-09\n",
      "Iteration: 7980 lambda_n: 0.9897509827554771 Loss: 1.468192867878892e-09\n",
      "Iteration: 7981 lambda_n: 0.9426315153073576 Loss: 1.4675874281306825e-09\n",
      "Iteration: 7982 lambda_n: 0.8985328605850459 Loss: 1.4670139050018165e-09\n",
      "Iteration: 7983 lambda_n: 1.040472720719179 Loss: 1.4664700186559546e-09\n",
      "Iteration: 7984 lambda_n: 0.9793065355819175 Loss: 1.4658433328800337e-09\n",
      "Iteration: 7985 lambda_n: 1.011570116957378 Loss: 1.4652568356413688e-09\n",
      "Iteration: 7986 lambda_n: 1.0475828351714112 Loss: 1.4646542580700386e-09\n",
      "Iteration: 7987 lambda_n: 0.9416238167651272 Loss: 1.4640336940755276e-09\n",
      "Iteration: 7988 lambda_n: 0.8938578697833146 Loss: 1.4634790678094636e-09\n",
      "Iteration: 7989 lambda_n: 1.0388751400449054 Loss: 1.4629552771822377e-09\n",
      "Iteration: 7990 lambda_n: 0.9069712947694604 Loss: 1.462349535442067e-09\n",
      "Iteration: 7991 lambda_n: 0.9546149343006223 Loss: 1.4618236789224753e-09\n",
      "Iteration: 7992 lambda_n: 0.9586395795147621 Loss: 1.461272964292744e-09\n",
      "Iteration: 7993 lambda_n: 0.953529686310475 Loss: 1.4607228317011127e-09\n",
      "Iteration: 7994 lambda_n: 0.9627005877554599 Loss: 1.460178504644304e-09\n",
      "Iteration: 7995 lambda_n: 0.9606306871296679 Loss: 1.4596318253648086e-09\n",
      "Iteration: 7996 lambda_n: 0.9922631406121641 Loss: 1.4590892128773208e-09\n",
      "Iteration: 7997 lambda_n: 0.951936560906151 Loss: 1.4585317010493314e-09\n",
      "Iteration: 7998 lambda_n: 1.0250633247103844 Loss: 1.4579997598502237e-09\n",
      "Iteration: 7999 lambda_n: 0.897539050597772 Loss: 1.4574299804848787e-09\n",
      "Iteration: 8000 lambda_n: 0.9196705481694991 Loss: 1.4569338738559363e-09\n",
      "Iteration: 8001 lambda_n: 1.0110895095194559 Loss: 1.456428065347202e-09\n",
      "Iteration: 8002 lambda_n: 0.9338620372594972 Loss: 1.4558748159903167e-09\n",
      "Iteration: 8003 lambda_n: 1.0421295772238783 Loss: 1.455366660221379e-09\n",
      "Iteration: 8004 lambda_n: 1.0080289988517495 Loss: 1.4548025548894478e-09\n",
      "Iteration: 8005 lambda_n: 1.048714589880403 Loss: 1.4542600459694768e-09\n",
      "Iteration: 8006 lambda_n: 0.9043537645114851 Loss: 1.4536987861094828e-09\n",
      "Iteration: 8007 lambda_n: 0.9253247717102543 Loss: 1.4532175770086784e-09\n",
      "Iteration: 8008 lambda_n: 1.0444182855225017 Loss: 1.452727676740389e-09\n",
      "Iteration: 8009 lambda_n: 0.9737649955431635 Loss: 1.4521775891870629e-09\n",
      "Iteration: 8010 lambda_n: 0.9226980103260995 Loss: 1.4516676694803231e-09\n",
      "Iteration: 8011 lambda_n: 0.9644613624581129 Loss: 1.451187100949597e-09\n",
      "Iteration: 8012 lambda_n: 1.0096287255022487 Loss: 1.4506873581718044e-09\n",
      "Iteration: 8013 lambda_n: 1.021770481057067 Loss: 1.4501670388367183e-09\n",
      "Iteration: 8014 lambda_n: 0.925532568968748 Loss: 1.4496434206886058e-09\n",
      "Iteration: 8015 lambda_n: 0.8971754175022101 Loss: 1.4491717983188193e-09\n",
      "Iteration: 8016 lambda_n: 0.9698125694000683 Loss: 1.4487169728615867e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8017 lambda_n: 0.9937473118772766 Loss: 1.4482278005897787e-09\n",
      "Iteration: 8018 lambda_n: 1.0483520513387254 Loss: 1.4477292721365029e-09\n",
      "Iteration: 8019 lambda_n: 0.9401364286022117 Loss: 1.447206276868733e-09\n",
      "Iteration: 8020 lambda_n: 1.0125654849889296 Loss: 1.4467399879104312e-09\n",
      "Iteration: 8021 lambda_n: 1.027725798214629 Loss: 1.4462404370629969e-09\n",
      "Iteration: 8022 lambda_n: 1.0123002079040793 Loss: 1.445736280193334e-09\n",
      "Iteration: 8023 lambda_n: 1.0145067346862293 Loss: 1.445242542350466e-09\n",
      "Iteration: 8024 lambda_n: 0.9849369368602583 Loss: 1.4447505321391317e-09\n",
      "Iteration: 8025 lambda_n: 0.9911990572597219 Loss: 1.4442755785251044e-09\n",
      "Iteration: 8026 lambda_n: 1.032706248101104 Loss: 1.4438002369124811e-09\n",
      "Iteration: 8027 lambda_n: 0.9790860119091418 Loss: 1.4433077561235678e-09\n",
      "Iteration: 8028 lambda_n: 0.8947215460013302 Loss: 1.4428435377008347e-09\n",
      "Iteration: 8029 lambda_n: 0.9625348071988458 Loss: 1.442421635137409e-09\n",
      "Iteration: 8030 lambda_n: 0.8941660798000715 Loss: 1.4419700554753235e-09\n",
      "Iteration: 8031 lambda_n: 0.9635936092115036 Loss: 1.4415528104802319e-09\n",
      "Iteration: 8032 lambda_n: 0.8972014232474598 Loss: 1.4411054361915746e-09\n",
      "Iteration: 8033 lambda_n: 0.9443042083035037 Loss: 1.4406911354115973e-09\n",
      "Iteration: 8034 lambda_n: 0.9299637625563544 Loss: 1.440257299611818e-09\n",
      "Iteration: 8035 lambda_n: 0.8998042066503683 Loss: 1.4398323222744656e-09\n",
      "Iteration: 8036 lambda_n: 0.925067754048051 Loss: 1.4394232733929078e-09\n",
      "Iteration: 8037 lambda_n: 0.9508889760887937 Loss: 1.4390048793939687e-09\n",
      "Iteration: 8038 lambda_n: 0.9224523650521618 Loss: 1.4385770553208288e-09\n",
      "Iteration: 8039 lambda_n: 0.9900418167839735 Loss: 1.4381642419877239e-09\n",
      "Iteration: 8040 lambda_n: 1.0313626938515672 Loss: 1.4377235190377464e-09\n",
      "Iteration: 8041 lambda_n: 0.9341442335211162 Loss: 1.4372669682108171e-09\n",
      "Iteration: 8042 lambda_n: 0.9559855971857859 Loss: 1.4368558462075639e-09\n",
      "Iteration: 8043 lambda_n: 0.9906355363252761 Loss: 1.4364373350939603e-09\n",
      "Iteration: 8044 lambda_n: 1.0001329381436281 Loss: 1.4360060167930137e-09\n",
      "Iteration: 8045 lambda_n: 1.0135977653822967 Loss: 1.4355729994186386e-09\n",
      "Iteration: 8046 lambda_n: 0.8970599016062348 Loss: 1.43513664809923e-09\n",
      "Iteration: 8047 lambda_n: 0.9684745514520947 Loss: 1.4347526693024565e-09\n",
      "Iteration: 8048 lambda_n: 0.9461519728816082 Loss: 1.4343402416693692e-09\n",
      "Iteration: 8049 lambda_n: 1.023538795495855 Loss: 1.4339395261231094e-09\n",
      "Iteration: 8050 lambda_n: 0.9199995245493721 Loss: 1.4335083889258348e-09\n",
      "Iteration: 8051 lambda_n: 1.013553935551104 Loss: 1.4331230915314782e-09\n",
      "Iteration: 8052 lambda_n: 1.0201097270218547 Loss: 1.4327008592237091e-09\n",
      "Iteration: 8053 lambda_n: 0.9574698756266153 Loss: 1.432278344061109e-09\n",
      "Iteration: 8054 lambda_n: 0.9104618919691675 Loss: 1.4318840595822516e-09\n",
      "Iteration: 8055 lambda_n: 1.0394275154103443 Loss: 1.431511166130861e-09\n",
      "Iteration: 8056 lambda_n: 1.0301818705658905 Loss: 1.431087688793274e-09\n",
      "Iteration: 8057 lambda_n: 1.0153685620163542 Loss: 1.4306704562783212e-09\n",
      "Iteration: 8058 lambda_n: 1.0450190319551405 Loss: 1.4302616408949652e-09\n",
      "Iteration: 8059 lambda_n: 0.9553135893987195 Loss: 1.4298433309138074e-09\n",
      "Iteration: 8060 lambda_n: 0.9406552717533025 Loss: 1.4294631853127917e-09\n",
      "Iteration: 8061 lambda_n: 0.9196776788741066 Loss: 1.4290909204441377e-09\n",
      "Iteration: 8062 lambda_n: 0.8948383385186859 Loss: 1.4287289024285755e-09\n",
      "Iteration: 8063 lambda_n: 1.023533236062055 Loss: 1.428378512103961e-09\n",
      "Iteration: 8064 lambda_n: 1.0178327194230297 Loss: 1.4279798092575686e-09\n",
      "Iteration: 8065 lambda_n: 0.9280720262782888 Loss: 1.4275856385399063e-09\n",
      "Iteration: 8066 lambda_n: 0.947032915181569 Loss: 1.4272283072829354e-09\n",
      "Iteration: 8067 lambda_n: 0.9001794493594515 Loss: 1.4268656130428694e-09\n",
      "Iteration: 8068 lambda_n: 1.016442749290373 Loss: 1.4265227261606365e-09\n",
      "Iteration: 8069 lambda_n: 1.0003455002047945 Loss: 1.4261375669570676e-09\n",
      "Iteration: 8070 lambda_n: 0.9903697899763656 Loss: 1.425760722788211e-09\n",
      "Iteration: 8071 lambda_n: 0.9213148883506833 Loss: 1.4253897673061008e-09\n",
      "Iteration: 8072 lambda_n: 0.9126815421103156 Loss: 1.4250466258804185e-09\n",
      "Iteration: 8073 lambda_n: 1.0459977688016904 Loss: 1.4247084950139297e-09\n",
      "Iteration: 8074 lambda_n: 0.9850403619158079 Loss: 1.424323038069348e-09\n",
      "Iteration: 8075 lambda_n: 1.0149142791786785 Loss: 1.4239622086362257e-09\n",
      "Iteration: 8076 lambda_n: 0.9237561464623336 Loss: 1.423592540313454e-09\n",
      "Iteration: 8077 lambda_n: 0.9205906104308261 Loss: 1.4232580259035028e-09\n",
      "Iteration: 8078 lambda_n: 0.9636693453282733 Loss: 1.4229264229535447e-09\n",
      "Iteration: 8079 lambda_n: 1.004638040359826 Loss: 1.4225811526453445e-09\n",
      "Iteration: 8080 lambda_n: 1.0153499994586068 Loss: 1.4222232051061078e-09\n",
      "Iteration: 8081 lambda_n: 1.0226648026374965 Loss: 1.4218635298541055e-09\n",
      "Iteration: 8082 lambda_n: 0.966404963930068 Loss: 1.4215033880050907e-09\n",
      "Iteration: 8083 lambda_n: 0.998227907747186 Loss: 1.4211650553795973e-09\n",
      "Iteration: 8084 lambda_n: 0.9766782306046061 Loss: 1.4208175344942708e-09\n",
      "Iteration: 8085 lambda_n: 0.9498126798073951 Loss: 1.4204794617036779e-09\n",
      "Iteration: 8086 lambda_n: 0.9439537146937156 Loss: 1.4201525349962145e-09\n",
      "Iteration: 8087 lambda_n: 1.0241276787273208 Loss: 1.4198294047729747e-09\n",
      "Iteration: 8088 lambda_n: 0.9043953009040542 Loss: 1.419480749696748e-09\n",
      "Iteration: 8089 lambda_n: 0.9410868011109033 Loss: 1.419174659148455e-09\n",
      "Iteration: 8090 lambda_n: 0.9921841257610132 Loss: 1.4188578199148833e-09\n",
      "Iteration: 8091 lambda_n: 1.0114679458108307 Loss: 1.4185256011861395e-09\n",
      "Iteration: 8092 lambda_n: 0.9931912484297433 Loss: 1.4181888697693706e-09\n",
      "Iteration: 8093 lambda_n: 0.9491273240074789 Loss: 1.41786015200629e-09\n",
      "Iteration: 8094 lambda_n: 0.9228130450165057 Loss: 1.4175478073807362e-09\n",
      "Iteration: 8095 lambda_n: 1.0227874161870478 Loss: 1.417245789014301e-09\n",
      "Iteration: 8096 lambda_n: 1.0156944254457871 Loss: 1.4169128530608688e-09\n",
      "Iteration: 8097 lambda_n: 0.9671282122666749 Loss: 1.4165841804677139e-09\n",
      "Iteration: 8098 lambda_n: 0.9870653496048094 Loss: 1.416273060261606e-09\n",
      "Iteration: 8099 lambda_n: 0.9131811878254966 Loss: 1.4159573084976867e-09\n",
      "Iteration: 8100 lambda_n: 1.0386694078224816 Loss: 1.4156668391760211e-09\n",
      "Iteration: 8101 lambda_n: 0.9011814329115834 Loss: 1.4153382302012162e-09\n",
      "Iteration: 8102 lambda_n: 0.9975661114289075 Loss: 1.4150548090113747e-09\n",
      "Iteration: 8103 lambda_n: 0.9245789040992148 Loss: 1.4147427299415975e-09\n",
      "Iteration: 8104 lambda_n: 1.0258428658439505 Loss: 1.4144551484667775e-09\n",
      "Iteration: 8105 lambda_n: 0.9711780367660255 Loss: 1.4141377996610972e-09\n",
      "Iteration: 8106 lambda_n: 1.0318823690559045 Loss: 1.4138391372888684e-09\n",
      "Iteration: 8107 lambda_n: 0.99029711275637 Loss: 1.4135236085363211e-09\n",
      "Iteration: 8108 lambda_n: 0.9648879833793021 Loss: 1.413222596802561e-09\n",
      "Iteration: 8109 lambda_n: 1.012753855830778 Loss: 1.4129309900289255e-09\n",
      "Iteration: 8110 lambda_n: 1.0448333841554192 Loss: 1.4126266400938497e-09\n",
      "Iteration: 8111 lambda_n: 0.9798360854685725 Loss: 1.4123144923279811e-09\n",
      "Iteration: 8112 lambda_n: 1.0380444769431416 Loss: 1.4120235337906733e-09\n",
      "Iteration: 8113 lambda_n: 0.9992445064476219 Loss: 1.411717053485297e-09\n",
      "Iteration: 8114 lambda_n: 0.903738431063962 Loss: 1.4114237996092788e-09\n",
      "Iteration: 8115 lambda_n: 0.9444573826443919 Loss: 1.411160099878676e-09\n",
      "Iteration: 8116 lambda_n: 1.0286542907438039 Loss: 1.4108859770530935e-09\n",
      "Iteration: 8117 lambda_n: 0.9845484005549698 Loss: 1.4105890651712666e-09\n",
      "Iteration: 8118 lambda_n: 1.0219671255755225 Loss: 1.41030657267606e-09\n",
      "Iteration: 8119 lambda_n: 0.9244010919116527 Loss: 1.4100150261787424e-09\n",
      "Iteration: 8120 lambda_n: 0.9980308067054338 Loss: 1.4097528690041357e-09\n",
      "Iteration: 8121 lambda_n: 0.9883378006101879 Loss: 1.4094713655358185e-09\n",
      "Iteration: 8122 lambda_n: 1.0464316656009824 Loss: 1.409194212873387e-09\n",
      "Iteration: 8123 lambda_n: 1.04810884173837 Loss: 1.4089024615600833e-09\n",
      "Iteration: 8124 lambda_n: 0.9268774020967996 Loss: 1.4086120198515983e-09\n",
      "Iteration: 8125 lambda_n: 1.0252300219322563 Loss: 1.4083567247403853e-09\n",
      "Iteration: 8126 lambda_n: 1.0466092636394289 Loss: 1.408075870950966e-09\n",
      "Iteration: 8127 lambda_n: 0.9612922161960553 Loss: 1.4077908836219856e-09\n",
      "Iteration: 8128 lambda_n: 0.938172108727968 Loss: 1.407530709321698e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8129 lambda_n: 0.9592699303542623 Loss: 1.4072782060506324e-09\n",
      "Iteration: 8130 lambda_n: 0.9819322008152174 Loss: 1.407021439443223e-09\n",
      "Iteration: 8131 lambda_n: 0.9097447207353637 Loss: 1.4067600807272102e-09\n",
      "Iteration: 8132 lambda_n: 0.9821708193585578 Loss: 1.4065193011990423e-09\n",
      "Iteration: 8133 lambda_n: 0.9479310140689841 Loss: 1.4062607496641995e-09\n",
      "Iteration: 8134 lambda_n: 1.0468700435777336 Loss: 1.4060126334580688e-09\n",
      "Iteration: 8135 lambda_n: 0.9863517710472152 Loss: 1.4057401489650537e-09\n",
      "Iteration: 8136 lambda_n: 0.9558068486908664 Loss: 1.4054849677422519e-09\n",
      "Iteration: 8137 lambda_n: 0.9162944555183063 Loss: 1.4052391102783477e-09\n",
      "Iteration: 8138 lambda_n: 0.9008450871875078 Loss: 1.4050047246279233e-09\n",
      "Iteration: 8139 lambda_n: 0.9679270338277389 Loss: 1.4047755182291385e-09\n",
      "Iteration: 8140 lambda_n: 1.0072862106975398 Loss: 1.4045305370087183e-09\n",
      "Iteration: 8141 lambda_n: 0.9288221779259501 Loss: 1.404277049509899e-09\n",
      "Iteration: 8142 lambda_n: 0.8941550512011149 Loss: 1.4040446669598025e-09\n",
      "Iteration: 8143 lambda_n: 0.9971589182032804 Loss: 1.403822157391944e-09\n",
      "Iteration: 8144 lambda_n: 0.9330910778421898 Loss: 1.4035753257915696e-09\n",
      "Iteration: 8145 lambda_n: 0.9024829938910615 Loss: 1.4033456870758238e-09\n",
      "Iteration: 8146 lambda_n: 0.9995707442716631 Loss: 1.4031247871928113e-09\n",
      "Iteration: 8147 lambda_n: 0.9981618177769597 Loss: 1.4028814271929632e-09\n",
      "Iteration: 8148 lambda_n: 0.9073278961170252 Loss: 1.4026398175249104e-09\n",
      "Iteration: 8149 lambda_n: 1.018550971267746 Loss: 1.4024214614607167e-09\n",
      "Iteration: 8150 lambda_n: 0.9007366708470569 Loss: 1.4024224165621938e-09\n",
      "Iteration: 8151 lambda_n: 0.9564624213314603 Loss: 1.4024976075680151e-09\n",
      "Iteration: 8152 lambda_n: 1.0154206319723562 Loss: 1.4025773609027247e-09\n",
      "Iteration: 8153 lambda_n: 0.9756557599361542 Loss: 1.4026619431966808e-09\n",
      "Iteration: 8154 lambda_n: 0.9851720085050041 Loss: 1.4027430896807622e-09\n",
      "Iteration: 8155 lambda_n: 0.9373576186270588 Loss: 1.4028249296469727e-09\n",
      "Iteration: 8156 lambda_n: 1.0375043892147744 Loss: 1.4029026872549397e-09\n",
      "Iteration: 8157 lambda_n: 0.9337851629048508 Loss: 1.4029886585372963e-09\n",
      "Iteration: 8158 lambda_n: 1.0355510647776878 Loss: 1.4030659121012538e-09\n",
      "Iteration: 8159 lambda_n: 0.9158712306455234 Loss: 1.403151487703322e-09\n",
      "Iteration: 8160 lambda_n: 0.9986432320149387 Loss: 1.4032270539382981e-09\n",
      "Iteration: 8161 lambda_n: 0.9553492685457923 Loss: 1.4033093487487625e-09\n",
      "Iteration: 8162 lambda_n: 1.044293800976559 Loss: 1.403387957794627e-09\n",
      "Iteration: 8163 lambda_n: 0.948642583832404 Loss: 1.4034737772569446e-09\n",
      "Iteration: 8164 lambda_n: 1.0424958901197297 Loss: 1.403551600872352e-09\n",
      "Iteration: 8165 lambda_n: 0.9401438554816821 Loss: 1.403637027732753e-09\n",
      "Iteration: 8166 lambda_n: 0.9580347705755287 Loss: 1.4037139301617343e-09\n",
      "Iteration: 8167 lambda_n: 0.931644299739532 Loss: 1.4037921916126062e-09\n",
      "Iteration: 8168 lambda_n: 0.9803148833124039 Loss: 1.4038681803423173e-09\n",
      "Iteration: 8169 lambda_n: 0.9120527265164678 Loss: 1.4039480297402378e-09\n",
      "Iteration: 8170 lambda_n: 1.0031794230291313 Loss: 1.4040221982625338e-09\n",
      "Iteration: 8171 lambda_n: 1.024771589979535 Loss: 1.4041036779537485e-09\n",
      "Iteration: 8172 lambda_n: 0.996050149348483 Loss: 1.404186779882686e-09\n",
      "Iteration: 8173 lambda_n: 0.9538099862977009 Loss: 1.4042674176335147e-09\n",
      "Iteration: 8174 lambda_n: 1.03955636217696 Loss: 1.4043445124319122e-09\n",
      "Iteration: 8175 lambda_n: 0.9302306244411019 Loss: 1.4044284130774035e-09\n",
      "Iteration: 8176 lambda_n: 1.0449276979561057 Loss: 1.4045033545546307e-09\n",
      "Iteration: 8177 lambda_n: 0.9797522717087157 Loss: 1.4045874220943259e-09\n",
      "Iteration: 8178 lambda_n: 1.0129868421676822 Loss: 1.4046660999328529e-09\n",
      "Iteration: 8179 lambda_n: 0.9692110014610855 Loss: 1.404747319453541e-09\n",
      "Iteration: 8180 lambda_n: 1.0319021018097299 Loss: 1.404824889760036e-09\n",
      "Iteration: 8181 lambda_n: 0.9879078177804519 Loss: 1.4049073463011151e-09\n",
      "Iteration: 8182 lambda_n: 0.899942339992601 Loss: 1.4049861434006626e-09\n",
      "Iteration: 8183 lambda_n: 1.037969582128604 Loss: 1.4050577959733398e-09\n",
      "Iteration: 8184 lambda_n: 0.9891710504244879 Loss: 1.4051403212191134e-09\n",
      "Iteration: 8185 lambda_n: 0.957958351445434 Loss: 1.4052188215535827e-09\n",
      "Iteration: 8186 lambda_n: 1.0100885950324805 Loss: 1.4052947069660331e-09\n",
      "Iteration: 8187 lambda_n: 0.9516496105247327 Loss: 1.4053745884530484e-09\n",
      "Iteration: 8188 lambda_n: 0.9055764958721044 Loss: 1.4054496958518016e-09\n",
      "Iteration: 8189 lambda_n: 0.9840602792606843 Loss: 1.4055210463828817e-09\n",
      "Iteration: 8190 lambda_n: 1.0310459734359818 Loss: 1.4055984649081175e-09\n",
      "Iteration: 8191 lambda_n: 0.9549805787773997 Loss: 1.4056794355301292e-09\n",
      "Iteration: 8192 lambda_n: 0.9613393216498745 Loss: 1.405754285189887e-09\n",
      "Iteration: 8193 lambda_n: 1.0163092011965358 Loss: 1.4058295022218046e-09\n",
      "Iteration: 8194 lambda_n: 0.9686884596535076 Loss: 1.4059088758721842e-09\n",
      "Iteration: 8195 lambda_n: 1.0151239814749944 Loss: 1.4059843820195462e-09\n",
      "Iteration: 8196 lambda_n: 0.9359097808397143 Loss: 1.4060633703230166e-09\n",
      "Iteration: 8197 lambda_n: 0.9102964270882845 Loss: 1.406136041095602e-09\n",
      "Iteration: 8198 lambda_n: 0.8954618100161637 Loss: 1.4062065945860218e-09\n",
      "Iteration: 8199 lambda_n: 0.8948210295235753 Loss: 1.4062758737394592e-09\n",
      "Iteration: 8200 lambda_n: 0.9968519696126585 Loss: 1.4063449842673152e-09\n",
      "Iteration: 8201 lambda_n: 0.8972998334940517 Loss: 1.4064218459501795e-09\n",
      "Iteration: 8202 lambda_n: 0.9512712403591903 Loss: 1.406490889102048e-09\n",
      "Iteration: 8203 lambda_n: 0.9544380709017722 Loss: 1.4065639581598542e-09\n",
      "Iteration: 8204 lambda_n: 1.0002690532222216 Loss: 1.4066371291102472e-09\n",
      "Iteration: 8205 lambda_n: 1.0488503008835748 Loss: 1.4067136707600332e-09\n",
      "Iteration: 8206 lambda_n: 0.953392176944005 Loss: 1.4067937724390826e-09\n",
      "Iteration: 8207 lambda_n: 0.9106292242347462 Loss: 1.4068664180442393e-09\n",
      "Iteration: 8208 lambda_n: 0.9242656998681228 Loss: 1.4069356665130117e-09\n",
      "Iteration: 8209 lambda_n: 0.8971335459900104 Loss: 1.407005821434454e-09\n",
      "Iteration: 8210 lambda_n: 1.0157823961622137 Loss: 1.4070737840390829e-09\n",
      "Iteration: 8211 lambda_n: 0.9850307396141124 Loss: 1.407150597430229e-09\n",
      "Iteration: 8212 lambda_n: 1.02592444183498 Loss: 1.407224928703721e-09\n",
      "Iteration: 8213 lambda_n: 0.931369059678087 Loss: 1.4073021831491857e-09\n",
      "Iteration: 8214 lambda_n: 0.9311399873042953 Loss: 1.4073721595515434e-09\n",
      "Iteration: 8215 lambda_n: 0.9884785194237412 Loss: 1.4074419780049144e-09\n",
      "Iteration: 8216 lambda_n: 0.9314128893465604 Loss: 1.4075159515496154e-09\n",
      "Iteration: 8217 lambda_n: 0.8933477202887047 Loss: 1.4075854993077918e-09\n",
      "Iteration: 8218 lambda_n: 0.9637483839340831 Loss: 1.4076520688367958e-09\n",
      "Iteration: 8219 lambda_n: 0.9380083503932373 Loss: 1.4077237487442496e-09\n",
      "Iteration: 8220 lambda_n: 0.9289200425798718 Loss: 1.4077933642755744e-09\n",
      "Iteration: 8221 lambda_n: 0.969298624679441 Loss: 1.4078621613088326e-09\n",
      "Iteration: 8222 lambda_n: 0.9012506848853389 Loss: 1.4079338041172258e-09\n",
      "Iteration: 8223 lambda_n: 0.9287027947799867 Loss: 1.408000268143728e-09\n",
      "Iteration: 8224 lambda_n: 1.0100182198938124 Loss: 1.4080686236558458e-09\n",
      "Iteration: 8225 lambda_n: 0.9478080333216857 Loss: 1.4081428074876115e-09\n",
      "Iteration: 8226 lambda_n: 1.0300711019343096 Loss: 1.4082122617091377e-09\n",
      "Iteration: 8227 lambda_n: 0.8984704123927817 Loss: 1.4082875890453853e-09\n",
      "Iteration: 8228 lambda_n: 1.0189331424151846 Loss: 1.4083531255957547e-09\n",
      "Iteration: 8229 lambda_n: 1.0484555519735723 Loss: 1.4084273047442525e-09\n",
      "Iteration: 8230 lambda_n: 0.9645725134037048 Loss: 1.4085034590979409e-09\n",
      "Iteration: 8231 lambda_n: 1.036364346756316 Loss: 1.4085733422050797e-09\n",
      "Iteration: 8232 lambda_n: 0.971078989511538 Loss: 1.4086482607476244e-09\n",
      "Iteration: 8233 lambda_n: 0.9534043275584636 Loss: 1.4087182842659984e-09\n",
      "Iteration: 8234 lambda_n: 0.9630074482269092 Loss: 1.4087868731198655e-09\n",
      "Iteration: 8235 lambda_n: 0.9269129886964076 Loss: 1.4088559968014063e-09\n",
      "Iteration: 8236 lambda_n: 0.9883443983052449 Loss: 1.4089223745104638e-09\n",
      "Iteration: 8237 lambda_n: 0.9257017713982612 Loss: 1.4089929993900368e-09\n",
      "Iteration: 8238 lambda_n: 0.9897704849843914 Loss: 1.4090589850705798e-09\n",
      "Iteration: 8239 lambda_n: 0.9995719552367709 Loss: 1.409129383999089e-09\n",
      "Iteration: 8240 lambda_n: 0.9851725130673705 Loss: 1.4092003137531806e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8241 lambda_n: 0.9544330875110784 Loss: 1.4092700449097132e-09\n",
      "Iteration: 8242 lambda_n: 1.0001600785579623 Loss: 1.4093374376472436e-09\n",
      "Iteration: 8243 lambda_n: 0.9879696085201867 Loss: 1.409407895282127e-09\n",
      "Iteration: 8244 lambda_n: 0.8998446532154589 Loss: 1.4094773212026092e-09\n",
      "Iteration: 8245 lambda_n: 1.0334925359880671 Loss: 1.4095403965810878e-09\n",
      "Iteration: 8246 lambda_n: 0.9301263773847308 Loss: 1.4096126845590498e-09\n",
      "Iteration: 8247 lambda_n: 1.0385921162843665 Loss: 1.4096775669293426e-09\n",
      "Iteration: 8248 lambda_n: 0.9813792629884798 Loss: 1.4097498510530857e-09\n",
      "Iteration: 8249 lambda_n: 0.9352632483246474 Loss: 1.4098179675337168e-09\n",
      "Iteration: 8250 lambda_n: 0.9609955602351244 Loss: 1.409882722320813e-09\n",
      "Iteration: 8251 lambda_n: 0.9143615427348479 Loss: 1.4099491012892857e-09\n",
      "Iteration: 8252 lambda_n: 1.0243080774713482 Loss: 1.4100120985000054e-09\n",
      "Iteration: 8253 lambda_n: 1.0390757222344467 Loss: 1.4100825139157595e-09\n",
      "Iteration: 8254 lambda_n: 0.9952215642413588 Loss: 1.410153758418268e-09\n",
      "Iteration: 8255 lambda_n: 0.9880121156273879 Loss: 1.4102218097782713e-09\n",
      "Iteration: 8256 lambda_n: 0.9731188240499508 Loss: 1.4102891906139589e-09\n",
      "Iteration: 8257 lambda_n: 0.9788661479653827 Loss: 1.4103553869443946e-09\n",
      "Iteration: 8258 lambda_n: 1.0286201801451 Loss: 1.4104217998648748e-09\n",
      "Iteration: 8259 lambda_n: 0.9348526075673953 Loss: 1.410491414301063e-09\n",
      "Iteration: 8260 lambda_n: 1.0006978595740714 Loss: 1.41055451036294e-09\n",
      "Iteration: 8261 lambda_n: 0.9594647068298202 Loss: 1.4106218817919194e-09\n",
      "Iteration: 8262 lambda_n: 1.0405504502805238 Loss: 1.4106863027295511e-09\n",
      "Iteration: 8263 lambda_n: 0.9889619176884511 Loss: 1.4107559921775866e-09\n",
      "Iteration: 8264 lambda_n: 0.9295312502889059 Loss: 1.4108220381195669e-09\n",
      "Iteration: 8265 lambda_n: 1.0009638762280775 Loss: 1.4108839476610161e-09\n",
      "Iteration: 8266 lambda_n: 1.0439039808570723 Loss: 1.4109504512771016e-09\n",
      "Iteration: 8267 lambda_n: 0.9500431856082062 Loss: 1.411019621060742e-09\n",
      "Iteration: 8268 lambda_n: 0.9255629641997918 Loss: 1.4110823880621122e-09\n",
      "Iteration: 8269 lambda_n: 0.9770021610145344 Loss: 1.4111433764194268e-09\n",
      "Iteration: 8270 lambda_n: 1.0142233228039015 Loss: 1.411207597092e-09\n",
      "Iteration: 8271 lambda_n: 0.9139580765190266 Loss: 1.4112740847450275e-09\n",
      "Iteration: 8272 lambda_n: 1.0336427970340474 Loss: 1.41133382242657e-09\n",
      "Iteration: 8273 lambda_n: 0.9865690566847336 Loss: 1.4114012243492447e-09\n",
      "Iteration: 8274 lambda_n: 1.034874410521679 Loss: 1.4114653670177826e-09\n",
      "Iteration: 8275 lambda_n: 1.035489525273103 Loss: 1.411532460770058e-09\n",
      "Iteration: 8276 lambda_n: 1.0494254955842284 Loss: 1.4115994074848259e-09\n",
      "Iteration: 8277 lambda_n: 0.9285633428254069 Loss: 1.4116670591426414e-09\n",
      "Iteration: 8278 lambda_n: 1.0309277331731277 Loss: 1.411726738051813e-09\n",
      "Iteration: 8279 lambda_n: 1.0050897944923238 Loss: 1.4117928187037587e-09\n",
      "Iteration: 8280 lambda_n: 1.0233608056357515 Loss: 1.4118570537434725e-09\n",
      "Iteration: 8281 lambda_n: 0.9664058882243955 Loss: 1.4119222772326876e-09\n",
      "Iteration: 8282 lambda_n: 1.0442620069968296 Loss: 1.4119836853128257e-09\n",
      "Iteration: 8283 lambda_n: 1.0082959056987937 Loss: 1.4120498575391216e-09\n",
      "Iteration: 8284 lambda_n: 1.008767594284555 Loss: 1.412113557855092e-09\n",
      "Iteration: 8285 lambda_n: 0.9754062663629236 Loss: 1.4121770985842348e-09\n",
      "Iteration: 8286 lambda_n: 0.9270593564319521 Loss: 1.4122383623194814e-09\n",
      "Iteration: 8287 lambda_n: 0.9644773818138116 Loss: 1.4122964223367372e-09\n",
      "Iteration: 8288 lambda_n: 0.9111135187379789 Loss: 1.4123566607332004e-09\n",
      "Iteration: 8289 lambda_n: 0.990562789601701 Loss: 1.4124134024475264e-09\n",
      "Iteration: 8290 lambda_n: 0.9516429615552098 Loss: 1.4124749347387176e-09\n",
      "Iteration: 8291 lambda_n: 0.9059737968182191 Loss: 1.4125338773641327e-09\n",
      "Iteration: 8292 lambda_n: 0.9120559303398209 Loss: 1.4125898293701791e-09\n",
      "Iteration: 8293 lambda_n: 0.90086365619016 Loss: 1.4126460040699386e-09\n",
      "Iteration: 8294 lambda_n: 1.0420701931256222 Loss: 1.4127013459964484e-09\n",
      "Iteration: 8295 lambda_n: 0.905037445984422 Loss: 1.4127651898815464e-09\n",
      "Iteration: 8296 lambda_n: 1.0079120609749854 Loss: 1.412820463173425e-09\n",
      "Iteration: 8297 lambda_n: 0.9322463293209395 Loss: 1.4128818580473519e-09\n",
      "Iteration: 8298 lambda_n: 0.9722190857316324 Loss: 1.4129384660036163e-09\n",
      "Iteration: 8299 lambda_n: 0.9858741682003221 Loss: 1.4129973425295157e-09\n",
      "Iteration: 8300 lambda_n: 1.0416359723593007 Loss: 1.4130568676392092e-09\n",
      "Iteration: 8301 lambda_n: 0.9873853527963119 Loss: 1.413119576707164e-09\n",
      "Iteration: 8302 lambda_n: 0.921379272731019 Loss: 1.4131788301583094e-09\n",
      "Iteration: 8303 lambda_n: 0.9955532421471694 Loss: 1.4132339486843315e-09\n",
      "Iteration: 8304 lambda_n: 0.9457768946564813 Loss: 1.4132933481470069e-09\n",
      "Iteration: 8305 lambda_n: 0.9063647683417322 Loss: 1.4133495995896057e-09\n",
      "Iteration: 8306 lambda_n: 0.9055338680980262 Loss: 1.4134033515088998e-09\n",
      "Iteration: 8307 lambda_n: 1.013513703327678 Loss: 1.4134569043338036e-09\n",
      "Iteration: 8308 lambda_n: 0.9764056783115375 Loss: 1.4135166752126879e-09\n",
      "Iteration: 8309 lambda_n: 0.9086071101961921 Loss: 1.413574076194177e-09\n",
      "Iteration: 8310 lambda_n: 0.8940296727828833 Loss: 1.4136273306998729e-09\n",
      "Iteration: 8311 lambda_n: 1.0341464858316862 Loss: 1.413679581019433e-09\n",
      "Iteration: 8312 lambda_n: 0.9912739927402904 Loss: 1.413739859355629e-09\n",
      "Iteration: 8313 lambda_n: 0.9697875220693231 Loss: 1.4137974474303554e-09\n",
      "Iteration: 8314 lambda_n: 0.9284538287984324 Loss: 1.413853616649073e-09\n",
      "Iteration: 8315 lambda_n: 1.0079077501513871 Loss: 1.4139072252631e-09\n",
      "Iteration: 8316 lambda_n: 1.0204846012596758 Loss: 1.4139652523413208e-09\n",
      "Iteration: 8317 lambda_n: 0.9490834226959732 Loss: 1.4140238186744955e-09\n",
      "Iteration: 8318 lambda_n: 1.0311853720672282 Loss: 1.4140781087112578e-09\n",
      "Iteration: 8319 lambda_n: 0.9006010699841251 Loss: 1.4141369242934179e-09\n",
      "Iteration: 8320 lambda_n: 1.0175356492256977 Loss: 1.4141881172740697e-09\n",
      "Iteration: 8321 lambda_n: 1.016134408354078 Loss: 1.41424579385816e-09\n",
      "Iteration: 8322 lambda_n: 1.012078212924778 Loss: 1.4143032070415478e-09\n",
      "Iteration: 8323 lambda_n: 0.9329135766494174 Loss: 1.4143602023683117e-09\n",
      "Iteration: 8324 lambda_n: 1.0339326489831904 Loss: 1.4144125651201137e-09\n",
      "Iteration: 8325 lambda_n: 0.9080001746157901 Loss: 1.4144704297698288e-09\n",
      "Iteration: 8326 lambda_n: 0.9891354403866364 Loss: 1.414521076398818e-09\n",
      "Iteration: 8327 lambda_n: 0.9070884323641651 Loss: 1.414576085225139e-09\n",
      "Iteration: 8328 lambda_n: 1.0472428297469416 Loss: 1.4146263681064277e-09\n",
      "Iteration: 8329 lambda_n: 0.9420529819709084 Loss: 1.414684259874682e-09\n",
      "Iteration: 8330 lambda_n: 0.9183829931619835 Loss: 1.4147361500510784e-09\n",
      "Iteration: 8331 lambda_n: 1.036607786837437 Loss: 1.414786582789406e-09\n",
      "Iteration: 8332 lambda_n: 0.9572771807126492 Loss: 1.4148433410190229e-09\n",
      "Iteration: 8333 lambda_n: 0.8987094409433147 Loss: 1.4148955750174858e-09\n",
      "Iteration: 8334 lambda_n: 0.9099633862573782 Loss: 1.4149444567248816e-09\n",
      "Iteration: 8335 lambda_n: 0.9805571125125128 Loss: 1.414993804685021e-09\n",
      "Iteration: 8336 lambda_n: 0.9773266498037467 Loss: 1.4150468254302934e-09\n",
      "Iteration: 8337 lambda_n: 0.8925636939732816 Loss: 1.4150995012675545e-09\n",
      "Iteration: 8338 lambda_n: 0.9611707965899775 Loss: 1.4151474503361627e-09\n",
      "Iteration: 8339 lambda_n: 0.9318687811011969 Loss: 1.415198935677979e-09\n",
      "Iteration: 8340 lambda_n: 1.0030951739898653 Loss: 1.4152486905936375e-09\n",
      "Iteration: 8341 lambda_n: 1.0160495520077717 Loss: 1.4153020853712142e-09\n",
      "Iteration: 8342 lambda_n: 0.9310289286772103 Loss: 1.4153559889141487e-09\n",
      "Iteration: 8343 lambda_n: 1.0196456299314085 Loss: 1.4154052116761646e-09\n",
      "Iteration: 8344 lambda_n: 0.9774483769624148 Loss: 1.4154589537073837e-09\n",
      "Iteration: 8345 lambda_n: 0.9194243137009162 Loss: 1.4155102983993593e-09\n",
      "Iteration: 8346 lambda_n: 0.9267260877629244 Loss: 1.415558429483443e-09\n",
      "Iteration: 8347 lambda_n: 0.9790811311104712 Loss: 1.4156067977522523e-09\n",
      "Iteration: 8348 lambda_n: 0.94273582226522 Loss: 1.4156577381338573e-09\n",
      "Iteration: 8349 lambda_n: 0.9492132731732602 Loss: 1.4157066201468867e-09\n",
      "Iteration: 8350 lambda_n: 0.9015904706105918 Loss: 1.4157556895561975e-09\n",
      "Iteration: 8351 lambda_n: 0.8959131980938388 Loss: 1.415802141801593e-09\n",
      "Iteration: 8352 lambda_n: 1.0131846517299565 Loss: 1.4158481626285317e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8353 lambda_n: 0.9377871079824378 Loss: 1.4159000459323969e-09\n",
      "Iteration: 8354 lambda_n: 0.9189874395691958 Loss: 1.4159479065614206e-09\n",
      "Iteration: 8355 lambda_n: 1.0449783151705965 Loss: 1.4159946561787327e-09\n",
      "Iteration: 8356 lambda_n: 1.011604911332977 Loss: 1.4160476496846167e-09\n",
      "Iteration: 8357 lambda_n: 1.0199016838677328 Loss: 1.4160987675579616e-09\n",
      "Iteration: 8358 lambda_n: 0.9096754979904086 Loss: 1.4161501264180702e-09\n",
      "Iteration: 8359 lambda_n: 0.9049093286258298 Loss: 1.4161957718710181e-09\n",
      "Iteration: 8360 lambda_n: 1.0242208375997386 Loss: 1.4162410340842567e-09\n",
      "Iteration: 8361 lambda_n: 1.0087505075482837 Loss: 1.416292104726149e-09\n",
      "Iteration: 8362 lambda_n: 1.0097829870626251 Loss: 1.4163422318509482e-09\n",
      "Iteration: 8363 lambda_n: 0.8960558512574133 Loss: 1.4163922285697397e-09\n",
      "Iteration: 8364 lambda_n: 0.9775073513233465 Loss: 1.4164364400644665e-09\n",
      "Iteration: 8365 lambda_n: 0.9769541860178015 Loss: 1.4164845174864818e-09\n",
      "Iteration: 8366 lambda_n: 0.901862136642055 Loss: 1.4165324076766534e-09\n",
      "Iteration: 8367 lambda_n: 0.9666262340193632 Loss: 1.4165764623278614e-09\n",
      "Iteration: 8368 lambda_n: 0.9719177432210447 Loss: 1.4166235381154558e-09\n",
      "Iteration: 8369 lambda_n: 1.0170376173092406 Loss: 1.4166707081406581e-09\n",
      "Iteration: 8370 lambda_n: 0.9360692704388854 Loss: 1.416719901732583e-09\n",
      "Iteration: 8371 lambda_n: 1.0368455855559653 Loss: 1.4167650154938362e-09\n",
      "Iteration: 8372 lambda_n: 0.9417222149002783 Loss: 1.4168148235844306e-09\n",
      "Iteration: 8373 lambda_n: 0.9841333304609108 Loss: 1.4168598978418973e-09\n",
      "Iteration: 8374 lambda_n: 0.918162683627378 Loss: 1.4169068403671042e-09\n",
      "Iteration: 8375 lambda_n: 0.9975745791723893 Loss: 1.4169504837100351e-09\n",
      "Iteration: 8376 lambda_n: 0.9893781010481345 Loss: 1.4169977521641606e-09\n",
      "Iteration: 8377 lambda_n: 1.0352650577590592 Loss: 1.4170444619146302e-09\n",
      "Iteration: 8378 lambda_n: 1.001848531843023 Loss: 1.4170931750279042e-09\n",
      "Iteration: 8379 lambda_n: 0.9539484235784498 Loss: 1.417140133498596e-09\n",
      "Iteration: 8380 lambda_n: 0.9855085751367801 Loss: 1.4171846915403872e-09\n",
      "Iteration: 8381 lambda_n: 0.9212627960650215 Loss: 1.4172305656711897e-09\n",
      "Iteration: 8382 lambda_n: 1.0083399969237399 Loss: 1.4172732947376893e-09\n",
      "Iteration: 8383 lambda_n: 1.0035613418628033 Loss: 1.4173199113585625e-09\n",
      "Iteration: 8384 lambda_n: 0.9910321905637886 Loss: 1.4173661426832277e-09\n",
      "Iteration: 8385 lambda_n: 0.9698883527524368 Loss: 1.4174116310470208e-09\n",
      "Iteration: 8386 lambda_n: 0.9827525111514759 Loss: 1.4174559872448102e-09\n",
      "Iteration: 8387 lambda_n: 0.9861656740437305 Loss: 1.4175007779939124e-09\n",
      "Iteration: 8388 lambda_n: 0.994448540630881 Loss: 1.4175455671097117e-09\n",
      "Iteration: 8389 lambda_n: 1.048502120068667 Loss: 1.4175905650488895e-09\n",
      "Iteration: 8390 lambda_n: 0.9509223651893483 Loss: 1.417637843013078e-09\n",
      "Iteration: 8391 lambda_n: 1.007674267762228 Loss: 1.4176805561868653e-09\n",
      "Iteration: 8392 lambda_n: 0.8949718973747081 Loss: 1.4177256646900563e-09\n",
      "Iteration: 8393 lambda_n: 0.9565071364633765 Loss: 1.4177655769072446e-09\n",
      "Iteration: 8394 lambda_n: 1.0039459833811628 Loss: 1.4178081001390798e-09\n",
      "Iteration: 8395 lambda_n: 0.9125930095826315 Loss: 1.4178525751751326e-09\n",
      "Iteration: 8396 lambda_n: 0.9645760501650892 Loss: 1.4178928527540739e-09\n",
      "Iteration: 8397 lambda_n: 1.039509590214799 Loss: 1.4179352820862403e-09\n",
      "Iteration: 8398 lambda_n: 0.9269165797199547 Loss: 1.417980853318575e-09\n",
      "Iteration: 8399 lambda_n: 0.943865513175425 Loss: 1.4180213292113533e-09\n",
      "Iteration: 8400 lambda_n: 1.0443742956384743 Loss: 1.4180624015329068e-09\n",
      "Iteration: 8401 lambda_n: 0.9848914882392359 Loss: 1.4181076987416464e-09\n",
      "Iteration: 8402 lambda_n: 1.0416533121656433 Loss: 1.4181502483591243e-09\n",
      "Iteration: 8403 lambda_n: 0.964735709473184 Loss: 1.4181950839964823e-09\n",
      "Iteration: 8404 lambda_n: 0.9607780815416611 Loss: 1.4182364503236505e-09\n",
      "Iteration: 8405 lambda_n: 0.9430504009201212 Loss: 1.4182774976570996e-09\n",
      "Iteration: 8406 lambda_n: 1.0248990404050875 Loss: 1.4183176497185617e-09\n",
      "Iteration: 8407 lambda_n: 1.031912205199585 Loss: 1.4183611317624482e-09\n",
      "Iteration: 8408 lambda_n: 1.008908750410205 Loss: 1.418404743860575e-09\n",
      "Iteration: 8409 lambda_n: 0.8929724353694256 Loss: 1.4184472203002767e-09\n",
      "Iteration: 8410 lambda_n: 0.9140908232749063 Loss: 1.4184846790483504e-09\n",
      "Iteration: 8411 lambda_n: 1.0125041584034367 Loss: 1.4185228900335947e-09\n",
      "Iteration: 8412 lambda_n: 0.8952292046408726 Loss: 1.4185650776973083e-09\n",
      "Iteration: 8413 lambda_n: 1.0042907079936008 Loss: 1.4186022343427036e-09\n",
      "Iteration: 8414 lambda_n: 0.9882117322551518 Loss: 1.4186437789158386e-09\n",
      "Iteration: 8415 lambda_n: 0.9077558517992361 Loss: 1.4186845019335468e-09\n",
      "Iteration: 8416 lambda_n: 0.9690660794126101 Loss: 1.418721774354876e-09\n",
      "Iteration: 8417 lambda_n: 0.895862110008646 Loss: 1.4187614273754632e-09\n",
      "Iteration: 8418 lambda_n: 0.9998706680903601 Loss: 1.4187979539817744e-09\n",
      "Iteration: 8419 lambda_n: 0.9525174789686696 Loss: 1.4188385798633766e-09\n",
      "Iteration: 8420 lambda_n: 0.9520323627428682 Loss: 1.4188771358555022e-09\n",
      "Iteration: 8421 lambda_n: 1.0483119276307054 Loss: 1.4189155395930905e-09\n",
      "Iteration: 8422 lambda_n: 0.9383621294626836 Loss: 1.4189576677895718e-09\n",
      "Iteration: 8423 lambda_n: 1.0068950796883804 Loss: 1.4189952334326957e-09\n",
      "Iteration: 8424 lambda_n: 0.9346303068505497 Loss: 1.4190353974812996e-09\n",
      "Iteration: 8425 lambda_n: 0.9623333836182463 Loss: 1.4190725376566477e-09\n",
      "Iteration: 8426 lambda_n: 0.9719246420915079 Loss: 1.4191106405688461e-09\n",
      "Iteration: 8427 lambda_n: 0.9253846405254121 Loss: 1.4191489857273611e-09\n",
      "Iteration: 8428 lambda_n: 0.9089541576063954 Loss: 1.4191853595353701e-09\n",
      "Iteration: 8429 lambda_n: 1.0448831253942883 Loss: 1.4192209602866054e-09\n",
      "Iteration: 8430 lambda_n: 0.9166942512756026 Loss: 1.4192617463452895e-09\n",
      "Iteration: 8431 lambda_n: 0.9244024954084794 Loss: 1.4192973823657167e-09\n",
      "Iteration: 8432 lambda_n: 0.9129222288284212 Loss: 1.4193331954284e-09\n",
      "Iteration: 8433 lambda_n: 0.993732217469603 Loss: 1.4193684358907182e-09\n",
      "Iteration: 8434 lambda_n: 0.9459935517020059 Loss: 1.4194066616775578e-09\n",
      "Iteration: 8435 lambda_n: 0.9218920672091351 Loss: 1.4194429176854272e-09\n",
      "Iteration: 8436 lambda_n: 1.0345572352475596 Loss: 1.4194781163678605e-09\n",
      "Iteration: 8437 lambda_n: 1.0442250764238072 Loss: 1.4195174827762417e-09\n",
      "Iteration: 8438 lambda_n: 0.9841510676311986 Loss: 1.419557056679804e-09\n",
      "Iteration: 8439 lambda_n: 0.9741375623015373 Loss: 1.4195942033190955e-09\n",
      "Iteration: 8440 lambda_n: 0.9651390339709127 Loss: 1.4196308300904898e-09\n",
      "Iteration: 8441 lambda_n: 0.9523124313648036 Loss: 1.419666986210091e-09\n",
      "Iteration: 8442 lambda_n: 0.9402111265721879 Loss: 1.4197025283347676e-09\n",
      "Iteration: 8443 lambda_n: 1.013405070357738 Loss: 1.4197374925194675e-09\n",
      "Iteration: 8444 lambda_n: 1.0312475343261807 Loss: 1.4197750377216417e-09\n",
      "Iteration: 8445 lambda_n: 0.974256420428439 Loss: 1.4198130966554429e-09\n",
      "Iteration: 8446 lambda_n: 1.0361134004135835 Loss: 1.4198489113070814e-09\n",
      "Iteration: 8447 lambda_n: 0.9211816288345318 Loss: 1.419886851510454e-09\n",
      "Iteration: 8448 lambda_n: 0.9743715483658784 Loss: 1.41992044965358e-09\n",
      "Iteration: 8449 lambda_n: 0.9811960133044982 Loss: 1.4199558569443167e-09\n",
      "Iteration: 8450 lambda_n: 0.9830693272572933 Loss: 1.4199913805326395e-09\n",
      "Iteration: 8451 lambda_n: 0.9468637425828853 Loss: 1.4200268357519686e-09\n",
      "Iteration: 8452 lambda_n: 0.9018812774646218 Loss: 1.4200608525152815e-09\n",
      "Iteration: 8453 lambda_n: 0.9718952616148688 Loss: 1.420093135524928e-09\n",
      "Iteration: 8454 lambda_n: 1.0054338391016608 Loss: 1.4201278040332268e-09\n",
      "Iteration: 8455 lambda_n: 1.044114155125423 Loss: 1.4201635287087003e-09\n",
      "Iteration: 8456 lambda_n: 1.0201172033728096 Loss: 1.4202004875634e-09\n",
      "Iteration: 8457 lambda_n: 0.9600149502555976 Loss: 1.4202364436331465e-09\n",
      "Iteration: 8458 lambda_n: 0.9239067331914269 Loss: 1.4202701495098793e-09\n",
      "Iteration: 8459 lambda_n: 0.9952348643922458 Loss: 1.420302460830137e-09\n",
      "Iteration: 8460 lambda_n: 0.9279265631342517 Loss: 1.4203371474306014e-09\n",
      "Iteration: 8461 lambda_n: 0.9757721903642472 Loss: 1.4203693591460007e-09\n",
      "Iteration: 8462 lambda_n: 1.012422153358299 Loss: 1.420403104396108e-09\n",
      "Iteration: 8463 lambda_n: 0.9800442612276462 Loss: 1.4204379865788143e-09\n",
      "Iteration: 8464 lambda_n: 0.9084489226316217 Loss: 1.4204716224893377e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8465 lambda_n: 0.898295799137229 Loss: 1.4205026769681477e-09\n",
      "Iteration: 8466 lambda_n: 1.0260512110323323 Loss: 1.420533272076361e-09\n",
      "Iteration: 8467 lambda_n: 0.9789454993534978 Loss: 1.4205680960507813e-09\n",
      "Iteration: 8468 lambda_n: 0.9877266312850453 Loss: 1.4206011865258217e-09\n",
      "Iteration: 8469 lambda_n: 0.9411634544121522 Loss: 1.4206344437846876e-09\n",
      "Iteration: 8470 lambda_n: 0.923543870706034 Loss: 1.4206660133174732e-09\n",
      "Iteration: 8471 lambda_n: 0.9004397921351489 Loss: 1.4206968725773918e-09\n",
      "Iteration: 8472 lambda_n: 0.9851905347856678 Loss: 1.420726850620507e-09\n",
      "Iteration: 8473 lambda_n: 0.9456845570294106 Loss: 1.4207595347129856e-09\n",
      "Iteration: 8474 lambda_n: 0.9748710028732838 Loss: 1.4207907785315401e-09\n",
      "Iteration: 8475 lambda_n: 1.044784301512504 Loss: 1.4208228727674078e-09\n",
      "Iteration: 8476 lambda_n: 0.9356697744629164 Loss: 1.4208571351662597e-09\n",
      "Iteration: 8477 lambda_n: 1.0288124338732545 Loss: 1.420887687561249e-09\n",
      "Iteration: 8478 lambda_n: 0.9695600961301645 Loss: 1.4209211608396951e-09\n",
      "Iteration: 8479 lambda_n: 0.9192583039013106 Loss: 1.4209525709425068e-09\n",
      "Iteration: 8480 lambda_n: 0.9178682996952928 Loss: 1.420982238558726e-09\n",
      "Iteration: 8481 lambda_n: 0.8975592454491483 Loss: 1.4210117547473887e-09\n",
      "Iteration: 8482 lambda_n: 0.9361217948667869 Loss: 1.4210405095067064e-09\n",
      "Iteration: 8483 lambda_n: 1.0288678542432013 Loss: 1.4210703944959864e-09\n",
      "Iteration: 8484 lambda_n: 0.9685745560033119 Loss: 1.421103115376592e-09\n",
      "Iteration: 8485 lambda_n: 0.924731488278086 Loss: 1.4211337912459161e-09\n",
      "Iteration: 8486 lambda_n: 0.9137924180353664 Loss: 1.4211629711649456e-09\n",
      "Iteration: 8487 lambda_n: 0.9386495466330184 Loss: 1.4211916940322698e-09\n",
      "Iteration: 8488 lambda_n: 0.9832969369243982 Loss: 1.4212210888413403e-09\n",
      "Iteration: 8489 lambda_n: 0.9672574397013791 Loss: 1.4212517682641853e-09\n",
      "Iteration: 8490 lambda_n: 0.9359218243505467 Loss: 1.4212818266082557e-09\n",
      "Iteration: 8491 lambda_n: 1.0340997172349427 Loss: 1.4213107946450277e-09\n",
      "Iteration: 8492 lambda_n: 1.0200857667266 Loss: 1.4213426863561951e-09\n",
      "Iteration: 8493 lambda_n: 0.9946785816441809 Loss: 1.4213740133176075e-09\n",
      "Iteration: 8494 lambda_n: 0.907382117982122 Loss: 1.4214044342412215e-09\n",
      "Iteration: 8495 lambda_n: 0.9257430152714139 Loss: 1.4214320703520616e-09\n",
      "Iteration: 8496 lambda_n: 0.912225467574849 Loss: 1.4214601649434365e-09\n",
      "Iteration: 8497 lambda_n: 1.0373374063754943 Loss: 1.4214877457876505e-09\n",
      "Iteration: 8498 lambda_n: 0.9079031492122293 Loss: 1.421518990569594e-09\n",
      "Iteration: 8499 lambda_n: 0.9930078769116626 Loss: 1.4215462259470736e-09\n",
      "Iteration: 8500 lambda_n: 1.0381902599517658 Loss: 1.4215759004688328e-09\n",
      "Iteration: 8501 lambda_n: 0.9376058975272454 Loss: 1.4216068053167772e-09\n",
      "Iteration: 8502 lambda_n: 0.9103566027307247 Loss: 1.4216345981882832e-09\n",
      "Iteration: 8503 lambda_n: 0.9069997889111232 Loss: 1.4216614753343033e-09\n",
      "Iteration: 8504 lambda_n: 0.9825278174872961 Loss: 1.4216881595134637e-09\n",
      "Iteration: 8505 lambda_n: 0.914295142653162 Loss: 1.4217169563234852e-09\n",
      "Iteration: 8506 lambda_n: 0.9775117854603024 Loss: 1.4217436461417802e-09\n",
      "Iteration: 8507 lambda_n: 1.0112382635975057 Loss: 1.421772078730487e-09\n",
      "Iteration: 8508 lambda_n: 0.9654956992439149 Loss: 1.421801371836837e-09\n",
      "Iteration: 8509 lambda_n: 0.9254289528520169 Loss: 1.4218292268127822e-09\n",
      "Iteration: 8510 lambda_n: 0.9632073122673155 Loss: 1.421855819142887e-09\n",
      "Iteration: 8511 lambda_n: 0.9450070688687412 Loss: 1.4218833897693989e-09\n",
      "Iteration: 8512 lambda_n: 0.9920452929230305 Loss: 1.4219103357866705e-09\n",
      "Iteration: 8513 lambda_n: 0.9969121454661776 Loss: 1.421938511667718e-09\n",
      "Iteration: 8514 lambda_n: 0.9913875951490524 Loss: 1.4219667139803803e-09\n",
      "Iteration: 8515 lambda_n: 0.9252770161840062 Loss: 1.4219946405778824e-09\n",
      "Iteration: 8516 lambda_n: 0.9244905214741861 Loss: 1.4220206032462732e-09\n",
      "Iteration: 8517 lambda_n: 1.0442339059003798 Loss: 1.4220464451665088e-09\n",
      "Iteration: 8518 lambda_n: 1.015908614475221 Loss: 1.422075518866953e-09\n",
      "Iteration: 8519 lambda_n: 0.9779489089499956 Loss: 1.4221036891686758e-09\n",
      "Iteration: 8520 lambda_n: 1.0433654449577683 Loss: 1.4221306910195192e-09\n",
      "Iteration: 8521 lambda_n: 0.9964741901393469 Loss: 1.4221593780104138e-09\n",
      "Iteration: 8522 lambda_n: 0.9612371625510008 Loss: 1.4221866628366778e-09\n",
      "Iteration: 8523 lambda_n: 0.9573967796646385 Loss: 1.4222128730201048e-09\n",
      "Iteration: 8524 lambda_n: 1.0022983138559003 Loss: 1.4222388780941062e-09\n",
      "Iteration: 8525 lambda_n: 0.945913970090264 Loss: 1.422265989838895e-09\n",
      "Iteration: 8526 lambda_n: 0.9132142679641134 Loss: 1.4222914760831795e-09\n",
      "Iteration: 8527 lambda_n: 0.9851991784018591 Loss: 1.4223159791495884e-09\n",
      "Iteration: 8528 lambda_n: 0.9866065872566453 Loss: 1.4223423213700895e-09\n",
      "Iteration: 8529 lambda_n: 0.9349551707417354 Loss: 1.4223685890962376e-09\n",
      "Iteration: 8530 lambda_n: 0.9641246940474091 Loss: 1.4223933840220928e-09\n",
      "Iteration: 8531 lambda_n: 0.9604804577289039 Loss: 1.4224188502288842e-09\n",
      "Iteration: 8532 lambda_n: 0.9813615434276055 Loss: 1.4224441223124713e-09\n",
      "Iteration: 8533 lambda_n: 1.0173086165445173 Loss: 1.422469835980444e-09\n",
      "Iteration: 8534 lambda_n: 0.94842512480441 Loss: 1.422496385643547e-09\n",
      "Iteration: 8535 lambda_n: 0.9427387657159054 Loss: 1.422521035048857e-09\n",
      "Iteration: 8536 lambda_n: 1.0202735269488952 Loss: 1.4225454369965496e-09\n",
      "Iteration: 8537 lambda_n: 1.04451597631679 Loss: 1.4225717460284326e-09\n",
      "Iteration: 8538 lambda_n: 0.9944840474530687 Loss: 1.422598561936503e-09\n",
      "Iteration: 8539 lambda_n: 1.036104241021592 Loss: 1.4226239866818613e-09\n",
      "Iteration: 8540 lambda_n: 1.019612340467076 Loss: 1.4226503614013097e-09\n",
      "Iteration: 8541 lambda_n: 0.9786819254398543 Loss: 1.4226762089663435e-09\n",
      "Iteration: 8542 lambda_n: 0.9454963311930186 Loss: 1.4227009121696354e-09\n",
      "Iteration: 8543 lambda_n: 0.9444374629288111 Loss: 1.4227246757735856e-09\n",
      "Iteration: 8544 lambda_n: 0.9003487420717746 Loss: 1.4227483263475814e-09\n",
      "Iteration: 8545 lambda_n: 0.9095520274927806 Loss: 1.4227707774639753e-09\n",
      "Iteration: 8546 lambda_n: 1.0346855041135183 Loss: 1.422793378935036e-09\n",
      "Iteration: 8547 lambda_n: 0.9547536327249148 Loss: 1.4228189898364144e-09\n",
      "Iteration: 8548 lambda_n: 0.9602854908856139 Loss: 1.4228425169370967e-09\n",
      "Iteration: 8549 lambda_n: 0.9705910706868897 Loss: 1.422866090381794e-09\n",
      "Iteration: 8550 lambda_n: 0.9048055255242097 Loss: 1.4228898195561505e-09\n",
      "Iteration: 8551 lambda_n: 1.0066649318905452 Loss: 1.422911844914174e-09\n",
      "Iteration: 8552 lambda_n: 0.9312463497161786 Loss: 1.4229362644829898e-09\n",
      "Iteration: 8553 lambda_n: 0.9050140020697868 Loss: 1.4229587542960043e-09\n",
      "Iteration: 8554 lambda_n: 1.022334367870735 Loss: 1.4229805308662832e-09\n",
      "Iteration: 8555 lambda_n: 0.9363293827921629 Loss: 1.4230050340596095e-09\n",
      "Iteration: 8556 lambda_n: 0.9664371123124214 Loss: 1.4230273769458245e-09\n",
      "Iteration: 8557 lambda_n: 1.0429190985099344 Loss: 1.4230503510010828e-09\n",
      "Iteration: 8558 lambda_n: 0.9512573283350589 Loss: 1.423075039735466e-09\n",
      "Iteration: 8559 lambda_n: 0.9409445553752273 Loss: 1.4230974653635599e-09\n",
      "Iteration: 8560 lambda_n: 1.0072824450393028 Loss: 1.4231195554429527e-09\n",
      "Iteration: 8561 lambda_n: 0.9224695698903421 Loss: 1.4231431116512497e-09\n",
      "Iteration: 8562 lambda_n: 1.0082344563751913 Loss: 1.4231645919149343e-09\n",
      "Iteration: 8563 lambda_n: 0.9022466989204121 Loss: 1.423187974256934e-09\n",
      "Iteration: 8564 lambda_n: 0.9970382543093017 Loss: 1.4232088143182628e-09\n",
      "Iteration: 8565 lambda_n: 0.991938367762458 Loss: 1.4232317557093393e-09\n",
      "Iteration: 8566 lambda_n: 1.0050734160781827 Loss: 1.4232544808520861e-09\n",
      "Iteration: 8567 lambda_n: 0.8923104329246494 Loss: 1.4232774126436507e-09\n",
      "Iteration: 8568 lambda_n: 0.9660144174319892 Loss: 1.4232976861150826e-09\n",
      "Iteration: 8569 lambda_n: 0.8944454726380338 Loss: 1.4233195510883687e-09\n",
      "Iteration: 8570 lambda_n: 1.0004220532616839 Loss: 1.4233397134389833e-09\n",
      "Iteration: 8571 lambda_n: 0.9879901649668823 Loss: 1.423362179302078e-09\n",
      "Iteration: 8572 lambda_n: 0.946798781808526 Loss: 1.423384268485592e-09\n",
      "Iteration: 8573 lambda_n: 0.9815285467073108 Loss: 1.4234053526036102e-09\n",
      "Iteration: 8574 lambda_n: 0.9343229130650026 Loss: 1.4234271186096849e-09\n",
      "Iteration: 8575 lambda_n: 0.9192008766787593 Loss: 1.423447755753037e-09\n",
      "Iteration: 8576 lambda_n: 1.0063316151069657 Loss: 1.4234679773765633e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8577 lambda_n: 0.9298244841662727 Loss: 1.423490025860646e-09\n",
      "Iteration: 8578 lambda_n: 0.9509884090825796 Loss: 1.4235103167444683e-09\n",
      "Iteration: 8579 lambda_n: 1.0113499430897246 Loss: 1.4235309847589346e-09\n",
      "Iteration: 8580 lambda_n: 0.9045569848077822 Loss: 1.4235528754244284e-09\n",
      "Iteration: 8581 lambda_n: 0.9400447446930043 Loss: 1.4235723666430574e-09\n",
      "Iteration: 8582 lambda_n: 0.9003257805197669 Loss: 1.423592551350251e-09\n",
      "Iteration: 8583 lambda_n: 0.9906748695399206 Loss: 1.4236118044841043e-09\n",
      "Iteration: 8584 lambda_n: 0.9139048871187497 Loss: 1.4236329053458878e-09\n",
      "Iteration: 8585 lambda_n: 0.9478301186592035 Loss: 1.4236522884697361e-09\n",
      "Iteration: 8586 lambda_n: 0.9955506249379205 Loss: 1.4236723149237623e-09\n",
      "Iteration: 8587 lambda_n: 0.9015154264207944 Loss: 1.423693264270215e-09\n",
      "Iteration: 8588 lambda_n: 0.9306962885449179 Loss: 1.4237121555536266e-09\n",
      "Iteration: 8589 lambda_n: 0.9897360570723491 Loss: 1.423731585157889e-09\n",
      "Iteration: 8590 lambda_n: 0.9799712766889421 Loss: 1.423752163981496e-09\n",
      "Iteration: 8591 lambda_n: 0.9179849385691916 Loss: 1.4237724488661046e-09\n",
      "Iteration: 8592 lambda_n: 0.9230153724962097 Loss: 1.4237913776103733e-09\n",
      "Iteration: 8593 lambda_n: 0.945049966245536 Loss: 1.4238103338934422e-09\n",
      "Iteration: 8594 lambda_n: 1.0291503788514698 Loss: 1.423829665338101e-09\n",
      "Iteration: 8595 lambda_n: 0.9326518361699275 Loss: 1.4238506284039152e-09\n",
      "Iteration: 8596 lambda_n: 1.0459193400951854 Loss: 1.4238695468027685e-09\n",
      "Iteration: 8597 lambda_n: 1.0017295190351698 Loss: 1.4238906789655154e-09\n",
      "Iteration: 8598 lambda_n: 0.9859434903387782 Loss: 1.4239108271355861e-09\n",
      "Iteration: 8599 lambda_n: 0.9753669838505805 Loss: 1.4239305728146288e-09\n",
      "Iteration: 8600 lambda_n: 0.9668441343903 Loss: 1.4239500184627104e-09\n",
      "Iteration: 8601 lambda_n: 1.0425325115947373 Loss: 1.423969220397562e-09\n",
      "Iteration: 8602 lambda_n: 0.9265839886991368 Loss: 1.423989835998488e-09\n",
      "Iteration: 8603 lambda_n: 1.014250667581318 Loss: 1.424008080587724e-09\n",
      "Iteration: 8604 lambda_n: 0.8935832222167283 Loss: 1.4240279665512315e-09\n",
      "Iteration: 8605 lambda_n: 1.0396854944538996 Loss: 1.4240454139917256e-09\n",
      "Iteration: 8606 lambda_n: 0.9656081604482368 Loss: 1.4240656314491648e-09\n",
      "Iteration: 8607 lambda_n: 0.9045481323147296 Loss: 1.424084330624176e-09\n",
      "Iteration: 8608 lambda_n: 1.0273240379678596 Loss: 1.4241017711894393e-09\n",
      "Iteration: 8609 lambda_n: 0.9460547136651741 Loss: 1.424121499581715e-09\n",
      "Iteration: 8610 lambda_n: 1.0324860045867452 Loss: 1.4241395891008032e-09\n",
      "Iteration: 8611 lambda_n: 0.9907305702028922 Loss: 1.424159252515476e-09\n",
      "Iteration: 8612 lambda_n: 1.021268821619489 Loss: 1.4241780330206916e-09\n",
      "Iteration: 8613 lambda_n: 1.0272738780259618 Loss: 1.4241973140210551e-09\n",
      "Iteration: 8614 lambda_n: 0.9948592882211604 Loss: 1.4242166194956997e-09\n",
      "Iteration: 8615 lambda_n: 1.0493429869442763 Loss: 1.424235237625629e-09\n",
      "Iteration: 8616 lambda_n: 0.9412285943240791 Loss: 1.424254789080625e-09\n",
      "Iteration: 8617 lambda_n: 0.9508009693192003 Loss: 1.4242722467610343e-09\n",
      "Iteration: 8618 lambda_n: 0.9561337728448699 Loss: 1.4242898099915415e-09\n",
      "Iteration: 8619 lambda_n: 0.9752660921644183 Loss: 1.4243073970727648e-09\n",
      "Iteration: 8620 lambda_n: 1.0468569647176706 Loss: 1.4243252668100948e-09\n",
      "Iteration: 8621 lambda_n: 1.0386046708556416 Loss: 1.4243443609841086e-09\n",
      "Iteration: 8622 lambda_n: 0.9285477207686047 Loss: 1.4243632254526326e-09\n",
      "Iteration: 8623 lambda_n: 0.9421470409639984 Loss: 1.4243800146396898e-09\n",
      "Iteration: 8624 lambda_n: 1.0337104403392936 Loss: 1.4243969790588533e-09\n",
      "Iteration: 8625 lambda_n: 0.9571469723193221 Loss: 1.4244155122230177e-09\n",
      "Iteration: 8626 lambda_n: 0.9319206385271972 Loss: 1.4244326015632057e-09\n",
      "Iteration: 8627 lambda_n: 0.9167303357983326 Loss: 1.4244491717624638e-09\n",
      "Iteration: 8628 lambda_n: 0.8994096095585213 Loss: 1.4244654061073025e-09\n",
      "Iteration: 8629 lambda_n: 1.0145983225152495 Loss: 1.4244812668259499e-09\n",
      "Iteration: 8630 lambda_n: 0.9843766183705681 Loss: 1.4244990951378035e-09\n",
      "Iteration: 8631 lambda_n: 0.9148895239392412 Loss: 1.424516310498631e-09\n",
      "Iteration: 8632 lambda_n: 1.043588424571837 Loss: 1.424532244903653e-09\n",
      "Iteration: 8633 lambda_n: 0.950950654196376 Loss: 1.4245503510272722e-09\n",
      "Iteration: 8634 lambda_n: 0.9743436932623082 Loss: 1.4245667747680252e-09\n",
      "Iteration: 8635 lambda_n: 0.9786506267415915 Loss: 1.424583531937796e-09\n",
      "Iteration: 8636 lambda_n: 0.955463300134224 Loss: 1.424600290812838e-09\n",
      "Iteration: 8637 lambda_n: 0.9431169607603974 Loss: 1.424616583810851e-09\n",
      "Iteration: 8638 lambda_n: 0.9881442467815484 Loss: 1.42463259548504e-09\n",
      "Iteration: 8639 lambda_n: 0.9693266651355984 Loss: 1.4246493091045148e-09\n",
      "Iteration: 8640 lambda_n: 0.9655112573212459 Loss: 1.4246656330993188e-09\n",
      "Iteration: 8641 lambda_n: 1.0459546151041716 Loss: 1.4246818237451408e-09\n",
      "Iteration: 8642 lambda_n: 0.9747298935688926 Loss: 1.4246992903158113e-09\n",
      "Iteration: 8643 lambda_n: 0.906735594543421 Loss: 1.4247154942940323e-09\n",
      "Iteration: 8644 lambda_n: 1.0435367943163896 Loss: 1.4247305025346578e-09\n",
      "Iteration: 8645 lambda_n: 1.0294620510499872 Loss: 1.4247477076102492e-09\n",
      "Iteration: 8646 lambda_n: 0.9473302298598019 Loss: 1.424764600438569e-09\n",
      "Iteration: 8647 lambda_n: 0.9881990010669484 Loss: 1.4247800795707731e-09\n",
      "Iteration: 8648 lambda_n: 0.9586496635894772 Loss: 1.4247961586665132e-09\n",
      "Iteration: 8649 lambda_n: 0.9220511919533272 Loss: 1.4248116902176555e-09\n",
      "Iteration: 8650 lambda_n: 1.0409008992242088 Loss: 1.4248265615758638e-09\n",
      "Iteration: 8651 lambda_n: 1.0274574536171046 Loss: 1.4248432886248556e-09\n",
      "Iteration: 8652 lambda_n: 0.9993879031952335 Loss: 1.424859723514258e-09\n",
      "Iteration: 8653 lambda_n: 1.0376259935413632 Loss: 1.424875637936992e-09\n",
      "Iteration: 8654 lambda_n: 0.9774340438451136 Loss: 1.424892082057107e-09\n",
      "Iteration: 8655 lambda_n: 0.9295633205264163 Loss: 1.4249075078162586e-09\n",
      "Iteration: 8656 lambda_n: 0.9737517054821081 Loss: 1.4249221173420538e-09\n",
      "Iteration: 8657 lambda_n: 0.9634219503798083 Loss: 1.424937354429179e-09\n",
      "Iteration: 8658 lambda_n: 1.0231744224338262 Loss: 1.4249523702764555e-09\n",
      "Iteration: 8659 lambda_n: 0.9262448705679261 Loss: 1.4249682445350426e-09\n",
      "Iteration: 8660 lambda_n: 0.9519890611469948 Loss: 1.4249825558836757e-09\n",
      "Iteration: 8661 lambda_n: 0.9260466274100955 Loss: 1.42499720084076e-09\n",
      "Iteration: 8662 lambda_n: 0.939480318955731 Loss: 1.4250113898886696e-09\n",
      "Iteration: 8663 lambda_n: 1.0155677818713955 Loss: 1.425025726880819e-09\n",
      "Iteration: 8664 lambda_n: 1.030387523136469 Loss: 1.4250411602355722e-09\n",
      "Iteration: 8665 lambda_n: 0.9194966689618228 Loss: 1.425056746362711e-09\n",
      "Iteration: 8666 lambda_n: 0.9697660277323613 Loss: 1.4250705960934287e-09\n",
      "Iteration: 8667 lambda_n: 1.0109280529415026 Loss: 1.4250851439243646e-09\n",
      "Iteration: 8668 lambda_n: 1.003787785331179 Loss: 1.4251002387850466e-09\n",
      "Iteration: 8669 lambda_n: 0.9593310251938841 Loss: 1.4251151676280142e-09\n",
      "Iteration: 8670 lambda_n: 1.0018464296253948 Loss: 1.4251293712849317e-09\n",
      "Iteration: 8671 lambda_n: 1.039053446213551 Loss: 1.4251441417510385e-09\n",
      "Iteration: 8672 lambda_n: 0.9050726181408025 Loss: 1.4251593896581205e-09\n",
      "Iteration: 8673 lambda_n: 0.93533615665524 Loss: 1.4251726146768372e-09\n",
      "Iteration: 8674 lambda_n: 0.924344834651065 Loss: 1.4251862276528813e-09\n",
      "Iteration: 8675 lambda_n: 0.9037157513554326 Loss: 1.425199622323964e-09\n",
      "Iteration: 8676 lambda_n: 0.9141982879749586 Loss: 1.4252126698829175e-09\n",
      "Iteration: 8677 lambda_n: 0.9793689877036444 Loss: 1.4252258143385336e-09\n",
      "Iteration: 8678 lambda_n: 1.0466402823719578 Loss: 1.4252398385923264e-09\n",
      "Iteration: 8679 lambda_n: 0.8947332982828283 Loss: 1.425254759502708e-09\n",
      "Iteration: 8680 lambda_n: 0.975953031535842 Loss: 1.4252674574607349e-09\n",
      "Iteration: 8681 lambda_n: 0.9654641920687308 Loss: 1.4252812499803123e-09\n",
      "Iteration: 8682 lambda_n: 0.9834347863147215 Loss: 1.4252948432854814e-09\n",
      "Iteration: 8683 lambda_n: 1.0479585996700864 Loss: 1.4253086258365888e-09\n",
      "Iteration: 8684 lambda_n: 0.9025977005667654 Loss: 1.4253232533827574e-09\n",
      "Iteration: 8685 lambda_n: 0.922501509024185 Loss: 1.4253357866860996e-09\n",
      "Iteration: 8686 lambda_n: 0.9308484176444132 Loss: 1.4253485516304387e-09\n",
      "Iteration: 8687 lambda_n: 0.9988036570366624 Loss: 1.4253613786414624e-09\n",
      "Iteration: 8688 lambda_n: 1.0257109165334912 Loss: 1.425375082125955e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8689 lambda_n: 1.008910537718928 Loss: 1.4253890971512432e-09\n",
      "Iteration: 8690 lambda_n: 1.046866295828376 Loss: 1.4254028203796827e-09\n",
      "Iteration: 8691 lambda_n: 1.0248774400875191 Loss: 1.4254169953021513e-09\n",
      "Iteration: 8692 lambda_n: 1.0298898414424147 Loss: 1.4254308056240512e-09\n",
      "Iteration: 8693 lambda_n: 0.9412038457048176 Loss: 1.4254446206192023e-09\n",
      "Iteration: 8694 lambda_n: 0.9613165824600158 Loss: 1.4254571933543223e-09\n",
      "Iteration: 8695 lambda_n: 0.9179841824726996 Loss: 1.4254699790263143e-09\n",
      "Iteration: 8696 lambda_n: 0.8977531957687429 Loss: 1.4254821346869672e-09\n",
      "Iteration: 8697 lambda_n: 0.9469529879084578 Loss: 1.4254939770732143e-09\n",
      "Iteration: 8698 lambda_n: 0.9280449376149146 Loss: 1.4255064132021745e-09\n",
      "Iteration: 8699 lambda_n: 1.040054454208082 Loss: 1.4255185561969363e-09\n",
      "Iteration: 8700 lambda_n: 0.9380866334248935 Loss: 1.425532107153941e-09\n",
      "Iteration: 8701 lambda_n: 0.9454693900517127 Loss: 1.425544271677406e-09\n",
      "Iteration: 8702 lambda_n: 0.997986773980036 Loss: 1.425556479451643e-09\n",
      "Iteration: 8703 lambda_n: 0.9936945210498638 Loss: 1.4255693149089533e-09\n",
      "Iteration: 8704 lambda_n: 0.9633106818450441 Loss: 1.425582038539442e-09\n",
      "Iteration: 8705 lambda_n: 0.9711548663834509 Loss: 1.4255943189234375e-09\n",
      "Iteration: 8706 lambda_n: 1.000894001682767 Loss: 1.4256066434999845e-09\n",
      "Iteration: 8707 lambda_n: 1.0480046274056651 Loss: 1.4256192959727428e-09\n",
      "Iteration: 8708 lambda_n: 1.0144331067718428 Loss: 1.4256324832804553e-09\n",
      "Iteration: 8709 lambda_n: 1.049316039408057 Loss: 1.4256451862296731e-09\n",
      "Iteration: 8710 lambda_n: 0.894612761109684 Loss: 1.4256582727234053e-09\n",
      "Iteration: 8711 lambda_n: 0.9462389434293595 Loss: 1.4256693719140984e-09\n",
      "Iteration: 8712 lambda_n: 0.9108746307161815 Loss: 1.4256810698508866e-09\n",
      "Iteration: 8713 lambda_n: 0.9053687026509484 Loss: 1.4256922808320727e-09\n",
      "Iteration: 8714 lambda_n: 1.0217096236958336 Loss: 1.4257033818158028e-09\n",
      "Iteration: 8715 lambda_n: 0.9706688646138947 Loss: 1.4257158580467653e-09\n",
      "Iteration: 8716 lambda_n: 1.03995379111518 Loss: 1.4257276543419739e-09\n",
      "Iteration: 8717 lambda_n: 0.9665056294617537 Loss: 1.4257402417837444e-09\n",
      "Iteration: 8718 lambda_n: 0.9285205067771587 Loss: 1.4257518829278038e-09\n",
      "Iteration: 8719 lambda_n: 0.9352352616524572 Loss: 1.425763021405786e-09\n",
      "Iteration: 8720 lambda_n: 1.047953627628158 Loss: 1.4257741912919533e-09\n",
      "Iteration: 8721 lambda_n: 1.0192119360680356 Loss: 1.425786654648366e-09\n",
      "Iteration: 8722 lambda_n: 0.9270659571105466 Loss: 1.4257987246161603e-09\n",
      "Iteration: 8723 lambda_n: 1.0490291173480946 Loss: 1.4258096507357434e-09\n",
      "Iteration: 8724 lambda_n: 1.0260034312827444 Loss: 1.425821966897955e-09\n",
      "Iteration: 8725 lambda_n: 1.033757911895575 Loss: 1.4258339573206775e-09\n",
      "Iteration: 8726 lambda_n: 1.028808151789261 Loss: 1.42584597569391e-09\n",
      "Iteration: 8727 lambda_n: 1.0103403731867282 Loss: 1.425857886398315e-09\n",
      "Iteration: 8728 lambda_n: 0.9624870500607132 Loss: 1.4258695273704862e-09\n",
      "Iteration: 8729 lambda_n: 0.9180038790098056 Loss: 1.425880573028398e-09\n",
      "Iteration: 8730 lambda_n: 1.0259064196553227 Loss: 1.425891060340517e-09\n",
      "Iteration: 8731 lambda_n: 0.9542578075353637 Loss: 1.4259027302423844e-09\n",
      "Iteration: 8732 lambda_n: 0.9557244822654519 Loss: 1.4259135334216637e-09\n",
      "Iteration: 8733 lambda_n: 0.9700782988069867 Loss: 1.425924312210394e-09\n",
      "Iteration: 8734 lambda_n: 0.9213117169729625 Loss: 1.4259352050385197e-09\n",
      "Iteration: 8735 lambda_n: 1.0314747004107976 Loss: 1.4259455044975458e-09\n",
      "Iteration: 8736 lambda_n: 0.9018342267272453 Loss: 1.42595698450447e-09\n",
      "Iteration: 8737 lambda_n: 1.0399498900353217 Loss: 1.4259669801269124e-09\n",
      "Iteration: 8738 lambda_n: 0.9103816034913372 Loss: 1.4259784587357546e-09\n",
      "Iteration: 8739 lambda_n: 1.0062220225347331 Loss: 1.4259884564320674e-09\n",
      "Iteration: 8740 lambda_n: 1.0162032662211244 Loss: 1.4259994691151438e-09\n",
      "Iteration: 8741 lambda_n: 1.0280203631326124 Loss: 1.4260105374854502e-09\n",
      "Iteration: 8742 lambda_n: 0.9323985262810494 Loss: 1.4260216836702935e-09\n",
      "Iteration: 8743 lambda_n: 1.0273424212796294 Loss: 1.4260317441660596e-09\n",
      "Iteration: 8744 lambda_n: 0.977642017388952 Loss: 1.4260427874168533e-09\n",
      "Iteration: 8745 lambda_n: 0.9574457462253928 Loss: 1.4260532464604436e-09\n",
      "Iteration: 8746 lambda_n: 0.9608936692402548 Loss: 1.4260634443706728e-09\n",
      "Iteration: 8747 lambda_n: 0.9797895724969946 Loss: 1.4260736373096828e-09\n",
      "Iteration: 8748 lambda_n: 0.8981416255277695 Loss: 1.42608398670962e-09\n",
      "Iteration: 8749 lambda_n: 1.0446325461515673 Loss: 1.4260934284361238e-09\n",
      "Iteration: 8750 lambda_n: 1.0268001788391252 Loss: 1.4261043710032341e-09\n",
      "Iteration: 8751 lambda_n: 0.9274528612495072 Loss: 1.426115075063334e-09\n",
      "Iteration: 8752 lambda_n: 0.9274779445602387 Loss: 1.426124695199709e-09\n",
      "Iteration: 8753 lambda_n: 0.9612631896768512 Loss: 1.4261342783769066e-09\n",
      "Iteration: 8754 lambda_n: 0.9231551163512138 Loss: 1.4261441709330456e-09\n",
      "Iteration: 8755 lambda_n: 1.045905092482271 Loss: 1.4261536308305972e-09\n",
      "Iteration: 8756 lambda_n: 0.9534086926164596 Loss: 1.4261643032141115e-09\n",
      "Iteration: 8757 lambda_n: 0.983325931245078 Loss: 1.4261739871631922e-09\n",
      "Iteration: 8758 lambda_n: 0.9264623902725063 Loss: 1.4261839305500388e-09\n",
      "Iteration: 8759 lambda_n: 1.0029064683245204 Loss: 1.4261932592884467e-09\n",
      "Iteration: 8760 lambda_n: 0.9036107374528963 Loss: 1.4262033113926011e-09\n",
      "Iteration: 8761 lambda_n: 0.9170566841018433 Loss: 1.4262123314142193e-09\n",
      "Iteration: 8762 lambda_n: 0.9143174112291649 Loss: 1.4262214492184855e-09\n",
      "Iteration: 8763 lambda_n: 0.9010453526881 Loss: 1.4262305007935806e-09\n",
      "Iteration: 8764 lambda_n: 1.0455696175632443 Loss: 1.426239386453357e-09\n",
      "Iteration: 8765 lambda_n: 0.9313866126032616 Loss: 1.4262496537511086e-09\n",
      "Iteration: 8766 lambda_n: 0.9929589525886819 Loss: 1.4262587570594698e-09\n",
      "Iteration: 8767 lambda_n: 0.9435478392021803 Loss: 1.426268422768114e-09\n",
      "Iteration: 8768 lambda_n: 0.9037468784286803 Loss: 1.426277566186533e-09\n",
      "Iteration: 8769 lambda_n: 1.041696867504179 Loss: 1.4262862863989964e-09\n",
      "Iteration: 8770 lambda_n: 0.9615171351241457 Loss: 1.4262962937517095e-09\n",
      "Iteration: 8771 lambda_n: 1.0469440053115755 Loss: 1.4263054940647754e-09\n",
      "Iteration: 8772 lambda_n: 0.9722312260664161 Loss: 1.4263154662969212e-09\n",
      "Iteration: 8773 lambda_n: 1.048697977407316 Loss: 1.4263246781596134e-09\n",
      "Iteration: 8774 lambda_n: 0.9788365923921546 Loss: 1.4263345756866618e-09\n",
      "Iteration: 8775 lambda_n: 1.0051475201455815 Loss: 1.4263437712024264e-09\n",
      "Iteration: 8776 lambda_n: 0.9933006584263671 Loss: 1.426353172086238e-09\n",
      "Iteration: 8777 lambda_n: 0.8976802371426104 Loss: 1.4263624194402155e-09\n",
      "Iteration: 8778 lambda_n: 0.972411534468448 Loss: 1.4263707385543357e-09\n",
      "Iteration: 8779 lambda_n: 1.0485177683745093 Loss: 1.4263797139709193e-09\n",
      "Iteration: 8780 lambda_n: 0.9397239114612449 Loss: 1.426389345088765e-09\n",
      "Iteration: 8781 lambda_n: 0.9754669009597887 Loss: 1.4263979416612596e-09\n",
      "Iteration: 8782 lambda_n: 0.9926913716507033 Loss: 1.4264068270915635e-09\n",
      "Iteration: 8783 lambda_n: 0.9084506432534445 Loss: 1.4264158287522528e-09\n",
      "Iteration: 8784 lambda_n: 0.9079621769531876 Loss: 1.4264240250986927e-09\n",
      "Iteration: 8785 lambda_n: 1.0248585865524946 Loss: 1.4264321888643692e-09\n",
      "Iteration: 8786 lambda_n: 1.0129461839652854 Loss: 1.4264413656570958e-09\n",
      "Iteration: 8787 lambda_n: 1.049266663298549 Loss: 1.4264503941877547e-09\n",
      "Iteration: 8788 lambda_n: 0.935322173235399 Loss: 1.426459699503836e-09\n",
      "Iteration: 8789 lambda_n: 0.9173081784001494 Loss: 1.4264679615788398e-09\n",
      "Iteration: 8790 lambda_n: 1.0000734610056112 Loss: 1.4264760287211896e-09\n",
      "Iteration: 8791 lambda_n: 1.0164769186609668 Loss: 1.4264847859560048e-09\n",
      "Iteration: 8792 lambda_n: 1.0031764709294926 Loss: 1.4264936433224646e-09\n",
      "Iteration: 8793 lambda_n: 0.9028633133571135 Loss: 1.4265023512452682e-09\n",
      "Iteration: 8794 lambda_n: 0.9776790310291132 Loss: 1.4265101500588238e-09\n",
      "Iteration: 8795 lambda_n: 0.9372962426628072 Loss: 1.4265185632847412e-09\n",
      "Iteration: 8796 lambda_n: 0.9903248077060223 Loss: 1.4265265881128413e-09\n",
      "Iteration: 8797 lambda_n: 0.9426474109400849 Loss: 1.4265350357951164e-09\n",
      "Iteration: 8798 lambda_n: 0.9034397766380207 Loss: 1.4265430415306793e-09\n",
      "Iteration: 8799 lambda_n: 0.9647241287587986 Loss: 1.4265506775461436e-09\n",
      "Iteration: 8800 lambda_n: 0.989376370471288 Loss: 1.426558802877839e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8801 lambda_n: 0.9806193364830673 Loss: 1.426567099072704e-09\n",
      "Iteration: 8802 lambda_n: 0.9973411375069764 Loss: 1.426575285402805e-09\n",
      "Iteration: 8803 lambda_n: 0.9118829887775494 Loss: 1.4265835733251416e-09\n",
      "Iteration: 8804 lambda_n: 1.0379769636156162 Loss: 1.4265911181242283e-09\n",
      "Iteration: 8805 lambda_n: 0.9635532106042225 Loss: 1.4265996665073936e-09\n",
      "Iteration: 8806 lambda_n: 1.0428230687361963 Loss: 1.4266075697711568e-09\n",
      "Iteration: 8807 lambda_n: 0.9793385422010611 Loss: 1.4266160844264663e-09\n",
      "Iteration: 8808 lambda_n: 0.9336696064826656 Loss: 1.4266240423131324e-09\n",
      "Iteration: 8809 lambda_n: 0.9738930368956812 Loss: 1.4266315957354986e-09\n",
      "Iteration: 8810 lambda_n: 0.9546384403247512 Loss: 1.4266394398748107e-09\n",
      "Iteration: 8811 lambda_n: 0.9585990945683828 Loss: 1.4266470970948264e-09\n",
      "Iteration: 8812 lambda_n: 1.0477659917447704 Loss: 1.426654750470883e-09\n",
      "Iteration: 8813 lambda_n: 0.9084236784997203 Loss: 1.4266630813858853e-09\n",
      "Iteration: 8814 lambda_n: 0.9109578801320576 Loss: 1.4266702715897162e-09\n",
      "Iteration: 8815 lambda_n: 1.015620821330526 Loss: 1.4266774498697433e-09\n",
      "Iteration: 8816 lambda_n: 1.042221845847843 Loss: 1.4266854239172466e-09\n",
      "Iteration: 8817 lambda_n: 0.9894769397077996 Loss: 1.4266935668184888e-09\n",
      "Iteration: 8818 lambda_n: 1.0408599812463557 Loss: 1.4267012586600033e-09\n",
      "Iteration: 8819 lambda_n: 1.013706641677967 Loss: 1.4267093187559224e-09\n",
      "Iteration: 8820 lambda_n: 0.9782192031319371 Loss: 1.426717124508161e-09\n",
      "Iteration: 8821 lambda_n: 0.9867195043402911 Loss: 1.4267246307517953e-09\n",
      "Iteration: 8822 lambda_n: 0.9936294090943769 Loss: 1.4267321675916292e-09\n",
      "Iteration: 8823 lambda_n: 0.9704698841539547 Loss: 1.4267397194808134e-09\n",
      "Iteration: 8824 lambda_n: 0.9047228719034461 Loss: 1.4267470653459058e-09\n",
      "Iteration: 8825 lambda_n: 0.9557871455061564 Loss: 1.4267538809546102e-09\n",
      "Iteration: 8826 lambda_n: 0.922501049380348 Loss: 1.4267610553954804e-09\n",
      "Iteration: 8827 lambda_n: 0.9023267624713929 Loss: 1.4267679452090288e-09\n",
      "Iteration: 8828 lambda_n: 0.9210733573194005 Loss: 1.4267746596738982e-09\n",
      "Iteration: 8829 lambda_n: 0.9421006322047751 Loss: 1.426781482262896e-09\n",
      "Iteration: 8830 lambda_n: 0.9106843581585704 Loss: 1.4267884363151474e-09\n",
      "Iteration: 8831 lambda_n: 0.8971141416800021 Loss: 1.4267951254472017e-09\n",
      "Iteration: 8832 lambda_n: 0.9080870967917531 Loss: 1.426801689822774e-09\n",
      "Iteration: 8833 lambda_n: 0.982892290009967 Loss: 1.4268083061103292e-09\n",
      "Iteration: 8834 lambda_n: 0.8989133880687787 Loss: 1.4268154410348642e-09\n",
      "Iteration: 8835 lambda_n: 1.005879986476642 Loss: 1.4268219349176067e-09\n",
      "Iteration: 8836 lambda_n: 0.9232938130562393 Loss: 1.42682917444881e-09\n",
      "Iteration: 8837 lambda_n: 0.9583100192930215 Loss: 1.4268357869261417e-09\n",
      "Iteration: 8838 lambda_n: 0.943766456417392 Loss: 1.4268426209785548e-09\n",
      "Iteration: 8839 lambda_n: 0.9704594222560734 Loss: 1.4268493248351486e-09\n",
      "Iteration: 8840 lambda_n: 1.0342394554550398 Loss: 1.426856188005085e-09\n",
      "Iteration: 8841 lambda_n: 0.9106087340632848 Loss: 1.4268634704386331e-09\n",
      "Iteration: 8842 lambda_n: 1.0179301821302786 Loss: 1.4268698531824308e-09\n",
      "Iteration: 8843 lambda_n: 1.000091618904351 Loss: 1.4268769568987506e-09\n",
      "Iteration: 8844 lambda_n: 0.951474555126 Loss: 1.4268839020410375e-09\n",
      "Iteration: 8845 lambda_n: 0.9970234645850762 Loss: 1.4268904860073027e-09\n",
      "Iteration: 8846 lambda_n: 1.0492096513291298 Loss: 1.4268973522575834e-09\n",
      "Iteration: 8847 lambda_n: 1.0014819928317806 Loss: 1.426904545543275e-09\n",
      "Iteration: 8848 lambda_n: 0.9457655671442038 Loss: 1.4269113749283428e-09\n",
      "Iteration: 8849 lambda_n: 0.9612060672012706 Loss: 1.4269177992091688e-09\n",
      "Iteration: 8850 lambda_n: 0.9530829904358694 Loss: 1.4269242995787206e-09\n",
      "Iteration: 8851 lambda_n: 0.9430911477506455 Loss: 1.4269307110810517e-09\n",
      "Iteration: 8852 lambda_n: 0.9619206329273174 Loss: 1.4269370361895153e-09\n",
      "Iteration: 8853 lambda_n: 1.0243077508721499 Loss: 1.426943457024015e-09\n",
      "Iteration: 8854 lambda_n: 0.9447116907622148 Loss: 1.4269502641824893e-09\n",
      "Iteration: 8855 lambda_n: 1.017810821017939 Loss: 1.426956511066784e-09\n",
      "Iteration: 8856 lambda_n: 0.9265815407186081 Loss: 1.4269632145059254e-09\n",
      "Iteration: 8857 lambda_n: 0.9979060311059494 Loss: 1.4269692861557331e-09\n",
      "Iteration: 8858 lambda_n: 1.0091510781472177 Loss: 1.4269758039972193e-09\n",
      "Iteration: 8859 lambda_n: 0.9650288172872763 Loss: 1.4269823629427897e-09\n",
      "Iteration: 8860 lambda_n: 0.9170160298287675 Loss: 1.42698860614173e-09\n",
      "Iteration: 8861 lambda_n: 0.9253816298947951 Loss: 1.4269945115100311e-09\n",
      "Iteration: 8862 lambda_n: 1.0383673197399546 Loss: 1.427000445219866e-09\n",
      "Iteration: 8863 lambda_n: 1.0247227207218312 Loss: 1.427007077442141e-09\n",
      "Iteration: 8864 lambda_n: 0.9825714355714246 Loss: 1.4270135884630429e-09\n",
      "Iteration: 8865 lambda_n: 1.0330426087921707 Loss: 1.4270198077777022e-09\n",
      "Iteration: 8866 lambda_n: 1.0014180558674781 Loss: 1.4270263114153921e-09\n",
      "Iteration: 8867 lambda_n: 0.987167562393752 Loss: 1.427032592080446e-09\n",
      "Iteration: 8868 lambda_n: 0.9487655961499653 Loss: 1.4270387540479883e-09\n",
      "Iteration: 8869 lambda_n: 1.01320751528616 Loss: 1.4270446470660999e-09\n",
      "Iteration: 8870 lambda_n: 0.9267487919818026 Loss: 1.427050914680703e-09\n",
      "Iteration: 8871 lambda_n: 1.0002188870819855 Loss: 1.427056621357024e-09\n",
      "Iteration: 8872 lambda_n: 0.9318218645965939 Loss: 1.4270627548085197e-09\n",
      "Iteration: 8873 lambda_n: 1.0167830245353369 Loss: 1.4270684377851205e-09\n",
      "Iteration: 8874 lambda_n: 0.9454643536622646 Loss: 1.4270746191262725e-09\n",
      "Iteration: 8875 lambda_n: 1.0089829820417042 Loss: 1.4270803397639713e-09\n",
      "Iteration: 8876 lambda_n: 0.9501790701197378 Loss: 1.4270864177422672e-09\n",
      "Iteration: 8877 lambda_n: 0.994575265034994 Loss: 1.4270921139357656e-09\n",
      "Iteration: 8878 lambda_n: 0.9025580659729868 Loss: 1.4270980487876444e-09\n",
      "Iteration: 8879 lambda_n: 0.9434731470518553 Loss: 1.4271034137843307e-09\n",
      "Iteration: 8880 lambda_n: 1.0264677378636555 Loss: 1.4271089977500068e-09\n",
      "Iteration: 8881 lambda_n: 0.9543332463120285 Loss: 1.427115047708751e-09\n",
      "Iteration: 8882 lambda_n: 0.9824839049764853 Loss: 1.4271206413528498e-09\n",
      "Iteration: 8883 lambda_n: 1.046020935231546 Loss: 1.4271263806023176e-09\n",
      "Iteration: 8884 lambda_n: 0.9910585370455848 Loss: 1.4271324628809685e-09\n",
      "Iteration: 8885 lambda_n: 1.0418040279009901 Loss: 1.4271381983056615e-09\n",
      "Iteration: 8886 lambda_n: 0.8992057963410884 Loss: 1.4271441993667182e-09\n",
      "Iteration: 8887 lambda_n: 0.9366022484819363 Loss: 1.4271493509734216e-09\n",
      "Iteration: 8888 lambda_n: 1.016994861771415 Loss: 1.4271546993591737e-09\n",
      "Iteration: 8889 lambda_n: 0.9724374607570413 Loss: 1.4271604805130372e-09\n",
      "Iteration: 8890 lambda_n: 0.9349806478825171 Loss: 1.4271659828482972e-09\n",
      "Iteration: 8891 lambda_n: 0.8992009204436313 Loss: 1.4271712464613103e-09\n",
      "Iteration: 8892 lambda_n: 0.9120090062187295 Loss: 1.4271762917766332e-09\n",
      "Iteration: 8893 lambda_n: 0.9213294929066402 Loss: 1.427181386603462e-09\n",
      "Iteration: 8894 lambda_n: 0.9920515696125528 Loss: 1.4271865133365168e-09\n",
      "Iteration: 8895 lambda_n: 0.970425649049818 Loss: 1.427192008198568e-09\n",
      "Iteration: 8896 lambda_n: 0.969642690599797 Loss: 1.4271973603216244e-09\n",
      "Iteration: 8897 lambda_n: 0.9808807828778414 Loss: 1.4272026842958741e-09\n",
      "Iteration: 8898 lambda_n: 0.9510091598698271 Loss: 1.4272080456055741e-09\n",
      "Iteration: 8899 lambda_n: 1.013264876546462 Loss: 1.427213220986985e-09\n",
      "Iteration: 8900 lambda_n: 0.903095790149211 Loss: 1.4272187101998826e-09\n",
      "Iteration: 8901 lambda_n: 0.9486545943408892 Loss: 1.427223579549509e-09\n",
      "Iteration: 8902 lambda_n: 0.9955746048463053 Loss: 1.4272286748499133e-09\n",
      "Iteration: 8903 lambda_n: 0.9921629512942592 Loss: 1.4272339977162338e-09\n",
      "Iteration: 8904 lambda_n: 0.9764020343551152 Loss: 1.4272392799405938e-09\n",
      "Iteration: 8905 lambda_n: 1.0161360384377325 Loss: 1.427244452470035e-09\n",
      "Iteration: 8906 lambda_n: 0.9773585484679349 Loss: 1.4272498136003317e-09\n",
      "Iteration: 8907 lambda_n: 1.006643485870412 Loss: 1.4272549477611595e-09\n",
      "Iteration: 8908 lambda_n: 1.0343288725899447 Loss: 1.4272602108314768e-09\n",
      "Iteration: 8909 lambda_n: 1.014232665231519 Loss: 1.4272655939583686e-09\n",
      "Iteration: 8910 lambda_n: 0.9154465173112025 Loss: 1.4272708469832008e-09\n",
      "Iteration: 8911 lambda_n: 1.0345118501106558 Loss: 1.4272755658903816e-09\n",
      "Iteration: 8912 lambda_n: 0.9664957177077462 Loss: 1.4272808775157825e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8913 lambda_n: 0.927566924203227 Loss: 1.4272858156676816e-09\n",
      "Iteration: 8914 lambda_n: 0.9507557290051156 Loss: 1.4272905335716478e-09\n",
      "Iteration: 8915 lambda_n: 0.9849681135916897 Loss: 1.4272953497283965e-09\n",
      "Iteration: 8916 lambda_n: 0.9404871421891652 Loss: 1.4273003152966955e-09\n",
      "Iteration: 8917 lambda_n: 0.8971562405838589 Loss: 1.427305038060673e-09\n",
      "Iteration: 8918 lambda_n: 1.0487190472553927 Loss: 1.4273095209647644e-09\n",
      "Iteration: 8919 lambda_n: 0.9313497720648587 Loss: 1.427314741950673e-09\n",
      "Iteration: 8920 lambda_n: 0.9611326320364115 Loss: 1.4273193577946697e-09\n",
      "Iteration: 8921 lambda_n: 1.0431846087870542 Loss: 1.4273241004334026e-09\n",
      "Iteration: 8922 lambda_n: 0.9206552535037001 Loss: 1.4273292229896306e-09\n",
      "Iteration: 8923 lambda_n: 1.0229695625515907 Loss: 1.4273337245281377e-09\n",
      "Iteration: 8924 lambda_n: 1.0320031466006274 Loss: 1.427338705854537e-09\n",
      "Iteration: 8925 lambda_n: 1.0313540890948745 Loss: 1.4273437070599724e-09\n",
      "Iteration: 8926 lambda_n: 1.0120474712519474 Loss: 1.427348680408567e-09\n",
      "Iteration: 8927 lambda_n: 0.9988724143126032 Loss: 1.4273535378848002e-09\n",
      "Iteration: 8928 lambda_n: 0.8937185571014162 Loss: 1.4273583099200725e-09\n",
      "Iteration: 8929 lambda_n: 0.9957882097104713 Loss: 1.4273625611145741e-09\n",
      "Iteration: 8930 lambda_n: 1.0157089158079244 Loss: 1.4273672790004417e-09\n",
      "Iteration: 8931 lambda_n: 0.9825229925596296 Loss: 1.427372069676011e-09\n",
      "Iteration: 8932 lambda_n: 0.9241275680919322 Loss: 1.427376679352042e-09\n",
      "Iteration: 8933 lambda_n: 1.044154494179706 Loss: 1.427380997714748e-09\n",
      "Iteration: 8934 lambda_n: 0.9478974559028586 Loss: 1.4273858565656064e-09\n",
      "Iteration: 8935 lambda_n: 0.9519753841407559 Loss: 1.4273902454064328e-09\n",
      "Iteration: 8936 lambda_n: 1.0092221121118545 Loss: 1.4273946326302825e-09\n",
      "Iteration: 8937 lambda_n: 0.9611993576667165 Loss: 1.4273992655168807e-09\n",
      "Iteration: 8938 lambda_n: 1.0231963848738086 Loss: 1.427403658466224e-09\n",
      "Iteration: 8939 lambda_n: 1.0223517288668982 Loss: 1.427408313282197e-09\n",
      "Iteration: 8940 lambda_n: 0.9931748978177923 Loss: 1.42741294045974e-09\n",
      "Iteration: 8941 lambda_n: 1.0494019562757597 Loss: 1.4274174165507182e-09\n",
      "Iteration: 8942 lambda_n: 0.9178647349118153 Loss: 1.4274221232484867e-09\n",
      "Iteration: 8943 lambda_n: 0.9466656025107015 Loss: 1.427426216510499e-09\n",
      "Iteration: 8944 lambda_n: 0.904530845804271 Loss: 1.4274304258676048e-09\n",
      "Iteration: 8945 lambda_n: 0.9884014861480427 Loss: 1.4274344292143188e-09\n",
      "Iteration: 8946 lambda_n: 1.0108855444056495 Loss: 1.4274387862187818e-09\n",
      "Iteration: 8947 lambda_n: 0.9413339129428853 Loss: 1.4274432227179896e-09\n",
      "Iteration: 8948 lambda_n: 0.8956410487388121 Loss: 1.4274473315718206e-09\n",
      "Iteration: 8949 lambda_n: 0.9714600413135828 Loss: 1.427451226207533e-09\n",
      "Iteration: 8950 lambda_n: 1.0340929904427494 Loss: 1.4274554328162339e-09\n",
      "Iteration: 8951 lambda_n: 0.9711444210344808 Loss: 1.4274598917170519e-09\n",
      "Iteration: 8952 lambda_n: 1.0470226220915193 Loss: 1.4274640548515888e-09\n",
      "Iteration: 8953 lambda_n: 0.9634187460406888 Loss: 1.4274685310723757e-09\n",
      "Iteration: 8954 lambda_n: 0.8935947484150595 Loss: 1.4274726289550219e-09\n",
      "Iteration: 8955 lambda_n: 0.9295027978673525 Loss: 1.427476411892496e-09\n",
      "Iteration: 8956 lambda_n: 0.8952148812714517 Loss: 1.4274803313469858e-09\n",
      "Iteration: 8957 lambda_n: 1.0236369746877023 Loss: 1.4274840879741256e-09\n",
      "Iteration: 8958 lambda_n: 0.9843759775912244 Loss: 1.4274883687663153e-09\n",
      "Iteration: 8959 lambda_n: 1.0017818448061027 Loss: 1.4274924661333587e-09\n",
      "Iteration: 8960 lambda_n: 0.9120894475854563 Loss: 1.4274966153100281e-09\n",
      "Iteration: 8961 lambda_n: 0.9479746696350397 Loss: 1.427500373040669e-09\n",
      "Iteration: 8962 lambda_n: 0.9394339344529133 Loss: 1.4275042685407155e-09\n",
      "Iteration: 8963 lambda_n: 0.9052115464355945 Loss: 1.4275081102697174e-09\n",
      "Iteration: 8964 lambda_n: 0.8980060864309191 Loss: 1.4275117952759977e-09\n",
      "Iteration: 8965 lambda_n: 1.0464627810520262 Loss: 1.427515433747601e-09\n",
      "Iteration: 8966 lambda_n: 0.9591652847726753 Loss: 1.4275196611518757e-09\n",
      "Iteration: 8967 lambda_n: 1.0030290683223884 Loss: 1.4275235141348583e-09\n",
      "Iteration: 8968 lambda_n: 1.0319214722635954 Loss: 1.427527526902164e-09\n",
      "Iteration: 8969 lambda_n: 0.9604481526076118 Loss: 1.4275316316122389e-09\n",
      "Iteration: 8970 lambda_n: 1.000048079807807 Loss: 1.427535439729011e-09\n",
      "Iteration: 8971 lambda_n: 0.8970507703982721 Loss: 1.4275393855244065e-09\n",
      "Iteration: 8972 lambda_n: 0.9022793671502713 Loss: 1.4275429067110155e-09\n",
      "Iteration: 8973 lambda_n: 0.9469296708571648 Loss: 1.427546435776057e-09\n",
      "Iteration: 8974 lambda_n: 0.9296665642430445 Loss: 1.4275501219294881e-09\n",
      "Iteration: 8975 lambda_n: 1.0319452677548795 Loss: 1.427553726909627e-09\n",
      "Iteration: 8976 lambda_n: 1.0400159295362188 Loss: 1.427557711515213e-09\n",
      "Iteration: 8977 lambda_n: 0.9245552597174286 Loss: 1.4275617088650418e-09\n",
      "Iteration: 8978 lambda_n: 1.0186214222165086 Loss: 1.427565242048196e-09\n",
      "Iteration: 8979 lambda_n: 0.9259852351080343 Loss: 1.4275691234325339e-09\n",
      "Iteration: 8980 lambda_n: 0.9795732147572191 Loss: 1.4275726349716512e-09\n",
      "Iteration: 8981 lambda_n: 1.0094571519558686 Loss: 1.4275763291913016e-09\n",
      "Iteration: 8982 lambda_n: 0.9051172786801382 Loss: 1.4275801236196183e-09\n",
      "Iteration: 8983 lambda_n: 0.8959302633140325 Loss: 1.427583511222125e-09\n",
      "Iteration: 8984 lambda_n: 0.9592499089484834 Loss: 1.4275868489982159e-09\n",
      "Iteration: 8985 lambda_n: 0.9514170222219962 Loss: 1.4275904045649912e-09\n",
      "Iteration: 8986 lambda_n: 0.9324143708272684 Loss: 1.427593917364769e-09\n",
      "Iteration: 8987 lambda_n: 0.933510853514816 Loss: 1.4275973484692546e-09\n",
      "Iteration: 8988 lambda_n: 0.9271276775753416 Loss: 1.4276007665918093e-09\n",
      "Iteration: 8989 lambda_n: 1.004283857322808 Loss: 1.4276041436275416e-09\n",
      "Iteration: 8990 lambda_n: 1.0367379448833318 Loss: 1.4276077910825737e-09\n",
      "Iteration: 8991 lambda_n: 0.9741395257653137 Loss: 1.4276115359546116e-09\n",
      "Iteration: 8992 lambda_n: 1.0173378719666337 Loss: 1.4276150400510855e-09\n",
      "Iteration: 8993 lambda_n: 1.0182023380003502 Loss: 1.4276186826308492e-09\n",
      "Iteration: 8994 lambda_n: 0.9016255329469777 Loss: 1.4276223081011542e-09\n",
      "Iteration: 8995 lambda_n: 1.048153742243897 Loss: 1.4276255070302177e-09\n",
      "Iteration: 8996 lambda_n: 0.9124208737534549 Loss: 1.4276292107425641e-09\n",
      "Iteration: 8997 lambda_n: 0.9964186880705704 Loss: 1.4276324160061623e-09\n",
      "Iteration: 8998 lambda_n: 0.9595566484149326 Loss: 1.4276359050903727e-09\n",
      "Iteration: 8999 lambda_n: 1.0417492777257815 Loss: 1.427639249498302e-09\n",
      "Iteration: 9000 lambda_n: 1.0311861520604444 Loss: 1.4276428629926968e-09\n",
      "Iteration: 9001 lambda_n: 0.9790070365727178 Loss: 1.4276464221315676e-09\n",
      "Iteration: 9002 lambda_n: 0.92408727909164 Loss: 1.4276497820005555e-09\n",
      "Iteration: 9003 lambda_n: 1.0374647146470837 Loss: 1.4276529440697129e-09\n",
      "Iteration: 9004 lambda_n: 1.0229868726582847 Loss: 1.4276564777758719e-09\n",
      "Iteration: 9005 lambda_n: 1.0082991166617472 Loss: 1.4276599473381588e-09\n",
      "Iteration: 9006 lambda_n: 0.9542554236205305 Loss: 1.4276633496154057e-09\n",
      "Iteration: 9007 lambda_n: 0.9750320917947911 Loss: 1.427666554911842e-09\n",
      "Iteration: 9008 lambda_n: 1.0030545228694845 Loss: 1.4276698119639055e-09\n",
      "Iteration: 9009 lambda_n: 0.9399225674648775 Loss: 1.4276731536527341e-09\n",
      "Iteration: 9010 lambda_n: 0.9002087339460032 Loss: 1.4276762696731393e-09\n",
      "Iteration: 9011 lambda_n: 1.011647010189671 Loss: 1.4276792374914868e-09\n",
      "Iteration: 9012 lambda_n: 0.920155800648546 Loss: 1.4276825638872129e-09\n",
      "Iteration: 9013 lambda_n: 0.9403447658782459 Loss: 1.4276855734873006e-09\n",
      "Iteration: 9014 lambda_n: 0.9162962981592713 Loss: 1.4276886357663502e-09\n",
      "Iteration: 9015 lambda_n: 0.9598651336875433 Loss: 1.4276916094933097e-09\n",
      "Iteration: 9016 lambda_n: 0.9633035136574807 Loss: 1.4276947078508513e-09\n",
      "Iteration: 9017 lambda_n: 1.002464826808663 Loss: 1.4276978040807291e-09\n",
      "Iteration: 9018 lambda_n: 0.9268999011151684 Loss: 1.4277010125293575e-09\n",
      "Iteration: 9019 lambda_n: 0.9389069864315914 Loss: 1.4277039626157192e-09\n",
      "Iteration: 9020 lambda_n: 0.9592338012226148 Loss: 1.4277069427555715e-09\n",
      "Iteration: 9021 lambda_n: 0.9853736033194179 Loss: 1.4277099699592832e-09\n",
      "Iteration: 9022 lambda_n: 0.9316612782510476 Loss: 1.4277130704471638e-09\n",
      "Iteration: 9023 lambda_n: 1.0108339473448194 Loss: 1.427715986860894e-09\n",
      "Iteration: 9024 lambda_n: 0.9274668472150602 Loss: 1.4277191375149383e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9025 lambda_n: 0.8971391142969318 Loss: 1.4277220138189103e-09\n",
      "Iteration: 9026 lambda_n: 1.0033448014629358 Loss: 1.427724785106964e-09\n",
      "Iteration: 9027 lambda_n: 1.0337571334473046 Loss: 1.4277278681503938e-09\n",
      "Iteration: 9028 lambda_n: 1.0013408176555207 Loss: 1.4277310345932726e-09\n",
      "Iteration: 9029 lambda_n: 0.9392925440869858 Loss: 1.4277340869921924e-09\n",
      "Iteration: 9030 lambda_n: 0.9068250210314035 Loss: 1.4277369367716519e-09\n",
      "Iteration: 9031 lambda_n: 1.0396214983513221 Loss: 1.427739676916339e-09\n",
      "Iteration: 9032 lambda_n: 1.0045402301615334 Loss: 1.4277428051689112e-09\n",
      "Iteration: 9033 lambda_n: 1.0055344189445374 Loss: 1.4277458129820983e-09\n",
      "Iteration: 9034 lambda_n: 1.0427076951885694 Loss: 1.4277488056543622e-09\n",
      "Iteration: 9035 lambda_n: 0.9795996946224242 Loss: 1.4277518996932812e-09\n",
      "Iteration: 9036 lambda_n: 0.9374029723925975 Loss: 1.4277547886032615e-09\n",
      "Iteration: 9037 lambda_n: 0.893513228526311 Loss: 1.4277575459728824e-09\n",
      "Iteration: 9038 lambda_n: 0.9231104670075895 Loss: 1.4277601604327963e-09\n",
      "Iteration: 9039 lambda_n: 1.015604408460959 Loss: 1.4277628484834013e-09\n",
      "Iteration: 9040 lambda_n: 0.9769821146063902 Loss: 1.4277657974288993e-09\n",
      "Iteration: 9041 lambda_n: 0.9064751899970889 Loss: 1.4277686184805666e-09\n",
      "Iteration: 9042 lambda_n: 0.9908609691323873 Loss: 1.4277712239689064e-09\n",
      "Iteration: 9043 lambda_n: 0.9394113737867533 Loss: 1.4277740565359655e-09\n",
      "Iteration: 9044 lambda_n: 0.9672303986176792 Loss: 1.4277767339591212e-09\n",
      "Iteration: 9045 lambda_n: 0.9083572136740959 Loss: 1.4277794789613972e-09\n",
      "Iteration: 9046 lambda_n: 0.9653476708009094 Loss: 1.4277820418835666e-09\n",
      "Iteration: 9047 lambda_n: 0.9243525700957311 Loss: 1.4277847576704415e-09\n",
      "Iteration: 9048 lambda_n: 1.0120728956024563 Loss: 1.4277873469712295e-09\n",
      "Iteration: 9049 lambda_n: 0.9077964911792503 Loss: 1.4277901665135704e-09\n",
      "Iteration: 9050 lambda_n: 1.0362541033026098 Loss: 1.42779268735669e-09\n",
      "Iteration: 9051 lambda_n: 0.9591208965558311 Loss: 1.4277955531494368e-09\n",
      "Iteration: 9052 lambda_n: 0.8924869567047334 Loss: 1.4277981878100395e-09\n",
      "Iteration: 9053 lambda_n: 0.9264114592168254 Loss: 1.4278006347015276e-09\n",
      "Iteration: 9054 lambda_n: 0.9690750703433555 Loss: 1.427803163754994e-09\n",
      "Iteration: 9055 lambda_n: 0.9932097201606345 Loss: 1.4278057936483642e-09\n",
      "Iteration: 9056 lambda_n: 1.0064451787635222 Loss: 1.4278084804147944e-09\n",
      "Iteration: 9057 lambda_n: 1.0332314220917094 Loss: 1.4278111917914314e-09\n",
      "Iteration: 9058 lambda_n: 1.012266601684522 Loss: 1.4278139591842108e-09\n",
      "Iteration: 9059 lambda_n: 0.9012377871745008 Loss: 1.4278166554380633e-09\n",
      "Iteration: 9060 lambda_n: 1.0112169027181508 Loss: 1.4278190477897693e-09\n",
      "Iteration: 9061 lambda_n: 1.0259137510750767 Loss: 1.4278217170904734e-09\n",
      "Iteration: 9062 lambda_n: 0.943717821184064 Loss: 1.427824416998767e-09\n",
      "Iteration: 9063 lambda_n: 0.9825097711566466 Loss: 1.4278268882794497e-09\n",
      "Iteration: 9064 lambda_n: 0.90135996197526 Loss: 1.4278294479970531e-09\n",
      "Iteration: 9065 lambda_n: 0.9966078936983775 Loss: 1.4278317833632875e-09\n",
      "Iteration: 9066 lambda_n: 0.9786855749431334 Loss: 1.4278343593239238e-09\n",
      "Iteration: 9067 lambda_n: 0.9018610591302009 Loss: 1.4278368718668249e-09\n",
      "Iteration: 9068 lambda_n: 0.9540510749347071 Loss: 1.4278391828563438e-09\n",
      "Iteration: 9069 lambda_n: 0.9764370729777259 Loss: 1.4278416120788304e-09\n",
      "Iteration: 9070 lambda_n: 0.9733180322247066 Loss: 1.4278440926719449e-09\n",
      "Iteration: 9071 lambda_n: 0.8998714070804811 Loss: 1.4278465501298174e-09\n",
      "Iteration: 9072 lambda_n: 0.9017976798891463 Loss: 1.427848815149957e-09\n",
      "Iteration: 9073 lambda_n: 0.9693379376901978 Loss: 1.427851076497751e-09\n",
      "Iteration: 9074 lambda_n: 0.9783267133651784 Loss: 1.4278534924606407e-09\n",
      "Iteration: 9075 lambda_n: 1.0098579378933412 Loss: 1.4278559253154323e-09\n",
      "Iteration: 9076 lambda_n: 1.0322452705412726 Loss: 1.4278584208784235e-09\n",
      "Iteration: 9077 lambda_n: 0.9929253704081014 Loss: 1.4278609645267172e-09\n",
      "Iteration: 9078 lambda_n: 1.0313002123355708 Loss: 1.4278633948478703e-09\n",
      "Iteration: 9079 lambda_n: 0.9973103002528181 Loss: 1.4278659137440839e-09\n",
      "Iteration: 9080 lambda_n: 0.9692501364805449 Loss: 1.4278683328576517e-09\n",
      "Iteration: 9081 lambda_n: 0.8943355940381681 Loss: 1.4278706774271944e-09\n",
      "Iteration: 9082 lambda_n: 0.9160996818935012 Loss: 1.4278728264337594e-09\n",
      "Iteration: 9083 lambda_n: 0.9955979340503703 Loss: 1.4278750195861518e-09\n",
      "Iteration: 9084 lambda_n: 1.001497028369808 Loss: 1.4278773928508203e-09\n",
      "Iteration: 9085 lambda_n: 1.0241928686174178 Loss: 1.427879771645982e-09\n",
      "Iteration: 9086 lambda_n: 0.9571505514671232 Loss: 1.427882193040676e-09\n",
      "Iteration: 9087 lambda_n: 0.9830085214151376 Loss: 1.427884439876328e-09\n",
      "Iteration: 9088 lambda_n: 0.9882495402662519 Loss: 1.4278867443665995e-09\n",
      "Iteration: 9089 lambda_n: 0.9176743680906967 Loss: 1.4278890449582651e-09\n",
      "Iteration: 9090 lambda_n: 0.8957243877708453 Loss: 1.4278911763529e-09\n",
      "Iteration: 9091 lambda_n: 1.0145764255285072 Loss: 1.427893244669795e-09\n",
      "Iteration: 9092 lambda_n: 1.0086300244680322 Loss: 1.4278955806901736e-09\n",
      "Iteration: 9093 lambda_n: 0.9523385146186341 Loss: 1.4278978874775891e-09\n",
      "Iteration: 9094 lambda_n: 0.9970142851992253 Loss: 1.4279000605271906e-09\n",
      "Iteration: 9095 lambda_n: 1.0418780215911656 Loss: 1.427902321821448e-09\n",
      "Iteration: 9096 lambda_n: 1.0185830236603366 Loss: 1.4279046780642129e-09\n",
      "Iteration: 9097 lambda_n: 0.9993005385810992 Loss: 1.4279069659999287e-09\n",
      "Iteration: 9098 lambda_n: 0.9394742285738643 Loss: 1.427909205144159e-09\n",
      "Iteration: 9099 lambda_n: 0.9431741646266808 Loss: 1.4279112971909975e-09\n",
      "Iteration: 9100 lambda_n: 1.0072607567485399 Loss: 1.4279133857493646e-09\n",
      "Iteration: 9101 lambda_n: 0.9879633750040966 Loss: 1.4279156116008108e-09\n",
      "Iteration: 9102 lambda_n: 0.9737232456967149 Loss: 1.4279177801258446e-09\n",
      "Iteration: 9103 lambda_n: 0.8965461532152043 Loss: 1.4279199118419132e-09\n",
      "Iteration: 9104 lambda_n: 0.9864549238515115 Loss: 1.4279218635183881e-09\n",
      "Iteration: 9105 lambda_n: 1.0018762149846527 Loss: 1.4279240006302206e-09\n",
      "Iteration: 9106 lambda_n: 0.999906902494463 Loss: 1.4279261668242886e-09\n",
      "Iteration: 9107 lambda_n: 0.9340080199127059 Loss: 1.427928313634259e-09\n",
      "Iteration: 9108 lambda_n: 0.9303613521257852 Loss: 1.427930314011162e-09\n",
      "Iteration: 9109 lambda_n: 1.0208660761296373 Loss: 1.427932293566504e-09\n",
      "Iteration: 9110 lambda_n: 0.9351178370180159 Loss: 1.4279344599195659e-09\n",
      "Iteration: 9111 lambda_n: 0.9368168224295694 Loss: 1.4279364343477804e-09\n",
      "Iteration: 9112 lambda_n: 0.9292615180855827 Loss: 1.427938404181902e-09\n",
      "Iteration: 9113 lambda_n: 1.0440829935993905 Loss: 1.4279403472315328e-09\n",
      "Iteration: 9114 lambda_n: 1.0481108306146931 Loss: 1.4279425259080736e-09\n",
      "Iteration: 9115 lambda_n: 0.9841204199895681 Loss: 1.42794469692665e-09\n",
      "Iteration: 9116 lambda_n: 0.9144372064485548 Loss: 1.4279467297335671e-09\n",
      "Iteration: 9117 lambda_n: 0.9646531683634096 Loss: 1.4279486051046854e-09\n",
      "Iteration: 9118 lambda_n: 0.9757291525725907 Loss: 1.427950580571258e-09\n",
      "Iteration: 9119 lambda_n: 0.8981839161025139 Loss: 1.4279525665330295e-09\n",
      "Iteration: 9120 lambda_n: 0.9699529448103825 Loss: 1.4279543854856857e-09\n",
      "Iteration: 9121 lambda_n: 1.0428736906673222 Loss: 1.427956346291069e-09\n",
      "Iteration: 9122 lambda_n: 0.9959951607639195 Loss: 1.4279584434071398e-09\n",
      "Iteration: 9123 lambda_n: 0.9606020026223185 Loss: 1.427960435704419e-09\n",
      "Iteration: 9124 lambda_n: 1.0338580129995438 Loss: 1.4279623482052286e-09\n",
      "Iteration: 9125 lambda_n: 1.0366334409448383 Loss: 1.4279643972298671e-09\n",
      "Iteration: 9126 lambda_n: 0.9385843549310461 Loss: 1.4279664420617835e-09\n",
      "Iteration: 9127 lambda_n: 0.9605623383539722 Loss: 1.4279682852116601e-09\n",
      "Iteration: 9128 lambda_n: 1.0310531450639187 Loss: 1.4279701626871966e-09\n",
      "Iteration: 9129 lambda_n: 0.9795950265315051 Loss: 1.4279721703409729e-09\n",
      "Iteration: 9130 lambda_n: 1.0168827641656972 Loss: 1.4279740648835609e-09\n",
      "Iteration: 9131 lambda_n: 1.0133759941709417 Loss: 1.427976028887913e-09\n",
      "Iteration: 9132 lambda_n: 0.9650228441819872 Loss: 1.4279779745553754e-09\n",
      "Iteration: 9133 lambda_n: 1.0021253206366656 Loss: 1.4279798180174198e-09\n",
      "Iteration: 9134 lambda_n: 0.9475172002011807 Loss: 1.4279817249468214e-09\n",
      "Iteration: 9135 lambda_n: 1.0345178298035174 Loss: 1.4279835176407498e-09\n",
      "Iteration: 9136 lambda_n: 0.9407368365801453 Loss: 1.427985462457763e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9137 lambda_n: 0.9844363669197754 Loss: 1.4279872279875e-09\n",
      "Iteration: 9138 lambda_n: 1.0395068879137443 Loss: 1.4279890683106879e-09\n",
      "Iteration: 9139 lambda_n: 0.9977410182832126 Loss: 1.427991001783908e-09\n",
      "Iteration: 9140 lambda_n: 0.9192441832734376 Loss: 1.4279928483330185e-09\n",
      "Iteration: 9141 lambda_n: 1.0175496108721294 Loss: 1.4279945411277254e-09\n",
      "Iteration: 9142 lambda_n: 1.0475130970949706 Loss: 1.4279964059946264e-09\n",
      "Iteration: 9143 lambda_n: 0.9231261826174735 Loss: 1.4279983172726079e-09\n",
      "Iteration: 9144 lambda_n: 0.9607524407103796 Loss: 1.427999990726597e-09\n",
      "Iteration: 9145 lambda_n: 0.9498495188312832 Loss: 1.428001731661143e-09\n",
      "Iteration: 9146 lambda_n: 0.9660713314977791 Loss: 1.428003443341699e-09\n",
      "Iteration: 9147 lambda_n: 0.9621922999617687 Loss: 1.42800517625572e-09\n",
      "Iteration: 9148 lambda_n: 0.9824626733756662 Loss: 1.4280068935521482e-09\n",
      "Iteration: 9149 lambda_n: 1.0205106329784246 Loss: 1.4280086389274332e-09\n",
      "Iteration: 9150 lambda_n: 0.9741031272047226 Loss: 1.4280104399446194e-09\n",
      "Iteration: 9151 lambda_n: 0.9802270944361555 Loss: 1.4280121566278903e-09\n",
      "Iteration: 9152 lambda_n: 0.9005058593539927 Loss: 1.4280138749799153e-09\n",
      "Iteration: 9153 lambda_n: 0.9787631202683542 Loss: 1.4280154455800667e-09\n",
      "Iteration: 9154 lambda_n: 1.0304110412176892 Loss: 1.4280171417286824e-09\n",
      "Iteration: 9155 lambda_n: 0.930647861440597 Loss: 1.4280189256325819e-09\n",
      "Iteration: 9156 lambda_n: 0.986698579130425 Loss: 1.4280205279568933e-09\n",
      "Iteration: 9157 lambda_n: 1.0486796896200197 Loss: 1.4280222195857889e-09\n",
      "Iteration: 9158 lambda_n: 0.9830518213200329 Loss: 1.4280240081479007e-09\n",
      "Iteration: 9159 lambda_n: 0.9173880029479075 Loss: 1.428025674223454e-09\n",
      "Iteration: 9160 lambda_n: 0.978816103913967 Loss: 1.42802722681073e-09\n",
      "Iteration: 9161 lambda_n: 1.026024749004209 Loss: 1.4280288750941225e-09\n",
      "Iteration: 9162 lambda_n: 0.995242364856553 Loss: 1.4280305951557816e-09\n",
      "Iteration: 9163 lambda_n: 0.910984049375996 Loss: 1.4280322549705138e-09\n",
      "Iteration: 9164 lambda_n: 0.9583950068271223 Loss: 1.4280337640154543e-09\n",
      "Iteration: 9165 lambda_n: 1.039815648986835 Loss: 1.428035348701825e-09\n",
      "Iteration: 9166 lambda_n: 0.9756941549697479 Loss: 1.4280370607048936e-09\n",
      "Iteration: 9167 lambda_n: 0.8932204145221078 Loss: 1.4280386561975735e-09\n",
      "Iteration: 9168 lambda_n: 0.9553233760459807 Loss: 1.4280401138620736e-09\n",
      "Iteration: 9169 lambda_n: 0.8988361173097208 Loss: 1.4280416658950454e-09\n",
      "Iteration: 9170 lambda_n: 0.9530232055482581 Loss: 1.4280431186717876e-09\n",
      "Iteration: 9171 lambda_n: 0.9046236483197541 Loss: 1.4280446524537777e-09\n",
      "Iteration: 9172 lambda_n: 0.947068412626923 Loss: 1.4280461016552683e-09\n",
      "Iteration: 9173 lambda_n: 0.9361599061203104 Loss: 1.4280476134342903e-09\n",
      "Iteration: 9174 lambda_n: 0.9884660331379199 Loss: 1.4280491002830773e-09\n",
      "Iteration: 9175 lambda_n: 0.9965113189097352 Loss: 1.4280506581843142e-09\n",
      "Iteration: 9176 lambda_n: 0.9065460807853135 Loss: 1.4280522279755537e-09\n",
      "Iteration: 9177 lambda_n: 0.9220136329560209 Loss: 1.4280536495858962e-09\n",
      "Iteration: 9178 lambda_n: 0.9900731426109765 Loss: 1.4280550893023844e-09\n",
      "Iteration: 9179 lambda_n: 1.0229566872887754 Loss: 1.4280566248123819e-09\n",
      "Iteration: 9180 lambda_n: 0.9173775900266101 Loss: 1.4280582094905262e-09\n",
      "Iteration: 9181 lambda_n: 0.9407408214354339 Loss: 1.4280596212920587e-09\n",
      "Iteration: 9182 lambda_n: 1.038048086365953 Loss: 1.4280610644648491e-09\n",
      "Iteration: 9183 lambda_n: 1.0349616298448558 Loss: 1.4280626483979928e-09\n",
      "Iteration: 9184 lambda_n: 1.0178720427492238 Loss: 1.428064217013211e-09\n",
      "Iteration: 9185 lambda_n: 0.9317749349657733 Loss: 1.4280657585751152e-09\n",
      "Iteration: 9186 lambda_n: 0.9079628601145299 Loss: 1.4280671622147676e-09\n",
      "Iteration: 9187 lambda_n: 1.021692383353897 Loss: 1.4280685237948445e-09\n",
      "Iteration: 9188 lambda_n: 0.9837217781180221 Loss: 1.4280700480943432e-09\n",
      "Iteration: 9189 lambda_n: 0.9621958312242849 Loss: 1.4280715101098598e-09\n",
      "Iteration: 9190 lambda_n: 1.0029271305641296 Loss: 1.4280729318444103e-09\n",
      "Iteration: 9191 lambda_n: 1.0382372788211185 Loss: 1.4280744042223006e-09\n",
      "Iteration: 9192 lambda_n: 1.0107508049898741 Loss: 1.4280759275904592e-09\n",
      "Iteration: 9193 lambda_n: 1.0095615877158306 Loss: 1.4280774011733314e-09\n",
      "Iteration: 9194 lambda_n: 0.9398941998073287 Loss: 1.4280788634976923e-09\n",
      "Iteration: 9195 lambda_n: 0.9066297859981064 Loss: 1.42808022218829e-09\n",
      "Iteration: 9196 lambda_n: 0.9288588983442689 Loss: 1.4280815253049403e-09\n",
      "Iteration: 9197 lambda_n: 0.895691722468009 Loss: 1.4280828537230917e-09\n",
      "Iteration: 9198 lambda_n: 0.9185565961250283 Loss: 1.428084130790669e-09\n",
      "Iteration: 9199 lambda_n: 1.0243321254897675 Loss: 1.4280854302447138e-09\n",
      "Iteration: 9200 lambda_n: 1.037510914469026 Loss: 1.4280868778081094e-09\n",
      "Iteration: 9201 lambda_n: 1.0279143394578465 Loss: 1.4280883376694976e-09\n",
      "Iteration: 9202 lambda_n: 0.9925232243902883 Loss: 1.4280897759899165e-09\n",
      "Iteration: 9203 lambda_n: 1.017311378752328 Loss: 1.4280911591542233e-09\n",
      "Iteration: 9204 lambda_n: 0.9380299354106979 Loss: 1.4280925661669274e-09\n",
      "Iteration: 9205 lambda_n: 0.9215253919263806 Loss: 1.4280938627554714e-09\n",
      "Iteration: 9206 lambda_n: 1.0358948191988875 Loss: 1.4280951293873856e-09\n",
      "Iteration: 9207 lambda_n: 0.9770038078009451 Loss: 1.4280965481332703e-09\n",
      "Iteration: 9208 lambda_n: 0.9857745221091642 Loss: 1.4280978787216574e-09\n",
      "Iteration: 9209 lambda_n: 0.9076970667713061 Loss: 1.4280992135317978e-09\n",
      "Iteration: 9210 lambda_n: 0.937769588826806 Loss: 1.4281004383232091e-09\n",
      "Iteration: 9211 lambda_n: 1.0100961046459056 Loss: 1.4281016990498362e-09\n",
      "Iteration: 9212 lambda_n: 0.9077974620512083 Loss: 1.4281030507938878e-09\n",
      "Iteration: 9213 lambda_n: 0.966042858303779 Loss: 1.4281042601137671e-09\n",
      "Iteration: 9214 lambda_n: 1.0143489383671407 Loss: 1.4281055381807274e-09\n",
      "Iteration: 9215 lambda_n: 0.9067469879099589 Loss: 1.4281068786567547e-09\n",
      "Iteration: 9216 lambda_n: 0.9867601041665688 Loss: 1.428108071944107e-09\n",
      "Iteration: 9217 lambda_n: 0.9671001489780213 Loss: 1.4281093608726013e-09\n",
      "Iteration: 9218 lambda_n: 0.9565545648772893 Loss: 1.4281106220033798e-09\n",
      "Iteration: 9219 lambda_n: 0.9745097878263074 Loss: 1.428111862475806e-09\n",
      "Iteration: 9220 lambda_n: 1.0342899250015403 Loss: 1.428113121635497e-09\n",
      "Iteration: 9221 lambda_n: 0.9181852279430576 Loss: 1.428114450275289e-09\n",
      "Iteration: 9222 lambda_n: 0.9511717080772643 Loss: 1.4281156240297073e-09\n",
      "Iteration: 9223 lambda_n: 0.9320389085137057 Loss: 1.428116835389617e-09\n",
      "Iteration: 9224 lambda_n: 1.0437860126965037 Loss: 1.428118013763308e-09\n",
      "Iteration: 9225 lambda_n: 0.949912667489568 Loss: 1.4281193324212482e-09\n",
      "Iteration: 9226 lambda_n: 0.9250650068575932 Loss: 1.4281205263998038e-09\n",
      "Iteration: 9227 lambda_n: 0.954009638454915 Loss: 1.4281216836773634e-09\n",
      "Iteration: 9228 lambda_n: 0.9983625470421038 Loss: 1.4281228741054463e-09\n",
      "Iteration: 9229 lambda_n: 1.0206637256907587 Loss: 1.4281241096807796e-09\n",
      "Iteration: 9230 lambda_n: 0.9685860112194586 Loss: 1.4281253731046727e-09\n",
      "Iteration: 9231 lambda_n: 0.9945470768416744 Loss: 1.4281265647291337e-09\n",
      "Iteration: 9232 lambda_n: 1.0007187178025736 Loss: 1.4281277818182668e-09\n",
      "Iteration: 9233 lambda_n: 0.9131720296523594 Loss: 1.4281289994881617e-09\n",
      "Iteration: 9234 lambda_n: 0.9216895601508993 Loss: 1.42813010567893e-09\n",
      "Iteration: 9235 lambda_n: 1.0315743009633882 Loss: 1.4281312185747483e-09\n",
      "Iteration: 9236 lambda_n: 0.9602916067422096 Loss: 1.4281324586235767e-09\n",
      "Iteration: 9237 lambda_n: 0.9177890049390812 Loss: 1.4281336027014234e-09\n",
      "Iteration: 9238 lambda_n: 0.9821731629808446 Loss: 1.428134696350343e-09\n",
      "Iteration: 9239 lambda_n: 1.0421963845465645 Loss: 1.4281358609564e-09\n",
      "Iteration: 9240 lambda_n: 1.012860984517994 Loss: 1.4281370912434694e-09\n",
      "Iteration: 9241 lambda_n: 1.0462188169922952 Loss: 1.4281382784353655e-09\n",
      "Iteration: 9242 lambda_n: 0.9527476333073576 Loss: 1.4281395032517029e-09\n",
      "Iteration: 9243 lambda_n: 0.9531018158511716 Loss: 1.4281406107510595e-09\n",
      "Iteration: 9244 lambda_n: 0.9729908773989208 Loss: 1.4281417162872983e-09\n",
      "Iteration: 9245 lambda_n: 0.9421151347136832 Loss: 1.4281428395759901e-09\n",
      "Iteration: 9246 lambda_n: 1.0025202217169153 Loss: 1.4281439214022867e-09\n",
      "Iteration: 9247 lambda_n: 0.9745249849306534 Loss: 1.4281450638335297e-09\n",
      "Iteration: 9248 lambda_n: 0.9484396986791532 Loss: 1.4281461743799798e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9249 lambda_n: 0.9863653925563297 Loss: 1.4281472495816617e-09\n",
      "Iteration: 9250 lambda_n: 0.9733545350551889 Loss: 1.4281483600271195e-09\n",
      "Iteration: 9251 lambda_n: 0.9990571248531458 Loss: 1.4281494523129243e-09\n",
      "Iteration: 9252 lambda_n: 0.8972235751012843 Loss: 1.4281505692194934e-09\n",
      "Iteration: 9253 lambda_n: 0.9382673014415963 Loss: 1.4281515663506984e-09\n",
      "Iteration: 9254 lambda_n: 1.008380432807351 Loss: 1.42815260443512e-09\n",
      "Iteration: 9255 lambda_n: 0.9088097885352899 Loss: 1.4281537166285906e-09\n",
      "Iteration: 9256 lambda_n: 0.9528802450858603 Loss: 1.428154713458936e-09\n",
      "Iteration: 9257 lambda_n: 0.9229599253734562 Loss: 1.428155753799605e-09\n",
      "Iteration: 9258 lambda_n: 0.9354848624806333 Loss: 1.4281567587973524e-09\n",
      "Iteration: 9259 lambda_n: 0.9032369044416243 Loss: 1.4281577733736486e-09\n",
      "Iteration: 9260 lambda_n: 0.8976756338942528 Loss: 1.428158748093137e-09\n",
      "Iteration: 9261 lambda_n: 0.8995402670112944 Loss: 1.4281597106194033e-09\n",
      "Iteration: 9262 lambda_n: 1.0210276166282481 Loss: 1.4281606746362035e-09\n",
      "Iteration: 9263 lambda_n: 0.9853634063655898 Loss: 1.428161763535654e-09\n",
      "Iteration: 9264 lambda_n: 0.9775471621054707 Loss: 1.4281628081883978e-09\n",
      "Iteration: 9265 lambda_n: 0.9061539542386843 Loss: 1.4281638420315547e-09\n",
      "Iteration: 9266 lambda_n: 0.9429326485567797 Loss: 1.4281647948456838e-09\n",
      "Iteration: 9267 lambda_n: 0.9296572524554398 Loss: 1.4281657812100646e-09\n",
      "Iteration: 9268 lambda_n: 0.9694011788984076 Loss: 1.428166750779705e-09\n",
      "Iteration: 9269 lambda_n: 0.8964363966017058 Loss: 1.4281677549211464e-09\n",
      "Iteration: 9270 lambda_n: 0.9732154577945212 Loss: 1.4281686810415231e-09\n",
      "Iteration: 9271 lambda_n: 0.9523854134252797 Loss: 1.4281696821541173e-09\n",
      "Iteration: 9272 lambda_n: 1.008813338596971 Loss: 1.4281706567240859e-09\n",
      "Iteration: 9273 lambda_n: 0.8967840937591322 Loss: 1.4281716852272904e-09\n",
      "Iteration: 9274 lambda_n: 1.0282346911842704 Loss: 1.428172594696836e-09\n",
      "Iteration: 9275 lambda_n: 0.9648429982518753 Loss: 1.4281736294136889e-09\n",
      "Iteration: 9276 lambda_n: 0.9008937458454922 Loss: 1.4281745998144202e-09\n",
      "Iteration: 9277 lambda_n: 0.8937575526282999 Loss: 1.4281755041285864e-09\n",
      "Iteration: 9278 lambda_n: 0.9355892186769903 Loss: 1.4281763947133787e-09\n",
      "Iteration: 9279 lambda_n: 1.0426954193017801 Loss: 1.4281773217139244e-09\n",
      "Iteration: 9280 lambda_n: 0.9198560871742983 Loss: 1.4281783522192964e-09\n",
      "Iteration: 9281 lambda_n: 0.9081086955746115 Loss: 1.42817925854314e-09\n",
      "Iteration: 9282 lambda_n: 1.039311720417616 Loss: 1.4281801496466462e-09\n",
      "Iteration: 9283 lambda_n: 1.0006954612023502 Loss: 1.428181161971499e-09\n",
      "Iteration: 9284 lambda_n: 0.9994075575689102 Loss: 1.428182134111171e-09\n",
      "Iteration: 9285 lambda_n: 0.9338329856601673 Loss: 1.4281831001816744e-09\n",
      "Iteration: 9286 lambda_n: 1.0318563566392247 Loss: 1.4281839988041543e-09\n",
      "Iteration: 9287 lambda_n: 0.9164840094438742 Loss: 1.4281849870705963e-09\n",
      "Iteration: 9288 lambda_n: 1.0040625247829489 Loss: 1.4281858617368993e-09\n",
      "Iteration: 9289 lambda_n: 0.9878552765361073 Loss: 1.428186814505094e-09\n",
      "Iteration: 9290 lambda_n: 1.0171556060402251 Loss: 1.4281877469160774e-09\n",
      "Iteration: 9291 lambda_n: 0.9730006874441518 Loss: 1.428188702200163e-09\n",
      "Iteration: 9292 lambda_n: 0.9656404747952125 Loss: 1.4281896113069604e-09\n",
      "Iteration: 9293 lambda_n: 0.9826711454171557 Loss: 1.428190508409866e-09\n",
      "Iteration: 9294 lambda_n: 1.0060857375963679 Loss: 1.4281914174537955e-09\n",
      "Iteration: 9295 lambda_n: 0.9146215942274963 Loss: 1.4281923455858995e-09\n",
      "Iteration: 9296 lambda_n: 1.0337573971085163 Loss: 1.4281931842763376e-09\n",
      "Iteration: 9297 lambda_n: 1.035426859456889 Loss: 1.428194130470228e-09\n",
      "Iteration: 9298 lambda_n: 0.9989660584369632 Loss: 1.4281950742675903e-09\n",
      "Iteration: 9299 lambda_n: 1.0315545670503339 Loss: 1.4281959794697462e-09\n",
      "Iteration: 9300 lambda_n: 0.9822053797504671 Loss: 1.4281969098179212e-09\n",
      "Iteration: 9301 lambda_n: 0.893977008985308 Loss: 1.4281977908561362e-09\n",
      "Iteration: 9302 lambda_n: 1.043403049291981 Loss: 1.4281985896333847e-09\n",
      "Iteration: 9303 lambda_n: 1.0010352581290676 Loss: 1.4281995190116385e-09\n",
      "Iteration: 9304 lambda_n: 0.9199754046084258 Loss: 1.4282004051429338e-09\n",
      "Iteration: 9305 lambda_n: 0.9360594366990533 Loss: 1.4282012162017648e-09\n",
      "Iteration: 9306 lambda_n: 0.9651636940902908 Loss: 1.4282020377464882e-09\n",
      "Iteration: 9307 lambda_n: 0.898085836800213 Loss: 1.4282028822703739e-09\n",
      "Iteration: 9308 lambda_n: 0.9925613499158055 Loss: 1.4282036639150422e-09\n",
      "Iteration: 9309 lambda_n: 1.0047773215464835 Loss: 1.4282045246272026e-09\n",
      "Iteration: 9310 lambda_n: 1.0336502839486164 Loss: 1.4282053911742684e-09\n",
      "Iteration: 9311 lambda_n: 1.0195956319205086 Loss: 1.4282062798978578e-09\n",
      "Iteration: 9312 lambda_n: 0.9368667026301851 Loss: 1.4282071502359515e-09\n",
      "Iteration: 9313 lambda_n: 0.9817554332515251 Loss: 1.4282079460091356e-09\n",
      "Iteration: 9314 lambda_n: 0.8979650330775576 Loss: 1.4282087769261056e-09\n",
      "Iteration: 9315 lambda_n: 0.9381704713517337 Loss: 1.428209533915177e-09\n",
      "Iteration: 9316 lambda_n: 0.9173761403787132 Loss: 1.428210317038979e-09\n",
      "Iteration: 9317 lambda_n: 0.9762016783633789 Loss: 1.4282110842467782e-09\n",
      "Iteration: 9318 lambda_n: 0.9319071242598835 Loss: 1.4282118972294437e-09\n",
      "Iteration: 9319 lambda_n: 1.001243071262882 Loss: 1.428212668743291e-09\n",
      "Iteration: 9320 lambda_n: 1.0167210346503925 Loss: 1.4282134944466089e-09\n",
      "Iteration: 9321 lambda_n: 0.9597487760572115 Loss: 1.4282143298394017e-09\n",
      "Iteration: 9322 lambda_n: 0.988324555210776 Loss: 1.4282151140677926e-09\n",
      "Iteration: 9323 lambda_n: 1.0278653996714584 Loss: 1.4282159176714248e-09\n",
      "Iteration: 9324 lambda_n: 0.9112209645279091 Loss: 1.428216748161786e-09\n",
      "Iteration: 9325 lambda_n: 0.9852946780478813 Loss: 1.4282174813053518e-09\n",
      "Iteration: 9326 lambda_n: 0.9245343763549749 Loss: 1.428218271745713e-09\n",
      "Iteration: 9327 lambda_n: 0.955459869960156 Loss: 1.4282190056119346e-09\n",
      "Iteration: 9328 lambda_n: 1.0069746020218202 Loss: 1.4282197659328217e-09\n",
      "Iteration: 9329 lambda_n: 0.9516731234662767 Loss: 1.4282205634989668e-09\n",
      "Iteration: 9330 lambda_n: 0.9399465913012849 Loss: 1.4282213125513694e-09\n",
      "Iteration: 9331 lambda_n: 1.0433119839708407 Loss: 1.4282220493527267e-09\n",
      "Iteration: 9332 lambda_n: 0.8924691455906743 Loss: 1.4282228651653814e-09\n",
      "Iteration: 9333 lambda_n: 0.9499480256575891 Loss: 1.428223559391132e-09\n",
      "Iteration: 9334 lambda_n: 1.0073924468707847 Loss: 1.4282242956594988e-09\n",
      "Iteration: 9335 lambda_n: 0.9403295413556554 Loss: 1.4282250732508422e-09\n",
      "Iteration: 9336 lambda_n: 0.9594297119726962 Loss: 1.4282257955422691e-09\n",
      "Iteration: 9337 lambda_n: 0.9872844443894894 Loss: 1.4282265297234708e-09\n",
      "Iteration: 9338 lambda_n: 0.941325284871517 Loss: 1.4282272809622352e-09\n",
      "Iteration: 9339 lambda_n: 1.020593740676579 Loss: 1.4282279944513561e-09\n",
      "Iteration: 9340 lambda_n: 1.0000554658858984 Loss: 1.428228764756636e-09\n",
      "Iteration: 9341 lambda_n: 0.9362760916885716 Loss: 1.4282295138065276e-09\n",
      "Iteration: 9342 lambda_n: 0.9424672345276066 Loss: 1.428230211739673e-09\n",
      "Iteration: 9343 lambda_n: 0.8958000386937903 Loss: 1.4282309105134238e-09\n",
      "Iteration: 9344 lambda_n: 1.0281567327362044 Loss: 1.4282315724911432e-09\n",
      "Iteration: 9345 lambda_n: 0.9724024916979587 Loss: 1.428232329582812e-09\n",
      "Iteration: 9346 lambda_n: 0.9749673089024282 Loss: 1.4282330387657363e-09\n",
      "Iteration: 9347 lambda_n: 1.0345554707485285 Loss: 1.4282337504247602e-09\n",
      "Iteration: 9348 lambda_n: 0.9917906118972002 Loss: 1.4282345006969825e-09\n",
      "Iteration: 9349 lambda_n: 1.0113250371775173 Loss: 1.428235216854962e-09\n",
      "Iteration: 9350 lambda_n: 1.0010122363357576 Loss: 1.4282359442180054e-09\n",
      "Iteration: 9351 lambda_n: 0.9570795705749154 Loss: 1.4282366603605093e-09\n",
      "Iteration: 9352 lambda_n: 0.9204450697980507 Loss: 1.4282373421459151e-09\n",
      "Iteration: 9353 lambda_n: 0.9633523174980445 Loss: 1.4282379952916574e-09\n",
      "Iteration: 9354 lambda_n: 0.9829441410813909 Loss: 1.4282386751949582e-09\n",
      "Iteration: 9355 lambda_n: 0.9697902601506287 Loss: 1.4282393660584737e-09\n",
      "Iteration: 9356 lambda_n: 0.9409600057018821 Loss: 1.4282400449803302e-09\n",
      "Iteration: 9357 lambda_n: 0.9790272269231882 Loss: 1.428240701414817e-09\n",
      "Iteration: 9358 lambda_n: 1.0293012800149461 Loss: 1.4282413807983304e-09\n",
      "Iteration: 9359 lambda_n: 1.0154069732617332 Loss: 1.4282420882799813e-09\n",
      "Iteration: 9360 lambda_n: 0.9694586120665449 Loss: 1.4282427875295126e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9361 lambda_n: 0.9352103924561656 Loss: 1.4282434523374655e-09\n",
      "Iteration: 9362 lambda_n: 1.0313959812346398 Loss: 1.4282440935972687e-09\n",
      "Iteration: 9363 lambda_n: 1.0046594051070914 Loss: 1.4282447949115977e-09\n",
      "Iteration: 9364 lambda_n: 0.9789947307837669 Loss: 1.4282454764315454e-09\n",
      "Iteration: 9365 lambda_n: 1.0453521384710212 Loss: 1.4282461360332358e-09\n",
      "Iteration: 9366 lambda_n: 1.0181083462739542 Loss: 1.4282468361518486e-09\n",
      "Iteration: 9367 lambda_n: 1.0061916648524731 Loss: 1.428247515393998e-09\n",
      "Iteration: 9368 lambda_n: 0.9932138711794329 Loss: 1.428248183734613e-09\n",
      "Iteration: 9369 lambda_n: 1.006484747962155 Loss: 1.4282488355862764e-09\n",
      "Iteration: 9370 lambda_n: 0.9337504923695218 Loss: 1.428249497829088e-09\n",
      "Iteration: 9371 lambda_n: 1.0055781628135214 Loss: 1.428250108107275e-09\n",
      "Iteration: 9372 lambda_n: 1.0440195741905791 Loss: 1.4282507644721305e-09\n",
      "Iteration: 9373 lambda_n: 0.8978621977973941 Loss: 1.4282514415289692e-09\n",
      "Iteration: 9374 lambda_n: 1.0313177912558982 Loss: 1.4282520227238355e-09\n",
      "Iteration: 9375 lambda_n: 1.0235683892341483 Loss: 1.4282526844807626e-09\n",
      "Iteration: 9376 lambda_n: 1.0007457773228217 Loss: 1.4282533399523946e-09\n",
      "Iteration: 9377 lambda_n: 1.0089298694098863 Loss: 1.428253977357278e-09\n",
      "Iteration: 9378 lambda_n: 0.9542019120202191 Loss: 1.42825461719777e-09\n",
      "Iteration: 9379 lambda_n: 0.9921140910315546 Loss: 1.428255218746666e-09\n",
      "Iteration: 9380 lambda_n: 1.0237411140511647 Loss: 1.428255838057849e-09\n",
      "Iteration: 9381 lambda_n: 0.9731761168089318 Loss: 1.4282564790005377e-09\n",
      "Iteration: 9382 lambda_n: 0.8990446450259175 Loss: 1.4282570844167785e-09\n",
      "Iteration: 9383 lambda_n: 1.0177391198505341 Loss: 1.4282576420611058e-09\n",
      "Iteration: 9384 lambda_n: 0.8931866873263878 Loss: 1.428258270522757e-09\n",
      "Iteration: 9385 lambda_n: 0.9838497111557878 Loss: 1.428258819317529e-09\n",
      "Iteration: 9386 lambda_n: 0.9852465952175977 Loss: 1.4282594208412872e-09\n",
      "Iteration: 9387 lambda_n: 1.0147028365728343 Loss: 1.4282600207224493e-09\n",
      "Iteration: 9388 lambda_n: 0.9126271738621318 Loss: 1.4282606352501177e-09\n",
      "Iteration: 9389 lambda_n: 1.0300073062904356 Loss: 1.4282611827125784e-09\n",
      "Iteration: 9390 lambda_n: 0.9401853313408721 Loss: 1.4282618044593195e-09\n",
      "Iteration: 9391 lambda_n: 1.0472693673341034 Loss: 1.42826236790575e-09\n",
      "Iteration: 9392 lambda_n: 1.0015024855708 Loss: 1.428262989053057e-09\n",
      "Iteration: 9393 lambda_n: 1.0408678894279455 Loss: 1.4282635801465065e-09\n",
      "Iteration: 9394 lambda_n: 0.946951356782431 Loss: 1.4282641934028333e-09\n",
      "Iteration: 9395 lambda_n: 0.9617129669781919 Loss: 1.428264747763531e-09\n",
      "Iteration: 9396 lambda_n: 0.9315651708345392 Loss: 1.4282653096084148e-09\n",
      "Iteration: 9397 lambda_n: 1.0092779249193191 Loss: 1.4282658471161624e-09\n",
      "Iteration: 9398 lambda_n: 1.009656480499073 Loss: 1.4282664322665699e-09\n",
      "Iteration: 9399 lambda_n: 0.8998932502825864 Loss: 1.4282670151564835e-09\n",
      "Iteration: 9400 lambda_n: 0.9574838624229784 Loss: 1.428267531480631e-09\n",
      "Iteration: 9401 lambda_n: 0.9749861409275324 Loss: 1.4282680773440192e-09\n",
      "Iteration: 9402 lambda_n: 1.0146360388769562 Loss: 1.4282686315211766e-09\n",
      "Iteration: 9403 lambda_n: 0.9002951541012112 Loss: 1.4282692020395983e-09\n",
      "Iteration: 9404 lambda_n: 1.0362177586008279 Loss: 1.428269710786719e-09\n",
      "Iteration: 9405 lambda_n: 0.9622648033191705 Loss: 1.4282702933661465e-09\n",
      "Iteration: 9406 lambda_n: 0.9535296753175088 Loss: 1.4282708296540098e-09\n",
      "Iteration: 9407 lambda_n: 0.9012276820958273 Loss: 1.428271359130429e-09\n",
      "Iteration: 9408 lambda_n: 0.9772487542786954 Loss: 1.4282718579911825e-09\n",
      "Iteration: 9409 lambda_n: 0.9127123538609193 Loss: 1.428272395977633e-09\n",
      "Iteration: 9410 lambda_n: 1.0015433943777605 Loss: 1.4282728951588593e-09\n",
      "Iteration: 9411 lambda_n: 1.0442146490641235 Loss: 1.4282734420596488e-09\n",
      "Iteration: 9412 lambda_n: 1.0254900684629789 Loss: 1.428274009611616e-09\n",
      "Iteration: 9413 lambda_n: 0.9100649184251633 Loss: 1.428274563107702e-09\n",
      "Iteration: 9414 lambda_n: 0.9145237273331104 Loss: 1.4282750528148324e-09\n",
      "Iteration: 9415 lambda_n: 1.0206523297035373 Loss: 1.4282755428845138e-09\n",
      "Iteration: 9416 lambda_n: 0.8997276957216586 Loss: 1.4282760875182831e-09\n",
      "Iteration: 9417 lambda_n: 0.910493304382679 Loss: 1.4282765603287083e-09\n",
      "Iteration: 9418 lambda_n: 1.0311478029133743 Loss: 1.42827704199715e-09\n",
      "Iteration: 9419 lambda_n: 0.9672986944076992 Loss: 1.4282775849914092e-09\n",
      "Iteration: 9420 lambda_n: 0.9467766484143642 Loss: 1.4282780905488868e-09\n",
      "Iteration: 9421 lambda_n: 0.9945218209876291 Loss: 1.4282785840062638e-09\n",
      "Iteration: 9422 lambda_n: 0.9320616075582052 Loss: 1.4282791011209423e-09\n",
      "Iteration: 9423 lambda_n: 0.9347933873916913 Loss: 1.4282795840544117e-09\n",
      "Iteration: 9424 lambda_n: 1.0494619643071788 Loss: 1.4282800667034492e-09\n",
      "Iteration: 9425 lambda_n: 1.006748972144512 Loss: 1.4282806011812246e-09\n",
      "Iteration: 9426 lambda_n: 0.923268772270867 Loss: 1.4282811157608718e-09\n",
      "Iteration: 9427 lambda_n: 1.0382366932254303 Loss: 1.4282815841066372e-09\n",
      "Iteration: 9428 lambda_n: 1.026249903351365 Loss: 1.4282821095271923e-09\n",
      "Iteration: 9429 lambda_n: 1.0046287741719404 Loss: 1.428282622710846e-09\n",
      "Iteration: 9430 lambda_n: 1.0464228920074383 Loss: 1.4282831283099201e-09\n",
      "Iteration: 9431 lambda_n: 0.9041197324262983 Loss: 1.4282836486435254e-09\n",
      "Iteration: 9432 lambda_n: 0.9986885239418909 Loss: 1.4282840970784018e-09\n",
      "Iteration: 9433 lambda_n: 0.9741896820961709 Loss: 1.4282845895190825e-09\n",
      "Iteration: 9434 lambda_n: 0.9573885861525322 Loss: 1.4282850674476707e-09\n",
      "Iteration: 9435 lambda_n: 0.9036158465351736 Loss: 1.4282855357378506e-09\n",
      "Iteration: 9436 lambda_n: 0.9172484959984655 Loss: 1.428285971874922e-09\n",
      "Iteration: 9437 lambda_n: 1.0460884592099555 Loss: 1.4282864172639835e-09\n",
      "Iteration: 9438 lambda_n: 0.8963935071422072 Loss: 1.4282869212967658e-09\n",
      "Iteration: 9439 lambda_n: 1.0056781476118954 Loss: 1.4282873526707592e-09\n",
      "Iteration: 9440 lambda_n: 1.0334382220400349 Loss: 1.4282878335307945e-09\n",
      "Iteration: 9441 lambda_n: 0.9762784813024556 Loss: 1.4282883239390694e-09\n",
      "Iteration: 9442 lambda_n: 1.0261042938138347 Loss: 1.4282887866382253e-09\n",
      "Iteration: 9443 lambda_n: 0.9481254420668227 Loss: 1.4282892697928848e-09\n",
      "Iteration: 9444 lambda_n: 0.9931868163814979 Loss: 1.4282897136043758e-09\n",
      "Iteration: 9445 lambda_n: 0.9631900415689976 Loss: 1.4282901765273486e-09\n",
      "Iteration: 9446 lambda_n: 1.0396466858119708 Loss: 1.428290623786836e-09\n",
      "Iteration: 9447 lambda_n: 0.9614011381807136 Loss: 1.42829110548373e-09\n",
      "Iteration: 9448 lambda_n: 1.0268201152810306 Loss: 1.4282915473220269e-09\n",
      "Iteration: 9449 lambda_n: 0.997062855054576 Loss: 1.428292018158025e-09\n",
      "Iteration: 9450 lambda_n: 0.9665348158676172 Loss: 1.4282924707929627e-09\n",
      "Iteration: 9451 lambda_n: 0.9361105419378855 Loss: 1.428292910607808e-09\n",
      "Iteration: 9452 lambda_n: 1.0021584491182898 Loss: 1.4282933358460258e-09\n",
      "Iteration: 9453 lambda_n: 0.9986645874056642 Loss: 1.4282937884202852e-09\n",
      "Iteration: 9454 lambda_n: 0.9777256546602866 Loss: 1.428294236872231e-09\n",
      "Iteration: 9455 lambda_n: 1.0326038236073634 Loss: 1.4282946749639354e-09\n",
      "Iteration: 9456 lambda_n: 0.895126947680092 Loss: 1.4282951352026184e-09\n",
      "Iteration: 9457 lambda_n: 0.9323339590048604 Loss: 1.4282955315822371e-09\n",
      "Iteration: 9458 lambda_n: 0.9606411141519854 Loss: 1.4282959425220712e-09\n",
      "Iteration: 9459 lambda_n: 0.997330372744169 Loss: 1.4282963642262546e-09\n",
      "Iteration: 9460 lambda_n: 1.0174042879201488 Loss: 1.428296801677817e-09\n",
      "Iteration: 9461 lambda_n: 0.9047963504501665 Loss: 1.42829724445964e-09\n",
      "Iteration: 9462 lambda_n: 0.9351010974714737 Loss: 1.428297636415408e-09\n",
      "Iteration: 9463 lambda_n: 0.9402591226432092 Loss: 1.4282980391131574e-09\n",
      "Iteration: 9464 lambda_n: 0.9617279078593227 Loss: 1.4282984428639946e-09\n",
      "Iteration: 9465 lambda_n: 0.9582801983930519 Loss: 1.4282988544137563e-09\n",
      "Iteration: 9466 lambda_n: 0.9576567289664213 Loss: 1.4282992615286773e-09\n",
      "Iteration: 9467 lambda_n: 0.907631511203924 Loss: 1.4282996668300759e-09\n",
      "Iteration: 9468 lambda_n: 0.9991104421528021 Loss: 1.428300045510149e-09\n",
      "Iteration: 9469 lambda_n: 0.9691226717650524 Loss: 1.4283004646677534e-09\n",
      "Iteration: 9470 lambda_n: 0.9001262013257669 Loss: 1.4283008687383983e-09\n",
      "Iteration: 9471 lambda_n: 1.0110988106680272 Loss: 1.4283012418832595e-09\n",
      "Iteration: 9472 lambda_n: 0.9233211677759471 Loss: 1.4283016607439434e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9473 lambda_n: 0.9552373008575166 Loss: 1.4283020392420032e-09\n",
      "Iteration: 9474 lambda_n: 0.8967814235149553 Loss: 1.4283024319519963e-09\n",
      "Iteration: 9475 lambda_n: 0.9911109834937363 Loss: 1.428302798202219e-09\n",
      "Iteration: 9476 lambda_n: 0.9130753519430607 Loss: 1.4283032024031893e-09\n",
      "Iteration: 9477 lambda_n: 1.0060115435917922 Loss: 1.4283035683623916e-09\n",
      "Iteration: 9478 lambda_n: 1.0067147410871187 Loss: 1.428303976143859e-09\n",
      "Iteration: 9479 lambda_n: 1.0015957420964507 Loss: 1.4283043793338022e-09\n",
      "Iteration: 9480 lambda_n: 0.9426687915635217 Loss: 1.4283047796899532e-09\n",
      "Iteration: 9481 lambda_n: 0.9846072575716731 Loss: 1.4283051504445233e-09\n",
      "Iteration: 9482 lambda_n: 0.9307966321810374 Loss: 1.4283055398465309e-09\n",
      "Iteration: 9483 lambda_n: 0.9112096970548439 Loss: 1.4283059076716466e-09\n",
      "Iteration: 9484 lambda_n: 1.0096157288405836 Loss: 1.4283062662896693e-09\n",
      "Iteration: 9485 lambda_n: 0.9076883711482209 Loss: 1.4283066622579537e-09\n",
      "Iteration: 9486 lambda_n: 0.9103730526071282 Loss: 1.4283070166875976e-09\n",
      "Iteration: 9487 lambda_n: 0.9080017477250371 Loss: 1.4283073671250108e-09\n",
      "Iteration: 9488 lambda_n: 0.9007122082014188 Loss: 1.4283077206909336e-09\n",
      "Iteration: 9489 lambda_n: 0.970428309070889 Loss: 1.428308068091129e-09\n",
      "Iteration: 9490 lambda_n: 0.9199019787481906 Loss: 1.4283084404091766e-09\n",
      "Iteration: 9491 lambda_n: 0.9364447583250542 Loss: 1.4283087916297123e-09\n",
      "Iteration: 9492 lambda_n: 0.9951663609067539 Loss: 1.4283091480516283e-09\n",
      "Iteration: 9493 lambda_n: 1.0076469765505331 Loss: 1.4283095215143362e-09\n",
      "Iteration: 9494 lambda_n: 0.9058180138652183 Loss: 1.4283099025972946e-09\n",
      "Iteration: 9495 lambda_n: 0.974666049456214 Loss: 1.4283102425589037e-09\n",
      "Iteration: 9496 lambda_n: 0.9965790158377672 Loss: 1.4283106067393618e-09\n",
      "Iteration: 9497 lambda_n: 0.907229145671021 Loss: 1.4283109771776075e-09\n",
      "Iteration: 9498 lambda_n: 0.9448717830946084 Loss: 1.4283113131370215e-09\n",
      "Iteration: 9499 lambda_n: 1.0230727885503021 Loss: 1.4283116576150875e-09\n",
      "Iteration: 9500 lambda_n: 0.9241985389557941 Loss: 1.4283120321922354e-09\n",
      "Iteration: 9501 lambda_n: 0.974556874962076 Loss: 1.4283123682465916e-09\n",
      "Iteration: 9502 lambda_n: 1.038121814768689 Loss: 1.428312722996493e-09\n",
      "Iteration: 9503 lambda_n: 0.8990677516496108 Loss: 1.428313099433395e-09\n",
      "Iteration: 9504 lambda_n: 1.0244233738061543 Loss: 1.4283134239251781e-09\n",
      "Iteration: 9505 lambda_n: 0.975306688319631 Loss: 1.428313791553846e-09\n",
      "Iteration: 9506 lambda_n: 0.9595882087184723 Loss: 1.4283141401500243e-09\n",
      "Iteration: 9507 lambda_n: 0.9743025753254267 Loss: 1.4283144805709561e-09\n",
      "Iteration: 9508 lambda_n: 1.0104080187559814 Loss: 1.4283148206723278e-09\n",
      "Iteration: 9509 lambda_n: 0.9221218330086767 Loss: 1.4283151783599313e-09\n",
      "Iteration: 9510 lambda_n: 0.9005701392195462 Loss: 1.428315501237704e-09\n",
      "Iteration: 9511 lambda_n: 0.9477733934984044 Loss: 1.4283158166940057e-09\n",
      "Iteration: 9512 lambda_n: 1.0224983588653447 Loss: 1.4283161453522817e-09\n",
      "Iteration: 9513 lambda_n: 0.9043285772762272 Loss: 1.4283164947615096e-09\n",
      "Iteration: 9514 lambda_n: 0.9213654950582113 Loss: 1.428316807910066e-09\n",
      "Iteration: 9515 lambda_n: 1.0238508262971202 Loss: 1.4283171220765187e-09\n",
      "Iteration: 9516 lambda_n: 1.0468520890568471 Loss: 1.4283174709522695e-09\n",
      "Iteration: 9517 lambda_n: 1.0197177787761993 Loss: 1.4283178210548777e-09\n",
      "Iteration: 9518 lambda_n: 0.9993073515228453 Loss: 1.4283181653743421e-09\n",
      "Iteration: 9519 lambda_n: 0.9106015766056632 Loss: 1.4283185014492405e-09\n",
      "Iteration: 9520 lambda_n: 1.044000894135584 Loss: 1.428318804857632e-09\n",
      "Iteration: 9521 lambda_n: 0.9330112832314293 Loss: 1.428319149239703e-09\n",
      "Iteration: 9522 lambda_n: 1.0443309427215677 Loss: 1.4283194598515942e-09\n",
      "Iteration: 9523 lambda_n: 0.9675058678334569 Loss: 1.4283198042698158e-09\n",
      "Iteration: 9524 lambda_n: 0.9336800866237848 Loss: 1.4283201224595339e-09\n",
      "Iteration: 9525 lambda_n: 0.9620673905629958 Loss: 1.428320427455958e-09\n",
      "Iteration: 9526 lambda_n: 0.9413023725404797 Loss: 1.428320735788396e-09\n",
      "Iteration: 9527 lambda_n: 1.007842735731168 Loss: 1.4283210417729357e-09\n",
      "Iteration: 9528 lambda_n: 0.945954158106265 Loss: 1.4283213661102055e-09\n",
      "Iteration: 9529 lambda_n: 0.9973837335376008 Loss: 1.4283216686126468e-09\n",
      "Iteration: 9530 lambda_n: 0.9503081516664802 Loss: 1.4283219817224264e-09\n",
      "Iteration: 9531 lambda_n: 0.9181110431828858 Loss: 1.4283222842697873e-09\n",
      "Iteration: 9532 lambda_n: 0.9147497529965813 Loss: 1.4283225743337808e-09\n",
      "Iteration: 9533 lambda_n: 0.9486303946685565 Loss: 1.4283228631830528e-09\n",
      "Iteration: 9534 lambda_n: 1.0308385301566996 Loss: 1.4283231609029467e-09\n",
      "Iteration: 9535 lambda_n: 1.0060079583081978 Loss: 1.4283234780741598e-09\n",
      "Iteration: 9536 lambda_n: 0.8929447515552125 Loss: 1.42832379071984e-09\n",
      "Iteration: 9537 lambda_n: 0.9312786660790145 Loss: 1.4283240681063938e-09\n",
      "Iteration: 9538 lambda_n: 0.8997395328794635 Loss: 1.4283243550344117e-09\n",
      "Iteration: 9539 lambda_n: 0.946528205366363 Loss: 1.4283246303598834e-09\n",
      "Iteration: 9540 lambda_n: 0.9153877054811435 Loss: 1.4283249154268413e-09\n",
      "Iteration: 9541 lambda_n: 0.9887654168469666 Loss: 1.4283251948360743e-09\n",
      "Iteration: 9542 lambda_n: 1.032922243925262 Loss: 1.4283254942091989e-09\n",
      "Iteration: 9543 lambda_n: 1.0381755898661646 Loss: 1.4283258050299048e-09\n",
      "Iteration: 9544 lambda_n: 0.9160791640328884 Loss: 1.4283261165673329e-09\n",
      "Iteration: 9545 lambda_n: 1.033153219265946 Loss: 1.428326388019178e-09\n",
      "Iteration: 9546 lambda_n: 1.0275637171866423 Loss: 1.4283266907080409e-09\n",
      "Iteration: 9547 lambda_n: 1.0234875917783317 Loss: 1.4283269960334912e-09\n",
      "Iteration: 9548 lambda_n: 1.0352056088432078 Loss: 1.4283272981396517e-09\n",
      "Iteration: 9549 lambda_n: 0.8983007752312077 Loss: 1.4283276031040007e-09\n",
      "Iteration: 9550 lambda_n: 0.9276789043499416 Loss: 1.4283278641244315e-09\n",
      "Iteration: 9551 lambda_n: 0.9580286693277852 Loss: 1.4283281332859906e-09\n",
      "Iteration: 9552 lambda_n: 1.0222014616684967 Loss: 1.4283284116003744e-09\n",
      "Iteration: 9553 lambda_n: 0.915474852673421 Loss: 1.4283287058217906e-09\n",
      "Iteration: 9554 lambda_n: 0.9158751350003415 Loss: 1.428328967600318e-09\n",
      "Iteration: 9555 lambda_n: 0.9040837917455163 Loss: 1.4283292276344697e-09\n",
      "Iteration: 9556 lambda_n: 0.9209415471763563 Loss: 1.4283294837091795e-09\n",
      "Iteration: 9557 lambda_n: 0.976388331993241 Loss: 1.4283297438749438e-09\n",
      "Iteration: 9558 lambda_n: 0.9512754481313407 Loss: 1.4283300192195473e-09\n",
      "Iteration: 9559 lambda_n: 0.9176705394933002 Loss: 1.4283302860824838e-09\n",
      "Iteration: 9560 lambda_n: 1.022295861801331 Loss: 1.4283305425518682e-09\n",
      "Iteration: 9561 lambda_n: 1.0476663315800343 Loss: 1.4283308276313767e-09\n",
      "Iteration: 9562 lambda_n: 1.0287328374234903 Loss: 1.4283311133731792e-09\n",
      "Iteration: 9563 lambda_n: 0.8975593432725544 Loss: 1.4283313971383852e-09\n",
      "Iteration: 9564 lambda_n: 0.9468899627321978 Loss: 1.4283316442850556e-09\n",
      "Iteration: 9565 lambda_n: 0.910669654205004 Loss: 1.4283319050438983e-09\n",
      "Iteration: 9566 lambda_n: 1.0137490791631951 Loss: 1.4283321532492172e-09\n",
      "Iteration: 9567 lambda_n: 0.9020558176376826 Loss: 1.4283324286017025e-09\n",
      "Iteration: 9568 lambda_n: 0.9342616713326115 Loss: 1.4283326718086607e-09\n",
      "Iteration: 9569 lambda_n: 0.9579648008591037 Loss: 1.4283329196223841e-09\n",
      "Iteration: 9570 lambda_n: 0.9501167257216305 Loss: 1.42833317659186e-09\n",
      "Iteration: 9571 lambda_n: 0.9413156061080925 Loss: 1.428333430155415e-09\n",
      "Iteration: 9572 lambda_n: 1.0256284440428227 Loss: 1.4283336794889761e-09\n",
      "Iteration: 9573 lambda_n: 0.9596644432336963 Loss: 1.4283339499081693e-09\n",
      "Iteration: 9574 lambda_n: 0.9904693251140465 Loss: 1.428334197296611e-09\n",
      "Iteration: 9575 lambda_n: 1.0082910730356915 Loss: 1.4283344558941804e-09\n",
      "Iteration: 9576 lambda_n: 1.016196269684769 Loss: 1.4283347187579322e-09\n",
      "Iteration: 9577 lambda_n: 0.9860455675421536 Loss: 1.428334980915719e-09\n",
      "Iteration: 9578 lambda_n: 0.967099333751957 Loss: 1.4283352353069428e-09\n",
      "Iteration: 9579 lambda_n: 0.9423702004946112 Loss: 1.4283354834338643e-09\n",
      "Iteration: 9580 lambda_n: 0.9942614515696147 Loss: 1.428335722894595e-09\n",
      "Iteration: 9581 lambda_n: 1.0468423903811521 Loss: 1.4283359747165327e-09\n",
      "Iteration: 9582 lambda_n: 0.9048149933461149 Loss: 1.4283362402846308e-09\n",
      "Iteration: 9583 lambda_n: 1.02901912216244 Loss: 1.4283364693302412e-09\n",
      "Iteration: 9584 lambda_n: 0.8954831411804239 Loss: 1.4283367277685198e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9585 lambda_n: 1.0121252826268055 Loss: 1.428336950899517e-09\n",
      "Iteration: 9586 lambda_n: 0.9843036008069461 Loss: 1.4283372025428917e-09\n",
      "Iteration: 9587 lambda_n: 0.9365474119070696 Loss: 1.428337441259361e-09\n",
      "Iteration: 9588 lambda_n: 0.8943502998323903 Loss: 1.4283376730705557e-09\n",
      "Iteration: 9589 lambda_n: 0.9861208656183162 Loss: 1.4283378915946373e-09\n",
      "Iteration: 9590 lambda_n: 1.002235537707896 Loss: 1.4283381323163785e-09\n",
      "Iteration: 9591 lambda_n: 0.9146010225357234 Loss: 1.428338375202197e-09\n",
      "Iteration: 9592 lambda_n: 1.0465367439498847 Loss: 1.4283385964328804e-09\n",
      "Iteration: 9593 lambda_n: 0.9168884263351116 Loss: 1.4283388483894164e-09\n",
      "Iteration: 9594 lambda_n: 0.9180740465556159 Loss: 1.4283390676431586e-09\n",
      "Iteration: 9595 lambda_n: 0.9552896934907045 Loss: 1.4283392869211333e-09\n",
      "Iteration: 9596 lambda_n: 0.9842434450112698 Loss: 1.4283395145753958e-09\n",
      "Iteration: 9597 lambda_n: 0.9324646293606431 Loss: 1.4283397431585961e-09\n",
      "Iteration: 9598 lambda_n: 0.9949795366284118 Loss: 1.4283399614585126e-09\n",
      "Iteration: 9599 lambda_n: 0.9345977392888761 Loss: 1.4283401972648447e-09\n",
      "Iteration: 9600 lambda_n: 1.0394409830810984 Loss: 1.4283404154088687e-09\n",
      "Iteration: 9601 lambda_n: 0.979240920954905 Loss: 1.4283406568338596e-09\n",
      "Iteration: 9602 lambda_n: 0.9448001617440929 Loss: 1.4283408795192392e-09\n",
      "Iteration: 9603 lambda_n: 1.0422501326223348 Loss: 1.4283410984191737e-09\n",
      "Iteration: 9604 lambda_n: 0.9665231478657623 Loss: 1.4283413370693986e-09\n",
      "Iteration: 9605 lambda_n: 0.9729472016830608 Loss: 1.4283415580149003e-09\n",
      "Iteration: 9606 lambda_n: 0.9363134464003773 Loss: 1.4283417791151557e-09\n",
      "Iteration: 9607 lambda_n: 0.9824345688267319 Loss: 1.4283419907002115e-09\n",
      "Iteration: 9608 lambda_n: 1.02469601236753 Loss: 1.4283422118163509e-09\n",
      "Iteration: 9609 lambda_n: 0.9013523475781167 Loss: 1.4283424369112916e-09\n",
      "Iteration: 9610 lambda_n: 1.0097798587237914 Loss: 1.428342638664979e-09\n",
      "Iteration: 9611 lambda_n: 0.9635521223725397 Loss: 1.4283428630140524e-09\n",
      "Iteration: 9612 lambda_n: 0.993131162282771 Loss: 1.4283430762384908e-09\n",
      "Iteration: 9613 lambda_n: 0.9283451695181619 Loss: 1.4283432955444684e-09\n",
      "Iteration: 9614 lambda_n: 0.9363858028606443 Loss: 1.4283434986521933e-09\n",
      "Iteration: 9615 lambda_n: 1.0209404815909182 Loss: 1.4283437000799604e-09\n",
      "Iteration: 9616 lambda_n: 0.95741646175799 Loss: 1.428343924150877e-09\n",
      "Iteration: 9617 lambda_n: 1.0259989388285464 Loss: 1.4283441326707719e-09\n",
      "Iteration: 9618 lambda_n: 1.0127013478215894 Loss: 1.4283443542813683e-09\n",
      "Iteration: 9619 lambda_n: 0.9879568463941021 Loss: 1.4283445676534463e-09\n",
      "Iteration: 9620 lambda_n: 1.0072831316272148 Loss: 1.4283447799834432e-09\n",
      "Iteration: 9621 lambda_n: 0.9519369165657895 Loss: 1.4283449950847718e-09\n",
      "Iteration: 9622 lambda_n: 0.9781645978461907 Loss: 1.4283451978637865e-09\n",
      "Iteration: 9623 lambda_n: 1.0052525665998608 Loss: 1.4283454045669513e-09\n",
      "Iteration: 9624 lambda_n: 1.0032461759567284 Loss: 1.4283456152619163e-09\n",
      "Iteration: 9625 lambda_n: 0.9648885149792646 Loss: 1.428345820630127e-09\n",
      "Iteration: 9626 lambda_n: 0.9125972191447784 Loss: 1.4283460208462866e-09\n",
      "Iteration: 9627 lambda_n: 1.0161283225969988 Loss: 1.4283462099385314e-09\n",
      "Iteration: 9628 lambda_n: 0.9078105400678508 Loss: 1.4283464193106479e-09\n",
      "Iteration: 9629 lambda_n: 0.9511471876618451 Loss: 1.4283466052473516e-09\n",
      "Iteration: 9630 lambda_n: 0.9482000997844611 Loss: 1.4283467993895756e-09\n",
      "Iteration: 9631 lambda_n: 0.9425007959932499 Loss: 1.428346992359989e-09\n",
      "Iteration: 9632 lambda_n: 0.9885703882553359 Loss: 1.4283471828713912e-09\n",
      "Iteration: 9633 lambda_n: 0.9166916428042776 Loss: 1.428347378022012e-09\n",
      "Iteration: 9634 lambda_n: 1.0349589921562459 Loss: 1.4283475630381939e-09\n",
      "Iteration: 9635 lambda_n: 0.9917058511225043 Loss: 1.4283477705186002e-09\n",
      "Iteration: 9636 lambda_n: 0.9443704345850488 Loss: 1.4283479677242117e-09\n",
      "Iteration: 9637 lambda_n: 0.9049361683778767 Loss: 1.4283481541446747e-09\n",
      "Iteration: 9638 lambda_n: 1.0043346332330214 Loss: 1.4283483326148138e-09\n",
      "Iteration: 9639 lambda_n: 0.9702426350990738 Loss: 1.4283485286788821e-09\n",
      "Iteration: 9640 lambda_n: 0.9399118013568817 Loss: 1.428348716321893e-09\n",
      "Iteration: 9641 lambda_n: 0.9034925444627643 Loss: 1.4283488943229087e-09\n",
      "Iteration: 9642 lambda_n: 0.9494169272580005 Loss: 1.428349070015584e-09\n",
      "Iteration: 9643 lambda_n: 0.9928042268114553 Loss: 1.4283492537065e-09\n",
      "Iteration: 9644 lambda_n: 0.954042624282443 Loss: 1.4283494437505011e-09\n",
      "Iteration: 9645 lambda_n: 1.0395674502096115 Loss: 1.4283496231113895e-09\n",
      "Iteration: 9646 lambda_n: 0.9545742442738094 Loss: 1.4283498216077785e-09\n",
      "Iteration: 9647 lambda_n: 0.9924242074532937 Loss: 1.4283500035799872e-09\n",
      "Iteration: 9648 lambda_n: 0.9660177835804145 Loss: 1.428350190096017e-09\n",
      "Iteration: 9649 lambda_n: 0.8976197842769903 Loss: 1.4283503668683264e-09\n",
      "Iteration: 9650 lambda_n: 1.0185273538919715 Loss: 1.4283505360472453e-09\n",
      "Iteration: 9651 lambda_n: 1.0458957574950771 Loss: 1.4283507259189335e-09\n",
      "Iteration: 9652 lambda_n: 0.9089083995358013 Loss: 1.4283509184588632e-09\n",
      "Iteration: 9653 lambda_n: 0.9466040180370399 Loss: 1.4283510860771607e-09\n",
      "Iteration: 9654 lambda_n: 0.9935188911770714 Loss: 1.4283512610267685e-09\n",
      "Iteration: 9655 lambda_n: 0.9028000951501187 Loss: 1.4283514431075386e-09\n",
      "Iteration: 9656 lambda_n: 0.9251771603575779 Loss: 1.4283516074661887e-09\n",
      "Iteration: 9657 lambda_n: 0.9129171157653725 Loss: 1.4283517711669653e-09\n",
      "Iteration: 9658 lambda_n: 0.9129448966335673 Loss: 1.428351937676336e-09\n",
      "Iteration: 9659 lambda_n: 0.9867941078502053 Loss: 1.4283521024258817e-09\n",
      "Iteration: 9660 lambda_n: 0.9974317069739653 Loss: 1.4283522797427077e-09\n",
      "Iteration: 9661 lambda_n: 0.9710695562286679 Loss: 1.428352458003226e-09\n",
      "Iteration: 9662 lambda_n: 0.9306158270556798 Loss: 1.4283526310052816e-09\n",
      "Iteration: 9663 lambda_n: 0.9867492174021119 Loss: 1.4283527914625301e-09\n",
      "Iteration: 9664 lambda_n: 1.0417456940006211 Loss: 1.42835296559634e-09\n",
      "Iteration: 9665 lambda_n: 1.0483592090834748 Loss: 1.428353147674225e-09\n",
      "Iteration: 9666 lambda_n: 1.0158951086366041 Loss: 1.4283533292889939e-09\n",
      "Iteration: 9667 lambda_n: 1.047883333934072 Loss: 1.4283535041397511e-09\n",
      "Iteration: 9668 lambda_n: 1.0297226636800287 Loss: 1.4283536833785993e-09\n",
      "Iteration: 9669 lambda_n: 0.9383386428120397 Loss: 1.4283538559739262e-09\n",
      "Iteration: 9670 lambda_n: 1.0161996566834974 Loss: 1.4283540157653773e-09\n",
      "Iteration: 9671 lambda_n: 1.0050494074247511 Loss: 1.4283541877301918e-09\n",
      "Iteration: 9672 lambda_n: 0.934518450469468 Loss: 1.4283543527266566e-09\n",
      "Iteration: 9673 lambda_n: 1.0417012511329282 Loss: 1.4283545098818473e-09\n",
      "Iteration: 9674 lambda_n: 1.005618568913698 Loss: 1.4283546849195923e-09\n",
      "Iteration: 9675 lambda_n: 0.9139483246635017 Loss: 1.4283548504654068e-09\n",
      "Iteration: 9676 lambda_n: 1.027868341663164 Loss: 1.4283550011024056e-09\n",
      "Iteration: 9677 lambda_n: 1.0116586706100363 Loss: 1.428355167217318e-09\n",
      "Iteration: 9678 lambda_n: 0.9709946748104837 Loss: 1.4283553349872836e-09\n",
      "Iteration: 9679 lambda_n: 1.042254500987378 Loss: 1.4283554935310984e-09\n",
      "Iteration: 9680 lambda_n: 0.9935898344973112 Loss: 1.4283556592015707e-09\n",
      "Iteration: 9681 lambda_n: 0.9067782994354714 Loss: 1.4283558211242195e-09\n",
      "Iteration: 9682 lambda_n: 0.8935296997713372 Loss: 1.4283559676733856e-09\n",
      "Iteration: 9683 lambda_n: 1.0491553534402394 Loss: 1.428356110883222e-09\n",
      "Iteration: 9684 lambda_n: 0.951512420798228 Loss: 1.428356278400315e-09\n",
      "Iteration: 9685 lambda_n: 0.9134623391379026 Loss: 1.4283564265595774e-09\n",
      "Iteration: 9686 lambda_n: 0.9605262453966491 Loss: 1.4283565715022508e-09\n",
      "Iteration: 9687 lambda_n: 1.0351735095193408 Loss: 1.4283567235095078e-09\n",
      "Iteration: 9688 lambda_n: 0.9916249708400732 Loss: 1.4283568872282311e-09\n",
      "Iteration: 9689 lambda_n: 0.9521055747178434 Loss: 1.4283570416327413e-09\n",
      "Iteration: 9690 lambda_n: 1.0094499940762294 Loss: 1.4283571857883953e-09\n",
      "Iteration: 9691 lambda_n: 1.0258128448992574 Loss: 1.4283573440201714e-09\n",
      "Iteration: 9692 lambda_n: 0.9546383928905825 Loss: 1.4283575025569681e-09\n",
      "Iteration: 9693 lambda_n: 0.9958566761071571 Loss: 1.428357648708906e-09\n",
      "Iteration: 9694 lambda_n: 0.8997221416712208 Loss: 1.4283578003780304e-09\n",
      "Iteration: 9695 lambda_n: 0.9104755053752241 Loss: 1.4283579325702877e-09\n",
      "Iteration: 9696 lambda_n: 0.9602393022316693 Loss: 1.4283580711453551e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9697 lambda_n: 1.0125824843957751 Loss: 1.428358216752762e-09\n",
      "Iteration: 9698 lambda_n: 1.0089466258928856 Loss: 1.4283583678085258e-09\n",
      "Iteration: 9699 lambda_n: 1.0077212994914533 Loss: 1.4283585145998093e-09\n",
      "Iteration: 9700 lambda_n: 1.0163389822760784 Loss: 1.4283586647871294e-09\n",
      "Iteration: 9701 lambda_n: 1.030058225317896 Loss: 1.4283588159507686e-09\n",
      "Iteration: 9702 lambda_n: 0.9015820863153223 Loss: 1.428358967756161e-09\n",
      "Iteration: 9703 lambda_n: 1.01997781825559 Loss: 1.4283590965362345e-09\n",
      "Iteration: 9704 lambda_n: 1.0394381351790514 Loss: 1.42835924669655e-09\n",
      "Iteration: 9705 lambda_n: 0.9921998781401149 Loss: 1.4283593982694156e-09\n",
      "Iteration: 9706 lambda_n: 0.9185123309841895 Loss: 1.4283595372333743e-09\n",
      "Iteration: 9707 lambda_n: 0.9969915395398724 Loss: 1.4283596702925812e-09\n",
      "Iteration: 9708 lambda_n: 0.9679020165303702 Loss: 1.428359813078666e-09\n",
      "Iteration: 9709 lambda_n: 0.9323401974440501 Loss: 1.4283599506449544e-09\n",
      "Iteration: 9710 lambda_n: 1.0442518144507011 Loss: 1.4283600824755495e-09\n",
      "Iteration: 9711 lambda_n: 0.9298883286639242 Loss: 1.4283602296687866e-09\n",
      "Iteration: 9712 lambda_n: 0.9731637876565604 Loss: 1.428360360096397e-09\n",
      "Iteration: 9713 lambda_n: 0.9857108805231286 Loss: 1.428360492360576e-09\n",
      "Iteration: 9714 lambda_n: 0.9644523707966304 Loss: 1.4283606316380494e-09\n",
      "Iteration: 9715 lambda_n: 0.9199047182831215 Loss: 1.4283607658028882e-09\n",
      "Iteration: 9716 lambda_n: 0.8989062067948058 Loss: 1.4283608926712558e-09\n",
      "Iteration: 9717 lambda_n: 0.9493010047146999 Loss: 1.4283610124561221e-09\n",
      "Iteration: 9718 lambda_n: 0.9703693889075887 Loss: 1.4283611441154822e-09\n",
      "Iteration: 9719 lambda_n: 0.9012714871451942 Loss: 1.428361276684273e-09\n",
      "Iteration: 9720 lambda_n: 1.027138563350779 Loss: 1.4283613988842246e-09\n",
      "Iteration: 9721 lambda_n: 1.0381834836435848 Loss: 1.4283615337082875e-09\n",
      "Iteration: 9722 lambda_n: 1.0065351943356056 Loss: 1.4283616744956507e-09\n",
      "Iteration: 9723 lambda_n: 1.012273300432872 Loss: 1.4283618090488689e-09\n",
      "Iteration: 9724 lambda_n: 1.041930555175127 Loss: 1.4283619444543316e-09\n",
      "Iteration: 9725 lambda_n: 0.9237531429255402 Loss: 1.4283620824442302e-09\n",
      "Iteration: 9726 lambda_n: 0.905279142197452 Loss: 1.4283622013842852e-09\n",
      "Iteration: 9727 lambda_n: 0.9909469631300503 Loss: 1.4283623217879021e-09\n",
      "Iteration: 9728 lambda_n: 0.9978370832059987 Loss: 1.428362452507221e-09\n",
      "Iteration: 9729 lambda_n: 1.0479997984757619 Loss: 1.428362583243765e-09\n",
      "Iteration: 9730 lambda_n: 0.8950217146111007 Loss: 1.4283627195553856e-09\n",
      "Iteration: 9731 lambda_n: 1.0419081689722214 Loss: 1.428362831705577e-09\n",
      "Iteration: 9732 lambda_n: 1.0138307671750226 Loss: 1.4283629681739074e-09\n",
      "Iteration: 9733 lambda_n: 0.9587671847315478 Loss: 1.428363096252384e-09\n",
      "Iteration: 9734 lambda_n: 0.9899237573515969 Loss: 1.4283632180095126e-09\n",
      "Iteration: 9735 lambda_n: 0.9809971689706322 Loss: 1.428363343528775e-09\n",
      "Iteration: 9736 lambda_n: 0.9853372343678025 Loss: 1.4283634677393615e-09\n",
      "Iteration: 9737 lambda_n: 0.9300339617786186 Loss: 1.428363592104203e-09\n",
      "Iteration: 9738 lambda_n: 1.0196578408687644 Loss: 1.4283637052135841e-09\n",
      "Iteration: 9739 lambda_n: 0.9224461885534361 Loss: 1.4283638334823575e-09\n",
      "Iteration: 9740 lambda_n: 0.9939091382465218 Loss: 1.4283639458137132e-09\n",
      "Iteration: 9741 lambda_n: 1.0220983729903432 Loss: 1.4283640685926016e-09\n",
      "Iteration: 9742 lambda_n: 0.9051190762093779 Loss: 1.4283641943854635e-09\n",
      "Iteration: 9743 lambda_n: 1.0396501736403623 Loss: 1.4283643035404235e-09\n",
      "Iteration: 9744 lambda_n: 1.0463569376474227 Loss: 1.4283644309011542e-09\n",
      "Iteration: 9745 lambda_n: 1.0319049425767395 Loss: 1.4283645577495837e-09\n",
      "Iteration: 9746 lambda_n: 1.0344308870501042 Loss: 1.428364683161899e-09\n",
      "Iteration: 9747 lambda_n: 0.9615058759256986 Loss: 1.4283648072694482e-09\n",
      "Iteration: 9748 lambda_n: 0.9273180311935488 Loss: 1.4283649179634321e-09\n",
      "Iteration: 9749 lambda_n: 0.9536087357962371 Loss: 1.4283650295951148e-09\n",
      "Iteration: 9750 lambda_n: 1.0302572294964936 Loss: 1.428365141924982e-09\n",
      "Iteration: 9751 lambda_n: 1.015931672204171 Loss: 1.428365263368717e-09\n",
      "Iteration: 9752 lambda_n: 1.0254590603815181 Loss: 1.4283653821731555e-09\n",
      "Iteration: 9753 lambda_n: 0.9198696655228401 Loss: 1.4283655022051811e-09\n",
      "Iteration: 9754 lambda_n: 1.020259418220696 Loss: 1.4283656090429417e-09\n",
      "Iteration: 9755 lambda_n: 0.9113602621048882 Loss: 1.4283657219583148e-09\n",
      "Iteration: 9756 lambda_n: 0.9096518161280654 Loss: 1.4283658284185332e-09\n",
      "Iteration: 9757 lambda_n: 0.9739402407828992 Loss: 1.428365932923973e-09\n",
      "Iteration: 9758 lambda_n: 0.9331675843258935 Loss: 1.42836604373666e-09\n",
      "Iteration: 9759 lambda_n: 0.9011896576848 Loss: 1.4283661493205476e-09\n",
      "Iteration: 9760 lambda_n: 0.9761588494757536 Loss: 1.4283662486216265e-09\n",
      "Iteration: 9761 lambda_n: 0.9832856908045421 Loss: 1.428366358534388e-09\n",
      "Iteration: 9762 lambda_n: 1.0403454980202578 Loss: 1.4283664696477463e-09\n",
      "Iteration: 9763 lambda_n: 0.9621875399382616 Loss: 1.4283665865793137e-09\n",
      "Iteration: 9764 lambda_n: 1.0443223502637433 Loss: 1.4283666936384578e-09\n",
      "Iteration: 9765 lambda_n: 1.0212283280306207 Loss: 1.4283668086260716e-09\n",
      "Iteration: 9766 lambda_n: 0.9326335205049772 Loss: 1.428366916878496e-09\n",
      "Iteration: 9767 lambda_n: 0.9979559423766832 Loss: 1.4283670189659151e-09\n",
      "Iteration: 9768 lambda_n: 0.9476448382781594 Loss: 1.428367127675626e-09\n",
      "Iteration: 9769 lambda_n: 1.0088138448020592 Loss: 1.4283672282088289e-09\n",
      "Iteration: 9770 lambda_n: 1.0275200369969297 Loss: 1.4283673362241422e-09\n",
      "Iteration: 9771 lambda_n: 0.9541486910258056 Loss: 1.4283674422197928e-09\n",
      "Iteration: 9772 lambda_n: 0.8935091342129837 Loss: 1.4283675455152072e-09\n",
      "Iteration: 9773 lambda_n: 0.9486844951557541 Loss: 1.4283676412895662e-09\n",
      "Iteration: 9774 lambda_n: 1.0322648497700322 Loss: 1.4283677404743208e-09\n",
      "Iteration: 9775 lambda_n: 1.024271286307626 Loss: 1.4283678443123043e-09\n",
      "Iteration: 9776 lambda_n: 0.9447709988584605 Loss: 1.4283679524590854e-09\n",
      "Iteration: 9777 lambda_n: 1.0164242222631628 Loss: 1.428368051437441e-09\n",
      "Iteration: 9778 lambda_n: 0.9177087032513767 Loss: 1.4283681558553133e-09\n",
      "Iteration: 9779 lambda_n: 1.0296283830966586 Loss: 1.428368250022395e-09\n",
      "Iteration: 9780 lambda_n: 0.9817770214172813 Loss: 1.428368355171385e-09\n",
      "Iteration: 9781 lambda_n: 0.9236871625505171 Loss: 1.4283684517227761e-09\n",
      "Iteration: 9782 lambda_n: 0.913044496006617 Loss: 1.4283685463651577e-09\n",
      "Iteration: 9783 lambda_n: 0.9960445363581177 Loss: 1.4283686403340745e-09\n",
      "Iteration: 9784 lambda_n: 0.9886888676573393 Loss: 1.428368741422658e-09\n",
      "Iteration: 9785 lambda_n: 0.9135465213280822 Loss: 1.4283688412596959e-09\n",
      "Iteration: 9786 lambda_n: 0.9912121746029644 Loss: 1.428368928801172e-09\n",
      "Iteration: 9787 lambda_n: 0.9311945767661343 Loss: 1.4283690296307335e-09\n",
      "Iteration: 9788 lambda_n: 0.9505348120300073 Loss: 1.428369123296857e-09\n",
      "Iteration: 9789 lambda_n: 1.0393596638719773 Loss: 1.4283692180044379e-09\n",
      "Iteration: 9790 lambda_n: 1.0298670983168041 Loss: 1.4283693205460817e-09\n",
      "Iteration: 9791 lambda_n: 1.0328875300526241 Loss: 1.4283694199173889e-09\n",
      "Iteration: 9792 lambda_n: 0.9649797974602985 Loss: 1.428369516941139e-09\n",
      "Iteration: 9793 lambda_n: 1.0172074596648524 Loss: 1.4283696104049498e-09\n",
      "Iteration: 9794 lambda_n: 0.8991113049362197 Loss: 1.4283697094333832e-09\n",
      "Iteration: 9795 lambda_n: 0.9936170731530004 Loss: 1.4283697961031408e-09\n",
      "Iteration: 9796 lambda_n: 1.0304907871547297 Loss: 1.4283698905315492e-09\n",
      "Iteration: 9797 lambda_n: 0.942094143111902 Loss: 1.4283699887717378e-09\n",
      "Iteration: 9798 lambda_n: 1.0315520773091558 Loss: 1.4283700781814577e-09\n",
      "Iteration: 9799 lambda_n: 0.9033703786143197 Loss: 1.4283701755426923e-09\n",
      "Iteration: 9800 lambda_n: 0.9128799811631346 Loss: 1.4283702571288944e-09\n",
      "Iteration: 9801 lambda_n: 0.8999312008485458 Loss: 1.428370343963978e-09\n",
      "Iteration: 9802 lambda_n: 0.9009979327283202 Loss: 1.4283704280008982e-09\n",
      "Iteration: 9803 lambda_n: 0.9449802741612413 Loss: 1.4283705129810968e-09\n",
      "Iteration: 9804 lambda_n: 0.9797296774714919 Loss: 1.4283705987772557e-09\n",
      "Iteration: 9805 lambda_n: 0.9308666980250924 Loss: 1.428370690335508e-09\n",
      "Iteration: 9806 lambda_n: 0.9559580878880126 Loss: 1.4283707767962088e-09\n",
      "Iteration: 9807 lambda_n: 0.953685733318392 Loss: 1.4283708637846648e-09\n",
      "Iteration: 9808 lambda_n: 1.0327524361201679 Loss: 1.4283709475107594e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9809 lambda_n: 1.03112538300355 Loss: 1.4283710428279338e-09\n",
      "Iteration: 9810 lambda_n: 1.04380744650249 Loss: 1.428371135919599e-09\n",
      "Iteration: 9811 lambda_n: 1.0489532684986884 Loss: 1.4283712268385147e-09\n",
      "Iteration: 9812 lambda_n: 1.0172500031079748 Loss: 1.4283713218417935e-09\n",
      "Iteration: 9813 lambda_n: 1.028741570270262 Loss: 1.4283714118165227e-09\n",
      "Iteration: 9814 lambda_n: 0.9080420483380922 Loss: 1.4283715036580307e-09\n",
      "Iteration: 9815 lambda_n: 0.9011318693354012 Loss: 1.428371579792353e-09\n",
      "Iteration: 9816 lambda_n: 0.9043255665508791 Loss: 1.4283716589518464e-09\n",
      "Iteration: 9817 lambda_n: 1.007033906263402 Loss: 1.4283717397454726e-09\n",
      "Iteration: 9818 lambda_n: 1.0472198859986892 Loss: 1.4283718256787229e-09\n",
      "Iteration: 9819 lambda_n: 0.9746819093093867 Loss: 1.428371917947332e-09\n",
      "Iteration: 9820 lambda_n: 1.0433081306016976 Loss: 1.4283720035329888e-09\n",
      "Iteration: 9821 lambda_n: 1.0250024813501357 Loss: 1.4283720896165949e-09\n",
      "Iteration: 9822 lambda_n: 0.9096613869590021 Loss: 1.428372179653993e-09\n",
      "Iteration: 9823 lambda_n: 1.042554392064971 Loss: 1.4283722575225577e-09\n",
      "Iteration: 9824 lambda_n: 1.002875803944123 Loss: 1.4283723422759488e-09\n",
      "Iteration: 9825 lambda_n: 0.9633934213539647 Loss: 1.4283724280903454e-09\n",
      "Iteration: 9826 lambda_n: 0.9220334715182642 Loss: 1.4283725095460703e-09\n",
      "Iteration: 9827 lambda_n: 0.9171339711341084 Loss: 1.4283725841579882e-09\n",
      "Iteration: 9828 lambda_n: 0.9802643102735917 Loss: 1.4283726628973674e-09\n",
      "Iteration: 9829 lambda_n: 0.960625697126846 Loss: 1.4283727465578626e-09\n",
      "Iteration: 9830 lambda_n: 1.006700724931915 Loss: 1.4283728224072088e-09\n",
      "Iteration: 9831 lambda_n: 0.9299337376456762 Loss: 1.4283729074204493e-09\n",
      "Iteration: 9832 lambda_n: 0.9198021675529504 Loss: 1.42837298313852e-09\n",
      "Iteration: 9833 lambda_n: 0.9645207387500928 Loss: 1.4283730584614305e-09\n",
      "Iteration: 9834 lambda_n: 0.9281633049777785 Loss: 1.428373137087811e-09\n",
      "Iteration: 9835 lambda_n: 1.005541125865516 Loss: 1.4283732125206722e-09\n",
      "Iteration: 9836 lambda_n: 1.0118672421524557 Loss: 1.4283732947243197e-09\n",
      "Iteration: 9837 lambda_n: 1.0467211905554037 Loss: 1.4283733719853497e-09\n",
      "Iteration: 9838 lambda_n: 1.0343737654012988 Loss: 1.4283734579653485e-09\n",
      "Iteration: 9839 lambda_n: 1.0265187183378752 Loss: 1.4283735402511529e-09\n",
      "Iteration: 9840 lambda_n: 1.0003478724530686 Loss: 1.4283736190769422e-09\n",
      "Iteration: 9841 lambda_n: 0.9504625420979973 Loss: 1.4283736988058625e-09\n",
      "Iteration: 9842 lambda_n: 1.0320964563934456 Loss: 1.4283737721485244e-09\n",
      "Iteration: 9843 lambda_n: 0.9830917152529761 Loss: 1.428373852109355e-09\n",
      "Iteration: 9844 lambda_n: 0.9133773804672872 Loss: 1.4283739282430366e-09\n",
      "Iteration: 9845 lambda_n: 0.9330235397852765 Loss: 1.4283739956369595e-09\n",
      "Iteration: 9846 lambda_n: 0.9348266810580544 Loss: 1.4283740693888879e-09\n",
      "Iteration: 9847 lambda_n: 0.9731144537389425 Loss: 1.4283741375779677e-09\n",
      "Iteration: 9848 lambda_n: 0.973932830636827 Loss: 1.4283742125725907e-09\n",
      "Iteration: 9849 lambda_n: 0.9345233317363117 Loss: 1.4283742877400065e-09\n",
      "Iteration: 9850 lambda_n: 0.9814630633098136 Loss: 1.4283743546112881e-09\n",
      "Iteration: 9851 lambda_n: 0.9218604314497153 Loss: 1.4283744286973024e-09\n",
      "Iteration: 9852 lambda_n: 0.9205850264469635 Loss: 1.4283744977086056e-09\n",
      "Iteration: 9853 lambda_n: 1.0177397463427909 Loss: 1.4283745664648408e-09\n",
      "Iteration: 9854 lambda_n: 0.9626497698715603 Loss: 1.4283746413443473e-09\n",
      "Iteration: 9855 lambda_n: 0.9427016920543386 Loss: 1.428374712378674e-09\n",
      "Iteration: 9856 lambda_n: 0.918760832100621 Loss: 1.428374778439783e-09\n",
      "Iteration: 9857 lambda_n: 0.939987931289453 Loss: 1.428374846862952e-09\n",
      "Iteration: 9858 lambda_n: 0.9506553235823267 Loss: 1.4283749148966018e-09\n",
      "Iteration: 9859 lambda_n: 0.9719615958202862 Loss: 1.4283749791451917e-09\n",
      "Iteration: 9860 lambda_n: 0.9653173719000087 Loss: 1.4283750501326285e-09\n",
      "Iteration: 9861 lambda_n: 0.9188260062206268 Loss: 1.4283751195286625e-09\n",
      "Iteration: 9862 lambda_n: 0.9198071113152276 Loss: 1.4283751811630224e-09\n",
      "Iteration: 9863 lambda_n: 1.0175164559006133 Loss: 1.4283752471028106e-09\n",
      "Iteration: 9864 lambda_n: 1.0454571019642136 Loss: 1.4283753186549003e-09\n",
      "Iteration: 9865 lambda_n: 0.9156896049020836 Loss: 1.4283753920445424e-09\n",
      "Iteration: 9866 lambda_n: 0.995976114196542 Loss: 1.4283754533471036e-09\n",
      "Iteration: 9867 lambda_n: 0.9796500865115588 Loss: 1.4283755257837751e-09\n",
      "Iteration: 9868 lambda_n: 0.9016979442569746 Loss: 1.4283755951097872e-09\n",
      "Iteration: 9869 lambda_n: 1.033972548174381 Loss: 1.4283756582734014e-09\n",
      "Iteration: 9870 lambda_n: 0.9296424533643702 Loss: 1.4283757291224552e-09\n",
      "Iteration: 9871 lambda_n: 0.9878737192568613 Loss: 1.4283757892832944e-09\n",
      "Iteration: 9872 lambda_n: 0.9975190973099648 Loss: 1.4283758572210565e-09\n",
      "Iteration: 9873 lambda_n: 0.9343337447041805 Loss: 1.4283759254794634e-09\n",
      "Iteration: 9874 lambda_n: 1.001377078302911 Loss: 1.4283759848033351e-09\n",
      "Iteration: 9875 lambda_n: 1.0217021295945308 Loss: 1.4283760529544085e-09\n",
      "Iteration: 9876 lambda_n: 0.9869434144998674 Loss: 1.4283761230111743e-09\n",
      "Iteration: 9877 lambda_n: 0.9464397002895799 Loss: 1.428376189493533e-09\n",
      "Iteration: 9878 lambda_n: 0.928211921733075 Loss: 1.4283762521961608e-09\n",
      "Iteration: 9879 lambda_n: 1.0308972540846688 Loss: 1.4283763125368652e-09\n",
      "Iteration: 9880 lambda_n: 1.0459505177824284 Loss: 1.4283763761111592e-09\n",
      "Iteration: 9881 lambda_n: 0.9416081735669136 Loss: 1.42837644579015e-09\n",
      "Iteration: 9882 lambda_n: 0.9167170071891655 Loss: 1.4283765069029083e-09\n",
      "Iteration: 9883 lambda_n: 0.9996202915660682 Loss: 1.428376565426056e-09\n",
      "Iteration: 9884 lambda_n: 1.0304541534077234 Loss: 1.4283766251963424e-09\n",
      "Iteration: 9885 lambda_n: 1.0280667387595712 Loss: 1.4283766918132255e-09\n",
      "Iteration: 9886 lambda_n: 0.8982223800148533 Loss: 1.428376758368749e-09\n",
      "Iteration: 9887 lambda_n: 1.0363924841505212 Loss: 1.4283768156916247e-09\n",
      "Iteration: 9888 lambda_n: 0.9578033833137964 Loss: 1.4283768814318272e-09\n",
      "Iteration: 9889 lambda_n: 1.0216720351604118 Loss: 1.428376942779075e-09\n",
      "Iteration: 9890 lambda_n: 1.043533772400293 Loss: 1.428377007449501e-09\n",
      "Iteration: 9891 lambda_n: 0.9724222733418338 Loss: 1.4283770681621339e-09\n",
      "Iteration: 9892 lambda_n: 0.9012901270840257 Loss: 1.428377130832316e-09\n",
      "Iteration: 9893 lambda_n: 0.9856110537475371 Loss: 1.428377186148338e-09\n",
      "Iteration: 9894 lambda_n: 0.9700745106876247 Loss: 1.428377246757058e-09\n",
      "Iteration: 9895 lambda_n: 0.9847345046322761 Loss: 1.4283773058551082e-09\n",
      "Iteration: 9896 lambda_n: 0.8946311674765351 Loss: 1.428377363455461e-09\n",
      "Iteration: 9897 lambda_n: 0.8967504275379727 Loss: 1.4283774201971333e-09\n",
      "Iteration: 9898 lambda_n: 0.9502843186157174 Loss: 1.428377475084177e-09\n",
      "Iteration: 9899 lambda_n: 1.0095526168379463 Loss: 1.4283775333441828e-09\n",
      "Iteration: 9900 lambda_n: 1.0363772976911316 Loss: 1.4283775957885359e-09\n",
      "Iteration: 9901 lambda_n: 0.9377214749051089 Loss: 1.4283776596804406e-09\n",
      "Iteration: 9902 lambda_n: 0.9300709720741468 Loss: 1.4283777164374846e-09\n",
      "Iteration: 9903 lambda_n: 0.900924053839005 Loss: 1.428377768154474e-09\n",
      "Iteration: 9904 lambda_n: 1.0349606405572842 Loss: 1.4283778222311092e-09\n",
      "Iteration: 9905 lambda_n: 0.907011838180894 Loss: 1.4283778837221935e-09\n",
      "Iteration: 9906 lambda_n: 0.9568945405983005 Loss: 1.4283779354972408e-09\n",
      "Iteration: 9907 lambda_n: 1.011081201329441 Loss: 1.4283779892134628e-09\n",
      "Iteration: 9908 lambda_n: 1.0168405084142784 Loss: 1.4283780438779873e-09\n",
      "Iteration: 9909 lambda_n: 1.0485015637412025 Loss: 1.4283781018335292e-09\n",
      "Iteration: 9910 lambda_n: 1.0178435619112234 Loss: 1.428378161329397e-09\n",
      "Iteration: 9911 lambda_n: 0.9530011471281636 Loss: 1.4283782185709151e-09\n",
      "Iteration: 9912 lambda_n: 0.9819129605930121 Loss: 1.428378273483097e-09\n",
      "Iteration: 9913 lambda_n: 0.9262187730240662 Loss: 1.4283783287258565e-09\n",
      "Iteration: 9914 lambda_n: 1.029726592755403 Loss: 1.4283783820908835e-09\n",
      "Iteration: 9915 lambda_n: 1.0358116392625714 Loss: 1.4283784390083983e-09\n",
      "Iteration: 9916 lambda_n: 0.9554869093458372 Loss: 1.4283784955212617e-09\n",
      "Iteration: 9917 lambda_n: 0.9255050774436604 Loss: 1.4283785472498736e-09\n",
      "Iteration: 9918 lambda_n: 0.9178514332546343 Loss: 1.4283785990339775e-09\n",
      "Iteration: 9919 lambda_n: 0.9654479484383787 Loss: 1.42837864943447e-09\n",
      "Iteration: 9920 lambda_n: 0.9521761756892875 Loss: 1.4283787015945097e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9921 lambda_n: 0.9685406296003298 Loss: 1.4283787529414131e-09\n",
      "Iteration: 9922 lambda_n: 0.9568788109211196 Loss: 1.4283788050136633e-09\n",
      "Iteration: 9923 lambda_n: 0.9834312960792296 Loss: 1.4283788562949241e-09\n",
      "Iteration: 9924 lambda_n: 0.8934011417557858 Loss: 1.4283789094617634e-09\n",
      "Iteration: 9925 lambda_n: 1.0264485394779943 Loss: 1.4283789547915507e-09\n",
      "Iteration: 9926 lambda_n: 1.0353583581322043 Loss: 1.4283790115921567e-09\n",
      "Iteration: 9927 lambda_n: 0.9836023168271675 Loss: 1.428379066412754e-09\n",
      "Iteration: 9928 lambda_n: 0.9453496015482769 Loss: 1.4283791191890332e-09\n",
      "Iteration: 9929 lambda_n: 0.9244801689607579 Loss: 1.4283791647816794e-09\n",
      "Iteration: 9930 lambda_n: 0.9266474419123162 Loss: 1.4283792143345778e-09\n",
      "Iteration: 9931 lambda_n: 0.9487053414431731 Loss: 1.4283792625136242e-09\n",
      "Iteration: 9932 lambda_n: 0.9681959355373856 Loss: 1.4283793085489842e-09\n",
      "Iteration: 9933 lambda_n: 1.030988005689888 Loss: 1.4283793591507873e-09\n",
      "Iteration: 9934 lambda_n: 0.9475766061202018 Loss: 1.4283794127050746e-09\n",
      "Iteration: 9935 lambda_n: 0.9930202054350893 Loss: 1.4283794582044672e-09\n",
      "Iteration: 9936 lambda_n: 1.0170323963868209 Loss: 1.428379510856597e-09\n",
      "Iteration: 9937 lambda_n: 0.893566177703709 Loss: 1.4283795634213781e-09\n",
      "Iteration: 9938 lambda_n: 0.8963513255369026 Loss: 1.4283796059576756e-09\n",
      "Iteration: 9939 lambda_n: 1.0320331725064382 Loss: 1.4283796519359656e-09\n",
      "Iteration: 9940 lambda_n: 1.0358834412813618 Loss: 1.4283797047619248e-09\n",
      "Iteration: 9941 lambda_n: 1.029197249239995 Loss: 1.4283797572462659e-09\n",
      "Iteration: 9942 lambda_n: 0.9712566108358108 Loss: 1.4283798052967608e-09\n",
      "Iteration: 9943 lambda_n: 0.9797637868301834 Loss: 1.4283798542340585e-09\n",
      "Iteration: 9944 lambda_n: 0.96446359620832 Loss: 1.428379903998716e-09\n",
      "Iteration: 9945 lambda_n: 1.0343563470565416 Loss: 1.428379947503648e-09\n",
      "Iteration: 9946 lambda_n: 0.9033629394422642 Loss: 1.4283799982736195e-09\n",
      "Iteration: 9947 lambda_n: 0.9397371570773155 Loss: 1.4283800422784266e-09\n",
      "Iteration: 9948 lambda_n: 1.0441240367617541 Loss: 1.4283800875455657e-09\n",
      "Iteration: 9949 lambda_n: 0.9807666493268767 Loss: 1.4283801373422693e-09\n",
      "Iteration: 9950 lambda_n: 0.9303951660361159 Loss: 1.4283801802294253e-09\n",
      "Iteration: 9951 lambda_n: 0.9865865874779716 Loss: 1.428380225615995e-09\n",
      "Iteration: 9952 lambda_n: 0.9106006168549927 Loss: 1.4283802719463952e-09\n",
      "Iteration: 9953 lambda_n: 0.893612462367887 Loss: 1.428380316656328e-09\n",
      "Iteration: 9954 lambda_n: 0.9733659528688532 Loss: 1.4283803592182586e-09\n",
      "Iteration: 9955 lambda_n: 0.9911920475088617 Loss: 1.4283804017161124e-09\n",
      "Iteration: 9956 lambda_n: 1.040112824522774 Loss: 1.4283804493069435e-09\n",
      "Iteration: 9957 lambda_n: 1.0013894684859075 Loss: 1.4283804982171987e-09\n",
      "Iteration: 9958 lambda_n: 0.9879789575751303 Loss: 1.428380544617862e-09\n",
      "Iteration: 9959 lambda_n: 1.0317963641583812 Loss: 1.4283805899186422e-09\n",
      "Iteration: 9960 lambda_n: 0.9127308029784267 Loss: 1.4283806373177795e-09\n",
      "Iteration: 9961 lambda_n: 1.0322448294380662 Loss: 1.4283806739564e-09\n",
      "Iteration: 9962 lambda_n: 0.9619383308103974 Loss: 1.4283807216842777e-09\n",
      "Iteration: 9963 lambda_n: 1.0129721565081122 Loss: 1.4283807648131562e-09\n",
      "Iteration: 9964 lambda_n: 1.0128942121666904 Loss: 1.4283808099656043e-09\n",
      "Iteration: 9965 lambda_n: 0.9977477293272723 Loss: 1.4283808507566521e-09\n",
      "Iteration: 9966 lambda_n: 0.934960734609064 Loss: 1.4283808957364847e-09\n",
      "Iteration: 9967 lambda_n: 0.8927569132451803 Loss: 1.4283809380228492e-09\n",
      "Iteration: 9968 lambda_n: 0.9197350727378674 Loss: 1.4283809778695939e-09\n",
      "Iteration: 9969 lambda_n: 0.9401250951568988 Loss: 1.4283810149581805e-09\n",
      "Iteration: 9970 lambda_n: 0.9586718515868519 Loss: 1.428381057608253e-09\n",
      "Iteration: 9971 lambda_n: 0.9373045925693831 Loss: 1.428381101012867e-09\n",
      "Iteration: 9972 lambda_n: 1.001466514258723 Loss: 1.428381143256991e-09\n",
      "Iteration: 9973 lambda_n: 0.995209245891746 Loss: 1.4283811846047381e-09\n",
      "Iteration: 9974 lambda_n: 0.9174214752791454 Loss: 1.4283812308804941e-09\n",
      "Iteration: 9975 lambda_n: 1.0342297680492878 Loss: 1.428381271712613e-09\n",
      "Iteration: 9976 lambda_n: 0.9197934885124245 Loss: 1.4283813176680681e-09\n",
      "Iteration: 9977 lambda_n: 0.9344590976194539 Loss: 1.4283813530434963e-09\n",
      "Iteration: 9978 lambda_n: 1.0276041572845358 Loss: 1.4283813945849542e-09\n",
      "Iteration: 9979 lambda_n: 0.9114354989032273 Loss: 1.428381437363944e-09\n",
      "Iteration: 9980 lambda_n: 0.9836869208232731 Loss: 1.428381472043363e-09\n",
      "Iteration: 9981 lambda_n: 0.9154707564163254 Loss: 1.4283815141037591e-09\n",
      "Iteration: 9982 lambda_n: 0.9083816883018252 Loss: 1.4283815521703142e-09\n",
      "Iteration: 9983 lambda_n: 1.0464973918728675 Loss: 1.4283815859457906e-09\n",
      "Iteration: 9984 lambda_n: 0.9789106803796831 Loss: 1.428381628911043e-09\n",
      "Iteration: 9985 lambda_n: 0.9406861110203234 Loss: 1.4283816689604701e-09\n",
      "Iteration: 9986 lambda_n: 0.9563253626053038 Loss: 1.4283817041263397e-09\n",
      "Iteration: 9987 lambda_n: 1.0429470460768127 Loss: 1.4283817440163516e-09\n",
      "Iteration: 9988 lambda_n: 0.9477804704156829 Loss: 1.428381787355647e-09\n",
      "Iteration: 9989 lambda_n: 1.034920851473323 Loss: 1.428381822178158e-09\n",
      "Iteration: 9990 lambda_n: 0.9013709566319107 Loss: 1.4283818663340218e-09\n",
      "Iteration: 9991 lambda_n: 0.9795062938895164 Loss: 1.428381902628576e-09\n",
      "Iteration: 9992 lambda_n: 0.9620528911418944 Loss: 1.4283819413378147e-09\n",
      "Iteration: 9993 lambda_n: 0.8962456500108869 Loss: 1.4283819766998938e-09\n",
      "Iteration: 9994 lambda_n: 0.9709041677934307 Loss: 1.4283820131255779e-09\n",
      "Iteration: 9995 lambda_n: 1.0241461735513282 Loss: 1.428382050231188e-09\n",
      "Iteration: 9996 lambda_n: 0.9778813304744512 Loss: 1.4283820864733039e-09\n",
      "Iteration: 9997 lambda_n: 0.9238336086265647 Loss: 1.4283821254174562e-09\n",
      "Iteration: 9998 lambda_n: 0.9944089345675796 Loss: 1.4283821614864476e-09\n",
      "Iteration: 9999 lambda_n: 0.9747108535524182 Loss: 1.4283821957235338e-09\n",
      "Iteration: 10000 lambda_n: 0.8926558240719498 Loss: 1.4283822343893444e-09\n",
      "beta:  0.002180216632244036\n",
      "gamma: 0.003967180688989444\n"
     ]
    }
   ],
   "source": [
    "# BA    \n",
    "ba = time.time()\n",
    "x_BA_list, z_BA_list, dual_BA_list, iterations_BA   = BA.Briceno_Arias(N, M, frobenius_norm, Grad_Phi_NA, Sigma, D, (x1,x2,x3),gamma=1e-3, lambdan=1e-3)\n",
    "fin = time.time()\n",
    "\n",
    "Times[\"BA\"] = fin - ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56a5dbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:\n",
      " 10000\n",
      "Primal: (x1,x2,x3)\n",
      " ((array([[1188.3, 1188.3, 1188.3, 1188.3, 1188.3],\n",
      "       [44.3, 44.3, 44.3, 44.3, 44.3],\n",
      "       [25.7, 25.7, 25.7, 25.7, 25.7]]), array([[186.6, 704.0, 833.3, 1188.3, 714.3],\n",
      "       [8.9, 30.9, 44.3, 42.8, 43.5],\n",
      "       [4.6, 15.1, 25.6, 19.0, 25.7]]), array([[0.0, 0.0, 96.7, 0.0, 716.5]])), 'infactible')\n",
      "Primal: (z1,z2,z3)\n",
      " (array([[1188.2, 1188.2, 1188.2, 1188.3, 1188.2],\n",
      "       [44.1, 44.1, 45.1, 44.1, 44.1],\n",
      "       [25.4, 25.4, 25.4, 25.4, 27.3]]), array([[186.6, 704.0, 833.3, 1188.3, 714.3],\n",
      "       [8.9, 30.9, 44.3, 42.8, 43.5],\n",
      "       [4.6, 15.1, 25.6, 19.0, 25.7]]), array([[0.0, 0.0, 96.7, 0.0, 716.5]]))\n",
      "Dual: (Capacity, Equilibrium)\n",
      " (array([[0.0, 0.0, 0.0, 25.0, 0.0],\n",
      "       [0.0, 0.0, 250.0, 0.0, 0.0],\n",
      "       [0.0, 0.0, 0.0, 0.0, 475.0]]), array([[1865.7, 6336.1, 10000.0, 8342.8, 10000.0]]))\n"
     ]
    }
   ],
   "source": [
    "iter_ = -1\n",
    "print(\"Iterations:\\n\",iterations_BA)\n",
    "print(\"Primal: (x1,x2,x3)\\n\", x_BA_list[iter_])\n",
    "print(\"Primal: (z1,z2,z3)\\n\", z_BA_list[iter_])\n",
    "print(\"Dual: (Capacity, Equilibrium)\\n\",dual_BA_list[iter_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9a848c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r: 1\n",
      "Iteration: 1 Loss: 3.4040428718878912\n",
      "Iteration: 2 Loss: 2.8228770933971834\n",
      "Iteration: 3 Loss: 2.3657117523399593\n",
      "Iteration: 4 Loss: 2.0006165730498244\n",
      "Iteration: 5 Loss: 1.7077220821521906\n",
      "Iteration: 6 Loss: 1.472425401869873\n",
      "Iteration: 7 Loss: 1.2832605605490934\n",
      "Iteration: 8 Loss: 1.1310591285182194\n",
      "Iteration: 9 Loss: 1.008267336779426\n",
      "Iteration: 10 Loss: 0.9089119555618903\n",
      "Iteration: 11 Loss: 0.8283183552029256\n",
      "Iteration: 12 Loss: 0.7627607231845481\n",
      "Iteration: 13 Loss: 0.709263386279433\n",
      "Iteration: 14 Loss: 0.6654486439879979\n",
      "Iteration: 15 Loss: 0.6294156325571457\n",
      "Iteration: 16 Loss: 0.5996436918382062\n",
      "Iteration: 17 Loss: 0.5749153090414474\n",
      "Iteration: 18 Loss: 0.554254708713121\n",
      "Iteration: 19 Loss: 0.536878940063906\n",
      "Iteration: 20 Loss: 0.5221589389887958\n",
      "Iteration: 21 Loss: 0.5095885445607371\n",
      "Iteration: 22 Loss: 0.49875985303299836\n",
      "Iteration: 23 Loss: 0.4893436160572594\n",
      "Iteration: 24 Loss: 0.48107364960837434\n",
      "Iteration: 25 Loss: 0.4737344285716483\n",
      "Iteration: 26 Loss: 0.4671512091690071\n",
      "Iteration: 27 Loss: 0.4611821554585864\n",
      "Iteration: 28 Loss: 0.45571205352713484\n",
      "Iteration: 29 Loss: 0.4506472829214161\n",
      "Iteration: 30 Loss: 0.44591178352878147\n",
      "Iteration: 31 Loss: 0.4414438108964572\n",
      "Iteration: 32 Loss: 0.43719331659652994\n",
      "Iteration: 33 Loss: 0.43311982489519507\n",
      "Iteration: 34 Loss: 0.4291907044431619\n",
      "Iteration: 35 Loss: 0.4253797554004872\n",
      "Iteration: 36 Loss: 0.42166604949950726\n",
      "Iteration: 37 Loss: 0.41803297396728534\n",
      "Iteration: 38 Loss: 0.414467440727298\n",
      "Iteration: 39 Loss: 0.4109592304859368\n",
      "Iteration: 40 Loss: 0.40750044767135457\n",
      "Iteration: 41 Loss: 0.4040850671222723\n",
      "Iteration: 42 Loss: 0.400708557236503\n",
      "Iteration: 43 Loss: 0.3973646412830686\n",
      "Iteration: 44 Loss: 0.3940509224888434\n",
      "Iteration: 45 Loss: 0.39076897297875623\n",
      "Iteration: 46 Loss: 0.3875181572674849\n",
      "Iteration: 47 Loss: 0.384297595541542\n",
      "Iteration: 48 Loss: 0.3811065997135975\n",
      "Iteration: 49 Loss: 0.37794469331895914\n",
      "Iteration: 50 Loss: 0.37481155578533004\n",
      "Iteration: 51 Loss: 0.37170696434660017\n",
      "Iteration: 52 Loss: 0.3686307477086936\n",
      "Iteration: 53 Loss: 0.36558275226057946\n",
      "Iteration: 54 Loss: 0.36256281891976716\n",
      "Iteration: 55 Loss: 0.35957076843442565\n",
      "Iteration: 56 Loss: 0.35660639320842585\n",
      "Iteration: 57 Loss: 0.3536694540271355\n",
      "Iteration: 58 Loss: 0.3507596803474599\n",
      "Iteration: 59 Loss: 0.34787677306261633\n",
      "Iteration: 60 Loss: 0.3450204088637915\n",
      "Iteration: 61 Loss: 0.3421902455022804\n",
      "Iteration: 62 Loss: 0.33938592741118656\n",
      "Iteration: 63 Loss: 0.3366070912784217\n",
      "Iteration: 64 Loss: 0.3338533712750282\n",
      "Iteration: 65 Loss: 0.33112440373683\n",
      "Iteration: 66 Loss: 0.3284198311748179\n",
      "Iteration: 67 Loss: 0.32573930555224306\n",
      "Iteration: 68 Loss: 0.32308249081565477\n",
      "Iteration: 69 Loss: 0.3204490647046909\n",
      "Iteration: 70 Loss: 0.3178387198928026\n",
      "Iteration: 71 Loss: 0.31525116452971413\n",
      "Iteration: 72 Loss: 0.31268612226772474\n",
      "Iteration: 73 Loss: 0.3101433318591987\n",
      "Iteration: 74 Loss: 0.30762254641302844\n",
      "Iteration: 75 Loss: 0.3051235323945346\n",
      "Iteration: 76 Loss: 0.3026453353761836\n",
      "Iteration: 77 Loss: 0.3001849271026265\n",
      "Iteration: 78 Loss: 0.2977450057335209\n",
      "Iteration: 79 Loss: 0.2953264202373536\n",
      "Iteration: 80 Loss: 0.2929293092920702\n",
      "Iteration: 81 Loss: 0.2905535131051727\n",
      "Iteration: 82 Loss: 0.2881987646598992\n",
      "Iteration: 83 Loss: 0.28586477348432526\n",
      "Iteration: 84 Loss: 0.2835512585724323\n",
      "Iteration: 85 Loss: 0.28125795862348013\n",
      "Iteration: 86 Loss: 0.27898463288772857\n",
      "Iteration: 87 Loss: 0.27673105860848474\n",
      "Iteration: 88 Loss: 0.2744970276298804\n",
      "Iteration: 89 Loss: 0.2722823431881058\n",
      "Iteration: 90 Loss: 0.2700868172224298\n",
      "Iteration: 91 Loss: 0.2679102682572087\n",
      "Iteration: 92 Loss: 0.265749396751001\n",
      "Iteration: 93 Loss: 0.263605684649443\n",
      "Iteration: 94 Loss: 0.2614810606413931\n",
      "Iteration: 95 Loss: 0.25937557670618105\n",
      "Iteration: 96 Loss: 0.2572889163691235\n",
      "Iteration: 97 Loss: 0.2552207423599898\n",
      "Iteration: 98 Loss: 0.25317075533510774\n",
      "Iteration: 99 Loss: 0.25113869400257066\n",
      "Iteration: 100 Loss: 0.2491243263219762\n",
      "Iteration: 101 Loss: 0.24712744158549405\n",
      "Iteration: 102 Loss: 0.24514784462682282\n",
      "Iteration: 103 Loss: 0.24318535178837009\n",
      "Iteration: 104 Loss: 0.24123978813318253\n",
      "Iteration: 105 Loss: 0.23931098550380492\n",
      "Iteration: 106 Loss: 0.23739878115341645\n",
      "Iteration: 107 Loss: 0.23550301676546181\n",
      "Iteration: 108 Loss: 0.23362353773948683\n",
      "Iteration: 109 Loss: 0.23176019266151068\n",
      "Iteration: 110 Loss: 0.22991283290400108\n",
      "Iteration: 111 Loss: 0.22808131231817513\n",
      "Iteration: 112 Loss: 0.22626548699310564\n",
      "Iteration: 113 Loss: 0.22446521506400213\n",
      "Iteration: 114 Loss: 0.22268035655736798\n",
      "Iteration: 115 Loss: 0.2209107732643931\n",
      "Iteration: 116 Loss: 0.2191563286364618\n",
      "Iteration: 117 Loss: 0.2174168876984182\n",
      "Iteration: 118 Loss: 0.21569210704169955\n",
      "Iteration: 119 Loss: 0.21397928373009403\n",
      "Iteration: 120 Loss: 0.21228013355896622\n",
      "Iteration: 121 Loss: 0.21059532215478757\n",
      "Iteration: 122 Loss: 0.20892517865034077\n",
      "Iteration: 123 Loss: 0.2072697938219443\n",
      "Iteration: 124 Loss: 0.20562910637019927\n",
      "Iteration: 125 Loss: 0.2040029690645441\n",
      "Iteration: 126 Loss: 0.20239119342239187\n",
      "Iteration: 127 Loss: 0.20079357700922426\n",
      "Iteration: 128 Loss: 0.19920991862229687\n",
      "Iteration: 129 Loss: 0.19764002586381427\n",
      "Iteration: 130 Loss: 0.1960837183416791\n",
      "Iteration: 131 Loss: 0.19454082859819363\n",
      "Iteration: 132 Loss: 0.1930112020673153\n",
      "Iteration: 133 Loss: 0.19149469692235582\n",
      "Iteration: 134 Loss: 0.18999118460400508\n",
      "Iteration: 135 Loss: 0.18850055227433188\n",
      "Iteration: 136 Loss: 0.18702271017354136\n",
      "Iteration: 137 Loss: 0.1855576130296091\n",
      "Iteration: 138 Loss: 0.18410533141223717\n",
      "Iteration: 139 Loss: 0.18266637456527993\n",
      "Iteration: 140 Loss: 0.18124458217907186\n",
      "Iteration: 141 Loss: 0.1799574977658156\n",
      "Iteration: 142 Loss: 0.1780190144956559\n",
      "Iteration: 143 Loss: 0.1739795981246595\n",
      "Iteration: 144 Loss: 0.16856616320140902\n",
      "Iteration: 145 Loss: 0.16238569260042876\n",
      "Iteration: 146 Loss: 0.15591858839269757\n",
      "Iteration: 147 Loss: 0.1495360563907627\n",
      "Iteration: 148 Loss: 0.1435043770848041\n",
      "Iteration: 149 Loss: 0.13800063236566099\n",
      "Iteration: 150 Loss: 0.1331277183455429\n",
      "Iteration: 151 Loss: 0.1289288827338541\n",
      "Iteration: 152 Loss: 0.12835431613989892\n",
      "Iteration: 153 Loss: 0.12895187710474268\n",
      "Iteration: 154 Loss: 0.1290077369933957\n",
      "Iteration: 155 Loss: 0.1285987913670645\n",
      "Iteration: 156 Loss: 0.12780830385872463\n",
      "Iteration: 157 Loss: 0.12671850802210333\n",
      "Iteration: 158 Loss: 0.1254034668180947\n",
      "Iteration: 159 Loss: 0.12392847444428927\n",
      "Iteration: 160 Loss: 0.1223521757820631\n",
      "Iteration: 161 Loss: 0.1207256822060015\n",
      "Iteration: 162 Loss: 0.11909128742658366\n",
      "Iteration: 163 Loss: 0.11748216096546685\n",
      "Iteration: 164 Loss: 0.11592277885018172\n",
      "Iteration: 165 Loss: 0.1144297700373778\n",
      "Iteration: 166 Loss: 0.1130129768063387\n",
      "Iteration: 167 Loss: 0.11167660169484686\n",
      "Iteration: 168 Loss: 0.11042035657233006\n",
      "Iteration: 169 Loss: 0.1093646360601495\n",
      "Iteration: 170 Loss: 0.10858209453653409\n",
      "Iteration: 171 Loss: 0.10775446701385853\n",
      "Iteration: 172 Loss: 0.10688980237235138\n",
      "Iteration: 173 Loss: 0.10599630071919185\n",
      "Iteration: 174 Loss: 0.10508188904244198\n",
      "Iteration: 175 Loss: 0.10415391070843599\n",
      "Iteration: 176 Loss: 0.10321891603786844\n",
      "Iteration: 177 Loss: 0.10228254025680707\n",
      "Iteration: 178 Loss: 0.10134945432866875\n",
      "Iteration: 179 Loss: 0.10042337394568965\n",
      "Iteration: 180 Loss: 0.0995071124449244\n",
      "Iteration: 181 Loss: 0.09860266453040441\n",
      "Iteration: 182 Loss: 0.09771130924282374\n",
      "Iteration: 183 Loss: 0.09683372242471502\n",
      "Iteration: 184 Loss: 0.09599351792402849\n",
      "Iteration: 185 Loss: 0.09519202606829613\n",
      "Iteration: 186 Loss: 0.09439199494436902\n",
      "Iteration: 187 Loss: 0.0935939477685308\n",
      "Iteration: 188 Loss: 0.09279852140981458\n",
      "Iteration: 189 Loss: 0.09200640391786304\n",
      "Iteration: 190 Loss: 0.09121828513852749\n",
      "Iteration: 191 Loss: 0.09043481955422857\n",
      "Iteration: 192 Loss: 0.08965660011482629\n",
      "Iteration: 193 Loss: 0.08888414160153314\n",
      "Iteration: 194 Loss: 0.08811787196650717\n",
      "Iteration: 195 Loss: 0.08735813008959663\n",
      "Iteration: 196 Loss: 0.08660516846773113\n",
      "Iteration: 197 Loss: 0.08585915948007834\n",
      "Iteration: 198 Loss: 0.08512020403417225\n",
      "Iteration: 199 Loss: 0.0843883415785096\n",
      "Iteration: 200 Loss: 0.0836635606523594\n",
      "Iteration: 201 Loss: 0.08294580932341829\n",
      "Iteration: 202 Loss: 0.08223500503098057\n",
      "Iteration: 203 Loss: 0.08153104350142201\n",
      "Iteration: 204 Loss: 0.08083380653114117\n",
      "Iteration: 205 Loss: 0.08014316853850638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 206 Loss: 0.07945900187106297\n",
      "Iteration: 207 Loss: 0.07878118091851051\n",
      "Iteration: 208 Loss: 0.07810958512771535\n",
      "Iteration: 209 Loss: 0.077444101045664\n",
      "Iteration: 210 Loss: 0.07678462353235509\n",
      "Iteration: 211 Loss: 0.07613105629077196\n",
      "Iteration: 212 Loss: 0.07548331185775721\n",
      "Iteration: 213 Loss: 0.07484131119010295\n",
      "Iteration: 214 Loss: 0.07420498296647607\n",
      "Iteration: 215 Loss: 0.07357426270963806\n",
      "Iteration: 216 Loss: 0.07294909181618815\n",
      "Iteration: 217 Loss: 0.07232941656386854\n",
      "Iteration: 218 Loss: 0.07171518715014644\n",
      "Iteration: 219 Loss: 0.07110635680090191\n",
      "Iteration: 220 Loss: 0.07050288097498884\n",
      "Iteration: 221 Loss: 0.06990431589007547\n",
      "Iteration: 222 Loss: 0.06931015829719504\n",
      "Iteration: 223 Loss: 0.06872078887762181\n",
      "Iteration: 224 Loss: 0.06813636233068317\n",
      "Iteration: 225 Loss: 0.06755700600761588\n",
      "Iteration: 226 Loss: 0.06698280772274791\n",
      "Iteration: 227 Loss: 0.06641381267207024\n",
      "Iteration: 228 Loss: 0.06585002734939313\n",
      "Iteration: 229 Loss: 0.06529142730065908\n",
      "Iteration: 230 Loss: 0.06473796615818779\n",
      "Iteration: 231 Loss: 0.06418958418383491\n",
      "Iteration: 232 Loss: 0.06364621529093778\n",
      "Iteration: 233 Loss: 0.06310779212392953\n",
      "Iteration: 234 Loss: 0.06257424921682335\n",
      "Iteration: 235 Loss: 0.062045524526892624\n",
      "Iteration: 236 Loss: 0.06152155977006043\n",
      "Iteration: 237 Loss: 0.06100230000395129\n",
      "Iteration: 238 Loss: 0.06048769285064607\n",
      "Iteration: 239 Loss: 0.05997768765827097\n",
      "Iteration: 240 Loss: 0.059472234796232835\n",
      "Iteration: 241 Loss: 0.058971285182605415\n",
      "Iteration: 242 Loss: 0.058474643160612294\n",
      "Iteration: 243 Loss: 0.057978952281518004\n",
      "Iteration: 244 Loss: 0.05748698502418166\n",
      "Iteration: 245 Loss: 0.05699946691891381\n",
      "Iteration: 246 Loss: 0.05651674736211456\n",
      "Iteration: 247 Loss: 0.05603890660057292\n",
      "Iteration: 248 Loss: 0.05556587073689295\n",
      "Iteration: 249 Loss: 0.055097498553413254\n",
      "Iteration: 250 Loss: 0.05463363453585799\n",
      "Iteration: 251 Loss: 0.05417413521717855\n",
      "Iteration: 252 Loss: 0.053718878506685976\n",
      "Iteration: 253 Loss: 0.0532677639892272\n",
      "Iteration: 254 Loss: 0.05282070947304367\n",
      "Iteration: 255 Loss: 0.05237764671264515\n",
      "Iteration: 256 Loss: 0.05193851762357573\n",
      "Iteration: 257 Loss: 0.05150327138178881\n",
      "Iteration: 258 Loss: 0.05107186236214536\n",
      "Iteration: 259 Loss: 0.050644248720671664\n",
      "Iteration: 260 Loss: 0.050220391418002296\n",
      "Iteration: 261 Loss: 0.04980025352751691\n",
      "Iteration: 262 Loss: 0.04938379972507255\n",
      "Iteration: 263 Loss: 0.048970995899484525\n",
      "Iteration: 264 Loss: 0.048561808850284806\n",
      "Iteration: 265 Loss: 0.04815620605438213\n",
      "Iteration: 266 Loss: 0.04775415549024442\n",
      "Iteration: 267 Loss: 0.04735562551080323\n",
      "Iteration: 268 Loss: 0.046960584756972554\n",
      "Iteration: 269 Loss: 0.046569002103886564\n",
      "Iteration: 270 Loss: 0.04618084663233561\n",
      "Iteration: 271 Loss: 0.04579608761857505\n",
      "Iteration: 272 Loss: 0.045414694536633146\n",
      "Iteration: 273 Loss: 0.04503663706832718\n",
      "Iteration: 274 Loss: 0.04466188511726907\n",
      "Iteration: 275 Loss: 0.0442904088241283\n",
      "Iteration: 276 Loss: 0.04392217858126299\n",
      "Iteration: 277 Loss: 0.043557165045509584\n",
      "Iteration: 278 Loss: 0.04319533914845836\n",
      "Iteration: 279 Loss: 0.042836672103933854\n",
      "Iteration: 280 Loss: 0.04248113541268185\n",
      "Iteration: 281 Loss: 0.042128700864452244\n",
      "Iteration: 282 Loss: 0.041779340537786015\n",
      "Iteration: 283 Loss: 0.041433026797874244\n",
      "Iteration: 284 Loss: 0.0410897322928834\n",
      "Iteration: 285 Loss: 0.040749429949128\n",
      "Iteration: 286 Loss: 0.04041209296545188\n",
      "Iteration: 287 Loss: 0.04007769480713485\n",
      "Iteration: 288 Loss: 0.03974620919959899\n",
      "Iteration: 289 Loss: 0.03941761012213861\n",
      "Iteration: 290 Loss: 0.03909187180185192\n",
      "Iteration: 291 Loss: 0.038768968707902175\n",
      "Iteration: 292 Loss: 0.03844887554620476\n",
      "Iteration: 293 Loss: 0.03813156725458815\n",
      "Iteration: 294 Loss: 0.03781701899845724\n",
      "Iteration: 295 Loss: 0.03750520616695636\n",
      "Iteration: 296 Loss: 0.037196104369611016\n",
      "Iteration: 297 Loss: 0.036889689433415564\n",
      "Iteration: 298 Loss: 0.03658593740031767\n",
      "Iteration: 299 Loss: 0.03628482452505179\n",
      "Iteration: 300 Loss: 0.03598632727326378\n",
      "Iteration: 301 Loss: 0.03569042231987399\n",
      "Iteration: 302 Loss: 0.035397086547624024\n",
      "Iteration: 303 Loss: 0.0351062970457572\n",
      "Iteration: 304 Loss: 0.03481803110878955\n",
      "Iteration: 305 Loss: 0.034532266235330185\n",
      "Iteration: 306 Loss: 0.03424898012691705\n",
      "Iteration: 307 Loss: 0.03396815068684426\n",
      "Iteration: 308 Loss: 0.03368975601895355\n",
      "Iteration: 309 Loss: 0.03341377442638062\n",
      "Iteration: 310 Loss: 0.03314018441024156\n",
      "Iteration: 311 Loss: 0.03286896466825608\n",
      "Iteration: 312 Loss: 0.032600050288019496\n",
      "Iteration: 313 Loss: 0.03233338763709222\n",
      "Iteration: 314 Loss: 0.03206897581049963\n",
      "Iteration: 315 Loss: 0.031806811764880316\n",
      "Iteration: 316 Loss: 0.031546888803498596\n",
      "Iteration: 317 Loss: 0.03128919693032785\n",
      "Iteration: 318 Loss: 0.03103372336333663\n",
      "Iteration: 319 Loss: 0.030780453128570705\n",
      "Iteration: 320 Loss: 0.03052936963134061\n",
      "Iteration: 321 Loss: 0.030280455151602877\n",
      "Iteration: 322 Loss: 0.03003369124557704\n",
      "Iteration: 323 Loss: 0.029789059055247348\n",
      "Iteration: 324 Loss: 0.029546539536809677\n",
      "Iteration: 325 Loss: 0.02930611362242286\n",
      "Iteration: 326 Loss: 0.029067762329641162\n",
      "Iteration: 327 Loss: 0.028831466831361587\n",
      "Iteration: 328 Loss: 0.0285972084970169\n",
      "Iteration: 329 Loss: 0.028364968913597365\n",
      "Iteration: 330 Loss: 0.028134729893126553\n",
      "Iteration: 331 Loss: 0.027906473471553145\n",
      "Iteration: 332 Loss: 0.027680181902648444\n",
      "Iteration: 333 Loss: 0.027455837649417815\n",
      "Iteration: 334 Loss: 0.027233423374692766\n",
      "Iteration: 335 Loss: 0.027012921931944277\n",
      "Iteration: 336 Loss: 0.026794316356907252\n",
      "Iteration: 337 Loss: 0.0265775898602839\n",
      "Iteration: 338 Loss: 0.02636272582159644\n",
      "Iteration: 339 Loss: 0.026149707784122157\n",
      "Iteration: 340 Loss: 0.02593851945078773\n",
      "Iteration: 341 Loss: 0.025729144680863136\n",
      "Iteration: 342 Loss: 0.02552156748730008\n",
      "Iteration: 343 Loss: 0.025315772034572782\n",
      "Iteration: 344 Loss: 0.025111742636900032\n",
      "Iteration: 345 Loss: 0.024909463756752733\n",
      "Iteration: 346 Loss: 0.024708920003577425\n",
      "Iteration: 347 Loss: 0.02451009613268383\n",
      "Iteration: 348 Loss: 0.024312977044265244\n",
      "Iteration: 349 Loss: 0.02411754778253502\n",
      "Iteration: 350 Loss: 0.023923793534972544\n",
      "Iteration: 351 Loss: 0.023731699631681577\n",
      "Iteration: 352 Loss: 0.023541251544871227\n",
      "Iteration: 353 Loss: 0.023352434888472554\n",
      "Iteration: 354 Loss: 0.023165235417911823\n",
      "Iteration: 355 Loss: 0.022979639030061196\n",
      "Iteration: 356 Loss: 0.02279563176339676\n",
      "Iteration: 357 Loss: 0.022613199798396894\n",
      "Iteration: 358 Loss: 0.022432329458218127\n",
      "Iteration: 359 Loss: 0.022253007209700283\n",
      "Iteration: 360 Loss: 0.022075219664756788\n",
      "Iteration: 361 Loss: 0.021898953582223445\n",
      "Iteration: 362 Loss: 0.02172419587025646\n",
      "Iteration: 363 Loss: 0.021550933589390097\n",
      "Iteration: 364 Loss: 0.021379153956397983\n",
      "Iteration: 365 Loss: 0.021208844349136006\n",
      "Iteration: 366 Loss: 0.021039992312593566\n",
      "Iteration: 367 Loss: 0.02087258556644764\n",
      "Iteration: 368 Loss: 0.02070661201449473\n",
      "Iteration: 369 Loss: 0.020542059756453002\n",
      "Iteration: 370 Loss: 0.020378917102776967\n",
      "Iteration: 371 Loss: 0.02021717259333957\n",
      "Iteration: 372 Loss: 0.020056815021118547\n",
      "Iteration: 373 Loss: 0.019897833462431878\n",
      "Iteration: 374 Loss: 0.019740217315827868\n",
      "Iteration: 375 Loss: 0.01958395635255005\n",
      "Iteration: 376 Loss: 0.01942904078268\n",
      "Iteration: 377 Loss: 0.019275461342796785\n",
      "Iteration: 378 Loss: 0.01912320941362145\n",
      "Iteration: 379 Loss: 0.018972277180124574\n",
      "Iteration: 380 Loss: 0.018822657852861756\n",
      "Iteration: 381 Loss: 0.018674345979364623\n",
      "Iteration: 382 Loss: 0.018527337890934102\n",
      "Iteration: 383 Loss: 0.01838163235810809\n",
      "Iteration: 384 Loss: 0.01823723157674373\n",
      "Iteration: 385 Loss: 0.018094142694561336\n",
      "Iteration: 386 Loss: 0.01795238025318206\n",
      "Iteration: 387 Loss: 0.017811970245300148\n",
      "Iteration: 388 Loss: 0.017672957157556544\n",
      "Iteration: 389 Loss: 0.0175354168373411\n",
      "Iteration: 390 Loss: 0.01739948143883612\n",
      "Iteration: 391 Loss: 0.017265391188906107\n",
      "Iteration: 392 Loss: 0.017133609923499625\n",
      "Iteration: 393 Loss: 0.017005098775912948\n",
      "Iteration: 394 Loss: 0.01688194540171217\n",
      "Iteration: 395 Loss: 0.016768244363396634\n",
      "Iteration: 396 Loss: 0.016668421006829397\n",
      "Iteration: 397 Loss: 0.01639637550140808\n",
      "Iteration: 398 Loss: 0.01587082612811561\n",
      "Iteration: 399 Loss: 0.015182097907580296\n",
      "Iteration: 400 Loss: 0.01440609436883867\n",
      "Iteration: 401 Loss: 0.013603305753109874\n",
      "Iteration: 402 Loss: 0.012819601305376245\n",
      "Iteration: 403 Loss: 0.012087742989951345\n",
      "Iteration: 404 Loss: 0.011429189640520754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 405 Loss: 0.010855983589803699\n",
      "Iteration: 406 Loss: 0.010372595020227094\n",
      "Iteration: 407 Loss: 0.010351796697910875\n",
      "Iteration: 408 Loss: 0.010475856918931476\n",
      "Iteration: 409 Loss: 0.010529281303325824\n",
      "Iteration: 410 Loss: 0.010522064463527239\n",
      "Iteration: 411 Loss: 0.010464799076940085\n",
      "Iteration: 412 Loss: 0.010368013636570455\n",
      "Iteration: 413 Loss: 0.010241678209857762\n",
      "Iteration: 414 Loss: 0.010094853643302872\n",
      "Iteration: 415 Loss: 0.009935465983551469\n",
      "Iteration: 416 Loss: 0.009770189042440265\n",
      "Iteration: 417 Loss: 0.009604416743619597\n",
      "Iteration: 418 Loss: 0.009442305953128564\n",
      "Iteration: 419 Loss: 0.009286870908056835\n",
      "Iteration: 420 Loss: 0.009140111958009619\n",
      "Iteration: 421 Loss: 0.00900316365513642\n",
      "Iteration: 422 Loss: 0.008877543985578697\n",
      "Iteration: 423 Loss: 0.008835729615441644\n",
      "Iteration: 424 Loss: 0.008785835693583082\n",
      "Iteration: 425 Loss: 0.008728682440526454\n",
      "Iteration: 426 Loss: 0.008665265400383926\n",
      "Iteration: 427 Loss: 0.008596664705059338\n",
      "Iteration: 428 Loss: 0.008523972505790338\n",
      "Iteration: 429 Loss: 0.008448237597391954\n",
      "Iteration: 430 Loss: 0.00837042568616543\n",
      "Iteration: 431 Loss: 0.008291393385586507\n",
      "Iteration: 432 Loss: 0.008211873833381405\n",
      "Iteration: 433 Loss: 0.008132471777814045\n",
      "Iteration: 434 Loss: 0.008053666048813355\n",
      "Iteration: 435 Loss: 0.00797581748135547\n",
      "Iteration: 436 Loss: 0.007899180567203479\n",
      "Iteration: 437 Loss: 0.007823917352989802\n",
      "Iteration: 438 Loss: 0.0077501123578644845\n",
      "Iteration: 439 Loss: 0.0076777875366583846\n",
      "Iteration: 440 Loss: 0.007606916552811868\n",
      "Iteration: 441 Loss: 0.007537437840904588\n",
      "Iteration: 442 Loss: 0.007469266126371256\n",
      "Iteration: 443 Loss: 0.0074023022274728205\n",
      "Iteration: 444 Loss: 0.007336441091536794\n",
      "Iteration: 445 Loss: 0.0072715781151940495\n",
      "Iteration: 446 Loss: 0.0072076138693104586\n",
      "Iteration: 447 Loss: 0.007144457396717175\n",
      "Iteration: 448 Loss: 0.007082028278256316\n",
      "Iteration: 449 Loss: 0.00702025767372929\n",
      "Iteration: 450 Loss: 0.006959088542611463\n",
      "Iteration: 451 Loss: 0.006898475238155769\n",
      "Iteration: 452 Loss: 0.006838382650683335\n",
      "Iteration: 453 Loss: 0.006778785053936268\n",
      "Iteration: 454 Loss: 0.006719664784465998\n",
      "Iteration: 455 Loss: 0.006661010859761258\n",
      "Iteration: 456 Loss: 0.006602817617484937\n",
      "Iteration: 457 Loss: 0.006545083436660315\n",
      "Iteration: 458 Loss: 0.00648780958253219\n",
      "Iteration: 459 Loss: 0.006430999200440906\n",
      "Iteration: 460 Loss: 0.0063746564705123195\n",
      "Iteration: 461 Loss: 0.006318785924220537\n",
      "Iteration: 462 Loss: 0.006263391915762118\n",
      "Iteration: 463 Loss: 0.006208478235413563\n",
      "Iteration: 464 Loss: 0.006154047848342092\n",
      "Iteration: 465 Loss: 0.006100102740359081\n",
      "Iteration: 466 Loss: 0.006046643851509269\n",
      "Iteration: 467 Loss: 0.005993671078877985\n",
      "Iteration: 468 Loss: 0.005941183331261611\n",
      "Iteration: 469 Loss: 0.005889178620144682\n",
      "Iteration: 470 Loss: 0.005837654173529391\n",
      "Iteration: 471 Loss: 0.00578660656139814\n",
      "Iteration: 472 Loss: 0.005736031823812602\n",
      "Iteration: 473 Loss: 0.005685925594761994\n",
      "Iteration: 474 Loss: 0.005636283216790488\n",
      "Iteration: 475 Loss: 0.005587099843116952\n",
      "Iteration: 476 Loss: 0.00553837052538236\n",
      "Iteration: 477 Loss: 0.005490090286326297\n",
      "Iteration: 478 Loss: 0.005442254177591198\n",
      "Iteration: 479 Loss: 0.005394857323532807\n",
      "Iteration: 480 Loss: 0.005347894952372351\n",
      "Iteration: 481 Loss: 0.0053013624163086946\n",
      "Iteration: 482 Loss: 0.005255255202341785\n",
      "Iteration: 483 Loss: 0.005209568935578908\n",
      "Iteration: 484 Loss: 0.005164299376713102\n",
      "Iteration: 485 Loss: 0.005119442415226697\n",
      "Iteration: 486 Loss: 0.0050749940597037205\n",
      "Iteration: 487 Loss: 0.005030950426419318\n",
      "Iteration: 488 Loss: 0.0049873077271733795\n",
      "Iteration: 489 Loss: 0.0049440622571340674\n",
      "Iteration: 490 Loss: 0.004901210383261226\n",
      "Iteration: 491 Loss: 0.004858748533712854\n",
      "Iteration: 492 Loss: 0.00481667318849045\n",
      "Iteration: 493 Loss: 0.004774980871454658\n",
      "Iteration: 494 Loss: 0.0047336681437413364\n",
      "Iteration: 495 Loss: 0.004692731598537981\n",
      "Iteration: 496 Loss: 0.004652167857118992\n",
      "Iteration: 497 Loss: 0.0046119735660077035\n",
      "Iteration: 498 Loss: 0.004572145395105347\n",
      "Iteration: 499 Loss: 0.004532680036627466\n",
      "Iteration: 500 Loss: 0.004493574204680699\n",
      "Iteration: 501 Loss: 0.004454824635331744\n",
      "Iteration: 502 Loss: 0.004416428087025957\n",
      "Iteration: 503 Loss: 0.004378381341237053\n",
      "Iteration: 504 Loss: 0.004340681203243895\n",
      "Iteration: 505 Loss: 0.004303324502953051\n",
      "Iteration: 506 Loss: 0.004266308095704677\n",
      "Iteration: 507 Loss: 0.004229628863007863\n",
      "Iteration: 508 Loss: 0.0041932837131821105\n",
      "Iteration: 509 Loss: 0.004157269581878152\n",
      "Iteration: 510 Loss: 0.0041215834324742585\n",
      "Iteration: 511 Loss: 0.004086222256345094\n",
      "Iteration: 512 Loss: 0.004051183073010444\n",
      "Iteration: 513 Loss: 0.00401646293017447\n",
      "Iteration: 514 Loss: 0.00398205890366745\n",
      "Iteration: 515 Loss: 0.003947968097306679\n",
      "Iteration: 516 Loss: 0.003914187642688851\n",
      "Iteration: 517 Loss: 0.003880714698934195\n",
      "Iteration: 518 Loss: 0.0038475464523889257\n",
      "Iteration: 519 Loss: 0.0038146801163045202\n",
      "Iteration: 520 Loss: 0.003782112930499814\n",
      "Iteration: 521 Loss: 0.0037498421610181773\n",
      "Iteration: 522 Loss: 0.0037178650997850646\n",
      "Iteration: 523 Loss: 0.0036861790642700666\n",
      "Iteration: 524 Loss: 0.0036547813971616327\n",
      "Iteration: 525 Loss: 0.003623669466050859\n",
      "Iteration: 526 Loss: 0.003592840663131256\n",
      "Iteration: 527 Loss: 0.0035622924049121263\n",
      "Iteration: 528 Loss: 0.0035320221319447955\n",
      "Iteration: 529 Loss: 0.0035020273085645463\n",
      "Iteration: 530 Loss: 0.0034723054226411267\n",
      "Iteration: 531 Loss: 0.003442853985343125\n",
      "Iteration: 532 Loss: 0.003413670530910601\n",
      "Iteration: 533 Loss: 0.0033847526164350033\n",
      "Iteration: 534 Loss: 0.0033560978216460456\n",
      "Iteration: 535 Loss: 0.0033277037487046404\n",
      "Iteration: 536 Loss: 0.003299568021999322\n",
      "Iteration: 537 Loss: 0.003271688287944955\n",
      "Iteration: 538 Loss: 0.003244062214786394\n",
      "Iteration: 539 Loss: 0.0032166874924015455\n",
      "Iteration: 540 Loss: 0.0031895618321077495\n",
      "Iteration: 541 Loss: 0.0031626829664672534\n",
      "Iteration: 542 Loss: 0.0031360486490960913\n",
      "Iteration: 543 Loss: 0.0031096566544704927\n",
      "Iteration: 544 Loss: 0.003083504777737093\n",
      "Iteration: 545 Loss: 0.0030575908345221235\n",
      "Iteration: 546 Loss: 0.003031912660741563\n",
      "Iteration: 547 Loss: 0.0030064681124125853\n",
      "Iteration: 548 Loss: 0.0029812550654659525\n",
      "Iteration: 549 Loss: 0.0029562714155583567\n",
      "Iteration: 550 Loss: 0.002931515077888921\n",
      "Iteration: 551 Loss: 0.0029069839870130066\n",
      "Iteration: 552 Loss: 0.0028826760966610917\n",
      "Iteration: 553 Loss: 0.00285858937955598\n",
      "Iteration: 554 Loss: 0.00283472182723463\n",
      "Iteration: 555 Loss: 0.002811071449869145\n",
      "Iteration: 556 Loss: 0.0027876362760905224\n",
      "Iteration: 557 Loss: 0.0027644143528136747\n",
      "Iteration: 558 Loss: 0.0027414037450649675\n",
      "Iteration: 559 Loss: 0.0027186025358099013\n",
      "Iteration: 560 Loss: 0.002696008825784066\n",
      "Iteration: 561 Loss: 0.002673620733324551\n",
      "Iteration: 562 Loss: 0.0026514363942033493\n",
      "Iteration: 563 Loss: 0.002629453961462507\n",
      "Iteration: 564 Loss: 0.002607671605250813\n",
      "Iteration: 565 Loss: 0.0025860875126623257\n",
      "Iteration: 566 Loss: 0.0025646998875750245\n",
      "Iteration: 567 Loss: 0.0025435069504932527\n",
      "Iteration: 568 Loss: 0.0025225069383897247\n",
      "Iteration: 569 Loss: 0.002501698104549483\n",
      "Iteration: 570 Loss: 0.002481078718415557\n",
      "Iteration: 571 Loss: 0.0024606470654365683\n",
      "Iteration: 572 Loss: 0.0024404014469138293\n",
      "Iteration: 573 Loss: 0.002420340179852291\n",
      "Iteration: 574 Loss: 0.0024004615968106022\n",
      "Iteration: 575 Loss: 0.0023807640457538175\n",
      "Iteration: 576 Loss: 0.0023612458899073424\n",
      "Iteration: 577 Loss: 0.0023419055076118116\n",
      "Iteration: 578 Loss: 0.0023227412921795433\n",
      "Iteration: 579 Loss: 0.0023037516517518852\n",
      "Iteration: 580 Loss: 0.002284935009158536\n",
      "Iteration: 581 Loss: 0.0022662898017767616\n",
      "Iteration: 582 Loss: 0.0022478144813938483\n",
      "Iteration: 583 Loss: 0.002229507514069571\n",
      "Iteration: 584 Loss: 0.0022113673799997813\n",
      "Iteration: 585 Loss: 0.0021933925733812628\n",
      "Iteration: 586 Loss: 0.0021755816022791187\n",
      "Iteration: 587 Loss: 0.002157932988493154\n",
      "Iteration: 588 Loss: 0.002140445267427346\n",
      "Iteration: 589 Loss: 0.0021231169879598003\n",
      "Iteration: 590 Loss: 0.0021059467123136677\n",
      "Iteration: 591 Loss: 0.0020889330159291303\n",
      "Iteration: 592 Loss: 0.002072074487337013\n",
      "Iteration: 593 Loss: 0.002055369728033911\n",
      "Iteration: 594 Loss: 0.0020388173523576323\n",
      "Iteration: 595 Loss: 0.002022415987363511\n",
      "Iteration: 596 Loss: 0.0020061642727030394\n",
      "Iteration: 597 Loss: 0.001990060860502765\n",
      "Iteration: 598 Loss: 0.0019741044152445466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 599 Loss: 0.0019582936136466208\n",
      "Iteration: 600 Loss: 0.0019426271445460157\n",
      "Iteration: 601 Loss: 0.001927103708781977\n",
      "Iteration: 602 Loss: 0.0019117220190804382\n",
      "Iteration: 603 Loss: 0.0018964807999417436\n",
      "Iteration: 604 Loss: 0.0018813787875228516\n",
      "Iteration: 605 Loss: 0.0018664147295285158\n",
      "Iteration: 606 Loss: 0.001851587385098471\n",
      "Iteration: 607 Loss: 0.0018368955246982652\n",
      "Iteration: 608 Loss: 0.0018223379300088753\n",
      "Iteration: 609 Loss: 0.0018079133938188295\n",
      "Iteration: 610 Loss: 0.0017936207199178678\n",
      "Iteration: 611 Loss: 0.0017794587229894103\n",
      "Iteration: 612 Loss: 0.0017654262285061333\n",
      "Iteration: 613 Loss: 0.001751522072625531\n",
      "Iteration: 614 Loss: 0.0017377451020862801\n",
      "Iteration: 615 Loss: 0.0017240941741063502\n",
      "Iteration: 616 Loss: 0.0017105681562819928\n",
      "Iteration: 617 Loss: 0.0016971659264865464\n",
      "Iteration: 618 Loss: 0.0016838863727714423\n",
      "Iteration: 619 Loss: 0.0016707283932680668\n",
      "Iteration: 620 Loss: 0.0016576908960897465\n",
      "Iteration: 621 Loss: 0.0016447727992349745\n",
      "Iteration: 622 Loss: 0.0016319730304924813\n",
      "Iteration: 623 Loss: 0.001619290527345764\n",
      "Iteration: 624 Loss: 0.001606724236880259\n",
      "Iteration: 625 Loss: 0.0015942731156887758\n",
      "Iteration: 626 Loss: 0.0015819361297812547\n",
      "Iteration: 627 Loss: 0.0015697122544925152\n",
      "Iteration: 628 Loss: 0.001557600474392333\n",
      "Iteration: 629 Loss: 0.0015455997831964005\n",
      "Iteration: 630 Loss: 0.0015337091836775086\n",
      "Iteration: 631 Loss: 0.001521927687578279\n",
      "Iteration: 632 Loss: 0.001510254315524746\n",
      "Iteration: 633 Loss: 0.0014986880969398073\n",
      "Iteration: 634 Loss: 0.0014872280699588032\n",
      "Iteration: 635 Loss: 0.0014758732813453276\n",
      "Iteration: 636 Loss: 0.0014646227864078892\n",
      "Iteration: 637 Loss: 0.0014534756489175977\n",
      "Iteration: 638 Loss: 0.0014424309410269091\n",
      "Iteration: 639 Loss: 0.0014314877431882832\n",
      "Iteration: 640 Loss: 0.0014206451440750435\n",
      "Iteration: 641 Loss: 0.0014099022405021306\n",
      "Iteration: 642 Loss: 0.0013992581373473815\n",
      "Iteration: 643 Loss: 0.0013887119474750244\n",
      "Iteration: 644 Loss: 0.0013782627916585502\n",
      "Iteration: 645 Loss: 0.0013679097985048933\n",
      "Iteration: 646 Loss: 0.0013576521043796197\n",
      "Iteration: 647 Loss: 0.0013474888533324483\n",
      "Iteration: 648 Loss: 0.0013374191970250168\n",
      "Iteration: 649 Loss: 0.0013274422946571139\n",
      "Iteration: 650 Loss: 0.0013175573128951295\n",
      "Iteration: 651 Loss: 0.00130776342580169\n",
      "Iteration: 652 Loss: 0.001298059814763869\n",
      "Iteration: 653 Loss: 0.0012884456684258909\n",
      "Iteration: 654 Loss: 0.0012789201826184189\n",
      "Iteration: 655 Loss: 0.001269482560292116\n",
      "Iteration: 656 Loss: 0.001260132011448859\n",
      "Iteration: 657 Loss: 0.0012508677530768794\n",
      "Iteration: 658 Loss: 0.0012416890090842757\n",
      "Iteration: 659 Loss: 0.0012325950102337262\n",
      "Iteration: 660 Loss: 0.0012235849940785638\n",
      "Iteration: 661 Loss: 0.0012146582048991298\n",
      "Iteration: 662 Loss: 0.0012058138936399522\n",
      "Iteration: 663 Loss: 0.0011970513178477023\n",
      "Iteration: 664 Loss: 0.0011883697416098602\n",
      "Iteration: 665 Loss: 0.0011797684354932236\n",
      "Iteration: 666 Loss: 0.0011712466764847989\n",
      "Iteration: 667 Loss: 0.0011628037479314438\n",
      "Iteration: 668 Loss: 0.0011544389394824277\n",
      "Iteration: 669 Loss: 0.0011461515470298001\n",
      "Iteration: 670 Loss: 0.001137940872652523\n",
      "Iteration: 671 Loss: 0.0011298062245589085\n",
      "Iteration: 672 Loss: 0.0011217469170301713\n",
      "Iteration: 673 Loss: 0.0011137622703657126\n",
      "Iteration: 674 Loss: 0.0011058516108279493\n",
      "Iteration: 675 Loss: 0.0010980142705880086\n",
      "Iteration: 676 Loss: 0.0010902495876727737\n",
      "Iteration: 677 Loss: 0.0010825569059105884\n",
      "Iteration: 678 Loss: 0.0010749355748796827\n",
      "Iteration: 679 Loss: 0.001067384949856433\n",
      "Iteration: 680 Loss: 0.0010599043917638967\n",
      "Iteration: 681 Loss: 0.0010524932671213147\n",
      "Iteration: 682 Loss: 0.001045150947993003\n",
      "Iteration: 683 Loss: 0.0010378768119406316\n",
      "Iteration: 684 Loss: 0.0010306702419728307\n",
      "Iteration: 685 Loss: 0.0010235306264972496\n",
      "Iteration: 686 Loss: 0.0010164573592719927\n",
      "Iteration: 687 Loss: 0.0010094498393587307\n",
      "Iteration: 688 Loss: 0.001002507471075659\n",
      "Iteration: 689 Loss: 0.0009956296639516392\n",
      "Iteration: 690 Loss: 0.0009888158326789318\n",
      "Iteration: 691 Loss: 0.0009820653970691404\n",
      "Iteration: 692 Loss: 0.000975377782007025\n",
      "Iteration: 693 Loss: 0.0009687524174074046\n",
      "Iteration: 694 Loss: 0.0009621887381698988\n",
      "Iteration: 695 Loss: 0.0009556861841356764\n",
      "Iteration: 696 Loss: 0.0009492442000447312\n",
      "Iteration: 697 Loss: 0.0009428622354926589\n",
      "Iteration: 698 Loss: 0.0009365397448888214\n",
      "Iteration: 699 Loss: 0.0009302761874137184\n",
      "Iteration: 700 Loss: 0.0009240710269783568\n",
      "Iteration: 701 Loss: 0.0009179237321827168\n",
      "Iteration: 702 Loss: 0.0009118337762753464\n",
      "Iteration: 703 Loss: 0.0009058006371130027\n",
      "Iteration: 704 Loss: 0.0008998237971204786\n",
      "Iteration: 705 Loss: 0.0008939027432509104\n",
      "Iteration: 706 Loss: 0.0008880369669474133\n",
      "Iteration: 707 Loss: 0.0008822259641030227\n",
      "Iteration: 708 Loss: 0.0008764692350237705\n",
      "Iteration: 709 Loss: 0.0008707662843883305\n",
      "Iteration: 710 Loss: 0.0008651166212128818\n",
      "Iteration: 711 Loss: 0.0008595197588110818\n",
      "Iteration: 712 Loss: 0.0008539752147578757\n",
      "Iteration: 713 Loss: 0.0008484825108533296\n",
      "Iteration: 714 Loss: 0.0008430411730846849\n",
      "Iteration: 715 Loss: 0.0008376507315909662\n",
      "Iteration: 716 Loss: 0.000832310720626529\n",
      "Iteration: 717 Loss: 0.0008270206785247471\n",
      "Iteration: 718 Loss: 0.0008217801476644637\n",
      "Iteration: 719 Loss: 0.0008165886744326169\n",
      "Iteration: 720 Loss: 0.0008114458091900287\n",
      "Iteration: 721 Loss: 0.000806351106237262\n",
      "Iteration: 722 Loss: 0.0008013041237794886\n",
      "Iteration: 723 Loss: 0.0007963044238926927\n",
      "Iteration: 724 Loss: 0.0007913515724893536\n",
      "Iteration: 725 Loss: 0.0007864451392852028\n",
      "Iteration: 726 Loss: 0.0007815846977650147\n",
      "Iteration: 727 Loss: 0.0007767698251499977\n",
      "Iteration: 728 Loss: 0.0007720001023646339\n",
      "Iteration: 729 Loss: 0.0007672751140034258\n",
      "Iteration: 730 Loss: 0.0007625944482984442\n",
      "Iteration: 731 Loss: 0.0007579576970872423\n",
      "Iteration: 732 Loss: 0.0007533644557807103\n",
      "Iteration: 733 Loss: 0.0007488143233303314\n",
      "Iteration: 734 Loss: 0.0007443069021970503\n",
      "Iteration: 735 Loss: 0.000739841798319821\n",
      "Iteration: 736 Loss: 0.0007354186210831894\n",
      "Iteration: 737 Loss: 0.0007310369832876875\n",
      "Iteration: 738 Loss: 0.0007266965011169183\n",
      "Iteration: 739 Loss: 0.0007223967941087318\n",
      "Iteration: 740 Loss: 0.0007181374851235957\n",
      "Iteration: 741 Loss: 0.0007139182003139202\n",
      "Iteration: 742 Loss: 0.0007097385690939778\n",
      "Iteration: 743 Loss: 0.0007055982241102706\n",
      "Iteration: 744 Loss: 0.0007014968012116797\n",
      "Iteration: 745 Loss: 0.0006974339394192076\n",
      "Iteration: 746 Loss: 0.0006934092808969694\n",
      "Iteration: 747 Loss: 0.000689422470922555\n",
      "Iteration: 748 Loss: 0.0006854731578584581\n",
      "Iteration: 749 Loss: 0.0006815609931224907\n",
      "Iteration: 750 Loss: 0.0006776856311598503\n",
      "Iteration: 751 Loss: 0.0006738467294136388\n",
      "Iteration: 752 Loss: 0.0006700439482972715\n",
      "Iteration: 753 Loss: 0.0006662769511663429\n",
      "Iteration: 754 Loss: 0.0006625454042898189\n",
      "Iteration: 755 Loss: 0.0006588489768229042\n",
      "Iteration: 756 Loss: 0.0006551873407798643\n",
      "Iteration: 757 Loss: 0.0006515601710057727\n",
      "Iteration: 758 Loss: 0.0006479671451496481\n",
      "Iteration: 759 Loss: 0.0006444079436384183\n",
      "Iteration: 760 Loss: 0.0006408822496488856\n",
      "Iteration: 761 Loss: 0.0006373897490812129\n",
      "Iteration: 762 Loss: 0.0006339301305336754\n",
      "Iteration: 763 Loss: 0.0006305030852760969\n",
      "Iteration: 764 Loss: 0.0006271083072226056\n",
      "Iteration: 765 Loss: 0.0006237454929083983\n",
      "Iteration: 766 Loss: 0.00062041434146227\n",
      "Iteration: 767 Loss: 0.0006171145545822832\n",
      "Iteration: 768 Loss: 0.0006138458365101584\n",
      "Iteration: 769 Loss: 0.0006106078940071118\n",
      "Iteration: 770 Loss: 0.0006074004363291979\n",
      "Iteration: 771 Loss: 0.0006042231752026748\n",
      "Iteration: 772 Loss: 0.0006010758247999231\n",
      "Iteration: 773 Loss: 0.0005979581017159363\n",
      "Iteration: 774 Loss: 0.0005948697249447352\n",
      "Iteration: 775 Loss: 0.0005918104158551007\n",
      "Iteration: 776 Loss: 0.000588779898169122\n",
      "Iteration: 777 Loss: 0.0005857778979376716\n",
      "Iteration: 778 Loss: 0.0005828041435192137\n",
      "Iteration: 779 Loss: 0.0005798583655563827\n",
      "Iteration: 780 Loss: 0.0005769402969548707\n",
      "Iteration: 781 Loss: 0.0005740496728603333\n",
      "Iteration: 782 Loss: 0.0005711862306386892\n",
      "Iteration: 783 Loss: 0.0005683497098529621\n",
      "Iteration: 784 Loss: 0.0005655398522436229\n",
      "Iteration: 785 Loss: 0.0005627564017074048\n",
      "Iteration: 786 Loss: 0.0005599991042767922\n",
      "Iteration: 787 Loss: 0.0005572677080999155\n",
      "Iteration: 788 Loss: 0.0005545619634210627\n",
      "Iteration: 789 Loss: 0.0005518816225604501\n",
      "Iteration: 790 Loss: 0.0005492264398950677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 791 Loss: 0.0005465961718402278\n",
      "Iteration: 792 Loss: 0.0005439905768302484\n",
      "Iteration: 793 Loss: 0.0005414094152998539\n",
      "Iteration: 794 Loss: 0.0005388524496669892\n",
      "Iteration: 795 Loss: 0.0005363194443139018\n",
      "Iteration: 796 Loss: 0.0005338101655702483\n",
      "Iteration: 797 Loss: 0.0005313243816953653\n",
      "Iteration: 798 Loss: 0.0005288618628614111\n",
      "Iteration: 799 Loss: 0.0005264223811371598\n",
      "Iteration: 800 Loss: 0.000524005710470778\n",
      "Iteration: 801 Loss: 0.0005216116266742706\n",
      "Iteration: 802 Loss: 0.0005192399074075891\n",
      "Iteration: 803 Loss: 0.0005168903321632314\n",
      "Iteration: 804 Loss: 0.0005145626822504313\n",
      "Iteration: 805 Loss: 0.0005122567407809635\n",
      "Iteration: 806 Loss: 0.0005099722926535215\n",
      "Iteration: 807 Loss: 0.0005077091245406567\n",
      "Iteration: 808 Loss: 0.0005054670248731286\n",
      "Iteration: 809 Loss: 0.0005032457838279512\n",
      "Iteration: 810 Loss: 0.0005010451933126387\n",
      "Iteration: 811 Loss: 0.0004988650469544408\n",
      "Iteration: 812 Loss: 0.0004967051400846261\n",
      "Iteration: 813 Loss: 0.0004945652697277235\n",
      "Iteration: 814 Loss: 0.0004924452345885441\n",
      "Iteration: 815 Loss: 0.0004903448350396338\n",
      "Iteration: 816 Loss: 0.0004882638731098944\n",
      "Iteration: 817 Loss: 0.00048620215247198635\n",
      "Iteration: 818 Loss: 0.00048415947843206654\n",
      "Iteration: 819 Loss: 0.00048213565791751075\n",
      "Iteration: 820 Loss: 0.0004801304994665485\n",
      "Iteration: 821 Loss: 0.0004781438132172197\n",
      "Iteration: 822 Loss: 0.0004761754108968452\n",
      "Iteration: 823 Loss: 0.0004742251058124406\n",
      "Iteration: 824 Loss: 0.0004722927128395678\n",
      "Iteration: 825 Loss: 0.0004703780484133557\n",
      "Iteration: 826 Loss: 0.00046848093051816227\n",
      "Iteration: 827 Loss: 0.0004666011786784855\n",
      "Iteration: 828 Loss: 0.00046473861394954135\n",
      "Iteration: 829 Loss: 0.0004628930589085175\n",
      "Iteration: 830 Loss: 0.0004610643376453897\n",
      "Iteration: 831 Loss: 0.0004592522757542533\n",
      "Iteration: 832 Loss: 0.00045745670032423607\n",
      "Iteration: 833 Loss: 0.0004556774399326002\n",
      "Iteration: 834 Loss: 0.00045391432463444924\n",
      "Iteration: 835 Loss: 0.0004521671859565835\n",
      "Iteration: 836 Loss: 0.000450435856887926\n",
      "Iteration: 837 Loss: 0.0004487201718732351\n",
      "Iteration: 838 Loss: 0.00044701996680358136\n",
      "Iteration: 839 Loss: 0.0004453350790110665\n",
      "Iteration: 840 Loss: 0.00044366534725899244\n",
      "Iteration: 841 Loss: 0.0004420106117362783\n",
      "Iteration: 842 Loss: 0.00044037071404908287\n",
      "Iteration: 843 Loss: 0.00043874549721452285\n",
      "Iteration: 844 Loss: 0.0004371348056526717\n",
      "Iteration: 845 Loss: 0.00043553848518063006\n",
      "Iteration: 846 Loss: 0.00043395638300442656\n",
      "Iteration: 847 Loss: 0.00043238834771338117\n",
      "Iteration: 848 Loss: 0.0004308342292726186\n",
      "Iteration: 849 Loss: 0.00042929387901605143\n",
      "Iteration: 850 Loss: 0.00042776714964020025\n",
      "Iteration: 851 Loss: 0.0004262538951970864\n",
      "Iteration: 852 Loss: 0.00042475397108879915\n",
      "Iteration: 853 Loss: 0.000423267234059351\n",
      "Iteration: 854 Loss: 0.00042179354218872193\n",
      "Iteration: 855 Loss: 0.00042033275488690254\n",
      "Iteration: 856 Loss: 0.0004188847328866924\n",
      "Iteration: 857 Loss: 0.000417449338237273\n",
      "Iteration: 858 Loss: 0.00041602643429760824\n",
      "Iteration: 859 Loss: 0.00041461588573075666\n",
      "Iteration: 860 Loss: 0.00041321755849556404\n",
      "Iteration: 861 Loss: 0.00041183131984295175\n",
      "Iteration: 862 Loss: 0.0004104570383063558\n",
      "Iteration: 863 Loss: 0.00040909458369751343\n",
      "Iteration: 864 Loss: 0.0004077438270984126\n",
      "Iteration: 865 Loss: 0.000406404640855159\n",
      "Iteration: 866 Loss: 0.0004050768985716017\n",
      "Iteration: 867 Loss: 0.0004037604751026577\n",
      "Iteration: 868 Loss: 0.00040245524654738935\n",
      "Iteration: 869 Loss: 0.00040116109024237366\n",
      "Iteration: 870 Loss: 0.00039987788475468\n",
      "Iteration: 871 Loss: 0.00039860550987548413\n",
      "Iteration: 872 Loss: 0.00039734384661320904\n",
      "Iteration: 873 Loss: 0.0003960927771860232\n",
      "Iteration: 874 Loss: 0.0003948521850161934\n",
      "Iteration: 875 Loss: 0.0003936219547214283\n",
      "Iteration: 876 Loss: 0.00039240197210912194\n",
      "Iteration: 877 Loss: 0.00039119212416892547\n",
      "Iteration: 878 Loss: 0.00038999229906515955\n",
      "Iteration: 879 Loss: 0.0003888023861304158\n",
      "Iteration: 880 Loss: 0.0003876222758576307\n",
      "Iteration: 881 Loss: 0.0003864518598931496\n",
      "Iteration: 882 Loss: 0.00038529103102927725\n",
      "Iteration: 883 Loss: 0.00038413968319675934\n",
      "Iteration: 884 Loss: 0.00038299771145799277\n",
      "Iteration: 885 Loss: 0.00038186501199856057\n",
      "Iteration: 886 Loss: 0.0003807414821199079\n",
      "Iteration: 887 Loss: 0.00037962702023253055\n",
      "Iteration: 888 Loss: 0.00037852152584731414\n",
      "Iteration: 889 Loss: 0.000377424899568285\n",
      "Iteration: 890 Loss: 0.0003763370430848769\n",
      "Iteration: 891 Loss: 0.00037525785916423177\n",
      "Iteration: 892 Loss: 0.0003741872516427725\n",
      "Iteration: 893 Loss: 0.0003731251254187447\n",
      "Iteration: 894 Loss: 0.0003720713864444964\n",
      "Iteration: 895 Loss: 0.0003710259417183543\n",
      "Iteration: 896 Loss: 0.0003699886992759503\n",
      "Iteration: 897 Loss: 0.0003689595681825605\n",
      "Iteration: 898 Loss: 0.00036793845852545827\n",
      "Iteration: 899 Loss: 0.00036692528140565023\n",
      "Iteration: 900 Loss: 0.0003659199489289732\n",
      "Iteration: 901 Loss: 0.0003649223741986932\n",
      "Iteration: 902 Loss: 0.0003639324713064305\n",
      "Iteration: 903 Loss: 0.00036295015532515455\n",
      "Iteration: 904 Loss: 0.0003619753422994451\n",
      "Iteration: 905 Loss: 0.00036100794923827424\n",
      "Iteration: 906 Loss: 0.00036004789410616183\n",
      "Iteration: 907 Loss: 0.0003590950958148924\n",
      "Iteration: 908 Loss: 0.0003581494742150387\n",
      "Iteration: 909 Loss: 0.0003572109500877055\n",
      "Iteration: 910 Loss: 0.00035627944513647085\n",
      "Iteration: 911 Loss: 0.00035535488197839093\n",
      "Iteration: 912 Loss: 0.0003544371841355765\n",
      "Iteration: 913 Loss: 0.0003535262760272854\n",
      "Iteration: 914 Loss: 0.00035262208296117924\n",
      "Iteration: 915 Loss: 0.00035172453112436225\n",
      "Iteration: 916 Loss: 0.0003508335475760161\n",
      "Iteration: 917 Loss: 0.0003499490602385927\n",
      "Iteration: 918 Loss: 0.0003490709978890409\n",
      "Iteration: 919 Loss: 0.00034819929015036325\n",
      "Iteration: 920 Loss: 0.0003473338674835824\n",
      "Iteration: 921 Loss: 0.0003464746611795178\n",
      "Iteration: 922 Loss: 0.0003456216033492112\n",
      "Iteration: 923 Loss: 0.00034477462691721834\n",
      "Iteration: 924 Loss: 0.0003439336656122624\n",
      "Iteration: 925 Loss: 0.0003430986539593119\n",
      "Iteration: 926 Loss: 0.000342269527270412\n",
      "Iteration: 927 Loss: 0.0003414462216378192\n",
      "Iteration: 928 Loss: 0.0003406286739245306\n",
      "Iteration: 929 Loss: 0.000339816821756948\n",
      "Iteration: 930 Loss: 0.0003390106035155429\n",
      "Iteration: 931 Loss: 0.00033820995832830025\n",
      "Iteration: 932 Loss: 0.00033741482606068304\n",
      "Iteration: 933 Loss: 0.0003366251473096837\n",
      "Iteration: 934 Loss: 0.0003358408633935699\n",
      "Iteration: 935 Loss: 0.00033506191634549324\n",
      "Iteration: 936 Loss: 0.00033428824890473625\n",
      "Iteration: 937 Loss: 0.00033351980450911547\n",
      "Iteration: 938 Loss: 0.00033275652728639664\n",
      "Iteration: 939 Loss: 0.00033199836204749573\n",
      "Iteration: 940 Loss: 0.00033124525427799474\n",
      "Iteration: 941 Loss: 0.00033049715013057547\n",
      "Iteration: 942 Loss: 0.0003297539964171658\n",
      "Iteration: 943 Loss: 0.0003290157406011522\n",
      "Iteration: 944 Loss: 0.0003282823307898123\n",
      "Iteration: 945 Loss: 0.00032755371572791347\n",
      "Iteration: 946 Loss: 0.00032682984478840936\n",
      "Iteration: 947 Loss: 0.0003261106679654842\n",
      "Iteration: 948 Loss: 0.00032539613586764286\n",
      "Iteration: 949 Loss: 0.00032468619971069826\n",
      "Iteration: 950 Loss: 0.0003239808113090577\n",
      "Iteration: 951 Loss: 0.0003232799230700363\n",
      "Iteration: 952 Loss: 0.0003225834879855774\n",
      "Iteration: 953 Loss: 0.00032189145962583515\n",
      "Iteration: 954 Loss: 0.00032120379213128103\n",
      "Iteration: 955 Loss: 0.0003205204402069019\n",
      "Iteration: 956 Loss: 0.00031984135911383\n",
      "Iteration: 957 Loss: 0.00031916650466385217\n",
      "Iteration: 958 Loss: 0.00031849583321168814\n",
      "Iteration: 959 Loss: 0.0003178293016485743\n",
      "Iteration: 960 Loss: 0.00031716686739566465\n",
      "Iteration: 961 Loss: 0.000316508488396736\n",
      "Iteration: 962 Loss: 0.000315854123112272\n",
      "Iteration: 963 Loss: 0.0003152037305133658\n",
      "Iteration: 964 Loss: 0.00031455727007392495\n",
      "Iteration: 965 Loss: 0.00031391470176530916\n",
      "Iteration: 966 Loss: 0.0003132759860497915\n",
      "Iteration: 967 Loss: 0.0003126410838745407\n",
      "Iteration: 968 Loss: 0.0003120099566646917\n",
      "Iteration: 969 Loss: 0.00031138256631801085\n",
      "Iteration: 970 Loss: 0.00031075887519829926\n",
      "Iteration: 971 Loss: 0.0003101388461299124\n",
      "Iteration: 972 Loss: 0.0003095224423911903\n",
      "Iteration: 973 Loss: 0.0003089096277091149\n",
      "Iteration: 974 Loss: 0.0003083003662531213\n",
      "Iteration: 975 Loss: 0.0003076946226296439\n",
      "Iteration: 976 Loss: 0.0003070923618767346\n",
      "Iteration: 977 Loss: 0.00030649354945701577\n",
      "Iteration: 978 Loss: 0.000305898151254194\n",
      "Iteration: 979 Loss: 0.00030530613356620533\n",
      "Iteration: 980 Loss: 0.000304717463100262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 981 Loss: 0.0003041321069669449\n",
      "Iteration: 982 Loss: 0.0003035500326755229\n",
      "Iteration: 983 Loss: 0.00030297120812860135\n",
      "Iteration: 984 Loss: 0.00030239560161673853\n",
      "Iteration: 985 Loss: 0.00030182318181365273\n",
      "Iteration: 986 Loss: 0.0003012539177705723\n",
      "Iteration: 987 Loss: 0.0003006877789121563\n",
      "Iteration: 988 Loss: 0.00030012473503057604\n",
      "Iteration: 989 Loss: 0.00029956475628154476\n",
      "Iteration: 990 Loss: 0.00029900781317875334\n",
      "Iteration: 991 Loss: 0.0002984538765898215\n",
      "Iteration: 992 Loss: 0.0002979029177303047\n",
      "Iteration: 993 Loss: 0.0002973549081614561\n",
      "Iteration: 994 Loss: 0.00029680981978298074\n",
      "Iteration: 995 Loss: 0.0002962676248304738\n",
      "Iteration: 996 Loss: 0.0002957282958699581\n",
      "Iteration: 997 Loss: 0.0002951918057933897\n",
      "Iteration: 998 Loss: 0.0002946581278150947\n",
      "Iteration: 999 Loss: 0.0002941272354670607\n",
      "Iteration: 1000 Loss: 0.00029359910259442294\n",
      "Iteration: 1001 Loss: 0.0002930737033521007\n",
      "Iteration: 1002 Loss: 0.000292551012199625\n",
      "Iteration: 1003 Loss: 0.0002920310038976089\n",
      "Iteration: 1004 Loss: 0.0002915136535042105\n",
      "Iteration: 1005 Loss: 0.00029099893637031944\n",
      "Iteration: 1006 Loss: 0.00029048682813594183\n",
      "Iteration: 1007 Loss: 0.0002899773047263638\n",
      "Iteration: 1008 Loss: 0.0002894703423489257\n",
      "Iteration: 1009 Loss: 0.0002889659174883683\n",
      "Iteration: 1010 Loss: 0.0002884640069031775\n",
      "Iteration: 1011 Loss: 0.00028796458762260583\n",
      "Iteration: 1012 Loss: 0.0002874676369427208\n",
      "Iteration: 1013 Loss: 0.00028697313242303303\n",
      "Iteration: 1014 Loss: 0.0002864810518825782\n",
      "Iteration: 1015 Loss: 0.00028599137339677463\n",
      "Iteration: 1016 Loss: 0.0002855040752935485\n",
      "Iteration: 1017 Loss: 0.00028501913615102087\n",
      "Iteration: 1018 Loss: 0.00028453653479282727\n",
      "Iteration: 1019 Loss: 0.00028405625028613587\n",
      "Iteration: 1020 Loss: 0.00028357826193744096\n",
      "Iteration: 1021 Loss: 0.00028310254928971866\n",
      "Iteration: 1022 Loss: 0.0002826290921199235\n",
      "Iteration: 1023 Loss: 0.00028215787043474214\n",
      "Iteration: 1024 Loss: 0.00028168886446830146\n",
      "Iteration: 1025 Loss: 0.00028122205467850394\n",
      "Iteration: 1026 Loss: 0.0002807574217456455\n",
      "Iteration: 1027 Loss: 0.00028029494656718954\n",
      "Iteration: 1028 Loss: 0.00027983461025665745\n",
      "Iteration: 1029 Loss: 0.0002793763941401689\n",
      "Iteration: 1030 Loss: 0.0002789202797537823\n",
      "Iteration: 1031 Loss: 0.00027846624884026756\n",
      "Iteration: 1032 Loss: 0.00027801428334726274\n",
      "Iteration: 1033 Loss: 0.0002775643654232882\n",
      "Iteration: 1034 Loss: 0.00027711647741649414\n",
      "Iteration: 1035 Loss: 0.0002766706018712296\n",
      "Iteration: 1036 Loss: 0.0002762267215258857\n",
      "Iteration: 1037 Loss: 0.0002757848193096911\n",
      "Iteration: 1038 Loss: 0.0002753448783408149\n",
      "Iteration: 1039 Loss: 0.00027490688192423194\n",
      "Iteration: 1040 Loss: 0.00027447081354809\n",
      "Iteration: 1041 Loss: 0.0002740366568828212\n",
      "Iteration: 1042 Loss: 0.000273604395777299\n",
      "Iteration: 1043 Loss: 0.0002731740142575364\n",
      "Iteration: 1044 Loss: 0.0002727454965242435\n",
      "Iteration: 1045 Loss: 0.0002723188269502387\n",
      "Iteration: 1046 Loss: 0.00027189399007812213\n",
      "Iteration: 1047 Loss: 0.0002714709706188346\n",
      "Iteration: 1048 Loss: 0.0002710497534483011\n",
      "Iteration: 1049 Loss: 0.0002706303236066737\n",
      "Iteration: 1050 Loss: 0.00027021266629534304\n",
      "Iteration: 1051 Loss: 0.00026979676687517056\n",
      "Iteration: 1052 Loss: 0.00026938261086398025\n",
      "Iteration: 1053 Loss: 0.00026897018393502053\n",
      "Iteration: 1054 Loss: 0.0002685594719152497\n",
      "Iteration: 1055 Loss: 0.0002681504607824427\n",
      "Iteration: 1056 Loss: 0.00026774313666426733\n",
      "Iteration: 1057 Loss: 0.00026733748583552755\n",
      "Iteration: 1058 Loss: 0.0002669334947168336\n",
      "Iteration: 1059 Loss: 0.00026653114987310527\n",
      "Iteration: 1060 Loss: 0.0002661304380105894\n",
      "Iteration: 1061 Loss: 0.00026573134597601\n",
      "Iteration: 1062 Loss: 0.00026533386075471927\n",
      "Iteration: 1063 Loss: 0.0002649379694682511\n",
      "Iteration: 1064 Loss: 0.00026454365937329664\n",
      "Iteration: 1065 Loss: 0.00026415091786032084\n",
      "Iteration: 1066 Loss: 0.00026375973245083487\n",
      "Iteration: 1067 Loss: 0.0002633700907963432\n",
      "Iteration: 1068 Loss: 0.00026298198067712536\n",
      "Iteration: 1069 Loss: 0.0002625953899998476\n",
      "Iteration: 1070 Loss: 0.0002622103067963049\n",
      "Iteration: 1071 Loss: 0.00026182671922203934\n",
      "Iteration: 1072 Loss: 0.0002614446155549119\n",
      "Iteration: 1073 Loss: 0.00026106398419263274\n",
      "Iteration: 1074 Loss: 0.0002606848136529297\n",
      "Iteration: 1075 Loss: 0.0002603070925705012\n",
      "Iteration: 1076 Loss: 0.0002599308096961375\n",
      "Iteration: 1077 Loss: 0.00025955595389564473\n",
      "Iteration: 1078 Loss: 0.0002591825141481066\n",
      "Iteration: 1079 Loss: 0.00025881047954491907\n",
      "Iteration: 1080 Loss: 0.00025843983928745134\n",
      "Iteration: 1081 Loss: 0.0002580705826869609\n",
      "Iteration: 1082 Loss: 0.00025770269916174007\n",
      "Iteration: 1083 Loss: 0.0002573361782376377\n",
      "Iteration: 1084 Loss: 0.00025697100954477387\n",
      "Iteration: 1085 Loss: 0.00025660718281835876\n",
      "Iteration: 1086 Loss: 0.00025624468789568923\n",
      "Iteration: 1087 Loss: 0.00025588351471614437\n",
      "Iteration: 1088 Loss: 0.0002555236533187617\n",
      "Iteration: 1089 Loss: 0.00025516509384259576\n",
      "Iteration: 1090 Loss: 0.00025480782652375435\n",
      "Iteration: 1091 Loss: 0.00025445184169555217\n",
      "Iteration: 1092 Loss: 0.00025409712978683363\n",
      "Iteration: 1093 Loss: 0.0002537436813211154\n",
      "Iteration: 1094 Loss: 0.0002533914869147478\n",
      "Iteration: 1095 Loss: 0.0002530405372771488\n",
      "Iteration: 1096 Loss: 0.0002526908232082867\n",
      "Iteration: 1097 Loss: 0.0002523423355985013\n",
      "Iteration: 1098 Loss: 0.0002519950654271543\n",
      "Iteration: 1099 Loss: 0.0002516490037611276\n",
      "Iteration: 1100 Loss: 0.00025130414175515546\n",
      "Iteration: 1101 Loss: 0.0002509604706490907\n",
      "Iteration: 1102 Loss: 0.00025061798176817464\n",
      "Iteration: 1103 Loss: 0.0002502766665213588\n",
      "Iteration: 1104 Loss: 0.0002499365164010003\n",
      "Iteration: 1105 Loss: 0.00024959752298111686\n",
      "Iteration: 1106 Loss: 0.00024925967791658544\n",
      "Iteration: 1107 Loss: 0.00024892297294281605\n",
      "Iteration: 1108 Loss: 0.0002485873998744535\n",
      "Iteration: 1109 Loss: 0.00024825295060469345\n",
      "Iteration: 1110 Loss: 0.00024791961710316515\n",
      "Iteration: 1111 Loss: 0.00024758739141663924\n",
      "Iteration: 1112 Loss: 0.0002472562656676397\n",
      "Iteration: 1113 Loss: 0.00024692623205391733\n",
      "Iteration: 1114 Loss: 0.00024659728284612183\n",
      "Iteration: 1115 Loss: 0.0002462694103886414\n",
      "Iteration: 1116 Loss: 0.00024594260709789065\n",
      "Iteration: 1117 Loss: 0.000245616865461985\n",
      "Iteration: 1118 Loss: 0.00024529217803939755\n",
      "Iteration: 1119 Loss: 0.0002449685374589622\n",
      "Iteration: 1120 Loss: 0.0002446459364181556\n",
      "Iteration: 1121 Loss: 0.00024432436768273113\n",
      "Iteration: 1122 Loss: 0.00024400382408624818\n",
      "Iteration: 1123 Loss: 0.00024368429852894522\n",
      "Iteration: 1124 Loss: 0.00024336578397713356\n",
      "Iteration: 1125 Loss: 0.00024304827346259268\n",
      "Iteration: 1126 Loss: 0.00024273176008152573\n",
      "Iteration: 1127 Loss: 0.0002424162369937372\n",
      "Iteration: 1128 Loss: 0.00024210169742300202\n",
      "Iteration: 1129 Loss: 0.00024178813465525675\n",
      "Iteration: 1130 Loss: 0.00024147554203753866\n",
      "Iteration: 1131 Loss: 0.00024116391297885115\n",
      "Iteration: 1132 Loss: 0.00024085324094831883\n",
      "Iteration: 1133 Loss: 0.0002405435194749982\n",
      "Iteration: 1134 Loss: 0.0002402347421472855\n",
      "Iteration: 1135 Loss: 0.0002399269026115549\n",
      "Iteration: 1136 Loss: 0.00023961999457244512\n",
      "Iteration: 1137 Loss: 0.00023931401179165833\n",
      "Iteration: 1138 Loss: 0.00023900894808793924\n",
      "Iteration: 1139 Loss: 0.0002387047973356744\n",
      "Iteration: 1140 Loss: 0.0002384015534648103\n",
      "Iteration: 1141 Loss: 0.0002380992104601544\n",
      "Iteration: 1142 Loss: 0.0002377977623604347\n",
      "Iteration: 1143 Loss: 0.0002374972032586459\n",
      "Iteration: 1144 Loss: 0.00023719752730046634\n",
      "Iteration: 1145 Loss: 0.00023689872868432946\n",
      "Iteration: 1146 Loss: 0.00023660080166027902\n",
      "Iteration: 1147 Loss: 0.00023630374053046113\n",
      "Iteration: 1148 Loss: 0.00023600753964714207\n",
      "Iteration: 1149 Loss: 0.00023571219341374076\n",
      "Iteration: 1150 Loss: 0.00023541769628304272\n",
      "Iteration: 1151 Loss: 0.00023512404275719917\n",
      "Iteration: 1152 Loss: 0.00023483122738740094\n",
      "Iteration: 1153 Loss: 0.00023453924477291027\n",
      "Iteration: 1154 Loss: 0.00023424808956058125\n",
      "Iteration: 1155 Loss: 0.00023395775644455592\n",
      "Iteration: 1156 Loss: 0.00023366824016648433\n",
      "Iteration: 1157 Loss: 0.00023337953551354646\n",
      "Iteration: 1158 Loss: 0.00023309163731907725\n",
      "Iteration: 1159 Loss: 0.00023280454046213804\n",
      "Iteration: 1160 Loss: 0.0002325182398658537\n",
      "Iteration: 1161 Loss: 0.00023223273049867847\n",
      "Iteration: 1162 Loss: 0.00023194800737266403\n",
      "Iteration: 1163 Loss: 0.00023166406554294436\n",
      "Iteration: 1164 Loss: 0.00023138090010857394\n",
      "Iteration: 1165 Loss: 0.0002310985062107566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1166 Loss: 0.00023081687903309667\n",
      "Iteration: 1167 Loss: 0.00023053601380108163\n",
      "Iteration: 1168 Loss: 0.0002302559057814464\n",
      "Iteration: 1169 Loss: 0.00022997655028229684\n",
      "Iteration: 1170 Loss: 0.00022969794265133658\n",
      "Iteration: 1171 Loss: 0.00022942007827741264\n",
      "Iteration: 1172 Loss: 0.00022914295258868314\n",
      "Iteration: 1173 Loss: 0.00022886656105263928\n",
      "Iteration: 1174 Loss: 0.00022859089917566798\n",
      "Iteration: 1175 Loss: 0.00022831596250289093\n",
      "Iteration: 1176 Loss: 0.00022804174661807082\n",
      "Iteration: 1177 Loss: 0.00022776824714194606\n",
      "Iteration: 1178 Loss: 0.00022749545973288024\n",
      "Iteration: 1179 Loss: 0.00022722338008679158\n",
      "Iteration: 1180 Loss: 0.00022695200393588752\n",
      "Iteration: 1181 Loss: 0.00022668132704861156\n",
      "Iteration: 1182 Loss: 0.0002264113452297152\n",
      "Iteration: 1183 Loss: 0.00022614205431940092\n",
      "Iteration: 1184 Loss: 0.00022587345019313431\n",
      "Iteration: 1185 Loss: 0.00022560552876206224\n",
      "Iteration: 1186 Loss: 0.00022533828597080108\n",
      "Iteration: 1187 Loss: 0.0002250717177987803\n",
      "Iteration: 1188 Loss: 0.00022480582025911273\n",
      "Iteration: 1189 Loss: 0.00022454058939905424\n",
      "Iteration: 1190 Loss: 0.00022427602129869258\n",
      "Iteration: 1191 Loss: 0.00022401211207107537\n",
      "Iteration: 1192 Loss: 0.00022374885786232598\n",
      "Iteration: 1193 Loss: 0.00022348625485071267\n",
      "Iteration: 1194 Loss: 0.00022322429924600027\n",
      "Iteration: 1195 Loss: 0.00022296298729030075\n",
      "Iteration: 1196 Loss: 0.00022270231525712878\n",
      "Iteration: 1197 Loss: 0.00022244227945058693\n",
      "Iteration: 1198 Loss: 0.00022218287620618464\n",
      "Iteration: 1199 Loss: 0.00022192410188999096\n",
      "Iteration: 1200 Loss: 0.00022166595289773922\n",
      "Iteration: 1201 Loss: 0.00022140842565557964\n",
      "Iteration: 1202 Loss: 0.00022115151661916018\n",
      "Iteration: 1203 Loss: 0.0002208952222733367\n",
      "Iteration: 1204 Loss: 0.00022063953913253052\n",
      "Iteration: 1205 Loss: 0.0002203844637396767\n",
      "Iteration: 1206 Loss: 0.0002201299926665861\n",
      "Iteration: 1207 Loss: 0.00021987612251340438\n",
      "Iteration: 1208 Loss: 0.0002196228499080454\n",
      "Iteration: 1209 Loss: 0.00021937017150619586\n",
      "Iteration: 1210 Loss: 0.00021911808399125224\n",
      "Iteration: 1211 Loss: 0.00021886658407388343\n",
      "Iteration: 1212 Loss: 0.00021861566849164314\n",
      "Iteration: 1213 Loss: 0.0002183653340086441\n",
      "Iteration: 1214 Loss: 0.00021811557741606733\n",
      "Iteration: 1215 Loss: 0.00021786639553129767\n",
      "Iteration: 1216 Loss: 0.00021761778519739376\n",
      "Iteration: 1217 Loss: 0.0002173697432835941\n",
      "Iteration: 1218 Loss: 0.00021712226668425317\n",
      "Iteration: 1219 Loss: 0.00021687535231975212\n",
      "Iteration: 1220 Loss: 0.0002166289971349373\n",
      "Iteration: 1221 Loss: 0.0002163831980997554\n",
      "Iteration: 1222 Loss: 0.00021613795220887702\n",
      "Iteration: 1223 Loss: 0.00021589325648125308\n",
      "Iteration: 1224 Loss: 0.0002156491079600276\n",
      "Iteration: 1225 Loss: 0.00021540550371246733\n",
      "Iteration: 1226 Loss: 0.00021516244082972068\n",
      "Iteration: 1227 Loss: 0.0002149199164262332\n",
      "Iteration: 1228 Loss: 0.0002146779276397764\n",
      "Iteration: 1229 Loss: 0.00021443647163167873\n",
      "Iteration: 1230 Loss: 0.00021419554558554056\n",
      "Iteration: 1231 Loss: 0.0002139551467082954\n",
      "Iteration: 1232 Loss: 0.00021371527222900424\n",
      "Iteration: 1233 Loss: 0.00021347591939941223\n",
      "Iteration: 1234 Loss: 0.00021323708549297402\n",
      "Iteration: 1235 Loss: 0.00021299876780524324\n",
      "Iteration: 1236 Loss: 0.00021276096365386646\n",
      "Iteration: 1237 Loss: 0.00021252367037739379\n",
      "Iteration: 1238 Loss: 0.0002122868853362357\n",
      "Iteration: 1239 Loss: 0.00021205060591137314\n",
      "Iteration: 1240 Loss: 0.0002118148295055692\n",
      "Iteration: 1241 Loss: 0.00021157955354138578\n",
      "Iteration: 1242 Loss: 0.0002113447754629784\n",
      "Iteration: 1243 Loss: 0.00021111049273467032\n",
      "Iteration: 1244 Loss: 0.00021087670284054437\n",
      "Iteration: 1245 Loss: 0.00021064340328546649\n",
      "Iteration: 1246 Loss: 0.00021041059159330642\n",
      "Iteration: 1247 Loss: 0.0002101782653085812\n",
      "Iteration: 1248 Loss: 0.00020994642199511225\n",
      "Iteration: 1249 Loss: 0.0002097150592354607\n",
      "Iteration: 1250 Loss: 0.00020948417463201077\n",
      "Iteration: 1251 Loss: 0.0002092537658059893\n",
      "Iteration: 1252 Loss: 0.0002090238303979544\n",
      "Iteration: 1253 Loss: 0.00020879436606644028\n",
      "Iteration: 1254 Loss: 0.00020856537048927288\n",
      "Iteration: 1255 Loss: 0.00020833684136227436\n",
      "Iteration: 1256 Loss: 0.0002081087763996507\n",
      "Iteration: 1257 Loss: 0.00020788117333332977\n",
      "Iteration: 1258 Loss: 0.00020765402991364517\n",
      "Iteration: 1259 Loss: 0.00020742734390862547\n",
      "Iteration: 1260 Loss: 0.00020720111310368515\n",
      "Iteration: 1261 Loss: 0.0002069753353018862\n",
      "Iteration: 1262 Loss: 0.00020675000832378742\n",
      "Iteration: 1263 Loss: 0.00020652513000676598\n",
      "Iteration: 1264 Loss: 0.0002063006982054702\n",
      "Iteration: 1265 Loss: 0.00020607671079142628\n",
      "Iteration: 1266 Loss: 0.00020585316565301145\n",
      "Iteration: 1267 Loss: 0.00020563006069502645\n",
      "Iteration: 1268 Loss: 0.0002054073938388984\n",
      "Iteration: 1269 Loss: 0.00020518516302195816\n",
      "Iteration: 1270 Loss: 0.0002049633661983812\n",
      "Iteration: 1271 Loss: 0.0002047420013381666\n",
      "Iteration: 1272 Loss: 0.0002045210664271299\n",
      "Iteration: 1273 Loss: 0.00020430055946683276\n",
      "Iteration: 1274 Loss: 0.00020408047847433544\n",
      "Iteration: 1275 Loss: 0.00020386082148303054\n",
      "Iteration: 1276 Loss: 0.00020364158654094815\n",
      "Iteration: 1277 Loss: 0.0002034227717116537\n",
      "Iteration: 1278 Loss: 0.0002032043750738654\n",
      "Iteration: 1279 Loss: 0.00020298639472096639\n",
      "Iteration: 1280 Loss: 0.00020276882876206135\n",
      "Iteration: 1281 Loss: 0.00020255167532040063\n",
      "Iteration: 1282 Loss: 0.00020233493253387272\n",
      "Iteration: 1283 Loss: 0.00020211859855505133\n",
      "Iteration: 1284 Loss: 0.0002019026715511396\n",
      "Iteration: 1285 Loss: 0.00020168714970307902\n",
      "Iteration: 1286 Loss: 0.00020147203120680319\n",
      "Iteration: 1287 Loss: 0.0002012573142714437\n",
      "Iteration: 1288 Loss: 0.00020104299712058103\n",
      "Iteration: 1289 Loss: 0.0002008290779918777\n",
      "Iteration: 1290 Loss: 0.00020061555513621398\n",
      "Iteration: 1291 Loss: 0.00020040242681797357\n",
      "Iteration: 1292 Loss: 0.0002001896913157009\n",
      "Iteration: 1293 Loss: 0.00019997734692065312\n",
      "Iteration: 1294 Loss: 0.0001997653919380759\n",
      "Iteration: 1295 Loss: 0.00019955382468562295\n",
      "Iteration: 1296 Loss: 0.00019934264349471233\n",
      "Iteration: 1297 Loss: 0.00019913184670912478\n",
      "Iteration: 1298 Loss: 0.0001989214326857944\n",
      "Iteration: 1299 Loss: 0.00019871139979458317\n",
      "Iteration: 1300 Loss: 0.0001985017464180871\n",
      "Iteration: 1301 Loss: 0.00019829247095097663\n",
      "Iteration: 1302 Loss: 0.00019808357180049331\n",
      "Iteration: 1303 Loss: 0.00019787504738638388\n",
      "Iteration: 1304 Loss: 0.0001976668961408896\n",
      "Iteration: 1305 Loss: 0.0001974591165077675\n",
      "Iteration: 1306 Loss: 0.00019725170694354427\n",
      "Iteration: 1307 Loss: 0.0001970446659162412\n",
      "Iteration: 1308 Loss: 0.00019683799190619216\n",
      "Iteration: 1309 Loss: 0.00019663168340527197\n",
      "Iteration: 1310 Loss: 0.00019642573891638965\n",
      "Iteration: 1311 Loss: 0.00019622015695541374\n",
      "Iteration: 1312 Loss: 0.0001960149360487986\n",
      "Iteration: 1313 Loss: 0.00019581007473461345\n",
      "Iteration: 1314 Loss: 0.00019560557156214156\n",
      "Iteration: 1315 Loss: 0.00019540142509207699\n",
      "Iteration: 1316 Loss: 0.00019519763389616216\n",
      "Iteration: 1317 Loss: 0.0001949941965572151\n",
      "Iteration: 1318 Loss: 0.0001947911116692406\n",
      "Iteration: 1319 Loss: 0.0001945883778365762\n",
      "Iteration: 1320 Loss: 0.00019438599367513235\n",
      "Iteration: 1321 Loss: 0.00019418395781083662\n",
      "Iteration: 1322 Loss: 0.00019398226888101466\n",
      "Iteration: 1323 Loss: 0.0001937809255328232\n",
      "Iteration: 1324 Loss: 0.00019357992642442308\n",
      "Iteration: 1325 Loss: 0.00019337927022403112\n",
      "Iteration: 1326 Loss: 0.0001931789556101282\n",
      "Iteration: 1327 Loss: 0.0001929789812720179\n",
      "Iteration: 1328 Loss: 0.00019277934590858259\n",
      "Iteration: 1329 Loss: 0.00019258004822872066\n",
      "Iteration: 1330 Loss: 0.00019238108695202654\n",
      "Iteration: 1331 Loss: 0.00019218246080736213\n",
      "Iteration: 1332 Loss: 0.00019198416853377642\n",
      "Iteration: 1333 Loss: 0.00019178620887975702\n",
      "Iteration: 1334 Loss: 0.00019158858060384007\n",
      "Iteration: 1335 Loss: 0.00019139128247422311\n",
      "Iteration: 1336 Loss: 0.00019119431326817502\n",
      "Iteration: 1337 Loss: 0.000190997671773613\n",
      "Iteration: 1338 Loss: 0.00019080135678629034\n",
      "Iteration: 1339 Loss: 0.0001906053671125875\n",
      "Iteration: 1340 Loss: 0.00019040970156735716\n",
      "Iteration: 1341 Loss: 0.00019021435897502364\n",
      "Iteration: 1342 Loss: 0.00019001933816935757\n",
      "Iteration: 1343 Loss: 0.0001898246379925896\n",
      "Iteration: 1344 Loss: 0.00018963025729648323\n",
      "Iteration: 1345 Loss: 0.00018943619494186123\n",
      "Iteration: 1346 Loss: 0.0001892424497978666\n",
      "Iteration: 1347 Loss: 0.00018904902074303606\n",
      "Iteration: 1348 Loss: 0.00018885590666391607\n",
      "Iteration: 1349 Loss: 0.00018866310645645535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1350 Loss: 0.00018847061902460546\n",
      "Iteration: 1351 Loss: 0.00018827844328148403\n",
      "Iteration: 1352 Loss: 0.0001880865781480228\n",
      "Iteration: 1353 Loss: 0.0001878950225544983\n",
      "Iteration: 1354 Loss: 0.00018770377543859512\n",
      "Iteration: 1355 Loss: 0.0001875128357469132\n",
      "Iteration: 1356 Loss: 0.00018732220243371448\n",
      "Iteration: 1357 Loss: 0.00018713187446243028\n",
      "Iteration: 1358 Loss: 0.0001869418508037604\n",
      "Iteration: 1359 Loss: 0.0001867521304367519\n",
      "Iteration: 1360 Loss: 0.00018656271234831833\n",
      "Iteration: 1361 Loss: 0.00018637359553352949\n",
      "Iteration: 1362 Loss: 0.0001861847789953154\n",
      "Iteration: 1363 Loss: 0.00018599626174460007\n",
      "Iteration: 1364 Loss: 0.00018580804279952082\n",
      "Iteration: 1365 Loss: 0.00018562012118650704\n",
      "Iteration: 1366 Loss: 0.0001854324959395392\n",
      "Iteration: 1367 Loss: 0.00018524516610027284\n",
      "Iteration: 1368 Loss: 0.00018505813071763568\n",
      "Iteration: 1369 Loss: 0.0001848713888481177\n",
      "Iteration: 1370 Loss: 0.00018468493955608537\n",
      "Iteration: 1371 Loss: 0.00018449878191273983\n",
      "Iteration: 1372 Loss: 0.00018431291499714636\n",
      "Iteration: 1373 Loss: 0.00018412733789537126\n",
      "Iteration: 1374 Loss: 0.00018394204970107315\n",
      "Iteration: 1375 Loss: 0.00018375704951449417\n",
      "Iteration: 1376 Loss: 0.00018357233644358437\n",
      "Iteration: 1377 Loss: 0.00018338790960326697\n",
      "Iteration: 1378 Loss: 0.00018320376811533016\n",
      "Iteration: 1379 Loss: 0.00018301991110871663\n",
      "Iteration: 1380 Loss: 0.00018283633771937034\n",
      "Iteration: 1381 Loss: 0.00018265304709029479\n",
      "Iteration: 1382 Loss: 0.00018247003837095407\n",
      "Iteration: 1383 Loss: 0.00018228731071788154\n",
      "Iteration: 1384 Loss: 0.00018210486329431387\n",
      "Iteration: 1385 Loss: 0.00018192269527039591\n",
      "Iteration: 1386 Loss: 0.0001817408058229953\n",
      "Iteration: 1387 Loss: 0.00018155919413502525\n",
      "Iteration: 1388 Loss: 0.00018137785939651907\n",
      "Iteration: 1389 Loss: 0.00018119680080388713\n",
      "Iteration: 1390 Loss: 0.00018101601756028552\n",
      "Iteration: 1391 Loss: 0.00018083550887504127\n",
      "Iteration: 1392 Loss: 0.00018065527396415615\n",
      "Iteration: 1393 Loss: 0.0001804753120497148\n",
      "Iteration: 1394 Loss: 0.00018029562236035387\n",
      "Iteration: 1395 Loss: 0.00018011620413097956\n",
      "Iteration: 1396 Loss: 0.00017993705660284944\n",
      "Iteration: 1397 Loss: 0.00017975817902312\n",
      "Iteration: 1398 Loss: 0.00017957957064535426\n",
      "Iteration: 1399 Loss: 0.00017940123072915384\n",
      "Iteration: 1400 Loss: 0.00017922315854043214\n",
      "Iteration: 1401 Loss: 0.00017904535335066353\n",
      "Iteration: 1402 Loss: 0.0001788678144377232\n",
      "Iteration: 1403 Loss: 0.00017869054108565088\n",
      "Iteration: 1404 Loss: 0.00017851353258415027\n",
      "Iteration: 1405 Loss: 0.00017833678822881327\n",
      "Iteration: 1406 Loss: 0.0001781603073210281\n",
      "Iteration: 1407 Loss: 0.00017798408916852785\n",
      "Iteration: 1408 Loss: 0.0001778081330838631\n",
      "Iteration: 1409 Loss: 0.0001776324383864055\n",
      "Iteration: 1410 Loss: 0.00017745700440062435\n",
      "Iteration: 1411 Loss: 0.0001772818304570019\n",
      "Iteration: 1412 Loss: 0.0001771069158914009\n",
      "Iteration: 1413 Loss: 0.0001769322600452851\n",
      "Iteration: 1414 Loss: 0.00017675786226590737\n",
      "Iteration: 1415 Loss: 0.00017658372190587103\n",
      "Iteration: 1416 Loss: 0.00017640983832334356\n",
      "Iteration: 1417 Loss: 0.00017623621088236352\n",
      "Iteration: 1418 Loss: 0.00017606283895200462\n",
      "Iteration: 1419 Loss: 0.00017588972190655112\n",
      "Iteration: 1420 Loss: 0.00017571685912610744\n",
      "Iteration: 1421 Loss: 0.00017554424999602507\n",
      "Iteration: 1422 Loss: 0.00017537189390716672\n",
      "Iteration: 1423 Loss: 0.00017519979025547351\n",
      "Iteration: 1424 Loss: 0.00017502793844199007\n",
      "Iteration: 1425 Loss: 0.00017485633787340477\n",
      "Iteration: 1426 Loss: 0.00017468498796129498\n",
      "Iteration: 1427 Loss: 0.0001745138881222699\n",
      "Iteration: 1428 Loss: 0.00017434303777840582\n",
      "Iteration: 1429 Loss: 0.00017417243635703362\n",
      "Iteration: 1430 Loss: 0.0001740020832898477\n",
      "Iteration: 1431 Loss: 0.00017383197801445387\n",
      "Iteration: 1432 Loss: 0.00017366211997347827\n",
      "Iteration: 1433 Loss: 0.00017349250861364073\n",
      "Iteration: 1434 Loss: 0.00017332314338729435\n",
      "Iteration: 1435 Loss: 0.0001731540237519368\n",
      "Iteration: 1436 Loss: 0.00017298514916927577\n",
      "Iteration: 1437 Loss: 0.00017281651910654974\n",
      "Iteration: 1438 Loss: 0.0001726481330357975\n",
      "Iteration: 1439 Loss: 0.00017247999043337758\n",
      "Iteration: 1440 Loss: 0.00017231209078112306\n",
      "Iteration: 1441 Loss: 0.00017214443356522555\n",
      "Iteration: 1442 Loss: 0.00017197701827698455\n",
      "Iteration: 1443 Loss: 0.00017180984441219165\n",
      "Iteration: 1444 Loss: 0.0001716429114718302\n",
      "Iteration: 1445 Loss: 0.00017147621896056072\n",
      "Iteration: 1446 Loss: 0.0001713097663887491\n",
      "Iteration: 1447 Loss: 0.00017114355327086373\n",
      "Iteration: 1448 Loss: 0.0001709775791262903\n",
      "Iteration: 1449 Loss: 0.0001708118434788866\n",
      "Iteration: 1450 Loss: 0.00017064634585708526\n",
      "Iteration: 1451 Loss: 0.0001704810857939869\n",
      "Iteration: 1452 Loss: 0.00017031606282693452\n",
      "Iteration: 1453 Loss: 0.0001701512764982073\n",
      "Iteration: 1454 Loss: 0.00016998672635438358\n",
      "Iteration: 1455 Loss: 0.0001698224119463457\n",
      "Iteration: 1456 Loss: 0.00016965833283008738\n",
      "Iteration: 1457 Loss: 0.00016949448856512633\n",
      "Iteration: 1458 Loss: 0.00016933087871599857\n",
      "Iteration: 1459 Loss: 0.0001691675028511785\n",
      "Iteration: 1460 Loss: 0.00016900436054414152\n",
      "Iteration: 1461 Loss: 0.0001688414513723017\n",
      "Iteration: 1462 Loss: 0.00016867877491751252\n",
      "Iteration: 1463 Loss: 0.00016851633076621697\n",
      "Iteration: 1464 Loss: 0.00016835411850873677\n",
      "Iteration: 1465 Loss: 0.00016819213773966866\n",
      "Iteration: 1466 Loss: 0.0001680303880582968\n",
      "Iteration: 1467 Loss: 0.00016786886906791447\n",
      "Iteration: 1468 Loss: 0.0001677075803760706\n",
      "Iteration: 1469 Loss: 0.00016754652159435289\n",
      "Iteration: 1470 Loss: 0.00016738569233920136\n",
      "Iteration: 1471 Loss: 0.00016722509223061582\n",
      "Iteration: 1472 Loss: 0.00016706472089277744\n",
      "Iteration: 1473 Loss: 0.00016690457795402028\n",
      "Iteration: 1474 Loss: 0.00016674466304750777\n",
      "Iteration: 1475 Loss: 0.0001665849758097631\n",
      "Iteration: 1476 Loss: 0.00016642551588192704\n",
      "Iteration: 1477 Loss: 0.00016626628290897818\n",
      "Iteration: 1478 Loss: 0.0001661072765397387\n",
      "Iteration: 1479 Loss: 0.00016594849642739403\n",
      "Iteration: 1480 Loss: 0.00016578994222955437\n",
      "Iteration: 1481 Loss: 0.00016563161360731355\n",
      "Iteration: 1482 Loss: 0.0001654735102258564\n",
      "Iteration: 1483 Loss: 0.00016531563175471977\n",
      "Iteration: 1484 Loss: 0.00016515797786713254\n",
      "Iteration: 1485 Loss: 0.00016500054824083462\n",
      "Iteration: 1486 Loss: 0.0001648433425571399\n",
      "Iteration: 1487 Loss: 0.0001646863605010922\n",
      "Iteration: 1488 Loss: 0.00016452960176222258\n",
      "Iteration: 1489 Loss: 0.00016437306603369418\n",
      "Iteration: 1490 Loss: 0.00016421675301285657\n",
      "Iteration: 1491 Loss: 0.0001640606624007363\n",
      "Iteration: 1492 Loss: 0.00016390479390287682\n",
      "Iteration: 1493 Loss: 0.00016374914722787083\n",
      "Iteration: 1494 Loss: 0.0001635937220888435\n",
      "Iteration: 1495 Loss: 0.00016343851820266623\n",
      "Iteration: 1496 Loss: 0.00016328353529013572\n",
      "Iteration: 1497 Loss: 0.00016312877307585947\n",
      "Iteration: 1498 Loss: 0.00016297423128835547\n",
      "Iteration: 1499 Loss: 0.00016281990966000067\n",
      "Iteration: 1500 Loss: 0.00016266580792703163\n",
      "Iteration: 1501 Loss: 0.00016251192582974463\n",
      "Iteration: 1502 Loss: 0.00016235826311222619\n",
      "Iteration: 1503 Loss: 0.0001622048195221788\n",
      "Iteration: 1504 Loss: 0.00016205159481121788\n",
      "Iteration: 1505 Loss: 0.00016189858873522515\n",
      "Iteration: 1506 Loss: 0.00016174580105356314\n",
      "Iteration: 1507 Loss: 0.00016159323152947243\n",
      "Iteration: 1508 Loss: 0.0001614408799298999\n",
      "Iteration: 1509 Loss: 0.00016128874602568967\n",
      "Iteration: 1510 Loss: 0.00016113682959158503\n",
      "Iteration: 1511 Loss: 0.00016098513040607102\n",
      "Iteration: 1512 Loss: 0.00016083364825182273\n",
      "Iteration: 1513 Loss: 0.0001606823829146512\n",
      "Iteration: 1514 Loss: 0.0001605313341847857\n",
      "Iteration: 1515 Loss: 0.00016038050185603184\n",
      "Iteration: 1516 Loss: 0.00016022988572618894\n",
      "Iteration: 1517 Loss: 0.00016007948559641327\n",
      "Iteration: 1518 Loss: 0.00015992930127204605\n",
      "Iteration: 1519 Loss: 0.00015977933256231805\n",
      "Iteration: 1520 Loss: 0.0001596295792800159\n",
      "Iteration: 1521 Loss: 0.00015948004124215464\n",
      "Iteration: 1522 Loss: 0.00015933071826905156\n",
      "Iteration: 1523 Loss: 0.00015918161018497527\n",
      "Iteration: 1524 Loss: 0.00015903271681825694\n",
      "Iteration: 1525 Loss: 0.00015888403800084523\n",
      "Iteration: 1526 Loss: 0.00015873557356877078\n",
      "Iteration: 1527 Loss: 0.00015858732336161764\n",
      "Iteration: 1528 Loss: 0.000158439287222779\n",
      "Iteration: 1529 Loss: 0.00015829146500030613\n",
      "Iteration: 1530 Loss: 0.0001581438565445775\n",
      "Iteration: 1531 Loss: 0.00015799646171141577\n",
      "Iteration: 1532 Loss: 0.00015784928035938209\n",
      "Iteration: 1533 Loss: 0.00015770231235148215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1534 Loss: 0.0001575555575543044\n",
      "Iteration: 1535 Loss: 0.00015740901583843412\n",
      "Iteration: 1536 Loss: 0.00015726268707869914\n",
      "Iteration: 1537 Loss: 0.00015711657115325396\n",
      "Iteration: 1538 Loss: 0.00015697066794420996\n",
      "Iteration: 1539 Loss: 0.00015682497733808656\n",
      "Iteration: 1540 Loss: 0.00015667949922488954\n",
      "Iteration: 1541 Loss: 0.000156534233498608\n",
      "Iteration: 1542 Loss: 0.00015638918005759635\n",
      "Iteration: 1543 Loss: 0.00015624433880360986\n",
      "Iteration: 1544 Loss: 0.00015609970964299463\n",
      "Iteration: 1545 Loss: 0.00015595529248546266\n",
      "Iteration: 1546 Loss: 0.00015581108724504537\n",
      "Iteration: 1547 Loss: 0.00015566709383991388\n",
      "Iteration: 1548 Loss: 0.00015552331219216526\n",
      "Iteration: 1549 Loss: 0.000155379742227992\n",
      "Iteration: 1550 Loss: 0.00015523638387757573\n",
      "Iteration: 1551 Loss: 0.00015509323707492507\n",
      "Iteration: 1552 Loss: 0.00015495030175846682\n",
      "Iteration: 1553 Loss: 0.00015480757787096698\n",
      "Iteration: 1554 Loss: 0.0001546650653590205\n",
      "Iteration: 1555 Loss: 0.00015452276417306864\n",
      "Iteration: 1556 Loss: 0.0001543806742680098\n",
      "Iteration: 1557 Loss: 0.0001542387956035371\n",
      "Iteration: 1558 Loss: 0.00015409712814278519\n",
      "Iteration: 1559 Loss: 0.00015395567185338987\n",
      "Iteration: 1560 Loss: 0.00015381442670730992\n",
      "Iteration: 1561 Loss: 0.00015367339268090055\n",
      "Iteration: 1562 Loss: 0.00015353256975441225\n",
      "Iteration: 1563 Loss: 0.0001533919579126669\n",
      "Iteration: 1564 Loss: 0.00015325155714499442\n",
      "Iteration: 1565 Loss: 0.00015311136744468836\n",
      "Iteration: 1566 Loss: 0.00015297138881035596\n",
      "Iteration: 1567 Loss: 0.00015283162124404435\n",
      "Iteration: 1568 Loss: 0.00015269206475274064\n",
      "Iteration: 1569 Loss: 0.00015255271934759316\n",
      "Iteration: 1570 Loss: 0.0001524135850453233\n",
      "Iteration: 1571 Loss: 0.0001522746618658845\n",
      "Iteration: 1572 Loss: 0.00015213594983456077\n",
      "Iteration: 1573 Loss: 0.0001519974489815844\n",
      "Iteration: 1574 Loss: 0.00015185915934114565\n",
      "Iteration: 1575 Loss: 0.00015172108095221694\n",
      "Iteration: 1576 Loss: 0.00015158321385893157\n",
      "Iteration: 1577 Loss: 0.00015144555811004301\n",
      "Iteration: 1578 Loss: 0.0001513081137589513\n",
      "Iteration: 1579 Loss: 0.00015117088086431506\n",
      "Iteration: 1580 Loss: 0.00015103385948939924\n",
      "Iteration: 1581 Loss: 0.0001508970497022381\n",
      "Iteration: 1582 Loss: 0.0001507604515762157\n",
      "Iteration: 1583 Loss: 0.00015062406518938408\n",
      "Iteration: 1584 Loss: 0.00015048789062503524\n",
      "Iteration: 1585 Loss: 0.0001503519279718606\n",
      "Iteration: 1586 Loss: 0.00015021617732343754\n",
      "Iteration: 1587 Loss: 0.0001500806387787455\n",
      "Iteration: 1588 Loss: 0.00014994531244176594\n",
      "Iteration: 1589 Loss: 0.00014981019842181585\n",
      "Iteration: 1590 Loss: 0.00014967529683399377\n",
      "Iteration: 1591 Loss: 0.00014954060779837606\n",
      "Iteration: 1592 Loss: 0.00014940613144081265\n",
      "Iteration: 1593 Loss: 0.00014927186789249154\n",
      "Iteration: 1594 Loss: 0.00014913781729022931\n",
      "Iteration: 1595 Loss: 0.00014900397977683118\n",
      "Iteration: 1596 Loss: 0.00014887035550031005\n",
      "Iteration: 1597 Loss: 0.0001487369446146096\n",
      "Iteration: 1598 Loss: 0.00014860374727948943\n",
      "Iteration: 1599 Loss: 0.00014847076366097096\n",
      "Iteration: 1600 Loss: 0.0001483379939306588\n",
      "Iteration: 1601 Loss: 0.00014820543826617836\n",
      "Iteration: 1602 Loss: 0.00014807309685178205\n",
      "Iteration: 1603 Loss: 0.00014794096987731631\n",
      "Iteration: 1604 Loss: 0.0001478090575392663\n",
      "Iteration: 1605 Loss: 0.0001476773600403999\n",
      "Iteration: 1606 Loss: 0.00014754587758981078\n",
      "Iteration: 1607 Loss: 0.0001474146104028772\n",
      "Iteration: 1608 Loss: 0.00014728355870174636\n",
      "Iteration: 1609 Loss: 0.0001471527227155491\n",
      "Iteration: 1610 Loss: 0.00014702210267964486\n",
      "Iteration: 1611 Loss: 0.000146891698836581\n",
      "Iteration: 1612 Loss: 0.00014676151143513293\n",
      "Iteration: 1613 Loss: 0.00014663154073188035\n",
      "Iteration: 1614 Loss: 0.00014650178699035014\n",
      "Iteration: 1615 Loss: 0.00014637225048105717\n",
      "Iteration: 1616 Loss: 0.00014624293148181273\n",
      "Iteration: 1617 Loss: 0.00014611383027759406\n",
      "Iteration: 1618 Loss: 0.0001459849471615774\n",
      "Iteration: 1619 Loss: 0.00014585628243362942\n",
      "Iteration: 1620 Loss: 0.00014572783640198514\n",
      "Iteration: 1621 Loss: 0.00014559960938273218\n",
      "Iteration: 1622 Loss: 0.0001454716016991298\n",
      "Iteration: 1623 Loss: 0.00014534381368316786\n",
      "Iteration: 1624 Loss: 0.0001452162456745325\n",
      "Iteration: 1625 Loss: 0.00014508889802150457\n",
      "Iteration: 1626 Loss: 0.00014496177108061888\n",
      "Iteration: 1627 Loss: 0.00014483486521686486\n",
      "Iteration: 1628 Loss: 0.00014470818080354394\n",
      "Iteration: 1629 Loss: 0.0001445817182234345\n",
      "Iteration: 1630 Loss: 0.00014445547786761493\n",
      "Iteration: 1631 Loss: 0.0001443294601363949\n",
      "Iteration: 1632 Loss: 0.00014420366543933019\n",
      "Iteration: 1633 Loss: 0.00014407809419486717\n",
      "Iteration: 1634 Loss: 0.0001439527468314192\n",
      "Iteration: 1635 Loss: 0.0001438276237865022\n",
      "Iteration: 1636 Loss: 0.0001437027255075795\n",
      "Iteration: 1637 Loss: 0.0001435780524521344\n",
      "Iteration: 1638 Loss: 0.0001434536050875696\n",
      "Iteration: 1639 Loss: 0.00014332938389179927\n",
      "Iteration: 1640 Loss: 0.00014320538935215916\n",
      "Iteration: 1641 Loss: 0.00014308162196737925\n",
      "Iteration: 1642 Loss: 0.0001429580822463347\n",
      "Iteration: 1643 Loss: 0.00014283477070900453\n",
      "Iteration: 1644 Loss: 0.0001427116878868205\n",
      "Iteration: 1645 Loss: 0.0001425888343217489\n",
      "Iteration: 1646 Loss: 0.00014246621056704422\n",
      "Iteration: 1647 Loss: 0.00014234381718829532\n",
      "Iteration: 1648 Loss: 0.000142221654762713\n",
      "Iteration: 1649 Loss: 0.00014209972387862308\n",
      "Iteration: 1650 Loss: 0.00014197802513753085\n",
      "Iteration: 1651 Loss: 0.00014185655915253535\n",
      "Iteration: 1652 Loss: 0.0001417353265498593\n",
      "Iteration: 1653 Loss: 0.00014161432796831657\n",
      "Iteration: 1654 Loss: 0.00014149356405933027\n",
      "Iteration: 1655 Loss: 0.0001413730354882021\n",
      "Iteration: 1656 Loss: 0.00014125274293337681\n",
      "Iteration: 1657 Loss: 0.00014113268708671384\n",
      "Iteration: 1658 Loss: 0.00014101286865411664\n",
      "Iteration: 1659 Loss: 0.0001408932883556585\n",
      "Iteration: 1660 Loss: 0.00014077394692572705\n",
      "Iteration: 1661 Loss: 0.00014065484511277272\n",
      "Iteration: 1662 Loss: 0.00014053598368090033\n",
      "Iteration: 1663 Loss: 0.00014041736340926285\n",
      "Iteration: 1664 Loss: 0.0001402989850917651\n",
      "Iteration: 1665 Loss: 0.00014018084953838653\n",
      "Iteration: 1666 Loss: 0.00014006295757463633\n",
      "Iteration: 1667 Loss: 0.00013994531004247798\n",
      "Iteration: 1668 Loss: 0.00013982790779997267\n",
      "Iteration: 1669 Loss: 0.0001397107517220331\n",
      "Iteration: 1670 Loss: 0.00013959384270078557\n",
      "Iteration: 1671 Loss: 0.0001394771816450269\n",
      "Iteration: 1672 Loss: 0.00013936076948194392\n",
      "Iteration: 1673 Loss: 0.00013924460715582246\n",
      "Iteration: 1674 Loss: 0.00013912869562917964\n",
      "Iteration: 1675 Loss: 0.0001390130358832679\n",
      "Iteration: 1676 Loss: 0.00013889762891783686\n",
      "Iteration: 1677 Loss: 0.0001387824757519031\n",
      "Iteration: 1678 Loss: 0.0001386675774238182\n",
      "Iteration: 1679 Loss: 0.0001385529349915332\n",
      "Iteration: 1680 Loss: 0.00013843854953352947\n",
      "Iteration: 1681 Loss: 0.00013832442214804406\n",
      "Iteration: 1682 Loss: 0.00013821055395429841\n",
      "Iteration: 1683 Loss: 0.00013809694609267722\n",
      "Iteration: 1684 Loss: 0.0001379835997245197\n",
      "Iteration: 1685 Loss: 0.0001378705160333978\n",
      "Iteration: 1686 Loss: 0.0001377576962249929\n",
      "Iteration: 1687 Loss: 0.00013764514152709266\n",
      "Iteration: 1688 Loss: 0.0001375328531904205\n",
      "Iteration: 1689 Loss: 0.00013742083248867917\n",
      "Iteration: 1690 Loss: 0.00013730908071945465\n",
      "Iteration: 1691 Loss: 0.00013719759920436734\n",
      "Iteration: 1692 Loss: 0.0001370863892886968\n",
      "Iteration: 1693 Loss: 0.0001369754523429144\n",
      "Iteration: 1694 Loss: 0.00013686478976222108\n",
      "Iteration: 1695 Loss: 0.00013675440296748645\n",
      "Iteration: 1696 Loss: 0.00013666555177379731\n",
      "Iteration: 1697 Loss: 0.00013659529885952961\n",
      "Iteration: 1698 Loss: 0.00013652528908556313\n",
      "Iteration: 1699 Loss: 0.00013645552401180317\n",
      "Iteration: 1700 Loss: 0.00013638600522551458\n",
      "Iteration: 1701 Loss: 0.0001363167343417504\n",
      "Iteration: 1702 Loss: 0.00013624771300370354\n",
      "Iteration: 1703 Loss: 0.00013617894288275252\n",
      "Iteration: 1704 Loss: 0.00013611042567913186\n",
      "Iteration: 1705 Loss: 0.00013604216312242066\n",
      "Iteration: 1706 Loss: 0.00013597415697185858\n",
      "Iteration: 1707 Loss: 0.00013592507697581581\n",
      "Iteration: 1708 Loss: 0.00013589908218868707\n",
      "Iteration: 1709 Loss: 0.00013587331062041516\n",
      "Iteration: 1710 Loss: 0.00013584776418753994\n",
      "Iteration: 1711 Loss: 0.00013582244483789145\n",
      "Iteration: 1712 Loss: 0.00013579735455058076\n",
      "Iteration: 1713 Loss: 0.00013577249533667035\n",
      "Iteration: 1714 Loss: 0.00013574786924025508\n",
      "Iteration: 1715 Loss: 0.0001357234783371531\n",
      "Iteration: 1716 Loss: 0.0001356993247373984\n",
      "Iteration: 1717 Loss: 0.00013567541058323004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1718 Loss: 0.00013565173805113397\n",
      "Iteration: 1719 Loss: 0.0001356283093510718\n",
      "Iteration: 1720 Loss: 0.00013560512672732832\n",
      "Iteration: 1721 Loss: 0.0001355821924586896\n",
      "Iteration: 1722 Loss: 0.00013555950885816567\n",
      "Iteration: 1723 Loss: 0.0001355370782738058\n",
      "Iteration: 1724 Loss: 0.00013551490308838892\n",
      "Iteration: 1725 Loss: 0.00013549298571979344\n",
      "Iteration: 1726 Loss: 0.00013547132862151348\n",
      "Iteration: 1727 Loss: 0.0001354499342818884\n",
      "Iteration: 1728 Loss: 0.0001354288052254615\n",
      "Iteration: 1729 Loss: 0.00013540794401160103\n",
      "Iteration: 1730 Loss: 0.00013538735323551955\n",
      "Iteration: 1731 Loss: 0.0001353670355282038\n",
      "Iteration: 1732 Loss: 0.0001353469935555798\n",
      "Iteration: 1733 Loss: 0.00013532723001967976\n",
      "Iteration: 1734 Loss: 0.00013530774765715855\n",
      "Iteration: 1735 Loss: 0.00013528854924049345\n",
      "Iteration: 1736 Loss: 0.00013526963757633578\n",
      "Iteration: 1737 Loss: 0.00013525101550681404\n",
      "Iteration: 1738 Loss: 0.00013523268590806878\n",
      "Iteration: 1739 Loss: 0.00013521465169092717\n",
      "Iteration: 1740 Loss: 0.00013519691579922318\n",
      "Iteration: 1741 Loss: 0.000135179481210127\n",
      "Iteration: 1742 Loss: 0.00013516235093447257\n",
      "Iteration: 1743 Loss: 0.00013514552801429245\n",
      "Iteration: 1744 Loss: 0.00013512901552380395\n",
      "Iteration: 1745 Loss: 0.000135112816568053\n",
      "Iteration: 1746 Loss: 0.00013509693428228898\n",
      "Iteration: 1747 Loss: 0.00013508137183132908\n",
      "Iteration: 1748 Loss: 0.00013506613240808706\n",
      "Iteration: 1749 Loss: 0.0001350512192333565\n",
      "Iteration: 1750 Loss: 0.0001350366355542393\n",
      "Iteration: 1751 Loss: 0.0001350223846434897\n",
      "Iteration: 1752 Loss: 0.00013500846979774684\n",
      "Iteration: 1753 Loss: 0.00013499489433679504\n",
      "Iteration: 1754 Loss: 0.0001349816616021356\n",
      "Iteration: 1755 Loss: 0.00013496877495490712\n",
      "Iteration: 1756 Loss: 0.0001349562377754605\n",
      "Iteration: 1757 Loss: 0.0001349440534608777\n",
      "Iteration: 1758 Loss: 0.0001349322254235191\n",
      "Iteration: 1759 Loss: 0.0001349207570889036\n",
      "Iteration: 1760 Loss: 0.0001349096518947905\n",
      "Iteration: 1761 Loss: 0.00013489891328729968\n",
      "Iteration: 1762 Loss: 0.0001348885447214407\n",
      "Iteration: 1763 Loss: 0.00013487854965668977\n",
      "Iteration: 1764 Loss: 0.0001348689315554664\n",
      "Iteration: 1765 Loss: 0.00013485969388037585\n",
      "Iteration: 1766 Loss: 0.00013485084009284135\n",
      "Iteration: 1767 Loss: 0.00013484237364928502\n",
      "Iteration: 1768 Loss: 0.0001348342979990774\n",
      "Iteration: 1769 Loss: 0.0001348266165814842\n",
      "Iteration: 1770 Loss: 0.00013481933282273397\n",
      "Iteration: 1771 Loss: 0.00013481245013341364\n",
      "Iteration: 1772 Loss: 0.00013480597190463218\n",
      "Iteration: 1773 Loss: 0.00013479990150621433\n",
      "Iteration: 1774 Loss: 0.00013479424228195915\n",
      "Iteration: 1775 Loss: 0.0001347889975470262\n",
      "Iteration: 1776 Loss: 0.0001347841705845951\n",
      "Iteration: 1777 Loss: 0.0001347797646422266\n",
      "Iteration: 1778 Loss: 0.00013477578292824379\n",
      "Iteration: 1779 Loss: 0.00013477222860841103\n",
      "Iteration: 1780 Loss: 0.00013476910480230802\n",
      "Iteration: 1781 Loss: 0.00013476641457891894\n",
      "Iteration: 1782 Loss: 0.00013476416095319004\n",
      "Iteration: 1783 Loss: 0.00013476234688272683\n",
      "Iteration: 1784 Loss: 0.00013476097526336222\n",
      "Iteration: 1785 Loss: 0.00013476004892581678\n",
      "Iteration: 1786 Loss: 0.00013475957063118692\n",
      "Iteration: 1787 Loss: 0.00013475954306752118\n",
      "Iteration: 1788 Loss: 0.00013475996884616167\n",
      "Iteration: 1789 Loss: 0.00013476085049709605\n",
      "Iteration: 1790 Loss: 0.0001347621904657111\n",
      "Iteration: 1791 Loss: 0.0001347639911092252\n",
      "Iteration: 1792 Loss: 0.00013476625469259392\n",
      "Iteration: 1793 Loss: 0.00013476898338398588\n",
      "Iteration: 1794 Loss: 0.00013477217925287154\n",
      "Iteration: 1795 Loss: 0.00013477584426549268\n",
      "Iteration: 1796 Loss: 0.00013477998028127582\n",
      "Iteration: 1797 Loss: 0.00013478458904983985\n",
      "Iteration: 1798 Loss: 0.00013478967220791594\n",
      "Iteration: 1799 Loss: 0.00013479523127535022\n",
      "Iteration: 1800 Loss: 0.0001348012676533035\n",
      "Iteration: 1801 Loss: 0.00013480778262044498\n",
      "Iteration: 1802 Loss: 0.00013481477733087518\n",
      "Iteration: 1803 Loss: 0.00013482225281136808\n",
      "Iteration: 1804 Loss: 0.0001348302099587747\n",
      "Iteration: 1805 Loss: 0.00013483864953817373\n",
      "Iteration: 1806 Loss: 0.00013484757218092634\n",
      "Iteration: 1807 Loss: 0.00013166393384899666\n",
      "Iteration: 1808 Loss: 0.00012455278717458785\n",
      "Iteration: 1809 Loss: 0.00011459337730658465\n",
      "Iteration: 1810 Loss: 0.00010276179609639233\n",
      "Iteration: 1811 Loss: 8.992138889045541e-05\n",
      "Iteration: 1812 Loss: 7.681569516307754e-05\n",
      "Iteration: 1813 Loss: 6.407358327993625e-05\n",
      "Iteration: 1814 Loss: 5.22326498228977e-05\n",
      "Iteration: 1815 Loss: 4.180285967172255e-05\n",
      "Iteration: 1816 Loss: 3.343723345263936e-05\n",
      "Iteration: 1817 Loss: 2.822817165850845e-05\n",
      "Iteration: 1818 Loss: 2.7043346834506392e-05\n",
      "Iteration: 1819 Loss: 2.8835258617116604e-05\n",
      "Iteration: 1820 Loss: 3.144873482533207e-05\n",
      "Iteration: 1821 Loss: 3.3592124623098326e-05\n",
      "Iteration: 1822 Loss: 3.4946976393371964e-05\n",
      "Iteration: 1823 Loss: 3.551759066081141e-05\n",
      "Iteration: 1824 Loss: 3.540985527691434e-05\n",
      "Iteration: 1825 Loss: 3.476529649228738e-05\n",
      "Iteration: 1826 Loss: 3.373561893250921e-05\n",
      "Iteration: 1827 Loss: 3.247040347842621e-05\n",
      "Iteration: 1828 Loss: 3.1109916571203745e-05\n",
      "Iteration: 1829 Loss: 2.9779666348262745e-05\n",
      "Iteration: 1830 Loss: 2.8584627881982714e-05\n",
      "Iteration: 1831 Loss: 2.760197471102597e-05\n",
      "Iteration: 1832 Loss: 2.6873159930758536e-05\n",
      "Iteration: 1833 Loss: 2.6399308660147804e-05\n",
      "Iteration: 1834 Loss: 2.6145243509136633e-05\n",
      "Iteration: 1835 Loss: 2.60533576145677e-05\n",
      "Iteration: 1836 Loss: 2.6061392314529116e-05\n",
      "Iteration: 1837 Loss: 2.6116069144130812e-05\n",
      "Iteration: 1838 Loss: 2.617918346896033e-05\n",
      "Iteration: 1839 Loss: 2.6227745555472043e-05\n",
      "Iteration: 1840 Loss: 2.625116786253172e-05\n",
      "Iteration: 1841 Loss: 2.6247657554340077e-05\n",
      "Iteration: 1842 Loss: 2.622086891953867e-05\n",
      "Iteration: 1843 Loss: 2.6177191458283628e-05\n",
      "Iteration: 1844 Loss: 2.6123742592288012e-05\n",
      "Iteration: 1845 Loss: 2.606701949394737e-05\n",
      "Iteration: 1846 Loss: 2.6012124362466405e-05\n",
      "Iteration: 1847 Loss: 2.5962458643270287e-05\n",
      "Iteration: 1848 Loss: 2.5919768995131526e-05\n",
      "Iteration: 1849 Loss: 2.5884422481344554e-05\n",
      "Iteration: 1850 Loss: 2.5855795079962594e-05\n",
      "Iteration: 1851 Loss: 2.5832677252599263e-05\n",
      "Iteration: 1852 Loss: 2.581362898238143e-05\n",
      "Iteration: 1853 Loss: 2.5797247571080722e-05\n",
      "Iteration: 1854 Loss: 2.578233820136972e-05\n",
      "Iteration: 1855 Loss: 2.5767996186854225e-05\n",
      "Iteration: 1856 Loss: 2.5753620253926046e-05\n",
      "Iteration: 1857 Loss: 2.5738879553519894e-05\n",
      "Iteration: 1858 Loss: 2.5723655619417514e-05\n",
      "Iteration: 1859 Loss: 2.5707976360906373e-05\n",
      "Iteration: 1860 Loss: 2.5691954101145544e-05\n",
      "Iteration: 1861 Loss: 2.5675734769423447e-05\n",
      "Iteration: 1862 Loss: 2.565946125112042e-05\n",
      "Iteration: 1863 Loss: 2.5643250866260015e-05\n",
      "Iteration: 1864 Loss: 2.5627184995680725e-05\n",
      "Iteration: 1865 Loss: 2.5611307908139482e-05\n",
      "Iteration: 1866 Loss: 2.5595631602877087e-05\n",
      "Iteration: 1867 Loss: 2.5580143792483094e-05\n",
      "Iteration: 1868 Loss: 2.556481673525387e-05\n",
      "Iteration: 1869 Loss: 2.5549615338441776e-05\n",
      "Iteration: 1870 Loss: 2.5534503618665488e-05\n",
      "Iteration: 1871 Loss: 2.5519449172775798e-05\n",
      "Iteration: 1872 Loss: 2.550442571892243e-05\n",
      "Iteration: 1873 Loss: 2.5489414022004725e-05\n",
      "Iteration: 1874 Loss: 2.5474401630331684e-05\n",
      "Iteration: 1875 Loss: 2.545938187017911e-05\n",
      "Iteration: 1876 Loss: 2.5444352490028847e-05\n",
      "Iteration: 1877 Loss: 2.542931425064344e-05\n",
      "Iteration: 1878 Loss: 2.541426966800616e-05\n",
      "Iteration: 1879 Loss: 2.5399222015145155e-05\n",
      "Iteration: 1880 Loss: 2.5384174614540806e-05\n",
      "Iteration: 1881 Loss: 2.536913041130584e-05\n",
      "Iteration: 1882 Loss: 2.535409176932441e-05\n",
      "Iteration: 1883 Loss: 2.533906043640507e-05\n",
      "Iteration: 1884 Loss: 2.5324037610312448e-05\n",
      "Iteration: 1885 Loss: 2.530902405872658e-05\n",
      "Iteration: 1886 Loss: 2.5294020249557438e-05\n",
      "Iteration: 1887 Loss: 2.5279026468839327e-05\n",
      "Iteration: 1888 Loss: 2.526404290974192e-05\n",
      "Iteration: 1889 Loss: 2.5249069731314556e-05\n",
      "Iteration: 1890 Loss: 2.5234107088665527e-05\n",
      "Iteration: 1891 Loss: 2.5219155141580006e-05\n",
      "Iteration: 1892 Loss: 2.5204214047191512e-05\n",
      "Iteration: 1893 Loss: 2.518928394720118e-05\n",
      "Iteration: 1894 Loss: 2.5174364952653086e-05\n",
      "Iteration: 1895 Loss: 2.5159457131307286e-05\n",
      "Iteration: 1896 Loss: 2.5144560501565173e-05\n",
      "Iteration: 1897 Loss: 2.512967502978413e-05\n",
      "Iteration: 1898 Loss: 2.511480063470377e-05\n",
      "Iteration: 1899 Loss: 2.5099937194235146e-05\n",
      "Iteration: 1900 Loss: 2.5085084555846633e-05\n",
      "Iteration: 1901 Loss: 2.5070242547480034e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1902 Loss: 2.505541098860226e-05\n",
      "Iteration: 1903 Loss: 2.5040589699113282e-05\n",
      "Iteration: 1904 Loss: 2.502577850834496e-05\n",
      "Iteration: 1905 Loss: 2.501097725946995e-05\n",
      "Iteration: 1906 Loss: 2.4996185814583263e-05\n",
      "Iteration: 1907 Loss: 2.4981404055770237e-05\n",
      "Iteration: 1908 Loss: 2.4966631886085012e-05\n",
      "Iteration: 1909 Loss: 2.4951869227919018e-05\n",
      "Iteration: 1910 Loss: 2.4937116022773778e-05\n",
      "Iteration: 1911 Loss: 2.4922372227347415e-05\n",
      "Iteration: 1912 Loss: 2.490763781237816e-05\n",
      "Iteration: 1913 Loss: 2.489291275936652e-05\n",
      "Iteration: 1914 Loss: 2.487819705908188e-05\n",
      "Iteration: 1915 Loss: 2.486349070804726e-05\n",
      "Iteration: 1916 Loss: 2.484879370744003e-05\n",
      "Iteration: 1917 Loss: 2.483410606090213e-05\n",
      "Iteration: 1918 Loss: 2.4819427774170616e-05\n",
      "Iteration: 1919 Loss: 2.4804758852019144e-05\n",
      "Iteration: 1920 Loss: 2.4790099299496666e-05\n",
      "Iteration: 1921 Loss: 2.477544911954245e-05\n",
      "Iteration: 1922 Loss: 2.476080831331733e-05\n",
      "Iteration: 1923 Loss: 2.4746176879645323e-05\n",
      "Iteration: 1924 Loss: 2.4731554814757814e-05\n",
      "Iteration: 1925 Loss: 2.4716942112799366e-05\n",
      "Iteration: 1926 Loss: 2.4702338765292858e-05\n",
      "Iteration: 1927 Loss: 2.4687744761720698e-05\n",
      "Iteration: 1928 Loss: 2.4673160089093013e-05\n",
      "Iteration: 1929 Loss: 2.4658584732948642e-05\n",
      "Iteration: 1930 Loss: 2.4644018676961185e-05\n",
      "Iteration: 1931 Loss: 2.4629461904542102e-05\n",
      "Iteration: 1932 Loss: 2.4614914396864164e-05\n",
      "Iteration: 1933 Loss: 2.460037613528583e-05\n",
      "Iteration: 1934 Loss: 2.458584710041396e-05\n",
      "Iteration: 1935 Loss: 2.457132727248385e-05\n",
      "Iteration: 1936 Loss: 2.4556816632092315e-05\n",
      "Iteration: 1937 Loss: 2.45423151599164e-05\n",
      "Iteration: 1938 Loss: 2.452782283635217e-05\n",
      "Iteration: 1939 Loss: 2.4513339642854847e-05\n",
      "Iteration: 1940 Loss: 2.4498865560958036e-05\n",
      "Iteration: 1941 Loss: 2.4484400572485685e-05\n",
      "Iteration: 1942 Loss: 2.4469944660289438e-05\n",
      "Iteration: 1943 Loss: 2.445549780689755e-05\n",
      "Iteration: 1944 Loss: 2.4441059995921553e-05\n",
      "Iteration: 1945 Loss: 2.4426631211200614e-05\n",
      "Iteration: 1946 Loss: 2.4412211436756212e-05\n",
      "Iteration: 1947 Loss: 2.4397800656961375e-05\n",
      "Iteration: 1948 Loss: 2.4383398857080302e-05\n",
      "Iteration: 1949 Loss: 2.436900602197754e-05\n",
      "Iteration: 1950 Loss: 2.435462213673584e-05\n",
      "Iteration: 1951 Loss: 2.4340247187096313e-05\n",
      "Iteration: 1952 Loss: 2.4325881158343585e-05\n",
      "Iteration: 1953 Loss: 2.4311524036390367e-05\n",
      "Iteration: 1954 Loss: 2.429717580709087e-05\n",
      "Iteration: 1955 Loss: 2.4282836456080858e-05\n",
      "Iteration: 1956 Loss: 2.426850596938718e-05\n",
      "Iteration: 1957 Loss: 2.425418433282666e-05\n",
      "Iteration: 1958 Loss: 2.423987153243747e-05\n",
      "Iteration: 1959 Loss: 2.4225567554246584e-05\n",
      "Iteration: 1960 Loss: 2.42112723844278e-05\n",
      "Iteration: 1961 Loss: 2.419698600883459e-05\n",
      "Iteration: 1962 Loss: 2.4182708413597016e-05\n",
      "Iteration: 1963 Loss: 2.416843958476613e-05\n",
      "Iteration: 1964 Loss: 2.4154179508707415e-05\n",
      "Iteration: 1965 Loss: 2.4139928171275343e-05\n",
      "Iteration: 1966 Loss: 2.4125685558900535e-05\n",
      "Iteration: 1967 Loss: 2.4111451657916823e-05\n",
      "Iteration: 1968 Loss: 2.4097226454784415e-05\n",
      "Iteration: 1969 Loss: 2.4083009935664033e-05\n",
      "Iteration: 1970 Loss: 2.4068802087272584e-05\n",
      "Iteration: 1971 Loss: 2.4054602896022745e-05\n",
      "Iteration: 1972 Loss: 2.404041234869215e-05\n",
      "Iteration: 1973 Loss: 2.4026230431692797e-05\n",
      "Iteration: 1974 Loss: 2.4012057131983968e-05\n",
      "Iteration: 1975 Loss: 2.3997892436371053e-05\n",
      "Iteration: 1976 Loss: 2.3983736331966683e-05\n",
      "Iteration: 1977 Loss: 2.3969588805765382e-05\n",
      "Iteration: 1978 Loss: 2.3955449844698665e-05\n",
      "Iteration: 1979 Loss: 2.3941319435948253e-05\n",
      "Iteration: 1980 Loss: 2.3927197566888627e-05\n",
      "Iteration: 1981 Loss: 2.391308422470324e-05\n",
      "Iteration: 1982 Loss: 2.3898979396757143e-05\n",
      "Iteration: 1983 Loss: 2.388488307054803e-05\n",
      "Iteration: 1984 Loss: 2.387079523345674e-05\n",
      "Iteration: 1985 Loss: 2.3856715873188866e-05\n",
      "Iteration: 1986 Loss: 2.3842644977513217e-05\n",
      "Iteration: 1987 Loss: 2.3828582534026113e-05\n",
      "Iteration: 1988 Loss: 2.3814528530714423e-05\n",
      "Iteration: 1989 Loss: 2.3800482955126445e-05\n",
      "Iteration: 1990 Loss: 2.378644579556163e-05\n",
      "Iteration: 1991 Loss: 2.3772417039666105e-05\n",
      "Iteration: 1992 Loss: 2.37583966757169e-05\n",
      "Iteration: 1993 Loss: 2.3744384691558192e-05\n",
      "Iteration: 1994 Loss: 2.3730381075639467e-05\n",
      "Iteration: 1995 Loss: 2.3716385816122538e-05\n",
      "Iteration: 1996 Loss: 2.3702398900947275e-05\n",
      "Iteration: 1997 Loss: 2.368842031899341e-05\n",
      "Iteration: 1998 Loss: 2.3674450058125385e-05\n",
      "Iteration: 1999 Loss: 2.366048810721096e-05\n",
      "Iteration: 2000 Loss: 2.3646534454447035e-05\n",
      "Iteration: 2001 Loss: 2.3632589088715142e-05\n",
      "Iteration: 2002 Loss: 2.36186519983643e-05\n",
      "Iteration: 2003 Loss: 2.3604723172262643e-05\n",
      "Iteration: 2004 Loss: 2.3590802598916963e-05\n",
      "Iteration: 2005 Loss: 2.3576890267355284e-05\n",
      "Iteration: 2006 Loss: 2.3562986166630598e-05\n",
      "Iteration: 2007 Loss: 2.3549090285376158e-05\n",
      "Iteration: 2008 Loss: 2.3535202612458706e-05\n",
      "Iteration: 2009 Loss: 2.352132313700523e-05\n",
      "Iteration: 2010 Loss: 2.3507451848503977e-05\n",
      "Iteration: 2011 Loss: 2.349358873560826e-05\n",
      "Iteration: 2012 Loss: 2.347973378774007e-05\n",
      "Iteration: 2013 Loss: 2.346588699385234e-05\n",
      "Iteration: 2014 Loss: 2.3452048343609205e-05\n",
      "Iteration: 2015 Loss: 2.3438217826061487e-05\n",
      "Iteration: 2016 Loss: 2.3424395430762266e-05\n",
      "Iteration: 2017 Loss: 2.3410581147198203e-05\n",
      "Iteration: 2018 Loss: 2.339677496480867e-05\n",
      "Iteration: 2019 Loss: 2.338297687310874e-05\n",
      "Iteration: 2020 Loss: 2.336918686167889e-05\n",
      "Iteration: 2021 Loss: 2.335540492017003e-05\n",
      "Iteration: 2022 Loss: 2.334163103857258e-05\n",
      "Iteration: 2023 Loss: 2.3327865206554993e-05\n",
      "Iteration: 2024 Loss: 2.3314107413690912e-05\n",
      "Iteration: 2025 Loss: 2.3300357650078424e-05\n",
      "Iteration: 2026 Loss: 2.3286615905571644e-05\n",
      "Iteration: 2027 Loss: 2.3272882170056785e-05\n",
      "Iteration: 2028 Loss: 2.325915643338292e-05\n",
      "Iteration: 2029 Loss: 2.3245438685815306e-05\n",
      "Iteration: 2030 Loss: 2.3231728917307527e-05\n",
      "Iteration: 2031 Loss: 2.3218027118174026e-05\n",
      "Iteration: 2032 Loss: 2.320433327870614e-05\n",
      "Iteration: 2033 Loss: 2.319064738867092e-05\n",
      "Iteration: 2034 Loss: 2.3176969438727895e-05\n",
      "Iteration: 2035 Loss: 2.3163299419309687e-05\n",
      "Iteration: 2036 Loss: 2.3149637320547426e-05\n",
      "Iteration: 2037 Loss: 2.313598313309351e-05\n",
      "Iteration: 2038 Loss: 2.312233684715553e-05\n",
      "Iteration: 2039 Loss: 2.3108698453463285e-05\n",
      "Iteration: 2040 Loss: 2.3095067942477045e-05\n",
      "Iteration: 2041 Loss: 2.3081445305088022e-05\n",
      "Iteration: 2042 Loss: 2.306783053123887e-05\n",
      "Iteration: 2043 Loss: 2.3054223612219026e-05\n",
      "Iteration: 2044 Loss: 2.3040624538533268e-05\n",
      "Iteration: 2045 Loss: 2.3027033300951574e-05\n",
      "Iteration: 2046 Loss: 2.301344989012955e-05\n",
      "Iteration: 2047 Loss: 2.2999874297181488e-05\n",
      "Iteration: 2048 Loss: 2.2986306513275464e-05\n",
      "Iteration: 2049 Loss: 2.297274652876826e-05\n",
      "Iteration: 2050 Loss: 2.2959194334912676e-05\n",
      "Iteration: 2051 Loss: 2.294564992278314e-05\n",
      "Iteration: 2052 Loss: 2.2932113283377663e-05\n",
      "Iteration: 2053 Loss: 2.291858440789373e-05\n",
      "Iteration: 2054 Loss: 2.2905063287363355e-05\n",
      "Iteration: 2055 Loss: 2.2891549912905466e-05\n",
      "Iteration: 2056 Loss: 2.2878044275542417e-05\n",
      "Iteration: 2057 Loss: 2.286454636708143e-05\n",
      "Iteration: 2058 Loss: 2.2851056178319424e-05\n",
      "Iteration: 2059 Loss: 2.2837573701147124e-05\n",
      "Iteration: 2060 Loss: 2.2824098926547998e-05\n",
      "Iteration: 2061 Loss: 2.281063184604031e-05\n",
      "Iteration: 2062 Loss: 2.2797172451115435e-05\n",
      "Iteration: 2063 Loss: 2.2783720733071737e-05\n",
      "Iteration: 2064 Loss: 2.2770276683405273e-05\n",
      "Iteration: 2065 Loss: 2.275684029396459e-05\n",
      "Iteration: 2066 Loss: 2.2743411556382065e-05\n",
      "Iteration: 2067 Loss: 2.2729990462018892e-05\n",
      "Iteration: 2068 Loss: 2.2716577002575064e-05\n",
      "Iteration: 2069 Loss: 2.2703171169991785e-05\n",
      "Iteration: 2070 Loss: 2.26897729557602e-05\n",
      "Iteration: 2071 Loss: 2.2676382351756482e-05\n",
      "Iteration: 2072 Loss: 2.2662999350029786e-05\n",
      "Iteration: 2073 Loss: 2.264962394223871e-05\n",
      "Iteration: 2074 Loss: 2.2636256120354524e-05\n",
      "Iteration: 2075 Loss: 2.2622895876158283e-05\n",
      "Iteration: 2076 Loss: 2.260954320166689e-05\n",
      "Iteration: 2077 Loss: 2.2596198088827776e-05\n",
      "Iteration: 2078 Loss: 2.258286052959771e-05\n",
      "Iteration: 2079 Loss: 2.2569530516333538e-05\n",
      "Iteration: 2080 Loss: 2.2556208040867233e-05\n",
      "Iteration: 2081 Loss: 2.2542893095443332e-05\n",
      "Iteration: 2082 Loss: 2.2529585672214977e-05\n",
      "Iteration: 2083 Loss: 2.2516285763284274e-05\n",
      "Iteration: 2084 Loss: 2.2502993360887337e-05\n",
      "Iteration: 2085 Loss: 2.248970845749245e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2086 Loss: 2.247643104503669e-05\n",
      "Iteration: 2087 Loss: 2.2463161116199715e-05\n",
      "Iteration: 2088 Loss: 2.24498986628949e-05\n",
      "Iteration: 2089 Loss: 2.2436643677899362e-05\n",
      "Iteration: 2090 Loss: 2.2423396153473956e-05\n",
      "Iteration: 2091 Loss: 2.241015608208136e-05\n",
      "Iteration: 2092 Loss: 2.2396923456061037e-05\n",
      "Iteration: 2093 Loss: 2.238369826797025e-05\n",
      "Iteration: 2094 Loss: 2.2370480510369905e-05\n",
      "Iteration: 2095 Loss: 2.2357270176113398e-05\n",
      "Iteration: 2096 Loss: 2.2344067257395533e-05\n",
      "Iteration: 2097 Loss: 2.2330871747011276e-05\n",
      "Iteration: 2098 Loss: 2.231768363783953e-05\n",
      "Iteration: 2099 Loss: 2.2304502921913077e-05\n",
      "Iteration: 2100 Loss: 2.2291329592386705e-05\n",
      "Iteration: 2101 Loss: 2.2278163641985107e-05\n",
      "Iteration: 2102 Loss: 2.2265005063641165e-05\n",
      "Iteration: 2103 Loss: 2.2251853849841984e-05\n",
      "Iteration: 2104 Loss: 2.2238709993660394e-05\n",
      "Iteration: 2105 Loss: 2.2225573487872057e-05\n",
      "Iteration: 2106 Loss: 2.221244432530228e-05\n",
      "Iteration: 2107 Loss: 2.2199322498896598e-05\n",
      "Iteration: 2108 Loss: 2.218620800140664e-05\n",
      "Iteration: 2109 Loss: 2.2173100825969494e-05\n",
      "Iteration: 2110 Loss: 2.2160000965497035e-05\n",
      "Iteration: 2111 Loss: 2.2146908413162985e-05\n",
      "Iteration: 2112 Loss: 2.2133823162031522e-05\n",
      "Iteration: 2113 Loss: 2.2120745204902203e-05\n",
      "Iteration: 2114 Loss: 2.210767453518114e-05\n",
      "Iteration: 2115 Loss: 2.2094611145893687e-05\n",
      "Iteration: 2116 Loss: 2.2081555029957023e-05\n",
      "Iteration: 2117 Loss: 2.2068506180950155e-05\n",
      "Iteration: 2118 Loss: 2.2055464591712383e-05\n",
      "Iteration: 2119 Loss: 2.2042430255778122e-05\n",
      "Iteration: 2120 Loss: 2.2029403165892217e-05\n",
      "Iteration: 2121 Loss: 2.201638331576695e-05\n",
      "Iteration: 2122 Loss: 2.200337069874106e-05\n",
      "Iteration: 2123 Loss: 2.1990365307984096e-05\n",
      "Iteration: 2124 Loss: 2.1977367136918475e-05\n",
      "Iteration: 2125 Loss: 2.1964376178914172e-05\n",
      "Iteration: 2126 Loss: 2.1951392427248405e-05\n",
      "Iteration: 2127 Loss: 2.1938415875415982e-05\n",
      "Iteration: 2128 Loss: 2.1925446517042515e-05\n",
      "Iteration: 2129 Loss: 2.1912484345477143e-05\n",
      "Iteration: 2130 Loss: 2.1899529353965713e-05\n",
      "Iteration: 2131 Loss: 2.188658153609327e-05\n",
      "Iteration: 2132 Loss: 2.18736408856381e-05\n",
      "Iteration: 2133 Loss: 2.186070739607528e-05\n",
      "Iteration: 2134 Loss: 2.184778106090387e-05\n",
      "Iteration: 2135 Loss: 2.1834861873884264e-05\n",
      "Iteration: 2136 Loss: 2.1821949828708947e-05\n",
      "Iteration: 2137 Loss: 2.1809044918842643e-05\n",
      "Iteration: 2138 Loss: 2.1796147137994493e-05\n",
      "Iteration: 2139 Loss: 2.1783256479741557e-05\n",
      "Iteration: 2140 Loss: 2.1770372938284824e-05\n",
      "Iteration: 2141 Loss: 2.1757496507058234e-05\n",
      "Iteration: 2142 Loss: 2.1744627179605058e-05\n",
      "Iteration: 2143 Loss: 2.1731764950197426e-05\n",
      "Iteration: 2144 Loss: 2.171890981224355e-05\n",
      "Iteration: 2145 Loss: 2.170606175973408e-05\n",
      "Iteration: 2146 Loss: 2.169322078628544e-05\n",
      "Iteration: 2147 Loss: 2.1680386886062957e-05\n",
      "Iteration: 2148 Loss: 2.1667560052792195e-05\n",
      "Iteration: 2149 Loss: 2.1654740280779727e-05\n",
      "Iteration: 2150 Loss: 2.1641927563616574e-05\n",
      "Iteration: 2151 Loss: 2.1629121895073366e-05\n",
      "Iteration: 2152 Loss: 2.161632326954833e-05\n",
      "Iteration: 2153 Loss: 2.1603531680444862e-05\n",
      "Iteration: 2154 Loss: 2.159074712257921e-05\n",
      "Iteration: 2155 Loss: 2.1577969589613313e-05\n",
      "Iteration: 2156 Loss: 2.1565199075501348e-05\n",
      "Iteration: 2157 Loss: 2.1552435574114374e-05\n",
      "Iteration: 2158 Loss: 2.1539679080054662e-05\n",
      "Iteration: 2159 Loss: 2.15269295869642e-05\n",
      "Iteration: 2160 Loss: 2.1514187089243055e-05\n",
      "Iteration: 2161 Loss: 2.150145158101924e-05\n",
      "Iteration: 2162 Loss: 2.1488723056275395e-05\n",
      "Iteration: 2163 Loss: 2.1476001509592072e-05\n",
      "Iteration: 2164 Loss: 2.146328693482859e-05\n",
      "Iteration: 2165 Loss: 2.1450579326325416e-05\n",
      "Iteration: 2166 Loss: 2.1437878678405547e-05\n",
      "Iteration: 2167 Loss: 2.1425184985289265e-05\n",
      "Iteration: 2168 Loss: 2.141249824110811e-05\n",
      "Iteration: 2169 Loss: 2.1399818440302482e-05\n",
      "Iteration: 2170 Loss: 2.138714557718242e-05\n",
      "Iteration: 2171 Loss: 2.1374479646170843e-05\n",
      "Iteration: 2172 Loss: 2.1361820641399166e-05\n",
      "Iteration: 2173 Loss: 2.134916855757807e-05\n",
      "Iteration: 2174 Loss: 2.133652338866583e-05\n",
      "Iteration: 2175 Loss: 2.1323885129408345e-05\n",
      "Iteration: 2176 Loss: 2.1311253774160092e-05\n",
      "Iteration: 2177 Loss: 2.1298629317228275e-05\n",
      "Iteration: 2178 Loss: 2.1286011753193578e-05\n",
      "Iteration: 2179 Loss: 2.127340107648471e-05\n",
      "Iteration: 2180 Loss: 2.1260797281749718e-05\n",
      "Iteration: 2181 Loss: 2.1248200363316603e-05\n",
      "Iteration: 2182 Loss: 2.1235610315663224e-05\n",
      "Iteration: 2183 Loss: 2.1223027133502696e-05\n",
      "Iteration: 2184 Loss: 2.1210450811364423e-05\n",
      "Iteration: 2185 Loss: 2.119788134363375e-05\n",
      "Iteration: 2186 Loss: 2.1185318725118002e-05\n",
      "Iteration: 2187 Loss: 2.1172762950131915e-05\n",
      "Iteration: 2188 Loss: 2.116021401368089e-05\n",
      "Iteration: 2189 Loss: 2.1147671910127284e-05\n",
      "Iteration: 2190 Loss: 2.113513663425803e-05\n",
      "Iteration: 2191 Loss: 2.1122608180786825e-05\n",
      "Iteration: 2192 Loss: 2.1110086544308337e-05\n",
      "Iteration: 2193 Loss: 2.1097571719389307e-05\n",
      "Iteration: 2194 Loss: 2.108506370086784e-05\n",
      "Iteration: 2195 Loss: 2.1072562483436795e-05\n",
      "Iteration: 2196 Loss: 2.106006806202869e-05\n",
      "Iteration: 2197 Loss: 2.1047580431274018e-05\n",
      "Iteration: 2198 Loss: 2.1035099586176932e-05\n",
      "Iteration: 2199 Loss: 2.102262552105673e-05\n",
      "Iteration: 2200 Loss: 2.1010158231169717e-05\n",
      "Iteration: 2201 Loss: 2.0997697711057958e-05\n",
      "Iteration: 2202 Loss: 2.0985243955543358e-05\n",
      "Iteration: 2203 Loss: 2.097279695966494e-05\n",
      "Iteration: 2204 Loss: 2.09603567181782e-05\n",
      "Iteration: 2205 Loss: 2.0947923226030172e-05\n",
      "Iteration: 2206 Loss: 2.0935496478002544e-05\n",
      "Iteration: 2207 Loss: 2.0923076469128602e-05\n",
      "Iteration: 2208 Loss: 2.0910663194193408e-05\n",
      "Iteration: 2209 Loss: 2.0898256648023694e-05\n",
      "Iteration: 2210 Loss: 2.088585682600267e-05\n",
      "Iteration: 2211 Loss: 2.0873463722955673e-05\n",
      "Iteration: 2212 Loss: 2.0861077333547097e-05\n",
      "Iteration: 2213 Loss: 2.0848697652960563e-05\n",
      "Iteration: 2214 Loss: 2.0836324676059297e-05\n",
      "Iteration: 2215 Loss: 2.0823958398330062e-05\n",
      "Iteration: 2216 Loss: 2.0811598814117606e-05\n",
      "Iteration: 2217 Loss: 2.079924591897319e-05\n",
      "Iteration: 2218 Loss: 2.078689970771689e-05\n",
      "Iteration: 2219 Loss: 2.0774560175478274e-05\n",
      "Iteration: 2220 Loss: 2.076222731754563e-05\n",
      "Iteration: 2221 Loss: 2.074990112874885e-05\n",
      "Iteration: 2222 Loss: 2.0737581604162632e-05\n",
      "Iteration: 2223 Loss: 2.072526873898286e-05\n",
      "Iteration: 2224 Loss: 2.071296252856606e-05\n",
      "Iteration: 2225 Loss: 2.0700662967708877e-05\n",
      "Iteration: 2226 Loss: 2.0688370051838956e-05\n",
      "Iteration: 2227 Loss: 2.067608377593685e-05\n",
      "Iteration: 2228 Loss: 2.0663804135480793e-05\n",
      "Iteration: 2229 Loss: 2.0651531125196957e-05\n",
      "Iteration: 2230 Loss: 2.0639264740600515e-05\n",
      "Iteration: 2231 Loss: 2.062700497701465e-05\n",
      "Iteration: 2232 Loss: 2.0614751829422025e-05\n",
      "Iteration: 2233 Loss: 2.060250529310383e-05\n",
      "Iteration: 2234 Loss: 2.0590265363428152e-05\n",
      "Iteration: 2235 Loss: 2.0578032035712907e-05\n",
      "Iteration: 2236 Loss: 2.056580530501311e-05\n",
      "Iteration: 2237 Loss: 2.0553585166704685e-05\n",
      "Iteration: 2238 Loss: 2.054137161603671e-05\n",
      "Iteration: 2239 Loss: 2.0529164648491814e-05\n",
      "Iteration: 2240 Loss: 2.051696425922031e-05\n",
      "Iteration: 2241 Loss: 2.0504770443646918e-05\n",
      "Iteration: 2242 Loss: 2.0492583197207804e-05\n",
      "Iteration: 2243 Loss: 2.0480402515333097e-05\n",
      "Iteration: 2244 Loss: 2.0468228392821936e-05\n",
      "Iteration: 2245 Loss: 2.045606082567454e-05\n",
      "Iteration: 2246 Loss: 2.0443899809061195e-05\n",
      "Iteration: 2247 Loss: 2.0431745338486508e-05\n",
      "Iteration: 2248 Loss: 2.0419597409166247e-05\n",
      "Iteration: 2249 Loss: 2.040745601656431e-05\n",
      "Iteration: 2250 Loss: 2.039532115638897e-05\n",
      "Iteration: 2251 Loss: 2.03831928236989e-05\n",
      "Iteration: 2252 Loss: 2.0371071014238152e-05\n",
      "Iteration: 2253 Loss: 2.0358955723507784e-05\n",
      "Iteration: 2254 Loss: 2.0346846946629503e-05\n",
      "Iteration: 2255 Loss: 2.033474467924739e-05\n",
      "Iteration: 2256 Loss: 2.0322648917293784e-05\n",
      "Iteration: 2257 Loss: 2.0310559655525818e-05\n",
      "Iteration: 2258 Loss: 2.0298476889970016e-05\n",
      "Iteration: 2259 Loss: 2.028640061580632e-05\n",
      "Iteration: 2260 Loss: 2.0274330829000448e-05\n",
      "Iteration: 2261 Loss: 2.026226752476252e-05\n",
      "Iteration: 2262 Loss: 2.0250210698806314e-05\n",
      "Iteration: 2263 Loss: 2.023816034686979e-05\n",
      "Iteration: 2264 Loss: 2.022611646429327e-05\n",
      "Iteration: 2265 Loss: 2.0214079046704783e-05\n",
      "Iteration: 2266 Loss: 2.0202048089686195e-05\n",
      "Iteration: 2267 Loss: 2.0190023588787985e-05\n",
      "Iteration: 2268 Loss: 2.0178005539785548e-05\n",
      "Iteration: 2269 Loss: 2.0165993938072635e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2270 Loss: 2.0153988779446157e-05\n",
      "Iteration: 2271 Loss: 2.01419900595926e-05\n",
      "Iteration: 2272 Loss: 2.0129997774019643e-05\n",
      "Iteration: 2273 Loss: 2.0118011918602074e-05\n",
      "Iteration: 2274 Loss: 2.0106032488983248e-05\n",
      "Iteration: 2275 Loss: 2.0094059480647862e-05\n",
      "Iteration: 2276 Loss: 2.008209288947765e-05\n",
      "Iteration: 2277 Loss: 2.0070132711139667e-05\n",
      "Iteration: 2278 Loss: 2.0058178941056935e-05\n",
      "Iteration: 2279 Loss: 2.004623157529714e-05\n",
      "Iteration: 2280 Loss: 2.0034290609423717e-05\n",
      "Iteration: 2281 Loss: 2.0022356039535815e-05\n",
      "Iteration: 2282 Loss: 2.001042786083287e-05\n",
      "Iteration: 2283 Loss: 1.9998506069509897e-05\n",
      "Iteration: 2284 Loss: 1.998659066107116e-05\n",
      "Iteration: 2285 Loss: 1.9974681631273996e-05\n",
      "Iteration: 2286 Loss: 1.9962778976107962e-05\n",
      "Iteration: 2287 Loss: 1.9950882691193765e-05\n",
      "Iteration: 2288 Loss: 1.993899277235999e-05\n",
      "Iteration: 2289 Loss: 1.992710921527852e-05\n",
      "Iteration: 2290 Loss: 1.991523201600273e-05\n",
      "Iteration: 2291 Loss: 1.9903361170197593e-05\n",
      "Iteration: 2292 Loss: 1.9891496673719513e-05\n",
      "Iteration: 2293 Loss: 1.9879638522400227e-05\n",
      "Iteration: 2294 Loss: 1.9867786712184694e-05\n",
      "Iteration: 2295 Loss: 1.9855941238885646e-05\n",
      "Iteration: 2296 Loss: 1.9844102098158973e-05\n",
      "Iteration: 2297 Loss: 1.9832269286129947e-05\n",
      "Iteration: 2298 Loss: 1.9820442798569668e-05\n",
      "Iteration: 2299 Loss: 1.9808622631515533e-05\n",
      "Iteration: 2300 Loss: 1.979680878087757e-05\n",
      "Iteration: 2301 Loss: 1.978500124240645e-05\n",
      "Iteration: 2302 Loss: 1.9773200012106936e-05\n",
      "Iteration: 2303 Loss: 1.9761405085838472e-05\n",
      "Iteration: 2304 Loss: 1.9749616459516477e-05\n",
      "Iteration: 2305 Loss: 1.9737834129031736e-05\n",
      "Iteration: 2306 Loss: 1.9726058090427993e-05\n",
      "Iteration: 2307 Loss: 1.9714288339497066e-05\n",
      "Iteration: 2308 Loss: 1.970252487237791e-05\n",
      "Iteration: 2309 Loss: 1.969076768479903e-05\n",
      "Iteration: 2310 Loss: 1.9679016773016738e-05\n",
      "Iteration: 2311 Loss: 1.966727213282734e-05\n",
      "Iteration: 2312 Loss: 1.965553376046859e-05\n",
      "Iteration: 2313 Loss: 1.964380165162937e-05\n",
      "Iteration: 2314 Loss: 1.9632075802605716e-05\n",
      "Iteration: 2315 Loss: 1.9620356209067653e-05\n",
      "Iteration: 2316 Loss: 1.9608642867198788e-05\n",
      "Iteration: 2317 Loss: 1.9596935773013768e-05\n",
      "Iteration: 2318 Loss: 1.9585234922727694e-05\n",
      "Iteration: 2319 Loss: 1.9573540312124496e-05\n",
      "Iteration: 2320 Loss: 1.9561851937060704e-05\n",
      "Iteration: 2321 Loss: 1.955016979386488e-05\n",
      "Iteration: 2322 Loss: 1.9538493878867288e-05\n",
      "Iteration: 2323 Loss: 1.9526824187892887e-05\n",
      "Iteration: 2324 Loss: 1.9515160716971383e-05\n",
      "Iteration: 2325 Loss: 1.95035034620481e-05\n",
      "Iteration: 2326 Loss: 1.9491852419322987e-05\n",
      "Iteration: 2327 Loss: 1.94802075849412e-05\n",
      "Iteration: 2328 Loss: 1.9468568954958515e-05\n",
      "Iteration: 2329 Loss: 1.945693652529066e-05\n",
      "Iteration: 2330 Loss: 1.9445310292624343e-05\n",
      "Iteration: 2331 Loss: 1.943369025260058e-05\n",
      "Iteration: 2332 Loss: 1.942207640130061e-05\n",
      "Iteration: 2333 Loss: 1.9410468735032887e-05\n",
      "Iteration: 2334 Loss: 1.939886724997045e-05\n",
      "Iteration: 2335 Loss: 1.938727194216268e-05\n",
      "Iteration: 2336 Loss: 1.9375682808045876e-05\n",
      "Iteration: 2337 Loss: 1.936409984339374e-05\n",
      "Iteration: 2338 Loss: 1.935252304429003e-05\n",
      "Iteration: 2339 Loss: 1.9340952407219975e-05\n",
      "Iteration: 2340 Loss: 1.9329387928387195e-05\n",
      "Iteration: 2341 Loss: 1.9317829603819575e-05\n",
      "Iteration: 2342 Loss: 1.9306277429797733e-05\n",
      "Iteration: 2343 Loss: 1.9294731402419974e-05\n",
      "Iteration: 2344 Loss: 1.928319151783236e-05\n",
      "Iteration: 2345 Loss: 1.9271657772456893e-05\n",
      "Iteration: 2346 Loss: 1.9260130162204052e-05\n",
      "Iteration: 2347 Loss: 1.9248608683495275e-05\n",
      "Iteration: 2348 Loss: 1.9237093332648812e-05\n",
      "Iteration: 2349 Loss: 1.922558410581931e-05\n",
      "Iteration: 2350 Loss: 1.9214080999292623e-05\n",
      "Iteration: 2351 Loss: 1.9202584009192938e-05\n",
      "Iteration: 2352 Loss: 1.9191093131766266e-05\n",
      "Iteration: 2353 Loss: 1.9179608363633142e-05\n",
      "Iteration: 2354 Loss: 1.9168129700576573e-05\n",
      "Iteration: 2355 Loss: 1.9156657138976817e-05\n",
      "Iteration: 2356 Loss: 1.9145190675483497e-05\n",
      "Iteration: 2357 Loss: 1.9133730305846794e-05\n",
      "Iteration: 2358 Loss: 1.9122276026386627e-05\n",
      "Iteration: 2359 Loss: 1.911082783372289e-05\n",
      "Iteration: 2360 Loss: 1.909938572389756e-05\n",
      "Iteration: 2361 Loss: 1.9087949693364333e-05\n",
      "Iteration: 2362 Loss: 1.9076519738388654e-05\n",
      "Iteration: 2363 Loss: 1.906509585520038e-05\n",
      "Iteration: 2364 Loss: 1.9053678040567766e-05\n",
      "Iteration: 2365 Loss: 1.9042266290217512e-05\n",
      "Iteration: 2366 Loss: 1.903086060064892e-05\n",
      "Iteration: 2367 Loss: 1.9019460968238633e-05\n",
      "Iteration: 2368 Loss: 1.9008067389457375e-05\n",
      "Iteration: 2369 Loss: 1.8996679860382068e-05\n",
      "Iteration: 2370 Loss: 1.8985298377469273e-05\n",
      "Iteration: 2371 Loss: 1.8973922937177837e-05\n",
      "Iteration: 2372 Loss: 1.8962553535829263e-05\n",
      "Iteration: 2373 Loss: 1.8951190169896983e-05\n",
      "Iteration: 2374 Loss: 1.8939832835661888e-05\n",
      "Iteration: 2375 Loss: 1.892848152939921e-05\n",
      "Iteration: 2376 Loss: 1.8917136247578536e-05\n",
      "Iteration: 2377 Loss: 1.890579698677027e-05\n",
      "Iteration: 2378 Loss: 1.8894463743047024e-05\n",
      "Iteration: 2379 Loss: 1.888313651298338e-05\n",
      "Iteration: 2380 Loss: 1.8871815293154416e-05\n",
      "Iteration: 2381 Loss: 1.886050007939019e-05\n",
      "Iteration: 2382 Loss: 1.884919086864104e-05\n",
      "Iteration: 2383 Loss: 1.883788765721472e-05\n",
      "Iteration: 2384 Loss: 1.88265904414842e-05\n",
      "Iteration: 2385 Loss: 1.8815299217824226e-05\n",
      "Iteration: 2386 Loss: 1.8804013982778092e-05\n",
      "Iteration: 2387 Loss: 1.879273473262062e-05\n",
      "Iteration: 2388 Loss: 1.87814614642163e-05\n",
      "Iteration: 2389 Loss: 1.877019417352817e-05\n",
      "Iteration: 2390 Loss: 1.875893285705248e-05\n",
      "Iteration: 2391 Loss: 1.8747677511647393e-05\n",
      "Iteration: 2392 Loss: 1.8736428133321948e-05\n",
      "Iteration: 2393 Loss: 1.8725184718644797e-05\n",
      "Iteration: 2394 Loss: 1.8713947264336956e-05\n",
      "Iteration: 2395 Loss: 1.8702715766473135e-05\n",
      "Iteration: 2396 Loss: 1.8691490221873587e-05\n",
      "Iteration: 2397 Loss: 1.868027062712674e-05\n",
      "Iteration: 2398 Loss: 1.8669056978569886e-05\n",
      "Iteration: 2399 Loss: 1.8657849272398772e-05\n",
      "Iteration: 2400 Loss: 1.8646647505436656e-05\n",
      "Iteration: 2401 Loss: 1.8635451674091866e-05\n",
      "Iteration: 2402 Loss: 1.862426177481619e-05\n",
      "Iteration: 2403 Loss: 1.8613077804330408e-05\n",
      "Iteration: 2404 Loss: 1.860189975889116e-05\n",
      "Iteration: 2405 Loss: 1.8590727635221546e-05\n",
      "Iteration: 2406 Loss: 1.857956142983337e-05\n",
      "Iteration: 2407 Loss: 1.8568401139100746e-05\n",
      "Iteration: 2408 Loss: 1.855724675957964e-05\n",
      "Iteration: 2409 Loss: 1.8546098287843483e-05\n",
      "Iteration: 2410 Loss: 1.8534955720640905e-05\n",
      "Iteration: 2411 Loss: 1.8523819054285118e-05\n",
      "Iteration: 2412 Loss: 1.8512688285594204e-05\n",
      "Iteration: 2413 Loss: 1.850156341064423e-05\n",
      "Iteration: 2414 Loss: 1.8490444426592327e-05\n",
      "Iteration: 2415 Loss: 1.8479331329779734e-05\n",
      "Iteration: 2416 Loss: 1.8468224116781438e-05\n",
      "Iteration: 2417 Loss: 1.8457122784074255e-05\n",
      "Iteration: 2418 Loss: 1.8446027328325972e-05\n",
      "Iteration: 2419 Loss: 1.8434937745986463e-05\n",
      "Iteration: 2420 Loss: 1.8423854034076978e-05\n",
      "Iteration: 2421 Loss: 1.841277618869478e-05\n",
      "Iteration: 2422 Loss: 1.8401704206331505e-05\n",
      "Iteration: 2423 Loss: 1.8390638083788303e-05\n",
      "Iteration: 2424 Loss: 1.8379577817902552e-05\n",
      "Iteration: 2425 Loss: 1.836852340475177e-05\n",
      "Iteration: 2426 Loss: 1.8357474841583063e-05\n",
      "Iteration: 2427 Loss: 1.834643212493865e-05\n",
      "Iteration: 2428 Loss: 1.8335395250975016e-05\n",
      "Iteration: 2429 Loss: 1.8324364216661682e-05\n",
      "Iteration: 2430 Loss: 1.8313339018544322e-05\n",
      "Iteration: 2431 Loss: 1.8302319653240728e-05\n",
      "Iteration: 2432 Loss: 1.8291306117296928e-05\n",
      "Iteration: 2433 Loss: 1.8280298407430877e-05\n",
      "Iteration: 2434 Loss: 1.8269296520433496e-05\n",
      "Iteration: 2435 Loss: 1.825830045270175e-05\n",
      "Iteration: 2436 Loss: 1.824731020133657e-05\n",
      "Iteration: 2437 Loss: 1.823632576246654e-05\n",
      "Iteration: 2438 Loss: 1.822534713294194e-05\n",
      "Iteration: 2439 Loss: 1.821437430966731e-05\n",
      "Iteration: 2440 Loss: 1.820340728883492e-05\n",
      "Iteration: 2441 Loss: 1.8192446067596e-05\n",
      "Iteration: 2442 Loss: 1.8181490642267772e-05\n",
      "Iteration: 2443 Loss: 1.8170541009734408e-05\n",
      "Iteration: 2444 Loss: 1.8159597166494104e-05\n",
      "Iteration: 2445 Loss: 1.8148659109397e-05\n",
      "Iteration: 2446 Loss: 1.813772683506065e-05\n",
      "Iteration: 2447 Loss: 1.8126800340163337e-05\n",
      "Iteration: 2448 Loss: 1.811587962150494e-05\n",
      "Iteration: 2449 Loss: 1.8104964675444315e-05\n",
      "Iteration: 2450 Loss: 1.8094055499295975e-05\n",
      "Iteration: 2451 Loss: 1.808315208949825e-05\n",
      "Iteration: 2452 Loss: 1.8072254442594336e-05\n",
      "Iteration: 2453 Loss: 1.8061362555649997e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2454 Loss: 1.8050476424936765e-05\n",
      "Iteration: 2455 Loss: 1.80395960472792e-05\n",
      "Iteration: 2456 Loss: 1.8028721419371013e-05\n",
      "Iteration: 2457 Loss: 1.8017852538147822e-05\n",
      "Iteration: 2458 Loss: 1.800698940056643e-05\n",
      "Iteration: 2459 Loss: 1.7996132003010663e-05\n",
      "Iteration: 2460 Loss: 1.7985280342104323e-05\n",
      "Iteration: 2461 Loss: 1.7974434414906013e-05\n",
      "Iteration: 2462 Loss: 1.796359421804516e-05\n",
      "Iteration: 2463 Loss: 1.7952759748336502e-05\n",
      "Iteration: 2464 Loss: 1.794193100235078e-05\n",
      "Iteration: 2465 Loss: 1.7931107977080383e-05\n",
      "Iteration: 2466 Loss: 1.7920290669121044e-05\n",
      "Iteration: 2467 Loss: 1.790947907508847e-05\n",
      "Iteration: 2468 Loss: 1.7898673192079898e-05\n",
      "Iteration: 2469 Loss: 1.7887873016716786e-05\n",
      "Iteration: 2470 Loss: 1.7877078545715967e-05\n",
      "Iteration: 2471 Loss: 1.7866289775916314e-05\n",
      "Iteration: 2472 Loss: 1.7855506704024036e-05\n",
      "Iteration: 2473 Loss: 1.7844729327061066e-05\n",
      "Iteration: 2474 Loss: 1.783395764161613e-05\n",
      "Iteration: 2475 Loss: 1.782319164453064e-05\n",
      "Iteration: 2476 Loss: 1.781243133241839e-05\n",
      "Iteration: 2477 Loss: 1.7801676702508547e-05\n",
      "Iteration: 2478 Loss: 1.7790927751358512e-05\n",
      "Iteration: 2479 Loss: 1.7780184475776203e-05\n",
      "Iteration: 2480 Loss: 1.7769446872595445e-05\n",
      "Iteration: 2481 Loss: 1.7758714938548164e-05\n",
      "Iteration: 2482 Loss: 1.7747988670598916e-05\n",
      "Iteration: 2483 Loss: 1.7737268065465126e-05\n",
      "Iteration: 2484 Loss: 1.7726553120011154e-05\n",
      "Iteration: 2485 Loss: 1.771584383094588e-05\n",
      "Iteration: 2486 Loss: 1.770514019508955e-05\n",
      "Iteration: 2487 Loss: 1.769444220931765e-05\n",
      "Iteration: 2488 Loss: 1.7683749870427708e-05\n",
      "Iteration: 2489 Loss: 1.7673063175147532e-05\n",
      "Iteration: 2490 Loss: 1.766238212070294e-05\n",
      "Iteration: 2491 Loss: 1.765170670348325e-05\n",
      "Iteration: 2492 Loss: 1.7641036920651526e-05\n",
      "Iteration: 2493 Loss: 1.763037276916857e-05\n",
      "Iteration: 2494 Loss: 1.7619714245673363e-05\n",
      "Iteration: 2495 Loss: 1.7609061346997395e-05\n",
      "Iteration: 2496 Loss: 1.7598414070049556e-05\n",
      "Iteration: 2497 Loss: 1.7587772411782602e-05\n",
      "Iteration: 2498 Loss: 1.757713636882343e-05\n",
      "Iteration: 2499 Loss: 1.7566505938062185e-05\n",
      "Iteration: 2500 Loss: 1.7555881116197596e-05\n",
      "Iteration: 2501 Loss: 1.754526190076372e-05\n",
      "Iteration: 2502 Loss: 1.7534648288154454e-05\n",
      "Iteration: 2503 Loss: 1.7524040275345735e-05\n",
      "Iteration: 2504 Loss: 1.7513437859265477e-05\n",
      "Iteration: 2505 Loss: 1.750284103679516e-05\n",
      "Iteration: 2506 Loss: 1.749224980497719e-05\n",
      "Iteration: 2507 Loss: 1.7481664160436162e-05\n",
      "Iteration: 2508 Loss: 1.7471084099928786e-05\n",
      "Iteration: 2509 Loss: 1.7460509620339674e-05\n",
      "Iteration: 2510 Loss: 1.7449940719024917e-05\n",
      "Iteration: 2511 Loss: 1.7439377392844152e-05\n",
      "Iteration: 2512 Loss: 1.74288196381726e-05\n",
      "Iteration: 2513 Loss: 1.741826745253516e-05\n",
      "Iteration: 2514 Loss: 1.740772083222095e-05\n",
      "Iteration: 2515 Loss: 1.739717977472557e-05\n",
      "Iteration: 2516 Loss: 1.738664427682645e-05\n",
      "Iteration: 2517 Loss: 1.7376114335168277e-05\n",
      "Iteration: 2518 Loss: 1.7365589946945964e-05\n",
      "Iteration: 2519 Loss: 1.7355071108942294e-05\n",
      "Iteration: 2520 Loss: 1.7344557818141628e-05\n",
      "Iteration: 2521 Loss: 1.733405007130873e-05\n",
      "Iteration: 2522 Loss: 1.7323547865750776e-05\n",
      "Iteration: 2523 Loss: 1.7313051197972038e-05\n",
      "Iteration: 2524 Loss: 1.730256006522623e-05\n",
      "Iteration: 2525 Loss: 1.7292074464037434e-05\n",
      "Iteration: 2526 Loss: 1.7281594391812516e-05\n",
      "Iteration: 2527 Loss: 1.7271119845399373e-05\n",
      "Iteration: 2528 Loss: 1.7260650821551742e-05\n",
      "Iteration: 2529 Loss: 1.7250187317567825e-05\n",
      "Iteration: 2530 Loss: 1.723972933011565e-05\n",
      "Iteration: 2531 Loss: 1.7229276856227716e-05\n",
      "Iteration: 2532 Loss: 1.721882989282798e-05\n",
      "Iteration: 2533 Loss: 1.720838843659707e-05\n",
      "Iteration: 2534 Loss: 1.719795248498617e-05\n",
      "Iteration: 2535 Loss: 1.718752203487145e-05\n",
      "Iteration: 2536 Loss: 1.7177097082900673e-05\n",
      "Iteration: 2537 Loss: 1.7166677626478775e-05\n",
      "Iteration: 2538 Loss: 1.7156263662542974e-05\n",
      "Iteration: 2539 Loss: 1.714585518782036e-05\n",
      "Iteration: 2540 Loss: 1.7135452199316e-05\n",
      "Iteration: 2541 Loss: 1.7125054694083053e-05\n",
      "Iteration: 2542 Loss: 1.711466266930556e-05\n",
      "Iteration: 2543 Loss: 1.7104276121578114e-05\n",
      "Iteration: 2544 Loss: 1.7093895047779992e-05\n",
      "Iteration: 2545 Loss: 1.7083519445601762e-05\n",
      "Iteration: 2546 Loss: 1.707314931164393e-05\n",
      "Iteration: 2547 Loss: 1.706278464274341e-05\n",
      "Iteration: 2548 Loss: 1.7052425436087304e-05\n",
      "Iteration: 2549 Loss: 1.7042071688475997e-05\n",
      "Iteration: 2550 Loss: 1.7031723397288442e-05\n",
      "Iteration: 2551 Loss: 1.702138055944962e-05\n",
      "Iteration: 2552 Loss: 1.7011043171731337e-05\n",
      "Iteration: 2553 Loss: 1.700071123127539e-05\n",
      "Iteration: 2554 Loss: 1.6990384735142608e-05\n",
      "Iteration: 2555 Loss: 1.6980063680271136e-05\n",
      "Iteration: 2556 Loss: 1.6969748063443993e-05\n",
      "Iteration: 2557 Loss: 1.695943788229486e-05\n",
      "Iteration: 2558 Loss: 1.6949133133424098e-05\n",
      "Iteration: 2559 Loss: 1.6938833814070895e-05\n",
      "Iteration: 2560 Loss: 1.6928539921010683e-05\n",
      "Iteration: 2561 Loss: 1.6918251451192428e-05\n",
      "Iteration: 2562 Loss: 1.6907968402227832e-05\n",
      "Iteration: 2563 Loss: 1.689769077054681e-05\n",
      "Iteration: 2564 Loss: 1.688741855344546e-05\n",
      "Iteration: 2565 Loss: 1.687715174789765e-05\n",
      "Iteration: 2566 Loss: 1.6866890351071272e-05\n",
      "Iteration: 2567 Loss: 1.685663436008872e-05\n",
      "Iteration: 2568 Loss: 1.684638377155887e-05\n",
      "Iteration: 2569 Loss: 1.6836138583121557e-05\n",
      "Iteration: 2570 Loss: 1.6825898791464278e-05\n",
      "Iteration: 2571 Loss: 1.681566439353535e-05\n",
      "Iteration: 2572 Loss: 1.6805435386660416e-05\n",
      "Iteration: 2573 Loss: 1.679521176780952e-05\n",
      "Iteration: 2574 Loss: 1.678499353388766e-05\n",
      "Iteration: 2575 Loss: 1.6774780682362142e-05\n",
      "Iteration: 2576 Loss: 1.6764573209972503e-05\n",
      "Iteration: 2577 Loss: 1.6754371114099523e-05\n",
      "Iteration: 2578 Loss: 1.674417439145122e-05\n",
      "Iteration: 2579 Loss: 1.673398303910191e-05\n",
      "Iteration: 2580 Loss: 1.6723797054352294e-05\n",
      "Iteration: 2581 Loss: 1.671361643425777e-05\n",
      "Iteration: 2582 Loss: 1.6703441175784367e-05\n",
      "Iteration: 2583 Loss: 1.6693271276166912e-05\n",
      "Iteration: 2584 Loss: 1.668310673219164e-05\n",
      "Iteration: 2585 Loss: 1.667294754108355e-05\n",
      "Iteration: 2586 Loss: 1.666279370015516e-05\n",
      "Iteration: 2587 Loss: 1.6652645206327865e-05\n",
      "Iteration: 2588 Loss: 1.6642502056812232e-05\n",
      "Iteration: 2589 Loss: 1.663236424875097e-05\n",
      "Iteration: 2590 Loss: 1.662223177894935e-05\n",
      "Iteration: 2591 Loss: 1.6612104644701493e-05\n",
      "Iteration: 2592 Loss: 1.6601982842976027e-05\n",
      "Iteration: 2593 Loss: 1.659186637095676e-05\n",
      "Iteration: 2594 Loss: 1.6581755225822983e-05\n",
      "Iteration: 2595 Loss: 1.6571649404558236e-05\n",
      "Iteration: 2596 Loss: 1.6561548904328846e-05\n",
      "Iteration: 2597 Loss: 1.6551453722358368e-05\n",
      "Iteration: 2598 Loss: 1.6541363855581708e-05\n",
      "Iteration: 2599 Loss: 1.6531279301327175e-05\n",
      "Iteration: 2600 Loss: 1.652120005642512e-05\n",
      "Iteration: 2601 Loss: 1.6511126118233243e-05\n",
      "Iteration: 2602 Loss: 1.6501057483882763e-05\n",
      "Iteration: 2603 Loss: 1.649099415054189e-05\n",
      "Iteration: 2604 Loss: 1.648093611522168e-05\n",
      "Iteration: 2605 Loss: 1.6470883374986644e-05\n",
      "Iteration: 2606 Loss: 1.6460835926993077e-05\n",
      "Iteration: 2607 Loss: 1.6450793768495804e-05\n",
      "Iteration: 2608 Loss: 1.644075689651558e-05\n",
      "Iteration: 2609 Loss: 1.6430725308316605e-05\n",
      "Iteration: 2610 Loss: 1.6420699000961698e-05\n",
      "Iteration: 2611 Loss: 1.6410677971598777e-05\n",
      "Iteration: 2612 Loss: 1.640066221753141e-05\n",
      "Iteration: 2613 Loss: 1.6390651735851587e-05\n",
      "Iteration: 2614 Loss: 1.6380646523234304e-05\n",
      "Iteration: 2615 Loss: 1.6370646577266023e-05\n",
      "Iteration: 2616 Loss: 1.636065189519365e-05\n",
      "Iteration: 2617 Loss: 1.6350662474117905e-05\n",
      "Iteration: 2618 Loss: 1.6340678310953016e-05\n",
      "Iteration: 2619 Loss: 1.6330699402994234e-05\n",
      "Iteration: 2620 Loss: 1.6320725747450302e-05\n",
      "Iteration: 2621 Loss: 1.631075734162607e-05\n",
      "Iteration: 2622 Loss: 1.6300794182445122e-05\n",
      "Iteration: 2623 Loss: 1.6290836267257636e-05\n",
      "Iteration: 2624 Loss: 1.6280883592933717e-05\n",
      "Iteration: 2625 Loss: 1.6270936156884358e-05\n",
      "Iteration: 2626 Loss: 1.626099395636523e-05\n",
      "Iteration: 2627 Loss: 1.625105698847443e-05\n",
      "Iteration: 2628 Loss: 1.6241125250278203e-05\n",
      "Iteration: 2629 Loss: 1.623119873905889e-05\n",
      "Iteration: 2630 Loss: 1.6221277452010127e-05\n",
      "Iteration: 2631 Loss: 1.6211361386555915e-05\n",
      "Iteration: 2632 Loss: 1.6201450539415596e-05\n",
      "Iteration: 2633 Loss: 1.619154490803015e-05\n",
      "Iteration: 2634 Loss: 1.618164448946513e-05\n",
      "Iteration: 2635 Loss: 1.617174928085043e-05\n",
      "Iteration: 2636 Loss: 1.616185927945283e-05\n",
      "Iteration: 2637 Loss: 1.6151974482544942e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2638 Loss: 1.6142094887469036e-05\n",
      "Iteration: 2639 Loss: 1.613222049134319e-05\n",
      "Iteration: 2640 Loss: 1.612235129107079e-05\n",
      "Iteration: 2641 Loss: 1.6112487284153482e-05\n",
      "Iteration: 2642 Loss: 1.6102628467959825e-05\n",
      "Iteration: 2643 Loss: 1.6092774839318908e-05\n",
      "Iteration: 2644 Loss: 1.6082926395396812e-05\n",
      "Iteration: 2645 Loss: 1.607308313381424e-05\n",
      "Iteration: 2646 Loss: 1.6063245051623618e-05\n",
      "Iteration: 2647 Loss: 1.605341214584209e-05\n",
      "Iteration: 2648 Loss: 1.6043584413896862e-05\n",
      "Iteration: 2649 Loss: 1.6033761853021093e-05\n",
      "Iteration: 2650 Loss: 1.6023944460533593e-05\n",
      "Iteration: 2651 Loss: 1.601413223331587e-05\n",
      "Iteration: 2652 Loss: 1.600432516894476e-05\n",
      "Iteration: 2653 Loss: 1.5994523264335178e-05\n",
      "Iteration: 2654 Loss: 1.5984726516807345e-05\n",
      "Iteration: 2655 Loss: 1.5974934923676176e-05\n",
      "Iteration: 2656 Loss: 1.5965148482213887e-05\n",
      "Iteration: 2657 Loss: 1.595536718958369e-05\n",
      "Iteration: 2658 Loss: 1.5945591042857605e-05\n",
      "Iteration: 2659 Loss: 1.5935820039641554e-05\n",
      "Iteration: 2660 Loss: 1.5926054176887358e-05\n",
      "Iteration: 2661 Loss: 1.5916293451799605e-05\n",
      "Iteration: 2662 Loss: 1.5906537861819956e-05\n",
      "Iteration: 2663 Loss: 1.589678740421447e-05\n",
      "Iteration: 2664 Loss: 1.5887042076061824e-05\n",
      "Iteration: 2665 Loss: 1.5877301874843083e-05\n",
      "Iteration: 2666 Loss: 1.5867566797564855e-05\n",
      "Iteration: 2667 Loss: 1.585783684147617e-05\n",
      "Iteration: 2668 Loss: 1.584811200396313e-05\n",
      "Iteration: 2669 Loss: 1.5838392282413485e-05\n",
      "Iteration: 2670 Loss: 1.582867767387888e-05\n",
      "Iteration: 2671 Loss: 1.5818968175628715e-05\n",
      "Iteration: 2672 Loss: 1.5809263785088764e-05\n",
      "Iteration: 2673 Loss: 1.5799564499405152e-05\n",
      "Iteration: 2674 Loss: 1.5789870315750075e-05\n",
      "Iteration: 2675 Loss: 1.5780181231650036e-05\n",
      "Iteration: 2676 Loss: 1.577049724409061e-05\n",
      "Iteration: 2677 Loss: 1.576081835046878e-05\n",
      "Iteration: 2678 Loss: 1.5751144547976763e-05\n",
      "Iteration: 2679 Loss: 1.574147583384298e-05\n",
      "Iteration: 2680 Loss: 1.573181220557793e-05\n",
      "Iteration: 2681 Loss: 1.572215366039685e-05\n",
      "Iteration: 2682 Loss: 1.5712500195563217e-05\n",
      "Iteration: 2683 Loss: 1.570285180825348e-05\n",
      "Iteration: 2684 Loss: 1.5693208495938332e-05\n",
      "Iteration: 2685 Loss: 1.5683570255642046e-05\n",
      "Iteration: 2686 Loss: 1.5673937085154442e-05\n",
      "Iteration: 2687 Loss: 1.566430898127298e-05\n",
      "Iteration: 2688 Loss: 1.565468594140649e-05\n",
      "Iteration: 2689 Loss: 1.5645067962861382e-05\n",
      "Iteration: 2690 Loss: 1.5635455043082806e-05\n",
      "Iteration: 2691 Loss: 1.562584717907452e-05\n",
      "Iteration: 2692 Loss: 1.5616244368483325e-05\n",
      "Iteration: 2693 Loss: 1.5606646608394635e-05\n",
      "Iteration: 2694 Loss: 1.5597053896066846e-05\n",
      "Iteration: 2695 Loss: 1.55874662288323e-05\n",
      "Iteration: 2696 Loss: 1.557788360408047e-05\n",
      "Iteration: 2697 Loss: 1.5568306019184288e-05\n",
      "Iteration: 2698 Loss: 1.5558733471452854e-05\n",
      "Iteration: 2699 Loss: 1.554916595809665e-05\n",
      "Iteration: 2700 Loss: 1.553960347641566e-05\n",
      "Iteration: 2701 Loss: 1.5530046023766624e-05\n",
      "Iteration: 2702 Loss: 1.5520493597423192e-05\n",
      "Iteration: 2703 Loss: 1.5510946194318306e-05\n",
      "Iteration: 2704 Loss: 1.5501403812398172e-05\n",
      "Iteration: 2705 Loss: 1.5491866449006863e-05\n",
      "Iteration: 2706 Loss: 1.5482334101049745e-05\n",
      "Iteration: 2707 Loss: 1.5472806765870726e-05\n",
      "Iteration: 2708 Loss: 1.546328444110168e-05\n",
      "Iteration: 2709 Loss: 1.54537671237471e-05\n",
      "Iteration: 2710 Loss: 1.5444254811291533e-05\n",
      "Iteration: 2711 Loss: 1.543474750137129e-05\n",
      "Iteration: 2712 Loss: 1.5425245190899203e-05\n",
      "Iteration: 2713 Loss: 1.5415747877389863e-05\n",
      "Iteration: 2714 Loss: 1.5406255558241542e-05\n",
      "Iteration: 2715 Loss: 1.5396768230754645e-05\n",
      "Iteration: 2716 Loss: 1.538728589189576e-05\n",
      "Iteration: 2717 Loss: 1.5377808539309095e-05\n",
      "Iteration: 2718 Loss: 1.536833617020673e-05\n",
      "Iteration: 2719 Loss: 1.535886878239223e-05\n",
      "Iteration: 2720 Loss: 1.534940637284644e-05\n",
      "Iteration: 2721 Loss: 1.533994893900806e-05\n",
      "Iteration: 2722 Loss: 1.5330496478165512e-05\n",
      "Iteration: 2723 Loss: 1.532104898771875e-05\n",
      "Iteration: 2724 Loss: 1.531160646502979e-05\n",
      "Iteration: 2725 Loss: 1.530216890737697e-05\n",
      "Iteration: 2726 Loss: 1.5292736312094133e-05\n",
      "Iteration: 2727 Loss: 1.5283308676685347e-05\n",
      "Iteration: 2728 Loss: 1.5273885998272644e-05\n",
      "Iteration: 2729 Loss: 1.5264468274251706e-05\n",
      "Iteration: 2730 Loss: 1.5255055502477431e-05\n",
      "Iteration: 2731 Loss: 1.5245647679861902e-05\n",
      "Iteration: 2732 Loss: 1.5236244803708552e-05\n",
      "Iteration: 2733 Loss: 1.5226846871556388e-05\n",
      "Iteration: 2734 Loss: 1.5217453880666157e-05\n",
      "Iteration: 2735 Loss: 1.5208065828611797e-05\n",
      "Iteration: 2736 Loss: 1.5198682712530383e-05\n",
      "Iteration: 2737 Loss: 1.5189304530067128e-05\n",
      "Iteration: 2738 Loss: 1.5179931278540852e-05\n",
      "Iteration: 2739 Loss: 1.5170562955087339e-05\n",
      "Iteration: 2740 Loss: 1.5161199557258585e-05\n",
      "Iteration: 2741 Loss: 1.5151841082321272e-05\n",
      "Iteration: 2742 Loss: 1.5142487527881472e-05\n",
      "Iteration: 2743 Loss: 1.5133138891172986e-05\n",
      "Iteration: 2744 Loss: 1.512379516959722e-05\n",
      "Iteration: 2745 Loss: 1.5114456360399e-05\n",
      "Iteration: 2746 Loss: 1.5105122461230825e-05\n",
      "Iteration: 2747 Loss: 1.5095793469441017e-05\n",
      "Iteration: 2748 Loss: 1.5086469382428634e-05\n",
      "Iteration: 2749 Loss: 1.507715019749369e-05\n",
      "Iteration: 2750 Loss: 1.5067835912164794e-05\n",
      "Iteration: 2751 Loss: 1.5058526523667594e-05\n",
      "Iteration: 2752 Loss: 1.5049222029356493e-05\n",
      "Iteration: 2753 Loss: 1.5039922426670236e-05\n",
      "Iteration: 2754 Loss: 1.5030627713077845e-05\n",
      "Iteration: 2755 Loss: 1.5021337886127539e-05\n",
      "Iteration: 2756 Loss: 1.5012052943066743e-05\n",
      "Iteration: 2757 Loss: 1.500277288126299e-05\n",
      "Iteration: 2758 Loss: 1.4993497698358261e-05\n",
      "Iteration: 2759 Loss: 1.4984227391613796e-05\n",
      "Iteration: 2760 Loss: 1.4974961958358284e-05\n",
      "Iteration: 2761 Loss: 1.4965701396087657e-05\n",
      "Iteration: 2762 Loss: 1.4956445702242985e-05\n",
      "Iteration: 2763 Loss: 1.4947194874212535e-05\n",
      "Iteration: 2764 Loss: 1.4937948909403747e-05\n",
      "Iteration: 2765 Loss: 1.4928707805330682e-05\n",
      "Iteration: 2766 Loss: 1.4919471559346246e-05\n",
      "Iteration: 2767 Loss: 1.4910240168719183e-05\n",
      "Iteration: 2768 Loss: 1.4901013631127648e-05\n",
      "Iteration: 2769 Loss: 1.4891791943703397e-05\n",
      "Iteration: 2770 Loss: 1.4882575104344363e-05\n",
      "Iteration: 2771 Loss: 1.4873363110138387e-05\n",
      "Iteration: 2772 Loss: 1.4864155958618567e-05\n",
      "Iteration: 2773 Loss: 1.4854953647288972e-05\n",
      "Iteration: 2774 Loss: 1.4845756173540604e-05\n",
      "Iteration: 2775 Loss: 1.4836563534729296e-05\n",
      "Iteration: 2776 Loss: 1.4827375728250127e-05\n",
      "Iteration: 2777 Loss: 1.4818192751672385e-05\n",
      "Iteration: 2778 Loss: 1.4809014602699534e-05\n",
      "Iteration: 2779 Loss: 1.4799841278419995e-05\n",
      "Iteration: 2780 Loss: 1.4790672776537489e-05\n",
      "Iteration: 2781 Loss: 1.4781509094287334e-05\n",
      "Iteration: 2782 Loss: 1.4772350228917912e-05\n",
      "Iteration: 2783 Loss: 1.4763196178181656e-05\n",
      "Iteration: 2784 Loss: 1.4754046939537634e-05\n",
      "Iteration: 2785 Loss: 1.4744902510740846e-05\n",
      "Iteration: 2786 Loss: 1.4735762889058426e-05\n",
      "Iteration: 2787 Loss: 1.4726628071635774e-05\n",
      "Iteration: 2788 Loss: 1.4717498056329868e-05\n",
      "Iteration: 2789 Loss: 1.4708372840735238e-05\n",
      "Iteration: 2790 Loss: 1.4699252422307909e-05\n",
      "Iteration: 2791 Loss: 1.4690136798166243e-05\n",
      "Iteration: 2792 Loss: 1.4681025966252708e-05\n",
      "Iteration: 2793 Loss: 1.4671919923823737e-05\n",
      "Iteration: 2794 Loss: 1.4662818668384451e-05\n",
      "Iteration: 2795 Loss: 1.4653722197368052e-05\n",
      "Iteration: 2796 Loss: 1.4644630508617385e-05\n",
      "Iteration: 2797 Loss: 1.463554359956212e-05\n",
      "Iteration: 2798 Loss: 1.4626461467663466e-05\n",
      "Iteration: 2799 Loss: 1.4617384110995947e-05\n",
      "Iteration: 2800 Loss: 1.4608311526872852e-05\n",
      "Iteration: 2801 Loss: 1.4599243712593838e-05\n",
      "Iteration: 2802 Loss: 1.459018066644275e-05\n",
      "Iteration: 2803 Loss: 1.4581122385898751e-05\n",
      "Iteration: 2804 Loss: 1.4572068868762768e-05\n",
      "Iteration: 2805 Loss: 1.4563020113186235e-05\n",
      "Iteration: 2806 Loss: 1.4553976116812002e-05\n",
      "Iteration: 2807 Loss: 1.454493687816745e-05\n",
      "Iteration: 2808 Loss: 1.4535902395022129e-05\n",
      "Iteration: 2809 Loss: 1.4526872666019963e-05\n",
      "Iteration: 2810 Loss: 1.4517847690083599e-05\n",
      "Iteration: 2811 Loss: 1.450882746637922e-05\n",
      "Iteration: 2812 Loss: 1.449981199525343e-05\n",
      "Iteration: 2813 Loss: 1.4490801277316979e-05\n",
      "Iteration: 2814 Loss: 1.4481795315480081e-05\n",
      "Iteration: 2815 Loss: 1.4472794114292836e-05\n",
      "Iteration: 2816 Loss: 1.4463797684007545e-05\n",
      "Iteration: 2817 Loss: 1.4454806042728864e-05\n",
      "Iteration: 2818 Loss: 1.4445819226487532e-05\n",
      "Iteration: 2819 Loss: 1.4436837311707418e-05\n",
      "Iteration: 2820 Loss: 1.442786049107111e-05\n",
      "Iteration: 2821 Loss: 1.4418889399605306e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2822 Loss: 1.4409928065246318e-05\n",
      "Iteration: 2823 Loss: 1.4256792609835707e-05\n",
      "Iteration: 2824 Loss: 1.3574759770423325e-05\n",
      "Iteration: 2825 Loss: 1.2487862337175585e-05\n",
      "Iteration: 2826 Loss: 1.1120415019042104e-05\n",
      "Iteration: 2827 Loss: 9.583041035425202e-06\n",
      "Iteration: 2828 Loss: 7.971016507991355e-06\n",
      "Iteration: 2829 Loss: 6.363561566962442e-06\n",
      "Iteration: 2830 Loss: 4.823841249981002e-06\n",
      "Iteration: 2831 Loss: 3.399693041479925e-06\n",
      "Iteration: 2832 Loss: 2.124789868654993e-06\n",
      "Iteration: 2833 Loss: 1.0201209039955709e-06\n",
      "Iteration: 2834 Loss: 3.236524683124801e-07\n",
      "Iteration: 2835 Loss: 6.69162335656813e-07\n",
      "Iteration: 2836 Loss: 1.2336712488269003e-06\n",
      "Iteration: 2837 Loss: 1.6360383589772957e-06\n",
      "Iteration: 2838 Loss: 1.8926386757218621e-06\n",
      "Iteration: 2839 Loss: 2.02239007177435e-06\n",
      "Iteration: 2840 Loss: 2.0456483364000337e-06\n",
      "Iteration: 2841 Loss: 1.9833663577461286e-06\n",
      "Iteration: 2842 Loss: 1.855398422410907e-06\n",
      "Iteration: 2843 Loss: 1.679416172670959e-06\n",
      "Iteration: 2844 Loss: 1.4716546795099577e-06\n",
      "Iteration: 2845 Loss: 1.2467543858746366e-06\n",
      "Iteration: 2846 Loss: 1.0171127803374077e-06\n",
      "Iteration: 2847 Loss: 7.928343279706386e-07\n",
      "Iteration: 2848 Loss: 5.817904983692063e-07\n",
      "Iteration: 2849 Loss: 3.897511746933943e-07\n",
      "Iteration: 2850 Loss: 2.2057456373216758e-07\n",
      "Iteration: 2851 Loss: 7.643558837565073e-08\n",
      "Iteration: 2852 Loss: 4.602614058453724e-08\n",
      "Iteration: 2853 Loss: 1.382841695516662e-07\n",
      "Iteration: 2854 Loss: 2.0657259588411385e-07\n",
      "Iteration: 2855 Loss: 2.5296109472300473e-07\n",
      "Iteration: 2856 Loss: 2.79979323377771e-07\n",
      "Iteration: 2857 Loss: 2.9044731562792306e-07\n",
      "Iteration: 2858 Loss: 2.872637699534269e-07\n",
      "Iteration: 2859 Loss: 2.7340140603150647e-07\n",
      "Iteration: 2860 Loss: 2.515771134441864e-07\n",
      "Iteration: 2861 Loss: 2.240846906431463e-07\n",
      "Iteration: 2862 Loss: 1.931539287484693e-07\n",
      "Iteration: 2863 Loss: 1.6070601760558633e-07\n",
      "Iteration: 2864 Loss: 1.2833850150085414e-07\n",
      "Iteration: 2865 Loss: 9.732626309812782e-08\n",
      "Iteration: 2866 Loss: 6.863519063727694e-08\n",
      "Iteration: 2867 Loss: 4.2945441161458605e-08\n",
      "Iteration: 2868 Loss: 2.0681449699043264e-08\n",
      "Iteration: 2869 Loss: 2.0461877782161916e-09\n",
      "Iteration: 2870 Loss: 1.35758303742978e-08\n",
      "Iteration: 2871 Loss: 2.4931229711565258e-08\n",
      "Iteration: 2872 Loss: 3.302249992672469e-08\n",
      "Iteration: 2873 Loss: 3.81797780671374e-08\n",
      "Iteration: 2874 Loss: 4.078644446745125e-08\n",
      "Iteration: 2875 Loss: 4.124942076920046e-08\n",
      "Iteration: 2876 Loss: 3.998559441545045e-08\n",
      "Iteration: 2877 Loss: 3.74170309887514e-08\n",
      "Iteration: 2878 Loss: 3.3876610903031924e-08\n",
      "Iteration: 2879 Loss: 2.969478087520073e-08\n",
      "Iteration: 2880 Loss: 2.5166560549972912e-08\n",
      "Iteration: 2881 Loss: 2.0541822628442697e-08\n",
      "Iteration: 2882 Loss: 1.6024329552664614e-08\n",
      "Iteration: 2883 Loss: 1.177282038161597e-08\n",
      "Iteration: 2884 Loss: 7.903683060759136e-09\n",
      "Iteration: 2885 Loss: 4.494781502012965e-09\n",
      "Iteration: 2886 Loss: 1.5900767052961193e-09\n",
      "Iteration: 2887 Loss: 9.55230487679481e-10\n",
      "Iteration: 2888 Loss: 2.8123314079465517e-09\n",
      "Iteration: 2889 Loss: 4.186878011512714e-09\n",
      "Iteration: 2890 Loss: 5.120507406383723e-09\n",
      "Iteration: 2891 Loss: 5.664187423120456e-09\n",
      "Iteration: 2892 Loss: 5.874409087272092e-09\n",
      "Iteration: 2893 Loss: 5.810283639883345e-09\n",
      "Iteration: 2894 Loss: 5.532511235391733e-09\n",
      "Iteration: 2895 Loss: 5.09256750396366e-09\n",
      "Iteration: 2896 Loss: 4.538541377038357e-09\n",
      "Iteration: 2897 Loss: 3.9153170416167155e-09\n",
      "Iteration: 2898 Loss: 3.261572107549859e-09\n",
      "Iteration: 2899 Loss: 2.60947538684697e-09\n",
      "Iteration: 2900 Loss: 1.984704029259084e-09\n",
      "Iteration: 2901 Loss: 1.4067166202467053e-09\n",
      "Iteration: 2902 Loss: 8.892248943895057e-10\n",
      "Iteration: 2903 Loss: 4.4083559650392046e-10\n",
      "Iteration: 2904 Loss: 6.865981238995124e-11\n",
      "Iteration: 2905 Loss: 3.147733479017183e-10\n",
      "Iteration: 2906 Loss: 5.433132277493219e-10\n",
      "Iteration: 2907 Loss: 7.062275527772004e-10\n",
      "Iteration: 2908 Loss: 8.100725246977537e-10\n",
      "Iteration: 2909 Loss: 8.625005546716597e-10\n",
      "Iteration: 2910 Loss: 8.716926953375207e-10\n",
      "Iteration: 2911 Loss: 8.459210406760619e-10\n",
      "Iteration: 2912 Loss: 7.931996170192353e-10\n",
      "Iteration: 2913 Loss: 7.215957370095095e-10\n",
      "Iteration: 2914 Loss: 6.371486203667278e-10\n",
      "Iteration: 2915 Loss: 5.457640272891265e-10\n",
      "Iteration: 2916 Loss: 4.5247463309257673e-10\n",
      "Iteration: 2917 Loss: 3.6138755848643956e-10\n",
      "Iteration: 2918 Loss: 2.757125700538495e-10\n",
      "Iteration: 2919 Loss: 1.9783118170542025e-10\n",
      "Iteration: 2920 Loss: 1.2944043056951729e-10\n",
      "Iteration: 2921 Loss: 7.218259428650158e-11\n",
      "Iteration: 2922 Loss: 4.6785333379288556e-11\n",
      "Iteration: 2923 Loss: 7.861315297920667e-11\n",
      "Iteration: 2924 Loss: 1.0555299275215889e-10\n",
      "Iteration: 2925 Loss: 1.2422800601616869e-10\n",
      "Iteration: 2926 Loss: 1.351666041530881e-10\n",
      "Iteration: 2927 Loss: 1.3942208715099083e-10\n",
      "Iteration: 2928 Loss: 1.3814280573344635e-10\n",
      "Iteration: 2929 Loss: 1.3248205134278188e-10\n",
      "Iteration: 2930 Loss: 1.2354658380538024e-10\n",
      "Iteration: 2931 Loss: 1.123617255579037e-10\n",
      "Iteration: 2932 Loss: 9.985103234421638e-11\n",
      "Iteration: 2933 Loss: 8.683045165517812e-11\n",
      "Iteration: 2934 Loss: 7.401957073514894e-11\n",
      "Iteration: 2935 Loss: 6.209120359309413e-11\n",
      "Iteration: 2936 Loss: 5.1817518448771445e-11\n",
      "Iteration: 2937 Loss: 4.4468384766423505e-11\n",
      "Iteration: 2938 Loss: 4.199170218852522e-11\n",
      "Iteration: 2939 Loss: 4.409724107544883e-11\n",
      "Iteration: 2940 Loss: 4.7744593246385416e-11\n",
      "Iteration: 2941 Loss: 5.1139370777829035e-11\n",
      "Iteration: 2942 Loss: 5.371964109714998e-11\n",
      "Iteration: 2943 Loss: 5.5372622131812975e-11\n",
      "Iteration: 2944 Loss: 5.614895140690308e-11\n",
      "Iteration: 2945 Loss: 5.616419419847998e-11\n",
      "Iteration: 2946 Loss: 5.5560304595402473e-11\n",
      "Iteration: 2947 Loss: 5.4486396847867635e-11\n",
      "Iteration: 2948 Loss: 5.308702530432344e-11\n",
      "Iteration: 2949 Loss: 5.149533194552041e-11\n",
      "Iteration: 2950 Loss: 4.982836087277125e-11\n",
      "Iteration: 2951 Loss: 4.81837797558244e-11\n",
      "Iteration: 2952 Loss: 4.663728388281756e-11\n",
      "Iteration: 2953 Loss: 4.524185217500872e-11\n",
      "Iteration: 2954 Loss: 4.4028288233760694e-11\n",
      "Iteration: 2955 Loss: 4.3006851495900975e-11\n",
      "Iteration: 2956 Loss: 4.217140211938368e-11\n",
      "Iteration: 2957 Loss: 4.150399832017409e-11\n",
      "Iteration: 2958 Loss: 4.098045721092612e-11\n",
      "Iteration: 2959 Loss: 4.057649107415591e-11\n",
      "Iteration: 2960 Loss: 4.0268717066809216e-11\n",
      "Iteration: 2961 Loss: 4.0037935247005264e-11\n",
      "Iteration: 2962 Loss: 3.9868998102248083e-11\n",
      "Iteration: 2963 Loss: 3.974953250364246e-11\n",
      "Iteration: 2964 Loss: 3.9669538193863485e-11\n",
      "Iteration: 2965 Loss: 3.9620719800440625e-11\n",
      "Iteration: 2966 Loss: 3.959523995473178e-11\n",
      "Iteration: 2967 Loss: 3.958621752415297e-11\n",
      "Iteration: 2968 Loss: 3.958700124992376e-11\n",
      "Iteration: 2969 Loss: 3.9591401724491183e-11\n",
      "Iteration: 2970 Loss: 3.959417720381812e-11\n",
      "Iteration: 2971 Loss: 3.959039339489881e-11\n",
      "Iteration: 2972 Loss: 3.9577086950474156e-11\n",
      "Iteration: 2973 Loss: 3.955121021714142e-11\n",
      "Iteration: 2974 Loss: 3.951149698492757e-11\n",
      "Iteration: 2975 Loss: 3.945704775579199e-11\n",
      "Iteration: 2976 Loss: 3.9388213071459333e-11\n",
      "Iteration: 2977 Loss: 3.9305618210497156e-11\n",
      "Iteration: 2978 Loss: 3.9210467926142056e-11\n",
      "Iteration: 2979 Loss: 3.910471167241063e-11\n",
      "Iteration: 2980 Loss: 3.899008868807543e-11\n",
      "Iteration: 2981 Loss: 3.886828016016452e-11\n",
      "Iteration: 2982 Loss: 3.8741129398399954e-11\n",
      "Iteration: 2983 Loss: 3.8610519091842134e-11\n",
      "Iteration: 2984 Loss: 3.847793686218602e-11\n",
      "Iteration: 2985 Loss: 3.834455729066046e-11\n",
      "Iteration: 2986 Loss: 3.821173855721197e-11\n",
      "Iteration: 2987 Loss: 3.8080282294337714e-11\n",
      "Iteration: 2988 Loss: 3.795077263764663e-11\n",
      "Iteration: 2989 Loss: 3.7823908328060916e-11\n",
      "Iteration: 2990 Loss: 3.7699957232486624e-11\n",
      "Iteration: 2991 Loss: 3.757908495696105e-11\n",
      "Iteration: 2992 Loss: 3.746126233307856e-11\n",
      "Iteration: 2993 Loss: 3.7346563088133347e-11\n",
      "Iteration: 2994 Loss: 3.723478710177915e-11\n",
      "Iteration: 2995 Loss: 3.712563546023983e-11\n",
      "Iteration: 2996 Loss: 3.7019074597647635e-11\n",
      "Iteration: 2997 Loss: 3.6914862586593975e-11\n",
      "Iteration: 2998 Loss: 3.6812728451910034e-11\n",
      "Iteration: 2999 Loss: 3.671208652165323e-11\n",
      "Iteration: 3000 Loss: 3.6612882973835896e-11\n",
      "Iteration: 3001 Loss: 3.651490417886905e-11\n",
      "Iteration: 3002 Loss: 3.6417953873655015e-11\n",
      "Iteration: 3003 Loss: 3.632185423965784e-11\n",
      "Iteration: 3004 Loss: 3.6226399396677277e-11\n",
      "Iteration: 3005 Loss: 3.6131562131186456e-11\n",
      "Iteration: 3006 Loss: 3.603720322522407e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3007 Loss: 3.594327420845773e-11\n",
      "Iteration: 3008 Loss: 3.5849698950952667e-11\n",
      "Iteration: 3009 Loss: 3.575651243696858e-11\n",
      "Iteration: 3010 Loss: 3.566365939495602e-11\n",
      "Iteration: 3011 Loss: 3.5571175490413994e-11\n",
      "Iteration: 3012 Loss: 3.547903949316216e-11\n",
      "Iteration: 3013 Loss: 3.538730407013714e-11\n",
      "Iteration: 3014 Loss: 3.5296023456304926e-11\n",
      "Iteration: 3015 Loss: 3.520518814427959e-11\n",
      "Iteration: 3016 Loss: 3.511483392997472e-11\n",
      "Iteration: 3017 Loss: 3.502499516047954e-11\n",
      "Iteration: 3018 Loss: 3.493572351145527e-11\n",
      "Iteration: 3019 Loss: 3.4847092843552e-11\n",
      "Iteration: 3020 Loss: 3.475897052157044e-11\n",
      "Iteration: 3021 Loss: 3.467148235468832e-11\n",
      "Iteration: 3022 Loss: 3.458465081970752e-11\n",
      "Iteration: 3023 Loss: 3.4498452958657e-11\n",
      "Iteration: 3024 Loss: 3.441291847646174e-11\n",
      "Iteration: 3025 Loss: 3.4327993168702036e-11\n",
      "Iteration: 3026 Loss: 3.4243733495929156e-11\n",
      "Iteration: 3027 Loss: 3.4160124717823615e-11\n",
      "Iteration: 3028 Loss: 3.407711579611833e-11\n",
      "Iteration: 3029 Loss: 3.3994759202523545e-11\n",
      "Iteration: 3030 Loss: 3.3913013268512e-11\n",
      "Iteration: 3031 Loss: 3.383190943187774e-11\n",
      "Iteration: 3032 Loss: 3.3751399584682457e-11\n",
      "Iteration: 3033 Loss: 3.3671364221047065e-11\n",
      "Iteration: 3034 Loss: 3.3592108659423065e-11\n",
      "Iteration: 3035 Loss: 3.351334174142413e-11\n",
      "Iteration: 3036 Loss: 3.3435240468731286e-11\n",
      "Iteration: 3037 Loss: 3.335767139107637e-11\n",
      "Iteration: 3038 Loss: 3.328066637101511e-11\n",
      "Iteration: 3039 Loss: 3.3204158000613886e-11\n",
      "Iteration: 3040 Loss: 3.312822563166292e-11\n",
      "Iteration: 3041 Loss: 3.3052908077764985e-11\n",
      "Iteration: 3042 Loss: 3.2978045022590336e-11\n",
      "Iteration: 3043 Loss: 3.29037197475187e-11\n",
      "Iteration: 3044 Loss: 3.2829879763800663e-11\n",
      "Iteration: 3045 Loss: 3.2756569207709707e-11\n",
      "Iteration: 3046 Loss: 3.2683769946305996e-11\n",
      "Iteration: 3047 Loss: 3.2611458429130425e-11\n",
      "Iteration: 3048 Loss: 3.2539755591768984e-11\n",
      "Iteration: 3049 Loss: 3.246853320967673e-11\n",
      "Iteration: 3050 Loss: 3.239775146126184e-11\n",
      "Iteration: 3051 Loss: 3.232750711481406e-11\n",
      "Iteration: 3052 Loss: 3.225775638198165e-11\n",
      "Iteration: 3053 Loss: 3.2188481530198574e-11\n",
      "Iteration: 3054 Loss: 3.211969643788284e-11\n",
      "Iteration: 3055 Loss: 3.205140486102015e-11\n",
      "Iteration: 3056 Loss: 3.19836246784219e-11\n",
      "Iteration: 3057 Loss: 3.191631060041237e-11\n",
      "Iteration: 3058 Loss: 3.184948931331085e-11\n",
      "Iteration: 3059 Loss: 3.1783098889767893e-11\n",
      "Iteration: 3060 Loss: 3.171722914134849e-11\n",
      "Iteration: 3061 Loss: 3.16518677490184e-11\n",
      "Iteration: 3062 Loss: 3.158695644244566e-11\n",
      "Iteration: 3063 Loss: 3.152245550821889e-11\n",
      "Iteration: 3064 Loss: 3.145847614564877e-11\n",
      "Iteration: 3065 Loss: 3.139490739277914e-11\n",
      "Iteration: 3066 Loss: 3.1331804360062464e-11\n",
      "Iteration: 3067 Loss: 3.126915936882731e-11\n",
      "Iteration: 3068 Loss: 3.120693552745791e-11\n",
      "Iteration: 3069 Loss: 3.1145175444408326e-11\n",
      "Iteration: 3070 Loss: 3.108386509128846e-11\n",
      "Iteration: 3071 Loss: 3.102295682529896e-11\n",
      "Iteration: 3072 Loss: 3.0962414902938505e-11\n",
      "Iteration: 3073 Loss: 3.0902491498431515e-11\n",
      "Iteration: 3074 Loss: 3.084287290404362e-11\n",
      "Iteration: 3075 Loss: 3.078366951540535e-11\n",
      "Iteration: 3076 Loss: 3.07249739699579e-11\n",
      "Iteration: 3077 Loss: 3.066661507021075e-11\n",
      "Iteration: 3078 Loss: 3.0608686018915445e-11\n",
      "Iteration: 3079 Loss: 3.055117815840098e-11\n",
      "Iteration: 3080 Loss: 3.049409294385749e-11\n",
      "Iteration: 3081 Loss: 3.043739874181664e-11\n",
      "Iteration: 3082 Loss: 3.038111474203846e-11\n",
      "Iteration: 3083 Loss: 3.032524570589139e-11\n",
      "Iteration: 3084 Loss: 3.02697690196815e-11\n",
      "Iteration: 3085 Loss: 3.0214688098048565e-11\n",
      "Iteration: 3086 Loss: 3.015999580674596e-11\n",
      "Iteration: 3087 Loss: 3.01057056033135e-11\n",
      "Iteration: 3088 Loss: 3.005176992882301e-11\n",
      "Iteration: 3089 Loss: 2.9998252005562226e-11\n",
      "Iteration: 3090 Loss: 2.9945075963461046e-11\n",
      "Iteration: 3091 Loss: 2.98922935435499e-11\n",
      "Iteration: 3092 Loss: 2.9839883662286895e-11\n",
      "Iteration: 3093 Loss: 2.978787107664261e-11\n",
      "Iteration: 3094 Loss: 2.973623317820214e-11\n",
      "Iteration: 3095 Loss: 2.968497004101846e-11\n",
      "Iteration: 3096 Loss: 2.9634020501155535e-11\n",
      "Iteration: 3097 Loss: 2.958344936491833e-11\n",
      "Iteration: 3098 Loss: 2.9533306445710217e-11\n",
      "Iteration: 3099 Loss: 2.948343787154641e-11\n",
      "Iteration: 3100 Loss: 2.943394781285146e-11\n",
      "Iteration: 3101 Loss: 2.938481774146979e-11\n",
      "Iteration: 3102 Loss: 2.9336029994083385e-11\n",
      "Iteration: 3103 Loss: 2.9287602121888335e-11\n",
      "Iteration: 3104 Loss: 2.923949270578261e-11\n",
      "Iteration: 3105 Loss: 2.919164015927939e-11\n",
      "Iteration: 3106 Loss: 2.914424305060728e-11\n",
      "Iteration: 3107 Loss: 2.909713850062932e-11\n",
      "Iteration: 3108 Loss: 2.9050428010784817e-11\n",
      "Iteration: 3109 Loss: 2.900411333126305e-11\n",
      "Iteration: 3110 Loss: 2.895802006463708e-11\n",
      "Iteration: 3111 Loss: 2.8912235186648925e-11\n",
      "Iteration: 3112 Loss: 2.886679984121632e-11\n",
      "Iteration: 3113 Loss: 2.8821691584113742e-11\n",
      "Iteration: 3114 Loss: 2.8776894568883356e-11\n",
      "Iteration: 3115 Loss: 2.873243404475452e-11\n",
      "Iteration: 3116 Loss: 2.868828706254796e-11\n",
      "Iteration: 3117 Loss: 2.864456017240456e-11\n",
      "Iteration: 3118 Loss: 2.860109526179455e-11\n",
      "Iteration: 3119 Loss: 2.8557850346613947e-11\n",
      "Iteration: 3120 Loss: 2.8515013157618428e-11\n",
      "Iteration: 3121 Loss: 2.8472386557254994e-11\n",
      "Iteration: 3122 Loss: 2.843009929667289e-11\n",
      "Iteration: 3123 Loss: 2.838810713586288e-11\n",
      "Iteration: 3124 Loss: 2.83464274048389e-11\n",
      "Iteration: 3125 Loss: 2.830508620653676e-11\n",
      "Iteration: 3126 Loss: 2.8263951484717e-11\n",
      "Iteration: 3127 Loss: 2.822309878747763e-11\n",
      "Iteration: 3128 Loss: 2.818257376368801e-11\n",
      "Iteration: 3129 Loss: 2.814235758402951e-11\n",
      "Iteration: 3130 Loss: 2.810243072229707e-11\n",
      "Iteration: 3131 Loss: 2.8062777903794157e-11\n",
      "Iteration: 3132 Loss: 2.8023420817351862e-11\n",
      "Iteration: 3133 Loss: 2.7984444948386357e-11\n",
      "Iteration: 3134 Loss: 2.7945616721122074e-11\n",
      "Iteration: 3135 Loss: 2.7907051245046564e-11\n",
      "Iteration: 3136 Loss: 2.7868798556177374e-11\n",
      "Iteration: 3137 Loss: 2.783079732286846e-11\n",
      "Iteration: 3138 Loss: 2.7793061932211795e-11\n",
      "Iteration: 3139 Loss: 2.775558333762233e-11\n",
      "Iteration: 3140 Loss: 2.771838182540863e-11\n",
      "Iteration: 3141 Loss: 2.7681447439601143e-11\n",
      "Iteration: 3142 Loss: 2.7644778996679878e-11\n",
      "Iteration: 3143 Loss: 2.7608373015064138e-11\n",
      "Iteration: 3144 Loss: 2.757232542365911e-11\n",
      "Iteration: 3145 Loss: 2.7536408676886332e-11\n",
      "Iteration: 3146 Loss: 2.7500703016218347e-11\n",
      "Iteration: 3147 Loss: 2.7465321728805874e-11\n",
      "Iteration: 3148 Loss: 2.7430192496448856e-11\n",
      "Iteration: 3149 Loss: 2.739537358274855e-11\n",
      "Iteration: 3150 Loss: 2.7360723064296714e-11\n",
      "Iteration: 3151 Loss: 2.7326345716315477e-11\n",
      "Iteration: 3152 Loss: 2.729220463494774e-11\n",
      "Iteration: 3153 Loss: 2.7258217003084947e-11\n",
      "Iteration: 3154 Loss: 2.722456068341104e-11\n",
      "Iteration: 3155 Loss: 2.7191158829774707e-11\n",
      "Iteration: 3156 Loss: 2.715799594746226e-11\n",
      "Iteration: 3157 Loss: 2.7125064510905955e-11\n",
      "Iteration: 3158 Loss: 2.709236542742665e-11\n",
      "Iteration: 3159 Loss: 2.7059914827223455e-11\n",
      "Iteration: 3160 Loss: 2.702778119380179e-11\n",
      "Iteration: 3161 Loss: 2.6995778625452913e-11\n",
      "Iteration: 3162 Loss: 2.6964056360593392e-11\n",
      "Iteration: 3163 Loss: 2.6932486805693234e-11\n",
      "Iteration: 3164 Loss: 2.6901174639349155e-11\n",
      "Iteration: 3165 Loss: 2.6870067764154313e-11\n",
      "Iteration: 3166 Loss: 2.6839189204144248e-11\n",
      "Iteration: 3167 Loss: 2.680854570874911e-11\n",
      "Iteration: 3168 Loss: 2.677807453820923e-11\n",
      "Iteration: 3169 Loss: 2.6747826966444598e-11\n",
      "Iteration: 3170 Loss: 2.6717810312837393e-11\n",
      "Iteration: 3171 Loss: 2.6688014942936873e-11\n",
      "Iteration: 3172 Loss: 2.6658305321367464e-11\n",
      "Iteration: 3173 Loss: 2.6628890199413098e-11\n",
      "Iteration: 3174 Loss: 2.659959738963777e-11\n",
      "Iteration: 3175 Loss: 2.657072344925175e-11\n",
      "Iteration: 3176 Loss: 2.6541968096658315e-11\n",
      "Iteration: 3177 Loss: 2.6513410306637347e-11\n",
      "Iteration: 3178 Loss: 2.648504764515354e-11\n",
      "Iteration: 3179 Loss: 2.645688760439738e-11\n",
      "Iteration: 3180 Loss: 2.6428933628309598e-11\n",
      "Iteration: 3181 Loss: 2.640128903227159e-11\n",
      "Iteration: 3182 Loss: 2.637372569891774e-11\n",
      "Iteration: 3183 Loss: 2.634636433194676e-11\n",
      "Iteration: 3184 Loss: 2.6319206142241868e-11\n",
      "Iteration: 3185 Loss: 2.629222336061004e-11\n",
      "Iteration: 3186 Loss: 2.6265445652825293e-11\n",
      "Iteration: 3187 Loss: 2.6238854257034926e-11\n",
      "Iteration: 3188 Loss: 2.6212453470484464e-11\n",
      "Iteration: 3189 Loss: 2.6186235269105157e-11\n",
      "Iteration: 3190 Loss: 2.616021878845518e-11\n",
      "Iteration: 3191 Loss: 2.613436876253737e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3192 Loss: 2.6108712339705436e-11\n",
      "Iteration: 3193 Loss: 2.6083241462163834e-11\n",
      "Iteration: 3194 Loss: 2.6058009595654826e-11\n",
      "Iteration: 3195 Loss: 2.6032922562354952e-11\n",
      "Iteration: 3196 Loss: 2.600797367060438e-11\n",
      "Iteration: 3197 Loss: 2.5983221997912347e-11\n",
      "Iteration: 3198 Loss: 2.595864265277288e-11\n",
      "Iteration: 3199 Loss: 2.5934238413328738e-11\n",
      "Iteration: 3200 Loss: 2.591017303631403e-11\n",
      "Iteration: 3201 Loss: 2.588609479578993e-11\n",
      "Iteration: 3202 Loss: 2.586220384178617e-11\n",
      "Iteration: 3203 Loss: 2.5838492263398295e-11\n",
      "Iteration: 3204 Loss: 2.5814910583139798e-11\n",
      "Iteration: 3205 Loss: 2.5791497806001545e-11\n",
      "Iteration: 3206 Loss: 2.5768293668671028e-11\n",
      "Iteration: 3207 Loss: 2.574524977852446e-11\n",
      "Iteration: 3208 Loss: 2.572237336477907e-11\n",
      "Iteration: 3209 Loss: 2.56996568048988e-11\n",
      "Iteration: 3210 Loss: 2.567710341444105e-11\n",
      "Iteration: 3211 Loss: 2.5654710207752138e-11\n",
      "Iteration: 3212 Loss: 2.563248618351738e-11\n",
      "Iteration: 3213 Loss: 2.5610410019320102e-11\n",
      "Iteration: 3214 Loss: 2.558849465410078e-11\n",
      "Iteration: 3215 Loss: 2.5566701189211588e-11\n",
      "Iteration: 3216 Loss: 2.5545100258780602e-11\n",
      "Iteration: 3217 Loss: 2.5523646099564577e-11\n",
      "Iteration: 3218 Loss: 2.5502323831343233e-11\n",
      "Iteration: 3219 Loss: 2.5481159062303156e-11\n",
      "Iteration: 3220 Loss: 2.546013611475695e-11\n",
      "Iteration: 3221 Loss: 2.5439291818026815e-11\n",
      "Iteration: 3222 Loss: 2.541854224974752e-11\n",
      "Iteration: 3223 Loss: 2.5397959725869152e-11\n",
      "Iteration: 3224 Loss: 2.5377588142663127e-11\n",
      "Iteration: 3225 Loss: 2.535727235871452e-11\n",
      "Iteration: 3226 Loss: 2.533713920365245e-11\n",
      "Iteration: 3227 Loss: 2.531714402945212e-11\n",
      "Iteration: 3228 Loss: 2.5297294865801314e-11\n",
      "Iteration: 3229 Loss: 2.5277580392377328e-11\n",
      "Iteration: 3230 Loss: 2.5258013539755844e-11\n",
      "Iteration: 3231 Loss: 2.5238537733419522e-11\n",
      "Iteration: 3232 Loss: 2.52192610184556e-11\n",
      "Iteration: 3233 Loss: 2.520010185509154e-11\n",
      "Iteration: 3234 Loss: 2.5181111567382552e-11\n",
      "Iteration: 3235 Loss: 2.516222677629073e-11\n",
      "Iteration: 3236 Loss: 2.51434798237031e-11\n",
      "Iteration: 3237 Loss: 2.5124857951279287e-11\n",
      "Iteration: 3238 Loss: 2.510626951747325e-11\n",
      "Iteration: 3239 Loss: 2.5087923542341577e-11\n",
      "Iteration: 3240 Loss: 2.506970309675457e-11\n",
      "Iteration: 3241 Loss: 2.505150768268746e-11\n",
      "Iteration: 3242 Loss: 2.5033542863450456e-11\n",
      "Iteration: 3243 Loss: 2.501571491571926e-11\n",
      "Iteration: 3244 Loss: 2.49979963948373e-11\n",
      "Iteration: 3245 Loss: 2.498041481036897e-11\n",
      "Iteration: 3246 Loss: 2.496306723084342e-11\n",
      "Iteration: 3247 Loss: 2.494573139732786e-11\n",
      "Iteration: 3248 Loss: 2.4928528512452408e-11\n",
      "Iteration: 3249 Loss: 2.4911435754435835e-11\n",
      "Iteration: 3250 Loss: 2.4894472468098336e-11\n",
      "Iteration: 3251 Loss: 2.4877731973203082e-11\n",
      "Iteration: 3252 Loss: 2.4861017135947535e-11\n",
      "Iteration: 3253 Loss: 2.4844465744607257e-11\n",
      "Iteration: 3254 Loss: 2.4827950405347308e-11\n",
      "Iteration: 3255 Loss: 2.481157947547698e-11\n",
      "Iteration: 3256 Loss: 2.479532322932191e-11\n",
      "Iteration: 3257 Loss: 2.4779186062088254e-11\n",
      "Iteration: 3258 Loss: 2.4763165439799577e-11\n",
      "Iteration: 3259 Loss: 2.4747262929557685e-11\n",
      "Iteration: 3260 Loss: 2.4731460753693187e-11\n",
      "Iteration: 3261 Loss: 2.471578885898734e-11\n",
      "Iteration: 3262 Loss: 2.4700266740627113e-11\n",
      "Iteration: 3263 Loss: 2.4684780425155463e-11\n",
      "Iteration: 3264 Loss: 2.4669426740653842e-11\n",
      "Iteration: 3265 Loss: 2.4654192471599575e-11\n",
      "Iteration: 3266 Loss: 2.4639062420188674e-11\n",
      "Iteration: 3267 Loss: 2.462404356638458e-11\n",
      "Iteration: 3268 Loss: 2.460912037767209e-11\n",
      "Iteration: 3269 Loss: 2.4594283908970596e-11\n",
      "Iteration: 3270 Loss: 2.4579579660154058e-11\n",
      "Iteration: 3271 Loss: 2.4564986133769745e-11\n",
      "Iteration: 3272 Loss: 2.4550512962403967e-11\n",
      "Iteration: 3273 Loss: 2.453611798199879e-11\n",
      "Iteration: 3274 Loss: 2.4521836309387608e-11\n",
      "Iteration: 3275 Loss: 2.4507645089231542e-11\n",
      "Iteration: 3276 Loss: 2.4493556218587687e-11\n",
      "Iteration: 3277 Loss: 2.4479560770495538e-11\n",
      "Iteration: 3278 Loss: 2.4465679063385174e-11\n",
      "Iteration: 3279 Loss: 2.445188412993592e-11\n",
      "Iteration: 3280 Loss: 2.443820177191548e-11\n",
      "Iteration: 3281 Loss: 2.4424661588892836e-11\n",
      "Iteration: 3282 Loss: 2.441113719277588e-11\n",
      "Iteration: 3283 Loss: 2.4397739734092464e-11\n",
      "Iteration: 3284 Loss: 2.4384433288134302e-11\n",
      "Iteration: 3285 Loss: 2.437122325486057e-11\n",
      "Iteration: 3286 Loss: 2.435811682974847e-11\n",
      "Iteration: 3287 Loss: 2.4345086063227646e-11\n",
      "Iteration: 3288 Loss: 2.4332152909262236e-11\n",
      "Iteration: 3289 Loss: 2.431932826504413e-11\n",
      "Iteration: 3290 Loss: 2.430662656550549e-11\n",
      "Iteration: 3291 Loss: 2.429394941376493e-11\n",
      "Iteration: 3292 Loss: 2.4281379248913103e-11\n",
      "Iteration: 3293 Loss: 2.426890388734194e-11\n",
      "Iteration: 3294 Loss: 2.425651753234447e-11\n",
      "Iteration: 3295 Loss: 2.4244221558082615e-11\n",
      "Iteration: 3296 Loss: 2.4232008427076135e-11\n",
      "Iteration: 3297 Loss: 2.4219882536317492e-11\n",
      "Iteration: 3298 Loss: 2.4207848671124587e-11\n",
      "Iteration: 3299 Loss: 2.4195944577483804e-11\n",
      "Iteration: 3300 Loss: 2.418404499599508e-11\n",
      "Iteration: 3301 Loss: 2.417227314967231e-11\n",
      "Iteration: 3302 Loss: 2.4160571524175466e-11\n",
      "Iteration: 3303 Loss: 2.41489473325034e-11\n",
      "Iteration: 3304 Loss: 2.413742075389906e-11\n",
      "Iteration: 3305 Loss: 2.4125939118508567e-11\n",
      "Iteration: 3306 Loss: 2.411457412467989e-11\n",
      "Iteration: 3307 Loss: 2.410336044032135e-11\n",
      "Iteration: 3308 Loss: 2.4092146334556655e-11\n",
      "Iteration: 3309 Loss: 2.4080924736349987e-11\n",
      "Iteration: 3310 Loss: 2.4069879463259092e-11\n",
      "Iteration: 3311 Loss: 2.405892073951321e-11\n",
      "Iteration: 3312 Loss: 2.404798535854825e-11\n",
      "Iteration: 3313 Loss: 2.4037175118786163e-11\n",
      "Iteration: 3314 Loss: 2.4026434247534658e-11\n",
      "Iteration: 3315 Loss: 2.401587625055463e-11\n",
      "Iteration: 3316 Loss: 2.400529313649846e-11\n",
      "Iteration: 3317 Loss: 2.3994684410443038e-11\n",
      "Iteration: 3318 Loss: 2.398421744440673e-11\n",
      "Iteration: 3319 Loss: 2.397386892594645e-11\n",
      "Iteration: 3320 Loss: 2.3963524971782875e-11\n",
      "Iteration: 3321 Loss: 2.3953290151270924e-11\n",
      "Iteration: 3322 Loss: 2.3943120846727864e-11\n",
      "Iteration: 3323 Loss: 2.3933019523985976e-11\n",
      "Iteration: 3324 Loss: 2.3922997035744747e-11\n",
      "Iteration: 3325 Loss: 2.391304508858488e-11\n",
      "Iteration: 3326 Loss: 2.3903213160972484e-11\n",
      "Iteration: 3327 Loss: 2.3893428099231655e-11\n",
      "Iteration: 3328 Loss: 2.3883748611456175e-11\n",
      "Iteration: 3329 Loss: 2.387420353269553e-11\n",
      "Iteration: 3330 Loss: 2.3864605848154854e-11\n",
      "Iteration: 3331 Loss: 2.3855047512790025e-11\n",
      "Iteration: 3332 Loss: 2.3845632839495812e-11\n",
      "Iteration: 3333 Loss: 2.383619549001664e-11\n",
      "Iteration: 3334 Loss: 2.3826822587948532e-11\n",
      "Iteration: 3335 Loss: 2.3817584738029638e-11\n",
      "Iteration: 3336 Loss: 2.380835436538881e-11\n",
      "Iteration: 3337 Loss: 2.3799214859577484e-11\n",
      "Iteration: 3338 Loss: 2.3790048964637342e-11\n",
      "Iteration: 3339 Loss: 2.3781038493940872e-11\n",
      "Iteration: 3340 Loss: 2.3772100061326146e-11\n",
      "Iteration: 3341 Loss: 2.3763225910215918e-11\n",
      "Iteration: 3342 Loss: 2.3754402574779348e-11\n",
      "Iteration: 3343 Loss: 2.3745653864312485e-11\n",
      "Iteration: 3344 Loss: 2.3736964844357663e-11\n",
      "Iteration: 3345 Loss: 2.3728330004133144e-11\n",
      "Iteration: 3346 Loss: 2.3719769388732814e-11\n",
      "Iteration: 3347 Loss: 2.3711260982170538e-11\n",
      "Iteration: 3348 Loss: 2.3702811466777955e-11\n",
      "Iteration: 3349 Loss: 2.3694431913380888e-11\n",
      "Iteration: 3350 Loss: 2.3686098304531015e-11\n",
      "Iteration: 3351 Loss: 2.3677826111012247e-11\n",
      "Iteration: 3352 Loss: 2.366961608155772e-11\n",
      "Iteration: 3353 Loss: 2.3661572065948298e-11\n",
      "Iteration: 3354 Loss: 2.365348273211751e-11\n",
      "Iteration: 3355 Loss: 2.364544699473412e-11\n",
      "Iteration: 3356 Loss: 2.363745889968066e-11\n",
      "Iteration: 3357 Loss: 2.3629610052120128e-11\n",
      "Iteration: 3358 Loss: 2.3621705935463263e-11\n",
      "Iteration: 3359 Loss: 2.3613894241703492e-11\n",
      "Iteration: 3360 Loss: 2.3606245465902427e-11\n",
      "Iteration: 3361 Loss: 2.359854972664296e-11\n",
      "Iteration: 3362 Loss: 2.3590909611739904e-11\n",
      "Iteration: 3363 Loss: 2.3583308761860736e-11\n",
      "Iteration: 3364 Loss: 2.3575833103077034e-11\n",
      "Iteration: 3365 Loss: 2.3568313643601225e-11\n",
      "Iteration: 3366 Loss: 2.356089627981591e-11\n",
      "Iteration: 3367 Loss: 2.3553515060392956e-11\n",
      "Iteration: 3368 Loss: 2.3546192440314548e-11\n",
      "Iteration: 3369 Loss: 2.3538923666300202e-11\n",
      "Iteration: 3370 Loss: 2.3531695027071845e-11\n",
      "Iteration: 3371 Loss: 2.3524528226875004e-11\n",
      "Iteration: 3372 Loss: 2.35174102739139e-11\n",
      "Iteration: 3373 Loss: 2.3510348185867104e-11\n",
      "Iteration: 3374 Loss: 2.3503330756974475e-11\n",
      "Iteration: 3375 Loss: 2.3496367908649925e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3376 Loss: 2.3489440316291843e-11\n",
      "Iteration: 3377 Loss: 2.3482576991196734e-11\n",
      "Iteration: 3378 Loss: 2.3475816529663276e-11\n",
      "Iteration: 3379 Loss: 2.3469062590473664e-11\n",
      "Iteration: 3380 Loss: 2.346231692717524e-11\n",
      "Iteration: 3381 Loss: 2.345563487127029e-11\n",
      "Iteration: 3382 Loss: 2.34490036208791e-11\n",
      "Iteration: 3383 Loss: 2.3442428769943047e-11\n",
      "Iteration: 3384 Loss: 2.3435950611360402e-11\n",
      "Iteration: 3385 Loss: 2.342948512039933e-11\n",
      "Iteration: 3386 Loss: 2.3423006556986643e-11\n",
      "Iteration: 3387 Loss: 2.3416617975835212e-11\n",
      "Iteration: 3388 Loss: 2.3410264824871842e-11\n",
      "Iteration: 3389 Loss: 2.3403962878438398e-11\n",
      "Iteration: 3390 Loss: 2.3397696580815836e-11\n",
      "Iteration: 3391 Loss: 2.3391481129260604e-11\n",
      "Iteration: 3392 Loss: 2.3385304296583695e-11\n",
      "Iteration: 3393 Loss: 2.337915961620606e-11\n",
      "Iteration: 3394 Loss: 2.3373066208154306e-11\n",
      "Iteration: 3395 Loss: 2.3367025455094666e-11\n",
      "Iteration: 3396 Loss: 2.3361023782897182e-11\n",
      "Iteration: 3397 Loss: 2.335507642157115e-11\n",
      "Iteration: 3398 Loss: 2.3349156608914328e-11\n",
      "Iteration: 3399 Loss: 2.3343276510766804e-11\n",
      "Iteration: 3400 Loss: 2.333745604299931e-11\n",
      "Iteration: 3401 Loss: 2.3331667736222706e-11\n",
      "Iteration: 3402 Loss: 2.332591809746332e-11\n",
      "Iteration: 3403 Loss: 2.3320209529538098e-11\n",
      "Iteration: 3404 Loss: 2.331453813497519e-11\n",
      "Iteration: 3405 Loss: 2.3308904762962228e-11\n",
      "Iteration: 3406 Loss: 2.3303325305378525e-11\n",
      "Iteration: 3407 Loss: 2.329778062131313e-11\n",
      "Iteration: 3408 Loss: 2.3292269379535053e-11\n",
      "Iteration: 3409 Loss: 2.3286696245229206e-11\n",
      "Iteration: 3410 Loss: 2.3281271333173642e-11\n",
      "Iteration: 3411 Loss: 2.3275874134690697e-11\n",
      "Iteration: 3412 Loss: 2.3270525792129106e-11\n",
      "Iteration: 3413 Loss: 2.3265207097191667e-11\n",
      "Iteration: 3414 Loss: 2.3259928954289172e-11\n",
      "Iteration: 3415 Loss: 2.3254687540489818e-11\n",
      "Iteration: 3416 Loss: 2.324949390557445e-11\n",
      "Iteration: 3417 Loss: 2.3244429248257797e-11\n",
      "Iteration: 3418 Loss: 2.3239305793583885e-11\n",
      "Iteration: 3419 Loss: 2.32342062998468e-11\n",
      "Iteration: 3420 Loss: 2.322916181453046e-11\n",
      "Iteration: 3421 Loss: 2.32241403890593e-11\n",
      "Iteration: 3422 Loss: 2.3219139352553746e-11\n",
      "Iteration: 3423 Loss: 2.321419599076626e-11\n",
      "Iteration: 3424 Loss: 2.3209287113196576e-11\n",
      "Iteration: 3425 Loss: 2.3204406367846706e-11\n",
      "Iteration: 3426 Loss: 2.3199558059435982e-11\n",
      "Iteration: 3427 Loss: 2.319474183110729e-11\n",
      "Iteration: 3428 Loss: 2.318997841331394e-11\n",
      "Iteration: 3429 Loss: 2.3185234080243904e-11\n",
      "Iteration: 3430 Loss: 2.3180517237944586e-11\n",
      "Iteration: 3431 Loss: 2.3175842189105584e-11\n",
      "Iteration: 3432 Loss: 2.317120409039892e-11\n",
      "Iteration: 3433 Loss: 2.3166584659160072e-11\n",
      "Iteration: 3434 Loss: 2.316201912315875e-11\n",
      "Iteration: 3435 Loss: 2.31574751219894e-11\n",
      "Iteration: 3436 Loss: 2.31529616503118e-11\n",
      "Iteration: 3437 Loss: 2.3148481030119424e-11\n",
      "Iteration: 3438 Loss: 2.3144030697606983e-11\n",
      "Iteration: 3439 Loss: 2.3139609042867964e-11\n",
      "Iteration: 3440 Loss: 2.3135222670088996e-11\n",
      "Iteration: 3441 Loss: 2.3130863249970907e-11\n",
      "Iteration: 3442 Loss: 2.3126541188376657e-11\n",
      "Iteration: 3443 Loss: 2.3122256193526404e-11\n",
      "Iteration: 3444 Loss: 2.311798825527725e-11\n",
      "Iteration: 3445 Loss: 2.311376230994346e-11\n",
      "Iteration: 3446 Loss: 2.310955905505066e-11\n",
      "Iteration: 3447 Loss: 2.310538422010831e-11\n",
      "Iteration: 3448 Loss: 2.310124651478582e-11\n",
      "Iteration: 3449 Loss: 2.309713134451054e-11\n",
      "Iteration: 3450 Loss: 2.309304518782364e-11\n",
      "Iteration: 3451 Loss: 2.308898930353628e-11\n",
      "Iteration: 3452 Loss: 2.308495880456534e-11\n",
      "Iteration: 3453 Loss: 2.3080969054175683e-11\n",
      "Iteration: 3454 Loss: 2.3076991807728973e-11\n",
      "Iteration: 3455 Loss: 2.30730497713093e-11\n",
      "Iteration: 3456 Loss: 2.3069144692377994e-11\n",
      "Iteration: 3457 Loss: 2.3065250797072753e-11\n",
      "Iteration: 3458 Loss: 2.3061392478039468e-11\n",
      "Iteration: 3459 Loss: 2.305756743859487e-11\n",
      "Iteration: 3460 Loss: 2.305375565938538e-11\n",
      "Iteration: 3461 Loss: 2.304999020598332e-11\n",
      "Iteration: 3462 Loss: 2.3046240278731666e-11\n",
      "Iteration: 3463 Loss: 2.304251042090618e-11\n",
      "Iteration: 3464 Loss: 2.3038810587789254e-11\n",
      "Iteration: 3465 Loss: 2.3035146336574093e-11\n",
      "Iteration: 3466 Loss: 2.3031496418270663e-11\n",
      "Iteration: 3467 Loss: 2.302787879171564e-11\n",
      "Iteration: 3468 Loss: 2.3024292487893114e-11\n",
      "Iteration: 3469 Loss: 2.3020720432089174e-11\n",
      "Iteration: 3470 Loss: 2.3017180950484836e-11\n",
      "Iteration: 3471 Loss: 2.301365645376852e-11\n",
      "Iteration: 3472 Loss: 2.3010169945740242e-11\n",
      "Iteration: 3473 Loss: 2.30067010868653e-11\n",
      "Iteration: 3474 Loss: 2.3003258804397465e-11\n",
      "Iteration: 3475 Loss: 2.2999841673342225e-11\n",
      "Iteration: 3476 Loss: 2.299644321317927e-11\n",
      "Iteration: 3477 Loss: 2.299306984721006e-11\n",
      "Iteration: 3478 Loss: 2.2989720437427732e-11\n",
      "Iteration: 3479 Loss: 2.2986402866579035e-11\n",
      "Iteration: 3480 Loss: 2.298310877123456e-11\n",
      "Iteration: 3481 Loss: 2.2979828019866935e-11\n",
      "Iteration: 3482 Loss: 2.297667371372322e-11\n",
      "Iteration: 3483 Loss: 2.2973454978954125e-11\n",
      "Iteration: 3484 Loss: 2.2970133024697124e-11\n",
      "Iteration: 3485 Loss: 2.296698361772714e-11\n",
      "Iteration: 3486 Loss: 2.2963828038729653e-11\n",
      "Iteration: 3487 Loss: 2.2960686636991688e-11\n",
      "Iteration: 3488 Loss: 2.295756565399928e-11\n",
      "Iteration: 3489 Loss: 2.2954476369586186e-11\n",
      "Iteration: 3490 Loss: 2.2951405561672615e-11\n",
      "Iteration: 3491 Loss: 2.2948255028884772e-11\n",
      "Iteration: 3492 Loss: 2.2945211510797335e-11\n",
      "Iteration: 3493 Loss: 2.2942211060747915e-11\n",
      "Iteration: 3494 Loss: 2.2939222665845116e-11\n",
      "Iteration: 3495 Loss: 2.2936260998979127e-11\n",
      "Iteration: 3496 Loss: 2.2933317155249943e-11\n",
      "Iteration: 3497 Loss: 2.293040501971815e-11\n",
      "Iteration: 3498 Loss: 2.2927495021401237e-11\n",
      "Iteration: 3499 Loss: 2.292461372910463e-11\n",
      "Iteration: 3500 Loss: 2.2921809683867643e-11\n",
      "Iteration: 3501 Loss: 2.2918935983391276e-11\n",
      "Iteration: 3502 Loss: 2.2916122334061663e-11\n",
      "Iteration: 3503 Loss: 2.2913322634224608e-11\n",
      "Iteration: 3504 Loss: 2.291054269617532e-11\n",
      "Iteration: 3505 Loss: 2.2907776826438577e-11\n",
      "Iteration: 3506 Loss: 2.2905038161724133e-11\n",
      "Iteration: 3507 Loss: 2.2902215928692543e-11\n",
      "Iteration: 3508 Loss: 2.2899508290409452e-11\n",
      "Iteration: 3509 Loss: 2.2896928379106662e-11\n",
      "Iteration: 3510 Loss: 2.2894259140661935e-11\n",
      "Iteration: 3511 Loss: 2.2891618270732116e-11\n",
      "Iteration: 3512 Loss: 2.288899682581482e-11\n",
      "Iteration: 3513 Loss: 2.2886383608955304e-11\n",
      "Iteration: 3514 Loss: 2.2883696763064674e-11\n",
      "Iteration: 3515 Loss: 2.288111806483887e-11\n",
      "Iteration: 3516 Loss: 2.287858078088905e-11\n",
      "Iteration: 3517 Loss: 2.287594181885749e-11\n",
      "Iteration: 3518 Loss: 2.287340013589835e-11\n",
      "Iteration: 3519 Loss: 2.2870887636848014e-11\n",
      "Iteration: 3520 Loss: 2.2868409368112362e-11\n",
      "Iteration: 3521 Loss: 2.286593700577835e-11\n",
      "Iteration: 3522 Loss: 2.2863494294850326e-11\n",
      "Iteration: 3523 Loss: 2.2861068272045866e-11\n",
      "Iteration: 3524 Loss: 2.2858655430409277e-11\n",
      "Iteration: 3525 Loss: 2.2856259209259312e-11\n",
      "Iteration: 3526 Loss: 2.2853984101076088e-11\n",
      "Iteration: 3527 Loss: 2.2851633491569792e-11\n",
      "Iteration: 3528 Loss: 2.2849385193027712e-11\n",
      "Iteration: 3529 Loss: 2.2847058931415013e-11\n",
      "Iteration: 3530 Loss: 2.2844742095006627e-11\n",
      "Iteration: 3531 Loss: 2.2842448597583253e-11\n",
      "Iteration: 3532 Loss: 2.2840203999827727e-11\n",
      "Iteration: 3533 Loss: 2.2837934162259305e-11\n",
      "Iteration: 3534 Loss: 2.283568871053338e-11\n",
      "Iteration: 3535 Loss: 2.283346389594091e-11\n",
      "Iteration: 3536 Loss: 2.2831252884900987e-11\n",
      "Iteration: 3537 Loss: 2.282904855953632e-11\n",
      "Iteration: 3538 Loss: 2.2826862621990043e-11\n",
      "Iteration: 3539 Loss: 2.2824713244301265e-11\n",
      "Iteration: 3540 Loss: 2.2822523417880305e-11\n",
      "Iteration: 3541 Loss: 2.2820383468090568e-11\n",
      "Iteration: 3542 Loss: 2.2818259528545924e-11\n",
      "Iteration: 3543 Loss: 2.281614824664887e-11\n",
      "Iteration: 3544 Loss: 2.2814166539856836e-11\n",
      "Iteration: 3545 Loss: 2.2812085231887924e-11\n",
      "Iteration: 3546 Loss: 2.2810028721889358e-11\n",
      "Iteration: 3547 Loss: 2.2807981968900754e-11\n",
      "Iteration: 3548 Loss: 2.280594422242709e-11\n",
      "Iteration: 3549 Loss: 2.2803925154869196e-11\n",
      "Iteration: 3550 Loss: 2.280191872556766e-11\n",
      "Iteration: 3551 Loss: 2.2799955595898097e-11\n",
      "Iteration: 3552 Loss: 2.279797965407982e-11\n",
      "Iteration: 3553 Loss: 2.279601573316263e-11\n",
      "Iteration: 3554 Loss: 2.2794073765162535e-11\n",
      "Iteration: 3555 Loss: 2.2792131587867318e-11\n",
      "Iteration: 3556 Loss: 2.2790213143726344e-11\n",
      "Iteration: 3557 Loss: 2.278830280984975e-11\n",
      "Iteration: 3558 Loss: 2.278641743792944e-11\n",
      "Iteration: 3559 Loss: 2.2784530269473574e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3560 Loss: 2.2782669385118223e-11\n",
      "Iteration: 3561 Loss: 2.2780713599942313e-11\n",
      "Iteration: 3562 Loss: 2.2778868398125386e-11\n",
      "Iteration: 3563 Loss: 2.2777144151496656e-11\n",
      "Iteration: 3564 Loss: 2.2775337440712477e-11\n",
      "Iteration: 3565 Loss: 2.277353334971008e-11\n",
      "Iteration: 3566 Loss: 2.2771738957196478e-11\n",
      "Iteration: 3567 Loss: 2.2769966534393972e-11\n",
      "Iteration: 3568 Loss: 2.2768200750867534e-11\n",
      "Iteration: 3569 Loss: 2.2766451276244335e-11\n",
      "Iteration: 3570 Loss: 2.2764708169342116e-11\n",
      "Iteration: 3571 Loss: 2.27629871814778e-11\n",
      "Iteration: 3572 Loss: 2.2761268895193353e-11\n",
      "Iteration: 3573 Loss: 2.2759546110757772e-11\n",
      "Iteration: 3574 Loss: 2.2757850712641343e-11\n",
      "Iteration: 3575 Loss: 2.2756173423404757e-11\n",
      "Iteration: 3576 Loss: 2.275450990463604e-11\n",
      "Iteration: 3577 Loss: 2.275285815482894e-11\n",
      "Iteration: 3578 Loss: 2.2751216119789924e-11\n",
      "Iteration: 3579 Loss: 2.2749482608060766e-11\n",
      "Iteration: 3580 Loss: 2.2747862631272e-11\n",
      "Iteration: 3581 Loss: 2.274636510785556e-11\n",
      "Iteration: 3582 Loss: 2.2744753759762175e-11\n",
      "Iteration: 3583 Loss: 2.2743066601595037e-11\n",
      "Iteration: 3584 Loss: 2.2741496669716173e-11\n",
      "Iteration: 3585 Loss: 2.2739939602622732e-11\n",
      "Iteration: 3586 Loss: 2.2738486762884047e-11\n",
      "Iteration: 3587 Loss: 2.2736968422099928e-11\n",
      "Iteration: 3588 Loss: 2.2735438330449546e-11\n",
      "Iteration: 3589 Loss: 2.2733921489259128e-11\n",
      "Iteration: 3590 Loss: 2.27324220389328e-11\n",
      "Iteration: 3591 Loss: 2.273091971999901e-11\n",
      "Iteration: 3592 Loss: 2.2729429849805177e-11\n",
      "Iteration: 3593 Loss: 2.2727954716366738e-11\n",
      "Iteration: 3594 Loss: 2.2726487018702612e-11\n",
      "Iteration: 3595 Loss: 2.2725034608908918e-11\n",
      "Iteration: 3596 Loss: 2.272369386384479e-11\n",
      "Iteration: 3597 Loss: 2.272215165506604e-11\n",
      "Iteration: 3598 Loss: 2.2720727798345668e-11\n",
      "Iteration: 3599 Loss: 2.271931703824735e-11\n",
      "Iteration: 3600 Loss: 2.2717903677627215e-11\n",
      "Iteration: 3601 Loss: 2.2716518941107138e-11\n",
      "Iteration: 3602 Loss: 2.2715130499561756e-11\n",
      "Iteration: 3603 Loss: 2.27137614593526e-11\n",
      "Iteration: 3604 Loss: 2.271238731701901e-11\n",
      "Iteration: 3605 Loss: 2.2710937970602702e-11\n",
      "Iteration: 3606 Loss: 2.2709590337030628e-11\n",
      "Iteration: 3607 Loss: 2.2708252550489736e-11\n",
      "Iteration: 3608 Loss: 2.2707078876809732e-11\n",
      "Iteration: 3609 Loss: 2.27057963077102e-11\n",
      "Iteration: 3610 Loss: 2.270451270983448e-11\n",
      "Iteration: 3611 Loss: 2.270319119132532e-11\n",
      "Iteration: 3612 Loss: 2.2701897494190785e-11\n",
      "Iteration: 3613 Loss: 2.2700620825396606e-11\n",
      "Iteration: 3614 Loss: 2.269934373465519e-11\n",
      "Iteration: 3615 Loss: 2.269809033971051e-11\n",
      "Iteration: 3616 Loss: 2.269682621974273e-11\n",
      "Iteration: 3617 Loss: 2.2695588161399616e-11\n",
      "Iteration: 3618 Loss: 2.2694324376761685e-11\n",
      "Iteration: 3619 Loss: 2.269307236492736e-11\n",
      "Iteration: 3620 Loss: 2.2691855416638424e-11\n",
      "Iteration: 3621 Loss: 2.2690647064204534e-11\n",
      "Iteration: 3622 Loss: 2.2689451027064785e-11\n",
      "Iteration: 3623 Loss: 2.26882616439299e-11\n",
      "Iteration: 3624 Loss: 2.2687069196378018e-11\n",
      "Iteration: 3625 Loss: 2.2685899114388092e-11\n",
      "Iteration: 3626 Loss: 2.2684731021037623e-11\n",
      "Iteration: 3627 Loss: 2.268357043898572e-11\n",
      "Iteration: 3628 Loss: 2.2682422245134388e-11\n",
      "Iteration: 3629 Loss: 2.2681280076993627e-11\n",
      "Iteration: 3630 Loss: 2.2680145396230902e-11\n",
      "Iteration: 3631 Loss: 2.2679016917028076e-11\n",
      "Iteration: 3632 Loss: 2.2677895256333057e-11\n",
      "Iteration: 3633 Loss: 2.2676791275408013e-11\n",
      "Iteration: 3634 Loss: 2.2675693224575453e-11\n",
      "Iteration: 3635 Loss: 2.2674593780444817e-11\n",
      "Iteration: 3636 Loss: 2.26735122720363e-11\n",
      "Iteration: 3637 Loss: 2.2672437256144857e-11\n",
      "Iteration: 3638 Loss: 2.2671357173422198e-11\n",
      "Iteration: 3639 Loss: 2.267029164573211e-11\n",
      "Iteration: 3640 Loss: 2.2669246700591763e-11\n",
      "Iteration: 3641 Loss: 2.2668201841632178e-11\n",
      "Iteration: 3642 Loss: 2.266715714880549e-11\n",
      "Iteration: 3643 Loss: 2.2666122061125036e-11\n",
      "Iteration: 3644 Loss: 2.2665095020047368e-11\n",
      "Iteration: 3645 Loss: 2.266407318731555e-11\n",
      "Iteration: 3646 Loss: 2.266306685133714e-11\n",
      "Iteration: 3647 Loss: 2.266195637735546e-11\n",
      "Iteration: 3648 Loss: 2.266095496232159e-11\n",
      "Iteration: 3649 Loss: 2.2659969612531115e-11\n",
      "Iteration: 3650 Loss: 2.265898574475132e-11\n",
      "Iteration: 3651 Loss: 2.265811305603826e-11\n",
      "Iteration: 3652 Loss: 2.265703017262124e-11\n",
      "Iteration: 3653 Loss: 2.265608043385828e-11\n",
      "Iteration: 3654 Loss: 2.265512097277983e-11\n",
      "Iteration: 3655 Loss: 2.26541757946758e-11\n",
      "Iteration: 3656 Loss: 2.2653232227088264e-11\n",
      "Iteration: 3657 Loss: 2.265229069620056e-11\n",
      "Iteration: 3658 Loss: 2.265125559274017e-11\n",
      "Iteration: 3659 Loss: 2.2650339364440242e-11\n",
      "Iteration: 3660 Loss: 2.2649425071095022e-11\n",
      "Iteration: 3661 Loss: 2.2648516972573235e-11\n",
      "Iteration: 3662 Loss: 2.2647618204326344e-11\n",
      "Iteration: 3663 Loss: 2.2646716980392006e-11\n",
      "Iteration: 3664 Loss: 2.2645819440485093e-11\n",
      "Iteration: 3665 Loss: 2.2644948928460127e-11\n",
      "Iteration: 3666 Loss: 2.2644062762258833e-11\n",
      "Iteration: 3667 Loss: 2.2643201599051585e-11\n",
      "Iteration: 3668 Loss: 2.2642439358029248e-11\n",
      "Iteration: 3669 Loss: 2.2641577367043895e-11\n",
      "Iteration: 3670 Loss: 2.264072536468564e-11\n",
      "Iteration: 3671 Loss: 2.2639875727385703e-11\n",
      "Iteration: 3672 Loss: 2.26390319162161e-11\n",
      "Iteration: 3673 Loss: 2.2638196439388946e-11\n",
      "Iteration: 3674 Loss: 2.263737748317021e-11\n",
      "Iteration: 3675 Loss: 2.2636553227708288e-11\n",
      "Iteration: 3676 Loss: 2.2635737332202523e-11\n",
      "Iteration: 3677 Loss: 2.263492223068783e-11\n",
      "Iteration: 3678 Loss: 2.263411551730631e-11\n",
      "Iteration: 3679 Loss: 2.2633318003790028e-11\n",
      "Iteration: 3680 Loss: 2.263252432598011e-11\n",
      "Iteration: 3681 Loss: 2.2631734959324117e-11\n",
      "Iteration: 3682 Loss: 2.2630962075032322e-11\n",
      "Iteration: 3683 Loss: 2.2630179509360993e-11\n",
      "Iteration: 3684 Loss: 2.2629412325553477e-11\n",
      "Iteration: 3685 Loss: 2.2628636738369815e-11\n",
      "Iteration: 3686 Loss: 2.26278797296763e-11\n",
      "Iteration: 3687 Loss: 2.2627127456621408e-11\n",
      "Iteration: 3688 Loss: 2.2626376634102917e-11\n",
      "Iteration: 3689 Loss: 2.2625632786436454e-11\n",
      "Iteration: 3690 Loss: 2.2624895093978836e-11\n",
      "Iteration: 3691 Loss: 2.2624165406572582e-11\n",
      "Iteration: 3692 Loss: 2.2623430515112504e-11\n",
      "Iteration: 3693 Loss: 2.2622707592241056e-11\n",
      "Iteration: 3694 Loss: 2.2621989198070637e-11\n",
      "Iteration: 3695 Loss: 2.2621273968883702e-11\n",
      "Iteration: 3696 Loss: 2.2620564033407364e-11\n",
      "Iteration: 3697 Loss: 2.2619863259400486e-11\n",
      "Iteration: 3698 Loss: 2.2619165772105552e-11\n",
      "Iteration: 3699 Loss: 2.2618572406945634e-11\n",
      "Iteration: 3700 Loss: 2.2617786608530572e-11\n",
      "Iteration: 3701 Loss: 2.2617106747177806e-11\n",
      "Iteration: 3702 Loss: 2.2616415984920605e-11\n",
      "Iteration: 3703 Loss: 2.2615647937642696e-11\n",
      "Iteration: 3704 Loss: 2.2614979538958984e-11\n",
      "Iteration: 3705 Loss: 2.2614314131987426e-11\n",
      "Iteration: 3706 Loss: 2.2613652122455163e-11\n",
      "Iteration: 3707 Loss: 2.2613001399955465e-11\n",
      "Iteration: 3708 Loss: 2.2612358631417164e-11\n",
      "Iteration: 3709 Loss: 2.2611707496770845e-11\n",
      "Iteration: 3710 Loss: 2.261106637215137e-11\n",
      "Iteration: 3711 Loss: 2.261053120377663e-11\n",
      "Iteration: 3712 Loss: 2.2609908833749414e-11\n",
      "Iteration: 3713 Loss: 2.260927739073752e-11\n",
      "Iteration: 3714 Loss: 2.2608649431761844e-11\n",
      "Iteration: 3715 Loss: 2.2608032069420827e-11\n",
      "Iteration: 3716 Loss: 2.260730803920661e-11\n",
      "Iteration: 3717 Loss: 2.2606810369455584e-11\n",
      "Iteration: 3718 Loss: 2.260620812342861e-11\n",
      "Iteration: 3719 Loss: 2.2605612220284543e-11\n",
      "Iteration: 3720 Loss: 2.2605005820194974e-11\n",
      "Iteration: 3721 Loss: 2.2604517433039734e-11\n",
      "Iteration: 3722 Loss: 2.260383182279374e-11\n",
      "Iteration: 3723 Loss: 2.260324998829869e-11\n",
      "Iteration: 3724 Loss: 2.260266518802419e-11\n",
      "Iteration: 3725 Loss: 2.2602096231478432e-11\n",
      "Iteration: 3726 Loss: 2.26015244265848e-11\n",
      "Iteration: 3727 Loss: 2.2600950993405947e-11\n",
      "Iteration: 3728 Loss: 2.2600391566898492e-11\n",
      "Iteration: 3729 Loss: 2.2599829254269168e-11\n",
      "Iteration: 3730 Loss: 2.2599273492577846e-11\n",
      "Iteration: 3731 Loss: 2.2598722085794812e-11\n",
      "Iteration: 3732 Loss: 2.2598171512223215e-11\n",
      "Iteration: 3733 Loss: 2.2597634140600124e-11\n",
      "Iteration: 3734 Loss: 2.259710245262683e-11\n",
      "Iteration: 3735 Loss: 2.259656600586328e-11\n",
      "Iteration: 3736 Loss: 2.2596026084935724e-11\n",
      "Iteration: 3737 Loss: 2.2595508565670253e-11\n",
      "Iteration: 3738 Loss: 2.2594972420223077e-11\n",
      "Iteration: 3739 Loss: 2.2594454774457576e-11\n",
      "Iteration: 3740 Loss: 2.2593950294779455e-11\n",
      "Iteration: 3741 Loss: 2.2593423337050475e-11\n",
      "Iteration: 3742 Loss: 2.2592920437669038e-11\n",
      "Iteration: 3743 Loss: 2.2592409904824196e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3744 Loss: 2.2591904971325148e-11\n",
      "Iteration: 3745 Loss: 2.2591411926166156e-11\n",
      "Iteration: 3746 Loss: 2.2590918989787396e-11\n",
      "Iteration: 3747 Loss: 2.2590421149451184e-11\n",
      "Iteration: 3748 Loss: 2.258993112273409e-11\n",
      "Iteration: 3749 Loss: 2.258944419934915e-11\n",
      "Iteration: 3750 Loss: 2.2588968739815204e-11\n",
      "Iteration: 3751 Loss: 2.258848544763125e-11\n",
      "Iteration: 3752 Loss: 2.258801431302051e-11\n",
      "Iteration: 3753 Loss: 2.2587551635743826e-11\n",
      "Iteration: 3754 Loss: 2.25870860043864e-11\n",
      "Iteration: 3755 Loss: 2.25866152979235e-11\n",
      "Iteration: 3756 Loss: 2.2586152976713682e-11\n",
      "Iteration: 3757 Loss: 2.258569994109327e-11\n",
      "Iteration: 3758 Loss: 2.258524824794602e-11\n",
      "Iteration: 3759 Loss: 2.2584797896561373e-11\n",
      "Iteration: 3760 Loss: 2.258434756754736e-11\n",
      "Iteration: 3761 Loss: 2.2583906982607967e-11\n",
      "Iteration: 3762 Loss: 2.2583462540255646e-11\n",
      "Iteration: 3763 Loss: 2.258301903549413e-11\n",
      "Iteration: 3764 Loss: 2.2582591931076886e-11\n",
      "Iteration: 3765 Loss: 2.2582162514412603e-11\n",
      "Iteration: 3766 Loss: 2.258173054559371e-11\n",
      "Iteration: 3767 Loss: 2.258129886087324e-11\n",
      "Iteration: 3768 Loss: 2.2580881454482658e-11\n",
      "Iteration: 3769 Loss: 2.2580461204399014e-11\n",
      "Iteration: 3770 Loss: 2.25800416484504e-11\n",
      "Iteration: 3771 Loss: 2.25796247046576e-11\n",
      "Iteration: 3772 Loss: 2.257921414729468e-11\n",
      "Iteration: 3773 Loss: 2.257881092567325e-11\n",
      "Iteration: 3774 Loss: 2.2578401028544344e-11\n",
      "Iteration: 3775 Loss: 2.257799659110377e-11\n",
      "Iteration: 3776 Loss: 2.2577608095234845e-11\n",
      "Iteration: 3777 Loss: 2.2577220760613694e-11\n",
      "Iteration: 3778 Loss: 2.2576809806287233e-11\n",
      "Iteration: 3779 Loss: 2.2576419822993862e-11\n",
      "Iteration: 3780 Loss: 2.2576036102858032e-11\n",
      "Iteration: 3781 Loss: 2.2575653415704686e-11\n",
      "Iteration: 3782 Loss: 2.2575274288703792e-11\n",
      "Iteration: 3783 Loss: 2.2574884541838228e-11\n",
      "Iteration: 3784 Loss: 2.257451821757858e-11\n",
      "Iteration: 3785 Loss: 2.257413487404664e-11\n",
      "Iteration: 3786 Loss: 2.257376678034534e-11\n",
      "Iteration: 3787 Loss: 2.257340090179437e-11\n",
      "Iteration: 3788 Loss: 2.2573021791131928e-11\n",
      "Iteration: 3789 Loss: 2.257266534716733e-11\n",
      "Iteration: 3790 Loss: 2.2572310513711458e-11\n",
      "Iteration: 3791 Loss: 2.2571950640433656e-11\n",
      "Iteration: 3792 Loss: 2.2571584842194845e-11\n",
      "Iteration: 3793 Loss: 2.2571237773169088e-11\n",
      "Iteration: 3794 Loss: 2.257088615696168e-11\n",
      "Iteration: 3795 Loss: 2.257053303256154e-11\n",
      "Iteration: 3796 Loss: 2.2570193427855444e-11\n",
      "Iteration: 3797 Loss: 2.256985341350738e-11\n",
      "Iteration: 3798 Loss: 2.2569515070326518e-11\n",
      "Iteration: 3799 Loss: 2.2569176211796906e-11\n",
      "Iteration: 3800 Loss: 2.256873785365641e-11\n",
      "Iteration: 3801 Loss: 2.2568509837134977e-11\n",
      "Iteration: 3802 Loss: 2.2568170778630028e-11\n",
      "Iteration: 3803 Loss: 2.2567840675449428e-11\n",
      "Iteration: 3804 Loss: 2.2567620282042302e-11\n",
      "Iteration: 3805 Loss: 2.2567292367769218e-11\n",
      "Iteration: 3806 Loss: 2.256698267037323e-11\n",
      "Iteration: 3807 Loss: 2.256665052269421e-11\n",
      "Iteration: 3808 Loss: 2.2566337504715406e-11\n",
      "Iteration: 3809 Loss: 2.2566029724947378e-11\n",
      "Iteration: 3810 Loss: 2.2565612455598925e-11\n",
      "Iteration: 3811 Loss: 2.256529853231328e-11\n",
      "Iteration: 3812 Loss: 2.2564990399604155e-11\n",
      "Iteration: 3813 Loss: 2.2564677565340292e-11\n",
      "Iteration: 3814 Loss: 2.256437710475668e-11\n",
      "Iteration: 3815 Loss: 2.2564075124247524e-11\n",
      "Iteration: 3816 Loss: 2.2563785717975273e-11\n",
      "Iteration: 3817 Loss: 2.2563477777469263e-11\n",
      "Iteration: 3818 Loss: 2.2563185397394647e-11\n",
      "Iteration: 3819 Loss: 2.2562896413113742e-11\n",
      "Iteration: 3820 Loss: 2.2562503643260895e-11\n",
      "Iteration: 3821 Loss: 2.256221588659928e-11\n",
      "Iteration: 3822 Loss: 2.2561931297401474e-11\n",
      "Iteration: 3823 Loss: 2.2561741955627003e-11\n",
      "Iteration: 3824 Loss: 2.2561462844458986e-11\n",
      "Iteration: 3825 Loss: 2.25611867439201e-11\n",
      "Iteration: 3826 Loss: 2.2560913743137897e-11\n",
      "Iteration: 3827 Loss: 2.256071674965091e-11\n",
      "Iteration: 3828 Loss: 2.2560451933076293e-11\n",
      "Iteration: 3829 Loss: 2.2560076271510943e-11\n",
      "Iteration: 3830 Loss: 2.255981283211139e-11\n",
      "Iteration: 3831 Loss: 2.2559553580922952e-11\n",
      "Iteration: 3832 Loss: 2.2559275005818168e-11\n",
      "Iteration: 3833 Loss: 2.2559011304860687e-11\n",
      "Iteration: 3834 Loss: 2.255874852406981e-11\n",
      "Iteration: 3835 Loss: 2.2558486708262225e-11\n",
      "Iteration: 3836 Loss: 2.25582241887849e-11\n",
      "Iteration: 3837 Loss: 2.255795414386526e-11\n",
      "Iteration: 3838 Loss: 2.2557706496262344e-11\n",
      "Iteration: 3839 Loss: 2.255745625425387e-11\n",
      "Iteration: 3840 Loss: 2.2557205408432502e-11\n",
      "Iteration: 3841 Loss: 2.2556950767268016e-11\n",
      "Iteration: 3842 Loss: 2.255670660693478e-11\n",
      "Iteration: 3843 Loss: 2.255646888840506e-11\n",
      "Iteration: 3844 Loss: 2.2556220974652815e-11\n",
      "Iteration: 3845 Loss: 2.2555972810443584e-11\n",
      "Iteration: 3846 Loss: 2.2555623045413774e-11\n",
      "Iteration: 3847 Loss: 2.2555416920124152e-11\n",
      "Iteration: 3848 Loss: 2.2555208611295886e-11\n",
      "Iteration: 3849 Loss: 2.2554971266895398e-11\n",
      "Iteration: 3850 Loss: 2.255472926983344e-11\n",
      "Iteration: 3851 Loss: 2.2554507447546658e-11\n",
      "Iteration: 3852 Loss: 2.255427301895683e-11\n",
      "Iteration: 3853 Loss: 2.2554038559716544e-11\n",
      "Iteration: 3854 Loss: 2.2553809179127e-11\n",
      "Iteration: 3855 Loss: 2.2553589244284277e-11\n",
      "Iteration: 3856 Loss: 2.2553468251065027e-11\n",
      "Iteration: 3857 Loss: 2.2553249459410093e-11\n",
      "Iteration: 3858 Loss: 2.255302857315343e-11\n",
      "Iteration: 3859 Loss: 2.2552804040089252e-11\n",
      "Iteration: 3860 Loss: 2.255258838108564e-11\n",
      "Iteration: 3861 Loss: 2.2552369441339986e-11\n",
      "Iteration: 3862 Loss: 2.2552158056041313e-11\n",
      "Iteration: 3863 Loss: 2.255194174021145e-11\n",
      "Iteration: 3864 Loss: 2.2551720641895807e-11\n",
      "Iteration: 3865 Loss: 2.255150455681871e-11\n",
      "Iteration: 3866 Loss: 2.255131274652236e-11\n",
      "Iteration: 3867 Loss: 2.255100392608346e-11\n",
      "Iteration: 3868 Loss: 2.255079227070464e-11\n",
      "Iteration: 3869 Loss: 2.255059141440551e-11\n",
      "Iteration: 3870 Loss: 2.255039043132127e-11\n",
      "Iteration: 3871 Loss: 2.2550190218464564e-11\n",
      "Iteration: 3872 Loss: 2.2549991062370385e-11\n",
      "Iteration: 3873 Loss: 2.254979391506764e-11\n",
      "Iteration: 3874 Loss: 2.254956945347158e-11\n",
      "Iteration: 3875 Loss: 2.2549351237256712e-11\n",
      "Iteration: 3876 Loss: 2.2549143588613877e-11\n",
      "Iteration: 3877 Loss: 2.2548950407791134e-11\n",
      "Iteration: 3878 Loss: 2.2548764003512532e-11\n",
      "Iteration: 3879 Loss: 2.2548565717199645e-11\n",
      "Iteration: 3880 Loss: 2.254838043740915e-11\n",
      "Iteration: 3881 Loss: 2.2548203569602404e-11\n",
      "Iteration: 3882 Loss: 2.254799833297174e-11\n",
      "Iteration: 3883 Loss: 2.2547832329463535e-11\n",
      "Iteration: 3884 Loss: 2.2547642469641614e-11\n",
      "Iteration: 3885 Loss: 2.2547572654200448e-11\n",
      "Iteration: 3886 Loss: 2.2547388599883536e-11\n",
      "Iteration: 3887 Loss: 2.2547208389562955e-11\n",
      "Iteration: 3888 Loss: 2.2547135821682333e-11\n",
      "Iteration: 3889 Loss: 2.254686101199051e-11\n",
      "Iteration: 3890 Loss: 2.2546683948402255e-11\n",
      "Iteration: 3891 Loss: 2.2546512577874817e-11\n",
      "Iteration: 3892 Loss: 2.2546338094671417e-11\n",
      "Iteration: 3893 Loss: 2.25461629734991e-11\n",
      "Iteration: 3894 Loss: 2.2546014054012288e-11\n",
      "Iteration: 3895 Loss: 2.2545878110929716e-11\n",
      "Iteration: 3896 Loss: 2.254570301756273e-11\n",
      "Iteration: 3897 Loss: 2.254554078737733e-11\n",
      "Iteration: 3898 Loss: 2.2545377691757953e-11\n",
      "Iteration: 3899 Loss: 2.2545205730208717e-11\n",
      "Iteration: 3900 Loss: 2.2544944360418424e-11\n",
      "Iteration: 3901 Loss: 2.2544880144953948e-11\n",
      "Iteration: 3902 Loss: 2.2544717941854337e-11\n",
      "Iteration: 3903 Loss: 2.2544549999133942e-11\n",
      "Iteration: 3904 Loss: 2.2544399349979594e-11\n",
      "Iteration: 3905 Loss: 2.2544246603913656e-11\n",
      "Iteration: 3906 Loss: 2.2544089917236595e-11\n",
      "Iteration: 3907 Loss: 2.2543932421087944e-11\n",
      "Iteration: 3908 Loss: 2.2543781307049067e-11\n",
      "Iteration: 3909 Loss: 2.2543624072549513e-11\n",
      "Iteration: 3910 Loss: 2.254348171810791e-11\n",
      "Iteration: 3911 Loss: 2.2543325859880336e-11\n",
      "Iteration: 3912 Loss: 2.254318111187138e-11\n",
      "Iteration: 3913 Loss: 2.2543031499207373e-11\n",
      "Iteration: 3914 Loss: 2.2542872853015596e-11\n",
      "Iteration: 3915 Loss: 2.2542734439224946e-11\n",
      "Iteration: 3916 Loss: 2.2542586442768366e-11\n",
      "Iteration: 3917 Loss: 2.254243571778044e-11\n",
      "Iteration: 3918 Loss: 2.2542299468186217e-11\n",
      "Iteration: 3919 Loss: 2.2542155167816432e-11\n",
      "Iteration: 3920 Loss: 2.2542004438990807e-11\n",
      "Iteration: 3921 Loss: 2.25418647066225e-11\n",
      "Iteration: 3922 Loss: 2.2541731724199545e-11\n",
      "Iteration: 3923 Loss: 2.254159047904456e-11\n",
      "Iteration: 3924 Loss: 2.2541454703014466e-11\n",
      "Iteration: 3925 Loss: 2.254120826708607e-11\n",
      "Iteration: 3926 Loss: 2.2541085582945614e-11\n",
      "Iteration: 3927 Loss: 2.2540940895061994e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3928 Loss: 2.2540817928857357e-11\n",
      "Iteration: 3929 Loss: 2.2540682684207953e-11\n",
      "Iteration: 3930 Loss: 2.254055085016365e-11\n",
      "Iteration: 3931 Loss: 2.254042667617911e-11\n",
      "Iteration: 3932 Loss: 2.2540299229843897e-11\n",
      "Iteration: 3933 Loss: 2.2540266573721628e-11\n",
      "Iteration: 3934 Loss: 2.254014230016143e-11\n",
      "Iteration: 3935 Loss: 2.2540017724192226e-11\n",
      "Iteration: 3936 Loss: 2.2539892106165923e-11\n",
      "Iteration: 3937 Loss: 2.253977255911314e-11\n",
      "Iteration: 3938 Loss: 2.2539638987328955e-11\n",
      "Iteration: 3939 Loss: 2.253951953773325e-11\n",
      "Iteration: 3940 Loss: 2.253939882887707e-11\n",
      "Iteration: 3941 Loss: 2.2539276283047434e-11\n",
      "Iteration: 3942 Loss: 2.253915104471773e-11\n",
      "Iteration: 3943 Loss: 2.2539031563857067e-11\n",
      "Iteration: 3944 Loss: 2.2538919477734063e-11\n",
      "Iteration: 3945 Loss: 2.2538794344118937e-11\n",
      "Iteration: 3946 Loss: 2.2538680215870195e-11\n",
      "Iteration: 3947 Loss: 2.2538566721916383e-11\n",
      "Iteration: 3948 Loss: 2.2538436436690374e-11\n",
      "Iteration: 3949 Loss: 2.2538323564877713e-11\n",
      "Iteration: 3950 Loss: 2.2538215811428393e-11\n",
      "Iteration: 3951 Loss: 2.25380956354044e-11\n",
      "Iteration: 3952 Loss: 2.253788572679095e-11\n",
      "Iteration: 3953 Loss: 2.2537879410219546e-11\n",
      "Iteration: 3954 Loss: 2.2537876676966696e-11\n",
      "Iteration: 3955 Loss: 2.253776562613791e-11\n",
      "Iteration: 3956 Loss: 2.2537655984544083e-11\n",
      "Iteration: 3957 Loss: 2.2537548444732768e-11\n",
      "Iteration: 3958 Loss: 2.2537452390789606e-11\n",
      "Iteration: 3959 Loss: 2.2537234006657356e-11\n",
      "Iteration: 3960 Loss: 2.2537131639979606e-11\n",
      "Iteration: 3961 Loss: 2.2537016647444224e-11\n",
      "Iteration: 3962 Loss: 2.2536925187239362e-11\n",
      "Iteration: 3963 Loss: 2.2536821263081466e-11\n",
      "Iteration: 3964 Loss: 2.2536710776418346e-11\n",
      "Iteration: 3965 Loss: 2.2536610212700766e-11\n",
      "Iteration: 3966 Loss: 2.2536511246582055e-11\n",
      "Iteration: 3967 Loss: 2.2536414273635943e-11\n",
      "Iteration: 3968 Loss: 2.2536316765226145e-11\n",
      "Iteration: 3969 Loss: 2.2536212198630972e-11\n",
      "Iteration: 3970 Loss: 2.253611057330081e-11\n",
      "Iteration: 3971 Loss: 2.2536012970633346e-11\n",
      "Iteration: 3972 Loss: 2.2535919259257135e-11\n",
      "Iteration: 3973 Loss: 2.253581785761668e-11\n",
      "Iteration: 3974 Loss: 2.2535720870678786e-11\n",
      "Iteration: 3975 Loss: 2.2535628121209767e-11\n",
      "Iteration: 3976 Loss: 2.2535530260499423e-11\n",
      "Iteration: 3977 Loss: 2.253543788254869e-11\n",
      "Iteration: 3978 Loss: 2.253533833928924e-11\n",
      "Iteration: 3979 Loss: 2.253525572659113e-11\n",
      "Iteration: 3980 Loss: 2.253515800376789e-11\n",
      "Iteration: 3981 Loss: 2.2535071284308702e-11\n",
      "Iteration: 3982 Loss: 2.2534985857669812e-11\n",
      "Iteration: 3983 Loss: 2.25348872459954e-11\n",
      "Iteration: 3984 Loss: 2.2534703054794036e-11\n",
      "Iteration: 3985 Loss: 2.2534710204851447e-11\n",
      "Iteration: 3986 Loss: 2.253462823473181e-11\n",
      "Iteration: 3987 Loss: 2.253454226993322e-11\n",
      "Iteration: 3988 Loss: 2.2534466328059238e-11\n",
      "Iteration: 3989 Loss: 2.2534276116801258e-11\n",
      "Iteration: 3990 Loss: 2.253419025549843e-11\n",
      "Iteration: 3991 Loss: 2.2534109979739574e-11\n",
      "Iteration: 3992 Loss: 2.253402012362393e-11\n",
      "Iteration: 3993 Loss: 2.2534042401656703e-11\n",
      "Iteration: 3994 Loss: 2.25339651050848e-11\n",
      "Iteration: 3995 Loss: 2.253387943302748e-11\n",
      "Iteration: 3996 Loss: 2.2533802358256786e-11\n",
      "Iteration: 3997 Loss: 2.2533723504299516e-11\n",
      "Iteration: 3998 Loss: 2.2533644181879606e-11\n",
      "Iteration: 3999 Loss: 2.2533555731340367e-11\n",
      "Iteration: 4000 Loss: 2.2533383736549664e-11\n",
      "Iteration: 4001 Loss: 2.2533305744702912e-11\n",
      "Iteration: 4002 Loss: 2.2533224910618536e-11\n",
      "Iteration: 4003 Loss: 2.253315056495963e-11\n",
      "Iteration: 4004 Loss: 2.25330702223164e-11\n",
      "Iteration: 4005 Loss: 2.253309417345116e-11\n",
      "Iteration: 4006 Loss: 2.2533021235794245e-11\n",
      "Iteration: 4007 Loss: 2.253284068495639e-11\n",
      "Iteration: 4008 Loss: 2.253276237934832e-11\n",
      "Iteration: 4009 Loss: 2.2532690542320946e-11\n",
      "Iteration: 4010 Loss: 2.253261438952535e-11\n",
      "Iteration: 4011 Loss: 2.2532544257555867e-11\n",
      "Iteration: 4012 Loss: 2.2532575828530548e-11\n",
      "Iteration: 4013 Loss: 2.2532494756029164e-11\n",
      "Iteration: 4014 Loss: 2.2532327254459253e-11\n",
      "Iteration: 4015 Loss: 2.2532355537206563e-11\n",
      "Iteration: 4016 Loss: 2.2532284122305545e-11\n",
      "Iteration: 4017 Loss: 2.253211112989512e-11\n",
      "Iteration: 4018 Loss: 2.2532040295974933e-11\n",
      "Iteration: 4019 Loss: 2.2532069779001488e-11\n",
      "Iteration: 4020 Loss: 2.253210858564133e-11\n",
      "Iteration: 4021 Loss: 2.253203073778186e-11\n",
      "Iteration: 4022 Loss: 2.2531959355235934e-11\n",
      "Iteration: 4023 Loss: 2.25318994956386e-11\n",
      "Iteration: 4024 Loss: 2.2531822777671016e-11\n",
      "Iteration: 4025 Loss: 2.2531768324654873e-11\n",
      "Iteration: 4026 Loss: 2.253169486403987e-11\n",
      "Iteration: 4027 Loss: 2.2531539476271515e-11\n",
      "Iteration: 4028 Loss: 2.2531465766592196e-11\n",
      "Iteration: 4029 Loss: 2.2531504156328513e-11\n",
      "Iteration: 4030 Loss: 2.2531440511779908e-11\n",
      "Iteration: 4031 Loss: 2.2531390804273432e-11\n",
      "Iteration: 4032 Loss: 2.2531321153780322e-11\n",
      "Iteration: 4033 Loss: 2.2531265847528996e-11\n",
      "Iteration: 4034 Loss: 2.2531203600819202e-11\n",
      "Iteration: 4035 Loss: 2.2531142415139543e-11\n",
      "Iteration: 4036 Loss: 2.2531083604212964e-11\n",
      "Iteration: 4037 Loss: 2.253102212995217e-11\n",
      "Iteration: 4038 Loss: 2.2530972644102134e-11\n",
      "Iteration: 4039 Loss: 2.25309093204326e-11\n",
      "Iteration: 4040 Loss: 2.253085453832354e-11\n",
      "Iteration: 4041 Loss: 2.2530793978807282e-11\n",
      "Iteration: 4042 Loss: 2.2530734722914996e-11\n",
      "Iteration: 4043 Loss: 2.2530669715318636e-11\n",
      "Iteration: 4044 Loss: 2.2530625172230372e-11\n",
      "Iteration: 4045 Loss: 2.253056777985963e-11\n",
      "Iteration: 4046 Loss: 2.2530508634507282e-11\n",
      "Iteration: 4047 Loss: 2.2530455690154505e-11\n",
      "Iteration: 4048 Loss: 2.253039137700462e-11\n",
      "Iteration: 4049 Loss: 2.2530243488009765e-11\n",
      "Iteration: 4050 Loss: 2.2530188201583557e-11\n",
      "Iteration: 4051 Loss: 2.2530129164165553e-11\n",
      "Iteration: 4052 Loss: 2.2530076189661028e-11\n",
      "Iteration: 4053 Loss: 2.2530020423118933e-11\n",
      "Iteration: 4054 Loss: 2.252997904325695e-11\n",
      "Iteration: 4055 Loss: 2.2529923180694735e-11\n",
      "Iteration: 4056 Loss: 2.2529870479184567e-11\n",
      "Iteration: 4057 Loss: 2.252992383961642e-11\n",
      "Iteration: 4058 Loss: 2.2529864562684866e-11\n",
      "Iteration: 4059 Loss: 2.252981420828393e-11\n",
      "Iteration: 4060 Loss: 2.2529757610253066e-11\n",
      "Iteration: 4061 Loss: 2.2529704856293178e-11\n",
      "Iteration: 4062 Loss: 2.2529555302975056e-11\n",
      "Iteration: 4063 Loss: 2.252950978632293e-11\n",
      "Iteration: 4064 Loss: 2.2529458494665447e-11\n",
      "Iteration: 4065 Loss: 2.2529408046714747e-11\n",
      "Iteration: 4066 Loss: 2.2529362268823962e-11\n",
      "Iteration: 4067 Loss: 2.2529307388875794e-11\n",
      "Iteration: 4068 Loss: 2.2529262076147568e-11\n",
      "Iteration: 4069 Loss: 2.2529211165192643e-11\n",
      "Iteration: 4070 Loss: 2.2529061842919242e-11\n",
      "Iteration: 4071 Loss: 2.252911111470036e-11\n",
      "Iteration: 4072 Loss: 2.2529066847506536e-11\n",
      "Iteration: 4073 Loss: 2.252902298961604e-11\n",
      "Iteration: 4074 Loss: 2.252897365858746e-11\n",
      "Iteration: 4075 Loss: 2.2528931868979847e-11\n",
      "Iteration: 4076 Loss: 2.2528878374875437e-11\n",
      "Iteration: 4077 Loss: 2.2528836401801e-11\n",
      "Iteration: 4078 Loss: 2.2528783027338103e-11\n",
      "Iteration: 4079 Loss: 2.2528740852831184e-11\n",
      "Iteration: 4080 Loss: 2.2528697367247386e-11\n",
      "Iteration: 4081 Loss: 2.2528652597328326e-11\n",
      "Iteration: 4082 Loss: 2.2528606613077954e-11\n",
      "Iteration: 4083 Loss: 2.252856763415923e-11\n",
      "Iteration: 4084 Loss: 2.2528534137511393e-11\n",
      "Iteration: 4085 Loss: 2.2528481021164716e-11\n",
      "Iteration: 4086 Loss: 2.252843322628844e-11\n",
      "Iteration: 4087 Loss: 2.2528399765048587e-11\n",
      "Iteration: 4088 Loss: 2.252834797696185e-11\n",
      "Iteration: 4089 Loss: 2.252831977228301e-11\n",
      "Iteration: 4090 Loss: 2.2528282086874388e-11\n",
      "Iteration: 4091 Loss: 2.252823943394448e-11\n",
      "Iteration: 4092 Loss: 2.2528187782613762e-11\n",
      "Iteration: 4093 Loss: 2.2528060078847955e-11\n",
      "Iteration: 4094 Loss: 2.2528012737935377e-11\n",
      "Iteration: 4095 Loss: 2.2528086679759284e-11\n",
      "Iteration: 4096 Loss: 2.2528135256046887e-11\n",
      "Iteration: 4097 Loss: 2.252810889017933e-11\n",
      "Iteration: 4098 Loss: 2.252806407900361e-11\n",
      "Iteration: 4099 Loss: 2.2528028498536864e-11\n",
      "Iteration: 4100 Loss: 2.252799215073252e-11\n",
      "Iteration: 4101 Loss: 2.2527955184753498e-11\n",
      "Iteration: 4102 Loss: 2.252792482488215e-11\n",
      "Iteration: 4103 Loss: 2.252788375488848e-11\n",
      "Iteration: 4104 Loss: 2.252784624346472e-11\n",
      "Iteration: 4105 Loss: 2.252781021602236e-11\n",
      "Iteration: 4106 Loss: 2.252778145043107e-11\n",
      "Iteration: 4107 Loss: 2.252774698752317e-11\n",
      "Iteration: 4108 Loss: 2.252760332958216e-11\n",
      "Iteration: 4109 Loss: 2.2527568993462117e-11\n",
      "Iteration: 4110 Loss: 2.2527537582342207e-11\n",
      "Iteration: 4111 Loss: 2.2527503083076022e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4112 Loss: 2.2527467114338427e-11\n",
      "Iteration: 4113 Loss: 2.2527437606861614e-11\n",
      "Iteration: 4114 Loss: 2.2527403265771876e-11\n",
      "Iteration: 4115 Loss: 2.252737443773297e-11\n",
      "Iteration: 4116 Loss: 2.2527343169698813e-11\n",
      "Iteration: 4117 Loss: 2.2527305654309087e-11\n",
      "Iteration: 4118 Loss: 2.2527269612717336e-11\n",
      "Iteration: 4119 Loss: 2.2527233162739938e-11\n",
      "Iteration: 4120 Loss: 2.252731952635692e-11\n",
      "Iteration: 4121 Loss: 2.2527381919924965e-11\n",
      "Iteration: 4122 Loss: 2.2527243432617493e-11\n",
      "Iteration: 4123 Loss: 2.2527214599753258e-11\n",
      "Iteration: 4124 Loss: 2.2527192298071587e-11\n",
      "Iteration: 4125 Loss: 2.2527152781203093e-11\n",
      "Iteration: 4126 Loss: 2.252712311668924e-11\n",
      "Iteration: 4127 Loss: 2.2527097452814568e-11\n",
      "Iteration: 4128 Loss: 2.252706509849675e-11\n",
      "Iteration: 4129 Loss: 2.252703573116598e-11\n",
      "Iteration: 4130 Loss: 2.2527003081850036e-11\n",
      "Iteration: 4131 Loss: 2.252697502920413e-11\n",
      "Iteration: 4132 Loss: 2.252694604070832e-11\n",
      "Iteration: 4133 Loss: 2.2526921993884213e-11\n",
      "Iteration: 4134 Loss: 2.252688533770554e-11\n",
      "Iteration: 4135 Loss: 2.2526861304349978e-11\n",
      "Iteration: 4136 Loss: 2.2526832130015808e-11\n",
      "Iteration: 4137 Loss: 2.252679979285992e-11\n",
      "Iteration: 4138 Loss: 2.2526773263935204e-11\n",
      "Iteration: 4139 Loss: 2.252674505185511e-11\n",
      "Iteration: 4140 Loss: 2.252672003432284e-11\n",
      "Iteration: 4141 Loss: 2.252668674517896e-11\n",
      "Iteration: 4142 Loss: 2.252666274824048e-11\n",
      "Iteration: 4143 Loss: 2.2526629679271828e-11\n",
      "Iteration: 4144 Loss: 2.2526600068960966e-11\n",
      "Iteration: 4145 Loss: 2.2526682140106927e-11\n",
      "Iteration: 4146 Loss: 2.2526657347163765e-11\n",
      "Iteration: 4147 Loss: 2.252663318806969e-11\n",
      "Iteration: 4148 Loss: 2.252659877820401e-11\n",
      "Iteration: 4149 Loss: 2.2526574046443863e-11\n",
      "Iteration: 4150 Loss: 2.2526550367805812e-11\n",
      "Iteration: 4151 Loss: 2.2526522470054192e-11\n",
      "Iteration: 4152 Loss: 2.2526497738801882e-11\n",
      "Iteration: 4153 Loss: 2.2526363652679033e-11\n",
      "Iteration: 4154 Loss: 2.2526341189260265e-11\n",
      "Iteration: 4155 Loss: 2.252631347501461e-11\n",
      "Iteration: 4156 Loss: 2.2526294279409896e-11\n",
      "Iteration: 4157 Loss: 2.2526269834441362e-11\n",
      "Iteration: 4158 Loss: 2.252624343386215e-11\n",
      "Iteration: 4159 Loss: 2.252621583007614e-11\n",
      "Iteration: 4160 Loss: 2.252619446193322e-11\n",
      "Iteration: 4161 Loss: 2.2526171986142804e-11\n",
      "Iteration: 4162 Loss: 2.2526147275877847e-11\n",
      "Iteration: 4163 Loss: 2.2526117340700876e-11\n",
      "Iteration: 4164 Loss: 2.252609816440646e-11\n",
      "Iteration: 4165 Loss: 2.2526070482591172e-11\n",
      "Iteration: 4166 Loss: 2.2526054706504347e-11\n",
      "Iteration: 4167 Loss: 2.252602306067843e-11\n",
      "Iteration: 4168 Loss: 2.2525998557850345e-11\n",
      "Iteration: 4169 Loss: 2.2525973998986957e-11\n",
      "Iteration: 4170 Loss: 2.2525954948950535e-11\n",
      "Iteration: 4171 Loss: 2.252593883665303e-11\n",
      "Iteration: 4172 Loss: 2.252580992463747e-11\n",
      "Iteration: 4173 Loss: 2.252578331006127e-11\n",
      "Iteration: 4174 Loss: 2.2525760217463354e-11\n",
      "Iteration: 4175 Loss: 2.2525741137719684e-11\n",
      "Iteration: 4176 Loss: 2.252571673662387e-11\n",
      "Iteration: 4177 Loss: 2.2525693912077872e-11\n",
      "Iteration: 4178 Loss: 2.2525663755019673e-11\n",
      "Iteration: 4179 Loss: 2.2525647847365664e-11\n",
      "Iteration: 4180 Loss: 2.2525626246918057e-11\n",
      "Iteration: 4181 Loss: 2.2525604851144598e-11\n",
      "Iteration: 4182 Loss: 2.252558190922076e-11\n",
      "Iteration: 4183 Loss: 2.2525560742543897e-11\n",
      "Iteration: 4184 Loss: 2.2525543430621436e-11\n",
      "Iteration: 4185 Loss: 2.252551951270981e-11\n",
      "Iteration: 4186 Loss: 2.2525500483617965e-11\n",
      "Iteration: 4187 Loss: 2.2525482974283658e-11\n",
      "Iteration: 4188 Loss: 2.2525452576374785e-11\n",
      "Iteration: 4189 Loss: 2.2525438448689168e-11\n",
      "Iteration: 4190 Loss: 2.252541165352188e-11\n",
      "Iteration: 4191 Loss: 2.25253899716306e-11\n",
      "Iteration: 4192 Loss: 2.2525375815978414e-11\n",
      "Iteration: 4193 Loss: 2.2525354643775334e-11\n",
      "Iteration: 4194 Loss: 2.2525333177344902e-11\n",
      "Iteration: 4195 Loss: 2.2525311819472976e-11\n",
      "Iteration: 4196 Loss: 2.252529240331207e-11\n",
      "Iteration: 4197 Loss: 2.2525275551343464e-11\n",
      "Iteration: 4198 Loss: 2.2525269564298124e-11\n",
      "Iteration: 4199 Loss: 2.2525243059569766e-11\n",
      "Iteration: 4200 Loss: 2.2525216590121955e-11\n",
      "Iteration: 4201 Loss: 2.2525196733065435e-11\n",
      "Iteration: 4202 Loss: 2.2525175580352304e-11\n",
      "Iteration: 4203 Loss: 2.252516059874257e-11\n",
      "Iteration: 4204 Loss: 2.2525149576220335e-11\n",
      "Iteration: 4205 Loss: 2.2525123023104406e-11\n",
      "Iteration: 4206 Loss: 2.2525209780383014e-11\n",
      "Iteration: 4207 Loss: 2.252518462117903e-11\n",
      "Iteration: 4208 Loss: 2.2525177069921364e-11\n",
      "Iteration: 4209 Loss: 2.2525059967618913e-11\n",
      "Iteration: 4210 Loss: 2.2525041876047314e-11\n",
      "Iteration: 4211 Loss: 2.252502307435726e-11\n",
      "Iteration: 4212 Loss: 2.2525003541740438e-11\n",
      "Iteration: 4213 Loss: 2.252498487579269e-11\n",
      "Iteration: 4214 Loss: 2.2524975680058785e-11\n",
      "Iteration: 4215 Loss: 2.252495054012156e-11\n",
      "Iteration: 4216 Loss: 2.2524938031004423e-11\n",
      "Iteration: 4217 Loss: 2.2524919456787778e-11\n",
      "Iteration: 4218 Loss: 2.252501649743922e-11\n",
      "Iteration: 4219 Loss: 2.2524991132849222e-11\n",
      "Iteration: 4220 Loss: 2.2524972732248872e-11\n",
      "Iteration: 4221 Loss: 2.252495469552367e-11\n",
      "Iteration: 4222 Loss: 2.2524941886319454e-11\n",
      "Iteration: 4223 Loss: 2.252492991703229e-11\n",
      "Iteration: 4224 Loss: 2.252502008703664e-11\n",
      "Iteration: 4225 Loss: 2.252489902819996e-11\n",
      "Iteration: 4226 Loss: 2.252488027745651e-11\n",
      "Iteration: 4227 Loss: 2.2524860102711703e-11\n",
      "Iteration: 4228 Loss: 2.2524851078189628e-11\n",
      "Iteration: 4229 Loss: 2.252483254564797e-11\n",
      "Iteration: 4230 Loss: 2.2524822967350195e-11\n",
      "Iteration: 4231 Loss: 2.2524912462802902e-11\n",
      "Iteration: 4232 Loss: 2.252489071493202e-11\n",
      "Iteration: 4233 Loss: 2.2524884682430557e-11\n",
      "Iteration: 4234 Loss: 2.2524875164471643e-11\n",
      "Iteration: 4235 Loss: 2.252486032891394e-11\n",
      "Iteration: 4236 Loss: 2.252483938889469e-11\n",
      "Iteration: 4237 Loss: 2.2524832087914837e-11\n",
      "Iteration: 4238 Loss: 2.2524822207954713e-11\n",
      "Iteration: 4239 Loss: 2.2524697431874133e-11\n",
      "Iteration: 4240 Loss: 2.2524682307879207e-11\n",
      "Iteration: 4241 Loss: 2.2524677573330724e-11\n",
      "Iteration: 4242 Loss: 2.2524664495317406e-11\n",
      "Iteration: 4243 Loss: 2.252465171143454e-11\n",
      "Iteration: 4244 Loss: 2.2524641819212292e-11\n",
      "Iteration: 4245 Loss: 2.252452942988457e-11\n",
      "Iteration: 4246 Loss: 2.252451646368494e-11\n",
      "Iteration: 4247 Loss: 2.252450805910468e-11\n",
      "Iteration: 4248 Loss: 2.2524495008381392e-11\n",
      "Iteration: 4249 Loss: 2.252448813433659e-11\n",
      "Iteration: 4250 Loss: 2.2524577548971386e-11\n",
      "Iteration: 4251 Loss: 2.2524568174061032e-11\n",
      "Iteration: 4252 Loss: 2.2524555889768386e-11\n",
      "Iteration: 4253 Loss: 2.252454358964601e-11\n",
      "Iteration: 4254 Loss: 2.2524535051187753e-11\n",
      "Iteration: 4255 Loss: 2.25245221538298e-11\n",
      "Iteration: 4256 Loss: 2.2524513690193388e-11\n",
      "Iteration: 4257 Loss: 2.2524503671658016e-11\n",
      "Iteration: 4258 Loss: 2.2524494580508277e-11\n",
      "Iteration: 4259 Loss: 2.2524485163732426e-11\n",
      "Iteration: 4260 Loss: 2.2524472272672938e-11\n",
      "Iteration: 4261 Loss: 2.252436107077198e-11\n",
      "Iteration: 4262 Loss: 2.2524351262450215e-11\n",
      "Iteration: 4263 Loss: 2.25243416277675e-11\n",
      "Iteration: 4264 Loss: 2.2524435697872642e-11\n",
      "Iteration: 4265 Loss: 2.252432447712468e-11\n",
      "Iteration: 4266 Loss: 2.2524316056028084e-11\n",
      "Iteration: 4267 Loss: 2.252440838213192e-11\n",
      "Iteration: 4268 Loss: 2.2524400069872118e-11\n",
      "Iteration: 4269 Loss: 2.2524390100415575e-11\n",
      "Iteration: 4270 Loss: 2.252438135919557e-11\n",
      "Iteration: 4271 Loss: 2.252437337889183e-11\n",
      "Iteration: 4272 Loss: 2.2524364827709064e-11\n",
      "Iteration: 4273 Loss: 2.2524354515359956e-11\n",
      "Iteration: 4274 Loss: 2.2524346417480047e-11\n",
      "Iteration: 4275 Loss: 2.252433929957699e-11\n",
      "Iteration: 4276 Loss: 2.2524330774826282e-11\n",
      "Iteration: 4277 Loss: 2.252431932970576e-11\n",
      "Iteration: 4278 Loss: 2.2524312583885854e-11\n",
      "Iteration: 4279 Loss: 2.2524298970998735e-11\n",
      "Iteration: 4280 Loss: 2.2524290576097064e-11\n",
      "Iteration: 4281 Loss: 2.2524177657301607e-11\n",
      "Iteration: 4282 Loss: 2.2524175243989814e-11\n",
      "Iteration: 4283 Loss: 2.252416439835834e-11\n",
      "Iteration: 4284 Loss: 2.2524155660107446e-11\n",
      "Iteration: 4285 Loss: 2.2524144607037955e-11\n",
      "Iteration: 4286 Loss: 2.2524138829142088e-11\n",
      "Iteration: 4287 Loss: 2.2524133469097277e-11\n",
      "Iteration: 4288 Loss: 2.2524125139415605e-11\n",
      "Iteration: 4289 Loss: 2.252412456534506e-11\n",
      "Iteration: 4290 Loss: 2.252411092941047e-11\n",
      "Iteration: 4291 Loss: 2.2524099912863113e-11\n",
      "Iteration: 4292 Loss: 2.2524083614956387e-11\n",
      "Iteration: 4293 Loss: 2.252407463186055e-11\n",
      "Iteration: 4294 Loss: 2.2524067035812987e-11\n",
      "Iteration: 4295 Loss: 2.252406131129568e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4296 Loss: 2.2524150358455376e-11\n",
      "Iteration: 4297 Loss: 2.252414668894048e-11\n",
      "Iteration: 4298 Loss: 2.2524141785554108e-11\n",
      "Iteration: 4299 Loss: 2.2524133034051395e-11\n",
      "Iteration: 4300 Loss: 2.252412786281346e-11\n",
      "Iteration: 4301 Loss: 2.2524019864165338e-11\n",
      "Iteration: 4302 Loss: 2.2523997891937698e-11\n",
      "Iteration: 4303 Loss: 2.252409528331207e-11\n",
      "Iteration: 4304 Loss: 2.2523992247381908e-11\n",
      "Iteration: 4305 Loss: 2.2524085927614197e-11\n",
      "Iteration: 4306 Loss: 2.2524083396061536e-11\n",
      "Iteration: 4307 Loss: 2.2524074020108854e-11\n",
      "Iteration: 4308 Loss: 2.2524059949438047e-11\n",
      "Iteration: 4309 Loss: 2.2524060199487114e-11\n",
      "Iteration: 4310 Loss: 2.2524053313539942e-11\n",
      "Iteration: 4311 Loss: 2.252415005554704e-11\n",
      "Iteration: 4312 Loss: 2.2524136862809563e-11\n",
      "Iteration: 4313 Loss: 2.2524023070341907e-11\n",
      "Iteration: 4314 Loss: 2.2524023260800867e-11\n",
      "Iteration: 4315 Loss: 2.252401647824957e-11\n",
      "Iteration: 4316 Loss: 2.252401094279673e-11\n",
      "Iteration: 4317 Loss: 2.2524002640225794e-11\n",
      "Iteration: 4318 Loss: 2.2523997025474478e-11\n",
      "Iteration: 4319 Loss: 2.2523990203545722e-11\n",
      "Iteration: 4320 Loss: 2.2523984680543384e-11\n",
      "Iteration: 4321 Loss: 2.2523978851476344e-11\n",
      "Iteration: 4322 Loss: 2.252397197490612e-11\n",
      "Iteration: 4323 Loss: 2.2523963915167016e-11\n",
      "Iteration: 4324 Loss: 2.2523959581752956e-11\n",
      "Iteration: 4325 Loss: 2.2523952551165587e-11\n",
      "Iteration: 4326 Loss: 2.2523945906576254e-11\n",
      "Iteration: 4327 Loss: 2.252394021888352e-11\n",
      "Iteration: 4328 Loss: 2.2523934992105363e-11\n",
      "Iteration: 4329 Loss: 2.2523930769885887e-11\n",
      "Iteration: 4330 Loss: 2.252392663825658e-11\n",
      "Iteration: 4331 Loss: 2.2523918457211644e-11\n",
      "Iteration: 4332 Loss: 2.2523914146908302e-11\n",
      "Iteration: 4333 Loss: 2.2523908847227972e-11\n",
      "Iteration: 4334 Loss: 2.2523904977272255e-11\n",
      "Iteration: 4335 Loss: 2.2523897790272096e-11\n",
      "Iteration: 4336 Loss: 2.252389236666383e-11\n",
      "Iteration: 4337 Loss: 2.2523886793894013e-11\n",
      "Iteration: 4338 Loss: 2.252387978329561e-11\n",
      "Iteration: 4339 Loss: 2.2523875579144898e-11\n",
      "Iteration: 4340 Loss: 2.2523871635521975e-11\n",
      "Iteration: 4341 Loss: 2.2523967432744493e-11\n",
      "Iteration: 4342 Loss: 2.2523963398221436e-11\n",
      "Iteration: 4343 Loss: 2.2523956546597255e-11\n",
      "Iteration: 4344 Loss: 2.252384801443547e-11\n",
      "Iteration: 4345 Loss: 2.2523740297080787e-11\n",
      "Iteration: 4346 Loss: 2.2523736486208685e-11\n",
      "Iteration: 4347 Loss: 2.252373228275565e-11\n",
      "Iteration: 4348 Loss: 2.2523725168798523e-11\n",
      "Iteration: 4349 Loss: 2.2523822193456125e-11\n",
      "Iteration: 4350 Loss: 2.2523921048864756e-11\n",
      "Iteration: 4351 Loss: 2.252391557611174e-11\n",
      "Iteration: 4352 Loss: 2.2523911656126427e-11\n",
      "Iteration: 4353 Loss: 2.2523804497823532e-11\n",
      "Iteration: 4354 Loss: 2.2523796501785587e-11\n",
      "Iteration: 4355 Loss: 2.252379251260159e-11\n",
      "Iteration: 4356 Loss: 2.2523788552972985e-11\n",
      "Iteration: 4357 Loss: 2.252378311508472e-11\n",
      "Iteration: 4358 Loss: 2.252377762264023e-11\n",
      "Iteration: 4359 Loss: 2.2523774800170737e-11\n",
      "Iteration: 4360 Loss: 2.2523769493340677e-11\n",
      "Iteration: 4361 Loss: 2.2523765324976317e-11\n",
      "Iteration: 4362 Loss: 2.2523762596669474e-11\n",
      "Iteration: 4363 Loss: 2.2523757317330178e-11\n",
      "Iteration: 4364 Loss: 2.2523753268900412e-11\n",
      "Iteration: 4365 Loss: 2.25237486511245e-11\n",
      "Iteration: 4366 Loss: 2.252374376180482e-11\n",
      "Iteration: 4367 Loss: 2.2523740945192342e-11\n",
      "Iteration: 4368 Loss: 2.2523735452728694e-11\n",
      "Iteration: 4369 Loss: 2.252373089674664e-11\n",
      "Iteration: 4370 Loss: 2.252372730642331e-11\n",
      "Iteration: 4371 Loss: 2.252372180294929e-11\n",
      "Iteration: 4372 Loss: 2.252371950801636e-11\n",
      "Iteration: 4373 Loss: 2.2523713857528423e-11\n",
      "Iteration: 4374 Loss: 2.2523712776330735e-11\n",
      "Iteration: 4375 Loss: 2.2523708664973944e-11\n",
      "Iteration: 4376 Loss: 2.2523701648494492e-11\n",
      "Iteration: 4377 Loss: 2.2523697620463614e-11\n",
      "Iteration: 4378 Loss: 2.2523695228378945e-11\n",
      "Iteration: 4379 Loss: 2.2523691070192878e-11\n",
      "Iteration: 4380 Loss: 2.2523687063229905e-11\n",
      "Iteration: 4381 Loss: 2.2523680059579214e-11\n",
      "Iteration: 4382 Loss: 2.252367573329108e-11\n",
      "Iteration: 4383 Loss: 2.2523673102287514e-11\n",
      "Iteration: 4384 Loss: 2.252366946627073e-11\n",
      "Iteration: 4385 Loss: 2.2523665360303767e-11\n",
      "Iteration: 4386 Loss: 2.2523661427297904e-11\n",
      "Iteration: 4387 Loss: 2.2523657404012034e-11\n",
      "Iteration: 4388 Loss: 2.2523552171228748e-11\n",
      "Iteration: 4389 Loss: 2.2523546325752303e-11\n",
      "Iteration: 4390 Loss: 2.2523542299898576e-11\n",
      "Iteration: 4391 Loss: 2.252354132297454e-11\n",
      "Iteration: 4392 Loss: 2.2523533039271082e-11\n",
      "Iteration: 4393 Loss: 2.2523533494987665e-11\n",
      "Iteration: 4394 Loss: 2.252352903374325e-11\n",
      "Iteration: 4395 Loss: 2.2523525191741516e-11\n",
      "Iteration: 4396 Loss: 2.2523519891895068e-11\n",
      "Iteration: 4397 Loss: 2.2523520085633666e-11\n",
      "Iteration: 4398 Loss: 2.2523513066598627e-11\n",
      "Iteration: 4399 Loss: 2.25235115284761e-11\n",
      "Iteration: 4400 Loss: 2.2523610141528183e-11\n",
      "Iteration: 4401 Loss: 2.252360617054801e-11\n",
      "Iteration: 4402 Loss: 2.252360524795133e-11\n",
      "Iteration: 4403 Loss: 2.25236022717581e-11\n",
      "Iteration: 4404 Loss: 2.252349578223649e-11\n",
      "Iteration: 4405 Loss: 2.2523491892552847e-11\n",
      "Iteration: 4406 Loss: 2.252349063250383e-11\n",
      "Iteration: 4407 Loss: 2.252347727164474e-11\n",
      "Iteration: 4408 Loss: 2.2523471835401105e-11\n",
      "Iteration: 4409 Loss: 2.25233682484417e-11\n",
      "Iteration: 4410 Loss: 2.2523365513290532e-11\n",
      "Iteration: 4411 Loss: 2.2523361474978347e-11\n",
      "Iteration: 4412 Loss: 2.2523357231714327e-11\n",
      "Iteration: 4413 Loss: 2.2523455400964633e-11\n",
      "Iteration: 4414 Loss: 2.2523454884942254e-11\n",
      "Iteration: 4415 Loss: 2.252355310841934e-11\n",
      "Iteration: 4416 Loss: 2.252355210675311e-11\n",
      "Iteration: 4417 Loss: 2.2523548241125207e-11\n",
      "Iteration: 4418 Loss: 2.252354548356682e-11\n",
      "Iteration: 4419 Loss: 2.2523541560541833e-11\n",
      "Iteration: 4420 Loss: 2.2523540046677962e-11\n",
      "Iteration: 4421 Loss: 2.2523433643665784e-11\n",
      "Iteration: 4422 Loss: 2.252353225042455e-11\n",
      "Iteration: 4423 Loss: 2.2523530967538902e-11\n",
      "Iteration: 4424 Loss: 2.2523526889434222e-11\n",
      "Iteration: 4425 Loss: 2.2523522404367935e-11\n",
      "Iteration: 4426 Loss: 2.2523520123159067e-11\n",
      "Iteration: 4427 Loss: 2.252351643902822e-11\n",
      "Iteration: 4428 Loss: 2.2523514833892175e-11\n",
      "Iteration: 4429 Loss: 2.252351077676656e-11\n",
      "Iteration: 4430 Loss: 2.2523506842241492e-11\n",
      "Iteration: 4431 Loss: 2.2523507053359178e-11\n",
      "Iteration: 4432 Loss: 2.2523501231756338e-11\n",
      "Iteration: 4433 Loss: 2.2523498977342092e-11\n",
      "Iteration: 4434 Loss: 2.2523497344501993e-11\n",
      "Iteration: 4435 Loss: 2.2523494615220522e-11\n",
      "Iteration: 4436 Loss: 2.2523489653953555e-11\n",
      "Iteration: 4437 Loss: 2.2523489646545658e-11\n",
      "Iteration: 4438 Loss: 2.2523486001097403e-11\n",
      "Iteration: 4439 Loss: 2.2523481784169654e-11\n",
      "Iteration: 4440 Loss: 2.2523478000180227e-11\n",
      "Iteration: 4441 Loss: 2.2523480349005594e-11\n",
      "Iteration: 4442 Loss: 2.252347398094385e-11\n",
      "Iteration: 4443 Loss: 2.2523472339440422e-11\n",
      "Iteration: 4444 Loss: 2.252346870310984e-11\n",
      "Iteration: 4445 Loss: 2.2523467226348253e-11\n",
      "Iteration: 4446 Loss: 2.2523459217256057e-11\n",
      "Iteration: 4447 Loss: 2.2523360352873735e-11\n",
      "Iteration: 4448 Loss: 2.252335849177635e-11\n",
      "Iteration: 4449 Loss: 2.2523356937394053e-11\n",
      "Iteration: 4450 Loss: 2.2523352806437126e-11\n",
      "Iteration: 4451 Loss: 2.252335021091355e-11\n",
      "Iteration: 4452 Loss: 2.252334875707657e-11\n",
      "Iteration: 4453 Loss: 2.2523450374311872e-11\n",
      "Iteration: 4454 Loss: 2.2523442118479234e-11\n",
      "Iteration: 4455 Loss: 2.252344098098096e-11\n",
      "Iteration: 4456 Loss: 2.2523437706411484e-11\n",
      "Iteration: 4457 Loss: 2.252343708719671e-11\n",
      "Iteration: 4458 Loss: 2.2523439899978783e-11\n",
      "Iteration: 4459 Loss: 2.2523438618920055e-11\n",
      "Iteration: 4460 Loss: 2.2523435630518793e-11\n",
      "Iteration: 4461 Loss: 2.2523429117369047e-11\n",
      "Iteration: 4462 Loss: 2.2523431849173103e-11\n",
      "Iteration: 4463 Loss: 2.2523326449468578e-11\n",
      "Iteration: 4464 Loss: 2.252332354197579e-11\n",
      "Iteration: 4465 Loss: 2.252331834052804e-11\n",
      "Iteration: 4466 Loss: 2.252331438999546e-11\n",
      "Iteration: 4467 Loss: 2.2523314414254817e-11\n",
      "Iteration: 4468 Loss: 2.252331297018588e-11\n",
      "Iteration: 4469 Loss: 2.2523206807785702e-11\n",
      "Iteration: 4470 Loss: 2.2523209248748472e-11\n",
      "Iteration: 4471 Loss: 2.2523203207369282e-11\n",
      "Iteration: 4472 Loss: 2.252319998882952e-11\n",
      "Iteration: 4473 Loss: 2.2523197293653167e-11\n",
      "Iteration: 4474 Loss: 2.252319741206284e-11\n",
      "Iteration: 4475 Loss: 2.252320025845358e-11\n",
      "Iteration: 4476 Loss: 2.2523182628608864e-11\n",
      "Iteration: 4477 Loss: 2.252319125059707e-11\n",
      "Iteration: 4478 Loss: 2.2523189615349037e-11\n",
      "Iteration: 4479 Loss: 2.25231856740352e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4480 Loss: 2.2523178435650576e-11\n",
      "Iteration: 4481 Loss: 2.2523178294776435e-11\n",
      "Iteration: 4482 Loss: 2.2523076608169215e-11\n",
      "Iteration: 4483 Loss: 2.252307031928539e-11\n",
      "Iteration: 4484 Loss: 2.252307448499411e-11\n",
      "Iteration: 4485 Loss: 2.252306620946125e-11\n",
      "Iteration: 4486 Loss: 2.2523070238692735e-11\n",
      "Iteration: 4487 Loss: 2.2523067414751564e-11\n",
      "Iteration: 4488 Loss: 2.2523170612920808e-11\n",
      "Iteration: 4489 Loss: 2.252316662249799e-11\n",
      "Iteration: 4490 Loss: 2.252316019252086e-11\n",
      "Iteration: 4491 Loss: 2.2523162524417793e-11\n",
      "Iteration: 4492 Loss: 2.2523256030174243e-11\n",
      "Iteration: 4493 Loss: 2.2523260678215254e-11\n",
      "Iteration: 4494 Loss: 2.2523259912040562e-11\n",
      "Iteration: 4495 Loss: 2.2523250825103774e-11\n",
      "Iteration: 4496 Loss: 2.252325598279695e-11\n",
      "Iteration: 4497 Loss: 2.2523250809974844e-11\n",
      "Iteration: 4498 Loss: 2.2523252095198747e-11\n",
      "Iteration: 4499 Loss: 2.2523145866128308e-11\n",
      "Iteration: 4500 Loss: 2.2523241123455647e-11\n",
      "Iteration: 4501 Loss: 2.2523241539147512e-11\n",
      "Iteration: 4502 Loss: 2.2523244211643416e-11\n",
      "Iteration: 4503 Loss: 2.2523137077700757e-11\n",
      "Iteration: 4504 Loss: 2.2523134804550673e-11\n",
      "Iteration: 4505 Loss: 2.2523137899324483e-11\n",
      "Iteration: 4506 Loss: 2.2523135296474983e-11\n",
      "Iteration: 4507 Loss: 2.2523229628771545e-11\n",
      "Iteration: 4508 Loss: 2.2523230063641154e-11\n",
      "Iteration: 4509 Loss: 2.2523235155034798e-11\n",
      "Iteration: 4510 Loss: 2.2523226202117955e-11\n",
      "Iteration: 4511 Loss: 2.252322991222055e-11\n",
      "Iteration: 4512 Loss: 2.2523224918804608e-11\n",
      "Iteration: 4513 Loss: 2.2523227343079164e-11\n",
      "Iteration: 4514 Loss: 2.2523226057851915e-11\n",
      "Iteration: 4515 Loss: 2.2523218338610697e-11\n",
      "Iteration: 4516 Loss: 2.2523221826876574e-11\n",
      "Iteration: 4517 Loss: 2.2523216213093473e-11\n",
      "Iteration: 4518 Loss: 2.252322228061557e-11\n",
      "Iteration: 4519 Loss: 2.252321295032521e-11\n",
      "Iteration: 4520 Loss: 2.2523216672195224e-11\n",
      "Iteration: 4521 Loss: 2.252321722581454e-11\n",
      "Iteration: 4522 Loss: 2.2523217028279592e-11\n",
      "Iteration: 4523 Loss: 2.2523209891337608e-11\n",
      "Iteration: 4524 Loss: 2.2523210389651003e-11\n",
      "Iteration: 4525 Loss: 2.2523210296223583e-11\n",
      "Iteration: 4526 Loss: 2.2523208956089952e-11\n",
      "Iteration: 4527 Loss: 2.2523209249425755e-11\n",
      "Iteration: 4528 Loss: 2.2523206620239688e-11\n",
      "Iteration: 4529 Loss: 2.252310294417157e-11\n",
      "Iteration: 4530 Loss: 2.2523097046863253e-11\n",
      "Iteration: 4531 Loss: 2.25231005084582e-11\n",
      "Iteration: 4532 Loss: 2.252310049617913e-11\n",
      "Iteration: 4533 Loss: 2.2523098751969686e-11\n",
      "Iteration: 4534 Loss: 2.2523198902476797e-11\n",
      "Iteration: 4535 Loss: 2.2523094985255428e-11\n",
      "Iteration: 4536 Loss: 2.2523094723022123e-11\n",
      "Iteration: 4537 Loss: 2.2522990942973785e-11\n",
      "Iteration: 4538 Loss: 2.252309339119212e-11\n",
      "Iteration: 4539 Loss: 2.2523094068065096e-11\n",
      "Iteration: 4540 Loss: 2.2523188329573192e-11\n",
      "Iteration: 4541 Loss: 2.2523189627305613e-11\n",
      "Iteration: 4542 Loss: 2.2523189745570863e-11\n",
      "Iteration: 4543 Loss: 2.2523189745570863e-11\n",
      "Iteration: 4544 Loss: 2.252318452492995e-11\n",
      "Iteration: 4545 Loss: 2.2523188442216767e-11\n",
      "Iteration: 4546 Loss: 2.252318582605069e-11\n",
      "Iteration: 4547 Loss: 2.2523185634874575e-11\n",
      "Iteration: 4548 Loss: 2.252318457241565e-11\n",
      "Iteration: 4549 Loss: 2.252318465030392e-11\n",
      "Iteration: 4550 Loss: 2.252318205717423e-11\n",
      "Iteration: 4551 Loss: 2.2523176429238432e-11\n",
      "Iteration: 4552 Loss: 2.2523175674716486e-11\n",
      "Iteration: 4553 Loss: 2.252317538081657e-11\n",
      "Iteration: 4554 Loss: 2.2523280607061735e-11\n",
      "Iteration: 4555 Loss: 2.2523274230692572e-11\n",
      "Iteration: 4556 Loss: 2.2523274457541575e-11\n",
      "Iteration: 4557 Loss: 2.2523279843564834e-11\n",
      "Iteration: 4558 Loss: 2.2523273246225266e-11\n",
      "Iteration: 4559 Loss: 2.2523273167664892e-11\n",
      "Iteration: 4560 Loss: 2.252327036375389e-11\n",
      "Iteration: 4561 Loss: 2.2523269094724927e-11\n",
      "Iteration: 4562 Loss: 2.2523269231627426e-11\n",
      "Iteration: 4563 Loss: 2.2523269296071842e-11\n",
      "Iteration: 4564 Loss: 2.2523266644840956e-11\n",
      "Iteration: 4565 Loss: 2.2523265406350845e-11\n",
      "Iteration: 4566 Loss: 2.2523265268421115e-11\n",
      "Iteration: 4567 Loss: 2.252315999944859e-11\n",
      "Iteration: 4568 Loss: 2.2523158981991094e-11\n",
      "Iteration: 4569 Loss: 2.252326685692221e-11\n",
      "Iteration: 4570 Loss: 2.2523261623929565e-11\n",
      "Iteration: 4571 Loss: 2.252325867942793e-11\n",
      "Iteration: 4572 Loss: 2.2523257442167205e-11\n",
      "Iteration: 4573 Loss: 2.2523257016839985e-11\n",
      "Iteration: 4574 Loss: 2.252325746519961e-11\n",
      "Iteration: 4575 Loss: 2.2523254858585505e-11\n",
      "Iteration: 4576 Loss: 2.252326040352876e-11\n",
      "Iteration: 4577 Loss: 2.2523253040812627e-11\n",
      "Iteration: 4578 Loss: 2.2523252350096924e-11\n",
      "Iteration: 4579 Loss: 2.2523354528945852e-11\n",
      "Iteration: 4580 Loss: 2.2523355135853098e-11\n",
      "Iteration: 4581 Loss: 2.2523351801192668e-11\n",
      "Iteration: 4582 Loss: 2.2523350453340065e-11\n",
      "Iteration: 4583 Loss: 2.2523351102846328e-11\n",
      "Iteration: 4584 Loss: 2.25233485965572e-11\n",
      "Iteration: 4585 Loss: 2.2523347330224815e-11\n",
      "Iteration: 4586 Loss: 2.252334703247741e-11\n",
      "Iteration: 4587 Loss: 2.252324216341949e-11\n",
      "Iteration: 4588 Loss: 2.252324225383448e-11\n",
      "Iteration: 4589 Loss: 2.252324203022274e-11\n",
      "Iteration: 4590 Loss: 2.2523240982976597e-11\n",
      "Iteration: 4591 Loss: 2.2523237698599262e-11\n",
      "Iteration: 4592 Loss: 2.252323696851453e-11\n",
      "Iteration: 4593 Loss: 2.2523236308984245e-11\n",
      "Iteration: 4594 Loss: 2.2523134313306034e-11\n",
      "Iteration: 4595 Loss: 2.2523134280288422e-11\n",
      "Iteration: 4596 Loss: 2.252312623333517e-11\n",
      "Iteration: 4597 Loss: 2.2523130860788567e-11\n",
      "Iteration: 4598 Loss: 2.2523129020899297e-11\n",
      "Iteration: 4599 Loss: 2.2523129082009174e-11\n",
      "Iteration: 4600 Loss: 2.2523134387190782e-11\n",
      "Iteration: 4601 Loss: 2.252312127372781e-11\n",
      "Iteration: 4602 Loss: 2.252312151574929e-11\n",
      "Iteration: 4603 Loss: 2.2523120490146177e-11\n",
      "Iteration: 4604 Loss: 2.2523117389765973e-11\n",
      "Iteration: 4605 Loss: 2.2523220128892806e-11\n",
      "Iteration: 4606 Loss: 2.2523224552453956e-11\n",
      "Iteration: 4607 Loss: 2.2523219051310886e-11\n",
      "Iteration: 4608 Loss: 2.2523216165285952e-11\n",
      "Iteration: 4609 Loss: 2.252332402980566e-11\n",
      "Iteration: 4610 Loss: 2.2523317622397795e-11\n",
      "Iteration: 4611 Loss: 2.252331492662648e-11\n",
      "Iteration: 4612 Loss: 2.252332038423601e-11\n",
      "Iteration: 4613 Loss: 2.2523217983118953e-11\n",
      "Iteration: 4614 Loss: 2.2523216411024627e-11\n",
      "Iteration: 4615 Loss: 2.2523216636718802e-11\n",
      "Iteration: 4616 Loss: 2.2523214063279547e-11\n",
      "Iteration: 4617 Loss: 2.252320718054777e-11\n",
      "Iteration: 4618 Loss: 2.2523211370857402e-11\n",
      "Iteration: 4619 Loss: 2.2523211766419278e-11\n",
      "Iteration: 4620 Loss: 2.2523211485884304e-11\n",
      "Iteration: 4621 Loss: 2.2523208847320742e-11\n",
      "Iteration: 4622 Loss: 2.252310091588933e-11\n",
      "Iteration: 4623 Loss: 2.2523099835398732e-11\n",
      "Iteration: 4624 Loss: 2.2523099899176357e-11\n",
      "Iteration: 4625 Loss: 2.2523102633181343e-11\n",
      "Iteration: 4626 Loss: 2.252310016955803e-11\n",
      "Iteration: 4627 Loss: 2.2523097307619692e-11\n",
      "Iteration: 4628 Loss: 2.2523198382109878e-11\n",
      "Iteration: 4629 Loss: 2.2523201265977887e-11\n",
      "Iteration: 4630 Loss: 2.2523200869104452e-11\n",
      "Iteration: 4631 Loss: 2.2523199564266207e-11\n",
      "Iteration: 4632 Loss: 2.25232002019356e-11\n",
      "Iteration: 4633 Loss: 2.2523300064151678e-11\n",
      "Iteration: 4634 Loss: 2.2523302275716278e-11\n",
      "Iteration: 4635 Loss: 2.252330024341068e-11\n",
      "Iteration: 4636 Loss: 2.2523298961689115e-11\n",
      "Iteration: 4637 Loss: 2.252329343543916e-11\n",
      "Iteration: 4638 Loss: 2.25230886126476e-11\n",
      "Iteration: 4639 Loss: 2.2523085433397386e-11\n",
      "Iteration: 4640 Loss: 2.252308541093692e-11\n",
      "Iteration: 4641 Loss: 2.252308937335742e-11\n",
      "Iteration: 4642 Loss: 2.252319166293214e-11\n",
      "Iteration: 4643 Loss: 2.252308971540305e-11\n",
      "Iteration: 4644 Loss: 2.2523088870243793e-11\n",
      "Iteration: 4645 Loss: 2.252308971098751e-11\n",
      "Iteration: 4646 Loss: 2.2523080387613682e-11\n",
      "Iteration: 4647 Loss: 2.252308071643327e-11\n",
      "Iteration: 4648 Loss: 2.2523079184400673e-11\n",
      "Iteration: 4649 Loss: 2.252308454459499e-11\n",
      "Iteration: 4650 Loss: 2.252308496931711e-11\n",
      "Iteration: 4651 Loss: 2.252308217753601e-11\n",
      "Iteration: 4652 Loss: 2.2523084274769318e-11\n",
      "Iteration: 4653 Loss: 2.252308482051183e-11\n",
      "Iteration: 4654 Loss: 2.2523076901321892e-11\n",
      "Iteration: 4655 Loss: 2.2523075580241193e-11\n",
      "Iteration: 4656 Loss: 2.2523075520744855e-11\n",
      "Iteration: 4657 Loss: 2.2523075807283482e-11\n",
      "Iteration: 4658 Loss: 2.2523073146625464e-11\n",
      "Iteration: 4659 Loss: 2.2523071479496764e-11\n",
      "Iteration: 4660 Loss: 2.252307794281337e-11\n",
      "Iteration: 4661 Loss: 2.252307737088548e-11\n",
      "Iteration: 4662 Loss: 2.2523073280927855e-11\n",
      "Iteration: 4663 Loss: 2.25230661258236e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4664 Loss: 2.2523071844698846e-11\n",
      "Iteration: 4665 Loss: 2.2523071890393822e-11\n",
      "Iteration: 4666 Loss: 2.2523071921336458e-11\n",
      "Iteration: 4667 Loss: 2.2523074528833737e-11\n",
      "Iteration: 4668 Loss: 2.2523073221355526e-11\n",
      "Iteration: 4669 Loss: 2.2523074631120106e-11\n",
      "Iteration: 4670 Loss: 2.252307467289158e-11\n",
      "Iteration: 4671 Loss: 2.2523067759841364e-11\n",
      "Iteration: 4672 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4673 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4674 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4675 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4676 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4677 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4678 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4679 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4680 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4681 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4682 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4683 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4684 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4685 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4686 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4687 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4688 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4689 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4690 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4691 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4692 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4693 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4694 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4695 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4696 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4697 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4698 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4699 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4700 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4701 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4702 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4703 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4704 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4705 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4706 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4707 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4708 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4709 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4710 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4711 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4712 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4713 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4714 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4715 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4716 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4717 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4718 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4719 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4720 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4721 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4722 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4723 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4724 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4725 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4726 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4727 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4728 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4729 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4730 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4731 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4732 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4733 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4734 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4735 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4736 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4737 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4738 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4739 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4740 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4741 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4742 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4743 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4744 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4745 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4746 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4747 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4748 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4749 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4750 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4751 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4752 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4753 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4754 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4755 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4756 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4757 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4758 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4759 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4760 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4761 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4762 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4763 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4764 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4765 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4766 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4767 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4768 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4769 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4770 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4771 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4772 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4773 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4774 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4775 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4776 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4777 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4778 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4779 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4780 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4781 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4782 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4783 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4784 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4785 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4786 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4787 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4788 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4789 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4790 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4791 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4792 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4793 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4794 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4795 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4796 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4797 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4798 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4799 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4800 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4801 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4802 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4803 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4804 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4805 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4806 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4807 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4808 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4809 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4810 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4811 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4812 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4813 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4814 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4815 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4816 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4817 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4818 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4819 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4820 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4821 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4822 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4823 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4824 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4825 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4826 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4827 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4828 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4829 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4830 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4831 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4832 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4833 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4834 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4835 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4836 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4837 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4838 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4839 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4840 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4841 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4842 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4843 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4844 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4845 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4846 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4847 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4848 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4849 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4850 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4851 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4852 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4853 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4854 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4855 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4856 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4857 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4858 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4859 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4860 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4861 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4862 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4863 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4864 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4865 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4866 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4867 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4868 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4869 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4870 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4871 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4872 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4873 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4874 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4875 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4876 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4877 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4878 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4879 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4880 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4881 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4882 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4883 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4884 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4885 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4886 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4887 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4888 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4889 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4890 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4891 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4892 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4893 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4894 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4895 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4896 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4897 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4898 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4899 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4900 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4901 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4902 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4903 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4904 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4905 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4906 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4907 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4908 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4909 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4910 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4911 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4912 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4913 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4914 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4915 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4916 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4917 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4918 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4919 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4920 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4921 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4922 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4923 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4924 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4925 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4926 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4927 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4928 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4929 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4930 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4931 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4932 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4933 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4934 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4935 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4936 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4937 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4938 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4939 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4940 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4941 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4942 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4943 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4944 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4945 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4946 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4947 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4948 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4949 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4950 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4951 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4952 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4953 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4954 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4955 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4956 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4957 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4958 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4959 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4960 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4961 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4962 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4963 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4964 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4965 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4966 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4967 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4968 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4969 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4970 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4971 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4972 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4973 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4974 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4975 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4976 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4977 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4978 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4979 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4980 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4981 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4982 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4983 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4984 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4985 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4986 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4987 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4988 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4989 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4990 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4991 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4992 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4993 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4994 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4995 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4996 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4997 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4998 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4999 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5000 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5001 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5002 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5003 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5004 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5005 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5006 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5007 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5008 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5009 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5010 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5011 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5012 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5013 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5014 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5015 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5016 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5017 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5018 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5019 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5020 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5021 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5022 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5023 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5024 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5025 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5026 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5027 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5028 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5029 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5030 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5031 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5032 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5033 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5034 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5035 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5036 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5037 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5038 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5039 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5040 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5041 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5042 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5043 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5044 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5045 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5046 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5047 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5048 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5049 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5050 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5051 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5052 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5053 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5054 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5055 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5056 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5057 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5058 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5059 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5060 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5061 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5062 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5063 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5064 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5065 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5066 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5067 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5068 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5069 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5070 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5071 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5072 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5073 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5074 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5075 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5076 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5077 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5078 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5079 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5080 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5081 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5082 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5083 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5084 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5085 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5086 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5087 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5088 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5089 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5090 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5091 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5092 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5093 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5094 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5095 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5096 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5097 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5098 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5099 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5100 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5101 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5102 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5103 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5104 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5105 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5106 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5107 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5108 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5109 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5110 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5111 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5112 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5113 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5114 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5115 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5116 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5117 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5118 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5119 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5120 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5121 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5122 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5123 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5124 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5125 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5126 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5127 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5128 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5129 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5130 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5131 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5132 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5133 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5134 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5135 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5136 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5137 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5138 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5139 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5140 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5141 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5142 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5143 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5144 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5145 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5146 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5147 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5148 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5149 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5150 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5151 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5152 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5153 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5154 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5155 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5156 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5157 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5158 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5159 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5160 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5161 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5162 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5163 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5164 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5165 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5166 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5167 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5168 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5169 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5170 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5171 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5172 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5173 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5174 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5175 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5176 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5177 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5178 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5179 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5180 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5181 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5182 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5183 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5184 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5185 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5186 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5187 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5188 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5189 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5190 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5191 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5192 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5193 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5194 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5195 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5196 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5197 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5198 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5199 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5200 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5201 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5202 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5203 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5204 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5205 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5206 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5207 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5208 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5209 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5210 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5211 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5212 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5213 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5214 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5215 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5216 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5217 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5218 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5219 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5220 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5221 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5222 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5223 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5224 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5225 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5226 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5227 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5228 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5229 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5230 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5231 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5232 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5233 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5234 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5235 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5236 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5237 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5238 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5239 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5240 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5241 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5242 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5243 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5244 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5245 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5246 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5247 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5248 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5249 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5250 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5251 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5252 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5253 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5254 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5255 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5256 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5257 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5258 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5259 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5260 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5261 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5262 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5263 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5264 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5265 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5266 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5267 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5268 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5269 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5270 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5271 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5272 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5273 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5274 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5275 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5276 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5277 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5278 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5279 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5280 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5281 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5282 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5283 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5284 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5285 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5286 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5287 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5288 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5289 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5290 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5291 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5292 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5293 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5294 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5295 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5296 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5297 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5298 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5299 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5300 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5301 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5302 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5303 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5304 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5305 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5306 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5307 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5308 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5309 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5310 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5311 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5312 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5313 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5314 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5315 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5316 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5317 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5318 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5319 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5320 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5321 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5322 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5323 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5324 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5325 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5326 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5327 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5328 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5329 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5330 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5331 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5332 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5333 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5334 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5335 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5336 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5337 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5338 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5339 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5340 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5341 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5342 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5343 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5344 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5345 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5346 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5347 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5348 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5349 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5350 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5351 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5352 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5353 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5354 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5355 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5356 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5357 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5358 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5359 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5360 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5361 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5362 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5363 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5364 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5365 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5366 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5367 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5368 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5369 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5370 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5371 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5372 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5373 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5374 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5375 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5376 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5377 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5378 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5379 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5380 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5381 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5382 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5383 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5384 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5385 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5386 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5387 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5388 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5389 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5390 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5391 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5392 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5393 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5394 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5395 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5396 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5397 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5398 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5399 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5400 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5401 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5402 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5403 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5404 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5405 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5406 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5407 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5408 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5409 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5410 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5411 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5412 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5413 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5414 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5415 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5416 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5417 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5418 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5419 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5420 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5421 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5422 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5423 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5424 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5425 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5426 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5427 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5428 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5429 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5430 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5431 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5432 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5433 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5434 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5435 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5436 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5437 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5438 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5439 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5440 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5441 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5442 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5443 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5444 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5445 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5446 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5447 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5448 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5449 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5450 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5451 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5452 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5453 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5454 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5455 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5456 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5457 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5458 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5459 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5460 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5461 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5462 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5463 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5464 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5465 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5466 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5467 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5468 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5469 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5470 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5471 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5472 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5473 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5474 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5475 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5476 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5477 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5478 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5479 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5480 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5481 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5482 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5483 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5484 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5485 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5486 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5487 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5488 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5489 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5490 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5491 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5492 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5493 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5494 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5495 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5496 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5497 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5498 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5499 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5500 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5501 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5502 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5503 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5504 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5505 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5506 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5507 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5508 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5509 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5510 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5511 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5512 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5513 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5514 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5515 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5516 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5517 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5518 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5519 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5520 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5521 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5522 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5523 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5524 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5525 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5526 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5527 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5528 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5529 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5530 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5531 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5532 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5533 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5534 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5535 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5536 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5537 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5538 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5539 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5540 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5541 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5542 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5543 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5544 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5545 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5546 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5547 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5548 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5549 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5550 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5551 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5552 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5553 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5554 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5555 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5556 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5557 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5558 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5559 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5560 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5561 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5562 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5563 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5564 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5565 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5566 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5567 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5568 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5569 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5570 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5571 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5572 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5573 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5574 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5575 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5576 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5577 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5578 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5579 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5580 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5581 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5582 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5583 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5584 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5585 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5586 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5587 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5588 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5589 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5590 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5591 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5592 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5593 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5594 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5595 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5596 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5597 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5598 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5599 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5600 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5601 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5602 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5603 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5604 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5605 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5606 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5607 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5608 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5609 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5610 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5611 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5612 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5613 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5614 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5615 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5616 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5617 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5618 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5619 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5620 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5621 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5622 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5623 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5624 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5625 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5626 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5627 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5628 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5629 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5630 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5631 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5632 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5633 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5634 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5635 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5636 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5637 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5638 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5639 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5640 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5641 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5642 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5643 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5644 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5645 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5646 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5647 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5648 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5649 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5650 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5651 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5652 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5653 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5654 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5655 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5656 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5657 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5658 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5659 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5660 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5661 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5662 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5663 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5664 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5665 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5666 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5667 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5668 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5669 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5670 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5671 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5672 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5673 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5674 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5675 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5676 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5677 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5678 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5679 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5680 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5681 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5682 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5683 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5684 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5685 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5686 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5687 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5688 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5689 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5690 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5691 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5692 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5693 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5694 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5695 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5696 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5697 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5698 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5699 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5700 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5701 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5702 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5703 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5704 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5705 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5706 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5707 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5708 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5709 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5710 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5711 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5712 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5713 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5714 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5715 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5716 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5717 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5718 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5719 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5720 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5721 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5722 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5723 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5724 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5725 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5726 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5727 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5728 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5729 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5730 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5731 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5732 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5733 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5734 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5735 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5736 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5737 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5738 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5739 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5740 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5741 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5742 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5743 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5744 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5745 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5746 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5747 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5748 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5749 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5750 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5751 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5752 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5753 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5754 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5755 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5756 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5757 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5758 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5759 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5760 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5761 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5762 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5763 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5764 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5765 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5766 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5767 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5768 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5769 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5770 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5771 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5772 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5773 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5774 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5775 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5776 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5777 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5778 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5779 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5780 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5781 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5782 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5783 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5784 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5785 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5786 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5787 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5788 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5789 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5790 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5791 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5792 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5793 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5794 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5795 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5796 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5797 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5798 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5799 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5800 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5801 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5802 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5803 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5804 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5805 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5806 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5807 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5808 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5809 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5810 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5811 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5812 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5813 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5814 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5815 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5816 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5817 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5818 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5819 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5820 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5821 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5822 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5823 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5824 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5825 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5826 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5827 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5828 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5829 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5830 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5831 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5832 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5833 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5834 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5835 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5836 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5837 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5838 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5839 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5840 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5841 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5842 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5843 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5844 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5845 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5846 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5847 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5848 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5849 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5850 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5851 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5852 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5853 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5854 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5855 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5856 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5857 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5858 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5859 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5860 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5861 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5862 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5863 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5864 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5865 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5866 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5867 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5868 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5869 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5870 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5871 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5872 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5873 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5874 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5875 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5876 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5877 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5878 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5879 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5880 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5881 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5882 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5883 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5884 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5885 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5886 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5887 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5888 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5889 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5890 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5891 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5892 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5893 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5894 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5895 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5896 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5897 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5898 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5899 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5900 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5901 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5902 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5903 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5904 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5905 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5906 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5907 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5908 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5909 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5910 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5911 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5912 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5913 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5914 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5915 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5916 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5917 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5918 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5919 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5920 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5921 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5922 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5923 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5924 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5925 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5926 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5927 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5928 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5929 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5930 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5931 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5932 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5933 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5934 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5935 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5936 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5937 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5938 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5939 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5940 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5941 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5942 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5943 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5944 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5945 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5946 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5947 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5948 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5949 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5950 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5951 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5952 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5953 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5954 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5955 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5956 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5957 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5958 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5959 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5960 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5961 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5962 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5963 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5964 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5965 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5966 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5967 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5968 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5969 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5970 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5971 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5972 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5973 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5974 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5975 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5976 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5977 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5978 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5979 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5980 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5981 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5982 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5983 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5984 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5985 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5986 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5987 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5988 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5989 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5990 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5991 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5992 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5993 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5994 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5995 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5996 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5997 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5998 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5999 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6000 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6001 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6002 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6003 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6004 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6005 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6006 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6007 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6008 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6009 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6010 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6011 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6012 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6013 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6014 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6015 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6016 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6017 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6018 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6019 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6020 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6021 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6022 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6023 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6024 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6025 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6026 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6027 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6028 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6029 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6030 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6031 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6032 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6033 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6034 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6035 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6036 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6037 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6038 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6039 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6040 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6041 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6042 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6043 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6044 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6045 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6046 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6047 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6048 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6049 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6050 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6051 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6052 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6053 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6054 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6055 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6056 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6057 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6058 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6059 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6060 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6061 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6062 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6063 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6064 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6065 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6066 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6067 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6068 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6069 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6070 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6071 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6072 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6073 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6074 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6075 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6076 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6077 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6078 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6079 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6080 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6081 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6082 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6083 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6084 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6085 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6086 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6087 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6088 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6089 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6090 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6091 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6092 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6093 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6094 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6095 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6096 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6097 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6098 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6099 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6100 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6101 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6102 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6103 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6104 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6105 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6106 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6107 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6108 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6109 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6110 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6111 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6112 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6113 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6114 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6115 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6116 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6117 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6118 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6119 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6120 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6121 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6122 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6123 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6124 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6125 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6126 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6127 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6128 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6129 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6130 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6131 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6132 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6133 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6134 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6135 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6136 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6137 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6138 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6139 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6140 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6141 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6142 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6143 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6144 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6145 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6146 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6147 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6148 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6149 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6150 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6151 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6152 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6153 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6154 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6155 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6156 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6157 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6158 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6159 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6160 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6161 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6162 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6163 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6164 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6165 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6166 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6167 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6168 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6169 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6170 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6171 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6172 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6173 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6174 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6175 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6176 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6177 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6178 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6179 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6180 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6181 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6182 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6183 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6184 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6185 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6186 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6187 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6188 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6189 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6190 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6191 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6192 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6193 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6194 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6195 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6196 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6197 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6198 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6199 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6200 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6201 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6202 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6203 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6204 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6205 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6206 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6207 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6208 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6209 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6210 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6211 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6212 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6213 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6214 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6215 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6216 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6217 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6218 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6219 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6220 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6221 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6222 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6223 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6224 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6225 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6226 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6227 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6228 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6229 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6230 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6231 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6232 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6233 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6234 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6235 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6236 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6237 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6238 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6239 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6240 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6241 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6242 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6243 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6244 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6245 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6246 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6247 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6248 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6249 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6250 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6251 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6252 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6253 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6254 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6255 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6256 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6257 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6258 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6259 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6260 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6261 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6262 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6263 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6264 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6265 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6266 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6267 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6268 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6269 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6270 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6271 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6272 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6273 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6274 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6275 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6276 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6277 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6278 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6279 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6280 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6281 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6282 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6283 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6284 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6285 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6286 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6287 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6288 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6289 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6290 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6291 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6292 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6293 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6294 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6295 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6296 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6297 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6298 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6299 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6300 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6301 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6302 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6303 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6304 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6305 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6306 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6307 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6308 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6309 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6310 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6311 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6312 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6313 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6314 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6315 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6316 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6317 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6318 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6319 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6320 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6321 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6322 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6323 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6324 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6325 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6326 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6327 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6328 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6329 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6330 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6331 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6332 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6333 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6334 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6335 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6336 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6337 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6338 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6339 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6340 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6341 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6342 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6343 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6344 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6345 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6346 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6347 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6348 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6349 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6350 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6351 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6352 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6353 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6354 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6355 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6356 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6357 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6358 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6359 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6360 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6361 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6362 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6363 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6364 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6365 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6366 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6367 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6368 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6369 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6370 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6371 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6372 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6373 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6374 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6375 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6376 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6377 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6378 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6379 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6380 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6381 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6382 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6383 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6384 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6385 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6386 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6387 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6388 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6389 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6390 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6391 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6392 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6393 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6394 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6395 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6396 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6397 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6398 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6399 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6400 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6401 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6402 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6403 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6404 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6405 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6406 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6407 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6408 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6409 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6410 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6411 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6412 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6413 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6414 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6415 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6416 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6417 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6418 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6419 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6420 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6421 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6422 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6423 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6424 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6425 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6426 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6427 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6428 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6429 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6430 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6431 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6432 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6433 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6434 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6435 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6436 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6437 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6438 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6439 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6440 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6441 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6442 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6443 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6444 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6445 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6446 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6447 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6448 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6449 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6450 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6451 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6452 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6453 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6454 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6455 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6456 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6457 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6458 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6459 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6460 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6461 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6462 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6463 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6464 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6465 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6466 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6467 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6468 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6469 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6470 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6471 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6472 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6473 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6474 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6475 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6476 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6477 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6478 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6479 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6480 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6481 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6482 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6483 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6484 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6485 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6486 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6487 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6488 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6489 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6490 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6491 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6492 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6493 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6494 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6495 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6496 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6497 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6498 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6499 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6500 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6501 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6502 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6503 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6504 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6505 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6506 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6507 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6508 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6509 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6510 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6511 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6512 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6513 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6514 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6515 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6516 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6517 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6518 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6519 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6520 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6521 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6522 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6523 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6524 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6525 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6526 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6527 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6528 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6529 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6530 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6531 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6532 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6533 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6534 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6535 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6536 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6537 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6538 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6539 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6540 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6541 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6542 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6543 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6544 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6545 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6546 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6547 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6548 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6549 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6550 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6551 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6552 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6553 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6554 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6555 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6556 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6557 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6558 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6559 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6560 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6561 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6562 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6563 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6564 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6565 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6566 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6567 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6568 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6569 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6570 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6571 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6572 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6573 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6574 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6575 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6576 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6577 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6578 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6579 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6580 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6581 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6582 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6583 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6584 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6585 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6586 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6587 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6588 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6589 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6590 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6591 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6592 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6593 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6594 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6595 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6596 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6597 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6598 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6599 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6600 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6601 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6602 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6603 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6604 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6605 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6606 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6607 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6608 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6609 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6610 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6611 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6612 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6613 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6614 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6615 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6616 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6617 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6618 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6619 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6620 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6621 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6622 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6623 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6624 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6625 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6626 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6627 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6628 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6629 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6630 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6631 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6632 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6633 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6634 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6635 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6636 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6637 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6638 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6639 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6640 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6641 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6642 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6643 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6644 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6645 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6646 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6647 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6648 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6649 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6650 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6651 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6652 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6653 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6654 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6655 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6656 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6657 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6658 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6659 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6660 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6661 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6662 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6663 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6664 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6665 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6666 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6667 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6668 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6669 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6670 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6671 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6672 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6673 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6674 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6675 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6676 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6677 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6678 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6679 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6680 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6681 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6682 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6683 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6684 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6685 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6686 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6687 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6688 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6689 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6690 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6691 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6692 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6693 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6694 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6695 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6696 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6697 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6698 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6699 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6700 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6701 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6702 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6703 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6704 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6705 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6706 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6707 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6708 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6709 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6710 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6711 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6712 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6713 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6714 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6715 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6716 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6717 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6718 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6719 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6720 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6721 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6722 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6723 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6724 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6725 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6726 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6727 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6728 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6729 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6730 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6731 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6732 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6733 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6734 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6735 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6736 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6737 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6738 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6739 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6740 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6741 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6742 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6743 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6744 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6745 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6746 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6747 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6748 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6749 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6750 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6751 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6752 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6753 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6754 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6755 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6756 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6757 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6758 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6759 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6760 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6761 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6762 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6763 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6764 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6765 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6766 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6767 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6768 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6769 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6770 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6771 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6772 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6773 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6774 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6775 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6776 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6777 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6778 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6779 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6780 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6781 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6782 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6783 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6784 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6785 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6786 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6787 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6788 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6789 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6790 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6791 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6792 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6793 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6794 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6795 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6796 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6797 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6798 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6799 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6800 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6801 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6802 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6803 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6804 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6805 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6806 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6807 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6808 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6809 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6810 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6811 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6812 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6813 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6814 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6815 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6816 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6817 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6818 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6819 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6820 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6821 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6822 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6823 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6824 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6825 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6826 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6827 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6828 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6829 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6830 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6831 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6832 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6833 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6834 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6835 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6836 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6837 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6838 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6839 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6840 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6841 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6842 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6843 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6844 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6845 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6846 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6847 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6848 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6849 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6850 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6851 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6852 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6853 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6854 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6855 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6856 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6857 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6858 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6859 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6860 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6861 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6862 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6863 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6864 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6865 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6866 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6867 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6868 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6869 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6870 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6871 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6872 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6873 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6874 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6875 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6876 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6877 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6878 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6879 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6880 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6881 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6882 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6883 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6884 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6885 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6886 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6887 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6888 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6889 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6890 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6891 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6892 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6893 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6894 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6895 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6896 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6897 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6898 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6899 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6900 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6901 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6902 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6903 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6904 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6905 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6906 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6907 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6908 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6909 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6910 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6911 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6912 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6913 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6914 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6915 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6916 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6917 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6918 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6919 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6920 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6921 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6922 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6923 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6924 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6925 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6926 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6927 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6928 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6929 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6930 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6931 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6932 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6933 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6934 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6935 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6936 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6937 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6938 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6939 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6940 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6941 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6942 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6943 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6944 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6945 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6946 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6947 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6948 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6949 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6950 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6951 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6952 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6953 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6954 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6955 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6956 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6957 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6958 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6959 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6960 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6961 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6962 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6963 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6964 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6965 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6966 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6967 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6968 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6969 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6970 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6971 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6972 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6973 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6974 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6975 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6976 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6977 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6978 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6979 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6980 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6981 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6982 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6983 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6984 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6985 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6986 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6987 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6988 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6989 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6990 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6991 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6992 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6993 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6994 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6995 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6996 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6997 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6998 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6999 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7000 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7001 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7002 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7003 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7004 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7005 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7006 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7007 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7008 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7009 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7010 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7011 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7012 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7013 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7014 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7015 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7016 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7017 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7018 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7019 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7020 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7021 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7022 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7023 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7024 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7025 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7026 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7027 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7028 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7029 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7030 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7031 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7032 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7033 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7034 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7035 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7036 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7037 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7038 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7039 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7040 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7041 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7042 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7043 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7044 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7045 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7046 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7047 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7048 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7049 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7050 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7051 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7052 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7053 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7054 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7055 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7056 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7057 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7058 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7059 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7060 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7061 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7062 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7063 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7064 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7065 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7066 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7067 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7068 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7069 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7070 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7071 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7072 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7073 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7074 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7075 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7076 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7077 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7078 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7079 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7080 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7081 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7082 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7083 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7084 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7085 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7086 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7087 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7088 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7089 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7090 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7091 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7092 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7093 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7094 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7095 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7096 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7097 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7098 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7099 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7100 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7101 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7102 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7103 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7104 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7105 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7106 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7107 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7108 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7109 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7110 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7111 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7112 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7113 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7114 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7115 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7116 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7117 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7118 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7119 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7120 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7121 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7122 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7123 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7124 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7125 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7126 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7127 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7128 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7129 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7130 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7131 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7132 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7133 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7134 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7135 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7136 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7137 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7138 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7139 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7140 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7141 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7142 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7143 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7144 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7145 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7146 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7147 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7148 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7149 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7150 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7151 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7152 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7153 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7154 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7155 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7156 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7157 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7158 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7159 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7160 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7161 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7162 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7163 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7164 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7165 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7166 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7167 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7168 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7169 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7170 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7171 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7172 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7173 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7174 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7175 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7176 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7177 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7178 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7179 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7180 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7181 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7182 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7183 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7184 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7185 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7186 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7187 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7188 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7189 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7190 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7191 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7192 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7193 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7194 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7195 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7196 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7197 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7198 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7199 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7200 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7201 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7202 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7203 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7204 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7205 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7206 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7207 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7208 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7209 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7210 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7211 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7212 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7213 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7214 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7215 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7216 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7217 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7218 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7219 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7220 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7221 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7222 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7223 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7224 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7225 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7226 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7227 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7228 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7229 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7230 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7231 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7232 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7233 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7234 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7235 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7236 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7237 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7238 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7239 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7240 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7241 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7242 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7243 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7244 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7245 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7246 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7247 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7248 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7249 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7250 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7251 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7252 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7253 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7254 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7255 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7256 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7257 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7258 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7259 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7260 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7261 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7262 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7263 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7264 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7265 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7266 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7267 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7268 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7269 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7270 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7271 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7272 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7273 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7274 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7275 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7276 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7277 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7278 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7279 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7280 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7281 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7282 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7283 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7284 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7285 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7286 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7287 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7288 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7289 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7290 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7291 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7292 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7293 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7294 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7295 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7296 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7297 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7298 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7299 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7300 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7301 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7302 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7303 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7304 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7305 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7306 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7307 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7308 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7309 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7310 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7311 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7312 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7313 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7314 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7315 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7316 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7317 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7318 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7319 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7320 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7321 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7322 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7323 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7324 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7325 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7326 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7327 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7328 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7329 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7330 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7331 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7332 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7333 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7334 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7335 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7336 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7337 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7338 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7339 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7340 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7341 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7342 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7343 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7344 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7345 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7346 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7347 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7348 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7349 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7350 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7351 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7352 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7353 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7354 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7355 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7356 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7357 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7358 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7359 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7360 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7361 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7362 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7363 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7364 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7365 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7366 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7367 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7368 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7369 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7370 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7371 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7372 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7373 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7374 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7375 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7376 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7377 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7378 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7379 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7380 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7381 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7382 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7383 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7384 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7385 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7386 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7387 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7388 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7389 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7390 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7391 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7392 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7393 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7394 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7395 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7396 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7397 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7398 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7399 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7400 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7401 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7402 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7403 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7404 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7405 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7406 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7407 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7408 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7409 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7410 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7411 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7412 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7413 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7414 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7415 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7416 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7417 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7418 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7419 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7420 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7421 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7422 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7423 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7424 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7425 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7426 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7427 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7428 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7429 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7430 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7431 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7432 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7433 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7434 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7435 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7436 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7437 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7438 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7439 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7440 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7441 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7442 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7443 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7444 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7445 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7446 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7447 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7448 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7449 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7450 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7451 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7452 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7453 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7454 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7455 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7456 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7457 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7458 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7459 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7460 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7461 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7462 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7463 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7464 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7465 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7466 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7467 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7468 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7469 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7470 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7471 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7472 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7473 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7474 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7475 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7476 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7477 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7478 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7479 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7480 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7481 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7482 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7483 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7484 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7485 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7486 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7487 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7488 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7489 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7490 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7491 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7492 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7493 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7494 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7495 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7496 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7497 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7498 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7499 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7500 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7501 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7502 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7503 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7504 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7505 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7506 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7507 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7508 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7509 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7510 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7511 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7512 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7513 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7514 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7515 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7516 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7517 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7518 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7519 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7520 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7521 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7522 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7523 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7524 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7525 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7526 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7527 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7528 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7529 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7530 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7531 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7532 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7533 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7534 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7535 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7536 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7537 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7538 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7539 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7540 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7541 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7542 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7543 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7544 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7545 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7546 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7547 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7548 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7549 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7550 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7551 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7552 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7553 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7554 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7555 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7556 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7557 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7558 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7559 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7560 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7561 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7562 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7563 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7564 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7565 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7566 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7567 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7568 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7569 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7570 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7571 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7572 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7573 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7574 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7575 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7576 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7577 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7578 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7579 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7580 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7581 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7582 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7583 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7584 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7585 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7586 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7587 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7588 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7589 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7590 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7591 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7592 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7593 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7594 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7595 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7596 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7597 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7598 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7599 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7600 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7601 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7602 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7603 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7604 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7605 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7606 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7607 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7608 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7609 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7610 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7611 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7612 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7613 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7614 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7615 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7616 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7617 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7618 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7619 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7620 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7621 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7622 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7623 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7624 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7625 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7626 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7627 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7628 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7629 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7630 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7631 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7632 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7633 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7634 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7635 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7636 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7637 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7638 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7639 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7640 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7641 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7642 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7643 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7644 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7645 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7646 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7647 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7648 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7649 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7650 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7651 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7652 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7653 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7654 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7655 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7656 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7657 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7658 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7659 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7660 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7661 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7662 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7663 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7664 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7665 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7666 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7667 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7668 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7669 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7670 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7671 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7672 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7673 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7674 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7675 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7676 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7677 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7678 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7679 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7680 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7681 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7682 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7683 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7684 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7685 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7686 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7687 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7688 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7689 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7690 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7691 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7692 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7693 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7694 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7695 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7696 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7697 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7698 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7699 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7700 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7701 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7702 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7703 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7704 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7705 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7706 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7707 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7708 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7709 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7710 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7711 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7712 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7713 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7714 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7715 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7716 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7717 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7718 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7719 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7720 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7721 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7722 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7723 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7724 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7725 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7726 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7727 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7728 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7729 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7730 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7731 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7732 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7733 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7734 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7735 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7736 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7737 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7738 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7739 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7740 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7741 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7742 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7743 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7744 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7745 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7746 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7747 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7748 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7749 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7750 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7751 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7752 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7753 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7754 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7755 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7756 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7757 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7758 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7759 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7760 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7761 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7762 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7763 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7764 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7765 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7766 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7767 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7768 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7769 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7770 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7771 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7772 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7773 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7774 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7775 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7776 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7777 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7778 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7779 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7780 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7781 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7782 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7783 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7784 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7785 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7786 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7787 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7788 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7789 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7790 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7791 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7792 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7793 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7794 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7795 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7796 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7797 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7798 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7799 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7800 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7801 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7802 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7803 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7804 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7805 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7806 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7807 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7808 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7809 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7810 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7811 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7812 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7813 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7814 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7815 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7816 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7817 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7818 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7819 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7820 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7821 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7822 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7823 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7824 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7825 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7826 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7827 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7828 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7829 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7830 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7831 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7832 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7833 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7834 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7835 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7836 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7837 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7838 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7839 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7840 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7841 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7842 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7843 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7844 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7845 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7846 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7847 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7848 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7849 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7850 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7851 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7852 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7853 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7854 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7855 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7856 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7857 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7858 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7859 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7860 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7861 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7862 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7863 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7864 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7865 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7866 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7867 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7868 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7869 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7870 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7871 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7872 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7873 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7874 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7875 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7876 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7877 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7878 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7879 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7880 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7881 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7882 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7883 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7884 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7885 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7886 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7887 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7888 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7889 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7890 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7891 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7892 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7893 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7894 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7895 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7896 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7897 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7898 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7899 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7900 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7901 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7902 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7903 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7904 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7905 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7906 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7907 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7908 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7909 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7910 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7911 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7912 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7913 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7914 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7915 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7916 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7917 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7918 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7919 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7920 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7921 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7922 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7923 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7924 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7925 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7926 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7927 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7928 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7929 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7930 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7931 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7932 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7933 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7934 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7935 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7936 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7937 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7938 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7939 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7940 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7941 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7942 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7943 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7944 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7945 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7946 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7947 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7948 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7949 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7950 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7951 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7952 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7953 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7954 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7955 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7956 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7957 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7958 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7959 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7960 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7961 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7962 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7963 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7964 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7965 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7966 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7967 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7968 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7969 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7970 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7971 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7972 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7973 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7974 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7975 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7976 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7977 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7978 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7979 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7980 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7981 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7982 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7983 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7984 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7985 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7986 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7987 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7988 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7989 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7990 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7991 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7992 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7993 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7994 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7995 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7996 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7997 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7998 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7999 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8000 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8001 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8002 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8003 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8004 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8005 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8006 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8007 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8008 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8009 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8010 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8011 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8012 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8013 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8014 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8015 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8016 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8017 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8018 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8019 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8020 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8021 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8022 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8023 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8024 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8025 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8026 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8027 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8028 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8029 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8030 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8031 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8032 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8033 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8034 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8035 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8036 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8037 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8038 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8039 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8040 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8041 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8042 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8043 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8044 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8045 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8046 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8047 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8048 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8049 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8050 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8051 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8052 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8053 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8054 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8055 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8056 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8057 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8058 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8059 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8060 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8061 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8062 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8063 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8064 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8065 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8066 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8067 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8068 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8069 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8070 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8071 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8072 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8073 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8074 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8075 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8076 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8077 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8078 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8079 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8080 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8081 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8082 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8083 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8084 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8085 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8086 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8087 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8088 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8089 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8090 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8091 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8092 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8093 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8094 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8095 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8096 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8097 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8098 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8099 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8100 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8101 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8102 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8103 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8104 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8105 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8106 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8107 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8108 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8109 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8110 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8111 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8112 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8113 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8114 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8115 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8116 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8117 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8118 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8119 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8120 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8121 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8122 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8123 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8124 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8125 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8126 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8127 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8128 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8129 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8130 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8131 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8132 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8133 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8134 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8135 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8136 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8137 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8138 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8139 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8140 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8141 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8142 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8143 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8144 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8145 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8146 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8147 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8148 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8149 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8150 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8151 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8152 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8153 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8154 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8155 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8156 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8157 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8158 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8159 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8160 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8161 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8162 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8163 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8164 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8165 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8166 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8167 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8168 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8169 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8170 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8171 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8172 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8173 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8174 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8175 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8176 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8177 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8178 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8179 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8180 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8181 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8182 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8183 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8184 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8185 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8186 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8187 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8188 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8189 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8190 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8191 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8192 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8193 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8194 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8195 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8196 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8197 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8198 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8199 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8200 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8201 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8202 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8203 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8204 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8205 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8206 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8207 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8208 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8209 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8210 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8211 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8212 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8213 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8214 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8215 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8216 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8217 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8218 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8219 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8220 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8221 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8222 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8223 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8224 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8225 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8226 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8227 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8228 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8229 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8230 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8231 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8232 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8233 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8234 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8235 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8236 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8237 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8238 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8239 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8240 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8241 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8242 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8243 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8244 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8245 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8246 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8247 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8248 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8249 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8250 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8251 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8252 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8253 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8254 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8255 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8256 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8257 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8258 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8259 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8260 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8261 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8262 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8263 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8264 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8265 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8266 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8267 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8268 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8269 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8270 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8271 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8272 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8273 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8274 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8275 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8276 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8277 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8278 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8279 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8280 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8281 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8282 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8283 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8284 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8285 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8286 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8287 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8288 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8289 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8290 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8291 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8292 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8293 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8294 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8295 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8296 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8297 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8298 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8299 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8300 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8301 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8302 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8303 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8304 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8305 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8306 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8307 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8308 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8309 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8310 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8311 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8312 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8313 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8314 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8315 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8316 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8317 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8318 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8319 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8320 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8321 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8322 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8323 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8324 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8325 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8326 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8327 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8328 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8329 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8330 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8331 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8332 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8333 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8334 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8335 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8336 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8337 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8338 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8339 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8340 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8341 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8342 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8343 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8344 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8345 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8346 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8347 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8348 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8349 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8350 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8351 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8352 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8353 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8354 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8355 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8356 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8357 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8358 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8359 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8360 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8361 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8362 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8363 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8364 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8365 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8366 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8367 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8368 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8369 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8370 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8371 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8372 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8373 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8374 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8375 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8376 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8377 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8378 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8379 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8380 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8381 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8382 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8383 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8384 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8385 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8386 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8387 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8388 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8389 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8390 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8391 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8392 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8393 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8394 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8395 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8396 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8397 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8398 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8399 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8400 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8401 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8402 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8403 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8404 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8405 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8406 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8407 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8408 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8409 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8410 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8411 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8412 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8413 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8414 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8415 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8416 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8417 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8418 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8419 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8420 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8421 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8422 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8423 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8424 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8425 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8426 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8427 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8428 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8429 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8430 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8431 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8432 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8433 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8434 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8435 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8436 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8437 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8438 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8439 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8440 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8441 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8442 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8443 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8444 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8445 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8446 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8447 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8448 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8449 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8450 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8451 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8452 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8453 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8454 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8455 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8456 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8457 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8458 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8459 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8460 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8461 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8462 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8463 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8464 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8465 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8466 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8467 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8468 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8469 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8470 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8471 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8472 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8473 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8474 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8475 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8476 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8477 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8478 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8479 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8480 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8481 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8482 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8483 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8484 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8485 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8486 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8487 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8488 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8489 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8490 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8491 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8492 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8493 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8494 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8495 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8496 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8497 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8498 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8499 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8500 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8501 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8502 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8503 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8504 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8505 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8506 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8507 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8508 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8509 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8510 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8511 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8512 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8513 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8514 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8515 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8516 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8517 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8518 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8519 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8520 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8521 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8522 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8523 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8524 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8525 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8526 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8527 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8528 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8529 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8530 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8531 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8532 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8533 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8534 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8535 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8536 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8537 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8538 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8539 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8540 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8541 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8542 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8543 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8544 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8545 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8546 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8547 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8548 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8549 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8550 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8551 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8552 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8553 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8554 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8555 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8556 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8557 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8558 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8559 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8560 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8561 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8562 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8563 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8564 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8565 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8566 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8567 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8568 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8569 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8570 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8571 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8572 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8573 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8574 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8575 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8576 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8577 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8578 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8579 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8580 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8581 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8582 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8583 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8584 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8585 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8586 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8587 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8588 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8589 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8590 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8591 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8592 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8593 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8594 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8595 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8596 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8597 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8598 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8599 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8600 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8601 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8602 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8603 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8604 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8605 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8606 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8607 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8608 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8609 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8610 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8611 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8612 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8613 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8614 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8615 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8616 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8617 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8618 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8619 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8620 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8621 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8622 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8623 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8624 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8625 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8626 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8627 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8628 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8629 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8630 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8631 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8632 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8633 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8634 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8635 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8636 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8637 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8638 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8639 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8640 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8641 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8642 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8643 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8644 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8645 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8646 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8647 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8648 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8649 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8650 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8651 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8652 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8653 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8654 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8655 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8656 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8657 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8658 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8659 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8660 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8661 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8662 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8663 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8664 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8665 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8666 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8667 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8668 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8669 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8670 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8671 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8672 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8673 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8674 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8675 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8676 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8677 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8678 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8679 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8680 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8681 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8682 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8683 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8684 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8685 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8686 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8687 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8688 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8689 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8690 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8691 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8692 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8693 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8694 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8695 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8696 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8697 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8698 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8699 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8700 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8701 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8702 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8703 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8704 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8705 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8706 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8707 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8708 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8709 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8710 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8711 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8712 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8713 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8714 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8715 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8716 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8717 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8718 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8719 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8720 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8721 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8722 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8723 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8724 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8725 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8726 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8727 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8728 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8729 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8730 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8731 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8732 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8733 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8734 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8735 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8736 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8737 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8738 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8739 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8740 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8741 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8742 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8743 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8744 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8745 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8746 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8747 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8748 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8749 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8750 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8751 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8752 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8753 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8754 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8755 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8756 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8757 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8758 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8759 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8760 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8761 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8762 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8763 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8764 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8765 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8766 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8767 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8768 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8769 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8770 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8771 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8772 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8773 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8774 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8775 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8776 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8777 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8778 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8779 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8780 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8781 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8782 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8783 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8784 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8785 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8786 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8787 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8788 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8789 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8790 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8791 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8792 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8793 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8794 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8795 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8796 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8797 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8798 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8799 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8800 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8801 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8802 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8803 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8804 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8805 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8806 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8807 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8808 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8809 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8810 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8811 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8812 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8813 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8814 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8815 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8816 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8817 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8818 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8819 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8820 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8821 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8822 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8823 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8824 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8825 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8826 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8827 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8828 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8829 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8830 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8831 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8832 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8833 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8834 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8835 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8836 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8837 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8838 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8839 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8840 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8841 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8842 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8843 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8844 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8845 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8846 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8847 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8848 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8849 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8850 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8851 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8852 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8853 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8854 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8855 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8856 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8857 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8858 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8859 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8860 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8861 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8862 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8863 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8864 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8865 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8866 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8867 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8868 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8869 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8870 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8871 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8872 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8873 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8874 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8875 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8876 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8877 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8878 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8879 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8880 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8881 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8882 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8883 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8884 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8885 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8886 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8887 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8888 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8889 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8890 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8891 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8892 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8893 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8894 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8895 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8896 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8897 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8898 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8899 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8900 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8901 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8902 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8903 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8904 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8905 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8906 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8907 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8908 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8909 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8910 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8911 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8912 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8913 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8914 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8915 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8916 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8917 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8918 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8919 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8920 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8921 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8922 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8923 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8924 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8925 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8926 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8927 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8928 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8929 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8930 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8931 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8932 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8933 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8934 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8935 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8936 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8937 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8938 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8939 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8940 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8941 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8942 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8943 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8944 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8945 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8946 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8947 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8948 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8949 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8950 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8951 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8952 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8953 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8954 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8955 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8956 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8957 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8958 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8959 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8960 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8961 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8962 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8963 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8964 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8965 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8966 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8967 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8968 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8969 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8970 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8971 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8972 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8973 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8974 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8975 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8976 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8977 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8978 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8979 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8980 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8981 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8982 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8983 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8984 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8985 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8986 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8987 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8988 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8989 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8990 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8991 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8992 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8993 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8994 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8995 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8996 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8997 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8998 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8999 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9000 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9001 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9002 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9003 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9004 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9005 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9006 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9007 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9008 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9009 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9010 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9011 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9012 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9013 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9014 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9015 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9016 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9017 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9018 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9019 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9020 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9021 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9022 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9023 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9024 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9025 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9026 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9027 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9028 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9029 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9030 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9031 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9032 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9033 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9034 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9035 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9036 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9037 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9038 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9039 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9040 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9041 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9042 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9043 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9044 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9045 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9046 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9047 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9048 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9049 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9050 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9051 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9052 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9053 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9054 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9055 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9056 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9057 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9058 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9059 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9060 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9061 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9062 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9063 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9064 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9065 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9066 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9067 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9068 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9069 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9070 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9071 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9072 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9073 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9074 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9075 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9076 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9077 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9078 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9079 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9080 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9081 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9082 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9083 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9084 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9085 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9086 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9087 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9088 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9089 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9090 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9091 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9092 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9093 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9094 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9095 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9096 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9097 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9098 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9099 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9100 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9101 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9102 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9103 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9104 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9105 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9106 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9107 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9108 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9109 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9110 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9111 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9112 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9113 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9114 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9115 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9116 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9117 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9118 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9119 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9120 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9121 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9122 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9123 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9124 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9125 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9126 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9127 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9128 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9129 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9130 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9131 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9132 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9133 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9134 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9135 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9136 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9137 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9138 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9139 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9140 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9141 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9142 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9143 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9144 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9145 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9146 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9147 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9148 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9149 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9150 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9151 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9152 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9153 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9154 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9155 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9156 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9157 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9158 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9159 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9160 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9161 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9162 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9163 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9164 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9165 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9166 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9167 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9168 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9169 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9170 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9171 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9172 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9173 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9174 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9175 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9176 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9177 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9178 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9179 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9180 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9181 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9182 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9183 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9184 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9185 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9186 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9187 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9188 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9189 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9190 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9191 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9192 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9193 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9194 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9195 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9196 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9197 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9198 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9199 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9200 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9201 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9202 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9203 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9204 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9205 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9206 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9207 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9208 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9209 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9210 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9211 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9212 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9213 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9214 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9215 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9216 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9217 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9218 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9219 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9220 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9221 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9222 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9223 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9224 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9225 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9226 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9227 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9228 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9229 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9230 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9231 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9232 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9233 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9234 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9235 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9236 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9237 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9238 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9239 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9240 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9241 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9242 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9243 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9244 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9245 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9246 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9247 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9248 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9249 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9250 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9251 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9252 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9253 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9254 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9255 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9256 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9257 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9258 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9259 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9260 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9261 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9262 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9263 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9264 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9265 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9266 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9267 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9268 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9269 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9270 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9271 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9272 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9273 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9274 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9275 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9276 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9277 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9278 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9279 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9280 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9281 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9282 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9283 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9284 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9285 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9286 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9287 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9288 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9289 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9290 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9291 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9292 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9293 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9294 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9295 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9296 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9297 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9298 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9299 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9300 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9301 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9302 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9303 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9304 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9305 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9306 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9307 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9308 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9309 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9310 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9311 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9312 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9313 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9314 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9315 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9316 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9317 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9318 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9319 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9320 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9321 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9322 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9323 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9324 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9325 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9326 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9327 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9328 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9329 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9330 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9331 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9332 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9333 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9334 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9335 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9336 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9337 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9338 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9339 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9340 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9341 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9342 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9343 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9344 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9345 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9346 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9347 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9348 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9349 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9350 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9351 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9352 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9353 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9354 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9355 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9356 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9357 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9358 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9359 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9360 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9361 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9362 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9363 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9364 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9365 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9366 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9367 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9368 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9369 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9370 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9371 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9372 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9373 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9374 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9375 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9376 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9377 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9378 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9379 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9380 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9381 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9382 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9383 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9384 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9385 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9386 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9387 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9388 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9389 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9390 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9391 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9392 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9393 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9394 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9395 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9396 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9397 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9398 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9399 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9400 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9401 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9402 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9403 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9404 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9405 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9406 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9407 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9408 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9409 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9410 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9411 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9412 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9413 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9414 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9415 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9416 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9417 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9418 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9419 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9420 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9421 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9422 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9423 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9424 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9425 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9426 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9427 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9428 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9429 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9430 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9431 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9432 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9433 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9434 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9435 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9436 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9437 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9438 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9439 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9440 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9441 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9442 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9443 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9444 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9445 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9446 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9447 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9448 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9449 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9450 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9451 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9452 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9453 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9454 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9455 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9456 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9457 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9458 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9459 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9460 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9461 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9462 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9463 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9464 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9465 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9466 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9467 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9468 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9469 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9470 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9471 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9472 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9473 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9474 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9475 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9476 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9477 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9478 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9479 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9480 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9481 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9482 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9483 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9484 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9485 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9486 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9487 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9488 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9489 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9490 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9491 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9492 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9493 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9494 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9495 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9496 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9497 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9498 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9499 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9500 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9501 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9502 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9503 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9504 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9505 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9506 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9507 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9508 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9509 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9510 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9511 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9512 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9513 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9514 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9515 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9516 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9517 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9518 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9519 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9520 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9521 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9522 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9523 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9524 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9525 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9526 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9527 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9528 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9529 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9530 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9531 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9532 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9533 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9534 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9535 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9536 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9537 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9538 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9539 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9540 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9541 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9542 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9543 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9544 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9545 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9546 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9547 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9548 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9549 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9550 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9551 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9552 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9553 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9554 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9555 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9556 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9557 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9558 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9559 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9560 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9561 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9562 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9563 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9564 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9565 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9566 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9567 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9568 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9569 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9570 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9571 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9572 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9573 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9574 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9575 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9576 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9577 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9578 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9579 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9580 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9581 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9582 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9583 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9584 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9585 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9586 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9587 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9588 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9589 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9590 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9591 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9592 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9593 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9594 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9595 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9596 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9597 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9598 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9599 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9600 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9601 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9602 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9603 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9604 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9605 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9606 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9607 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9608 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9609 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9610 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9611 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9612 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9613 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9614 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9615 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9616 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9617 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9618 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9619 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9620 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9621 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9622 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9623 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9624 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9625 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9626 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9627 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9628 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9629 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9630 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9631 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9632 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9633 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9634 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9635 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9636 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9637 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9638 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9639 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9640 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9641 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9642 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9643 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9644 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9645 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9646 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9647 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9648 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9649 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9650 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9651 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9652 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9653 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9654 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9655 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9656 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9657 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9658 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9659 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9660 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9661 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9662 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9663 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9664 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9665 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9666 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9667 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9668 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9669 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9670 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9671 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9672 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9673 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9674 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9675 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9676 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9677 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9678 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9679 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9680 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9681 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9682 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9683 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9684 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9685 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9686 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9687 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9688 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9689 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9690 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9691 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9692 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9693 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9694 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9695 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9696 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9697 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9698 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9699 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9700 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9701 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9702 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9703 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9704 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9705 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9706 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9707 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9708 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9709 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9710 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9711 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9712 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9713 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9714 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9715 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9716 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9717 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9718 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9719 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9720 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9721 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9722 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9723 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9724 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9725 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9726 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9727 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9728 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9729 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9730 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9731 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9732 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9733 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9734 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9735 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9736 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9737 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9738 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9739 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9740 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9741 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9742 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9743 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9744 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9745 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9746 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9747 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9748 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9749 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9750 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9751 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9752 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9753 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9754 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9755 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9756 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9757 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9758 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9759 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9760 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9761 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9762 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9763 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9764 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9765 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9766 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9767 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9768 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9769 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9770 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9771 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9772 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9773 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9774 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9775 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9776 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9777 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9778 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9779 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9780 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9781 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9782 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9783 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9784 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9785 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9786 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9787 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9788 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9789 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9790 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9791 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9792 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9793 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9794 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9795 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9796 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9797 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9798 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9799 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9800 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9801 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9802 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9803 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9804 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9805 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9806 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9807 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9808 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9809 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9810 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9811 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9812 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9813 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9814 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9815 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9816 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9817 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9818 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9819 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9820 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9821 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9822 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9823 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9824 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9825 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9826 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9827 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9828 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9829 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9830 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9831 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9832 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9833 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9834 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9835 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9836 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9837 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9838 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9839 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9840 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9841 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9842 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9843 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9844 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9845 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9846 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9847 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9848 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9849 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9850 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9851 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9852 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9853 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9854 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9855 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9856 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9857 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9858 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9859 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9860 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9861 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9862 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9863 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9864 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9865 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9866 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9867 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9868 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9869 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9870 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9871 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9872 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9873 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9874 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9875 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9876 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9877 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9878 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9879 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9880 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9881 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9882 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9883 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9884 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9885 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9886 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9887 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9888 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9889 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9890 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9891 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9892 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9893 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9894 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9895 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9896 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9897 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9898 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9899 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9900 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9901 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9902 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9903 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9904 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9905 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9906 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9907 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9908 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9909 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9910 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9911 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9912 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9913 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9914 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9915 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9916 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9917 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9918 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9919 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9920 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9921 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9922 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9923 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9924 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9925 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9926 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9927 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9928 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9929 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9930 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9931 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9932 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9933 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9934 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9935 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9936 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9937 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9938 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9939 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9940 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9941 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9942 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9943 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9944 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9945 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9946 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9947 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9948 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9949 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9950 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9951 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9952 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9953 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9954 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9955 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9956 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9957 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9958 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9959 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9960 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9961 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9962 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9963 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9964 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9965 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9966 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9967 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9968 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9969 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9970 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9971 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9972 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9973 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9974 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9975 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9976 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9977 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9978 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9979 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9980 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9981 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9982 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9983 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9984 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9985 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9986 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9987 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9988 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9989 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9990 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9991 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9992 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9993 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9994 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9995 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9996 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9997 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9998 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9999 Loss: 2.2523072727373513e-11\n",
      "Iteration: 10000 Loss: 2.2523072727373513e-11\n",
      "r: 1\n"
     ]
    }
   ],
   "source": [
    "adm = time.time()\n",
    "ADMM_list, dual_ADMM_list, iterations_ADMM = admm.ADMM(N, M, (Q1,B1), (Q2,B2), (Q3,B3), Sigma, D, e1, e2, e31, e32, (x1,x2,x3), 1)\n",
    "fin = time.time()\n",
    "\n",
    "Times[\"ADMM\"] = fin - adm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33f4c36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 10000\n",
      "Primal: (x1,x2,x3),(y1,y2,y3),(z1,z2,z3)\n",
      " ((array([[1188.3, 1188.3, 1188.3, 1188.3, 1188.3],\n",
      "       [44.3, 44.3, 44.3, 44.3, 44.3],\n",
      "       [25.7, 25.7, 25.7, 25.7, 25.7]]), array([[186.6, 704.0, 833.3, 1188.3, 714.3],\n",
      "       [8.9, 30.9, 44.3, 42.8, 43.5],\n",
      "       [4.6, 15.1, 25.6, 19.0, 25.7]]), array([[-0.0, -0.0, 96.7, -0.0, 716.5]])), (array([[1188.3, 1188.3, 1188.3, 1188.3, 1188.3],\n",
      "       [44.3, 44.3, 44.3, 44.3, 44.3],\n",
      "       [25.7, 25.7, 25.7, 25.7, 25.7]]), array([[186.6, 704.0, 833.3, 1188.3, 714.3],\n",
      "       [8.9, 30.9, 44.3, 42.8, 43.5],\n",
      "       [4.6, 15.1, 25.6, 19.0, 25.7]]), array([[0.0, 0.0, 96.7, 0.0, 716.5]])), (array([[-5.0, -5.0, -5.0, 20.0, -5.0],\n",
      "       [-50.0, -50.0, 200.0, -50.0, -50.0],\n",
      "       [-95.0, -95.0, -95.0, -95.0, 380.0]]), array([[-1865.7, -6336.1, -10000.0, -8342.8, -10000.0],\n",
      "       [-1865.7, -6336.1, -10000.0, -8342.8, -10000.0],\n",
      "       [-1865.7, -6336.1, -10000.0, -8342.8, -10000.0]]), array([[-10000.0, -10000.0, -10000.0, -10000.0, -10000.0]])), 'infactible')\n",
      "Dual: (Equilibrium)\n",
      " [[1865.7 6336.1 10000.0 8342.8 10000.0]]\n"
     ]
    }
   ],
   "source": [
    "iter_ = -1\n",
    "print(\"Iterations:\",iterations_ADMM)\n",
    "print(\"Primal: (x1,x2,x3),(y1,y2,y3),(z1,z2,z3)\\n\",ADMM_list[iter_])\n",
    "print(\"Dual: (Equilibrium)\\n\",dual_ADMM_list[iter_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1d7890b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_iters_list = max([iterations_DY,iterations_BA,iterations_ADMM])\n",
    "max_iters_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8240e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_adjusted(x_sol, x_teo, sigma):\n",
    "    x_sol_1, x_sol_2, x_sol_3 = x_sol\n",
    "    x_teo_1, x_teo_2, x_teo_3 = x_teo\n",
    "    sigma=sigma[0]\n",
    "    \n",
    "    return LA.norm(x_sol_1-x_teo_1) + sum([sigma[xi]*LA.norm(x_sol_2[:,xi]-x_teo_2[:,xi]) for xi in range(M)]) + sum([sigma[xi]*LA.norm(x_sol_3[:,xi]-x_teo_3[:,xi]) for xi in range(M)])\n",
    "\n",
    "\n",
    "def norm_adjusted_N(x_sol, x_teo, sigma):\n",
    "    x_sol_1, x_sol_2, x_sol_3 = x_sol\n",
    "    x_teo_1, x_teo_2, x_teo_3 = x_teo\n",
    "    sigma=sigma[0]\n",
    "    \n",
    "    return sum([sigma[xi]*LA.norm(x_sol_1[:,xi][:,np.newaxis]-x_teo_1) for xi in range(M)]) + sum([sigma[xi]*LA.norm(x_sol_2[:,xi]-x_teo_2[:,xi]) for xi in range(M)]) + sum([sigma[xi]*LA.norm(x_sol_3[:,xi]-x_teo_3[:,xi]) for xi in range(M)])\n",
    "\n",
    "\n",
    "\n",
    "def generate_list(lista, lista_2, algoritmo, solution, objective_function, Demanda, max_iterations, P):\n",
    "\n",
    "    # unpack solution\n",
    "    x1, x2, x3 = solution\n",
    "\n",
    "    # create a list with index of graphics\n",
    "    iterations = list(range(max_iterations))\n",
    "\n",
    "    # create list to return\n",
    "    x_solution     = []\n",
    "    Fx_solution    = []\n",
    "    Non_anti_sol   = []\n",
    "    equili_solut   = []\n",
    "    capacity_solut = []\n",
    "    demand_solu    = []\n",
    "    dual_solut     = []\n",
    "    \n",
    "    zero_1 = np.zeros((N,1))\n",
    "    zero_1_N = np.zeros((N,M))\n",
    "    zero_2 = np.zeros((N,M))\n",
    "    zero_3 = np.zeros((1,M))\n",
    "    zeroo = (zero_1, zero_2, zero_3)\n",
    "    zeroo_N = (zero_1_N, zero_2, zero_3)\n",
    "    \n",
    "    # create arrays for each graph\n",
    "\n",
    "    if algoritmo == \"DY\":\n",
    "        for x_algo, x_fact in lista:\n",
    "            x1_algo, x2_algo, x3_algo = x_algo\n",
    "            x_solution.append( norm_adjusted(x_algo, (x1,x2,x3), P)/norm_adjusted((x1,x2,x3), zeroo, P) )\n",
    "            Fx_solution.append( abs(objective_function(x1_algo, x2_algo, x3_algo)[0][0] - objective_function(x1, x2, x3)[0][0] )/abs(objective_function(x1, x2, x3)[0][0]) )\n",
    "            Non_anti_sol.append( LA.norm(x1_algo - np.roll(x1_algo, 1, axis=1)) ) #No aplica a DY\n",
    "            equili_solut.append( norm_adjusted((zero_1,zero_2,x2_algo.sum(axis=0) - (D - x3_algo)), zeroo, P) )\n",
    "            capacity_solut.append( norm_adjusted((zero_1,x1_algo - x2_algo,zero_3), zeroo, P) )\n",
    "            demand_solu.append(norm_adjusted((zero_1,zero_2,D - x3_algo), zeroo, P))\n",
    "\n",
    "        for lambda1, lambda2 in lista_2:\n",
    "            dual_solut.append(norm_adjusted((zero_1,zero_2,rho - lambda2),zeroo,P)/norm_adjusted((zero_1,zero_2,rho),zeroo,P))\n",
    "    \n",
    "\n",
    "    elif algoritmo == \"ADMM\":\n",
    "        for elemento in lista:\n",
    "            x1_algo, x2_algo, x3_algo = elemento[0]\n",
    "            x_solution.append( norm_adjusted_N((x1_algo, x2_algo, x3_algo), (x1,x2,x3), P)/norm_adjusted((x1,x2,x3), zeroo, P) )\n",
    "            Fx_solution.append( abs(objective_function(x1_algo, x2_algo, x3_algo)[0][0] - objective_function(x1, x2, x3)[0][0] )/abs(objective_function(x1, x2, x3)[0][0]) )\n",
    "            Non_anti_sol.append( norm_adjusted_N((x1_algo - np.roll(x1_algo, 1, axis=1),zero_2, zero_3), zeroo, P) ) #No aplica a DY\n",
    "            equili_solut.append( norm_adjusted_N((zero_1_N,zero_2,x2_algo.sum(axis=0) - (D - x3_algo)), zeroo, P) )\n",
    "            capacity_solut.append( norm_adjusted_N((zero_1_N,x1_algo - x2_algo,zero_3), zeroo, P) )\n",
    "            demand_solu.append(norm_adjusted_N((zero_1_N,zero_2,D - x3_algo), zeroo, P))\n",
    "\n",
    "        for lambda2 in lista_2:\n",
    "            dual_solut.append(norm_adjusted_N((zero_1_N,zero_2,rho - lambda2),zeroo,P)/norm_adjusted((zero_1,zero_2,rho),zeroo,P))\n",
    "\n",
    "\n",
    "    elif algoritmo == \"BA\":\n",
    "        for x_algo, x_fact in lista:\n",
    "            x1_algo, x2_algo, x3_algo = x_algo\n",
    "            x_solution.append( norm_adjusted_N((x1_algo, x2_algo, x3_algo), (x1,x2,x3), P)/norm_adjusted((x1,x2,x3), zeroo, P) )\n",
    "            Fx_solution.append( abs(objective_function(x1_algo, x2_algo, x3_algo)[0][0] - objective_function(x1, x2, x3)[0][0] )/abs(objective_function(x1, x2, x3)[0][0]) )\n",
    "            Non_anti_sol.append( norm_adjusted_N((x1_algo - np.roll(x1_algo, 1, axis=1),zero_2, zero_3), zeroo, P) ) #No aplica a DY\n",
    "            equili_solut.append( norm_adjusted_N((zero_1_N,zero_2,x2_algo.sum(axis=0) - (D - x3_algo)), zeroo, P) )\n",
    "            capacity_solut.append( norm_adjusted_N((zero_1_N,x1_algo - x2_algo,zero_3), zeroo, P) )\n",
    "            demand_solu.append(norm_adjusted_N((zero_1_N,zero_2,D - x3_algo), zeroo, P))\n",
    "            \n",
    "        \n",
    "        for lambda1, lambda2 in lista_2:\n",
    "            dual_solut.append(norm_adjusted_N((zero_1_N,zero_2,rho - lambda2),zeroo,P)/norm_adjusted((zero_1,zero_2,rho),zeroo,P))\n",
    "            \n",
    "    \n",
    "    print(\"Completando listas\")\n",
    "    \n",
    "    x_solution     = x_solution     + [None]*(max_iterations-len(lista))\n",
    "    Fx_solution    = Fx_solution    + [None]*(max_iterations-len(lista))\n",
    "    Non_anti_sol   = Non_anti_sol   + [None]*(max_iterations-len(lista))\n",
    "    equili_solut   = equili_solut   + [None]*(max_iterations-len(lista))\n",
    "    capacity_solut = capacity_solut + [None]*(max_iterations-len(lista))\n",
    "    demand_solu    = demand_solu    + [None]*(max_iterations-len(lista))\n",
    "    dual_solut     = dual_solut     + [None]*(max_iterations-len(lista))\n",
    "\n",
    "    k = 0\n",
    "\n",
    "    return iterations[k:], x_solution[k:], Fx_solution[k:], Non_anti_sol[k:], equili_solut[k:], capacity_solut[k:], demand_solu[k:], dual_solut[k:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9094918c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DY\n",
      "Completando listas\n",
      "ADMM\n",
      "Completando listas\n",
      "BA\n",
      "Completando listas\n"
     ]
    }
   ],
   "source": [
    "print(\"DY\")\n",
    "iter_DY, x_DY_sol, Fx_DY_sol, Non_anti_DY, equili_DY_solu, capacity_DY_solu, demand_DY_sol, dual_DY_sol = generate_list(DY_list, Dual_DY_list, \"DY\", (x1, x2, x3), objective_function, D, max_iters_list, Sigma)\n",
    "print(\"ADMM\")\n",
    "iter_ADMM, x_ADMM_sol, Fx_ADMM_sol, Non_anti_ADMM, equili_ADMM_solu, capacity_ADMM_solu, demand_ADMM_sol, dual_ADMM_sol  = generate_list(ADMM_list, dual_ADMM_list, \"ADMM\", (x1, x2, x3), objective_function, D, max_iters_list,Sigma)\n",
    "print(\"BA\")\n",
    "iter_BA, x_BA_sol, Fx_BA_sol, Non_anti_BA, equili_BA_solu, capacity_BA_solu, demand_BA_sol, BA_dual_sol = generate_list(x_BA_list, dual_BA_list, \"BA\", (x1, x2, x3), objective_function, D, max_iters_list, Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56958ac3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Crear un rango de iteraciones para las gráficas\n",
    "k = 0\n",
    "l = int(max_iters_list / 1) + 1\n",
    "\n",
    "zero_1 = np.zeros((N,1))\n",
    "zero_2 = np.zeros((N,M))\n",
    "zero_3 = np.zeros((1,M))\n",
    "zeroo = (zero_1, zero_2, zero_3)\n",
    "\n",
    "# Definir los niveles teóricos según tu solución teórica\n",
    "nivel_teorico_equilibrio = norm_adjusted((zero_1,zero_2,x2.sum(axis=0) - (D - x3)), zeroo, Sigma)\n",
    "nivel_teorico_capacidad = norm_adjusted((zero_1,x1 - x2,zero_3), zeroo, Sigma)\n",
    "nivel_teorico_demanda = norm_adjusted((zero_1,zero_2,D - x3), zeroo, Sigma)\n",
    "\n",
    "# Variable de sufijo para los nombres de archivo\n",
    "suffix = \"_1\"\n",
    "\n",
    "# Función para configurar y guardar cada gráfico\n",
    "def configurar_grafico(ax, x_data, y_data, labels, colors, title, y_label, width, height, y_lim=None, nivel_teorico=None):\n",
    "    for x, y, label, color in zip(x_data, y_data, labels, colors):\n",
    "        ax.plot(x[k:l], y[k:l], '-', linewidth=1.5, label=label, color=color)\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_xlabel('Iteraciones')\n",
    "    ax.legend()\n",
    "    if y_lim:\n",
    "        ax.set_ylim(y_lim)\n",
    "    if nivel_teorico is not None:\n",
    "        ax.axhline(y=nivel_teorico, color='gray', linestyle='--', linewidth=1)\n",
    "    ax.figure.set_figwidth(width)\n",
    "    ax.figure.set_figheight(height)\n",
    "\n",
    "# Datos para los gráficos\n",
    "iter_data = [iter_DY, iter_ADMM, iter_BA]\n",
    "labels = ['TOPS', 'ADMM', 'FPIS']\n",
    "colors = ['green', 'blue', 'red']\n",
    "\n",
    "# Definir la altura y el ancho deseado para cada gráfico\n",
    "width = 10  # Ajusta esta variable según sea necesario\n",
    "height = 3  # Ajusta esta variable según sea necesario\n",
    "\n",
    "# Crear y guardar cada figura individualmente\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "configurar_grafico(ax, iter_data, [x_DY_sol, x_ADMM_sol, x_BA_sol], labels, colors, 'Distancia al Óptimo', \n",
    "                   r'$\\frac{\\|x^{k} - x^{*}\\|}{\\|x^{*}\\|}$', width, height, y_lim=(0, 1))\n",
    "plt.savefig(f'images/caso{suffix}/distancia_al_optimo{suffix}.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "configurar_grafico(ax, iter_data, [Fx_DY_sol, Fx_ADMM_sol, Fx_BA_sol], labels, colors, 'Diferencia Valor Función Objetivo', \n",
    "                   r'$\\|f(x^{k})-f(x^{*})\\|$', width, height, y_lim=(0, 1))\n",
    "plt.savefig(f'images/caso{suffix}/diferencia_valor_funcion_objetivo{suffix}.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "configurar_grafico(ax, iter_data[1:], [Non_anti_DY, Non_anti_ADMM, Non_anti_BA][1:], labels[1:], colors[1:], 'No-Anticipatividad', \n",
    "                   r'$\\|c^{k}-P_{\\mathcal{N}}(c^{k})\\|$', width, height)#, y_lim=(0, 25))\n",
    "plt.savefig(f'images/caso{suffix}/no_anticipatividad{suffix}.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "configurar_grafico(ax, iter_data, [equili_DY_solu, equili_ADMM_solu, equili_BA_solu], labels, colors, 'Restricción de Equilibrio', \n",
    "                   r'$\\|\\textbf{1}^{T}g^{k}-(D-q^{k})\\|$', width, height, y_lim=(-0.1, 1.0), nivel_teorico=nivel_teorico_equilibrio)\n",
    "plt.savefig(f'images/caso{suffix}/restriccion_de_equilibrio{suffix}.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "configurar_grafico(ax, iter_data, [capacity_DY_solu, capacity_ADMM_solu, capacity_BA_solu], labels, colors, 'Restricción de Capacidad de Producción', \n",
    "                   r'$\\|c^{k} - g^{k}\\|$', width, height, nivel_teorico=nivel_teorico_capacidad)\n",
    "plt.savefig(f'images/caso{suffix}/restriccion_de_capacidad_de_produccion{suffix}.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "configurar_grafico(ax, iter_data, [demand_DY_sol, demand_ADMM_sol, demand_BA_sol], labels, colors, 'Diferencia de Demanda Satisfecha', \n",
    "                   r'$\\|D-q^{k}\\|$', width, height, y_lim=(0, 4000), nivel_teorico=nivel_teorico_demanda)\n",
    "plt.savefig(f'images/caso{suffix}/diferencia_de_demanda_satisfecha{suffix}.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "configurar_grafico(ax, iter_data, [dual_DY_sol, dual_ADMM_sol, BA_dual_sol], labels, colors, 'Diferencia al Precio Óptimo', \n",
    "                   r'$\\frac{\\|\\rho^{k}-\\rho^{*}\\|}{\\|\\rho^{*}\\|}$', width, height, y_lim=(0, 1))\n",
    "plt.savefig(f'images/caso{suffix}/diferencia_al_precio_optimo{suffix}.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0041b318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CP': 0.21850204467773438,\n",
       " 'DY': 1190.7133128643036,\n",
       " 'BA': 629.5290949344635,\n",
       " 'ADMM': 601.9758615493774}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e36f3d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9999,\n",
       " 0.003097221600233452,\n",
       " 0.00013854712864954226,\n",
       " 0.0,\n",
       " 9.391435861516586e-06,\n",
       " 464.2596049813523,\n",
       " 775.6655535991703,\n",
       " 0.002810432893130672)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_DY[-1], x_DY_sol[-1], Fx_DY_sol[-1], Non_anti_DY[-1], equili_DY_solu[-1], capacity_DY_solu[-1], demand_DY_sol[-1], dual_DY_sol[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb25dc82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9999,\n",
       " 2.438746827678754e-09,\n",
       " 1.1436660878588725e-11,\n",
       " 0.0,\n",
       " 9.391314188178513e-06,\n",
       " 464.5878350997092,\n",
       " 777.3599568808092,\n",
       " 2.7022146753616922e-11)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_BA[-1], x_BA_sol[-1], Fx_BA_sol[-1], Non_anti_BA[-1], equili_BA_solu[-1], capacity_BA_solu[-1], demand_BA_sol[-1], BA_dual_sol[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1a81cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9999,\n",
       " 3.8454743304016674e-11,\n",
       " 1.6132027363332004e-12,\n",
       " 4.5474735088646414e-14,\n",
       " 9.394005041940546e-06,\n",
       " 464.58783155365506,\n",
       " 777.3599568381936,\n",
       " 1.8358130691611741e-13)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_ADMM[-1], x_ADMM_sol[-1], Fx_ADMM_sol[-1], Non_anti_ADMM[-1], equili_ADMM_solu[-1], capacity_ADMM_solu[-1], demand_ADMM_sol[-1], dual_ADMM_sol[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30c264f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9.393999260964846e-06, 464.5878315116832, 777.3599568385031)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nivel_teorico_equilibrio, nivel_teorico_capacidad, nivel_teorico_demanda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9557bad2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
