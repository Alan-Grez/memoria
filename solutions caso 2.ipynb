{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0227457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import centralized as CP\n",
    "import davisyin as DY\n",
    "import admm as admm\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from numpy import linalg as LA\n",
    "from numpy.linalg import inv\n",
    "import matplotlib as plt\n",
    "from matplotlib import rc\n",
    "# Configura el tipo de letra globalmente\n",
    "rc('font', **{'family': 'serif', 'serif': ['Computer Modern Roman']})\n",
    "rc('text', usetex=True)\n",
    "#plt.rcParams['text.usetex'] = True\n",
    "import matplotlib.pyplot as plt\n",
    "import proyecciones as pro\n",
    "import time\n",
    "import briceno as BA\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96d1d966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20, 0.03, 0.40, 0.14, 0.23]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caso 1: Caso base\n",
    "    \n",
    "# Cambiar criterio de parada por errores relativos.\n",
    "# Establecer la semilla\n",
    "seed = 40\n",
    "#41\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Seteamos los parámetros:\n",
    "N, M = 3, 5  # Son 2 tecnologías, 10 escenarios\n",
    "\n",
    "# Probabilidades:\n",
    "inv_, mc_, voll_, d_ = [50.0,  1000.0, 10000.0, 1000.0]\n",
    "                       #[10.0, 2000.0, 10000.0, 1000.0]\n",
    "                       #[50.0,  1000.0, 10000.0, 1000.0]\n",
    "#Sigma = np.ones((1,M))\n",
    "Sigma = np.random.rand(1,M)\n",
    "Sigma /= Sigma.sum()\n",
    "Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1895a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      " [[5.00]\n",
      " [50.00]\n",
      " [95.00]]\n",
      "MC\n",
      " [[[1000.00 0.00 0.00]\n",
      "  [0.00 21000.00 0.00]\n",
      "  [0.00 0.00 41000.00]]\n",
      "\n",
      " [[900.00 0.00 0.00]\n",
      "  [0.00 20500.00 0.00]\n",
      "  [0.00 0.00 42000.00]]\n",
      "\n",
      " [[1200.00 0.00 0.00]\n",
      "  [0.00 22000.00 0.00]\n",
      "  [0.00 0.00 39000.00]]\n",
      "\n",
      " [[700.00 0.00 0.00]\n",
      "  [0.00 19500.00 0.00]\n",
      "  [0.00 0.00 44000.00]]\n",
      "\n",
      " [[1400.00 0.00 0.00]\n",
      "  [0.00 23000.00 0.00]\n",
      "  [0.00 0.00 37000.00]]]\n",
      "VOLL\n",
      " 10000.0\n",
      "D\n",
      " [[200.00 750.00 1000.00 1250.00 1500.00]]\n",
      "frobenius_norm: 453.2003170270881\n"
     ]
    }
   ],
   "source": [
    "Times = {}\n",
    "r_ = 1\n",
    "\n",
    "\n",
    "# Parámetros funciones:\n",
    "I    = inv_ * np.ones((N, 1)) + r_*np.array([[-45], [0], [45]])\n",
    "print(\"I\\n\",I)\n",
    "aux  = np.array([1 + r_*20*i for i in range(N)])\n",
    "\n",
    "mc_11 = 100\n",
    "mc_22 = 500\n",
    "mc_33 = 1000\n",
    "MC   = np.array([np.diag(mc_*aux + r_*np.array([((-1)**m)*mc_11*m, ((-1)**m)*mc_22*m, ((-1)**(m+1))*mc_33*m])) for m in range(M)])\n",
    "print(\"MC\\n\",MC)\n",
    "VOLL = voll_\n",
    "print(\"VOLL\\n\",VOLL)\n",
    "D    = d_*np.ones((1,M)) + r_*np.array([-800, -250, 0, 250, 500])[np.newaxis]\n",
    "print(\"D\\n\",D)\n",
    "\n",
    "e_ = 0\n",
    "\n",
    "e1  = e_\n",
    "e2  = e_\n",
    "e31 = e_*1e2/2\n",
    "e32 = e_\n",
    "\n",
    "Q1, B1 = np.zeros((N,N)), I\n",
    "Q2, B2 = 0.01*MC, np.zeros((N,M))\n",
    "Q3, B3 = np.zeros((1,M)), VOLL*np.ones((1,M))\n",
    "\n",
    "\n",
    "frobenius_norm = (e1+e2)*np.sqrt(N)+e31+e32+np.array([LA.norm(np.einsum('i,ikl->ikl',Sigma[0],0.01*MC)[xi], 'fro') for xi in range(M)]).sum()\n",
    "#frobenius_norm = max([e1*np.sqrt(N),e31+e32,e2*np.sqrt(N)+np.array([LA.norm(np.einsum('i,ikl->ikl',Sigma[0],0.01*MC)[xi], 'fro') for xi in range(M)]).sum()])\n",
    "print(\"frobenius_norm:\",frobenius_norm)\n",
    "\n",
    "def Grad_Phi_1(x1, Q_1 = np.zeros((N,N)), B_1 = I, e1 = e1, N = N):\n",
    "       return np.dot(Q_1,x1)+B_1# - e1*np.dot(np.identity(N),np.maximum(-x1,0))\n",
    "\n",
    "\n",
    "def Grad_Phi_2(x2, Q_2 = 0.01*MC, B_2 = np.zeros((N,M)), e2 = e2, N = N, M = M):\n",
    "\n",
    "    return np.einsum('ijk,ki->ji', Q_2, x2)+B_2# - e2*np.einsum('ijk,ki->ji', np.array([np.diag(np.ones(N)) for m in range(M)]), np.maximum(-x2,0))\n",
    "\n",
    "\n",
    "def Grad_Phi_3(x3, Q_3 = np.zeros((1,M)), B_3 = VOLL*np.ones((1,M)), D=D, e31=e31, e32= e32, M = M):\n",
    "    return Q_3*x3+B_3 #- e31*np.dot(np.maximum(-x3,0),np.identity(M)) - e32*np.dot(np.maximum(-D+x3,0),np.identity(M))\n",
    "\n",
    "\n",
    "def Grad_Phi(x1,x2,x3, P = Sigma):\n",
    "    return Grad_Phi_1(x1), P*Grad_Phi_2(x2), P*Grad_Phi_3(x3)\n",
    "\n",
    "def Grad_Phi_NA(x1,x2,x3, P = Sigma):\n",
    "    return P*Grad_Phi_1(x1), P*Grad_Phi_2(x2), P*Grad_Phi_3(x3)\n",
    "\n",
    "def Phi_1(x1, Q_1 = np.zeros((N,N)), B_1 = I, C_1 = 0.0, e1 = e1):\n",
    "    return 0.5*np.einsum('ij,ji -> i', x1.T,np.dot(Q_1,x1))[:,np.newaxis]+np.dot(x1.T, B_1)+C_1 + e1/2*LA.norm(np.maximum(-x1.flatten(),0))**2\n",
    "\n",
    "def Phi_2_xi(x2, Q_2 = 0.01*MC, B_2 = np.zeros((N,M)), C_2 = np.zeros((M, 1))):\n",
    "    return 0.5*np.einsum('ij,ji -> i', x2.T, np.einsum('ijk,ki -> ji', Q_2, x2))[:,np.newaxis]+np.einsum('ij,ji->i',x2.T,B_2)[:,np.newaxis]+C_2 + e2/2*LA.norm(np.maximum(-x2.flatten(),0))**2\n",
    "\n",
    "def Phi_3_xi(x3, Q_3 = np.zeros((1,M)), B_3 = VOLL*np.ones((1,M)), C_3 = -VOLL*D ):\n",
    "    return (0.5*x3*Q_3*x3+B_3*x3+C_3).T + e31/2*LA.norm(np.maximum(-x3.flatten(),0))**2 + e32/2*LA.norm(np.maximum((D-x3).flatten(),0))**2\n",
    "\n",
    "\n",
    "def objective_function(x1, x2, x3, P = Sigma, NA = True):\n",
    "\n",
    "# NA = True, cumple la funcion que si se impuso \n",
    "#      la condición de no anticipatividad para x1\n",
    "#      entonces, Phi_1(x1).shape == (M,1)\n",
    "    if NA:\n",
    "        return np.dot(P, Phi_1(x1) +Phi_2_xi(x2)+Phi_3_xi(x3))\n",
    "    else:\n",
    "        return Phi_1(x1)+ np.dot(P, Phi_2_xi(x2)+Phi_3_xi(x3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4f57c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = time.time()\n",
    "x1, x2, x3, rho, mu = map(np.array, CP.modelo(Sigma, N, M, \\\n",
    "                                              parametros = [I.T[0].tolist(),\\\n",
    "                                                            np.array([mc_*aux + r_*np.array([((-1)**m)*mc_11*m, ((-1)**m)*mc_22*m, ((-1)**(m+1))*mc_33*m]) for m in range(M)]).T.tolist(),\\\n",
    "                                                            VOLL,\\\n",
    "                                                            D[0]] , show = 0))\n",
    "fin = time.time()\n",
    "\n",
    "\n",
    "Times[\"CP\"] = fin - cp\n",
    "\n",
    "x1 = x1[:,np.newaxis]\n",
    "x2 = x2.T\n",
    "x3 = x3[np.newaxis,:][0]\n",
    "rho = rho[np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1265a4e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primal:\n",
      "[[1188.19]\n",
      " [44.88]\n",
      " [25.89]]\n",
      "[[186.57 704.01 833.33 1188.19 714.29]\n",
      " [8.88 30.91 44.88 42.83 43.48]\n",
      " [4.55 15.09 25.64 18.98 25.89]]\n",
      "[[0.00 0.00 96.14 0.00 716.34]]\n",
      "Dual:\n",
      "[[0.00 0.00 0.00 34.62 0.00]\n",
      " [0.00 0.00 126.14 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00 419.62]]\n",
      "[[1865.66 6336.06 10000.00 8351.93 10000.00]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"primal:\\n{x1}\\n{x2}\\n{x3}\")\n",
    "print(f\"Dual:\\n{mu}\\n{rho}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c84c5f51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta: 0.0022065297892106934\n",
      "Gamma: 0.003952278108170807\n",
      "Lambda_k: 1\n",
      "Iteration: 1 lambda_k: 1 Loss: 0.935093944351969\n",
      "Iteration: 2 lambda_k: 1 Loss: 0.8317539385093687\n",
      "Iteration: 3 lambda_k: 1 Loss: 0.7899579480348987\n",
      "Iteration: 4 lambda_k: 1 Loss: 0.7974379801062146\n",
      "Iteration: 5 lambda_k: 1 Loss: 0.7868074424584822\n",
      "Iteration: 6 lambda_k: 1 Loss: 0.7760271808846004\n",
      "Iteration: 7 lambda_k: 1 Loss: 0.7688170970944757\n",
      "Iteration: 8 lambda_k: 1 Loss: 0.7635700245610965\n",
      "Iteration: 9 lambda_k: 1 Loss: 0.7596371567303392\n",
      "Iteration: 10 lambda_k: 1 Loss: 0.7561239269847461\n",
      "Iteration: 11 lambda_k: 1 Loss: 0.7527380760804242\n",
      "Iteration: 12 lambda_k: 1 Loss: 0.7494596449563052\n",
      "Iteration: 13 lambda_k: 1 Loss: 0.7462678061064898\n",
      "Iteration: 14 lambda_k: 1 Loss: 0.7431467854172109\n",
      "Iteration: 15 lambda_k: 1 Loss: 0.740088874976767\n",
      "Iteration: 16 lambda_k: 1 Loss: 0.7371056457308612\n",
      "Iteration: 17 lambda_k: 1 Loss: 0.734195260356565\n",
      "Iteration: 18 lambda_k: 1 Loss: 0.7313117269057752\n",
      "Iteration: 19 lambda_k: 1 Loss: 0.7284529125349894\n",
      "Iteration: 20 lambda_k: 1 Loss: 0.7256209290466915\n",
      "Iteration: 21 lambda_k: 1 Loss: 0.7228160539374482\n",
      "Iteration: 22 lambda_k: 1 Loss: 0.7200378465332183\n",
      "Iteration: 23 lambda_k: 1 Loss: 0.7175336998578331\n",
      "Iteration: 24 lambda_k: 1 Loss: 0.7151018318098631\n",
      "Iteration: 25 lambda_k: 1 Loss: 0.7127035534119187\n",
      "Iteration: 26 lambda_k: 1 Loss: 0.7103364475503645\n",
      "Iteration: 27 lambda_k: 1 Loss: 0.7079984305704486\n",
      "Iteration: 28 lambda_k: 1 Loss: 0.7056876736206433\n",
      "Iteration: 29 lambda_k: 1 Loss: 0.7034025526192176\n",
      "Iteration: 30 lambda_k: 1 Loss: 0.7011416118300424\n",
      "Iteration: 31 lambda_k: 1 Loss: 0.6989035365018267\n",
      "Iteration: 32 lambda_k: 1 Loss: 0.696687132335056\n",
      "Iteration: 33 lambda_k: 1 Loss: 0.6944913098341438\n",
      "Iteration: 34 lambda_k: 1 Loss: 0.6923150719793225\n",
      "Iteration: 35 lambda_k: 1 Loss: 0.6901575041794976\n",
      "Iteration: 36 lambda_k: 1 Loss: 0.6880177658370512\n",
      "Iteration: 37 lambda_k: 1 Loss: 0.6858950831540218\n",
      "Iteration: 38 lambda_k: 1 Loss: 0.6837887428848136\n",
      "Iteration: 39 lambda_k: 1 Loss: 0.6816980868551731\n",
      "Iteration: 40 lambda_k: 1 Loss: 0.6796225071056134\n",
      "Iteration: 41 lambda_k: 1 Loss: 0.6775614415486297\n",
      "Iteration: 42 lambda_k: 1 Loss: 0.675514370088368\n",
      "Iteration: 43 lambda_k: 1 Loss: 0.6734808110865563\n",
      "Iteration: 44 lambda_k: 1 Loss: 0.6714603181948159\n",
      "Iteration: 45 lambda_k: 1 Loss: 0.6694524774581035\n",
      "Iteration: 46 lambda_k: 1 Loss: 0.6674569046881037\n",
      "Iteration: 47 lambda_k: 1 Loss: 0.6654732430554735\n",
      "Iteration: 48 lambda_k: 1 Loss: 0.6635011609244243\n",
      "Iteration: 49 lambda_k: 1 Loss: 0.6615403498113358\n",
      "Iteration: 50 lambda_k: 1 Loss: 0.6595905225793934\n",
      "Iteration: 51 lambda_k: 1 Loss: 0.6576514117469774\n",
      "Iteration: 52 lambda_k: 1 Loss: 0.6557227679354873\n",
      "Iteration: 53 lambda_k: 1 Loss: 0.6538043584816359\n",
      "Iteration: 54 lambda_k: 1 Loss: 0.6518959661133852\n",
      "Iteration: 55 lambda_k: 1 Loss: 0.6499973877270216\n",
      "Iteration: 56 lambda_k: 1 Loss: 0.6481084333765472\n",
      "Iteration: 57 lambda_k: 1 Loss: 0.6462289310963192\n",
      "Iteration: 58 lambda_k: 1 Loss: 0.6443587524716956\n",
      "Iteration: 59 lambda_k: 1 Loss: 0.6424977101344549\n",
      "Iteration: 60 lambda_k: 1 Loss: 0.6406456648562726\n",
      "Iteration: 61 lambda_k: 1 Loss: 0.6388024744684058\n",
      "Iteration: 62 lambda_k: 1 Loss: 0.63696800510543\n",
      "Iteration: 63 lambda_k: 1 Loss: 0.6351421307118835\n",
      "Iteration: 64 lambda_k: 1 Loss: 0.633324732578307\n",
      "Iteration: 65 lambda_k: 1 Loss: 0.6315156987980093\n",
      "Iteration: 66 lambda_k: 1 Loss: 0.6297149236953401\n",
      "Iteration: 67 lambda_k: 1 Loss: 0.6279223073503284\n",
      "Iteration: 68 lambda_k: 1 Loss: 0.6261377551270719\n",
      "Iteration: 69 lambda_k: 1 Loss: 0.6243611772765278\n",
      "Iteration: 70 lambda_k: 1 Loss: 0.6225924885574969\n",
      "Iteration: 71 lambda_k: 1 Loss: 0.6208316078957793\n",
      "Iteration: 72 lambda_k: 1 Loss: 0.6190784580651614\n",
      "Iteration: 73 lambda_k: 1 Loss: 0.6173329653922339\n",
      "Iteration: 74 lambda_k: 1 Loss: 0.6155950595002903\n",
      "Iteration: 75 lambda_k: 1 Loss: 0.613864673055419\n",
      "Iteration: 76 lambda_k: 1 Loss: 0.6121417415438835\n",
      "Iteration: 77 lambda_k: 1 Loss: 0.6104262030642861\n",
      "Iteration: 78 lambda_k: 1 Loss: 0.6087179981358519\n",
      "Iteration: 79 lambda_k: 1 Loss: 0.6070170695226036\n",
      "Iteration: 80 lambda_k: 1 Loss: 0.6053233620711094\n",
      "Iteration: 81 lambda_k: 1 Loss: 0.6036368225610274\n",
      "Iteration: 82 lambda_k: 1 Loss: 0.6019573995673617\n",
      "Iteration: 83 lambda_k: 1 Loss: 0.6002850433334629\n",
      "Iteration: 84 lambda_k: 1 Loss: 0.5986197056539498\n",
      "Iteration: 85 lambda_k: 1 Loss: 0.5969613397667431\n",
      "Iteration: 86 lambda_k: 1 Loss: 0.5953099002534288\n",
      "Iteration: 87 lambda_k: 1 Loss: 0.5936653429473151\n",
      "Iteration: 88 lambda_k: 1 Loss: 0.5920276248487132\n",
      "Iteration: 89 lambda_k: 1 Loss: 0.5903967040465938\n",
      "Iteration: 90 lambda_k: 1 Loss: 0.5887725396459925\n",
      "Iteration: 91 lambda_k: 1 Loss: 0.5871550917023959\n",
      "Iteration: 92 lambda_k: 1 Loss: 0.5855443211587054\n",
      "Iteration: 93 lambda_k: 1 Loss: 0.583940189789237\n",
      "Iteration: 94 lambda_k: 1 Loss: 0.5823426601468301\n",
      "Iteration: 95 lambda_k: 1 Loss: 0.5807516955143378\n",
      "Iteration: 96 lambda_k: 1 Loss: 0.5791672598597326\n",
      "Iteration: 97 lambda_k: 1 Loss: 0.5775893177946113\n",
      "Iteration: 98 lambda_k: 1 Loss: 0.5760178345358288\n",
      "Iteration: 99 lambda_k: 1 Loss: 0.5744527758700251\n",
      "Iteration: 100 lambda_k: 1 Loss: 0.5728941081208172\n",
      "Iteration: 101 lambda_k: 1 Loss: 0.5713417981184507\n",
      "Iteration: 102 lambda_k: 1 Loss: 0.5697958131717156\n",
      "Iteration: 103 lambda_k: 1 Loss: 0.5682561210419588\n",
      "Iteration: 104 lambda_k: 1 Loss: 0.5667226899190246\n",
      "Iteration: 105 lambda_k: 1 Loss: 0.5651954883989769\n",
      "Iteration: 106 lambda_k: 1 Loss: 0.5636744854634623\n",
      "Iteration: 107 lambda_k: 1 Loss: 0.5621596504606489\n",
      "Iteration: 108 lambda_k: 1 Loss: 0.5606509530873266\n",
      "Iteration: 109 lambda_k: 1 Loss: 0.559148363372786\n",
      "Iteration: 110 lambda_k: 1 Loss: 0.5576518516633671\n",
      "Iteration: 111 lambda_k: 1 Loss: 0.556161388608306\n",
      "Iteration: 112 lambda_k: 1 Loss: 0.5546769451469469\n",
      "Iteration: 113 lambda_k: 1 Loss: 0.5531984924961232\n",
      "Iteration: 114 lambda_k: 1 Loss: 0.5517260021391542\n",
      "Iteration: 115 lambda_k: 1 Loss: 0.5502594458154207\n",
      "Iteration: 116 lambda_k: 1 Loss: 0.5487987955103423\n",
      "Iteration: 117 lambda_k: 1 Loss: 0.5473440234464947\n",
      "Iteration: 118 lambda_k: 1 Loss: 0.5458951020751325\n",
      "Iteration: 119 lambda_k: 1 Loss: 0.5444520040683392\n",
      "Iteration: 120 lambda_k: 1 Loss: 0.5430147023117189\n",
      "Iteration: 121 lambda_k: 1 Loss: 0.5415831698975875\n",
      "Iteration: 122 lambda_k: 1 Loss: 0.5401573801186258\n",
      "Iteration: 123 lambda_k: 1 Loss: 0.5387373064619583\n",
      "Iteration: 124 lambda_k: 1 Loss: 0.5373229226036287\n",
      "Iteration: 125 lambda_k: 1 Loss: 0.5359142024034396\n",
      "Iteration: 126 lambda_k: 1 Loss: 0.53451111990013\n",
      "Iteration: 127 lambda_k: 1 Loss: 0.5331136493068653\n",
      "Iteration: 128 lambda_k: 1 Loss: 0.5317217650070164\n",
      "Iteration: 129 lambda_k: 1 Loss: 0.5303354415502058\n",
      "Iteration: 130 lambda_k: 1 Loss: 0.5289546536486022\n",
      "Iteration: 131 lambda_k: 1 Loss: 0.5275793761734429\n",
      "Iteration: 132 lambda_k: 1 Loss: 0.5262095841517691\n",
      "Iteration: 133 lambda_k: 1 Loss: 0.5248452527633578\n",
      "Iteration: 134 lambda_k: 1 Loss: 0.5234863573378348\n",
      "Iteration: 135 lambda_k: 1 Loss: 0.5221328733520422\n",
      "Iteration: 136 lambda_k: 1 Loss: 0.5207847764270599\n",
      "Iteration: 137 lambda_k: 1 Loss: 0.5194420423265808\n",
      "Iteration: 138 lambda_k: 1 Loss: 0.5181046469539047\n",
      "Iteration: 139 lambda_k: 1 Loss: 0.5167725663500695\n",
      "Iteration: 140 lambda_k: 1 Loss: 0.5154457766917919\n",
      "Iteration: 141 lambda_k: 1 Loss: 0.5141242542895161\n",
      "Iteration: 142 lambda_k: 1 Loss: 0.5128079755855766\n",
      "Iteration: 143 lambda_k: 1 Loss: 0.5114969171524518\n",
      "Iteration: 144 lambda_k: 1 Loss: 0.5101910556911042\n",
      "Iteration: 145 lambda_k: 1 Loss: 0.5088903680286043\n",
      "Iteration: 146 lambda_k: 1 Loss: 0.5075948311193856\n",
      "Iteration: 147 lambda_k: 1 Loss: 0.506304422040325\n",
      "Iteration: 148 lambda_k: 1 Loss: 0.5050191179912282\n",
      "Iteration: 149 lambda_k: 1 Loss: 0.5037388962931408\n",
      "Iteration: 150 lambda_k: 1 Loss: 0.5024637343871421\n",
      "Iteration: 151 lambda_k: 1 Loss: 0.5011936098331093\n",
      "Iteration: 152 lambda_k: 1 Loss: 0.4999285003085509\n",
      "Iteration: 153 lambda_k: 1 Loss: 0.4986683836074936\n",
      "Iteration: 154 lambda_k: 1 Loss: 0.4974132376394242\n",
      "Iteration: 155 lambda_k: 1 Loss: 0.4961630404282031\n",
      "Iteration: 156 lambda_k: 1 Loss: 0.49491777011114113\n",
      "Iteration: 157 lambda_k: 1 Loss: 0.49367740493783246\n",
      "Iteration: 158 lambda_k: 1 Loss: 0.49244192326969255\n",
      "Iteration: 159 lambda_k: 1 Loss: 0.491211303578479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 160 lambda_k: 1 Loss: 0.48998552444575394\n",
      "Iteration: 161 lambda_k: 1 Loss: 0.48876456456191836\n",
      "Iteration: 162 lambda_k: 1 Loss: 0.4875484027254454\n",
      "Iteration: 163 lambda_k: 1 Loss: 0.48633701784203065\n",
      "Iteration: 164 lambda_k: 1 Loss: 0.4851303889238207\n",
      "Iteration: 165 lambda_k: 1 Loss: 0.4839284950886486\n",
      "Iteration: 166 lambda_k: 1 Loss: 0.48273131555928683\n",
      "Iteration: 167 lambda_k: 1 Loss: 0.4815388296627205\n",
      "Iteration: 168 lambda_k: 1 Loss: 0.48035101683125014\n",
      "Iteration: 169 lambda_k: 1 Loss: 0.4791678565952989\n",
      "Iteration: 170 lambda_k: 1 Loss: 0.47798932859094445\n",
      "Iteration: 171 lambda_k: 1 Loss: 0.47681541255536136\n",
      "Iteration: 172 lambda_k: 1 Loss: 0.4756460883263321\n",
      "Iteration: 173 lambda_k: 1 Loss: 0.47448133584165925\n",
      "Iteration: 174 lambda_k: 1 Loss: 0.4733211351386697\n",
      "Iteration: 175 lambda_k: 1 Loss: 0.47216546635362333\n",
      "Iteration: 176 lambda_k: 1 Loss: 0.4710143097210927\n",
      "Iteration: 177 lambda_k: 1 Loss: 0.4698676455733522\n",
      "Iteration: 178 lambda_k: 1 Loss: 0.46872545433978824\n",
      "Iteration: 179 lambda_k: 1 Loss: 0.4675877165463186\n",
      "Iteration: 180 lambda_k: 1 Loss: 0.46645441281482364\n",
      "Iteration: 181 lambda_k: 1 Loss: 0.4653255238625817\n",
      "Iteration: 182 lambda_k: 1 Loss: 0.464201030501714\n",
      "Iteration: 183 lambda_k: 1 Loss: 0.4630809136386368\n",
      "Iteration: 184 lambda_k: 1 Loss: 0.4619651542735203\n",
      "Iteration: 185 lambda_k: 1 Loss: 0.4608537334997542\n",
      "Iteration: 186 lambda_k: 1 Loss: 0.45974663250342035\n",
      "Iteration: 187 lambda_k: 1 Loss: 0.458643832562772\n",
      "Iteration: 188 lambda_k: 1 Loss: 0.45754531504771817\n",
      "Iteration: 189 lambda_k: 1 Loss: 0.4564510614193137\n",
      "Iteration: 190 lambda_k: 1 Loss: 0.4553610532292565\n",
      "Iteration: 191 lambda_k: 1 Loss: 0.45427527211938823\n",
      "Iteration: 192 lambda_k: 1 Loss: 0.4531936998212015\n",
      "Iteration: 193 lambda_k: 1 Loss: 0.45211631815535136\n",
      "Iteration: 194 lambda_k: 1 Loss: 0.45104310903117206\n",
      "Iteration: 195 lambda_k: 1 Loss: 0.44997405444619804\n",
      "Iteration: 196 lambda_k: 1 Loss: 0.44890913648569\n",
      "Iteration: 197 lambda_k: 1 Loss: 0.4478483373221647\n",
      "Iteration: 198 lambda_k: 1 Loss: 0.44679163921493026\n",
      "Iteration: 199 lambda_k: 1 Loss: 0.44573902450962355\n",
      "Iteration: 200 lambda_k: 1 Loss: 0.44469047563775366\n",
      "Iteration: 201 lambda_k: 1 Loss: 0.44364597511624804\n",
      "Iteration: 202 lambda_k: 1 Loss: 0.44260550554700323\n",
      "Iteration: 203 lambda_k: 1 Loss: 0.44156904961643834\n",
      "Iteration: 204 lambda_k: 1 Loss: 0.440536590095053\n",
      "Iteration: 205 lambda_k: 1 Loss: 0.4395081098369888\n",
      "Iteration: 206 lambda_k: 1 Loss: 0.4384835917795932\n",
      "Iteration: 207 lambda_k: 1 Loss: 0.4374630189429883\n",
      "Iteration: 208 lambda_k: 1 Loss: 0.4364463744296415\n",
      "Iteration: 209 lambda_k: 1 Loss: 0.43543364142394086\n",
      "Iteration: 210 lambda_k: 1 Loss: 0.4344248031917717\n",
      "Iteration: 211 lambda_k: 1 Loss: 0.433419843080098\n",
      "Iteration: 212 lambda_k: 1 Loss: 0.43241874451654627\n",
      "Iteration: 213 lambda_k: 1 Loss: 0.4314214910089919\n",
      "Iteration: 214 lambda_k: 1 Loss: 0.430428066145149\n",
      "Iteration: 215 lambda_k: 1 Loss: 0.4294384535921633\n",
      "Iteration: 216 lambda_k: 1 Loss: 0.42845263709620673\n",
      "Iteration: 217 lambda_k: 1 Loss: 0.4274706004820763\n",
      "Iteration: 218 lambda_k: 1 Loss: 0.4264923276527944\n",
      "Iteration: 219 lambda_k: 1 Loss: 0.4255178025892125\n",
      "Iteration: 220 lambda_k: 1 Loss: 0.4245470093496175\n",
      "Iteration: 221 lambda_k: 1 Loss: 0.42357993206933975\n",
      "Iteration: 222 lambda_k: 1 Loss: 0.42261655496036504\n",
      "Iteration: 223 lambda_k: 1 Loss: 0.4216568623109482\n",
      "Iteration: 224 lambda_k: 1 Loss: 0.4207008384852286\n",
      "Iteration: 225 lambda_k: 1 Loss: 0.41974846792285014\n",
      "Iteration: 226 lambda_k: 1 Loss: 0.41879973513858126\n",
      "Iteration: 227 lambda_k: 1 Loss: 0.4178546247219389\n",
      "Iteration: 228 lambda_k: 1 Loss: 0.4169131213368141\n",
      "Iteration: 229 lambda_k: 1 Loss: 0.41597520972110014\n",
      "Iteration: 230 lambda_k: 1 Loss: 0.4150408746863231\n",
      "Iteration: 231 lambda_k: 1 Loss: 0.4141101011172739\n",
      "Iteration: 232 lambda_k: 1 Loss: 0.41318287397164416\n",
      "Iteration: 233 lambda_k: 1 Loss: 0.41225917827966224\n",
      "Iteration: 234 lambda_k: 1 Loss: 0.41133899914373323\n",
      "Iteration: 235 lambda_k: 1 Loss: 0.4104223217380798\n",
      "Iteration: 236 lambda_k: 1 Loss: 0.40950913130838573\n",
      "Iteration: 237 lambda_k: 1 Loss: 0.408599413171442\n",
      "Iteration: 238 lambda_k: 1 Loss: 0.40769315271479445\n",
      "Iteration: 239 lambda_k: 1 Loss: 0.40679033539639226\n",
      "Iteration: 240 lambda_k: 1 Loss: 0.4058909467442407\n",
      "Iteration: 241 lambda_k: 1 Loss: 0.404994972356055\n",
      "Iteration: 242 lambda_k: 1 Loss: 0.4041023978989157\n",
      "Iteration: 243 lambda_k: 1 Loss: 0.4032132091089265\n",
      "Iteration: 244 lambda_k: 1 Loss: 0.402327391790874\n",
      "Iteration: 245 lambda_k: 1 Loss: 0.401444931817889\n",
      "Iteration: 246 lambda_k: 1 Loss: 0.4005658151311099\n",
      "Iteration: 247 lambda_k: 1 Loss: 0.3996900277393487\n",
      "Iteration: 248 lambda_k: 1 Loss: 0.3988175557187574\n",
      "Iteration: 249 lambda_k: 1 Loss: 0.3979483852124979\n",
      "Iteration: 250 lambda_k: 1 Loss: 0.39708250243041227\n",
      "Iteration: 251 lambda_k: 1 Loss: 0.39621989364869564\n",
      "Iteration: 252 lambda_k: 1 Loss: 0.39536054520946584\n",
      "Iteration: 253 lambda_k: 1 Loss: 0.39450444352081876\n",
      "Iteration: 254 lambda_k: 1 Loss: 0.3936515750560437\n",
      "Iteration: 255 lambda_k: 1 Loss: 0.3928019263537032\n",
      "Iteration: 256 lambda_k: 1 Loss: 0.3919554840165285\n",
      "Iteration: 257 lambda_k: 1 Loss: 0.3911122347121094\n",
      "Iteration: 258 lambda_k: 1 Loss: 0.39027216517218893\n",
      "Iteration: 259 lambda_k: 1 Loss: 0.3894352621922458\n",
      "Iteration: 260 lambda_k: 1 Loss: 0.3886015126312132\n",
      "Iteration: 261 lambda_k: 1 Loss: 0.3877709034112\n",
      "Iteration: 262 lambda_k: 1 Loss: 0.3869434215171899\n",
      "Iteration: 263 lambda_k: 1 Loss: 0.3861190539967311\n",
      "Iteration: 264 lambda_k: 1 Loss: 0.3852977879596285\n",
      "Iteration: 265 lambda_k: 1 Loss: 0.3844796105776392\n",
      "Iteration: 266 lambda_k: 1 Loss: 0.3836645090841791\n",
      "Iteration: 267 lambda_k: 1 Loss: 0.38285247077401213\n",
      "Iteration: 268 lambda_k: 1 Loss: 0.3820434830029517\n",
      "Iteration: 269 lambda_k: 1 Loss: 0.38123753318757025\n",
      "Iteration: 270 lambda_k: 1 Loss: 0.3804346088049009\n",
      "Iteration: 271 lambda_k: 1 Loss: 0.3796346973921432\n",
      "Iteration: 272 lambda_k: 1 Loss: 0.3788377865463713\n",
      "Iteration: 273 lambda_k: 1 Loss: 0.37804386392424255\n",
      "Iteration: 274 lambda_k: 1 Loss: 0.37725291724170795\n",
      "Iteration: 275 lambda_k: 1 Loss: 0.376464934273724\n",
      "Iteration: 276 lambda_k: 1 Loss: 0.3756799028539695\n",
      "Iteration: 277 lambda_k: 1 Loss: 0.37489781087455487\n",
      "Iteration: 278 lambda_k: 1 Loss: 0.374118646285742\n",
      "Iteration: 279 lambda_k: 1 Loss: 0.37334239709566214\n",
      "Iteration: 280 lambda_k: 1 Loss: 0.37256905137003493\n",
      "Iteration: 281 lambda_k: 1 Loss: 0.3717985972318906\n",
      "Iteration: 282 lambda_k: 1 Loss: 0.37103102286128736\n",
      "Iteration: 283 lambda_k: 1 Loss: 0.3702663164950411\n",
      "Iteration: 284 lambda_k: 1 Loss: 0.3695044664264472\n",
      "Iteration: 285 lambda_k: 1 Loss: 0.3687454610050076\n",
      "Iteration: 286 lambda_k: 1 Loss: 0.36798928863615854\n",
      "Iteration: 287 lambda_k: 1 Loss: 0.3672359377809999\n",
      "Iteration: 288 lambda_k: 1 Loss: 0.3664853969560254\n",
      "Iteration: 289 lambda_k: 1 Loss: 0.36573765473285574\n",
      "Iteration: 290 lambda_k: 1 Loss: 0.3649926997379695\n",
      "Iteration: 291 lambda_k: 1 Loss: 0.36425052065243935\n",
      "Iteration: 292 lambda_k: 1 Loss: 0.36351110621166827\n",
      "Iteration: 293 lambda_k: 1 Loss: 0.3627744452051257\n",
      "Iteration: 294 lambda_k: 1 Loss: 0.362040526476087\n",
      "Iteration: 295 lambda_k: 1 Loss: 0.36130933892137296\n",
      "Iteration: 296 lambda_k: 1 Loss: 0.36058087149109064\n",
      "Iteration: 297 lambda_k: 1 Loss: 0.3598551131883764\n",
      "Iteration: 298 lambda_k: 1 Loss: 0.35913205306913887\n",
      "Iteration: 299 lambda_k: 1 Loss: 0.35841168024180475\n",
      "Iteration: 300 lambda_k: 1 Loss: 0.3576939838670642\n",
      "Iteration: 301 lambda_k: 1 Loss: 0.356978953157619\n",
      "Iteration: 302 lambda_k: 1 Loss: 0.35626657737793105\n",
      "Iteration: 303 lambda_k: 1 Loss: 0.3555568458439721\n",
      "Iteration: 304 lambda_k: 1 Loss: 0.35484974792297524\n",
      "Iteration: 305 lambda_k: 1 Loss: 0.35414527303318716\n",
      "Iteration: 306 lambda_k: 1 Loss: 0.35344341064362217\n",
      "Iteration: 307 lambda_k: 1 Loss: 0.35274415027381617\n",
      "Iteration: 308 lambda_k: 1 Loss: 0.3520474814935835\n",
      "Iteration: 309 lambda_k: 1 Loss: 0.3513533939223954\n",
      "Iteration: 310 lambda_k: 1 Loss: 0.35066187723048864\n",
      "Iteration: 311 lambda_k: 1 Loss: 0.3499729211368652\n",
      "Iteration: 312 lambda_k: 1 Loss: 0.34928651541003136\n",
      "Iteration: 313 lambda_k: 1 Loss: 0.3486026498675717\n",
      "Iteration: 314 lambda_k: 1 Loss: 0.34792131437591156\n",
      "Iteration: 315 lambda_k: 1 Loss: 0.3472424988500793\n",
      "Iteration: 316 lambda_k: 1 Loss: 0.34656619325347265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 317 lambda_k: 1 Loss: 0.3458923875976249\n",
      "Iteration: 318 lambda_k: 1 Loss: 0.34522107194197205\n",
      "Iteration: 319 lambda_k: 1 Loss: 0.34455223639362215\n",
      "Iteration: 320 lambda_k: 1 Loss: 0.3438858711071251\n",
      "Iteration: 321 lambda_k: 1 Loss: 0.34322196628424334\n",
      "Iteration: 322 lambda_k: 1 Loss: 0.342560512173724\n",
      "Iteration: 323 lambda_k: 1 Loss: 0.3419014990710723\n",
      "Iteration: 324 lambda_k: 1 Loss: 0.3412449173183258\n",
      "Iteration: 325 lambda_k: 1 Loss: 0.3405907573038293\n",
      "Iteration: 326 lambda_k: 1 Loss: 0.3399390094620121\n",
      "Iteration: 327 lambda_k: 1 Loss: 0.3392896642731648\n",
      "Iteration: 328 lambda_k: 1 Loss: 0.33864271226321824\n",
      "Iteration: 329 lambda_k: 1 Loss: 0.33799814400352324\n",
      "Iteration: 330 lambda_k: 1 Loss: 0.33735595011063074\n",
      "Iteration: 331 lambda_k: 1 Loss: 0.3367161212460745\n",
      "Iteration: 332 lambda_k: 1 Loss: 0.33607864811615284\n",
      "Iteration: 333 lambda_k: 1 Loss: 0.33544352147171347\n",
      "Iteration: 334 lambda_k: 1 Loss: 0.33481073210793755\n",
      "Iteration: 335 lambda_k: 1 Loss: 0.33418027086412605\n",
      "Iteration: 336 lambda_k: 1 Loss: 0.33355212862348654\n",
      "Iteration: 337 lambda_k: 1 Loss: 0.33292629631292114\n",
      "Iteration: 338 lambda_k: 1 Loss: 0.33230276490281524\n",
      "Iteration: 339 lambda_k: 1 Loss: 0.3316815254068281\n",
      "Iteration: 340 lambda_k: 1 Loss: 0.33106256888168295\n",
      "Iteration: 341 lambda_k: 1 Loss: 0.3304458864269592\n",
      "Iteration: 342 lambda_k: 1 Loss: 0.3298314691848859\n",
      "Iteration: 343 lambda_k: 1 Loss: 0.3292193083401348\n",
      "Iteration: 344 lambda_k: 1 Loss: 0.3286093951196159\n",
      "Iteration: 345 lambda_k: 1 Loss: 0.32800172079227297\n",
      "Iteration: 346 lambda_k: 1 Loss: 0.32739627666888027\n",
      "Iteration: 347 lambda_k: 1 Loss: 0.32679305410184084\n",
      "Iteration: 348 lambda_k: 1 Loss: 0.32619204448498457\n",
      "Iteration: 349 lambda_k: 1 Loss: 0.3255932392533684\n",
      "Iteration: 350 lambda_k: 1 Loss: 0.32499662988307665\n",
      "Iteration: 351 lambda_k: 1 Loss: 0.3244022078910226\n",
      "Iteration: 352 lambda_k: 1 Loss: 0.3238099648347513\n",
      "Iteration: 353 lambda_k: 1 Loss: 0.3232198923122428\n",
      "Iteration: 354 lambda_k: 1 Loss: 0.32263198196171633\n",
      "Iteration: 355 lambda_k: 1 Loss: 0.32204622546143585\n",
      "Iteration: 356 lambda_k: 1 Loss: 0.32146261452951624\n",
      "Iteration: 357 lambda_k: 1 Loss: 0.32088114092373016\n",
      "Iteration: 358 lambda_k: 1 Loss: 0.3203017964413166\n",
      "Iteration: 359 lambda_k: 1 Loss: 0.3197245729187889\n",
      "Iteration: 360 lambda_k: 1 Loss: 0.3191494622317449\n",
      "Iteration: 361 lambda_k: 1 Loss: 0.31857645629467873\n",
      "Iteration: 362 lambda_k: 1 Loss: 0.3180055470607914\n",
      "Iteration: 363 lambda_k: 1 Loss: 0.3174367265218034\n",
      "Iteration: 364 lambda_k: 1 Loss: 0.31686998670776784\n",
      "Iteration: 365 lambda_k: 1 Loss: 0.31630531968688486\n",
      "Iteration: 366 lambda_k: 1 Loss: 0.31574271756531697\n",
      "Iteration: 367 lambda_k: 1 Loss: 0.3151821724870047\n",
      "Iteration: 368 lambda_k: 1 Loss: 0.3146236766334837\n",
      "Iteration: 369 lambda_k: 1 Loss: 0.314067222223702\n",
      "Iteration: 370 lambda_k: 1 Loss: 0.31351280151383903\n",
      "Iteration: 371 lambda_k: 1 Loss: 0.3129604067971249\n",
      "Iteration: 372 lambda_k: 1 Loss: 0.31241003040366006\n",
      "Iteration: 373 lambda_k: 1 Loss: 0.31186166470034693\n",
      "Iteration: 374 lambda_k: 1 Loss: 0.3113153020867006\n",
      "Iteration: 375 lambda_k: 1 Loss: 0.3107709350089193\n",
      "Iteration: 376 lambda_k: 1 Loss: 0.3102285559401557\n",
      "Iteration: 377 lambda_k: 1 Loss: 0.30968815739210215\n",
      "Iteration: 378 lambda_k: 1 Loss: 0.3091497319122944\n",
      "Iteration: 379 lambda_k: 1 Loss: 0.3086132720839628\n",
      "Iteration: 380 lambda_k: 1 Loss: 0.30807877052729044\n",
      "Iteration: 381 lambda_k: 1 Loss: 0.3075462198937926\n",
      "Iteration: 382 lambda_k: 1 Loss: 0.30701561287356766\n",
      "Iteration: 383 lambda_k: 1 Loss: 0.3064869421905644\n",
      "Iteration: 384 lambda_k: 1 Loss: 0.3059602006036452\n",
      "Iteration: 385 lambda_k: 1 Loss: 0.30543538090624867\n",
      "Iteration: 386 lambda_k: 1 Loss: 0.3049124759262526\n",
      "Iteration: 387 lambda_k: 1 Loss: 0.3043914785258112\n",
      "Iteration: 388 lambda_k: 1 Loss: 0.30387238160118546\n",
      "Iteration: 389 lambda_k: 1 Loss: 0.3033551780825752\n",
      "Iteration: 390 lambda_k: 1 Loss: 0.30283986093395415\n",
      "Iteration: 391 lambda_k: 1 Loss: 0.3023264231529055\n",
      "Iteration: 392 lambda_k: 1 Loss: 0.3018148577704582\n",
      "Iteration: 393 lambda_k: 1 Loss: 0.3013051578509244\n",
      "Iteration: 394 lambda_k: 1 Loss: 0.3007973164917362\n",
      "Iteration: 395 lambda_k: 1 Loss: 0.30029132682328574\n",
      "Iteration: 396 lambda_k: 1 Loss: 0.299787182008763\n",
      "Iteration: 397 lambda_k: 1 Loss: 0.2992848752439964\n",
      "Iteration: 398 lambda_k: 1 Loss: 0.2987843997572937\n",
      "Iteration: 399 lambda_k: 1 Loss: 0.29828574880928305\n",
      "Iteration: 400 lambda_k: 1 Loss: 0.2977889156927555\n",
      "Iteration: 401 lambda_k: 1 Loss: 0.2972938937325077\n",
      "Iteration: 402 lambda_k: 1 Loss: 0.2968006762851858\n",
      "Iteration: 403 lambda_k: 1 Loss: 0.29630925673912933\n",
      "Iteration: 404 lambda_k: 1 Loss: 0.29581962851421695\n",
      "Iteration: 405 lambda_k: 1 Loss: 0.29533178506171176\n",
      "Iteration: 406 lambda_k: 1 Loss: 0.2948457198641078\n",
      "Iteration: 407 lambda_k: 1 Loss: 0.2943614264349775\n",
      "Iteration: 408 lambda_k: 1 Loss: 0.2938788983188195\n",
      "Iteration: 409 lambda_k: 1 Loss: 0.29339812909090723\n",
      "Iteration: 410 lambda_k: 1 Loss: 0.29291911235713836\n",
      "Iteration: 411 lambda_k: 1 Loss: 0.2924418417542682\n",
      "Iteration: 412 lambda_k: 1 Loss: 0.2919663109484091\n",
      "Iteration: 413 lambda_k: 1 Loss: 0.2914925136366123\n",
      "Iteration: 414 lambda_k: 1 Loss: 0.29102044354578416\n",
      "Iteration: 415 lambda_k: 1 Loss: 0.29055009443270935\n",
      "Iteration: 416 lambda_k: 1 Loss: 0.2900814600839062\n",
      "Iteration: 417 lambda_k: 1 Loss: 0.2896145343154809\n",
      "Iteration: 418 lambda_k: 1 Loss: 0.28914931097298296\n",
      "Iteration: 419 lambda_k: 1 Loss: 0.2886857839312604\n",
      "Iteration: 420 lambda_k: 1 Loss: 0.28822394709431565\n",
      "Iteration: 421 lambda_k: 1 Loss: 0.2877637943951629\n",
      "Iteration: 422 lambda_k: 1 Loss: 0.287305319795685\n",
      "Iteration: 423 lambda_k: 1 Loss: 0.2868485172864919\n",
      "Iteration: 424 lambda_k: 1 Loss: 0.286393380886779\n",
      "Iteration: 425 lambda_k: 1 Loss: 0.28593990464418695\n",
      "Iteration: 426 lambda_k: 1 Loss: 0.2854880826346615\n",
      "Iteration: 427 lambda_k: 1 Loss: 0.28503790896231407\n",
      "Iteration: 428 lambda_k: 1 Loss: 0.2845893777592831\n",
      "Iteration: 429 lambda_k: 1 Loss: 0.2841424831855961\n",
      "Iteration: 430 lambda_k: 1 Loss: 0.28369721942903225\n",
      "Iteration: 431 lambda_k: 1 Loss: 0.2832535807049851\n",
      "Iteration: 432 lambda_k: 1 Loss: 0.28281156125632695\n",
      "Iteration: 433 lambda_k: 1 Loss: 0.2823711553532735\n",
      "Iteration: 434 lambda_k: 1 Loss: 0.2819323572932481\n",
      "Iteration: 435 lambda_k: 1 Loss: 0.28149516140074965\n",
      "Iteration: 436 lambda_k: 1 Loss: 0.28105956202722143\n",
      "Iteration: 437 lambda_k: 1 Loss: 0.28062555355097607\n",
      "Iteration: 438 lambda_k: 1 Loss: 0.280193130376762\n",
      "Iteration: 439 lambda_k: 1 Loss: 0.27976228693612815\n",
      "Iteration: 440 lambda_k: 1 Loss: 0.27933301768694735\n",
      "Iteration: 441 lambda_k: 1 Loss: 0.27890531711337263\n",
      "Iteration: 442 lambda_k: 1 Loss: 0.27847917972570696\n",
      "Iteration: 443 lambda_k: 1 Loss: 0.27805460006027444\n",
      "Iteration: 444 lambda_k: 1 Loss: 0.27763157267929095\n",
      "Iteration: 445 lambda_k: 1 Loss: 0.27721009217073594\n",
      "Iteration: 446 lambda_k: 1 Loss: 0.2767901531482246\n",
      "Iteration: 447 lambda_k: 1 Loss: 0.27637175025088084\n",
      "Iteration: 448 lambda_k: 1 Loss: 0.27595487814321124\n",
      "Iteration: 449 lambda_k: 1 Loss: 0.2755395315149782\n",
      "Iteration: 450 lambda_k: 1 Loss: 0.2751257050810748\n",
      "Iteration: 451 lambda_k: 1 Loss: 0.27471339358140046\n",
      "Iteration: 452 lambda_k: 1 Loss: 0.27430259178073596\n",
      "Iteration: 453 lambda_k: 1 Loss: 0.27389329446861976\n",
      "Iteration: 454 lambda_k: 1 Loss: 0.27348549645922543\n",
      "Iteration: 455 lambda_k: 1 Loss: 0.2730791925840619\n",
      "Iteration: 456 lambda_k: 1 Loss: 0.2726743777200461\n",
      "Iteration: 457 lambda_k: 1 Loss: 0.27227104674796676\n",
      "Iteration: 458 lambda_k: 1 Loss: 0.27186919457941233\n",
      "Iteration: 459 lambda_k: 1 Loss: 0.27146881614984414\n",
      "Iteration: 460 lambda_k: 1 Loss: 0.27106990641866324\n",
      "Iteration: 461 lambda_k: 1 Loss: 0.2706724603690624\n",
      "Iteration: 462 lambda_k: 1 Loss: 0.270276473007912\n",
      "Iteration: 463 lambda_k: 1 Loss: 0.269881939365647\n",
      "Iteration: 464 lambda_k: 1 Loss: 0.26948885449831406\n",
      "Iteration: 465 lambda_k: 1 Loss: 0.2690972134791364\n",
      "Iteration: 466 lambda_k: 1 Loss: 0.2687070114103943\n",
      "Iteration: 467 lambda_k: 1 Loss: 0.2683182434153684\n",
      "Iteration: 468 lambda_k: 1 Loss: 0.2679309046405356\n",
      "Iteration: 469 lambda_k: 1 Loss: 0.26754499025518735\n",
      "Iteration: 470 lambda_k: 1 Loss: 0.2671604954513924\n",
      "Iteration: 471 lambda_k: 1 Loss: 0.26677741544380523\n",
      "Iteration: 472 lambda_k: 1 Loss: 0.2663957454696044\n",
      "Iteration: 473 lambda_k: 1 Loss: 0.26601548078836645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 474 lambda_k: 1 Loss: 0.2656366166819535\n",
      "Iteration: 475 lambda_k: 1 Loss: 0.26525914845440307\n",
      "Iteration: 476 lambda_k: 1 Loss: 0.26488307143181533\n",
      "Iteration: 477 lambda_k: 1 Loss: 0.26450838096224283\n",
      "Iteration: 478 lambda_k: 1 Loss: 0.2641350724155803\n",
      "Iteration: 479 lambda_k: 1 Loss: 0.26376314118345456\n",
      "Iteration: 480 lambda_k: 1 Loss: 0.26339258268112636\n",
      "Iteration: 481 lambda_k: 1 Loss: 0.2630233923391791\n",
      "Iteration: 482 lambda_k: 1 Loss: 0.26265556561595277\n",
      "Iteration: 483 lambda_k: 1 Loss: 0.26228909798886496\n",
      "Iteration: 484 lambda_k: 1 Loss: 0.26192398495657926\n",
      "Iteration: 485 lambda_k: 1 Loss: 0.26156022203885004\n",
      "Iteration: 486 lambda_k: 1 Loss: 0.2611978047762934\n",
      "Iteration: 487 lambda_k: 1 Loss: 0.26083672873062247\n",
      "Iteration: 488 lambda_k: 1 Loss: 0.2604769894842208\n",
      "Iteration: 489 lambda_k: 1 Loss: 0.2601185826400823\n",
      "Iteration: 490 lambda_k: 1 Loss: 0.2597615038217776\n",
      "Iteration: 491 lambda_k: 1 Loss: 0.259405748673335\n",
      "Iteration: 492 lambda_k: 1 Loss: 0.25905131285913463\n",
      "Iteration: 493 lambda_k: 1 Loss: 0.2586981920541991\n",
      "Iteration: 494 lambda_k: 1 Loss: 0.25834638198261417\n",
      "Iteration: 495 lambda_k: 1 Loss: 0.25799587835923865\n",
      "Iteration: 496 lambda_k: 1 Loss: 0.2576466769291793\n",
      "Iteration: 497 lambda_k: 1 Loss: 0.2572987734570177\n",
      "Iteration: 498 lambda_k: 1 Loss: 0.25695216372701746\n",
      "Iteration: 499 lambda_k: 1 Loss: 0.25660684354308105\n",
      "Iteration: 500 lambda_k: 1 Loss: 0.2562628087286821\n",
      "Iteration: 501 lambda_k: 1 Loss: 0.2559200551267607\n",
      "Iteration: 502 lambda_k: 1 Loss: 0.2555785785996073\n",
      "Iteration: 503 lambda_k: 1 Loss: 0.2552383750287573\n",
      "Iteration: 504 lambda_k: 1 Loss: 0.25489944031489575\n",
      "Iteration: 505 lambda_k: 1 Loss: 0.25456177037098376\n",
      "Iteration: 506 lambda_k: 1 Loss: 0.254225361148595\n",
      "Iteration: 507 lambda_k: 1 Loss: 0.25389020859876954\n",
      "Iteration: 508 lambda_k: 1 Loss: 0.25355630869814355\n",
      "Iteration: 509 lambda_k: 1 Loss: 0.2532236574460639\n",
      "Iteration: 510 lambda_k: 1 Loss: 0.2528922508470274\n",
      "Iteration: 511 lambda_k: 1 Loss: 0.25256208493766175\n",
      "Iteration: 512 lambda_k: 1 Loss: 0.25223315576789995\n",
      "Iteration: 513 lambda_k: 1 Loss: 0.2519054594063827\n",
      "Iteration: 514 lambda_k: 1 Loss: 0.25157899193995414\n",
      "Iteration: 515 lambda_k: 1 Loss: 0.2512537494735543\n",
      "Iteration: 516 lambda_k: 1 Loss: 0.250929728130133\n",
      "Iteration: 517 lambda_k: 1 Loss: 0.25060692405058743\n",
      "Iteration: 518 lambda_k: 1 Loss: 0.2502853333937343\n",
      "Iteration: 519 lambda_k: 1 Loss: 0.24996495233635055\n",
      "Iteration: 520 lambda_k: 1 Loss: 0.249645777073373\n",
      "Iteration: 521 lambda_k: 1 Loss: 0.24932780381846417\n",
      "Iteration: 522 lambda_k: 1 Loss: 0.24901102880539291\n",
      "Iteration: 523 lambda_k: 1 Loss: 0.2486954482037375\n",
      "Iteration: 524 lambda_k: 1 Loss: 0.248381058411651\n",
      "Iteration: 525 lambda_k: 1 Loss: 0.2480678556290031\n",
      "Iteration: 526 lambda_k: 1 Loss: 0.2477558361731278\n",
      "Iteration: 527 lambda_k: 1 Loss: 0.2474449963726737\n",
      "Iteration: 528 lambda_k: 1 Loss: 0.24713533246534644\n",
      "Iteration: 529 lambda_k: 1 Loss: 0.24682684097791094\n",
      "Iteration: 530 lambda_k: 1 Loss: 0.24651951838064431\n",
      "Iteration: 531 lambda_k: 1 Loss: 0.24621336163499802\n",
      "Iteration: 532 lambda_k: 1 Loss: 0.24590838486433914\n",
      "Iteration: 533 lambda_k: 1 Loss: 0.2456045795100226\n",
      "Iteration: 534 lambda_k: 1 Loss: 0.24530193694263744\n",
      "Iteration: 535 lambda_k: 1 Loss: 0.24500044987619105\n",
      "Iteration: 536 lambda_k: 1 Loss: 0.24470010969996325\n",
      "Iteration: 537 lambda_k: 1 Loss: 0.24440090772205517\n",
      "Iteration: 538 lambda_k: 1 Loss: 0.244102835518778\n",
      "Iteration: 539 lambda_k: 1 Loss: 0.24380588478386173\n",
      "Iteration: 540 lambda_k: 1 Loss: 0.24351004748147825\n",
      "Iteration: 541 lambda_k: 1 Loss: 0.24321531568698312\n",
      "Iteration: 542 lambda_k: 1 Loss: 0.2429216816176821\n",
      "Iteration: 543 lambda_k: 1 Loss: 0.2426291376227195\n",
      "Iteration: 544 lambda_k: 1 Loss: 0.24233767613071766\n",
      "Iteration: 545 lambda_k: 1 Loss: 0.2420472896991517\n",
      "Iteration: 546 lambda_k: 1 Loss: 0.24175797096740248\n",
      "Iteration: 547 lambda_k: 1 Loss: 0.241469712668648\n",
      "Iteration: 548 lambda_k: 1 Loss: 0.24118250761637286\n",
      "Iteration: 549 lambda_k: 1 Loss: 0.24089634870994334\n",
      "Iteration: 550 lambda_k: 1 Loss: 0.2406112289259834\n",
      "Iteration: 551 lambda_k: 1 Loss: 0.24032714130727145\n",
      "Iteration: 552 lambda_k: 1 Loss: 0.2400440789788369\n",
      "Iteration: 553 lambda_k: 1 Loss: 0.23976203513278854\n",
      "Iteration: 554 lambda_k: 1 Loss: 0.23948100303149958\n",
      "Iteration: 555 lambda_k: 1 Loss: 0.2392009760052281\n",
      "Iteration: 556 lambda_k: 1 Loss: 0.23892194745096254\n",
      "Iteration: 557 lambda_k: 1 Loss: 0.23864391083133019\n",
      "Iteration: 558 lambda_k: 1 Loss: 0.23836685967360552\n",
      "Iteration: 559 lambda_k: 1 Loss: 0.23809078756880775\n",
      "Iteration: 560 lambda_k: 1 Loss: 0.23781568817086715\n",
      "Iteration: 561 lambda_k: 1 Loss: 0.23754155519583214\n",
      "Iteration: 562 lambda_k: 1 Loss: 0.23726838242132284\n",
      "Iteration: 563 lambda_k: 1 Loss: 0.23699616368522683\n",
      "Iteration: 564 lambda_k: 1 Loss: 0.236724892885894\n",
      "Iteration: 565 lambda_k: 1 Loss: 0.23645456398090284\n",
      "Iteration: 566 lambda_k: 1 Loss: 0.23618517098663877\n",
      "Iteration: 567 lambda_k: 1 Loss: 0.23591670797765926\n",
      "Iteration: 568 lambda_k: 1 Loss: 0.2356491690861282\n",
      "Iteration: 569 lambda_k: 1 Loss: 0.23538254850124612\n",
      "Iteration: 570 lambda_k: 1 Loss: 0.23511684046832132\n",
      "Iteration: 571 lambda_k: 1 Loss: 0.23485203928941498\n",
      "Iteration: 572 lambda_k: 1 Loss: 0.2345881393216386\n",
      "Iteration: 573 lambda_k: 1 Loss: 0.23432513497672772\n",
      "Iteration: 574 lambda_k: 1 Loss: 0.23406302072086663\n",
      "Iteration: 575 lambda_k: 1 Loss: 0.23380179107414795\n",
      "Iteration: 576 lambda_k: 1 Loss: 0.23354144061001492\n",
      "Iteration: 577 lambda_k: 1 Loss: 0.23328196395474896\n",
      "Iteration: 578 lambda_k: 1 Loss: 0.23302335578698313\n",
      "Iteration: 579 lambda_k: 1 Loss: 0.2327656108372234\n",
      "Iteration: 580 lambda_k: 1 Loss: 0.23250872388740995\n",
      "Iteration: 581 lambda_k: 1 Loss: 0.23225268977031283\n",
      "Iteration: 582 lambda_k: 1 Loss: 0.2319975033691996\n",
      "Iteration: 583 lambda_k: 1 Loss: 0.23174315961735953\n",
      "Iteration: 584 lambda_k: 1 Loss: 0.2314896534976035\n",
      "Iteration: 585 lambda_k: 1 Loss: 0.23123698004181675\n",
      "Iteration: 586 lambda_k: 1 Loss: 0.23098513433051845\n",
      "Iteration: 587 lambda_k: 1 Loss: 0.23073411149242054\n",
      "Iteration: 588 lambda_k: 1 Loss: 0.23048390670398883\n",
      "Iteration: 589 lambda_k: 1 Loss: 0.23023451518900814\n",
      "Iteration: 590 lambda_k: 1 Loss: 0.22998593221815317\n",
      "Iteration: 591 lambda_k: 1 Loss: 0.2297381531085626\n",
      "Iteration: 592 lambda_k: 1 Loss: 0.2294911732233779\n",
      "Iteration: 593 lambda_k: 1 Loss: 0.2292449879714726\n",
      "Iteration: 594 lambda_k: 1 Loss: 0.22899959280688187\n",
      "Iteration: 595 lambda_k: 1 Loss: 0.22875498322840843\n",
      "Iteration: 596 lambda_k: 1 Loss: 0.22851115477927358\n",
      "Iteration: 597 lambda_k: 1 Loss: 0.22826810304670891\n",
      "Iteration: 598 lambda_k: 1 Loss: 0.22802582366155014\n",
      "Iteration: 599 lambda_k: 1 Loss: 0.22778431229784127\n",
      "Iteration: 600 lambda_k: 1 Loss: 0.22754356467247028\n",
      "Iteration: 601 lambda_k: 1 Loss: 0.22730357654470354\n",
      "Iteration: 602 lambda_k: 1 Loss: 0.2270643437158884\n",
      "Iteration: 603 lambda_k: 1 Loss: 0.22682586202906674\n",
      "Iteration: 604 lambda_k: 1 Loss: 0.22658812736857084\n",
      "Iteration: 605 lambda_k: 1 Loss: 0.22635113565965786\n",
      "Iteration: 606 lambda_k: 1 Loss: 0.22611488286811876\n",
      "Iteration: 607 lambda_k: 1 Loss: 0.22587936500000735\n",
      "Iteration: 608 lambda_k: 1 Loss: 0.22564457810118543\n",
      "Iteration: 609 lambda_k: 1 Loss: 0.22541051825696964\n",
      "Iteration: 610 lambda_k: 1 Loss: 0.22517718159181174\n",
      "Iteration: 611 lambda_k: 1 Loss: 0.22494456426894321\n",
      "Iteration: 612 lambda_k: 1 Loss: 0.22471266249011246\n",
      "Iteration: 613 lambda_k: 1 Loss: 0.22448147249490197\n",
      "Iteration: 614 lambda_k: 1 Loss: 0.22425099056078554\n",
      "Iteration: 615 lambda_k: 1 Loss: 0.22402121300272518\n",
      "Iteration: 616 lambda_k: 1 Loss: 0.22379213617274304\n",
      "Iteration: 617 lambda_k: 1 Loss: 0.22356375645944737\n",
      "Iteration: 618 lambda_k: 1 Loss: 0.22333607028786348\n",
      "Iteration: 619 lambda_k: 1 Loss: 0.22310907411918465\n",
      "Iteration: 620 lambda_k: 1 Loss: 0.222882764450361\n",
      "Iteration: 621 lambda_k: 1 Loss: 0.222657137813756\n",
      "Iteration: 622 lambda_k: 1 Loss: 0.2224321907768397\n",
      "Iteration: 623 lambda_k: 1 Loss: 0.22220791994184522\n",
      "Iteration: 624 lambda_k: 1 Loss: 0.2219843219446844\n",
      "Iteration: 625 lambda_k: 1 Loss: 0.22176139345866547\n",
      "Iteration: 626 lambda_k: 1 Loss: 0.22153913118631283\n",
      "Iteration: 627 lambda_k: 1 Loss: 0.22131753185815498\n",
      "Iteration: 628 lambda_k: 1 Loss: 0.22109609790077175\n",
      "Iteration: 629 lambda_k: 1 Loss: 0.22087282455718377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 630 lambda_k: 1 Loss: 0.22065124703452071\n",
      "Iteration: 631 lambda_k: 1 Loss: 0.2204314485075592\n",
      "Iteration: 632 lambda_k: 1 Loss: 0.22021321453166443\n",
      "Iteration: 633 lambda_k: 1 Loss: 0.21999636333032302\n",
      "Iteration: 634 lambda_k: 1 Loss: 0.21978084774476214\n",
      "Iteration: 635 lambda_k: 1 Loss: 0.21956645602062524\n",
      "Iteration: 636 lambda_k: 1 Loss: 0.21935280043749233\n",
      "Iteration: 637 lambda_k: 1 Loss: 0.21913973571913867\n",
      "Iteration: 638 lambda_k: 1 Loss: 0.21892716436198992\n",
      "Iteration: 639 lambda_k: 1 Loss: 0.2187150397958738\n",
      "Iteration: 640 lambda_k: 1 Loss: 0.2185033414162025\n",
      "Iteration: 641 lambda_k: 1 Loss: 0.2182920583239265\n",
      "Iteration: 642 lambda_k: 1 Loss: 0.21808118333230259\n",
      "Iteration: 643 lambda_k: 1 Loss: 0.21787071169778116\n",
      "Iteration: 644 lambda_k: 1 Loss: 0.21766064069409335\n",
      "Iteration: 645 lambda_k: 1 Loss: 0.21745096876303832\n",
      "Iteration: 646 lambda_k: 1 Loss: 0.21724169471289015\n",
      "Iteration: 647 lambda_k: 1 Loss: 0.21703281733903845\n",
      "Iteration: 648 lambda_k: 1 Loss: 0.2168243353471786\n",
      "Iteration: 649 lambda_k: 1 Loss: 0.21661624735909868\n",
      "Iteration: 650 lambda_k: 1 Loss: 0.2164085519094789\n",
      "Iteration: 651 lambda_k: 1 Loss: 0.21620124743550445\n",
      "Iteration: 652 lambda_k: 1 Loss: 0.21599433227435536\n",
      "Iteration: 653 lambda_k: 1 Loss: 0.21578780466999642\n",
      "Iteration: 654 lambda_k: 1 Loss: 0.2155816627838562\n",
      "Iteration: 655 lambda_k: 1 Loss: 0.21537590470547094\n",
      "Iteration: 656 lambda_k: 1 Loss: 0.21517052846203483\n",
      "Iteration: 657 lambda_k: 1 Loss: 0.21496553202702823\n",
      "Iteration: 658 lambda_k: 1 Loss: 0.21476091332814345\n",
      "Iteration: 659 lambda_k: 1 Loss: 0.21455667025454844\n",
      "Iteration: 660 lambda_k: 1 Loss: 0.2143528006634692\n",
      "Iteration: 661 lambda_k: 1 Loss: 0.2141493023861051\n",
      "Iteration: 662 lambda_k: 1 Loss: 0.2139461732329242\n",
      "Iteration: 663 lambda_k: 1 Loss: 0.21374341099839508\n",
      "Iteration: 664 lambda_k: 1 Loss: 0.21354101346520843\n",
      "Iteration: 665 lambda_k: 1 Loss: 0.21333897840803343\n",
      "Iteration: 666 lambda_k: 1 Loss: 0.21313730359685445\n",
      "Iteration: 667 lambda_k: 1 Loss: 0.21293598679992465\n",
      "Iteration: 668 lambda_k: 1 Loss: 0.21273502578637465\n",
      "Iteration: 669 lambda_k: 1 Loss: 0.21253441832851033\n",
      "Iteration: 670 lambda_k: 1 Loss: 0.2123341622038309\n",
      "Iteration: 671 lambda_k: 1 Loss: 0.21213425519677825\n",
      "Iteration: 672 lambda_k: 1 Loss: 0.21193469510035334\n",
      "Iteration: 673 lambda_k: 1 Loss: 0.2117354797173397\n",
      "Iteration: 674 lambda_k: 1 Loss: 0.21153660686155773\n",
      "Iteration: 675 lambda_k: 1 Loss: 0.21133807435885846\n",
      "Iteration: 676 lambda_k: 1 Loss: 0.21113988004791126\n",
      "Iteration: 677 lambda_k: 1 Loss: 0.2109420217810706\n",
      "Iteration: 678 lambda_k: 1 Loss: 0.21074449742491005\n",
      "Iteration: 679 lambda_k: 1 Loss: 0.21054730486078688\n",
      "Iteration: 680 lambda_k: 1 Loss: 0.21035044198528863\n",
      "Iteration: 681 lambda_k: 1 Loss: 0.210153906710604\n",
      "Iteration: 682 lambda_k: 1 Loss: 0.20995769696482938\n",
      "Iteration: 683 lambda_k: 1 Loss: 0.2097618106922177\n",
      "Iteration: 684 lambda_k: 1 Loss: 0.20956624585337663\n",
      "Iteration: 685 lambda_k: 1 Loss: 0.20937100042542306\n",
      "Iteration: 686 lambda_k: 1 Loss: 0.20917607240209835\n",
      "Iteration: 687 lambda_k: 1 Loss: 0.20898145979385027\n",
      "Iteration: 688 lambda_k: 1 Loss: 0.2087871606278857\n",
      "Iteration: 689 lambda_k: 1 Loss: 0.20859317294819782\n",
      "Iteration: 690 lambda_k: 1 Loss: 0.20839949481557077\n",
      "Iteration: 691 lambda_k: 1 Loss: 0.20820612430756577\n",
      "Iteration: 692 lambda_k: 1 Loss: 0.20801305951849\n",
      "Iteration: 693 lambda_k: 1 Loss: 0.20782029855935152\n",
      "Iteration: 694 lambda_k: 1 Loss: 0.20762783955780123\n",
      "Iteration: 695 lambda_k: 1 Loss: 0.2074356806580649\n",
      "Iteration: 696 lambda_k: 1 Loss: 0.20724382002086497\n",
      "Iteration: 697 lambda_k: 1 Loss: 0.20705225582333528\n",
      "Iteration: 698 lambda_k: 1 Loss: 0.20686098625892865\n",
      "Iteration: 699 lambda_k: 1 Loss: 0.2066700095373189\n",
      "Iteration: 700 lambda_k: 1 Loss: 0.20647932388429743\n",
      "Iteration: 701 lambda_k: 1 Loss: 0.20628892754166653\n",
      "Iteration: 702 lambda_k: 1 Loss: 0.2060988187671285\n",
      "Iteration: 703 lambda_k: 1 Loss: 0.20590899583417216\n",
      "Iteration: 704 lambda_k: 1 Loss: 0.20571945703195688\n",
      "Iteration: 705 lambda_k: 1 Loss: 0.2055302006651948\n",
      "Iteration: 706 lambda_k: 1 Loss: 0.20534122505403155\n",
      "Iteration: 707 lambda_k: 1 Loss: 0.20515252853392502\n",
      "Iteration: 708 lambda_k: 1 Loss: 0.20496410945552437\n",
      "Iteration: 709 lambda_k: 1 Loss: 0.20477596618454733\n",
      "Iteration: 710 lambda_k: 1 Loss: 0.20458809710165754\n",
      "Iteration: 711 lambda_k: 1 Loss: 0.20440050060234133\n",
      "Iteration: 712 lambda_k: 1 Loss: 0.20421317509678483\n",
      "Iteration: 713 lambda_k: 1 Loss: 0.20402611900975035\n",
      "Iteration: 714 lambda_k: 1 Loss: 0.20383933078045383\n",
      "Iteration: 715 lambda_k: 1 Loss: 0.20365280886244178\n",
      "Iteration: 716 lambda_k: 1 Loss: 0.2034665517234689\n",
      "Iteration: 717 lambda_k: 1 Loss: 0.20328055784537627\n",
      "Iteration: 718 lambda_k: 1 Loss: 0.2030948257239696\n",
      "Iteration: 719 lambda_k: 1 Loss: 0.20290935386889863\n",
      "Iteration: 720 lambda_k: 1 Loss: 0.20272414080353654\n",
      "Iteration: 721 lambda_k: 1 Loss: 0.2025391850648605\n",
      "Iteration: 722 lambda_k: 1 Loss: 0.20235448520333182\n",
      "Iteration: 723 lambda_k: 1 Loss: 0.20217003978277948\n",
      "Iteration: 724 lambda_k: 1 Loss: 0.20198584738028524\n",
      "Iteration: 725 lambda_k: 1 Loss: 0.2018019065860564\n",
      "Iteration: 726 lambda_k: 1 Loss: 0.20161821600332097\n",
      "Iteration: 727 lambda_k: 1 Loss: 0.20143477424820785\n",
      "Iteration: 728 lambda_k: 1 Loss: 0.20125157994963305\n",
      "Iteration: 729 lambda_k: 1 Loss: 0.20106863174918618\n",
      "Iteration: 730 lambda_k: 1 Loss: 0.200885928301018\n",
      "Iteration: 731 lambda_k: 1 Loss: 0.20070346827172864\n",
      "Iteration: 732 lambda_k: 1 Loss: 0.20052125034025634\n",
      "Iteration: 733 lambda_k: 1 Loss: 0.20033927319776737\n",
      "Iteration: 734 lambda_k: 1 Loss: 0.20015753554754712\n",
      "Iteration: 735 lambda_k: 1 Loss: 0.1999760361048911\n",
      "Iteration: 736 lambda_k: 1 Loss: 0.1997947735969976\n",
      "Iteration: 737 lambda_k: 1 Loss: 0.19961374676286076\n",
      "Iteration: 738 lambda_k: 1 Loss: 0.19943295435316485\n",
      "Iteration: 739 lambda_k: 1 Loss: 0.19925239513017878\n",
      "Iteration: 740 lambda_k: 1 Loss: 0.19907206786765225\n",
      "Iteration: 741 lambda_k: 1 Loss: 0.19889197135071165\n",
      "Iteration: 742 lambda_k: 1 Loss: 0.19871210437575793\n",
      "Iteration: 743 lambda_k: 1 Loss: 0.19853246575036443\n",
      "Iteration: 744 lambda_k: 1 Loss: 0.19835305429317715\n",
      "Iteration: 745 lambda_k: 1 Loss: 0.19817386883381516\n",
      "Iteration: 746 lambda_k: 1 Loss: 0.19799490821276003\n",
      "Iteration: 747 lambda_k: 1 Loss: 0.19781617128127305\n",
      "Iteration: 748 lambda_k: 1 Loss: 0.19763765690128932\n",
      "Iteration: 749 lambda_k: 1 Loss: 0.19745936394532146\n",
      "Iteration: 750 lambda_k: 1 Loss: 0.1972812912963632\n",
      "Iteration: 751 lambda_k: 1 Loss: 0.19710343784779397\n",
      "Iteration: 752 lambda_k: 1 Loss: 0.196925802503284\n",
      "Iteration: 753 lambda_k: 1 Loss: 0.19674838417767274\n",
      "Iteration: 754 lambda_k: 1 Loss: 0.1965711817927993\n",
      "Iteration: 755 lambda_k: 1 Loss: 0.19639419428394705\n",
      "Iteration: 756 lambda_k: 1 Loss: 0.1962174205949059\n",
      "Iteration: 757 lambda_k: 1 Loss: 0.19604085967935755\n",
      "Iteration: 758 lambda_k: 1 Loss: 0.1958645105007682\n",
      "Iteration: 759 lambda_k: 1 Loss: 0.19568837203224412\n",
      "Iteration: 760 lambda_k: 1 Loss: 0.19551244325645467\n",
      "Iteration: 761 lambda_k: 1 Loss: 0.19533672316553155\n",
      "Iteration: 762 lambda_k: 1 Loss: 0.19516121076098578\n",
      "Iteration: 763 lambda_k: 1 Loss: 0.19498590505362243\n",
      "Iteration: 764 lambda_k: 1 Loss: 0.19481080506345408\n",
      "Iteration: 765 lambda_k: 1 Loss: 0.19463590981961443\n",
      "Iteration: 766 lambda_k: 1 Loss: 0.19446121836027266\n",
      "Iteration: 767 lambda_k: 1 Loss: 0.19428672973254874\n",
      "Iteration: 768 lambda_k: 1 Loss: 0.1941124429924299\n",
      "Iteration: 769 lambda_k: 1 Loss: 0.1939383572046866\n",
      "Iteration: 770 lambda_k: 1 Loss: 0.19376447144279035\n",
      "Iteration: 771 lambda_k: 1 Loss: 0.1935907847888313\n",
      "Iteration: 772 lambda_k: 1 Loss: 0.1934172963334367\n",
      "Iteration: 773 lambda_k: 1 Loss: 0.19324400517569018\n",
      "Iteration: 774 lambda_k: 1 Loss: 0.19307091042305127\n",
      "Iteration: 775 lambda_k: 1 Loss: 0.19289801119127567\n",
      "Iteration: 776 lambda_k: 1 Loss: 0.1927253066043361\n",
      "Iteration: 777 lambda_k: 1 Loss: 0.192552795794344\n",
      "Iteration: 778 lambda_k: 1 Loss: 0.19238047790147125\n",
      "Iteration: 779 lambda_k: 1 Loss: 0.19220835207387274\n",
      "Iteration: 780 lambda_k: 1 Loss: 0.1920364174676096\n",
      "Iteration: 781 lambda_k: 1 Loss: 0.19186467324657258\n",
      "Iteration: 782 lambda_k: 1 Loss: 0.19169311858240604\n",
      "Iteration: 783 lambda_k: 1 Loss: 0.19152175265446686\n",
      "Iteration: 784 lambda_k: 1 Loss: 0.19135057464965735\n",
      "Iteration: 785 lambda_k: 1 Loss: 0.1911795837613326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 786 lambda_k: 1 Loss: 0.19100877919392695\n",
      "Iteration: 787 lambda_k: 1 Loss: 0.1908381601552226\n",
      "Iteration: 788 lambda_k: 1 Loss: 0.19066772586191646\n",
      "Iteration: 789 lambda_k: 1 Loss: 0.19049747553792634\n",
      "Iteration: 790 lambda_k: 1 Loss: 0.1903274084142853\n",
      "Iteration: 791 lambda_k: 1 Loss: 0.19015752372911024\n",
      "Iteration: 792 lambda_k: 1 Loss: 0.18998782072755196\n",
      "Iteration: 793 lambda_k: 1 Loss: 0.18981829866172695\n",
      "Iteration: 794 lambda_k: 1 Loss: 0.18964895679064295\n",
      "Iteration: 795 lambda_k: 1 Loss: 0.18947979438012694\n",
      "Iteration: 796 lambda_k: 1 Loss: 0.18931081070275943\n",
      "Iteration: 797 lambda_k: 1 Loss: 0.18914200503780648\n",
      "Iteration: 798 lambda_k: 1 Loss: 0.18897337667115285\n",
      "Iteration: 799 lambda_k: 1 Loss: 0.18880492489523576\n",
      "Iteration: 800 lambda_k: 1 Loss: 0.18863664900897906\n",
      "Iteration: 801 lambda_k: 1 Loss: 0.1884685483177279\n",
      "Iteration: 802 lambda_k: 1 Loss: 0.1883006221331837\n",
      "Iteration: 803 lambda_k: 1 Loss: 0.18813286977333996\n",
      "Iteration: 804 lambda_k: 1 Loss: 0.1879652905624185\n",
      "Iteration: 805 lambda_k: 1 Loss: 0.18779788383080634\n",
      "Iteration: 806 lambda_k: 1 Loss: 0.1876306489149929\n",
      "Iteration: 807 lambda_k: 1 Loss: 0.18746358515750772\n",
      "Iteration: 808 lambda_k: 1 Loss: 0.18729669190685916\n",
      "Iteration: 809 lambda_k: 1 Loss: 0.187129968517473\n",
      "Iteration: 810 lambda_k: 1 Loss: 0.18696341434963204\n",
      "Iteration: 811 lambda_k: 1 Loss: 0.18679702876941603\n",
      "Iteration: 812 lambda_k: 1 Loss: 0.18663081114864236\n",
      "Iteration: 813 lambda_k: 1 Loss: 0.18646476086480684\n",
      "Iteration: 814 lambda_k: 1 Loss: 0.18629887730102568\n",
      "Iteration: 815 lambda_k: 1 Loss: 0.18613315984597745\n",
      "Iteration: 816 lambda_k: 1 Loss: 0.1859676078938457\n",
      "Iteration: 817 lambda_k: 1 Loss: 0.18580222084426276\n",
      "Iteration: 818 lambda_k: 1 Loss: 0.185636998102253\n",
      "Iteration: 819 lambda_k: 1 Loss: 0.18547193907817752\n",
      "Iteration: 820 lambda_k: 1 Loss: 0.18530704318767907\n",
      "Iteration: 821 lambda_k: 1 Loss: 0.1851423098516276\n",
      "Iteration: 822 lambda_k: 1 Loss: 0.18497773849606658\n",
      "Iteration: 823 lambda_k: 1 Loss: 0.18481332855215934\n",
      "Iteration: 824 lambda_k: 1 Loss: 0.1846490794561365\n",
      "Iteration: 825 lambda_k: 1 Loss: 0.18448499064924387\n",
      "Iteration: 826 lambda_k: 1 Loss: 0.1843210615776909\n",
      "Iteration: 827 lambda_k: 1 Loss: 0.18415729169259967\n",
      "Iteration: 828 lambda_k: 1 Loss: 0.18399368044995443\n",
      "Iteration: 829 lambda_k: 1 Loss: 0.18383022731055204\n",
      "Iteration: 830 lambda_k: 1 Loss: 0.18366693173995255\n",
      "Iteration: 831 lambda_k: 1 Loss: 0.18350379320843085\n",
      "Iteration: 832 lambda_k: 1 Loss: 0.18334081119092868\n",
      "Iteration: 833 lambda_k: 1 Loss: 0.18317798516700703\n",
      "Iteration: 834 lambda_k: 1 Loss: 0.18301531462079992\n",
      "Iteration: 835 lambda_k: 1 Loss: 0.18285279904096774\n",
      "Iteration: 836 lambda_k: 1 Loss: 0.1826904379206523\n",
      "Iteration: 837 lambda_k: 1 Loss: 0.18252823075743133\n",
      "Iteration: 838 lambda_k: 1 Loss: 0.1823661770532751\n",
      "Iteration: 839 lambda_k: 1 Loss: 0.18220427631450226\n",
      "Iteration: 840 lambda_k: 1 Loss: 0.18204252805173685\n",
      "Iteration: 841 lambda_k: 1 Loss: 0.18188093177986642\n",
      "Iteration: 842 lambda_k: 1 Loss: 0.18171948701799995\n",
      "Iteration: 843 lambda_k: 1 Loss: 0.18155819328942727\n",
      "Iteration: 844 lambda_k: 1 Loss: 0.18139705012157836\n",
      "Iteration: 845 lambda_k: 1 Loss: 0.18123605704598378\n",
      "Iteration: 846 lambda_k: 1 Loss: 0.18107521359823558\n",
      "Iteration: 847 lambda_k: 1 Loss: 0.18091451931794908\n",
      "Iteration: 848 lambda_k: 1 Loss: 0.18075397374872487\n",
      "Iteration: 849 lambda_k: 1 Loss: 0.1805935764381118\n",
      "Iteration: 850 lambda_k: 1 Loss: 0.18043332693757094\n",
      "Iteration: 851 lambda_k: 1 Loss: 0.18027322480243935\n",
      "Iteration: 852 lambda_k: 1 Loss: 0.18011326959189522\n",
      "Iteration: 853 lambda_k: 1 Loss: 0.1799534608689234\n",
      "Iteration: 854 lambda_k: 1 Loss: 0.17979379820028174\n",
      "Iteration: 855 lambda_k: 1 Loss: 0.17963428115646776\n",
      "Iteration: 856 lambda_k: 1 Loss: 0.1794749093116866\n",
      "Iteration: 857 lambda_k: 1 Loss: 0.17931568224381886\n",
      "Iteration: 858 lambda_k: 1 Loss: 0.17915659953438978\n",
      "Iteration: 859 lambda_k: 1 Loss: 0.17899766076853862\n",
      "Iteration: 860 lambda_k: 1 Loss: 0.17883886553498896\n",
      "Iteration: 861 lambda_k: 1 Loss: 0.1786802134260194\n",
      "Iteration: 862 lambda_k: 1 Loss: 0.17852170403743528\n",
      "Iteration: 863 lambda_k: 1 Loss: 0.1783633369685407\n",
      "Iteration: 864 lambda_k: 1 Loss: 0.17820511182211146\n",
      "Iteration: 865 lambda_k: 1 Loss: 0.17804702820436819\n",
      "Iteration: 866 lambda_k: 1 Loss: 0.1778890857249508\n",
      "Iteration: 867 lambda_k: 1 Loss: 0.17773128399689286\n",
      "Iteration: 868 lambda_k: 1 Loss: 0.17757362263659726\n",
      "Iteration: 869 lambda_k: 1 Loss: 0.17741610126381177\n",
      "Iteration: 870 lambda_k: 1 Loss: 0.17725871950160596\n",
      "Iteration: 871 lambda_k: 1 Loss: 0.177101476976348\n",
      "Iteration: 872 lambda_k: 1 Loss: 0.17694437331768276\n",
      "Iteration: 873 lambda_k: 1 Loss: 0.17678740815850985\n",
      "Iteration: 874 lambda_k: 1 Loss: 0.1766305811349628\n",
      "Iteration: 875 lambda_k: 1 Loss: 0.17647389188638973\n",
      "Iteration: 876 lambda_k: 1 Loss: 0.17631734005533112\n",
      "Iteration: 877 lambda_k: 1 Loss: 0.17616092528749963\n",
      "Iteration: 878 lambda_k: 1 Loss: 0.17600464723176523\n",
      "Iteration: 879 lambda_k: 1 Loss: 0.17584850554013454\n",
      "Iteration: 880 lambda_k: 1 Loss: 0.17569249986773317\n",
      "Iteration: 881 lambda_k: 1 Loss: 0.17553662987278867\n",
      "Iteration: 882 lambda_k: 1 Loss: 0.17538089521661365\n",
      "Iteration: 883 lambda_k: 1 Loss: 0.17522529556358998\n",
      "Iteration: 884 lambda_k: 1 Loss: 0.17506983058115244\n",
      "Iteration: 885 lambda_k: 1 Loss: 0.17491449993977393\n",
      "Iteration: 886 lambda_k: 1 Loss: 0.17475930331295025\n",
      "Iteration: 887 lambda_k: 1 Loss: 0.17460424037718603\n",
      "Iteration: 888 lambda_k: 1 Loss: 0.17444931081198053\n",
      "Iteration: 889 lambda_k: 1 Loss: 0.17429451429981424\n",
      "Iteration: 890 lambda_k: 1 Loss: 0.17413985052613587\n",
      "Iteration: 891 lambda_k: 1 Loss: 0.1739853191793493\n",
      "Iteration: 892 lambda_k: 1 Loss: 0.17383091995080163\n",
      "Iteration: 893 lambda_k: 1 Loss: 0.17367665253477096\n",
      "Iteration: 894 lambda_k: 1 Loss: 0.1735225166284548\n",
      "Iteration: 895 lambda_k: 1 Loss: 0.17336851193195887\n",
      "Iteration: 896 lambda_k: 1 Loss: 0.17321463814828614\n",
      "Iteration: 897 lambda_k: 1 Loss: 0.17306089498332608\n",
      "Iteration: 898 lambda_k: 1 Loss: 0.17290728214584436\n",
      "Iteration: 899 lambda_k: 1 Loss: 0.1727537993474731\n",
      "Iteration: 900 lambda_k: 1 Loss: 0.17260044630270052\n",
      "Iteration: 901 lambda_k: 1 Loss: 0.17244722272886198\n",
      "Iteration: 902 lambda_k: 1 Loss: 0.1722941283461302\n",
      "Iteration: 903 lambda_k: 1 Loss: 0.17214116287750666\n",
      "Iteration: 904 lambda_k: 1 Loss: 0.17198832604881245\n",
      "Iteration: 905 lambda_k: 1 Loss: 0.17183561758867977\n",
      "Iteration: 906 lambda_k: 1 Loss: 0.17168303722854336\n",
      "Iteration: 907 lambda_k: 1 Loss: 0.17153058470263247\n",
      "Iteration: 908 lambda_k: 1 Loss: 0.17137825974796236\n",
      "Iteration: 909 lambda_k: 1 Loss: 0.17122606210432675\n",
      "Iteration: 910 lambda_k: 1 Loss: 0.17107399151428973\n",
      "Iteration: 911 lambda_k: 1 Loss: 0.17092204772317807\n",
      "Iteration: 912 lambda_k: 1 Loss: 0.1707702304790736\n",
      "Iteration: 913 lambda_k: 1 Loss: 0.17061853953280592\n",
      "Iteration: 914 lambda_k: 1 Loss: 0.1704669746379447\n",
      "Iteration: 915 lambda_k: 1 Loss: 0.17031553555079235\n",
      "Iteration: 916 lambda_k: 1 Loss: 0.1701642220303769\n",
      "Iteration: 917 lambda_k: 1 Loss: 0.17001303383844701\n",
      "Iteration: 918 lambda_k: 1 Loss: 0.16986197074175682\n",
      "Iteration: 919 lambda_k: 1 Loss: 0.1697110325025563\n",
      "Iteration: 920 lambda_k: 1 Loss: 0.16956021889338183\n",
      "Iteration: 921 lambda_k: 1 Loss: 0.1694095296868406\n",
      "Iteration: 922 lambda_k: 1 Loss: 0.16925896465808787\n",
      "Iteration: 923 lambda_k: 1 Loss: 0.1691085235849789\n",
      "Iteration: 924 lambda_k: 1 Loss: 0.16895820624811153\n",
      "Iteration: 925 lambda_k: 1 Loss: 0.16880801243070975\n",
      "Iteration: 926 lambda_k: 1 Loss: 0.16865794191865946\n",
      "Iteration: 927 lambda_k: 1 Loss: 0.1685079945004939\n",
      "Iteration: 928 lambda_k: 1 Loss: 0.16835816996741204\n",
      "Iteration: 929 lambda_k: 1 Loss: 0.16820846811428342\n",
      "Iteration: 930 lambda_k: 1 Loss: 0.16805888873066774\n",
      "Iteration: 931 lambda_k: 1 Loss: 0.1679094316266714\n",
      "Iteration: 932 lambda_k: 1 Loss: 0.1677600965983843\n",
      "Iteration: 933 lambda_k: 1 Loss: 0.16761088345001074\n",
      "Iteration: 934 lambda_k: 1 Loss: 0.16746179198862657\n",
      "Iteration: 935 lambda_k: 1 Loss: 0.167312822023788\n",
      "Iteration: 936 lambda_k: 1 Loss: 0.16716397336749136\n",
      "Iteration: 937 lambda_k: 1 Loss: 0.167015245834276\n",
      "Iteration: 938 lambda_k: 1 Loss: 0.16686663924126874\n",
      "Iteration: 939 lambda_k: 1 Loss: 0.16671815340816748\n",
      "Iteration: 940 lambda_k: 1 Loss: 0.16656978815721457\n",
      "Iteration: 941 lambda_k: 1 Loss: 0.16642154331318307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 942 lambda_k: 1 Loss: 0.16627341870336973\n",
      "Iteration: 943 lambda_k: 1 Loss: 0.1661254141575888\n",
      "Iteration: 944 lambda_k: 1 Loss: 0.16597752950816322\n",
      "Iteration: 945 lambda_k: 1 Loss: 0.1658297645899154\n",
      "Iteration: 946 lambda_k: 1 Loss: 0.16568211924015785\n",
      "Iteration: 947 lambda_k: 1 Loss: 0.16553459329868386\n",
      "Iteration: 948 lambda_k: 1 Loss: 0.1653871866077574\n",
      "Iteration: 949 lambda_k: 1 Loss: 0.16523989901226735\n",
      "Iteration: 950 lambda_k: 1 Loss: 0.16509273035903085\n",
      "Iteration: 951 lambda_k: 1 Loss: 0.1649456804978881\n",
      "Iteration: 952 lambda_k: 1 Loss: 0.16479874928088198\n",
      "Iteration: 953 lambda_k: 1 Loss: 0.1646519365624706\n",
      "Iteration: 954 lambda_k: 1 Loss: 0.16450524219953336\n",
      "Iteration: 955 lambda_k: 1 Loss: 0.16435866605135724\n",
      "Iteration: 956 lambda_k: 1 Loss: 0.16421220797961986\n",
      "Iteration: 957 lambda_k: 1 Loss: 0.16406586784837685\n",
      "Iteration: 958 lambda_k: 1 Loss: 0.16391964552405136\n",
      "Iteration: 959 lambda_k: 1 Loss: 0.16377354087542503\n",
      "Iteration: 960 lambda_k: 1 Loss: 0.16362755377362617\n",
      "Iteration: 961 lambda_k: 1 Loss: 0.16348168409211908\n",
      "Iteration: 962 lambda_k: 1 Loss: 0.1633359317066923\n",
      "Iteration: 963 lambda_k: 1 Loss: 0.16319029649544753\n",
      "Iteration: 964 lambda_k: 1 Loss: 0.16304477833878778\n",
      "Iteration: 965 lambda_k: 1 Loss: 0.16289937711940614\n",
      "Iteration: 966 lambda_k: 1 Loss: 0.16275409272227362\n",
      "Iteration: 967 lambda_k: 1 Loss: 0.16260892503462793\n",
      "Iteration: 968 lambda_k: 1 Loss: 0.16246387394596098\n",
      "Iteration: 969 lambda_k: 1 Loss: 0.16231893934800726\n",
      "Iteration: 970 lambda_k: 1 Loss: 0.16217412113473156\n",
      "Iteration: 971 lambda_k: 1 Loss: 0.1620294192023169\n",
      "Iteration: 972 lambda_k: 1 Loss: 0.16188483344915236\n",
      "Iteration: 973 lambda_k: 1 Loss: 0.16174036377582054\n",
      "Iteration: 974 lambda_k: 1 Loss: 0.1615960100850853\n",
      "Iteration: 975 lambda_k: 1 Loss: 0.1614517722818793\n",
      "Iteration: 976 lambda_k: 1 Loss: 0.1613076502732912\n",
      "Iteration: 977 lambda_k: 1 Loss: 0.16116364396855365\n",
      "Iteration: 978 lambda_k: 1 Loss: 0.16101975327902987\n",
      "Iteration: 979 lambda_k: 1 Loss: 0.16087597811820156\n",
      "Iteration: 980 lambda_k: 1 Loss: 0.1607323184016559\n",
      "Iteration: 981 lambda_k: 1 Loss: 0.16058877404707267\n",
      "Iteration: 982 lambda_k: 1 Loss: 0.1604453449742115\n",
      "Iteration: 983 lambda_k: 1 Loss: 0.16030203110489905\n",
      "Iteration: 984 lambda_k: 1 Loss: 0.1601588323630158\n",
      "Iteration: 985 lambda_k: 1 Loss: 0.16001574867448368\n",
      "Iteration: 986 lambda_k: 1 Loss: 0.15987277996725227\n",
      "Iteration: 987 lambda_k: 1 Loss: 0.15972992617128667\n",
      "Iteration: 988 lambda_k: 1 Loss: 0.1595871872185538\n",
      "Iteration: 989 lambda_k: 1 Loss: 0.15944456304300972\n",
      "Iteration: 990 lambda_k: 1 Loss: 0.15930205358058658\n",
      "Iteration: 991 lambda_k: 1 Loss: 0.15915965876917948\n",
      "Iteration: 992 lambda_k: 1 Loss: 0.1590173785486334\n",
      "Iteration: 993 lambda_k: 1 Loss: 0.15887521286073022\n",
      "Iteration: 994 lambda_k: 1 Loss: 0.15873316164917567\n",
      "Iteration: 995 lambda_k: 1 Loss: 0.1585912248595863\n",
      "Iteration: 996 lambda_k: 1 Loss: 0.15844940243947644\n",
      "Iteration: 997 lambda_k: 1 Loss: 0.1583076943382451\n",
      "Iteration: 998 lambda_k: 1 Loss: 0.15816610050716312\n",
      "Iteration: 999 lambda_k: 1 Loss: 0.15802462089936023\n",
      "Iteration: 1000 lambda_k: 1 Loss: 0.15788325546981183\n",
      "Iteration: 1001 lambda_k: 1 Loss: 0.15774200417532647\n",
      "Iteration: 1002 lambda_k: 1 Loss: 0.1576008669745326\n",
      "Iteration: 1003 lambda_k: 1 Loss: 0.15745984382786582\n",
      "Iteration: 1004 lambda_k: 1 Loss: 0.15731893469755626\n",
      "Iteration: 1005 lambda_k: 1 Loss: 0.15717813954761564\n",
      "Iteration: 1006 lambda_k: 1 Loss: 0.15703745834382432\n",
      "Iteration: 1007 lambda_k: 1 Loss: 0.1568968910537191\n",
      "Iteration: 1008 lambda_k: 1 Loss: 0.15675643764658023\n",
      "Iteration: 1009 lambda_k: 1 Loss: 0.15661609809341878\n",
      "Iteration: 1010 lambda_k: 1 Loss: 0.15647587236696428\n",
      "Iteration: 1011 lambda_k: 1 Loss: 0.15633576044165226\n",
      "Iteration: 1012 lambda_k: 1 Loss: 0.1561957622936116\n",
      "Iteration: 1013 lambda_k: 1 Loss: 0.15605587790065228\n",
      "Iteration: 1014 lambda_k: 1 Loss: 0.15591610724225322\n",
      "Iteration: 1015 lambda_k: 1 Loss: 0.15577645029954987\n",
      "Iteration: 1016 lambda_k: 1 Loss: 0.15563690705532193\n",
      "Iteration: 1017 lambda_k: 1 Loss: 0.15549747749398157\n",
      "Iteration: 1018 lambda_k: 1 Loss: 0.15535816160156096\n",
      "Iteration: 1019 lambda_k: 1 Loss: 0.15521895936570074\n",
      "Iteration: 1020 lambda_k: 1 Loss: 0.1550798707756378\n",
      "Iteration: 1021 lambda_k: 1 Loss: 0.15494089582219372\n",
      "Iteration: 1022 lambda_k: 1 Loss: 0.1548020344977627\n",
      "Iteration: 1023 lambda_k: 1 Loss: 0.15466328679630023\n",
      "Iteration: 1024 lambda_k: 1 Loss: 0.15452465271331123\n",
      "Iteration: 1025 lambda_k: 1 Loss: 0.15438613224583875\n",
      "Iteration: 1026 lambda_k: 1 Loss: 0.1542477253924525\n",
      "Iteration: 1027 lambda_k: 1 Loss: 0.1541094321532373\n",
      "Iteration: 1028 lambda_k: 1 Loss: 0.15397125252978222\n",
      "Iteration: 1029 lambda_k: 1 Loss: 0.15383318652516886\n",
      "Iteration: 1030 lambda_k: 1 Loss: 0.15369523414396086\n",
      "Iteration: 1031 lambda_k: 1 Loss: 0.15355739539219276\n",
      "Iteration: 1032 lambda_k: 1 Loss: 0.1534196702773588\n",
      "Iteration: 1033 lambda_k: 1 Loss: 0.1532820588084022\n",
      "Iteration: 1034 lambda_k: 1 Loss: 0.1531445609957046\n",
      "Iteration: 1035 lambda_k: 1 Loss: 0.15300717685107532\n",
      "Iteration: 1036 lambda_k: 1 Loss: 0.15286990638774117\n",
      "Iteration: 1037 lambda_k: 1 Loss: 0.15273274962033498\n",
      "Iteration: 1038 lambda_k: 1 Loss: 0.15259570656488608\n",
      "Iteration: 1039 lambda_k: 1 Loss: 0.15245877723881013\n",
      "Iteration: 1040 lambda_k: 1 Loss: 0.1523219616608985\n",
      "Iteration: 1041 lambda_k: 1 Loss: 0.15218525985130849\n",
      "Iteration: 1042 lambda_k: 1 Loss: 0.15204867183155318\n",
      "Iteration: 1043 lambda_k: 1 Loss: 0.15191219762449176\n",
      "Iteration: 1044 lambda_k: 1 Loss: 0.15177583725431928\n",
      "Iteration: 1045 lambda_k: 1 Loss: 0.1516395907465577\n",
      "Iteration: 1046 lambda_k: 1 Loss: 0.15150345812804547\n",
      "Iteration: 1047 lambda_k: 1 Loss: 0.15136743942692873\n",
      "Iteration: 1048 lambda_k: 1 Loss: 0.15123153467265146\n",
      "Iteration: 1049 lambda_k: 1 Loss: 0.1510957438959463\n",
      "Iteration: 1050 lambda_k: 1 Loss: 0.15096006712883364\n",
      "Iteration: 1051 lambda_k: 1 Loss: 0.1508245044045886\n",
      "Iteration: 1052 lambda_k: 1 Loss: 0.1506890557577536\n",
      "Iteration: 1053 lambda_k: 1 Loss: 0.1505537212241255\n",
      "Iteration: 1054 lambda_k: 1 Loss: 0.15041850084074473\n",
      "Iteration: 1055 lambda_k: 1 Loss: 0.1502833946458861\n",
      "Iteration: 1056 lambda_k: 1 Loss: 0.1501484026790509\n",
      "Iteration: 1057 lambda_k: 1 Loss: 0.1500135249809582\n",
      "Iteration: 1058 lambda_k: 1 Loss: 0.14987876159353647\n",
      "Iteration: 1059 lambda_k: 1 Loss: 0.1497441125599154\n",
      "Iteration: 1060 lambda_k: 1 Loss: 0.14960957792441734\n",
      "Iteration: 1061 lambda_k: 1 Loss: 0.14947515773254932\n",
      "Iteration: 1062 lambda_k: 1 Loss: 0.14934085203099517\n",
      "Iteration: 1063 lambda_k: 1 Loss: 0.1492066608676075\n",
      "Iteration: 1064 lambda_k: 1 Loss: 0.14907258429139963\n",
      "Iteration: 1065 lambda_k: 1 Loss: 0.14893862235253838\n",
      "Iteration: 1066 lambda_k: 1 Loss: 0.1488047751023359\n",
      "Iteration: 1067 lambda_k: 1 Loss: 0.14867104259324235\n",
      "Iteration: 1068 lambda_k: 1 Loss: 0.1485374248788386\n",
      "Iteration: 1069 lambda_k: 1 Loss: 0.14840392201382857\n",
      "Iteration: 1070 lambda_k: 1 Loss: 0.14827053405403232\n",
      "Iteration: 1071 lambda_k: 1 Loss: 0.14813726105637862\n",
      "Iteration: 1072 lambda_k: 1 Loss: 0.14800410307889802\n",
      "Iteration: 1073 lambda_k: 1 Loss: 0.14787106018071594\n",
      "Iteration: 1074 lambda_k: 1 Loss: 0.14773813242204567\n",
      "Iteration: 1075 lambda_k: 1 Loss: 0.14760531986418177\n",
      "Iteration: 1076 lambda_k: 1 Loss: 0.14747262256949317\n",
      "Iteration: 1077 lambda_k: 1 Loss: 0.14734004060141678\n",
      "Iteration: 1078 lambda_k: 1 Loss: 0.14720757402445098\n",
      "Iteration: 1079 lambda_k: 1 Loss: 0.14707522290414926\n",
      "Iteration: 1080 lambda_k: 1 Loss: 0.14694298730711372\n",
      "Iteration: 1081 lambda_k: 1 Loss: 0.146810867304555\n",
      "Iteration: 1082 lambda_k: 1 Loss: 0.1466788629580895\n",
      "Iteration: 1083 lambda_k: 1 Loss: 0.14654697434037536\n",
      "Iteration: 1084 lambda_k: 1 Loss: 0.14641520152280163\n",
      "Iteration: 1085 lambda_k: 1 Loss: 0.14628354457719003\n",
      "Iteration: 1086 lambda_k: 1 Loss: 0.14615200357619748\n",
      "Iteration: 1087 lambda_k: 1 Loss: 0.14602057859355608\n",
      "Iteration: 1088 lambda_k: 1 Loss: 0.1458892697040498\n",
      "Iteration: 1089 lambda_k: 1 Loss: 0.14575807698345825\n",
      "Iteration: 1090 lambda_k: 1 Loss: 0.1456270005085414\n",
      "Iteration: 1091 lambda_k: 1 Loss: 0.14549604035704317\n",
      "Iteration: 1092 lambda_k: 1 Loss: 0.14536519660769034\n",
      "Iteration: 1093 lambda_k: 1 Loss: 0.14523446934018716\n",
      "Iteration: 1094 lambda_k: 1 Loss: 0.1451038586352091\n",
      "Iteration: 1095 lambda_k: 1 Loss: 0.14497336457439738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1096 lambda_k: 1 Loss: 0.1448429872403541\n",
      "Iteration: 1097 lambda_k: 1 Loss: 0.14471272671663787\n",
      "Iteration: 1098 lambda_k: 1 Loss: 0.14458258308410257\n",
      "Iteration: 1099 lambda_k: 1 Loss: 0.1444525564354091\n",
      "Iteration: 1100 lambda_k: 1 Loss: 0.144322646854038\n",
      "Iteration: 1101 lambda_k: 1 Loss: 0.14419285442660884\n",
      "Iteration: 1102 lambda_k: 1 Loss: 0.14406317924130693\n",
      "Iteration: 1103 lambda_k: 1 Loss: 0.14393362138745072\n",
      "Iteration: 1104 lambda_k: 1 Loss: 0.1438041809552177\n",
      "Iteration: 1105 lambda_k: 1 Loss: 0.1436748580356531\n",
      "Iteration: 1106 lambda_k: 1 Loss: 0.14354565272071976\n",
      "Iteration: 1107 lambda_k: 1 Loss: 0.14341656510330517\n",
      "Iteration: 1108 lambda_k: 1 Loss: 0.14328759527720974\n",
      "Iteration: 1109 lambda_k: 1 Loss: 0.1431587433371388\n",
      "Iteration: 1110 lambda_k: 1 Loss: 0.14303000937869917\n",
      "Iteration: 1111 lambda_k: 1 Loss: 0.14290139349839728\n",
      "Iteration: 1112 lambda_k: 1 Loss: 0.1427728957936367\n",
      "Iteration: 1113 lambda_k: 1 Loss: 0.14264451636271425\n",
      "Iteration: 1114 lambda_k: 1 Loss: 0.1425162553048174\n",
      "Iteration: 1115 lambda_k: 1 Loss: 0.14238811272002108\n",
      "Iteration: 1116 lambda_k: 1 Loss: 0.14226008870928483\n",
      "Iteration: 1117 lambda_k: 1 Loss: 0.1421321833744502\n",
      "Iteration: 1118 lambda_k: 1 Loss: 0.14200439681823784\n",
      "Iteration: 1119 lambda_k: 1 Loss: 0.1418767291442453\n",
      "Iteration: 1120 lambda_k: 1 Loss: 0.14174918045695534\n",
      "Iteration: 1121 lambda_k: 1 Loss: 0.14162175086175413\n",
      "Iteration: 1122 lambda_k: 1 Loss: 0.14149444046886847\n",
      "Iteration: 1123 lambda_k: 1 Loss: 0.14136724937746883\n",
      "Iteration: 1124 lambda_k: 1 Loss: 0.14124017769854041\n",
      "Iteration: 1125 lambda_k: 1 Loss: 0.14111322554177072\n",
      "Iteration: 1126 lambda_k: 1 Loss: 0.14098639301695112\n",
      "Iteration: 1127 lambda_k: 1 Loss: 0.1408596802344816\n",
      "Iteration: 1128 lambda_k: 1 Loss: 0.14073308730570072\n",
      "Iteration: 1129 lambda_k: 1 Loss: 0.14060661434293553\n",
      "Iteration: 1130 lambda_k: 1 Loss: 0.14048026145920933\n",
      "Iteration: 1131 lambda_k: 1 Loss: 0.14035402876846698\n",
      "Iteration: 1132 lambda_k: 1 Loss: 0.1402279163855144\n",
      "Iteration: 1133 lambda_k: 1 Loss: 0.14010192442600997\n",
      "Iteration: 1134 lambda_k: 1 Loss: 0.13997605300646196\n",
      "Iteration: 1135 lambda_k: 1 Loss: 0.13985030224422884\n",
      "Iteration: 1136 lambda_k: 1 Loss: 0.139724672257519\n",
      "Iteration: 1137 lambda_k: 1 Loss: 0.13959916316538992\n",
      "Iteration: 1138 lambda_k: 1 Loss: 0.13947377508774758\n",
      "Iteration: 1139 lambda_k: 1 Loss: 0.13934850814534616\n",
      "Iteration: 1140 lambda_k: 1 Loss: 0.13922336245978764\n",
      "Iteration: 1141 lambda_k: 1 Loss: 0.13909833815352193\n",
      "Iteration: 1142 lambda_k: 1 Loss: 0.13897343534984707\n",
      "Iteration: 1143 lambda_k: 1 Loss: 0.13884865417290926\n",
      "Iteration: 1144 lambda_k: 1 Loss: 0.13872399474770333\n",
      "Iteration: 1145 lambda_k: 1 Loss: 0.13859945720007324\n",
      "Iteration: 1146 lambda_k: 1 Loss: 0.13847504165671287\n",
      "Iteration: 1147 lambda_k: 1 Loss: 0.13835074824516647\n",
      "Iteration: 1148 lambda_k: 1 Loss: 0.1382265770938301\n",
      "Iteration: 1149 lambda_k: 1 Loss: 0.1381025283319525\n",
      "Iteration: 1150 lambda_k: 1 Loss: 0.1379786020896364\n",
      "Iteration: 1151 lambda_k: 1 Loss: 0.1378547984978401\n",
      "Iteration: 1152 lambda_k: 1 Loss: 0.137731117688379\n",
      "Iteration: 1153 lambda_k: 1 Loss: 0.1376075597939276\n",
      "Iteration: 1154 lambda_k: 1 Loss: 0.1374841249480211\n",
      "Iteration: 1155 lambda_k: 1 Loss: 0.13736081328505792\n",
      "Iteration: 1156 lambda_k: 1 Loss: 0.1372376249403021\n",
      "Iteration: 1157 lambda_k: 1 Loss: 0.13711456004988545\n",
      "Iteration: 1158 lambda_k: 1 Loss: 0.1369916187468405\n",
      "Iteration: 1159 lambda_k: 1 Loss: 0.13686880117674574\n",
      "Iteration: 1160 lambda_k: 1 Loss: 0.1367461074755163\n",
      "Iteration: 1161 lambda_k: 1 Loss: 0.13662353778183212\n",
      "Iteration: 1162 lambda_k: 1 Loss: 0.1365010922360489\n",
      "Iteration: 1163 lambda_k: 1 Loss: 0.1363787709797026\n",
      "Iteration: 1164 lambda_k: 1 Loss: 0.1362565741551578\n",
      "Iteration: 1165 lambda_k: 1 Loss: 0.13613450190561924\n",
      "Iteration: 1166 lambda_k: 1 Loss: 0.13601255437516915\n",
      "Iteration: 1167 lambda_k: 1 Loss: 0.13589073170880167\n",
      "Iteration: 1168 lambda_k: 1 Loss: 0.1357690340524323\n",
      "Iteration: 1169 lambda_k: 1 Loss: 0.13564746155289062\n",
      "Iteration: 1170 lambda_k: 1 Loss: 0.13552601435792466\n",
      "Iteration: 1171 lambda_k: 1 Loss: 0.13540469261620947\n",
      "Iteration: 1172 lambda_k: 1 Loss: 0.13528349647736926\n",
      "Iteration: 1173 lambda_k: 1 Loss: 0.1351624260919336\n",
      "Iteration: 1174 lambda_k: 1 Loss: 0.13504148161139193\n",
      "Iteration: 1175 lambda_k: 1 Loss: 0.13492066318819035\n",
      "Iteration: 1176 lambda_k: 1 Loss: 0.13479997097573584\n",
      "Iteration: 1177 lambda_k: 1 Loss: 0.13467940512840332\n",
      "Iteration: 1178 lambda_k: 1 Loss: 0.13455896580154378\n",
      "Iteration: 1179 lambda_k: 1 Loss: 0.13443865315470113\n",
      "Iteration: 1180 lambda_k: 1 Loss: 0.1343184673347725\n",
      "Iteration: 1181 lambda_k: 1 Loss: 0.13419840851138506\n",
      "Iteration: 1182 lambda_k: 1 Loss: 0.13407847682902682\n",
      "Iteration: 1183 lambda_k: 1 Loss: 0.13395867247845386\n",
      "Iteration: 1184 lambda_k: 1 Loss: 0.1338389955704498\n",
      "Iteration: 1185 lambda_k: 1 Loss: 0.13371935022772388\n",
      "Iteration: 1186 lambda_k: 1 Loss: 0.13360000137014374\n",
      "Iteration: 1187 lambda_k: 1 Loss: 0.13348085394625847\n",
      "Iteration: 1188 lambda_k: 1 Loss: 0.13336182506172697\n",
      "Iteration: 1189 lambda_k: 1 Loss: 0.1332428874036446\n",
      "Iteration: 1190 lambda_k: 1 Loss: 0.1331240487238455\n",
      "Iteration: 1191 lambda_k: 1 Loss: 0.1330053242025721\n",
      "Iteration: 1192 lambda_k: 1 Loss: 0.13288672190776474\n",
      "Iteration: 1193 lambda_k: 1 Loss: 0.1327682414983914\n",
      "Iteration: 1194 lambda_k: 1 Loss: 0.13264987858227276\n",
      "Iteration: 1195 lambda_k: 1 Loss: 0.1325316286242218\n",
      "Iteration: 1196 lambda_k: 1 Loss: 0.13241348857132595\n",
      "Iteration: 1197 lambda_k: 1 Loss: 0.1322954566873403\n",
      "Iteration: 1198 lambda_k: 1 Loss: 0.13217753176408567\n",
      "Iteration: 1199 lambda_k: 1 Loss: 0.13205971261555827\n",
      "Iteration: 1200 lambda_k: 1 Loss: 0.13194199788841104\n",
      "Iteration: 1201 lambda_k: 1 Loss: 0.13182438611834488\n",
      "Iteration: 1202 lambda_k: 1 Loss: 0.13170687584191101\n",
      "Iteration: 1203 lambda_k: 1 Loss: 0.13158946566349244\n",
      "Iteration: 1204 lambda_k: 1 Loss: 0.13147215426569564\n",
      "Iteration: 1205 lambda_k: 1 Loss: 0.13135494039158593\n",
      "Iteration: 1206 lambda_k: 1 Loss: 0.13123782282528704\n",
      "Iteration: 1207 lambda_k: 1 Loss: 0.13112080038175494\n",
      "Iteration: 1208 lambda_k: 1 Loss: 0.13100387190472562\n",
      "Iteration: 1209 lambda_k: 1 Loss: 0.13088703626802153\n",
      "Iteration: 1210 lambda_k: 1 Loss: 0.1307702923766791\n",
      "Iteration: 1211 lambda_k: 1 Loss: 0.13065363916637623\n",
      "Iteration: 1212 lambda_k: 1 Loss: 0.1305370756013366\n",
      "Iteration: 1213 lambda_k: 1 Loss: 0.13042060066122815\n",
      "Iteration: 1214 lambda_k: 1 Loss: 0.13030421343133436\n",
      "Iteration: 1215 lambda_k: 1 Loss: 0.1301879123291914\n",
      "Iteration: 1216 lambda_k: 1 Loss: 0.13006634720991786\n",
      "Iteration: 1217 lambda_k: 1 Loss: 0.1299471416543063\n",
      "Iteration: 1218 lambda_k: 1 Loss: 0.12983133889319762\n",
      "Iteration: 1219 lambda_k: 1 Loss: 0.12971682011147642\n",
      "Iteration: 1220 lambda_k: 1 Loss: 0.1296025085625385\n",
      "Iteration: 1221 lambda_k: 1 Loss: 0.12948829792431502\n",
      "Iteration: 1222 lambda_k: 1 Loss: 0.12937445857811566\n",
      "Iteration: 1223 lambda_k: 1 Loss: 0.1292611674726609\n",
      "Iteration: 1224 lambda_k: 1 Loss: 0.12914839044142126\n",
      "Iteration: 1225 lambda_k: 1 Loss: 0.12903597177302173\n",
      "Iteration: 1226 lambda_k: 1 Loss: 0.12892375047854226\n",
      "Iteration: 1227 lambda_k: 1 Loss: 0.1288116196526175\n",
      "Iteration: 1228 lambda_k: 1 Loss: 0.12869952692872014\n",
      "Iteration: 1229 lambda_k: 1 Loss: 0.12858744911449052\n",
      "Iteration: 1230 lambda_k: 1 Loss: 0.12847537019916927\n",
      "Iteration: 1231 lambda_k: 1 Loss: 0.12836327281810334\n",
      "Iteration: 1232 lambda_k: 1 Loss: 0.1282511393450032\n",
      "Iteration: 1233 lambda_k: 1 Loss: 0.12813895549323256\n",
      "Iteration: 1234 lambda_k: 1 Loss: 0.12802671221788908\n",
      "Iteration: 1235 lambda_k: 1 Loss: 0.12791440533680226\n",
      "Iteration: 1236 lambda_k: 1 Loss: 0.12780203405663346\n",
      "Iteration: 1237 lambda_k: 1 Loss: 0.12768959958268244\n",
      "Iteration: 1238 lambda_k: 1 Loss: 0.12757710428899294\n",
      "Iteration: 1239 lambda_k: 1 Loss: 0.12746455136379867\n",
      "Iteration: 1240 lambda_k: 1 Loss: 0.12735194466739871\n",
      "Iteration: 1241 lambda_k: 1 Loss: 0.12723928861466127\n",
      "Iteration: 1242 lambda_k: 1 Loss: 0.12712658802465865\n",
      "Iteration: 1243 lambda_k: 1 Loss: 0.12701384795743006\n",
      "Iteration: 1244 lambda_k: 1 Loss: 0.12690107357166563\n",
      "Iteration: 1245 lambda_k: 1 Loss: 0.12678827001957102\n",
      "Iteration: 1246 lambda_k: 1 Loss: 0.12667544237638403\n",
      "Iteration: 1247 lambda_k: 1 Loss: 0.12656259559392388\n",
      "Iteration: 1248 lambda_k: 1 Loss: 0.12644973446848967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1249 lambda_k: 1 Loss: 0.12633686361758206\n",
      "Iteration: 1250 lambda_k: 1 Loss: 0.12622398746326885\n",
      "Iteration: 1251 lambda_k: 1 Loss: 0.12611111022144914\n",
      "Iteration: 1252 lambda_k: 1 Loss: 0.12599823589640477\n",
      "Iteration: 1253 lambda_k: 1 Loss: 0.12588536827976085\n",
      "Iteration: 1254 lambda_k: 1 Loss: 0.12577251095283767\n",
      "Iteration: 1255 lambda_k: 1 Loss: 0.12565966729145508\n",
      "Iteration: 1256 lambda_k: 1 Loss: 0.12554684047245268\n",
      "Iteration: 1257 lambda_k: 1 Loss: 0.125434033481396\n",
      "Iteration: 1258 lambda_k: 1 Loss: 0.12532124912108913\n",
      "Iteration: 1259 lambda_k: 1 Loss: 0.12520849002061063\n",
      "Iteration: 1260 lambda_k: 1 Loss: 0.12509575864464953\n",
      "Iteration: 1261 lambda_k: 1 Loss: 0.12498305730295853\n",
      "Iteration: 1262 lambda_k: 1 Loss: 0.1248703881597783\n",
      "Iteration: 1263 lambda_k: 1 Loss: 0.1247577532431196\n",
      "Iteration: 1264 lambda_k: 1 Loss: 0.1246451544538182\n",
      "Iteration: 1265 lambda_k: 1 Loss: 0.12453259357020581\n",
      "Iteration: 1266 lambda_k: 1 Loss: 0.12442007227145772\n",
      "Iteration: 1267 lambda_k: 1 Loss: 0.12430759212528752\n",
      "Iteration: 1268 lambda_k: 1 Loss: 0.12419515460797576\n",
      "Iteration: 1269 lambda_k: 1 Loss: 0.12408276111058036\n",
      "Iteration: 1270 lambda_k: 1 Loss: 0.12397041294028727\n",
      "Iteration: 1271 lambda_k: 1 Loss: 0.123858111333901\n",
      "Iteration: 1272 lambda_k: 1 Loss: 0.12374585746039007\n",
      "Iteration: 1273 lambda_k: 1 Loss: 0.12363365242738263\n",
      "Iteration: 1274 lambda_k: 1 Loss: 0.1235214972866694\n",
      "Iteration: 1275 lambda_k: 1 Loss: 0.12340939303940518\n",
      "Iteration: 1276 lambda_k: 1 Loss: 0.1232973406410946\n",
      "Iteration: 1277 lambda_k: 1 Loss: 0.12318534100709023\n",
      "Iteration: 1278 lambda_k: 1 Loss: 0.12307339500711809\n",
      "Iteration: 1279 lambda_k: 1 Loss: 0.12296150349728706\n",
      "Iteration: 1280 lambda_k: 1 Loss: 0.12284966729467355\n",
      "Iteration: 1281 lambda_k: 1 Loss: 0.122738620821156\n",
      "Iteration: 1282 lambda_k: 1 Loss: 0.12262949470035875\n",
      "Iteration: 1283 lambda_k: 1 Loss: 0.12252043289687434\n",
      "Iteration: 1284 lambda_k: 1 Loss: 0.12241143535173317\n",
      "Iteration: 1285 lambda_k: 1 Loss: 0.12230250203159351\n",
      "Iteration: 1286 lambda_k: 1 Loss: 0.12219363288833035\n",
      "Iteration: 1287 lambda_k: 1 Loss: 0.12208482786047821\n",
      "Iteration: 1288 lambda_k: 1 Loss: 0.12197608687454081\n",
      "Iteration: 1289 lambda_k: 1 Loss: 0.12186740984619317\n",
      "Iteration: 1290 lambda_k: 1 Loss: 0.12175879668138763\n",
      "Iteration: 1291 lambda_k: 1 Loss: 0.12165024727737016\n",
      "Iteration: 1292 lambda_k: 1 Loss: 0.1215417615236104\n",
      "Iteration: 1293 lambda_k: 1 Loss: 0.12143333930265209\n",
      "Iteration: 1294 lambda_k: 1 Loss: 0.12132498049088969\n",
      "Iteration: 1295 lambda_k: 1 Loss: 0.12121668495927787\n",
      "Iteration: 1296 lambda_k: 1 Loss: 0.1211084525739787\n",
      "Iteration: 1297 lambda_k: 1 Loss: 0.12100028320793492\n",
      "Iteration: 1298 lambda_k: 1 Loss: 0.12089217669755384\n",
      "Iteration: 1299 lambda_k: 1 Loss: 0.12078413290884754\n",
      "Iteration: 1300 lambda_k: 1 Loss: 0.12067615169423487\n",
      "Iteration: 1301 lambda_k: 1 Loss: 0.12056823290381091\n",
      "Iteration: 1302 lambda_k: 1 Loss: 0.12046037638571147\n",
      "Iteration: 1303 lambda_k: 1 Loss: 0.12035258198645538\n",
      "Iteration: 1304 lambda_k: 1 Loss: 0.12024484955125407\n",
      "Iteration: 1305 lambda_k: 1 Loss: 0.12013717892428827\n",
      "Iteration: 1306 lambda_k: 1 Loss: 0.1200295699489563\n",
      "Iteration: 1307 lambda_k: 1 Loss: 0.11992202246809933\n",
      "Iteration: 1308 lambda_k: 1 Loss: 0.11981453632420495\n",
      "Iteration: 1309 lambda_k: 1 Loss: 0.1197071113595919\n",
      "Iteration: 1310 lambda_k: 1 Loss: 0.1195997474165764\n",
      "Iteration: 1311 lambda_k: 1 Loss: 0.11949244433762185\n",
      "Iteration: 1312 lambda_k: 1 Loss: 0.11938520196547303\n",
      "Iteration: 1313 lambda_k: 1 Loss: 0.11927802014327643\n",
      "Iteration: 1314 lambda_k: 1 Loss: 0.11917089871468821\n",
      "Iteration: 1315 lambda_k: 1 Loss: 0.11906383752397025\n",
      "Iteration: 1316 lambda_k: 1 Loss: 0.1189568364160761\n",
      "Iteration: 1317 lambda_k: 1 Loss: 0.11884989523672713\n",
      "Iteration: 1318 lambda_k: 1 Loss: 0.11874301383248047\n",
      "Iteration: 1319 lambda_k: 1 Loss: 0.11863619205078822\n",
      "Iteration: 1320 lambda_k: 1 Loss: 0.11852942974005039\n",
      "Iteration: 1321 lambda_k: 1 Loss: 0.11842272674966056\n",
      "Iteration: 1322 lambda_k: 1 Loss: 0.11831608293004629\n",
      "Iteration: 1323 lambda_k: 1 Loss: 0.11820949813270332\n",
      "Iteration: 1324 lambda_k: 1 Loss: 0.11810297221022587\n",
      "Iteration: 1325 lambda_k: 1 Loss: 0.11799650501633172\n",
      "Iteration: 1326 lambda_k: 1 Loss: 0.11789009640588378\n",
      "Iteration: 1327 lambda_k: 1 Loss: 0.11778374623490781\n",
      "Iteration: 1328 lambda_k: 1 Loss: 0.11767745436060702\n",
      "Iteration: 1329 lambda_k: 1 Loss: 0.11757122064137357\n",
      "Iteration: 1330 lambda_k: 1 Loss: 0.11746504493679807\n",
      "Iteration: 1331 lambda_k: 1 Loss: 0.1173589271076746\n",
      "Iteration: 1332 lambda_k: 1 Loss: 0.11725286700827103\n",
      "Iteration: 1333 lambda_k: 1 Loss: 0.11714686451705801\n",
      "Iteration: 1334 lambda_k: 1 Loss: 0.11704091949097435\n",
      "Iteration: 1335 lambda_k: 1 Loss: 0.1169350317956412\n",
      "Iteration: 1336 lambda_k: 1 Loss: 0.1168292012978943\n",
      "Iteration: 1337 lambda_k: 1 Loss: 0.1167234278657851\n",
      "Iteration: 1338 lambda_k: 1 Loss: 0.11661771136856959\n",
      "Iteration: 1339 lambda_k: 1 Loss: 0.11651205167669786\n",
      "Iteration: 1340 lambda_k: 1 Loss: 0.11640644866180704\n",
      "Iteration: 1341 lambda_k: 1 Loss: 0.11630090219671535\n",
      "Iteration: 1342 lambda_k: 1 Loss: 0.1161954121554153\n",
      "Iteration: 1343 lambda_k: 1 Loss: 0.11608997841306565\n",
      "Iteration: 1344 lambda_k: 1 Loss: 0.11598460084598206\n",
      "Iteration: 1345 lambda_k: 1 Loss: 0.11587927933162749\n",
      "Iteration: 1346 lambda_k: 1 Loss: 0.11577401374860219\n",
      "Iteration: 1347 lambda_k: 1 Loss: 0.11566880397663369\n",
      "Iteration: 1348 lambda_k: 1 Loss: 0.11556364989656652\n",
      "Iteration: 1349 lambda_k: 1 Loss: 0.1154585513903515\n",
      "Iteration: 1350 lambda_k: 1 Loss: 0.11535350834103511\n",
      "Iteration: 1351 lambda_k: 1 Loss: 0.11524852063274843\n",
      "Iteration: 1352 lambda_k: 1 Loss: 0.11514358815069603\n",
      "Iteration: 1353 lambda_k: 1 Loss: 0.11503871078114486\n",
      "Iteration: 1354 lambda_k: 1 Loss: 0.1149338884114129\n",
      "Iteration: 1355 lambda_k: 1 Loss: 0.11482912092985804\n",
      "Iteration: 1356 lambda_k: 1 Loss: 0.11472440822586667\n",
      "Iteration: 1357 lambda_k: 1 Loss: 0.11461975018984229\n",
      "Iteration: 1358 lambda_k: 1 Loss: 0.11451514671319438\n",
      "Iteration: 1359 lambda_k: 1 Loss: 0.11441059768832709\n",
      "Iteration: 1360 lambda_k: 1 Loss: 0.11430610300862801\n",
      "Iteration: 1361 lambda_k: 1 Loss: 0.1142016625684569\n",
      "Iteration: 1362 lambda_k: 1 Loss: 0.11409727626313483\n",
      "Iteration: 1363 lambda_k: 1 Loss: 0.11399294398893281\n",
      "Iteration: 1364 lambda_k: 1 Loss: 0.11388866564306119\n",
      "Iteration: 1365 lambda_k: 1 Loss: 0.1137844411236585\n",
      "Iteration: 1366 lambda_k: 1 Loss: 0.11368027032978081\n",
      "Iteration: 1367 lambda_k: 1 Loss: 0.11357615319837269\n",
      "Iteration: 1368 lambda_k: 1 Loss: 0.11347208955650108\n",
      "Iteration: 1369 lambda_k: 1 Loss: 0.11336807934269383\n",
      "Iteration: 1370 lambda_k: 1 Loss: 0.11326412245958431\n",
      "Iteration: 1371 lambda_k: 1 Loss: 0.11316021881066915\n",
      "Iteration: 1372 lambda_k: 1 Loss: 0.11305636830029017\n",
      "Iteration: 1373 lambda_k: 1 Loss: 0.11295257083363094\n",
      "Iteration: 1374 lambda_k: 1 Loss: 0.11284882631671143\n",
      "Iteration: 1375 lambda_k: 1 Loss: 0.11274513465637877\n",
      "Iteration: 1376 lambda_k: 1 Loss: 0.11264149576029645\n",
      "Iteration: 1377 lambda_k: 1 Loss: 0.1125379095369338\n",
      "Iteration: 1378 lambda_k: 1 Loss: 0.11243437589555633\n",
      "Iteration: 1379 lambda_k: 1 Loss: 0.1123308947462166\n",
      "Iteration: 1380 lambda_k: 1 Loss: 0.11222746599974537\n",
      "Iteration: 1381 lambda_k: 1 Loss: 0.11212408956774249\n",
      "Iteration: 1382 lambda_k: 1 Loss: 0.11202076536256818\n",
      "Iteration: 1383 lambda_k: 1 Loss: 0.1119174932973339\n",
      "Iteration: 1384 lambda_k: 1 Loss: 0.1118142732858937\n",
      "Iteration: 1385 lambda_k: 1 Loss: 0.11171110524283548\n",
      "Iteration: 1386 lambda_k: 1 Loss: 0.11160798908347244\n",
      "Iteration: 1387 lambda_k: 1 Loss: 0.11150492472383494\n",
      "Iteration: 1388 lambda_k: 1 Loss: 0.11140191208066193\n",
      "Iteration: 1389 lambda_k: 1 Loss: 0.11129895107139288\n",
      "Iteration: 1390 lambda_k: 1 Loss: 0.11119604161415965\n",
      "Iteration: 1391 lambda_k: 1 Loss: 0.11109318362777859\n",
      "Iteration: 1392 lambda_k: 1 Loss: 0.11099037703174257\n",
      "Iteration: 1393 lambda_k: 1 Loss: 0.11088762174621325\n",
      "Iteration: 1394 lambda_k: 1 Loss: 0.11078491769201336\n",
      "Iteration: 1395 lambda_k: 1 Loss: 0.11068226479061918\n",
      "Iteration: 1396 lambda_k: 1 Loss: 0.11057966296415311\n",
      "Iteration: 1397 lambda_k: 1 Loss: 0.11047711213537609\n",
      "Iteration: 1398 lambda_k: 1 Loss: 0.11037461222768054\n",
      "Iteration: 1399 lambda_k: 1 Loss: 0.11027216316508304\n",
      "Iteration: 1400 lambda_k: 1 Loss: 0.1101697648722172\n",
      "Iteration: 1401 lambda_k: 1 Loss: 0.1100674172743269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1402 lambda_k: 1 Loss: 0.1099651202972589\n",
      "Iteration: 1403 lambda_k: 1 Loss: 0.10986287386745647\n",
      "Iteration: 1404 lambda_k: 1 Loss: 0.10976067791195247\n",
      "Iteration: 1405 lambda_k: 1 Loss: 0.1096585323583626\n",
      "Iteration: 1406 lambda_k: 1 Loss: 0.10955643713487898\n",
      "Iteration: 1407 lambda_k: 1 Loss: 0.10945439217026351\n",
      "Iteration: 1408 lambda_k: 1 Loss: 0.1093523973938416\n",
      "Iteration: 1409 lambda_k: 1 Loss: 0.10925045273549576\n",
      "Iteration: 1410 lambda_k: 1 Loss: 0.1091485581256592\n",
      "Iteration: 1411 lambda_k: 1 Loss: 0.1090467134953101\n",
      "Iteration: 1412 lambda_k: 1 Loss: 0.10894491877596503\n",
      "Iteration: 1413 lambda_k: 1 Loss: 0.1088431738996732\n",
      "Iteration: 1414 lambda_k: 1 Loss: 0.10874147879901049\n",
      "Iteration: 1415 lambda_k: 1 Loss: 0.10863983340707341\n",
      "Iteration: 1416 lambda_k: 1 Loss: 0.10853823765747352\n",
      "Iteration: 1417 lambda_k: 1 Loss: 0.10843669148433153\n",
      "Iteration: 1418 lambda_k: 1 Loss: 0.10833519482227182\n",
      "Iteration: 1419 lambda_k: 1 Loss: 0.10823374760641666\n",
      "Iteration: 1420 lambda_k: 1 Loss: 0.10813234977238079\n",
      "Iteration: 1421 lambda_k: 1 Loss: 0.10803100125626594\n",
      "Iteration: 1422 lambda_k: 1 Loss: 0.10792970199465549\n",
      "Iteration: 1423 lambda_k: 1 Loss: 0.1078284519246091\n",
      "Iteration: 1424 lambda_k: 1 Loss: 0.10772725098365744\n",
      "Iteration: 1425 lambda_k: 1 Loss: 0.1076260991097971\n",
      "Iteration: 1426 lambda_k: 1 Loss: 0.10752499624148533\n",
      "Iteration: 1427 lambda_k: 1 Loss: 0.10742394231763498\n",
      "Iteration: 1428 lambda_k: 1 Loss: 0.1073229372776096\n",
      "Iteration: 1429 lambda_k: 1 Loss: 0.10722198106121836\n",
      "Iteration: 1430 lambda_k: 1 Loss: 0.10712107360871126\n",
      "Iteration: 1431 lambda_k: 1 Loss: 0.10702021486077433\n",
      "Iteration: 1432 lambda_k: 1 Loss: 0.1069194047585246\n",
      "Iteration: 1433 lambda_k: 1 Loss: 0.1068186432435058\n",
      "Iteration: 1434 lambda_k: 1 Loss: 0.10671793025768334\n",
      "Iteration: 1435 lambda_k: 1 Loss: 0.10661726574343981\n",
      "Iteration: 1436 lambda_k: 1 Loss: 0.1065166496435706\n",
      "Iteration: 1437 lambda_k: 1 Loss: 0.10641608190127916\n",
      "Iteration: 1438 lambda_k: 1 Loss: 0.10631556246017265\n",
      "Iteration: 1439 lambda_k: 1 Loss: 0.10621509126425754\n",
      "Iteration: 1440 lambda_k: 1 Loss: 0.10611466825793538\n",
      "Iteration: 1441 lambda_k: 1 Loss: 0.10601429338599823\n",
      "Iteration: 1442 lambda_k: 1 Loss: 0.10591396659362481\n",
      "Iteration: 1443 lambda_k: 1 Loss: 0.10581368782637607\n",
      "Iteration: 1444 lambda_k: 1 Loss: 0.10571345703019087\n",
      "Iteration: 1445 lambda_k: 1 Loss: 0.10561327415138237\n",
      "Iteration: 1446 lambda_k: 1 Loss: 0.10551313913663361\n",
      "Iteration: 1447 lambda_k: 1 Loss: 0.10541305193299362\n",
      "Iteration: 1448 lambda_k: 1 Loss: 0.10531301248787348\n",
      "Iteration: 1449 lambda_k: 1 Loss: 0.10521302074904235\n",
      "Iteration: 1450 lambda_k: 1 Loss: 0.10511307666462374\n",
      "Iteration: 1451 lambda_k: 1 Loss: 0.10501318018309169\n",
      "Iteration: 1452 lambda_k: 1 Loss: 0.10491333125326668\n",
      "Iteration: 1453 lambda_k: 1 Loss: 0.10481352982431243\n",
      "Iteration: 1454 lambda_k: 1 Loss: 0.10471377584573173\n",
      "Iteration: 1455 lambda_k: 1 Loss: 0.10461406926736316\n",
      "Iteration: 1456 lambda_k: 1 Loss: 0.10451441003937734\n",
      "Iteration: 1457 lambda_k: 1 Loss: 0.10441479811227318\n",
      "Iteration: 1458 lambda_k: 1 Loss: 0.10431523343687474\n",
      "Iteration: 1459 lambda_k: 1 Loss: 0.10421571596432745\n",
      "Iteration: 1460 lambda_k: 1 Loss: 0.10411624564609487\n",
      "Iteration: 1461 lambda_k: 1 Loss: 0.1040168224339552\n",
      "Iteration: 1462 lambda_k: 1 Loss: 0.10391744627999791\n",
      "Iteration: 1463 lambda_k: 1 Loss: 0.10381811713662038\n",
      "Iteration: 1464 lambda_k: 1 Loss: 0.10371883495652484\n",
      "Iteration: 1465 lambda_k: 1 Loss: 0.10361959969271477\n",
      "Iteration: 1466 lambda_k: 1 Loss: 0.10352041129849206\n",
      "Iteration: 1467 lambda_k: 1 Loss: 0.10342126972745352\n",
      "Iteration: 1468 lambda_k: 1 Loss: 0.10332217493348796\n",
      "Iteration: 1469 lambda_k: 1 Loss: 0.10322312687077302\n",
      "Iteration: 1470 lambda_k: 1 Loss: 0.10312412549377212\n",
      "Iteration: 1471 lambda_k: 1 Loss: 0.10302517075723137\n",
      "Iteration: 1472 lambda_k: 1 Loss: 0.10292626261617667\n",
      "Iteration: 1473 lambda_k: 1 Loss: 0.10282740102591068\n",
      "Iteration: 1474 lambda_k: 1 Loss: 0.10272858594200986\n",
      "Iteration: 1475 lambda_k: 1 Loss: 0.10262981732032173\n",
      "Iteration: 1476 lambda_k: 1 Loss: 0.10253109511696173\n",
      "Iteration: 1477 lambda_k: 1 Loss: 0.10243241928831076\n",
      "Iteration: 1478 lambda_k: 1 Loss: 0.10233378979101215\n",
      "Iteration: 1479 lambda_k: 1 Loss: 0.10223520658196886\n",
      "Iteration: 1480 lambda_k: 1 Loss: 0.10213666961834084\n",
      "Iteration: 1481 lambda_k: 1 Loss: 0.1020381788575423\n",
      "Iteration: 1482 lambda_k: 1 Loss: 0.10193973425723912\n",
      "Iteration: 1483 lambda_k: 1 Loss: 0.10184133577534603\n",
      "Iteration: 1484 lambda_k: 1 Loss: 0.1017429833700241\n",
      "Iteration: 1485 lambda_k: 1 Loss: 0.10164467699967827\n",
      "Iteration: 1486 lambda_k: 1 Loss: 0.10154641662295452\n",
      "Iteration: 1487 lambda_k: 1 Loss: 0.10144820219873757\n",
      "Iteration: 1488 lambda_k: 1 Loss: 0.10135003367114816\n",
      "Iteration: 1489 lambda_k: 1 Loss: 0.10125191102946958\n",
      "Iteration: 1490 lambda_k: 1 Loss: 0.10115383421836036\n",
      "Iteration: 1491 lambda_k: 1 Loss: 0.10105580319763509\n",
      "Iteration: 1492 lambda_k: 1 Loss: 0.10095781792733495\n",
      "Iteration: 1493 lambda_k: 1 Loss: 0.10085987836772513\n",
      "Iteration: 1494 lambda_k: 1 Loss: 0.10076198447929263\n",
      "Iteration: 1495 lambda_k: 1 Loss: 0.10066413622274391\n",
      "Iteration: 1496 lambda_k: 1 Loss: 0.10056633355900253\n",
      "Iteration: 1497 lambda_k: 1 Loss: 0.10046857644920706\n",
      "Iteration: 1498 lambda_k: 1 Loss: 0.10037086485470871\n",
      "Iteration: 1499 lambda_k: 1 Loss: 0.10027319873706918\n",
      "Iteration: 1500 lambda_k: 1 Loss: 0.10017557805805839\n",
      "Iteration: 1501 lambda_k: 1 Loss: 0.10007800277965241\n",
      "Iteration: 1502 lambda_k: 1 Loss: 0.09998047286403144\n",
      "Iteration: 1503 lambda_k: 1 Loss: 0.09988298827357736\n",
      "Iteration: 1504 lambda_k: 1 Loss: 0.099785548970872\n",
      "Iteration: 1505 lambda_k: 1 Loss: 0.09968815491869484\n",
      "Iteration: 1506 lambda_k: 1 Loss: 0.09959080608002116\n",
      "Iteration: 1507 lambda_k: 1 Loss: 0.09949350241801981\n",
      "Iteration: 1508 lambda_k: 1 Loss: 0.09939624389605133\n",
      "Iteration: 1509 lambda_k: 1 Loss: 0.09929903047766599\n",
      "Iteration: 1510 lambda_k: 1 Loss: 0.0992018621266018\n",
      "Iteration: 1511 lambda_k: 1 Loss: 0.09910473880678261\n",
      "Iteration: 1512 lambda_k: 1 Loss: 0.09900766048231623\n",
      "Iteration: 1513 lambda_k: 1 Loss: 0.09891062711749234\n",
      "Iteration: 1514 lambda_k: 1 Loss: 0.09881363867678092\n",
      "Iteration: 1515 lambda_k: 1 Loss: 0.09871669512483025\n",
      "Iteration: 1516 lambda_k: 1 Loss: 0.09861979642646505\n",
      "Iteration: 1517 lambda_k: 1 Loss: 0.09852294254668476\n",
      "Iteration: 1518 lambda_k: 1 Loss: 0.09842613345066165\n",
      "Iteration: 1519 lambda_k: 1 Loss: 0.09832936910373911\n",
      "Iteration: 1520 lambda_k: 1 Loss: 0.09823264947143005\n",
      "Iteration: 1521 lambda_k: 1 Loss: 0.09813597451941489\n",
      "Iteration: 1522 lambda_k: 1 Loss: 0.09803934421354005\n",
      "Iteration: 1523 lambda_k: 1 Loss: 0.09794275851981626\n",
      "Iteration: 1524 lambda_k: 1 Loss: 0.09784621740441668\n",
      "Iteration: 1525 lambda_k: 1 Loss: 0.09774972083367554\n",
      "Iteration: 1526 lambda_k: 1 Loss: 0.09765326877408628\n",
      "Iteration: 1527 lambda_k: 1 Loss: 0.09755686119230013\n",
      "Iteration: 1528 lambda_k: 1 Loss: 0.09746049805512431\n",
      "Iteration: 1529 lambda_k: 1 Loss: 0.09736417932952052\n",
      "Iteration: 1530 lambda_k: 1 Loss: 0.09726790498260345\n",
      "Iteration: 1531 lambda_k: 1 Loss: 0.09717167498163921\n",
      "Iteration: 1532 lambda_k: 1 Loss: 0.09707548929404364\n",
      "Iteration: 1533 lambda_k: 1 Loss: 0.0969793478873811\n",
      "Iteration: 1534 lambda_k: 1 Loss: 0.09688325072936262\n",
      "Iteration: 1535 lambda_k: 1 Loss: 0.09678719778784475\n",
      "Iteration: 1536 lambda_k: 1 Loss: 0.09669118903082784\n",
      "Iteration: 1537 lambda_k: 1 Loss: 0.0965952244264548\n",
      "Iteration: 1538 lambda_k: 1 Loss: 0.09649930394300943\n",
      "Iteration: 1539 lambda_k: 1 Loss: 0.09640342754891526\n",
      "Iteration: 1540 lambda_k: 1 Loss: 0.09630759521273394\n",
      "Iteration: 1541 lambda_k: 1 Loss: 0.09621180690316403\n",
      "Iteration: 1542 lambda_k: 1 Loss: 0.09611606258903944\n",
      "Iteration: 1543 lambda_k: 1 Loss: 0.0960203622393283\n",
      "Iteration: 1544 lambda_k: 1 Loss: 0.09592470582313133\n",
      "Iteration: 1545 lambda_k: 1 Loss: 0.09582909330968084\n",
      "Iteration: 1546 lambda_k: 1 Loss: 0.09573352466833913\n",
      "Iteration: 1547 lambda_k: 1 Loss: 0.09563799986859743\n",
      "Iteration: 1548 lambda_k: 1 Loss: 0.09554251888007442\n",
      "Iteration: 1549 lambda_k: 1 Loss: 0.09544708167251506\n",
      "Iteration: 1550 lambda_k: 1 Loss: 0.09535168821578935\n",
      "Iteration: 1551 lambda_k: 1 Loss: 0.09525633847989108\n",
      "Iteration: 1552 lambda_k: 1 Loss: 0.09516103243493647\n",
      "Iteration: 1553 lambda_k: 1 Loss: 0.09506577005116318\n",
      "Iteration: 1554 lambda_k: 1 Loss: 0.09497055129892898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1555 lambda_k: 1 Loss: 0.09487537614871051\n",
      "Iteration: 1556 lambda_k: 1 Loss: 0.09478024457110225\n",
      "Iteration: 1557 lambda_k: 1 Loss: 0.09468515653681514\n",
      "Iteration: 1558 lambda_k: 1 Loss: 0.0945901120166757\n",
      "Iteration: 1559 lambda_k: 1 Loss: 0.09449511098162476\n",
      "Iteration: 1560 lambda_k: 1 Loss: 0.09440015340271622\n",
      "Iteration: 1561 lambda_k: 1 Loss: 0.09430523925111611\n",
      "Iteration: 1562 lambda_k: 1 Loss: 0.09421036849810144\n",
      "Iteration: 1563 lambda_k: 1 Loss: 0.09411554111505904\n",
      "Iteration: 1564 lambda_k: 1 Loss: 0.0940207570734846\n",
      "Iteration: 1565 lambda_k: 1 Loss: 0.09392601634498156\n",
      "Iteration: 1566 lambda_k: 1 Loss: 0.09383131890125988\n",
      "Iteration: 1567 lambda_k: 1 Loss: 0.09373666471413532\n",
      "Iteration: 1568 lambda_k: 1 Loss: 0.09364205375552814\n",
      "Iteration: 1569 lambda_k: 1 Loss: 0.09354748599746228\n",
      "Iteration: 1570 lambda_k: 1 Loss: 0.09345296141206413\n",
      "Iteration: 1571 lambda_k: 1 Loss: 0.09335847997156177\n",
      "Iteration: 1572 lambda_k: 1 Loss: 0.09326404164828372\n",
      "Iteration: 1573 lambda_k: 1 Loss: 0.0931696464146583\n",
      "Iteration: 1574 lambda_k: 1 Loss: 0.09307529424321222\n",
      "Iteration: 1575 lambda_k: 1 Loss: 0.09298098510027825\n",
      "Iteration: 1576 lambda_k: 1 Loss: 0.09288671897091416\n",
      "Iteration: 1577 lambda_k: 1 Loss: 0.0927924958219249\n",
      "Iteration: 1578 lambda_k: 1 Loss: 0.09269831562620633\n",
      "Iteration: 1579 lambda_k: 1 Loss: 0.09260417835675387\n",
      "Iteration: 1580 lambda_k: 1 Loss: 0.09251008398666537\n",
      "Iteration: 1581 lambda_k: 1 Loss: 0.09241603248913403\n",
      "Iteration: 1582 lambda_k: 1 Loss: 0.09232202383744306\n",
      "Iteration: 1583 lambda_k: 1 Loss: 0.09222805800496461\n",
      "Iteration: 1584 lambda_k: 1 Loss: 0.09213413496515985\n",
      "Iteration: 1585 lambda_k: 1 Loss: 0.09204025469157863\n",
      "Iteration: 1586 lambda_k: 1 Loss: 0.09194641715785938\n",
      "Iteration: 1587 lambda_k: 1 Loss: 0.09185262233772755\n",
      "Iteration: 1588 lambda_k: 1 Loss: 0.09175887020499494\n",
      "Iteration: 1589 lambda_k: 1 Loss: 0.0916651607335585\n",
      "Iteration: 1590 lambda_k: 1 Loss: 0.0915714938973997\n",
      "Iteration: 1591 lambda_k: 1 Loss: 0.09147786967058355\n",
      "Iteration: 1592 lambda_k: 1 Loss: 0.09138428802725826\n",
      "Iteration: 1593 lambda_k: 1 Loss: 0.09129074897471119\n",
      "Iteration: 1594 lambda_k: 1 Loss: 0.09119725242145373\n",
      "Iteration: 1595 lambda_k: 1 Loss: 0.09110379837458328\n",
      "Iteration: 1596 lambda_k: 1 Loss: 0.09101038680858958\n",
      "Iteration: 1597 lambda_k: 1 Loss: 0.09091701769803402\n",
      "Iteration: 1598 lambda_k: 1 Loss: 0.09082369101754702\n",
      "Iteration: 1599 lambda_k: 1 Loss: 0.09073040674183357\n",
      "Iteration: 1600 lambda_k: 1 Loss: 0.09063716484567662\n",
      "Iteration: 1601 lambda_k: 1 Loss: 0.0905439653039366\n",
      "Iteration: 1602 lambda_k: 1 Loss: 0.09045080809155005\n",
      "Iteration: 1603 lambda_k: 1 Loss: 0.09035769318352793\n",
      "Iteration: 1604 lambda_k: 1 Loss: 0.09026462055495452\n",
      "Iteration: 1605 lambda_k: 1 Loss: 0.09017159018098736\n",
      "Iteration: 1606 lambda_k: 1 Loss: 0.09007860203685635\n",
      "Iteration: 1607 lambda_k: 1 Loss: 0.08998565609786344\n",
      "Iteration: 1608 lambda_k: 1 Loss: 0.08989275231455167\n",
      "Iteration: 1609 lambda_k: 1 Loss: 0.08979989071197919\n",
      "Iteration: 1610 lambda_k: 1 Loss: 0.08970707124087905\n",
      "Iteration: 1611 lambda_k: 1 Loss: 0.0896142938768341\n",
      "Iteration: 1612 lambda_k: 1 Loss: 0.08952155859549611\n",
      "Iteration: 1613 lambda_k: 1 Loss: 0.0894288653725847\n",
      "Iteration: 1614 lambda_k: 1 Loss: 0.08933621418388704\n",
      "Iteration: 1615 lambda_k: 1 Loss: 0.08924360500525687\n",
      "Iteration: 1616 lambda_k: 1 Loss: 0.08915103781261424\n",
      "Iteration: 1617 lambda_k: 1 Loss: 0.08905851258194464\n",
      "Iteration: 1618 lambda_k: 1 Loss: 0.08896602928929842\n",
      "Iteration: 1619 lambda_k: 1 Loss: 0.08887358791079045\n",
      "Iteration: 1620 lambda_k: 1 Loss: 0.08878118842259922\n",
      "Iteration: 1621 lambda_k: 1 Loss: 0.08868883080096633\n",
      "Iteration: 1622 lambda_k: 1 Loss: 0.08859651502219607\n",
      "Iteration: 1623 lambda_k: 1 Loss: 0.08850424106265467\n",
      "Iteration: 1624 lambda_k: 1 Loss: 0.08841200889876975\n",
      "Iteration: 1625 lambda_k: 1 Loss: 0.08831981850703002\n",
      "Iteration: 1626 lambda_k: 1 Loss: 0.0882276698639843\n",
      "Iteration: 1627 lambda_k: 1 Loss: 0.0881355629462413\n",
      "Iteration: 1628 lambda_k: 1 Loss: 0.088043497730469\n",
      "Iteration: 1629 lambda_k: 1 Loss: 0.08795147419339401\n",
      "Iteration: 1630 lambda_k: 1 Loss: 0.08785949231180115\n",
      "Iteration: 1631 lambda_k: 1 Loss: 0.0877675520625329\n",
      "Iteration: 1632 lambda_k: 1 Loss: 0.08767565342248884\n",
      "Iteration: 1633 lambda_k: 1 Loss: 0.08758379636862525\n",
      "Iteration: 1634 lambda_k: 1 Loss: 0.0874919808779544\n",
      "Iteration: 1635 lambda_k: 1 Loss: 0.08740020692754423\n",
      "Iteration: 1636 lambda_k: 1 Loss: 0.0873084744945178\n",
      "Iteration: 1637 lambda_k: 1 Loss: 0.08721678355605286\n",
      "Iteration: 1638 lambda_k: 1 Loss: 0.08712513408972042\n",
      "Iteration: 1639 lambda_k: 1 Loss: 0.0870335260721253\n",
      "Iteration: 1640 lambda_k: 1 Loss: 0.08694195948094743\n",
      "Iteration: 1641 lambda_k: 1 Loss: 0.08685043429357804\n",
      "Iteration: 1642 lambda_k: 1 Loss: 0.08675895048746081\n",
      "Iteration: 1643 lambda_k: 1 Loss: 0.08666750804009098\n",
      "Iteration: 1644 lambda_k: 1 Loss: 0.08657610692901528\n",
      "Iteration: 1645 lambda_k: 1 Loss: 0.08648474713183119\n",
      "Iteration: 1646 lambda_k: 1 Loss: 0.08639342862618661\n",
      "Iteration: 1647 lambda_k: 1 Loss: 0.08630215138977938\n",
      "Iteration: 1648 lambda_k: 1 Loss: 0.08621091540035698\n",
      "Iteration: 1649 lambda_k: 1 Loss: 0.08611972063571598\n",
      "Iteration: 1650 lambda_k: 1 Loss: 0.08602856707370149\n",
      "Iteration: 1651 lambda_k: 1 Loss: 0.085937454692207\n",
      "Iteration: 1652 lambda_k: 1 Loss: 0.08584638346917375\n",
      "Iteration: 1653 lambda_k: 1 Loss: 0.08575535338259044\n",
      "Iteration: 1654 lambda_k: 1 Loss: 0.08566436441049269\n",
      "Iteration: 1655 lambda_k: 1 Loss: 0.08557341653096291\n",
      "Iteration: 1656 lambda_k: 1 Loss: 0.0854825097221294\n",
      "Iteration: 1657 lambda_k: 1 Loss: 0.08539164396216638\n",
      "Iteration: 1658 lambda_k: 1 Loss: 0.08530081922904688\n",
      "Iteration: 1659 lambda_k: 1 Loss: 0.08521003550153172\n",
      "Iteration: 1660 lambda_k: 1 Loss: 0.08511929275768067\n",
      "Iteration: 1661 lambda_k: 1 Loss: 0.0850285909758473\n",
      "Iteration: 1662 lambda_k: 1 Loss: 0.0849379301344289\n",
      "Iteration: 1663 lambda_k: 1 Loss: 0.0848473102118664\n",
      "Iteration: 1664 lambda_k: 1 Loss: 0.08475673118664369\n",
      "Iteration: 1665 lambda_k: 1 Loss: 0.08466619303728756\n",
      "Iteration: 1666 lambda_k: 1 Loss: 0.0845756957423671\n",
      "Iteration: 1667 lambda_k: 1 Loss: 0.08448523928049331\n",
      "Iteration: 1668 lambda_k: 1 Loss: 0.0843948236303191\n",
      "Iteration: 1669 lambda_k: 1 Loss: 0.08430444877053866\n",
      "Iteration: 1670 lambda_k: 1 Loss: 0.08421411467988693\n",
      "Iteration: 1671 lambda_k: 1 Loss: 0.08412382133713975\n",
      "Iteration: 1672 lambda_k: 1 Loss: 0.08403356872111316\n",
      "Iteration: 1673 lambda_k: 1 Loss: 0.08394335681066295\n",
      "Iteration: 1674 lambda_k: 1 Loss: 0.08385318558468491\n",
      "Iteration: 1675 lambda_k: 1 Loss: 0.08376305502211381\n",
      "Iteration: 1676 lambda_k: 1 Loss: 0.08367296510192347\n",
      "Iteration: 1677 lambda_k: 1 Loss: 0.08358291580312652\n",
      "Iteration: 1678 lambda_k: 1 Loss: 0.08349290710477361\n",
      "Iteration: 1679 lambda_k: 1 Loss: 0.08340293898595359\n",
      "Iteration: 1680 lambda_k: 1 Loss: 0.08331301142579302\n",
      "Iteration: 1681 lambda_k: 1 Loss: 0.08322312440345579\n",
      "Iteration: 1682 lambda_k: 1 Loss: 0.08313327789814282\n",
      "Iteration: 1683 lambda_k: 1 Loss: 0.08304347188909184\n",
      "Iteration: 1684 lambda_k: 1 Loss: 0.0829537063555771\n",
      "Iteration: 1685 lambda_k: 1 Loss: 0.08286398127690882\n",
      "Iteration: 1686 lambda_k: 1 Loss: 0.08277429663243346\n",
      "Iteration: 1687 lambda_k: 1 Loss: 0.08268465240153282\n",
      "Iteration: 1688 lambda_k: 1 Loss: 0.0825950485636241\n",
      "Iteration: 1689 lambda_k: 1 Loss: 0.08250548509815939\n",
      "Iteration: 1690 lambda_k: 1 Loss: 0.08241596198462564\n",
      "Iteration: 1691 lambda_k: 1 Loss: 0.08232647920254417\n",
      "Iteration: 1692 lambda_k: 1 Loss: 0.08223703673147063\n",
      "Iteration: 1693 lambda_k: 1 Loss: 0.08214763455099441\n",
      "Iteration: 1694 lambda_k: 1 Loss: 0.08205827264073864\n",
      "Iteration: 1695 lambda_k: 1 Loss: 0.08196895098035989\n",
      "Iteration: 1696 lambda_k: 1 Loss: 0.08187966954954752\n",
      "Iteration: 1697 lambda_k: 1 Loss: 0.08179042832802412\n",
      "Iteration: 1698 lambda_k: 1 Loss: 0.08170122729554463\n",
      "Iteration: 1699 lambda_k: 1 Loss: 0.08161206643189645\n",
      "Iteration: 1700 lambda_k: 1 Loss: 0.08152294571689893\n",
      "Iteration: 1701 lambda_k: 1 Loss: 0.08143386513040331\n",
      "Iteration: 1702 lambda_k: 1 Loss: 0.0813448246522924\n",
      "Iteration: 1703 lambda_k: 1 Loss: 0.0812558242624803\n",
      "Iteration: 1704 lambda_k: 1 Loss: 0.08116686394091237\n",
      "Iteration: 1705 lambda_k: 1 Loss: 0.08107794366756454\n",
      "Iteration: 1706 lambda_k: 1 Loss: 0.08098906342244355\n",
      "Iteration: 1707 lambda_k: 1 Loss: 0.08090022318558635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1708 lambda_k: 1 Loss: 0.08081142293706016\n",
      "Iteration: 1709 lambda_k: 1 Loss: 0.08072266265696207\n",
      "Iteration: 1710 lambda_k: 1 Loss: 0.08063394232541883\n",
      "Iteration: 1711 lambda_k: 1 Loss: 0.08054526191650924\n",
      "Iteration: 1712 lambda_k: 1 Loss: 0.08045662142232038\n",
      "Iteration: 1713 lambda_k: 1 Loss: 0.08036802081727529\n",
      "Iteration: 1714 lambda_k: 1 Loss: 0.08027946008160103\n",
      "Iteration: 1715 lambda_k: 1 Loss: 0.08019093919555882\n",
      "Iteration: 1716 lambda_k: 1 Loss: 0.08010245813944769\n",
      "Iteration: 1717 lambda_k: 1 Loss: 0.08001401689359781\n",
      "Iteration: 1718 lambda_k: 1 Loss: 0.07992561543836649\n",
      "Iteration: 1719 lambda_k: 1 Loss: 0.07983725375413689\n",
      "Iteration: 1720 lambda_k: 1 Loss: 0.07974893182131937\n",
      "Iteration: 1721 lambda_k: 1 Loss: 0.07966064962035171\n",
      "Iteration: 1722 lambda_k: 1 Loss: 0.07957240713169937\n",
      "Iteration: 1723 lambda_k: 1 Loss: 0.07948420433585492\n",
      "Iteration: 1724 lambda_k: 1 Loss: 0.07939604121333772\n",
      "Iteration: 1725 lambda_k: 1 Loss: 0.0793079177446935\n",
      "Iteration: 1726 lambda_k: 1 Loss: 0.07921983391049447\n",
      "Iteration: 1727 lambda_k: 1 Loss: 0.07913178969133874\n",
      "Iteration: 1728 lambda_k: 1 Loss: 0.07904378506785041\n",
      "Iteration: 1729 lambda_k: 1 Loss: 0.07895582002067952\n",
      "Iteration: 1730 lambda_k: 1 Loss: 0.07886789453050153\n",
      "Iteration: 1731 lambda_k: 1 Loss: 0.07878000857801719\n",
      "Iteration: 1732 lambda_k: 1 Loss: 0.07869216214395282\n",
      "Iteration: 1733 lambda_k: 1 Loss: 0.07860435520905945\n",
      "Iteration: 1734 lambda_k: 1 Loss: 0.07851658775411308\n",
      "Iteration: 1735 lambda_k: 1 Loss: 0.07842885975991436\n",
      "Iteration: 1736 lambda_k: 1 Loss: 0.07834117120728849\n",
      "Iteration: 1737 lambda_k: 1 Loss: 0.07825352207708486\n",
      "Iteration: 1738 lambda_k: 1 Loss: 0.07816591235017716\n",
      "Iteration: 1739 lambda_k: 1 Loss: 0.07807834200746297\n",
      "Iteration: 1740 lambda_k: 1 Loss: 0.07799081102986381\n",
      "Iteration: 1741 lambda_k: 1 Loss: 0.07790331939832464\n",
      "Iteration: 1742 lambda_k: 1 Loss: 0.07781586709381409\n",
      "Iteration: 1743 lambda_k: 1 Loss: 0.07772845409732397\n",
      "Iteration: 1744 lambda_k: 1 Loss: 0.07764108038986929\n",
      "Iteration: 1745 lambda_k: 1 Loss: 0.07755374595248818\n",
      "Iteration: 1746 lambda_k: 1 Loss: 0.07746645076624152\n",
      "Iteration: 1747 lambda_k: 1 Loss: 0.07737919481221282\n",
      "Iteration: 1748 lambda_k: 1 Loss: 0.07729197807150817\n",
      "Iteration: 1749 lambda_k: 1 Loss: 0.077204800525256\n",
      "Iteration: 1750 lambda_k: 1 Loss: 0.07711766215460703\n",
      "Iteration: 1751 lambda_k: 1 Loss: 0.0770305629407341\n",
      "Iteration: 1752 lambda_k: 1 Loss: 0.07694350286483188\n",
      "Iteration: 1753 lambda_k: 1 Loss: 0.07685648190811674\n",
      "Iteration: 1754 lambda_k: 1 Loss: 0.07676950005182691\n",
      "Iteration: 1755 lambda_k: 1 Loss: 0.07668255727722192\n",
      "Iteration: 1756 lambda_k: 1 Loss: 0.07659565356558279\n",
      "Iteration: 1757 lambda_k: 1 Loss: 0.07650878889821162\n",
      "Iteration: 1758 lambda_k: 1 Loss: 0.07642196325643164\n",
      "Iteration: 1759 lambda_k: 1 Loss: 0.07633517662158706\n",
      "Iteration: 1760 lambda_k: 1 Loss: 0.07624842897504279\n",
      "Iteration: 1761 lambda_k: 1 Loss: 0.07616172029818441\n",
      "Iteration: 1762 lambda_k: 1 Loss: 0.07607505057858927\n",
      "Iteration: 1763 lambda_k: 1 Loss: 0.0759884197855977\n",
      "Iteration: 1764 lambda_k: 1 Loss: 0.07590182790653817\n",
      "Iteration: 1765 lambda_k: 1 Loss: 0.07581527492289374\n",
      "Iteration: 1766 lambda_k: 1 Loss: 0.07572876081616199\n",
      "Iteration: 1767 lambda_k: 1 Loss: 0.07564228556785088\n",
      "Iteration: 1768 lambda_k: 1 Loss: 0.07555584915948509\n",
      "Iteration: 1769 lambda_k: 1 Loss: 0.07546945157261002\n",
      "Iteration: 1770 lambda_k: 1 Loss: 0.07538309278879193\n",
      "Iteration: 1771 lambda_k: 1 Loss: 0.07529677278961715\n",
      "Iteration: 1772 lambda_k: 1 Loss: 0.07521049155669086\n",
      "Iteration: 1773 lambda_k: 1 Loss: 0.07512424907163705\n",
      "Iteration: 1774 lambda_k: 1 Loss: 0.07503804531609833\n",
      "Iteration: 1775 lambda_k: 1 Loss: 0.07495188027173617\n",
      "Iteration: 1776 lambda_k: 1 Loss: 0.07486575392023088\n",
      "Iteration: 1777 lambda_k: 1 Loss: 0.07477966624328164\n",
      "Iteration: 1778 lambda_k: 1 Loss: 0.07469361722260597\n",
      "Iteration: 1779 lambda_k: 1 Loss: 0.0746076068399401\n",
      "Iteration: 1780 lambda_k: 1 Loss: 0.07452163507703827\n",
      "Iteration: 1781 lambda_k: 1 Loss: 0.07443570191567318\n",
      "Iteration: 1782 lambda_k: 1 Loss: 0.07434980733763537\n",
      "Iteration: 1783 lambda_k: 1 Loss: 0.07426395132473364\n",
      "Iteration: 1784 lambda_k: 1 Loss: 0.07417813385879446\n",
      "Iteration: 1785 lambda_k: 1 Loss: 0.07409235492166229\n",
      "Iteration: 1786 lambda_k: 1 Loss: 0.07400661449519914\n",
      "Iteration: 1787 lambda_k: 1 Loss: 0.0739209125612847\n",
      "Iteration: 1788 lambda_k: 1 Loss: 0.07383524910181605\n",
      "Iteration: 1789 lambda_k: 1 Loss: 0.07374962409870764\n",
      "Iteration: 1790 lambda_k: 1 Loss: 0.07366403753389122\n",
      "Iteration: 1791 lambda_k: 1 Loss: 0.07357848938931577\n",
      "Iteration: 1792 lambda_k: 1 Loss: 0.07349297964694733\n",
      "Iteration: 1793 lambda_k: 1 Loss: 0.07340750828876884\n",
      "Iteration: 1794 lambda_k: 1 Loss: 0.07332207529678023\n",
      "Iteration: 1795 lambda_k: 1 Loss: 0.07323668065299815\n",
      "Iteration: 1796 lambda_k: 1 Loss: 0.073151324339456\n",
      "Iteration: 1797 lambda_k: 1 Loss: 0.07306600633820363\n",
      "Iteration: 1798 lambda_k: 1 Loss: 0.07298072663130765\n",
      "Iteration: 1799 lambda_k: 1 Loss: 0.07289548520085092\n",
      "Iteration: 1800 lambda_k: 1 Loss: 0.07281028202893274\n",
      "Iteration: 1801 lambda_k: 1 Loss: 0.07272511709766853\n",
      "Iteration: 1802 lambda_k: 1 Loss: 0.0726399903891899\n",
      "Iteration: 1803 lambda_k: 1 Loss: 0.07255490188564453\n",
      "Iteration: 1804 lambda_k: 1 Loss: 0.07246985156919612\n",
      "Iteration: 1805 lambda_k: 1 Loss: 0.07238483942202416\n",
      "Iteration: 1806 lambda_k: 1 Loss: 0.07229986542632401\n",
      "Iteration: 1807 lambda_k: 1 Loss: 0.0722149295643068\n",
      "Iteration: 1808 lambda_k: 1 Loss: 0.0721300318181992\n",
      "Iteration: 1809 lambda_k: 1 Loss: 0.0720451721702435\n",
      "Iteration: 1810 lambda_k: 1 Loss: 0.0719603506026974\n",
      "Iteration: 1811 lambda_k: 1 Loss: 0.07187556709783398\n",
      "Iteration: 1812 lambda_k: 1 Loss: 0.07179082163794179\n",
      "Iteration: 1813 lambda_k: 1 Loss: 0.07170611420532437\n",
      "Iteration: 1814 lambda_k: 1 Loss: 0.07162144478230055\n",
      "Iteration: 1815 lambda_k: 1 Loss: 0.07153681335120432\n",
      "Iteration: 1816 lambda_k: 1 Loss: 0.07145221989438448\n",
      "Iteration: 1817 lambda_k: 1 Loss: 0.07136766439420486\n",
      "Iteration: 1818 lambda_k: 1 Loss: 0.07128314683304407\n",
      "Iteration: 1819 lambda_k: 1 Loss: 0.07119866719329558\n",
      "Iteration: 1820 lambda_k: 1 Loss: 0.07111422545736751\n",
      "Iteration: 1821 lambda_k: 1 Loss: 0.07102982160768255\n",
      "Iteration: 1822 lambda_k: 1 Loss: 0.07094545562667796\n",
      "Iteration: 1823 lambda_k: 1 Loss: 0.07086112749680558\n",
      "Iteration: 1824 lambda_k: 1 Loss: 0.07077683719438048\n",
      "Iteration: 1825 lambda_k: 1 Loss: 0.07069258471392763\n",
      "Iteration: 1826 lambda_k: 1 Loss: 0.07060837003208158\n",
      "Iteration: 1827 lambda_k: 1 Loss: 0.0705241931313349\n",
      "Iteration: 1828 lambda_k: 1 Loss: 0.07044005399419996\n",
      "Iteration: 1829 lambda_k: 1 Loss: 0.07035595260321283\n",
      "Iteration: 1830 lambda_k: 1 Loss: 0.07027188894092666\n",
      "Iteration: 1831 lambda_k: 1 Loss: 0.07018786298990741\n",
      "Iteration: 1832 lambda_k: 1 Loss: 0.07010387473273343\n",
      "Iteration: 1833 lambda_k: 1 Loss: 0.07001992415199626\n",
      "Iteration: 1834 lambda_k: 1 Loss: 0.06993601123030158\n",
      "Iteration: 1835 lambda_k: 1 Loss: 0.0698521359502689\n",
      "Iteration: 1836 lambda_k: 1 Loss: 0.06976829829453193\n",
      "Iteration: 1837 lambda_k: 1 Loss: 0.06968449824573783\n",
      "Iteration: 1838 lambda_k: 1 Loss: 0.06960073578654714\n",
      "Iteration: 1839 lambda_k: 1 Loss: 0.06951701089963391\n",
      "Iteration: 1840 lambda_k: 1 Loss: 0.06943332356768532\n",
      "Iteration: 1841 lambda_k: 1 Loss: 0.06934967377340195\n",
      "Iteration: 1842 lambda_k: 1 Loss: 0.06926606149949771\n",
      "Iteration: 1843 lambda_k: 1 Loss: 0.06918248672869969\n",
      "Iteration: 1844 lambda_k: 1 Loss: 0.06909894944374811\n",
      "Iteration: 1845 lambda_k: 1 Loss: 0.0690154496273963\n",
      "Iteration: 1846 lambda_k: 1 Loss: 0.06893198726241048\n",
      "Iteration: 1847 lambda_k: 1 Loss: 0.06884856233156998\n",
      "Iteration: 1848 lambda_k: 1 Loss: 0.06876517481766702\n",
      "Iteration: 1849 lambda_k: 1 Loss: 0.06868182470350646\n",
      "Iteration: 1850 lambda_k: 1 Loss: 0.06859851197190618\n",
      "Iteration: 1851 lambda_k: 1 Loss: 0.06851523660569675\n",
      "Iteration: 1852 lambda_k: 1 Loss: 0.06843199858772141\n",
      "Iteration: 1853 lambda_k: 1 Loss: 0.06834879790083602\n",
      "Iteration: 1854 lambda_k: 1 Loss: 0.06826563452790886\n",
      "Iteration: 1855 lambda_k: 1 Loss: 0.0681825084518211\n",
      "Iteration: 1856 lambda_k: 1 Loss: 0.06809941965546604\n",
      "Iteration: 1857 lambda_k: 1 Loss: 0.06801636812174952\n",
      "Iteration: 1858 lambda_k: 1 Loss: 0.0679333538335898\n",
      "Iteration: 1859 lambda_k: 1 Loss: 0.0678503767739174\n",
      "Iteration: 1860 lambda_k: 1 Loss: 0.0677674369256752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1861 lambda_k: 1 Loss: 0.06768453427181814\n",
      "Iteration: 1862 lambda_k: 1 Loss: 0.06760166879531339\n",
      "Iteration: 1863 lambda_k: 1 Loss: 0.06751884047914035\n",
      "Iteration: 1864 lambda_k: 1 Loss: 0.06743604930629035\n",
      "Iteration: 1865 lambda_k: 1 Loss: 0.06735329525976677\n",
      "Iteration: 1866 lambda_k: 1 Loss: 0.06727057832258508\n",
      "Iteration: 1867 lambda_k: 1 Loss: 0.06718789847777261\n",
      "Iteration: 1868 lambda_k: 1 Loss: 0.06710525570836853\n",
      "Iteration: 1869 lambda_k: 1 Loss: 0.06702264999742394\n",
      "Iteration: 1870 lambda_k: 1 Loss: 0.06694008132800161\n",
      "Iteration: 1871 lambda_k: 1 Loss: 0.06685754968317624\n",
      "Iteration: 1872 lambda_k: 1 Loss: 0.06677505504603405\n",
      "Iteration: 1873 lambda_k: 1 Loss: 0.06669259739967313\n",
      "Iteration: 1874 lambda_k: 1 Loss: 0.06661017672720299\n",
      "Iteration: 1875 lambda_k: 1 Loss: 0.06652779301174479\n",
      "Iteration: 1876 lambda_k: 1 Loss: 0.06644544623643125\n",
      "Iteration: 1877 lambda_k: 1 Loss: 0.06636313638440658\n",
      "Iteration: 1878 lambda_k: 1 Loss: 0.06628086343882642\n",
      "Iteration: 1879 lambda_k: 1 Loss: 0.0661986273828578\n",
      "Iteration: 1880 lambda_k: 1 Loss: 0.06611642819967917\n",
      "Iteration: 1881 lambda_k: 1 Loss: 0.0660342658724803\n",
      "Iteration: 1882 lambda_k: 1 Loss: 0.06595214038446223\n",
      "Iteration: 1883 lambda_k: 1 Loss: 0.06587005171883724\n",
      "Iteration: 1884 lambda_k: 1 Loss: 0.06578799985882876\n",
      "Iteration: 1885 lambda_k: 1 Loss: 0.0657059847876715\n",
      "Iteration: 1886 lambda_k: 1 Loss: 0.06562400648861134\n",
      "Iteration: 1887 lambda_k: 1 Loss: 0.06554206494490503\n",
      "Iteration: 1888 lambda_k: 1 Loss: 0.06546016013982067\n",
      "Iteration: 1889 lambda_k: 1 Loss: 0.06537829205663713\n",
      "Iteration: 1890 lambda_k: 1 Loss: 0.06529646067864432\n",
      "Iteration: 1891 lambda_k: 1 Loss: 0.06521466598914329\n",
      "Iteration: 1892 lambda_k: 1 Loss: 0.06513290797144572\n",
      "Iteration: 1893 lambda_k: 1 Loss: 0.06505118660887427\n",
      "Iteration: 1894 lambda_k: 1 Loss: 0.0649695018847626\n",
      "Iteration: 1895 lambda_k: 1 Loss: 0.0648878537824549\n",
      "Iteration: 1896 lambda_k: 1 Loss: 0.0648062422853063\n",
      "Iteration: 1897 lambda_k: 1 Loss: 0.06472466737668266\n",
      "Iteration: 1898 lambda_k: 1 Loss: 0.06464312903996039\n",
      "Iteration: 1899 lambda_k: 1 Loss: 0.06456162725852675\n",
      "Iteration: 1900 lambda_k: 1 Loss: 0.06448016201577951\n",
      "Iteration: 1901 lambda_k: 1 Loss: 0.06439873329512716\n",
      "Iteration: 1902 lambda_k: 1 Loss: 0.0643173410799886\n",
      "Iteration: 1903 lambda_k: 1 Loss: 0.06423598535379328\n",
      "Iteration: 1904 lambda_k: 1 Loss: 0.06415466609998142\n",
      "Iteration: 1905 lambda_k: 1 Loss: 0.06407338330200338\n",
      "Iteration: 1906 lambda_k: 1 Loss: 0.06399213694332004\n",
      "Iteration: 1907 lambda_k: 1 Loss: 0.0639109270074028\n",
      "Iteration: 1908 lambda_k: 1 Loss: 0.06382975347773337\n",
      "Iteration: 1909 lambda_k: 1 Loss: 0.06374861633780382\n",
      "Iteration: 1910 lambda_k: 1 Loss: 0.06366751557111644\n",
      "Iteration: 1911 lambda_k: 1 Loss: 0.06358645116118397\n",
      "Iteration: 1912 lambda_k: 1 Loss: 0.0635054230915292\n",
      "Iteration: 1913 lambda_k: 1 Loss: 0.0634244313456854\n",
      "Iteration: 1914 lambda_k: 1 Loss: 0.06334347590719579\n",
      "Iteration: 1915 lambda_k: 1 Loss: 0.06326255675961386\n",
      "Iteration: 1916 lambda_k: 1 Loss: 0.0631816738865033\n",
      "Iteration: 1917 lambda_k: 1 Loss: 0.06310082727143791\n",
      "Iteration: 1918 lambda_k: 1 Loss: 0.06302001689800148\n",
      "Iteration: 1919 lambda_k: 1 Loss: 0.06293924274978788\n",
      "Iteration: 1920 lambda_k: 1 Loss: 0.06285850481040099\n",
      "Iteration: 1921 lambda_k: 1 Loss: 0.06277780306345473\n",
      "Iteration: 1922 lambda_k: 1 Loss: 0.06269713749257294\n",
      "Iteration: 1923 lambda_k: 1 Loss: 0.0626165080813895\n",
      "Iteration: 1924 lambda_k: 1 Loss: 0.06253591481354814\n",
      "Iteration: 1925 lambda_k: 1 Loss: 0.06245535767270257\n",
      "Iteration: 1926 lambda_k: 1 Loss: 0.06237483664251631\n",
      "Iteration: 1927 lambda_k: 1 Loss: 0.06229435170666268\n",
      "Iteration: 1928 lambda_k: 1 Loss: 0.06221390284882487\n",
      "Iteration: 1929 lambda_k: 1 Loss: 0.062133490052695826\n",
      "Iteration: 1930 lambda_k: 1 Loss: 0.06205311330197824\n",
      "Iteration: 1931 lambda_k: 1 Loss: 0.06197277258038466\n",
      "Iteration: 1932 lambda_k: 1 Loss: 0.061892467871637276\n",
      "Iteration: 1933 lambda_k: 1 Loss: 0.06181219915946794\n",
      "Iteration: 1934 lambda_k: 1 Loss: 0.06173196642761817\n",
      "Iteration: 1935 lambda_k: 1 Loss: 0.061651769659839324\n",
      "Iteration: 1936 lambda_k: 1 Loss: 0.06157160883989213\n",
      "Iteration: 1937 lambda_k: 1 Loss: 0.06149148395154711\n",
      "Iteration: 1938 lambda_k: 1 Loss: 0.06141139497858417\n",
      "Iteration: 1939 lambda_k: 1 Loss: 0.06133134190479285\n",
      "Iteration: 1940 lambda_k: 1 Loss: 0.06125132471397237\n",
      "Iteration: 1941 lambda_k: 1 Loss: 0.06117134338993123\n",
      "Iteration: 1942 lambda_k: 1 Loss: 0.06109139791648758\n",
      "Iteration: 1943 lambda_k: 1 Loss: 0.06101148827746894\n",
      "Iteration: 1944 lambda_k: 1 Loss: 0.060931614456712316\n",
      "Iteration: 1945 lambda_k: 1 Loss: 0.060851776438064185\n",
      "Iteration: 1946 lambda_k: 1 Loss: 0.060771974205380275\n",
      "Iteration: 1947 lambda_k: 1 Loss: 0.0606922077425259\n",
      "Iteration: 1948 lambda_k: 1 Loss: 0.06061247703337556\n",
      "Iteration: 1949 lambda_k: 1 Loss: 0.060532782061813166\n",
      "Iteration: 1950 lambda_k: 1 Loss: 0.060453122811731916\n",
      "Iteration: 1951 lambda_k: 1 Loss: 0.06037349926703429\n",
      "Iteration: 1952 lambda_k: 1 Loss: 0.0602939114116322\n",
      "Iteration: 1953 lambda_k: 1 Loss: 0.06021435922944667\n",
      "Iteration: 1954 lambda_k: 1 Loss: 0.060134842704407956\n",
      "Iteration: 1955 lambda_k: 1 Loss: 0.06005536182045554\n",
      "Iteration: 1956 lambda_k: 1 Loss: 0.05997591656153819\n",
      "Iteration: 1957 lambda_k: 1 Loss: 0.05989650691161382\n",
      "Iteration: 1958 lambda_k: 1 Loss: 0.05981713285464939\n",
      "Iteration: 1959 lambda_k: 1 Loss: 0.05973779437462122\n",
      "Iteration: 1960 lambda_k: 1 Loss: 0.05965849145551453\n",
      "Iteration: 1961 lambda_k: 1 Loss: 0.0595792240813238\n",
      "Iteration: 1962 lambda_k: 1 Loss: 0.05949999223605251\n",
      "Iteration: 1963 lambda_k: 1 Loss: 0.059420795903713325\n",
      "Iteration: 1964 lambda_k: 1 Loss: 0.0593416350683278\n",
      "Iteration: 1965 lambda_k: 1 Loss: 0.059262509713926693\n",
      "Iteration: 1966 lambda_k: 1 Loss: 0.05918341982454966\n",
      "Iteration: 1967 lambda_k: 1 Loss: 0.05910436538424544\n",
      "Iteration: 1968 lambda_k: 1 Loss: 0.059025346377071616\n",
      "Iteration: 1969 lambda_k: 1 Loss: 0.05894636278709491\n",
      "Iteration: 1970 lambda_k: 1 Loss: 0.0588674145983909\n",
      "Iteration: 1971 lambda_k: 1 Loss: 0.05878850179504415\n",
      "Iteration: 1972 lambda_k: 1 Loss: 0.05870962436114804\n",
      "Iteration: 1973 lambda_k: 1 Loss: 0.05863078228080497\n",
      "Iteration: 1974 lambda_k: 1 Loss: 0.05855197551750224\n",
      "Iteration: 1975 lambda_k: 1 Loss: 0.05847320409240548\n",
      "Iteration: 1976 lambda_k: 1 Loss: 0.0583944679793611\n",
      "Iteration: 1977 lambda_k: 1 Loss: 0.0583157671594985\n",
      "Iteration: 1978 lambda_k: 1 Loss: 0.05823710161352837\n",
      "Iteration: 1979 lambda_k: 1 Loss: 0.05815847132446606\n",
      "Iteration: 1980 lambda_k: 1 Loss: 0.05807987627680842\n",
      "Iteration: 1981 lambda_k: 1 Loss: 0.05800131645535959\n",
      "Iteration: 1982 lambda_k: 1 Loss: 0.05792279184467837\n",
      "Iteration: 1983 lambda_k: 1 Loss: 0.05784430242903692\n",
      "Iteration: 1984 lambda_k: 1 Loss: 0.05776584819257443\n",
      "Iteration: 1985 lambda_k: 1 Loss: 0.057687429119435245\n",
      "Iteration: 1986 lambda_k: 1 Loss: 0.05760904519382315\n",
      "Iteration: 1987 lambda_k: 1 Loss: 0.057530696399990824\n",
      "Iteration: 1988 lambda_k: 1 Loss: 0.0574523827222089\n",
      "Iteration: 1989 lambda_k: 1 Loss: 0.05737410414474628\n",
      "Iteration: 1990 lambda_k: 1 Loss: 0.05729586065186665\n",
      "Iteration: 1991 lambda_k: 1 Loss: 0.057217652227834606\n",
      "Iteration: 1992 lambda_k: 1 Loss: 0.057139478856921874\n",
      "Iteration: 1993 lambda_k: 1 Loss: 0.05706134052341015\n",
      "Iteration: 1994 lambda_k: 1 Loss: 0.05698323722657736\n",
      "Iteration: 1995 lambda_k: 1 Loss: 0.05690516892046358\n",
      "Iteration: 1996 lambda_k: 1 Loss: 0.05682713560010873\n",
      "Iteration: 1997 lambda_k: 1 Loss: 0.05674913725362617\n",
      "Iteration: 1998 lambda_k: 1 Loss: 0.05667117386808472\n",
      "Iteration: 1999 lambda_k: 1 Loss: 0.05659324542775494\n",
      "Iteration: 2000 lambda_k: 1 Loss: 0.05651535191561503\n",
      "Iteration: 2001 lambda_k: 1 Loss: 0.056437493315080786\n",
      "Iteration: 2002 lambda_k: 1 Loss: 0.056359669610428834\n",
      "Iteration: 2003 lambda_k: 1 Loss: 0.05628188078633529\n",
      "Iteration: 2004 lambda_k: 1 Loss: 0.05620412682749645\n",
      "Iteration: 2005 lambda_k: 1 Loss: 0.0561264077184133\n",
      "Iteration: 2006 lambda_k: 1 Loss: 0.056048723443415625\n",
      "Iteration: 2007 lambda_k: 1 Loss: 0.05597107398678106\n",
      "Iteration: 2008 lambda_k: 1 Loss: 0.055893459332827466\n",
      "Iteration: 2009 lambda_k: 1 Loss: 0.0558158794659383\n",
      "Iteration: 2010 lambda_k: 1 Loss: 0.05573833437054186\n",
      "Iteration: 2011 lambda_k: 1 Loss: 0.05566082403108215\n",
      "Iteration: 2012 lambda_k: 1 Loss: 0.05558334843200232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2013 lambda_k: 1 Loss: 0.05550590755774374\n",
      "Iteration: 2014 lambda_k: 1 Loss: 0.05542850139275175\n",
      "Iteration: 2015 lambda_k: 1 Loss: 0.05535112992148159\n",
      "Iteration: 2016 lambda_k: 1 Loss: 0.05527379312839996\n",
      "Iteration: 2017 lambda_k: 1 Loss: 0.055196490997984585\n",
      "Iteration: 2018 lambda_k: 1 Loss: 0.05511922351472266\n",
      "Iteration: 2019 lambda_k: 1 Loss: 0.05504199066310978\n",
      "Iteration: 2020 lambda_k: 1 Loss: 0.05496479242764961\n",
      "Iteration: 2021 lambda_k: 1 Loss: 0.05488762879285399\n",
      "Iteration: 2022 lambda_k: 1 Loss: 0.05481049974324314\n",
      "Iteration: 2023 lambda_k: 1 Loss: 0.05473340526334597\n",
      "Iteration: 2024 lambda_k: 1 Loss: 0.05465634533769972\n",
      "Iteration: 2025 lambda_k: 1 Loss: 0.054579319950850115\n",
      "Iteration: 2026 lambda_k: 1 Loss: 0.05450232908735141\n",
      "Iteration: 2027 lambda_k: 1 Loss: 0.05442537273176601\n",
      "Iteration: 2028 lambda_k: 1 Loss: 0.054348450868664556\n",
      "Iteration: 2029 lambda_k: 1 Loss: 0.054271563482626045\n",
      "Iteration: 2030 lambda_k: 1 Loss: 0.054194710558237644\n",
      "Iteration: 2031 lambda_k: 1 Loss: 0.05411789208009498\n",
      "Iteration: 2032 lambda_k: 1 Loss: 0.05404110803280168\n",
      "Iteration: 2033 lambda_k: 1 Loss: 0.05396435840096965\n",
      "Iteration: 2034 lambda_k: 1 Loss: 0.053887643169219096\n",
      "Iteration: 2035 lambda_k: 1 Loss: 0.05381096232217834\n",
      "Iteration: 2036 lambda_k: 1 Loss: 0.053734315844483904\n",
      "Iteration: 2037 lambda_k: 1 Loss: 0.053657703720780485\n",
      "Iteration: 2038 lambda_k: 1 Loss: 0.053581125935720855\n",
      "Iteration: 2039 lambda_k: 1 Loss: 0.05350458247396598\n",
      "Iteration: 2040 lambda_k: 1 Loss: 0.053428073320185114\n",
      "Iteration: 2041 lambda_k: 1 Loss: 0.053351598459055405\n",
      "Iteration: 2042 lambda_k: 1 Loss: 0.053275157875262276\n",
      "Iteration: 2043 lambda_k: 1 Loss: 0.0531987515534993\n",
      "Iteration: 2044 lambda_k: 1 Loss: 0.05312237947846793\n",
      "Iteration: 2045 lambda_k: 1 Loss: 0.053046041634878006\n",
      "Iteration: 2046 lambda_k: 1 Loss: 0.05296973800744722\n",
      "Iteration: 2047 lambda_k: 1 Loss: 0.05289346858090147\n",
      "Iteration: 2048 lambda_k: 1 Loss: 0.052817233339974776\n",
      "Iteration: 2049 lambda_k: 1 Loss: 0.05274103226940902\n",
      "Iteration: 2050 lambda_k: 1 Loss: 0.052664865353954385\n",
      "Iteration: 2051 lambda_k: 1 Loss: 0.052588732578368956\n",
      "Iteration: 2052 lambda_k: 1 Loss: 0.052512633927418985\n",
      "Iteration: 2053 lambda_k: 1 Loss: 0.05243656938587856\n",
      "Iteration: 2054 lambda_k: 1 Loss: 0.05236053893853\n",
      "Iteration: 2055 lambda_k: 1 Loss: 0.05228454257016356\n",
      "Iteration: 2056 lambda_k: 1 Loss: 0.05220858026557753\n",
      "Iteration: 2057 lambda_k: 1 Loss: 0.052132652009578194\n",
      "Iteration: 2058 lambda_k: 1 Loss: 0.05205675778697988\n",
      "Iteration: 2059 lambda_k: 1 Loss: 0.05198089758260486\n",
      "Iteration: 2060 lambda_k: 1 Loss: 0.05190507138128343\n",
      "Iteration: 2061 lambda_k: 1 Loss: 0.05182927916785396\n",
      "Iteration: 2062 lambda_k: 1 Loss: 0.051753520927162656\n",
      "Iteration: 2063 lambda_k: 1 Loss: 0.051677796644063814\n",
      "Iteration: 2064 lambda_k: 1 Loss: 0.051602106303419645\n",
      "Iteration: 2065 lambda_k: 1 Loss: 0.05152644989010029\n",
      "Iteration: 2066 lambda_k: 1 Loss: 0.05145082738898387\n",
      "Iteration: 2067 lambda_k: 1 Loss: 0.05137523878495652\n",
      "Iteration: 2068 lambda_k: 1 Loss: 0.051299684062912305\n",
      "Iteration: 2069 lambda_k: 1 Loss: 0.051224163207753144\n",
      "Iteration: 2070 lambda_k: 1 Loss: 0.05114867620438903\n",
      "Iteration: 2071 lambda_k: 1 Loss: 0.05107322303773787\n",
      "Iteration: 2072 lambda_k: 1 Loss: 0.05099780369272534\n",
      "Iteration: 2073 lambda_k: 1 Loss: 0.05092241815428521\n",
      "Iteration: 2074 lambda_k: 1 Loss: 0.050847066407359115\n",
      "Iteration: 2075 lambda_k: 1 Loss: 0.05077174843689663\n",
      "Iteration: 2076 lambda_k: 1 Loss: 0.05069646422785513\n",
      "Iteration: 2077 lambda_k: 1 Loss: 0.05062121376520006\n",
      "Iteration: 2078 lambda_k: 1 Loss: 0.050545997033904624\n",
      "Iteration: 2079 lambda_k: 1 Loss: 0.050470814018950046\n",
      "Iteration: 2080 lambda_k: 1 Loss: 0.05039566470532535\n",
      "Iteration: 2081 lambda_k: 1 Loss: 0.050320549078027424\n",
      "Iteration: 2082 lambda_k: 1 Loss: 0.050245467122061155\n",
      "Iteration: 2083 lambda_k: 1 Loss: 0.05017041882243919\n",
      "Iteration: 2084 lambda_k: 1 Loss: 0.050095404164182164\n",
      "Iteration: 2085 lambda_k: 1 Loss: 0.05002042313231842\n",
      "Iteration: 2086 lambda_k: 1 Loss: 0.04994547571188434\n",
      "Iteration: 2087 lambda_k: 1 Loss: 0.049870561887924096\n",
      "Iteration: 2088 lambda_k: 1 Loss: 0.04979568164548979\n",
      "Iteration: 2089 lambda_k: 1 Loss: 0.04972083496964119\n",
      "Iteration: 2090 lambda_k: 1 Loss: 0.049646021845446135\n",
      "Iteration: 2091 lambda_k: 1 Loss: 0.049571242257980194\n",
      "Iteration: 2092 lambda_k: 1 Loss: 0.04949649619232682\n",
      "Iteration: 2093 lambda_k: 1 Loss: 0.049421783633577396\n",
      "Iteration: 2094 lambda_k: 1 Loss: 0.04934710456683096\n",
      "Iteration: 2095 lambda_k: 1 Loss: 0.04927245897719452\n",
      "Iteration: 2096 lambda_k: 1 Loss: 0.04919784684978283\n",
      "Iteration: 2097 lambda_k: 1 Loss: 0.04912326816971856\n",
      "Iteration: 2098 lambda_k: 1 Loss: 0.04904872292213216\n",
      "Iteration: 2099 lambda_k: 1 Loss: 0.04897421109216186\n",
      "Iteration: 2100 lambda_k: 1 Loss: 0.04889973266495387\n",
      "Iteration: 2101 lambda_k: 1 Loss: 0.048825287625662114\n",
      "Iteration: 2102 lambda_k: 1 Loss: 0.04875087595944837\n",
      "Iteration: 2103 lambda_k: 1 Loss: 0.04867649765148215\n",
      "Iteration: 2104 lambda_k: 1 Loss: 0.048602152686940764\n",
      "Iteration: 2105 lambda_k: 1 Loss: 0.04852784105100958\n",
      "Iteration: 2106 lambda_k: 1 Loss: 0.048453562728881454\n",
      "Iteration: 2107 lambda_k: 1 Loss: 0.04837931770575725\n",
      "Iteration: 2108 lambda_k: 1 Loss: 0.04830510596684555\n",
      "Iteration: 2109 lambda_k: 1 Loss: 0.04823092749736287\n",
      "Iteration: 2110 lambda_k: 1 Loss: 0.04815678228253338\n",
      "Iteration: 2111 lambda_k: 1 Loss: 0.04808267030758908\n",
      "Iteration: 2112 lambda_k: 1 Loss: 0.04800859155776972\n",
      "Iteration: 2113 lambda_k: 1 Loss: 0.047934546018323\n",
      "Iteration: 2114 lambda_k: 1 Loss: 0.04786053367450429\n",
      "Iteration: 2115 lambda_k: 1 Loss: 0.047786554511576734\n",
      "Iteration: 2116 lambda_k: 1 Loss: 0.04771260851481145\n",
      "Iteration: 2117 lambda_k: 1 Loss: 0.04763869566948713\n",
      "Iteration: 2118 lambda_k: 1 Loss: 0.047564815960890315\n",
      "Iteration: 2119 lambda_k: 1 Loss: 0.047490969374315364\n",
      "Iteration: 2120 lambda_k: 1 Loss: 0.04741715589506444\n",
      "Iteration: 2121 lambda_k: 1 Loss: 0.04734337550844746\n",
      "Iteration: 2122 lambda_k: 1 Loss: 0.04726962819978218\n",
      "Iteration: 2123 lambda_k: 1 Loss: 0.047195913954394035\n",
      "Iteration: 2124 lambda_k: 1 Loss: 0.04712223275761624\n",
      "Iteration: 2125 lambda_k: 1 Loss: 0.04704858459479\n",
      "Iteration: 2126 lambda_k: 1 Loss: 0.046974969451264025\n",
      "Iteration: 2127 lambda_k: 1 Loss: 0.04690138731239496\n",
      "Iteration: 2128 lambda_k: 1 Loss: 0.04682783816354715\n",
      "Iteration: 2129 lambda_k: 1 Loss: 0.04675432199009286\n",
      "Iteration: 2130 lambda_k: 1 Loss: 0.04668083877741203\n",
      "Iteration: 2131 lambda_k: 1 Loss: 0.046607388510892286\n",
      "Iteration: 2132 lambda_k: 1 Loss: 0.04653397117592925\n",
      "Iteration: 2133 lambda_k: 1 Loss: 0.04646058675792618\n",
      "Iteration: 2134 lambda_k: 1 Loss: 0.046387235242294135\n",
      "Iteration: 2135 lambda_k: 1 Loss: 0.04631391661445188\n",
      "Iteration: 2136 lambda_k: 1 Loss: 0.046240630859826146\n",
      "Iteration: 2137 lambda_k: 1 Loss: 0.046167377963851244\n",
      "Iteration: 2138 lambda_k: 1 Loss: 0.04609415791196933\n",
      "Iteration: 2139 lambda_k: 1 Loss: 0.04602097068963039\n",
      "Iteration: 2140 lambda_k: 1 Loss: 0.04594781628229215\n",
      "Iteration: 2141 lambda_k: 1 Loss: 0.045874694675420054\n",
      "Iteration: 2142 lambda_k: 1 Loss: 0.045801605854487414\n",
      "Iteration: 2143 lambda_k: 1 Loss: 0.04572854980497526\n",
      "Iteration: 2144 lambda_k: 1 Loss: 0.04565552651237378\n",
      "Iteration: 2145 lambda_k: 1 Loss: 0.04558253596217972\n",
      "Iteration: 2146 lambda_k: 1 Loss: 0.04550957813989728\n",
      "Iteration: 2147 lambda_k: 1 Loss: 0.04543665303103818\n",
      "Iteration: 2148 lambda_k: 1 Loss: 0.045363760621121955\n",
      "Iteration: 2149 lambda_k: 1 Loss: 0.045290900895676146\n",
      "Iteration: 2150 lambda_k: 1 Loss: 0.0452180738402363\n",
      "Iteration: 2151 lambda_k: 1 Loss: 0.045145279440345965\n",
      "Iteration: 2152 lambda_k: 1 Loss: 0.04507251768155632\n",
      "Iteration: 2153 lambda_k: 1 Loss: 0.04499978854942632\n",
      "Iteration: 2154 lambda_k: 1 Loss: 0.044927092029522726\n",
      "Iteration: 2155 lambda_k: 1 Loss: 0.04485442810741993\n",
      "Iteration: 2156 lambda_k: 1 Loss: 0.04478179676870025\n",
      "Iteration: 2157 lambda_k: 1 Loss: 0.0447091979989539\n",
      "Iteration: 2158 lambda_k: 1 Loss: 0.044636631783778656\n",
      "Iteration: 2159 lambda_k: 1 Loss: 0.044564098108780335\n",
      "Iteration: 2160 lambda_k: 1 Loss: 0.04449159695957242\n",
      "Iteration: 2161 lambda_k: 1 Loss: 0.04441912832177628\n",
      "Iteration: 2162 lambda_k: 1 Loss: 0.04434669218102107\n",
      "Iteration: 2163 lambda_k: 1 Loss: 0.04427428852294371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2164 lambda_k: 1 Loss: 0.04420191733318901\n",
      "Iteration: 2165 lambda_k: 1 Loss: 0.0441295785974095\n",
      "Iteration: 2166 lambda_k: 1 Loss: 0.044057272301265604\n",
      "Iteration: 2167 lambda_k: 1 Loss: 0.04398499843042546\n",
      "Iteration: 2168 lambda_k: 1 Loss: 0.043912756970565146\n",
      "Iteration: 2169 lambda_k: 1 Loss: 0.04384054790736867\n",
      "Iteration: 2170 lambda_k: 1 Loss: 0.04376837122652754\n",
      "Iteration: 2171 lambda_k: 1 Loss: 0.04369622691374144\n",
      "Iteration: 2172 lambda_k: 1 Loss: 0.043624114954717685\n",
      "Iteration: 2173 lambda_k: 1 Loss: 0.04355203533517144\n",
      "Iteration: 2174 lambda_k: 1 Loss: 0.04347998804082578\n",
      "Iteration: 2175 lambda_k: 1 Loss: 0.043407973057411675\n",
      "Iteration: 2176 lambda_k: 1 Loss: 0.043335990370667765\n",
      "Iteration: 2177 lambda_k: 1 Loss: 0.043264039966340664\n",
      "Iteration: 2178 lambda_k: 1 Loss: 0.04319212183018484\n",
      "Iteration: 2179 lambda_k: 1 Loss: 0.04312023594796253\n",
      "Iteration: 2180 lambda_k: 1 Loss: 0.04304838230544398\n",
      "Iteration: 2181 lambda_k: 1 Loss: 0.04297656088840727\n",
      "Iteration: 2182 lambda_k: 1 Loss: 0.04290477168263821\n",
      "Iteration: 2183 lambda_k: 1 Loss: 0.04283301467393065\n",
      "Iteration: 2184 lambda_k: 1 Loss: 0.042761289848086234\n",
      "Iteration: 2185 lambda_k: 1 Loss: 0.04268959719091445\n",
      "Iteration: 2186 lambda_k: 1 Loss: 0.04261793668823281\n",
      "Iteration: 2187 lambda_k: 1 Loss: 0.04254630832586657\n",
      "Iteration: 2188 lambda_k: 1 Loss: 0.042474712089648946\n",
      "Iteration: 2189 lambda_k: 1 Loss: 0.04240314796542112\n",
      "Iteration: 2190 lambda_k: 1 Loss: 0.04233161593903214\n",
      "Iteration: 2191 lambda_k: 1 Loss: 0.04226011599633885\n",
      "Iteration: 2192 lambda_k: 1 Loss: 0.04218864812320614\n",
      "Iteration: 2193 lambda_k: 1 Loss: 0.042117212305506795\n",
      "Iteration: 2194 lambda_k: 1 Loss: 0.04204580852912156\n",
      "Iteration: 2195 lambda_k: 1 Loss: 0.04197443677993903\n",
      "Iteration: 2196 lambda_k: 1 Loss: 0.041903097043855794\n",
      "Iteration: 2197 lambda_k: 1 Loss: 0.04183178930677634\n",
      "Iteration: 2198 lambda_k: 1 Loss: 0.04176051355461315\n",
      "Iteration: 2199 lambda_k: 1 Loss: 0.04168926977328658\n",
      "Iteration: 2200 lambda_k: 1 Loss: 0.041618057948725036\n",
      "Iteration: 2201 lambda_k: 1 Loss: 0.041546878066864884\n",
      "Iteration: 2202 lambda_k: 1 Loss: 0.041475730113650404\n",
      "Iteration: 2203 lambda_k: 1 Loss: 0.041404614075033985\n",
      "Iteration: 2204 lambda_k: 1 Loss: 0.04133352993697581\n",
      "Iteration: 2205 lambda_k: 1 Loss: 0.04126247768544407\n",
      "Iteration: 2206 lambda_k: 1 Loss: 0.04119145730641512\n",
      "Iteration: 2207 lambda_k: 1 Loss: 0.04112046878587321\n",
      "Iteration: 2208 lambda_k: 1 Loss: 0.04104951210981056\n",
      "Iteration: 2209 lambda_k: 1 Loss: 0.040978587264227456\n",
      "Iteration: 2210 lambda_k: 1 Loss: 0.04090769423513225\n",
      "Iteration: 2211 lambda_k: 1 Loss: 0.04083683300854131\n",
      "Iteration: 2212 lambda_k: 1 Loss: 0.04076600357047899\n",
      "Iteration: 2213 lambda_k: 1 Loss: 0.04069520590697762\n",
      "Iteration: 2214 lambda_k: 1 Loss: 0.04062444000407773\n",
      "Iteration: 2215 lambda_k: 1 Loss: 0.04055370584782778\n",
      "Iteration: 2216 lambda_k: 1 Loss: 0.04048300342428456\n",
      "Iteration: 2217 lambda_k: 1 Loss: 0.04041233271951258\n",
      "Iteration: 2218 lambda_k: 1 Loss: 0.04034169371958466\n",
      "Iteration: 2219 lambda_k: 1 Loss: 0.04027108641058163\n",
      "Iteration: 2220 lambda_k: 1 Loss: 0.04020051077859241\n",
      "Iteration: 2221 lambda_k: 1 Loss: 0.040129966809714084\n",
      "Iteration: 2222 lambda_k: 1 Loss: 0.04005945449005173\n",
      "Iteration: 2223 lambda_k: 1 Loss: 0.03998897380571875\n",
      "Iteration: 2224 lambda_k: 1 Loss: 0.03991852474283649\n",
      "Iteration: 2225 lambda_k: 1 Loss: 0.039848107287534566\n",
      "Iteration: 2226 lambda_k: 1 Loss: 0.039777721425950624\n",
      "Iteration: 2227 lambda_k: 1 Loss: 0.03970736714423067\n",
      "Iteration: 2228 lambda_k: 1 Loss: 0.039637044428528646\n",
      "Iteration: 2229 lambda_k: 1 Loss: 0.03956675326500676\n",
      "Iteration: 2230 lambda_k: 1 Loss: 0.039496493639835484\n",
      "Iteration: 2231 lambda_k: 1 Loss: 0.03942626553919327\n",
      "Iteration: 2232 lambda_k: 1 Loss: 0.039356068949267076\n",
      "Iteration: 2233 lambda_k: 1 Loss: 0.03928590385625185\n",
      "Iteration: 2234 lambda_k: 1 Loss: 0.03921577024635088\n",
      "Iteration: 2235 lambda_k: 1 Loss: 0.03914566810577559\n",
      "Iteration: 2236 lambda_k: 1 Loss: 0.039075597420745734\n",
      "Iteration: 2237 lambda_k: 1 Loss: 0.0390055581774893\n",
      "Iteration: 2238 lambda_k: 1 Loss: 0.038935550362242576\n",
      "Iteration: 2239 lambda_k: 1 Loss: 0.038865573961249976\n",
      "Iteration: 2240 lambda_k: 1 Loss: 0.038795628960764404\n",
      "Iteration: 2241 lambda_k: 1 Loss: 0.03872571534704694\n",
      "Iteration: 2242 lambda_k: 1 Loss: 0.03865583310636708\n",
      "Iteration: 2243 lambda_k: 1 Loss: 0.03858598222500247\n",
      "Iteration: 2244 lambda_k: 1 Loss: 0.03851616268923926\n",
      "Iteration: 2245 lambda_k: 1 Loss: 0.03844637448537191\n",
      "Iteration: 2246 lambda_k: 1 Loss: 0.03837661759970311\n",
      "Iteration: 2247 lambda_k: 1 Loss: 0.03830689201854396\n",
      "Iteration: 2248 lambda_k: 1 Loss: 0.0382371977282142\n",
      "Iteration: 2249 lambda_k: 1 Loss: 0.03816753471504152\n",
      "Iteration: 2250 lambda_k: 1 Loss: 0.03809790296536242\n",
      "Iteration: 2251 lambda_k: 1 Loss: 0.03802830246552161\n",
      "Iteration: 2252 lambda_k: 1 Loss: 0.037958733201872215\n",
      "Iteration: 2253 lambda_k: 1 Loss: 0.037889195160775865\n",
      "Iteration: 2254 lambda_k: 1 Loss: 0.03781968832860263\n",
      "Iteration: 2255 lambda_k: 1 Loss: 0.037750212691731115\n",
      "Iteration: 2256 lambda_k: 1 Loss: 0.03768076823654827\n",
      "Iteration: 2257 lambda_k: 1 Loss: 0.037611354949449644\n",
      "Iteration: 2258 lambda_k: 1 Loss: 0.03754197281683928\n",
      "Iteration: 2259 lambda_k: 1 Loss: 0.03747262182512973\n",
      "Iteration: 2260 lambda_k: 1 Loss: 0.037403301960742166\n",
      "Iteration: 2261 lambda_k: 1 Loss: 0.03733401321010611\n",
      "Iteration: 2262 lambda_k: 1 Loss: 0.03726475555965982\n",
      "Iteration: 2263 lambda_k: 1 Loss: 0.03719552899585011\n",
      "Iteration: 2264 lambda_k: 1 Loss: 0.0371263335051324\n",
      "Iteration: 2265 lambda_k: 1 Loss: 0.037057169073970685\n",
      "Iteration: 2266 lambda_k: 1 Loss: 0.036988035688837545\n",
      "Iteration: 2267 lambda_k: 1 Loss: 0.03691893333621432\n",
      "Iteration: 2268 lambda_k: 1 Loss: 0.03684986200259098\n",
      "Iteration: 2269 lambda_k: 1 Loss: 0.036780821674466\n",
      "Iteration: 2270 lambda_k: 1 Loss: 0.036711812338346736\n",
      "Iteration: 2271 lambda_k: 1 Loss: 0.03664283398074926\n",
      "Iteration: 2272 lambda_k: 1 Loss: 0.036573886594372454\n",
      "Iteration: 2273 lambda_k: 1 Loss: 0.036504970153683085\n",
      "Iteration: 2274 lambda_k: 1 Loss: 0.03643608465108087\n",
      "Iteration: 2275 lambda_k: 1 Loss: 0.03636723007313367\n",
      "Iteration: 2276 lambda_k: 1 Loss: 0.03629840640641289\n",
      "Iteration: 2277 lambda_k: 1 Loss: 0.03622961363748865\n",
      "Iteration: 2278 lambda_k: 1 Loss: 0.036160851752936485\n",
      "Iteration: 2279 lambda_k: 1 Loss: 0.036092120739341704\n",
      "Iteration: 2280 lambda_k: 1 Loss: 0.03602342058330004\n",
      "Iteration: 2281 lambda_k: 1 Loss: 0.03595475127141661\n",
      "Iteration: 2282 lambda_k: 1 Loss: 0.03588611279030527\n",
      "Iteration: 2283 lambda_k: 1 Loss: 0.03581750512658818\n",
      "Iteration: 2284 lambda_k: 1 Loss: 0.03574892826689645\n",
      "Iteration: 2285 lambda_k: 1 Loss: 0.03568038219786983\n",
      "Iteration: 2286 lambda_k: 1 Loss: 0.03561186690615714\n",
      "Iteration: 2287 lambda_k: 1 Loss: 0.03554338237841637\n",
      "Iteration: 2288 lambda_k: 1 Loss: 0.03547492860131449\n",
      "Iteration: 2289 lambda_k: 1 Loss: 0.035406505561527475\n",
      "Iteration: 2290 lambda_k: 1 Loss: 0.03533811324574032\n",
      "Iteration: 2291 lambda_k: 1 Loss: 0.035269751640646975\n",
      "Iteration: 2292 lambda_k: 1 Loss: 0.03520142073295073\n",
      "Iteration: 2293 lambda_k: 1 Loss: 0.03513312050936375\n",
      "Iteration: 2294 lambda_k: 1 Loss: 0.03506485095660741\n",
      "Iteration: 2295 lambda_k: 1 Loss: 0.03499661206141224\n",
      "Iteration: 2296 lambda_k: 1 Loss: 0.03492840381051808\n",
      "Iteration: 2297 lambda_k: 1 Loss: 0.03486022619067366\n",
      "Iteration: 2298 lambda_k: 1 Loss: 0.034792079188637276\n",
      "Iteration: 2299 lambda_k: 1 Loss: 0.034723962791176254\n",
      "Iteration: 2300 lambda_k: 1 Loss: 0.03465587698506728\n",
      "Iteration: 2301 lambda_k: 1 Loss: 0.03458782175709632\n",
      "Iteration: 2302 lambda_k: 1 Loss: 0.034519797094058564\n",
      "Iteration: 2303 lambda_k: 1 Loss: 0.03445180298275872\n",
      "Iteration: 2304 lambda_k: 1 Loss: 0.03438383941001072\n",
      "Iteration: 2305 lambda_k: 1 Loss: 0.03431590636263789\n",
      "Iteration: 2306 lambda_k: 1 Loss: 0.034248003827473145\n",
      "Iteration: 2307 lambda_k: 1 Loss: 0.03418013179135867\n",
      "Iteration: 2308 lambda_k: 1 Loss: 0.03411229024114608\n",
      "Iteration: 2309 lambda_k: 1 Loss: 0.03404447916369663\n",
      "Iteration: 2310 lambda_k: 1 Loss: 0.033976698545881104\n",
      "Iteration: 2311 lambda_k: 1 Loss: 0.03390894837457962\n",
      "Iteration: 2312 lambda_k: 1 Loss: 0.03384122863668219\n",
      "Iteration: 2313 lambda_k: 1 Loss: 0.03377353931908814\n",
      "Iteration: 2314 lambda_k: 1 Loss: 0.03370588040870663\n",
      "Iteration: 2315 lambda_k: 1 Loss: 0.03363825189245636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2316 lambda_k: 1 Loss: 0.03357065375726575\n",
      "Iteration: 2317 lambda_k: 1 Loss: 0.0335030859900729\n",
      "Iteration: 2318 lambda_k: 1 Loss: 0.033435548577825686\n",
      "Iteration: 2319 lambda_k: 1 Loss: 0.03336804150748184\n",
      "Iteration: 2320 lambda_k: 1 Loss: 0.03330056476600879\n",
      "Iteration: 2321 lambda_k: 1 Loss: 0.03323311834038378\n",
      "Iteration: 2322 lambda_k: 1 Loss: 0.033165702217594066\n",
      "Iteration: 2323 lambda_k: 1 Loss: 0.03309831638463664\n",
      "Iteration: 2324 lambda_k: 1 Loss: 0.033030960828518426\n",
      "Iteration: 2325 lambda_k: 1 Loss: 0.03296363553625647\n",
      "Iteration: 2326 lambda_k: 1 Loss: 0.03289634049487757\n",
      "Iteration: 2327 lambda_k: 1 Loss: 0.03282907569141884\n",
      "Iteration: 2328 lambda_k: 1 Loss: 0.032761841112927126\n",
      "Iteration: 2329 lambda_k: 1 Loss: 0.032694636746459685\n",
      "Iteration: 2330 lambda_k: 1 Loss: 0.03262746257908359\n",
      "Iteration: 2331 lambda_k: 1 Loss: 0.03256031859787626\n",
      "Iteration: 2332 lambda_k: 1 Loss: 0.032493204789925276\n",
      "Iteration: 2333 lambda_k: 1 Loss: 0.03242612114232845\n",
      "Iteration: 2334 lambda_k: 1 Loss: 0.03235906764219372\n",
      "Iteration: 2335 lambda_k: 1 Loss: 0.03229204427663943\n",
      "Iteration: 2336 lambda_k: 1 Loss: 0.032225051032794313\n",
      "Iteration: 2337 lambda_k: 1 Loss: 0.032158087897797435\n",
      "Iteration: 2338 lambda_k: 1 Loss: 0.03209115485879826\n",
      "Iteration: 2339 lambda_k: 1 Loss: 0.03202425190295649\n",
      "Iteration: 2340 lambda_k: 1 Loss: 0.03195737901744262\n",
      "Iteration: 2341 lambda_k: 1 Loss: 0.03189053618943753\n",
      "Iteration: 2342 lambda_k: 1 Loss: 0.03182372340613262\n",
      "Iteration: 2343 lambda_k: 1 Loss: 0.03175694065472997\n",
      "Iteration: 2344 lambda_k: 1 Loss: 0.03169018792244218\n",
      "Iteration: 2345 lambda_k: 1 Loss: 0.03162346519649268\n",
      "Iteration: 2346 lambda_k: 1 Loss: 0.03155677246411544\n",
      "Iteration: 2347 lambda_k: 1 Loss: 0.03149010970637591\n",
      "Iteration: 2348 lambda_k: 1 Loss: 0.03142347692260008\n",
      "Iteration: 2349 lambda_k: 1 Loss: 0.031356874094198454\n",
      "Iteration: 2350 lambda_k: 1 Loss: 0.03129030120843112\n",
      "Iteration: 2351 lambda_k: 1 Loss: 0.031223758252574736\n",
      "Iteration: 2352 lambda_k: 1 Loss: 0.03115724521392687\n",
      "Iteration: 2353 lambda_k: 1 Loss: 0.031090762079799807\n",
      "Iteration: 2354 lambda_k: 1 Loss: 0.03102430883751595\n",
      "Iteration: 2355 lambda_k: 1 Loss: 0.030957885474407146\n",
      "Iteration: 2356 lambda_k: 1 Loss: 0.030891491977816012\n",
      "Iteration: 2357 lambda_k: 1 Loss: 0.030825128335096427\n",
      "Iteration: 2358 lambda_k: 1 Loss: 0.030758794533613854\n",
      "Iteration: 2359 lambda_k: 1 Loss: 0.030692490560745474\n",
      "Iteration: 2360 lambda_k: 1 Loss: 0.030626216403879785\n",
      "Iteration: 2361 lambda_k: 1 Loss: 0.030559972050416613\n",
      "Iteration: 2362 lambda_k: 1 Loss: 0.030493757487767188\n",
      "Iteration: 2363 lambda_k: 1 Loss: 0.030427572703354323\n",
      "Iteration: 2364 lambda_k: 1 Loss: 0.030361417684612084\n",
      "Iteration: 2365 lambda_k: 1 Loss: 0.03029529241898641\n",
      "Iteration: 2366 lambda_k: 1 Loss: 0.03022919689393492\n",
      "Iteration: 2367 lambda_k: 1 Loss: 0.030163131096926792\n",
      "Iteration: 2368 lambda_k: 1 Loss: 0.03009709501544306\n",
      "Iteration: 2369 lambda_k: 1 Loss: 0.030031088636976504\n",
      "Iteration: 2370 lambda_k: 1 Loss: 0.02996511194903182\n",
      "Iteration: 2371 lambda_k: 1 Loss: 0.029899164939125554\n",
      "Iteration: 2372 lambda_k: 1 Loss: 0.029833247594786384\n",
      "Iteration: 2373 lambda_k: 1 Loss: 0.029767359903554825\n",
      "Iteration: 2374 lambda_k: 1 Loss: 0.02970150185298354\n",
      "Iteration: 2375 lambda_k: 1 Loss: 0.029635673430637464\n",
      "Iteration: 2376 lambda_k: 1 Loss: 0.029569874624093637\n",
      "Iteration: 2377 lambda_k: 1 Loss: 0.02950410542094131\n",
      "Iteration: 2378 lambda_k: 1 Loss: 0.02943836580878212\n",
      "Iteration: 2379 lambda_k: 1 Loss: 0.029372655775230117\n",
      "Iteration: 2380 lambda_k: 1 Loss: 0.029306975307911758\n",
      "Iteration: 2381 lambda_k: 1 Loss: 0.02924132439446598\n",
      "Iteration: 2382 lambda_k: 1 Loss: 0.029175703022544432\n",
      "Iteration: 2383 lambda_k: 1 Loss: 0.029110111179811264\n",
      "Iteration: 2384 lambda_k: 1 Loss: 0.0290445488539433\n",
      "Iteration: 2385 lambda_k: 1 Loss: 0.028979016032630256\n",
      "Iteration: 2386 lambda_k: 1 Loss: 0.028913512703574572\n",
      "Iteration: 2387 lambda_k: 1 Loss: 0.028848038854491758\n",
      "Iteration: 2388 lambda_k: 1 Loss: 0.028782594473110053\n",
      "Iteration: 2389 lambda_k: 1 Loss: 0.0287171795471709\n",
      "Iteration: 2390 lambda_k: 1 Loss: 0.02865179406442885\n",
      "Iteration: 2391 lambda_k: 1 Loss: 0.028586438012651556\n",
      "Iteration: 2392 lambda_k: 1 Loss: 0.028521111379619973\n",
      "Iteration: 2393 lambda_k: 1 Loss: 0.028455814153128412\n",
      "Iteration: 2394 lambda_k: 1 Loss: 0.02839054632098444\n",
      "Iteration: 2395 lambda_k: 1 Loss: 0.028325307871009207\n",
      "Iteration: 2396 lambda_k: 1 Loss: 0.028260098791037535\n",
      "Iteration: 2397 lambda_k: 1 Loss: 0.02819491906891761\n",
      "Iteration: 2398 lambda_k: 1 Loss: 0.028129768692511513\n",
      "Iteration: 2399 lambda_k: 1 Loss: 0.028064647649695094\n",
      "Iteration: 2400 lambda_k: 1 Loss: 0.027999555928357994\n",
      "Iteration: 2401 lambda_k: 1 Loss: 0.027934493516403792\n",
      "Iteration: 2402 lambda_k: 1 Loss: 0.02786946040175014\n",
      "Iteration: 2403 lambda_k: 1 Loss: 0.02780445657232883\n",
      "Iteration: 2404 lambda_k: 1 Loss: 0.027739482016085767\n",
      "Iteration: 2405 lambda_k: 1 Loss: 0.02767453672098119\n",
      "Iteration: 2406 lambda_k: 1 Loss: 0.027609620674989652\n",
      "Iteration: 2407 lambda_k: 1 Loss: 0.027544733866100214\n",
      "Iteration: 2408 lambda_k: 1 Loss: 0.02747987628231644\n",
      "Iteration: 2409 lambda_k: 1 Loss: 0.027415047911656526\n",
      "Iteration: 2410 lambda_k: 1 Loss: 0.02735024874215347\n",
      "Iteration: 2411 lambda_k: 1 Loss: 0.027285478761855043\n",
      "Iteration: 2412 lambda_k: 1 Loss: 0.02722073795882389\n",
      "Iteration: 2413 lambda_k: 1 Loss: 0.02715602632113766\n",
      "Iteration: 2414 lambda_k: 1 Loss: 0.02709134383688915\n",
      "Iteration: 2415 lambda_k: 1 Loss: 0.02702669049418629\n",
      "Iteration: 2416 lambda_k: 1 Loss: 0.02696206628115233\n",
      "Iteration: 2417 lambda_k: 1 Loss: 0.02689747118592597\n",
      "Iteration: 2418 lambda_k: 1 Loss: 0.026832905196661327\n",
      "Iteration: 2419 lambda_k: 1 Loss: 0.026768368301528213\n",
      "Iteration: 2420 lambda_k: 1 Loss: 0.026703860488712025\n",
      "Iteration: 2421 lambda_k: 1 Loss: 0.02663938174641408\n",
      "Iteration: 2422 lambda_k: 1 Loss: 0.02657493206285156\n",
      "Iteration: 2423 lambda_k: 1 Loss: 0.026510511426257693\n",
      "Iteration: 2424 lambda_k: 1 Loss: 0.026446119824881806\n",
      "Iteration: 2425 lambda_k: 1 Loss: 0.02638175724698951\n",
      "Iteration: 2426 lambda_k: 1 Loss: 0.02631742368086271\n",
      "Iteration: 2427 lambda_k: 1 Loss: 0.026253119114799925\n",
      "Iteration: 2428 lambda_k: 1 Loss: 0.02618884353711613\n",
      "Iteration: 2429 lambda_k: 1 Loss: 0.026124596936143076\n",
      "Iteration: 2430 lambda_k: 1 Loss: 0.02606037930022936\n",
      "Iteration: 2431 lambda_k: 1 Loss: 0.02599619061774041\n",
      "Iteration: 2432 lambda_k: 1 Loss: 0.025932030877058834\n",
      "Iteration: 2433 lambda_k: 1 Loss: 0.025867900066584463\n",
      "Iteration: 2434 lambda_k: 1 Loss: 0.02580379817473436\n",
      "Iteration: 2435 lambda_k: 1 Loss: 0.02573972518994313\n",
      "Iteration: 2436 lambda_k: 1 Loss: 0.025675681100662914\n",
      "Iteration: 2437 lambda_k: 1 Loss: 0.025611665895363424\n",
      "Iteration: 2438 lambda_k: 1 Loss: 0.025547679562532465\n",
      "Iteration: 2439 lambda_k: 1 Loss: 0.025483722090675637\n",
      "Iteration: 2440 lambda_k: 1 Loss: 0.02541979346831678\n",
      "Iteration: 2441 lambda_k: 1 Loss: 0.025355893683997897\n",
      "Iteration: 2442 lambda_k: 1 Loss: 0.025292022726279482\n",
      "Iteration: 2443 lambda_k: 1 Loss: 0.025228180583740503\n",
      "Iteration: 2444 lambda_k: 1 Loss: 0.025164367244978496\n",
      "Iteration: 2445 lambda_k: 1 Loss: 0.02510058269861001\n",
      "Iteration: 2446 lambda_k: 1 Loss: 0.025036826933270605\n",
      "Iteration: 2447 lambda_k: 1 Loss: 0.024973099937614783\n",
      "Iteration: 2448 lambda_k: 1 Loss: 0.024909401700316497\n",
      "Iteration: 2449 lambda_k: 1 Loss: 0.024845732210068982\n",
      "Iteration: 2450 lambda_k: 1 Loss: 0.024782091455585267\n",
      "Iteration: 2451 lambda_k: 1 Loss: 0.024718479425597923\n",
      "Iteration: 2452 lambda_k: 1 Loss: 0.024654896108859653\n",
      "Iteration: 2453 lambda_k: 1 Loss: 0.024591341494143062\n",
      "Iteration: 2454 lambda_k: 1 Loss: 0.02452781557024118\n",
      "Iteration: 2455 lambda_k: 1 Loss: 0.024464318325967273\n",
      "Iteration: 2456 lambda_k: 1 Loss: 0.02440084975015527\n",
      "Iteration: 2457 lambda_k: 1 Loss: 0.02433740983166\n",
      "Iteration: 2458 lambda_k: 1 Loss: 0.024273998559357055\n",
      "Iteration: 2459 lambda_k: 1 Loss: 0.02421061592214311\n",
      "Iteration: 2460 lambda_k: 1 Loss: 0.024147261908936436\n",
      "Iteration: 2461 lambda_k: 1 Loss: 0.024083936508676553\n",
      "Iteration: 2462 lambda_k: 1 Loss: 0.02402063971032467\n",
      "Iteration: 2463 lambda_k: 1 Loss: 0.0239573715028639\n",
      "Iteration: 2464 lambda_k: 1 Loss: 0.023894131875299447\n",
      "Iteration: 2465 lambda_k: 1 Loss: 0.023830920816658767\n",
      "Iteration: 2466 lambda_k: 1 Loss: 0.0237677383159918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2467 lambda_k: 1 Loss: 0.023704584362370978\n",
      "Iteration: 2468 lambda_k: 1 Loss: 0.023641458944891804\n",
      "Iteration: 2469 lambda_k: 1 Loss: 0.023578362052672722\n",
      "Iteration: 2470 lambda_k: 1 Loss: 0.02351529367485553\n",
      "Iteration: 2471 lambda_k: 1 Loss: 0.023452253800605515\n",
      "Iteration: 2472 lambda_k: 1 Loss: 0.023389242419111562\n",
      "Iteration: 2473 lambda_k: 1 Loss: 0.023326259519586612\n",
      "Iteration: 2474 lambda_k: 1 Loss: 0.023263305091267648\n",
      "Iteration: 2475 lambda_k: 1 Loss: 0.023200379123416145\n",
      "Iteration: 2476 lambda_k: 1 Loss: 0.023137481605318255\n",
      "Iteration: 2477 lambda_k: 1 Loss: 0.023074612526284715\n",
      "Iteration: 2478 lambda_k: 1 Loss: 0.02301177187565158\n",
      "Iteration: 2479 lambda_k: 1 Loss: 0.0229489596427802\n",
      "Iteration: 2480 lambda_k: 1 Loss: 0.022886175817057323\n",
      "Iteration: 2481 lambda_k: 1 Loss: 0.02282342038789579\n",
      "Iteration: 2482 lambda_k: 1 Loss: 0.022760693344734236\n",
      "Iteration: 2483 lambda_k: 1 Loss: 0.02269799467703783\n",
      "Iteration: 2484 lambda_k: 1 Loss: 0.022635324374298254\n",
      "Iteration: 2485 lambda_k: 1 Loss: 0.022572682426034\n",
      "Iteration: 2486 lambda_k: 1 Loss: 0.022510068821790772\n",
      "Iteration: 2487 lambda_k: 1 Loss: 0.022447483551141612\n",
      "Iteration: 2488 lambda_k: 1 Loss: 0.022384926603687183\n",
      "Iteration: 2489 lambda_k: 1 Loss: 0.022322397969056215\n",
      "Iteration: 2490 lambda_k: 1 Loss: 0.022259897636905605\n",
      "Iteration: 2491 lambda_k: 1 Loss: 0.022197425596920846\n",
      "Iteration: 2492 lambda_k: 1 Loss: 0.022134981838816143\n",
      "Iteration: 2493 lambda_k: 1 Loss: 0.022072566352334816\n",
      "Iteration: 2494 lambda_k: 1 Loss: 0.022010179127249736\n",
      "Iteration: 2495 lambda_k: 1 Loss: 0.02194782015336344\n",
      "Iteration: 2496 lambda_k: 1 Loss: 0.021885489420508445\n",
      "Iteration: 2497 lambda_k: 1 Loss: 0.02182318691854778\n",
      "Iteration: 2498 lambda_k: 1 Loss: 0.02176091263737499\n",
      "Iteration: 2499 lambda_k: 1 Loss: 0.021698666566914725\n",
      "Iteration: 2500 lambda_k: 1 Loss: 0.02163644869712311\n",
      "Iteration: 2501 lambda_k: 1 Loss: 0.021574259017987752\n",
      "Iteration: 2502 lambda_k: 1 Loss: 0.021512097519528345\n",
      "Iteration: 2503 lambda_k: 1 Loss: 0.021449964191797018\n",
      "Iteration: 2504 lambda_k: 1 Loss: 0.021387859024878712\n",
      "Iteration: 2505 lambda_k: 1 Loss: 0.02132578200889124\n",
      "Iteration: 2506 lambda_k: 1 Loss: 0.021263733133986162\n",
      "Iteration: 2507 lambda_k: 1 Loss: 0.02120171239034873\n",
      "Iteration: 2508 lambda_k: 1 Loss: 0.02113971976819843\n",
      "Iteration: 2509 lambda_k: 1 Loss: 0.02107775525778939\n",
      "Iteration: 2510 lambda_k: 1 Loss: 0.02101581884941072\n",
      "Iteration: 2511 lambda_k: 1 Loss: 0.020953910533386983\n",
      "Iteration: 2512 lambda_k: 1 Loss: 0.020892030300078437\n",
      "Iteration: 2513 lambda_k: 1 Loss: 0.020830178139881667\n",
      "Iteration: 2514 lambda_k: 1 Loss: 0.020768354043229825\n",
      "Iteration: 2515 lambda_k: 1 Loss: 0.020706558000593123\n",
      "Iteration: 2516 lambda_k: 1 Loss: 0.020644790002479343\n",
      "Iteration: 2517 lambda_k: 1 Loss: 0.020583050039434246\n",
      "Iteration: 2518 lambda_k: 1 Loss: 0.020521338102041806\n",
      "Iteration: 2519 lambda_k: 1 Loss: 0.020459654180924977\n",
      "Iteration: 2520 lambda_k: 1 Loss: 0.020397998266745987\n",
      "Iteration: 2521 lambda_k: 1 Loss: 0.02033637035020676\n",
      "Iteration: 2522 lambda_k: 1 Loss: 0.020274770422049577\n",
      "Iteration: 2523 lambda_k: 1 Loss: 0.020213198473057328\n",
      "Iteration: 2524 lambda_k: 1 Loss: 0.020151654494054334\n",
      "Iteration: 2525 lambda_k: 1 Loss: 0.02009013847590654\n",
      "Iteration: 2526 lambda_k: 1 Loss: 0.020028650409522054\n",
      "Iteration: 2527 lambda_k: 1 Loss: 0.01996719028585189\n",
      "Iteration: 2528 lambda_k: 1 Loss: 0.019905758095890295\n",
      "Iteration: 2529 lambda_k: 1 Loss: 0.01984435383067543\n",
      "Iteration: 2530 lambda_k: 1 Loss: 0.019782977481289776\n",
      "Iteration: 2531 lambda_k: 1 Loss: 0.019721629038860704\n",
      "Iteration: 2532 lambda_k: 1 Loss: 0.019660308494561143\n",
      "Iteration: 2533 lambda_k: 1 Loss: 0.019599015839610157\n",
      "Iteration: 2534 lambda_k: 1 Loss: 0.01953775106527346\n",
      "Iteration: 2535 lambda_k: 1 Loss: 0.019476514162864087\n",
      "Iteration: 2536 lambda_k: 1 Loss: 0.01941530512374293\n",
      "Iteration: 2537 lambda_k: 1 Loss: 0.019354123939319238\n",
      "Iteration: 2538 lambda_k: 1 Loss: 0.019292970601051628\n",
      "Iteration: 2539 lambda_k: 1 Loss: 0.0192318451004483\n",
      "Iteration: 2540 lambda_k: 1 Loss: 0.019170747429067967\n",
      "Iteration: 2541 lambda_k: 1 Loss: 0.019109677578520362\n",
      "Iteration: 2542 lambda_k: 1 Loss: 0.01904863554046706\n",
      "Iteration: 2543 lambda_k: 1 Loss: 0.01898762130662211\n",
      "Iteration: 2544 lambda_k: 1 Loss: 0.018926634868752613\n",
      "Iteration: 2545 lambda_k: 1 Loss: 0.01886567621867967\n",
      "Iteration: 2546 lambda_k: 1 Loss: 0.01880474534827884\n",
      "Iteration: 2547 lambda_k: 1 Loss: 0.018743842249481096\n",
      "Iteration: 2548 lambda_k: 1 Loss: 0.018682966914273506\n",
      "Iteration: 2549 lambda_k: 1 Loss: 0.01862211933469989\n",
      "Iteration: 2550 lambda_k: 1 Loss: 0.018561299502861818\n",
      "Iteration: 2551 lambda_k: 1 Loss: 0.018500507410919142\n",
      "Iteration: 2552 lambda_k: 1 Loss: 0.018439743051091154\n",
      "Iteration: 2553 lambda_k: 1 Loss: 0.01837900641565697\n",
      "Iteration: 2554 lambda_k: 1 Loss: 0.0183182974969568\n",
      "Iteration: 2555 lambda_k: 1 Loss: 0.018257616287392534\n",
      "Iteration: 2556 lambda_k: 1 Loss: 0.018196962779428658\n",
      "Iteration: 2557 lambda_k: 1 Loss: 0.018136336965593188\n",
      "Iteration: 2558 lambda_k: 1 Loss: 0.018075738838478652\n",
      "Iteration: 2559 lambda_k: 1 Loss: 0.018015168390742718\n",
      "Iteration: 2560 lambda_k: 1 Loss: 0.017954625615109503\n",
      "Iteration: 2561 lambda_k: 1 Loss: 0.017894110504370314\n",
      "Iteration: 2562 lambda_k: 1 Loss: 0.01783362305138465\n",
      "Iteration: 2563 lambda_k: 1 Loss: 0.017773163249081205\n",
      "Iteration: 2564 lambda_k: 1 Loss: 0.01771273109045892\n",
      "Iteration: 2565 lambda_k: 1 Loss: 0.017652326568588003\n",
      "Iteration: 2566 lambda_k: 1 Loss: 0.01759194967661084\n",
      "Iteration: 2567 lambda_k: 1 Loss: 0.017531600407743188\n",
      "Iteration: 2568 lambda_k: 1 Loss: 0.01747127875527532\n",
      "Iteration: 2569 lambda_k: 1 Loss: 0.017410984712573144\n",
      "Iteration: 2570 lambda_k: 1 Loss: 0.017350718273079192\n",
      "Iteration: 2571 lambda_k: 1 Loss: 0.0172904794303139\n",
      "Iteration: 2572 lambda_k: 1 Loss: 0.01723026817787669\n",
      "Iteration: 2573 lambda_k: 1 Loss: 0.017170084509447264\n",
      "Iteration: 2574 lambda_k: 1 Loss: 0.01710992841878682\n",
      "Iteration: 2575 lambda_k: 1 Loss: 0.01704979989973923\n",
      "Iteration: 2576 lambda_k: 1 Loss: 0.016989698946232357\n",
      "Iteration: 2577 lambda_k: 1 Loss: 0.01692962555227941\n",
      "Iteration: 2578 lambda_k: 1 Loss: 0.0168695797119802\n",
      "Iteration: 2579 lambda_k: 1 Loss: 0.016809561419522386\n",
      "Iteration: 2580 lambda_k: 1 Loss: 0.016749570669183273\n",
      "Iteration: 2581 lambda_k: 1 Loss: 0.016689607455330643\n",
      "Iteration: 2582 lambda_k: 1 Loss: 0.01662967177242465\n",
      "Iteration: 2583 lambda_k: 1 Loss: 0.01656976361501909\n",
      "Iteration: 2584 lambda_k: 1 Loss: 0.01650988297776278\n",
      "Iteration: 2585 lambda_k: 1 Loss: 0.016450029855401456\n",
      "Iteration: 2586 lambda_k: 1 Loss: 0.016390204242779035\n",
      "Iteration: 2587 lambda_k: 1 Loss: 0.016330406134839118\n",
      "Iteration: 2588 lambda_k: 1 Loss: 0.01627063552662691\n",
      "Iteration: 2589 lambda_k: 1 Loss: 0.016210892413290684\n",
      "Iteration: 2590 lambda_k: 1 Loss: 0.016151176790083516\n",
      "Iteration: 2591 lambda_k: 1 Loss: 0.016091488652365017\n",
      "Iteration: 2592 lambda_k: 1 Loss: 0.01603182799560309\n",
      "Iteration: 2593 lambda_k: 1 Loss: 0.015972194815375675\n",
      "Iteration: 2594 lambda_k: 1 Loss: 0.015912589107372716\n",
      "Iteration: 2595 lambda_k: 1 Loss: 0.01585301086739789\n",
      "Iteration: 2596 lambda_k: 1 Loss: 0.015793460091370646\n",
      "Iteration: 2597 lambda_k: 1 Loss: 0.015733936775327955\n",
      "Iteration: 2598 lambda_k: 1 Loss: 0.015674440915426645\n",
      "Iteration: 2599 lambda_k: 1 Loss: 0.015614972507945147\n",
      "Iteration: 2600 lambda_k: 1 Loss: 0.015555531549285762\n",
      "Iteration: 2601 lambda_k: 1 Loss: 0.015496118035976593\n",
      "Iteration: 2602 lambda_k: 1 Loss: 0.015436731964673786\n",
      "Iteration: 2603 lambda_k: 1 Loss: 0.015377373332163874\n",
      "Iteration: 2604 lambda_k: 1 Loss: 0.015318042135366076\n",
      "Iteration: 2605 lambda_k: 1 Loss: 0.01525873837133427\n",
      "Iteration: 2606 lambda_k: 1 Loss: 0.015199462037259807\n",
      "Iteration: 2607 lambda_k: 1 Loss: 0.015140213130473602\n",
      "Iteration: 2608 lambda_k: 1 Loss: 0.015080991648448828\n",
      "Iteration: 2609 lambda_k: 1 Loss: 0.015021797588803135\n",
      "Iteration: 2610 lambda_k: 1 Loss: 0.014962630949301767\n",
      "Iteration: 2611 lambda_k: 1 Loss: 0.014903491727859696\n",
      "Iteration: 2612 lambda_k: 1 Loss: 0.01484437992254447\n",
      "Iteration: 2613 lambda_k: 1 Loss: 0.014785295531579153\n",
      "Iteration: 2614 lambda_k: 1 Loss: 0.01472623855334493\n",
      "Iteration: 2615 lambda_k: 1 Loss: 0.014667208986384192\n",
      "Iteration: 2616 lambda_k: 1 Loss: 0.014608206829403213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2617 lambda_k: 1 Loss: 0.014549232081275626\n",
      "Iteration: 2618 lambda_k: 1 Loss: 0.014490284741044985\n",
      "Iteration: 2619 lambda_k: 1 Loss: 0.014431364807928261\n",
      "Iteration: 2620 lambda_k: 1 Loss: 0.014372472281318953\n",
      "Iteration: 2621 lambda_k: 1 Loss: 0.014313607160790602\n",
      "Iteration: 2622 lambda_k: 1 Loss: 0.014254769446099899\n",
      "Iteration: 2623 lambda_k: 1 Loss: 0.014195959137190288\n",
      "Iteration: 2624 lambda_k: 1 Loss: 0.014137176234195485\n",
      "Iteration: 2625 lambda_k: 1 Loss: 0.014078420737443179\n",
      "Iteration: 2626 lambda_k: 1 Loss: 0.01401969264745862\n",
      "Iteration: 2627 lambda_k: 1 Loss: 0.013960991964968637\n",
      "Iteration: 2628 lambda_k: 1 Loss: 0.013902318690905215\n",
      "Iteration: 2629 lambda_k: 1 Loss: 0.013843672826409874\n",
      "Iteration: 2630 lambda_k: 1 Loss: 0.013785054372837318\n",
      "Iteration: 2631 lambda_k: 1 Loss: 0.01372646333175991\n",
      "Iteration: 2632 lambda_k: 1 Loss: 0.01366789970497202\n",
      "Iteration: 2633 lambda_k: 1 Loss: 0.013609363494494226\n",
      "Iteration: 2634 lambda_k: 1 Loss: 0.013550854702577683\n",
      "Iteration: 2635 lambda_k: 1 Loss: 0.013492373331709162\n",
      "Iteration: 2636 lambda_k: 1 Loss: 0.01343391938461532\n",
      "Iteration: 2637 lambda_k: 1 Loss: 0.013375492864267717\n",
      "Iteration: 2638 lambda_k: 1 Loss: 0.013317093773887928\n",
      "Iteration: 2639 lambda_k: 1 Loss: 0.013258722116952325\n",
      "Iteration: 2640 lambda_k: 1 Loss: 0.01320037789719743\n",
      "Iteration: 2641 lambda_k: 1 Loss: 0.013142061118625212\n",
      "Iteration: 2642 lambda_k: 1 Loss: 0.013083771785508593\n",
      "Iteration: 2643 lambda_k: 1 Loss: 0.01302550990239708\n",
      "Iteration: 2644 lambda_k: 1 Loss: 0.01296727547412233\n",
      "Iteration: 2645 lambda_k: 1 Loss: 0.012909068505804172\n",
      "Iteration: 2646 lambda_k: 1 Loss: 0.012850889002856757\n",
      "Iteration: 2647 lambda_k: 1 Loss: 0.012792736970994532\n",
      "Iteration: 2648 lambda_k: 1 Loss: 0.012734612416238588\n",
      "Iteration: 2649 lambda_k: 1 Loss: 0.012676515344923553\n",
      "Iteration: 2650 lambda_k: 1 Loss: 0.012618445763703762\n",
      "Iteration: 2651 lambda_k: 1 Loss: 0.012560403679560432\n",
      "Iteration: 2652 lambda_k: 1 Loss: 0.012502389099808688\n",
      "Iteration: 2653 lambda_k: 1 Loss: 0.012444402032104663\n",
      "Iteration: 2654 lambda_k: 1 Loss: 0.012386442484452996\n",
      "Iteration: 2655 lambda_k: 1 Loss: 0.01232851046521445\n",
      "Iteration: 2656 lambda_k: 1 Loss: 0.012270605983113764\n",
      "Iteration: 2657 lambda_k: 1 Loss: 0.012212729047247536\n",
      "Iteration: 2658 lambda_k: 1 Loss: 0.01215487966709248\n",
      "Iteration: 2659 lambda_k: 1 Loss: 0.012097057852514094\n",
      "Iteration: 2660 lambda_k: 1 Loss: 0.012039263613775168\n",
      "Iteration: 2661 lambda_k: 1 Loss: 0.01198149696154474\n",
      "Iteration: 2662 lambda_k: 1 Loss: 0.01192375790690737\n",
      "Iteration: 2663 lambda_k: 1 Loss: 0.01186604646137227\n",
      "Iteration: 2664 lambda_k: 1 Loss: 0.01180836263688326\n",
      "Iteration: 2665 lambda_k: 1 Loss: 0.01175070644582877\n",
      "Iteration: 2666 lambda_k: 1 Loss: 0.011693077901051764\n",
      "Iteration: 2667 lambda_k: 1 Loss: 0.011635477015860485\n",
      "Iteration: 2668 lambda_k: 1 Loss: 0.011577903804039185\n",
      "Iteration: 2669 lambda_k: 1 Loss: 0.011520358279859323\n",
      "Iteration: 2670 lambda_k: 1 Loss: 0.011462840458090814\n",
      "Iteration: 2671 lambda_k: 1 Loss: 0.011405350354013787\n",
      "Iteration: 2672 lambda_k: 1 Loss: 0.011347887983430973\n",
      "Iteration: 2673 lambda_k: 1 Loss: 0.01129045336267979\n",
      "Iteration: 2674 lambda_k: 1 Loss: 0.011233046508645182\n",
      "Iteration: 2675 lambda_k: 1 Loss: 0.011175667438772957\n",
      "Iteration: 2676 lambda_k: 1 Loss: 0.01111831617108315\n",
      "Iteration: 2677 lambda_k: 1 Loss: 0.01106099272418412\n",
      "Iteration: 2678 lambda_k: 1 Loss: 0.011003697117286607\n",
      "Iteration: 2679 lambda_k: 1 Loss: 0.010946429370218913\n",
      "Iteration: 2680 lambda_k: 1 Loss: 0.010889189503441738\n",
      "Iteration: 2681 lambda_k: 1 Loss: 0.010831977538063987\n",
      "Iteration: 2682 lambda_k: 1 Loss: 0.010774793495859008\n",
      "Iteration: 2683 lambda_k: 1 Loss: 0.010717637399280928\n",
      "Iteration: 2684 lambda_k: 1 Loss: 0.01066050927148203\n",
      "Iteration: 2685 lambda_k: 1 Loss: 0.010603409136330193\n",
      "Iteration: 2686 lambda_k: 1 Loss: 0.01054633701842719\n",
      "Iteration: 2687 lambda_k: 1 Loss: 0.010489292943127303\n",
      "Iteration: 2688 lambda_k: 1 Loss: 0.010432276936556772\n",
      "Iteration: 2689 lambda_k: 1 Loss: 0.010375289025633483\n",
      "Iteration: 2690 lambda_k: 1 Loss: 0.010318329238087613\n",
      "Iteration: 2691 lambda_k: 1 Loss: 0.010261397602482897\n",
      "Iteration: 2692 lambda_k: 1 Loss: 0.01020449414823819\n",
      "Iteration: 2693 lambda_k: 1 Loss: 0.010147618905650054\n",
      "Iteration: 2694 lambda_k: 1 Loss: 0.010090771905916238\n",
      "Iteration: 2695 lambda_k: 1 Loss: 0.010033953181159218\n",
      "Iteration: 2696 lambda_k: 1 Loss: 0.009977162764451271\n",
      "Iteration: 2697 lambda_k: 1 Loss: 0.009920400689839829\n",
      "Iteration: 2698 lambda_k: 1 Loss: 0.009863666992373801\n",
      "Iteration: 2699 lambda_k: 1 Loss: 0.00980696170813094\n",
      "Iteration: 2700 lambda_k: 1 Loss: 0.009750284874245795\n",
      "Iteration: 2701 lambda_k: 1 Loss: 0.009693636528938815\n",
      "Iteration: 2702 lambda_k: 1 Loss: 0.0096370167115463\n",
      "Iteration: 2703 lambda_k: 1 Loss: 0.009580425462551289\n",
      "Iteration: 2704 lambda_k: 1 Loss: 0.00952386282361574\n",
      "Iteration: 2705 lambda_k: 1 Loss: 0.009467328837613483\n",
      "Iteration: 2706 lambda_k: 1 Loss: 0.009410823548664505\n",
      "Iteration: 2707 lambda_k: 1 Loss: 0.00935434700217017\n",
      "Iteration: 2708 lambda_k: 1 Loss: 0.00929789924484991\n",
      "Iteration: 2709 lambda_k: 1 Loss: 0.009241480324778957\n",
      "Iteration: 2710 lambda_k: 1 Loss: 0.009185090291427583\n",
      "Iteration: 2711 lambda_k: 1 Loss: 0.00912872919570141\n",
      "Iteration: 2712 lambda_k: 1 Loss: 0.009072397089983393\n",
      "Iteration: 2713 lambda_k: 1 Loss: 0.009016094028177277\n",
      "Iteration: 2714 lambda_k: 1 Loss: 0.008959820065752116\n",
      "Iteration: 2715 lambda_k: 1 Loss: 0.00890357525978918\n",
      "Iteration: 2716 lambda_k: 1 Loss: 0.00884735966902972\n",
      "Iteration: 2717 lambda_k: 1 Loss: 0.0087911733539251\n",
      "Iteration: 2718 lambda_k: 1 Loss: 0.008735016376688264\n",
      "Iteration: 2719 lambda_k: 1 Loss: 0.008678888801347193\n",
      "Iteration: 2720 lambda_k: 1 Loss: 0.008622790693800652\n",
      "Iteration: 2721 lambda_k: 1 Loss: 0.008566722121875318\n",
      "Iteration: 2722 lambda_k: 1 Loss: 0.008510683155385616\n",
      "Iteration: 2723 lambda_k: 1 Loss: 0.008454673866195521\n",
      "Iteration: 2724 lambda_k: 1 Loss: 0.008398694328282651\n",
      "Iteration: 2725 lambda_k: 1 Loss: 0.008342744617804796\n",
      "Iteration: 2726 lambda_k: 1 Loss: 0.008286824813168973\n",
      "Iteration: 2727 lambda_k: 1 Loss: 0.00823093499510309\n",
      "Iteration: 2728 lambda_k: 1 Loss: 0.008175075246730312\n",
      "Iteration: 2729 lambda_k: 1 Loss: 0.00811924565364624\n",
      "Iteration: 2730 lambda_k: 1 Loss: 0.008063446303999333\n",
      "Iteration: 2731 lambda_k: 1 Loss: 0.008007677288574093\n",
      "Iteration: 2732 lambda_k: 1 Loss: 0.007951938700877808\n",
      "Iteration: 2733 lambda_k: 1 Loss: 0.00789623063723044\n",
      "Iteration: 2734 lambda_k: 1 Loss: 0.007840553196858285\n",
      "Iteration: 2735 lambda_k: 1 Loss: 0.007784906481991172\n",
      "Iteration: 2736 lambda_k: 1 Loss: 0.007729290597963949\n",
      "Iteration: 2737 lambda_k: 1 Loss: 0.007673705653321311\n",
      "Iteration: 2738 lambda_k: 1 Loss: 0.0076181517599278195\n",
      "Iteration: 2739 lambda_k: 1 Loss: 0.007562629033081694\n",
      "Iteration: 2740 lambda_k: 1 Loss: 0.007507137591633438\n",
      "Iteration: 2741 lambda_k: 1 Loss: 0.00745167755810965\n",
      "Iteration: 2742 lambda_k: 1 Loss: 0.007396249058841858\n",
      "Iteration: 2743 lambda_k: 1 Loss: 0.007340852224100366\n",
      "Iteration: 2744 lambda_k: 1 Loss: 0.007285487188234406\n",
      "Iteration: 2745 lambda_k: 1 Loss: 0.007230154089817586\n",
      "Iteration: 2746 lambda_k: 1 Loss: 0.007174853071800196\n",
      "Iteration: 2747 lambda_k: 1 Loss: 0.007119584281667558\n",
      "Iteration: 2748 lambda_k: 1 Loss: 0.007064347871605285\n",
      "Iteration: 2749 lambda_k: 1 Loss: 0.0070091439986721335\n",
      "Iteration: 2750 lambda_k: 1 Loss: 0.006953972824979883\n",
      "Iteration: 2751 lambda_k: 1 Loss: 0.006898834517881716\n",
      "Iteration: 2752 lambda_k: 1 Loss: 0.0068437292501685185\n",
      "Iteration: 2753 lambda_k: 1 Loss: 0.006788657200274359\n",
      "Iteration: 2754 lambda_k: 1 Loss: 0.006733618552490823\n",
      "Iteration: 2755 lambda_k: 1 Loss: 0.006678613497191403\n",
      "Iteration: 2756 lambda_k: 1 Loss: 0.006623642231066078\n",
      "Iteration: 2757 lambda_k: 1 Loss: 0.006568704957366695\n",
      "Iteration: 2758 lambda_k: 1 Loss: 0.006513801886163643\n",
      "Iteration: 2759 lambda_k: 1 Loss: 0.0064589332346145925\n",
      "Iteration: 2760 lambda_k: 1 Loss: 0.006404099227245865\n",
      "Iteration: 2761 lambda_k: 1 Loss: 0.006349300096247379\n",
      "Iteration: 2762 lambda_k: 1 Loss: 0.006294536081781272\n",
      "Iteration: 2763 lambda_k: 1 Loss: 0.006239807432305497\n",
      "Iteration: 2764 lambda_k: 1 Loss: 0.00618511440491332\n",
      "Iteration: 2765 lambda_k: 1 Loss: 0.0061304572656894155\n",
      "Iteration: 2766 lambda_k: 1 Loss: 0.006075836290083076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2767 lambda_k: 1 Loss: 0.006021251763300348\n",
      "Iteration: 2768 lambda_k: 1 Loss: 0.0059667039807156505\n",
      "Iteration: 2769 lambda_k: 1 Loss: 0.005912193248304028\n",
      "Iteration: 2770 lambda_k: 1 Loss: 0.005857719889830873\n",
      "Iteration: 2771 lambda_k: 1 Loss: 0.005803284221115246\n",
      "Iteration: 2772 lambda_k: 1 Loss: 0.005748886588726886\n",
      "Iteration: 2773 lambda_k: 1 Loss: 0.005694527345860234\n",
      "Iteration: 2774 lambda_k: 1 Loss: 0.005640206858813389\n",
      "Iteration: 2775 lambda_k: 1 Loss: 0.005585925507570961\n",
      "Iteration: 2776 lambda_k: 1 Loss: 0.0055316836864324\n",
      "Iteration: 2777 lambda_k: 1 Loss: 0.005477481804669772\n",
      "Iteration: 2778 lambda_k: 1 Loss: 0.005423320287214214\n",
      "Iteration: 2779 lambda_k: 1 Loss: 0.005369199575376907\n",
      "Iteration: 2780 lambda_k: 1 Loss: 0.005315120127609934\n",
      "Iteration: 2781 lambda_k: 1 Loss: 0.005261082420310838\n",
      "Iteration: 2782 lambda_k: 1 Loss: 0.005207086948672298\n",
      "Iteration: 2783 lambda_k: 1 Loss: 0.005153134227580891\n",
      "Iteration: 2784 lambda_k: 1 Loss: 0.005099224792566836\n",
      "Iteration: 2785 lambda_k: 1 Loss: 0.005045359200808542\n",
      "Iteration: 2786 lambda_k: 1 Loss: 0.0049915380321953395\n",
      "Iteration: 2787 lambda_k: 1 Loss: 0.004937761890453294\n",
      "Iteration: 2788 lambda_k: 1 Loss: 0.004884031404337195\n",
      "Iteration: 2789 lambda_k: 1 Loss: 0.004830347228894811\n",
      "Iteration: 2790 lambda_k: 1 Loss: 0.004776710046807213\n",
      "Iteration: 2791 lambda_k: 1 Loss: 0.004723120569811375\n",
      "Iteration: 2792 lambda_k: 1 Loss: 0.00466957954021067\n",
      "Iteration: 2793 lambda_k: 1 Loss: 0.004616087732480092\n",
      "Iteration: 2794 lambda_k: 1 Loss: 0.004562645954972092\n",
      "Iteration: 2795 lambda_k: 1 Loss: 0.004509255051732108\n",
      "Iteration: 2796 lambda_k: 1 Loss: 0.004455915904430638\n",
      "Iteration: 2797 lambda_k: 1 Loss: 0.004402629434422168\n",
      "Iteration: 2798 lambda_k: 1 Loss: 0.004349396604938892\n",
      "Iteration: 2799 lambda_k: 1 Loss: 0.004296218423432151\n",
      "Iteration: 2800 lambda_k: 1 Loss: 0.004243095944070872\n",
      "Iteration: 2801 lambda_k: 1 Loss: 0.004190030270412058\n",
      "Iteration: 2802 lambda_k: 1 Loss: 0.004137022558254981\n",
      "Iteration: 2803 lambda_k: 1 Loss: 0.004084074018696826\n",
      "Iteration: 2804 lambda_k: 1 Loss: 0.004031185921404582\n",
      "Iteration: 2805 lambda_k: 1 Loss: 0.0039783595981236096\n",
      "Iteration: 2806 lambda_k: 1 Loss: 0.003925596446442392\n",
      "Iteration: 2807 lambda_k: 1 Loss: 0.0038728979338365317\n",
      "Iteration: 2808 lambda_k: 1 Loss: 0.0038202656020176065\n",
      "Iteration: 2809 lambda_k: 1 Loss: 0.003767701071615312\n",
      "Iteration: 2810 lambda_k: 1 Loss: 0.0037152060472251222\n",
      "Iteration: 2811 lambda_k: 1 Loss: 0.003662782322861152\n",
      "Iteration: 2812 lambda_k: 1 Loss: 0.0036104317878623217\n",
      "Iteration: 2813 lambda_k: 1 Loss: 0.003558156433325921\n",
      "Iteration: 2814 lambda_k: 1 Loss: 0.0035059583592160193\n",
      "Iteration: 2815 lambda_k: 1 Loss: 0.0034538397826236176\n",
      "Iteration: 2816 lambda_k: 1 Loss: 0.003401803050568236\n",
      "Iteration: 2817 lambda_k: 1 Loss: 0.0033498508143178928\n",
      "Iteration: 2818 lambda_k: 1 Loss: 0.003303734526291787\n",
      "Iteration: 2819 lambda_k: 1 Loss: 0.0032602958935316314\n",
      "Iteration: 2820 lambda_k: 1 Loss: 0.0032186676140018\n",
      "Iteration: 2821 lambda_k: 1 Loss: 0.0031787587579385286\n",
      "Iteration: 2822 lambda_k: 1 Loss: 0.003140313235677131\n",
      "Iteration: 2823 lambda_k: 1 Loss: 0.0031030765402993337\n",
      "Iteration: 2824 lambda_k: 1 Loss: 0.0030668845053977976\n",
      "Iteration: 2825 lambda_k: 1 Loss: 0.0030316704821973977\n",
      "Iteration: 2826 lambda_k: 1 Loss: 0.00299742759108432\n",
      "Iteration: 2827 lambda_k: 1 Loss: 0.0029641657413154803\n",
      "Iteration: 2828 lambda_k: 1 Loss: 0.0029318846820087802\n",
      "Iteration: 2829 lambda_k: 1 Loss: 0.002900566014443876\n",
      "Iteration: 2830 lambda_k: 1 Loss: 0.002870177448535674\n",
      "Iteration: 2831 lambda_k: 1 Loss: 0.0028406810557531728\n",
      "Iteration: 2832 lambda_k: 1 Loss: 0.0028120399846234333\n",
      "Iteration: 2833 lambda_k: 1 Loss: 0.002784221660742147\n",
      "Iteration: 2834 lambda_k: 1 Loss: 0.0027571980356036023\n",
      "Iteration: 2835 lambda_k: 1 Loss: 0.002730944285147756\n",
      "Iteration: 2836 lambda_k: 1 Loss: 0.002705437320136532\n",
      "Iteration: 2837 lambda_k: 1 Loss: 0.0026806547697107407\n",
      "Iteration: 2838 lambda_k: 1 Loss: 0.002656574578538392\n",
      "Iteration: 2839 lambda_k: 1 Loss: 0.0026331750348836075\n",
      "Iteration: 2840 lambda_k: 1 Loss: 0.0026104349755460845\n",
      "Iteration: 2841 lambda_k: 1 Loss: 0.0025883339787997144\n",
      "Iteration: 2842 lambda_k: 1 Loss: 0.0025668524608707458\n",
      "Iteration: 2843 lambda_k: 1 Loss: 0.0025459716739324586\n",
      "Iteration: 2844 lambda_k: 1 Loss: 0.0025256736438357043\n",
      "Iteration: 2845 lambda_k: 1 Loss: 0.002505941090271831\n",
      "Iteration: 2846 lambda_k: 1 Loss: 0.0024867574478033695\n",
      "Iteration: 2847 lambda_k: 1 Loss: 0.002468106568055149\n",
      "Iteration: 2848 lambda_k: 1 Loss: 0.0024499729136248038\n",
      "Iteration: 2849 lambda_k: 1 Loss: 0.00243234146022191\n",
      "Iteration: 2850 lambda_k: 1 Loss: 0.00241519770459027\n",
      "Iteration: 2851 lambda_k: 1 Loss: 0.0023985276427970405\n",
      "Iteration: 2852 lambda_k: 1 Loss: 0.0023823177435256344\n",
      "Iteration: 2853 lambda_k: 1 Loss: 0.0023665549218962463\n",
      "Iteration: 2854 lambda_k: 1 Loss: 0.0023512265047415295\n",
      "Iteration: 2855 lambda_k: 1 Loss: 0.0023363202284248526\n",
      "Iteration: 2856 lambda_k: 1 Loss: 0.0023218242373149018\n",
      "Iteration: 2857 lambda_k: 1 Loss: 0.002307727034240477\n",
      "Iteration: 2858 lambda_k: 1 Loss: 0.002294017474426407\n",
      "Iteration: 2859 lambda_k: 1 Loss: 0.00228068475115928\n",
      "Iteration: 2860 lambda_k: 1 Loss: 0.002267718383894772\n",
      "Iteration: 2861 lambda_k: 1 Loss: 0.0022551082065263126\n",
      "Iteration: 2862 lambda_k: 1 Loss: 0.002242844355452866\n",
      "Iteration: 2863 lambda_k: 1 Loss: 0.0022309172577789035\n",
      "Iteration: 2864 lambda_k: 1 Loss: 0.0022193176200277186\n",
      "Iteration: 2865 lambda_k: 1 Loss: 0.0022080364175777745\n",
      "Iteration: 2866 lambda_k: 1 Loss: 0.0021970648848570343\n",
      "Iteration: 2867 lambda_k: 1 Loss: 0.002186394506218736\n",
      "Iteration: 2868 lambda_k: 1 Loss: 0.002176017007383377\n",
      "Iteration: 2869 lambda_k: 1 Loss: 0.002165924347342963\n",
      "Iteration: 2870 lambda_k: 1 Loss: 0.0021561087106593147\n",
      "Iteration: 2871 lambda_k: 1 Loss: 0.002146562500120339\n",
      "Iteration: 2872 lambda_k: 1 Loss: 0.00213727832973704\n",
      "Iteration: 2873 lambda_k: 1 Loss: 0.00212824901806724\n",
      "Iteration: 2874 lambda_k: 1 Loss: 0.0021194675818506\n",
      "Iteration: 2875 lambda_k: 1 Loss: 0.002110927229932952\n",
      "Iteration: 2876 lambda_k: 1 Loss: 0.0021026213574560602\n",
      "Iteration: 2877 lambda_k: 1 Loss: 0.0020945435402236214\n",
      "Iteration: 2878 lambda_k: 1 Loss: 0.0020866875294903117\n",
      "Iteration: 2879 lambda_k: 1 Loss: 0.002079047246742271\n",
      "Iteration: 2880 lambda_k: 1 Loss: 0.0020716167787659555\n",
      "Iteration: 2881 lambda_k: 1 Loss: 0.0020643903728640874\n",
      "Iteration: 2882 lambda_k: 1 Loss: 0.0020573624322348227\n",
      "Iteration: 2883 lambda_k: 1 Loss: 0.0020505275115018503\n",
      "Iteration: 2884 lambda_k: 1 Loss: 0.002043880703292519\n",
      "Iteration: 2885 lambda_k: 1 Loss: 0.0020374164436963507\n",
      "Iteration: 2886 lambda_k: 1 Loss: 0.002031129791453286\n",
      "Iteration: 2887 lambda_k: 1 Loss: 0.002025015831439301\n",
      "Iteration: 2888 lambda_k: 1 Loss: 0.0020190698187698393\n",
      "Iteration: 2889 lambda_k: 1 Loss: 0.002013287159098959\n",
      "Iteration: 2890 lambda_k: 1 Loss: 0.0020076633209558273\n",
      "Iteration: 2891 lambda_k: 1 Loss: 0.0020021940216155398\n",
      "Iteration: 2892 lambda_k: 1 Loss: 0.001996875035689652\n",
      "Iteration: 2893 lambda_k: 1 Loss: 0.0019917022473582135\n",
      "Iteration: 2894 lambda_k: 1 Loss: 0.0019866716414720154\n",
      "Iteration: 2895 lambda_k: 1 Loss: 0.00198177930748919\n",
      "Iteration: 2896 lambda_k: 1 Loss: 0.001977021442453577\n",
      "Iteration: 2897 lambda_k: 1 Loss: 0.0019723943508019063\n",
      "Iteration: 2898 lambda_k: 1 Loss: 0.001967894441572484\n",
      "Iteration: 2899 lambda_k: 1 Loss: 0.001963518224409273\n",
      "Iteration: 2900 lambda_k: 1 Loss: 0.0019592623055150493\n",
      "Iteration: 2901 lambda_k: 1 Loss: 0.0019551233841397725\n",
      "Iteration: 2902 lambda_k: 1 Loss: 0.001951098176198684\n",
      "Iteration: 2903 lambda_k: 1 Loss: 0.0019471836405746303\n",
      "Iteration: 2904 lambda_k: 1 Loss: 0.0019433767199034358\n",
      "Iteration: 2905 lambda_k: 1 Loss: 0.0019396744651822672\n",
      "Iteration: 2906 lambda_k: 1 Loss: 0.001936074002009397\n",
      "Iteration: 2907 lambda_k: 1 Loss: 0.001932572529932367\n",
      "Iteration: 2908 lambda_k: 1 Loss: 0.0019291673245299353\n",
      "Iteration: 2909 lambda_k: 1 Loss: 0.0019258557357942575\n",
      "Iteration: 2910 lambda_k: 1 Loss: 0.0019226360743481356\n",
      "Iteration: 2911 lambda_k: 1 Loss: 0.001919504826596446\n",
      "Iteration: 2912 lambda_k: 1 Loss: 0.0019164597676701483\n",
      "Iteration: 2913 lambda_k: 1 Loss: 0.0019134984554006864\n",
      "Iteration: 2914 lambda_k: 1 Loss: 0.0019106185917189653\n",
      "Iteration: 2915 lambda_k: 1 Loss: 0.0019078179827020426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2916 lambda_k: 1 Loss: 0.0019050945032850247\n",
      "Iteration: 2917 lambda_k: 1 Loss: 0.00190244589333499\n",
      "Iteration: 2918 lambda_k: 1 Loss: 0.0018998703348471887\n",
      "Iteration: 2919 lambda_k: 1 Loss: 0.001897365817643333\n",
      "Iteration: 2920 lambda_k: 1 Loss: 0.0018949304196237062\n",
      "Iteration: 2921 lambda_k: 1 Loss: 0.001892562250971516\n",
      "Iteration: 2922 lambda_k: 1 Loss: 0.0018902594666807635\n",
      "Iteration: 2923 lambda_k: 1 Loss: 0.0018880202746367118\n",
      "Iteration: 2924 lambda_k: 1 Loss: 0.0018858429376147402\n",
      "Iteration: 2925 lambda_k: 1 Loss: 0.0018837257710246152\n",
      "Iteration: 2926 lambda_k: 1 Loss: 0.0018816671388863064\n",
      "Iteration: 2927 lambda_k: 1 Loss: 0.001879665449852686\n",
      "Iteration: 2928 lambda_k: 1 Loss: 0.0018777191541154527\n",
      "Iteration: 2929 lambda_k: 1 Loss: 0.0018758267412674816\n",
      "Iteration: 2930 lambda_k: 1 Loss: 0.0018739867387909575\n",
      "Iteration: 2931 lambda_k: 1 Loss: 0.0018721977107582107\n",
      "Iteration: 2932 lambda_k: 1 Loss: 0.001870458256444435\n",
      "Iteration: 2933 lambda_k: 1 Loss: 0.0018687670087256811\n",
      "Iteration: 2934 lambda_k: 1 Loss: 0.0018671226322760058\n",
      "Iteration: 2935 lambda_k: 1 Loss: 0.0018655238216460672\n",
      "Iteration: 2936 lambda_k: 1 Loss: 0.0018639719957946532\n",
      "Iteration: 2937 lambda_k: 1 Loss: 0.0018624627936048755\n",
      "Iteration: 2938 lambda_k: 1 Loss: 0.0018609957352412367\n",
      "Iteration: 2939 lambda_k: 1 Loss: 0.001859569449226341\n",
      "Iteration: 2940 lambda_k: 1 Loss: 0.0018581828122881956\n",
      "Iteration: 2941 lambda_k: 1 Loss: 0.001856834838826456\n",
      "Iteration: 2942 lambda_k: 1 Loss: 0.0018555245878755383\n",
      "Iteration: 2943 lambda_k: 1 Loss: 0.0018542505211643022\n",
      "Iteration: 2944 lambda_k: 1 Loss: 0.0018530124337032404\n",
      "Iteration: 2945 lambda_k: 1 Loss: 0.0018518091970044972\n",
      "Iteration: 2946 lambda_k: 1 Loss: 0.0018506398811586218\n",
      "Iteration: 2947 lambda_k: 1 Loss: 0.001849503510338272\n",
      "Iteration: 2948 lambda_k: 1 Loss: 0.001848399099591946\n",
      "Iteration: 2949 lambda_k: 1 Loss: 0.001847325676403638\n",
      "Iteration: 2950 lambda_k: 1 Loss: 0.0018462822846493717\n",
      "Iteration: 2951 lambda_k: 1 Loss: 0.0018452679766016462\n",
      "Iteration: 2952 lambda_k: 1 Loss: 0.0018442817998727008\n",
      "Iteration: 2953 lambda_k: 1 Loss: 0.0018433299508319523\n",
      "Iteration: 2954 lambda_k: 1 Loss: 0.0018424034891231513\n",
      "Iteration: 2955 lambda_k: 1 Loss: 0.0018415034620255875\n",
      "Iteration: 2956 lambda_k: 1 Loss: 0.001840628475792122\n",
      "Iteration: 2957 lambda_k: 1 Loss: 0.0018397776109883054\n",
      "Iteration: 2958 lambda_k: 1 Loss: 0.0018389501302435616\n",
      "Iteration: 2959 lambda_k: 1 Loss: 0.0018381452134716156\n",
      "Iteration: 2960 lambda_k: 1 Loss: 0.0018373617855433988\n",
      "Iteration: 2961 lambda_k: 1 Loss: 0.001836598433080984\n",
      "Iteration: 2962 lambda_k: 1 Loss: 0.0018358509252985726\n",
      "Iteration: 2963 lambda_k: 1 Loss: 0.0018351202370408685\n",
      "Iteration: 2964 lambda_k: 1 Loss: 0.0018344032545780215\n",
      "Iteration: 2965 lambda_k: 1 Loss: 0.0018336972294965012\n",
      "Iteration: 2966 lambda_k: 1 Loss: 0.0018329991268642158\n",
      "Iteration: 2967 lambda_k: 1 Loss: 0.0018323058861884792\n",
      "Iteration: 2968 lambda_k: 1 Loss: 0.001831617025796601\n",
      "Iteration: 2969 lambda_k: 1 Loss: 0.0018309276482393303\n",
      "Iteration: 2970 lambda_k: 1 Loss: 0.0018302367314892432\n",
      "Iteration: 2971 lambda_k: 1 Loss: 0.0018295437142016374\n",
      "Iteration: 2972 lambda_k: 1 Loss: 0.0018288485343817308\n",
      "Iteration: 2973 lambda_k: 1 Loss: 0.0018281453214491596\n",
      "Iteration: 2974 lambda_k: 1 Loss: 0.0018274438274288245\n",
      "Iteration: 2975 lambda_k: 1 Loss: 0.001826742725288597\n",
      "Iteration: 2976 lambda_k: 1 Loss: 0.0018260420448346392\n",
      "Iteration: 2977 lambda_k: 1 Loss: 0.0018253422159109514\n",
      "Iteration: 2978 lambda_k: 1 Loss: 0.0018246436615842634\n",
      "Iteration: 2979 lambda_k: 1 Loss: 0.00182394732035425\n",
      "Iteration: 2980 lambda_k: 1 Loss: 0.0018232528425616299\n",
      "Iteration: 2981 lambda_k: 1 Loss: 0.0018225606347074702\n",
      "Iteration: 2982 lambda_k: 1 Loss: 0.0018218710478051369\n",
      "Iteration: 2983 lambda_k: 1 Loss: 0.0018211843999978762\n",
      "Iteration: 2984 lambda_k: 1 Loss: 0.0018204989731741646\n",
      "Iteration: 2985 lambda_k: 1 Loss: 0.0018198178165228724\n",
      "Iteration: 2986 lambda_k: 1 Loss: 0.0018191407430520094\n",
      "Iteration: 2987 lambda_k: 1 Loss: 0.0018184678625985662\n",
      "Iteration: 2988 lambda_k: 1 Loss: 0.0018177993664175325\n",
      "Iteration: 2989 lambda_k: 1 Loss: 0.001817135419297529\n",
      "Iteration: 2990 lambda_k: 1 Loss: 0.001816476147738357\n",
      "Iteration: 2991 lambda_k: 1 Loss: 0.001815821657591092\n",
      "Iteration: 2992 lambda_k: 1 Loss: 0.0018151720468295605\n",
      "Iteration: 2993 lambda_k: 1 Loss: 0.0018145274084383732\n",
      "Iteration: 2994 lambda_k: 1 Loss: 0.0018138878285053263\n",
      "Iteration: 2995 lambda_k: 1 Loss: 0.0018132533839262733\n",
      "Iteration: 2996 lambda_k: 1 Loss: 0.0018126241413384573\n",
      "Iteration: 2997 lambda_k: 1 Loss: 0.0018120001572153434\n",
      "Iteration: 2998 lambda_k: 1 Loss: 0.0018113814785632864\n",
      "Iteration: 2999 lambda_k: 1 Loss: 0.0018107681437524762\n",
      "Iteration: 3000 lambda_k: 1 Loss: 0.0018101601832535453\n",
      "Iteration: 3001 lambda_k: 1 Loss: 0.0018095578170992193\n",
      "Iteration: 3002 lambda_k: 1 Loss: 0.0018089608465991088\n",
      "Iteration: 3003 lambda_k: 1 Loss: 0.001808369295957016\n",
      "Iteration: 3004 lambda_k: 1 Loss: 0.00180778317085595\n",
      "Iteration: 3005 lambda_k: 1 Loss: 0.0018072024657464216\n",
      "Iteration: 3006 lambda_k: 1 Loss: 0.0018066271723652138\n",
      "Iteration: 3007 lambda_k: 1 Loss: 0.001806057280687857\n",
      "Iteration: 3008 lambda_k: 1 Loss: 0.001805492777905496\n",
      "Iteration: 3009 lambda_k: 1 Loss: 0.0018049336477961512\n",
      "Iteration: 3010 lambda_k: 1 Loss: 0.00180437896562499\n",
      "Iteration: 3011 lambda_k: 1 Loss: 0.0018038298262434448\n",
      "Iteration: 3012 lambda_k: 1 Loss: 0.0018032860742111178\n",
      "Iteration: 3013 lambda_k: 1 Loss: 0.001802747655907456\n",
      "Iteration: 3014 lambda_k: 1 Loss: 0.0018022145531203428\n",
      "Iteration: 3015 lambda_k: 1 Loss: 0.0018016867401973757\n",
      "Iteration: 3016 lambda_k: 1 Loss: 0.0018011641778470182\n",
      "Iteration: 3017 lambda_k: 1 Loss: 0.0018006468199400212\n",
      "Iteration: 3018 lambda_k: 1 Loss: 0.001800134619434529\n",
      "Iteration: 3019 lambda_k: 1 Loss: 0.001799627530549218\n",
      "Iteration: 3020 lambda_k: 1 Loss: 0.001799125508670493\n",
      "Iteration: 3021 lambda_k: 1 Loss: 0.0017986285096547414\n",
      "Iteration: 3022 lambda_k: 1 Loss: 0.001798136489313328\n",
      "Iteration: 3023 lambda_k: 1 Loss: 0.0017976494032332846\n",
      "Iteration: 3024 lambda_k: 1 Loss: 0.001797167206821479\n",
      "Iteration: 3025 lambda_k: 1 Loss: 0.0017966898554217703\n",
      "Iteration: 3026 lambda_k: 1 Loss: 0.0017962173044078714\n",
      "Iteration: 3027 lambda_k: 1 Loss: 0.0017957495092255713\n",
      "Iteration: 3028 lambda_k: 1 Loss: 0.0017952864254022086\n",
      "Iteration: 3029 lambda_k: 1 Loss: 0.0017948280085491322\n",
      "Iteration: 3030 lambda_k: 1 Loss: 0.0017943742143731423\n",
      "Iteration: 3031 lambda_k: 1 Loss: 0.0017939249986969304\n",
      "Iteration: 3032 lambda_k: 1 Loss: 0.001793480350169688\n",
      "Iteration: 3033 lambda_k: 1 Loss: 0.0017930401845360209\n",
      "Iteration: 3034 lambda_k: 1 Loss: 0.0017926044630933156\n",
      "Iteration: 3035 lambda_k: 1 Loss: 0.0017921731432996844\n",
      "Iteration: 3036 lambda_k: 1 Loss: 0.001791746181382433\n",
      "Iteration: 3037 lambda_k: 1 Loss: 0.0017913235339456312\n",
      "Iteration: 3038 lambda_k: 1 Loss: 0.001790905158290682\n",
      "Iteration: 3039 lambda_k: 1 Loss: 0.0017904910122109581\n",
      "Iteration: 3040 lambda_k: 1 Loss: 0.0017900810537838715\n",
      "Iteration: 3041 lambda_k: 1 Loss: 0.0017896752412959352\n",
      "Iteration: 3042 lambda_k: 1 Loss: 0.0017892735332552755\n",
      "Iteration: 3043 lambda_k: 1 Loss: 0.0017888758884299816\n",
      "Iteration: 3044 lambda_k: 1 Loss: 0.0017884822658787106\n",
      "Iteration: 3045 lambda_k: 1 Loss: 0.0017880926249674027\n",
      "Iteration: 3046 lambda_k: 1 Loss: 0.0017877069253761793\n",
      "Iteration: 3047 lambda_k: 1 Loss: 0.0017873251271032587\n",
      "Iteration: 3048 lambda_k: 1 Loss: 0.00178694719046855\n",
      "Iteration: 3049 lambda_k: 1 Loss: 0.0017865730761188524\n",
      "Iteration: 3050 lambda_k: 1 Loss: 0.001786202745033392\n",
      "Iteration: 3051 lambda_k: 1 Loss: 0.0017858361585286772\n",
      "Iteration: 3052 lambda_k: 1 Loss: 0.0017854732782624076\n",
      "Iteration: 3053 lambda_k: 1 Loss: 0.0017851140662365538\n",
      "Iteration: 3054 lambda_k: 1 Loss: 0.0017847584847996234\n",
      "Iteration: 3055 lambda_k: 1 Loss: 0.001784406496648324\n",
      "Iteration: 3056 lambda_k: 1 Loss: 0.0017840580648287446\n",
      "Iteration: 3057 lambda_k: 1 Loss: 0.0017837131527375925\n",
      "Iteration: 3058 lambda_k: 1 Loss: 0.001783371724122631\n",
      "Iteration: 3059 lambda_k: 1 Loss: 0.001783033828855075\n",
      "Iteration: 3060 lambda_k: 1 Loss: 0.0017826993456685496\n",
      "Iteration: 3061 lambda_k: 1 Loss: 0.0017823682396515285\n",
      "Iteration: 3062 lambda_k: 1 Loss: 0.001782040475285585\n",
      "Iteration: 3063 lambda_k: 1 Loss: 0.0017817160155212005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3064 lambda_k: 1 Loss: 0.001781394825064186\n",
      "Iteration: 3065 lambda_k: 1 Loss: 0.0017810768700401017\n",
      "Iteration: 3066 lambda_k: 1 Loss: 0.0017807621172923017\n",
      "Iteration: 3067 lambda_k: 1 Loss: 0.0017804505340706074\n",
      "Iteration: 3068 lambda_k: 1 Loss: 0.001780142087937835\n",
      "Iteration: 3069 lambda_k: 1 Loss: 0.0017798367467282878\n",
      "Iteration: 3070 lambda_k: 1 Loss: 0.0017795344785372182\n",
      "Iteration: 3071 lambda_k: 1 Loss: 0.0017792352517421552\n",
      "Iteration: 3072 lambda_k: 1 Loss: 0.0017789390350336002\n",
      "Iteration: 3073 lambda_k: 1 Loss: 0.0017786457974322282\n",
      "Iteration: 3074 lambda_k: 1 Loss: 0.0017783555082874854\n",
      "Iteration: 3075 lambda_k: 1 Loss: 0.0017780681372668698\n",
      "Iteration: 3076 lambda_k: 1 Loss: 0.0017777836543469469\n",
      "Iteration: 3077 lambda_k: 1 Loss: 0.0017775020298093148\n",
      "Iteration: 3078 lambda_k: 1 Loss: 0.0017772232342397661\n",
      "Iteration: 3079 lambda_k: 1 Loss: 0.0017769472385281441\n",
      "Iteration: 3080 lambda_k: 1 Loss: 0.001776674013866995\n",
      "Iteration: 3081 lambda_k: 1 Loss: 0.0017764035317491393\n",
      "Iteration: 3082 lambda_k: 1 Loss: 0.001776135763964634\n",
      "Iteration: 3083 lambda_k: 1 Loss: 0.0017758706825980275\n",
      "Iteration: 3084 lambda_k: 1 Loss: 0.0017756082600256106\n",
      "Iteration: 3085 lambda_k: 1 Loss: 0.0017753484689130479\n",
      "Iteration: 3086 lambda_k: 1 Loss: 0.0017750912822128667\n",
      "Iteration: 3087 lambda_k: 1 Loss: 0.0017748366731618426\n",
      "Iteration: 3088 lambda_k: 1 Loss: 0.0017745846152781401\n",
      "Iteration: 3089 lambda_k: 1 Loss: 0.0017743346733187723\n",
      "Iteration: 3090 lambda_k: 1 Loss: 0.001774087291514527\n",
      "Iteration: 3091 lambda_k: 1 Loss: 0.0017738423971543156\n",
      "Iteration: 3092 lambda_k: 1 Loss: 0.0017735999567600007\n",
      "Iteration: 3093 lambda_k: 1 Loss: 0.0017733599542231907\n",
      "Iteration: 3094 lambda_k: 1 Loss: 0.001773122371439616\n",
      "Iteration: 3095 lambda_k: 1 Loss: 0.0017728871850000128\n",
      "Iteration: 3096 lambda_k: 1 Loss: 0.00177265436871246\n",
      "Iteration: 3097 lambda_k: 1 Loss: 0.0017724238960580073\n",
      "Iteration: 3098 lambda_k: 1 Loss: 0.0017721957412419934\n",
      "Iteration: 3099 lambda_k: 1 Loss: 0.001771969879304585\n",
      "Iteration: 3100 lambda_k: 1 Loss: 0.001771746285894225\n",
      "Iteration: 3101 lambda_k: 1 Loss: 0.0017715249370364686\n",
      "Iteration: 3102 lambda_k: 1 Loss: 0.0017713058090116432\n",
      "Iteration: 3103 lambda_k: 1 Loss: 0.0017710888783348153\n",
      "Iteration: 3104 lambda_k: 1 Loss: 0.001770874121785966\n",
      "Iteration: 3105 lambda_k: 1 Loss: 0.001770661516441268\n",
      "Iteration: 3106 lambda_k: 1 Loss: 0.001770451039683673\n",
      "Iteration: 3107 lambda_k: 1 Loss: 0.0017702426691949562\n",
      "Iteration: 3108 lambda_k: 1 Loss: 0.0017700363829411159\n",
      "Iteration: 3109 lambda_k: 1 Loss: 0.0017698321591605505\n",
      "Iteration: 3110 lambda_k: 1 Loss: 0.0017696299763569228\n",
      "Iteration: 3111 lambda_k: 1 Loss: 0.0017694298132953715\n",
      "Iteration: 3112 lambda_k: 1 Loss: 0.0017692316489984847\n",
      "Iteration: 3113 lambda_k: 1 Loss: 0.0017690354627419836\n",
      "Iteration: 3114 lambda_k: 1 Loss: 0.0017688412340496188\n",
      "Iteration: 3115 lambda_k: 1 Loss: 0.001768648942687935\n",
      "Iteration: 3116 lambda_k: 1 Loss: 0.0017684585686612545\n",
      "Iteration: 3117 lambda_k: 1 Loss: 0.0017682700922072453\n",
      "Iteration: 3118 lambda_k: 1 Loss: 0.0017680834937925075\n",
      "Iteration: 3119 lambda_k: 1 Loss: 0.001767898754108771\n",
      "Iteration: 3120 lambda_k: 1 Loss: 0.001767715854068651\n",
      "Iteration: 3121 lambda_k: 1 Loss: 0.001767534774801901\n",
      "Iteration: 3122 lambda_k: 1 Loss: 0.001767355497651761\n",
      "Iteration: 3123 lambda_k: 1 Loss: 0.0017671780041713407\n",
      "Iteration: 3124 lambda_k: 1 Loss: 0.0017670022761202533\n",
      "Iteration: 3125 lambda_k: 1 Loss: 0.0017668282954611308\n",
      "Iteration: 3126 lambda_k: 1 Loss: 0.0017666560443567012\n",
      "Iteration: 3127 lambda_k: 1 Loss: 0.0017664855051665494\n",
      "Iteration: 3128 lambda_k: 1 Loss: 0.0017663166604442323\n",
      "Iteration: 3129 lambda_k: 1 Loss: 0.0017661494929343332\n",
      "Iteration: 3130 lambda_k: 1 Loss: 0.00176598398556977\n",
      "Iteration: 3131 lambda_k: 1 Loss: 0.0017658201214689243\n",
      "Iteration: 3132 lambda_k: 1 Loss: 0.001765657883933332\n",
      "Iteration: 3133 lambda_k: 1 Loss: 0.001765497256441052\n",
      "Iteration: 3134 lambda_k: 1 Loss: 0.0017653382226468814\n",
      "Iteration: 3135 lambda_k: 1 Loss: 0.0017651807663877474\n",
      "Iteration: 3136 lambda_k: 1 Loss: 0.0017650248716728032\n",
      "Iteration: 3137 lambda_k: 1 Loss: 0.0017648705226821185\n",
      "Iteration: 3138 lambda_k: 1 Loss: 0.0017647177037647247\n",
      "Iteration: 3139 lambda_k: 1 Loss: 0.0017645663994364047\n",
      "Iteration: 3140 lambda_k: 1 Loss: 0.0017644165943774678\n",
      "Iteration: 3141 lambda_k: 1 Loss: 0.0017642682734306861\n",
      "Iteration: 3142 lambda_k: 1 Loss: 0.0017641214215991733\n",
      "Iteration: 3143 lambda_k: 1 Loss: 0.001763976024044426\n",
      "Iteration: 3144 lambda_k: 1 Loss: 0.0017638320660845151\n",
      "Iteration: 3145 lambda_k: 1 Loss: 0.001763689533192134\n",
      "Iteration: 3146 lambda_k: 1 Loss: 0.0017635484109927394\n",
      "Iteration: 3147 lambda_k: 1 Loss: 0.0017634086852628117\n",
      "Iteration: 3148 lambda_k: 1 Loss: 0.0017632703419278924\n",
      "Iteration: 3149 lambda_k: 1 Loss: 0.0017631333670610858\n",
      "Iteration: 3150 lambda_k: 1 Loss: 0.001762997746881151\n",
      "Iteration: 3151 lambda_k: 1 Loss: 0.0017628634677508287\n",
      "Iteration: 3152 lambda_k: 1 Loss: 0.0017627305161753044\n",
      "Iteration: 3153 lambda_k: 1 Loss: 0.0017625988788005187\n",
      "Iteration: 3154 lambda_k: 1 Loss: 0.0017624685424114498\n",
      "Iteration: 3155 lambda_k: 1 Loss: 0.001762339493930631\n",
      "Iteration: 3156 lambda_k: 1 Loss: 0.0017622117204165802\n",
      "Iteration: 3157 lambda_k: 1 Loss: 0.0017620852090623324\n",
      "Iteration: 3158 lambda_k: 1 Loss: 0.0017619599471937618\n",
      "Iteration: 3159 lambda_k: 1 Loss: 0.0017618359222683513\n",
      "Iteration: 3160 lambda_k: 1 Loss: 0.0017617131218734218\n",
      "Iteration: 3161 lambda_k: 1 Loss: 0.0017615915337248847\n",
      "Iteration: 3162 lambda_k: 1 Loss: 0.0017614711456658276\n",
      "Iteration: 3163 lambda_k: 1 Loss: 0.0017613519456649161\n",
      "Iteration: 3164 lambda_k: 1 Loss: 0.001761233921815243\n",
      "Iteration: 3165 lambda_k: 1 Loss: 0.0017611170623327954\n",
      "Iteration: 3166 lambda_k: 1 Loss: 0.001761001355555176\n",
      "Iteration: 3167 lambda_k: 1 Loss: 0.0017608867899402698\n",
      "Iteration: 3168 lambda_k: 1 Loss: 0.0017607733540648284\n",
      "Iteration: 3169 lambda_k: 1 Loss: 0.0017606610366234369\n",
      "Iteration: 3170 lambda_k: 1 Loss: 0.0017605498264268767\n",
      "Iteration: 3171 lambda_k: 1 Loss: 0.0017604397124010734\n",
      "Iteration: 3172 lambda_k: 1 Loss: 0.001760330683585877\n",
      "Iteration: 3173 lambda_k: 1 Loss: 0.00176022272913366\n",
      "Iteration: 3174 lambda_k: 1 Loss: 0.0017601158383083017\n",
      "Iteration: 3175 lambda_k: 1 Loss: 0.001760010000483809\n",
      "Iteration: 3176 lambda_k: 1 Loss: 0.0017599052051431791\n",
      "Iteration: 3177 lambda_k: 1 Loss: 0.0017598014418772044\n",
      "Iteration: 3178 lambda_k: 1 Loss: 0.001759698700383488\n",
      "Iteration: 3179 lambda_k: 1 Loss: 0.001759596970465064\n",
      "Iteration: 3180 lambda_k: 1 Loss: 0.001759496242029419\n",
      "Iteration: 3181 lambda_k: 1 Loss: 0.001759396505087201\n",
      "Iteration: 3182 lambda_k: 1 Loss: 0.0017592977497512816\n",
      "Iteration: 3183 lambda_k: 1 Loss: 0.0017591999662356053\n",
      "Iteration: 3184 lambda_k: 1 Loss: 0.0017591031448540666\n",
      "Iteration: 3185 lambda_k: 1 Loss: 0.001759007276019536\n",
      "Iteration: 3186 lambda_k: 1 Loss: 0.0017589123502426947\n",
      "Iteration: 3187 lambda_k: 1 Loss: 0.0017588183581311125\n",
      "Iteration: 3188 lambda_k: 1 Loss: 0.0017587252903881196\n",
      "Iteration: 3189 lambda_k: 1 Loss: 0.001758633137811779\n",
      "Iteration: 3190 lambda_k: 1 Loss: 0.0017585418912939603\n",
      "Iteration: 3191 lambda_k: 1 Loss: 0.0017584515418192824\n",
      "Iteration: 3192 lambda_k: 1 Loss: 0.0017583620804641265\n",
      "Iteration: 3193 lambda_k: 1 Loss: 0.0017582734983957935\n",
      "Iteration: 3194 lambda_k: 1 Loss: 0.0017581857868712721\n",
      "Iteration: 3195 lambda_k: 1 Loss: 0.0017580989372365068\n",
      "Iteration: 3196 lambda_k: 1 Loss: 0.0017580129409254532\n",
      "Iteration: 3197 lambda_k: 1 Loss: 0.0017579277894590121\n",
      "Iteration: 3198 lambda_k: 1 Loss: 0.001757843474444289\n",
      "Iteration: 3199 lambda_k: 1 Loss: 0.0017577599875735202\n",
      "Iteration: 3200 lambda_k: 1 Loss: 0.0017576773206232222\n",
      "Iteration: 3201 lambda_k: 1 Loss: 0.001757595465453338\n",
      "Iteration: 3202 lambda_k: 1 Loss: 0.0017575144140062663\n",
      "Iteration: 3203 lambda_k: 1 Loss: 0.0017574341583062485\n",
      "Iteration: 3204 lambda_k: 1 Loss: 0.0017573546904582566\n",
      "Iteration: 3205 lambda_k: 1 Loss: 0.0017572760026471417\n",
      "Iteration: 3206 lambda_k: 1 Loss: 0.001757198087136905\n",
      "Iteration: 3207 lambda_k: 1 Loss: 0.001757120936269881\n",
      "Iteration: 3208 lambda_k: 1 Loss: 0.0017570445424658079\n",
      "Iteration: 3209 lambda_k: 1 Loss: 0.0017569688982211334\n",
      "Iteration: 3210 lambda_k: 1 Loss: 0.0017568939961080828\n",
      "Iteration: 3211 lambda_k: 1 Loss: 0.001756819828774015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3212 lambda_k: 1 Loss: 0.0017567463889404146\n",
      "Iteration: 3213 lambda_k: 1 Loss: 0.0017566736694024849\n",
      "Iteration: 3214 lambda_k: 1 Loss: 0.001756601663027911\n",
      "Iteration: 3215 lambda_k: 1 Loss: 0.0017565303627565592\n",
      "Iteration: 3216 lambda_k: 1 Loss: 0.0017564597615993274\n",
      "Iteration: 3217 lambda_k: 1 Loss: 0.0017563898526377405\n",
      "Iteration: 3218 lambda_k: 1 Loss: 0.001756320629022911\n",
      "Iteration: 3219 lambda_k: 1 Loss: 0.0017562520839750576\n",
      "Iteration: 3220 lambda_k: 1 Loss: 0.0017561842107827155\n",
      "Iteration: 3221 lambda_k: 1 Loss: 0.0017561170028019047\n",
      "Iteration: 3222 lambda_k: 1 Loss: 0.0017560504534555766\n",
      "Iteration: 3223 lambda_k: 1 Loss: 0.0017559845562328477\n",
      "Iteration: 3224 lambda_k: 1 Loss: 0.0017559193046883762\n",
      "Iteration: 3225 lambda_k: 1 Loss: 0.001755854692441558\n",
      "Iteration: 3226 lambda_k: 1 Loss: 0.0017557907131759566\n",
      "Iteration: 3227 lambda_k: 1 Loss: 0.00175572736063866\n",
      "Iteration: 3228 lambda_k: 1 Loss: 0.0017556646286395773\n",
      "Iteration: 3229 lambda_k: 1 Loss: 0.0017556025110506532\n",
      "Iteration: 3230 lambda_k: 1 Loss: 0.0017555410018055529\n",
      "Iteration: 3231 lambda_k: 1 Loss: 0.00175548009489869\n",
      "Iteration: 3232 lambda_k: 1 Loss: 0.001755419784384838\n",
      "Iteration: 3233 lambda_k: 1 Loss: 0.0017553600643783007\n",
      "Iteration: 3234 lambda_k: 1 Loss: 0.001755300929052426\n",
      "Iteration: 3235 lambda_k: 1 Loss: 0.0017552423726389925\n",
      "Iteration: 3236 lambda_k: 1 Loss: 0.0017551843894276448\n",
      "Iteration: 3237 lambda_k: 1 Loss: 0.0017551269737651305\n",
      "Iteration: 3238 lambda_k: 1 Loss: 0.0017550701200548813\n",
      "Iteration: 3239 lambda_k: 1 Loss: 0.0017550138227563864\n",
      "Iteration: 3240 lambda_k: 1 Loss: 0.0017549580763845413\n",
      "Iteration: 3241 lambda_k: 1 Loss: 0.001754902875509116\n",
      "Iteration: 3242 lambda_k: 1 Loss: 0.0017548482147542969\n",
      "Iteration: 3243 lambda_k: 1 Loss: 0.0017547940887979894\n",
      "Iteration: 3244 lambda_k: 1 Loss: 0.0017547404923713525\n",
      "Iteration: 3245 lambda_k: 1 Loss: 0.0017546874202580942\n",
      "Iteration: 3246 lambda_k: 1 Loss: 0.0017546348672940837\n",
      "Iteration: 3247 lambda_k: 1 Loss: 0.0017545828283668772\n",
      "Iteration: 3248 lambda_k: 1 Loss: 0.0017545312984150855\n",
      "Iteration: 3249 lambda_k: 1 Loss: 0.001754480272427696\n",
      "Iteration: 3250 lambda_k: 1 Loss: 0.0017544297454439215\n",
      "Iteration: 3251 lambda_k: 1 Loss: 0.0017543797125523473\n",
      "Iteration: 3252 lambda_k: 1 Loss: 0.0017543301688906449\n",
      "Iteration: 3253 lambda_k: 1 Loss: 0.0017542811096448406\n",
      "Iteration: 3254 lambda_k: 1 Loss: 0.0017542325300491672\n",
      "Iteration: 3255 lambda_k: 1 Loss: 0.0017541844253852567\n",
      "Iteration: 3256 lambda_k: 1 Loss: 0.0017541367909817234\n",
      "Iteration: 3257 lambda_k: 1 Loss: 0.001754089622213836\n",
      "Iteration: 3258 lambda_k: 1 Loss: 0.001754042914502909\n",
      "Iteration: 3259 lambda_k: 1 Loss: 0.0017539966633157462\n",
      "Iteration: 3260 lambda_k: 1 Loss: 0.0017539508641644386\n",
      "Iteration: 3261 lambda_k: 1 Loss: 0.0017539055126056518\n",
      "Iteration: 3262 lambda_k: 1 Loss: 0.0017538606042403814\n",
      "Iteration: 3263 lambda_k: 1 Loss: 0.0017538161347132456\n",
      "Iteration: 3264 lambda_k: 1 Loss: 0.0017537720997122667\n",
      "Iteration: 3265 lambda_k: 1 Loss: 0.0017537284949683382\n",
      "Iteration: 3266 lambda_k: 1 Loss: 0.0017536853162548453\n",
      "Iteration: 3267 lambda_k: 1 Loss: 0.001753642559387118\n",
      "Iteration: 3268 lambda_k: 1 Loss: 0.0017536002202221315\n",
      "Iteration: 3269 lambda_k: 1 Loss: 0.0017535582946579498\n",
      "Iteration: 3270 lambda_k: 1 Loss: 0.001753516778633445\n",
      "Iteration: 3271 lambda_k: 1 Loss: 0.0017534756681278295\n",
      "Iteration: 3272 lambda_k: 1 Loss: 0.001753434959160162\n",
      "Iteration: 3273 lambda_k: 1 Loss: 0.0017533946477891614\n",
      "Iteration: 3274 lambda_k: 1 Loss: 0.001753354730112507\n",
      "Iteration: 3275 lambda_k: 1 Loss: 0.0017533152022667124\n",
      "Iteration: 3276 lambda_k: 1 Loss: 0.001753276060426498\n",
      "Iteration: 3277 lambda_k: 1 Loss: 0.0017532373008046548\n",
      "Iteration: 3278 lambda_k: 1 Loss: 0.0017531989196514727\n",
      "Iteration: 3279 lambda_k: 1 Loss: 0.0017531609132544386\n",
      "Iteration: 3280 lambda_k: 1 Loss: 0.0017531232779378052\n",
      "Iteration: 3281 lambda_k: 1 Loss: 0.0017530860100622367\n",
      "Iteration: 3282 lambda_k: 1 Loss: 0.0017530491060245594\n",
      "Iteration: 3283 lambda_k: 1 Loss: 0.0017530125622572008\n",
      "Iteration: 3284 lambda_k: 1 Loss: 0.0017529763752278903\n",
      "Iteration: 3285 lambda_k: 1 Loss: 0.0017529405414394806\n",
      "Iteration: 3286 lambda_k: 1 Loss: 0.0017529050574292466\n",
      "Iteration: 3287 lambda_k: 1 Loss: 0.0017528699197689422\n",
      "Iteration: 3288 lambda_k: 1 Loss: 0.0017528351250640814\n",
      "Iteration: 3289 lambda_k: 1 Loss: 0.0017528006699538733\n",
      "Iteration: 3290 lambda_k: 1 Loss: 0.0017527665511107076\n",
      "Iteration: 3291 lambda_k: 1 Loss: 0.001752732765239995\n",
      "Iteration: 3292 lambda_k: 1 Loss: 0.001752699309079541\n",
      "Iteration: 3293 lambda_k: 1 Loss: 0.0017526661793996264\n",
      "Iteration: 3294 lambda_k: 1 Loss: 0.0017526333730022493\n",
      "Iteration: 3295 lambda_k: 1 Loss: 0.001752600886721125\n",
      "Iteration: 3296 lambda_k: 1 Loss: 0.0017525687174212976\n",
      "Iteration: 3297 lambda_k: 1 Loss: 0.001752536861998729\n",
      "Iteration: 3298 lambda_k: 1 Loss: 0.0017525053173799573\n",
      "Iteration: 3299 lambda_k: 1 Loss: 0.0017524740805220714\n",
      "Iteration: 3300 lambda_k: 1 Loss: 0.0017524431484120377\n",
      "Iteration: 3301 lambda_k: 1 Loss: 0.0017524125180666311\n",
      "Iteration: 3302 lambda_k: 1 Loss: 0.001752382186532087\n",
      "Iteration: 3303 lambda_k: 1 Loss: 0.0017523521508838237\n",
      "Iteration: 3304 lambda_k: 1 Loss: 0.0017523224082259825\n",
      "Iteration: 3305 lambda_k: 1 Loss: 0.0017522929556914582\n",
      "Iteration: 3306 lambda_k: 1 Loss: 0.0017522637904413007\n",
      "Iteration: 3307 lambda_k: 1 Loss: 0.0017522349096646178\n",
      "Iteration: 3308 lambda_k: 1 Loss: 0.0017522063105782333\n",
      "Iteration: 3309 lambda_k: 1 Loss: 0.001752177990426397\n",
      "Iteration: 3310 lambda_k: 1 Loss: 0.0017521499464805807\n",
      "Iteration: 3311 lambda_k: 1 Loss: 0.001752122176038989\n",
      "Iteration: 3312 lambda_k: 1 Loss: 0.001752094676426696\n",
      "Iteration: 3313 lambda_k: 1 Loss: 0.0017520674449949187\n",
      "Iteration: 3314 lambda_k: 1 Loss: 0.0017520404791211332\n",
      "Iteration: 3315 lambda_k: 1 Loss: 0.0017520137762085182\n",
      "Iteration: 3316 lambda_k: 1 Loss: 0.001751987333685885\n",
      "Iteration: 3317 lambda_k: 1 Loss: 0.0017519611490073822\n",
      "Iteration: 3318 lambda_k: 1 Loss: 0.0017519352196522211\n",
      "Iteration: 3319 lambda_k: 1 Loss: 0.0017519095431244118\n",
      "Iteration: 3320 lambda_k: 1 Loss: 0.0017518841169525715\n",
      "Iteration: 3321 lambda_k: 1 Loss: 0.0017518589386895725\n",
      "Iteration: 3322 lambda_k: 1 Loss: 0.001751834005912446\n",
      "Iteration: 3323 lambda_k: 1 Loss: 0.0017518093162220117\n",
      "Iteration: 3324 lambda_k: 1 Loss: 0.0017517848672426672\n",
      "Iteration: 3325 lambda_k: 1 Loss: 0.0017517606566222889\n",
      "Iteration: 3326 lambda_k: 1 Loss: 0.0017517366820318171\n",
      "Iteration: 3327 lambda_k: 1 Loss: 0.0017517129411650583\n",
      "Iteration: 3328 lambda_k: 1 Loss: 0.001751689431738527\n",
      "Iteration: 3329 lambda_k: 1 Loss: 0.0017516661514912029\n",
      "Iteration: 3330 lambda_k: 1 Loss: 0.001751643098184283\n",
      "Iteration: 3331 lambda_k: 1 Loss: 0.0017516202696009839\n",
      "Iteration: 3332 lambda_k: 1 Loss: 0.001751597663546272\n",
      "Iteration: 3333 lambda_k: 1 Loss: 0.001751575277846717\n",
      "Iteration: 3334 lambda_k: 1 Loss: 0.0017515531103502152\n",
      "Iteration: 3335 lambda_k: 1 Loss: 0.0017515311589258477\n",
      "Iteration: 3336 lambda_k: 1 Loss: 0.001751509421463543\n",
      "Iteration: 3337 lambda_k: 1 Loss: 0.0017514878958740563\n",
      "Iteration: 3338 lambda_k: 1 Loss: 0.0017514665800885637\n",
      "Iteration: 3339 lambda_k: 1 Loss: 0.0017514454720586354\n",
      "Iteration: 3340 lambda_k: 1 Loss: 0.001751424569755938\n",
      "Iteration: 3341 lambda_k: 1 Loss: 0.00175140387117201\n",
      "Iteration: 3342 lambda_k: 1 Loss: 0.001751383374318137\n",
      "Iteration: 3343 lambda_k: 1 Loss: 0.0017513630772251585\n",
      "Iteration: 3344 lambda_k: 1 Loss: 0.0017513429779430689\n",
      "Iteration: 3345 lambda_k: 1 Loss: 0.001751323074541237\n",
      "Iteration: 3346 lambda_k: 1 Loss: 0.0017513033651077572\n",
      "Iteration: 3347 lambda_k: 1 Loss: 0.0017512838477496185\n",
      "Iteration: 3348 lambda_k: 1 Loss: 0.0017512645205923446\n",
      "Iteration: 3349 lambda_k: 1 Loss: 0.0017512453817797983\n",
      "Iteration: 3350 lambda_k: 1 Loss: 0.0017512264294741312\n",
      "Iteration: 3351 lambda_k: 1 Loss: 0.0017512076618554204\n",
      "Iteration: 3352 lambda_k: 1 Loss: 0.0017511890771216968\n",
      "Iteration: 3353 lambda_k: 1 Loss: 0.001751170673488558\n",
      "Iteration: 3354 lambda_k: 1 Loss: 0.001751152449189123\n",
      "Iteration: 3355 lambda_k: 1 Loss: 0.001751134402473809\n",
      "Iteration: 3356 lambda_k: 1 Loss: 0.0017511165316101854\n",
      "Iteration: 3357 lambda_k: 1 Loss: 0.0017510988348828692\n",
      "Iteration: 3358 lambda_k: 1 Loss: 0.0017510813105931812\n",
      "Iteration: 3359 lambda_k: 1 Loss: 0.0017510639570592278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3360 lambda_k: 1 Loss: 0.0017510467726154486\n",
      "Iteration: 3361 lambda_k: 1 Loss: 0.0017510297556125895\n",
      "Iteration: 3362 lambda_k: 1 Loss: 0.0017510129044176594\n",
      "Iteration: 3363 lambda_k: 1 Loss: 0.0017509962174136222\n",
      "Iteration: 3364 lambda_k: 1 Loss: 0.0017509796929992684\n",
      "Iteration: 3365 lambda_k: 1 Loss: 0.0017509633295891046\n",
      "Iteration: 3366 lambda_k: 1 Loss: 0.0017509471256130428\n",
      "Iteration: 3367 lambda_k: 1 Loss: 0.001750931079516529\n",
      "Iteration: 3368 lambda_k: 1 Loss: 0.0017509151897600634\n",
      "Iteration: 3369 lambda_k: 1 Loss: 0.0017508994548193148\n",
      "Iteration: 3370 lambda_k: 1 Loss: 0.001750883873184917\n",
      "Iteration: 3371 lambda_k: 1 Loss: 0.0017508684433621463\n",
      "Iteration: 3372 lambda_k: 1 Loss: 0.0017508531638709569\n",
      "Iteration: 3373 lambda_k: 1 Loss: 0.0017508380332458327\n",
      "Iteration: 3374 lambda_k: 1 Loss: 0.0017508230500355526\n",
      "Iteration: 3375 lambda_k: 1 Loss: 0.001750808212803161\n",
      "Iteration: 3376 lambda_k: 1 Loss: 0.0017507935201256453\n",
      "Iteration: 3377 lambda_k: 1 Loss: 0.0017507789705939203\n",
      "Iteration: 3378 lambda_k: 1 Loss: 0.001750764562812774\n",
      "Iteration: 3379 lambda_k: 1 Loss: 0.0017507502954005685\n",
      "Iteration: 3380 lambda_k: 1 Loss: 0.0017507361669892756\n",
      "Iteration: 3381 lambda_k: 1 Loss: 0.0017507221762241435\n",
      "Iteration: 3382 lambda_k: 1 Loss: 0.0017507083217636217\n",
      "Iteration: 3383 lambda_k: 1 Loss: 0.0017506946022794606\n",
      "Iteration: 3384 lambda_k: 1 Loss: 0.001750681016456191\n",
      "Iteration: 3385 lambda_k: 1 Loss: 0.0017506675629913742\n",
      "Iteration: 3386 lambda_k: 1 Loss: 0.0017506542405951332\n",
      "Iteration: 3387 lambda_k: 1 Loss: 0.0017506410479903043\n",
      "Iteration: 3388 lambda_k: 1 Loss: 0.0017506279839122779\n",
      "Iteration: 3389 lambda_k: 1 Loss: 0.001750615047108639\n",
      "Iteration: 3390 lambda_k: 1 Loss: 0.0017506022363393459\n",
      "Iteration: 3391 lambda_k: 1 Loss: 0.001750589550376359\n",
      "Iteration: 3392 lambda_k: 1 Loss: 0.0017505769880037517\n",
      "Iteration: 3393 lambda_k: 1 Loss: 0.001750564548017346\n",
      "Iteration: 3394 lambda_k: 1 Loss: 0.0017505522292248694\n",
      "Iteration: 3395 lambda_k: 1 Loss: 0.0017505400304455296\n",
      "Iteration: 3396 lambda_k: 1 Loss: 0.0017505279505102226\n",
      "Iteration: 3397 lambda_k: 1 Loss: 0.0017505159882612099\n",
      "Iteration: 3398 lambda_k: 1 Loss: 0.0017505041425520866\n",
      "Iteration: 3399 lambda_k: 1 Loss: 0.001750492412247551\n",
      "Iteration: 3400 lambda_k: 1 Loss: 0.0017504807962234307\n",
      "Iteration: 3401 lambda_k: 1 Loss: 0.0017504692933665931\n",
      "Iteration: 3402 lambda_k: 1 Loss: 0.00175045790257474\n",
      "Iteration: 3403 lambda_k: 1 Loss: 0.0017504466227563054\n",
      "Iteration: 3404 lambda_k: 1 Loss: 0.0017504354528303898\n",
      "Iteration: 3405 lambda_k: 1 Loss: 0.0017504243917266893\n",
      "Iteration: 3406 lambda_k: 1 Loss: 0.0017504134383852182\n",
      "Iteration: 3407 lambda_k: 1 Loss: 0.0017504025917565325\n",
      "Iteration: 3408 lambda_k: 1 Loss: 0.0017503918508012847\n",
      "Iteration: 3409 lambda_k: 1 Loss: 0.0017503812144903002\n",
      "Iteration: 3410 lambda_k: 1 Loss: 0.0017503706818045152\n",
      "Iteration: 3411 lambda_k: 1 Loss: 0.0017503602517347045\n",
      "Iteration: 3412 lambda_k: 1 Loss: 0.001750349923281564\n",
      "Iteration: 3413 lambda_k: 1 Loss: 0.0017503396954554854\n",
      "Iteration: 3414 lambda_k: 1 Loss: 0.0017503295672765982\n",
      "Iteration: 3415 lambda_k: 1 Loss: 0.0017503195377745584\n",
      "Iteration: 3416 lambda_k: 1 Loss: 0.0017503096059884131\n",
      "Iteration: 3417 lambda_k: 1 Loss: 0.0017502997709667165\n",
      "Iteration: 3418 lambda_k: 1 Loss: 0.0017502900317671643\n",
      "Iteration: 3419 lambda_k: 1 Loss: 0.001750280387456751\n",
      "Iteration: 3420 lambda_k: 1 Loss: 0.001750270837111525\n",
      "Iteration: 3421 lambda_k: 1 Loss: 0.0017502613798165627\n",
      "Iteration: 3422 lambda_k: 1 Loss: 0.001750252014665832\n",
      "Iteration: 3423 lambda_k: 1 Loss: 0.0017502427407621902\n",
      "Iteration: 3424 lambda_k: 1 Loss: 0.0017502335572171961\n",
      "Iteration: 3425 lambda_k: 1 Loss: 0.0017502244631510814\n",
      "Iteration: 3426 lambda_k: 1 Loss: 0.001750215457692714\n",
      "Iteration: 3427 lambda_k: 1 Loss: 0.0017502065399793766\n",
      "Iteration: 3428 lambda_k: 1 Loss: 0.0017501977091567466\n",
      "Iteration: 3429 lambda_k: 1 Loss: 0.0017501889643789321\n",
      "Iteration: 3430 lambda_k: 1 Loss: 0.001750180304808172\n",
      "Iteration: 3431 lambda_k: 1 Loss: 0.0017501717296149364\n",
      "Iteration: 3432 lambda_k: 1 Loss: 0.0017501632379778376\n",
      "Iteration: 3433 lambda_k: 1 Loss: 0.0017501548290833925\n",
      "Iteration: 3434 lambda_k: 1 Loss: 0.0017501465021261115\n",
      "Iteration: 3435 lambda_k: 1 Loss: 0.0017501382563083353\n",
      "Iteration: 3436 lambda_k: 1 Loss: 0.0017501300908401244\n",
      "Iteration: 3437 lambda_k: 1 Loss: 0.0017501220049393272\n",
      "Iteration: 3438 lambda_k: 1 Loss: 0.001750113997831354\n",
      "Iteration: 3439 lambda_k: 1 Loss: 0.001750106068749212\n",
      "Iteration: 3440 lambda_k: 1 Loss: 0.0017500982169333943\n",
      "Iteration: 3441 lambda_k: 1 Loss: 0.0017500904416316984\n",
      "Iteration: 3442 lambda_k: 1 Loss: 0.0017500827420993577\n",
      "Iteration: 3443 lambda_k: 1 Loss: 0.0017500751175988163\n",
      "Iteration: 3444 lambda_k: 1 Loss: 0.0017500675673996736\n",
      "Iteration: 3445 lambda_k: 1 Loss: 0.0017500600907786052\n",
      "Iteration: 3446 lambda_k: 1 Loss: 0.001750052687019528\n",
      "Iteration: 3447 lambda_k: 1 Loss: 0.0017500453554131733\n",
      "Iteration: 3448 lambda_k: 1 Loss: 0.0017500380952572097\n",
      "Iteration: 3449 lambda_k: 1 Loss: 0.0017500309058561042\n",
      "Iteration: 3450 lambda_k: 1 Loss: 0.0017500237865212549\n",
      "Iteration: 3451 lambda_k: 1 Loss: 0.0017500167365706103\n",
      "Iteration: 3452 lambda_k: 1 Loss: 0.001750009755328785\n",
      "Iteration: 3453 lambda_k: 1 Loss: 0.0017500028421270325\n",
      "Iteration: 3454 lambda_k: 1 Loss: 0.0017499959963031048\n",
      "Iteration: 3455 lambda_k: 1 Loss: 0.0017499892172011804\n",
      "Iteration: 3456 lambda_k: 1 Loss: 0.0017499825041718626\n",
      "Iteration: 3457 lambda_k: 1 Loss: 0.001749975856572012\n",
      "Iteration: 3458 lambda_k: 1 Loss: 0.0017499692737648455\n",
      "Iteration: 3459 lambda_k: 1 Loss: 0.0017499627551197032\n",
      "Iteration: 3460 lambda_k: 1 Loss: 0.0017499563000120167\n",
      "Iteration: 3461 lambda_k: 1 Loss: 0.0017499499078234527\n",
      "Iteration: 3462 lambda_k: 1 Loss: 0.001749943577941598\n",
      "Iteration: 3463 lambda_k: 1 Loss: 0.0017499373097599957\n",
      "Iteration: 3464 lambda_k: 1 Loss: 0.001749931102678196\n",
      "Iteration: 3465 lambda_k: 1 Loss: 0.0017499249561014794\n",
      "Iteration: 3466 lambda_k: 1 Loss: 0.0017499188694409594\n",
      "Iteration: 3467 lambda_k: 1 Loss: 0.0017499128421134447\n",
      "Iteration: 3468 lambda_k: 1 Loss: 0.0017499068735415293\n",
      "Iteration: 3469 lambda_k: 1 Loss: 0.0017499009631533275\n",
      "Iteration: 3470 lambda_k: 1 Loss: 0.0017498951103825362\n",
      "Iteration: 3471 lambda_k: 1 Loss: 0.0017498893146683993\n",
      "Iteration: 3472 lambda_k: 1 Loss: 0.0017498835754555537\n",
      "Iteration: 3473 lambda_k: 1 Loss: 0.0017498778921941603\n",
      "Iteration: 3474 lambda_k: 1 Loss: 0.0017498722643395582\n",
      "Iteration: 3475 lambda_k: 1 Loss: 0.0017498666913525862\n",
      "Iteration: 3476 lambda_k: 1 Loss: 0.0017498611726992333\n",
      "Iteration: 3477 lambda_k: 1 Loss: 0.0017498557078506046\n",
      "Iteration: 3478 lambda_k: 1 Loss: 0.0017498502962831012\n",
      "Iteration: 3479 lambda_k: 1 Loss: 0.001749844937478194\n",
      "Iteration: 3480 lambda_k: 1 Loss: 0.0017498396309222717\n",
      "Iteration: 3481 lambda_k: 1 Loss: 0.0017498343761068581\n",
      "Iteration: 3482 lambda_k: 1 Loss: 0.001749829172528368\n",
      "Iteration: 3483 lambda_k: 1 Loss: 0.00174982401968814\n",
      "Iteration: 3484 lambda_k: 1 Loss: 0.0017498189170923773\n",
      "Iteration: 3485 lambda_k: 1 Loss: 0.0017498138642520606\n",
      "Iteration: 3486 lambda_k: 1 Loss: 0.0017498088606829082\n",
      "Iteration: 3487 lambda_k: 1 Loss: 0.0017498039059053764\n",
      "Iteration: 3488 lambda_k: 1 Loss: 0.0017497989994446047\n",
      "Iteration: 3489 lambda_k: 1 Loss: 0.001749794140830302\n",
      "Iteration: 3490 lambda_k: 1 Loss: 0.0017497893295967948\n",
      "Iteration: 3491 lambda_k: 1 Loss: 0.0017497845652829486\n",
      "Iteration: 3492 lambda_k: 1 Loss: 0.0017497798474321459\n",
      "Iteration: 3493 lambda_k: 1 Loss: 0.0017497751755920939\n",
      "Iteration: 3494 lambda_k: 1 Loss: 0.001749770549315011\n",
      "Iteration: 3495 lambda_k: 1 Loss: 0.0017497659681574747\n",
      "Iteration: 3496 lambda_k: 1 Loss: 0.0017497614316802558\n",
      "Iteration: 3497 lambda_k: 1 Loss: 0.0017497569394485742\n",
      "Iteration: 3498 lambda_k: 1 Loss: 0.001749752491031745\n",
      "Iteration: 3499 lambda_k: 1 Loss: 0.0017497480860032754\n",
      "Iteration: 3500 lambda_k: 1 Loss: 0.0017497437239409074\n",
      "Iteration: 3501 lambda_k: 1 Loss: 0.0017497394044264033\n",
      "Iteration: 3502 lambda_k: 1 Loss: 0.0017497351270456822\n",
      "Iteration: 3503 lambda_k: 1 Loss: 0.0017497308913885792\n",
      "Iteration: 3504 lambda_k: 1 Loss: 0.0017497266970490381\n",
      "Iteration: 3505 lambda_k: 1 Loss: 0.0017497225436249445\n",
      "Iteration: 3506 lambda_k: 1 Loss: 0.0017497184307179753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3507 lambda_k: 1 Loss: 0.0017497143579337992\n",
      "Iteration: 3508 lambda_k: 1 Loss: 0.0017497103248818495\n",
      "Iteration: 3509 lambda_k: 1 Loss: 0.0017497063311753532\n",
      "Iteration: 3510 lambda_k: 1 Loss: 0.001749702376431403\n",
      "Iteration: 3511 lambda_k: 1 Loss: 0.0017496984602707168\n",
      "Iteration: 3512 lambda_k: 1 Loss: 0.0017496945823177686\n",
      "Iteration: 3513 lambda_k: 1 Loss: 0.001749690742200604\n",
      "Iteration: 3514 lambda_k: 1 Loss: 0.0017496869395509365\n",
      "Iteration: 3515 lambda_k: 1 Loss: 0.001749683174004122\n",
      "Iteration: 3516 lambda_k: 1 Loss: 0.0017496794451989381\n",
      "Iteration: 3517 lambda_k: 1 Loss: 0.0017496757527777729\n",
      "Iteration: 3518 lambda_k: 1 Loss: 0.0017496720963864564\n",
      "Iteration: 3519 lambda_k: 1 Loss: 0.0017496684756742337\n",
      "Iteration: 3520 lambda_k: 1 Loss: 0.001749664890293813\n",
      "Iteration: 3521 lambda_k: 1 Loss: 0.0017496613399012205\n",
      "Iteration: 3522 lambda_k: 1 Loss: 0.0017496578241559457\n",
      "Iteration: 3523 lambda_k: 1 Loss: 0.0017496543427206438\n",
      "Iteration: 3524 lambda_k: 1 Loss: 0.0017496508952613093\n",
      "Iteration: 3525 lambda_k: 1 Loss: 0.0017496474814472334\n",
      "Iteration: 3526 lambda_k: 1 Loss: 0.0017496441009508556\n",
      "Iteration: 3527 lambda_k: 1 Loss: 0.0017496407534478785\n",
      "Iteration: 3528 lambda_k: 1 Loss: 0.0017496374386170958\n",
      "Iteration: 3529 lambda_k: 1 Loss: 0.0017496341561403796\n",
      "Iteration: 3530 lambda_k: 1 Loss: 0.001749630905702822\n",
      "Iteration: 3531 lambda_k: 1 Loss: 0.0017496276869924463\n",
      "Iteration: 3532 lambda_k: 1 Loss: 0.0017496244997004412\n",
      "Iteration: 3533 lambda_k: 1 Loss: 0.0017496213435209199\n",
      "Iteration: 3534 lambda_k: 1 Loss: 0.0017496182181509449\n",
      "Iteration: 3535 lambda_k: 1 Loss: 0.00174961512329059\n",
      "Iteration: 3536 lambda_k: 1 Loss: 0.0017496120586427983\n",
      "Iteration: 3537 lambda_k: 1 Loss: 0.0017496090239134726\n",
      "Iteration: 3538 lambda_k: 1 Loss: 0.0017496060188112992\n",
      "Iteration: 3539 lambda_k: 1 Loss: 0.001749603043047789\n",
      "Iteration: 3540 lambda_k: 1 Loss: 0.0017496000963373125\n",
      "Iteration: 3541 lambda_k: 1 Loss: 0.0017495971783969748\n",
      "Iteration: 3542 lambda_k: 1 Loss: 0.001749594288946688\n",
      "Iteration: 3543 lambda_k: 1 Loss: 0.0017495914277090706\n",
      "Iteration: 3544 lambda_k: 1 Loss: 0.0017495885944093877\n",
      "Iteration: 3545 lambda_k: 1 Loss: 0.001749585788775616\n",
      "Iteration: 3546 lambda_k: 1 Loss: 0.001749583010538358\n",
      "Iteration: 3547 lambda_k: 1 Loss: 0.0017495802594308856\n",
      "Iteration: 3548 lambda_k: 1 Loss: 0.0017495775351890326\n",
      "Iteration: 3549 lambda_k: 1 Loss: 0.001749574837551175\n",
      "Iteration: 3550 lambda_k: 1 Loss: 0.0017495721662582602\n",
      "Iteration: 3551 lambda_k: 1 Loss: 0.001749569521053766\n",
      "Iteration: 3552 lambda_k: 1 Loss: 0.0017495669016835756\n",
      "Iteration: 3553 lambda_k: 1 Loss: 0.0017495643078962258\n",
      "Iteration: 3554 lambda_k: 1 Loss: 0.0017495617394425304\n",
      "Iteration: 3555 lambda_k: 1 Loss: 0.001749559196075833\n",
      "Iteration: 3556 lambda_k: 1 Loss: 0.0017495566775517868\n",
      "Iteration: 3557 lambda_k: 1 Loss: 0.0017495541836284772\n",
      "Iteration: 3558 lambda_k: 1 Loss: 0.0017495517140662983\n",
      "Iteration: 3559 lambda_k: 1 Loss: 0.0017495492686280425\n",
      "Iteration: 3560 lambda_k: 1 Loss: 0.0017495468470786993\n",
      "Iteration: 3561 lambda_k: 1 Loss: 0.0017495444491856802\n",
      "Iteration: 3562 lambda_k: 1 Loss: 0.0017495420747186106\n",
      "Iteration: 3563 lambda_k: 1 Loss: 0.0017495397234493066\n",
      "Iteration: 3564 lambda_k: 1 Loss: 0.001749537395151852\n",
      "Iteration: 3565 lambda_k: 1 Loss: 0.0017495350896025042\n",
      "Iteration: 3566 lambda_k: 1 Loss: 0.0017495328065796967\n",
      "Iteration: 3567 lambda_k: 1 Loss: 0.0017495305458640402\n",
      "Iteration: 3568 lambda_k: 1 Loss: 0.0017495283072382318\n",
      "Iteration: 3569 lambda_k: 1 Loss: 0.001749526090487084\n",
      "Iteration: 3570 lambda_k: 1 Loss: 0.0017495238953975486\n",
      "Iteration: 3571 lambda_k: 1 Loss: 0.0017495217217586827\n",
      "Iteration: 3572 lambda_k: 1 Loss: 0.0017495195693614752\n",
      "Iteration: 3573 lambda_k: 1 Loss: 0.0017495174379990723\n",
      "Iteration: 3574 lambda_k: 1 Loss: 0.0017495153274665709\n",
      "Iteration: 3575 lambda_k: 1 Loss: 0.0017495132375610063\n",
      "Iteration: 3576 lambda_k: 1 Loss: 0.0017495111680814778\n",
      "Iteration: 3577 lambda_k: 1 Loss: 0.0017495091188290013\n",
      "Iteration: 3578 lambda_k: 1 Loss: 0.001749507089606588\n",
      "Iteration: 3579 lambda_k: 1 Loss: 0.00174950508021907\n",
      "Iteration: 3580 lambda_k: 1 Loss: 0.001749503090473138\n",
      "Iteration: 3581 lambda_k: 1 Loss: 0.001749501120177522\n",
      "Iteration: 3582 lambda_k: 1 Loss: 0.0017494991691427112\n",
      "Iteration: 3583 lambda_k: 1 Loss: 0.0017494972371810858\n",
      "Iteration: 3584 lambda_k: 1 Loss: 0.0017494953241068035\n",
      "Iteration: 3585 lambda_k: 1 Loss: 0.001749493429735816\n",
      "Iteration: 3586 lambda_k: 1 Loss: 0.001749491553885883\n",
      "Iteration: 3587 lambda_k: 1 Loss: 0.001749489696376612\n",
      "Iteration: 3588 lambda_k: 1 Loss: 0.001749487857029241\n",
      "Iteration: 3589 lambda_k: 1 Loss: 0.0017494860356668878\n",
      "Iteration: 3590 lambda_k: 1 Loss: 0.0017494842321142284\n",
      "Iteration: 3591 lambda_k: 1 Loss: 0.001749482446197731\n",
      "Iteration: 3592 lambda_k: 1 Loss: 0.0017494806777455893\n",
      "Iteration: 3593 lambda_k: 1 Loss: 0.0017494789265875977\n",
      "Iteration: 3594 lambda_k: 1 Loss: 0.0017494771925552516\n",
      "Iteration: 3595 lambda_k: 1 Loss: 0.0017494754754816542\n",
      "Iteration: 3596 lambda_k: 1 Loss: 0.0017494737752015393\n",
      "Iteration: 3597 lambda_k: 1 Loss: 0.001749472091551255\n",
      "Iteration: 3598 lambda_k: 1 Loss: 0.0017494704243687976\n",
      "Iteration: 3599 lambda_k: 1 Loss: 0.0017494687734935877\n",
      "Iteration: 3600 lambda_k: 1 Loss: 0.0017494671387667164\n",
      "Iteration: 3601 lambda_k: 1 Loss: 0.0017494655200308035\n",
      "Iteration: 3602 lambda_k: 1 Loss: 0.001749463917130056\n",
      "Iteration: 3603 lambda_k: 1 Loss: 0.0017494623299100693\n",
      "Iteration: 3604 lambda_k: 1 Loss: 0.0017494607582180725\n",
      "Iteration: 3605 lambda_k: 1 Loss: 0.0017494592019026874\n",
      "Iteration: 3606 lambda_k: 1 Loss: 0.0017494576608139993\n",
      "Iteration: 3607 lambda_k: 1 Loss: 0.0017494561348035842\n",
      "Iteration: 3608 lambda_k: 1 Loss: 0.0017494546237245905\n",
      "Iteration: 3609 lambda_k: 1 Loss: 0.001749453127431386\n",
      "Iteration: 3610 lambda_k: 1 Loss: 0.0017494516457798523\n",
      "Iteration: 3611 lambda_k: 1 Loss: 0.0017494501786272582\n",
      "Iteration: 3612 lambda_k: 1 Loss: 0.0017494487258322671\n",
      "Iteration: 3613 lambda_k: 1 Loss: 0.0017494472872548656\n",
      "Iteration: 3614 lambda_k: 1 Loss: 0.001749445862756528\n",
      "Iteration: 3615 lambda_k: 1 Loss: 0.0017494444521999398\n",
      "Iteration: 3616 lambda_k: 1 Loss: 0.0017494430554492386\n",
      "Iteration: 3617 lambda_k: 1 Loss: 0.0017494416723698232\n",
      "Iteration: 3618 lambda_k: 1 Loss: 0.0017494403028284003\n",
      "Iteration: 3619 lambda_k: 1 Loss: 0.0017494389466929032\n",
      "Iteration: 3620 lambda_k: 1 Loss: 0.0017494376038327392\n",
      "Iteration: 3621 lambda_k: 1 Loss: 0.0017494362741183687\n",
      "Iteration: 3622 lambda_k: 1 Loss: 0.0017494349574216366\n",
      "Iteration: 3623 lambda_k: 1 Loss: 0.0017494336536156983\n",
      "Iteration: 3624 lambda_k: 1 Loss: 0.0017494323625747677\n",
      "Iteration: 3625 lambda_k: 1 Loss: 0.0017494310841744357\n",
      "Iteration: 3626 lambda_k: 1 Loss: 0.0017494298182913564\n",
      "Iteration: 3627 lambda_k: 1 Loss: 0.0017494285648035528\n",
      "Iteration: 3628 lambda_k: 1 Loss: 0.001749427323590101\n",
      "Iteration: 3629 lambda_k: 1 Loss: 0.001749426094531262\n",
      "Iteration: 3630 lambda_k: 1 Loss: 0.0017494248775085642\n",
      "Iteration: 3631 lambda_k: 1 Loss: 0.0017494236724046178\n",
      "Iteration: 3632 lambda_k: 1 Loss: 0.0017494224791031316\n",
      "Iteration: 3633 lambda_k: 1 Loss: 0.0017494212974890947\n",
      "Iteration: 3634 lambda_k: 1 Loss: 0.0017494201274484327\n",
      "Iteration: 3635 lambda_k: 1 Loss: 0.0017494189688683087\n",
      "Iteration: 3636 lambda_k: 1 Loss: 0.0017494178216368725\n",
      "Iteration: 3637 lambda_k: 1 Loss: 0.0017494166856435358\n",
      "Iteration: 3638 lambda_k: 1 Loss: 0.0017494155607786094\n",
      "Iteration: 3639 lambda_k: 1 Loss: 0.00174941444693353\n",
      "Iteration: 3640 lambda_k: 1 Loss: 0.0017494133440008385\n",
      "Iteration: 3641 lambda_k: 1 Loss: 0.0017494122518740328\n",
      "Iteration: 3642 lambda_k: 1 Loss: 0.0017494111704477516\n",
      "Iteration: 3643 lambda_k: 1 Loss: 0.0017494100996175654\n",
      "Iteration: 3644 lambda_k: 1 Loss: 0.0017494090392801628\n",
      "Iteration: 3645 lambda_k: 1 Loss: 0.0017494079893331492\n",
      "Iteration: 3646 lambda_k: 1 Loss: 0.001749406949675133\n",
      "Iteration: 3647 lambda_k: 1 Loss: 0.0017494059202057455\n",
      "Iteration: 3648 lambda_k: 1 Loss: 0.0017494049008255914\n",
      "Iteration: 3649 lambda_k: 1 Loss: 0.001749403891436212\n",
      "Iteration: 3650 lambda_k: 1 Loss: 0.0017494028919400814\n",
      "Iteration: 3651 lambda_k: 1 Loss: 0.0017494019022407646\n",
      "Iteration: 3652 lambda_k: 1 Loss: 0.0017494009222425372\n",
      "Iteration: 3653 lambda_k: 1 Loss: 0.001749399951850838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3654 lambda_k: 1 Loss: 0.001749398990971896\n",
      "Iteration: 3655 lambda_k: 1 Loss: 0.0017493980395129147\n",
      "Iteration: 3656 lambda_k: 1 Loss: 0.0017493970973818905\n",
      "Iteration: 3657 lambda_k: 1 Loss: 0.001749396164487845\n",
      "Iteration: 3658 lambda_k: 1 Loss: 0.0017493952407405579\n",
      "Iteration: 3659 lambda_k: 1 Loss: 0.0017493943260508229\n",
      "Iteration: 3660 lambda_k: 1 Loss: 0.0017493934203302584\n",
      "Iteration: 3661 lambda_k: 1 Loss: 0.001749392523491275\n",
      "Iteration: 3662 lambda_k: 1 Loss: 0.0017493916354471783\n",
      "Iteration: 3663 lambda_k: 1 Loss: 0.0017493907561121711\n",
      "Iteration: 3664 lambda_k: 1 Loss: 0.0017493898854012112\n",
      "Iteration: 3665 lambda_k: 1 Loss: 0.0017493890232301611\n",
      "Iteration: 3666 lambda_k: 1 Loss: 0.0017493881695156283\n",
      "Iteration: 3667 lambda_k: 1 Loss: 0.0017493873241749963\n",
      "Iteration: 3668 lambda_k: 1 Loss: 0.0017493864871266083\n",
      "Iteration: 3669 lambda_k: 1 Loss: 0.001749385658289481\n",
      "Iteration: 3670 lambda_k: 1 Loss: 0.0017493848375834216\n",
      "Iteration: 3671 lambda_k: 1 Loss: 0.0017493840249290598\n",
      "Iteration: 3672 lambda_k: 1 Loss: 0.001749383220247769\n",
      "Iteration: 3673 lambda_k: 1 Loss: 0.0017493824234617458\n",
      "Iteration: 3674 lambda_k: 1 Loss: 0.0017493816344938719\n",
      "Iteration: 3675 lambda_k: 1 Loss: 0.001749380853267786\n",
      "Iteration: 3676 lambda_k: 1 Loss: 0.0017493800797079434\n",
      "Iteration: 3677 lambda_k: 1 Loss: 0.0017493793137394738\n",
      "Iteration: 3678 lambda_k: 1 Loss: 0.0017493785552882438\n",
      "Iteration: 3679 lambda_k: 1 Loss: 0.0017493778042808206\n",
      "Iteration: 3680 lambda_k: 1 Loss: 0.0017493770606445518\n",
      "Iteration: 3681 lambda_k: 1 Loss: 0.0017493763243074272\n",
      "Iteration: 3682 lambda_k: 1 Loss: 0.0017493755951981046\n",
      "Iteration: 3683 lambda_k: 1 Loss: 0.0017493748732460466\n",
      "Iteration: 3684 lambda_k: 1 Loss: 0.0017493741583813538\n",
      "Iteration: 3685 lambda_k: 1 Loss: 0.0017493734505347836\n",
      "Iteration: 3686 lambda_k: 1 Loss: 0.0017493727496378083\n",
      "Iteration: 3687 lambda_k: 1 Loss: 0.0017493720556225683\n",
      "Iteration: 3688 lambda_k: 1 Loss: 0.0017493713684217443\n",
      "Iteration: 3689 lambda_k: 1 Loss: 0.001749370687968843\n",
      "Iteration: 3690 lambda_k: 1 Loss: 0.0017493700141979452\n",
      "Iteration: 3691 lambda_k: 1 Loss: 0.0017493693470437628\n",
      "Iteration: 3692 lambda_k: 1 Loss: 0.0017493686864416865\n",
      "Iteration: 3693 lambda_k: 1 Loss: 0.0017493680323276116\n",
      "Iteration: 3694 lambda_k: 1 Loss: 0.0017493673846382032\n",
      "Iteration: 3695 lambda_k: 1 Loss: 0.001749366743310679\n",
      "Iteration: 3696 lambda_k: 1 Loss: 0.0017493661082829588\n",
      "Iteration: 3697 lambda_k: 1 Loss: 0.0017493654794933915\n",
      "Iteration: 3698 lambda_k: 1 Loss: 0.001749364856881077\n",
      "Iteration: 3699 lambda_k: 1 Loss: 0.0017493642403856283\n",
      "Iteration: 3700 lambda_k: 1 Loss: 0.0017493636299472856\n",
      "Iteration: 3701 lambda_k: 1 Loss: 0.0017493630255068153\n",
      "Iteration: 3702 lambda_k: 1 Loss: 0.0017493624270055834\n",
      "Iteration: 3703 lambda_k: 1 Loss: 0.0017493618343855973\n",
      "Iteration: 3704 lambda_k: 1 Loss: 0.0017493612475893868\n",
      "Iteration: 3705 lambda_k: 1 Loss: 0.0017493606665599827\n",
      "Iteration: 3706 lambda_k: 1 Loss: 0.0017493600912410413\n",
      "Iteration: 3707 lambda_k: 1 Loss: 0.0017493595215767657\n",
      "Iteration: 3708 lambda_k: 1 Loss: 0.001749358957511908\n",
      "Iteration: 3709 lambda_k: 1 Loss: 0.0017493583989917096\n",
      "Iteration: 3710 lambda_k: 1 Loss: 0.0017493578459619374\n",
      "Iteration: 3711 lambda_k: 1 Loss: 0.001749357298368895\n",
      "Iteration: 3712 lambda_k: 1 Loss: 0.0017493567561595081\n",
      "Iteration: 3713 lambda_k: 1 Loss: 0.0017493562192811064\n",
      "Iteration: 3714 lambda_k: 1 Loss: 0.0017493556876815538\n",
      "Iteration: 3715 lambda_k: 1 Loss: 0.0017493551613092747\n",
      "Iteration: 3716 lambda_k: 1 Loss: 0.0017493546401131366\n",
      "Iteration: 3717 lambda_k: 1 Loss: 0.0017493541240425643\n",
      "Iteration: 3718 lambda_k: 1 Loss: 0.0017493536130474185\n",
      "Iteration: 3719 lambda_k: 1 Loss: 0.0017493531070780601\n",
      "Iteration: 3720 lambda_k: 1 Loss: 0.0017493526060853267\n",
      "Iteration: 3721 lambda_k: 1 Loss: 0.001749352110020545\n",
      "Iteration: 3722 lambda_k: 1 Loss: 0.0017493516188355326\n",
      "Iteration: 3723 lambda_k: 1 Loss: 0.0017493511324826025\n",
      "Iteration: 3724 lambda_k: 1 Loss: 0.0017493506509144489\n",
      "Iteration: 3725 lambda_k: 1 Loss: 0.0017493501740842997\n",
      "Iteration: 3726 lambda_k: 1 Loss: 0.0017493497019458594\n",
      "Iteration: 3727 lambda_k: 1 Loss: 0.001749349234453158\n",
      "Iteration: 3728 lambda_k: 1 Loss: 0.0017493487715607773\n",
      "Iteration: 3729 lambda_k: 1 Loss: 0.001749348313223691\n",
      "Iteration: 3730 lambda_k: 1 Loss: 0.001749347859397382\n",
      "Iteration: 3731 lambda_k: 1 Loss: 0.0017493474100376766\n",
      "Iteration: 3732 lambda_k: 1 Loss: 0.0017493469651009557\n",
      "Iteration: 3733 lambda_k: 1 Loss: 0.001749346524543862\n",
      "Iteration: 3734 lambda_k: 1 Loss: 0.0017493460883235554\n",
      "Iteration: 3735 lambda_k: 1 Loss: 0.0017493456563976145\n",
      "Iteration: 3736 lambda_k: 1 Loss: 0.0017493452287240983\n",
      "Iteration: 3737 lambda_k: 1 Loss: 0.0017493448052612564\n",
      "Iteration: 3738 lambda_k: 1 Loss: 0.0017493443859680257\n",
      "Iteration: 3739 lambda_k: 1 Loss: 0.0017493439708034936\n",
      "Iteration: 3740 lambda_k: 1 Loss: 0.0017493435597272582\n",
      "Iteration: 3741 lambda_k: 1 Loss: 0.001749343152699412\n",
      "Iteration: 3742 lambda_k: 1 Loss: 0.001749342749680223\n",
      "Iteration: 3743 lambda_k: 1 Loss: 0.0017493423506305449\n",
      "Iteration: 3744 lambda_k: 1 Loss: 0.0017493419555114542\n",
      "Iteration: 3745 lambda_k: 1 Loss: 0.0017493415642845212\n",
      "Iteration: 3746 lambda_k: 1 Loss: 0.0017493411769116149\n",
      "Iteration: 3747 lambda_k: 1 Loss: 0.001749340793354999\n",
      "Iteration: 3748 lambda_k: 1 Loss: 0.0017493404135773922\n",
      "Iteration: 3749 lambda_k: 1 Loss: 0.0017493400375417035\n",
      "Iteration: 3750 lambda_k: 1 Loss: 0.0017493396652113484\n",
      "Iteration: 3751 lambda_k: 1 Loss: 0.0017493392965500694\n",
      "Iteration: 3752 lambda_k: 1 Loss: 0.001749338931521873\n",
      "Iteration: 3753 lambda_k: 1 Loss: 0.001749338570091305\n",
      "Iteration: 3754 lambda_k: 1 Loss: 0.0017493382122230235\n",
      "Iteration: 3755 lambda_k: 1 Loss: 0.001749337857882146\n",
      "Iteration: 3756 lambda_k: 1 Loss: 0.001749337507034242\n",
      "Iteration: 3757 lambda_k: 1 Loss: 0.0017493371596449525\n",
      "Iteration: 3758 lambda_k: 1 Loss: 0.0017493368156805578\n",
      "Iteration: 3759 lambda_k: 1 Loss: 0.0017493364751074304\n",
      "Iteration: 3760 lambda_k: 1 Loss: 0.0017493361378923449\n",
      "Iteration: 3761 lambda_k: 1 Loss: 0.0017493358040024333\n",
      "Iteration: 3762 lambda_k: 1 Loss: 0.001749335473405149\n",
      "Iteration: 3763 lambda_k: 1 Loss: 0.0017493351460681943\n",
      "Iteration: 3764 lambda_k: 1 Loss: 0.0017493348219596273\n",
      "Iteration: 3765 lambda_k: 1 Loss: 0.0017493345010478776\n",
      "Iteration: 3766 lambda_k: 1 Loss: 0.0017493341833015915\n",
      "Iteration: 3767 lambda_k: 1 Loss: 0.001749333868689815\n",
      "Iteration: 3768 lambda_k: 1 Loss: 0.0017493335571817318\n",
      "Iteration: 3769 lambda_k: 1 Loss: 0.0017493332487470011\n",
      "Iteration: 3770 lambda_k: 1 Loss: 0.0017493329433554549\n",
      "Iteration: 3771 lambda_k: 1 Loss: 0.001749332640977344\n",
      "Iteration: 3772 lambda_k: 1 Loss: 0.001749332341583112\n",
      "Iteration: 3773 lambda_k: 1 Loss: 0.001749332045143505\n",
      "Iteration: 3774 lambda_k: 1 Loss: 0.0017493317516295997\n",
      "Iteration: 3775 lambda_k: 1 Loss: 0.0017493314610126497\n",
      "Iteration: 3776 lambda_k: 1 Loss: 0.0017493311732642968\n",
      "Iteration: 3777 lambda_k: 1 Loss: 0.0017493308883564413\n",
      "Iteration: 3778 lambda_k: 1 Loss: 0.0017493306062612618\n",
      "Iteration: 3779 lambda_k: 1 Loss: 0.0017493303269511887\n",
      "Iteration: 3780 lambda_k: 1 Loss: 0.0017493300503988768\n",
      "Iteration: 3781 lambda_k: 1 Loss: 0.0017493297765773133\n",
      "Iteration: 3782 lambda_k: 1 Loss: 0.0017493295054596888\n",
      "Iteration: 3783 lambda_k: 1 Loss: 0.0017493292370195338\n",
      "Iteration: 3784 lambda_k: 1 Loss: 0.001749328971230572\n",
      "Iteration: 3785 lambda_k: 1 Loss: 0.0017493287080668273\n",
      "Iteration: 3786 lambda_k: 1 Loss: 0.001749328447502596\n",
      "Iteration: 3787 lambda_k: 1 Loss: 0.001749328189512339\n",
      "Iteration: 3788 lambda_k: 1 Loss: 0.0017493279340708343\n",
      "Iteration: 3789 lambda_k: 1 Loss: 0.0017493276811530872\n",
      "Iteration: 3790 lambda_k: 1 Loss: 0.0017493274307343824\n",
      "Iteration: 3791 lambda_k: 1 Loss: 0.00174932718279012\n",
      "Iteration: 3792 lambda_k: 1 Loss: 0.0017493269372960661\n",
      "Iteration: 3793 lambda_k: 1 Loss: 0.0017493266942282175\n",
      "Iteration: 3794 lambda_k: 1 Loss: 0.0017493264535627594\n",
      "Iteration: 3795 lambda_k: 1 Loss: 0.0017493262152760763\n",
      "Iteration: 3796 lambda_k: 1 Loss: 0.001749325979344863\n",
      "Iteration: 3797 lambda_k: 1 Loss: 0.0017493257457460373\n",
      "Iteration: 3798 lambda_k: 1 Loss: 0.0017493255144566914\n",
      "Iteration: 3799 lambda_k: 1 Loss: 0.0017493252854542194\n",
      "Iteration: 3800 lambda_k: 1 Loss: 0.001749325058716157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3801 lambda_k: 1 Loss: 0.0017493248342202211\n",
      "Iteration: 3802 lambda_k: 1 Loss: 0.0017493246119444628\n",
      "Iteration: 3803 lambda_k: 1 Loss: 0.0017493243918671325\n",
      "Iteration: 3804 lambda_k: 1 Loss: 0.0017493241739665677\n",
      "Iteration: 3805 lambda_k: 1 Loss: 0.0017493239582214587\n",
      "Iteration: 3806 lambda_k: 1 Loss: 0.0017493237446106133\n",
      "Iteration: 3807 lambda_k: 1 Loss: 0.0017493235331131546\n",
      "Iteration: 3808 lambda_k: 1 Loss: 0.001749323323708292\n",
      "Iteration: 3809 lambda_k: 1 Loss: 0.0017493231163754738\n",
      "Iteration: 3810 lambda_k: 1 Loss: 0.0017493229110943805\n",
      "Iteration: 3811 lambda_k: 1 Loss: 0.0017493227078448621\n",
      "Iteration: 3812 lambda_k: 1 Loss: 0.001749322506606922\n",
      "Iteration: 3813 lambda_k: 1 Loss: 0.0017493223073608971\n",
      "Iteration: 3814 lambda_k: 1 Loss: 0.0017493221100871332\n",
      "Iteration: 3815 lambda_k: 1 Loss: 0.0017493219147663415\n",
      "Iteration: 3816 lambda_k: 1 Loss: 0.001749321721379331\n",
      "Iteration: 3817 lambda_k: 1 Loss: 0.0017493215299071254\n",
      "Iteration: 3818 lambda_k: 1 Loss: 0.0017493213403308985\n",
      "Iteration: 3819 lambda_k: 1 Loss: 0.001749321152631999\n",
      "Iteration: 3820 lambda_k: 1 Loss: 0.0017493209667920372\n",
      "Iteration: 3821 lambda_k: 1 Loss: 0.0017493207827927438\n",
      "Iteration: 3822 lambda_k: 1 Loss: 0.0017493206006159916\n",
      "Iteration: 3823 lambda_k: 1 Loss: 0.0017493204202439154\n",
      "Iteration: 3824 lambda_k: 1 Loss: 0.0017493202416588016\n",
      "Iteration: 3825 lambda_k: 1 Loss: 0.001749320064843095\n",
      "Iteration: 3826 lambda_k: 1 Loss: 0.001749319889779356\n",
      "Iteration: 3827 lambda_k: 1 Loss: 0.0017493197164504194\n",
      "Iteration: 3828 lambda_k: 1 Loss: 0.001749319544839285\n",
      "Iteration: 3829 lambda_k: 1 Loss: 0.001749319374928955\n",
      "Iteration: 3830 lambda_k: 1 Loss: 0.001749319206702843\n",
      "Iteration: 3831 lambda_k: 1 Loss: 0.0017493190401443061\n",
      "Iteration: 3832 lambda_k: 1 Loss: 0.0017493188752369286\n",
      "Iteration: 3833 lambda_k: 1 Loss: 0.001749318711964549\n",
      "Iteration: 3834 lambda_k: 1 Loss: 0.00174931855031106\n",
      "Iteration: 3835 lambda_k: 1 Loss: 0.0017493183902606412\n",
      "Iteration: 3836 lambda_k: 1 Loss: 0.0017493182317974163\n",
      "Iteration: 3837 lambda_k: 1 Loss: 0.0017493180749058692\n",
      "Iteration: 3838 lambda_k: 1 Loss: 0.0017493179195704573\n",
      "Iteration: 3839 lambda_k: 1 Loss: 0.0017493177657759429\n",
      "Iteration: 3840 lambda_k: 1 Loss: 0.0017493176135071422\n",
      "Iteration: 3841 lambda_k: 1 Loss: 0.0017493174627490685\n",
      "Iteration: 3842 lambda_k: 1 Loss: 0.0017493173134869118\n",
      "Iteration: 3843 lambda_k: 1 Loss: 0.0017493171657058841\n",
      "Iteration: 3844 lambda_k: 1 Loss: 0.0017493170193914735\n",
      "Iteration: 3845 lambda_k: 1 Loss: 0.0017493168745292287\n",
      "Iteration: 3846 lambda_k: 1 Loss: 0.001749316731104871\n",
      "Iteration: 3847 lambda_k: 1 Loss: 0.0017493165891042663\n",
      "Iteration: 3848 lambda_k: 1 Loss: 0.0017493164485134051\n",
      "Iteration: 3849 lambda_k: 1 Loss: 0.0017493163093184538\n",
      "Iteration: 3850 lambda_k: 1 Loss: 0.0017493161715056065\n",
      "Iteration: 3851 lambda_k: 1 Loss: 0.0017493160350612685\n",
      "Iteration: 3852 lambda_k: 1 Loss: 0.0017493158999720647\n",
      "Iteration: 3853 lambda_k: 1 Loss: 0.0017493157662245606\n",
      "Iteration: 3854 lambda_k: 1 Loss: 0.0017493156338056147\n",
      "Iteration: 3855 lambda_k: 1 Loss: 0.0017493155027021251\n",
      "Iteration: 3856 lambda_k: 1 Loss: 0.0017493153729011386\n",
      "Iteration: 3857 lambda_k: 1 Loss: 0.0017493152443898885\n",
      "Iteration: 3858 lambda_k: 1 Loss: 0.0017493151171556328\n",
      "Iteration: 3859 lambda_k: 1 Loss: 0.0017493149911858377\n",
      "Iteration: 3860 lambda_k: 1 Loss: 0.0017493148664679877\n",
      "Iteration: 3861 lambda_k: 1 Loss: 0.001749314742989828\n",
      "Iteration: 3862 lambda_k: 1 Loss: 0.001749314620739136\n",
      "Iteration: 3863 lambda_k: 1 Loss: 0.0017493144997037739\n",
      "Iteration: 3864 lambda_k: 1 Loss: 0.0017493143798718118\n",
      "Iteration: 3865 lambda_k: 1 Loss: 0.001749314261231452\n",
      "Iteration: 3866 lambda_k: 1 Loss: 0.0017493141437708957\n",
      "Iteration: 3867 lambda_k: 1 Loss: 0.0017493140274785247\n",
      "Iteration: 3868 lambda_k: 1 Loss: 0.001749313912342843\n",
      "Iteration: 3869 lambda_k: 1 Loss: 0.001749313798352493\n",
      "Iteration: 3870 lambda_k: 1 Loss: 0.0017493136854961793\n",
      "Iteration: 3871 lambda_k: 1 Loss: 0.001749313573762686\n",
      "Iteration: 3872 lambda_k: 1 Loss: 0.0017493134631410064\n",
      "Iteration: 3873 lambda_k: 1 Loss: 0.0017493133536202034\n",
      "Iteration: 3874 lambda_k: 1 Loss: 0.0017493132451893688\n",
      "Iteration: 3875 lambda_k: 1 Loss: 0.0017493131378377822\n",
      "Iteration: 3876 lambda_k: 1 Loss: 0.0017493130315548208\n",
      "Iteration: 3877 lambda_k: 1 Loss: 0.00174931292632995\n",
      "Iteration: 3878 lambda_k: 1 Loss: 0.0017493128221527374\n",
      "Iteration: 3879 lambda_k: 1 Loss: 0.0017493127190128458\n",
      "Iteration: 3880 lambda_k: 1 Loss: 0.0017493126169000996\n",
      "Iteration: 3881 lambda_k: 1 Loss: 0.0017493125158043214\n",
      "Iteration: 3882 lambda_k: 1 Loss: 0.0017493124157155365\n",
      "Iteration: 3883 lambda_k: 1 Loss: 0.001749312316623766\n",
      "Iteration: 3884 lambda_k: 1 Loss: 0.0017493122185191752\n",
      "Iteration: 3885 lambda_k: 1 Loss: 0.0017493121213920744\n",
      "Iteration: 3886 lambda_k: 1 Loss: 0.0017493120252328029\n",
      "Iteration: 3887 lambda_k: 1 Loss: 0.0017493119300318236\n",
      "Iteration: 3888 lambda_k: 1 Loss: 0.0017493118357796696\n",
      "Iteration: 3889 lambda_k: 1 Loss: 0.0017493117424670494\n",
      "Iteration: 3890 lambda_k: 1 Loss: 0.001749311650084584\n",
      "Iteration: 3891 lambda_k: 1 Loss: 0.0017493115586232199\n",
      "Iteration: 3892 lambda_k: 1 Loss: 0.0017493114680737656\n",
      "Iteration: 3893 lambda_k: 1 Loss: 0.001749311378427318\n",
      "Iteration: 3894 lambda_k: 1 Loss: 0.001749311289674885\n",
      "Iteration: 3895 lambda_k: 1 Loss: 0.0017493112018077037\n",
      "Iteration: 3896 lambda_k: 1 Loss: 0.001749311114817034\n",
      "Iteration: 3897 lambda_k: 1 Loss: 0.0017493110286942116\n",
      "Iteration: 3898 lambda_k: 1 Loss: 0.001749310943430716\n",
      "Iteration: 3899 lambda_k: 1 Loss: 0.0017493108590180097\n",
      "Iteration: 3900 lambda_k: 1 Loss: 0.00174931077544774\n",
      "Iteration: 3901 lambda_k: 1 Loss: 0.0017493106927115314\n",
      "Iteration: 3902 lambda_k: 1 Loss: 0.0017493106108011811\n",
      "Iteration: 3903 lambda_k: 1 Loss: 0.0017493105297085526\n",
      "Iteration: 3904 lambda_k: 1 Loss: 0.001749310449425561\n",
      "Iteration: 3905 lambda_k: 1 Loss: 0.0017493103699441944\n",
      "Iteration: 3906 lambda_k: 1 Loss: 0.0017493102912565343\n",
      "Iteration: 3907 lambda_k: 1 Loss: 0.001749310213354808\n",
      "Iteration: 3908 lambda_k: 1 Loss: 0.0017493101362312156\n",
      "Iteration: 3909 lambda_k: 1 Loss: 0.00174931005987804\n",
      "Iteration: 3910 lambda_k: 1 Loss: 0.001749309984287754\n",
      "Iteration: 3911 lambda_k: 1 Loss: 0.0017493099094527359\n",
      "Iteration: 3912 lambda_k: 1 Loss: 0.0017493098353655984\n",
      "Iteration: 3913 lambda_k: 1 Loss: 0.001749309762018845\n",
      "Iteration: 3914 lambda_k: 1 Loss: 0.0017493096894052695\n",
      "Iteration: 3915 lambda_k: 1 Loss: 0.001749309617517566\n",
      "Iteration: 3916 lambda_k: 1 Loss: 0.0017493095463485804\n",
      "Iteration: 3917 lambda_k: 1 Loss: 0.0017493094758912209\n",
      "Iteration: 3918 lambda_k: 1 Loss: 0.0017493094061385088\n",
      "Iteration: 3919 lambda_k: 1 Loss: 0.0017493093370834249\n",
      "Iteration: 3920 lambda_k: 1 Loss: 0.0017493092687190324\n",
      "Iteration: 3921 lambda_k: 1 Loss: 0.0017493092010385342\n",
      "Iteration: 3922 lambda_k: 1 Loss: 0.0017493091340352062\n",
      "Iteration: 3923 lambda_k: 1 Loss: 0.0017493090677023501\n",
      "Iteration: 3924 lambda_k: 1 Loss: 0.0017493090020333005\n",
      "Iteration: 3925 lambda_k: 1 Loss: 0.0017493089370214908\n",
      "Iteration: 3926 lambda_k: 1 Loss: 0.001749308872660454\n",
      "Iteration: 3927 lambda_k: 1 Loss: 0.0017493088089437032\n",
      "Iteration: 3928 lambda_k: 1 Loss: 0.0017493087458649833\n",
      "Iteration: 3929 lambda_k: 1 Loss: 0.0017493086834178407\n",
      "Iteration: 3930 lambda_k: 1 Loss: 0.001749308621596141\n",
      "Iteration: 3931 lambda_k: 1 Loss: 0.0017493085603936282\n",
      "Iteration: 3932 lambda_k: 1 Loss: 0.0017493084998042208\n",
      "Iteration: 3933 lambda_k: 1 Loss: 0.0017493084398217964\n",
      "Iteration: 3934 lambda_k: 1 Loss: 0.001749308380440379\n",
      "Iteration: 3935 lambda_k: 1 Loss: 0.0017493083216540703\n",
      "Iteration: 3936 lambda_k: 1 Loss: 0.0017493082634569257\n",
      "Iteration: 3937 lambda_k: 1 Loss: 0.0017493082058431082\n",
      "Iteration: 3938 lambda_k: 1 Loss: 0.0017493081488068824\n",
      "Iteration: 3939 lambda_k: 1 Loss: 0.001749308092342528\n",
      "Iteration: 3940 lambda_k: 1 Loss: 0.001749308036444358\n",
      "Iteration: 3941 lambda_k: 1 Loss: 0.001749307981106771\n",
      "Iteration: 3942 lambda_k: 1 Loss: 0.0017493079263242036\n",
      "Iteration: 3943 lambda_k: 1 Loss: 0.0017493078720911925\n",
      "Iteration: 3944 lambda_k: 1 Loss: 0.0017493078184023217\n",
      "Iteration: 3945 lambda_k: 1 Loss: 0.0017493077652521048\n",
      "Iteration: 3946 lambda_k: 1 Loss: 0.0017493077126352992\n",
      "Iteration: 3947 lambda_k: 1 Loss: 0.0017493076605466032\n",
      "Iteration: 3948 lambda_k: 1 Loss: 0.001749307608980718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3949 lambda_k: 1 Loss: 0.001749307557932546\n",
      "Iteration: 3950 lambda_k: 1 Loss: 0.0017493075073969832\n",
      "Iteration: 3951 lambda_k: 1 Loss: 0.0017493074573688983\n",
      "Iteration: 3952 lambda_k: 1 Loss: 0.001749307407843252\n",
      "Iteration: 3953 lambda_k: 1 Loss: 0.0017493073588150576\n",
      "Iteration: 3954 lambda_k: 1 Loss: 0.0017493073102794335\n",
      "Iteration: 3955 lambda_k: 1 Loss: 0.0017493072622314831\n",
      "Iteration: 3956 lambda_k: 1 Loss: 0.0017493072146663113\n",
      "Iteration: 3957 lambda_k: 1 Loss: 0.0017493071675792067\n",
      "Iteration: 3958 lambda_k: 1 Loss: 0.0017493071209654016\n",
      "Iteration: 3959 lambda_k: 1 Loss: 0.0017493070748202388\n",
      "Iteration: 3960 lambda_k: 1 Loss: 0.0017493070291390142\n",
      "Iteration: 3961 lambda_k: 1 Loss: 0.0017493069839171385\n",
      "Iteration: 3962 lambda_k: 1 Loss: 0.001749306939150031\n",
      "Iteration: 3963 lambda_k: 1 Loss: 0.0017493068948332725\n",
      "Iteration: 3964 lambda_k: 1 Loss: 0.0017493068509623077\n",
      "Iteration: 3965 lambda_k: 1 Loss: 0.0017493068075327843\n",
      "Iteration: 3966 lambda_k: 1 Loss: 0.00174930676454031\n",
      "Iteration: 3967 lambda_k: 1 Loss: 0.0017493067219805183\n",
      "Iteration: 3968 lambda_k: 1 Loss: 0.0017493066798490939\n",
      "Iteration: 3969 lambda_k: 1 Loss: 0.0017493066381418674\n",
      "Iteration: 3970 lambda_k: 1 Loss: 0.0017493065968545857\n",
      "Iteration: 3971 lambda_k: 1 Loss: 0.0017493065559830636\n",
      "Iteration: 3972 lambda_k: 1 Loss: 0.0017493065155231777\n",
      "Iteration: 3973 lambda_k: 1 Loss: 0.0017493064754708558\n",
      "Iteration: 3974 lambda_k: 1 Loss: 0.0017493064358220403\n",
      "Iteration: 3975 lambda_k: 1 Loss: 0.001749306396572734\n",
      "Iteration: 3976 lambda_k: 1 Loss: 0.00174930635771893\n",
      "Iteration: 3977 lambda_k: 1 Loss: 0.001749306319256766\n",
      "Iteration: 3978 lambda_k: 1 Loss: 0.0017493062811822948\n",
      "Iteration: 3979 lambda_k: 1 Loss: 0.0017493062434916816\n",
      "Iteration: 3980 lambda_k: 1 Loss: 0.0017493062061811947\n",
      "Iteration: 3981 lambda_k: 1 Loss: 0.0017493061692469545\n",
      "Iteration: 3982 lambda_k: 1 Loss: 0.0017493061326852655\n",
      "Iteration: 3983 lambda_k: 1 Loss: 0.0017493060964924418\n",
      "Iteration: 3984 lambda_k: 1 Loss: 0.0017493060606647698\n",
      "Iteration: 3985 lambda_k: 1 Loss: 0.0017493060251986292\n",
      "Iteration: 3986 lambda_k: 1 Loss: 0.0017493059900904433\n",
      "Iteration: 3987 lambda_k: 1 Loss: 0.001749305955336716\n",
      "Iteration: 3988 lambda_k: 1 Loss: 0.0017493059209338201\n",
      "Iteration: 3989 lambda_k: 1 Loss: 0.00174930588687835\n",
      "Iteration: 3990 lambda_k: 1 Loss: 0.0017493058531668115\n",
      "Iteration: 3991 lambda_k: 1 Loss: 0.0017493058197957435\n",
      "Iteration: 3992 lambda_k: 1 Loss: 0.0017493057867618259\n",
      "Iteration: 3993 lambda_k: 1 Loss: 0.0017493057540616732\n",
      "Iteration: 3994 lambda_k: 1 Loss: 0.0017493057216919593\n",
      "Iteration: 3995 lambda_k: 1 Loss: 0.0017493056896494177\n",
      "Iteration: 3996 lambda_k: 1 Loss: 0.0017493056579307776\n",
      "Iteration: 3997 lambda_k: 1 Loss: 0.001749305626532871\n",
      "Iteration: 3998 lambda_k: 1 Loss: 0.0017493055954524673\n",
      "Iteration: 3999 lambda_k: 1 Loss: 0.0017493055646864316\n",
      "Iteration: 4000 lambda_k: 1 Loss: 0.0017493055342315555\n",
      "Iteration: 4001 lambda_k: 1 Loss: 0.001749305504084819\n",
      "Iteration: 4002 lambda_k: 1 Loss: 0.0017493054742431509\n",
      "Iteration: 4003 lambda_k: 1 Loss: 0.0017493054447034724\n",
      "Iteration: 4004 lambda_k: 1 Loss: 0.0017493054154628191\n",
      "Iteration: 4005 lambda_k: 1 Loss: 0.0017493053865181793\n",
      "Iteration: 4006 lambda_k: 1 Loss: 0.0017493053578666307\n",
      "Iteration: 4007 lambda_k: 1 Loss: 0.0017493053295052348\n",
      "Iteration: 4008 lambda_k: 1 Loss: 0.0017493053014310956\n",
      "Iteration: 4009 lambda_k: 1 Loss: 0.0017493052736413833\n",
      "Iteration: 4010 lambda_k: 1 Loss: 0.0017493052461332417\n",
      "Iteration: 4011 lambda_k: 1 Loss: 0.0017493052189038454\n",
      "Iteration: 4012 lambda_k: 1 Loss: 0.0017493051919504786\n",
      "Iteration: 4013 lambda_k: 1 Loss: 0.00174930516527031\n",
      "Iteration: 4014 lambda_k: 1 Loss: 0.001749305138860686\n",
      "Iteration: 4015 lambda_k: 1 Loss: 0.0017493051127188844\n",
      "Iteration: 4016 lambda_k: 1 Loss: 0.001749305086842262\n",
      "Iteration: 4017 lambda_k: 1 Loss: 0.0017493050612281086\n",
      "Iteration: 4018 lambda_k: 1 Loss: 0.001749305035873881\n",
      "Iteration: 4019 lambda_k: 1 Loss: 0.0017493050107769308\n",
      "Iteration: 4020 lambda_k: 1 Loss: 0.0017493049859347307\n",
      "Iteration: 4021 lambda_k: 1 Loss: 0.0017493049613447368\n",
      "Iteration: 4022 lambda_k: 1 Loss: 0.0017493049370044215\n",
      "Iteration: 4023 lambda_k: 1 Loss: 0.0017493049129112617\n",
      "Iteration: 4024 lambda_k: 1 Loss: 0.0017493048890628569\n",
      "Iteration: 4025 lambda_k: 1 Loss: 0.0017493048654567116\n",
      "Iteration: 4026 lambda_k: 1 Loss: 0.001749304842090436\n",
      "Iteration: 4027 lambda_k: 1 Loss: 0.0017493048189615996\n",
      "Iteration: 4028 lambda_k: 1 Loss: 0.0017493047960678645\n",
      "Iteration: 4029 lambda_k: 1 Loss: 0.0017493047734068819\n",
      "Iteration: 4030 lambda_k: 1 Loss: 0.0017493047509763774\n",
      "Iteration: 4031 lambda_k: 1 Loss: 0.0017493047287739899\n",
      "Iteration: 4032 lambda_k: 1 Loss: 0.0017493047067974328\n",
      "Iteration: 4033 lambda_k: 1 Loss: 0.0017493046850444772\n",
      "Iteration: 4034 lambda_k: 1 Loss: 0.0017493046635128945\n",
      "Iteration: 4035 lambda_k: 1 Loss: 0.001749304642200466\n",
      "Iteration: 4036 lambda_k: 1 Loss: 0.0017493046211050146\n",
      "Iteration: 4037 lambda_k: 1 Loss: 0.0017493046002243023\n",
      "Iteration: 4038 lambda_k: 1 Loss: 0.001749304579556235\n",
      "Iteration: 4039 lambda_k: 1 Loss: 0.0017493045590987035\n",
      "Iteration: 4040 lambda_k: 1 Loss: 0.0017493045388496207\n",
      "Iteration: 4041 lambda_k: 1 Loss: 0.0017493045188068544\n",
      "Iteration: 4042 lambda_k: 1 Loss: 0.001749304498968371\n",
      "Iteration: 4043 lambda_k: 1 Loss: 0.0017493044793320638\n",
      "Iteration: 4044 lambda_k: 1 Loss: 0.001749304459896001\n",
      "Iteration: 4045 lambda_k: 1 Loss: 0.0017493044406581416\n",
      "Iteration: 4046 lambda_k: 1 Loss: 0.0017493044216165064\n",
      "Iteration: 4047 lambda_k: 1 Loss: 0.0017493044027690895\n",
      "Iteration: 4048 lambda_k: 1 Loss: 0.0017493043841139812\n",
      "Iteration: 4049 lambda_k: 1 Loss: 0.001749304365649323\n",
      "Iteration: 4050 lambda_k: 1 Loss: 0.0017493043473731243\n",
      "Iteration: 4051 lambda_k: 1 Loss: 0.0017493043292835019\n",
      "Iteration: 4052 lambda_k: 1 Loss: 0.0017493043113786188\n",
      "Iteration: 4053 lambda_k: 1 Loss: 0.0017493042936566514\n",
      "Iteration: 4054 lambda_k: 1 Loss: 0.0017493042761156942\n",
      "Iteration: 4055 lambda_k: 1 Loss: 0.001749304258753989\n",
      "Iteration: 4056 lambda_k: 1 Loss: 0.0017493042415697435\n",
      "Iteration: 4057 lambda_k: 1 Loss: 0.001749304224561121\n",
      "Iteration: 4058 lambda_k: 1 Loss: 0.0017493042077264\n",
      "Iteration: 4059 lambda_k: 1 Loss: 0.0017493041910638394\n",
      "Iteration: 4060 lambda_k: 1 Loss: 0.0017493041745717652\n",
      "Iteration: 4061 lambda_k: 1 Loss: 0.001749304158248424\n",
      "Iteration: 4062 lambda_k: 1 Loss: 0.001749304142092137\n",
      "Iteration: 4063 lambda_k: 1 Loss: 0.0017493041261012082\n",
      "Iteration: 4064 lambda_k: 1 Loss: 0.001749304110273959\n",
      "Iteration: 4065 lambda_k: 1 Loss: 0.0017493040946087576\n",
      "Iteration: 4066 lambda_k: 1 Loss: 0.0017493040791040572\n",
      "Iteration: 4067 lambda_k: 1 Loss: 0.0017493040637581736\n",
      "Iteration: 4068 lambda_k: 1 Loss: 0.0017493040485695307\n",
      "Iteration: 4069 lambda_k: 1 Loss: 0.0017493040335365764\n",
      "Iteration: 4070 lambda_k: 1 Loss: 0.0017493040186576738\n",
      "Iteration: 4071 lambda_k: 1 Loss: 0.0017493040039313359\n",
      "Iteration: 4072 lambda_k: 1 Loss: 0.0017493039893560333\n",
      "Iteration: 4073 lambda_k: 1 Loss: 0.001749303974930218\n",
      "Iteration: 4074 lambda_k: 1 Loss: 0.0017493039606524595\n",
      "Iteration: 4075 lambda_k: 1 Loss: 0.001749303946521254\n",
      "Iteration: 4076 lambda_k: 1 Loss: 0.0017493039325350806\n",
      "Iteration: 4077 lambda_k: 1 Loss: 0.0017493039186925103\n",
      "Iteration: 4078 lambda_k: 1 Loss: 0.0017493039049920884\n",
      "Iteration: 4079 lambda_k: 1 Loss: 0.001749303891432374\n",
      "Iteration: 4080 lambda_k: 1 Loss: 0.001749303878011973\n",
      "Iteration: 4081 lambda_k: 1 Loss: 0.001749303864729453\n",
      "Iteration: 4082 lambda_k: 1 Loss: 0.0017493038515834737\n",
      "Iteration: 4083 lambda_k: 1 Loss: 0.001749303838572622\n",
      "Iteration: 4084 lambda_k: 1 Loss: 0.0017493038256956284\n",
      "Iteration: 4085 lambda_k: 1 Loss: 0.00174930381295103\n",
      "Iteration: 4086 lambda_k: 1 Loss: 0.0017493038003375773\n",
      "Iteration: 4087 lambda_k: 1 Loss: 0.0017493037878539579\n",
      "Iteration: 4088 lambda_k: 1 Loss: 0.0017493037754988331\n",
      "Iteration: 4089 lambda_k: 1 Loss: 0.0017493037632708686\n",
      "Iteration: 4090 lambda_k: 1 Loss: 0.0017493037511688847\n",
      "Iteration: 4091 lambda_k: 1 Loss: 0.0017493037391915324\n",
      "Iteration: 4092 lambda_k: 1 Loss: 0.0017493037273375635\n",
      "Iteration: 4093 lambda_k: 1 Loss: 0.001749303715605771\n",
      "Iteration: 4094 lambda_k: 1 Loss: 0.0017493037039949505\n",
      "Iteration: 4095 lambda_k: 1 Loss: 0.001749303692503761\n",
      "Iteration: 4096 lambda_k: 1 Loss: 0.001749303681131153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4097 lambda_k: 1 Loss: 0.001749303669875836\n",
      "Iteration: 4098 lambda_k: 1 Loss: 0.001749303658736683\n",
      "Iteration: 4099 lambda_k: 1 Loss: 0.0017493036477124794\n",
      "Iteration: 4100 lambda_k: 1 Loss: 0.0017493036368020718\n",
      "Iteration: 4101 lambda_k: 1 Loss: 0.0017493036260043328\n",
      "Iteration: 4102 lambda_k: 1 Loss: 0.0017493036153180942\n",
      "Iteration: 4103 lambda_k: 1 Loss: 0.0017493036047422953\n",
      "Iteration: 4104 lambda_k: 1 Loss: 0.0017493035942757766\n",
      "Iteration: 4105 lambda_k: 1 Loss: 0.0017493035839173977\n",
      "Iteration: 4106 lambda_k: 1 Loss: 0.0017493035736661285\n",
      "Iteration: 4107 lambda_k: 1 Loss: 0.0017493035635208981\n",
      "Iteration: 4108 lambda_k: 1 Loss: 0.001749303553480602\n",
      "Iteration: 4109 lambda_k: 1 Loss: 0.001749303543544244\n",
      "Iteration: 4110 lambda_k: 1 Loss: 0.0017493035337107208\n",
      "Iteration: 4111 lambda_k: 1 Loss: 0.0017493035239789221\n",
      "Iteration: 4112 lambda_k: 1 Loss: 0.0017493035143479085\n",
      "Iteration: 4113 lambda_k: 1 Loss: 0.0017493035048167112\n",
      "Iteration: 4114 lambda_k: 1 Loss: 0.0017493034953842532\n",
      "Iteration: 4115 lambda_k: 1 Loss: 0.0017493034860495563\n",
      "Iteration: 4116 lambda_k: 1 Loss: 0.0017493034768115887\n",
      "Iteration: 4117 lambda_k: 1 Loss: 0.0017493034676693885\n",
      "Iteration: 4118 lambda_k: 1 Loss: 0.0017493034586220455\n",
      "Iteration: 4119 lambda_k: 1 Loss: 0.001749303449668549\n",
      "Iteration: 4120 lambda_k: 1 Loss: 0.0017493034408079726\n",
      "Iteration: 4121 lambda_k: 1 Loss: 0.0017493034320393672\n",
      "Iteration: 4122 lambda_k: 1 Loss: 0.001749303423361813\n",
      "Iteration: 4123 lambda_k: 1 Loss: 0.0017493034147744214\n",
      "Iteration: 4124 lambda_k: 1 Loss: 0.0017493034062762021\n",
      "Iteration: 4125 lambda_k: 1 Loss: 0.0017493033978663094\n",
      "Iteration: 4126 lambda_k: 1 Loss: 0.001749303389543856\n",
      "Iteration: 4127 lambda_k: 1 Loss: 0.001749303381307929\n",
      "Iteration: 4128 lambda_k: 1 Loss: 0.001749303373157669\n",
      "Iteration: 4129 lambda_k: 1 Loss: 0.0017493033650922338\n",
      "Iteration: 4130 lambda_k: 1 Loss: 0.0017493033571107112\n",
      "Iteration: 4131 lambda_k: 1 Loss: 0.0017493033492122438\n",
      "Iteration: 4132 lambda_k: 1 Loss: 0.0017493033413960705\n",
      "Iteration: 4133 lambda_k: 1 Loss: 0.0017493033336612666\n",
      "Iteration: 4134 lambda_k: 1 Loss: 0.001749303326007045\n",
      "Iteration: 4135 lambda_k: 1 Loss: 0.0017493033184325978\n",
      "Iteration: 4136 lambda_k: 1 Loss: 0.0017493033109370853\n",
      "Iteration: 4137 lambda_k: 1 Loss: 0.001749303303519791\n",
      "Iteration: 4138 lambda_k: 1 Loss: 0.0017493032961798784\n",
      "Iteration: 4139 lambda_k: 1 Loss: 0.0017493032889165417\n",
      "Iteration: 4140 lambda_k: 1 Loss: 0.0017493032817289915\n",
      "Iteration: 4141 lambda_k: 1 Loss: 0.001749303274616471\n",
      "Iteration: 4142 lambda_k: 1 Loss: 0.0017493032675782146\n",
      "Iteration: 4143 lambda_k: 1 Loss: 0.0017493032606135056\n",
      "Iteration: 4144 lambda_k: 1 Loss: 0.0017493032537215984\n",
      "Iteration: 4145 lambda_k: 1 Loss: 0.0017493032469017033\n",
      "Iteration: 4146 lambda_k: 1 Loss: 0.0017493032401531697\n",
      "Iteration: 4147 lambda_k: 1 Loss: 0.0017493032334751685\n",
      "Iteration: 4148 lambda_k: 1 Loss: 0.0017493032268670492\n",
      "Iteration: 4149 lambda_k: 1 Loss: 0.001749303220328082\n",
      "Iteration: 4150 lambda_k: 1 Loss: 0.0017493032138576018\n",
      "Iteration: 4151 lambda_k: 1 Loss: 0.0017493032074548686\n",
      "Iteration: 4152 lambda_k: 1 Loss: 0.001749303201119203\n",
      "Iteration: 4153 lambda_k: 1 Loss: 0.0017493031948499465\n",
      "Iteration: 4154 lambda_k: 1 Loss: 0.0017493031886464223\n",
      "Iteration: 4155 lambda_k: 1 Loss: 0.0017493031825079706\n",
      "Iteration: 4156 lambda_k: 1 Loss: 0.0017493031764339155\n",
      "Iteration: 4157 lambda_k: 1 Loss: 0.0017493031704235538\n",
      "Iteration: 4158 lambda_k: 1 Loss: 0.001749303164476298\n",
      "Iteration: 4159 lambda_k: 1 Loss: 0.0017493031585914742\n",
      "Iteration: 4160 lambda_k: 1 Loss: 0.0017493031527684698\n",
      "Iteration: 4161 lambda_k: 1 Loss: 0.0017493031470066814\n",
      "Iteration: 4162 lambda_k: 1 Loss: 0.001749303141305419\n",
      "Iteration: 4163 lambda_k: 1 Loss: 0.0017493031356640753\n",
      "Iteration: 4164 lambda_k: 1 Loss: 0.0017493031300820942\n",
      "Iteration: 4165 lambda_k: 1 Loss: 0.0017493031245587863\n",
      "Iteration: 4166 lambda_k: 1 Loss: 0.0017493031190936182\n",
      "Iteration: 4167 lambda_k: 1 Loss: 0.001749303113686042\n",
      "Iteration: 4168 lambda_k: 1 Loss: 0.001749303108335367\n",
      "Iteration: 4169 lambda_k: 1 Loss: 0.0017493031030410531\n",
      "Iteration: 4170 lambda_k: 1 Loss: 0.00174930309780261\n",
      "Iteration: 4171 lambda_k: 1 Loss: 0.0017493030926193241\n",
      "Iteration: 4172 lambda_k: 1 Loss: 0.0017493030874907462\n",
      "Iteration: 4173 lambda_k: 1 Loss: 0.0017493030824162455\n",
      "Iteration: 4174 lambda_k: 1 Loss: 0.0017493030773953193\n",
      "Iteration: 4175 lambda_k: 1 Loss: 0.0017493030724274072\n",
      "Iteration: 4176 lambda_k: 1 Loss: 0.0017493030675119008\n",
      "Iteration: 4177 lambda_k: 1 Loss: 0.0017493030626483802\n",
      "Iteration: 4178 lambda_k: 1 Loss: 0.0017493030578362696\n",
      "Iteration: 4179 lambda_k: 1 Loss: 0.0017493030530750292\n",
      "Iteration: 4180 lambda_k: 1 Loss: 0.0017493030483641497\n",
      "Iteration: 4181 lambda_k: 1 Loss: 0.0017493030437030633\n",
      "Iteration: 4182 lambda_k: 1 Loss: 0.0017493030390913478\n",
      "Iteration: 4183 lambda_k: 1 Loss: 0.001749303034528444\n",
      "Iteration: 4184 lambda_k: 1 Loss: 0.0017493030300138475\n",
      "Iteration: 4185 lambda_k: 1 Loss: 0.0017493030255470805\n",
      "Iteration: 4186 lambda_k: 1 Loss: 0.0017493030211276888\n",
      "Iteration: 4187 lambda_k: 1 Loss: 0.0017493030167551953\n",
      "Iteration: 4188 lambda_k: 1 Loss: 0.0017493030124290283\n",
      "Iteration: 4189 lambda_k: 1 Loss: 0.0017493030081487942\n",
      "Iteration: 4190 lambda_k: 1 Loss: 0.0017493030039140267\n",
      "Iteration: 4191 lambda_k: 1 Loss: 0.0017493029997242135\n",
      "Iteration: 4192 lambda_k: 1 Loss: 0.0017493029955789146\n",
      "Iteration: 4193 lambda_k: 1 Loss: 0.0017493029914776882\n",
      "Iteration: 4194 lambda_k: 1 Loss: 0.0017493029874200263\n",
      "Iteration: 4195 lambda_k: 1 Loss: 0.0017493029834054913\n",
      "Iteration: 4196 lambda_k: 1 Loss: 0.001749302979433724\n",
      "Iteration: 4197 lambda_k: 1 Loss: 0.0017493029755042075\n",
      "Iteration: 4198 lambda_k: 1 Loss: 0.0017493029716165922\n",
      "Iteration: 4199 lambda_k: 1 Loss: 0.0017493029677703336\n",
      "Iteration: 4200 lambda_k: 1 Loss: 0.0017493029639650776\n",
      "Iteration: 4201 lambda_k: 1 Loss: 0.0017493029602003824\n",
      "Iteration: 4202 lambda_k: 1 Loss: 0.0017493029564758728\n",
      "Iteration: 4203 lambda_k: 1 Loss: 0.0017493029527910877\n",
      "Iteration: 4204 lambda_k: 1 Loss: 0.0017493029491456118\n",
      "Iteration: 4205 lambda_k: 1 Loss: 0.0017493029455391059\n",
      "Iteration: 4206 lambda_k: 1 Loss: 0.0017493029419710724\n",
      "Iteration: 4207 lambda_k: 1 Loss: 0.0017493029384412005\n",
      "Iteration: 4208 lambda_k: 1 Loss: 0.0017493029349491035\n",
      "Iteration: 4209 lambda_k: 1 Loss: 0.001749302931494347\n",
      "Iteration: 4210 lambda_k: 1 Loss: 0.0017493029280765263\n",
      "Iteration: 4211 lambda_k: 1 Loss: 0.001749302924695345\n",
      "Iteration: 4212 lambda_k: 1 Loss: 0.0017493029213503377\n",
      "Iteration: 4213 lambda_k: 1 Loss: 0.0017493029180411985\n",
      "Iteration: 4214 lambda_k: 1 Loss: 0.001749302914767545\n",
      "Iteration: 4215 lambda_k: 1 Loss: 0.001749302911528996\n",
      "Iteration: 4216 lambda_k: 1 Loss: 0.0017493029083251563\n",
      "Iteration: 4217 lambda_k: 1 Loss: 0.0017493029051557064\n",
      "Iteration: 4218 lambda_k: 1 Loss: 0.001749302902020267\n",
      "Iteration: 4219 lambda_k: 1 Loss: 0.001749302898918518\n",
      "Iteration: 4220 lambda_k: 1 Loss: 0.0017493028958501277\n",
      "Iteration: 4221 lambda_k: 1 Loss: 0.0017493028928147248\n",
      "Iteration: 4222 lambda_k: 1 Loss: 0.001749302889811992\n",
      "Iteration: 4223 lambda_k: 1 Loss: 0.0017493028868415825\n",
      "Iteration: 4224 lambda_k: 1 Loss: 0.001749302883903125\n",
      "Iteration: 4225 lambda_k: 1 Loss: 0.001749302880996352\n",
      "Iteration: 4226 lambda_k: 1 Loss: 0.0017493028781208819\n",
      "Iteration: 4227 lambda_k: 1 Loss: 0.001749302875276378\n",
      "Iteration: 4228 lambda_k: 1 Loss: 0.0017493028724625817\n",
      "Iteration: 4229 lambda_k: 1 Loss: 0.0017493028696791944\n",
      "Iteration: 4230 lambda_k: 1 Loss: 0.001749302866925795\n",
      "Iteration: 4231 lambda_k: 1 Loss: 0.001749302864202139\n",
      "Iteration: 4232 lambda_k: 1 Loss: 0.0017493028615079456\n",
      "Iteration: 4233 lambda_k: 1 Loss: 0.001749302858842907\n",
      "Iteration: 4234 lambda_k: 1 Loss: 0.0017493028562066916\n",
      "Iteration: 4235 lambda_k: 1 Loss: 0.0017493028535989787\n",
      "Iteration: 4236 lambda_k: 1 Loss: 0.001749302851019477\n",
      "Iteration: 4237 lambda_k: 1 Loss: 0.0017493028484679262\n",
      "Iteration: 4238 lambda_k: 1 Loss: 0.0017493028459440155\n",
      "Iteration: 4239 lambda_k: 1 Loss: 0.0017493028434474687\n",
      "Iteration: 4240 lambda_k: 1 Loss: 0.001749302840978041\n",
      "Iteration: 4241 lambda_k: 1 Loss: 0.0017493028385353747\n",
      "Iteration: 4242 lambda_k: 1 Loss: 0.001749302836119287\n",
      "Iteration: 4243 lambda_k: 1 Loss: 0.0017493028337294482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4244 lambda_k: 1 Loss: 0.0017493028313655663\n",
      "Iteration: 4245 lambda_k: 1 Loss: 0.0017493028290274113\n",
      "Iteration: 4246 lambda_k: 1 Loss: 0.0017493028267146907\n",
      "Iteration: 4247 lambda_k: 1 Loss: 0.001749302824427142\n",
      "Iteration: 4248 lambda_k: 1 Loss: 0.0017493028221644822\n",
      "Iteration: 4249 lambda_k: 1 Loss: 0.001749302819926533\n",
      "Iteration: 4250 lambda_k: 1 Loss: 0.0017493028177129305\n",
      "Iteration: 4251 lambda_k: 1 Loss: 0.0017493028155235082\n",
      "Iteration: 4252 lambda_k: 1 Loss: 0.0017493028133579536\n",
      "Iteration: 4253 lambda_k: 1 Loss: 0.0017493028112160866\n",
      "Iteration: 4254 lambda_k: 1 Loss: 0.0017493028090975807\n",
      "Iteration: 4255 lambda_k: 1 Loss: 0.0017493028070022661\n",
      "Iteration: 4256 lambda_k: 1 Loss: 0.0017493028049298995\n",
      "Iteration: 4257 lambda_k: 1 Loss: 0.001749302802880194\n",
      "Iteration: 4258 lambda_k: 1 Loss: 0.0017493028008529485\n",
      "Iteration: 4259 lambda_k: 1 Loss: 0.001749302798847886\n",
      "Iteration: 4260 lambda_k: 1 Loss: 0.0017493027968647878\n",
      "Iteration: 4261 lambda_k: 1 Loss: 0.0017493027949034559\n",
      "Iteration: 4262 lambda_k: 1 Loss: 0.0017493027929636302\n",
      "Iteration: 4263 lambda_k: 1 Loss: 0.0017493027910451464\n",
      "Iteration: 4264 lambda_k: 1 Loss: 0.00174930278914774\n",
      "Iteration: 4265 lambda_k: 1 Loss: 0.0017493027872711325\n",
      "Iteration: 4266 lambda_k: 1 Loss: 0.0017493027854151718\n",
      "Iteration: 4267 lambda_k: 1 Loss: 0.0017493027835796527\n",
      "Iteration: 4268 lambda_k: 1 Loss: 0.0017493027817643684\n",
      "Iteration: 4269 lambda_k: 1 Loss: 0.0017493027799690474\n",
      "Iteration: 4270 lambda_k: 1 Loss: 0.001749302778193538\n",
      "Iteration: 4271 lambda_k: 1 Loss: 0.0017493027764376242\n",
      "Iteration: 4272 lambda_k: 1 Loss: 0.001749302774701091\n",
      "Iteration: 4273 lambda_k: 1 Loss: 0.0017493027729837362\n",
      "Iteration: 4274 lambda_k: 1 Loss: 0.0017493027712853291\n",
      "Iteration: 4275 lambda_k: 1 Loss: 0.0017493027696056727\n",
      "Iteration: 4276 lambda_k: 1 Loss: 0.0017493027679446097\n",
      "Iteration: 4277 lambda_k: 1 Loss: 0.0017493027663019302\n",
      "Iteration: 4278 lambda_k: 1 Loss: 0.001749302764677493\n",
      "Iteration: 4279 lambda_k: 1 Loss: 0.0017493027630710023\n",
      "Iteration: 4280 lambda_k: 1 Loss: 0.0017493027614823933\n",
      "Iteration: 4281 lambda_k: 1 Loss: 0.0017493027599113735\n",
      "Iteration: 4282 lambda_k: 1 Loss: 0.0017493027583577892\n",
      "Iteration: 4283 lambda_k: 1 Loss: 0.0017493027568215094\n",
      "Iteration: 4284 lambda_k: 1 Loss: 0.001749302755302297\n",
      "Iteration: 4285 lambda_k: 1 Loss: 0.001749302753799941\n",
      "Iteration: 4286 lambda_k: 1 Loss: 0.0017493027523143448\n",
      "Iteration: 4287 lambda_k: 1 Loss: 0.001749302750845306\n",
      "Iteration: 4288 lambda_k: 1 Loss: 0.0017493027493926078\n",
      "Iteration: 4289 lambda_k: 1 Loss: 0.0017493027479561563\n",
      "Iteration: 4290 lambda_k: 1 Loss: 0.0017493027465356724\n",
      "Iteration: 4291 lambda_k: 1 Loss: 0.001749302745131068\n",
      "Iteration: 4292 lambda_k: 1 Loss: 0.0017493027437421665\n",
      "Iteration: 4293 lambda_k: 1 Loss: 0.0017493027423687892\n",
      "Iteration: 4294 lambda_k: 1 Loss: 0.0017493027410107505\n",
      "Iteration: 4295 lambda_k: 1 Loss: 0.0017493027396679765\n",
      "Iteration: 4296 lambda_k: 1 Loss: 0.0017493027383402543\n",
      "Iteration: 4297 lambda_k: 1 Loss: 0.0017493027370274318\n",
      "Iteration: 4298 lambda_k: 1 Loss: 0.0017493027357292574\n",
      "Iteration: 4299 lambda_k: 1 Loss: 0.0017493027344457298\n",
      "Iteration: 4300 lambda_k: 1 Loss: 0.0017493027331766135\n",
      "Iteration: 4301 lambda_k: 1 Loss: 0.0017493027319217837\n",
      "Iteration: 4302 lambda_k: 1 Loss: 0.0017493027306810579\n",
      "Iteration: 4303 lambda_k: 1 Loss: 0.0017493027294542892\n",
      "Iteration: 4304 lambda_k: 1 Loss: 0.0017493027282412907\n",
      "Iteration: 4305 lambda_k: 1 Loss: 0.001749302727041996\n",
      "Iteration: 4306 lambda_k: 1 Loss: 0.00174930272585624\n",
      "Iteration: 4307 lambda_k: 1 Loss: 0.0017493027246838838\n",
      "Iteration: 4308 lambda_k: 1 Loss: 0.0017493027235247626\n",
      "Iteration: 4309 lambda_k: 1 Loss: 0.0017493027223787843\n",
      "Iteration: 4310 lambda_k: 1 Loss: 0.0017493027212457039\n",
      "Iteration: 4311 lambda_k: 1 Loss: 0.0017493027201255157\n",
      "Iteration: 4312 lambda_k: 1 Loss: 0.0017493027190179943\n",
      "Iteration: 4313 lambda_k: 1 Loss: 0.001749302717923038\n",
      "Iteration: 4314 lambda_k: 1 Loss: 0.0017493027168404807\n",
      "Iteration: 4315 lambda_k: 1 Loss: 0.0017493027157702416\n",
      "Iteration: 4316 lambda_k: 1 Loss: 0.0017493027147121407\n",
      "Iteration: 4317 lambda_k: 1 Loss: 0.0017493027136661077\n",
      "Iteration: 4318 lambda_k: 1 Loss: 0.0017493027126319716\n",
      "Iteration: 4319 lambda_k: 1 Loss: 0.0017493027116096411\n",
      "Iteration: 4320 lambda_k: 1 Loss: 0.0017493027105989585\n",
      "Iteration: 4321 lambda_k: 1 Loss: 0.0017493027095997356\n",
      "Iteration: 4322 lambda_k: 1 Loss: 0.0017493027086119709\n",
      "Iteration: 4323 lambda_k: 1 Loss: 0.0017493027076354935\n",
      "Iteration: 4324 lambda_k: 1 Loss: 0.001749302706670209\n",
      "Iteration: 4325 lambda_k: 1 Loss: 0.001749302705715942\n",
      "Iteration: 4326 lambda_k: 1 Loss: 0.001749302704772597\n",
      "Iteration: 4327 lambda_k: 1 Loss: 0.0017493027038400935\n",
      "Iteration: 4328 lambda_k: 1 Loss: 0.0017493027029182593\n",
      "Iteration: 4329 lambda_k: 1 Loss: 0.0017493027020069943\n",
      "Iteration: 4330 lambda_k: 1 Loss: 0.0017493027011061956\n",
      "Iteration: 4331 lambda_k: 1 Loss: 0.0017493027002157707\n",
      "Iteration: 4332 lambda_k: 1 Loss: 0.001749302699335565\n",
      "Iteration: 4333 lambda_k: 1 Loss: 0.0017493026984655264\n",
      "Iteration: 4334 lambda_k: 1 Loss: 0.0017493026976055088\n",
      "Iteration: 4335 lambda_k: 1 Loss: 0.0017493026967554152\n",
      "Iteration: 4336 lambda_k: 1 Loss: 0.001749302695915198\n",
      "Iteration: 4337 lambda_k: 1 Loss: 0.0017493026950847045\n",
      "Iteration: 4338 lambda_k: 1 Loss: 0.0017493026942637568\n",
      "Iteration: 4339 lambda_k: 1 Loss: 0.0017493026934523107\n",
      "Iteration: 4340 lambda_k: 1 Loss: 0.0017493026926502836\n",
      "Iteration: 4341 lambda_k: 1 Loss: 0.0017493026918575664\n",
      "Iteration: 4342 lambda_k: 1 Loss: 0.001749302691074079\n",
      "Iteration: 4343 lambda_k: 1 Loss: 0.0017493026902997014\n",
      "Iteration: 4344 lambda_k: 1 Loss: 0.0017493026895342772\n",
      "Iteration: 4345 lambda_k: 1 Loss: 0.0017493026887778205\n",
      "Iteration: 4346 lambda_k: 1 Loss: 0.0017493026880301635\n",
      "Iteration: 4347 lambda_k: 1 Loss: 0.0017493026872912145\n",
      "Iteration: 4348 lambda_k: 1 Loss: 0.0017493026865608792\n",
      "Iteration: 4349 lambda_k: 1 Loss: 0.001749302685839102\n",
      "Iteration: 4350 lambda_k: 1 Loss: 0.0017493026851257254\n",
      "Iteration: 4351 lambda_k: 1 Loss: 0.001749302684420713\n",
      "Iteration: 4352 lambda_k: 1 Loss: 0.0017493026837239571\n",
      "Iteration: 4353 lambda_k: 1 Loss: 0.0017493026830354144\n",
      "Iteration: 4354 lambda_k: 1 Loss: 0.0017493026823549274\n",
      "Iteration: 4355 lambda_k: 1 Loss: 0.0017493026816824675\n",
      "Iteration: 4356 lambda_k: 1 Loss: 0.0017493026810178522\n",
      "Iteration: 4357 lambda_k: 1 Loss: 0.0017493026803610987\n",
      "Iteration: 4358 lambda_k: 1 Loss: 0.0017493026797120608\n",
      "Iteration: 4359 lambda_k: 1 Loss: 0.00174930267907068\n",
      "Iteration: 4360 lambda_k: 1 Loss: 0.0017493026784369291\n",
      "Iteration: 4361 lambda_k: 1 Loss: 0.0017493026778106437\n",
      "Iteration: 4362 lambda_k: 1 Loss: 0.0017493026771917834\n",
      "Iteration: 4363 lambda_k: 1 Loss: 0.0017493026765802234\n",
      "Iteration: 4364 lambda_k: 1 Loss: 0.0017493026759759405\n",
      "Iteration: 4365 lambda_k: 1 Loss: 0.0017493026753788092\n",
      "Iteration: 4366 lambda_k: 1 Loss: 0.0017493026747888037\n",
      "Iteration: 4367 lambda_k: 1 Loss: 0.00174930267420583\n",
      "Iteration: 4368 lambda_k: 1 Loss: 0.0017493026736297907\n",
      "Iteration: 4369 lambda_k: 1 Loss: 0.0017493026730606147\n",
      "Iteration: 4370 lambda_k: 1 Loss: 0.0017493026724981979\n",
      "Iteration: 4371 lambda_k: 1 Loss: 0.0017493026719425067\n",
      "Iteration: 4372 lambda_k: 1 Loss: 0.0017493026713934934\n",
      "Iteration: 4373 lambda_k: 1 Loss: 0.0017493026708510098\n",
      "Iteration: 4374 lambda_k: 1 Loss: 0.0017493026703150798\n",
      "Iteration: 4375 lambda_k: 1 Loss: 0.0017493026697855594\n",
      "Iteration: 4376 lambda_k: 1 Loss: 0.001749302669262368\n",
      "Iteration: 4377 lambda_k: 1 Loss: 0.0017493026687455118\n",
      "Iteration: 4378 lambda_k: 1 Loss: 0.0017493026682348406\n",
      "Iteration: 4379 lambda_k: 1 Loss: 0.001749302667730367\n",
      "Iteration: 4380 lambda_k: 1 Loss: 0.0017493026672319872\n",
      "Iteration: 4381 lambda_k: 1 Loss: 0.0017493026667396046\n",
      "Iteration: 4382 lambda_k: 1 Loss: 0.001749302666253144\n",
      "Iteration: 4383 lambda_k: 1 Loss: 0.0017493026657725706\n",
      "Iteration: 4384 lambda_k: 1 Loss: 0.0017493026652978536\n",
      "Iteration: 4385 lambda_k: 1 Loss: 0.001749302664828888\n",
      "Iteration: 4386 lambda_k: 1 Loss: 0.0017493026643656145\n",
      "Iteration: 4387 lambda_k: 1 Loss: 0.0017493026639080062\n",
      "Iteration: 4388 lambda_k: 1 Loss: 0.001749302663455934\n",
      "Iteration: 4389 lambda_k: 1 Loss: 0.0017493026630093678\n",
      "Iteration: 4390 lambda_k: 1 Loss: 0.001749302662568257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4391 lambda_k: 1 Loss: 0.0017493026621325668\n",
      "Iteration: 4392 lambda_k: 1 Loss: 0.001749302661702182\n",
      "Iteration: 4393 lambda_k: 1 Loss: 0.0017493026612771425\n",
      "Iteration: 4394 lambda_k: 1 Loss: 0.0017493026608573134\n",
      "Iteration: 4395 lambda_k: 1 Loss: 0.0017493026604425872\n",
      "Iteration: 4396 lambda_k: 1 Loss: 0.0017493026600329962\n",
      "Iteration: 4397 lambda_k: 1 Loss: 0.0017493026596284106\n",
      "Iteration: 4398 lambda_k: 1 Loss: 0.0017493026592288634\n",
      "Iteration: 4399 lambda_k: 1 Loss: 0.0017493026588342436\n",
      "Iteration: 4400 lambda_k: 1 Loss: 0.0017493026584445074\n",
      "Iteration: 4401 lambda_k: 1 Loss: 0.0017493026580596175\n",
      "Iteration: 4402 lambda_k: 1 Loss: 0.0017493026576795119\n",
      "Iteration: 4403 lambda_k: 1 Loss: 0.0017493026573040837\n",
      "Iteration: 4404 lambda_k: 1 Loss: 0.0017493026569333572\n",
      "Iteration: 4405 lambda_k: 1 Loss: 0.0017493026565672028\n",
      "Iteration: 4406 lambda_k: 1 Loss: 0.0017493026562056726\n",
      "Iteration: 4407 lambda_k: 1 Loss: 0.0017493026558486149\n",
      "Iteration: 4408 lambda_k: 1 Loss: 0.0017493026554960109\n",
      "Iteration: 4409 lambda_k: 1 Loss: 0.0017493026551478764\n",
      "Iteration: 4410 lambda_k: 1 Loss: 0.0017493026548041032\n",
      "Iteration: 4411 lambda_k: 1 Loss: 0.0017493026544646247\n",
      "Iteration: 4412 lambda_k: 1 Loss: 0.0017493026541294068\n",
      "Iteration: 4413 lambda_k: 1 Loss: 0.0017493026537984005\n",
      "Iteration: 4414 lambda_k: 1 Loss: 0.001749302653471562\n",
      "Iteration: 4415 lambda_k: 1 Loss: 0.0017493026531489003\n",
      "Iteration: 4416 lambda_k: 1 Loss: 0.0017493026528303019\n",
      "Iteration: 4417 lambda_k: 1 Loss: 0.001749302652515723\n",
      "Iteration: 4418 lambda_k: 1 Loss: 0.0017493026522051416\n",
      "Iteration: 4419 lambda_k: 1 Loss: 0.001749302651898528\n",
      "Iteration: 4420 lambda_k: 1 Loss: 0.0017493026515957974\n",
      "Iteration: 4421 lambda_k: 1 Loss: 0.0017493026512969642\n",
      "Iteration: 4422 lambda_k: 1 Loss: 0.001749302651001907\n",
      "Iteration: 4423 lambda_k: 1 Loss: 0.0017493026507106584\n",
      "Iteration: 4424 lambda_k: 1 Loss: 0.0017493026504231542\n",
      "Iteration: 4425 lambda_k: 1 Loss: 0.0017493026501392895\n",
      "Iteration: 4426 lambda_k: 1 Loss: 0.00174930264985906\n",
      "Iteration: 4427 lambda_k: 1 Loss: 0.0017493026495824337\n",
      "Iteration: 4428 lambda_k: 1 Loss: 0.001749302649309423\n",
      "Iteration: 4429 lambda_k: 1 Loss: 0.0017493026490399168\n",
      "Iteration: 4430 lambda_k: 1 Loss: 0.0017493026487738765\n",
      "Iteration: 4431 lambda_k: 1 Loss: 0.0017493026485113047\n",
      "Iteration: 4432 lambda_k: 1 Loss: 0.0017493026482521578\n",
      "Iteration: 4433 lambda_k: 1 Loss: 0.0017493026479963826\n",
      "Iteration: 4434 lambda_k: 1 Loss: 0.0017493026477439259\n",
      "Iteration: 4435 lambda_k: 1 Loss: 0.0017493026474947463\n",
      "Iteration: 4436 lambda_k: 1 Loss: 0.0017493026472488514\n",
      "Iteration: 4437 lambda_k: 1 Loss: 0.001749302647006194\n",
      "Iteration: 4438 lambda_k: 1 Loss: 0.0017493026467667232\n",
      "Iteration: 4439 lambda_k: 1 Loss: 0.0017493026465303964\n",
      "Iteration: 4440 lambda_k: 1 Loss: 0.0017493026462971624\n",
      "Iteration: 4441 lambda_k: 1 Loss: 0.0017493026460670199\n",
      "Iteration: 4442 lambda_k: 1 Loss: 0.0017493026458399229\n",
      "Iteration: 4443 lambda_k: 1 Loss: 0.001749302645615832\n",
      "Iteration: 4444 lambda_k: 1 Loss: 0.0017493026453946935\n",
      "Iteration: 4445 lambda_k: 1 Loss: 0.0017493026451765167\n",
      "Iteration: 4446 lambda_k: 1 Loss: 0.001749302644961247\n",
      "Iteration: 4447 lambda_k: 1 Loss: 0.0017493026447488401\n",
      "Iteration: 4448 lambda_k: 1 Loss: 0.00174930264453933\n",
      "Iteration: 4449 lambda_k: 1 Loss: 0.001749302644332577\n",
      "Iteration: 4450 lambda_k: 1 Loss: 0.0017493026441285891\n",
      "Iteration: 4451 lambda_k: 1 Loss: 0.0017493026439273757\n",
      "Iteration: 4452 lambda_k: 1 Loss: 0.0017493026437288492\n",
      "Iteration: 4453 lambda_k: 1 Loss: 0.0017493026435330087\n",
      "Iteration: 4454 lambda_k: 1 Loss: 0.0017493026433398492\n",
      "Iteration: 4455 lambda_k: 1 Loss: 0.0017493026431492837\n",
      "Iteration: 4456 lambda_k: 1 Loss: 0.0017493026429613073\n",
      "Iteration: 4457 lambda_k: 1 Loss: 0.001749302642775857\n",
      "Iteration: 4458 lambda_k: 1 Loss: 0.0017493026425929707\n",
      "Iteration: 4459 lambda_k: 1 Loss: 0.001749302642412589\n",
      "Iteration: 4460 lambda_k: 1 Loss: 0.001749302642234713\n",
      "Iteration: 4461 lambda_k: 1 Loss: 0.0017493026420592535\n",
      "Iteration: 4462 lambda_k: 1 Loss: 0.0017493026418862049\n",
      "Iteration: 4463 lambda_k: 1 Loss: 0.0017493026417155597\n",
      "Iteration: 4464 lambda_k: 1 Loss: 0.0017493026415472885\n",
      "Iteration: 4465 lambda_k: 1 Loss: 0.0017493026413813394\n",
      "Iteration: 4466 lambda_k: 1 Loss: 0.0017493026412176657\n",
      "Iteration: 4467 lambda_k: 1 Loss: 0.0017493026410563342\n",
      "Iteration: 4468 lambda_k: 1 Loss: 0.0017493026408972171\n",
      "Iteration: 4469 lambda_k: 1 Loss: 0.0017493026407403318\n",
      "Iteration: 4470 lambda_k: 1 Loss: 0.0017493026405856415\n",
      "Iteration: 4471 lambda_k: 1 Loss: 0.0017493026404331433\n",
      "Iteration: 4472 lambda_k: 1 Loss: 0.0017493026402827883\n",
      "Iteration: 4473 lambda_k: 1 Loss: 0.0017493026401345796\n",
      "Iteration: 4474 lambda_k: 1 Loss: 0.001749302639988475\n",
      "Iteration: 4475 lambda_k: 1 Loss: 0.00174930263984442\n",
      "Iteration: 4476 lambda_k: 1 Loss: 0.0017493026397024192\n",
      "Iteration: 4477 lambda_k: 1 Loss: 0.0017493026395624198\n",
      "Iteration: 4478 lambda_k: 1 Loss: 0.0017493026394244204\n",
      "Iteration: 4479 lambda_k: 1 Loss: 0.0017493026392884504\n",
      "Iteration: 4480 lambda_k: 1 Loss: 0.0017493026391544036\n",
      "Iteration: 4481 lambda_k: 1 Loss: 0.0017493026390223354\n",
      "Iteration: 4482 lambda_k: 1 Loss: 0.0017493026388921366\n",
      "Iteration: 4483 lambda_k: 1 Loss: 0.0017493026387638741\n",
      "Iteration: 4484 lambda_k: 1 Loss: 0.0017493026386374302\n",
      "Iteration: 4485 lambda_k: 1 Loss: 0.0017493026385128915\n",
      "Iteration: 4486 lambda_k: 1 Loss: 0.0017493026383901204\n",
      "Iteration: 4487 lambda_k: 1 Loss: 0.0017493026382691393\n",
      "Iteration: 4488 lambda_k: 1 Loss: 0.0017493026381499523\n",
      "Iteration: 4489 lambda_k: 1 Loss: 0.0017493026380325139\n",
      "Iteration: 4490 lambda_k: 1 Loss: 0.0017493026379168247\n",
      "Iteration: 4491 lambda_k: 1 Loss: 0.0017493026378029087\n",
      "Iteration: 4492 lambda_k: 1 Loss: 0.001749302637690636\n",
      "Iteration: 4493 lambda_k: 1 Loss: 0.0017493026375800798\n",
      "Iteration: 4494 lambda_k: 1 Loss: 0.0017493026374712024\n",
      "Iteration: 4495 lambda_k: 1 Loss: 0.0017493026373639271\n",
      "Iteration: 4496 lambda_k: 1 Loss: 0.0017493026372582881\n",
      "Iteration: 4497 lambda_k: 1 Loss: 0.001749302637154195\n",
      "Iteration: 4498 lambda_k: 1 Loss: 0.0017493026370517329\n",
      "Iteration: 4499 lambda_k: 1 Loss: 0.0017493026369508038\n",
      "Iteration: 4500 lambda_k: 1 Loss: 0.001749302636851463\n",
      "Iteration: 4501 lambda_k: 1 Loss: 0.0017493026367535957\n",
      "Iteration: 4502 lambda_k: 1 Loss: 0.0017493026366572797\n",
      "Iteration: 4503 lambda_k: 1 Loss: 0.0017493026365623906\n",
      "Iteration: 4504 lambda_k: 1 Loss: 0.0017493026364690082\n",
      "Iteration: 4505 lambda_k: 1 Loss: 0.0017493026363770692\n",
      "Iteration: 4506 lambda_k: 1 Loss: 0.001749302636286583\n",
      "Iteration: 4507 lambda_k: 1 Loss: 0.0017493026361974934\n",
      "Iteration: 4508 lambda_k: 1 Loss: 0.0017493026361098175\n",
      "Iteration: 4509 lambda_k: 1 Loss: 0.0017493026360235545\n",
      "Iteration: 4510 lambda_k: 1 Loss: 0.00174930263593858\n",
      "Iteration: 4511 lambda_k: 1 Loss: 0.0017493026358549933\n",
      "Iteration: 4512 lambda_k: 1 Loss: 0.0017493026357727761\n",
      "Iteration: 4513 lambda_k: 1 Loss: 0.0017493026356918842\n",
      "Iteration: 4514 lambda_k: 1 Loss: 0.0017493026356122758\n",
      "Iteration: 4515 lambda_k: 1 Loss: 0.0017493026355339448\n",
      "Iteration: 4516 lambda_k: 1 Loss: 0.0017493026354568797\n",
      "Iteration: 4517 lambda_k: 1 Loss: 0.0017493026353810343\n",
      "Iteration: 4518 lambda_k: 1 Loss: 0.001749302635306424\n",
      "Iteration: 4519 lambda_k: 1 Loss: 0.0017493026352330653\n",
      "Iteration: 4520 lambda_k: 1 Loss: 0.0017493026351608953\n",
      "Iteration: 4521 lambda_k: 1 Loss: 0.0017493026350899191\n",
      "Iteration: 4522 lambda_k: 1 Loss: 0.001749302635020151\n",
      "Iteration: 4523 lambda_k: 1 Loss: 0.0017493026349515316\n",
      "Iteration: 4524 lambda_k: 1 Loss: 0.0017493026348840597\n",
      "Iteration: 4525 lambda_k: 1 Loss: 0.0017493026348177738\n",
      "Iteration: 4526 lambda_k: 1 Loss: 0.0017493026347525926\n",
      "Iteration: 4527 lambda_k: 1 Loss: 0.0017493026346885215\n",
      "Iteration: 4528 lambda_k: 1 Loss: 0.001749302634625508\n",
      "Iteration: 4529 lambda_k: 1 Loss: 0.0017493026345636181\n",
      "Iteration: 4530 lambda_k: 1 Loss: 0.0017493026345027452\n",
      "Iteration: 4531 lambda_k: 1 Loss: 0.0017493026344429644\n",
      "Iteration: 4532 lambda_k: 1 Loss: 0.0017493026343841833\n",
      "Iteration: 4533 lambda_k: 1 Loss: 0.0017493026343264307\n",
      "Iteration: 4534 lambda_k: 1 Loss: 0.0017493026342697382\n",
      "Iteration: 4535 lambda_k: 1 Loss: 0.0017493026342140154\n",
      "Iteration: 4536 lambda_k: 1 Loss: 0.0017493026341592977\n",
      "Iteration: 4537 lambda_k: 1 Loss: 0.0017493026341055748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4538 lambda_k: 1 Loss: 0.001749302634052787\n",
      "Iteration: 4539 lambda_k: 1 Loss: 0.0017493026340010128\n",
      "Iteration: 4540 lambda_k: 1 Loss: 0.001749302633950133\n",
      "Iteration: 4541 lambda_k: 1 Loss: 0.0017493026339002284\n",
      "Iteration: 4542 lambda_k: 1 Loss: 0.0017493026338512266\n",
      "Iteration: 4543 lambda_k: 1 Loss: 0.001749302633803118\n",
      "Iteration: 4544 lambda_k: 1 Loss: 0.00174930263375592\n",
      "Iteration: 4545 lambda_k: 1 Loss: 0.0017493026337095986\n",
      "Iteration: 4546 lambda_k: 1 Loss: 0.00174930263366414\n",
      "Iteration: 4547 lambda_k: 1 Loss: 0.0017493026336195588\n",
      "Iteration: 4548 lambda_k: 1 Loss: 0.0017493026335757855\n",
      "Iteration: 4549 lambda_k: 1 Loss: 0.0017493026335329161\n",
      "Iteration: 4550 lambda_k: 1 Loss: 0.0017493026334908523\n",
      "Iteration: 4551 lambda_k: 1 Loss: 0.0017493026334495748\n",
      "Iteration: 4552 lambda_k: 1 Loss: 0.0017493026334091534\n",
      "Iteration: 4553 lambda_k: 1 Loss: 0.0017493026333695108\n",
      "Iteration: 4554 lambda_k: 1 Loss: 0.0017493026333306463\n",
      "Iteration: 4555 lambda_k: 1 Loss: 0.0017493026332925739\n",
      "Iteration: 4556 lambda_k: 1 Loss: 0.0017493026332552582\n",
      "Iteration: 4557 lambda_k: 1 Loss: 0.0017493026332186877\n",
      "Iteration: 4558 lambda_k: 1 Loss: 0.0017493026331828684\n",
      "Iteration: 4559 lambda_k: 1 Loss: 0.0017493026331478066\n",
      "Iteration: 4560 lambda_k: 1 Loss: 0.0017493026331134374\n",
      "Iteration: 4561 lambda_k: 1 Loss: 0.0017493026330797602\n",
      "Iteration: 4562 lambda_k: 1 Loss: 0.0017493026330468078\n",
      "Iteration: 4563 lambda_k: 1 Loss: 0.001749302633014579\n",
      "Iteration: 4564 lambda_k: 1 Loss: 0.0017493026329829958\n",
      "Iteration: 4565 lambda_k: 1 Loss: 0.001749302632952166\n",
      "Iteration: 4566 lambda_k: 1 Loss: 0.0017493026329220183\n",
      "Iteration: 4567 lambda_k: 1 Loss: 0.0017493026328924592\n",
      "Iteration: 4568 lambda_k: 1 Loss: 0.001749302632863592\n",
      "Iteration: 4569 lambda_k: 1 Loss: 0.0017493026328354209\n",
      "Iteration: 4570 lambda_k: 1 Loss: 0.0017493026328078158\n",
      "Iteration: 4571 lambda_k: 1 Loss: 0.0017493026327808367\n",
      "Iteration: 4572 lambda_k: 1 Loss: 0.0017493026327545151\n",
      "Iteration: 4573 lambda_k: 1 Loss: 0.00174930263272876\n",
      "Iteration: 4574 lambda_k: 1 Loss: 0.0017493026327036678\n",
      "Iteration: 4575 lambda_k: 1 Loss: 0.001749302632679185\n",
      "Iteration: 4576 lambda_k: 1 Loss: 0.0017493026326552842\n",
      "Iteration: 4577 lambda_k: 1 Loss: 0.0017493026326319333\n",
      "Iteration: 4578 lambda_k: 1 Loss: 0.0017493026326091954\n",
      "Iteration: 4579 lambda_k: 1 Loss: 0.0017493026325870217\n",
      "Iteration: 4580 lambda_k: 1 Loss: 0.001749302632565407\n",
      "Iteration: 4581 lambda_k: 1 Loss: 0.0017493026325443447\n",
      "Iteration: 4582 lambda_k: 1 Loss: 0.001749302632523783\n",
      "Iteration: 4583 lambda_k: 1 Loss: 0.0017493026325037255\n",
      "Iteration: 4584 lambda_k: 1 Loss: 0.0017493026324842623\n",
      "Iteration: 4585 lambda_k: 1 Loss: 0.0017493026324653222\n",
      "Iteration: 4586 lambda_k: 1 Loss: 0.0017493026324468335\n",
      "Iteration: 4587 lambda_k: 1 Loss: 0.0017493026324288746\n",
      "Iteration: 4588 lambda_k: 1 Loss: 0.0017493026324114681\n",
      "Iteration: 4589 lambda_k: 1 Loss: 0.0017493026323945188\n",
      "Iteration: 4590 lambda_k: 1 Loss: 0.0017493026323780584\n",
      "Iteration: 4591 lambda_k: 1 Loss: 0.0017493026323620973\n",
      "Iteration: 4592 lambda_k: 1 Loss: 0.0017493026323466135\n",
      "Iteration: 4593 lambda_k: 1 Loss: 0.0017493026323315737\n",
      "Iteration: 4594 lambda_k: 1 Loss: 0.0017493026323169656\n",
      "Iteration: 4595 lambda_k: 1 Loss: 0.0017493026323027842\n",
      "Iteration: 4596 lambda_k: 1 Loss: 0.0017493026322891036\n",
      "Iteration: 4597 lambda_k: 1 Loss: 0.0017493026322758208\n",
      "Iteration: 4598 lambda_k: 1 Loss: 0.0017493026322630125\n",
      "Iteration: 4599 lambda_k: 1 Loss: 0.00174930263225066\n",
      "Iteration: 4600 lambda_k: 1 Loss: 0.0017493026322387142\n",
      "Iteration: 4601 lambda_k: 1 Loss: 0.0017493026322271722\n",
      "Iteration: 4602 lambda_k: 1 Loss: 0.0017493026322160325\n",
      "Iteration: 4603 lambda_k: 1 Loss: 0.0017493026322053147\n",
      "Iteration: 4604 lambda_k: 1 Loss: 0.0017493026321950048\n",
      "Iteration: 4605 lambda_k: 1 Loss: 0.0017493026321850737\n",
      "Iteration: 4606 lambda_k: 1 Loss: 0.0017493026321754974\n",
      "Iteration: 4607 lambda_k: 1 Loss: 0.0017493026321663547\n",
      "Iteration: 4608 lambda_k: 1 Loss: 0.0017493026321575979\n",
      "Iteration: 4609 lambda_k: 1 Loss: 0.0017493026321491422\n",
      "Iteration: 4610 lambda_k: 1 Loss: 0.0017493026321410597\n",
      "Iteration: 4611 lambda_k: 1 Loss: 0.0017493026321333744\n",
      "Iteration: 4612 lambda_k: 1 Loss: 0.0017493026321260046\n",
      "Iteration: 4613 lambda_k: 1 Loss: 0.0017493026321190365\n",
      "Iteration: 4614 lambda_k: 1 Loss: 0.001749302632112408\n",
      "Iteration: 4615 lambda_k: 1 Loss: 0.001749302632106123\n",
      "Iteration: 4616 lambda_k: 1 Loss: 0.0017493026321001137\n",
      "Iteration: 4617 lambda_k: 1 Loss: 0.0017493026320944554\n",
      "Iteration: 4618 lambda_k: 1 Loss: 0.0017493026320891396\n",
      "Iteration: 4619 lambda_k: 1 Loss: 0.001749302632084116\n",
      "Iteration: 4620 lambda_k: 1 Loss: 0.001749302632079466\n",
      "Iteration: 4621 lambda_k: 1 Loss: 0.0017493026320751191\n",
      "Iteration: 4622 lambda_k: 1 Loss: 0.0017493026320710699\n",
      "Iteration: 4623 lambda_k: 1 Loss: 0.00174930263206732\n",
      "Iteration: 4624 lambda_k: 1 Loss: 0.0017493026320638241\n",
      "Iteration: 4625 lambda_k: 1 Loss: 0.001749302632060635\n",
      "Iteration: 4626 lambda_k: 1 Loss: 0.0017493026320577574\n",
      "Iteration: 4627 lambda_k: 1 Loss: 0.0017493026320551663\n",
      "Iteration: 4628 lambda_k: 1 Loss: 0.0017493026320528308\n",
      "Iteration: 4629 lambda_k: 1 Loss: 0.0017493026320507408\n",
      "Iteration: 4630 lambda_k: 1 Loss: 0.0017493026320489203\n",
      "Iteration: 4631 lambda_k: 1 Loss: 0.001749302632047377\n",
      "Iteration: 4632 lambda_k: 1 Loss: 0.001749302632046129\n",
      "Iteration: 4633 lambda_k: 1 Loss: 0.0017493026320451609\n",
      "Iteration: 4634 lambda_k: 1 Loss: 0.0017493026320444178\n",
      "Iteration: 4635 lambda_k: 1 Loss: 0.0017493026320438924\n",
      "Iteration: 4636 lambda_k: 1 Loss: 0.0017493026320436747\n",
      "Iteration: 4637 lambda_k: 1 Loss: 0.0017493026320436946\n",
      "Iteration: 4638 lambda_k: 1 Loss: 0.0017493026320439405\n",
      "Iteration: 4639 lambda_k: 1 Loss: 0.001749302632044424\n",
      "Iteration: 4640 lambda_k: 1 Loss: 0.0017493026320451433\n",
      "Iteration: 4641 lambda_k: 1 Loss: 0.0017493026320460577\n",
      "Iteration: 4642 lambda_k: 1 Loss: 0.0017493026320472209\n",
      "Iteration: 4643 lambda_k: 1 Loss: 0.0017493026320485733\n",
      "Iteration: 4644 lambda_k: 1 Loss: 0.0017493026320501816\n",
      "Iteration: 4645 lambda_k: 1 Loss: 0.0017493026320519979\n",
      "Iteration: 4646 lambda_k: 1 Loss: 0.0017493026320540008\n",
      "Iteration: 4647 lambda_k: 1 Loss: 0.0017493026320562033\n",
      "Iteration: 4648 lambda_k: 1 Loss: 0.0017493026320586154\n",
      "Iteration: 4649 lambda_k: 1 Loss: 0.001749302632061245\n",
      "Iteration: 4650 lambda_k: 1 Loss: 0.0017493026320640752\n",
      "Iteration: 4651 lambda_k: 1 Loss: 0.001749302632067061\n",
      "Iteration: 4652 lambda_k: 1 Loss: 0.0017493026320702855\n",
      "Iteration: 4653 lambda_k: 1 Loss: 0.0017493026320736609\n",
      "Iteration: 4654 lambda_k: 1 Loss: 0.0017493026320772687\n",
      "Iteration: 4655 lambda_k: 1 Loss: 0.0017493026320810445\n",
      "Iteration: 4656 lambda_k: 1 Loss: 0.0017493026320849786\n",
      "Iteration: 4657 lambda_k: 1 Loss: 0.0017493026320890882\n",
      "Iteration: 4658 lambda_k: 1 Loss: 0.00174930263209337\n",
      "Iteration: 4659 lambda_k: 1 Loss: 0.001749302632097829\n",
      "Iteration: 4660 lambda_k: 1 Loss: 0.0017493026321024005\n",
      "Iteration: 4661 lambda_k: 1 Loss: 0.001749302632107165\n",
      "Iteration: 4662 lambda_k: 1 Loss: 0.0017493026321121225\n",
      "Iteration: 4663 lambda_k: 1 Loss: 0.0017493026321172024\n",
      "Iteration: 4664 lambda_k: 1 Loss: 0.0017493026321223968\n",
      "Iteration: 4665 lambda_k: 1 Loss: 0.0017493026321277938\n",
      "Iteration: 4666 lambda_k: 1 Loss: 0.0017493026321333338\n",
      "Iteration: 4667 lambda_k: 1 Loss: 0.001749302632139018\n",
      "Iteration: 4668 lambda_k: 1 Loss: 0.00174930263214483\n",
      "Iteration: 4669 lambda_k: 1 Loss: 0.0017493026321507917\n",
      "Iteration: 4670 lambda_k: 1 Loss: 0.0017493026321568994\n",
      "Iteration: 4671 lambda_k: 1 Loss: 0.0017493026321630863\n",
      "Iteration: 4672 lambda_k: 1 Loss: 0.0017493026321694178\n",
      "Iteration: 4673 lambda_k: 1 Loss: 0.001749302632175909\n",
      "Iteration: 4674 lambda_k: 1 Loss: 0.0017493026321825\n",
      "Iteration: 4675 lambda_k: 1 Loss: 0.001749302632189253\n",
      "Iteration: 4676 lambda_k: 1 Loss: 0.0017493026321961102\n",
      "Iteration: 4677 lambda_k: 1 Loss: 0.001749302632203063\n",
      "Iteration: 4678 lambda_k: 1 Loss: 0.0017493026322101958\n",
      "Iteration: 4679 lambda_k: 1 Loss: 0.0017493026322174378\n",
      "Iteration: 4680 lambda_k: 1 Loss: 0.0017493026322247917\n",
      "Iteration: 4681 lambda_k: 1 Loss: 0.0017493026322322627\n",
      "Iteration: 4682 lambda_k: 1 Loss: 0.0017493026322397609\n",
      "Iteration: 4683 lambda_k: 1 Loss: 0.0017493026322473832\n",
      "Iteration: 4684 lambda_k: 1 Loss: 0.001749302632255123\n",
      "Iteration: 4685 lambda_k: 1 Loss: 0.0017493026322630014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4686 lambda_k: 1 Loss: 0.0017493026322709338\n",
      "Iteration: 4687 lambda_k: 1 Loss: 0.0017493026322790031\n",
      "Iteration: 4688 lambda_k: 1 Loss: 0.0017493026322871232\n",
      "Iteration: 4689 lambda_k: 1 Loss: 0.0017493026322953948\n",
      "Iteration: 4690 lambda_k: 1 Loss: 0.0017493026323037394\n",
      "Iteration: 4691 lambda_k: 1 Loss: 0.0017493026323121502\n",
      "Iteration: 4692 lambda_k: 1 Loss: 0.0017493026323206348\n",
      "Iteration: 4693 lambda_k: 1 Loss: 0.0017493026323292678\n",
      "Iteration: 4694 lambda_k: 1 Loss: 0.001749302632337919\n",
      "Iteration: 4695 lambda_k: 1 Loss: 0.001749302632346723\n",
      "Iteration: 4696 lambda_k: 1 Loss: 0.0017493026323555547\n",
      "Iteration: 4697 lambda_k: 1 Loss: 0.001749302632364477\n",
      "Iteration: 4698 lambda_k: 1 Loss: 0.0017493026323734861\n",
      "Iteration: 4699 lambda_k: 1 Loss: 0.0017493026323826004\n",
      "Iteration: 4700 lambda_k: 1 Loss: 0.0017493026323917309\n",
      "Iteration: 4701 lambda_k: 1 Loss: 0.0017493026324009666\n",
      "Iteration: 4702 lambda_k: 1 Loss: 0.0017493026324102347\n",
      "Iteration: 4703 lambda_k: 1 Loss: 0.0017493026324196137\n",
      "Iteration: 4704 lambda_k: 1 Loss: 0.0017493026324290311\n",
      "Iteration: 4705 lambda_k: 1 Loss: 0.0017493026324385606\n",
      "Iteration: 4706 lambda_k: 1 Loss: 0.0017493026324481393\n",
      "Iteration: 4707 lambda_k: 1 Loss: 0.0017493026324577662\n",
      "Iteration: 4708 lambda_k: 1 Loss: 0.0017493026324674429\n",
      "Iteration: 4709 lambda_k: 1 Loss: 0.001749302632477162\n",
      "Iteration: 4710 lambda_k: 1 Loss: 0.0017493026324869342\n",
      "Iteration: 4711 lambda_k: 1 Loss: 0.0017493026324967734\n",
      "Iteration: 4712 lambda_k: 1 Loss: 0.0017493026325066756\n",
      "Iteration: 4713 lambda_k: 1 Loss: 0.0017493026325166357\n",
      "Iteration: 4714 lambda_k: 1 Loss: 0.0017493026325266642\n",
      "Iteration: 4715 lambda_k: 1 Loss: 0.0017493026325367666\n",
      "Iteration: 4716 lambda_k: 1 Loss: 0.0017493026325468644\n",
      "Iteration: 4717 lambda_k: 1 Loss: 0.0017493026325570416\n",
      "Iteration: 4718 lambda_k: 1 Loss: 0.0017493026325672986\n",
      "Iteration: 4719 lambda_k: 1 Loss: 0.0017493026325775536\n",
      "Iteration: 4720 lambda_k: 1 Loss: 0.0017493026325878893\n",
      "Iteration: 4721 lambda_k: 1 Loss: 0.0017493026325982497\n",
      "Iteration: 4722 lambda_k: 1 Loss: 0.0017493026326086073\n",
      "Iteration: 4723 lambda_k: 1 Loss: 0.001749302632619058\n",
      "Iteration: 4724 lambda_k: 1 Loss: 0.00174930263262954\n",
      "Iteration: 4725 lambda_k: 1 Loss: 0.001749302632640029\n",
      "Iteration: 4726 lambda_k: 1 Loss: 0.0017493026326505362\n",
      "Iteration: 4727 lambda_k: 1 Loss: 0.0017493026326611532\n",
      "Iteration: 4728 lambda_k: 1 Loss: 0.0017493026326717944\n",
      "Iteration: 4729 lambda_k: 1 Loss: 0.0017493026326824781\n",
      "Iteration: 4730 lambda_k: 1 Loss: 0.001749302632693191\n",
      "Iteration: 4731 lambda_k: 1 Loss: 0.0017493026327039351\n",
      "Iteration: 4732 lambda_k: 1 Loss: 0.0017493026327146484\n",
      "Iteration: 4733 lambda_k: 1 Loss: 0.0017493026327253824\n",
      "Iteration: 4734 lambda_k: 1 Loss: 0.0017493026327361542\n",
      "Iteration: 4735 lambda_k: 1 Loss: 0.0017493026327469832\n",
      "Iteration: 4736 lambda_k: 1 Loss: 0.0017493026327578534\n",
      "Iteration: 4737 lambda_k: 1 Loss: 0.0017493026327687162\n",
      "Iteration: 4738 lambda_k: 1 Loss: 0.0017493026327796296\n",
      "Iteration: 4739 lambda_k: 1 Loss: 0.0017493026327905184\n",
      "Iteration: 4740 lambda_k: 1 Loss: 0.0017493026328014624\n",
      "Iteration: 4741 lambda_k: 1 Loss: 0.0017493026328123818\n",
      "Iteration: 4742 lambda_k: 1 Loss: 0.001749302632823362\n",
      "Iteration: 4743 lambda_k: 1 Loss: 0.0017493026328343373\n",
      "Iteration: 4744 lambda_k: 1 Loss: 0.0017493026328453817\n",
      "Iteration: 4745 lambda_k: 1 Loss: 0.0017493026328564202\n",
      "Iteration: 4746 lambda_k: 1 Loss: 0.001749302632867446\n",
      "Iteration: 4747 lambda_k: 1 Loss: 0.0017493026328784576\n",
      "Iteration: 4748 lambda_k: 1 Loss: 0.001749302632889551\n",
      "Iteration: 4749 lambda_k: 1 Loss: 0.0017493026329006647\n",
      "Iteration: 4750 lambda_k: 1 Loss: 0.0017493026329117756\n",
      "Iteration: 4751 lambda_k: 1 Loss: 0.001749302632922884\n",
      "Iteration: 4752 lambda_k: 1 Loss: 0.0017493026329339894\n",
      "Iteration: 4753 lambda_k: 1 Loss: 0.0017493026329451046\n",
      "Iteration: 4754 lambda_k: 1 Loss: 0.0017493026329562385\n",
      "Iteration: 4755 lambda_k: 1 Loss: 0.0017493026329673852\n",
      "Iteration: 4756 lambda_k: 1 Loss: 0.0017493026329785386\n",
      "Iteration: 4757 lambda_k: 1 Loss: 0.0017493026329896426\n",
      "Iteration: 4758 lambda_k: 1 Loss: 0.0017493026330007632\n",
      "Iteration: 4759 lambda_k: 1 Loss: 0.0017493026330119106\n",
      "Iteration: 4760 lambda_k: 1 Loss: 0.0017493026330230854\n",
      "Iteration: 4761 lambda_k: 1 Loss: 0.00174930263303427\n",
      "Iteration: 4762 lambda_k: 1 Loss: 0.001749302633045421\n",
      "Iteration: 4763 lambda_k: 1 Loss: 0.0017493026330566025\n",
      "Iteration: 4764 lambda_k: 1 Loss: 0.001749302633067732\n",
      "Iteration: 4765 lambda_k: 1 Loss: 0.0017493026330788917\n",
      "Iteration: 4766 lambda_k: 1 Loss: 0.0017493026330900126\n",
      "Iteration: 4767 lambda_k: 1 Loss: 0.00174930263310117\n",
      "Iteration: 4768 lambda_k: 1 Loss: 0.0017493026331122886\n",
      "Iteration: 4769 lambda_k: 1 Loss: 0.0017493026331234578\n",
      "Iteration: 4770 lambda_k: 1 Loss: 0.001749302633134598\n",
      "Iteration: 4771 lambda_k: 1 Loss: 0.0017493026331457857\n",
      "Iteration: 4772 lambda_k: 1 Loss: 0.0017493026331569363\n",
      "Iteration: 4773 lambda_k: 1 Loss: 0.0017493026331680506\n",
      "Iteration: 4774 lambda_k: 1 Loss: 0.0017493026331792207\n",
      "Iteration: 4775 lambda_k: 1 Loss: 0.00174930263319037\n",
      "Iteration: 4776 lambda_k: 1 Loss: 0.0017493026332015087\n",
      "Iteration: 4777 lambda_k: 1 Loss: 0.001749302633212619\n",
      "Iteration: 4778 lambda_k: 1 Loss: 0.0017493026332237034\n",
      "Iteration: 4779 lambda_k: 1 Loss: 0.0017493026332347692\n",
      "Iteration: 4780 lambda_k: 1 Loss: 0.0017493026332458192\n",
      "Iteration: 4781 lambda_k: 1 Loss: 0.0017493026332568492\n",
      "Iteration: 4782 lambda_k: 1 Loss: 0.001749302633267864\n",
      "Iteration: 4783 lambda_k: 1 Loss: 0.0017493026332788843\n",
      "Iteration: 4784 lambda_k: 1 Loss: 0.0017493026332898829\n",
      "Iteration: 4785 lambda_k: 1 Loss: 0.0017493026333008771\n",
      "Iteration: 4786 lambda_k: 1 Loss: 0.0017493026333118742\n",
      "Iteration: 4787 lambda_k: 1 Loss: 0.0017493026333228686\n",
      "Iteration: 4788 lambda_k: 1 Loss: 0.0017493026333338557\n",
      "Iteration: 4789 lambda_k: 1 Loss: 0.0017493026333448526\n",
      "Iteration: 4790 lambda_k: 1 Loss: 0.001749302633355848\n",
      "Iteration: 4791 lambda_k: 1 Loss: 0.0017493026333667667\n",
      "Iteration: 4792 lambda_k: 1 Loss: 0.0017493026333776853\n",
      "Iteration: 4793 lambda_k: 1 Loss: 0.0017493026333886105\n",
      "Iteration: 4794 lambda_k: 1 Loss: 0.0017493026333995514\n",
      "Iteration: 4795 lambda_k: 1 Loss: 0.0017493026334104165\n",
      "Iteration: 4796 lambda_k: 1 Loss: 0.0017493026334212984\n",
      "Iteration: 4797 lambda_k: 1 Loss: 0.0017493026334321196\n",
      "Iteration: 4798 lambda_k: 1 Loss: 0.0017493026334429512\n",
      "Iteration: 4799 lambda_k: 1 Loss: 0.0017493026334538025\n",
      "Iteration: 4800 lambda_k: 1 Loss: 0.0017493026334645845\n",
      "Iteration: 4801 lambda_k: 1 Loss: 0.0017493026334753946\n",
      "Iteration: 4802 lambda_k: 1 Loss: 0.0017493026334861512\n",
      "Iteration: 4803 lambda_k: 1 Loss: 0.001749302633496927\n",
      "Iteration: 4804 lambda_k: 1 Loss: 0.0017493026335076507\n",
      "Iteration: 4805 lambda_k: 1 Loss: 0.001749302633518399\n",
      "Iteration: 4806 lambda_k: 1 Loss: 0.0017493026335290904\n",
      "Iteration: 4807 lambda_k: 1 Loss: 0.001749302633539734\n",
      "Iteration: 4808 lambda_k: 1 Loss: 0.001749302633550407\n",
      "Iteration: 4809 lambda_k: 1 Loss: 0.0017493026335610269\n",
      "Iteration: 4810 lambda_k: 1 Loss: 0.00174930263357161\n",
      "Iteration: 4811 lambda_k: 1 Loss: 0.0017493026335822217\n",
      "Iteration: 4812 lambda_k: 1 Loss: 0.0017493026335927895\n",
      "Iteration: 4813 lambda_k: 1 Loss: 0.0017493026336033249\n",
      "Iteration: 4814 lambda_k: 1 Loss: 0.001749302633613809\n",
      "Iteration: 4815 lambda_k: 1 Loss: 0.0017493026336243421\n",
      "Iteration: 4816 lambda_k: 1 Loss: 0.0017493026336348465\n",
      "Iteration: 4817 lambda_k: 1 Loss: 0.0017493026336453026\n",
      "Iteration: 4818 lambda_k: 1 Loss: 0.001749302633655723\n",
      "Iteration: 4819 lambda_k: 1 Loss: 0.0017493026336661004\n",
      "Iteration: 4820 lambda_k: 1 Loss: 0.001749302633676446\n",
      "Iteration: 4821 lambda_k: 1 Loss: 0.0017493026336867753\n",
      "Iteration: 4822 lambda_k: 1 Loss: 0.0017493026336970632\n",
      "Iteration: 4823 lambda_k: 1 Loss: 0.0017493026337073358\n",
      "Iteration: 4824 lambda_k: 1 Loss: 0.001749302633717579\n",
      "Iteration: 4825 lambda_k: 1 Loss: 0.0017493026337278752\n",
      "Iteration: 4826 lambda_k: 1 Loss: 0.0017493026337380593\n",
      "Iteration: 4827 lambda_k: 1 Loss: 0.0017493026337482192\n",
      "Iteration: 4828 lambda_k: 1 Loss: 0.0017493026337583636\n",
      "Iteration: 4829 lambda_k: 1 Loss: 0.0017493026337684872\n",
      "Iteration: 4830 lambda_k: 1 Loss: 0.0017493026337785894\n",
      "Iteration: 4831 lambda_k: 1 Loss: 0.0017493026337886835\n",
      "Iteration: 4832 lambda_k: 1 Loss: 0.0017493026337987564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4833 lambda_k: 1 Loss: 0.0017493026338088185\n",
      "Iteration: 4834 lambda_k: 1 Loss: 0.0017493026338188775\n",
      "Iteration: 4835 lambda_k: 1 Loss: 0.0017493026338288341\n",
      "Iteration: 4836 lambda_k: 1 Loss: 0.0017493026338387656\n",
      "Iteration: 4837 lambda_k: 1 Loss: 0.001749302633848686\n",
      "Iteration: 4838 lambda_k: 1 Loss: 0.0017493026338586115\n",
      "Iteration: 4839 lambda_k: 1 Loss: 0.0017493026338685222\n",
      "Iteration: 4840 lambda_k: 1 Loss: 0.0017493026338783445\n",
      "Iteration: 4841 lambda_k: 1 Loss: 0.0017493026338881654\n",
      "Iteration: 4842 lambda_k: 1 Loss: 0.0017493026338979852\n",
      "Iteration: 4843 lambda_k: 1 Loss: 0.0017493026339077205\n",
      "Iteration: 4844 lambda_k: 1 Loss: 0.0017493026339174564\n",
      "Iteration: 4845 lambda_k: 1 Loss: 0.001749302633927188\n",
      "Iteration: 4846 lambda_k: 1 Loss: 0.0017493026339368482\n",
      "Iteration: 4847 lambda_k: 1 Loss: 0.0017493026339464963\n",
      "Iteration: 4848 lambda_k: 1 Loss: 0.001749302633956158\n",
      "Iteration: 4849 lambda_k: 1 Loss: 0.0017493026339657517\n",
      "Iteration: 4850 lambda_k: 1 Loss: 0.0017493026339753337\n",
      "Iteration: 4851 lambda_k: 1 Loss: 0.001749302633984852\n",
      "Iteration: 4852 lambda_k: 1 Loss: 0.0017493026339943619\n",
      "Iteration: 4853 lambda_k: 1 Loss: 0.0017493026340038027\n",
      "Iteration: 4854 lambda_k: 1 Loss: 0.0017493026340132606\n",
      "Iteration: 4855 lambda_k: 1 Loss: 0.001749302634022648\n",
      "Iteration: 4856 lambda_k: 1 Loss: 0.0017493026340320546\n",
      "Iteration: 4857 lambda_k: 1 Loss: 0.0017493026340413868\n",
      "Iteration: 4858 lambda_k: 1 Loss: 0.001749302634050719\n",
      "Iteration: 4859 lambda_k: 1 Loss: 0.001749302634059998\n",
      "Iteration: 4860 lambda_k: 1 Loss: 0.0017493026340692878\n",
      "Iteration: 4861 lambda_k: 1 Loss: 0.0017493026340785248\n",
      "Iteration: 4862 lambda_k: 1 Loss: 0.001749302634087778\n",
      "Iteration: 4863 lambda_k: 1 Loss: 0.0017493026340969597\n",
      "Iteration: 4864 lambda_k: 1 Loss: 0.0017493026341060792\n",
      "Iteration: 4865 lambda_k: 1 Loss: 0.0017493026341152212\n",
      "Iteration: 4866 lambda_k: 1 Loss: 0.0017493026341243115\n",
      "Iteration: 4867 lambda_k: 1 Loss: 0.0017493026341333386\n",
      "Iteration: 4868 lambda_k: 1 Loss: 0.0017493026341423865\n",
      "Iteration: 4869 lambda_k: 1 Loss: 0.0017493026341513875\n",
      "Iteration: 4870 lambda_k: 1 Loss: 0.0017493026341603303\n",
      "Iteration: 4871 lambda_k: 1 Loss: 0.0017493026341692994\n",
      "Iteration: 4872 lambda_k: 1 Loss: 0.0017493026341782135\n",
      "Iteration: 4873 lambda_k: 1 Loss: 0.001749302634187077\n",
      "Iteration: 4874 lambda_k: 1 Loss: 0.0017493026341958851\n",
      "Iteration: 4875 lambda_k: 1 Loss: 0.001749302634204725\n",
      "Iteration: 4876 lambda_k: 1 Loss: 0.0017493026342135275\n",
      "Iteration: 4877 lambda_k: 1 Loss: 0.0017493026342222766\n",
      "Iteration: 4878 lambda_k: 1 Loss: 0.001749302634230971\n",
      "Iteration: 4879 lambda_k: 1 Loss: 0.0017493026342396193\n",
      "Iteration: 4880 lambda_k: 1 Loss: 0.001749302634248221\n",
      "Iteration: 4881 lambda_k: 1 Loss: 0.0017493026342568563\n",
      "Iteration: 4882 lambda_k: 1 Loss: 0.0017493026342654497\n",
      "Iteration: 4883 lambda_k: 1 Loss: 0.0017493026342740008\n",
      "Iteration: 4884 lambda_k: 1 Loss: 0.0017493026342825103\n",
      "Iteration: 4885 lambda_k: 1 Loss: 0.001749302634290985\n",
      "Iteration: 4886 lambda_k: 1 Loss: 0.0017493026342994076\n",
      "Iteration: 4887 lambda_k: 1 Loss: 0.001749302634307863\n",
      "Iteration: 4888 lambda_k: 1 Loss: 0.0017493026343162845\n",
      "Iteration: 4889 lambda_k: 1 Loss: 0.0017493026343246788\n",
      "Iteration: 4890 lambda_k: 1 Loss: 0.0017493026343330309\n",
      "Iteration: 4891 lambda_k: 1 Loss: 0.0017493026343413447\n",
      "Iteration: 4892 lambda_k: 1 Loss: 0.001749302634349628\n",
      "Iteration: 4893 lambda_k: 1 Loss: 0.0017493026343578676\n",
      "Iteration: 4894 lambda_k: 1 Loss: 0.0017493026343660845\n",
      "Iteration: 4895 lambda_k: 1 Loss: 0.0017493026343742527\n",
      "Iteration: 4896 lambda_k: 1 Loss: 0.0017493026343823991\n",
      "Iteration: 4897 lambda_k: 1 Loss: 0.0017493026343905105\n",
      "Iteration: 4898 lambda_k: 1 Loss: 0.0017493026343985958\n",
      "Iteration: 4899 lambda_k: 1 Loss: 0.0017493026344066488\n",
      "Iteration: 4900 lambda_k: 1 Loss: 0.0017493026344146713\n",
      "Iteration: 4901 lambda_k: 1 Loss: 0.0017493026344226718\n",
      "Iteration: 4902 lambda_k: 1 Loss: 0.0017493026344306412\n",
      "Iteration: 4903 lambda_k: 1 Loss: 0.001749302634438579\n",
      "Iteration: 4904 lambda_k: 1 Loss: 0.001749302634446492\n",
      "Iteration: 4905 lambda_k: 1 Loss: 0.0017493026344543815\n",
      "Iteration: 4906 lambda_k: 1 Loss: 0.0017493026344622522\n",
      "Iteration: 4907 lambda_k: 1 Loss: 0.0017493026344700912\n",
      "Iteration: 4908 lambda_k: 1 Loss: 0.0017493026344778339\n",
      "Iteration: 4909 lambda_k: 1 Loss: 0.0017493026344855456\n",
      "Iteration: 4910 lambda_k: 1 Loss: 0.001749302634493227\n",
      "Iteration: 4911 lambda_k: 1 Loss: 0.0017493026345008916\n",
      "Iteration: 4912 lambda_k: 1 Loss: 0.0017493026345085354\n",
      "Iteration: 4913 lambda_k: 1 Loss: 0.0017493026345161671\n",
      "Iteration: 4914 lambda_k: 1 Loss: 0.0017493026345237837\n",
      "Iteration: 4915 lambda_k: 1 Loss: 0.0017493026345312959\n",
      "Iteration: 4916 lambda_k: 1 Loss: 0.0017493026345387825\n",
      "Iteration: 4917 lambda_k: 1 Loss: 0.0017493026345462607\n",
      "Iteration: 4918 lambda_k: 1 Loss: 0.0017493026345537228\n",
      "Iteration: 4919 lambda_k: 1 Loss: 0.0017493026345611706\n",
      "Iteration: 4920 lambda_k: 1 Loss: 0.0017493026345685306\n",
      "Iteration: 4921 lambda_k: 1 Loss: 0.001749302634575859\n",
      "Iteration: 4922 lambda_k: 1 Loss: 0.0017493026345831784\n",
      "Iteration: 4923 lambda_k: 1 Loss: 0.001749302634590492\n",
      "Iteration: 4924 lambda_k: 1 Loss: 0.001749302634597797\n",
      "Iteration: 4925 lambda_k: 1 Loss: 0.0017493026346050045\n",
      "Iteration: 4926 lambda_k: 1 Loss: 0.0017493026346121988\n",
      "Iteration: 4927 lambda_k: 1 Loss: 0.0017493026346193804\n",
      "Iteration: 4928 lambda_k: 1 Loss: 0.0017493026346265554\n",
      "Iteration: 4929 lambda_k: 1 Loss: 0.001749302634633643\n",
      "Iteration: 4930 lambda_k: 1 Loss: 0.0017493026346407157\n",
      "Iteration: 4931 lambda_k: 1 Loss: 0.0017493026346477793\n",
      "Iteration: 4932 lambda_k: 1 Loss: 0.0017493026346548373\n",
      "Iteration: 4933 lambda_k: 1 Loss: 0.001749302634661809\n",
      "Iteration: 4934 lambda_k: 1 Loss: 0.0017493026346687677\n",
      "Iteration: 4935 lambda_k: 1 Loss: 0.0017493026346757298\n",
      "Iteration: 4936 lambda_k: 1 Loss: 0.0017493026346826154\n",
      "Iteration: 4937 lambda_k: 1 Loss: 0.0017493026346894792\n",
      "Iteration: 4938 lambda_k: 1 Loss: 0.0017493026346963498\n",
      "Iteration: 4939 lambda_k: 1 Loss: 0.0017493026347031235\n",
      "Iteration: 4940 lambda_k: 1 Loss: 0.0017493026347099074\n",
      "Iteration: 4941 lambda_k: 1 Loss: 0.001749302634716678\n",
      "Iteration: 4942 lambda_k: 1 Loss: 0.001749302634723371\n",
      "Iteration: 4943 lambda_k: 1 Loss: 0.0017493026347300694\n",
      "Iteration: 4944 lambda_k: 1 Loss: 0.0017493026347367754\n",
      "Iteration: 4945 lambda_k: 1 Loss: 0.0017493026347433862\n",
      "Iteration: 4946 lambda_k: 1 Loss: 0.0017493026347499975\n",
      "Iteration: 4947 lambda_k: 1 Loss: 0.001749302634756611\n",
      "Iteration: 4948 lambda_k: 1 Loss: 0.0017493026347631397\n",
      "Iteration: 4949 lambda_k: 1 Loss: 0.0017493026347696788\n",
      "Iteration: 4950 lambda_k: 1 Loss: 0.0017493026347761447\n",
      "Iteration: 4951 lambda_k: 1 Loss: 0.0017493026347826066\n",
      "Iteration: 4952 lambda_k: 1 Loss: 0.0017493026347890751\n",
      "Iteration: 4953 lambda_k: 1 Loss: 0.001749302634795476\n",
      "Iteration: 4954 lambda_k: 1 Loss: 0.0017493026348018744\n",
      "Iteration: 4955 lambda_k: 1 Loss: 0.0017493026348081979\n",
      "Iteration: 4956 lambda_k: 1 Loss: 0.0017493026348145181\n",
      "Iteration: 4957 lambda_k: 1 Loss: 0.001749302634820772\n",
      "Iteration: 4958 lambda_k: 1 Loss: 0.0017493026348270274\n",
      "Iteration: 4959 lambda_k: 1 Loss: 0.0017493026348332945\n",
      "Iteration: 4960 lambda_k: 1 Loss: 0.0017493026348394953\n",
      "Iteration: 4961 lambda_k: 1 Loss: 0.0017493026348456967\n",
      "Iteration: 4962 lambda_k: 1 Loss: 0.001749302634851831\n",
      "Iteration: 4963 lambda_k: 1 Loss: 0.0017493026348579807\n",
      "Iteration: 4964 lambda_k: 1 Loss: 0.0017493026348640534\n",
      "Iteration: 4965 lambda_k: 1 Loss: 0.0017493026348701338\n",
      "Iteration: 4966 lambda_k: 1 Loss: 0.0017493026348761502\n",
      "Iteration: 4967 lambda_k: 1 Loss: 0.0017493026348821758\n",
      "Iteration: 4968 lambda_k: 1 Loss: 0.001749302634888133\n",
      "Iteration: 4969 lambda_k: 1 Loss: 0.0017493026348941083\n",
      "Iteration: 4970 lambda_k: 1 Loss: 0.0017493026349000083\n",
      "Iteration: 4971 lambda_k: 1 Loss: 0.0017493026349059244\n",
      "Iteration: 4972 lambda_k: 1 Loss: 0.0017493026349117717\n",
      "Iteration: 4973 lambda_k: 1 Loss: 0.0017493026349176414\n",
      "Iteration: 4974 lambda_k: 1 Loss: 0.0017493026349234384\n",
      "Iteration: 4975 lambda_k: 1 Loss: 0.0017493026349292432\n",
      "Iteration: 4976 lambda_k: 1 Loss: 0.0017493026349349947\n",
      "Iteration: 4977 lambda_k: 1 Loss: 0.0017493026349407572\n",
      "Iteration: 4978 lambda_k: 1 Loss: 0.0017493026349464616\n",
      "Iteration: 4979 lambda_k: 1 Loss: 0.001749302634952184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4980 lambda_k: 1 Loss: 0.0017493026349578375\n",
      "Iteration: 4981 lambda_k: 1 Loss: 0.001749302634963432\n",
      "Iteration: 4982 lambda_k: 1 Loss: 0.001749302634969037\n",
      "Iteration: 4983 lambda_k: 1 Loss: 0.0017493026349745908\n",
      "Iteration: 4984 lambda_k: 1 Loss: 0.0017493026349801528\n",
      "Iteration: 4985 lambda_k: 1 Loss: 0.0017493026349856603\n",
      "Iteration: 4986 lambda_k: 1 Loss: 0.0017493026349911897\n",
      "Iteration: 4987 lambda_k: 1 Loss: 0.0017493026349966634\n",
      "Iteration: 4988 lambda_k: 1 Loss: 0.0017493026350020688\n",
      "Iteration: 4989 lambda_k: 1 Loss: 0.0017493026350074929\n",
      "Iteration: 4990 lambda_k: 1 Loss: 0.0017493026350128608\n",
      "Iteration: 4991 lambda_k: 1 Loss: 0.0017493026350182475\n",
      "Iteration: 4992 lambda_k: 1 Loss: 0.0017493026350235842\n",
      "Iteration: 4993 lambda_k: 1 Loss: 0.0017493026350289482\n",
      "Iteration: 4994 lambda_k: 1 Loss: 0.0017493026350342525\n",
      "Iteration: 4995 lambda_k: 1 Loss: 0.0017493026350394983\n",
      "Iteration: 4996 lambda_k: 1 Loss: 0.0017493026350447664\n",
      "Iteration: 4997 lambda_k: 1 Loss: 0.0017493026350499737\n",
      "Iteration: 4998 lambda_k: 1 Loss: 0.001749302635055209\n",
      "Iteration: 4999 lambda_k: 1 Loss: 0.0017493026350603887\n",
      "Iteration: 5000 lambda_k: 1 Loss: 0.0017493026350655092\n",
      "Iteration: 5001 lambda_k: 1 Loss: 0.0017493026350706602\n",
      "Iteration: 5002 lambda_k: 1 Loss: 0.001749302635075757\n",
      "Iteration: 5003 lambda_k: 1 Loss: 0.0017493026350807975\n",
      "Iteration: 5004 lambda_k: 1 Loss: 0.0017493026350858538\n",
      "Iteration: 5005 lambda_k: 1 Loss: 0.0017493026350908646\n",
      "Iteration: 5006 lambda_k: 1 Loss: 0.001749302635095817\n",
      "Iteration: 5007 lambda_k: 1 Loss: 0.0017493026351008104\n",
      "Iteration: 5008 lambda_k: 1 Loss: 0.0017493026351057572\n",
      "Iteration: 5009 lambda_k: 1 Loss: 0.0017493026351107174\n",
      "Iteration: 5010 lambda_k: 1 Loss: 0.0017493026351156305\n",
      "Iteration: 5011 lambda_k: 1 Loss: 0.0017493026351204928\n",
      "Iteration: 5012 lambda_k: 1 Loss: 0.001749302635125299\n",
      "Iteration: 5013 lambda_k: 1 Loss: 0.0017493026351301432\n",
      "Iteration: 5014 lambda_k: 1 Loss: 0.0017493026351349335\n",
      "Iteration: 5015 lambda_k: 1 Loss: 0.0017493026351396842\n",
      "Iteration: 5016 lambda_k: 1 Loss: 0.001749302635144446\n",
      "Iteration: 5017 lambda_k: 1 Loss: 0.001749302635149159\n",
      "Iteration: 5018 lambda_k: 1 Loss: 0.0017493026351538302\n",
      "Iteration: 5019 lambda_k: 1 Loss: 0.0017493026351585405\n",
      "Iteration: 5020 lambda_k: 1 Loss: 0.001749302635163211\n",
      "Iteration: 5021 lambda_k: 1 Loss: 0.0017493026351678282\n",
      "Iteration: 5022 lambda_k: 1 Loss: 0.0017493026351724683\n",
      "Iteration: 5023 lambda_k: 1 Loss: 0.0017493026351770788\n",
      "Iteration: 5024 lambda_k: 1 Loss: 0.0017493026351816257\n",
      "Iteration: 5025 lambda_k: 1 Loss: 0.0017493026351862065\n",
      "Iteration: 5026 lambda_k: 1 Loss: 0.0017493026351907497\n",
      "Iteration: 5027 lambda_k: 1 Loss: 0.00174930263519525\n",
      "Iteration: 5028 lambda_k: 1 Loss: 0.001749302635199691\n",
      "Iteration: 5029 lambda_k: 1 Loss: 0.0017493026352041734\n",
      "Iteration: 5030 lambda_k: 1 Loss: 0.001749302635208615\n",
      "Iteration: 5031 lambda_k: 1 Loss: 0.0017493026352130231\n",
      "Iteration: 5032 lambda_k: 1 Loss: 0.001749302635217453\n",
      "Iteration: 5033 lambda_k: 1 Loss: 0.0017493026352218418\n",
      "Iteration: 5034 lambda_k: 1 Loss: 0.00174930263522619\n",
      "Iteration: 5035 lambda_k: 1 Loss: 0.0017493026352304807\n",
      "Iteration: 5036 lambda_k: 1 Loss: 0.0017493026352348162\n",
      "Iteration: 5037 lambda_k: 1 Loss: 0.001749302635239114\n",
      "Iteration: 5038 lambda_k: 1 Loss: 0.0017493026352433667\n",
      "Iteration: 5039 lambda_k: 1 Loss: 0.0017493026352475697\n",
      "Iteration: 5040 lambda_k: 1 Loss: 0.0017493026352518174\n",
      "Iteration: 5041 lambda_k: 1 Loss: 0.0017493026352560258\n",
      "Iteration: 5042 lambda_k: 1 Loss: 0.0017493026352601922\n",
      "Iteration: 5043 lambda_k: 1 Loss: 0.0017493026352643193\n",
      "Iteration: 5044 lambda_k: 1 Loss: 0.0017493026352684749\n",
      "Iteration: 5045 lambda_k: 1 Loss: 0.0017493026352725929\n",
      "Iteration: 5046 lambda_k: 1 Loss: 0.001749302635276676\n",
      "Iteration: 5047 lambda_k: 1 Loss: 0.0017493026352807168\n",
      "Iteration: 5048 lambda_k: 1 Loss: 0.0017493026352848014\n",
      "Iteration: 5049 lambda_k: 1 Loss: 0.0017493026352888453\n",
      "Iteration: 5050 lambda_k: 1 Loss: 0.0017493026352928527\n",
      "Iteration: 5051 lambda_k: 1 Loss: 0.0017493026352968146\n",
      "Iteration: 5052 lambda_k: 1 Loss: 0.0017493026353008268\n",
      "Iteration: 5053 lambda_k: 1 Loss: 0.0017493026353047922\n",
      "Iteration: 5054 lambda_k: 1 Loss: 0.0017493026353087285\n",
      "Iteration: 5055 lambda_k: 1 Loss: 0.0017493026353126262\n",
      "Iteration: 5056 lambda_k: 1 Loss: 0.0017493026353164844\n",
      "Iteration: 5057 lambda_k: 1 Loss: 0.0017493026353203722\n",
      "Iteration: 5058 lambda_k: 1 Loss: 0.0017493026353242352\n",
      "Iteration: 5059 lambda_k: 1 Loss: 0.0017493026353280615\n",
      "Iteration: 5060 lambda_k: 1 Loss: 0.0017493026353318417\n",
      "Iteration: 5061 lambda_k: 1 Loss: 0.0017493026353355967\n",
      "Iteration: 5062 lambda_k: 1 Loss: 0.0017493026353393871\n",
      "Iteration: 5063 lambda_k: 1 Loss: 0.0017493026353431506\n",
      "Iteration: 5064 lambda_k: 1 Loss: 0.001749302635346883\n",
      "Iteration: 5065 lambda_k: 1 Loss: 0.0017493026353505722\n",
      "Iteration: 5066 lambda_k: 1 Loss: 0.001749302635354223\n",
      "Iteration: 5067 lambda_k: 1 Loss: 0.0017493026353579155\n",
      "Iteration: 5068 lambda_k: 1 Loss: 0.0017493026353615888\n",
      "Iteration: 5069 lambda_k: 1 Loss: 0.0017493026353652223\n",
      "Iteration: 5070 lambda_k: 1 Loss: 0.0017493026353688171\n",
      "Iteration: 5071 lambda_k: 1 Loss: 0.001749302635372372\n",
      "Iteration: 5072 lambda_k: 1 Loss: 0.0017493026353759809\n",
      "Iteration: 5073 lambda_k: 1 Loss: 0.001749302635379558\n",
      "Iteration: 5074 lambda_k: 1 Loss: 0.0017493026353831006\n",
      "Iteration: 5075 lambda_k: 1 Loss: 0.0017493026353866078\n",
      "Iteration: 5076 lambda_k: 1 Loss: 0.001749302635390076\n",
      "Iteration: 5077 lambda_k: 1 Loss: 0.0017493026353935992\n",
      "Iteration: 5078 lambda_k: 1 Loss: 0.0017493026353970838\n",
      "Iteration: 5079 lambda_k: 1 Loss: 0.0017493026354005365\n",
      "Iteration: 5080 lambda_k: 1 Loss: 0.0017493026354039644\n",
      "Iteration: 5081 lambda_k: 1 Loss: 0.0017493026354073555\n",
      "Iteration: 5082 lambda_k: 1 Loss: 0.001749302635410722\n",
      "Iteration: 5083 lambda_k: 1 Loss: 0.0017493026354140498\n",
      "Iteration: 5084 lambda_k: 1 Loss: 0.0017493026354174323\n",
      "Iteration: 5085 lambda_k: 1 Loss: 0.0017493026354207801\n",
      "Iteration: 5086 lambda_k: 1 Loss: 0.0017493026354240937\n",
      "Iteration: 5087 lambda_k: 1 Loss: 0.001749302635427379\n",
      "Iteration: 5088 lambda_k: 1 Loss: 0.0017493026354306362\n",
      "Iteration: 5089 lambda_k: 1 Loss: 0.0017493026354339263\n",
      "Iteration: 5090 lambda_k: 1 Loss: 0.0017493026354372062\n",
      "Iteration: 5091 lambda_k: 1 Loss: 0.0017493026354404556\n",
      "Iteration: 5092 lambda_k: 1 Loss: 0.0017493026354436752\n",
      "Iteration: 5093 lambda_k: 1 Loss: 0.001749302635446861\n",
      "Iteration: 5094 lambda_k: 1 Loss: 0.001749302635450019\n",
      "Iteration: 5095 lambda_k: 1 Loss: 0.0017493026354531529\n",
      "Iteration: 5096 lambda_k: 1 Loss: 0.0017493026354563352\n",
      "Iteration: 5097 lambda_k: 1 Loss: 0.0017493026354594929\n",
      "Iteration: 5098 lambda_k: 1 Loss: 0.0017493026354626334\n",
      "Iteration: 5099 lambda_k: 1 Loss: 0.0017493026354657253\n",
      "Iteration: 5100 lambda_k: 1 Loss: 0.0017493026354687942\n",
      "Iteration: 5101 lambda_k: 1 Loss: 0.0017493026354718385\n",
      "Iteration: 5102 lambda_k: 1 Loss: 0.001749302635474848\n",
      "Iteration: 5103 lambda_k: 1 Loss: 0.001749302635477905\n",
      "Iteration: 5104 lambda_k: 1 Loss: 0.001749302635480947\n",
      "Iteration: 5105 lambda_k: 1 Loss: 0.001749302635483963\n",
      "Iteration: 5106 lambda_k: 1 Loss: 0.0017493026354869537\n",
      "Iteration: 5107 lambda_k: 1 Loss: 0.0017493026354899117\n",
      "Iteration: 5108 lambda_k: 1 Loss: 0.0017493026354928345\n",
      "Iteration: 5109 lambda_k: 1 Loss: 0.001749302635495736\n",
      "Iteration: 5110 lambda_k: 1 Loss: 0.0017493026354987\n",
      "Iteration: 5111 lambda_k: 1 Loss: 0.001749302635501643\n",
      "Iteration: 5112 lambda_k: 1 Loss: 0.0017493026355045599\n",
      "Iteration: 5113 lambda_k: 1 Loss: 0.001749302635507442\n",
      "Iteration: 5114 lambda_k: 1 Loss: 0.0017493026355102914\n",
      "Iteration: 5115 lambda_k: 1 Loss: 0.0017493026355131275\n",
      "Iteration: 5116 lambda_k: 1 Loss: 0.0017493026355159332\n",
      "Iteration: 5117 lambda_k: 1 Loss: 0.0017493026355187137\n",
      "Iteration: 5118 lambda_k: 1 Loss: 0.0017493026355215532\n",
      "Iteration: 5119 lambda_k: 1 Loss: 0.0017493026355243628\n",
      "Iteration: 5120 lambda_k: 1 Loss: 0.0017493026355271549\n",
      "Iteration: 5121 lambda_k: 1 Loss: 0.0017493026355299196\n",
      "Iteration: 5122 lambda_k: 1 Loss: 0.0017493026355326548\n",
      "Iteration: 5123 lambda_k: 1 Loss: 0.0017493026355353653\n",
      "Iteration: 5124 lambda_k: 1 Loss: 0.0017493026355380511\n",
      "Iteration: 5125 lambda_k: 1 Loss: 0.0017493026355407122\n",
      "Iteration: 5126 lambda_k: 1 Loss: 0.0017493026355434348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5127 lambda_k: 1 Loss: 0.0017493026355461403\n",
      "Iteration: 5128 lambda_k: 1 Loss: 0.001749302635548821\n",
      "Iteration: 5129 lambda_k: 1 Loss: 0.0017493026355514735\n",
      "Iteration: 5130 lambda_k: 1 Loss: 0.0017493026355541099\n",
      "Iteration: 5131 lambda_k: 1 Loss: 0.0017493026355567141\n",
      "Iteration: 5132 lambda_k: 1 Loss: 0.0017493026355592978\n",
      "Iteration: 5133 lambda_k: 1 Loss: 0.0017493026355618537\n",
      "Iteration: 5134 lambda_k: 1 Loss: 0.0017493026355644018\n",
      "Iteration: 5135 lambda_k: 1 Loss: 0.0017493026355669087\n",
      "Iteration: 5136 lambda_k: 1 Loss: 0.001749302635569477\n",
      "Iteration: 5137 lambda_k: 1 Loss: 0.0017493026355719582\n",
      "Iteration: 5138 lambda_k: 1 Loss: 0.0017493026355744857\n",
      "Iteration: 5139 lambda_k: 1 Loss: 0.0017493026355769898\n",
      "Iteration: 5140 lambda_k: 1 Loss: 0.0017493026355794798\n",
      "Iteration: 5141 lambda_k: 1 Loss: 0.001749302635581946\n",
      "Iteration: 5142 lambda_k: 1 Loss: 0.0017493026355843877\n",
      "Iteration: 5143 lambda_k: 1 Loss: 0.0017493026355868083\n",
      "Iteration: 5144 lambda_k: 1 Loss: 0.001749302635589215\n",
      "Iteration: 5145 lambda_k: 1 Loss: 0.0017493026355915894\n",
      "Iteration: 5146 lambda_k: 1 Loss: 0.0017493026355940272\n",
      "Iteration: 5147 lambda_k: 1 Loss: 0.0017493026355964582\n",
      "Iteration: 5148 lambda_k: 1 Loss: 0.0017493026355988727\n",
      "Iteration: 5149 lambda_k: 1 Loss: 0.0017493026356012597\n",
      "Iteration: 5150 lambda_k: 1 Loss: 0.001749302635603618\n",
      "Iteration: 5151 lambda_k: 1 Loss: 0.0017493026356059625\n",
      "Iteration: 5152 lambda_k: 1 Loss: 0.0017493026356082827\n",
      "Iteration: 5153 lambda_k: 1 Loss: 0.0017493026356105886\n",
      "Iteration: 5154 lambda_k: 1 Loss: 0.0017493026356128745\n",
      "Iteration: 5155 lambda_k: 1 Loss: 0.0017493026356151435\n",
      "Iteration: 5156 lambda_k: 1 Loss: 0.0017493026356173863\n",
      "Iteration: 5157 lambda_k: 1 Loss: 0.0017493026356196102\n",
      "Iteration: 5158 lambda_k: 1 Loss: 0.0017493026356218914\n",
      "Iteration: 5159 lambda_k: 1 Loss: 0.0017493026356241604\n",
      "Iteration: 5160 lambda_k: 1 Loss: 0.0017493026356264095\n",
      "Iteration: 5161 lambda_k: 1 Loss: 0.0017493026356286499\n",
      "Iteration: 5162 lambda_k: 1 Loss: 0.0017493026356308606\n",
      "Iteration: 5163 lambda_k: 1 Loss: 0.0017493026356330502\n",
      "Iteration: 5164 lambda_k: 1 Loss: 0.0017493026356352242\n",
      "Iteration: 5165 lambda_k: 1 Loss: 0.0017493026356373753\n",
      "Iteration: 5166 lambda_k: 1 Loss: 0.0017493026356395134\n",
      "Iteration: 5167 lambda_k: 1 Loss: 0.001749302635641633\n",
      "Iteration: 5168 lambda_k: 1 Loss: 0.0017493026356437365\n",
      "Iteration: 5169 lambda_k: 1 Loss: 0.001749302635645818\n",
      "Iteration: 5170 lambda_k: 1 Loss: 0.0017493026356478765\n",
      "Iteration: 5171 lambda_k: 1 Loss: 0.0017493026356499204\n",
      "Iteration: 5172 lambda_k: 1 Loss: 0.001749302635652035\n",
      "Iteration: 5173 lambda_k: 1 Loss: 0.0017493026356540564\n",
      "Iteration: 5174 lambda_k: 1 Loss: 0.0017493026356561441\n",
      "Iteration: 5175 lambda_k: 1 Loss: 0.0017493026356582126\n",
      "Iteration: 5176 lambda_k: 1 Loss: 0.0017493026356602557\n",
      "Iteration: 5177 lambda_k: 1 Loss: 0.0017493026356622864\n",
      "Iteration: 5178 lambda_k: 1 Loss: 0.001749302635664294\n",
      "Iteration: 5179 lambda_k: 1 Loss: 0.0017493026356662817\n",
      "Iteration: 5180 lambda_k: 1 Loss: 0.0017493026356682564\n",
      "Iteration: 5181 lambda_k: 1 Loss: 0.0017493026356702223\n",
      "Iteration: 5182 lambda_k: 1 Loss: 0.0017493026356721628\n",
      "Iteration: 5183 lambda_k: 1 Loss: 0.001749302635674092\n",
      "Iteration: 5184 lambda_k: 1 Loss: 0.0017493026356760022\n",
      "Iteration: 5185 lambda_k: 1 Loss: 0.0017493026356779015\n",
      "Iteration: 5186 lambda_k: 1 Loss: 0.0017493026356798676\n",
      "Iteration: 5187 lambda_k: 1 Loss: 0.0017493026356817415\n",
      "Iteration: 5188 lambda_k: 1 Loss: 0.0017493026356836775\n",
      "Iteration: 5189 lambda_k: 1 Loss: 0.001749302635685596\n",
      "Iteration: 5190 lambda_k: 1 Loss: 0.0017493026356875006\n",
      "Iteration: 5191 lambda_k: 1 Loss: 0.0017493026356893814\n",
      "Iteration: 5192 lambda_k: 1 Loss: 0.001749302635691252\n",
      "Iteration: 5193 lambda_k: 1 Loss: 0.0017493026356931005\n",
      "Iteration: 5194 lambda_k: 1 Loss: 0.00174930263569495\n",
      "Iteration: 5195 lambda_k: 1 Loss: 0.0017493026356967764\n",
      "Iteration: 5196 lambda_k: 1 Loss: 0.001749302635698581\n",
      "Iteration: 5197 lambda_k: 1 Loss: 0.001749302635700376\n",
      "Iteration: 5198 lambda_k: 1 Loss: 0.0017493026357021497\n",
      "Iteration: 5199 lambda_k: 1 Loss: 0.0017493026357039117\n",
      "Iteration: 5200 lambda_k: 1 Loss: 0.001749302635705661\n",
      "Iteration: 5201 lambda_k: 1 Loss: 0.0017493026357073929\n",
      "Iteration: 5202 lambda_k: 1 Loss: 0.0017493026357091085\n",
      "Iteration: 5203 lambda_k: 1 Loss: 0.0017493026357108183\n",
      "Iteration: 5204 lambda_k: 1 Loss: 0.0017493026357125868\n",
      "Iteration: 5205 lambda_k: 1 Loss: 0.001749302635714348\n",
      "Iteration: 5206 lambda_k: 1 Loss: 0.001749302635716105\n",
      "Iteration: 5207 lambda_k: 1 Loss: 0.0017493026357178452\n",
      "Iteration: 5208 lambda_k: 1 Loss: 0.001749302635719572\n",
      "Iteration: 5209 lambda_k: 1 Loss: 0.0017493026357212839\n",
      "Iteration: 5210 lambda_k: 1 Loss: 0.0017493026357229685\n",
      "Iteration: 5211 lambda_k: 1 Loss: 0.0017493026357246436\n",
      "Iteration: 5212 lambda_k: 1 Loss: 0.0017493026357263113\n",
      "Iteration: 5213 lambda_k: 1 Loss: 0.0017493026357279582\n",
      "Iteration: 5214 lambda_k: 1 Loss: 0.0017493026357296077\n",
      "Iteration: 5215 lambda_k: 1 Loss: 0.0017493026357312353\n",
      "Iteration: 5216 lambda_k: 1 Loss: 0.001749302635732852\n",
      "Iteration: 5217 lambda_k: 1 Loss: 0.0017493026357344483\n",
      "Iteration: 5218 lambda_k: 1 Loss: 0.0017493026357360342\n",
      "Iteration: 5219 lambda_k: 1 Loss: 0.0017493026357376005\n",
      "Iteration: 5220 lambda_k: 1 Loss: 0.001749302635739157\n",
      "Iteration: 5221 lambda_k: 1 Loss: 0.0017493026357407037\n",
      "Iteration: 5222 lambda_k: 1 Loss: 0.001749302635742234\n",
      "Iteration: 5223 lambda_k: 1 Loss: 0.0017493026357437557\n",
      "Iteration: 5224 lambda_k: 1 Loss: 0.0017493026357452606\n",
      "Iteration: 5225 lambda_k: 1 Loss: 0.001749302635746838\n",
      "Iteration: 5226 lambda_k: 1 Loss: 0.001749302635748416\n",
      "Iteration: 5227 lambda_k: 1 Loss: 0.0017493026357499693\n",
      "Iteration: 5228 lambda_k: 1 Loss: 0.0017493026357515197\n",
      "Iteration: 5229 lambda_k: 1 Loss: 0.0017493026357530592\n",
      "Iteration: 5230 lambda_k: 1 Loss: 0.0017493026357545802\n",
      "Iteration: 5231 lambda_k: 1 Loss: 0.0017493026357560874\n",
      "Iteration: 5232 lambda_k: 1 Loss: 0.001749302635757587\n",
      "Iteration: 5233 lambda_k: 1 Loss: 0.0017493026357590735\n",
      "Iteration: 5234 lambda_k: 1 Loss: 0.001749302635760548\n",
      "Iteration: 5235 lambda_k: 1 Loss: 0.001749302635762012\n",
      "Iteration: 5236 lambda_k: 1 Loss: 0.001749302635763468\n",
      "Iteration: 5237 lambda_k: 1 Loss: 0.0017493026357649042\n",
      "Iteration: 5238 lambda_k: 1 Loss: 0.0017493026357663299\n",
      "Iteration: 5239 lambda_k: 1 Loss: 0.0017493026357677415\n",
      "Iteration: 5240 lambda_k: 1 Loss: 0.0017493026357691412\n",
      "Iteration: 5241 lambda_k: 1 Loss: 0.0017493026357705361\n",
      "Iteration: 5242 lambda_k: 1 Loss: 0.001749302635771913\n",
      "Iteration: 5243 lambda_k: 1 Loss: 0.00174930263577328\n",
      "Iteration: 5244 lambda_k: 1 Loss: 0.0017493026357746375\n",
      "Iteration: 5245 lambda_k: 1 Loss: 0.0017493026357759877\n",
      "Iteration: 5246 lambda_k: 1 Loss: 0.0017493026357773178\n",
      "Iteration: 5247 lambda_k: 1 Loss: 0.001749302635778643\n",
      "Iteration: 5248 lambda_k: 1 Loss: 0.0017493026357799741\n",
      "Iteration: 5249 lambda_k: 1 Loss: 0.001749302635781282\n",
      "Iteration: 5250 lambda_k: 1 Loss: 0.001749302635782651\n",
      "Iteration: 5251 lambda_k: 1 Loss: 0.0017493026357839417\n",
      "Iteration: 5252 lambda_k: 1 Loss: 0.0017493026357852902\n",
      "Iteration: 5253 lambda_k: 1 Loss: 0.0017493026357866374\n",
      "Iteration: 5254 lambda_k: 1 Loss: 0.0017493026357879807\n",
      "Iteration: 5255 lambda_k: 1 Loss: 0.0017493026357893059\n",
      "Iteration: 5256 lambda_k: 1 Loss: 0.0017493026357906203\n",
      "Iteration: 5257 lambda_k: 1 Loss: 0.00174930263579193\n",
      "Iteration: 5258 lambda_k: 1 Loss: 0.0017493026357932242\n",
      "Iteration: 5259 lambda_k: 1 Loss: 0.001749302635794514\n",
      "Iteration: 5260 lambda_k: 1 Loss: 0.0017493026357957866\n",
      "Iteration: 5261 lambda_k: 1 Loss: 0.0017493026357970599\n",
      "Iteration: 5262 lambda_k: 1 Loss: 0.0017493026357983164\n",
      "Iteration: 5263 lambda_k: 1 Loss: 0.001749302635799569\n",
      "Iteration: 5264 lambda_k: 1 Loss: 0.0017493026358008175\n",
      "Iteration: 5265 lambda_k: 1 Loss: 0.0017493026358020398\n",
      "Iteration: 5266 lambda_k: 1 Loss: 0.0017493026358032572\n",
      "Iteration: 5267 lambda_k: 1 Loss: 0.001749302635804466\n",
      "Iteration: 5268 lambda_k: 1 Loss: 0.0017493026358056786\n",
      "Iteration: 5269 lambda_k: 1 Loss: 0.0017493026358068667\n",
      "Iteration: 5270 lambda_k: 1 Loss: 0.0017493026358080552\n",
      "Iteration: 5271 lambda_k: 1 Loss: 0.0017493026358092257\n",
      "Iteration: 5272 lambda_k: 1 Loss: 0.0017493026358103847\n",
      "Iteration: 5273 lambda_k: 1 Loss: 0.0017493026358115392\n",
      "Iteration: 5274 lambda_k: 1 Loss: 0.0017493026358126823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5275 lambda_k: 1 Loss: 0.0017493026358138257\n",
      "Iteration: 5276 lambda_k: 1 Loss: 0.0017493026358149566\n",
      "Iteration: 5277 lambda_k: 1 Loss: 0.0017493026358160716\n",
      "Iteration: 5278 lambda_k: 1 Loss: 0.0017493026358171725\n",
      "Iteration: 5279 lambda_k: 1 Loss: 0.0017493026358182684\n",
      "Iteration: 5280 lambda_k: 1 Loss: 0.0017493026358193664\n",
      "Iteration: 5281 lambda_k: 1 Loss: 0.0017493026358204565\n",
      "Iteration: 5282 lambda_k: 1 Loss: 0.0017493026358216127\n",
      "Iteration: 5283 lambda_k: 1 Loss: 0.0017493026358227726\n",
      "Iteration: 5284 lambda_k: 1 Loss: 0.0017493026358239132\n",
      "Iteration: 5285 lambda_k: 1 Loss: 0.0017493026358250498\n",
      "Iteration: 5286 lambda_k: 1 Loss: 0.001749302635826176\n",
      "Iteration: 5287 lambda_k: 1 Loss: 0.0017493026358273082\n",
      "Iteration: 5288 lambda_k: 1 Loss: 0.0017493026358284115\n",
      "Iteration: 5289 lambda_k: 1 Loss: 0.001749302635829511\n",
      "Iteration: 5290 lambda_k: 1 Loss: 0.0017493026358306042\n",
      "Iteration: 5291 lambda_k: 1 Loss: 0.0017493026358316943\n",
      "Iteration: 5292 lambda_k: 1 Loss: 0.001749302635832771\n",
      "Iteration: 5293 lambda_k: 1 Loss: 0.0017493026358338473\n",
      "Iteration: 5294 lambda_k: 1 Loss: 0.0017493026358349172\n",
      "Iteration: 5295 lambda_k: 1 Loss: 0.00174930263583597\n",
      "Iteration: 5296 lambda_k: 1 Loss: 0.001749302635837009\n",
      "Iteration: 5297 lambda_k: 1 Loss: 0.0017493026358380514\n",
      "Iteration: 5298 lambda_k: 1 Loss: 0.0017493026358390833\n",
      "Iteration: 5299 lambda_k: 1 Loss: 0.0017493026358401105\n",
      "Iteration: 5300 lambda_k: 1 Loss: 0.0017493026358411236\n",
      "Iteration: 5301 lambda_k: 1 Loss: 0.0017493026358421306\n",
      "Iteration: 5302 lambda_k: 1 Loss: 0.001749302635843134\n",
      "Iteration: 5303 lambda_k: 1 Loss: 0.001749302635844127\n",
      "Iteration: 5304 lambda_k: 1 Loss: 0.0017493026358451186\n",
      "Iteration: 5305 lambda_k: 1 Loss: 0.0017493026358461029\n",
      "Iteration: 5306 lambda_k: 1 Loss: 0.0017493026358470695\n",
      "Iteration: 5307 lambda_k: 1 Loss: 0.0017493026358480308\n",
      "Iteration: 5308 lambda_k: 1 Loss: 0.0017493026358489845\n",
      "Iteration: 5309 lambda_k: 1 Loss: 0.0017493026358499325\n",
      "Iteration: 5310 lambda_k: 1 Loss: 0.0017493026358508788\n",
      "Iteration: 5311 lambda_k: 1 Loss: 0.0017493026358518173\n",
      "Iteration: 5312 lambda_k: 1 Loss: 0.0017493026358527456\n",
      "Iteration: 5313 lambda_k: 1 Loss: 0.0017493026358536708\n",
      "Iteration: 5314 lambda_k: 1 Loss: 0.0017493026358545779\n",
      "Iteration: 5315 lambda_k: 1 Loss: 0.0017493026358554953\n",
      "Iteration: 5316 lambda_k: 1 Loss: 0.0017493026358563974\n",
      "Iteration: 5317 lambda_k: 1 Loss: 0.0017493026358572875\n",
      "Iteration: 5318 lambda_k: 1 Loss: 0.0017493026358581765\n",
      "Iteration: 5319 lambda_k: 1 Loss: 0.0017493026358590517\n",
      "Iteration: 5320 lambda_k: 1 Loss: 0.0017493026358599356\n",
      "Iteration: 5321 lambda_k: 1 Loss: 0.0017493026358607981\n",
      "Iteration: 5322 lambda_k: 1 Loss: 0.0017493026358616586\n",
      "Iteration: 5323 lambda_k: 1 Loss: 0.0017493026358625105\n",
      "Iteration: 5324 lambda_k: 1 Loss: 0.0017493026358633608\n",
      "Iteration: 5325 lambda_k: 1 Loss: 0.0017493026358642006\n",
      "Iteration: 5326 lambda_k: 1 Loss: 0.0017493026358650434\n",
      "Iteration: 5327 lambda_k: 1 Loss: 0.001749302635865871\n",
      "Iteration: 5328 lambda_k: 1 Loss: 0.0017493026358666897\n",
      "Iteration: 5329 lambda_k: 1 Loss: 0.0017493026358675011\n",
      "Iteration: 5330 lambda_k: 1 Loss: 0.0017493026358683102\n",
      "Iteration: 5331 lambda_k: 1 Loss: 0.0017493026358691057\n",
      "Iteration: 5332 lambda_k: 1 Loss: 0.001749302635869907\n",
      "Iteration: 5333 lambda_k: 1 Loss: 0.001749302635870776\n",
      "Iteration: 5334 lambda_k: 1 Loss: 0.0017493026358715545\n",
      "Iteration: 5335 lambda_k: 1 Loss: 0.0017493026358723247\n",
      "Iteration: 5336 lambda_k: 1 Loss: 0.0017493026358731019\n",
      "Iteration: 5337 lambda_k: 1 Loss: 0.0017493026358739562\n",
      "Iteration: 5338 lambda_k: 1 Loss: 0.0017493026358748054\n",
      "Iteration: 5339 lambda_k: 1 Loss: 0.001749302635875646\n",
      "Iteration: 5340 lambda_k: 1 Loss: 0.0017493026358764846\n",
      "Iteration: 5341 lambda_k: 1 Loss: 0.0017493026358772383\n",
      "Iteration: 5342 lambda_k: 1 Loss: 0.0017493026358780662\n",
      "Iteration: 5343 lambda_k: 1 Loss: 0.001749302635878884\n",
      "Iteration: 5344 lambda_k: 1 Loss: 0.001749302635879702\n",
      "Iteration: 5345 lambda_k: 1 Loss: 0.0017493026358805113\n",
      "Iteration: 5346 lambda_k: 1 Loss: 0.0017493026358813106\n",
      "Iteration: 5347 lambda_k: 1 Loss: 0.001749302635882106\n",
      "Iteration: 5348 lambda_k: 1 Loss: 0.0017493026358828864\n",
      "Iteration: 5349 lambda_k: 1 Loss: 0.001749302635883671\n",
      "Iteration: 5350 lambda_k: 1 Loss: 0.0017493026358844507\n",
      "Iteration: 5351 lambda_k: 1 Loss: 0.0017493026358852259\n",
      "Iteration: 5352 lambda_k: 1 Loss: 0.0017493026358859918\n",
      "Iteration: 5353 lambda_k: 1 Loss: 0.001749302635886749\n",
      "Iteration: 5354 lambda_k: 1 Loss: 0.001749302635887505\n",
      "Iteration: 5355 lambda_k: 1 Loss: 0.001749302635888255\n",
      "Iteration: 5356 lambda_k: 1 Loss: 0.0017493026358889937\n",
      "Iteration: 5357 lambda_k: 1 Loss: 0.0017493026358897225\n",
      "Iteration: 5358 lambda_k: 1 Loss: 0.0017493026358904587\n",
      "Iteration: 5359 lambda_k: 1 Loss: 0.0017493026358911944\n",
      "Iteration: 5360 lambda_k: 1 Loss: 0.0017493026358919187\n",
      "Iteration: 5361 lambda_k: 1 Loss: 0.0017493026358926327\n",
      "Iteration: 5362 lambda_k: 1 Loss: 0.0017493026358933478\n",
      "Iteration: 5363 lambda_k: 1 Loss: 0.0017493026358940493\n",
      "Iteration: 5364 lambda_k: 1 Loss: 0.0017493026358947478\n",
      "Iteration: 5365 lambda_k: 1 Loss: 0.0017493026358954464\n",
      "Iteration: 5366 lambda_k: 1 Loss: 0.0017493026358961353\n",
      "Iteration: 5367 lambda_k: 1 Loss: 0.0017493026358968227\n",
      "Iteration: 5368 lambda_k: 1 Loss: 0.0017493026358975088\n",
      "Iteration: 5369 lambda_k: 1 Loss: 0.0017493026358981864\n",
      "Iteration: 5370 lambda_k: 1 Loss: 0.0017493026358988625\n",
      "Iteration: 5371 lambda_k: 1 Loss: 0.0017493026358995274\n",
      "Iteration: 5372 lambda_k: 1 Loss: 0.0017493026359001957\n",
      "Iteration: 5373 lambda_k: 1 Loss: 0.0017493026359008594\n",
      "Iteration: 5374 lambda_k: 1 Loss: 0.0017493026359015264\n",
      "Iteration: 5375 lambda_k: 1 Loss: 0.0017493026359021772\n",
      "Iteration: 5376 lambda_k: 1 Loss: 0.001749302635902831\n",
      "Iteration: 5377 lambda_k: 1 Loss: 0.0017493026359034782\n",
      "Iteration: 5378 lambda_k: 1 Loss: 0.0017493026359041194\n",
      "Iteration: 5379 lambda_k: 1 Loss: 0.0017493026359047435\n",
      "Iteration: 5380 lambda_k: 1 Loss: 0.0017493026359053795\n",
      "Iteration: 5381 lambda_k: 1 Loss: 0.0017493026359060003\n",
      "Iteration: 5382 lambda_k: 1 Loss: 0.00174930263590661\n",
      "Iteration: 5383 lambda_k: 1 Loss: 0.0017493026359072204\n",
      "Iteration: 5384 lambda_k: 1 Loss: 0.0017493026359078326\n",
      "Iteration: 5385 lambda_k: 1 Loss: 0.0017493026359084417\n",
      "Iteration: 5386 lambda_k: 1 Loss: 0.0017493026359090408\n",
      "Iteration: 5387 lambda_k: 1 Loss: 0.0017493026359096365\n",
      "Iteration: 5388 lambda_k: 1 Loss: 0.001749302635910219\n",
      "Iteration: 5389 lambda_k: 1 Loss: 0.0017493026359107966\n",
      "Iteration: 5390 lambda_k: 1 Loss: 0.001749302635911385\n",
      "Iteration: 5391 lambda_k: 1 Loss: 0.0017493026359119695\n",
      "Iteration: 5392 lambda_k: 1 Loss: 0.0017493026359125393\n",
      "Iteration: 5393 lambda_k: 1 Loss: 0.0017493026359131129\n",
      "Iteration: 5394 lambda_k: 1 Loss: 0.001749302635913684\n",
      "Iteration: 5395 lambda_k: 1 Loss: 0.0017493026359142396\n",
      "Iteration: 5396 lambda_k: 1 Loss: 0.0017493026359148005\n",
      "Iteration: 5397 lambda_k: 1 Loss: 0.0017493026359153546\n",
      "Iteration: 5398 lambda_k: 1 Loss: 0.0017493026359159077\n",
      "Iteration: 5399 lambda_k: 1 Loss: 0.0017493026359164487\n",
      "Iteration: 5400 lambda_k: 1 Loss: 0.001749302635916997\n",
      "Iteration: 5401 lambda_k: 1 Loss: 0.001749302635917538\n",
      "Iteration: 5402 lambda_k: 1 Loss: 0.001749302635918069\n",
      "Iteration: 5403 lambda_k: 1 Loss: 0.001749302635918593\n",
      "Iteration: 5404 lambda_k: 1 Loss: 0.0017493026359191102\n",
      "Iteration: 5405 lambda_k: 1 Loss: 0.0017493026359196304\n",
      "Iteration: 5406 lambda_k: 1 Loss: 0.0017493026359201454\n",
      "Iteration: 5407 lambda_k: 1 Loss: 0.0017493026359206583\n",
      "Iteration: 5408 lambda_k: 1 Loss: 0.0017493026359211672\n",
      "Iteration: 5409 lambda_k: 1 Loss: 0.0017493026359216683\n",
      "Iteration: 5410 lambda_k: 1 Loss: 0.0017493026359221727\n",
      "Iteration: 5411 lambda_k: 1 Loss: 0.0017493026359226755\n",
      "Iteration: 5412 lambda_k: 1 Loss: 0.0017493026359231677\n",
      "Iteration: 5413 lambda_k: 1 Loss: 0.0017493026359236643\n",
      "Iteration: 5414 lambda_k: 1 Loss: 0.0017493026359241466\n",
      "Iteration: 5415 lambda_k: 1 Loss: 0.001749302635924621\n",
      "Iteration: 5416 lambda_k: 1 Loss: 0.0017493026359251\n",
      "Iteration: 5417 lambda_k: 1 Loss: 0.0017493026359255799\n",
      "Iteration: 5418 lambda_k: 1 Loss: 0.001749302635926057\n",
      "Iteration: 5419 lambda_k: 1 Loss: 0.0017493026359265273\n",
      "Iteration: 5420 lambda_k: 1 Loss: 0.0017493026359269915\n",
      "Iteration: 5421 lambda_k: 1 Loss: 0.0017493026359274523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5422 lambda_k: 1 Loss: 0.0017493026359279087\n",
      "Iteration: 5423 lambda_k: 1 Loss: 0.0017493026359283682\n",
      "Iteration: 5424 lambda_k: 1 Loss: 0.0017493026359288188\n",
      "Iteration: 5425 lambda_k: 1 Loss: 0.0017493026359292753\n",
      "Iteration: 5426 lambda_k: 1 Loss: 0.0017493026359297183\n",
      "Iteration: 5427 lambda_k: 1 Loss: 0.0017493026359301515\n",
      "Iteration: 5428 lambda_k: 1 Loss: 0.0017493026359305915\n",
      "Iteration: 5429 lambda_k: 1 Loss: 0.0017493026359310258\n",
      "Iteration: 5430 lambda_k: 1 Loss: 0.0017493026359314625\n",
      "Iteration: 5431 lambda_k: 1 Loss: 0.0017493026359318873\n",
      "Iteration: 5432 lambda_k: 1 Loss: 0.0017493026359323108\n",
      "Iteration: 5433 lambda_k: 1 Loss: 0.0017493026359327985\n",
      "Iteration: 5434 lambda_k: 1 Loss: 0.0017493026359332246\n",
      "Iteration: 5435 lambda_k: 1 Loss: 0.0017493026359336398\n",
      "Iteration: 5436 lambda_k: 1 Loss: 0.0017493026359340494\n",
      "Iteration: 5437 lambda_k: 1 Loss: 0.001749302635934459\n",
      "Iteration: 5438 lambda_k: 1 Loss: 0.00174930263593487\n",
      "Iteration: 5439 lambda_k: 1 Loss: 0.001749302635935269\n",
      "Iteration: 5440 lambda_k: 1 Loss: 0.0017493026359356604\n",
      "Iteration: 5441 lambda_k: 1 Loss: 0.001749302635936129\n",
      "Iteration: 5442 lambda_k: 1 Loss: 0.001749302635936522\n",
      "Iteration: 5443 lambda_k: 1 Loss: 0.0017493026359369178\n",
      "Iteration: 5444 lambda_k: 1 Loss: 0.001749302635937376\n",
      "Iteration: 5445 lambda_k: 1 Loss: 0.0017493026359377563\n",
      "Iteration: 5446 lambda_k: 1 Loss: 0.0017493026359382171\n",
      "Iteration: 5447 lambda_k: 1 Loss: 0.0017493026359385975\n",
      "Iteration: 5448 lambda_k: 1 Loss: 0.0017493026359389839\n",
      "Iteration: 5449 lambda_k: 1 Loss: 0.0017493026359394397\n",
      "Iteration: 5450 lambda_k: 1 Loss: 0.0017493026359398111\n",
      "Iteration: 5451 lambda_k: 1 Loss: 0.0017493026359402632\n",
      "Iteration: 5452 lambda_k: 1 Loss: 0.0017493026359407058\n",
      "Iteration: 5453 lambda_k: 1 Loss: 0.001749302635941153\n",
      "Iteration: 5454 lambda_k: 1 Loss: 0.0017493026359415183\n",
      "Iteration: 5455 lambda_k: 1 Loss: 0.0017493026359419578\n",
      "Iteration: 5456 lambda_k: 1 Loss: 0.0017493026359423956\n",
      "Iteration: 5457 lambda_k: 1 Loss: 0.0017493026359428293\n",
      "Iteration: 5458 lambda_k: 1 Loss: 0.0017493026359431886\n",
      "Iteration: 5459 lambda_k: 1 Loss: 0.0017493026359436143\n",
      "Iteration: 5460 lambda_k: 1 Loss: 0.0017493026359440395\n",
      "Iteration: 5461 lambda_k: 1 Loss: 0.0017493026359443834\n",
      "Iteration: 5462 lambda_k: 1 Loss: 0.001749302635944806\n",
      "Iteration: 5463 lambda_k: 1 Loss: 0.0017493026359451523\n",
      "Iteration: 5464 lambda_k: 1 Loss: 0.001749302635945574\n",
      "Iteration: 5465 lambda_k: 1 Loss: 0.0017493026359459085\n",
      "Iteration: 5466 lambda_k: 1 Loss: 0.001749302635946316\n",
      "Iteration: 5467 lambda_k: 1 Loss: 0.0017493026359467272\n",
      "Iteration: 5468 lambda_k: 1 Loss: 0.0017493026359471405\n",
      "Iteration: 5469 lambda_k: 1 Loss: 0.001749302635947542\n",
      "Iteration: 5470 lambda_k: 1 Loss: 0.0017493026359479398\n",
      "Iteration: 5471 lambda_k: 1 Loss: 0.0017493026359483399\n",
      "Iteration: 5472 lambda_k: 1 Loss: 0.0017493026359487417\n",
      "Iteration: 5473 lambda_k: 1 Loss: 0.0017493026359491433\n",
      "Iteration: 5474 lambda_k: 1 Loss: 0.0017493026359494527\n",
      "Iteration: 5475 lambda_k: 1 Loss: 0.0017493026359498383\n",
      "Iteration: 5476 lambda_k: 1 Loss: 0.0017493026359502188\n",
      "Iteration: 5477 lambda_k: 1 Loss: 0.0017493026359506013\n",
      "Iteration: 5478 lambda_k: 1 Loss: 0.0017493026359509823\n",
      "Iteration: 5479 lambda_k: 1 Loss: 0.0017493026359513566\n",
      "Iteration: 5480 lambda_k: 1 Loss: 0.0017493026359517347\n",
      "Iteration: 5481 lambda_k: 1 Loss: 0.0017493026359521094\n",
      "Iteration: 5482 lambda_k: 1 Loss: 0.0017493026359524902\n",
      "Iteration: 5483 lambda_k: 1 Loss: 0.0017493026359528651\n",
      "Iteration: 5484 lambda_k: 1 Loss: 0.0017493026359532288\n",
      "Iteration: 5485 lambda_k: 1 Loss: 0.0017493026359535983\n",
      "Iteration: 5486 lambda_k: 1 Loss: 0.001749302635953963\n",
      "Iteration: 5487 lambda_k: 1 Loss: 0.0017493026359543238\n",
      "Iteration: 5488 lambda_k: 1 Loss: 0.0017493026359546825\n",
      "Iteration: 5489 lambda_k: 1 Loss: 0.0017493026359550379\n",
      "Iteration: 5490 lambda_k: 1 Loss: 0.0017493026359553963\n",
      "Iteration: 5491 lambda_k: 1 Loss: 0.0017493026359557526\n",
      "Iteration: 5492 lambda_k: 1 Loss: 0.0017493026359561021\n",
      "Iteration: 5493 lambda_k: 1 Loss: 0.0017493026359564482\n",
      "Iteration: 5494 lambda_k: 1 Loss: 0.0017493026359567925\n",
      "Iteration: 5495 lambda_k: 1 Loss: 0.001749302635957144\n",
      "Iteration: 5496 lambda_k: 1 Loss: 0.0017493026359574823\n",
      "Iteration: 5497 lambda_k: 1 Loss: 0.0017493026359578178\n",
      "Iteration: 5498 lambda_k: 1 Loss: 0.001749302635958159\n",
      "Iteration: 5499 lambda_k: 1 Loss: 0.001749302635958498\n",
      "Iteration: 5500 lambda_k: 1 Loss: 0.001749302635958838\n",
      "Iteration: 5501 lambda_k: 1 Loss: 0.0017493026359591752\n",
      "Iteration: 5502 lambda_k: 1 Loss: 0.0017493026359595037\n",
      "Iteration: 5503 lambda_k: 1 Loss: 0.0017493026359598283\n",
      "Iteration: 5504 lambda_k: 1 Loss: 0.0017493026359601627\n",
      "Iteration: 5505 lambda_k: 1 Loss: 0.00174930263596049\n",
      "Iteration: 5506 lambda_k: 1 Loss: 0.001749302635960816\n",
      "Iteration: 5507 lambda_k: 1 Loss: 0.0017493026359611437\n",
      "Iteration: 5508 lambda_k: 1 Loss: 0.0017493026359614722\n",
      "Iteration: 5509 lambda_k: 1 Loss: 0.001749302635961793\n",
      "Iteration: 5510 lambda_k: 1 Loss: 0.0017493026359621097\n",
      "Iteration: 5511 lambda_k: 1 Loss: 0.0017493026359624204\n",
      "Iteration: 5512 lambda_k: 1 Loss: 0.0017493026359627366\n",
      "Iteration: 5513 lambda_k: 1 Loss: 0.001749302635963049\n",
      "Iteration: 5514 lambda_k: 1 Loss: 0.0017493026359633617\n",
      "Iteration: 5515 lambda_k: 1 Loss: 0.00174930263596367\n",
      "Iteration: 5516 lambda_k: 1 Loss: 0.001749302635963973\n",
      "Iteration: 5517 lambda_k: 1 Loss: 0.0017493026359642703\n",
      "Iteration: 5518 lambda_k: 1 Loss: 0.0017493026359645689\n",
      "Iteration: 5519 lambda_k: 1 Loss: 0.0017493026359648631\n",
      "Iteration: 5520 lambda_k: 1 Loss: 0.0017493026359651643\n",
      "Iteration: 5521 lambda_k: 1 Loss: 0.001749302635965455\n",
      "Iteration: 5522 lambda_k: 1 Loss: 0.001749302635965747\n",
      "Iteration: 5523 lambda_k: 1 Loss: 0.0017493026359660325\n",
      "Iteration: 5524 lambda_k: 1 Loss: 0.0017493026359663222\n",
      "Iteration: 5525 lambda_k: 1 Loss: 0.0017493026359666117\n",
      "Iteration: 5526 lambda_k: 1 Loss: 0.0017493026359668867\n",
      "Iteration: 5527 lambda_k: 1 Loss: 0.001749302635967165\n",
      "Iteration: 5528 lambda_k: 1 Loss: 0.0017493026359674364\n",
      "Iteration: 5529 lambda_k: 1 Loss: 0.0017493026359677178\n",
      "Iteration: 5530 lambda_k: 1 Loss: 0.0017493026359679876\n",
      "Iteration: 5531 lambda_k: 1 Loss: 0.0017493026359682614\n",
      "Iteration: 5532 lambda_k: 1 Loss: 0.0017493026359685266\n",
      "Iteration: 5533 lambda_k: 1 Loss: 0.0017493026359687934\n",
      "Iteration: 5534 lambda_k: 1 Loss: 0.0017493026359690622\n",
      "Iteration: 5535 lambda_k: 1 Loss: 0.0017493026359693248\n",
      "Iteration: 5536 lambda_k: 1 Loss: 0.0017493026359695833\n",
      "Iteration: 5537 lambda_k: 1 Loss: 0.0017493026359698546\n",
      "Iteration: 5538 lambda_k: 1 Loss: 0.0017493026359701102\n",
      "Iteration: 5539 lambda_k: 1 Loss: 0.0017493026359703715\n",
      "Iteration: 5540 lambda_k: 1 Loss: 0.0017493026359706278\n",
      "Iteration: 5541 lambda_k: 1 Loss: 0.0017493026359708887\n",
      "Iteration: 5542 lambda_k: 1 Loss: 0.0017493026359711476\n",
      "Iteration: 5543 lambda_k: 1 Loss: 0.0017493026359713948\n",
      "Iteration: 5544 lambda_k: 1 Loss: 0.0017493026359716509\n",
      "Iteration: 5545 lambda_k: 1 Loss: 0.0017493026359718961\n",
      "Iteration: 5546 lambda_k: 1 Loss: 0.0017493026359721435\n",
      "Iteration: 5547 lambda_k: 1 Loss: 0.001749302635972391\n",
      "Iteration: 5548 lambda_k: 1 Loss: 0.0017493026359726358\n",
      "Iteration: 5549 lambda_k: 1 Loss: 0.0017493026359728812\n",
      "Iteration: 5550 lambda_k: 1 Loss: 0.0017493026359731234\n",
      "Iteration: 5551 lambda_k: 1 Loss: 0.0017493026359733652\n",
      "Iteration: 5552 lambda_k: 1 Loss: 0.001749302635973601\n",
      "Iteration: 5553 lambda_k: 1 Loss: 0.0017493026359738444\n",
      "Iteration: 5554 lambda_k: 1 Loss: 0.001749302635974082\n",
      "Iteration: 5555 lambda_k: 1 Loss: 0.0017493026359743182\n",
      "Iteration: 5556 lambda_k: 1 Loss: 0.0017493026359745498\n",
      "Iteration: 5557 lambda_k: 1 Loss: 0.0017493026359747827\n",
      "Iteration: 5558 lambda_k: 1 Loss: 0.0017493026359750028\n",
      "Iteration: 5559 lambda_k: 1 Loss: 0.0017493026359752333\n",
      "Iteration: 5560 lambda_k: 1 Loss: 0.0017493026359754634\n",
      "Iteration: 5561 lambda_k: 1 Loss: 0.0017493026359756986\n",
      "Iteration: 5562 lambda_k: 1 Loss: 0.0017493026359759198\n",
      "Iteration: 5563 lambda_k: 1 Loss: 0.0017493026359761438\n",
      "Iteration: 5564 lambda_k: 1 Loss: 0.0017493026359763643\n",
      "Iteration: 5565 lambda_k: 1 Loss: 0.0017493026359765844\n",
      "Iteration: 5566 lambda_k: 1 Loss: 0.001749302635976806\n",
      "Iteration: 5567 lambda_k: 1 Loss: 0.0017493026359770238\n",
      "Iteration: 5568 lambda_k: 1 Loss: 0.0017493026359772477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5569 lambda_k: 1 Loss: 0.0017493026359774659\n",
      "Iteration: 5570 lambda_k: 1 Loss: 0.0017493026359776812\n",
      "Iteration: 5571 lambda_k: 1 Loss: 0.0017493026359779037\n",
      "Iteration: 5572 lambda_k: 1 Loss: 0.0017493026359781186\n",
      "Iteration: 5573 lambda_k: 1 Loss: 0.0017493026359783341\n",
      "Iteration: 5574 lambda_k: 1 Loss: 0.0017493026359785447\n",
      "Iteration: 5575 lambda_k: 1 Loss: 0.0017493026359787622\n",
      "Iteration: 5576 lambda_k: 1 Loss: 0.0017493026359789731\n",
      "Iteration: 5577 lambda_k: 1 Loss: 0.0017493026359791757\n",
      "Iteration: 5578 lambda_k: 1 Loss: 0.0017493026359793763\n",
      "Iteration: 5579 lambda_k: 1 Loss: 0.0017493026359795727\n",
      "Iteration: 5580 lambda_k: 1 Loss: 0.0017493026359797785\n",
      "Iteration: 5581 lambda_k: 1 Loss: 0.0017493026359799771\n",
      "Iteration: 5582 lambda_k: 1 Loss: 0.001749302635980176\n",
      "Iteration: 5583 lambda_k: 1 Loss: 0.0017493026359803776\n",
      "Iteration: 5584 lambda_k: 1 Loss: 0.00174930263598058\n",
      "Iteration: 5585 lambda_k: 1 Loss: 0.0017493026359807803\n",
      "Iteration: 5586 lambda_k: 1 Loss: 0.0017493026359809742\n",
      "Iteration: 5587 lambda_k: 1 Loss: 0.0017493026359811736\n",
      "Iteration: 5588 lambda_k: 1 Loss: 0.0017493026359813738\n",
      "Iteration: 5589 lambda_k: 1 Loss: 0.0017493026359815655\n",
      "Iteration: 5590 lambda_k: 1 Loss: 0.0017493026359817645\n",
      "Iteration: 5591 lambda_k: 1 Loss: 0.0017493026359819512\n",
      "Iteration: 5592 lambda_k: 1 Loss: 0.0017493026359821473\n",
      "Iteration: 5593 lambda_k: 1 Loss: 0.0017493026359823275\n",
      "Iteration: 5594 lambda_k: 1 Loss: 0.0017493026359825129\n",
      "Iteration: 5595 lambda_k: 1 Loss: 0.0017493026359827067\n",
      "Iteration: 5596 lambda_k: 1 Loss: 0.001749302635982894\n",
      "Iteration: 5597 lambda_k: 1 Loss: 0.0017493026359830831\n",
      "Iteration: 5598 lambda_k: 1 Loss: 0.001749302635983263\n",
      "Iteration: 5599 lambda_k: 1 Loss: 0.0017493026359834464\n",
      "Iteration: 5600 lambda_k: 1 Loss: 0.0017493026359836192\n",
      "Iteration: 5601 lambda_k: 1 Loss: 0.001749302635983796\n",
      "Iteration: 5602 lambda_k: 1 Loss: 0.0017493026359839722\n",
      "Iteration: 5603 lambda_k: 1 Loss: 0.001749302635984147\n",
      "Iteration: 5604 lambda_k: 1 Loss: 0.0017493026359843248\n",
      "Iteration: 5605 lambda_k: 1 Loss: 0.0017493026359844978\n",
      "Iteration: 5606 lambda_k: 1 Loss: 0.0017493026359846726\n",
      "Iteration: 5607 lambda_k: 1 Loss: 0.0017493026359848471\n",
      "Iteration: 5608 lambda_k: 1 Loss: 0.0017493026359850206\n",
      "Iteration: 5609 lambda_k: 1 Loss: 0.0017493026359851945\n",
      "Iteration: 5610 lambda_k: 1 Loss: 0.0017493026359853667\n",
      "Iteration: 5611 lambda_k: 1 Loss: 0.0017493026359855334\n",
      "Iteration: 5612 lambda_k: 1 Loss: 0.001749302635985704\n",
      "Iteration: 5613 lambda_k: 1 Loss: 0.0017493026359858715\n",
      "Iteration: 5614 lambda_k: 1 Loss: 0.001749302635986042\n",
      "Iteration: 5615 lambda_k: 1 Loss: 0.0017493026359862085\n",
      "Iteration: 5616 lambda_k: 1 Loss: 0.0017493026359863672\n",
      "Iteration: 5617 lambda_k: 1 Loss: 0.0017493026359865253\n",
      "Iteration: 5618 lambda_k: 1 Loss: 0.0017493026359866827\n",
      "Iteration: 5619 lambda_k: 1 Loss: 0.0017493026359868442\n",
      "Iteration: 5620 lambda_k: 1 Loss: 0.0017493026359870008\n",
      "Iteration: 5621 lambda_k: 1 Loss: 0.0017493026359871593\n",
      "Iteration: 5622 lambda_k: 1 Loss: 0.0017493026359873165\n",
      "Iteration: 5623 lambda_k: 1 Loss: 0.0017493026359874635\n",
      "Iteration: 5624 lambda_k: 1 Loss: 0.0017493026359876184\n",
      "Iteration: 5625 lambda_k: 1 Loss: 0.001749302635987773\n",
      "Iteration: 5626 lambda_k: 1 Loss: 0.001749302635987916\n",
      "Iteration: 5627 lambda_k: 1 Loss: 0.0017493026359880685\n",
      "Iteration: 5628 lambda_k: 1 Loss: 0.001749302635988217\n",
      "Iteration: 5629 lambda_k: 1 Loss: 0.00174930263598836\n",
      "Iteration: 5630 lambda_k: 1 Loss: 0.0017493026359885074\n",
      "Iteration: 5631 lambda_k: 1 Loss: 0.0017493026359886538\n",
      "Iteration: 5632 lambda_k: 1 Loss: 0.001749302635988797\n",
      "Iteration: 5633 lambda_k: 1 Loss: 0.0017493026359889376\n",
      "Iteration: 5634 lambda_k: 1 Loss: 0.001749302635989083\n",
      "Iteration: 5635 lambda_k: 1 Loss: 0.0017493026359892306\n",
      "Iteration: 5636 lambda_k: 1 Loss: 0.001749302635989376\n",
      "Iteration: 5637 lambda_k: 1 Loss: 0.0017493026359895092\n",
      "Iteration: 5638 lambda_k: 1 Loss: 0.0017493026359896408\n",
      "Iteration: 5639 lambda_k: 1 Loss: 0.0017493026359897824\n",
      "Iteration: 5640 lambda_k: 1 Loss: 0.0017493026359899114\n",
      "Iteration: 5641 lambda_k: 1 Loss: 0.0017493026359900504\n",
      "Iteration: 5642 lambda_k: 1 Loss: 0.0017493026359901797\n",
      "Iteration: 5643 lambda_k: 1 Loss: 0.0017493026359903122\n",
      "Iteration: 5644 lambda_k: 1 Loss: 0.001749302635990443\n",
      "Iteration: 5645 lambda_k: 1 Loss: 0.001749302635990572\n",
      "Iteration: 5646 lambda_k: 1 Loss: 0.001749302635990703\n",
      "Iteration: 5647 lambda_k: 1 Loss: 0.0017493026359908233\n",
      "Iteration: 5648 lambda_k: 1 Loss: 0.0017493026359909475\n",
      "Iteration: 5649 lambda_k: 1 Loss: 0.001749302635991072\n",
      "Iteration: 5650 lambda_k: 1 Loss: 0.001749302635991193\n",
      "Iteration: 5651 lambda_k: 1 Loss: 0.0017493026359913086\n",
      "Iteration: 5652 lambda_k: 1 Loss: 0.0017493026359914358\n",
      "Iteration: 5653 lambda_k: 1 Loss: 0.0017493026359915562\n",
      "Iteration: 5654 lambda_k: 1 Loss: 0.0017493026359916828\n",
      "Iteration: 5655 lambda_k: 1 Loss: 0.0017493026359918\n",
      "Iteration: 5656 lambda_k: 1 Loss: 0.001749302635991921\n",
      "Iteration: 5657 lambda_k: 1 Loss: 0.001749302635992037\n",
      "Iteration: 5658 lambda_k: 1 Loss: 0.0017493026359921525\n",
      "Iteration: 5659 lambda_k: 1 Loss: 0.0017493026359922679\n",
      "Iteration: 5660 lambda_k: 1 Loss: 0.0017493026359923839\n",
      "Iteration: 5661 lambda_k: 1 Loss: 0.0017493026359924934\n",
      "Iteration: 5662 lambda_k: 1 Loss: 0.001749302635992607\n",
      "Iteration: 5663 lambda_k: 1 Loss: 0.0017493026359927263\n",
      "Iteration: 5664 lambda_k: 1 Loss: 0.0017493026359928449\n",
      "Iteration: 5665 lambda_k: 1 Loss: 0.0017493026359929602\n",
      "Iteration: 5666 lambda_k: 1 Loss: 0.0017493026359930756\n",
      "Iteration: 5667 lambda_k: 1 Loss: 0.0017493026359931823\n",
      "Iteration: 5668 lambda_k: 1 Loss: 0.00174930263599329\n",
      "Iteration: 5669 lambda_k: 1 Loss: 0.0017493026359934045\n",
      "Iteration: 5670 lambda_k: 1 Loss: 0.0017493026359935082\n",
      "Iteration: 5671 lambda_k: 1 Loss: 0.0017493026359936138\n",
      "Iteration: 5672 lambda_k: 1 Loss: 0.001749302635993719\n",
      "Iteration: 5673 lambda_k: 1 Loss: 0.0017493026359938237\n",
      "Iteration: 5674 lambda_k: 1 Loss: 0.0017493026359939354\n",
      "Iteration: 5675 lambda_k: 1 Loss: 0.0017493026359940414\n",
      "Iteration: 5676 lambda_k: 1 Loss: 0.0017493026359941524\n",
      "Iteration: 5677 lambda_k: 1 Loss: 0.0017493026359942498\n",
      "Iteration: 5678 lambda_k: 1 Loss: 0.001749302635994351\n",
      "Iteration: 5679 lambda_k: 1 Loss: 0.0017493026359944562\n",
      "Iteration: 5680 lambda_k: 1 Loss: 0.0017493026359945577\n",
      "Iteration: 5681 lambda_k: 1 Loss: 0.0017493026359946663\n",
      "Iteration: 5682 lambda_k: 1 Loss: 0.0017493026359947708\n",
      "Iteration: 5683 lambda_k: 1 Loss: 0.0017493026359948762\n",
      "Iteration: 5684 lambda_k: 1 Loss: 0.0017493026359949732\n",
      "Iteration: 5685 lambda_k: 1 Loss: 0.0017493026359950814\n",
      "Iteration: 5686 lambda_k: 1 Loss: 0.0017493026359951772\n",
      "Iteration: 5687 lambda_k: 1 Loss: 0.0017493026359952815\n",
      "Iteration: 5688 lambda_k: 1 Loss: 0.0017493026359953825\n",
      "Iteration: 5689 lambda_k: 1 Loss: 0.001749302635995478\n",
      "Iteration: 5690 lambda_k: 1 Loss: 0.0017493026359955794\n",
      "Iteration: 5691 lambda_k: 1 Loss: 0.0017493026359956735\n",
      "Iteration: 5692 lambda_k: 1 Loss: 0.0017493026359957722\n",
      "Iteration: 5693 lambda_k: 1 Loss: 0.0017493026359958655\n",
      "Iteration: 5694 lambda_k: 1 Loss: 0.0017493026359959563\n",
      "Iteration: 5695 lambda_k: 1 Loss: 0.0017493026359960472\n",
      "Iteration: 5696 lambda_k: 1 Loss: 0.0017493026359961434\n",
      "Iteration: 5697 lambda_k: 1 Loss: 0.0017493026359962287\n",
      "Iteration: 5698 lambda_k: 1 Loss: 0.001749302635996318\n",
      "Iteration: 5699 lambda_k: 1 Loss: 0.0017493026359964119\n",
      "Iteration: 5700 lambda_k: 1 Loss: 0.0017493026359965045\n",
      "Iteration: 5701 lambda_k: 1 Loss: 0.001749302635996598\n",
      "Iteration: 5702 lambda_k: 1 Loss: 0.0017493026359966894\n",
      "Iteration: 5703 lambda_k: 1 Loss: 0.0017493026359967775\n",
      "Iteration: 5704 lambda_k: 1 Loss: 0.0017493026359968647\n",
      "Iteration: 5705 lambda_k: 1 Loss: 0.0017493026359969562\n",
      "Iteration: 5706 lambda_k: 1 Loss: 0.0017493026359970453\n",
      "Iteration: 5707 lambda_k: 1 Loss: 0.00174930263599713\n",
      "Iteration: 5708 lambda_k: 1 Loss: 0.0017493026359972131\n",
      "Iteration: 5709 lambda_k: 1 Loss: 0.0017493026359972962\n",
      "Iteration: 5710 lambda_k: 1 Loss: 0.0017493026359973892\n",
      "Iteration: 5711 lambda_k: 1 Loss: 0.0017493026359974712\n",
      "Iteration: 5712 lambda_k: 1 Loss: 0.0017493026359975525\n",
      "Iteration: 5713 lambda_k: 1 Loss: 0.0017493026359976438\n",
      "Iteration: 5714 lambda_k: 1 Loss: 0.001749302635997725\n",
      "Iteration: 5715 lambda_k: 1 Loss: 0.001749302635997808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5716 lambda_k: 1 Loss: 0.0017493026359978918\n",
      "Iteration: 5717 lambda_k: 1 Loss: 0.0017493026359979777\n",
      "Iteration: 5718 lambda_k: 1 Loss: 0.0017493026359980625\n",
      "Iteration: 5719 lambda_k: 1 Loss: 0.0017493026359981477\n",
      "Iteration: 5720 lambda_k: 1 Loss: 0.0017493026359982299\n",
      "Iteration: 5721 lambda_k: 1 Loss: 0.0017493026359983168\n",
      "Iteration: 5722 lambda_k: 1 Loss: 0.001749302635998398\n",
      "Iteration: 5723 lambda_k: 1 Loss: 0.0017493026359984845\n",
      "Iteration: 5724 lambda_k: 1 Loss: 0.001749302635998569\n",
      "Iteration: 5725 lambda_k: 1 Loss: 0.0017493026359986501\n",
      "Iteration: 5726 lambda_k: 1 Loss: 0.0017493026359987325\n",
      "Iteration: 5727 lambda_k: 1 Loss: 0.0017493026359988117\n",
      "Iteration: 5728 lambda_k: 1 Loss: 0.0017493026359988834\n",
      "Iteration: 5729 lambda_k: 1 Loss: 0.0017493026359989554\n",
      "Iteration: 5730 lambda_k: 1 Loss: 0.0017493026359990322\n",
      "Iteration: 5731 lambda_k: 1 Loss: 0.0017493026359991092\n",
      "Iteration: 5732 lambda_k: 1 Loss: 0.0017493026359991788\n",
      "Iteration: 5733 lambda_k: 1 Loss: 0.0017493026359992597\n",
      "Iteration: 5734 lambda_k: 1 Loss: 0.0017493026359993293\n",
      "Iteration: 5735 lambda_k: 1 Loss: 0.001749302635999408\n",
      "Iteration: 5736 lambda_k: 1 Loss: 0.0017493026359994845\n",
      "Iteration: 5737 lambda_k: 1 Loss: 0.0017493026359995526\n",
      "Iteration: 5738 lambda_k: 1 Loss: 0.0017493026359996207\n",
      "Iteration: 5739 lambda_k: 1 Loss: 0.0017493026359996964\n",
      "Iteration: 5740 lambda_k: 1 Loss: 0.0017493026359997673\n",
      "Iteration: 5741 lambda_k: 1 Loss: 0.0017493026359998312\n",
      "Iteration: 5742 lambda_k: 1 Loss: 0.0017493026359998991\n",
      "Iteration: 5743 lambda_k: 1 Loss: 0.0017493026359999676\n",
      "Iteration: 5744 lambda_k: 1 Loss: 0.00174930263600003\n",
      "Iteration: 5745 lambda_k: 1 Loss: 0.0017493026360000993\n",
      "Iteration: 5746 lambda_k: 1 Loss: 0.0017493026360001682\n",
      "Iteration: 5747 lambda_k: 1 Loss: 0.001749302636000228\n",
      "Iteration: 5748 lambda_k: 1 Loss: 0.0017493026360002955\n",
      "Iteration: 5749 lambda_k: 1 Loss: 0.001749302636000364\n",
      "Iteration: 5750 lambda_k: 1 Loss: 0.0017493026360004278\n",
      "Iteration: 5751 lambda_k: 1 Loss: 0.0017493026360004978\n",
      "Iteration: 5752 lambda_k: 1 Loss: 0.001749302636000564\n",
      "Iteration: 5753 lambda_k: 1 Loss: 0.0017493026360006344\n",
      "Iteration: 5754 lambda_k: 1 Loss: 0.0017493026360006956\n",
      "Iteration: 5755 lambda_k: 1 Loss: 0.0017493026360007671\n",
      "Iteration: 5756 lambda_k: 1 Loss: 0.0017493026360008252\n",
      "Iteration: 5757 lambda_k: 1 Loss: 0.0017493026360008847\n",
      "Iteration: 5758 lambda_k: 1 Loss: 0.0017493026360009523\n",
      "Iteration: 5759 lambda_k: 1 Loss: 0.001749302636001016\n",
      "Iteration: 5760 lambda_k: 1 Loss: 0.001749302636001076\n",
      "Iteration: 5761 lambda_k: 1 Loss: 0.0017493026360011423\n",
      "Iteration: 5762 lambda_k: 1 Loss: 0.0017493026360012017\n",
      "Iteration: 5763 lambda_k: 1 Loss: 0.0017493026360011809\n",
      "Iteration: 5764 lambda_k: 1 Loss: 0.0017493026360012325\n",
      "Iteration: 5765 lambda_k: 1 Loss: 0.0017493026360012997\n",
      "Iteration: 5766 lambda_k: 1 Loss: 0.0017493026360013606\n",
      "Iteration: 5767 lambda_k: 1 Loss: 0.0017493026360014194\n",
      "Iteration: 5768 lambda_k: 1 Loss: 0.0017493026360014768\n",
      "Iteration: 5769 lambda_k: 1 Loss: 0.001749302636001533\n",
      "Iteration: 5770 lambda_k: 1 Loss: 0.0017493026360015929\n",
      "Iteration: 5771 lambda_k: 1 Loss: 0.0017493026360016538\n",
      "Iteration: 5772 lambda_k: 1 Loss: 0.001749302636001717\n",
      "Iteration: 5773 lambda_k: 1 Loss: 0.0017493026360017759\n",
      "Iteration: 5774 lambda_k: 1 Loss: 0.0017493026360018275\n",
      "Iteration: 5775 lambda_k: 1 Loss: 0.0017493026360018858\n",
      "Iteration: 5776 lambda_k: 1 Loss: 0.0017493026360019454\n",
      "Iteration: 5777 lambda_k: 1 Loss: 0.001749302636002002\n",
      "Iteration: 5778 lambda_k: 1 Loss: 0.001749302636002067\n",
      "Iteration: 5779 lambda_k: 1 Loss: 0.0017493026360021172\n",
      "Iteration: 5780 lambda_k: 1 Loss: 0.0017493026360021684\n",
      "Iteration: 5781 lambda_k: 1 Loss: 0.0017493026360022273\n",
      "Iteration: 5782 lambda_k: 1 Loss: 0.0017493026360022841\n",
      "Iteration: 5783 lambda_k: 1 Loss: 0.001749302636002339\n",
      "Iteration: 5784 lambda_k: 1 Loss: 0.0017493026360023958\n",
      "Iteration: 5785 lambda_k: 1 Loss: 0.001749302636002456\n",
      "Iteration: 5786 lambda_k: 1 Loss: 0.0017493026360025138\n",
      "Iteration: 5787 lambda_k: 1 Loss: 0.0017493026360025736\n",
      "Iteration: 5788 lambda_k: 1 Loss: 0.0017493026360026224\n",
      "Iteration: 5789 lambda_k: 1 Loss: 0.0017493026360026784\n",
      "Iteration: 5790 lambda_k: 1 Loss: 0.0017493026360027343\n",
      "Iteration: 5791 lambda_k: 1 Loss: 0.0017493026360027814\n",
      "Iteration: 5792 lambda_k: 1 Loss: 0.0017493026360028286\n",
      "Iteration: 5793 lambda_k: 1 Loss: 0.0017493026360028774\n",
      "Iteration: 5794 lambda_k: 1 Loss: 0.001749302636002934\n",
      "Iteration: 5795 lambda_k: 1 Loss: 0.0017493026360029832\n",
      "Iteration: 5796 lambda_k: 1 Loss: 0.0017493026360030288\n",
      "Iteration: 5797 lambda_k: 1 Loss: 0.0017493026360030737\n",
      "Iteration: 5798 lambda_k: 1 Loss: 0.00174930263600312\n",
      "Iteration: 5799 lambda_k: 1 Loss: 0.001749302636003165\n",
      "Iteration: 5800 lambda_k: 1 Loss: 0.0017493026360032207\n",
      "Iteration: 5801 lambda_k: 1 Loss: 0.0017493026360032662\n",
      "Iteration: 5802 lambda_k: 1 Loss: 0.0017493026360033118\n",
      "Iteration: 5803 lambda_k: 1 Loss: 0.0017493026360033681\n",
      "Iteration: 5804 lambda_k: 1 Loss: 0.0017493026360034176\n",
      "Iteration: 5805 lambda_k: 1 Loss: 0.001749302636003463\n",
      "Iteration: 5806 lambda_k: 1 Loss: 0.0017493026360035093\n",
      "Iteration: 5807 lambda_k: 1 Loss: 0.0017493026360035544\n",
      "Iteration: 5808 lambda_k: 1 Loss: 0.0017493026360036036\n",
      "Iteration: 5809 lambda_k: 1 Loss: 0.0017493026360036526\n",
      "Iteration: 5810 lambda_k: 1 Loss: 0.0017493026360036997\n",
      "Iteration: 5811 lambda_k: 1 Loss: 0.0017493026360037472\n",
      "Iteration: 5812 lambda_k: 1 Loss: 0.0017493026360037899\n",
      "Iteration: 5813 lambda_k: 1 Loss: 0.0017493026360038356\n",
      "Iteration: 5814 lambda_k: 1 Loss: 0.0017493026360038812\n",
      "Iteration: 5815 lambda_k: 1 Loss: 0.0017493026360039209\n",
      "Iteration: 5816 lambda_k: 1 Loss: 0.0017493026360039668\n",
      "Iteration: 5817 lambda_k: 1 Loss: 0.0017493026360040041\n",
      "Iteration: 5818 lambda_k: 1 Loss: 0.0017493026360040503\n",
      "Iteration: 5819 lambda_k: 1 Loss: 0.0017493026360040982\n",
      "Iteration: 5820 lambda_k: 1 Loss: 0.0017493026360041475\n",
      "Iteration: 5821 lambda_k: 1 Loss: 0.0017493026360041975\n",
      "Iteration: 5822 lambda_k: 1 Loss: 0.0017493026360042416\n",
      "Iteration: 5823 lambda_k: 1 Loss: 0.0017493026360042917\n",
      "Iteration: 5824 lambda_k: 1 Loss: 0.0017493026360043342\n",
      "Iteration: 5825 lambda_k: 1 Loss: 0.0017493026360043788\n",
      "Iteration: 5826 lambda_k: 1 Loss: 0.001749302636004426\n",
      "Iteration: 5827 lambda_k: 1 Loss: 0.0017493026360044625\n",
      "Iteration: 5828 lambda_k: 1 Loss: 0.0017493026360045107\n",
      "Iteration: 5829 lambda_k: 1 Loss: 0.0017493026360045495\n",
      "Iteration: 5830 lambda_k: 1 Loss: 0.0017493026360045894\n",
      "Iteration: 5831 lambda_k: 1 Loss: 0.0017493026360046377\n",
      "Iteration: 5832 lambda_k: 1 Loss: 0.001749302636004674\n",
      "Iteration: 5833 lambda_k: 1 Loss: 0.0017493026360047089\n",
      "Iteration: 5834 lambda_k: 1 Loss: 0.0017493026360047453\n",
      "Iteration: 5835 lambda_k: 1 Loss: 0.0017493026360047817\n",
      "Iteration: 5836 lambda_k: 1 Loss: 0.001749302636004824\n",
      "Iteration: 5837 lambda_k: 1 Loss: 0.0017493026360048593\n",
      "Iteration: 5838 lambda_k: 1 Loss: 0.0017493026360048945\n",
      "Iteration: 5839 lambda_k: 1 Loss: 0.0017493026360049344\n",
      "Iteration: 5840 lambda_k: 1 Loss: 0.0017493026360049684\n",
      "Iteration: 5841 lambda_k: 1 Loss: 0.001749302636005013\n",
      "Iteration: 5842 lambda_k: 1 Loss: 0.0017493026360050502\n",
      "Iteration: 5843 lambda_k: 1 Loss: 0.0017493026360050953\n",
      "Iteration: 5844 lambda_k: 1 Loss: 0.0017493026360051304\n",
      "Iteration: 5845 lambda_k: 1 Loss: 0.0017493026360051683\n",
      "Iteration: 5846 lambda_k: 1 Loss: 0.0017493026360052054\n",
      "Iteration: 5847 lambda_k: 1 Loss: 0.001749302636005249\n",
      "Iteration: 5848 lambda_k: 1 Loss: 0.0017493026360052935\n",
      "Iteration: 5849 lambda_k: 1 Loss: 0.0017493026360053303\n",
      "Iteration: 5850 lambda_k: 1 Loss: 0.001749302636005364\n",
      "Iteration: 5851 lambda_k: 1 Loss: 0.0017493026360053978\n",
      "Iteration: 5852 lambda_k: 1 Loss: 0.0017493026360054314\n",
      "Iteration: 5853 lambda_k: 1 Loss: 0.001749302636005465\n",
      "Iteration: 5854 lambda_k: 1 Loss: 0.0017493026360054982\n",
      "Iteration: 5855 lambda_k: 1 Loss: 0.0017493026360055253\n",
      "Iteration: 5856 lambda_k: 1 Loss: 0.0017493026360055636\n",
      "Iteration: 5857 lambda_k: 1 Loss: 0.001749302636005601\n",
      "Iteration: 5858 lambda_k: 1 Loss: 0.0017493026360056335\n",
      "Iteration: 5859 lambda_k: 1 Loss: 0.0017493026360056677\n",
      "Iteration: 5860 lambda_k: 1 Loss: 0.0017493026360057035\n",
      "Iteration: 5861 lambda_k: 1 Loss: 0.0017493026360057378\n",
      "Iteration: 5862 lambda_k: 1 Loss: 0.0017493026360057703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5863 lambda_k: 1 Loss: 0.0017493026360058076\n",
      "Iteration: 5864 lambda_k: 1 Loss: 0.001749302636005843\n",
      "Iteration: 5865 lambda_k: 1 Loss: 0.0017493026360058757\n",
      "Iteration: 5866 lambda_k: 1 Loss: 0.0017493026360059104\n",
      "Iteration: 5867 lambda_k: 1 Loss: 0.001749302636005946\n",
      "Iteration: 5868 lambda_k: 1 Loss: 0.001749302636005974\n",
      "Iteration: 5869 lambda_k: 1 Loss: 0.0017493026360060066\n",
      "Iteration: 5870 lambda_k: 1 Loss: 0.0017493026360060392\n",
      "Iteration: 5871 lambda_k: 1 Loss: 0.0017493026360060756\n",
      "Iteration: 5872 lambda_k: 1 Loss: 0.0017493026360061105\n",
      "Iteration: 5873 lambda_k: 1 Loss: 0.001749302636006143\n",
      "Iteration: 5874 lambda_k: 1 Loss: 0.0017493026360061773\n",
      "Iteration: 5875 lambda_k: 1 Loss: 0.001749302636006212\n",
      "Iteration: 5876 lambda_k: 1 Loss: 0.0017493026360062445\n",
      "Iteration: 5877 lambda_k: 1 Loss: 0.0017493026360062799\n",
      "Iteration: 5878 lambda_k: 1 Loss: 0.0017493026360063141\n",
      "Iteration: 5879 lambda_k: 1 Loss: 0.0017493026360063467\n",
      "Iteration: 5880 lambda_k: 1 Loss: 0.0017493026360063813\n",
      "Iteration: 5881 lambda_k: 1 Loss: 0.0017493026360064165\n",
      "Iteration: 5882 lambda_k: 1 Loss: 0.0017493026360064442\n",
      "Iteration: 5883 lambda_k: 1 Loss: 0.00174930263600648\n",
      "Iteration: 5884 lambda_k: 1 Loss: 0.0017493026360065058\n",
      "Iteration: 5885 lambda_k: 1 Loss: 0.0017493026360065407\n",
      "Iteration: 5886 lambda_k: 1 Loss: 0.0017493026360065667\n",
      "Iteration: 5887 lambda_k: 1 Loss: 0.0017493026360065958\n",
      "Iteration: 5888 lambda_k: 1 Loss: 0.001749302636006619\n",
      "Iteration: 5889 lambda_k: 1 Loss: 0.0017493026360066487\n",
      "Iteration: 5890 lambda_k: 1 Loss: 0.0017493026360066723\n",
      "Iteration: 5891 lambda_k: 1 Loss: 0.001749302636006696\n",
      "Iteration: 5892 lambda_k: 1 Loss: 0.001749302636006723\n",
      "Iteration: 5893 lambda_k: 1 Loss: 0.0017493026360067463\n",
      "Iteration: 5894 lambda_k: 1 Loss: 0.0017493026360067788\n",
      "Iteration: 5895 lambda_k: 1 Loss: 0.0017493026360068025\n",
      "Iteration: 5896 lambda_k: 1 Loss: 0.001749302636006826\n",
      "Iteration: 5897 lambda_k: 1 Loss: 0.0017493026360068536\n",
      "Iteration: 5898 lambda_k: 1 Loss: 0.001749302636006874\n",
      "Iteration: 5899 lambda_k: 1 Loss: 0.001749302636006905\n",
      "Iteration: 5900 lambda_k: 1 Loss: 0.0017493026360069267\n",
      "Iteration: 5901 lambda_k: 1 Loss: 0.0017493026360069512\n",
      "Iteration: 5902 lambda_k: 1 Loss: 0.0017493026360069787\n",
      "Iteration: 5903 lambda_k: 1 Loss: 0.0017493026360069991\n",
      "Iteration: 5904 lambda_k: 1 Loss: 0.0017493026360070303\n",
      "Iteration: 5905 lambda_k: 1 Loss: 0.001749302636007052\n",
      "Iteration: 5906 lambda_k: 1 Loss: 0.0017493026360070763\n",
      "Iteration: 5907 lambda_k: 1 Loss: 0.001749302636007104\n",
      "Iteration: 5908 lambda_k: 1 Loss: 0.0017493026360071245\n",
      "Iteration: 5909 lambda_k: 1 Loss: 0.0017493026360071555\n",
      "Iteration: 5910 lambda_k: 1 Loss: 0.0017493026360071772\n",
      "Iteration: 5911 lambda_k: 1 Loss: 0.0017493026360072017\n",
      "Iteration: 5912 lambda_k: 1 Loss: 0.0017493026360072264\n",
      "Iteration: 5913 lambda_k: 1 Loss: 0.0017493026360072491\n",
      "Iteration: 5914 lambda_k: 1 Loss: 0.0017493026360072754\n",
      "Iteration: 5915 lambda_k: 1 Loss: 0.0017493026360072962\n",
      "Iteration: 5916 lambda_k: 1 Loss: 0.0017493026360073285\n",
      "Iteration: 5917 lambda_k: 1 Loss: 0.001749302636007352\n",
      "Iteration: 5918 lambda_k: 1 Loss: 0.0017493026360073751\n",
      "Iteration: 5919 lambda_k: 1 Loss: 0.0017493026360073981\n",
      "Iteration: 5920 lambda_k: 1 Loss: 0.0017493026360074241\n",
      "Iteration: 5921 lambda_k: 1 Loss: 0.0017493026360074417\n",
      "Iteration: 5922 lambda_k: 1 Loss: 0.0017493026360074658\n",
      "Iteration: 5923 lambda_k: 1 Loss: 0.0017493026360074948\n",
      "Iteration: 5924 lambda_k: 1 Loss: 0.0017493026360075195\n",
      "Iteration: 5925 lambda_k: 1 Loss: 0.0017493026360075388\n",
      "Iteration: 5926 lambda_k: 1 Loss: 0.001749302636007555\n",
      "Iteration: 5927 lambda_k: 1 Loss: 0.0017493026360075822\n",
      "Iteration: 5928 lambda_k: 1 Loss: 0.001749302636007601\n",
      "Iteration: 5929 lambda_k: 1 Loss: 0.0017493026360076247\n",
      "Iteration: 5930 lambda_k: 1 Loss: 0.0017493026360076533\n",
      "Iteration: 5931 lambda_k: 1 Loss: 0.0017493026360076757\n",
      "Iteration: 5932 lambda_k: 1 Loss: 0.0017493026360077017\n",
      "Iteration: 5933 lambda_k: 1 Loss: 0.0017493026360077292\n",
      "Iteration: 5934 lambda_k: 1 Loss: 0.0017493026360077529\n",
      "Iteration: 5935 lambda_k: 1 Loss: 0.0017493026360077741\n",
      "Iteration: 5936 lambda_k: 1 Loss: 0.0017493026360077893\n",
      "Iteration: 5937 lambda_k: 1 Loss: 0.0017493026360078149\n",
      "Iteration: 5938 lambda_k: 1 Loss: 0.001749302636007835\n",
      "Iteration: 5939 lambda_k: 1 Loss: 0.0017493026360078604\n",
      "Iteration: 5940 lambda_k: 1 Loss: 0.0017493026360078739\n",
      "Iteration: 5941 lambda_k: 1 Loss: 0.0017493026360078977\n",
      "Iteration: 5942 lambda_k: 1 Loss: 0.001749302636007916\n",
      "Iteration: 5943 lambda_k: 1 Loss: 0.0017493026360079404\n",
      "Iteration: 5944 lambda_k: 1 Loss: 0.0017493026360079534\n",
      "Iteration: 5945 lambda_k: 1 Loss: 0.0017493026360079775\n",
      "Iteration: 5946 lambda_k: 1 Loss: 0.0017493026360080005\n",
      "Iteration: 5947 lambda_k: 1 Loss: 0.0017493026360080217\n",
      "Iteration: 5948 lambda_k: 1 Loss: 0.0017493026360080439\n",
      "Iteration: 5949 lambda_k: 1 Loss: 0.0017493026360080642\n",
      "Iteration: 5950 lambda_k: 1 Loss: 0.001749302636008086\n",
      "Iteration: 5951 lambda_k: 1 Loss: 0.0017493026360081078\n",
      "Iteration: 5952 lambda_k: 1 Loss: 0.001749302636008123\n",
      "Iteration: 5953 lambda_k: 1 Loss: 0.001749302636008138\n",
      "Iteration: 5954 lambda_k: 1 Loss: 0.0017493026360081605\n",
      "Iteration: 5955 lambda_k: 1 Loss: 0.0017493026360081803\n",
      "Iteration: 5956 lambda_k: 1 Loss: 0.0017493026360082045\n",
      "Iteration: 5957 lambda_k: 1 Loss: 0.0017493026360082295\n",
      "Iteration: 5958 lambda_k: 1 Loss: 0.0017493026360082533\n",
      "Iteration: 5959 lambda_k: 1 Loss: 0.0017493026360082679\n",
      "Iteration: 5960 lambda_k: 1 Loss: 0.0017493026360082863\n",
      "Iteration: 5961 lambda_k: 1 Loss: 0.0017493026360083104\n",
      "Iteration: 5962 lambda_k: 1 Loss: 0.0017493026360083225\n",
      "Iteration: 5963 lambda_k: 1 Loss: 0.0017493026360083438\n",
      "Iteration: 5964 lambda_k: 1 Loss: 0.0017493026360083626\n",
      "Iteration: 5965 lambda_k: 1 Loss: 0.0017493026360083793\n",
      "Iteration: 5966 lambda_k: 1 Loss: 0.0017493026360083919\n",
      "Iteration: 5967 lambda_k: 1 Loss: 0.0017493026360084149\n",
      "Iteration: 5968 lambda_k: 1 Loss: 0.0017493026360084307\n",
      "Iteration: 5969 lambda_k: 1 Loss: 0.0017493026360084433\n",
      "Iteration: 5970 lambda_k: 1 Loss: 0.0017493026360084543\n",
      "Iteration: 5971 lambda_k: 1 Loss: 0.001749302636008467\n",
      "Iteration: 5972 lambda_k: 1 Loss: 0.0017493026360084795\n",
      "Iteration: 5973 lambda_k: 1 Loss: 0.0017493026360085027\n",
      "Iteration: 5974 lambda_k: 1 Loss: 0.0017493026360085148\n",
      "Iteration: 5975 lambda_k: 1 Loss: 0.0017493026360085263\n",
      "Iteration: 5976 lambda_k: 1 Loss: 0.0017493026360085383\n",
      "Iteration: 5977 lambda_k: 1 Loss: 0.0017493026360085498\n",
      "Iteration: 5978 lambda_k: 1 Loss: 0.0017493026360085608\n",
      "Iteration: 5979 lambda_k: 1 Loss: 0.0017493026360085714\n",
      "Iteration: 5980 lambda_k: 1 Loss: 0.0017493026360085827\n",
      "Iteration: 5981 lambda_k: 1 Loss: 0.0017493026360085946\n",
      "Iteration: 5982 lambda_k: 1 Loss: 0.0017493026360086057\n",
      "Iteration: 5983 lambda_k: 1 Loss: 0.0017493026360086176\n",
      "Iteration: 5984 lambda_k: 1 Loss: 0.0017493026360086295\n",
      "Iteration: 5985 lambda_k: 1 Loss: 0.0017493026360086406\n",
      "Iteration: 5986 lambda_k: 1 Loss: 0.0017493026360086554\n",
      "Iteration: 5987 lambda_k: 1 Loss: 0.0017493026360086684\n",
      "Iteration: 5988 lambda_k: 1 Loss: 0.0017493026360086874\n",
      "Iteration: 5989 lambda_k: 1 Loss: 0.0017493026360086996\n",
      "Iteration: 5990 lambda_k: 1 Loss: 0.0017493026360087102\n",
      "Iteration: 5991 lambda_k: 1 Loss: 0.001749302636008722\n",
      "Iteration: 5992 lambda_k: 1 Loss: 0.0017493026360087334\n",
      "Iteration: 5993 lambda_k: 1 Loss: 0.0017493026360087443\n",
      "Iteration: 5994 lambda_k: 1 Loss: 0.0017493026360087547\n",
      "Iteration: 5995 lambda_k: 1 Loss: 0.0017493026360087668\n",
      "Iteration: 5996 lambda_k: 1 Loss: 0.0017493026360087776\n",
      "Iteration: 5997 lambda_k: 1 Loss: 0.0017493026360087887\n",
      "Iteration: 5998 lambda_k: 1 Loss: 0.0017493026360088009\n",
      "Iteration: 5999 lambda_k: 1 Loss: 0.0017493026360088117\n",
      "Iteration: 6000 lambda_k: 1 Loss: 0.0017493026360088228\n",
      "Iteration: 6001 lambda_k: 1 Loss: 0.0017493026360088351\n",
      "Iteration: 6002 lambda_k: 1 Loss: 0.001749302636008846\n",
      "Iteration: 6003 lambda_k: 1 Loss: 0.0017493026360088568\n",
      "Iteration: 6004 lambda_k: 1 Loss: 0.0017493026360088692\n",
      "Iteration: 6005 lambda_k: 1 Loss: 0.00174930263600888\n",
      "Iteration: 6006 lambda_k: 1 Loss: 0.001749302636008891\n",
      "Iteration: 6007 lambda_k: 1 Loss: 0.0017493026360089036\n",
      "Iteration: 6008 lambda_k: 1 Loss: 0.0017493026360089143\n",
      "Iteration: 6009 lambda_k: 1 Loss: 0.0017493026360089255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6010 lambda_k: 1 Loss: 0.0017493026360089375\n",
      "Iteration: 6011 lambda_k: 1 Loss: 0.0017493026360089596\n",
      "Iteration: 6012 lambda_k: 1 Loss: 0.0017493026360089704\n",
      "Iteration: 6013 lambda_k: 1 Loss: 0.0017493026360089823\n",
      "Iteration: 6014 lambda_k: 1 Loss: 0.001749302636008994\n",
      "Iteration: 6015 lambda_k: 1 Loss: 0.0017493026360090053\n",
      "Iteration: 6016 lambda_k: 1 Loss: 0.0017493026360090216\n",
      "Iteration: 6017 lambda_k: 1 Loss: 0.0017493026360090342\n",
      "Iteration: 6018 lambda_k: 1 Loss: 0.0017493026360090454\n",
      "Iteration: 6019 lambda_k: 1 Loss: 0.0017493026360090565\n",
      "Iteration: 6020 lambda_k: 1 Loss: 0.0017493026360090645\n",
      "Iteration: 6021 lambda_k: 1 Loss: 0.001749302636009076\n",
      "Iteration: 6022 lambda_k: 1 Loss: 0.001749302636009095\n",
      "Iteration: 6023 lambda_k: 1 Loss: 0.001749302636009109\n",
      "Iteration: 6024 lambda_k: 1 Loss: 0.0017493026360091211\n",
      "Iteration: 6025 lambda_k: 1 Loss: 0.001749302636009132\n",
      "Iteration: 6026 lambda_k: 1 Loss: 0.0017493026360091478\n",
      "Iteration: 6027 lambda_k: 1 Loss: 0.0017493026360091593\n",
      "Iteration: 6028 lambda_k: 1 Loss: 0.0017493026360091708\n",
      "Iteration: 6029 lambda_k: 1 Loss: 0.0017493026360091825\n",
      "Iteration: 6030 lambda_k: 1 Loss: 0.0017493026360091931\n",
      "Iteration: 6031 lambda_k: 1 Loss: 0.0017493026360092014\n",
      "Iteration: 6032 lambda_k: 1 Loss: 0.0017493026360092233\n",
      "Iteration: 6033 lambda_k: 1 Loss: 0.0017493026360092343\n",
      "Iteration: 6034 lambda_k: 1 Loss: 0.0017493026360092445\n",
      "Iteration: 6035 lambda_k: 1 Loss: 0.0017493026360092606\n",
      "Iteration: 6036 lambda_k: 1 Loss: 0.0017493026360092755\n",
      "Iteration: 6037 lambda_k: 1 Loss: 0.0017493026360092922\n",
      "Iteration: 6038 lambda_k: 1 Loss: 0.001749302636009303\n",
      "Iteration: 6039 lambda_k: 1 Loss: 0.0017493026360093156\n",
      "Iteration: 6040 lambda_k: 1 Loss: 0.001749302636009331\n",
      "Iteration: 6041 lambda_k: 1 Loss: 0.0017493026360093453\n",
      "Iteration: 6042 lambda_k: 1 Loss: 0.0017493026360093594\n",
      "Iteration: 6043 lambda_k: 1 Loss: 0.0017493026360093679\n",
      "Iteration: 6044 lambda_k: 1 Loss: 0.0017493026360093785\n",
      "Iteration: 6045 lambda_k: 1 Loss: 0.0017493026360093842\n",
      "Iteration: 6046 lambda_k: 1 Loss: 0.0017493026360093948\n",
      "Iteration: 6047 lambda_k: 1 Loss: 0.0017493026360093995\n",
      "Iteration: 6048 lambda_k: 1 Loss: 0.00174930263600941\n",
      "Iteration: 6049 lambda_k: 1 Loss: 0.001749302636009419\n",
      "Iteration: 6050 lambda_k: 1 Loss: 0.0017493026360094306\n",
      "Iteration: 6051 lambda_k: 1 Loss: 0.0017493026360094403\n",
      "Iteration: 6052 lambda_k: 1 Loss: 0.0017493026360094447\n",
      "Iteration: 6053 lambda_k: 1 Loss: 0.0017493026360094544\n",
      "Iteration: 6054 lambda_k: 1 Loss: 0.0017493026360094661\n",
      "Iteration: 6055 lambda_k: 1 Loss: 0.0017493026360094774\n",
      "Iteration: 6056 lambda_k: 1 Loss: 0.0017493026360094889\n",
      "Iteration: 6057 lambda_k: 1 Loss: 0.001749302636009499\n",
      "Iteration: 6058 lambda_k: 1 Loss: 0.0017493026360095023\n",
      "Iteration: 6059 lambda_k: 1 Loss: 0.0017493026360095136\n",
      "Iteration: 6060 lambda_k: 1 Loss: 0.001749302636009525\n",
      "Iteration: 6061 lambda_k: 1 Loss: 0.0017493026360095364\n",
      "Iteration: 6062 lambda_k: 1 Loss: 0.001749302636009547\n",
      "Iteration: 6063 lambda_k: 1 Loss: 0.0017493026360095609\n",
      "Iteration: 6064 lambda_k: 1 Loss: 0.0017493026360095717\n",
      "Iteration: 6065 lambda_k: 1 Loss: 0.001749302636009583\n",
      "Iteration: 6066 lambda_k: 1 Loss: 0.0017493026360095934\n",
      "Iteration: 6067 lambda_k: 1 Loss: 0.0017493026360096045\n",
      "Iteration: 6068 lambda_k: 1 Loss: 0.0017493026360096155\n",
      "Iteration: 6069 lambda_k: 1 Loss: 0.0017493026360096255\n",
      "Iteration: 6070 lambda_k: 1 Loss: 0.0017493026360096287\n",
      "Iteration: 6071 lambda_k: 1 Loss: 0.001749302636009638\n",
      "Iteration: 6072 lambda_k: 1 Loss: 0.0017493026360096413\n",
      "Iteration: 6073 lambda_k: 1 Loss: 0.001749302636009652\n",
      "Iteration: 6074 lambda_k: 1 Loss: 0.0017493026360096626\n",
      "Iteration: 6075 lambda_k: 1 Loss: 0.0017493026360096743\n",
      "Iteration: 6076 lambda_k: 1 Loss: 0.0017493026360096845\n",
      "Iteration: 6077 lambda_k: 1 Loss: 0.001749302636009696\n",
      "Iteration: 6078 lambda_k: 1 Loss: 0.0017493026360097072\n",
      "Iteration: 6079 lambda_k: 1 Loss: 0.0017493026360097192\n",
      "Iteration: 6080 lambda_k: 1 Loss: 0.0017493026360097294\n",
      "Iteration: 6081 lambda_k: 1 Loss: 0.001749302636009741\n",
      "Iteration: 6082 lambda_k: 1 Loss: 0.0017493026360097528\n",
      "Iteration: 6083 lambda_k: 1 Loss: 0.001749302636009764\n",
      "Iteration: 6084 lambda_k: 1 Loss: 0.001749302636009775\n",
      "Iteration: 6085 lambda_k: 1 Loss: 0.0017493026360097868\n",
      "Iteration: 6086 lambda_k: 1 Loss: 0.001749302636009798\n",
      "Iteration: 6087 lambda_k: 1 Loss: 0.0017493026360098096\n",
      "Iteration: 6088 lambda_k: 1 Loss: 0.001749302636009821\n",
      "Iteration: 6089 lambda_k: 1 Loss: 0.0017493026360098326\n",
      "Iteration: 6090 lambda_k: 1 Loss: 0.0017493026360098423\n",
      "Iteration: 6091 lambda_k: 1 Loss: 0.0017493026360098538\n",
      "Iteration: 6092 lambda_k: 1 Loss: 0.0017493026360098634\n",
      "Iteration: 6093 lambda_k: 1 Loss: 0.0017493026360098668\n",
      "Iteration: 6094 lambda_k: 1 Loss: 0.0017493026360098775\n",
      "Iteration: 6095 lambda_k: 1 Loss: 0.0017493026360098894\n",
      "Iteration: 6096 lambda_k: 1 Loss: 0.0017493026360099007\n",
      "Iteration: 6097 lambda_k: 1 Loss: 0.0017493026360099126\n",
      "Iteration: 6098 lambda_k: 1 Loss: 0.0017493026360099247\n",
      "Iteration: 6099 lambda_k: 1 Loss: 0.0017493026360099358\n",
      "Iteration: 6100 lambda_k: 1 Loss: 0.0017493026360099469\n",
      "Iteration: 6101 lambda_k: 1 Loss: 0.0017493026360099577\n",
      "Iteration: 6102 lambda_k: 1 Loss: 0.0017493026360099688\n",
      "Iteration: 6103 lambda_k: 1 Loss: 0.0017493026360099802\n",
      "Iteration: 6104 lambda_k: 1 Loss: 0.0017493026360099907\n",
      "Iteration: 6105 lambda_k: 1 Loss: 0.001749302636010005\n",
      "Iteration: 6106 lambda_k: 1 Loss: 0.001749302636010007\n",
      "Iteration: 6107 lambda_k: 1 Loss: 0.0017493026360100173\n",
      "Iteration: 6108 lambda_k: 1 Loss: 0.001749302636010028\n",
      "Iteration: 6109 lambda_k: 1 Loss: 0.0017493026360100386\n",
      "Iteration: 6110 lambda_k: 1 Loss: 0.0017493026360100492\n",
      "Iteration: 6111 lambda_k: 1 Loss: 0.001749302636010061\n",
      "Iteration: 6112 lambda_k: 1 Loss: 0.001749302636010071\n",
      "Iteration: 6113 lambda_k: 1 Loss: 0.0017493026360100826\n",
      "Iteration: 6114 lambda_k: 1 Loss: 0.0017493026360100943\n",
      "Iteration: 6115 lambda_k: 1 Loss: 0.0017493026360101045\n",
      "Iteration: 6116 lambda_k: 1 Loss: 0.0017493026360101186\n",
      "Iteration: 6117 lambda_k: 1 Loss: 0.0017493026360101208\n",
      "Iteration: 6118 lambda_k: 1 Loss: 0.0017493026360101314\n",
      "Iteration: 6119 lambda_k: 1 Loss: 0.0017493026360101422\n",
      "Iteration: 6120 lambda_k: 1 Loss: 0.0017493026360101524\n",
      "Iteration: 6121 lambda_k: 1 Loss: 0.0017493026360101563\n",
      "Iteration: 6122 lambda_k: 1 Loss: 0.001749302636010166\n",
      "Iteration: 6123 lambda_k: 1 Loss: 0.0017493026360101771\n",
      "Iteration: 6124 lambda_k: 1 Loss: 0.001749302636010187\n",
      "Iteration: 6125 lambda_k: 1 Loss: 0.0017493026360101936\n",
      "Iteration: 6126 lambda_k: 1 Loss: 0.0017493026360102042\n",
      "Iteration: 6127 lambda_k: 1 Loss: 0.0017493026360102192\n",
      "Iteration: 6128 lambda_k: 1 Loss: 0.0017493026360102318\n",
      "Iteration: 6129 lambda_k: 1 Loss: 0.0017493026360102346\n",
      "Iteration: 6130 lambda_k: 1 Loss: 0.0017493026360102452\n",
      "Iteration: 6131 lambda_k: 1 Loss: 0.0017493026360102559\n",
      "Iteration: 6132 lambda_k: 1 Loss: 0.0017493026360102578\n",
      "Iteration: 6133 lambda_k: 1 Loss: 0.0017493026360102708\n",
      "Iteration: 6134 lambda_k: 1 Loss: 0.0017493026360102723\n",
      "Iteration: 6135 lambda_k: 1 Loss: 0.0017493026360102743\n",
      "Iteration: 6136 lambda_k: 1 Loss: 0.0017493026360102754\n",
      "Iteration: 6137 lambda_k: 1 Loss: 0.0017493026360102758\n",
      "Iteration: 6138 lambda_k: 1 Loss: 0.0017493026360102758\n",
      "Iteration: 6139 lambda_k: 1 Loss: 0.0017493026360102756\n",
      "Iteration: 6140 lambda_k: 1 Loss: 0.0017493026360102756\n",
      "Iteration: 6141 lambda_k: 1 Loss: 0.0017493026360102756\n",
      "Iteration: 6142 lambda_k: 1 Loss: 0.0017493026360102756\n",
      "Iteration: 6143 lambda_k: 1 Loss: 0.0017493026360102758\n",
      "Iteration: 6144 lambda_k: 1 Loss: 0.0017493026360102754\n",
      "Iteration: 6145 lambda_k: 1 Loss: 0.0017493026360102754\n",
      "Iteration: 6146 lambda_k: 1 Loss: 0.0017493026360102754\n",
      "Iteration: 6147 lambda_k: 1 Loss: 0.0017493026360102754\n",
      "Iteration: 6148 lambda_k: 1 Loss: 0.0017493026360102758\n",
      "Iteration: 6149 lambda_k: 1 Loss: 0.0017493026360102758\n",
      "Iteration: 6150 lambda_k: 1 Loss: 0.0017493026360102782\n",
      "Iteration: 6151 lambda_k: 1 Loss: 0.0017493026360102795\n",
      "Iteration: 6152 lambda_k: 1 Loss: 0.0017493026360102786\n",
      "Iteration: 6153 lambda_k: 1 Loss: 0.0017493026360102786\n",
      "Iteration: 6154 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6155 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6156 lambda_k: 1 Loss: 0.0017493026360102767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6157 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6158 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6159 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6160 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6161 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6162 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6163 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6164 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6165 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6166 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6167 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6168 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6169 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6170 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6171 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6172 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6173 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6174 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6175 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6176 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6177 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6178 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6179 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6180 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6181 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6182 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6183 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6184 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6185 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6186 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6187 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6188 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6189 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6190 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6191 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6192 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6193 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6194 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6195 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6196 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6197 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6198 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6199 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6200 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6201 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6202 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6203 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6204 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6205 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6206 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6207 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6208 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6209 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6210 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6211 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6212 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6213 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6214 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6215 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6216 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6217 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6218 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6219 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6220 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6221 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6222 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6223 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6224 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6225 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6226 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6227 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6228 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6229 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6230 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6231 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6232 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6233 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6234 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6235 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6236 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6237 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6238 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6239 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6240 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6241 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6242 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6243 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6244 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6245 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6246 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6247 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6248 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6249 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6250 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6251 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6252 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6253 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6254 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6255 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6256 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6257 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6258 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6259 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6260 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6261 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6262 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6263 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6264 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6265 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6266 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6267 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6268 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6269 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6270 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6271 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6272 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6273 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6274 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6275 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6276 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6277 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6278 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6279 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6280 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6281 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6282 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6283 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6284 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6285 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6286 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6287 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6288 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6289 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6290 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6291 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6292 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6293 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6294 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6295 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6296 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6297 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6298 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6299 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6300 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6301 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6302 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6303 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6304 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6305 lambda_k: 1 Loss: 0.001749302636010277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6306 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6307 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6308 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6309 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6310 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6311 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6312 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6313 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6314 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6315 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6316 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6317 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6318 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6319 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6320 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6321 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6322 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6323 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6324 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6325 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6326 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6327 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6328 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6329 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6330 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6331 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6332 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6333 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6334 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6335 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6336 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6337 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6338 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6339 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6340 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6341 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6342 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6343 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6344 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6345 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6346 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6347 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6348 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6349 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6350 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6351 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6352 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6353 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6354 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6355 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6356 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6357 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6358 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6359 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6360 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6361 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6362 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6363 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6364 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6365 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6366 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6367 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6368 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6369 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6370 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6371 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6372 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6373 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6374 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6375 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6376 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6377 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6378 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6379 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6380 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6381 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6382 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6383 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6384 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6385 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6386 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6387 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6388 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6389 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6390 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6391 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6392 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6393 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6394 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6395 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6396 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6397 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6398 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6399 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6400 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6401 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6402 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6403 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6404 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6405 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6406 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6407 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6408 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6409 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6410 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6411 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6412 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6413 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6414 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6415 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6416 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6417 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6418 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6419 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6420 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6421 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6422 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6423 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6424 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6425 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6426 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6427 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6428 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6429 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6430 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6431 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6432 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6433 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6434 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6435 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6436 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6437 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6438 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6439 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6440 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6441 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6442 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6443 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6444 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6445 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6446 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6447 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6448 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6449 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6450 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6451 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6452 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6453 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6454 lambda_k: 1 Loss: 0.001749302636010277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6455 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6456 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6457 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6458 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6459 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6460 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6461 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6462 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6463 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6464 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6465 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6466 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6467 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6468 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6469 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6470 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6471 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6472 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6473 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6474 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6475 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6476 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6477 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6478 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6479 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6480 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6481 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6482 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6483 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6484 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6485 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6486 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6487 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6488 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6489 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6490 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6491 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6492 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6493 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6494 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6495 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6496 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6497 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6498 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6499 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6500 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6501 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6502 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6503 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6504 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6505 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6506 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6507 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6508 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6509 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6510 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6511 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6512 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6513 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6514 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6515 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6516 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6517 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6518 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6519 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6520 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6521 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6522 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6523 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6524 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6525 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6526 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6527 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6528 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6529 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6530 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6531 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6532 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6533 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6534 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6535 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6536 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6537 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6538 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6539 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6540 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6541 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6542 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6543 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6544 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6545 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6546 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6547 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6548 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6549 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6550 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6551 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6552 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6553 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6554 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6555 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6556 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6557 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6558 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6559 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6560 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6561 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6562 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6563 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6564 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6565 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6566 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6567 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6568 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6569 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6570 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6571 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6572 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6573 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6574 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6575 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6576 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6577 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6578 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6579 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6580 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6581 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6582 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6583 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6584 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6585 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6586 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6587 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6588 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6589 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6590 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6591 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6592 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6593 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6594 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6595 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6596 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6597 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6598 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6599 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6600 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6601 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6602 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6603 lambda_k: 1 Loss: 0.001749302636010277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6604 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6605 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6606 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6607 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6608 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6609 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6610 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6611 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6612 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6613 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6614 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6615 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6616 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6617 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6618 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6619 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6620 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6621 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6622 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6623 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6624 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6625 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6626 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6627 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6628 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6629 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6630 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6631 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6632 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6633 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6634 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6635 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6636 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6637 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6638 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6639 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6640 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6641 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6642 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6643 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6644 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6645 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6646 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6647 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6648 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6649 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6650 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6651 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6652 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6653 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6654 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6655 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6656 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6657 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6658 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6659 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6660 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6661 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6662 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6663 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6664 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6665 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6666 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6667 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6668 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6669 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6670 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6671 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6672 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6673 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6674 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6675 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6676 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6677 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6678 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6679 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6680 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6681 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6682 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6683 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6684 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6685 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6686 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6687 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6688 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6689 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6690 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6691 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6692 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6693 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6694 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6695 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6696 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6697 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6698 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6699 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6700 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6701 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6702 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6703 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6704 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6705 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6706 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6707 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6708 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6709 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6710 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6711 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6712 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6713 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6714 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6715 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6716 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6717 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6718 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6719 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6720 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6721 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6722 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6723 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6724 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6725 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6726 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6727 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6728 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6729 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6730 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6731 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6732 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6733 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6734 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6735 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6736 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6737 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6738 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6739 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6740 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6741 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6742 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6743 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6744 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6745 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6746 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6747 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6748 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6749 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6750 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6751 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6752 lambda_k: 1 Loss: 0.001749302636010277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6753 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6754 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6755 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6756 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6757 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6758 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6759 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6760 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6761 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6762 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6763 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6764 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6765 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6766 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6767 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6768 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6769 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6770 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6771 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6772 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6773 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6774 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6775 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6776 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6777 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6778 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6779 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6780 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6781 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6782 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6783 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6784 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6785 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6786 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6787 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6788 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6789 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6790 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6791 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6792 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6793 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6794 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6795 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6796 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6797 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6798 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6799 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6800 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6801 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6802 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6803 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6804 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6805 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6806 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6807 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6808 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6809 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6810 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6811 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6812 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6813 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6814 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6815 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6816 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6817 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6818 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6819 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6820 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6821 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6822 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6823 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6824 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6825 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6826 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6827 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6828 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6829 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6830 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6831 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6832 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6833 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6834 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6835 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6836 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6837 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6838 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6839 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6840 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6841 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6842 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6843 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6844 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6845 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6846 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6847 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6848 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6849 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6850 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6851 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6852 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6853 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6854 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6855 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6856 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6857 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6858 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6859 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6860 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6861 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6862 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6863 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6864 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6865 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6866 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6867 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6868 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6869 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6870 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6871 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6872 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6873 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6874 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6875 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6876 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6877 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6878 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6879 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6880 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6881 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6882 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6883 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6884 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6885 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6886 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6887 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6888 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6889 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6890 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6891 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6892 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6893 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6894 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6895 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6896 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6897 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6898 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6899 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6900 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6901 lambda_k: 1 Loss: 0.0017493026360102767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6902 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6903 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6904 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6905 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6906 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6907 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6908 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6909 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6910 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6911 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6912 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6913 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6914 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6915 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6916 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6917 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6918 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6919 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6920 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6921 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6922 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6923 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6924 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6925 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6926 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6927 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6928 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6929 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6930 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6931 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6932 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6933 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6934 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6935 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6936 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6937 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6938 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6939 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6940 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6941 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6942 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6943 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6944 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6945 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6946 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6947 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6948 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6949 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6950 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6951 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6952 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6953 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6954 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6955 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6956 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6957 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6958 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6959 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6960 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6961 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6962 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6963 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6964 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6965 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6966 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6967 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6968 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6969 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6970 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6971 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6972 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6973 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6974 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6975 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6976 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6977 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6978 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6979 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6980 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6981 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6982 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6983 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6984 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6985 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6986 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6987 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6988 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6989 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6990 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6991 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6992 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6993 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6994 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6995 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6996 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 6997 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6998 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 6999 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7000 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7001 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7002 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7003 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7004 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7005 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7006 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7007 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7008 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7009 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7010 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7011 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7012 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7013 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7014 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7015 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7016 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7017 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7018 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7019 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7020 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7021 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7022 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7023 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7024 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7025 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7026 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7027 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7028 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7029 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7030 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7031 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7032 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7033 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7034 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7035 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7036 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7037 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7038 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7039 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7040 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7041 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7042 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7043 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7044 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7045 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7046 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7047 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7048 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7049 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7050 lambda_k: 1 Loss: 0.001749302636010277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7051 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7052 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7053 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7054 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7055 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7056 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7057 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7058 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7059 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7060 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7061 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7062 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7063 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7064 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7065 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7066 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7067 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7068 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7069 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7070 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7071 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7072 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7073 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7074 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7075 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7076 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7077 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7078 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7079 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7080 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7081 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7082 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7083 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7084 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7085 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7086 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7087 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7088 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7089 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7090 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7091 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7092 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7093 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7094 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7095 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7096 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7097 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7098 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7099 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7100 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7101 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7102 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7103 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7104 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7105 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7106 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7107 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7108 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7109 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7110 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7111 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7112 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7113 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7114 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7115 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7116 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7117 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7118 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7119 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7120 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7121 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7122 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7123 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7124 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7125 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7126 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7127 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7128 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7129 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7130 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7131 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7132 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7133 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7134 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7135 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7136 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7137 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7138 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7139 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7140 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7141 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7142 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7143 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7144 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7145 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7146 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7147 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7148 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7149 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7150 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7151 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7152 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7153 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7154 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7155 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7156 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7157 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7158 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7159 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7160 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7161 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7162 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7163 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7164 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7165 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7166 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7167 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7168 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7169 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7170 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7171 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7172 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7173 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7174 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7175 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7176 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7177 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7178 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7179 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7180 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7181 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7182 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7183 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7184 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7185 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7186 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7187 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7188 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7189 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7190 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7191 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7192 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7193 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7194 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7195 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7196 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7197 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7198 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7199 lambda_k: 1 Loss: 0.001749302636010277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7200 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7201 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7202 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7203 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7204 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7205 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7206 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7207 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7208 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7209 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7210 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7211 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7212 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7213 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7214 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7215 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7216 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7217 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7218 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7219 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7220 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7221 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7222 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7223 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7224 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7225 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7226 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7227 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7228 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7229 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7230 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7231 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7232 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7233 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7234 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7235 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7236 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7237 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7238 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7239 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7240 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7241 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7242 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7243 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7244 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7245 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7246 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7247 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7248 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7249 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7250 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7251 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7252 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7253 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7254 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7255 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7256 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7257 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7258 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7259 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7260 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7261 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7262 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7263 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7264 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7265 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7266 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7267 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7268 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7269 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7270 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7271 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7272 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7273 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7274 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7275 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7276 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7277 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7278 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7279 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7280 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7281 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7282 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7283 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7284 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7285 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7286 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7287 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7288 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7289 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7290 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7291 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7292 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7293 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7294 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7295 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7296 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7297 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7298 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7299 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7300 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7301 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7302 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7303 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7304 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7305 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7306 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7307 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7308 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7309 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7310 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7311 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7312 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7313 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7314 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7315 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7316 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7317 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7318 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7319 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7320 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7321 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7322 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7323 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7324 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7325 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7326 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7327 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7328 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7329 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7330 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7331 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7332 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7333 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7334 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7335 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7336 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7337 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7338 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7339 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7340 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7341 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7342 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7343 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7344 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7345 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7346 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7347 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7348 lambda_k: 1 Loss: 0.001749302636010277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7349 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7350 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7351 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7352 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7353 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7354 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7355 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7356 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7357 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7358 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7359 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7360 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7361 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7362 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7363 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7364 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7365 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7366 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7367 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7368 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7369 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7370 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7371 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7372 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7373 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7374 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7375 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7376 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7377 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7378 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7379 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7380 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7381 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7382 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7383 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7384 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7385 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7386 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7387 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7388 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7389 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7390 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7391 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7392 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7393 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7394 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7395 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7396 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7397 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7398 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7399 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7400 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7401 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7402 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7403 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7404 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7405 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7406 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7407 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7408 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7409 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7410 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7411 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7412 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7413 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7414 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7415 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7416 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7417 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7418 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7419 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7420 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7421 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7422 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7423 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7424 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7425 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7426 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7427 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7428 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7429 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7430 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7431 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7432 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7433 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7434 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7435 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7436 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7437 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7438 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7439 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7440 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7441 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7442 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7443 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7444 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7445 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7446 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7447 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7448 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7449 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7450 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7451 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7452 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7453 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7454 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7455 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7456 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7457 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7458 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7459 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7460 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7461 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7462 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7463 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7464 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7465 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7466 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7467 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7468 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7469 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7470 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7471 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7472 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7473 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7474 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7475 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7476 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7477 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7478 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7479 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7480 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7481 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7482 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7483 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7484 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7485 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7486 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7487 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7488 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7489 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7490 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7491 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7492 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7493 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7494 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7495 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7496 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7497 lambda_k: 1 Loss: 0.001749302636010277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7498 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7499 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7500 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7501 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7502 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7503 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7504 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7505 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7506 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7507 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7508 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7509 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7510 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7511 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7512 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7513 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7514 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7515 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7516 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7517 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7518 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7519 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7520 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7521 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7522 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7523 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7524 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7525 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7526 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7527 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7528 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7529 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7530 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7531 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7532 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7533 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7534 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7535 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7536 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7537 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7538 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7539 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7540 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7541 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7542 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7543 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7544 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7545 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7546 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7547 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7548 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7549 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7550 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7551 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7552 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7553 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7554 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7555 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7556 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7557 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7558 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7559 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7560 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7561 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7562 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7563 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7564 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7565 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7566 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7567 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7568 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7569 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7570 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7571 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7572 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7573 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7574 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7575 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7576 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7577 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7578 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7579 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7580 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7581 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7582 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7583 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7584 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7585 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7586 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7587 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7588 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7589 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7590 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7591 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7592 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7593 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7594 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7595 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7596 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7597 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7598 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7599 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7600 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7601 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7602 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7603 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7604 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7605 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7606 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7607 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7608 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7609 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7610 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7611 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7612 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7613 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7614 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7615 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7616 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7617 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7618 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7619 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7620 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7621 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7622 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7623 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7624 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7625 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7626 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7627 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7628 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7629 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7630 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7631 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7632 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7633 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7634 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7635 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7636 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7637 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7638 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7639 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7640 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7641 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7642 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7643 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7644 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7645 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7646 lambda_k: 1 Loss: 0.0017493026360102767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7647 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7648 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7649 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7650 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7651 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7652 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7653 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7654 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7655 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7656 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7657 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7658 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7659 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7660 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7661 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7662 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7663 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7664 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7665 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7666 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7667 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7668 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7669 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7670 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7671 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7672 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7673 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7674 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7675 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7676 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7677 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7678 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7679 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7680 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7681 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7682 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7683 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7684 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7685 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7686 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7687 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7688 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7689 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7690 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7691 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7692 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7693 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7694 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7695 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7696 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7697 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7698 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7699 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7700 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7701 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7702 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7703 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7704 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7705 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7706 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7707 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7708 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7709 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7710 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7711 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7712 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7713 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7714 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7715 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7716 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7717 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7718 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7719 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7720 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7721 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7722 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7723 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7724 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7725 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7726 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7727 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7728 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7729 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7730 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7731 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7732 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7733 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7734 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7735 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7736 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7737 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7738 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7739 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7740 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7741 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7742 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7743 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7744 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7745 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7746 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7747 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7748 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7749 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7750 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7751 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7752 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7753 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7754 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7755 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7756 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7757 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7758 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7759 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7760 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7761 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7762 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7763 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7764 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7765 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7766 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7767 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7768 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7769 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7770 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7771 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7772 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7773 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7774 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7775 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7776 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7777 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7778 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7779 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7780 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7781 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7782 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7783 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7784 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7785 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7786 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7787 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7788 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7789 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7790 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7791 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7792 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7793 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7794 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7795 lambda_k: 1 Loss: 0.001749302636010277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7796 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7797 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7798 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7799 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7800 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7801 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7802 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7803 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7804 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7805 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7806 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7807 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7808 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7809 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7810 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7811 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7812 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7813 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7814 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7815 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7816 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7817 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7818 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7819 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7820 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7821 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7822 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7823 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7824 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7825 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7826 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7827 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7828 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7829 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7830 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7831 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7832 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7833 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7834 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7835 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7836 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7837 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7838 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7839 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7840 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7841 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7842 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7843 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7844 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7845 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7846 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7847 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7848 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7849 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7850 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7851 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7852 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7853 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7854 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7855 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7856 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7857 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7858 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7859 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7860 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7861 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7862 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7863 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7864 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7865 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7866 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7867 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7868 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7869 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7870 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7871 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7872 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7873 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7874 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7875 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7876 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7877 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7878 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7879 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7880 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7881 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7882 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7883 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7884 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7885 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7886 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7887 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7888 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7889 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7890 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7891 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7892 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7893 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7894 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7895 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7896 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7897 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7898 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7899 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7900 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7901 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7902 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7903 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7904 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7905 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7906 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7907 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7908 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7909 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7910 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7911 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7912 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7913 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7914 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7915 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7916 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7917 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7918 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7919 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7920 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7921 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7922 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7923 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7924 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7925 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7926 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7927 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7928 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7929 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7930 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7931 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7932 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7933 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7934 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7935 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7936 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7937 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7938 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7939 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7940 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7941 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7942 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7943 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7944 lambda_k: 1 Loss: 0.001749302636010277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7945 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7946 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7947 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7948 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7949 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7950 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7951 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7952 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7953 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7954 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7955 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7956 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7957 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7958 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7959 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7960 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7961 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7962 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7963 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7964 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7965 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7966 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7967 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7968 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7969 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7970 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7971 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7972 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7973 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7974 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7975 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7976 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7977 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7978 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7979 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7980 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7981 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7982 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7983 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7984 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7985 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7986 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7987 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7988 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7989 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7990 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7991 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7992 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7993 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7994 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7995 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7996 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 7997 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7998 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 7999 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8000 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8001 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8002 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8003 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8004 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8005 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8006 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8007 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8008 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8009 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8010 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8011 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8012 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8013 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8014 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8015 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8016 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8017 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8018 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8019 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8020 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8021 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8022 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8023 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8024 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8025 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8026 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8027 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8028 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8029 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8030 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8031 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8032 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8033 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8034 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8035 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8036 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8037 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8038 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8039 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8040 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8041 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8042 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8043 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8044 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8045 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8046 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8047 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8048 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8049 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8050 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8051 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8052 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8053 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8054 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8055 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8056 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8057 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8058 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8059 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8060 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8061 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8062 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8063 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8064 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8065 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8066 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8067 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8068 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8069 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8070 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8071 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8072 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8073 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8074 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8075 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8076 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8077 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8078 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8079 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8080 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8081 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8082 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8083 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8084 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8085 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8086 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8087 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8088 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8089 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8090 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8091 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8092 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8093 lambda_k: 1 Loss: 0.001749302636010277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8094 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8095 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8096 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8097 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8098 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8099 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8100 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8101 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8102 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8103 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8104 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8105 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8106 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8107 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8108 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8109 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8110 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8111 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8112 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8113 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8114 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8115 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8116 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8117 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8118 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8119 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8120 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8121 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8122 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8123 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8124 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8125 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8126 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8127 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8128 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8129 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8130 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8131 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8132 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8133 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8134 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8135 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8136 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8137 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8138 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8139 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8140 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8141 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8142 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8143 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8144 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8145 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8146 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8147 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8148 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8149 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8150 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8151 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8152 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8153 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8154 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8155 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8156 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8157 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8158 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8159 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8160 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8161 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8162 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8163 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8164 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8165 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8166 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8167 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8168 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8169 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8170 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8171 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8172 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8173 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8174 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8175 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8176 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8177 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8178 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8179 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8180 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8181 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8182 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8183 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8184 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8185 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8186 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8187 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8188 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8189 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8190 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8191 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8192 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8193 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8194 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8195 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8196 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8197 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8198 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8199 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8200 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8201 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8202 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8203 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8204 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8205 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8206 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8207 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8208 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8209 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8210 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8211 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8212 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8213 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8214 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8215 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8216 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8217 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8218 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8219 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8220 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8221 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8222 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8223 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8224 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8225 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8226 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8227 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8228 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8229 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8230 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8231 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8232 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8233 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8234 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8235 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8236 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8237 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8238 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8239 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8240 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8241 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8242 lambda_k: 1 Loss: 0.001749302636010277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8243 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8244 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8245 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8246 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8247 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8248 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8249 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8250 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8251 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8252 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8253 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8254 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8255 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8256 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8257 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8258 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8259 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8260 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8261 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8262 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8263 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8264 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8265 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8266 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8267 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8268 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8269 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8270 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8271 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8272 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8273 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8274 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8275 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8276 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8277 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8278 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8279 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8280 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8281 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8282 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8283 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8284 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8285 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8286 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8287 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8288 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8289 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8290 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8291 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8292 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8293 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8294 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8295 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8296 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8297 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8298 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8299 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8300 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8301 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8302 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8303 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8304 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8305 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8306 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8307 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8308 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8309 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8310 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8311 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8312 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8313 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8314 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8315 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8316 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8317 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8318 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8319 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8320 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8321 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8322 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8323 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8324 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8325 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8326 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8327 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8328 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8329 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8330 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8331 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8332 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8333 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8334 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8335 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8336 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8337 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8338 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8339 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8340 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8341 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8342 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8343 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8344 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8345 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8346 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8347 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8348 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8349 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8350 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8351 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8352 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8353 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8354 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8355 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8356 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8357 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8358 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8359 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8360 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8361 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8362 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8363 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8364 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8365 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8366 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8367 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8368 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8369 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8370 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8371 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8372 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8373 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8374 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8375 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8376 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8377 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8378 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8379 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8380 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8381 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8382 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8383 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8384 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8385 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8386 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8387 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8388 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8389 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8390 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8391 lambda_k: 1 Loss: 0.0017493026360102767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8392 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8393 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8394 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8395 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8396 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8397 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8398 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8399 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8400 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8401 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8402 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8403 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8404 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8405 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8406 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8407 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8408 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8409 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8410 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8411 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8412 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8413 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8414 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8415 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8416 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8417 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8418 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8419 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8420 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8421 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8422 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8423 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8424 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8425 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8426 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8427 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8428 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8429 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8430 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8431 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8432 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8433 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8434 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8435 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8436 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8437 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8438 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8439 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8440 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8441 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8442 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8443 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8444 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8445 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8446 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8447 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8448 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8449 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8450 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8451 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8452 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8453 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8454 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8455 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8456 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8457 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8458 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8459 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8460 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8461 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8462 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8463 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8464 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8465 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8466 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8467 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8468 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8469 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8470 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8471 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8472 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8473 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8474 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8475 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8476 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8477 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8478 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8479 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8480 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8481 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8482 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8483 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8484 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8485 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8486 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8487 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8488 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8489 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8490 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8491 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8492 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8493 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8494 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8495 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8496 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8497 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8498 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8499 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8500 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8501 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8502 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8503 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8504 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8505 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8506 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8507 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8508 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8509 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8510 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8511 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8512 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8513 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8514 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8515 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8516 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8517 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8518 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8519 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8520 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8521 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8522 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8523 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8524 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8525 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8526 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8527 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8528 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8529 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8530 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8531 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8532 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8533 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8534 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8535 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8536 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8537 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8538 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8539 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8540 lambda_k: 1 Loss: 0.001749302636010277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8541 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8542 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8543 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8544 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8545 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8546 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8547 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8548 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8549 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8550 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8551 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8552 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8553 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8554 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8555 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8556 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8557 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8558 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8559 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8560 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8561 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8562 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8563 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8564 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8565 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8566 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8567 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8568 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8569 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8570 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8571 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8572 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8573 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8574 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8575 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8576 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8577 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8578 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8579 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8580 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8581 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8582 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8583 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8584 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8585 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8586 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8587 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8588 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8589 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8590 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8591 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8592 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8593 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8594 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8595 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8596 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8597 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8598 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8599 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8600 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8601 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8602 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8603 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8604 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8605 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8606 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8607 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8608 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8609 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8610 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8611 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8612 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8613 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8614 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8615 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8616 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8617 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8618 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8619 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8620 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8621 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8622 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8623 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8624 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8625 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8626 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8627 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8628 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8629 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8630 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8631 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8632 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8633 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8634 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8635 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8636 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8637 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8638 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8639 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8640 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8641 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8642 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8643 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8644 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8645 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8646 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8647 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8648 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8649 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8650 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8651 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8652 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8653 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8654 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8655 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8656 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8657 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8658 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8659 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8660 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8661 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8662 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8663 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8664 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8665 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8666 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8667 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8668 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8669 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8670 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8671 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8672 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8673 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8674 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8675 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8676 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8677 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8678 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8679 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8680 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8681 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8682 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8683 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8684 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8685 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8686 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8687 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8688 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8689 lambda_k: 1 Loss: 0.001749302636010277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8690 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8691 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8692 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8693 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8694 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8695 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8696 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8697 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8698 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8699 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8700 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8701 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8702 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8703 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8704 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8705 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8706 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8707 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8708 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8709 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8710 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8711 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8712 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8713 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8714 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8715 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8716 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8717 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8718 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8719 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8720 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8721 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8722 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8723 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8724 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8725 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8726 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8727 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8728 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8729 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8730 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8731 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8732 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8733 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8734 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8735 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8736 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8737 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8738 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8739 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8740 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8741 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8742 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8743 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8744 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8745 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8746 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8747 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8748 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8749 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8750 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8751 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8752 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8753 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8754 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8755 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8756 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8757 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8758 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8759 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8760 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8761 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8762 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8763 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8764 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8765 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8766 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8767 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8768 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8769 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8770 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8771 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8772 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8773 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8774 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8775 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8776 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8777 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8778 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8779 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8780 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8781 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8782 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8783 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8784 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8785 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8786 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8787 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8788 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8789 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8790 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8791 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8792 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8793 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8794 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8795 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8796 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8797 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8798 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8799 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8800 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8801 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8802 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8803 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8804 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8805 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8806 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8807 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8808 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8809 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8810 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8811 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8812 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8813 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8814 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8815 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8816 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8817 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8818 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8819 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8820 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8821 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8822 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8823 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8824 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8825 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8826 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8827 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8828 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8829 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8830 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8831 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8832 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8833 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8834 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8835 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8836 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8837 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8838 lambda_k: 1 Loss: 0.001749302636010277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8839 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8840 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8841 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8842 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8843 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8844 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8845 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8846 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8847 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8848 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8849 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8850 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8851 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8852 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8853 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8854 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8855 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8856 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8857 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8858 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8859 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8860 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8861 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8862 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8863 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8864 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8865 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8866 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8867 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8868 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8869 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8870 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8871 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8872 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8873 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8874 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8875 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8876 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8877 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8878 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8879 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8880 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8881 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8882 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8883 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8884 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8885 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8886 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8887 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8888 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8889 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8890 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8891 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8892 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8893 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8894 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8895 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8896 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8897 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8898 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8899 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8900 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8901 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8902 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8903 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8904 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8905 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8906 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8907 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8908 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8909 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8910 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8911 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8912 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8913 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8914 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8915 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8916 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8917 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8918 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8919 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8920 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8921 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8922 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8923 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8924 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8925 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8926 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8927 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8928 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8929 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8930 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8931 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8932 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8933 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8934 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8935 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8936 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8937 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8938 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8939 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8940 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8941 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8942 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8943 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8944 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8945 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8946 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8947 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8948 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8949 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8950 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8951 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8952 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8953 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8954 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8955 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8956 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8957 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8958 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8959 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8960 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8961 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8962 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8963 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8964 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8965 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8966 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8967 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8968 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8969 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8970 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8971 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8972 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8973 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8974 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8975 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8976 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8977 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8978 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8979 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8980 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8981 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8982 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8983 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8984 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8985 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8986 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8987 lambda_k: 1 Loss: 0.001749302636010277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8988 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8989 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8990 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8991 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8992 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8993 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8994 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8995 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8996 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 8997 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8998 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 8999 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9000 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9001 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9002 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9003 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9004 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9005 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9006 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9007 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9008 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9009 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9010 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9011 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9012 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9013 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9014 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9015 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9016 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9017 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9018 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9019 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9020 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9021 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9022 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9023 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9024 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9025 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9026 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9027 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9028 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9029 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9030 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9031 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9032 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9033 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9034 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9035 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9036 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9037 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9038 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9039 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9040 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9041 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9042 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9043 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9044 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9045 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9046 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9047 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9048 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9049 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9050 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9051 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9052 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9053 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9054 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9055 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9056 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9057 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9058 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9059 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9060 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9061 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9062 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9063 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9064 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9065 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9066 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9067 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9068 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9069 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9070 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9071 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9072 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9073 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9074 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9075 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9076 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9077 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9078 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9079 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9080 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9081 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9082 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9083 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9084 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9085 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9086 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9087 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9088 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9089 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9090 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9091 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9092 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9093 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9094 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9095 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9096 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9097 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9098 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9099 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9100 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9101 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9102 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9103 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9104 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9105 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9106 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9107 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9108 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9109 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9110 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9111 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9112 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9113 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9114 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9115 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9116 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9117 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9118 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9119 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9120 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9121 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9122 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9123 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9124 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9125 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9126 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9127 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9128 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9129 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9130 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9131 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9132 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9133 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9134 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9135 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9136 lambda_k: 1 Loss: 0.0017493026360102767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9137 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9138 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9139 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9140 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9141 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9142 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9143 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9144 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9145 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9146 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9147 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9148 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9149 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9150 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9151 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9152 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9153 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9154 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9155 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9156 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9157 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9158 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9159 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9160 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9161 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9162 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9163 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9164 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9165 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9166 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9167 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9168 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9169 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9170 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9171 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9172 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9173 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9174 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9175 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9176 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9177 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9178 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9179 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9180 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9181 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9182 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9183 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9184 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9185 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9186 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9187 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9188 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9189 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9190 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9191 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9192 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9193 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9194 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9195 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9196 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9197 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9198 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9199 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9200 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9201 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9202 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9203 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9204 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9205 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9206 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9207 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9208 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9209 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9210 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9211 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9212 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9213 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9214 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9215 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9216 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9217 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9218 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9219 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9220 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9221 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9222 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9223 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9224 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9225 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9226 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9227 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9228 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9229 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9230 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9231 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9232 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9233 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9234 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9235 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9236 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9237 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9238 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9239 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9240 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9241 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9242 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9243 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9244 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9245 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9246 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9247 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9248 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9249 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9250 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9251 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9252 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9253 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9254 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9255 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9256 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9257 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9258 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9259 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9260 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9261 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9262 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9263 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9264 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9265 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9266 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9267 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9268 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9269 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9270 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9271 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9272 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9273 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9274 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9275 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9276 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9277 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9278 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9279 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9280 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9281 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9282 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9283 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9284 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9285 lambda_k: 1 Loss: 0.001749302636010277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9286 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9287 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9288 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9289 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9290 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9291 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9292 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9293 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9294 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9295 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9296 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9297 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9298 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9299 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9300 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9301 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9302 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9303 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9304 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9305 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9306 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9307 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9308 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9309 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9310 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9311 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9312 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9313 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9314 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9315 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9316 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9317 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9318 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9319 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9320 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9321 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9322 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9323 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9324 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9325 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9326 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9327 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9328 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9329 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9330 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9331 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9332 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9333 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9334 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9335 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9336 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9337 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9338 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9339 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9340 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9341 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9342 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9343 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9344 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9345 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9346 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9347 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9348 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9349 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9350 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9351 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9352 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9353 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9354 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9355 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9356 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9357 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9358 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9359 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9360 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9361 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9362 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9363 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9364 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9365 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9366 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9367 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9368 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9369 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9370 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9371 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9372 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9373 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9374 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9375 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9376 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9377 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9378 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9379 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9380 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9381 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9382 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9383 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9384 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9385 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9386 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9387 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9388 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9389 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9390 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9391 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9392 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9393 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9394 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9395 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9396 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9397 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9398 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9399 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9400 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9401 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9402 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9403 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9404 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9405 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9406 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9407 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9408 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9409 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9410 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9411 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9412 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9413 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9414 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9415 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9416 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9417 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9418 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9419 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9420 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9421 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9422 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9423 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9424 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9425 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9426 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9427 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9428 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9429 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9430 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9431 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9432 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9433 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9434 lambda_k: 1 Loss: 0.001749302636010277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9435 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9436 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9437 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9438 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9439 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9440 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9441 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9442 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9443 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9444 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9445 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9446 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9447 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9448 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9449 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9450 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9451 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9452 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9453 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9454 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9455 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9456 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9457 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9458 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9459 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9460 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9461 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9462 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9463 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9464 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9465 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9466 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9467 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9468 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9469 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9470 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9471 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9472 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9473 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9474 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9475 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9476 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9477 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9478 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9479 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9480 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9481 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9482 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9483 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9484 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9485 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9486 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9487 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9488 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9489 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9490 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9491 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9492 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9493 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9494 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9495 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9496 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9497 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9498 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9499 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9500 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9501 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9502 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9503 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9504 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9505 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9506 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9507 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9508 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9509 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9510 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9511 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9512 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9513 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9514 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9515 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9516 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9517 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9518 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9519 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9520 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9521 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9522 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9523 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9524 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9525 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9526 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9527 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9528 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9529 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9530 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9531 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9532 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9533 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9534 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9535 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9536 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9537 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9538 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9539 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9540 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9541 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9542 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9543 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9544 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9545 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9546 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9547 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9548 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9549 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9550 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9551 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9552 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9553 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9554 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9555 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9556 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9557 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9558 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9559 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9560 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9561 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9562 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9563 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9564 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9565 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9566 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9567 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9568 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9569 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9570 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9571 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9572 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9573 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9574 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9575 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9576 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9577 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9578 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9579 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9580 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9581 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9582 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9583 lambda_k: 1 Loss: 0.001749302636010277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9584 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9585 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9586 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9587 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9588 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9589 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9590 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9591 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9592 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9593 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9594 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9595 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9596 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9597 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9598 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9599 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9600 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9601 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9602 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9603 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9604 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9605 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9606 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9607 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9608 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9609 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9610 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9611 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9612 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9613 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9614 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9615 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9616 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9617 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9618 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9619 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9620 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9621 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9622 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9623 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9624 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9625 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9626 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9627 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9628 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9629 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9630 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9631 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9632 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9633 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9634 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9635 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9636 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9637 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9638 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9639 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9640 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9641 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9642 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9643 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9644 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9645 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9646 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9647 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9648 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9649 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9650 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9651 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9652 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9653 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9654 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9655 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9656 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9657 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9658 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9659 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9660 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9661 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9662 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9663 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9664 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9665 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9666 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9667 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9668 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9669 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9670 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9671 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9672 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9673 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9674 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9675 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9676 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9677 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9678 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9679 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9680 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9681 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9682 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9683 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9684 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9685 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9686 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9687 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9688 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9689 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9690 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9691 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9692 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9693 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9694 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9695 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9696 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9697 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9698 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9699 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9700 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9701 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9702 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9703 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9704 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9705 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9706 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9707 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9708 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9709 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9710 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9711 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9712 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9713 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9714 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9715 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9716 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9717 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9718 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9719 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9720 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9721 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9722 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9723 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9724 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9725 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9726 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9727 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9728 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9729 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9730 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9731 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9732 lambda_k: 1 Loss: 0.001749302636010277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9733 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9734 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9735 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9736 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9737 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9738 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9739 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9740 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9741 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9742 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9743 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9744 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9745 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9746 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9747 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9748 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9749 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9750 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9751 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9752 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9753 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9754 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9755 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9756 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9757 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9758 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9759 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9760 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9761 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9762 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9763 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9764 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9765 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9766 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9767 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9768 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9769 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9770 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9771 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9772 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9773 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9774 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9775 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9776 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9777 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9778 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9779 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9780 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9781 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9782 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9783 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9784 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9785 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9786 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9787 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9788 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9789 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9790 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9791 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9792 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9793 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9794 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9795 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9796 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9797 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9798 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9799 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9800 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9801 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9802 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9803 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9804 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9805 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9806 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9807 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9808 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9809 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9810 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9811 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9812 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9813 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9814 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9815 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9816 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9817 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9818 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9819 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9820 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9821 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9822 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9823 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9824 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9825 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9826 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9827 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9828 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9829 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9830 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9831 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9832 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9833 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9834 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9835 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9836 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9837 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9838 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9839 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9840 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9841 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9842 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9843 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9844 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9845 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9846 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9847 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9848 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9849 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9850 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9851 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9852 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9853 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9854 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9855 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9856 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9857 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9858 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9859 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9860 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9861 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9862 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9863 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9864 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9865 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9866 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9867 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9868 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9869 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9870 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9871 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9872 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9873 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9874 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9875 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9876 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9877 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9878 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9879 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9880 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9881 lambda_k: 1 Loss: 0.0017493026360102767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9882 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9883 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9884 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9885 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9886 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9887 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9888 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9889 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9890 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9891 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9892 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9893 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9894 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9895 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9896 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9897 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9898 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9899 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9900 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9901 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9902 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9903 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9904 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9905 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9906 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9907 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9908 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9909 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9910 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9911 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9912 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9913 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9914 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9915 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9916 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9917 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9918 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9919 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9920 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9921 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9922 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9923 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9924 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9925 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9926 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9927 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9928 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9929 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9930 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9931 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9932 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9933 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9934 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9935 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9936 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9937 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9938 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9939 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9940 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9941 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9942 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9943 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9944 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9945 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9946 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9947 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9948 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9949 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9950 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9951 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9952 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9953 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9954 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9955 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9956 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9957 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9958 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9959 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9960 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9961 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9962 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9963 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9964 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9965 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9966 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9967 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9968 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9969 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9970 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9971 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9972 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9973 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9974 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9975 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9976 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9977 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9978 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9979 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9980 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9981 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9982 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9983 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9984 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9985 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9986 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9987 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9988 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9989 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9990 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9991 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9992 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9993 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9994 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9995 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9996 lambda_k: 1 Loss: 0.0017493026360102767\n",
      "Iteration: 9997 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9998 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 9999 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Iteration: 10000 lambda_k: 1 Loss: 0.001749302636010277\n",
      "Beta: 0.0022065297892106934\n",
      "Gamma: 0.003952278108170807\n",
      "Lambda_k: 1\n"
     ]
    }
   ],
   "source": [
    "# DY\n",
    "    \n",
    "dy = time.time()\n",
    "DY_list, DY_f_list, DY_z_list, Dual_DY_list, iterations_DY   = DY.Davis_Yin(N, M, frobenius_norm, Grad_Phi, D, (x1,x2,x3), Sigma)\n",
    "fin = time.time()\n",
    "\n",
    "Times[\"DY\"] = fin - dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87f81b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:\n",
      "10000\n",
      "Primal: (x1,x2,x3)\n",
      " ((array([[1186.74],\n",
      "       [44.01],\n",
      "       [24.78]]), array([[186.57, 704.01, 833.33, 1186.74, 714.29],\n",
      "       [8.88, 30.91, 44.01, 43.83, 43.48],\n",
      "       [4.55, 15.09, 24.78, 19.42, 24.78]]), array([[0.00, 0.00, 97.87, 0.00, 717.45]])), 'infactible')\n",
      "Primal: (xf1,xf2,xf3)\n",
      " ((array([[1186.74],\n",
      "       [44.01],\n",
      "       [24.78]]), array([[186.57, 704.01, 833.33, 1186.74, 714.29],\n",
      "       [8.88, 30.91, 44.01, 43.83, 43.48],\n",
      "       [4.55, 15.09, 24.78, 19.42, 24.78]]), array([[0.00, 0.00, 97.87, 0.00, 717.45]])), 'infactible')\n",
      "Primal: (z1,z2,z3)\n",
      " ((array([[1186.73],\n",
      "       [43.81],\n",
      "       [24.41]]), array([[186.57, 704.01, 833.33, 1186.88, 714.29],\n",
      "       [8.88, 30.91, 44.51, 43.83, 43.48],\n",
      "       [4.55, 15.09, 25.31, 19.42, 25.53]]), array([[0.00, 0.00, 97.87, 0.00, 717.45]])), 'infactible')\n",
      "Dual: (Capacity, Equilibrium)\n",
      " (array([[0.00, 0.00, 0.00, 34.62, 0.00],\n",
      "       [0.00, 0.00, 126.14, 0.00, 0.00],\n",
      "       [0.00, 0.00, 132.38, 0.00, 187.83]]), array([[1865.66, 6336.06, 10000.00, 8546.91, 10000.00]]))\n"
     ]
    }
   ],
   "source": [
    "iter_ = -2\n",
    "print(f\"Iterations:\\n{iterations_DY}\")\n",
    "print(\"Primal: (x1,x2,x3)\\n\",DY_list[iter_])\n",
    "print(\"Primal: (xf1,xf2,xf3)\\n\",DY_f_list[iter_])\n",
    "print(\"Primal: (z1,z2,z3)\\n\",DY_z_list[iter_])\n",
    "print(\"Dual: (Capacity, Equilibrium)\\n\",Dual_DY_list[iter_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "999e17b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta:  0.0022065297892106934\n",
      "gamma: 0.004099555511074575\n",
      "Iteration: 1 lambda_n: 0.9796505734384187 Loss: 0.5896406610236383\n",
      "Iteration: 2 lambda_n: 1.0034723735576268 Loss: 0.5140032064605676\n",
      "Iteration: 3 lambda_n: 0.9893730493422823 Loss: 0.49587051213404004\n",
      "Iteration: 4 lambda_n: 1.0352678538488005 Loss: 0.48958808936766435\n",
      "Iteration: 5 lambda_n: 0.9760692621851123 Loss: 0.48573720569568857\n",
      "Iteration: 6 lambda_n: 1.009264651924267 Loss: 0.4824467179876218\n",
      "Iteration: 7 lambda_n: 0.9928620696061934 Loss: 0.47943575879546246\n",
      "Iteration: 8 lambda_n: 0.886789022213463 Loss: 0.47682741622721303\n",
      "Iteration: 9 lambda_n: 1.0233273603878803 Loss: 0.47475362770968166\n",
      "Iteration: 10 lambda_n: 0.9525670318179469 Loss: 0.47257529167477885\n",
      "Iteration: 11 lambda_n: 0.9010216810994416 Loss: 0.47071951143399915\n",
      "Iteration: 12 lambda_n: 1.0125867470202046 Loss: 0.4690716071150566\n",
      "Iteration: 13 lambda_n: 0.9139981153153838 Loss: 0.46730088782327\n",
      "Iteration: 14 lambda_n: 0.9096304280094878 Loss: 0.46575561319188225\n",
      "Iteration: 15 lambda_n: 0.9524490199205692 Loss: 0.46424718496690476\n",
      "Iteration: 16 lambda_n: 0.9868116894212171 Loss: 0.46268667540821007\n",
      "Iteration: 17 lambda_n: 1.0316018795969186 Loss: 0.4610817671766152\n",
      "Iteration: 18 lambda_n: 1.0122804270309746 Loss: 0.45941140245293727\n",
      "Iteration: 19 lambda_n: 0.961662778616784 Loss: 0.4577769379275898\n",
      "Iteration: 20 lambda_n: 0.9806178984866782 Loss: 0.4562281510017915\n",
      "Iteration: 21 lambda_n: 1.0125454008247128 Loss: 0.45465399094864617\n",
      "Iteration: 22 lambda_n: 0.9521662924258036 Loss: 0.4530356500179005\n",
      "Iteration: 23 lambda_n: 1.0012165481089186 Loss: 0.4515221969944645\n",
      "Iteration: 24 lambda_n: 0.9816922698100927 Loss: 0.44994127246004784\n",
      "Iteration: 25 lambda_n: 0.9389232112405014 Loss: 0.44840345928688985\n",
      "Iteration: 26 lambda_n: 0.8847706703766579 Loss: 0.4469456279140378\n",
      "Iteration: 27 lambda_n: 0.9669071584483611 Loss: 0.4455847894890837\n",
      "Iteration: 28 lambda_n: 0.9320489840703814 Loss: 0.4441253431273461\n",
      "Iteration: 29 lambda_n: 0.9050810336136336 Loss: 0.4429008784639767\n",
      "Iteration: 30 lambda_n: 0.9679828319885916 Loss: 0.441736786071556\n",
      "Iteration: 31 lambda_n: 0.9116983771885129 Loss: 0.4405118182119422\n",
      "Iteration: 32 lambda_n: 0.9446364569742268 Loss: 0.43937676133254455\n",
      "Iteration: 33 lambda_n: 0.9198833549769323 Loss: 0.4382178981602566\n",
      "Iteration: 34 lambda_n: 0.8882093563951058 Loss: 0.4371057901581226\n",
      "Iteration: 35 lambda_n: 0.9710434712239608 Loss: 0.4360465243821831\n",
      "Iteration: 36 lambda_n: 1.0262080896593746 Loss: 0.43490303059308116\n",
      "Iteration: 37 lambda_n: 0.910386124208961 Loss: 0.4337104934275463\n",
      "Iteration: 38 lambda_n: 0.9919950149836747 Loss: 0.4326665804197777\n",
      "Iteration: 39 lambda_n: 0.9946324146137524 Loss: 0.43154187560906077\n",
      "Iteration: 40 lambda_n: 0.8873386653079658 Loss: 0.4304273859147787\n",
      "Iteration: 41 lambda_n: 0.9732674590873186 Loss: 0.4294442353648366\n",
      "Iteration: 42 lambda_n: 0.9911540322120219 Loss: 0.42837615724215333\n",
      "Iteration: 43 lambda_n: 1.024197337649114 Loss: 0.42729933653155294\n",
      "Iteration: 44 lambda_n: 1.0330500455222582 Loss: 0.42619744097370355\n",
      "Iteration: 45 lambda_n: 1.0126536129574966 Loss: 0.42509667235300386\n",
      "Iteration: 46 lambda_n: 1.0306242130006564 Loss: 0.4240275682211601\n",
      "Iteration: 47 lambda_n: 0.9613537316254149 Loss: 0.4229488436284233\n",
      "Iteration: 48 lambda_n: 0.9127911897685 Loss: 0.4219510114715219\n",
      "Iteration: 49 lambda_n: 0.8933470535554454 Loss: 0.4210106114168013\n",
      "Iteration: 50 lambda_n: 1.026680103778842 Loss: 0.42009644944939084\n",
      "Iteration: 51 lambda_n: 0.9434443028152587 Loss: 0.4190525114205675\n",
      "Iteration: 52 lambda_n: 0.9460675493128309 Loss: 0.4180999203416848\n",
      "Iteration: 53 lambda_n: 0.9091168716904857 Loss: 0.4171505587970623\n",
      "Iteration: 54 lambda_n: 0.9031117267473183 Loss: 0.4162436846308607\n",
      "Iteration: 55 lambda_n: 0.9619229529301757 Loss: 0.41534773720981827\n",
      "Iteration: 56 lambda_n: 0.9355668404217561 Loss: 0.41439845521202207\n",
      "Iteration: 57 lambda_n: 0.9536594614986251 Loss: 0.4134801633872308\n",
      "Iteration: 58 lambda_n: 0.8900697910938719 Loss: 0.41254884829902827\n",
      "Iteration: 59 lambda_n: 0.9224625971708872 Loss: 0.41168396065117785\n",
      "Iteration: 60 lambda_n: 1.025947831353409 Loss: 0.4107916246334842\n",
      "Iteration: 61 lambda_n: 0.8962607001758744 Loss: 0.40980366710984373\n",
      "Iteration: 62 lambda_n: 0.9779693577406011 Loss: 0.40894479431025443\n",
      "Iteration: 63 lambda_n: 0.9995094613741619 Loss: 0.4080114807965938\n",
      "Iteration: 64 lambda_n: 0.9554228568401553 Loss: 0.40706177901909785\n",
      "Iteration: 65 lambda_n: 0.9811503925520129 Loss: 0.4061579045007764\n",
      "Iteration: 66 lambda_n: 0.9492657522285467 Loss: 0.40523343365330244\n",
      "Iteration: 67 lambda_n: 1.031717651208767 Loss: 0.4043426147117674\n",
      "Iteration: 68 lambda_n: 1.0195258018238322 Loss: 0.40337810942275415\n",
      "Iteration: 69 lambda_n: 0.9009090199393238 Loss: 0.402428856913937\n",
      "Iteration: 70 lambda_n: 0.8945625149969849 Loss: 0.40159331769524065\n",
      "Iteration: 71 lambda_n: 1.0219060232510757 Loss: 0.4007664641294508\n",
      "Iteration: 72 lambda_n: 0.8843825136336074 Loss: 0.39982501593884595\n",
      "Iteration: 73 lambda_n: 0.9986394859931228 Loss: 0.39901327157929095\n",
      "Iteration: 74 lambda_n: 0.9033261791294538 Loss: 0.39809953402306075\n",
      "Iteration: 75 lambda_n: 0.905511268680846 Loss: 0.3972758910722709\n",
      "Iteration: 76 lambda_n: 0.9534710653373609 Loss: 0.39645282268660387\n",
      "Iteration: 77 lambda_n: 1.0107999431756236 Loss: 0.3955888269613031\n",
      "Iteration: 78 lambda_n: 0.9450592680263423 Loss: 0.3946758121163506\n",
      "Iteration: 79 lambda_n: 0.9850567529754327 Loss: 0.39382503534549934\n",
      "Iteration: 80 lambda_n: 0.9422357567440997 Loss: 0.39294099473455935\n",
      "Iteration: 81 lambda_n: 1.0276868612285357 Loss: 0.3920980808015244\n",
      "Iteration: 82 lambda_n: 0.9045228980227715 Loss: 0.39118150198922047\n",
      "Iteration: 83 lambda_n: 1.0174502226160849 Loss: 0.3903774038949993\n",
      "Iteration: 84 lambda_n: 0.9593048163016195 Loss: 0.3894754949369289\n",
      "Iteration: 85 lambda_n: 0.9032255004769354 Loss: 0.38862783302541337\n",
      "Iteration: 86 lambda_n: 1.018973465816467 Loss: 0.3878320996806962\n",
      "Iteration: 87 lambda_n: 0.9000547355831459 Loss: 0.38693689650758445\n",
      "Iteration: 88 lambda_n: 0.9641456250402066 Loss: 0.3861486376521935\n",
      "Iteration: 89 lambda_n: 0.9267128997382706 Loss: 0.38530656843354594\n",
      "Iteration: 90 lambda_n: 0.9107274392267913 Loss: 0.3844995622111905\n",
      "Iteration: 91 lambda_n: 0.9863295346616766 Loss: 0.38370869939923824\n",
      "Iteration: 92 lambda_n: 0.9634279671350435 Loss: 0.38285453693647803\n",
      "Iteration: 93 lambda_n: 0.927602155160742 Loss: 0.3820226790569328\n",
      "Iteration: 94 lambda_n: 0.9931927899938239 Loss: 0.3812240648493512\n",
      "Iteration: 95 lambda_n: 1.032909253855967 Loss: 0.3803713510446561\n",
      "Iteration: 96 lambda_n: 0.8831587802803955 Loss: 0.37948716411912925\n",
      "Iteration: 97 lambda_n: 0.9164193073548823 Loss: 0.37873348640875115\n",
      "Iteration: 98 lambda_n: 0.9543361528961978 Loss: 0.37795347569061344\n",
      "Iteration: 99 lambda_n: 1.0267284217262762 Loss: 0.3771433999314384\n",
      "Iteration: 100 lambda_n: 0.9454387091152516 Loss: 0.3762743397355837\n",
      "Iteration: 101 lambda_n: 1.0233595151584793 Loss: 0.37547651628835\n",
      "Iteration: 102 lambda_n: 1.024296682232973 Loss: 0.37461535371363613\n",
      "Iteration: 103 lambda_n: 0.9535940380073281 Loss: 0.3737560095554677\n",
      "Iteration: 104 lambda_n: 0.9989689551810896 Loss: 0.37295840211510745\n",
      "Iteration: 105 lambda_n: 0.9210210397480542 Loss: 0.3721251965517896\n",
      "Iteration: 106 lambda_n: 1.0012141418263194 Loss: 0.37135927044695566\n",
      "Iteration: 107 lambda_n: 1.008271916680745 Loss: 0.3705289223329282\n",
      "Iteration: 108 lambda_n: 1.0336230641258615 Loss: 0.36969519527935807\n",
      "Iteration: 109 lambda_n: 1.0054904114037535 Loss: 0.3688430539227201\n",
      "Iteration: 110 lambda_n: 0.9791785187303836 Loss: 0.36801663980343596\n",
      "Iteration: 111 lambda_n: 0.8901801037561222 Loss: 0.36721424626397237\n",
      "Iteration: 112 lambda_n: 0.9757852564127197 Loss: 0.36648689744880797\n",
      "Iteration: 113 lambda_n: 0.9227608399441493 Loss: 0.36569170758880387\n",
      "Iteration: 114 lambda_n: 0.9419781321896092 Loss: 0.3649419045931992\n",
      "Iteration: 115 lambda_n: 0.9751603174603448 Loss: 0.36417858380212637\n",
      "Iteration: 116 lambda_n: 0.9221462812073596 Loss: 0.36339058695316506\n",
      "Iteration: 117 lambda_n: 0.8825673439004578 Loss: 0.3626475899831646\n",
      "Iteration: 118 lambda_n: 0.914521327588815 Loss: 0.36193843497985706\n",
      "Iteration: 119 lambda_n: 0.9411051235023403 Loss: 0.3612055382233134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 120 lambda_n: 0.9603152705717422 Loss: 0.36045339572856566\n",
      "Iteration: 121 lambda_n: 0.9505928960723992 Loss: 0.3596880580842628\n",
      "Iteration: 122 lambda_n: 0.9484879748206765 Loss: 0.3589326442611299\n",
      "Iteration: 123 lambda_n: 0.9279137076694626 Loss: 0.3581810482118677\n",
      "Iteration: 124 lambda_n: 0.9877465430956136 Loss: 0.3574478455679026\n",
      "Iteration: 125 lambda_n: 0.9742894950656195 Loss: 0.35666953938002743\n",
      "Iteration: 126 lambda_n: 1.0152773099025154 Loss: 0.3559041146876093\n",
      "Iteration: 127 lambda_n: 0.8834125261952733 Loss: 0.3551088271205356\n",
      "Iteration: 128 lambda_n: 0.8953975993786271 Loss: 0.3544189459460431\n",
      "Iteration: 129 lambda_n: 0.9629944735600476 Loss: 0.3537215680878801\n",
      "Iteration: 130 lambda_n: 0.9249559844807503 Loss: 0.3529735708284255\n",
      "Iteration: 131 lambda_n: 0.9658355757148739 Loss: 0.3522572098972385\n",
      "Iteration: 132 lambda_n: 0.9006718042954567 Loss: 0.35151128246603464\n",
      "Iteration: 133 lambda_n: 1.0091510322184183 Loss: 0.3508177159930463\n",
      "Iteration: 134 lambda_n: 1.0256807586327117 Loss: 0.3500427388118718\n",
      "Iteration: 135 lambda_n: 1.037033280107849 Loss: 0.34925748107761656\n",
      "Iteration: 136 lambda_n: 0.9201514506034414 Loss: 0.34846600733342636\n",
      "Iteration: 137 lambda_n: 0.9967789144590503 Loss: 0.3477659536886644\n",
      "Iteration: 138 lambda_n: 0.8903976463635412 Loss: 0.34700972902284083\n",
      "Iteration: 139 lambda_n: 0.927218822166793 Loss: 0.34633626494255615\n",
      "Iteration: 140 lambda_n: 1.0098794507622482 Loss: 0.345636858112592\n",
      "Iteration: 141 lambda_n: 0.9538542362363143 Loss: 0.3448772602042057\n",
      "Iteration: 142 lambda_n: 0.9469696420558734 Loss: 0.3441620189469571\n",
      "Iteration: 143 lambda_n: 0.924046955534155 Loss: 0.34345401485345284\n",
      "Iteration: 144 lambda_n: 0.9046022523879986 Loss: 0.3427651549025605\n",
      "Iteration: 145 lambda_n: 0.9926626771353169 Loss: 0.342092703219134\n",
      "Iteration: 146 lambda_n: 0.9470449491880698 Loss: 0.34135684261535576\n",
      "Iteration: 147 lambda_n: 0.9553164404811936 Loss: 0.3406569411034837\n",
      "Iteration: 148 lambda_n: 0.9435064224201811 Loss: 0.33995298520781453\n",
      "Iteration: 149 lambda_n: 0.9315125914301947 Loss: 0.33925977844550753\n",
      "Iteration: 150 lambda_n: 0.9347867126604336 Loss: 0.33857737523170395\n",
      "Iteration: 151 lambda_n: 0.8937913309894662 Loss: 0.3378945428280067\n",
      "Iteration: 152 lambda_n: 0.9900362271319155 Loss: 0.3372435415262396\n",
      "Iteration: 153 lambda_n: 0.9016710541744637 Loss: 0.3365244338723489\n",
      "Iteration: 154 lambda_n: 0.9768683582737907 Loss: 0.33587151548792143\n",
      "Iteration: 155 lambda_n: 1.0163225479397742 Loss: 0.3351661220079126\n",
      "Iteration: 156 lambda_n: 0.8845211021118592 Loss: 0.33443446185505393\n",
      "Iteration: 157 lambda_n: 0.9855106810578572 Loss: 0.3337996931640815\n",
      "Iteration: 158 lambda_n: 0.9155889422593221 Loss: 0.33309439417710407\n",
      "Iteration: 159 lambda_n: 0.981428321916158 Loss: 0.3324411418404335\n",
      "Iteration: 160 lambda_n: 0.9228638878323461 Loss: 0.3317429093224652\n",
      "Iteration: 161 lambda_n: 0.8915924280055932 Loss: 0.3310883468582471\n",
      "Iteration: 162 lambda_n: 0.9278871901741718 Loss: 0.3304577816464344\n",
      "Iteration: 163 lambda_n: 1.0348202918307712 Loss: 0.32980337147950206\n",
      "Iteration: 164 lambda_n: 0.9333338620142716 Loss: 0.3290756581000401\n",
      "Iteration: 165 lambda_n: 0.9012027033965857 Loss: 0.3284214309724189\n",
      "Iteration: 166 lambda_n: 0.960775532352838 Loss: 0.3277915672467049\n",
      "Iteration: 167 lambda_n: 0.9642789659191393 Loss: 0.3271219589731399\n",
      "Iteration: 168 lambda_n: 1.0215664702499174 Loss: 0.32645192789311334\n",
      "Iteration: 169 lambda_n: 1.0036912249268184 Loss: 0.32574423270180264\n",
      "Iteration: 170 lambda_n: 0.8981547976213071 Loss: 0.32505114401828467\n",
      "Iteration: 171 lambda_n: 0.9182573949453577 Loss: 0.32443288111125584\n",
      "Iteration: 172 lambda_n: 0.964578460824759 Loss: 0.3238025598039949\n",
      "Iteration: 173 lambda_n: 0.9999299806387444 Loss: 0.32314234938099495\n",
      "Iteration: 174 lambda_n: 1.0075964781065065 Loss: 0.32246001412581915\n",
      "Iteration: 175 lambda_n: 1.0217688727199126 Loss: 0.321774605664827\n",
      "Iteration: 176 lambda_n: 0.9030798328768143 Loss: 0.3210817562894908\n",
      "Iteration: 177 lambda_n: 1.006253310877944 Loss: 0.3204713533808846\n",
      "Iteration: 178 lambda_n: 0.9974025912834762 Loss: 0.3197931466080207\n",
      "Iteration: 179 lambda_n: 0.923257323445418 Loss: 0.31912304969490946\n",
      "Iteration: 180 lambda_n: 0.9976594741755029 Loss: 0.31850471255781176\n",
      "Iteration: 181 lambda_n: 1.0020010374504924 Loss: 0.31783846538825133\n",
      "Iteration: 182 lambda_n: 0.9879056085750183 Loss: 0.31717137948731355\n",
      "Iteration: 183 lambda_n: 1.0246264060871308 Loss: 0.31651570057971123\n",
      "Iteration: 184 lambda_n: 0.9518992955453148 Loss: 0.3158377065244338\n",
      "Iteration: 185 lambda_n: 0.9345564963693539 Loss: 0.31520980794114917\n",
      "Iteration: 186 lambda_n: 0.9963521838570273 Loss: 0.31459514267212496\n",
      "Iteration: 187 lambda_n: 0.8961430433139731 Loss: 0.3139417079375552\n",
      "Iteration: 188 lambda_n: 0.9926564670133965 Loss: 0.3133557849369576\n",
      "Iteration: 189 lambda_n: 0.9458008941076362 Loss: 0.3127085430490933\n",
      "Iteration: 190 lambda_n: 0.9723388678513392 Loss: 0.31209373071886043\n",
      "Iteration: 191 lambda_n: 0.9205460890047391 Loss: 0.3114635048738299\n",
      "Iteration: 192 lambda_n: 0.922419469821715 Loss: 0.31086863284527666\n",
      "Iteration: 193 lambda_n: 0.9295429494299422 Loss: 0.3102742398505191\n",
      "Iteration: 194 lambda_n: 0.8840023536744678 Loss: 0.30967695933337286\n",
      "Iteration: 195 lambda_n: 0.9173633109677011 Loss: 0.3091105688051734\n",
      "Iteration: 196 lambda_n: 0.9439898805299255 Loss: 0.30852440713177065\n",
      "Iteration: 197 lambda_n: 0.8951347244348968 Loss: 0.3079229405222212\n",
      "Iteration: 198 lambda_n: 0.8872697201059987 Loss: 0.30735426437935576\n",
      "Iteration: 199 lambda_n: 1.0081292426458097 Loss: 0.306792143600539\n",
      "Iteration: 200 lambda_n: 0.9367246951210169 Loss: 0.30615520616826253\n",
      "Iteration: 201 lambda_n: 0.8829486370898382 Loss: 0.3055652256160102\n",
      "Iteration: 202 lambda_n: 1.0201222963574101 Loss: 0.30501072495050097\n",
      "Iteration: 203 lambda_n: 0.9360966750198602 Loss: 0.3043718285282016\n",
      "Iteration: 204 lambda_n: 0.9464801362150377 Loss: 0.3037874053701662\n",
      "Iteration: 205 lambda_n: 1.0361080037451336 Loss: 0.3031982104083244\n",
      "Iteration: 206 lambda_n: 0.9305958887686372 Loss: 0.3025551101822814\n",
      "Iteration: 207 lambda_n: 1.0107466953509066 Loss: 0.30197934993539194\n",
      "Iteration: 208 lambda_n: 0.9705979209068928 Loss: 0.3013558011566565\n",
      "Iteration: 209 lambda_n: 0.971169740677278 Loss: 0.3007588922709087\n",
      "Iteration: 210 lambda_n: 0.9155672440165867 Loss: 0.3001634245358203\n",
      "Iteration: 211 lambda_n: 0.9453224714615619 Loss: 0.29960373457779177\n",
      "Iteration: 212 lambda_n: 0.9898446205435065 Loss: 0.29902749149120433\n",
      "Iteration: 213 lambda_n: 0.9786665534574152 Loss: 0.29842587299513956\n",
      "Iteration: 214 lambda_n: 1.0240049567989182 Loss: 0.2978328685511711\n",
      "Iteration: 215 lambda_n: 0.8875304940060403 Loss: 0.2972142697371203\n",
      "Iteration: 216 lambda_n: 0.9140567541508305 Loss: 0.2966798102843066\n",
      "Iteration: 217 lambda_n: 0.9518380638756876 Loss: 0.2961308869378694\n",
      "Iteration: 218 lambda_n: 1.0374364714077153 Loss: 0.2955608892643282\n",
      "Iteration: 219 lambda_n: 0.8839677344101216 Loss: 0.2949414595915444\n",
      "Iteration: 220 lambda_n: 0.887531276193425 Loss: 0.29441535217664566\n",
      "Iteration: 221 lambda_n: 0.9835261342675418 Loss: 0.2938885656618146\n",
      "Iteration: 222 lambda_n: 1.0147830526107815 Loss: 0.29330640269967734\n",
      "Iteration: 223 lambda_n: 0.9876468778871249 Loss: 0.2927075620006536\n",
      "Iteration: 224 lambda_n: 0.9539414510066947 Loss: 0.29212655945481486\n",
      "Iteration: 225 lambda_n: 0.9101836176737799 Loss: 0.2915670941009852\n",
      "Iteration: 226 lambda_n: 0.9556067615505274 Loss: 0.2910348616368329\n",
      "Iteration: 227 lambda_n: 0.9235081991436225 Loss: 0.2904776363339016\n",
      "Iteration: 228 lambda_n: 1.011207641338961 Loss: 0.2899407137934644\n",
      "Iteration: 229 lambda_n: 0.9942367370195071 Loss: 0.28935447711599027\n",
      "Iteration: 230 lambda_n: 0.9954816228057868 Loss: 0.2887798742708418\n",
      "Iteration: 231 lambda_n: 0.9542475732116239 Loss: 0.28820631326632756\n",
      "Iteration: 232 lambda_n: 0.9972505495221637 Loss: 0.28765819411814386\n",
      "Iteration: 233 lambda_n: 1.0010040159717668 Loss: 0.2870870564612403\n",
      "Iteration: 234 lambda_n: 0.9088178903557692 Loss: 0.2865155277127639\n",
      "Iteration: 235 lambda_n: 0.942444658872634 Loss: 0.2859982293233095\n",
      "Iteration: 236 lambda_n: 0.9741624457601409 Loss: 0.2854632893818116\n",
      "Iteration: 237 lambda_n: 0.9846462418166549 Loss: 0.28491194758969335\n",
      "Iteration: 238 lambda_n: 0.9671843854555311 Loss: 0.2843563397880545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 239 lambda_n: 0.9527963777704386 Loss: 0.2838122348297034\n",
      "Iteration: 240 lambda_n: 0.9649088711719571 Loss: 0.28327781488429343\n",
      "Iteration: 241 lambda_n: 0.9016090318648158 Loss: 0.28273818306273063\n",
      "Iteration: 242 lambda_n: 0.9319188748002202 Loss: 0.28223544340315965\n",
      "Iteration: 243 lambda_n: 1.0108528236507601 Loss: 0.28171723920277636\n",
      "Iteration: 244 lambda_n: 0.9139506784801581 Loss: 0.281156748618522\n",
      "Iteration: 245 lambda_n: 0.9608073935080171 Loss: 0.28065155557897886\n",
      "Iteration: 246 lambda_n: 0.9399388871441542 Loss: 0.28012194847063954\n",
      "Iteration: 247 lambda_n: 1.01475507380358 Loss: 0.27960536730688657\n",
      "Iteration: 248 lambda_n: 0.8949813149139009 Loss: 0.27904927196004914\n",
      "Iteration: 249 lambda_n: 0.8855630618004228 Loss: 0.2785603342183131\n",
      "Iteration: 250 lambda_n: 1.0120813814385818 Loss: 0.2780778647610777\n",
      "Iteration: 251 lambda_n: 0.9736506334634875 Loss: 0.2775279587665164\n",
      "Iteration: 252 lambda_n: 1.013841971204513 Loss: 0.27700056809699014\n",
      "Iteration: 253 lambda_n: 0.9911945433245692 Loss: 0.27645303931959425\n",
      "Iteration: 254 lambda_n: 0.9811283630455657 Loss: 0.27591939646094205\n",
      "Iteration: 255 lambda_n: 0.9751525932606799 Loss: 0.27539276912421123\n",
      "Iteration: 256 lambda_n: 0.9079478630303991 Loss: 0.2748709140641826\n",
      "Iteration: 257 lambda_n: 0.9512561799176343 Loss: 0.2743864662045372\n",
      "Iteration: 258 lambda_n: 0.9179792751876675 Loss: 0.273880313814028\n",
      "Iteration: 259 lambda_n: 0.9821059893060692 Loss: 0.27339328110358424\n",
      "Iteration: 260 lambda_n: 1.0323174036646106 Loss: 0.2728736812933421\n",
      "Iteration: 261 lambda_n: 0.9644306229830428 Loss: 0.27232914725013424\n",
      "Iteration: 262 lambda_n: 0.9275045494640233 Loss: 0.27182201749003\n",
      "Iteration: 263 lambda_n: 1.032240681995722 Loss: 0.2713357326154839\n",
      "Iteration: 264 lambda_n: 0.978279065501567 Loss: 0.2707960596958238\n",
      "Iteration: 265 lambda_n: 0.982075976476844 Loss: 0.270286199819663\n",
      "Iteration: 266 lambda_n: 0.9761700585632974 Loss: 0.26977587916656304\n",
      "Iteration: 267 lambda_n: 0.9569841902470011 Loss: 0.26927013688046886\n",
      "Iteration: 268 lambda_n: 0.9694856145065244 Loss: 0.26877580017400987\n",
      "Iteration: 269 lambda_n: 0.925803088257554 Loss: 0.2682764565544415\n",
      "Iteration: 270 lambda_n: 0.8833569717972894 Loss: 0.26780101029942244\n",
      "Iteration: 271 lambda_n: 0.9450641846165851 Loss: 0.26734863184380414\n",
      "Iteration: 272 lambda_n: 0.9322555758989695 Loss: 0.2668659449510778\n",
      "Iteration: 273 lambda_n: 0.9405292119564945 Loss: 0.26639115917430417\n",
      "Iteration: 274 lambda_n: 0.9579066488587803 Loss: 0.26591350801398755\n",
      "Iteration: 275 lambda_n: 0.8876134836623577 Loss: 0.2654284124924477\n",
      "Iteration: 276 lambda_n: 0.9657571289406448 Loss: 0.2649802123277474\n",
      "Iteration: 277 lambda_n: 0.9889292194003233 Loss: 0.26449385902245626\n",
      "Iteration: 278 lambda_n: 0.9806534821496895 Loss: 0.2639972856035594\n",
      "Iteration: 279 lambda_n: 0.9044735705076632 Loss: 0.26350633399096623\n",
      "Iteration: 280 lambda_n: 0.9354239193648425 Loss: 0.263054856647014\n",
      "Iteration: 281 lambda_n: 0.8957668772685926 Loss: 0.26258920066655017\n",
      "Iteration: 282 lambda_n: 0.9218594066782866 Loss: 0.26214453971763496\n",
      "Iteration: 283 lambda_n: 0.9007404703545256 Loss: 0.2616881582617487\n",
      "Iteration: 284 lambda_n: 0.9596076244521361 Loss: 0.2612434664552337\n",
      "Iteration: 285 lambda_n: 0.9143531575755056 Loss: 0.26077099354554906\n",
      "Iteration: 286 lambda_n: 0.9591602848199957 Loss: 0.2603220978145135\n",
      "Iteration: 287 lambda_n: 0.9514640560256927 Loss: 0.25985249567999835\n",
      "Iteration: 288 lambda_n: 0.93100806664818 Loss: 0.25938800052290056\n",
      "Iteration: 289 lambda_n: 0.9699515111255921 Loss: 0.2589347868418986\n",
      "Iteration: 290 lambda_n: 0.9624572163226932 Loss: 0.2584639317781316\n",
      "Iteration: 291 lambda_n: 0.9517305499584291 Loss: 0.25799807053356394\n",
      "Iteration: 292 lambda_n: 0.891301026318147 Loss: 0.2575387270217001\n",
      "Iteration: 293 lambda_n: 0.9130602984066997 Loss: 0.2571097722925814\n",
      "Iteration: 294 lambda_n: 0.9836828236451222 Loss: 0.25667151551404077\n",
      "Iteration: 295 lambda_n: 1.0027402910182985 Loss: 0.25620064842115586\n",
      "Iteration: 296 lambda_n: 0.9466484410438075 Loss: 0.2557220678622303\n",
      "Iteration: 297 lambda_n: 0.891136927698503 Loss: 0.25527160892102807\n",
      "Iteration: 298 lambda_n: 0.9083937000065039 Loss: 0.25484876080579505\n",
      "Iteration: 299 lambda_n: 1.0349893151852994 Loss: 0.25441886850428364\n",
      "Iteration: 300 lambda_n: 0.9098229421397724 Loss: 0.25393039117712424\n",
      "Iteration: 301 lambda_n: 0.9786377434182264 Loss: 0.2535023092109042\n",
      "Iteration: 302 lambda_n: 1.0353001881199682 Loss: 0.25304309539530895\n",
      "Iteration: 303 lambda_n: 0.9514528205972241 Loss: 0.2525587068408128\n",
      "Iteration: 304 lambda_n: 0.883515990628438 Loss: 0.25211491640714295\n",
      "Iteration: 305 lambda_n: 0.9781737551191009 Loss: 0.2517039774649356\n",
      "Iteration: 306 lambda_n: 0.9596041751237274 Loss: 0.2512502048165808\n",
      "Iteration: 307 lambda_n: 0.9443257725132064 Loss: 0.25080633757398824\n",
      "Iteration: 308 lambda_n: 0.9104249541435875 Loss: 0.25037077942670666\n",
      "Iteration: 309 lambda_n: 1.010754125499622 Loss: 0.2499520318173299\n",
      "Iteration: 310 lambda_n: 0.8983387196999603 Loss: 0.24948839175073004\n",
      "Iteration: 311 lambda_n: 0.9409385702669155 Loss: 0.249077548849181\n",
      "Iteration: 312 lambda_n: 0.9836659472048906 Loss: 0.2486483668168689\n",
      "Iteration: 313 lambda_n: 1.0372631824674752 Loss: 0.24820094383537933\n",
      "Iteration: 314 lambda_n: 0.9447533682517673 Loss: 0.24773051315317235\n",
      "Iteration: 315 lambda_n: 0.9312607728498041 Loss: 0.2473033496992025\n",
      "Iteration: 316 lambda_n: 1.0140966783893481 Loss: 0.2468834602111076\n",
      "Iteration: 317 lambda_n: 0.9779456279741015 Loss: 0.2464274774918175\n",
      "Iteration: 318 lambda_n: 1.0149075906721754 Loss: 0.24598906358553346\n",
      "Iteration: 319 lambda_n: 0.9906769649365524 Loss: 0.24553539019614617\n",
      "Iteration: 320 lambda_n: 0.9086270237694202 Loss: 0.2450938707097316\n",
      "Iteration: 321 lambda_n: 0.9347228677829563 Loss: 0.24469009821455487\n",
      "Iteration: 322 lambda_n: 1.0137214385079751 Loss: 0.2442758389789292\n",
      "Iteration: 323 lambda_n: 0.8981965567576276 Loss: 0.24382780282511318\n",
      "Iteration: 324 lambda_n: 0.9900456436084949 Loss: 0.24343200627039288\n",
      "Iteration: 325 lambda_n: 0.9580879311413244 Loss: 0.2429968864046548\n",
      "Iteration: 326 lambda_n: 1.0016112957171792 Loss: 0.2425770342762739\n",
      "Iteration: 327 lambda_n: 0.9061049732758434 Loss: 0.242139342404183\n",
      "Iteration: 328 lambda_n: 0.9440005918497915 Loss: 0.24174454683974508\n",
      "Iteration: 329 lambda_n: 0.9456691793334413 Loss: 0.24133433135656243\n",
      "Iteration: 330 lambda_n: 0.8835536512539842 Loss: 0.24092452597826614\n",
      "Iteration: 331 lambda_n: 0.9902439505682054 Loss: 0.24054269689175808\n",
      "Iteration: 332 lambda_n: 0.9325091825089818 Loss: 0.24011586738619628\n",
      "Iteration: 333 lambda_n: 1.0138858615824955 Loss: 0.23971508596056176\n",
      "Iteration: 334 lambda_n: 0.9223123093971111 Loss: 0.23928051680573706\n",
      "Iteration: 335 lambda_n: 1.002202733578306 Loss: 0.23888636663775215\n",
      "Iteration: 336 lambda_n: 0.9032295583904931 Loss: 0.23845922786470072\n",
      "Iteration: 337 lambda_n: 1.003695925162601 Loss: 0.23807539530767746\n",
      "Iteration: 338 lambda_n: 0.9310994454819743 Loss: 0.23764999193267972\n",
      "Iteration: 339 lambda_n: 0.9806581554602398 Loss: 0.23725651031270448\n",
      "Iteration: 340 lambda_n: 0.9541058648205056 Loss: 0.23684320832151146\n",
      "Iteration: 341 lambda_n: 0.9247659376655544 Loss: 0.23644224343686324\n",
      "Iteration: 342 lambda_n: 0.9782031786400028 Loss: 0.23605468617869202\n",
      "Iteration: 343 lambda_n: 0.9295514382332447 Loss: 0.23564583562564342\n",
      "Iteration: 344 lambda_n: 0.8829891741724941 Loss: 0.2352584225731585\n",
      "Iteration: 345 lambda_n: 0.916238225809693 Loss: 0.2348914076878355\n",
      "Iteration: 346 lambda_n: 0.8853960624067145 Loss: 0.2345115481411932\n",
      "Iteration: 347 lambda_n: 1.036829105431938 Loss: 0.23414544988779382\n",
      "Iteration: 348 lambda_n: 0.9003006378643862 Loss: 0.23371783689497053\n",
      "Iteration: 349 lambda_n: 0.9602752366772621 Loss: 0.23334764521245055\n",
      "Iteration: 350 lambda_n: 0.9318294277919251 Loss: 0.23295382170667944\n",
      "Iteration: 351 lambda_n: 1.0228484958098467 Loss: 0.23257272529988893\n",
      "Iteration: 352 lambda_n: 0.9652010802653859 Loss: 0.23215553144559403\n",
      "Iteration: 353 lambda_n: 1.0059394780972424 Loss: 0.23176301327397622\n",
      "Iteration: 354 lambda_n: 0.8981386570768207 Loss: 0.23135506814606904\n",
      "Iteration: 355 lambda_n: 0.9116727594226358 Loss: 0.23099189662214353\n",
      "Iteration: 356 lambda_n: 1.0050319972198387 Loss: 0.23062420721007643\n",
      "Iteration: 357 lambda_n: 0.9599243075995536 Loss: 0.23021993040146624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 358 lambda_n: 0.9039937958121828 Loss: 0.2298349157769877\n",
      "Iteration: 359 lambda_n: 0.9885905196885213 Loss: 0.22947333581028093\n",
      "Iteration: 360 lambda_n: 0.9216588047310906 Loss: 0.2290789477321021\n",
      "Iteration: 361 lambda_n: 0.9632471474127449 Loss: 0.22871230617158833\n",
      "Iteration: 362 lambda_n: 0.9696963245635523 Loss: 0.22833013568673913\n",
      "Iteration: 363 lambda_n: 0.9669095295129269 Loss: 0.2279464709203142\n",
      "Iteration: 364 lambda_n: 0.9238545101249485 Loss: 0.22756497363325429\n",
      "Iteration: 365 lambda_n: 0.8931404218857488 Loss: 0.2272014747334748\n",
      "Iteration: 366 lambda_n: 0.9679060261309689 Loss: 0.22685099127481287\n",
      "Iteration: 367 lambda_n: 1.0240650403608684 Loss: 0.22647214113739567\n",
      "Iteration: 368 lambda_n: 0.972879192982784 Loss: 0.22607242126475643\n",
      "Iteration: 369 lambda_n: 1.0208378603836297 Loss: 0.22569379347580554\n",
      "Iteration: 370 lambda_n: 0.8998569985029585 Loss: 0.22529760707806598\n",
      "Iteration: 371 lambda_n: 0.9608529807633347 Loss: 0.22494939192658558\n",
      "Iteration: 372 lambda_n: 1.0058355523625009 Loss: 0.22457852976946827\n",
      "Iteration: 373 lambda_n: 0.9661157708894529 Loss: 0.22419137124908922\n",
      "Iteration: 374 lambda_n: 1.021483682769157 Loss: 0.223820568781163\n",
      "Iteration: 375 lambda_n: 1.0043397535865646 Loss: 0.2234295965541185\n",
      "Iteration: 376 lambda_n: 0.9540365944796183 Loss: 0.22304630558275954\n",
      "Iteration: 377 lambda_n: 0.9901442478653589 Loss: 0.22268325369953418\n",
      "Iteration: 378 lambda_n: 0.8840134007504974 Loss: 0.22230748519688753\n",
      "Iteration: 379 lambda_n: 0.9369910016616436 Loss: 0.22197293916512148\n",
      "Iteration: 380 lambda_n: 0.8977570215830927 Loss: 0.22161923623738666\n",
      "Iteration: 381 lambda_n: 0.979554822885982 Loss: 0.22128124622869105\n",
      "Iteration: 382 lambda_n: 0.9342118356777105 Loss: 0.22091340198563572\n",
      "Iteration: 383 lambda_n: 0.8937625132146516 Loss: 0.2205635607157687\n",
      "Iteration: 384 lambda_n: 0.9009294254719743 Loss: 0.22022975407594633\n",
      "Iteration: 385 lambda_n: 1.010330607532588 Loss: 0.21989412387472154\n",
      "Iteration: 386 lambda_n: 0.9806271917128409 Loss: 0.21951869970404453\n",
      "Iteration: 387 lambda_n: 1.023319140027281 Loss: 0.2191553560060627\n",
      "Iteration: 388 lambda_n: 0.9794732312872176 Loss: 0.21877724731752662\n",
      "Iteration: 389 lambda_n: 0.9088414639789797 Loss: 0.21841638740913524\n",
      "Iteration: 390 lambda_n: 1.0307630905182215 Loss: 0.2180824772396044\n",
      "Iteration: 391 lambda_n: 0.8899611944834007 Loss: 0.21770474663878106\n",
      "Iteration: 392 lambda_n: 0.9001883079192834 Loss: 0.2173795631418099\n",
      "Iteration: 393 lambda_n: 0.9879166446344648 Loss: 0.21705146950154325\n",
      "Iteration: 394 lambda_n: 0.9169847936134374 Loss: 0.216692316714282\n",
      "Iteration: 395 lambda_n: 0.9329853866447901 Loss: 0.2163598796569956\n",
      "Iteration: 396 lambda_n: 0.901376945683439 Loss: 0.21602251646600973\n",
      "Iteration: 397 lambda_n: 0.9735012129851565 Loss: 0.21569743954533976\n",
      "Iteration: 398 lambda_n: 0.9888546885119079 Loss: 0.21534724311430473\n",
      "Iteration: 399 lambda_n: 0.9067921363048126 Loss: 0.2149924985380865\n",
      "Iteration: 400 lambda_n: 0.9977922325634543 Loss: 0.21466809797437714\n",
      "Iteration: 401 lambda_n: 0.959930224422228 Loss: 0.21431205323799998\n",
      "Iteration: 402 lambda_n: 0.9736937911152956 Loss: 0.21397047926427096\n",
      "Iteration: 403 lambda_n: 0.8920058980852191 Loss: 0.2136249420443658\n",
      "Iteration: 404 lambda_n: 0.978617492544137 Loss: 0.21330925850784796\n",
      "Iteration: 405 lambda_n: 0.9847677212464425 Loss: 0.2129637901183785\n",
      "Iteration: 406 lambda_n: 1.016259740636256 Loss: 0.21261710467838724\n",
      "Iteration: 407 lambda_n: 0.9711634509124166 Loss: 0.21226032018279342\n",
      "Iteration: 408 lambda_n: 1.0233663423286263 Loss: 0.21192033828598145\n",
      "Iteration: 409 lambda_n: 1.0123067852797036 Loss: 0.2115630556811218\n",
      "Iteration: 410 lambda_n: 0.970887774611011 Loss: 0.21121064616602017\n",
      "Iteration: 411 lambda_n: 1.0143966268503184 Loss: 0.21087361222567005\n",
      "Iteration: 412 lambda_n: 0.8865596334917458 Loss: 0.2105224303427386\n",
      "Iteration: 413 lambda_n: 1.009828025684112 Loss: 0.21021637441918875\n",
      "Iteration: 414 lambda_n: 1.0322104762634525 Loss: 0.20986862750794488\n",
      "Iteration: 415 lambda_n: 0.9146680042403963 Loss: 0.2095141745749519\n",
      "Iteration: 416 lambda_n: 0.9712756874437554 Loss: 0.20920098834309808\n",
      "Iteration: 417 lambda_n: 1.0311467695523129 Loss: 0.2088692674468272\n",
      "Iteration: 418 lambda_n: 0.9194802647220351 Loss: 0.20851805181889832\n",
      "Iteration: 419 lambda_n: 0.9562597895401582 Loss: 0.20820576910868874\n",
      "Iteration: 420 lambda_n: 0.8943854664866049 Loss: 0.20788182604210476\n",
      "Iteration: 421 lambda_n: 0.9236213473530605 Loss: 0.20757964900292794\n",
      "Iteration: 422 lambda_n: 0.9877440727776032 Loss: 0.20726837019652145\n",
      "Iteration: 423 lambda_n: 0.89322734696267 Loss: 0.20693633520717755\n",
      "Iteration: 424 lambda_n: 1.0080963776133727 Loss: 0.20663689555561957\n",
      "Iteration: 425 lambda_n: 0.9923674135482499 Loss: 0.20629978621240264\n",
      "Iteration: 426 lambda_n: 1.0003334803413475 Loss: 0.20596886447507096\n",
      "Iteration: 427 lambda_n: 0.8853129585818156 Loss: 0.20563620400164734\n",
      "Iteration: 428 lambda_n: 0.8963655585572449 Loss: 0.20534260901639148\n",
      "Iteration: 429 lambda_n: 0.9727512390283933 Loss: 0.2050460774311221\n",
      "Iteration: 430 lambda_n: 0.9646190071902659 Loss: 0.20472507495253342\n",
      "Iteration: 431 lambda_n: 1.033593637458325 Loss: 0.20440761257351112\n",
      "Iteration: 432 lambda_n: 0.9581773493753887 Loss: 0.20406835770366402\n",
      "Iteration: 433 lambda_n: 1.0315562109046004 Loss: 0.20375475450085168\n",
      "Iteration: 434 lambda_n: 0.9024450830147667 Loss: 0.2034180287600465\n",
      "Iteration: 435 lambda_n: 0.940580396471485 Loss: 0.2031242863286325\n",
      "Iteration: 436 lambda_n: 1.0197409016475338 Loss: 0.20281889332149414\n",
      "Iteration: 437 lambda_n: 1.0176482442963393 Loss: 0.20248865704058394\n",
      "Iteration: 438 lambda_n: 1.0337493273466671 Loss: 0.20216002454417148\n",
      "Iteration: 439 lambda_n: 1.0186992817533935 Loss: 0.20182712820069892\n",
      "Iteration: 440 lambda_n: 0.8984894136811883 Loss: 0.2015000117930412\n",
      "Iteration: 441 lambda_n: 0.9941307262127654 Loss: 0.20121230427489084\n",
      "Iteration: 442 lambda_n: 0.9241279203691225 Loss: 0.2008947580395603\n",
      "Iteration: 443 lambda_n: 0.9200766997766062 Loss: 0.20060037832774313\n",
      "Iteration: 444 lambda_n: 0.9499384694874637 Loss: 0.2003080330328361\n",
      "Iteration: 445 lambda_n: 1.0079667925080475 Loss: 0.2000069619477532\n",
      "Iteration: 446 lambda_n: 0.8876903206731532 Loss: 0.19968833242654413\n",
      "Iteration: 447 lambda_n: 0.9230990463719715 Loss: 0.19940849883699388\n",
      "Iteration: 448 lambda_n: 0.9251890827104262 Loss: 0.19911821121538184\n",
      "Iteration: 449 lambda_n: 0.9396406475436732 Loss: 0.19882800209672544\n",
      "Iteration: 450 lambda_n: 0.9885354959338294 Loss: 0.19853400659499582\n",
      "Iteration: 451 lambda_n: 0.9902452876388744 Loss: 0.198225508395754\n",
      "Iteration: 452 lambda_n: 0.8864708091447864 Loss: 0.19791731222058173\n",
      "Iteration: 453 lambda_n: 0.974362054956883 Loss: 0.19764216051146294\n",
      "Iteration: 454 lambda_n: 0.8889994050260333 Loss: 0.1973404611227645\n",
      "Iteration: 455 lambda_n: 0.9181395754694979 Loss: 0.19706592541185586\n",
      "Iteration: 456 lambda_n: 1.0266011755782618 Loss: 0.19678307892636043\n",
      "Iteration: 457 lambda_n: 0.9912518793364694 Loss: 0.1964676118900175\n",
      "Iteration: 458 lambda_n: 0.9549250856645936 Loss: 0.19616385999367134\n",
      "Iteration: 459 lambda_n: 0.9585701650154009 Loss: 0.19587203017605867\n",
      "Iteration: 460 lambda_n: 0.9807201748836354 Loss: 0.19557984838431383\n",
      "Iteration: 461 lambda_n: 1.009625107012845 Loss: 0.1952816952415061\n",
      "Iteration: 462 lambda_n: 0.9189986456169122 Loss: 0.1949755737803727\n",
      "Iteration: 463 lambda_n: 1.0027837431346873 Loss: 0.19469769528954778\n",
      "Iteration: 464 lambda_n: 0.9690333866989513 Loss: 0.19439524034438022\n",
      "Iteration: 465 lambda_n: 0.9555433206175558 Loss: 0.19410376112922664\n",
      "Iteration: 466 lambda_n: 0.9201015755991041 Loss: 0.1938170958378696\n",
      "Iteration: 467 lambda_n: 0.8866977933388275 Loss: 0.19354177882253398\n",
      "Iteration: 468 lambda_n: 0.911631689578167 Loss: 0.19327711908370815\n",
      "Iteration: 469 lambda_n: 0.8831367144164237 Loss: 0.19300567131454754\n",
      "Iteration: 470 lambda_n: 0.9213650630768603 Loss: 0.19274335771788864\n",
      "Iteration: 471 lambda_n: 0.9857424049597074 Loss: 0.19247034407110433\n",
      "Iteration: 472 lambda_n: 1.0079433979133192 Loss: 0.19217898328044106\n",
      "Iteration: 473 lambda_n: 1.0346275566221896 Loss: 0.19188185513266162\n",
      "Iteration: 474 lambda_n: 1.0300033696881137 Loss: 0.19157769231351365\n",
      "Iteration: 475 lambda_n: 1.0244387662962702 Loss: 0.19127573568293144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 476 lambda_n: 0.9440452831087804 Loss: 0.19097624598056595\n",
      "Iteration: 477 lambda_n: 0.9362253386215469 Loss: 0.19070102197876054\n",
      "Iteration: 478 lambda_n: 0.9835406934443367 Loss: 0.19042877304838743\n",
      "Iteration: 479 lambda_n: 1.0142433933362989 Loss: 0.1901434874293571\n",
      "Iteration: 480 lambda_n: 0.9386895409773944 Loss: 0.18985007628996534\n",
      "Iteration: 481 lambda_n: 1.032512818048625 Loss: 0.18957926396958907\n",
      "Iteration: 482 lambda_n: 0.9546157229524496 Loss: 0.18928213689021198\n",
      "Iteration: 483 lambda_n: 0.9934104297890549 Loss: 0.18900818945777556\n",
      "Iteration: 484 lambda_n: 0.9825691414745269 Loss: 0.18872384123703712\n",
      "Iteration: 485 lambda_n: 0.8935076124751199 Loss: 0.18844334729228926\n",
      "Iteration: 486 lambda_n: 0.8851834851218887 Loss: 0.18818895091863483\n",
      "Iteration: 487 lambda_n: 0.916412044385455 Loss: 0.18793752933295574\n",
      "Iteration: 488 lambda_n: 0.9484268434684044 Loss: 0.18767785641440443\n",
      "Iteration: 489 lambda_n: 0.9199211396805492 Loss: 0.18740977276582568\n",
      "Iteration: 490 lambda_n: 1.0105526626112395 Loss: 0.1871504078875259\n",
      "Iteration: 491 lambda_n: 0.9659115976115485 Loss: 0.18686619302123592\n",
      "Iteration: 492 lambda_n: 1.0037096608008615 Loss: 0.18659526869304474\n",
      "Iteration: 493 lambda_n: 0.938129759421992 Loss: 0.18631447085363734\n",
      "Iteration: 494 lambda_n: 1.035317669801083 Loss: 0.1860527244806954\n",
      "Iteration: 495 lambda_n: 1.0299469636600667 Loss: 0.1857645870767345\n",
      "Iteration: 496 lambda_n: 1.0010067612994586 Loss: 0.1854787378250303\n",
      "Iteration: 497 lambda_n: 1.0173485911128668 Loss: 0.18520168510046664\n",
      "Iteration: 498 lambda_n: 1.0019701708849775 Loss: 0.1849208622181524\n",
      "Iteration: 499 lambda_n: 0.89160295535789 Loss: 0.18464503534892382\n",
      "Iteration: 500 lambda_n: 1.0104213892213347 Loss: 0.18440024666459115\n",
      "Iteration: 501 lambda_n: 0.9649918755378362 Loss: 0.1841234964310577\n",
      "Iteration: 502 lambda_n: 0.9083952261431915 Loss: 0.18385990080649603\n",
      "Iteration: 503 lambda_n: 1.0025743191897134 Loss: 0.18361240272815774\n",
      "Iteration: 504 lambda_n: 0.9715456805797423 Loss: 0.18333990592340654\n",
      "Iteration: 505 lambda_n: 1.0310158150109237 Loss: 0.18307654706594714\n",
      "Iteration: 506 lambda_n: 0.9847158695962257 Loss: 0.1827977899363131\n",
      "Iteration: 507 lambda_n: 0.9517838703578453 Loss: 0.18253228059192422\n",
      "Iteration: 508 lambda_n: 0.9281077277074895 Loss: 0.18227632208105504\n",
      "Iteration: 509 lambda_n: 0.9831804544972081 Loss: 0.18202736149278553\n",
      "Iteration: 510 lambda_n: 0.9258210465448827 Loss: 0.1817642777936489\n",
      "Iteration: 511 lambda_n: 1.0025316565959317 Loss: 0.18151718862443106\n",
      "Iteration: 512 lambda_n: 0.9033743314246033 Loss: 0.1812502835403221\n",
      "Iteration: 513 lambda_n: 0.90851254588205 Loss: 0.18101041601461584\n",
      "Iteration: 514 lambda_n: 0.9294283206364728 Loss: 0.18076976148293197\n",
      "Iteration: 515 lambda_n: 1.029045652401385 Loss: 0.18052415891065832\n",
      "Iteration: 516 lambda_n: 0.9383290635067716 Loss: 0.18025290153715015\n",
      "Iteration: 517 lambda_n: 1.0190489439349266 Loss: 0.18000623019679263\n",
      "Iteration: 518 lambda_n: 0.9784090371146735 Loss: 0.1797390037689024\n",
      "Iteration: 519 lambda_n: 0.9684198097517083 Loss: 0.17948312520940465\n",
      "Iteration: 520 lambda_n: 0.9108079865006219 Loss: 0.179230513550156\n",
      "Iteration: 521 lambda_n: 0.9501896434914943 Loss: 0.178993537136054\n",
      "Iteration: 522 lambda_n: 0.883539749111837 Loss: 0.1787469085469109\n",
      "Iteration: 523 lambda_n: 0.883863914056864 Loss: 0.1785181539456678\n",
      "Iteration: 524 lambda_n: 0.9079430348344077 Loss: 0.17828984839835374\n",
      "Iteration: 525 lambda_n: 0.896532911822443 Loss: 0.1780558693598481\n",
      "Iteration: 526 lambda_n: 1.0125890888599849 Loss: 0.17782538317096286\n",
      "Iteration: 527 lambda_n: 0.9569487297403443 Loss: 0.17756567525926445\n",
      "Iteration: 528 lambda_n: 0.9946014441214298 Loss: 0.177320891715902\n",
      "Iteration: 529 lambda_n: 0.895238080744483 Loss: 0.17706711713958198\n",
      "Iteration: 530 lambda_n: 1.016270521294038 Loss: 0.1768392922610031\n",
      "Iteration: 531 lambda_n: 0.9702449436523078 Loss: 0.17658127504244123\n",
      "Iteration: 532 lambda_n: 1.021257887552436 Loss: 0.17633560036741197\n",
      "Iteration: 533 lambda_n: 0.9906686278292139 Loss: 0.176077667424229\n",
      "Iteration: 534 lambda_n: 1.014124236841466 Loss: 0.1758281304811407\n",
      "Iteration: 535 lambda_n: 1.0349581759137325 Loss: 0.17557334895730806\n",
      "Iteration: 536 lambda_n: 0.9457874126742533 Loss: 0.17531402435841334\n",
      "Iteration: 537 lambda_n: 1.0125605436796676 Loss: 0.1750776850621159\n",
      "Iteration: 538 lambda_n: 0.9442019808816423 Loss: 0.17482528675225237\n",
      "Iteration: 539 lambda_n: 0.8969311289266451 Loss: 0.17459055142631505\n",
      "Iteration: 540 lambda_n: 0.9825107437367205 Loss: 0.17436811849730618\n",
      "Iteration: 541 lambda_n: 0.9940585745283304 Loss: 0.17412503390800782\n",
      "Iteration: 542 lambda_n: 0.9513836748994624 Loss: 0.17387972366374602\n",
      "Iteration: 543 lambda_n: 0.905137370419356 Loss: 0.17364555398117357\n",
      "Iteration: 544 lambda_n: 0.9284711812517877 Loss: 0.1734233203335018\n",
      "Iteration: 545 lambda_n: 0.9616761037549525 Loss: 0.17319589605654737\n",
      "Iteration: 546 lambda_n: 0.9587439949212163 Loss: 0.17296090884397694\n",
      "Iteration: 547 lambda_n: 0.9918791943322274 Loss: 0.17272722534402285\n",
      "Iteration: 548 lambda_n: 0.8971779867141368 Loss: 0.17248606946691097\n",
      "Iteration: 549 lambda_n: 0.9986603497713803 Loss: 0.1722685015325935\n",
      "Iteration: 550 lambda_n: 0.9491077374293694 Loss: 0.17202688964746596\n",
      "Iteration: 551 lambda_n: 0.9127703878324515 Loss: 0.17179786279934153\n",
      "Iteration: 552 lambda_n: 1.0026878919468576 Loss: 0.1715781479410841\n",
      "Iteration: 553 lambda_n: 0.9585512739785302 Loss: 0.17133736167183333\n",
      "Iteration: 554 lambda_n: 1.0168352538611172 Loss: 0.1711077739127673\n",
      "Iteration: 555 lambda_n: 0.9300671248616066 Loss: 0.17086483257386165\n",
      "Iteration: 556 lambda_n: 0.9315687622917599 Loss: 0.17064320802490812\n",
      "Iteration: 557 lambda_n: 0.9553212397933459 Loss: 0.170421761217948\n",
      "Iteration: 558 lambda_n: 0.8952210972725939 Loss: 0.1701952166946057\n",
      "Iteration: 559 lambda_n: 0.9304563918030804 Loss: 0.16998344977067043\n",
      "Iteration: 560 lambda_n: 0.9329751264558042 Loss: 0.16976385833270957\n",
      "Iteration: 561 lambda_n: 0.8929347525214224 Loss: 0.16954420293583342\n",
      "Iteration: 562 lambda_n: 1.0368502679380944 Loss: 0.16933448199383808\n",
      "Iteration: 563 lambda_n: 0.9708680564258474 Loss: 0.16909152290066987\n",
      "Iteration: 564 lambda_n: 0.956024795633961 Loss: 0.16886463480447417\n",
      "Iteration: 565 lambda_n: 0.9794717335652705 Loss: 0.16864177607083045\n",
      "Iteration: 566 lambda_n: 1.0233003051955136 Loss: 0.1684140155284971\n",
      "Iteration: 567 lambda_n: 1.03682912494807 Loss: 0.16817666521997082\n",
      "Iteration: 568 lambda_n: 1.017231362628555 Loss: 0.16793681206384034\n",
      "Iteration: 569 lambda_n: 0.900031409311774 Loss: 0.1677021226316619\n",
      "Iteration: 570 lambda_n: 0.9906162933256775 Loss: 0.16749506991220692\n",
      "Iteration: 571 lambda_n: 0.9404950005273189 Loss: 0.16726780375990594\n",
      "Iteration: 572 lambda_n: 0.9509716600386202 Loss: 0.16705272266684365\n",
      "Iteration: 573 lambda_n: 1.0365771134776782 Loss: 0.16683593146360776\n",
      "Iteration: 574 lambda_n: 0.929988288868602 Loss: 0.16660040087215724\n",
      "Iteration: 575 lambda_n: 1.0169405920671697 Loss: 0.16638986100364062\n",
      "Iteration: 576 lambda_n: 1.0088121734015394 Loss: 0.16616040222253223\n",
      "Iteration: 577 lambda_n: 0.9895159616255425 Loss: 0.16593361247369184\n",
      "Iteration: 578 lambda_n: 0.8933357779878663 Loss: 0.16571197439220597\n",
      "Iteration: 579 lambda_n: 1.004771285124993 Loss: 0.1655125987602301\n",
      "Iteration: 580 lambda_n: 0.9222190056120994 Loss: 0.16528908134501638\n",
      "Iteration: 581 lambda_n: 1.0181667713262241 Loss: 0.1650846765999629\n",
      "Iteration: 582 lambda_n: 1.0240872114273394 Loss: 0.16485976011452366\n",
      "Iteration: 583 lambda_n: 0.9174721389970628 Loss: 0.1646343685982551\n",
      "Iteration: 584 lambda_n: 0.9267343719955714 Loss: 0.16443318710046728\n",
      "Iteration: 585 lambda_n: 0.9080575875747633 Loss: 0.16423064424881398\n",
      "Iteration: 586 lambda_n: 0.8907426381999778 Loss: 0.16403284163251897\n",
      "Iteration: 587 lambda_n: 1.0232247526628828 Loss: 0.16383943918165794\n",
      "Iteration: 588 lambda_n: 0.9005537219270293 Loss: 0.16361797528377398\n",
      "Iteration: 589 lambda_n: 0.961510280903557 Loss: 0.1634237680747433\n",
      "Iteration: 590 lambda_n: 1.0105190664118975 Loss: 0.16321707417824244\n",
      "Iteration: 591 lambda_n: 0.924943612591121 Loss: 0.16300057909052149\n",
      "Iteration: 592 lambda_n: 1.0231433873905955 Loss: 0.1628031185309631\n",
      "Iteration: 593 lambda_n: 1.022361300019563 Loss: 0.16258539832540536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 594 lambda_n: 1.0089119415636827 Loss: 0.162368617362077\n",
      "Iteration: 595 lambda_n: 0.9813068158062282 Loss: 0.16215544423478567\n",
      "Iteration: 596 lambda_n: 1.018825547608283 Loss: 0.1619488237276686\n",
      "Iteration: 597 lambda_n: 0.9421773517018773 Loss: 0.16173502491221325\n",
      "Iteration: 598 lambda_n: 0.9102560406926041 Loss: 0.16153799780181388\n",
      "Iteration: 599 lambda_n: 0.9118323626551785 Loss: 0.1613482552644726\n",
      "Iteration: 600 lambda_n: 1.0042445384431948 Loss: 0.16115876951144392\n",
      "Iteration: 601 lambda_n: 1.020075397957292 Loss: 0.1609507212748501\n",
      "Iteration: 602 lambda_n: 1.0284647869826185 Loss: 0.16074010570215\n",
      "Iteration: 603 lambda_n: 0.8867547068682787 Loss: 0.16052848175692155\n",
      "Iteration: 604 lambda_n: 1.0151852531337509 Loss: 0.16034664091144582\n",
      "Iteration: 605 lambda_n: 0.9193647296822978 Loss: 0.1601390751811274\n",
      "Iteration: 606 lambda_n: 0.9329166755160794 Loss: 0.15995173006100394\n",
      "Iteration: 607 lambda_n: 1.0273264804939126 Loss: 0.1597621971010115\n",
      "Iteration: 608 lambda_n: 0.9478809690044665 Loss: 0.1595541203926787\n",
      "Iteration: 609 lambda_n: 1.0180913037730772 Loss: 0.1593627765070562\n",
      "Iteration: 610 lambda_n: 0.9521399161345641 Loss: 0.15915789088193166\n",
      "Iteration: 611 lambda_n: 1.0010669848844818 Loss: 0.15896690669327898\n",
      "Iteration: 612 lambda_n: 0.8970020143932026 Loss: 0.1587667223997442\n",
      "Iteration: 613 lambda_n: 0.9967144186839846 Loss: 0.15858792175310496\n",
      "Iteration: 614 lambda_n: 0.9871066341397357 Loss: 0.15838981248539\n",
      "Iteration: 615 lambda_n: 1.0254713202896713 Loss: 0.1581942323638111\n",
      "Iteration: 616 lambda_n: 0.9329038438455165 Loss: 0.15799168336552374\n",
      "Iteration: 617 lambda_n: 0.8963521726215138 Loss: 0.1578080109384977\n",
      "Iteration: 618 lambda_n: 1.0304676123110266 Loss: 0.15763204902470304\n",
      "Iteration: 619 lambda_n: 0.9492347284253444 Loss: 0.1574303232437356\n",
      "Iteration: 620 lambda_n: 1.0318239311037207 Loss: 0.15724509243533497\n",
      "Iteration: 621 lambda_n: 0.930787771761092 Loss: 0.15704433452701239\n",
      "Iteration: 622 lambda_n: 1.0075528157386466 Loss: 0.15686380773540148\n",
      "Iteration: 623 lambda_n: 0.9749615329922814 Loss: 0.156668947662412\n",
      "Iteration: 624 lambda_n: 0.8973650287464061 Loss: 0.15648096788879415\n",
      "Iteration: 625 lambda_n: 1.0321007845550187 Loss: 0.1563084591745051\n",
      "Iteration: 626 lambda_n: 0.8918619539316408 Loss: 0.156110585156041\n",
      "Iteration: 627 lambda_n: 1.032792959730141 Loss: 0.1559401261582959\n",
      "Iteration: 628 lambda_n: 0.9123679062280755 Loss: 0.1557432566793783\n",
      "Iteration: 629 lambda_n: 0.966732384684617 Loss: 0.1555698753637146\n",
      "Iteration: 630 lambda_n: 0.9950574189739944 Loss: 0.1553866581226952\n",
      "Iteration: 631 lambda_n: 0.899211845185795 Loss: 0.15519860880954284\n",
      "Iteration: 632 lambda_n: 0.9394163722293029 Loss: 0.15502916736037814\n",
      "Iteration: 633 lambda_n: 0.9938880330245077 Loss: 0.15485261370023465\n",
      "Iteration: 634 lambda_n: 0.977406476385154 Loss: 0.1546663315634273\n",
      "Iteration: 635 lambda_n: 1.0248444417275049 Loss: 0.15448366394957663\n",
      "Iteration: 636 lambda_n: 0.9369508300988223 Loss: 0.15429266840510275\n",
      "Iteration: 637 lambda_n: 0.9513872221817729 Loss: 0.15411856442232752\n",
      "Iteration: 638 lambda_n: 0.9675796232474577 Loss: 0.15394224890587938\n",
      "Iteration: 639 lambda_n: 0.9778951384191995 Loss: 0.15376341542427324\n",
      "Iteration: 640 lambda_n: 1.0204335657880725 Loss: 0.15358316802126942\n",
      "Iteration: 641 lambda_n: 0.9766672612366586 Loss: 0.15339559559543792\n",
      "Iteration: 642 lambda_n: 0.9878627594281882 Loss: 0.15321657908336586\n",
      "Iteration: 643 lambda_n: 1.0181801397659012 Loss: 0.15303600136866277\n",
      "Iteration: 644 lambda_n: 0.965551212070594 Loss: 0.15285038964093334\n",
      "Iteration: 645 lambda_n: 1.0194810839509831 Loss: 0.15267486444315964\n",
      "Iteration: 646 lambda_n: 0.9750219595776822 Loss: 0.1524900248900722\n",
      "Iteration: 647 lambda_n: 0.9848000065173285 Loss: 0.15231373637927734\n",
      "Iteration: 648 lambda_n: 0.9134898231078903 Loss: 0.15213614994483468\n",
      "Iteration: 649 lambda_n: 0.9851724063815481 Loss: 0.15197185950840078\n",
      "Iteration: 650 lambda_n: 1.018366805174048 Loss: 0.1517951110343534\n",
      "Iteration: 651 lambda_n: 0.9503816175872073 Loss: 0.15161288751909371\n",
      "Iteration: 652 lambda_n: 0.8954334935009568 Loss: 0.15144328865283918\n",
      "Iteration: 653 lambda_n: 0.9304348531699732 Loss: 0.15128389642156928\n",
      "Iteration: 654 lambda_n: 1.038065888950416 Loss: 0.1511186636334772\n",
      "Iteration: 655 lambda_n: 1.0232444528252498 Loss: 0.15093476608959336\n",
      "Iteration: 656 lambda_n: 0.9686252294656215 Loss: 0.15075398419941247\n",
      "Iteration: 657 lambda_n: 0.965550722040582 Loss: 0.1505833056252055\n",
      "Iteration: 658 lambda_n: 0.949261964831377 Loss: 0.15041359345951977\n",
      "Iteration: 659 lambda_n: 0.9107085085655132 Loss: 0.15024715737423247\n",
      "Iteration: 660 lambda_n: 0.9999852148211703 Loss: 0.1500878676056514\n",
      "Iteration: 661 lambda_n: 0.8904336322005836 Loss: 0.14991336737740535\n",
      "Iteration: 662 lambda_n: 0.9535680375919282 Loss: 0.14975837665878147\n",
      "Iteration: 663 lambda_n: 1.0055561854920947 Loss: 0.1495927684308343\n",
      "Iteration: 664 lambda_n: 1.0068358146350893 Loss: 0.14941854828742826\n",
      "Iteration: 665 lambda_n: 1.00132996129144 Loss: 0.14924454337414927\n",
      "Iteration: 666 lambda_n: 0.9970039291960449 Loss: 0.1490719217604574\n",
      "Iteration: 667 lambda_n: 0.9514070759115453 Loss: 0.1489004702026054\n",
      "Iteration: 668 lambda_n: 1.015124563412258 Loss: 0.14873725975191987\n",
      "Iteration: 669 lambda_n: 1.0148480996579814 Loss: 0.14856352325440725\n",
      "Iteration: 670 lambda_n: 1.0269970580323422 Loss: 0.1483902622124927\n",
      "Iteration: 671 lambda_n: 1.018979854714144 Loss: 0.1482153568909761\n",
      "Iteration: 672 lambda_n: 1.0154932252606363 Loss: 0.14804224521725778\n",
      "Iteration: 673 lambda_n: 0.9689504119663953 Loss: 0.14787014607817445\n",
      "Iteration: 674 lambda_n: 0.9352154632426172 Loss: 0.14770633110555975\n",
      "Iteration: 675 lambda_n: 0.9815468859260513 Loss: 0.147548581814112\n",
      "Iteration: 676 lambda_n: 0.9147703519033132 Loss: 0.14738338204279594\n",
      "Iteration: 677 lambda_n: 0.9500005950752289 Loss: 0.14722977493816305\n",
      "Iteration: 678 lambda_n: 0.9854627771548626 Loss: 0.14707059219421897\n",
      "Iteration: 679 lambda_n: 1.0301037599108276 Loss: 0.14690583134858595\n",
      "Iteration: 680 lambda_n: 1.0355298543013713 Loss: 0.14673399880110377\n",
      "Iteration: 681 lambda_n: 0.8980408836872918 Loss: 0.14656166977908056\n",
      "Iteration: 682 lambda_n: 0.9538387566240552 Loss: 0.14641257433981558\n",
      "Iteration: 683 lambda_n: 0.9186412373569764 Loss: 0.14625453845389963\n",
      "Iteration: 684 lambda_n: 0.9848513945449126 Loss: 0.14610266249410972\n",
      "Iteration: 685 lambda_n: 0.9787372467768684 Loss: 0.14594017704075585\n",
      "Iteration: 686 lambda_n: 0.9528264986302475 Loss: 0.1457790565334749\n",
      "Iteration: 687 lambda_n: 0.9553052952810968 Loss: 0.14562254353996976\n",
      "Iteration: 688 lambda_n: 0.9725619439466807 Loss: 0.14546595491915365\n",
      "Iteration: 689 lambda_n: 0.9279362272829316 Loss: 0.14530687376098078\n",
      "Iteration: 690 lambda_n: 1.0266672568489759 Loss: 0.14515541599572052\n",
      "Iteration: 691 lambda_n: 0.905668963957088 Loss: 0.14498818334880254\n",
      "Iteration: 692 lambda_n: 0.9548476285878896 Loss: 0.1448409889714309\n",
      "Iteration: 693 lambda_n: 0.97092831370602 Loss: 0.144686105910638\n",
      "Iteration: 694 lambda_n: 0.911198034616576 Loss: 0.1445289382185824\n",
      "Iteration: 695 lambda_n: 0.9230086136445991 Loss: 0.14438174590717204\n",
      "Iteration: 696 lambda_n: 0.9590330162333998 Loss: 0.144232935359051\n",
      "Iteration: 697 lambda_n: 0.9859876244040905 Loss: 0.1440786197004398\n",
      "Iteration: 698 lambda_n: 0.9983281581385598 Loss: 0.1439202881792828\n",
      "Iteration: 699 lambda_n: 0.936898345677688 Loss: 0.1437603071348439\n",
      "Iteration: 700 lambda_n: 0.9849098964342482 Loss: 0.1436104833109197\n",
      "Iteration: 701 lambda_n: 0.9482610689426735 Loss: 0.14345328875688623\n",
      "Iteration: 702 lambda_n: 0.9665538339086236 Loss: 0.14330225187902998\n",
      "Iteration: 703 lambda_n: 0.9713179717125019 Loss: 0.14314860204422147\n",
      "Iteration: 704 lambda_n: 0.9351210758960037 Loss: 0.14299450072157507\n",
      "Iteration: 705 lambda_n: 0.9847646025575776 Loss: 0.14284643584690151\n",
      "Iteration: 706 lambda_n: 0.9559942455211657 Loss: 0.142690806519341\n",
      "Iteration: 707 lambda_n: 0.9452784899698441 Loss: 0.14254002432586935\n",
      "Iteration: 708 lambda_n: 0.9932995904702797 Loss: 0.1423912185850716\n",
      "Iteration: 709 lambda_n: 1.0103086564803752 Loss: 0.14223514901717596\n",
      "Iteration: 710 lambda_n: 1.0007393314007644 Loss: 0.14207672072254018\n",
      "Iteration: 711 lambda_n: 0.9162427732358686 Loss: 0.14192010683671685\n",
      "Iteration: 712 lambda_n: 0.9926794432727962 Loss: 0.14177699888069076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 713 lambda_n: 0.9366740671174393 Loss: 0.14162223084922737\n",
      "Iteration: 714 lambda_n: 1.0130704551055905 Loss: 0.14147647714245082\n",
      "Iteration: 715 lambda_n: 0.9805314234473494 Loss: 0.14131912230817742\n",
      "Iteration: 716 lambda_n: 0.9942034059067651 Loss: 0.14116711945006727\n",
      "Iteration: 717 lambda_n: 0.9156088195788151 Loss: 0.14101328752280687\n",
      "Iteration: 718 lambda_n: 0.9134947933691916 Loss: 0.14087188548291185\n",
      "Iteration: 719 lambda_n: 0.9441679875857667 Loss: 0.14073105559689225\n",
      "Iteration: 720 lambda_n: 0.9810473217853154 Loss: 0.14058574876131763\n",
      "Iteration: 721 lambda_n: 0.9457439162391199 Loss: 0.140435034958504\n",
      "Iteration: 722 lambda_n: 0.8957432710945343 Loss: 0.14029001187403523\n",
      "Iteration: 723 lambda_n: 0.8879066280503736 Loss: 0.14015289831508534\n",
      "Iteration: 724 lambda_n: 0.9776712120247505 Loss: 0.14001721041508605\n",
      "Iteration: 725 lambda_n: 1.0323012375930296 Loss: 0.1398680504206221\n",
      "Iteration: 726 lambda_n: 1.026073445646444 Loss: 0.13971083928755973\n",
      "Iteration: 727 lambda_n: 0.9477594193448378 Loss: 0.1395548720233671\n",
      "Iteration: 728 lambda_n: 0.9286249576679795 Loss: 0.1394110779117037\n",
      "Iteration: 729 lambda_n: 0.9180878051479437 Loss: 0.13927042887134863\n",
      "Iteration: 730 lambda_n: 0.9094132505363223 Loss: 0.13913160870399688\n",
      "Iteration: 731 lambda_n: 0.9943160176313727 Loss: 0.13899432684559215\n",
      "Iteration: 732 lambda_n: 0.9543926049418092 Loss: 0.13884447260048904\n",
      "Iteration: 733 lambda_n: 0.999570003524253 Loss: 0.13870088960450802\n",
      "Iteration: 734 lambda_n: 0.8941647016154639 Loss: 0.13855076415873183\n",
      "Iteration: 735 lambda_n: 0.9282654706787614 Loss: 0.13841670577917067\n",
      "Iteration: 736 lambda_n: 1.0244221494035175 Loss: 0.13827775308934126\n",
      "Iteration: 737 lambda_n: 0.9120291684998526 Loss: 0.13812465540513072\n",
      "Iteration: 738 lambda_n: 0.8989247776856268 Loss: 0.13798859689145754\n",
      "Iteration: 739 lambda_n: 0.9565407165214689 Loss: 0.1378547047020401\n",
      "Iteration: 740 lambda_n: 0.9365921953126436 Loss: 0.13771245134735507\n",
      "Iteration: 741 lambda_n: 0.960384775010794 Loss: 0.13757339285782477\n",
      "Iteration: 742 lambda_n: 0.9921992750783123 Loss: 0.13743102959834128\n",
      "Iteration: 743 lambda_n: 0.9109933007353471 Loss: 0.13728419012728027\n",
      "Iteration: 744 lambda_n: 0.9118066011492088 Loss: 0.13714959437711505\n",
      "Iteration: 745 lambda_n: 0.9060969050426206 Loss: 0.13701508475920418\n",
      "Iteration: 746 lambda_n: 0.9625811716212626 Loss: 0.13688162137203774\n",
      "Iteration: 747 lambda_n: 0.9370897396912422 Loss: 0.13674005233637193\n",
      "Iteration: 748 lambda_n: 0.9235534650610124 Loss: 0.13660245233791596\n",
      "Iteration: 749 lambda_n: 0.9596200742140237 Loss: 0.13646704971295628\n",
      "Iteration: 750 lambda_n: 0.9922033523031015 Loss: 0.13632657296371264\n",
      "Iteration: 751 lambda_n: 0.9048917027777872 Loss: 0.13618155452228725\n",
      "Iteration: 752 lambda_n: 0.9944162306537556 Loss: 0.1360495107858807\n",
      "Iteration: 753 lambda_n: 0.9985665575598496 Loss: 0.13590461648148414\n",
      "Iteration: 754 lambda_n: 0.9971751682286669 Loss: 0.13575935086939192\n",
      "Iteration: 755 lambda_n: 0.9677632724652052 Loss: 0.13561452022651485\n",
      "Iteration: 756 lambda_n: 0.9614805066925481 Loss: 0.135474185245666\n",
      "Iteration: 757 lambda_n: 0.9997063474956647 Loss: 0.13533497585218926\n",
      "Iteration: 758 lambda_n: 0.8949904859023914 Loss: 0.13519045225108964\n",
      "Iteration: 759 lambda_n: 1.0323295363201148 Loss: 0.13506127047491148\n",
      "Iteration: 760 lambda_n: 0.9873738050852456 Loss: 0.13491247489046468\n",
      "Iteration: 761 lambda_n: 0.9260740783248798 Loss: 0.13477038824151105\n",
      "Iteration: 762 lambda_n: 0.949789928312304 Loss: 0.13463732705977605\n",
      "Iteration: 763 lambda_n: 1.0057207596822002 Loss: 0.13450105377791288\n",
      "Iteration: 764 lambda_n: 0.9572708233254023 Loss: 0.13435696683601034\n",
      "Iteration: 765 lambda_n: 0.9364285060616498 Loss: 0.13422003239252098\n",
      "Iteration: 766 lambda_n: 1.0317519487094728 Loss: 0.13408627486386585\n",
      "Iteration: 767 lambda_n: 0.9020124721449618 Loss: 0.1339391113596029\n",
      "Iteration: 768 lambda_n: 0.9953078230830227 Loss: 0.13381065344030862\n",
      "Iteration: 769 lambda_n: 0.8874143879526611 Loss: 0.13366910169776633\n",
      "Iteration: 770 lambda_n: 1.0212165735386753 Loss: 0.13354308227863984\n",
      "Iteration: 771 lambda_n: 0.9972585663210606 Loss: 0.1333982542516349\n",
      "Iteration: 772 lambda_n: 1.0118781146817784 Loss: 0.1332570382626315\n",
      "Iteration: 773 lambda_n: 0.9736934840621891 Loss: 0.13311396325239463\n",
      "Iteration: 774 lambda_n: 0.9390738478726797 Loss: 0.13297649212460308\n",
      "Iteration: 775 lambda_n: 0.9248902743887704 Loss: 0.132844097624064\n",
      "Iteration: 776 lambda_n: 0.9583004883819843 Loss: 0.13271388115688112\n",
      "Iteration: 777 lambda_n: 0.9859917128654808 Loss: 0.13257914196271806\n",
      "Iteration: 778 lambda_n: 0.9828065855855196 Loss: 0.1324407013226052\n",
      "Iteration: 779 lambda_n: 0.9123678882418108 Loss: 0.13230290356705968\n",
      "Iteration: 780 lambda_n: 0.9687090237048755 Loss: 0.13217516168614815\n",
      "Iteration: 781 lambda_n: 1.0275254474548645 Loss: 0.13203970795095887\n",
      "Iteration: 782 lambda_n: 0.8969168679807626 Loss: 0.13189622770776505\n",
      "Iteration: 783 lambda_n: 0.8866434810922156 Loss: 0.1317711666571515\n",
      "Iteration: 784 lambda_n: 0.9678320779584627 Loss: 0.13164769398536744\n",
      "Iteration: 785 lambda_n: 0.9242816819240118 Loss: 0.13151308275505993\n",
      "Iteration: 786 lambda_n: 0.9488189275071838 Loss: 0.13138470217864778\n",
      "Iteration: 787 lambda_n: 0.9810937109568044 Loss: 0.13125308270239397\n",
      "Iteration: 788 lambda_n: 1.0382025682943796 Loss: 0.13111716480906874\n",
      "Iteration: 789 lambda_n: 0.8948341597803463 Loss: 0.13097352976944096\n",
      "Iteration: 790 lambda_n: 0.9404242908955692 Loss: 0.1308499055120273\n",
      "Iteration: 791 lambda_n: 0.8836244069046993 Loss: 0.1307201416514031\n",
      "Iteration: 792 lambda_n: 0.95925984160101 Loss: 0.13059837098647276\n",
      "Iteration: 793 lambda_n: 0.9940537307375549 Loss: 0.13046633551662795\n",
      "Iteration: 794 lambda_n: 0.9667982382522243 Loss: 0.13032968800792905\n",
      "Iteration: 795 lambda_n: 0.9622262305174869 Loss: 0.1301969644654268\n",
      "Iteration: 796 lambda_n: 0.9650885519575 Loss: 0.1300650392823151\n",
      "Iteration: 797 lambda_n: 0.9484306373987922 Loss: 0.12993289114891177\n",
      "Iteration: 798 lambda_n: 1.0123609515610372 Loss: 0.12980319003171212\n",
      "Iteration: 799 lambda_n: 0.9167521492841362 Loss: 0.12966491976013442\n",
      "Iteration: 800 lambda_n: 0.9048394819038076 Loss: 0.12953987423888172\n",
      "Iteration: 801 lambda_n: 0.9852122372702383 Loss: 0.12941660164100374\n",
      "Iteration: 802 lambda_n: 0.9195881588064864 Loss: 0.12928253788111554\n",
      "Iteration: 803 lambda_n: 0.9912477730445743 Loss: 0.1291575638348528\n",
      "Iteration: 804 lambda_n: 1.0112198263324752 Loss: 0.12902301149437162\n",
      "Iteration: 805 lambda_n: 0.9949948792599272 Loss: 0.12888592341909114\n",
      "Iteration: 806 lambda_n: 0.9360726060888057 Loss: 0.1287512097532089\n",
      "Iteration: 807 lambda_n: 1.0147529326331655 Loss: 0.12862463444694422\n",
      "Iteration: 808 lambda_n: 1.0282155682204004 Loss: 0.1284875835921619\n",
      "Iteration: 809 lambda_n: 0.9382691579756883 Loss: 0.1283488929777862\n",
      "Iteration: 810 lambda_n: 0.9425930590996328 Loss: 0.12822249852406223\n",
      "Iteration: 811 lambda_n: 1.022999565472393 Loss: 0.12809567116378828\n",
      "Iteration: 812 lambda_n: 0.9761812122340883 Loss: 0.12795818750232432\n",
      "Iteration: 813 lambda_n: 1.0114336046985564 Loss: 0.12782716286290952\n",
      "Iteration: 814 lambda_n: 1.0021361195120304 Loss: 0.12769157105426301\n",
      "Iteration: 815 lambda_n: 0.9261968662712659 Loss: 0.1275573934389812\n",
      "Iteration: 816 lambda_n: 1.000143411671311 Loss: 0.12743353603207874\n",
      "Iteration: 817 lambda_n: 0.901244351906737 Loss: 0.12729994196509478\n",
      "Iteration: 818 lambda_n: 0.8971872033142445 Loss: 0.1271797048982769\n",
      "Iteration: 819 lambda_n: 0.8942980984131431 Loss: 0.12706014015923317\n",
      "Iteration: 820 lambda_n: 0.9905681451821788 Loss: 0.1269410898655944\n",
      "Iteration: 821 lambda_n: 1.034525001901081 Loss: 0.12680936653873887\n",
      "Iteration: 822 lambda_n: 0.9932431643210936 Loss: 0.1266719618590129\n",
      "Iteration: 823 lambda_n: 0.941877735669815 Loss: 0.1265402033861686\n",
      "Iteration: 824 lambda_n: 0.9452987652588353 Loss: 0.12641540651146577\n",
      "Iteration: 825 lambda_n: 0.9414947193271898 Loss: 0.12629029640577508\n",
      "Iteration: 826 lambda_n: 0.9191793144985851 Loss: 0.12616582904509482\n",
      "Iteration: 827 lambda_n: 0.9068201212970017 Loss: 0.12604444652701688\n",
      "Iteration: 828 lambda_n: 0.9772750700909886 Loss: 0.12592482524949028\n",
      "Iteration: 829 lambda_n: 1.026906506874894 Loss: 0.12579604701310668\n",
      "Iteration: 830 lambda_n: 0.9605695122523072 Loss: 0.12566088298045844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 831 lambda_n: 0.9688814512812849 Loss: 0.12553460084466056\n",
      "Iteration: 832 lambda_n: 0.9704997318778564 Loss: 0.12540736743670586\n",
      "Iteration: 833 lambda_n: 1.0086304302896894 Loss: 0.12528006372189276\n",
      "Iteration: 834 lambda_n: 0.8868765814181189 Loss: 0.1251479057292671\n",
      "Iteration: 835 lambda_n: 1.021021934748655 Loss: 0.12503183434624734\n",
      "Iteration: 836 lambda_n: 0.9344359160270296 Loss: 0.12489834191386473\n",
      "Iteration: 837 lambda_n: 1.0309648908447548 Loss: 0.12477631129154848\n",
      "Iteration: 838 lambda_n: 0.9848244366935079 Loss: 0.12464181723081405\n",
      "Iteration: 839 lambda_n: 0.8973927999997648 Loss: 0.12451349137250482\n",
      "Iteration: 840 lambda_n: 0.9635437324437705 Loss: 0.12439668700043345\n",
      "Iteration: 841 lambda_n: 0.9585796141677022 Loss: 0.12427139844257969\n",
      "Iteration: 842 lambda_n: 0.9191011188924563 Loss: 0.12414688908262601\n",
      "Iteration: 843 lambda_n: 0.9534983251662621 Loss: 0.12402763438897335\n",
      "Iteration: 844 lambda_n: 0.9801175279203633 Loss: 0.123904042439938\n",
      "Iteration: 845 lambda_n: 0.9022467753412925 Loss: 0.12377713367761072\n",
      "Iteration: 846 lambda_n: 0.8926785932921518 Loss: 0.12366043329306453\n",
      "Iteration: 847 lambda_n: 0.9356069330539637 Loss: 0.12354508440612251\n",
      "Iteration: 848 lambda_n: 0.9788578525508737 Loss: 0.12342430626537236\n",
      "Iteration: 849 lambda_n: 1.0289634498451172 Loss: 0.12329807345043149\n",
      "Iteration: 850 lambda_n: 0.9197082008266246 Loss: 0.12316551989489505\n",
      "Iteration: 851 lambda_n: 1.0161151924486096 Loss: 0.12304717199452207\n",
      "Iteration: 852 lambda_n: 0.9512338911150564 Loss: 0.12291654809726313\n",
      "Iteration: 853 lambda_n: 0.9484847558278076 Loss: 0.12279439769651372\n",
      "Iteration: 854 lambda_n: 0.9062954655580586 Loss: 0.12267272396148497\n",
      "Iteration: 855 lambda_n: 0.9222988914417787 Loss: 0.12255657949720496\n",
      "Iteration: 856 lambda_n: 1.0290479946354067 Loss: 0.12243849775027794\n",
      "Iteration: 857 lambda_n: 1.0005194986506032 Loss: 0.12230687775953288\n",
      "Iteration: 858 lambda_n: 0.939849893029281 Loss: 0.1221790452606916\n",
      "Iteration: 859 lambda_n: 0.9990065579842525 Loss: 0.1220590901125092\n",
      "Iteration: 860 lambda_n: 0.9213395802193048 Loss: 0.12193171019555836\n",
      "Iteration: 861 lambda_n: 0.9820403895052036 Loss: 0.12181435533812913\n",
      "Iteration: 862 lambda_n: 1.005110828652318 Loss: 0.12168938868127399\n",
      "Iteration: 863 lambda_n: 0.9221032429075007 Loss: 0.12156161636983925\n",
      "Iteration: 864 lambda_n: 0.9266154668014799 Loss: 0.12144451742433138\n",
      "Iteration: 865 lambda_n: 0.9604636566386612 Loss: 0.12132695702191376\n",
      "Iteration: 866 lambda_n: 0.990930158633293 Loss: 0.12120521810712659\n",
      "Iteration: 867 lambda_n: 1.0232129207824476 Loss: 0.12107974090312802\n",
      "Iteration: 868 lambda_n: 1.0174310967284614 Loss: 0.12095030672085413\n",
      "Iteration: 869 lambda_n: 1.0354846041974854 Loss: 0.12082173752386059\n",
      "Iteration: 870 lambda_n: 1.0215709723150965 Loss: 0.12069102166130954\n",
      "Iteration: 871 lambda_n: 0.9565764726605662 Loss: 0.1205621967083123\n",
      "Iteration: 872 lambda_n: 1.016763380297551 Loss: 0.12044169135752095\n",
      "Iteration: 873 lambda_n: 1.0213790109593937 Loss: 0.12031372679297216\n",
      "Iteration: 874 lambda_n: 0.9919218322833483 Loss: 0.12018531173178233\n",
      "Iteration: 875 lambda_n: 1.0051063696399092 Loss: 0.12006072675605148\n",
      "Iteration: 876 lambda_n: 0.9807023692824008 Loss: 0.11993460994087383\n",
      "Iteration: 877 lambda_n: 0.8882866296715303 Loss: 0.11981167729873181\n",
      "Iteration: 878 lambda_n: 1.015026088161865 Loss: 0.11970043627053205\n",
      "Iteration: 879 lambda_n: 0.9876170700355728 Loss: 0.11957343487167095\n",
      "Iteration: 880 lambda_n: 0.9938418600207385 Loss: 0.11944998553738534\n",
      "Iteration: 881 lambda_n: 0.9067407704876412 Loss: 0.11932587780171533\n",
      "Iteration: 882 lambda_n: 0.9748385174445919 Loss: 0.11921275603240994\n",
      "Iteration: 883 lambda_n: 0.9995076134675652 Loss: 0.11909124577771578\n",
      "Iteration: 884 lambda_n: 0.9900761225366932 Loss: 0.11896677806861493\n",
      "Iteration: 885 lambda_n: 0.9831927648736332 Loss: 0.11884360352752664\n",
      "Iteration: 886 lambda_n: 0.8950172420675417 Loss: 0.11872140160677612\n",
      "Iteration: 887 lambda_n: 0.9524571894154075 Loss: 0.11861026346719707\n",
      "Iteration: 888 lambda_n: 1.0300509228300923 Loss: 0.11849209402336268\n",
      "Iteration: 889 lambda_n: 0.9371015307280367 Loss: 0.11836441387371746\n",
      "Iteration: 890 lambda_n: 0.9355887238272773 Loss: 0.11824836844731701\n",
      "Iteration: 891 lambda_n: 0.9403988175629412 Loss: 0.11813261305362371\n",
      "Iteration: 892 lambda_n: 0.9602279032916298 Loss: 0.11801636522884917\n",
      "Iteration: 893 lambda_n: 0.9905129018056411 Loss: 0.11789777129373728\n",
      "Iteration: 894 lambda_n: 1.008051091611587 Loss: 0.11777554728078364\n",
      "Iteration: 895 lambda_n: 1.007172597109974 Loss: 0.11765127444604254\n",
      "Iteration: 896 lambda_n: 0.9275400087762128 Loss: 0.11752722661027473\n",
      "Iteration: 897 lambda_n: 0.8891036120536722 Loss: 0.1174130933083888\n",
      "Iteration: 898 lambda_n: 0.9551025541549198 Loss: 0.11730378348069175\n",
      "Iteration: 899 lambda_n: 1.0073320085130404 Loss: 0.11718645626006298\n",
      "Iteration: 900 lambda_n: 0.9415060719242611 Loss: 0.11706282222378679\n",
      "Iteration: 901 lambda_n: 1.0365754584443847 Loss: 0.11694737400721443\n",
      "Iteration: 902 lambda_n: 0.9716995353205695 Loss: 0.11682037843202261\n",
      "Iteration: 903 lambda_n: 0.8971734965496211 Loss: 0.11670144361802995\n",
      "Iteration: 904 lambda_n: 0.8876619462837969 Loss: 0.11659172764267237\n",
      "Iteration: 905 lambda_n: 0.9465492335631472 Loss: 0.11648326330958844\n",
      "Iteration: 906 lambda_n: 0.8897770301863308 Loss: 0.11636769678988272\n",
      "Iteration: 907 lambda_n: 1.0000662395803606 Loss: 0.11625915447441955\n",
      "Iteration: 908 lambda_n: 0.985046833935135 Loss: 0.11613725655518575\n",
      "Iteration: 909 lambda_n: 1.0244219626227768 Loss: 0.11601729728912938\n",
      "Iteration: 910 lambda_n: 1.0361040349094575 Loss: 0.11589265332005291\n",
      "Iteration: 911 lambda_n: 0.9789773947669151 Loss: 0.11576670354107846\n",
      "Iteration: 912 lambda_n: 0.9713205298347025 Loss: 0.1156478078568872\n",
      "Iteration: 913 lambda_n: 1.0345476317855846 Loss: 0.11552994477043455\n",
      "Iteration: 914 lambda_n: 1.0355030217375913 Loss: 0.11540451796635375\n",
      "Iteration: 915 lambda_n: 0.9155184826544759 Loss: 0.11527909023849892\n",
      "Iteration: 916 lambda_n: 1.0143391332594285 Loss: 0.1151682967170025\n",
      "Iteration: 917 lambda_n: 1.0140024777271714 Loss: 0.11504564351631487\n",
      "Iteration: 918 lambda_n: 1.0111447021389255 Loss: 0.11492314016331978\n",
      "Iteration: 919 lambda_n: 0.9159251413805688 Loss: 0.11480109045502855\n",
      "Iteration: 920 lambda_n: 0.9768887401966111 Loss: 0.11469063134039907\n",
      "Iteration: 921 lambda_n: 0.9220756567899471 Loss: 0.11457291431552247\n",
      "Iteration: 922 lambda_n: 0.9581412136320535 Loss: 0.11446189639523881\n",
      "Iteration: 923 lambda_n: 1.021460015706909 Loss: 0.11434662847234796\n",
      "Iteration: 924 lambda_n: 0.96923969375776 Loss: 0.11422384509973162\n",
      "Iteration: 925 lambda_n: 0.9225238403721686 Loss: 0.11410744114353162\n",
      "Iteration: 926 lambda_n: 0.911619497181544 Loss: 0.11399673978649541\n",
      "Iteration: 927 lambda_n: 0.9134062889379789 Loss: 0.11388743341984561\n",
      "Iteration: 928 lambda_n: 0.8972189893192373 Loss: 0.11377799822678543\n",
      "Iteration: 929 lambda_n: 1.0307452183335857 Loss: 0.11367058616432392\n",
      "Iteration: 930 lambda_n: 1.0093730570829598 Loss: 0.11354728370864588\n",
      "Iteration: 931 lambda_n: 0.9121636138962603 Loss: 0.11342664352376755\n",
      "Iteration: 932 lambda_n: 1.0374548667346768 Loss: 0.11331771463183832\n",
      "Iteration: 933 lambda_n: 1.0040751466357236 Loss: 0.113193919825381\n",
      "Iteration: 934 lambda_n: 0.9645612786577585 Loss: 0.11307421273657302\n",
      "Iteration: 935 lambda_n: 0.9249959964664121 Loss: 0.11295931348360734\n",
      "Iteration: 936 lambda_n: 0.9045758185453221 Loss: 0.11284921628033122\n",
      "Iteration: 937 lambda_n: 0.9078877868654859 Loss: 0.11274163285796271\n",
      "Iteration: 938 lambda_n: 0.9328824059283831 Loss: 0.11263373712726166\n",
      "Iteration: 939 lambda_n: 0.9337173430663358 Loss: 0.11252295496620547\n",
      "Iteration: 940 lambda_n: 1.0338322060078848 Loss: 0.11241215967381879\n",
      "Iteration: 941 lambda_n: 0.9816740839009696 Loss: 0.11228958024477276\n",
      "Iteration: 942 lambda_n: 0.8951933706570402 Loss: 0.11217328447314706\n",
      "Iteration: 943 lambda_n: 0.8891014411448676 Loss: 0.11206731939361855\n",
      "Iteration: 944 lambda_n: 1.0186161947859995 Loss: 0.11196215301850833\n",
      "Iteration: 945 lambda_n: 0.9906816816287087 Loss: 0.11184175584671921\n",
      "Iteration: 946 lambda_n: 0.9456163152611743 Loss: 0.11172475817188607\n",
      "Iteration: 947 lambda_n: 1.02175122886007 Loss: 0.11161317298248263\n",
      "Iteration: 948 lambda_n: 0.9387455389447765 Loss: 0.1114926971589972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 949 lambda_n: 0.9970782723591256 Loss: 0.1113821004052701\n",
      "Iteration: 950 lambda_n: 0.9865838273681345 Loss: 0.11126472120625991\n",
      "Iteration: 951 lambda_n: 0.8998661933277222 Loss: 0.11114867133611464\n",
      "Iteration: 952 lambda_n: 1.0265017588650438 Loss: 0.11104290602524522\n",
      "Iteration: 953 lambda_n: 0.9968047482198068 Loss: 0.1109223449786916\n",
      "Iteration: 954 lambda_n: 1.0034916801356348 Loss: 0.11080536850703615\n",
      "Iteration: 955 lambda_n: 1.0242947554954072 Loss: 0.11068770173219795\n",
      "Iteration: 956 lambda_n: 0.99683249273069 Loss: 0.1105676924445349\n",
      "Iteration: 957 lambda_n: 0.9892371041037014 Loss: 0.1104509963405681\n",
      "Iteration: 958 lambda_n: 1.0166960819896196 Loss: 0.1103352815679758\n",
      "Iteration: 959 lambda_n: 0.9933703705258102 Loss: 0.11021644871030209\n",
      "Iteration: 960 lambda_n: 1.0241273672920164 Loss: 0.11010043594973479\n",
      "Iteration: 961 lambda_n: 1.0347196794915237 Loss: 0.10998092559957459\n",
      "Iteration: 962 lambda_n: 0.9081330695443458 Loss: 0.10986027714371396\n",
      "Iteration: 963 lambda_n: 1.000381053153335 Loss: 0.10975447468935552\n",
      "Iteration: 964 lambda_n: 0.9673431626181901 Loss: 0.10963800873042047\n",
      "Iteration: 965 lambda_n: 0.9992830504723583 Loss: 0.10952547757065699\n",
      "Iteration: 966 lambda_n: 0.9696392246885299 Loss: 0.10940931927864937\n",
      "Iteration: 967 lambda_n: 0.916727778185909 Loss: 0.10929669493078935\n",
      "Iteration: 968 lambda_n: 0.8875368652613067 Loss: 0.10919029676387934\n",
      "Iteration: 969 lambda_n: 0.8908702829676934 Loss: 0.10908736012907966\n",
      "Iteration: 970 lambda_n: 0.9694956918394204 Loss: 0.10898410830989942\n",
      "Iteration: 971 lambda_n: 0.8832649845791505 Loss: 0.10887182200001998\n",
      "Iteration: 972 lambda_n: 0.9679802643464975 Loss: 0.10876959943749477\n",
      "Iteration: 973 lambda_n: 1.0175457406113322 Loss: 0.10865764958382493\n",
      "Iteration: 974 lambda_n: 0.9488438553417754 Loss: 0.10854005563279127\n",
      "Iteration: 975 lambda_n: 0.8935757319028417 Loss: 0.10843048704553292\n",
      "Iteration: 976 lambda_n: 1.0042604443772063 Loss: 0.10832737571543598\n",
      "Iteration: 977 lambda_n: 0.9242954513993623 Loss: 0.10821157239548838\n",
      "Iteration: 978 lambda_n: 0.9837574194844458 Loss: 0.10810507172200776\n",
      "Iteration: 979 lambda_n: 1.0241905491467225 Loss: 0.10799180011276925\n",
      "Iteration: 980 lambda_n: 0.8915480687062792 Loss: 0.10787396180289238\n",
      "Iteration: 981 lambda_n: 0.9653534550021486 Loss: 0.10777146419431397\n",
      "Iteration: 982 lambda_n: 1.0143543225428544 Loss: 0.10766088852988402\n",
      "Iteration: 983 lambda_n: 0.9567995550765773 Loss: 0.10754535620102663\n",
      "Iteration: 984 lambda_n: 0.8866146029599838 Loss: 0.10743645784722183\n",
      "Iteration: 985 lambda_n: 0.8873159887609288 Loss: 0.1073356160997705\n",
      "Iteration: 986 lambda_n: 0.9023779387788066 Loss: 0.10723475824101103\n",
      "Iteration: 987 lambda_n: 0.9409679046255255 Loss: 0.10713225306831942\n",
      "Iteration: 988 lambda_n: 0.9413866504549743 Loss: 0.10702543288621187\n",
      "Iteration: 989 lambda_n: 0.8968109376309232 Loss: 0.10691863638637805\n",
      "Iteration: 990 lambda_n: 0.9017392437532121 Loss: 0.10681696433934994\n",
      "Iteration: 991 lambda_n: 0.9010608388917973 Loss: 0.10671479831302022\n",
      "Iteration: 992 lambda_n: 0.9475626436307981 Loss: 0.1066127740374501\n",
      "Iteration: 993 lambda_n: 0.8913379938344261 Loss: 0.10650555277564885\n",
      "Iteration: 994 lambda_n: 0.9453856462200733 Loss: 0.10640476051488042\n",
      "Iteration: 995 lambda_n: 0.8917109861833021 Loss: 0.10629792368125768\n",
      "Iteration: 996 lambda_n: 0.9241121511151756 Loss: 0.1061972190596755\n",
      "Iteration: 997 lambda_n: 0.9144190453805997 Loss: 0.1060929204962051\n",
      "Iteration: 998 lambda_n: 1.0118924589353802 Loss: 0.10598978250910351\n",
      "Iteration: 999 lambda_n: 1.0375098695970986 Loss: 0.10587572372239812\n",
      "Iteration: 1000 lambda_n: 0.9891343167057725 Loss: 0.10575885994058903\n",
      "Iteration: 1001 lambda_n: 0.9719375777251691 Loss: 0.10564752525450642\n",
      "Iteration: 1002 lambda_n: 0.9625128116900263 Loss: 0.10553820122864278\n",
      "Iteration: 1003 lambda_n: 1.036834400472249 Loss: 0.10543001019212322\n",
      "Iteration: 1004 lambda_n: 0.9438716105692961 Loss: 0.10531354307749714\n",
      "Iteration: 1005 lambda_n: 0.9750927018457823 Loss: 0.10520759394384874\n",
      "Iteration: 1006 lambda_n: 1.0297792858299346 Loss: 0.10509821168410888\n",
      "Iteration: 1007 lambda_n: 0.9182193755741281 Loss: 0.10498277276772945\n",
      "Iteration: 1008 lambda_n: 0.9409656093309874 Loss: 0.10487991217571356\n",
      "Iteration: 1009 lambda_n: 0.8903925792976815 Loss: 0.10477457008625336\n",
      "Iteration: 1010 lambda_n: 0.9337423394249263 Loss: 0.10467495379388002\n",
      "Iteration: 1011 lambda_n: 0.8855954114973136 Loss: 0.10457055147259318\n",
      "Iteration: 1012 lambda_n: 1.0174786201594208 Loss: 0.10447159550157635\n",
      "Iteration: 1013 lambda_n: 1.0241473042495965 Loss: 0.1043579724098739\n",
      "Iteration: 1014 lambda_n: 0.90398858197661 Loss: 0.10424368401402273\n",
      "Iteration: 1015 lambda_n: 1.008773584166071 Loss: 0.10414287437612833\n",
      "Iteration: 1016 lambda_n: 1.0209558536908308 Loss: 0.10403044916345083\n",
      "Iteration: 1017 lambda_n: 1.0149782862324943 Loss: 0.10391674426944923\n",
      "Iteration: 1018 lambda_n: 0.944146493119918 Loss: 0.10380378331978488\n",
      "Iteration: 1019 lambda_n: 1.0305241049300433 Loss: 0.10369877738374807\n",
      "Iteration: 1020 lambda_n: 0.9693760204159694 Loss: 0.10358423831919787\n",
      "Iteration: 1021 lambda_n: 0.8860411400555395 Loss: 0.10347657029830531\n",
      "Iteration: 1022 lambda_n: 0.8969906307604407 Loss: 0.10337822216475465\n",
      "Iteration: 1023 lambda_n: 0.8935972513233525 Loss: 0.10327871810060922\n",
      "Iteration: 1024 lambda_n: 1.006426980145591 Loss: 0.1031796502305226\n",
      "Iteration: 1025 lambda_n: 1.0140125310193646 Loss: 0.10306848113069565\n",
      "Iteration: 1026 lambda_n: 0.9058421223978017 Loss: 0.10295743126042123\n",
      "Iteration: 1027 lambda_n: 0.9867169377067103 Loss: 0.10285828473984923\n",
      "Iteration: 1028 lambda_n: 0.9417233258515678 Loss: 0.10275034263176497\n",
      "Iteration: 1029 lambda_n: 0.9432935413578715 Loss: 0.10264738052149874\n",
      "Iteration: 1030 lambda_n: 0.9790557171919981 Loss: 0.10254430227077634\n",
      "Iteration: 1031 lambda_n: 0.9598360390012715 Loss: 0.1024373739846632\n",
      "Iteration: 1032 lambda_n: 0.8836288572946981 Loss: 0.10233260336458437\n",
      "Iteration: 1033 lambda_n: 0.9345086819040012 Loss: 0.10223620369150899\n",
      "Iteration: 1034 lambda_n: 0.9594849045212319 Loss: 0.10213430499727932\n",
      "Iteration: 1035 lambda_n: 0.9821874686724178 Loss: 0.1020297388897405\n",
      "Iteration: 1036 lambda_n: 0.9291423526597572 Loss: 0.10192275741727014\n",
      "Iteration: 1037 lambda_n: 1.0060819079649905 Loss: 0.10182161024711567\n",
      "Iteration: 1038 lambda_n: 0.8897658833177237 Loss: 0.10171214589699135\n",
      "Iteration: 1039 lambda_n: 0.9391465351440731 Loss: 0.10161539214764777\n",
      "Iteration: 1040 lambda_n: 0.9153125202586062 Loss: 0.10151332084090525\n",
      "Iteration: 1041 lambda_n: 0.9460766040055434 Loss: 0.10141389318156899\n",
      "Iteration: 1042 lambda_n: 0.9349169296403751 Loss: 0.10131117756969617\n",
      "Iteration: 1043 lambda_n: 0.9972467664647305 Loss: 0.10120972833998466\n",
      "Iteration: 1044 lambda_n: 0.9622445321214144 Loss: 0.10110157367363301\n",
      "Iteration: 1045 lambda_n: 0.9586769726987873 Loss: 0.10099727435021985\n",
      "Iteration: 1046 lambda_n: 0.9585206435692901 Loss: 0.10089341877357916\n",
      "Iteration: 1047 lambda_n: 0.9099433655226478 Loss: 0.1007896369423588\n",
      "Iteration: 1048 lambda_n: 0.9954156804593516 Loss: 0.10069116838214023\n",
      "Iteration: 1049 lambda_n: 1.0145623521285334 Loss: 0.10058350687561958\n",
      "Iteration: 1050 lambda_n: 0.8917108588603424 Loss: 0.10047383692938422\n",
      "Iteration: 1051 lambda_n: 0.9465528365275064 Loss: 0.10037750195670088\n",
      "Iteration: 1052 lambda_n: 1.0290829967217652 Loss: 0.10027529446314302\n",
      "Iteration: 1053 lambda_n: 0.9015227053364873 Loss: 0.10016423588475458\n",
      "Iteration: 1054 lambda_n: 0.888637137676871 Loss: 0.10006700013173668\n",
      "Iteration: 1055 lambda_n: 1.032619743516304 Loss: 0.09997120339509037\n",
      "Iteration: 1056 lambda_n: 0.9550507450059186 Loss: 0.09985994220516099\n",
      "Iteration: 1057 lambda_n: 0.9412819697055342 Loss: 0.09975709901866828\n",
      "Iteration: 1058 lambda_n: 0.9542895357133647 Loss: 0.0996557936220885\n",
      "Iteration: 1059 lambda_n: 0.9607880416826227 Loss: 0.09955314345420962\n",
      "Iteration: 1060 lambda_n: 0.9498640153973474 Loss: 0.09944985050400083\n",
      "Iteration: 1061 lambda_n: 0.981918624340487 Loss: 0.09934778784534888\n",
      "Iteration: 1062 lambda_n: 0.9533698949619114 Loss: 0.09924233819207186\n",
      "Iteration: 1063 lambda_n: 0.9354083191927416 Loss: 0.09914001157861424\n",
      "Iteration: 1064 lambda_n: 0.9811798956739971 Loss: 0.0990396672618383\n",
      "Iteration: 1065 lambda_n: 0.9885766068831774 Loss: 0.09893446919194392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1066 lambda_n: 0.9295063090280182 Loss: 0.09882853735798187\n",
      "Iteration: 1067 lambda_n: 0.9407998829841066 Loss: 0.09872899107542225\n",
      "Iteration: 1068 lambda_n: 0.9974995439892123 Loss: 0.09862828869496869\n",
      "Iteration: 1069 lambda_n: 1.0097040745493875 Loss: 0.09852157472395988\n",
      "Iteration: 1070 lambda_n: 1.0040115188258525 Loss: 0.09841361652087403\n",
      "Iteration: 1071 lambda_n: 0.971384261040893 Loss: 0.09830632866914842\n",
      "Iteration: 1072 lambda_n: 0.8913530705052178 Loss: 0.09820258653172799\n",
      "Iteration: 1073 lambda_n: 0.9717782676303354 Loss: 0.09810744389152097\n",
      "Iteration: 1074 lambda_n: 0.9708339874997043 Loss: 0.0980037697276907\n",
      "Iteration: 1075 lambda_n: 1.0102299112939546 Loss: 0.09790025360706629\n",
      "Iteration: 1076 lambda_n: 1.0353963876368686 Loss: 0.09779259659089469\n",
      "Iteration: 1077 lambda_n: 0.8980462905559015 Loss: 0.09768232125015791\n",
      "Iteration: 1078 lambda_n: 0.9808349120659 Loss: 0.09758673021838406\n",
      "Iteration: 1079 lambda_n: 0.9995638275169726 Loss: 0.09748238066077439\n",
      "Iteration: 1080 lambda_n: 0.9391848372932674 Loss: 0.0973760980287623\n",
      "Iteration: 1081 lambda_n: 0.9183774806737701 Loss: 0.09727629198024276\n",
      "Iteration: 1082 lambda_n: 0.9321333416037713 Loss: 0.09717874918697975\n",
      "Iteration: 1083 lambda_n: 1.0141556911257419 Loss: 0.09707979717208998\n",
      "Iteration: 1084 lambda_n: 0.9488823667133691 Loss: 0.0969721954769908\n",
      "Iteration: 1085 lambda_n: 0.991041579901696 Loss: 0.0968715770824128\n",
      "Iteration: 1086 lambda_n: 0.919873954097484 Loss: 0.09676654513420299\n",
      "Iteration: 1087 lambda_n: 0.9825964873838745 Loss: 0.0966691103045987\n",
      "Iteration: 1088 lambda_n: 0.9701028629844625 Loss: 0.09656508657477307\n",
      "Iteration: 1089 lambda_n: 0.9061578322679968 Loss: 0.09646244287790144\n",
      "Iteration: 1090 lambda_n: 1.0091510351058555 Loss: 0.09636661767585637\n",
      "Iteration: 1091 lambda_n: 0.891078329608798 Loss: 0.09625995660030466\n",
      "Iteration: 1092 lambda_n: 1.0275554677433838 Loss: 0.0961658286768491\n",
      "Iteration: 1093 lambda_n: 0.9695501212576565 Loss: 0.09605733989788083\n",
      "Iteration: 1094 lambda_n: 0.9979681754531349 Loss: 0.09595503489686912\n",
      "Iteration: 1095 lambda_n: 0.8947110268155337 Loss: 0.09584978953456941\n",
      "Iteration: 1096 lambda_n: 1.0030091378837698 Loss: 0.09575548679278101\n",
      "Iteration: 1097 lambda_n: 0.9947393141257201 Loss: 0.09564982377615285\n",
      "Iteration: 1098 lambda_n: 0.8906061398279442 Loss: 0.0955450917510236\n",
      "Iteration: 1099 lambda_n: 0.9978134508968965 Loss: 0.0954513761389865\n",
      "Iteration: 1100 lambda_n: 1.0136274045282645 Loss: 0.09534643320346349\n",
      "Iteration: 1101 lambda_n: 0.9528454943074434 Loss: 0.09523988771315917\n",
      "Iteration: 1102 lambda_n: 1.0115141336880984 Loss: 0.09513978872603872\n",
      "Iteration: 1103 lambda_n: 0.9067684137627771 Loss: 0.09503358438396636\n",
      "Iteration: 1104 lambda_n: 0.9577830833774743 Loss: 0.09493843224945042\n",
      "Iteration: 1105 lambda_n: 0.888519658322397 Loss: 0.09483797900783159\n",
      "Iteration: 1106 lambda_n: 0.9687716479390086 Loss: 0.09474484072464007\n",
      "Iteration: 1107 lambda_n: 1.0198994034470745 Loss: 0.09464334186806471\n",
      "Iteration: 1108 lambda_n: 1.019831817204402 Loss: 0.09453654554284775\n",
      "Iteration: 1109 lambda_n: 1.0183757820976158 Loss: 0.09442981834456118\n",
      "Iteration: 1110 lambda_n: 0.9451937837395475 Loss: 0.09432330544291086\n",
      "Iteration: 1111 lambda_n: 0.9748447517003258 Loss: 0.09422450375229681\n",
      "Iteration: 1112 lambda_n: 1.019208083963208 Loss: 0.09412265765550795\n",
      "Iteration: 1113 lambda_n: 0.9968017005491632 Loss: 0.09401623611473117\n",
      "Iteration: 1114 lambda_n: 0.9869209470339965 Loss: 0.09391221451680593\n",
      "Iteration: 1115 lambda_n: 0.9717763395050697 Loss: 0.0938092825062292\n",
      "Iteration: 1116 lambda_n: 0.9313729739017282 Loss: 0.09370798697845836\n",
      "Iteration: 1117 lambda_n: 0.91237741193192 Loss: 0.09361095661215288\n",
      "Iteration: 1118 lambda_n: 0.9800963419439743 Loss: 0.09351595560560388\n",
      "Iteration: 1119 lambda_n: 0.9650528293007266 Loss: 0.0934139568115155\n",
      "Iteration: 1120 lambda_n: 1.038230212556417 Loss: 0.09331357967902085\n",
      "Iteration: 1121 lambda_n: 1.0348451899273259 Loss: 0.09320565103638252\n",
      "Iteration: 1122 lambda_n: 0.8982006752357873 Loss: 0.09309813799499195\n",
      "Iteration: 1123 lambda_n: 1.0031607614344376 Loss: 0.09300487586711742\n",
      "Iteration: 1124 lambda_n: 0.9148514461773166 Loss: 0.09290076941064247\n",
      "Iteration: 1125 lambda_n: 0.882655561429497 Loss: 0.09280588153932859\n",
      "Iteration: 1126 lambda_n: 1.021212568601753 Loss: 0.09271438069128976\n",
      "Iteration: 1127 lambda_n: 0.8887372324589732 Loss: 0.0926085703126826\n",
      "Iteration: 1128 lambda_n: 0.9349305293048139 Loss: 0.09251653917766779\n",
      "Iteration: 1129 lambda_n: 0.9257499582569328 Loss: 0.0924197739606373\n",
      "Iteration: 1130 lambda_n: 1.0020242696921042 Loss: 0.09232401007155439\n",
      "Iteration: 1131 lambda_n: 0.8891007775486501 Loss: 0.09222041121303491\n",
      "Iteration: 1132 lambda_n: 0.9385304529063816 Loss: 0.09212853962834876\n",
      "Iteration: 1133 lambda_n: 0.933612477918441 Loss: 0.09203160994363534\n",
      "Iteration: 1134 lambda_n: 0.988117774492557 Loss: 0.09193523989694803\n",
      "Iteration: 1135 lambda_n: 0.9795530434231737 Loss: 0.09183329837829439\n",
      "Iteration: 1136 lambda_n: 1.02605691793007 Loss: 0.09173229753628785\n",
      "Iteration: 1137 lambda_n: 0.8877221011583951 Loss: 0.0916265612388484\n",
      "Iteration: 1138 lambda_n: 0.9446799082763816 Loss: 0.09153513361740095\n",
      "Iteration: 1139 lambda_n: 1.0108918619046252 Loss: 0.09143788953176012\n",
      "Iteration: 1140 lambda_n: 0.9896708403012695 Loss: 0.09133388627364906\n",
      "Iteration: 1141 lambda_n: 0.9209066814958594 Loss: 0.0912321251261757\n",
      "Iteration: 1142 lambda_n: 0.9798401111868001 Loss: 0.09113748789765008\n",
      "Iteration: 1143 lambda_n: 0.9858664378043281 Loss: 0.09103684775816702\n",
      "Iteration: 1144 lambda_n: 1.0127723813881937 Loss: 0.09093564552934684\n",
      "Iteration: 1145 lambda_n: 1.030008153289781 Loss: 0.09083174019671274\n",
      "Iteration: 1146 lambda_n: 0.9850912352450277 Loss: 0.09072612800013918\n",
      "Iteration: 1147 lambda_n: 0.9979988911411756 Loss: 0.0906251808167233\n",
      "Iteration: 1148 lambda_n: 0.9901838356825909 Loss: 0.09052296876853332\n",
      "Iteration: 1149 lambda_n: 1.0177785009171973 Loss: 0.09042161514280567\n",
      "Iteration: 1150 lambda_n: 1.001089561997045 Loss: 0.09031749630560765\n",
      "Iteration: 1151 lambda_n: 0.9380383652100863 Loss: 0.09021514449970959\n",
      "Iteration: 1152 lambda_n: 1.0375950719443192 Loss: 0.09011929391467717\n",
      "Iteration: 1153 lambda_n: 0.8861035045886486 Loss: 0.09001332805376447\n",
      "Iteration: 1154 lambda_n: 0.9878124446808608 Loss: 0.08992288677474769\n",
      "Iteration: 1155 lambda_n: 0.8906554346004063 Loss: 0.08982211627153831\n",
      "Iteration: 1156 lambda_n: 0.9502873182061092 Loss: 0.08973130829415368\n",
      "Iteration: 1157 lambda_n: 1.0309661710292084 Loss: 0.08963447036003155\n",
      "Iteration: 1158 lambda_n: 1.0053448452386697 Loss: 0.08952946876498567\n",
      "Iteration: 1159 lambda_n: 0.9458480190632862 Loss: 0.08942713724976653\n",
      "Iteration: 1160 lambda_n: 1.0103891036817363 Loss: 0.08933091719267956\n",
      "Iteration: 1161 lambda_n: 0.899388348635047 Loss: 0.08922818772714391\n",
      "Iteration: 1162 lambda_n: 0.9296811213556078 Loss: 0.08913679677552436\n",
      "Iteration: 1163 lambda_n: 1.031454665694898 Loss: 0.0890423767140598\n",
      "Iteration: 1164 lambda_n: 1.014833121059061 Loss: 0.08893767698019393\n",
      "Iteration: 1165 lambda_n: 1.0271854156286162 Loss: 0.08883472560343766\n",
      "Iteration: 1166 lambda_n: 1.0086548799309183 Loss: 0.08873058216071748\n",
      "Iteration: 1167 lambda_n: 0.9315170218260808 Loss: 0.08862837796893147\n",
      "Iteration: 1168 lambda_n: 0.9271801831638498 Loss: 0.08853404451550126\n",
      "Iteration: 1169 lambda_n: 1.0230535065081652 Loss: 0.08844020070566798\n",
      "Iteration: 1170 lambda_n: 0.9573384544655642 Loss: 0.08833670906232564\n",
      "Iteration: 1171 lambda_n: 0.9881975007144002 Loss: 0.08823992203133965\n",
      "Iteration: 1172 lambda_n: 0.9392845469127977 Loss: 0.08814007055172851\n",
      "Iteration: 1173 lambda_n: 0.8922175462579465 Loss: 0.08804521540838361\n",
      "Iteration: 1174 lambda_n: 1.0179553334376916 Loss: 0.08795516211788323\n",
      "Iteration: 1175 lambda_n: 0.9414383400007171 Loss: 0.08785247150692455\n",
      "Iteration: 1176 lambda_n: 1.001940242553528 Loss: 0.08775755544525815\n",
      "Iteration: 1177 lambda_n: 0.9598308862502258 Loss: 0.08765659490036871\n",
      "Iteration: 1178 lambda_n: 0.9255177212539706 Loss: 0.08755993340390429\n",
      "Iteration: 1179 lambda_n: 1.0106378298631238 Loss: 0.08746677912570758\n",
      "Iteration: 1180 lambda_n: 0.8915444143918521 Loss: 0.08736511239185757\n",
      "Iteration: 1181 lambda_n: 0.936848010000258 Loss: 0.08727547804910753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1182 lambda_n: 0.9508953655486203 Loss: 0.08718133785457459\n",
      "Iteration: 1183 lambda_n: 0.9579213115048852 Loss: 0.08708583807692713\n",
      "Iteration: 1184 lambda_n: 0.9409010970859532 Loss: 0.08698968577936547\n",
      "Iteration: 1185 lambda_n: 1.0228683219286852 Loss: 0.0868952943271178\n",
      "Iteration: 1186 lambda_n: 0.9685222637805648 Loss: 0.08679273637631688\n",
      "Iteration: 1187 lambda_n: 1.0332305507403485 Loss: 0.08669568486216434\n",
      "Iteration: 1188 lambda_n: 0.9197356207618275 Loss: 0.08659220780518347\n",
      "Iteration: 1189 lambda_n: 0.9299238028336722 Loss: 0.08650015195078271\n",
      "Iteration: 1190 lambda_n: 0.9826475583926698 Loss: 0.08640712619284595\n",
      "Iteration: 1191 lambda_n: 0.8978382061037804 Loss: 0.0863088796040075\n",
      "Iteration: 1192 lambda_n: 0.891086898883985 Loss: 0.0862191633293795\n",
      "Iteration: 1193 lambda_n: 1.023310146765225 Loss: 0.0861301681879597\n",
      "Iteration: 1194 lambda_n: 1.0056945290509693 Loss: 0.08602802129490512\n",
      "Iteration: 1195 lambda_n: 0.9772243149498718 Loss: 0.08592769255562194\n",
      "Iteration: 1196 lambda_n: 0.8991447545736637 Loss: 0.0858302610281353\n",
      "Iteration: 1197 lambda_n: 1.0308453700559794 Loss: 0.08574066493891486\n",
      "Iteration: 1198 lambda_n: 0.9727136366005166 Loss: 0.08563799999435333\n",
      "Iteration: 1199 lambda_n: 0.9706889283614161 Loss: 0.08554118255150575\n",
      "Iteration: 1200 lambda_n: 0.9064349273980495 Loss: 0.08544462149387758\n",
      "Iteration: 1201 lambda_n: 0.9083101175277748 Loss: 0.0853545030447877\n",
      "Iteration: 1202 lambda_n: 1.0158187854814489 Loss: 0.08526424600203245\n",
      "Iteration: 1203 lambda_n: 1.0116181453860436 Loss: 0.08516336018613235\n",
      "Iteration: 1204 lambda_n: 0.9521269786076877 Loss: 0.08506295121950579\n",
      "Iteration: 1205 lambda_n: 0.9418264538297699 Loss: 0.08496850273883513\n",
      "Iteration: 1206 lambda_n: 0.9199885164122241 Loss: 0.0848751280612971\n",
      "Iteration: 1207 lambda_n: 0.9163017375170486 Loss: 0.08478396864368798\n",
      "Iteration: 1208 lambda_n: 0.9322123585382499 Loss: 0.08469322345743382\n",
      "Iteration: 1209 lambda_n: 1.0348103862777174 Loss: 0.08460095223247009\n",
      "Iteration: 1210 lambda_n: 0.955281304255162 Loss: 0.08449858230041896\n",
      "Iteration: 1211 lambda_n: 0.9721084534083304 Loss: 0.08440413685370515\n",
      "Iteration: 1212 lambda_n: 0.9037491575972161 Loss: 0.08430808173144677\n",
      "Iteration: 1213 lambda_n: 0.9853853686289628 Loss: 0.08421883191452748\n",
      "Iteration: 1214 lambda_n: 1.003633581168159 Loss: 0.08412157218922683\n",
      "Iteration: 1215 lambda_n: 0.9968823738279775 Loss: 0.08402256878631091\n",
      "Iteration: 1216 lambda_n: 0.9702975238215447 Loss: 0.08392428934935589\n",
      "Iteration: 1217 lambda_n: 0.9690221021583841 Loss: 0.0838286867867452\n",
      "Iteration: 1218 lambda_n: 0.9754925570774512 Loss: 0.08373326440532892\n",
      "Iteration: 1219 lambda_n: 0.8885323666732524 Loss: 0.08363725970126139\n",
      "Iteration: 1220 lambda_n: 0.9898282526502595 Loss: 0.0835498631685588\n",
      "Iteration: 1221 lambda_n: 1.0199377922123778 Loss: 0.08345255463274913\n",
      "Iteration: 1222 lambda_n: 0.9168514454705872 Loss: 0.08335234475717497\n",
      "Iteration: 1223 lambda_n: 0.9628951984348887 Loss: 0.08326231691269273\n",
      "Iteration: 1224 lambda_n: 0.8908448944902645 Loss: 0.08316781932256169\n",
      "Iteration: 1225 lambda_n: 0.9167785478153538 Loss: 0.08308044204483314\n",
      "Iteration: 1226 lambda_n: 0.9319238990637835 Loss: 0.08299056855254724\n",
      "Iteration: 1227 lambda_n: 0.9223861654383679 Loss: 0.08289925991274227\n",
      "Iteration: 1228 lambda_n: 0.9138695421114177 Loss: 0.08280893552278323\n",
      "Iteration: 1229 lambda_n: 0.9226701345972712 Loss: 0.08271949391329635\n",
      "Iteration: 1230 lambda_n: 0.9169768662685766 Loss: 0.08262923986850085\n",
      "Iteration: 1231 lambda_n: 1.0236730721657026 Loss: 0.08253959171095282\n",
      "Iteration: 1232 lambda_n: 0.9781335915200591 Loss: 0.08243956737231535\n",
      "Iteration: 1233 lambda_n: 0.9875853708998528 Loss: 0.08234405050592297\n",
      "Iteration: 1234 lambda_n: 0.9071288461162132 Loss: 0.0822476666510688\n",
      "Iteration: 1235 lambda_n: 0.9757508959007528 Loss: 0.08215918647340623\n",
      "Iteration: 1236 lambda_n: 0.9914522019610118 Loss: 0.08206406461292629\n",
      "Iteration: 1237 lambda_n: 0.979802276385769 Loss: 0.08196746820423857\n",
      "Iteration: 1238 lambda_n: 1.035543003476576 Loss: 0.08187206302719964\n",
      "Iteration: 1239 lambda_n: 1.0375656369217123 Loss: 0.08177128933899473\n",
      "Iteration: 1240 lambda_n: 0.8899212010677839 Loss: 0.08167038104237807\n",
      "Iteration: 1241 lambda_n: 0.9992129586279093 Loss: 0.0815838846194416\n",
      "Iteration: 1242 lambda_n: 0.9486093908929604 Loss: 0.08148681760390634\n",
      "Iteration: 1243 lambda_n: 1.0160286481077059 Loss: 0.08139472100435591\n",
      "Iteration: 1244 lambda_n: 0.929347313830633 Loss: 0.08129613512315376\n",
      "Iteration: 1245 lambda_n: 1.0132871399807946 Loss: 0.08120601419968485\n",
      "Iteration: 1246 lambda_n: 0.9288479373041961 Loss: 0.08110780842854344\n",
      "Iteration: 1247 lambda_n: 0.9694988674805186 Loss: 0.0810178403822133\n",
      "Iteration: 1248 lambda_n: 0.9082766326546602 Loss: 0.0809239872424142\n",
      "Iteration: 1249 lambda_n: 1.010314903607414 Loss: 0.08083611143197551\n",
      "Iteration: 1250 lambda_n: 0.9564615017526933 Loss: 0.08073841711149626\n",
      "Iteration: 1251 lambda_n: 0.8984766496991743 Loss: 0.08064598589398618\n",
      "Iteration: 1252 lambda_n: 0.9363449818268519 Loss: 0.08055920770690464\n",
      "Iteration: 1253 lambda_n: 0.9872860653787109 Loss: 0.0804688209535691\n",
      "Iteration: 1254 lambda_n: 1.014077430306272 Loss: 0.08037357057872804\n",
      "Iteration: 1255 lambda_n: 0.9236256163948002 Loss: 0.08027579356262493\n",
      "Iteration: 1256 lambda_n: 0.9715980312825893 Loss: 0.0801867916216121\n",
      "Iteration: 1257 lambda_n: 0.8847935177999655 Loss: 0.08009321919919164\n",
      "Iteration: 1258 lambda_n: 1.0040667134699717 Loss: 0.08000805604646642\n",
      "Iteration: 1259 lambda_n: 0.9184993395126503 Loss: 0.07991146472262187\n",
      "Iteration: 1260 lambda_n: 0.8993015492019747 Loss: 0.07982315792363429\n",
      "Iteration: 1261 lambda_n: 0.9784902014111183 Loss: 0.07973674456423052\n",
      "Iteration: 1262 lambda_n: 1.016372160732995 Loss: 0.07964277340256556\n",
      "Iteration: 1263 lambda_n: 1.0225966267695499 Loss: 0.07954522197410556\n",
      "Iteration: 1264 lambda_n: 0.8838420981192932 Loss: 0.07944713333213685\n",
      "Iteration: 1265 lambda_n: 0.9427162399728808 Loss: 0.07936240579768873\n",
      "Iteration: 1266 lambda_n: 1.0252087590383263 Loss: 0.07927208299132076\n",
      "Iteration: 1267 lambda_n: 0.922299839423835 Loss: 0.07917391293131384\n",
      "Iteration: 1268 lambda_n: 0.9222164832500372 Loss: 0.0790856512302516\n",
      "Iteration: 1269 lambda_n: 1.0179208397962065 Loss: 0.07899744675868121\n",
      "Iteration: 1270 lambda_n: 0.9837736661371133 Loss: 0.07890014367763093\n",
      "Iteration: 1271 lambda_n: 0.9877120094249473 Loss: 0.07880616252397966\n",
      "Iteration: 1272 lambda_n: 0.8954199127618216 Loss: 0.07871186143718886\n",
      "Iteration: 1273 lambda_n: 1.02274879348039 Loss: 0.07862642260712531\n",
      "Iteration: 1274 lambda_n: 0.9562721826053864 Loss: 0.07852888818294614\n",
      "Iteration: 1275 lambda_n: 0.8947601432529745 Loss: 0.0784377495785798\n",
      "Iteration: 1276 lambda_n: 1.0016818050916183 Loss: 0.07835252268980755\n",
      "Iteration: 1277 lambda_n: 0.9914772298149731 Loss: 0.07825716396703059\n",
      "Iteration: 1278 lambda_n: 0.992719839825003 Loss: 0.078162834193614\n",
      "Iteration: 1279 lambda_n: 0.8971987467359479 Loss: 0.0780684432442003\n",
      "Iteration: 1280 lambda_n: 0.9067443422067775 Loss: 0.07798318584397118\n",
      "Iteration: 1281 lambda_n: 0.9563591190126344 Loss: 0.07789706856691322\n",
      "Iteration: 1282 lambda_n: 0.9317682794373194 Loss: 0.0778062897244426\n",
      "Iteration: 1283 lambda_n: 0.9259366530974729 Loss: 0.07771789659710493\n",
      "Iteration: 1284 lambda_n: 0.9650118353048417 Loss: 0.07763010668232063\n",
      "Iteration: 1285 lambda_n: 1.0377378372538009 Loss: 0.07753866401588465\n",
      "Iteration: 1286 lambda_n: 0.956358343513569 Loss: 0.07744038851616776\n",
      "Iteration: 1287 lambda_n: 1.0141279821565752 Loss: 0.0773498768757221\n",
      "Iteration: 1288 lambda_n: 0.9274360989866557 Loss: 0.0772539544285817\n",
      "Iteration: 1289 lambda_n: 0.9894622757782081 Loss: 0.07716628590255556\n",
      "Iteration: 1290 lambda_n: 0.9551310814767309 Loss: 0.07707280780786151\n",
      "Iteration: 1291 lambda_n: 1.0220135238717087 Loss: 0.07698262774988324\n",
      "Iteration: 1292 lambda_n: 0.987133464323039 Loss: 0.07688618998136551\n",
      "Iteration: 1293 lambda_n: 0.8982313584956273 Loss: 0.07679310187026268\n",
      "Iteration: 1294 lambda_n: 0.899791252644537 Loss: 0.0767084483384047\n",
      "Iteration: 1295 lambda_n: 1.0220240721915266 Loss: 0.076623694740006\n",
      "Iteration: 1296 lambda_n: 1.034512615924626 Loss: 0.07652748193210566\n",
      "Iteration: 1297 lambda_n: 1.030920141653461 Loss: 0.07643015496893943\n",
      "Iteration: 1298 lambda_n: 1.0276569786965428 Loss: 0.07633322793755101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1299 lambda_n: 1.036940430260431 Loss: 0.0762366692648496\n",
      "Iteration: 1300 lambda_n: 0.9501304215762685 Loss: 0.07613930033185717\n",
      "Iteration: 1301 lambda_n: 0.9788967943156364 Loss: 0.0760501396366087\n",
      "Iteration: 1302 lambda_n: 1.024288490281558 Loss: 0.075958333767082\n",
      "Iteration: 1303 lambda_n: 0.9621906734156671 Loss: 0.07586232946788056\n",
      "Iteration: 1304 lambda_n: 0.9310935520670638 Loss: 0.0757722024212355\n",
      "Iteration: 1305 lambda_n: 0.9203034745966019 Loss: 0.07568504015261528\n",
      "Iteration: 1306 lambda_n: 0.9784011830543498 Loss: 0.07559893779325448\n",
      "Iteration: 1307 lambda_n: 1.0088008574811238 Loss: 0.07550745269634357\n",
      "Iteration: 1308 lambda_n: 0.9247082060752613 Loss: 0.07541318278135277\n",
      "Iteration: 1309 lambda_n: 0.9015861863311271 Loss: 0.07532682495235238\n",
      "Iteration: 1310 lambda_n: 0.9175478156044229 Loss: 0.0752426749212827\n",
      "Iteration: 1311 lambda_n: 0.9793004959794742 Loss: 0.07515708342468035\n",
      "Iteration: 1312 lambda_n: 1.0301931934037196 Loss: 0.07506578426124201\n",
      "Iteration: 1313 lambda_n: 0.9408697897041737 Loss: 0.07496979962574737\n",
      "Iteration: 1314 lambda_n: 0.937108376598344 Loss: 0.07488219337945635\n",
      "Iteration: 1315 lambda_n: 0.9135711256362502 Loss: 0.07479498880607582\n",
      "Iteration: 1316 lambda_n: 1.0345665637555088 Loss: 0.07471002438210674\n",
      "Iteration: 1317 lambda_n: 0.9646625641095896 Loss: 0.07461386313162548\n",
      "Iteration: 1318 lambda_n: 0.9952442675924937 Loss: 0.07452425719813957\n",
      "Iteration: 1319 lambda_n: 1.0176524538434937 Loss: 0.07443186690742226\n",
      "Iteration: 1320 lambda_n: 0.9632517838136585 Loss: 0.07433745580768804\n",
      "Iteration: 1321 lambda_n: 0.9987641570144233 Loss: 0.07424814862753173\n",
      "Iteration: 1322 lambda_n: 0.9360049861355002 Loss: 0.07415560549264169\n",
      "Iteration: 1323 lambda_n: 1.0151681872425509 Loss: 0.0740689318001794\n",
      "Iteration: 1324 lambda_n: 0.9243128142468688 Loss: 0.073974983842707\n",
      "Iteration: 1325 lambda_n: 0.9096355388981302 Loss: 0.07388949842333725\n",
      "Iteration: 1326 lambda_n: 0.9695242396206464 Loss: 0.07380541963298151\n",
      "Iteration: 1327 lambda_n: 1.0096055213556736 Loss: 0.07371585738569934\n",
      "Iteration: 1328 lambda_n: 0.9887570288653108 Loss: 0.07362265025246514\n",
      "Iteration: 1329 lambda_n: 0.8834922319767323 Loss: 0.073531426319203\n",
      "Iteration: 1330 lambda_n: 0.9936389423224111 Loss: 0.07344996489123401\n",
      "Iteration: 1331 lambda_n: 0.9053060259588899 Loss: 0.073358399880497\n",
      "Iteration: 1332 lambda_n: 1.0138898160198275 Loss: 0.07327502716860781\n",
      "Iteration: 1333 lambda_n: 0.926811770277777 Loss: 0.07318170939946084\n",
      "Iteration: 1334 lambda_n: 1.0074956016882262 Loss: 0.07309646096458317\n",
      "Iteration: 1335 lambda_n: 0.9428166506713235 Loss: 0.07300384679317705\n",
      "Iteration: 1336 lambda_n: 0.9665898087000332 Loss: 0.07291723377541508\n",
      "Iteration: 1337 lambda_n: 0.9428996870900579 Loss: 0.07282849071384921\n",
      "Iteration: 1338 lambda_n: 0.9397961396359261 Loss: 0.0727419762644532\n",
      "Iteration: 1339 lambda_n: 0.9593752562752498 Loss: 0.07265579887318796\n",
      "Iteration: 1340 lambda_n: 0.9185221719354042 Loss: 0.07256787952379354\n",
      "Iteration: 1341 lambda_n: 0.9201905682399909 Loss: 0.07248375585965738\n",
      "Iteration: 1342 lambda_n: 0.9268346732568462 Loss: 0.07239952940658394\n",
      "Iteration: 1343 lambda_n: 0.8942394886278454 Loss: 0.07231474534322066\n",
      "Iteration: 1344 lambda_n: 1.014701777363216 Loss: 0.07223299186730797\n",
      "Iteration: 1345 lambda_n: 0.993844123520303 Loss: 0.07214028017142134\n",
      "Iteration: 1346 lambda_n: 0.9901531108724902 Loss: 0.07204953388216417\n",
      "Iteration: 1347 lambda_n: 0.904942599472384 Loss: 0.07195918302027256\n",
      "Iteration: 1348 lambda_n: 0.9893305544250427 Loss: 0.07187666021931531\n",
      "Iteration: 1349 lambda_n: 1.0129163315111152 Loss: 0.07178649588011635\n",
      "Iteration: 1350 lambda_n: 0.9671035317138932 Loss: 0.0716942418565324\n",
      "Iteration: 1351 lambda_n: 1.0380597652497643 Loss: 0.07160621837750866\n",
      "Iteration: 1352 lambda_n: 0.961960993342627 Loss: 0.07151179709200493\n",
      "Iteration: 1353 lambda_n: 0.9398613402312781 Loss: 0.07142435674230135\n",
      "Iteration: 1354 lambda_n: 0.977980527489653 Loss: 0.07133897907334222\n",
      "Iteration: 1355 lambda_n: 0.9115578991675579 Loss: 0.0712501939173208\n",
      "Iteration: 1356 lambda_n: 0.9193877219801114 Loss: 0.0711674917808333\n",
      "Iteration: 1357 lambda_n: 1.0209712773780304 Loss: 0.0710841295713563\n",
      "Iteration: 1358 lambda_n: 1.0011468690617802 Loss: 0.07099161381308529\n",
      "Iteration: 1359 lambda_n: 0.9072196266174695 Loss: 0.07090095570120601\n",
      "Iteration: 1360 lambda_n: 0.8993233876153433 Loss: 0.07081885702888094\n",
      "Iteration: 1361 lambda_n: 0.9527665400131574 Loss: 0.07073752199828814\n",
      "Iteration: 1362 lambda_n: 0.9314123160937611 Loss: 0.07065140564224637\n",
      "Iteration: 1363 lambda_n: 0.9917312851007338 Loss: 0.0705672728018993\n",
      "Iteration: 1364 lambda_n: 0.945416769688395 Loss: 0.07047774779788246\n",
      "Iteration: 1365 lambda_n: 0.9485506941554626 Loss: 0.0703924600441788\n",
      "Iteration: 1366 lambda_n: 0.9273050549607119 Loss: 0.07030694395093011\n",
      "Iteration: 1367 lambda_n: 0.9535756463923654 Loss: 0.07022339646150964\n",
      "Iteration: 1368 lambda_n: 0.9324935881630135 Loss: 0.07013753603088496\n",
      "Iteration: 1369 lambda_n: 1.0162438875848046 Loss: 0.07005362779198382\n",
      "Iteration: 1370 lambda_n: 0.9125220948551966 Loss: 0.06996224200395372\n",
      "Iteration: 1371 lambda_n: 1.0106671344563423 Loss: 0.0698802391875054\n",
      "Iteration: 1372 lambda_n: 0.9638673751175783 Loss: 0.06978947396992145\n",
      "Iteration: 1373 lambda_n: 0.9908772508163886 Loss: 0.06970297098696247\n",
      "Iteration: 1374 lambda_n: 1.0378797773313075 Loss: 0.06961410284754749\n",
      "Iteration: 1375 lambda_n: 1.016890471260531 Loss: 0.06952108291356206\n",
      "Iteration: 1376 lambda_n: 1.0346867710563166 Loss: 0.06943000897849379\n",
      "Iteration: 1377 lambda_n: 0.9786145020284844 Loss: 0.06933740632116167\n",
      "Iteration: 1378 lambda_n: 0.9410434226223275 Loss: 0.06924988416654507\n",
      "Iteration: 1379 lambda_n: 0.9697052463431913 Loss: 0.0691657789337375\n",
      "Iteration: 1380 lambda_n: 1.0267957379159494 Loss: 0.06907916902909775\n",
      "Iteration: 1381 lambda_n: 0.9842792143819413 Loss: 0.06898752261500395\n",
      "Iteration: 1382 lambda_n: 0.9280934102152958 Loss: 0.06889973367614649\n",
      "Iteration: 1383 lambda_n: 0.8858565104911971 Loss: 0.06881701264806878\n",
      "Iteration: 1384 lambda_n: 0.9997138038914241 Loss: 0.06873810738001421\n",
      "Iteration: 1385 lambda_n: 0.9563350550838151 Loss: 0.06864911738757339\n",
      "Iteration: 1386 lambda_n: 1.0278515626941997 Loss: 0.06856404860217424\n",
      "Iteration: 1387 lambda_n: 0.9080246541981082 Loss: 0.06847268107589721\n",
      "Iteration: 1388 lambda_n: 1.0159160774742257 Loss: 0.06839202318406531\n",
      "Iteration: 1389 lambda_n: 0.9506167349701244 Loss: 0.06830184128048922\n",
      "Iteration: 1390 lambda_n: 0.9054003891812276 Loss: 0.06821751682277351\n",
      "Iteration: 1391 lambda_n: 0.9214670576580775 Loss: 0.06813725784533176\n",
      "Iteration: 1392 lambda_n: 1.0270396305226295 Loss: 0.06805562827623556\n",
      "Iteration: 1393 lambda_n: 1.029080371482231 Loss: 0.06796470842225431\n",
      "Iteration: 1394 lambda_n: 0.9172438237669339 Loss: 0.06787367611461162\n",
      "Iteration: 1395 lambda_n: 0.9720196590656888 Loss: 0.0677925968158314\n",
      "Iteration: 1396 lambda_n: 0.9017021280628051 Loss: 0.06770673414171029\n",
      "Iteration: 1397 lambda_n: 0.9678249964400708 Loss: 0.06762713932831327\n",
      "Iteration: 1398 lambda_n: 1.02839658321034 Loss: 0.06754176555131966\n",
      "Iteration: 1399 lambda_n: 0.9535800572374125 Loss: 0.0674511147815741\n",
      "Iteration: 1400 lambda_n: 0.9676917297440881 Loss: 0.06736797036630937\n",
      "Iteration: 1401 lambda_n: 0.9398983909808436 Loss: 0.0672844050883989\n",
      "Iteration: 1402 lambda_n: 0.884473155957985 Loss: 0.0672033381988888\n",
      "Iteration: 1403 lambda_n: 1.0037823841603772 Loss: 0.06712711112604067\n",
      "Iteration: 1404 lambda_n: 0.9749923929925288 Loss: 0.06704066511138636\n",
      "Iteration: 1405 lambda_n: 0.9755156686628987 Loss: 0.0669567670413491\n",
      "Iteration: 1406 lambda_n: 0.9909516377447845 Loss: 0.06687289056362006\n",
      "Iteration: 1407 lambda_n: 0.9798408878487408 Loss: 0.06678775450945493\n",
      "Iteration: 1408 lambda_n: 1.025970922171588 Loss: 0.06670364055695717\n",
      "Iteration: 1409 lambda_n: 1.0238851057972882 Loss: 0.06661563679207186\n",
      "Iteration: 1410 lambda_n: 0.9210876880129222 Loss: 0.06652788469431804\n",
      "Iteration: 1411 lambda_n: 0.9575056485124697 Loss: 0.06644900725819096\n",
      "Iteration: 1412 lambda_n: 0.8933389297052042 Loss: 0.06636707221588159\n",
      "Iteration: 1413 lambda_n: 0.9384043096509188 Loss: 0.06629068635911782\n",
      "Iteration: 1414 lambda_n: 1.0102069062304115 Loss: 0.06621050497918082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1415 lambda_n: 0.8849732367207198 Loss: 0.06612425393742209\n",
      "Iteration: 1416 lambda_n: 0.8865175352024983 Loss: 0.0660487554737613\n",
      "Iteration: 1417 lambda_n: 0.9890530731473821 Loss: 0.06597317873864611\n",
      "Iteration: 1418 lambda_n: 0.9987468084021738 Loss: 0.0658889211683968\n",
      "Iteration: 1419 lambda_n: 1.035162205918549 Loss: 0.06580390492738294\n",
      "Iteration: 1420 lambda_n: 0.88949304636777 Loss: 0.06571585924862165\n",
      "Iteration: 1421 lambda_n: 0.9099832210533194 Loss: 0.0656402646672417\n",
      "Iteration: 1422 lambda_n: 1.0244860348762581 Loss: 0.06556298346664988\n",
      "Iteration: 1423 lambda_n: 0.9087531225429116 Loss: 0.06547604174339333\n",
      "Iteration: 1424 lambda_n: 0.9551497199134654 Loss: 0.06539898329834677\n",
      "Iteration: 1425 lambda_n: 0.9455235316931846 Loss: 0.06531804922583814\n",
      "Iteration: 1426 lambda_n: 1.032185994696985 Loss: 0.0652379912564173\n",
      "Iteration: 1427 lambda_n: 1.01983154573862 Loss: 0.06515066153539352\n",
      "Iteration: 1428 lambda_n: 0.9191443871510753 Loss: 0.0650644472713725\n",
      "Iteration: 1429 lambda_n: 0.9181003648403566 Loss: 0.06498680659880832\n",
      "Iteration: 1430 lambda_n: 0.8853807836782904 Loss: 0.06490931025813122\n",
      "Iteration: 1431 lambda_n: 0.9232023914799307 Loss: 0.06483462952722846\n",
      "Iteration: 1432 lambda_n: 1.0310321662967634 Loss: 0.06475681310448808\n",
      "Iteration: 1433 lambda_n: 1.008055650478786 Loss: 0.06466997172103442\n",
      "Iteration: 1434 lambda_n: 0.9235614546069344 Loss: 0.06458513416216738\n",
      "Iteration: 1435 lambda_n: 1.0117622470677479 Loss: 0.06450746844786331\n",
      "Iteration: 1436 lambda_n: 0.9411549948284237 Loss: 0.06442244796646278\n",
      "Iteration: 1437 lambda_n: 0.9971497817109902 Loss: 0.0643434229212377\n",
      "Iteration: 1438 lambda_n: 0.9838039326630286 Loss: 0.06425975839950701\n",
      "Iteration: 1439 lambda_n: 0.9536915160952095 Loss: 0.06417727796360943\n",
      "Iteration: 1440 lambda_n: 0.9163665282448167 Loss: 0.06409738339622671\n",
      "Iteration: 1441 lambda_n: 0.9242950941163678 Loss: 0.0640206726451049\n",
      "Iteration: 1442 lambda_n: 0.956036334292695 Loss: 0.0639433536438831\n",
      "Iteration: 1443 lambda_n: 0.982987528468868 Loss: 0.06386343741743297\n",
      "Iteration: 1444 lambda_n: 0.9987886723278435 Loss: 0.0637813298584277\n",
      "Iteration: 1445 lambda_n: 0.9668516724385484 Loss: 0.06369796657747533\n",
      "Iteration: 1446 lambda_n: 0.9855985943886898 Loss: 0.06361733149965978\n",
      "Iteration: 1447 lambda_n: 1.0066806156938881 Loss: 0.06353519504934406\n",
      "Iteration: 1448 lambda_n: 0.8883933163403174 Loss: 0.06345136631033077\n",
      "Iteration: 1449 lambda_n: 0.989690250258001 Loss: 0.06337744475243134\n",
      "Iteration: 1450 lambda_n: 0.9809562659543762 Loss: 0.06329515228803956\n",
      "Iteration: 1451 lambda_n: 0.926576300325941 Loss: 0.06321364881456625\n",
      "Iteration: 1452 lambda_n: 0.9989417841290565 Loss: 0.06313672186829009\n",
      "Iteration: 1453 lambda_n: 0.9681415155973824 Loss: 0.06305384733755348\n",
      "Iteration: 1454 lambda_n: 0.9656992124861512 Loss: 0.06297359020434307\n",
      "Iteration: 1455 lambda_n: 0.9213014766460927 Loss: 0.06289359577322373\n",
      "Iteration: 1456 lambda_n: 0.9796482721735905 Loss: 0.0628173359771647\n",
      "Iteration: 1457 lambda_n: 0.9401746053139355 Loss: 0.06273630513058781\n",
      "Iteration: 1458 lambda_n: 0.9336155283467962 Loss: 0.06265859817430396\n",
      "Iteration: 1459 lambda_n: 0.9982355386094927 Loss: 0.06258148962700216\n",
      "Iteration: 1460 lambda_n: 0.9845285646317027 Loss: 0.06249910436371268\n",
      "Iteration: 1461 lambda_n: 0.9477045698429262 Loss: 0.062417913204692005\n",
      "Iteration: 1462 lambda_n: 1.0319793835982347 Loss: 0.062339818231424926\n",
      "Iteration: 1463 lambda_n: 0.9281552000052496 Loss: 0.06225484196875732\n",
      "Iteration: 1464 lambda_n: 1.0057814483650438 Loss: 0.06217847524862085\n",
      "Iteration: 1465 lambda_n: 0.9605833908154372 Loss: 0.062095781891483307\n",
      "Iteration: 1466 lambda_n: 1.0062637354327604 Loss: 0.06201686586962886\n",
      "Iteration: 1467 lambda_n: 0.9163223879689215 Loss: 0.06193425902913866\n",
      "Iteration: 1468 lambda_n: 0.8893730368257953 Loss: 0.06185909368662002\n",
      "Iteration: 1469 lambda_n: 1.014377377419314 Loss: 0.061786190616011934\n",
      "Iteration: 1470 lambda_n: 1.0346613731860692 Loss: 0.061703099279012436\n",
      "Iteration: 1471 lambda_n: 0.922261156225106 Loss: 0.061618413279149416\n",
      "Iteration: 1472 lambda_n: 0.9972204087629808 Loss: 0.0615429867593337\n",
      "Iteration: 1473 lambda_n: 0.9144739251364712 Loss: 0.06146148876369413\n",
      "Iteration: 1474 lambda_n: 0.9885922605101316 Loss: 0.06138681034915849\n",
      "Iteration: 1475 lambda_n: 0.9875491545314276 Loss: 0.061306137161388094\n",
      "Iteration: 1476 lambda_n: 0.9673333655038721 Loss: 0.06122561088120288\n",
      "Iteration: 1477 lambda_n: 0.9548670594745913 Loss: 0.06114679327032628\n",
      "Iteration: 1478 lambda_n: 0.9686739532985741 Loss: 0.061069049676491634\n",
      "Iteration: 1479 lambda_n: 1.0210841897690026 Loss: 0.06099024049668237\n",
      "Iteration: 1480 lambda_n: 0.9299507699594406 Loss: 0.06090723026798459\n",
      "Iteration: 1481 lambda_n: 0.9332070417419895 Loss: 0.06083168794824229\n",
      "Iteration: 1482 lambda_n: 0.8859476757569942 Loss: 0.06075593584797334\n",
      "Iteration: 1483 lambda_n: 0.9116141532055445 Loss: 0.06068407167128657\n",
      "Iteration: 1484 lambda_n: 0.9664122424173132 Loss: 0.06061017661300233\n",
      "Iteration: 1485 lambda_n: 0.8935823029433602 Loss: 0.06053189558355707\n",
      "Iteration: 1486 lambda_n: 0.9392333368700216 Loss: 0.06045956760283172\n",
      "Iteration: 1487 lambda_n: 0.9841825603201619 Loss: 0.06038359771547639\n",
      "Iteration: 1488 lambda_n: 0.9902347633650922 Loss: 0.06030405060646126\n",
      "Iteration: 1489 lambda_n: 0.9299929612602529 Loss: 0.06022407558104567\n",
      "Iteration: 1490 lambda_n: 0.899992012534941 Loss: 0.06014902316948229\n",
      "Iteration: 1491 lambda_n: 0.9676916672293767 Loss: 0.06007644413987549\n",
      "Iteration: 1492 lambda_n: 0.9897465890597505 Loss: 0.05999846076341539\n",
      "Iteration: 1493 lambda_n: 0.8969158076292991 Loss: 0.05991876027823409\n",
      "Iteration: 1494 lambda_n: 0.9993516333335338 Loss: 0.0598465898995713\n",
      "Iteration: 1495 lambda_n: 1.0150193613849334 Loss: 0.059766234121229306\n",
      "Iteration: 1496 lambda_n: 1.024429574123264 Loss: 0.05968468216834509\n",
      "Iteration: 1497 lambda_n: 0.9815278047745041 Loss: 0.059602439287552676\n",
      "Iteration: 1498 lambda_n: 1.0123891917391634 Loss: 0.059523703088526024\n",
      "Iteration: 1499 lambda_n: 1.0089250126373568 Loss: 0.05944255365651132\n",
      "Iteration: 1500 lambda_n: 0.9462948048895297 Loss: 0.05936174567399052\n",
      "Iteration: 1501 lambda_n: 1.0118933933624332 Loss: 0.05928601297471288\n",
      "Iteration: 1502 lambda_n: 0.891282123927383 Loss: 0.05920509079033031\n",
      "Iteration: 1503 lambda_n: 1.0224153911244405 Loss: 0.059133869223574595\n",
      "Iteration: 1504 lambda_n: 1.004252379394972 Loss: 0.05905222707468971\n",
      "Iteration: 1505 lambda_n: 0.9952625063653838 Loss: 0.058972099119853065\n",
      "Iteration: 1506 lambda_n: 0.8846034651009468 Loss: 0.058892750660273364\n",
      "Iteration: 1507 lambda_n: 1.0306323951475609 Loss: 0.058822278493404566\n",
      "Iteration: 1508 lambda_n: 1.0126958150113718 Loss: 0.058740231183690574\n",
      "Iteration: 1509 lambda_n: 1.0378845427810388 Loss: 0.05865967658827719\n",
      "Iteration: 1510 lambda_n: 0.9823418647430148 Loss: 0.05857718405166146\n",
      "Iteration: 1511 lambda_n: 0.9306616337397812 Loss: 0.05849916901831342\n",
      "Iteration: 1512 lambda_n: 1.0075187633156728 Loss: 0.05842531467633418\n",
      "Iteration: 1513 lambda_n: 1.0192233544671205 Loss: 0.05834542029667855\n",
      "Iteration: 1514 lambda_n: 0.9121902145298988 Loss: 0.058264661733791516\n",
      "Iteration: 1515 lambda_n: 0.920082720969041 Loss: 0.058192440774363516\n",
      "Iteration: 1516 lambda_n: 1.0141643208847486 Loss: 0.058119647167104244\n",
      "Iteration: 1517 lambda_n: 0.9012788168174619 Loss: 0.05803946911882083\n",
      "Iteration: 1518 lambda_n: 0.9237787539774318 Loss: 0.05796827132669105\n",
      "Iteration: 1519 lambda_n: 0.9905587907459568 Loss: 0.05789534803728851\n",
      "Iteration: 1520 lambda_n: 1.022349705328824 Loss: 0.05781721061929096\n",
      "Iteration: 1521 lambda_n: 0.9818931188215063 Loss: 0.057736628673155564\n",
      "Iteration: 1522 lambda_n: 0.9471461318154304 Loss: 0.057659297422853785\n",
      "Iteration: 1523 lambda_n: 0.8958327467293182 Loss: 0.05758476011988106\n",
      "Iteration: 1524 lambda_n: 0.891760539212868 Loss: 0.057514313174086236\n",
      "Iteration: 1525 lambda_n: 0.897620105963347 Loss: 0.057444235979165724\n",
      "Iteration: 1526 lambda_n: 0.9050388936440305 Loss: 0.05737374802169378\n",
      "Iteration: 1527 lambda_n: 0.9523827562378897 Loss: 0.05730272794814338\n",
      "Iteration: 1528 lambda_n: 0.9810433544037004 Loss: 0.057228046640373634\n",
      "Iteration: 1529 lambda_n: 0.9493396909765123 Loss: 0.05715117614137245\n",
      "Iteration: 1530 lambda_n: 1.0198878215151272 Loss: 0.057076847236024054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1531 lambda_n: 0.9014056273901382 Loss: 0.05699705553640621\n",
      "Iteration: 1532 lambda_n: 1.0065702164065504 Loss: 0.05692658922818099\n",
      "Iteration: 1533 lambda_n: 0.9679440407829859 Loss: 0.056847959184969245\n",
      "Iteration: 1534 lambda_n: 0.8977850851806227 Loss: 0.05677240648503321\n",
      "Iteration: 1535 lambda_n: 0.9679628860940186 Loss: 0.05670238320904651\n",
      "Iteration: 1536 lambda_n: 0.974862930082231 Loss: 0.056626940969644234\n",
      "Iteration: 1537 lambda_n: 0.9686817186943033 Loss: 0.056551019502925653\n",
      "Iteration: 1538 lambda_n: 0.9381194028798898 Loss: 0.0564756378835542\n",
      "Iteration: 1539 lambda_n: 1.034342696480806 Loss: 0.05640269058993509\n",
      "Iteration: 1540 lambda_n: 1.029298469911676 Loss: 0.056322322290571006\n",
      "Iteration: 1541 lambda_n: 0.974876893790665 Loss: 0.056242411854704334\n",
      "Iteration: 1542 lambda_n: 1.0293636202840244 Loss: 0.05616678808754715\n",
      "Iteration: 1543 lambda_n: 0.9736108714078522 Loss: 0.05608700045776804\n",
      "Iteration: 1544 lambda_n: 0.987512903118382 Loss: 0.05601159582736575\n",
      "Iteration: 1545 lambda_n: 0.9512853937056241 Loss: 0.055935174251701275\n",
      "Iteration: 1546 lambda_n: 1.0118096828967988 Loss: 0.05586161409120664\n",
      "Iteration: 1547 lambda_n: 0.9228473863079979 Loss: 0.055783434112617054\n",
      "Iteration: 1548 lambda_n: 0.9087041812855492 Loss: 0.05571218498849147\n",
      "Iteration: 1549 lambda_n: 1.0318837678606005 Loss: 0.05564207964353692\n",
      "Iteration: 1550 lambda_n: 0.929587718982286 Loss: 0.05556253072731129\n",
      "Iteration: 1551 lambda_n: 1.0232350767359402 Loss: 0.05549092631505403\n",
      "Iteration: 1552 lambda_n: 0.9563664746693123 Loss: 0.0554121684973033\n",
      "Iteration: 1553 lambda_n: 1.0028471707520874 Loss: 0.05533861748309743\n",
      "Iteration: 1554 lambda_n: 1.0371771757547872 Loss: 0.055261551815247584\n",
      "Iteration: 1555 lambda_n: 0.8840341888054533 Loss: 0.05518191293192124\n",
      "Iteration: 1556 lambda_n: 0.9108504852738789 Loss: 0.05511408835238064\n",
      "Iteration: 1557 lambda_n: 0.9094473844967691 Loss: 0.05504425662774688\n",
      "Iteration: 1558 lambda_n: 0.8911845783757829 Loss: 0.054974583867891294\n",
      "Iteration: 1559 lambda_n: 0.9007468015125694 Loss: 0.05490636033877449\n",
      "Iteration: 1560 lambda_n: 0.958267289249901 Loss: 0.05483745471809947\n",
      "Iteration: 1561 lambda_n: 1.0361608777207778 Loss: 0.054764203113464915\n",
      "Iteration: 1562 lambda_n: 0.9096213668368601 Loss: 0.054685059799212496\n",
      "Iteration: 1563 lambda_n: 0.8976432011632121 Loss: 0.054615638933977174\n",
      "Iteration: 1564 lambda_n: 1.0020277429228077 Loss: 0.05454718282358722\n",
      "Iteration: 1565 lambda_n: 1.0007329696215985 Loss: 0.05447082326154792\n",
      "Iteration: 1566 lambda_n: 0.8998922044975893 Loss: 0.05439462468195086\n",
      "Iteration: 1567 lambda_n: 1.005750958748805 Loss: 0.054326159307313805\n",
      "Iteration: 1568 lambda_n: 0.9273975154191475 Loss: 0.05424969756092588\n",
      "Iteration: 1569 lambda_n: 0.9075093278241516 Loss: 0.05417924974672849\n",
      "Iteration: 1570 lambda_n: 1.033527064559016 Loss: 0.05411036484029884\n",
      "Iteration: 1571 lambda_n: 0.9607004553560751 Loss: 0.0540319744327117\n",
      "Iteration: 1572 lambda_n: 1.0025629057235963 Loss: 0.05395916870450915\n",
      "Iteration: 1573 lambda_n: 0.882682381684672 Loss: 0.05388325095154692\n",
      "Iteration: 1574 lambda_n: 0.9973028275245616 Loss: 0.05381646482709952\n",
      "Iteration: 1575 lambda_n: 0.9269909907392875 Loss: 0.053741062442260124\n",
      "Iteration: 1576 lambda_n: 0.8901098054991575 Loss: 0.053671032894949545\n",
      "Iteration: 1577 lambda_n: 0.9529377641073661 Loss: 0.05360384056983924\n",
      "Iteration: 1578 lambda_n: 0.8913290691526399 Loss: 0.05353195913122139\n",
      "Iteration: 1579 lambda_n: 0.9346722525225853 Loss: 0.053464777227032585\n",
      "Iteration: 1580 lambda_n: 0.9484042411333079 Loss: 0.053394380907036194\n",
      "Iteration: 1581 lambda_n: 1.0186083363933116 Loss: 0.05332300582819973\n",
      "Iteration: 1582 lambda_n: 0.8949751655716036 Loss: 0.05324640852872219\n",
      "Iteration: 1583 lambda_n: 0.9503060342071923 Loss: 0.053179163758477574\n",
      "Iteration: 1584 lambda_n: 1.0020686617487504 Loss: 0.053107815446857304\n",
      "Iteration: 1585 lambda_n: 0.9463216392819501 Loss: 0.05303264100281458\n",
      "Iteration: 1586 lambda_n: 0.9975030172854894 Loss: 0.05296170729066898\n",
      "Iteration: 1587 lambda_n: 0.987459734161246 Loss: 0.05288699684917905\n",
      "Iteration: 1588 lambda_n: 1.0253320581355752 Loss: 0.05281310013736745\n",
      "Iteration: 1589 lambda_n: 0.9859006945224728 Loss: 0.052736433120494315\n",
      "Iteration: 1590 lambda_n: 1.0070015436066888 Loss: 0.052662777317656506\n",
      "Iteration: 1591 lambda_n: 0.9888815644784197 Loss: 0.05258760756497153\n",
      "Iteration: 1592 lambda_n: 0.8974763225743357 Loss: 0.0525138526086701\n",
      "Iteration: 1593 lambda_n: 0.9826476109245301 Loss: 0.052446969642960244\n",
      "Iteration: 1594 lambda_n: 1.0270684780868542 Loss: 0.05237379582737846\n",
      "Iteration: 1595 lambda_n: 0.89223591296791 Loss: 0.052297378103546975\n",
      "Iteration: 1596 lambda_n: 0.9669509622586275 Loss: 0.052231048412136084\n",
      "Iteration: 1597 lambda_n: 1.0111794100819533 Loss: 0.05215921946550727\n",
      "Iteration: 1598 lambda_n: 0.9370789425824427 Loss: 0.0520841670905165\n",
      "Iteration: 1599 lambda_n: 0.9316571046388437 Loss: 0.05201467332292719\n",
      "Iteration: 1600 lambda_n: 1.0348546996891514 Loss: 0.05194563649465915\n",
      "Iteration: 1601 lambda_n: 0.9008219631392455 Loss: 0.05186901469629085\n",
      "Iteration: 1602 lambda_n: 0.968558823518048 Loss: 0.05180237394610938\n",
      "Iteration: 1603 lambda_n: 0.9604363702606217 Loss: 0.05173077801390411\n",
      "Iteration: 1604 lambda_n: 1.0212878295189052 Loss: 0.05165984102866361\n",
      "Iteration: 1605 lambda_n: 0.9551730726569843 Loss: 0.051584472298034235\n",
      "Iteration: 1606 lambda_n: 0.9752001894334958 Loss: 0.05151404340290092\n",
      "Iteration: 1607 lambda_n: 1.0147368869856777 Loss: 0.05144219689651971\n",
      "Iteration: 1608 lambda_n: 0.8891857210920981 Loss: 0.05136750063094698\n",
      "Iteration: 1609 lambda_n: 1.0062587303611095 Loss: 0.05130210189742938\n",
      "Iteration: 1610 lambda_n: 0.9017408027927488 Loss: 0.05122815073241156\n",
      "Iteration: 1611 lambda_n: 0.9766538570994958 Loss: 0.05116193685411187\n",
      "Iteration: 1612 lambda_n: 1.0341220112744691 Loss: 0.051090278937499914\n",
      "Iteration: 1613 lambda_n: 1.021722739385776 Loss: 0.05101446932381439\n",
      "Iteration: 1614 lambda_n: 0.9329406418405343 Loss: 0.0509396354803082\n",
      "Iteration: 1615 lambda_n: 1.0012899789482037 Loss: 0.05087136360852269\n",
      "Iteration: 1616 lambda_n: 1.0349652997344911 Loss: 0.050798150208742364\n",
      "Iteration: 1617 lambda_n: 0.9196217569945858 Loss: 0.050722540790055545\n",
      "Iteration: 1618 lambda_n: 0.9598183611078703 Loss: 0.05065541680721378\n",
      "Iteration: 1619 lambda_n: 0.9972329274415167 Loss: 0.05058541549407102\n",
      "Iteration: 1620 lambda_n: 0.979189690887184 Loss: 0.05051274688191033\n",
      "Iteration: 1621 lambda_n: 0.8869420258755629 Loss: 0.05044145500208474\n",
      "Iteration: 1622 lambda_n: 0.9430276081254981 Loss: 0.05037693357520778\n",
      "Iteration: 1623 lambda_n: 0.9955860884273708 Loss: 0.05030838620365662\n",
      "Iteration: 1624 lambda_n: 0.8915562848394315 Loss: 0.050236079047150405\n",
      "Iteration: 1625 lambda_n: 0.9949546573508389 Loss: 0.050171382676817214\n",
      "Iteration: 1626 lambda_n: 1.0282507478181926 Loss: 0.050099241242684646\n",
      "Iteration: 1627 lambda_n: 0.9356164369808178 Loss: 0.05002475146930511\n",
      "Iteration: 1628 lambda_n: 0.9816752712833241 Loss: 0.04995703270997503\n",
      "Iteration: 1629 lambda_n: 0.8908577376095309 Loss: 0.04988603969318187\n",
      "Iteration: 1630 lambda_n: 0.9361668385098141 Loss: 0.049821669280662034\n",
      "Iteration: 1631 lambda_n: 0.9509657885862647 Loss: 0.049754079034774426\n",
      "Iteration: 1632 lambda_n: 1.0104801601768252 Loss: 0.04968547760999204\n",
      "Iteration: 1633 lambda_n: 1.020672283804493 Loss: 0.04961264543044443\n",
      "Iteration: 1634 lambda_n: 0.9245866508390774 Loss: 0.049539145019939376\n",
      "Iteration: 1635 lambda_n: 0.8864765990709156 Loss: 0.04947262325214098\n",
      "Iteration: 1636 lambda_n: 1.0329306228877222 Loss: 0.04940889562850789\n",
      "Iteration: 1637 lambda_n: 0.8962673245020109 Loss: 0.049334700766954495\n",
      "Iteration: 1638 lambda_n: 0.9947630500639514 Loss: 0.04927038018016306\n",
      "Iteration: 1639 lambda_n: 0.9399684935122671 Loss: 0.04919904995944262\n",
      "Iteration: 1640 lambda_n: 0.9316932775855598 Loss: 0.049131708370626075\n",
      "Iteration: 1641 lambda_n: 0.9580291006723315 Loss: 0.049065016056076216\n",
      "Iteration: 1642 lambda_n: 0.989155874363445 Loss: 0.04899649660277833\n",
      "Iteration: 1643 lambda_n: 1.028360666654351 Loss: 0.04892581264372417\n",
      "Iteration: 1644 lambda_n: 0.9391561629804411 Loss: 0.04885239357128179\n",
      "Iteration: 1645 lambda_n: 0.9711809579319566 Loss: 0.048785404466766914\n",
      "Iteration: 1646 lambda_n: 1.0135272020099924 Loss: 0.04871619063783384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1647 lambda_n: 0.9781615365022442 Loss: 0.04864402337676824\n",
      "Iteration: 1648 lambda_n: 1.0014070309246992 Loss: 0.04857443812028717\n",
      "Iteration: 1649 lambda_n: 1.0292905681967286 Loss: 0.0485032631947751\n",
      "Iteration: 1650 lambda_n: 1.0065754113433 Loss: 0.048430173905294326\n",
      "Iteration: 1651 lambda_n: 0.9876135134740484 Loss: 0.048358764677175245\n",
      "Iteration: 1652 lambda_n: 0.9582355349144891 Loss: 0.04828876511805006\n",
      "Iteration: 1653 lambda_n: 0.9398178372461057 Loss: 0.048220909051918545\n",
      "Iteration: 1654 lambda_n: 0.9225222543842521 Loss: 0.04815441569364494\n",
      "Iteration: 1655 lambda_n: 0.8943239592551747 Loss: 0.04808920238267854\n",
      "Iteration: 1656 lambda_n: 0.8848284998834441 Loss: 0.04802603593555941\n",
      "Iteration: 1657 lambda_n: 1.007940004787057 Loss: 0.047963591775735094\n",
      "Iteration: 1658 lambda_n: 0.9367356835759563 Loss: 0.04789251970319033\n",
      "Iteration: 1659 lambda_n: 1.0292105449433517 Loss: 0.047826529211329696\n",
      "Iteration: 1660 lambda_n: 0.9569073169801008 Loss: 0.04775408883244649\n",
      "Iteration: 1661 lambda_n: 0.9039371846903177 Loss: 0.04768680101713023\n",
      "Iteration: 1662 lambda_n: 0.9846810577871318 Loss: 0.04762329401450668\n",
      "Iteration: 1663 lambda_n: 1.0269376421214222 Loss: 0.04755417405433419\n",
      "Iteration: 1664 lambda_n: 0.9615037499026843 Loss: 0.0474821551187387\n",
      "Iteration: 1665 lambda_n: 0.9642589280019878 Loss: 0.04741478908127068\n",
      "Iteration: 1666 lambda_n: 0.9906833781932034 Loss: 0.04734729117514359\n",
      "Iteration: 1667 lambda_n: 0.9442830188260464 Loss: 0.047278007024702716\n",
      "Iteration: 1668 lambda_n: 1.0245915151853278 Loss: 0.04721202899930642\n",
      "Iteration: 1669 lambda_n: 1.0206218301205658 Loss: 0.04714050503805418\n",
      "Iteration: 1670 lambda_n: 0.956435483050633 Loss: 0.04706932733163685\n",
      "Iteration: 1671 lambda_n: 0.9896464656590522 Loss: 0.047002689616618375\n",
      "Iteration: 1672 lambda_n: 0.9576658424983816 Loss: 0.046933801295690636\n",
      "Iteration: 1673 lambda_n: 0.9773114123025821 Loss: 0.046867201517319684\n",
      "Iteration: 1674 lambda_n: 0.9818401101532048 Loss: 0.046799298008578204\n",
      "Iteration: 1675 lambda_n: 0.9572186404684638 Loss: 0.04673114373448748\n",
      "Iteration: 1676 lambda_n: 0.9424784021499691 Loss: 0.046664760728938125\n",
      "Iteration: 1677 lambda_n: 0.9799764315080886 Loss: 0.046599459840095295\n",
      "Iteration: 1678 lambda_n: 0.9804435070913566 Loss: 0.046531623067013826\n",
      "Iteration: 1679 lambda_n: 1.0177734506309601 Loss: 0.046463818119838296\n",
      "Iteration: 1680 lambda_n: 0.9889350527738403 Loss: 0.04639349886230306\n",
      "Iteration: 1681 lambda_n: 0.9426547308900155 Loss: 0.046325238947801425\n",
      "Iteration: 1682 lambda_n: 0.9503327508062468 Loss: 0.046260235167762245\n",
      "Iteration: 1683 lambda_n: 1.0113810965931072 Loss: 0.046194762106962745\n",
      "Iteration: 1684 lambda_n: 0.9506606254711334 Loss: 0.0461251486824864\n",
      "Iteration: 1685 lambda_n: 0.983920037603444 Loss: 0.0460597782603133\n",
      "Iteration: 1686 lambda_n: 0.957629161249501 Loss: 0.04599218428207794\n",
      "Iteration: 1687 lambda_n: 0.9060080407177716 Loss: 0.04592645945207618\n",
      "Iteration: 1688 lambda_n: 0.9461201640451922 Loss: 0.045864335190393575\n",
      "Iteration: 1689 lambda_n: 1.032220099506593 Loss: 0.04579951895307054\n",
      "Iteration: 1690 lambda_n: 0.9798816735139307 Loss: 0.04572887172652255\n",
      "Iteration: 1691 lambda_n: 0.9015586054566911 Loss: 0.04566187413668702\n",
      "Iteration: 1692 lambda_n: 0.9154210342867627 Loss: 0.04560029029644054\n",
      "Iteration: 1693 lambda_n: 0.9506570868514874 Loss: 0.045537815672570224\n",
      "Iteration: 1694 lambda_n: 0.9558081000268253 Loss: 0.04547299590509364\n",
      "Iteration: 1695 lambda_n: 0.8948902750924493 Loss: 0.04540788669534853\n",
      "Iteration: 1696 lambda_n: 0.9781836189035222 Loss: 0.04534698434702338\n",
      "Iteration: 1697 lambda_n: 1.0379184219418696 Loss: 0.04528047441511909\n",
      "Iteration: 1698 lambda_n: 0.9894197614967422 Loss: 0.045209973173462994\n",
      "Iteration: 1699 lambda_n: 0.9686102361923832 Loss: 0.04514283540600379\n",
      "Iteration: 1700 lambda_n: 0.9189911187019038 Loss: 0.04507717474983473\n",
      "Iteration: 1701 lambda_n: 1.002370438217535 Loss: 0.045014937718000386\n",
      "Iteration: 1702 lambda_n: 1.0355764400934244 Loss: 0.04494711850525445\n",
      "Iteration: 1703 lambda_n: 0.8965721792391269 Loss: 0.04487712434421803\n",
      "Iteration: 1704 lambda_n: 1.0199937635806513 Loss: 0.04481658683594905\n",
      "Iteration: 1705 lambda_n: 0.9840614942596716 Loss: 0.044747780851940214\n",
      "Iteration: 1706 lambda_n: 0.9844963535090007 Loss: 0.04468146711369353\n",
      "Iteration: 1707 lambda_n: 0.9757580288760274 Loss: 0.0446151907555242\n",
      "Iteration: 1708 lambda_n: 0.9405655528369504 Loss: 0.04454956869712962\n",
      "Iteration: 1709 lambda_n: 1.0300119181619283 Loss: 0.04448637611806758\n",
      "Iteration: 1710 lambda_n: 0.9725283734462652 Loss: 0.04441724263678367\n",
      "Iteration: 1711 lambda_n: 0.9972625092922429 Loss: 0.04435203563243869\n",
      "Iteration: 1712 lambda_n: 0.8942184629418064 Loss: 0.04428523783898954\n",
      "Iteration: 1713 lambda_n: 1.0238033223176919 Loss: 0.04422540214730001\n",
      "Iteration: 1714 lambda_n: 1.0240590400666685 Loss: 0.04415696144258287\n",
      "Iteration: 1715 lambda_n: 0.9988312116392384 Loss: 0.04408857650098651\n",
      "Iteration: 1716 lambda_n: 0.9157406589271894 Loss: 0.0440219469216099\n",
      "Iteration: 1717 lambda_n: 1.0094211790014298 Loss: 0.04396092238353111\n",
      "Iteration: 1718 lambda_n: 1.0160357944969147 Loss: 0.043893721283892684\n",
      "Iteration: 1719 lambda_n: 0.9530060120545325 Loss: 0.04382615157311327\n",
      "Iteration: 1720 lambda_n: 0.9234474327677884 Loss: 0.04376284010436438\n",
      "Iteration: 1721 lambda_n: 1.014056236331921 Loss: 0.04370155339818353\n",
      "Iteration: 1722 lambda_n: 1.0101425537758133 Loss: 0.043634320635835964\n",
      "Iteration: 1723 lambda_n: 0.8922744118553082 Loss: 0.04356741916940456\n",
      "Iteration: 1724 lambda_n: 0.9296580887591067 Loss: 0.04350838537673194\n",
      "Iteration: 1725 lambda_n: 1.0216082773565076 Loss: 0.04344693727649806\n",
      "Iteration: 1726 lambda_n: 0.9453524210681246 Loss: 0.04337948023134574\n",
      "Iteration: 1727 lambda_n: 0.9667177183649216 Loss: 0.04331712508948529\n",
      "Iteration: 1728 lambda_n: 0.8908043935147936 Loss: 0.04325342564690153\n",
      "Iteration: 1729 lambda_n: 0.9858677091556683 Loss: 0.043194787884108854\n",
      "Iteration: 1730 lambda_n: 1.005133726097244 Loss: 0.043129956546087374\n",
      "Iteration: 1731 lambda_n: 0.9298499724601905 Loss: 0.04306392889455149\n",
      "Iteration: 1732 lambda_n: 0.9948841601797516 Loss: 0.04300291161600283\n",
      "Iteration: 1733 lambda_n: 0.9337840388702024 Loss: 0.04293769380043323\n",
      "Iteration: 1734 lambda_n: 0.9622828330027029 Loss: 0.042876546266590536\n",
      "Iteration: 1735 lambda_n: 1.020106193647152 Loss: 0.04281359711227156\n",
      "Iteration: 1736 lambda_n: 0.9965549426547167 Loss: 0.04274693659128283\n",
      "Iteration: 1737 lambda_n: 0.9768532993691347 Loss: 0.0426818872315634\n",
      "Iteration: 1738 lambda_n: 1.0155929034966853 Loss: 0.042618193145992606\n",
      "Iteration: 1739 lambda_n: 0.9362796426823967 Loss: 0.042552044992762335\n",
      "Iteration: 1740 lambda_n: 0.9002576619838866 Loss: 0.04249112942085001\n",
      "Iteration: 1741 lambda_n: 0.9987130632570592 Loss: 0.04243261737435635\n",
      "Iteration: 1742 lambda_n: 0.9809676509748848 Loss: 0.04236777293696783\n",
      "Iteration: 1743 lambda_n: 0.9466784873630633 Loss: 0.04230415088593367\n",
      "Iteration: 1744 lambda_n: 0.914891964670165 Loss: 0.042242819021113\n",
      "Iteration: 1745 lambda_n: 0.8837000082862554 Loss: 0.04218360844192054\n",
      "Iteration: 1746 lambda_n: 0.9803059977043068 Loss: 0.042126474459179206\n",
      "Iteration: 1747 lambda_n: 1.0176461994859027 Loss: 0.042063159304540326\n",
      "Iteration: 1748 lambda_n: 0.9915598302287293 Loss: 0.04199750559144691\n",
      "Iteration: 1749 lambda_n: 0.9170743425377985 Loss: 0.04193360758140107\n",
      "Iteration: 1750 lambda_n: 0.9694852153996572 Loss: 0.041874574244952875\n",
      "Iteration: 1751 lambda_n: 0.9663442712115133 Loss: 0.041812232966373784\n",
      "Iteration: 1752 lambda_n: 0.9606184850246857 Loss: 0.041750161913343894\n",
      "Iteration: 1753 lambda_n: 1.020151706918634 Loss: 0.04168852631300764\n",
      "Iteration: 1754 lambda_n: 0.964527228246641 Loss: 0.04162314387864702\n",
      "Iteration: 1755 lambda_n: 0.9732756984757877 Loss: 0.04156139737606543\n",
      "Iteration: 1756 lambda_n: 1.011474796213033 Loss: 0.04149915984887624\n",
      "Iteration: 1757 lambda_n: 0.9383581012131015 Loss: 0.04143455276134003\n",
      "Iteration: 1758 lambda_n: 1.0221167133160132 Loss: 0.04137468426369902\n",
      "Iteration: 1759 lambda_n: 0.9993591307864172 Loss: 0.04130954436115341\n",
      "Iteration: 1760 lambda_n: 1.0093451538861467 Loss: 0.04124592966384955\n",
      "Iteration: 1761 lambda_n: 0.9593073763984785 Loss: 0.04118175404327809\n",
      "Iteration: 1762 lambda_n: 0.9905188627676321 Loss: 0.04112083048377923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1763 lambda_n: 0.9801666024136239 Loss: 0.04105799583795696\n",
      "Iteration: 1764 lambda_n: 0.8870884400756099 Loss: 0.04099588972009083\n",
      "Iteration: 1765 lambda_n: 0.8890324486800042 Loss: 0.040939744080656586\n",
      "Iteration: 1766 lambda_n: 0.9465662601321347 Loss: 0.040883534125472853\n",
      "Iteration: 1767 lambda_n: 0.8986956002547648 Loss: 0.04082375046093012\n",
      "Iteration: 1768 lambda_n: 0.9640283411862351 Loss: 0.040767052749560516\n",
      "Iteration: 1769 lambda_n: 0.9652686799714701 Loss: 0.040706299423541994\n",
      "Iteration: 1770 lambda_n: 0.9417028407434158 Loss: 0.04064553759833267\n",
      "Iteration: 1771 lambda_n: 0.9441472935229651 Loss: 0.04058632682415918\n",
      "Iteration: 1772 lambda_n: 0.9656970628128885 Loss: 0.04052702913853501\n",
      "Iteration: 1773 lambda_n: 0.9151297506690972 Loss: 0.040466447016989475\n",
      "Iteration: 1774 lambda_n: 0.9365260619593588 Loss: 0.040409102660661285\n",
      "Iteration: 1775 lambda_n: 0.9315367086899562 Loss: 0.04035048262644508\n",
      "Iteration: 1776 lambda_n: 0.9655339502899009 Loss: 0.04029224064954186\n",
      "Iteration: 1777 lambda_n: 0.9223635792026669 Loss: 0.04023194184323155\n",
      "Iteration: 1778 lambda_n: 0.8926577135038564 Loss: 0.0401744055971247\n",
      "Iteration: 1779 lambda_n: 0.9382134564551651 Loss: 0.040118784199187546\n",
      "Iteration: 1780 lambda_n: 1.0300281368342816 Loss: 0.04006038882345165\n",
      "Iteration: 1781 lambda_n: 1.0260199231538392 Loss: 0.03999635451562161\n",
      "Iteration: 1782 lambda_n: 0.8948160111696588 Loss: 0.03993264980399315\n",
      "Iteration: 1783 lambda_n: 0.9684868843988662 Loss: 0.039877158726495304\n",
      "Iteration: 1784 lambda_n: 0.9995039869600788 Loss: 0.03981716693116009\n",
      "Iteration: 1785 lambda_n: 0.8828482365484911 Loss: 0.039755328636657185\n",
      "Iteration: 1786 lambda_n: 0.8953405820490146 Loss: 0.03970077291020598\n",
      "Iteration: 1787 lambda_n: 0.9835435364512206 Loss: 0.03964550616118051\n",
      "Iteration: 1788 lambda_n: 0.9894274202033466 Loss: 0.039584864673166305\n",
      "Iteration: 1789 lambda_n: 1.0185292572434401 Loss: 0.03952393548014537\n",
      "Iteration: 1790 lambda_n: 0.9210572071612464 Loss: 0.03946129262589868\n",
      "Iteration: 1791 lambda_n: 0.9103351408528431 Loss: 0.03940471492392226\n",
      "Iteration: 1792 lambda_n: 0.8944508007767311 Loss: 0.03934886047899442\n",
      "Iteration: 1793 lambda_n: 0.9538697932635062 Loss: 0.03929404337699914\n",
      "Iteration: 1794 lambda_n: 0.9345095248492841 Loss: 0.03923565232282886\n",
      "Iteration: 1795 lambda_n: 0.9095750262752756 Loss: 0.039178515243160315\n",
      "Iteration: 1796 lambda_n: 1.0358540399863432 Loss: 0.039122968297691844\n",
      "Iteration: 1797 lambda_n: 0.9025000394443249 Loss: 0.03905978626961512\n",
      "Iteration: 1798 lambda_n: 1.0149673126335623 Loss: 0.03900480822228632\n",
      "Iteration: 1799 lambda_n: 1.003990463846177 Loss: 0.03894305341830985\n",
      "Iteration: 1800 lambda_n: 0.9517589506294617 Loss: 0.038882046052174817\n",
      "Iteration: 1801 lambda_n: 1.0053659635888963 Loss: 0.03882428627615521\n",
      "Iteration: 1802 lambda_n: 1.0297637169052576 Loss: 0.038763349789449925\n",
      "Iteration: 1803 lambda_n: 1.0056425361710428 Loss: 0.03870101666993219\n",
      "Iteration: 1804 lambda_n: 0.9826802314214615 Loss: 0.03864022470882255\n",
      "Iteration: 1805 lambda_n: 0.9508737210144476 Loss: 0.038580898344676325\n",
      "Iteration: 1806 lambda_n: 0.982561555820949 Loss: 0.038523565382147046\n",
      "Iteration: 1807 lambda_n: 0.949018921422335 Loss: 0.03846439665026909\n",
      "Iteration: 1808 lambda_n: 0.8932209241073488 Loss: 0.03840732104127505\n",
      "Iteration: 1809 lambda_n: 0.9380550650089724 Loss: 0.038353667354498716\n",
      "Iteration: 1810 lambda_n: 0.982324852514864 Loss: 0.038297388402509736\n",
      "Iteration: 1811 lambda_n: 0.8914241593883488 Loss: 0.03823852810283138\n",
      "Iteration: 1812 lambda_n: 0.8945913144868614 Loss: 0.03818518239679731\n",
      "Iteration: 1813 lambda_n: 0.9155901283788981 Loss: 0.038131711054449235\n",
      "Iteration: 1814 lambda_n: 1.013427983602936 Loss: 0.03807705073012618\n",
      "Iteration: 1815 lambda_n: 0.9024936606694443 Loss: 0.03801662660892799\n",
      "Iteration: 1816 lambda_n: 0.9877746091839904 Loss: 0.03796288769478261\n",
      "Iteration: 1817 lambda_n: 0.9290686908292478 Loss: 0.03790344246256503\n",
      "Iteration: 1818 lambda_n: 0.8928785485651604 Loss: 0.03784452608348485\n",
      "Iteration: 1819 lambda_n: 0.9482639075868562 Loss: 0.037786883347054497\n",
      "Iteration: 1820 lambda_n: 0.9951919240943652 Loss: 0.03772536940096059\n",
      "Iteration: 1821 lambda_n: 1.0054116319311812 Loss: 0.037660864034651444\n",
      "Iteration: 1822 lambda_n: 0.9815144274113755 Loss: 0.03759596105512629\n",
      "Iteration: 1823 lambda_n: 0.9836894366444897 Loss: 0.03753295624243616\n",
      "Iteration: 1824 lambda_n: 0.9291507456635203 Loss: 0.03747019771512534\n",
      "Iteration: 1825 lambda_n: 0.9982763729729017 Loss: 0.0374112889612032\n",
      "Iteration: 1826 lambda_n: 0.9327131210066647 Loss: 0.03734838190894307\n",
      "Iteration: 1827 lambda_n: 0.9930629700783186 Loss: 0.03728997538775619\n",
      "Iteration: 1828 lambda_n: 0.9148852688606239 Loss: 0.03722815873870441\n",
      "Iteration: 1829 lambda_n: 1.0140045541518836 Loss: 0.037171548753253486\n",
      "Iteration: 1830 lambda_n: 1.0040412062529818 Loss: 0.03710915735528758\n",
      "Iteration: 1831 lambda_n: 0.9522348129716733 Loss: 0.037047739668785114\n",
      "Iteration: 1832 lambda_n: 0.9623833976989594 Loss: 0.036989809574877575\n",
      "Iteration: 1833 lambda_n: 1.0183007976407756 Loss: 0.03693155609944422\n",
      "Iteration: 1834 lambda_n: 1.0349423627367182 Loss: 0.03687021797169926\n",
      "Iteration: 1835 lambda_n: 0.8925210334103106 Loss: 0.03680817421876279\n",
      "Iteration: 1836 lambda_n: 0.9211787041732026 Loss: 0.03675489635472751\n",
      "Iteration: 1837 lambda_n: 0.8853141369415534 Loss: 0.03670010478049785\n",
      "Iteration: 1838 lambda_n: 1.0246009148719128 Loss: 0.036647621994269276\n",
      "Iteration: 1839 lambda_n: 1.010718474489963 Loss: 0.036587072896862624\n",
      "Iteration: 1840 lambda_n: 0.9544059185086362 Loss: 0.036527531422928014\n",
      "Iteration: 1841 lambda_n: 0.9524675966055248 Loss: 0.036471462492261415\n",
      "Iteration: 1842 lambda_n: 0.885183154993214 Loss: 0.03641564247948374\n",
      "Iteration: 1843 lambda_n: 0.9936595901667749 Loss: 0.0363638770927833\n",
      "Iteration: 1844 lambda_n: 0.9921855343535467 Loss: 0.03630588185758419\n",
      "Iteration: 1845 lambda_n: 0.9773353008139736 Loss: 0.03624808479023787\n",
      "Iteration: 1846 lambda_n: 0.8870961127003795 Loss: 0.0361912535471926\n",
      "Iteration: 1847 lambda_n: 1.0378561243759454 Loss: 0.03613975065707503\n",
      "Iteration: 1848 lambda_n: 0.8865787143740825 Loss: 0.0360795829545098\n",
      "Iteration: 1849 lambda_n: 0.9457439207106636 Loss: 0.03602825893453605\n",
      "Iteration: 1850 lambda_n: 0.8863664697016856 Loss: 0.03597357818913896\n",
      "Iteration: 1851 lambda_n: 1.0262700087242909 Loss: 0.03592239256200847\n",
      "Iteration: 1852 lambda_n: 0.9522393347011284 Loss: 0.03586319724661404\n",
      "Iteration: 1853 lambda_n: 0.9545883044050684 Loss: 0.03580833803908501\n",
      "Iteration: 1854 lambda_n: 0.8986303571935356 Loss: 0.03575340422331746\n",
      "Iteration: 1855 lambda_n: 0.9327856924609831 Loss: 0.03570174497538397\n",
      "Iteration: 1856 lambda_n: 1.0204164688888953 Loss: 0.035648175709704424\n",
      "Iteration: 1857 lambda_n: 1.0174901544621815 Loss: 0.03558963426154345\n",
      "Iteration: 1858 lambda_n: 0.9963309186677618 Loss: 0.03553132312415307\n",
      "Iteration: 1859 lambda_n: 0.9595780941202229 Loss: 0.03547428408453653\n",
      "Iteration: 1860 lambda_n: 0.9558722694560019 Loss: 0.03541940408507112\n",
      "Iteration: 1861 lambda_n: 0.8973463951919851 Loss: 0.03536478870794566\n",
      "Iteration: 1862 lambda_n: 0.8859660774725199 Loss: 0.035313565192097746\n",
      "Iteration: 1863 lambda_n: 0.912795601851195 Loss: 0.035263036055119636\n",
      "Iteration: 1864 lambda_n: 1.0118384850511037 Loss: 0.035211022626118485\n",
      "Iteration: 1865 lambda_n: 0.9847714109459762 Loss: 0.03515341885453554\n",
      "Iteration: 1866 lambda_n: 0.9367754977413558 Loss: 0.03509741094635696\n",
      "Iteration: 1867 lambda_n: 1.0044226701167065 Loss: 0.03504418311068023\n",
      "Iteration: 1868 lambda_n: 1.0352189198443822 Loss: 0.034987164752453496\n",
      "Iteration: 1869 lambda_n: 0.9397306410275202 Loss: 0.03492845611452565\n",
      "Iteration: 1870 lambda_n: 0.9158771550220793 Loss: 0.03487521485693876\n",
      "Iteration: 1871 lambda_n: 1.029955975396148 Loss: 0.03482337210549089\n",
      "Iteration: 1872 lambda_n: 0.9497648022861807 Loss: 0.0347651259804801\n",
      "Iteration: 1873 lambda_n: 1.0100841597567585 Loss: 0.034711467501837776\n",
      "Iteration: 1874 lambda_n: 1.0001476353010994 Loss: 0.03465445512338446\n",
      "Iteration: 1875 lambda_n: 0.9130689821381998 Loss: 0.034598059208643415\n",
      "Iteration: 1876 lambda_n: 1.028651708600368 Loss: 0.034546622586413056\n",
      "Iteration: 1877 lambda_n: 0.9115992214895471 Loss: 0.034488728776155025\n",
      "Iteration: 1878 lambda_n: 0.996066981305929 Loss: 0.03443747300052005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1879 lambda_n: 0.9276670106240276 Loss: 0.034381519636116895\n",
      "Iteration: 1880 lambda_n: 0.9034501681172444 Loss: 0.03432945331313704\n",
      "Iteration: 1881 lambda_n: 0.8934553953032001 Loss: 0.034278763288386596\n",
      "Iteration: 1882 lambda_n: 0.9966612400808182 Loss: 0.03422865308820333\n",
      "Iteration: 1883 lambda_n: 0.9368221379920175 Loss: 0.034172783676212896\n",
      "Iteration: 1884 lambda_n: 0.8976203907587015 Loss: 0.03412030261284973\n",
      "Iteration: 1885 lambda_n: 0.9578080047939881 Loss: 0.03407005357379621\n",
      "Iteration: 1886 lambda_n: 1.0202872180824105 Loss: 0.034016477179332205\n",
      "Iteration: 1887 lambda_n: 0.9533523461748183 Loss: 0.03395945693132518\n",
      "Iteration: 1888 lambda_n: 1.0069683037466688 Loss: 0.03390622878086389\n",
      "Iteration: 1889 lambda_n: 0.911966886175061 Loss: 0.03385006177328321\n",
      "Iteration: 1890 lambda_n: 0.953496724040195 Loss: 0.03379924517511524\n",
      "Iteration: 1891 lambda_n: 1.0237135947397227 Loss: 0.033746166097937076\n",
      "Iteration: 1892 lambda_n: 0.9547778564870658 Loss: 0.033689237351055515\n",
      "Iteration: 1893 lambda_n: 0.8982965431202635 Loss: 0.03363619965982386\n",
      "Iteration: 1894 lambda_n: 1.009384528502037 Loss: 0.033586350459842484\n",
      "Iteration: 1895 lambda_n: 0.9151233108745055 Loss: 0.03353039348065706\n",
      "Iteration: 1896 lambda_n: 0.9971019085893975 Loss: 0.033479716799487705\n",
      "Iteration: 1897 lambda_n: 0.900062287250601 Loss: 0.03342455733533872\n",
      "Iteration: 1898 lambda_n: 0.9460663074042729 Loss: 0.03337481937013861\n",
      "Iteration: 1899 lambda_n: 0.975121605398431 Loss: 0.03332259187537355\n",
      "Iteration: 1900 lambda_n: 1.0291007150426368 Loss: 0.0332688171377271\n",
      "Iteration: 1901 lambda_n: 0.8973455440439779 Loss: 0.03321212775898277\n",
      "Iteration: 1902 lambda_n: 0.8827324217892153 Loss: 0.033162750638678476\n",
      "Iteration: 1903 lambda_n: 0.9013269653274698 Loss: 0.03311422567276604\n",
      "Iteration: 1904 lambda_n: 0.9952714235102468 Loss: 0.03306472728143388\n",
      "Iteration: 1905 lambda_n: 0.9339741124696618 Loss: 0.033010125897733666\n",
      "Iteration: 1906 lambda_n: 0.9050262589414547 Loss: 0.032958942957657784\n",
      "Iteration: 1907 lambda_n: 0.9528550682210634 Loss: 0.03290939738364781\n",
      "Iteration: 1908 lambda_n: 0.9973499767188763 Loss: 0.03285728665830169\n",
      "Iteration: 1909 lambda_n: 0.9669701965404 Loss: 0.032802801028665264\n",
      "Iteration: 1910 lambda_n: 1.012214398202762 Loss: 0.03275003320143649\n",
      "Iteration: 1911 lambda_n: 0.9609022904023484 Loss: 0.032694856672636906\n",
      "Iteration: 1912 lambda_n: 1.0338229740868274 Loss: 0.03264253550591261\n",
      "Iteration: 1913 lambda_n: 0.9660174794033718 Loss: 0.03258630548252154\n",
      "Iteration: 1914 lambda_n: 0.9013064917922307 Loss: 0.03253382303571858\n",
      "Iteration: 1915 lambda_n: 0.8891543275119874 Loss: 0.032484908198422244\n",
      "Iteration: 1916 lambda_n: 0.8830320262658325 Loss: 0.03243670142227136\n",
      "Iteration: 1917 lambda_n: 0.8839255174875634 Loss: 0.03238887423973569\n",
      "Iteration: 1918 lambda_n: 0.9613251640183728 Loss: 0.03234104616515606\n",
      "Iteration: 1919 lambda_n: 0.9202731411510972 Loss: 0.0322890830921628\n",
      "Iteration: 1920 lambda_n: 0.9919198953580245 Loss: 0.03223939223929998\n",
      "Iteration: 1921 lambda_n: 0.9320239931549925 Loss: 0.032185889594027135\n",
      "Iteration: 1922 lambda_n: 0.9152165806014911 Loss: 0.032135673008112695\n",
      "Iteration: 1923 lambda_n: 0.9212508615388421 Loss: 0.03208641372148944\n",
      "Iteration: 1924 lambda_n: 0.9575274267493776 Loss: 0.03203688117300809\n",
      "Iteration: 1925 lambda_n: 1.0221975045531901 Loss: 0.031985452589670074\n",
      "Iteration: 1926 lambda_n: 0.9263037637977246 Loss: 0.03193061150888292\n",
      "Iteration: 1927 lambda_n: 0.9026612757481343 Loss: 0.03188097142725527\n",
      "Iteration: 1928 lambda_n: 0.9015273056258205 Loss: 0.03183264906176821\n",
      "Iteration: 1929 lambda_n: 1.0294614670914346 Loss: 0.031784437148494236\n",
      "Iteration: 1930 lambda_n: 0.9493066912659242 Loss: 0.03172944269135146\n",
      "Iteration: 1931 lambda_n: 0.9155068069543291 Loss: 0.0316787886428338\n",
      "Iteration: 1932 lambda_n: 0.9141314319272129 Loss: 0.03162999082082604\n",
      "Iteration: 1933 lambda_n: 0.9167294514772337 Loss: 0.03158131759353211\n",
      "Iteration: 1934 lambda_n: 0.9974148742720714 Loss: 0.031532557485253576\n",
      "Iteration: 1935 lambda_n: 1.02097287240836 Loss: 0.03147956335843307\n",
      "Iteration: 1936 lambda_n: 1.0010506675216642 Loss: 0.031425380554229755\n",
      "Iteration: 1937 lambda_n: 0.9162592650805189 Loss: 0.031372317493083966\n",
      "Iteration: 1938 lambda_n: 0.9631171969826149 Loss: 0.03132380402583828\n",
      "Iteration: 1939 lambda_n: 0.898776507914667 Loss: 0.03127286470373716\n",
      "Iteration: 1940 lambda_n: 0.9561150678239231 Loss: 0.031225380642707096\n",
      "Iteration: 1941 lambda_n: 1.0135923646673277 Loss: 0.03117492125530998\n",
      "Iteration: 1942 lambda_n: 1.0267409811593555 Loss: 0.031121489337310484\n",
      "Iteration: 1943 lambda_n: 1.0075546967117677 Loss: 0.031067428795564837\n",
      "Iteration: 1944 lambda_n: 0.9574183008720375 Loss: 0.03101444203739579\n",
      "Iteration: 1945 lambda_n: 1.0341631876713726 Loss: 0.030964150701837695\n",
      "Iteration: 1946 lambda_n: 1.0254252537681314 Loss: 0.03090989085795443\n",
      "Iteration: 1947 lambda_n: 1.0071107659076095 Loss: 0.03085615499981233\n",
      "Iteration: 1948 lambda_n: 0.9435715461695937 Loss: 0.030803442571473296\n",
      "Iteration: 1949 lambda_n: 0.9482313661589129 Loss: 0.03075411364942212\n",
      "Iteration: 1950 lambda_n: 1.0200575133980232 Loss: 0.03070459675999126\n",
      "Iteration: 1951 lambda_n: 0.9047316334068044 Loss: 0.030651390577921343\n",
      "Iteration: 1952 lambda_n: 0.8937825469510581 Loss: 0.03060425523495641\n",
      "Iteration: 1953 lambda_n: 1.0085010744868665 Loss: 0.03055774047516505\n",
      "Iteration: 1954 lambda_n: 0.9570074245604655 Loss: 0.030505313805203597\n",
      "Iteration: 1955 lambda_n: 0.8873920227188679 Loss: 0.030455623264767206\n",
      "Iteration: 1956 lambda_n: 0.8876734752466023 Loss: 0.03040959917345908\n",
      "Iteration: 1957 lambda_n: 0.8946184168804505 Loss: 0.030363609680528293\n",
      "Iteration: 1958 lambda_n: 0.9824936664358869 Loss: 0.03031731012605375\n",
      "Iteration: 1959 lambda_n: 0.9622919326039453 Loss: 0.03026651935821955\n",
      "Iteration: 1960 lambda_n: 0.906731717421785 Loss: 0.030216831751150544\n",
      "Iteration: 1961 lambda_n: 0.9369962551596125 Loss: 0.03017006667746488\n",
      "Iteration: 1962 lambda_n: 0.8973927915870017 Loss: 0.030121794521342192\n",
      "Iteration: 1963 lambda_n: 1.0197366502046346 Loss: 0.03007561474054719\n",
      "Iteration: 1964 lambda_n: 0.9295018202187143 Loss: 0.03002319905410498\n",
      "Iteration: 1965 lambda_n: 0.9962734935244371 Loss: 0.029975479508871198\n",
      "Iteration: 1966 lambda_n: 0.8883028878004301 Loss: 0.029924391554094796\n",
      "Iteration: 1967 lambda_n: 0.9694881327427022 Loss: 0.02987889409320508\n",
      "Iteration: 1968 lambda_n: 0.934544584424883 Loss: 0.029829294283260914\n",
      "Iteration: 1969 lambda_n: 0.9637171279076882 Loss: 0.029781538736897987\n",
      "Iteration: 1970 lambda_n: 0.9358516069854743 Loss: 0.02973234989299725\n",
      "Iteration: 1971 lambda_n: 0.9013287026445542 Loss: 0.02968463981702096\n",
      "Iteration: 1972 lambda_n: 1.0162749787529728 Loss: 0.0296387424774152\n",
      "Iteration: 1973 lambda_n: 0.9591901947624384 Loss: 0.029587052181603325\n",
      "Iteration: 1974 lambda_n: 1.006745732290921 Loss: 0.029538326054948178\n",
      "Iteration: 1975 lambda_n: 1.0327964763190964 Loss: 0.029487246363608093\n",
      "Iteration: 1976 lambda_n: 1.0003105249645192 Loss: 0.029434911494906082\n",
      "Iteration: 1977 lambda_n: 1.0198384767090916 Loss: 0.029384287807547614\n",
      "Iteration: 1978 lambda_n: 1.0242548234162667 Loss: 0.029332741166686525\n",
      "Iteration: 1979 lambda_n: 0.9883133882673667 Loss: 0.029281037934473623\n",
      "Iteration: 1980 lambda_n: 0.9886157922918264 Loss: 0.02923121280978274\n",
      "Iteration: 1981 lambda_n: 0.9929850483461413 Loss: 0.02918143480569157\n",
      "Iteration: 1982 lambda_n: 1.001211999523283 Loss: 0.029131499610206953\n",
      "Iteration: 1983 lambda_n: 0.9412258046496822 Loss: 0.029081214447525226\n",
      "Iteration: 1984 lambda_n: 0.9298770686307476 Loss: 0.02903414352580466\n",
      "Iteration: 1985 lambda_n: 0.908122656900446 Loss: 0.028987786461511466\n",
      "Iteration: 1986 lambda_n: 0.9715504618036069 Loss: 0.028942566282474727\n",
      "Iteration: 1987 lambda_n: 0.9894538606098371 Loss: 0.028894244177571648\n",
      "Iteration: 1988 lambda_n: 0.9387600750295138 Loss: 0.028845092197463458\n",
      "Iteration: 1989 lambda_n: 1.0178420336084486 Loss: 0.028798515744891905\n",
      "Iteration: 1990 lambda_n: 0.9931365005503296 Loss: 0.028748077334962534\n",
      "Iteration: 1991 lambda_n: 1.0263709918983028 Loss: 0.028698926238176107\n",
      "Iteration: 1992 lambda_n: 0.998678999626798 Loss: 0.02864819523279041\n",
      "Iteration: 1993 lambda_n: 0.986739533158236 Loss: 0.028598897011596267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1994 lambda_n: 0.9542376563497359 Loss: 0.028550250117262605\n",
      "Iteration: 1995 lambda_n: 0.898302699843833 Loss: 0.028503264438738345\n",
      "Iteration: 1996 lambda_n: 0.9006579834669433 Loss: 0.028459086113607528\n",
      "Iteration: 1997 lambda_n: 0.9550053749865721 Loss: 0.028414843254778954\n",
      "Iteration: 1998 lambda_n: 1.0255518757296773 Loss: 0.028367986359894334\n",
      "Iteration: 1999 lambda_n: 0.9775471839565473 Loss: 0.028317731880129017\n",
      "Iteration: 2000 lambda_n: 0.9493159152673373 Loss: 0.028269892488288678\n",
      "Iteration: 2001 lambda_n: 0.9244226144275975 Loss: 0.02822349319289365\n",
      "Iteration: 2002 lambda_n: 0.916302064323017 Loss: 0.02817836604326007\n",
      "Iteration: 2003 lambda_n: 1.035503628749091 Loss: 0.02813368921312543\n",
      "Iteration: 2004 lambda_n: 0.9591541739828763 Loss: 0.028083263696598634\n",
      "Iteration: 2005 lambda_n: 0.9633048256372323 Loss: 0.02803661810353484\n",
      "Iteration: 2006 lambda_n: 1.0111075379681878 Loss: 0.027989829943633394\n",
      "Iteration: 2007 lambda_n: 0.8983745936856372 Loss: 0.027940783530208613\n",
      "Iteration: 2008 lambda_n: 0.9123470240967595 Loss: 0.027897261652484902\n",
      "Iteration: 2009 lambda_n: 0.8848102362813021 Loss: 0.027853115874278026\n",
      "Iteration: 2010 lambda_n: 0.9516100896794732 Loss: 0.027810353964600885\n",
      "Iteration: 2011 lambda_n: 0.9436123031512949 Loss: 0.027764419382771858\n",
      "Iteration: 2012 lambda_n: 1.0076620024855838 Loss: 0.027718928657425743\n",
      "Iteration: 2013 lambda_n: 1.0140187126163827 Loss: 0.027670413050189104\n",
      "Iteration: 2014 lambda_n: 0.9076269690928065 Loss: 0.027621657671572614\n",
      "Iteration: 2015 lambda_n: 0.9256190612639902 Loss: 0.027578075242302602\n",
      "Iteration: 2016 lambda_n: 1.0342356477249426 Loss: 0.027533683761639446\n",
      "Iteration: 2017 lambda_n: 0.9576888272868185 Loss: 0.02748414790580901\n",
      "Iteration: 2018 lambda_n: 0.9746607004697939 Loss: 0.027438341141713635\n",
      "Iteration: 2019 lambda_n: 1.0002423381476075 Loss: 0.02739178379748208\n",
      "Iteration: 2020 lambda_n: 0.9903138568392613 Loss: 0.027344068665032445\n",
      "Iteration: 2021 lambda_n: 1.0067942709977493 Loss: 0.0272968916519816\n",
      "Iteration: 2022 lambda_n: 1.0084854309584446 Loss: 0.0272489951497751\n",
      "Iteration: 2023 lambda_n: 0.9158877213899093 Loss: 0.02720108475643625\n",
      "Iteration: 2024 lambda_n: 1.0261656157131749 Loss: 0.027157632079779438\n",
      "Iteration: 2025 lambda_n: 0.967904322267013 Loss: 0.027109011813549046\n",
      "Iteration: 2026 lambda_n: 0.9718891570648192 Loss: 0.027063216039506302\n",
      "Iteration: 2027 lambda_n: 0.9490479317560137 Loss: 0.02701729378745616\n",
      "Iteration: 2028 lambda_n: 0.978229856643496 Loss: 0.026972511139047965\n",
      "Iteration: 2029 lambda_n: 0.9140681247525675 Loss: 0.026926413478890708\n",
      "Iteration: 2030 lambda_n: 0.9143230721291339 Loss: 0.026883397125256094\n",
      "Iteration: 2031 lambda_n: 0.9312288694676732 Loss: 0.026840424135386776\n",
      "Iteration: 2032 lambda_n: 0.9689994353501025 Loss: 0.026796713416502003\n",
      "Iteration: 2033 lambda_n: 0.8953069144608298 Loss: 0.02675129058526389\n",
      "Iteration: 2034 lambda_n: 0.8852333433876294 Loss: 0.026709378302351476\n",
      "Iteration: 2035 lambda_n: 0.8971296947140717 Loss: 0.02666799016088893\n",
      "Iteration: 2036 lambda_n: 0.9225863074808641 Loss: 0.026626099022892504\n",
      "Iteration: 2037 lambda_n: 1.0198759589439355 Loss: 0.02658307501568713\n",
      "Iteration: 2038 lambda_n: 0.9509317631848878 Loss: 0.026535579267829467\n",
      "Iteration: 2039 lambda_n: 1.0171005536305895 Loss: 0.026491357624503428\n",
      "Iteration: 2040 lambda_n: 0.8878401746051624 Loss: 0.026444125369861755\n",
      "Iteration: 2041 lambda_n: 0.8910173492747779 Loss: 0.026402953596840573\n",
      "Iteration: 2042 lambda_n: 0.9121631518628063 Loss: 0.026361687745423756\n",
      "Iteration: 2043 lambda_n: 1.0194185511633678 Loss: 0.026319497750137304\n",
      "Iteration: 2044 lambda_n: 0.9675785865513042 Loss: 0.02627241225362567\n",
      "Iteration: 2045 lambda_n: 0.9835565736196553 Loss: 0.026227786550885517\n",
      "Iteration: 2046 lambda_n: 0.9448332418345612 Loss: 0.02618248866346535\n",
      "Iteration: 2047 lambda_n: 1.012511352246563 Loss: 0.026139036221022618\n",
      "Iteration: 2048 lambda_n: 1.024373343449075 Loss: 0.026092537856757417\n",
      "Iteration: 2049 lambda_n: 1.0279123727672232 Loss: 0.02604556549374148\n",
      "Iteration: 2050 lambda_n: 0.9428243133034929 Loss: 0.025998502572057814\n",
      "Iteration: 2051 lambda_n: 0.925193599460537 Loss: 0.0259553994434655\n",
      "Iteration: 2052 lambda_n: 0.9686877255292702 Loss: 0.025913161449643892\n",
      "Iteration: 2053 lambda_n: 0.9540575649022848 Loss: 0.025869000101292557\n",
      "Iteration: 2054 lambda_n: 0.8911290383881683 Loss: 0.025825568618019455\n",
      "Iteration: 2055 lambda_n: 0.932477695243189 Loss: 0.02578505868017352\n",
      "Iteration: 2056 lambda_n: 1.008311869616019 Loss: 0.025742727063602797\n",
      "Iteration: 2057 lambda_n: 0.9290333940034404 Loss: 0.02569701935864326\n",
      "Iteration: 2058 lambda_n: 1.0123125987181865 Loss: 0.025654968035967177\n",
      "Iteration: 2059 lambda_n: 0.9369774184277191 Loss: 0.02560921417649289\n",
      "Iteration: 2060 lambda_n: 1.0348412389913544 Loss: 0.025566928960516728\n",
      "Iteration: 2061 lambda_n: 0.9615013434442026 Loss: 0.025520296845899813\n",
      "Iteration: 2062 lambda_n: 1.0163874208541677 Loss: 0.025477036688951675\n",
      "Iteration: 2063 lambda_n: 0.8873900693846041 Loss: 0.025431376305478884\n",
      "Iteration: 2064 lambda_n: 1.0064656305818158 Loss: 0.025391570736916\n",
      "Iteration: 2065 lambda_n: 1.0291517724347226 Loss: 0.025346489047966398\n",
      "Iteration: 2066 lambda_n: 0.9783492107051324 Loss: 0.02530046399362471\n",
      "Iteration: 2067 lambda_n: 0.8985298136371892 Loss: 0.025256779892632253\n",
      "Iteration: 2068 lambda_n: 1.0105591314020794 Loss: 0.02521671938326769\n",
      "Iteration: 2069 lambda_n: 0.8830313936107603 Loss: 0.02517173064299446\n",
      "Iteration: 2070 lambda_n: 0.95690509575305 Loss: 0.02513247889505485\n",
      "Iteration: 2071 lambda_n: 1.0127958646867699 Loss: 0.02509000458388942\n",
      "Iteration: 2072 lambda_n: 1.0314038834738413 Loss: 0.025045119111174425\n",
      "Iteration: 2073 lambda_n: 0.9507760870702031 Loss: 0.024999483079004767\n",
      "Iteration: 2074 lambda_n: 0.9178303973332238 Loss: 0.024957481727817105\n",
      "Iteration: 2075 lambda_n: 1.001144446374509 Loss: 0.024916996666469963\n",
      "Iteration: 2076 lambda_n: 0.9412635270943563 Loss: 0.024872903951280674\n",
      "Iteration: 2077 lambda_n: 0.9710652544787354 Loss: 0.024831513852722243\n",
      "Iteration: 2078 lambda_n: 0.8951882858385413 Loss: 0.02478887899080254\n",
      "Iteration: 2079 lambda_n: 1.0352420915319742 Loss: 0.024749635543876998\n",
      "Iteration: 2080 lambda_n: 0.9620052761212294 Loss: 0.024704322348848867\n",
      "Iteration: 2081 lambda_n: 0.9410162201951081 Loss: 0.02466228395734516\n",
      "Iteration: 2082 lambda_n: 1.0126660240782275 Loss: 0.024621226950005223\n",
      "Iteration: 2083 lambda_n: 0.9771464629385832 Loss: 0.024577114052700383\n",
      "Iteration: 2084 lambda_n: 0.9072865687227775 Loss: 0.024534618469044544\n",
      "Iteration: 2085 lambda_n: 0.98820242318504 Loss: 0.0244952230233864\n",
      "Iteration: 2086 lambda_n: 1.0360748916194964 Loss: 0.024452380849613046\n",
      "Iteration: 2087 lambda_n: 0.8924114224280075 Loss: 0.02440753842122244\n",
      "Iteration: 2088 lambda_n: 0.9401605782131872 Loss: 0.0243689771316124\n",
      "Iteration: 2089 lambda_n: 0.9517383585119945 Loss: 0.024328414532291075\n",
      "Iteration: 2090 lambda_n: 0.9957606463344036 Loss: 0.02428741756177189\n",
      "Iteration: 2091 lambda_n: 1.0231535906052083 Loss: 0.024244594306438706\n",
      "Iteration: 2092 lambda_n: 0.9328787792642294 Loss: 0.02420066787056243\n",
      "Iteration: 2093 lambda_n: 0.996825519902394 Loss: 0.024160684308101595\n",
      "Iteration: 2094 lambda_n: 1.0255575617587096 Loss: 0.024118029614175664\n",
      "Iteration: 2095 lambda_n: 0.9838809572004494 Loss: 0.024074221024750854\n",
      "Iteration: 2096 lambda_n: 0.9866835110557404 Loss: 0.024032265458971705\n",
      "Iteration: 2097 lambda_n: 0.8926448078664145 Loss: 0.023990261682977902\n",
      "Iteration: 2098 lambda_n: 1.0201266390947201 Loss: 0.023952323527913537\n",
      "Iteration: 2099 lambda_n: 0.9438768902679734 Loss: 0.02390903804997039\n",
      "Iteration: 2100 lambda_n: 0.9804461990333727 Loss: 0.02386905690644068\n",
      "Iteration: 2101 lambda_n: 1.0283406951088685 Loss: 0.0238275961494897\n",
      "Iteration: 2102 lambda_n: 0.8831468890683548 Loss: 0.023784186099119507\n",
      "Iteration: 2103 lambda_n: 0.9344116828675133 Loss: 0.02374696883824242\n",
      "Iteration: 2104 lambda_n: 0.9672625027782685 Loss: 0.023707653884002984\n",
      "Iteration: 2105 lambda_n: 0.9174807904234795 Loss: 0.023667024926410873\n",
      "Iteration: 2106 lambda_n: 0.9619489428370496 Loss: 0.02362855181863328\n",
      "Iteration: 2107 lambda_n: 0.9904255826689368 Loss: 0.023588281170610157\n",
      "Iteration: 2108 lambda_n: 0.9819777571971551 Loss: 0.02354689049549163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2109 lambda_n: 0.9364368086662805 Loss: 0.023505925514836747\n",
      "Iteration: 2110 lambda_n: 0.9917859907837865 Loss: 0.02346692814554529\n",
      "Iteration: 2111 lambda_n: 1.0125855367881589 Loss: 0.023425697294887124\n",
      "Iteration: 2112 lambda_n: 0.9919697578406093 Loss: 0.023383678086017826\n",
      "Iteration: 2113 lambda_n: 0.9852461379210516 Loss: 0.023342589618202875\n",
      "Iteration: 2114 lambda_n: 0.9473952062176265 Loss: 0.023301853407372366\n",
      "Iteration: 2115 lambda_n: 0.928887492895202 Loss: 0.02326275187763789\n",
      "Iteration: 2116 lambda_n: 0.9147621666676848 Loss: 0.023224480535558353\n",
      "Iteration: 2117 lambda_n: 1.0156819422021854 Loss: 0.023186855443805752\n",
      "Iteration: 2118 lambda_n: 0.9473024613334492 Loss: 0.023145153373917864\n",
      "Iteration: 2119 lambda_n: 0.9174886957257169 Loss: 0.023106330368464854\n",
      "Iteration: 2120 lambda_n: 1.0346153811833538 Loss: 0.023068794896653314\n",
      "Iteration: 2121 lambda_n: 0.9305755365705017 Loss: 0.023026544159811473\n",
      "Iteration: 2122 lambda_n: 1.0135335128325265 Loss: 0.022988613114043027\n",
      "Iteration: 2123 lambda_n: 1.0107137088021696 Loss: 0.02294737584943447\n",
      "Iteration: 2124 lambda_n: 0.9028365649168217 Loss: 0.022906332262537417\n",
      "Iteration: 2125 lambda_n: 0.8854707675225723 Loss: 0.0228697369237752\n",
      "Iteration: 2126 lambda_n: 0.9036478462588713 Loss: 0.02283390692769927\n",
      "Iteration: 2127 lambda_n: 0.9052828401573679 Loss: 0.022797404016030958\n",
      "Iteration: 2128 lambda_n: 0.975800726655088 Loss: 0.02276089870936747\n",
      "Iteration: 2129 lambda_n: 0.9904955400973978 Loss: 0.02272162077264262\n",
      "Iteration: 2130 lambda_n: 0.9052836151633029 Loss: 0.022681827253665243\n",
      "Iteration: 2131 lambda_n: 0.9019394121569979 Loss: 0.022645524853303033\n",
      "Iteration: 2132 lambda_n: 0.9320266525603699 Loss: 0.022609420460487622\n",
      "Iteration: 2133 lambda_n: 0.8920113467972024 Loss: 0.022572178598861296\n",
      "Iteration: 2134 lambda_n: 0.9067867222685114 Loss: 0.022536599947916604\n",
      "Iteration: 2135 lambda_n: 0.8873690361956046 Loss: 0.022500496177252707\n",
      "Iteration: 2136 lambda_n: 0.983155460626609 Loss: 0.02246522854083755\n",
      "Iteration: 2137 lambda_n: 0.9563770960707918 Loss: 0.02242622607716467\n",
      "Iteration: 2138 lambda_n: 1.003228426311804 Loss: 0.022388359700594423\n",
      "Iteration: 2139 lambda_n: 0.8826907750424176 Loss: 0.022348716134715195\n",
      "Iteration: 2140 lambda_n: 0.8911853462759709 Loss: 0.022313902825824575\n",
      "Iteration: 2141 lambda_n: 0.9006857008376272 Loss: 0.022278817553209745\n",
      "Iteration: 2142 lambda_n: 0.953996455597136 Loss: 0.02224342276670182\n",
      "Iteration: 2143 lambda_n: 1.026748843266841 Loss: 0.02220600357214885\n",
      "Iteration: 2144 lambda_n: 0.9477241149704251 Loss: 0.022165811959429897\n",
      "Iteration: 2145 lambda_n: 1.0374639329303814 Loss: 0.022128789632599666\n",
      "Iteration: 2146 lambda_n: 0.9485003812172703 Loss: 0.02208834416167152\n",
      "Iteration: 2147 lambda_n: 1.017329306478041 Loss: 0.022051443750461374\n",
      "Iteration: 2148 lambda_n: 0.9216130137883721 Loss: 0.022011946255647463\n",
      "Iteration: 2149 lambda_n: 0.9246751146936153 Loss: 0.02197623818699452\n",
      "Iteration: 2150 lambda_n: 1.0155101887821776 Loss: 0.021940481029441253\n",
      "Iteration: 2151 lambda_n: 0.9000284498501063 Loss: 0.02190129112665533\n",
      "Iteration: 2152 lambda_n: 0.9725149870970061 Loss: 0.02186662912733427\n",
      "Iteration: 2153 lambda_n: 0.8903120381605166 Loss: 0.02182924977453754\n",
      "Iteration: 2154 lambda_n: 0.9150812393774529 Loss: 0.021795098683208055\n",
      "Iteration: 2155 lambda_n: 0.9357244810623501 Loss: 0.02176006544739657\n",
      "Iteration: 2156 lambda_n: 0.9046903014973614 Loss: 0.021724313346674132\n",
      "Iteration: 2157 lambda_n: 0.9209549730892641 Loss: 0.02168981616736373\n",
      "Iteration: 2158 lambda_n: 0.9826758259666647 Loss: 0.021654768512986963\n",
      "Iteration: 2159 lambda_n: 0.9930664834199726 Loss: 0.021617449492186903\n",
      "Iteration: 2160 lambda_n: 0.9128365356964476 Loss: 0.021579817689282386\n",
      "Iteration: 2161 lambda_n: 1.037580995224851 Loss: 0.021545299454150377\n",
      "Iteration: 2162 lambda_n: 1.020596653583263 Loss: 0.02150614814585251\n",
      "Iteration: 2163 lambda_n: 0.9031149534280074 Loss: 0.021467726199924313\n",
      "Iteration: 2164 lambda_n: 0.984136118641864 Loss: 0.021433801022426074\n",
      "Iteration: 2165 lambda_n: 0.9122325025961742 Loss: 0.0213969103480998\n",
      "Iteration: 2166 lambda_n: 0.9834597230309108 Loss: 0.021362788796527112\n",
      "Iteration: 2167 lambda_n: 0.9745413611690349 Loss: 0.021326081849296498\n",
      "Iteration: 2168 lambda_n: 1.0302877406698945 Loss: 0.021289789193382452\n",
      "Iteration: 2169 lambda_n: 0.95506675465132 Loss: 0.0212515084577902\n",
      "Iteration: 2170 lambda_n: 0.990942976139805 Loss: 0.021216104368358756\n",
      "Iteration: 2171 lambda_n: 0.8948971548284955 Loss: 0.021179453070081813\n",
      "Iteration: 2172 lambda_n: 0.8831204416269973 Loss: 0.021146427490218504\n",
      "Iteration: 2173 lambda_n: 0.9294113607907628 Loss: 0.021113904500786342\n",
      "Iteration: 2174 lambda_n: 0.9115518854471792 Loss: 0.02107974952658903\n",
      "Iteration: 2175 lambda_n: 1.0281281575771801 Loss: 0.021046323958146273\n",
      "Iteration: 2176 lambda_n: 1.030762582258555 Loss: 0.021008709909652184\n",
      "Iteration: 2177 lambda_n: 1.0135439388352978 Loss: 0.020971092332048098\n",
      "Iteration: 2178 lambda_n: 0.9613059442651115 Loss: 0.02093419422147166\n",
      "Iteration: 2179 lambda_n: 0.988132193250256 Loss: 0.02089928176244908\n",
      "Iteration: 2180 lambda_n: 1.0192690736425045 Loss: 0.0208634799843046\n",
      "Iteration: 2181 lambda_n: 1.008010267280365 Loss: 0.020826640569815823\n",
      "Iteration: 2182 lambda_n: 0.9027716543771199 Loss: 0.02079029904653654\n",
      "Iteration: 2183 lambda_n: 0.9613148114014118 Loss: 0.02075782922066208\n",
      "Iteration: 2184 lambda_n: 0.9624815576367085 Loss: 0.020723333670540585\n",
      "Iteration: 2185 lambda_n: 0.9415411403191855 Loss: 0.020688879418418144\n",
      "Iteration: 2186 lambda_n: 0.9090348509434975 Loss: 0.020655255691831697\n",
      "Iteration: 2187 lambda_n: 0.9910647002430255 Loss: 0.020622869055606154\n",
      "Iteration: 2188 lambda_n: 0.965462429316666 Loss: 0.02058764486151089\n",
      "Iteration: 2189 lambda_n: 0.9454729569388914 Loss: 0.02055341677495076\n",
      "Iteration: 2190 lambda_n: 1.0076039723505814 Loss: 0.020519980038278565\n",
      "Iteration: 2191 lambda_n: 0.9489787095247006 Loss: 0.020484435920743723\n",
      "Iteration: 2192 lambda_n: 1.0374424694224782 Loss: 0.0204510456580611\n",
      "Iteration: 2193 lambda_n: 1.0010042816508724 Loss: 0.020414637452216233\n",
      "Iteration: 2194 lambda_n: 1.0290715125326841 Loss: 0.020379602888348444\n",
      "Iteration: 2195 lambda_n: 1.0032619910795224 Loss: 0.020343683131438785\n",
      "Iteration: 2196 lambda_n: 0.9821192971128593 Loss: 0.020308759767713316\n",
      "Iteration: 2197 lambda_n: 0.9061079329687246 Loss: 0.02027466406645764\n",
      "Iteration: 2198 lambda_n: 0.9963908133355273 Loss: 0.020243288270585105\n",
      "Iteration: 2199 lambda_n: 0.9108083868454216 Loss: 0.020208875481742283\n",
      "Iteration: 2200 lambda_n: 0.9483775044573945 Loss: 0.020177501494694768\n",
      "Iteration: 2201 lambda_n: 0.8993474208387764 Loss: 0.020144917343066532\n",
      "Iteration: 2202 lambda_n: 0.9885566443625522 Loss: 0.020114097633280113\n",
      "Iteration: 2203 lambda_n: 0.9033355378360955 Loss: 0.020080310138939614\n",
      "Iteration: 2204 lambda_n: 0.973698439727633 Loss: 0.02004951839949479\n",
      "Iteration: 2205 lambda_n: 0.89230433368736 Loss: 0.0200164165586927\n",
      "Iteration: 2206 lambda_n: 0.9954047308930402 Loss: 0.019986163356987104\n",
      "Iteration: 2207 lambda_n: 0.9315198671580961 Loss: 0.019952506040866142\n",
      "Iteration: 2208 lambda_n: 0.9156598408144323 Loss: 0.019921097466483084\n",
      "Iteration: 2209 lambda_n: 0.9579590227843724 Loss: 0.01989030732163084\n",
      "Iteration: 2210 lambda_n: 1.0200528343569772 Loss: 0.019858183717791973\n",
      "Iteration: 2211 lambda_n: 1.0111971012394474 Loss: 0.019824078235872074\n",
      "Iteration: 2212 lambda_n: 1.0310846062011483 Loss: 0.01979037190584753\n",
      "Iteration: 2213 lambda_n: 0.960846466607952 Loss: 0.019756108730369575\n",
      "Iteration: 2214 lambda_n: 0.9776045928200983 Loss: 0.01972427695151294\n",
      "Iteration: 2215 lambda_n: 1.0311919271163714 Loss: 0.019691986480027786\n",
      "Iteration: 2216 lambda_n: 0.9713585557656723 Loss: 0.019658031862143783\n",
      "Iteration: 2217 lambda_n: 0.904503148374864 Loss: 0.019626147931789663\n",
      "Iteration: 2218 lambda_n: 0.8885601670141421 Loss: 0.01959654667595403\n",
      "Iteration: 2219 lambda_n: 1.0322605277699743 Loss: 0.019567550228642528\n",
      "Iteration: 2220 lambda_n: 0.9085030549069251 Loss: 0.019533967493591323\n",
      "Iteration: 2221 lambda_n: 0.8988426667329007 Loss: 0.01950450441105997\n",
      "Iteration: 2222 lambda_n: 0.960626838428534 Loss: 0.01947544071126102\n",
      "Iteration: 2223 lambda_n: 1.0168934709609438 Loss: 0.019444474113036188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2224 lambda_n: 0.8897104032430718 Loss: 0.019411801322171324\n",
      "Iteration: 2225 lambda_n: 0.980747504315433 Loss: 0.019386546891969097\n",
      "Iteration: 2226 lambda_n: 1.0312688843549187 Loss: 0.019359902209696345\n",
      "Iteration: 2227 lambda_n: 0.9943509715830104 Loss: 0.019331994326975445\n",
      "Iteration: 2228 lambda_n: 0.8880615003782318 Loss: 0.01930513596462175\n",
      "Iteration: 2229 lambda_n: 0.9657813079369278 Loss: 0.01928125410536604\n",
      "Iteration: 2230 lambda_n: 0.8854353745946816 Loss: 0.019255425208398798\n",
      "Iteration: 2231 lambda_n: 0.9442938732584396 Loss: 0.019231900798537354\n",
      "Iteration: 2232 lambda_n: 0.9995870606694572 Loss: 0.019206978713247176\n",
      "Iteration: 2233 lambda_n: 1.0060378072252787 Loss: 0.019180788982238774\n",
      "Iteration: 2234 lambda_n: 1.0341752566840543 Loss: 0.0191546342474176\n",
      "Iteration: 2235 lambda_n: 1.0099898538982686 Loss: 0.019127958150860797\n",
      "Iteration: 2236 lambda_n: 0.9793888333137182 Loss: 0.019102112271521397\n",
      "Iteration: 2237 lambda_n: 0.9917477030816853 Loss: 0.019077240672871063\n",
      "Iteration: 2238 lambda_n: 0.9581097509328261 Loss: 0.019052239537259274\n",
      "Iteration: 2239 lambda_n: 0.9366733390587646 Loss: 0.01902826138255023\n",
      "Iteration: 2240 lambda_n: 1.013871278313404 Loss: 0.019004981120043955\n",
      "Iteration: 2241 lambda_n: 0.9798167411243803 Loss: 0.018979950906457967\n",
      "Iteration: 2242 lambda_n: 0.9949690884944666 Loss: 0.018955931363059484\n",
      "Iteration: 2243 lambda_n: 0.9571043686223217 Loss: 0.018931703884224677\n",
      "Iteration: 2244 lambda_n: 0.9631115699518082 Loss: 0.018908553282126556\n",
      "Iteration: 2245 lambda_n: 0.9532368516288531 Loss: 0.018885404443042913\n",
      "Iteration: 2246 lambda_n: 1.033200917610232 Loss: 0.018862635764083355\n",
      "Iteration: 2247 lambda_n: 1.012963073374197 Loss: 0.018838108434266847\n",
      "Iteration: 2248 lambda_n: 0.9680661473915352 Loss: 0.01881421682085779\n",
      "Iteration: 2249 lambda_n: 0.916034833413745 Loss: 0.018791525764625505\n",
      "Iteration: 2250 lambda_n: 1.0128486348599153 Loss: 0.01877017929876713\n",
      "Iteration: 2251 lambda_n: 0.8859502323812694 Loss: 0.01874670706727554\n",
      "Iteration: 2252 lambda_n: 0.9049761986156907 Loss: 0.01872629578494088\n",
      "Iteration: 2253 lambda_n: 0.9938738874718759 Loss: 0.018705552953443016\n",
      "Iteration: 2254 lambda_n: 0.9461314644890991 Loss: 0.018682890967984516\n",
      "Iteration: 2255 lambda_n: 1.0277905008155883 Loss: 0.018661436934548213\n",
      "Iteration: 2256 lambda_n: 1.0060108819853788 Loss: 0.018638253859008896\n",
      "Iteration: 2257 lambda_n: 0.9099981098711837 Loss: 0.018615688186180805\n",
      "Iteration: 2258 lambda_n: 0.9784019257701687 Loss: 0.018595384496367082\n",
      "Iteration: 2259 lambda_n: 0.9188138806425338 Loss: 0.01857365973703058\n",
      "Iteration: 2260 lambda_n: 0.8933284588047683 Loss: 0.018553360616758732\n",
      "Iteration: 2261 lambda_n: 0.9373875440043183 Loss: 0.018533716598544348\n",
      "Iteration: 2262 lambda_n: 0.9786419016122433 Loss: 0.018513196647998713\n",
      "Iteration: 2263 lambda_n: 0.8886280410516217 Loss: 0.018491873430034805\n",
      "Iteration: 2264 lambda_n: 0.9882581979633921 Loss: 0.018472602891991777\n",
      "Iteration: 2265 lambda_n: 0.9874966332350602 Loss: 0.018451264315627606\n",
      "Iteration: 2266 lambda_n: 1.010505591260854 Loss: 0.01843004184991451\n",
      "Iteration: 2267 lambda_n: 1.0118451058516427 Loss: 0.018408425014629498\n",
      "Iteration: 2268 lambda_n: 0.9803690063437351 Loss: 0.018386879814892912\n",
      "Iteration: 2269 lambda_n: 0.944716781857059 Loss: 0.018366099827454272\n",
      "Iteration: 2270 lambda_n: 0.9846229633711651 Loss: 0.018346162387870932\n",
      "Iteration: 2271 lambda_n: 0.9265712721364409 Loss: 0.018325469020138135\n",
      "Iteration: 2272 lambda_n: 0.9435095846305463 Loss: 0.018306077820143166\n",
      "Iteration: 2273 lambda_n: 0.9614788419871247 Loss: 0.01828640994726652\n",
      "Iteration: 2274 lambda_n: 0.9392682855344902 Loss: 0.01826644676530082\n",
      "Iteration: 2275 lambda_n: 0.9798812586335529 Loss: 0.018247021850384293\n",
      "Iteration: 2276 lambda_n: 1.025008241152062 Loss: 0.018226834667769382\n",
      "Iteration: 2277 lambda_n: 0.9913228460723293 Loss: 0.01820580098446284\n",
      "Iteration: 2278 lambda_n: 0.9391277828573722 Loss: 0.018185540430316123\n",
      "Iteration: 2279 lambda_n: 1.0195096527634695 Loss: 0.01816642004515299\n",
      "Iteration: 2280 lambda_n: 0.9441758798407687 Loss: 0.018145738222580136\n",
      "Iteration: 2281 lambda_n: 0.9658754355410314 Loss: 0.018126657586903447\n",
      "Iteration: 2282 lambda_n: 0.984367310086484 Loss: 0.01810720689541288\n",
      "Iteration: 2283 lambda_n: 0.9786618670885804 Loss: 0.018087453883369388\n",
      "Iteration: 2284 lambda_n: 1.0242710228471876 Loss: 0.01806788488235098\n",
      "Iteration: 2285 lambda_n: 0.9318228712400813 Loss: 0.018047475260163277\n",
      "Iteration: 2286 lambda_n: 0.9008730165876612 Loss: 0.018028973565519267\n",
      "Iteration: 2287 lambda_n: 0.9713109188318069 Loss: 0.01801114350526735\n",
      "Iteration: 2288 lambda_n: 0.8972106639193501 Loss: 0.01799197847779783\n",
      "Iteration: 2289 lambda_n: 0.9335648503120324 Loss: 0.01797433257501784\n",
      "Iteration: 2290 lambda_n: 1.007662170446749 Loss: 0.017956026142495838\n",
      "Iteration: 2291 lambda_n: 0.938907065275243 Loss: 0.017936327092117975\n",
      "Iteration: 2292 lambda_n: 1.0209804424539362 Loss: 0.01791803098523817\n",
      "Iteration: 2293 lambda_n: 1.0105862525985239 Loss: 0.0178981949339838\n",
      "Iteration: 2294 lambda_n: 1.0071531914024545 Loss: 0.01787862306246774\n",
      "Iteration: 2295 lambda_n: 0.9420177069460425 Loss: 0.017859177975635118\n",
      "Iteration: 2296 lambda_n: 0.9873654663550804 Loss: 0.017841045328640937\n",
      "Iteration: 2297 lambda_n: 1.0257313672010728 Loss: 0.017822093173243046\n",
      "Iteration: 2298 lambda_n: 0.9172798453292449 Loss: 0.01780246167464303\n",
      "Iteration: 2299 lambda_n: 0.8878271181998102 Loss: 0.01778495723782066\n",
      "Iteration: 2300 lambda_n: 1.011126878170304 Loss: 0.017768058840043607\n",
      "Iteration: 2301 lambda_n: 0.9255892984950203 Loss: 0.017748862061870745\n",
      "Iteration: 2302 lambda_n: 0.9514037343240713 Loss: 0.01773133796404586\n",
      "Iteration: 2303 lambda_n: 0.9390709386379515 Loss: 0.01771337059881789\n",
      "Iteration: 2304 lambda_n: 0.9229782067611436 Loss: 0.017695681358064853\n",
      "Iteration: 2305 lambda_n: 0.9351229450841689 Loss: 0.017678338376949196\n",
      "Iteration: 2306 lambda_n: 0.9522456321455604 Loss: 0.017660809541486633\n",
      "Iteration: 2307 lambda_n: 0.9688613099105776 Loss: 0.01764300274595114\n",
      "Iteration: 2308 lambda_n: 1.0035832008896455 Loss: 0.01762492906460196\n",
      "Iteration: 2309 lambda_n: 0.9185389943984863 Loss: 0.017606253152485395\n",
      "Iteration: 2310 lambda_n: 0.962172091121484 Loss: 0.01758920181867849\n",
      "Iteration: 2311 lambda_n: 0.8880737287916177 Loss: 0.0175713804985643\n",
      "Iteration: 2312 lambda_n: 0.9939954152984382 Loss: 0.01755496928815717\n",
      "Iteration: 2313 lambda_n: 0.9075964691243061 Loss: 0.017536639594840447\n",
      "Iteration: 2314 lambda_n: 0.9066235285992872 Loss: 0.01751994161588989\n",
      "Iteration: 2315 lambda_n: 0.9084110035692106 Loss: 0.017503296297972207\n",
      "Iteration: 2316 lambda_n: 0.9807637587265714 Loss: 0.017486652426884193\n",
      "Iteration: 2317 lambda_n: 0.9369922690236978 Loss: 0.017468719636152874\n",
      "Iteration: 2318 lambda_n: 0.9988690925658594 Loss: 0.017451624105160816\n",
      "Iteration: 2319 lambda_n: 0.9308473682244341 Loss: 0.01743343693701977\n",
      "Iteration: 2320 lambda_n: 0.9683394295068557 Loss: 0.017416524383435324\n",
      "Iteration: 2321 lambda_n: 0.9274317717273912 Loss: 0.017398965343487883\n",
      "Iteration: 2322 lambda_n: 0.8986729872352078 Loss: 0.017382181906453978\n",
      "Iteration: 2323 lambda_n: 1.0298915471039003 Loss: 0.017365949819426997\n",
      "Iteration: 2324 lambda_n: 0.9639748107823165 Loss: 0.017347381887602657\n",
      "Iteration: 2325 lambda_n: 0.9168684784229847 Loss: 0.017330037981526113\n",
      "Iteration: 2326 lambda_n: 0.9579947071852097 Loss: 0.017313572824728653\n",
      "Iteration: 2327 lambda_n: 0.9521405848410986 Loss: 0.01729639986585608\n",
      "Iteration: 2328 lambda_n: 1.0349955093663576 Loss: 0.017279363169000132\n",
      "Iteration: 2329 lambda_n: 0.8957863930509601 Loss: 0.01726087748333738\n",
      "Iteration: 2330 lambda_n: 0.8964621853984194 Loss: 0.01724490871146321\n",
      "Iteration: 2331 lambda_n: 0.9053917290825882 Loss: 0.017228954180422397\n",
      "Iteration: 2332 lambda_n: 0.8949274241577961 Loss: 0.01721286692848792\n",
      "Iteration: 2333 lambda_n: 0.8836063771826196 Loss: 0.017196991343222636\n",
      "Iteration: 2334 lambda_n: 0.9137292612097654 Loss: 0.017181341333787955\n",
      "Iteration: 2335 lambda_n: 0.9039532347528898 Loss: 0.01716518278707018\n",
      "Iteration: 2336 lambda_n: 1.019929994427713 Loss: 0.01714922223492907\n",
      "Iteration: 2337 lambda_n: 0.890295028013813 Loss: 0.01713124184338654\n",
      "Iteration: 2338 lambda_n: 0.9923840274078986 Loss: 0.017115573375304654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2339 lambda_n: 0.9536847866541212 Loss: 0.01709813410940782\n",
      "Iteration: 2340 lambda_n: 0.985257899108016 Loss: 0.017081401946578114\n",
      "Iteration: 2341 lambda_n: 1.0134899302321738 Loss: 0.01706414239443589\n",
      "Iteration: 2342 lambda_n: 0.9850544357610427 Loss: 0.0170464160584751\n",
      "Iteration: 2343 lambda_n: 0.8855538705754863 Loss: 0.0170292143040113\n",
      "Iteration: 2344 lambda_n: 1.0282220788443701 Loss: 0.017013773405918527\n",
      "Iteration: 2345 lambda_n: 0.9438404753744123 Loss: 0.016995869257313174\n",
      "Iteration: 2346 lambda_n: 1.0369014089351423 Loss: 0.016979459620735438\n",
      "Iteration: 2347 lambda_n: 0.9511632325139788 Loss: 0.016961457328602957\n",
      "Iteration: 2348 lambda_n: 0.9132291958165935 Loss: 0.016944968413835047\n",
      "Iteration: 2349 lambda_n: 0.9463784299021488 Loss: 0.016929158680045846\n",
      "Iteration: 2350 lambda_n: 1.0119528681432701 Loss: 0.01691279632349673\n",
      "Iteration: 2351 lambda_n: 0.9695452377200277 Loss: 0.016895323483621837\n",
      "Iteration: 2352 lambda_n: 0.927610317026657 Loss: 0.01687860618680135\n",
      "Iteration: 2353 lambda_n: 0.9566368033564451 Loss: 0.016862632986163008\n",
      "Iteration: 2354 lambda_n: 0.917434891445409 Loss: 0.016846180500669758\n",
      "Iteration: 2355 lambda_n: 1.0119015419864243 Loss: 0.016830422156914264\n",
      "Iteration: 2356 lambda_n: 0.9405069670923618 Loss: 0.0168130621681144\n",
      "Iteration: 2357 lambda_n: 1.0023943869121048 Loss: 0.01679694797231829\n",
      "Iteration: 2358 lambda_n: 0.8993959174197559 Loss: 0.016779794042116266\n",
      "Iteration: 2359 lambda_n: 1.007704938533215 Loss: 0.01676442195316743\n",
      "Iteration: 2360 lambda_n: 1.0279814228198956 Loss: 0.01674721799062601\n",
      "Iteration: 2361 lambda_n: 1.0230293574953344 Loss: 0.016729689479706423\n",
      "Iteration: 2362 lambda_n: 1.0159822105718397 Loss: 0.016712266989015727\n",
      "Iteration: 2363 lambda_n: 0.9882823055831134 Loss: 0.016694985507688\n",
      "Iteration: 2364 lambda_n: 0.9023168568320175 Loss: 0.016678195136397445\n",
      "Iteration: 2365 lambda_n: 1.0301830537789862 Loss: 0.016662882655292338\n",
      "Iteration: 2366 lambda_n: 0.9182546683903042 Loss: 0.01664541834846581\n",
      "Iteration: 2367 lambda_n: 0.9972411037258021 Loss: 0.016629869420373478\n",
      "Iteration: 2368 lambda_n: 0.9196206474592459 Loss: 0.016613000253323282\n",
      "Iteration: 2369 lambda_n: 1.0121254845190246 Loss: 0.016597460982116126\n",
      "Iteration: 2370 lambda_n: 0.9901946611731607 Loss: 0.01658037565801656\n",
      "Iteration: 2371 lambda_n: 0.9665357674561647 Loss: 0.016563678518781044\n",
      "Iteration: 2372 lambda_n: 0.9978111564145122 Loss: 0.016547397236439873\n",
      "Iteration: 2373 lambda_n: 0.8991171679645835 Loss: 0.016530605960589324\n",
      "Iteration: 2374 lambda_n: 0.9321555206714023 Loss: 0.016515490850707002\n",
      "Iteration: 2375 lambda_n: 0.8972241959634728 Loss: 0.016499834548209642\n",
      "Iteration: 2376 lambda_n: 0.9223695769629879 Loss: 0.01648477888919736\n",
      "Iteration: 2377 lambda_n: 1.0163530220268113 Loss: 0.016469314938897958\n",
      "Iteration: 2378 lambda_n: 0.9844379090598209 Loss: 0.016452290632580732\n",
      "Iteration: 2379 lambda_n: 0.9163988534733918 Loss: 0.01643581693352205\n",
      "Iteration: 2380 lambda_n: 0.976248522680478 Loss: 0.016420496008676382\n",
      "Iteration: 2381 lambda_n: 1.027762944748011 Loss: 0.016404188454900423\n",
      "Iteration: 2382 lambda_n: 1.011599071400377 Loss: 0.01638703583965665\n",
      "Iteration: 2383 lambda_n: 1.0376339756970465 Loss: 0.01637016871555023\n",
      "Iteration: 2384 lambda_n: 0.9010905950579126 Loss: 0.016352883165850422\n",
      "Iteration: 2385 lambda_n: 0.971317095536484 Loss: 0.01633788589186389\n",
      "Iteration: 2386 lambda_n: 1.0007585908994092 Loss: 0.01632173252224264\n",
      "Iteration: 2387 lambda_n: 1.0172508950183154 Loss: 0.016305103434852978\n",
      "Iteration: 2388 lambda_n: 1.036963869513428 Loss: 0.01628821465139541\n",
      "Iteration: 2389 lambda_n: 0.9628430307995675 Loss: 0.01627101323926982\n",
      "Iteration: 2390 lambda_n: 0.8958959885391926 Loss: 0.016255054970881738\n",
      "Iteration: 2391 lambda_n: 1.0167213158079729 Loss: 0.016240217875666835\n",
      "Iteration: 2392 lambda_n: 0.9719821437020787 Loss: 0.01622339194833012\n",
      "Iteration: 2393 lambda_n: 1.0147326798924972 Loss: 0.01620731935345599\n",
      "Iteration: 2394 lambda_n: 0.9275510149497252 Loss: 0.01619055261276849\n",
      "Iteration: 2395 lambda_n: 0.9043551583823084 Loss: 0.016175238351370993\n",
      "Iteration: 2396 lambda_n: 0.9692149666620672 Loss: 0.01616031758919082\n",
      "Iteration: 2397 lambda_n: 0.946813306761691 Loss: 0.016144337618009378\n",
      "Iteration: 2398 lambda_n: 1.005152902107565 Loss: 0.01612873821170866\n",
      "Iteration: 2399 lambda_n: 0.9850658562312824 Loss: 0.01611218913421614\n",
      "Iteration: 2400 lambda_n: 0.8910616664509946 Loss: 0.01609598255098995\n",
      "Iteration: 2401 lambda_n: 1.026708388870682 Loss: 0.01608133281055763\n",
      "Iteration: 2402 lambda_n: 0.9620379622930207 Loss: 0.01606446358786178\n",
      "Iteration: 2403 lambda_n: 0.8845639876246932 Loss: 0.01604866818099204\n",
      "Iteration: 2404 lambda_n: 1.0245102803701593 Loss: 0.016034154346790216\n",
      "Iteration: 2405 lambda_n: 1.032249068127125 Loss: 0.01601735442032163\n",
      "Iteration: 2406 lambda_n: 0.9457730244195974 Loss: 0.01600043919624782\n",
      "Iteration: 2407 lambda_n: 1.0245592109059722 Loss: 0.01598495155397493\n",
      "Iteration: 2408 lambda_n: 1.028251403337404 Loss: 0.015968184096136786\n",
      "Iteration: 2409 lambda_n: 0.970482111781248 Loss: 0.015951367290667744\n",
      "Iteration: 2410 lambda_n: 0.9394865864586036 Loss: 0.015935505606475702\n",
      "Iteration: 2411 lambda_n: 0.9730698770168026 Loss: 0.01592015981911866\n",
      "Iteration: 2412 lambda_n: 0.9708744697675868 Loss: 0.01590427469935798\n",
      "Iteration: 2413 lambda_n: 0.9455323763075362 Loss: 0.015888434813253277\n",
      "Iteration: 2414 lambda_n: 0.9549121177242467 Loss: 0.01587301738214243\n",
      "Iteration: 2415 lambda_n: 0.9351984984584617 Loss: 0.01585745575085154\n",
      "Iteration: 2416 lambda_n: 0.9167460488997922 Loss: 0.015842223903789043\n",
      "Iteration: 2417 lambda_n: 0.9868569917375375 Loss: 0.015827300671978274\n",
      "Iteration: 2418 lambda_n: 0.8966795942063776 Loss: 0.015811244580565383\n",
      "Iteration: 2419 lambda_n: 1.0074435600707634 Loss: 0.015796663770092714\n",
      "Iteration: 2420 lambda_n: 0.9153928116849251 Loss: 0.015780290059471708\n",
      "Iteration: 2421 lambda_n: 1.0094199591405293 Loss: 0.015765420650243982\n",
      "Iteration: 2422 lambda_n: 0.8990478926671859 Loss: 0.015749032070847678\n",
      "Iteration: 2423 lambda_n: 0.920456579768243 Loss: 0.015734443329773376\n",
      "Iteration: 2424 lambda_n: 0.8854715340626956 Loss: 0.015719514309391863\n",
      "Iteration: 2425 lambda_n: 1.0172760656774433 Loss: 0.015705159624963435\n",
      "Iteration: 2426 lambda_n: 0.9574164747369099 Loss: 0.01568867579799108\n",
      "Iteration: 2427 lambda_n: 0.9794204302622834 Loss: 0.015673169975125915\n",
      "Iteration: 2428 lambda_n: 1.0310001239673139 Loss: 0.015657315447763533\n",
      "Iteration: 2429 lambda_n: 1.023866443486847 Loss: 0.015640634114361868\n",
      "Iteration: 2430 lambda_n: 0.9628224299937118 Loss: 0.015624076590123737\n",
      "Iteration: 2431 lambda_n: 0.9487747617808577 Loss: 0.015608513953762315\n",
      "Iteration: 2432 lambda_n: 0.9778947895082883 Loss: 0.01559318543402786\n",
      "Iteration: 2433 lambda_n: 0.9476255793616737 Loss: 0.015577393530820922\n",
      "Iteration: 2434 lambda_n: 1.0142949762817923 Loss: 0.015562097411374338\n",
      "Iteration: 2435 lambda_n: 0.8902415958458518 Loss: 0.015545732302461082\n",
      "Iteration: 2436 lambda_n: 0.9834236176066611 Loss: 0.01553137532598496\n",
      "Iteration: 2437 lambda_n: 1.0014816884745599 Loss: 0.015515521950885305\n",
      "Iteration: 2438 lambda_n: 0.9088979135334947 Loss: 0.015499384510784156\n",
      "Iteration: 2439 lambda_n: 0.9609242350097001 Loss: 0.015484745326170682\n",
      "Iteration: 2440 lambda_n: 0.9977455935402376 Loss: 0.015469274264568376\n",
      "Iteration: 2441 lambda_n: 0.9986642592637197 Loss: 0.015453216967297968\n",
      "Iteration: 2442 lambda_n: 0.9374096474095097 Loss: 0.015437151643410673\n",
      "Iteration: 2443 lambda_n: 0.9586685640802343 Loss: 0.015422077964157201\n",
      "Iteration: 2444 lambda_n: 0.8909380997493761 Loss: 0.015406668375426048\n",
      "Iteration: 2445 lambda_n: 0.9996504300412549 Loss: 0.015392353039750875\n",
      "Iteration: 2446 lambda_n: 1.0311380012347675 Loss: 0.01537629669705068\n",
      "Iteration: 2447 lambda_n: 0.9354330670162694 Loss: 0.015359741161968761\n",
      "Iteration: 2448 lambda_n: 0.9049813837260039 Loss: 0.01534472826236425\n",
      "Iteration: 2449 lambda_n: 0.9120229791594935 Loss: 0.015330209319738533\n",
      "Iteration: 2450 lambda_n: 0.9537818335523537 Loss: 0.015315582450793777\n",
      "Iteration: 2451 lambda_n: 0.8827203330304607 Loss: 0.015300291118105156\n",
      "Iteration: 2452 lambda_n: 1.0168812846509225 Loss: 0.015286144076862962\n",
      "Iteration: 2453 lambda_n: 0.9670472230062581 Loss: 0.015269852192385665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2454 lambda_n: 0.964855326564678 Loss: 0.015254364434784637\n",
      "Iteration: 2455 lambda_n: 1.0222465463918735 Loss: 0.015238917137030539\n",
      "Iteration: 2456 lambda_n: 0.9925140046515721 Loss: 0.015222556605239718\n",
      "Iteration: 2457 lambda_n: 1.0280345909318787 Loss: 0.015206677598754142\n",
      "Iteration: 2458 lambda_n: 0.9264821830716611 Loss: 0.015190235941823331\n",
      "Iteration: 2459 lambda_n: 0.9904180436559733 Loss: 0.015175423616418032\n",
      "Iteration: 2460 lambda_n: 0.9737542236488118 Loss: 0.015159594041003496\n",
      "Iteration: 2461 lambda_n: 0.931033573875649 Loss: 0.015144035914152042\n",
      "Iteration: 2462 lambda_n: 0.9498890312838831 Loss: 0.015129165098072583\n",
      "Iteration: 2463 lambda_n: 0.935300159737371 Loss: 0.015113997690055022\n",
      "Iteration: 2464 lambda_n: 0.9733964134823773 Loss: 0.015099067765969885\n",
      "Iteration: 2465 lambda_n: 1.0344418701070666 Loss: 0.015083534317929838\n",
      "Iteration: 2466 lambda_n: 0.8967260045878691 Loss: 0.015067031729596274\n",
      "Iteration: 2467 lambda_n: 1.0008479562577663 Loss: 0.015052730688013109\n",
      "Iteration: 2468 lambda_n: 0.906632137086493 Loss: 0.015036773467889248\n",
      "Iteration: 2469 lambda_n: 0.975524396175991 Loss: 0.015022322737008993\n",
      "Iteration: 2470 lambda_n: 1.0080263061195711 Loss: 0.015006778129555623\n",
      "Iteration: 2471 lambda_n: 1.016070581886207 Loss: 0.014990720216599236\n",
      "Iteration: 2472 lambda_n: 0.9996449483157273 Loss: 0.014974538885229429\n",
      "Iteration: 2473 lambda_n: 0.949894016091551 Loss: 0.014958623762564719\n",
      "Iteration: 2474 lambda_n: 0.9219864678762986 Loss: 0.014943504975843503\n",
      "Iteration: 2475 lambda_n: 0.9139250003047644 Loss: 0.014928834255711267\n",
      "Iteration: 2476 lambda_n: 0.932574344716685 Loss: 0.014914295500752064\n",
      "Iteration: 2477 lambda_n: 0.9214062407341257 Loss: 0.01489946376155607\n",
      "Iteration: 2478 lambda_n: 0.8998852975359012 Loss: 0.014884813314575042\n",
      "Iteration: 2479 lambda_n: 0.9714919271732013 Loss: 0.014870508553909794\n",
      "Iteration: 2480 lambda_n: 0.9581677963994538 Loss: 0.014855069174532854\n",
      "Iteration: 2481 lambda_n: 0.9582523716491523 Loss: 0.014839845386986229\n",
      "Iteration: 2482 lambda_n: 1.0101451044249385 Loss: 0.014824623995547194\n",
      "Iteration: 2483 lambda_n: 0.977241153989666 Loss: 0.014808582210100183\n",
      "Iteration: 2484 lambda_n: 0.9413170868785363 Loss: 0.014793066880629585\n",
      "Iteration: 2485 lambda_n: 0.9382640563080243 Loss: 0.014778125509628919\n",
      "Iteration: 2486 lambda_n: 0.9873228534552374 Loss: 0.014763236017784427\n",
      "Iteration: 2487 lambda_n: 0.9844159791733456 Loss: 0.014747571548509836\n",
      "Iteration: 2488 lambda_n: 1.0192928462411477 Loss: 0.014731956870006334\n",
      "Iteration: 2489 lambda_n: 0.9455913161773456 Loss: 0.014715792723821293\n",
      "Iteration: 2490 lambda_n: 0.9771424569660759 Loss: 0.014700800895344559\n",
      "Iteration: 2491 lambda_n: 0.9003925459038244 Loss: 0.01468531220062476\n",
      "Iteration: 2492 lambda_n: 1.0002499512682106 Loss: 0.014671043223360363\n",
      "Iteration: 2493 lambda_n: 0.949286205836921 Loss: 0.014655194953716299\n",
      "Iteration: 2494 lambda_n: 0.9295583962515296 Loss: 0.014640157493056707\n",
      "Iteration: 2495 lambda_n: 0.9846163725835176 Loss: 0.014625435587593435\n",
      "Iteration: 2496 lambda_n: 0.9804541042233541 Loss: 0.014609844829962285\n",
      "Iteration: 2497 lambda_n: 0.9611858545657811 Loss: 0.014594323237480777\n",
      "Iteration: 2498 lambda_n: 1.0015600602112662 Loss: 0.014579109820976013\n",
      "Iteration: 2499 lambda_n: 1.0007886491025209 Loss: 0.014563260541712565\n",
      "Iteration: 2500 lambda_n: 0.9891660743662337 Loss: 0.014547426727698532\n",
      "Iteration: 2501 lambda_n: 0.9781941878190783 Loss: 0.014531779974215325\n",
      "Iteration: 2502 lambda_n: 0.8957138360952642 Loss: 0.014516309839892513\n",
      "Iteration: 2503 lambda_n: 0.9573083625966745 Loss: 0.0145021468685208\n",
      "Iteration: 2504 lambda_n: 0.9954651650399717 Loss: 0.014487012619647941\n",
      "Iteration: 2505 lambda_n: 1.0063799916447735 Loss: 0.014471278054890423\n",
      "Iteration: 2506 lambda_n: 0.9375759376421781 Loss: 0.014455373989566287\n",
      "Iteration: 2507 lambda_n: 1.0207681425963429 Loss: 0.014440560057881628\n",
      "Iteration: 2508 lambda_n: 0.9484910312379888 Loss: 0.0144244344857232\n",
      "Iteration: 2509 lambda_n: 0.9423764041191615 Loss: 0.014409453519245602\n",
      "Iteration: 2510 lambda_n: 0.9016658075913387 Loss: 0.014394571692442897\n",
      "Iteration: 2511 lambda_n: 0.9182555860924108 Loss: 0.014380335164441484\n",
      "Iteration: 2512 lambda_n: 0.8990770969578593 Loss: 0.014365839015465558\n",
      "Iteration: 2513 lambda_n: 1.0009635037473632 Loss: 0.014351647912877215\n",
      "Iteration: 2514 lambda_n: 0.9924806831679344 Loss: 0.014335851091526602\n",
      "Iteration: 2515 lambda_n: 0.9740586577362979 Loss: 0.014320190827163641\n",
      "Iteration: 2516 lambda_n: 0.9495295692398403 Loss: 0.014304823820997892\n",
      "Iteration: 2517 lambda_n: 1.0369305853093076 Loss: 0.014289846228058086\n",
      "Iteration: 2518 lambda_n: 1.0062439115268764 Loss: 0.014273492563001136\n",
      "Iteration: 2519 lambda_n: 0.9793942295781989 Loss: 0.014257625545504828\n",
      "Iteration: 2520 lambda_n: 1.0219005671901737 Loss: 0.014242184408100083\n",
      "Iteration: 2521 lambda_n: 0.9128431311718525 Loss: 0.014226075623593046\n",
      "Iteration: 2522 lambda_n: 0.998879118333783 Loss: 0.01421168827608831\n",
      "Iteration: 2523 lambda_n: 0.9237229349359354 Loss: 0.014195947141953357\n",
      "Iteration: 2524 lambda_n: 1.0259051124807945 Loss: 0.014181392603678945\n",
      "Iteration: 2525 lambda_n: 1.0236616921264006 Loss: 0.014165230304693176\n",
      "Iteration: 2526 lambda_n: 0.9187971208754754 Loss: 0.01414910582225867\n",
      "Iteration: 2527 lambda_n: 0.9223299356631771 Loss: 0.014134635326051541\n",
      "Iteration: 2528 lambda_n: 0.9494243881274426 Loss: 0.014120111134952623\n",
      "Iteration: 2529 lambda_n: 0.9532846821835036 Loss: 0.014105162266771526\n",
      "Iteration: 2530 lambda_n: 0.9906282286307161 Loss: 0.014090154646641536\n",
      "Iteration: 2531 lambda_n: 0.903106132091281 Loss: 0.014074561217449018\n",
      "Iteration: 2532 lambda_n: 0.8953663057851081 Loss: 0.014060347425010559\n",
      "Iteration: 2533 lambda_n: 1.0267233619163425 Loss: 0.014046257195437561\n",
      "Iteration: 2534 lambda_n: 0.95165059126649 Loss: 0.014030101789379938\n",
      "Iteration: 2535 lambda_n: 0.9061142527813669 Loss: 0.014015129709444789\n",
      "Iteration: 2536 lambda_n: 1.0216638620408456 Loss: 0.01400087583850128\n",
      "Iteration: 2537 lambda_n: 0.9754477003506077 Loss: 0.013984806193835126\n",
      "Iteration: 2538 lambda_n: 0.9578771978251652 Loss: 0.013969465507262665\n",
      "Iteration: 2539 lambda_n: 0.9819665521152703 Loss: 0.013954403027182706\n",
      "Iteration: 2540 lambda_n: 0.9467896144511426 Loss: 0.013938963614221672\n",
      "Iteration: 2541 lambda_n: 0.972565588633594 Loss: 0.013924079110268862\n",
      "Iteration: 2542 lambda_n: 0.9827705545280785 Loss: 0.013908791166786616\n",
      "Iteration: 2543 lambda_n: 0.9699073163748783 Loss: 0.013893344640246715\n",
      "Iteration: 2544 lambda_n: 0.9102547518949697 Loss: 0.013878102092485883\n",
      "Iteration: 2545 lambda_n: 0.98312646320005 Loss: 0.013863798661384853\n",
      "Iteration: 2546 lambda_n: 1.0242717602648406 Loss: 0.013848351802448445\n",
      "Iteration: 2547 lambda_n: 0.9750496047364289 Loss: 0.013832260307990785\n",
      "Iteration: 2548 lambda_n: 0.938018830072993 Loss: 0.01381694390172636\n",
      "Iteration: 2549 lambda_n: 0.8924215029999346 Loss: 0.013802210814121026\n",
      "Iteration: 2550 lambda_n: 0.914493081135622 Loss: 0.013788195376250485\n",
      "Iteration: 2551 lambda_n: 1.0265070942017422 Loss: 0.01377383472332501\n",
      "Iteration: 2552 lambda_n: 0.9754536472786413 Loss: 0.01375771668366349\n",
      "Iteration: 2553 lambda_n: 0.9010141921597898 Loss: 0.01374240197606277\n",
      "Iteration: 2554 lambda_n: 0.8969668594339172 Loss: 0.013728257446747773\n",
      "Iteration: 2555 lambda_n: 0.9896612764683798 Loss: 0.01371417779261425\n",
      "Iteration: 2556 lambda_n: 1.0032622727735878 Loss: 0.013698644572693099\n",
      "Iteration: 2557 lambda_n: 1.003821999533319 Loss: 0.013682899486203635\n",
      "Iteration: 2558 lambda_n: 0.9020910486718987 Loss: 0.013667147225872495\n",
      "Iteration: 2559 lambda_n: 0.9578246738902293 Loss: 0.013652992785235933\n",
      "Iteration: 2560 lambda_n: 0.9862035402378715 Loss: 0.013637965193111258\n",
      "Iteration: 2561 lambda_n: 0.9353387523602336 Loss: 0.01362249381348993\n",
      "Iteration: 2562 lambda_n: 0.9892495632481173 Loss: 0.013607821796014156\n",
      "Iteration: 2563 lambda_n: 0.8952524995366177 Loss: 0.013592305509702772\n",
      "Iteration: 2564 lambda_n: 0.896709423367758 Loss: 0.013578264874655445\n",
      "Iteration: 2565 lambda_n: 0.8962398028144039 Loss: 0.013564202569395828\n",
      "Iteration: 2566 lambda_n: 0.9285618519996414 Loss: 0.01355014879628644\n",
      "Iteration: 2567 lambda_n: 0.9485449538672598 Loss: 0.013535589382872203\n",
      "Iteration: 2568 lambda_n: 0.9259309065909492 Loss: 0.013520717895214564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2569 lambda_n: 0.9115692025288274 Loss: 0.013506202188372137\n",
      "Iteration: 2570 lambda_n: 0.9309950039830669 Loss: 0.01349191279931621\n",
      "Iteration: 2571 lambda_n: 0.8943987478962997 Loss: 0.013477320063581077\n",
      "Iteration: 2572 lambda_n: 0.9339834395249302 Loss: 0.013463302079065893\n",
      "Iteration: 2573 lambda_n: 0.9016334481863534 Loss: 0.013448664800648316\n",
      "Iteration: 2574 lambda_n: 0.9834246043734446 Loss: 0.013434535623539618\n",
      "Iteration: 2575 lambda_n: 0.8856526966507775 Loss: 0.013419125888943074\n",
      "Iteration: 2576 lambda_n: 0.9503322770003536 Loss: 0.013405249315199963\n",
      "Iteration: 2577 lambda_n: 1.0216514326103616 Loss: 0.013390360407398851\n",
      "Iteration: 2578 lambda_n: 0.9150484829047646 Loss: 0.013374355368165221\n",
      "Iteration: 2579 lambda_n: 0.9685099615280882 Loss: 0.013360021523433943\n",
      "Iteration: 2580 lambda_n: 0.9118588780655282 Loss: 0.01334485132248646\n",
      "Iteration: 2581 lambda_n: 0.8974756652769936 Loss: 0.013330569550573662\n",
      "Iteration: 2582 lambda_n: 1.0010263392614824 Loss: 0.01331651403957435\n",
      "Iteration: 2583 lambda_n: 1.032984555696533 Loss: 0.013300837877356062\n",
      "Iteration: 2584 lambda_n: 0.9075298783059248 Loss: 0.01328466246668228\n",
      "Iteration: 2585 lambda_n: 0.9756026204492572 Loss: 0.013270452630624227\n",
      "Iteration: 2586 lambda_n: 0.971576533898467 Loss: 0.013255177950474878\n",
      "Iteration: 2587 lambda_n: 0.96584702924765 Loss: 0.013239967383383329\n",
      "Iteration: 2588 lambda_n: 1.0032477757346914 Loss: 0.013224847569183533\n",
      "Iteration: 2589 lambda_n: 1.0333914311319947 Loss: 0.01320914334242641\n",
      "Iteration: 2590 lambda_n: 1.0325016393532125 Loss: 0.013192968402726974\n",
      "Iteration: 2591 lambda_n: 0.8872846557346252 Loss: 0.013176808546131385\n",
      "Iteration: 2592 lambda_n: 0.941679768836683 Loss: 0.013162922483839863\n",
      "Iteration: 2593 lambda_n: 0.9099901963562195 Loss: 0.013148186017072409\n",
      "Iteration: 2594 lambda_n: 0.8825095815291844 Loss: 0.013133946359174052\n",
      "Iteration: 2595 lambda_n: 0.9075465088149822 Loss: 0.013120137403971017\n",
      "Iteration: 2596 lambda_n: 0.8855743009173482 Loss: 0.013105937572570505\n",
      "Iteration: 2597 lambda_n: 0.8831357678505862 Loss: 0.013092082374490315\n",
      "Iteration: 2598 lambda_n: 0.8994236927202423 Loss: 0.013078266129268764\n",
      "Iteration: 2599 lambda_n: 0.9876737807102202 Loss: 0.01306419586371131\n",
      "Iteration: 2600 lambda_n: 1.0358971928814436 Loss: 0.013048745921130842\n",
      "Iteration: 2601 lambda_n: 0.8856213291611702 Loss: 0.013032542624825245\n",
      "Iteration: 2602 lambda_n: 1.0291090413329975 Loss: 0.013018690790025754\n",
      "Iteration: 2603 lambda_n: 0.935554949289295 Loss: 0.01300259555100908\n",
      "Iteration: 2604 lambda_n: 0.9099413807202282 Loss: 0.012987964392009686\n",
      "Iteration: 2605 lambda_n: 0.9298996308120678 Loss: 0.01297373458653648\n",
      "Iteration: 2606 lambda_n: 0.9029738349691075 Loss: 0.012959193439749921\n",
      "Iteration: 2607 lambda_n: 0.9404402308078897 Loss: 0.012945074094753185\n",
      "Iteration: 2608 lambda_n: 0.9831927451858713 Loss: 0.012930369661072978\n",
      "Iteration: 2609 lambda_n: 1.029771937833138 Loss: 0.012914997574599246\n",
      "Iteration: 2610 lambda_n: 0.9638901840807482 Loss: 0.012898898107999542\n",
      "Iteration: 2611 lambda_n: 0.9029521456303806 Loss: 0.01288382948868968\n",
      "Iteration: 2612 lambda_n: 0.8843695409436328 Loss: 0.012869714258559236\n",
      "Iteration: 2613 lambda_n: 0.9544815575726134 Loss: 0.012855890185885881\n",
      "Iteration: 2614 lambda_n: 0.9532976930627078 Loss: 0.012840970852318875\n",
      "Iteration: 2615 lambda_n: 0.9526439304510749 Loss: 0.012826070768621065\n",
      "Iteration: 2616 lambda_n: 1.0307297301881782 Loss: 0.012811181638233115\n",
      "Iteration: 2617 lambda_n: 0.9172824244846118 Loss: 0.012795072869417643\n",
      "Iteration: 2618 lambda_n: 0.9487536708774941 Loss: 0.012780737859843919\n",
      "Iteration: 2619 lambda_n: 1.0052421646939962 Loss: 0.012765911706654967\n",
      "Iteration: 2620 lambda_n: 0.8899665921363684 Loss: 0.012750203545266767\n",
      "Iteration: 2621 lambda_n: 0.9158171712453373 Loss: 0.012736297390389903\n",
      "Iteration: 2622 lambda_n: 0.9858310952977911 Loss: 0.012721987921973998\n",
      "Iteration: 2623 lambda_n: 0.971335420930018 Loss: 0.01270658517240532\n",
      "Iteration: 2624 lambda_n: 0.908640505848408 Loss: 0.012691409610605615\n",
      "Iteration: 2625 lambda_n: 1.027539627282114 Loss: 0.012677214198821953\n",
      "Iteration: 2626 lambda_n: 1.0235244050583465 Loss: 0.012661161934519235\n",
      "Iteration: 2627 lambda_n: 0.9092106396640924 Loss: 0.012645173144202382\n",
      "Iteration: 2628 lambda_n: 0.9680064883366382 Loss: 0.012630970737703843\n",
      "Iteration: 2629 lambda_n: 1.0220809781003974 Loss: 0.012615850515926222\n",
      "Iteration: 2630 lambda_n: 0.9547611637469928 Loss: 0.01259988633150599\n",
      "Iteration: 2631 lambda_n: 1.0052332016323915 Loss: 0.012584974296231585\n",
      "Iteration: 2632 lambda_n: 0.8991976100116608 Loss: 0.01256927460065598\n",
      "Iteration: 2633 lambda_n: 0.9726413777197989 Loss: 0.01255523156263045\n",
      "Iteration: 2634 lambda_n: 1.008805663529323 Loss: 0.012540042102836243\n",
      "Iteration: 2635 lambda_n: 1.0050450819313377 Loss: 0.012524288509823815\n",
      "Iteration: 2636 lambda_n: 0.9332087236163095 Loss: 0.012508594289576267\n",
      "Iteration: 2637 lambda_n: 0.9674315688507652 Loss: 0.012494022416762052\n",
      "Iteration: 2638 lambda_n: 0.8892567997730407 Loss: 0.01247891672316789\n",
      "Iteration: 2639 lambda_n: 1.0294881077599918 Loss: 0.012465032197399807\n",
      "Iteration: 2640 lambda_n: 0.9980080174397311 Loss: 0.012448958709296989\n",
      "Iteration: 2641 lambda_n: 0.9186901751595822 Loss: 0.012433377340756141\n",
      "Iteration: 2642 lambda_n: 0.9355352594266297 Loss: 0.01241903486409223\n",
      "Iteration: 2643 lambda_n: 0.9404864177352409 Loss: 0.012404429908534381\n",
      "Iteration: 2644 lambda_n: 0.9139772119442511 Loss: 0.012389748169386573\n",
      "Iteration: 2645 lambda_n: 0.9540005594025824 Loss: 0.012375480753046885\n",
      "Iteration: 2646 lambda_n: 0.9050319166402709 Loss: 0.01236058905664352\n",
      "Iteration: 2647 lambda_n: 0.9709499240912485 Loss: 0.01234646223186475\n",
      "Iteration: 2648 lambda_n: 1.0149412833308413 Loss: 0.012331306966839505\n",
      "Iteration: 2649 lambda_n: 0.9580645832816792 Loss: 0.012315465594151299\n",
      "Iteration: 2650 lambda_n: 0.8835738864859768 Loss: 0.012300512488999221\n",
      "Iteration: 2651 lambda_n: 0.9205853473746596 Loss: 0.012286722458779419\n",
      "Iteration: 2652 lambda_n: 0.9772419802483528 Loss: 0.01227235521687438\n",
      "Iteration: 2653 lambda_n: 1.022146948473532 Loss: 0.012257104225986764\n",
      "Iteration: 2654 lambda_n: 0.8964954006184427 Loss: 0.012241152957718252\n",
      "Iteration: 2655 lambda_n: 0.9828649109146277 Loss: 0.012227163031555317\n",
      "Iteration: 2656 lambda_n: 0.9763675575970719 Loss: 0.012211825742767727\n",
      "Iteration: 2657 lambda_n: 0.9657388524068837 Loss: 0.012196590321918088\n",
      "Iteration: 2658 lambda_n: 0.9610866797210126 Loss: 0.01218152121821068\n",
      "Iteration: 2659 lambda_n: 0.9141750732155406 Loss: 0.012166525157767895\n",
      "Iteration: 2660 lambda_n: 0.9736803054028556 Loss: 0.012152261492991421\n",
      "Iteration: 2661 lambda_n: 0.9471913638128484 Loss: 0.012137069805256502\n",
      "Iteration: 2662 lambda_n: 0.9040503365273838 Loss: 0.012122291840744114\n",
      "Iteration: 2663 lambda_n: 0.9253970604965632 Loss: 0.012108187355307058\n",
      "Iteration: 2664 lambda_n: 0.9607951042177624 Loss: 0.012093750214734762\n",
      "Iteration: 2665 lambda_n: 1.0173131516664273 Loss: 0.012078761232585217\n",
      "Iteration: 2666 lambda_n: 0.9915285785907234 Loss: 0.012062890974128583\n",
      "Iteration: 2667 lambda_n: 0.97011524036312 Loss: 0.012047423407278372\n",
      "Iteration: 2668 lambda_n: 0.9048519353633431 Loss: 0.012032290304287456\n",
      "Iteration: 2669 lambda_n: 1.0042209163852325 Loss: 0.012018175642341448\n",
      "Iteration: 2670 lambda_n: 0.9894129798396333 Loss: 0.012002511326390899\n",
      "Iteration: 2671 lambda_n: 1.0029322048543343 Loss: 0.011987078412360378\n",
      "Iteration: 2672 lambda_n: 0.94381978357781 Loss: 0.011971435039807745\n",
      "Iteration: 2673 lambda_n: 0.9594972043940266 Loss: 0.011956714072395154\n",
      "Iteration: 2674 lambda_n: 1.0264191354797458 Loss: 0.011941748950395459\n",
      "Iteration: 2675 lambda_n: 1.0237885910550637 Loss: 0.011925740455347464\n",
      "Iteration: 2676 lambda_n: 0.9357159516691159 Loss: 0.011909773406464716\n",
      "Iteration: 2677 lambda_n: 0.9074995960105213 Loss: 0.011895180319185425\n",
      "Iteration: 2678 lambda_n: 0.9761882106757933 Loss: 0.01188102761414527\n",
      "Iteration: 2679 lambda_n: 1.001853472172316 Loss: 0.011865804031973305\n",
      "Iteration: 2680 lambda_n: 0.9595036055086156 Loss: 0.011850180573816475\n",
      "Iteration: 2681 lambda_n: 0.9393458709332355 Loss: 0.011835217904015736\n",
      "Iteration: 2682 lambda_n: 0.9164900231737869 Loss: 0.011820569911951876\n",
      "Iteration: 2683 lambda_n: 0.8887031117092292 Loss: 0.011806278645619728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2684 lambda_n: 0.894240941402987 Loss: 0.011792420969136205\n",
      "Iteration: 2685 lambda_n: 0.9191688950720461 Loss: 0.011778477225403093\n",
      "Iteration: 2686 lambda_n: 0.8863825967665399 Loss: 0.01176414507595167\n",
      "Iteration: 2687 lambda_n: 0.9692393303564909 Loss: 0.011750324432928378\n",
      "Iteration: 2688 lambda_n: 0.9894891950341574 Loss: 0.011735212170334186\n",
      "Iteration: 2689 lambda_n: 1.0196695916325271 Loss: 0.011719784503400311\n",
      "Iteration: 2690 lambda_n: 0.92892746069489 Loss: 0.011703886619564487\n",
      "Iteration: 2691 lambda_n: 0.9628842984728492 Loss: 0.01168940383267431\n",
      "Iteration: 2692 lambda_n: 0.9596963080292421 Loss: 0.011674391925128475\n",
      "Iteration: 2693 lambda_n: 0.9862303276978385 Loss: 0.011659430022430445\n",
      "Iteration: 2694 lambda_n: 1.0308164395791592 Loss: 0.011644054753828209\n",
      "Iteration: 2695 lambda_n: 0.9984132392538374 Loss: 0.011627984715439741\n",
      "Iteration: 2696 lambda_n: 0.9429904438186748 Loss: 0.011612420155562683\n",
      "Iteration: 2697 lambda_n: 0.945844979357515 Loss: 0.011597719891519658\n",
      "Iteration: 2698 lambda_n: 1.03615477316456 Loss: 0.011582975402847378\n",
      "Iteration: 2699 lambda_n: 1.0063538404885632 Loss: 0.011566823400821255\n",
      "Iteration: 2700 lambda_n: 0.8926692961443451 Loss: 0.011551136261623664\n",
      "Iteration: 2701 lambda_n: 0.9373815770773805 Loss: 0.011537221514738334\n",
      "Iteration: 2702 lambda_n: 0.950916021383617 Loss: 0.01152261004759221\n",
      "Iteration: 2703 lambda_n: 0.9888522932176864 Loss: 0.01150778787065044\n",
      "Iteration: 2704 lambda_n: 1.0345953114519355 Loss: 0.011492374641050861\n",
      "Iteration: 2705 lambda_n: 1.0287583225268278 Loss: 0.011476248705851138\n",
      "Iteration: 2706 lambda_n: 0.9594523915984223 Loss: 0.011460214048443945\n",
      "Iteration: 2707 lambda_n: 0.9185161216522703 Loss: 0.011445259895406564\n",
      "Iteration: 2708 lambda_n: 1.0137540262520706 Loss: 0.011430944062818653\n",
      "Iteration: 2709 lambda_n: 0.9783863222856863 Loss: 0.011415144088925603\n",
      "Iteration: 2710 lambda_n: 1.0249035813160134 Loss: 0.011399895596210503\n",
      "Iteration: 2711 lambda_n: 0.9250411427333413 Loss: 0.011383922376299805\n",
      "Iteration: 2712 lambda_n: 0.947801634374714 Loss: 0.011369505767448378\n",
      "Iteration: 2713 lambda_n: 1.025069075835843 Loss: 0.011354734665496682\n",
      "Iteration: 2714 lambda_n: 0.8931879291307884 Loss: 0.011338759629390722\n",
      "Iteration: 2715 lambda_n: 0.889070528821585 Loss: 0.011324840105901367\n",
      "Iteration: 2716 lambda_n: 0.8846445587057153 Loss: 0.011310984946038709\n",
      "Iteration: 2717 lambda_n: 0.9411836091796566 Loss: 0.011297198953609623\n",
      "Iteration: 2718 lambda_n: 0.9554366910449461 Loss: 0.011282532079200228\n",
      "Iteration: 2719 lambda_n: 0.895265180132814 Loss: 0.011267643309496705\n",
      "Iteration: 2720 lambda_n: 0.9826273042426327 Loss: 0.011253692408704224\n",
      "Iteration: 2721 lambda_n: 1.035295387708264 Loss: 0.011238380352675617\n",
      "Iteration: 2722 lambda_n: 0.976003786984568 Loss: 0.011222247818831153\n",
      "Iteration: 2723 lambda_n: 1.0360553500833134 Loss: 0.01120703943116873\n",
      "Iteration: 2724 lambda_n: 0.9228955628295175 Loss: 0.011190895531446263\n",
      "Iteration: 2725 lambda_n: 1.0049626902897844 Loss: 0.011176515111262714\n",
      "Iteration: 2726 lambda_n: 0.9040790515378575 Loss: 0.011160856092909562\n",
      "Iteration: 2727 lambda_n: 0.9895521271308148 Loss: 0.011146769242153114\n",
      "Iteration: 2728 lambda_n: 1.0050605346842412 Loss: 0.011131350799832872\n",
      "Iteration: 2729 lambda_n: 0.9480775416485266 Loss: 0.01111569093454036\n",
      "Iteration: 2730 lambda_n: 1.0322670457515877 Loss: 0.011100919125389694\n",
      "Iteration: 2731 lambda_n: 0.9168106952168675 Loss: 0.011084835781235404\n",
      "Iteration: 2732 lambda_n: 0.9636208965412206 Loss: 0.01107055151207653\n",
      "Iteration: 2733 lambda_n: 1.0031841371408998 Loss: 0.01105553810155071\n",
      "Iteration: 2734 lambda_n: 0.9619962576939903 Loss: 0.011039908528361907\n",
      "Iteration: 2735 lambda_n: 1.019656982148656 Loss: 0.011024920819657896\n",
      "Iteration: 2736 lambda_n: 0.9573458592347751 Loss: 0.011009034952083793\n",
      "Iteration: 2737 lambda_n: 0.9310726197609821 Loss: 0.010994120053554298\n",
      "Iteration: 2738 lambda_n: 0.8984090788706511 Loss: 0.01097961464644074\n",
      "Iteration: 2739 lambda_n: 0.9126888287740571 Loss: 0.010965618229198233\n",
      "Iteration: 2740 lambda_n: 0.8900348678482646 Loss: 0.010951399570096968\n",
      "Iteration: 2741 lambda_n: 0.9449980434520345 Loss: 0.01093753396673753\n",
      "Iteration: 2742 lambda_n: 0.9177938760600232 Loss: 0.010922812256792465\n",
      "Iteration: 2743 lambda_n: 0.9343622112131402 Loss: 0.010908514504049947\n",
      "Iteration: 2744 lambda_n: 0.9067022144496849 Loss: 0.01089395879644168\n",
      "Iteration: 2745 lambda_n: 0.9449088494077515 Loss: 0.010879834132654686\n",
      "Iteration: 2746 lambda_n: 0.9737445198516543 Loss: 0.010865114433893574\n",
      "Iteration: 2747 lambda_n: 1.025569394316531 Loss: 0.010849945695699666\n",
      "Iteration: 2748 lambda_n: 0.9262497514235382 Loss: 0.01083396981471832\n",
      "Iteration: 2749 lambda_n: 1.0350312011096976 Loss: 0.010819541210953455\n",
      "Iteration: 2750 lambda_n: 0.9949189900249754 Loss: 0.01080341826405056\n",
      "Iteration: 2751 lambda_n: 0.9782958818781259 Loss: 0.01078792033759285\n",
      "Iteration: 2752 lambda_n: 0.8897846809260753 Loss: 0.010772681513934847\n",
      "Iteration: 2753 lambda_n: 0.9721005344720429 Loss: 0.010758821563311049\n",
      "Iteration: 2754 lambda_n: 0.9882770181783979 Loss: 0.010743679538363254\n",
      "Iteration: 2755 lambda_n: 1.0167942246290165 Loss: 0.010728285690742181\n",
      "Iteration: 2756 lambda_n: 1.0109888417707305 Loss: 0.010712447803084375\n",
      "Iteration: 2757 lambda_n: 0.9525687665180205 Loss: 0.010696700500096091\n",
      "Iteration: 2758 lambda_n: 0.9930647587594109 Loss: 0.010681863302734377\n",
      "Iteration: 2759 lambda_n: 0.9952794579118668 Loss: 0.010666395482515488\n",
      "Iteration: 2760 lambda_n: 0.9348873718222972 Loss: 0.010650893313290671\n",
      "Iteration: 2761 lambda_n: 0.9371470936084596 Loss: 0.010636331929310653\n",
      "Iteration: 2762 lambda_n: 0.9886885327130818 Loss: 0.01062173547602315\n",
      "Iteration: 2763 lambda_n: 0.9506352905485916 Loss: 0.010606336376494638\n",
      "Iteration: 2764 lambda_n: 1.0065361716299401 Loss: 0.010591530100144499\n",
      "Iteration: 2765 lambda_n: 1.010347054974287 Loss: 0.010575853294045024\n",
      "Iteration: 2766 lambda_n: 0.951519977997118 Loss: 0.010560117274410613\n",
      "Iteration: 2767 lambda_n: 0.9250124078898925 Loss: 0.010545297610236763\n",
      "Iteration: 2768 lambda_n: 0.9118270854246157 Loss: 0.010530890913330504\n",
      "Iteration: 2769 lambda_n: 0.9171740943189162 Loss: 0.010516689685364834\n",
      "Iteration: 2770 lambda_n: 0.952291544035086 Loss: 0.010502405291200783\n",
      "Iteration: 2771 lambda_n: 0.9203429358495105 Loss: 0.010487574079768375\n",
      "Iteration: 2772 lambda_n: 0.9027605113641941 Loss: 0.010473240556851329\n",
      "Iteration: 2773 lambda_n: 0.9478442281103733 Loss: 0.010459180970859077\n",
      "Iteration: 2774 lambda_n: 1.0115174574802759 Loss: 0.010444419359687611\n",
      "Iteration: 2775 lambda_n: 1.0030772326690751 Loss: 0.010428666229421608\n",
      "Iteration: 2776 lambda_n: 1.0226764905508523 Loss: 0.01041304467074549\n",
      "Iteration: 2777 lambda_n: 0.9993052104677471 Loss: 0.010397118005804038\n",
      "Iteration: 2778 lambda_n: 0.9425913561256043 Loss: 0.010381555437192266\n",
      "Iteration: 2779 lambda_n: 0.972786692677769 Loss: 0.010366876207810507\n",
      "Iteration: 2780 lambda_n: 0.9436467048298152 Loss: 0.010351726846412114\n",
      "Iteration: 2781 lambda_n: 0.899998003664311 Loss: 0.010337031393616004\n",
      "Iteration: 2782 lambda_n: 1.0285591459069159 Loss: 0.010323015781798868\n",
      "Iteration: 2783 lambda_n: 0.9439126118104989 Loss: 0.010306998201099026\n",
      "Iteration: 2784 lambda_n: 0.9468467764980152 Loss: 0.010292298916064235\n",
      "Iteration: 2785 lambda_n: 0.977319151029733 Loss: 0.010277554037524506\n",
      "Iteration: 2786 lambda_n: 0.9917533211470045 Loss: 0.010262334726287374\n",
      "Iteration: 2787 lambda_n: 0.9165745839910588 Loss: 0.01024689074421984\n",
      "Iteration: 2788 lambda_n: 0.9064060492892555 Loss: 0.010232617573354738\n",
      "Iteration: 2789 lambda_n: 1.0290810292446437 Loss: 0.010218502838146135\n",
      "Iteration: 2790 lambda_n: 0.9402482555497834 Loss: 0.01020247788184903\n",
      "Iteration: 2791 lambda_n: 0.993684970132578 Loss: 0.010187836339156516\n",
      "Iteration: 2792 lambda_n: 0.9214006079199696 Loss: 0.01017236277605245\n",
      "Iteration: 2793 lambda_n: 1.029606250067965 Loss: 0.010158014910660196\n",
      "Iteration: 2794 lambda_n: 1.0338150855507338 Loss: 0.010141982183939838\n",
      "Iteration: 2795 lambda_n: 0.9628816540268338 Loss: 0.010125884024021066\n",
      "Iteration: 2796 lambda_n: 0.9854001909027946 Loss: 0.010110890508896308\n",
      "Iteration: 2797 lambda_n: 1.0196448148606443 Loss: 0.010095546438097347\n",
      "Iteration: 2798 lambda_n: 0.985340319835622 Loss: 0.010079669226313554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2799 lambda_n: 0.9914585765768275 Loss: 0.010064326322207126\n",
      "Iteration: 2800 lambda_n: 1.0269563090923977 Loss: 0.01004888820682066\n",
      "Iteration: 2801 lambda_n: 0.9649617451485932 Loss: 0.010032897435956242\n",
      "Iteration: 2802 lambda_n: 0.9476001367182437 Loss: 0.010017872070605111\n",
      "Iteration: 2803 lambda_n: 0.8839807598020674 Loss: 0.010003117122611541\n",
      "Iteration: 2804 lambda_n: 0.9397710909228226 Loss: 0.009989352856937141\n",
      "Iteration: 2805 lambda_n: 1.020504010361387 Loss: 0.009974719965679767\n",
      "Iteration: 2806 lambda_n: 0.9375694050741892 Loss: 0.009958830090376987\n",
      "Iteration: 2807 lambda_n: 0.9543707821835083 Loss: 0.009944231640568095\n",
      "Iteration: 2808 lambda_n: 0.9707053076404656 Loss: 0.009929371660962347\n",
      "Iteration: 2809 lambda_n: 0.9928727797057025 Loss: 0.00991425742380879\n",
      "Iteration: 2810 lambda_n: 1.016477267018171 Loss: 0.009898798111520973\n",
      "Iteration: 2811 lambda_n: 1.028148352083292 Loss: 0.00988297135394356\n",
      "Iteration: 2812 lambda_n: 1.0026823733060874 Loss: 0.009866962960393426\n",
      "Iteration: 2813 lambda_n: 0.9798079552431045 Loss: 0.009851351158140178\n",
      "Iteration: 2814 lambda_n: 0.9326401691322752 Loss: 0.009836095589481045\n",
      "Iteration: 2815 lambda_n: 1.0167691379456425 Loss: 0.009821574492947198\n",
      "Iteration: 2816 lambda_n: 0.9666123958663648 Loss: 0.009805743592000683\n",
      "Iteration: 2817 lambda_n: 0.8889354751431672 Loss: 0.009790693697245126\n",
      "Iteration: 2818 lambda_n: 1.0319037651125609 Loss: 0.009776853276241703\n",
      "Iteration: 2819 lambda_n: 0.9133454873256457 Loss: 0.009760786956732848\n",
      "Iteration: 2820 lambda_n: 1.028766832171132 Loss: 0.009746566610808107\n",
      "Iteration: 2821 lambda_n: 0.9394054112740724 Loss: 0.009730549279428109\n",
      "Iteration: 2822 lambda_n: 0.9472114860646722 Loss: 0.00971592332586308\n",
      "Iteration: 2823 lambda_n: 1.0253466562920972 Loss: 0.009701175900216215\n",
      "Iteration: 2824 lambda_n: 0.9973689323506366 Loss: 0.009685212032914184\n",
      "Iteration: 2825 lambda_n: 0.9304341806965089 Loss: 0.009669683828933939\n",
      "Iteration: 2826 lambda_n: 0.9813725403807372 Loss: 0.009655197807323171\n",
      "Iteration: 2827 lambda_n: 0.9242150834287156 Loss: 0.009639918783719819\n",
      "Iteration: 2828 lambda_n: 0.9573892558765799 Loss: 0.009625529707728396\n",
      "Iteration: 2829 lambda_n: 0.9490918949597457 Loss: 0.00961062420313287\n",
      "Iteration: 2830 lambda_n: 0.9788019158933082 Loss: 0.009595847939327027\n",
      "Iteration: 2831 lambda_n: 1.020490328206649 Loss: 0.009580609185473519\n",
      "Iteration: 2832 lambda_n: 0.98138458544804 Loss: 0.009564721458295152\n",
      "Iteration: 2833 lambda_n: 0.9199436210429529 Loss: 0.009549442621265208\n",
      "Iteration: 2834 lambda_n: 0.8969646517267583 Loss: 0.009535120394175808\n",
      "Iteration: 2835 lambda_n: 0.9925242861079857 Loss: 0.009521155968662815\n",
      "Iteration: 2836 lambda_n: 1.0110720893263003 Loss: 0.00950570387460445\n",
      "Iteration: 2837 lambda_n: 0.9480202307089292 Loss: 0.009489963080658225\n",
      "Iteration: 2838 lambda_n: 0.9936877027234707 Loss: 0.009475203962165446\n",
      "Iteration: 2839 lambda_n: 0.9605890969513079 Loss: 0.009459733932265808\n",
      "Iteration: 2840 lambda_n: 0.9534839838722619 Loss: 0.009444779247627304\n",
      "Iteration: 2841 lambda_n: 0.8896318093894674 Loss: 0.009429935230428574\n",
      "Iteration: 2842 lambda_n: 1.0224882332689975 Loss: 0.00941608532460192\n",
      "Iteration: 2843 lambda_n: 0.9911524711031702 Loss: 0.009400167144312035\n",
      "Iteration: 2844 lambda_n: 0.9242905733487085 Loss: 0.009384736858785943\n",
      "Iteration: 2845 lambda_n: 0.9224816763399135 Loss: 0.009370347531821331\n",
      "Iteration: 2846 lambda_n: 0.897409518555695 Loss: 0.009355986412627835\n",
      "Iteration: 2847 lambda_n: 1.002417028295038 Loss: 0.009342015659775299\n",
      "Iteration: 2848 lambda_n: 0.888592996347731 Loss: 0.00932641021245625\n",
      "Iteration: 2849 lambda_n: 1.037331380996449 Loss: 0.009312576804474181\n",
      "Iteration: 2850 lambda_n: 0.9709766391918878 Loss: 0.009296427920846865\n",
      "Iteration: 2851 lambda_n: 0.9955434350783113 Loss: 0.009281312081649952\n",
      "Iteration: 2852 lambda_n: 0.9916697316832976 Loss: 0.009265813844632941\n",
      "Iteration: 2853 lambda_n: 1.0036791650683674 Loss: 0.009250375962230563\n",
      "Iteration: 2854 lambda_n: 1.0295236713474571 Loss: 0.009234751172341684\n",
      "Iteration: 2855 lambda_n: 0.9323736023501507 Loss: 0.009218724099208169\n",
      "Iteration: 2856 lambda_n: 1.013183968762064 Loss: 0.009204209453447921\n",
      "Iteration: 2857 lambda_n: 0.970049142833798 Loss: 0.00918843684531228\n",
      "Iteration: 2858 lambda_n: 0.9370867286182633 Loss: 0.009173335780141808\n",
      "Iteration: 2859 lambda_n: 1.0164834453601657 Loss: 0.009158747894604878\n",
      "Iteration: 2860 lambda_n: 0.9950765540097604 Loss: 0.009142924063551947\n",
      "Iteration: 2861 lambda_n: 0.9348722632142294 Loss: 0.009127433525525057\n",
      "Iteration: 2862 lambda_n: 0.95128000478585 Loss: 0.0091128802413733\n",
      "Iteration: 2863 lambda_n: 0.9425635108249091 Loss: 0.009098071576124819\n",
      "Iteration: 2864 lambda_n: 1.017767084883407 Loss: 0.009083398641682765\n",
      "Iteration: 2865 lambda_n: 0.9990964835074996 Loss: 0.009067555052141475\n",
      "Iteration: 2866 lambda_n: 0.9757010225009469 Loss: 0.009052002152766676\n",
      "Iteration: 2867 lambda_n: 0.9337546326215677 Loss: 0.009036813492093393\n",
      "Iteration: 2868 lambda_n: 0.9355778460469354 Loss: 0.009022277846704035\n",
      "Iteration: 2869 lambda_n: 0.9786417341619493 Loss: 0.009007713856748851\n",
      "Iteration: 2870 lambda_n: 0.9442002829753481 Loss: 0.008992479536777348\n",
      "Iteration: 2871 lambda_n: 0.9149627717538363 Loss: 0.008977781398436722\n",
      "Iteration: 2872 lambda_n: 1.006254437664202 Loss: 0.008963538428950939\n",
      "Iteration: 2873 lambda_n: 0.979831419721865 Loss: 0.008947874385302604\n",
      "Iteration: 2874 lambda_n: 0.9500965665654361 Loss: 0.008932621700143348\n",
      "Iteration: 2875 lambda_n: 0.8980363778584823 Loss: 0.008917831923908395\n",
      "Iteration: 2876 lambda_n: 0.9457762869597494 Loss: 0.008903852581576917\n",
      "Iteration: 2877 lambda_n: 0.8836953585369418 Loss: 0.008889130126034199\n",
      "Iteration: 2878 lambda_n: 0.918257799815318 Loss: 0.008875374087394422\n",
      "Iteration: 2879 lambda_n: 0.8857184364640609 Loss: 0.008861080063794243\n",
      "Iteration: 2880 lambda_n: 0.9412646276993577 Loss: 0.008847292593663871\n",
      "Iteration: 2881 lambda_n: 0.9487893377288985 Loss: 0.008832640499207949\n",
      "Iteration: 2882 lambda_n: 0.9410572872862113 Loss: 0.008817871305325237\n",
      "Iteration: 2883 lambda_n: 1.0196506516681034 Loss: 0.008803222504092413\n",
      "Iteration: 2884 lambda_n: 0.917767555245165 Loss: 0.008787350328199753\n",
      "Iteration: 2885 lambda_n: 0.9438011458505995 Loss: 0.008773064127479276\n",
      "Iteration: 2886 lambda_n: 0.9922991352190936 Loss: 0.008758372712185315\n",
      "Iteration: 2887 lambda_n: 0.8865933085340068 Loss: 0.008742926399621723\n",
      "Iteration: 2888 lambda_n: 1.009642740704797 Loss: 0.008729125554106513\n",
      "Iteration: 2889 lambda_n: 0.9798032223294291 Loss: 0.008713409333567042\n",
      "Iteration: 2890 lambda_n: 0.9991279091423839 Loss: 0.008698157632300293\n",
      "Iteration: 2891 lambda_n: 1.0319370928252831 Loss: 0.00868260515438464\n",
      "Iteration: 2892 lambda_n: 0.9519844243055368 Loss: 0.008666542001480232\n",
      "Iteration: 2893 lambda_n: 0.9595739610457485 Loss: 0.00865172342572768\n",
      "Iteration: 2894 lambda_n: 0.8960598937194356 Loss: 0.008636786741223933\n",
      "Iteration: 2895 lambda_n: 0.9980579574691483 Loss: 0.00862283874152255\n",
      "Iteration: 2896 lambda_n: 1.0347895630893067 Loss: 0.008607303076559157\n",
      "Iteration: 2897 lambda_n: 0.9213500885610384 Loss: 0.008591195684103192\n",
      "Iteration: 2898 lambda_n: 0.9160423427484407 Loss: 0.008576854104701551\n",
      "Iteration: 2899 lambda_n: 0.9783331705626264 Loss: 0.00856259517094974\n",
      "Iteration: 2900 lambda_n: 0.9982662770001973 Loss: 0.00854736665813328\n",
      "Iteration: 2901 lambda_n: 1.0079452460987854 Loss: 0.008531827900836914\n",
      "Iteration: 2902 lambda_n: 0.9727636118541311 Loss: 0.008516138513498566\n",
      "Iteration: 2903 lambda_n: 0.9336975922338011 Loss: 0.008500996782571914\n",
      "Iteration: 2904 lambda_n: 0.9808405514089958 Loss: 0.008486463167674454\n",
      "Iteration: 2905 lambda_n: 0.9345461990654624 Loss: 0.008471195768519488\n",
      "Iteration: 2906 lambda_n: 1.019061641789723 Loss: 0.008456648996446105\n",
      "Iteration: 2907 lambda_n: 0.982614305581913 Loss: 0.008440786718196407\n",
      "Iteration: 2908 lambda_n: 0.9925659487607503 Loss: 0.00842549179191522\n",
      "Iteration: 2909 lambda_n: 0.9967242248769558 Loss: 0.008410041990240546\n",
      "Iteration: 2910 lambda_n: 1.022147110236608 Loss: 0.008394527490299456\n",
      "Iteration: 2911 lambda_n: 0.9459784775126816 Loss: 0.008378617298731943\n",
      "Iteration: 2912 lambda_n: 0.9355456886483234 Loss: 0.008363892733245072\n",
      "Iteration: 2913 lambda_n: 0.9457299389914392 Loss: 0.008349330582425518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2914 lambda_n: 0.9373696750020901 Loss: 0.008334609933168334\n",
      "Iteration: 2915 lambda_n: 0.8833142219948328 Loss: 0.008320019437998292\n",
      "Iteration: 2916 lambda_n: 0.9076726646137931 Loss: 0.008306270356890562\n",
      "Iteration: 2917 lambda_n: 1.0032099000837713 Loss: 0.008292142149268905\n",
      "Iteration: 2918 lambda_n: 0.9424640328243563 Loss: 0.008276526898356795\n",
      "Iteration: 2919 lambda_n: 1.0347271494800239 Loss: 0.008261857198289264\n",
      "Iteration: 2920 lambda_n: 0.899518003350704 Loss: 0.008245751423300335\n",
      "Iteration: 2921 lambda_n: 0.8870605427790925 Loss: 0.008231750233921066\n",
      "Iteration: 2922 lambda_n: 0.9593981989846259 Loss: 0.008217942967236441\n",
      "Iteration: 2923 lambda_n: 0.8880995454643602 Loss: 0.008203009772059976\n",
      "Iteration: 2924 lambda_n: 0.9987162496690483 Loss: 0.00818918637302051\n",
      "Iteration: 2925 lambda_n: 1.026832394799584 Loss: 0.008173641230336374\n",
      "Iteration: 2926 lambda_n: 0.8932438932958904 Loss: 0.008157658480823667\n",
      "Iteration: 2927 lambda_n: 1.0282047468911877 Loss: 0.00814375507118006\n",
      "Iteration: 2928 lambda_n: 0.8836003165629663 Loss: 0.008127751007204637\n",
      "Iteration: 2929 lambda_n: 0.9078197581330002 Loss: 0.008113997740004625\n",
      "Iteration: 2930 lambda_n: 1.0325433204651238 Loss: 0.008099867514843121\n",
      "Iteration: 2931 lambda_n: 1.021634900443767 Loss: 0.008083795987515833\n",
      "Iteration: 2932 lambda_n: 1.0120603551457612 Loss: 0.008067894273390521\n",
      "Iteration: 2933 lambda_n: 1.0183743046338416 Loss: 0.008052141609800936\n",
      "Iteration: 2934 lambda_n: 0.8937855747498348 Loss: 0.00803629069272106\n",
      "Iteration: 2935 lambda_n: 1.0109062381356366 Loss: 0.008022379009266053\n",
      "Iteration: 2936 lambda_n: 0.9853469326615386 Loss: 0.008006644374166776\n",
      "Iteration: 2937 lambda_n: 0.9910520926687779 Loss: 0.007991307587978202\n",
      "Iteration: 2938 lambda_n: 1.0338465346115204 Loss: 0.007975882022589762\n",
      "Iteration: 2939 lambda_n: 0.9302697159008347 Loss: 0.007959790390356393\n",
      "Iteration: 2940 lambda_n: 1.019854815780826 Loss: 0.007945310932290853\n",
      "Iteration: 2941 lambda_n: 0.8897022910025586 Loss: 0.007929437120292685\n",
      "Iteration: 2942 lambda_n: 0.8853053871884923 Loss: 0.007915589121632498\n",
      "Iteration: 2943 lambda_n: 0.9605468243316571 Loss: 0.007901809575735257\n",
      "Iteration: 2944 lambda_n: 1.017587092201634 Loss: 0.00788685893395109\n",
      "Iteration: 2945 lambda_n: 0.9702648057946367 Loss: 0.007871020496115677\n",
      "Iteration: 2946 lambda_n: 0.9201378320208632 Loss: 0.007855918635003107\n",
      "Iteration: 2947 lambda_n: 0.9675930229169613 Loss: 0.007841597001683038\n",
      "Iteration: 2948 lambda_n: 0.9867314631625984 Loss: 0.007826536761896954\n",
      "Iteration: 2949 lambda_n: 0.9946141406114856 Loss: 0.007811178657667805\n",
      "Iteration: 2950 lambda_n: 0.9328536351698573 Loss: 0.007795697881408074\n",
      "Iteration: 2951 lambda_n: 0.9002507539205564 Loss: 0.007781178400583639\n",
      "Iteration: 2952 lambda_n: 0.9397964963739345 Loss: 0.00776716638592321\n",
      "Iteration: 2953 lambda_n: 0.9747793515867257 Loss: 0.0077525388747838484\n",
      "Iteration: 2954 lambda_n: 0.9780391280033465 Loss: 0.007737366888310507\n",
      "Iteration: 2955 lambda_n: 1.0366324808540557 Loss: 0.007722144182550017\n",
      "Iteration: 2956 lambda_n: 0.9891148390590876 Loss: 0.007706009518314211\n",
      "Iteration: 2957 lambda_n: 0.8949740476869478 Loss: 0.007690614460855666\n",
      "Iteration: 2958 lambda_n: 0.8936204061346817 Loss: 0.007676684671565304\n",
      "Iteration: 2959 lambda_n: 0.995562181207864 Loss: 0.007662775965273291\n",
      "Iteration: 2960 lambda_n: 0.9578489320655769 Loss: 0.0076472806077804295\n",
      "Iteration: 2961 lambda_n: 0.9617079154140646 Loss: 0.007632372252229645\n",
      "Iteration: 2962 lambda_n: 1.0365273050860035 Loss: 0.007617403849993744\n",
      "Iteration: 2963 lambda_n: 0.9439716163127905 Loss: 0.007601270946681567\n",
      "Iteration: 2964 lambda_n: 0.89376770605646 Loss: 0.007586578631806644\n",
      "Iteration: 2965 lambda_n: 0.9123308881179063 Loss: 0.007572667723029438\n",
      "Iteration: 2966 lambda_n: 0.9274563248603616 Loss: 0.0075584679043679045\n",
      "Iteration: 2967 lambda_n: 1.0350947518213776 Loss: 0.007544032682725789\n",
      "Iteration: 2968 lambda_n: 1.0372348472821513 Loss: 0.007527922159132108\n",
      "Iteration: 2969 lambda_n: 0.966222039846902 Loss: 0.007511778344325323\n",
      "Iteration: 2970 lambda_n: 0.8842363184112498 Loss: 0.007496739809207828\n",
      "Iteration: 2971 lambda_n: 0.9155781993583255 Loss: 0.007482977335349267\n",
      "Iteration: 2972 lambda_n: 0.9253917370396177 Loss: 0.007468727061911673\n",
      "Iteration: 2973 lambda_n: 1.0236647349921828 Loss: 0.007454324062004127\n",
      "Iteration: 2974 lambda_n: 0.9156187532061036 Loss: 0.007438391535082253\n",
      "Iteration: 2975 lambda_n: 0.948399306431134 Loss: 0.007424140672606921\n",
      "Iteration: 2976 lambda_n: 0.979247420068519 Loss: 0.007409379621286327\n",
      "Iteration: 2977 lambda_n: 0.9127715306321307 Loss: 0.007394138459290006\n",
      "Iteration: 2978 lambda_n: 1.0281874217150457 Loss: 0.007379931952418491\n",
      "Iteration: 2979 lambda_n: 1.0133120390834611 Loss: 0.007363929110990388\n",
      "Iteration: 2980 lambda_n: 1.0183527941688668 Loss: 0.007348157808017294\n",
      "Iteration: 2981 lambda_n: 1.0148216078100614 Loss: 0.007332308066039273\n",
      "Iteration: 2982 lambda_n: 0.9452754511284507 Loss: 0.00731651329956534\n",
      "Iteration: 2983 lambda_n: 0.9190614033474713 Loss: 0.007301800969547896\n",
      "Iteration: 2984 lambda_n: 0.8918721527256536 Loss: 0.007287496649752544\n",
      "Iteration: 2985 lambda_n: 1.0073162459262275 Loss: 0.007273615517103023\n",
      "Iteration: 2986 lambda_n: 0.987481858122081 Loss: 0.007257937621830336\n",
      "Iteration: 2987 lambda_n: 1.00543523955456 Loss: 0.007242568444171082\n",
      "Iteration: 2988 lambda_n: 0.9991598481615427 Loss: 0.007226919854592817\n",
      "Iteration: 2989 lambda_n: 0.9633363183998945 Loss: 0.007211368949894492\n",
      "Iteration: 2990 lambda_n: 0.8930468311136541 Loss: 0.007196375615867826\n",
      "Iteration: 2991 lambda_n: 0.998054806719489 Loss: 0.007182476277357429\n",
      "Iteration: 2992 lambda_n: 0.9884923739502066 Loss: 0.007166942612841258\n",
      "Iteration: 2993 lambda_n: 0.8856522049999137 Loss: 0.007151557791587184\n",
      "Iteration: 2994 lambda_n: 1.01170312720509 Loss: 0.0071377735792656416\n",
      "Iteration: 2995 lambda_n: 1.0067441894822962 Loss: 0.0071220275346644355\n",
      "Iteration: 2996 lambda_n: 0.9529267456540017 Loss: 0.0071063586848588885\n",
      "Iteration: 2997 lambda_n: 0.9276763625369826 Loss: 0.00709152745686234\n",
      "Iteration: 2998 lambda_n: 0.9688681983662547 Loss: 0.007077089234856775\n",
      "Iteration: 2999 lambda_n: 0.9858154261471328 Loss: 0.0070620099216617905\n",
      "Iteration: 3000 lambda_n: 1.0298550076407944 Loss: 0.007046666857766361\n",
      "Iteration: 3001 lambda_n: 1.0291189082344985 Loss: 0.0070306383834629425\n",
      "Iteration: 3002 lambda_n: 0.9410754930469906 Loss: 0.007014621380253042\n",
      "Iteration: 3003 lambda_n: 0.9859017961267488 Loss: 0.006999974680364413\n",
      "Iteration: 3004 lambda_n: 0.9998801831126256 Loss: 0.006984630326155555\n",
      "Iteration: 3005 lambda_n: 0.989482552375575 Loss: 0.006969068428896134\n",
      "Iteration: 3006 lambda_n: 0.9936678633403274 Loss: 0.006953668371234735\n",
      "Iteration: 3007 lambda_n: 0.9723903341738543 Loss: 0.006938203187699991\n",
      "Iteration: 3008 lambda_n: 0.9932766333046924 Loss: 0.0069230691748942245\n",
      "Iteration: 3009 lambda_n: 1.0170531143705035 Loss: 0.006907610106507445\n",
      "Iteration: 3010 lambda_n: 0.9070897739176118 Loss: 0.0068917810014182375\n",
      "Iteration: 3011 lambda_n: 1.0029064387175814 Loss: 0.0068776633442154825\n",
      "Iteration: 3012 lambda_n: 0.9385419782792723 Loss: 0.0068620544391958475\n",
      "Iteration: 3013 lambda_n: 0.9498811207672427 Loss: 0.006847447293622331\n",
      "Iteration: 3014 lambda_n: 0.9930370601842317 Loss: 0.006832663681265966\n",
      "Iteration: 3015 lambda_n: 0.974000657778643 Loss: 0.006817208417688171\n",
      "Iteration: 3016 lambda_n: 0.9571195072073989 Loss: 0.00680204944224537\n",
      "Iteration: 3017 lambda_n: 0.9070678450570109 Loss: 0.006787153210686226\n",
      "Iteration: 3018 lambda_n: 0.9943305856686393 Loss: 0.0067730359746297625\n",
      "Iteration: 3019 lambda_n: 1.012159370620215 Loss: 0.0067575606286320745\n",
      "Iteration: 3020 lambda_n: 0.9139315412019402 Loss: 0.006741807815929585\n",
      "Iteration: 3021 lambda_n: 0.900340631019489 Loss: 0.006727583790490851\n",
      "Iteration: 3022 lambda_n: 0.8830149554114288 Loss: 0.0067135712984829655\n",
      "Iteration: 3023 lambda_n: 0.9354794245012328 Loss: 0.006699828465545951\n",
      "Iteration: 3024 lambda_n: 1.0254584538405611 Loss: 0.006685269110814338\n",
      "Iteration: 3025 lambda_n: 1.0322937060874937 Loss: 0.006669309378034936\n",
      "Iteration: 3026 lambda_n: 0.982872268705176 Loss: 0.006653243278257745\n",
      "Iteration: 3027 lambda_n: 1.0155360720427227 Loss: 0.0066379463615987505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3028 lambda_n: 1.007846647509738 Loss: 0.006622141095152575\n",
      "Iteration: 3029 lambda_n: 0.9007158740795719 Loss: 0.006606455515787773\n",
      "Iteration: 3030 lambda_n: 0.9941195548670314 Loss: 0.0065924372728963015\n",
      "Iteration: 3031 lambda_n: 0.8953811852143798 Loss: 0.006576965358501956\n",
      "Iteration: 3032 lambda_n: 1.0201253332493598 Loss: 0.0065630301631448605\n",
      "Iteration: 3033 lambda_n: 0.9581860974875347 Loss: 0.00654715353392203\n",
      "Iteration: 3034 lambda_n: 1.0174608453667289 Loss: 0.006532240902528107\n",
      "Iteration: 3035 lambda_n: 0.9443781153757019 Loss: 0.006516405767130863\n",
      "Iteration: 3036 lambda_n: 0.9762056406844642 Loss: 0.006501708058244395\n",
      "Iteration: 3037 lambda_n: 0.9873691415418224 Loss: 0.0064865150173897684\n",
      "Iteration: 3038 lambda_n: 1.0013974665686072 Loss: 0.006471148247064811\n",
      "Iteration: 3039 lambda_n: 0.9876067110826819 Loss: 0.006455563161464716\n",
      "Iteration: 3040 lambda_n: 0.9366776726737364 Loss: 0.006440192718360996\n",
      "Iteration: 3041 lambda_n: 0.9103243747177121 Loss: 0.00642561491176708\n",
      "Iteration: 3042 lambda_n: 1.0052296340707614 Loss: 0.006411447260333635\n",
      "Iteration: 3043 lambda_n: 0.9881169730117512 Loss: 0.0063958025816837315\n",
      "Iteration: 3044 lambda_n: 1.027039681070812 Loss: 0.006380424244666147\n",
      "Iteration: 3045 lambda_n: 0.9952774994915069 Loss: 0.006364440155655242\n",
      "Iteration: 3046 lambda_n: 0.9344628086807086 Loss: 0.006348950402496358\n",
      "Iteration: 3047 lambda_n: 0.8828970168733101 Loss: 0.006334407135012093\n",
      "Iteration: 3048 lambda_n: 0.9551054723217494 Loss: 0.0063206664083711415\n",
      "Iteration: 3049 lambda_n: 0.9163064882945597 Loss: 0.006305801895981541\n",
      "Iteration: 3050 lambda_n: 1.0317605346012084 Loss: 0.006291541231351926\n",
      "Iteration: 3051 lambda_n: 1.0301465189349635 Loss: 0.006275483744204479\n",
      "Iteration: 3052 lambda_n: 0.9755708818436437 Loss: 0.006259451389609415\n",
      "Iteration: 3053 lambda_n: 0.9781447702256605 Loss: 0.00624426841780139\n",
      "Iteration: 3054 lambda_n: 0.890924580107528 Loss: 0.006229045400130672\n",
      "Iteration: 3055 lambda_n: 0.9634153569004708 Loss: 0.006215179814324055\n",
      "Iteration: 3056 lambda_n: 1.0219614537611472 Loss: 0.0062001860553556505\n",
      "Iteration: 3057 lambda_n: 0.8880108875987294 Loss: 0.006184281148498481\n",
      "Iteration: 3058 lambda_n: 0.9718158268369987 Loss: 0.006170460941050079\n",
      "Iteration: 3059 lambda_n: 1.0102988021590513 Loss: 0.006155336479713714\n",
      "Iteration: 3060 lambda_n: 0.8973347273552574 Loss: 0.0061396131167915854\n",
      "Iteration: 3061 lambda_n: 1.0054886582856954 Loss: 0.006125647834033216\n",
      "Iteration: 3062 lambda_n: 0.8827754399146446 Loss: 0.006109999356061424\n",
      "Iteration: 3063 lambda_n: 0.9338481904065007 Loss: 0.006096260681766732\n",
      "Iteration: 3064 lambda_n: 1.0119028080521804 Loss: 0.006081727170643261\n",
      "Iteration: 3065 lambda_n: 0.887238242358881 Loss: 0.006065978905267299\n",
      "Iteration: 3066 lambda_n: 0.8920045486120542 Loss: 0.006052170808189893\n",
      "Iteration: 3067 lambda_n: 0.9969362890243527 Loss: 0.006038288543192982\n",
      "Iteration: 3068 lambda_n: 1.0095241979513416 Loss: 0.006022773237633279\n",
      "Iteration: 3069 lambda_n: 0.9755613665814866 Loss: 0.0060070620396267295\n",
      "Iteration: 3070 lambda_n: 0.9370408978894553 Loss: 0.0059918794167915625\n",
      "Iteration: 3071 lambda_n: 0.9461986404362372 Loss: 0.005977296298121031\n",
      "Iteration: 3072 lambda_n: 0.8892550874344233 Loss: 0.005962570669509277\n",
      "Iteration: 3073 lambda_n: 0.9533010909688963 Loss: 0.00594873126043149\n",
      "Iteration: 3074 lambda_n: 0.9103385134594864 Loss: 0.0059338951196288925\n",
      "Iteration: 3075 lambda_n: 0.9773591058218752 Loss: 0.005919727612768492\n",
      "Iteration: 3076 lambda_n: 0.919841123562555 Loss: 0.005904517083141508\n",
      "Iteration: 3077 lambda_n: 0.945137241904467 Loss: 0.005890201710932041\n",
      "Iteration: 3078 lambda_n: 1.0076290448442817 Loss: 0.005875492669976915\n",
      "Iteration: 3079 lambda_n: 0.9301557588859516 Loss: 0.005859811090597061\n",
      "Iteration: 3080 lambda_n: 1.0209781566035385 Loss: 0.0058453352283790476\n",
      "Iteration: 3081 lambda_n: 1.0131544124371628 Loss: 0.005829445925136235\n",
      "Iteration: 3082 lambda_n: 1.0281773627080468 Loss: 0.005813678395221528\n",
      "Iteration: 3083 lambda_n: 0.9546560449868319 Loss: 0.0057976770800313125\n",
      "Iteration: 3084 lambda_n: 0.9751219157581911 Loss: 0.005782819974990351\n",
      "Iteration: 3085 lambda_n: 0.9278550384922707 Loss: 0.005767644376723271\n",
      "Iteration: 3086 lambda_n: 0.9509702301266969 Loss: 0.0057532043940336335\n",
      "Iteration: 3087 lambda_n: 1.022333917563997 Loss: 0.0057384046873600845\n",
      "Iteration: 3088 lambda_n: 0.89806864689124 Loss: 0.0057224943795481705\n",
      "Iteration: 3089 lambda_n: 0.9007394540190259 Loss: 0.005708517990701002\n",
      "Iteration: 3090 lambda_n: 0.9740389853975413 Loss: 0.005694500047994702\n",
      "Iteration: 3091 lambda_n: 1.035674264000611 Loss: 0.005679341378823256\n",
      "Iteration: 3092 lambda_n: 1.033365858948583 Loss: 0.005663223513151227\n",
      "Iteration: 3093 lambda_n: 0.885847234642295 Loss: 0.005647141587375839\n",
      "Iteration: 3094 lambda_n: 1.02987645833273 Loss: 0.005633355456481683\n",
      "Iteration: 3095 lambda_n: 0.9436116634774816 Loss: 0.005617327862845537\n",
      "Iteration: 3096 lambda_n: 0.9761549022049724 Loss: 0.0056026427901888195\n",
      "Iteration: 3097 lambda_n: 0.9998493850140809 Loss: 0.005587451272667021\n",
      "Iteration: 3098 lambda_n: 0.9297955186529495 Loss: 0.005571891021258901\n",
      "Iteration: 3099 lambda_n: 0.910128107185899 Loss: 0.0055574210027883875\n",
      "Iteration: 3100 lambda_n: 0.8859176812662961 Loss: 0.005543257072117368\n",
      "Iteration: 3101 lambda_n: 0.8889370054352924 Loss: 0.005529469929412225\n",
      "Iteration: 3102 lambda_n: 0.9121424196567385 Loss: 0.005515635809802612\n",
      "Iteration: 3103 lambda_n: 0.9733840026721879 Loss: 0.0055014405669638715\n",
      "Iteration: 3104 lambda_n: 0.9551495444216886 Loss: 0.005486292263661535\n",
      "Iteration: 3105 lambda_n: 0.9255986629737658 Loss: 0.005471427748041443\n",
      "Iteration: 3106 lambda_n: 1.0358715623789858 Loss: 0.005457023131018669\n",
      "Iteration: 3107 lambda_n: 0.9783810499218397 Loss: 0.005440902408534874\n",
      "Iteration: 3108 lambda_n: 0.9821489918130232 Loss: 0.0054256763954433395\n",
      "Iteration: 3109 lambda_n: 0.891399913739398 Loss: 0.005410391758488181\n",
      "Iteration: 3110 lambda_n: 1.0257408495592133 Loss: 0.005396519411608618\n",
      "Iteration: 3111 lambda_n: 0.9632965813188048 Loss: 0.005380556408526483\n",
      "Iteration: 3112 lambda_n: 0.9663143292092519 Loss: 0.0053655652037711095\n",
      "Iteration: 3113 lambda_n: 0.9879846031347136 Loss: 0.005350527050067299\n",
      "Iteration: 3114 lambda_n: 0.931830458563451 Loss: 0.005335151670293689\n",
      "Iteration: 3115 lambda_n: 0.9533485004099124 Loss: 0.005320650196050392\n",
      "Iteration: 3116 lambda_n: 0.9221146402558015 Loss: 0.005305813864615605\n",
      "Iteration: 3117 lambda_n: 0.9895713299806506 Loss: 0.005291463618820648\n",
      "Iteration: 3118 lambda_n: 0.9261318999439706 Loss: 0.005276063605294377\n",
      "Iteration: 3119 lambda_n: 0.962452560939897 Loss: 0.005261650870013882\n",
      "Iteration: 3120 lambda_n: 0.9784432889260194 Loss: 0.005246672916671565\n",
      "Iteration: 3121 lambda_n: 1.0202379706251945 Loss: 0.005231446126645606\n",
      "Iteration: 3122 lambda_n: 1.0362378812185442 Loss: 0.0052155689335532055\n",
      "Iteration: 3123 lambda_n: 0.8942521957024979 Loss: 0.005199442763519954\n",
      "Iteration: 3124 lambda_n: 1.008323540842288 Loss: 0.0051855262215343804\n",
      "Iteration: 3125 lambda_n: 0.9507909534194118 Loss: 0.005169834493097247\n",
      "Iteration: 3126 lambda_n: 0.9614457703127104 Loss: 0.005155038113798653\n",
      "Iteration: 3127 lambda_n: 0.9208147264360624 Loss: 0.005140075937927754\n",
      "Iteration: 3128 lambda_n: 0.9420742148642918 Loss: 0.005125746083829873\n",
      "Iteration: 3129 lambda_n: 0.9612842132305522 Loss: 0.005111085401537366\n",
      "Iteration: 3130 lambda_n: 0.8832556951179469 Loss: 0.005096125786578138\n",
      "Iteration: 3131 lambda_n: 0.935939120812561 Loss: 0.005082380474616688\n",
      "Iteration: 3132 lambda_n: 1.0052673547727953 Loss: 0.005067815312932113\n",
      "Iteration: 3133 lambda_n: 0.9946724350490861 Loss: 0.005052171276784004\n",
      "Iteration: 3134 lambda_n: 0.9862220257556158 Loss: 0.0050366921372158886\n",
      "Iteration: 3135 lambda_n: 0.9225870199239334 Loss: 0.005021344520880343\n",
      "Iteration: 3136 lambda_n: 0.9259090923221392 Loss: 0.005006987210373273\n",
      "Iteration: 3137 lambda_n: 1.0027825117826512 Loss: 0.004992578217345283\n",
      "Iteration: 3138 lambda_n: 0.9767127704788586 Loss: 0.004976972938243516\n",
      "Iteration: 3139 lambda_n: 0.9045864062988561 Loss: 0.004961773373800369\n",
      "Iteration: 3140 lambda_n: 0.9136449762990123 Loss: 0.0049476962528326104\n",
      "Iteration: 3141 lambda_n: 1.010354348700034 Loss: 0.004933478178544486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3142 lambda_n: 0.9957367090927581 Loss: 0.004917755138652901\n",
      "Iteration: 3143 lambda_n: 0.999220839219267 Loss: 0.004902259596197005\n",
      "Iteration: 3144 lambda_n: 1.0275475723090752 Loss: 0.004886709853304116\n",
      "Iteration: 3145 lambda_n: 1.0362567130858245 Loss: 0.004870719313745014\n",
      "Iteration: 3146 lambda_n: 0.9457280701243488 Loss: 0.004854593264811286\n",
      "Iteration: 3147 lambda_n: 0.925668550167204 Loss: 0.004839876025641957\n",
      "Iteration: 3148 lambda_n: 1.0095725157468454 Loss: 0.004825470966155815\n",
      "Iteration: 3149 lambda_n: 0.9275612558790545 Loss: 0.004809760230371414\n",
      "Iteration: 3150 lambda_n: 0.9760312727005696 Loss: 0.004795325753238745\n",
      "Iteration: 3151 lambda_n: 0.8960565092857207 Loss: 0.004780137016741724\n",
      "Iteration: 3152 lambda_n: 0.9744387345618752 Loss: 0.004766192843352263\n",
      "Iteration: 3153 lambda_n: 0.8892216323790848 Loss: 0.004751028927238211\n",
      "Iteration: 3154 lambda_n: 0.9780659692934603 Loss: 0.004737191150877484\n",
      "Iteration: 3155 lambda_n: 0.9132506693088085 Loss: 0.004721970827237424\n",
      "Iteration: 3156 lambda_n: 1.0020042528345339 Loss: 0.00470775915526307\n",
      "Iteration: 3157 lambda_n: 0.9165852012002106 Loss: 0.00469216635315261\n",
      "Iteration: 3158 lambda_n: 0.8868556474682627 Loss: 0.0046779028283429214\n",
      "Iteration: 3159 lambda_n: 0.9365567981466211 Loss: 0.004664101960214194\n",
      "Iteration: 3160 lambda_n: 1.0191600170986461 Loss: 0.004649527682851401\n",
      "Iteration: 3161 lambda_n: 1.020784759376619 Loss: 0.004633667993330235\n",
      "Iteration: 3162 lambda_n: 1.027268797599306 Loss: 0.0046177830437136675\n",
      "Iteration: 3163 lambda_n: 1.0312676617748004 Loss: 0.004601797216537588\n",
      "Iteration: 3164 lambda_n: 0.934857484448255 Loss: 0.004585749185401411\n",
      "Iteration: 3165 lambda_n: 0.9156889856348752 Loss: 0.004571201458729317\n",
      "Iteration: 3166 lambda_n: 0.9109657501138545 Loss: 0.004556952041233827\n",
      "Iteration: 3167 lambda_n: 0.9870615268949955 Loss: 0.0045427761435571316\n",
      "Iteration: 3168 lambda_n: 1.0311346004904982 Loss: 0.004527416111673945\n",
      "Iteration: 3169 lambda_n: 0.9945005680985171 Loss: 0.004511370267218613\n",
      "Iteration: 3170 lambda_n: 0.9572483211784639 Loss: 0.004495894522103399\n",
      "Iteration: 3171 lambda_n: 0.9393894884247961 Loss: 0.004480998494117662\n",
      "Iteration: 3172 lambda_n: 0.8881273675323621 Loss: 0.004466380394784521\n",
      "Iteration: 3173 lambda_n: 1.02065557378139 Loss: 0.0044525600197194955\n",
      "Iteration: 3174 lambda_n: 0.910604787565316 Loss: 0.0044366773641052615\n",
      "Iteration: 3175 lambda_n: 0.9076342106604633 Loss: 0.004422507256406555\n",
      "Iteration: 3176 lambda_n: 0.9428492662386713 Loss: 0.0044083833956279255\n",
      "Iteration: 3177 lambda_n: 0.9694739426338383 Loss: 0.004393711569547642\n",
      "Iteration: 3178 lambda_n: 0.9102953943109487 Loss: 0.004378625456903592\n",
      "Iteration: 3179 lambda_n: 0.9789248347542042 Loss: 0.0043644602521054334\n",
      "Iteration: 3180 lambda_n: 0.9429194095439031 Loss: 0.00434922712181132\n",
      "Iteration: 3181 lambda_n: 0.9395970907308208 Loss: 0.00433455429932258\n",
      "Iteration: 3182 lambda_n: 0.8829202281545762 Loss: 0.004319933199677163\n",
      "Iteration: 3183 lambda_n: 0.8898669695765393 Loss: 0.004306194072568763\n",
      "Iteration: 3184 lambda_n: 1.0006761358906604 Loss: 0.004292346869039676\n",
      "Iteration: 3185 lambda_n: 1.0094133862544175 Loss: 0.004276775392560844\n",
      "Iteration: 3186 lambda_n: 0.9998391221992616 Loss: 0.004261067984818661\n",
      "Iteration: 3187 lambda_n: 0.928476181646608 Loss: 0.004245509590226098\n",
      "Iteration: 3188 lambda_n: 0.9826917695449151 Loss: 0.004231061692965832\n",
      "Iteration: 3189 lambda_n: 0.9922372706117376 Loss: 0.004215770181395443\n",
      "Iteration: 3190 lambda_n: 0.9887568834065881 Loss: 0.004200330162708921\n",
      "Iteration: 3191 lambda_n: 0.9973066945468724 Loss: 0.00418494433090346\n",
      "Iteration: 3192 lambda_n: 0.9388595851107278 Loss: 0.004169425487214574\n",
      "Iteration: 3193 lambda_n: 0.9205422562513292 Loss: 0.004154816152327343\n",
      "Iteration: 3194 lambda_n: 0.9250988708663686 Loss: 0.004140491874723807\n",
      "Iteration: 3195 lambda_n: 0.9244313556508221 Loss: 0.004126096719616683\n",
      "Iteration: 3196 lambda_n: 0.9029838310774956 Loss: 0.004111711978396191\n",
      "Iteration: 3197 lambda_n: 0.8919826856788234 Loss: 0.004097661000573545\n",
      "Iteration: 3198 lambda_n: 0.9059683614761073 Loss: 0.004083781232990649\n",
      "Iteration: 3199 lambda_n: 0.8896550260429045 Loss: 0.004069683866621303\n",
      "Iteration: 3200 lambda_n: 1.0230307228215256 Loss: 0.004055840370931186\n",
      "Iteration: 3201 lambda_n: 0.9023793513256786 Loss: 0.004039921512316864\n",
      "Iteration: 3202 lambda_n: 1.0285121640105832 Loss: 0.00402588007722352\n",
      "Iteration: 3203 lambda_n: 0.9023586476031983 Loss: 0.004009875991555641\n",
      "Iteration: 3204 lambda_n: 1.0321128208972523 Loss: 0.003995834938023029\n",
      "Iteration: 3205 lambda_n: 0.9044267465424355 Loss: 0.0039797748933878795\n",
      "Iteration: 3206 lambda_n: 0.9509064365635872 Loss: 0.003965701720478816\n",
      "Iteration: 3207 lambda_n: 0.9781988550166798 Loss: 0.003950905339831568\n",
      "Iteration: 3208 lambda_n: 0.9447465474071927 Loss: 0.00393568431506192\n",
      "Iteration: 3209 lambda_n: 0.9968955915061098 Loss: 0.003920983849740566\n",
      "Iteration: 3210 lambda_n: 0.993407789036981 Loss: 0.0039054719692795275\n",
      "Iteration: 3211 lambda_n: 0.895265584514385 Loss: 0.0038900143964062652\n",
      "Iteration: 3212 lambda_n: 0.9724208268371888 Loss: 0.0038760839626271314\n",
      "Iteration: 3213 lambda_n: 0.9659507571420393 Loss: 0.0038609530190712345\n",
      "Iteration: 3214 lambda_n: 0.9750829639280769 Loss: 0.0038459227863265498\n",
      "Iteration: 3215 lambda_n: 0.9265148993175635 Loss: 0.0038307504929266808\n",
      "Iteration: 3216 lambda_n: 1.0192652832771831 Loss: 0.0038163339534594354\n",
      "Iteration: 3217 lambda_n: 1.0201937808262793 Loss: 0.003800474260792681\n",
      "Iteration: 3218 lambda_n: 0.9721196264154123 Loss: 0.003784600162785553\n",
      "Iteration: 3219 lambda_n: 0.8881275211082416 Loss: 0.003769474132711283\n",
      "Iteration: 3220 lambda_n: 1.0178424690477326 Loss: 0.003755655040933037\n",
      "Iteration: 3221 lambda_n: 0.8925576575373411 Loss: 0.003739817650374124\n",
      "Iteration: 3222 lambda_n: 0.9847886376768232 Loss: 0.0037259296983307797\n",
      "Iteration: 3223 lambda_n: 0.9395670872693355 Loss: 0.0037106066972362375\n",
      "Iteration: 3224 lambda_n: 0.8900398831749907 Loss: 0.0036959873685203566\n",
      "Iteration: 3225 lambda_n: 0.9566475891619639 Loss: 0.003682138701256259\n",
      "Iteration: 3226 lambda_n: 1.0074037732558423 Loss: 0.00366725368384463\n",
      "Iteration: 3227 lambda_n: 0.9343697580324857 Loss: 0.0036515789670319094\n",
      "Iteration: 3228 lambda_n: 0.9099310589017874 Loss: 0.0036370406656983747\n",
      "Iteration: 3229 lambda_n: 0.9647422831748111 Loss: 0.003622882656448295\n",
      "Iteration: 3230 lambda_n: 0.939895558070558 Loss: 0.003607871858072713\n",
      "Iteration: 3231 lambda_n: 1.0082748372646748 Loss: 0.0035932477019327124\n",
      "Iteration: 3232 lambda_n: 0.9808789426363934 Loss: 0.003577559656372496\n",
      "Iteration: 3233 lambda_n: 0.951764011107064 Loss: 0.003562297919083749\n",
      "Iteration: 3234 lambda_n: 0.8917383957029691 Loss: 0.0035474892335390445\n",
      "Iteration: 3235 lambda_n: 0.9357767575498179 Loss: 0.0035336145395351765\n",
      "Iteration: 3236 lambda_n: 0.9145968750020739 Loss: 0.0035190546891626494\n",
      "Iteration: 3237 lambda_n: 1.024063485995957 Loss: 0.003504824422174574\n",
      "Iteration: 3238 lambda_n: 0.9933670278243731 Loss: 0.003488891009188467\n",
      "Iteration: 3239 lambda_n: 0.9379381307026293 Loss: 0.003473435255376389\n",
      "Iteration: 3240 lambda_n: 0.9804526803136655 Loss: 0.0034588419656321765\n",
      "Iteration: 3241 lambda_n: 0.9762571940359998 Loss: 0.003443587246896956\n",
      "Iteration: 3242 lambda_n: 0.9001166894242525 Loss: 0.0034283978574420506\n",
      "Iteration: 3243 lambda_n: 0.9653202105038038 Loss: 0.0034143931696942074\n",
      "Iteration: 3244 lambda_n: 1.0092197174298145 Loss: 0.0033993740475279895\n",
      "Iteration: 3245 lambda_n: 0.9271320821612931 Loss: 0.003383671963188084\n",
      "Iteration: 3246 lambda_n: 0.9244766827177913 Loss: 0.003369247102592365\n",
      "Iteration: 3247 lambda_n: 0.914120954436521 Loss: 0.003354863606442228\n",
      "Iteration: 3248 lambda_n: 0.9071625087354029 Loss: 0.0033406412801250214\n",
      "Iteration: 3249 lambda_n: 1.0136825011821504 Loss: 0.0033265272663552175\n",
      "Iteration: 3250 lambda_n: 0.8978514987156581 Loss: 0.0033107560294271246\n",
      "Iteration: 3251 lambda_n: 1.0225260252241224 Loss: 0.0032967869860380406\n",
      "Iteration: 3252 lambda_n: 0.9265705852938472 Loss: 0.003280878280564124\n",
      "Iteration: 3253 lambda_n: 0.9956446164969335 Loss: 0.003266462530404194\n",
      "Iteration: 3254 lambda_n: 1.0305279804147953 Loss: 0.0032509721760182095\n",
      "Iteration: 3255 lambda_n: 1.0000293402858895 Loss: 0.0032349391706992868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3256 lambda_n: 0.910678543750617 Loss: 0.0032193807323634364\n",
      "Iteration: 3257 lambda_n: 0.9985501054438687 Loss: 0.0032052124709235673\n",
      "Iteration: 3258 lambda_n: 0.9962458411808414 Loss: 0.003189677176387333\n",
      "Iteration: 3259 lambda_n: 0.9296551806279783 Loss: 0.0031741778004432883\n",
      "Iteration: 3260 lambda_n: 0.9260497105240068 Loss: 0.0031597144907836074\n",
      "Iteration: 3261 lambda_n: 1.0024984872038418 Loss: 0.003145307335630033\n",
      "Iteration: 3262 lambda_n: 0.9861350675594441 Loss: 0.003129710887707518\n",
      "Iteration: 3263 lambda_n: 0.9792669699337953 Loss: 0.0031143690874284573\n",
      "Iteration: 3264 lambda_n: 0.9156036322189005 Loss: 0.0030991341621720144\n",
      "Iteration: 3265 lambda_n: 1.023603182822489 Loss: 0.0030848897759679383\n",
      "Iteration: 3266 lambda_n: 0.9259910671698536 Loss: 0.0030689652870735285\n",
      "Iteration: 3267 lambda_n: 1.0159284718120625 Loss: 0.0030545594534351402\n",
      "Iteration: 3268 lambda_n: 1.0004995230073015 Loss: 0.0030387545257135294\n",
      "Iteration: 3269 lambda_n: 0.9372874742879785 Loss: 0.0030231897107362306\n",
      "Iteration: 3270 lambda_n: 0.9471922309310108 Loss: 0.0030086083633005666\n",
      "Iteration: 3271 lambda_n: 0.9777551772918995 Loss: 0.0029938730024097375\n",
      "Iteration: 3272 lambda_n: 0.9105584735327192 Loss: 0.0029786622569291103\n",
      "Iteration: 3273 lambda_n: 0.9231362464493263 Loss: 0.0029644969512625163\n",
      "Iteration: 3274 lambda_n: 0.9519659226410716 Loss: 0.002950136050566038\n",
      "Iteration: 3275 lambda_n: 0.973431840493121 Loss: 0.002935326736078772\n",
      "Iteration: 3276 lambda_n: 1.0240872146707445 Loss: 0.002920183570280184\n",
      "Iteration: 3277 lambda_n: 0.9370560434953062 Loss: 0.0029042524793806562\n",
      "Iteration: 3278 lambda_n: 0.967514870393861 Loss: 0.002889675364289006\n",
      "Iteration: 3279 lambda_n: 1.0091148497389653 Loss: 0.002874624510340431\n",
      "Iteration: 3280 lambda_n: 0.9959382923995242 Loss: 0.0028589266151397676\n",
      "Iteration: 3281 lambda_n: 0.9884175938154419 Loss: 0.002843433793938469\n",
      "Iteration: 3282 lambda_n: 1.0141578181175934 Loss: 0.0028280580628307606\n",
      "Iteration: 3283 lambda_n: 0.9375613995817382 Loss: 0.0028122820225657376\n",
      "Iteration: 3284 lambda_n: 1.0224395229046899 Loss: 0.002797697595924373\n",
      "Iteration: 3285 lambda_n: 0.9436715241657193 Loss: 0.00278179293601296\n",
      "Iteration: 3286 lambda_n: 0.9617282508290146 Loss: 0.002767113659035612\n",
      "Iteration: 3287 lambda_n: 0.8957235909399824 Loss: 0.0027521536007326453\n",
      "Iteration: 3288 lambda_n: 0.9571072268001743 Loss: 0.0027382203629939825\n",
      "Iteration: 3289 lambda_n: 0.897229849206969 Loss: 0.002723332385248659\n",
      "Iteration: 3290 lambda_n: 0.9467388837393131 Loss: 0.0027093759065271745\n",
      "Iteration: 3291 lambda_n: 1.0306684883053585 Loss: 0.002694649412903076\n",
      "Iteration: 3292 lambda_n: 0.985579364398825 Loss: 0.002678617517994978\n",
      "Iteration: 3293 lambda_n: 1.0132045924390767 Loss: 0.0026632870979358667\n",
      "Iteration: 3294 lambda_n: 0.982827030744069 Loss: 0.002647527099797037\n",
      "Iteration: 3295 lambda_n: 1.0194271008114826 Loss: 0.0026322397358814974\n",
      "Iteration: 3296 lambda_n: 0.9626359128329426 Loss: 0.002616383207546995\n",
      "Iteration: 3297 lambda_n: 0.9409854616145354 Loss: 0.0026014101538087394\n",
      "Iteration: 3298 lambda_n: 0.9638449100953673 Loss: 0.002586773975042662\n",
      "Iteration: 3299 lambda_n: 0.9433533609479807 Loss: 0.0025717823624565425\n",
      "Iteration: 3300 lambda_n: 0.9143399516177597 Loss: 0.002557109598776652\n",
      "Iteration: 3301 lambda_n: 0.9404712107856514 Loss: 0.0025428882241512424\n",
      "Iteration: 3302 lambda_n: 0.9460730553743033 Loss: 0.0025282605363141307\n",
      "Iteration: 3303 lambda_n: 0.9342922820844394 Loss: 0.002513545849736906\n",
      "Iteration: 3304 lambda_n: 1.0160683759845293 Loss: 0.0024990145249849916\n",
      "Iteration: 3305 lambda_n: 0.9278677553021167 Loss: 0.00248321146226709\n",
      "Iteration: 3306 lambda_n: 0.8960549319255451 Loss: 0.0024687803359834675\n",
      "Iteration: 3307 lambda_n: 0.934215296007949 Loss: 0.0024548441229294716\n",
      "Iteration: 3308 lambda_n: 0.9854895102635731 Loss: 0.0024403145441224405\n",
      "Iteration: 3309 lambda_n: 1.006282460310351 Loss: 0.0024249876667561065\n",
      "Iteration: 3310 lambda_n: 1.031279750827598 Loss: 0.0024093375728147727\n",
      "Iteration: 3311 lambda_n: 0.8863132474073664 Loss: 0.00239329888998269\n",
      "Iteration: 3312 lambda_n: 0.9764570458160817 Loss: 0.002379514904612113\n",
      "Iteration: 3313 lambda_n: 0.9509583831224012 Loss: 0.0023643291596718995\n",
      "Iteration: 3314 lambda_n: 0.8957073071793453 Loss: 0.002349540132446281\n",
      "Iteration: 3315 lambda_n: 0.9752233832549702 Loss: 0.0023356105062155956\n",
      "Iteration: 3316 lambda_n: 1.009999259388779 Loss: 0.002320444454218815\n",
      "Iteration: 3317 lambda_n: 0.9205910652086886 Loss: 0.0023047377824033407\n",
      "Iteration: 3318 lambda_n: 0.929771558078137 Loss: 0.0022904216869489686\n",
      "Iteration: 3319 lambda_n: 1.009968805810449 Loss: 0.002275962997976183\n",
      "Iteration: 3320 lambda_n: 1.0118388544936066 Loss: 0.0022602573780711836\n",
      "Iteration: 3321 lambda_n: 0.9696816986363235 Loss: 0.002244522891464173\n",
      "Iteration: 3322 lambda_n: 0.9230933857728721 Loss: 0.0022294441701924813\n",
      "Iteration: 3323 lambda_n: 1.0224523914363766 Loss: 0.0022150900961065917\n",
      "Iteration: 3324 lambda_n: 0.9481772684676816 Loss: 0.002199191214465789\n",
      "Iteration: 3325 lambda_n: 1.0276769537854962 Loss: 0.0021844475057417084\n",
      "Iteration: 3326 lambda_n: 1.0224998725928205 Loss: 0.0021684678514610842\n",
      "Iteration: 3327 lambda_n: 0.9864188421118587 Loss: 0.0021525689475716827\n",
      "Iteration: 3328 lambda_n: 0.9606610274232543 Loss: 0.002137231311892112\n",
      "Iteration: 3329 lambda_n: 0.9164965390445609 Loss: 0.002122294413670363\n",
      "Iteration: 3330 lambda_n: 0.9995233586062824 Loss: 0.0021080444300912\n",
      "Iteration: 3331 lambda_n: 0.9718868860291856 Loss: 0.0020925037697084826\n",
      "Iteration: 3332 lambda_n: 1.0019546581292014 Loss: 0.0020773930601249014\n",
      "Iteration: 3333 lambda_n: 0.9835480526682306 Loss: 0.0020618151343195514\n",
      "Iteration: 3334 lambda_n: 0.9155324641415826 Loss: 0.002046523660544318\n",
      "Iteration: 3335 lambda_n: 0.8963817155588181 Loss: 0.0020322898932227185\n",
      "Iteration: 3336 lambda_n: 0.9438142550972856 Loss: 0.002018354101870108\n",
      "Iteration: 3337 lambda_n: 1.0088755295619223 Loss: 0.0020036811525100152\n",
      "Iteration: 3338 lambda_n: 0.9225561072912214 Loss: 0.001987997036709522\n",
      "Iteration: 3339 lambda_n: 0.9302669120132243 Loss: 0.001973655136659362\n",
      "Iteration: 3340 lambda_n: 0.9624583621445781 Loss: 0.0019591936455949127\n",
      "Iteration: 3341 lambda_n: 0.964635740530646 Loss: 0.0019442320244457423\n",
      "Iteration: 3342 lambda_n: 0.9467944203222396 Loss: 0.0019292368724948918\n",
      "Iteration: 3343 lambda_n: 0.8825982652358111 Loss: 0.0019145193783153696\n",
      "Iteration: 3344 lambda_n: 1.030519537173325 Loss: 0.0019008000739786917\n",
      "Iteration: 3345 lambda_n: 0.9593940012745698 Loss: 0.0018847818109303327\n",
      "Iteration: 3346 lambda_n: 0.9511551596646978 Loss: 0.0018698694743138101\n",
      "Iteration: 3347 lambda_n: 1.0296718564161027 Loss: 0.0018550855502858783\n",
      "Iteration: 3348 lambda_n: 0.9085596910630633 Loss: 0.0018390816374571199\n",
      "Iteration: 3349 lambda_n: 0.9511405409260456 Loss: 0.00182496049853713\n",
      "Iteration: 3350 lambda_n: 0.9084180785230354 Loss: 0.001810177925347043\n",
      "Iteration: 3351 lambda_n: 0.8989107515651514 Loss: 0.0017960597064769642\n",
      "Iteration: 3352 lambda_n: 0.9174814388967586 Loss: 0.001782089605682208\n",
      "Iteration: 3353 lambda_n: 0.887904467126712 Loss: 0.0017678312737242916\n",
      "Iteration: 3354 lambda_n: 0.8862775949799245 Loss: 0.0017540329632717681\n",
      "Iteration: 3355 lambda_n: 0.9710352435992677 Loss: 0.0017402603112666023\n",
      "Iteration: 3356 lambda_n: 0.9596008540825387 Loss: 0.0017251709785996105\n",
      "Iteration: 3357 lambda_n: 0.9583482849411942 Loss: 0.001710259798247414\n",
      "Iteration: 3358 lambda_n: 0.8930192662844708 Loss: 0.0016953685598705334\n",
      "Iteration: 3359 lambda_n: 0.8871498058175962 Loss: 0.0016814928752825918\n",
      "Iteration: 3360 lambda_n: 1.0064811392409752 Loss: 0.001667708824877643\n",
      "Iteration: 3361 lambda_n: 0.9232905438738971 Loss: 0.0016520712092500701\n",
      "Iteration: 3362 lambda_n: 1.0316043500724474 Loss: 0.0016377266389828859\n",
      "Iteration: 3363 lambda_n: 1.0279132630500873 Loss: 0.0016216998738654443\n",
      "Iteration: 3364 lambda_n: 0.9576285205023003 Loss: 0.0016057311102623025\n",
      "Iteration: 3365 lambda_n: 0.8841708468215939 Loss: 0.0015908548391747157\n",
      "Iteration: 3366 lambda_n: 0.9986911845926707 Loss: 0.001577120234605168\n",
      "Iteration: 3367 lambda_n: 0.9823558012353234 Loss: 0.0015616073267837492\n",
      "Iteration: 3368 lambda_n: 0.9095881199301483 Loss: 0.0015463488462148242\n",
      "Iteration: 3369 lambda_n: 0.970390482809176 Loss: 0.00153222125894559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3370 lambda_n: 0.9168857439141893 Loss: 0.0015171499842904779\n",
      "Iteration: 3371 lambda_n: 0.9981995760896148 Loss: 0.001502910371130568\n",
      "Iteration: 3372 lambda_n: 0.9315196041604302 Loss: 0.0014874086878204694\n",
      "Iteration: 3373 lambda_n: 0.9431664567011426 Loss: 0.0014729432664264232\n",
      "Iteration: 3374 lambda_n: 0.8908701944824334 Loss: 0.0014582977417125107\n",
      "Iteration: 3375 lambda_n: 0.9397353338272297 Loss: 0.0014444650000802282\n",
      "Iteration: 3376 lambda_n: 0.8968005253691506 Loss: 0.0014298743074699792\n",
      "Iteration: 3377 lambda_n: 1.037940531716084 Loss: 0.0014159510181922384\n",
      "Iteration: 3378 lambda_n: 0.986054914574145 Loss: 0.0013998374443478935\n",
      "Iteration: 3379 lambda_n: 0.950801297223144 Loss: 0.0013845303890291103\n",
      "Iteration: 3380 lambda_n: 0.9519465795350981 Loss: 0.0013697715687102459\n",
      "Iteration: 3381 lambda_n: 0.974888058632614 Loss: 0.0013549959651565494\n",
      "Iteration: 3382 lambda_n: 0.9599214054956877 Loss: 0.001339865346744367\n",
      "Iteration: 3383 lambda_n: 0.9460258718900523 Loss: 0.0013249681154939598\n",
      "Iteration: 3384 lambda_n: 0.92048173620499 Loss: 0.0013102876408887278\n",
      "Iteration: 3385 lambda_n: 0.980346921456659 Loss: 0.00129600465888904\n",
      "Iteration: 3386 lambda_n: 0.9994317286051623 Loss: 0.0012807939949243453\n",
      "Iteration: 3387 lambda_n: 0.9070595569243832 Loss: 0.0012652885865069112\n",
      "Iteration: 3388 lambda_n: 1.0330491868657545 Loss: 0.0012512175052681246\n",
      "Iteration: 3389 lambda_n: 0.9682916998162441 Loss: 0.0012351934693007314\n",
      "Iteration: 3390 lambda_n: 1.0205976493460482 Loss: 0.0012201754285968056\n",
      "Iteration: 3391 lambda_n: 1.0344729207319077 Loss: 0.001204347791161406\n",
      "Iteration: 3392 lambda_n: 0.9348903480456713 Loss: 0.0011883067910197893\n",
      "Iteration: 3393 lambda_n: 0.9799879183534136 Loss: 0.0011738116094653721\n",
      "Iteration: 3394 lambda_n: 0.9601363417524599 Loss: 0.0011586189597390898\n",
      "Iteration: 3395 lambda_n: 0.9078093324158427 Loss: 0.0011437358888441232\n",
      "Iteration: 3396 lambda_n: 0.9916658660108237 Loss: 0.0011296656731727217\n",
      "Iteration: 3397 lambda_n: 0.98136065447203 Loss: 0.0011142977742709503\n",
      "Iteration: 3398 lambda_n: 0.9730310793820498 Loss: 0.0010990917475582219\n",
      "Iteration: 3399 lambda_n: 0.89677309311793 Loss: 0.0010840170251568046\n",
      "Iteration: 3400 lambda_n: 0.9735820609261358 Loss: 0.001070125802989444\n",
      "Iteration: 3401 lambda_n: 0.8833916107703939 Loss: 0.0010550471517257454\n",
      "Iteration: 3402 lambda_n: 0.9750853559197544 Loss: 0.0010413675817727145\n",
      "Iteration: 3403 lambda_n: 0.927813651132458 Loss: 0.001026270695784474\n",
      "Iteration: 3404 lambda_n: 0.9137884259624093 Loss: 0.0010119083494831516\n",
      "Iteration: 3405 lambda_n: 0.9074689493832128 Loss: 0.0009977657678783643\n",
      "Iteration: 3406 lambda_n: 1.0005507108835845 Loss: 0.0009837237373231378\n",
      "Iteration: 3407 lambda_n: 0.8945523548607272 Loss: 0.0009682447177465663\n",
      "Iteration: 3408 lambda_n: 0.9795057622210399 Loss: 0.0009544086775944212\n",
      "Iteration: 3409 lambda_n: 0.9271068928235533 Loss: 0.0009392622489887329\n",
      "Iteration: 3410 lambda_n: 0.9720668377289742 Loss: 0.0009249297313696477\n",
      "Iteration: 3411 lambda_n: 0.9843030336199536 Loss: 0.0009099061990393672\n",
      "Iteration: 3412 lambda_n: 0.9839004669926105 Loss: 0.0008946980234993267\n",
      "Iteration: 3413 lambda_n: 0.9880899858503126 Loss: 0.0008795008488603882\n",
      "Iteration: 3414 lambda_n: 0.9692918098191682 Loss: 0.00086424408795351\n",
      "Iteration: 3415 lambda_n: 0.9693821239534415 Loss: 0.0008492829037720582\n",
      "Iteration: 3416 lambda_n: 0.9593164850435371 Loss: 0.0008343259510449034\n",
      "Iteration: 3417 lambda_n: 1.0024225747518496 Loss: 0.0008195302211276162\n",
      "Iteration: 3418 lambda_n: 0.975311835666643 Loss: 0.0008040763898315398\n",
      "Iteration: 3419 lambda_n: 1.034426847522283 Loss: 0.000789047593214832\n",
      "Iteration: 3420 lambda_n: 0.9885681873641512 Loss: 0.0007731160954545421\n",
      "Iteration: 3421 lambda_n: 0.9206260387856439 Loss: 0.0007578993973476949\n",
      "Iteration: 3422 lambda_n: 0.8849980849490214 Loss: 0.0007437365812871733\n",
      "Iteration: 3423 lambda_n: 0.9128819466716465 Loss: 0.000730129757585226\n",
      "Iteration: 3424 lambda_n: 1.0129433131731684 Loss: 0.0007161029438995429\n",
      "Iteration: 3425 lambda_n: 0.9437290058755834 Loss: 0.0007005498492603723\n",
      "Iteration: 3426 lambda_n: 0.9223874204358024 Loss: 0.0006860709969969121\n",
      "Iteration: 3427 lambda_n: 0.8896483804203867 Loss: 0.0006719312246647347\n",
      "Iteration: 3428 lambda_n: 1.0147443460823298 Loss: 0.000658305174326\n",
      "Iteration: 3429 lambda_n: 0.9901288121920903 Loss: 0.0006427786141326154\n",
      "Iteration: 3430 lambda_n: 1.0245510586179025 Loss: 0.0006276461249755335\n",
      "Iteration: 3431 lambda_n: 0.9212531488179869 Loss: 0.000612007546752678\n",
      "Iteration: 3432 lambda_n: 0.9035304653057368 Loss: 0.0005979648528258661\n",
      "Iteration: 3433 lambda_n: 1.0377932514474053 Loss: 0.0005842117540169446\n",
      "Iteration: 3434 lambda_n: 0.8895473514200991 Loss: 0.0005684412989535389\n",
      "Iteration: 3435 lambda_n: 0.9469939926006502 Loss: 0.0005549484800139991\n",
      "Iteration: 3436 lambda_n: 0.9515874897895303 Loss: 0.000540612451905358\n",
      "Iteration: 3437 lambda_n: 0.9345432369423236 Loss: 0.0005262395772597273\n",
      "Iteration: 3438 lambda_n: 0.8828718032027465 Loss: 0.0005121598921193364\n",
      "Iteration: 3439 lambda_n: 0.977304779247786 Loss: 0.0004988953035745104\n",
      "Iteration: 3440 lambda_n: 0.9639180364172343 Loss: 0.0004842588648983014\n",
      "Iteration: 3441 lambda_n: 1.024186913548208 Loss: 0.0004698779202294051\n",
      "Iteration: 3442 lambda_n: 0.9770982748688871 Loss: 0.000454666791563957\n",
      "Iteration: 3443 lambda_n: 0.9000394918564336 Loss: 0.0004402318410920764\n",
      "Iteration: 3444 lambda_n: 0.9433761922956865 Loss: 0.0004270125074412583\n",
      "Iteration: 3445 lambda_n: 1.0049161208736606 Loss: 0.0004132493606007394\n",
      "Iteration: 3446 lambda_n: 1.0165772567082236 Loss: 0.00039871124991480955\n",
      "Iteration: 3447 lambda_n: 1.038184026365281 Loss: 0.00038415854613130446\n",
      "Iteration: 3448 lambda_n: 1.0339450158750405 Loss: 0.0003694906558607039\n",
      "Iteration: 3449 lambda_n: 1.0248972618019059 Loss: 0.00035512220276230644\n",
      "Iteration: 3450 lambda_n: 0.9445160664139114 Loss: 0.0003408260778243792\n",
      "Iteration: 3451 lambda_n: 0.9047923771992321 Loss: 0.0003274845992920124\n",
      "Iteration: 3452 lambda_n: 0.8884907191833394 Loss: 0.00031508349776930187\n",
      "Iteration: 3453 lambda_n: 0.9304507784818229 Loss: 0.000303468959663603\n",
      "Iteration: 3454 lambda_n: 0.9946018744702859 Loss: 0.00029198609787220355\n",
      "Iteration: 3455 lambda_n: 1.0259532115129513 Loss: 0.00028051736717250164\n",
      "Iteration: 3456 lambda_n: 0.9525613332125992 Loss: 0.0002695774807906902\n",
      "Iteration: 3457 lambda_n: 0.9596543625585735 Loss: 0.0002602394653124779\n",
      "Iteration: 3458 lambda_n: 0.9794668549690215 Loss: 0.00025157338472829626\n",
      "Iteration: 3459 lambda_n: 0.9464131311647362 Loss: 0.0002434549522619258\n",
      "Iteration: 3460 lambda_n: 0.8946255374725763 Loss: 0.00023628100485795865\n",
      "Iteration: 3461 lambda_n: 0.9072571186488758 Loss: 0.00023007303858623806\n",
      "Iteration: 3462 lambda_n: 0.9361136865773393 Loss: 0.00022430219233946796\n",
      "Iteration: 3463 lambda_n: 0.9525164587942944 Loss: 0.00021886740809638037\n",
      "Iteration: 3464 lambda_n: 1.0150431857045015 Loss: 0.00021384804549203775\n",
      "Iteration: 3465 lambda_n: 0.9186808741993855 Loss: 0.0002090231316557072\n",
      "Iteration: 3466 lambda_n: 1.0084081445505975 Loss: 0.0002051058916695543\n",
      "Iteration: 3467 lambda_n: 1.0163271688888862 Loss: 0.0002012412647443336\n",
      "Iteration: 3468 lambda_n: 0.8973701191411934 Loss: 0.0001977797109496069\n",
      "Iteration: 3469 lambda_n: 1.0188724491989705 Loss: 0.00019506211563560065\n",
      "Iteration: 3470 lambda_n: 0.940960476089918 Loss: 0.00019231177043202083\n",
      "Iteration: 3471 lambda_n: 0.8876688958422441 Loss: 0.00019007084457254295\n",
      "Iteration: 3472 lambda_n: 0.9330368972816475 Loss: 0.00018819344893560702\n",
      "Iteration: 3473 lambda_n: 0.8950646264582894 Loss: 0.0001864405636342361\n",
      "Iteration: 3474 lambda_n: 0.9139912038011857 Loss: 0.00018495333510975143\n",
      "Iteration: 3475 lambda_n: 0.8875834208747014 Loss: 0.0001836096876759667\n",
      "Iteration: 3476 lambda_n: 1.0358543750440579 Loss: 0.0001824576693841413\n",
      "Iteration: 3477 lambda_n: 0.9567422282340436 Loss: 0.00018128006960562655\n",
      "Iteration: 3478 lambda_n: 0.9468199247674898 Loss: 0.00018033704226500996\n",
      "Iteration: 3479 lambda_n: 1.0107140740569434 Loss: 0.00017952308714408007\n",
      "Iteration: 3480 lambda_n: 0.9711098764162648 Loss: 0.00017876887338252526\n",
      "Iteration: 3481 lambda_n: 1.0210953880124864 Loss: 0.0001781422774921948\n",
      "Iteration: 3482 lambda_n: 0.9724393707977917 Loss: 0.00017757333803022422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3483 lambda_n: 1.0172814934614076 Loss: 0.00017710584775784552\n",
      "Iteration: 3484 lambda_n: 0.9406176944626937 Loss: 0.00017668391327940023\n",
      "Iteration: 3485 lambda_n: 0.9318509318205056 Loss: 0.00017634608998612434\n",
      "Iteration: 3486 lambda_n: 1.0312496559426443 Loss: 0.00017605425020969252\n",
      "Iteration: 3487 lambda_n: 0.8991608395163253 Loss: 0.00017577464536425784\n",
      "Iteration: 3488 lambda_n: 1.0175757744199951 Loss: 0.00017556148234659235\n",
      "Iteration: 3489 lambda_n: 0.9788385827740972 Loss: 0.00017535100530134698\n",
      "Iteration: 3490 lambda_n: 0.939951377362165 Loss: 0.00017517365153813495\n",
      "Iteration: 3491 lambda_n: 0.9114579183714557 Loss: 0.00017502268646389248\n",
      "Iteration: 3492 lambda_n: 0.9367525576384129 Loss: 0.00017489146064667384\n",
      "Iteration: 3493 lambda_n: 0.96358648566487 Loss: 0.00017476991786501776\n",
      "Iteration: 3494 lambda_n: 0.9732850214353025 Loss: 0.00017465653816569284\n",
      "Iteration: 3495 lambda_n: 0.9324462325224989 Loss: 0.00017455160298709572\n",
      "Iteration: 3496 lambda_n: 1.0111442763291512 Loss: 0.0001744578679210273\n",
      "Iteration: 3497 lambda_n: 0.9337949814927182 Loss: 0.0001743629288947606\n",
      "Iteration: 3498 lambda_n: 0.9091558866737267 Loss: 0.00017427884461120922\n",
      "Iteration: 3499 lambda_n: 1.0188102971640434 Loss: 0.0001741995419165713\n",
      "Iteration: 3500 lambda_n: 0.8960708372193879 Loss: 0.0001741136156695079\n",
      "Iteration: 3501 lambda_n: 0.9756377325406126 Loss: 0.000174038167975049\n",
      "Iteration: 3502 lambda_n: 0.9559858707528514 Loss: 0.0001739568587284816\n",
      "Iteration: 3503 lambda_n: 0.99991082382328 Loss: 0.0001738766455266517\n",
      "Iteration: 3504 lambda_n: 0.9674168517948588 Loss: 0.00017379201858619474\n",
      "Iteration: 3505 lambda_n: 0.9010921320948906 Loss: 0.0001737084171416839\n",
      "Iteration: 3506 lambda_n: 0.9519822472892366 Loss: 0.0001736284775142461\n",
      "Iteration: 3507 lambda_n: 0.9040061736912303 Loss: 0.00017354228975096422\n",
      "Iteration: 3508 lambda_n: 0.95083774418964 Loss: 0.0001734579981698899\n",
      "Iteration: 3509 lambda_n: 0.9250391201721214 Loss: 0.00017336712890764257\n",
      "Iteration: 3510 lambda_n: 1.0212696558575969 Loss: 0.00017327599019610786\n",
      "Iteration: 3511 lambda_n: 0.9966164434166533 Loss: 0.00017317280776049256\n",
      "Iteration: 3512 lambda_n: 0.9546002569972375 Loss: 0.00017306870973394337\n",
      "Iteration: 3513 lambda_n: 0.9230450711228303 Loss: 0.00017296571256707638\n",
      "Iteration: 3514 lambda_n: 0.89330621553311 Loss: 0.00017286309458795833\n",
      "Iteration: 3515 lambda_n: 0.9133296746877501 Loss: 0.00017276096156804233\n",
      "Iteration: 3516 lambda_n: 0.9968250086606338 Loss: 0.0001726538971510588\n",
      "Iteration: 3517 lambda_n: 0.9130262361300133 Loss: 0.0001725342814259928\n",
      "Iteration: 3518 lambda_n: 0.9740498677735191 Loss: 0.00017242154900221884\n",
      "Iteration: 3519 lambda_n: 1.0249508270815266 Loss: 0.00017229859323590057\n",
      "Iteration: 3520 lambda_n: 0.952558498686068 Loss: 0.00017216619843276514\n",
      "Iteration: 3521 lambda_n: 0.9072333786488456 Loss: 0.00017204000301827637\n",
      "Iteration: 3522 lambda_n: 0.9699800382727437 Loss: 0.0001719171636994104\n",
      "Iteration: 3523 lambda_n: 1.0051963334421181 Loss: 0.0001717834259548657\n",
      "Iteration: 3524 lambda_n: 1.0141067667429664 Loss: 0.0001716421784524108\n",
      "Iteration: 3525 lambda_n: 1.0051642386381245 Loss: 0.00017149694558472373\n",
      "Iteration: 3526 lambda_n: 0.8860331657787555 Loss: 0.0001713503363515728\n",
      "Iteration: 3527 lambda_n: 0.9077276125271352 Loss: 0.00017121872307614857\n",
      "Iteration: 3528 lambda_n: 0.9193398764716912 Loss: 0.0001710820265869306\n",
      "Iteration: 3529 lambda_n: 0.9405632038177865 Loss: 0.00017094171280005311\n",
      "Iteration: 3530 lambda_n: 0.9822363046358916 Loss: 0.00017079632021394466\n",
      "Iteration: 3531 lambda_n: 0.9756930747664252 Loss: 0.000170642636436905\n",
      "Iteration: 3532 lambda_n: 0.9258128324387778 Loss: 0.00017048808457490795\n",
      "Iteration: 3533 lambda_n: 0.9439351323611461 Loss: 0.00017033969679175443\n",
      "Iteration: 3534 lambda_n: 0.9269936082537886 Loss: 0.00017030284574367875\n",
      "Iteration: 3535 lambda_n: 0.9764417132344863 Loss: 0.00017069608070839802\n",
      "Iteration: 3536 lambda_n: 0.9004633044800073 Loss: 0.00017110263957819367\n",
      "Iteration: 3537 lambda_n: 1.035379919023672 Loss: 0.0001714701857266606\n",
      "Iteration: 3538 lambda_n: 1.0028115866588716 Loss: 0.0001718854143717508\n",
      "Iteration: 3539 lambda_n: 0.8898985730670004 Loss: 0.0001722793495970851\n",
      "Iteration: 3540 lambda_n: 0.927432223685868 Loss: 0.00017262198132952418\n",
      "Iteration: 3541 lambda_n: 1.0204515376978025 Loss: 0.00017297293901012608\n",
      "Iteration: 3542 lambda_n: 0.9884743605727654 Loss: 0.00017335228695178624\n",
      "Iteration: 3543 lambda_n: 0.9872393366971216 Loss: 0.00017371254278924236\n",
      "Iteration: 3544 lambda_n: 0.9966188663903228 Loss: 0.00017406559339643525\n",
      "Iteration: 3545 lambda_n: 0.9201503460612501 Loss: 0.00017441537334824525\n",
      "Iteration: 3546 lambda_n: 0.9366141005674371 Loss: 0.00017473223574241816\n",
      "Iteration: 3547 lambda_n: 0.8905275179701858 Loss: 0.00017504926980098392\n",
      "Iteration: 3548 lambda_n: 1.0322658909968063 Loss: 0.00017534546653676976\n",
      "Iteration: 3549 lambda_n: 0.8853894244127486 Loss: 0.00017568331835785778\n",
      "Iteration: 3550 lambda_n: 0.9959749468211114 Loss: 0.00017596756131013565\n",
      "Iteration: 3551 lambda_n: 0.9357469888852747 Loss: 0.0001762822759689886\n",
      "Iteration: 3552 lambda_n: 0.9821455922374372 Loss: 0.00017657263955158975\n",
      "Iteration: 3553 lambda_n: 1.0326063428767198 Loss: 0.00017687235670757798\n",
      "Iteration: 3554 lambda_n: 0.9648355298383301 Loss: 0.00017718203721973756\n",
      "Iteration: 3555 lambda_n: 1.032573355601719 Loss: 0.00017746610905561375\n",
      "Iteration: 3556 lambda_n: 0.9701357800895439 Loss: 0.00017776506266839413\n",
      "Iteration: 3557 lambda_n: 0.9054645444201684 Loss: 0.0001780408899457411\n",
      "Iteration: 3558 lambda_n: 0.9520186304792383 Loss: 0.00017829401343812237\n",
      "Iteration: 3559 lambda_n: 0.998732354520586 Loss: 0.00017855607610451032\n",
      "Iteration: 3560 lambda_n: 0.9089340418215478 Loss: 0.00017882659956280355\n",
      "Iteration: 3561 lambda_n: 0.9640885920312238 Loss: 0.0001790686269096847\n",
      "Iteration: 3562 lambda_n: 0.9495822402860186 Loss: 0.00017932148270222284\n",
      "Iteration: 3563 lambda_n: 0.9955964075271472 Loss: 0.00017956655348265463\n",
      "Iteration: 3564 lambda_n: 0.9139315486356934 Loss: 0.0001798195152213677\n",
      "Iteration: 3565 lambda_n: 0.9732074442241843 Loss: 0.00018004791805941066\n",
      "Iteration: 3566 lambda_n: 0.916578275127833 Loss: 0.00018028756471886429\n",
      "Iteration: 3567 lambda_n: 0.922902302589777 Loss: 0.00018050970887575625\n",
      "Iteration: 3568 lambda_n: 0.9410913261461837 Loss: 0.00018073011560746404\n",
      "Iteration: 3569 lambda_n: 1.0013631434841561 Loss: 0.00018095158524411082\n",
      "Iteration: 3570 lambda_n: 0.9972133363672865 Loss: 0.0001811837737380734\n",
      "Iteration: 3571 lambda_n: 1.0379228128932139 Loss: 0.0001814113760596514\n",
      "Iteration: 3572 lambda_n: 0.9599696679456351 Loss: 0.00018164462029214505\n",
      "Iteration: 3573 lambda_n: 0.9794795886488509 Loss: 0.0001818568625561349\n",
      "Iteration: 3574 lambda_n: 1.0137725566623474 Loss: 0.00018207024674410266\n",
      "Iteration: 3575 lambda_n: 0.9367795489470426 Loss: 0.00018228783059931586\n",
      "Iteration: 3576 lambda_n: 0.9939530308736111 Loss: 0.00018248578594019392\n",
      "Iteration: 3577 lambda_n: 0.8956725707560523 Loss: 0.00018269290077327228\n",
      "Iteration: 3578 lambda_n: 0.9668559802800879 Loss: 0.0001828767434663749\n",
      "Iteration: 3579 lambda_n: 0.9730914612690663 Loss: 0.00018307260068842616\n",
      "Iteration: 3580 lambda_n: 0.9263163754806499 Loss: 0.00018326692841520652\n",
      "Iteration: 3581 lambda_n: 0.9722744016661933 Loss: 0.00018344927731327225\n",
      "Iteration: 3582 lambda_n: 0.9933586418293764 Loss: 0.0001836381261627113\n",
      "Iteration: 3583 lambda_n: 0.8979572629743402 Loss: 0.000183828383006577\n",
      "Iteration: 3584 lambda_n: 1.0090932658995146 Loss: 0.00018399789974904624\n",
      "Iteration: 3585 lambda_n: 1.00048953477507 Loss: 0.0001841860137141753\n",
      "Iteration: 3586 lambda_n: 0.9061173517963305 Loss: 0.00018436987155865113\n",
      "Iteration: 3587 lambda_n: 0.9709990744754395 Loss: 0.00018453402943694516\n",
      "Iteration: 3588 lambda_n: 0.9814947342697339 Loss: 0.0001847077514110192\n",
      "Iteration: 3589 lambda_n: 0.9953722622328495 Loss: 0.0001848810036487264\n",
      "Iteration: 3590 lambda_n: 1.0034444650085999 Loss: 0.00018505434837931477\n",
      "Iteration: 3591 lambda_n: 0.9482340036701773 Loss: 0.00018522673561407172\n",
      "Iteration: 3592 lambda_n: 1.033335198480533 Loss: 0.00018538741343705452\n",
      "Iteration: 3593 lambda_n: 1.0173864668584738 Loss: 0.00018556031195352836\n",
      "Iteration: 3594 lambda_n: 1.0242571028543255 Loss: 0.00018572819253272421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3595 lambda_n: 0.9220061369171889 Loss: 0.0001858949334916329\n",
      "Iteration: 3596 lambda_n: 0.944253447201865 Loss: 0.00018604298064759582\n",
      "Iteration: 3597 lambda_n: 0.8920763800750836 Loss: 0.0001861927822743698\n",
      "Iteration: 3598 lambda_n: 0.8995060347659762 Loss: 0.00018633256008381891\n",
      "Iteration: 3599 lambda_n: 0.9358696978523928 Loss: 0.0001864718840969541\n",
      "Iteration: 3600 lambda_n: 1.008713234572941 Loss: 0.00018661518003553208\n",
      "Iteration: 3601 lambda_n: 0.9199774011548867 Loss: 0.00018676780925588275\n",
      "Iteration: 3602 lambda_n: 0.9688587192214188 Loss: 0.00018690521262291682\n",
      "Iteration: 3603 lambda_n: 0.9787528258482984 Loss: 0.00018704825597582085\n",
      "Iteration: 3604 lambda_n: 0.956904642283411 Loss: 0.00018719101315926054\n",
      "Iteration: 3605 lambda_n: 0.9906789771494964 Loss: 0.00018732888186919652\n",
      "Iteration: 3606 lambda_n: 0.9568010464843106 Loss: 0.0001874699398761727\n",
      "Iteration: 3607 lambda_n: 0.8906039438862092 Loss: 0.00018760451065163942\n",
      "Iteration: 3608 lambda_n: 0.9549379970745568 Loss: 0.00018772829580584343\n",
      "Iteration: 3609 lambda_n: 1.033495252155687 Loss: 0.0001878596060121332\n",
      "Iteration: 3610 lambda_n: 0.9542690776709447 Loss: 0.00018800010298866224\n",
      "Iteration: 3611 lambda_n: 0.9446640892332341 Loss: 0.00018812820579878242\n",
      "Iteration: 3612 lambda_n: 0.8932006722378293 Loss: 0.0001882535770272705\n",
      "Iteration: 3613 lambda_n: 0.9636137726867227 Loss: 0.00018837078288035103\n",
      "Iteration: 3614 lambda_n: 0.9502160098456601 Loss: 0.0001884959153382336\n",
      "Iteration: 3615 lambda_n: 0.8875594002679027 Loss: 0.000188617913809077\n",
      "Iteration: 3616 lambda_n: 0.9355414825015316 Loss: 0.00018873059575091826\n",
      "Iteration: 3617 lambda_n: 0.9727002917425221 Loss: 0.00018884815967912258\n",
      "Iteration: 3618 lambda_n: 0.9611373440295667 Loss: 0.00018896908471024285\n",
      "Iteration: 3619 lambda_n: 0.9204254495693723 Loss: 0.00018908723925101193\n",
      "Iteration: 3620 lambda_n: 0.9136956410084869 Loss: 0.000189199143043988\n",
      "Iteration: 3621 lambda_n: 0.925122159171676 Loss: 0.000189309069735699\n",
      "Iteration: 3622 lambda_n: 0.993889939037162 Loss: 0.00018941922780830558\n",
      "Iteration: 3623 lambda_n: 0.9813904975496919 Loss: 0.00018953636093135256\n",
      "Iteration: 3624 lambda_n: 0.916584023109397 Loss: 0.00018965073645342299\n",
      "Iteration: 3625 lambda_n: 0.9760895481249113 Loss: 0.00018975638472370793\n",
      "Iteration: 3626 lambda_n: 0.9087281066714777 Loss: 0.0001898677651524568\n",
      "Iteration: 3627 lambda_n: 0.9914379152448455 Loss: 0.00018997033605000518\n",
      "Iteration: 3628 lambda_n: 1.0057289194213805 Loss: 0.00019008114703178712\n",
      "Iteration: 3629 lambda_n: 1.00617419843868 Loss: 0.00019019234592563365\n",
      "Iteration: 3630 lambda_n: 0.9313499221578041 Loss: 0.00019030238343484577\n",
      "Iteration: 3631 lambda_n: 0.9150407404456524 Loss: 0.00019040312227086397\n",
      "Iteration: 3632 lambda_n: 0.9913719698844907 Loss: 0.00019050110714316634\n",
      "Iteration: 3633 lambda_n: 0.982087212792905 Loss: 0.00019060624315294454\n",
      "Iteration: 3634 lambda_n: 0.9224411670006856 Loss: 0.0001907092967285375\n",
      "Iteration: 3635 lambda_n: 0.9007437525878412 Loss: 0.0001908050777765787\n",
      "Iteration: 3636 lambda_n: 1.0054147888851452 Loss: 0.00019089769537946157\n",
      "Iteration: 3637 lambda_n: 1.0208233004124336 Loss: 0.00019100011765445406\n",
      "Iteration: 3638 lambda_n: 0.9928159340069397 Loss: 0.00019110302142809918\n",
      "Iteration: 3639 lambda_n: 0.9088135311694422 Loss: 0.00019120203653657752\n",
      "Iteration: 3640 lambda_n: 1.03538085056086 Loss: 0.00019129173186179996\n",
      "Iteration: 3641 lambda_n: 0.9515776679375746 Loss: 0.0001913929830755638\n",
      "Iteration: 3642 lambda_n: 0.9589998011561331 Loss: 0.0001914850390203816\n",
      "Iteration: 3643 lambda_n: 0.963816575216268 Loss: 0.00019157691286743596\n",
      "Iteration: 3644 lambda_n: 0.9030628707991065 Loss: 0.00019166834845432503\n",
      "Iteration: 3645 lambda_n: 1.0261969272772817 Loss: 0.0001917531761907413\n",
      "Iteration: 3646 lambda_n: 0.9245876880082451 Loss: 0.000191848710039827\n",
      "Iteration: 3647 lambda_n: 1.0178062618395007 Loss: 0.00019193388317775245\n",
      "Iteration: 3648 lambda_n: 1.0127032391652626 Loss: 0.0001920267885609763\n",
      "Iteration: 3649 lambda_n: 0.9664279648994732 Loss: 0.0001921182882833548\n",
      "Iteration: 3650 lambda_n: 0.8931095240218481 Loss: 0.0001922047216516177\n",
      "Iteration: 3651 lambda_n: 0.9474908490783729 Loss: 0.00019228382444094627\n",
      "Iteration: 3652 lambda_n: 1.0269813493974913 Loss: 0.00019236701070557942\n",
      "Iteration: 3653 lambda_n: 0.9086323481278855 Loss: 0.00019245634561515187\n",
      "Iteration: 3654 lambda_n: 1.023521877419941 Loss: 0.00019253457572003752\n",
      "Iteration: 3655 lambda_n: 1.006354852408908 Loss: 0.00019262192931177927\n",
      "Iteration: 3656 lambda_n: 0.9900012521404923 Loss: 0.00019270695869550987\n",
      "Iteration: 3657 lambda_n: 0.9003997902156761 Loss: 0.00019278978652677347\n",
      "Iteration: 3658 lambda_n: 0.9855715954157598 Loss: 0.0001928643861887913\n",
      "Iteration: 3659 lambda_n: 1.0343193945609543 Loss: 0.00019294534254302918\n",
      "Iteration: 3660 lambda_n: 0.9750312999422536 Loss: 0.00019302950261067115\n",
      "Iteration: 3661 lambda_n: 1.0331492477757243 Loss: 0.00019310804413909233\n",
      "Iteration: 3662 lambda_n: 0.9832189686281481 Loss: 0.00019319049716590752\n",
      "Iteration: 3663 lambda_n: 0.9343403016773123 Loss: 0.00019326818608661353\n",
      "Iteration: 3664 lambda_n: 0.9516877225569165 Loss: 0.00019334131707960693\n",
      "Iteration: 3665 lambda_n: 1.0097719486376142 Loss: 0.00019341514699267883\n",
      "Iteration: 3666 lambda_n: 0.8926902737461284 Loss: 0.00019349278309669683\n",
      "Iteration: 3667 lambda_n: 0.9931719806993304 Loss: 0.00019356075200105142\n",
      "Iteration: 3668 lambda_n: 1.025364578357095 Loss: 0.00019363574561605972\n",
      "Iteration: 3669 lambda_n: 0.9354009896776488 Loss: 0.00019371245028750457\n",
      "Iteration: 3670 lambda_n: 0.9188949567967619 Loss: 0.00019378174375826818\n",
      "Iteration: 3671 lambda_n: 0.9122427379672292 Loss: 0.00019384921728148575\n",
      "Iteration: 3672 lambda_n: 0.9934238672003036 Loss: 0.00019391562711241152\n",
      "Iteration: 3673 lambda_n: 0.9575384850733752 Loss: 0.00019398733959640204\n",
      "Iteration: 3674 lambda_n: 0.9391419233531589 Loss: 0.00019405581979887714\n",
      "Iteration: 3675 lambda_n: 0.9343045190246702 Loss: 0.0001941223859225834\n",
      "Iteration: 3676 lambda_n: 0.981213721392674 Loss: 0.00019418803274218877\n",
      "Iteration: 3677 lambda_n: 0.9701678551896263 Loss: 0.00019425638405221634\n",
      "Iteration: 3678 lambda_n: 1.0190818338668752 Loss: 0.00019432335297743246\n",
      "Iteration: 3679 lambda_n: 0.972973678389093 Loss: 0.00019439307385090635\n",
      "Iteration: 3680 lambda_n: 0.9247354421048566 Loss: 0.00019445901261834868\n",
      "Iteration: 3681 lambda_n: 0.9604968027904122 Loss: 0.00019452111894373972\n",
      "Iteration: 3682 lambda_n: 1.0044919835425938 Loss: 0.00019458508324260342\n",
      "Iteration: 3683 lambda_n: 0.9700212090920672 Loss: 0.00019465139303808787\n",
      "Iteration: 3684 lambda_n: 0.8864720960867557 Loss: 0.00019471483707090407\n",
      "Iteration: 3685 lambda_n: 0.944304655675585 Loss: 0.00019477229800410301\n",
      "Iteration: 3686 lambda_n: 0.9872896796303517 Loss: 0.00019483301770264143\n",
      "Iteration: 3687 lambda_n: 0.989979164418039 Loss: 0.0001948959593216288\n",
      "Iteration: 3688 lambda_n: 0.9079417298928322 Loss: 0.00019495850660811753\n",
      "Iteration: 3689 lambda_n: 0.9509022337305051 Loss: 0.0001950153502999691\n",
      "Iteration: 3690 lambda_n: 0.9538473140603392 Loss: 0.00019507439697360657\n",
      "Iteration: 3691 lambda_n: 0.93470126477908 Loss: 0.00019513311718194002\n",
      "Iteration: 3692 lambda_n: 0.9060207193321037 Loss: 0.0001951901615604163\n",
      "Iteration: 3693 lambda_n: 0.9631081446662849 Loss: 0.00019524498739481008\n",
      "Iteration: 3694 lambda_n: 0.9012440639744976 Loss: 0.00019530279537995237\n",
      "Iteration: 3695 lambda_n: 0.9745892866146464 Loss: 0.00019535641729299393\n",
      "Iteration: 3696 lambda_n: 0.9705499193368422 Loss: 0.000195413937537515\n",
      "Iteration: 3697 lambda_n: 0.9006704670549834 Loss: 0.00019547071732221237\n",
      "Iteration: 3698 lambda_n: 1.035924931299709 Loss: 0.00019552294587333794\n",
      "Iteration: 3699 lambda_n: 1.0294242350104528 Loss: 0.00019558254085296823\n",
      "Iteration: 3700 lambda_n: 0.9243065732896885 Loss: 0.00019564121146065636\n",
      "Iteration: 3701 lambda_n: 0.9638725100117046 Loss: 0.00019569339964369835\n",
      "Iteration: 3702 lambda_n: 0.8974461263949037 Loss: 0.00019574737400877813\n",
      "Iteration: 3703 lambda_n: 0.9719158179074635 Loss: 0.0001957971922664577\n",
      "Iteration: 3704 lambda_n: 1.0183234649351554 Loss: 0.0001958507160271184\n",
      "Iteration: 3705 lambda_n: 0.9793225000846981 Loss: 0.00019590631151881156\n",
      "Iteration: 3706 lambda_n: 0.9441460497563632 Loss: 0.00019595928995807952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3707 lambda_n: 1.0048366693678206 Loss: 0.0001960099178303271\n",
      "Iteration: 3708 lambda_n: 0.9766012482389627 Loss: 0.0001960633501552399\n",
      "Iteration: 3709 lambda_n: 0.9124021126380779 Loss: 0.0001961148150614721\n",
      "Iteration: 3710 lambda_n: 0.9232700192394504 Loss: 0.00019616247609066992\n",
      "Iteration: 3711 lambda_n: 0.9994522512768195 Loss: 0.00019621031411653318\n",
      "Iteration: 3712 lambda_n: 1.0083103232331438 Loss: 0.0001962616782731092\n",
      "Iteration: 3713 lambda_n: 0.9312617976432704 Loss: 0.00019631303800767553\n",
      "Iteration: 3714 lambda_n: 0.9690821842970934 Loss: 0.00019636004511102765\n",
      "Iteration: 3715 lambda_n: 0.9146117740562579 Loss: 0.0001964085587578067\n",
      "Iteration: 3716 lambda_n: 0.9084149592846946 Loss: 0.00019645394969520916\n",
      "Iteration: 3717 lambda_n: 0.882701948577219 Loss: 0.00019649866729395116\n",
      "Iteration: 3718 lambda_n: 0.9468502587040301 Loss: 0.00019654176835029658\n",
      "Iteration: 3719 lambda_n: 1.0319084513154377 Loss: 0.0001965876427951224\n",
      "Iteration: 3720 lambda_n: 0.9630381963784392 Loss: 0.00019663722272589268\n",
      "Iteration: 3721 lambda_n: 1.0225630710265345 Loss: 0.0001966830680326225\n",
      "Iteration: 3722 lambda_n: 0.9672583585807981 Loss: 0.0001967313345447258\n",
      "Iteration: 3723 lambda_n: 0.9073003002378512 Loss: 0.00019677657523618403\n",
      "Iteration: 3724 lambda_n: 0.8843840789175084 Loss: 0.00019681864627379422\n",
      "Iteration: 3725 lambda_n: 0.9438480820371458 Loss: 0.00019685932487217226\n",
      "Iteration: 3726 lambda_n: 1.0244159244757791 Loss: 0.0001969024013091403\n",
      "Iteration: 3727 lambda_n: 0.9476964946360861 Loss: 0.0001969487677725593\n",
      "Iteration: 3728 lambda_n: 1.0235141762073854 Loss: 0.00019699127063052996\n",
      "Iteration: 3729 lambda_n: 0.9772191968485374 Loss: 0.00019703679215956227\n",
      "Iteration: 3730 lambda_n: 1.012807476830226 Loss: 0.0001970798599050513\n",
      "Iteration: 3731 lambda_n: 1.033611937437531 Loss: 0.00019712411192920578\n",
      "Iteration: 3732 lambda_n: 0.9262849653029037 Loss: 0.00019716886954537904\n",
      "Iteration: 3733 lambda_n: 0.928348917092802 Loss: 0.0001972086100133092\n",
      "Iteration: 3734 lambda_n: 0.9122160006657966 Loss: 0.0001972481132422514\n",
      "Iteration: 3735 lambda_n: 0.917492048561793 Loss: 0.00019728661123835428\n",
      "Iteration: 3736 lambda_n: 0.9912858768025338 Loss: 0.00019732502012182411\n",
      "Iteration: 3737 lambda_n: 1.0161258061566079 Loss: 0.00019736618430628666\n",
      "Iteration: 3738 lambda_n: 1.0281389598840123 Loss: 0.00019740801133522306\n",
      "Iteration: 3739 lambda_n: 1.0329404047645734 Loss: 0.00019744995339111908\n",
      "Iteration: 3740 lambda_n: 1.0238706632477619 Loss: 0.00019749170877966725\n",
      "Iteration: 3741 lambda_n: 0.9751905474183551 Loss: 0.00019753271961173383\n",
      "Iteration: 3742 lambda_n: 1.0169203707338643 Loss: 0.00019757142592242652\n",
      "Iteration: 3743 lambda_n: 0.8873934581768138 Loss: 0.0001976114420513319\n",
      "Iteration: 3744 lambda_n: 0.9560721470454815 Loss: 0.00019764604435188796\n",
      "Iteration: 3745 lambda_n: 0.9440418458455668 Loss: 0.0001976830342448737\n",
      "Iteration: 3746 lambda_n: 1.0167209473771466 Loss: 0.00019771924988160402\n",
      "Iteration: 3747 lambda_n: 1.0284766998053263 Loss: 0.0001977579302833369\n",
      "Iteration: 3748 lambda_n: 0.9513549412032312 Loss: 0.00019779670666316492\n",
      "Iteration: 3749 lambda_n: 0.9882213914446706 Loss: 0.0001978322474124335\n",
      "Iteration: 3750 lambda_n: 0.9132805841324471 Loss: 0.00019786885590175187\n",
      "Iteration: 3751 lambda_n: 1.0251112245837186 Loss: 0.00019790239097495469\n",
      "Iteration: 3752 lambda_n: 1.0381820139874613 Loss: 0.00019793973125697043\n",
      "Iteration: 3753 lambda_n: 0.9514147386908107 Loss: 0.00019797720513824136\n",
      "Iteration: 3754 lambda_n: 0.9853725486331204 Loss: 0.00019801122981281393\n",
      "Iteration: 3755 lambda_n: 0.8840197896016792 Loss: 0.00019804617309592877\n",
      "Iteration: 3756 lambda_n: 1.0318387393478645 Loss: 0.00019807724689285407\n",
      "Iteration: 3757 lambda_n: 1.0364546098918963 Loss: 0.00019811323629354626\n",
      "Iteration: 3758 lambda_n: 0.8977593602078201 Loss: 0.00019814905661500994\n",
      "Iteration: 3759 lambda_n: 1.0375842359511365 Loss: 0.00019817979621182427\n",
      "Iteration: 3760 lambda_n: 1.0088093725431568 Loss: 0.00019821504411714628\n",
      "Iteration: 3761 lambda_n: 1.0102498031802254 Loss: 0.00019824899895998685\n",
      "Iteration: 3762 lambda_n: 0.93314953832344 Loss: 0.00019828269842681623\n",
      "Iteration: 3763 lambda_n: 0.9061784883394242 Loss: 0.00019831354594729937\n",
      "Iteration: 3764 lambda_n: 1.0282179200741581 Loss: 0.00019834325367935107\n",
      "Iteration: 3765 lambda_n: 0.9090689413250271 Loss: 0.00019837669393957908\n",
      "Iteration: 3766 lambda_n: 0.9305449018649272 Loss: 0.00019840598757610815\n",
      "Iteration: 3767 lambda_n: 1.003396982968496 Loss: 0.00019843573188263225\n",
      "Iteration: 3768 lambda_n: 1.0051784886902821 Loss: 0.00019846754143910375\n",
      "Iteration: 3769 lambda_n: 0.9614263207048023 Loss: 0.00019849912376760257\n",
      "Iteration: 3770 lambda_n: 0.9162618814529231 Loss: 0.00019852906116474273\n",
      "Iteration: 3771 lambda_n: 0.9016408536031898 Loss: 0.0001985573479681907\n",
      "Iteration: 3772 lambda_n: 0.9903800963996002 Loss: 0.0001985849566931995\n",
      "Iteration: 3773 lambda_n: 0.9993996085422436 Loss: 0.00019861504125224456\n",
      "Iteration: 3774 lambda_n: 0.9767625207407418 Loss: 0.00019864513277961093\n",
      "Iteration: 3775 lambda_n: 1.0189575267848447 Loss: 0.00019867428111020028\n",
      "Iteration: 3776 lambda_n: 0.987156972893656 Loss: 0.00019870442522453398\n",
      "Iteration: 3777 lambda_n: 0.9774163518190734 Loss: 0.00019873336343446084\n",
      "Iteration: 3778 lambda_n: 0.910897638277544 Loss: 0.0001987617643295979\n",
      "Iteration: 3779 lambda_n: 0.889707841064181 Loss: 0.00019878800127135347\n",
      "Iteration: 3780 lambda_n: 1.0335645017383668 Loss: 0.00019881341983272758\n",
      "Iteration: 3781 lambda_n: 0.887207806571233 Loss: 0.0001988427165488537\n",
      "Iteration: 3782 lambda_n: 0.9567296587318799 Loss: 0.00019886763139220407\n",
      "Iteration: 3783 lambda_n: 0.9495920017447042 Loss: 0.00019889428714247765\n",
      "Iteration: 3784 lambda_n: 1.0303483893065166 Loss: 0.0001989205183595222\n",
      "Iteration: 3785 lambda_n: 0.9422664821433899 Loss: 0.00019894874053750293\n",
      "Iteration: 3786 lambda_n: 1.0062356785997286 Loss: 0.00019897431182376062\n",
      "Iteration: 3787 lambda_n: 0.8896135262253618 Loss: 0.00019900139041292988\n",
      "Iteration: 3788 lambda_n: 0.976253151739345 Loss: 0.00019902511433225035\n",
      "Iteration: 3789 lambda_n: 0.8961158637151705 Loss: 0.0001990509430619142\n",
      "Iteration: 3790 lambda_n: 0.9849782289076957 Loss: 0.00019907444405842527\n",
      "Iteration: 3791 lambda_n: 0.9315972897878892 Loss: 0.00019910006981909282\n",
      "Iteration: 3792 lambda_n: 0.9236689816611707 Loss: 0.00019912409290741444\n",
      "Iteration: 3793 lambda_n: 0.9736369375102008 Loss: 0.00019914771315560051\n",
      "Iteration: 3794 lambda_n: 0.9376709581958546 Loss: 0.00019917240615170123\n",
      "Iteration: 3795 lambda_n: 0.932921670965044 Loss: 0.0001991959795458493\n",
      "Iteration: 3796 lambda_n: 0.9034848926543697 Loss: 0.00019921923675560662\n",
      "Iteration: 3797 lambda_n: 0.9867784416087959 Loss: 0.0001992415718042793\n",
      "Iteration: 3798 lambda_n: 0.9828444900412525 Loss: 0.00019926576954022734\n",
      "Iteration: 3799 lambda_n: 0.9390913685495768 Loss: 0.0001992896577961764\n",
      "Iteration: 3800 lambda_n: 0.9036327548478081 Loss: 0.00019931228124906098\n",
      "Iteration: 3801 lambda_n: 0.9579650902635638 Loss: 0.00019933386697467055\n",
      "Iteration: 3802 lambda_n: 0.916050999183076 Loss: 0.00019935656574865203\n",
      "Iteration: 3803 lambda_n: 1.0347814128282233 Loss: 0.00019937808456660703\n",
      "Iteration: 3804 lambda_n: 0.9964731955103335 Loss: 0.0001994021939165486\n",
      "Iteration: 3805 lambda_n: 0.9946976493384282 Loss: 0.000199425194796374\n",
      "Iteration: 3806 lambda_n: 0.9439771821124867 Loss: 0.0001994479493182729\n",
      "Iteration: 3807 lambda_n: 0.8834829689769109 Loss: 0.00019946935027347935\n",
      "Iteration: 3808 lambda_n: 0.9136751409058773 Loss: 0.0001994892094747849\n",
      "Iteration: 3809 lambda_n: 1.0357436734228749 Loss: 0.00019950958457522696\n",
      "Iteration: 3810 lambda_n: 0.9227170874871812 Loss: 0.00019953249324674492\n",
      "Iteration: 3811 lambda_n: 0.9409244336350745 Loss: 0.00019955271106167986\n",
      "Iteration: 3812 lambda_n: 0.923423965959232 Loss: 0.00019957315688361193\n",
      "Iteration: 3813 lambda_n: 0.969771373955459 Loss: 0.00019959305245151164\n",
      "Iteration: 3814 lambda_n: 0.9216916431932624 Loss: 0.00019961377332564865\n",
      "Iteration: 3815 lambda_n: 0.9375134891486675 Loss: 0.00019963329462128974\n",
      "Iteration: 3816 lambda_n: 1.0345554755826625 Loss: 0.00019965298633571382\n",
      "Iteration: 3817 lambda_n: 0.9520397446478046 Loss: 0.00019967453358569725\n",
      "Iteration: 3818 lambda_n: 0.8886604013166429 Loss: 0.0001996941767949615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3819 lambda_n: 0.9847646300358376 Loss: 0.00019971235455747068\n",
      "Iteration: 3820 lambda_n: 0.9600309796218004 Loss: 0.00019973233742951097\n",
      "Iteration: 3821 lambda_n: 0.9595071124274441 Loss: 0.00019975164519965644\n",
      "Iteration: 3822 lambda_n: 0.8893608272753721 Loss: 0.00019977077527602227\n",
      "Iteration: 3823 lambda_n: 0.995536783571843 Loss: 0.0001997883528397153\n",
      "Iteration: 3824 lambda_n: 0.9030230881455465 Loss: 0.00019980787161182667\n",
      "Iteration: 3825 lambda_n: 0.9299632840055478 Loss: 0.00019982541680119275\n",
      "Iteration: 3826 lambda_n: 0.917474562614614 Loss: 0.00019984333818300515\n",
      "Iteration: 3827 lambda_n: 1.0050180611641542 Loss: 0.00019986087023513477\n",
      "Iteration: 3828 lambda_n: 1.0343628607641038 Loss: 0.0001998799164245425\n",
      "Iteration: 3829 lambda_n: 0.9656455612583708 Loss: 0.0001998993407583352\n",
      "Iteration: 3830 lambda_n: 0.9023549803467595 Loss: 0.00019991730457116267\n",
      "Iteration: 3831 lambda_n: 0.9809907018939802 Loss: 0.0001999339439834259\n",
      "Iteration: 3832 lambda_n: 0.8907085868026875 Loss: 0.00019995188613521409\n",
      "Iteration: 3833 lambda_n: 1.00817946625339 Loss: 0.00019996803188790133\n",
      "Iteration: 3834 lambda_n: 0.9721829192002598 Loss: 0.00019998616024999617\n",
      "Iteration: 3835 lambda_n: 0.9227845523499827 Loss: 0.00020000348147788122\n",
      "Iteration: 3836 lambda_n: 0.9699382291183556 Loss: 0.00020001977747949106\n",
      "Iteration: 3837 lambda_n: 1.0251619568921297 Loss: 0.00020003676314546482\n",
      "Iteration: 3838 lambda_n: 0.9117225500581789 Loss: 0.00020005455827465095\n",
      "Iteration: 3839 lambda_n: 0.9095377020385574 Loss: 0.000200070236577986\n",
      "Iteration: 3840 lambda_n: 0.9690910510999042 Loss: 0.0002000857479066343\n",
      "Iteration: 3841 lambda_n: 1.0035998958257588 Loss: 0.00020010213870651813\n",
      "Iteration: 3842 lambda_n: 1.0338880153244998 Loss: 0.00020011896398201034\n",
      "Iteration: 3843 lambda_n: 0.9119061593251317 Loss: 0.000200136139184999\n",
      "Iteration: 3844 lambda_n: 0.8994440389643685 Loss: 0.0002001511451818452\n",
      "Iteration: 3845 lambda_n: 0.8923841592081723 Loss: 0.00020016582339841692\n",
      "Iteration: 3846 lambda_n: 0.9714717914931805 Loss: 0.00020018026730618665\n",
      "Iteration: 3847 lambda_n: 0.9137709162179196 Loss: 0.0002001958640542421\n",
      "Iteration: 3848 lambda_n: 1.0252375010189925 Loss: 0.00020021040457049306\n",
      "Iteration: 3849 lambda_n: 1.0081604540372129 Loss: 0.000200226583666406\n",
      "Iteration: 3850 lambda_n: 0.9723085242905284 Loss: 0.00020024234474594623\n",
      "Iteration: 3851 lambda_n: 0.901784844975632 Loss: 0.0002002574056749078\n",
      "Iteration: 3852 lambda_n: 0.9496354189729961 Loss: 0.00020027125026447336\n",
      "Iteration: 3853 lambda_n: 1.0187891029187992 Loss: 0.0002002857098975363\n",
      "Iteration: 3854 lambda_n: 0.8873382341370512 Loss: 0.00020030108854360092\n",
      "Iteration: 3855 lambda_n: 0.9810143534373083 Loss: 0.0002003143581170915\n",
      "Iteration: 3856 lambda_n: 0.9943002055822989 Loss: 0.0002003289102211816\n",
      "Iteration: 3857 lambda_n: 0.9532288720291762 Loss: 0.00020034352749966885\n",
      "Iteration: 3858 lambda_n: 0.8958704489484863 Loss: 0.00020035741374000538\n",
      "Iteration: 3859 lambda_n: 0.9629874552816837 Loss: 0.00020037035071441498\n",
      "Iteration: 3860 lambda_n: 1.0022079000822615 Loss: 0.00020038414342382242\n",
      "Iteration: 3861 lambda_n: 0.9174224765076714 Loss: 0.0002003983718146547\n",
      "Iteration: 3862 lambda_n: 0.9570836151610457 Loss: 0.00020041127703714147\n",
      "Iteration: 3863 lambda_n: 1.010809350182391 Loss: 0.00020042462746773203\n",
      "Iteration: 3864 lambda_n: 1.0376002464406366 Loss: 0.00020043860419494084\n",
      "Iteration: 3865 lambda_n: 1.0125775171417712 Loss: 0.00020045281890649535\n",
      "Iteration: 3866 lambda_n: 0.9266376541248422 Loss: 0.00020046655914564325\n",
      "Iteration: 3867 lambda_n: 0.9925982582660564 Loss: 0.00020047901654181194\n",
      "Iteration: 3868 lambda_n: 0.97009036707608 Loss: 0.00020049224777917286\n",
      "Iteration: 3869 lambda_n: 0.9855169623276234 Loss: 0.00020050506148381372\n",
      "Iteration: 3870 lambda_n: 0.9820251316587315 Loss: 0.00020051796342499778\n",
      "Iteration: 3871 lambda_n: 0.9312980310058799 Loss: 0.00020053070365476725\n",
      "Iteration: 3872 lambda_n: 0.929333541459104 Loss: 0.00020054267700386576\n",
      "Iteration: 3873 lambda_n: 1.0065177135276968 Loss: 0.0002005545231767353\n",
      "Iteration: 3874 lambda_n: 1.0215129955578914 Loss: 0.0002005672441854538\n",
      "Iteration: 3875 lambda_n: 0.9597316925444059 Loss: 0.00020058003567103197\n",
      "Iteration: 3876 lambda_n: 0.8919375088828767 Loss: 0.0002005919408426599\n",
      "Iteration: 3877 lambda_n: 0.9487299504374651 Loss: 0.0002006029075396256\n",
      "Iteration: 3878 lambda_n: 0.9622127118340326 Loss: 0.0002006144772402691\n",
      "Iteration: 3879 lambda_n: 0.97869634892101 Loss: 0.000200626109277116\n",
      "Iteration: 3880 lambda_n: 0.9334930413599722 Loss: 0.00020063783616871818\n",
      "Iteration: 3881 lambda_n: 0.9750707060734068 Loss: 0.00020064892086385443\n",
      "Iteration: 3882 lambda_n: 0.968489783047159 Loss: 0.00020066040014813494\n",
      "Iteration: 3883 lambda_n: 1.0330599261746245 Loss: 0.0002006716998631507\n",
      "Iteration: 3884 lambda_n: 0.9255845218682445 Loss: 0.00020068364588103852\n",
      "Iteration: 3885 lambda_n: 1.030727093744381 Loss: 0.0002006942472924433\n",
      "Iteration: 3886 lambda_n: 1.015154021474502 Loss: 0.00020070595280928205\n",
      "Iteration: 3887 lambda_n: 0.9749363376906238 Loss: 0.00020071737222258362\n",
      "Iteration: 3888 lambda_n: 1.0298575477821044 Loss: 0.00020072823680124824\n",
      "Iteration: 3889 lambda_n: 0.906156418867467 Loss: 0.00020073961064589847\n",
      "Iteration: 3890 lambda_n: 0.9168771576130115 Loss: 0.0002007495233097465\n",
      "Iteration: 3891 lambda_n: 1.0322276374315837 Loss: 0.00020075946966212295\n",
      "Iteration: 3892 lambda_n: 0.9895084122137844 Loss: 0.00020077057310524996\n",
      "Iteration: 3893 lambda_n: 0.9457009635213522 Loss: 0.00020078111582931394\n",
      "Iteration: 3894 lambda_n: 1.0282367777599515 Loss: 0.0002007910999493707\n",
      "Iteration: 3895 lambda_n: 0.8923043088515424 Loss: 0.0002008018610599048\n",
      "Iteration: 3896 lambda_n: 0.9819512051388831 Loss: 0.00020081111090895835\n",
      "Iteration: 3897 lambda_n: 0.9158590439561393 Loss: 0.00020082120654780887\n",
      "Iteration: 3898 lambda_n: 1.0166139488487713 Loss: 0.0002008305373875249\n",
      "Iteration: 3899 lambda_n: 0.9170715402089835 Loss: 0.00020084080748278887\n",
      "Iteration: 3900 lambda_n: 0.9746784595292648 Loss: 0.00020084998501160485\n",
      "Iteration: 3901 lambda_n: 0.8972198798522406 Loss: 0.00020085965666211093\n",
      "Iteration: 3902 lambda_n: 0.9775352272479594 Loss: 0.00020086847957071088\n",
      "Iteration: 3903 lambda_n: 0.8859841790560575 Loss: 0.00020087801284751885\n",
      "Iteration: 3904 lambda_n: 0.9830375731267127 Loss: 0.00020088657524581368\n",
      "Iteration: 3905 lambda_n: 0.9144739177202925 Loss: 0.0002008959980784926\n",
      "Iteration: 3906 lambda_n: 0.995048831856793 Loss: 0.00020090468408608188\n",
      "Iteration: 3907 lambda_n: 0.9296062470653722 Loss: 0.00020091405576397244\n",
      "Iteration: 3908 lambda_n: 0.9947792063284151 Loss: 0.00020092273056694337\n",
      "Iteration: 3909 lambda_n: 1.023641119509203 Loss: 0.00020093193395163527\n",
      "Iteration: 3910 lambda_n: 0.9417453113355513 Loss: 0.0002009413173881822\n",
      "Iteration: 3911 lambda_n: 0.9985613976786438 Loss: 0.00020094986837304807\n",
      "Iteration: 3912 lambda_n: 0.9742663681103311 Loss: 0.00020095885642755281\n",
      "Iteration: 3913 lambda_n: 1.0070560758675193 Loss: 0.00020096754484866426\n",
      "Iteration: 3914 lambda_n: 0.9516175600623674 Loss: 0.00020097644485408167\n",
      "Iteration: 3915 lambda_n: 0.998097816558051 Loss: 0.00020098477654988472\n",
      "Iteration: 3916 lambda_n: 0.9463825895211513 Loss: 0.00020099343835918403\n",
      "Iteration: 3917 lambda_n: 1.0134455386283958 Loss: 0.00020100157549723946\n",
      "Iteration: 3918 lambda_n: 0.9450511731434081 Loss: 0.00020101021305341714\n",
      "Iteration: 3919 lambda_n: 0.9279687605095401 Loss: 0.0002010181920915327\n",
      "Iteration: 3920 lambda_n: 0.9128827471866019 Loss: 0.00020102595837202355\n",
      "Iteration: 3921 lambda_n: 1.0022370496238204 Loss: 0.00020103353276721423\n",
      "Iteration: 3922 lambda_n: 0.8907926299989138 Loss: 0.00020104177838575768\n",
      "Iteration: 3923 lambda_n: 0.9433679956399836 Loss: 0.00020104903901856227\n",
      "Iteration: 3924 lambda_n: 0.9753266332175332 Loss: 0.00020105666481089104\n",
      "Iteration: 3925 lambda_n: 0.9904013992150019 Loss: 0.00020106448009533261\n",
      "Iteration: 3926 lambda_n: 0.8929430130026073 Loss: 0.00020107234449290702\n",
      "Iteration: 3927 lambda_n: 0.9289631878880295 Loss: 0.0002010793698654414\n",
      "Iteration: 3928 lambda_n: 0.8954400514800472 Loss: 0.00020108661819499688\n",
      "Iteration: 3929 lambda_n: 0.9993980291620748 Loss: 0.00020109354477993065\n",
      "Iteration: 3930 lambda_n: 0.9114235283236433 Loss: 0.00020110121145885748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3931 lambda_n: 0.9510305739494664 Loss: 0.00020110813840772576\n",
      "Iteration: 3932 lambda_n: 0.9179521339932857 Loss: 0.0002011153053315499\n",
      "Iteration: 3933 lambda_n: 0.885339190786132 Loss: 0.0002011221619437235\n",
      "Iteration: 3934 lambda_n: 0.9502494065161281 Loss: 0.00020112871862935602\n",
      "Iteration: 3935 lambda_n: 1.0137410501863307 Loss: 0.0002011356982910114\n",
      "Iteration: 3936 lambda_n: 1.0086928528853145 Loss: 0.00020114307871666324\n",
      "Iteration: 3937 lambda_n: 0.9678647318910886 Loss: 0.00020115035330746562\n",
      "Iteration: 3938 lambda_n: 0.9131863989045427 Loss: 0.00020115726807593855\n",
      "Iteration: 3939 lambda_n: 0.9484400332248927 Loss: 0.00020116373355353755\n",
      "Iteration: 3940 lambda_n: 0.9564982923374147 Loss: 0.0002011703917333402\n",
      "Iteration: 3941 lambda_n: 0.9134513100527564 Loss: 0.0002011770473609758\n",
      "Iteration: 3942 lambda_n: 0.9114481451223965 Loss: 0.00020118334696983452\n",
      "Iteration: 3943 lambda_n: 0.9517376778475036 Loss: 0.00020118957943852803\n",
      "Iteration: 3944 lambda_n: 0.9897613967129871 Loss: 0.00020119603233937668\n",
      "Iteration: 3945 lambda_n: 1.0326694754037697 Loss: 0.00020120268374115773\n",
      "Iteration: 3946 lambda_n: 0.9327734742647334 Loss: 0.00020120955970740436\n",
      "Iteration: 3947 lambda_n: 0.984320411926246 Loss: 0.0002012157108573844\n",
      "Iteration: 3948 lambda_n: 1.0270330613463572 Loss: 0.00020122214569615125\n",
      "Iteration: 3949 lambda_n: 0.8987047338688389 Loss: 0.00020122879836186747\n",
      "Iteration: 3950 lambda_n: 0.9560173817398 Loss: 0.00020123456411574685\n",
      "Iteration: 3951 lambda_n: 0.9137240353170032 Loss: 0.0002012406463519657\n",
      "Iteration: 3952 lambda_n: 1.0056039147639342 Loss: 0.00020124640781271947\n",
      "Iteration: 3953 lambda_n: 1.0033773583960899 Loss: 0.00020125269479504739\n",
      "Iteration: 3954 lambda_n: 0.8833970665457459 Loss: 0.0002012589091803071\n",
      "Iteration: 3955 lambda_n: 1.0256565460857459 Loss: 0.00020126432933846092\n",
      "Iteration: 3956 lambda_n: 1.0091737026091563 Loss: 0.00020127057070673876\n",
      "Iteration: 3957 lambda_n: 1.013294420679934 Loss: 0.0002012766531539049\n",
      "Iteration: 3958 lambda_n: 0.9990918999263838 Loss: 0.00020128270308183844\n",
      "Iteration: 3959 lambda_n: 0.9349082913236282 Loss: 0.0002012886119459562\n",
      "Iteration: 3960 lambda_n: 0.8924833462014555 Loss: 0.0002012940897565814\n",
      "Iteration: 3961 lambda_n: 0.9661065567641466 Loss: 0.0002012992734567372\n",
      "Iteration: 3962 lambda_n: 0.9544781101272632 Loss: 0.00020130483818236563\n",
      "Iteration: 3963 lambda_n: 0.8997182342904881 Loss: 0.000201310286463025\n",
      "Iteration: 3964 lambda_n: 0.9098080741935863 Loss: 0.00020131537649035843\n",
      "Iteration: 3965 lambda_n: 0.9268887360049368 Loss: 0.0002013204804711019\n",
      "Iteration: 3966 lambda_n: 0.8941464848364182 Loss: 0.0002013256362130817\n",
      "Iteration: 3967 lambda_n: 0.9616817177414002 Loss: 0.00020133056686733725\n",
      "Iteration: 3968 lambda_n: 0.9465378943237326 Loss: 0.00020133582578717228\n",
      "Iteration: 3969 lambda_n: 0.9966553620423743 Loss: 0.0002013409555025278\n",
      "Iteration: 3970 lambda_n: 0.9388134732381818 Loss: 0.00020134630920415236\n",
      "Iteration: 3971 lambda_n: 0.9675741547130278 Loss: 0.00020135130532809614\n",
      "Iteration: 3972 lambda_n: 0.9187099124347253 Loss: 0.00020135640946034855\n",
      "Iteration: 3973 lambda_n: 0.9406553886635853 Loss: 0.00020136121209018087\n",
      "Iteration: 3974 lambda_n: 0.9501485365513828 Loss: 0.00020136608733092054\n",
      "Iteration: 3975 lambda_n: 1.0227719900158578 Loss: 0.00020137096858407745\n",
      "Iteration: 3976 lambda_n: 0.9308088010419862 Loss: 0.000201376176404049\n",
      "Iteration: 3977 lambda_n: 0.9694218416142675 Loss: 0.00020138087071507006\n",
      "Iteration: 3978 lambda_n: 1.0238922933016148 Loss: 0.00020138571732974345\n",
      "Iteration: 3979 lambda_n: 0.9281851850145911 Loss: 0.00020139078999913232\n",
      "Iteration: 3980 lambda_n: 0.9887345244527143 Loss: 0.00020139534454652303\n",
      "Iteration: 3981 lambda_n: 1.0267528383263842 Loss: 0.00020140015421167047\n",
      "Iteration: 3982 lambda_n: 0.911575158847648 Loss: 0.00020140510274878972\n",
      "Iteration: 3983 lambda_n: 0.9888428363324033 Loss: 0.00020140945404028686\n",
      "Iteration: 3984 lambda_n: 1.0105064929119367 Loss: 0.00020141413402864814\n",
      "Iteration: 3985 lambda_n: 0.972827029998058 Loss: 0.0002014188724118607\n",
      "Iteration: 3986 lambda_n: 0.9378430156158396 Loss: 0.00020142339106819795\n",
      "Iteration: 3987 lambda_n: 0.9031284865387527 Loss: 0.0002014277076538531\n",
      "Iteration: 3988 lambda_n: 0.9576525723883148 Loss: 0.00020143182805115926\n",
      "Iteration: 3989 lambda_n: 0.9796872528358004 Loss: 0.00020143616037890594\n",
      "Iteration: 3990 lambda_n: 0.9938921583246284 Loss: 0.0002014405527610562\n",
      "Iteration: 3991 lambda_n: 0.9532315989224003 Loss: 0.00020144496806348523\n",
      "Iteration: 3992 lambda_n: 0.9015116181808315 Loss: 0.00020144916341134164\n",
      "Iteration: 3993 lambda_n: 1.0277593981990272 Loss: 0.00020145309578728973\n",
      "Iteration: 3994 lambda_n: 0.9655149250404772 Loss: 0.00020145754113698455\n",
      "Iteration: 3995 lambda_n: 0.9258285888838994 Loss: 0.00020146167714540138\n",
      "Iteration: 3996 lambda_n: 0.9312715920129825 Loss: 0.00020146560735978886\n",
      "Iteration: 3997 lambda_n: 1.0257046108294456 Loss: 0.00020146952648124105\n",
      "Iteration: 3998 lambda_n: 0.9530956121445912 Loss: 0.0002014738054716471\n",
      "Iteration: 3999 lambda_n: 0.9695238330609719 Loss: 0.0002014777434216903\n",
      "Iteration: 4000 lambda_n: 0.9639270090515656 Loss: 0.0002014817135689212\n",
      "Iteration: 4001 lambda_n: 0.9476206372926295 Loss: 0.0002014856250248167\n",
      "Iteration: 4002 lambda_n: 0.9633041804721018 Loss: 0.0002014894356592099\n",
      "Iteration: 4003 lambda_n: 0.9716539275345255 Loss: 0.0002014932750478142\n",
      "Iteration: 4004 lambda_n: 1.014445037902442 Loss: 0.00020149711283887588\n",
      "Iteration: 4005 lambda_n: 0.9544740857090143 Loss: 0.00020150108325212506\n",
      "Iteration: 4006 lambda_n: 0.9542061935580833 Loss: 0.00020150478349600806\n",
      "Iteration: 4007 lambda_n: 0.8922897653385831 Loss: 0.0002015084496826149\n",
      "Iteration: 4008 lambda_n: 0.9435142090349362 Loss: 0.00020151184737142412\n",
      "Iteration: 4009 lambda_n: 1.0146145845702728 Loss: 0.00020151541014182172\n",
      "Iteration: 4010 lambda_n: 0.9856701380782315 Loss: 0.0002015192075957499\n",
      "Iteration: 4011 lambda_n: 1.0116637670682584 Loss: 0.00020152286169780657\n",
      "Iteration: 4012 lambda_n: 0.900385407180602 Loss: 0.00020152657758665946\n",
      "Iteration: 4013 lambda_n: 1.0344900660293732 Loss: 0.00020152985342240673\n",
      "Iteration: 4014 lambda_n: 0.9680571631667578 Loss: 0.00020153358548711956\n",
      "Iteration: 4015 lambda_n: 0.9509423605755882 Loss: 0.00020153704406760583\n",
      "Iteration: 4016 lambda_n: 0.9871482092609662 Loss: 0.0002015404107224195\n",
      "Iteration: 4017 lambda_n: 1.0295487622043502 Loss: 0.0002015438744638741\n",
      "Iteration: 4018 lambda_n: 0.8844836012987323 Loss: 0.0002015474536154458\n",
      "Iteration: 4019 lambda_n: 1.0336994478566315 Loss: 0.00020155049880526523\n",
      "Iteration: 4020 lambda_n: 0.9981900773893461 Loss: 0.0002015540282949165\n",
      "Iteration: 4021 lambda_n: 0.9320958438100541 Loss: 0.00020155740355524114\n",
      "Iteration: 4022 lambda_n: 0.9815201993058934 Loss: 0.00020156052586370828\n",
      "Iteration: 4023 lambda_n: 0.9584025149816774 Loss: 0.00020156378505039043\n",
      "Iteration: 4024 lambda_n: 0.927147990621769 Loss: 0.00020156693822523877\n",
      "Iteration: 4025 lambda_n: 0.9443897528030284 Loss: 0.00020156996119456225\n",
      "Iteration: 4026 lambda_n: 1.0335372686400166 Loss: 0.00020157301365182192\n",
      "Iteration: 4027 lambda_n: 1.00386582224883 Loss: 0.00020157632472436507\n",
      "Iteration: 4028 lambda_n: 0.9845567812999114 Loss: 0.00020157950960879471\n",
      "Iteration: 4029 lambda_n: 1.026185528975636 Loss: 0.0002015826038634952\n",
      "Iteration: 4030 lambda_n: 1.017140071719077 Loss: 0.0002015857992160585\n",
      "Iteration: 4031 lambda_n: 0.9484941460419194 Loss: 0.0002015889359596161\n",
      "Iteration: 4032 lambda_n: 0.975138984665331 Loss: 0.0002015918331294615\n",
      "Iteration: 4033 lambda_n: 0.9596587829456769 Loss: 0.00020159478522549534\n",
      "Iteration: 4034 lambda_n: 0.936390107274936 Loss: 0.00020159766391621148\n",
      "Iteration: 4035 lambda_n: 0.892998438192795 Loss: 0.0002016004475518777\n",
      "Iteration: 4036 lambda_n: 1.0325453757319971 Loss: 0.00020160307890191616\n",
      "Iteration: 4037 lambda_n: 0.9313775073774372 Loss: 0.00020160609601041027\n",
      "Iteration: 4038 lambda_n: 0.9510940594079967 Loss: 0.000201608791163482\n",
      "Iteration: 4039 lambda_n: 0.8938931050764265 Loss: 0.00020161151935441012\n",
      "Iteration: 4040 lambda_n: 0.9005665808863434 Loss: 0.000201614060607188\n",
      "Iteration: 4041 lambda_n: 0.9651132786630645 Loss: 0.00020161659938667793\n",
      "Iteration: 4042 lambda_n: 0.9175370605193729 Loss: 0.00020161929717481776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4043 lambda_n: 0.9792472512152929 Loss: 0.0002016218387692854\n",
      "Iteration: 4044 lambda_n: 0.9691154249983066 Loss: 0.00020162452798304\n",
      "Iteration: 4045 lambda_n: 1.0039138890563333 Loss: 0.0002016271649447942\n",
      "Iteration: 4046 lambda_n: 0.9394815389921539 Loss: 0.00020162987178372986\n",
      "Iteration: 4047 lambda_n: 1.0169070578137351 Loss: 0.00020163238105073628\n",
      "Iteration: 4048 lambda_n: 0.9736414840049389 Loss: 0.00020163507320315678\n",
      "Iteration: 4049 lambda_n: 0.9676755139203074 Loss: 0.00020163762623811305\n",
      "Iteration: 4050 lambda_n: 0.8974981598216039 Loss: 0.00020164014046835648\n",
      "Iteration: 4051 lambda_n: 0.9888120607406646 Loss: 0.0002016424512011211\n",
      "Iteration: 4052 lambda_n: 0.9523135202126443 Loss: 0.00020164497562138783\n",
      "Iteration: 4053 lambda_n: 0.9745080500521328 Loss: 0.0002016473843181234\n",
      "Iteration: 4054 lambda_n: 0.9291820804506166 Loss: 0.00020164982714509998\n",
      "Iteration: 4055 lambda_n: 0.9926163086959798 Loss: 0.0002016521350649034\n",
      "Iteration: 4056 lambda_n: 0.9893935017470356 Loss: 0.000201654579068351\n",
      "Iteration: 4057 lambda_n: 1.0303554719652042 Loss: 0.00020165699246130374\n",
      "Iteration: 4058 lambda_n: 0.9489107216012865 Loss: 0.00020165948245620137\n",
      "Iteration: 4059 lambda_n: 0.9086893915301528 Loss: 0.0002016617534637077\n",
      "Iteration: 4060 lambda_n: 0.9801748859193831 Loss: 0.0002016639088538221\n",
      "Iteration: 4061 lambda_n: 0.9497090728228665 Loss: 0.00020166621399809524\n",
      "Iteration: 4062 lambda_n: 0.9854012957726451 Loss: 0.00020166842695889956\n",
      "Iteration: 4063 lambda_n: 1.0358741944735024 Loss: 0.00020167070263782557\n",
      "Iteration: 4064 lambda_n: 0.9483474420953769 Loss: 0.0002016730727721119\n",
      "Iteration: 4065 lambda_n: 0.9878029440526369 Loss: 0.00020167522155078402\n",
      "Iteration: 4066 lambda_n: 0.8916709078163454 Loss: 0.00020167743982130423\n",
      "Iteration: 4067 lambda_n: 0.9305905002387905 Loss: 0.00020167942365114294\n",
      "Iteration: 4068 lambda_n: 1.000745947292015 Loss: 0.00020168147675583755\n",
      "Iteration: 4069 lambda_n: 0.9306994788008717 Loss: 0.00020168366537089117\n",
      "Iteration: 4070 lambda_n: 0.9114642842265432 Loss: 0.00020168568168167194\n",
      "Iteration: 4071 lambda_n: 0.9512152877742149 Loss: 0.00020168763907799293\n",
      "Iteration: 4072 lambda_n: 0.8886410597376735 Loss: 0.00020168966437553214\n",
      "Iteration: 4073 lambda_n: 0.8848425969374877 Loss: 0.000201691539552864\n",
      "Iteration: 4074 lambda_n: 0.9916974779054297 Loss: 0.0002016933911473545\n",
      "Iteration: 4075 lambda_n: 1.0072204270363343 Loss: 0.00020169544912190525\n",
      "Iteration: 4076 lambda_n: 0.9021089863415465 Loss: 0.00020169751986197213\n",
      "Iteration: 4077 lambda_n: 0.9435353962221538 Loss: 0.00020169935697021557\n",
      "Iteration: 4078 lambda_n: 0.9034724502537007 Loss: 0.00020170126217964202\n",
      "Iteration: 4079 lambda_n: 0.9176663000542363 Loss: 0.00020170307033918534\n",
      "Iteration: 4080 lambda_n: 0.996420210999582 Loss: 0.00020170489133634778\n",
      "Iteration: 4081 lambda_n: 1.0198072605176325 Loss: 0.00020170685158909631\n",
      "Iteration: 4082 lambda_n: 0.9760179848448545 Loss: 0.0002017088390930756\n",
      "Iteration: 4083 lambda_n: 1.0285323395203136 Loss: 0.00020171072304977543\n",
      "Iteration: 4084 lambda_n: 1.0157470960232815 Loss: 0.00020171269019095606\n",
      "Iteration: 4085 lambda_n: 0.9488368717138527 Loss: 0.00020171461412710838\n",
      "Iteration: 4086 lambda_n: 0.9085260920951855 Loss: 0.0002017163941925869\n",
      "Iteration: 4087 lambda_n: 0.9299353959695501 Loss: 0.00020171808345344157\n",
      "Iteration: 4088 lambda_n: 1.0334192917752223 Loss: 0.00020171979777942067\n",
      "Iteration: 4089 lambda_n: 0.9336993794059961 Loss: 0.00020172168625517877\n",
      "Iteration: 4090 lambda_n: 0.8973129243436522 Loss: 0.00020172337594841767\n",
      "Iteration: 4091 lambda_n: 0.986489963446114 Loss: 0.00020172498556206216\n",
      "Iteration: 4092 lambda_n: 0.8921556753769501 Loss: 0.0002017267402437924\n",
      "Iteration: 4093 lambda_n: 1.0101372270801856 Loss: 0.00020172831243475708\n",
      "Iteration: 4094 lambda_n: 0.9455392873558709 Loss: 0.00020173007763666642\n",
      "Iteration: 4095 lambda_n: 1.0061947395449375 Loss: 0.00020173171428545627\n",
      "Iteration: 4096 lambda_n: 0.9944810555147934 Loss: 0.0002017334404691082\n",
      "Iteration: 4097 lambda_n: 1.0306601442582448 Loss: 0.00020173513044333013\n",
      "Iteration: 4098 lambda_n: 0.9835308028051012 Loss: 0.00020173686555067721\n",
      "Iteration: 4099 lambda_n: 0.9191690581015872 Loss: 0.00020173850529538407\n",
      "Iteration: 4100 lambda_n: 1.0053346976883535 Loss: 0.00020174002358566218\n",
      "Iteration: 4101 lambda_n: 0.9753730000795974 Loss: 0.0002017416698803437\n",
      "Iteration: 4102 lambda_n: 1.0176960502686239 Loss: 0.00020174325203644228\n",
      "Iteration: 4103 lambda_n: 0.8841494788531598 Loss: 0.0002017448877317541\n",
      "Iteration: 4104 lambda_n: 0.9916098799610155 Loss: 0.00020174629520341387\n",
      "Iteration: 4105 lambda_n: 0.9747724797661902 Loss: 0.0002017478606430079\n",
      "Iteration: 4106 lambda_n: 1.0175735088158864 Loss: 0.00020174938517621873\n",
      "Iteration: 4107 lambda_n: 0.9390357636410293 Loss: 0.0002017509620881424\n",
      "Iteration: 4108 lambda_n: 1.0351270796742922 Loss: 0.00020175240338810355\n",
      "Iteration: 4109 lambda_n: 0.9548109848288212 Loss: 0.00020175397817362987\n",
      "Iteration: 4110 lambda_n: 1.002256959736275 Loss: 0.00020175541665223495\n",
      "Iteration: 4111 lambda_n: 0.9169617329161122 Loss: 0.0002017569130778504\n",
      "Iteration: 4112 lambda_n: 0.9940661993713348 Loss: 0.00020175826926852327\n",
      "Iteration: 4113 lambda_n: 0.8997191083968957 Loss: 0.00020175972684317016\n",
      "Iteration: 4114 lambda_n: 0.9918809044623913 Loss: 0.00020176103376473348\n",
      "Iteration: 4115 lambda_n: 0.9704530617408066 Loss: 0.00020176246239244543\n",
      "Iteration: 4116 lambda_n: 0.9122453588745094 Loss: 0.00020176384714062627\n",
      "Iteration: 4117 lambda_n: 0.9089673250587549 Loss: 0.0002017651369706455\n",
      "Iteration: 4118 lambda_n: 1.0334005267610311 Loss: 0.00020176641115888526\n",
      "Iteration: 4119 lambda_n: 0.9219637830520891 Loss: 0.00020176784741908504\n",
      "Iteration: 4120 lambda_n: 0.9512720795773946 Loss: 0.0002017691163653775\n",
      "Iteration: 4121 lambda_n: 0.94102077083492 Loss: 0.0002017704143182369\n",
      "Iteration: 4122 lambda_n: 0.9933866402604182 Loss: 0.00020177168681660853\n",
      "Iteration: 4123 lambda_n: 0.9641341577025714 Loss: 0.00020177301826061988\n",
      "Iteration: 4124 lambda_n: 1.0335492332845404 Loss: 0.00020177429844474916\n",
      "Iteration: 4125 lambda_n: 0.9387158424923737 Loss: 0.0002017756583784578\n",
      "Iteration: 4126 lambda_n: 0.9017856940186603 Loss: 0.0002017768815438659\n",
      "Iteration: 4127 lambda_n: 0.9667289079993132 Loss: 0.00020177804623201835\n",
      "Iteration: 4128 lambda_n: 0.9351508434930836 Loss: 0.00020177928422743477\n",
      "Iteration: 4129 lambda_n: 0.9916608981928331 Loss: 0.00020178047091413067\n",
      "Iteration: 4130 lambda_n: 1.0043837239545994 Loss: 0.00020178171826404313\n",
      "Iteration: 4131 lambda_n: 0.9419615856197612 Loss: 0.0002017829698555796\n",
      "Iteration: 4132 lambda_n: 0.8987548301014134 Loss: 0.00020178413259128546\n",
      "Iteration: 4133 lambda_n: 0.9713322314743962 Loss: 0.00020178523218177666\n",
      "Iteration: 4134 lambda_n: 0.9315100229106871 Loss: 0.00020178641054200786\n",
      "Iteration: 4135 lambda_n: 0.958173033036703 Loss: 0.0002017875302864991\n",
      "Iteration: 4136 lambda_n: 0.996070029573383 Loss: 0.0002017886720097033\n",
      "Iteration: 4137 lambda_n: 0.8989755286282229 Loss: 0.00020178984821370914\n",
      "Iteration: 4138 lambda_n: 0.955275830603579 Loss: 0.00020179089983550827\n",
      "Iteration: 4139 lambda_n: 0.9469514670395505 Loss: 0.00020179200788725756\n",
      "Iteration: 4140 lambda_n: 0.9735071136986531 Loss: 0.0002017930964325441\n",
      "Iteration: 4141 lambda_n: 0.978392590347723 Loss: 0.0002017942055561576\n",
      "Iteration: 4142 lambda_n: 0.9635149089146756 Loss: 0.00020179531005843826\n",
      "Iteration: 4143 lambda_n: 0.9961764277941545 Loss: 0.00020179638777439229\n",
      "Iteration: 4144 lambda_n: 0.9837466804523906 Loss: 0.0002017974919449046\n",
      "Iteration: 4145 lambda_n: 0.948511486020774 Loss: 0.00020179857214063329\n",
      "Iteration: 4146 lambda_n: 0.8912447585759241 Loss: 0.00020179960402751882\n",
      "Iteration: 4147 lambda_n: 1.034747046890386 Loss: 0.00020180056497948583\n",
      "Iteration: 4148 lambda_n: 0.9355713889859482 Loss: 0.00020180167132557444\n",
      "Iteration: 4149 lambda_n: 1.0229178906459664 Loss: 0.0002018026619152651\n",
      "Iteration: 4150 lambda_n: 0.974504529247039 Loss: 0.00020180373547729635\n",
      "Iteration: 4151 lambda_n: 0.9069371879696514 Loss: 0.00020180474840727952\n",
      "Iteration: 4152 lambda_n: 0.986147570108153 Loss: 0.00020180568248096556\n",
      "Iteration: 4153 lambda_n: 0.8918552153455369 Loss: 0.00020180668948944493\n",
      "Iteration: 4154 lambda_n: 0.9876247770717145 Loss: 0.00020180759177901072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4155 lambda_n: 0.9920063936538046 Loss: 0.0002018085825950855\n",
      "Iteration: 4156 lambda_n: 0.9932809687427326 Loss: 0.0002018095685806164\n",
      "Iteration: 4157 lambda_n: 1.0121981799935371 Loss: 0.0002018105466399419\n",
      "Iteration: 4158 lambda_n: 0.8899455384228051 Loss: 0.00020181153403410034\n",
      "Iteration: 4159 lambda_n: 0.978332406295698 Loss: 0.00020181239392167504\n",
      "Iteration: 4160 lambda_n: 0.95456094116085 Loss: 0.00020181333131563987\n",
      "Iteration: 4161 lambda_n: 1.0199313794170473 Loss: 0.00020181423753363762\n",
      "Iteration: 4162 lambda_n: 0.9623246778783039 Loss: 0.00020181519713683238\n",
      "Iteration: 4163 lambda_n: 0.9377935347992108 Loss: 0.00020181609387240283\n",
      "Iteration: 4164 lambda_n: 0.9629386422654708 Loss: 0.00020181695985532246\n",
      "Iteration: 4165 lambda_n: 0.8992834817206826 Loss: 0.00020181784123147223\n",
      "Iteration: 4166 lambda_n: 0.957746914178404 Loss: 0.00020181865690427556\n",
      "Iteration: 4167 lambda_n: 0.9340587629012989 Loss: 0.00020181951827350452\n",
      "Iteration: 4168 lambda_n: 0.9717478206653634 Loss: 0.000201820350786822\n",
      "Iteration: 4169 lambda_n: 1.0263361002432247 Loss: 0.00020182120929965498\n",
      "Iteration: 4170 lambda_n: 1.0215990193590427 Loss: 0.00020182210777091193\n",
      "Iteration: 4171 lambda_n: 0.9292431426155807 Loss: 0.0002018229934808975\n",
      "Iteration: 4172 lambda_n: 1.018032103118255 Loss: 0.00020182379139476104\n",
      "Iteration: 4173 lambda_n: 0.9491925790120773 Loss: 0.00020182465792690002\n",
      "Iteration: 4174 lambda_n: 0.9633916228829242 Loss: 0.00020182545814442782\n",
      "Iteration: 4175 lambda_n: 0.9661613625916783 Loss: 0.00020182626309799923\n",
      "Iteration: 4176 lambda_n: 0.887428642972535 Loss: 0.0002018270630676281\n",
      "Iteration: 4177 lambda_n: 1.0120246990369253 Loss: 0.00020182779118496283\n",
      "Iteration: 4178 lambda_n: 0.9491803410585233 Loss: 0.00020182861461741743\n",
      "Iteration: 4179 lambda_n: 0.8902259962937943 Loss: 0.0002018293795818896\n",
      "Iteration: 4180 lambda_n: 0.9116899301930572 Loss: 0.00020183009064323747\n",
      "Iteration: 4181 lambda_n: 0.923049059497198 Loss: 0.00020183081276604366\n",
      "Iteration: 4182 lambda_n: 0.9224134832682536 Loss: 0.00020183153763180819\n",
      "Iteration: 4183 lambda_n: 0.9929224876463799 Loss: 0.0002018322557247305\n",
      "Iteration: 4184 lambda_n: 0.8941494334076819 Loss: 0.00020183302201907193\n",
      "Iteration: 4185 lambda_n: 0.9604495248167297 Loss: 0.00020183370565509897\n",
      "Iteration: 4186 lambda_n: 0.9206945187811535 Loss: 0.0002018344338219568\n",
      "Iteration: 4187 lambda_n: 1.0158132732812395 Loss: 0.0002018351255581599\n",
      "Iteration: 4188 lambda_n: 0.9032505673375503 Loss: 0.00020183588216726297\n",
      "Iteration: 4189 lambda_n: 0.8863931406890975 Loss: 0.00020183654852371668\n",
      "Iteration: 4190 lambda_n: 0.9859340130799013 Loss: 0.00020183719690251422\n",
      "Iteration: 4191 lambda_n: 0.8981626422292522 Loss: 0.00020183791209706493\n",
      "Iteration: 4192 lambda_n: 0.9861238343055512 Loss: 0.0002018385575956999\n",
      "Iteration: 4193 lambda_n: 0.8918740379793563 Loss: 0.00020183926034022867\n",
      "Iteration: 4194 lambda_n: 0.9576387503993823 Loss: 0.00020183989003909152\n",
      "Iteration: 4195 lambda_n: 0.9126413815928768 Loss: 0.00020184056051421553\n",
      "Iteration: 4196 lambda_n: 0.9854023369492804 Loss: 0.00020184119374498598\n",
      "Iteration: 4197 lambda_n: 0.9058624694676624 Loss: 0.00020184187160791447\n",
      "Iteration: 4198 lambda_n: 0.9361623776840365 Loss: 0.00020184248899472794\n",
      "Iteration: 4199 lambda_n: 0.9506562511787795 Loss: 0.00020184312161127205\n",
      "Iteration: 4200 lambda_n: 0.9035669100096345 Loss: 0.00020184375838137414\n",
      "Iteration: 4201 lambda_n: 0.9437439325892458 Loss: 0.00020184435821325437\n",
      "Iteration: 4202 lambda_n: 0.9773539220856587 Loss: 0.00020184497940748257\n",
      "Iteration: 4203 lambda_n: 0.985235857006521 Loss: 0.00020184561703069453\n",
      "Iteration: 4204 lambda_n: 1.0162956528387983 Loss: 0.00020184625390437844\n",
      "Iteration: 4205 lambda_n: 1.0265860830217506 Loss: 0.0002018469047857097\n",
      "Iteration: 4206 lambda_n: 0.9018085631749173 Loss: 0.00020184755599123909\n",
      "Iteration: 4207 lambda_n: 0.984534254071645 Loss: 0.00020184812253738984\n",
      "Iteration: 4208 lambda_n: 1.0251329777947242 Loss: 0.00020184873582439463\n",
      "Iteration: 4209 lambda_n: 0.9063235606551271 Loss: 0.00020184936850590432\n",
      "Iteration: 4210 lambda_n: 1.0116053219904555 Loss: 0.00020184992248425732\n",
      "Iteration: 4211 lambda_n: 0.8864276850925522 Loss: 0.0002018505355604836\n",
      "Iteration: 4212 lambda_n: 0.9244042839592883 Loss: 0.00020185106767744586\n",
      "Iteration: 4213 lambda_n: 0.885848666722051 Loss: 0.00020185161797964043\n",
      "Iteration: 4214 lambda_n: 0.8988810345354192 Loss: 0.00020185214075866704\n",
      "Iteration: 4215 lambda_n: 1.0079389548866386 Loss: 0.00020185266682295856\n",
      "Iteration: 4216 lambda_n: 1.0318150145946825 Loss: 0.00020185325174196505\n",
      "Iteration: 4217 lambda_n: 0.9408911212946758 Loss: 0.0002018538448584476\n",
      "Iteration: 4218 lambda_n: 0.8837374258809585 Loss: 0.0002018543804771368\n",
      "Iteration: 4219 lambda_n: 0.9746236318116519 Loss: 0.0002018548791224046\n",
      "Iteration: 4220 lambda_n: 0.9181678090108484 Loss: 0.00020185542449442342\n",
      "Iteration: 4221 lambda_n: 0.9024272545017871 Loss: 0.00020185593358111278\n",
      "Iteration: 4222 lambda_n: 0.9732609918837969 Loss: 0.000201856429633813\n",
      "Iteration: 4223 lambda_n: 0.9110595332706405 Loss: 0.00020185696009777208\n",
      "Iteration: 4224 lambda_n: 0.9683264731589608 Loss: 0.00020185745212939808\n",
      "Iteration: 4225 lambda_n: 0.9973858883262007 Loss: 0.00020185797062346477\n",
      "Iteration: 4226 lambda_n: 0.9407569637469129 Loss: 0.00020185849983063773\n",
      "Iteration: 4227 lambda_n: 0.8910717486300165 Loss: 0.0002018589943245312\n",
      "Iteration: 4228 lambda_n: 0.9494806346599679 Loss: 0.0002018594585724359\n",
      "Iteration: 4229 lambda_n: 1.0114704899376648 Loss: 0.000201859949120552\n",
      "Iteration: 4230 lambda_n: 1.0127875161592623 Loss: 0.00020186046704601877\n",
      "Iteration: 4231 lambda_n: 0.9897576147983898 Loss: 0.00020186098073023283\n",
      "Iteration: 4232 lambda_n: 1.030094172530845 Loss: 0.00020186147796930428\n",
      "Iteration: 4233 lambda_n: 0.9971612402630832 Loss: 0.0002018619906733911\n",
      "Iteration: 4234 lambda_n: 1.0021331458759768 Loss: 0.00020186248219531837\n",
      "Iteration: 4235 lambda_n: 0.9997859804143782 Loss: 0.00020186297155272602\n",
      "Iteration: 4236 lambda_n: 0.8991632473856933 Loss: 0.00020186345517992965\n",
      "Iteration: 4237 lambda_n: 0.9699022042671139 Loss: 0.00020186388605823068\n",
      "Iteration: 4238 lambda_n: 0.9362722633717199 Loss: 0.00020186434691954165\n",
      "Iteration: 4239 lambda_n: 0.9739461472758602 Loss: 0.00020186478775861163\n",
      "Iteration: 4240 lambda_n: 0.965176161996323 Loss: 0.00020186524231409066\n",
      "Iteration: 4241 lambda_n: 0.961717116144062 Loss: 0.00020186568866654476\n",
      "Iteration: 4242 lambda_n: 0.9332000733854661 Loss: 0.00020186612939820847\n",
      "Iteration: 4243 lambda_n: 0.9658230760765558 Loss: 0.00020186655320849616\n",
      "Iteration: 4244 lambda_n: 0.9664799263577715 Loss: 0.00020186698800047642\n",
      "Iteration: 4245 lambda_n: 0.8949689387444192 Loss: 0.00020186741915219646\n",
      "Iteration: 4246 lambda_n: 0.946579198239522 Loss: 0.00020186781478820985\n",
      "Iteration: 4247 lambda_n: 0.9114386998135862 Loss: 0.00020186822973205577\n",
      "Iteration: 4248 lambda_n: 0.8957522694830903 Loss: 0.00020186862572960394\n",
      "Iteration: 4249 lambda_n: 0.9115709243005776 Loss: 0.0002018690115898274\n",
      "Iteration: 4250 lambda_n: 0.9373033106161018 Loss: 0.0002018694009702691\n",
      "Iteration: 4251 lambda_n: 0.909590454795132 Loss: 0.00020186979792472326\n",
      "Iteration: 4252 lambda_n: 1.0035549846749923 Loss: 0.0002018701797614891\n",
      "Iteration: 4253 lambda_n: 0.9769019964942102 Loss: 0.00020187059745570984\n",
      "Iteration: 4254 lambda_n: 0.9181012085863528 Loss: 0.0002018710002358146\n",
      "Iteration: 4255 lambda_n: 0.9347080639680981 Loss: 0.000201871375309639\n",
      "Iteration: 4256 lambda_n: 0.9455028758934609 Loss: 0.00020187175388554742\n",
      "Iteration: 4257 lambda_n: 1.020170236458933 Loss: 0.00020187213348241573\n",
      "Iteration: 4258 lambda_n: 0.9357590588015516 Loss: 0.00020187253943118564\n",
      "Iteration: 4259 lambda_n: 0.8856701724629549 Loss: 0.00020187290823448712\n",
      "Iteration: 4260 lambda_n: 0.9929836783376019 Loss: 0.00020187325423886855\n",
      "Iteration: 4261 lambda_n: 0.9455235889094175 Loss: 0.00020187363895144228\n",
      "Iteration: 4262 lambda_n: 0.9868501129079962 Loss: 0.00020187400187151992\n",
      "Iteration: 4263 lambda_n: 0.9070064336516188 Loss: 0.00020187437730175798\n",
      "Iteration: 4264 lambda_n: 0.894933820136443 Loss: 0.00020187471916955545\n",
      "Iteration: 4265 lambda_n: 0.9005236023581875 Loss: 0.00020187505362347414\n",
      "Iteration: 4266 lambda_n: 1.0334005801059427 Loss: 0.00020187538734770248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4267 lambda_n: 1.0356641222816394 Loss: 0.00020187576708747559\n",
      "Iteration: 4268 lambda_n: 0.9421662807149859 Loss: 0.00020187614397861983\n",
      "Iteration: 4269 lambda_n: 0.9312733460274136 Loss: 0.00020187648352167835\n",
      "Iteration: 4270 lambda_n: 0.8960772095447816 Loss: 0.00020187681618023505\n",
      "Iteration: 4271 lambda_n: 0.935862613291017 Loss: 0.0002018771334772087\n",
      "Iteration: 4272 lambda_n: 0.9261120860729329 Loss: 0.00020187746208369256\n",
      "Iteration: 4273 lambda_n: 0.990910566357682 Loss: 0.00020187778441919103\n",
      "Iteration: 4274 lambda_n: 0.9473778015368772 Loss: 0.00020187812631978868\n",
      "Iteration: 4275 lambda_n: 0.9266519579518813 Loss: 0.00020187845016962793\n",
      "Iteration: 4276 lambda_n: 0.9199746988120806 Loss: 0.0002018787641271858\n",
      "Iteration: 4277 lambda_n: 0.8962014882355678 Loss: 0.0002018790731205543\n",
      "Iteration: 4278 lambda_n: 0.9527326217093113 Loss: 0.00020187937153878484\n",
      "Iteration: 4279 lambda_n: 1.021797741096611 Loss: 0.00020187968612154865\n",
      "Iteration: 4280 lambda_n: 0.8861133613144795 Loss: 0.00020188002050249102\n",
      "Iteration: 4281 lambda_n: 1.0163488677776726 Loss: 0.00020188030770959618\n",
      "Iteration: 4282 lambda_n: 0.9386288865040338 Loss: 0.0002018806343988352\n",
      "Iteration: 4283 lambda_n: 1.0232670354110422 Loss: 0.0002018809332384126\n",
      "Iteration: 4284 lambda_n: 0.9045987853334729 Loss: 0.00020188125616545563\n",
      "Iteration: 4285 lambda_n: 0.9103762461576578 Loss: 0.00020188153891086138\n",
      "Iteration: 4286 lambda_n: 0.9566756432037794 Loss: 0.00020188182105519348\n",
      "Iteration: 4287 lambda_n: 0.9841586070310745 Loss: 0.00020188211502488257\n",
      "Iteration: 4288 lambda_n: 0.9550461794550128 Loss: 0.00020188241473459022\n",
      "Iteration: 4289 lambda_n: 1.036956253706831 Loss: 0.0002018827029023892\n",
      "Iteration: 4290 lambda_n: 1.0264049689618797 Loss: 0.0002018830129915554\n",
      "Iteration: 4291 lambda_n: 0.9640899421770316 Loss: 0.00020188331695002014\n",
      "Iteration: 4292 lambda_n: 1.0116423188820494 Loss: 0.0002018835997151318\n",
      "Iteration: 4293 lambda_n: 0.9107285391642134 Loss: 0.00020188389375335489\n",
      "Iteration: 4294 lambda_n: 0.8853217778495808 Loss: 0.0002018841559573851\n",
      "Iteration: 4295 lambda_n: 0.9085665720881553 Loss: 0.00020188440867693986\n",
      "Iteration: 4296 lambda_n: 1.0104656090684387 Loss: 0.00020188466588584619\n",
      "Iteration: 4297 lambda_n: 0.9084091258812833 Loss: 0.00020188494951283174\n",
      "Iteration: 4298 lambda_n: 1.020887906769628 Loss: 0.00020188520208570698\n",
      "Iteration: 4299 lambda_n: 0.9833731939050216 Loss: 0.00020188548352257745\n",
      "Iteration: 4300 lambda_n: 1.015610512119945 Loss: 0.0002018857520312813\n",
      "Iteration: 4301 lambda_n: 0.9240512780632101 Loss: 0.00020188602679428326\n",
      "Iteration: 4302 lambda_n: 0.9958340059067394 Loss: 0.00020188627441451813\n",
      "Iteration: 4303 lambda_n: 0.9468528388120585 Loss: 0.00020188653896673856\n",
      "Iteration: 4304 lambda_n: 1.0189097607055602 Loss: 0.00020188678816640963\n",
      "Iteration: 4305 lambda_n: 0.9995696972053538 Loss: 0.00020188705395858048\n",
      "Iteration: 4306 lambda_n: 0.9608211831910459 Loss: 0.0002018873122238451\n",
      "Iteration: 4307 lambda_n: 0.9840612724142592 Loss: 0.00020188755815941386\n",
      "Iteration: 4308 lambda_n: 0.9625105949791405 Loss: 0.00020188780778306596\n",
      "Iteration: 4309 lambda_n: 0.8894233848773325 Loss: 0.00020188804969592872\n",
      "Iteration: 4310 lambda_n: 0.9367273636456398 Loss: 0.00020188827122980064\n",
      "Iteration: 4311 lambda_n: 0.9218463924158734 Loss: 0.0002018885026079773\n",
      "Iteration: 4312 lambda_n: 0.9557824120395115 Loss: 0.00020188872831856\n",
      "Iteration: 4313 lambda_n: 0.9805415241469869 Loss: 0.0002018889603237544\n",
      "Iteration: 4314 lambda_n: 1.0303412777937369 Loss: 0.00020188919621471181\n",
      "Iteration: 4315 lambda_n: 0.9142604340454663 Loss: 0.00020188944181675513\n",
      "Iteration: 4316 lambda_n: 1.0166203409097165 Loss: 0.00020188965765209494\n",
      "Iteration: 4317 lambda_n: 1.0369378630870532 Loss: 0.00020188989560370615\n",
      "Iteration: 4318 lambda_n: 0.9084128298325318 Loss: 0.0002018901360073783\n",
      "Iteration: 4319 lambda_n: 0.9646331921331496 Loss: 0.00020189034457506277\n",
      "Iteration: 4320 lambda_n: 0.8921989171908316 Loss: 0.00020189056417271558\n",
      "Iteration: 4321 lambda_n: 0.960386422227448 Loss: 0.00020189076545193115\n",
      "Iteration: 4322 lambda_n: 0.9294497117505116 Loss: 0.00020189098031000882\n",
      "Iteration: 4323 lambda_n: 1.0318788703132047 Loss: 0.00020189118638306197\n",
      "Iteration: 4324 lambda_n: 0.9414350478223777 Loss: 0.00020189141318176522\n",
      "Iteration: 4325 lambda_n: 0.9473991336954606 Loss: 0.00020189161810904962\n",
      "Iteration: 4326 lambda_n: 0.9010727829096813 Loss: 0.00020189182252287094\n",
      "Iteration: 4327 lambda_n: 0.9122718495768056 Loss: 0.00020189201522242855\n",
      "Iteration: 4328 lambda_n: 0.8885336249056766 Loss: 0.00020189220867673955\n",
      "Iteration: 4329 lambda_n: 0.9092884822250639 Loss: 0.00020189239549342006\n",
      "Iteration: 4330 lambda_n: 0.9009222163070107 Loss: 0.00020189258508905854\n",
      "Iteration: 4331 lambda_n: 0.957861824121033 Loss: 0.00020189277134674888\n",
      "Iteration: 4332 lambda_n: 0.8949711419827059 Loss: 0.0002018929677119265\n",
      "Iteration: 4333 lambda_n: 0.9692234163630782 Loss: 0.00020189314954492885\n",
      "Iteration: 4334 lambda_n: 0.9700591195267251 Loss: 0.0002018933448201073\n",
      "Iteration: 4335 lambda_n: 0.9537435357576903 Loss: 0.00020189353849685217\n",
      "Iteration: 4336 lambda_n: 0.8868159689094377 Loss: 0.00020189372719331766\n",
      "Iteration: 4337 lambda_n: 1.0302321319586392 Loss: 0.00020189390108768914\n",
      "Iteration: 4338 lambda_n: 0.978684149276959 Loss: 0.00020189410143371162\n",
      "Iteration: 4339 lambda_n: 0.9345813121663885 Loss: 0.00020189428992694016\n",
      "Iteration: 4340 lambda_n: 1.03052682440813 Loss: 0.0002018944682834232\n",
      "Iteration: 4341 lambda_n: 0.9166003261225343 Loss: 0.000201894663236582\n",
      "Iteration: 4342 lambda_n: 1.0080401897735403 Loss: 0.00020189483497126565\n",
      "Iteration: 4343 lambda_n: 0.8901532010322332 Loss: 0.00020189502222432102\n",
      "Iteration: 4344 lambda_n: 0.9911734046717584 Loss: 0.0002018951860248587\n",
      "Iteration: 4345 lambda_n: 0.9043910763181972 Loss: 0.00020189536690116993\n",
      "Iteration: 4346 lambda_n: 0.8852402000911949 Loss: 0.00020189553041603534\n",
      "Iteration: 4347 lambda_n: 0.8891713185070542 Loss: 0.00020189568911926887\n",
      "Iteration: 4348 lambda_n: 1.019232917184289 Loss: 0.00020189584721209765\n",
      "Iteration: 4349 lambda_n: 0.9571094430615984 Loss: 0.00020189602692800225\n",
      "Iteration: 4350 lambda_n: 0.9824580682213891 Loss: 0.0002018961940870653\n",
      "Iteration: 4351 lambda_n: 0.9490081021415064 Loss: 0.00020189636414295973\n",
      "Iteration: 4352 lambda_n: 0.8849271947899779 Loss: 0.0002018965269051929\n",
      "Iteration: 4353 lambda_n: 0.9496139445573287 Loss: 0.00020189667733505625\n",
      "Iteration: 4354 lambda_n: 0.9059013711502327 Loss: 0.0002018968374302605\n",
      "Iteration: 4355 lambda_n: 0.9367103395250703 Loss: 0.00020189698880484274\n",
      "Iteration: 4356 lambda_n: 0.9595684134099017 Loss: 0.00020189714400669484\n",
      "Iteration: 4357 lambda_n: 0.9783176216553366 Loss: 0.00020189730160863993\n",
      "Iteration: 4358 lambda_n: 0.9910210051656354 Loss: 0.0002018974608538741\n",
      "Iteration: 4359 lambda_n: 0.8920256605019515 Loss: 0.00020189762069705862\n",
      "Iteration: 4360 lambda_n: 0.9981608821126283 Loss: 0.00020189776324519522\n",
      "Iteration: 4361 lambda_n: 0.9065806724540928 Loss: 0.00020189792142903893\n",
      "Iteration: 4362 lambda_n: 0.9071068714813669 Loss: 0.00020189806376422954\n",
      "Iteration: 4363 lambda_n: 0.9763241785105442 Loss: 0.00020189820497982784\n",
      "Iteration: 4364 lambda_n: 0.9883277582383084 Loss: 0.00020189835568727341\n",
      "Iteration: 4365 lambda_n: 1.002838032985167 Loss: 0.00020189850686088497\n",
      "Iteration: 4366 lambda_n: 0.9268151433144318 Loss: 0.0002018986588426188\n",
      "Iteration: 4367 lambda_n: 0.9269481663644831 Loss: 0.00020189879799167103\n",
      "Iteration: 4368 lambda_n: 0.8901443599197991 Loss: 0.0002018989359600751\n",
      "Iteration: 4369 lambda_n: 0.9892497066851632 Loss: 0.0002018990673074109\n",
      "Iteration: 4370 lambda_n: 0.9384755195726139 Loss: 0.00020189921206916728\n",
      "Iteration: 4371 lambda_n: 0.972215145720783 Loss: 0.00020189934813650908\n",
      "Iteration: 4372 lambda_n: 0.9279742382123582 Loss: 0.0002018994878646221\n",
      "Iteration: 4373 lambda_n: 0.9233874028643263 Loss: 0.0002018996200277841\n",
      "Iteration: 4374 lambda_n: 0.9083955616791615 Loss: 0.00020189975040215322\n",
      "Iteration: 4375 lambda_n: 0.9831358106102154 Loss: 0.0002018998775578762\n",
      "Iteration: 4376 lambda_n: 0.8946731725841455 Loss: 0.00020190001401256325\n",
      "Iteration: 4377 lambda_n: 0.9490296857775481 Loss: 0.0002019001370533079\n",
      "Iteration: 4378 lambda_n: 0.9757295440963917 Loss: 0.00020190026648326075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4379 lambda_n: 0.9779050127482811 Loss: 0.00020190039837985528\n",
      "Iteration: 4380 lambda_n: 0.9662206420225373 Loss: 0.0002019005293708405\n",
      "Iteration: 4381 lambda_n: 0.9326647142070846 Loss: 0.0002019006576195384\n",
      "Iteration: 4382 lambda_n: 0.9913733147135225 Loss: 0.00020190078030190065\n",
      "Iteration: 4383 lambda_n: 0.8886729108749339 Loss: 0.00020190090957575247\n",
      "Iteration: 4384 lambda_n: 1.0337013111706044 Loss: 0.00020190102438932212\n",
      "Iteration: 4385 lambda_n: 0.9202887593440165 Loss: 0.00020190115683660654\n",
      "Iteration: 4386 lambda_n: 1.0222010534280759 Loss: 0.00020190127361915253\n",
      "Iteration: 4387 lambda_n: 0.9670442242027446 Loss: 0.0002019014022243615\n",
      "Iteration: 4388 lambda_n: 0.9646903855593696 Loss: 0.000201901522734021\n",
      "Iteration: 4389 lambda_n: 0.9953055998708131 Loss: 0.00020190164186971397\n",
      "Iteration: 4390 lambda_n: 0.9170314704476371 Loss: 0.00020190176368413992\n",
      "Iteration: 4391 lambda_n: 0.9323570138858032 Loss: 0.00020190187488045778\n",
      "Iteration: 4392 lambda_n: 1.0156487888506323 Loss: 0.00020190198697159887\n",
      "Iteration: 4393 lambda_n: 0.9885260200264778 Loss: 0.0002019021080184071\n",
      "Iteration: 4394 lambda_n: 0.9084194500576662 Loss: 0.0002019022247208052\n",
      "Iteration: 4395 lambda_n: 1.0080932305536041 Loss: 0.00020190233098100368\n",
      "Iteration: 4396 lambda_n: 0.9054806233278969 Loss: 0.0002019024479050762\n",
      "Iteration: 4397 lambda_n: 0.9120148044400287 Loss: 0.00020190255194400708\n",
      "Iteration: 4398 lambda_n: 0.9082115535132562 Loss: 0.00020190265585227568\n",
      "Iteration: 4399 lambda_n: 1.0142480647853378 Loss: 0.0002019027584506026\n",
      "Iteration: 4400 lambda_n: 0.9298133656485887 Loss: 0.00020190287206109823\n",
      "Iteration: 4401 lambda_n: 0.9589867345291891 Loss: 0.00020190297523256244\n",
      "Iteration: 4402 lambda_n: 0.8853035584119103 Loss: 0.00020190308072224578\n",
      "Iteration: 4403 lambda_n: 0.893208423624971 Loss: 0.000201903177239431\n",
      "Iteration: 4404 lambda_n: 0.8949823677796597 Loss: 0.00020190327381789899\n",
      "Iteration: 4405 lambda_n: 1.0065297206267951 Loss: 0.0002019033697856406\n",
      "Iteration: 4406 lambda_n: 0.9230900160820031 Loss: 0.00020190347681763615\n",
      "Iteration: 4407 lambda_n: 0.9082831980577827 Loss: 0.00020190357405959938\n",
      "Iteration: 4408 lambda_n: 0.9747326934744254 Loss: 0.00020190366892186283\n",
      "Iteration: 4409 lambda_n: 0.9824103200932458 Loss: 0.0002019037698659121\n",
      "Iteration: 4410 lambda_n: 1.0126912786279854 Loss: 0.00020190387068460587\n",
      "Iteration: 4411 lambda_n: 0.995620577052891 Loss: 0.00020190397366329622\n",
      "Iteration: 4412 lambda_n: 0.9339916473621094 Loss: 0.00020190407395460528\n",
      "Iteration: 4413 lambda_n: 0.9153719935407408 Loss: 0.00020190416716864672\n",
      "Iteration: 4414 lambda_n: 0.8993297601469727 Loss: 0.0002019042577326957\n",
      "Iteration: 4415 lambda_n: 1.0250285185931978 Loss: 0.00020190434595390147\n",
      "Iteration: 4416 lambda_n: 1.0169298624033585 Loss: 0.00020190444566678897\n",
      "Iteration: 4417 lambda_n: 1.012148033294047 Loss: 0.0002019045436511619\n",
      "Iteration: 4418 lambda_n: 0.8984653778290548 Loss: 0.00020190464025482456\n",
      "Iteration: 4419 lambda_n: 0.9846665969364682 Loss: 0.00020190472520309404\n",
      "Iteration: 4420 lambda_n: 1.0068866738588302 Loss: 0.000201904817525748\n",
      "Iteration: 4421 lambda_n: 0.9803175211410975 Loss: 0.00020190491106966753\n",
      "Iteration: 4422 lambda_n: 1.0054575949947617 Loss: 0.00020190500129483297\n",
      "Iteration: 4423 lambda_n: 0.9641278034195796 Loss: 0.0002019050929926459\n",
      "Iteration: 4424 lambda_n: 1.0342969711730923 Loss: 0.00020190518010147022\n",
      "Iteration: 4425 lambda_n: 1.0234674059730697 Loss: 0.00020190527271481433\n",
      "Iteration: 4426 lambda_n: 0.9556151562809277 Loss: 0.00020190536347975449\n",
      "Iteration: 4427 lambda_n: 0.9485366226932371 Loss: 0.00020190544742327627\n",
      "Iteration: 4428 lambda_n: 0.9632267307866734 Loss: 0.00020190553000699868\n",
      "Iteration: 4429 lambda_n: 1.0363113649422413 Loss: 0.00020190561313247142\n",
      "Iteration: 4430 lambda_n: 0.9064358683829652 Loss: 0.00020190570176678612\n",
      "Iteration: 4431 lambda_n: 0.9464311397446927 Loss: 0.00020190577854852\n",
      "Iteration: 4432 lambda_n: 0.8895285227640466 Loss: 0.0002019058580448317\n",
      "Iteration: 4433 lambda_n: 0.9721123914745821 Loss: 0.0002019059321063878\n",
      "Iteration: 4434 lambda_n: 0.9285591067092415 Loss: 0.00020190601237683568\n",
      "Iteration: 4435 lambda_n: 1.0284184569301942 Loss: 0.00020190608836045154\n",
      "Iteration: 4436 lambda_n: 1.0012086796207178 Loss: 0.00020190617179171718\n",
      "Iteration: 4437 lambda_n: 0.8957722289836024 Loss: 0.00020190625224189443\n",
      "Iteration: 4438 lambda_n: 0.9169570291790812 Loss: 0.00020190632355250498\n",
      "Iteration: 4439 lambda_n: 0.9080735136132533 Loss: 0.00020190639594404238\n",
      "Iteration: 4440 lambda_n: 0.953127485366109 Loss: 0.0002019064670255353\n",
      "Iteration: 4441 lambda_n: 0.904322239582584 Loss: 0.00020190654100644525\n",
      "Iteration: 4442 lambda_n: 0.9229383405942293 Loss: 0.00020190661057969432\n",
      "Iteration: 4443 lambda_n: 0.9264964126297708 Loss: 0.0002019066809907172\n",
      "Iteration: 4444 lambda_n: 0.9869952383049458 Loss: 0.0002019067510693067\n",
      "Iteration: 4445 lambda_n: 0.9628935844207075 Loss: 0.00020190682508369685\n",
      "Iteration: 4446 lambda_n: 0.9325322838623215 Loss: 0.00020190689663109979\n",
      "Iteration: 4447 lambda_n: 1.0372396635617191 Loss: 0.00020190696530503031\n",
      "Iteration: 4448 lambda_n: 0.9659674469913174 Loss: 0.00020190704103070737\n",
      "Iteration: 4449 lambda_n: 1.0250774381934191 Loss: 0.00020190711087616294\n",
      "Iteration: 4450 lambda_n: 0.9457083073098334 Loss: 0.00020190718433320655\n",
      "Iteration: 4451 lambda_n: 0.8871896337708816 Loss: 0.0002019072514599315\n",
      "Iteration: 4452 lambda_n: 0.892407000184124 Loss: 0.0002019073138820886\n",
      "Iteration: 4453 lambda_n: 0.8893368456007292 Loss: 0.0002019073761560424\n",
      "Iteration: 4454 lambda_n: 0.9855558716725561 Loss: 0.00020190743770352596\n",
      "Iteration: 4455 lambda_n: 1.037772882863591 Loss: 0.00020190750534894087\n",
      "Iteration: 4456 lambda_n: 1.017833385076494 Loss: 0.00020190757592915235\n",
      "Iteration: 4457 lambda_n: 0.9801700997179964 Loss: 0.0002019076444889755\n",
      "Iteration: 4458 lambda_n: 1.0307123824803257 Loss: 0.00020190770989051514\n",
      "Iteration: 4459 lambda_n: 0.9037780663907112 Loss: 0.00020190777804123257\n",
      "Iteration: 4460 lambda_n: 0.887194258859511 Loss: 0.0002019078372296413\n",
      "Iteration: 4461 lambda_n: 1.0371231593134425 Loss: 0.00020190789484657344\n",
      "Iteration: 4462 lambda_n: 0.9740362405522697 Loss: 0.00020190796164801418\n",
      "Iteration: 4463 lambda_n: 0.9799609199503765 Loss: 0.00020190802378467097\n",
      "Iteration: 4464 lambda_n: 0.9150390847441964 Loss: 0.00020190808573655449\n",
      "Iteration: 4465 lambda_n: 0.9503169589605123 Loss: 0.000201908143060321\n",
      "Iteration: 4466 lambda_n: 0.9348458224102035 Loss: 0.0002019082020907761\n",
      "Iteration: 4467 lambda_n: 0.9506127401459001 Loss: 0.00020190825965037463\n",
      "Iteration: 4468 lambda_n: 1.0314660405468454 Loss: 0.00020190831767528294\n",
      "Iteration: 4469 lambda_n: 0.883390842436576 Loss: 0.00020190838008257673\n",
      "Iteration: 4470 lambda_n: 0.9107420702207953 Loss: 0.00020190843302158503\n",
      "Iteration: 4471 lambda_n: 0.9806959062948924 Loss: 0.00020190848715440413\n",
      "Iteration: 4472 lambda_n: 0.9231011518265667 Loss: 0.0002019085449548884\n",
      "Iteration: 4473 lambda_n: 0.9471146991950492 Loss: 0.0002019085988681497\n",
      "Iteration: 4474 lambda_n: 0.9429549929799493 Loss: 0.00020190865371245257\n",
      "Iteration: 4475 lambda_n: 0.9159399107762897 Loss: 0.0002019087078384161\n",
      "Iteration: 4476 lambda_n: 0.9796538634192004 Loss: 0.00020190875995604267\n",
      "Iteration: 4477 lambda_n: 0.995326789606118 Loss: 0.00020190881522773936\n",
      "Iteration: 4478 lambda_n: 0.9484921984301247 Loss: 0.00020190887087592757\n",
      "Iteration: 4479 lambda_n: 0.9191517149627034 Loss: 0.00020190892341847476\n",
      "Iteration: 4480 lambda_n: 0.8993656693346563 Loss: 0.00020190897389000487\n",
      "Iteration: 4481 lambda_n: 1.0267149486576688 Loss: 0.0002019090228562054\n",
      "Iteration: 4482 lambda_n: 0.9856936023913632 Loss: 0.000201909078292116\n",
      "Iteration: 4483 lambda_n: 0.9052997650849789 Loss: 0.00020190913100901227\n",
      "Iteration: 4484 lambda_n: 0.883773857646516 Loss: 0.0002019091789860156\n",
      "Iteration: 4485 lambda_n: 0.9993534194268957 Loss: 0.00020190922543113283\n",
      "Iteration: 4486 lambda_n: 0.961243222550545 Loss: 0.0002019092775222458\n",
      "Iteration: 4487 lambda_n: 0.9457150470851858 Loss: 0.00020190932716507173\n",
      "Iteration: 4488 lambda_n: 0.9520008352858378 Loss: 0.0002019093755730138\n",
      "Iteration: 4489 lambda_n: 0.9486687192192476 Loss: 0.00020190942387776376\n",
      "Iteration: 4490 lambda_n: 1.0240789493692566 Loss: 0.00020190947159095393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4491 lambda_n: 0.9798421140577316 Loss: 0.00020190952264640487\n",
      "Iteration: 4492 lambda_n: 1.0272015916203727 Loss: 0.0002019095710353001\n",
      "Iteration: 4493 lambda_n: 1.0364737191884563 Loss: 0.00020190962130485753\n",
      "Iteration: 4494 lambda_n: 0.924390382970529 Loss: 0.00020190967154797124\n",
      "Iteration: 4495 lambda_n: 0.9019216403808894 Loss: 0.00020190971592982384\n",
      "Iteration: 4496 lambda_n: 0.9174499679153376 Loss: 0.00020190975886405632\n",
      "Iteration: 4497 lambda_n: 1.0129412242046918 Loss: 0.00020190980217456146\n",
      "Iteration: 4498 lambda_n: 0.9187449767843959 Loss: 0.00020190984958879358\n",
      "Iteration: 4499 lambda_n: 1.0135905404921126 Loss: 0.00020190989219257005\n",
      "Iteration: 4500 lambda_n: 0.9432712703980057 Loss: 0.00020190993879674548\n",
      "Iteration: 4501 lambda_n: 0.9605721627733889 Loss: 0.00020190998176279333\n",
      "Iteration: 4502 lambda_n: 0.9673613286028693 Loss: 0.00020191002513681767\n",
      "Iteration: 4503 lambda_n: 0.9649315913153077 Loss: 0.00020191006843101858\n",
      "Iteration: 4504 lambda_n: 1.0203107736792854 Loss: 0.00020191011123182838\n",
      "Iteration: 4505 lambda_n: 0.9592293104622541 Loss: 0.0002019101560870219\n",
      "Iteration: 4506 lambda_n: 0.9147521558345566 Loss: 0.00020191019786083264\n",
      "Iteration: 4507 lambda_n: 0.9511358305619525 Loss: 0.0002019102373459592\n",
      "Iteration: 4508 lambda_n: 0.9498704038085984 Loss: 0.00020191027805592203\n",
      "Iteration: 4509 lambda_n: 1.0230054490717737 Loss: 0.00020191031835584548\n",
      "Iteration: 4510 lambda_n: 1.0111126962938786 Loss: 0.00020191036137926483\n",
      "Iteration: 4511 lambda_n: 0.9440222979670829 Loss: 0.00020191040350225474\n",
      "Iteration: 4512 lambda_n: 0.9150870625430497 Loss: 0.00020191044246440945\n",
      "Iteration: 4513 lambda_n: 0.9819024396021334 Loss: 0.00020191047990435315\n",
      "Iteration: 4514 lambda_n: 1.0147388961018096 Loss: 0.00020191051973982363\n",
      "Iteration: 4515 lambda_n: 0.9153415850719666 Loss: 0.00020191056053568045\n",
      "Iteration: 4516 lambda_n: 1.0358681989223426 Loss: 0.0002019105969920146\n",
      "Iteration: 4517 lambda_n: 0.9143676515747813 Loss: 0.00020191063790143818\n",
      "Iteration: 4518 lambda_n: 0.9473083749939659 Loss: 0.0002019106736685144\n",
      "Iteration: 4519 lambda_n: 1.0103852389504353 Loss: 0.0002019107104126273\n",
      "Iteration: 4520 lambda_n: 0.9730074569239585 Loss: 0.00020191074926207488\n",
      "Iteration: 4521 lambda_n: 0.9045626529748962 Loss: 0.00020191078632686427\n",
      "Iteration: 4522 lambda_n: 0.9193434458784511 Loss: 0.00020191082047622854\n",
      "Iteration: 4523 lambda_n: 1.0106320219307818 Loss: 0.00020191085489508237\n",
      "Iteration: 4524 lambda_n: 1.0242242950691516 Loss: 0.0002019108924120202\n",
      "Iteration: 4525 lambda_n: 0.9370382326581148 Loss: 0.00020191093008045552\n",
      "Iteration: 4526 lambda_n: 0.8881563059812517 Loss: 0.00020191096421812892\n",
      "Iteration: 4527 lambda_n: 0.9709496152595742 Loss: 0.00020191099629644177\n",
      "Iteration: 4528 lambda_n: 0.9222652317166664 Loss: 0.00020191103107897957\n",
      "Iteration: 4529 lambda_n: 1.035252893656084 Loss: 0.00020191106382285646\n",
      "Iteration: 4530 lambda_n: 0.9696613241293658 Loss: 0.0002019111002669229\n",
      "Iteration: 4531 lambda_n: 1.0055023025644236 Loss: 0.00020191113407746735\n",
      "Iteration: 4532 lambda_n: 0.9883948376110586 Loss: 0.0002019111688255937\n",
      "Iteration: 4533 lambda_n: 0.9696184486580687 Loss: 0.00020191120266722703\n",
      "Iteration: 4534 lambda_n: 0.9060080199303877 Loss: 0.0002019112355647459\n",
      "Iteration: 4535 lambda_n: 1.0116403808849326 Loss: 0.0002019112660305042\n",
      "Iteration: 4536 lambda_n: 0.9925812790617544 Loss: 0.00020191129976544907\n",
      "Iteration: 4537 lambda_n: 0.956411365058806 Loss: 0.00020191133255756787\n",
      "Iteration: 4538 lambda_n: 0.9330602110212224 Loss: 0.00020191136386693838\n",
      "Iteration: 4539 lambda_n: 0.9274423270363232 Loss: 0.00020191139414386505\n",
      "Iteration: 4540 lambda_n: 0.943668457199246 Loss: 0.00020191142398088462\n",
      "Iteration: 4541 lambda_n: 0.8999932310359512 Loss: 0.00020191145408167885\n",
      "Iteration: 4542 lambda_n: 0.9754254268027633 Loss: 0.00020191148254086343\n",
      "Iteration: 4543 lambda_n: 1.03363093104093 Loss: 0.00020191151313074526\n",
      "Iteration: 4544 lambda_n: 0.9644327043828251 Loss: 0.00020191154525605215\n",
      "Iteration: 4545 lambda_n: 0.8923923032679288 Loss: 0.00020191157494660298\n",
      "Iteration: 4546 lambda_n: 1.0371793859908518 Loss: 0.0002019116021764505\n",
      "Iteration: 4547 lambda_n: 0.9371392971711081 Loss: 0.00020191163356534923\n",
      "Iteration: 4548 lambda_n: 1.00419588554345 Loss: 0.00020191166165703722\n",
      "Iteration: 4549 lambda_n: 0.9849515571658572 Loss: 0.00020191169150028052\n",
      "Iteration: 4550 lambda_n: 0.9192147898730334 Loss: 0.00020191172050224352\n",
      "Iteration: 4551 lambda_n: 0.9049518549803571 Loss: 0.0002019117473243198\n",
      "Iteration: 4552 lambda_n: 1.0239654827799012 Loss: 0.00020191177350781898\n",
      "Iteration: 4553 lambda_n: 1.009908510776229 Loss: 0.00020191180288920267\n",
      "Iteration: 4554 lambda_n: 1.0178513536735583 Loss: 0.0002019118315954499\n",
      "Iteration: 4555 lambda_n: 0.917750074078679 Loss: 0.00020191186025986208\n",
      "Iteration: 4556 lambda_n: 0.9911547727722891 Loss: 0.00020191188586434616\n",
      "Iteration: 4557 lambda_n: 1.0196385234272194 Loss: 0.00020191191328438536\n",
      "Iteration: 4558 lambda_n: 0.9243086773140561 Loss: 0.0002019119412364497\n",
      "Iteration: 4559 lambda_n: 0.897819519919728 Loss: 0.00020191196633864356\n",
      "Iteration: 4560 lambda_n: 0.8934439335578782 Loss: 0.00020191199051515903\n",
      "Iteration: 4561 lambda_n: 1.0361483216259866 Loss: 0.0002019120143761483\n",
      "Iteration: 4562 lambda_n: 0.9501456844744455 Loss: 0.0002019120418220626\n",
      "Iteration: 4563 lambda_n: 0.9457167719658225 Loss: 0.00020191206675127083\n",
      "Iteration: 4564 lambda_n: 0.9013369954797052 Loss: 0.0002019120913485759\n",
      "Iteration: 4565 lambda_n: 0.8959268554440485 Loss: 0.00020191211458877128\n",
      "Iteration: 4566 lambda_n: 0.9121675056638653 Loss: 0.00020191213749901337\n",
      "Iteration: 4567 lambda_n: 0.8881963531232283 Loss: 0.00020191216063340423\n",
      "Iteration: 4568 lambda_n: 0.9863744257040762 Loss: 0.00020191218297193464\n",
      "Iteration: 4569 lambda_n: 0.9300208523461743 Loss: 0.00020191220757819088\n",
      "Iteration: 4570 lambda_n: 0.9389310136777623 Loss: 0.00020191223056939162\n",
      "Iteration: 4571 lambda_n: 0.8943329562104178 Loss: 0.000201912253583491\n",
      "Iteration: 4572 lambda_n: 1.0332001864048488 Loss: 0.0002019122753163151\n",
      "Iteration: 4573 lambda_n: 1.0183366556525248 Loss: 0.00020191230021845895\n",
      "Iteration: 4574 lambda_n: 1.0335994415214012 Loss: 0.00020191232453060443\n",
      "Iteration: 4575 lambda_n: 0.9967103403501562 Loss: 0.00020191234897749615\n",
      "Iteration: 4576 lambda_n: 1.032268608342907 Loss: 0.00020191237232923556\n",
      "Iteration: 4577 lambda_n: 0.8992259414197272 Loss: 0.00020191239629386735\n",
      "Iteration: 4578 lambda_n: 1.0380438336545412 Loss: 0.00020191241697298097\n",
      "Iteration: 4579 lambda_n: 1.0173430396163858 Loss: 0.0002019124406483455\n",
      "Iteration: 4580 lambda_n: 0.9786126902000508 Loss: 0.00020191246363163024\n",
      "Iteration: 4581 lambda_n: 0.8870112049407192 Loss: 0.00020191248553454985\n",
      "Iteration: 4582 lambda_n: 0.8933930767365421 Loss: 0.00020191250520988982\n",
      "Iteration: 4583 lambda_n: 1.0083750232117452 Loss: 0.00020191252486633304\n",
      "Iteration: 4584 lambda_n: 1.0157481684166632 Loss: 0.00020191254687165736\n",
      "Iteration: 4585 lambda_n: 0.8960084213251902 Loss: 0.00020191256883388486\n",
      "Iteration: 4586 lambda_n: 0.9288262025526552 Loss: 0.0002019125880275449\n",
      "Iteration: 4587 lambda_n: 1.0051923496708788 Loss: 0.00020191260776151898\n",
      "Iteration: 4588 lambda_n: 0.9984264183246575 Loss: 0.00020191262893698984\n",
      "Iteration: 4589 lambda_n: 0.9237967199625416 Loss: 0.00020191264977706942\n",
      "Iteration: 4590 lambda_n: 0.9422301420827681 Loss: 0.0002019126688837953\n",
      "Iteration: 4591 lambda_n: 0.953355817358885 Loss: 0.000201912688207576\n",
      "Iteration: 4592 lambda_n: 1.005323653084082 Loss: 0.00020191270759150535\n",
      "Iteration: 4593 lambda_n: 0.912029011261519 Loss: 0.00020191272785439977\n",
      "Iteration: 4594 lambda_n: 0.9817634658516186 Loss: 0.00020191274606838627\n",
      "Iteration: 4595 lambda_n: 0.9128363829501644 Loss: 0.0002019127655120117\n",
      "Iteration: 4596 lambda_n: 1.013303765443798 Loss: 0.00020191278342878417\n",
      "Iteration: 4597 lambda_n: 0.9316608589807924 Loss: 0.0002019128031520232\n",
      "Iteration: 4598 lambda_n: 0.9937583494821955 Loss: 0.00020191282111867628\n",
      "Iteration: 4599 lambda_n: 0.9608620866662867 Loss: 0.00020191284012015771\n",
      "Iteration: 4600 lambda_n: 0.8962457190957214 Loss: 0.00020191285832631045\n",
      "Iteration: 4601 lambda_n: 1.032964759173384 Loss: 0.0002019128751594881\n",
      "Iteration: 4602 lambda_n: 0.9155362515619467 Loss: 0.00020191289440211856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4603 lambda_n: 0.9511418626851427 Loss: 0.0002019129112967927\n",
      "Iteration: 4604 lambda_n: 0.926333213850323 Loss: 0.00020191292870217585\n",
      "Iteration: 4605 lambda_n: 1.0355561835060434 Loss: 0.00020191294550676975\n",
      "Iteration: 4606 lambda_n: 0.9836777618729701 Loss: 0.00020191296413435802\n",
      "Iteration: 4607 lambda_n: 0.9666004171221176 Loss: 0.00020191298166195908\n",
      "Iteration: 4608 lambda_n: 0.8885095825157896 Loss: 0.00020191299873105912\n",
      "Iteration: 4609 lambda_n: 0.9429215951960496 Loss: 0.00020191301428314443\n",
      "Iteration: 4610 lambda_n: 0.9943181035148083 Loss: 0.0002019130306542208\n",
      "Iteration: 4611 lambda_n: 0.8959924073424028 Loss: 0.00020191304776955805\n",
      "Iteration: 4612 lambda_n: 0.9891511165894842 Loss: 0.0002019130630528975\n",
      "Iteration: 4613 lambda_n: 0.9811730794001353 Loss: 0.00020191307978776436\n",
      "Iteration: 4614 lambda_n: 0.9750046983252267 Loss: 0.00020191309623832646\n",
      "Iteration: 4615 lambda_n: 0.9031669129873318 Loss: 0.0002019131124396063\n",
      "Iteration: 4616 lambda_n: 0.9654345025479744 Loss: 0.0002019131273141483\n",
      "Iteration: 4617 lambda_n: 0.9177430882491583 Loss: 0.00020191314308364567\n",
      "Iteration: 4618 lambda_n: 1.0172552210525712 Loss: 0.0002019131579425848\n",
      "Iteration: 4619 lambda_n: 0.9103740690259718 Loss: 0.0002019131742753392\n",
      "Iteration: 4620 lambda_n: 0.9822793000374519 Loss: 0.00020191318875692813\n",
      "Iteration: 4621 lambda_n: 0.9198590075611305 Loss: 0.00020191320425306283\n",
      "Iteration: 4622 lambda_n: 0.9081718927422168 Loss: 0.00020191321863496098\n",
      "Iteration: 4623 lambda_n: 0.9438184444144115 Loss: 0.00020191323271550563\n",
      "Iteration: 4624 lambda_n: 0.9909284099679266 Loss: 0.00020191324722799946\n",
      "Iteration: 4625 lambda_n: 0.9133516014198033 Loss: 0.00020191326233426116\n",
      "Iteration: 4626 lambda_n: 0.9968502062619762 Loss: 0.0002019132761326156\n",
      "Iteration: 4627 lambda_n: 0.9069214349569313 Loss: 0.0002019132910675232\n",
      "Iteration: 4628 lambda_n: 0.9462268274172473 Loss: 0.00020191330453216098\n",
      "Iteration: 4629 lambda_n: 0.8839701760640102 Loss: 0.00020191331846470466\n",
      "Iteration: 4630 lambda_n: 0.9412780454455729 Loss: 0.0002019133313687922\n",
      "Iteration: 4631 lambda_n: 0.8967735774211786 Loss: 0.00020191334499923235\n",
      "Iteration: 4632 lambda_n: 0.9916707053347924 Loss: 0.00020191335787429882\n",
      "Iteration: 4633 lambda_n: 0.983596012159515 Loss: 0.00020191337199597682\n",
      "Iteration: 4634 lambda_n: 0.9716063456637377 Loss: 0.0002019133858766521\n",
      "Iteration: 4635 lambda_n: 0.945359818449126 Loss: 0.00020191339946580445\n",
      "Iteration: 4636 lambda_n: 1.014096740770643 Loss: 0.00020191341257137034\n",
      "Iteration: 4637 lambda_n: 0.9922660786594761 Loss: 0.00020191342650929987\n",
      "Iteration: 4638 lambda_n: 0.9775704893166995 Loss: 0.0002019134400218338\n",
      "Iteration: 4639 lambda_n: 1.01431091122912 Loss: 0.00020191345321450313\n",
      "Iteration: 4640 lambda_n: 0.9524254820081627 Loss: 0.0002019134667817071\n",
      "Iteration: 4641 lambda_n: 0.9270020449269845 Loss: 0.00020191347940404125\n",
      "Iteration: 4642 lambda_n: 0.9084860347147035 Loss: 0.00020191349158341275\n",
      "Iteration: 4643 lambda_n: 0.9149390419683265 Loss: 0.00020191350341927978\n",
      "Iteration: 4644 lambda_n: 0.9833729088430152 Loss: 0.00020191351524110685\n",
      "Iteration: 4645 lambda_n: 1.0008158636779159 Loss: 0.0002019135278418695\n",
      "Iteration: 4646 lambda_n: 0.9668864844401583 Loss: 0.00020191354055192968\n",
      "Iteration: 4647 lambda_n: 0.9852744859351377 Loss: 0.0002019135527198176\n",
      "Iteration: 4648 lambda_n: 0.9808988079683333 Loss: 0.0002019135650105554\n",
      "Iteration: 4649 lambda_n: 0.945080180070431 Loss: 0.00020191357713756996\n",
      "Iteration: 4650 lambda_n: 0.9499306867894403 Loss: 0.00020191358871801348\n",
      "Iteration: 4651 lambda_n: 0.8939998491516244 Loss: 0.00020191360025834646\n",
      "Iteration: 4652 lambda_n: 0.9761844462905813 Loss: 0.00020191361102583766\n",
      "Iteration: 4653 lambda_n: 0.9147096610338613 Loss: 0.00020191362268808124\n",
      "Iteration: 4654 lambda_n: 0.8980888029411577 Loss: 0.00020191363351940693\n",
      "Iteration: 4655 lambda_n: 1.0175065781151644 Loss: 0.00020191364406593336\n",
      "Iteration: 4656 lambda_n: 0.9423323535308028 Loss: 0.00020191365591775\n",
      "Iteration: 4657 lambda_n: 1.0161887911787157 Loss: 0.00020191366679294805\n",
      "Iteration: 4658 lambda_n: 1.0376141349329573 Loss: 0.00020191367842060224\n",
      "Iteration: 4659 lambda_n: 0.9516391665124203 Loss: 0.0002019136901843415\n",
      "Iteration: 4660 lambda_n: 0.891291028192394 Loss: 0.00020191370087215975\n",
      "Iteration: 4661 lambda_n: 0.9959909546895743 Loss: 0.00020191371079614093\n",
      "Iteration: 4662 lambda_n: 0.93860279721374 Loss: 0.00020191372179657345\n",
      "Iteration: 4663 lambda_n: 1.0323244890904308 Loss: 0.00020191373206987465\n",
      "Iteration: 4664 lambda_n: 1.0132909874068836 Loss: 0.00020191374327318854\n",
      "Iteration: 4665 lambda_n: 0.9377603235961182 Loss: 0.0002019137541674048\n",
      "Iteration: 4666 lambda_n: 1.0364395470792924 Loss: 0.0002019137641572999\n",
      "Iteration: 4667 lambda_n: 0.9863823577003733 Loss: 0.00020191377510494498\n",
      "Iteration: 4668 lambda_n: 0.9278496709059877 Loss: 0.00020191378542636093\n",
      "Iteration: 4669 lambda_n: 0.90401373228761 Loss: 0.00020191379504885104\n",
      "Iteration: 4670 lambda_n: 1.0292994706921326 Loss: 0.00020191380434564208\n",
      "Iteration: 4671 lambda_n: 0.9111263488044553 Loss: 0.0002019138148444973\n",
      "Iteration: 4672 lambda_n: 1.0374590732878193 Loss: 0.00020191382405168246\n",
      "Iteration: 4673 lambda_n: 0.966838735227921 Loss: 0.00020191383444931272\n",
      "Iteration: 4674 lambda_n: 0.9938809222384016 Loss: 0.00020191384404849713\n",
      "Iteration: 4675 lambda_n: 0.9967978981299245 Loss: 0.00020191385383010998\n",
      "Iteration: 4676 lambda_n: 0.907640681142816 Loss: 0.00020191386355250838\n",
      "Iteration: 4677 lambda_n: 1.0185105282973208 Loss: 0.00020191387232572944\n",
      "Iteration: 4678 lambda_n: 1.0081660469879123 Loss: 0.00020191388209008014\n",
      "Iteration: 4679 lambda_n: 0.9268389899852292 Loss: 0.00020191389166654048\n",
      "Iteration: 4680 lambda_n: 0.9401478956125874 Loss: 0.00020191390039050113\n",
      "Iteration: 4681 lambda_n: 0.9391498445273903 Loss: 0.00020191390916582142\n",
      "Iteration: 4682 lambda_n: 1.0000251306884438 Loss: 0.00020191391785757962\n",
      "Iteration: 4683 lambda_n: 0.9300650724559169 Loss: 0.00020191392703443814\n",
      "Iteration: 4684 lambda_n: 1.0256342972426236 Loss: 0.00020191393549242966\n",
      "Iteration: 4685 lambda_n: 1.00691928399066 Loss: 0.0002019139447414022\n",
      "Iteration: 4686 lambda_n: 0.995193500422238 Loss: 0.00020191395373775483\n",
      "Iteration: 4687 lambda_n: 0.9252697734909596 Loss: 0.000201913962548738\n",
      "Iteration: 4688 lambda_n: 0.9012610094891245 Loss: 0.00020191397066727262\n",
      "Iteration: 4689 lambda_n: 1.0149949847412967 Loss: 0.0002019139785092979\n",
      "Iteration: 4690 lambda_n: 0.9808048939842713 Loss: 0.00020191398726932466\n",
      "Iteration: 4691 lambda_n: 1.0064622378176096 Loss: 0.00020191399565696623\n",
      "Iteration: 4692 lambda_n: 0.9355994818313031 Loss: 0.00020191400418808196\n",
      "Iteration: 4693 lambda_n: 1.0204219595563755 Loss: 0.0002019140120467599\n",
      "Iteration: 4694 lambda_n: 1.0353086272646719 Loss: 0.00020191402054580335\n",
      "Iteration: 4695 lambda_n: 1.0210943220773074 Loss: 0.00020191402908971401\n",
      "Iteration: 4696 lambda_n: 0.8985699501147156 Loss: 0.00020191403743789467\n",
      "Iteration: 4697 lambda_n: 0.9980829796766246 Loss: 0.00020191404471692792\n",
      "Iteration: 4698 lambda_n: 1.0147135316164566 Loss: 0.00020191405273679175\n",
      "Iteration: 4699 lambda_n: 0.9375206100833511 Loss: 0.00020191406081715787\n",
      "Iteration: 4700 lambda_n: 0.9683100809344599 Loss: 0.00020191406821475717\n",
      "Iteration: 4701 lambda_n: 0.9624882167603533 Loss: 0.00020191407579096154\n",
      "Iteration: 4702 lambda_n: 0.9844321216018306 Loss: 0.00020191408325612018\n",
      "Iteration: 4703 lambda_n: 0.9270830892296917 Loss: 0.00020191409082549004\n",
      "Iteration: 4704 lambda_n: 0.9029337847522674 Loss: 0.0002019140978908983\n",
      "Iteration: 4705 lambda_n: 0.9625071771169929 Loss: 0.00020191410471498978\n",
      "Iteration: 4706 lambda_n: 0.986612843099429 Loss: 0.00020191411193036472\n",
      "Iteration: 4707 lambda_n: 1.0270488153430561 Loss: 0.00020191411926255895\n",
      "Iteration: 4708 lambda_n: 0.9321892773878075 Loss: 0.00020191412682768516\n",
      "Iteration: 4709 lambda_n: 0.9120404368650918 Loss: 0.0002019141336308266\n",
      "Iteration: 4710 lambda_n: 0.9309585116705814 Loss: 0.00020191414023126177\n",
      "Iteration: 4711 lambda_n: 0.9664251290641999 Loss: 0.00020191414691350066\n",
      "Iteration: 4712 lambda_n: 0.8937198012873869 Loss: 0.00020191415379240213\n",
      "Iteration: 4713 lambda_n: 0.9354021884308998 Loss: 0.00020191416009867976\n",
      "Iteration: 4714 lambda_n: 0.9351892635273904 Loss: 0.00020191416664617535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4715 lambda_n: 0.952507804836216 Loss: 0.0002019141731372935\n",
      "Iteration: 4716 lambda_n: 0.949960870135584 Loss: 0.00020191417969320298\n",
      "Iteration: 4717 lambda_n: 1.0347583939040437 Loss: 0.00020191418617579131\n",
      "Iteration: 4718 lambda_n: 0.9214208754883243 Loss: 0.00020191419317696559\n",
      "Iteration: 4719 lambda_n: 0.9796243771891294 Loss: 0.00020191419935351048\n",
      "Iteration: 4720 lambda_n: 0.9627947762616149 Loss: 0.00020191420586602345\n",
      "Iteration: 4721 lambda_n: 0.8866108342830424 Loss: 0.0002019142122105166\n",
      "Iteration: 4722 lambda_n: 0.9387504406201848 Loss: 0.00020191421800261502\n",
      "Iteration: 4723 lambda_n: 0.9700041343465835 Loss: 0.00020191422408666514\n",
      "Iteration: 4724 lambda_n: 0.900265434732586 Loss: 0.00020191423032045412\n",
      "Iteration: 4725 lambda_n: 0.9954662711737378 Loss: 0.00020191423605583938\n",
      "Iteration: 4726 lambda_n: 0.8972834415460851 Loss: 0.00020191424234664346\n",
      "Iteration: 4727 lambda_n: 1.0020225501331446 Loss: 0.0002019142479664848\n",
      "Iteration: 4728 lambda_n: 0.9289139726027418 Loss: 0.0002019142541919609\n",
      "Iteration: 4729 lambda_n: 0.973627408825092 Loss: 0.0002019142599115104\n",
      "Iteration: 4730 lambda_n: 0.9309273410359192 Loss: 0.00020191426585658347\n",
      "Iteration: 4731 lambda_n: 0.9807269171737542 Loss: 0.0002019142714914432\n",
      "Iteration: 4732 lambda_n: 0.9838710936882273 Loss: 0.00020191427737834258\n",
      "Iteration: 4733 lambda_n: 1.0250480914529667 Loss: 0.00020191428323234996\n",
      "Iteration: 4734 lambda_n: 0.9454714539876595 Loss: 0.00020191428927773738\n",
      "Iteration: 4735 lambda_n: 1.0053398305636019 Loss: 0.00020191429480274754\n",
      "Iteration: 4736 lambda_n: 0.9142495039406288 Loss: 0.0002019143006279943\n",
      "Iteration: 4737 lambda_n: 1.0075943018730373 Loss: 0.00020191430587787748\n",
      "Iteration: 4738 lambda_n: 0.8979352313964571 Loss: 0.00020191431161653307\n",
      "Iteration: 4739 lambda_n: 0.9458919485201749 Loss: 0.00020191431668463807\n",
      "Iteration: 4740 lambda_n: 0.940386171099533 Loss: 0.0002019143219806209\n",
      "Iteration: 4741 lambda_n: 0.9979533091612207 Loss: 0.00020191432720133726\n",
      "Iteration: 4742 lambda_n: 0.9988669988471841 Loss: 0.00020191433269515073\n",
      "Iteration: 4743 lambda_n: 0.9673702339018986 Loss: 0.00020191433814503859\n",
      "Iteration: 4744 lambda_n: 1.0259201774700673 Loss: 0.0002019143433760494\n",
      "Iteration: 4745 lambda_n: 0.9369994832325772 Loss: 0.00020191434887580596\n",
      "Iteration: 4746 lambda_n: 0.9853961457215398 Loss: 0.0002019143538529174\n",
      "Iteration: 4747 lambda_n: 0.9335157492442467 Loss: 0.00020191435904337896\n",
      "Iteration: 4748 lambda_n: 0.9001128703642414 Loss: 0.00020191436391736947\n",
      "Iteration: 4749 lambda_n: 0.945431326245789 Loss: 0.00020191436857786577\n",
      "Iteration: 4750 lambda_n: 0.8886862055796945 Loss: 0.00020191437343373918\n",
      "Iteration: 4751 lambda_n: 1.0124916572194467 Loss: 0.00020191437795971388\n",
      "Iteration: 4752 lambda_n: 1.0081746651317391 Loss: 0.00020191438307539482\n",
      "Iteration: 4753 lambda_n: 0.8926677678172752 Loss: 0.00020191438812332927\n",
      "Iteration: 4754 lambda_n: 0.8926694483393824 Loss: 0.0002019143925527911\n",
      "Iteration: 4755 lambda_n: 1.0233869936508362 Loss: 0.00020191439694706192\n",
      "Iteration: 4756 lambda_n: 1.0148050778019408 Loss: 0.00020191440194476792\n",
      "Iteration: 4757 lambda_n: 0.8995531747112168 Loss: 0.00020191440685542704\n",
      "Iteration: 4758 lambda_n: 1.00300350668405 Loss: 0.00020191441116906796\n",
      "Iteration: 4759 lambda_n: 0.963004089706853 Loss: 0.00020191441594029412\n",
      "Iteration: 4760 lambda_n: 0.9376859011689109 Loss: 0.00020191442048037077\n",
      "Iteration: 4761 lambda_n: 0.9903744961731128 Loss: 0.00020191442486322642\n",
      "Iteration: 4762 lambda_n: 0.9776644498981064 Loss: 0.00020191442945375406\n",
      "Iteration: 4763 lambda_n: 0.9013151835381283 Loss: 0.00020191443394547139\n",
      "Iteration: 4764 lambda_n: 1.0181449051970455 Loss: 0.00020191443805041664\n",
      "Iteration: 4765 lambda_n: 0.9037892147185507 Loss: 0.00020191444265031666\n",
      "Iteration: 4766 lambda_n: 0.9727880355832821 Loss: 0.0002019144466966203\n",
      "Iteration: 4767 lambda_n: 0.9470941588924341 Loss: 0.00020191445101686877\n",
      "Iteration: 4768 lambda_n: 0.9936805028469547 Loss: 0.00020191445518665595\n",
      "Iteration: 4769 lambda_n: 1.0307017434526604 Loss: 0.00020191445952474874\n",
      "Iteration: 4770 lambda_n: 0.9761000125910306 Loss: 0.00020191446398476205\n",
      "Iteration: 4771 lambda_n: 0.9058665248317052 Loss: 0.00020191446816985171\n",
      "Iteration: 4772 lambda_n: 1.0044975852137026 Loss: 0.00020191447202016314\n",
      "Iteration: 4773 lambda_n: 0.933745228809808 Loss: 0.00020191447625537333\n",
      "Iteration: 4774 lambda_n: 0.9542524441232779 Loss: 0.00020191448015719003\n",
      "Iteration: 4775 lambda_n: 0.932186699601481 Loss: 0.0002019144841116612\n",
      "Iteration: 4776 lambda_n: 0.8969123226448724 Loss: 0.0002019144879419922\n",
      "Iteration: 4777 lambda_n: 0.9364222694816157 Loss: 0.00020191449159691403\n",
      "Iteration: 4778 lambda_n: 0.9702986247273923 Loss: 0.0002019144953824978\n",
      "Iteration: 4779 lambda_n: 0.9376310438869577 Loss: 0.00020191449927245975\n",
      "Iteration: 4780 lambda_n: 0.9639006428688321 Loss: 0.00020191450299912695\n",
      "Iteration: 4781 lambda_n: 0.9148397208744246 Loss: 0.00020191450679837207\n",
      "Iteration: 4782 lambda_n: 1.00892396566514 Loss: 0.00020191451037344205\n",
      "Iteration: 4783 lambda_n: 1.0099699445844612 Loss: 0.00020191451428423464\n",
      "Iteration: 4784 lambda_n: 0.9755012635236616 Loss: 0.0002019145181640982\n",
      "Iteration: 4785 lambda_n: 0.9529162352617278 Loss: 0.00020191452187802726\n",
      "Iteration: 4786 lambda_n: 1.004061639092595 Loss: 0.0002019145254746402\n",
      "Iteration: 4787 lambda_n: 0.8915576250295918 Loss: 0.0002019145292323219\n",
      "Iteration: 4788 lambda_n: 1.0350119484270963 Loss: 0.0002019145325393096\n",
      "Iteration: 4789 lambda_n: 0.9576475017574891 Loss: 0.00020191453634810835\n",
      "Iteration: 4790 lambda_n: 1.0300300025554674 Loss: 0.00020191453983994085\n",
      "Iteration: 4791 lambda_n: 1.0221415040748152 Loss: 0.00020191454356389518\n",
      "Iteration: 4792 lambda_n: 0.9637155548194384 Loss: 0.00020191454722565287\n",
      "Iteration: 4793 lambda_n: 1.0230175173888088 Loss: 0.0002019145506469065\n",
      "Iteration: 4794 lambda_n: 0.90786183598961 Loss: 0.00020191455424774433\n",
      "Iteration: 4795 lambda_n: 0.9703451325039846 Loss: 0.00020191455741436554\n",
      "Iteration: 4796 lambda_n: 1.0067140784096262 Loss: 0.00020191456077176638\n",
      "Iteration: 4797 lambda_n: 1.0220185936925317 Loss: 0.0002019145642251407\n",
      "Iteration: 4798 lambda_n: 0.9354835925725462 Loss: 0.00020191456769984932\n",
      "Iteration: 4799 lambda_n: 0.9119790958643155 Loss: 0.00020191457085163422\n",
      "Iteration: 4800 lambda_n: 1.015365937490892 Loss: 0.00020191457389884384\n",
      "Iteration: 4801 lambda_n: 0.8952968064830618 Loss: 0.00020191457726420004\n",
      "Iteration: 4802 lambda_n: 0.9372426450457951 Loss: 0.00020191458020499777\n",
      "Iteration: 4803 lambda_n: 0.8869179648682084 Loss: 0.00020191458325924863\n",
      "Iteration: 4804 lambda_n: 0.9444844617839441 Loss: 0.00020191458612560487\n",
      "Iteration: 4805 lambda_n: 1.0177150056170903 Loss: 0.0002019145891541234\n",
      "Iteration: 4806 lambda_n: 0.993987393833176 Loss: 0.00020191459239027323\n",
      "Iteration: 4807 lambda_n: 0.9518541003420821 Loss: 0.00020191459552259915\n",
      "Iteration: 4808 lambda_n: 0.9409279020892539 Loss: 0.0002019145984958663\n",
      "Iteration: 4809 lambda_n: 0.9721219357047693 Loss: 0.00020191460141034005\n",
      "Iteration: 4810 lambda_n: 0.9917489653252543 Loss: 0.000201914604396469\n",
      "Iteration: 4811 lambda_n: 0.9270560580675659 Loss: 0.00020191460741679154\n",
      "Iteration: 4812 lambda_n: 0.9625695591009399 Loss: 0.0002019146102154327\n",
      "Iteration: 4813 lambda_n: 0.9623547085092229 Loss: 0.00020191461309754697\n",
      "Iteration: 4814 lambda_n: 0.9362696953217146 Loss: 0.00020191461595460544\n",
      "Iteration: 4815 lambda_n: 1.0087307364244327 Loss: 0.00020191461871066048\n",
      "Iteration: 4816 lambda_n: 0.9168683724703015 Loss: 0.00020191462165554668\n",
      "Iteration: 4817 lambda_n: 0.9192391583952467 Loss: 0.00020191462430848504\n",
      "Iteration: 4818 lambda_n: 0.98872120911081 Loss: 0.00020191462694682036\n",
      "Iteration: 4819 lambda_n: 0.985972980804738 Loss: 0.00020191462976163857\n",
      "Iteration: 4820 lambda_n: 0.9576437659330025 Loss: 0.0002019146325442116\n",
      "Iteration: 4821 lambda_n: 0.947667372689547 Loss: 0.00020191463522340302\n",
      "Iteration: 4822 lambda_n: 0.8867819945398678 Loss: 0.00020191463785235793\n",
      "Iteration: 4823 lambda_n: 1.0210408516193747 Loss: 0.00020191464029191322\n",
      "Iteration: 4824 lambda_n: 0.8895260853496105 Loss: 0.00020191464307893077\n",
      "Iteration: 4825 lambda_n: 0.8931704023481585 Loss: 0.0002019146454851773\n",
      "Iteration: 4826 lambda_n: 1.0313011675200472 Loss: 0.0002019146478824006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4827 lambda_n: 0.8995694726859981 Loss: 0.00020191465062863997\n",
      "Iteration: 4828 lambda_n: 0.8959606599903374 Loss: 0.00020191465300239736\n",
      "Iteration: 4829 lambda_n: 0.9643272868242716 Loss: 0.00020191465534795438\n",
      "Iteration: 4830 lambda_n: 0.909763352360921 Loss: 0.0002019146578526434\n",
      "Iteration: 4831 lambda_n: 0.9197580375653789 Loss: 0.00020191466019561053\n",
      "Iteration: 4832 lambda_n: 0.8829973377080269 Loss: 0.00020191466254541082\n",
      "Iteration: 4833 lambda_n: 0.9457499660237035 Loss: 0.00020191466478307714\n",
      "Iteration: 4834 lambda_n: 1.0080388894872994 Loss: 0.00020191466716121224\n",
      "Iteration: 4835 lambda_n: 0.9815632387285774 Loss: 0.0002019146696749478\n",
      "Iteration: 4836 lambda_n: 0.9338087853120706 Loss: 0.00020191467210103052\n",
      "Iteration: 4837 lambda_n: 0.9502974187677271 Loss: 0.00020191467438921833\n",
      "Iteration: 4838 lambda_n: 0.9837458763231548 Loss: 0.00020191467669875655\n",
      "Iteration: 4839 lambda_n: 0.9040124648898641 Loss: 0.00020191467906966552\n",
      "Iteration: 4840 lambda_n: 1.0309021708162618 Loss: 0.000201914681229638\n",
      "Iteration: 4841 lambda_n: 0.9123699988855438 Loss: 0.00020191468367328455\n",
      "Iteration: 4842 lambda_n: 0.9713070495916126 Loss: 0.00020191468581643755\n",
      "Iteration: 4843 lambda_n: 1.0324232823957389 Loss: 0.0002019146880798037\n",
      "Iteration: 4844 lambda_n: 0.9920042711106642 Loss: 0.00020191469046512925\n",
      "Iteration: 4845 lambda_n: 1.0028387104324081 Loss: 0.00020191469273636751\n",
      "Iteration: 4846 lambda_n: 0.9525917194866981 Loss: 0.00020191469501248282\n",
      "Iteration: 4847 lambda_n: 0.9298589287769631 Loss: 0.00020191469715557852\n",
      "Iteration: 4848 lambda_n: 0.9269697362392828 Loss: 0.00020191469923009448\n",
      "Iteration: 4849 lambda_n: 0.9477841828687407 Loss: 0.00020191470128134713\n",
      "Iteration: 4850 lambda_n: 0.9605259888660617 Loss: 0.00020191470336167137\n",
      "Iteration: 4851 lambda_n: 0.9030566511018889 Loss: 0.0002019147054524881\n",
      "Iteration: 4852 lambda_n: 1.024552909471589 Loss: 0.00020191470740170935\n",
      "Iteration: 4853 lambda_n: 0.9817602737251333 Loss: 0.00020191470959572997\n",
      "Iteration: 4854 lambda_n: 0.9845668336751536 Loss: 0.00020191471167928964\n",
      "Iteration: 4855 lambda_n: 1.0340175687823616 Loss: 0.00020191471375088785\n",
      "Iteration: 4856 lambda_n: 0.8854569491092568 Loss: 0.00020191471590783828\n",
      "Iteration: 4857 lambda_n: 0.8887653984253657 Loss: 0.0002019147177382157\n",
      "Iteration: 4858 lambda_n: 0.9578051389929915 Loss: 0.00020191471956123845\n",
      "Iteration: 4859 lambda_n: 0.997209599856267 Loss: 0.00020191472151062648\n",
      "Iteration: 4860 lambda_n: 0.9494788022102857 Loss: 0.00020191472352326236\n",
      "Iteration: 4861 lambda_n: 0.9894542003817978 Loss: 0.00020191472542288737\n",
      "Iteration: 4862 lambda_n: 1.0024143560445018 Loss: 0.00020191472738610489\n",
      "Iteration: 4863 lambda_n: 1.0305884363758278 Loss: 0.00020191472935787385\n",
      "Iteration: 4864 lambda_n: 0.9651021850648044 Loss: 0.00020191473136735273\n",
      "Iteration: 4865 lambda_n: 1.0167211795331592 Loss: 0.0002019147332322407\n",
      "Iteration: 4866 lambda_n: 0.9469286288905173 Loss: 0.0002019147351803528\n",
      "Iteration: 4867 lambda_n: 1.0325793349412868 Loss: 0.0002019147369786641\n",
      "Iteration: 4868 lambda_n: 0.929053742888947 Loss: 0.000201914738923464\n",
      "Iteration: 4869 lambda_n: 0.9559656083707585 Loss: 0.00020191474065754658\n",
      "Iteration: 4870 lambda_n: 0.921681078596705 Loss: 0.00020191474242741702\n",
      "Iteration: 4871 lambda_n: 0.9325113978020813 Loss: 0.00020191474411962898\n",
      "Iteration: 4872 lambda_n: 1.0016167133870348 Loss: 0.00020191474581799697\n",
      "Iteration: 4873 lambda_n: 0.9385166841134861 Loss: 0.0002019147476274241\n",
      "Iteration: 4874 lambda_n: 1.0082077752242629 Loss: 0.00020191474930808783\n",
      "Iteration: 4875 lambda_n: 0.9855437170746384 Loss: 0.00020191475109881583\n",
      "Iteration: 4876 lambda_n: 0.8951680722286274 Loss: 0.00020191475283394284\n",
      "Iteration: 4877 lambda_n: 0.9170589541621593 Loss: 0.0002019147543964505\n",
      "Iteration: 4878 lambda_n: 1.0243892944877753 Loss: 0.00020191475598471843\n",
      "Iteration: 4879 lambda_n: 1.0173516193647891 Loss: 0.0002019147577447363\n",
      "Iteration: 4880 lambda_n: 1.0221280532447503 Loss: 0.00020191475947711594\n",
      "Iteration: 4881 lambda_n: 0.9156852753863669 Loss: 0.0002019147612022429\n",
      "Iteration: 4882 lambda_n: 0.890411741278987 Loss: 0.0002019147627340031\n",
      "Iteration: 4883 lambda_n: 0.942047056668746 Loss: 0.0002019147642116447\n",
      "Iteration: 4884 lambda_n: 1.02014328694382 Loss: 0.00020191476576289402\n",
      "Iteration: 4885 lambda_n: 0.9858752487060385 Loss: 0.00020191476742901032\n",
      "Iteration: 4886 lambda_n: 1.0277696673986967 Loss: 0.00020191476902491046\n",
      "Iteration: 4887 lambda_n: 0.9828169308959405 Loss: 0.00020191477067440003\n",
      "Iteration: 4888 lambda_n: 0.993865929629759 Loss: 0.00020191477223769136\n",
      "Iteration: 4889 lambda_n: 1.0354375022930353 Loss: 0.00020191477380508338\n",
      "Iteration: 4890 lambda_n: 0.9633313789099909 Loss: 0.00020191477542396736\n",
      "Iteration: 4891 lambda_n: 0.9156114677417093 Loss: 0.0002019147769166023\n",
      "Iteration: 4892 lambda_n: 0.9560876645143174 Loss: 0.00020191477832345688\n",
      "Iteration: 4893 lambda_n: 0.9465591755513532 Loss: 0.00020191477978085908\n",
      "Iteration: 4894 lambda_n: 1.0354257629393682 Loss: 0.0002019147812117879\n",
      "Iteration: 4895 lambda_n: 1.0185486505697423 Loss: 0.00020191478276423272\n",
      "Iteration: 4896 lambda_n: 0.9959528559925346 Loss: 0.00020191478427768176\n",
      "Iteration: 4897 lambda_n: 0.9957300485499696 Loss: 0.00020191478574452682\n",
      "Iteration: 4898 lambda_n: 0.9983328102128755 Loss: 0.00020191478719840036\n",
      "Iteration: 4899 lambda_n: 1.0072455145115988 Loss: 0.00020191478864351972\n",
      "Iteration: 4900 lambda_n: 0.9447008593999535 Loss: 0.00020191479008895347\n",
      "Iteration: 4901 lambda_n: 0.9575368245876593 Loss: 0.0002019147914328309\n",
      "Iteration: 4902 lambda_n: 0.9326509244810101 Loss: 0.00020191479278384438\n",
      "Iteration: 4903 lambda_n: 1.0266477559416223 Loss: 0.00020191479408885352\n",
      "Iteration: 4904 lambda_n: 1.0291185231948015 Loss: 0.0002019147955138145\n",
      "Iteration: 4905 lambda_n: 0.9186892731906864 Loss: 0.00020191479692954352\n",
      "Iteration: 4906 lambda_n: 0.9465178307776735 Loss: 0.00020191479818212253\n",
      "Iteration: 4907 lambda_n: 0.9662036037582586 Loss: 0.00020191479946241367\n",
      "Iteration: 4908 lambda_n: 0.9846930795939689 Loss: 0.00020191480075864587\n",
      "Iteration: 4909 lambda_n: 0.9785301282633849 Loss: 0.0002019148020686706\n",
      "Iteration: 4910 lambda_n: 1.002519767598773 Loss: 0.00020191480335943706\n",
      "Iteration: 4911 lambda_n: 0.9347723293413817 Loss: 0.00020191480467069602\n",
      "Iteration: 4912 lambda_n: 0.8898393215626396 Loss: 0.00020191480588276233\n",
      "Iteration: 4913 lambda_n: 0.9631008208532441 Loss: 0.00020191480702727433\n",
      "Iteration: 4914 lambda_n: 1.0285078436912245 Loss: 0.0002019148082565191\n",
      "Iteration: 4915 lambda_n: 0.8938577874989958 Loss: 0.0002019148095583359\n",
      "Iteration: 4916 lambda_n: 0.8881790788793954 Loss: 0.00020191481067970967\n",
      "Iteration: 4917 lambda_n: 1.0105203117324713 Loss: 0.0002019148117853747\n",
      "Iteration: 4918 lambda_n: 0.9638784889271085 Loss: 0.00020191481303371517\n",
      "Iteration: 4919 lambda_n: 0.8898681418988476 Loss: 0.00020191481421407708\n",
      "Iteration: 4920 lambda_n: 0.9262533459179053 Loss: 0.00020191481529477088\n",
      "Iteration: 4921 lambda_n: 0.8961269817466363 Loss: 0.00020191481641103912\n",
      "Iteration: 4922 lambda_n: 0.9901148375272871 Loss: 0.00020191481748239754\n",
      "Iteration: 4923 lambda_n: 1.0376311830989982 Loss: 0.00020191481865699517\n",
      "Iteration: 4924 lambda_n: 0.9067709918545012 Loss: 0.00020191481987747118\n",
      "Iteration: 4925 lambda_n: 0.9936489467624422 Loss: 0.0002019148209345189\n",
      "Iteration: 4926 lambda_n: 0.9732871619309272 Loss: 0.00020191482208381544\n",
      "Iteration: 4927 lambda_n: 0.9449575833413509 Loss: 0.00020191482319994837\n",
      "Iteration: 4928 lambda_n: 0.9971975007328941 Loss: 0.00020191482427452784\n",
      "Iteration: 4929 lambda_n: 0.9720887350793227 Loss: 0.00020191482539930856\n",
      "Iteration: 4930 lambda_n: 0.9062414641150954 Loss: 0.00020191482648638273\n",
      "Iteration: 4931 lambda_n: 0.9751233005317839 Loss: 0.0002019148274913629\n",
      "Iteration: 4932 lambda_n: 0.9571555180194055 Loss: 0.0002019148285643173\n",
      "Iteration: 4933 lambda_n: 1.0203393436312982 Loss: 0.00020191482960869254\n",
      "Iteration: 4934 lambda_n: 0.9048152098375868 Loss: 0.00020191483071286504\n",
      "Iteration: 4935 lambda_n: 0.9217280808752409 Loss: 0.0002019148316834453\n",
      "Iteration: 4936 lambda_n: 0.9918255686416608 Loss: 0.00020191483266450688\n",
      "Iteration: 4937 lambda_n: 0.9730017494115764 Loss: 0.000201914833711837\n",
      "Iteration: 4938 lambda_n: 0.9096052345576104 Loss: 0.0002019148347305497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4939 lambda_n: 1.008462244544534 Loss: 0.0002019148356749494\n",
      "Iteration: 4940 lambda_n: 0.9534043697328632 Loss: 0.00020191483671383478\n",
      "Iteration: 4941 lambda_n: 0.9919642673353721 Loss: 0.00020191483768750606\n",
      "Iteration: 4942 lambda_n: 0.9369557200946422 Loss: 0.00020191483869229197\n",
      "Iteration: 4943 lambda_n: 1.0072266605680549 Loss: 0.00020191483963330175\n",
      "Iteration: 4944 lambda_n: 0.9041577090117761 Loss: 0.000201914840636771\n",
      "Iteration: 4945 lambda_n: 0.9746881566816886 Loss: 0.00020191484152979214\n",
      "Iteration: 4946 lambda_n: 1.012233330825428 Loss: 0.0002019148424850312\n",
      "Iteration: 4947 lambda_n: 0.9051955728319915 Loss: 0.0002019148434687962\n",
      "Iteration: 4948 lambda_n: 0.8875448628637692 Loss: 0.0002019148443409232\n",
      "Iteration: 4949 lambda_n: 0.970215669219938 Loss: 0.00020191484518941838\n",
      "Iteration: 4950 lambda_n: 0.9910643567626637 Loss: 0.0002019148461099153\n",
      "Iteration: 4951 lambda_n: 0.9841702676482098 Loss: 0.00020191484704240373\n",
      "Iteration: 4952 lambda_n: 0.9295997004159983 Loss: 0.00020191484796056524\n",
      "Iteration: 4953 lambda_n: 0.9104877834254573 Loss: 0.00020191484882052944\n",
      "Iteration: 4954 lambda_n: 0.9800200919001237 Loss: 0.0002019148496561264\n",
      "Iteration: 4955 lambda_n: 0.8845425267114005 Loss: 0.00020191485054854496\n",
      "Iteration: 4956 lambda_n: 0.9251927862141407 Loss: 0.00020191485134727803\n",
      "Iteration: 4957 lambda_n: 0.9551462045841883 Loss: 0.00020191485217642245\n",
      "Iteration: 4958 lambda_n: 1.0126409894747226 Loss: 0.00020191485302565245\n",
      "Iteration: 4959 lambda_n: 0.9290911497847362 Loss: 0.00020191485391866888\n",
      "Iteration: 4960 lambda_n: 0.8897320688921092 Loss: 0.00020191485473093144\n",
      "Iteration: 4961 lambda_n: 0.9379131262691491 Loss: 0.00020191485550263022\n",
      "Iteration: 4962 lambda_n: 0.9956709969502658 Loss: 0.00020191485630994974\n",
      "Iteration: 4963 lambda_n: 1.009672823318973 Loss: 0.00020191485716013294\n",
      "Iteration: 4964 lambda_n: 0.9474202087862709 Loss: 0.00020191485801496293\n",
      "Iteration: 4965 lambda_n: 0.9690217181535193 Loss: 0.00020191485881018736\n",
      "Iteration: 4966 lambda_n: 0.9957666923161979 Loss: 0.00020191485961698703\n",
      "Iteration: 4967 lambda_n: 0.9390727637524136 Loss: 0.0002019148604392146\n",
      "Iteration: 4968 lambda_n: 0.9687744067126564 Loss: 0.00020191486120805697\n",
      "Iteration: 4969 lambda_n: 0.9906716990661149 Loss: 0.00020191486199487983\n",
      "Iteration: 4970 lambda_n: 1.0202038727034661 Loss: 0.00020191486279286965\n",
      "Iteration: 4971 lambda_n: 0.9310613267648189 Loss: 0.0002019148636077113\n",
      "Iteration: 4972 lambda_n: 0.9967434295941852 Loss: 0.00020191486434491398\n",
      "Iteration: 4973 lambda_n: 0.9600476977579232 Loss: 0.00020191486512787925\n",
      "Iteration: 4974 lambda_n: 1.0169295677456542 Loss: 0.00020191486587563276\n",
      "Iteration: 4975 lambda_n: 0.8977033712256561 Loss: 0.00020191486666122714\n",
      "Iteration: 4976 lambda_n: 0.8898589566346237 Loss: 0.0002019148673487311\n",
      "Iteration: 4977 lambda_n: 0.9894634712633686 Loss: 0.00020191486802503757\n",
      "Iteration: 4978 lambda_n: 1.0178353148411707 Loss: 0.00020191486877136245\n",
      "Iteration: 4979 lambda_n: 0.9168184916957667 Loss: 0.0002019148695326365\n",
      "Iteration: 4980 lambda_n: 0.9963572328969825 Loss: 0.00020191487021243627\n",
      "Iteration: 4981 lambda_n: 0.9300858722711925 Loss: 0.00020191487094547173\n",
      "Iteration: 4982 lambda_n: 0.9380277014582525 Loss: 0.00020191487162397455\n",
      "Iteration: 4983 lambda_n: 0.9065285418214692 Loss: 0.00020191487230287604\n",
      "Iteration: 4984 lambda_n: 0.9352956103110277 Loss: 0.0002019148729537622\n",
      "Iteration: 4985 lambda_n: 0.9323306141241794 Loss: 0.00020191487362014718\n",
      "Iteration: 4986 lambda_n: 0.9426433151060577 Loss: 0.00020191487427915957\n",
      "Iteration: 4987 lambda_n: 1.0279098645438802 Loss: 0.00020191487494019468\n",
      "Iteration: 4988 lambda_n: 0.9278235255497104 Loss: 0.00020191487565527475\n",
      "Iteration: 4989 lambda_n: 0.9990294129517419 Loss: 0.0002019148762951082\n",
      "Iteration: 4990 lambda_n: 1.0325121822280732 Loss: 0.00020191487697864474\n",
      "Iteration: 4991 lambda_n: 0.9762607060887728 Loss: 0.0002019148776791186\n",
      "Iteration: 4992 lambda_n: 1.0013642777818152 Loss: 0.00020191487833564584\n",
      "Iteration: 4993 lambda_n: 1.014639502164906 Loss: 0.000201914879003502\n",
      "Iteration: 4994 lambda_n: 0.9777904013198906 Loss: 0.0002019148796744856\n",
      "Iteration: 4995 lambda_n: 0.9379186801418813 Loss: 0.000201914880315553\n",
      "Iteration: 4996 lambda_n: 0.9396162739269442 Loss: 0.00020191488092540053\n",
      "Iteration: 4997 lambda_n: 0.9339790217744647 Loss: 0.000201914881531505\n",
      "Iteration: 4998 lambda_n: 1.024726635394957 Loss: 0.00020191488212919692\n",
      "Iteration: 4999 lambda_n: 0.9953612694981804 Loss: 0.00020191488277979567\n",
      "Iteration: 5000 lambda_n: 0.9495620854681903 Loss: 0.00020191488340628538\n",
      "Iteration: 5001 lambda_n: 0.9371118045196051 Loss: 0.00020191488399892146\n",
      "Iteration: 5002 lambda_n: 0.9416209600433868 Loss: 0.00020191488457910992\n",
      "Iteration: 5003 lambda_n: 0.9338047575989246 Loss: 0.00020191488515748797\n",
      "Iteration: 5004 lambda_n: 1.031534864703734 Loss: 0.00020191488572650636\n",
      "Iteration: 5005 lambda_n: 0.9392977600443747 Loss: 0.00020191488635013213\n",
      "Iteration: 5006 lambda_n: 1.003193820195485 Loss: 0.00020191488691305686\n",
      "Iteration: 5007 lambda_n: 0.9504397469255335 Loss: 0.00020191488750951486\n",
      "Iteration: 5008 lambda_n: 1.0032352329684073 Loss: 0.0002019148880698401\n",
      "Iteration: 5009 lambda_n: 0.9628621744064044 Loss: 0.0002019148886565555\n",
      "Iteration: 5010 lambda_n: 1.00093411749813 Loss: 0.0002019148892149034\n",
      "Iteration: 5011 lambda_n: 0.9765810875638796 Loss: 0.00020191488979063405\n",
      "Iteration: 5012 lambda_n: 0.9549741847789764 Loss: 0.0002019148903476241\n",
      "Iteration: 5013 lambda_n: 0.9120984257422942 Loss: 0.0002019148908878125\n",
      "Iteration: 5014 lambda_n: 0.9469444168995522 Loss: 0.0002019148913996097\n",
      "Iteration: 5015 lambda_n: 0.8927906458063208 Loss: 0.00020191489192688262\n",
      "Iteration: 5016 lambda_n: 0.8881277654386623 Loss: 0.00020191489242003832\n",
      "Iteration: 5017 lambda_n: 1.0299459602850731 Loss: 0.0002019148929069497\n",
      "Iteration: 5018 lambda_n: 0.9978507413739115 Loss: 0.00020191489346740068\n",
      "Iteration: 5019 lambda_n: 0.9057462171523168 Loss: 0.00020191489400568592\n",
      "Iteration: 5020 lambda_n: 0.9337642589182448 Loss: 0.0002019148944901967\n",
      "Iteration: 5021 lambda_n: 0.9766670331272544 Loss: 0.0002019148949858969\n",
      "Iteration: 5022 lambda_n: 0.8835684069980667 Loss: 0.00020191489550031662\n",
      "Iteration: 5023 lambda_n: 0.9590883136200103 Loss: 0.00020191489596187425\n",
      "Iteration: 5024 lambda_n: 0.9127052298244301 Loss: 0.00020191489645918014\n",
      "Iteration: 5025 lambda_n: 0.9716178622766036 Loss: 0.00020191489692862963\n",
      "Iteration: 5026 lambda_n: 0.9802144999606759 Loss: 0.00020191489742455937\n",
      "Iteration: 5027 lambda_n: 0.9229340034257979 Loss: 0.0002019148979208068\n",
      "Iteration: 5028 lambda_n: 0.9327038126788335 Loss: 0.0002019148983842195\n",
      "Iteration: 5029 lambda_n: 0.9352283587338839 Loss: 0.00020191489884891517\n",
      "Iteration: 5030 lambda_n: 1.0164431926641107 Loss: 0.00020191489931122884\n",
      "Iteration: 5031 lambda_n: 0.9602909230901547 Loss: 0.00020191489980975942\n",
      "Iteration: 5032 lambda_n: 0.9776052316093841 Loss: 0.0002019149002767433\n",
      "Iteration: 5033 lambda_n: 0.9299300933412078 Loss: 0.0002019149007483205\n",
      "Iteration: 5034 lambda_n: 1.0084310312842457 Loss: 0.0002019149011932371\n",
      "Iteration: 5035 lambda_n: 1.011958599601111 Loss: 0.00020191490167195553\n",
      "Iteration: 5036 lambda_n: 0.9796646276212374 Loss: 0.00020191490214830385\n",
      "Iteration: 5037 lambda_n: 0.9406150069490801 Loss: 0.0002019149026055461\n",
      "Iteration: 5038 lambda_n: 1.017433442380332 Loss: 0.00020191490304097927\n",
      "Iteration: 5039 lambda_n: 0.9043838163689039 Loss: 0.00020191490350826837\n",
      "Iteration: 5040 lambda_n: 0.9638908664180244 Loss: 0.00020191490392011105\n",
      "Iteration: 5041 lambda_n: 0.9555946439850391 Loss: 0.0002019149043557359\n",
      "Iteration: 5042 lambda_n: 1.0155849249193103 Loss: 0.00020191490478412922\n",
      "Iteration: 5043 lambda_n: 0.9296890457036261 Loss: 0.00020191490523579665\n",
      "Iteration: 5044 lambda_n: 1.021390808021333 Loss: 0.00020191490564576043\n",
      "Iteration: 5045 lambda_n: 1.0232836753026162 Loss: 0.00020191490609266916\n",
      "Iteration: 5046 lambda_n: 0.909022956999148 Loss: 0.00020191490653658737\n",
      "Iteration: 5047 lambda_n: 0.9030657015368576 Loss: 0.0002019149069275754\n",
      "Iteration: 5048 lambda_n: 0.9007112432544138 Loss: 0.0002019149073130694\n",
      "Iteration: 5049 lambda_n: 0.9738859664859321 Loss: 0.0002019149076946649\n",
      "Iteration: 5050 lambda_n: 0.9779231595405002 Loss: 0.00020191490810416422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5051 lambda_n: 0.8932642996434732 Loss: 0.0002019149085120269\n",
      "Iteration: 5052 lambda_n: 0.8946602801424222 Loss: 0.0002019149088815532\n",
      "Iteration: 5053 lambda_n: 1.0026733668686616 Loss: 0.0002019149092489035\n",
      "Iteration: 5054 lambda_n: 0.8902269573850992 Loss: 0.00020191490965753542\n",
      "Iteration: 5055 lambda_n: 0.9182156835873664 Loss: 0.00020191491001732073\n",
      "Iteration: 5056 lambda_n: 0.9669508487531076 Loss: 0.00020191491038566768\n",
      "Iteration: 5057 lambda_n: 1.0355737291336722 Loss: 0.00020191491077060496\n",
      "Iteration: 5058 lambda_n: 1.0114641710320142 Loss: 0.00020191491117954572\n",
      "Iteration: 5059 lambda_n: 1.0225551691697923 Loss: 0.00020191491157553107\n",
      "Iteration: 5060 lambda_n: 1.0126661626205524 Loss: 0.000201914911972493\n",
      "Iteration: 5061 lambda_n: 1.0103739594534118 Loss: 0.00020191491236228848\n",
      "Iteration: 5062 lambda_n: 1.0312001760008043 Loss: 0.0002019149127479345\n",
      "Iteration: 5063 lambda_n: 1.0075684850443647 Loss: 0.0002019149131382182\n",
      "Iteration: 5064 lambda_n: 1.0005129134867667 Loss: 0.00020191491351630444\n",
      "Iteration: 5065 lambda_n: 0.9307618826313796 Loss: 0.00020191491388860846\n",
      "Iteration: 5066 lambda_n: 1.0103869729001216 Loss: 0.00020191491423208007\n",
      "Iteration: 5067 lambda_n: 0.935594710390648 Loss: 0.00020191491460206515\n",
      "Iteration: 5068 lambda_n: 0.9224899505169812 Loss: 0.00020191491494178925\n",
      "Iteration: 5069 lambda_n: 0.9625638494015805 Loss: 0.0002019149152741541\n",
      "Iteration: 5070 lambda_n: 0.8953105624384219 Loss: 0.00020191491561832127\n",
      "Iteration: 5071 lambda_n: 0.9967652495185494 Loss: 0.0002019149159358911\n",
      "Iteration: 5072 lambda_n: 1.011119387688305 Loss: 0.00020191491628682452\n",
      "Iteration: 5073 lambda_n: 0.9841910704720906 Loss: 0.00020191491663988022\n",
      "Iteration: 5074 lambda_n: 1.0315823257081171 Loss: 0.00020191491698064604\n",
      "Iteration: 5075 lambda_n: 0.9305553066016218 Loss: 0.00020191491733491943\n",
      "Iteration: 5076 lambda_n: 1.00626931684678 Loss: 0.00020191491765176667\n",
      "Iteration: 5077 lambda_n: 0.9594584875439043 Loss: 0.00020191491799177402\n",
      "Iteration: 5078 lambda_n: 1.002616590834675 Loss: 0.00020191491831326216\n",
      "Iteration: 5079 lambda_n: 0.9467822442390054 Loss: 0.00020191491864655252\n",
      "Iteration: 5080 lambda_n: 1.0100895235136682 Loss: 0.00020191491895867031\n",
      "Iteration: 5081 lambda_n: 0.9969595936354766 Loss: 0.00020191491928906263\n",
      "Iteration: 5082 lambda_n: 1.0148321017090405 Loss: 0.00020191491961244599\n",
      "Iteration: 5083 lambda_n: 0.9784199040291325 Loss: 0.00020191491993892124\n",
      "Iteration: 5084 lambda_n: 1.032696387426611 Loss: 0.0002019149202510461\n",
      "Iteration: 5085 lambda_n: 1.0228430456264124 Loss: 0.0002019149205778223\n",
      "Iteration: 5086 lambda_n: 0.8860157556918619 Loss: 0.00020191492089872626\n",
      "Iteration: 5087 lambda_n: 0.9243090837363623 Loss: 0.00020191492117435316\n",
      "Iteration: 5088 lambda_n: 1.0288170518665731 Loss: 0.00020191492145980937\n",
      "Iteration: 5089 lambda_n: 0.9376486898699014 Loss: 0.00020191492177510766\n",
      "Iteration: 5090 lambda_n: 0.9305104847416331 Loss: 0.00020191492206004266\n",
      "Iteration: 5091 lambda_n: 0.9386558369310902 Loss: 0.00020191492234062164\n",
      "Iteration: 5092 lambda_n: 0.9082895931355737 Loss: 0.00020191492262148898\n",
      "Iteration: 5093 lambda_n: 0.9494771820787747 Loss: 0.00020191492289117904\n",
      "Iteration: 5094 lambda_n: 0.9595743769589534 Loss: 0.0002019149231709843\n",
      "Iteration: 5095 lambda_n: 0.988735770111252 Loss: 0.00020191492345156956\n",
      "Iteration: 5096 lambda_n: 0.8919220697447989 Loss: 0.000201914923738395\n",
      "Iteration: 5097 lambda_n: 0.8982618026868343 Loss: 0.00020191492399502766\n",
      "Iteration: 5098 lambda_n: 0.9864189198810664 Loss: 0.00020191492425159456\n",
      "Iteration: 5099 lambda_n: 0.9680190218551418 Loss: 0.00020191492453126698\n",
      "Iteration: 5100 lambda_n: 0.954663491687883 Loss: 0.0002019149248034921\n",
      "Iteration: 5101 lambda_n: 0.9435852689495671 Loss: 0.00020191492506983023\n",
      "Iteration: 5102 lambda_n: 0.9117794461111497 Loss: 0.00020191492533101833\n",
      "Iteration: 5103 lambda_n: 0.9259781664827715 Loss: 0.00020191492558144377\n",
      "Iteration: 5104 lambda_n: 0.9189716874443692 Loss: 0.0002019149258338724\n",
      "Iteration: 5105 lambda_n: 0.9390881664422276 Loss: 0.00020191492608249078\n",
      "Iteration: 5106 lambda_n: 0.9478013253887475 Loss: 0.0002019149263346382\n",
      "Iteration: 5107 lambda_n: 0.9663721574239363 Loss: 0.0002019149265871656\n",
      "Iteration: 5108 lambda_n: 0.9151536138307756 Loss: 0.00020191492684264178\n",
      "Iteration: 5109 lambda_n: 0.9964371870446317 Loss: 0.00020191492708266094\n",
      "Iteration: 5110 lambda_n: 0.9082075463552474 Loss: 0.00020191492734204322\n",
      "Iteration: 5111 lambda_n: 1.0297148929138413 Loss: 0.00020191492757653254\n",
      "Iteration: 5112 lambda_n: 0.920118743634287 Loss: 0.00020191492784041903\n",
      "Iteration: 5113 lambda_n: 0.9452688847322843 Loss: 0.00020191492807423192\n",
      "Iteration: 5114 lambda_n: 0.912284713434713 Loss: 0.0002019149283126286\n",
      "Iteration: 5115 lambda_n: 1.0242103377292018 Loss: 0.00020191492854093092\n",
      "Iteration: 5116 lambda_n: 0.9539123563211169 Loss: 0.00020191492879532795\n",
      "Iteration: 5117 lambda_n: 0.9517568201833534 Loss: 0.0002019149290302858\n",
      "Iteration: 5118 lambda_n: 0.9638918931680104 Loss: 0.00020191492926288385\n",
      "Iteration: 5119 lambda_n: 0.927900866830942 Loss: 0.00020191492949661832\n",
      "Iteration: 5120 lambda_n: 1.0189100353607141 Loss: 0.00020191492971985395\n",
      "Iteration: 5121 lambda_n: 0.9750110805358294 Loss: 0.00020191492996312233\n",
      "Iteration: 5122 lambda_n: 0.981704369616264 Loss: 0.00020191493019397656\n",
      "Iteration: 5123 lambda_n: 1.0323493885764836 Loss: 0.00020191493042457032\n",
      "Iteration: 5124 lambda_n: 1.007898296106395 Loss: 0.00020191493066512505\n",
      "Iteration: 5125 lambda_n: 1.0203392709900383 Loss: 0.00020191493089800764\n",
      "Iteration: 5126 lambda_n: 1.0336864870921048 Loss: 0.00020191493113182993\n",
      "Iteration: 5127 lambda_n: 1.0018220155889426 Loss: 0.00020191493136674128\n",
      "Iteration: 5128 lambda_n: 0.975157225939823 Loss: 0.00020191493159248822\n",
      "Iteration: 5129 lambda_n: 0.9248954896987703 Loss: 0.00020191493181043147\n",
      "Iteration: 5130 lambda_n: 0.9304935595108564 Loss: 0.0002019149320155038\n",
      "Iteration: 5131 lambda_n: 0.9065095698059257 Loss: 0.0002019149322202668\n",
      "Iteration: 5132 lambda_n: 0.9575623450319193 Loss: 0.00020191493241823489\n",
      "Iteration: 5133 lambda_n: 0.9145172498297763 Loss: 0.00020191493262582594\n",
      "Iteration: 5134 lambda_n: 0.9815046344359494 Loss: 0.00020191493282253575\n",
      "Iteration: 5135 lambda_n: 0.8935406121847155 Loss: 0.00020191493303208766\n",
      "Iteration: 5136 lambda_n: 0.9534478947808609 Loss: 0.00020191493322133664\n",
      "Iteration: 5137 lambda_n: 1.0303281199353422 Loss: 0.0002019149334218085\n",
      "Iteration: 5138 lambda_n: 0.9900042306471052 Loss: 0.00020191493363676285\n",
      "Iteration: 5139 lambda_n: 0.9394004395048964 Loss: 0.00020191493384158252\n",
      "Iteration: 5140 lambda_n: 0.9528615760526659 Loss: 0.00020191493403436518\n",
      "Iteration: 5141 lambda_n: 0.9646918844488908 Loss: 0.0002019149342284316\n",
      "Iteration: 5142 lambda_n: 0.9294082122456153 Loss: 0.00020191493442338817\n",
      "Iteration: 5143 lambda_n: 1.0121553834315773 Loss: 0.00020191493460974076\n",
      "Iteration: 5144 lambda_n: 0.8897123093604505 Loss: 0.0002019149348111581\n",
      "Iteration: 5145 lambda_n: 0.9110336791294301 Loss: 0.00020191493498675854\n",
      "Iteration: 5146 lambda_n: 0.9260102095508956 Loss: 0.00020191493516526955\n",
      "Iteration: 5147 lambda_n: 1.0053410894880777 Loss: 0.00020191493534537546\n",
      "Iteration: 5148 lambda_n: 0.993309131240544 Loss: 0.0002019149355394484\n",
      "Iteration: 5149 lambda_n: 1.0213758248985045 Loss: 0.00020191493572963527\n",
      "Iteration: 5150 lambda_n: 0.9132925416905453 Loss: 0.00020191493592362447\n",
      "Iteration: 5151 lambda_n: 0.9215937383115769 Loss: 0.00020191493609565727\n",
      "Iteration: 5152 lambda_n: 0.9701111914576972 Loss: 0.00020191493626796697\n",
      "Iteration: 5153 lambda_n: 0.9764377519958108 Loss: 0.0002019149364479952\n",
      "Iteration: 5154 lambda_n: 0.9098730519106993 Loss: 0.00020191493662778025\n",
      "Iteration: 5155 lambda_n: 1.0321025170902547 Loss: 0.0002019149367939804\n",
      "Iteration: 5156 lambda_n: 0.9090311928143281 Loss: 0.00020191493698112288\n",
      "Iteration: 5157 lambda_n: 0.9378213324622654 Loss: 0.00020191493714458348\n",
      "Iteration: 5158 lambda_n: 0.9569550617644124 Loss: 0.00020191493731198465\n",
      "Iteration: 5159 lambda_n: 1.0089167149348894 Loss: 0.00020191493748151216\n",
      "Iteration: 5160 lambda_n: 0.9869012820988918 Loss: 0.00020191493765885587\n",
      "Iteration: 5161 lambda_n: 0.9228806798874084 Loss: 0.000201914937830921\n",
      "Iteration: 5162 lambda_n: 0.9777932908522453 Loss: 0.0002019149379905433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5163 lambda_n: 0.9642066193064686 Loss: 0.00020191493815840955\n",
      "Iteration: 5164 lambda_n: 0.9356792021079843 Loss: 0.00020191493832263363\n",
      "Iteration: 5165 lambda_n: 0.8931977449122401 Loss: 0.00020191493848076427\n",
      "Iteration: 5166 lambda_n: 0.9173714957230743 Loss: 0.00020191493863057987\n",
      "Iteration: 5167 lambda_n: 0.9546904185762016 Loss: 0.0002019149387833458\n",
      "Iteration: 5168 lambda_n: 0.9548361678870931 Loss: 0.00020191493894114473\n",
      "Iteration: 5169 lambda_n: 0.8854130290299969 Loss: 0.00020191493909776204\n",
      "Iteration: 5170 lambda_n: 0.9372653726788521 Loss: 0.00020191493924186796\n",
      "Iteration: 5171 lambda_n: 0.9779443531415668 Loss: 0.00020191493939333575\n",
      "Iteration: 5172 lambda_n: 1.003859319359612 Loss: 0.00020191493955018638\n",
      "Iteration: 5173 lambda_n: 1.0196480470246239 Loss: 0.00020191493970993013\n",
      "Iteration: 5174 lambda_n: 0.8930457660823109 Loss: 0.00020191493987087146\n",
      "Iteration: 5175 lambda_n: 0.9018885292955898 Loss: 0.00020191494001068047\n",
      "Iteration: 5176 lambda_n: 0.9517257416165152 Loss: 0.00020191494015086583\n",
      "Iteration: 5177 lambda_n: 1.0212465074572818 Loss: 0.00020191494029772264\n",
      "Iteration: 5178 lambda_n: 0.8855092722550852 Loss: 0.0002019149404541005\n",
      "Iteration: 5179 lambda_n: 0.9477175555512312 Loss: 0.00020191494058858933\n",
      "Iteration: 5180 lambda_n: 0.9552348522457442 Loss: 0.000201914940731499\n",
      "Iteration: 5181 lambda_n: 1.0046644054319396 Loss: 0.0002019149408744454\n",
      "Iteration: 5182 lambda_n: 0.8917368607620869 Loss: 0.0002019149410236472\n",
      "Iteration: 5183 lambda_n: 1.0371487544651772 Loss: 0.00020191494115500889\n",
      "Iteration: 5184 lambda_n: 0.90444708823549 Loss: 0.00020191494130670557\n",
      "Iteration: 5185 lambda_n: 0.9331938219570657 Loss: 0.00020191494143788732\n",
      "Iteration: 5186 lambda_n: 0.9457789195815919 Loss: 0.0002019149415722643\n",
      "Iteration: 5187 lambda_n: 0.8932176889717793 Loss: 0.00020191494170743458\n",
      "Iteration: 5188 lambda_n: 0.9117255093468493 Loss: 0.00020191494183412537\n",
      "Iteration: 5189 lambda_n: 0.964499502966578 Loss: 0.00020191494196252039\n",
      "Iteration: 5190 lambda_n: 0.9532541442121065 Loss: 0.00020191494209735648\n",
      "Iteration: 5191 lambda_n: 1.0047835695792815 Loss: 0.00020191494222959454\n",
      "Iteration: 5192 lambda_n: 0.9504525758069712 Loss: 0.00020191494236791388\n",
      "Iteration: 5193 lambda_n: 0.8854892243007234 Loss: 0.0002019149424977057\n",
      "Iteration: 5194 lambda_n: 1.0117049759310164 Loss: 0.00020191494261770758\n",
      "Iteration: 5195 lambda_n: 1.0302463597190188 Loss: 0.0002019149427538422\n",
      "Iteration: 5196 lambda_n: 1.0245532272159303 Loss: 0.00020191494289135357\n",
      "Iteration: 5197 lambda_n: 0.9296258952040075 Loss: 0.0002019149430269911\n",
      "Iteration: 5198 lambda_n: 0.9092882494667608 Loss: 0.00020191494314905403\n",
      "Iteration: 5199 lambda_n: 0.9736771240075252 Loss: 0.00020191494326755627\n",
      "Iteration: 5200 lambda_n: 0.8918951080623744 Loss: 0.0002019149433935279\n",
      "Iteration: 5201 lambda_n: 1.0303769198017705 Loss: 0.00020191494350802925\n",
      "Iteration: 5202 lambda_n: 0.9566386208261087 Loss: 0.00020191494363936975\n",
      "Iteration: 5203 lambda_n: 1.034726797078601 Loss: 0.00020191494376030727\n",
      "Iteration: 5204 lambda_n: 0.9353035218783338 Loss: 0.00020191494389011974\n",
      "Iteration: 5205 lambda_n: 1.0370191781629132 Loss: 0.0002019149440064957\n",
      "Iteration: 5206 lambda_n: 0.9486064320753878 Loss: 0.00020191494413456554\n",
      "Iteration: 5207 lambda_n: 0.8934411604869956 Loss: 0.00020191494425075116\n",
      "Iteration: 5208 lambda_n: 0.9417643673338874 Loss: 0.00020191494435935645\n",
      "Iteration: 5209 lambda_n: 0.9650611085416894 Loss: 0.00020191494447302218\n",
      "Iteration: 5210 lambda_n: 0.9229490925743973 Loss: 0.0002019149445886218\n",
      "Iteration: 5211 lambda_n: 0.8924400151639793 Loss: 0.00020191494469833375\n",
      "Iteration: 5212 lambda_n: 1.0335833970446016 Loss: 0.00020191494480363697\n",
      "Iteration: 5213 lambda_n: 0.8857029062346495 Loss: 0.00020191494492472872\n",
      "Iteration: 5214 lambda_n: 1.0376698760881518 Loss: 0.00020191494502764864\n",
      "Iteration: 5215 lambda_n: 0.9900733837111998 Loss: 0.00020191494514738059\n",
      "Iteration: 5216 lambda_n: 0.9218526915965456 Loss: 0.0002019149452606751\n",
      "Iteration: 5217 lambda_n: 0.9517825740059739 Loss: 0.0002019149453653385\n",
      "Iteration: 5218 lambda_n: 1.0000488851172507 Loss: 0.0002019149454726187\n",
      "Iteration: 5219 lambda_n: 0.9891508799551829 Loss: 0.00020191494558448382\n",
      "Iteration: 5220 lambda_n: 0.9172480282669974 Loss: 0.00020191494569425002\n",
      "Iteration: 5221 lambda_n: 0.9802375122534092 Loss: 0.0002019149457952402\n",
      "Iteration: 5222 lambda_n: 0.8875638614996586 Loss: 0.00020191494590238464\n",
      "Iteration: 5223 lambda_n: 0.9042249520940409 Loss: 0.00020191494599864997\n",
      "Iteration: 5224 lambda_n: 0.9569298005579957 Loss: 0.00020191494609602946\n",
      "Iteration: 5225 lambda_n: 0.96409913161813 Loss: 0.00020191494619834798\n",
      "Iteration: 5226 lambda_n: 0.987338564252579 Loss: 0.0002019149463006513\n",
      "Iteration: 5227 lambda_n: 0.9294773801303627 Loss: 0.00020191494640462915\n",
      "Iteration: 5228 lambda_n: 0.9495627855015911 Loss: 0.00020191494650174135\n",
      "Iteration: 5229 lambda_n: 0.8855203082401384 Loss: 0.00020191494660022844\n",
      "Iteration: 5230 lambda_n: 0.9884309812078229 Loss: 0.00020191494669138867\n",
      "Iteration: 5231 lambda_n: 1.0059425990099236 Loss: 0.00020191494679243046\n",
      "Iteration: 5232 lambda_n: 0.9623205202921581 Loss: 0.00020191494689445383\n",
      "Iteration: 5233 lambda_n: 1.026230518748091 Loss: 0.00020191494699128363\n",
      "Iteration: 5234 lambda_n: 0.9288442240826409 Loss: 0.00020191494709375556\n",
      "Iteration: 5235 lambda_n: 0.939148837882059 Loss: 0.00020191494718575805\n",
      "Iteration: 5236 lambda_n: 0.9687843655000744 Loss: 0.0002019149472780942\n",
      "Iteration: 5237 lambda_n: 0.9091696790406414 Loss: 0.00020191494737264215\n",
      "Iteration: 5238 lambda_n: 1.0343404334252488 Loss: 0.00020191494746070086\n",
      "Iteration: 5239 lambda_n: 0.9553855771151033 Loss: 0.0002019149475601595\n",
      "Iteration: 5240 lambda_n: 0.9955827794445677 Loss: 0.00020191494765127029\n",
      "Iteration: 5241 lambda_n: 1.0023387732761113 Loss: 0.00020191494774550637\n",
      "Iteration: 5242 lambda_n: 0.8994156881129917 Loss: 0.00020191494783963675\n",
      "Iteration: 5243 lambda_n: 0.914284183478121 Loss: 0.00020191494792343856\n",
      "Iteration: 5244 lambda_n: 0.9724147311546169 Loss: 0.00020191494800801964\n",
      "Iteration: 5245 lambda_n: 1.033479425269408 Loss: 0.0002019149480973323\n",
      "Iteration: 5246 lambda_n: 0.9216161298315934 Loss: 0.000201914948191524\n",
      "Iteration: 5247 lambda_n: 0.9302093384062908 Loss: 0.0002019149482748501\n",
      "Iteration: 5248 lambda_n: 0.9278957025419003 Loss: 0.00020191494835834076\n",
      "Iteration: 5249 lambda_n: 0.8912770460830883 Loss: 0.00020191494844101044\n",
      "Iteration: 5250 lambda_n: 0.9281682308564453 Loss: 0.0002019149485198433\n",
      "Iteration: 5251 lambda_n: 0.9523017602714828 Loss: 0.0002019149486013642\n",
      "Iteration: 5252 lambda_n: 0.9469879422066031 Loss: 0.0002019149486843933\n",
      "Iteration: 5253 lambda_n: 0.9539146048094994 Loss: 0.0002019149487663357\n",
      "Iteration: 5254 lambda_n: 0.958784152344003 Loss: 0.0002019149488482759\n",
      "Iteration: 5255 lambda_n: 0.9564260867615253 Loss: 0.000201914948930023\n",
      "Iteration: 5256 lambda_n: 0.9427197515238311 Loss: 0.00020191494901094874\n",
      "Iteration: 5257 lambda_n: 0.939997484812833 Loss: 0.00020191494909011927\n",
      "Iteration: 5258 lambda_n: 0.979087953029226 Loss: 0.00020191494916848343\n",
      "Iteration: 5259 lambda_n: 0.9350580746529574 Loss: 0.00020191494924950163\n",
      "Iteration: 5260 lambda_n: 0.9006097532051996 Loss: 0.00020191494932627984\n",
      "Iteration: 5261 lambda_n: 0.965943785359438 Loss: 0.00020191494939968626\n",
      "Iteration: 5262 lambda_n: 0.9183050567513367 Loss: 0.00020191494947786775\n",
      "Iteration: 5263 lambda_n: 0.9164038607225246 Loss: 0.00020191494955162944\n",
      "Iteration: 5264 lambda_n: 0.9763680716760404 Loss: 0.00020191494962471318\n",
      "Iteration: 5265 lambda_n: 1.035100601214605 Loss: 0.00020191494970201812\n",
      "Iteration: 5266 lambda_n: 1.0357003057407679 Loss: 0.00020191494978335245\n",
      "Iteration: 5267 lambda_n: 0.9702269299866757 Loss: 0.0002019149498640729\n",
      "Iteration: 5268 lambda_n: 0.9288124843619525 Loss: 0.00020191494993908034\n",
      "Iteration: 5269 lambda_n: 0.985568133181017 Loss: 0.0002019149500103389\n",
      "Iteration: 5270 lambda_n: 0.9744318764532609 Loss: 0.0002019149500854096\n",
      "Iteration: 5271 lambda_n: 0.9503481249422047 Loss: 0.00020191495015905813\n",
      "Iteration: 5272 lambda_n: 1.0338631155381859 Loss: 0.00020191495023034082\n",
      "Iteration: 5273 lambda_n: 1.0146821151652374 Loss: 0.00020191495030731437\n",
      "Iteration: 5274 lambda_n: 0.9412075690041017 Loss: 0.00020191495038225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5275 lambda_n: 0.9669260165194654 Loss: 0.00020191495045121237\n",
      "Iteration: 5276 lambda_n: 0.9612304918152748 Loss: 0.000201914950521537\n",
      "Iteration: 5277 lambda_n: 0.89469015895777 Loss: 0.00020191495059092139\n",
      "Iteration: 5278 lambda_n: 0.9643766723877163 Loss: 0.00020191495065501514\n",
      "Iteration: 5279 lambda_n: 1.0252750197024076 Loss: 0.00020191495072363098\n",
      "Iteration: 5280 lambda_n: 0.943497160163769 Loss: 0.00020191495079602384\n",
      "Iteration: 5281 lambda_n: 0.9688117137545452 Loss: 0.00020191495086211559\n",
      "Iteration: 5282 lambda_n: 0.9707722782918857 Loss: 0.00020191495092948645\n",
      "Iteration: 5283 lambda_n: 0.9780809377124648 Loss: 0.00020191495099648187\n",
      "Iteration: 5284 lambda_n: 0.9097485359524601 Loss: 0.0002019149510634738\n",
      "Iteration: 5285 lambda_n: 0.9476221962888782 Loss: 0.00020191495112530873\n",
      "Iteration: 5286 lambda_n: 0.9465049255560114 Loss: 0.00020191495118926082\n",
      "Iteration: 5287 lambda_n: 0.9823185251797599 Loss: 0.00020191495125266939\n",
      "Iteration: 5288 lambda_n: 0.932287841564002 Loss: 0.00020191495131799474\n",
      "Iteration: 5289 lambda_n: 0.9347303840007672 Loss: 0.0002019149513795158\n",
      "Iteration: 5290 lambda_n: 1.0307471328519942 Loss: 0.00020191495144075507\n",
      "Iteration: 5291 lambda_n: 0.9458685327778192 Loss: 0.00020191495150780249\n",
      "Iteration: 5292 lambda_n: 0.9419364761836252 Loss: 0.00020191495156883247\n",
      "Iteration: 5293 lambda_n: 0.883064486917652 Loss: 0.00020191495162917184\n",
      "Iteration: 5294 lambda_n: 0.9953756368979635 Loss: 0.00020191495168532408\n",
      "Iteration: 5295 lambda_n: 0.9138350692871174 Loss: 0.0002019149517481844\n",
      "Iteration: 5296 lambda_n: 0.9355461588148618 Loss: 0.00020191495180544485\n",
      "Iteration: 5297 lambda_n: 0.9217369406993172 Loss: 0.00020191495186365192\n",
      "Iteration: 5298 lambda_n: 1.0078425421528072 Loss: 0.00020191495192058647\n",
      "Iteration: 5299 lambda_n: 0.9574845717873601 Loss: 0.00020191495198239562\n",
      "Iteration: 5300 lambda_n: 1.0004699291707448 Loss: 0.00020191495204066267\n",
      "Iteration: 5301 lambda_n: 0.9753129901197917 Loss: 0.00020191495210109003\n",
      "Iteration: 5302 lambda_n: 0.9398069709742554 Loss: 0.0002019149521595443\n",
      "Iteration: 5303 lambda_n: 0.8959974753855304 Loss: 0.00020191495221544707\n",
      "Iteration: 5304 lambda_n: 0.9808082614361583 Loss: 0.0002019149522683602\n",
      "Iteration: 5305 lambda_n: 0.9515959667173084 Loss: 0.00020191495232587834\n",
      "Iteration: 5306 lambda_n: 0.9772505740206344 Loss: 0.00020191495238126698\n",
      "Iteration: 5307 lambda_n: 0.8856359257626375 Loss: 0.0002019149524377283\n",
      "Iteration: 5308 lambda_n: 0.9876777201864806 Loss: 0.00020191495248850702\n",
      "Iteration: 5309 lambda_n: 0.8895350733632699 Loss: 0.0002019149525447507\n",
      "Iteration: 5310 lambda_n: 1.030082878168526 Loss: 0.00020191495259502378\n",
      "Iteration: 5311 lambda_n: 0.935752717376177 Loss: 0.00020191495265283108\n",
      "Iteration: 5312 lambda_n: 1.0205886259859267 Loss: 0.00020191495270493185\n",
      "Iteration: 5313 lambda_n: 0.9551690722104572 Loss: 0.00020191495276135338\n",
      "Iteration: 5314 lambda_n: 0.9050545515797012 Loss: 0.00020191495281374997\n",
      "Iteration: 5315 lambda_n: 0.9653530209204023 Loss: 0.0002019149528630259\n",
      "Iteration: 5316 lambda_n: 0.9887324075204396 Loss: 0.00020191495291521996\n",
      "Iteration: 5317 lambda_n: 0.9726381063447149 Loss: 0.00020191495296827878\n",
      "Iteration: 5318 lambda_n: 0.953928057467143 Loss: 0.00020191495302007006\n",
      "Iteration: 5319 lambda_n: 0.8874502309909402 Loss: 0.0002019149530704923\n",
      "Iteration: 5320 lambda_n: 0.9262700779443769 Loss: 0.00020191495311705786\n",
      "Iteration: 5321 lambda_n: 0.9794503466173914 Loss: 0.00020191495316533237\n",
      "Iteration: 5322 lambda_n: 0.9037690025438414 Loss: 0.0002019149532160048\n",
      "Iteration: 5323 lambda_n: 1.0161656106980634 Loss: 0.0002019149532624112\n",
      "Iteration: 5324 lambda_n: 0.9328929677746132 Loss: 0.00020191495331423886\n",
      "Iteration: 5325 lambda_n: 1.0224063603593612 Loss: 0.00020191495336144272\n",
      "Iteration: 5326 lambda_n: 0.9148367837615797 Loss: 0.00020191495341280243\n",
      "Iteration: 5327 lambda_n: 0.9895567375152574 Loss: 0.00020191495345839677\n",
      "Iteration: 5328 lambda_n: 0.9136373895169999 Loss: 0.00020191495350737408\n",
      "Iteration: 5329 lambda_n: 0.9395616925564655 Loss: 0.0002019149535522509\n",
      "Iteration: 5330 lambda_n: 1.0246231922299778 Loss: 0.00020191495359807743\n",
      "Iteration: 5331 lambda_n: 1.0367765181078965 Loss: 0.00020191495364769012\n",
      "Iteration: 5332 lambda_n: 0.9857659741984031 Loss: 0.00020191495369750192\n",
      "Iteration: 5333 lambda_n: 0.9038993234074019 Loss: 0.000201914953744486\n",
      "Iteration: 5334 lambda_n: 0.9081130105563523 Loss: 0.00020191495378724635\n",
      "Iteration: 5335 lambda_n: 1.0190259789202307 Loss: 0.00020191495382990683\n",
      "Iteration: 5336 lambda_n: 0.9056588410221098 Loss: 0.00020191495387744817\n",
      "Iteration: 5337 lambda_n: 0.9283547978887511 Loss: 0.00020191495391937026\n",
      "Iteration: 5338 lambda_n: 0.9166344246256501 Loss: 0.00020191495396203893\n",
      "Iteration: 5339 lambda_n: 0.9189591451596332 Loss: 0.0002019149540038741\n",
      "Iteration: 5340 lambda_n: 1.0177354634826803 Loss: 0.00020191495404552363\n",
      "Iteration: 5341 lambda_n: 0.9750448187260311 Loss: 0.00020191495409132526\n",
      "Iteration: 5342 lambda_n: 0.9365520588535231 Loss: 0.00020191495413486474\n",
      "Iteration: 5343 lambda_n: 0.9211303536947221 Loss: 0.00020191495417637187\n",
      "Iteration: 5344 lambda_n: 0.9862350380738817 Loss: 0.00020191495421690274\n",
      "Iteration: 5345 lambda_n: 0.98457406648548 Loss: 0.0002019149542599938\n",
      "Iteration: 5346 lambda_n: 0.9424931923030877 Loss: 0.00020191495430269197\n",
      "Iteration: 5347 lambda_n: 0.9331172959724711 Loss: 0.00020191495434325484\n",
      "Iteration: 5348 lambda_n: 0.9238593763051255 Loss: 0.00020191495438313126\n",
      "Iteration: 5349 lambda_n: 1.0344528222888487 Loss: 0.00020191495442233295\n",
      "Iteration: 5350 lambda_n: 0.9694997164779284 Loss: 0.00020191495446592067\n",
      "Iteration: 5351 lambda_n: 0.9172059168858129 Loss: 0.00020191495450643787\n",
      "Iteration: 5352 lambda_n: 0.9090406580189937 Loss: 0.00020191495454449881\n",
      "Iteration: 5353 lambda_n: 0.9163209988358177 Loss: 0.0002019149545819534\n",
      "Iteration: 5354 lambda_n: 0.8993270780566952 Loss: 0.00020191495461944735\n",
      "Iteration: 5355 lambda_n: 0.9973149612760426 Loss: 0.00020191495465598835\n",
      "Iteration: 5356 lambda_n: 1.0277375307091379 Loss: 0.000201914954696233\n",
      "Iteration: 5357 lambda_n: 0.9810883429574878 Loss: 0.0002019149547373846\n",
      "Iteration: 5358 lambda_n: 0.9697437173762529 Loss: 0.00020191495477637082\n",
      "Iteration: 5359 lambda_n: 0.9767835992409776 Loss: 0.00020191495481461464\n",
      "Iteration: 5360 lambda_n: 0.9344652669694351 Loss: 0.00020191495485285675\n",
      "Iteration: 5361 lambda_n: 0.8999951283322968 Loss: 0.00020191495488916135\n",
      "Iteration: 5362 lambda_n: 1.0086877989598677 Loss: 0.00020191495492388807\n",
      "Iteration: 5363 lambda_n: 0.9237563489711442 Loss: 0.00020191495496254007\n",
      "Iteration: 5364 lambda_n: 1.017417525187259 Loss: 0.0002019149549976626\n",
      "Iteration: 5365 lambda_n: 0.9146876607318095 Loss: 0.00020191495503607874\n",
      "Iteration: 5366 lambda_n: 1.0336958565117704 Loss: 0.0002019149550703492\n",
      "Iteration: 5367 lambda_n: 0.9239141808087576 Loss: 0.0002019149551088171\n",
      "Iteration: 5368 lambda_n: 0.9192456508987991 Loss: 0.00020191495514292296\n",
      "Iteration: 5369 lambda_n: 0.9379643532209313 Loss: 0.00020191495517662576\n",
      "Iteration: 5370 lambda_n: 0.9841779109979086 Loss: 0.0002019149552107782\n",
      "Iteration: 5371 lambda_n: 0.9852917713807419 Loss: 0.00020191495524635113\n",
      "Iteration: 5372 lambda_n: 0.9742309006699977 Loss: 0.00020191495528170322\n",
      "Iteration: 5373 lambda_n: 0.9115320027684098 Loss: 0.00020191495531639728\n",
      "Iteration: 5374 lambda_n: 0.9217332704707913 Loss: 0.00020191495534862137\n",
      "Iteration: 5375 lambda_n: 0.9265889560838543 Loss: 0.00020191495538098697\n",
      "Iteration: 5376 lambda_n: 0.9140031376706711 Loss: 0.0002019149554132843\n",
      "Iteration: 5377 lambda_n: 0.8950926667665248 Loss: 0.00020191495544492576\n",
      "Iteration: 5378 lambda_n: 0.9409709740172671 Loss: 0.00020191495547570076\n",
      "Iteration: 5379 lambda_n: 0.9207154030521447 Loss: 0.00020191495550783226\n",
      "Iteration: 5380 lambda_n: 0.901216665696468 Loss: 0.00020191495553904798\n",
      "Iteration: 5381 lambda_n: 1.0242398358664728 Loss: 0.00020191495556938632\n",
      "Iteration: 5382 lambda_n: 0.9694619377600251 Loss: 0.00020191495560363323\n",
      "Iteration: 5383 lambda_n: 0.9271950502742007 Loss: 0.0002019149556357982\n",
      "Iteration: 5384 lambda_n: 0.9813804704744692 Loss: 0.00020191495566633673\n",
      "Iteration: 5385 lambda_n: 1.012204662580596 Loss: 0.00020191495569843787\n",
      "Iteration: 5386 lambda_n: 0.8869574304078197 Loss: 0.00020191495573130096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5387 lambda_n: 1.0037934434474227 Loss: 0.0002019149557598805\n",
      "Iteration: 5388 lambda_n: 0.9751259269081166 Loss: 0.00020191495579200598\n",
      "Iteration: 5389 lambda_n: 0.9990863478566284 Loss: 0.00020191495582297752\n",
      "Iteration: 5390 lambda_n: 0.94386566262318 Loss: 0.0002019149558544833\n",
      "Iteration: 5391 lambda_n: 0.9973560109585081 Loss: 0.00020191495588401634\n",
      "Iteration: 5392 lambda_n: 0.9991373331377471 Loss: 0.0002019149559150023\n",
      "Iteration: 5393 lambda_n: 0.961060681625255 Loss: 0.00020191495594581207\n",
      "Iteration: 5394 lambda_n: 1.0051056914563579 Loss: 0.00020191495597522143\n",
      "Iteration: 5395 lambda_n: 0.8839624537378518 Loss: 0.00020191495600575932\n",
      "Iteration: 5396 lambda_n: 0.8854972905098385 Loss: 0.00020191495603241624\n",
      "Iteration: 5397 lambda_n: 0.9210450453910588 Loss: 0.00020191495605894166\n",
      "Iteration: 5398 lambda_n: 0.9557650511160217 Loss: 0.00020191495608634947\n",
      "Iteration: 5399 lambda_n: 1.019807041757161 Loss: 0.00020191495611458608\n",
      "Iteration: 5400 lambda_n: 0.9595398237008775 Loss: 0.00020191495614450548\n",
      "Iteration: 5401 lambda_n: 0.9064189443553038 Loss: 0.00020191495617244103\n",
      "Iteration: 5402 lambda_n: 1.0055783284701507 Loss: 0.00020191495619863346\n",
      "Iteration: 5403 lambda_n: 1.0087728763153752 Loss: 0.00020191495622750726\n",
      "Iteration: 5404 lambda_n: 1.0031843666832858 Loss: 0.0002019149562562522\n",
      "Iteration: 5405 lambda_n: 0.9623324985462337 Loss: 0.00020191495628462113\n",
      "Iteration: 5406 lambda_n: 0.9665703730310706 Loss: 0.00020191495631162112\n",
      "Iteration: 5407 lambda_n: 1.016377673286359 Loss: 0.0002019149563385474\n",
      "Iteration: 5408 lambda_n: 1.037487366096521 Loss: 0.0002019149563666652\n",
      "Iteration: 5409 lambda_n: 0.9414853556916383 Loss: 0.00020191495639514527\n",
      "Iteration: 5410 lambda_n: 0.9634272020150838 Loss: 0.00020191495642078781\n",
      "Iteration: 5411 lambda_n: 0.8965708225403222 Loss: 0.0002019149564468497\n",
      "Iteration: 5412 lambda_n: 0.9295636581964949 Loss: 0.00020191495647092384\n",
      "Iteration: 5413 lambda_n: 0.9495624815222726 Loss: 0.0002019149564957113\n",
      "Iteration: 5414 lambda_n: 0.9650938210493925 Loss: 0.00020191495652086268\n",
      "Iteration: 5415 lambda_n: 0.882762689831939 Loss: 0.00020191495654624546\n",
      "Iteration: 5416 lambda_n: 0.898112344329044 Loss: 0.00020191495656929164\n",
      "Iteration: 5417 lambda_n: 1.0141610954260163 Loss: 0.00020191495659258813\n",
      "Iteration: 5418 lambda_n: 1.023749733860696 Loss: 0.0002019149566187143\n",
      "Iteration: 5419 lambda_n: 1.0133861799955914 Loss: 0.00020191495664488647\n",
      "Iteration: 5420 lambda_n: 0.9759672489904733 Loss: 0.00020191495667058978\n",
      "Iteration: 5421 lambda_n: 0.9546734720317173 Loss: 0.0002019149566951671\n",
      "Iteration: 5422 lambda_n: 0.973321047227627 Loss: 0.00020191495671902999\n",
      "Iteration: 5423 lambda_n: 0.9367340325810245 Loss: 0.00020191495674319254\n",
      "Iteration: 5424 lambda_n: 0.9862764250294124 Loss: 0.00020191495676627838\n",
      "Iteration: 5425 lambda_n: 0.9879055755927542 Loss: 0.00020191495679041537\n",
      "Iteration: 5426 lambda_n: 1.012317268255351 Loss: 0.00020191495681441384\n",
      "Iteration: 5427 lambda_n: 0.9899252883935763 Loss: 0.00020191495683882422\n",
      "Iteration: 5428 lambda_n: 0.9376516865425614 Loss: 0.00020191495686251238\n",
      "Iteration: 5429 lambda_n: 0.9331108801031515 Loss: 0.00020191495688477498\n",
      "Iteration: 5430 lambda_n: 0.9454401349092075 Loss: 0.00020191495690678892\n",
      "Iteration: 5431 lambda_n: 0.9484322110571606 Loss: 0.0002019149569289362\n",
      "Iteration: 5432 lambda_n: 0.9901057777463736 Loss: 0.00020191495695098905\n",
      "Iteration: 5433 lambda_n: 0.986489765311253 Loss: 0.00020191495697385474\n",
      "Iteration: 5434 lambda_n: 0.9229429500071017 Loss: 0.0002019149569964704\n",
      "Iteration: 5435 lambda_n: 0.9156705597872077 Loss: 0.00020191495701747352\n",
      "Iteration: 5436 lambda_n: 0.9608843214381184 Loss: 0.00020191495703817482\n",
      "Iteration: 5437 lambda_n: 0.9275351410408634 Loss: 0.0002019149570597475\n",
      "Iteration: 5438 lambda_n: 0.8981710245004674 Loss: 0.00020191495708041937\n",
      "Iteration: 5439 lambda_n: 0.9828599873507516 Loss: 0.00020191495710030085\n",
      "Iteration: 5440 lambda_n: 0.9354013744594553 Loss: 0.00020191495712191528\n",
      "Iteration: 5441 lambda_n: 0.9807932439395349 Loss: 0.00020191495714233314\n",
      "Iteration: 5442 lambda_n: 1.0237593246080714 Loss: 0.00020191495716358947\n",
      "Iteration: 5443 lambda_n: 0.9777982846099686 Loss: 0.00020191495718561785\n",
      "Iteration: 5444 lambda_n: 0.9366469545345498 Loss: 0.00020191495720650013\n",
      "Iteration: 5445 lambda_n: 0.9291242089192162 Loss: 0.00020191495722635225\n",
      "Iteration: 5446 lambda_n: 0.9071673298520672 Loss: 0.00020191495724591722\n",
      "Iteration: 5447 lambda_n: 0.9582268513792614 Loss: 0.00020191495726488724\n",
      "Iteration: 5448 lambda_n: 0.9554543406474466 Loss: 0.0002019149572847881\n",
      "Iteration: 5449 lambda_n: 0.9396094907514874 Loss: 0.00020191495730448794\n",
      "Iteration: 5450 lambda_n: 0.9171785103255429 Loss: 0.000201914957323725\n",
      "Iteration: 5451 lambda_n: 0.9620671303908198 Loss: 0.00020191495734237208\n",
      "Iteration: 5452 lambda_n: 0.9287800272628393 Loss: 0.00020191495736180504\n",
      "Iteration: 5453 lambda_n: 0.9509888099531494 Loss: 0.00020191495738043673\n",
      "Iteration: 5454 lambda_n: 0.923069990170984 Loss: 0.00020191495739937997\n",
      "Iteration: 5455 lambda_n: 1.0310960535475828 Loss: 0.0002019149574176295\n",
      "Iteration: 5456 lambda_n: 1.0013132295430416 Loss: 0.00020191495743788863\n",
      "Iteration: 5457 lambda_n: 0.91313351386272 Loss: 0.00020191495745740007\n",
      "Iteration: 5458 lambda_n: 1.030741508713935 Loss: 0.0002019149574750641\n",
      "Iteration: 5459 lambda_n: 1.0175597173301003 Loss: 0.00020191495749487628\n",
      "Iteration: 5460 lambda_n: 1.0244397234850453 Loss: 0.00020191495751428277\n",
      "Iteration: 5461 lambda_n: 0.9327143850274605 Loss: 0.00020191495753367814\n",
      "Iteration: 5462 lambda_n: 0.9982527174599756 Loss: 0.00020191495755119665\n",
      "Iteration: 5463 lambda_n: 0.9831703820257959 Loss: 0.00020191495756982314\n",
      "Iteration: 5464 lambda_n: 0.949227595981041 Loss: 0.0002019149575880329\n",
      "Iteration: 5465 lambda_n: 0.9723082068622628 Loss: 0.0002019149576054851\n",
      "Iteration: 5466 lambda_n: 1.0029312018063714 Loss: 0.0002019149576232301\n",
      "Iteration: 5467 lambda_n: 0.9249628981397339 Loss: 0.00020191495764141019\n",
      "Iteration: 5468 lambda_n: 0.9822646320404702 Loss: 0.00020191495765804667\n",
      "Iteration: 5469 lambda_n: 0.994232979266736 Loss: 0.00020191495767559898\n",
      "Iteration: 5470 lambda_n: 1.01755869576847 Loss: 0.00020191495769324088\n",
      "Iteration: 5471 lambda_n: 1.0327130959695712 Loss: 0.0002019149577111608\n",
      "Iteration: 5472 lambda_n: 0.884028963819799 Loss: 0.00020191495772920901\n",
      "Iteration: 5473 lambda_n: 0.8990432803794453 Loss: 0.0002019149577445478\n",
      "Iteration: 5474 lambda_n: 0.9585249908909752 Loss: 0.0002019149577600478\n",
      "Iteration: 5475 lambda_n: 0.9141207948122985 Loss: 0.00020191495777645648\n",
      "Iteration: 5476 lambda_n: 0.9223581867932332 Loss: 0.00020191495779199435\n",
      "Iteration: 5477 lambda_n: 0.9859310838085816 Loss: 0.00020191495780757092\n",
      "Iteration: 5478 lambda_n: 0.9125637383932244 Loss: 0.00020191495782410741\n",
      "Iteration: 5479 lambda_n: 0.8926765240249958 Loss: 0.00020191495783930224\n",
      "Iteration: 5480 lambda_n: 0.9745366865732311 Loss: 0.00020191495785407208\n",
      "Iteration: 5481 lambda_n: 1.0358019718177005 Loss: 0.000201914957870085\n",
      "Iteration: 5482 lambda_n: 0.9301778509841354 Loss: 0.00020191495788698984\n",
      "Iteration: 5483 lambda_n: 1.0326315594137645 Loss: 0.00020191495790204708\n",
      "Iteration: 5484 lambda_n: 0.9737809700702101 Loss: 0.00020191495791864998\n",
      "Iteration: 5485 lambda_n: 0.9239429602763023 Loss: 0.00020191495793419343\n",
      "Iteration: 5486 lambda_n: 0.9382606027950848 Loss: 0.00020191495794883067\n",
      "Iteration: 5487 lambda_n: 0.9695205532379675 Loss: 0.0002019149579635956\n",
      "Iteration: 5488 lambda_n: 0.9560707899354092 Loss: 0.00020191495797875105\n",
      "Iteration: 5489 lambda_n: 0.885061762452856 Loss: 0.00020191495799358713\n",
      "Iteration: 5490 lambda_n: 1.015012723087245 Loss: 0.0002019149580072159\n",
      "Iteration: 5491 lambda_n: 0.9821091156135824 Loss: 0.00020191495802274087\n",
      "Iteration: 5492 lambda_n: 0.9636871597015446 Loss: 0.0002019149580376639\n",
      "Iteration: 5493 lambda_n: 0.9340705712673641 Loss: 0.00020191495805220192\n",
      "Iteration: 5494 lambda_n: 0.8974206086061518 Loss: 0.0002019149580661922\n",
      "Iteration: 5495 lambda_n: 0.993728154485604 Loss: 0.00020191495807954612\n",
      "Iteration: 5496 lambda_n: 1.0276784028657473 Loss: 0.00020191495809423774\n",
      "Iteration: 5497 lambda_n: 0.8981983077975141 Loss: 0.000201914958109323\n",
      "Iteration: 5498 lambda_n: 1.01279792756908 Loss: 0.00020191495812240613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5499 lambda_n: 0.9872302269089601 Loss: 0.00020191495813705928\n",
      "Iteration: 5500 lambda_n: 0.9510177278652819 Loss: 0.00020191495815123926\n",
      "Iteration: 5501 lambda_n: 1.0201766389119478 Loss: 0.00020191495816480418\n",
      "Iteration: 5502 lambda_n: 1.0227830067012476 Loss: 0.00020191495817925768\n",
      "Iteration: 5503 lambda_n: 1.0210417834031915 Loss: 0.00020191495819363352\n",
      "Iteration: 5504 lambda_n: 0.9374086869070392 Loss: 0.00020191495820787785\n",
      "Iteration: 5505 lambda_n: 0.8838447969843019 Loss: 0.00020191495822086274\n",
      "Iteration: 5506 lambda_n: 0.8919519825513773 Loss: 0.00020191495823301578\n",
      "Iteration: 5507 lambda_n: 0.9050593923481606 Loss: 0.0002019149582452066\n",
      "Iteration: 5508 lambda_n: 0.8964343535668307 Loss: 0.00020191495825749433\n",
      "Iteration: 5509 lambda_n: 0.9598129719513091 Loss: 0.00020191495826959234\n",
      "Iteration: 5510 lambda_n: 0.9406865059259593 Loss: 0.00020191495828245106\n",
      "Iteration: 5511 lambda_n: 0.9392184353487987 Loss: 0.0002019149582949707\n",
      "Iteration: 5512 lambda_n: 0.9438008516817868 Loss: 0.0002019149583073798\n",
      "Iteration: 5513 lambda_n: 0.9768525129547162 Loss: 0.00020191495831976724\n",
      "Iteration: 5514 lambda_n: 0.9726342941276163 Loss: 0.00020191495833250743\n",
      "Iteration: 5515 lambda_n: 0.9855773693292441 Loss: 0.00020191495834510095\n",
      "Iteration: 5516 lambda_n: 0.9613140027924026 Loss: 0.00020191495835776614\n",
      "Iteration: 5517 lambda_n: 0.9827350104058585 Loss: 0.00020191495837003405\n",
      "Iteration: 5518 lambda_n: 1.0013942613286642 Loss: 0.00020191495838248758\n",
      "Iteration: 5519 lambda_n: 0.9517846519103497 Loss: 0.000201914958395087\n",
      "Iteration: 5520 lambda_n: 0.8866601714029191 Loss: 0.0002019149584069737\n",
      "Iteration: 5521 lambda_n: 0.8891786834878397 Loss: 0.00020191495841797246\n",
      "Iteration: 5522 lambda_n: 0.902001376909749 Loss: 0.00020191495842893035\n",
      "Iteration: 5523 lambda_n: 0.9239759919767806 Loss: 0.00020191495843997558\n",
      "Iteration: 5524 lambda_n: 0.954677257935461 Loss: 0.00020191495845121637\n",
      "Iteration: 5525 lambda_n: 0.9853135679836079 Loss: 0.0002019149584627466\n",
      "Iteration: 5526 lambda_n: 0.998755787425941 Loss: 0.00020191495847456732\n",
      "Iteration: 5527 lambda_n: 1.0337023081680325 Loss: 0.00020191495848646997\n",
      "Iteration: 5528 lambda_n: 0.9783984370912314 Loss: 0.00020191495849869185\n",
      "Iteration: 5529 lambda_n: 0.9927282648474152 Loss: 0.00020191495851018288\n",
      "Iteration: 5530 lambda_n: 0.8997395569732999 Loss: 0.00020191495852175628\n",
      "Iteration: 5531 lambda_n: 1.0275740040731247 Loss: 0.00020191495853216936\n",
      "Iteration: 5532 lambda_n: 0.9301409092048002 Loss: 0.00020191495854398242\n",
      "Iteration: 5533 lambda_n: 0.9626222403703609 Loss: 0.0002019149585545994\n",
      "Iteration: 5534 lambda_n: 1.0239143474627526 Loss: 0.00020191495856551004\n",
      "Iteration: 5535 lambda_n: 0.8951118177844957 Loss: 0.00020191495857704132\n",
      "Iteration: 5536 lambda_n: 0.89658522334604 Loss: 0.0002019149585870454\n",
      "Iteration: 5537 lambda_n: 1.0080240016058022 Loss: 0.00020191495859699904\n",
      "Iteration: 5538 lambda_n: 1.013375303169208 Loss: 0.0002019149586081167\n",
      "Iteration: 5539 lambda_n: 0.9032287961742244 Loss: 0.00020191495861921084\n",
      "Iteration: 5540 lambda_n: 0.9947672324547955 Loss: 0.00020191495862903393\n",
      "Iteration: 5541 lambda_n: 0.9295183842121998 Loss: 0.00020191495863977667\n",
      "Iteration: 5542 lambda_n: 0.9020462044266178 Loss: 0.00020191495864974385\n",
      "Iteration: 5543 lambda_n: 0.9793705171530859 Loss: 0.0002019149586593489\n",
      "Iteration: 5544 lambda_n: 0.8846648389945191 Loss: 0.00020191495866971436\n",
      "Iteration: 5545 lambda_n: 1.0273667273181348 Loss: 0.00020191495867901007\n",
      "Iteration: 5546 lambda_n: 1.0255856465080397 Loss: 0.00020191495868973633\n",
      "Iteration: 5547 lambda_n: 0.9881863617864384 Loss: 0.0002019149587003636\n",
      "Iteration: 5548 lambda_n: 0.9936463920335196 Loss: 0.00020191495871052333\n",
      "Iteration: 5549 lambda_n: 0.9820030583260072 Loss: 0.00020191495872066813\n",
      "Iteration: 5550 lambda_n: 1.0254712427234953 Loss: 0.0002019149587306218\n",
      "Iteration: 5551 lambda_n: 1.036983364075975 Loss: 0.00020191495874094238\n",
      "Iteration: 5552 lambda_n: 0.9080888225998239 Loss: 0.00020191495875130334\n",
      "Iteration: 5553 lambda_n: 0.8859916593212238 Loss: 0.00020191495876031073\n",
      "Iteration: 5554 lambda_n: 0.9176046347910976 Loss: 0.00020191495876903796\n",
      "Iteration: 5555 lambda_n: 0.9379617110016699 Loss: 0.00020191495877801795\n",
      "Iteration: 5556 lambda_n: 1.033436396269751 Loss: 0.00020191495878713652\n",
      "Iteration: 5557 lambda_n: 0.9862339491542346 Loss: 0.00020191495879712926\n",
      "Iteration: 5558 lambda_n: 0.9872578189269154 Loss: 0.00020191495880658345\n",
      "Iteration: 5559 lambda_n: 1.0269230629662394 Loss: 0.00020191495881598584\n",
      "Iteration: 5560 lambda_n: 1.000021307572159 Loss: 0.00020191495882569227\n",
      "Iteration: 5561 lambda_n: 0.9073677028395016 Loss: 0.0002019149588350783\n",
      "Iteration: 5562 lambda_n: 0.9501025998541979 Loss: 0.00020191495884352742\n",
      "Iteration: 5563 lambda_n: 0.9241888636618508 Loss: 0.00020191495885232402\n",
      "Iteration: 5564 lambda_n: 0.9100739032263799 Loss: 0.00020191495886082446\n",
      "Iteration: 5565 lambda_n: 1.0071945418027386 Loss: 0.0002019149588691365\n",
      "Iteration: 5566 lambda_n: 1.0360682718544663 Loss: 0.00020191495887827935\n",
      "Iteration: 5567 lambda_n: 0.9410661255413952 Loss: 0.0002019149588876121\n",
      "Iteration: 5568 lambda_n: 0.9012528973441086 Loss: 0.00020191495889601894\n",
      "Iteration: 5569 lambda_n: 0.982346919397269 Loss: 0.0002019149589040199\n",
      "Iteration: 5570 lambda_n: 0.8940249013898607 Loss: 0.00020191495891268467\n",
      "Iteration: 5571 lambda_n: 1.0195789711538255 Loss: 0.00020191495892051564\n",
      "Iteration: 5572 lambda_n: 0.9727247696109631 Loss: 0.00020191495892939174\n",
      "Iteration: 5573 lambda_n: 1.016557780335824 Loss: 0.00020191495893780116\n",
      "Iteration: 5574 lambda_n: 1.0015237777921477 Loss: 0.0002019149589465234\n",
      "Iteration: 5575 lambda_n: 0.9980164806618601 Loss: 0.00020191495895504957\n",
      "Iteration: 5576 lambda_n: 0.8970535266106503 Loss: 0.00020191495896348426\n",
      "Iteration: 5577 lambda_n: 0.9981326291570075 Loss: 0.00020191495897101168\n",
      "Iteration: 5578 lambda_n: 1.0266858729779393 Loss: 0.00020191495897934069\n",
      "Iteration: 5579 lambda_n: 0.9774060922040559 Loss: 0.00020191495898785097\n",
      "Iteration: 5580 lambda_n: 0.9628854628564876 Loss: 0.00020191495899588632\n",
      "Iteration: 5581 lambda_n: 1.0060115007871322 Loss: 0.00020191495900375104\n",
      "Iteration: 5582 lambda_n: 0.9700869405709673 Loss: 0.00020191495901191394\n",
      "Iteration: 5583 lambda_n: 0.902897890168865 Loss: 0.00020191495901973185\n",
      "Iteration: 5584 lambda_n: 1.0273495493273297 Loss: 0.00020191495902695096\n",
      "Iteration: 5585 lambda_n: 0.9054859373017206 Loss: 0.00020191495903511486\n",
      "Iteration: 5586 lambda_n: 1.0305779960699573 Loss: 0.0002019149590422652\n",
      "Iteration: 5587 lambda_n: 0.9250573319071068 Loss: 0.00020191495905033923\n",
      "Iteration: 5588 lambda_n: 0.9938915204973037 Loss: 0.00020191495905753923\n",
      "Iteration: 5589 lambda_n: 1.0337882304868424 Loss: 0.00020191495906521557\n",
      "Iteration: 5590 lambda_n: 0.9177286955813925 Loss: 0.00020191495907314277\n",
      "Iteration: 5591 lambda_n: 0.9307147396669585 Loss: 0.00020191495908013514\n",
      "Iteration: 5592 lambda_n: 0.9565127269920648 Loss: 0.0002019149590871768\n",
      "Iteration: 5593 lambda_n: 0.8937138170969046 Loss: 0.00020191495909436405\n",
      "Iteration: 5594 lambda_n: 0.9298122960830228 Loss: 0.00020191495910103316\n",
      "Iteration: 5595 lambda_n: 0.9373978369349445 Loss: 0.00020191495910792836\n",
      "Iteration: 5596 lambda_n: 1.0304886113690739 Loss: 0.00020191495911484135\n",
      "Iteration: 5597 lambda_n: 0.9448167150897581 Loss: 0.0002019149591223821\n",
      "Iteration: 5598 lambda_n: 0.9950233911798063 Loss: 0.00020191495912924877\n",
      "Iteration: 5599 lambda_n: 0.9174541010564017 Loss: 0.0002019149591364335\n",
      "Iteration: 5600 lambda_n: 0.9775152616636491 Loss: 0.00020191495914300642\n",
      "Iteration: 5601 lambda_n: 0.9321500735172383 Loss: 0.00020191495914996394\n",
      "Iteration: 5602 lambda_n: 1.0279166146579946 Loss: 0.0002019149591565547\n",
      "Iteration: 5603 lambda_n: 1.0101389511105268 Loss: 0.00020191495916376694\n",
      "Iteration: 5604 lambda_n: 0.8976457876691565 Loss: 0.00020191495917080683\n",
      "Iteration: 5605 lambda_n: 0.9431464016672082 Loss: 0.00020191495917702197\n",
      "Iteration: 5606 lambda_n: 0.9462569315178964 Loss: 0.00020191495918350468\n",
      "Iteration: 5607 lambda_n: 0.8969467659362581 Loss: 0.0002019149591899726\n",
      "Iteration: 5608 lambda_n: 0.894238714295469 Loss: 0.00020191495919605682\n",
      "Iteration: 5609 lambda_n: 0.9183433971442203 Loss: 0.0002019149592020857\n",
      "Iteration: 5610 lambda_n: 0.9176334972363651 Loss: 0.00020191495920824016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5611 lambda_n: 0.9002932490377601 Loss: 0.00020191495921434926\n",
      "Iteration: 5612 lambda_n: 0.9664179456369616 Loss: 0.00020191495922029906\n",
      "Iteration: 5613 lambda_n: 0.9080232273272381 Loss: 0.00020191495922665422\n",
      "Iteration: 5614 lambda_n: 0.9539973715683611 Loss: 0.0002019149592325834\n",
      "Iteration: 5615 lambda_n: 0.9501398670115531 Loss: 0.00020191495923877067\n",
      "Iteration: 5616 lambda_n: 0.916967748607934 Loss: 0.00020191495924488866\n",
      "Iteration: 5617 lambda_n: 0.8827001490139185 Loss: 0.00020191495925075804\n",
      "Iteration: 5618 lambda_n: 1.002558738211483 Loss: 0.0002019149592563631\n",
      "Iteration: 5619 lambda_n: 1.0280920347690135 Loss: 0.00020191495926269863\n",
      "Iteration: 5620 lambda_n: 0.9581591545151625 Loss: 0.00020191495926914595\n",
      "Iteration: 5621 lambda_n: 0.9133358963276949 Loss: 0.00020191495927510673\n",
      "Iteration: 5622 lambda_n: 0.9142489102796392 Loss: 0.00020191495928074949\n",
      "Iteration: 5623 lambda_n: 1.0281348273869535 Loss: 0.00020191495928636565\n",
      "Iteration: 5624 lambda_n: 0.9872002965595748 Loss: 0.00020191495929264822\n",
      "Iteration: 5625 lambda_n: 0.89223103187066 Loss: 0.0002019149592986291\n",
      "Iteration: 5626 lambda_n: 0.9190949496138269 Loss: 0.0002019149593039963\n",
      "Iteration: 5627 lambda_n: 0.9427382737728505 Loss: 0.00020191495930949217\n",
      "Iteration: 5628 lambda_n: 0.9540347881206176 Loss: 0.00020191495931509446\n",
      "Iteration: 5629 lambda_n: 1.009445934661473 Loss: 0.00020191495932072692\n",
      "Iteration: 5630 lambda_n: 0.940157490564346 Loss: 0.00020191495932664805\n",
      "Iteration: 5631 lambda_n: 1.000623583716316 Loss: 0.00020191495933211633\n",
      "Iteration: 5632 lambda_n: 0.9102921618625521 Loss: 0.00020191495933790182\n",
      "Iteration: 5633 lambda_n: 0.9275723825653648 Loss: 0.00020191495934313087\n",
      "Iteration: 5634 lambda_n: 0.9576952120632749 Loss: 0.00020191495934842224\n",
      "Iteration: 5635 lambda_n: 0.9785991953031458 Loss: 0.00020191495935384266\n",
      "Iteration: 5636 lambda_n: 0.9893907626470532 Loss: 0.00020191495935936295\n",
      "Iteration: 5637 lambda_n: 1.007160746715058 Loss: 0.00020191495936488862\n",
      "Iteration: 5638 lambda_n: 0.9908158171599486 Loss: 0.0002019149593704741\n",
      "Iteration: 5639 lambda_n: 0.9479694295954182 Loss: 0.00020191495937593677\n",
      "Iteration: 5640 lambda_n: 1.0189672176063513 Loss: 0.0002019149593811211\n",
      "Iteration: 5641 lambda_n: 0.9050701505571028 Loss: 0.00020191495938665926\n",
      "Iteration: 5642 lambda_n: 0.9313046768392936 Loss: 0.00020191495939154904\n",
      "Iteration: 5643 lambda_n: 0.9271593707164016 Loss: 0.00020191495939654198\n",
      "Iteration: 5644 lambda_n: 0.9686585167504119 Loss: 0.00020191495940148402\n",
      "Iteration: 5645 lambda_n: 1.010933701385557 Loss: 0.00020191495940661527\n",
      "Iteration: 5646 lambda_n: 0.9036980758443787 Loss: 0.00020191495941192936\n",
      "Iteration: 5647 lambda_n: 0.9935598420583983 Loss: 0.00020191495941665046\n",
      "Iteration: 5648 lambda_n: 0.985657960234732 Loss: 0.00020191495942180603\n",
      "Iteration: 5649 lambda_n: 0.9204673705928785 Loss: 0.00020191495942687695\n",
      "Iteration: 5650 lambda_n: 0.9452985428102195 Loss: 0.00020191495943159085\n",
      "Iteration: 5651 lambda_n: 1.0070230557779658 Loss: 0.00020191495943639842\n",
      "Iteration: 5652 lambda_n: 0.9813256028838152 Loss: 0.00020191495944148641\n",
      "Iteration: 5653 lambda_n: 0.9658350707647797 Loss: 0.00020191495944639994\n",
      "Iteration: 5654 lambda_n: 0.9736853894129383 Loss: 0.00020191495945120984\n",
      "Iteration: 5655 lambda_n: 1.0153640476449475 Loss: 0.00020191495945602372\n",
      "Iteration: 5656 lambda_n: 1.0255397366025913 Loss: 0.00020191495946101417\n",
      "Iteration: 5657 lambda_n: 0.9374864817761656 Loss: 0.00020191495946602392\n",
      "Iteration: 5658 lambda_n: 0.8847739762063415 Loss: 0.00020191495947055905\n",
      "Iteration: 5659 lambda_n: 0.9237939080504902 Loss: 0.00020191495947481923\n",
      "Iteration: 5660 lambda_n: 0.9160662045387574 Loss: 0.00020191495947923367\n",
      "Iteration: 5661 lambda_n: 0.941281001263761 Loss: 0.00020191495948358306\n",
      "Iteration: 5662 lambda_n: 0.9602402275734416 Loss: 0.00020191495948802758\n",
      "Iteration: 5663 lambda_n: 0.9309014098201169 Loss: 0.00020191495949253063\n",
      "Iteration: 5664 lambda_n: 0.9931801084016699 Loss: 0.00020191495949686768\n",
      "Iteration: 5665 lambda_n: 0.891794859343545 Loss: 0.0002019149595014663\n",
      "Iteration: 5666 lambda_n: 0.8849238678026273 Loss: 0.00020191495950556566\n",
      "Iteration: 5667 lambda_n: 0.932343668602707 Loss: 0.0002019149595096074\n",
      "Iteration: 5668 lambda_n: 1.036915585519705 Loss: 0.0002019149595138373\n",
      "Iteration: 5669 lambda_n: 0.9706341861544712 Loss: 0.00020191495951851356\n",
      "Iteration: 5670 lambda_n: 0.9578354738821383 Loss: 0.0002019149595228545\n",
      "Iteration: 5671 lambda_n: 1.0211170185160328 Loss: 0.00020191495952710835\n",
      "Iteration: 5672 lambda_n: 0.8863678997595698 Loss: 0.00020191495953162191\n",
      "Iteration: 5673 lambda_n: 1.0365990143164454 Loss: 0.00020191495953550238\n",
      "Iteration: 5674 lambda_n: 1.0056835693465396 Loss: 0.0002019149595400159\n",
      "Iteration: 5675 lambda_n: 1.0266735332500747 Loss: 0.00020191495954436113\n",
      "Iteration: 5676 lambda_n: 0.9331307114701012 Loss: 0.00020191495954876546\n",
      "Iteration: 5677 lambda_n: 0.9254629940649298 Loss: 0.0002019149595527424\n",
      "Iteration: 5678 lambda_n: 0.9883103393756506 Loss: 0.00020191495955666384\n",
      "Iteration: 5679 lambda_n: 0.9549171163433463 Loss: 0.0002019149595608243\n",
      "Iteration: 5680 lambda_n: 0.9294772427415136 Loss: 0.00020191495956481558\n",
      "Iteration: 5681 lambda_n: 0.9334983216002969 Loss: 0.00020191495956867864\n",
      "Iteration: 5682 lambda_n: 0.975889371093091 Loss: 0.00020191495957252897\n",
      "Iteration: 5683 lambda_n: 0.9636421736979939 Loss: 0.00020191495957652211\n",
      "Iteration: 5684 lambda_n: 0.8939678290848544 Loss: 0.00020191495958043785\n",
      "Iteration: 5685 lambda_n: 1.034664763338983 Loss: 0.00020191495958405114\n",
      "Iteration: 5686 lambda_n: 0.9800427528904239 Loss: 0.00020191495958821152\n",
      "Iteration: 5687 lambda_n: 1.0013666377527992 Loss: 0.0002019149595921192\n",
      "Iteration: 5688 lambda_n: 0.8870057574282048 Loss: 0.00020191495959608562\n",
      "Iteration: 5689 lambda_n: 0.9089949603591069 Loss: 0.00020191495959957412\n",
      "Iteration: 5690 lambda_n: 0.9063560064100039 Loss: 0.00020191495960312754\n",
      "Iteration: 5691 lambda_n: 1.0336924382358978 Loss: 0.00020191495960664304\n",
      "Iteration: 5692 lambda_n: 0.991699347299597 Loss: 0.0002019149596106322\n",
      "Iteration: 5693 lambda_n: 0.9641514683763696 Loss: 0.00020191495961443428\n",
      "Iteration: 5694 lambda_n: 0.9970474753398317 Loss: 0.00020191495961809699\n",
      "Iteration: 5695 lambda_n: 1.0210394252556374 Loss: 0.00020191495962186982\n",
      "Iteration: 5696 lambda_n: 0.9774177968273192 Loss: 0.0002019149596257015\n",
      "Iteration: 5697 lambda_n: 0.889991236804716 Loss: 0.0002019149596293419\n",
      "Iteration: 5698 lambda_n: 0.9506365639197925 Loss: 0.00020191495963263044\n",
      "Iteration: 5699 lambda_n: 0.9130127337283364 Loss: 0.0002019149596361314\n",
      "Iteration: 5700 lambda_n: 0.9464790607542563 Loss: 0.00020191495963946284\n",
      "Iteration: 5701 lambda_n: 0.9854286984672239 Loss: 0.0002019149596428945\n",
      "Iteration: 5702 lambda_n: 0.8953617306870012 Loss: 0.0002019149596464494\n",
      "Iteration: 5703 lambda_n: 0.9557824181279401 Loss: 0.00020191495964966277\n",
      "Iteration: 5704 lambda_n: 1.022452243910924 Loss: 0.0002019149596530667\n",
      "Iteration: 5705 lambda_n: 0.9454730163820603 Loss: 0.00020191495965668155\n",
      "Iteration: 5706 lambda_n: 0.9920557725946797 Loss: 0.000201914959659999\n",
      "Iteration: 5707 lambda_n: 1.03400959712027 Loss: 0.00020191495966345944\n",
      "Iteration: 5708 lambda_n: 0.9343552124811555 Loss: 0.00020191495966704335\n",
      "Iteration: 5709 lambda_n: 0.9644953095308103 Loss: 0.00020191495967025681\n",
      "Iteration: 5710 lambda_n: 0.9655277487328817 Loss: 0.00020191495967355374\n",
      "Iteration: 5711 lambda_n: 0.9690538879934872 Loss: 0.00020191495967683068\n",
      "Iteration: 5712 lambda_n: 1.0106001873379142 Loss: 0.00020191495968010443\n",
      "Iteration: 5713 lambda_n: 0.9098028454255916 Loss: 0.00020191495968348397\n",
      "Iteration: 5714 lambda_n: 0.923090156469193 Loss: 0.000201914959686507\n",
      "Iteration: 5715 lambda_n: 1.0141892120738132 Loss: 0.00020191495968955802\n",
      "Iteration: 5716 lambda_n: 1.00131264217368 Loss: 0.00020191495969288422\n",
      "Iteration: 5717 lambda_n: 1.025264386253382 Loss: 0.00020191495969614325\n",
      "Iteration: 5718 lambda_n: 0.9721620689584365 Loss: 0.0002019149596994631\n",
      "Iteration: 5719 lambda_n: 1.0289235525848466 Loss: 0.00020191495970258675\n",
      "Iteration: 5720 lambda_n: 0.9466768402664637 Loss: 0.0002019149597058719\n",
      "Iteration: 5721 lambda_n: 0.9345478820658779 Loss: 0.00020191495970888333\n",
      "Iteration: 5722 lambda_n: 0.9007079139390697 Loss: 0.0002019149597118301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5723 lambda_n: 0.956107256722655 Loss: 0.00020191495971464728\n",
      "Iteration: 5724 lambda_n: 0.9458835303003461 Loss: 0.00020191495971762322\n",
      "Iteration: 5725 lambda_n: 0.9994936645861118 Loss: 0.00020191495972054653\n",
      "Iteration: 5726 lambda_n: 0.9250802984774301 Loss: 0.00020191495972361707\n",
      "Iteration: 5727 lambda_n: 0.8954732588191424 Loss: 0.00020191495972643337\n",
      "Iteration: 5728 lambda_n: 1.0144654616527853 Loss: 0.00020191495972914726\n",
      "Iteration: 5729 lambda_n: 0.9765571420464061 Loss: 0.00020191495973220406\n",
      "Iteration: 5730 lambda_n: 0.8851493321929437 Loss: 0.00020191495973511436\n",
      "Iteration: 5731 lambda_n: 1.0312561204248418 Loss: 0.00020191495973774385\n",
      "Iteration: 5732 lambda_n: 0.8900073550715386 Loss: 0.00020191495974079064\n",
      "Iteration: 5733 lambda_n: 1.0099513582690245 Loss: 0.0002019149597433986\n",
      "Iteration: 5734 lambda_n: 0.8893150006898126 Loss: 0.0002019149597463365\n",
      "Iteration: 5735 lambda_n: 1.020052912189707 Loss: 0.00020191495974890687\n",
      "Iteration: 5736 lambda_n: 0.9961975931001992 Loss: 0.00020191495975183536\n",
      "Iteration: 5737 lambda_n: 1.024618728250567 Loss: 0.00020191495975468358\n",
      "Iteration: 5738 lambda_n: 0.9013061071562778 Loss: 0.00020191495975759093\n",
      "Iteration: 5739 lambda_n: 0.9735626158159592 Loss: 0.00020191495976012796\n",
      "Iteration: 5740 lambda_n: 0.9607973638343219 Loss: 0.0002019149597628531\n",
      "Iteration: 5741 lambda_n: 0.974601405332199 Loss: 0.00020191495976552403\n",
      "Iteration: 5742 lambda_n: 0.95415631959247 Loss: 0.00020191495976821608\n",
      "Iteration: 5743 lambda_n: 0.8883122068179993 Loss: 0.00020191495977083052\n",
      "Iteration: 5744 lambda_n: 0.9979795110011074 Loss: 0.00020191495977324865\n",
      "Iteration: 5745 lambda_n: 0.9197557944163927 Loss: 0.00020191495977594484\n",
      "Iteration: 5746 lambda_n: 0.893035371489221 Loss: 0.00020191495977841685\n",
      "Iteration: 5747 lambda_n: 1.0126607809342052 Loss: 0.00020191495978080554\n",
      "Iteration: 5748 lambda_n: 0.9316776600664376 Loss: 0.00020191495978349167\n",
      "Iteration: 5749 lambda_n: 0.9480802105092621 Loss: 0.0002019149597859445\n",
      "Iteration: 5750 lambda_n: 0.9806781873787922 Loss: 0.00020191495978843252\n",
      "Iteration: 5751 lambda_n: 1.0254392894869735 Loss: 0.00020191495979098332\n",
      "Iteration: 5752 lambda_n: 0.9737993876819927 Loss: 0.0002019149597936261\n",
      "Iteration: 5753 lambda_n: 1.035537341928108 Loss: 0.00020191495979613152\n",
      "Iteration: 5754 lambda_n: 0.8905451281952425 Loss: 0.0002019149597987704\n",
      "Iteration: 5755 lambda_n: 0.9706479222734635 Loss: 0.00020191495980102656\n",
      "Iteration: 5756 lambda_n: 0.9731651893112797 Loss: 0.0002019149598034688\n",
      "Iteration: 5757 lambda_n: 0.8918096158190933 Loss: 0.00020191495980590555\n",
      "Iteration: 5758 lambda_n: 0.8858761588038457 Loss: 0.0002019149598081146\n",
      "Iteration: 5759 lambda_n: 0.9897297558221186 Loss: 0.00020191495981029765\n",
      "Iteration: 5760 lambda_n: 0.9376767452504128 Loss: 0.00020191495981272247\n",
      "Iteration: 5761 lambda_n: 1.022019342997275 Loss: 0.0002019149598149993\n",
      "Iteration: 5762 lambda_n: 0.9991783372538752 Loss: 0.00020191495981747306\n",
      "Iteration: 5763 lambda_n: 0.9608080156140048 Loss: 0.00020191495981986877\n",
      "Iteration: 5764 lambda_n: 0.9506003396300661 Loss: 0.00020191495982216498\n",
      "Iteration: 5765 lambda_n: 0.953329090675978 Loss: 0.00020191495982441483\n",
      "Iteration: 5766 lambda_n: 1.0166441774803971 Loss: 0.00020191495982666347\n",
      "Iteration: 5767 lambda_n: 0.9046867337583939 Loss: 0.0002019149598290376\n",
      "Iteration: 5768 lambda_n: 0.9281862448654938 Loss: 0.00020191495983114076\n",
      "Iteration: 5769 lambda_n: 0.8870186176696535 Loss: 0.00020191495983328518\n",
      "Iteration: 5770 lambda_n: 0.8948369189280255 Loss: 0.00020191495983531882\n",
      "Iteration: 5771 lambda_n: 0.9190310046962747 Loss: 0.00020191495983735646\n",
      "Iteration: 5772 lambda_n: 0.9235821722825026 Loss: 0.0002019149598394408\n",
      "Iteration: 5773 lambda_n: 1.0342271171039557 Loss: 0.00020191495984152444\n",
      "Iteration: 5774 lambda_n: 0.8966673415777204 Loss: 0.0002019149598438437\n",
      "Iteration: 5775 lambda_n: 1.011996602084935 Loss: 0.00020191495984583203\n",
      "Iteration: 5776 lambda_n: 0.9373631733437174 Loss: 0.0002019149598480649\n",
      "Iteration: 5777 lambda_n: 1.0124764375451452 Loss: 0.00020191495985012633\n",
      "Iteration: 5778 lambda_n: 0.9150405362724444 Loss: 0.00020191495985232667\n",
      "Iteration: 5779 lambda_n: 0.9851531782350692 Loss: 0.0002019149598543059\n",
      "Iteration: 5780 lambda_n: 0.9464536788109592 Loss: 0.00020191495985641967\n",
      "Iteration: 5781 lambda_n: 0.9221852133463341 Loss: 0.0002019149598584445\n",
      "Iteration: 5782 lambda_n: 0.8933409153785896 Loss: 0.0002019149598604002\n",
      "Iteration: 5783 lambda_n: 0.9261919497639984 Loss: 0.0002019149598622864\n",
      "Iteration: 5784 lambda_n: 0.8947822229859416 Loss: 0.00020191495986422888\n",
      "Iteration: 5785 lambda_n: 0.9532978191530852 Loss: 0.00020191495986608744\n",
      "Iteration: 5786 lambda_n: 0.9459282533756451 Loss: 0.00020191495986806194\n",
      "Iteration: 5787 lambda_n: 0.9462148177469145 Loss: 0.0002019149598700066\n",
      "Iteration: 5788 lambda_n: 0.8831698817259586 Loss: 0.00020191495987194048\n",
      "Iteration: 5789 lambda_n: 0.9785625877419464 Loss: 0.00020191495987373652\n",
      "Iteration: 5790 lambda_n: 0.9496691999473073 Loss: 0.00020191495987571443\n",
      "Iteration: 5791 lambda_n: 0.9092721192426987 Loss: 0.00020191495987762018\n",
      "Iteration: 5792 lambda_n: 0.8944544421637263 Loss: 0.0002019149598794328\n",
      "Iteration: 5793 lambda_n: 1.0362182767937673 Loss: 0.00020191495988120092\n",
      "Iteration: 5794 lambda_n: 0.938745637525237 Loss: 0.0002019149598832404\n",
      "Iteration: 5795 lambda_n: 0.8826062055803202 Loss: 0.00020191495988508245\n",
      "Iteration: 5796 lambda_n: 1.0193622352125302 Loss: 0.00020191495988679527\n",
      "Iteration: 5797 lambda_n: 0.999874194508439 Loss: 0.00020191495988875616\n",
      "Iteration: 5798 lambda_n: 0.9764339639876826 Loss: 0.00020191495989067647\n",
      "Iteration: 5799 lambda_n: 0.9258968178357769 Loss: 0.00020191495989253555\n",
      "Iteration: 5800 lambda_n: 0.9267088404250299 Loss: 0.00020191495989428264\n",
      "Iteration: 5801 lambda_n: 1.0130073231110945 Loss: 0.00020191495989602167\n",
      "Iteration: 5802 lambda_n: 0.9707427323374554 Loss: 0.0002019149598979085\n",
      "Iteration: 5803 lambda_n: 1.0109082781798704 Loss: 0.00020191495989971595\n",
      "Iteration: 5804 lambda_n: 0.992330835701544 Loss: 0.00020191495990157945\n",
      "Iteration: 5805 lambda_n: 0.8912631786912293 Loss: 0.0002019149599033905\n",
      "Iteration: 5806 lambda_n: 0.9690141356210271 Loss: 0.00020191495990501342\n",
      "Iteration: 5807 lambda_n: 1.0346206325024314 Loss: 0.00020191495990676907\n",
      "Iteration: 5808 lambda_n: 1.0127414557932441 Loss: 0.0002019149599086197\n",
      "Iteration: 5809 lambda_n: 0.8975829170783131 Loss: 0.000201914959910426\n",
      "Iteration: 5810 lambda_n: 0.9895391190121713 Loss: 0.00020191495991201728\n",
      "Iteration: 5811 lambda_n: 0.8918809214048758 Loss: 0.00020191495991375642\n",
      "Iteration: 5812 lambda_n: 1.008035077845727 Loss: 0.0002019149599153137\n",
      "Iteration: 5813 lambda_n: 0.9985039446408311 Loss: 0.00020191495991706492\n",
      "Iteration: 5814 lambda_n: 1.0303782382328688 Loss: 0.00020191495991878325\n",
      "Iteration: 5815 lambda_n: 0.9695540736063606 Loss: 0.00020191495992054247\n",
      "Iteration: 5816 lambda_n: 0.9729283286725106 Loss: 0.00020191495992219268\n",
      "Iteration: 5817 lambda_n: 0.9833883154024102 Loss: 0.00020191495992384197\n",
      "Iteration: 5818 lambda_n: 0.9423033092272096 Loss: 0.00020191495992548709\n",
      "Iteration: 5819 lambda_n: 1.0348101922562771 Loss: 0.00020191495992705492\n",
      "Iteration: 5820 lambda_n: 1.0086967649661904 Loss: 0.00020191495992876688\n",
      "Iteration: 5821 lambda_n: 0.9487194847181465 Loss: 0.00020191495993042836\n",
      "Iteration: 5822 lambda_n: 0.9985163404971857 Loss: 0.0002019149599319764\n",
      "Iteration: 5823 lambda_n: 1.028348302761448 Loss: 0.00020191495993359952\n",
      "Iteration: 5824 lambda_n: 0.8993965471325515 Loss: 0.00020191495993525233\n",
      "Iteration: 5825 lambda_n: 0.9860830245572725 Loss: 0.0002019149599366957\n",
      "Iteration: 5826 lambda_n: 0.8891262374271964 Loss: 0.00020191495993826365\n",
      "Iteration: 5827 lambda_n: 0.9065830251824146 Loss: 0.00020191495993966853\n",
      "Iteration: 5828 lambda_n: 0.9483432563301889 Loss: 0.00020191495994108572\n",
      "Iteration: 5829 lambda_n: 1.013621370915579 Loss: 0.0002019149599425652\n",
      "Iteration: 5830 lambda_n: 1.029408864999937 Loss: 0.0002019149599441311\n",
      "Iteration: 5831 lambda_n: 0.9251838920281898 Loss: 0.00020191495994571656\n",
      "Iteration: 5832 lambda_n: 0.914647226192991 Loss: 0.00020191495994713296\n",
      "Iteration: 5833 lambda_n: 1.0169562478240919 Loss: 0.00020191495994852034\n",
      "Iteration: 5834 lambda_n: 0.8984103989713211 Loss: 0.00020191495995005353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5835 lambda_n: 0.9851092073584089 Loss: 0.00020191495995140464\n",
      "Iteration: 5836 lambda_n: 0.9351141849523659 Loss: 0.00020191495995286474\n",
      "Iteration: 5837 lambda_n: 0.9219050606966831 Loss: 0.00020191495995424647\n",
      "Iteration: 5838 lambda_n: 0.9042475925353953 Loss: 0.00020191495995559928\n",
      "Iteration: 5839 lambda_n: 0.9941548906181656 Loss: 0.00020191495995691485\n",
      "Iteration: 5840 lambda_n: 1.036355878821895 Loss: 0.00020191495995836142\n",
      "Iteration: 5841 lambda_n: 1.029651185974719 Loss: 0.00020191495995985732\n",
      "Iteration: 5842 lambda_n: 0.9078653388382295 Loss: 0.00020191495996133436\n",
      "Iteration: 5843 lambda_n: 0.9299235579114865 Loss: 0.00020191495996262426\n",
      "Iteration: 5844 lambda_n: 0.9142256861986453 Loss: 0.00020191495996394582\n",
      "Iteration: 5845 lambda_n: 0.8859216690254503 Loss: 0.00020191495996522624\n",
      "Iteration: 5846 lambda_n: 0.9902299739986531 Loss: 0.0002019149599664668\n",
      "Iteration: 5847 lambda_n: 0.9223890405075997 Loss: 0.00020191495996784166\n",
      "Iteration: 5848 lambda_n: 0.9783791416071118 Loss: 0.00020191495996911134\n",
      "Iteration: 5849 lambda_n: 0.9732251072495979 Loss: 0.00020191495997045732\n",
      "Iteration: 5850 lambda_n: 1.0151557579295263 Loss: 0.00020191495997178122\n",
      "Iteration: 5851 lambda_n: 1.0079091649042464 Loss: 0.00020191495997315444\n",
      "Iteration: 5852 lambda_n: 0.9090935787852457 Loss: 0.00020191495997451053\n",
      "Iteration: 5853 lambda_n: 1.0316555828695075 Loss: 0.0002019149599757236\n",
      "Iteration: 5854 lambda_n: 0.9029583154860146 Loss: 0.0002019149599770932\n",
      "Iteration: 5855 lambda_n: 1.0235521982211895 Loss: 0.00020191495997827925\n",
      "Iteration: 5856 lambda_n: 1.028235989357952 Loss: 0.00020191495997962165\n",
      "Iteration: 5857 lambda_n: 0.97696544054859 Loss: 0.00020191495998095533\n",
      "Iteration: 5858 lambda_n: 1.0116295828901263 Loss: 0.00020191495998221883\n",
      "Iteration: 5859 lambda_n: 1.0316134422051657 Loss: 0.00020191495998351787\n",
      "Iteration: 5860 lambda_n: 0.9964499530282398 Loss: 0.00020191495998482604\n",
      "Iteration: 5861 lambda_n: 1.0322697455595802 Loss: 0.00020191495998608832\n",
      "Iteration: 5862 lambda_n: 0.9257312227798746 Loss: 0.00020191495998738253\n",
      "Iteration: 5863 lambda_n: 0.9907681348895606 Loss: 0.00020191495998853206\n",
      "Iteration: 5864 lambda_n: 0.9643711208193277 Loss: 0.000201914959989759\n",
      "Iteration: 5865 lambda_n: 0.994300621101684 Loss: 0.0002019149599909437\n",
      "Iteration: 5866 lambda_n: 1.0232326735292523 Loss: 0.00020191495999215855\n",
      "Iteration: 5867 lambda_n: 0.9580238362651712 Loss: 0.00020191495999339658\n",
      "Iteration: 5868 lambda_n: 0.8981522650681733 Loss: 0.0002019149599945534\n",
      "Iteration: 5869 lambda_n: 1.0375829083827093 Loss: 0.00020191495999562442\n",
      "Iteration: 5870 lambda_n: 0.9536765248476863 Loss: 0.00020191495999685103\n",
      "Iteration: 5871 lambda_n: 0.9777252385078892 Loss: 0.00020191495999798218\n",
      "Iteration: 5872 lambda_n: 0.9696667238653204 Loss: 0.0002019149599991351\n",
      "Iteration: 5873 lambda_n: 0.9873092534379686 Loss: 0.000201914960000265\n",
      "Iteration: 5874 lambda_n: 0.8834663053004154 Loss: 0.00020191496000140994\n",
      "Iteration: 5875 lambda_n: 1.0100028972097959 Loss: 0.00020191496000242947\n",
      "Iteration: 5876 lambda_n: 0.9888705573328399 Loss: 0.00020191496000358547\n",
      "Iteration: 5877 lambda_n: 0.8962132153293756 Loss: 0.00020191496000471188\n",
      "Iteration: 5878 lambda_n: 1.015761079997186 Loss: 0.00020191496000572534\n",
      "Iteration: 5879 lambda_n: 0.9542118841012297 Loss: 0.00020191496000686535\n",
      "Iteration: 5880 lambda_n: 0.959732094538913 Loss: 0.00020191496000792884\n",
      "Iteration: 5881 lambda_n: 0.8913961840644697 Loss: 0.00020191496000899765\n",
      "Iteration: 5882 lambda_n: 1.0004908991771015 Loss: 0.00020191496000997714\n",
      "Iteration: 5883 lambda_n: 0.9502801340458212 Loss: 0.00020191496001107124\n",
      "Iteration: 5884 lambda_n: 0.9201664537425893 Loss: 0.00020191496001211224\n",
      "Iteration: 5885 lambda_n: 0.9336216648807932 Loss: 0.0002019149600131065\n",
      "Iteration: 5886 lambda_n: 0.9583948754159168 Loss: 0.0002019149600141142\n",
      "Iteration: 5887 lambda_n: 1.0034886740390176 Loss: 0.00020191496001513887\n",
      "Iteration: 5888 lambda_n: 0.8972236037533469 Loss: 0.00020191496001619635\n",
      "Iteration: 5889 lambda_n: 0.9697794376480828 Loss: 0.00020191496001714041\n",
      "Iteration: 5890 lambda_n: 0.9094296764906787 Loss: 0.00020191496001815376\n",
      "Iteration: 5891 lambda_n: 0.9887899873439798 Loss: 0.00020191496001909556\n",
      "Iteration: 5892 lambda_n: 1.0357723942031942 Loss: 0.0002019149600201227\n",
      "Iteration: 5893 lambda_n: 0.9414153835651008 Loss: 0.0002019149600211806\n",
      "Iteration: 5894 lambda_n: 0.9782501173826333 Loss: 0.00020191496002214317\n",
      "Iteration: 5895 lambda_n: 0.9504400971495307 Loss: 0.00020191496002312765\n",
      "Iteration: 5896 lambda_n: 0.9884629793468963 Loss: 0.00020191496002408316\n",
      "Iteration: 5897 lambda_n: 1.0019605804642995 Loss: 0.0002019149600250728\n",
      "Iteration: 5898 lambda_n: 1.0189854265688043 Loss: 0.00020191496002606773\n",
      "Iteration: 5899 lambda_n: 0.9574253522428562 Loss: 0.00020191496002707436\n",
      "Iteration: 5900 lambda_n: 1.0134855840638388 Loss: 0.00020191496002801418\n",
      "Iteration: 5901 lambda_n: 1.0144976967271644 Loss: 0.0002019149600290019\n",
      "Iteration: 5902 lambda_n: 0.9240585379556521 Loss: 0.00020191496002998607\n",
      "Iteration: 5903 lambda_n: 0.9951181138775972 Loss: 0.00020191496003087048\n",
      "Iteration: 5904 lambda_n: 0.9351117667865569 Loss: 0.00020191496003181355\n",
      "Iteration: 5905 lambda_n: 1.0366474186856582 Loss: 0.00020191496003270362\n",
      "Iteration: 5906 lambda_n: 0.925396998203148 Loss: 0.00020191496003367604\n",
      "Iteration: 5907 lambda_n: 0.9964481607952914 Loss: 0.00020191496003453966\n",
      "Iteration: 5908 lambda_n: 1.026286130681918 Loss: 0.00020191496003546836\n",
      "Iteration: 5909 lambda_n: 0.9645623836525248 Loss: 0.00020191496003641894\n",
      "Iteration: 5910 lambda_n: 1.0106123578666666 Loss: 0.00020191496003729942\n",
      "Iteration: 5911 lambda_n: 1.0264498403515603 Loss: 0.00020191496003821592\n",
      "Iteration: 5912 lambda_n: 1.0333930269181268 Loss: 0.00020191496003914972\n",
      "Iteration: 5913 lambda_n: 0.9499360570974774 Loss: 0.00020191496004008162\n",
      "Iteration: 5914 lambda_n: 0.9281497029041549 Loss: 0.00020191496004092494\n",
      "Iteration: 5915 lambda_n: 0.9138855671580729 Loss: 0.00020191496004174606\n",
      "Iteration: 5916 lambda_n: 1.0041355070417635 Loss: 0.00020191496004255157\n",
      "Iteration: 5917 lambda_n: 0.9055536316428103 Loss: 0.00020191496004342448\n",
      "Iteration: 5918 lambda_n: 1.032780112260547 Loss: 0.00020191496004421004\n",
      "Iteration: 5919 lambda_n: 0.8844336490204451 Loss: 0.00020191496004510202\n",
      "Iteration: 5920 lambda_n: 1.0024772445177395 Loss: 0.00020191496004585998\n",
      "Iteration: 5921 lambda_n: 0.9292262001100834 Loss: 0.00020191496004671336\n",
      "Iteration: 5922 lambda_n: 1.0171702022979967 Loss: 0.00020191496004749984\n",
      "Iteration: 5923 lambda_n: 0.9764233763313813 Loss: 0.00020191496004835717\n",
      "Iteration: 5924 lambda_n: 0.9378354958215459 Loss: 0.0002019149600491757\n",
      "Iteration: 5925 lambda_n: 1.0279455242842854 Loss: 0.0002019149600499545\n",
      "Iteration: 5926 lambda_n: 0.9593784106094918 Loss: 0.00020191496005080421\n",
      "Iteration: 5927 lambda_n: 0.9209654947549817 Loss: 0.0002019149600515888\n",
      "Iteration: 5928 lambda_n: 1.029942212660205 Loss: 0.000201914960052338\n",
      "Iteration: 5929 lambda_n: 0.9541492099802328 Loss: 0.000201914960053175\n",
      "Iteration: 5930 lambda_n: 0.9456402381599945 Loss: 0.0002019149600539417\n",
      "Iteration: 5931 lambda_n: 0.976980517191267 Loss: 0.00020191496005470105\n",
      "Iteration: 5932 lambda_n: 1.0019172126965257 Loss: 0.0002019149600554802\n",
      "Iteration: 5933 lambda_n: 0.9717068640396832 Loss: 0.00020191496005626932\n",
      "Iteration: 5934 lambda_n: 0.9679093923991194 Loss: 0.00020191496005702623\n",
      "Iteration: 5935 lambda_n: 0.9997038382688652 Loss: 0.0002019149600577795\n",
      "Iteration: 5936 lambda_n: 0.9438294366095848 Loss: 0.00020191496005855908\n",
      "Iteration: 5937 lambda_n: 0.935114133498593 Loss: 0.00020191496005928715\n",
      "Iteration: 5938 lambda_n: 1.020425850313099 Loss: 0.00020191496005999944\n",
      "Iteration: 5939 lambda_n: 0.892004674752172 Loss: 0.00020191496006077402\n",
      "Iteration: 5940 lambda_n: 0.9469851149941096 Loss: 0.00020191496006144905\n",
      "Iteration: 5941 lambda_n: 0.9792266321618232 Loss: 0.0002019149600621525\n",
      "Iteration: 5942 lambda_n: 0.9432680617932007 Loss: 0.00020191496006288263\n",
      "Iteration: 5943 lambda_n: 0.9456204508508381 Loss: 0.00020191496006357956\n",
      "Iteration: 5944 lambda_n: 1.0278405005945028 Loss: 0.00020191496006426879\n",
      "Iteration: 5945 lambda_n: 0.8955653943799518 Loss: 0.00020191496006501754\n",
      "Iteration: 5946 lambda_n: 0.9153191306318504 Loss: 0.00020191496006567025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5947 lambda_n: 0.9327817216668487 Loss: 0.00020191496006633273\n",
      "Iteration: 5948 lambda_n: 0.9972780008274729 Loss: 0.00020191496006700013\n",
      "Iteration: 5949 lambda_n: 0.9887263955677843 Loss: 0.00020191496006771297\n",
      "Iteration: 5950 lambda_n: 0.9460328506495956 Loss: 0.00020191496006841185\n",
      "Iteration: 5951 lambda_n: 0.8852013011974246 Loss: 0.00020191496006907107\n",
      "Iteration: 5952 lambda_n: 0.9931267024189399 Loss: 0.00020191496006969253\n",
      "Iteration: 5953 lambda_n: 0.9626627328072327 Loss: 0.00020191496007038322\n",
      "Iteration: 5954 lambda_n: 0.9419686674163609 Loss: 0.00020191496007104754\n",
      "Iteration: 5955 lambda_n: 1.0327907006821644 Loss: 0.0002019149600716936\n",
      "Iteration: 5956 lambda_n: 0.9138007579496935 Loss: 0.0002019149600723987\n",
      "Iteration: 5957 lambda_n: 1.0291429677384025 Loss: 0.00020191496007301976\n",
      "Iteration: 5958 lambda_n: 0.9239412493293042 Loss: 0.00020191496007371313\n",
      "Iteration: 5959 lambda_n: 1.0076683663630805 Loss: 0.00020191496007433156\n",
      "Iteration: 5960 lambda_n: 0.897147019106961 Loss: 0.00020191496007500824\n",
      "Iteration: 5961 lambda_n: 0.8834795388242209 Loss: 0.00020191496007560358\n",
      "Iteration: 5962 lambda_n: 0.9115912096006052 Loss: 0.0002019149600761836\n",
      "Iteration: 5963 lambda_n: 0.9970266245626153 Loss: 0.00020191496007677492\n",
      "Iteration: 5964 lambda_n: 0.9326371993671471 Loss: 0.0002019149600774191\n",
      "Iteration: 5965 lambda_n: 0.9318329296927875 Loss: 0.00020191496007802414\n",
      "Iteration: 5966 lambda_n: 0.9671276286184952 Loss: 0.00020191496007861923\n",
      "Iteration: 5967 lambda_n: 1.0084036465895732 Loss: 0.0002019149600792426\n",
      "Iteration: 5968 lambda_n: 0.8941070763162509 Loss: 0.00020191496007988214\n",
      "Iteration: 5969 lambda_n: 1.0098393635426042 Loss: 0.0002019149600804477\n",
      "Iteration: 5970 lambda_n: 0.9813329359668652 Loss: 0.00020191496008106988\n",
      "Iteration: 5971 lambda_n: 1.0317775697455014 Loss: 0.0002019149600816817\n",
      "Iteration: 5972 lambda_n: 0.99764768240301 Loss: 0.00020191496008231351\n",
      "Iteration: 5973 lambda_n: 0.8891543799557137 Loss: 0.0002019149600829223\n",
      "Iteration: 5974 lambda_n: 0.9731013620541679 Loss: 0.00020191496008346513\n",
      "Iteration: 5975 lambda_n: 0.9864407532815481 Loss: 0.00020191496008405298\n",
      "Iteration: 5976 lambda_n: 0.9113747022427435 Loss: 0.00020191496008464412\n",
      "Iteration: 5977 lambda_n: 0.9201408858983413 Loss: 0.0002019149600851878\n",
      "Iteration: 5978 lambda_n: 1.0047119238276034 Loss: 0.00020191496008572534\n",
      "Iteration: 5979 lambda_n: 0.9555761073090121 Loss: 0.0002019149600863116\n",
      "Iteration: 5980 lambda_n: 0.8998478234162296 Loss: 0.00020191496008686706\n",
      "Iteration: 5981 lambda_n: 0.9675061871556271 Loss: 0.00020191496008739392\n",
      "Iteration: 5982 lambda_n: 0.9891634831731949 Loss: 0.0002019149600879531\n",
      "Iteration: 5983 lambda_n: 0.9481706794625571 Loss: 0.0002019149600885177\n",
      "Iteration: 5984 lambda_n: 0.9567955562335551 Loss: 0.00020191496008905164\n",
      "Iteration: 5985 lambda_n: 0.9016310777748463 Loss: 0.0002019149600895923\n",
      "Iteration: 5986 lambda_n: 0.9783797401595504 Loss: 0.00020191496009009448\n",
      "Iteration: 5987 lambda_n: 0.9010302240489408 Loss: 0.00020191496009064135\n",
      "Iteration: 5988 lambda_n: 1.0252185602578932 Loss: 0.00020191496009114423\n",
      "Iteration: 5989 lambda_n: 0.9468845600287851 Loss: 0.0002019149600917112\n",
      "Iteration: 5990 lambda_n: 0.9744974757283145 Loss: 0.00020191496009222478\n",
      "Iteration: 5991 lambda_n: 0.9711585649320432 Loss: 0.00020191496009275452\n",
      "Iteration: 5992 lambda_n: 0.9058633478231699 Loss: 0.0002019149600932861\n",
      "Iteration: 5993 lambda_n: 0.950401879189518 Loss: 0.0002019149600937759\n",
      "Iteration: 5994 lambda_n: 0.9647197011940982 Loss: 0.00020191496009428313\n",
      "Iteration: 5995 lambda_n: 0.9976842589641062 Loss: 0.00020191496009479607\n",
      "Iteration: 5996 lambda_n: 0.8940566240100148 Loss: 0.00020191496009532578\n",
      "Iteration: 5997 lambda_n: 0.9986095175647122 Loss: 0.0002019149600957932\n",
      "Iteration: 5998 lambda_n: 0.9674707426829611 Loss: 0.00020191496009631135\n",
      "Iteration: 5999 lambda_n: 0.9957341779564516 Loss: 0.00020191496009681545\n",
      "Iteration: 6000 lambda_n: 0.8896810839142466 Loss: 0.00020191496009733034\n",
      "Iteration: 6001 lambda_n: 0.9310415805881136 Loss: 0.00020191496009778454\n",
      "Iteration: 6002 lambda_n: 0.9323440880754413 Loss: 0.00020191496009825635\n",
      "Iteration: 6003 lambda_n: 1.026918780091625 Loss: 0.00020191496009872725\n",
      "Iteration: 6004 lambda_n: 0.9516877528132962 Loss: 0.00020191496009924816\n",
      "Iteration: 6005 lambda_n: 0.9706828999545744 Loss: 0.0002019149600997249\n",
      "Iteration: 6006 lambda_n: 1.011949317694626 Loss: 0.0002019149601002061\n",
      "Iteration: 6007 lambda_n: 0.962757811820113 Loss: 0.00020191496010069887\n",
      "Iteration: 6008 lambda_n: 0.9806445735468016 Loss: 0.00020191496010117072\n",
      "Iteration: 6009 lambda_n: 1.0186270474695134 Loss: 0.00020191496010164728\n",
      "Iteration: 6010 lambda_n: 0.9289833680214067 Loss: 0.000201914960102135\n",
      "Iteration: 6011 lambda_n: 0.8879099044113606 Loss: 0.00020191496010258224\n",
      "Iteration: 6012 lambda_n: 0.9340243808172115 Loss: 0.00020191496010300887\n",
      "Iteration: 6013 lambda_n: 0.9616683958325544 Loss: 0.00020191496010344602\n",
      "Iteration: 6014 lambda_n: 1.01819640874598 Loss: 0.00020191496010390147\n",
      "Iteration: 6015 lambda_n: 0.9532531598292352 Loss: 0.0002019149601043777\n",
      "Iteration: 6016 lambda_n: 1.009179420149434 Loss: 0.00020191496010481141\n",
      "Iteration: 6017 lambda_n: 0.9777085848143875 Loss: 0.00020191496010528323\n",
      "Iteration: 6018 lambda_n: 0.9160834126928553 Loss: 0.00020191496010573087\n",
      "Iteration: 6019 lambda_n: 0.9338374568102885 Loss: 0.00020191496010613905\n",
      "Iteration: 6020 lambda_n: 0.8906776142775205 Loss: 0.0002019149601065603\n",
      "Iteration: 6021 lambda_n: 0.9984148777484236 Loss: 0.00020191496010696242\n",
      "Iteration: 6022 lambda_n: 0.9179515903791925 Loss: 0.0002019149601074059\n",
      "Iteration: 6023 lambda_n: 0.9687872409822182 Loss: 0.00020191496010782335\n",
      "Iteration: 6024 lambda_n: 0.9786779543053109 Loss: 0.00020191496010824546\n",
      "Iteration: 6025 lambda_n: 0.9591027320758935 Loss: 0.00020191496010867906\n",
      "Iteration: 6026 lambda_n: 0.9866819494725121 Loss: 0.00020191496010910263\n",
      "Iteration: 6027 lambda_n: 0.9670722020558661 Loss: 0.00020191496010952924\n",
      "Iteration: 6028 lambda_n: 1.00093604440653 Loss: 0.00020191496010994674\n",
      "Iteration: 6029 lambda_n: 0.9524770411848212 Loss: 0.00020191496011037077\n",
      "Iteration: 6030 lambda_n: 1.0381130320508865 Loss: 0.00020191496011077767\n",
      "Iteration: 6031 lambda_n: 0.9461003316571567 Loss: 0.00020191496011121786\n",
      "Iteration: 6032 lambda_n: 0.9828234649032788 Loss: 0.0002019149601116113\n",
      "Iteration: 6033 lambda_n: 0.9778332703446041 Loss: 0.00020191496011201981\n",
      "Iteration: 6034 lambda_n: 0.9295300768669279 Loss: 0.00020191496011242162\n",
      "Iteration: 6035 lambda_n: 1.0070824776169292 Loss: 0.0002019149601128036\n",
      "Iteration: 6036 lambda_n: 0.9278903818733553 Loss: 0.00020191496011321233\n",
      "Iteration: 6037 lambda_n: 0.9784611917590317 Loss: 0.00020191496011358294\n",
      "Iteration: 6038 lambda_n: 0.9114981868047981 Loss: 0.00020191496011398114\n",
      "Iteration: 6039 lambda_n: 0.9412221331413798 Loss: 0.0002019149601143443\n",
      "Iteration: 6040 lambda_n: 1.03130130722042 Loss: 0.00020191496011471566\n",
      "Iteration: 6041 lambda_n: 1.020540386816557 Loss: 0.00020191496011512874\n",
      "Iteration: 6042 lambda_n: 1.0123238976702584 Loss: 0.00020191496011552287\n",
      "Iteration: 6043 lambda_n: 0.9789676809901524 Loss: 0.000201914960115921\n",
      "Iteration: 6044 lambda_n: 0.976851559945918 Loss: 0.00020191496011630005\n",
      "Iteration: 6045 lambda_n: 1.001986781873698 Loss: 0.00020191496011667042\n",
      "Iteration: 6046 lambda_n: 0.9571649270933406 Loss: 0.00020191496011704628\n",
      "Iteration: 6047 lambda_n: 0.9577282798250826 Loss: 0.00020191496011741171\n",
      "Iteration: 6048 lambda_n: 0.8864514010799556 Loss: 0.00020191496011776885\n",
      "Iteration: 6049 lambda_n: 0.914904319742199 Loss: 0.00020191496011810235\n",
      "Iteration: 6050 lambda_n: 0.9792464082289242 Loss: 0.00020191496011844526\n",
      "Iteration: 6051 lambda_n: 0.8895299223302673 Loss: 0.00020191496011880456\n",
      "Iteration: 6052 lambda_n: 1.0375601061774142 Loss: 0.00020191496011913717\n",
      "Iteration: 6053 lambda_n: 0.9466666412037544 Loss: 0.00020191496011952144\n",
      "Iteration: 6054 lambda_n: 0.9254931031675636 Loss: 0.00020191496011986765\n",
      "Iteration: 6055 lambda_n: 1.0104001087653205 Loss: 0.0002019149601202034\n",
      "Iteration: 6056 lambda_n: 0.9210693592061919 Loss: 0.0002019149601205641\n",
      "Iteration: 6057 lambda_n: 0.8887728181989334 Loss: 0.0002019149601208998\n",
      "Iteration: 6058 lambda_n: 1.027528710509135 Loss: 0.0002019149601212094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6059 lambda_n: 1.0112964493391992 Loss: 0.00020191496012156608\n",
      "Iteration: 6060 lambda_n: 0.922411451740876 Loss: 0.0002019149601219158\n",
      "Iteration: 6061 lambda_n: 0.9800389667305288 Loss: 0.00020191496012223996\n",
      "Iteration: 6062 lambda_n: 0.8918367843570766 Loss: 0.0002019149601225802\n",
      "Iteration: 6063 lambda_n: 0.9658002458679485 Loss: 0.00020191496012288038\n",
      "Iteration: 6064 lambda_n: 0.9847951591529609 Loss: 0.00020191496012320935\n",
      "Iteration: 6065 lambda_n: 0.940496828768916 Loss: 0.00020191496012353976\n",
      "Iteration: 6066 lambda_n: 0.9116813675738251 Loss: 0.0002019149601238606\n",
      "Iteration: 6067 lambda_n: 0.994976446070495 Loss: 0.00020191496012416253\n",
      "Iteration: 6068 lambda_n: 1.0114267735889855 Loss: 0.00020191496012449613\n",
      "Iteration: 6069 lambda_n: 0.904604520430248 Loss: 0.0002019149601248286\n",
      "Iteration: 6070 lambda_n: 0.9079871644372727 Loss: 0.00020191496012512673\n",
      "Iteration: 6071 lambda_n: 1.0373381976239966 Loss: 0.00020191496012542088\n",
      "Iteration: 6072 lambda_n: 1.005325188131537 Loss: 0.00020191496012575465\n",
      "Iteration: 6073 lambda_n: 0.9751869037228768 Loss: 0.0002019149601260745\n",
      "Iteration: 6074 lambda_n: 0.9048958486909079 Loss: 0.00020191496012639162\n",
      "Iteration: 6075 lambda_n: 1.0183654831324138 Loss: 0.00020191496012667668\n",
      "Iteration: 6076 lambda_n: 0.9429997017148513 Loss: 0.00020191496012700178\n",
      "Iteration: 6077 lambda_n: 1.026758756482844 Loss: 0.00020191496012730422\n",
      "Iteration: 6078 lambda_n: 0.9483958523729474 Loss: 0.00020191496012762642\n",
      "Iteration: 6079 lambda_n: 0.9305525299617124 Loss: 0.00020191496012791918\n",
      "Iteration: 6080 lambda_n: 0.9326609513076931 Loss: 0.00020191496012820874\n",
      "Iteration: 6081 lambda_n: 1.0245829633808667 Loss: 0.00020191496012849877\n",
      "Iteration: 6082 lambda_n: 0.9626156366972686 Loss: 0.0002019149601288146\n",
      "Iteration: 6083 lambda_n: 0.9773830257428618 Loss: 0.00020191496012911147\n",
      "Iteration: 6084 lambda_n: 1.000963073055796 Loss: 0.0002019149601294028\n",
      "Iteration: 6085 lambda_n: 0.9159551451741952 Loss: 0.00020191496012970976\n",
      "Iteration: 6086 lambda_n: 1.0215923941502336 Loss: 0.00020191496012998618\n",
      "Iteration: 6087 lambda_n: 0.981206083207671 Loss: 0.0002019149601302868\n",
      "Iteration: 6088 lambda_n: 0.9173511964228673 Loss: 0.00020191496013057417\n",
      "Iteration: 6089 lambda_n: 0.9598679009581417 Loss: 0.00020191496013084045\n",
      "Iteration: 6090 lambda_n: 1.0317548257999782 Loss: 0.0002019149601311181\n",
      "Iteration: 6091 lambda_n: 0.9750134526947701 Loss: 0.00020191496013141454\n",
      "Iteration: 6092 lambda_n: 1.0072431812735936 Loss: 0.00020191496013169204\n",
      "Iteration: 6093 lambda_n: 1.0236184646756403 Loss: 0.00020191496013197312\n",
      "Iteration: 6094 lambda_n: 1.0045472180127366 Loss: 0.00020191496013226157\n",
      "Iteration: 6095 lambda_n: 1.0078803905035272 Loss: 0.00020191496013254\n",
      "Iteration: 6096 lambda_n: 1.0116649226633376 Loss: 0.0002019149601328214\n",
      "Iteration: 6097 lambda_n: 1.03667907796943 Loss: 0.0002019149601330949\n",
      "Iteration: 6098 lambda_n: 1.0359311943555443 Loss: 0.00020191496013337868\n",
      "Iteration: 6099 lambda_n: 1.0146089329452563 Loss: 0.00020191496013366103\n",
      "Iteration: 6100 lambda_n: 0.9970189776846297 Loss: 0.00020191496013393422\n",
      "Iteration: 6101 lambda_n: 0.9317995865893179 Loss: 0.0002019149601341967\n",
      "Iteration: 6102 lambda_n: 0.9239236677104556 Loss: 0.00020191496013444445\n",
      "Iteration: 6103 lambda_n: 0.9165061381781205 Loss: 0.00020191496013469138\n",
      "Iteration: 6104 lambda_n: 1.019876247364424 Loss: 0.00020191496013492792\n",
      "Iteration: 6105 lambda_n: 0.9248203039044246 Loss: 0.0002019149601352033\n",
      "Iteration: 6106 lambda_n: 0.9475521852285202 Loss: 0.000201914960135446\n",
      "Iteration: 6107 lambda_n: 0.9843483421241584 Loss: 0.00020191496013568762\n",
      "Iteration: 6108 lambda_n: 0.9501755027224138 Loss: 0.00020191496013594086\n",
      "Iteration: 6109 lambda_n: 1.0018610964639494 Loss: 0.00020191496013618598\n",
      "Iteration: 6110 lambda_n: 1.026913659183206 Loss: 0.00020191496013644374\n",
      "Iteration: 6111 lambda_n: 0.9116728314697199 Loss: 0.00020191496013670295\n",
      "Iteration: 6112 lambda_n: 0.8998927622064469 Loss: 0.00020191496013692911\n",
      "Iteration: 6113 lambda_n: 0.9374928113357829 Loss: 0.00020191496013715463\n",
      "Iteration: 6114 lambda_n: 0.9250470623176212 Loss: 0.00020191496013738827\n",
      "Iteration: 6115 lambda_n: 1.024994199370998 Loss: 0.00020191496013761856\n",
      "Iteration: 6116 lambda_n: 1.0177434228480422 Loss: 0.00020191496013787104\n",
      "Iteration: 6117 lambda_n: 0.9133571087384157 Loss: 0.00020191496013812263\n",
      "Iteration: 6118 lambda_n: 1.0095514908254841 Loss: 0.00020191496013834253\n",
      "Iteration: 6119 lambda_n: 0.9886641287297497 Loss: 0.00020191496013858358\n",
      "Iteration: 6120 lambda_n: 0.9721401141488569 Loss: 0.00020191496013882143\n",
      "Iteration: 6121 lambda_n: 0.9348752905280684 Loss: 0.00020191496013905136\n",
      "Iteration: 6122 lambda_n: 0.901711185620775 Loss: 0.00020191496013926947\n",
      "Iteration: 6123 lambda_n: 0.9347793492542023 Loss: 0.0002019149601394786\n",
      "Iteration: 6124 lambda_n: 0.9738598920640928 Loss: 0.0002019149601396978\n",
      "Iteration: 6125 lambda_n: 0.9182323726736328 Loss: 0.0002019149601399213\n",
      "Iteration: 6126 lambda_n: 0.9601885431302822 Loss: 0.00020191496014014033\n",
      "Iteration: 6127 lambda_n: 0.9827873044065637 Loss: 0.0002019149601403559\n",
      "Iteration: 6128 lambda_n: 0.906892704409934 Loss: 0.00020191496014058353\n",
      "Iteration: 6129 lambda_n: 1.0070716017757126 Loss: 0.00020191496014078278\n",
      "Iteration: 6130 lambda_n: 0.9610390018628643 Loss: 0.00020191496014100403\n",
      "Iteration: 6131 lambda_n: 1.0214270502205651 Loss: 0.00020191496014121708\n",
      "Iteration: 6132 lambda_n: 0.9231984914119505 Loss: 0.00020191496014144425\n",
      "Iteration: 6133 lambda_n: 0.8982162781491496 Loss: 0.00020191496014164916\n",
      "Iteration: 6134 lambda_n: 1.0037128290060529 Loss: 0.00020191496014184334\n",
      "Iteration: 6135 lambda_n: 1.0188216403855392 Loss: 0.00020191496014206428\n",
      "Iteration: 6136 lambda_n: 0.9417631803239263 Loss: 0.0002019149601422882\n",
      "Iteration: 6137 lambda_n: 0.9128832282101609 Loss: 0.0002019149601424873\n",
      "Iteration: 6138 lambda_n: 0.9734831924420948 Loss: 0.00020191496014268102\n",
      "Iteration: 6139 lambda_n: 0.9867797624514406 Loss: 0.00020191496014288743\n",
      "Iteration: 6140 lambda_n: 0.9401694447566863 Loss: 0.0002019149601430974\n",
      "Iteration: 6141 lambda_n: 0.9082943279107055 Loss: 0.00020191496014329512\n",
      "Iteration: 6142 lambda_n: 0.9243614227094624 Loss: 0.00020191496014348276\n",
      "Iteration: 6143 lambda_n: 0.8855934912079666 Loss: 0.0002019149601436723\n",
      "Iteration: 6144 lambda_n: 0.9557101589317223 Loss: 0.00020191496014384977\n",
      "Iteration: 6145 lambda_n: 0.9443978800426853 Loss: 0.0002019149601440502\n",
      "Iteration: 6146 lambda_n: 0.9057034383983892 Loss: 0.00020191496014424385\n",
      "Iteration: 6147 lambda_n: 1.006216580517836 Loss: 0.0002019149601444278\n",
      "Iteration: 6148 lambda_n: 1.031024742816241 Loss: 0.00020191496014462424\n",
      "Iteration: 6149 lambda_n: 0.9310399335531754 Loss: 0.00020191496014483544\n",
      "Iteration: 6150 lambda_n: 1.0225825174307928 Loss: 0.0002019149601450183\n",
      "Iteration: 6151 lambda_n: 0.9302988551718017 Loss: 0.00020191496014521987\n",
      "Iteration: 6152 lambda_n: 1.0283060557090784 Loss: 0.00020191496014539893\n",
      "Iteration: 6153 lambda_n: 0.8922058706469743 Loss: 0.0002019149601456011\n",
      "Iteration: 6154 lambda_n: 1.0354405672420075 Loss: 0.00020191496014577225\n",
      "Iteration: 6155 lambda_n: 1.015223007752104 Loss: 0.0002019149601459691\n",
      "Iteration: 6156 lambda_n: 0.926415810383008 Loss: 0.0002019149601461525\n",
      "Iteration: 6157 lambda_n: 0.9897056505422209 Loss: 0.0002019149601463226\n",
      "Iteration: 6158 lambda_n: 1.022917076878872 Loss: 0.00020191496014651322\n",
      "Iteration: 6159 lambda_n: 1.0092211289473274 Loss: 0.00020191496014670835\n",
      "Iteration: 6160 lambda_n: 0.9345947645929179 Loss: 0.0002019149601468909\n",
      "Iteration: 6161 lambda_n: 0.9256674084698382 Loss: 0.00020191496014705936\n",
      "Iteration: 6162 lambda_n: 0.9768885566692619 Loss: 0.00020191496014723082\n",
      "Iteration: 6163 lambda_n: 0.944952437880917 Loss: 0.00020191496014740926\n",
      "Iteration: 6164 lambda_n: 0.8882409100360005 Loss: 0.00020191496014757926\n",
      "Iteration: 6165 lambda_n: 0.9700062209330274 Loss: 0.00020191496014774593\n",
      "Iteration: 6166 lambda_n: 0.9658940109680131 Loss: 0.00020191496014791496\n",
      "Iteration: 6167 lambda_n: 0.8877679116618902 Loss: 0.0002019149601480849\n",
      "Iteration: 6168 lambda_n: 0.9974059201736644 Loss: 0.00020191496014824623\n",
      "Iteration: 6169 lambda_n: 1.0232261714531818 Loss: 0.00020191496014842396\n",
      "Iteration: 6170 lambda_n: 0.9077717660928194 Loss: 0.00020191496014860928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6171 lambda_n: 0.8942175532609172 Loss: 0.0002019149601487634\n",
      "Iteration: 6172 lambda_n: 1.0065910322783134 Loss: 0.00020191496014892505\n",
      "Iteration: 6173 lambda_n: 1.0097342699443934 Loss: 0.00020191496014909497\n",
      "Iteration: 6174 lambda_n: 0.9624739181434452 Loss: 0.00020191496014926435\n",
      "Iteration: 6175 lambda_n: 0.8911604413844182 Loss: 0.00020191496014942625\n",
      "Iteration: 6176 lambda_n: 0.9056944598345908 Loss: 0.00020191496014957118\n",
      "Iteration: 6177 lambda_n: 1.0337146188255075 Loss: 0.0002019149601497218\n",
      "Iteration: 6178 lambda_n: 0.9957578761478187 Loss: 0.0002019149601498944\n",
      "Iteration: 6179 lambda_n: 0.9652921276506167 Loss: 0.00020191496015006417\n",
      "Iteration: 6180 lambda_n: 1.019064281672604 Loss: 0.00020191496015022436\n",
      "Iteration: 6181 lambda_n: 0.9277530513153236 Loss: 0.00020191496015038385\n",
      "Iteration: 6182 lambda_n: 0.9333354472112254 Loss: 0.00020191496015054076\n",
      "Iteration: 6183 lambda_n: 0.8992639052878172 Loss: 0.00020191496015069168\n",
      "Iteration: 6184 lambda_n: 0.9898572394603112 Loss: 0.0002019149601508405\n",
      "Iteration: 6185 lambda_n: 0.8912303577375446 Loss: 0.00020191496015099875\n",
      "Iteration: 6186 lambda_n: 0.9380357482806753 Loss: 0.00020191496015113742\n",
      "Iteration: 6187 lambda_n: 0.9404051063169889 Loss: 0.0002019149601512853\n",
      "Iteration: 6188 lambda_n: 0.9904707034725593 Loss: 0.0002019149601514309\n",
      "Iteration: 6189 lambda_n: 0.8966319429423012 Loss: 0.0002019149601515821\n",
      "Iteration: 6190 lambda_n: 0.8936243265228688 Loss: 0.00020191496015171975\n",
      "Iteration: 6191 lambda_n: 0.9176098730247436 Loss: 0.00020191496015185739\n",
      "Iteration: 6192 lambda_n: 0.8895617397009121 Loss: 0.00020191496015199858\n",
      "Iteration: 6193 lambda_n: 1.0316451932747546 Loss: 0.0002019149601521298\n",
      "Iteration: 6194 lambda_n: 0.9530327876670784 Loss: 0.00020191496015228627\n",
      "Iteration: 6195 lambda_n: 0.9719123620292923 Loss: 0.00020191496015242166\n",
      "Iteration: 6196 lambda_n: 1.0095332912358934 Loss: 0.00020191496015256044\n",
      "Iteration: 6197 lambda_n: 1.0289582831458592 Loss: 0.00020191496015270268\n",
      "Iteration: 6198 lambda_n: 1.033855536022671 Loss: 0.00020191496015286212\n",
      "Iteration: 6199 lambda_n: 0.9470213355808187 Loss: 0.00020191496015301556\n",
      "Iteration: 6200 lambda_n: 0.8972355144273513 Loss: 0.00020191496015315203\n",
      "Iteration: 6201 lambda_n: 0.9306390794333178 Loss: 0.00020191496015327335\n",
      "Iteration: 6202 lambda_n: 0.9051819790338078 Loss: 0.00020191496015340828\n",
      "Iteration: 6203 lambda_n: 0.9530055950874822 Loss: 0.0002019149601535313\n",
      "Iteration: 6204 lambda_n: 0.9291888386848649 Loss: 0.00020191496015366288\n",
      "Iteration: 6205 lambda_n: 0.9774029934707561 Loss: 0.0002019149601537911\n",
      "Iteration: 6206 lambda_n: 0.9878433493229467 Loss: 0.000201914960153928\n",
      "Iteration: 6207 lambda_n: 0.9635050553024429 Loss: 0.00020191496015406203\n",
      "Iteration: 6208 lambda_n: 1.025097296460389 Loss: 0.0002019149601541884\n",
      "Iteration: 6209 lambda_n: 1.0287074620457473 Loss: 0.00020191496015432023\n",
      "Iteration: 6210 lambda_n: 0.9655788764385512 Loss: 0.00020191496015445695\n",
      "Iteration: 6211 lambda_n: 1.0145769068113466 Loss: 0.0002019149601545801\n",
      "Iteration: 6212 lambda_n: 0.961249405718488 Loss: 0.00020191496015470868\n",
      "Iteration: 6213 lambda_n: 0.9486139868272869 Loss: 0.00020191496015483498\n",
      "Iteration: 6214 lambda_n: 0.9753164044372071 Loss: 0.00020191496015495894\n",
      "Iteration: 6215 lambda_n: 0.8990343378986945 Loss: 0.00020191496015509313\n",
      "Iteration: 6216 lambda_n: 1.0234962695050132 Loss: 0.00020191496015520112\n",
      "Iteration: 6217 lambda_n: 0.9971909028093127 Loss: 0.00020191496015532719\n",
      "Iteration: 6218 lambda_n: 0.9014623197248837 Loss: 0.00020191496015544834\n",
      "Iteration: 6219 lambda_n: 0.9475337593257916 Loss: 0.00020191496015556173\n",
      "Iteration: 6220 lambda_n: 0.9371059455052468 Loss: 0.00020191496015567714\n",
      "Iteration: 6221 lambda_n: 0.8930623033729241 Loss: 0.0002019149601557961\n",
      "Iteration: 6222 lambda_n: 1.0088410517223299 Loss: 0.00020191496015590322\n",
      "Iteration: 6223 lambda_n: 0.9376601419538338 Loss: 0.00020191496015602373\n",
      "Iteration: 6224 lambda_n: 0.9855860953511442 Loss: 0.0002019149601561348\n",
      "Iteration: 6225 lambda_n: 0.9696990402557419 Loss: 0.0002019149601562571\n",
      "Iteration: 6226 lambda_n: 1.0159740744747907 Loss: 0.00020191496015638192\n",
      "Iteration: 6227 lambda_n: 0.8994021291770038 Loss: 0.0002019149601565006\n",
      "Iteration: 6228 lambda_n: 0.9560991982724977 Loss: 0.0002019149601566026\n",
      "Iteration: 6229 lambda_n: 0.9836848376564113 Loss: 0.00020191496015671347\n",
      "Iteration: 6230 lambda_n: 1.0236164878612082 Loss: 0.00020191496015682986\n",
      "Iteration: 6231 lambda_n: 0.9775699307230934 Loss: 0.00020191496015694162\n",
      "Iteration: 6232 lambda_n: 0.9167018576323746 Loss: 0.00020191496015706256\n",
      "Iteration: 6233 lambda_n: 1.0210504573160308 Loss: 0.00020191496015716737\n",
      "Iteration: 6234 lambda_n: 1.0203882529732304 Loss: 0.00020191496015728675\n",
      "Iteration: 6235 lambda_n: 0.9782502114518131 Loss: 0.00020191496015739985\n",
      "Iteration: 6236 lambda_n: 0.9134763594569906 Loss: 0.00020191496015751722\n",
      "Iteration: 6237 lambda_n: 1.0200693382171278 Loss: 0.0002019149601576219\n",
      "Iteration: 6238 lambda_n: 0.9543756317544012 Loss: 0.00020191496015773845\n",
      "Iteration: 6239 lambda_n: 0.8883472412411422 Loss: 0.00020191496015784186\n",
      "Iteration: 6240 lambda_n: 1.0324721113139887 Loss: 0.0002019149601579402\n",
      "Iteration: 6241 lambda_n: 0.9394720613560665 Loss: 0.00020191496015804745\n",
      "Iteration: 6242 lambda_n: 0.9810363443322195 Loss: 0.00020191496015815226\n",
      "Iteration: 6243 lambda_n: 0.9088627322241785 Loss: 0.00020191496015826128\n",
      "Iteration: 6244 lambda_n: 1.0230642324914856 Loss: 0.00020191496015836184\n",
      "Iteration: 6245 lambda_n: 0.9943077287303896 Loss: 0.00020191496015847338\n",
      "Iteration: 6246 lambda_n: 0.9456603449588912 Loss: 0.00020191496015857868\n",
      "Iteration: 6247 lambda_n: 0.9716629101958169 Loss: 0.0002019149601586756\n",
      "Iteration: 6248 lambda_n: 1.0024780738875063 Loss: 0.00020191496015878565\n",
      "Iteration: 6249 lambda_n: 0.8970408906765304 Loss: 0.00020191496015888874\n",
      "Iteration: 6250 lambda_n: 0.9819353269477069 Loss: 0.00020191496015899035\n",
      "Iteration: 6251 lambda_n: 0.9758377222432586 Loss: 0.0002019149601590968\n",
      "Iteration: 6252 lambda_n: 1.014533822378792 Loss: 0.00020191496015919678\n",
      "Iteration: 6253 lambda_n: 0.9462486192674427 Loss: 0.00020191496015930065\n",
      "Iteration: 6254 lambda_n: 0.9126528173125604 Loss: 0.00020191496015939308\n",
      "Iteration: 6255 lambda_n: 1.0079059656853528 Loss: 0.00020191496015949166\n",
      "Iteration: 6256 lambda_n: 1.014961506187546 Loss: 0.00020191496015960051\n",
      "Iteration: 6257 lambda_n: 1.0310145859127748 Loss: 0.0002019149601596997\n",
      "Iteration: 6258 lambda_n: 0.9344109976648789 Loss: 0.000201914960159802\n",
      "Iteration: 6259 lambda_n: 0.9060595646140037 Loss: 0.00020191496015989057\n",
      "Iteration: 6260 lambda_n: 0.8946059413640053 Loss: 0.00020191496015998568\n",
      "Iteration: 6261 lambda_n: 0.9785230766106147 Loss: 0.00020191496016007588\n",
      "Iteration: 6262 lambda_n: 0.9071197575144216 Loss: 0.00020191496016017463\n",
      "Iteration: 6263 lambda_n: 0.8927480867907507 Loss: 0.00020191496016026193\n",
      "Iteration: 6264 lambda_n: 0.9339506277138048 Loss: 0.00020191496016034734\n",
      "Iteration: 6265 lambda_n: 0.9597118119318541 Loss: 0.00020191496016044004\n",
      "Iteration: 6266 lambda_n: 0.960179342895974 Loss: 0.00020191496016053298\n",
      "Iteration: 6267 lambda_n: 1.0193662253175813 Loss: 0.00020191496016062858\n",
      "Iteration: 6268 lambda_n: 0.9024591654248559 Loss: 0.00020191496016071754\n",
      "Iteration: 6269 lambda_n: 0.9135159261373821 Loss: 0.00020191496016080197\n",
      "Iteration: 6270 lambda_n: 0.9116518256781106 Loss: 0.00020191496016088752\n",
      "Iteration: 6271 lambda_n: 0.9635481925301204 Loss: 0.00020191496016097246\n",
      "Iteration: 6272 lambda_n: 0.9695707881396313 Loss: 0.00020191496016106224\n",
      "Iteration: 6273 lambda_n: 0.9229461408646192 Loss: 0.00020191496016115553\n",
      "Iteration: 6274 lambda_n: 0.8882364538853526 Loss: 0.00020191496016124023\n",
      "Iteration: 6275 lambda_n: 0.8979836347543536 Loss: 0.0002019149601613243\n",
      "Iteration: 6276 lambda_n: 0.9438790410536003 Loss: 0.00020191496016140202\n",
      "Iteration: 6277 lambda_n: 0.9164077734439842 Loss: 0.00020191496016148358\n",
      "Iteration: 6278 lambda_n: 0.9760598711170474 Loss: 0.0002019149601615683\n",
      "Iteration: 6279 lambda_n: 0.9176909490547787 Loss: 0.00020191496016165554\n",
      "Iteration: 6280 lambda_n: 0.9382241730466803 Loss: 0.00020191496016173103\n",
      "Iteration: 6281 lambda_n: 1.0301154087425262 Loss: 0.00020191496016181557\n",
      "Iteration: 6282 lambda_n: 0.8929930466165481 Loss: 0.0002019149601619083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6283 lambda_n: 0.9710922921283279 Loss: 0.00020191496016198026\n",
      "Iteration: 6284 lambda_n: 0.9448051598145703 Loss: 0.00020191496016207027\n",
      "Iteration: 6285 lambda_n: 1.0192612180537648 Loss: 0.0002019149601621526\n",
      "Iteration: 6286 lambda_n: 0.9975574723812058 Loss: 0.00020191496016223916\n",
      "Iteration: 6287 lambda_n: 1.0234087664103457 Loss: 0.0002019149601623236\n",
      "Iteration: 6288 lambda_n: 1.0156596851736628 Loss: 0.00020191496016240963\n",
      "Iteration: 6289 lambda_n: 1.024829599665059 Loss: 0.0002019149601624905\n",
      "Iteration: 6290 lambda_n: 0.9020912520330131 Loss: 0.0002019149601625743\n",
      "Iteration: 6291 lambda_n: 0.9992992128268193 Loss: 0.00020191496016264918\n",
      "Iteration: 6292 lambda_n: 0.9303807104012599 Loss: 0.00020191496016273234\n",
      "Iteration: 6293 lambda_n: 1.0257070006661815 Loss: 0.00020191496016280417\n",
      "Iteration: 6294 lambda_n: 1.009296624767419 Loss: 0.00020191496016288562\n",
      "Iteration: 6295 lambda_n: 0.9807778988798743 Loss: 0.00020191496016296477\n",
      "Iteration: 6296 lambda_n: 0.9277899816622094 Loss: 0.00020191496016304234\n",
      "Iteration: 6297 lambda_n: 0.9094486592624647 Loss: 0.00020191496016311037\n",
      "Iteration: 6298 lambda_n: 0.8951272758059728 Loss: 0.00020191496016318743\n",
      "Iteration: 6299 lambda_n: 0.9342668459963138 Loss: 0.00020191496016325652\n",
      "Iteration: 6300 lambda_n: 0.9917029880761967 Loss: 0.00020191496016332806\n",
      "Iteration: 6301 lambda_n: 1.019000672887782 Loss: 0.0002019149601634088\n",
      "Iteration: 6302 lambda_n: 0.9645906908381815 Loss: 0.00020191496016349088\n",
      "Iteration: 6303 lambda_n: 0.9166705593246327 Loss: 0.0002019149601635665\n",
      "Iteration: 6304 lambda_n: 1.0043491012545949 Loss: 0.0002019149601636387\n",
      "Iteration: 6305 lambda_n: 0.9761684423076631 Loss: 0.00020191496016371\n",
      "Iteration: 6306 lambda_n: 0.8845667650340583 Loss: 0.00020191496016378703\n",
      "Iteration: 6307 lambda_n: 0.9851004007715356 Loss: 0.00020191496016385303\n",
      "Iteration: 6308 lambda_n: 0.9333884456338557 Loss: 0.0002019149601639284\n",
      "Iteration: 6309 lambda_n: 0.9337076799661808 Loss: 0.000201914960163994\n",
      "Iteration: 6310 lambda_n: 1.0046719784186788 Loss: 0.00020191496016406344\n",
      "Iteration: 6311 lambda_n: 0.9211457255807614 Loss: 0.0002019149601641312\n",
      "Iteration: 6312 lambda_n: 0.904181603856528 Loss: 0.00020191496016419647\n",
      "Iteration: 6313 lambda_n: 0.9637614859006605 Loss: 0.0002019149601642568\n",
      "Iteration: 6314 lambda_n: 0.9738801366807309 Loss: 0.00020191496016432904\n",
      "Iteration: 6315 lambda_n: 0.988702742691909 Loss: 0.00020191496016440207\n",
      "Iteration: 6316 lambda_n: 0.9332670888655922 Loss: 0.000201914960164478\n",
      "Iteration: 6317 lambda_n: 0.9258793828329699 Loss: 0.00020191496016454477\n",
      "Iteration: 6318 lambda_n: 0.9811003592681541 Loss: 0.00020191496016460866\n",
      "Iteration: 6319 lambda_n: 0.9331841993504593 Loss: 0.00020191496016468347\n",
      "Iteration: 6320 lambda_n: 0.9803518876444458 Loss: 0.00020191496016474966\n",
      "Iteration: 6321 lambda_n: 0.9980303743723091 Loss: 0.00020191496016482016\n",
      "Iteration: 6322 lambda_n: 1.0347671072219524 Loss: 0.0002019149601648857\n",
      "Iteration: 6323 lambda_n: 0.9471767315434771 Loss: 0.00020191496016495582\n",
      "Iteration: 6324 lambda_n: 0.9648997668561062 Loss: 0.00020191496016502461\n",
      "Iteration: 6325 lambda_n: 0.9164398706848697 Loss: 0.0002019149601650917\n",
      "Iteration: 6326 lambda_n: 0.9566422214540923 Loss: 0.00020191496016515472\n",
      "Iteration: 6327 lambda_n: 1.0367303181041905 Loss: 0.00020191496016522148\n",
      "Iteration: 6328 lambda_n: 1.0292381966613546 Loss: 0.00020191496016528905\n",
      "Iteration: 6329 lambda_n: 0.8933234097445558 Loss: 0.0002019149601653504\n",
      "Iteration: 6330 lambda_n: 0.9605872594598672 Loss: 0.00020191496016540783\n",
      "Iteration: 6331 lambda_n: 1.0314658143047108 Loss: 0.00020191496016547326\n",
      "Iteration: 6332 lambda_n: 1.0329626566206034 Loss: 0.00020191496016553045\n",
      "Iteration: 6333 lambda_n: 1.0074731244228878 Loss: 0.00020191496016559702\n",
      "Iteration: 6334 lambda_n: 0.9401081384900305 Loss: 0.00020191496016565806\n",
      "Iteration: 6335 lambda_n: 0.9313282726042456 Loss: 0.00020191496016571766\n",
      "Iteration: 6336 lambda_n: 1.0335856510657728 Loss: 0.0002019149601657752\n",
      "Iteration: 6337 lambda_n: 0.9654867918238709 Loss: 0.00020191496016583413\n",
      "Iteration: 6338 lambda_n: 0.9742369185944163 Loss: 0.00020191496016589282\n",
      "Iteration: 6339 lambda_n: 0.9945744013119269 Loss: 0.00020191496016595218\n",
      "Iteration: 6340 lambda_n: 1.0013617211076657 Loss: 0.00020191496016601113\n",
      "Iteration: 6341 lambda_n: 0.9485348672159674 Loss: 0.00020191496016606683\n",
      "Iteration: 6342 lambda_n: 0.959610815025585 Loss: 0.00020191496016612795\n",
      "Iteration: 6343 lambda_n: 0.9068293347598841 Loss: 0.00020191496016618563\n",
      "Iteration: 6344 lambda_n: 0.9883582592658224 Loss: 0.00020191496016623271\n",
      "Iteration: 6345 lambda_n: 0.9391799521290838 Loss: 0.00020191496016628793\n",
      "Iteration: 6346 lambda_n: 0.9774533205959545 Loss: 0.00020191496016634425\n",
      "Iteration: 6347 lambda_n: 1.0062055771409602 Loss: 0.00020191496016639838\n",
      "Iteration: 6348 lambda_n: 1.025310996270372 Loss: 0.00020191496016645208\n",
      "Iteration: 6349 lambda_n: 1.0196139773012287 Loss: 0.00020191496016650653\n",
      "Iteration: 6350 lambda_n: 0.9288517047931882 Loss: 0.00020191496016655624\n",
      "Iteration: 6351 lambda_n: 1.0232806955708176 Loss: 0.00020191496016660777\n",
      "Iteration: 6352 lambda_n: 0.9955716901687164 Loss: 0.00020191496016666355\n",
      "Iteration: 6353 lambda_n: 0.9771477578482343 Loss: 0.00020191496016671852\n",
      "Iteration: 6354 lambda_n: 0.9198341848805527 Loss: 0.00020191496016676267\n",
      "Iteration: 6355 lambda_n: 1.0136239272552168 Loss: 0.0002019149601668102\n",
      "Iteration: 6356 lambda_n: 1.0060199469599116 Loss: 0.0002019149601668623\n",
      "Iteration: 6357 lambda_n: 0.9848129138283616 Loss: 0.00020191496016691161\n",
      "Iteration: 6358 lambda_n: 1.008788742205528 Loss: 0.00020191496016696805\n",
      "Iteration: 6359 lambda_n: 0.9096077385459853 Loss: 0.00020191496016702353\n",
      "Iteration: 6360 lambda_n: 1.0290034014712812 Loss: 0.00020191496016707275\n",
      "Iteration: 6361 lambda_n: 0.9528366872185087 Loss: 0.00020191496016711886\n",
      "Iteration: 6362 lambda_n: 1.0054718799880633 Loss: 0.00020191496016716757\n",
      "Iteration: 6363 lambda_n: 0.9336029969907276 Loss: 0.00020191496016721877\n",
      "Iteration: 6364 lambda_n: 0.9554816133849022 Loss: 0.00020191496016726742\n",
      "Iteration: 6365 lambda_n: 0.937559996752541 Loss: 0.0002019149601673202\n",
      "Iteration: 6366 lambda_n: 1.0366696654621232 Loss: 0.00020191496016736592\n",
      "Iteration: 6367 lambda_n: 0.8957383408284566 Loss: 0.00020191496016741802\n",
      "Iteration: 6368 lambda_n: 0.9216773828927108 Loss: 0.00020191496016745572\n",
      "Iteration: 6369 lambda_n: 0.9878916925034161 Loss: 0.00020191496016750337\n",
      "Iteration: 6370 lambda_n: 1.0126807189358917 Loss: 0.0002019149601675488\n",
      "Iteration: 6371 lambda_n: 0.9834121751664148 Loss: 0.00020191496016759255\n",
      "Iteration: 6372 lambda_n: 1.0379299195304692 Loss: 0.0002019149601676416\n",
      "Iteration: 6373 lambda_n: 1.0262520701861524 Loss: 0.00020191496016768804\n",
      "Iteration: 6374 lambda_n: 0.9272199564647424 Loss: 0.0002019149601677326\n",
      "Iteration: 6375 lambda_n: 1.0349405666391223 Loss: 0.00020191496016777686\n",
      "Iteration: 6376 lambda_n: 0.9946388093794846 Loss: 0.0002019149601678221\n",
      "Iteration: 6377 lambda_n: 0.9786676647986636 Loss: 0.00020191496016786316\n",
      "Iteration: 6378 lambda_n: 0.936395750922748 Loss: 0.00020191496016791166\n",
      "Iteration: 6379 lambda_n: 0.8939477348367216 Loss: 0.00020191496016795825\n",
      "Iteration: 6380 lambda_n: 0.9285508834157689 Loss: 0.0002019149601679969\n",
      "Iteration: 6381 lambda_n: 0.8913559186345785 Loss: 0.00020191496016803826\n",
      "Iteration: 6382 lambda_n: 0.9910939964950624 Loss: 0.0002019149601680753\n",
      "Iteration: 6383 lambda_n: 0.9861258046946962 Loss: 0.00020191496016811814\n",
      "Iteration: 6384 lambda_n: 0.8963673646059946 Loss: 0.00020191496016816487\n",
      "Iteration: 6385 lambda_n: 0.9248743235660583 Loss: 0.00020191496016820168\n",
      "Iteration: 6386 lambda_n: 0.9642701315143412 Loss: 0.0002019149601682481\n",
      "Iteration: 6387 lambda_n: 0.9818823566289518 Loss: 0.00020191496016829392\n",
      "Iteration: 6388 lambda_n: 0.9405927991717797 Loss: 0.00020191496016834095\n",
      "Iteration: 6389 lambda_n: 0.9424780115040253 Loss: 0.00020191496016838515\n",
      "Iteration: 6390 lambda_n: 1.0165953932741614 Loss: 0.0002019149601684245\n",
      "Iteration: 6391 lambda_n: 0.9698163746629519 Loss: 0.00020191496016847146\n",
      "Iteration: 6392 lambda_n: 0.9700810304218738 Loss: 0.00020191496016851184\n",
      "Iteration: 6393 lambda_n: 0.9035257944366633 Loss: 0.000201914960168551\n",
      "Iteration: 6394 lambda_n: 0.9352130450112685 Loss: 0.00020191496016858538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6395 lambda_n: 0.952673975719114 Loss: 0.00020191496016862414\n",
      "Iteration: 6396 lambda_n: 0.9061552972760718 Loss: 0.00020191496016866572\n",
      "Iteration: 6397 lambda_n: 1.0034725621797902 Loss: 0.00020191496016870396\n",
      "Iteration: 6398 lambda_n: 0.9383649519006403 Loss: 0.0002019149601687462\n",
      "Iteration: 6399 lambda_n: 0.9733274678981337 Loss: 0.000201914960168784\n",
      "Iteration: 6400 lambda_n: 0.9943844892257464 Loss: 0.00020191496016881997\n",
      "Iteration: 6401 lambda_n: 0.9768352745968375 Loss: 0.00020191496016885846\n",
      "Iteration: 6402 lambda_n: 1.01681145237765 Loss: 0.00020191496016889402\n",
      "Iteration: 6403 lambda_n: 0.9808807028342971 Loss: 0.00020191496016893276\n",
      "Iteration: 6404 lambda_n: 0.9250894782449771 Loss: 0.00020191496016897138\n",
      "Iteration: 6405 lambda_n: 1.0132798032438015 Loss: 0.00020191496016901098\n",
      "Iteration: 6406 lambda_n: 0.9377790303856186 Loss: 0.00020191496016905066\n",
      "Iteration: 6407 lambda_n: 0.9288138255411335 Loss: 0.0002019149601690926\n",
      "Iteration: 6408 lambda_n: 0.896966845096628 Loss: 0.00020191496016912954\n",
      "Iteration: 6409 lambda_n: 0.8850388258455569 Loss: 0.0002019149601691599\n",
      "Iteration: 6410 lambda_n: 0.996392633131547 Loss: 0.00020191496016918806\n",
      "Iteration: 6411 lambda_n: 0.8936007568748284 Loss: 0.00020191496016922048\n",
      "Iteration: 6412 lambda_n: 0.9939307955443004 Loss: 0.00020191496016925243\n",
      "Iteration: 6413 lambda_n: 0.9794350587537528 Loss: 0.00020191496016928873\n",
      "Iteration: 6414 lambda_n: 0.92741766299986 Loss: 0.00020191496016932537\n",
      "Iteration: 6415 lambda_n: 0.9986304630002905 Loss: 0.00020191496016936365\n",
      "Iteration: 6416 lambda_n: 0.9683972247726305 Loss: 0.00020191496016940222\n",
      "Iteration: 6417 lambda_n: 0.8998922408985446 Loss: 0.00020191496016943341\n",
      "Iteration: 6418 lambda_n: 0.947870225937765 Loss: 0.00020191496016945808\n",
      "Iteration: 6419 lambda_n: 0.9899259894070748 Loss: 0.0002019149601694982\n",
      "Iteration: 6420 lambda_n: 1.0138592144402216 Loss: 0.0002019149601695335\n",
      "Iteration: 6421 lambda_n: 0.9048968393653843 Loss: 0.00020191496016956355\n",
      "Iteration: 6422 lambda_n: 0.9334477852362018 Loss: 0.00020191496016959718\n",
      "Iteration: 6423 lambda_n: 1.0142308421637347 Loss: 0.00020191496016963724\n",
      "Iteration: 6424 lambda_n: 0.9146605039318768 Loss: 0.00020191496016967386\n",
      "Iteration: 6425 lambda_n: 0.908827013345788 Loss: 0.00020191496016970777\n",
      "Iteration: 6426 lambda_n: 0.9454375921304792 Loss: 0.00020191496016974236\n",
      "Iteration: 6427 lambda_n: 0.9076029535990159 Loss: 0.00020191496016977553\n",
      "Iteration: 6428 lambda_n: 0.8941992307679097 Loss: 0.00020191496016980646\n",
      "Iteration: 6429 lambda_n: 0.9589003357567276 Loss: 0.00020191496016983817\n",
      "Iteration: 6430 lambda_n: 0.9738994044798648 Loss: 0.0002019149601698703\n",
      "Iteration: 6431 lambda_n: 0.9586589449163627 Loss: 0.0002019149601699058\n",
      "Iteration: 6432 lambda_n: 1.0332014719003173 Loss: 0.00020191496016993594\n",
      "Iteration: 6433 lambda_n: 1.0331783767060756 Loss: 0.00020191496016996947\n",
      "Iteration: 6434 lambda_n: 0.9529305010886459 Loss: 0.00020191496016999826\n",
      "Iteration: 6435 lambda_n: 0.9722304088241204 Loss: 0.00020191496017002878\n",
      "Iteration: 6436 lambda_n: 0.9043253516846455 Loss: 0.0002019149601700563\n",
      "Iteration: 6437 lambda_n: 0.9248542154916721 Loss: 0.00020191496017009188\n",
      "Iteration: 6438 lambda_n: 0.9907973488976433 Loss: 0.0002019149601701243\n",
      "Iteration: 6439 lambda_n: 0.886820685269214 Loss: 0.00020191496017015254\n",
      "Iteration: 6440 lambda_n: 0.9595004016352459 Loss: 0.00020191496017017804\n",
      "Iteration: 6441 lambda_n: 1.0007334610860699 Loss: 0.00020191496017021133\n",
      "Iteration: 6442 lambda_n: 0.9576876749018746 Loss: 0.0002019149601702403\n",
      "Iteration: 6443 lambda_n: 0.980076708489755 Loss: 0.00020191496017027297\n",
      "Iteration: 6444 lambda_n: 0.9496285121829608 Loss: 0.0002019149601703041\n",
      "Iteration: 6445 lambda_n: 0.9922435204035481 Loss: 0.00020191496017033428\n",
      "Iteration: 6446 lambda_n: 1.0191573499890205 Loss: 0.00020191496017035984\n",
      "Iteration: 6447 lambda_n: 0.9324319571494823 Loss: 0.0002019149601703873\n",
      "Iteration: 6448 lambda_n: 0.9498697346333935 Loss: 0.00020191496017041502\n",
      "Iteration: 6449 lambda_n: 0.9713642239622549 Loss: 0.00020191496017044495\n",
      "Iteration: 6450 lambda_n: 0.9819149073657663 Loss: 0.00020191496017047723\n",
      "Iteration: 6451 lambda_n: 0.9548155589042907 Loss: 0.00020191496017050675\n",
      "Iteration: 6452 lambda_n: 1.0142539968883764 Loss: 0.0002019149601705361\n",
      "Iteration: 6453 lambda_n: 0.9637308851665358 Loss: 0.00020191496017056657\n",
      "Iteration: 6454 lambda_n: 0.9004063631134614 Loss: 0.00020191496017059362\n",
      "Iteration: 6455 lambda_n: 0.8886857783501682 Loss: 0.00020191496017062574\n",
      "Iteration: 6456 lambda_n: 1.0221835046812497 Loss: 0.00020191496017065794\n",
      "Iteration: 6457 lambda_n: 1.0349156270535815 Loss: 0.00020191496017068269\n",
      "Iteration: 6458 lambda_n: 0.9100769277631624 Loss: 0.0002019149601707092\n",
      "Iteration: 6459 lambda_n: 0.9085834241192794 Loss: 0.00020191496017074094\n",
      "Iteration: 6460 lambda_n: 0.8895054609960306 Loss: 0.00020191496017076715\n",
      "Iteration: 6461 lambda_n: 0.9228666109623559 Loss: 0.00020191496017079786\n",
      "Iteration: 6462 lambda_n: 0.9193449733797019 Loss: 0.000201914960170826\n",
      "Iteration: 6463 lambda_n: 0.9197797135541088 Loss: 0.00020191496017085288\n",
      "Iteration: 6464 lambda_n: 0.8962526914958736 Loss: 0.00020191496017087532\n",
      "Iteration: 6465 lambda_n: 0.965788634043052 Loss: 0.0002019149601709058\n",
      "Iteration: 6466 lambda_n: 0.9966079175710355 Loss: 0.00020191496017092758\n",
      "Iteration: 6467 lambda_n: 1.0060134735725743 Loss: 0.00020191496017095108\n",
      "Iteration: 6468 lambda_n: 0.9312117876460566 Loss: 0.00020191496017097645\n",
      "Iteration: 6469 lambda_n: 0.9349910314079951 Loss: 0.00020191496017100453\n",
      "Iteration: 6470 lambda_n: 0.9204968410806884 Loss: 0.00020191496017102941\n",
      "Iteration: 6471 lambda_n: 0.9844215028706264 Loss: 0.00020191496017105378\n",
      "Iteration: 6472 lambda_n: 1.0163043069030637 Loss: 0.00020191496017107964\n",
      "Iteration: 6473 lambda_n: 1.0148351919377732 Loss: 0.00020191496017110206\n",
      "Iteration: 6474 lambda_n: 0.9137397828557402 Loss: 0.00020191496017112496\n",
      "Iteration: 6475 lambda_n: 0.9616070221971812 Loss: 0.00020191496017114995\n",
      "Iteration: 6476 lambda_n: 0.9986279239566929 Loss: 0.00020191496017117234\n",
      "Iteration: 6477 lambda_n: 0.930675829588016 Loss: 0.00020191496017119242\n",
      "Iteration: 6478 lambda_n: 1.0095744752941218 Loss: 0.00020191496017121517\n",
      "Iteration: 6479 lambda_n: 0.9642562848631955 Loss: 0.00020191496017123712\n",
      "Iteration: 6480 lambda_n: 0.8876997455043256 Loss: 0.0002019149601712573\n",
      "Iteration: 6481 lambda_n: 0.8907906513210383 Loss: 0.00020191496017128008\n",
      "Iteration: 6482 lambda_n: 1.0222258062583907 Loss: 0.00020191496017130103\n",
      "Iteration: 6483 lambda_n: 0.9824927067399296 Loss: 0.00020191496017132212\n",
      "Iteration: 6484 lambda_n: 0.974478512467985 Loss: 0.00020191496017134508\n",
      "Iteration: 6485 lambda_n: 1.0119118036312351 Loss: 0.00020191496017136573\n",
      "Iteration: 6486 lambda_n: 0.9518034626075496 Loss: 0.00020191496017138693\n",
      "Iteration: 6487 lambda_n: 0.9121175438649229 Loss: 0.00020191496017140983\n",
      "Iteration: 6488 lambda_n: 1.0287106753465085 Loss: 0.00020191496017143024\n",
      "Iteration: 6489 lambda_n: 0.9020999160618284 Loss: 0.0002019149601714546\n",
      "Iteration: 6490 lambda_n: 0.9138511498675502 Loss: 0.0002019149601714788\n",
      "Iteration: 6491 lambda_n: 0.9127503351562272 Loss: 0.00020191496017150058\n",
      "Iteration: 6492 lambda_n: 0.9976043352829332 Loss: 0.00020191496017152373\n",
      "Iteration: 6493 lambda_n: 1.024646232561577 Loss: 0.00020191496017154224\n",
      "Iteration: 6494 lambda_n: 0.8956328248965025 Loss: 0.00020191496017156103\n",
      "Iteration: 6495 lambda_n: 0.9121924848354199 Loss: 0.00020191496017158013\n",
      "Iteration: 6496 lambda_n: 0.9589835518016955 Loss: 0.00020191496017160404\n",
      "Iteration: 6497 lambda_n: 0.9760401375374257 Loss: 0.00020191496017162822\n",
      "Iteration: 6498 lambda_n: 0.9663133473261748 Loss: 0.00020191496017165264\n",
      "Iteration: 6499 lambda_n: 0.9296895829828988 Loss: 0.00020191496017167674\n",
      "Iteration: 6500 lambda_n: 1.0077526921092836 Loss: 0.0002019149601717008\n",
      "Iteration: 6501 lambda_n: 0.922874847215424 Loss: 0.00020191496017172032\n",
      "Iteration: 6502 lambda_n: 1.0207901555566037 Loss: 0.00020191496017174325\n",
      "Iteration: 6503 lambda_n: 0.9609568218472645 Loss: 0.00020191496017176472\n",
      "Iteration: 6504 lambda_n: 0.9952340269507516 Loss: 0.0002019149601717858\n",
      "Iteration: 6505 lambda_n: 0.999658814651996 Loss: 0.00020191496017181156\n",
      "Iteration: 6506 lambda_n: 0.9808130691841896 Loss: 0.0002019149601718325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6507 lambda_n: 0.9735780323112768 Loss: 0.00020191496017185384\n",
      "Iteration: 6508 lambda_n: 0.946220850450425 Loss: 0.00020191496017187566\n",
      "Iteration: 6509 lambda_n: 0.958911533007223 Loss: 0.00020191496017189594\n",
      "Iteration: 6510 lambda_n: 0.8964962123311794 Loss: 0.0002019149601719191\n",
      "Iteration: 6511 lambda_n: 1.009215127326683 Loss: 0.00020191496017194229\n",
      "Iteration: 6512 lambda_n: 0.9243489783103931 Loss: 0.00020191496017196088\n",
      "Iteration: 6513 lambda_n: 0.959953416316233 Loss: 0.00020191496017198386\n",
      "Iteration: 6514 lambda_n: 0.961049737332417 Loss: 0.00020191496017200433\n",
      "Iteration: 6515 lambda_n: 0.960200367116766 Loss: 0.00020191496017202466\n",
      "Iteration: 6516 lambda_n: 0.9744880159719337 Loss: 0.00020191496017204523\n",
      "Iteration: 6517 lambda_n: 0.9364190105351498 Loss: 0.00020191496017206073\n",
      "Iteration: 6518 lambda_n: 0.9565761569608227 Loss: 0.00020191496017207995\n",
      "Iteration: 6519 lambda_n: 0.9233908530855661 Loss: 0.00020191496017210245\n",
      "Iteration: 6520 lambda_n: 0.8898791496109438 Loss: 0.00020191496017211844\n",
      "Iteration: 6521 lambda_n: 1.007552640274542 Loss: 0.00020191496017213294\n",
      "Iteration: 6522 lambda_n: 0.9031830638135844 Loss: 0.00020191496017214853\n",
      "Iteration: 6523 lambda_n: 0.8909837657843367 Loss: 0.00020191496017216381\n",
      "Iteration: 6524 lambda_n: 1.0214272114640277 Loss: 0.00020191496017217897\n",
      "Iteration: 6525 lambda_n: 0.9269816164702883 Loss: 0.0002019149601721956\n",
      "Iteration: 6526 lambda_n: 0.9235238619718783 Loss: 0.00020191496017220997\n",
      "Iteration: 6527 lambda_n: 0.9955425240878527 Loss: 0.00020191496017223253\n",
      "Iteration: 6528 lambda_n: 1.0342886949793988 Loss: 0.0002019149601722512\n",
      "Iteration: 6529 lambda_n: 0.9042444769933735 Loss: 0.0002019149601722674\n",
      "Iteration: 6530 lambda_n: 1.0125693752408758 Loss: 0.00020191496017228454\n",
      "Iteration: 6531 lambda_n: 0.9146848715361171 Loss: 0.00020191496017230053\n",
      "Iteration: 6532 lambda_n: 1.0342789446790468 Loss: 0.00020191496017231552\n",
      "Iteration: 6533 lambda_n: 0.9184383350539729 Loss: 0.00020191496017232962\n",
      "Iteration: 6534 lambda_n: 1.0314056231008155 Loss: 0.00020191496017234466\n",
      "Iteration: 6535 lambda_n: 0.9252871429198454 Loss: 0.0002019149601723591\n",
      "Iteration: 6536 lambda_n: 0.9081587790281113 Loss: 0.00020191496017237355\n",
      "Iteration: 6537 lambda_n: 0.9570667852718511 Loss: 0.00020191496017238703\n",
      "Iteration: 6538 lambda_n: 0.9970715331005302 Loss: 0.00020191496017240096\n",
      "Iteration: 6539 lambda_n: 0.9016290432313623 Loss: 0.00020191496017241676\n",
      "Iteration: 6540 lambda_n: 0.8845685061691464 Loss: 0.00020191496017243107\n",
      "Iteration: 6541 lambda_n: 0.9723950311099367 Loss: 0.0002019149601724455\n",
      "Iteration: 6542 lambda_n: 0.8937690255559437 Loss: 0.00020191496017245872\n",
      "Iteration: 6543 lambda_n: 1.0060837168861505 Loss: 0.0002019149601724727\n",
      "Iteration: 6544 lambda_n: 1.0284043949225772 Loss: 0.00020191496017248837\n",
      "Iteration: 6545 lambda_n: 0.9927319585977004 Loss: 0.0002019149601725029\n",
      "Iteration: 6546 lambda_n: 0.9661781146161095 Loss: 0.00020191496017251767\n",
      "Iteration: 6547 lambda_n: 0.9145899972359633 Loss: 0.00020191496017253093\n",
      "Iteration: 6548 lambda_n: 0.9947125803647971 Loss: 0.00020191496017254567\n",
      "Iteration: 6549 lambda_n: 0.8930208234698078 Loss: 0.00020191496017255936\n",
      "Iteration: 6550 lambda_n: 1.0335565653573011 Loss: 0.00020191496017257473\n",
      "Iteration: 6551 lambda_n: 1.0145660847221563 Loss: 0.0002019149601725882\n",
      "Iteration: 6552 lambda_n: 0.8867276127191186 Loss: 0.00020191496017260202\n",
      "Iteration: 6553 lambda_n: 0.9878336054730181 Loss: 0.00020191496017261972\n",
      "Iteration: 6554 lambda_n: 0.9489830060222606 Loss: 0.00020191496017263184\n",
      "Iteration: 6555 lambda_n: 1.0357444957910595 Loss: 0.00020191496017264325\n",
      "Iteration: 6556 lambda_n: 0.8855677211368675 Loss: 0.00020191496017265645\n",
      "Iteration: 6557 lambda_n: 0.9143627342953379 Loss: 0.00020191496017267466\n",
      "Iteration: 6558 lambda_n: 1.0091725702407943 Loss: 0.00020191496017268426\n",
      "Iteration: 6559 lambda_n: 0.9855974785517666 Loss: 0.0002019149601726968\n",
      "Iteration: 6560 lambda_n: 0.9912228473432807 Loss: 0.00020191496017271407\n",
      "Iteration: 6561 lambda_n: 0.9382444840068178 Loss: 0.00020191496017272462\n",
      "Iteration: 6562 lambda_n: 1.0149320561878135 Loss: 0.00020191496017273776\n",
      "Iteration: 6563 lambda_n: 0.9067290552604295 Loss: 0.0002019149601727474\n",
      "Iteration: 6564 lambda_n: 0.9359210158915454 Loss: 0.00020191496017276175\n",
      "Iteration: 6565 lambda_n: 0.9167963350473198 Loss: 0.00020191496017277143\n",
      "Iteration: 6566 lambda_n: 0.9434107757918803 Loss: 0.0002019149601727868\n",
      "Iteration: 6567 lambda_n: 0.9174109314265014 Loss: 0.00020191496017280358\n",
      "Iteration: 6568 lambda_n: 0.966455916518912 Loss: 0.00020191496017281203\n",
      "Iteration: 6569 lambda_n: 0.9245745919015452 Loss: 0.00020191496017282586\n",
      "Iteration: 6570 lambda_n: 0.9986199922439403 Loss: 0.0002019149601728424\n",
      "Iteration: 6571 lambda_n: 0.9418982177239968 Loss: 0.00020191496017285586\n",
      "Iteration: 6572 lambda_n: 1.0095702974220186 Loss: 0.0002019149601728725\n",
      "Iteration: 6573 lambda_n: 0.9499755588962913 Loss: 0.00020191496017288869\n",
      "Iteration: 6574 lambda_n: 0.9164214473351605 Loss: 0.0002019149601729019\n",
      "Iteration: 6575 lambda_n: 0.9445861426156127 Loss: 0.00020191496017291555\n",
      "Iteration: 6576 lambda_n: 0.9106119178413347 Loss: 0.00020191496017293382\n",
      "Iteration: 6577 lambda_n: 0.928887431330878 Loss: 0.0002019149601729451\n",
      "Iteration: 6578 lambda_n: 0.8840208327042308 Loss: 0.00020191496017295897\n",
      "Iteration: 6579 lambda_n: 0.9759849796705417 Loss: 0.00020191496017297572\n",
      "Iteration: 6580 lambda_n: 1.0178158845622514 Loss: 0.0002019149601729895\n",
      "Iteration: 6581 lambda_n: 0.9682405849738379 Loss: 0.00020191496017300226\n",
      "Iteration: 6582 lambda_n: 0.9476522948539747 Loss: 0.0002019149601730215\n",
      "Iteration: 6583 lambda_n: 1.016186354767419 Loss: 0.00020191496017303213\n",
      "Iteration: 6584 lambda_n: 1.0252649146278594 Loss: 0.000201914960173045\n",
      "Iteration: 6585 lambda_n: 0.9571763519820052 Loss: 0.00020191496017306332\n",
      "Iteration: 6586 lambda_n: 1.0031292399150247 Loss: 0.00020191496017307425\n",
      "Iteration: 6587 lambda_n: 0.9146355307846463 Loss: 0.00020191496017308972\n",
      "Iteration: 6588 lambda_n: 0.9463275056555674 Loss: 0.0002019149601731068\n",
      "Iteration: 6589 lambda_n: 0.947692350957669 Loss: 0.00020191496017311789\n",
      "Iteration: 6590 lambda_n: 1.0319599413522105 Loss: 0.00020191496017313247\n",
      "Iteration: 6591 lambda_n: 1.0029377449674826 Loss: 0.00020191496017314952\n",
      "Iteration: 6592 lambda_n: 0.9714677822237235 Loss: 0.0002019149601731624\n",
      "Iteration: 6593 lambda_n: 0.9001167835287345 Loss: 0.00020191496017317513\n",
      "Iteration: 6594 lambda_n: 0.9780389236761355 Loss: 0.00020191496017319237\n",
      "Iteration: 6595 lambda_n: 1.0027090921293527 Loss: 0.0002019149601732051\n",
      "Iteration: 6596 lambda_n: 0.9566262514392481 Loss: 0.0002019149601732144\n",
      "Iteration: 6597 lambda_n: 1.0259460082948646 Loss: 0.00020191496017323224\n",
      "Iteration: 6598 lambda_n: 0.9060393212719434 Loss: 0.00020191496017324064\n",
      "Iteration: 6599 lambda_n: 0.941714644464663 Loss: 0.00020191496017324948\n",
      "Iteration: 6600 lambda_n: 0.9256398084418229 Loss: 0.00020191496017325878\n",
      "Iteration: 6601 lambda_n: 0.8943501508599401 Loss: 0.0002019149601732737\n",
      "Iteration: 6602 lambda_n: 1.0324512211252475 Loss: 0.00020191496017328334\n",
      "Iteration: 6603 lambda_n: 0.8889596536369878 Loss: 0.00020191496017329672\n",
      "Iteration: 6604 lambda_n: 0.9939039565118665 Loss: 0.00020191496017330515\n",
      "Iteration: 6605 lambda_n: 0.9650658770277373 Loss: 0.00020191496017331375\n",
      "Iteration: 6606 lambda_n: 0.97253919901649 Loss: 0.000201914960173323\n",
      "Iteration: 6607 lambda_n: 0.9266408045003381 Loss: 0.00020191496017332982\n",
      "Iteration: 6608 lambda_n: 0.8837005365529367 Loss: 0.0002019149601733389\n",
      "Iteration: 6609 lambda_n: 1.0178467536487907 Loss: 0.00020191496017334503\n",
      "Iteration: 6610 lambda_n: 1.0078109843944136 Loss: 0.00020191496017335178\n",
      "Iteration: 6611 lambda_n: 1.0302308741921427 Loss: 0.00020191496017336164\n",
      "Iteration: 6612 lambda_n: 0.9314349732355245 Loss: 0.0002019149601733696\n",
      "Iteration: 6613 lambda_n: 0.8910012235647955 Loss: 0.00020191496017337815\n",
      "Iteration: 6614 lambda_n: 0.9922896409307344 Loss: 0.00020191496017338414\n",
      "Iteration: 6615 lambda_n: 0.9812767039733213 Loss: 0.00020191496017339062\n",
      "Iteration: 6616 lambda_n: 0.9136116556590869 Loss: 0.00020191496017339918\n",
      "Iteration: 6617 lambda_n: 1.008361978135012 Loss: 0.00020191496017340547\n",
      "Iteration: 6618 lambda_n: 1.034797368536991 Loss: 0.0002019149601734122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6619 lambda_n: 0.9683924514045351 Loss: 0.00020191496017342073\n",
      "Iteration: 6620 lambda_n: 0.9399447721753141 Loss: 0.00020191496017342713\n",
      "Iteration: 6621 lambda_n: 0.9795824699597403 Loss: 0.00020191496017343553\n",
      "Iteration: 6622 lambda_n: 0.9723841166630363 Loss: 0.00020191496017344252\n",
      "Iteration: 6623 lambda_n: 0.9438003540962152 Loss: 0.00020191496017344876\n",
      "Iteration: 6624 lambda_n: 0.8969472714984464 Loss: 0.0002019149601734554\n",
      "Iteration: 6625 lambda_n: 0.9756459493640418 Loss: 0.0002019149601734637\n",
      "Iteration: 6626 lambda_n: 0.933139308512138 Loss: 0.00020191496017347012\n",
      "Iteration: 6627 lambda_n: 0.9386227001962261 Loss: 0.00020191496017347673\n",
      "Iteration: 6628 lambda_n: 0.9415979763112082 Loss: 0.00020191496017348546\n",
      "Iteration: 6629 lambda_n: 0.9851062109835166 Loss: 0.00020191496017349153\n",
      "Iteration: 6630 lambda_n: 0.9956013981001458 Loss: 0.0002019149601735\n",
      "Iteration: 6631 lambda_n: 1.0045072814485434 Loss: 0.0002019149601735063\n",
      "Iteration: 6632 lambda_n: 0.946232667680409 Loss: 0.00020191496017351262\n",
      "Iteration: 6633 lambda_n: 1.0033366818577887 Loss: 0.00020191496017352113\n",
      "Iteration: 6634 lambda_n: 0.9180382671405287 Loss: 0.00020191496017352742\n",
      "Iteration: 6635 lambda_n: 0.8866985291866794 Loss: 0.0002019149601735351\n",
      "Iteration: 6636 lambda_n: 1.0329445318249506 Loss: 0.0002019149601735416\n",
      "Iteration: 6637 lambda_n: 0.940499873401292 Loss: 0.0002019149601735502\n",
      "Iteration: 6638 lambda_n: 1.0283177327177284 Loss: 0.00020191496017355647\n",
      "Iteration: 6639 lambda_n: 0.976181420345329 Loss: 0.00020191496017356298\n",
      "Iteration: 6640 lambda_n: 0.8913769303038227 Loss: 0.0002019149601735716\n",
      "Iteration: 6641 lambda_n: 0.893219256205838 Loss: 0.000201914960173578\n",
      "Iteration: 6642 lambda_n: 0.970615333365762 Loss: 0.00020191496017358455\n",
      "Iteration: 6643 lambda_n: 0.9347514056908196 Loss: 0.0002019149601735927\n",
      "Iteration: 6644 lambda_n: 0.9878981808284054 Loss: 0.00020191496017359933\n",
      "Iteration: 6645 lambda_n: 0.922871625211186 Loss: 0.00020191496017360762\n",
      "Iteration: 6646 lambda_n: 0.8830028698212504 Loss: 0.000201914960173614\n",
      "Iteration: 6647 lambda_n: 0.9275572839025419 Loss: 0.00020191496017362063\n",
      "Iteration: 6648 lambda_n: 0.9792044398829426 Loss: 0.0002019149601736291\n",
      "Iteration: 6649 lambda_n: 0.9887019604548057 Loss: 0.00020191496017363532\n",
      "Iteration: 6650 lambda_n: 0.9018229265192915 Loss: 0.0002019149601736422\n",
      "Iteration: 6651 lambda_n: 0.9569504889922906 Loss: 0.00020191496017365064\n",
      "Iteration: 6652 lambda_n: 1.0191676152634828 Loss: 0.00020191496017365682\n",
      "Iteration: 6653 lambda_n: 0.8875162799074462 Loss: 0.00020191496017366365\n",
      "Iteration: 6654 lambda_n: 0.9078765379716837 Loss: 0.00020191496017367213\n",
      "Iteration: 6655 lambda_n: 0.993234934332825 Loss: 0.0002019149601736786\n",
      "Iteration: 6656 lambda_n: 1.0344204005684665 Loss: 0.00020191496017368522\n",
      "Iteration: 6657 lambda_n: 0.8962500915249704 Loss: 0.0002019149601736938\n",
      "Iteration: 6658 lambda_n: 0.9934479527133624 Loss: 0.0002019149601737001\n",
      "Iteration: 6659 lambda_n: 0.9209033599675558 Loss: 0.00020191496017370685\n",
      "Iteration: 6660 lambda_n: 1.0261168658799575 Loss: 0.00020191496017371523\n",
      "Iteration: 6661 lambda_n: 1.0360321113621465 Loss: 0.00020191496017372246\n",
      "Iteration: 6662 lambda_n: 0.9259277901438484 Loss: 0.00020191496017372973\n",
      "Iteration: 6663 lambda_n: 0.9167234090823457 Loss: 0.00020191496017373686\n",
      "Iteration: 6664 lambda_n: 1.0197647459095351 Loss: 0.00020191496017374404\n",
      "Iteration: 6665 lambda_n: 1.00085500646025 Loss: 0.00020191496017375128\n",
      "Iteration: 6666 lambda_n: 0.9741907204167279 Loss: 0.0002019149601737574\n",
      "Iteration: 6667 lambda_n: 0.9643284706972434 Loss: 0.00020191496017376426\n",
      "Iteration: 6668 lambda_n: 0.9609552158352873 Loss: 0.0002019149601737728\n",
      "Iteration: 6669 lambda_n: 0.9015451588697508 Loss: 0.00020191496017377998\n",
      "Iteration: 6670 lambda_n: 0.9771979741472031 Loss: 0.00020191496017378716\n",
      "Iteration: 6671 lambda_n: 1.0354121561284861 Loss: 0.00020191496017379437\n",
      "Iteration: 6672 lambda_n: 0.9256062740330483 Loss: 0.00020191496017380072\n",
      "Iteration: 6673 lambda_n: 0.8846409582813974 Loss: 0.00020191496017381023\n",
      "Iteration: 6674 lambda_n: 0.9013943390732871 Loss: 0.00020191496017382115\n",
      "Iteration: 6675 lambda_n: 1.0194200443299164 Loss: 0.00020191496017382888\n",
      "Iteration: 6676 lambda_n: 0.910070472732603 Loss: 0.00020191496017383446\n",
      "Iteration: 6677 lambda_n: 0.9031363938322867 Loss: 0.00020191496017384436\n",
      "Iteration: 6678 lambda_n: 1.0336942452202185 Loss: 0.0002019149601738531\n",
      "Iteration: 6679 lambda_n: 0.9374387130360271 Loss: 0.0002019149601738595\n",
      "Iteration: 6680 lambda_n: 0.9615110164157885 Loss: 0.00020191496017386954\n",
      "Iteration: 6681 lambda_n: 1.0206707086615106 Loss: 0.00020191496017387675\n",
      "Iteration: 6682 lambda_n: 0.9676188224649726 Loss: 0.00020191496017388656\n",
      "Iteration: 6683 lambda_n: 0.884289259929917 Loss: 0.00020191496017389054\n",
      "Iteration: 6684 lambda_n: 1.0141840778782119 Loss: 0.00020191496017389545\n",
      "Iteration: 6685 lambda_n: 0.8971511483775926 Loss: 0.00020191496017390258\n",
      "Iteration: 6686 lambda_n: 1.028173616263682 Loss: 0.00020191496017391065\n",
      "Iteration: 6687 lambda_n: 0.8897248968875052 Loss: 0.0002019149601739194\n",
      "Iteration: 6688 lambda_n: 0.940220228569036 Loss: 0.0002019149601739304\n",
      "Iteration: 6689 lambda_n: 0.948155241812178 Loss: 0.0002019149601739364\n",
      "Iteration: 6690 lambda_n: 0.9054806204245722 Loss: 0.00020191496017394275\n",
      "Iteration: 6691 lambda_n: 0.918115341803352 Loss: 0.00020191496017395237\n",
      "Iteration: 6692 lambda_n: 0.9474230295955817 Loss: 0.00020191496017395822\n",
      "Iteration: 6693 lambda_n: 0.9826201760448311 Loss: 0.00020191496017396711\n",
      "Iteration: 6694 lambda_n: 1.0103394611467826 Loss: 0.00020191496017397291\n",
      "Iteration: 6695 lambda_n: 0.9665653216565605 Loss: 0.00020191496017398\n",
      "Iteration: 6696 lambda_n: 0.9677844298714189 Loss: 0.0002019149601739902\n",
      "Iteration: 6697 lambda_n: 0.971296025164086 Loss: 0.00020191496017399444\n",
      "Iteration: 6698 lambda_n: 0.9694070939702009 Loss: 0.00020191496017400338\n",
      "Iteration: 6699 lambda_n: 0.9727572190821374 Loss: 0.00020191496017400888\n",
      "Iteration: 6700 lambda_n: 0.9950172474667183 Loss: 0.00020191496017401918\n",
      "Iteration: 6701 lambda_n: 0.9173188476641555 Loss: 0.0002019149601740271\n",
      "Iteration: 6702 lambda_n: 0.9182995913402671 Loss: 0.00020191496017403932\n",
      "Iteration: 6703 lambda_n: 0.9701470048520214 Loss: 0.0002019149601740442\n",
      "Iteration: 6704 lambda_n: 0.8891269498001829 Loss: 0.00020191496017405274\n",
      "Iteration: 6705 lambda_n: 0.9986873960819133 Loss: 0.00020191496017405716\n",
      "Iteration: 6706 lambda_n: 0.9601919772280487 Loss: 0.00020191496017406602\n",
      "Iteration: 6707 lambda_n: 0.9951974405653933 Loss: 0.0002019149601740744\n",
      "Iteration: 6708 lambda_n: 0.9862920669621026 Loss: 0.00020191496017408388\n",
      "Iteration: 6709 lambda_n: 0.9354938383703336 Loss: 0.00020191496017409\n",
      "Iteration: 6710 lambda_n: 0.8989912503676071 Loss: 0.00020191496017409627\n",
      "Iteration: 6711 lambda_n: 0.8858780494555067 Loss: 0.00020191496017410196\n",
      "Iteration: 6712 lambda_n: 0.91459758432917 Loss: 0.00020191496017410904\n",
      "Iteration: 6713 lambda_n: 0.8899643602593718 Loss: 0.00020191496017411465\n",
      "Iteration: 6714 lambda_n: 0.9248568188085057 Loss: 0.0002019149601741233\n",
      "Iteration: 6715 lambda_n: 0.9080754301100059 Loss: 0.00020191496017412958\n",
      "Iteration: 6716 lambda_n: 0.9034967314610578 Loss: 0.00020191496017413644\n",
      "Iteration: 6717 lambda_n: 1.022666745191309 Loss: 0.00020191496017414414\n",
      "Iteration: 6718 lambda_n: 0.9636072711680894 Loss: 0.00020191496017415102\n",
      "Iteration: 6719 lambda_n: 1.0306657719960197 Loss: 0.00020191496017415614\n",
      "Iteration: 6720 lambda_n: 1.0291366115466085 Loss: 0.00020191496017416577\n",
      "Iteration: 6721 lambda_n: 0.9161427911193283 Loss: 0.00020191496017417314\n",
      "Iteration: 6722 lambda_n: 0.9661278379955245 Loss: 0.00020191496017417796\n",
      "Iteration: 6723 lambda_n: 0.9572591302003546 Loss: 0.00020191496017418648\n",
      "Iteration: 6724 lambda_n: 1.0376332216144597 Loss: 0.00020191496017419458\n",
      "Iteration: 6725 lambda_n: 0.885356714375528 Loss: 0.00020191496017419962\n",
      "Iteration: 6726 lambda_n: 0.9627164428812739 Loss: 0.00020191496017420748\n",
      "Iteration: 6727 lambda_n: 1.0037350100951068 Loss: 0.00020191496017421588\n",
      "Iteration: 6728 lambda_n: 1.0093040098592512 Loss: 0.00020191496017422093\n",
      "Iteration: 6729 lambda_n: 0.947427162719449 Loss: 0.0002019149601742309\n",
      "Iteration: 6730 lambda_n: 0.8986122032744652 Loss: 0.00020191496017423784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6731 lambda_n: 0.9537077529023814 Loss: 0.0002019149601742436\n",
      "Iteration: 6732 lambda_n: 0.9529140564732517 Loss: 0.0002019149601742528\n",
      "Iteration: 6733 lambda_n: 0.8919912645061981 Loss: 0.0002019149601742573\n",
      "Iteration: 6734 lambda_n: 1.037380791917322 Loss: 0.00020191496017426633\n",
      "Iteration: 6735 lambda_n: 0.9294864011542043 Loss: 0.00020191496017427454\n",
      "Iteration: 6736 lambda_n: 0.9032863424943058 Loss: 0.0002019149601742823\n",
      "Iteration: 6737 lambda_n: 1.025446002893044 Loss: 0.0002019149601742867\n",
      "Iteration: 6738 lambda_n: 0.8911717449507549 Loss: 0.00020191496017429647\n",
      "Iteration: 6739 lambda_n: 1.030791243237601 Loss: 0.00020191496017430083\n",
      "Iteration: 6740 lambda_n: 0.8961006072185776 Loss: 0.00020191496017430633\n",
      "Iteration: 6741 lambda_n: 1.0068376516006208 Loss: 0.00020191496017431612\n",
      "Iteration: 6742 lambda_n: 0.9267622102287031 Loss: 0.00020191496017432325\n",
      "Iteration: 6743 lambda_n: 0.9593049894442122 Loss: 0.00020191496017432853\n",
      "Iteration: 6744 lambda_n: 0.9043779568770411 Loss: 0.00020191496017433916\n",
      "Iteration: 6745 lambda_n: 1.0297169734476852 Loss: 0.00020191496017434607\n",
      "Iteration: 6746 lambda_n: 0.978320397978236 Loss: 0.0002019149601743521\n",
      "Iteration: 6747 lambda_n: 0.9111722768620241 Loss: 0.0002019149601743587\n",
      "Iteration: 6748 lambda_n: 0.8972386274786401 Loss: 0.0002019149601743682\n",
      "Iteration: 6749 lambda_n: 0.9143121558990139 Loss: 0.0002019149601743729\n",
      "Iteration: 6750 lambda_n: 0.9195530164119978 Loss: 0.0002019149601743807\n",
      "Iteration: 6751 lambda_n: 0.9859693195691847 Loss: 0.00020191496017438976\n",
      "Iteration: 6752 lambda_n: 0.998683624371182 Loss: 0.00020191496017439746\n",
      "Iteration: 6753 lambda_n: 0.9336976858319836 Loss: 0.00020191496017440345\n",
      "Iteration: 6754 lambda_n: 0.9008462160790409 Loss: 0.00020191496017440567\n",
      "Iteration: 6755 lambda_n: 0.9500295114020668 Loss: 0.0002019149601744151\n",
      "Iteration: 6756 lambda_n: 0.9811749566393216 Loss: 0.00020191496017441635\n",
      "Iteration: 6757 lambda_n: 1.0268276078755503 Loss: 0.00020191496017441814\n",
      "Iteration: 6758 lambda_n: 0.9782695939857274 Loss: 0.0002019149601744177\n",
      "Iteration: 6759 lambda_n: 1.012884604043468 Loss: 0.0002019149601744202\n",
      "Iteration: 6760 lambda_n: 1.019436311274364 Loss: 0.0002019149601744267\n",
      "Iteration: 6761 lambda_n: 1.0261880742349714 Loss: 0.00020191496017442893\n",
      "Iteration: 6762 lambda_n: 0.9274133333120748 Loss: 0.00020191496017443164\n",
      "Iteration: 6763 lambda_n: 0.9321335048360663 Loss: 0.00020191496017443085\n",
      "Iteration: 6764 lambda_n: 0.9790328418972222 Loss: 0.00020191496017443308\n",
      "Iteration: 6765 lambda_n: 0.9733725778447001 Loss: 0.00020191496017443465\n",
      "Iteration: 6766 lambda_n: 0.9866028941337703 Loss: 0.0002019149601744341\n",
      "Iteration: 6767 lambda_n: 0.8913544827222579 Loss: 0.00020191496017443603\n",
      "Iteration: 6768 lambda_n: 0.9957311622885278 Loss: 0.00020191496017443603\n",
      "Iteration: 6769 lambda_n: 0.8883568511267784 Loss: 0.00020191496017443603\n",
      "Iteration: 6770 lambda_n: 0.947323475954979 Loss: 0.00020191496017443598\n",
      "Iteration: 6771 lambda_n: 0.9527475155930355 Loss: 0.00020191496017443603\n",
      "Iteration: 6772 lambda_n: 0.9135991190541778 Loss: 0.00020191496017443609\n",
      "Iteration: 6773 lambda_n: 0.9987352164558676 Loss: 0.000201914960174436\n",
      "Iteration: 6774 lambda_n: 1.0065423108120848 Loss: 0.00020191496017443606\n",
      "Iteration: 6775 lambda_n: 0.8889201659620207 Loss: 0.00020191496017443598\n",
      "Iteration: 6776 lambda_n: 0.9589188907573805 Loss: 0.00020191496017443603\n",
      "Iteration: 6777 lambda_n: 0.8874871470084655 Loss: 0.00020191496017443609\n",
      "Iteration: 6778 lambda_n: 0.900510289599374 Loss: 0.000201914960174436\n",
      "Iteration: 6779 lambda_n: 1.0147725771546365 Loss: 0.00020191496017443606\n",
      "Iteration: 6780 lambda_n: 0.9914273052786787 Loss: 0.00020191496017443598\n",
      "Iteration: 6781 lambda_n: 0.9979095422765348 Loss: 0.00020191496017443603\n",
      "Iteration: 6782 lambda_n: 0.9215499106675926 Loss: 0.00020191496017443609\n",
      "Iteration: 6783 lambda_n: 1.009213856488843 Loss: 0.000201914960174436\n",
      "Iteration: 6784 lambda_n: 1.0285659615258136 Loss: 0.00020191496017443603\n",
      "Iteration: 6785 lambda_n: 0.9046784355008316 Loss: 0.00020191496017443598\n",
      "Iteration: 6786 lambda_n: 0.9593074269744862 Loss: 0.00020191496017443603\n",
      "Iteration: 6787 lambda_n: 0.9188008785553303 Loss: 0.00020191496017443609\n",
      "Iteration: 6788 lambda_n: 0.9110428514372577 Loss: 0.000201914960174436\n",
      "Iteration: 6789 lambda_n: 0.9970834805184947 Loss: 0.00020191496017443603\n",
      "Iteration: 6790 lambda_n: 0.9892931572044041 Loss: 0.00020191496017443598\n",
      "Iteration: 6791 lambda_n: 0.9309286606453834 Loss: 0.00020191496017443603\n",
      "Iteration: 6792 lambda_n: 0.9231849865566772 Loss: 0.00020191496017443609\n",
      "Iteration: 6793 lambda_n: 0.9436666490903062 Loss: 0.000201914960174436\n",
      "Iteration: 6794 lambda_n: 0.9369844491966349 Loss: 0.00020191496017443603\n",
      "Iteration: 6795 lambda_n: 0.981027221378809 Loss: 0.00020191496017443598\n",
      "Iteration: 6796 lambda_n: 0.9502495191288105 Loss: 0.00020191496017443603\n",
      "Iteration: 6797 lambda_n: 1.0006557026595375 Loss: 0.00020191496017443609\n",
      "Iteration: 6798 lambda_n: 0.9183630249525618 Loss: 0.000201914960174436\n",
      "Iteration: 6799 lambda_n: 1.0023342391951797 Loss: 0.00020191496017443603\n",
      "Iteration: 6800 lambda_n: 1.0114913738973381 Loss: 0.00020191496017443598\n",
      "Iteration: 6801 lambda_n: 0.9252015353207268 Loss: 0.00020191496017443603\n",
      "Iteration: 6802 lambda_n: 1.029880481915268 Loss: 0.00020191496017443609\n",
      "Iteration: 6803 lambda_n: 1.0104515289593197 Loss: 0.000201914960174436\n",
      "Iteration: 6804 lambda_n: 1.0343918081422725 Loss: 0.00020191496017443603\n",
      "Iteration: 6805 lambda_n: 1.0195036509184296 Loss: 0.00020191496017443598\n",
      "Iteration: 6806 lambda_n: 0.9076049834008132 Loss: 0.00020191496017443603\n",
      "Iteration: 6807 lambda_n: 1.038229763866062 Loss: 0.00020191496017443609\n",
      "Iteration: 6808 lambda_n: 0.9987366351311697 Loss: 0.000201914960174436\n",
      "Iteration: 6809 lambda_n: 0.884339476221854 Loss: 0.00020191496017443603\n",
      "Iteration: 6810 lambda_n: 0.9965924607402045 Loss: 0.00020191496017443598\n",
      "Iteration: 6811 lambda_n: 0.9558696554505622 Loss: 0.00020191496017443603\n",
      "Iteration: 6812 lambda_n: 1.0361308653680523 Loss: 0.00020191496017443609\n",
      "Iteration: 6813 lambda_n: 0.9582129548306377 Loss: 0.000201914960174436\n",
      "Iteration: 6814 lambda_n: 0.9781499736474277 Loss: 0.00020191496017443603\n",
      "Iteration: 6815 lambda_n: 0.9247461923750692 Loss: 0.00020191496017443598\n",
      "Iteration: 6816 lambda_n: 1.0351658451945074 Loss: 0.00020191496017443603\n",
      "Iteration: 6817 lambda_n: 0.9476251249040013 Loss: 0.00020191496017443609\n",
      "Iteration: 6818 lambda_n: 0.9670174380399863 Loss: 0.000201914960174436\n",
      "Iteration: 6819 lambda_n: 0.941735981572247 Loss: 0.00020191496017443603\n",
      "Iteration: 6820 lambda_n: 1.0026455791236994 Loss: 0.00020191496017443598\n",
      "Iteration: 6821 lambda_n: 1.0342505588399749 Loss: 0.00020191496017443603\n",
      "Iteration: 6822 lambda_n: 0.918923275349563 Loss: 0.00020191496017443609\n",
      "Iteration: 6823 lambda_n: 0.9800220093297382 Loss: 0.000201914960174436\n",
      "Iteration: 6824 lambda_n: 0.8848466549022276 Loss: 0.00020191496017443603\n",
      "Iteration: 6825 lambda_n: 0.9075745762385927 Loss: 0.00020191496017443598\n",
      "Iteration: 6826 lambda_n: 1.028324235413311 Loss: 0.00020191496017443603\n",
      "Iteration: 6827 lambda_n: 0.9203322879848251 Loss: 0.00020191496017443609\n",
      "Iteration: 6828 lambda_n: 0.9858790508343593 Loss: 0.000201914960174436\n",
      "Iteration: 6829 lambda_n: 0.9683528239262895 Loss: 0.00020191496017443603\n",
      "Iteration: 6830 lambda_n: 1.0167916244799091 Loss: 0.00020191496017443598\n",
      "Iteration: 6831 lambda_n: 0.9242185026816602 Loss: 0.00020191496017443603\n",
      "Iteration: 6832 lambda_n: 0.9611323553113249 Loss: 0.00020191496017443609\n",
      "Iteration: 6833 lambda_n: 1.002379081352103 Loss: 0.000201914960174436\n",
      "Iteration: 6834 lambda_n: 1.0229970297972482 Loss: 0.00020191496017443603\n",
      "Iteration: 6835 lambda_n: 0.9238306600291958 Loss: 0.00020191496017443598\n",
      "Iteration: 6836 lambda_n: 0.9387917632737436 Loss: 0.00020191496017443603\n",
      "Iteration: 6837 lambda_n: 1.0090392132567603 Loss: 0.00020191496017443609\n",
      "Iteration: 6838 lambda_n: 0.9857012043395611 Loss: 0.000201914960174436\n",
      "Iteration: 6839 lambda_n: 0.9205615617812443 Loss: 0.00020191496017443603\n",
      "Iteration: 6840 lambda_n: 1.0297906145416542 Loss: 0.00020191496017443598\n",
      "Iteration: 6841 lambda_n: 0.9534209477858187 Loss: 0.00020191496017443603\n",
      "Iteration: 6842 lambda_n: 0.902613379522178 Loss: 0.00020191496017443609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6843 lambda_n: 0.8884816785479452 Loss: 0.000201914960174436\n",
      "Iteration: 6844 lambda_n: 0.9127519191566811 Loss: 0.00020191496017443606\n",
      "Iteration: 6845 lambda_n: 0.9140028326065879 Loss: 0.00020191496017443598\n",
      "Iteration: 6846 lambda_n: 0.8847971897265191 Loss: 0.00020191496017443603\n",
      "Iteration: 6847 lambda_n: 0.9527160183854098 Loss: 0.00020191496017443609\n",
      "Iteration: 6848 lambda_n: 0.9540026676351936 Loss: 0.000201914960174436\n",
      "Iteration: 6849 lambda_n: 0.9754718436580573 Loss: 0.00020191496017443606\n",
      "Iteration: 6850 lambda_n: 0.9926208042744661 Loss: 0.00020191496017443598\n",
      "Iteration: 6851 lambda_n: 0.9166263306424713 Loss: 0.00020191496017443603\n",
      "Iteration: 6852 lambda_n: 0.9422388435499861 Loss: 0.00020191496017443609\n",
      "Iteration: 6853 lambda_n: 1.0162480810840715 Loss: 0.000201914960174436\n",
      "Iteration: 6854 lambda_n: 0.8836130313513317 Loss: 0.00020191496017443603\n",
      "Iteration: 6855 lambda_n: 0.9449117861688006 Loss: 0.00020191496017443598\n",
      "Iteration: 6856 lambda_n: 0.9444909398474606 Loss: 0.00020191496017443603\n",
      "Iteration: 6857 lambda_n: 0.9681132690883953 Loss: 0.00020191496017443609\n",
      "Iteration: 6858 lambda_n: 0.9149368032692262 Loss: 0.000201914960174436\n",
      "Iteration: 6859 lambda_n: 0.958661814429887 Loss: 0.00020191496017443603\n",
      "Iteration: 6860 lambda_n: 1.0201307735276341 Loss: 0.00020191496017443598\n",
      "Iteration: 6861 lambda_n: 0.986589800766337 Loss: 0.00020191496017443603\n",
      "Iteration: 6862 lambda_n: 0.9994902693585977 Loss: 0.00020191496017443609\n",
      "Iteration: 6863 lambda_n: 1.0318268860810886 Loss: 0.000201914960174436\n",
      "Iteration: 6864 lambda_n: 0.9537003752582884 Loss: 0.00020191496017443603\n",
      "Iteration: 6865 lambda_n: 0.9957316709558675 Loss: 0.00020191496017443598\n",
      "Iteration: 6866 lambda_n: 0.975205990611439 Loss: 0.00020191496017443603\n",
      "Iteration: 6867 lambda_n: 0.9913663773066731 Loss: 0.00020191496017443609\n",
      "Iteration: 6868 lambda_n: 1.0359445636672808 Loss: 0.000201914960174436\n",
      "Iteration: 6869 lambda_n: 0.9688612690454271 Loss: 0.00020191496017443603\n",
      "Iteration: 6870 lambda_n: 0.8848890159819597 Loss: 0.00020191496017443598\n",
      "Iteration: 6871 lambda_n: 0.96233787983717 Loss: 0.00020191496017443603\n",
      "Iteration: 6872 lambda_n: 0.9289815411578063 Loss: 0.00020191496017443609\n",
      "Iteration: 6873 lambda_n: 0.8853299684405125 Loss: 0.000201914960174436\n",
      "Iteration: 6874 lambda_n: 1.0112655198778535 Loss: 0.00020191496017443603\n",
      "Iteration: 6875 lambda_n: 1.0177698556430974 Loss: 0.00020191496017443598\n",
      "Iteration: 6876 lambda_n: 1.0195285302706496 Loss: 0.00020191496017443603\n",
      "Iteration: 6877 lambda_n: 1.0269502621341267 Loss: 0.00020191496017443609\n",
      "Iteration: 6878 lambda_n: 0.965863136311209 Loss: 0.000201914960174436\n",
      "Iteration: 6879 lambda_n: 0.9357652739531384 Loss: 0.00020191496017443603\n",
      "Iteration: 6880 lambda_n: 0.9376371707264595 Loss: 0.00020191496017443598\n",
      "Iteration: 6881 lambda_n: 0.9937517336301174 Loss: 0.00020191496017443603\n",
      "Iteration: 6882 lambda_n: 0.9381930984080962 Loss: 0.00020191496017443609\n",
      "Iteration: 6883 lambda_n: 0.9769418108983091 Loss: 0.000201914960174436\n",
      "Iteration: 6884 lambda_n: 0.9276163812061798 Loss: 0.00020191496017443603\n",
      "Iteration: 6885 lambda_n: 0.9708555840069566 Loss: 0.00020191496017443598\n",
      "Iteration: 6886 lambda_n: 0.9887630813823999 Loss: 0.00020191496017443603\n",
      "Iteration: 6887 lambda_n: 0.8990020690699985 Loss: 0.00020191496017443609\n",
      "Iteration: 6888 lambda_n: 0.9918093146689977 Loss: 0.000201914960174436\n",
      "Iteration: 6889 lambda_n: 0.9818770383558761 Loss: 0.00020191496017443606\n",
      "Iteration: 6890 lambda_n: 1.0366272239095502 Loss: 0.00020191496017443598\n",
      "Iteration: 6891 lambda_n: 0.9516684108506372 Loss: 0.00020191496017443603\n",
      "Iteration: 6892 lambda_n: 0.9935747782406307 Loss: 0.00020191496017443609\n",
      "Iteration: 6893 lambda_n: 0.9506503314697579 Loss: 0.000201914960174436\n",
      "Iteration: 6894 lambda_n: 1.0048429475560887 Loss: 0.00020191496017443603\n",
      "Iteration: 6895 lambda_n: 0.8870424692994826 Loss: 0.00020191496017443598\n",
      "Iteration: 6896 lambda_n: 0.9299758664635549 Loss: 0.00020191496017443603\n",
      "Iteration: 6897 lambda_n: 1.0339867939678036 Loss: 0.00020191496017443609\n",
      "Iteration: 6898 lambda_n: 0.9255707481781486 Loss: 0.000201914960174436\n",
      "Iteration: 6899 lambda_n: 0.8957342265524598 Loss: 0.00020191496017443603\n",
      "Iteration: 6900 lambda_n: 1.0279102151298622 Loss: 0.00020191496017443598\n",
      "Iteration: 6901 lambda_n: 0.8975851282974376 Loss: 0.00020191496017443603\n",
      "Iteration: 6902 lambda_n: 0.8913807260993295 Loss: 0.00020191496017443609\n",
      "Iteration: 6903 lambda_n: 1.0345657437856082 Loss: 0.000201914960174436\n",
      "Iteration: 6904 lambda_n: 0.99387853625499 Loss: 0.00020191496017443606\n",
      "Iteration: 6905 lambda_n: 0.8850688367180273 Loss: 0.00020191496017443598\n",
      "Iteration: 6906 lambda_n: 0.9198730068310883 Loss: 0.00020191496017443603\n",
      "Iteration: 6907 lambda_n: 0.8886882477888577 Loss: 0.00020191496017443609\n",
      "Iteration: 6908 lambda_n: 0.8975932470873745 Loss: 0.000201914960174436\n",
      "Iteration: 6909 lambda_n: 0.94503961705699 Loss: 0.00020191496017443606\n",
      "Iteration: 6910 lambda_n: 0.9856046096574397 Loss: 0.00020191496017443598\n",
      "Iteration: 6911 lambda_n: 0.8975360921897344 Loss: 0.00020191496017443603\n",
      "Iteration: 6912 lambda_n: 0.9525127238779825 Loss: 0.00020191496017443609\n",
      "Iteration: 6913 lambda_n: 0.9150142107706103 Loss: 0.000201914960174436\n",
      "Iteration: 6914 lambda_n: 0.9752949501984611 Loss: 0.00020191496017443603\n",
      "Iteration: 6915 lambda_n: 0.9672961277327514 Loss: 0.00020191496017443598\n",
      "Iteration: 6916 lambda_n: 0.9042764009292976 Loss: 0.00020191496017443603\n",
      "Iteration: 6917 lambda_n: 0.9915525718596053 Loss: 0.00020191496017443609\n",
      "Iteration: 6918 lambda_n: 0.9117993712593181 Loss: 0.000201914960174436\n",
      "Iteration: 6919 lambda_n: 1.0319485429512694 Loss: 0.00020191496017443603\n",
      "Iteration: 6920 lambda_n: 0.9744504544239776 Loss: 0.00020191496017443598\n",
      "Iteration: 6921 lambda_n: 0.91022197861706 Loss: 0.00020191496017443603\n",
      "Iteration: 6922 lambda_n: 0.9680567441535413 Loss: 0.00020191496017443609\n",
      "Iteration: 6923 lambda_n: 0.9820930605529885 Loss: 0.000201914960174436\n",
      "Iteration: 6924 lambda_n: 0.9071954280475224 Loss: 0.00020191496017443603\n",
      "Iteration: 6925 lambda_n: 0.9708089256584627 Loss: 0.00020191496017443598\n",
      "Iteration: 6926 lambda_n: 1.02107214483079 Loss: 0.00020191496017443603\n",
      "Iteration: 6927 lambda_n: 1.002504260742059 Loss: 0.00020191496017443609\n",
      "Iteration: 6928 lambda_n: 0.9484151247250228 Loss: 0.000201914960174436\n",
      "Iteration: 6929 lambda_n: 1.0010594434902 Loss: 0.00020191496017443603\n",
      "Iteration: 6930 lambda_n: 0.9600482467625422 Loss: 0.00020191496017443598\n",
      "Iteration: 6931 lambda_n: 0.8865152434632128 Loss: 0.00020191496017443603\n",
      "Iteration: 6932 lambda_n: 1.0039518104466671 Loss: 0.00020191496017443609\n",
      "Iteration: 6933 lambda_n: 1.0197037719977737 Loss: 0.000201914960174436\n",
      "Iteration: 6934 lambda_n: 0.9855072776350006 Loss: 0.00020191496017443603\n",
      "Iteration: 6935 lambda_n: 0.9073943679413725 Loss: 0.00020191496017443598\n",
      "Iteration: 6936 lambda_n: 0.9064418245054324 Loss: 0.00020191496017443603\n",
      "Iteration: 6937 lambda_n: 1.0153294745616572 Loss: 0.00020191496017443609\n",
      "Iteration: 6938 lambda_n: 1.01325958203385 Loss: 0.000201914960174436\n",
      "Iteration: 6939 lambda_n: 1.0040465614213623 Loss: 0.00020191496017443603\n",
      "Iteration: 6940 lambda_n: 0.8834120587426495 Loss: 0.00020191496017443598\n",
      "Iteration: 6941 lambda_n: 0.8979567945504946 Loss: 0.00020191496017443603\n",
      "Iteration: 6942 lambda_n: 1.028773936891581 Loss: 0.00020191496017443609\n",
      "Iteration: 6943 lambda_n: 0.9926490271004369 Loss: 0.000201914960174436\n",
      "Iteration: 6944 lambda_n: 0.9196382594470803 Loss: 0.00020191496017443603\n",
      "Iteration: 6945 lambda_n: 0.9833672908037838 Loss: 0.00020191496017443598\n",
      "Iteration: 6946 lambda_n: 0.8870491884076459 Loss: 0.00020191496017443603\n",
      "Iteration: 6947 lambda_n: 0.9581524052052213 Loss: 0.00020191496017443609\n",
      "Iteration: 6948 lambda_n: 1.0167415541169305 Loss: 0.000201914960174436\n",
      "Iteration: 6949 lambda_n: 0.9278249864334867 Loss: 0.00020191496017443603\n",
      "Iteration: 6950 lambda_n: 0.9165216730262378 Loss: 0.00020191496017443598\n",
      "Iteration: 6951 lambda_n: 0.916450657679308 Loss: 0.00020191496017443603\n",
      "Iteration: 6952 lambda_n: 0.976661356517904 Loss: 0.00020191496017443609\n",
      "Iteration: 6953 lambda_n: 0.9314349663016588 Loss: 0.000201914960174436\n",
      "Iteration: 6954 lambda_n: 0.9864843906428744 Loss: 0.00020191496017443603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6955 lambda_n: 0.9583138371570937 Loss: 0.00020191496017443598\n",
      "Iteration: 6956 lambda_n: 0.9209643071191366 Loss: 0.00020191496017443603\n",
      "Iteration: 6957 lambda_n: 0.9678809857579753 Loss: 0.00020191496017443609\n",
      "Iteration: 6958 lambda_n: 0.9726152043033943 Loss: 0.000201914960174436\n",
      "Iteration: 6959 lambda_n: 0.8867124668786068 Loss: 0.00020191496017443603\n",
      "Iteration: 6960 lambda_n: 0.8980625085018117 Loss: 0.00020191496017443598\n",
      "Iteration: 6961 lambda_n: 1.030892038596414 Loss: 0.00020191496017443603\n",
      "Iteration: 6962 lambda_n: 0.9019140506928399 Loss: 0.00020191496017443609\n",
      "Iteration: 6963 lambda_n: 1.0052839540953606 Loss: 0.000201914960174436\n",
      "Iteration: 6964 lambda_n: 0.9510544810904215 Loss: 0.00020191496017443606\n",
      "Iteration: 6965 lambda_n: 1.0094218239815445 Loss: 0.00020191496017443598\n",
      "Iteration: 6966 lambda_n: 0.9774106575066431 Loss: 0.00020191496017443603\n",
      "Iteration: 6967 lambda_n: 0.9028904017329974 Loss: 0.00020191496017443609\n",
      "Iteration: 6968 lambda_n: 0.9580256446652363 Loss: 0.000201914960174436\n",
      "Iteration: 6969 lambda_n: 0.9956759357529974 Loss: 0.00020191496017443606\n",
      "Iteration: 6970 lambda_n: 0.9847181808442982 Loss: 0.00020191496017443598\n",
      "Iteration: 6971 lambda_n: 1.012705529050245 Loss: 0.00020191496017443603\n",
      "Iteration: 6972 lambda_n: 0.9176732139356615 Loss: 0.00020191496017443609\n",
      "Iteration: 6973 lambda_n: 0.9768864960552549 Loss: 0.000201914960174436\n",
      "Iteration: 6974 lambda_n: 0.9162221229229277 Loss: 0.00020191496017443603\n",
      "Iteration: 6975 lambda_n: 0.9624624188621176 Loss: 0.00020191496017443598\n",
      "Iteration: 6976 lambda_n: 0.9116836556728809 Loss: 0.00020191496017443603\n",
      "Iteration: 6977 lambda_n: 1.00912928739514 Loss: 0.00020191496017443609\n",
      "Iteration: 6978 lambda_n: 1.021921249730508 Loss: 0.000201914960174436\n",
      "Iteration: 6979 lambda_n: 0.9827261976919672 Loss: 0.00020191496017443603\n",
      "Iteration: 6980 lambda_n: 0.9493521685432714 Loss: 0.00020191496017443598\n",
      "Iteration: 6981 lambda_n: 0.9468680237359443 Loss: 0.00020191496017443603\n",
      "Iteration: 6982 lambda_n: 0.9140951449456988 Loss: 0.00020191496017443609\n",
      "Iteration: 6983 lambda_n: 1.0311698648293954 Loss: 0.000201914960174436\n",
      "Iteration: 6984 lambda_n: 1.0171841031978737 Loss: 0.00020191496017443606\n",
      "Iteration: 6985 lambda_n: 0.9064316767320453 Loss: 0.00020191496017443598\n",
      "Iteration: 6986 lambda_n: 0.8903435050308939 Loss: 0.00020191496017443603\n",
      "Iteration: 6987 lambda_n: 0.9228927268521382 Loss: 0.00020191496017443609\n",
      "Iteration: 6988 lambda_n: 0.9637937853211714 Loss: 0.000201914960174436\n",
      "Iteration: 6989 lambda_n: 0.8883918797124041 Loss: 0.00020191496017443603\n",
      "Iteration: 6990 lambda_n: 0.8954038544819733 Loss: 0.00020191496017443598\n",
      "Iteration: 6991 lambda_n: 0.9212161871624454 Loss: 0.00020191496017443603\n",
      "Iteration: 6992 lambda_n: 0.9387225090697953 Loss: 0.00020191496017443609\n",
      "Iteration: 6993 lambda_n: 0.9462033245536228 Loss: 0.000201914960174436\n",
      "Iteration: 6994 lambda_n: 0.8879652185814878 Loss: 0.00020191496017443603\n",
      "Iteration: 6995 lambda_n: 0.8836524462030505 Loss: 0.00020191496017443598\n",
      "Iteration: 6996 lambda_n: 0.9961813310663244 Loss: 0.00020191496017443603\n",
      "Iteration: 6997 lambda_n: 0.9681261052937995 Loss: 0.00020191496017443609\n",
      "Iteration: 6998 lambda_n: 1.000372446066758 Loss: 0.000201914960174436\n",
      "Iteration: 6999 lambda_n: 0.9063323483742877 Loss: 0.00020191496017443603\n",
      "Iteration: 7000 lambda_n: 0.9220444586610349 Loss: 0.00020191496017443598\n",
      "Iteration: 7001 lambda_n: 1.0343767629338443 Loss: 0.00020191496017443603\n",
      "Iteration: 7002 lambda_n: 0.8994159242603263 Loss: 0.00020191496017443609\n",
      "Iteration: 7003 lambda_n: 0.9493965794855292 Loss: 0.000201914960174436\n",
      "Iteration: 7004 lambda_n: 0.9837350293609826 Loss: 0.00020191496017443606\n",
      "Iteration: 7005 lambda_n: 1.003709358590618 Loss: 0.00020191496017443598\n",
      "Iteration: 7006 lambda_n: 0.9510193171911737 Loss: 0.00020191496017443603\n",
      "Iteration: 7007 lambda_n: 0.9488749658118417 Loss: 0.00020191496017443609\n",
      "Iteration: 7008 lambda_n: 0.9379001041732185 Loss: 0.000201914960174436\n",
      "Iteration: 7009 lambda_n: 0.9734471185950612 Loss: 0.00020191496017443603\n",
      "Iteration: 7010 lambda_n: 0.9429243650742948 Loss: 0.00020191496017443598\n",
      "Iteration: 7011 lambda_n: 0.9801266601382056 Loss: 0.00020191496017443603\n",
      "Iteration: 7012 lambda_n: 1.0268193038324531 Loss: 0.00020191496017443609\n",
      "Iteration: 7013 lambda_n: 1.0164230160662135 Loss: 0.000201914960174436\n",
      "Iteration: 7014 lambda_n: 1.0351408369638562 Loss: 0.00020191496017443603\n",
      "Iteration: 7015 lambda_n: 0.9441565811195516 Loss: 0.00020191496017443598\n",
      "Iteration: 7016 lambda_n: 1.00391578415389 Loss: 0.00020191496017443603\n",
      "Iteration: 7017 lambda_n: 1.013268521723392 Loss: 0.00020191496017443609\n",
      "Iteration: 7018 lambda_n: 0.9176350906239621 Loss: 0.000201914960174436\n",
      "Iteration: 7019 lambda_n: 0.9721977191365988 Loss: 0.00020191496017443603\n",
      "Iteration: 7020 lambda_n: 0.9991142071571566 Loss: 0.00020191496017443598\n",
      "Iteration: 7021 lambda_n: 0.9216618625136016 Loss: 0.00020191496017443603\n",
      "Iteration: 7022 lambda_n: 1.0102234952502136 Loss: 0.00020191496017443609\n",
      "Iteration: 7023 lambda_n: 0.9298944491668059 Loss: 0.000201914960174436\n",
      "Iteration: 7024 lambda_n: 0.9179397661620228 Loss: 0.00020191496017443603\n",
      "Iteration: 7025 lambda_n: 0.9273294041020984 Loss: 0.00020191496017443598\n",
      "Iteration: 7026 lambda_n: 1.0166043049703788 Loss: 0.00020191496017443603\n",
      "Iteration: 7027 lambda_n: 0.9812046436042489 Loss: 0.00020191496017443609\n",
      "Iteration: 7028 lambda_n: 0.923380382686651 Loss: 0.000201914960174436\n",
      "Iteration: 7029 lambda_n: 0.9460683875321622 Loss: 0.00020191496017443603\n",
      "Iteration: 7030 lambda_n: 0.9962113749576986 Loss: 0.00020191496017443598\n",
      "Iteration: 7031 lambda_n: 0.9640598577903453 Loss: 0.00020191496017443603\n",
      "Iteration: 7032 lambda_n: 0.928440263181265 Loss: 0.00020191496017443609\n",
      "Iteration: 7033 lambda_n: 0.9379335036002016 Loss: 0.000201914960174436\n",
      "Iteration: 7034 lambda_n: 1.005677534642949 Loss: 0.00020191496017443603\n",
      "Iteration: 7035 lambda_n: 0.9032307495862733 Loss: 0.00020191496017443598\n",
      "Iteration: 7036 lambda_n: 0.886321985326736 Loss: 0.00020191496017443603\n",
      "Iteration: 7037 lambda_n: 0.9012276841885454 Loss: 0.00020191496017443609\n",
      "Iteration: 7038 lambda_n: 0.9929124754590005 Loss: 0.000201914960174436\n",
      "Iteration: 7039 lambda_n: 1.0059444663660289 Loss: 0.00020191496017443606\n",
      "Iteration: 7040 lambda_n: 0.9753944633062965 Loss: 0.00020191496017443598\n",
      "Iteration: 7041 lambda_n: 0.971910805667915 Loss: 0.00020191496017443603\n",
      "Iteration: 7042 lambda_n: 0.9973686615978966 Loss: 0.00020191496017443609\n",
      "Iteration: 7043 lambda_n: 0.908407537392211 Loss: 0.000201914960174436\n",
      "Iteration: 7044 lambda_n: 0.9333780260130442 Loss: 0.00020191496017443603\n",
      "Iteration: 7045 lambda_n: 1.0297579849830687 Loss: 0.00020191496017443598\n",
      "Iteration: 7046 lambda_n: 0.8997883571494499 Loss: 0.00020191496017443603\n",
      "Iteration: 7047 lambda_n: 1.0309590713183547 Loss: 0.00020191496017443609\n",
      "Iteration: 7048 lambda_n: 0.9209904018565865 Loss: 0.000201914960174436\n",
      "Iteration: 7049 lambda_n: 0.9622546856095991 Loss: 0.00020191496017443603\n",
      "Iteration: 7050 lambda_n: 0.9371929355861004 Loss: 0.00020191496017443598\n",
      "Iteration: 7051 lambda_n: 1.0298449115604509 Loss: 0.00020191496017443603\n",
      "Iteration: 7052 lambda_n: 0.942116906447945 Loss: 0.00020191496017443609\n",
      "Iteration: 7053 lambda_n: 1.03234049674389 Loss: 0.000201914960174436\n",
      "Iteration: 7054 lambda_n: 0.961876095249858 Loss: 0.00020191496017443603\n",
      "Iteration: 7055 lambda_n: 0.9631273732145068 Loss: 0.00020191496017443598\n",
      "Iteration: 7056 lambda_n: 0.9268569425393232 Loss: 0.00020191496017443603\n",
      "Iteration: 7057 lambda_n: 0.9142823686823554 Loss: 0.00020191496017443609\n",
      "Iteration: 7058 lambda_n: 0.9922132178911288 Loss: 0.000201914960174436\n",
      "Iteration: 7059 lambda_n: 0.9046859173551278 Loss: 0.00020191496017443606\n",
      "Iteration: 7060 lambda_n: 0.899761305975065 Loss: 0.00020191496017443598\n",
      "Iteration: 7061 lambda_n: 0.9765347619830322 Loss: 0.00020191496017443603\n",
      "Iteration: 7062 lambda_n: 0.9721373232917112 Loss: 0.00020191496017443609\n",
      "Iteration: 7063 lambda_n: 0.9483383805659535 Loss: 0.000201914960174436\n",
      "Iteration: 7064 lambda_n: 1.0108639666998822 Loss: 0.00020191496017443606\n",
      "Iteration: 7065 lambda_n: 1.0335891326550077 Loss: 0.00020191496017443598\n",
      "Iteration: 7066 lambda_n: 0.9341761512547541 Loss: 0.00020191496017443603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7067 lambda_n: 1.0260371793432252 Loss: 0.00020191496017443609\n",
      "Iteration: 7068 lambda_n: 1.0281189663518893 Loss: 0.000201914960174436\n",
      "Iteration: 7069 lambda_n: 0.9665591762208092 Loss: 0.00020191496017443603\n",
      "Iteration: 7070 lambda_n: 0.9701966338048428 Loss: 0.00020191496017443598\n",
      "Iteration: 7071 lambda_n: 0.9465103354114571 Loss: 0.00020191496017443603\n",
      "Iteration: 7072 lambda_n: 0.9888589274396314 Loss: 0.00020191496017443609\n",
      "Iteration: 7073 lambda_n: 0.9424661389451042 Loss: 0.000201914960174436\n",
      "Iteration: 7074 lambda_n: 0.9129652151322001 Loss: 0.00020191496017443603\n",
      "Iteration: 7075 lambda_n: 0.935428988927599 Loss: 0.00020191496017443598\n",
      "Iteration: 7076 lambda_n: 0.9432178400609946 Loss: 0.00020191496017443603\n",
      "Iteration: 7077 lambda_n: 1.0295954067524211 Loss: 0.00020191496017443609\n",
      "Iteration: 7078 lambda_n: 0.889720608253202 Loss: 0.000201914960174436\n",
      "Iteration: 7079 lambda_n: 1.032541789188594 Loss: 0.00020191496017443603\n",
      "Iteration: 7080 lambda_n: 1.0269820688249964 Loss: 0.00020191496017443598\n",
      "Iteration: 7081 lambda_n: 0.9421321448340182 Loss: 0.00020191496017443603\n",
      "Iteration: 7082 lambda_n: 0.9182842115248259 Loss: 0.00020191496017443609\n",
      "Iteration: 7083 lambda_n: 0.9346733318179157 Loss: 0.000201914960174436\n",
      "Iteration: 7084 lambda_n: 0.993329302091256 Loss: 0.00020191496017443603\n",
      "Iteration: 7085 lambda_n: 0.9127746843482276 Loss: 0.00020191496017443598\n",
      "Iteration: 7086 lambda_n: 0.9976473225216421 Loss: 0.00020191496017443603\n",
      "Iteration: 7087 lambda_n: 0.9842320192043167 Loss: 0.00020191496017443609\n",
      "Iteration: 7088 lambda_n: 0.9965272803815216 Loss: 0.000201914960174436\n",
      "Iteration: 7089 lambda_n: 1.0161233903293423 Loss: 0.00020191496017443603\n",
      "Iteration: 7090 lambda_n: 0.9464081067884113 Loss: 0.00020191496017443598\n",
      "Iteration: 7091 lambda_n: 0.8926971058530443 Loss: 0.00020191496017443603\n",
      "Iteration: 7092 lambda_n: 0.9781771928633477 Loss: 0.00020191496017443609\n",
      "Iteration: 7093 lambda_n: 0.9300079518142971 Loss: 0.000201914960174436\n",
      "Iteration: 7094 lambda_n: 0.9464992058494986 Loss: 0.00020191496017443603\n",
      "Iteration: 7095 lambda_n: 0.9162021880138698 Loss: 0.00020191496017443598\n",
      "Iteration: 7096 lambda_n: 0.9498667463946814 Loss: 0.00020191496017443603\n",
      "Iteration: 7097 lambda_n: 0.9393358158816895 Loss: 0.00020191496017443609\n",
      "Iteration: 7098 lambda_n: 0.9381196516695511 Loss: 0.000201914960174436\n",
      "Iteration: 7099 lambda_n: 0.9432200996316391 Loss: 0.00020191496017443603\n",
      "Iteration: 7100 lambda_n: 0.9701052599035349 Loss: 0.00020191496017443598\n",
      "Iteration: 7101 lambda_n: 0.9835956593347509 Loss: 0.00020191496017443603\n",
      "Iteration: 7102 lambda_n: 1.020401109336599 Loss: 0.00020191496017443609\n",
      "Iteration: 7103 lambda_n: 0.911488470193474 Loss: 0.000201914960174436\n",
      "Iteration: 7104 lambda_n: 0.9821482277009191 Loss: 0.00020191496017443603\n",
      "Iteration: 7105 lambda_n: 1.0121598380705172 Loss: 0.00020191496017443598\n",
      "Iteration: 7106 lambda_n: 1.022190643386379 Loss: 0.00020191496017443603\n",
      "Iteration: 7107 lambda_n: 0.9072908216651288 Loss: 0.00020191496017443609\n",
      "Iteration: 7108 lambda_n: 0.9849955011211129 Loss: 0.000201914960174436\n",
      "Iteration: 7109 lambda_n: 1.0276520018186066 Loss: 0.00020191496017443606\n",
      "Iteration: 7110 lambda_n: 0.9566665943110206 Loss: 0.00020191496017443598\n",
      "Iteration: 7111 lambda_n: 1.030578544143337 Loss: 0.00020191496017443603\n",
      "Iteration: 7112 lambda_n: 1.0076361451083737 Loss: 0.00020191496017443609\n",
      "Iteration: 7113 lambda_n: 1.0050656332389385 Loss: 0.000201914960174436\n",
      "Iteration: 7114 lambda_n: 0.9247685106143514 Loss: 0.00020191496017443603\n",
      "Iteration: 7115 lambda_n: 1.009540395440858 Loss: 0.00020191496017443598\n",
      "Iteration: 7116 lambda_n: 1.0012073403481034 Loss: 0.00020191496017443603\n",
      "Iteration: 7117 lambda_n: 0.9349177922498456 Loss: 0.00020191496017443609\n",
      "Iteration: 7118 lambda_n: 0.9293293681717629 Loss: 0.000201914960174436\n",
      "Iteration: 7119 lambda_n: 1.0248442680582457 Loss: 0.00020191496017443603\n",
      "Iteration: 7120 lambda_n: 1.0205264549529343 Loss: 0.00020191496017443598\n",
      "Iteration: 7121 lambda_n: 1.012480891097472 Loss: 0.00020191496017443603\n",
      "Iteration: 7122 lambda_n: 0.9131313484116784 Loss: 0.00020191496017443609\n",
      "Iteration: 7123 lambda_n: 0.9366665776700348 Loss: 0.000201914960174436\n",
      "Iteration: 7124 lambda_n: 0.8856646267869133 Loss: 0.00020191496017443606\n",
      "Iteration: 7125 lambda_n: 0.886214818428392 Loss: 0.00020191496017443598\n",
      "Iteration: 7126 lambda_n: 0.9805968488192723 Loss: 0.00020191496017443603\n",
      "Iteration: 7127 lambda_n: 0.9682596337650946 Loss: 0.00020191496017443609\n",
      "Iteration: 7128 lambda_n: 0.9649701452767395 Loss: 0.000201914960174436\n",
      "Iteration: 7129 lambda_n: 0.9974856833843818 Loss: 0.00020191496017443606\n",
      "Iteration: 7130 lambda_n: 0.8881252028920567 Loss: 0.00020191496017443598\n",
      "Iteration: 7131 lambda_n: 1.0277105371594464 Loss: 0.00020191496017443603\n",
      "Iteration: 7132 lambda_n: 0.903650994815952 Loss: 0.00020191496017443609\n",
      "Iteration: 7133 lambda_n: 0.99137653675321 Loss: 0.000201914960174436\n",
      "Iteration: 7134 lambda_n: 0.9655003671223856 Loss: 0.00020191496017443606\n",
      "Iteration: 7135 lambda_n: 0.9262131525981283 Loss: 0.00020191496017443598\n",
      "Iteration: 7136 lambda_n: 0.9698053751836734 Loss: 0.00020191496017443603\n",
      "Iteration: 7137 lambda_n: 0.8996562056312372 Loss: 0.00020191496017443609\n",
      "Iteration: 7138 lambda_n: 1.032368392676164 Loss: 0.000201914960174436\n",
      "Iteration: 7139 lambda_n: 0.9916883450102756 Loss: 0.00020191496017443606\n",
      "Iteration: 7140 lambda_n: 1.0020250244144253 Loss: 0.00020191496017443598\n",
      "Iteration: 7141 lambda_n: 0.9231957810280996 Loss: 0.00020191496017443603\n",
      "Iteration: 7142 lambda_n: 0.9163791308310755 Loss: 0.00020191496017443609\n",
      "Iteration: 7143 lambda_n: 0.8957496630305437 Loss: 0.000201914960174436\n",
      "Iteration: 7144 lambda_n: 0.9479438338460894 Loss: 0.00020191496017443606\n",
      "Iteration: 7145 lambda_n: 0.9237780564862563 Loss: 0.00020191496017443598\n",
      "Iteration: 7146 lambda_n: 0.9054910876803351 Loss: 0.00020191496017443603\n",
      "Iteration: 7147 lambda_n: 0.922420655115188 Loss: 0.00020191496017443609\n",
      "Iteration: 7148 lambda_n: 0.8945463976737092 Loss: 0.000201914960174436\n",
      "Iteration: 7149 lambda_n: 1.0320420650679962 Loss: 0.00020191496017443603\n",
      "Iteration: 7150 lambda_n: 0.8935512602029515 Loss: 0.00020191496017443598\n",
      "Iteration: 7151 lambda_n: 1.0204139883730434 Loss: 0.00020191496017443603\n",
      "Iteration: 7152 lambda_n: 0.8878443301932027 Loss: 0.00020191496017443609\n",
      "Iteration: 7153 lambda_n: 0.9866419035415915 Loss: 0.000201914960174436\n",
      "Iteration: 7154 lambda_n: 0.9873690674916181 Loss: 0.00020191496017443606\n",
      "Iteration: 7155 lambda_n: 1.0139135953164253 Loss: 0.00020191496017443598\n",
      "Iteration: 7156 lambda_n: 1.0208072850554464 Loss: 0.00020191496017443603\n",
      "Iteration: 7157 lambda_n: 0.8913569466092475 Loss: 0.00020191496017443609\n",
      "Iteration: 7158 lambda_n: 1.02830311575551 Loss: 0.000201914960174436\n",
      "Iteration: 7159 lambda_n: 0.9857046842987992 Loss: 0.00020191496017443606\n",
      "Iteration: 7160 lambda_n: 0.8980900549479633 Loss: 0.00020191496017443598\n",
      "Iteration: 7161 lambda_n: 0.8993229261930834 Loss: 0.00020191496017443603\n",
      "Iteration: 7162 lambda_n: 0.955247732284213 Loss: 0.00020191496017443609\n",
      "Iteration: 7163 lambda_n: 1.0307071131001166 Loss: 0.000201914960174436\n",
      "Iteration: 7164 lambda_n: 0.9997874313813843 Loss: 0.00020191496017443603\n",
      "Iteration: 7165 lambda_n: 0.9258353039048726 Loss: 0.00020191496017443598\n",
      "Iteration: 7166 lambda_n: 0.9632643096598564 Loss: 0.00020191496017443603\n",
      "Iteration: 7167 lambda_n: 0.8945495872144669 Loss: 0.00020191496017443609\n",
      "Iteration: 7168 lambda_n: 0.9237316490584545 Loss: 0.000201914960174436\n",
      "Iteration: 7169 lambda_n: 0.991892829319441 Loss: 0.00020191496017443606\n",
      "Iteration: 7170 lambda_n: 0.9372233271782234 Loss: 0.00020191496017443598\n",
      "Iteration: 7171 lambda_n: 0.9392176589203816 Loss: 0.00020191496017443603\n",
      "Iteration: 7172 lambda_n: 0.9346868933490988 Loss: 0.00020191496017443609\n",
      "Iteration: 7173 lambda_n: 0.9777574328429116 Loss: 0.000201914960174436\n",
      "Iteration: 7174 lambda_n: 1.0360878444825663 Loss: 0.00020191496017443603\n",
      "Iteration: 7175 lambda_n: 0.952506864058002 Loss: 0.00020191496017443598\n",
      "Iteration: 7176 lambda_n: 1.0012952380411466 Loss: 0.00020191496017443603\n",
      "Iteration: 7177 lambda_n: 0.9036134978430838 Loss: 0.00020191496017443609\n",
      "Iteration: 7178 lambda_n: 0.9485386425369944 Loss: 0.000201914960174436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7179 lambda_n: 0.9255083147294603 Loss: 0.00020191496017443606\n",
      "Iteration: 7180 lambda_n: 0.8883374025798602 Loss: 0.00020191496017443598\n",
      "Iteration: 7181 lambda_n: 0.8854352243852309 Loss: 0.00020191496017443603\n",
      "Iteration: 7182 lambda_n: 0.9157136032695575 Loss: 0.00020191496017443609\n",
      "Iteration: 7183 lambda_n: 0.8873800870190208 Loss: 0.000201914960174436\n",
      "Iteration: 7184 lambda_n: 0.9891744556318894 Loss: 0.00020191496017443606\n",
      "Iteration: 7185 lambda_n: 0.9246398404179592 Loss: 0.00020191496017443598\n",
      "Iteration: 7186 lambda_n: 0.9812615736495501 Loss: 0.00020191496017443603\n",
      "Iteration: 7187 lambda_n: 0.9592738370875499 Loss: 0.00020191496017443609\n",
      "Iteration: 7188 lambda_n: 0.9246791876207485 Loss: 0.000201914960174436\n",
      "Iteration: 7189 lambda_n: 1.025989955119544 Loss: 0.00020191496017443603\n",
      "Iteration: 7190 lambda_n: 0.9904559584557663 Loss: 0.00020191496017443598\n",
      "Iteration: 7191 lambda_n: 0.8881471078915215 Loss: 0.00020191496017443603\n",
      "Iteration: 7192 lambda_n: 0.9006539920738011 Loss: 0.00020191496017443609\n",
      "Iteration: 7193 lambda_n: 0.9212062644213872 Loss: 0.000201914960174436\n",
      "Iteration: 7194 lambda_n: 1.0381045424799653 Loss: 0.00020191496017443606\n",
      "Iteration: 7195 lambda_n: 0.8902879797987956 Loss: 0.00020191496017443598\n",
      "Iteration: 7196 lambda_n: 1.0178495661642768 Loss: 0.00020191496017443603\n",
      "Iteration: 7197 lambda_n: 0.9143788575213208 Loss: 0.00020191496017443609\n",
      "Iteration: 7198 lambda_n: 1.0262559049080466 Loss: 0.000201914960174436\n",
      "Iteration: 7199 lambda_n: 1.0314311262065319 Loss: 0.00020191496017443606\n",
      "Iteration: 7200 lambda_n: 0.8956362732718912 Loss: 0.00020191496017443598\n",
      "Iteration: 7201 lambda_n: 0.985959949731605 Loss: 0.00020191496017443603\n",
      "Iteration: 7202 lambda_n: 1.0007857206435147 Loss: 0.00020191496017443609\n",
      "Iteration: 7203 lambda_n: 1.0252320552134182 Loss: 0.000201914960174436\n",
      "Iteration: 7204 lambda_n: 0.9330070147596995 Loss: 0.00020191496017443603\n",
      "Iteration: 7205 lambda_n: 1.0318303189982097 Loss: 0.00020191496017443598\n",
      "Iteration: 7206 lambda_n: 0.9544534562283994 Loss: 0.00020191496017443603\n",
      "Iteration: 7207 lambda_n: 1.0187562874895395 Loss: 0.00020191496017443609\n",
      "Iteration: 7208 lambda_n: 0.9355100440110297 Loss: 0.000201914960174436\n",
      "Iteration: 7209 lambda_n: 0.9678612043299242 Loss: 0.00020191496017443603\n",
      "Iteration: 7210 lambda_n: 1.0036224476163949 Loss: 0.00020191496017443598\n",
      "Iteration: 7211 lambda_n: 0.9705923205717369 Loss: 0.00020191496017443603\n",
      "Iteration: 7212 lambda_n: 0.927332961244629 Loss: 0.00020191496017443609\n",
      "Iteration: 7213 lambda_n: 0.950303385211298 Loss: 0.000201914960174436\n",
      "Iteration: 7214 lambda_n: 0.9219817504602199 Loss: 0.00020191496017443603\n",
      "Iteration: 7215 lambda_n: 0.9155862273434602 Loss: 0.00020191496017443598\n",
      "Iteration: 7216 lambda_n: 0.9467885068856382 Loss: 0.00020191496017443603\n",
      "Iteration: 7217 lambda_n: 1.0002875589202092 Loss: 0.00020191496017443609\n",
      "Iteration: 7218 lambda_n: 0.9562020354914913 Loss: 0.000201914960174436\n",
      "Iteration: 7219 lambda_n: 0.95600605202545 Loss: 0.00020191496017443603\n",
      "Iteration: 7220 lambda_n: 1.0295398991365081 Loss: 0.00020191496017443598\n",
      "Iteration: 7221 lambda_n: 0.9154172286327288 Loss: 0.00020191496017443603\n",
      "Iteration: 7222 lambda_n: 0.9414995793930491 Loss: 0.00020191496017443609\n",
      "Iteration: 7223 lambda_n: 0.9906624472539692 Loss: 0.000201914960174436\n",
      "Iteration: 7224 lambda_n: 1.032586475489192 Loss: 0.00020191496017443603\n",
      "Iteration: 7225 lambda_n: 1.0018035246524655 Loss: 0.00020191496017443598\n",
      "Iteration: 7226 lambda_n: 0.9019470583157988 Loss: 0.00020191496017443603\n",
      "Iteration: 7227 lambda_n: 1.006331029449444 Loss: 0.00020191496017443609\n",
      "Iteration: 7228 lambda_n: 0.88445971702654 Loss: 0.000201914960174436\n",
      "Iteration: 7229 lambda_n: 0.9151053143056916 Loss: 0.00020191496017443603\n",
      "Iteration: 7230 lambda_n: 0.9227588916897687 Loss: 0.00020191496017443598\n",
      "Iteration: 7231 lambda_n: 0.9666582247625823 Loss: 0.00020191496017443603\n",
      "Iteration: 7232 lambda_n: 1.0068409523004072 Loss: 0.00020191496017443609\n",
      "Iteration: 7233 lambda_n: 0.8992138369534868 Loss: 0.000201914960174436\n",
      "Iteration: 7234 lambda_n: 1.0274571288115095 Loss: 0.00020191496017443603\n",
      "Iteration: 7235 lambda_n: 0.8984518261905803 Loss: 0.00020191496017443598\n",
      "Iteration: 7236 lambda_n: 0.9353377709583562 Loss: 0.00020191496017443603\n",
      "Iteration: 7237 lambda_n: 0.9005838627980942 Loss: 0.00020191496017443609\n",
      "Iteration: 7238 lambda_n: 0.9715742607547919 Loss: 0.000201914960174436\n",
      "Iteration: 7239 lambda_n: 0.935712438218978 Loss: 0.00020191496017443606\n",
      "Iteration: 7240 lambda_n: 0.9459418066927514 Loss: 0.00020191496017443598\n",
      "Iteration: 7241 lambda_n: 1.0047436326663128 Loss: 0.00020191496017443603\n",
      "Iteration: 7242 lambda_n: 0.9852522318777269 Loss: 0.00020191496017443609\n",
      "Iteration: 7243 lambda_n: 0.9605791676631273 Loss: 0.000201914960174436\n",
      "Iteration: 7244 lambda_n: 0.8866030758752745 Loss: 0.00020191496017443603\n",
      "Iteration: 7245 lambda_n: 0.9608016515952014 Loss: 0.00020191496017443598\n",
      "Iteration: 7246 lambda_n: 0.9708989969686546 Loss: 0.00020191496017443603\n",
      "Iteration: 7247 lambda_n: 0.9042494519296498 Loss: 0.00020191496017443609\n",
      "Iteration: 7248 lambda_n: 0.9923018074622673 Loss: 0.000201914960174436\n",
      "Iteration: 7249 lambda_n: 0.9227096873980464 Loss: 0.00020191496017443606\n",
      "Iteration: 7250 lambda_n: 0.9250783909668977 Loss: 0.00020191496017443598\n",
      "Iteration: 7251 lambda_n: 1.0150870738102418 Loss: 0.00020191496017443603\n",
      "Iteration: 7252 lambda_n: 1.0174210270401511 Loss: 0.00020191496017443609\n",
      "Iteration: 7253 lambda_n: 0.9393912981275169 Loss: 0.000201914960174436\n",
      "Iteration: 7254 lambda_n: 0.970085409196876 Loss: 0.00020191496017443603\n",
      "Iteration: 7255 lambda_n: 0.929859952538667 Loss: 0.00020191496017443598\n",
      "Iteration: 7256 lambda_n: 1.0231113682272974 Loss: 0.00020191496017443603\n",
      "Iteration: 7257 lambda_n: 1.0063585801588757 Loss: 0.00020191496017443609\n",
      "Iteration: 7258 lambda_n: 0.8925949406973753 Loss: 0.000201914960174436\n",
      "Iteration: 7259 lambda_n: 1.0010620673394768 Loss: 0.00020191496017443603\n",
      "Iteration: 7260 lambda_n: 0.9166146232297072 Loss: 0.00020191496017443598\n",
      "Iteration: 7261 lambda_n: 0.994516854103446 Loss: 0.00020191496017443603\n",
      "Iteration: 7262 lambda_n: 1.0221046880015325 Loss: 0.00020191496017443609\n",
      "Iteration: 7263 lambda_n: 0.9545393685195381 Loss: 0.000201914960174436\n",
      "Iteration: 7264 lambda_n: 0.9726844137737458 Loss: 0.00020191496017443603\n",
      "Iteration: 7265 lambda_n: 0.8917073582354246 Loss: 0.00020191496017443598\n",
      "Iteration: 7266 lambda_n: 0.9446736598497245 Loss: 0.00020191496017443603\n",
      "Iteration: 7267 lambda_n: 1.0220909046887345 Loss: 0.00020191496017443609\n",
      "Iteration: 7268 lambda_n: 0.9081634454910069 Loss: 0.000201914960174436\n",
      "Iteration: 7269 lambda_n: 1.00718701682291 Loss: 0.00020191496017443603\n",
      "Iteration: 7270 lambda_n: 0.9755452223418437 Loss: 0.00020191496017443598\n",
      "Iteration: 7271 lambda_n: 0.8990583859877092 Loss: 0.00020191496017443603\n",
      "Iteration: 7272 lambda_n: 1.0155503752621586 Loss: 0.00020191496017443609\n",
      "Iteration: 7273 lambda_n: 0.9165848166440416 Loss: 0.000201914960174436\n",
      "Iteration: 7274 lambda_n: 0.9609762426283184 Loss: 0.00020191496017443603\n",
      "Iteration: 7275 lambda_n: 1.0237526666378471 Loss: 0.00020191496017443598\n",
      "Iteration: 7276 lambda_n: 1.0118212298810119 Loss: 0.00020191496017443603\n",
      "Iteration: 7277 lambda_n: 1.0282479466415235 Loss: 0.00020191496017443609\n",
      "Iteration: 7278 lambda_n: 0.9192215225937884 Loss: 0.000201914960174436\n",
      "Iteration: 7279 lambda_n: 0.9902868262607809 Loss: 0.00020191496017443603\n",
      "Iteration: 7280 lambda_n: 0.9334041606177951 Loss: 0.00020191496017443598\n",
      "Iteration: 7281 lambda_n: 1.0252891259939727 Loss: 0.00020191496017443603\n",
      "Iteration: 7282 lambda_n: 0.978860532991755 Loss: 0.00020191496017443609\n",
      "Iteration: 7283 lambda_n: 0.9198873912546192 Loss: 0.000201914960174436\n",
      "Iteration: 7284 lambda_n: 0.9142878114817984 Loss: 0.00020191496017443603\n",
      "Iteration: 7285 lambda_n: 1.0275976787014551 Loss: 0.00020191496017443598\n",
      "Iteration: 7286 lambda_n: 1.0245211742986833 Loss: 0.00020191496017443603\n",
      "Iteration: 7287 lambda_n: 0.8837885551467431 Loss: 0.00020191496017443609\n",
      "Iteration: 7288 lambda_n: 0.9558591376142453 Loss: 0.000201914960174436\n",
      "Iteration: 7289 lambda_n: 0.9458634670866609 Loss: 0.00020191496017443606\n",
      "Iteration: 7290 lambda_n: 0.9215474622842252 Loss: 0.00020191496017443598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7291 lambda_n: 0.9911111430618307 Loss: 0.00020191496017443603\n",
      "Iteration: 7292 lambda_n: 1.031188711268477 Loss: 0.00020191496017443609\n",
      "Iteration: 7293 lambda_n: 1.007857025390305 Loss: 0.000201914960174436\n",
      "Iteration: 7294 lambda_n: 0.9476705315289854 Loss: 0.00020191496017443603\n",
      "Iteration: 7295 lambda_n: 0.9671061147333545 Loss: 0.00020191496017443598\n",
      "Iteration: 7296 lambda_n: 1.0080598033720243 Loss: 0.00020191496017443603\n",
      "Iteration: 7297 lambda_n: 1.0106451137237848 Loss: 0.00020191496017443609\n",
      "Iteration: 7298 lambda_n: 1.00510888371148 Loss: 0.000201914960174436\n",
      "Iteration: 7299 lambda_n: 0.9572697823639519 Loss: 0.00020191496017443603\n",
      "Iteration: 7300 lambda_n: 0.9452431235341384 Loss: 0.00020191496017443598\n",
      "Iteration: 7301 lambda_n: 0.9665261870096022 Loss: 0.00020191496017443603\n",
      "Iteration: 7302 lambda_n: 0.8986653973140405 Loss: 0.00020191496017443609\n",
      "Iteration: 7303 lambda_n: 1.0363869185643313 Loss: 0.000201914960174436\n",
      "Iteration: 7304 lambda_n: 0.8852180747080723 Loss: 0.00020191496017443606\n",
      "Iteration: 7305 lambda_n: 1.0226668258572444 Loss: 0.00020191496017443598\n",
      "Iteration: 7306 lambda_n: 0.958303329635258 Loss: 0.00020191496017443603\n",
      "Iteration: 7307 lambda_n: 1.0275359851137922 Loss: 0.00020191496017443609\n",
      "Iteration: 7308 lambda_n: 1.0147794636892493 Loss: 0.000201914960174436\n",
      "Iteration: 7309 lambda_n: 0.9908474899521352 Loss: 0.00020191496017443606\n",
      "Iteration: 7310 lambda_n: 1.0329254880129708 Loss: 0.00020191496017443598\n",
      "Iteration: 7311 lambda_n: 0.9093970304708446 Loss: 0.00020191496017443603\n",
      "Iteration: 7312 lambda_n: 0.9505097415079913 Loss: 0.00020191496017443609\n",
      "Iteration: 7313 lambda_n: 1.0070521145034608 Loss: 0.000201914960174436\n",
      "Iteration: 7314 lambda_n: 0.8871583433567197 Loss: 0.00020191496017443603\n",
      "Iteration: 7315 lambda_n: 0.9663062878560535 Loss: 0.00020191496017443598\n",
      "Iteration: 7316 lambda_n: 1.0004790819234002 Loss: 0.00020191496017443603\n",
      "Iteration: 7317 lambda_n: 0.8839369031252962 Loss: 0.00020191496017443609\n",
      "Iteration: 7318 lambda_n: 0.9071152209013169 Loss: 0.000201914960174436\n",
      "Iteration: 7319 lambda_n: 0.9329931895202198 Loss: 0.00020191496017443606\n",
      "Iteration: 7320 lambda_n: 0.9069805466574733 Loss: 0.00020191496017443598\n",
      "Iteration: 7321 lambda_n: 0.9656026222480252 Loss: 0.00020191496017443603\n",
      "Iteration: 7322 lambda_n: 0.890909008650678 Loss: 0.00020191496017443609\n",
      "Iteration: 7323 lambda_n: 0.9725128227378412 Loss: 0.000201914960174436\n",
      "Iteration: 7324 lambda_n: 0.889011325107457 Loss: 0.00020191496017443606\n",
      "Iteration: 7325 lambda_n: 0.8949306870301252 Loss: 0.00020191496017443598\n",
      "Iteration: 7326 lambda_n: 0.9456198508898691 Loss: 0.00020191496017443603\n",
      "Iteration: 7327 lambda_n: 0.9384675288248003 Loss: 0.00020191496017443609\n",
      "Iteration: 7328 lambda_n: 0.9633131110580058 Loss: 0.000201914960174436\n",
      "Iteration: 7329 lambda_n: 1.0011051944154972 Loss: 0.00020191496017443606\n",
      "Iteration: 7330 lambda_n: 1.0377613992701062 Loss: 0.00020191496017443598\n",
      "Iteration: 7331 lambda_n: 0.9424012822158845 Loss: 0.00020191496017443603\n",
      "Iteration: 7332 lambda_n: 0.9449519936993792 Loss: 0.00020191496017443609\n",
      "Iteration: 7333 lambda_n: 0.9157020474129398 Loss: 0.000201914960174436\n",
      "Iteration: 7334 lambda_n: 0.9448442488459362 Loss: 0.00020191496017443603\n",
      "Iteration: 7335 lambda_n: 1.0013550816030525 Loss: 0.00020191496017443598\n",
      "Iteration: 7336 lambda_n: 0.950972354404107 Loss: 0.00020191496017443603\n",
      "Iteration: 7337 lambda_n: 0.999007463689821 Loss: 0.00020191496017443609\n",
      "Iteration: 7338 lambda_n: 0.9353305068452147 Loss: 0.000201914960174436\n",
      "Iteration: 7339 lambda_n: 0.9721401785802899 Loss: 0.00020191496017443603\n",
      "Iteration: 7340 lambda_n: 0.9672146420215824 Loss: 0.00020191496017443598\n",
      "Iteration: 7341 lambda_n: 0.8923818174067576 Loss: 0.00020191496017443603\n",
      "Iteration: 7342 lambda_n: 1.0328646916389774 Loss: 0.00020191496017443609\n",
      "Iteration: 7343 lambda_n: 0.9955870179784616 Loss: 0.000201914960174436\n",
      "Iteration: 7344 lambda_n: 0.9466076004548198 Loss: 0.00020191496017443603\n",
      "Iteration: 7345 lambda_n: 0.9119449253292563 Loss: 0.00020191496017443598\n",
      "Iteration: 7346 lambda_n: 0.9030306641332586 Loss: 0.00020191496017443603\n",
      "Iteration: 7347 lambda_n: 0.975907830374643 Loss: 0.00020191496017443609\n",
      "Iteration: 7348 lambda_n: 0.9753142193424346 Loss: 0.000201914960174436\n",
      "Iteration: 7349 lambda_n: 0.9217924635426571 Loss: 0.00020191496017443603\n",
      "Iteration: 7350 lambda_n: 0.9722175071057388 Loss: 0.00020191496017443598\n",
      "Iteration: 7351 lambda_n: 1.0045007467356781 Loss: 0.00020191496017443603\n",
      "Iteration: 7352 lambda_n: 0.9005115685125181 Loss: 0.00020191496017443609\n",
      "Iteration: 7353 lambda_n: 1.0116241025030959 Loss: 0.000201914960174436\n",
      "Iteration: 7354 lambda_n: 0.966076751559429 Loss: 0.00020191496017443606\n",
      "Iteration: 7355 lambda_n: 1.0354170377736112 Loss: 0.00020191496017443598\n",
      "Iteration: 7356 lambda_n: 1.0251833704478182 Loss: 0.00020191496017443603\n",
      "Iteration: 7357 lambda_n: 1.0083678580650404 Loss: 0.00020191496017443609\n",
      "Iteration: 7358 lambda_n: 0.9197261494385731 Loss: 0.000201914960174436\n",
      "Iteration: 7359 lambda_n: 1.0236465430620068 Loss: 0.00020191496017443603\n",
      "Iteration: 7360 lambda_n: 1.0072786673109675 Loss: 0.00020191496017443598\n",
      "Iteration: 7361 lambda_n: 0.9011339518864228 Loss: 0.00020191496017443603\n",
      "Iteration: 7362 lambda_n: 1.030659621536979 Loss: 0.00020191496017443609\n",
      "Iteration: 7363 lambda_n: 0.9410398829962059 Loss: 0.000201914960174436\n",
      "Iteration: 7364 lambda_n: 0.9133786520479154 Loss: 0.00020191496017443603\n",
      "Iteration: 7365 lambda_n: 0.917877176805223 Loss: 0.00020191496017443598\n",
      "Iteration: 7366 lambda_n: 1.0213632945670685 Loss: 0.00020191496017443603\n",
      "Iteration: 7367 lambda_n: 0.9518704104741678 Loss: 0.00020191496017443609\n",
      "Iteration: 7368 lambda_n: 0.9839853158216486 Loss: 0.000201914960174436\n",
      "Iteration: 7369 lambda_n: 0.9877664896143801 Loss: 0.00020191496017443603\n",
      "Iteration: 7370 lambda_n: 0.9030028269144742 Loss: 0.00020191496017443598\n",
      "Iteration: 7371 lambda_n: 0.8989108731804919 Loss: 0.00020191496017443603\n",
      "Iteration: 7372 lambda_n: 0.9268907557000519 Loss: 0.00020191496017443609\n",
      "Iteration: 7373 lambda_n: 0.9828608738133958 Loss: 0.000201914960174436\n",
      "Iteration: 7374 lambda_n: 0.9008940822045506 Loss: 0.00020191496017443603\n",
      "Iteration: 7375 lambda_n: 0.9370540681999554 Loss: 0.00020191496017443598\n",
      "Iteration: 7376 lambda_n: 1.00541919265174 Loss: 0.00020191496017443603\n",
      "Iteration: 7377 lambda_n: 0.8917727555768468 Loss: 0.00020191496017443609\n",
      "Iteration: 7378 lambda_n: 1.002206541037934 Loss: 0.000201914960174436\n",
      "Iteration: 7379 lambda_n: 0.9408843098859792 Loss: 0.00020191496017443606\n",
      "Iteration: 7380 lambda_n: 1.034538074298947 Loss: 0.00020191496017443598\n",
      "Iteration: 7381 lambda_n: 0.8834016620530879 Loss: 0.00020191496017443603\n",
      "Iteration: 7382 lambda_n: 0.9030955205638139 Loss: 0.00020191496017443609\n",
      "Iteration: 7383 lambda_n: 0.9516565543883609 Loss: 0.000201914960174436\n",
      "Iteration: 7384 lambda_n: 0.9487198112796167 Loss: 0.00020191496017443606\n",
      "Iteration: 7385 lambda_n: 0.9583391668360884 Loss: 0.00020191496017443598\n",
      "Iteration: 7386 lambda_n: 0.9057536442180129 Loss: 0.00020191496017443603\n",
      "Iteration: 7387 lambda_n: 0.9790182396801655 Loss: 0.00020191496017443609\n",
      "Iteration: 7388 lambda_n: 0.9558124466365151 Loss: 0.000201914960174436\n",
      "Iteration: 7389 lambda_n: 0.9254166455453982 Loss: 0.00020191496017443603\n",
      "Iteration: 7390 lambda_n: 0.9514505358913672 Loss: 0.00020191496017443598\n",
      "Iteration: 7391 lambda_n: 0.9079110064150239 Loss: 0.00020191496017443603\n",
      "Iteration: 7392 lambda_n: 1.004800071342977 Loss: 0.00020191496017443609\n",
      "Iteration: 7393 lambda_n: 0.8979622541186566 Loss: 0.000201914960174436\n",
      "Iteration: 7394 lambda_n: 0.9566466063888323 Loss: 0.00020191496017443603\n",
      "Iteration: 7395 lambda_n: 0.942572819991457 Loss: 0.00020191496017443598\n",
      "Iteration: 7396 lambda_n: 0.9533824300327084 Loss: 0.00020191496017443603\n",
      "Iteration: 7397 lambda_n: 0.9581620362605372 Loss: 0.00020191496017443609\n",
      "Iteration: 7398 lambda_n: 0.9156121537010944 Loss: 0.000201914960174436\n",
      "Iteration: 7399 lambda_n: 0.9595394088236978 Loss: 0.00020191496017443603\n",
      "Iteration: 7400 lambda_n: 1.019074577565995 Loss: 0.00020191496017443598\n",
      "Iteration: 7401 lambda_n: 0.9225189669197398 Loss: 0.00020191496017443603\n",
      "Iteration: 7402 lambda_n: 1.0370543605357918 Loss: 0.00020191496017443609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7403 lambda_n: 0.9172668537514962 Loss: 0.000201914960174436\n",
      "Iteration: 7404 lambda_n: 1.0228865986922977 Loss: 0.00020191496017443603\n",
      "Iteration: 7405 lambda_n: 1.0094101821855037 Loss: 0.00020191496017443598\n",
      "Iteration: 7406 lambda_n: 0.9723813187888071 Loss: 0.00020191496017443603\n",
      "Iteration: 7407 lambda_n: 1.032495880495023 Loss: 0.00020191496017443609\n",
      "Iteration: 7408 lambda_n: 0.9900278978812049 Loss: 0.000201914960174436\n",
      "Iteration: 7409 lambda_n: 1.0288956358884096 Loss: 0.00020191496017443603\n",
      "Iteration: 7410 lambda_n: 0.9980661324694228 Loss: 0.00020191496017443598\n",
      "Iteration: 7411 lambda_n: 0.8846257476249819 Loss: 0.00020191496017443603\n",
      "Iteration: 7412 lambda_n: 1.0211891921463307 Loss: 0.00020191496017443609\n",
      "Iteration: 7413 lambda_n: 0.8884229087186903 Loss: 0.000201914960174436\n",
      "Iteration: 7414 lambda_n: 0.9026742520209298 Loss: 0.00020191496017443603\n",
      "Iteration: 7415 lambda_n: 0.9669453414877789 Loss: 0.00020191496017443598\n",
      "Iteration: 7416 lambda_n: 0.9840676887833485 Loss: 0.00020191496017443603\n",
      "Iteration: 7417 lambda_n: 0.9221658816622452 Loss: 0.00020191496017443609\n",
      "Iteration: 7418 lambda_n: 0.9116932053719263 Loss: 0.000201914960174436\n",
      "Iteration: 7419 lambda_n: 0.9820285371859836 Loss: 0.00020191496017443603\n",
      "Iteration: 7420 lambda_n: 0.9524627785086902 Loss: 0.00020191496017443598\n",
      "Iteration: 7421 lambda_n: 1.0321868503950669 Loss: 0.00020191496017443603\n",
      "Iteration: 7422 lambda_n: 1.037278608930519 Loss: 0.00020191496017443609\n",
      "Iteration: 7423 lambda_n: 1.022272620646916 Loss: 0.000201914960174436\n",
      "Iteration: 7424 lambda_n: 1.0130219893817323 Loss: 0.00020191496017443603\n",
      "Iteration: 7425 lambda_n: 0.9888793752846508 Loss: 0.00020191496017443598\n",
      "Iteration: 7426 lambda_n: 0.9246050403518462 Loss: 0.00020191496017443603\n",
      "Iteration: 7427 lambda_n: 0.9838786383654504 Loss: 0.00020191496017443609\n",
      "Iteration: 7428 lambda_n: 1.0152052100831679 Loss: 0.000201914960174436\n",
      "Iteration: 7429 lambda_n: 0.9914105955265673 Loss: 0.00020191496017443603\n",
      "Iteration: 7430 lambda_n: 0.8856922755339751 Loss: 0.00020191496017443598\n",
      "Iteration: 7431 lambda_n: 0.949584064666678 Loss: 0.00020191496017443603\n",
      "Iteration: 7432 lambda_n: 0.9724341429468188 Loss: 0.00020191496017443609\n",
      "Iteration: 7433 lambda_n: 1.0104439991473129 Loss: 0.000201914960174436\n",
      "Iteration: 7434 lambda_n: 0.974032838301836 Loss: 0.00020191496017443603\n",
      "Iteration: 7435 lambda_n: 0.9151147626842726 Loss: 0.00020191496017443598\n",
      "Iteration: 7436 lambda_n: 0.8978899221386056 Loss: 0.00020191496017443603\n",
      "Iteration: 7437 lambda_n: 0.9252283805644431 Loss: 0.00020191496017443609\n",
      "Iteration: 7438 lambda_n: 0.9407356054868872 Loss: 0.000201914960174436\n",
      "Iteration: 7439 lambda_n: 0.9486061693628416 Loss: 0.00020191496017443603\n",
      "Iteration: 7440 lambda_n: 0.9778645313722252 Loss: 0.00020191496017443598\n",
      "Iteration: 7441 lambda_n: 0.9791926284286161 Loss: 0.00020191496017443603\n",
      "Iteration: 7442 lambda_n: 0.8893461157117976 Loss: 0.00020191496017443609\n",
      "Iteration: 7443 lambda_n: 0.889060153356146 Loss: 0.000201914960174436\n",
      "Iteration: 7444 lambda_n: 1.0057462870055636 Loss: 0.00020191496017443606\n",
      "Iteration: 7445 lambda_n: 0.995799303989329 Loss: 0.00020191496017443598\n",
      "Iteration: 7446 lambda_n: 1.0250282227041119 Loss: 0.00020191496017443603\n",
      "Iteration: 7447 lambda_n: 0.925671790010008 Loss: 0.00020191496017443609\n",
      "Iteration: 7448 lambda_n: 0.9703593736691938 Loss: 0.000201914960174436\n",
      "Iteration: 7449 lambda_n: 0.9684695578324218 Loss: 0.00020191496017443603\n",
      "Iteration: 7450 lambda_n: 0.8980368579890537 Loss: 0.00020191496017443598\n",
      "Iteration: 7451 lambda_n: 1.001451698720649 Loss: 0.00020191496017443603\n",
      "Iteration: 7452 lambda_n: 0.9259735420771678 Loss: 0.00020191496017443609\n",
      "Iteration: 7453 lambda_n: 0.9435243604697559 Loss: 0.000201914960174436\n",
      "Iteration: 7454 lambda_n: 0.937811938718142 Loss: 0.00020191496017443603\n",
      "Iteration: 7455 lambda_n: 0.950567128798578 Loss: 0.00020191496017443598\n",
      "Iteration: 7456 lambda_n: 0.9828640829176687 Loss: 0.00020191496017443603\n",
      "Iteration: 7457 lambda_n: 0.9586301584354097 Loss: 0.00020191496017443609\n",
      "Iteration: 7458 lambda_n: 0.9889994505813919 Loss: 0.000201914960174436\n",
      "Iteration: 7459 lambda_n: 0.9366026746888403 Loss: 0.00020191496017443603\n",
      "Iteration: 7460 lambda_n: 0.9965510023513817 Loss: 0.00020191496017443598\n",
      "Iteration: 7461 lambda_n: 0.9281694894969873 Loss: 0.00020191496017443603\n",
      "Iteration: 7462 lambda_n: 0.99997181596685 Loss: 0.00020191496017443609\n",
      "Iteration: 7463 lambda_n: 0.9876342863806873 Loss: 0.000201914960174436\n",
      "Iteration: 7464 lambda_n: 0.8974106393659326 Loss: 0.00020191496017443603\n",
      "Iteration: 7465 lambda_n: 0.9646999079880434 Loss: 0.00020191496017443598\n",
      "Iteration: 7466 lambda_n: 1.02600203437145 Loss: 0.00020191496017443603\n",
      "Iteration: 7467 lambda_n: 0.9248173900008327 Loss: 0.00020191496017443609\n",
      "Iteration: 7468 lambda_n: 0.924828125799242 Loss: 0.000201914960174436\n",
      "Iteration: 7469 lambda_n: 0.9075191949458078 Loss: 0.00020191496017443603\n",
      "Iteration: 7470 lambda_n: 0.9779260533658855 Loss: 0.00020191496017443598\n",
      "Iteration: 7471 lambda_n: 0.924421493805005 Loss: 0.00020191496017443603\n",
      "Iteration: 7472 lambda_n: 0.9739013562505399 Loss: 0.00020191496017443609\n",
      "Iteration: 7473 lambda_n: 0.969887837597201 Loss: 0.000201914960174436\n",
      "Iteration: 7474 lambda_n: 0.9932112435153084 Loss: 0.00020191496017443603\n",
      "Iteration: 7475 lambda_n: 0.9680992895735308 Loss: 0.00020191496017443598\n",
      "Iteration: 7476 lambda_n: 0.9173622463018439 Loss: 0.00020191496017443603\n",
      "Iteration: 7477 lambda_n: 1.028218314035483 Loss: 0.00020191496017443609\n",
      "Iteration: 7478 lambda_n: 0.9121412188666349 Loss: 0.000201914960174436\n",
      "Iteration: 7479 lambda_n: 0.9708726750357827 Loss: 0.00020191496017443603\n",
      "Iteration: 7480 lambda_n: 0.9341028606830415 Loss: 0.00020191496017443598\n",
      "Iteration: 7481 lambda_n: 0.9358644146445069 Loss: 0.00020191496017443603\n",
      "Iteration: 7482 lambda_n: 0.9608181803019916 Loss: 0.00020191496017443609\n",
      "Iteration: 7483 lambda_n: 0.8886809478570442 Loss: 0.000201914960174436\n",
      "Iteration: 7484 lambda_n: 0.9164087368245885 Loss: 0.00020191496017443603\n",
      "Iteration: 7485 lambda_n: 0.9141910378185107 Loss: 0.00020191496017443598\n",
      "Iteration: 7486 lambda_n: 0.9823165510633248 Loss: 0.00020191496017443603\n",
      "Iteration: 7487 lambda_n: 0.9461520096516672 Loss: 0.00020191496017443609\n",
      "Iteration: 7488 lambda_n: 0.9363018855841782 Loss: 0.000201914960174436\n",
      "Iteration: 7489 lambda_n: 0.9570544509178087 Loss: 0.00020191496017443603\n",
      "Iteration: 7490 lambda_n: 0.8981939982968716 Loss: 0.00020191496017443598\n",
      "Iteration: 7491 lambda_n: 1.0077861727298085 Loss: 0.00020191496017443603\n",
      "Iteration: 7492 lambda_n: 0.8887723384088966 Loss: 0.00020191496017443609\n",
      "Iteration: 7493 lambda_n: 0.9825721128742043 Loss: 0.000201914960174436\n",
      "Iteration: 7494 lambda_n: 0.9762800018768918 Loss: 0.00020191496017443606\n",
      "Iteration: 7495 lambda_n: 0.9649103823907064 Loss: 0.00020191496017443598\n",
      "Iteration: 7496 lambda_n: 0.9506521302653105 Loss: 0.00020191496017443603\n",
      "Iteration: 7497 lambda_n: 0.9439011024120453 Loss: 0.00020191496017443609\n",
      "Iteration: 7498 lambda_n: 1.018311979235295 Loss: 0.000201914960174436\n",
      "Iteration: 7499 lambda_n: 0.9831587796089781 Loss: 0.00020191496017443603\n",
      "Iteration: 7500 lambda_n: 0.9275567585028066 Loss: 0.00020191496017443598\n",
      "Iteration: 7501 lambda_n: 0.8971037296409208 Loss: 0.00020191496017443603\n",
      "Iteration: 7502 lambda_n: 0.91187974766574 Loss: 0.00020191496017443609\n",
      "Iteration: 7503 lambda_n: 0.8970220044370577 Loss: 0.000201914960174436\n",
      "Iteration: 7504 lambda_n: 0.9910126044510194 Loss: 0.00020191496017443606\n",
      "Iteration: 7505 lambda_n: 1.013851189530752 Loss: 0.00020191496017443598\n",
      "Iteration: 7506 lambda_n: 0.9366924121854703 Loss: 0.00020191496017443603\n",
      "Iteration: 7507 lambda_n: 0.9505257818344818 Loss: 0.00020191496017443609\n",
      "Iteration: 7508 lambda_n: 0.9642684419403161 Loss: 0.000201914960174436\n",
      "Iteration: 7509 lambda_n: 1.0366954308513263 Loss: 0.00020191496017443603\n",
      "Iteration: 7510 lambda_n: 0.995258138397846 Loss: 0.00020191496017443598\n",
      "Iteration: 7511 lambda_n: 1.0203632121112018 Loss: 0.00020191496017443603\n",
      "Iteration: 7512 lambda_n: 0.9717354333987908 Loss: 0.00020191496017443609\n",
      "Iteration: 7513 lambda_n: 0.9119786591134619 Loss: 0.000201914960174436\n",
      "Iteration: 7514 lambda_n: 0.9684714323552699 Loss: 0.00020191496017443603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7515 lambda_n: 0.9323544392571053 Loss: 0.00020191496017443598\n",
      "Iteration: 7516 lambda_n: 0.9784545992992338 Loss: 0.00020191496017443603\n",
      "Iteration: 7517 lambda_n: 0.9401954946606238 Loss: 0.00020191496017443609\n",
      "Iteration: 7518 lambda_n: 1.0127927801763834 Loss: 0.000201914960174436\n",
      "Iteration: 7519 lambda_n: 0.9601964649398037 Loss: 0.00020191496017443603\n",
      "Iteration: 7520 lambda_n: 1.016767035767453 Loss: 0.00020191496017443598\n",
      "Iteration: 7521 lambda_n: 1.014359716768899 Loss: 0.00020191496017443603\n",
      "Iteration: 7522 lambda_n: 0.9771688196890576 Loss: 0.00020191496017443609\n",
      "Iteration: 7523 lambda_n: 0.9693144809562503 Loss: 0.000201914960174436\n",
      "Iteration: 7524 lambda_n: 0.9972327599419378 Loss: 0.00020191496017443603\n",
      "Iteration: 7525 lambda_n: 1.033467137070302 Loss: 0.00020191496017443598\n",
      "Iteration: 7526 lambda_n: 0.9005049281387212 Loss: 0.00020191496017443603\n",
      "Iteration: 7527 lambda_n: 0.9643869154581873 Loss: 0.00020191496017443609\n",
      "Iteration: 7528 lambda_n: 0.9341498433958466 Loss: 0.000201914960174436\n",
      "Iteration: 7529 lambda_n: 0.9779608699869793 Loss: 0.00020191496017443603\n",
      "Iteration: 7530 lambda_n: 1.0256865050953257 Loss: 0.00020191496017443598\n",
      "Iteration: 7531 lambda_n: 0.9900884149064035 Loss: 0.00020191496017443603\n",
      "Iteration: 7532 lambda_n: 0.9967961663428178 Loss: 0.00020191496017443609\n",
      "Iteration: 7533 lambda_n: 0.9654222579777161 Loss: 0.000201914960174436\n",
      "Iteration: 7534 lambda_n: 0.9811849138984886 Loss: 0.00020191496017443603\n",
      "Iteration: 7535 lambda_n: 0.9697322906419731 Loss: 0.00020191496017443598\n",
      "Iteration: 7536 lambda_n: 1.0314875620414554 Loss: 0.00020191496017443603\n",
      "Iteration: 7537 lambda_n: 0.9948677676964695 Loss: 0.00020191496017443609\n",
      "Iteration: 7538 lambda_n: 0.9385996263918062 Loss: 0.000201914960174436\n",
      "Iteration: 7539 lambda_n: 1.0333092675417834 Loss: 0.00020191496017443603\n",
      "Iteration: 7540 lambda_n: 1.0134369644079524 Loss: 0.00020191496017443598\n",
      "Iteration: 7541 lambda_n: 0.9527486422562138 Loss: 0.00020191496017443603\n",
      "Iteration: 7542 lambda_n: 0.9690121594066259 Loss: 0.00020191496017443609\n",
      "Iteration: 7543 lambda_n: 0.9211755725208596 Loss: 0.000201914960174436\n",
      "Iteration: 7544 lambda_n: 1.0209917196445992 Loss: 0.00020191496017443603\n",
      "Iteration: 7545 lambda_n: 0.9019637939270577 Loss: 0.00020191496017443598\n",
      "Iteration: 7546 lambda_n: 1.0037287619391784 Loss: 0.00020191496017443603\n",
      "Iteration: 7547 lambda_n: 1.0140848326294236 Loss: 0.00020191496017443609\n",
      "Iteration: 7548 lambda_n: 0.9807766685373221 Loss: 0.000201914960174436\n",
      "Iteration: 7549 lambda_n: 1.017872731919965 Loss: 0.00020191496017443603\n",
      "Iteration: 7550 lambda_n: 0.9818718892678969 Loss: 0.00020191496017443598\n",
      "Iteration: 7551 lambda_n: 0.9021082824045192 Loss: 0.00020191496017443603\n",
      "Iteration: 7552 lambda_n: 1.035267995703965 Loss: 0.00020191496017443609\n",
      "Iteration: 7553 lambda_n: 0.930888402976092 Loss: 0.000201914960174436\n",
      "Iteration: 7554 lambda_n: 0.9798192097255 Loss: 0.00020191496017443603\n",
      "Iteration: 7555 lambda_n: 0.9055888689195022 Loss: 0.00020191496017443598\n",
      "Iteration: 7556 lambda_n: 1.019181382507814 Loss: 0.00020191496017443603\n",
      "Iteration: 7557 lambda_n: 0.9371824708832097 Loss: 0.00020191496017443609\n",
      "Iteration: 7558 lambda_n: 0.9123310028551516 Loss: 0.000201914960174436\n",
      "Iteration: 7559 lambda_n: 0.9239074154123331 Loss: 0.00020191496017443603\n",
      "Iteration: 7560 lambda_n: 0.8948688169864044 Loss: 0.00020191496017443598\n",
      "Iteration: 7561 lambda_n: 0.9958851431176563 Loss: 0.00020191496017443603\n",
      "Iteration: 7562 lambda_n: 1.029866345976001 Loss: 0.00020191496017443609\n",
      "Iteration: 7563 lambda_n: 0.8884778196622921 Loss: 0.000201914960174436\n",
      "Iteration: 7564 lambda_n: 0.9288886877585893 Loss: 0.00020191496017443603\n",
      "Iteration: 7565 lambda_n: 0.9982039096482354 Loss: 0.00020191496017443598\n",
      "Iteration: 7566 lambda_n: 0.9087760703367203 Loss: 0.00020191496017443603\n",
      "Iteration: 7567 lambda_n: 1.0360797171284082 Loss: 0.00020191496017443609\n",
      "Iteration: 7568 lambda_n: 0.9702884924197808 Loss: 0.000201914960174436\n",
      "Iteration: 7569 lambda_n: 0.8863964867251093 Loss: 0.00020191496017443603\n",
      "Iteration: 7570 lambda_n: 0.9334051178232919 Loss: 0.00020191496017443598\n",
      "Iteration: 7571 lambda_n: 0.9867630681104702 Loss: 0.00020191496017443603\n",
      "Iteration: 7572 lambda_n: 0.9017232665943257 Loss: 0.00020191496017443609\n",
      "Iteration: 7573 lambda_n: 0.9349716845749143 Loss: 0.000201914960174436\n",
      "Iteration: 7574 lambda_n: 0.9067006211446974 Loss: 0.00020191496017443606\n",
      "Iteration: 7575 lambda_n: 0.956790823922839 Loss: 0.00020191496017443598\n",
      "Iteration: 7576 lambda_n: 0.9978889087700218 Loss: 0.00020191496017443603\n",
      "Iteration: 7577 lambda_n: 0.9558456084647761 Loss: 0.00020191496017443609\n",
      "Iteration: 7578 lambda_n: 1.0091934960425526 Loss: 0.000201914960174436\n",
      "Iteration: 7579 lambda_n: 1.015215102835737 Loss: 0.00020191496017443606\n",
      "Iteration: 7580 lambda_n: 0.9579277725396667 Loss: 0.00020191496017443598\n",
      "Iteration: 7581 lambda_n: 1.0034681875153924 Loss: 0.00020191496017443603\n",
      "Iteration: 7582 lambda_n: 1.0374595608968942 Loss: 0.00020191496017443609\n",
      "Iteration: 7583 lambda_n: 0.8920265047028866 Loss: 0.000201914960174436\n",
      "Iteration: 7584 lambda_n: 1.0111973119034248 Loss: 0.00020191496017443603\n",
      "Iteration: 7585 lambda_n: 1.030743797598662 Loss: 0.00020191496017443598\n",
      "Iteration: 7586 lambda_n: 0.8933683357479711 Loss: 0.00020191496017443603\n",
      "Iteration: 7587 lambda_n: 0.9944122648100907 Loss: 0.00020191496017443609\n",
      "Iteration: 7588 lambda_n: 0.9563533285738601 Loss: 0.000201914960174436\n",
      "Iteration: 7589 lambda_n: 0.9085499113952875 Loss: 0.00020191496017443603\n",
      "Iteration: 7590 lambda_n: 0.9215242016211592 Loss: 0.00020191496017443598\n",
      "Iteration: 7591 lambda_n: 0.9159224714517398 Loss: 0.00020191496017443603\n",
      "Iteration: 7592 lambda_n: 1.0284879680191756 Loss: 0.00020191496017443609\n",
      "Iteration: 7593 lambda_n: 0.9373540777227839 Loss: 0.000201914960174436\n",
      "Iteration: 7594 lambda_n: 0.9580071848848855 Loss: 0.00020191496017443603\n",
      "Iteration: 7595 lambda_n: 0.9768467520862485 Loss: 0.00020191496017443598\n",
      "Iteration: 7596 lambda_n: 0.9253710867663552 Loss: 0.00020191496017443603\n",
      "Iteration: 7597 lambda_n: 0.9364824754598222 Loss: 0.00020191496017443609\n",
      "Iteration: 7598 lambda_n: 0.9253479997490839 Loss: 0.000201914960174436\n",
      "Iteration: 7599 lambda_n: 0.9096519784976983 Loss: 0.00020191496017443603\n",
      "Iteration: 7600 lambda_n: 0.9623196067690899 Loss: 0.00020191496017443598\n",
      "Iteration: 7601 lambda_n: 0.912827029317936 Loss: 0.00020191496017443603\n",
      "Iteration: 7602 lambda_n: 1.0191042280815201 Loss: 0.00020191496017443609\n",
      "Iteration: 7603 lambda_n: 0.893558895278307 Loss: 0.000201914960174436\n",
      "Iteration: 7604 lambda_n: 1.0294894422447924 Loss: 0.00020191496017443603\n",
      "Iteration: 7605 lambda_n: 0.9654862538689287 Loss: 0.00020191496017443598\n",
      "Iteration: 7606 lambda_n: 0.986481107643732 Loss: 0.00020191496017443603\n",
      "Iteration: 7607 lambda_n: 0.9013807030164736 Loss: 0.00020191496017443609\n",
      "Iteration: 7608 lambda_n: 0.9751879939541928 Loss: 0.000201914960174436\n",
      "Iteration: 7609 lambda_n: 0.8940369972953384 Loss: 0.00020191496017443606\n",
      "Iteration: 7610 lambda_n: 1.0186573141365998 Loss: 0.00020191496017443598\n",
      "Iteration: 7611 lambda_n: 1.0031896377564726 Loss: 0.00020191496017443603\n",
      "Iteration: 7612 lambda_n: 0.9031515577607007 Loss: 0.00020191496017443609\n",
      "Iteration: 7613 lambda_n: 1.0381696806272271 Loss: 0.000201914960174436\n",
      "Iteration: 7614 lambda_n: 0.9915712492472823 Loss: 0.00020191496017443606\n",
      "Iteration: 7615 lambda_n: 0.8907470624599317 Loss: 0.00020191496017443598\n",
      "Iteration: 7616 lambda_n: 0.9595456316027108 Loss: 0.00020191496017443603\n",
      "Iteration: 7617 lambda_n: 0.9930567457210253 Loss: 0.00020191496017443609\n",
      "Iteration: 7618 lambda_n: 0.8949387431268757 Loss: 0.000201914960174436\n",
      "Iteration: 7619 lambda_n: 0.9343779178216445 Loss: 0.00020191496017443606\n",
      "Iteration: 7620 lambda_n: 0.9040860339111743 Loss: 0.00020191496017443598\n",
      "Iteration: 7621 lambda_n: 0.9543479390989756 Loss: 0.00020191496017443603\n",
      "Iteration: 7622 lambda_n: 0.9296397534823456 Loss: 0.00020191496017443609\n",
      "Iteration: 7623 lambda_n: 1.0285256853737654 Loss: 0.000201914960174436\n",
      "Iteration: 7624 lambda_n: 0.9346538715274606 Loss: 0.00020191496017443603\n",
      "Iteration: 7625 lambda_n: 0.9442512845901004 Loss: 0.00020191496017443598\n",
      "Iteration: 7626 lambda_n: 1.0320170316609194 Loss: 0.00020191496017443603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7627 lambda_n: 0.9351061702800392 Loss: 0.00020191496017443609\n",
      "Iteration: 7628 lambda_n: 0.9910303825466447 Loss: 0.000201914960174436\n",
      "Iteration: 7629 lambda_n: 1.0069815069054535 Loss: 0.00020191496017443603\n",
      "Iteration: 7630 lambda_n: 0.953249251349233 Loss: 0.00020191496017443598\n",
      "Iteration: 7631 lambda_n: 1.0056716216562926 Loss: 0.00020191496017443603\n",
      "Iteration: 7632 lambda_n: 0.9328337410184733 Loss: 0.00020191496017443609\n",
      "Iteration: 7633 lambda_n: 0.9222073150199991 Loss: 0.000201914960174436\n",
      "Iteration: 7634 lambda_n: 0.8901598770149188 Loss: 0.00020191496017443603\n",
      "Iteration: 7635 lambda_n: 0.9479328059084542 Loss: 0.00020191496017443598\n",
      "Iteration: 7636 lambda_n: 0.969546507257818 Loss: 0.00020191496017443603\n",
      "Iteration: 7637 lambda_n: 1.007927156269482 Loss: 0.00020191496017443609\n",
      "Iteration: 7638 lambda_n: 0.929943907741488 Loss: 0.000201914960174436\n",
      "Iteration: 7639 lambda_n: 0.9998004570221173 Loss: 0.00020191496017443603\n",
      "Iteration: 7640 lambda_n: 0.8855658169273974 Loss: 0.00020191496017443598\n",
      "Iteration: 7641 lambda_n: 0.8923698538345254 Loss: 0.00020191496017443603\n",
      "Iteration: 7642 lambda_n: 0.9199247589677704 Loss: 0.00020191496017443609\n",
      "Iteration: 7643 lambda_n: 0.9803978472748999 Loss: 0.000201914960174436\n",
      "Iteration: 7644 lambda_n: 1.0271529462107705 Loss: 0.00020191496017443603\n",
      "Iteration: 7645 lambda_n: 0.9819863238988426 Loss: 0.00020191496017443598\n",
      "Iteration: 7646 lambda_n: 0.8837593182307233 Loss: 0.00020191496017443603\n",
      "Iteration: 7647 lambda_n: 1.0358112743634806 Loss: 0.00020191496017443609\n",
      "Iteration: 7648 lambda_n: 0.9047250020515052 Loss: 0.000201914960174436\n",
      "Iteration: 7649 lambda_n: 0.9897178264308196 Loss: 0.00020191496017443603\n",
      "Iteration: 7650 lambda_n: 0.8859675801924892 Loss: 0.00020191496017443598\n",
      "Iteration: 7651 lambda_n: 0.9434458992815846 Loss: 0.00020191496017443603\n",
      "Iteration: 7652 lambda_n: 0.9131167660939125 Loss: 0.00020191496017443609\n",
      "Iteration: 7653 lambda_n: 0.9969325825141088 Loss: 0.000201914960174436\n",
      "Iteration: 7654 lambda_n: 0.9704120435907612 Loss: 0.00020191496017443606\n",
      "Iteration: 7655 lambda_n: 0.9714663796144403 Loss: 0.00020191496017443598\n",
      "Iteration: 7656 lambda_n: 0.9766887573306818 Loss: 0.00020191496017443603\n",
      "Iteration: 7657 lambda_n: 0.9893777063854593 Loss: 0.00020191496017443609\n",
      "Iteration: 7658 lambda_n: 0.9042037860528007 Loss: 0.000201914960174436\n",
      "Iteration: 7659 lambda_n: 0.912057813414197 Loss: 0.00020191496017443603\n",
      "Iteration: 7660 lambda_n: 1.0312093657149897 Loss: 0.00020191496017443598\n",
      "Iteration: 7661 lambda_n: 1.0032778805620965 Loss: 0.00020191496017443603\n",
      "Iteration: 7662 lambda_n: 0.9928134870638268 Loss: 0.00020191496017443609\n",
      "Iteration: 7663 lambda_n: 0.909152056113532 Loss: 0.000201914960174436\n",
      "Iteration: 7664 lambda_n: 0.8985017342086594 Loss: 0.00020191496017443603\n",
      "Iteration: 7665 lambda_n: 0.8956505757366919 Loss: 0.00020191496017443598\n",
      "Iteration: 7666 lambda_n: 0.9556469850674987 Loss: 0.00020191496017443603\n",
      "Iteration: 7667 lambda_n: 0.9060368178205548 Loss: 0.00020191496017443609\n",
      "Iteration: 7668 lambda_n: 0.9045529219217113 Loss: 0.000201914960174436\n",
      "Iteration: 7669 lambda_n: 0.9235508665085043 Loss: 0.00020191496017443606\n",
      "Iteration: 7670 lambda_n: 0.9958739838381672 Loss: 0.00020191496017443598\n",
      "Iteration: 7671 lambda_n: 0.9164077576659388 Loss: 0.00020191496017443603\n",
      "Iteration: 7672 lambda_n: 0.9812076323314549 Loss: 0.00020191496017443609\n",
      "Iteration: 7673 lambda_n: 0.9616359352315917 Loss: 0.000201914960174436\n",
      "Iteration: 7674 lambda_n: 0.9504715500022201 Loss: 0.00020191496017443603\n",
      "Iteration: 7675 lambda_n: 0.9886596814170165 Loss: 0.00020191496017443598\n",
      "Iteration: 7676 lambda_n: 0.987013754758218 Loss: 0.00020191496017443603\n",
      "Iteration: 7677 lambda_n: 1.018327780383299 Loss: 0.00020191496017443609\n",
      "Iteration: 7678 lambda_n: 0.9456351778439516 Loss: 0.000201914960174436\n",
      "Iteration: 7679 lambda_n: 0.9620170883394062 Loss: 0.00020191496017443603\n",
      "Iteration: 7680 lambda_n: 1.0375656798893214 Loss: 0.00020191496017443598\n",
      "Iteration: 7681 lambda_n: 1.0255249281327579 Loss: 0.00020191496017443603\n",
      "Iteration: 7682 lambda_n: 0.9249546349711512 Loss: 0.00020191496017443609\n",
      "Iteration: 7683 lambda_n: 0.9762910446067679 Loss: 0.000201914960174436\n",
      "Iteration: 7684 lambda_n: 0.9795454910388567 Loss: 0.00020191496017443603\n",
      "Iteration: 7685 lambda_n: 0.9474280306179216 Loss: 0.00020191496017443598\n",
      "Iteration: 7686 lambda_n: 0.9978342020070061 Loss: 0.00020191496017443603\n",
      "Iteration: 7687 lambda_n: 0.9422937147683497 Loss: 0.00020191496017443609\n",
      "Iteration: 7688 lambda_n: 0.9983255548127704 Loss: 0.000201914960174436\n",
      "Iteration: 7689 lambda_n: 0.9194806597801951 Loss: 0.00020191496017443603\n",
      "Iteration: 7690 lambda_n: 0.9183549040948765 Loss: 0.00020191496017443598\n",
      "Iteration: 7691 lambda_n: 0.9054224973153424 Loss: 0.00020191496017443603\n",
      "Iteration: 7692 lambda_n: 1.009925111798667 Loss: 0.00020191496017443609\n",
      "Iteration: 7693 lambda_n: 0.9903024413984387 Loss: 0.000201914960174436\n",
      "Iteration: 7694 lambda_n: 0.9161680317481419 Loss: 0.00020191496017443603\n",
      "Iteration: 7695 lambda_n: 0.9762377913586002 Loss: 0.00020191496017443598\n",
      "Iteration: 7696 lambda_n: 1.0118566049481441 Loss: 0.00020191496017443603\n",
      "Iteration: 7697 lambda_n: 1.0173396029046535 Loss: 0.00020191496017443609\n",
      "Iteration: 7698 lambda_n: 0.9636606567502279 Loss: 0.000201914960174436\n",
      "Iteration: 7699 lambda_n: 1.028397678520286 Loss: 0.00020191496017443603\n",
      "Iteration: 7700 lambda_n: 0.9020482210078669 Loss: 0.00020191496017443598\n",
      "Iteration: 7701 lambda_n: 0.9583292809606067 Loss: 0.00020191496017443603\n",
      "Iteration: 7702 lambda_n: 0.9948524388368122 Loss: 0.00020191496017443609\n",
      "Iteration: 7703 lambda_n: 0.9425133812636821 Loss: 0.000201914960174436\n",
      "Iteration: 7704 lambda_n: 0.9739371119422873 Loss: 0.00020191496017443603\n",
      "Iteration: 7705 lambda_n: 0.9808504082108095 Loss: 0.00020191496017443598\n",
      "Iteration: 7706 lambda_n: 0.9774550698050258 Loss: 0.00020191496017443603\n",
      "Iteration: 7707 lambda_n: 0.8918449422988771 Loss: 0.00020191496017443609\n",
      "Iteration: 7708 lambda_n: 0.9036580987643354 Loss: 0.000201914960174436\n",
      "Iteration: 7709 lambda_n: 0.939495509903712 Loss: 0.00020191496017443606\n",
      "Iteration: 7710 lambda_n: 0.9342982449733501 Loss: 0.00020191496017443598\n",
      "Iteration: 7711 lambda_n: 1.0349341013017186 Loss: 0.00020191496017443603\n",
      "Iteration: 7712 lambda_n: 1.03196983079207 Loss: 0.00020191496017443609\n",
      "Iteration: 7713 lambda_n: 1.0184465073519793 Loss: 0.000201914960174436\n",
      "Iteration: 7714 lambda_n: 0.8876142132347791 Loss: 0.00020191496017443603\n",
      "Iteration: 7715 lambda_n: 0.9700753506554016 Loss: 0.00020191496017443598\n",
      "Iteration: 7716 lambda_n: 1.0294090401659108 Loss: 0.00020191496017443603\n",
      "Iteration: 7717 lambda_n: 0.9727100386830201 Loss: 0.00020191496017443609\n",
      "Iteration: 7718 lambda_n: 0.9457641802926287 Loss: 0.000201914960174436\n",
      "Iteration: 7719 lambda_n: 1.0321543082355953 Loss: 0.00020191496017443603\n",
      "Iteration: 7720 lambda_n: 0.9354876290589548 Loss: 0.00020191496017443598\n",
      "Iteration: 7721 lambda_n: 0.9503467478164832 Loss: 0.00020191496017443603\n",
      "Iteration: 7722 lambda_n: 0.9556205550880551 Loss: 0.00020191496017443609\n",
      "Iteration: 7723 lambda_n: 0.9881059099135525 Loss: 0.000201914960174436\n",
      "Iteration: 7724 lambda_n: 1.02931150891017 Loss: 0.00020191496017443603\n",
      "Iteration: 7725 lambda_n: 0.8889966918638648 Loss: 0.00020191496017443598\n",
      "Iteration: 7726 lambda_n: 0.9925643268555735 Loss: 0.00020191496017443603\n",
      "Iteration: 7727 lambda_n: 0.8918046934686938 Loss: 0.00020191496017443609\n",
      "Iteration: 7728 lambda_n: 0.984168282636426 Loss: 0.000201914960174436\n",
      "Iteration: 7729 lambda_n: 0.9319695658052944 Loss: 0.00020191496017443606\n",
      "Iteration: 7730 lambda_n: 0.9294013765996988 Loss: 0.00020191496017443598\n",
      "Iteration: 7731 lambda_n: 1.028114322639803 Loss: 0.00020191496017443603\n",
      "Iteration: 7732 lambda_n: 1.0337056332656247 Loss: 0.00020191496017443609\n",
      "Iteration: 7733 lambda_n: 0.8989479156637872 Loss: 0.000201914960174436\n",
      "Iteration: 7734 lambda_n: 0.9993572567405677 Loss: 0.00020191496017443603\n",
      "Iteration: 7735 lambda_n: 1.0232653293066205 Loss: 0.00020191496017443598\n",
      "Iteration: 7736 lambda_n: 1.036852258777007 Loss: 0.00020191496017443603\n",
      "Iteration: 7737 lambda_n: 0.9567812425637829 Loss: 0.00020191496017443609\n",
      "Iteration: 7738 lambda_n: 0.976146073555793 Loss: 0.000201914960174436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7739 lambda_n: 0.9947832291527702 Loss: 0.00020191496017443603\n",
      "Iteration: 7740 lambda_n: 1.0025429196578184 Loss: 0.00020191496017443598\n",
      "Iteration: 7741 lambda_n: 0.948393430658427 Loss: 0.00020191496017443603\n",
      "Iteration: 7742 lambda_n: 0.9031409040049727 Loss: 0.00020191496017443609\n",
      "Iteration: 7743 lambda_n: 0.9157183797292131 Loss: 0.000201914960174436\n",
      "Iteration: 7744 lambda_n: 0.8996827846560047 Loss: 0.00020191496017443606\n",
      "Iteration: 7745 lambda_n: 0.8969356566056412 Loss: 0.00020191496017443598\n",
      "Iteration: 7746 lambda_n: 1.0357229874493235 Loss: 0.00020191496017443603\n",
      "Iteration: 7747 lambda_n: 1.0180667018140626 Loss: 0.00020191496017443609\n",
      "Iteration: 7748 lambda_n: 0.960943497588621 Loss: 0.000201914960174436\n",
      "Iteration: 7749 lambda_n: 1.031358897828603 Loss: 0.00020191496017443606\n",
      "Iteration: 7750 lambda_n: 1.0048397981339325 Loss: 0.00020191496017443598\n",
      "Iteration: 7751 lambda_n: 0.9820522423532863 Loss: 0.00020191496017443603\n",
      "Iteration: 7752 lambda_n: 0.9959282781229584 Loss: 0.00020191496017443609\n",
      "Iteration: 7753 lambda_n: 0.9420663004809805 Loss: 0.000201914960174436\n",
      "Iteration: 7754 lambda_n: 0.9549309147806148 Loss: 0.00020191496017443603\n",
      "Iteration: 7755 lambda_n: 0.9307337884000983 Loss: 0.00020191496017443598\n",
      "Iteration: 7756 lambda_n: 0.9538397642775741 Loss: 0.00020191496017443603\n",
      "Iteration: 7757 lambda_n: 1.0381517941710336 Loss: 0.00020191496017443609\n",
      "Iteration: 7758 lambda_n: 0.9228701227982838 Loss: 0.000201914960174436\n",
      "Iteration: 7759 lambda_n: 1.0222634494371057 Loss: 0.00020191496017443603\n",
      "Iteration: 7760 lambda_n: 1.0112339447999323 Loss: 0.00020191496017443598\n",
      "Iteration: 7761 lambda_n: 0.9661436669125791 Loss: 0.00020191496017443603\n",
      "Iteration: 7762 lambda_n: 0.903741128462449 Loss: 0.00020191496017443609\n",
      "Iteration: 7763 lambda_n: 1.0292049784144497 Loss: 0.000201914960174436\n",
      "Iteration: 7764 lambda_n: 0.9358430104581796 Loss: 0.00020191496017443606\n",
      "Iteration: 7765 lambda_n: 0.9494990965512068 Loss: 0.00020191496017443598\n",
      "Iteration: 7766 lambda_n: 0.9253394123452844 Loss: 0.00020191496017443603\n",
      "Iteration: 7767 lambda_n: 1.0351074487716496 Loss: 0.00020191496017443609\n",
      "Iteration: 7768 lambda_n: 1.0167941668658846 Loss: 0.000201914960174436\n",
      "Iteration: 7769 lambda_n: 0.9338784812204803 Loss: 0.00020191496017443603\n",
      "Iteration: 7770 lambda_n: 1.0318035388400444 Loss: 0.00020191496017443598\n",
      "Iteration: 7771 lambda_n: 0.952089968494932 Loss: 0.00020191496017443603\n",
      "Iteration: 7772 lambda_n: 0.8885143064443728 Loss: 0.00020191496017443609\n",
      "Iteration: 7773 lambda_n: 0.9109297047295367 Loss: 0.000201914960174436\n",
      "Iteration: 7774 lambda_n: 0.96207005802723 Loss: 0.00020191496017443606\n",
      "Iteration: 7775 lambda_n: 0.934235172424593 Loss: 0.00020191496017443598\n",
      "Iteration: 7776 lambda_n: 0.9859130181868224 Loss: 0.00020191496017443603\n",
      "Iteration: 7777 lambda_n: 0.9573953142760709 Loss: 0.00020191496017443609\n",
      "Iteration: 7778 lambda_n: 0.9129850327226106 Loss: 0.000201914960174436\n",
      "Iteration: 7779 lambda_n: 0.9412612680652253 Loss: 0.00020191496017443603\n",
      "Iteration: 7780 lambda_n: 0.9565730250515223 Loss: 0.00020191496017443598\n",
      "Iteration: 7781 lambda_n: 0.9141111047401598 Loss: 0.00020191496017443603\n",
      "Iteration: 7782 lambda_n: 0.9636641542317839 Loss: 0.00020191496017443609\n",
      "Iteration: 7783 lambda_n: 0.9280270510546426 Loss: 0.000201914960174436\n",
      "Iteration: 7784 lambda_n: 0.970374763379621 Loss: 0.00020191496017443603\n",
      "Iteration: 7785 lambda_n: 0.97718661921363 Loss: 0.00020191496017443598\n",
      "Iteration: 7786 lambda_n: 0.9751716351726444 Loss: 0.00020191496017443603\n",
      "Iteration: 7787 lambda_n: 0.9125033550058456 Loss: 0.00020191496017443609\n",
      "Iteration: 7788 lambda_n: 0.9592624001396277 Loss: 0.000201914960174436\n",
      "Iteration: 7789 lambda_n: 1.0182632387870048 Loss: 0.00020191496017443606\n",
      "Iteration: 7790 lambda_n: 0.9726242130312063 Loss: 0.00020191496017443598\n",
      "Iteration: 7791 lambda_n: 0.9181680012920607 Loss: 0.00020191496017443603\n",
      "Iteration: 7792 lambda_n: 1.007395966005334 Loss: 0.00020191496017443609\n",
      "Iteration: 7793 lambda_n: 0.8989708429385225 Loss: 0.000201914960174436\n",
      "Iteration: 7794 lambda_n: 0.8885831375710368 Loss: 0.00020191496017443603\n",
      "Iteration: 7795 lambda_n: 0.9034931578307732 Loss: 0.00020191496017443598\n",
      "Iteration: 7796 lambda_n: 0.9177717027250882 Loss: 0.00020191496017443603\n",
      "Iteration: 7797 lambda_n: 1.0191414865674702 Loss: 0.00020191496017443609\n",
      "Iteration: 7798 lambda_n: 0.9976902074031606 Loss: 0.000201914960174436\n",
      "Iteration: 7799 lambda_n: 0.8894163280442915 Loss: 0.00020191496017443603\n",
      "Iteration: 7800 lambda_n: 0.9921522857238028 Loss: 0.00020191496017443598\n",
      "Iteration: 7801 lambda_n: 0.9762631626105626 Loss: 0.00020191496017443603\n",
      "Iteration: 7802 lambda_n: 1.0141209754106175 Loss: 0.00020191496017443609\n",
      "Iteration: 7803 lambda_n: 0.940376727593103 Loss: 0.000201914960174436\n",
      "Iteration: 7804 lambda_n: 0.8838769214575551 Loss: 0.00020191496017443603\n",
      "Iteration: 7805 lambda_n: 0.9928121849021261 Loss: 0.00020191496017443598\n",
      "Iteration: 7806 lambda_n: 0.917559890559509 Loss: 0.00020191496017443603\n",
      "Iteration: 7807 lambda_n: 1.0048662678144666 Loss: 0.00020191496017443609\n",
      "Iteration: 7808 lambda_n: 0.9255896986050461 Loss: 0.000201914960174436\n",
      "Iteration: 7809 lambda_n: 0.9683580593813295 Loss: 0.00020191496017443603\n",
      "Iteration: 7810 lambda_n: 0.9165181830333077 Loss: 0.00020191496017443598\n",
      "Iteration: 7811 lambda_n: 0.9511192430673213 Loss: 0.00020191496017443603\n",
      "Iteration: 7812 lambda_n: 1.0009486397439464 Loss: 0.00020191496017443609\n",
      "Iteration: 7813 lambda_n: 0.899224854221565 Loss: 0.000201914960174436\n",
      "Iteration: 7814 lambda_n: 0.9627646107977333 Loss: 0.00020191496017443603\n",
      "Iteration: 7815 lambda_n: 0.8934905266816453 Loss: 0.00020191496017443598\n",
      "Iteration: 7816 lambda_n: 1.0381730176994706 Loss: 0.00020191496017443603\n",
      "Iteration: 7817 lambda_n: 0.9172875117616058 Loss: 0.00020191496017443609\n",
      "Iteration: 7818 lambda_n: 0.9855418256698815 Loss: 0.000201914960174436\n",
      "Iteration: 7819 lambda_n: 0.9931574281786221 Loss: 0.00020191496017443603\n",
      "Iteration: 7820 lambda_n: 0.9859933705651723 Loss: 0.00020191496017443598\n",
      "Iteration: 7821 lambda_n: 0.9892119465312679 Loss: 0.00020191496017443603\n",
      "Iteration: 7822 lambda_n: 1.0379382926574126 Loss: 0.00020191496017443609\n",
      "Iteration: 7823 lambda_n: 1.0089086233209756 Loss: 0.000201914960174436\n",
      "Iteration: 7824 lambda_n: 0.9892980901485168 Loss: 0.00020191496017443603\n",
      "Iteration: 7825 lambda_n: 0.940588157153879 Loss: 0.00020191496017443598\n",
      "Iteration: 7826 lambda_n: 1.013613059251697 Loss: 0.00020191496017443603\n",
      "Iteration: 7827 lambda_n: 0.9229374595574297 Loss: 0.00020191496017443609\n",
      "Iteration: 7828 lambda_n: 0.8839746531643973 Loss: 0.000201914960174436\n",
      "Iteration: 7829 lambda_n: 0.9879275994285291 Loss: 0.00020191496017443603\n",
      "Iteration: 7830 lambda_n: 0.9215908810845098 Loss: 0.00020191496017443598\n",
      "Iteration: 7831 lambda_n: 0.9984979121596634 Loss: 0.00020191496017443603\n",
      "Iteration: 7832 lambda_n: 0.9777235680741448 Loss: 0.00020191496017443609\n",
      "Iteration: 7833 lambda_n: 0.9260550948360214 Loss: 0.000201914960174436\n",
      "Iteration: 7834 lambda_n: 0.9448379028695961 Loss: 0.00020191496017443603\n",
      "Iteration: 7835 lambda_n: 0.9847437263762567 Loss: 0.00020191496017443598\n",
      "Iteration: 7836 lambda_n: 1.0068710135876744 Loss: 0.00020191496017443603\n",
      "Iteration: 7837 lambda_n: 0.9181518375514968 Loss: 0.00020191496017443609\n",
      "Iteration: 7838 lambda_n: 1.0047113723936139 Loss: 0.000201914960174436\n",
      "Iteration: 7839 lambda_n: 0.8892577455696145 Loss: 0.00020191496017443603\n",
      "Iteration: 7840 lambda_n: 0.9987044905481425 Loss: 0.00020191496017443598\n",
      "Iteration: 7841 lambda_n: 0.8984277018420489 Loss: 0.00020191496017443603\n",
      "Iteration: 7842 lambda_n: 0.9822930157267795 Loss: 0.00020191496017443609\n",
      "Iteration: 7843 lambda_n: 0.9201079570459059 Loss: 0.000201914960174436\n",
      "Iteration: 7844 lambda_n: 0.9089379176876808 Loss: 0.00020191496017443603\n",
      "Iteration: 7845 lambda_n: 0.960469068974921 Loss: 0.00020191496017443598\n",
      "Iteration: 7846 lambda_n: 0.9591118523963539 Loss: 0.00020191496017443603\n",
      "Iteration: 7847 lambda_n: 0.9989193337624112 Loss: 0.00020191496017443609\n",
      "Iteration: 7848 lambda_n: 0.9303458429353644 Loss: 0.000201914960174436\n",
      "Iteration: 7849 lambda_n: 0.9802992484110377 Loss: 0.00020191496017443603\n",
      "Iteration: 7850 lambda_n: 0.9469596606675694 Loss: 0.00020191496017443598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7851 lambda_n: 1.0229914616804594 Loss: 0.00020191496017443603\n",
      "Iteration: 7852 lambda_n: 1.0324283960157694 Loss: 0.00020191496017443609\n",
      "Iteration: 7853 lambda_n: 0.997987976711919 Loss: 0.000201914960174436\n",
      "Iteration: 7854 lambda_n: 0.9433161994706104 Loss: 0.00020191496017443603\n",
      "Iteration: 7855 lambda_n: 1.0107314542961896 Loss: 0.00020191496017443598\n",
      "Iteration: 7856 lambda_n: 0.9369173115094223 Loss: 0.00020191496017443603\n",
      "Iteration: 7857 lambda_n: 0.9699015845428482 Loss: 0.00020191496017443609\n",
      "Iteration: 7858 lambda_n: 0.9286234332528756 Loss: 0.000201914960174436\n",
      "Iteration: 7859 lambda_n: 0.8980910036170591 Loss: 0.00020191496017443603\n",
      "Iteration: 7860 lambda_n: 0.8872277487775977 Loss: 0.00020191496017443598\n",
      "Iteration: 7861 lambda_n: 0.9010998510916499 Loss: 0.00020191496017443603\n",
      "Iteration: 7862 lambda_n: 0.9425504670171581 Loss: 0.00020191496017443609\n",
      "Iteration: 7863 lambda_n: 0.9687541995761734 Loss: 0.000201914960174436\n",
      "Iteration: 7864 lambda_n: 0.9645980389278682 Loss: 0.00020191496017443603\n",
      "Iteration: 7865 lambda_n: 0.8881423659173561 Loss: 0.00020191496017443598\n",
      "Iteration: 7866 lambda_n: 0.9761198920054405 Loss: 0.00020191496017443603\n",
      "Iteration: 7867 lambda_n: 0.8991382954975701 Loss: 0.00020191496017443609\n",
      "Iteration: 7868 lambda_n: 1.0347523572841564 Loss: 0.000201914960174436\n",
      "Iteration: 7869 lambda_n: 0.9084382633530678 Loss: 0.00020191496017443606\n",
      "Iteration: 7870 lambda_n: 1.008056330174422 Loss: 0.00020191496017443598\n",
      "Iteration: 7871 lambda_n: 0.9205912583075565 Loss: 0.00020191496017443603\n",
      "Iteration: 7872 lambda_n: 1.0198592739234704 Loss: 0.00020191496017443609\n",
      "Iteration: 7873 lambda_n: 0.966621748153219 Loss: 0.000201914960174436\n",
      "Iteration: 7874 lambda_n: 0.9962760480717613 Loss: 0.00020191496017443606\n",
      "Iteration: 7875 lambda_n: 0.9434144899372188 Loss: 0.00020191496017443598\n",
      "Iteration: 7876 lambda_n: 0.9119036998623833 Loss: 0.00020191496017443603\n",
      "Iteration: 7877 lambda_n: 0.9012939369038273 Loss: 0.00020191496017443609\n",
      "Iteration: 7878 lambda_n: 0.8874394909522029 Loss: 0.000201914960174436\n",
      "Iteration: 7879 lambda_n: 1.0316545685142395 Loss: 0.00020191496017443606\n",
      "Iteration: 7880 lambda_n: 0.9162846620269596 Loss: 0.00020191496017443598\n",
      "Iteration: 7881 lambda_n: 0.9443401336807291 Loss: 0.00020191496017443603\n",
      "Iteration: 7882 lambda_n: 0.9518497835015584 Loss: 0.00020191496017443609\n",
      "Iteration: 7883 lambda_n: 0.9049313431814231 Loss: 0.000201914960174436\n",
      "Iteration: 7884 lambda_n: 0.8909340330818789 Loss: 0.00020191496017443603\n",
      "Iteration: 7885 lambda_n: 0.8834705508337831 Loss: 0.00020191496017443598\n",
      "Iteration: 7886 lambda_n: 0.9801730217356062 Loss: 0.00020191496017443603\n",
      "Iteration: 7887 lambda_n: 0.9175611304437361 Loss: 0.00020191496017443609\n",
      "Iteration: 7888 lambda_n: 0.9619107977303567 Loss: 0.000201914960174436\n",
      "Iteration: 7889 lambda_n: 0.929697707176434 Loss: 0.00020191496017443603\n",
      "Iteration: 7890 lambda_n: 0.9622307893620181 Loss: 0.00020191496017443598\n",
      "Iteration: 7891 lambda_n: 0.9459786878347574 Loss: 0.00020191496017443603\n",
      "Iteration: 7892 lambda_n: 0.9640923528377306 Loss: 0.00020191496017443609\n",
      "Iteration: 7893 lambda_n: 0.957772950898049 Loss: 0.000201914960174436\n",
      "Iteration: 7894 lambda_n: 0.8880767074012483 Loss: 0.00020191496017443603\n",
      "Iteration: 7895 lambda_n: 0.9069196529095983 Loss: 0.00020191496017443598\n",
      "Iteration: 7896 lambda_n: 0.9614452122395852 Loss: 0.00020191496017443603\n",
      "Iteration: 7897 lambda_n: 0.9479724431932613 Loss: 0.00020191496017443609\n",
      "Iteration: 7898 lambda_n: 0.9928577074230976 Loss: 0.000201914960174436\n",
      "Iteration: 7899 lambda_n: 1.0349557133951925 Loss: 0.00020191496017443603\n",
      "Iteration: 7900 lambda_n: 0.9968438990689775 Loss: 0.00020191496017443598\n",
      "Iteration: 7901 lambda_n: 0.9313918162996011 Loss: 0.00020191496017443603\n",
      "Iteration: 7902 lambda_n: 0.9316623639611409 Loss: 0.00020191496017443609\n",
      "Iteration: 7903 lambda_n: 1.0021564705253339 Loss: 0.000201914960174436\n",
      "Iteration: 7904 lambda_n: 0.9403336627821361 Loss: 0.00020191496017443603\n",
      "Iteration: 7905 lambda_n: 0.9181353207624253 Loss: 0.00020191496017443598\n",
      "Iteration: 7906 lambda_n: 0.8990410942862083 Loss: 0.00020191496017443603\n",
      "Iteration: 7907 lambda_n: 0.9626844072018231 Loss: 0.00020191496017443609\n",
      "Iteration: 7908 lambda_n: 0.8960910426812945 Loss: 0.000201914960174436\n",
      "Iteration: 7909 lambda_n: 1.0167217672779318 Loss: 0.00020191496017443603\n",
      "Iteration: 7910 lambda_n: 0.9021084063548879 Loss: 0.00020191496017443598\n",
      "Iteration: 7911 lambda_n: 1.0310132695258651 Loss: 0.00020191496017443603\n",
      "Iteration: 7912 lambda_n: 0.9914433128901868 Loss: 0.00020191496017443609\n",
      "Iteration: 7913 lambda_n: 0.9175521366667778 Loss: 0.000201914960174436\n",
      "Iteration: 7914 lambda_n: 0.9458680882529314 Loss: 0.00020191496017443603\n",
      "Iteration: 7915 lambda_n: 0.9887022007838009 Loss: 0.00020191496017443598\n",
      "Iteration: 7916 lambda_n: 0.88574720134745 Loss: 0.00020191496017443603\n",
      "Iteration: 7917 lambda_n: 0.8875820724261005 Loss: 0.00020191496017443609\n",
      "Iteration: 7918 lambda_n: 0.937547055213316 Loss: 0.000201914960174436\n",
      "Iteration: 7919 lambda_n: 0.9009866642070499 Loss: 0.00020191496017443606\n",
      "Iteration: 7920 lambda_n: 0.9134427331174862 Loss: 0.00020191496017443598\n",
      "Iteration: 7921 lambda_n: 0.9373424443309616 Loss: 0.00020191496017443603\n",
      "Iteration: 7922 lambda_n: 0.9481312440316106 Loss: 0.00020191496017443609\n",
      "Iteration: 7923 lambda_n: 0.8873542776525745 Loss: 0.000201914960174436\n",
      "Iteration: 7924 lambda_n: 0.9104102655839312 Loss: 0.00020191496017443606\n",
      "Iteration: 7925 lambda_n: 1.0120064759157361 Loss: 0.00020191496017443598\n",
      "Iteration: 7926 lambda_n: 0.9006945750860819 Loss: 0.00020191496017443603\n",
      "Iteration: 7927 lambda_n: 0.8936104845626051 Loss: 0.00020191496017443609\n",
      "Iteration: 7928 lambda_n: 0.8962962013219961 Loss: 0.000201914960174436\n",
      "Iteration: 7929 lambda_n: 0.9927722855001122 Loss: 0.00020191496017443606\n",
      "Iteration: 7930 lambda_n: 0.9035551380444922 Loss: 0.00020191496017443598\n",
      "Iteration: 7931 lambda_n: 0.9491619640365014 Loss: 0.00020191496017443603\n",
      "Iteration: 7932 lambda_n: 1.020206664700708 Loss: 0.00020191496017443609\n",
      "Iteration: 7933 lambda_n: 1.023401520652018 Loss: 0.000201914960174436\n",
      "Iteration: 7934 lambda_n: 0.9662262973451622 Loss: 0.00020191496017443606\n",
      "Iteration: 7935 lambda_n: 0.9357291841965515 Loss: 0.00020191496017443598\n",
      "Iteration: 7936 lambda_n: 0.9064616096010353 Loss: 0.00020191496017443603\n",
      "Iteration: 7937 lambda_n: 0.9514908656537511 Loss: 0.00020191496017443609\n",
      "Iteration: 7938 lambda_n: 0.9924953659019023 Loss: 0.000201914960174436\n",
      "Iteration: 7939 lambda_n: 1.0298686224703062 Loss: 0.00020191496017443603\n",
      "Iteration: 7940 lambda_n: 1.00749864738617 Loss: 0.00020191496017443598\n",
      "Iteration: 7941 lambda_n: 0.897622750876184 Loss: 0.00020191496017443603\n",
      "Iteration: 7942 lambda_n: 0.9739097644205643 Loss: 0.00020191496017443609\n",
      "Iteration: 7943 lambda_n: 0.9929193022234033 Loss: 0.000201914960174436\n",
      "Iteration: 7944 lambda_n: 1.000022093882027 Loss: 0.00020191496017443603\n",
      "Iteration: 7945 lambda_n: 1.0195989213623868 Loss: 0.00020191496017443598\n",
      "Iteration: 7946 lambda_n: 0.883807164811398 Loss: 0.00020191496017443603\n",
      "Iteration: 7947 lambda_n: 0.8845310086391484 Loss: 0.00020191496017443609\n",
      "Iteration: 7948 lambda_n: 1.0341576905708085 Loss: 0.000201914960174436\n",
      "Iteration: 7949 lambda_n: 0.8984798937472428 Loss: 0.00020191496017443606\n",
      "Iteration: 7950 lambda_n: 0.8872383834980317 Loss: 0.00020191496017443598\n",
      "Iteration: 7951 lambda_n: 0.9295229577428006 Loss: 0.00020191496017443603\n",
      "Iteration: 7952 lambda_n: 0.9793348978397407 Loss: 0.00020191496017443609\n",
      "Iteration: 7953 lambda_n: 0.950341955056706 Loss: 0.000201914960174436\n",
      "Iteration: 7954 lambda_n: 0.9639576138287203 Loss: 0.00020191496017443606\n",
      "Iteration: 7955 lambda_n: 0.9010378188308746 Loss: 0.00020191496017443598\n",
      "Iteration: 7956 lambda_n: 0.8927686454603828 Loss: 0.00020191496017443603\n",
      "Iteration: 7957 lambda_n: 0.9831013455178788 Loss: 0.00020191496017443609\n",
      "Iteration: 7958 lambda_n: 0.9939284303846465 Loss: 0.000201914960174436\n",
      "Iteration: 7959 lambda_n: 0.8848705493684972 Loss: 0.00020191496017443603\n",
      "Iteration: 7960 lambda_n: 0.9497101713793461 Loss: 0.00020191496017443598\n",
      "Iteration: 7961 lambda_n: 1.0358241760551714 Loss: 0.00020191496017443603\n",
      "Iteration: 7962 lambda_n: 0.9017836139149478 Loss: 0.00020191496017443609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7963 lambda_n: 1.0100473900604425 Loss: 0.000201914960174436\n",
      "Iteration: 7964 lambda_n: 0.913925171337591 Loss: 0.00020191496017443606\n",
      "Iteration: 7965 lambda_n: 0.9369211636246444 Loss: 0.00020191496017443598\n",
      "Iteration: 7966 lambda_n: 0.9449313853795956 Loss: 0.00020191496017443603\n",
      "Iteration: 7967 lambda_n: 0.911864720779888 Loss: 0.00020191496017443609\n",
      "Iteration: 7968 lambda_n: 0.90651638245183 Loss: 0.000201914960174436\n",
      "Iteration: 7969 lambda_n: 0.963007718356057 Loss: 0.00020191496017443606\n",
      "Iteration: 7970 lambda_n: 0.9809722182532093 Loss: 0.00020191496017443598\n",
      "Iteration: 7971 lambda_n: 0.9425339948409007 Loss: 0.00020191496017443603\n",
      "Iteration: 7972 lambda_n: 0.9105668686481706 Loss: 0.00020191496017443609\n",
      "Iteration: 7973 lambda_n: 0.8828269714965087 Loss: 0.000201914960174436\n",
      "Iteration: 7974 lambda_n: 0.9760303625392552 Loss: 0.00020191496017443606\n",
      "Iteration: 7975 lambda_n: 1.027092846459975 Loss: 0.00020191496017443598\n",
      "Iteration: 7976 lambda_n: 0.9307137224302909 Loss: 0.00020191496017443603\n",
      "Iteration: 7977 lambda_n: 0.8897232940867629 Loss: 0.00020191496017443609\n",
      "Iteration: 7978 lambda_n: 0.9752212521125848 Loss: 0.000201914960174436\n",
      "Iteration: 7979 lambda_n: 1.035887455268812 Loss: 0.00020191496017443606\n",
      "Iteration: 7980 lambda_n: 0.9080178333306065 Loss: 0.00020191496017443598\n",
      "Iteration: 7981 lambda_n: 0.9307304781123441 Loss: 0.00020191496017443603\n",
      "Iteration: 7982 lambda_n: 0.9294738177659874 Loss: 0.00020191496017443609\n",
      "Iteration: 7983 lambda_n: 0.9229846566593255 Loss: 0.000201914960174436\n",
      "Iteration: 7984 lambda_n: 0.9604509888946597 Loss: 0.00020191496017443606\n",
      "Iteration: 7985 lambda_n: 0.981536537796987 Loss: 0.00020191496017443598\n",
      "Iteration: 7986 lambda_n: 1.0090354016610428 Loss: 0.00020191496017443603\n",
      "Iteration: 7987 lambda_n: 0.9378296766287866 Loss: 0.00020191496017443609\n",
      "Iteration: 7988 lambda_n: 0.9513151778830597 Loss: 0.000201914960174436\n",
      "Iteration: 7989 lambda_n: 0.8929878739098855 Loss: 0.00020191496017443603\n",
      "Iteration: 7990 lambda_n: 0.9068185113544046 Loss: 0.00020191496017443598\n",
      "Iteration: 7991 lambda_n: 0.9163272630838671 Loss: 0.00020191496017443603\n",
      "Iteration: 7992 lambda_n: 0.9793764349942725 Loss: 0.00020191496017443609\n",
      "Iteration: 7993 lambda_n: 0.9756043531528835 Loss: 0.000201914960174436\n",
      "Iteration: 7994 lambda_n: 0.9091824216251372 Loss: 0.00020191496017443603\n",
      "Iteration: 7995 lambda_n: 0.9071584840348318 Loss: 0.00020191496017443598\n",
      "Iteration: 7996 lambda_n: 0.9997765235527114 Loss: 0.00020191496017443603\n",
      "Iteration: 7997 lambda_n: 0.9025372269734663 Loss: 0.00020191496017443609\n",
      "Iteration: 7998 lambda_n: 0.9166115193912487 Loss: 0.000201914960174436\n",
      "Iteration: 7999 lambda_n: 0.9075376047428292 Loss: 0.00020191496017443606\n",
      "Iteration: 8000 lambda_n: 0.9615295169603 Loss: 0.00020191496017443598\n",
      "Iteration: 8001 lambda_n: 1.0261969479749964 Loss: 0.00020191496017443603\n",
      "Iteration: 8002 lambda_n: 0.9487857612033229 Loss: 0.00020191496017443609\n",
      "Iteration: 8003 lambda_n: 0.9663807490876453 Loss: 0.000201914960174436\n",
      "Iteration: 8004 lambda_n: 0.9945533244492157 Loss: 0.00020191496017443606\n",
      "Iteration: 8005 lambda_n: 0.9183594120904939 Loss: 0.00020191496017443598\n",
      "Iteration: 8006 lambda_n: 0.9978519396920459 Loss: 0.00020191496017443603\n",
      "Iteration: 8007 lambda_n: 0.9209024512759928 Loss: 0.00020191496017443609\n",
      "Iteration: 8008 lambda_n: 1.0096819058677293 Loss: 0.000201914960174436\n",
      "Iteration: 8009 lambda_n: 0.9857986531406354 Loss: 0.00020191496017443603\n",
      "Iteration: 8010 lambda_n: 1.0342045601775791 Loss: 0.00020191496017443598\n",
      "Iteration: 8011 lambda_n: 0.9100020133283091 Loss: 0.00020191496017443603\n",
      "Iteration: 8012 lambda_n: 0.8882161233668473 Loss: 0.00020191496017443609\n",
      "Iteration: 8013 lambda_n: 1.0303849468245745 Loss: 0.000201914960174436\n",
      "Iteration: 8014 lambda_n: 0.9352828083936422 Loss: 0.00020191496017443606\n",
      "Iteration: 8015 lambda_n: 0.9005484400054814 Loss: 0.00020191496017443598\n",
      "Iteration: 8016 lambda_n: 0.9881268910666171 Loss: 0.00020191496017443603\n",
      "Iteration: 8017 lambda_n: 0.9546506620025811 Loss: 0.00020191496017443609\n",
      "Iteration: 8018 lambda_n: 0.971171314439012 Loss: 0.000201914960174436\n",
      "Iteration: 8019 lambda_n: 0.9973818341175058 Loss: 0.00020191496017443603\n",
      "Iteration: 8020 lambda_n: 1.0263740325178516 Loss: 0.00020191496017443598\n",
      "Iteration: 8021 lambda_n: 1.0127609321857638 Loss: 0.00020191496017443603\n",
      "Iteration: 8022 lambda_n: 0.9099903008004839 Loss: 0.00020191496017443609\n",
      "Iteration: 8023 lambda_n: 0.9590820834378921 Loss: 0.000201914960174436\n",
      "Iteration: 8024 lambda_n: 0.942035175558584 Loss: 0.00020191496017443606\n",
      "Iteration: 8025 lambda_n: 0.9482102409046469 Loss: 0.00020191496017443598\n",
      "Iteration: 8026 lambda_n: 0.9774024229655401 Loss: 0.00020191496017443603\n",
      "Iteration: 8027 lambda_n: 0.9211212346909047 Loss: 0.00020191496017443609\n",
      "Iteration: 8028 lambda_n: 1.0304566022200927 Loss: 0.000201914960174436\n",
      "Iteration: 8029 lambda_n: 0.9874245806561687 Loss: 0.00020191496017443603\n",
      "Iteration: 8030 lambda_n: 0.9625898068497073 Loss: 0.00020191496017443598\n",
      "Iteration: 8031 lambda_n: 0.8962474352789719 Loss: 0.00020191496017443603\n",
      "Iteration: 8032 lambda_n: 0.9123865749923288 Loss: 0.00020191496017443609\n",
      "Iteration: 8033 lambda_n: 0.9417412559107422 Loss: 0.000201914960174436\n",
      "Iteration: 8034 lambda_n: 0.89395343864357 Loss: 0.00020191496017443606\n",
      "Iteration: 8035 lambda_n: 0.9516433158211404 Loss: 0.00020191496017443598\n",
      "Iteration: 8036 lambda_n: 0.9235495308727648 Loss: 0.00020191496017443603\n",
      "Iteration: 8037 lambda_n: 0.9790047681223515 Loss: 0.00020191496017443609\n",
      "Iteration: 8038 lambda_n: 0.9978241705488586 Loss: 0.000201914960174436\n",
      "Iteration: 8039 lambda_n: 0.9093835931658695 Loss: 0.00020191496017443606\n",
      "Iteration: 8040 lambda_n: 1.023771363333506 Loss: 0.00020191496017443598\n",
      "Iteration: 8041 lambda_n: 1.0261448888261102 Loss: 0.00020191496017443603\n",
      "Iteration: 8042 lambda_n: 0.9723163618235158 Loss: 0.00020191496017443609\n",
      "Iteration: 8043 lambda_n: 0.896904755352747 Loss: 0.000201914960174436\n",
      "Iteration: 8044 lambda_n: 1.0215187422280283 Loss: 0.00020191496017443606\n",
      "Iteration: 8045 lambda_n: 1.0277273934242517 Loss: 0.00020191496017443598\n",
      "Iteration: 8046 lambda_n: 1.0151309087254428 Loss: 0.00020191496017443603\n",
      "Iteration: 8047 lambda_n: 0.9894026969605674 Loss: 0.00020191496017443609\n",
      "Iteration: 8048 lambda_n: 1.0009287821651036 Loss: 0.000201914960174436\n",
      "Iteration: 8049 lambda_n: 0.9936487388376005 Loss: 0.00020191496017443603\n",
      "Iteration: 8050 lambda_n: 0.9402725996400263 Loss: 0.00020191496017443598\n",
      "Iteration: 8051 lambda_n: 0.9163969687549796 Loss: 0.00020191496017443603\n",
      "Iteration: 8052 lambda_n: 0.9602330372292577 Loss: 0.00020191496017443609\n",
      "Iteration: 8053 lambda_n: 0.9678617084793503 Loss: 0.000201914960174436\n",
      "Iteration: 8054 lambda_n: 0.9884761793413366 Loss: 0.00020191496017443603\n",
      "Iteration: 8055 lambda_n: 0.9722324921223299 Loss: 0.00020191496017443598\n",
      "Iteration: 8056 lambda_n: 0.893806560770215 Loss: 0.00020191496017443603\n",
      "Iteration: 8057 lambda_n: 0.9700731860030191 Loss: 0.00020191496017443609\n",
      "Iteration: 8058 lambda_n: 0.9439636431687228 Loss: 0.000201914960174436\n",
      "Iteration: 8059 lambda_n: 0.9417920871798108 Loss: 0.00020191496017443603\n",
      "Iteration: 8060 lambda_n: 1.0002611358205227 Loss: 0.00020191496017443598\n",
      "Iteration: 8061 lambda_n: 1.020412227408495 Loss: 0.00020191496017443603\n",
      "Iteration: 8062 lambda_n: 0.887125977395909 Loss: 0.00020191496017443609\n",
      "Iteration: 8063 lambda_n: 0.9510319873545295 Loss: 0.000201914960174436\n",
      "Iteration: 8064 lambda_n: 0.9418277319453449 Loss: 0.00020191496017443606\n",
      "Iteration: 8065 lambda_n: 1.0198330970495246 Loss: 0.00020191496017443598\n",
      "Iteration: 8066 lambda_n: 0.9435250451844569 Loss: 0.00020191496017443603\n",
      "Iteration: 8067 lambda_n: 0.9180333121107864 Loss: 0.00020191496017443609\n",
      "Iteration: 8068 lambda_n: 0.9681750388403616 Loss: 0.000201914960174436\n",
      "Iteration: 8069 lambda_n: 1.0011240140924662 Loss: 0.00020191496017443603\n",
      "Iteration: 8070 lambda_n: 0.9558016902430401 Loss: 0.00020191496017443598\n",
      "Iteration: 8071 lambda_n: 1.0138346609275184 Loss: 0.00020191496017443603\n",
      "Iteration: 8072 lambda_n: 0.9077890222443854 Loss: 0.00020191496017443609\n",
      "Iteration: 8073 lambda_n: 0.9327467310553974 Loss: 0.000201914960174436\n",
      "Iteration: 8074 lambda_n: 0.9544669659159857 Loss: 0.00020191496017443606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8075 lambda_n: 1.0345410996273414 Loss: 0.00020191496017443598\n",
      "Iteration: 8076 lambda_n: 0.9595610839915852 Loss: 0.00020191496017443603\n",
      "Iteration: 8077 lambda_n: 0.9413179224590983 Loss: 0.00020191496017443609\n",
      "Iteration: 8078 lambda_n: 1.0113549991735518 Loss: 0.000201914960174436\n",
      "Iteration: 8079 lambda_n: 0.9301466774206801 Loss: 0.00020191496017443603\n",
      "Iteration: 8080 lambda_n: 0.9813281046009862 Loss: 0.00020191496017443598\n",
      "Iteration: 8081 lambda_n: 0.9768400606628498 Loss: 0.00020191496017443603\n",
      "Iteration: 8082 lambda_n: 1.0065991235036222 Loss: 0.00020191496017443609\n",
      "Iteration: 8083 lambda_n: 0.9466187804621953 Loss: 0.000201914960174436\n",
      "Iteration: 8084 lambda_n: 0.9449139040088514 Loss: 0.00020191496017443603\n",
      "Iteration: 8085 lambda_n: 1.0069063852297868 Loss: 0.00020191496017443598\n",
      "Iteration: 8086 lambda_n: 0.9396600313657526 Loss: 0.00020191496017443603\n",
      "Iteration: 8087 lambda_n: 1.0334992664622176 Loss: 0.00020191496017443609\n",
      "Iteration: 8088 lambda_n: 0.9371603717563113 Loss: 0.000201914960174436\n",
      "Iteration: 8089 lambda_n: 1.0015285519515684 Loss: 0.00020191496017443603\n",
      "Iteration: 8090 lambda_n: 0.9720556748010577 Loss: 0.00020191496017443598\n",
      "Iteration: 8091 lambda_n: 1.0344346795837298 Loss: 0.00020191496017443603\n",
      "Iteration: 8092 lambda_n: 1.028320804487974 Loss: 0.00020191496017443609\n",
      "Iteration: 8093 lambda_n: 1.032908938073804 Loss: 0.000201914960174436\n",
      "Iteration: 8094 lambda_n: 0.9476433301641051 Loss: 0.00020191496017443603\n",
      "Iteration: 8095 lambda_n: 0.9598617877087228 Loss: 0.00020191496017443598\n",
      "Iteration: 8096 lambda_n: 1.019265555290801 Loss: 0.00020191496017443603\n",
      "Iteration: 8097 lambda_n: 0.9708991458128801 Loss: 0.00020191496017443609\n",
      "Iteration: 8098 lambda_n: 0.9743735209247084 Loss: 0.000201914960174436\n",
      "Iteration: 8099 lambda_n: 1.0149083109599006 Loss: 0.00020191496017443603\n",
      "Iteration: 8100 lambda_n: 1.0379796611389172 Loss: 0.00020191496017443598\n",
      "Iteration: 8101 lambda_n: 1.0302625102894922 Loss: 0.00020191496017443603\n",
      "Iteration: 8102 lambda_n: 0.9189775286937457 Loss: 0.00020191496017443609\n",
      "Iteration: 8103 lambda_n: 1.0318195139712492 Loss: 0.000201914960174436\n",
      "Iteration: 8104 lambda_n: 0.9065754926451264 Loss: 0.00020191496017443603\n",
      "Iteration: 8105 lambda_n: 0.9381420352875478 Loss: 0.00020191496017443598\n",
      "Iteration: 8106 lambda_n: 0.9888025878465284 Loss: 0.00020191496017443603\n",
      "Iteration: 8107 lambda_n: 0.886441823924543 Loss: 0.00020191496017443609\n",
      "Iteration: 8108 lambda_n: 0.9349431338819874 Loss: 0.000201914960174436\n",
      "Iteration: 8109 lambda_n: 0.8849419402525183 Loss: 0.00020191496017443606\n",
      "Iteration: 8110 lambda_n: 0.9382063349013476 Loss: 0.00020191496017443598\n",
      "Iteration: 8111 lambda_n: 0.9487308038974315 Loss: 0.00020191496017443603\n",
      "Iteration: 8112 lambda_n: 0.9764676685675764 Loss: 0.00020191496017443609\n",
      "Iteration: 8113 lambda_n: 1.018734710565471 Loss: 0.000201914960174436\n",
      "Iteration: 8114 lambda_n: 1.0241062175083075 Loss: 0.00020191496017443606\n",
      "Iteration: 8115 lambda_n: 0.9166269996000782 Loss: 0.00020191496017443598\n",
      "Iteration: 8116 lambda_n: 0.9781174249602 Loss: 0.00020191496017443603\n",
      "Iteration: 8117 lambda_n: 0.9622911932979756 Loss: 0.00020191496017443609\n",
      "Iteration: 8118 lambda_n: 1.0252713523314738 Loss: 0.000201914960174436\n",
      "Iteration: 8119 lambda_n: 0.9938543693316394 Loss: 0.00020191496017443603\n",
      "Iteration: 8120 lambda_n: 0.9591536656034536 Loss: 0.00020191496017443598\n",
      "Iteration: 8121 lambda_n: 0.8972539736348335 Loss: 0.00020191496017443603\n",
      "Iteration: 8122 lambda_n: 0.9819124306100068 Loss: 0.00020191496017443609\n",
      "Iteration: 8123 lambda_n: 1.009825678412308 Loss: 0.000201914960174436\n",
      "Iteration: 8124 lambda_n: 0.950881477829209 Loss: 0.00020191496017443603\n",
      "Iteration: 8125 lambda_n: 0.9453669272146122 Loss: 0.00020191496017443598\n",
      "Iteration: 8126 lambda_n: 0.9497872839574807 Loss: 0.00020191496017443603\n",
      "Iteration: 8127 lambda_n: 0.9851108791495168 Loss: 0.00020191496017443609\n",
      "Iteration: 8128 lambda_n: 0.9131443278558331 Loss: 0.000201914960174436\n",
      "Iteration: 8129 lambda_n: 0.899956455047058 Loss: 0.00020191496017443603\n",
      "Iteration: 8130 lambda_n: 0.922912389542083 Loss: 0.00020191496017443598\n",
      "Iteration: 8131 lambda_n: 1.0184369853228843 Loss: 0.00020191496017443603\n",
      "Iteration: 8132 lambda_n: 0.9839183403527014 Loss: 0.00020191496017443609\n",
      "Iteration: 8133 lambda_n: 0.9871530241397903 Loss: 0.000201914960174436\n",
      "Iteration: 8134 lambda_n: 1.0272188103880695 Loss: 0.00020191496017443603\n",
      "Iteration: 8135 lambda_n: 0.9372069815016364 Loss: 0.00020191496017443598\n",
      "Iteration: 8136 lambda_n: 0.9300085883961385 Loss: 0.00020191496017443603\n",
      "Iteration: 8137 lambda_n: 0.899711194441933 Loss: 0.00020191496017443609\n",
      "Iteration: 8138 lambda_n: 0.9640418000077617 Loss: 0.000201914960174436\n",
      "Iteration: 8139 lambda_n: 0.9919135647799123 Loss: 0.00020191496017443606\n",
      "Iteration: 8140 lambda_n: 0.9253153343510552 Loss: 0.00020191496017443598\n",
      "Iteration: 8141 lambda_n: 1.018496495327165 Loss: 0.00020191496017443603\n",
      "Iteration: 8142 lambda_n: 0.9331286335475865 Loss: 0.00020191496017443609\n",
      "Iteration: 8143 lambda_n: 0.974562332939424 Loss: 0.000201914960174436\n",
      "Iteration: 8144 lambda_n: 0.9215119463292089 Loss: 0.00020191496017443603\n",
      "Iteration: 8145 lambda_n: 1.0284049025598836 Loss: 0.00020191496017443598\n",
      "Iteration: 8146 lambda_n: 0.901069474645821 Loss: 0.00020191496017443603\n",
      "Iteration: 8147 lambda_n: 0.9744518333158665 Loss: 0.00020191496017443609\n",
      "Iteration: 8148 lambda_n: 0.8995878130894164 Loss: 0.000201914960174436\n",
      "Iteration: 8149 lambda_n: 0.9221947138518075 Loss: 0.00020191496017443603\n",
      "Iteration: 8150 lambda_n: 0.9977257204709885 Loss: 0.00020191496017443598\n",
      "Iteration: 8151 lambda_n: 0.926535620471354 Loss: 0.00020191496017443603\n",
      "Iteration: 8152 lambda_n: 0.9416202888600551 Loss: 0.00020191496017443609\n",
      "Iteration: 8153 lambda_n: 0.9839684040941789 Loss: 0.000201914960174436\n",
      "Iteration: 8154 lambda_n: 0.9464645640663684 Loss: 0.00020191496017443603\n",
      "Iteration: 8155 lambda_n: 0.9411994443525273 Loss: 0.00020191496017443598\n",
      "Iteration: 8156 lambda_n: 1.0007057347050263 Loss: 0.00020191496017443603\n",
      "Iteration: 8157 lambda_n: 0.9221560798549522 Loss: 0.00020191496017443609\n",
      "Iteration: 8158 lambda_n: 1.0223551166596427 Loss: 0.000201914960174436\n",
      "Iteration: 8159 lambda_n: 0.8884242818462867 Loss: 0.00020191496017443603\n",
      "Iteration: 8160 lambda_n: 0.88638346998029 Loss: 0.00020191496017443598\n",
      "Iteration: 8161 lambda_n: 0.9836135159347081 Loss: 0.00020191496017443603\n",
      "Iteration: 8162 lambda_n: 0.9882668959316865 Loss: 0.00020191496017443609\n",
      "Iteration: 8163 lambda_n: 0.9159735092317559 Loss: 0.000201914960174436\n",
      "Iteration: 8164 lambda_n: 0.8885345313723425 Loss: 0.00020191496017443603\n",
      "Iteration: 8165 lambda_n: 0.9950135837003409 Loss: 0.00020191496017443598\n",
      "Iteration: 8166 lambda_n: 1.020467586936979 Loss: 0.00020191496017443603\n",
      "Iteration: 8167 lambda_n: 1.004952958682912 Loss: 0.00020191496017443609\n",
      "Iteration: 8168 lambda_n: 1.026267208051662 Loss: 0.000201914960174436\n",
      "Iteration: 8169 lambda_n: 0.8976839365690724 Loss: 0.00020191496017443603\n",
      "Iteration: 8170 lambda_n: 0.9549971245322079 Loss: 0.00020191496017443598\n",
      "Iteration: 8171 lambda_n: 0.985849478691089 Loss: 0.00020191496017443603\n",
      "Iteration: 8172 lambda_n: 0.8991686142056933 Loss: 0.00020191496017443609\n",
      "Iteration: 8173 lambda_n: 0.8861908823576302 Loss: 0.000201914960174436\n",
      "Iteration: 8174 lambda_n: 1.0343485792779028 Loss: 0.00020191496017443606\n",
      "Iteration: 8175 lambda_n: 0.9969869142446268 Loss: 0.00020191496017443598\n",
      "Iteration: 8176 lambda_n: 1.0289479546016405 Loss: 0.00020191496017443603\n",
      "Iteration: 8177 lambda_n: 0.8879465370897921 Loss: 0.00020191496017443609\n",
      "Iteration: 8178 lambda_n: 1.0327651418943407 Loss: 0.000201914960174436\n",
      "Iteration: 8179 lambda_n: 0.9178381338732566 Loss: 0.00020191496017443606\n",
      "Iteration: 8180 lambda_n: 0.9535984089325577 Loss: 0.00020191496017443598\n",
      "Iteration: 8181 lambda_n: 0.9602506300098068 Loss: 0.00020191496017443603\n",
      "Iteration: 8182 lambda_n: 0.9052657567705609 Loss: 0.00020191496017443609\n",
      "Iteration: 8183 lambda_n: 1.0289639363298406 Loss: 0.000201914960174436\n",
      "Iteration: 8184 lambda_n: 1.0119415265276 Loss: 0.00020191496017443606\n",
      "Iteration: 8185 lambda_n: 0.9470641692980392 Loss: 0.00020191496017443598\n",
      "Iteration: 8186 lambda_n: 0.8848130273361687 Loss: 0.00020191496017443603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8187 lambda_n: 1.004382123810618 Loss: 0.00020191496017443609\n",
      "Iteration: 8188 lambda_n: 0.9027333376623267 Loss: 0.000201914960174436\n",
      "Iteration: 8189 lambda_n: 0.9917518636358891 Loss: 0.00020191496017443603\n",
      "Iteration: 8190 lambda_n: 1.0058513799956204 Loss: 0.00020191496017443598\n",
      "Iteration: 8191 lambda_n: 0.9535450802413545 Loss: 0.00020191496017443603\n",
      "Iteration: 8192 lambda_n: 0.9705086175496059 Loss: 0.00020191496017443609\n",
      "Iteration: 8193 lambda_n: 0.9859335259965444 Loss: 0.000201914960174436\n",
      "Iteration: 8194 lambda_n: 0.9778910948137053 Loss: 0.00020191496017443603\n",
      "Iteration: 8195 lambda_n: 0.9655507397765415 Loss: 0.00020191496017443598\n",
      "Iteration: 8196 lambda_n: 0.9005493385571075 Loss: 0.00020191496017443603\n",
      "Iteration: 8197 lambda_n: 0.9311690871414054 Loss: 0.00020191496017443609\n",
      "Iteration: 8198 lambda_n: 0.8891528019546336 Loss: 0.000201914960174436\n",
      "Iteration: 8199 lambda_n: 0.9687028664746978 Loss: 0.00020191496017443603\n",
      "Iteration: 8200 lambda_n: 0.9625428781577064 Loss: 0.00020191496017443598\n",
      "Iteration: 8201 lambda_n: 0.8857097584666571 Loss: 0.00020191496017443603\n",
      "Iteration: 8202 lambda_n: 0.8977262646251662 Loss: 0.00020191496017443609\n",
      "Iteration: 8203 lambda_n: 0.9476523094747359 Loss: 0.000201914960174436\n",
      "Iteration: 8204 lambda_n: 0.939985971040051 Loss: 0.00020191496017443606\n",
      "Iteration: 8205 lambda_n: 0.9367710627402026 Loss: 0.00020191496017443598\n",
      "Iteration: 8206 lambda_n: 0.9984268026414023 Loss: 0.00020191496017443603\n",
      "Iteration: 8207 lambda_n: 0.9569362803627517 Loss: 0.00020191496017443609\n",
      "Iteration: 8208 lambda_n: 0.9245627750889048 Loss: 0.000201914960174436\n",
      "Iteration: 8209 lambda_n: 1.0283901890715457 Loss: 0.00020191496017443603\n",
      "Iteration: 8210 lambda_n: 1.0079030483962568 Loss: 0.00020191496017443598\n",
      "Iteration: 8211 lambda_n: 0.9604350122376257 Loss: 0.00020191496017443603\n",
      "Iteration: 8212 lambda_n: 0.9955869555610296 Loss: 0.00020191496017443609\n",
      "Iteration: 8213 lambda_n: 1.0264995450332335 Loss: 0.000201914960174436\n",
      "Iteration: 8214 lambda_n: 0.9445499457313745 Loss: 0.00020191496017443603\n",
      "Iteration: 8215 lambda_n: 0.9469526240837494 Loss: 0.00020191496017443598\n",
      "Iteration: 8216 lambda_n: 0.9789995813035094 Loss: 0.00020191496017443603\n",
      "Iteration: 8217 lambda_n: 0.917146994714766 Loss: 0.00020191496017443609\n",
      "Iteration: 8218 lambda_n: 1.0231775516328432 Loss: 0.000201914960174436\n",
      "Iteration: 8219 lambda_n: 0.9000553356363896 Loss: 0.00020191496017443603\n",
      "Iteration: 8220 lambda_n: 0.9308076340689208 Loss: 0.00020191496017443598\n",
      "Iteration: 8221 lambda_n: 1.0119412629527533 Loss: 0.00020191496017443603\n",
      "Iteration: 8222 lambda_n: 1.0147326498737612 Loss: 0.00020191496017443609\n",
      "Iteration: 8223 lambda_n: 0.9610736158730764 Loss: 0.000201914960174436\n",
      "Iteration: 8224 lambda_n: 1.0172699662271343 Loss: 0.00020191496017443603\n",
      "Iteration: 8225 lambda_n: 0.9723022598684589 Loss: 0.00020191496017443598\n",
      "Iteration: 8226 lambda_n: 0.972058718439262 Loss: 0.00020191496017443603\n",
      "Iteration: 8227 lambda_n: 0.9013942958744489 Loss: 0.00020191496017443609\n",
      "Iteration: 8228 lambda_n: 0.9591442055148078 Loss: 0.000201914960174436\n",
      "Iteration: 8229 lambda_n: 1.0173530236451023 Loss: 0.00020191496017443606\n",
      "Iteration: 8230 lambda_n: 1.0027027895899308 Loss: 0.00020191496017443598\n",
      "Iteration: 8231 lambda_n: 1.0028930804850924 Loss: 0.00020191496017443603\n",
      "Iteration: 8232 lambda_n: 0.9222064067000852 Loss: 0.00020191496017443609\n",
      "Iteration: 8233 lambda_n: 0.9908848475764679 Loss: 0.000201914960174436\n",
      "Iteration: 8234 lambda_n: 0.964200291259918 Loss: 0.00020191496017443603\n",
      "Iteration: 8235 lambda_n: 0.9612526909718607 Loss: 0.00020191496017443598\n",
      "Iteration: 8236 lambda_n: 0.9990438683954675 Loss: 0.00020191496017443603\n",
      "Iteration: 8237 lambda_n: 0.900778720255154 Loss: 0.00020191496017443609\n",
      "Iteration: 8238 lambda_n: 0.8866262042637396 Loss: 0.000201914960174436\n",
      "Iteration: 8239 lambda_n: 0.9731748740583771 Loss: 0.00020191496017443606\n",
      "Iteration: 8240 lambda_n: 1.0005985365521277 Loss: 0.00020191496017443598\n",
      "Iteration: 8241 lambda_n: 0.9943902112890565 Loss: 0.00020191496017443603\n",
      "Iteration: 8242 lambda_n: 0.9659301759474616 Loss: 0.00020191496017443609\n",
      "Iteration: 8243 lambda_n: 0.9621494988195496 Loss: 0.000201914960174436\n",
      "Iteration: 8244 lambda_n: 0.9087173433492508 Loss: 0.00020191496017443603\n",
      "Iteration: 8245 lambda_n: 0.9979884985335096 Loss: 0.00020191496017443598\n",
      "Iteration: 8246 lambda_n: 1.0059310009910383 Loss: 0.00020191496017443603\n",
      "Iteration: 8247 lambda_n: 0.9824822763699163 Loss: 0.00020191496017443609\n",
      "Iteration: 8248 lambda_n: 0.9398954980049157 Loss: 0.000201914960174436\n",
      "Iteration: 8249 lambda_n: 1.009586440025949 Loss: 0.00020191496017443603\n",
      "Iteration: 8250 lambda_n: 0.8904111734505896 Loss: 0.00020191496017443598\n",
      "Iteration: 8251 lambda_n: 0.945835726039429 Loss: 0.00020191496017443603\n",
      "Iteration: 8252 lambda_n: 0.9667738137758145 Loss: 0.00020191496017443609\n",
      "Iteration: 8253 lambda_n: 0.9828215073366192 Loss: 0.000201914960174436\n",
      "Iteration: 8254 lambda_n: 0.9700723837294087 Loss: 0.00020191496017443603\n",
      "Iteration: 8255 lambda_n: 0.9679896087981262 Loss: 0.00020191496017443598\n",
      "Iteration: 8256 lambda_n: 0.9253221483261055 Loss: 0.00020191496017443603\n",
      "Iteration: 8257 lambda_n: 1.0198616708346433 Loss: 0.00020191496017443609\n",
      "Iteration: 8258 lambda_n: 1.0108142427477778 Loss: 0.000201914960174436\n",
      "Iteration: 8259 lambda_n: 1.0277843282112156 Loss: 0.00020191496017443603\n",
      "Iteration: 8260 lambda_n: 0.9556957371982993 Loss: 0.00020191496017443598\n",
      "Iteration: 8261 lambda_n: 0.9651346616358715 Loss: 0.00020191496017443603\n",
      "Iteration: 8262 lambda_n: 0.9354511972823661 Loss: 0.00020191496017443609\n",
      "Iteration: 8263 lambda_n: 0.8896728931343735 Loss: 0.000201914960174436\n",
      "Iteration: 8264 lambda_n: 1.0010906659188548 Loss: 0.00020191496017443603\n",
      "Iteration: 8265 lambda_n: 1.0041901689076527 Loss: 0.00020191496017443598\n",
      "Iteration: 8266 lambda_n: 0.9979444091501904 Loss: 0.00020191496017443603\n",
      "Iteration: 8267 lambda_n: 1.023144337370555 Loss: 0.00020191496017443609\n",
      "Iteration: 8268 lambda_n: 0.9363000419028646 Loss: 0.000201914960174436\n",
      "Iteration: 8269 lambda_n: 0.9104044813644379 Loss: 0.00020191496017443603\n",
      "Iteration: 8270 lambda_n: 0.9911369273730972 Loss: 0.00020191496017443598\n",
      "Iteration: 8271 lambda_n: 0.9234516934534884 Loss: 0.00020191496017443603\n",
      "Iteration: 8272 lambda_n: 1.0342328486120018 Loss: 0.00020191496017443609\n",
      "Iteration: 8273 lambda_n: 1.024507629768243 Loss: 0.000201914960174436\n",
      "Iteration: 8274 lambda_n: 0.8855094289389664 Loss: 0.00020191496017443603\n",
      "Iteration: 8275 lambda_n: 1.0048802346586956 Loss: 0.00020191496017443598\n",
      "Iteration: 8276 lambda_n: 1.030480524683435 Loss: 0.00020191496017443603\n",
      "Iteration: 8277 lambda_n: 0.9830386134237225 Loss: 0.00020191496017443609\n",
      "Iteration: 8278 lambda_n: 0.9240596129083266 Loss: 0.000201914960174436\n",
      "Iteration: 8279 lambda_n: 1.0076890190623882 Loss: 0.00020191496017443603\n",
      "Iteration: 8280 lambda_n: 0.910893429739233 Loss: 0.00020191496017443598\n",
      "Iteration: 8281 lambda_n: 0.9773479336936922 Loss: 0.00020191496017443603\n",
      "Iteration: 8282 lambda_n: 1.0089136029615706 Loss: 0.00020191496017443609\n",
      "Iteration: 8283 lambda_n: 0.9468132818091447 Loss: 0.000201914960174436\n",
      "Iteration: 8284 lambda_n: 0.9934584097991761 Loss: 0.00020191496017443603\n",
      "Iteration: 8285 lambda_n: 1.023239007161509 Loss: 0.00020191496017443598\n",
      "Iteration: 8286 lambda_n: 1.0091898889429667 Loss: 0.00020191496017443603\n",
      "Iteration: 8287 lambda_n: 0.8960201559582079 Loss: 0.00020191496017443609\n",
      "Iteration: 8288 lambda_n: 0.9800601120414103 Loss: 0.000201914960174436\n",
      "Iteration: 8289 lambda_n: 0.8969717960142752 Loss: 0.00020191496017443606\n",
      "Iteration: 8290 lambda_n: 0.9890432614148008 Loss: 0.00020191496017443598\n",
      "Iteration: 8291 lambda_n: 0.9008452651755257 Loss: 0.00020191496017443603\n",
      "Iteration: 8292 lambda_n: 1.0091655147780918 Loss: 0.00020191496017443609\n",
      "Iteration: 8293 lambda_n: 0.9976378922300827 Loss: 0.000201914960174436\n",
      "Iteration: 8294 lambda_n: 0.9076576765097344 Loss: 0.00020191496017443606\n",
      "Iteration: 8295 lambda_n: 0.9853458337858433 Loss: 0.00020191496017443598\n",
      "Iteration: 8296 lambda_n: 0.8965373074432955 Loss: 0.00020191496017443603\n",
      "Iteration: 8297 lambda_n: 0.9407107137678417 Loss: 0.00020191496017443609\n",
      "Iteration: 8298 lambda_n: 0.9358483329267631 Loss: 0.000201914960174436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8299 lambda_n: 1.03780315837493 Loss: 0.00020191496017443606\n",
      "Iteration: 8300 lambda_n: 0.8973365564837172 Loss: 0.00020191496017443598\n",
      "Iteration: 8301 lambda_n: 0.8998613287043602 Loss: 0.00020191496017443603\n",
      "Iteration: 8302 lambda_n: 0.9041496921554711 Loss: 0.00020191496017443609\n",
      "Iteration: 8303 lambda_n: 0.9943863437371991 Loss: 0.000201914960174436\n",
      "Iteration: 8304 lambda_n: 1.0371782276937107 Loss: 0.00020191496017443606\n",
      "Iteration: 8305 lambda_n: 1.0101311616861697 Loss: 0.00020191496017443598\n",
      "Iteration: 8306 lambda_n: 1.0362369879978226 Loss: 0.00020191496017443603\n",
      "Iteration: 8307 lambda_n: 0.9964156665621486 Loss: 0.00020191496017443609\n",
      "Iteration: 8308 lambda_n: 0.9196140421733309 Loss: 0.000201914960174436\n",
      "Iteration: 8309 lambda_n: 0.9270011107317824 Loss: 0.00020191496017443603\n",
      "Iteration: 8310 lambda_n: 0.8922043749603291 Loss: 0.00020191496017443598\n",
      "Iteration: 8311 lambda_n: 0.9356596235135018 Loss: 0.00020191496017443603\n",
      "Iteration: 8312 lambda_n: 0.9932937471439458 Loss: 0.00020191496017443609\n",
      "Iteration: 8313 lambda_n: 1.0324462115186746 Loss: 0.000201914960174436\n",
      "Iteration: 8314 lambda_n: 0.9980137413497513 Loss: 0.00020191496017443603\n",
      "Iteration: 8315 lambda_n: 1.0147032563243266 Loss: 0.00020191496017443598\n",
      "Iteration: 8316 lambda_n: 0.9923544409286701 Loss: 0.00020191496017443603\n",
      "Iteration: 8317 lambda_n: 0.8950712631222693 Loss: 0.00020191496017443609\n",
      "Iteration: 8318 lambda_n: 0.939184894031934 Loss: 0.000201914960174436\n",
      "Iteration: 8319 lambda_n: 1.0083910294668441 Loss: 0.00020191496017443606\n",
      "Iteration: 8320 lambda_n: 0.9039939391017627 Loss: 0.00020191496017443598\n",
      "Iteration: 8321 lambda_n: 0.9627303389558289 Loss: 0.00020191496017443603\n",
      "Iteration: 8322 lambda_n: 0.9269848143180927 Loss: 0.00020191496017443609\n",
      "Iteration: 8323 lambda_n: 0.9087089238287638 Loss: 0.000201914960174436\n",
      "Iteration: 8324 lambda_n: 0.9585630863225252 Loss: 0.00020191496017443603\n",
      "Iteration: 8325 lambda_n: 0.9366440000447381 Loss: 0.00020191496017443598\n",
      "Iteration: 8326 lambda_n: 1.037974676370567 Loss: 0.00020191496017443603\n",
      "Iteration: 8327 lambda_n: 1.0085530527134172 Loss: 0.00020191496017443609\n",
      "Iteration: 8328 lambda_n: 0.9771061574928077 Loss: 0.000201914960174436\n",
      "Iteration: 8329 lambda_n: 0.9367190735026507 Loss: 0.00020191496017443603\n",
      "Iteration: 8330 lambda_n: 1.0197975458152675 Loss: 0.00020191496017443598\n",
      "Iteration: 8331 lambda_n: 1.006442332938814 Loss: 0.00020191496017443603\n",
      "Iteration: 8332 lambda_n: 0.9329255059113248 Loss: 0.00020191496017443609\n",
      "Iteration: 8333 lambda_n: 0.893561579962065 Loss: 0.000201914960174436\n",
      "Iteration: 8334 lambda_n: 0.964801342164503 Loss: 0.00020191496017443603\n",
      "Iteration: 8335 lambda_n: 0.9126333385288086 Loss: 0.00020191496017443598\n",
      "Iteration: 8336 lambda_n: 0.9084229591099352 Loss: 0.00020191496017443603\n",
      "Iteration: 8337 lambda_n: 0.9987630962818126 Loss: 0.00020191496017443609\n",
      "Iteration: 8338 lambda_n: 0.9010427277898776 Loss: 0.000201914960174436\n",
      "Iteration: 8339 lambda_n: 1.023085345050205 Loss: 0.00020191496017443603\n",
      "Iteration: 8340 lambda_n: 1.0212840594858759 Loss: 0.00020191496017443598\n",
      "Iteration: 8341 lambda_n: 0.93783728164081 Loss: 0.00020191496017443603\n",
      "Iteration: 8342 lambda_n: 0.9291195675389124 Loss: 0.00020191496017443609\n",
      "Iteration: 8343 lambda_n: 0.9540330183187932 Loss: 0.000201914960174436\n",
      "Iteration: 8344 lambda_n: 1.0374174952506232 Loss: 0.00020191496017443603\n",
      "Iteration: 8345 lambda_n: 1.0186243537656356 Loss: 0.00020191496017443598\n",
      "Iteration: 8346 lambda_n: 0.9266203234672672 Loss: 0.00020191496017443603\n",
      "Iteration: 8347 lambda_n: 0.9521719271640239 Loss: 0.00020191496017443609\n",
      "Iteration: 8348 lambda_n: 1.0037536065905837 Loss: 0.000201914960174436\n",
      "Iteration: 8349 lambda_n: 1.0309211042660003 Loss: 0.00020191496017443603\n",
      "Iteration: 8350 lambda_n: 0.8840221631879253 Loss: 0.00020191496017443598\n",
      "Iteration: 8351 lambda_n: 1.003914464254247 Loss: 0.00020191496017443603\n",
      "Iteration: 8352 lambda_n: 0.8832595735851537 Loss: 0.00020191496017443609\n",
      "Iteration: 8353 lambda_n: 0.9414450333528814 Loss: 0.000201914960174436\n",
      "Iteration: 8354 lambda_n: 0.9859141455354308 Loss: 0.00020191496017443606\n",
      "Iteration: 8355 lambda_n: 0.9730112974352038 Loss: 0.00020191496017443598\n",
      "Iteration: 8356 lambda_n: 1.0072744608409623 Loss: 0.00020191496017443603\n",
      "Iteration: 8357 lambda_n: 0.9690083639241268 Loss: 0.00020191496017443609\n",
      "Iteration: 8358 lambda_n: 0.9010143366004751 Loss: 0.000201914960174436\n",
      "Iteration: 8359 lambda_n: 0.9994822137834215 Loss: 0.00020191496017443603\n",
      "Iteration: 8360 lambda_n: 1.032848887624169 Loss: 0.00020191496017443598\n",
      "Iteration: 8361 lambda_n: 1.0301756967702675 Loss: 0.00020191496017443603\n",
      "Iteration: 8362 lambda_n: 0.9516284187040143 Loss: 0.00020191496017443609\n",
      "Iteration: 8363 lambda_n: 1.020402556333977 Loss: 0.000201914960174436\n",
      "Iteration: 8364 lambda_n: 1.0366086391096185 Loss: 0.00020191496017443603\n",
      "Iteration: 8365 lambda_n: 1.003476527449938 Loss: 0.00020191496017443598\n",
      "Iteration: 8366 lambda_n: 0.8930183633047641 Loss: 0.00020191496017443603\n",
      "Iteration: 8367 lambda_n: 0.9399773389124826 Loss: 0.00020191496017443609\n",
      "Iteration: 8368 lambda_n: 1.015208278973475 Loss: 0.000201914960174436\n",
      "Iteration: 8369 lambda_n: 0.9833674854826291 Loss: 0.00020191496017443603\n",
      "Iteration: 8370 lambda_n: 0.9297461197567382 Loss: 0.00020191496017443598\n",
      "Iteration: 8371 lambda_n: 1.0364076448242856 Loss: 0.00020191496017443603\n",
      "Iteration: 8372 lambda_n: 0.9921436578272799 Loss: 0.00020191496017443609\n",
      "Iteration: 8373 lambda_n: 1.0010508999393264 Loss: 0.000201914960174436\n",
      "Iteration: 8374 lambda_n: 0.8978707309924183 Loss: 0.00020191496017443603\n",
      "Iteration: 8375 lambda_n: 0.908301910183162 Loss: 0.00020191496017443598\n",
      "Iteration: 8376 lambda_n: 0.8993813794146096 Loss: 0.00020191496017443603\n",
      "Iteration: 8377 lambda_n: 0.9226864503116103 Loss: 0.00020191496017443609\n",
      "Iteration: 8378 lambda_n: 1.0362193081447943 Loss: 0.000201914960174436\n",
      "Iteration: 8379 lambda_n: 1.034370027118516 Loss: 0.00020191496017443603\n",
      "Iteration: 8380 lambda_n: 0.9086792278739065 Loss: 0.00020191496017443598\n",
      "Iteration: 8381 lambda_n: 1.0177238373008441 Loss: 0.00020191496017443603\n",
      "Iteration: 8382 lambda_n: 1.0182899464252821 Loss: 0.00020191496017443609\n",
      "Iteration: 8383 lambda_n: 0.9048836917404393 Loss: 0.000201914960174436\n",
      "Iteration: 8384 lambda_n: 0.9563896969824741 Loss: 0.00020191496017443603\n",
      "Iteration: 8385 lambda_n: 0.8983334397443752 Loss: 0.00020191496017443598\n",
      "Iteration: 8386 lambda_n: 0.9531241286232349 Loss: 0.00020191496017443603\n",
      "Iteration: 8387 lambda_n: 0.9033372038126474 Loss: 0.00020191496017443609\n",
      "Iteration: 8388 lambda_n: 1.037776421686174 Loss: 0.000201914960174436\n",
      "Iteration: 8389 lambda_n: 0.8945536478382 Loss: 0.00020191496017443606\n",
      "Iteration: 8390 lambda_n: 0.9683013348595065 Loss: 0.00020191496017443598\n",
      "Iteration: 8391 lambda_n: 0.9813501062996905 Loss: 0.00020191496017443603\n",
      "Iteration: 8392 lambda_n: 1.0043821946666618 Loss: 0.00020191496017443609\n",
      "Iteration: 8393 lambda_n: 1.0052383483485885 Loss: 0.000201914960174436\n",
      "Iteration: 8394 lambda_n: 1.001769977988699 Loss: 0.00020191496017443606\n",
      "Iteration: 8395 lambda_n: 1.0144998443727116 Loss: 0.00020191496017443598\n",
      "Iteration: 8396 lambda_n: 0.9016882385551026 Loss: 0.00020191496017443603\n",
      "Iteration: 8397 lambda_n: 0.9107868314775502 Loss: 0.00020191496017443609\n",
      "Iteration: 8398 lambda_n: 0.8933585092882907 Loss: 0.000201914960174436\n",
      "Iteration: 8399 lambda_n: 0.9948554835623241 Loss: 0.00020191496017443606\n",
      "Iteration: 8400 lambda_n: 0.8949853701356596 Loss: 0.00020191496017443598\n",
      "Iteration: 8401 lambda_n: 0.9392616950209292 Loss: 0.00020191496017443603\n",
      "Iteration: 8402 lambda_n: 0.9611760101720181 Loss: 0.00020191496017443609\n",
      "Iteration: 8403 lambda_n: 1.0008297260392995 Loss: 0.000201914960174436\n",
      "Iteration: 8404 lambda_n: 1.030676397652755 Loss: 0.00020191496017443603\n",
      "Iteration: 8405 lambda_n: 0.9207283048514981 Loss: 0.00020191496017443598\n",
      "Iteration: 8406 lambda_n: 0.9528674868561684 Loss: 0.00020191496017443603\n",
      "Iteration: 8407 lambda_n: 1.0178887770770921 Loss: 0.00020191496017443609\n",
      "Iteration: 8408 lambda_n: 0.9888400227074977 Loss: 0.000201914960174436\n",
      "Iteration: 8409 lambda_n: 0.9795062731956349 Loss: 0.00020191496017443603\n",
      "Iteration: 8410 lambda_n: 0.9546399396710877 Loss: 0.00020191496017443598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8411 lambda_n: 0.9502020833063107 Loss: 0.00020191496017443603\n",
      "Iteration: 8412 lambda_n: 0.965782630149396 Loss: 0.00020191496017443609\n",
      "Iteration: 8413 lambda_n: 1.0094459141906602 Loss: 0.000201914960174436\n",
      "Iteration: 8414 lambda_n: 1.0359747538618185 Loss: 0.00020191496017443603\n",
      "Iteration: 8415 lambda_n: 0.9183113570787141 Loss: 0.00020191496017443598\n",
      "Iteration: 8416 lambda_n: 1.006628638974422 Loss: 0.00020191496017443603\n",
      "Iteration: 8417 lambda_n: 0.9331444897142536 Loss: 0.00020191496017443609\n",
      "Iteration: 8418 lambda_n: 0.9868423319989476 Loss: 0.000201914960174436\n",
      "Iteration: 8419 lambda_n: 0.958029891959698 Loss: 0.00020191496017443603\n",
      "Iteration: 8420 lambda_n: 1.0005354797025356 Loss: 0.00020191496017443598\n",
      "Iteration: 8421 lambda_n: 0.9278798695739665 Loss: 0.00020191496017443603\n",
      "Iteration: 8422 lambda_n: 0.9723476581640685 Loss: 0.00020191496017443609\n",
      "Iteration: 8423 lambda_n: 1.0311178457691146 Loss: 0.000201914960174436\n",
      "Iteration: 8424 lambda_n: 0.9317366452263515 Loss: 0.00020191496017443603\n",
      "Iteration: 8425 lambda_n: 0.9822551242889644 Loss: 0.00020191496017443598\n",
      "Iteration: 8426 lambda_n: 1.0369406297926231 Loss: 0.00020191496017443603\n",
      "Iteration: 8427 lambda_n: 0.9052181416996892 Loss: 0.00020191496017443609\n",
      "Iteration: 8428 lambda_n: 0.9533540314812649 Loss: 0.000201914960174436\n",
      "Iteration: 8429 lambda_n: 0.9561482731780442 Loss: 0.00020191496017443606\n",
      "Iteration: 8430 lambda_n: 0.9644343880322452 Loss: 0.00020191496017443598\n",
      "Iteration: 8431 lambda_n: 0.886643438242637 Loss: 0.00020191496017443603\n",
      "Iteration: 8432 lambda_n: 0.9136821083909152 Loss: 0.00020191496017443609\n",
      "Iteration: 8433 lambda_n: 0.9683962321103191 Loss: 0.000201914960174436\n",
      "Iteration: 8434 lambda_n: 0.914966845729452 Loss: 0.00020191496017443606\n",
      "Iteration: 8435 lambda_n: 0.9205120681594581 Loss: 0.00020191496017443598\n",
      "Iteration: 8436 lambda_n: 0.921609678332789 Loss: 0.00020191496017443603\n",
      "Iteration: 8437 lambda_n: 0.9071624264087113 Loss: 0.00020191496017443609\n",
      "Iteration: 8438 lambda_n: 1.0318151626338097 Loss: 0.000201914960174436\n",
      "Iteration: 8439 lambda_n: 0.8964274289562999 Loss: 0.00020191496017443606\n",
      "Iteration: 8440 lambda_n: 0.9970473914635611 Loss: 0.00020191496017443598\n",
      "Iteration: 8441 lambda_n: 0.9088608703928002 Loss: 0.00020191496017443603\n",
      "Iteration: 8442 lambda_n: 0.919388889178038 Loss: 0.00020191496017443609\n",
      "Iteration: 8443 lambda_n: 0.9664982302820337 Loss: 0.00020191496017443516\n",
      "Iteration: 8444 lambda_n: 0.9997990544931497 Loss: 0.0002019149601744345\n",
      "Iteration: 8445 lambda_n: 1.012918156373949 Loss: 0.0002019149601744359\n",
      "Iteration: 8446 lambda_n: 0.9766889568769347 Loss: 0.0002019149601744349\n",
      "Iteration: 8447 lambda_n: 0.9331791788679731 Loss: 0.0002019149601744361\n",
      "Iteration: 8448 lambda_n: 0.9408968125840821 Loss: 0.000201914960174436\n",
      "Iteration: 8449 lambda_n: 0.9086343488390008 Loss: 0.00020191496017443595\n",
      "Iteration: 8450 lambda_n: 0.8944576793054434 Loss: 0.00020191496017443587\n",
      "Iteration: 8451 lambda_n: 0.9781222524192777 Loss: 0.00020191496017443584\n",
      "Iteration: 8452 lambda_n: 1.0231194300574316 Loss: 0.00020191496017443495\n",
      "Iteration: 8453 lambda_n: 0.9073815024144704 Loss: 0.00020191496017443535\n",
      "Iteration: 8454 lambda_n: 0.9980842834730903 Loss: 0.00020191496017443468\n",
      "Iteration: 8455 lambda_n: 1.0210804410495453 Loss: 0.00020191496017443703\n",
      "Iteration: 8456 lambda_n: 1.0019412655355242 Loss: 0.00020191496017443557\n",
      "Iteration: 8457 lambda_n: 1.0330040779002272 Loss: 0.00020191496017443468\n",
      "Iteration: 8458 lambda_n: 0.9686653566870698 Loss: 0.0002019149601744371\n",
      "Iteration: 8459 lambda_n: 1.0154006859536364 Loss: 0.00020191496017443565\n",
      "Iteration: 8460 lambda_n: 0.9976555385111925 Loss: 0.00020191496017443573\n",
      "Iteration: 8461 lambda_n: 0.9886510108279425 Loss: 0.00020191496017443478\n",
      "Iteration: 8462 lambda_n: 0.9350309269223361 Loss: 0.00020191496017443443\n",
      "Iteration: 8463 lambda_n: 0.9611218788727321 Loss: 0.00020191496017443584\n",
      "Iteration: 8464 lambda_n: 0.9482393588237816 Loss: 0.00020191496017443492\n",
      "Iteration: 8465 lambda_n: 1.0146800335151906 Loss: 0.00020191496017443454\n",
      "Iteration: 8466 lambda_n: 0.985630690461408 Loss: 0.0002019149601744359\n",
      "Iteration: 8467 lambda_n: 0.9532695126039373 Loss: 0.00020191496017443503\n",
      "Iteration: 8468 lambda_n: 0.9014194708551242 Loss: 0.00020191496017443625\n",
      "Iteration: 8469 lambda_n: 0.9150787559752442 Loss: 0.00020191496017443609\n",
      "Iteration: 8470 lambda_n: 0.9219395724152188 Loss: 0.00020191496017443598\n",
      "Iteration: 8471 lambda_n: 0.9979053489918035 Loss: 0.00020191496017443603\n",
      "Iteration: 8472 lambda_n: 1.014770268170555 Loss: 0.00020191496017443609\n",
      "Iteration: 8473 lambda_n: 1.0008576207829067 Loss: 0.000201914960174436\n",
      "Iteration: 8474 lambda_n: 0.8958423261895779 Loss: 0.00020191496017443603\n",
      "Iteration: 8475 lambda_n: 0.9926080446229247 Loss: 0.00020191496017443598\n",
      "Iteration: 8476 lambda_n: 0.9470854439848995 Loss: 0.00020191496017443603\n",
      "Iteration: 8477 lambda_n: 0.8844067658095122 Loss: 0.00020191496017443609\n",
      "Iteration: 8478 lambda_n: 0.8983855890896558 Loss: 0.000201914960174436\n",
      "Iteration: 8479 lambda_n: 0.9175853719847749 Loss: 0.00020191496017443606\n",
      "Iteration: 8480 lambda_n: 0.9385184415283339 Loss: 0.00020191496017443598\n",
      "Iteration: 8481 lambda_n: 0.9698481611529577 Loss: 0.00020191496017443603\n",
      "Iteration: 8482 lambda_n: 0.9099761852925669 Loss: 0.00020191496017443609\n",
      "Iteration: 8483 lambda_n: 0.9082489316955104 Loss: 0.000201914960174436\n",
      "Iteration: 8484 lambda_n: 1.0379981665494473 Loss: 0.00020191496017443606\n",
      "Iteration: 8485 lambda_n: 1.0264827836299306 Loss: 0.00020191496017443598\n",
      "Iteration: 8486 lambda_n: 0.9379956976655838 Loss: 0.00020191496017443603\n",
      "Iteration: 8487 lambda_n: 0.9564824690813339 Loss: 0.00020191496017443609\n",
      "Iteration: 8488 lambda_n: 0.9108078331965646 Loss: 0.000201914960174436\n",
      "Iteration: 8489 lambda_n: 0.970106197154405 Loss: 0.00020191496017443603\n",
      "Iteration: 8490 lambda_n: 1.0217021937513093 Loss: 0.00020191496017443598\n",
      "Iteration: 8491 lambda_n: 0.9235809585340622 Loss: 0.00020191496017443603\n",
      "Iteration: 8492 lambda_n: 1.0346336090661474 Loss: 0.00020191496017443609\n",
      "Iteration: 8493 lambda_n: 1.012671363687429 Loss: 0.000201914960174436\n",
      "Iteration: 8494 lambda_n: 0.9705922204182861 Loss: 0.00020191496017443603\n",
      "Iteration: 8495 lambda_n: 0.8868594918381716 Loss: 0.00020191496017443598\n",
      "Iteration: 8496 lambda_n: 0.9234215948166763 Loss: 0.00020191496017443603\n",
      "Iteration: 8497 lambda_n: 0.9404334138020167 Loss: 0.00020191496017443609\n",
      "Iteration: 8498 lambda_n: 0.9634880895055129 Loss: 0.000201914960174436\n",
      "Iteration: 8499 lambda_n: 0.9495347337335835 Loss: 0.00020191496017443603\n",
      "Iteration: 8500 lambda_n: 0.9144038063208545 Loss: 0.00020191496017443598\n",
      "Iteration: 8501 lambda_n: 0.9627988303369296 Loss: 0.00020191496017443603\n",
      "Iteration: 8502 lambda_n: 0.9253896183629231 Loss: 0.00020191496017443609\n",
      "Iteration: 8503 lambda_n: 0.9129901848774491 Loss: 0.000201914960174436\n",
      "Iteration: 8504 lambda_n: 0.9888337195902753 Loss: 0.00020191496017443603\n",
      "Iteration: 8505 lambda_n: 0.9372363701423879 Loss: 0.00020191496017443598\n",
      "Iteration: 8506 lambda_n: 0.9503289034753967 Loss: 0.00020191496017443603\n",
      "Iteration: 8507 lambda_n: 0.9790942645252979 Loss: 0.00020191496017443609\n",
      "Iteration: 8508 lambda_n: 1.0118260803287111 Loss: 0.000201914960174436\n",
      "Iteration: 8509 lambda_n: 0.9035888766825151 Loss: 0.00020191496017443603\n",
      "Iteration: 8510 lambda_n: 1.0338291738384373 Loss: 0.00020191496017443598\n",
      "Iteration: 8511 lambda_n: 0.9882604149602663 Loss: 0.00020191496017443603\n",
      "Iteration: 8512 lambda_n: 0.9790078484772471 Loss: 0.00020191496017443609\n",
      "Iteration: 8513 lambda_n: 0.9366926700423122 Loss: 0.000201914960174436\n",
      "Iteration: 8514 lambda_n: 1.0176749308836477 Loss: 0.00020191496017443603\n",
      "Iteration: 8515 lambda_n: 0.920159895482301 Loss: 0.00020191496017443598\n",
      "Iteration: 8516 lambda_n: 0.9741884826838971 Loss: 0.00020191496017443603\n",
      "Iteration: 8517 lambda_n: 0.9854876779137139 Loss: 0.00020191496017443609\n",
      "Iteration: 8518 lambda_n: 0.9916454750138808 Loss: 0.000201914960174436\n",
      "Iteration: 8519 lambda_n: 1.012048758197456 Loss: 0.00020191496017443603\n",
      "Iteration: 8520 lambda_n: 0.9278167939191948 Loss: 0.00020191496017443598\n",
      "Iteration: 8521 lambda_n: 1.0192220642926881 Loss: 0.00020191496017443603\n",
      "Iteration: 8522 lambda_n: 0.9244363914000072 Loss: 0.00020191496017443609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8523 lambda_n: 1.0025232040890022 Loss: 0.000201914960174436\n",
      "Iteration: 8524 lambda_n: 0.9470780915910931 Loss: 0.00020191496017443603\n",
      "Iteration: 8525 lambda_n: 1.0221787760330527 Loss: 0.00020191496017443598\n",
      "Iteration: 8526 lambda_n: 0.9397795090385369 Loss: 0.00020191496017443603\n",
      "Iteration: 8527 lambda_n: 0.8969946980932727 Loss: 0.00020191496017443609\n",
      "Iteration: 8528 lambda_n: 0.9730679655639576 Loss: 0.000201914960174436\n",
      "Iteration: 8529 lambda_n: 0.8931200087665417 Loss: 0.00020191496017443606\n",
      "Iteration: 8530 lambda_n: 0.9948718917964396 Loss: 0.00020191496017443598\n",
      "Iteration: 8531 lambda_n: 0.9428430799171704 Loss: 0.00020191496017443603\n",
      "Iteration: 8532 lambda_n: 0.892342531044281 Loss: 0.00020191496017443609\n",
      "Iteration: 8533 lambda_n: 1.010323036913188 Loss: 0.000201914960174436\n",
      "Iteration: 8534 lambda_n: 1.014804911563688 Loss: 0.00020191496017443606\n",
      "Iteration: 8535 lambda_n: 0.9467496781049072 Loss: 0.00020191496017443598\n",
      "Iteration: 8536 lambda_n: 0.9222052211710943 Loss: 0.00020191496017443603\n",
      "Iteration: 8537 lambda_n: 0.9467922843860894 Loss: 0.00020191496017443609\n",
      "Iteration: 8538 lambda_n: 0.9990100663339654 Loss: 0.000201914960174436\n",
      "Iteration: 8539 lambda_n: 0.8977987806963575 Loss: 0.00020191496017443606\n",
      "Iteration: 8540 lambda_n: 0.9228707382291945 Loss: 0.00020191496017443598\n",
      "Iteration: 8541 lambda_n: 0.963840914019524 Loss: 0.00020191496017443603\n",
      "Iteration: 8542 lambda_n: 0.9089275568190666 Loss: 0.00020191496017443609\n",
      "Iteration: 8543 lambda_n: 1.0065064679511444 Loss: 0.000201914960174436\n",
      "Iteration: 8544 lambda_n: 1.009721518395507 Loss: 0.00020191496017443606\n",
      "Iteration: 8545 lambda_n: 0.9721883856266222 Loss: 0.00020191496017443598\n",
      "Iteration: 8546 lambda_n: 1.0122345491849611 Loss: 0.00020191496017443603\n",
      "Iteration: 8547 lambda_n: 1.0344077582305526 Loss: 0.00020191496017443609\n",
      "Iteration: 8548 lambda_n: 1.0058382698022788 Loss: 0.000201914960174436\n",
      "Iteration: 8549 lambda_n: 1.0162285057104936 Loss: 0.00020191496017443606\n",
      "Iteration: 8550 lambda_n: 0.8897091154323536 Loss: 0.00020191496017443598\n",
      "Iteration: 8551 lambda_n: 0.9545541600957795 Loss: 0.00020191496017443603\n",
      "Iteration: 8552 lambda_n: 0.9947307015545749 Loss: 0.00020191496017443609\n",
      "Iteration: 8553 lambda_n: 0.9875385394038765 Loss: 0.000201914960174436\n",
      "Iteration: 8554 lambda_n: 1.0189020746693935 Loss: 0.00020191496017443603\n",
      "Iteration: 8555 lambda_n: 0.977288366145498 Loss: 0.00020191496017443598\n",
      "Iteration: 8556 lambda_n: 1.0323688020361117 Loss: 0.00020191496017443603\n",
      "Iteration: 8557 lambda_n: 1.0368829959156216 Loss: 0.00020191496017443609\n",
      "Iteration: 8558 lambda_n: 0.9626547014050111 Loss: 0.000201914960174436\n",
      "Iteration: 8559 lambda_n: 0.9516733406189746 Loss: 0.00020191496017443603\n",
      "Iteration: 8560 lambda_n: 0.8889865605259138 Loss: 0.00020191496017443598\n",
      "Iteration: 8561 lambda_n: 0.9835375481291418 Loss: 0.00020191496017443603\n",
      "Iteration: 8562 lambda_n: 0.9321349285951026 Loss: 0.00020191496017443609\n",
      "Iteration: 8563 lambda_n: 0.9805320053716339 Loss: 0.000201914960174436\n",
      "Iteration: 8564 lambda_n: 0.886382718327727 Loss: 0.00020191496017443603\n",
      "Iteration: 8565 lambda_n: 0.9779778614029733 Loss: 0.00020191496017443598\n",
      "Iteration: 8566 lambda_n: 1.01948733273859 Loss: 0.00020191496017443603\n",
      "Iteration: 8567 lambda_n: 1.0270309282471293 Loss: 0.00020191496017443609\n",
      "Iteration: 8568 lambda_n: 1.0092581599217783 Loss: 0.000201914960174436\n",
      "Iteration: 8569 lambda_n: 0.9646844584288752 Loss: 0.00020191496017443603\n",
      "Iteration: 8570 lambda_n: 0.908533624237669 Loss: 0.00020191496017443598\n",
      "Iteration: 8571 lambda_n: 1.0335723319621593 Loss: 0.00020191496017443603\n",
      "Iteration: 8572 lambda_n: 0.9341903650059231 Loss: 0.00020191496017443609\n",
      "Iteration: 8573 lambda_n: 1.0170481476954614 Loss: 0.000201914960174436\n",
      "Iteration: 8574 lambda_n: 0.9033443719830234 Loss: 0.00020191496017443603\n",
      "Iteration: 8575 lambda_n: 0.9107209213966148 Loss: 0.00020191496017443598\n",
      "Iteration: 8576 lambda_n: 0.8902231569039223 Loss: 0.00020191496017443603\n",
      "Iteration: 8577 lambda_n: 0.9277205214580589 Loss: 0.00020191496017443609\n",
      "Iteration: 8578 lambda_n: 0.9908308514113477 Loss: 0.000201914960174436\n",
      "Iteration: 8579 lambda_n: 0.972877723943731 Loss: 0.00020191496017443603\n",
      "Iteration: 8580 lambda_n: 0.9288452973117802 Loss: 0.00020191496017443598\n",
      "Iteration: 8581 lambda_n: 0.9992302362844141 Loss: 0.00020191496017443603\n",
      "Iteration: 8582 lambda_n: 1.0195433340193856 Loss: 0.00020191496017443609\n",
      "Iteration: 8583 lambda_n: 1.0115894521730684 Loss: 0.000201914960174436\n",
      "Iteration: 8584 lambda_n: 0.9636219746698446 Loss: 0.00020191496017443603\n",
      "Iteration: 8585 lambda_n: 0.99675311248533 Loss: 0.00020191496017443598\n",
      "Iteration: 8586 lambda_n: 0.9804760423236528 Loss: 0.00020191496017443603\n",
      "Iteration: 8587 lambda_n: 0.9653230110725914 Loss: 0.00020191496017443609\n",
      "Iteration: 8588 lambda_n: 1.0335510651037705 Loss: 0.000201914960174436\n",
      "Iteration: 8589 lambda_n: 0.99975440862764 Loss: 0.00020191496017443603\n",
      "Iteration: 8590 lambda_n: 0.9089762537939765 Loss: 0.00020191496017443598\n",
      "Iteration: 8591 lambda_n: 0.9337321728638861 Loss: 0.00020191496017443603\n",
      "Iteration: 8592 lambda_n: 0.9261440625307087 Loss: 0.00020191496017443609\n",
      "Iteration: 8593 lambda_n: 0.9958119245019852 Loss: 0.000201914960174436\n",
      "Iteration: 8594 lambda_n: 0.973109377362882 Loss: 0.00020191496017443603\n",
      "Iteration: 8595 lambda_n: 1.0021964497154128 Loss: 0.00020191496017443598\n",
      "Iteration: 8596 lambda_n: 0.9296271512743955 Loss: 0.00020191496017443603\n",
      "Iteration: 8597 lambda_n: 1.0178589147189017 Loss: 0.00020191496017443609\n",
      "Iteration: 8598 lambda_n: 0.9653158506454383 Loss: 0.000201914960174436\n",
      "Iteration: 8599 lambda_n: 1.0125976767376452 Loss: 0.00020191496017443603\n",
      "Iteration: 8600 lambda_n: 1.016822847021106 Loss: 0.00020191496017443598\n",
      "Iteration: 8601 lambda_n: 0.9095106645080305 Loss: 0.00020191496017443603\n",
      "Iteration: 8602 lambda_n: 0.9151442142120967 Loss: 0.00020191496017443609\n",
      "Iteration: 8603 lambda_n: 0.887985198022136 Loss: 0.000201914960174436\n",
      "Iteration: 8604 lambda_n: 0.9328465708472271 Loss: 0.00020191496017443606\n",
      "Iteration: 8605 lambda_n: 0.9003214357006095 Loss: 0.00020191496017443598\n",
      "Iteration: 8606 lambda_n: 0.9235320631028382 Loss: 0.00020191496017443603\n",
      "Iteration: 8607 lambda_n: 0.9903242921723896 Loss: 0.00020191496017443609\n",
      "Iteration: 8608 lambda_n: 0.9455422800097799 Loss: 0.000201914960174436\n",
      "Iteration: 8609 lambda_n: 1.0244360473587157 Loss: 0.00020191496017443603\n",
      "Iteration: 8610 lambda_n: 0.9149054750419005 Loss: 0.00020191496017443598\n",
      "Iteration: 8611 lambda_n: 0.9567959230333681 Loss: 0.00020191496017443603\n",
      "Iteration: 8612 lambda_n: 0.8938229835288721 Loss: 0.00020191496017443609\n",
      "Iteration: 8613 lambda_n: 1.0280200964637722 Loss: 0.000201914960174436\n",
      "Iteration: 8614 lambda_n: 0.9839902667663531 Loss: 0.00020191496017443606\n",
      "Iteration: 8615 lambda_n: 0.931144773567991 Loss: 0.00020191496017443598\n",
      "Iteration: 8616 lambda_n: 0.9638234380194377 Loss: 0.00020191496017443603\n",
      "Iteration: 8617 lambda_n: 1.0202420919146369 Loss: 0.00020191496017443609\n",
      "Iteration: 8618 lambda_n: 0.9050236669119165 Loss: 0.000201914960174436\n",
      "Iteration: 8619 lambda_n: 0.8975531215778089 Loss: 0.00020191496017443603\n",
      "Iteration: 8620 lambda_n: 1.035775172737143 Loss: 0.00020191496017443598\n",
      "Iteration: 8621 lambda_n: 1.0337049333736494 Loss: 0.00020191496017443603\n",
      "Iteration: 8622 lambda_n: 0.9193834998883914 Loss: 0.00020191496017443609\n",
      "Iteration: 8623 lambda_n: 0.9082868623373807 Loss: 0.000201914960174436\n",
      "Iteration: 8624 lambda_n: 1.0105454258342639 Loss: 0.00020191496017443603\n",
      "Iteration: 8625 lambda_n: 1.020519724705071 Loss: 0.00020191496017443598\n",
      "Iteration: 8626 lambda_n: 0.9951195305426557 Loss: 0.00020191496017443603\n",
      "Iteration: 8627 lambda_n: 1.0153540473684302 Loss: 0.00020191496017443609\n",
      "Iteration: 8628 lambda_n: 0.9860534754418071 Loss: 0.000201914960174436\n",
      "Iteration: 8629 lambda_n: 1.0335767556510984 Loss: 0.00020191496017443603\n",
      "Iteration: 8630 lambda_n: 1.0072151079404672 Loss: 0.00020191496017443598\n",
      "Iteration: 8631 lambda_n: 0.9662609353430843 Loss: 0.00020191496017443603\n",
      "Iteration: 8632 lambda_n: 0.9544003656150427 Loss: 0.00020191496017443609\n",
      "Iteration: 8633 lambda_n: 1.0193458025093218 Loss: 0.000201914960174436\n",
      "Iteration: 8634 lambda_n: 0.8906842531050981 Loss: 0.00020191496017443603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8635 lambda_n: 0.9142630267483153 Loss: 0.00020191496017443598\n",
      "Iteration: 8636 lambda_n: 1.0155106362822697 Loss: 0.00020191496017443603\n",
      "Iteration: 8637 lambda_n: 1.0024080966788316 Loss: 0.00020191496017443609\n",
      "Iteration: 8638 lambda_n: 1.0259236162093726 Loss: 0.000201914960174436\n",
      "Iteration: 8639 lambda_n: 0.9921790495314244 Loss: 0.00020191496017443603\n",
      "Iteration: 8640 lambda_n: 0.9742734796577374 Loss: 0.00020191496017443598\n",
      "Iteration: 8641 lambda_n: 0.9193181543368166 Loss: 0.00020191496017443603\n",
      "Iteration: 8642 lambda_n: 0.9499690725132691 Loss: 0.00020191496017443609\n",
      "Iteration: 8643 lambda_n: 0.9971393819346185 Loss: 0.000201914960174436\n",
      "Iteration: 8644 lambda_n: 0.9461760158946771 Loss: 0.00020191496017443603\n",
      "Iteration: 8645 lambda_n: 0.9288781455892717 Loss: 0.00020191496017443598\n",
      "Iteration: 8646 lambda_n: 0.9354388088356775 Loss: 0.00020191496017443603\n",
      "Iteration: 8647 lambda_n: 0.88566927669891 Loss: 0.00020191496017443609\n",
      "Iteration: 8648 lambda_n: 0.9119289399345744 Loss: 0.000201914960174436\n",
      "Iteration: 8649 lambda_n: 0.930028377273273 Loss: 0.00020191496017443606\n",
      "Iteration: 8650 lambda_n: 0.9269130594762715 Loss: 0.00020191496017443598\n",
      "Iteration: 8651 lambda_n: 0.9255112381775837 Loss: 0.00020191496017443603\n",
      "Iteration: 8652 lambda_n: 1.0018316034468269 Loss: 0.00020191496017443609\n",
      "Iteration: 8653 lambda_n: 1.0324904950801308 Loss: 0.000201914960174436\n",
      "Iteration: 8654 lambda_n: 0.9769621592404317 Loss: 0.00020191496017443603\n",
      "Iteration: 8655 lambda_n: 1.0078801399613597 Loss: 0.00020191496017443598\n",
      "Iteration: 8656 lambda_n: 0.886984115843537 Loss: 0.00020191496017443603\n",
      "Iteration: 8657 lambda_n: 1.0321017388159397 Loss: 0.00020191496017443609\n",
      "Iteration: 8658 lambda_n: 0.9875439417975682 Loss: 0.000201914960174436\n",
      "Iteration: 8659 lambda_n: 1.01225129744303 Loss: 0.00020191496017443603\n",
      "Iteration: 8660 lambda_n: 1.0210761444772325 Loss: 0.00020191496017443598\n",
      "Iteration: 8661 lambda_n: 0.9443244219655047 Loss: 0.00020191496017443603\n",
      "Iteration: 8662 lambda_n: 0.9612751652513237 Loss: 0.00020191496017443609\n",
      "Iteration: 8663 lambda_n: 0.9098285844390815 Loss: 0.000201914960174436\n",
      "Iteration: 8664 lambda_n: 0.9911075731013991 Loss: 0.00020191496017443603\n",
      "Iteration: 8665 lambda_n: 0.8902765081482167 Loss: 0.00020191496017443598\n",
      "Iteration: 8666 lambda_n: 0.929542453514587 Loss: 0.00020191496017443603\n",
      "Iteration: 8667 lambda_n: 0.9684942350235868 Loss: 0.00020191496017443609\n",
      "Iteration: 8668 lambda_n: 0.9010704438740227 Loss: 0.000201914960174436\n",
      "Iteration: 8669 lambda_n: 0.9752095666991774 Loss: 0.00020191496017443603\n",
      "Iteration: 8670 lambda_n: 0.9895129809346697 Loss: 0.00020191496017443598\n",
      "Iteration: 8671 lambda_n: 0.9364224856730465 Loss: 0.00020191496017443603\n",
      "Iteration: 8672 lambda_n: 0.9177245161042713 Loss: 0.00020191496017443609\n",
      "Iteration: 8673 lambda_n: 1.0345845639522924 Loss: 0.000201914960174436\n",
      "Iteration: 8674 lambda_n: 0.8958647131938247 Loss: 0.00020191496017443603\n",
      "Iteration: 8675 lambda_n: 1.0299333503458288 Loss: 0.00020191496017443598\n",
      "Iteration: 8676 lambda_n: 0.9359856064090593 Loss: 0.00020191496017443603\n",
      "Iteration: 8677 lambda_n: 0.9357213297714382 Loss: 0.00020191496017443609\n",
      "Iteration: 8678 lambda_n: 1.0253462378831457 Loss: 0.000201914960174436\n",
      "Iteration: 8679 lambda_n: 0.9043556453943156 Loss: 0.00020191496017443603\n",
      "Iteration: 8680 lambda_n: 0.9644504319195175 Loss: 0.00020191496017443598\n",
      "Iteration: 8681 lambda_n: 1.0271689892241551 Loss: 0.00020191496017443603\n",
      "Iteration: 8682 lambda_n: 1.0313566534554108 Loss: 0.00020191496017443609\n",
      "Iteration: 8683 lambda_n: 0.9779689150334988 Loss: 0.000201914960174436\n",
      "Iteration: 8684 lambda_n: 0.9283559455058294 Loss: 0.00020191496017443603\n",
      "Iteration: 8685 lambda_n: 0.9850193211580807 Loss: 0.00020191496017443598\n",
      "Iteration: 8686 lambda_n: 0.9198662022187172 Loss: 0.00020191496017443603\n",
      "Iteration: 8687 lambda_n: 0.9869486163781379 Loss: 0.00020191496017443609\n",
      "Iteration: 8688 lambda_n: 1.0198894735248538 Loss: 0.000201914960174436\n",
      "Iteration: 8689 lambda_n: 1.0308796464459198 Loss: 0.00020191496017443603\n",
      "Iteration: 8690 lambda_n: 1.0031832299286956 Loss: 0.00020191496017443598\n",
      "Iteration: 8691 lambda_n: 0.9302046643815587 Loss: 0.00020191496017443603\n",
      "Iteration: 8692 lambda_n: 0.9203311183250893 Loss: 0.00020191496017443609\n",
      "Iteration: 8693 lambda_n: 0.952090695470025 Loss: 0.000201914960174436\n",
      "Iteration: 8694 lambda_n: 0.9064325097534669 Loss: 0.00020191496017443603\n",
      "Iteration: 8695 lambda_n: 0.9367343488733137 Loss: 0.00020191496017443598\n",
      "Iteration: 8696 lambda_n: 0.9720297999961421 Loss: 0.00020191496017443603\n",
      "Iteration: 8697 lambda_n: 0.9181605285749961 Loss: 0.00020191496017443609\n",
      "Iteration: 8698 lambda_n: 1.0148180517783554 Loss: 0.000201914960174436\n",
      "Iteration: 8699 lambda_n: 1.0227540575063458 Loss: 0.00020191496017443603\n",
      "Iteration: 8700 lambda_n: 0.9511326444683516 Loss: 0.00020191496017443598\n",
      "Iteration: 8701 lambda_n: 1.0113315191040955 Loss: 0.00020191496017443603\n",
      "Iteration: 8702 lambda_n: 1.004942973648455 Loss: 0.00020191496017443609\n",
      "Iteration: 8703 lambda_n: 1.0241561197692157 Loss: 0.000201914960174436\n",
      "Iteration: 8704 lambda_n: 1.0007793297761258 Loss: 0.00020191496017443603\n",
      "Iteration: 8705 lambda_n: 0.9377814830012414 Loss: 0.00020191496017443598\n",
      "Iteration: 8706 lambda_n: 0.9615462325579579 Loss: 0.00020191496017443603\n",
      "Iteration: 8707 lambda_n: 0.9028690774097524 Loss: 0.00020191496017443609\n",
      "Iteration: 8708 lambda_n: 0.9908856003842146 Loss: 0.000201914960174436\n",
      "Iteration: 8709 lambda_n: 0.9213055026915864 Loss: 0.00020191496017443606\n",
      "Iteration: 8710 lambda_n: 0.9916129781620896 Loss: 0.00020191496017443598\n",
      "Iteration: 8711 lambda_n: 0.9029647677781149 Loss: 0.00020191496017443603\n",
      "Iteration: 8712 lambda_n: 1.0309321330093102 Loss: 0.00020191496017443609\n",
      "Iteration: 8713 lambda_n: 0.9523355643901134 Loss: 0.000201914960174436\n",
      "Iteration: 8714 lambda_n: 0.9413525245430688 Loss: 0.00020191496017443603\n",
      "Iteration: 8715 lambda_n: 0.9681985383705382 Loss: 0.00020191496017443598\n",
      "Iteration: 8716 lambda_n: 0.970174958949551 Loss: 0.00020191496017443603\n",
      "Iteration: 8717 lambda_n: 0.981230583262387 Loss: 0.00020191496017443609\n",
      "Iteration: 8718 lambda_n: 0.9242282630462472 Loss: 0.000201914960174436\n",
      "Iteration: 8719 lambda_n: 1.0046450881294515 Loss: 0.00020191496017443603\n",
      "Iteration: 8720 lambda_n: 0.9559200149352184 Loss: 0.00020191496017443598\n",
      "Iteration: 8721 lambda_n: 0.9602669739777858 Loss: 0.00020191496017443603\n",
      "Iteration: 8722 lambda_n: 0.9121697508252736 Loss: 0.00020191496017443609\n",
      "Iteration: 8723 lambda_n: 0.9158622162230582 Loss: 0.000201914960174436\n",
      "Iteration: 8724 lambda_n: 0.9793241364604253 Loss: 0.00020191496017443606\n",
      "Iteration: 8725 lambda_n: 0.97850124322282 Loss: 0.00020191496017443598\n",
      "Iteration: 8726 lambda_n: 0.9294272493363656 Loss: 0.00020191496017443603\n",
      "Iteration: 8727 lambda_n: 0.9314164069826784 Loss: 0.00020191496017443609\n",
      "Iteration: 8728 lambda_n: 0.8970826742275538 Loss: 0.000201914960174436\n",
      "Iteration: 8729 lambda_n: 1.013369087616491 Loss: 0.00020191496017443603\n",
      "Iteration: 8730 lambda_n: 0.9040713627888249 Loss: 0.00020191496017443598\n",
      "Iteration: 8731 lambda_n: 0.9836065128286585 Loss: 0.00020191496017443603\n",
      "Iteration: 8732 lambda_n: 1.0207078773654539 Loss: 0.00020191496017443609\n",
      "Iteration: 8733 lambda_n: 0.9942197668786814 Loss: 0.000201914960174436\n",
      "Iteration: 8734 lambda_n: 0.9607048465527571 Loss: 0.00020191496017443603\n",
      "Iteration: 8735 lambda_n: 0.9883869820670094 Loss: 0.00020191496017443598\n",
      "Iteration: 8736 lambda_n: 0.9448599824346081 Loss: 0.00020191496017443603\n",
      "Iteration: 8737 lambda_n: 1.0071268475652828 Loss: 0.00020191496017443609\n",
      "Iteration: 8738 lambda_n: 0.9631506333184721 Loss: 0.000201914960174436\n",
      "Iteration: 8739 lambda_n: 0.9240604177031575 Loss: 0.00020191496017443603\n",
      "Iteration: 8740 lambda_n: 0.9057094140591481 Loss: 0.00020191496017443598\n",
      "Iteration: 8741 lambda_n: 0.9680053239309332 Loss: 0.00020191496017443603\n",
      "Iteration: 8742 lambda_n: 0.8904659139939716 Loss: 0.00020191496017443609\n",
      "Iteration: 8743 lambda_n: 0.9969158112110744 Loss: 0.000201914960174436\n",
      "Iteration: 8744 lambda_n: 0.8909444007949798 Loss: 0.00020191496017443606\n",
      "Iteration: 8745 lambda_n: 0.8964762838725148 Loss: 0.00020191496017443598\n",
      "Iteration: 8746 lambda_n: 0.8937943020365564 Loss: 0.00020191496017443603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8747 lambda_n: 0.8944825455693197 Loss: 0.00020191496017443609\n",
      "Iteration: 8748 lambda_n: 0.9214117440848407 Loss: 0.000201914960174436\n",
      "Iteration: 8749 lambda_n: 0.9103406388952671 Loss: 0.00020191496017443606\n",
      "Iteration: 8750 lambda_n: 0.9763576252434075 Loss: 0.00020191496017443598\n",
      "Iteration: 8751 lambda_n: 1.0104862643951469 Loss: 0.00020191496017443603\n",
      "Iteration: 8752 lambda_n: 0.9427961981439444 Loss: 0.00020191496017443609\n",
      "Iteration: 8753 lambda_n: 1.0234695022774574 Loss: 0.00020191496017443516\n",
      "Iteration: 8754 lambda_n: 0.9564357721788269 Loss: 0.00020191496017443446\n",
      "Iteration: 8755 lambda_n: 0.8939794950541675 Loss: 0.00020191496017443584\n",
      "Iteration: 8756 lambda_n: 0.8983781710482771 Loss: 0.00020191496017443497\n",
      "Iteration: 8757 lambda_n: 0.9614859918539496 Loss: 0.00020191496017443443\n",
      "Iteration: 8758 lambda_n: 0.9787930833145172 Loss: 0.0002019149601744358\n",
      "Iteration: 8759 lambda_n: 0.9592192702552332 Loss: 0.00020191496017443487\n",
      "Iteration: 8760 lambda_n: 1.0205134723726808 Loss: 0.00020191496017443443\n",
      "Iteration: 8761 lambda_n: 0.9659134798395163 Loss: 0.00020191496017443598\n",
      "Iteration: 8762 lambda_n: 1.006703944748439 Loss: 0.000201914960174435\n",
      "Iteration: 8763 lambda_n: 1.019867504915758 Loss: 0.00020191496017443443\n",
      "Iteration: 8764 lambda_n: 0.8881549508005376 Loss: 0.00020191496017443587\n",
      "Iteration: 8765 lambda_n: 1.014667450148025 Loss: 0.00020191496017443495\n",
      "Iteration: 8766 lambda_n: 0.8958760267117107 Loss: 0.00020191496017443606\n",
      "Iteration: 8767 lambda_n: 0.9760350056767115 Loss: 0.00020191496017443603\n",
      "Iteration: 8768 lambda_n: 0.9051186219336657 Loss: 0.00020191496017443603\n",
      "Iteration: 8769 lambda_n: 0.9054164224833685 Loss: 0.00020191496017443603\n",
      "Iteration: 8770 lambda_n: 0.9560953062464297 Loss: 0.00020191496017443603\n",
      "Iteration: 8771 lambda_n: 0.9587564534963829 Loss: 0.00020191496017443595\n",
      "Iteration: 8772 lambda_n: 1.0198441292663856 Loss: 0.000201914960174436\n",
      "Iteration: 8773 lambda_n: 1.0321512905711712 Loss: 0.0002019149601744359\n",
      "Iteration: 8774 lambda_n: 0.9881966269314142 Loss: 0.00020191496017443598\n",
      "Iteration: 8775 lambda_n: 1.0292425376870444 Loss: 0.00020191496017443603\n",
      "Iteration: 8776 lambda_n: 0.8929160181567932 Loss: 0.00020191496017443595\n",
      "Iteration: 8777 lambda_n: 0.9576350176386592 Loss: 0.000201914960174436\n",
      "Iteration: 8778 lambda_n: 0.9550519934375205 Loss: 0.0002019149601744359\n",
      "Iteration: 8779 lambda_n: 0.8916065257049599 Loss: 0.00020191496017443598\n",
      "Iteration: 8780 lambda_n: 0.9194613742352175 Loss: 0.00020191496017443603\n",
      "Iteration: 8781 lambda_n: 0.9644972932886068 Loss: 0.00020191496017443595\n",
      "Iteration: 8782 lambda_n: 0.9054053009995664 Loss: 0.000201914960174436\n",
      "Iteration: 8783 lambda_n: 0.9082289923344171 Loss: 0.0002019149601744359\n",
      "Iteration: 8784 lambda_n: 1.033948457778604 Loss: 0.00020191496017443598\n",
      "Iteration: 8785 lambda_n: 0.8856922578882266 Loss: 0.00020191496017443603\n",
      "Iteration: 8786 lambda_n: 0.8859348719638058 Loss: 0.00020191496017443595\n",
      "Iteration: 8787 lambda_n: 0.9404543843986075 Loss: 0.000201914960174436\n",
      "Iteration: 8788 lambda_n: 0.9783087823648897 Loss: 0.0002019149601744359\n",
      "Iteration: 8789 lambda_n: 0.9876329186312202 Loss: 0.00020191496017443598\n",
      "Iteration: 8790 lambda_n: 1.0073591868416647 Loss: 0.00020191496017443603\n",
      "Iteration: 8791 lambda_n: 0.89028844097907 Loss: 0.00020191496017443595\n",
      "Iteration: 8792 lambda_n: 0.948703696275173 Loss: 0.000201914960174436\n",
      "Iteration: 8793 lambda_n: 0.8913528234969582 Loss: 0.0002019149601744359\n",
      "Iteration: 8794 lambda_n: 1.0261714741159442 Loss: 0.00020191496017443598\n",
      "Iteration: 8795 lambda_n: 0.9888258905802483 Loss: 0.00020191496017443603\n",
      "Iteration: 8796 lambda_n: 0.9848562708452631 Loss: 0.00020191496017443595\n",
      "Iteration: 8797 lambda_n: 0.9832607448114377 Loss: 0.000201914960174436\n",
      "Iteration: 8798 lambda_n: 0.8906383242217203 Loss: 0.0002019149601744359\n",
      "Iteration: 8799 lambda_n: 0.9281701344911714 Loss: 0.00020191496017443598\n",
      "Iteration: 8800 lambda_n: 0.9177945234691787 Loss: 0.00020191496017443603\n",
      "Iteration: 8801 lambda_n: 0.9984763954264808 Loss: 0.00020191496017443595\n",
      "Iteration: 8802 lambda_n: 0.9763223759213738 Loss: 0.000201914960174436\n",
      "Iteration: 8803 lambda_n: 0.9861832275693004 Loss: 0.0002019149601744359\n",
      "Iteration: 8804 lambda_n: 0.9566725529418468 Loss: 0.00020191496017443598\n",
      "Iteration: 8805 lambda_n: 1.0068903543652652 Loss: 0.00020191496017443603\n",
      "Iteration: 8806 lambda_n: 0.9387361580350821 Loss: 0.00020191496017443595\n",
      "Iteration: 8807 lambda_n: 0.9298012494921857 Loss: 0.000201914960174436\n",
      "Iteration: 8808 lambda_n: 0.9229137899257345 Loss: 0.0002019149601744359\n",
      "Iteration: 8809 lambda_n: 0.9702032783983128 Loss: 0.00020191496017443598\n",
      "Iteration: 8810 lambda_n: 0.9233294866029285 Loss: 0.00020191496017443603\n",
      "Iteration: 8811 lambda_n: 0.9195811405485018 Loss: 0.00020191496017443595\n",
      "Iteration: 8812 lambda_n: 0.9685564789241711 Loss: 0.000201914960174436\n",
      "Iteration: 8813 lambda_n: 0.9201363289700606 Loss: 0.0002019149601744359\n",
      "Iteration: 8814 lambda_n: 0.9022207489508799 Loss: 0.00020191496017443598\n",
      "Iteration: 8815 lambda_n: 0.9049940269986724 Loss: 0.00020191496017443603\n",
      "Iteration: 8816 lambda_n: 1.0014636653820248 Loss: 0.00020191496017443595\n",
      "Iteration: 8817 lambda_n: 0.9634633199015596 Loss: 0.000201914960174436\n",
      "Iteration: 8818 lambda_n: 0.9879216055921334 Loss: 0.0002019149601744359\n",
      "Iteration: 8819 lambda_n: 0.9652095414390203 Loss: 0.00020191496017443598\n",
      "Iteration: 8820 lambda_n: 0.9583379195837003 Loss: 0.00020191496017443603\n",
      "Iteration: 8821 lambda_n: 0.9151866800571642 Loss: 0.00020191496017443595\n",
      "Iteration: 8822 lambda_n: 1.0318745745955222 Loss: 0.000201914960174436\n",
      "Iteration: 8823 lambda_n: 0.9253920315410727 Loss: 0.0002019149601744359\n",
      "Iteration: 8824 lambda_n: 0.9298978182701959 Loss: 0.00020191496017443598\n",
      "Iteration: 8825 lambda_n: 0.9261142271338111 Loss: 0.00020191496017443603\n",
      "Iteration: 8826 lambda_n: 1.0219244386083346 Loss: 0.00020191496017443595\n",
      "Iteration: 8827 lambda_n: 0.9070853505243042 Loss: 0.000201914960174436\n",
      "Iteration: 8828 lambda_n: 0.8911961039881338 Loss: 0.0002019149601744359\n",
      "Iteration: 8829 lambda_n: 0.931663913172309 Loss: 0.00020191496017443598\n",
      "Iteration: 8830 lambda_n: 0.9114574130923926 Loss: 0.00020191496017443603\n",
      "Iteration: 8831 lambda_n: 0.9890044176684752 Loss: 0.00020191496017443595\n",
      "Iteration: 8832 lambda_n: 1.0104225300333411 Loss: 0.000201914960174436\n",
      "Iteration: 8833 lambda_n: 0.9695833537940154 Loss: 0.0002019149601744359\n",
      "Iteration: 8834 lambda_n: 1.014757489057667 Loss: 0.00020191496017443598\n",
      "Iteration: 8835 lambda_n: 0.9681318381063144 Loss: 0.00020191496017443603\n",
      "Iteration: 8836 lambda_n: 0.9482923137299549 Loss: 0.00020191496017443595\n",
      "Iteration: 8837 lambda_n: 0.9503413324342566 Loss: 0.000201914960174436\n",
      "Iteration: 8838 lambda_n: 1.0113426865033033 Loss: 0.0002019149601744359\n",
      "Iteration: 8839 lambda_n: 1.0378055537164026 Loss: 0.00020191496017443598\n",
      "Iteration: 8840 lambda_n: 0.9485827539675914 Loss: 0.00020191496017443603\n",
      "Iteration: 8841 lambda_n: 0.9618980159957189 Loss: 0.00020191496017443595\n",
      "Iteration: 8842 lambda_n: 0.9269553092912588 Loss: 0.000201914960174436\n",
      "Iteration: 8843 lambda_n: 0.9831323968423605 Loss: 0.0002019149601744359\n",
      "Iteration: 8844 lambda_n: 1.031652751053772 Loss: 0.00020191496017443598\n",
      "Iteration: 8845 lambda_n: 0.9297642371507808 Loss: 0.00020191496017443603\n",
      "Iteration: 8846 lambda_n: 1.0042902469667012 Loss: 0.00020191496017443595\n",
      "Iteration: 8847 lambda_n: 0.9868154696336107 Loss: 0.000201914960174436\n",
      "Iteration: 8848 lambda_n: 0.9101367667113001 Loss: 0.0002019149601744359\n",
      "Iteration: 8849 lambda_n: 0.9301122399045296 Loss: 0.00020191496017443598\n",
      "Iteration: 8850 lambda_n: 0.9640020055009092 Loss: 0.00020191496017443603\n",
      "Iteration: 8851 lambda_n: 0.9987662966467747 Loss: 0.00020191496017443595\n",
      "Iteration: 8852 lambda_n: 0.9699895887617346 Loss: 0.000201914960174436\n",
      "Iteration: 8853 lambda_n: 0.951255293412647 Loss: 0.0002019149601744359\n",
      "Iteration: 8854 lambda_n: 0.900166777858007 Loss: 0.00020191496017443598\n",
      "Iteration: 8855 lambda_n: 1.0283398293094042 Loss: 0.00020191496017443603\n",
      "Iteration: 8856 lambda_n: 0.9189578139583 Loss: 0.00020191496017443595\n",
      "Iteration: 8857 lambda_n: 0.9204057411942689 Loss: 0.000201914960174436\n",
      "Iteration: 8858 lambda_n: 0.9023925291411538 Loss: 0.0002019149601744359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8859 lambda_n: 0.883811843140567 Loss: 0.00020191496017443598\n",
      "Iteration: 8860 lambda_n: 1.0359767768699746 Loss: 0.00020191496017443603\n",
      "Iteration: 8861 lambda_n: 0.8884835132992739 Loss: 0.00020191496017443595\n",
      "Iteration: 8862 lambda_n: 0.90614270077875 Loss: 0.000201914960174436\n",
      "Iteration: 8863 lambda_n: 0.9424834056948114 Loss: 0.0002019149601744359\n",
      "Iteration: 8864 lambda_n: 0.9203557705371126 Loss: 0.00020191496017443598\n",
      "Iteration: 8865 lambda_n: 0.9909824458199986 Loss: 0.00020191496017443603\n",
      "Iteration: 8866 lambda_n: 0.9423646752874233 Loss: 0.00020191496017443595\n",
      "Iteration: 8867 lambda_n: 0.9096880979497671 Loss: 0.000201914960174436\n",
      "Iteration: 8868 lambda_n: 1.002117561338862 Loss: 0.0002019149601744359\n",
      "Iteration: 8869 lambda_n: 1.0349946481887222 Loss: 0.00020191496017443598\n",
      "Iteration: 8870 lambda_n: 0.9660934564999044 Loss: 0.00020191496017443603\n",
      "Iteration: 8871 lambda_n: 0.9799533721319938 Loss: 0.00020191496017443595\n",
      "Iteration: 8872 lambda_n: 1.0102808082167334 Loss: 0.000201914960174436\n",
      "Iteration: 8873 lambda_n: 0.9157648630019144 Loss: 0.0002019149601744359\n",
      "Iteration: 8874 lambda_n: 0.9177670576647263 Loss: 0.00020191496017443598\n",
      "Iteration: 8875 lambda_n: 1.0031536533531262 Loss: 0.00020191496017443603\n",
      "Iteration: 8876 lambda_n: 0.9559248629676088 Loss: 0.00020191496017443595\n",
      "Iteration: 8877 lambda_n: 1.005542089011977 Loss: 0.000201914960174436\n",
      "Iteration: 8878 lambda_n: 0.9099819606198253 Loss: 0.0002019149601744359\n",
      "Iteration: 8879 lambda_n: 0.9506470431156196 Loss: 0.00020191496017443598\n",
      "Iteration: 8880 lambda_n: 0.9461851019023199 Loss: 0.00020191496017443603\n",
      "Iteration: 8881 lambda_n: 0.9142693037259931 Loss: 0.00020191496017443595\n",
      "Iteration: 8882 lambda_n: 0.8953338318341476 Loss: 0.000201914960174436\n",
      "Iteration: 8883 lambda_n: 0.8897938454295268 Loss: 0.0002019149601744359\n",
      "Iteration: 8884 lambda_n: 0.9004217358032196 Loss: 0.00020191496017443598\n",
      "Iteration: 8885 lambda_n: 1.002545229762296 Loss: 0.00020191496017443603\n",
      "Iteration: 8886 lambda_n: 0.9792173877782172 Loss: 0.00020191496017443595\n",
      "Iteration: 8887 lambda_n: 1.030818456078102 Loss: 0.000201914960174436\n",
      "Iteration: 8888 lambda_n: 0.948021598969821 Loss: 0.0002019149601744359\n",
      "Iteration: 8889 lambda_n: 0.9570734562940888 Loss: 0.00020191496017443598\n",
      "Iteration: 8890 lambda_n: 0.9136070872197608 Loss: 0.00020191496017443603\n",
      "Iteration: 8891 lambda_n: 0.8942308853738731 Loss: 0.00020191496017443595\n",
      "Iteration: 8892 lambda_n: 0.9709425597654788 Loss: 0.000201914960174436\n",
      "Iteration: 8893 lambda_n: 0.9637896401702392 Loss: 0.0002019149601744359\n",
      "Iteration: 8894 lambda_n: 0.9538295100226998 Loss: 0.00020191496017443598\n",
      "Iteration: 8895 lambda_n: 0.8998198720952794 Loss: 0.00020191496017443603\n",
      "Iteration: 8896 lambda_n: 0.9042048034467125 Loss: 0.00020191496017443595\n",
      "Iteration: 8897 lambda_n: 0.90153721505011 Loss: 0.000201914960174436\n",
      "Iteration: 8898 lambda_n: 1.03514112379166 Loss: 0.0002019149601744359\n",
      "Iteration: 8899 lambda_n: 0.9312654661885911 Loss: 0.00020191496017443598\n",
      "Iteration: 8900 lambda_n: 1.0035470390192673 Loss: 0.00020191496017443603\n",
      "Iteration: 8901 lambda_n: 1.0208866189968338 Loss: 0.00020191496017443595\n",
      "Iteration: 8902 lambda_n: 0.9079237397400117 Loss: 0.000201914960174436\n",
      "Iteration: 8903 lambda_n: 1.030376070866291 Loss: 0.0002019149601744359\n",
      "Iteration: 8904 lambda_n: 1.0248831164412535 Loss: 0.00020191496017443598\n",
      "Iteration: 8905 lambda_n: 1.0259548682579407 Loss: 0.00020191496017443603\n",
      "Iteration: 8906 lambda_n: 1.0100674185594318 Loss: 0.00020191496017443595\n",
      "Iteration: 8907 lambda_n: 1.032841586614487 Loss: 0.000201914960174436\n",
      "Iteration: 8908 lambda_n: 0.9486353073907616 Loss: 0.0002019149601744359\n",
      "Iteration: 8909 lambda_n: 0.9377957035177087 Loss: 0.00020191496017443598\n",
      "Iteration: 8910 lambda_n: 0.9185625479686081 Loss: 0.00020191496017443603\n",
      "Iteration: 8911 lambda_n: 0.9250932457990493 Loss: 0.00020191496017443595\n",
      "Iteration: 8912 lambda_n: 0.9308835884564439 Loss: 0.000201914960174436\n",
      "Iteration: 8913 lambda_n: 0.9543888378069693 Loss: 0.0002019149601744359\n",
      "Iteration: 8914 lambda_n: 0.8996749031115897 Loss: 0.00020191496017443598\n",
      "Iteration: 8915 lambda_n: 1.0024142908930425 Loss: 0.00020191496017443603\n",
      "Iteration: 8916 lambda_n: 0.9024857377240072 Loss: 0.00020191496017443595\n",
      "Iteration: 8917 lambda_n: 0.9909777751570931 Loss: 0.000201914960174436\n",
      "Iteration: 8918 lambda_n: 0.9556366697730733 Loss: 0.0002019149601744359\n",
      "Iteration: 8919 lambda_n: 0.9133663071667029 Loss: 0.00020191496017443598\n",
      "Iteration: 8920 lambda_n: 1.0146556563599871 Loss: 0.00020191496017443603\n",
      "Iteration: 8921 lambda_n: 1.0298702984392374 Loss: 0.00020191496017443595\n",
      "Iteration: 8922 lambda_n: 0.9903312935968357 Loss: 0.000201914960174436\n",
      "Iteration: 8923 lambda_n: 1.014882047583998 Loss: 0.0002019149601744359\n",
      "Iteration: 8924 lambda_n: 0.8868460966879377 Loss: 0.00020191496017443598\n",
      "Iteration: 8925 lambda_n: 0.9160472536032207 Loss: 0.00020191496017443603\n",
      "Iteration: 8926 lambda_n: 0.9959616473509716 Loss: 0.00020191496017443595\n",
      "Iteration: 8927 lambda_n: 0.9518844667809379 Loss: 0.000201914960174436\n",
      "Iteration: 8928 lambda_n: 0.9161987247873168 Loss: 0.0002019149601744359\n",
      "Iteration: 8929 lambda_n: 1.0306674755682856 Loss: 0.00020191496017443598\n",
      "Iteration: 8930 lambda_n: 0.8847577283052095 Loss: 0.00020191496017443603\n",
      "Iteration: 8931 lambda_n: 0.9720614983525894 Loss: 0.00020191496017443595\n",
      "Iteration: 8932 lambda_n: 1.003898924172715 Loss: 0.000201914960174436\n",
      "Iteration: 8933 lambda_n: 1.020079025448032 Loss: 0.0002019149601744359\n",
      "Iteration: 8934 lambda_n: 0.9236894029232579 Loss: 0.00020191496017443598\n",
      "Iteration: 8935 lambda_n: 0.9686931300529821 Loss: 0.00020191496017443603\n",
      "Iteration: 8936 lambda_n: 0.8973853505050406 Loss: 0.00020191496017443595\n",
      "Iteration: 8937 lambda_n: 0.8881915190948929 Loss: 0.000201914960174436\n",
      "Iteration: 8938 lambda_n: 1.035093151418554 Loss: 0.0002019149601744359\n",
      "Iteration: 8939 lambda_n: 0.8845058248486869 Loss: 0.00020191496017443598\n",
      "Iteration: 8940 lambda_n: 0.9623662176958139 Loss: 0.00020191496017443603\n",
      "Iteration: 8941 lambda_n: 0.9646749118292555 Loss: 0.00020191496017443595\n",
      "Iteration: 8942 lambda_n: 0.9558499607638961 Loss: 0.000201914960174436\n",
      "Iteration: 8943 lambda_n: 0.9972393872064136 Loss: 0.0002019149601744359\n",
      "Iteration: 8944 lambda_n: 0.9721831264662344 Loss: 0.00020191496017443598\n",
      "Iteration: 8945 lambda_n: 0.9367394613288706 Loss: 0.00020191496017443603\n",
      "Iteration: 8946 lambda_n: 0.973782813453877 Loss: 0.00020191496017443595\n",
      "Iteration: 8947 lambda_n: 1.003342426607549 Loss: 0.000201914960174436\n",
      "Iteration: 8948 lambda_n: 0.8982820535927816 Loss: 0.0002019149601744359\n",
      "Iteration: 8949 lambda_n: 0.8891197008925006 Loss: 0.00020191496017443598\n",
      "Iteration: 8950 lambda_n: 0.9103695712813031 Loss: 0.00020191496017443603\n",
      "Iteration: 8951 lambda_n: 1.0051893947089223 Loss: 0.00020191496017443595\n",
      "Iteration: 8952 lambda_n: 1.0297824085278555 Loss: 0.000201914960174436\n",
      "Iteration: 8953 lambda_n: 0.9774503334161888 Loss: 0.0002019149601744359\n",
      "Iteration: 8954 lambda_n: 0.935758852825645 Loss: 0.00020191496017443598\n",
      "Iteration: 8955 lambda_n: 1.0244763226382607 Loss: 0.00020191496017443603\n",
      "Iteration: 8956 lambda_n: 0.9076309941558139 Loss: 0.00020191496017443595\n",
      "Iteration: 8957 lambda_n: 0.9266832132969326 Loss: 0.000201914960174436\n",
      "Iteration: 8958 lambda_n: 0.9400440131714043 Loss: 0.0002019149601744359\n",
      "Iteration: 8959 lambda_n: 0.93265896001011 Loss: 0.00020191496017443598\n",
      "Iteration: 8960 lambda_n: 0.9560411730095363 Loss: 0.00020191496017443603\n",
      "Iteration: 8961 lambda_n: 0.9816838644271899 Loss: 0.00020191496017443595\n",
      "Iteration: 8962 lambda_n: 1.0130801738650232 Loss: 0.000201914960174436\n",
      "Iteration: 8963 lambda_n: 0.987593254907985 Loss: 0.0002019149601744359\n",
      "Iteration: 8964 lambda_n: 0.9687002387924473 Loss: 0.00020191496017443598\n",
      "Iteration: 8965 lambda_n: 0.9782614733162852 Loss: 0.00020191496017443603\n",
      "Iteration: 8966 lambda_n: 0.973015313690053 Loss: 0.00020191496017443595\n",
      "Iteration: 8967 lambda_n: 1.0328948104950901 Loss: 0.000201914960174436\n",
      "Iteration: 8968 lambda_n: 0.9712101212872732 Loss: 0.0002019149601744359\n",
      "Iteration: 8969 lambda_n: 1.0218995610481476 Loss: 0.00020191496017443598\n",
      "Iteration: 8970 lambda_n: 0.9206208023094947 Loss: 0.00020191496017443603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8971 lambda_n: 0.9195508657020847 Loss: 0.00020191496017443595\n",
      "Iteration: 8972 lambda_n: 0.9100922550967285 Loss: 0.000201914960174436\n",
      "Iteration: 8973 lambda_n: 0.9312250041501134 Loss: 0.0002019149601744359\n",
      "Iteration: 8974 lambda_n: 0.957485244156991 Loss: 0.00020191496017443598\n",
      "Iteration: 8975 lambda_n: 1.0187751854068046 Loss: 0.00020191496017443603\n",
      "Iteration: 8976 lambda_n: 0.9770668681543433 Loss: 0.00020191496017443595\n",
      "Iteration: 8977 lambda_n: 1.0017396492754236 Loss: 0.000201914960174436\n",
      "Iteration: 8978 lambda_n: 0.9183133561572364 Loss: 0.0002019149601744359\n",
      "Iteration: 8979 lambda_n: 0.9678556439830327 Loss: 0.00020191496017443598\n",
      "Iteration: 8980 lambda_n: 1.0096249930529841 Loss: 0.00020191496017443603\n",
      "Iteration: 8981 lambda_n: 0.9319805224509269 Loss: 0.00020191496017443595\n",
      "Iteration: 8982 lambda_n: 0.975431499889704 Loss: 0.000201914960174436\n",
      "Iteration: 8983 lambda_n: 0.9193817290765792 Loss: 0.0002019149601744359\n",
      "Iteration: 8984 lambda_n: 0.9940842298064637 Loss: 0.00020191496017443598\n",
      "Iteration: 8985 lambda_n: 0.9518781711409701 Loss: 0.00020191496017443603\n",
      "Iteration: 8986 lambda_n: 1.0152030830297771 Loss: 0.00020191496017443595\n",
      "Iteration: 8987 lambda_n: 0.9082611221229453 Loss: 0.000201914960174436\n",
      "Iteration: 8988 lambda_n: 1.0191089951878134 Loss: 0.0002019149601744359\n",
      "Iteration: 8989 lambda_n: 0.987468603016632 Loss: 0.00020191496017443598\n",
      "Iteration: 8990 lambda_n: 1.0301119154895937 Loss: 0.00020191496017443603\n",
      "Iteration: 8991 lambda_n: 0.9524485467068402 Loss: 0.00020191496017443595\n",
      "Iteration: 8992 lambda_n: 0.967944660163464 Loss: 0.000201914960174436\n",
      "Iteration: 8993 lambda_n: 0.9096709765667214 Loss: 0.0002019149601744359\n",
      "Iteration: 8994 lambda_n: 0.9090269795563853 Loss: 0.00020191496017443598\n",
      "Iteration: 8995 lambda_n: 1.012201261808536 Loss: 0.00020191496017443603\n",
      "Iteration: 8996 lambda_n: 0.8857184576993761 Loss: 0.00020191496017443595\n",
      "Iteration: 8997 lambda_n: 1.0226582790475527 Loss: 0.000201914960174436\n",
      "Iteration: 8998 lambda_n: 0.987297743589851 Loss: 0.0002019149601744359\n",
      "Iteration: 8999 lambda_n: 0.9807485501295413 Loss: 0.00020191496017443598\n",
      "Iteration: 9000 lambda_n: 0.9678290643161923 Loss: 0.00020191496017443603\n",
      "Iteration: 9001 lambda_n: 0.9544256029215343 Loss: 0.00020191496017443595\n",
      "Iteration: 9002 lambda_n: 0.9205878542463626 Loss: 0.000201914960174436\n",
      "Iteration: 9003 lambda_n: 1.013754683292275 Loss: 0.0002019149601744359\n",
      "Iteration: 9004 lambda_n: 1.011766374744103 Loss: 0.00020191496017443598\n",
      "Iteration: 9005 lambda_n: 0.9507287558119812 Loss: 0.00020191496017443603\n",
      "Iteration: 9006 lambda_n: 0.9426496535858738 Loss: 0.00020191496017443595\n",
      "Iteration: 9007 lambda_n: 0.9711259281738043 Loss: 0.000201914960174436\n",
      "Iteration: 9008 lambda_n: 0.9159892304721119 Loss: 0.0002019149601744359\n",
      "Iteration: 9009 lambda_n: 0.9111780924229026 Loss: 0.00020191496017443598\n",
      "Iteration: 9010 lambda_n: 0.9875151498836319 Loss: 0.00020191496017443603\n",
      "Iteration: 9011 lambda_n: 0.950833327967808 Loss: 0.00020191496017443595\n",
      "Iteration: 9012 lambda_n: 0.9026310234106604 Loss: 0.000201914960174436\n",
      "Iteration: 9013 lambda_n: 0.9028881430621631 Loss: 0.0002019149601744359\n",
      "Iteration: 9014 lambda_n: 0.9613154424540508 Loss: 0.00020191496017443598\n",
      "Iteration: 9015 lambda_n: 0.96779748589628 Loss: 0.00020191496017443603\n",
      "Iteration: 9016 lambda_n: 1.0074377963131984 Loss: 0.00020191496017443595\n",
      "Iteration: 9017 lambda_n: 0.9663662931751903 Loss: 0.000201914960174436\n",
      "Iteration: 9018 lambda_n: 0.9497472034384369 Loss: 0.0002019149601744359\n",
      "Iteration: 9019 lambda_n: 0.9396597859233143 Loss: 0.00020191496017443598\n",
      "Iteration: 9020 lambda_n: 0.9494802824329156 Loss: 0.00020191496017443603\n",
      "Iteration: 9021 lambda_n: 1.0338311208581694 Loss: 0.00020191496017443595\n",
      "Iteration: 9022 lambda_n: 1.0301346286951285 Loss: 0.000201914960174436\n",
      "Iteration: 9023 lambda_n: 0.9854743675408474 Loss: 0.0002019149601744359\n",
      "Iteration: 9024 lambda_n: 0.9935465579044682 Loss: 0.00020191496017443598\n",
      "Iteration: 9025 lambda_n: 0.9651536511198692 Loss: 0.00020191496017443603\n",
      "Iteration: 9026 lambda_n: 0.9686347666883854 Loss: 0.00020191496017443595\n",
      "Iteration: 9027 lambda_n: 0.9876477155406034 Loss: 0.000201914960174436\n",
      "Iteration: 9028 lambda_n: 1.0040354655022163 Loss: 0.0002019149601744359\n",
      "Iteration: 9029 lambda_n: 0.9334120032117157 Loss: 0.00020191496017443598\n",
      "Iteration: 9030 lambda_n: 0.9491464916941285 Loss: 0.00020191496017443603\n",
      "Iteration: 9031 lambda_n: 0.8949324839810374 Loss: 0.00020191496017443595\n",
      "Iteration: 9032 lambda_n: 0.9903456063770762 Loss: 0.000201914960174436\n",
      "Iteration: 9033 lambda_n: 0.8834775070251769 Loss: 0.0002019149601744359\n",
      "Iteration: 9034 lambda_n: 0.9586183409664919 Loss: 0.00020191496017443598\n",
      "Iteration: 9035 lambda_n: 0.9153755255862955 Loss: 0.00020191496017443603\n",
      "Iteration: 9036 lambda_n: 0.8938647240006004 Loss: 0.00020191496017443595\n",
      "Iteration: 9037 lambda_n: 0.9273614180134793 Loss: 0.000201914960174436\n",
      "Iteration: 9038 lambda_n: 0.9958339842300227 Loss: 0.0002019149601744359\n",
      "Iteration: 9039 lambda_n: 0.9659006095989008 Loss: 0.00020191496017443598\n",
      "Iteration: 9040 lambda_n: 0.9594190498006094 Loss: 0.00020191496017443603\n",
      "Iteration: 9041 lambda_n: 0.8925320053323876 Loss: 0.00020191496017443595\n",
      "Iteration: 9042 lambda_n: 0.8949750210677818 Loss: 0.000201914960174436\n",
      "Iteration: 9043 lambda_n: 0.9762360320755612 Loss: 0.0002019149601744359\n",
      "Iteration: 9044 lambda_n: 0.9962881703086157 Loss: 0.00020191496017443598\n",
      "Iteration: 9045 lambda_n: 0.9541127043026788 Loss: 0.00020191496017443603\n",
      "Iteration: 9046 lambda_n: 0.952841672816909 Loss: 0.00020191496017443595\n",
      "Iteration: 9047 lambda_n: 0.9021138658130099 Loss: 0.000201914960174436\n",
      "Iteration: 9048 lambda_n: 0.9469617992466483 Loss: 0.0002019149601744359\n",
      "Iteration: 9049 lambda_n: 0.9717416286041087 Loss: 0.00020191496017443598\n",
      "Iteration: 9050 lambda_n: 0.9851876644133604 Loss: 0.00020191496017443603\n",
      "Iteration: 9051 lambda_n: 1.030054299079297 Loss: 0.00020191496017443595\n",
      "Iteration: 9052 lambda_n: 0.9572185102423417 Loss: 0.000201914960174436\n",
      "Iteration: 9053 lambda_n: 0.8887012536195884 Loss: 0.0002019149601744359\n",
      "Iteration: 9054 lambda_n: 0.9891185149997191 Loss: 0.00020191496017443598\n",
      "Iteration: 9055 lambda_n: 0.9426029821225853 Loss: 0.00020191496017443603\n",
      "Iteration: 9056 lambda_n: 0.9952988527899953 Loss: 0.00020191496017443595\n",
      "Iteration: 9057 lambda_n: 0.9620259657220418 Loss: 0.000201914960174436\n",
      "Iteration: 9058 lambda_n: 0.9331227050885961 Loss: 0.0002019149601744359\n",
      "Iteration: 9059 lambda_n: 0.9645820262940591 Loss: 0.00020191496017443598\n",
      "Iteration: 9060 lambda_n: 1.0308804396176374 Loss: 0.00020191496017443603\n",
      "Iteration: 9061 lambda_n: 0.9921140762908972 Loss: 0.00020191496017443595\n",
      "Iteration: 9062 lambda_n: 0.9516201164844124 Loss: 0.000201914960174436\n",
      "Iteration: 9063 lambda_n: 1.0319508658768988 Loss: 0.0002019149601744359\n",
      "Iteration: 9064 lambda_n: 0.8884212373250874 Loss: 0.00020191496017443598\n",
      "Iteration: 9065 lambda_n: 1.0248640019713613 Loss: 0.00020191496017443603\n",
      "Iteration: 9066 lambda_n: 1.030138126791532 Loss: 0.00020191496017443595\n",
      "Iteration: 9067 lambda_n: 0.9966702443044311 Loss: 0.000201914960174436\n",
      "Iteration: 9068 lambda_n: 0.9124845155064027 Loss: 0.0002019149601744359\n",
      "Iteration: 9069 lambda_n: 1.0071223977620911 Loss: 0.00020191496017443598\n",
      "Iteration: 9070 lambda_n: 0.9683486153780664 Loss: 0.00020191496017443603\n",
      "Iteration: 9071 lambda_n: 0.9115361730367957 Loss: 0.00020191496017443595\n",
      "Iteration: 9072 lambda_n: 1.0355007401690257 Loss: 0.000201914960174436\n",
      "Iteration: 9073 lambda_n: 0.9456221688263958 Loss: 0.0002019149601744359\n",
      "Iteration: 9074 lambda_n: 0.9576172771565239 Loss: 0.00020191496017443598\n",
      "Iteration: 9075 lambda_n: 0.9368367940039058 Loss: 0.00020191496017443603\n",
      "Iteration: 9076 lambda_n: 0.991486498381262 Loss: 0.00020191496017443595\n",
      "Iteration: 9077 lambda_n: 0.9773016546285287 Loss: 0.000201914960174436\n",
      "Iteration: 9078 lambda_n: 0.9177099118416819 Loss: 0.0002019149601744359\n",
      "Iteration: 9079 lambda_n: 0.9469797924821152 Loss: 0.00020191496017443598\n",
      "Iteration: 9080 lambda_n: 1.0292403064135014 Loss: 0.00020191496017443603\n",
      "Iteration: 9081 lambda_n: 0.9983191219113416 Loss: 0.00020191496017443595\n",
      "Iteration: 9082 lambda_n: 0.9027479470114574 Loss: 0.000201914960174436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9083 lambda_n: 1.0037983536365025 Loss: 0.0002019149601744359\n",
      "Iteration: 9084 lambda_n: 0.93132043004177 Loss: 0.00020191496017443598\n",
      "Iteration: 9085 lambda_n: 0.8912818546814154 Loss: 0.00020191496017443603\n",
      "Iteration: 9086 lambda_n: 0.8962094909467303 Loss: 0.00020191496017443595\n",
      "Iteration: 9087 lambda_n: 0.9459065945661987 Loss: 0.000201914960174436\n",
      "Iteration: 9088 lambda_n: 0.9557123854605899 Loss: 0.0002019149601744359\n",
      "Iteration: 9089 lambda_n: 0.9476516669231385 Loss: 0.00020191496017443598\n",
      "Iteration: 9090 lambda_n: 0.9343209046651145 Loss: 0.00020191496017443603\n",
      "Iteration: 9091 lambda_n: 1.0210621470071175 Loss: 0.00020191496017443595\n",
      "Iteration: 9092 lambda_n: 1.0350396783518703 Loss: 0.000201914960174436\n",
      "Iteration: 9093 lambda_n: 0.8991956445972921 Loss: 0.0002019149601744359\n",
      "Iteration: 9094 lambda_n: 0.9061146457256771 Loss: 0.00020191496017443598\n",
      "Iteration: 9095 lambda_n: 0.9066422592385284 Loss: 0.00020191496017443603\n",
      "Iteration: 9096 lambda_n: 0.9647496643736417 Loss: 0.00020191496017443595\n",
      "Iteration: 9097 lambda_n: 0.902875305413596 Loss: 0.000201914960174436\n",
      "Iteration: 9098 lambda_n: 0.9407248621798485 Loss: 0.0002019149601744359\n",
      "Iteration: 9099 lambda_n: 0.9144059637195967 Loss: 0.00020191496017443598\n",
      "Iteration: 9100 lambda_n: 1.008773887145978 Loss: 0.00020191496017443603\n",
      "Iteration: 9101 lambda_n: 0.8895726490819551 Loss: 0.00020191496017443595\n",
      "Iteration: 9102 lambda_n: 1.0194013478323516 Loss: 0.000201914960174436\n",
      "Iteration: 9103 lambda_n: 0.9491635669670484 Loss: 0.0002019149601744359\n",
      "Iteration: 9104 lambda_n: 0.9692250983091044 Loss: 0.00020191496017443598\n",
      "Iteration: 9105 lambda_n: 1.011503271061871 Loss: 0.00020191496017443603\n",
      "Iteration: 9106 lambda_n: 0.9897151558223608 Loss: 0.00020191496017443595\n",
      "Iteration: 9107 lambda_n: 1.0024358682497563 Loss: 0.000201914960174436\n",
      "Iteration: 9108 lambda_n: 0.9916486639470505 Loss: 0.0002019149601744359\n",
      "Iteration: 9109 lambda_n: 0.9553969694029324 Loss: 0.00020191496017443598\n",
      "Iteration: 9110 lambda_n: 1.0190448433723625 Loss: 0.00020191496017443603\n",
      "Iteration: 9111 lambda_n: 0.9249282934930337 Loss: 0.00020191496017443595\n",
      "Iteration: 9112 lambda_n: 0.917480773095322 Loss: 0.000201914960174436\n",
      "Iteration: 9113 lambda_n: 0.9966683643764641 Loss: 0.0002019149601744359\n",
      "Iteration: 9114 lambda_n: 1.0151009161721118 Loss: 0.00020191496017443598\n",
      "Iteration: 9115 lambda_n: 0.9234702712856002 Loss: 0.00020191496017443603\n",
      "Iteration: 9116 lambda_n: 0.9181445183551935 Loss: 0.00020191496017443595\n",
      "Iteration: 9117 lambda_n: 0.8826018794197903 Loss: 0.000201914960174436\n",
      "Iteration: 9118 lambda_n: 1.023501955094639 Loss: 0.0002019149601744359\n",
      "Iteration: 9119 lambda_n: 0.8843475977345455 Loss: 0.00020191496017443598\n",
      "Iteration: 9120 lambda_n: 0.9699741688688526 Loss: 0.00020191496017443603\n",
      "Iteration: 9121 lambda_n: 0.9036000220960135 Loss: 0.00020191496017443595\n",
      "Iteration: 9122 lambda_n: 0.92177860479207 Loss: 0.000201914960174436\n",
      "Iteration: 9123 lambda_n: 1.0362613809237053 Loss: 0.0002019149601744359\n",
      "Iteration: 9124 lambda_n: 0.9792536221153217 Loss: 0.00020191496017443598\n",
      "Iteration: 9125 lambda_n: 1.0282858539323172 Loss: 0.00020191496017443603\n",
      "Iteration: 9126 lambda_n: 1.0380991173495877 Loss: 0.00020191496017443595\n",
      "Iteration: 9127 lambda_n: 0.9501634286213921 Loss: 0.000201914960174436\n",
      "Iteration: 9128 lambda_n: 0.9462436790318429 Loss: 0.0002019149601744359\n",
      "Iteration: 9129 lambda_n: 1.0228014594321733 Loss: 0.00020191496017443598\n",
      "Iteration: 9130 lambda_n: 0.8936613033311192 Loss: 0.00020191496017443603\n",
      "Iteration: 9131 lambda_n: 0.9222505147935819 Loss: 0.00020191496017443595\n",
      "Iteration: 9132 lambda_n: 0.9367666502891204 Loss: 0.000201914960174436\n",
      "Iteration: 9133 lambda_n: 1.007947632522014 Loss: 0.0002019149601744359\n",
      "Iteration: 9134 lambda_n: 0.9058287111933164 Loss: 0.00020191496017443598\n",
      "Iteration: 9135 lambda_n: 0.8936288825833706 Loss: 0.00020191496017443603\n",
      "Iteration: 9136 lambda_n: 0.9071419096469928 Loss: 0.00020191496017443595\n",
      "Iteration: 9137 lambda_n: 0.9068656459328686 Loss: 0.000201914960174436\n",
      "Iteration: 9138 lambda_n: 1.0061635662589397 Loss: 0.0002019149601744359\n",
      "Iteration: 9139 lambda_n: 0.9945460617984743 Loss: 0.00020191496017443598\n",
      "Iteration: 9140 lambda_n: 0.9120658237733883 Loss: 0.00020191496017443603\n",
      "Iteration: 9141 lambda_n: 0.9239179361506208 Loss: 0.00020191496017443595\n",
      "Iteration: 9142 lambda_n: 0.9364922219113254 Loss: 0.000201914960174436\n",
      "Iteration: 9143 lambda_n: 0.9817031451422147 Loss: 0.0002019149601744359\n",
      "Iteration: 9144 lambda_n: 1.0042336570275652 Loss: 0.00020191496017443598\n",
      "Iteration: 9145 lambda_n: 0.9349263917919015 Loss: 0.00020191496017443603\n",
      "Iteration: 9146 lambda_n: 1.0238239868310028 Loss: 0.00020191496017443595\n",
      "Iteration: 9147 lambda_n: 0.9055391677383187 Loss: 0.000201914960174436\n",
      "Iteration: 9148 lambda_n: 1.0251299502587359 Loss: 0.0002019149601744359\n",
      "Iteration: 9149 lambda_n: 1.016452249447858 Loss: 0.00020191496017443598\n",
      "Iteration: 9150 lambda_n: 0.9176001753857266 Loss: 0.00020191496017443603\n",
      "Iteration: 9151 lambda_n: 0.9004879944728286 Loss: 0.00020191496017443595\n",
      "Iteration: 9152 lambda_n: 1.0095791658267959 Loss: 0.000201914960174436\n",
      "Iteration: 9153 lambda_n: 0.9592001226440183 Loss: 0.0002019149601744359\n",
      "Iteration: 9154 lambda_n: 0.9157187611392199 Loss: 0.00020191496017443598\n",
      "Iteration: 9155 lambda_n: 0.9347000006013404 Loss: 0.00020191496017443603\n",
      "Iteration: 9156 lambda_n: 0.8978323644773412 Loss: 0.00020191496017443595\n",
      "Iteration: 9157 lambda_n: 0.9928083380462825 Loss: 0.000201914960174436\n",
      "Iteration: 9158 lambda_n: 0.9569937139577979 Loss: 0.0002019149601744359\n",
      "Iteration: 9159 lambda_n: 1.028739491112538 Loss: 0.00020191496017443598\n",
      "Iteration: 9160 lambda_n: 0.9914420274304894 Loss: 0.00020191496017443603\n",
      "Iteration: 9161 lambda_n: 0.9738585259323034 Loss: 0.00020191496017443595\n",
      "Iteration: 9162 lambda_n: 0.9722085480455056 Loss: 0.000201914960174436\n",
      "Iteration: 9163 lambda_n: 0.9092382829739426 Loss: 0.0002019149601744359\n",
      "Iteration: 9164 lambda_n: 0.9495697584795578 Loss: 0.00020191496017443598\n",
      "Iteration: 9165 lambda_n: 0.900470362962955 Loss: 0.00020191496017443603\n",
      "Iteration: 9166 lambda_n: 1.003214315015246 Loss: 0.00020191496017443595\n",
      "Iteration: 9167 lambda_n: 0.9644103056684687 Loss: 0.000201914960174436\n",
      "Iteration: 9168 lambda_n: 1.0152305531543333 Loss: 0.0002019149601744359\n",
      "Iteration: 9169 lambda_n: 0.9485699967059319 Loss: 0.00020191496017443598\n",
      "Iteration: 9170 lambda_n: 0.9817838618990019 Loss: 0.00020191496017443603\n",
      "Iteration: 9171 lambda_n: 0.9113698871676381 Loss: 0.00020191496017443595\n",
      "Iteration: 9172 lambda_n: 0.9657806765089886 Loss: 0.000201914960174436\n",
      "Iteration: 9173 lambda_n: 0.9572285909744588 Loss: 0.0002019149601744359\n",
      "Iteration: 9174 lambda_n: 0.9268480397995946 Loss: 0.00020191496017443598\n",
      "Iteration: 9175 lambda_n: 0.9706332250451211 Loss: 0.00020191496017443603\n",
      "Iteration: 9176 lambda_n: 1.007938931745357 Loss: 0.00020191496017443595\n",
      "Iteration: 9177 lambda_n: 0.9717792034097478 Loss: 0.000201914960174436\n",
      "Iteration: 9178 lambda_n: 0.9446899200301893 Loss: 0.0002019149601744359\n",
      "Iteration: 9179 lambda_n: 0.9752558675614772 Loss: 0.00020191496017443598\n",
      "Iteration: 9180 lambda_n: 0.9590922960279633 Loss: 0.00020191496017443603\n",
      "Iteration: 9181 lambda_n: 1.0056892114190532 Loss: 0.00020191496017443595\n",
      "Iteration: 9182 lambda_n: 0.9298583933680562 Loss: 0.000201914960174436\n",
      "Iteration: 9183 lambda_n: 0.9151639504677734 Loss: 0.0002019149601744359\n",
      "Iteration: 9184 lambda_n: 0.9294080656524278 Loss: 0.00020191496017443598\n",
      "Iteration: 9185 lambda_n: 1.0280181836550328 Loss: 0.00020191496017443603\n",
      "Iteration: 9186 lambda_n: 0.9945823751308611 Loss: 0.00020191496017443595\n",
      "Iteration: 9187 lambda_n: 0.9702480295318825 Loss: 0.000201914960174436\n",
      "Iteration: 9188 lambda_n: 0.919804340183141 Loss: 0.0002019149601744359\n",
      "Iteration: 9189 lambda_n: 0.9124325402527445 Loss: 0.00020191496017443598\n",
      "Iteration: 9190 lambda_n: 0.9813759275509992 Loss: 0.00020191496017443603\n",
      "Iteration: 9191 lambda_n: 0.9947077915267118 Loss: 0.00020191496017443595\n",
      "Iteration: 9192 lambda_n: 0.9584840355416281 Loss: 0.000201914960174436\n",
      "Iteration: 9193 lambda_n: 0.8827798462194143 Loss: 0.0002019149601744359\n",
      "Iteration: 9194 lambda_n: 0.9003501694015621 Loss: 0.00020191496017443598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9195 lambda_n: 0.9843943092887804 Loss: 0.00020191496017443603\n",
      "Iteration: 9196 lambda_n: 0.981690312891661 Loss: 0.00020191496017443595\n",
      "Iteration: 9197 lambda_n: 0.9347598279788204 Loss: 0.000201914960174436\n",
      "Iteration: 9198 lambda_n: 1.019161782144138 Loss: 0.0002019149601744359\n",
      "Iteration: 9199 lambda_n: 0.96736088819631 Loss: 0.00020191496017443598\n",
      "Iteration: 9200 lambda_n: 0.9097246240604085 Loss: 0.00020191496017443603\n",
      "Iteration: 9201 lambda_n: 0.9088664470132759 Loss: 0.00020191496017443595\n",
      "Iteration: 9202 lambda_n: 1.016539220891665 Loss: 0.000201914960174436\n",
      "Iteration: 9203 lambda_n: 0.9358254941123335 Loss: 0.0002019149601744359\n",
      "Iteration: 9204 lambda_n: 0.9154851184505732 Loss: 0.00020191496017443598\n",
      "Iteration: 9205 lambda_n: 1.005634739446972 Loss: 0.00020191496017443603\n",
      "Iteration: 9206 lambda_n: 1.0205861966640315 Loss: 0.00020191496017443595\n",
      "Iteration: 9207 lambda_n: 0.914355315860961 Loss: 0.000201914960174436\n",
      "Iteration: 9208 lambda_n: 0.8866072405161076 Loss: 0.0002019149601744359\n",
      "Iteration: 9209 lambda_n: 0.8919929747475966 Loss: 0.00020191496017443598\n",
      "Iteration: 9210 lambda_n: 1.0187209843556155 Loss: 0.00020191496017443603\n",
      "Iteration: 9211 lambda_n: 1.0316955148760119 Loss: 0.00020191496017443595\n",
      "Iteration: 9212 lambda_n: 0.9959498885608606 Loss: 0.000201914960174436\n",
      "Iteration: 9213 lambda_n: 0.9748213803711504 Loss: 0.0002019149601744359\n",
      "Iteration: 9214 lambda_n: 0.9737695257355246 Loss: 0.00020191496017443598\n",
      "Iteration: 9215 lambda_n: 1.027728964854957 Loss: 0.00020191496017443603\n",
      "Iteration: 9216 lambda_n: 0.9441767007573817 Loss: 0.00020191496017443595\n",
      "Iteration: 9217 lambda_n: 0.8942123466563693 Loss: 0.000201914960174436\n",
      "Iteration: 9218 lambda_n: 0.9259176354186424 Loss: 0.0002019149601744359\n",
      "Iteration: 9219 lambda_n: 0.9198156713248531 Loss: 0.00020191496017443598\n",
      "Iteration: 9220 lambda_n: 1.0063836579523826 Loss: 0.00020191496017443603\n",
      "Iteration: 9221 lambda_n: 0.9053324769182194 Loss: 0.00020191496017443595\n",
      "Iteration: 9222 lambda_n: 1.0073890259696408 Loss: 0.000201914960174436\n",
      "Iteration: 9223 lambda_n: 0.9710503323941994 Loss: 0.0002019149601744359\n",
      "Iteration: 9224 lambda_n: 1.0128201080535606 Loss: 0.00020191496017443598\n",
      "Iteration: 9225 lambda_n: 0.9894795230196943 Loss: 0.00020191496017443603\n",
      "Iteration: 9226 lambda_n: 0.9135588481096258 Loss: 0.00020191496017443595\n",
      "Iteration: 9227 lambda_n: 1.0067835384260941 Loss: 0.000201914960174436\n",
      "Iteration: 9228 lambda_n: 0.9348166404732995 Loss: 0.0002019149601744359\n",
      "Iteration: 9229 lambda_n: 0.9585938731003769 Loss: 0.00020191496017443598\n",
      "Iteration: 9230 lambda_n: 0.9184884794668613 Loss: 0.00020191496017443603\n",
      "Iteration: 9231 lambda_n: 0.9856770107183548 Loss: 0.00020191496017443595\n",
      "Iteration: 9232 lambda_n: 0.9112047459001814 Loss: 0.000201914960174436\n",
      "Iteration: 9233 lambda_n: 0.9785967861707989 Loss: 0.0002019149601744359\n",
      "Iteration: 9234 lambda_n: 1.018102683512262 Loss: 0.00020191496017443598\n",
      "Iteration: 9235 lambda_n: 0.9889547268611829 Loss: 0.00020191496017443603\n",
      "Iteration: 9236 lambda_n: 0.9556318380923987 Loss: 0.00020191496017443595\n",
      "Iteration: 9237 lambda_n: 0.9511479938588003 Loss: 0.000201914960174436\n",
      "Iteration: 9238 lambda_n: 0.9426462480778439 Loss: 0.0002019149601744359\n",
      "Iteration: 9239 lambda_n: 1.011468379626317 Loss: 0.00020191496017443598\n",
      "Iteration: 9240 lambda_n: 1.0059232195602024 Loss: 0.00020191496017443603\n",
      "Iteration: 9241 lambda_n: 0.9653251707270224 Loss: 0.00020191496017443595\n",
      "Iteration: 9242 lambda_n: 0.9370023333833238 Loss: 0.000201914960174436\n",
      "Iteration: 9243 lambda_n: 0.9594906385662906 Loss: 0.0002019149601744359\n",
      "Iteration: 9244 lambda_n: 0.9324492162188918 Loss: 0.00020191496017443598\n",
      "Iteration: 9245 lambda_n: 0.9740725453284432 Loss: 0.00020191496017443603\n",
      "Iteration: 9246 lambda_n: 0.9181214357975537 Loss: 0.00020191496017443595\n",
      "Iteration: 9247 lambda_n: 0.9191471628297678 Loss: 0.000201914960174436\n",
      "Iteration: 9248 lambda_n: 0.954831002824893 Loss: 0.0002019149601744359\n",
      "Iteration: 9249 lambda_n: 0.9949188225649139 Loss: 0.00020191496017443598\n",
      "Iteration: 9250 lambda_n: 0.9298289955942854 Loss: 0.00020191496017443603\n",
      "Iteration: 9251 lambda_n: 1.0146867104526271 Loss: 0.00020191496017443595\n",
      "Iteration: 9252 lambda_n: 0.9612224674115207 Loss: 0.000201914960174436\n",
      "Iteration: 9253 lambda_n: 0.9437915050581137 Loss: 0.0002019149601744359\n",
      "Iteration: 9254 lambda_n: 0.9589967139665867 Loss: 0.00020191496017443598\n",
      "Iteration: 9255 lambda_n: 0.9451387699031584 Loss: 0.00020191496017443603\n",
      "Iteration: 9256 lambda_n: 0.9501539568651305 Loss: 0.00020191496017443595\n",
      "Iteration: 9257 lambda_n: 0.9403051997241109 Loss: 0.000201914960174436\n",
      "Iteration: 9258 lambda_n: 0.8868351760954799 Loss: 0.0002019149601744359\n",
      "Iteration: 9259 lambda_n: 0.9032654464941585 Loss: 0.00020191496017443598\n",
      "Iteration: 9260 lambda_n: 0.9353238435571668 Loss: 0.00020191496017443603\n",
      "Iteration: 9261 lambda_n: 0.9778909313929175 Loss: 0.00020191496017443595\n",
      "Iteration: 9262 lambda_n: 0.9182527318348056 Loss: 0.000201914960174436\n",
      "Iteration: 9263 lambda_n: 0.8879510294861618 Loss: 0.0002019149601744359\n",
      "Iteration: 9264 lambda_n: 1.0033166681497456 Loss: 0.00020191496017443598\n",
      "Iteration: 9265 lambda_n: 1.0162279297327284 Loss: 0.00020191496017443603\n",
      "Iteration: 9266 lambda_n: 0.9822654673116002 Loss: 0.00020191496017443595\n",
      "Iteration: 9267 lambda_n: 0.9886710720813536 Loss: 0.000201914960174436\n",
      "Iteration: 9268 lambda_n: 0.8933514458376512 Loss: 0.0002019149601744359\n",
      "Iteration: 9269 lambda_n: 0.9543049079191072 Loss: 0.00020191496017443598\n",
      "Iteration: 9270 lambda_n: 0.8962180347660238 Loss: 0.00020191496017443603\n",
      "Iteration: 9271 lambda_n: 1.0269639400504815 Loss: 0.00020191496017443595\n",
      "Iteration: 9272 lambda_n: 0.9143328324303094 Loss: 0.000201914960174436\n",
      "Iteration: 9273 lambda_n: 1.0259778816675922 Loss: 0.0002019149601744359\n",
      "Iteration: 9274 lambda_n: 1.0237277953120292 Loss: 0.00020191496017443598\n",
      "Iteration: 9275 lambda_n: 0.9781592133429861 Loss: 0.00020191496017443603\n",
      "Iteration: 9276 lambda_n: 1.0047678132552027 Loss: 0.00020191496017443595\n",
      "Iteration: 9277 lambda_n: 1.0198731551876905 Loss: 0.000201914960174436\n",
      "Iteration: 9278 lambda_n: 0.9325447558273023 Loss: 0.0002019149601744359\n",
      "Iteration: 9279 lambda_n: 0.9864500473915466 Loss: 0.00020191496017443598\n",
      "Iteration: 9280 lambda_n: 0.9526553394506987 Loss: 0.00020191496017443603\n",
      "Iteration: 9281 lambda_n: 1.006417481679108 Loss: 0.00020191496017443595\n",
      "Iteration: 9282 lambda_n: 0.9849791531208844 Loss: 0.000201914960174436\n",
      "Iteration: 9283 lambda_n: 0.8955225924197651 Loss: 0.0002019149601744359\n",
      "Iteration: 9284 lambda_n: 0.9904333746405237 Loss: 0.00020191496017443598\n",
      "Iteration: 9285 lambda_n: 0.9531134395265539 Loss: 0.00020191496017443603\n",
      "Iteration: 9286 lambda_n: 0.9807198508700606 Loss: 0.00020191496017443595\n",
      "Iteration: 9287 lambda_n: 0.954174642889746 Loss: 0.000201914960174436\n",
      "Iteration: 9288 lambda_n: 0.9044170637172647 Loss: 0.0002019149601744359\n",
      "Iteration: 9289 lambda_n: 0.9753102493473987 Loss: 0.00020191496017443598\n",
      "Iteration: 9290 lambda_n: 1.0285198864396148 Loss: 0.00020191496017443603\n",
      "Iteration: 9291 lambda_n: 1.0021928566888556 Loss: 0.00020191496017443595\n",
      "Iteration: 9292 lambda_n: 1.0083039387700046 Loss: 0.000201914960174436\n",
      "Iteration: 9293 lambda_n: 0.8922292516319352 Loss: 0.0002019149601744359\n",
      "Iteration: 9294 lambda_n: 1.0160473482633057 Loss: 0.00020191496017443598\n",
      "Iteration: 9295 lambda_n: 1.0146888100376854 Loss: 0.00020191496017443603\n",
      "Iteration: 9296 lambda_n: 0.9032399752766624 Loss: 0.00020191496017443595\n",
      "Iteration: 9297 lambda_n: 0.8928545971363152 Loss: 0.000201914960174436\n",
      "Iteration: 9298 lambda_n: 0.9351900981507907 Loss: 0.0002019149601744359\n",
      "Iteration: 9299 lambda_n: 0.902934156570093 Loss: 0.00020191496017443598\n",
      "Iteration: 9300 lambda_n: 0.9473431000102343 Loss: 0.00020191496017443603\n",
      "Iteration: 9301 lambda_n: 0.9065156600315002 Loss: 0.00020191496017443595\n",
      "Iteration: 9302 lambda_n: 0.995895248861447 Loss: 0.000201914960174436\n",
      "Iteration: 9303 lambda_n: 0.9167073410722795 Loss: 0.0002019149601744359\n",
      "Iteration: 9304 lambda_n: 0.9873138891787634 Loss: 0.00020191496017443598\n",
      "Iteration: 9305 lambda_n: 0.9697926122448759 Loss: 0.00020191496017443603\n",
      "Iteration: 9306 lambda_n: 1.0285189170033742 Loss: 0.00020191496017443595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9307 lambda_n: 0.9159288457198095 Loss: 0.000201914960174436\n",
      "Iteration: 9308 lambda_n: 0.9454835479650618 Loss: 0.0002019149601744359\n",
      "Iteration: 9309 lambda_n: 0.9115775870431385 Loss: 0.00020191496017443598\n",
      "Iteration: 9310 lambda_n: 1.018701552664839 Loss: 0.00020191496017443603\n",
      "Iteration: 9311 lambda_n: 0.9745663714855015 Loss: 0.00020191496017443595\n",
      "Iteration: 9312 lambda_n: 0.9945707469909698 Loss: 0.000201914960174436\n",
      "Iteration: 9313 lambda_n: 0.92939139215464 Loss: 0.0002019149601744359\n",
      "Iteration: 9314 lambda_n: 1.0045375455283196 Loss: 0.00020191496017443598\n",
      "Iteration: 9315 lambda_n: 0.9040872593692748 Loss: 0.00020191496017443603\n",
      "Iteration: 9316 lambda_n: 1.0179382147884632 Loss: 0.00020191496017443595\n",
      "Iteration: 9317 lambda_n: 0.88936091166861 Loss: 0.000201914960174436\n",
      "Iteration: 9318 lambda_n: 0.9038122119816588 Loss: 0.0002019149601744359\n",
      "Iteration: 9319 lambda_n: 0.8946247883000635 Loss: 0.00020191496017443598\n",
      "Iteration: 9320 lambda_n: 0.977708939261618 Loss: 0.00020191496017443603\n",
      "Iteration: 9321 lambda_n: 1.0362199878679443 Loss: 0.00020191496017443595\n",
      "Iteration: 9322 lambda_n: 0.9653269273167964 Loss: 0.000201914960174436\n",
      "Iteration: 9323 lambda_n: 1.0234231044680813 Loss: 0.0002019149601744359\n",
      "Iteration: 9324 lambda_n: 0.947830893352256 Loss: 0.00020191496017443598\n",
      "Iteration: 9325 lambda_n: 0.9459743760378027 Loss: 0.00020191496017443603\n",
      "Iteration: 9326 lambda_n: 0.9389841822504083 Loss: 0.00020191496017443595\n",
      "Iteration: 9327 lambda_n: 0.893766569657362 Loss: 0.000201914960174436\n",
      "Iteration: 9328 lambda_n: 0.932322622770331 Loss: 0.0002019149601744359\n",
      "Iteration: 9329 lambda_n: 1.0352364158305318 Loss: 0.00020191496017443598\n",
      "Iteration: 9330 lambda_n: 0.945686912790261 Loss: 0.00020191496017443603\n",
      "Iteration: 9331 lambda_n: 1.0077287241835253 Loss: 0.00020191496017443595\n",
      "Iteration: 9332 lambda_n: 0.9410961671544672 Loss: 0.000201914960174436\n",
      "Iteration: 9333 lambda_n: 0.9866577156033185 Loss: 0.0002019149601744359\n",
      "Iteration: 9334 lambda_n: 0.9422646828002567 Loss: 0.00020191496017443598\n",
      "Iteration: 9335 lambda_n: 0.9113259844710906 Loss: 0.00020191496017443603\n",
      "Iteration: 9336 lambda_n: 0.9210544537070647 Loss: 0.00020191496017443595\n",
      "Iteration: 9337 lambda_n: 1.0229864262365818 Loss: 0.000201914960174436\n",
      "Iteration: 9338 lambda_n: 1.027599133575799 Loss: 0.0002019149601744359\n",
      "Iteration: 9339 lambda_n: 1.0275275992834803 Loss: 0.00020191496017443598\n",
      "Iteration: 9340 lambda_n: 0.9427406988431747 Loss: 0.00020191496017443603\n",
      "Iteration: 9341 lambda_n: 1.0091918019167587 Loss: 0.00020191496017443595\n",
      "Iteration: 9342 lambda_n: 0.9022502216126572 Loss: 0.000201914960174436\n",
      "Iteration: 9343 lambda_n: 0.9049093301153318 Loss: 0.0002019149601744359\n",
      "Iteration: 9344 lambda_n: 0.9790211258908521 Loss: 0.00020191496017443598\n",
      "Iteration: 9345 lambda_n: 1.030465894735501 Loss: 0.00020191496017443603\n",
      "Iteration: 9346 lambda_n: 0.963072318281096 Loss: 0.00020191496017443595\n",
      "Iteration: 9347 lambda_n: 0.9813257215201006 Loss: 0.000201914960174436\n",
      "Iteration: 9348 lambda_n: 1.036607633700525 Loss: 0.0002019149601744359\n",
      "Iteration: 9349 lambda_n: 0.933736845634408 Loss: 0.00020191496017443598\n",
      "Iteration: 9350 lambda_n: 0.9999517037114218 Loss: 0.00020191496017443603\n",
      "Iteration: 9351 lambda_n: 0.9009317401459782 Loss: 0.00020191496017443595\n",
      "Iteration: 9352 lambda_n: 0.9768269037122718 Loss: 0.000201914960174436\n",
      "Iteration: 9353 lambda_n: 1.0323248712774418 Loss: 0.0002019149601744359\n",
      "Iteration: 9354 lambda_n: 1.017032619286113 Loss: 0.00020191496017443598\n",
      "Iteration: 9355 lambda_n: 1.010649820067496 Loss: 0.00020191496017443603\n",
      "Iteration: 9356 lambda_n: 1.0200317042255098 Loss: 0.00020191496017443595\n",
      "Iteration: 9357 lambda_n: 1.0265581371148749 Loss: 0.000201914960174436\n",
      "Iteration: 9358 lambda_n: 1.0011485799802218 Loss: 0.0002019149601744359\n",
      "Iteration: 9359 lambda_n: 0.9497561566077177 Loss: 0.00020191496017443598\n",
      "Iteration: 9360 lambda_n: 0.9752686481970781 Loss: 0.00020191496017443603\n",
      "Iteration: 9361 lambda_n: 1.0195701442444212 Loss: 0.00020191496017443595\n",
      "Iteration: 9362 lambda_n: 0.9035141148906685 Loss: 0.000201914960174436\n",
      "Iteration: 9363 lambda_n: 0.956630353527687 Loss: 0.0002019149601744359\n",
      "Iteration: 9364 lambda_n: 0.8977163915461727 Loss: 0.00020191496017443598\n",
      "Iteration: 9365 lambda_n: 0.9955957164310815 Loss: 0.00020191496017443603\n",
      "Iteration: 9366 lambda_n: 0.9512509481117432 Loss: 0.00020191496017443595\n",
      "Iteration: 9367 lambda_n: 0.9559739697712853 Loss: 0.000201914960174436\n",
      "Iteration: 9368 lambda_n: 0.9993577853542936 Loss: 0.0002019149601744359\n",
      "Iteration: 9369 lambda_n: 1.0250198116070741 Loss: 0.00020191496017443598\n",
      "Iteration: 9370 lambda_n: 0.923172712955944 Loss: 0.00020191496017443603\n",
      "Iteration: 9371 lambda_n: 0.889803299789178 Loss: 0.00020191496017443595\n",
      "Iteration: 9372 lambda_n: 1.0124353604716834 Loss: 0.000201914960174436\n",
      "Iteration: 9373 lambda_n: 0.9923543255267876 Loss: 0.0002019149601744359\n",
      "Iteration: 9374 lambda_n: 0.9642284508346072 Loss: 0.00020191496017443598\n",
      "Iteration: 9375 lambda_n: 0.8905720402447741 Loss: 0.00020191496017443603\n",
      "Iteration: 9376 lambda_n: 1.0290874706284174 Loss: 0.00020191496017443595\n",
      "Iteration: 9377 lambda_n: 0.9955254881529972 Loss: 0.000201914960174436\n",
      "Iteration: 9378 lambda_n: 0.9017807123123784 Loss: 0.0002019149601744359\n",
      "Iteration: 9379 lambda_n: 1.0144886555266996 Loss: 0.00020191496017443598\n",
      "Iteration: 9380 lambda_n: 0.9505420432067772 Loss: 0.00020191496017443603\n",
      "Iteration: 9381 lambda_n: 1.0165861526474176 Loss: 0.00020191496017443595\n",
      "Iteration: 9382 lambda_n: 0.9797971318590224 Loss: 0.000201914960174436\n",
      "Iteration: 9383 lambda_n: 0.95205403508791 Loss: 0.0002019149601744359\n",
      "Iteration: 9384 lambda_n: 1.0105410802016417 Loss: 0.00020191496017443598\n",
      "Iteration: 9385 lambda_n: 1.0205287657992799 Loss: 0.00020191496017443603\n",
      "Iteration: 9386 lambda_n: 0.9094371352450769 Loss: 0.00020191496017443595\n",
      "Iteration: 9387 lambda_n: 1.0068547898599745 Loss: 0.000201914960174436\n",
      "Iteration: 9388 lambda_n: 0.9722691898472912 Loss: 0.0002019149601744359\n",
      "Iteration: 9389 lambda_n: 0.9146473612337025 Loss: 0.00020191496017443598\n",
      "Iteration: 9390 lambda_n: 1.0268758899788781 Loss: 0.00020191496017443603\n",
      "Iteration: 9391 lambda_n: 1.0215512837318685 Loss: 0.00020191496017443595\n",
      "Iteration: 9392 lambda_n: 0.9767158553780506 Loss: 0.000201914960174436\n",
      "Iteration: 9393 lambda_n: 0.9296021137332445 Loss: 0.0002019149601744359\n",
      "Iteration: 9394 lambda_n: 0.8993867164397973 Loss: 0.00020191496017443598\n",
      "Iteration: 9395 lambda_n: 0.8844892404399971 Loss: 0.00020191496017443603\n",
      "Iteration: 9396 lambda_n: 0.8954257443241226 Loss: 0.00020191496017443595\n",
      "Iteration: 9397 lambda_n: 1.007199961533415 Loss: 0.000201914960174436\n",
      "Iteration: 9398 lambda_n: 0.8951133134852433 Loss: 0.0002019149601744359\n",
      "Iteration: 9399 lambda_n: 0.9048567282262443 Loss: 0.00020191496017443598\n",
      "Iteration: 9400 lambda_n: 0.9867882998058969 Loss: 0.00020191496017443603\n",
      "Iteration: 9401 lambda_n: 0.99992487520382 Loss: 0.00020191496017443595\n",
      "Iteration: 9402 lambda_n: 0.9972829086956007 Loss: 0.000201914960174436\n",
      "Iteration: 9403 lambda_n: 0.9919077717424335 Loss: 0.0002019149601744359\n",
      "Iteration: 9404 lambda_n: 0.9422091589250758 Loss: 0.00020191496017443598\n",
      "Iteration: 9405 lambda_n: 0.9092591530797555 Loss: 0.00020191496017443603\n",
      "Iteration: 9406 lambda_n: 0.9507678400263591 Loss: 0.00020191496017443595\n",
      "Iteration: 9407 lambda_n: 0.9687266420097611 Loss: 0.000201914960174436\n",
      "Iteration: 9408 lambda_n: 0.9908689887172674 Loss: 0.0002019149601744359\n",
      "Iteration: 9409 lambda_n: 0.9957810671908321 Loss: 0.00020191496017443598\n",
      "Iteration: 9410 lambda_n: 0.9565197490032827 Loss: 0.00020191496017443603\n",
      "Iteration: 9411 lambda_n: 0.949398129239176 Loss: 0.00020191496017443595\n",
      "Iteration: 9412 lambda_n: 0.894555379337872 Loss: 0.000201914960174436\n",
      "Iteration: 9413 lambda_n: 1.022295087619449 Loss: 0.0002019149601744359\n",
      "Iteration: 9414 lambda_n: 1.0013499953443146 Loss: 0.00020191496017443598\n",
      "Iteration: 9415 lambda_n: 0.9923714264098485 Loss: 0.00020191496017443603\n",
      "Iteration: 9416 lambda_n: 0.9274704558082774 Loss: 0.00020191496017443595\n",
      "Iteration: 9417 lambda_n: 1.0018832087957772 Loss: 0.000201914960174436\n",
      "Iteration: 9418 lambda_n: 0.938571241408283 Loss: 0.0002019149601744359\n",
      "Iteration: 9419 lambda_n: 0.8980771750901836 Loss: 0.00020191496017443598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9420 lambda_n: 1.006654878802302 Loss: 0.00020191496017443603\n",
      "Iteration: 9421 lambda_n: 1.0126127981327373 Loss: 0.00020191496017443595\n",
      "Iteration: 9422 lambda_n: 0.9525790338964623 Loss: 0.000201914960174436\n",
      "Iteration: 9423 lambda_n: 1.0319022723574167 Loss: 0.0002019149601744359\n",
      "Iteration: 9424 lambda_n: 1.023736558776876 Loss: 0.00020191496017443598\n",
      "Iteration: 9425 lambda_n: 0.9721652692617043 Loss: 0.00020191496017443603\n",
      "Iteration: 9426 lambda_n: 0.9803446287407283 Loss: 0.00020191496017443595\n",
      "Iteration: 9427 lambda_n: 0.8829768220866597 Loss: 0.000201914960174436\n",
      "Iteration: 9428 lambda_n: 0.9917834967301573 Loss: 0.0002019149601744359\n",
      "Iteration: 9429 lambda_n: 1.0096533963357877 Loss: 0.00020191496017443598\n",
      "Iteration: 9430 lambda_n: 1.010335249237971 Loss: 0.00020191496017443603\n",
      "Iteration: 9431 lambda_n: 0.942706553589666 Loss: 0.00020191496017443595\n",
      "Iteration: 9432 lambda_n: 0.9303635640689671 Loss: 0.000201914960174436\n",
      "Iteration: 9433 lambda_n: 0.9495987946291039 Loss: 0.0002019149601744359\n",
      "Iteration: 9434 lambda_n: 1.0170614870730317 Loss: 0.00020191496017443598\n",
      "Iteration: 9435 lambda_n: 0.9054929416589841 Loss: 0.00020191496017443603\n",
      "Iteration: 9436 lambda_n: 1.0311564087761311 Loss: 0.00020191496017443595\n",
      "Iteration: 9437 lambda_n: 1.0242334669406488 Loss: 0.000201914960174436\n",
      "Iteration: 9438 lambda_n: 1.0174448558731881 Loss: 0.0002019149601744359\n",
      "Iteration: 9439 lambda_n: 0.8895688890383858 Loss: 0.00020191496017443598\n",
      "Iteration: 9440 lambda_n: 0.979499843543848 Loss: 0.00020191496017443603\n",
      "Iteration: 9441 lambda_n: 0.9681260290852461 Loss: 0.00020191496017443595\n",
      "Iteration: 9442 lambda_n: 0.9777606086663744 Loss: 0.000201914960174436\n",
      "Iteration: 9443 lambda_n: 0.8879305404535768 Loss: 0.0002019149601744359\n",
      "Iteration: 9444 lambda_n: 0.9947643012168713 Loss: 0.00020191496017443598\n",
      "Iteration: 9445 lambda_n: 1.0202302622655501 Loss: 0.00020191496017443603\n",
      "Iteration: 9446 lambda_n: 0.968784492358238 Loss: 0.00020191496017443595\n",
      "Iteration: 9447 lambda_n: 0.9114914732290278 Loss: 0.000201914960174436\n",
      "Iteration: 9448 lambda_n: 0.9373875490341017 Loss: 0.0002019149601744359\n",
      "Iteration: 9449 lambda_n: 0.9178397342189961 Loss: 0.00020191496017443598\n",
      "Iteration: 9450 lambda_n: 0.9491232417441673 Loss: 0.00020191496017443603\n",
      "Iteration: 9451 lambda_n: 0.9261268631619866 Loss: 0.00020191496017443595\n",
      "Iteration: 9452 lambda_n: 0.929004166923999 Loss: 0.000201914960174436\n",
      "Iteration: 9453 lambda_n: 1.0139140735002838 Loss: 0.0002019149601744359\n",
      "Iteration: 9454 lambda_n: 1.0007785045331605 Loss: 0.00020191496017443598\n",
      "Iteration: 9455 lambda_n: 0.9156496665967492 Loss: 0.00020191496017443603\n",
      "Iteration: 9456 lambda_n: 0.9167903651535011 Loss: 0.00020191496017443595\n",
      "Iteration: 9457 lambda_n: 0.921656475108999 Loss: 0.000201914960174436\n",
      "Iteration: 9458 lambda_n: 0.984566293581449 Loss: 0.0002019149601744359\n",
      "Iteration: 9459 lambda_n: 0.958440861842826 Loss: 0.00020191496017443598\n",
      "Iteration: 9460 lambda_n: 0.9034727481046604 Loss: 0.00020191496017443603\n",
      "Iteration: 9461 lambda_n: 0.9476866777751244 Loss: 0.00020191496017443595\n",
      "Iteration: 9462 lambda_n: 0.956368515997038 Loss: 0.000201914960174436\n",
      "Iteration: 9463 lambda_n: 1.02586240597616 Loss: 0.0002019149601744359\n",
      "Iteration: 9464 lambda_n: 0.9376478716153017 Loss: 0.00020191496017443598\n",
      "Iteration: 9465 lambda_n: 0.942322573880055 Loss: 0.00020191496017443603\n",
      "Iteration: 9466 lambda_n: 1.0046720105506848 Loss: 0.00020191496017443595\n",
      "Iteration: 9467 lambda_n: 0.9884840829446889 Loss: 0.000201914960174436\n",
      "Iteration: 9468 lambda_n: 0.9046933076231128 Loss: 0.0002019149601744359\n",
      "Iteration: 9469 lambda_n: 0.9387439312984602 Loss: 0.00020191496017443598\n",
      "Iteration: 9470 lambda_n: 1.0215086593902403 Loss: 0.00020191496017443603\n",
      "Iteration: 9471 lambda_n: 1.017667686268667 Loss: 0.00020191496017443595\n",
      "Iteration: 9472 lambda_n: 0.997299232651337 Loss: 0.000201914960174436\n",
      "Iteration: 9473 lambda_n: 1.0073908649249659 Loss: 0.0002019149601744359\n",
      "Iteration: 9474 lambda_n: 0.9960662641831948 Loss: 0.00020191496017443598\n",
      "Iteration: 9475 lambda_n: 0.9472044742235064 Loss: 0.00020191496017443603\n",
      "Iteration: 9476 lambda_n: 0.9840546635064907 Loss: 0.00020191496017443595\n",
      "Iteration: 9477 lambda_n: 0.9160058574337911 Loss: 0.000201914960174436\n",
      "Iteration: 9478 lambda_n: 0.9661174129762505 Loss: 0.0002019149601744359\n",
      "Iteration: 9479 lambda_n: 0.9788517073832893 Loss: 0.00020191496017443598\n",
      "Iteration: 9480 lambda_n: 0.9812738605492952 Loss: 0.00020191496017443603\n",
      "Iteration: 9481 lambda_n: 0.8858849070522437 Loss: 0.00020191496017443595\n",
      "Iteration: 9482 lambda_n: 0.9156203084560023 Loss: 0.000201914960174436\n",
      "Iteration: 9483 lambda_n: 1.0028609713356178 Loss: 0.0002019149601744359\n",
      "Iteration: 9484 lambda_n: 0.9775019913101063 Loss: 0.00020191496017443598\n",
      "Iteration: 9485 lambda_n: 0.9340823481254307 Loss: 0.00020191496017443603\n",
      "Iteration: 9486 lambda_n: 1.0276065650338766 Loss: 0.00020191496017443595\n",
      "Iteration: 9487 lambda_n: 0.9497177801297797 Loss: 0.000201914960174436\n",
      "Iteration: 9488 lambda_n: 0.9142996300778394 Loss: 0.0002019149601744359\n",
      "Iteration: 9489 lambda_n: 1.0312546229885045 Loss: 0.00020191496017443598\n",
      "Iteration: 9490 lambda_n: 0.9777212961752163 Loss: 0.00020191496017443603\n",
      "Iteration: 9491 lambda_n: 0.8879869692965915 Loss: 0.00020191496017443595\n",
      "Iteration: 9492 lambda_n: 0.8980375388587916 Loss: 0.000201914960174436\n",
      "Iteration: 9493 lambda_n: 1.0132178450179887 Loss: 0.0002019149601744359\n",
      "Iteration: 9494 lambda_n: 0.9149822300783229 Loss: 0.00020191496017443598\n",
      "Iteration: 9495 lambda_n: 1.0263333739325518 Loss: 0.00020191496017443603\n",
      "Iteration: 9496 lambda_n: 0.967758354208155 Loss: 0.00020191496017443595\n",
      "Iteration: 9497 lambda_n: 0.9154999661372549 Loss: 0.000201914960174436\n",
      "Iteration: 9498 lambda_n: 0.9398945187612437 Loss: 0.0002019149601744359\n",
      "Iteration: 9499 lambda_n: 0.991722371238347 Loss: 0.00020191496017443598\n",
      "Iteration: 9500 lambda_n: 0.9898958334694803 Loss: 0.00020191496017443603\n",
      "Iteration: 9501 lambda_n: 0.9163734980134998 Loss: 0.00020191496017443595\n",
      "Iteration: 9502 lambda_n: 1.0238887859080559 Loss: 0.000201914960174436\n",
      "Iteration: 9503 lambda_n: 1.0169538847408122 Loss: 0.0002019149601744359\n",
      "Iteration: 9504 lambda_n: 0.9417476582182747 Loss: 0.00020191496017443598\n",
      "Iteration: 9505 lambda_n: 0.985156569601195 Loss: 0.00020191496017443603\n",
      "Iteration: 9506 lambda_n: 0.8899475079002682 Loss: 0.00020191496017443595\n",
      "Iteration: 9507 lambda_n: 0.918951274091884 Loss: 0.000201914960174436\n",
      "Iteration: 9508 lambda_n: 1.0178566186623565 Loss: 0.0002019149601744359\n",
      "Iteration: 9509 lambda_n: 0.8826828050615607 Loss: 0.00020191496017443598\n",
      "Iteration: 9510 lambda_n: 0.8981014183215233 Loss: 0.00020191496017443603\n",
      "Iteration: 9511 lambda_n: 0.9142666398272473 Loss: 0.00020191496017443595\n",
      "Iteration: 9512 lambda_n: 0.9826083772783403 Loss: 0.000201914960174436\n",
      "Iteration: 9513 lambda_n: 0.9184404861368444 Loss: 0.0002019149601744359\n",
      "Iteration: 9514 lambda_n: 0.9983379016756478 Loss: 0.00020191496017443598\n",
      "Iteration: 9515 lambda_n: 0.8882655853045888 Loss: 0.00020191496017443603\n",
      "Iteration: 9516 lambda_n: 0.9585548086004946 Loss: 0.00020191496017443595\n",
      "Iteration: 9517 lambda_n: 0.9868610919510228 Loss: 0.000201914960174436\n",
      "Iteration: 9518 lambda_n: 1.0059645593330182 Loss: 0.0002019149601744359\n",
      "Iteration: 9519 lambda_n: 1.0286707484857713 Loss: 0.00020191496017443598\n",
      "Iteration: 9520 lambda_n: 1.0179265167583196 Loss: 0.00020191496017443603\n",
      "Iteration: 9521 lambda_n: 1.0039490433986205 Loss: 0.00020191496017443595\n",
      "Iteration: 9522 lambda_n: 0.9185059614676111 Loss: 0.000201914960174436\n",
      "Iteration: 9523 lambda_n: 1.017575620944471 Loss: 0.0002019149601744359\n",
      "Iteration: 9524 lambda_n: 0.9841450993351002 Loss: 0.00020191496017443598\n",
      "Iteration: 9525 lambda_n: 0.9961772534348492 Loss: 0.00020191496017443603\n",
      "Iteration: 9526 lambda_n: 1.0349458539427467 Loss: 0.00020191496017443595\n",
      "Iteration: 9527 lambda_n: 1.0262581983758687 Loss: 0.000201914960174436\n",
      "Iteration: 9528 lambda_n: 1.0127858175886983 Loss: 0.0002019149601744359\n",
      "Iteration: 9529 lambda_n: 0.8892817714280619 Loss: 0.00020191496017443598\n",
      "Iteration: 9530 lambda_n: 0.8954259108990384 Loss: 0.00020191496017443603\n",
      "Iteration: 9531 lambda_n: 1.024990683316665 Loss: 0.00020191496017443595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9532 lambda_n: 0.95201176701759 Loss: 0.000201914960174436\n",
      "Iteration: 9533 lambda_n: 0.9818738530643538 Loss: 0.0002019149601744359\n",
      "Iteration: 9534 lambda_n: 0.9035498449927414 Loss: 0.00020191496017443598\n",
      "Iteration: 9535 lambda_n: 0.9819705612795472 Loss: 0.00020191496017443603\n",
      "Iteration: 9536 lambda_n: 0.9600635469053054 Loss: 0.00020191496017443595\n",
      "Iteration: 9537 lambda_n: 1.030233428850038 Loss: 0.000201914960174436\n",
      "Iteration: 9538 lambda_n: 0.9099680146955661 Loss: 0.0002019149601744359\n",
      "Iteration: 9539 lambda_n: 0.9946155080580619 Loss: 0.00020191496017443598\n",
      "Iteration: 9540 lambda_n: 0.9504098581597035 Loss: 0.00020191496017443603\n",
      "Iteration: 9541 lambda_n: 0.9034720006011896 Loss: 0.00020191496017443595\n",
      "Iteration: 9542 lambda_n: 0.9571577906330976 Loss: 0.000201914960174436\n",
      "Iteration: 9543 lambda_n: 0.9548802537950934 Loss: 0.0002019149601744359\n",
      "Iteration: 9544 lambda_n: 0.9811488468345928 Loss: 0.00020191496017443598\n",
      "Iteration: 9545 lambda_n: 0.9602219691120343 Loss: 0.00020191496017443603\n",
      "Iteration: 9546 lambda_n: 0.9926899566605418 Loss: 0.00020191496017443595\n",
      "Iteration: 9547 lambda_n: 0.955318928694789 Loss: 0.000201914960174436\n",
      "Iteration: 9548 lambda_n: 1.0001443203373472 Loss: 0.0002019149601744359\n",
      "Iteration: 9549 lambda_n: 1.031854296829027 Loss: 0.00020191496017443598\n",
      "Iteration: 9550 lambda_n: 0.9481466384020585 Loss: 0.00020191496017443603\n",
      "Iteration: 9551 lambda_n: 0.8984331828827893 Loss: 0.00020191496017443595\n",
      "Iteration: 9552 lambda_n: 0.9714409990664666 Loss: 0.000201914960174436\n",
      "Iteration: 9553 lambda_n: 0.9130769209375956 Loss: 0.0002019149601744359\n",
      "Iteration: 9554 lambda_n: 1.0149532900467209 Loss: 0.00020191496017443598\n",
      "Iteration: 9555 lambda_n: 0.8978588478862587 Loss: 0.00020191496017443603\n",
      "Iteration: 9556 lambda_n: 0.9585470312641236 Loss: 0.00020191496017443595\n",
      "Iteration: 9557 lambda_n: 0.9782510861927131 Loss: 0.000201914960174436\n",
      "Iteration: 9558 lambda_n: 0.9699380663684409 Loss: 0.0002019149601744359\n",
      "Iteration: 9559 lambda_n: 1.0277433433303387 Loss: 0.00020191496017443598\n",
      "Iteration: 9560 lambda_n: 0.9225780925151255 Loss: 0.00020191496017443603\n",
      "Iteration: 9561 lambda_n: 0.9042687805168576 Loss: 0.00020191496017443595\n",
      "Iteration: 9562 lambda_n: 1.004951196553877 Loss: 0.000201914960174436\n",
      "Iteration: 9563 lambda_n: 1.017014385515872 Loss: 0.0002019149601744359\n",
      "Iteration: 9564 lambda_n: 1.0141031329706005 Loss: 0.00020191496017443598\n",
      "Iteration: 9565 lambda_n: 0.8970418007473762 Loss: 0.00020191496017443603\n",
      "Iteration: 9566 lambda_n: 0.9602636101185689 Loss: 0.00020191496017443595\n",
      "Iteration: 9567 lambda_n: 0.9677202700662729 Loss: 0.000201914960174436\n",
      "Iteration: 9568 lambda_n: 0.9272618763678292 Loss: 0.0002019149601744359\n",
      "Iteration: 9569 lambda_n: 0.8987043011562228 Loss: 0.00020191496017443598\n",
      "Iteration: 9570 lambda_n: 0.8956356530282372 Loss: 0.00020191496017443603\n",
      "Iteration: 9571 lambda_n: 0.9000236143700494 Loss: 0.00020191496017443595\n",
      "Iteration: 9572 lambda_n: 0.9528396442803302 Loss: 0.000201914960174436\n",
      "Iteration: 9573 lambda_n: 1.018354039882515 Loss: 0.0002019149601744359\n",
      "Iteration: 9574 lambda_n: 0.952899006370464 Loss: 0.00020191496017443598\n",
      "Iteration: 9575 lambda_n: 0.9068600032479623 Loss: 0.00020191496017443603\n",
      "Iteration: 9576 lambda_n: 0.9274478894288146 Loss: 0.00020191496017443595\n",
      "Iteration: 9577 lambda_n: 0.9640834425688879 Loss: 0.000201914960174436\n",
      "Iteration: 9578 lambda_n: 0.9862901594003944 Loss: 0.0002019149601744359\n",
      "Iteration: 9579 lambda_n: 1.0087858047964713 Loss: 0.00020191496017443598\n",
      "Iteration: 9580 lambda_n: 0.9992072294041889 Loss: 0.00020191496017443603\n",
      "Iteration: 9581 lambda_n: 1.009157477211224 Loss: 0.00020191496017443595\n",
      "Iteration: 9582 lambda_n: 1.0078715399930374 Loss: 0.000201914960174436\n",
      "Iteration: 9583 lambda_n: 0.9474080588423557 Loss: 0.0002019149601744359\n",
      "Iteration: 9584 lambda_n: 0.9302506204159693 Loss: 0.00020191496017443598\n",
      "Iteration: 9585 lambda_n: 0.9737900401435933 Loss: 0.00020191496017443603\n",
      "Iteration: 9586 lambda_n: 0.9285541100201343 Loss: 0.00020191496017443595\n",
      "Iteration: 9587 lambda_n: 0.9591278201167153 Loss: 0.000201914960174436\n",
      "Iteration: 9588 lambda_n: 0.9522962060547835 Loss: 0.0002019149601744359\n",
      "Iteration: 9589 lambda_n: 0.8968956561831576 Loss: 0.00020191496017443598\n",
      "Iteration: 9590 lambda_n: 0.9195544737610233 Loss: 0.00020191496017443603\n",
      "Iteration: 9591 lambda_n: 0.98805245248214 Loss: 0.00020191496017443595\n",
      "Iteration: 9592 lambda_n: 0.9398171123436759 Loss: 0.000201914960174436\n",
      "Iteration: 9593 lambda_n: 0.9584947519487326 Loss: 0.0002019149601744359\n",
      "Iteration: 9594 lambda_n: 1.0219075894126515 Loss: 0.00020191496017443598\n",
      "Iteration: 9595 lambda_n: 0.9581350393340929 Loss: 0.00020191496017443603\n",
      "Iteration: 9596 lambda_n: 0.9791528044652524 Loss: 0.00020191496017443595\n",
      "Iteration: 9597 lambda_n: 1.0022771841455482 Loss: 0.000201914960174436\n",
      "Iteration: 9598 lambda_n: 0.9373809721234707 Loss: 0.0002019149601744359\n",
      "Iteration: 9599 lambda_n: 0.9633817488339501 Loss: 0.00020191496017443598\n",
      "Iteration: 9600 lambda_n: 0.8925837103057337 Loss: 0.00020191496017443603\n",
      "Iteration: 9601 lambda_n: 0.8881165985827294 Loss: 0.00020191496017443595\n",
      "Iteration: 9602 lambda_n: 0.9495053375115043 Loss: 0.000201914960174436\n",
      "Iteration: 9603 lambda_n: 0.9200067420414898 Loss: 0.0002019149601744359\n",
      "Iteration: 9604 lambda_n: 0.9893741013492658 Loss: 0.00020191496017443598\n",
      "Iteration: 9605 lambda_n: 1.0324750980298247 Loss: 0.00020191496017443603\n",
      "Iteration: 9606 lambda_n: 0.9060561890172004 Loss: 0.00020191496017443595\n",
      "Iteration: 9607 lambda_n: 0.9245337416563965 Loss: 0.000201914960174436\n",
      "Iteration: 9608 lambda_n: 1.035096922191892 Loss: 0.0002019149601744359\n",
      "Iteration: 9609 lambda_n: 0.9547871605456557 Loss: 0.00020191496017443598\n",
      "Iteration: 9610 lambda_n: 0.9779741364241856 Loss: 0.00020191496017443603\n",
      "Iteration: 9611 lambda_n: 0.8922425539950132 Loss: 0.00020191496017443595\n",
      "Iteration: 9612 lambda_n: 0.97450329307408 Loss: 0.000201914960174436\n",
      "Iteration: 9613 lambda_n: 0.9557096760416336 Loss: 0.0002019149601744359\n",
      "Iteration: 9614 lambda_n: 1.0081970409950942 Loss: 0.00020191496017443598\n",
      "Iteration: 9615 lambda_n: 0.9395935259310875 Loss: 0.00020191496017443603\n",
      "Iteration: 9616 lambda_n: 0.9481683987166812 Loss: 0.00020191496017443595\n",
      "Iteration: 9617 lambda_n: 0.8825586737853279 Loss: 0.000201914960174436\n",
      "Iteration: 9618 lambda_n: 0.9024836155532522 Loss: 0.0002019149601744359\n",
      "Iteration: 9619 lambda_n: 0.943220490235473 Loss: 0.00020191496017443598\n",
      "Iteration: 9620 lambda_n: 0.9525430982581948 Loss: 0.00020191496017443603\n",
      "Iteration: 9621 lambda_n: 0.950716158239216 Loss: 0.00020191496017443595\n",
      "Iteration: 9622 lambda_n: 0.8852043718728938 Loss: 0.000201914960174436\n",
      "Iteration: 9623 lambda_n: 0.9718232444211998 Loss: 0.0002019149601744359\n",
      "Iteration: 9624 lambda_n: 0.9970329665031817 Loss: 0.00020191496017443598\n",
      "Iteration: 9625 lambda_n: 0.9954826509513567 Loss: 0.00020191496017443603\n",
      "Iteration: 9626 lambda_n: 0.9007815319271492 Loss: 0.00020191496017443595\n",
      "Iteration: 9627 lambda_n: 0.977733260436405 Loss: 0.000201914960174436\n",
      "Iteration: 9628 lambda_n: 0.8963360445787477 Loss: 0.0002019149601744359\n",
      "Iteration: 9629 lambda_n: 0.9774468015168315 Loss: 0.00020191496017443598\n",
      "Iteration: 9630 lambda_n: 0.9622444144096559 Loss: 0.00020191496017443603\n",
      "Iteration: 9631 lambda_n: 0.8834832159575212 Loss: 0.00020191496017443595\n",
      "Iteration: 9632 lambda_n: 0.9288373462180706 Loss: 0.000201914960174436\n",
      "Iteration: 9633 lambda_n: 1.0213907057244593 Loss: 0.0002019149601744359\n",
      "Iteration: 9634 lambda_n: 0.9722020424341222 Loss: 0.00020191496017443598\n",
      "Iteration: 9635 lambda_n: 1.0048627357962678 Loss: 0.00020191496017443603\n",
      "Iteration: 9636 lambda_n: 0.9312401673693416 Loss: 0.00020191496017443595\n",
      "Iteration: 9637 lambda_n: 0.928594117019318 Loss: 0.000201914960174436\n",
      "Iteration: 9638 lambda_n: 1.018852153265616 Loss: 0.0002019149601744359\n",
      "Iteration: 9639 lambda_n: 0.883480472436964 Loss: 0.00020191496017443598\n",
      "Iteration: 9640 lambda_n: 1.0190046874774603 Loss: 0.00020191496017443603\n",
      "Iteration: 9641 lambda_n: 1.011322505829212 Loss: 0.00020191496017443595\n",
      "Iteration: 9642 lambda_n: 0.960098745499258 Loss: 0.000201914960174436\n",
      "Iteration: 9643 lambda_n: 0.9596032580697028 Loss: 0.0002019149601744359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9644 lambda_n: 0.9140799918941538 Loss: 0.00020191496017443598\n",
      "Iteration: 9645 lambda_n: 0.9298723505511393 Loss: 0.00020191496017443603\n",
      "Iteration: 9646 lambda_n: 0.9047640637257472 Loss: 0.00020191496017443595\n",
      "Iteration: 9647 lambda_n: 0.9016238661413054 Loss: 0.000201914960174436\n",
      "Iteration: 9648 lambda_n: 0.9844939906456728 Loss: 0.0002019149601744359\n",
      "Iteration: 9649 lambda_n: 1.029467717836553 Loss: 0.00020191496017443598\n",
      "Iteration: 9650 lambda_n: 1.0299545074478405 Loss: 0.00020191496017443603\n",
      "Iteration: 9651 lambda_n: 0.940389000076854 Loss: 0.00020191496017443595\n",
      "Iteration: 9652 lambda_n: 0.8835235010349466 Loss: 0.000201914960174436\n",
      "Iteration: 9653 lambda_n: 1.024157405793431 Loss: 0.0002019149601744359\n",
      "Iteration: 9654 lambda_n: 0.9398840707414238 Loss: 0.00020191496017443598\n",
      "Iteration: 9655 lambda_n: 0.9085853173421845 Loss: 0.00020191496017443603\n",
      "Iteration: 9656 lambda_n: 1.033500157470648 Loss: 0.00020191496017443595\n",
      "Iteration: 9657 lambda_n: 0.9184031008447843 Loss: 0.000201914960174436\n",
      "Iteration: 9658 lambda_n: 0.9431890937027174 Loss: 0.0002019149601744359\n",
      "Iteration: 9659 lambda_n: 0.9364435961324692 Loss: 0.00020191496017443598\n",
      "Iteration: 9660 lambda_n: 1.0342246969351294 Loss: 0.00020191496017443603\n",
      "Iteration: 9661 lambda_n: 0.9484357061771393 Loss: 0.00020191496017443595\n",
      "Iteration: 9662 lambda_n: 0.9180866708013624 Loss: 0.000201914960174436\n",
      "Iteration: 9663 lambda_n: 0.8985888157698109 Loss: 0.0002019149601744359\n",
      "Iteration: 9664 lambda_n: 1.008390453663464 Loss: 0.00020191496017443598\n",
      "Iteration: 9665 lambda_n: 0.9782327700864623 Loss: 0.00020191496017443603\n",
      "Iteration: 9666 lambda_n: 1.032854961784657 Loss: 0.00020191496017443595\n",
      "Iteration: 9667 lambda_n: 1.0186495979939776 Loss: 0.000201914960174436\n",
      "Iteration: 9668 lambda_n: 0.9985310946661403 Loss: 0.0002019149601744359\n",
      "Iteration: 9669 lambda_n: 1.0288763666489207 Loss: 0.00020191496017443598\n",
      "Iteration: 9670 lambda_n: 0.9737048173094632 Loss: 0.00020191496017443603\n",
      "Iteration: 9671 lambda_n: 0.9354489341260942 Loss: 0.00020191496017443595\n",
      "Iteration: 9672 lambda_n: 1.0281732386642 Loss: 0.000201914960174436\n",
      "Iteration: 9673 lambda_n: 0.8911232049482767 Loss: 0.0002019149601744359\n",
      "Iteration: 9674 lambda_n: 0.9962588817009675 Loss: 0.00020191496017443598\n",
      "Iteration: 9675 lambda_n: 0.9712951035828868 Loss: 0.00020191496017443603\n",
      "Iteration: 9676 lambda_n: 0.8891730448280025 Loss: 0.00020191496017443595\n",
      "Iteration: 9677 lambda_n: 0.9092604414439069 Loss: 0.000201914960174436\n",
      "Iteration: 9678 lambda_n: 0.9822791096250313 Loss: 0.0002019149601744359\n",
      "Iteration: 9679 lambda_n: 1.0369660507051164 Loss: 0.00020191496017443598\n",
      "Iteration: 9680 lambda_n: 1.0197326576978556 Loss: 0.00020191496017443603\n",
      "Iteration: 9681 lambda_n: 0.9425265673308255 Loss: 0.00020191496017443595\n",
      "Iteration: 9682 lambda_n: 0.9734970092661053 Loss: 0.000201914960174436\n",
      "Iteration: 9683 lambda_n: 0.9781984620688233 Loss: 0.0002019149601744359\n",
      "Iteration: 9684 lambda_n: 1.023040839680918 Loss: 0.00020191496017443598\n",
      "Iteration: 9685 lambda_n: 1.0355923687108892 Loss: 0.00020191496017443603\n",
      "Iteration: 9686 lambda_n: 0.993668364564032 Loss: 0.00020191496017443595\n",
      "Iteration: 9687 lambda_n: 0.9070041436891504 Loss: 0.000201914960174436\n",
      "Iteration: 9688 lambda_n: 0.9390661134310359 Loss: 0.0002019149601744359\n",
      "Iteration: 9689 lambda_n: 0.9889995357032193 Loss: 0.00020191496017443598\n",
      "Iteration: 9690 lambda_n: 1.006738142727339 Loss: 0.00020191496017443603\n",
      "Iteration: 9691 lambda_n: 0.9664270235570216 Loss: 0.00020191496017443595\n",
      "Iteration: 9692 lambda_n: 1.0367960492270076 Loss: 0.000201914960174436\n",
      "Iteration: 9693 lambda_n: 0.9945090872900414 Loss: 0.0002019149601744359\n",
      "Iteration: 9694 lambda_n: 0.9362954803621077 Loss: 0.00020191496017443598\n",
      "Iteration: 9695 lambda_n: 1.0086496602760076 Loss: 0.00020191496017443603\n",
      "Iteration: 9696 lambda_n: 0.9211453378771278 Loss: 0.00020191496017443595\n",
      "Iteration: 9697 lambda_n: 0.9757378739986331 Loss: 0.000201914960174436\n",
      "Iteration: 9698 lambda_n: 0.9505363746521635 Loss: 0.0002019149601744359\n",
      "Iteration: 9699 lambda_n: 1.0281567383162258 Loss: 0.00020191496017443598\n",
      "Iteration: 9700 lambda_n: 0.9692181111044477 Loss: 0.00020191496017443603\n",
      "Iteration: 9701 lambda_n: 1.0364655868377288 Loss: 0.00020191496017443595\n",
      "Iteration: 9702 lambda_n: 0.934783212811485 Loss: 0.000201914960174436\n",
      "Iteration: 9703 lambda_n: 0.9025657276883552 Loss: 0.0002019149601744359\n",
      "Iteration: 9704 lambda_n: 0.9173105519377605 Loss: 0.00020191496017443598\n",
      "Iteration: 9705 lambda_n: 0.978266315801991 Loss: 0.00020191496017443603\n",
      "Iteration: 9706 lambda_n: 0.9628432539340135 Loss: 0.00020191496017443595\n",
      "Iteration: 9707 lambda_n: 0.8900312321788721 Loss: 0.000201914960174436\n",
      "Iteration: 9708 lambda_n: 0.9251330120950214 Loss: 0.0002019149601744359\n",
      "Iteration: 9709 lambda_n: 0.9505829820709145 Loss: 0.00020191496017443598\n",
      "Iteration: 9710 lambda_n: 0.9701828087497963 Loss: 0.00020191496017443603\n",
      "Iteration: 9711 lambda_n: 0.9945797098610086 Loss: 0.00020191496017443595\n",
      "Iteration: 9712 lambda_n: 0.8848322055065941 Loss: 0.000201914960174436\n",
      "Iteration: 9713 lambda_n: 0.9327444556631078 Loss: 0.0002019149601744359\n",
      "Iteration: 9714 lambda_n: 0.9453556950231454 Loss: 0.00020191496017443598\n",
      "Iteration: 9715 lambda_n: 0.9799590218752247 Loss: 0.00020191496017443603\n",
      "Iteration: 9716 lambda_n: 0.9102453811477849 Loss: 0.00020191496017443595\n",
      "Iteration: 9717 lambda_n: 0.908020800064687 Loss: 0.000201914960174436\n",
      "Iteration: 9718 lambda_n: 0.8924903466061763 Loss: 0.0002019149601744359\n",
      "Iteration: 9719 lambda_n: 0.939184142543191 Loss: 0.00020191496017443598\n",
      "Iteration: 9720 lambda_n: 0.9956315375266427 Loss: 0.00020191496017443603\n",
      "Iteration: 9721 lambda_n: 0.9176316165788175 Loss: 0.00020191496017443595\n",
      "Iteration: 9722 lambda_n: 0.9735526326527492 Loss: 0.000201914960174436\n",
      "Iteration: 9723 lambda_n: 0.9633836257455513 Loss: 0.0002019149601744359\n",
      "Iteration: 9724 lambda_n: 0.976437045966498 Loss: 0.00020191496017443598\n",
      "Iteration: 9725 lambda_n: 0.9648472281885405 Loss: 0.00020191496017443603\n",
      "Iteration: 9726 lambda_n: 0.9124574659418725 Loss: 0.00020191496017443595\n",
      "Iteration: 9727 lambda_n: 0.9446387503043965 Loss: 0.000201914960174436\n",
      "Iteration: 9728 lambda_n: 0.9509604255820238 Loss: 0.0002019149601744359\n",
      "Iteration: 9729 lambda_n: 1.0072028369173427 Loss: 0.00020191496017443598\n",
      "Iteration: 9730 lambda_n: 0.9098184447712553 Loss: 0.00020191496017443603\n",
      "Iteration: 9731 lambda_n: 1.0259520576585048 Loss: 0.00020191496017443595\n",
      "Iteration: 9732 lambda_n: 0.9653328234013462 Loss: 0.000201914960174436\n",
      "Iteration: 9733 lambda_n: 0.9615612868620177 Loss: 0.0002019149601744359\n",
      "Iteration: 9734 lambda_n: 1.0372685890570077 Loss: 0.00020191496017443598\n",
      "Iteration: 9735 lambda_n: 0.9470786540727436 Loss: 0.00020191496017443603\n",
      "Iteration: 9736 lambda_n: 0.8923779720499012 Loss: 0.00020191496017443595\n",
      "Iteration: 9737 lambda_n: 1.0286990070032636 Loss: 0.000201914960174436\n",
      "Iteration: 9738 lambda_n: 0.8955622346901477 Loss: 0.0002019149601744359\n",
      "Iteration: 9739 lambda_n: 0.9466973575044142 Loss: 0.00020191496017443598\n",
      "Iteration: 9740 lambda_n: 0.9858283006663937 Loss: 0.00020191496017443603\n",
      "Iteration: 9741 lambda_n: 1.0367841541225686 Loss: 0.00020191496017443595\n",
      "Iteration: 9742 lambda_n: 0.9694817119942173 Loss: 0.000201914960174436\n",
      "Iteration: 9743 lambda_n: 0.8890778620542945 Loss: 0.0002019149601744359\n",
      "Iteration: 9744 lambda_n: 0.9394210887447277 Loss: 0.00020191496017443598\n",
      "Iteration: 9745 lambda_n: 1.0340400883792091 Loss: 0.00020191496017443603\n",
      "Iteration: 9746 lambda_n: 0.9984757889474984 Loss: 0.00020191496017443595\n",
      "Iteration: 9747 lambda_n: 0.9302294981458087 Loss: 0.000201914960174436\n",
      "Iteration: 9748 lambda_n: 0.9040893975594279 Loss: 0.0002019149601744359\n",
      "Iteration: 9749 lambda_n: 0.9832752400730462 Loss: 0.00020191496017443598\n",
      "Iteration: 9750 lambda_n: 0.9727647502458256 Loss: 0.00020191496017443603\n",
      "Iteration: 9751 lambda_n: 1.0204749733075755 Loss: 0.00020191496017443595\n",
      "Iteration: 9752 lambda_n: 0.9259890504077525 Loss: 0.000201914960174436\n",
      "Iteration: 9753 lambda_n: 0.9310616978040744 Loss: 0.0002019149601744359\n",
      "Iteration: 9754 lambda_n: 0.9130493350419625 Loss: 0.00020191496017443598\n",
      "Iteration: 9755 lambda_n: 0.9655155078647043 Loss: 0.00020191496017443603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9756 lambda_n: 1.0251634381026145 Loss: 0.00020191496017443595\n",
      "Iteration: 9757 lambda_n: 0.9934580614607397 Loss: 0.000201914960174436\n",
      "Iteration: 9758 lambda_n: 0.9029006480229551 Loss: 0.0002019149601744359\n",
      "Iteration: 9759 lambda_n: 0.9488407106976188 Loss: 0.00020191496017443598\n",
      "Iteration: 9760 lambda_n: 0.9967602795871627 Loss: 0.00020191496017443603\n",
      "Iteration: 9761 lambda_n: 0.977651239472408 Loss: 0.00020191496017443595\n",
      "Iteration: 9762 lambda_n: 1.035985203354959 Loss: 0.000201914960174436\n",
      "Iteration: 9763 lambda_n: 0.9570555284334811 Loss: 0.0002019149601744359\n",
      "Iteration: 9764 lambda_n: 1.0374801604639334 Loss: 0.00020191496017443598\n",
      "Iteration: 9765 lambda_n: 0.9125890003178777 Loss: 0.00020191496017443603\n",
      "Iteration: 9766 lambda_n: 1.024219662685883 Loss: 0.00020191496017443595\n",
      "Iteration: 9767 lambda_n: 0.9758322089328166 Loss: 0.000201914960174436\n",
      "Iteration: 9768 lambda_n: 1.0155002282796068 Loss: 0.0002019149601744359\n",
      "Iteration: 9769 lambda_n: 0.9042374509596984 Loss: 0.00020191496017443598\n",
      "Iteration: 9770 lambda_n: 0.9093138379109104 Loss: 0.00020191496017443603\n",
      "Iteration: 9771 lambda_n: 0.929219349619344 Loss: 0.00020191496017443595\n",
      "Iteration: 9772 lambda_n: 0.9937277528545684 Loss: 0.000201914960174436\n",
      "Iteration: 9773 lambda_n: 1.0042827452719303 Loss: 0.0002019149601744359\n",
      "Iteration: 9774 lambda_n: 0.9432377583702276 Loss: 0.00020191496017443598\n",
      "Iteration: 9775 lambda_n: 0.9204452509200398 Loss: 0.00020191496017443603\n",
      "Iteration: 9776 lambda_n: 0.9370363801635284 Loss: 0.00020191496017443595\n",
      "Iteration: 9777 lambda_n: 1.0002180344003049 Loss: 0.000201914960174436\n",
      "Iteration: 9778 lambda_n: 0.9818196092632675 Loss: 0.0002019149601744359\n",
      "Iteration: 9779 lambda_n: 0.9128892211390658 Loss: 0.00020191496017443598\n",
      "Iteration: 9780 lambda_n: 0.9981097771055536 Loss: 0.00020191496017443603\n",
      "Iteration: 9781 lambda_n: 0.8847579970541779 Loss: 0.00020191496017443595\n",
      "Iteration: 9782 lambda_n: 0.9434821852825466 Loss: 0.000201914960174436\n",
      "Iteration: 9783 lambda_n: 1.0295956274466485 Loss: 0.0002019149601744359\n",
      "Iteration: 9784 lambda_n: 0.9400750820288646 Loss: 0.00020191496017443598\n",
      "Iteration: 9785 lambda_n: 0.9017195011606267 Loss: 0.00020191496017443603\n",
      "Iteration: 9786 lambda_n: 1.0147991463845802 Loss: 0.00020191496017443595\n",
      "Iteration: 9787 lambda_n: 0.9544814693076801 Loss: 0.000201914960174436\n",
      "Iteration: 9788 lambda_n: 0.8936296609251999 Loss: 0.0002019149601744359\n",
      "Iteration: 9789 lambda_n: 1.0143620947866143 Loss: 0.00020191496017443598\n",
      "Iteration: 9790 lambda_n: 1.001913566267748 Loss: 0.00020191496017443603\n",
      "Iteration: 9791 lambda_n: 0.8923229038015698 Loss: 0.00020191496017443595\n",
      "Iteration: 9792 lambda_n: 0.9793307972095683 Loss: 0.000201914960174436\n",
      "Iteration: 9793 lambda_n: 0.9388763052950204 Loss: 0.0002019149601744359\n",
      "Iteration: 9794 lambda_n: 0.9260376371371065 Loss: 0.00020191496017443598\n",
      "Iteration: 9795 lambda_n: 1.0160992309721026 Loss: 0.00020191496017443603\n",
      "Iteration: 9796 lambda_n: 0.9800779143100342 Loss: 0.00020191496017443595\n",
      "Iteration: 9797 lambda_n: 1.0172718459463501 Loss: 0.000201914960174436\n",
      "Iteration: 9798 lambda_n: 0.9828537749330735 Loss: 0.0002019149601744359\n",
      "Iteration: 9799 lambda_n: 0.969353649083942 Loss: 0.00020191496017443598\n",
      "Iteration: 9800 lambda_n: 0.9390003804224739 Loss: 0.00020191496017443603\n",
      "Iteration: 9801 lambda_n: 1.026494321632282 Loss: 0.00020191496017443595\n",
      "Iteration: 9802 lambda_n: 0.9374366858946983 Loss: 0.000201914960174436\n",
      "Iteration: 9803 lambda_n: 1.016939968010702 Loss: 0.0002019149601744359\n",
      "Iteration: 9804 lambda_n: 0.9076829176051857 Loss: 0.00020191496017443598\n",
      "Iteration: 9805 lambda_n: 0.9079559777455967 Loss: 0.00020191496017443603\n",
      "Iteration: 9806 lambda_n: 0.980946952538345 Loss: 0.00020191496017443595\n",
      "Iteration: 9807 lambda_n: 1.001236526725172 Loss: 0.000201914960174436\n",
      "Iteration: 9808 lambda_n: 0.9494113778329974 Loss: 0.0002019149601744359\n",
      "Iteration: 9809 lambda_n: 0.945136715052308 Loss: 0.00020191496017443598\n",
      "Iteration: 9810 lambda_n: 0.9071862500103269 Loss: 0.00020191496017443603\n",
      "Iteration: 9811 lambda_n: 0.8844564978041701 Loss: 0.00020191496017443595\n",
      "Iteration: 9812 lambda_n: 0.9873823532558017 Loss: 0.000201914960174436\n",
      "Iteration: 9813 lambda_n: 0.9080129589945077 Loss: 0.0002019149601744359\n",
      "Iteration: 9814 lambda_n: 0.9000752886983525 Loss: 0.00020191496017443598\n",
      "Iteration: 9815 lambda_n: 0.9568517922542258 Loss: 0.00020191496017443603\n",
      "Iteration: 9816 lambda_n: 0.948184621986809 Loss: 0.00020191496017443595\n",
      "Iteration: 9817 lambda_n: 0.9860045632440408 Loss: 0.000201914960174436\n",
      "Iteration: 9818 lambda_n: 0.9702063141934973 Loss: 0.0002019149601744359\n",
      "Iteration: 9819 lambda_n: 0.9797162914281002 Loss: 0.00020191496017443598\n",
      "Iteration: 9820 lambda_n: 0.9067406033127443 Loss: 0.00020191496017443603\n",
      "Iteration: 9821 lambda_n: 0.99697742083872 Loss: 0.00020191496017443595\n",
      "Iteration: 9822 lambda_n: 0.8964286801351423 Loss: 0.000201914960174436\n",
      "Iteration: 9823 lambda_n: 0.9938360813224111 Loss: 0.0002019149601744359\n",
      "Iteration: 9824 lambda_n: 1.0196774565946267 Loss: 0.00020191496017443598\n",
      "Iteration: 9825 lambda_n: 0.9385417810901951 Loss: 0.00020191496017443603\n",
      "Iteration: 9826 lambda_n: 0.9989550247794622 Loss: 0.00020191496017443595\n",
      "Iteration: 9827 lambda_n: 0.9871918879871936 Loss: 0.000201914960174436\n",
      "Iteration: 9828 lambda_n: 0.9238794949662051 Loss: 0.0002019149601744359\n",
      "Iteration: 9829 lambda_n: 0.968021639627359 Loss: 0.00020191496017443598\n",
      "Iteration: 9830 lambda_n: 0.9691884755795646 Loss: 0.00020191496017443603\n",
      "Iteration: 9831 lambda_n: 0.8893525567431664 Loss: 0.00020191496017443595\n",
      "Iteration: 9832 lambda_n: 0.9585831770104112 Loss: 0.000201914960174436\n",
      "Iteration: 9833 lambda_n: 0.9210011780962531 Loss: 0.0002019149601744359\n",
      "Iteration: 9834 lambda_n: 1.004283534361944 Loss: 0.00020191496017443598\n",
      "Iteration: 9835 lambda_n: 0.985494750108088 Loss: 0.00020191496017443603\n",
      "Iteration: 9836 lambda_n: 0.9043031986542875 Loss: 0.00020191496017443595\n",
      "Iteration: 9837 lambda_n: 1.0250559504676207 Loss: 0.000201914960174436\n",
      "Iteration: 9838 lambda_n: 0.913517851006186 Loss: 0.0002019149601744359\n",
      "Iteration: 9839 lambda_n: 0.9590565906018409 Loss: 0.00020191496017443598\n",
      "Iteration: 9840 lambda_n: 0.9751096819535275 Loss: 0.00020191496017443603\n",
      "Iteration: 9841 lambda_n: 0.8912387408256364 Loss: 0.00020191496017443595\n",
      "Iteration: 9842 lambda_n: 1.0322837705254158 Loss: 0.000201914960174436\n",
      "Iteration: 9843 lambda_n: 0.921023141665789 Loss: 0.0002019149601744359\n",
      "Iteration: 9844 lambda_n: 0.9176133613198958 Loss: 0.00020191496017443598\n",
      "Iteration: 9845 lambda_n: 1.000421822446015 Loss: 0.00020191496017443603\n",
      "Iteration: 9846 lambda_n: 0.9196675975407063 Loss: 0.00020191496017443595\n",
      "Iteration: 9847 lambda_n: 0.984751856927306 Loss: 0.000201914960174436\n",
      "Iteration: 9848 lambda_n: 0.9927265105078148 Loss: 0.0002019149601744359\n",
      "Iteration: 9849 lambda_n: 1.0116361135460417 Loss: 0.00020191496017443598\n",
      "Iteration: 9850 lambda_n: 0.885998154663963 Loss: 0.00020191496017443603\n",
      "Iteration: 9851 lambda_n: 1.017645177159611 Loss: 0.00020191496017443595\n",
      "Iteration: 9852 lambda_n: 0.9085964688102749 Loss: 0.000201914960174436\n",
      "Iteration: 9853 lambda_n: 0.9235047864087919 Loss: 0.0002019149601744359\n",
      "Iteration: 9854 lambda_n: 0.9260761373308112 Loss: 0.00020191496017443598\n",
      "Iteration: 9855 lambda_n: 0.9306961334688265 Loss: 0.00020191496017443603\n",
      "Iteration: 9856 lambda_n: 1.02633957457573 Loss: 0.00020191496017443595\n",
      "Iteration: 9857 lambda_n: 0.9787056097872372 Loss: 0.000201914960174436\n",
      "Iteration: 9858 lambda_n: 0.901862045854938 Loss: 0.0002019149601744359\n",
      "Iteration: 9859 lambda_n: 0.9497929081688682 Loss: 0.00020191496017443598\n",
      "Iteration: 9860 lambda_n: 0.9455209361096939 Loss: 0.00020191496017443603\n",
      "Iteration: 9861 lambda_n: 0.9937067553862132 Loss: 0.00020191496017443595\n",
      "Iteration: 9862 lambda_n: 0.8873789422740095 Loss: 0.000201914960174436\n",
      "Iteration: 9863 lambda_n: 0.9213719741752691 Loss: 0.0002019149601744359\n",
      "Iteration: 9864 lambda_n: 0.9017418245562223 Loss: 0.00020191496017443598\n",
      "Iteration: 9865 lambda_n: 0.9047064880338553 Loss: 0.00020191496017443603\n",
      "Iteration: 9866 lambda_n: 0.9014212983644104 Loss: 0.00020191496017443595\n",
      "Iteration: 9867 lambda_n: 0.9949320738387016 Loss: 0.000201914960174436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9868 lambda_n: 0.9397562860043096 Loss: 0.0002019149601744359\n",
      "Iteration: 9869 lambda_n: 0.9847413406529738 Loss: 0.00020191496017443598\n",
      "Iteration: 9870 lambda_n: 1.0355932953502456 Loss: 0.00020191496017443603\n",
      "Iteration: 9871 lambda_n: 0.9832972167032582 Loss: 0.00020191496017443595\n",
      "Iteration: 9872 lambda_n: 0.907391548041782 Loss: 0.000201914960174436\n",
      "Iteration: 9873 lambda_n: 0.9438493347696643 Loss: 0.0002019149601744359\n",
      "Iteration: 9874 lambda_n: 0.9906223685693203 Loss: 0.00020191496017443598\n",
      "Iteration: 9875 lambda_n: 0.946743888048797 Loss: 0.00020191496017443603\n",
      "Iteration: 9876 lambda_n: 0.89392909183102 Loss: 0.00020191496017443595\n",
      "Iteration: 9877 lambda_n: 1.0139964416080904 Loss: 0.000201914960174436\n",
      "Iteration: 9878 lambda_n: 0.95952168312501 Loss: 0.0002019149601744359\n",
      "Iteration: 9879 lambda_n: 1.0139142445620235 Loss: 0.00020191496017443598\n",
      "Iteration: 9880 lambda_n: 0.8991388536641157 Loss: 0.00020191496017443603\n",
      "Iteration: 9881 lambda_n: 0.8989254520701042 Loss: 0.00020191496017443595\n",
      "Iteration: 9882 lambda_n: 0.9007902412555938 Loss: 0.000201914960174436\n",
      "Iteration: 9883 lambda_n: 0.8854997106704784 Loss: 0.0002019149601744359\n",
      "Iteration: 9884 lambda_n: 0.9977994091201047 Loss: 0.00020191496017443598\n",
      "Iteration: 9885 lambda_n: 0.9308775928531183 Loss: 0.00020191496017443603\n",
      "Iteration: 9886 lambda_n: 0.962863485495244 Loss: 0.00020191496017443595\n",
      "Iteration: 9887 lambda_n: 1.0221642974143907 Loss: 0.000201914960174436\n",
      "Iteration: 9888 lambda_n: 0.8870470325107875 Loss: 0.0002019149601744359\n",
      "Iteration: 9889 lambda_n: 0.8940359900616685 Loss: 0.00020191496017443598\n",
      "Iteration: 9890 lambda_n: 0.8936740632238299 Loss: 0.00020191496017443603\n",
      "Iteration: 9891 lambda_n: 1.025528972251721 Loss: 0.00020191496017443595\n",
      "Iteration: 9892 lambda_n: 0.964996768004312 Loss: 0.000201914960174436\n",
      "Iteration: 9893 lambda_n: 0.948961077986577 Loss: 0.0002019149601744359\n",
      "Iteration: 9894 lambda_n: 0.9287211654919909 Loss: 0.00020191496017443598\n",
      "Iteration: 9895 lambda_n: 0.9961282059231599 Loss: 0.00020191496017443603\n",
      "Iteration: 9896 lambda_n: 0.978282556187826 Loss: 0.00020191496017443595\n",
      "Iteration: 9897 lambda_n: 0.896328473574747 Loss: 0.000201914960174436\n",
      "Iteration: 9898 lambda_n: 0.9870488242801383 Loss: 0.0002019149601744359\n",
      "Iteration: 9899 lambda_n: 0.9673394778602697 Loss: 0.00020191496017443598\n",
      "Iteration: 9900 lambda_n: 0.9406533198185245 Loss: 0.00020191496017443603\n",
      "Iteration: 9901 lambda_n: 1.0092115876216858 Loss: 0.00020191496017443595\n",
      "Iteration: 9902 lambda_n: 0.9724797056827912 Loss: 0.000201914960174436\n",
      "Iteration: 9903 lambda_n: 0.9283806987000875 Loss: 0.0002019149601744359\n",
      "Iteration: 9904 lambda_n: 0.9493348460806231 Loss: 0.00020191496017443598\n",
      "Iteration: 9905 lambda_n: 0.9423138879456908 Loss: 0.00020191496017443603\n",
      "Iteration: 9906 lambda_n: 1.0050265725891172 Loss: 0.00020191496017443595\n",
      "Iteration: 9907 lambda_n: 1.0294198608232847 Loss: 0.000201914960174436\n",
      "Iteration: 9908 lambda_n: 0.9921344930000264 Loss: 0.0002019149601744359\n",
      "Iteration: 9909 lambda_n: 0.9659860819928091 Loss: 0.00020191496017443598\n",
      "Iteration: 9910 lambda_n: 0.9964592706272932 Loss: 0.00020191496017443603\n",
      "Iteration: 9911 lambda_n: 1.010904026656461 Loss: 0.00020191496017443595\n",
      "Iteration: 9912 lambda_n: 1.0184934783191004 Loss: 0.000201914960174436\n",
      "Iteration: 9913 lambda_n: 1.0130740580752735 Loss: 0.0002019149601744359\n",
      "Iteration: 9914 lambda_n: 0.9950812322949013 Loss: 0.00020191496017443598\n",
      "Iteration: 9915 lambda_n: 0.8897028565860173 Loss: 0.00020191496017443603\n",
      "Iteration: 9916 lambda_n: 0.9863394718788157 Loss: 0.00020191496017443595\n",
      "Iteration: 9917 lambda_n: 1.02657503421056 Loss: 0.000201914960174436\n",
      "Iteration: 9918 lambda_n: 1.0287554581727993 Loss: 0.0002019149601744359\n",
      "Iteration: 9919 lambda_n: 0.9690879018688744 Loss: 0.00020191496017443598\n",
      "Iteration: 9920 lambda_n: 1.0308839830218661 Loss: 0.00020191496017443603\n",
      "Iteration: 9921 lambda_n: 0.9064234331447405 Loss: 0.00020191496017443595\n",
      "Iteration: 9922 lambda_n: 0.9199285301880064 Loss: 0.000201914960174436\n",
      "Iteration: 9923 lambda_n: 0.9862686288749185 Loss: 0.0002019149601744359\n",
      "Iteration: 9924 lambda_n: 0.9366194282867425 Loss: 0.00020191496017443598\n",
      "Iteration: 9925 lambda_n: 0.9165962507415094 Loss: 0.00020191496017443603\n",
      "Iteration: 9926 lambda_n: 0.8878685433531447 Loss: 0.00020191496017443595\n",
      "Iteration: 9927 lambda_n: 0.9940033568085077 Loss: 0.000201914960174436\n",
      "Iteration: 9928 lambda_n: 0.9446480385413681 Loss: 0.0002019149601744359\n",
      "Iteration: 9929 lambda_n: 0.8906209157579892 Loss: 0.00020191496017443598\n",
      "Iteration: 9930 lambda_n: 0.973915951241268 Loss: 0.00020191496017443603\n",
      "Iteration: 9931 lambda_n: 1.0214103524690143 Loss: 0.00020191496017443595\n",
      "Iteration: 9932 lambda_n: 0.9097829696757552 Loss: 0.000201914960174436\n",
      "Iteration: 9933 lambda_n: 0.9392499887897264 Loss: 0.0002019149601744359\n",
      "Iteration: 9934 lambda_n: 0.9394642568000151 Loss: 0.00020191496017443598\n",
      "Iteration: 9935 lambda_n: 0.9530065106660852 Loss: 0.00020191496017443603\n",
      "Iteration: 9936 lambda_n: 1.0267703282664478 Loss: 0.00020191496017443595\n",
      "Iteration: 9937 lambda_n: 1.029043180402042 Loss: 0.000201914960174436\n",
      "Iteration: 9938 lambda_n: 1.0230764825093253 Loss: 0.0002019149601744359\n",
      "Iteration: 9939 lambda_n: 0.9873861207793786 Loss: 0.00020191496017443598\n",
      "Iteration: 9940 lambda_n: 1.0004865001175947 Loss: 0.00020191496017443603\n",
      "Iteration: 9941 lambda_n: 0.9614692962451268 Loss: 0.00020191496017443595\n",
      "Iteration: 9942 lambda_n: 0.9011550145281099 Loss: 0.000201914960174436\n",
      "Iteration: 9943 lambda_n: 0.8924063088444509 Loss: 0.0002019149601744359\n",
      "Iteration: 9944 lambda_n: 0.9701621245187354 Loss: 0.00020191496017443598\n",
      "Iteration: 9945 lambda_n: 1.0124785584380216 Loss: 0.00020191496017443603\n",
      "Iteration: 9946 lambda_n: 0.9239718424902238 Loss: 0.00020191496017443595\n",
      "Iteration: 9947 lambda_n: 0.970394571990947 Loss: 0.000201914960174436\n",
      "Iteration: 9948 lambda_n: 0.914046530115596 Loss: 0.0002019149601744359\n",
      "Iteration: 9949 lambda_n: 0.9789213065252501 Loss: 0.00020191496017443598\n",
      "Iteration: 9950 lambda_n: 0.9774287371537836 Loss: 0.00020191496017443603\n",
      "Iteration: 9951 lambda_n: 0.9393280595364725 Loss: 0.00020191496017443595\n",
      "Iteration: 9952 lambda_n: 0.9424365380026487 Loss: 0.000201914960174436\n",
      "Iteration: 9953 lambda_n: 0.9656111624817738 Loss: 0.0002019149601744359\n",
      "Iteration: 9954 lambda_n: 1.0137597744382016 Loss: 0.00020191496017443598\n",
      "Iteration: 9955 lambda_n: 0.9996576255115119 Loss: 0.00020191496017443603\n",
      "Iteration: 9956 lambda_n: 0.9252330030897817 Loss: 0.00020191496017443595\n",
      "Iteration: 9957 lambda_n: 1.0133167497326616 Loss: 0.000201914960174436\n",
      "Iteration: 9958 lambda_n: 0.9726341788448639 Loss: 0.0002019149601744359\n",
      "Iteration: 9959 lambda_n: 0.959984948614128 Loss: 0.00020191496017443598\n",
      "Iteration: 9960 lambda_n: 0.9514856665734429 Loss: 0.00020191496017443603\n",
      "Iteration: 9961 lambda_n: 0.9232802127312806 Loss: 0.00020191496017443595\n",
      "Iteration: 9962 lambda_n: 0.9662740271045954 Loss: 0.000201914960174436\n",
      "Iteration: 9963 lambda_n: 0.931936403866458 Loss: 0.0002019149601744359\n",
      "Iteration: 9964 lambda_n: 0.9957492650374008 Loss: 0.00020191496017443598\n",
      "Iteration: 9965 lambda_n: 0.9420507488787344 Loss: 0.00020191496017443603\n",
      "Iteration: 9966 lambda_n: 0.9459093280744908 Loss: 0.00020191496017443595\n",
      "Iteration: 9967 lambda_n: 0.8917672240487806 Loss: 0.000201914960174436\n",
      "Iteration: 9968 lambda_n: 0.9224674781149647 Loss: 0.0002019149601744359\n",
      "Iteration: 9969 lambda_n: 0.9183038459426606 Loss: 0.00020191496017443598\n",
      "Iteration: 9970 lambda_n: 0.9020776286823262 Loss: 0.00020191496017443603\n",
      "Iteration: 9971 lambda_n: 0.9875162094349554 Loss: 0.00020191496017443595\n",
      "Iteration: 9972 lambda_n: 0.9972888252525374 Loss: 0.000201914960174436\n",
      "Iteration: 9973 lambda_n: 0.9501248307826884 Loss: 0.0002019149601744359\n",
      "Iteration: 9974 lambda_n: 0.9227382195339896 Loss: 0.00020191496017443598\n",
      "Iteration: 9975 lambda_n: 1.0073899948617118 Loss: 0.00020191496017443603\n",
      "Iteration: 9976 lambda_n: 1.0372077163694664 Loss: 0.00020191496017443595\n",
      "Iteration: 9977 lambda_n: 0.9178537000944699 Loss: 0.000201914960174436\n",
      "Iteration: 9978 lambda_n: 0.9341555453516721 Loss: 0.0002019149601744359\n",
      "Iteration: 9979 lambda_n: 0.9036723456705361 Loss: 0.00020191496017443598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9980 lambda_n: 0.9443819262155397 Loss: 0.00020191496017443603\n",
      "Iteration: 9981 lambda_n: 0.9197847746896386 Loss: 0.00020191496017443595\n",
      "Iteration: 9982 lambda_n: 0.9824013319441286 Loss: 0.000201914960174436\n",
      "Iteration: 9983 lambda_n: 0.9562944351426851 Loss: 0.0002019149601744359\n",
      "Iteration: 9984 lambda_n: 0.9573441810748691 Loss: 0.00020191496017443598\n",
      "Iteration: 9985 lambda_n: 1.0139064821517698 Loss: 0.00020191496017443603\n",
      "Iteration: 9986 lambda_n: 0.9844138547056236 Loss: 0.00020191496017443595\n",
      "Iteration: 9987 lambda_n: 0.977919283027195 Loss: 0.000201914960174436\n",
      "Iteration: 9988 lambda_n: 0.9851498329618321 Loss: 0.0002019149601744359\n",
      "Iteration: 9989 lambda_n: 1.0247399425679096 Loss: 0.00020191496017443598\n",
      "Iteration: 9990 lambda_n: 0.9769924238559634 Loss: 0.00020191496017443603\n",
      "Iteration: 9991 lambda_n: 0.8980187893608671 Loss: 0.00020191496017443595\n",
      "Iteration: 9992 lambda_n: 0.9668975718842814 Loss: 0.000201914960174436\n",
      "Iteration: 9993 lambda_n: 0.8906046495837914 Loss: 0.0002019149601744359\n",
      "Iteration: 9994 lambda_n: 0.9204803384341501 Loss: 0.00020191496017443598\n",
      "Iteration: 9995 lambda_n: 0.9900055404251669 Loss: 0.00020191496017443603\n",
      "Iteration: 9996 lambda_n: 0.9491525988641545 Loss: 0.00020191496017443595\n",
      "Iteration: 9997 lambda_n: 0.9514146896834454 Loss: 0.000201914960174436\n",
      "Iteration: 9998 lambda_n: 1.0083569860056805 Loss: 0.0002019149601744359\n",
      "Iteration: 9999 lambda_n: 0.9213271249604441 Loss: 0.00020191496017443598\n",
      "Iteration: 10000 lambda_n: 0.9478186284738817 Loss: 0.00020191496017443603\n",
      "beta:  0.0022065297892106934\n",
      "gamma: 0.004099555511074575\n"
     ]
    }
   ],
   "source": [
    "# BA    \n",
    "ba = time.time()\n",
    "x_BA_list, z_BA_list, dual_BA_list, iterations_BA   = BA.Briceno_Arias(N, M, frobenius_norm, Grad_Phi_NA, Sigma, D, (x1,x2,x3),gamma=1e-3, lambdan=1e-3)\n",
    "fin = time.time()\n",
    "\n",
    "Times[\"BA\"] = fin - ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05a2ff12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:\n",
      " 10000\n",
      "Primal: (x1,x2,x3)\n",
      " ((array([[1188.10, 1188.10, 1188.10, 1188.10, 1188.10],\n",
      "       [44.67, 44.67, 44.67, 44.67, 44.67],\n",
      "       [25.59, 25.59, 25.59, 25.59, 25.59]]), array([[186.57, 704.01, 833.33, 1188.10, 714.29],\n",
      "       [8.88, 30.91, 44.67, 42.89, 43.48],\n",
      "       [4.55, 15.09, 25.59, 19.01, 25.59]]), array([[0.00, 0.00, 96.41, 0.00, 716.65]])), 'infactible')\n",
      "Primal: (z1,z2,z3)\n",
      " (array([[1188.07, 1187.90, 1188.09, 1188.25, 1188.08],\n",
      "       [44.40, 42.67, 45.24, 44.29, 44.43],\n",
      "       [25.07, 21.78, 25.41, 24.85, 27.30]]), array([[186.57, 704.01, 833.33, 1188.10, 714.29],\n",
      "       [8.88, 30.91, 44.67, 42.89, 43.48],\n",
      "       [4.55, 15.09, 25.59, 19.01, 25.59]]), array([[0.00, 0.00, 96.41, 0.00, 716.65]]))\n",
      "Dual: (Capacity, Equilibrium)\n",
      " (array([[0.00, 0.00, 0.00, 47.09, 0.00],\n",
      "       [0.00, 0.00, 171.56, 0.00, 0.00],\n",
      "       [0.00, 0.00, 21.45, 0.00, 533.17]]), array([[1865.66, 6336.06, 10000.00, 8363.79, 10000.00]]))\n"
     ]
    }
   ],
   "source": [
    "iter_ = -2\n",
    "print(\"Iterations:\\n\",iterations_BA)\n",
    "print(\"Primal: (x1,x2,x3)\\n\", x_BA_list[iter_])\n",
    "print(\"Primal: (z1,z2,z3)\\n\", z_BA_list[iter_])\n",
    "print(\"Dual: (Capacity, Equilibrium)\\n\",dual_BA_list[iter_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9a848c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r: 1\n",
      "Iteration: 1 Loss: 3.3812817860841715\n",
      "Iteration: 2 Loss: 2.6043290153410883\n",
      "Iteration: 3 Loss: 2.0768935825620094\n",
      "Iteration: 4 Loss: 1.7074166949971805\n",
      "Iteration: 5 Loss: 1.4425624147549017\n",
      "Iteration: 6 Loss: 1.2485521325809281\n",
      "Iteration: 7 Loss: 1.1034924000614938\n",
      "Iteration: 8 Loss: 0.9929462696357049\n",
      "Iteration: 9 Loss: 0.9072207005242396\n",
      "Iteration: 10 Loss: 0.839682886097685\n",
      "Iteration: 11 Loss: 0.7855535307624141\n",
      "Iteration: 12 Loss: 0.7415649061256537\n",
      "Iteration: 13 Loss: 0.7054095155802201\n",
      "Iteration: 14 Loss: 0.6753788718445587\n",
      "Iteration: 15 Loss: 0.6501812321581387\n",
      "Iteration: 16 Loss: 0.6288251992897902\n",
      "Iteration: 17 Loss: 0.6105411753765044\n",
      "Iteration: 18 Loss: 0.5947266425347872\n",
      "Iteration: 19 Loss: 0.5809069042966676\n",
      "Iteration: 20 Loss: 0.5687061643491618\n",
      "Iteration: 21 Loss: 0.5578257547837141\n",
      "Iteration: 22 Loss: 0.5480274812264406\n",
      "Iteration: 23 Loss: 0.5391207432591351\n",
      "Iteration: 24 Loss: 0.5309525073665806\n",
      "Iteration: 25 Loss: 0.5233994706270942\n",
      "Iteration: 26 Loss: 0.5163619231548581\n",
      "Iteration: 27 Loss: 0.5097589338355502\n",
      "Iteration: 28 Loss: 0.503524568283528\n",
      "Iteration: 29 Loss: 0.4975654862150525\n",
      "Iteration: 30 Loss: 0.49178528605385285\n",
      "Iteration: 31 Loss: 0.4861797499138025\n",
      "Iteration: 32 Loss: 0.4807577736527737\n",
      "Iteration: 33 Loss: 0.4755168440772761\n",
      "Iteration: 34 Loss: 0.4704572971165988\n",
      "Iteration: 35 Loss: 0.46557168965284995\n",
      "Iteration: 36 Loss: 0.4608464476987116\n",
      "Iteration: 37 Loss: 0.4562655819712019\n",
      "Iteration: 38 Loss: 0.45181287124733427\n",
      "Iteration: 39 Loss: 0.44747322250707194\n",
      "Iteration: 40 Loss: 0.44323345037674433\n",
      "Iteration: 41 Loss: 0.43908260121266673\n",
      "Iteration: 42 Loss: 0.4350119404758111\n",
      "Iteration: 43 Loss: 0.4310147209867635\n",
      "Iteration: 44 Loss: 0.4270858366301383\n",
      "Iteration: 45 Loss: 0.4232214437686315\n",
      "Iteration: 46 Loss: 0.419418607009683\n",
      "Iteration: 47 Loss: 0.41567500191146073\n",
      "Iteration: 48 Loss: 0.4119886876179063\n",
      "Iteration: 49 Loss: 0.4083579484098197\n",
      "Iteration: 50 Loss: 0.4047811946197895\n",
      "Iteration: 51 Loss: 0.4012569094015993\n",
      "Iteration: 52 Loss: 0.39778362726321653\n",
      "Iteration: 53 Loss: 0.39435993184136897\n",
      "Iteration: 54 Loss: 0.3909844630577316\n",
      "Iteration: 55 Loss: 0.3876559267507126\n",
      "Iteration: 56 Loss: 0.38437310259332086\n",
      "Iteration: 57 Loss: 0.3811348482987318\n",
      "Iteration: 58 Loss: 0.3779400996811947\n",
      "Iteration: 59 Loss: 0.37478786710949574\n",
      "Iteration: 60 Loss: 0.371677229365686\n",
      "Iteration: 61 Loss: 0.36860732603547347\n",
      "Iteration: 62 Loss: 0.3655773494399251\n",
      "Iteration: 63 Loss: 0.3625865368828599\n",
      "Iteration: 64 Loss: 0.35963416371865886\n",
      "Iteration: 65 Loss: 0.35671953749638147\n",
      "Iteration: 66 Loss: 0.35384050467569994\n",
      "Iteration: 67 Loss: 0.35099361968222\n",
      "Iteration: 68 Loss: 0.3481814961362868\n",
      "Iteration: 69 Loss: 0.3454058169029975\n",
      "Iteration: 70 Loss: 0.34266617090269497\n",
      "Iteration: 71 Loss: 0.3399617039233863\n",
      "Iteration: 72 Loss: 0.33729107946811765\n",
      "Iteration: 73 Loss: 0.33465393654296477\n",
      "Iteration: 74 Loss: 0.33204967151040077\n",
      "Iteration: 75 Loss: 0.3294776873370724\n",
      "Iteration: 76 Loss: 0.32693740988915826\n",
      "Iteration: 77 Loss: 0.32442829195006107\n",
      "Iteration: 78 Loss: 0.32194981227290875\n",
      "Iteration: 79 Loss: 0.3195014726772428\n",
      "Iteration: 80 Loss: 0.31708279472080125\n",
      "Iteration: 81 Loss: 0.314693316660925\n",
      "Iteration: 82 Loss: 0.31233259096422794\n",
      "Iteration: 83 Loss: 0.31000018238866994\n",
      "Iteration: 84 Loss: 0.3076956665590691\n",
      "Iteration: 85 Loss: 0.30541862892655375\n",
      "Iteration: 86 Loss: 0.3031686640075605\n",
      "Iteration: 87 Loss: 0.3009453748171317\n",
      "Iteration: 88 Loss: 0.2987483724329664\n",
      "Iteration: 89 Loss: 0.29657727564578645\n",
      "Iteration: 90 Loss: 0.2944317106664137\n",
      "Iteration: 91 Loss: 0.2923113108706012\n",
      "Iteration: 92 Loss: 0.290215716569858\n",
      "Iteration: 93 Loss: 0.2881445748011572\n",
      "Iteration: 94 Loss: 0.28609753913128644\n",
      "Iteration: 95 Loss: 0.2840742694733007\n",
      "Iteration: 96 Loss: 0.28207443191350556\n",
      "Iteration: 97 Loss: 0.28009769854791516\n",
      "Iteration: 98 Loss: 0.27814374732740577\n",
      "Iteration: 99 Loss: 0.2762122619109129\n",
      "Iteration: 100 Loss: 0.27430293152610163\n",
      "Iteration: 101 Loss: 0.27241545083699104\n",
      "Iteration: 102 Loss: 0.27054951981808156\n",
      "Iteration: 103 Loss: 0.268704843634611\n",
      "Iteration: 104 Loss: 0.2668811325286804\n",
      "Iteration: 105 Loss: 0.26507810171113383\n",
      "Iteration: 106 Loss: 0.26329547125927955\n",
      "Iteration: 107 Loss: 0.2615329660208131\n",
      "Iteration: 108 Loss: 0.25979031552469734\n",
      "Iteration: 109 Loss: 0.2580672539003444\n",
      "Iteration: 110 Loss: 0.25636351980737593\n",
      "Iteration: 111 Loss: 0.25467885637974785\n",
      "Iteration: 112 Loss: 0.253013010586766\n",
      "Iteration: 113 Loss: 0.2513640406367289\n",
      "Iteration: 114 Loss: 0.2497333724961476\n",
      "Iteration: 115 Loss: 0.24812109883411992\n",
      "Iteration: 116 Loss: 0.24652702843462806\n",
      "Iteration: 117 Loss: 0.2449508774286649\n",
      "Iteration: 118 Loss: 0.24339235181280253\n",
      "Iteration: 119 Loss: 0.2418511775667387\n",
      "Iteration: 120 Loss: 0.2403271125604428\n",
      "Iteration: 121 Loss: 0.23881996297194485\n",
      "Iteration: 122 Loss: 0.237268383242267\n",
      "Iteration: 123 Loss: 0.23569016252869995\n",
      "Iteration: 124 Loss: 0.23411262380222583\n",
      "Iteration: 125 Loss: 0.2327825983036135\n",
      "Iteration: 126 Loss: 0.22968315231401604\n",
      "Iteration: 127 Loss: 0.2242493512937348\n",
      "Iteration: 128 Loss: 0.21744050750317914\n",
      "Iteration: 129 Loss: 0.21002119900725683\n",
      "Iteration: 130 Loss: 0.20255392020934085\n",
      "Iteration: 131 Loss: 0.19543845017742423\n",
      "Iteration: 132 Loss: 0.18879621660379262\n",
      "Iteration: 133 Loss: 0.18286310053411473\n",
      "Iteration: 134 Loss: 0.17815713207543604\n",
      "Iteration: 135 Loss: 0.17981885004838438\n",
      "Iteration: 136 Loss: 0.18064697361921567\n",
      "Iteration: 137 Loss: 0.1807427960643741\n",
      "Iteration: 138 Loss: 0.180223547796717\n",
      "Iteration: 139 Loss: 0.1792114576564041\n",
      "Iteration: 140 Loss: 0.17781782788651146\n",
      "Iteration: 141 Loss: 0.17618911542133917\n",
      "Iteration: 142 Loss: 0.17438381008758985\n",
      "Iteration: 143 Loss: 0.17248467239202145\n",
      "Iteration: 144 Loss: 0.17056283842814995\n",
      "Iteration: 145 Loss: 0.16867491190357117\n",
      "Iteration: 146 Loss: 0.16686287920444318\n",
      "Iteration: 147 Loss: 0.16515518107076674\n",
      "Iteration: 148 Loss: 0.16356836274192363\n",
      "Iteration: 149 Loss: 0.16210900421683266\n",
      "Iteration: 150 Loss: 0.16115537869680202\n",
      "Iteration: 151 Loss: 0.1604991628763608\n",
      "Iteration: 152 Loss: 0.15975857174941624\n",
      "Iteration: 153 Loss: 0.15894731440707913\n",
      "Iteration: 154 Loss: 0.1580799590675146\n",
      "Iteration: 155 Loss: 0.15717097526041943\n",
      "Iteration: 156 Loss: 0.15619869701514066\n",
      "Iteration: 157 Loss: 0.15510879182220152\n",
      "Iteration: 158 Loss: 0.15392694949733454\n",
      "Iteration: 159 Loss: 0.15267061293101747\n",
      "Iteration: 160 Loss: 0.15135738335577273\n",
      "Iteration: 161 Loss: 0.1500036357935868\n",
      "Iteration: 162 Loss: 0.14862397006663214\n",
      "Iteration: 163 Loss: 0.1472310056884086\n",
      "Iteration: 164 Loss: 0.14583537666598273\n",
      "Iteration: 165 Loss: 0.14446589072729338\n",
      "Iteration: 166 Loss: 0.1431549215395742\n",
      "Iteration: 167 Loss: 0.1418407122246324\n",
      "Iteration: 168 Loss: 0.14053059871062745\n",
      "Iteration: 169 Loss: 0.1392310910012442\n",
      "Iteration: 170 Loss: 0.1379477974123998\n",
      "Iteration: 171 Loss: 0.13668539012081046\n",
      "Iteration: 172 Loss: 0.13544760613252665\n",
      "Iteration: 173 Loss: 0.13423727765143995\n",
      "Iteration: 174 Loss: 0.13305638583640342\n",
      "Iteration: 175 Loss: 0.13190613212678778\n",
      "Iteration: 176 Loss: 0.13078702168745707\n",
      "Iteration: 177 Loss: 0.12969895404539986\n",
      "Iteration: 178 Loss: 0.12864131661875586\n",
      "Iteration: 179 Loss: 0.12762255235724226\n",
      "Iteration: 180 Loss: 0.12663298512913407\n",
      "Iteration: 181 Loss: 0.12566702711528685\n",
      "Iteration: 182 Loss: 0.12472317376706563\n",
      "Iteration: 183 Loss: 0.12379973466900804\n",
      "Iteration: 184 Loss: 0.12289513604101794\n",
      "Iteration: 185 Loss: 0.12200803424290778\n",
      "Iteration: 186 Loss: 0.12113693130069267\n",
      "Iteration: 187 Loss: 0.12028036833749167\n",
      "Iteration: 188 Loss: 0.11943694432169119\n",
      "Iteration: 189 Loss: 0.11860533117877282\n",
      "Iteration: 190 Loss: 0.11778428618424819\n",
      "Iteration: 191 Loss: 0.11697266226985142\n",
      "Iteration: 192 Loss: 0.11616941705263809\n",
      "Iteration: 193 Loss: 0.11537362219330434\n",
      "Iteration: 194 Loss: 0.11458447731987181\n",
      "Iteration: 195 Loss: 0.11380134239566823\n",
      "Iteration: 196 Loss: 0.11302384773963288\n",
      "Iteration: 197 Loss: 0.11225246652657696\n",
      "Iteration: 198 Loss: 0.11149566949884145\n",
      "Iteration: 199 Loss: 0.11086889119447241\n",
      "Iteration: 200 Loss: 0.10931639991459664\n",
      "Iteration: 201 Loss: 0.10675937025891692\n",
      "Iteration: 202 Loss: 0.10375156118769015\n",
      "Iteration: 203 Loss: 0.10070581045199432\n",
      "Iteration: 204 Loss: 0.09790619176158057\n",
      "Iteration: 205 Loss: 0.0973557607352874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 206 Loss: 0.09788445334480073\n",
      "Iteration: 207 Loss: 0.09792197799280167\n",
      "Iteration: 208 Loss: 0.09752853440225936\n",
      "Iteration: 209 Loss: 0.09679723848807675\n",
      "Iteration: 210 Loss: 0.0958223517039786\n",
      "Iteration: 211 Loss: 0.09468972876743448\n",
      "Iteration: 212 Loss: 0.09348433122266885\n",
      "Iteration: 213 Loss: 0.09228030580768751\n",
      "Iteration: 214 Loss: 0.09113435062364324\n",
      "Iteration: 215 Loss: 0.09009317890838568\n",
      "Iteration: 216 Loss: 0.08980287127975663\n",
      "Iteration: 217 Loss: 0.0894030969229908\n",
      "Iteration: 218 Loss: 0.08890381515617503\n",
      "Iteration: 219 Loss: 0.08832196419255053\n",
      "Iteration: 220 Loss: 0.08767808567868854\n",
      "Iteration: 221 Loss: 0.0869937139173083\n",
      "Iteration: 222 Loss: 0.08628928711870164\n",
      "Iteration: 223 Loss: 0.08558253192773174\n",
      "Iteration: 224 Loss: 0.0848874240319776\n",
      "Iteration: 225 Loss: 0.0842137475395284\n",
      "Iteration: 226 Loss: 0.08367311429807225\n",
      "Iteration: 227 Loss: 0.08316710794721931\n",
      "Iteration: 228 Loss: 0.08264166412260232\n",
      "Iteration: 229 Loss: 0.08209975348522751\n",
      "Iteration: 230 Loss: 0.081545552714181\n",
      "Iteration: 231 Loss: 0.08098374295335053\n",
      "Iteration: 232 Loss: 0.0804189359513605\n",
      "Iteration: 233 Loss: 0.07985524832809258\n",
      "Iteration: 234 Loss: 0.07929602762221442\n",
      "Iteration: 235 Loss: 0.07874371947858456\n",
      "Iteration: 236 Loss: 0.07819985454638494\n",
      "Iteration: 237 Loss: 0.07768117984337128\n",
      "Iteration: 238 Loss: 0.07717362470429814\n",
      "Iteration: 239 Loss: 0.07666503110061523\n",
      "Iteration: 240 Loss: 0.07615616371367863\n",
      "Iteration: 241 Loss: 0.07564796542615809\n",
      "Iteration: 242 Loss: 0.07514141749223291\n",
      "Iteration: 243 Loss: 0.07463743158025699\n",
      "Iteration: 244 Loss: 0.07413677594756105\n",
      "Iteration: 245 Loss: 0.07364735749623802\n",
      "Iteration: 246 Loss: 0.07316444231843514\n",
      "Iteration: 247 Loss: 0.07268385898981154\n",
      "Iteration: 248 Loss: 0.07220549887324786\n",
      "Iteration: 249 Loss: 0.07172938474491672\n",
      "Iteration: 250 Loss: 0.07125563225537826\n",
      "Iteration: 251 Loss: 0.07078441242653273\n",
      "Iteration: 252 Loss: 0.0703159185034837\n",
      "Iteration: 253 Loss: 0.06985033922172028\n",
      "Iteration: 254 Loss: 0.06938783941455177\n",
      "Iteration: 255 Loss: 0.06892854794874378\n",
      "Iteration: 256 Loss: 0.0684725522766459\n",
      "Iteration: 257 Loss: 0.06801989843799526\n",
      "Iteration: 258 Loss: 0.06757059511643289\n",
      "Iteration: 259 Loss: 0.06712462031971132\n",
      "Iteration: 260 Loss: 0.06668192936373463\n",
      "Iteration: 261 Loss: 0.06624246305107462\n",
      "Iteration: 262 Loss: 0.06580615519882786\n",
      "Iteration: 263 Loss: 0.06537293894934865\n",
      "Iteration: 264 Loss: 0.06494275155977655\n",
      "Iteration: 265 Loss: 0.06451553759111875\n",
      "Iteration: 266 Loss: 0.06409125059276849\n",
      "Iteration: 267 Loss: 0.06366985349952367\n",
      "Iteration: 268 Loss: 0.06325131802751488\n",
      "Iteration: 269 Loss: 0.06283562337950366\n",
      "Iteration: 270 Loss: 0.062422754558022805\n",
      "Iteration: 271 Loss: 0.06201270054712987\n",
      "Iteration: 272 Loss: 0.06160545257032166\n",
      "Iteration: 273 Loss: 0.061201002572453624\n",
      "Iteration: 274 Loss: 0.06079934201473105\n",
      "Iteration: 275 Loss: 0.060400461019387064\n",
      "Iteration: 276 Loss: 0.0600043478579757\n",
      "Iteration: 277 Loss: 0.05961098874590586\n",
      "Iteration: 278 Loss: 0.0592203678860587\n",
      "Iteration: 279 Loss: 0.058832467695021\n",
      "Iteration: 280 Loss: 0.05844726914489168\n",
      "Iteration: 281 Loss: 0.05806475215960249\n",
      "Iteration: 282 Loss: 0.05768489601499374\n",
      "Iteration: 283 Loss: 0.05730767970443509\n",
      "Iteration: 284 Loss: 0.056933082244782905\n",
      "Iteration: 285 Loss: 0.056561082909563576\n",
      "Iteration: 286 Loss: 0.05619166138654943\n",
      "Iteration: 287 Loss: 0.05582479786480048\n",
      "Iteration: 288 Loss: 0.05546047306166275\n",
      "Iteration: 289 Loss: 0.05509866820321198\n",
      "Iteration: 290 Loss: 0.054739364972591104\n",
      "Iteration: 291 Loss: 0.054382545439999534\n",
      "Iteration: 292 Loss: 0.05402819198625757\n",
      "Iteration: 293 Loss: 0.05367628722935669\n",
      "Iteration: 294 Loss: 0.05332681396063421\n",
      "Iteration: 295 Loss: 0.05297975509451115\n",
      "Iteration: 296 Loss: 0.05263509363334147\n",
      "Iteration: 297 Loss: 0.05229281264699864\n",
      "Iteration: 298 Loss: 0.05195289526544227\n",
      "Iteration: 299 Loss: 0.051615324681635884\n",
      "Iteration: 300 Loss: 0.051280084161821074\n",
      "Iteration: 301 Loss: 0.05094715706015177\n",
      "Iteration: 302 Loss: 0.05061652683500338\n",
      "Iteration: 303 Loss: 0.05028817706476475\n",
      "Iteration: 304 Loss: 0.04996209146149678\n",
      "Iteration: 305 Loss: 0.04963825388145003\n",
      "Iteration: 306 Loss: 0.0493166483319674\n",
      "Iteration: 307 Loss: 0.04899725897477142\n",
      "Iteration: 308 Loss: 0.04868007012596834\n",
      "Iteration: 309 Loss: 0.04836506625333794\n",
      "Iteration: 310 Loss: 0.04805223197160106\n",
      "Iteration: 311 Loss: 0.0477415520363731\n",
      "Iteration: 312 Loss: 0.04743301133747484\n",
      "Iteration: 313 Loss: 0.04712659489216745\n",
      "Iteration: 314 Loss: 0.04682228783875517\n",
      "Iteration: 315 Loss: 0.04652007543086413\n",
      "Iteration: 316 Loss: 0.04621994303257516\n",
      "Iteration: 317 Loss: 0.045921876114472314\n",
      "Iteration: 318 Loss: 0.045625860250584085\n",
      "Iteration: 319 Loss: 0.04533188111611784\n",
      "Iteration: 320 Loss: 0.04503992448586405\n",
      "Iteration: 321 Loss: 0.04474997623311148\n",
      "Iteration: 322 Loss: 0.0444620223289271\n",
      "Iteration: 323 Loss: 0.044176048841665615\n",
      "Iteration: 324 Loss: 0.04389204193659415\n",
      "Iteration: 325 Loss: 0.04360998787554312\n",
      "Iteration: 326 Loss: 0.04332987301653156\n",
      "Iteration: 327 Loss: 0.04305168381332319\n",
      "Iteration: 328 Loss: 0.042775406814911335\n",
      "Iteration: 329 Loss: 0.04250102866492563\n",
      "Iteration: 330 Loss: 0.04222853610098536\n",
      "Iteration: 331 Loss: 0.04195791595400884\n",
      "Iteration: 332 Loss: 0.04168915514750966\n",
      "Iteration: 333 Loss: 0.04142224069689411\n",
      "Iteration: 334 Loss: 0.04115715970878228\n",
      "Iteration: 335 Loss: 0.04089389938035574\n",
      "Iteration: 336 Loss: 0.04063244699874774\n",
      "Iteration: 337 Loss: 0.04037278994047166\n",
      "Iteration: 338 Loss: 0.04011491567088562\n",
      "Iteration: 339 Loss: 0.039858811743691484\n",
      "Iteration: 340 Loss: 0.03960446580045467\n",
      "Iteration: 341 Loss: 0.03935186557013895\n",
      "Iteration: 342 Loss: 0.039100998868649435\n",
      "Iteration: 343 Loss: 0.038851853598369665\n",
      "Iteration: 344 Loss: 0.03860441774769518\n",
      "Iteration: 345 Loss: 0.03835867939054812\n",
      "Iteration: 346 Loss: 0.03811462668587887\n",
      "Iteration: 347 Loss: 0.03787224787714577\n",
      "Iteration: 348 Loss: 0.037631531291778555\n",
      "Iteration: 349 Loss: 0.03739246534062055\n",
      "Iteration: 350 Loss: 0.037155038517356624\n",
      "Iteration: 351 Loss: 0.03691923939792512\n",
      "Iteration: 352 Loss: 0.03668505663992052\n",
      "Iteration: 353 Loss: 0.03645247898198471\n",
      "Iteration: 354 Loss: 0.036221495243194395\n",
      "Iteration: 355 Loss: 0.03599209432244463\n",
      "Iteration: 356 Loss: 0.03576426519782816\n",
      "Iteration: 357 Loss: 0.03553799692602044\n",
      "Iteration: 358 Loss: 0.035313278641659514\n",
      "Iteration: 359 Loss: 0.03509009955673417\n",
      "Iteration: 360 Loss: 0.03486844895997386\n",
      "Iteration: 361 Loss: 0.03464831621624387\n",
      "Iteration: 362 Loss: 0.03442969076594412\n",
      "Iteration: 363 Loss: 0.034212562124414016\n",
      "Iteration: 364 Loss: 0.0339969198813439\n",
      "Iteration: 365 Loss: 0.03378275370019103\n",
      "Iteration: 366 Loss: 0.033570053317602426\n",
      "Iteration: 367 Loss: 0.03335880854284321\n",
      "Iteration: 368 Loss: 0.03314900925723286\n",
      "Iteration: 369 Loss: 0.032940645413585365\n",
      "Iteration: 370 Loss: 0.03273370703565942\n",
      "Iteration: 371 Loss: 0.032528184217612\n",
      "Iteration: 372 Loss: 0.0323240671234612\n",
      "Iteration: 373 Loss: 0.0321213459865531\n",
      "Iteration: 374 Loss: 0.031920011109039546\n",
      "Iteration: 375 Loss: 0.03172005286135637\n",
      "Iteration: 376 Loss: 0.031521461681713984\n",
      "Iteration: 377 Loss: 0.03132422807559087\n",
      "Iteration: 378 Loss: 0.031128342615234037\n",
      "Iteration: 379 Loss: 0.030933795939165544\n",
      "Iteration: 380 Loss: 0.0307405787516943\n",
      "Iteration: 381 Loss: 0.030548681822433747\n",
      "Iteration: 382 Loss: 0.03035809598582463\n",
      "Iteration: 383 Loss: 0.03016881214066197\n",
      "Iteration: 384 Loss: 0.02998082124962912\n",
      "Iteration: 385 Loss: 0.029794114338832\n",
      "Iteration: 386 Loss: 0.02960868249734339\n",
      "Iteration: 387 Loss: 0.02942451687674729\n",
      "Iteration: 388 Loss: 0.02924160869068707\n",
      "Iteration: 389 Loss: 0.02905994921442157\n",
      "Iteration: 390 Loss: 0.028879529784379455\n",
      "Iteration: 391 Loss: 0.028700341797721345\n",
      "Iteration: 392 Loss: 0.028522376711904055\n",
      "Iteration: 393 Loss: 0.028345626044247015\n",
      "Iteration: 394 Loss: 0.028170081371503685\n",
      "Iteration: 395 Loss: 0.027995734329436067\n",
      "Iteration: 396 Loss: 0.027822576612391917\n",
      "Iteration: 397 Loss: 0.02765059997288358\n",
      "Iteration: 398 Loss: 0.027479796221173716\n",
      "Iteration: 399 Loss: 0.027310157224860267\n",
      "Iteration: 400 Loss: 0.02714167490846497\n",
      "Iteration: 401 Loss: 0.02697434125302876\n",
      "Iteration: 402 Loss: 0.026808148295705155\n",
      "Iteration: 403 Loss: 0.026643088129358276\n",
      "Iteration: 404 Loss: 0.026479152902165987\n",
      "Iteration: 405 Loss: 0.026316334817222283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 406 Loss: 0.02615462613214493\n",
      "Iteration: 407 Loss: 0.025994019158686536\n",
      "Iteration: 408 Loss: 0.025834506262345895\n",
      "Iteration: 409 Loss: 0.025676079861984044\n",
      "Iteration: 410 Loss: 0.02551873242944361\n",
      "Iteration: 411 Loss: 0.025362456489169714\n",
      "Iteration: 412 Loss: 0.025207244617834846\n",
      "Iteration: 413 Loss: 0.025053089443965285\n",
      "Iteration: 414 Loss: 0.024899983647571747\n",
      "Iteration: 415 Loss: 0.02474791995978096\n",
      "Iteration: 416 Loss: 0.024596891162473294\n",
      "Iteration: 417 Loss: 0.02444689008791882\n",
      "Iteration: 418 Loss: 0.024297909618419523\n",
      "Iteration: 419 Loss: 0.024149942685953905\n",
      "Iteration: 420 Loss: 0.024002982271822706\n",
      "Iteration: 421 Loss: 0.023857021406297335\n",
      "Iteration: 422 Loss: 0.02371205316827339\n",
      "Iteration: 423 Loss: 0.02356807068492435\n",
      "Iteration: 424 Loss: 0.023425067131359816\n",
      "Iteration: 425 Loss: 0.023283035730283366\n",
      "Iteration: 426 Loss: 0.023141969751657856\n",
      "Iteration: 427 Loss: 0.02300186251236768\n",
      "Iteration: 428 Loss: 0.022862707375888875\n",
      "Iteration: 429 Loss: 0.02272449775195671\n",
      "Iteration: 430 Loss: 0.022587227096240392\n",
      "Iteration: 431 Loss: 0.022450888910018046\n",
      "Iteration: 432 Loss: 0.02231547673985217\n",
      "Iteration: 433 Loss: 0.022180984177271424\n",
      "Iteration: 434 Loss: 0.022047404858453096\n",
      "Iteration: 435 Loss: 0.021914732463906773\n",
      "Iteration: 436 Loss: 0.021782960718162526\n",
      "Iteration: 437 Loss: 0.021652083389459054\n",
      "Iteration: 438 Loss: 0.021522094289437004\n",
      "Iteration: 439 Loss: 0.021392987272831336\n",
      "Iteration: 440 Loss: 0.021264756237168742\n",
      "Iteration: 441 Loss: 0.02113739512246595\n",
      "Iteration: 442 Loss: 0.0210108979109307\n",
      "Iteration: 443 Loss: 0.02088525862666298\n",
      "Iteration: 444 Loss: 0.020760471335362702\n",
      "Iteration: 445 Loss: 0.020636530144035808\n",
      "Iteration: 446 Loss: 0.0205134292007038\n",
      "Iteration: 447 Loss: 0.020391162694114222\n",
      "Iteration: 448 Loss: 0.020269724853456017\n",
      "Iteration: 449 Loss: 0.020149109948075387\n",
      "Iteration: 450 Loss: 0.020029312287190914\n",
      "Iteration: 451 Loss: 0.019910326219617136\n",
      "Iteration: 452 Loss: 0.01979214613348346\n",
      "Iteration: 453 Loss: 0.01967476645596054\n",
      "Iteration: 454 Loss: 0.01955818165298407\n",
      "Iteration: 455 Loss: 0.01944238622898396\n",
      "Iteration: 456 Loss: 0.01932737472661348\n",
      "Iteration: 457 Loss: 0.019213141726481385\n",
      "Iteration: 458 Loss: 0.019099681846886825\n",
      "Iteration: 459 Loss: 0.018986989743552594\n",
      "Iteration: 460 Loss: 0.01887506010936519\n",
      "Iteration: 461 Loss: 0.018763887674113442\n",
      "Iteration: 462 Loss: 0.018653467204228506\n",
      "Iteration: 463 Loss: 0.01854379350252945\n",
      "Iteration: 464 Loss: 0.018434861407966602\n",
      "Iteration: 465 Loss: 0.01832666579536942\n",
      "Iteration: 466 Loss: 0.018219201575195177\n",
      "Iteration: 467 Loss: 0.01811246369327872\n",
      "Iteration: 468 Loss: 0.01800644713059014\n",
      "Iteration: 469 Loss: 0.017901146902970202\n",
      "Iteration: 470 Loss: 0.01779655806091364\n",
      "Iteration: 471 Loss: 0.017692675689308743\n",
      "Iteration: 472 Loss: 0.01758949490720196\n",
      "Iteration: 473 Loss: 0.017487010867557575\n",
      "Iteration: 474 Loss: 0.017385218757020964\n",
      "Iteration: 475 Loss: 0.017284113795684092\n",
      "Iteration: 476 Loss: 0.01718369123684968\n",
      "Iteration: 477 Loss: 0.017083946366801685\n",
      "Iteration: 478 Loss: 0.0169848745045726\n",
      "Iteration: 479 Loss: 0.016886471001715798\n",
      "Iteration: 480 Loss: 0.016788731242077616\n",
      "Iteration: 481 Loss: 0.016691650641571732\n",
      "Iteration: 482 Loss: 0.01659522464795593\n",
      "Iteration: 483 Loss: 0.016499448740609136\n",
      "Iteration: 484 Loss: 0.016404318430311125\n",
      "Iteration: 485 Loss: 0.016309829259022544\n",
      "Iteration: 486 Loss: 0.016215976799667218\n",
      "Iteration: 487 Loss: 0.01612275665591735\n",
      "Iteration: 488 Loss: 0.01603016446197716\n",
      "Iteration: 489 Loss: 0.015938195882371393\n",
      "Iteration: 490 Loss: 0.015846846611731562\n",
      "Iteration: 491 Loss: 0.01575611237458894\n",
      "Iteration: 492 Loss: 0.015665988925162827\n",
      "Iteration: 493 Loss: 0.01557647204715613\n",
      "Iteration: 494 Loss: 0.015487557553547693\n",
      "Iteration: 495 Loss: 0.01539924128638912\n",
      "Iteration: 496 Loss: 0.015311519116602355\n",
      "Iteration: 497 Loss: 0.015224386943777842\n",
      "Iteration: 498 Loss: 0.015137840695974304\n",
      "Iteration: 499 Loss: 0.01505187632952093\n",
      "Iteration: 500 Loss: 0.014966489828820474\n",
      "Iteration: 501 Loss: 0.014881677206152473\n",
      "Iteration: 502 Loss: 0.014797434501480048\n",
      "Iteration: 503 Loss: 0.01471375778225676\n",
      "Iteration: 504 Loss: 0.01463064314323471\n",
      "Iteration: 505 Loss: 0.01454808670627484\n",
      "Iteration: 506 Loss: 0.014466084620157494\n",
      "Iteration: 507 Loss: 0.014384633060395252\n",
      "Iteration: 508 Loss: 0.01430372822904698\n",
      "Iteration: 509 Loss: 0.01422336635453211\n",
      "Iteration: 510 Loss: 0.014143543691447599\n",
      "Iteration: 511 Loss: 0.014064256520386488\n",
      "Iteration: 512 Loss: 0.013985501147754334\n",
      "Iteration: 513 Loss: 0.013907273905592012\n",
      "Iteration: 514 Loss: 0.013829571151396908\n",
      "Iteration: 515 Loss: 0.01375238926794407\n",
      "Iteration: 516 Loss: 0.013675724663111673\n",
      "Iteration: 517 Loss: 0.013599573769706129\n",
      "Iteration: 518 Loss: 0.013523933045287624\n",
      "Iteration: 519 Loss: 0.013448798971999028\n",
      "Iteration: 520 Loss: 0.013374168056394277\n",
      "Iteration: 521 Loss: 0.013300036829268516\n",
      "Iteration: 522 Loss: 0.013226401845488659\n",
      "Iteration: 523 Loss: 0.01315325968382741\n",
      "Iteration: 524 Loss: 0.013080606946796353\n",
      "Iteration: 525 Loss: 0.013008440260479835\n",
      "Iteration: 526 Loss: 0.012936756274371792\n",
      "Iteration: 527 Loss: 0.012865551661213072\n",
      "Iteration: 528 Loss: 0.012794823116829886\n",
      "Iteration: 529 Loss: 0.012724567359971696\n",
      "Iteration: 530 Loss: 0.012654235871508783\n",
      "Iteration: 531 Loss: 0.012584248716268957\n",
      "Iteration: 532 Loss: 0.012514820416500835\n",
      "Iteration: 533 Loss: 0.012445992458009644\n",
      "Iteration: 534 Loss: 0.012377738345879914\n",
      "Iteration: 535 Loss: 0.012310006034335016\n",
      "Iteration: 536 Loss: 0.01224275005324137\n",
      "Iteration: 537 Loss: 0.012175943923754352\n",
      "Iteration: 538 Loss: 0.012109578272062162\n",
      "Iteration: 539 Loss: 0.012043653420421732\n",
      "Iteration: 540 Loss: 0.011978172548884707\n",
      "Iteration: 541 Loss: 0.011913137713138029\n",
      "Iteration: 542 Loss: 0.011848548493883344\n",
      "Iteration: 543 Loss: 0.011784402169006057\n",
      "Iteration: 544 Loss: 0.011720694423891858\n",
      "Iteration: 545 Loss: 0.011657420059488912\n",
      "Iteration: 546 Loss: 0.011594573524480538\n",
      "Iteration: 547 Loss: 0.011532149279230736\n",
      "Iteration: 548 Loss: 0.011470142041703343\n",
      "Iteration: 549 Loss: 0.011408546949268626\n",
      "Iteration: 550 Loss: 0.011347359648939861\n",
      "Iteration: 551 Loss: 0.01128657632097821\n",
      "Iteration: 552 Loss: 0.01122619364450857\n",
      "Iteration: 553 Loss: 0.01116620872045412\n",
      "Iteration: 554 Loss: 0.011106618970777695\n",
      "Iteration: 555 Loss: 0.011047422032229002\n",
      "Iteration: 556 Loss: 0.010988615658705226\n",
      "Iteration: 557 Loss: 0.01093019764106451\n",
      "Iteration: 558 Loss: 0.010872165748306729\n",
      "Iteration: 559 Loss: 0.010814517690266246\n",
      "Iteration: 560 Loss: 0.0107572510994821\n",
      "Iteration: 561 Loss: 0.010700363528544154\n",
      "Iteration: 562 Loss: 0.010643852458691889\n",
      "Iteration: 563 Loss: 0.010587715315510145\n",
      "Iteration: 564 Loss: 0.010531949488044624\n",
      "Iteration: 565 Loss: 0.010476552348348498\n",
      "Iteration: 566 Loss: 0.010421521269271124\n",
      "Iteration: 567 Loss: 0.010366853639102936\n",
      "Iteration: 568 Loss: 0.010312546872395755\n",
      "Iteration: 569 Loss: 0.010258598416862197\n",
      "Iteration: 570 Loss: 0.010205005756691074\n",
      "Iteration: 571 Loss: 0.010151766412887682\n",
      "Iteration: 572 Loss: 0.010098877941391034\n",
      "Iteration: 573 Loss: 0.010046337929740039\n",
      "Iteration: 574 Loss: 0.00999414399300403\n",
      "Iteration: 575 Loss: 0.009942293769574978\n",
      "Iteration: 576 Loss: 0.00989078491728148\n",
      "Iteration: 577 Loss: 0.009839615110123998\n",
      "Iteration: 578 Loss: 0.009788782035801418\n",
      "Iteration: 579 Loss: 0.009738283394061777\n",
      "Iteration: 580 Loss: 0.009688116895839313\n",
      "Iteration: 581 Loss: 0.009638280263049581\n",
      "Iteration: 582 Loss: 0.00958877122890047\n",
      "Iteration: 583 Loss: 0.009539587538545457\n",
      "Iteration: 584 Loss: 0.00949072694993138\n",
      "Iteration: 585 Loss: 0.009442187234694106\n",
      "Iteration: 586 Loss: 0.009393966178993882\n",
      "Iteration: 587 Loss: 0.009346061584221362\n",
      "Iteration: 588 Loss: 0.00929847126752087\n",
      "Iteration: 589 Loss: 0.009251193062117585\n",
      "Iteration: 590 Loss: 0.0092042248174541\n",
      "Iteration: 591 Loss: 0.009157564399159305\n",
      "Iteration: 592 Loss: 0.009111209688882772\n",
      "Iteration: 593 Loss: 0.009065158584030784\n",
      "Iteration: 594 Loss: 0.009019408997445811\n",
      "Iteration: 595 Loss: 0.008973958857060118\n",
      "Iteration: 596 Loss: 0.008928806105552587\n",
      "Iteration: 597 Loss: 0.008883948700032236\n",
      "Iteration: 598 Loss: 0.008839384611760965\n",
      "Iteration: 599 Loss: 0.008795111825921601\n",
      "Iteration: 600 Loss: 0.008751128341431094\n",
      "Iteration: 601 Loss: 0.008707432170800758\n",
      "Iteration: 602 Loss: 0.0086640213400309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 603 Loss: 0.008620893888537037\n",
      "Iteration: 604 Loss: 0.00857804786909171\n",
      "Iteration: 605 Loss: 0.00853548134778133\n",
      "Iteration: 606 Loss: 0.008493192403967459\n",
      "Iteration: 607 Loss: 0.008451179130244487\n",
      "Iteration: 608 Loss: 0.008409439632394442\n",
      "Iteration: 609 Loss: 0.008367972029329228\n",
      "Iteration: 610 Loss: 0.008326774453029127\n",
      "Iteration: 611 Loss: 0.008285845048467824\n",
      "Iteration: 612 Loss: 0.008245181973531274\n",
      "Iteration: 613 Loss: 0.008204783398929352\n",
      "Iteration: 614 Loss: 0.008164647508103303\n",
      "Iteration: 615 Loss: 0.008124772497129158\n",
      "Iteration: 616 Loss: 0.008085156574621133\n",
      "Iteration: 617 Loss: 0.008045797961633962\n",
      "Iteration: 618 Loss: 0.008006694891566623\n",
      "Iteration: 619 Loss: 0.007967845610068443\n",
      "Iteration: 620 Loss: 0.007929248374948562\n",
      "Iteration: 621 Loss: 0.007890901456085606\n",
      "Iteration: 622 Loss: 0.007852803135342439\n",
      "Iteration: 623 Loss: 0.007814951706479474\n",
      "Iteration: 624 Loss: 0.0077773454750746035\n",
      "Iteration: 625 Loss: 0.00773998275844108\n",
      "Iteration: 626 Loss: 0.007702861885547091\n",
      "Iteration: 627 Loss: 0.007665981196937216\n",
      "Iteration: 628 Loss: 0.007629339044654563\n",
      "Iteration: 629 Loss: 0.007592933792161951\n",
      "Iteration: 630 Loss: 0.007556763814263512\n",
      "Iteration: 631 Loss: 0.0075208274970284174\n",
      "Iteration: 632 Loss: 0.007485123237712409\n",
      "Iteration: 633 Loss: 0.007449649444680717\n",
      "Iteration: 634 Loss: 0.007414404537330481\n",
      "Iteration: 635 Loss: 0.00737938694601604\n",
      "Iteration: 636 Loss: 0.007344595111970238\n",
      "Iteration: 637 Loss: 0.007310027487229482\n",
      "Iteration: 638 Loss: 0.0072756825345580796\n",
      "Iteration: 639 Loss: 0.007241558727375196\n",
      "Iteration: 640 Loss: 0.007207654549678663\n",
      "Iteration: 641 Loss: 0.0071739684959722765\n",
      "Iteration: 642 Loss: 0.007140499071192363\n",
      "Iteration: 643 Loss: 0.007107244790636329\n",
      "Iteration: 644 Loss: 0.007074204179889218\n",
      "Iteration: 645 Loss: 0.007041375774753007\n",
      "Iteration: 646 Loss: 0.007008758121176627\n",
      "Iteration: 647 Loss: 0.006976349775185093\n",
      "Iteration: 648 Loss: 0.006944149302809488\n",
      "Iteration: 649 Loss: 0.006912155280017619\n",
      "Iteration: 650 Loss: 0.006880366292645323\n",
      "Iteration: 651 Loss: 0.006848780936328107\n",
      "Iteration: 652 Loss: 0.006817397816433671\n",
      "Iteration: 653 Loss: 0.00678621554799409\n",
      "Iteration: 654 Loss: 0.006755232755637\n",
      "Iteration: 655 Loss: 0.006724448073521507\n",
      "Iteration: 656 Loss: 0.006693860145272404\n",
      "Iteration: 657 Loss: 0.0066634676239110124\n",
      "Iteration: 658 Loss: 0.006633269171794032\n",
      "Iteration: 659 Loss: 0.006603263460545789\n",
      "Iteration: 660 Loss: 0.006573449170996326\n",
      "Iteration: 661 Loss: 0.0065438249931156454\n",
      "Iteration: 662 Loss: 0.006514389625950727\n",
      "Iteration: 663 Loss: 0.006485141777563447\n",
      "Iteration: 664 Loss: 0.006456080164967055\n",
      "Iteration: 665 Loss: 0.00642720351406409\n",
      "Iteration: 666 Loss: 0.006398510559586048\n",
      "Iteration: 667 Loss: 0.006370000045030195\n",
      "Iteration: 668 Loss: 0.006341670722600671\n",
      "Iteration: 669 Loss: 0.006313521353146722\n",
      "Iteration: 670 Loss: 0.006285550706102616\n",
      "Iteration: 671 Loss: 0.006257757559430239\n",
      "Iteration: 672 Loss: 0.00623014069955754\n",
      "Iteration: 673 Loss: 0.006202698921320727\n",
      "Iteration: 674 Loss: 0.006175431027906489\n",
      "Iteration: 675 Loss: 0.006148335830792557\n",
      "Iteration: 676 Loss: 0.006121412149692355\n",
      "Iteration: 677 Loss: 0.006094658812495748\n",
      "Iteration: 678 Loss: 0.006068074655214291\n",
      "Iteration: 679 Loss: 0.006041658521923237\n",
      "Iteration: 680 Loss: 0.006015409264706284\n",
      "Iteration: 681 Loss: 0.0059893257436006465\n",
      "Iteration: 682 Loss: 0.005963406826541142\n",
      "Iteration: 683 Loss: 0.005937651389305177\n",
      "Iteration: 684 Loss: 0.005912058315459141\n",
      "Iteration: 685 Loss: 0.005886626496304161\n",
      "Iteration: 686 Loss: 0.005861354830822004\n",
      "Iteration: 687 Loss: 0.005836242225622745\n",
      "Iteration: 688 Loss: 0.005811287594890532\n",
      "Iteration: 689 Loss: 0.0057864898603325445\n",
      "Iteration: 690 Loss: 0.0057618479511253795\n",
      "Iteration: 691 Loss: 0.005737360803864911\n",
      "Iteration: 692 Loss: 0.005713027362512891\n",
      "Iteration: 693 Loss: 0.00568884657834751\n",
      "Iteration: 694 Loss: 0.005664817409912756\n",
      "Iteration: 695 Loss: 0.005640938822965748\n",
      "Iteration: 696 Loss: 0.005617209790429365\n",
      "Iteration: 697 Loss: 0.005593629292341782\n",
      "Iteration: 698 Loss: 0.005570196315806251\n",
      "Iteration: 699 Loss: 0.005546909854942695\n",
      "Iteration: 700 Loss: 0.005523768910839168\n",
      "Iteration: 701 Loss: 0.005500772491503795\n",
      "Iteration: 702 Loss: 0.005477919611816345\n",
      "Iteration: 703 Loss: 0.005455209293480853\n",
      "Iteration: 704 Loss: 0.005432640564977467\n",
      "Iteration: 705 Loss: 0.005410212461517239\n",
      "Iteration: 706 Loss: 0.005387924024993487\n",
      "Iteration: 707 Loss: 0.005365774303937373\n",
      "Iteration: 708 Loss: 0.005343762353469832\n",
      "Iteration: 709 Loss: 0.005321887235257406\n",
      "Iteration: 710 Loss: 0.005300148017466621\n",
      "Iteration: 711 Loss: 0.005278543774718756\n",
      "Iteration: 712 Loss: 0.005257073588044183\n",
      "Iteration: 713 Loss: 0.005235736544839937\n",
      "Iteration: 714 Loss: 0.005214531738823822\n",
      "Iteration: 715 Loss: 0.005193458269991489\n",
      "Iteration: 716 Loss: 0.005172515244572055\n",
      "Iteration: 717 Loss: 0.005151701774986175\n",
      "Iteration: 718 Loss: 0.0051310169798022215\n",
      "Iteration: 719 Loss: 0.005110459983694644\n",
      "Iteration: 720 Loss: 0.005090029917399084\n",
      "Iteration: 721 Loss: 0.005069725917673703\n",
      "Iteration: 722 Loss: 0.005049547127255934\n",
      "Iteration: 723 Loss: 0.005029492694819565\n",
      "Iteration: 724 Loss: 0.00500956177493627\n",
      "Iteration: 725 Loss: 0.0049897535280333544\n",
      "Iteration: 726 Loss: 0.004970067120352839\n",
      "Iteration: 727 Loss: 0.004950501723912077\n",
      "Iteration: 728 Loss: 0.004931056516463234\n",
      "Iteration: 729 Loss: 0.004911730681453245\n",
      "Iteration: 730 Loss: 0.004892523407984772\n",
      "Iteration: 731 Loss: 0.0048734338907772965\n",
      "Iteration: 732 Loss: 0.004854461330127566\n",
      "Iteration: 733 Loss: 0.0048356049318708325\n",
      "Iteration: 734 Loss: 0.004816863907342646\n",
      "Iteration: 735 Loss: 0.004798237473342313\n",
      "Iteration: 736 Loss: 0.004779724852092513\n",
      "Iteration: 737 Loss: 0.004761325271202481\n",
      "Iteration: 738 Loss: 0.004743037963631847\n",
      "Iteration: 739 Loss: 0.004724862167652302\n",
      "Iteration: 740 Loss: 0.004706797126811504\n",
      "Iteration: 741 Loss: 0.004688842089895986\n",
      "Iteration: 742 Loss: 0.0046709963108951934\n",
      "Iteration: 743 Loss: 0.004653259048964888\n",
      "Iteration: 744 Loss: 0.0046356295683927734\n",
      "Iteration: 745 Loss: 0.004618107138560947\n",
      "Iteration: 746 Loss: 0.004600691033911844\n",
      "Iteration: 747 Loss: 0.00458338053391411\n",
      "Iteration: 748 Loss: 0.00456617492302453\n",
      "Iteration: 749 Loss: 0.004549073490657073\n",
      "Iteration: 750 Loss: 0.0045320755311466674\n",
      "Iteration: 751 Loss: 0.004515180343715468\n",
      "Iteration: 752 Loss: 0.004498387232438459\n",
      "Iteration: 753 Loss: 0.004481695506210449\n",
      "Iteration: 754 Loss: 0.004465104478713078\n",
      "Iteration: 755 Loss: 0.004448613468380837\n",
      "Iteration: 756 Loss: 0.004432221798368219\n",
      "Iteration: 757 Loss: 0.004415928796518242\n",
      "Iteration: 758 Loss: 0.00439973379532748\n",
      "Iteration: 759 Loss: 0.004383636131916536\n",
      "Iteration: 760 Loss: 0.004367635147996\n",
      "Iteration: 761 Loss: 0.004351730189836214\n",
      "Iteration: 762 Loss: 0.00433592060823383\n",
      "Iteration: 763 Loss: 0.004320205758482855\n",
      "Iteration: 764 Loss: 0.004304585000340741\n",
      "Iteration: 765 Loss: 0.0042890576980002\n",
      "Iteration: 766 Loss: 0.004273623220056377\n",
      "Iteration: 767 Loss: 0.004258280939477208\n",
      "Iteration: 768 Loss: 0.004243030233573716\n",
      "Iteration: 769 Loss: 0.004227870483967893\n",
      "Iteration: 770 Loss: 0.004212801076565602\n",
      "Iteration: 771 Loss: 0.0041978214015255064\n",
      "Iteration: 772 Loss: 0.004182930853228136\n",
      "Iteration: 773 Loss: 0.004168128830250172\n",
      "Iteration: 774 Loss: 0.004153414735331981\n",
      "Iteration: 775 Loss: 0.0041387879753508995\n",
      "Iteration: 776 Loss: 0.004124247961291288\n",
      "Iteration: 777 Loss: 0.004109794108216937\n",
      "Iteration: 778 Loss: 0.0040954258352426135\n",
      "Iteration: 779 Loss: 0.004081142565505258\n",
      "Iteration: 780 Loss: 0.004066943726137851\n",
      "Iteration: 781 Loss: 0.004052828748240046\n",
      "Iteration: 782 Loss: 0.0040387970668504065\n",
      "Iteration: 783 Loss: 0.004024848120922364\n",
      "Iteration: 784 Loss: 0.004010981353292465\n",
      "Iteration: 785 Loss: 0.003997196210657086\n",
      "Iteration: 786 Loss: 0.003983492143544009\n",
      "Iteration: 787 Loss: 0.003969868606286299\n",
      "Iteration: 788 Loss: 0.003956325056995841\n",
      "Iteration: 789 Loss: 0.003942860957537842\n",
      "Iteration: 790 Loss: 0.003929475773503468\n",
      "Iteration: 791 Loss: 0.003916168974185516\n",
      "Iteration: 792 Loss: 0.0039029400325519253\n",
      "Iteration: 793 Loss: 0.003889788425220853\n",
      "Iteration: 794 Loss: 0.0038767136324346103\n",
      "Iteration: 795 Loss: 0.0038637151380351694\n",
      "Iteration: 796 Loss: 0.003850792429439046\n",
      "Iteration: 797 Loss: 0.0038379449976127106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 798 Loss: 0.0038251723370480366\n",
      "Iteration: 799 Loss: 0.003812473945737049\n",
      "Iteration: 800 Loss: 0.003799849325149945\n",
      "Iteration: 801 Loss: 0.003787297980208174\n",
      "Iteration: 802 Loss: 0.0037748194192627114\n",
      "Iteration: 803 Loss: 0.003762413154068536\n",
      "Iteration: 804 Loss: 0.0037500786997625483\n",
      "Iteration: 805 Loss: 0.0037378155748398513\n",
      "Iteration: 806 Loss: 0.003725623301129951\n",
      "Iteration: 807 Loss: 0.0037135014037738786\n",
      "Iteration: 808 Loss: 0.003701449411200895\n",
      "Iteration: 809 Loss: 0.0036894668551076338\n",
      "Iteration: 810 Loss: 0.003677553270432971\n",
      "Iteration: 811 Loss: 0.0036657081953365526\n",
      "Iteration: 812 Loss: 0.0036539311711773744\n",
      "Iteration: 813 Loss: 0.003642221742490675\n",
      "Iteration: 814 Loss: 0.0036305794569656256\n",
      "Iteration: 815 Loss: 0.0036190038654244417\n",
      "Iteration: 816 Loss: 0.003607494521800304\n",
      "Iteration: 817 Loss: 0.003596050983115872\n",
      "Iteration: 818 Loss: 0.003584672809461373\n",
      "Iteration: 819 Loss: 0.003573359563974572\n",
      "Iteration: 820 Loss: 0.003562110812818308\n",
      "Iteration: 821 Loss: 0.0035509261251599677\n",
      "Iteration: 822 Loss: 0.003539805073151796\n",
      "Iteration: 823 Loss: 0.0035287472319091135\n",
      "Iteration: 824 Loss: 0.0035177521794897103\n",
      "Iteration: 825 Loss: 0.0035068194968744747\n",
      "Iteration: 826 Loss: 0.003495948767945355\n",
      "Iteration: 827 Loss: 0.003485139579467781\n",
      "Iteration: 828 Loss: 0.003474391521067753\n",
      "Iteration: 829 Loss: 0.003463704185215854\n",
      "Iteration: 830 Loss: 0.0034530771672038507\n",
      "Iteration: 831 Loss: 0.00344251006512669\n",
      "Iteration: 832 Loss: 0.003432002479863955\n",
      "Iteration: 833 Loss: 0.0034215540150588615\n",
      "Iteration: 834 Loss: 0.003411164277100819\n",
      "Iteration: 835 Loss: 0.0034008328751049637\n",
      "Iteration: 836 Loss: 0.0033905594208945124\n",
      "Iteration: 837 Loss: 0.0033803435289809403\n",
      "Iteration: 838 Loss: 0.003370184816546773\n",
      "Iteration: 839 Loss: 0.0033600829034253345\n",
      "Iteration: 840 Loss: 0.003350037412083747\n",
      "Iteration: 841 Loss: 0.0033400479676044944\n",
      "Iteration: 842 Loss: 0.003330114197666628\n",
      "Iteration: 843 Loss: 0.0033202357325289966\n",
      "Iteration: 844 Loss: 0.0033104122050101816\n",
      "Iteration: 845 Loss: 0.00330064325047433\n",
      "Iteration: 846 Loss: 0.0032909285068100434\n",
      "Iteration: 847 Loss: 0.0032812676144146884\n",
      "Iteration: 848 Loss: 0.0032716602161767176\n",
      "Iteration: 849 Loss: 0.003262105957459099\n",
      "Iteration: 850 Loss: 0.0032526044860808206\n",
      "Iteration: 851 Loss: 0.0032431554523010167\n",
      "Iteration: 852 Loss: 0.003233758508801298\n",
      "Iteration: 853 Loss: 0.0032244133106692796\n",
      "Iteration: 854 Loss: 0.00321511951538263\n",
      "Iteration: 855 Loss: 0.00320587678279089\n",
      "Iteration: 856 Loss: 0.0031966847751010463\n",
      "Iteration: 857 Loss: 0.0031875431568600694\n",
      "Iteration: 858 Loss: 0.003178451594938555\n",
      "Iteration: 859 Loss: 0.0031694097585156394\n",
      "Iteration: 860 Loss: 0.003160417319062098\n",
      "Iteration: 861 Loss: 0.003151473950324539\n",
      "Iteration: 862 Loss: 0.0031425793283098285\n",
      "Iteration: 863 Loss: 0.0031337331312698367\n",
      "Iteration: 864 Loss: 0.0031249350396857683\n",
      "Iteration: 865 Loss: 0.0031161847362521705\n",
      "Iteration: 866 Loss: 0.0031074819058622416\n",
      "Iteration: 867 Loss: 0.0030988262355921173\n",
      "Iteration: 868 Loss: 0.003090217414685883\n",
      "Iteration: 869 Loss: 0.003081655134540959\n",
      "Iteration: 870 Loss: 0.0030731390886934965\n",
      "Iteration: 871 Loss: 0.003064668972802189\n",
      "Iteration: 872 Loss: 0.0030562444846352383\n",
      "Iteration: 873 Loss: 0.0030478653240536054\n",
      "Iteration: 874 Loss: 0.0030395311929993633\n",
      "Iteration: 875 Loss: 0.003031241795478477\n",
      "Iteration: 876 Loss: 0.003022996837548761\n",
      "Iteration: 877 Loss: 0.0030147960273045737\n",
      "Iteration: 878 Loss: 0.0030066390748628535\n",
      "Iteration: 879 Loss: 0.0029985256923487\n",
      "Iteration: 880 Loss: 0.002990455593882303\n",
      "Iteration: 881 Loss: 0.002982428495564626\n",
      "Iteration: 882 Loss: 0.00297444411546385\n",
      "Iteration: 883 Loss: 0.002966502173601859\n",
      "Iteration: 884 Loss: 0.002958602391940174\n",
      "Iteration: 885 Loss: 0.0029507444943674887\n",
      "Iteration: 886 Loss: 0.0029429282066851283\n",
      "Iteration: 887 Loss: 0.002935153256595\n",
      "Iteration: 888 Loss: 0.0029274193736852543\n",
      "Iteration: 889 Loss: 0.0029197262894185517\n",
      "Iteration: 890 Loss: 0.002912073737117688\n",
      "Iteration: 891 Loss: 0.0029044614519535173\n",
      "Iteration: 892 Loss: 0.002896889170932212\n",
      "Iteration: 893 Loss: 0.0028893566328817826\n",
      "Iteration: 894 Loss: 0.0028818635784407976\n",
      "Iteration: 895 Loss: 0.002874409750044796\n",
      "Iteration: 896 Loss: 0.0028669948919133608\n",
      "Iteration: 897 Loss: 0.0028596187500396397\n",
      "Iteration: 898 Loss: 0.0028522810721760515\n",
      "Iteration: 899 Loss: 0.0028449816078234035\n",
      "Iteration: 900 Loss: 0.002837720108217611\n",
      "Iteration: 901 Loss: 0.002830496326318818\n",
      "Iteration: 902 Loss: 0.0028233100167993725\n",
      "Iteration: 903 Loss: 0.002816160936030959\n",
      "Iteration: 904 Loss: 0.0028090488420731553\n",
      "Iteration: 905 Loss: 0.0028019734946627115\n",
      "Iteration: 906 Loss: 0.0027949346552007684\n",
      "Iteration: 907 Loss: 0.0027879320867422786\n",
      "Iteration: 908 Loss: 0.002780965553982919\n",
      "Iteration: 909 Loss: 0.002774034823249426\n",
      "Iteration: 910 Loss: 0.0027671396624876984\n",
      "Iteration: 911 Loss: 0.0027602798412514357\n",
      "Iteration: 912 Loss: 0.00275345513069054\n",
      "Iteration: 913 Loss: 0.002746665303540898\n",
      "Iteration: 914 Loss: 0.002739910134112265\n",
      "Iteration: 915 Loss: 0.002733189398279629\n",
      "Iteration: 916 Loss: 0.002726502873468918\n",
      "Iteration: 917 Loss: 0.002719850338649221\n",
      "Iteration: 918 Loss: 0.0027132315743200156\n",
      "Iteration: 919 Loss: 0.002706646362503047\n",
      "Iteration: 920 Loss: 0.0027000944867279068\n",
      "Iteration: 921 Loss: 0.002693575732025473\n",
      "Iteration: 922 Loss: 0.0026870898849145203\n",
      "Iteration: 923 Loss: 0.0026806367333936042\n",
      "Iteration: 924 Loss: 0.002674216066928855\n",
      "Iteration: 925 Loss: 0.0026678276764451644\n",
      "Iteration: 926 Loss: 0.0026614713543155638\n",
      "Iteration: 927 Loss: 0.0026551468943508513\n",
      "Iteration: 928 Loss: 0.0026488540917902013\n",
      "Iteration: 929 Loss: 0.0026425927432902003\n",
      "Iteration: 930 Loss: 0.002636362646916506\n",
      "Iteration: 931 Loss: 0.0026301636021320647\n",
      "Iteration: 932 Loss: 0.002623995409788598\n",
      "Iteration: 933 Loss: 0.002617857872117477\n",
      "Iteration: 934 Loss: 0.0026117507927192426\n",
      "Iteration: 935 Loss: 0.0026056739765545397\n",
      "Iteration: 936 Loss: 0.002599627229933028\n",
      "Iteration: 937 Loss: 0.0025936103605058514\n",
      "Iteration: 938 Loss: 0.002587623177255854\n",
      "Iteration: 939 Loss: 0.002581665490488288\n",
      "Iteration: 940 Loss: 0.0025757371118205084\n",
      "Iteration: 941 Loss: 0.002569837854174237\n",
      "Iteration: 942 Loss: 0.002563967531765155\n",
      "Iteration: 943 Loss: 0.002558125960095579\n",
      "Iteration: 944 Loss: 0.0025523129559433936\n",
      "Iteration: 945 Loss: 0.002546528337355175\n",
      "Iteration: 946 Loss: 0.002540771923635481\n",
      "Iteration: 947 Loss: 0.0025350435353388105\n",
      "Iteration: 948 Loss: 0.002529342994261449\n",
      "Iteration: 949 Loss: 0.002523670123432303\n",
      "Iteration: 950 Loss: 0.002518024747102649\n",
      "Iteration: 951 Loss: 0.0025124066907402653\n",
      "Iteration: 952 Loss: 0.0025068157810202328\n",
      "Iteration: 953 Loss: 0.002501251845814176\n",
      "Iteration: 954 Loss: 0.002495714714184675\n",
      "Iteration: 955 Loss: 0.0024902042163760523\n",
      "Iteration: 956 Loss: 0.0024847201838049033\n",
      "Iteration: 957 Loss: 0.0024792624490532662\n",
      "Iteration: 958 Loss: 0.002473830845860187\n",
      "Iteration: 959 Loss: 0.002468425209112305\n",
      "Iteration: 960 Loss: 0.0024630453748372833\n",
      "Iteration: 961 Loss: 0.0024576911801955136\n",
      "Iteration: 962 Loss: 0.0024523624634713616\n",
      "Iteration: 963 Loss: 0.0024470590640658214\n",
      "Iteration: 964 Loss: 0.0024417808224887713\n",
      "Iteration: 965 Loss: 0.00243652758035044\n",
      "Iteration: 966 Loss: 0.0024312991803540083\n",
      "Iteration: 967 Loss: 0.0024260954662883023\n",
      "Iteration: 968 Loss: 0.0024209162830190452\n",
      "Iteration: 969 Loss: 0.0024157614764830303\n",
      "Iteration: 970 Loss: 0.002410630893679001\n",
      "Iteration: 971 Loss: 0.002405524382660002\n",
      "Iteration: 972 Loss: 0.002400441792526919\n",
      "Iteration: 973 Loss: 0.0023953829734202335\n",
      "Iteration: 974 Loss: 0.0023903477765130745\n",
      "Iteration: 975 Loss: 0.00238533605400409\n",
      "Iteration: 976 Loss: 0.0023803476591088978\n",
      "Iteration: 977 Loss: 0.0023753824460537648\n",
      "Iteration: 978 Loss: 0.002370440270070126\n",
      "Iteration: 979 Loss: 0.002365520987383395\n",
      "Iteration: 980 Loss: 0.002360624455209405\n",
      "Iteration: 981 Loss: 0.002355750531745757\n",
      "Iteration: 982 Loss: 0.0023508990761649294\n",
      "Iteration: 983 Loss: 0.002346069948607495\n",
      "Iteration: 984 Loss: 0.002341263010175565\n",
      "Iteration: 985 Loss: 0.0023364781229256977\n",
      "Iteration: 986 Loss: 0.002331715149860913\n",
      "Iteration: 987 Loss: 0.002326973954925376\n",
      "Iteration: 988 Loss: 0.002322254402996426\n",
      "Iteration: 989 Loss: 0.002317556359880226\n",
      "Iteration: 990 Loss: 0.0023128796923009647\n",
      "Iteration: 991 Loss: 0.002308224267898485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 992 Loss: 0.002303589955219281\n",
      "Iteration: 993 Loss: 0.002298976623710548\n",
      "Iteration: 994 Loss: 0.002294384143713302\n",
      "Iteration: 995 Loss: 0.002289812386456542\n",
      "Iteration: 996 Loss: 0.002285261224050341\n",
      "Iteration: 997 Loss: 0.0022807305294798553\n",
      "Iteration: 998 Loss: 0.0022762201765987324\n",
      "Iteration: 999 Loss: 0.002271730040122382\n",
      "Iteration: 1000 Loss: 0.0022672599956225035\n",
      "Iteration: 1001 Loss: 0.002262809919520407\n",
      "Iteration: 1002 Loss: 0.0022583796890805557\n",
      "Iteration: 1003 Loss: 0.0022539691824059647\n",
      "Iteration: 1004 Loss: 0.002249578278428926\n",
      "Iteration: 1005 Loss: 0.0022452068569083273\n",
      "Iteration: 1006 Loss: 0.002240854798422072\n",
      "Iteration: 1007 Loss: 0.002236521984360409\n",
      "Iteration: 1008 Loss: 0.0022322082969213675\n",
      "Iteration: 1009 Loss: 0.0022279136191039202\n",
      "Iteration: 1010 Loss: 0.0022236378347025476\n",
      "Iteration: 1011 Loss: 0.002219380828299921\n",
      "Iteration: 1012 Loss: 0.0022151424852646716\n",
      "Iteration: 1013 Loss: 0.0022109226917408978\n",
      "Iteration: 1014 Loss: 0.0022067213346457287\n",
      "Iteration: 1015 Loss: 0.0022025383016636693\n",
      "Iteration: 1016 Loss: 0.0021983734812380403\n",
      "Iteration: 1017 Loss: 0.0021942267625688966\n",
      "Iteration: 1018 Loss: 0.0021900980356040455\n",
      "Iteration: 1019 Loss: 0.002185987191036157\n",
      "Iteration: 1020 Loss: 0.0021818941202958178\n",
      "Iteration: 1021 Loss: 0.002177818715546367\n",
      "Iteration: 1022 Loss: 0.002173760869678826\n",
      "Iteration: 1023 Loss: 0.002169720476305698\n",
      "Iteration: 1024 Loss: 0.0021656974297562565\n",
      "Iteration: 1025 Loss: 0.002161691625070349\n",
      "Iteration: 1026 Loss: 0.002157702957993854\n",
      "Iteration: 1027 Loss: 0.0021537313249728853\n",
      "Iteration: 1028 Loss: 0.002149776623149114\n",
      "Iteration: 1029 Loss: 0.002145838750353438\n",
      "Iteration: 1030 Loss: 0.002141917605101635\n",
      "Iteration: 1031 Loss: 0.0021380130865891905\n",
      "Iteration: 1032 Loss: 0.0021341250946853114\n",
      "Iteration: 1033 Loss: 0.0021302535299294397\n",
      "Iteration: 1034 Loss: 0.002126398293524203\n",
      "Iteration: 1035 Loss: 0.002122559287331673\n",
      "Iteration: 1036 Loss: 0.002118736413868296\n",
      "Iteration: 1037 Loss: 0.00211492957629853\n",
      "Iteration: 1038 Loss: 0.0021111386784318583\n",
      "Iteration: 1039 Loss: 0.002107363624716298\n",
      "Iteration: 1040 Loss: 0.0021036043202343874\n",
      "Iteration: 1041 Loss: 0.0020998606706977797\n",
      "Iteration: 1042 Loss: 0.002096132582443213\n",
      "Iteration: 1043 Loss: 0.0020924199624266994\n",
      "Iteration: 1044 Loss: 0.0020887227182190293\n",
      "Iteration: 1045 Loss: 0.002085040758001113\n",
      "Iteration: 1046 Loss: 0.002081373990559746\n",
      "Iteration: 1047 Loss: 0.002077722325281895\n",
      "Iteration: 1048 Loss: 0.002074085672151307\n",
      "Iteration: 1049 Loss: 0.002070463941742206\n",
      "Iteration: 1050 Loss: 0.002066857045216732\n",
      "Iteration: 1051 Loss: 0.0020632648943185125\n",
      "Iteration: 1052 Loss: 0.0020596874013695888\n",
      "Iteration: 1053 Loss: 0.0020561244792642688\n",
      "Iteration: 1054 Loss: 0.0020525760414665554\n",
      "Iteration: 1055 Loss: 0.00204904200200392\n",
      "Iteration: 1056 Loss: 0.0020455222754641386\n",
      "Iteration: 1057 Loss: 0.002042016776989844\n",
      "Iteration: 1058 Loss: 0.0020385254222752237\n",
      "Iteration: 1059 Loss: 0.0020350481275608406\n",
      "Iteration: 1060 Loss: 0.0020315848096296965\n",
      "Iteration: 1061 Loss: 0.0020281353858037747\n",
      "Iteration: 1062 Loss: 0.002024699773936879\n",
      "Iteration: 1063 Loss: 0.0020212778924147273\n",
      "Iteration: 1064 Loss: 0.0020178696601463896\n",
      "Iteration: 1065 Loss: 0.002014474996563096\n",
      "Iteration: 1066 Loss: 0.0020110938216135674\n",
      "Iteration: 1067 Loss: 0.0020077260557587744\n",
      "Iteration: 1068 Loss: 0.002004371619968567\n",
      "Iteration: 1069 Loss: 0.0020010304357182445\n",
      "Iteration: 1070 Loss: 0.0019977024249829564\n",
      "Iteration: 1071 Loss: 0.0019943875102348568\n",
      "Iteration: 1072 Loss: 0.0019910856144394018\n",
      "Iteration: 1073 Loss: 0.001987796661050613\n",
      "Iteration: 1074 Loss: 0.0019845205740071717\n",
      "Iteration: 1075 Loss: 0.0019812572777283695\n",
      "Iteration: 1076 Loss: 0.0019780066971126936\n",
      "Iteration: 1077 Loss: 0.0019747687575289246\n",
      "Iteration: 1078 Loss: 0.0019715433848178517\n",
      "Iteration: 1079 Loss: 0.0019683305052841517\n",
      "Iteration: 1080 Loss: 0.001965130045695242\n",
      "Iteration: 1081 Loss: 0.0019619419332770325\n",
      "Iteration: 1082 Loss: 0.001958766095709715\n",
      "Iteration: 1083 Loss: 0.0019556024611245165\n",
      "Iteration: 1084 Loss: 0.0019524509580992624\n",
      "Iteration: 1085 Loss: 0.0019493115156559524\n",
      "Iteration: 1086 Loss: 0.0019461840632563957\n",
      "Iteration: 1087 Loss: 0.001943068530798447\n",
      "Iteration: 1088 Loss: 0.0019399648486137675\n",
      "Iteration: 1089 Loss: 0.001936872947462627\n",
      "Iteration: 1090 Loss: 0.001933792758531363\n",
      "Iteration: 1091 Loss: 0.001930724213429043\n",
      "Iteration: 1092 Loss: 0.0019276672441838566\n",
      "Iteration: 1093 Loss: 0.0019246217832395024\n",
      "Iteration: 1094 Loss: 0.0019215877634518796\n",
      "Iteration: 1095 Loss: 0.0019185651180863418\n",
      "Iteration: 1096 Loss: 0.0019155537808128883\n",
      "Iteration: 1097 Loss: 0.0019125536857045825\n",
      "Iteration: 1098 Loss: 0.0019095647672338091\n",
      "Iteration: 1099 Loss: 0.0019065869602685216\n",
      "Iteration: 1100 Loss: 0.0019036202000698792\n",
      "Iteration: 1101 Loss: 0.0019006644222878527\n",
      "Iteration: 1102 Loss: 0.0018977195629588444\n",
      "Iteration: 1103 Loss: 0.0018947855585029192\n",
      "Iteration: 1104 Loss: 0.0018918623457201\n",
      "Iteration: 1105 Loss: 0.0018889498617882467\n",
      "Iteration: 1106 Loss: 0.0018860480442583585\n",
      "Iteration: 1107 Loss: 0.0018831568310525254\n",
      "Iteration: 1108 Loss: 0.0018802761604619217\n",
      "Iteration: 1109 Loss: 0.0018774059711422152\n",
      "Iteration: 1110 Loss: 0.001874546202111947\n",
      "Iteration: 1111 Loss: 0.001871696792748185\n",
      "Iteration: 1112 Loss: 0.0018688576827847773\n",
      "Iteration: 1113 Loss: 0.0018660288123099522\n",
      "Iteration: 1114 Loss: 0.0018632101217621125\n",
      "Iteration: 1115 Loss: 0.0018604015519278602\n",
      "Iteration: 1116 Loss: 0.0018576030439396449\n",
      "Iteration: 1117 Loss: 0.0018548145392720257\n",
      "Iteration: 1118 Loss: 0.0018520359797396984\n",
      "Iteration: 1119 Loss: 0.0018492673074947635\n",
      "Iteration: 1120 Loss: 0.0018465084650237178\n",
      "Iteration: 1121 Loss: 0.0018437593951456633\n",
      "Iteration: 1122 Loss: 0.001841020041008106\n",
      "Iteration: 1123 Loss: 0.0018382903460870059\n",
      "Iteration: 1124 Loss: 0.0018355702541815568\n",
      "Iteration: 1125 Loss: 0.0018328597094130837\n",
      "Iteration: 1126 Loss: 0.0018301586562224598\n",
      "Iteration: 1127 Loss: 0.0018274670393672952\n",
      "Iteration: 1128 Loss: 0.0018247848039202941\n",
      "Iteration: 1129 Loss: 0.00182211189526589\n",
      "Iteration: 1130 Loss: 0.0018194482590984773\n",
      "Iteration: 1131 Loss: 0.0018167938414193592\n",
      "Iteration: 1132 Loss: 0.0018141485885364767\n",
      "Iteration: 1133 Loss: 0.0018115124470592069\n",
      "Iteration: 1134 Loss: 0.0018088853638983113\n",
      "Iteration: 1135 Loss: 0.0018062672862623871\n",
      "Iteration: 1136 Loss: 0.001803658161657423\n",
      "Iteration: 1137 Loss: 0.0018010579378824941\n",
      "Iteration: 1138 Loss: 0.0017984665630286962\n",
      "Iteration: 1139 Loss: 0.0017958839854771583\n",
      "Iteration: 1140 Loss: 0.0017933101538973458\n",
      "Iteration: 1141 Loss: 0.001790745017242877\n",
      "Iteration: 1142 Loss: 0.0017881885247527124\n",
      "Iteration: 1143 Loss: 0.0017856406259456744\n",
      "Iteration: 1144 Loss: 0.0017831012706216343\n",
      "Iteration: 1145 Loss: 0.001780570408857972\n",
      "Iteration: 1146 Loss: 0.0017780479910067877\n",
      "Iteration: 1147 Loss: 0.0017755339676949458\n",
      "Iteration: 1148 Loss: 0.001773028289821031\n",
      "Iteration: 1149 Loss: 0.0017705309085532296\n",
      "Iteration: 1150 Loss: 0.0017680417753291081\n",
      "Iteration: 1151 Loss: 0.001765560841852072\n",
      "Iteration: 1152 Loss: 0.0017630880600900483\n",
      "Iteration: 1153 Loss: 0.0017606233822740581\n",
      "Iteration: 1154 Loss: 0.0017581667608964162\n",
      "Iteration: 1155 Loss: 0.001755718148708901\n",
      "Iteration: 1156 Loss: 0.0017532774987215687\n",
      "Iteration: 1157 Loss: 0.0017508447641995857\n",
      "Iteration: 1158 Loss: 0.0017484198986632581\n",
      "Iteration: 1159 Loss: 0.0017460028558857364\n",
      "Iteration: 1160 Loss: 0.0017435935898922354\n",
      "Iteration: 1161 Loss: 0.0017411920549566936\n",
      "Iteration: 1162 Loss: 0.0017387982056019486\n",
      "Iteration: 1163 Loss: 0.0017364119965971624\n",
      "Iteration: 1164 Loss: 0.0017340333829574805\n",
      "Iteration: 1165 Loss: 0.001731662319940863\n",
      "Iteration: 1166 Loss: 0.0017292987630483248\n",
      "Iteration: 1167 Loss: 0.0017269426680216128\n",
      "Iteration: 1168 Loss: 0.0017245939908416084\n",
      "Iteration: 1169 Loss: 0.0017222526877278292\n",
      "Iteration: 1170 Loss: 0.0017199187151359346\n",
      "Iteration: 1171 Loss: 0.0017175920297578444\n",
      "Iteration: 1172 Loss: 0.0017152725885190968\n",
      "Iteration: 1173 Loss: 0.0017129603485781743\n",
      "Iteration: 1174 Loss: 0.0017106552673250603\n",
      "Iteration: 1175 Loss: 0.0017083573023800682\n",
      "Iteration: 1176 Loss: 0.0017060664115921892\n",
      "Iteration: 1177 Loss: 0.0017037825530391102\n",
      "Iteration: 1178 Loss: 0.0017015056850246293\n",
      "Iteration: 1179 Loss: 0.0016992357660777602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1180 Loss: 0.0016969727549524056\n",
      "Iteration: 1181 Loss: 0.0016947166106259304\n",
      "Iteration: 1182 Loss: 0.0016924672922969406\n",
      "Iteration: 1183 Loss: 0.0016902247593854258\n",
      "Iteration: 1184 Loss: 0.001687988971531217\n",
      "Iteration: 1185 Loss: 0.0016857598885930464\n",
      "Iteration: 1186 Loss: 0.0016835374706469781\n",
      "Iteration: 1187 Loss: 0.0016813216779866975\n",
      "Iteration: 1188 Loss: 0.0016791124711207281\n",
      "Iteration: 1189 Loss: 0.001676909810773432\n",
      "Iteration: 1190 Loss: 0.0016747136578819397\n",
      "Iteration: 1191 Loss: 0.0016725239735965773\n",
      "Iteration: 1192 Loss: 0.0016703407192799446\n",
      "Iteration: 1193 Loss: 0.0016681638565045312\n",
      "Iteration: 1194 Loss: 0.0016659933470540053\n",
      "Iteration: 1195 Loss: 0.001663829152920317\n",
      "Iteration: 1196 Loss: 0.0016616712363041399\n",
      "Iteration: 1197 Loss: 0.0016595195596137125\n",
      "Iteration: 1198 Loss: 0.0016573740854632342\n",
      "Iteration: 1199 Loss: 0.001655234776672939\n",
      "Iteration: 1200 Loss: 0.0016531015962681932\n",
      "Iteration: 1201 Loss: 0.00165097450747724\n",
      "Iteration: 1202 Loss: 0.0016488534737331218\n",
      "Iteration: 1203 Loss: 0.0016467384586704653\n",
      "Iteration: 1204 Loss: 0.001644629426125132\n",
      "Iteration: 1205 Loss: 0.0016425263401352614\n",
      "Iteration: 1206 Loss: 0.0016404291649377992\n",
      "Iteration: 1207 Loss: 0.0016383378649699372\n",
      "Iteration: 1208 Loss: 0.0016362524048669559\n",
      "Iteration: 1209 Loss: 0.0016341727494625837\n",
      "Iteration: 1210 Loss: 0.0016320988637880804\n",
      "Iteration: 1211 Loss: 0.0016300307130702656\n",
      "Iteration: 1212 Loss: 0.0016279682627329643\n",
      "Iteration: 1213 Loss: 0.0016259114783949355\n",
      "Iteration: 1214 Loss: 0.001623860325868845\n",
      "Iteration: 1215 Loss: 0.0016218147711622713\n",
      "Iteration: 1216 Loss: 0.0016197747804749677\n",
      "Iteration: 1217 Loss: 0.0016177403202001308\n",
      "Iteration: 1218 Loss: 0.001615711356922677\n",
      "Iteration: 1219 Loss: 0.0016136878574186813\n",
      "Iteration: 1220 Loss: 0.0016116697886552577\n",
      "Iteration: 1221 Loss: 0.001609657117789344\n",
      "Iteration: 1222 Loss: 0.0016076498121675945\n",
      "Iteration: 1223 Loss: 0.001605647839326363\n",
      "Iteration: 1224 Loss: 0.001603651166989191\n",
      "Iteration: 1225 Loss: 0.0016016597630682011\n",
      "Iteration: 1226 Loss: 0.0015996735956623363\n",
      "Iteration: 1227 Loss: 0.0015976926330573892\n",
      "Iteration: 1228 Loss: 0.0015957168437253943\n",
      "Iteration: 1229 Loss: 0.0015937461963233016\n",
      "Iteration: 1230 Loss: 0.0015917806596937674\n",
      "Iteration: 1231 Loss: 0.0015898202028640836\n",
      "Iteration: 1232 Loss: 0.0015878647950443487\n",
      "Iteration: 1233 Loss: 0.001585914405629559\n",
      "Iteration: 1234 Loss: 0.0015839690041964123\n",
      "Iteration: 1235 Loss: 0.0015820285605043654\n",
      "Iteration: 1236 Loss: 0.0015800930444950416\n",
      "Iteration: 1237 Loss: 0.0015781624262906383\n",
      "Iteration: 1238 Loss: 0.0015762366761943947\n",
      "Iteration: 1239 Loss: 0.0015743157646901237\n",
      "Iteration: 1240 Loss: 0.0015723996624415282\n",
      "Iteration: 1241 Loss: 0.0015704883402911662\n",
      "Iteration: 1242 Loss: 0.0015685817692605566\n",
      "Iteration: 1243 Loss: 0.0015666799205494022\n",
      "Iteration: 1244 Loss: 0.001564782765534849\n",
      "Iteration: 1245 Loss: 0.0015628902757721743\n",
      "Iteration: 1246 Loss: 0.001561002422992846\n",
      "Iteration: 1247 Loss: 0.0015591191791042224\n",
      "Iteration: 1248 Loss: 0.0015572405161899425\n",
      "Iteration: 1249 Loss: 0.0015553664065093345\n",
      "Iteration: 1250 Loss: 0.0015534968224958318\n",
      "Iteration: 1251 Loss: 0.0015516317367574198\n",
      "Iteration: 1252 Loss: 0.0015497711220760035\n",
      "Iteration: 1253 Loss: 0.0015479149514069342\n",
      "Iteration: 1254 Loss: 0.0015460631978779667\n",
      "Iteration: 1255 Loss: 0.0015442158347893228\n",
      "Iteration: 1256 Loss: 0.0015423728356133645\n",
      "Iteration: 1257 Loss: 0.0015405341739937785\n",
      "Iteration: 1258 Loss: 0.0015386998237445022\n",
      "Iteration: 1259 Loss: 0.001536869758850472\n",
      "Iteration: 1260 Loss: 0.0015350439534664727\n",
      "Iteration: 1261 Loss: 0.0015332223819160557\n",
      "Iteration: 1262 Loss: 0.0015314050186919586\n",
      "Iteration: 1263 Loss: 0.0015295918384559963\n",
      "Iteration: 1264 Loss: 0.0015277828160373454\n",
      "Iteration: 1265 Loss: 0.001525977926431683\n",
      "Iteration: 1266 Loss: 0.0015241771448028928\n",
      "Iteration: 1267 Loss: 0.001522380446479889\n",
      "Iteration: 1268 Loss: 0.0015205878069590518\n",
      "Iteration: 1269 Loss: 0.0015187992019006671\n",
      "Iteration: 1270 Loss: 0.001517014607130171\n",
      "Iteration: 1271 Loss: 0.001515233998637706\n",
      "Iteration: 1272 Loss: 0.0015134573525769051\n",
      "Iteration: 1273 Loss: 0.0015116846452647116\n",
      "Iteration: 1274 Loss: 0.0015099158531804997\n",
      "Iteration: 1275 Loss: 0.0015081509529656259\n",
      "Iteration: 1276 Loss: 0.0015063899214239039\n",
      "Iteration: 1277 Loss: 0.001504632735519276\n",
      "Iteration: 1278 Loss: 0.0015028793723769437\n",
      "Iteration: 1279 Loss: 0.001501129809281986\n",
      "Iteration: 1280 Loss: 0.001499384023678905\n",
      "Iteration: 1281 Loss: 0.0014976419931706083\n",
      "Iteration: 1282 Loss: 0.0014959036955185764\n",
      "Iteration: 1283 Loss: 0.001494169108642673\n",
      "Iteration: 1284 Loss: 0.0014924382106190181\n",
      "Iteration: 1285 Loss: 0.0014907109796812581\n",
      "Iteration: 1286 Loss: 0.0014889873942193588\n",
      "Iteration: 1287 Loss: 0.0014872674327775582\n",
      "Iteration: 1288 Loss: 0.0014855510740554969\n",
      "Iteration: 1289 Loss: 0.0014838382969067684\n",
      "Iteration: 1290 Loss: 0.0014821290803399777\n",
      "Iteration: 1291 Loss: 0.001480423403515229\n",
      "Iteration: 1292 Loss: 0.0014787212457466007\n",
      "Iteration: 1293 Loss: 0.00147702258649902\n",
      "Iteration: 1294 Loss: 0.0014753274053900594\n",
      "Iteration: 1295 Loss: 0.0014736356821860265\n",
      "Iteration: 1296 Loss: 0.001471947396805204\n",
      "Iteration: 1297 Loss: 0.0014702625293143803\n",
      "Iteration: 1298 Loss: 0.0014685810599295419\n",
      "Iteration: 1299 Loss: 0.0014669029690145031\n",
      "Iteration: 1300 Loss: 0.0014652282370816775\n",
      "Iteration: 1301 Loss: 0.0014635568447889978\n",
      "Iteration: 1302 Loss: 0.0014618887729416794\n",
      "Iteration: 1303 Loss: 0.0014602240024906225\n",
      "Iteration: 1304 Loss: 0.0014585625145309125\n",
      "Iteration: 1305 Loss: 0.0014569042903031233\n",
      "Iteration: 1306 Loss: 0.001455249311190429\n",
      "Iteration: 1307 Loss: 0.0014535975587200646\n",
      "Iteration: 1308 Loss: 0.0014519490145615814\n",
      "Iteration: 1309 Loss: 0.0014503036605255277\n",
      "Iteration: 1310 Loss: 0.0014486614785637745\n",
      "Iteration: 1311 Loss: 0.0014470224507688726\n",
      "Iteration: 1312 Loss: 0.0014453865593728548\n",
      "Iteration: 1313 Loss: 0.0014437537867464257\n",
      "Iteration: 1314 Loss: 0.0014421241153990511\n",
      "Iteration: 1315 Loss: 0.001440497527978032\n",
      "Iteration: 1316 Loss: 0.0014388740072667431\n",
      "Iteration: 1317 Loss: 0.0014372535361856958\n",
      "Iteration: 1318 Loss: 0.0014356360977901828\n",
      "Iteration: 1319 Loss: 0.0014340216752708242\n",
      "Iteration: 1320 Loss: 0.0014324102519524594\n",
      "Iteration: 1321 Loss: 0.0014308018112926307\n",
      "Iteration: 1322 Loss: 0.0014291963368823519\n",
      "Iteration: 1323 Loss: 0.0014275938124437661\n",
      "Iteration: 1324 Loss: 0.001425994221830809\n",
      "Iteration: 1325 Loss: 0.0014243975490281792\n",
      "Iteration: 1326 Loss: 0.0014228037781497727\n",
      "Iteration: 1327 Loss: 0.0014212128934384673\n",
      "Iteration: 1328 Loss: 0.001419624879266028\n",
      "Iteration: 1329 Loss: 0.0014180397201312659\n",
      "Iteration: 1330 Loss: 0.0014164574006608017\n",
      "Iteration: 1331 Loss: 0.0014148779056059287\n",
      "Iteration: 1332 Loss: 0.0014133012198450283\n",
      "Iteration: 1333 Loss: 0.0014117273283791485\n",
      "Iteration: 1334 Loss: 0.001410156216334812\n",
      "Iteration: 1335 Loss: 0.0014085878689611563\n",
      "Iteration: 1336 Loss: 0.0014070222716293624\n",
      "Iteration: 1337 Loss: 0.0014054594098325953\n",
      "Iteration: 1338 Loss: 0.001403899269184653\n",
      "Iteration: 1339 Loss: 0.001402341835419501\n",
      "Iteration: 1340 Loss: 0.0014007870943910138\n",
      "Iteration: 1341 Loss: 0.0013992350320709609\n",
      "Iteration: 1342 Loss: 0.0013976856345492956\n",
      "Iteration: 1343 Loss: 0.0013961388880335212\n",
      "Iteration: 1344 Loss: 0.0013945947788462766\n",
      "Iteration: 1345 Loss: 0.0013930532934266111\n",
      "Iteration: 1346 Loss: 0.001391514418328613\n",
      "Iteration: 1347 Loss: 0.0013899781402191857\n",
      "Iteration: 1348 Loss: 0.001388444445879544\n",
      "Iteration: 1349 Loss: 0.001386913322203216\n",
      "Iteration: 1350 Loss: 0.0013853847561959597\n",
      "Iteration: 1351 Loss: 0.0013838587349735208\n",
      "Iteration: 1352 Loss: 0.0013823352457625869\n",
      "Iteration: 1353 Loss: 0.0013808142758988983\n",
      "Iteration: 1354 Loss: 0.0013792958128267024\n",
      "Iteration: 1355 Loss: 0.0013777798440988579\n",
      "Iteration: 1356 Loss: 0.0013762663573747371\n",
      "Iteration: 1357 Loss: 0.0013747553404209515\n",
      "Iteration: 1358 Loss: 0.0013732467811084566\n",
      "Iteration: 1359 Loss: 0.001371740667413689\n",
      "Iteration: 1360 Loss: 0.0013702369874178681\n",
      "Iteration: 1361 Loss: 0.001368735729304168\n",
      "Iteration: 1362 Loss: 0.0013672368813598336\n",
      "Iteration: 1363 Loss: 0.001365740431972707\n",
      "Iteration: 1364 Loss: 0.0013642463696319307\n",
      "Iteration: 1365 Loss: 0.001362754682926919\n",
      "Iteration: 1366 Loss: 0.0013612653605471182\n",
      "Iteration: 1367 Loss: 0.0013597783912803522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1368 Loss: 0.001358293764013017\n",
      "Iteration: 1369 Loss: 0.0013568114677273721\n",
      "Iteration: 1370 Loss: 0.001355331491503334\n",
      "Iteration: 1371 Loss: 0.001353853824516387\n",
      "Iteration: 1372 Loss: 0.0013523784560371867\n",
      "Iteration: 1373 Loss: 0.0013509053754300056\n",
      "Iteration: 1374 Loss: 0.0013494345721530873\n",
      "Iteration: 1375 Loss: 0.0013479660357573948\n",
      "Iteration: 1376 Loss: 0.0013464997558862012\n",
      "Iteration: 1377 Loss: 0.0013450357222735201\n",
      "Iteration: 1378 Loss: 0.0013435739247442601\n",
      "Iteration: 1379 Loss: 0.0013421143532129534\n",
      "Iteration: 1380 Loss: 0.0013406569976831598\n",
      "Iteration: 1381 Loss: 0.0013392018482464498\n",
      "Iteration: 1382 Loss: 0.0013377488950834235\n",
      "Iteration: 1383 Loss: 0.001336298128459772\n",
      "Iteration: 1384 Loss: 0.0013348495387286985\n",
      "Iteration: 1385 Loss: 0.001333403116328298\n",
      "Iteration: 1386 Loss: 0.0013319588517812495\n",
      "Iteration: 1387 Loss: 0.0013305167356944397\n",
      "Iteration: 1388 Loss: 0.001329076758757759\n",
      "Iteration: 1389 Loss: 0.0013276389117442022\n",
      "Iteration: 1390 Loss: 0.0013262031855085486\n",
      "Iteration: 1391 Loss: 0.001324769570986141\n",
      "Iteration: 1392 Loss: 0.0013233380591934486\n",
      "Iteration: 1393 Loss: 0.0013219086412268452\n",
      "Iteration: 1394 Loss: 0.0013204813082615657\n",
      "Iteration: 1395 Loss: 0.0013190560515506094\n",
      "Iteration: 1396 Loss: 0.001317632862425571\n",
      "Iteration: 1397 Loss: 0.0013162117322942917\n",
      "Iteration: 1398 Loss: 0.0013147926526415627\n",
      "Iteration: 1399 Loss: 0.0013133756150277122\n",
      "Iteration: 1400 Loss: 0.0013119606110880756\n",
      "Iteration: 1401 Loss: 0.0013105476325318114\n",
      "Iteration: 1402 Loss: 0.0013091366711424411\n",
      "Iteration: 1403 Loss: 0.0013077277187760379\n",
      "Iteration: 1404 Loss: 0.0013063207673609466\n",
      "Iteration: 1405 Loss: 0.0013049158088972498\n",
      "Iteration: 1406 Loss: 0.0013035128354562597\n",
      "Iteration: 1407 Loss: 0.001302111839179772\n",
      "Iteration: 1408 Loss: 0.0013007128122786518\n",
      "Iteration: 1409 Loss: 0.0012993157470336024\n",
      "Iteration: 1410 Loss: 0.001297920635792424\n",
      "Iteration: 1411 Loss: 0.001296527470972567\n",
      "Iteration: 1412 Loss: 0.0012951362450570863\n",
      "Iteration: 1413 Loss: 0.0012937469505965148\n",
      "Iteration: 1414 Loss: 0.0012923595802074091\n",
      "Iteration: 1415 Loss: 0.001290974126570461\n",
      "Iteration: 1416 Loss: 0.001289590582432117\n",
      "Iteration: 1417 Loss: 0.00128820894060266\n",
      "Iteration: 1418 Loss: 0.0012868291939563954\n",
      "Iteration: 1419 Loss: 0.0012854513354289682\n",
      "Iteration: 1420 Loss: 0.001284075358019384\n",
      "Iteration: 1421 Loss: 0.0012827012547882467\n",
      "Iteration: 1422 Loss: 0.0012813290188572334\n",
      "Iteration: 1423 Loss: 0.0012799586434084544\n",
      "Iteration: 1424 Loss: 0.0012785901216830376\n",
      "Iteration: 1425 Loss: 0.0012772234469832478\n",
      "Iteration: 1426 Loss: 0.0012758586126688932\n",
      "Iteration: 1427 Loss: 0.0012744956121583943\n",
      "Iteration: 1428 Loss: 0.0012731344389272932\n",
      "Iteration: 1429 Loss: 0.0012717750865084453\n",
      "Iteration: 1430 Loss: 0.0012704175484911818\n",
      "Iteration: 1431 Loss: 0.0012690618185212984\n",
      "Iteration: 1432 Loss: 0.001267707890299552\n",
      "Iteration: 1433 Loss: 0.001266355757580802\n",
      "Iteration: 1434 Loss: 0.001265005414175777\n",
      "Iteration: 1435 Loss: 0.0012636568539476907\n",
      "Iteration: 1436 Loss: 0.0012623100708135041\n",
      "Iteration: 1437 Loss: 0.0012609650587431444\n",
      "Iteration: 1438 Loss: 0.0012596218117584916\n",
      "Iteration: 1439 Loss: 0.00125828032393262\n",
      "Iteration: 1440 Loss: 0.001256940589390393\n",
      "Iteration: 1441 Loss: 0.00125560260230722\n",
      "Iteration: 1442 Loss: 0.0012542663569079042\n",
      "Iteration: 1443 Loss: 0.0012529318474682292\n",
      "Iteration: 1444 Loss: 0.001251599068311947\n",
      "Iteration: 1445 Loss: 0.0012502680138112563\n",
      "Iteration: 1446 Loss: 0.0012489386783876117\n",
      "Iteration: 1447 Loss: 0.0012476110565093452\n",
      "Iteration: 1448 Loss: 0.0012462851426922558\n",
      "Iteration: 1449 Loss: 0.0012449609314988898\n",
      "Iteration: 1450 Loss: 0.0012436384175374994\n",
      "Iteration: 1451 Loss: 0.0012423175954626574\n",
      "Iteration: 1452 Loss: 0.0012409984599742188\n",
      "Iteration: 1453 Loss: 0.001239681005815848\n",
      "Iteration: 1454 Loss: 0.0012383652277765608\n",
      "Iteration: 1455 Loss: 0.0012370511206887698\n",
      "Iteration: 1456 Loss: 0.0012357386794290299\n",
      "Iteration: 1457 Loss: 0.0012344278989159917\n",
      "Iteration: 1458 Loss: 0.0012331187741109553\n",
      "Iteration: 1459 Loss: 0.001231811300018083\n",
      "Iteration: 1460 Loss: 0.0012305054716828744\n",
      "Iteration: 1461 Loss: 0.0012292012841914534\n",
      "Iteration: 1462 Loss: 0.0012278987326717538\n",
      "Iteration: 1463 Loss: 0.0012265978122911001\n",
      "Iteration: 1464 Loss: 0.001225298518257372\n",
      "Iteration: 1465 Loss: 0.0012240008458178735\n",
      "Iteration: 1466 Loss: 0.0012227047902590553\n",
      "Iteration: 1467 Loss: 0.0012214103469061115\n",
      "Iteration: 1468 Loss: 0.0012201175111226988\n",
      "Iteration: 1469 Loss: 0.001218826278310038\n",
      "Iteration: 1470 Loss: 0.0012175366439068306\n",
      "Iteration: 1471 Loss: 0.0012162486033896012\n",
      "Iteration: 1472 Loss: 0.0012149621522704465\n",
      "Iteration: 1473 Loss: 0.001213677286099493\n",
      "Iteration: 1474 Loss: 0.0012123940004614869\n",
      "Iteration: 1475 Loss: 0.0012111122909770568\n",
      "Iteration: 1476 Loss: 0.0012098321533021686\n",
      "Iteration: 1477 Loss: 0.0012085535831278449\n",
      "Iteration: 1478 Loss: 0.0012072765761793679\n",
      "Iteration: 1479 Loss: 0.0012060011282163047\n",
      "Iteration: 1480 Loss: 0.0012047272350319445\n",
      "Iteration: 1481 Loss: 0.0012034548924519568\n",
      "Iteration: 1482 Loss: 0.0012021840963368391\n",
      "Iteration: 1483 Loss: 0.0012009148425785245\n",
      "Iteration: 1484 Loss: 0.0011996471271020015\n",
      "Iteration: 1485 Loss: 0.0011983809458632954\n",
      "Iteration: 1486 Loss: 0.0011971162948512647\n",
      "Iteration: 1487 Loss: 0.0011958531700855032\n",
      "Iteration: 1488 Loss: 0.0011945915676166525\n",
      "Iteration: 1489 Loss: 0.0011933314835257853\n",
      "Iteration: 1490 Loss: 0.001192072913924572\n",
      "Iteration: 1491 Loss: 0.001190815854954545\n",
      "Iteration: 1492 Loss: 0.001189560302787757\n",
      "Iteration: 1493 Loss: 0.0011883062536240925\n",
      "Iteration: 1494 Loss: 0.0011870537036941188\n",
      "Iteration: 1495 Loss: 0.0011858026492560325\n",
      "Iteration: 1496 Loss: 0.0011845530865964408\n",
      "Iteration: 1497 Loss: 0.0011833050120307206\n",
      "Iteration: 1498 Loss: 0.001182058421901797\n",
      "Iteration: 1499 Loss: 0.001180813312580484\n",
      "Iteration: 1500 Loss: 0.0011795696804649028\n",
      "Iteration: 1501 Loss: 0.0011783275219789313\n",
      "Iteration: 1502 Loss: 0.0011770868335742202\n",
      "Iteration: 1503 Loss: 0.0011758476117285893\n",
      "Iteration: 1504 Loss: 0.0011746098529459605\n",
      "Iteration: 1505 Loss: 0.0011733735537558404\n",
      "Iteration: 1506 Loss: 0.0011721387107135142\n",
      "Iteration: 1507 Loss: 0.0011709053203990182\n",
      "Iteration: 1508 Loss: 0.0011696733794186655\n",
      "Iteration: 1509 Loss: 0.0011684428844026728\n",
      "Iteration: 1510 Loss: 0.0011672138320049925\n",
      "Iteration: 1511 Loss: 0.0011659862189052257\n",
      "Iteration: 1512 Loss: 0.001164760041806001\n",
      "Iteration: 1513 Loss: 0.001163535297434036\n",
      "Iteration: 1514 Loss: 0.0011623119825385705\n",
      "Iteration: 1515 Loss: 0.0011610900938935252\n",
      "Iteration: 1516 Loss: 0.0011598696282942619\n",
      "Iteration: 1517 Loss: 0.0011586505825600339\n",
      "Iteration: 1518 Loss: 0.0011574329535314577\n",
      "Iteration: 1519 Loss: 0.0011562167380727247\n",
      "Iteration: 1520 Loss: 0.0011550019330686932\n",
      "Iteration: 1521 Loss: 0.0011537885354262962\n",
      "Iteration: 1522 Loss: 0.0011525765420738311\n",
      "Iteration: 1523 Loss: 0.0011513659499613575\n",
      "Iteration: 1524 Loss: 0.0011501567560604994\n",
      "Iteration: 1525 Loss: 0.001148948957361838\n",
      "Iteration: 1526 Loss: 0.0011477425508783338\n",
      "Iteration: 1527 Loss: 0.0011465375336422328\n",
      "Iteration: 1528 Loss: 0.0011453339027060914\n",
      "Iteration: 1529 Loss: 0.0011441316551431035\n",
      "Iteration: 1530 Loss: 0.0011429307880454138\n",
      "Iteration: 1531 Loss: 0.001141731298524671\n",
      "Iteration: 1532 Loss: 0.0011405331837123333\n",
      "Iteration: 1533 Loss: 0.001139336440758601\n",
      "Iteration: 1534 Loss: 0.0011381410668327655\n",
      "Iteration: 1535 Loss: 0.0011369470591227906\n",
      "Iteration: 1536 Loss: 0.0011357544148347127\n",
      "Iteration: 1537 Loss: 0.001134563131193022\n",
      "Iteration: 1538 Loss: 0.001133373205440817\n",
      "Iteration: 1539 Loss: 0.0011321846348380752\n",
      "Iteration: 1540 Loss: 0.0011309974166635488\n",
      "Iteration: 1541 Loss: 0.001129811548212742\n",
      "Iteration: 1542 Loss: 0.001128627026798942\n",
      "Iteration: 1543 Loss: 0.0011274438497527106\n",
      "Iteration: 1544 Loss: 0.0011262620144211956\n",
      "Iteration: 1545 Loss: 0.001125081518168394\n",
      "Iteration: 1546 Loss: 0.001123902358374913\n",
      "Iteration: 1547 Loss: 0.0011227245324372532\n",
      "Iteration: 1548 Loss: 0.0011215480377693692\n",
      "Iteration: 1549 Loss: 0.0011203728718010143\n",
      "Iteration: 1550 Loss: 0.0011191990319763968\n",
      "Iteration: 1551 Loss: 0.0011180265157570135\n",
      "Iteration: 1552 Loss: 0.0011168553206189534\n",
      "Iteration: 1553 Loss: 0.0011156854440554159\n",
      "Iteration: 1554 Loss: 0.0011145168835726318\n",
      "Iteration: 1555 Loss: 0.0011133496366933459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1556 Loss: 0.0011121837009541744\n",
      "Iteration: 1557 Loss: 0.001111019073907713\n",
      "Iteration: 1558 Loss: 0.0011098557531201028\n",
      "Iteration: 1559 Loss: 0.0011086937361730108\n",
      "Iteration: 1560 Loss: 0.0011075330206613224\n",
      "Iteration: 1561 Loss: 0.0011063736041945764\n",
      "Iteration: 1562 Loss: 0.0011052154843963265\n",
      "Iteration: 1563 Loss: 0.001104058658903775\n",
      "Iteration: 1564 Loss: 0.0011029031253685846\n",
      "Iteration: 1565 Loss: 0.0011017488814548512\n",
      "Iteration: 1566 Loss: 0.0011005959248407413\n",
      "Iteration: 1567 Loss: 0.0010994442532175218\n",
      "Iteration: 1568 Loss: 0.001098293864289296\n",
      "Iteration: 1569 Loss: 0.001097144755773851\n",
      "Iteration: 1570 Loss: 0.0010959969254014752\n",
      "Iteration: 1571 Loss: 0.0010948503709156414\n",
      "Iteration: 1572 Loss: 0.0010937050900712904\n",
      "Iteration: 1573 Loss: 0.0010925610806371698\n",
      "Iteration: 1574 Loss: 0.0010914183403930178\n",
      "Iteration: 1575 Loss: 0.0010902768671315866\n",
      "Iteration: 1576 Loss: 0.0010891366586579432\n",
      "Iteration: 1577 Loss: 0.0010879977127889143\n",
      "Iteration: 1578 Loss: 0.0010868600273530751\n",
      "Iteration: 1579 Loss: 0.0010857236001902636\n",
      "Iteration: 1580 Loss: 0.0010845884291525061\n",
      "Iteration: 1581 Loss: 0.001083454512103235\n",
      "Iteration: 1582 Loss: 0.0010823218469172538\n",
      "Iteration: 1583 Loss: 0.0010811904314803609\n",
      "Iteration: 1584 Loss: 0.0010800602636890772\n",
      "Iteration: 1585 Loss: 0.0010789313414518275\n",
      "Iteration: 1586 Loss: 0.0010778036626869826\n",
      "Iteration: 1587 Loss: 0.0010766772253249285\n",
      "Iteration: 1588 Loss: 0.0010755520273055318\n",
      "Iteration: 1589 Loss: 0.0010744280665794987\n",
      "Iteration: 1590 Loss: 0.0010733053411085903\n",
      "Iteration: 1591 Loss: 0.0010721838488644544\n",
      "Iteration: 1592 Loss: 0.0010710635878290835\n",
      "Iteration: 1593 Loss: 0.0010699445559943929\n",
      "Iteration: 1594 Loss: 0.0010688267513626894\n",
      "Iteration: 1595 Loss: 0.0010677101719453537\n",
      "Iteration: 1596 Loss: 0.0010665948157648473\n",
      "Iteration: 1597 Loss: 0.0010654806808527968\n",
      "Iteration: 1598 Loss: 0.001064367765250261\n",
      "Iteration: 1599 Loss: 0.0010632560670077234\n",
      "Iteration: 1600 Loss: 0.0010621455841855363\n",
      "Iteration: 1601 Loss: 0.0010610363148533075\n",
      "Iteration: 1602 Loss: 0.0010599282570893664\n",
      "Iteration: 1603 Loss: 0.0010588214089823189\n",
      "Iteration: 1604 Loss: 0.0010577157686286116\n",
      "Iteration: 1605 Loss: 0.0010566113341343418\n",
      "Iteration: 1606 Loss: 0.0010555081036144878\n",
      "Iteration: 1607 Loss: 0.0010544060751927521\n",
      "Iteration: 1608 Loss: 0.0010533052470013868\n",
      "Iteration: 1609 Loss: 0.0010522056171810273\n",
      "Iteration: 1610 Loss: 0.0010511071838815323\n",
      "Iteration: 1611 Loss: 0.001050009945260185\n",
      "Iteration: 1612 Loss: 0.001048913899484356\n",
      "Iteration: 1613 Loss: 0.0010478190447282016\n",
      "Iteration: 1614 Loss: 0.0010467253791738758\n",
      "Iteration: 1615 Loss: 0.001045632901013091\n",
      "Iteration: 1616 Loss: 0.0010445416084443374\n",
      "Iteration: 1617 Loss: 0.0010434514996746993\n",
      "Iteration: 1618 Loss: 0.0010423625729186164\n",
      "Iteration: 1619 Loss: 0.0010412748263993429\n",
      "Iteration: 1620 Loss: 0.0010401882583470027\n",
      "Iteration: 1621 Loss: 0.001039102866999424\n",
      "Iteration: 1622 Loss: 0.0010380186506020453\n",
      "Iteration: 1623 Loss: 0.0010369356074079303\n",
      "Iteration: 1624 Loss: 0.001035853735678118\n",
      "Iteration: 1625 Loss: 0.0010347730336798381\n",
      "Iteration: 1626 Loss: 0.0010336934996883998\n",
      "Iteration: 1627 Loss: 0.0010326151319869938\n",
      "Iteration: 1628 Loss: 0.0010315379288639587\n",
      "Iteration: 1629 Loss: 0.0010304618886167567\n",
      "Iteration: 1630 Loss: 0.0010293870095485346\n",
      "Iteration: 1631 Loss: 0.001028313289969517\n",
      "Iteration: 1632 Loss: 0.001027240728198058\n",
      "Iteration: 1633 Loss: 0.0010261693225583504\n",
      "Iteration: 1634 Loss: 0.001025099071381197\n",
      "Iteration: 1635 Loss: 0.0010240299730042317\n",
      "Iteration: 1636 Loss: 0.0010229620257718951\n",
      "Iteration: 1637 Loss: 0.0010218952280348988\n",
      "Iteration: 1638 Loss: 0.0010208295781511025\n",
      "Iteration: 1639 Loss: 0.0010197650744834422\n",
      "Iteration: 1640 Loss: 0.0010187017154029618\n",
      "Iteration: 1641 Loss: 0.001017639499286037\n",
      "Iteration: 1642 Loss: 0.0010165784245157181\n",
      "Iteration: 1643 Loss: 0.0010155184894805912\n",
      "Iteration: 1644 Loss: 0.0010144596925758863\n",
      "Iteration: 1645 Loss: 0.0010134020322027752\n",
      "Iteration: 1646 Loss: 0.0010123455067689818\n",
      "Iteration: 1647 Loss: 0.00101129011468708\n",
      "Iteration: 1648 Loss: 0.0010102358543769307\n",
      "Iteration: 1649 Loss: 0.0010091827242627253\n",
      "Iteration: 1650 Loss: 0.0010081307227766247\n",
      "Iteration: 1651 Loss: 0.001007079848354006\n",
      "Iteration: 1652 Loss: 0.0010060300994378117\n",
      "Iteration: 1653 Loss: 0.0010049814744755422\n",
      "Iteration: 1654 Loss: 0.0010039339719211716\n",
      "Iteration: 1655 Loss: 0.0010028875902338196\n",
      "Iteration: 1656 Loss: 0.0010018423278782512\n",
      "Iteration: 1657 Loss: 0.0010007981833241178\n",
      "Iteration: 1658 Loss: 0.0009997551550471446\n",
      "Iteration: 1659 Loss: 0.0009987132415276673\n",
      "Iteration: 1660 Loss: 0.0009976724412523983\n",
      "Iteration: 1661 Loss: 0.000996632752712565\n",
      "Iteration: 1662 Loss: 0.000995594174404904\n",
      "Iteration: 1663 Loss: 0.0009945567048310002\n",
      "Iteration: 1664 Loss: 0.0009935203424981886\n",
      "Iteration: 1665 Loss: 0.0009924850859179078\n",
      "Iteration: 1666 Loss: 0.0009914509336071636\n",
      "Iteration: 1667 Loss: 0.0009904178840879436\n",
      "Iteration: 1668 Loss: 0.0009893859358874849\n",
      "Iteration: 1669 Loss: 0.0009883550875375835\n",
      "Iteration: 1670 Loss: 0.0009873253375750769\n",
      "Iteration: 1671 Loss: 0.0009862966845412715\n",
      "Iteration: 1672 Loss: 0.0009852691269826637\n",
      "Iteration: 1673 Loss: 0.0009842426634502353\n",
      "Iteration: 1674 Loss: 0.0009832172924994162\n",
      "Iteration: 1675 Loss: 0.0009821930126909806\n",
      "Iteration: 1676 Loss: 0.0009811698225895408\n",
      "Iteration: 1677 Loss: 0.0009801477207648287\n",
      "Iteration: 1678 Loss: 0.0009791267057906081\n",
      "Iteration: 1679 Loss: 0.000978106776245808\n",
      "Iteration: 1680 Loss: 0.0009770879307133965\n",
      "Iteration: 1681 Loss: 0.0009760701677805886\n",
      "Iteration: 1682 Loss: 0.0009750534860398139\n",
      "Iteration: 1683 Loss: 0.0009740378840868042\n",
      "Iteration: 1684 Loss: 0.0009730233605220816\n",
      "Iteration: 1685 Loss: 0.0009720099139502617\n",
      "Iteration: 1686 Loss: 0.0009709975429805861\n",
      "Iteration: 1687 Loss: 0.0009699862462262324\n",
      "Iteration: 1688 Loss: 0.0009689760223044344\n",
      "Iteration: 1689 Loss: 0.0009679668698364669\n",
      "Iteration: 1690 Loss: 0.0009669587874484678\n",
      "Iteration: 1691 Loss: 0.0009659517737695672\n",
      "Iteration: 1692 Loss: 0.0009649458274335588\n",
      "Iteration: 1693 Loss: 0.0009639409470785435\n",
      "Iteration: 1694 Loss: 0.0009629371313459074\n",
      "Iteration: 1695 Loss: 0.0009619343788807036\n",
      "Iteration: 1696 Loss: 0.0009609326883331469\n",
      "Iteration: 1697 Loss: 0.0009599320583563388\n",
      "Iteration: 1698 Loss: 0.0009589324876080108\n",
      "Iteration: 1699 Loss: 0.000957933974748633\n",
      "Iteration: 1700 Loss: 0.0009569365184430072\n",
      "Iteration: 1701 Loss: 0.0009559401173599059\n",
      "Iteration: 1702 Loss: 0.0009549447701711986\n",
      "Iteration: 1703 Loss: 0.0009539504755531142\n",
      "Iteration: 1704 Loss: 0.0009529572321849795\n",
      "Iteration: 1705 Loss: 0.000951965038750263\n",
      "Iteration: 1706 Loss: 0.0009509738939356246\n",
      "Iteration: 1707 Loss: 0.0009499837964319443\n",
      "Iteration: 1708 Loss: 0.0009489947449324599\n",
      "Iteration: 1709 Loss: 0.0009480067381355838\n",
      "Iteration: 1710 Loss: 0.0009470197747415222\n",
      "Iteration: 1711 Loss: 0.0009460338534553871\n",
      "Iteration: 1712 Loss: 0.0009450489729843672\n",
      "Iteration: 1713 Loss: 0.000944065132040148\n",
      "Iteration: 1714 Loss: 0.0009430823293371227\n",
      "Iteration: 1715 Loss: 0.0009421005635935715\n",
      "Iteration: 1716 Loss: 0.0009411198335307317\n",
      "Iteration: 1717 Loss: 0.0009401401378735968\n",
      "Iteration: 1718 Loss: 0.0009391614753505157\n",
      "Iteration: 1719 Loss: 0.0009381838446918121\n",
      "Iteration: 1720 Loss: 0.0009372072446324589\n",
      "Iteration: 1721 Loss: 0.0009362316739102235\n",
      "Iteration: 1722 Loss: 0.0009352571312660582\n",
      "Iteration: 1723 Loss: 0.000934283615443775\n",
      "Iteration: 1724 Loss: 0.000933311125190446\n",
      "Iteration: 1725 Loss: 0.0009323396592566746\n",
      "Iteration: 1726 Loss: 0.0009313692163953821\n",
      "Iteration: 1727 Loss: 0.0009303997953638456\n",
      "Iteration: 1728 Loss: 0.0009294313949212181\n",
      "Iteration: 1729 Loss: 0.0009284640138297956\n",
      "Iteration: 1730 Loss: 0.0009274976508555585\n",
      "Iteration: 1731 Loss: 0.0009265323047669701\n",
      "Iteration: 1732 Loss: 0.0009255679743353922\n",
      "Iteration: 1733 Loss: 0.0009246046583354364\n",
      "Iteration: 1734 Loss: 0.0009236423555446362\n",
      "Iteration: 1735 Loss: 0.0009226810647429535\n",
      "Iteration: 1736 Loss: 0.0009217207847141798\n",
      "Iteration: 1737 Loss: 0.0009207615142441221\n",
      "Iteration: 1738 Loss: 0.0009198032521216578\n",
      "Iteration: 1739 Loss: 0.0009188459971382697\n",
      "Iteration: 1740 Loss: 0.0009178897480890826\n",
      "Iteration: 1741 Loss: 0.0009169345037708838\n",
      "Iteration: 1742 Loss: 0.0009159802629839117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1743 Loss: 0.0009150270245312288\n",
      "Iteration: 1744 Loss: 0.0009140747872180261\n",
      "Iteration: 1745 Loss: 0.0009131235498526845\n",
      "Iteration: 1746 Loss: 0.0009121733112460427\n",
      "Iteration: 1747 Loss: 0.0009112240702115028\n",
      "Iteration: 1748 Loss: 0.0009102758255655026\n",
      "Iteration: 1749 Loss: 0.0009093285761277286\n",
      "Iteration: 1750 Loss: 0.0009083823207194063\n",
      "Iteration: 1751 Loss: 0.0009074370581644571\n",
      "Iteration: 1752 Loss: 0.0009064927872895655\n",
      "Iteration: 1753 Loss: 0.0009055495069244273\n",
      "Iteration: 1754 Loss: 0.0009046072159001271\n",
      "Iteration: 1755 Loss: 0.0009036659130519945\n",
      "Iteration: 1756 Loss: 0.0009027255972163768\n",
      "Iteration: 1757 Loss: 0.0009017862672325957\n",
      "Iteration: 1758 Loss: 0.0009008479219421732\n",
      "Iteration: 1759 Loss: 0.000899910560190534\n",
      "Iteration: 1760 Loss: 0.0008989741808234414\n",
      "Iteration: 1761 Loss: 0.000898038782690801\n",
      "Iteration: 1762 Loss: 0.0008971043646444167\n",
      "Iteration: 1763 Loss: 0.0008961709255384667\n",
      "Iteration: 1764 Loss: 0.0008952384642291884\n",
      "Iteration: 1765 Loss: 0.0008943069795754486\n",
      "Iteration: 1766 Loss: 0.0008933764704383238\n",
      "Iteration: 1767 Loss: 0.000892446935681772\n",
      "Iteration: 1768 Loss: 0.0008915183741711266\n",
      "Iteration: 1769 Loss: 0.0008905907847753252\n",
      "Iteration: 1770 Loss: 0.0008896641663643106\n",
      "Iteration: 1771 Loss: 0.0008887385178115665\n",
      "Iteration: 1772 Loss: 0.0008878138379920357\n",
      "Iteration: 1773 Loss: 0.0008868901257824616\n",
      "Iteration: 1774 Loss: 0.0008859673800629548\n",
      "Iteration: 1775 Loss: 0.0008850455997156467\n",
      "Iteration: 1776 Loss: 0.000884124783623934\n",
      "Iteration: 1777 Loss: 0.000883204930674397\n",
      "Iteration: 1778 Loss: 0.0008822860397554425\n",
      "Iteration: 1779 Loss: 0.0008813681097577772\n",
      "Iteration: 1780 Loss: 0.0008804511395750928\n",
      "Iteration: 1781 Loss: 0.0008795351281016296\n",
      "Iteration: 1782 Loss: 0.0008786200742348283\n",
      "Iteration: 1783 Loss: 0.0008777059768737256\n",
      "Iteration: 1784 Loss: 0.0008767928349196521\n",
      "Iteration: 1785 Loss: 0.0008758806472767206\n",
      "Iteration: 1786 Loss: 0.000874969412850103\n",
      "Iteration: 1787 Loss: 0.0008740591305475733\n",
      "Iteration: 1788 Loss: 0.0008731497992791193\n",
      "Iteration: 1789 Loss: 0.000872241417956188\n",
      "Iteration: 1790 Loss: 0.0008713339854933578\n",
      "Iteration: 1791 Loss: 0.0008704275008060832\n",
      "Iteration: 1792 Loss: 0.0008695219628125198\n",
      "Iteration: 1793 Loss: 0.0008686173704322819\n",
      "Iteration: 1794 Loss: 0.0008677137225879897\n",
      "Iteration: 1795 Loss: 0.0008668110182034732\n",
      "Iteration: 1796 Loss: 0.000865909256204614\n",
      "Iteration: 1797 Loss: 0.0008650084355191357\n",
      "Iteration: 1798 Loss: 0.0008641085550768436\n",
      "Iteration: 1799 Loss: 0.0008632096138101165\n",
      "Iteration: 1800 Loss: 0.0008623116106523052\n",
      "Iteration: 1801 Loss: 0.0008614145445390119\n",
      "Iteration: 1802 Loss: 0.0008605184144085838\n",
      "Iteration: 1803 Loss: 0.0008596232192003306\n",
      "Iteration: 1804 Loss: 0.0008587289578557915\n",
      "Iteration: 1805 Loss: 0.000857835629318594\n",
      "Iteration: 1806 Loss: 0.0008569432325335224\n",
      "Iteration: 1807 Loss: 0.000856051766447668\n",
      "Iteration: 1808 Loss: 0.000855161230010019\n",
      "Iteration: 1809 Loss: 0.000854271622171244\n",
      "Iteration: 1810 Loss: 0.0008533829418848193\n",
      "Iteration: 1811 Loss: 0.0008524951881046105\n",
      "Iteration: 1812 Loss: 0.000851608359787397\n",
      "Iteration: 1813 Loss: 0.0008507224558908661\n",
      "Iteration: 1814 Loss: 0.0008498374753756422\n",
      "Iteration: 1815 Loss: 0.0008489534172025813\n",
      "Iteration: 1816 Loss: 0.0008480702803360529\n",
      "Iteration: 1817 Loss: 0.0008471880637408198\n",
      "Iteration: 1818 Loss: 0.0008463067663842239\n",
      "Iteration: 1819 Loss: 0.0008454263872347637\n",
      "Iteration: 1820 Loss: 0.0008445469252635551\n",
      "Iteration: 1821 Loss: 0.0008436683794423263\n",
      "Iteration: 1822 Loss: 0.0008427907487460592\n",
      "Iteration: 1823 Loss: 0.0008419140321497252\n",
      "Iteration: 1824 Loss: 0.0008410382286315355\n",
      "Iteration: 1825 Loss: 0.0008401633371702006\n",
      "Iteration: 1826 Loss: 0.0008392893567472422\n",
      "Iteration: 1827 Loss: 0.0008384162863448565\n",
      "Iteration: 1828 Loss: 0.0008375441249475037\n",
      "Iteration: 1829 Loss: 0.0008366728715413695\n",
      "Iteration: 1830 Loss: 0.0008358025251137557\n",
      "Iteration: 1831 Loss: 0.000834933084654101\n",
      "Iteration: 1832 Loss: 0.000834064549153878\n",
      "Iteration: 1833 Loss: 0.0008331969176055531\n",
      "Iteration: 1834 Loss: 0.000832330189002994\n",
      "Iteration: 1835 Loss: 0.0008314643623421655\n",
      "Iteration: 1836 Loss: 0.0008305994366215043\n",
      "Iteration: 1837 Loss: 0.0008297354108396726\n",
      "Iteration: 1838 Loss: 0.0008288722839972305\n",
      "Iteration: 1839 Loss: 0.0008280100550969591\n",
      "Iteration: 1840 Loss: 0.000827148723142146\n",
      "Iteration: 1841 Loss: 0.0008262882871393945\n",
      "Iteration: 1842 Loss: 0.000825428746095331\n",
      "Iteration: 1843 Loss: 0.0008245700990188033\n",
      "Iteration: 1844 Loss: 0.0008237123449201182\n",
      "Iteration: 1845 Loss: 0.0008228554828109296\n",
      "Iteration: 1846 Loss: 0.0008219995117051361\n",
      "Iteration: 1847 Loss: 0.000821144430616994\n",
      "Iteration: 1848 Loss: 0.0008202902385637721\n",
      "Iteration: 1849 Loss: 0.0008194369345635611\n",
      "Iteration: 1850 Loss: 0.0008185845176353133\n",
      "Iteration: 1851 Loss: 0.0008177329868003364\n",
      "Iteration: 1852 Loss: 0.0008168823410818476\n",
      "Iteration: 1853 Loss: 0.0008160325795031847\n",
      "Iteration: 1854 Loss: 0.0008151837010905624\n",
      "Iteration: 1855 Loss: 0.000814335704871123\n",
      "Iteration: 1856 Loss: 0.0008134885898734109\n",
      "Iteration: 1857 Loss: 0.0008126423551276659\n",
      "Iteration: 1858 Loss: 0.0008117969996652653\n",
      "Iteration: 1859 Loss: 0.0008109525225195267\n",
      "Iteration: 1860 Loss: 0.0008101089227243357\n",
      "Iteration: 1861 Loss: 0.0008092661993160596\n",
      "Iteration: 1862 Loss: 0.0008084243513329141\n",
      "Iteration: 1863 Loss: 0.0008075833778132146\n",
      "Iteration: 1864 Loss: 0.0008067432777970616\n",
      "Iteration: 1865 Loss: 0.0008059040503269303\n",
      "Iteration: 1866 Loss: 0.0008050656944456399\n",
      "Iteration: 1867 Loss: 0.0008042282091981704\n",
      "Iteration: 1868 Loss: 0.0008033915936300677\n",
      "Iteration: 1869 Loss: 0.0008025558467893451\n",
      "Iteration: 1870 Loss: 0.0008017209677246602\n",
      "Iteration: 1871 Loss: 0.0008008869554863061\n",
      "Iteration: 1872 Loss: 0.0008000538091262728\n",
      "Iteration: 1873 Loss: 0.0007992215276977187\n",
      "Iteration: 1874 Loss: 0.0007983901102550059\n",
      "Iteration: 1875 Loss: 0.0007975595558538621\n",
      "Iteration: 1876 Loss: 0.0007967298635521809\n",
      "Iteration: 1877 Loss: 0.0007959010324073968\n",
      "Iteration: 1878 Loss: 0.0007950730614808518\n",
      "Iteration: 1879 Loss: 0.0007942459498331721\n",
      "Iteration: 1880 Loss: 0.0007934196965271959\n",
      "Iteration: 1881 Loss: 0.0007925943006272419\n",
      "Iteration: 1882 Loss: 0.0007917697611988761\n",
      "Iteration: 1883 Loss: 0.0007909460773088632\n",
      "Iteration: 1884 Loss: 0.0007901232480250048\n",
      "Iteration: 1885 Loss: 0.0007893012724171674\n",
      "Iteration: 1886 Loss: 0.0007884801495554705\n",
      "Iteration: 1887 Loss: 0.0007876598785131772\n",
      "Iteration: 1888 Loss: 0.00078684045836282\n",
      "Iteration: 1889 Loss: 0.000786021888179642\n",
      "Iteration: 1890 Loss: 0.0007852041670392521\n",
      "Iteration: 1891 Loss: 0.0007843872940195485\n",
      "Iteration: 1892 Loss: 0.0007835712681993181\n",
      "Iteration: 1893 Loss: 0.0007827560886580157\n",
      "Iteration: 1894 Loss: 0.0007819417544776622\n",
      "Iteration: 1895 Loss: 0.0007811282647398114\n",
      "Iteration: 1896 Loss: 0.0007803156185293436\n",
      "Iteration: 1897 Loss: 0.0007795038149303969\n",
      "Iteration: 1898 Loss: 0.0007786928530302665\n",
      "Iteration: 1899 Loss: 0.0007778827319162934\n",
      "Iteration: 1900 Loss: 0.0007770734506775081\n",
      "Iteration: 1901 Loss: 0.0007762650084037667\n",
      "Iteration: 1902 Loss: 0.0007754574041873583\n",
      "Iteration: 1903 Loss: 0.000774650637120407\n",
      "Iteration: 1904 Loss: 0.0007738447062975196\n",
      "Iteration: 1905 Loss: 0.0007730396108131894\n",
      "Iteration: 1906 Loss: 0.0007722353497642443\n",
      "Iteration: 1907 Loss: 0.0007714319222483045\n",
      "Iteration: 1908 Loss: 0.000770629327363953\n",
      "Iteration: 1909 Loss: 0.0007698275642120135\n",
      "Iteration: 1910 Loss: 0.0007690266318941061\n",
      "Iteration: 1911 Loss: 0.0007682265295124764\n",
      "Iteration: 1912 Loss: 0.0007674272561706606\n",
      "Iteration: 1913 Loss: 0.0007666288109740418\n",
      "Iteration: 1914 Loss: 0.0007658311930293938\n",
      "Iteration: 1915 Loss: 0.0007650344014432909\n",
      "Iteration: 1916 Loss: 0.0007642384353247721\n",
      "Iteration: 1917 Loss: 0.0007634432937844853\n",
      "Iteration: 1918 Loss: 0.0007626489759328853\n",
      "Iteration: 1919 Loss: 0.0007618554808819174\n",
      "Iteration: 1920 Loss: 0.0007610628077455437\n",
      "Iteration: 1921 Loss: 0.0007602709556382824\n",
      "Iteration: 1922 Loss: 0.0007594799236760507\n",
      "Iteration: 1923 Loss: 0.0007586897109760871\n",
      "Iteration: 1924 Loss: 0.0007579003166568414\n",
      "Iteration: 1925 Loss: 0.0007571117398369046\n",
      "Iteration: 1926 Loss: 0.0007563239796370174\n",
      "Iteration: 1927 Loss: 0.0007555370351797214\n",
      "Iteration: 1928 Loss: 0.0007547509055871508\n",
      "Iteration: 1929 Loss: 0.0007539655899831818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1930 Loss: 0.0007531810874929343\n",
      "Iteration: 1931 Loss: 0.000752397397243701\n",
      "Iteration: 1932 Loss: 0.0007516145183621913\n",
      "Iteration: 1933 Loss: 0.0007508324499774018\n",
      "Iteration: 1934 Loss: 0.0007500511912187073\n",
      "Iteration: 1935 Loss: 0.0007492707412174798\n",
      "Iteration: 1936 Loss: 0.0007484910991056908\n",
      "Iteration: 1937 Loss: 0.0007477122640161663\n",
      "Iteration: 1938 Loss: 0.0007469342350836134\n",
      "Iteration: 1939 Loss: 0.0007461570114436577\n",
      "Iteration: 1940 Loss: 0.000745380592232698\n",
      "Iteration: 1941 Loss: 0.000744604976587917\n",
      "Iteration: 1942 Loss: 0.0007438301636484257\n",
      "Iteration: 1943 Loss: 0.0007430561525542508\n",
      "Iteration: 1944 Loss: 0.0007422829424467143\n",
      "Iteration: 1945 Loss: 0.0007415105324674506\n",
      "Iteration: 1946 Loss: 0.0007407389217596879\n",
      "Iteration: 1947 Loss: 0.0007399681094681543\n",
      "Iteration: 1948 Loss: 0.0007391980947380163\n",
      "Iteration: 1949 Loss: 0.0007384288767161357\n",
      "Iteration: 1950 Loss: 0.0007376604545497158\n",
      "Iteration: 1951 Loss: 0.0007368928273878047\n",
      "Iteration: 1952 Loss: 0.0007361259943797517\n",
      "Iteration: 1953 Loss: 0.0007353599546770218\n",
      "Iteration: 1954 Loss: 0.0007345947074316054\n",
      "Iteration: 1955 Loss: 0.0007338302517960108\n",
      "Iteration: 1956 Loss: 0.0007330665869243237\n",
      "Iteration: 1957 Loss: 0.0007323037119724571\n",
      "Iteration: 1958 Loss: 0.0007315416260963923\n",
      "Iteration: 1959 Loss: 0.0007307803284537819\n",
      "Iteration: 1960 Loss: 0.0007300198182022905\n",
      "Iteration: 1961 Loss: 0.0007292600945017853\n",
      "Iteration: 1962 Loss: 0.0007285011565124753\n",
      "Iteration: 1963 Loss: 0.0007277430033964402\n",
      "Iteration: 1964 Loss: 0.0007269856343162965\n",
      "Iteration: 1965 Loss: 0.0007262290484350142\n",
      "Iteration: 1966 Loss: 0.0007254732449178708\n",
      "Iteration: 1967 Loss: 0.0007247182229307745\n",
      "Iteration: 1968 Loss: 0.0007239639816405134\n",
      "Iteration: 1969 Loss: 0.0007232105202144544\n",
      "Iteration: 1970 Loss: 0.0007224578378221037\n",
      "Iteration: 1971 Loss: 0.0007217059336332673\n",
      "Iteration: 1972 Loss: 0.0007209548068192192\n",
      "Iteration: 1973 Loss: 0.0007202044565511081\n",
      "Iteration: 1974 Loss: 0.0007194548820025974\n",
      "Iteration: 1975 Loss: 0.0007187060823475927\n",
      "Iteration: 1976 Loss: 0.0007179580567611027\n",
      "Iteration: 1977 Loss: 0.0007172108044194692\n",
      "Iteration: 1978 Loss: 0.000716464324499599\n",
      "Iteration: 1979 Loss: 0.0007157186161800417\n",
      "Iteration: 1980 Loss: 0.0007149736786391425\n",
      "Iteration: 1981 Loss: 0.0007142295110576918\n",
      "Iteration: 1982 Loss: 0.0007134861126167106\n",
      "Iteration: 1983 Loss: 0.0007127434824980437\n",
      "Iteration: 1984 Loss: 0.000712001619885555\n",
      "Iteration: 1985 Loss: 0.0007112605239630607\n",
      "Iteration: 1986 Loss: 0.0007105201939154391\n",
      "Iteration: 1987 Loss: 0.0007097806289294589\n",
      "Iteration: 1988 Loss: 0.0007090418281923532\n",
      "Iteration: 1989 Loss: 0.0007083037908921209\n",
      "Iteration: 1990 Loss: 0.0007075665162174855\n",
      "Iteration: 1991 Loss: 0.0007068300033589358\n",
      "Iteration: 1992 Loss: 0.000706094251507476\n",
      "Iteration: 1993 Loss: 0.0007053592598553345\n",
      "Iteration: 1994 Loss: 0.0007046250275958002\n",
      "Iteration: 1995 Loss: 0.0007038915539225791\n",
      "Iteration: 1996 Loss: 0.0007031588380310582\n",
      "Iteration: 1997 Loss: 0.0007024268791171691\n",
      "Iteration: 1998 Loss: 0.0007016956763778822\n",
      "Iteration: 1999 Loss: 0.0007009652290110782\n",
      "Iteration: 2000 Loss: 0.0007002355362161161\n",
      "Iteration: 2001 Loss: 0.0006995065971924248\n",
      "Iteration: 2002 Loss: 0.0006987784111414158\n",
      "Iteration: 2003 Loss: 0.0006980509772643327\n",
      "Iteration: 2004 Loss: 0.0006973242947640882\n",
      "Iteration: 2005 Loss: 0.0006965983628448024\n",
      "Iteration: 2006 Loss: 0.0006958731807107613\n",
      "Iteration: 2007 Loss: 0.000695148747568408\n",
      "Iteration: 2008 Loss: 0.0006944250626238753\n",
      "Iteration: 2009 Loss: 0.0006937021250843972\n",
      "Iteration: 2010 Loss: 0.0006929799341587381\n",
      "Iteration: 2011 Loss: 0.0006922584890564821\n",
      "Iteration: 2012 Loss: 0.0006915377889881971\n",
      "Iteration: 2013 Loss: 0.0006908178331647861\n",
      "Iteration: 2014 Loss: 0.0006900986207991534\n",
      "Iteration: 2015 Loss: 0.0006893801511038616\n",
      "Iteration: 2016 Loss: 0.0006886624232933947\n",
      "Iteration: 2017 Loss: 0.000687945436583061\n",
      "Iteration: 2018 Loss: 0.0006872291901884816\n",
      "Iteration: 2019 Loss: 0.0006865136833270466\n",
      "Iteration: 2020 Loss: 0.0006857989152160829\n",
      "Iteration: 2021 Loss: 0.0006850848850752004\n",
      "Iteration: 2022 Loss: 0.0006843715921238262\n",
      "Iteration: 2023 Loss: 0.000683659035582876\n",
      "Iteration: 2024 Loss: 0.0006829472146729548\n",
      "Iteration: 2025 Loss: 0.0006822361286176733\n",
      "Iteration: 2026 Loss: 0.0006815257766398809\n",
      "Iteration: 2027 Loss: 0.0006808161579639281\n",
      "Iteration: 2028 Loss: 0.0006801072718152321\n",
      "Iteration: 2029 Loss: 0.0006793991174198671\n",
      "Iteration: 2030 Loss: 0.000678691694005569\n",
      "Iteration: 2031 Loss: 0.0006779850007998027\n",
      "Iteration: 2032 Loss: 0.0006772790370312825\n",
      "Iteration: 2033 Loss: 0.000676573801929873\n",
      "Iteration: 2034 Loss: 0.0006758692947265244\n",
      "Iteration: 2035 Loss: 0.0006751655146529476\n",
      "Iteration: 2036 Loss: 0.0006744624609413641\n",
      "Iteration: 2037 Loss: 0.0006737601328246947\n",
      "Iteration: 2038 Loss: 0.0006730585295381909\n",
      "Iteration: 2039 Loss: 0.0006723576503167219\n",
      "Iteration: 2040 Loss: 0.0006716574943964644\n",
      "Iteration: 2041 Loss: 0.0006709580610140012\n",
      "Iteration: 2042 Loss: 0.0006702593494075703\n",
      "Iteration: 2043 Loss: 0.0006695613588161033\n",
      "Iteration: 2044 Loss: 0.0006688640884788466\n",
      "Iteration: 2045 Loss: 0.0006681675376364116\n",
      "Iteration: 2046 Loss: 0.0006674717055302075\n",
      "Iteration: 2047 Loss: 0.000666776591402735\n",
      "Iteration: 2048 Loss: 0.0006660821944973387\n",
      "Iteration: 2049 Loss: 0.0006653885140573008\n",
      "Iteration: 2050 Loss: 0.0006646955493283853\n",
      "Iteration: 2051 Loss: 0.00066400329955611\n",
      "Iteration: 2052 Loss: 0.0006633117639868077\n",
      "Iteration: 2053 Loss: 0.0006626209418683012\n",
      "Iteration: 2054 Loss: 0.0006619308324491475\n",
      "Iteration: 2055 Loss: 0.0006612414349783355\n",
      "Iteration: 2056 Loss: 0.0006605527487057682\n",
      "Iteration: 2057 Loss: 0.0006598647728833164\n",
      "Iteration: 2058 Loss: 0.0006591775067625432\n",
      "Iteration: 2059 Loss: 0.0006584909495965429\n",
      "Iteration: 2060 Loss: 0.0006578051006376833\n",
      "Iteration: 2061 Loss: 0.0006571199591413578\n",
      "Iteration: 2062 Loss: 0.0006564355243628649\n",
      "Iteration: 2063 Loss: 0.0006557517955585469\n",
      "Iteration: 2064 Loss: 0.0006550687719851255\n",
      "Iteration: 2065 Loss: 0.0006543864529005312\n",
      "Iteration: 2066 Loss: 0.00065370483756333\n",
      "Iteration: 2067 Loss: 0.0006530239252336206\n",
      "Iteration: 2068 Loss: 0.000652343715171146\n",
      "Iteration: 2069 Loss: 0.000651664206637608\n",
      "Iteration: 2070 Loss: 0.0006509853988951426\n",
      "Iteration: 2071 Loss: 0.0006503072912068266\n",
      "Iteration: 2072 Loss: 0.0006496298828363371\n",
      "Iteration: 2073 Loss: 0.0006489531730478869\n",
      "Iteration: 2074 Loss: 0.0006482771611073815\n",
      "Iteration: 2075 Loss: 0.0006476018462813116\n",
      "Iteration: 2076 Loss: 0.0006469272278366394\n",
      "Iteration: 2077 Loss: 0.0006462533050417022\n",
      "Iteration: 2078 Loss: 0.000645580077165161\n",
      "Iteration: 2079 Loss: 0.0006449075434762238\n",
      "Iteration: 2080 Loss: 0.0006442357032461738\n",
      "Iteration: 2081 Loss: 0.0006435645557456513\n",
      "Iteration: 2082 Loss: 0.0006428941002470653\n",
      "Iteration: 2083 Loss: 0.0006422243360232221\n",
      "Iteration: 2084 Loss: 0.000641555262348762\n",
      "Iteration: 2085 Loss: 0.0006408868784974471\n",
      "Iteration: 2086 Loss: 0.00064021918374457\n",
      "Iteration: 2087 Loss: 0.0006395521773672624\n",
      "Iteration: 2088 Loss: 0.0006388858586424118\n",
      "Iteration: 2089 Loss: 0.0006382202268473858\n",
      "Iteration: 2090 Loss: 0.000637555281261668\n",
      "Iteration: 2091 Loss: 0.0006368910211641286\n",
      "Iteration: 2092 Loss: 0.0006362274458353716\n",
      "Iteration: 2093 Loss: 0.0006355645545564858\n",
      "Iteration: 2094 Loss: 0.000634902346610223\n",
      "Iteration: 2095 Loss: 0.0006342408212785474\n",
      "Iteration: 2096 Loss: 0.0006335799778456272\n",
      "Iteration: 2097 Loss: 0.0006329198155950121\n",
      "Iteration: 2098 Loss: 0.0006322603338127066\n",
      "Iteration: 2099 Loss: 0.00063160153178446\n",
      "Iteration: 2100 Loss: 0.0006309434087975052\n",
      "Iteration: 2101 Loss: 0.0006302859641387733\n",
      "Iteration: 2102 Loss: 0.0006296291970971002\n",
      "Iteration: 2103 Loss: 0.0006289731069619324\n",
      "Iteration: 2104 Loss: 0.0006283176930229019\n",
      "Iteration: 2105 Loss: 0.0006276629545707831\n",
      "Iteration: 2106 Loss: 0.0006270088908976009\n",
      "Iteration: 2107 Loss: 0.000626355501295897\n",
      "Iteration: 2108 Loss: 0.0006257027850584438\n",
      "Iteration: 2109 Loss: 0.0006250507414796894\n",
      "Iteration: 2110 Loss: 0.0006243993698537075\n",
      "Iteration: 2111 Loss: 0.0006237486694770948\n",
      "Iteration: 2112 Loss: 0.0006230986396454093\n",
      "Iteration: 2113 Loss: 0.000622449279656239\n",
      "Iteration: 2114 Loss: 0.0006218005888072793\n",
      "Iteration: 2115 Loss: 0.0006211525663976854\n",
      "Iteration: 2116 Loss: 0.0006205052117267068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2117 Loss: 0.0006198585240946621\n",
      "Iteration: 2118 Loss: 0.0006192125028026898\n",
      "Iteration: 2119 Loss: 0.0006185671471525618\n",
      "Iteration: 2120 Loss: 0.0006179224564471481\n",
      "Iteration: 2121 Loss: 0.0006172784299900472\n",
      "Iteration: 2122 Loss: 0.0006166350670850002\n",
      "Iteration: 2123 Loss: 0.0006159923670369592\n",
      "Iteration: 2124 Loss: 0.0006153503291518322\n",
      "Iteration: 2125 Loss: 0.0006147089527363899\n",
      "Iteration: 2126 Loss: 0.0006140682370973631\n",
      "Iteration: 2127 Loss: 0.0006134281815434615\n",
      "Iteration: 2128 Loss: 0.0006127887853826246\n",
      "Iteration: 2129 Loss: 0.0006121500479255376\n",
      "Iteration: 2130 Loss: 0.0006115119684821968\n",
      "Iteration: 2131 Loss: 0.000610874546363529\n",
      "Iteration: 2132 Loss: 0.0006102377808819615\n",
      "Iteration: 2133 Loss: 0.0006096016713497894\n",
      "Iteration: 2134 Loss: 0.0006089662170803406\n",
      "Iteration: 2135 Loss: 0.0006083314173880743\n",
      "Iteration: 2136 Loss: 0.0006076972715879694\n",
      "Iteration: 2137 Loss: 0.0006070637789953722\n",
      "Iteration: 2138 Loss: 0.000606430938927205\n",
      "Iteration: 2139 Loss: 0.0006057987507001882\n",
      "Iteration: 2140 Loss: 0.0006051672136324098\n",
      "Iteration: 2141 Loss: 0.0006045363270426833\n",
      "Iteration: 2142 Loss: 0.0006039060902508669\n",
      "Iteration: 2143 Loss: 0.0006032765025767291\n",
      "Iteration: 2144 Loss: 0.0006026475633416589\n",
      "Iteration: 2145 Loss: 0.000602019271867521\n",
      "Iteration: 2146 Loss: 0.0006013916274762434\n",
      "Iteration: 2147 Loss: 0.0006007646294918069\n",
      "Iteration: 2148 Loss: 0.0006001382772374124\n",
      "Iteration: 2149 Loss: 0.0005995125700379211\n",
      "Iteration: 2150 Loss: 0.0005988875072196134\n",
      "Iteration: 2151 Loss: 0.000598263088108259\n",
      "Iteration: 2152 Loss: 0.0005976393120306036\n",
      "Iteration: 2153 Loss: 0.0005970161783149601\n",
      "Iteration: 2154 Loss: 0.0005963936862885522\n",
      "Iteration: 2155 Loss: 0.0005957718352815569\n",
      "Iteration: 2156 Loss: 0.0005951506246238702\n",
      "Iteration: 2157 Loss: 0.0005945300536459598\n",
      "Iteration: 2158 Loss: 0.0005939101216795969\n",
      "Iteration: 2159 Loss: 0.0005932908280566692\n",
      "Iteration: 2160 Loss: 0.0005926721721099533\n",
      "Iteration: 2161 Loss: 0.0005920541531731074\n",
      "Iteration: 2162 Loss: 0.000591436770580744\n",
      "Iteration: 2163 Loss: 0.0005908200236676624\n",
      "Iteration: 2164 Loss: 0.0005902039117700121\n",
      "Iteration: 2165 Loss: 0.0005895884342235545\n",
      "Iteration: 2166 Loss: 0.0005889735903663308\n",
      "Iteration: 2167 Loss: 0.0005883593795358362\n",
      "Iteration: 2168 Loss: 0.0005877458010713622\n",
      "Iteration: 2169 Loss: 0.000587132854312003\n",
      "Iteration: 2170 Loss: 0.0005865205385981932\n",
      "Iteration: 2171 Loss: 0.0005859088532705103\n",
      "Iteration: 2172 Loss: 0.0005852977976706836\n",
      "Iteration: 2173 Loss: 0.0005846873711412696\n",
      "Iteration: 2174 Loss: 0.0005840775730249839\n",
      "Iteration: 2175 Loss: 0.0005834684026656238\n",
      "Iteration: 2176 Loss: 0.0005828598594079576\n",
      "Iteration: 2177 Loss: 0.0005822519425973046\n",
      "Iteration: 2178 Loss: 0.0005816446515793364\n",
      "Iteration: 2179 Loss: 0.0005810379857011303\n",
      "Iteration: 2180 Loss: 0.000580431944309765\n",
      "Iteration: 2181 Loss: 0.0005798265267535129\n",
      "Iteration: 2182 Loss: 0.0005792217323809745\n",
      "Iteration: 2183 Loss: 0.0005786175605417589\n",
      "Iteration: 2184 Loss: 0.0005780140105857876\n",
      "Iteration: 2185 Loss: 0.0005774110818642321\n",
      "Iteration: 2186 Loss: 0.0005768087737286729\n",
      "Iteration: 2187 Loss: 0.0005762070855314168\n",
      "Iteration: 2188 Loss: 0.0005756060166257428\n",
      "Iteration: 2189 Loss: 0.0005750055663649821\n",
      "Iteration: 2190 Loss: 0.000574405734104594\n",
      "Iteration: 2191 Loss: 0.000573806519199105\n",
      "Iteration: 2192 Loss: 0.0005732079210044215\n",
      "Iteration: 2193 Loss: 0.0005726099388766127\n",
      "Iteration: 2194 Loss: 0.0005720125721743181\n",
      "Iteration: 2195 Loss: 0.0005714158202547662\n",
      "Iteration: 2196 Loss: 0.0005708196824765746\n",
      "Iteration: 2197 Loss: 0.000570224158199387\n",
      "Iteration: 2198 Loss: 0.0005696292467833119\n",
      "Iteration: 2199 Loss: 0.0005690349475888884\n",
      "Iteration: 2200 Loss: 0.0005684412599777747\n",
      "Iteration: 2201 Loss: 0.0005678481833123492\n",
      "Iteration: 2202 Loss: 0.0005672557169555269\n",
      "Iteration: 2203 Loss: 0.0005666638602703751\n",
      "Iteration: 2204 Loss: 0.0005660726126216942\n",
      "Iteration: 2205 Loss: 0.0005654819733745319\n",
      "Iteration: 2206 Loss: 0.0005648919418941687\n",
      "Iteration: 2207 Loss: 0.0005643025175473364\n",
      "Iteration: 2208 Loss: 0.000563713699701506\n",
      "Iteration: 2209 Loss: 0.0005631254877237654\n",
      "Iteration: 2210 Loss: 0.0005625378809825604\n",
      "Iteration: 2211 Loss: 0.0005619508788472333\n",
      "Iteration: 2212 Loss: 0.0005613644806878575\n",
      "Iteration: 2213 Loss: 0.00056077868587497\n",
      "Iteration: 2214 Loss: 0.0005601934937795623\n",
      "Iteration: 2215 Loss: 0.0005596089037736003\n",
      "Iteration: 2216 Loss: 0.0005590249152298105\n",
      "Iteration: 2217 Loss: 0.0005584415275214122\n",
      "Iteration: 2218 Loss: 0.0005578587400220904\n",
      "Iteration: 2219 Loss: 0.0005572765521068881\n",
      "Iteration: 2220 Loss: 0.0005566949631511847\n",
      "Iteration: 2221 Loss: 0.0005561139725311104\n",
      "Iteration: 2222 Loss: 0.0005555335796231402\n",
      "Iteration: 2223 Loss: 0.0005549537838049399\n",
      "Iteration: 2224 Loss: 0.0005543745844547787\n",
      "Iteration: 2225 Loss: 0.0005537959809512737\n",
      "Iteration: 2226 Loss: 0.0005532179726738821\n",
      "Iteration: 2227 Loss: 0.0005526405590026413\n",
      "Iteration: 2228 Loss: 0.0005520637393185836\n",
      "Iteration: 2229 Loss: 0.0005514875130034992\n",
      "Iteration: 2230 Loss: 0.0005509118794396307\n",
      "Iteration: 2231 Loss: 0.0005503368380093904\n",
      "Iteration: 2232 Loss: 0.0005497623880969531\n",
      "Iteration: 2233 Loss: 0.0005491885290863941\n",
      "Iteration: 2234 Loss: 0.0005486152603626708\n",
      "Iteration: 2235 Loss: 0.0005480425813122419\n",
      "Iteration: 2236 Loss: 0.0005474704913205644\n",
      "Iteration: 2237 Loss: 0.0005468989897754298\n",
      "Iteration: 2238 Loss: 0.0005463280760643493\n",
      "Iteration: 2239 Loss: 0.0005457577495757019\n",
      "Iteration: 2240 Loss: 0.00054518800969881\n",
      "Iteration: 2241 Loss: 0.0005446188558237264\n",
      "Iteration: 2242 Loss: 0.0005440502873411778\n",
      "Iteration: 2243 Loss: 0.000543482303642252\n",
      "Iteration: 2244 Loss: 0.0005429149041188858\n",
      "Iteration: 2245 Loss: 0.0005423480881642581\n",
      "Iteration: 2246 Loss: 0.0005417818551711713\n",
      "Iteration: 2247 Loss: 0.0005412162045344504\n",
      "Iteration: 2248 Loss: 0.0005406511356487186\n",
      "Iteration: 2249 Loss: 0.0005400866479096252\n",
      "Iteration: 2250 Loss: 0.0005395227407136681\n",
      "Iteration: 2251 Loss: 0.0005389594134576804\n",
      "Iteration: 2252 Loss: 0.0005383966655400891\n",
      "Iteration: 2253 Loss: 0.0005378344963592336\n",
      "Iteration: 2254 Loss: 0.0005372729053145638\n",
      "Iteration: 2255 Loss: 0.0005367118918062485\n",
      "Iteration: 2256 Loss: 0.0005361514552357378\n",
      "Iteration: 2257 Loss: 0.000535591595004612\n",
      "Iteration: 2258 Loss: 0.0005350323105151967\n",
      "Iteration: 2259 Loss: 0.0005344736011705715\n",
      "Iteration: 2260 Loss: 0.0005339154663758932\n",
      "Iteration: 2261 Loss: 0.0005333579055358896\n",
      "Iteration: 2262 Loss: 0.0005328009180566797\n",
      "Iteration: 2263 Loss: 0.0005322445033448731\n",
      "Iteration: 2264 Loss: 0.0005316886608085756\n",
      "Iteration: 2265 Loss: 0.000531133389856697\n",
      "Iteration: 2266 Loss: 0.0005305786898994648\n",
      "Iteration: 2267 Loss: 0.000530024560347252\n",
      "Iteration: 2268 Loss: 0.0005294710006123094\n",
      "Iteration: 2269 Loss: 0.0005289180101079005\n",
      "Iteration: 2270 Loss: 0.0005283655882480878\n",
      "Iteration: 2271 Loss: 0.0005278137344490203\n",
      "Iteration: 2272 Loss: 0.0005272624481276701\n",
      "Iteration: 2273 Loss: 0.0005267117287018873\n",
      "Iteration: 2274 Loss: 0.0005261615755923564\n",
      "Iteration: 2275 Loss: 0.0005256119882199004\n",
      "Iteration: 2276 Loss: 0.0005250629660087799\n",
      "Iteration: 2277 Loss: 0.0005245145083837585\n",
      "Iteration: 2278 Loss: 0.0005239666147733715\n",
      "Iteration: 2279 Loss: 0.0005234192846073778\n",
      "Iteration: 2280 Loss: 0.0005228725173181885\n",
      "Iteration: 2281 Loss: 0.000522326312342239\n",
      "Iteration: 2282 Loss: 0.0005217806691182248\n",
      "Iteration: 2283 Loss: 0.0005212355870898808\n",
      "Iteration: 2284 Loss: 0.0005206910657042464\n",
      "Iteration: 2285 Loss: 0.0005201471044150447\n",
      "Iteration: 2286 Loss: 0.0005196037026812355\n",
      "Iteration: 2287 Loss: 0.0005190608599683773\n",
      "Iteration: 2288 Loss: 0.0005185185757522116\n",
      "Iteration: 2289 Loss: 0.0005179768495174768\n",
      "Iteration: 2290 Loss: 0.0005174356807611475\n",
      "Iteration: 2291 Loss: 0.000516895068996704\n",
      "Iteration: 2292 Loss: 0.0005163550137556847\n",
      "Iteration: 2293 Loss: 0.0005158155145939095\n",
      "Iteration: 2294 Loss: 0.0005152765710976804\n",
      "Iteration: 2295 Loss: 0.00051473818289112\n",
      "Iteration: 2296 Loss: 0.0005142003496491193\n",
      "Iteration: 2297 Loss: 0.000513663071114662\n",
      "Iteration: 2298 Loss: 0.000513126347121529\n",
      "Iteration: 2299 Loss: 0.0005125901776284766\n",
      "Iteration: 2300 Loss: 0.0005120545627715436\n",
      "Iteration: 2301 Loss: 0.000511519502943326\n",
      "Iteration: 2302 Loss: 0.0005109849989191731\n",
      "Iteration: 2303 Loss: 0.0005104510520663317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2304 Loss: 0.0005099176647052979\n",
      "Iteration: 2305 Loss: 0.0005093848407792657\n",
      "Iteration: 2306 Loss: 0.0005088525871574125\n",
      "Iteration: 2307 Loss: 0.0005083209164450363\n",
      "Iteration: 2308 Loss: 0.0005077898537103628\n",
      "Iteration: 2309 Loss: 0.0005072594552594795\n",
      "Iteration: 2310 Loss: 0.0005067298731418137\n",
      "Iteration: 2311 Loss: 0.0005062016476882322\n",
      "Iteration: 2312 Loss: 0.000505634589287831\n",
      "Iteration: 2313 Loss: 0.0005024285237107978\n",
      "Iteration: 2314 Loss: 0.0004976116075010513\n",
      "Iteration: 2315 Loss: 0.0004917007041902893\n",
      "Iteration: 2316 Loss: 0.00048518726901179254\n",
      "Iteration: 2317 Loss: 0.0004785012346606502\n",
      "Iteration: 2318 Loss: 0.00047199410683911654\n",
      "Iteration: 2319 Loss: 0.0004659341078305744\n",
      "Iteration: 2320 Loss: 0.0004605086267162941\n",
      "Iteration: 2321 Loss: 0.0004558313532907706\n",
      "Iteration: 2322 Loss: 0.0004542293753844768\n",
      "Iteration: 2323 Loss: 0.00045621710596824945\n",
      "Iteration: 2324 Loss: 0.000457575080208898\n",
      "Iteration: 2325 Loss: 0.0004582540593898039\n",
      "Iteration: 2326 Loss: 0.0004583457798874195\n",
      "Iteration: 2327 Loss: 0.00045795173465489975\n",
      "Iteration: 2328 Loss: 0.0004571768188667734\n",
      "Iteration: 2329 Loss: 0.00045612210278022324\n",
      "Iteration: 2330 Loss: 0.0004548807638725486\n",
      "Iteration: 2331 Loss: 0.00045352916093223703\n",
      "Iteration: 2332 Loss: 0.0004521361024796397\n",
      "Iteration: 2333 Loss: 0.00045075719941648824\n",
      "Iteration: 2334 Loss: 0.0004494354126816515\n",
      "Iteration: 2335 Loss: 0.0004482000676405816\n",
      "Iteration: 2336 Loss: 0.00044707026029063094\n",
      "Iteration: 2337 Loss: 0.00044610487726846963\n",
      "Iteration: 2338 Loss: 0.00044571390315699204\n",
      "Iteration: 2339 Loss: 0.0004455039554341641\n",
      "Iteration: 2340 Loss: 0.00044522530542858097\n",
      "Iteration: 2341 Loss: 0.0004448711365122374\n",
      "Iteration: 2342 Loss: 0.00044444874877876887\n",
      "Iteration: 2343 Loss: 0.00044396657321613164\n",
      "Iteration: 2344 Loss: 0.00044343923882082255\n",
      "Iteration: 2345 Loss: 0.00044288105064727614\n",
      "Iteration: 2346 Loss: 0.0004423030326066813\n",
      "Iteration: 2347 Loss: 0.0004417153584185401\n",
      "Iteration: 2348 Loss: 0.00044112718075382644\n",
      "Iteration: 2349 Loss: 0.00044054995117035053\n",
      "Iteration: 2350 Loss: 0.0004399836917434565\n",
      "Iteration: 2351 Loss: 0.0004394311114833051\n",
      "Iteration: 2352 Loss: 0.0004388969552639122\n",
      "Iteration: 2353 Loss: 0.0004384281504109697\n",
      "Iteration: 2354 Loss: 0.0004380073119442141\n",
      "Iteration: 2355 Loss: 0.0004375766221438533\n",
      "Iteration: 2356 Loss: 0.00043713398426870596\n",
      "Iteration: 2357 Loss: 0.00043668095650146375\n",
      "Iteration: 2358 Loss: 0.00043621948429876836\n",
      "Iteration: 2359 Loss: 0.0004357515855745349\n",
      "Iteration: 2360 Loss: 0.00043527926311684884\n",
      "Iteration: 2361 Loss: 0.00043480426188806257\n",
      "Iteration: 2362 Loss: 0.0004343279909924073\n",
      "Iteration: 2363 Loss: 0.0004338517821964287\n",
      "Iteration: 2364 Loss: 0.0004333767595623867\n",
      "Iteration: 2365 Loss: 0.0004329039139315147\n",
      "Iteration: 2366 Loss: 0.00043243426144245497\n",
      "Iteration: 2367 Loss: 0.000431969098082895\n",
      "Iteration: 2368 Loss: 0.00043150943149843096\n",
      "Iteration: 2369 Loss: 0.00043105410002637054\n",
      "Iteration: 2370 Loss: 0.0004306005601650972\n",
      "Iteration: 2371 Loss: 0.0004301472113827692\n",
      "Iteration: 2372 Loss: 0.00042969352385839295\n",
      "Iteration: 2373 Loss: 0.0004292394549049155\n",
      "Iteration: 2374 Loss: 0.0004287851473892975\n",
      "Iteration: 2375 Loss: 0.00042833081105876636\n",
      "Iteration: 2376 Loss: 0.00042787668795573735\n",
      "Iteration: 2377 Loss: 0.0004274229453431999\n",
      "Iteration: 2378 Loss: 0.00042696975919498446\n",
      "Iteration: 2379 Loss: 0.0004265172592771117\n",
      "Iteration: 2380 Loss: 0.0004260655174685202\n",
      "Iteration: 2381 Loss: 0.00042561454459042825\n",
      "Iteration: 2382 Loss: 0.0004251643005269782\n",
      "Iteration: 2383 Loss: 0.00042471471577128884\n",
      "Iteration: 2384 Loss: 0.0004242657153503088\n",
      "Iteration: 2385 Loss: 0.00042381723610580927\n",
      "Iteration: 2386 Loss: 0.00042336923426624284\n",
      "Iteration: 2387 Loss: 0.00042292168554494556\n",
      "Iteration: 2388 Loss: 0.0004224745809218939\n",
      "Iteration: 2389 Loss: 0.00042202792174804915\n",
      "Iteration: 2390 Loss: 0.000421581715213701\n",
      "Iteration: 2391 Loss: 0.00042113597414513527\n",
      "Iteration: 2392 Loss: 0.00042069070360134454\n",
      "Iteration: 2393 Loss: 0.00042024590825513037\n",
      "Iteration: 2394 Loss: 0.0004198015921751377\n",
      "Iteration: 2395 Loss: 0.0004193577571851431\n",
      "Iteration: 2396 Loss: 0.00041891440324073924\n",
      "Iteration: 2397 Loss: 0.0004184715290482892\n",
      "Iteration: 2398 Loss: 0.00041802913266810647\n",
      "Iteration: 2399 Loss: 0.00041758721199688784\n",
      "Iteration: 2400 Loss: 0.00041714576510593825\n",
      "Iteration: 2401 Loss: 0.00041670479038339523\n",
      "Iteration: 2402 Loss: 0.000416264286457337\n",
      "Iteration: 2403 Loss: 0.00041582425228703737\n",
      "Iteration: 2404 Loss: 0.00041538468709280884\n",
      "Iteration: 2405 Loss: 0.00041494559029949294\n",
      "Iteration: 2406 Loss: 0.0004145069621665674\n",
      "Iteration: 2407 Loss: 0.0004140688010410393\n",
      "Iteration: 2408 Loss: 0.00041363110636568766\n",
      "Iteration: 2409 Loss: 0.00041319387755893824\n",
      "Iteration: 2410 Loss: 0.0004127571140143519\n",
      "Iteration: 2411 Loss: 0.00041232081510865905\n",
      "Iteration: 2412 Loss: 0.00041188498021353095\n",
      "Iteration: 2413 Loss: 0.00041144960870694844\n",
      "Iteration: 2414 Loss: 0.0004110146999807066\n",
      "Iteration: 2415 Loss: 0.00041058025344601045\n",
      "Iteration: 2416 Loss: 0.00041014626853391593\n",
      "Iteration: 2417 Loss: 0.000409712744695718\n",
      "Iteration: 2418 Loss: 0.000409279681396418\n",
      "Iteration: 2419 Loss: 0.00040884707811307666\n",
      "Iteration: 2420 Loss: 0.0004084149343298484\n",
      "Iteration: 2421 Loss: 0.00040798324953419853\n",
      "Iteration: 2422 Loss: 0.00040755202321419043\n",
      "Iteration: 2423 Loss: 0.0004071212548573113\n",
      "Iteration: 2424 Loss: 0.0004066909439490996\n",
      "Iteration: 2425 Loss: 0.0004062610899723711\n",
      "Iteration: 2426 Loss: 0.0004058316924069213\n",
      "Iteration: 2427 Loss: 0.00040540275073106484\n",
      "Iteration: 2428 Loss: 0.00040497426442187275\n",
      "Iteration: 2429 Loss: 0.0004045462329559446\n",
      "Iteration: 2430 Loss: 0.0004041186558086904\n",
      "Iteration: 2431 Loss: 0.0004036915324565957\n",
      "Iteration: 2432 Loss: 0.0004032648623761788\n",
      "Iteration: 2433 Loss: 0.0004028386450455126\n",
      "Iteration: 2434 Loss: 0.000402412879943706\n",
      "Iteration: 2435 Loss: 0.0004019875665511687\n",
      "Iteration: 2436 Loss: 0.0004015627043505813\n",
      "Iteration: 2437 Loss: 0.0004011382928258096\n",
      "Iteration: 2438 Loss: 0.00040071433146257374\n",
      "Iteration: 2439 Loss: 0.0004002908197482468\n",
      "Iteration: 2440 Loss: 0.0003998677571725557\n",
      "Iteration: 2441 Loss: 0.00039944514322643886\n",
      "Iteration: 2442 Loss: 0.00039902297740293253\n",
      "Iteration: 2443 Loss: 0.00039860125919685853\n",
      "Iteration: 2444 Loss: 0.0003981799881042881\n",
      "Iteration: 2445 Loss: 0.0003977591636228175\n",
      "Iteration: 2446 Loss: 0.000397338785251981\n",
      "Iteration: 2447 Loss: 0.0003969188524926076\n",
      "Iteration: 2448 Loss: 0.00039649936484676345\n",
      "Iteration: 2449 Loss: 0.00039608032181753024\n",
      "Iteration: 2450 Loss: 0.00039566172290973143\n",
      "Iteration: 2451 Loss: 0.00039524356762902395\n",
      "Iteration: 2452 Loss: 0.00039482585548224387\n",
      "Iteration: 2453 Loss: 0.00039440858597705183\n",
      "Iteration: 2454 Loss: 0.00039399175862259064\n",
      "Iteration: 2455 Loss: 0.0003935753729284011\n",
      "Iteration: 2456 Loss: 0.0003931594284059063\n",
      "Iteration: 2457 Loss: 0.0003927439245666747\n",
      "Iteration: 2458 Loss: 0.0003923288609238741\n",
      "Iteration: 2459 Loss: 0.0003919142369912029\n",
      "Iteration: 2460 Loss: 0.0003915000522836124\n",
      "Iteration: 2461 Loss: 0.0003910863063168608\n",
      "Iteration: 2462 Loss: 0.00039067299860766585\n",
      "Iteration: 2463 Loss: 0.0003902601286737252\n",
      "Iteration: 2464 Loss: 0.00038984769603382706\n",
      "Iteration: 2465 Loss: 0.00038943570020733297\n",
      "Iteration: 2466 Loss: 0.0003890241407145105\n",
      "Iteration: 2467 Loss: 0.0003886130170769809\n",
      "Iteration: 2468 Loss: 0.000388202328816956\n",
      "Iteration: 2469 Loss: 0.00038779207545750773\n",
      "Iteration: 2470 Loss: 0.00038738225652274545\n",
      "Iteration: 2471 Loss: 0.00038697287153764714\n",
      "Iteration: 2472 Loss: 0.00038656392002778695\n",
      "Iteration: 2473 Loss: 0.0003861554015198387\n",
      "Iteration: 2474 Loss: 0.0003857473155413065\n",
      "Iteration: 2475 Loss: 0.0003853396616203792\n",
      "Iteration: 2476 Loss: 0.00038493243928632053\n",
      "Iteration: 2477 Loss: 0.0003845256480684185\n",
      "Iteration: 2478 Loss: 0.00038411928749822156\n",
      "Iteration: 2479 Loss: 0.00038371335710644114\n",
      "Iteration: 2480 Loss: 0.0003833078564258503\n",
      "Iteration: 2481 Loss: 0.0003829027849890368\n",
      "Iteration: 2482 Loss: 0.0003824981423301015\n",
      "Iteration: 2483 Loss: 0.0003820939279834097\n",
      "Iteration: 2484 Loss: 0.0003816901414841182\n",
      "Iteration: 2485 Loss: 0.0003812867823685912\n",
      "Iteration: 2486 Loss: 0.00038088385017333944\n",
      "Iteration: 2487 Loss: 0.0003804813444357779\n",
      "Iteration: 2488 Loss: 0.00038007926469411164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2489 Loss: 0.0003796776104870539\n",
      "Iteration: 2490 Loss: 0.0003792763813544126\n",
      "Iteration: 2491 Loss: 0.000378875576836648\n",
      "Iteration: 2492 Loss: 0.00037847519647441483\n",
      "Iteration: 2493 Loss: 0.0003780752398096236\n",
      "Iteration: 2494 Loss: 0.0003776757063845352\n",
      "Iteration: 2495 Loss: 0.00037727659574219967\n",
      "Iteration: 2496 Loss: 0.00037687790742641056\n",
      "Iteration: 2497 Loss: 0.0003764796409814091\n",
      "Iteration: 2498 Loss: 0.00037608179595238924\n",
      "Iteration: 2499 Loss: 0.0003756843718850185\n",
      "Iteration: 2500 Loss: 0.00037528736832576466\n",
      "Iteration: 2501 Loss: 0.00037489078482145393\n",
      "Iteration: 2502 Loss: 0.00037449462092000433\n",
      "Iteration: 2503 Loss: 0.0003740988761697044\n",
      "Iteration: 2504 Loss: 0.0003737035501194345\n",
      "Iteration: 2505 Loss: 0.0003733086423189216\n",
      "Iteration: 2506 Loss: 0.0003729141523183486\n",
      "Iteration: 2507 Loss: 0.0003725200796686584\n",
      "Iteration: 2508 Loss: 0.000372126423921389\n",
      "Iteration: 2509 Loss: 0.00037173318462865835\n",
      "Iteration: 2510 Loss: 0.00037134036134330354\n",
      "Iteration: 2511 Loss: 0.00037094795361860167\n",
      "Iteration: 2512 Loss: 0.00037055596100884624\n",
      "Iteration: 2513 Loss: 0.0003701643830683225\n",
      "Iteration: 2514 Loss: 0.00036977321935240115\n",
      "Iteration: 2515 Loss: 0.0003693824694168003\n",
      "Iteration: 2516 Loss: 0.00036899213281811345\n",
      "Iteration: 2517 Loss: 0.00036860220911350764\n",
      "Iteration: 2518 Loss: 0.0003682126978604456\n",
      "Iteration: 2519 Loss: 0.00036782359861717073\n",
      "Iteration: 2520 Loss: 0.00036743491094258485\n",
      "Iteration: 2521 Loss: 0.00036704663439611593\n",
      "Iteration: 2522 Loss: 0.00036665876853775623\n",
      "Iteration: 2523 Loss: 0.0003662713129280688\n",
      "Iteration: 2524 Loss: 0.0003658842671282356\n",
      "Iteration: 2525 Loss: 0.0003654976307000314\n",
      "Iteration: 2526 Loss: 0.00036511140320583677\n",
      "Iteration: 2527 Loss: 0.0003647255842085515\n",
      "Iteration: 2528 Loss: 0.00036434017327150587\n",
      "Iteration: 2529 Loss: 0.000363955169958732\n",
      "Iteration: 2530 Loss: 0.00036357057383494914\n",
      "Iteration: 2531 Loss: 0.00036318638446533266\n",
      "Iteration: 2532 Loss: 0.0003628026014154662\n",
      "Iteration: 2533 Loss: 0.00036241922425162806\n",
      "Iteration: 2534 Loss: 0.00036203625254068144\n",
      "Iteration: 2535 Loss: 0.0003616536858499117\n",
      "Iteration: 2536 Loss: 0.0003612715237473169\n",
      "Iteration: 2537 Loss: 0.00036088976580129784\n",
      "Iteration: 2538 Loss: 0.00036050841158071585\n",
      "Iteration: 2539 Loss: 0.00036012746065525083\n",
      "Iteration: 2540 Loss: 0.00035974691259493314\n",
      "Iteration: 2541 Loss: 0.0003593667669703188\n",
      "Iteration: 2542 Loss: 0.00035898702335262627\n",
      "Iteration: 2543 Loss: 0.00035860768131326356\n",
      "Iteration: 2544 Loss: 0.0003582287404245013\n",
      "Iteration: 2545 Loss: 0.0003578502002590945\n",
      "Iteration: 2546 Loss: 0.00035747206039011425\n",
      "Iteration: 2547 Loss: 0.0003570943203913462\n",
      "Iteration: 2548 Loss: 0.00035671697983704644\n",
      "Iteration: 2549 Loss: 0.00035634003830180706\n",
      "Iteration: 2550 Loss: 0.0003559634953610157\n",
      "Iteration: 2551 Loss: 0.0003555873505903586\n",
      "Iteration: 2552 Loss: 0.0003552116035661588\n",
      "Iteration: 2553 Loss: 0.0003548362538653342\n",
      "Iteration: 2554 Loss: 0.0003544613010647566\n",
      "Iteration: 2555 Loss: 0.0003540867447421391\n",
      "Iteration: 2556 Loss: 0.00035371258447610435\n",
      "Iteration: 2557 Loss: 0.00035333881984535363\n",
      "Iteration: 2558 Loss: 0.00035296545042894735\n",
      "Iteration: 2559 Loss: 0.00035259247580670766\n",
      "Iteration: 2560 Loss: 0.000352219895558872\n",
      "Iteration: 2561 Loss: 0.0003518477092660483\n",
      "Iteration: 2562 Loss: 0.0003514759165095341\n",
      "Iteration: 2563 Loss: 0.00035110451687091025\n",
      "Iteration: 2564 Loss: 0.00035073350993237523\n",
      "Iteration: 2565 Loss: 0.00035036289527654946\n",
      "Iteration: 2566 Loss: 0.000349992672486486\n",
      "Iteration: 2567 Loss: 0.00034962284114579457\n",
      "Iteration: 2568 Loss: 0.00034925340083857305\n",
      "Iteration: 2569 Loss: 0.000348884351149263\n",
      "Iteration: 2570 Loss: 0.000348515691662984\n",
      "Iteration: 2571 Loss: 0.00034814742196508417\n",
      "Iteration: 2572 Loss: 0.00034777954164162515\n",
      "Iteration: 2573 Loss: 0.00034741205027903984\n",
      "Iteration: 2574 Loss: 0.0003470449474641215\n",
      "Iteration: 2575 Loss: 0.0003466782327841561\n",
      "Iteration: 2576 Loss: 0.0003463119058269916\n",
      "Iteration: 2577 Loss: 0.0003459459661809692\n",
      "Iteration: 2578 Loss: 0.0003455804134346518\n",
      "Iteration: 2579 Loss: 0.0003452152471773322\n",
      "Iteration: 2580 Loss: 0.0003448504669988561\n",
      "Iteration: 2581 Loss: 0.00034448607248896835\n",
      "Iteration: 2582 Loss: 0.0003441220632382472\n",
      "Iteration: 2583 Loss: 0.0003437584388379124\n",
      "Iteration: 2584 Loss: 0.0003433951988794104\n",
      "Iteration: 2585 Loss: 0.0003430323429546408\n",
      "Iteration: 2586 Loss: 0.00034266987065591036\n",
      "Iteration: 2587 Loss: 0.00034230778157619973\n",
      "Iteration: 2588 Loss: 0.0003419460753085167\n",
      "Iteration: 2589 Loss: 0.00034158475144695086\n",
      "Iteration: 2590 Loss: 0.00034122380958536287\n",
      "Iteration: 2591 Loss: 0.000340863249318407\n",
      "Iteration: 2592 Loss: 0.00034050307024125425\n",
      "Iteration: 2593 Loss: 0.0003401432719490958\n",
      "Iteration: 2594 Loss: 0.0003397838540384579\n",
      "Iteration: 2595 Loss: 0.0003394248161054927\n",
      "Iteration: 2596 Loss: 0.0003390661577470397\n",
      "Iteration: 2597 Loss: 0.0003387078785603464\n",
      "Iteration: 2598 Loss: 0.0003383499781432514\n",
      "Iteration: 2599 Loss: 0.0003379924560937946\n",
      "Iteration: 2600 Loss: 0.0003376353120106464\n",
      "Iteration: 2601 Loss: 0.0003372785454928899\n",
      "Iteration: 2602 Loss: 0.0003369221561399944\n",
      "Iteration: 2603 Loss: 0.00033656614355188235\n",
      "Iteration: 2604 Loss: 0.000336210507328848\n",
      "Iteration: 2605 Loss: 0.00033585524707175626\n",
      "Iteration: 2606 Loss: 0.00033550036238182764\n",
      "Iteration: 2607 Loss: 0.00033514585286081286\n",
      "Iteration: 2608 Loss: 0.000334791718110761\n",
      "Iteration: 2609 Loss: 0.0003344379577341839\n",
      "Iteration: 2610 Loss: 0.0003340845713343278\n",
      "Iteration: 2611 Loss: 0.00033373155851416214\n",
      "Iteration: 2612 Loss: 0.0003333789188776845\n",
      "Iteration: 2613 Loss: 0.00033302665202898695\n",
      "Iteration: 2614 Loss: 0.0003326747575731984\n",
      "Iteration: 2615 Loss: 0.0003323232351151925\n",
      "Iteration: 2616 Loss: 0.00033197208426055506\n",
      "Iteration: 2617 Loss: 0.0003316213046152949\n",
      "Iteration: 2618 Loss: 0.00033127089578577844\n",
      "Iteration: 2619 Loss: 0.00033092085737889277\n",
      "Iteration: 2620 Loss: 0.00033057118900189225\n",
      "Iteration: 2621 Loss: 0.0003302218902624093\n",
      "Iteration: 2622 Loss: 0.0003298729607686177\n",
      "Iteration: 2623 Loss: 0.00032952440012905894\n",
      "Iteration: 2624 Loss: 0.0003291762079526221\n",
      "Iteration: 2625 Loss: 0.00032882838384881875\n",
      "Iteration: 2626 Loss: 0.00032848092742735505\n",
      "Iteration: 2627 Loss: 0.00032813383829859215\n",
      "Iteration: 2628 Loss: 0.00032778711607306946\n",
      "Iteration: 2629 Loss: 0.00032744076036189505\n",
      "Iteration: 2630 Loss: 0.00032709477077661115\n",
      "Iteration: 2631 Loss: 0.00032674914692910553\n",
      "Iteration: 2632 Loss: 0.0003264038884316284\n",
      "Iteration: 2633 Loss: 0.0003260589948970805\n",
      "Iteration: 2634 Loss: 0.00032571446593856963\n",
      "Iteration: 2635 Loss: 0.00032537030116972215\n",
      "Iteration: 2636 Loss: 0.00032502650020454474\n",
      "Iteration: 2637 Loss: 0.0003246830626574894\n",
      "Iteration: 2638 Loss: 0.00032433998814337744\n",
      "Iteration: 2639 Loss: 0.00032399727627722215\n",
      "Iteration: 2640 Loss: 0.0003236549266750112\n",
      "Iteration: 2641 Loss: 0.00032331293895264656\n",
      "Iteration: 2642 Loss: 0.0003229713127268558\n",
      "Iteration: 2643 Loss: 0.00032263004761465784\n",
      "Iteration: 2644 Loss: 0.0003222891432329185\n",
      "Iteration: 2645 Loss: 0.00032194859919949115\n",
      "Iteration: 2646 Loss: 0.0003216084151327981\n",
      "Iteration: 2647 Loss: 0.0003212685906513177\n",
      "Iteration: 2648 Loss: 0.00032092912537391954\n",
      "Iteration: 2649 Loss: 0.00032059001892010945\n",
      "Iteration: 2650 Loss: 0.00032025127090962825\n",
      "Iteration: 2651 Loss: 0.0003199128809626632\n",
      "Iteration: 2652 Loss: 0.00031957484869986574\n",
      "Iteration: 2653 Loss: 0.0003192371737421957\n",
      "Iteration: 2654 Loss: 0.0003188998557111842\n",
      "Iteration: 2655 Loss: 0.0003185628942285901\n",
      "Iteration: 2656 Loss: 0.0003182262889167304\n",
      "Iteration: 2657 Loss: 0.0003178900393982223\n",
      "Iteration: 2658 Loss: 0.00031755414529619354\n",
      "Iteration: 2659 Loss: 0.0003172186062340242\n",
      "Iteration: 2660 Loss: 0.000316883421835684\n",
      "Iteration: 2661 Loss: 0.0003165485917253057\n",
      "Iteration: 2662 Loss: 0.0003162141155276967\n",
      "Iteration: 2663 Loss: 0.00031587999286787687\n",
      "Iteration: 2664 Loss: 0.0003155462233712131\n",
      "Iteration: 2665 Loss: 0.00031521280666382384\n",
      "Iteration: 2666 Loss: 0.00031487974237187454\n",
      "Iteration: 2667 Loss: 0.0003145470301221277\n",
      "Iteration: 2668 Loss: 0.00031421466954160813\n",
      "Iteration: 2669 Loss: 0.0003138826602578402\n",
      "Iteration: 2670 Loss: 0.0003135510018987617\n",
      "Iteration: 2671 Loss: 0.00031321969409262506\n",
      "Iteration: 2672 Loss: 0.00031288873646812026\n",
      "Iteration: 2673 Loss: 0.00031255812865435984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2674 Loss: 0.00031222787028082596\n",
      "Iteration: 2675 Loss: 0.00031189796097744497\n",
      "Iteration: 2676 Loss: 0.0003115684003744592\n",
      "Iteration: 2677 Loss: 0.00031123918810257143\n",
      "Iteration: 2678 Loss: 0.00031091032379274366\n",
      "Iteration: 2679 Loss: 0.00031058180707667767\n",
      "Iteration: 2680 Loss: 0.0003102536375860925\n",
      "Iteration: 2681 Loss: 0.0003099258149532787\n",
      "Iteration: 2682 Loss: 0.00030959833881088626\n",
      "Iteration: 2683 Loss: 0.0003092712087920104\n",
      "Iteration: 2684 Loss: 0.000308944424530079\n",
      "Iteration: 2685 Loss: 0.00030861798565894933\n",
      "Iteration: 2686 Loss: 0.00030829189181287705\n",
      "Iteration: 2687 Loss: 0.0003079661426264875\n",
      "Iteration: 2688 Loss: 0.00030764073773474556\n",
      "Iteration: 2689 Loss: 0.00030731567677309086\n",
      "Iteration: 2690 Loss: 0.00030699095937742\n",
      "Iteration: 2691 Loss: 0.00030666658518373175\n",
      "Iteration: 2692 Loss: 0.0003063425538287066\n",
      "Iteration: 2693 Loss: 0.00030601886494940287\n",
      "Iteration: 2694 Loss: 0.0003056955181830235\n",
      "Iteration: 2695 Loss: 0.00030537251316737664\n",
      "Iteration: 2696 Loss: 0.0003050498495406348\n",
      "Iteration: 2697 Loss: 0.00030472752694139766\n",
      "Iteration: 2698 Loss: 0.00030440554500851396\n",
      "Iteration: 2699 Loss: 0.00030408390338126066\n",
      "Iteration: 2700 Loss: 0.0003037626016992381\n",
      "Iteration: 2701 Loss: 0.00030344163960261143\n",
      "Iteration: 2702 Loss: 0.00030312101673183207\n",
      "Iteration: 2703 Loss: 0.0003028007327277268\n",
      "Iteration: 2704 Loss: 0.00030248078723160937\n",
      "Iteration: 2705 Loss: 0.0003021611798849903\n",
      "Iteration: 2706 Loss: 0.00030184191032991086\n",
      "Iteration: 2707 Loss: 0.00030152297820870217\n",
      "Iteration: 2708 Loss: 0.00030120438316421207\n",
      "Iteration: 2709 Loss: 0.00030088612483946184\n",
      "Iteration: 2710 Loss: 0.0003005682028781\n",
      "Iteration: 2711 Loss: 0.00030025061692393696\n",
      "Iteration: 2712 Loss: 0.00029993336662141774\n",
      "Iteration: 2713 Loss: 0.0002996164516150505\n",
      "Iteration: 2714 Loss: 0.00029929987154990776\n",
      "Iteration: 2715 Loss: 0.00029898362607146806\n",
      "Iteration: 2716 Loss: 0.00029866771482553716\n",
      "Iteration: 2717 Loss: 0.00029835213745832623\n",
      "Iteration: 2718 Loss: 0.00029803689361635664\n",
      "Iteration: 2719 Loss: 0.00029772198294655247\n",
      "Iteration: 2720 Loss: 0.00029740740509628455\n",
      "Iteration: 2721 Loss: 0.00029709315971321147\n",
      "Iteration: 2722 Loss: 0.0002967792464454427\n",
      "Iteration: 2723 Loss: 0.00029646566494143247\n",
      "Iteration: 2724 Loss: 0.00029615241484994063\n",
      "Iteration: 2725 Loss: 0.0002958394958203634\n",
      "Iteration: 2726 Loss: 0.00029552690750206644\n",
      "Iteration: 2727 Loss: 0.0002952146495450465\n",
      "Iteration: 2728 Loss: 0.0002949027215997916\n",
      "Iteration: 2729 Loss: 0.0002945911233168316\n",
      "Iteration: 2730 Loss: 0.0002942798543472896\n",
      "Iteration: 2731 Loss: 0.00029396891434258377\n",
      "Iteration: 2732 Loss: 0.0002936583029545186\n",
      "Iteration: 2733 Loss: 0.0002933480198354659\n",
      "Iteration: 2734 Loss: 0.00029303806463776814\n",
      "Iteration: 2735 Loss: 0.00029272843701446576\n",
      "Iteration: 2736 Loss: 0.00029241913661885526\n",
      "Iteration: 2737 Loss: 0.0002921101631046505\n",
      "Iteration: 2738 Loss: 0.000291801516125677\n",
      "Iteration: 2739 Loss: 0.00029149319533661747\n",
      "Iteration: 2740 Loss: 0.00029118520039199516\n",
      "Iteration: 2741 Loss: 0.00029087753094745325\n",
      "Iteration: 2742 Loss: 0.00029057018665805867\n",
      "Iteration: 2743 Loss: 0.0002902631671798668\n",
      "Iteration: 2744 Loss: 0.00028995647216907593\n",
      "Iteration: 2745 Loss: 0.0002896501012823592\n",
      "Iteration: 2746 Loss: 0.0002893440541769639\n",
      "Iteration: 2747 Loss: 0.00028903833050973685\n",
      "Iteration: 2748 Loss: 0.0002887329299386968\n",
      "Iteration: 2749 Loss: 0.00028842785212179376\n",
      "Iteration: 2750 Loss: 0.00028812309671754754\n",
      "Iteration: 2751 Loss: 0.0002878186633848776\n",
      "Iteration: 2752 Loss: 0.0002875145517828638\n",
      "Iteration: 2753 Loss: 0.0002872107615710568\n",
      "Iteration: 2754 Loss: 0.000286907292409401\n",
      "Iteration: 2755 Loss: 0.0002866041439581243\n",
      "Iteration: 2756 Loss: 0.00028630131587781857\n",
      "Iteration: 2757 Loss: 0.00028599880782986207\n",
      "Iteration: 2758 Loss: 0.00028569661947507865\n",
      "Iteration: 2759 Loss: 0.0002853947504754004\n",
      "Iteration: 2760 Loss: 0.00028509320049271904\n",
      "Iteration: 2761 Loss: 0.000284791969189884\n",
      "Iteration: 2762 Loss: 0.00028449105622946146\n",
      "Iteration: 2763 Loss: 0.00028419046127468775\n",
      "Iteration: 2764 Loss: 0.000283890183988978\n",
      "Iteration: 2765 Loss: 0.00028359022403631317\n",
      "Iteration: 2766 Loss: 0.00028329058108085125\n",
      "Iteration: 2767 Loss: 0.0002829912547873792\n",
      "Iteration: 2768 Loss: 0.00028269224482061557\n",
      "Iteration: 2769 Loss: 0.00028239355084589893\n",
      "Iteration: 2770 Loss: 0.00028209517252902427\n",
      "Iteration: 2771 Loss: 0.0002817971095358627\n",
      "Iteration: 2772 Loss: 0.0002814993615329238\n",
      "Iteration: 2773 Loss: 0.00028120192818684425\n",
      "Iteration: 2774 Loss: 0.00028090480916479545\n",
      "Iteration: 2775 Loss: 0.00028060800413421446\n",
      "Iteration: 2776 Loss: 0.0002803115127627156\n",
      "Iteration: 2777 Loss: 0.00028001533471877125\n",
      "Iteration: 2778 Loss: 0.00027971946967069063\n",
      "Iteration: 2779 Loss: 0.0002794239172874456\n",
      "Iteration: 2780 Loss: 0.0002791286772381705\n",
      "Iteration: 2781 Loss: 0.0002788337491925389\n",
      "Iteration: 2782 Loss: 0.00027853913282026933\n",
      "Iteration: 2783 Loss: 0.00027824482779180757\n",
      "Iteration: 2784 Loss: 0.0002779508337777344\n",
      "Iteration: 2785 Loss: 0.0002776571504490146\n",
      "Iteration: 2786 Loss: 0.0002773637774768455\n",
      "Iteration: 2787 Loss: 0.0002770707145330715\n",
      "Iteration: 2788 Loss: 0.00027677796128970606\n",
      "Iteration: 2789 Loss: 0.0002764855174190014\n",
      "Iteration: 2790 Loss: 0.00027619338259377214\n",
      "Iteration: 2791 Loss: 0.0002759015564869988\n",
      "Iteration: 2792 Loss: 0.00027561003877220184\n",
      "Iteration: 2793 Loss: 0.0002753188291231855\n",
      "Iteration: 2794 Loss: 0.0002750279272139324\n",
      "Iteration: 2795 Loss: 0.0002747373327190313\n",
      "Iteration: 2796 Loss: 0.00027444704531325384\n",
      "Iteration: 2797 Loss: 0.0002741570646716472\n",
      "Iteration: 2798 Loss: 0.00027386739046980234\n",
      "Iteration: 2799 Loss: 0.00027357802238356697\n",
      "Iteration: 2800 Loss: 0.00027328896008919006\n",
      "Iteration: 2801 Loss: 0.0002730002032630407\n",
      "Iteration: 2802 Loss: 0.00027271175158211406\n",
      "Iteration: 2803 Loss: 0.0002724236047236653\n",
      "Iteration: 2804 Loss: 0.0002721357623652244\n",
      "Iteration: 2805 Loss: 0.00027184822418463135\n",
      "Iteration: 2806 Loss: 0.00027156098986018645\n",
      "Iteration: 2807 Loss: 0.0002712740590704911\n",
      "Iteration: 2808 Loss: 0.0002709874314944785\n",
      "Iteration: 2809 Loss: 0.0002707011068114679\n",
      "Iteration: 2810 Loss: 0.0002704150847010672\n",
      "Iteration: 2811 Loss: 0.00027012936484320147\n",
      "Iteration: 2812 Loss: 0.00026984394691822875\n",
      "Iteration: 2813 Loss: 0.00026955883060671436\n",
      "Iteration: 2814 Loss: 0.0002692740155896336\n",
      "Iteration: 2815 Loss: 0.0002689895015484144\n",
      "Iteration: 2816 Loss: 0.00026870528816464557\n",
      "Iteration: 2817 Loss: 0.00026842137512039587\n",
      "Iteration: 2818 Loss: 0.0002681377620979454\n",
      "Iteration: 2819 Loss: 0.00026785444877992743\n",
      "Iteration: 2820 Loss: 0.00026757143484938076\n",
      "Iteration: 2821 Loss: 0.00026728871998964486\n",
      "Iteration: 2822 Loss: 0.00026700630388453545\n",
      "Iteration: 2823 Loss: 0.00026672418621825433\n",
      "Iteration: 2824 Loss: 0.0002664423666746828\n",
      "Iteration: 2825 Loss: 0.0002661608449383748\n",
      "Iteration: 2826 Loss: 0.00026587962069487674\n",
      "Iteration: 2827 Loss: 0.0002655986936294804\n",
      "Iteration: 2828 Loss: 0.00026531806342778766\n",
      "Iteration: 2829 Loss: 0.0002650377297758032\n",
      "Iteration: 2830 Loss: 0.0002647576923599614\n",
      "Iteration: 2831 Loss: 0.00026447795086687243\n",
      "Iteration: 2832 Loss: 0.0002641985049836654\n",
      "Iteration: 2833 Loss: 0.000263919354397893\n",
      "Iteration: 2834 Loss: 0.00026364049879659907\n",
      "Iteration: 2835 Loss: 0.0002633619378683024\n",
      "Iteration: 2836 Loss: 0.0002630836713014349\n",
      "Iteration: 2837 Loss: 0.000262805698784595\n",
      "Iteration: 2838 Loss: 0.0002625280200067546\n",
      "Iteration: 2839 Loss: 0.00026225063465729314\n",
      "Iteration: 2840 Loss: 0.00026197354242600026\n",
      "Iteration: 2841 Loss: 0.0002616967430028378\n",
      "Iteration: 2842 Loss: 0.00026142023607799835\n",
      "Iteration: 2843 Loss: 0.00026114402134234276\n",
      "Iteration: 2844 Loss: 0.0002608680984867853\n",
      "Iteration: 2845 Loss: 0.0002605924672026675\n",
      "Iteration: 2846 Loss: 0.00026031712718172287\n",
      "Iteration: 2847 Loss: 0.0002600420781158664\n",
      "Iteration: 2848 Loss: 0.00025976731969739095\n",
      "Iteration: 2849 Loss: 0.0002594928516190704\n",
      "Iteration: 2850 Loss: 0.0002592186735737076\n",
      "Iteration: 2851 Loss: 0.00025894478525462434\n",
      "Iteration: 2852 Loss: 0.0002586711863554548\n",
      "Iteration: 2853 Loss: 0.0002583978765702278\n",
      "Iteration: 2854 Loss: 0.0002581248555931977\n",
      "Iteration: 2855 Loss: 0.00025785212311890877\n",
      "Iteration: 2856 Loss: 0.0002575796788423075\n",
      "Iteration: 2857 Loss: 0.00025730752245871076\n",
      "Iteration: 2858 Loss: 0.0002570356536635851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2859 Loss: 0.00025676407215275316\n",
      "Iteration: 2860 Loss: 0.00025649277762247025\n",
      "Iteration: 2861 Loss: 0.0002562217697694829\n",
      "Iteration: 2862 Loss: 0.0002559510482904813\n",
      "Iteration: 2863 Loss: 0.0002556806128826735\n",
      "Iteration: 2864 Loss: 0.0002554104632435861\n",
      "Iteration: 2865 Loss: 0.00025514059907094476\n",
      "Iteration: 2866 Loss: 0.00025487102006305917\n",
      "Iteration: 2867 Loss: 0.00025460172591817987\n",
      "Iteration: 2868 Loss: 0.00025433271633530543\n",
      "Iteration: 2869 Loss: 0.0002540639910133834\n",
      "Iteration: 2870 Loss: 0.00025379554965200644\n",
      "Iteration: 2871 Loss: 0.0002535273919507425\n",
      "Iteration: 2872 Loss: 0.0002532595176097852\n",
      "Iteration: 2873 Loss: 0.00025299192632953623\n",
      "Iteration: 2874 Loss: 0.00025272461781056525\n",
      "Iteration: 2875 Loss: 0.00025245759175398765\n",
      "Iteration: 2876 Loss: 0.0002521908478611745\n",
      "Iteration: 2877 Loss: 0.00025192438583373204\n",
      "Iteration: 2878 Loss: 0.00025165820537356937\n",
      "Iteration: 2879 Loss: 0.0002513923061829522\n",
      "Iteration: 2880 Loss: 0.0002511266879647031\n",
      "Iteration: 2881 Loss: 0.000250861350421538\n",
      "Iteration: 2882 Loss: 0.0002505962932566816\n",
      "Iteration: 2883 Loss: 0.0002503315161738183\n",
      "Iteration: 2884 Loss: 0.0002500670188768833\n",
      "Iteration: 2885 Loss: 0.0002498028010697545\n",
      "Iteration: 2886 Loss: 0.000249538862457129\n",
      "Iteration: 2887 Loss: 0.0002492752027436719\n",
      "Iteration: 2888 Loss: 0.00024901182163456474\n",
      "Iteration: 2889 Loss: 0.00024874871883534387\n",
      "Iteration: 2890 Loss: 0.0002484858940516707\n",
      "Iteration: 2891 Loss: 0.0002482233469896064\n",
      "Iteration: 2892 Loss: 0.00024796107735554167\n",
      "Iteration: 2893 Loss: 0.0002476990848561324\n",
      "Iteration: 2894 Loss: 0.0002474373691984388\n",
      "Iteration: 2895 Loss: 0.0002471759300896583\n",
      "Iteration: 2896 Loss: 0.00024691476723750377\n",
      "Iteration: 2897 Loss: 0.0002466538803497796\n",
      "Iteration: 2898 Loss: 0.00024639326913477715\n",
      "Iteration: 2899 Loss: 0.0002461329333010802\n",
      "Iteration: 2900 Loss: 0.00024587287255751854\n",
      "Iteration: 2901 Loss: 0.00024561308661321627\n",
      "Iteration: 2902 Loss: 0.00024535357517774736\n",
      "Iteration: 2903 Loss: 0.0002450943379606222\n",
      "Iteration: 2904 Loss: 0.0002448353746721542\n",
      "Iteration: 2905 Loss: 0.00024457668502284843\n",
      "Iteration: 2906 Loss: 0.000244318268723376\n",
      "Iteration: 2907 Loss: 0.0002440601254845224\n",
      "Iteration: 2908 Loss: 0.0002438022550176748\n",
      "Iteration: 2909 Loss: 0.00024354465703451428\n",
      "Iteration: 2910 Loss: 0.00024328733124687\n",
      "Iteration: 2911 Loss: 0.00024303027736718015\n",
      "Iteration: 2912 Loss: 0.0002427734951077095\n",
      "Iteration: 2913 Loss: 0.00024251698418144868\n",
      "Iteration: 2914 Loss: 0.00024226074430162275\n",
      "Iteration: 2915 Loss: 0.00024200477518154686\n",
      "Iteration: 2916 Loss: 0.0002417490765350303\n",
      "Iteration: 2917 Loss: 0.0002414936480761434\n",
      "Iteration: 2918 Loss: 0.0002412384895191991\n",
      "Iteration: 2919 Loss: 0.0002409836005788704\n",
      "Iteration: 2920 Loss: 0.00024072898097015784\n",
      "Iteration: 2921 Loss: 0.00024047463040834376\n",
      "Iteration: 2922 Loss: 0.00024022054860907736\n",
      "Iteration: 2923 Loss: 0.00023996673528805688\n",
      "Iteration: 2924 Loss: 0.0002397131901615309\n",
      "Iteration: 2925 Loss: 0.0002394599129459935\n",
      "Iteration: 2926 Loss: 0.00023920690335817066\n",
      "Iteration: 2927 Loss: 0.0002389541611152376\n",
      "Iteration: 2928 Loss: 0.00023870168593442904\n",
      "Iteration: 2929 Loss: 0.00023844947753358462\n",
      "Iteration: 2930 Loss: 0.00023819753563054716\n",
      "Iteration: 2931 Loss: 0.00023794585994361844\n",
      "Iteration: 2932 Loss: 0.00023769445019141953\n",
      "Iteration: 2933 Loss: 0.00023744330609288346\n",
      "Iteration: 2934 Loss: 0.0002371924273670484\n",
      "Iteration: 2935 Loss: 0.00023694181373342367\n",
      "Iteration: 2936 Loss: 0.0002366914649118112\n",
      "Iteration: 2937 Loss: 0.00023644138062214965\n",
      "Iteration: 2938 Loss: 0.00023619156058494705\n",
      "Iteration: 2939 Loss: 0.0002359420045208244\n",
      "Iteration: 2940 Loss: 0.00023569271215063387\n",
      "Iteration: 2941 Loss: 0.0002354436831957533\n",
      "Iteration: 2942 Loss: 0.00023519491737769007\n",
      "Iteration: 2943 Loss: 0.00023494641441818974\n",
      "Iteration: 2944 Loss: 0.00023469817403953054\n",
      "Iteration: 2945 Loss: 0.0002344501959640799\n",
      "Iteration: 2946 Loss: 0.00023420247991456445\n",
      "Iteration: 2947 Loss: 0.00023395502561396214\n",
      "Iteration: 2948 Loss: 0.0002337078327855644\n",
      "Iteration: 2949 Loss: 0.00023346090115307775\n",
      "Iteration: 2950 Loss: 0.0002332142304403075\n",
      "Iteration: 2951 Loss: 0.00023296782037151139\n",
      "Iteration: 2952 Loss: 0.00023272167067102682\n",
      "Iteration: 2953 Loss: 0.000232475781063813\n",
      "Iteration: 2954 Loss: 0.00023223015127483006\n",
      "Iteration: 2955 Loss: 0.0002319847810294434\n",
      "Iteration: 2956 Loss: 0.00023173967005333634\n",
      "Iteration: 2957 Loss: 0.0002314948180724359\n",
      "Iteration: 2958 Loss: 0.00023125022481293073\n",
      "Iteration: 2959 Loss: 0.00023100589000132107\n",
      "Iteration: 2960 Loss: 0.00023076181336442386\n",
      "Iteration: 2961 Loss: 0.00023051799462946748\n",
      "Iteration: 2962 Loss: 0.00023027443352370062\n",
      "Iteration: 2963 Loss: 0.00023003112977490383\n",
      "Iteration: 2964 Loss: 0.00022978808311097313\n",
      "Iteration: 2965 Loss: 0.00022954529326014105\n",
      "Iteration: 2966 Loss: 0.00022930275995089924\n",
      "Iteration: 2967 Loss: 0.00022906048291218402\n",
      "Iteration: 2968 Loss: 0.0002288184618730792\n",
      "Iteration: 2969 Loss: 0.00022857669656294968\n",
      "Iteration: 2970 Loss: 0.00022833518671145972\n",
      "Iteration: 2971 Loss: 0.00022809393204878463\n",
      "Iteration: 2972 Loss: 0.00022785293230500852\n",
      "Iteration: 2973 Loss: 0.0002276121872107127\n",
      "Iteration: 2974 Loss: 0.00022737169649667083\n",
      "Iteration: 2975 Loss: 0.0002271314598941223\n",
      "Iteration: 2976 Loss: 0.0002268914771344117\n",
      "Iteration: 2977 Loss: 0.00022665174794923572\n",
      "Iteration: 2978 Loss: 0.00022641227207051412\n",
      "Iteration: 2979 Loss: 0.00022617304923050902\n",
      "Iteration: 2980 Loss: 0.00022593407916188832\n",
      "Iteration: 2981 Loss: 0.00022569536159739093\n",
      "Iteration: 2982 Loss: 0.00022545689627004697\n",
      "Iteration: 2983 Loss: 0.0002252186829133115\n",
      "Iteration: 2984 Loss: 0.00022498072126079206\n",
      "Iteration: 2985 Loss: 0.00022474301104653498\n",
      "Iteration: 2986 Loss: 0.00022450555200472611\n",
      "Iteration: 2987 Loss: 0.0002242683438699769\n",
      "Iteration: 2988 Loss: 0.0002240313863769975\n",
      "Iteration: 2989 Loss: 0.0002237946792608473\n",
      "Iteration: 2990 Loss: 0.00022355822225686957\n",
      "Iteration: 2991 Loss: 0.00022332201510078664\n",
      "Iteration: 2992 Loss: 0.00022308605752845116\n",
      "Iteration: 2993 Loss: 0.00022285034927616024\n",
      "Iteration: 2994 Loss: 0.00022261489008023222\n",
      "Iteration: 2995 Loss: 0.00022237967967749387\n",
      "Iteration: 2996 Loss: 0.00022214471780508004\n",
      "Iteration: 2997 Loss: 0.00022191000420016576\n",
      "Iteration: 2998 Loss: 0.00022167553860041696\n",
      "Iteration: 2999 Loss: 0.00022144132074373074\n",
      "Iteration: 3000 Loss: 0.00022120735036815234\n",
      "Iteration: 3001 Loss: 0.00022097362721221293\n",
      "Iteration: 3002 Loss: 0.00022074015101459645\n",
      "Iteration: 3003 Loss: 0.00022050692151425147\n",
      "Iteration: 3004 Loss: 0.00022027393845040405\n",
      "Iteration: 3005 Loss: 0.00022004120156256875\n",
      "Iteration: 3006 Loss: 0.00021980871059060962\n",
      "Iteration: 3007 Loss: 0.000219576465274621\n",
      "Iteration: 3008 Loss: 0.0002193444653549754\n",
      "Iteration: 3009 Loss: 0.00021911271057236222\n",
      "Iteration: 3010 Loss: 0.00021888120066754002\n",
      "Iteration: 3011 Loss: 0.00021864993538174066\n",
      "Iteration: 3012 Loss: 0.0002184189144565446\n",
      "Iteration: 3013 Loss: 0.00021818813763350238\n",
      "Iteration: 3014 Loss: 0.00021795760465466546\n",
      "Iteration: 3015 Loss: 0.00021772731526237\n",
      "Iteration: 3016 Loss: 0.00021749726919935924\n",
      "Iteration: 3017 Loss: 0.00021726746620795895\n",
      "Iteration: 3018 Loss: 0.00021703790603159196\n",
      "Iteration: 3019 Loss: 0.00021680858841354182\n",
      "Iteration: 3020 Loss: 0.00021657951309748665\n",
      "Iteration: 3021 Loss: 0.00021635067982726566\n",
      "Iteration: 3022 Loss: 0.00021612208834717936\n",
      "Iteration: 3023 Loss: 0.0002158937384016083\n",
      "Iteration: 3024 Loss: 0.00021566562973523526\n",
      "Iteration: 3025 Loss: 0.0002154377620931386\n",
      "Iteration: 3026 Loss: 0.00021521013522057285\n",
      "Iteration: 3027 Loss: 0.0002149827488629778\n",
      "Iteration: 3028 Loss: 0.00021475560276648518\n",
      "Iteration: 3029 Loss: 0.00021452869667652218\n",
      "Iteration: 3030 Loss: 0.0002143020303400111\n",
      "Iteration: 3031 Loss: 0.00021407560350334293\n",
      "Iteration: 3032 Loss: 0.0002138494159133762\n",
      "Iteration: 3033 Loss: 0.0002136234673173611\n",
      "Iteration: 3034 Loss: 0.0002133977574625916\n",
      "Iteration: 3035 Loss: 0.00021317228609678762\n",
      "Iteration: 3036 Loss: 0.0002129470529680802\n",
      "Iteration: 3037 Loss: 0.0002127220578242119\n",
      "Iteration: 3038 Loss: 0.0002124973004139301\n",
      "Iteration: 3039 Loss: 0.00021227278048618016\n",
      "Iteration: 3040 Loss: 0.00021204849778985429\n",
      "Iteration: 3041 Loss: 0.00021182445207411246\n",
      "Iteration: 3042 Loss: 0.00021160064308859477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3043 Loss: 0.00021137707058312374\n",
      "Iteration: 3044 Loss: 0.00021115373430766903\n",
      "Iteration: 3045 Loss: 0.00021093063401266075\n",
      "Iteration: 3046 Loss: 0.0002107077694486764\n",
      "Iteration: 3047 Loss: 0.00021048514036662075\n",
      "Iteration: 3048 Loss: 0.00021026274651747795\n",
      "Iteration: 3049 Loss: 0.00021004058765275406\n",
      "Iteration: 3050 Loss: 0.00020981866352414718\n",
      "Iteration: 3051 Loss: 0.0002095969738834808\n",
      "Iteration: 3052 Loss: 0.0002093755184829258\n",
      "Iteration: 3053 Loss: 0.00020915429707501202\n",
      "Iteration: 3054 Loss: 0.00020893330941241449\n",
      "Iteration: 3055 Loss: 0.00020871255524817342\n",
      "Iteration: 3056 Loss: 0.0002084920343354447\n",
      "Iteration: 3057 Loss: 0.00020827174642768314\n",
      "Iteration: 3058 Loss: 0.00020805169127871857\n",
      "Iteration: 3059 Loss: 0.0002078318686424628\n",
      "Iteration: 3060 Loss: 0.00020761227827323004\n",
      "Iteration: 3061 Loss: 0.0002073929199253635\n",
      "Iteration: 3062 Loss: 0.00020717379335429752\n",
      "Iteration: 3063 Loss: 0.00020695489831438417\n",
      "Iteration: 3064 Loss: 0.00020673623456131916\n",
      "Iteration: 3065 Loss: 0.00020651780185048177\n",
      "Iteration: 3066 Loss: 0.0002062995999378141\n",
      "Iteration: 3067 Loss: 0.0002060816285793426\n",
      "Iteration: 3068 Loss: 0.00020586388753138343\n",
      "Iteration: 3069 Loss: 0.0002056463765506348\n",
      "Iteration: 3070 Loss: 0.00020542909539391044\n",
      "Iteration: 3071 Loss: 0.00020521204381825528\n",
      "Iteration: 3072 Loss: 0.00020499522158117368\n",
      "Iteration: 3073 Loss: 0.00020477862844022003\n",
      "Iteration: 3074 Loss: 0.00020456226415329427\n",
      "Iteration: 3075 Loss: 0.00020434612847846918\n",
      "Iteration: 3076 Loss: 0.0002041302211742886\n",
      "Iteration: 3077 Loss: 0.00020391454199939824\n",
      "Iteration: 3078 Loss: 0.00020369909071276423\n",
      "Iteration: 3079 Loss: 0.00020348386707333424\n",
      "Iteration: 3080 Loss: 0.00020326887084055905\n",
      "Iteration: 3081 Loss: 0.00020305410177423301\n",
      "Iteration: 3082 Loss: 0.00020283955963418555\n",
      "Iteration: 3083 Loss: 0.00020262524418077142\n",
      "Iteration: 3084 Loss: 0.0002024111551742422\n",
      "Iteration: 3085 Loss: 0.00020219729237535314\n",
      "Iteration: 3086 Loss: 0.00020198365554510196\n",
      "Iteration: 3087 Loss: 0.00020177024444458842\n",
      "Iteration: 3088 Loss: 0.0002015570588351909\n",
      "Iteration: 3089 Loss: 0.0002013440984789417\n",
      "Iteration: 3090 Loss: 0.0002011313631375265\n",
      "Iteration: 3091 Loss: 0.00020091885257329185\n",
      "Iteration: 3092 Loss: 0.00020070656654860546\n",
      "Iteration: 3093 Loss: 0.00020049450482621459\n",
      "Iteration: 3094 Loss: 0.000200282667168987\n",
      "Iteration: 3095 Loss: 0.0002000710533401863\n",
      "Iteration: 3096 Loss: 0.0001998596631033702\n",
      "Iteration: 3097 Loss: 0.00019964849622213762\n",
      "Iteration: 3098 Loss: 0.00019943755246049376\n",
      "Iteration: 3099 Loss: 0.0001992268315826106\n",
      "Iteration: 3100 Loss: 0.0001990163333530112\n",
      "Iteration: 3101 Loss: 0.00019880605753634848\n",
      "Iteration: 3102 Loss: 0.00019859600389767927\n",
      "Iteration: 3103 Loss: 0.00019838617220216856\n",
      "Iteration: 3104 Loss: 0.00019817656221515737\n",
      "Iteration: 3105 Loss: 0.0001979671737024437\n",
      "Iteration: 3106 Loss: 0.00019775800643004958\n",
      "Iteration: 3107 Loss: 0.00019754906016404244\n",
      "Iteration: 3108 Loss: 0.00019734033467092206\n",
      "Iteration: 3109 Loss: 0.0001971318297174673\n",
      "Iteration: 3110 Loss: 0.0001969235450705603\n",
      "Iteration: 3111 Loss: 0.00019671548049728655\n",
      "Iteration: 3112 Loss: 0.0001965076357651653\n",
      "Iteration: 3113 Loss: 0.00019630001064185792\n",
      "Iteration: 3114 Loss: 0.00019609260489532763\n",
      "Iteration: 3115 Loss: 0.0001958854182937504\n",
      "Iteration: 3116 Loss: 0.00019567845060557132\n",
      "Iteration: 3117 Loss: 0.00019547170159926356\n",
      "Iteration: 3118 Loss: 0.0001952651710439625\n",
      "Iteration: 3119 Loss: 0.00019505885870868268\n",
      "Iteration: 3120 Loss: 0.0001948527643628323\n",
      "Iteration: 3121 Loss: 0.00019464688777609494\n",
      "Iteration: 3122 Loss: 0.0001944412287185813\n",
      "Iteration: 3123 Loss: 0.00019423578695977936\n",
      "Iteration: 3124 Loss: 0.0001940305622706179\n",
      "Iteration: 3125 Loss: 0.00019382555442160656\n",
      "Iteration: 3126 Loss: 0.00019362076318343388\n",
      "Iteration: 3127 Loss: 0.000193416188327302\n",
      "Iteration: 3128 Loss: 0.00019321182962463894\n",
      "Iteration: 3129 Loss: 0.00019300768684695285\n",
      "Iteration: 3130 Loss: 0.00019280375976603293\n",
      "Iteration: 3131 Loss: 0.0001926000481539329\n",
      "Iteration: 3132 Loss: 0.0001923965517829576\n",
      "Iteration: 3133 Loss: 0.00019219327042561426\n",
      "Iteration: 3134 Loss: 0.00019199020385484689\n",
      "Iteration: 3135 Loss: 0.0001917873518435906\n",
      "Iteration: 3136 Loss: 0.0001915847141650906\n",
      "Iteration: 3137 Loss: 0.00019138229059290017\n",
      "Iteration: 3138 Loss: 0.00019118008090069665\n",
      "Iteration: 3139 Loss: 0.00019097808486263988\n",
      "Iteration: 3140 Loss: 0.00019077630225267472\n",
      "Iteration: 3141 Loss: 0.00019057473284546538\n",
      "Iteration: 3142 Loss: 0.0001903733764158283\n",
      "Iteration: 3143 Loss: 0.00019017223273843037\n",
      "Iteration: 3144 Loss: 0.00018997130158850486\n",
      "Iteration: 3145 Loss: 0.00018977058274132115\n",
      "Iteration: 3146 Loss: 0.00018957007597297722\n",
      "Iteration: 3147 Loss: 0.00018936978105907483\n",
      "Iteration: 3148 Loss: 0.00018916969777575392\n",
      "Iteration: 3149 Loss: 0.000188969825899495\n",
      "Iteration: 3150 Loss: 0.00018877016520674542\n",
      "Iteration: 3151 Loss: 0.00018857071547439495\n",
      "Iteration: 3152 Loss: 0.00018837147647959247\n",
      "Iteration: 3153 Loss: 0.00018817244799971774\n",
      "Iteration: 3154 Loss: 0.00018797362981202683\n",
      "Iteration: 3155 Loss: 0.00018777502169444927\n",
      "Iteration: 3156 Loss: 0.0001875766234249897\n",
      "Iteration: 3157 Loss: 0.00018737843478198364\n",
      "Iteration: 3158 Loss: 0.00018718045554361597\n",
      "Iteration: 3159 Loss: 0.0001869826854890495\n",
      "Iteration: 3160 Loss: 0.00018678512439709096\n",
      "Iteration: 3161 Loss: 0.00018658777204673234\n",
      "Iteration: 3162 Loss: 0.00018639062821763415\n",
      "Iteration: 3163 Loss: 0.00018619369268928923\n",
      "Iteration: 3164 Loss: 0.00018599696524171464\n",
      "Iteration: 3165 Loss: 0.00018580044565491986\n",
      "Iteration: 3166 Loss: 0.00018560413370930488\n",
      "Iteration: 3167 Loss: 0.000185408029185438\n",
      "Iteration: 3168 Loss: 0.0001852121318640894\n",
      "Iteration: 3169 Loss: 0.00018501644152626236\n",
      "Iteration: 3170 Loss: 0.00018482095795343235\n",
      "Iteration: 3171 Loss: 0.00018462568092693891\n",
      "Iteration: 3172 Loss: 0.00018443061022874397\n",
      "Iteration: 3173 Loss: 0.00018423574564061112\n",
      "Iteration: 3174 Loss: 0.00018404108694475217\n",
      "Iteration: 3175 Loss: 0.00018384663392370472\n",
      "Iteration: 3176 Loss: 0.00018365238636004893\n",
      "Iteration: 3177 Loss: 0.00018345834403685627\n",
      "Iteration: 3178 Loss: 0.00018326450673690974\n",
      "Iteration: 3179 Loss: 0.0001830708742437877\n",
      "Iteration: 3180 Loss: 0.00018287744634098543\n",
      "Iteration: 3181 Loss: 0.00018268422281241632\n",
      "Iteration: 3182 Loss: 0.000182491203442017\n",
      "Iteration: 3183 Loss: 0.00018229838801410396\n",
      "Iteration: 3184 Loss: 0.00018210577631318505\n",
      "Iteration: 3185 Loss: 0.00018191336812399528\n",
      "Iteration: 3186 Loss: 0.00018172116323119164\n",
      "Iteration: 3187 Loss: 0.00018152916142051206\n",
      "Iteration: 3188 Loss: 0.00018133736247695533\n",
      "Iteration: 3189 Loss: 0.00018114576618634263\n",
      "Iteration: 3190 Loss: 0.00018095437233448726\n",
      "Iteration: 3191 Loss: 0.00018076318070739877\n",
      "Iteration: 3192 Loss: 0.00018057219109138763\n",
      "Iteration: 3193 Loss: 0.00018038140327305164\n",
      "Iteration: 3194 Loss: 0.0001801908170391379\n",
      "Iteration: 3195 Loss: 0.00018000043217671253\n",
      "Iteration: 3196 Loss: 0.0001798102484728576\n",
      "Iteration: 3197 Loss: 0.00017962026571511372\n",
      "Iteration: 3198 Loss: 0.00017943048369103848\n",
      "Iteration: 3199 Loss: 0.00017924090218859428\n",
      "Iteration: 3200 Loss: 0.00017905152099591067\n",
      "Iteration: 3201 Loss: 0.00017886233990129598\n",
      "Iteration: 3202 Loss: 0.00017867335869334926\n",
      "Iteration: 3203 Loss: 0.00017848457716074733\n",
      "Iteration: 3204 Loss: 0.000178295995092632\n",
      "Iteration: 3205 Loss: 0.0001781076122781129\n",
      "Iteration: 3206 Loss: 0.00017791942850674792\n",
      "Iteration: 3207 Loss: 0.0001777314435681435\n",
      "Iteration: 3208 Loss: 0.00017754365725219698\n",
      "Iteration: 3209 Loss: 0.00017735606934909611\n",
      "Iteration: 3210 Loss: 0.00017716867964912814\n",
      "Iteration: 3211 Loss: 0.00017698148794288666\n",
      "Iteration: 3212 Loss: 0.00017679449402110154\n",
      "Iteration: 3213 Loss: 0.00017660769767476574\n",
      "Iteration: 3214 Loss: 0.00017642109869524063\n",
      "Iteration: 3215 Loss: 0.00017623469687399465\n",
      "Iteration: 3216 Loss: 0.00017604849200253842\n",
      "Iteration: 3217 Loss: 0.00017586248387274406\n",
      "Iteration: 3218 Loss: 0.00017567667227682572\n",
      "Iteration: 3219 Loss: 0.00017549105700705167\n",
      "Iteration: 3220 Loss: 0.00017530563785603535\n",
      "Iteration: 3221 Loss: 0.00017512041461651798\n",
      "Iteration: 3222 Loss: 0.000174935387081478\n",
      "Iteration: 3223 Loss: 0.00017475055504405482\n",
      "Iteration: 3224 Loss: 0.00017456591829776875\n",
      "Iteration: 3225 Loss: 0.00017438147663630858\n",
      "Iteration: 3226 Loss: 0.00017419722985337384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3227 Loss: 0.00017401317774336036\n",
      "Iteration: 3228 Loss: 0.0001738293201001707\n",
      "Iteration: 3229 Loss: 0.00017364565671843288\n",
      "Iteration: 3230 Loss: 0.00017346218739274324\n",
      "Iteration: 3231 Loss: 0.00017327891191851625\n",
      "Iteration: 3232 Loss: 0.00017309583009070752\n",
      "Iteration: 3233 Loss: 0.00017291294170456069\n",
      "Iteration: 3234 Loss: 0.00017273024655569343\n",
      "Iteration: 3235 Loss: 0.00017254774443999982\n",
      "Iteration: 3236 Loss: 0.00017236543515350997\n",
      "Iteration: 3237 Loss: 0.00017218331849232907\n",
      "Iteration: 3238 Loss: 0.0001720013942531387\n",
      "Iteration: 3239 Loss: 0.0001718196622325541\n",
      "Iteration: 3240 Loss: 0.00017163812222763182\n",
      "Iteration: 3241 Loss: 0.00017145677403527642\n",
      "Iteration: 3242 Loss: 0.0001712756174524391\n",
      "Iteration: 3243 Loss: 0.00017109465227718013\n",
      "Iteration: 3244 Loss: 0.00017091387830709017\n",
      "Iteration: 3245 Loss: 0.0001707332953401191\n",
      "Iteration: 3246 Loss: 0.00017055290317427964\n",
      "Iteration: 3247 Loss: 0.0001703727016083246\n",
      "Iteration: 3248 Loss: 0.00017019269044067314\n",
      "Iteration: 3249 Loss: 0.00017001286947013083\n",
      "Iteration: 3250 Loss: 0.00016983323849579187\n",
      "Iteration: 3251 Loss: 0.00016965379731687452\n",
      "Iteration: 3252 Loss: 0.00016947454573269598\n",
      "Iteration: 3253 Loss: 0.00016929548354314408\n",
      "Iteration: 3254 Loss: 0.00016911661054793504\n",
      "Iteration: 3255 Loss: 0.00016893792654716698\n",
      "Iteration: 3256 Loss: 0.00016875943134127746\n",
      "Iteration: 3257 Loss: 0.00016858112473067168\n",
      "Iteration: 3258 Loss: 0.00016840300651606107\n",
      "Iteration: 3259 Loss: 0.00016822507649836722\n",
      "Iteration: 3260 Loss: 0.00016804733447883647\n",
      "Iteration: 3261 Loss: 0.00016786978025872928\n",
      "Iteration: 3262 Loss: 0.00016769241363960978\n",
      "Iteration: 3263 Loss: 0.00016751523442334431\n",
      "Iteration: 3264 Loss: 0.00016733824241182355\n",
      "Iteration: 3265 Loss: 0.00016716143740728236\n",
      "Iteration: 3266 Loss: 0.00016698481921211472\n",
      "Iteration: 3267 Loss: 0.0001668083876289246\n",
      "Iteration: 3268 Loss: 0.00016663214246048053\n",
      "Iteration: 3269 Loss: 0.00016645608350990601\n",
      "Iteration: 3270 Loss: 0.00016628021058041176\n",
      "Iteration: 3271 Loss: 0.0001661045234754248\n",
      "Iteration: 3272 Loss: 0.00016592902199860735\n",
      "Iteration: 3273 Loss: 0.0001657537059538532\n",
      "Iteration: 3274 Loss: 0.00016557857514521357\n",
      "Iteration: 3275 Loss: 0.00016540362937688422\n",
      "Iteration: 3276 Loss: 0.00016522886845346874\n",
      "Iteration: 3277 Loss: 0.00016505429217943682\n",
      "Iteration: 3278 Loss: 0.00016487990035996355\n",
      "Iteration: 3279 Loss: 0.00016470569280002459\n",
      "Iteration: 3280 Loss: 0.00016453166930487459\n",
      "Iteration: 3281 Loss: 0.00016435782968012813\n",
      "Iteration: 3282 Loss: 0.00016418417373145912\n",
      "Iteration: 3283 Loss: 0.0001640107012647039\n",
      "Iteration: 3284 Loss: 0.00016383741208609052\n",
      "Iteration: 3285 Loss: 0.00016366430600193028\n",
      "Iteration: 3286 Loss: 0.00016349138281876585\n",
      "Iteration: 3287 Loss: 0.00016331864234336502\n",
      "Iteration: 3288 Loss: 0.00016314608438265318\n",
      "Iteration: 3289 Loss: 0.00016297370874380812\n",
      "Iteration: 3290 Loss: 0.0001628015152342039\n",
      "Iteration: 3291 Loss: 0.0001626295036612841\n",
      "Iteration: 3292 Loss: 0.0001624576738329308\n",
      "Iteration: 3293 Loss: 0.00016228602555721136\n",
      "Iteration: 3294 Loss: 0.0001621145586417954\n",
      "Iteration: 3295 Loss: 0.0001619432728957795\n",
      "Iteration: 3296 Loss: 0.00016177216812711406\n",
      "Iteration: 3297 Loss: 0.00016160124414465153\n",
      "Iteration: 3298 Loss: 0.00016143050075776292\n",
      "Iteration: 3299 Loss: 0.00016125993777539618\n",
      "Iteration: 3300 Loss: 0.00016108955500698953\n",
      "Iteration: 3301 Loss: 0.00016091935226203593\n",
      "Iteration: 3302 Loss: 0.00016074932935042087\n",
      "Iteration: 3303 Loss: 0.00016057948608211414\n",
      "Iteration: 3304 Loss: 0.0001604098222672334\n",
      "Iteration: 3305 Loss: 0.00016024033771619396\n",
      "Iteration: 3306 Loss: 0.00016007103223983837\n",
      "Iteration: 3307 Loss: 0.0001599019056484556\n",
      "Iteration: 3308 Loss: 0.00015973295775330778\n",
      "Iteration: 3309 Loss: 0.00015956418836584536\n",
      "Iteration: 3310 Loss: 0.00015939559729669512\n",
      "Iteration: 3311 Loss: 0.00015922718435816967\n",
      "Iteration: 3312 Loss: 0.0001590589493616236\n",
      "Iteration: 3313 Loss: 0.00015889089211942\n",
      "Iteration: 3314 Loss: 0.0001587230124435723\n",
      "Iteration: 3315 Loss: 0.00015855531014644737\n",
      "Iteration: 3316 Loss: 0.00015838778504061613\n",
      "Iteration: 3317 Loss: 0.00015822043693883888\n",
      "Iteration: 3318 Loss: 0.00015805326565422083\n",
      "Iteration: 3319 Loss: 0.00015788627099973967\n",
      "Iteration: 3320 Loss: 0.00015771945278891017\n",
      "Iteration: 3321 Loss: 0.0001575528108353299\n",
      "Iteration: 3322 Loss: 0.0001573863449526502\n",
      "Iteration: 3323 Loss: 0.00015722005495493743\n",
      "Iteration: 3324 Loss: 0.00015705394065622974\n",
      "Iteration: 3325 Loss: 0.000156888001870898\n",
      "Iteration: 3326 Loss: 0.00015672223841364466\n",
      "Iteration: 3327 Loss: 0.0001565566500991305\n",
      "Iteration: 3328 Loss: 0.0001563912367422786\n",
      "Iteration: 3329 Loss: 0.0001562259981581963\n",
      "Iteration: 3330 Loss: 0.00015606093416233315\n",
      "Iteration: 3331 Loss: 0.00015589604457007586\n",
      "Iteration: 3332 Loss: 0.00015573132919730873\n",
      "Iteration: 3333 Loss: 0.00015556678786005185\n",
      "Iteration: 3334 Loss: 0.00015540242037403692\n",
      "Iteration: 3335 Loss: 0.000155238226555798\n",
      "Iteration: 3336 Loss: 0.00015507420622181745\n",
      "Iteration: 3337 Loss: 0.00015491035918886568\n",
      "Iteration: 3338 Loss: 0.00015474668527359\n",
      "Iteration: 3339 Loss: 0.00015458318429336675\n",
      "Iteration: 3340 Loss: 0.00015441985606514232\n",
      "Iteration: 3341 Loss: 0.00015425670040681224\n",
      "Iteration: 3342 Loss: 0.00015409371713580194\n",
      "Iteration: 3343 Loss: 0.00015393090606995673\n",
      "Iteration: 3344 Loss: 0.00015376826702733927\n",
      "Iteration: 3345 Loss: 0.00015360579982620894\n",
      "Iteration: 3346 Loss: 0.00015344350428504234\n",
      "Iteration: 3347 Loss: 0.00015328138022241872\n",
      "Iteration: 3348 Loss: 0.00015311942745718037\n",
      "Iteration: 3349 Loss: 0.0001529576458083067\n",
      "Iteration: 3350 Loss: 0.00015279603509498848\n",
      "Iteration: 3351 Loss: 0.000152634595136628\n",
      "Iteration: 3352 Loss: 0.00015247332575283768\n",
      "Iteration: 3353 Loss: 0.00015231222676335068\n",
      "Iteration: 3354 Loss: 0.0001521512979881468\n",
      "Iteration: 3355 Loss: 0.00015199053924739965\n",
      "Iteration: 3356 Loss: 0.00015182995036138377\n",
      "Iteration: 3357 Loss: 0.0001516695311507038\n",
      "Iteration: 3358 Loss: 0.00015150928143601807\n",
      "Iteration: 3359 Loss: 0.00015134920103825506\n",
      "Iteration: 3360 Loss: 0.00015118928977859785\n",
      "Iteration: 3361 Loss: 0.00015102954747818424\n",
      "Iteration: 3362 Loss: 0.0001508699739585656\n",
      "Iteration: 3363 Loss: 0.0001507105690414021\n",
      "Iteration: 3364 Loss: 0.00015055133254857826\n",
      "Iteration: 3365 Loss: 0.0001503922643021188\n",
      "Iteration: 3366 Loss: 0.0001502333641241555\n",
      "Iteration: 3367 Loss: 0.00015007463183730073\n",
      "Iteration: 3368 Loss: 0.00014991606726382536\n",
      "Iteration: 3369 Loss: 0.00014975767022687162\n",
      "Iteration: 3370 Loss: 0.00014959944054936559\n",
      "Iteration: 3371 Loss: 0.0001494413780543966\n",
      "Iteration: 3372 Loss: 0.00014928348256527086\n",
      "Iteration: 3373 Loss: 0.0001491257539057739\n",
      "Iteration: 3374 Loss: 0.00014896819189907313\n",
      "Iteration: 3375 Loss: 0.0001488107963696375\n",
      "Iteration: 3376 Loss: 0.00014865356714140773\n",
      "Iteration: 3377 Loss: 0.00014849650403856045\n",
      "Iteration: 3378 Loss: 0.00014833960688564673\n",
      "Iteration: 3379 Loss: 0.00014818287550726986\n",
      "Iteration: 3380 Loss: 0.00014802630972833106\n",
      "Iteration: 3381 Loss: 0.0001478699093738393\n",
      "Iteration: 3382 Loss: 0.00014771367426891428\n",
      "Iteration: 3383 Loss: 0.00014755760423909605\n",
      "Iteration: 3384 Loss: 0.00014740169910986494\n",
      "Iteration: 3385 Loss: 0.00014724595870700344\n",
      "Iteration: 3386 Loss: 0.00014709038285644227\n",
      "Iteration: 3387 Loss: 0.00014693497138430534\n",
      "Iteration: 3388 Loss: 0.0001467797241169426\n",
      "Iteration: 3389 Loss: 0.00014662464088080544\n",
      "Iteration: 3390 Loss: 0.0001464697215026102\n",
      "Iteration: 3391 Loss: 0.0001463149658092017\n",
      "Iteration: 3392 Loss: 0.0001461603736276351\n",
      "Iteration: 3393 Loss: 0.000146005944785079\n",
      "Iteration: 3394 Loss: 0.00014585167910920253\n",
      "Iteration: 3395 Loss: 0.00014569757642720222\n",
      "Iteration: 3396 Loss: 0.00014554363656681937\n",
      "Iteration: 3397 Loss: 0.0001453898593563943\n",
      "Iteration: 3398 Loss: 0.00014523624462392152\n",
      "Iteration: 3399 Loss: 0.00014508279219766122\n",
      "Iteration: 3400 Loss: 0.0001449295019063322\n",
      "Iteration: 3401 Loss: 0.00014477637357822727\n",
      "Iteration: 3402 Loss: 0.00014462340704260812\n",
      "Iteration: 3403 Loss: 0.0001444706021280783\n",
      "Iteration: 3404 Loss: 0.00014431795866408988\n",
      "Iteration: 3405 Loss: 0.00014416547647985408\n",
      "Iteration: 3406 Loss: 0.00014401315540532591\n",
      "Iteration: 3407 Loss: 0.00014386099527009732\n",
      "Iteration: 3408 Loss: 0.00014370899590387338\n",
      "Iteration: 3409 Loss: 0.00014355715713718214\n",
      "Iteration: 3410 Loss: 0.00014340547880015516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3411 Loss: 0.0001432539607232544\n",
      "Iteration: 3412 Loss: 0.00014310260273712634\n",
      "Iteration: 3413 Loss: 0.00014295140467262063\n",
      "Iteration: 3414 Loss: 0.0001428003663607507\n",
      "Iteration: 3415 Loss: 0.0001426494876325157\n",
      "Iteration: 3416 Loss: 0.0001424987683196896\n",
      "Iteration: 3417 Loss: 0.00014234820825359903\n",
      "Iteration: 3418 Loss: 0.00014219780726599973\n",
      "Iteration: 3419 Loss: 0.00014204756518881772\n",
      "Iteration: 3420 Loss: 0.0001418974818540863\n",
      "Iteration: 3421 Loss: 0.0001417475570941104\n",
      "Iteration: 3422 Loss: 0.00014159779074131868\n",
      "Iteration: 3423 Loss: 0.0001414481826282955\n",
      "Iteration: 3424 Loss: 0.00014129873258806932\n",
      "Iteration: 3425 Loss: 0.00014114944045320933\n",
      "Iteration: 3426 Loss: 0.0001410003060570342\n",
      "Iteration: 3427 Loss: 0.00014085132923289973\n",
      "Iteration: 3428 Loss: 0.0001407025098142873\n",
      "Iteration: 3429 Loss: 0.0001405538476348577\n",
      "Iteration: 3430 Loss: 0.00014040534252849422\n",
      "Iteration: 3431 Loss: 0.00014025699432919996\n",
      "Iteration: 3432 Loss: 0.00014010880287116325\n",
      "Iteration: 3433 Loss: 0.0001399607679887953\n",
      "Iteration: 3434 Loss: 0.00013981288951662484\n",
      "Iteration: 3435 Loss: 0.00013966516728938604\n",
      "Iteration: 3436 Loss: 0.00013951760114198312\n",
      "Iteration: 3437 Loss: 0.0001393701909094966\n",
      "Iteration: 3438 Loss: 0.00013922293642714296\n",
      "Iteration: 3439 Loss: 0.00013907583753039086\n",
      "Iteration: 3440 Loss: 0.00013892889405480824\n",
      "Iteration: 3441 Loss: 0.000138782105836191\n",
      "Iteration: 3442 Loss: 0.00013863547271045597\n",
      "Iteration: 3443 Loss: 0.00013848899451373182\n",
      "Iteration: 3444 Loss: 0.00013834267108234384\n",
      "Iteration: 3445 Loss: 0.00013819650225269786\n",
      "Iteration: 3446 Loss: 0.00013805048786147436\n",
      "Iteration: 3447 Loss: 0.00013790462774547368\n",
      "Iteration: 3448 Loss: 0.0001377589217417037\n",
      "Iteration: 3449 Loss: 0.00013761336968728225\n",
      "Iteration: 3450 Loss: 0.00013746797141954054\n",
      "Iteration: 3451 Loss: 0.00013732272677600152\n",
      "Iteration: 3452 Loss: 0.0001371776355943366\n",
      "Iteration: 3453 Loss: 0.00013703269771236785\n",
      "Iteration: 3454 Loss: 0.00013688791296813258\n",
      "Iteration: 3455 Loss: 0.00013674328119981106\n",
      "Iteration: 3456 Loss: 0.00013659880224575703\n",
      "Iteration: 3457 Loss: 0.00013645447594451028\n",
      "Iteration: 3458 Loss: 0.00013631030213474276\n",
      "Iteration: 3459 Loss: 0.00013616628065535197\n",
      "Iteration: 3460 Loss: 0.00013602241134556315\n",
      "Iteration: 3461 Loss: 0.00013587869404399524\n",
      "Iteration: 3462 Loss: 0.00013573512859062727\n",
      "Iteration: 3463 Loss: 0.0001355917148248113\n",
      "Iteration: 3464 Loss: 0.0001354484525862672\n",
      "Iteration: 3465 Loss: 0.00013530534171489914\n",
      "Iteration: 3466 Loss: 0.00013516238205072095\n",
      "Iteration: 3467 Loss: 0.00013501957343401352\n",
      "Iteration: 3468 Loss: 0.0001348769157051613\n",
      "Iteration: 3469 Loss: 0.00013473440870470638\n",
      "Iteration: 3470 Loss: 0.0001345920522734043\n",
      "Iteration: 3471 Loss: 0.0001344498462521508\n",
      "Iteration: 3472 Loss: 0.00013430779048198845\n",
      "Iteration: 3473 Loss: 0.00013416588480421392\n",
      "Iteration: 3474 Loss: 0.00013402412906021198\n",
      "Iteration: 3475 Loss: 0.00013388252309154034\n",
      "Iteration: 3476 Loss: 0.00013374106673995527\n",
      "Iteration: 3477 Loss: 0.00013359975984736036\n",
      "Iteration: 3478 Loss: 0.00013345860225587354\n",
      "Iteration: 3479 Loss: 0.00013331759380768386\n",
      "Iteration: 3480 Loss: 0.0001331767343452184\n",
      "Iteration: 3481 Loss: 0.00013303602371105165\n",
      "Iteration: 3482 Loss: 0.00013289546174795924\n",
      "Iteration: 3483 Loss: 0.00013275504829882883\n",
      "Iteration: 3484 Loss: 0.00013261478320670696\n",
      "Iteration: 3485 Loss: 0.00013247466631488\n",
      "Iteration: 3486 Loss: 0.0001323346974667371\n",
      "Iteration: 3487 Loss: 0.00013219487650583875\n",
      "Iteration: 3488 Loss: 0.00013205520327599658\n",
      "Iteration: 3489 Loss: 0.0001319156776210402\n",
      "Iteration: 3490 Loss: 0.00013177629938505508\n",
      "Iteration: 3491 Loss: 0.000131637068412308\n",
      "Iteration: 3492 Loss: 0.00013149798454718022\n",
      "Iteration: 3493 Loss: 0.00013135904763419932\n",
      "Iteration: 3494 Loss: 0.00013122025751810377\n",
      "Iteration: 3495 Loss: 0.00013108161404383663\n",
      "Iteration: 3496 Loss: 0.0001309431170564143\n",
      "Iteration: 3497 Loss: 0.00013080476640106662\n",
      "Iteration: 3498 Loss: 0.00013066656192318174\n",
      "Iteration: 3499 Loss: 0.00013052850346838778\n",
      "Iteration: 3500 Loss: 0.0001303905908822096\n",
      "Iteration: 3501 Loss: 0.00013025282401061723\n",
      "Iteration: 3502 Loss: 0.00013011520269966227\n",
      "Iteration: 3503 Loss: 0.00012997772679553245\n",
      "Iteration: 3504 Loss: 0.00012984039614447794\n",
      "Iteration: 3505 Loss: 0.0001297032105932502\n",
      "Iteration: 3506 Loss: 0.00012956616998840173\n",
      "Iteration: 3507 Loss: 0.0001294292741768476\n",
      "Iteration: 3508 Loss: 0.0001292925230055552\n",
      "Iteration: 3509 Loss: 0.00012915591632167353\n",
      "Iteration: 3510 Loss: 0.0001290194539725435\n",
      "Iteration: 3511 Loss: 0.0001288831358057061\n",
      "Iteration: 3512 Loss: 0.00012874696166877752\n",
      "Iteration: 3513 Loss: 0.00012861093140955292\n",
      "Iteration: 3514 Loss: 0.00012847504487603742\n",
      "Iteration: 3515 Loss: 0.00012833930191640276\n",
      "Iteration: 3516 Loss: 0.00012820370237888443\n",
      "Iteration: 3517 Loss: 0.00012806824611201156\n",
      "Iteration: 3518 Loss: 0.0001279329329643621\n",
      "Iteration: 3519 Loss: 0.00012779776278477013\n",
      "Iteration: 3520 Loss: 0.00012766273542206898\n",
      "Iteration: 3521 Loss: 0.0001275278507256747\n",
      "Iteration: 3522 Loss: 0.00012739310854419705\n",
      "Iteration: 3523 Loss: 0.000127258508727662\n",
      "Iteration: 3524 Loss: 0.0001271240511254629\n",
      "Iteration: 3525 Loss: 0.00012698973558728538\n",
      "Iteration: 3526 Loss: 0.00012685556196305258\n",
      "Iteration: 3527 Loss: 0.00012672153010281637\n",
      "Iteration: 3528 Loss: 0.0001265876398567925\n",
      "Iteration: 3529 Loss: 0.00012645389107537544\n",
      "Iteration: 3530 Loss: 0.0001263202836090886\n",
      "Iteration: 3531 Loss: 0.00012618681730859708\n",
      "Iteration: 3532 Loss: 0.0001260534920247428\n",
      "Iteration: 3533 Loss: 0.0001259203076085748\n",
      "Iteration: 3534 Loss: 0.0001257872639111742\n",
      "Iteration: 3535 Loss: 0.00012565436078393897\n",
      "Iteration: 3536 Loss: 0.00012552159807830302\n",
      "Iteration: 3537 Loss: 0.0001253889756459178\n",
      "Iteration: 3538 Loss: 0.0001252564933386789\n",
      "Iteration: 3539 Loss: 0.00012512415100830021\n",
      "Iteration: 3540 Loss: 0.000124991948506981\n",
      "Iteration: 3541 Loss: 0.00012485988568698732\n",
      "Iteration: 3542 Loss: 0.00012472796240066558\n",
      "Iteration: 3543 Loss: 0.00012459617850073195\n",
      "Iteration: 3544 Loss: 0.0001244645338399196\n",
      "Iteration: 3545 Loss: 0.00012433302827101548\n",
      "Iteration: 3546 Loss: 0.0001242016616470874\n",
      "Iteration: 3547 Loss: 0.00012407043382131359\n",
      "Iteration: 3548 Loss: 0.00012393934464703335\n",
      "Iteration: 3549 Loss: 0.00012380839397773741\n",
      "Iteration: 3550 Loss: 0.00012367758166713033\n",
      "Iteration: 3551 Loss: 0.00012354690756903464\n",
      "Iteration: 3552 Loss: 0.000123416371537596\n",
      "Iteration: 3553 Loss: 0.0001232859734265057\n",
      "Iteration: 3554 Loss: 0.00012315571309024273\n",
      "Iteration: 3555 Loss: 0.00012302559038305692\n",
      "Iteration: 3556 Loss: 0.00012289560515992988\n",
      "Iteration: 3557 Loss: 0.0001227657572754227\n",
      "Iteration: 3558 Loss: 0.00012263604658438684\n",
      "Iteration: 3559 Loss: 0.00012250647294187434\n",
      "Iteration: 3560 Loss: 0.00012237703620312658\n",
      "Iteration: 3561 Loss: 0.00012224773622343332\n",
      "Iteration: 3562 Loss: 0.00012211857285832908\n",
      "Iteration: 3563 Loss: 0.00012198954596343447\n",
      "Iteration: 3564 Loss: 0.00012186065539458282\n",
      "Iteration: 3565 Loss: 0.00012173190100775787\n",
      "Iteration: 3566 Loss: 0.00012160328265905528\n",
      "Iteration: 3567 Loss: 0.00012147480020485937\n",
      "Iteration: 3568 Loss: 0.00012134645350135303\n",
      "Iteration: 3569 Loss: 0.0001212182424053108\n",
      "Iteration: 3570 Loss: 0.00012109016677327876\n",
      "Iteration: 3571 Loss: 0.00012096222646219893\n",
      "Iteration: 3572 Loss: 0.00012083442132910893\n",
      "Iteration: 3573 Loss: 0.00012070675123114968\n",
      "Iteration: 3574 Loss: 0.00012057921602567871\n",
      "Iteration: 3575 Loss: 0.00012045181557019775\n",
      "Iteration: 3576 Loss: 0.00012032454972227994\n",
      "Iteration: 3577 Loss: 0.00012019741833972601\n",
      "Iteration: 3578 Loss: 0.00012007042128065827\n",
      "Iteration: 3579 Loss: 0.00011994355840259698\n",
      "Iteration: 3580 Loss: 0.00011981682956405483\n",
      "Iteration: 3581 Loss: 0.00011969023462374135\n",
      "Iteration: 3582 Loss: 0.00011956377343989918\n",
      "Iteration: 3583 Loss: 0.00011943744587125061\n",
      "Iteration: 3584 Loss: 0.00011931125177662275\n",
      "Iteration: 3585 Loss: 0.00011918519101492685\n",
      "Iteration: 3586 Loss: 0.0001190592634453471\n",
      "Iteration: 3587 Loss: 0.00011893346892714663\n",
      "Iteration: 3588 Loss: 0.0001188078073197381\n",
      "Iteration: 3589 Loss: 0.00011868227848267163\n",
      "Iteration: 3590 Loss: 0.0001185568822756821\n",
      "Iteration: 3591 Loss: 0.00011843161855865013\n",
      "Iteration: 3592 Loss: 0.00011830648719157592\n",
      "Iteration: 3593 Loss: 0.000118181488034615\n",
      "Iteration: 3594 Loss: 0.00011805662094808104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3595 Loss: 0.00011793188579244478\n",
      "Iteration: 3596 Loss: 0.00011780728242828584\n",
      "Iteration: 3597 Loss: 0.00011768281071638141\n",
      "Iteration: 3598 Loss: 0.0001175584705175967\n",
      "Iteration: 3599 Loss: 0.0001174342616930349\n",
      "Iteration: 3600 Loss: 0.00011731018410387585\n",
      "Iteration: 3601 Loss: 0.00011718623761164485\n",
      "Iteration: 3602 Loss: 0.00011706242207725022\n",
      "Iteration: 3603 Loss: 0.00011693873736289698\n",
      "Iteration: 3604 Loss: 0.00011681518333016688\n",
      "Iteration: 3605 Loss: 0.00011669175984097234\n",
      "Iteration: 3606 Loss: 0.00011656846675741764\n",
      "Iteration: 3607 Loss: 0.00011644530394169825\n",
      "Iteration: 3608 Loss: 0.00011632227125619058\n",
      "Iteration: 3609 Loss: 0.00011619936856339803\n",
      "Iteration: 3610 Loss: 0.00011607659572606042\n",
      "Iteration: 3611 Loss: 0.00011595395260680827\n",
      "Iteration: 3612 Loss: 0.00011583143906863615\n",
      "Iteration: 3613 Loss: 0.000115709054974702\n",
      "Iteration: 3614 Loss: 0.00011558680018809904\n",
      "Iteration: 3615 Loss: 0.00011546467457236688\n",
      "Iteration: 3616 Loss: 0.00011534267799079966\n",
      "Iteration: 3617 Loss: 0.00011522081030734231\n",
      "Iteration: 3618 Loss: 0.00011509907138571723\n",
      "Iteration: 3619 Loss: 0.00011497746108981637\n",
      "Iteration: 3620 Loss: 0.00011485597928379191\n",
      "Iteration: 3621 Loss: 0.00011473462583185882\n",
      "Iteration: 3622 Loss: 0.00011461340059841275\n",
      "Iteration: 3623 Loss: 0.00011449230344792402\n",
      "Iteration: 3624 Loss: 0.0001143713342451539\n",
      "Iteration: 3625 Loss: 0.00011425049285488635\n",
      "Iteration: 3626 Loss: 0.00011412977914205169\n",
      "Iteration: 3627 Loss: 0.00011400919297176106\n",
      "Iteration: 3628 Loss: 0.00011388873420926561\n",
      "Iteration: 3629 Loss: 0.00011376840271989573\n",
      "Iteration: 3630 Loss: 0.00011364819836929603\n",
      "Iteration: 3631 Loss: 0.00011352812102309484\n",
      "Iteration: 3632 Loss: 0.0001134081705470303\n",
      "Iteration: 3633 Loss: 0.00011328834680709762\n",
      "Iteration: 3634 Loss: 0.00011316864966943206\n",
      "Iteration: 3635 Loss: 0.00011304907900018998\n",
      "Iteration: 3636 Loss: 0.0001129296346657862\n",
      "Iteration: 3637 Loss: 0.00011281031653277636\n",
      "Iteration: 3638 Loss: 0.00011269112446774232\n",
      "Iteration: 3639 Loss: 0.0001125720583375818\n",
      "Iteration: 3640 Loss: 0.00011245311800916138\n",
      "Iteration: 3641 Loss: 0.00011233430334955626\n",
      "Iteration: 3642 Loss: 0.00011221561422604972\n",
      "Iteration: 3643 Loss: 0.00011209705050597528\n",
      "Iteration: 3644 Loss: 0.00011197861205677656\n",
      "Iteration: 3645 Loss: 0.00011186029874623304\n",
      "Iteration: 3646 Loss: 0.00011174211044194979\n",
      "Iteration: 3647 Loss: 0.00011162404701202357\n",
      "Iteration: 3648 Loss: 0.00011150610832439069\n",
      "Iteration: 3649 Loss: 0.00011138829424736517\n",
      "Iteration: 3650 Loss: 0.00011127060464913638\n",
      "Iteration: 3651 Loss: 0.0001111530393982826\n",
      "Iteration: 3652 Loss: 0.00011103559836341288\n",
      "Iteration: 3653 Loss: 0.00011091828141326179\n",
      "Iteration: 3654 Loss: 0.00011080108841674835\n",
      "Iteration: 3655 Loss: 0.00011068401924289463\n",
      "Iteration: 3656 Loss: 0.00011056707376085197\n",
      "Iteration: 3657 Loss: 0.00011045025183995041\n",
      "Iteration: 3658 Loss: 0.00011033355334969482\n",
      "Iteration: 3659 Loss: 0.00011021697815958961\n",
      "Iteration: 3660 Loss: 0.0001101005261394042\n",
      "Iteration: 3661 Loss: 0.0001099841971590015\n",
      "Iteration: 3662 Loss: 0.00010986799108834395\n",
      "Iteration: 3663 Loss: 0.00010975190779758235\n",
      "Iteration: 3664 Loss: 0.00010963594715699959\n",
      "Iteration: 3665 Loss: 0.0001095201090370866\n",
      "Iteration: 3666 Loss: 0.00010940439330823262\n",
      "Iteration: 3667 Loss: 0.00010928879984123105\n",
      "Iteration: 3668 Loss: 0.00010917332850690345\n",
      "Iteration: 3669 Loss: 0.00010905797917614066\n",
      "Iteration: 3670 Loss: 0.00010894275172009817\n",
      "Iteration: 3671 Loss: 0.00010882764600999728\n",
      "Iteration: 3672 Loss: 0.00010871266191723093\n",
      "Iteration: 3673 Loss: 0.00010859779931320452\n",
      "Iteration: 3674 Loss: 0.0001084830580696836\n",
      "Iteration: 3675 Loss: 0.00010836843805831139\n",
      "Iteration: 3676 Loss: 0.00010825393915107098\n",
      "Iteration: 3677 Loss: 0.00010813956121999696\n",
      "Iteration: 3678 Loss: 0.00010802530413727336\n",
      "Iteration: 3679 Loss: 0.00010791116777540599\n",
      "Iteration: 3680 Loss: 0.00010779715200645042\n",
      "Iteration: 3681 Loss: 0.0001076832567029833\n",
      "Iteration: 3682 Loss: 0.0001075694817381909\n",
      "Iteration: 3683 Loss: 0.00010745582698465605\n",
      "Iteration: 3684 Loss: 0.00010734229231537857\n",
      "Iteration: 3685 Loss: 0.0001072288776037072\n",
      "Iteration: 3686 Loss: 0.00010711558272222017\n",
      "Iteration: 3687 Loss: 0.00010700240754502373\n",
      "Iteration: 3688 Loss: 0.00010688935194534885\n",
      "Iteration: 3689 Loss: 0.00010677641579688802\n",
      "Iteration: 3690 Loss: 0.0001066635989734197\n",
      "Iteration: 3691 Loss: 0.00010655090134892614\n",
      "Iteration: 3692 Loss: 0.00010643832279741141\n",
      "Iteration: 3693 Loss: 0.00010632586319304696\n",
      "Iteration: 3694 Loss: 0.00010621352241017502\n",
      "Iteration: 3695 Loss: 0.00010610130032334568\n",
      "Iteration: 3696 Loss: 0.00010598919680701223\n",
      "Iteration: 3697 Loss: 0.00010587721173597285\n",
      "Iteration: 3698 Loss: 0.00010576534498499764\n",
      "Iteration: 3699 Loss: 0.0001056535964291847\n",
      "Iteration: 3700 Loss: 0.00010554196594355992\n",
      "Iteration: 3701 Loss: 0.00010543045340344566\n",
      "Iteration: 3702 Loss: 0.00010531905868421214\n",
      "Iteration: 3703 Loss: 0.0001052077816613429\n",
      "Iteration: 3704 Loss: 0.00010509662221049414\n",
      "Iteration: 3705 Loss: 0.00010498558020748165\n",
      "Iteration: 3706 Loss: 0.00010487465552815929\n",
      "Iteration: 3707 Loss: 0.00010476384804857089\n",
      "Iteration: 3708 Loss: 0.00010465315764489446\n",
      "Iteration: 3709 Loss: 0.00010454258419343246\n",
      "Iteration: 3710 Loss: 0.00010443212757066279\n",
      "Iteration: 3711 Loss: 0.00010432178765308484\n",
      "Iteration: 3712 Loss: 0.00010421156431738165\n",
      "Iteration: 3713 Loss: 0.00010410145744041546\n",
      "Iteration: 3714 Loss: 0.00010399146689918547\n",
      "Iteration: 3715 Loss: 0.00010388159257066603\n",
      "Iteration: 3716 Loss: 0.00010377183433217303\n",
      "Iteration: 3717 Loss: 0.00010366219206098483\n",
      "Iteration: 3718 Loss: 0.00010355266563459402\n",
      "Iteration: 3719 Loss: 0.0001034432549305619\n",
      "Iteration: 3720 Loss: 0.00010333395982670849\n",
      "Iteration: 3721 Loss: 0.00010322478020080007\n",
      "Iteration: 3722 Loss: 0.00010311571593088623\n",
      "Iteration: 3723 Loss: 0.00010300676689510843\n",
      "Iteration: 3724 Loss: 0.00010289793297167183\n",
      "Iteration: 3725 Loss: 0.00010278921403893087\n",
      "Iteration: 3726 Loss: 0.00010268060997536346\n",
      "Iteration: 3727 Loss: 0.00010257212065969832\n",
      "Iteration: 3728 Loss: 0.00010246374597062662\n",
      "Iteration: 3729 Loss: 0.0001023554857870647\n",
      "Iteration: 3730 Loss: 0.00010224733998798462\n",
      "Iteration: 3731 Loss: 0.00010213930845267003\n",
      "Iteration: 3732 Loss: 0.00010203139106022927\n",
      "Iteration: 3733 Loss: 0.00010192358769016515\n",
      "Iteration: 3734 Loss: 0.00010181589822197522\n",
      "Iteration: 3735 Loss: 0.00010170832253526534\n",
      "Iteration: 3736 Loss: 0.00010160086050988895\n",
      "Iteration: 3737 Loss: 0.00010149351202573327\n",
      "Iteration: 3738 Loss: 0.00010138627696284243\n",
      "Iteration: 3739 Loss: 0.0001012791552013517\n",
      "Iteration: 3740 Loss: 0.00010117214662155913\n",
      "Iteration: 3741 Loss: 0.0001010652511038746\n",
      "Iteration: 3742 Loss: 0.00010095846852907265\n",
      "Iteration: 3743 Loss: 0.0001008517987773473\n",
      "Iteration: 3744 Loss: 0.00010074524172981352\n",
      "Iteration: 3745 Loss: 0.0001006387972672829\n",
      "Iteration: 3746 Loss: 0.00010053246527081078\n",
      "Iteration: 3747 Loss: 0.00010042624562143875\n",
      "Iteration: 3748 Loss: 0.00010032013820077639\n",
      "Iteration: 3749 Loss: 0.00010021414289012521\n",
      "Iteration: 3750 Loss: 0.00010010825957104879\n",
      "Iteration: 3751 Loss: 0.00010000248812513556\n",
      "Iteration: 3752 Loss: 9.989682843421762e-05\n",
      "Iteration: 3753 Loss: 9.979128038022644e-05\n",
      "Iteration: 3754 Loss: 9.968584384518265e-05\n",
      "Iteration: 3755 Loss: 9.958051871130907e-05\n",
      "Iteration: 3756 Loss: 9.947530486086103e-05\n",
      "Iteration: 3757 Loss: 9.937020217627272e-05\n",
      "Iteration: 3758 Loss: 9.926521054009566e-05\n",
      "Iteration: 3759 Loss: 9.91603298349926e-05\n",
      "Iteration: 3760 Loss: 9.90555599437489e-05\n",
      "Iteration: 3761 Loss: 9.895090074937406e-05\n",
      "Iteration: 3762 Loss: 9.884635213476698e-05\n",
      "Iteration: 3763 Loss: 9.874191398320823e-05\n",
      "Iteration: 3764 Loss: 9.863758617787476e-05\n",
      "Iteration: 3765 Loss: 9.853336860228833e-05\n",
      "Iteration: 3766 Loss: 9.84292611398821e-05\n",
      "Iteration: 3767 Loss: 9.832526367436165e-05\n",
      "Iteration: 3768 Loss: 9.822137608950467e-05\n",
      "Iteration: 3769 Loss: 9.811759826920849e-05\n",
      "Iteration: 3770 Loss: 9.801393009755166e-05\n",
      "Iteration: 3771 Loss: 9.791037145854668e-05\n",
      "Iteration: 3772 Loss: 9.78069222365862e-05\n",
      "Iteration: 3773 Loss: 9.770358231607021e-05\n",
      "Iteration: 3774 Loss: 9.760035158143209e-05\n",
      "Iteration: 3775 Loss: 9.749722991739039e-05\n",
      "Iteration: 3776 Loss: 9.739421720868296e-05\n",
      "Iteration: 3777 Loss: 9.729131334016696e-05\n",
      "Iteration: 3778 Loss: 9.718851819687105e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3779 Loss: 9.708583166387822e-05\n",
      "Iteration: 3780 Loss: 9.698325362641019e-05\n",
      "Iteration: 3781 Loss: 9.688078396994015e-05\n",
      "Iteration: 3782 Loss: 9.67784225799091e-05\n",
      "Iteration: 3783 Loss: 9.66761693418746e-05\n",
      "Iteration: 3784 Loss: 9.657402414164753e-05\n",
      "Iteration: 3785 Loss: 9.647198686501087e-05\n",
      "Iteration: 3786 Loss: 9.637005739794925e-05\n",
      "Iteration: 3787 Loss: 9.626823562660619e-05\n",
      "Iteration: 3788 Loss: 9.616652143709671e-05\n",
      "Iteration: 3789 Loss: 9.606491471586774e-05\n",
      "Iteration: 3790 Loss: 9.596341534931297e-05\n",
      "Iteration: 3791 Loss: 9.586202322412898e-05\n",
      "Iteration: 3792 Loss: 9.576073822682016e-05\n",
      "Iteration: 3793 Loss: 9.565956024438616e-05\n",
      "Iteration: 3794 Loss: 9.55584891633173e-05\n",
      "Iteration: 3795 Loss: 9.545752487112056e-05\n",
      "Iteration: 3796 Loss: 9.535666725481453e-05\n",
      "Iteration: 3797 Loss: 9.525591620174588e-05\n",
      "Iteration: 3798 Loss: 9.515527159915235e-05\n",
      "Iteration: 3799 Loss: 9.505473333479976e-05\n",
      "Iteration: 3800 Loss: 9.495430129628902e-05\n",
      "Iteration: 3801 Loss: 9.485397537129195e-05\n",
      "Iteration: 3802 Loss: 9.475375544765925e-05\n",
      "Iteration: 3803 Loss: 9.465364141351178e-05\n",
      "Iteration: 3804 Loss: 9.455363315689483e-05\n",
      "Iteration: 3805 Loss: 9.445373056605071e-05\n",
      "Iteration: 3806 Loss: 9.435393352942139e-05\n",
      "Iteration: 3807 Loss: 9.425424193549228e-05\n",
      "Iteration: 3808 Loss: 9.415465567265031e-05\n",
      "Iteration: 3809 Loss: 9.405517462973842e-05\n",
      "Iteration: 3810 Loss: 9.395579869559647e-05\n",
      "Iteration: 3811 Loss: 9.385652775897834e-05\n",
      "Iteration: 3812 Loss: 9.375736170927462e-05\n",
      "Iteration: 3813 Loss: 9.365830043554527e-05\n",
      "Iteration: 3814 Loss: 9.355934382696568e-05\n",
      "Iteration: 3815 Loss: 9.346049177311518e-05\n",
      "Iteration: 3816 Loss: 9.336174416341343e-05\n",
      "Iteration: 3817 Loss: 9.326310088751593e-05\n",
      "Iteration: 3818 Loss: 9.316456183526638e-05\n",
      "Iteration: 3819 Loss: 9.306612689645731e-05\n",
      "Iteration: 3820 Loss: 9.296779596110869e-05\n",
      "Iteration: 3821 Loss: 9.286956891934258e-05\n",
      "Iteration: 3822 Loss: 9.277144566140255e-05\n",
      "Iteration: 3823 Loss: 9.267342607760524e-05\n",
      "Iteration: 3824 Loss: 9.257551005841807e-05\n",
      "Iteration: 3825 Loss: 9.24776974944059e-05\n",
      "Iteration: 3826 Loss: 9.23799882763208e-05\n",
      "Iteration: 3827 Loss: 9.228238229491058e-05\n",
      "Iteration: 3828 Loss: 9.218487944111154e-05\n",
      "Iteration: 3829 Loss: 9.208747960599363e-05\n",
      "Iteration: 3830 Loss: 9.19901826807376e-05\n",
      "Iteration: 3831 Loss: 9.189298855645086e-05\n",
      "Iteration: 3832 Loss: 9.179589712469681e-05\n",
      "Iteration: 3833 Loss: 9.169890827686275e-05\n",
      "Iteration: 3834 Loss: 9.160202190465482e-05\n",
      "Iteration: 3835 Loss: 9.15052378999132e-05\n",
      "Iteration: 3836 Loss: 9.140855615416159e-05\n",
      "Iteration: 3837 Loss: 9.131197655946426e-05\n",
      "Iteration: 3838 Loss: 9.121549900793035e-05\n",
      "Iteration: 3839 Loss: 9.111912339179108e-05\n",
      "Iteration: 3840 Loss: 9.102284960321793e-05\n",
      "Iteration: 3841 Loss: 9.092667753474572e-05\n",
      "Iteration: 3842 Loss: 9.083060707864726e-05\n",
      "Iteration: 3843 Loss: 9.073463812800869e-05\n",
      "Iteration: 3844 Loss: 9.063877057537252e-05\n",
      "Iteration: 3845 Loss: 9.054300431356835e-05\n",
      "Iteration: 3846 Loss: 9.044733923551931e-05\n",
      "Iteration: 3847 Loss: 9.035177523441841e-05\n",
      "Iteration: 3848 Loss: 9.02563122034618e-05\n",
      "Iteration: 3849 Loss: 9.016095003598879e-05\n",
      "Iteration: 3850 Loss: 9.006568862537048e-05\n",
      "Iteration: 3851 Loss: 8.997052786518897e-05\n",
      "Iteration: 3852 Loss: 8.987546764899162e-05\n",
      "Iteration: 3853 Loss: 8.978050787067687e-05\n",
      "Iteration: 3854 Loss: 8.968564842405594e-05\n",
      "Iteration: 3855 Loss: 8.95908892031913e-05\n",
      "Iteration: 3856 Loss: 8.949623010214735e-05\n",
      "Iteration: 3857 Loss: 8.940167101512609e-05\n",
      "Iteration: 3858 Loss: 8.930721183649624e-05\n",
      "Iteration: 3859 Loss: 8.921285246062974e-05\n",
      "Iteration: 3860 Loss: 8.911859278208527e-05\n",
      "Iteration: 3861 Loss: 8.902443269561803e-05\n",
      "Iteration: 3862 Loss: 8.89303720958868e-05\n",
      "Iteration: 3863 Loss: 8.883641087786508e-05\n",
      "Iteration: 3864 Loss: 8.874254893649661e-05\n",
      "Iteration: 3865 Loss: 8.864878616694695e-05\n",
      "Iteration: 3866 Loss: 8.855512246433537e-05\n",
      "Iteration: 3867 Loss: 8.846155772407487e-05\n",
      "Iteration: 3868 Loss: 8.836809184163672e-05\n",
      "Iteration: 3869 Loss: 8.82747247123099e-05\n",
      "Iteration: 3870 Loss: 8.81814562322051e-05\n",
      "Iteration: 3871 Loss: 8.80882862967758e-05\n",
      "Iteration: 3872 Loss: 8.799521480191215e-05\n",
      "Iteration: 3873 Loss: 8.790224164369433e-05\n",
      "Iteration: 3874 Loss: 8.780936671819143e-05\n",
      "Iteration: 3875 Loss: 8.771658992164423e-05\n",
      "Iteration: 3876 Loss: 8.762391115048591e-05\n",
      "Iteration: 3877 Loss: 8.753133030083418e-05\n",
      "Iteration: 3878 Loss: 8.743884726919415e-05\n",
      "Iteration: 3879 Loss: 8.73464619525699e-05\n",
      "Iteration: 3880 Loss: 8.725417424761889e-05\n",
      "Iteration: 3881 Loss: 8.716198405109509e-05\n",
      "Iteration: 3882 Loss: 8.70698912600973e-05\n",
      "Iteration: 3883 Loss: 8.697789577162928e-05\n",
      "Iteration: 3884 Loss: 8.688599748290254e-05\n",
      "Iteration: 3885 Loss: 8.679419629120681e-05\n",
      "Iteration: 3886 Loss: 8.670249209397998e-05\n",
      "Iteration: 3887 Loss: 8.661088478873163e-05\n",
      "Iteration: 3888 Loss: 8.651937427307465e-05\n",
      "Iteration: 3889 Loss: 8.64279604447585e-05\n",
      "Iteration: 3890 Loss: 8.633664320156088e-05\n",
      "Iteration: 3891 Loss: 8.62454224415819e-05\n",
      "Iteration: 3892 Loss: 8.615429806275569e-05\n",
      "Iteration: 3893 Loss: 8.606326996323894e-05\n",
      "Iteration: 3894 Loss: 8.597233804148426e-05\n",
      "Iteration: 3895 Loss: 8.588150219565328e-05\n",
      "Iteration: 3896 Loss: 8.579076232426967e-05\n",
      "Iteration: 3897 Loss: 8.570011832587291e-05\n",
      "Iteration: 3898 Loss: 8.560957009945105e-05\n",
      "Iteration: 3899 Loss: 8.551911754359707e-05\n",
      "Iteration: 3900 Loss: 8.542876055726731e-05\n",
      "Iteration: 3901 Loss: 8.533849903948645e-05\n",
      "Iteration: 3902 Loss: 8.524833288938631e-05\n",
      "Iteration: 3903 Loss: 8.515826200617504e-05\n",
      "Iteration: 3904 Loss: 8.506828628922972e-05\n",
      "Iteration: 3905 Loss: 8.497840563802901e-05\n",
      "Iteration: 3906 Loss: 8.488861995208189e-05\n",
      "Iteration: 3907 Loss: 8.479892913100083e-05\n",
      "Iteration: 3908 Loss: 8.470933307466985e-05\n",
      "Iteration: 3909 Loss: 8.46198316829064e-05\n",
      "Iteration: 3910 Loss: 8.453042485567087e-05\n",
      "Iteration: 3911 Loss: 8.444111249307222e-05\n",
      "Iteration: 3912 Loss: 8.435189449555275e-05\n",
      "Iteration: 3913 Loss: 8.426277076273609e-05\n",
      "Iteration: 3914 Loss: 8.417374119564033e-05\n",
      "Iteration: 3915 Loss: 8.408480569456241e-05\n",
      "Iteration: 3916 Loss: 8.399596416019275e-05\n",
      "Iteration: 3917 Loss: 8.390721649314978e-05\n",
      "Iteration: 3918 Loss: 8.381856259433316e-05\n",
      "Iteration: 3919 Loss: 8.373000236465074e-05\n",
      "Iteration: 3920 Loss: 8.364153570507807e-05\n",
      "Iteration: 3921 Loss: 8.355316251680733e-05\n",
      "Iteration: 3922 Loss: 8.346488270102748e-05\n",
      "Iteration: 3923 Loss: 8.337669615911516e-05\n",
      "Iteration: 3924 Loss: 8.328860279253081e-05\n",
      "Iteration: 3925 Loss: 8.3200602502826e-05\n",
      "Iteration: 3926 Loss: 8.311269519164988e-05\n",
      "Iteration: 3927 Loss: 8.30248807608144e-05\n",
      "Iteration: 3928 Loss: 8.293715911214616e-05\n",
      "Iteration: 3929 Loss: 8.28495301475658e-05\n",
      "Iteration: 3930 Loss: 8.276199376919872e-05\n",
      "Iteration: 3931 Loss: 8.2674549879186e-05\n",
      "Iteration: 3932 Loss: 8.258719837985185e-05\n",
      "Iteration: 3933 Loss: 8.249993917352146e-05\n",
      "Iteration: 3934 Loss: 8.241277216270485e-05\n",
      "Iteration: 3935 Loss: 8.232569725005692e-05\n",
      "Iteration: 3936 Loss: 8.223871433822885e-05\n",
      "Iteration: 3937 Loss: 8.215182332996358e-05\n",
      "Iteration: 3938 Loss: 8.206502412820659e-05\n",
      "Iteration: 3939 Loss: 8.197831663596125e-05\n",
      "Iteration: 3940 Loss: 8.189170075629233e-05\n",
      "Iteration: 3941 Loss: 8.180517639242433e-05\n",
      "Iteration: 3942 Loss: 8.171874344766968e-05\n",
      "Iteration: 3943 Loss: 8.163240182543106e-05\n",
      "Iteration: 3944 Loss: 8.15461514292378e-05\n",
      "Iteration: 3945 Loss: 8.145999216279934e-05\n",
      "Iteration: 3946 Loss: 8.13739239296472e-05\n",
      "Iteration: 3947 Loss: 8.128794663369023e-05\n",
      "Iteration: 3948 Loss: 8.12020601787908e-05\n",
      "Iteration: 3949 Loss: 8.111626446903889e-05\n",
      "Iteration: 3950 Loss: 8.10305594084985e-05\n",
      "Iteration: 3951 Loss: 8.094494490141143e-05\n",
      "Iteration: 3952 Loss: 8.085942085197075e-05\n",
      "Iteration: 3953 Loss: 8.077398716484034e-05\n",
      "Iteration: 3954 Loss: 8.068864374450208e-05\n",
      "Iteration: 3955 Loss: 8.060339049553729e-05\n",
      "Iteration: 3956 Loss: 8.051822732267402e-05\n",
      "Iteration: 3957 Loss: 8.043315413068079e-05\n",
      "Iteration: 3958 Loss: 8.034817082456922e-05\n",
      "Iteration: 3959 Loss: 8.026327730927361e-05\n",
      "Iteration: 3960 Loss: 8.01784734900163e-05\n",
      "Iteration: 3961 Loss: 8.009375927200284e-05\n",
      "Iteration: 3962 Loss: 8.000913456057773e-05\n",
      "Iteration: 3963 Loss: 7.992459926115764e-05\n",
      "Iteration: 3964 Loss: 7.984015327926867e-05\n",
      "Iteration: 3965 Loss: 7.975579652051896e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3966 Loss: 7.96715288906462e-05\n",
      "Iteration: 3967 Loss: 7.958735029548076e-05\n",
      "Iteration: 3968 Loss: 7.950326064098303e-05\n",
      "Iteration: 3969 Loss: 7.941925983315406e-05\n",
      "Iteration: 3970 Loss: 7.933534777811224e-05\n",
      "Iteration: 3971 Loss: 7.925152438212539e-05\n",
      "Iteration: 3972 Loss: 7.916778955143966e-05\n",
      "Iteration: 3973 Loss: 7.908414319274053e-05\n",
      "Iteration: 3974 Loss: 7.900058521216735e-05\n",
      "Iteration: 3975 Loss: 7.891711551652463e-05\n",
      "Iteration: 3976 Loss: 7.883373401252732e-05\n",
      "Iteration: 3977 Loss: 7.87504406069542e-05\n",
      "Iteration: 3978 Loss: 7.866723520678436e-05\n",
      "Iteration: 3979 Loss: 7.858411771899843e-05\n",
      "Iteration: 3980 Loss: 7.850108805073773e-05\n",
      "Iteration: 3981 Loss: 7.841814610921203e-05\n",
      "Iteration: 3982 Loss: 7.833529180169448e-05\n",
      "Iteration: 3983 Loss: 7.825252503540915e-05\n",
      "Iteration: 3984 Loss: 7.816984571830035e-05\n",
      "Iteration: 3985 Loss: 7.808725375769045e-05\n",
      "Iteration: 3986 Loss: 7.800474906136095e-05\n",
      "Iteration: 3987 Loss: 7.792233153707782e-05\n",
      "Iteration: 3988 Loss: 7.784000109271682e-05\n",
      "Iteration: 3989 Loss: 7.775775763629236e-05\n",
      "Iteration: 3990 Loss: 7.76756010759311e-05\n",
      "Iteration: 3991 Loss: 7.759353131976079e-05\n",
      "Iteration: 3992 Loss: 7.751154827610224e-05\n",
      "Iteration: 3993 Loss: 7.742965185333033e-05\n",
      "Iteration: 3994 Loss: 7.734784196001179e-05\n",
      "Iteration: 3995 Loss: 7.726611850463018e-05\n",
      "Iteration: 3996 Loss: 7.718448139592632e-05\n",
      "Iteration: 3997 Loss: 7.710293054239913e-05\n",
      "Iteration: 3998 Loss: 7.702146585324069e-05\n",
      "Iteration: 3999 Loss: 7.694008723731106e-05\n",
      "Iteration: 4000 Loss: 7.685879460365927e-05\n",
      "Iteration: 4001 Loss: 7.677758786134073e-05\n",
      "Iteration: 4002 Loss: 7.669646691976087e-05\n",
      "Iteration: 4003 Loss: 7.661543168819627e-05\n",
      "Iteration: 4004 Loss: 7.653448207609815e-05\n",
      "Iteration: 4005 Loss: 7.645361799297286e-05\n",
      "Iteration: 4006 Loss: 7.637283934843285e-05\n",
      "Iteration: 4007 Loss: 7.629214605229932e-05\n",
      "Iteration: 4008 Loss: 7.621153801435372e-05\n",
      "Iteration: 4009 Loss: 7.613101514452525e-05\n",
      "Iteration: 4010 Loss: 7.605057735289099e-05\n",
      "Iteration: 4011 Loss: 7.597022454920218e-05\n",
      "Iteration: 4012 Loss: 7.588995664411611e-05\n",
      "Iteration: 4013 Loss: 7.580977354774503e-05\n",
      "Iteration: 4014 Loss: 7.572967517051556e-05\n",
      "Iteration: 4015 Loss: 7.564966142286175e-05\n",
      "Iteration: 4016 Loss: 7.556973221538898e-05\n",
      "Iteration: 4017 Loss: 7.548988745875179e-05\n",
      "Iteration: 4018 Loss: 7.541012706374939e-05\n",
      "Iteration: 4019 Loss: 7.53304509413446e-05\n",
      "Iteration: 4020 Loss: 7.525085900233047e-05\n",
      "Iteration: 4021 Loss: 7.517135115786023e-05\n",
      "Iteration: 4022 Loss: 7.509192731906671e-05\n",
      "Iteration: 4023 Loss: 7.501258739717047e-05\n",
      "Iteration: 4024 Loss: 7.49333313035437e-05\n",
      "Iteration: 4025 Loss: 7.485415894959715e-05\n",
      "Iteration: 4026 Loss: 7.477507024681904e-05\n",
      "Iteration: 4027 Loss: 7.469606510685776e-05\n",
      "Iteration: 4028 Loss: 7.461714344141282e-05\n",
      "Iteration: 4029 Loss: 7.453830516230642e-05\n",
      "Iteration: 4030 Loss: 7.445955018143329e-05\n",
      "Iteration: 4031 Loss: 7.438087841079029e-05\n",
      "Iteration: 4032 Loss: 7.430228976241116e-05\n",
      "Iteration: 4033 Loss: 7.422378414850829e-05\n",
      "Iteration: 4034 Loss: 7.414536148133342e-05\n",
      "Iteration: 4035 Loss: 7.40670216732832e-05\n",
      "Iteration: 4036 Loss: 7.398876463697485e-05\n",
      "Iteration: 4037 Loss: 7.391059028439365e-05\n",
      "Iteration: 4038 Loss: 7.383249852871773e-05\n",
      "Iteration: 4039 Loss: 7.37544892824983e-05\n",
      "Iteration: 4040 Loss: 7.367656245859554e-05\n",
      "Iteration: 4041 Loss: 7.359871796989482e-05\n",
      "Iteration: 4042 Loss: 7.352095572937863e-05\n",
      "Iteration: 4043 Loss: 7.344327565018869e-05\n",
      "Iteration: 4044 Loss: 7.336567764554654e-05\n",
      "Iteration: 4045 Loss: 7.328816162864228e-05\n",
      "Iteration: 4046 Loss: 7.321072751291139e-05\n",
      "Iteration: 4047 Loss: 7.31333752118141e-05\n",
      "Iteration: 4048 Loss: 7.3056104638917e-05\n",
      "Iteration: 4049 Loss: 7.297891570785526e-05\n",
      "Iteration: 4050 Loss: 7.290180833237445e-05\n",
      "Iteration: 4051 Loss: 7.282478242629977e-05\n",
      "Iteration: 4052 Loss: 7.274783790355204e-05\n",
      "Iteration: 4053 Loss: 7.267097467811341e-05\n",
      "Iteration: 4054 Loss: 7.259419266409785e-05\n",
      "Iteration: 4055 Loss: 7.251749177574148e-05\n",
      "Iteration: 4056 Loss: 7.244087192731373e-05\n",
      "Iteration: 4057 Loss: 7.23643330331633e-05\n",
      "Iteration: 4058 Loss: 7.228787500779926e-05\n",
      "Iteration: 4059 Loss: 7.221149776576185e-05\n",
      "Iteration: 4060 Loss: 7.213520122169527e-05\n",
      "Iteration: 4061 Loss: 7.205898529029542e-05\n",
      "Iteration: 4062 Loss: 7.198284988642991e-05\n",
      "Iteration: 4063 Loss: 7.190679492494726e-05\n",
      "Iteration: 4064 Loss: 7.183082032091998e-05\n",
      "Iteration: 4065 Loss: 7.175492598947815e-05\n",
      "Iteration: 4066 Loss: 7.16791118457755e-05\n",
      "Iteration: 4067 Loss: 7.160337780507989e-05\n",
      "Iteration: 4068 Loss: 7.152772378276611e-05\n",
      "Iteration: 4069 Loss: 7.145214969430092e-05\n",
      "Iteration: 4070 Loss: 7.137665545521833e-05\n",
      "Iteration: 4071 Loss: 7.130124098114689e-05\n",
      "Iteration: 4072 Loss: 7.122590618781761e-05\n",
      "Iteration: 4073 Loss: 7.115065099099429e-05\n",
      "Iteration: 4074 Loss: 7.107547530662363e-05\n",
      "Iteration: 4075 Loss: 7.100037905067322e-05\n",
      "Iteration: 4076 Loss: 7.092536213924756e-05\n",
      "Iteration: 4077 Loss: 7.085042448850735e-05\n",
      "Iteration: 4078 Loss: 7.077556601471349e-05\n",
      "Iteration: 4079 Loss: 7.070078663416353e-05\n",
      "Iteration: 4080 Loss: 7.062608626351335e-05\n",
      "Iteration: 4081 Loss: 7.055146481877931e-05\n",
      "Iteration: 4082 Loss: 7.047692221707086e-05\n",
      "Iteration: 4083 Loss: 7.04024583749063e-05\n",
      "Iteration: 4084 Loss: 7.032807320900648e-05\n",
      "Iteration: 4085 Loss: 7.025376663631818e-05\n",
      "Iteration: 4086 Loss: 7.017953857379134e-05\n",
      "Iteration: 4087 Loss: 7.010538893856166e-05\n",
      "Iteration: 4088 Loss: 7.003131764756134e-05\n",
      "Iteration: 4089 Loss: 6.995732461816787e-05\n",
      "Iteration: 4090 Loss: 6.988340976765821e-05\n",
      "Iteration: 4091 Loss: 6.980957301338034e-05\n",
      "Iteration: 4092 Loss: 6.973581427287334e-05\n",
      "Iteration: 4093 Loss: 6.966213346368293e-05\n",
      "Iteration: 4094 Loss: 6.9588530503512e-05\n",
      "Iteration: 4095 Loss: 6.951500531014145e-05\n",
      "Iteration: 4096 Loss: 6.94415578012104e-05\n",
      "Iteration: 4097 Loss: 6.93681878948575e-05\n",
      "Iteration: 4098 Loss: 6.929489550901386e-05\n",
      "Iteration: 4099 Loss: 6.922168056170902e-05\n",
      "Iteration: 4100 Loss: 6.914854297126524e-05\n",
      "Iteration: 4101 Loss: 6.907548265579338e-05\n",
      "Iteration: 4102 Loss: 6.900249953373207e-05\n",
      "Iteration: 4103 Loss: 6.892959352359006e-05\n",
      "Iteration: 4104 Loss: 6.885676454386613e-05\n",
      "Iteration: 4105 Loss: 6.878401251300421e-05\n",
      "Iteration: 4106 Loss: 6.871133734977782e-05\n",
      "Iteration: 4107 Loss: 6.863873897289e-05\n",
      "Iteration: 4108 Loss: 6.856621730149729e-05\n",
      "Iteration: 4109 Loss: 6.849377225438937e-05\n",
      "Iteration: 4110 Loss: 6.842140375060519e-05\n",
      "Iteration: 4111 Loss: 6.834911170924174e-05\n",
      "Iteration: 4112 Loss: 6.827689604952217e-05\n",
      "Iteration: 4113 Loss: 6.820475669078348e-05\n",
      "Iteration: 4114 Loss: 6.813269355245083e-05\n",
      "Iteration: 4115 Loss: 6.806070655383185e-05\n",
      "Iteration: 4116 Loss: 6.798879561462114e-05\n",
      "Iteration: 4117 Loss: 6.791696065441241e-05\n",
      "Iteration: 4118 Loss: 6.784520159293044e-05\n",
      "Iteration: 4119 Loss: 6.77735183501494e-05\n",
      "Iteration: 4120 Loss: 6.770191084563024e-05\n",
      "Iteration: 4121 Loss: 6.76303789994291e-05\n",
      "Iteration: 4122 Loss: 6.75589227317218e-05\n",
      "Iteration: 4123 Loss: 6.748754196270162e-05\n",
      "Iteration: 4124 Loss: 6.741623661244924e-05\n",
      "Iteration: 4125 Loss: 6.734500660124498e-05\n",
      "Iteration: 4126 Loss: 6.727385184971218e-05\n",
      "Iteration: 4127 Loss: 6.720277227830576e-05\n",
      "Iteration: 4128 Loss: 6.713176780738904e-05\n",
      "Iteration: 4129 Loss: 6.706083835776545e-05\n",
      "Iteration: 4130 Loss: 6.69899838501341e-05\n",
      "Iteration: 4131 Loss: 6.691920420530879e-05\n",
      "Iteration: 4132 Loss: 6.684849934439918e-05\n",
      "Iteration: 4133 Loss: 6.677786918798712e-05\n",
      "Iteration: 4134 Loss: 6.670731365729835e-05\n",
      "Iteration: 4135 Loss: 6.663683267332916e-05\n",
      "Iteration: 4136 Loss: 6.656642615769766e-05\n",
      "Iteration: 4137 Loss: 6.649609403156072e-05\n",
      "Iteration: 4138 Loss: 6.642583621627705e-05\n",
      "Iteration: 4139 Loss: 6.635565263337418e-05\n",
      "Iteration: 4140 Loss: 6.62855432044047e-05\n",
      "Iteration: 4141 Loss: 6.621550785101556e-05\n",
      "Iteration: 4142 Loss: 6.614554649493932e-05\n",
      "Iteration: 4143 Loss: 6.607565905799747e-05\n",
      "Iteration: 4144 Loss: 6.600584546204457e-05\n",
      "Iteration: 4145 Loss: 6.593610562914145e-05\n",
      "Iteration: 4146 Loss: 6.58664394813182e-05\n",
      "Iteration: 4147 Loss: 6.579684694065813e-05\n",
      "Iteration: 4148 Loss: 6.572732792946958e-05\n",
      "Iteration: 4149 Loss: 6.565788237003091e-05\n",
      "Iteration: 4150 Loss: 6.558851018475224e-05\n",
      "Iteration: 4151 Loss: 6.551921129611223e-05\n",
      "Iteration: 4152 Loss: 6.54499856266596e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4153 Loss: 6.538083309903668e-05\n",
      "Iteration: 4154 Loss: 6.531175363595765e-05\n",
      "Iteration: 4155 Loss: 6.52427471602262e-05\n",
      "Iteration: 4156 Loss: 6.517381359472795e-05\n",
      "Iteration: 4157 Loss: 6.510495286243e-05\n",
      "Iteration: 4158 Loss: 6.503616488657573e-05\n",
      "Iteration: 4159 Loss: 6.496744958989352e-05\n",
      "Iteration: 4160 Loss: 6.489880689554986e-05\n",
      "Iteration: 4161 Loss: 6.483023672731877e-05\n",
      "Iteration: 4162 Loss: 6.476173900833196e-05\n",
      "Iteration: 4163 Loss: 6.469331366199755e-05\n",
      "Iteration: 4164 Loss: 6.462496061187726e-05\n",
      "Iteration: 4165 Loss: 6.455667978160417e-05\n",
      "Iteration: 4166 Loss: 6.448847109506362e-05\n",
      "Iteration: 4167 Loss: 6.442033447550023e-05\n",
      "Iteration: 4168 Loss: 6.435226984725118e-05\n",
      "Iteration: 4169 Loss: 6.428427713431304e-05\n",
      "Iteration: 4170 Loss: 6.42163562604945e-05\n",
      "Iteration: 4171 Loss: 6.414850714948981e-05\n",
      "Iteration: 4172 Loss: 6.40807297258425e-05\n",
      "Iteration: 4173 Loss: 6.401302391404601e-05\n",
      "Iteration: 4174 Loss: 6.394538963827494e-05\n",
      "Iteration: 4175 Loss: 6.387782682291422e-05\n",
      "Iteration: 4176 Loss: 6.381033539245139e-05\n",
      "Iteration: 4177 Loss: 6.374291527141911e-05\n",
      "Iteration: 4178 Loss: 6.367556638453476e-05\n",
      "Iteration: 4179 Loss: 6.360828865638772e-05\n",
      "Iteration: 4180 Loss: 6.3541082012151e-05\n",
      "Iteration: 4181 Loss: 6.347394637656316e-05\n",
      "Iteration: 4182 Loss: 6.3406881674347e-05\n",
      "Iteration: 4183 Loss: 6.333988783077759e-05\n",
      "Iteration: 4184 Loss: 6.327296477094721e-05\n",
      "Iteration: 4185 Loss: 6.320611242007972e-05\n",
      "Iteration: 4186 Loss: 6.313933070345771e-05\n",
      "Iteration: 4187 Loss: 6.307261954644962e-05\n",
      "Iteration: 4188 Loss: 6.300597887451175e-05\n",
      "Iteration: 4189 Loss: 6.293940861316421e-05\n",
      "Iteration: 4190 Loss: 6.287290868801604e-05\n",
      "Iteration: 4191 Loss: 6.280647902475349e-05\n",
      "Iteration: 4192 Loss: 6.274011954913684e-05\n",
      "Iteration: 4193 Loss: 6.267383018700993e-05\n",
      "Iteration: 4194 Loss: 6.26076108642543e-05\n",
      "Iteration: 4195 Loss: 6.254146150688625e-05\n",
      "Iteration: 4196 Loss: 6.247538204107885e-05\n",
      "Iteration: 4197 Loss: 6.24093723928355e-05\n",
      "Iteration: 4198 Loss: 6.234343248858545e-05\n",
      "Iteration: 4199 Loss: 6.227756225448479e-05\n",
      "Iteration: 4200 Loss: 6.221176161684428e-05\n",
      "Iteration: 4201 Loss: 6.214603050228071e-05\n",
      "Iteration: 4202 Loss: 6.208036883718432e-05\n",
      "Iteration: 4203 Loss: 6.201477654840579e-05\n",
      "Iteration: 4204 Loss: 6.194925356252846e-05\n",
      "Iteration: 4205 Loss: 6.188379980633044e-05\n",
      "Iteration: 4206 Loss: 6.181841520666574e-05\n",
      "Iteration: 4207 Loss: 6.175309969046329e-05\n",
      "Iteration: 4208 Loss: 6.168785318469433e-05\n",
      "Iteration: 4209 Loss: 6.16226756166377e-05\n",
      "Iteration: 4210 Loss: 6.155756691314732e-05\n",
      "Iteration: 4211 Loss: 6.14925270015951e-05\n",
      "Iteration: 4212 Loss: 6.142755580928487e-05\n",
      "Iteration: 4213 Loss: 6.136265326366857e-05\n",
      "Iteration: 4214 Loss: 6.12978192922123e-05\n",
      "Iteration: 4215 Loss: 6.123305382252974e-05\n",
      "Iteration: 4216 Loss: 6.116835678211807e-05\n",
      "Iteration: 4217 Loss: 6.110372809863931e-05\n",
      "Iteration: 4218 Loss: 6.103916769997744e-05\n",
      "Iteration: 4219 Loss: 6.097467551395713e-05\n",
      "Iteration: 4220 Loss: 6.091025146857958e-05\n",
      "Iteration: 4221 Loss: 6.084589549165286e-05\n",
      "Iteration: 4222 Loss: 6.078160751141989e-05\n",
      "Iteration: 4223 Loss: 6.0717387456070105e-05\n",
      "Iteration: 4224 Loss: 6.06532352536434e-05\n",
      "Iteration: 4225 Loss: 6.058915083260581e-05\n",
      "Iteration: 4226 Loss: 6.0525134121374875e-05\n",
      "Iteration: 4227 Loss: 6.046118504830427e-05\n",
      "Iteration: 4228 Loss: 6.03973035418787e-05\n",
      "Iteration: 4229 Loss: 6.033348953083488e-05\n",
      "Iteration: 4230 Loss: 6.026974294393203e-05\n",
      "Iteration: 4231 Loss: 6.020606370969781e-05\n",
      "Iteration: 4232 Loss: 6.014245175709024e-05\n",
      "Iteration: 4233 Loss: 6.007890701501983e-05\n",
      "Iteration: 4234 Loss: 6.001542941242981e-05\n",
      "Iteration: 4235 Loss: 5.9952018878467276e-05\n",
      "Iteration: 4236 Loss: 5.988867534217062e-05\n",
      "Iteration: 4237 Loss: 5.9825398732708564e-05\n",
      "Iteration: 4238 Loss: 5.976218897957704e-05\n",
      "Iteration: 4239 Loss: 5.9699046012111505e-05\n",
      "Iteration: 4240 Loss: 5.963596975955524e-05\n",
      "Iteration: 4241 Loss: 5.95729601515687e-05\n",
      "Iteration: 4242 Loss: 5.951001711777989e-05\n",
      "Iteration: 4243 Loss: 5.944714058776429e-05\n",
      "Iteration: 4244 Loss: 5.9384330491182726e-05\n",
      "Iteration: 4245 Loss: 5.9321586757998014e-05\n",
      "Iteration: 4246 Loss: 5.925890931797343e-05\n",
      "Iteration: 4247 Loss: 5.919629810103058e-05\n",
      "Iteration: 4248 Loss: 5.9133753037315126e-05\n",
      "Iteration: 4249 Loss: 5.907127405677669e-05\n",
      "Iteration: 4250 Loss: 5.900886108982441e-05\n",
      "Iteration: 4251 Loss: 5.89465140666724e-05\n",
      "Iteration: 4252 Loss: 5.888423291745417e-05\n",
      "Iteration: 4253 Loss: 5.882201757271776e-05\n",
      "Iteration: 4254 Loss: 5.875986796298625e-05\n",
      "Iteration: 4255 Loss: 5.86977840186846e-05\n",
      "Iteration: 4256 Loss: 5.863576567038968e-05\n",
      "Iteration: 4257 Loss: 5.857381284891583e-05\n",
      "Iteration: 4258 Loss: 5.851192548498431e-05\n",
      "Iteration: 4259 Loss: 5.845010350944357e-05\n",
      "Iteration: 4260 Loss: 5.838834685315972e-05\n",
      "Iteration: 4261 Loss: 5.832665544720024e-05\n",
      "Iteration: 4262 Loss: 5.8265029222582794e-05\n",
      "Iteration: 4263 Loss: 5.8203468110399495e-05\n",
      "Iteration: 4264 Loss: 5.814197204187732e-05\n",
      "Iteration: 4265 Loss: 5.80805409483161e-05\n",
      "Iteration: 4266 Loss: 5.8019174761061736e-05\n",
      "Iteration: 4267 Loss: 5.7957873411541e-05\n",
      "Iteration: 4268 Loss: 5.789663683124216e-05\n",
      "Iteration: 4269 Loss: 5.7835464951728856e-05\n",
      "Iteration: 4270 Loss: 5.77743577046432e-05\n",
      "Iteration: 4271 Loss: 5.771331502169492e-05\n",
      "Iteration: 4272 Loss: 5.765233683474632e-05\n",
      "Iteration: 4273 Loss: 5.759142307545484e-05\n",
      "Iteration: 4274 Loss: 5.753057367590955e-05\n",
      "Iteration: 4275 Loss: 5.746978856814332e-05\n",
      "Iteration: 4276 Loss: 5.740906768403551e-05\n",
      "Iteration: 4277 Loss: 5.7348410955882856e-05\n",
      "Iteration: 4278 Loss: 5.7287818315863356e-05\n",
      "Iteration: 4279 Loss: 5.722728969626237e-05\n",
      "Iteration: 4280 Loss: 5.716682502943721e-05\n",
      "Iteration: 4281 Loss: 5.7106424247815404e-05\n",
      "Iteration: 4282 Loss: 5.70460872839828e-05\n",
      "Iteration: 4283 Loss: 5.698581407031049e-05\n",
      "Iteration: 4284 Loss: 5.692560453960023e-05\n",
      "Iteration: 4285 Loss: 5.6865458624602175e-05\n",
      "Iteration: 4286 Loss: 5.680537625791075e-05\n",
      "Iteration: 4287 Loss: 5.674535737261726e-05\n",
      "Iteration: 4288 Loss: 5.668540190140967e-05\n",
      "Iteration: 4289 Loss: 5.662550977744356e-05\n",
      "Iteration: 4290 Loss: 5.6565680933832735e-05\n",
      "Iteration: 4291 Loss: 5.650591530352214e-05\n",
      "Iteration: 4292 Loss: 5.6446212819944184e-05\n",
      "Iteration: 4293 Loss: 5.6386573416161123e-05\n",
      "Iteration: 4294 Loss: 5.632699702587307e-05\n",
      "Iteration: 4295 Loss: 5.626748358206196e-05\n",
      "Iteration: 4296 Loss: 5.620803301858528e-05\n",
      "Iteration: 4297 Loss: 5.6148645268478907e-05\n",
      "Iteration: 4298 Loss: 5.608932026573833e-05\n",
      "Iteration: 4299 Loss: 5.603005794421187e-05\n",
      "Iteration: 4300 Loss: 5.59708582375442e-05\n",
      "Iteration: 4301 Loss: 5.5911721079564806e-05\n",
      "Iteration: 4302 Loss: 5.585264640416351e-05\n",
      "Iteration: 4303 Loss: 5.5793634145563026e-05\n",
      "Iteration: 4304 Loss: 5.5734684237277106e-05\n",
      "Iteration: 4305 Loss: 5.5675796613826686e-05\n",
      "Iteration: 4306 Loss: 5.561697120936465e-05\n",
      "Iteration: 4307 Loss: 5.555820795810905e-05\n",
      "Iteration: 4308 Loss: 5.5499506794395504e-05\n",
      "Iteration: 4309 Loss: 5.5440867652619814e-05\n",
      "Iteration: 4310 Loss: 5.5382290467254286e-05\n",
      "Iteration: 4311 Loss: 5.5323775172837546e-05\n",
      "Iteration: 4312 Loss: 5.526532170393573e-05\n",
      "Iteration: 4313 Loss: 5.520692999534695e-05\n",
      "Iteration: 4314 Loss: 5.514859998169703e-05\n",
      "Iteration: 4315 Loss: 5.509033159783974e-05\n",
      "Iteration: 4316 Loss: 5.503212477865838e-05\n",
      "Iteration: 4317 Loss: 5.4973979459109654e-05\n",
      "Iteration: 4318 Loss: 5.491589557428795e-05\n",
      "Iteration: 4319 Loss: 5.4857873059050836e-05\n",
      "Iteration: 4320 Loss: 5.479991184879516e-05\n",
      "Iteration: 4321 Loss: 5.4742011878662e-05\n",
      "Iteration: 4322 Loss: 5.4684173083950784e-05\n",
      "Iteration: 4323 Loss: 5.4626395400027724e-05\n",
      "Iteration: 4324 Loss: 5.4568678762322786e-05\n",
      "Iteration: 4325 Loss: 5.4511023106333714e-05\n",
      "Iteration: 4326 Loss: 5.445342836787392e-05\n",
      "Iteration: 4327 Loss: 5.439589448229788e-05\n",
      "Iteration: 4328 Loss: 5.4338421385147764e-05\n",
      "Iteration: 4329 Loss: 5.428100901259956e-05\n",
      "Iteration: 4330 Loss: 5.422365730008997e-05\n",
      "Iteration: 4331 Loss: 5.4166366183812225e-05\n",
      "Iteration: 4332 Loss: 5.410913559941682e-05\n",
      "Iteration: 4333 Loss: 5.4051965483388355e-05\n",
      "Iteration: 4334 Loss: 5.39948557715648e-05\n",
      "Iteration: 4335 Loss: 5.393780640023061e-05\n",
      "Iteration: 4336 Loss: 5.3880817305609746e-05\n",
      "Iteration: 4337 Loss: 5.382388842400357e-05\n",
      "Iteration: 4338 Loss: 5.3767019691870925e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4339 Loss: 5.3710211045349185e-05\n",
      "Iteration: 4340 Loss: 5.36534624211022e-05\n",
      "Iteration: 4341 Loss: 5.359677375604905e-05\n",
      "Iteration: 4342 Loss: 5.354014498658079e-05\n",
      "Iteration: 4343 Loss: 5.3483576049421335e-05\n",
      "Iteration: 4344 Loss: 5.342706688135541e-05\n",
      "Iteration: 4345 Loss: 5.337061741919018e-05\n",
      "Iteration: 4346 Loss: 5.3314227599922204e-05\n",
      "Iteration: 4347 Loss: 5.325789736045412e-05\n",
      "Iteration: 4348 Loss: 5.3201626637655754e-05\n",
      "Iteration: 4349 Loss: 5.314541536907415e-05\n",
      "Iteration: 4350 Loss: 5.308926349168126e-05\n",
      "Iteration: 4351 Loss: 5.303317094273736e-05\n",
      "Iteration: 4352 Loss: 5.297713765974875e-05\n",
      "Iteration: 4353 Loss: 5.292116357969262e-05\n",
      "Iteration: 4354 Loss: 5.28652486402234e-05\n",
      "Iteration: 4355 Loss: 5.28093927786433e-05\n",
      "Iteration: 4356 Loss: 5.275359593294258e-05\n",
      "Iteration: 4357 Loss: 5.269785804056185e-05\n",
      "Iteration: 4358 Loss: 5.2642179039219934e-05\n",
      "Iteration: 4359 Loss: 5.2586558866685054e-05\n",
      "Iteration: 4360 Loss: 5.253099746100688e-05\n",
      "Iteration: 4361 Loss: 5.247549475969006e-05\n",
      "Iteration: 4362 Loss: 5.242005070090883e-05\n",
      "Iteration: 4363 Loss: 5.2364665222708036e-05\n",
      "Iteration: 4364 Loss: 5.230933826318888e-05\n",
      "Iteration: 4365 Loss: 5.225406976052354e-05\n",
      "Iteration: 4366 Loss: 5.2198859652949544e-05\n",
      "Iteration: 4367 Loss: 5.214370787876746e-05\n",
      "Iteration: 4368 Loss: 5.20886143763415e-05\n",
      "Iteration: 4369 Loss: 5.2033579084106786e-05\n",
      "Iteration: 4370 Loss: 5.197860194055845e-05\n",
      "Iteration: 4371 Loss: 5.192368288425867e-05\n",
      "Iteration: 4372 Loss: 5.186882185383424e-05\n",
      "Iteration: 4373 Loss: 5.181401878797435e-05\n",
      "Iteration: 4374 Loss: 5.175927362543719e-05\n",
      "Iteration: 4375 Loss: 5.1704586305129384e-05\n",
      "Iteration: 4376 Loss: 5.164995676572945e-05\n",
      "Iteration: 4377 Loss: 5.159538494634742e-05\n",
      "Iteration: 4378 Loss: 5.1540870785959776e-05\n",
      "Iteration: 4379 Loss: 5.148641422365107e-05\n",
      "Iteration: 4380 Loss: 5.143201519863532e-05\n",
      "Iteration: 4381 Loss: 5.137767364992764e-05\n",
      "Iteration: 4382 Loss: 5.1323389517036826e-05\n",
      "Iteration: 4383 Loss: 5.126916273913868e-05\n",
      "Iteration: 4384 Loss: 5.121499325559408e-05\n",
      "Iteration: 4385 Loss: 5.116088100598995e-05\n",
      "Iteration: 4386 Loss: 5.110682592981073e-05\n",
      "Iteration: 4387 Loss: 5.1052827966651734e-05\n",
      "Iteration: 4388 Loss: 5.09988870561678e-05\n",
      "Iteration: 4389 Loss: 5.094500313807574e-05\n",
      "Iteration: 4390 Loss: 5.089117615216558e-05\n",
      "Iteration: 4391 Loss: 5.08374060382785e-05\n",
      "Iteration: 4392 Loss: 5.078369273644401e-05\n",
      "Iteration: 4393 Loss: 5.0730036186403995e-05\n",
      "Iteration: 4394 Loss: 5.067643632831318e-05\n",
      "Iteration: 4395 Loss: 5.062289310227061e-05\n",
      "Iteration: 4396 Loss: 5.0569406448440934e-05\n",
      "Iteration: 4397 Loss: 5.0515976307053946e-05\n",
      "Iteration: 4398 Loss: 5.0462602618394515e-05\n",
      "Iteration: 4399 Loss: 5.040928532282052e-05\n",
      "Iteration: 4400 Loss: 5.035602436082925e-05\n",
      "Iteration: 4401 Loss: 5.030281967277968e-05\n",
      "Iteration: 4402 Loss: 5.0249671199136235e-05\n",
      "Iteration: 4403 Loss: 5.019657888065785e-05\n",
      "Iteration: 4404 Loss: 5.014354265801973e-05\n",
      "Iteration: 4405 Loss: 5.0090562471909926e-05\n",
      "Iteration: 4406 Loss: 5.003763826311601e-05\n",
      "Iteration: 4407 Loss: 4.9984769972502764e-05\n",
      "Iteration: 4408 Loss: 4.993195754106567e-05\n",
      "Iteration: 4409 Loss: 4.987920090958597e-05\n",
      "Iteration: 4410 Loss: 4.982650001926412e-05\n",
      "Iteration: 4411 Loss: 4.9773854811168734e-05\n",
      "Iteration: 4412 Loss: 4.9721265226465994e-05\n",
      "Iteration: 4413 Loss: 4.966873120639029e-05\n",
      "Iteration: 4414 Loss: 4.961625269222524e-05\n",
      "Iteration: 4415 Loss: 4.9563829625334597e-05\n",
      "Iteration: 4416 Loss: 4.95114619471234e-05\n",
      "Iteration: 4417 Loss: 4.94591495990417e-05\n",
      "Iteration: 4418 Loss: 4.9406892522641806e-05\n",
      "Iteration: 4419 Loss: 4.935469065962466e-05\n",
      "Iteration: 4420 Loss: 4.9302543951496804e-05\n",
      "Iteration: 4421 Loss: 4.925045234008216e-05\n",
      "Iteration: 4422 Loss: 4.919841576712488e-05\n",
      "Iteration: 4423 Loss: 4.914643417451289e-05\n",
      "Iteration: 4424 Loss: 4.909450750414315e-05\n",
      "Iteration: 4425 Loss: 4.9042635697979603e-05\n",
      "Iteration: 4426 Loss: 4.899081869805388e-05\n",
      "Iteration: 4427 Loss: 4.893905644646098e-05\n",
      "Iteration: 4428 Loss: 4.888734888546829e-05\n",
      "Iteration: 4429 Loss: 4.8835695957065546e-05\n",
      "Iteration: 4430 Loss: 4.878409760360343e-05\n",
      "Iteration: 4431 Loss: 4.8732553767455686e-05\n",
      "Iteration: 4432 Loss: 4.868106439101148e-05\n",
      "Iteration: 4433 Loss: 4.862962941681193e-05\n",
      "Iteration: 4434 Loss: 4.857824878729421e-05\n",
      "Iteration: 4435 Loss: 4.852692244504977e-05\n",
      "Iteration: 4436 Loss: 4.847565033260962e-05\n",
      "Iteration: 4437 Loss: 4.8424432392906727e-05\n",
      "Iteration: 4438 Loss: 4.837326856858655e-05\n",
      "Iteration: 4439 Loss: 4.832215880240041e-05\n",
      "Iteration: 4440 Loss: 4.827110303740615e-05\n",
      "Iteration: 4441 Loss: 4.8220101216456836e-05\n",
      "Iteration: 4442 Loss: 4.816915328250322e-05\n",
      "Iteration: 4443 Loss: 4.811825917869451e-05\n",
      "Iteration: 4444 Loss: 4.806741884807478e-05\n",
      "Iteration: 4445 Loss: 4.8016632233867373e-05\n",
      "Iteration: 4446 Loss: 4.796589927931663e-05\n",
      "Iteration: 4447 Loss: 4.791521992772371e-05\n",
      "Iteration: 4448 Loss: 4.786459412252028e-05\n",
      "Iteration: 4449 Loss: 4.7814021806990885e-05\n",
      "Iteration: 4450 Loss: 4.776350292471604e-05\n",
      "Iteration: 4451 Loss: 4.771303741921949e-05\n",
      "Iteration: 4452 Loss: 4.766262523410455e-05\n",
      "Iteration: 4453 Loss: 4.7612266313036427e-05\n",
      "Iteration: 4454 Loss: 4.7561960599735695e-05\n",
      "Iteration: 4455 Loss: 4.7511708038062337e-05\n",
      "Iteration: 4456 Loss: 4.7461508571756606e-05\n",
      "Iteration: 4457 Loss: 4.7411362144659364e-05\n",
      "Iteration: 4458 Loss: 4.73612687008533e-05\n",
      "Iteration: 4459 Loss: 4.731122818427844e-05\n",
      "Iteration: 4460 Loss: 4.726124053909722e-05\n",
      "Iteration: 4461 Loss: 4.721130570940949e-05\n",
      "Iteration: 4462 Loss: 4.71614236394066e-05\n",
      "Iteration: 4463 Loss: 4.7111594273343687e-05\n",
      "Iteration: 4464 Loss: 4.706181755565416e-05\n",
      "Iteration: 4465 Loss: 4.701209343048001e-05\n",
      "Iteration: 4466 Loss: 4.6962421842370566e-05\n",
      "Iteration: 4467 Loss: 4.691280273580753e-05\n",
      "Iteration: 4468 Loss: 4.6863236055352316e-05\n",
      "Iteration: 4469 Loss: 4.68137217456052e-05\n",
      "Iteration: 4470 Loss: 4.676425975127715e-05\n",
      "Iteration: 4471 Loss: 4.6714850017013096e-05\n",
      "Iteration: 4472 Loss: 4.666549248763459e-05\n",
      "Iteration: 4473 Loss: 4.6616187107865553e-05\n",
      "Iteration: 4474 Loss: 4.656693382284075e-05\n",
      "Iteration: 4475 Loss: 4.651773257740446e-05\n",
      "Iteration: 4476 Loss: 4.646858331668178e-05\n",
      "Iteration: 4477 Loss: 4.641948598549141e-05\n",
      "Iteration: 4478 Loss: 4.6370440529102846e-05\n",
      "Iteration: 4479 Loss: 4.632144689275834e-05\n",
      "Iteration: 4480 Loss: 4.6272505021657895e-05\n",
      "Iteration: 4481 Loss: 4.622361486111426e-05\n",
      "Iteration: 4482 Loss: 4.617477635649119e-05\n",
      "Iteration: 4483 Loss: 4.612598945320858e-05\n",
      "Iteration: 4484 Loss: 4.607725409674858e-05\n",
      "Iteration: 4485 Loss: 4.602857023264556e-05\n",
      "Iteration: 4486 Loss: 4.597993780649624e-05\n",
      "Iteration: 4487 Loss: 4.5931356763950885e-05\n",
      "Iteration: 4488 Loss: 4.588282705071893e-05\n",
      "Iteration: 4489 Loss: 4.58343486125694e-05\n",
      "Iteration: 4490 Loss: 4.578592139532425e-05\n",
      "Iteration: 4491 Loss: 4.5737545344867815e-05\n",
      "Iteration: 4492 Loss: 4.568922040721999e-05\n",
      "Iteration: 4493 Loss: 4.564094652817159e-05\n",
      "Iteration: 4494 Loss: 4.559272365393454e-05\n",
      "Iteration: 4495 Loss: 4.554455173054361e-05\n",
      "Iteration: 4496 Loss: 4.5496430704242114e-05\n",
      "Iteration: 4497 Loss: 4.544836052121884e-05\n",
      "Iteration: 4498 Loss: 4.5400341127788295e-05\n",
      "Iteration: 4499 Loss: 4.535237247029256e-05\n",
      "Iteration: 4500 Loss: 4.5304454494962924e-05\n",
      "Iteration: 4501 Loss: 4.5256587148406977e-05\n",
      "Iteration: 4502 Loss: 4.520877037705563e-05\n",
      "Iteration: 4503 Loss: 4.516100412755476e-05\n",
      "Iteration: 4504 Loss: 4.511328834648141e-05\n",
      "Iteration: 4505 Loss: 4.5065622980509224e-05\n",
      "Iteration: 4506 Loss: 4.501800797637693e-05\n",
      "Iteration: 4507 Loss: 4.497044328087212e-05\n",
      "Iteration: 4508 Loss: 4.492292884083544e-05\n",
      "Iteration: 4509 Loss: 4.4875464603172727e-05\n",
      "Iteration: 4510 Loss: 4.482805051488533e-05\n",
      "Iteration: 4511 Loss: 4.478068652289874e-05\n",
      "Iteration: 4512 Loss: 4.473337257452965e-05\n",
      "Iteration: 4513 Loss: 4.4686108616496194e-05\n",
      "Iteration: 4514 Loss: 4.4638894596266994e-05\n",
      "Iteration: 4515 Loss: 4.459173046087736e-05\n",
      "Iteration: 4516 Loss: 4.454461615785775e-05\n",
      "Iteration: 4517 Loss: 4.449755163411739e-05\n",
      "Iteration: 4518 Loss: 4.445053683770161e-05\n",
      "Iteration: 4519 Loss: 4.4403571715630636e-05\n",
      "Iteration: 4520 Loss: 4.435665621553972e-05\n",
      "Iteration: 4521 Loss: 4.430979028499916e-05\n",
      "Iteration: 4522 Loss: 4.4262973871676604e-05\n",
      "Iteration: 4523 Loss: 4.4216206923212924e-05\n",
      "Iteration: 4524 Loss: 4.416948938734369e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4525 Loss: 4.4122821211860505e-05\n",
      "Iteration: 4526 Loss: 4.4076202344615804e-05\n",
      "Iteration: 4527 Loss: 4.4029632733505755e-05\n",
      "Iteration: 4528 Loss: 4.398311232649211e-05\n",
      "Iteration: 4529 Loss: 4.393664107158543e-05\n",
      "Iteration: 4530 Loss: 4.389021891685474e-05\n",
      "Iteration: 4531 Loss: 4.3843845810416104e-05\n",
      "Iteration: 4532 Loss: 4.3797521700453285e-05\n",
      "Iteration: 4533 Loss: 4.375124653523589e-05\n",
      "Iteration: 4534 Loss: 4.370502026301047e-05\n",
      "Iteration: 4535 Loss: 4.365884283235791e-05\n",
      "Iteration: 4536 Loss: 4.361271419107548e-05\n",
      "Iteration: 4537 Loss: 4.3566634288047266e-05\n",
      "Iteration: 4538 Loss: 4.352060307150432e-05\n",
      "Iteration: 4539 Loss: 4.347462049040461e-05\n",
      "Iteration: 4540 Loss: 4.342868649327293e-05\n",
      "Iteration: 4541 Loss: 4.338280102855221e-05\n",
      "Iteration: 4542 Loss: 4.333696404507753e-05\n",
      "Iteration: 4543 Loss: 4.329117549162193e-05\n",
      "Iteration: 4544 Loss: 4.324543531701935e-05\n",
      "Iteration: 4545 Loss: 4.319974347015624e-05\n",
      "Iteration: 4546 Loss: 4.315409989996177e-05\n",
      "Iteration: 4547 Loss: 4.310850455543705e-05\n",
      "Iteration: 4548 Loss: 4.306295738562679e-05\n",
      "Iteration: 4549 Loss: 4.3017458339626565e-05\n",
      "Iteration: 4550 Loss: 4.2972007366439884e-05\n",
      "Iteration: 4551 Loss: 4.292660441552291e-05\n",
      "Iteration: 4552 Loss: 4.288124943604975e-05\n",
      "Iteration: 4553 Loss: 4.2835942377324564e-05\n",
      "Iteration: 4554 Loss: 4.2790683188808134e-05\n",
      "Iteration: 4555 Loss: 4.274547181979513e-05\n",
      "Iteration: 4556 Loss: 4.270030821979816e-05\n",
      "Iteration: 4557 Loss: 4.265519233826326e-05\n",
      "Iteration: 4558 Loss: 4.261012412489694e-05\n",
      "Iteration: 4559 Loss: 4.256510352936839e-05\n",
      "Iteration: 4560 Loss: 4.252013050124954e-05\n",
      "Iteration: 4561 Loss: 4.247520499036844e-05\n",
      "Iteration: 4562 Loss: 4.243032694646603e-05\n",
      "Iteration: 4563 Loss: 4.2385496319322366e-05\n",
      "Iteration: 4564 Loss: 4.2340713058911495e-05\n",
      "Iteration: 4565 Loss: 4.2295977115229705e-05\n",
      "Iteration: 4566 Loss: 4.225128843824315e-05\n",
      "Iteration: 4567 Loss: 4.220664697793368e-05\n",
      "Iteration: 4568 Loss: 4.216205268452603e-05\n",
      "Iteration: 4569 Loss: 4.2117505508038623e-05\n",
      "Iteration: 4570 Loss: 4.207300539891224e-05\n",
      "Iteration: 4571 Loss: 4.202855230742248e-05\n",
      "Iteration: 4572 Loss: 4.198414618385629e-05\n",
      "Iteration: 4573 Loss: 4.193978697830702e-05\n",
      "Iteration: 4574 Loss: 4.1895474641204125e-05\n",
      "Iteration: 4575 Loss: 4.185120912342488e-05\n",
      "Iteration: 4576 Loss: 4.1806990375379294e-05\n",
      "Iteration: 4577 Loss: 4.176281834753949e-05\n",
      "Iteration: 4578 Loss: 4.171869299057438e-05\n",
      "Iteration: 4579 Loss: 4.1674614255179925e-05\n",
      "Iteration: 4580 Loss: 4.163058209209168e-05\n",
      "Iteration: 4581 Loss: 4.158659645210576e-05\n",
      "Iteration: 4582 Loss: 4.1542657285982406e-05\n",
      "Iteration: 4583 Loss: 4.149876454474534e-05\n",
      "Iteration: 4584 Loss: 4.145491817929966e-05\n",
      "Iteration: 4585 Loss: 4.141111814064334e-05\n",
      "Iteration: 4586 Loss: 4.136736437983425e-05\n",
      "Iteration: 4587 Loss: 4.1323656847974535e-05\n",
      "Iteration: 4588 Loss: 4.127999549622091e-05\n",
      "Iteration: 4589 Loss: 4.123638027577769e-05\n",
      "Iteration: 4590 Loss: 4.1192811137905556e-05\n",
      "Iteration: 4591 Loss: 4.114928803391473e-05\n",
      "Iteration: 4592 Loss: 4.110581091516644e-05\n",
      "Iteration: 4593 Loss: 4.106237973308267e-05\n",
      "Iteration: 4594 Loss: 4.101899443911271e-05\n",
      "Iteration: 4595 Loss: 4.097565498478689e-05\n",
      "Iteration: 4596 Loss: 4.093236132165958e-05\n",
      "Iteration: 4597 Loss: 4.0889113401363306e-05\n",
      "Iteration: 4598 Loss: 4.0845911175753805e-05\n",
      "Iteration: 4599 Loss: 4.080275459636162e-05\n",
      "Iteration: 4600 Loss: 4.075964361475241e-05\n",
      "Iteration: 4601 Loss: 4.071657818274826e-05\n",
      "Iteration: 4602 Loss: 4.067355825242441e-05\n",
      "Iteration: 4603 Loss: 4.063058377591029e-05\n",
      "Iteration: 4604 Loss: 4.058765470497296e-05\n",
      "Iteration: 4605 Loss: 4.054477099164235e-05\n",
      "Iteration: 4606 Loss: 4.050193258799673e-05\n",
      "Iteration: 4607 Loss: 4.0459139446156174e-05\n",
      "Iteration: 4608 Loss: 4.041639151830617e-05\n",
      "Iteration: 4609 Loss: 4.037368875687e-05\n",
      "Iteration: 4610 Loss: 4.033103111352344e-05\n",
      "Iteration: 4611 Loss: 4.028841854120814e-05\n",
      "Iteration: 4612 Loss: 4.024585099209256e-05\n",
      "Iteration: 4613 Loss: 4.020332841860661e-05\n",
      "Iteration: 4614 Loss: 4.016085077323648e-05\n",
      "Iteration: 4615 Loss: 4.011841800850447e-05\n",
      "Iteration: 4616 Loss: 4.0076030077002026e-05\n",
      "Iteration: 4617 Loss: 4.003368693134947e-05\n",
      "Iteration: 4618 Loss: 3.99913885244338e-05\n",
      "Iteration: 4619 Loss: 3.994913480858126e-05\n",
      "Iteration: 4620 Loss: 3.990692573697718e-05\n",
      "Iteration: 4621 Loss: 3.986476126204415e-05\n",
      "Iteration: 4622 Loss: 3.982264133666691e-05\n",
      "Iteration: 4623 Loss: 3.978056591417804e-05\n",
      "Iteration: 4624 Loss: 3.9738534947354234e-05\n",
      "Iteration: 4625 Loss: 3.969654838942796e-05\n",
      "Iteration: 4626 Loss: 3.965460619315502e-05\n",
      "Iteration: 4627 Loss: 3.9612708311540984e-05\n",
      "Iteration: 4628 Loss: 3.957085469813207e-05\n",
      "Iteration: 4629 Loss: 3.952904530606982e-05\n",
      "Iteration: 4630 Loss: 3.948728008859311e-05\n",
      "Iteration: 4631 Loss: 3.9445558999031545e-05\n",
      "Iteration: 4632 Loss: 3.94038819907543e-05\n",
      "Iteration: 4633 Loss: 3.9362249017191226e-05\n",
      "Iteration: 4634 Loss: 3.932066003181592e-05\n",
      "Iteration: 4635 Loss: 3.9279114988351104e-05\n",
      "Iteration: 4636 Loss: 3.923761383976493e-05\n",
      "Iteration: 4637 Loss: 3.919615654008993e-05\n",
      "Iteration: 4638 Loss: 3.9154743043186826e-05\n",
      "Iteration: 4639 Loss: 3.911337330257837e-05\n",
      "Iteration: 4640 Loss: 3.907204727203459e-05\n",
      "Iteration: 4641 Loss: 3.903076490536902e-05\n",
      "Iteration: 4642 Loss: 3.898952615644867e-05\n",
      "Iteration: 4643 Loss: 3.894833097919122e-05\n",
      "Iteration: 4644 Loss: 3.89071793275571e-05\n",
      "Iteration: 4645 Loss: 3.886607115556072e-05\n",
      "Iteration: 4646 Loss: 3.882500641725956e-05\n",
      "Iteration: 4647 Loss: 3.878398506676506e-05\n",
      "Iteration: 4648 Loss: 3.874300705823441e-05\n",
      "Iteration: 4649 Loss: 3.870207234587425e-05\n",
      "Iteration: 4650 Loss: 3.866118088393826e-05\n",
      "Iteration: 4651 Loss: 3.8620332626731835e-05\n",
      "Iteration: 4652 Loss: 3.857952752860194e-05\n",
      "Iteration: 4653 Loss: 3.853876554395075e-05\n",
      "Iteration: 4654 Loss: 3.849804662722459e-05\n",
      "Iteration: 4655 Loss: 3.845737073292087e-05\n",
      "Iteration: 4656 Loss: 3.8416737815582454e-05\n",
      "Iteration: 4657 Loss: 3.8376147829799745e-05\n",
      "Iteration: 4658 Loss: 3.833560073021249e-05\n",
      "Iteration: 4659 Loss: 3.829509647151302e-05\n",
      "Iteration: 4660 Loss: 3.825463500842805e-05\n",
      "Iteration: 4661 Loss: 3.8214216295751557e-05\n",
      "Iteration: 4662 Loss: 3.817384028830862e-05\n",
      "Iteration: 4663 Loss: 3.8133506941178165e-05\n",
      "Iteration: 4664 Loss: 3.809321620888793e-05\n",
      "Iteration: 4665 Loss: 3.80529680466115e-05\n",
      "Iteration: 4666 Loss: 3.801276240937046e-05\n",
      "Iteration: 4667 Loss: 3.797259925223611e-05\n",
      "Iteration: 4668 Loss: 3.793247853032246e-05\n",
      "Iteration: 4669 Loss: 3.78924001987972e-05\n",
      "Iteration: 4670 Loss: 3.7852364212865185e-05\n",
      "Iteration: 4671 Loss: 3.7812370527590045e-05\n",
      "Iteration: 4672 Loss: 3.777241909868066e-05\n",
      "Iteration: 4673 Loss: 3.773250988128646e-05\n",
      "Iteration: 4674 Loss: 3.7692642830808364e-05\n",
      "Iteration: 4675 Loss: 3.7652817902693786e-05\n",
      "Iteration: 4676 Loss: 3.7613035052438125e-05\n",
      "Iteration: 4677 Loss: 3.7573294235584784e-05\n",
      "Iteration: 4678 Loss: 3.753359540771976e-05\n",
      "Iteration: 4679 Loss: 3.7493938524481765e-05\n",
      "Iteration: 4680 Loss: 3.7454323541546925e-05\n",
      "Iteration: 4681 Loss: 3.74147504146518e-05\n",
      "Iteration: 4682 Loss: 3.7375219099571616e-05\n",
      "Iteration: 4683 Loss: 3.733572955212502e-05\n",
      "Iteration: 4684 Loss: 3.729628172818213e-05\n",
      "Iteration: 4685 Loss: 3.725687558366763e-05\n",
      "Iteration: 4686 Loss: 3.7217511074533333e-05\n",
      "Iteration: 4687 Loss: 3.7178188156789664e-05\n",
      "Iteration: 4688 Loss: 3.713890678650114e-05\n",
      "Iteration: 4689 Loss: 3.709966691976237e-05\n",
      "Iteration: 4690 Loss: 3.70604685130057e-05\n",
      "Iteration: 4691 Loss: 3.702131152210361e-05\n",
      "Iteration: 4692 Loss: 3.6982195903133226e-05\n",
      "Iteration: 4693 Loss: 3.694312161230597e-05\n",
      "Iteration: 4694 Loss: 3.690408860647867e-05\n",
      "Iteration: 4695 Loss: 3.686509684158928e-05\n",
      "Iteration: 4696 Loss: 3.682614627446155e-05\n",
      "Iteration: 4697 Loss: 3.678723686136902e-05\n",
      "Iteration: 4698 Loss: 3.6748368558827074e-05\n",
      "Iteration: 4699 Loss: 3.670954132340539e-05\n",
      "Iteration: 4700 Loss: 3.667075511170661e-05\n",
      "Iteration: 4701 Loss: 3.6632009880390415e-05\n",
      "Iteration: 4702 Loss: 3.6593305586153245e-05\n",
      "Iteration: 4703 Loss: 3.65546421857492e-05\n",
      "Iteration: 4704 Loss: 3.651601963596952e-05\n",
      "Iteration: 4705 Loss: 3.6477437893648435e-05\n",
      "Iteration: 4706 Loss: 3.6438896915672726e-05\n",
      "Iteration: 4707 Loss: 3.640039665897445e-05\n",
      "Iteration: 4708 Loss: 3.6361937080522993e-05\n",
      "Iteration: 4709 Loss: 3.6323518137341935e-05\n",
      "Iteration: 4710 Loss: 3.628513978649983e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4711 Loss: 3.6246801985102736e-05\n",
      "Iteration: 4712 Loss: 3.6208504690309085e-05\n",
      "Iteration: 4713 Loss: 3.6170247859321176e-05\n",
      "Iteration: 4714 Loss: 3.6132031449389987e-05\n",
      "Iteration: 4715 Loss: 3.609385541780313e-05\n",
      "Iteration: 4716 Loss: 3.60557197220992e-05\n",
      "Iteration: 4717 Loss: 3.6017624319257395e-05\n",
      "Iteration: 4718 Loss: 3.5979569166911744e-05\n",
      "Iteration: 4719 Loss: 3.5941554222731565e-05\n",
      "Iteration: 4720 Loss: 3.590357944383589e-05\n",
      "Iteration: 4721 Loss: 3.586564478797999e-05\n",
      "Iteration: 4722 Loss: 3.5827750212779724e-05\n",
      "Iteration: 4723 Loss: 3.578989567588383e-05\n",
      "Iteration: 4724 Loss: 3.5752081134988985e-05\n",
      "Iteration: 4725 Loss: 3.5714306547834674e-05\n",
      "Iteration: 4726 Loss: 3.567657187221009e-05\n",
      "Iteration: 4727 Loss: 3.5638877065944556e-05\n",
      "Iteration: 4728 Loss: 3.5601222086713406e-05\n",
      "Iteration: 4729 Loss: 3.556360689283534e-05\n",
      "Iteration: 4730 Loss: 3.552603144167335e-05\n",
      "Iteration: 4731 Loss: 3.548849569204187e-05\n",
      "Iteration: 4732 Loss: 3.545099960179116e-05\n",
      "Iteration: 4733 Loss: 3.5413543128413054e-05\n",
      "Iteration: 4734 Loss: 3.537612623065724e-05\n",
      "Iteration: 4735 Loss: 3.5338748866502554e-05\n",
      "Iteration: 4736 Loss: 3.530141099418478e-05\n",
      "Iteration: 4737 Loss: 3.5264112571772535e-05\n",
      "Iteration: 4738 Loss: 3.522685355814782e-05\n",
      "Iteration: 4739 Loss: 3.5189633911154097e-05\n",
      "Iteration: 4740 Loss: 3.515245358955295e-05\n",
      "Iteration: 4741 Loss: 3.51153125513978e-05\n",
      "Iteration: 4742 Loss: 3.507821075538155e-05\n",
      "Iteration: 4743 Loss: 3.5041148160041966e-05\n",
      "Iteration: 4744 Loss: 3.500412472395881e-05\n",
      "Iteration: 4745 Loss: 3.496714040576267e-05\n",
      "Iteration: 4746 Loss: 3.493019516411966e-05\n",
      "Iteration: 4747 Loss: 3.489328895774209e-05\n",
      "Iteration: 4748 Loss: 3.485642174538711e-05\n",
      "Iteration: 4749 Loss: 3.481959348585334e-05\n",
      "Iteration: 4750 Loss: 3.478280413798629e-05\n",
      "Iteration: 4751 Loss: 3.47460536606759e-05\n",
      "Iteration: 4752 Loss: 3.470934201284515e-05\n",
      "Iteration: 4753 Loss: 3.467266915347531e-05\n",
      "Iteration: 4754 Loss: 3.463603504157779e-05\n",
      "Iteration: 4755 Loss: 3.4599439636216644e-05\n",
      "Iteration: 4756 Loss: 3.4562882896494635e-05\n",
      "Iteration: 4757 Loss: 3.452636478156093e-05\n",
      "Iteration: 4758 Loss: 3.448988525060338e-05\n",
      "Iteration: 4759 Loss: 3.4453444262852286e-05\n",
      "Iteration: 4760 Loss: 3.441704177759093e-05\n",
      "Iteration: 4761 Loss: 3.43806777541321e-05\n",
      "Iteration: 4762 Loss: 3.4344352151883997e-05\n",
      "Iteration: 4763 Loss: 3.430806493016741e-05\n",
      "Iteration: 4764 Loss: 3.427181604847279e-05\n",
      "Iteration: 4765 Loss: 3.4235605466491403e-05\n",
      "Iteration: 4766 Loss: 3.419943314315673e-05\n",
      "Iteration: 4767 Loss: 3.416329903864143e-05\n",
      "Iteration: 4768 Loss: 3.412720311237087e-05\n",
      "Iteration: 4769 Loss: 3.409114532411956e-05\n",
      "Iteration: 4770 Loss: 3.4055125633361724e-05\n",
      "Iteration: 4771 Loss: 3.401914399996084e-05\n",
      "Iteration: 4772 Loss: 3.398320038370499e-05\n",
      "Iteration: 4773 Loss: 3.394729474442637e-05\n",
      "Iteration: 4774 Loss: 3.3911427042001744e-05\n",
      "Iteration: 4775 Loss: 3.387559723634736e-05\n",
      "Iteration: 4776 Loss: 3.3839805287419734e-05\n",
      "Iteration: 4777 Loss: 3.38040511551082e-05\n",
      "Iteration: 4778 Loss: 3.3768334799687064e-05\n",
      "Iteration: 4779 Loss: 3.373265618112561e-05\n",
      "Iteration: 4780 Loss: 3.369701525955596e-05\n",
      "Iteration: 4781 Loss: 3.3661411995142106e-05\n",
      "Iteration: 4782 Loss: 3.362584634810098e-05\n",
      "Iteration: 4783 Loss: 3.359031827868868e-05\n",
      "Iteration: 4784 Loss: 3.355482774719746e-05\n",
      "Iteration: 4785 Loss: 3.351937471396967e-05\n",
      "Iteration: 4786 Loss: 3.348395913938336e-05\n",
      "Iteration: 4787 Loss: 3.344858098386256e-05\n",
      "Iteration: 4788 Loss: 3.341324020786778e-05\n",
      "Iteration: 4789 Loss: 3.337793677190908e-05\n",
      "Iteration: 4790 Loss: 3.334267063653332e-05\n",
      "Iteration: 4791 Loss: 3.330744176232815e-05\n",
      "Iteration: 4792 Loss: 3.327225010992318e-05\n",
      "Iteration: 4793 Loss: 3.3237095639996425e-05\n",
      "Iteration: 4794 Loss: 3.320197831325678e-05\n",
      "Iteration: 4795 Loss: 3.316689809046177e-05\n",
      "Iteration: 4796 Loss: 3.3131854932409214e-05\n",
      "Iteration: 4797 Loss: 3.309684879993534e-05\n",
      "Iteration: 4798 Loss: 3.306187965392215e-05\n",
      "Iteration: 4799 Loss: 3.302694745528933e-05\n",
      "Iteration: 4800 Loss: 3.2992052165000284e-05\n",
      "Iteration: 4801 Loss: 3.295719374405799e-05\n",
      "Iteration: 4802 Loss: 3.292237215350735e-05\n",
      "Iteration: 4803 Loss: 3.288758735443729e-05\n",
      "Iteration: 4804 Loss: 3.2852839307970803e-05\n",
      "Iteration: 4805 Loss: 3.2818127975276857e-05\n",
      "Iteration: 4806 Loss: 3.27834533175682e-05\n",
      "Iteration: 4807 Loss: 3.27488152960957e-05\n",
      "Iteration: 4808 Loss: 3.2714213872143044e-05\n",
      "Iteration: 4809 Loss: 3.267964900704826e-05\n",
      "Iteration: 4810 Loss: 3.2645120662182655e-05\n",
      "Iteration: 4811 Loss: 3.2610628798962424e-05\n",
      "Iteration: 4812 Loss: 3.2576173378838604e-05\n",
      "Iteration: 4813 Loss: 3.25417543633813e-05\n",
      "Iteration: 4814 Loss: 3.250737171398084e-05\n",
      "Iteration: 4815 Loss: 3.247302539228642e-05\n",
      "Iteration: 4816 Loss: 3.243871535991237e-05\n",
      "Iteration: 4817 Loss: 3.2404441578519705e-05\n",
      "Iteration: 4818 Loss: 3.237020400968988e-05\n",
      "Iteration: 4819 Loss: 3.233600261543617e-05\n",
      "Iteration: 4820 Loss: 3.230183735733805e-05\n",
      "Iteration: 4821 Loss: 3.226770819725608e-05\n",
      "Iteration: 4822 Loss: 3.2233615097254226e-05\n",
      "Iteration: 4823 Loss: 3.219955801882576e-05\n",
      "Iteration: 4824 Loss: 3.2165536924113495e-05\n",
      "Iteration: 4825 Loss: 3.2131551775100804e-05\n",
      "Iteration: 4826 Loss: 3.209760253380381e-05\n",
      "Iteration: 4827 Loss: 3.2063689162088285e-05\n",
      "Iteration: 4828 Loss: 3.202981162256655e-05\n",
      "Iteration: 4829 Loss: 3.1995969876952074e-05\n",
      "Iteration: 4830 Loss: 3.196216388754048e-05\n",
      "Iteration: 4831 Loss: 3.192839361655212e-05\n",
      "Iteration: 4832 Loss: 3.189465902625047e-05\n",
      "Iteration: 4833 Loss: 3.186096007881809e-05\n",
      "Iteration: 4834 Loss: 3.182729673682591e-05\n",
      "Iteration: 4835 Loss: 3.179366896254062e-05\n",
      "Iteration: 4836 Loss: 3.176007671849765e-05\n",
      "Iteration: 4837 Loss: 3.172651996681117e-05\n",
      "Iteration: 4838 Loss: 3.169299867032592e-05\n",
      "Iteration: 4839 Loss: 3.165951279146544e-05\n",
      "Iteration: 4840 Loss: 3.162606229281128e-05\n",
      "Iteration: 4841 Loss: 3.159264713697627e-05\n",
      "Iteration: 4842 Loss: 3.1559267286624244e-05\n",
      "Iteration: 4843 Loss: 3.152592270444857e-05\n",
      "Iteration: 4844 Loss: 3.149261335318752e-05\n",
      "Iteration: 4845 Loss: 3.145933919561657e-05\n",
      "Iteration: 4846 Loss: 3.142610019466678e-05\n",
      "Iteration: 4847 Loss: 3.1392896312961914e-05\n",
      "Iteration: 4848 Loss: 3.1359727513511716e-05\n",
      "Iteration: 4849 Loss: 3.1326593759247225e-05\n",
      "Iteration: 4850 Loss: 3.129349501314481e-05\n",
      "Iteration: 4851 Loss: 3.1260431238210474e-05\n",
      "Iteration: 4852 Loss: 3.122740239749837e-05\n",
      "Iteration: 4853 Loss: 3.119440845409808e-05\n",
      "Iteration: 4854 Loss: 3.116144937113747e-05\n",
      "Iteration: 4855 Loss: 3.112852511178421e-05\n",
      "Iteration: 4856 Loss: 3.109563563924412e-05\n",
      "Iteration: 4857 Loss: 3.1062780916761976e-05\n",
      "Iteration: 4858 Loss: 3.102996090762143e-05\n",
      "Iteration: 4859 Loss: 3.099717557514895e-05\n",
      "Iteration: 4860 Loss: 3.0964424882702437e-05\n",
      "Iteration: 4861 Loss: 3.09317087936845e-05\n",
      "Iteration: 4862 Loss: 3.089902727153263e-05\n",
      "Iteration: 4863 Loss: 3.0866380279609105e-05\n",
      "Iteration: 4864 Loss: 3.0833767781742915e-05\n",
      "Iteration: 4865 Loss: 3.0801189741171294e-05\n",
      "Iteration: 4866 Loss: 3.0768646121647505e-05\n",
      "Iteration: 4867 Loss: 3.073613688687905e-05\n",
      "Iteration: 4868 Loss: 3.0703662000306285e-05\n",
      "Iteration: 4869 Loss: 3.067122142575453e-05\n",
      "Iteration: 4870 Loss: 3.063881512696723e-05\n",
      "Iteration: 4871 Loss: 3.060644306773209e-05\n",
      "Iteration: 4872 Loss: 3.05741052117554e-05\n",
      "Iteration: 4873 Loss: 3.0541801523131494e-05\n",
      "Iteration: 4874 Loss: 3.0509531965643738e-05\n",
      "Iteration: 4875 Loss: 3.047729650331242e-05\n",
      "Iteration: 4876 Loss: 3.0445095099989143e-05\n",
      "Iteration: 4877 Loss: 3.04129277196493e-05\n",
      "Iteration: 4878 Loss: 3.0380794326469594e-05\n",
      "Iteration: 4879 Loss: 3.034869488449502e-05\n",
      "Iteration: 4880 Loss: 3.0316629357857762e-05\n",
      "Iteration: 4881 Loss: 3.0284597710720616e-05\n",
      "Iteration: 4882 Loss: 3.0252599907289214e-05\n",
      "Iteration: 4883 Loss: 3.0220635911805207e-05\n",
      "Iteration: 4884 Loss: 3.018870568854678e-05\n",
      "Iteration: 4885 Loss: 3.015680920203271e-05\n",
      "Iteration: 4886 Loss: 3.012494641621796e-05\n",
      "Iteration: 4887 Loss: 3.009311729569284e-05\n",
      "Iteration: 4888 Loss: 3.0061321804889772e-05\n",
      "Iteration: 4889 Loss: 3.0029559908277284e-05\n",
      "Iteration: 4890 Loss: 2.999783157035878e-05\n",
      "Iteration: 4891 Loss: 2.9966136755477613e-05\n",
      "Iteration: 4892 Loss: 2.9934475428612672e-05\n",
      "Iteration: 4893 Loss: 2.9902847554183835e-05\n",
      "Iteration: 4894 Loss: 2.987125309705067e-05\n",
      "Iteration: 4895 Loss: 2.9839692021497662e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4896 Loss: 2.9808164292658898e-05\n",
      "Iteration: 4897 Loss: 2.9776669874696495e-05\n",
      "Iteration: 4898 Loss: 2.974520873281934e-05\n",
      "Iteration: 4899 Loss: 2.971378083218521e-05\n",
      "Iteration: 4900 Loss: 2.9682386137240978e-05\n",
      "Iteration: 4901 Loss: 2.9651024612899915e-05\n",
      "Iteration: 4902 Loss: 2.9619696224347985e-05\n",
      "Iteration: 4903 Loss: 2.958840093645814e-05\n",
      "Iteration: 4904 Loss: 2.9557138714255917e-05\n",
      "Iteration: 4905 Loss: 2.952590952281175e-05\n",
      "Iteration: 4906 Loss: 2.9494713327218375e-05\n",
      "Iteration: 4907 Loss: 2.9463550092617845e-05\n",
      "Iteration: 4908 Loss: 2.943241978418171e-05\n",
      "Iteration: 4909 Loss: 2.9401322367124648e-05\n",
      "Iteration: 4910 Loss: 2.9370257806692723e-05\n",
      "Iteration: 4911 Loss: 2.9339226068167405e-05\n",
      "Iteration: 4912 Loss: 2.9308227116874235e-05\n",
      "Iteration: 4913 Loss: 2.927726091817251e-05\n",
      "Iteration: 4914 Loss: 2.9246327437453393e-05\n",
      "Iteration: 4915 Loss: 2.921542664014801e-05\n",
      "Iteration: 4916 Loss: 2.9184558491723822e-05\n",
      "Iteration: 4917 Loss: 2.9153722957687223e-05\n",
      "Iteration: 4918 Loss: 2.912292000357855e-05\n",
      "Iteration: 4919 Loss: 2.909214959497174e-05\n",
      "Iteration: 4920 Loss: 2.9061411697483032e-05\n",
      "Iteration: 4921 Loss: 2.9030706276763302e-05\n",
      "Iteration: 4922 Loss: 2.9000033298496173e-05\n",
      "Iteration: 4923 Loss: 2.8969392728404508e-05\n",
      "Iteration: 4924 Loss: 2.893878453224633e-05\n",
      "Iteration: 4925 Loss: 2.890820867581686e-05\n",
      "Iteration: 4926 Loss: 2.887766512494523e-05\n",
      "Iteration: 4927 Loss: 2.8847153845499755e-05\n",
      "Iteration: 4928 Loss: 2.8816674803382883e-05\n",
      "Iteration: 4929 Loss: 2.8786227964537277e-05\n",
      "Iteration: 4930 Loss: 2.875581329493059e-05\n",
      "Iteration: 4931 Loss: 2.8725430760582515e-05\n",
      "Iteration: 4932 Loss: 2.8695080327531626e-05\n",
      "Iteration: 4933 Loss: 2.8664761961864432e-05\n",
      "Iteration: 4934 Loss: 2.8634475629699594e-05\n",
      "Iteration: 4935 Loss: 2.8604221297192383e-05\n",
      "Iteration: 4936 Loss: 2.8573998930530005e-05\n",
      "Iteration: 4937 Loss: 2.8543808495941346e-05\n",
      "Iteration: 4938 Loss: 2.8513649959685156e-05\n",
      "Iteration: 4939 Loss: 2.8483523288059966e-05\n",
      "Iteration: 4940 Loss: 2.8453428447399693e-05\n",
      "Iteration: 4941 Loss: 2.8423365404069787e-05\n",
      "Iteration: 4942 Loss: 2.839333412447595e-05\n",
      "Iteration: 4943 Loss: 2.8363334575059236e-05\n",
      "Iteration: 4944 Loss: 2.8333366722293454e-05\n",
      "Iteration: 4945 Loss: 2.830343053268545e-05\n",
      "Iteration: 4946 Loss: 2.8273525972783267e-05\n",
      "Iteration: 4947 Loss: 2.824365300916803e-05\n",
      "Iteration: 4948 Loss: 2.8213811608457536e-05\n",
      "Iteration: 4949 Loss: 2.8184001737300743e-05\n",
      "Iteration: 4950 Loss: 2.8154223362384725e-05\n",
      "Iteration: 4951 Loss: 2.8124476450432253e-05\n",
      "Iteration: 4952 Loss: 2.809476096820186e-05\n",
      "Iteration: 4953 Loss: 2.8065076882484255e-05\n",
      "Iteration: 4954 Loss: 2.8035424160104045e-05\n",
      "Iteration: 4955 Loss: 2.8005802767930174e-05\n",
      "Iteration: 4956 Loss: 2.7976212672969914e-05\n",
      "Iteration: 4957 Loss: 2.7946653841928228e-05\n",
      "Iteration: 4958 Loss: 2.7917126241882942e-05\n",
      "Iteration: 4959 Loss: 2.7887629839728094e-05\n",
      "Iteration: 4960 Loss: 2.785816460272679e-05\n",
      "Iteration: 4961 Loss: 2.7828730497836522e-05\n",
      "Iteration: 4962 Loss: 2.7799327492163422e-05\n",
      "Iteration: 4963 Loss: 2.7769955552849596e-05\n",
      "Iteration: 4964 Loss: 2.774061464718623e-05\n",
      "Iteration: 4965 Loss: 2.7711304742155284e-05\n",
      "Iteration: 4966 Loss: 2.7682025805116884e-05\n",
      "Iteration: 4967 Loss: 2.7652777803349365e-05\n",
      "Iteration: 4968 Loss: 2.7623560704168777e-05\n",
      "Iteration: 4969 Loss: 2.7594374474922937e-05\n",
      "Iteration: 4970 Loss: 2.756521908299587e-05\n",
      "Iteration: 4971 Loss: 2.75360944956945e-05\n",
      "Iteration: 4972 Loss: 2.7507000680697455e-05\n",
      "Iteration: 4973 Loss: 2.747793760537826e-05\n",
      "Iteration: 4974 Loss: 2.744890523745756e-05\n",
      "Iteration: 4975 Loss: 2.7419903544090028e-05\n",
      "Iteration: 4976 Loss: 2.7390932493184283e-05\n",
      "Iteration: 4977 Loss: 2.736199205213179e-05\n",
      "Iteration: 4978 Loss: 2.7333082188704943e-05\n",
      "Iteration: 4979 Loss: 2.73042028703964e-05\n",
      "Iteration: 4980 Loss: 2.7275354065337368e-05\n",
      "Iteration: 4981 Loss: 2.7246535741085087e-05\n",
      "Iteration: 4982 Loss: 2.7217747865430436e-05\n",
      "Iteration: 4983 Loss: 2.7188990406210648e-05\n",
      "Iteration: 4984 Loss: 2.7160263331283493e-05\n",
      "Iteration: 4985 Loss: 2.7131566608547734e-05\n",
      "Iteration: 4986 Loss: 2.710290020593293e-05\n",
      "Iteration: 4987 Loss: 2.7074264091403374e-05\n",
      "Iteration: 4988 Loss: 2.7045658232957736e-05\n",
      "Iteration: 4989 Loss: 2.70170825986312e-05\n",
      "Iteration: 4990 Loss: 2.6988537156371286e-05\n",
      "Iteration: 4991 Loss: 2.6960021874509026e-05\n",
      "Iteration: 4992 Loss: 2.6931536721061485e-05\n",
      "Iteration: 4993 Loss: 2.690308166419764e-05\n",
      "Iteration: 4994 Loss: 2.68746566721201e-05\n",
      "Iteration: 4995 Loss: 2.6846261713059712e-05\n",
      "Iteration: 4996 Loss: 2.6817896755287067e-05\n",
      "Iteration: 4997 Loss: 2.678956176710062e-05\n",
      "Iteration: 4998 Loss: 2.6761256716837956e-05\n",
      "Iteration: 4999 Loss: 2.6732981572868963e-05\n",
      "Iteration: 5000 Loss: 2.670473630359161e-05\n",
      "Iteration: 5001 Loss: 2.6676520877443144e-05\n",
      "Iteration: 5002 Loss: 2.6648335262892487e-05\n",
      "Iteration: 5003 Loss: 2.662017942844154e-05\n",
      "Iteration: 5004 Loss: 2.659205334262401e-05\n",
      "Iteration: 5005 Loss: 2.6563956974009967e-05\n",
      "Iteration: 5006 Loss: 2.653589029119927e-05\n",
      "Iteration: 5007 Loss: 2.6507853262830257e-05\n",
      "Iteration: 5008 Loss: 2.6479845857767833e-05\n",
      "Iteration: 5009 Loss: 2.64518680441154e-05\n",
      "Iteration: 5010 Loss: 2.6423919791201538e-05\n",
      "Iteration: 5011 Loss: 2.6396001067599686e-05\n",
      "Iteration: 5012 Loss: 2.636811184210783e-05\n",
      "Iteration: 5013 Loss: 2.634025208356189e-05\n",
      "Iteration: 5014 Loss: 2.631242176082271e-05\n",
      "Iteration: 5015 Loss: 2.6284620842791186e-05\n",
      "Iteration: 5016 Loss: 2.6256849298401904e-05\n",
      "Iteration: 5017 Loss: 2.6229107096618143e-05\n",
      "Iteration: 5018 Loss: 2.6201394206433025e-05\n",
      "Iteration: 5019 Loss: 2.617371059688356e-05\n",
      "Iteration: 5020 Loss: 2.614605623702813e-05\n",
      "Iteration: 5021 Loss: 2.6118431095965064e-05\n",
      "Iteration: 5022 Loss: 2.6090835142819584e-05\n",
      "Iteration: 5023 Loss: 2.6063268346755086e-05\n",
      "Iteration: 5024 Loss: 2.6035730676850615e-05\n",
      "Iteration: 5025 Loss: 2.600822210267184e-05\n",
      "Iteration: 5026 Loss: 2.5980742593142002e-05\n",
      "Iteration: 5027 Loss: 2.595329211765717e-05\n",
      "Iteration: 5028 Loss: 2.5925870645547716e-05\n",
      "Iteration: 5029 Loss: 2.58984781461638e-05\n",
      "Iteration: 5030 Loss: 2.587111458889937e-05\n",
      "Iteration: 5031 Loss: 2.584377994317155e-05\n",
      "Iteration: 5032 Loss: 2.5816474178435495e-05\n",
      "Iteration: 5033 Loss: 2.5789197264174125e-05\n",
      "Iteration: 5034 Loss: 2.576194916990439e-05\n",
      "Iteration: 5035 Loss: 2.5734729865179753e-05\n",
      "Iteration: 5036 Loss: 2.570753931957875e-05\n",
      "Iteration: 5037 Loss: 2.5680377502715416e-05\n",
      "Iteration: 5038 Loss: 2.565324438423673e-05\n",
      "Iteration: 5039 Loss: 2.5626139933821693e-05\n",
      "Iteration: 5040 Loss: 2.5599064121293815e-05\n",
      "Iteration: 5041 Loss: 2.557201691616688e-05\n",
      "Iteration: 5042 Loss: 2.5544998288328885e-05\n",
      "Iteration: 5043 Loss: 2.551800820758498e-05\n",
      "Iteration: 5044 Loss: 2.54910466437738e-05\n",
      "Iteration: 5045 Loss: 2.5464113566766023e-05\n",
      "Iteration: 5046 Loss: 2.543720894646269e-05\n",
      "Iteration: 5047 Loss: 2.541033275268276e-05\n",
      "Iteration: 5048 Loss: 2.538348495562165e-05\n",
      "Iteration: 5049 Loss: 2.5356665525159518e-05\n",
      "Iteration: 5050 Loss: 2.5329874431327756e-05\n",
      "Iteration: 5051 Loss: 2.5303111644184388e-05\n",
      "Iteration: 5052 Loss: 2.527637713382181e-05\n",
      "Iteration: 5053 Loss: 2.5249670870365064e-05\n",
      "Iteration: 5054 Loss: 2.5222992824170915e-05\n",
      "Iteration: 5055 Loss: 2.519634296501896e-05\n",
      "Iteration: 5056 Loss: 2.516972126313428e-05\n",
      "Iteration: 5057 Loss: 2.5143127689276077e-05\n",
      "Iteration: 5058 Loss: 2.5116562213186912e-05\n",
      "Iteration: 5059 Loss: 2.5090024805519552e-05\n",
      "Iteration: 5060 Loss: 2.5063515436506306e-05\n",
      "Iteration: 5061 Loss: 2.503703407651624e-05\n",
      "Iteration: 5062 Loss: 2.5010580695962167e-05\n",
      "Iteration: 5063 Loss: 2.498415526527865e-05\n",
      "Iteration: 5064 Loss: 2.495775775493676e-05\n",
      "Iteration: 5065 Loss: 2.4931388135435242e-05\n",
      "Iteration: 5066 Loss: 2.4905046377305736e-05\n",
      "Iteration: 5067 Loss: 2.487873245110813e-05\n",
      "Iteration: 5068 Loss: 2.4852446327440998e-05\n",
      "Iteration: 5069 Loss: 2.4826187976924278e-05\n",
      "Iteration: 5070 Loss: 2.479995737033065e-05\n",
      "Iteration: 5071 Loss: 2.477375447811792e-05\n",
      "Iteration: 5072 Loss: 2.4747579271119925e-05\n",
      "Iteration: 5073 Loss: 2.4721431720283498e-05\n",
      "Iteration: 5074 Loss: 2.469531179606941e-05\n",
      "Iteration: 5075 Loss: 2.4669219469326197e-05\n",
      "Iteration: 5076 Loss: 2.464315471076944e-05\n",
      "Iteration: 5077 Loss: 2.461711749168001e-05\n",
      "Iteration: 5078 Loss: 2.459110778275754e-05\n",
      "Iteration: 5079 Loss: 2.456512555493588e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5080 Loss: 2.453917077917737e-05\n",
      "Iteration: 5081 Loss: 2.4513243426481193e-05\n",
      "Iteration: 5082 Loss: 2.4487343468071558e-05\n",
      "Iteration: 5083 Loss: 2.44614708746018e-05\n",
      "Iteration: 5084 Loss: 2.4435625617161086e-05\n",
      "Iteration: 5085 Loss: 2.4409807667265492e-05\n",
      "Iteration: 5086 Loss: 2.4384016995663824e-05\n",
      "Iteration: 5087 Loss: 2.4358253573934792e-05\n",
      "Iteration: 5088 Loss: 2.433251737308769e-05\n",
      "Iteration: 5089 Loss: 2.4306808364360253e-05\n",
      "Iteration: 5090 Loss: 2.4281126519019857e-05\n",
      "Iteration: 5091 Loss: 2.4255471808372323e-05\n",
      "Iteration: 5092 Loss: 2.422984420374353e-05\n",
      "Iteration: 5093 Loss: 2.4204243676495225e-05\n",
      "Iteration: 5094 Loss: 2.417867019802051e-05\n",
      "Iteration: 5095 Loss: 2.4153123739734775e-05\n",
      "Iteration: 5096 Loss: 2.4127604273095145e-05\n",
      "Iteration: 5097 Loss: 2.410211176958032e-05\n",
      "Iteration: 5098 Loss: 2.407664620070087e-05\n",
      "Iteration: 5099 Loss: 2.4051207538000315e-05\n",
      "Iteration: 5100 Loss: 2.4025795753051117e-05\n",
      "Iteration: 5101 Loss: 2.400041081745204e-05\n",
      "Iteration: 5102 Loss: 2.3975052702836873e-05\n",
      "Iteration: 5103 Loss: 2.394972138086661e-05\n",
      "Iteration: 5104 Loss: 2.3924416823234626e-05\n",
      "Iteration: 5105 Loss: 2.3899139001658842e-05\n",
      "Iteration: 5106 Loss: 2.387388788789442e-05\n",
      "Iteration: 5107 Loss: 2.3848663453720926e-05\n",
      "Iteration: 5108 Loss: 2.3823465670950675e-05\n",
      "Iteration: 5109 Loss: 2.3798294511422738e-05\n",
      "Iteration: 5110 Loss: 2.3773149947007702e-05\n",
      "Iteration: 5111 Loss: 2.374803194960906e-05\n",
      "Iteration: 5112 Loss: 2.3722940491153168e-05\n",
      "Iteration: 5113 Loss: 2.36978755436025e-05\n",
      "Iteration: 5114 Loss: 2.3672837078944497e-05\n",
      "Iteration: 5115 Loss: 2.3647825069199365e-05\n",
      "Iteration: 5116 Loss: 2.3622839486416185e-05\n",
      "Iteration: 5117 Loss: 2.3597880302670156e-05\n",
      "Iteration: 5118 Loss: 2.357294749007155e-05\n",
      "Iteration: 5119 Loss: 2.3548041020755906e-05\n",
      "Iteration: 5120 Loss: 2.352316086689076e-05\n",
      "Iteration: 5121 Loss: 2.3498307000671413e-05\n",
      "Iteration: 5122 Loss: 2.3473479394322288e-05\n",
      "Iteration: 5123 Loss: 2.3448678020098138e-05\n",
      "Iteration: 5124 Loss: 2.342390285028849e-05\n",
      "Iteration: 5125 Loss: 2.3399153857198053e-05\n",
      "Iteration: 5126 Loss: 2.3374431013172442e-05\n",
      "Iteration: 5127 Loss: 2.3349734290583794e-05\n",
      "Iteration: 5128 Loss: 2.3325063661833856e-05\n",
      "Iteration: 5129 Loss: 2.330041909935076e-05\n",
      "Iteration: 5130 Loss: 2.3275800575594343e-05\n",
      "Iteration: 5131 Loss: 2.3251208063053126e-05\n",
      "Iteration: 5132 Loss: 2.322664153424297e-05\n",
      "Iteration: 5133 Loss: 2.3202100961713954e-05\n",
      "Iteration: 5134 Loss: 2.3177586318040677e-05\n",
      "Iteration: 5135 Loss: 2.315309757582e-05\n",
      "Iteration: 5136 Loss: 2.3128634707696807e-05\n",
      "Iteration: 5137 Loss: 2.3104197686325315e-05\n",
      "Iteration: 5138 Loss: 2.3079786484395022e-05\n",
      "Iteration: 5139 Loss: 2.3055401074634416e-05\n",
      "Iteration: 5140 Loss: 2.303104142978665e-05\n",
      "Iteration: 5141 Loss: 2.3006707522628673e-05\n",
      "Iteration: 5142 Loss: 2.29823993259699e-05\n",
      "Iteration: 5143 Loss: 2.2958116812642e-05\n",
      "Iteration: 5144 Loss: 2.293385995550991e-05\n",
      "Iteration: 5145 Loss: 2.2909628727468197e-05\n",
      "Iteration: 5146 Loss: 2.288542310143452e-05\n",
      "Iteration: 5147 Loss: 2.286124305036019e-05\n",
      "Iteration: 5148 Loss: 2.2837088547225672e-05\n",
      "Iteration: 5149 Loss: 2.2812959565035104e-05\n",
      "Iteration: 5150 Loss: 2.2788856076824053e-05\n",
      "Iteration: 5151 Loss: 2.2764778055654878e-05\n",
      "Iteration: 5152 Loss: 2.2740725474621653e-05\n",
      "Iteration: 5153 Loss: 2.2716698306847003e-05\n",
      "Iteration: 5154 Loss: 2.269269652547725e-05\n",
      "Iteration: 5155 Loss: 2.266872010389101e-05\n",
      "Iteration: 5156 Loss: 2.264476901489424e-05\n",
      "Iteration: 5157 Loss: 2.2620843231919643e-05\n",
      "Iteration: 5158 Loss: 2.259694272822881e-05\n",
      "Iteration: 5159 Loss: 2.257306747711414e-05\n",
      "Iteration: 5160 Loss: 2.2549217451893515e-05\n",
      "Iteration: 5161 Loss: 2.2525392625914262e-05\n",
      "Iteration: 5162 Loss: 2.2501592972555345e-05\n",
      "Iteration: 5163 Loss: 2.247781846521568e-05\n",
      "Iteration: 5164 Loss: 2.245406907712703e-05\n",
      "Iteration: 5165 Loss: 2.2430344782152705e-05\n",
      "Iteration: 5166 Loss: 2.2406645553573104e-05\n",
      "Iteration: 5167 Loss: 2.2382971364908837e-05\n",
      "Iteration: 5168 Loss: 2.2359322189707955e-05\n",
      "Iteration: 5169 Loss: 2.2335698001529685e-05\n",
      "Iteration: 5170 Loss: 2.2312098773985487e-05\n",
      "Iteration: 5171 Loss: 2.2288524480697256e-05\n",
      "Iteration: 5172 Loss: 2.226497509531713e-05\n",
      "Iteration: 5173 Loss: 2.224145059153253e-05\n",
      "Iteration: 5174 Loss: 2.2217950943057308e-05\n",
      "Iteration: 5175 Loss: 2.2194476123621877e-05\n",
      "Iteration: 5176 Loss: 2.2171026106994518e-05\n",
      "Iteration: 5177 Loss: 2.2147600867093765e-05\n",
      "Iteration: 5178 Loss: 2.2124200377501257e-05\n",
      "Iteration: 5179 Loss: 2.210082461218493e-05\n",
      "Iteration: 5180 Loss: 2.207747354502055e-05\n",
      "Iteration: 5181 Loss: 2.2054147149912592e-05\n",
      "Iteration: 5182 Loss: 2.2030845400680312e-05\n",
      "Iteration: 5183 Loss: 2.200756827151257e-05\n",
      "Iteration: 5184 Loss: 2.1984315736279832e-05\n",
      "Iteration: 5185 Loss: 2.1961087768999983e-05\n",
      "Iteration: 5186 Loss: 2.1937884343713985e-05\n",
      "Iteration: 5187 Loss: 2.1914705434490476e-05\n",
      "Iteration: 5188 Loss: 2.189155101542776e-05\n",
      "Iteration: 5189 Loss: 2.186842106064967e-05\n",
      "Iteration: 5190 Loss: 2.184531554430747e-05\n",
      "Iteration: 5191 Loss: 2.1822234440582628e-05\n",
      "Iteration: 5192 Loss: 2.1799177723677947e-05\n",
      "Iteration: 5193 Loss: 2.177614536782986e-05\n",
      "Iteration: 5194 Loss: 2.1753137347295395e-05\n",
      "Iteration: 5195 Loss: 2.173015363636836e-05\n",
      "Iteration: 5196 Loss: 2.1707194209359834e-05\n",
      "Iteration: 5197 Loss: 2.168425904061299e-05\n",
      "Iteration: 5198 Loss: 2.166134810449518e-05\n",
      "Iteration: 5199 Loss: 2.1638461375407428e-05\n",
      "Iteration: 5200 Loss: 2.161559882777175e-05\n",
      "Iteration: 5201 Loss: 2.1592760436035348e-05\n",
      "Iteration: 5202 Loss: 2.1569946174680397e-05\n",
      "Iteration: 5203 Loss: 2.154715601820843e-05\n",
      "Iteration: 5204 Loss: 2.1524389941152585e-05\n",
      "Iteration: 5205 Loss: 2.150164791807055e-05\n",
      "Iteration: 5206 Loss: 2.147892992354769e-05\n",
      "Iteration: 5207 Loss: 2.1456235932194353e-05\n",
      "Iteration: 5208 Loss: 2.143356591865258e-05\n",
      "Iteration: 5209 Loss: 2.1410919857586443e-05\n",
      "Iteration: 5210 Loss: 2.1388297723690515e-05\n",
      "Iteration: 5211 Loss: 2.136569949168089e-05\n",
      "Iteration: 5212 Loss: 2.134312513630407e-05\n",
      "Iteration: 5213 Loss: 2.1320574632333527e-05\n",
      "Iteration: 5214 Loss: 2.129804795457048e-05\n",
      "Iteration: 5215 Loss: 2.12755450778358e-05\n",
      "Iteration: 5216 Loss: 2.1253065976987672e-05\n",
      "Iteration: 5217 Loss: 2.12306106269041e-05\n",
      "Iteration: 5218 Loss: 2.120817900248701e-05\n",
      "Iteration: 5219 Loss: 2.1185771078672027e-05\n",
      "Iteration: 5220 Loss: 2.116338683041603e-05\n",
      "Iteration: 5221 Loss: 2.1141026232705454e-05\n",
      "Iteration: 5222 Loss: 2.1118689260549372e-05\n",
      "Iteration: 5223 Loss: 2.109637588898843e-05\n",
      "Iteration: 5224 Loss: 2.1074086093085933e-05\n",
      "Iteration: 5225 Loss: 2.105181984793219e-05\n",
      "Iteration: 5226 Loss: 2.1029577128645155e-05\n",
      "Iteration: 5227 Loss: 2.1007357910365933e-05\n",
      "Iteration: 5228 Loss: 2.098516216826695e-05\n",
      "Iteration: 5229 Loss: 2.096298987754245e-05\n",
      "Iteration: 5230 Loss: 2.0940841013413143e-05\n",
      "Iteration: 5231 Loss: 2.0918715551130113e-05\n",
      "Iteration: 5232 Loss: 2.0896613465965663e-05\n",
      "Iteration: 5233 Loss: 2.087453473321914e-05\n",
      "Iteration: 5234 Loss: 2.0852479328216837e-05\n",
      "Iteration: 5235 Loss: 2.0830447226316817e-05\n",
      "Iteration: 5236 Loss: 2.0808438402895598e-05\n",
      "Iteration: 5237 Loss: 2.078645283335169e-05\n",
      "Iteration: 5238 Loss: 2.076449049312298e-05\n",
      "Iteration: 5239 Loss: 2.0742551357662524e-05\n",
      "Iteration: 5240 Loss: 2.0720635402452013e-05\n",
      "Iteration: 5241 Loss: 2.069874260300208e-05\n",
      "Iteration: 5242 Loss: 2.067687293484673e-05\n",
      "Iteration: 5243 Loss: 2.0655026373546292e-05\n",
      "Iteration: 5244 Loss: 2.0633202894681705e-05\n",
      "Iteration: 5245 Loss: 2.061140247387313e-05\n",
      "Iteration: 5246 Loss: 2.0589625086754258e-05\n",
      "Iteration: 5247 Loss: 2.0567870708991972e-05\n",
      "Iteration: 5248 Loss: 2.054613931626304e-05\n",
      "Iteration: 5249 Loss: 2.0524430884298556e-05\n",
      "Iteration: 5250 Loss: 2.0502745388827407e-05\n",
      "Iteration: 5251 Loss: 2.04810828056196e-05\n",
      "Iteration: 5252 Loss: 2.0459443110462448e-05\n",
      "Iteration: 5253 Loss: 2.0437826279179658e-05\n",
      "Iteration: 5254 Loss: 2.0416232287615106e-05\n",
      "Iteration: 5255 Loss: 2.039466111162924e-05\n",
      "Iteration: 5256 Loss: 2.0373112727119915e-05\n",
      "Iteration: 5257 Loss: 2.035158711000504e-05\n",
      "Iteration: 5258 Loss: 2.0330084236228863e-05\n",
      "Iteration: 5259 Loss: 2.030860408176658e-05\n",
      "Iteration: 5260 Loss: 2.0287146622609786e-05\n",
      "Iteration: 5261 Loss: 2.0265711834778883e-05\n",
      "Iteration: 5262 Loss: 2.0244299694318737e-05\n",
      "Iteration: 5263 Loss: 2.02229101773013e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5264 Loss: 2.0201543259823756e-05\n",
      "Iteration: 5265 Loss: 2.0180198918327756e-05\n",
      "Iteration: 5266 Loss: 2.0158877128328376e-05\n",
      "Iteration: 5267 Loss: 2.013757786630783e-05\n",
      "Iteration: 5268 Loss: 2.0116301108467104e-05\n",
      "Iteration: 5269 Loss: 2.0095046831026636e-05\n",
      "Iteration: 5270 Loss: 2.0073815010439534e-05\n",
      "Iteration: 5271 Loss: 2.0052605622575615e-05\n",
      "Iteration: 5272 Loss: 2.0031418643934205e-05\n",
      "Iteration: 5273 Loss: 2.0010254050835358e-05\n",
      "Iteration: 5274 Loss: 1.998911181951355e-05\n",
      "Iteration: 5275 Loss: 1.9967991926371516e-05\n",
      "Iteration: 5276 Loss: 1.9946894347893766e-05\n",
      "Iteration: 5277 Loss: 1.99258190604996e-05\n",
      "Iteration: 5278 Loss: 1.990476604084648e-05\n",
      "Iteration: 5279 Loss: 1.9883735265196888e-05\n",
      "Iteration: 5280 Loss: 1.9862726710054347e-05\n",
      "Iteration: 5281 Loss: 1.9841740352137654e-05\n",
      "Iteration: 5282 Loss: 1.982077616759788e-05\n",
      "Iteration: 5283 Loss: 1.9799834133201954e-05\n",
      "Iteration: 5284 Loss: 1.9778914225552385e-05\n",
      "Iteration: 5285 Loss: 1.975801642126495e-05\n",
      "Iteration: 5286 Loss: 1.973714069699297e-05\n",
      "Iteration: 5287 Loss: 1.9716287029395703e-05\n",
      "Iteration: 5288 Loss: 1.969545539518234e-05\n",
      "Iteration: 5289 Loss: 1.9674645771067156e-05\n",
      "Iteration: 5290 Loss: 1.9653858133791568e-05\n",
      "Iteration: 5291 Loss: 1.9633092460129735e-05\n",
      "Iteration: 5292 Loss: 1.9612348726873704e-05\n",
      "Iteration: 5293 Loss: 1.9591626910841296e-05\n",
      "Iteration: 5294 Loss: 1.957092698887756e-05\n",
      "Iteration: 5295 Loss: 1.9550248937845775e-05\n",
      "Iteration: 5296 Loss: 1.9529592734640994e-05\n",
      "Iteration: 5297 Loss: 1.9508958356181726e-05\n",
      "Iteration: 5298 Loss: 1.94883457794026e-05\n",
      "Iteration: 5299 Loss: 1.9467754981273007e-05\n",
      "Iteration: 5300 Loss: 1.9447185938780166e-05\n",
      "Iteration: 5301 Loss: 1.9426638628939455e-05\n",
      "Iteration: 5302 Loss: 1.9406113028787613e-05\n",
      "Iteration: 5303 Loss: 1.9385609115386142e-05\n",
      "Iteration: 5304 Loss: 1.936512686582263e-05\n",
      "Iteration: 5305 Loss: 1.934466625720758e-05\n",
      "Iteration: 5306 Loss: 1.9324227266675917e-05\n",
      "Iteration: 5307 Loss: 1.9303809871387114e-05\n",
      "Iteration: 5308 Loss: 1.9283414048521036e-05\n",
      "Iteration: 5309 Loss: 1.9263039775288733e-05\n",
      "Iteration: 5310 Loss: 1.9242687028920757e-05\n",
      "Iteration: 5311 Loss: 1.9222355786672852e-05\n",
      "Iteration: 5312 Loss: 1.9202046025818827e-05\n",
      "Iteration: 5313 Loss: 1.9181757723669082e-05\n",
      "Iteration: 5314 Loss: 1.9161490857548037e-05\n",
      "Iteration: 5315 Loss: 1.9141245404808117e-05\n",
      "Iteration: 5316 Loss: 1.9121021342823083e-05\n",
      "Iteration: 5317 Loss: 1.91008186489924e-05\n",
      "Iteration: 5318 Loss: 1.9080637300739908e-05\n",
      "Iteration: 5319 Loss: 1.906047727551198e-05\n",
      "Iteration: 5320 Loss: 1.9040338550779452e-05\n",
      "Iteration: 5321 Loss: 1.902022110403887e-05\n",
      "Iteration: 5322 Loss: 1.9000124912803416e-05\n",
      "Iteration: 5323 Loss: 1.8980049954620967e-05\n",
      "Iteration: 5324 Loss: 1.8959996207052187e-05\n",
      "Iteration: 5325 Loss: 1.8939963647689995e-05\n",
      "Iteration: 5326 Loss: 1.8919952254146914e-05\n",
      "Iteration: 5327 Loss: 1.8899962004058716e-05\n",
      "Iteration: 5328 Loss: 1.8879992875086933e-05\n",
      "Iteration: 5329 Loss: 1.8860044844913924e-05\n",
      "Iteration: 5330 Loss: 1.884011789125029e-05\n",
      "Iteration: 5331 Loss: 1.882021199182433e-05\n",
      "Iteration: 5332 Loss: 1.8800327124391634e-05\n",
      "Iteration: 5333 Loss: 1.8780463266730316e-05\n",
      "Iteration: 5334 Loss: 1.8760620396645034e-05\n",
      "Iteration: 5335 Loss: 1.8740798491955567e-05\n",
      "Iteration: 5336 Loss: 1.8720997530514395e-05\n",
      "Iteration: 5337 Loss: 1.8701217490305523e-05\n",
      "Iteration: 5338 Loss: 1.868145834899887e-05\n",
      "Iteration: 5339 Loss: 1.8661720084623805e-05\n",
      "Iteration: 5340 Loss: 1.8642002675124464e-05\n",
      "Iteration: 5341 Loss: 1.86223060984633e-05\n",
      "Iteration: 5342 Loss: 1.8602630332519252e-05\n",
      "Iteration: 5343 Loss: 1.858297535553091e-05\n",
      "Iteration: 5344 Loss: 1.856334114541847e-05\n",
      "Iteration: 5345 Loss: 1.854372768023861e-05\n",
      "Iteration: 5346 Loss: 1.8524134938076976e-05\n",
      "Iteration: 5347 Loss: 1.850456289703387e-05\n",
      "Iteration: 5348 Loss: 1.8485011535242877e-05\n",
      "Iteration: 5349 Loss: 1.846548083084759e-05\n",
      "Iteration: 5350 Loss: 1.84459707620277e-05\n",
      "Iteration: 5351 Loss: 1.842648130697633e-05\n",
      "Iteration: 5352 Loss: 1.840701244391594e-05\n",
      "Iteration: 5353 Loss: 1.8387564151090276e-05\n",
      "Iteration: 5354 Loss: 1.836813640676412e-05\n",
      "Iteration: 5355 Loss: 1.8348729189226163e-05\n",
      "Iteration: 5356 Loss: 1.832934247678845e-05\n",
      "Iteration: 5357 Loss: 1.830997624778802e-05\n",
      "Iteration: 5358 Loss: 1.8290630480579e-05\n",
      "Iteration: 5359 Loss: 1.8271305153544637e-05\n",
      "Iteration: 5360 Loss: 1.8252000245088083e-05\n",
      "Iteration: 5361 Loss: 1.8232715733639234e-05\n",
      "Iteration: 5362 Loss: 1.821345159775312e-05\n",
      "Iteration: 5363 Loss: 1.819420781567777e-05\n",
      "Iteration: 5364 Loss: 1.8174984366023547e-05\n",
      "Iteration: 5365 Loss: 1.8155781227305782e-05\n",
      "Iteration: 5366 Loss: 1.813659837806541e-05\n",
      "Iteration: 5367 Loss: 1.811743579686487e-05\n",
      "Iteration: 5368 Loss: 1.8098293462174326e-05\n",
      "Iteration: 5369 Loss: 1.8079171352828896e-05\n",
      "Iteration: 5370 Loss: 1.806006944735135e-05\n",
      "Iteration: 5371 Loss: 1.804098772438718e-05\n",
      "Iteration: 5372 Loss: 1.8021926162617548e-05\n",
      "Iteration: 5373 Loss: 1.8002884740738928e-05\n",
      "Iteration: 5374 Loss: 1.7983863437472752e-05\n",
      "Iteration: 5375 Loss: 1.796486223156072e-05\n",
      "Iteration: 5376 Loss: 1.794588110176893e-05\n",
      "Iteration: 5377 Loss: 1.792692002688536e-05\n",
      "Iteration: 5378 Loss: 1.7907978985722566e-05\n",
      "Iteration: 5379 Loss: 1.7889057957112533e-05\n",
      "Iteration: 5380 Loss: 1.7870156919908355e-05\n",
      "Iteration: 5381 Loss: 1.7851275852990646e-05\n",
      "Iteration: 5382 Loss: 1.783241473537251e-05\n",
      "Iteration: 5383 Loss: 1.7813573545750048e-05\n",
      "Iteration: 5384 Loss: 1.7794752263175088e-05\n",
      "Iteration: 5385 Loss: 1.7775950866620717e-05\n",
      "Iteration: 5386 Loss: 1.775716933507647e-05\n",
      "Iteration: 5387 Loss: 1.773840764754942e-05\n",
      "Iteration: 5388 Loss: 1.7719665783074225e-05\n",
      "Iteration: 5389 Loss: 1.770094372070766e-05\n",
      "Iteration: 5390 Loss: 1.7682241439527735e-05\n",
      "Iteration: 5391 Loss: 1.7663558918633025e-05\n",
      "Iteration: 5392 Loss: 1.7644896137145883e-05\n",
      "Iteration: 5393 Loss: 1.762625307420973e-05\n",
      "Iteration: 5394 Loss: 1.760762970899032e-05\n",
      "Iteration: 5395 Loss: 1.7589026020674954e-05\n",
      "Iteration: 5396 Loss: 1.757044198847778e-05\n",
      "Iteration: 5397 Loss: 1.7551877591624803e-05\n",
      "Iteration: 5398 Loss: 1.7533332809260663e-05\n",
      "Iteration: 5399 Loss: 1.75148076208857e-05\n",
      "Iteration: 5400 Loss: 1.7496302005686375e-05\n",
      "Iteration: 5401 Loss: 1.7477815943181522e-05\n",
      "Iteration: 5402 Loss: 1.74593494121107e-05\n",
      "Iteration: 5403 Loss: 1.744090239243917e-05\n",
      "Iteration: 5404 Loss: 1.7422474863350803e-05\n",
      "Iteration: 5405 Loss: 1.7404066804253527e-05\n",
      "Iteration: 5406 Loss: 1.7385678194574854e-05\n",
      "Iteration: 5407 Loss: 1.736730901376532e-05\n",
      "Iteration: 5408 Loss: 1.7348959241295787e-05\n",
      "Iteration: 5409 Loss: 1.733062885666413e-05\n",
      "Iteration: 5410 Loss: 1.7312317839381346e-05\n",
      "Iteration: 5411 Loss: 1.7294026168986236e-05\n",
      "Iteration: 5412 Loss: 1.7275753825038588e-05\n",
      "Iteration: 5413 Loss: 1.7257500787116268e-05\n",
      "Iteration: 5414 Loss: 1.723926703482215e-05\n",
      "Iteration: 5415 Loss: 1.7221052547779946e-05\n",
      "Iteration: 5416 Loss: 1.7202857305632262e-05\n",
      "Iteration: 5417 Loss: 1.7184681288048597e-05\n",
      "Iteration: 5418 Loss: 1.7166524474715436e-05\n",
      "Iteration: 5419 Loss: 1.714838684534332e-05\n",
      "Iteration: 5420 Loss: 1.713026837966094e-05\n",
      "Iteration: 5421 Loss: 1.7112169057423142e-05\n",
      "Iteration: 5422 Loss: 1.7094088858401254e-05\n",
      "Iteration: 5423 Loss: 1.707602776239025e-05\n",
      "Iteration: 5424 Loss: 1.7057985749207466e-05\n",
      "Iteration: 5425 Loss: 1.703996279869105e-05\n",
      "Iteration: 5426 Loss: 1.7021958890698216e-05\n",
      "Iteration: 5427 Loss: 1.700397400511122e-05\n",
      "Iteration: 5428 Loss: 1.698600812182887e-05\n",
      "Iteration: 5429 Loss: 1.6968061220776245e-05\n",
      "Iteration: 5430 Loss: 1.6950133281977904e-05\n",
      "Iteration: 5431 Loss: 1.693222428527734e-05\n",
      "Iteration: 5432 Loss: 1.6914334210900726e-05\n",
      "Iteration: 5433 Loss: 1.68964630383744e-05\n",
      "Iteration: 5434 Loss: 1.6878610747848112e-05\n",
      "Iteration: 5435 Loss: 1.68607773197332e-05\n",
      "Iteration: 5436 Loss: 1.68429627338989e-05\n",
      "Iteration: 5437 Loss: 1.6825166970435253e-05\n",
      "Iteration: 5438 Loss: 1.6807390009456645e-05\n",
      "Iteration: 5439 Loss: 1.6789631831100214e-05\n",
      "Iteration: 5440 Loss: 1.67718924155157e-05\n",
      "Iteration: 5441 Loss: 1.675417174288153e-05\n",
      "Iteration: 5442 Loss: 1.673646979359368e-05\n",
      "Iteration: 5443 Loss: 1.6718786547470846e-05\n",
      "Iteration: 5444 Loss: 1.6701121985152297e-05\n",
      "Iteration: 5445 Loss: 1.6683476086493866e-05\n",
      "Iteration: 5446 Loss: 1.6665848831776593e-05\n",
      "Iteration: 5447 Loss: 1.6648240201502135e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5448 Loss: 1.6630650176194115e-05\n",
      "Iteration: 5449 Loss: 1.6613078735997077e-05\n",
      "Iteration: 5450 Loss: 1.6595525861270442e-05\n",
      "Iteration: 5451 Loss: 1.6577991532601654e-05\n",
      "Iteration: 5452 Loss: 1.6560475729993194e-05\n",
      "Iteration: 5453 Loss: 1.6542978434069742e-05\n",
      "Iteration: 5454 Loss: 1.6525499625277445e-05\n",
      "Iteration: 5455 Loss: 1.6508039284291174e-05\n",
      "Iteration: 5456 Loss: 1.649059739098947e-05\n",
      "Iteration: 5457 Loss: 1.6473173926485183e-05\n",
      "Iteration: 5458 Loss: 1.6455768871106892e-05\n",
      "Iteration: 5459 Loss: 1.6438382205399922e-05\n",
      "Iteration: 5460 Loss: 1.642101390993849e-05\n",
      "Iteration: 5461 Loss: 1.640366396531181e-05\n",
      "Iteration: 5462 Loss: 1.6386332352133802e-05\n",
      "Iteration: 5463 Loss: 1.6369019050829184e-05\n",
      "Iteration: 5464 Loss: 1.6351724042454494e-05\n",
      "Iteration: 5465 Loss: 1.633444730748611e-05\n",
      "Iteration: 5466 Loss: 1.6317188826609442e-05\n",
      "Iteration: 5467 Loss: 1.6299948580542935e-05\n",
      "Iteration: 5468 Loss: 1.6282726550018117e-05\n",
      "Iteration: 5469 Loss: 1.6265522715786644e-05\n",
      "Iteration: 5470 Loss: 1.624833705862722e-05\n",
      "Iteration: 5471 Loss: 1.6231169559332623e-05\n",
      "Iteration: 5472 Loss: 1.621402019871747e-05\n",
      "Iteration: 5473 Loss: 1.6196888957617166e-05\n",
      "Iteration: 5474 Loss: 1.6179775816885756e-05\n",
      "Iteration: 5475 Loss: 1.6162680757401485e-05\n",
      "Iteration: 5476 Loss: 1.614560376001972e-05\n",
      "Iteration: 5477 Loss: 1.612854480573511e-05\n",
      "Iteration: 5478 Loss: 1.611150387544486e-05\n",
      "Iteration: 5479 Loss: 1.6094480950105725e-05\n",
      "Iteration: 5480 Loss: 1.6077476010693944e-05\n",
      "Iteration: 5481 Loss: 1.6060489038203923e-05\n",
      "Iteration: 5482 Loss: 1.6043520013656178e-05\n",
      "Iteration: 5483 Loss: 1.602656891808318e-05\n",
      "Iteration: 5484 Loss: 1.6009635732545568e-05\n",
      "Iteration: 5485 Loss: 1.599272043811929e-05\n",
      "Iteration: 5486 Loss: 1.5975823015898178e-05\n",
      "Iteration: 5487 Loss: 1.5958943447004028e-05\n",
      "Iteration: 5488 Loss: 1.5942081712567652e-05\n",
      "Iteration: 5489 Loss: 1.5925237793751328e-05\n",
      "Iteration: 5490 Loss: 1.5908411671727443e-05\n",
      "Iteration: 5491 Loss: 1.589160332780935e-05\n",
      "Iteration: 5492 Loss: 1.587481274298339e-05\n",
      "Iteration: 5493 Loss: 1.5858039898599662e-05\n",
      "Iteration: 5494 Loss: 1.584128477591509e-05\n",
      "Iteration: 5495 Loss: 1.5824547356205113e-05\n",
      "Iteration: 5496 Loss: 1.5807827620649147e-05\n",
      "Iteration: 5497 Loss: 1.579112555079513e-05\n",
      "Iteration: 5498 Loss: 1.5774441127861722e-05\n",
      "Iteration: 5499 Loss: 1.575777433320068e-05\n",
      "Iteration: 5500 Loss: 1.5741125148192177e-05\n",
      "Iteration: 5501 Loss: 1.572449355422684e-05\n",
      "Iteration: 5502 Loss: 1.570787953271882e-05\n",
      "Iteration: 5503 Loss: 1.5691283065102377e-05\n",
      "Iteration: 5504 Loss: 1.5674704132829544e-05\n",
      "Iteration: 5505 Loss: 1.5658142717374514e-05\n",
      "Iteration: 5506 Loss: 1.564159880022871e-05\n",
      "Iteration: 5507 Loss: 1.5625072362904284e-05\n",
      "Iteration: 5508 Loss: 1.560856338693175e-05\n",
      "Iteration: 5509 Loss: 1.5592071853860985e-05\n",
      "Iteration: 5510 Loss: 1.557559774526297e-05\n",
      "Iteration: 5511 Loss: 1.5559141042729845e-05\n",
      "Iteration: 5512 Loss: 1.5542701727871196e-05\n",
      "Iteration: 5513 Loss: 1.5526279782309066e-05\n",
      "Iteration: 5514 Loss: 1.5509875187700123e-05\n",
      "Iteration: 5515 Loss: 1.5493487925707395e-05\n",
      "Iteration: 5516 Loss: 1.5477117978018743e-05\n",
      "Iteration: 5517 Loss: 1.5460765326453908e-05\n",
      "Iteration: 5518 Loss: 1.544442995251017e-05\n",
      "Iteration: 5519 Loss: 1.542811183804368e-05\n",
      "Iteration: 5520 Loss: 1.5411810964826673e-05\n",
      "Iteration: 5521 Loss: 1.539552731463361e-05\n",
      "Iteration: 5522 Loss: 1.5379260869273563e-05\n",
      "Iteration: 5523 Loss: 1.536301161056327e-05\n",
      "Iteration: 5524 Loss: 1.5346779520232918e-05\n",
      "Iteration: 5525 Loss: 1.5330564580372034e-05\n",
      "Iteration: 5526 Loss: 1.5314366772742578e-05\n",
      "Iteration: 5527 Loss: 1.5298186079245428e-05\n",
      "Iteration: 5528 Loss: 1.5282022481797664e-05\n",
      "Iteration: 5529 Loss: 1.5265875962336287e-05\n",
      "Iteration: 5530 Loss: 1.5249746502817104e-05\n",
      "Iteration: 5531 Loss: 1.5233634085214663e-05\n",
      "Iteration: 5532 Loss: 1.5217538691523694e-05\n",
      "Iteration: 5533 Loss: 1.5201460303754855e-05\n",
      "Iteration: 5534 Loss: 1.5185398903945823e-05\n",
      "Iteration: 5535 Loss: 1.5169354474141138e-05\n",
      "Iteration: 5536 Loss: 1.515332699641486e-05\n",
      "Iteration: 5537 Loss: 1.5137316452853821e-05\n",
      "Iteration: 5538 Loss: 1.5121322825683857e-05\n",
      "Iteration: 5539 Loss: 1.5105346096796306e-05\n",
      "Iteration: 5540 Loss: 1.5089386248458845e-05\n",
      "Iteration: 5541 Loss: 1.5073443262830271e-05\n",
      "Iteration: 5542 Loss: 1.5057517122094581e-05\n",
      "Iteration: 5543 Loss: 1.5041607808457379e-05\n",
      "Iteration: 5544 Loss: 1.5025715304021162e-05\n",
      "Iteration: 5545 Loss: 1.500983959125823e-05\n",
      "Iteration: 5546 Loss: 1.4993980652310851e-05\n",
      "Iteration: 5547 Loss: 1.4978138469572346e-05\n",
      "Iteration: 5548 Loss: 1.4962313025104652e-05\n",
      "Iteration: 5549 Loss: 1.4946504301228483e-05\n",
      "Iteration: 5550 Loss: 1.493071228050249e-05\n",
      "Iteration: 5551 Loss: 1.4914936945167491e-05\n",
      "Iteration: 5552 Loss: 1.4899178277590741e-05\n",
      "Iteration: 5553 Loss: 1.4883436260163855e-05\n",
      "Iteration: 5554 Loss: 1.4867710875293625e-05\n",
      "Iteration: 5555 Loss: 1.4852002105405691e-05\n",
      "Iteration: 5556 Loss: 1.4836309932947587e-05\n",
      "Iteration: 5557 Loss: 1.4820634340382817e-05\n",
      "Iteration: 5558 Loss: 1.480497531030544e-05\n",
      "Iteration: 5559 Loss: 1.478933282498974e-05\n",
      "Iteration: 5560 Loss: 1.4773706866953406e-05\n",
      "Iteration: 5561 Loss: 1.4758097418963277e-05\n",
      "Iteration: 5562 Loss: 1.4742504463463553e-05\n",
      "Iteration: 5563 Loss: 1.4726927983023006e-05\n",
      "Iteration: 5564 Loss: 1.4711367960238461e-05\n",
      "Iteration: 5565 Loss: 1.4695824377721856e-05\n",
      "Iteration: 5566 Loss: 1.4680297218101259e-05\n",
      "Iteration: 5567 Loss: 1.4664786464024493e-05\n",
      "Iteration: 5568 Loss: 1.4649292098160577e-05\n",
      "Iteration: 5569 Loss: 1.463381410319273e-05\n",
      "Iteration: 5570 Loss: 1.4618352461822123e-05\n",
      "Iteration: 5571 Loss: 1.4602907156771858e-05\n",
      "Iteration: 5572 Loss: 1.4587478170778829e-05\n",
      "Iteration: 5573 Loss: 1.4572065486604438e-05\n",
      "Iteration: 5574 Loss: 1.4556669087024798e-05\n",
      "Iteration: 5575 Loss: 1.4541288954831415e-05\n",
      "Iteration: 5576 Loss: 1.4525925072837909e-05\n",
      "Iteration: 5577 Loss: 1.4510577423874403e-05\n",
      "Iteration: 5578 Loss: 1.4495245990789176e-05\n",
      "Iteration: 5579 Loss: 1.4479930756450075e-05\n",
      "Iteration: 5580 Loss: 1.4464631703740108e-05\n",
      "Iteration: 5581 Loss: 1.4449348815363275e-05\n",
      "Iteration: 5582 Loss: 1.443408207464196e-05\n",
      "Iteration: 5583 Loss: 1.4418831464516785e-05\n",
      "Iteration: 5584 Loss: 1.4403596967538693e-05\n",
      "Iteration: 5585 Loss: 1.438837856688786e-05\n",
      "Iteration: 5586 Loss: 1.4373176245555576e-05\n",
      "Iteration: 5587 Loss: 1.4357989986553545e-05\n",
      "Iteration: 5588 Loss: 1.4342819772910618e-05\n",
      "Iteration: 5589 Loss: 1.4327665587672729e-05\n",
      "Iteration: 5590 Loss: 1.4312527413904606e-05\n",
      "Iteration: 5591 Loss: 1.4297405234692282e-05\n",
      "Iteration: 5592 Loss: 1.4282299033132549e-05\n",
      "Iteration: 5593 Loss: 1.4267208792346177e-05\n",
      "Iteration: 5594 Loss: 1.4252134495466503e-05\n",
      "Iteration: 5595 Loss: 1.4237076125649491e-05\n",
      "Iteration: 5596 Loss: 1.4222033666069978e-05\n",
      "Iteration: 5597 Loss: 1.4207007100115605e-05\n",
      "Iteration: 5598 Loss: 1.4191996410593903e-05\n",
      "Iteration: 5599 Loss: 1.4177001580928771e-05\n",
      "Iteration: 5600 Loss: 1.4162022594363078e-05\n",
      "Iteration: 5601 Loss: 1.4147059434358175e-05\n",
      "Iteration: 5602 Loss: 1.4132112083793293e-05\n",
      "Iteration: 5603 Loss: 1.4117180526162763e-05\n",
      "Iteration: 5604 Loss: 1.410226474477952e-05\n",
      "Iteration: 5605 Loss: 1.408736472297847e-05\n",
      "Iteration: 5606 Loss: 1.4072480444106186e-05\n",
      "Iteration: 5607 Loss: 1.4057611891526786e-05\n",
      "Iteration: 5608 Loss: 1.4042759048630354e-05\n",
      "Iteration: 5609 Loss: 1.4027921898814061e-05\n",
      "Iteration: 5610 Loss: 1.4013100425296452e-05\n",
      "Iteration: 5611 Loss: 1.3998294611712442e-05\n",
      "Iteration: 5612 Loss: 1.39835044419276e-05\n",
      "Iteration: 5613 Loss: 1.3968729898803867e-05\n",
      "Iteration: 5614 Loss: 1.395397096623116e-05\n",
      "Iteration: 5615 Loss: 1.3939227627317418e-05\n",
      "Iteration: 5616 Loss: 1.3924499865783076e-05\n",
      "Iteration: 5617 Loss: 1.3909787665175222e-05\n",
      "Iteration: 5618 Loss: 1.3895091009048249e-05\n",
      "Iteration: 5619 Loss: 1.388040988089447e-05\n",
      "Iteration: 5620 Loss: 1.3865744264477217e-05\n",
      "Iteration: 5621 Loss: 1.3851094143324338e-05\n",
      "Iteration: 5622 Loss: 1.3836459501063858e-05\n",
      "Iteration: 5623 Loss: 1.3821840321343144e-05\n",
      "Iteration: 5624 Loss: 1.380723658781768e-05\n",
      "Iteration: 5625 Loss: 1.3792648284173168e-05\n",
      "Iteration: 5626 Loss: 1.3778075393902512e-05\n",
      "Iteration: 5627 Loss: 1.3763517901124915e-05\n",
      "Iteration: 5628 Loss: 1.3748975789370346e-05\n",
      "Iteration: 5629 Loss: 1.3734449042187829e-05\n",
      "Iteration: 5630 Loss: 1.3719937643740866e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5631 Loss: 1.370544157761561e-05\n",
      "Iteration: 5632 Loss: 1.3690960827613564e-05\n",
      "Iteration: 5633 Loss: 1.3676495377548754e-05\n",
      "Iteration: 5634 Loss: 1.3662045211257051e-05\n",
      "Iteration: 5635 Loss: 1.3647610312591038e-05\n",
      "Iteration: 5636 Loss: 1.3633190665301386e-05\n",
      "Iteration: 5637 Loss: 1.3618786253508556e-05\n",
      "Iteration: 5638 Loss: 1.3604397060996145e-05\n",
      "Iteration: 5639 Loss: 1.3590023071683109e-05\n",
      "Iteration: 5640 Loss: 1.3575664269515273e-05\n",
      "Iteration: 5641 Loss: 1.3561320638435085e-05\n",
      "Iteration: 5642 Loss: 1.3546992162418345e-05\n",
      "Iteration: 5643 Loss: 1.3532678825450678e-05\n",
      "Iteration: 5644 Loss: 1.3518380611540732e-05\n",
      "Iteration: 5645 Loss: 1.3504097504705202e-05\n",
      "Iteration: 5646 Loss: 1.3489829488983184e-05\n",
      "Iteration: 5647 Loss: 1.3475576548632023e-05\n",
      "Iteration: 5648 Loss: 1.3461338667322197e-05\n",
      "Iteration: 5649 Loss: 1.3447115829344443e-05\n",
      "Iteration: 5650 Loss: 1.3432908018799424e-05\n",
      "Iteration: 5651 Loss: 1.3418715219815621e-05\n",
      "Iteration: 5652 Loss: 1.3404537416532592e-05\n",
      "Iteration: 5653 Loss: 1.3390374593098564e-05\n",
      "Iteration: 5654 Loss: 1.3376226733692441e-05\n",
      "Iteration: 5655 Loss: 1.3362093822505105e-05\n",
      "Iteration: 5656 Loss: 1.334797584373926e-05\n",
      "Iteration: 5657 Loss: 1.3333872781620826e-05\n",
      "Iteration: 5658 Loss: 1.3319784620383318e-05\n",
      "Iteration: 5659 Loss: 1.3305711344288082e-05\n",
      "Iteration: 5660 Loss: 1.3291652937602332e-05\n",
      "Iteration: 5661 Loss: 1.3277609384622912e-05\n",
      "Iteration: 5662 Loss: 1.3263580669653106e-05\n",
      "Iteration: 5663 Loss: 1.3249566777131785e-05\n",
      "Iteration: 5664 Loss: 1.3235567691164591e-05\n",
      "Iteration: 5665 Loss: 1.3221583396107428e-05\n",
      "Iteration: 5666 Loss: 1.3207613876564391e-05\n",
      "Iteration: 5667 Loss: 1.3193659116806111e-05\n",
      "Iteration: 5668 Loss: 1.3179719101245962e-05\n",
      "Iteration: 5669 Loss: 1.3165793814297239e-05\n",
      "Iteration: 5670 Loss: 1.315188324040236e-05\n",
      "Iteration: 5671 Loss: 1.3137987364012682e-05\n",
      "Iteration: 5672 Loss: 1.3124106169599488e-05\n",
      "Iteration: 5673 Loss: 1.311023964165538e-05\n",
      "Iteration: 5674 Loss: 1.309638776468332e-05\n",
      "Iteration: 5675 Loss: 1.3082550523197775e-05\n",
      "Iteration: 5676 Loss: 1.3068727901737306e-05\n",
      "Iteration: 5677 Loss: 1.3054919884857381e-05\n",
      "Iteration: 5678 Loss: 1.3041126457125891e-05\n",
      "Iteration: 5679 Loss: 1.302734760312824e-05\n",
      "Iteration: 5680 Loss: 1.3013583307464886e-05\n",
      "Iteration: 5681 Loss: 1.2999833554758168e-05\n",
      "Iteration: 5682 Loss: 1.2986098329638874e-05\n",
      "Iteration: 5683 Loss: 1.297237761675718e-05\n",
      "Iteration: 5684 Loss: 1.2958671400780284e-05\n",
      "Iteration: 5685 Loss: 1.294497966639186e-05\n",
      "Iteration: 5686 Loss: 1.293130239829229e-05\n",
      "Iteration: 5687 Loss: 1.2917639581194564e-05\n",
      "Iteration: 5688 Loss: 1.2903991199833398e-05\n",
      "Iteration: 5689 Loss: 1.2890357238955914e-05\n",
      "Iteration: 5690 Loss: 1.2876737683319984e-05\n",
      "Iteration: 5691 Loss: 1.2863132517712434e-05\n",
      "Iteration: 5692 Loss: 1.2849541726925576e-05\n",
      "Iteration: 5693 Loss: 1.2835965295775035e-05\n",
      "Iteration: 5694 Loss: 1.2822403209086498e-05\n",
      "Iteration: 5695 Loss: 1.280885545169745e-05\n",
      "Iteration: 5696 Loss: 1.2795322008477832e-05\n",
      "Iteration: 5697 Loss: 1.278180286430094e-05\n",
      "Iteration: 5698 Loss: 1.2768298004059716e-05\n",
      "Iteration: 5699 Loss: 1.2754807412657141e-05\n",
      "Iteration: 5700 Loss: 1.2741331075021999e-05\n",
      "Iteration: 5701 Loss: 1.2727868976089365e-05\n",
      "Iteration: 5702 Loss: 1.2714421100819832e-05\n",
      "Iteration: 5703 Loss: 1.2700987434187298e-05\n",
      "Iteration: 5704 Loss: 1.2687567961285393e-05\n",
      "Iteration: 5705 Loss: 1.26741626668949e-05\n",
      "Iteration: 5706 Loss: 1.2660771536037775e-05\n",
      "Iteration: 5707 Loss: 1.2647394553969255e-05\n",
      "Iteration: 5708 Loss: 1.2634031705631764e-05\n",
      "Iteration: 5709 Loss: 1.2620682976087668e-05\n",
      "Iteration: 5710 Loss: 1.2607348350424544e-05\n",
      "Iteration: 5711 Loss: 1.2594027813739744e-05\n",
      "Iteration: 5712 Loss: 1.2580721351146126e-05\n",
      "Iteration: 5713 Loss: 1.2567428947654196e-05\n",
      "Iteration: 5714 Loss: 1.255415058876331e-05\n",
      "Iteration: 5715 Loss: 1.2540886259280377e-05\n",
      "Iteration: 5716 Loss: 1.2527635944708798e-05\n",
      "Iteration: 5717 Loss: 1.2514399629825086e-05\n",
      "Iteration: 5718 Loss: 1.2501177300050365e-05\n",
      "Iteration: 5719 Loss: 1.2487968940603376e-05\n",
      "Iteration: 5720 Loss: 1.2474774536724211e-05\n",
      "Iteration: 5721 Loss: 1.2461594073667168e-05\n",
      "Iteration: 5722 Loss: 1.244842753670496e-05\n",
      "Iteration: 5723 Loss: 1.2435274911007738e-05\n",
      "Iteration: 5724 Loss: 1.2422136182105071e-05\n",
      "Iteration: 5725 Loss: 1.2409011335316711e-05\n",
      "Iteration: 5726 Loss: 1.2395900355542806e-05\n",
      "Iteration: 5727 Loss: 1.2382803228647843e-05\n",
      "Iteration: 5728 Loss: 1.2369719939800263e-05\n",
      "Iteration: 5729 Loss: 1.2356650474368105e-05\n",
      "Iteration: 5730 Loss: 1.234359481775559e-05\n",
      "Iteration: 5731 Loss: 1.2330552955368136e-05\n",
      "Iteration: 5732 Loss: 1.2317524872633748e-05\n",
      "Iteration: 5733 Loss: 1.2304510554990498e-05\n",
      "Iteration: 5734 Loss: 1.2291509987901369e-05\n",
      "Iteration: 5735 Loss: 1.2278523156829078e-05\n",
      "Iteration: 5736 Loss: 1.226555004726438e-05\n",
      "Iteration: 5737 Loss: 1.225259064470545e-05\n",
      "Iteration: 5738 Loss: 1.2239644934680028e-05\n",
      "Iteration: 5739 Loss: 1.2226712902713724e-05\n",
      "Iteration: 5740 Loss: 1.2213794534352351e-05\n",
      "Iteration: 5741 Loss: 1.2200889815162428e-05\n",
      "Iteration: 5742 Loss: 1.2187998730724695e-05\n",
      "Iteration: 5743 Loss: 1.217512126663417e-05\n",
      "Iteration: 5744 Loss: 1.2162257408490766e-05\n",
      "Iteration: 5745 Loss: 1.214940714192836e-05\n",
      "Iteration: 5746 Loss: 1.2136570452584403e-05\n",
      "Iteration: 5747 Loss: 1.212374732611303e-05\n",
      "Iteration: 5748 Loss: 1.211093774818193e-05\n",
      "Iteration: 5749 Loss: 1.209814170447817e-05\n",
      "Iteration: 5750 Loss: 1.2085359180701697e-05\n",
      "Iteration: 5751 Loss: 1.2072590162682454e-05\n",
      "Iteration: 5752 Loss: 1.2059834635805415e-05\n",
      "Iteration: 5753 Loss: 1.204709258616024e-05\n",
      "Iteration: 5754 Loss: 1.2034363999393439e-05\n",
      "Iteration: 5755 Loss: 1.2021648861284288e-05\n",
      "Iteration: 5756 Loss: 1.200894715761834e-05\n",
      "Iteration: 5757 Loss: 1.1996258874200232e-05\n",
      "Iteration: 5758 Loss: 1.1983583996970037e-05\n",
      "Iteration: 5759 Loss: 1.1970922511528848e-05\n",
      "Iteration: 5760 Loss: 1.1958274403729558e-05\n",
      "Iteration: 5761 Loss: 1.1945639659670682e-05\n",
      "Iteration: 5762 Loss: 1.1933018265111223e-05\n",
      "Iteration: 5763 Loss: 1.1920410205948854e-05\n",
      "Iteration: 5764 Loss: 1.1907815468095859e-05\n",
      "Iteration: 5765 Loss: 1.1895234037478044e-05\n",
      "Iteration: 5766 Loss: 1.188266590003156e-05\n",
      "Iteration: 5767 Loss: 1.1870111041914509e-05\n",
      "Iteration: 5768 Loss: 1.1857569448896018e-05\n",
      "Iteration: 5769 Loss: 1.1845041106554854e-05\n",
      "Iteration: 5770 Loss: 1.1832526001699856e-05\n",
      "Iteration: 5771 Loss: 1.1820024119536943e-05\n",
      "Iteration: 5772 Loss: 1.1807535446813506e-05\n",
      "Iteration: 5773 Loss: 1.1795059968942917e-05\n",
      "Iteration: 5774 Loss: 1.1782597672499152e-05\n",
      "Iteration: 5775 Loss: 1.177014854324115e-05\n",
      "Iteration: 5776 Loss: 1.1757712567483513e-05\n",
      "Iteration: 5777 Loss: 1.1745289731216708e-05\n",
      "Iteration: 5778 Loss: 1.1732880020555156e-05\n",
      "Iteration: 5779 Loss: 1.172048342163319e-05\n",
      "Iteration: 5780 Loss: 1.170809992059808e-05\n",
      "Iteration: 5781 Loss: 1.1695729503607191e-05\n",
      "Iteration: 5782 Loss: 1.1683372156840018e-05\n",
      "Iteration: 5783 Loss: 1.1671027866485118e-05\n",
      "Iteration: 5784 Loss: 1.1658696618748393e-05\n",
      "Iteration: 5785 Loss: 1.1646378399848122e-05\n",
      "Iteration: 5786 Loss: 1.1634073196020006e-05\n",
      "Iteration: 5787 Loss: 1.1621780993509806e-05\n",
      "Iteration: 5788 Loss: 1.1609501778584695e-05\n",
      "Iteration: 5789 Loss: 1.1597235537518667e-05\n",
      "Iteration: 5790 Loss: 1.1584982256607533e-05\n",
      "Iteration: 5791 Loss: 1.1572741922156161e-05\n",
      "Iteration: 5792 Loss: 1.156051452048349e-05\n",
      "Iteration: 5793 Loss: 1.1548300037928329e-05\n",
      "Iteration: 5794 Loss: 1.153609846084053e-05\n",
      "Iteration: 5795 Loss: 1.1523909775583742e-05\n",
      "Iteration: 5796 Loss: 1.1511733968536746e-05\n",
      "Iteration: 5797 Loss: 1.1499571026090949e-05\n",
      "Iteration: 5798 Loss: 1.1487420934658112e-05\n",
      "Iteration: 5799 Loss: 1.1475283680859173e-05\n",
      "Iteration: 5800 Loss: 1.1463159250726379e-05\n",
      "Iteration: 5801 Loss: 1.1451047630913361e-05\n",
      "Iteration: 5802 Loss: 1.1438948807886362e-05\n",
      "Iteration: 5803 Loss: 1.142686276812164e-05\n",
      "Iteration: 5804 Loss: 1.1414789498113287e-05\n",
      "Iteration: 5805 Loss: 1.1402728984572993e-05\n",
      "Iteration: 5806 Loss: 1.1390681213620187e-05\n",
      "Iteration: 5807 Loss: 1.1378646171992282e-05\n",
      "Iteration: 5808 Loss: 1.1366623846034277e-05\n",
      "Iteration: 5809 Loss: 1.1354614222717974e-05\n",
      "Iteration: 5810 Loss: 1.1342617288420794e-05\n",
      "Iteration: 5811 Loss: 1.1330633029735713e-05\n",
      "Iteration: 5812 Loss: 1.131866143306421e-05\n",
      "Iteration: 5813 Loss: 1.1306702485550143e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5814 Loss: 1.1294756173399788e-05\n",
      "Iteration: 5815 Loss: 1.1282822483255676e-05\n",
      "Iteration: 5816 Loss: 1.127090140202189e-05\n",
      "Iteration: 5817 Loss: 1.1258992916249693e-05\n",
      "Iteration: 5818 Loss: 1.1247097012638233e-05\n",
      "Iteration: 5819 Loss: 1.1235213677892364e-05\n",
      "Iteration: 5820 Loss: 1.1223342898731591e-05\n",
      "Iteration: 5821 Loss: 1.121148466189038e-05\n",
      "Iteration: 5822 Loss: 1.1199638954116795e-05\n",
      "Iteration: 5823 Loss: 1.1187805762171656e-05\n",
      "Iteration: 5824 Loss: 1.1175985072836302e-05\n",
      "Iteration: 5825 Loss: 1.1164176873095331e-05\n",
      "Iteration: 5826 Loss: 1.1152381149353147e-05\n",
      "Iteration: 5827 Loss: 1.1140597888625755e-05\n",
      "Iteration: 5828 Loss: 1.1128827077958071e-05\n",
      "Iteration: 5829 Loss: 1.1117068703778986e-05\n",
      "Iteration: 5830 Loss: 1.1105322752957106e-05\n",
      "Iteration: 5831 Loss: 1.1093589212764847e-05\n",
      "Iteration: 5832 Loss: 1.1081868069688074e-05\n",
      "Iteration: 5833 Loss: 1.1070159311028807e-05\n",
      "Iteration: 5834 Loss: 1.1058462923504678e-05\n",
      "Iteration: 5835 Loss: 1.1046778894157455e-05\n",
      "Iteration: 5836 Loss: 1.1035107209699182e-05\n",
      "Iteration: 5837 Loss: 1.1023447857204331e-05\n",
      "Iteration: 5838 Loss: 1.1011800823640586e-05\n",
      "Iteration: 5839 Loss: 1.1000166095995324e-05\n",
      "Iteration: 5840 Loss: 1.0988543661147583e-05\n",
      "Iteration: 5841 Loss: 1.0976933506340684e-05\n",
      "Iteration: 5842 Loss: 1.0965335618485912e-05\n",
      "Iteration: 5843 Loss: 1.0953749984621548e-05\n",
      "Iteration: 5844 Loss: 1.0942176591800725e-05\n",
      "Iteration: 5845 Loss: 1.0930615427088948e-05\n",
      "Iteration: 5846 Loss: 1.0919066477567676e-05\n",
      "Iteration: 5847 Loss: 1.0907529730330577e-05\n",
      "Iteration: 5848 Loss: 1.089600517248372e-05\n",
      "Iteration: 5849 Loss: 1.0884492791148878e-05\n",
      "Iteration: 5850 Loss: 1.0872992573461533e-05\n",
      "Iteration: 5851 Loss: 1.0861504506567846e-05\n",
      "Iteration: 5852 Loss: 1.0850028577631978e-05\n",
      "Iteration: 5853 Loss: 1.083856477382826e-05\n",
      "Iteration: 5854 Loss: 1.0827113082346426e-05\n",
      "Iteration: 5855 Loss: 1.081567349038666e-05\n",
      "Iteration: 5856 Loss: 1.0804245985166879e-05\n",
      "Iteration: 5857 Loss: 1.0792830553916503e-05\n",
      "Iteration: 5858 Loss: 1.0781427183877541e-05\n",
      "Iteration: 5859 Loss: 1.0770035862309657e-05\n",
      "Iteration: 5860 Loss: 1.0758656576478634e-05\n",
      "Iteration: 5861 Loss: 1.074728931366883e-05\n",
      "Iteration: 5862 Loss: 1.073593406117941e-05\n",
      "Iteration: 5863 Loss: 1.0724590806317129e-05\n",
      "Iteration: 5864 Loss: 1.0713259536408875e-05\n",
      "Iteration: 5865 Loss: 1.0701940238790153e-05\n",
      "Iteration: 5866 Loss: 1.0690632900811435e-05\n",
      "Iteration: 5867 Loss: 1.0679337509837356e-05\n",
      "Iteration: 5868 Loss: 1.0668054053241824e-05\n",
      "Iteration: 5869 Loss: 1.065678251842174e-05\n",
      "Iteration: 5870 Loss: 1.064552289277363e-05\n",
      "Iteration: 5871 Loss: 1.0634275163834569e-05\n",
      "Iteration: 5872 Loss: 1.0623039318804265e-05\n",
      "Iteration: 5873 Loss: 1.0611815345238583e-05\n",
      "Iteration: 5874 Loss: 1.0600603230593449e-05\n",
      "Iteration: 5875 Loss: 1.058940296234941e-05\n",
      "Iteration: 5876 Loss: 1.0578214527978812e-05\n",
      "Iteration: 5877 Loss: 1.0567037914867991e-05\n",
      "Iteration: 5878 Loss: 1.0555873110753613e-05\n",
      "Iteration: 5879 Loss: 1.054472010304778e-05\n",
      "Iteration: 5880 Loss: 1.0533578879284735e-05\n",
      "Iteration: 5881 Loss: 1.052244942701436e-05\n",
      "Iteration: 5882 Loss: 1.051133173379916e-05\n",
      "Iteration: 5883 Loss: 1.0500225787212577e-05\n",
      "Iteration: 5884 Loss: 1.0489131574848606e-05\n",
      "Iteration: 5885 Loss: 1.04780490845072e-05\n",
      "Iteration: 5886 Loss: 1.0466978303399873e-05\n",
      "Iteration: 5887 Loss: 1.0455919219358384e-05\n",
      "Iteration: 5888 Loss: 1.0444871820224039e-05\n",
      "Iteration: 5889 Loss: 1.043383609324688e-05\n",
      "Iteration: 5890 Loss: 1.0422812026095773e-05\n",
      "Iteration: 5891 Loss: 1.0411799606859736e-05\n",
      "Iteration: 5892 Loss: 1.0400798822819955e-05\n",
      "Iteration: 5893 Loss: 1.038980966208816e-05\n",
      "Iteration: 5894 Loss: 1.0378832112185859e-05\n",
      "Iteration: 5895 Loss: 1.0367866160839463e-05\n",
      "Iteration: 5896 Loss: 1.0356911795800394e-05\n",
      "Iteration: 5897 Loss: 1.0345969004824128e-05\n",
      "Iteration: 5898 Loss: 1.033503777568175e-05\n",
      "Iteration: 5899 Loss: 1.0324118096160033e-05\n",
      "Iteration: 5900 Loss: 1.0313209954050215e-05\n",
      "Iteration: 5901 Loss: 1.0302313337165734e-05\n",
      "Iteration: 5902 Loss: 1.029142823333085e-05\n",
      "Iteration: 5903 Loss: 1.0280554630380205e-05\n",
      "Iteration: 5904 Loss: 1.026969251615992e-05\n",
      "Iteration: 5905 Loss: 1.0258841878534361e-05\n",
      "Iteration: 5906 Loss: 1.0248002705372157e-05\n",
      "Iteration: 5907 Loss: 1.0237174984569649e-05\n",
      "Iteration: 5908 Loss: 1.0226358704021896e-05\n",
      "Iteration: 5909 Loss: 1.0215553851639108e-05\n",
      "Iteration: 5910 Loss: 1.020476041534905e-05\n",
      "Iteration: 5911 Loss: 1.0193978383086638e-05\n",
      "Iteration: 5912 Loss: 1.0183207742807242e-05\n",
      "Iteration: 5913 Loss: 1.0172448482476722e-05\n",
      "Iteration: 5914 Loss: 1.0161700590063422e-05\n",
      "Iteration: 5915 Loss: 1.0150964053560985e-05\n",
      "Iteration: 5916 Loss: 1.0140238860967934e-05\n",
      "Iteration: 5917 Loss: 1.0129525000305174e-05\n",
      "Iteration: 5918 Loss: 1.0118822459592961e-05\n",
      "Iteration: 5919 Loss: 1.0108131226874262e-05\n",
      "Iteration: 5920 Loss: 1.0097451290201032e-05\n",
      "Iteration: 5921 Loss: 1.008678263763603e-05\n",
      "Iteration: 5922 Loss: 1.0076125257260586e-05\n",
      "Iteration: 5923 Loss: 1.0065479137160158e-05\n",
      "Iteration: 5924 Loss: 1.005484426544125e-05\n",
      "Iteration: 5925 Loss: 1.0044220630219658e-05\n",
      "Iteration: 5926 Loss: 1.0033608219619092e-05\n",
      "Iteration: 5927 Loss: 1.0023007021784555e-05\n",
      "Iteration: 5928 Loss: 1.0012417025066787e-05\n",
      "Iteration: 5929 Loss: 1.0001838217230138e-05\n",
      "Iteration: 5930 Loss: 9.99127058665554e-06\n",
      "Iteration: 5931 Loss: 9.980714121528822e-06\n",
      "Iteration: 5932 Loss: 9.970168810057229e-06\n",
      "Iteration: 5933 Loss: 9.959634640453963e-06\n",
      "Iteration: 5934 Loss: 9.949111600746953e-06\n",
      "Iteration: 5935 Loss: 9.93859967957669e-06\n",
      "Iteration: 5936 Loss: 9.928098864997395e-06\n",
      "Iteration: 5937 Loss: 9.917609145272225e-06\n",
      "Iteration: 5938 Loss: 9.907130508679783e-06\n",
      "Iteration: 5939 Loss: 9.89666294351082e-06\n",
      "Iteration: 5940 Loss: 9.886206438064151e-06\n",
      "Iteration: 5941 Loss: 9.875760980656569e-06\n",
      "Iteration: 5942 Loss: 9.865326559616064e-06\n",
      "Iteration: 5943 Loss: 9.854903163481658e-06\n",
      "Iteration: 5944 Loss: 9.844490779999671e-06\n",
      "Iteration: 5945 Loss: 9.834089398143015e-06\n",
      "Iteration: 5946 Loss: 9.823699006081135e-06\n",
      "Iteration: 5947 Loss: 9.813319592209504e-06\n",
      "Iteration: 5948 Loss: 9.802951145118996e-06\n",
      "Iteration: 5949 Loss: 9.792593652829967e-06\n",
      "Iteration: 5950 Loss: 9.782247103967734e-06\n",
      "Iteration: 5951 Loss: 9.771911486965601e-06\n",
      "Iteration: 5952 Loss: 9.761586790272053e-06\n",
      "Iteration: 5953 Loss: 9.751273002353825e-06\n",
      "Iteration: 5954 Loss: 9.740970111890328e-06\n",
      "Iteration: 5955 Loss: 9.730678106840458e-06\n",
      "Iteration: 5956 Loss: 9.720396975937962e-06\n",
      "Iteration: 5957 Loss: 9.710126708093538e-06\n",
      "Iteration: 5958 Loss: 9.699867291198895e-06\n",
      "Iteration: 5959 Loss: 9.689618714307156e-06\n",
      "Iteration: 5960 Loss: 9.679380965761928e-06\n",
      "Iteration: 5961 Loss: 9.669154034124258e-06\n",
      "Iteration: 5962 Loss: 9.65893790796606e-06\n",
      "Iteration: 5963 Loss: 9.648732575866876e-06\n",
      "Iteration: 5964 Loss: 9.63853802642416e-06\n",
      "Iteration: 5965 Loss: 9.628354248247796e-06\n",
      "Iteration: 5966 Loss: 9.61818122995363e-06\n",
      "Iteration: 5967 Loss: 9.608018960174507e-06\n",
      "Iteration: 5968 Loss: 9.597867427554312e-06\n",
      "Iteration: 5969 Loss: 9.587726620746463e-06\n",
      "Iteration: 5970 Loss: 9.57759652842034e-06\n",
      "Iteration: 5971 Loss: 9.567477139252974e-06\n",
      "Iteration: 5972 Loss: 9.557368441938764e-06\n",
      "Iteration: 5973 Loss: 9.547270425181334e-06\n",
      "Iteration: 5974 Loss: 9.53718307769242e-06\n",
      "Iteration: 5975 Loss: 9.527106388200392e-06\n",
      "Iteration: 5976 Loss: 9.517040345445997e-06\n",
      "Iteration: 5977 Loss: 9.50698493817849e-06\n",
      "Iteration: 5978 Loss: 9.496940155161997e-06\n",
      "Iteration: 5979 Loss: 9.486905985171567e-06\n",
      "Iteration: 5980 Loss: 9.47688241698935e-06\n",
      "Iteration: 5981 Loss: 9.466869439421863e-06\n",
      "Iteration: 5982 Loss: 9.456867041269716e-06\n",
      "Iteration: 5983 Loss: 9.446875211363234e-06\n",
      "Iteration: 5984 Loss: 9.436893938533705e-06\n",
      "Iteration: 5985 Loss: 9.426923211627642e-06\n",
      "Iteration: 5986 Loss: 9.416963019496335e-06\n",
      "Iteration: 5987 Loss: 9.407013351017433e-06\n",
      "Iteration: 5988 Loss: 9.397074195383615e-06\n",
      "Iteration: 5989 Loss: 9.387145541058797e-06\n",
      "Iteration: 5990 Loss: 9.377227376744503e-06\n",
      "Iteration: 5991 Loss: 9.36731969178762e-06\n",
      "Iteration: 5992 Loss: 9.357422475002216e-06\n",
      "Iteration: 5993 Loss: 9.347535715126096e-06\n",
      "Iteration: 5994 Loss: 9.337659401716051e-06\n",
      "Iteration: 5995 Loss: 9.32779352312879e-06\n",
      "Iteration: 5996 Loss: 9.317938068544242e-06\n",
      "Iteration: 5997 Loss: 9.308093026945311e-06\n",
      "Iteration: 5998 Loss: 9.298258387127268e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5999 Loss: 9.288434138300753e-06\n",
      "Iteration: 6000 Loss: 9.278620269693855e-06\n",
      "Iteration: 6001 Loss: 9.2688167701349e-06\n",
      "Iteration: 6002 Loss: 9.259023628666842e-06\n",
      "Iteration: 6003 Loss: 9.249240834348235e-06\n",
      "Iteration: 6004 Loss: 9.239468376242829e-06\n",
      "Iteration: 6005 Loss: 9.229706243432406e-06\n",
      "Iteration: 6006 Loss: 9.219954425007866e-06\n",
      "Iteration: 6007 Loss: 9.210212910070684e-06\n",
      "Iteration: 6008 Loss: 9.200481687733906e-06\n",
      "Iteration: 6009 Loss: 9.190760747123233e-06\n",
      "Iteration: 6010 Loss: 9.181050077376598e-06\n",
      "Iteration: 6011 Loss: 9.17134966764001e-06\n",
      "Iteration: 6012 Loss: 9.16165950707195e-06\n",
      "Iteration: 6013 Loss: 9.151979584960847e-06\n",
      "Iteration: 6014 Loss: 9.142309890260491e-06\n",
      "Iteration: 6015 Loss: 9.1326504122748e-06\n",
      "Iteration: 6016 Loss: 9.123001140214395e-06\n",
      "Iteration: 6017 Loss: 9.11336206329324e-06\n",
      "Iteration: 6018 Loss: 9.103733170740506e-06\n",
      "Iteration: 6019 Loss: 9.094114451795927e-06\n",
      "Iteration: 6020 Loss: 9.084505895707168e-06\n",
      "Iteration: 6021 Loss: 9.074907491740896e-06\n",
      "Iteration: 6022 Loss: 9.065319229054405e-06\n",
      "Iteration: 6023 Loss: 9.055741097162964e-06\n",
      "Iteration: 6024 Loss: 9.046173085241028e-06\n",
      "Iteration: 6025 Loss: 9.036615182605348e-06\n",
      "Iteration: 6026 Loss: 9.027067378570685e-06\n",
      "Iteration: 6027 Loss: 9.017529662468587e-06\n",
      "Iteration: 6028 Loss: 9.008002023637984e-06\n",
      "Iteration: 6029 Loss: 8.998484451432962e-06\n",
      "Iteration: 6030 Loss: 8.988976935218817e-06\n",
      "Iteration: 6031 Loss: 8.979479464484063e-06\n",
      "Iteration: 6032 Loss: 8.96999202827035e-06\n",
      "Iteration: 6033 Loss: 8.960514616319048e-06\n",
      "Iteration: 6034 Loss: 8.951047218042772e-06\n",
      "Iteration: 6035 Loss: 8.941589822625953e-06\n",
      "Iteration: 6036 Loss: 8.932142419618196e-06\n",
      "Iteration: 6037 Loss: 8.922704998463798e-06\n",
      "Iteration: 6038 Loss: 8.913277548610665e-06\n",
      "Iteration: 6039 Loss: 8.903860059528596e-06\n",
      "Iteration: 6040 Loss: 8.894452520574738e-06\n",
      "Iteration: 6041 Loss: 8.88505492146815e-06\n",
      "Iteration: 6042 Loss: 8.875667251591323e-06\n",
      "Iteration: 6043 Loss: 8.866289500451372e-06\n",
      "Iteration: 6044 Loss: 8.856921657571512e-06\n",
      "Iteration: 6045 Loss: 8.847563712483685e-06\n",
      "Iteration: 6046 Loss: 8.838215654725954e-06\n",
      "Iteration: 6047 Loss: 8.828877473854005e-06\n",
      "Iteration: 6048 Loss: 8.819549159427032e-06\n",
      "Iteration: 6049 Loss: 8.810230701033334e-06\n",
      "Iteration: 6050 Loss: 8.800922088246907e-06\n",
      "Iteration: 6051 Loss: 8.791623310671138e-06\n",
      "Iteration: 6052 Loss: 8.782334357912488e-06\n",
      "Iteration: 6053 Loss: 8.773055219586318e-06\n",
      "Iteration: 6054 Loss: 8.763785885335387e-06\n",
      "Iteration: 6055 Loss: 8.754526344789383e-06\n",
      "Iteration: 6056 Loss: 8.7452765876009e-06\n",
      "Iteration: 6057 Loss: 8.736036603439005e-06\n",
      "Iteration: 6058 Loss: 8.72680638197895e-06\n",
      "Iteration: 6059 Loss: 8.717585912895033e-06\n",
      "Iteration: 6060 Loss: 8.708375185894727e-06\n",
      "Iteration: 6061 Loss: 8.699174190681269e-06\n",
      "Iteration: 6062 Loss: 8.68998291697017e-06\n",
      "Iteration: 6063 Loss: 8.680801354488412e-06\n",
      "Iteration: 6064 Loss: 8.67162949298173e-06\n",
      "Iteration: 6065 Loss: 8.662467322191658e-06\n",
      "Iteration: 6066 Loss: 8.653314831890992e-06\n",
      "Iteration: 6067 Loss: 8.644172011842347e-06\n",
      "Iteration: 6068 Loss: 8.635038851828116e-06\n",
      "Iteration: 6069 Loss: 8.62591534164997e-06\n",
      "Iteration: 6070 Loss: 8.616801471100382e-06\n",
      "Iteration: 6071 Loss: 8.60769723000441e-06\n",
      "Iteration: 6072 Loss: 8.598602608188275e-06\n",
      "Iteration: 6073 Loss: 8.589517595478176e-06\n",
      "Iteration: 6074 Loss: 8.58044218173308e-06\n",
      "Iteration: 6075 Loss: 8.571376356800694e-06\n",
      "Iteration: 6076 Loss: 8.562320110556032e-06\n",
      "Iteration: 6077 Loss: 8.553273432879718e-06\n",
      "Iteration: 6078 Loss: 8.544236313658798e-06\n",
      "Iteration: 6079 Loss: 8.535208742795813e-06\n",
      "Iteration: 6080 Loss: 8.526190710199436e-06\n",
      "Iteration: 6081 Loss: 8.517182205794865e-06\n",
      "Iteration: 6082 Loss: 8.508183219511002e-06\n",
      "Iteration: 6083 Loss: 8.49919374149837e-06\n",
      "Iteration: 6084 Loss: 8.4902137613038e-06\n",
      "Iteration: 6085 Loss: 8.481243269098312e-06\n",
      "Iteration: 6086 Loss: 8.472282255050129e-06\n",
      "Iteration: 6087 Loss: 8.463330708546023e-06\n",
      "Iteration: 6088 Loss: 8.45438862018772e-06\n",
      "Iteration: 6089 Loss: 8.445455979778788e-06\n",
      "Iteration: 6090 Loss: 8.436532777136419e-06\n",
      "Iteration: 6091 Loss: 8.427619002693134e-06\n",
      "Iteration: 6092 Loss: 8.418714646281263e-06\n",
      "Iteration: 6093 Loss: 8.409819697956925e-06\n",
      "Iteration: 6094 Loss: 8.400934147774567e-06\n",
      "Iteration: 6095 Loss: 8.392057985804247e-06\n",
      "Iteration: 6096 Loss: 8.383191202127398e-06\n",
      "Iteration: 6097 Loss: 8.374333786834504e-06\n",
      "Iteration: 6098 Loss: 8.365485730033959e-06\n",
      "Iteration: 6099 Loss: 8.356647021830195e-06\n",
      "Iteration: 6100 Loss: 8.347817652543691e-06\n",
      "Iteration: 6101 Loss: 8.338997611834432e-06\n",
      "Iteration: 6102 Loss: 8.330186890200391e-06\n",
      "Iteration: 6103 Loss: 8.321385477723787e-06\n",
      "Iteration: 6104 Loss: 8.312593364562486e-06\n",
      "Iteration: 6105 Loss: 8.303810540775098e-06\n",
      "Iteration: 6106 Loss: 8.295036996777984e-06\n",
      "Iteration: 6107 Loss: 8.286272722651565e-06\n",
      "Iteration: 6108 Loss: 8.277517708606742e-06\n",
      "Iteration: 6109 Loss: 8.268771944850787e-06\n",
      "Iteration: 6110 Loss: 8.260035421611942e-06\n",
      "Iteration: 6111 Loss: 8.251308129133807e-06\n",
      "Iteration: 6112 Loss: 8.242590057653637e-06\n",
      "Iteration: 6113 Loss: 8.233881197440963e-06\n",
      "Iteration: 6114 Loss: 8.225181538750354e-06\n",
      "Iteration: 6115 Loss: 8.21649107186614e-06\n",
      "Iteration: 6116 Loss: 8.207809787079207e-06\n",
      "Iteration: 6117 Loss: 8.199137674682397e-06\n",
      "Iteration: 6118 Loss: 8.190474724985604e-06\n",
      "Iteration: 6119 Loss: 8.181820928311511e-06\n",
      "Iteration: 6120 Loss: 8.173176274988316e-06\n",
      "Iteration: 6121 Loss: 8.16454075534922e-06\n",
      "Iteration: 6122 Loss: 8.155914359755739e-06\n",
      "Iteration: 6123 Loss: 8.147297078552134e-06\n",
      "Iteration: 6124 Loss: 8.138688902122995e-06\n",
      "Iteration: 6125 Loss: 8.130089820841926e-06\n",
      "Iteration: 6126 Loss: 8.12149982509604e-06\n",
      "Iteration: 6127 Loss: 8.112918905292833e-06\n",
      "Iteration: 6128 Loss: 8.10434705183949e-06\n",
      "Iteration: 6129 Loss: 8.095784255151675e-06\n",
      "Iteration: 6130 Loss: 8.087230505670355e-06\n",
      "Iteration: 6131 Loss: 8.078685793830318e-06\n",
      "Iteration: 6132 Loss: 8.070150110083334e-06\n",
      "Iteration: 6133 Loss: 8.061623444893707e-06\n",
      "Iteration: 6134 Loss: 8.053105788727022e-06\n",
      "Iteration: 6135 Loss: 8.044597132068634e-06\n",
      "Iteration: 6136 Loss: 8.036097465612535e-06\n",
      "Iteration: 6137 Loss: 8.02760677945415e-06\n",
      "Iteration: 6138 Loss: 8.019125064305694e-06\n",
      "Iteration: 6139 Loss: 8.010652310690931e-06\n",
      "Iteration: 6140 Loss: 8.002188508940614e-06\n",
      "Iteration: 6141 Loss: 7.993733649996044e-06\n",
      "Iteration: 6142 Loss: 7.985287724213841e-06\n",
      "Iteration: 6143 Loss: 7.976850722144437e-06\n",
      "Iteration: 6144 Loss: 7.96842263436866e-06\n",
      "Iteration: 6145 Loss: 7.9600034514669e-06\n",
      "Iteration: 6146 Loss: 7.951593164026954e-06\n",
      "Iteration: 6147 Loss: 7.943191762650799e-06\n",
      "Iteration: 6148 Loss: 7.9347992379479e-06\n",
      "Iteration: 6149 Loss: 7.926415580545835e-06\n",
      "Iteration: 6150 Loss: 7.918040781073377e-06\n",
      "Iteration: 6151 Loss: 7.909674830281699e-06\n",
      "Iteration: 6152 Loss: 7.901317718595995e-06\n",
      "Iteration: 6153 Loss: 7.892969436790897e-06\n",
      "Iteration: 6154 Loss: 7.884629975538913e-06\n",
      "Iteration: 6155 Loss: 7.876299325517237e-06\n",
      "Iteration: 6156 Loss: 7.867977477419435e-06\n",
      "Iteration: 6157 Loss: 7.859664421940752e-06\n",
      "Iteration: 6158 Loss: 7.851360149682532e-06\n",
      "Iteration: 6159 Loss: 7.843064651703673e-06\n",
      "Iteration: 6160 Loss: 7.834777918395333e-06\n",
      "Iteration: 6161 Loss: 7.826499940602794e-06\n",
      "Iteration: 6162 Loss: 7.818230709084034e-06\n",
      "Iteration: 6163 Loss: 7.809970214593764e-06\n",
      "Iteration: 6164 Loss: 7.801718447788753e-06\n",
      "Iteration: 6165 Loss: 7.793475399669212e-06\n",
      "Iteration: 6166 Loss: 7.785241060920929e-06\n",
      "Iteration: 6167 Loss: 7.777015422328777e-06\n",
      "Iteration: 6168 Loss: 7.7687984747092e-06\n",
      "Iteration: 6169 Loss: 7.760590208880643e-06\n",
      "Iteration: 6170 Loss: 7.752390615663145e-06\n",
      "Iteration: 6171 Loss: 7.744199685895451e-06\n",
      "Iteration: 6172 Loss: 7.736017410430463e-06\n",
      "Iteration: 6173 Loss: 7.727843780112857e-06\n",
      "Iteration: 6174 Loss: 7.719678785819011e-06\n",
      "Iteration: 6175 Loss: 7.711522418419012e-06\n",
      "Iteration: 6176 Loss: 7.70337466880431e-06\n",
      "Iteration: 6177 Loss: 7.695235527858411e-06\n",
      "Iteration: 6178 Loss: 7.687104986609246e-06\n",
      "Iteration: 6179 Loss: 7.678983035622132e-06\n",
      "Iteration: 6180 Loss: 7.670869666165755e-06\n",
      "Iteration: 6181 Loss: 7.662764869063104e-06\n",
      "Iteration: 6182 Loss: 7.65466863524799e-06\n",
      "Iteration: 6183 Loss: 7.646580955681487e-06\n",
      "Iteration: 6184 Loss: 7.638501821319557e-06\n",
      "Iteration: 6185 Loss: 7.630431223250867e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6186 Loss: 7.622369152227108e-06\n",
      "Iteration: 6187 Loss: 7.614315599352776e-06\n",
      "Iteration: 6188 Loss: 7.606270555627226e-06\n",
      "Iteration: 6189 Loss: 7.598234012061194e-06\n",
      "Iteration: 6190 Loss: 7.590205959672312e-06\n",
      "Iteration: 6191 Loss: 7.582186389490056e-06\n",
      "Iteration: 6192 Loss: 7.574175292550997e-06\n",
      "Iteration: 6193 Loss: 7.5661726599058235e-06\n",
      "Iteration: 6194 Loss: 7.558178482608193e-06\n",
      "Iteration: 6195 Loss: 7.550192751724565e-06\n",
      "Iteration: 6196 Loss: 7.5422154583320085e-06\n",
      "Iteration: 6197 Loss: 7.534246593397497e-06\n",
      "Iteration: 6198 Loss: 7.52628614845245e-06\n",
      "Iteration: 6199 Loss: 7.518334114081297e-06\n",
      "Iteration: 6200 Loss: 7.510390481597481e-06\n",
      "Iteration: 6201 Loss: 7.502455242125057e-06\n",
      "Iteration: 6202 Loss: 7.4945283867962205e-06\n",
      "Iteration: 6203 Loss: 7.486609906748663e-06\n",
      "Iteration: 6204 Loss: 7.478699793140218e-06\n",
      "Iteration: 6205 Loss: 7.470798037121839e-06\n",
      "Iteration: 6206 Loss: 7.462904629871835e-06\n",
      "Iteration: 6207 Loss: 7.4550195623640075e-06\n",
      "Iteration: 6208 Loss: 7.447142826191877e-06\n",
      "Iteration: 6209 Loss: 7.4392744123490504e-06\n",
      "Iteration: 6210 Loss: 7.43141431204192e-06\n",
      "Iteration: 6211 Loss: 7.423562516490683e-06\n",
      "Iteration: 6212 Loss: 7.415719016913155e-06\n",
      "Iteration: 6213 Loss: 7.407883804552643e-06\n",
      "Iteration: 6214 Loss: 7.400056870647786e-06\n",
      "Iteration: 6215 Loss: 7.392238206453911e-06\n",
      "Iteration: 6216 Loss: 7.384427803232547e-06\n",
      "Iteration: 6217 Loss: 7.376625652255992e-06\n",
      "Iteration: 6218 Loss: 7.368831744919758e-06\n",
      "Iteration: 6219 Loss: 7.361046072285344e-06\n",
      "Iteration: 6220 Loss: 7.3532686257620785e-06\n",
      "Iteration: 6221 Loss: 7.345499396551592e-06\n",
      "Iteration: 6222 Loss: 7.337738376194263e-06\n",
      "Iteration: 6223 Loss: 7.329985555909565e-06\n",
      "Iteration: 6224 Loss: 7.322240927024507e-06\n",
      "Iteration: 6225 Loss: 7.314504480886648e-06\n",
      "Iteration: 6226 Loss: 7.30677620885944e-06\n",
      "Iteration: 6227 Loss: 7.299056102291773e-06\n",
      "Iteration: 6228 Loss: 7.291344152570414e-06\n",
      "Iteration: 6229 Loss: 7.283640351067919e-06\n",
      "Iteration: 6230 Loss: 7.275944689176694e-06\n",
      "Iteration: 6231 Loss: 7.268257158299489e-06\n",
      "Iteration: 6232 Loss: 7.260577749841189e-06\n",
      "Iteration: 6233 Loss: 7.25290645522337e-06\n",
      "Iteration: 6234 Loss: 7.245243265872366e-06\n",
      "Iteration: 6235 Loss: 7.237588173222418e-06\n",
      "Iteration: 6236 Loss: 7.229941168721761e-06\n",
      "Iteration: 6237 Loss: 7.222302243824903e-06\n",
      "Iteration: 6238 Loss: 7.214671390305766e-06\n",
      "Iteration: 6239 Loss: 7.207048599012061e-06\n",
      "Iteration: 6240 Loss: 7.1994338619360645e-06\n",
      "Iteration: 6241 Loss: 7.191827170170048e-06\n",
      "Iteration: 6242 Loss: 7.1842285152104e-06\n",
      "Iteration: 6243 Loss: 7.1766378889712e-06\n",
      "Iteration: 6244 Loss: 7.169055282762301e-06\n",
      "Iteration: 6245 Loss: 7.1614806881188415e-06\n",
      "Iteration: 6246 Loss: 7.153914096366108e-06\n",
      "Iteration: 6247 Loss: 7.146355499143296e-06\n",
      "Iteration: 6248 Loss: 7.138804888428021e-06\n",
      "Iteration: 6249 Loss: 7.131262255464715e-06\n",
      "Iteration: 6250 Loss: 7.1237275920324e-06\n",
      "Iteration: 6251 Loss: 7.1162008893068935e-06\n",
      "Iteration: 6252 Loss: 7.108682139074878e-06\n",
      "Iteration: 6253 Loss: 7.1011713329304315e-06\n",
      "Iteration: 6254 Loss: 7.093668462489938e-06\n",
      "Iteration: 6255 Loss: 7.086173519363706e-06\n",
      "Iteration: 6256 Loss: 7.078686495175828e-06\n",
      "Iteration: 6257 Loss: 7.071207381874528e-06\n",
      "Iteration: 6258 Loss: 7.063736170474028e-06\n",
      "Iteration: 6259 Loss: 7.0562728529330626e-06\n",
      "Iteration: 6260 Loss: 7.048817420920807e-06\n",
      "Iteration: 6261 Loss: 7.0413698662976124e-06\n",
      "Iteration: 6262 Loss: 7.03393018034463e-06\n",
      "Iteration: 6263 Loss: 7.026498354945747e-06\n",
      "Iteration: 6264 Loss: 7.019074381596428e-06\n",
      "Iteration: 6265 Loss: 7.011658252288162e-06\n",
      "Iteration: 6266 Loss: 7.004249958560185e-06\n",
      "Iteration: 6267 Loss: 6.9968494924202885e-06\n",
      "Iteration: 6268 Loss: 6.989456845395602e-06\n",
      "Iteration: 6269 Loss: 6.9820720092282195e-06\n",
      "Iteration: 6270 Loss: 6.974694975662335e-06\n",
      "Iteration: 6271 Loss: 6.967325736455518e-06\n",
      "Iteration: 6272 Loss: 6.959964283373188e-06\n",
      "Iteration: 6273 Loss: 6.952610608188119e-06\n",
      "Iteration: 6274 Loss: 6.945264702679968e-06\n",
      "Iteration: 6275 Loss: 6.937926558644916e-06\n",
      "Iteration: 6276 Loss: 6.930596167874995e-06\n",
      "Iteration: 6277 Loss: 6.9232735221838025e-06\n",
      "Iteration: 6278 Loss: 6.91595861339071e-06\n",
      "Iteration: 6279 Loss: 6.908651433311528e-06\n",
      "Iteration: 6280 Loss: 6.901351973789332e-06\n",
      "Iteration: 6281 Loss: 6.894060226659768e-06\n",
      "Iteration: 6282 Loss: 6.886776183781146e-06\n",
      "Iteration: 6283 Loss: 6.87949983701067e-06\n",
      "Iteration: 6284 Loss: 6.872231178216001e-06\n",
      "Iteration: 6285 Loss: 6.864970199273113e-06\n",
      "Iteration: 6286 Loss: 6.857716892067543e-06\n",
      "Iteration: 6287 Loss: 6.850471248498388e-06\n",
      "Iteration: 6288 Loss: 6.8432332604629415e-06\n",
      "Iteration: 6289 Loss: 6.836002919875638e-06\n",
      "Iteration: 6290 Loss: 6.8287802186554694e-06\n",
      "Iteration: 6291 Loss: 6.821565148730097e-06\n",
      "Iteration: 6292 Loss: 6.814357702038429e-06\n",
      "Iteration: 6293 Loss: 6.807157870522965e-06\n",
      "Iteration: 6294 Loss: 6.79996564614133e-06\n",
      "Iteration: 6295 Loss: 6.792781020854845e-06\n",
      "Iteration: 6296 Loss: 6.785603986634051e-06\n",
      "Iteration: 6297 Loss: 6.778434535457856e-06\n",
      "Iteration: 6298 Loss: 6.771272659319026e-06\n",
      "Iteration: 6299 Loss: 6.764118350202092e-06\n",
      "Iteration: 6300 Loss: 6.756971600125156e-06\n",
      "Iteration: 6301 Loss: 6.749832401097594e-06\n",
      "Iteration: 6302 Loss: 6.742700745133975e-06\n",
      "Iteration: 6303 Loss: 6.735576624275394e-06\n",
      "Iteration: 6304 Loss: 6.728460030555171e-06\n",
      "Iteration: 6305 Loss: 6.721350956018875e-06\n",
      "Iteration: 6306 Loss: 6.714249392722132e-06\n",
      "Iteration: 6307 Loss: 6.707155332730574e-06\n",
      "Iteration: 6308 Loss: 6.700068768117404e-06\n",
      "Iteration: 6309 Loss: 6.6929896909633016e-06\n",
      "Iteration: 6310 Loss: 6.685918093354536e-06\n",
      "Iteration: 6311 Loss: 6.678853967389451e-06\n",
      "Iteration: 6312 Loss: 6.671797305175394e-06\n",
      "Iteration: 6313 Loss: 6.664748098823122e-06\n",
      "Iteration: 6314 Loss: 6.657706340458554e-06\n",
      "Iteration: 6315 Loss: 6.650672022210213e-06\n",
      "Iteration: 6316 Loss: 6.643645136217421e-06\n",
      "Iteration: 6317 Loss: 6.636625674626951e-06\n",
      "Iteration: 6318 Loss: 6.62961362959822e-06\n",
      "Iteration: 6319 Loss: 6.622608993288675e-06\n",
      "Iteration: 6320 Loss: 6.615611757875644e-06\n",
      "Iteration: 6321 Loss: 6.608621915536803e-06\n",
      "Iteration: 6322 Loss: 6.601639458462725e-06\n",
      "Iteration: 6323 Loss: 6.594664378848366e-06\n",
      "Iteration: 6324 Loss: 6.587696668899479e-06\n",
      "Iteration: 6325 Loss: 6.580736320832812e-06\n",
      "Iteration: 6326 Loss: 6.5737833268649455e-06\n",
      "Iteration: 6327 Loss: 6.56683767922777e-06\n",
      "Iteration: 6328 Loss: 6.5598993701597366e-06\n",
      "Iteration: 6329 Loss: 6.55296839210818e-06\n",
      "Iteration: 6330 Loss: 6.546044736925257e-06\n",
      "Iteration: 6331 Loss: 6.539128397075086e-06\n",
      "Iteration: 6332 Loss: 6.532219364831386e-06\n",
      "Iteration: 6333 Loss: 6.525317632463662e-06\n",
      "Iteration: 6334 Loss: 6.5184231920666534e-06\n",
      "Iteration: 6335 Loss: 6.511536036338433e-06\n",
      "Iteration: 6336 Loss: 6.504656157375814e-06\n",
      "Iteration: 6337 Loss: 6.497783547490916e-06\n",
      "Iteration: 6338 Loss: 6.490918199006939e-06\n",
      "Iteration: 6339 Loss: 6.4840601042514625e-06\n",
      "Iteration: 6340 Loss: 6.47720925555734e-06\n",
      "Iteration: 6341 Loss: 6.470365645271514e-06\n",
      "Iteration: 6342 Loss: 6.4635292657454345e-06\n",
      "Iteration: 6343 Loss: 6.45670010933611e-06\n",
      "Iteration: 6344 Loss: 6.449878168417761e-06\n",
      "Iteration: 6345 Loss: 6.443063435364468e-06\n",
      "Iteration: 6346 Loss: 6.436255902554263e-06\n",
      "Iteration: 6347 Loss: 6.429455562388304e-06\n",
      "Iteration: 6348 Loss: 6.422662407263802e-06\n",
      "Iteration: 6349 Loss: 6.415876429591228e-06\n",
      "Iteration: 6350 Loss: 6.409097621782942e-06\n",
      "Iteration: 6351 Loss: 6.402325976262278e-06\n",
      "Iteration: 6352 Loss: 6.395561485470033e-06\n",
      "Iteration: 6353 Loss: 6.388804141839256e-06\n",
      "Iteration: 6354 Loss: 6.38205393781905e-06\n",
      "Iteration: 6355 Loss: 6.375310865870051e-06\n",
      "Iteration: 6356 Loss: 6.368574918454062e-06\n",
      "Iteration: 6357 Loss: 6.361846088043647e-06\n",
      "Iteration: 6358 Loss: 6.355124367119608e-06\n",
      "Iteration: 6359 Loss: 6.348409748169806e-06\n",
      "Iteration: 6360 Loss: 6.341702223696004e-06\n",
      "Iteration: 6361 Loss: 6.335001786188567e-06\n",
      "Iteration: 6362 Loss: 6.328308428172576e-06\n",
      "Iteration: 6363 Loss: 6.321622142166046e-06\n",
      "Iteration: 6364 Loss: 6.31494292068856e-06\n",
      "Iteration: 6365 Loss: 6.308270756285125e-06\n",
      "Iteration: 6366 Loss: 6.301605641495951e-06\n",
      "Iteration: 6367 Loss: 6.294947568874552e-06\n",
      "Iteration: 6368 Loss: 6.288296530976111e-06\n",
      "Iteration: 6369 Loss: 6.281652520372719e-06\n",
      "Iteration: 6370 Loss: 6.27501552963571e-06\n",
      "Iteration: 6371 Loss: 6.268385551465618e-06\n",
      "Iteration: 6372 Loss: 6.2617625782225404e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6373 Loss: 6.2551466026187685e-06\n",
      "Iteration: 6374 Loss: 6.248537617264909e-06\n",
      "Iteration: 6375 Loss: 6.2419356147703436e-06\n",
      "Iteration: 6376 Loss: 6.235340587760657e-06\n",
      "Iteration: 6377 Loss: 6.22875252886242e-06\n",
      "Iteration: 6378 Loss: 6.222171430716618e-06\n",
      "Iteration: 6379 Loss: 6.215597285968689e-06\n",
      "Iteration: 6380 Loss: 6.209030087067558e-06\n",
      "Iteration: 6381 Loss: 6.202469827279657e-06\n",
      "Iteration: 6382 Loss: 6.1959164986716494e-06\n",
      "Iteration: 6383 Loss: 6.1893700942038755e-06\n",
      "Iteration: 6384 Loss: 6.182830606390975e-06\n",
      "Iteration: 6385 Loss: 6.176298028010877e-06\n",
      "Iteration: 6386 Loss: 6.169772351763127e-06\n",
      "Iteration: 6387 Loss: 6.163253570153494e-06\n",
      "Iteration: 6388 Loss: 6.1567416762988746e-06\n",
      "Iteration: 6389 Loss: 6.150236662722497e-06\n",
      "Iteration: 6390 Loss: 6.143738522152906e-06\n",
      "Iteration: 6391 Loss: 6.137247247329813e-06\n",
      "Iteration: 6392 Loss: 6.130762830998163e-06\n",
      "Iteration: 6393 Loss: 6.124285265912821e-06\n",
      "Iteration: 6394 Loss: 6.117814544834709e-06\n",
      "Iteration: 6395 Loss: 6.111350660529351e-06\n",
      "Iteration: 6396 Loss: 6.1048936057785725e-06\n",
      "Iteration: 6397 Loss: 6.098443373362581e-06\n",
      "Iteration: 6398 Loss: 6.091999956275541e-06\n",
      "Iteration: 6399 Loss: 6.085563346914244e-06\n",
      "Iteration: 6400 Loss: 6.079133538086505e-06\n",
      "Iteration: 6401 Loss: 6.072710523006508e-06\n",
      "Iteration: 6402 Loss: 6.066294294298014e-06\n",
      "Iteration: 6403 Loss: 6.059884844792003e-06\n",
      "Iteration: 6404 Loss: 6.053482167321118e-06\n",
      "Iteration: 6405 Loss: 6.047086254733695e-06\n",
      "Iteration: 6406 Loss: 6.040697099879916e-06\n",
      "Iteration: 6407 Loss: 6.034314695623855e-06\n",
      "Iteration: 6408 Loss: 6.027939034828886e-06\n",
      "Iteration: 6409 Loss: 6.0215701103723085e-06\n",
      "Iteration: 6410 Loss: 6.015207915132007e-06\n",
      "Iteration: 6411 Loss: 6.008852442005173e-06\n",
      "Iteration: 6412 Loss: 6.002503683884365e-06\n",
      "Iteration: 6413 Loss: 5.996161633678513e-06\n",
      "Iteration: 6414 Loss: 5.9898262842997335e-06\n",
      "Iteration: 6415 Loss: 5.983497628661342e-06\n",
      "Iteration: 6416 Loss: 5.9771756596995705e-06\n",
      "Iteration: 6417 Loss: 5.97086037034214e-06\n",
      "Iteration: 6418 Loss: 5.964551753538117e-06\n",
      "Iteration: 6419 Loss: 5.958249802235847e-06\n",
      "Iteration: 6420 Loss: 5.951954509391996e-06\n",
      "Iteration: 6421 Loss: 5.94566586797095e-06\n",
      "Iteration: 6422 Loss: 5.939383870942996e-06\n",
      "Iteration: 6423 Loss: 5.933108511292637e-06\n",
      "Iteration: 6424 Loss: 5.9268397820033715e-06\n",
      "Iteration: 6425 Loss: 5.9205776760734645e-06\n",
      "Iteration: 6426 Loss: 5.91432218649964e-06\n",
      "Iteration: 6427 Loss: 5.908073306291642e-06\n",
      "Iteration: 6428 Loss: 5.901831028472774e-06\n",
      "Iteration: 6429 Loss: 5.895595346058241e-06\n",
      "Iteration: 6430 Loss: 5.889366252086948e-06\n",
      "Iteration: 6431 Loss: 5.883143739595738e-06\n",
      "Iteration: 6432 Loss: 5.876927801629571e-06\n",
      "Iteration: 6433 Loss: 5.870718431240004e-06\n",
      "Iteration: 6434 Loss: 5.864515621492692e-06\n",
      "Iteration: 6435 Loss: 5.858319365453652e-06\n",
      "Iteration: 6436 Loss: 5.8521296561962265e-06\n",
      "Iteration: 6437 Loss: 5.845946486807359e-06\n",
      "Iteration: 6438 Loss: 5.83976985037653e-06\n",
      "Iteration: 6439 Loss: 5.833599739995499e-06\n",
      "Iteration: 6440 Loss: 5.827436148775962e-06\n",
      "Iteration: 6441 Loss: 5.821279069826425e-06\n",
      "Iteration: 6442 Loss: 5.8151284962690054e-06\n",
      "Iteration: 6443 Loss: 5.8089844212264415e-06\n",
      "Iteration: 6444 Loss: 5.80284683803587e-06\n",
      "Iteration: 6445 Loss: 5.7967157394367644e-06\n",
      "Iteration: 6446 Loss: 5.790591118780348e-06\n",
      "Iteration: 6447 Loss: 5.784472969214033e-06\n",
      "Iteration: 6448 Loss: 5.7783612839105535e-06\n",
      "Iteration: 6449 Loss: 5.772256056234303e-06\n",
      "Iteration: 6450 Loss: 5.766157278965643e-06\n",
      "Iteration: 6451 Loss: 5.760064945485962e-06\n",
      "Iteration: 6452 Loss: 5.753979048786991e-06\n",
      "Iteration: 6453 Loss: 5.747899582469845e-06\n",
      "Iteration: 6454 Loss: 5.741826539338214e-06\n",
      "Iteration: 6455 Loss: 5.735759913009768e-06\n",
      "Iteration: 6456 Loss: 5.72969969650117e-06\n",
      "Iteration: 6457 Loss: 5.7236458830406375e-06\n",
      "Iteration: 6458 Loss: 5.717598465862611e-06\n",
      "Iteration: 6459 Loss: 5.711557438209951e-06\n",
      "Iteration: 6460 Loss: 5.70552279333159e-06\n",
      "Iteration: 6461 Loss: 5.699494524483468e-06\n",
      "Iteration: 6462 Loss: 5.693472625040766e-06\n",
      "Iteration: 6463 Loss: 5.687457088048656e-06\n",
      "Iteration: 6464 Loss: 5.681447906898427e-06\n",
      "Iteration: 6465 Loss: 5.675445074874307e-06\n",
      "Iteration: 6466 Loss: 5.6694485852665425e-06\n",
      "Iteration: 6467 Loss: 5.6634584313747966e-06\n",
      "Iteration: 6468 Loss: 5.657474606505275e-06\n",
      "Iteration: 6469 Loss: 5.651497103971872e-06\n",
      "Iteration: 6470 Loss: 5.6455259169765316e-06\n",
      "Iteration: 6471 Loss: 5.639561039079827e-06\n",
      "Iteration: 6472 Loss: 5.633602463498498e-06\n",
      "Iteration: 6473 Loss: 5.627650183575916e-06\n",
      "Iteration: 6474 Loss: 5.621704192658451e-06\n",
      "Iteration: 6475 Loss: 5.615764483897975e-06\n",
      "Iteration: 6476 Loss: 5.609831051266885e-06\n",
      "Iteration: 6477 Loss: 5.603903887525243e-06\n",
      "Iteration: 6478 Loss: 5.59798298625491e-06\n",
      "Iteration: 6479 Loss: 5.592068340830332e-06\n",
      "Iteration: 6480 Loss: 5.58615994464935e-06\n",
      "Iteration: 6481 Loss: 5.5802577911106e-06\n",
      "Iteration: 6482 Loss: 5.57436187361657e-06\n",
      "Iteration: 6483 Loss: 5.568472185575373e-06\n",
      "Iteration: 6484 Loss: 5.562588720402484e-06\n",
      "Iteration: 6485 Loss: 5.556711471532541e-06\n",
      "Iteration: 6486 Loss: 5.550840432389876e-06\n",
      "Iteration: 6487 Loss: 5.544975596421476e-06\n",
      "Iteration: 6488 Loss: 5.539116957061113e-06\n",
      "Iteration: 6489 Loss: 5.53326450777432e-06\n",
      "Iteration: 6490 Loss: 5.527418242010164e-06\n",
      "Iteration: 6491 Loss: 5.521578153241862e-06\n",
      "Iteration: 6492 Loss: 5.515744234940938e-06\n",
      "Iteration: 6493 Loss: 5.5099164805890615e-06\n",
      "Iteration: 6494 Loss: 5.504094883671661e-06\n",
      "Iteration: 6495 Loss: 5.498279437685807e-06\n",
      "Iteration: 6496 Loss: 5.492470136127945e-06\n",
      "Iteration: 6497 Loss: 5.486666972510409e-06\n",
      "Iteration: 6498 Loss: 5.480869940347554e-06\n",
      "Iteration: 6499 Loss: 5.4750790331582875e-06\n",
      "Iteration: 6500 Loss: 5.46929424447406e-06\n",
      "Iteration: 6501 Loss: 5.463515567829438e-06\n",
      "Iteration: 6502 Loss: 5.457742996764557e-06\n",
      "Iteration: 6503 Loss: 5.451976525032506e-06\n",
      "Iteration: 6504 Loss: 5.446216145785397e-06\n",
      "Iteration: 6505 Loss: 5.440461852585458e-06\n",
      "Iteration: 6506 Loss: 5.434713639404449e-06\n",
      "Iteration: 6507 Loss: 5.428971499619614e-06\n",
      "Iteration: 6508 Loss: 5.42323542681358e-06\n",
      "Iteration: 6509 Loss: 5.4175054145710744e-06\n",
      "Iteration: 6510 Loss: 5.411781456495169e-06\n",
      "Iteration: 6511 Loss: 5.40606354618544e-06\n",
      "Iteration: 6512 Loss: 5.400351677254673e-06\n",
      "Iteration: 6513 Loss: 5.3946458433175174e-06\n",
      "Iteration: 6514 Loss: 5.388946037998193e-06\n",
      "Iteration: 6515 Loss: 5.38325225492826e-06\n",
      "Iteration: 6516 Loss: 5.377564487741658e-06\n",
      "Iteration: 6517 Loss: 5.371882730086473e-06\n",
      "Iteration: 6518 Loss: 5.366206975609374e-06\n",
      "Iteration: 6519 Loss: 5.360537217969331e-06\n",
      "Iteration: 6520 Loss: 5.3548734508291896e-06\n",
      "Iteration: 6521 Loss: 5.349215667862026e-06\n",
      "Iteration: 6522 Loss: 5.3435638627445135e-06\n",
      "Iteration: 6523 Loss: 5.337918029156401e-06\n",
      "Iteration: 6524 Loss: 5.332278160994795e-06\n",
      "Iteration: 6525 Loss: 5.3266442513515285e-06\n",
      "Iteration: 6526 Loss: 5.32101629453347e-06\n",
      "Iteration: 6527 Loss: 5.315394284050577e-06\n",
      "Iteration: 6528 Loss: 5.309778213619536e-06\n",
      "Iteration: 6529 Loss: 5.304168076966728e-06\n",
      "Iteration: 6530 Loss: 5.2985638679344026e-06\n",
      "Iteration: 6531 Loss: 5.292965580032144e-06\n",
      "Iteration: 6532 Loss: 5.287373207117056e-06\n",
      "Iteration: 6533 Loss: 5.281786742940336e-06\n",
      "Iteration: 6534 Loss: 5.276206181259464e-06\n",
      "Iteration: 6535 Loss: 5.270631515838075e-06\n",
      "Iteration: 6536 Loss: 5.265062740444231e-06\n",
      "Iteration: 6537 Loss: 5.259499848856558e-06\n",
      "Iteration: 6538 Loss: 5.2539428347439185e-06\n",
      "Iteration: 6539 Loss: 5.2483916921246286e-06\n",
      "Iteration: 6540 Loss: 5.242846414681567e-06\n",
      "Iteration: 6541 Loss: 5.2373069962160605e-06\n",
      "Iteration: 6542 Loss: 5.231773430539523e-06\n",
      "Iteration: 6543 Loss: 5.22624571146734e-06\n",
      "Iteration: 6544 Loss: 5.220723832823448e-06\n",
      "Iteration: 6545 Loss: 5.215207788433647e-06\n",
      "Iteration: 6546 Loss: 5.20969757213583e-06\n",
      "Iteration: 6547 Loss: 5.204193177775301e-06\n",
      "Iteration: 6548 Loss: 5.198694599191696e-06\n",
      "Iteration: 6549 Loss: 5.1932018302497525e-06\n",
      "Iteration: 6550 Loss: 5.187714864806176e-06\n",
      "Iteration: 6551 Loss: 5.182233696730937e-06\n",
      "Iteration: 6552 Loss: 5.176758319900826e-06\n",
      "Iteration: 6553 Loss: 5.171288728390435e-06\n",
      "Iteration: 6554 Loss: 5.16582491569394e-06\n",
      "Iteration: 6555 Loss: 5.160366875900896e-06\n",
      "Iteration: 6556 Loss: 5.154914602915215e-06\n",
      "Iteration: 6557 Loss: 5.149468090642169e-06\n",
      "Iteration: 6558 Loss: 5.144027332996886e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6559 Loss: 5.138592323691004e-06\n",
      "Iteration: 6560 Loss: 5.133163057061351e-06\n",
      "Iteration: 6561 Loss: 5.127739526838136e-06\n",
      "Iteration: 6562 Loss: 5.122321726958007e-06\n",
      "Iteration: 6563 Loss: 5.116909651366813e-06\n",
      "Iteration: 6564 Loss: 5.11150329401787e-06\n",
      "Iteration: 6565 Loss: 5.1061026488716956e-06\n",
      "Iteration: 6566 Loss: 5.100707709884e-06\n",
      "Iteration: 6567 Loss: 5.0953184710334e-06\n",
      "Iteration: 6568 Loss: 5.08993492629725e-06\n",
      "Iteration: 6569 Loss: 5.0845570696562155e-06\n",
      "Iteration: 6570 Loss: 5.0791848951023076e-06\n",
      "Iteration: 6571 Loss: 5.07381839663234e-06\n",
      "Iteration: 6572 Loss: 5.068457568247099e-06\n",
      "Iteration: 6573 Loss: 5.06310240395697e-06\n",
      "Iteration: 6574 Loss: 5.057752897780166e-06\n",
      "Iteration: 6575 Loss: 5.052409043732852e-06\n",
      "Iteration: 6576 Loss: 5.047070835846939e-06\n",
      "Iteration: 6577 Loss: 5.041738268156112e-06\n",
      "Iteration: 6578 Loss: 5.036411334701524e-06\n",
      "Iteration: 6579 Loss: 5.031090029527807e-06\n",
      "Iteration: 6580 Loss: 5.025774346690965e-06\n",
      "Iteration: 6581 Loss: 5.020464280251309e-06\n",
      "Iteration: 6582 Loss: 5.015159824270713e-06\n",
      "Iteration: 6583 Loss: 5.009860972826399e-06\n",
      "Iteration: 6584 Loss: 5.004567719993925e-06\n",
      "Iteration: 6585 Loss: 4.999280059855163e-06\n",
      "Iteration: 6586 Loss: 4.9939979865061505e-06\n",
      "Iteration: 6587 Loss: 4.988721494042634e-06\n",
      "Iteration: 6588 Loss: 4.983450576566295e-06\n",
      "Iteration: 6589 Loss: 4.978185228186382e-06\n",
      "Iteration: 6590 Loss: 4.972925443022364e-06\n",
      "Iteration: 6591 Loss: 4.967671215193791e-06\n",
      "Iteration: 6592 Loss: 4.962422538830085e-06\n",
      "Iteration: 6593 Loss: 4.957179408065943e-06\n",
      "Iteration: 6594 Loss: 4.951941817036298e-06\n",
      "Iteration: 6595 Loss: 4.946709759894563e-06\n",
      "Iteration: 6596 Loss: 4.941483230792761e-06\n",
      "Iteration: 6597 Loss: 4.9362622240944194e-06\n",
      "Iteration: 6598 Loss: 4.931046733551836e-06\n",
      "Iteration: 6599 Loss: 4.925836753346745e-06\n",
      "Iteration: 6600 Loss: 4.920632278055515e-06\n",
      "Iteration: 6601 Loss: 4.915433301662823e-06\n",
      "Iteration: 6602 Loss: 4.910239818354067e-06\n",
      "Iteration: 6603 Loss: 4.905051822332042e-06\n",
      "Iteration: 6604 Loss: 4.899869307795819e-06\n",
      "Iteration: 6605 Loss: 4.894692269150365e-06\n",
      "Iteration: 6606 Loss: 4.889520700214839e-06\n",
      "Iteration: 6607 Loss: 4.88435459540723e-06\n",
      "Iteration: 6608 Loss: 4.8791939489568816e-06\n",
      "Iteration: 6609 Loss: 4.874038755094763e-06\n",
      "Iteration: 6610 Loss: 4.868889008060193e-06\n",
      "Iteration: 6611 Loss: 4.863744702097431e-06\n",
      "Iteration: 6612 Loss: 4.858605831257867e-06\n",
      "Iteration: 6613 Loss: 4.853472390200697e-06\n",
      "Iteration: 6614 Loss: 4.848344372988829e-06\n",
      "Iteration: 6615 Loss: 4.843221773889053e-06\n",
      "Iteration: 6616 Loss: 4.838104587178625e-06\n",
      "Iteration: 6617 Loss: 4.832992807139726e-06\n",
      "Iteration: 6618 Loss: 4.827886428057201e-06\n",
      "Iteration: 6619 Loss: 4.8227854442263365e-06\n",
      "Iteration: 6620 Loss: 4.817689849946769e-06\n",
      "Iteration: 6621 Loss: 4.812599639525359e-06\n",
      "Iteration: 6622 Loss: 4.8075148072696285e-06\n",
      "Iteration: 6623 Loss: 4.802435347501355e-06\n",
      "Iteration: 6624 Loss: 4.797361254540333e-06\n",
      "Iteration: 6625 Loss: 4.792292522718146e-06\n",
      "Iteration: 6626 Loss: 4.787229146371713e-06\n",
      "Iteration: 6627 Loss: 4.782171120042369e-06\n",
      "Iteration: 6628 Loss: 4.777118437676868e-06\n",
      "Iteration: 6629 Loss: 4.7720710938246125e-06\n",
      "Iteration: 6630 Loss: 4.7670290826505e-06\n",
      "Iteration: 6631 Loss: 4.761992398917009e-06\n",
      "Iteration: 6632 Loss: 4.756961036799294e-06\n",
      "Iteration: 6633 Loss: 4.751934990672155e-06\n",
      "Iteration: 6634 Loss: 4.746914255117606e-06\n",
      "Iteration: 6635 Loss: 4.741898824127181e-06\n",
      "Iteration: 6636 Loss: 4.736888692093669e-06\n",
      "Iteration: 6637 Loss: 4.731883853819821e-06\n",
      "Iteration: 6638 Loss: 4.726884303513157e-06\n",
      "Iteration: 6639 Loss: 4.721890035584142e-06\n",
      "Iteration: 6640 Loss: 4.716901044453267e-06\n",
      "Iteration: 6641 Loss: 4.711917324548011e-06\n",
      "Iteration: 6642 Loss: 4.706938870409479e-06\n",
      "Iteration: 6643 Loss: 4.701965676244459e-06\n",
      "Iteration: 6644 Loss: 4.696997736613535e-06\n",
      "Iteration: 6645 Loss: 4.692035045847368e-06\n",
      "Iteration: 6646 Loss: 4.6870775986293224e-06\n",
      "Iteration: 6647 Loss: 4.682125389305664e-06\n",
      "Iteration: 6648 Loss: 4.6771784123430466e-06\n",
      "Iteration: 6649 Loss: 4.672236662212878e-06\n",
      "Iteration: 6650 Loss: 4.667300133390644e-06\n",
      "Iteration: 6651 Loss: 4.662368820361153e-06\n",
      "Iteration: 6652 Loss: 4.6574427176134555e-06\n",
      "Iteration: 6653 Loss: 4.6525218196423555e-06\n",
      "Iteration: 6654 Loss: 4.647606120947607e-06\n",
      "Iteration: 6655 Loss: 4.6426956160405795e-06\n",
      "Iteration: 6656 Loss: 4.63779029942676e-06\n",
      "Iteration: 6657 Loss: 4.632890165628813e-06\n",
      "Iteration: 6658 Loss: 4.627995209167775e-06\n",
      "Iteration: 6659 Loss: 4.623105424577121e-06\n",
      "Iteration: 6660 Loss: 4.618220806504466e-06\n",
      "Iteration: 6661 Loss: 4.6133413492625805e-06\n",
      "Iteration: 6662 Loss: 4.608467047511931e-06\n",
      "Iteration: 6663 Loss: 4.603597895808528e-06\n",
      "Iteration: 6664 Loss: 4.598733888708893e-06\n",
      "Iteration: 6665 Loss: 4.593875020977087e-06\n",
      "Iteration: 6666 Loss: 4.589021286581445e-06\n",
      "Iteration: 6667 Loss: 4.5841726807023505e-06\n",
      "Iteration: 6668 Loss: 4.57932919771821e-06\n",
      "Iteration: 6669 Loss: 4.574490832216077e-06\n",
      "Iteration: 6670 Loss: 4.569657578791301e-06\n",
      "Iteration: 6671 Loss: 4.564829432041542e-06\n",
      "Iteration: 6672 Loss: 4.5600063865702984e-06\n",
      "Iteration: 6673 Loss: 4.5551884369886475e-06\n",
      "Iteration: 6674 Loss: 4.550375577912464e-06\n",
      "Iteration: 6675 Loss: 4.545567803962266e-06\n",
      "Iteration: 6676 Loss: 4.540765109766683e-06\n",
      "Iteration: 6677 Loss: 4.5359674899572976e-06\n",
      "Iteration: 6678 Loss: 4.531174939173386e-06\n",
      "Iteration: 6679 Loss: 4.5263874520593865e-06\n",
      "Iteration: 6680 Loss: 4.52160502326406e-06\n",
      "Iteration: 6681 Loss: 4.516827647447363e-06\n",
      "Iteration: 6682 Loss: 4.512055319262877e-06\n",
      "Iteration: 6683 Loss: 4.5072880333800875e-06\n",
      "Iteration: 6684 Loss: 4.502525784476731e-06\n",
      "Iteration: 6685 Loss: 4.4977685672262505e-06\n",
      "Iteration: 6686 Loss: 4.493016376307674e-06\n",
      "Iteration: 6687 Loss: 4.488269206419437e-06\n",
      "Iteration: 6688 Loss: 4.483527052252645e-06\n",
      "Iteration: 6689 Loss: 4.478789908508181e-06\n",
      "Iteration: 6690 Loss: 4.4740577697758564e-06\n",
      "Iteration: 6691 Loss: 4.4693306309995015e-06\n",
      "Iteration: 6692 Loss: 4.464608486779445e-06\n",
      "Iteration: 6693 Loss: 4.459891331839999e-06\n",
      "Iteration: 6694 Loss: 4.455179160908552e-06\n",
      "Iteration: 6695 Loss: 4.450471968720479e-06\n",
      "Iteration: 6696 Loss: 4.4457697500117674e-06\n",
      "Iteration: 6697 Loss: 4.441072499532689e-06\n",
      "Iteration: 6698 Loss: 4.436380212232701e-06\n",
      "Iteration: 6699 Loss: 4.4316928826688906e-06\n",
      "Iteration: 6700 Loss: 4.427010505196282e-06\n",
      "Iteration: 6701 Loss: 4.422333075188527e-06\n",
      "Iteration: 6702 Loss: 4.417660587332334e-06\n",
      "Iteration: 6703 Loss: 4.412993036262703e-06\n",
      "Iteration: 6704 Loss: 4.4083304167045036e-06\n",
      "Iteration: 6705 Loss: 4.4036727235343144e-06\n",
      "Iteration: 6706 Loss: 4.399019951346475e-06\n",
      "Iteration: 6707 Loss: 4.394372095343197e-06\n",
      "Iteration: 6708 Loss: 4.389729150129158e-06\n",
      "Iteration: 6709 Loss: 4.385091110513948e-06\n",
      "Iteration: 6710 Loss: 4.380457971318586e-06\n",
      "Iteration: 6711 Loss: 4.375829727160924e-06\n",
      "Iteration: 6712 Loss: 4.371206373272936e-06\n",
      "Iteration: 6713 Loss: 4.3665879042863285e-06\n",
      "Iteration: 6714 Loss: 4.361974315038971e-06\n",
      "Iteration: 6715 Loss: 4.357365600374672e-06\n",
      "Iteration: 6716 Loss: 4.352761755144589e-06\n",
      "Iteration: 6717 Loss: 4.348162774202602e-06\n",
      "Iteration: 6718 Loss: 4.3435686524100084e-06\n",
      "Iteration: 6719 Loss: 4.338979384633831e-06\n",
      "Iteration: 6720 Loss: 4.334394965744946e-06\n",
      "Iteration: 6721 Loss: 4.329815390618422e-06\n",
      "Iteration: 6722 Loss: 4.325240654338275e-06\n",
      "Iteration: 6723 Loss: 4.320670751593329e-06\n",
      "Iteration: 6724 Loss: 4.316105677072933e-06\n",
      "Iteration: 6725 Loss: 4.311545425874407e-06\n",
      "Iteration: 6726 Loss: 4.306989992705813e-06\n",
      "Iteration: 6727 Loss: 4.302439372875851e-06\n",
      "Iteration: 6728 Loss: 4.2978935610985495e-06\n",
      "Iteration: 6729 Loss: 4.293352552092024e-06\n",
      "Iteration: 6730 Loss: 4.288816341385714e-06\n",
      "Iteration: 6731 Loss: 4.284284923306403e-06\n",
      "Iteration: 6732 Loss: 4.279758292989558e-06\n",
      "Iteration: 6733 Loss: 4.275236445496313e-06\n",
      "Iteration: 6734 Loss: 4.270719375540311e-06\n",
      "Iteration: 6735 Loss: 4.2662070781859055e-06\n",
      "Iteration: 6736 Loss: 4.261699548396544e-06\n",
      "Iteration: 6737 Loss: 4.257196780930409e-06\n",
      "Iteration: 6738 Loss: 4.252698771156131e-06\n",
      "Iteration: 6739 Loss: 4.248205513850228e-06\n",
      "Iteration: 6740 Loss: 4.2437170039867465e-06\n",
      "Iteration: 6741 Loss: 4.239233236554878e-06\n",
      "Iteration: 6742 Loss: 4.234754206540098e-06\n",
      "Iteration: 6743 Loss: 4.230279908735124e-06\n",
      "Iteration: 6744 Loss: 4.22581033854489e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6745 Loss: 4.221345490455094e-06\n",
      "Iteration: 6746 Loss: 4.216885360312248e-06\n",
      "Iteration: 6747 Loss: 4.2124299424115285e-06\n",
      "Iteration: 6748 Loss: 4.207979231976314e-06\n",
      "Iteration: 6749 Loss: 4.203533224031483e-06\n",
      "Iteration: 6750 Loss: 4.199091913607557e-06\n",
      "Iteration: 6751 Loss: 4.194655295744944e-06\n",
      "Iteration: 6752 Loss: 4.190223365483357e-06\n",
      "Iteration: 6753 Loss: 4.1857961178702705e-06\n",
      "Iteration: 6754 Loss: 4.181373547958259e-06\n",
      "Iteration: 6755 Loss: 4.17695565080368e-06\n",
      "Iteration: 6756 Loss: 4.172542421470754e-06\n",
      "Iteration: 6757 Loss: 4.168133855028217e-06\n",
      "Iteration: 6758 Loss: 4.163729946546737e-06\n",
      "Iteration: 6759 Loss: 4.15933069110991e-06\n",
      "Iteration: 6760 Loss: 4.154936083795706e-06\n",
      "Iteration: 6761 Loss: 4.1505461196948826e-06\n",
      "Iteration: 6762 Loss: 4.146160793904235e-06\n",
      "Iteration: 6763 Loss: 4.141780101518799e-06\n",
      "Iteration: 6764 Loss: 4.137404037849405e-06\n",
      "Iteration: 6765 Loss: 4.13303259759747e-06\n",
      "Iteration: 6766 Loss: 4.128665776083738e-06\n",
      "Iteration: 6767 Loss: 4.124303568426682e-06\n",
      "Iteration: 6768 Loss: 4.1199459697496855e-06\n",
      "Iteration: 6769 Loss: 4.1155929751859556e-06\n",
      "Iteration: 6770 Loss: 4.111244579868649e-06\n",
      "Iteration: 6771 Loss: 4.106900778940278e-06\n",
      "Iteration: 6772 Loss: 4.102561567544942e-06\n",
      "Iteration: 6773 Loss: 4.098226940836086e-06\n",
      "Iteration: 6774 Loss: 4.093896893962251e-06\n",
      "Iteration: 6775 Loss: 4.089571422093049e-06\n",
      "Iteration: 6776 Loss: 4.085250520390479e-06\n",
      "Iteration: 6777 Loss: 4.080934184028096e-06\n",
      "Iteration: 6778 Loss: 4.0766224081810415e-06\n",
      "Iteration: 6779 Loss: 4.072315188027906e-06\n",
      "Iteration: 6780 Loss: 4.068012518762264e-06\n",
      "Iteration: 6781 Loss: 4.063714395572782e-06\n",
      "Iteration: 6782 Loss: 4.059420813648508e-06\n",
      "Iteration: 6783 Loss: 4.055131768201713e-06\n",
      "Iteration: 6784 Loss: 4.0508472544356264e-06\n",
      "Iteration: 6785 Loss: 4.046567267761735e-06\n",
      "Iteration: 6786 Loss: 4.042291802996837e-06\n",
      "Iteration: 6787 Loss: 4.0380208553623235e-06\n",
      "Iteration: 6788 Loss: 4.03375442048735e-06\n",
      "Iteration: 6789 Loss: 4.029492493402753e-06\n",
      "Iteration: 6790 Loss: 4.025235069347451e-06\n",
      "Iteration: 6791 Loss: 4.020982143562049e-06\n",
      "Iteration: 6792 Loss: 4.016733711294963e-06\n",
      "Iteration: 6793 Loss: 4.012489767997571e-06\n",
      "Iteration: 6794 Loss: 4.0082503085251324e-06\n",
      "Iteration: 6795 Loss: 4.0040153283440375e-06\n",
      "Iteration: 6796 Loss: 3.999784822919832e-06\n",
      "Iteration: 6797 Loss: 3.995558787123867e-06\n",
      "Iteration: 6798 Loss: 3.991337216232229e-06\n",
      "Iteration: 6799 Loss: 3.98712010593218e-06\n",
      "Iteration: 6800 Loss: 3.9829074511061295e-06\n",
      "Iteration: 6801 Loss: 3.978699247447007e-06\n",
      "Iteration: 6802 Loss: 3.974495490055539e-06\n",
      "Iteration: 6803 Loss: 3.97029617423292e-06\n",
      "Iteration: 6804 Loss: 3.966101295396986e-06\n",
      "Iteration: 6805 Loss: 3.961910848635368e-06\n",
      "Iteration: 6806 Loss: 3.957724829376121e-06\n",
      "Iteration: 6807 Loss: 3.953543232943839e-06\n",
      "Iteration: 6808 Loss: 3.949366054663102e-06\n",
      "Iteration: 6809 Loss: 3.945193289870896e-06\n",
      "Iteration: 6810 Loss: 3.941024933897108e-06\n",
      "Iteration: 6811 Loss: 3.93686098208808e-06\n",
      "Iteration: 6812 Loss: 3.932701429673636e-06\n",
      "Iteration: 6813 Loss: 3.928546272435765e-06\n",
      "Iteration: 6814 Loss: 3.924395505219475e-06\n",
      "Iteration: 6815 Loss: 3.9202491235787486e-06\n",
      "Iteration: 6816 Loss: 3.916107122685071e-06\n",
      "Iteration: 6817 Loss: 3.9119694983079076e-06\n",
      "Iteration: 6818 Loss: 3.907836245624078e-06\n",
      "Iteration: 6819 Loss: 3.9037073602179534e-06\n",
      "Iteration: 6820 Loss: 3.899582837071093e-06\n",
      "Iteration: 6821 Loss: 3.895462671773887e-06\n",
      "Iteration: 6822 Loss: 3.891346859723277e-06\n",
      "Iteration: 6823 Loss: 3.8872353963184665e-06\n",
      "Iteration: 6824 Loss: 3.88312827696999e-06\n",
      "Iteration: 6825 Loss: 3.879025497080847e-06\n",
      "Iteration: 6826 Loss: 3.874927052070158e-06\n",
      "Iteration: 6827 Loss: 3.870832937473321e-06\n",
      "Iteration: 6828 Loss: 3.866743148482981e-06\n",
      "Iteration: 6829 Loss: 3.862657680442825e-06\n",
      "Iteration: 6830 Loss: 3.858576529190877e-06\n",
      "Iteration: 6831 Loss: 3.8544996899636145e-06\n",
      "Iteration: 6832 Loss: 3.850427158206912e-06\n",
      "Iteration: 6833 Loss: 3.8463589293675815e-06\n",
      "Iteration: 6834 Loss: 3.842294998901267e-06\n",
      "Iteration: 6835 Loss: 3.838235362351956e-06\n",
      "Iteration: 6836 Loss: 3.834180015009473e-06\n",
      "Iteration: 6837 Loss: 3.83012895242921e-06\n",
      "Iteration: 6838 Loss: 3.826082170284109e-06\n",
      "Iteration: 6839 Loss: 3.8220396634498215e-06\n",
      "Iteration: 6840 Loss: 3.818001428013251e-06\n",
      "Iteration: 6841 Loss: 3.8139674592554105e-06\n",
      "Iteration: 6842 Loss: 3.8099377524713314e-06\n",
      "Iteration: 6843 Loss: 3.805912303759266e-06\n",
      "Iteration: 6844 Loss: 3.8018911078176354e-06\n",
      "Iteration: 6845 Loss: 3.797874160756055e-06\n",
      "Iteration: 6846 Loss: 3.793861457883704e-06\n",
      "Iteration: 6847 Loss: 3.7898529947167935e-06\n",
      "Iteration: 6848 Loss: 3.785848766773572e-06\n",
      "Iteration: 6849 Loss: 3.7818487695826694e-06\n",
      "Iteration: 6850 Loss: 3.7778529986712743e-06\n",
      "Iteration: 6851 Loss: 3.773861449576094e-06\n",
      "Iteration: 6852 Loss: 3.7698741178337673e-06\n",
      "Iteration: 6853 Loss: 3.7658909989907003e-06\n",
      "Iteration: 6854 Loss: 3.7619120885954758e-06\n",
      "Iteration: 6855 Loss: 3.7579373822012147e-06\n",
      "Iteration: 6856 Loss: 3.7539668753659396e-06\n",
      "Iteration: 6857 Loss: 3.7500005636513087e-06\n",
      "Iteration: 6858 Loss: 3.746038442626738e-06\n",
      "Iteration: 6859 Loss: 3.7420805078639874e-06\n",
      "Iteration: 6860 Loss: 3.7381267549390018e-06\n",
      "Iteration: 6861 Loss: 3.7341771794355876e-06\n",
      "Iteration: 6862 Loss: 3.730231776939111e-06\n",
      "Iteration: 6863 Loss: 3.7262905430373024e-06\n",
      "Iteration: 6864 Loss: 3.722353473330555e-06\n",
      "Iteration: 6865 Loss: 3.7184205634162998e-06\n",
      "Iteration: 6866 Loss: 3.7144918089028026e-06\n",
      "Iteration: 6867 Loss: 3.710567205391985e-06\n",
      "Iteration: 6868 Loss: 3.706646748506437e-06\n",
      "Iteration: 6869 Loss: 3.7027304338600234e-06\n",
      "Iteration: 6870 Loss: 3.698818257078095e-06\n",
      "Iteration: 6871 Loss: 3.694910213787909e-06\n",
      "Iteration: 6872 Loss: 3.69100629962261e-06\n",
      "Iteration: 6873 Loss: 3.6871065102187174e-06\n",
      "Iteration: 6874 Loss: 3.6832108412198076e-06\n",
      "Iteration: 6875 Loss: 3.679319288270088e-06\n",
      "Iteration: 6876 Loss: 3.6754318472245667e-06\n",
      "Iteration: 6877 Loss: 3.671548513132594e-06\n",
      "Iteration: 6878 Loss: 3.667669282258459e-06\n",
      "Iteration: 6879 Loss: 3.6637941500683913e-06\n",
      "Iteration: 6880 Loss: 3.659923112228581e-06\n",
      "Iteration: 6881 Loss: 3.656056164415979e-06\n",
      "Iteration: 6882 Loss: 3.652193302307258e-06\n",
      "Iteration: 6883 Loss: 3.6483345215857792e-06\n",
      "Iteration: 6884 Loss: 3.644479817940818e-06\n",
      "Iteration: 6885 Loss: 3.6406291870622608e-06\n",
      "Iteration: 6886 Loss: 3.636782624649464e-06\n",
      "Iteration: 6887 Loss: 3.632940126401229e-06\n",
      "Iteration: 6888 Loss: 3.6291016880239764e-06\n",
      "Iteration: 6889 Loss: 3.625267305231889e-06\n",
      "Iteration: 6890 Loss: 3.621436973734353e-06\n",
      "Iteration: 6891 Loss: 3.617610689254463e-06\n",
      "Iteration: 6892 Loss: 3.61378844751428e-06\n",
      "Iteration: 6893 Loss: 3.6099702442446784e-06\n",
      "Iteration: 6894 Loss: 3.60615607517837e-06\n",
      "Iteration: 6895 Loss: 3.602345936047399e-06\n",
      "Iteration: 6896 Loss: 3.5985398226016413e-06\n",
      "Iteration: 6897 Loss: 3.5947377305834028e-06\n",
      "Iteration: 6898 Loss: 3.5909396557462497e-06\n",
      "Iteration: 6899 Loss: 3.587145593844677e-06\n",
      "Iteration: 6900 Loss: 3.58335554063762e-06\n",
      "Iteration: 6901 Loss: 3.579569491888437e-06\n",
      "Iteration: 6902 Loss: 3.5757874433723464e-06\n",
      "Iteration: 6903 Loss: 3.572009390854743e-06\n",
      "Iteration: 6904 Loss: 3.5682353301201305e-06\n",
      "Iteration: 6905 Loss: 3.5644652569471235e-06\n",
      "Iteration: 6906 Loss: 3.5606991673240796e-06\n",
      "Iteration: 6907 Loss: 3.5569370566399333e-06\n",
      "Iteration: 6908 Loss: 3.5531789208951134e-06\n",
      "Iteration: 6909 Loss: 3.549424755683401e-06\n",
      "Iteration: 6910 Loss: 3.5456745572153964e-06\n",
      "Iteration: 6911 Loss: 3.541928321096775e-06\n",
      "Iteration: 6912 Loss: 3.5381860431418824e-06\n",
      "Iteration: 6913 Loss: 3.5344477191712417e-06\n",
      "Iteration: 6914 Loss: 3.5307133450016466e-06\n",
      "Iteration: 6915 Loss: 3.5269829164658784e-06\n",
      "Iteration: 6916 Loss: 3.5232564293915275e-06\n",
      "Iteration: 6917 Loss: 3.519533879613744e-06\n",
      "Iteration: 6918 Loss: 3.5158152629732302e-06\n",
      "Iteration: 6919 Loss: 3.512100575317604e-06\n",
      "Iteration: 6920 Loss: 3.5083898124898413e-06\n",
      "Iteration: 6921 Loss: 3.504682970348138e-06\n",
      "Iteration: 6922 Loss: 3.5009800447467905e-06\n",
      "Iteration: 6923 Loss: 3.4972810315494384e-06\n",
      "Iteration: 6924 Loss: 3.4935859266205133e-06\n",
      "Iteration: 6925 Loss: 3.4898947258331675e-06\n",
      "Iteration: 6926 Loss: 3.4862074250612827e-06\n",
      "Iteration: 6927 Loss: 3.482524020298407e-06\n",
      "Iteration: 6928 Loss: 3.478844507197029e-06\n",
      "Iteration: 6929 Loss: 3.4751688817617578e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6930 Loss: 3.471497139887925e-06\n",
      "Iteration: 6931 Loss: 3.4678292774685455e-06\n",
      "Iteration: 6932 Loss: 3.464165290404761e-06\n",
      "Iteration: 6933 Loss: 3.460505174805989e-06\n",
      "Iteration: 6934 Loss: 3.456848926175946e-06\n",
      "Iteration: 6935 Loss: 3.453196540518815e-06\n",
      "Iteration: 6936 Loss: 3.44954801398162e-06\n",
      "Iteration: 6937 Loss: 3.445903342368127e-06\n",
      "Iteration: 6938 Loss: 3.4422625214086802e-06\n",
      "Iteration: 6939 Loss: 3.4386255475501615e-06\n",
      "Iteration: 6940 Loss: 3.4349924161835564e-06\n",
      "Iteration: 6941 Loss: 3.431363123589916e-06\n",
      "Iteration: 6942 Loss: 3.4277376656047895e-06\n",
      "Iteration: 6943 Loss: 3.424116038171257e-06\n",
      "Iteration: 6944 Loss: 3.4204982372426436e-06\n",
      "Iteration: 6945 Loss: 3.4168842587797253e-06\n",
      "Iteration: 6946 Loss: 3.4132740987386753e-06\n",
      "Iteration: 6947 Loss: 3.409667753087945e-06\n",
      "Iteration: 6948 Loss: 3.40606521779803e-06\n",
      "Iteration: 6949 Loss: 3.4024664888417585e-06\n",
      "Iteration: 6950 Loss: 3.3988715621970425e-06\n",
      "Iteration: 6951 Loss: 3.395280433847473e-06\n",
      "Iteration: 6952 Loss: 3.3916930997795593e-06\n",
      "Iteration: 6953 Loss: 3.3881095561834675e-06\n",
      "Iteration: 6954 Loss: 3.3845297986589834e-06\n",
      "Iteration: 6955 Loss: 3.3809538233986355e-06\n",
      "Iteration: 6956 Loss: 3.377381626610233e-06\n",
      "Iteration: 6957 Loss: 3.373813203902031e-06\n",
      "Iteration: 6958 Loss: 3.370248551483624e-06\n",
      "Iteration: 6959 Loss: 3.366687665373944e-06\n",
      "Iteration: 6960 Loss: 3.3631305413925267e-06\n",
      "Iteration: 6961 Loss: 3.3595771759631183e-06\n",
      "Iteration: 6962 Loss: 3.3560275649185972e-06\n",
      "Iteration: 6963 Loss: 3.352481704288497e-06\n",
      "Iteration: 6964 Loss: 3.3489395901107636e-06\n",
      "Iteration: 6965 Loss: 3.3454012184293773e-06\n",
      "Iteration: 6966 Loss: 3.3418665852878755e-06\n",
      "Iteration: 6967 Loss: 3.338335686736369e-06\n",
      "Iteration: 6968 Loss: 3.3348085188289785e-06\n",
      "Iteration: 6969 Loss: 3.3312850776248183e-06\n",
      "Iteration: 6970 Loss: 3.3277653591879273e-06\n",
      "Iteration: 6971 Loss: 3.32424935958095e-06\n",
      "Iteration: 6972 Loss: 3.320737074877573e-06\n",
      "Iteration: 6973 Loss: 3.317228501149799e-06\n",
      "Iteration: 6974 Loss: 3.313723634279006e-06\n",
      "Iteration: 6975 Loss: 3.310222470748931e-06\n",
      "Iteration: 6976 Loss: 3.306725006446904e-06\n",
      "Iteration: 6977 Loss: 3.3032312374651593e-06\n",
      "Iteration: 6978 Loss: 3.299741159896037e-06\n",
      "Iteration: 6979 Loss: 3.296254769842453e-06\n",
      "Iteration: 6980 Loss: 3.292772063404774e-06\n",
      "Iteration: 6981 Loss: 3.2892930366964865e-06\n",
      "Iteration: 6982 Loss: 3.285817685623198e-06\n",
      "Iteration: 6983 Loss: 3.282346006706854e-06\n",
      "Iteration: 6984 Loss: 3.2788779958640612e-06\n",
      "Iteration: 6985 Loss: 3.2754136492212428e-06\n",
      "Iteration: 6986 Loss: 3.2719529629058086e-06\n",
      "Iteration: 6987 Loss: 3.268495933049354e-06\n",
      "Iteration: 6988 Loss: 3.2650425557917026e-06\n",
      "Iteration: 6989 Loss: 3.2615928272730438e-06\n",
      "Iteration: 6990 Loss: 3.2581467436363573e-06\n",
      "Iteration: 6991 Loss: 3.2547043010303117e-06\n",
      "Iteration: 6992 Loss: 3.2512654956087088e-06\n",
      "Iteration: 6993 Loss: 3.2478303235301165e-06\n",
      "Iteration: 6994 Loss: 3.244398780952522e-06\n",
      "Iteration: 6995 Loss: 3.2409708640445604e-06\n",
      "Iteration: 6996 Loss: 3.2375465689721307e-06\n",
      "Iteration: 6997 Loss: 3.234125891910671e-06\n",
      "Iteration: 6998 Loss: 3.2307088290379238e-06\n",
      "Iteration: 6999 Loss: 3.2272953765324087e-06\n",
      "Iteration: 7000 Loss: 3.2238855305821615e-06\n",
      "Iteration: 7001 Loss: 3.2204792873766305e-06\n",
      "Iteration: 7002 Loss: 3.217076643108495e-06\n",
      "Iteration: 7003 Loss: 3.21367759397476e-06\n",
      "Iteration: 7004 Loss: 3.2102821361762976e-06\n",
      "Iteration: 7005 Loss: 3.2068902659230435e-06\n",
      "Iteration: 7006 Loss: 3.2035019794155772e-06\n",
      "Iteration: 7007 Loss: 3.200117272876441e-06\n",
      "Iteration: 7008 Loss: 3.196736142517953e-06\n",
      "Iteration: 7009 Loss: 3.1933585847656592e-06\n",
      "Iteration: 7010 Loss: 3.1899845954431525e-06\n",
      "Iteration: 7011 Loss: 3.18661417097585e-06\n",
      "Iteration: 7012 Loss: 3.183247307603696e-06\n",
      "Iteration: 7013 Loss: 3.179884001560691e-06\n",
      "Iteration: 7014 Loss: 3.1765242490888933e-06\n",
      "Iteration: 7015 Loss: 3.173168046432374e-06\n",
      "Iteration: 7016 Loss: 3.1698153898434673e-06\n",
      "Iteration: 7017 Loss: 3.1664662755735805e-06\n",
      "Iteration: 7018 Loss: 3.1631206998797005e-06\n",
      "Iteration: 7019 Loss: 3.1597786590247384e-06\n",
      "Iteration: 7020 Loss: 3.1564401492740726e-06\n",
      "Iteration: 7021 Loss: 3.1531051668893916e-06\n",
      "Iteration: 7022 Loss: 3.1497737081547133e-06\n",
      "Iteration: 7023 Loss: 3.1464457693435366e-06\n",
      "Iteration: 7024 Loss: 3.143121346732245e-06\n",
      "Iteration: 7025 Loss: 3.1398004366114535e-06\n",
      "Iteration: 7026 Loss: 3.136483035264607e-06\n",
      "Iteration: 7027 Loss: 3.1331691389910133e-06\n",
      "Iteration: 7028 Loss: 3.1298587440794616e-06\n",
      "Iteration: 7029 Loss: 3.126551846837034e-06\n",
      "Iteration: 7030 Loss: 3.1232484435680627e-06\n",
      "Iteration: 7031 Loss: 3.119948530578779e-06\n",
      "Iteration: 7032 Loss: 3.1166521041768453e-06\n",
      "Iteration: 7033 Loss: 3.1133591606852537e-06\n",
      "Iteration: 7034 Loss: 3.110069696423555e-06\n",
      "Iteration: 7035 Loss: 3.106783707713124e-06\n",
      "Iteration: 7036 Loss: 3.1035011908821953e-06\n",
      "Iteration: 7037 Loss: 3.1002221422620445e-06\n",
      "Iteration: 7038 Loss: 3.096946558189041e-06\n",
      "Iteration: 7039 Loss: 3.093674435004556e-06\n",
      "Iteration: 7040 Loss: 3.0904057690493453e-06\n",
      "Iteration: 7041 Loss: 3.087140556671872e-06\n",
      "Iteration: 7042 Loss: 3.0838787942201004e-06\n",
      "Iteration: 7043 Loss: 3.080620478055484e-06\n",
      "Iteration: 7044 Loss: 3.077365604532452e-06\n",
      "Iteration: 7045 Loss: 3.0741141702125395e-06\n",
      "Iteration: 7046 Loss: 3.070866171063845e-06\n",
      "Iteration: 7047 Loss: 3.067621603656111e-06\n",
      "Iteration: 7048 Loss: 3.064380464362445e-06\n",
      "Iteration: 7049 Loss: 3.0611427495643685e-06\n",
      "Iteration: 7050 Loss: 3.057908455438767e-06\n",
      "Iteration: 7051 Loss: 3.0546775787747657e-06\n",
      "Iteration: 7052 Loss: 3.05145011576178e-06\n",
      "Iteration: 7053 Loss: 3.0482260627909646e-06\n",
      "Iteration: 7054 Loss: 3.04500541626226e-06\n",
      "Iteration: 7055 Loss: 3.0417881725755704e-06\n",
      "Iteration: 7056 Loss: 3.0385743281335077e-06\n",
      "Iteration: 7057 Loss: 3.0353638793449e-06\n",
      "Iteration: 7058 Loss: 3.03215682262461e-06\n",
      "Iteration: 7059 Loss: 3.028953154387737e-06\n",
      "Iteration: 7060 Loss: 3.0257528710515146e-06\n",
      "Iteration: 7061 Loss: 3.0225559690420297e-06\n",
      "Iteration: 7062 Loss: 3.0193624447862583e-06\n",
      "Iteration: 7063 Loss: 3.0161722947149412e-06\n",
      "Iteration: 7064 Loss: 3.0129855152642072e-06\n",
      "Iteration: 7065 Loss: 3.0098021028698895e-06\n",
      "Iteration: 7066 Loss: 3.0066220539777e-06\n",
      "Iteration: 7067 Loss: 3.0034453650326667e-06\n",
      "Iteration: 7068 Loss: 3.0002720324836716e-06\n",
      "Iteration: 7069 Loss: 2.997102052788089e-06\n",
      "Iteration: 7070 Loss: 2.9939354223985927e-06\n",
      "Iteration: 7071 Loss: 2.9907721377786395e-06\n",
      "Iteration: 7072 Loss: 2.9876121953929685e-06\n",
      "Iteration: 7073 Loss: 2.984455591710953e-06\n",
      "Iteration: 7074 Loss: 2.981302323202399e-06\n",
      "Iteration: 7075 Loss: 2.978152386349373e-06\n",
      "Iteration: 7076 Loss: 2.9750057776245123e-06\n",
      "Iteration: 7077 Loss: 2.9718624935149834e-06\n",
      "Iteration: 7078 Loss: 2.9687225305089387e-06\n",
      "Iteration: 7079 Loss: 2.9655858850956584e-06\n",
      "Iteration: 7080 Loss: 2.962452553771525e-06\n",
      "Iteration: 7081 Loss: 2.9593225330340957e-06\n",
      "Iteration: 7082 Loss: 2.956195819384733e-06\n",
      "Iteration: 7083 Loss: 2.9530724093297007e-06\n",
      "Iteration: 7084 Loss: 2.9499522993793975e-06\n",
      "Iteration: 7085 Loss: 2.9468354860460334e-06\n",
      "Iteration: 7086 Loss: 2.94372196584619e-06\n",
      "Iteration: 7087 Loss: 2.9406117353019975e-06\n",
      "Iteration: 7088 Loss: 2.9375047909363166e-06\n",
      "Iteration: 7089 Loss: 2.93440112927864e-06\n",
      "Iteration: 7090 Loss: 2.9313007469728827e-06\n",
      "Iteration: 7091 Loss: 2.9282036402116175e-06\n",
      "Iteration: 7092 Loss: 2.925109805878531e-06\n",
      "Iteration: 7093 Loss: 2.922019240399e-06\n",
      "Iteration: 7094 Loss: 2.918931940322802e-06\n",
      "Iteration: 7095 Loss: 2.9158479021985755e-06\n",
      "Iteration: 7096 Loss: 2.912767122576818e-06\n",
      "Iteration: 7097 Loss: 2.909689598019335e-06\n",
      "Iteration: 7098 Loss: 2.906615325083e-06\n",
      "Iteration: 7099 Loss: 2.9035443003340584e-06\n",
      "Iteration: 7100 Loss: 2.9004765203409566e-06\n",
      "Iteration: 7101 Loss: 2.8974119816737306e-06\n",
      "Iteration: 7102 Loss: 2.8943506809094563e-06\n",
      "Iteration: 7103 Loss: 2.891292614741415e-06\n",
      "Iteration: 7104 Loss: 2.8882377795234868e-06\n",
      "Iteration: 7105 Loss: 2.8851861719540664e-06\n",
      "Iteration: 7106 Loss: 2.882137788625358e-06\n",
      "Iteration: 7107 Loss: 2.879092626127724e-06\n",
      "Iteration: 7108 Loss: 2.876050680860481e-06\n",
      "Iteration: 7109 Loss: 2.8730119500254125e-06\n",
      "Iteration: 7110 Loss: 2.869976429623439e-06\n",
      "Iteration: 7111 Loss: 2.866944116464919e-06\n",
      "Iteration: 7112 Loss: 2.86391500716e-06\n",
      "Iteration: 7113 Loss: 2.860889098323602e-06\n",
      "Iteration: 7114 Loss: 2.85786638657386e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7115 Loss: 2.854846868418801e-06\n",
      "Iteration: 7116 Loss: 2.851830540714093e-06\n",
      "Iteration: 7117 Loss: 2.8488173999724373e-06\n",
      "Iteration: 7118 Loss: 2.845807442827563e-06\n",
      "Iteration: 7119 Loss: 2.8428006657157093e-06\n",
      "Iteration: 7120 Loss: 2.8397970656778315e-06\n",
      "Iteration: 7121 Loss: 2.8367966391536186e-06\n",
      "Iteration: 7122 Loss: 2.8337993831979434e-06\n",
      "Iteration: 7123 Loss: 2.8308052936519707e-06\n",
      "Iteration: 7124 Loss: 2.8278143676889118e-06\n",
      "Iteration: 7125 Loss: 2.824826601738296e-06\n",
      "Iteration: 7126 Loss: 2.8218419925732402e-06\n",
      "Iteration: 7127 Loss: 2.818860536858239e-06\n",
      "Iteration: 7128 Loss: 2.815882231147151e-06\n",
      "Iteration: 7129 Loss: 2.812907072342318e-06\n",
      "Iteration: 7130 Loss: 2.8099350570039044e-06\n",
      "Iteration: 7131 Loss: 2.8069661816091997e-06\n",
      "Iteration: 7132 Loss: 2.8040004432431728e-06\n",
      "Iteration: 7133 Loss: 2.8010378383899013e-06\n",
      "Iteration: 7134 Loss: 2.7980783638521887e-06\n",
      "Iteration: 7135 Loss: 2.7951220159828535e-06\n",
      "Iteration: 7136 Loss: 2.7921687918135177e-06\n",
      "Iteration: 7137 Loss: 2.789218687940581e-06\n",
      "Iteration: 7138 Loss: 2.786271701059597e-06\n",
      "Iteration: 7139 Loss: 2.783327827876904e-06\n",
      "Iteration: 7140 Loss: 2.7803870651068516e-06\n",
      "Iteration: 7141 Loss: 2.7774494094624853e-06\n",
      "Iteration: 7142 Loss: 2.774514857653765e-06\n",
      "Iteration: 7143 Loss: 2.771583406412753e-06\n",
      "Iteration: 7144 Loss: 2.7686550525711262e-06\n",
      "Iteration: 7145 Loss: 2.76572979262722e-06\n",
      "Iteration: 7146 Loss: 2.762807623427857e-06\n",
      "Iteration: 7147 Loss: 2.759888541708102e-06\n",
      "Iteration: 7148 Loss: 2.7569725440903818e-06\n",
      "Iteration: 7149 Loss: 2.75405962754473e-06\n",
      "Iteration: 7150 Loss: 2.751149788703652e-06\n",
      "Iteration: 7151 Loss: 2.7482430243114345e-06\n",
      "Iteration: 7152 Loss: 2.745339331121307e-06\n",
      "Iteration: 7153 Loss: 2.7424387058908863e-06\n",
      "Iteration: 7154 Loss: 2.7395411453756755e-06\n",
      "Iteration: 7155 Loss: 2.7366466463380866e-06\n",
      "Iteration: 7156 Loss: 2.733755205545175e-06\n",
      "Iteration: 7157 Loss: 2.730866819763452e-06\n",
      "Iteration: 7158 Loss: 2.727981485966758e-06\n",
      "Iteration: 7159 Loss: 2.7250992005297006e-06\n",
      "Iteration: 7160 Loss: 2.72221996063268e-06\n",
      "Iteration: 7161 Loss: 2.7193437626560296e-06\n",
      "Iteration: 7162 Loss: 2.7164706035865978e-06\n",
      "Iteration: 7163 Loss: 2.7136004803262684e-06\n",
      "Iteration: 7164 Loss: 2.7107333894406414e-06\n",
      "Iteration: 7165 Loss: 2.707869327725133e-06\n",
      "Iteration: 7166 Loss: 2.705008292208009e-06\n",
      "Iteration: 7167 Loss: 2.702150279376356e-06\n",
      "Iteration: 7168 Loss: 2.6992952862377107e-06\n",
      "Iteration: 7169 Loss: 2.696443309802005e-06\n",
      "Iteration: 7170 Loss: 2.69359434668387e-06\n",
      "Iteration: 7171 Loss: 2.690748393694137e-06\n",
      "Iteration: 7172 Loss: 2.6879054476577784e-06\n",
      "Iteration: 7173 Loss: 2.685065505510068e-06\n",
      "Iteration: 7174 Loss: 2.6822285640496956e-06\n",
      "Iteration: 7175 Loss: 2.6793946198155604e-06\n",
      "Iteration: 7176 Loss: 2.676563669846071e-06\n",
      "Iteration: 7177 Loss: 2.673735710659155e-06\n",
      "Iteration: 7178 Loss: 2.670910739727168e-06\n",
      "Iteration: 7179 Loss: 2.6680887535773814e-06\n",
      "Iteration: 7180 Loss: 2.6652697488549833e-06\n",
      "Iteration: 7181 Loss: 2.6624537228118726e-06\n",
      "Iteration: 7182 Loss: 2.6596406722995516e-06\n",
      "Iteration: 7183 Loss: 2.6568305937750902e-06\n",
      "Iteration: 7184 Loss: 2.654023484297116e-06\n",
      "Iteration: 7185 Loss: 2.6512193407293318e-06\n",
      "Iteration: 7186 Loss: 2.648418159937226e-06\n",
      "Iteration: 7187 Loss: 2.6456199387930725e-06\n",
      "Iteration: 7188 Loss: 2.6428246741662652e-06\n",
      "Iteration: 7189 Loss: 2.640032362934259e-06\n",
      "Iteration: 7190 Loss: 2.6372430019786665e-06\n",
      "Iteration: 7191 Loss: 2.634456588180766e-06\n",
      "Iteration: 7192 Loss: 2.6316731184244277e-06\n",
      "Iteration: 7193 Loss: 2.6288925896031684e-06\n",
      "Iteration: 7194 Loss: 2.626114998605574e-06\n",
      "Iteration: 7195 Loss: 2.623340342333121e-06\n",
      "Iteration: 7196 Loss: 2.620568617678085e-06\n",
      "Iteration: 7197 Loss: 2.6177998216641376e-06\n",
      "Iteration: 7198 Loss: 2.6150339509634166e-06\n",
      "Iteration: 7199 Loss: 2.6122710024859902e-06\n",
      "Iteration: 7200 Loss: 2.609510973373813e-06\n",
      "Iteration: 7201 Loss: 2.6067538604296825e-06\n",
      "Iteration: 7202 Loss: 2.6039996605688576e-06\n",
      "Iteration: 7203 Loss: 2.6012483707169686e-06\n",
      "Iteration: 7204 Loss: 2.5984999877974026e-06\n",
      "Iteration: 7205 Loss: 2.595754508738667e-06\n",
      "Iteration: 7206 Loss: 2.5930119304734955e-06\n",
      "Iteration: 7207 Loss: 2.5902722499358685e-06\n",
      "Iteration: 7208 Loss: 2.587535464064958e-06\n",
      "Iteration: 7209 Loss: 2.584801569803576e-06\n",
      "Iteration: 7210 Loss: 2.5820705640944228e-06\n",
      "Iteration: 7211 Loss: 2.5793424438856737e-06\n",
      "Iteration: 7212 Loss: 2.576617206132866e-06\n",
      "Iteration: 7213 Loss: 2.5738948477810504e-06\n",
      "Iteration: 7214 Loss: 2.5711753658000687e-06\n",
      "Iteration: 7215 Loss: 2.5684587571413044e-06\n",
      "Iteration: 7216 Loss: 2.5657450187688157e-06\n",
      "Iteration: 7217 Loss: 2.5630341476565443e-06\n",
      "Iteration: 7218 Loss: 2.5603261407709785e-06\n",
      "Iteration: 7219 Loss: 2.5576209950887444e-06\n",
      "Iteration: 7220 Loss: 2.5549187075824293e-06\n",
      "Iteration: 7221 Loss: 2.5522192752349606e-06\n",
      "Iteration: 7222 Loss: 2.549522695029041e-06\n",
      "Iteration: 7223 Loss: 2.546828963953629e-06\n",
      "Iteration: 7224 Loss: 2.5441380789918135e-06\n",
      "Iteration: 7225 Loss: 2.5414500371420797e-06\n",
      "Iteration: 7226 Loss: 2.5387648353994474e-06\n",
      "Iteration: 7227 Loss: 2.536082470762352e-06\n",
      "Iteration: 7228 Loss: 2.533402940235021e-06\n",
      "Iteration: 7229 Loss: 2.5307262408188175e-06\n",
      "Iteration: 7230 Loss: 2.528052369528624e-06\n",
      "Iteration: 7231 Loss: 2.525381323370215e-06\n",
      "Iteration: 7232 Loss: 2.5227130993622025e-06\n",
      "Iteration: 7233 Loss: 2.520047694524066e-06\n",
      "Iteration: 7234 Loss: 2.5173851058724387e-06\n",
      "Iteration: 7235 Loss: 2.5147253304351734e-06\n",
      "Iteration: 7236 Loss: 2.5120683652397433e-06\n",
      "Iteration: 7237 Loss: 2.5094142073163075e-06\n",
      "Iteration: 7238 Loss: 2.5067628536979625e-06\n",
      "Iteration: 7239 Loss: 2.5041143016249612e-06\n",
      "Iteration: 7240 Loss: 2.5014685477337627e-06\n",
      "Iteration: 7241 Loss: 2.498825589469536e-06\n",
      "Iteration: 7242 Loss: 2.4961854232764187e-06\n",
      "Iteration: 7243 Loss: 2.493548046806788e-06\n",
      "Iteration: 7244 Loss: 2.4909134569119884e-06\n",
      "Iteration: 7245 Loss: 2.4882816506491174e-06\n",
      "Iteration: 7246 Loss: 2.4856526250739364e-06\n",
      "Iteration: 7247 Loss: 2.4830263772516764e-06\n",
      "Iteration: 7248 Loss: 2.480402904246958e-06\n",
      "Iteration: 7249 Loss: 2.4777822031270182e-06\n",
      "Iteration: 7250 Loss: 2.475164270965412e-06\n",
      "Iteration: 7251 Loss: 2.4725491048309362e-06\n",
      "Iteration: 7252 Loss: 2.4699367016053515e-06\n",
      "Iteration: 7253 Loss: 2.467327058769844e-06\n",
      "Iteration: 7254 Loss: 2.4647201732074287e-06\n",
      "Iteration: 7255 Loss: 2.462116042003329e-06\n",
      "Iteration: 7256 Loss: 2.45951466225e-06\n",
      "Iteration: 7257 Loss: 2.456916031037862e-06\n",
      "Iteration: 7258 Loss: 2.454320145465272e-06\n",
      "Iteration: 7259 Loss: 2.4517270026266588e-06\n",
      "Iteration: 7260 Loss: 2.4491365996284644e-06\n",
      "Iteration: 7261 Loss: 2.446548933575788e-06\n",
      "Iteration: 7262 Loss: 2.4439640015746856e-06\n",
      "Iteration: 7263 Loss: 2.4413818007374003e-06\n",
      "Iteration: 7264 Loss: 2.4388023281792913e-06\n",
      "Iteration: 7265 Loss: 2.43622558121602e-06\n",
      "Iteration: 7266 Loss: 2.4336515566843807e-06\n",
      "Iteration: 7267 Loss: 2.431080251676871e-06\n",
      "Iteration: 7268 Loss: 2.428511663232655e-06\n",
      "Iteration: 7269 Loss: 2.425945788886864e-06\n",
      "Iteration: 7270 Loss: 2.4233826255685674e-06\n",
      "Iteration: 7271 Loss: 2.4208221702983575e-06\n",
      "Iteration: 7272 Loss: 2.4182644204464887e-06\n",
      "Iteration: 7273 Loss: 2.4157093730384888e-06\n",
      "Iteration: 7274 Loss: 2.413157025218437e-06\n",
      "Iteration: 7275 Loss: 2.410607374136256e-06\n",
      "Iteration: 7276 Loss: 2.4080604169398816e-06\n",
      "Iteration: 7277 Loss: 2.405516150785693e-06\n",
      "Iteration: 7278 Loss: 2.4029745728261826e-06\n",
      "Iteration: 7279 Loss: 2.400435680225322e-06\n",
      "Iteration: 7280 Loss: 2.397899470145002e-06\n",
      "Iteration: 7281 Loss: 2.395365939750128e-06\n",
      "Iteration: 7282 Loss: 2.3928350862099856e-06\n",
      "Iteration: 7283 Loss: 2.390306906695243e-06\n",
      "Iteration: 7284 Loss: 2.387781398381219e-06\n",
      "Iteration: 7285 Loss: 2.385258558446785e-06\n",
      "Iteration: 7286 Loss: 2.382738384071517e-06\n",
      "Iteration: 7287 Loss: 2.3802208724375882e-06\n",
      "Iteration: 7288 Loss: 2.3777060207322485e-06\n",
      "Iteration: 7289 Loss: 2.37519382615015e-06\n",
      "Iteration: 7290 Loss: 2.372684285875912e-06\n",
      "Iteration: 7291 Loss: 2.37017739710963e-06\n",
      "Iteration: 7292 Loss: 2.367673157050612e-06\n",
      "Iteration: 7293 Loss: 2.3651715628979348e-06\n",
      "Iteration: 7294 Loss: 2.362672611855985e-06\n",
      "Iteration: 7295 Loss: 2.3601763011339267e-06\n",
      "Iteration: 7296 Loss: 2.3576826279393838e-06\n",
      "Iteration: 7297 Loss: 2.355191589489751e-06\n",
      "Iteration: 7298 Loss: 2.352703182996866e-06\n",
      "Iteration: 7299 Loss: 2.350217405681503e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7300 Loss: 2.3477342547689885e-06\n",
      "Iteration: 7301 Loss: 2.3452537274781456e-06\n",
      "Iteration: 7302 Loss: 2.342775821041572e-06\n",
      "Iteration: 7303 Loss: 2.3403005326886205e-06\n",
      "Iteration: 7304 Loss: 2.3378278596523503e-06\n",
      "Iteration: 7305 Loss: 2.335357799168869e-06\n",
      "Iteration: 7306 Loss: 2.332890348484123e-06\n",
      "Iteration: 7307 Loss: 2.3304255048350643e-06\n",
      "Iteration: 7308 Loss: 2.3279632654637535e-06\n",
      "Iteration: 7309 Loss: 2.3255036274252077e-06\n",
      "Iteration: 7310 Loss: 2.323046588368403e-06\n",
      "Iteration: 7311 Loss: 2.32059214554893e-06\n",
      "Iteration: 7312 Loss: 2.3181402958197344e-06\n",
      "Iteration: 7313 Loss: 2.315691036645405e-06\n",
      "Iteration: 7314 Loss: 2.313244365489224e-06\n",
      "Iteration: 7315 Loss: 2.3108002792115183e-06\n",
      "Iteration: 7316 Loss: 2.3083587754863577e-06\n",
      "Iteration: 7317 Loss: 2.3059198511810494e-06\n",
      "Iteration: 7318 Loss: 2.3034835037738544e-06\n",
      "Iteration: 7319 Loss: 2.3010497305395196e-06\n",
      "Iteration: 7320 Loss: 2.2986185287574937e-06\n",
      "Iteration: 7321 Loss: 2.2961898955133613e-06\n",
      "Iteration: 7322 Loss: 2.2937638284916535e-06\n",
      "Iteration: 7323 Loss: 2.2913403245815395e-06\n",
      "Iteration: 7324 Loss: 2.288919381474793e-06\n",
      "Iteration: 7325 Loss: 2.2865009962671477e-06\n",
      "Iteration: 7326 Loss: 2.284085166255717e-06\n",
      "Iteration: 7327 Loss: 2.2816718887390585e-06\n",
      "Iteration: 7328 Loss: 2.27926116101996e-06\n",
      "Iteration: 7329 Loss: 2.276852980407516e-06\n",
      "Iteration: 7330 Loss: 2.2744473440057984e-06\n",
      "Iteration: 7331 Loss: 2.272044249530982e-06\n",
      "Iteration: 7332 Loss: 2.269643694095471e-06\n",
      "Iteration: 7333 Loss: 2.267245675017488e-06\n",
      "Iteration: 7334 Loss: 2.264850189615767e-06\n",
      "Iteration: 7335 Loss: 2.262457235214366e-06\n",
      "Iteration: 7336 Loss: 2.2600668091396863e-06\n",
      "Iteration: 7337 Loss: 2.2576789087192785e-06\n",
      "Iteration: 7338 Loss: 2.2552935312839273e-06\n",
      "Iteration: 7339 Loss: 2.2529106739688517e-06\n",
      "Iteration: 7340 Loss: 2.2505303345116333e-06\n",
      "Iteration: 7341 Loss: 2.248152510053114e-06\n",
      "Iteration: 7342 Loss: 2.245777198135094e-06\n",
      "Iteration: 7343 Loss: 2.2434043957004327e-06\n",
      "Iteration: 7344 Loss: 2.241034100301798e-06\n",
      "Iteration: 7345 Loss: 2.238666309286596e-06\n",
      "Iteration: 7346 Loss: 2.2363010202122224e-06\n",
      "Iteration: 7347 Loss: 2.2339382300320728e-06\n",
      "Iteration: 7348 Loss: 2.2315779363088138e-06\n",
      "Iteration: 7349 Loss: 2.2292201364004644e-06\n",
      "Iteration: 7350 Loss: 2.226864827474676e-06\n",
      "Iteration: 7351 Loss: 2.224512007299881e-06\n",
      "Iteration: 7352 Loss: 2.222161673048736e-06\n",
      "Iteration: 7353 Loss: 2.2198138220913257e-06\n",
      "Iteration: 7354 Loss: 2.2174684518036043e-06\n",
      "Iteration: 7355 Loss: 2.21512555956686e-06\n",
      "Iteration: 7356 Loss: 2.2127851427612674e-06\n",
      "Iteration: 7357 Loss: 2.210447198772534e-06\n",
      "Iteration: 7358 Loss: 2.208111724987186e-06\n",
      "Iteration: 7359 Loss: 2.205778718794231e-06\n",
      "Iteration: 7360 Loss: 2.2034481775891306e-06\n",
      "Iteration: 7361 Loss: 2.201120098764474e-06\n",
      "Iteration: 7362 Loss: 2.1987944797220926e-06\n",
      "Iteration: 7363 Loss: 2.1964713178591668e-06\n",
      "Iteration: 7364 Loss: 2.1941506105816735e-06\n",
      "Iteration: 7365 Loss: 2.1918323552970736e-06\n",
      "Iteration: 7366 Loss: 2.1895165494112063e-06\n",
      "Iteration: 7367 Loss: 2.187203190544066e-06\n",
      "Iteration: 7368 Loss: 2.1848922756994545e-06\n",
      "Iteration: 7369 Loss: 2.182583802499653e-06\n",
      "Iteration: 7370 Loss: 2.1802777683672107e-06\n",
      "Iteration: 7371 Loss: 2.1779741707233495e-06\n",
      "Iteration: 7372 Loss: 2.17567300699545e-06\n",
      "Iteration: 7373 Loss: 2.1733742746066834e-06\n",
      "Iteration: 7374 Loss: 2.1710779709926733e-06\n",
      "Iteration: 7375 Loss: 2.1687840935871993e-06\n",
      "Iteration: 7376 Loss: 2.1664926398254345e-06\n",
      "Iteration: 7377 Loss: 2.1642036071456177e-06\n",
      "Iteration: 7378 Loss: 2.1619169929906565e-06\n",
      "Iteration: 7379 Loss: 2.1596327948095117e-06\n",
      "Iteration: 7380 Loss: 2.1573510100423286e-06\n",
      "Iteration: 7381 Loss: 2.155071636144294e-06\n",
      "Iteration: 7382 Loss: 2.1527946705649895e-06\n",
      "Iteration: 7383 Loss: 2.1505201107613415e-06\n",
      "Iteration: 7384 Loss: 2.148247954190437e-06\n",
      "Iteration: 7385 Loss: 2.145978198314257e-06\n",
      "Iteration: 7386 Loss: 2.1437108405984252e-06\n",
      "Iteration: 7387 Loss: 2.141445878505506e-06\n",
      "Iteration: 7388 Loss: 2.139183309504961e-06\n",
      "Iteration: 7389 Loss: 2.1369231310691077e-06\n",
      "Iteration: 7390 Loss: 2.1346653406714893e-06\n",
      "Iteration: 7391 Loss: 2.1324099357902392e-06\n",
      "Iteration: 7392 Loss: 2.130156913904458e-06\n",
      "Iteration: 7393 Loss: 2.127906272494685e-06\n",
      "Iteration: 7394 Loss: 2.1256580090467014e-06\n",
      "Iteration: 7395 Loss: 2.1234121210503583e-06\n",
      "Iteration: 7396 Loss: 2.1211686059926977e-06\n",
      "Iteration: 7397 Loss: 2.1189274613679017e-06\n",
      "Iteration: 7398 Loss: 2.116688684670955e-06\n",
      "Iteration: 7399 Loss: 2.114452273399286e-06\n",
      "Iteration: 7400 Loss: 2.112218225057073e-06\n",
      "Iteration: 7401 Loss: 2.1099865371437714e-06\n",
      "Iteration: 7402 Loss: 2.107757207167279e-06\n",
      "Iteration: 7403 Loss: 2.1055302326344908e-06\n",
      "Iteration: 7404 Loss: 2.1033056110591203e-06\n",
      "Iteration: 7405 Loss: 2.101083339955361e-06\n",
      "Iteration: 7406 Loss: 2.0988634168404856e-06\n",
      "Iteration: 7407 Loss: 2.0966458392251967e-06\n",
      "Iteration: 7408 Loss: 2.0944306046403885e-06\n",
      "Iteration: 7409 Loss: 2.0922177106104403e-06\n",
      "Iteration: 7410 Loss: 2.0900071546605517e-06\n",
      "Iteration: 7411 Loss: 2.0877989343147887e-06\n",
      "Iteration: 7412 Loss: 2.0855930471127065e-06\n",
      "Iteration: 7413 Loss: 2.0833894905861655e-06\n",
      "Iteration: 7414 Loss: 2.081188262274955e-06\n",
      "Iteration: 7415 Loss: 2.078989359717196e-06\n",
      "Iteration: 7416 Loss: 2.076792780454197e-06\n",
      "Iteration: 7417 Loss: 2.0745985220342717e-06\n",
      "Iteration: 7418 Loss: 2.072406582003425e-06\n",
      "Iteration: 7419 Loss: 2.0702169579131067e-06\n",
      "Iteration: 7420 Loss: 2.0680296473155206e-06\n",
      "Iteration: 7421 Loss: 2.0658446477657407e-06\n",
      "Iteration: 7422 Loss: 2.0636619568230965e-06\n",
      "Iteration: 7423 Loss: 2.0614815720475937e-06\n",
      "Iteration: 7424 Loss: 2.0593034910045223e-06\n",
      "Iteration: 7425 Loss: 2.0571277114614077e-06\n",
      "Iteration: 7426 Loss: 2.054954230579404e-06\n",
      "Iteration: 7427 Loss: 2.0527830459335407e-06\n",
      "Iteration: 7428 Loss: 2.0506141554994184e-06\n",
      "Iteration: 7429 Loss: 2.0484475566517426e-06\n",
      "Iteration: 7430 Loss: 2.0462832469685638e-06\n",
      "Iteration: 7431 Loss: 2.04412122403495e-06\n",
      "Iteration: 7432 Loss: 2.0419614855427584e-06\n",
      "Iteration: 7433 Loss: 2.039804028855188e-06\n",
      "Iteration: 7434 Loss: 2.037648851672188e-06\n",
      "Iteration: 7435 Loss: 2.035495951471519e-06\n",
      "Iteration: 7436 Loss: 2.033345326077637e-06\n",
      "Iteration: 7437 Loss: 2.0311969729715156e-06\n",
      "Iteration: 7438 Loss: 2.0290508897523802e-06\n",
      "Iteration: 7439 Loss: 2.02690707402385e-06\n",
      "Iteration: 7440 Loss: 2.0247655233871094e-06\n",
      "Iteration: 7441 Loss: 2.0226262354527877e-06\n",
      "Iteration: 7442 Loss: 2.02048920782627e-06\n",
      "Iteration: 7443 Loss: 2.018354438119891e-06\n",
      "Iteration: 7444 Loss: 2.0162219239492883e-06\n",
      "Iteration: 7445 Loss: 2.014091662931353e-06\n",
      "Iteration: 7446 Loss: 2.0119636526866407e-06\n",
      "Iteration: 7447 Loss: 2.0098378908332494e-06\n",
      "Iteration: 7448 Loss: 2.0077143749996354e-06\n",
      "Iteration: 7449 Loss: 2.0055931028100905e-06\n",
      "Iteration: 7450 Loss: 2.003474071896083e-06\n",
      "Iteration: 7451 Loss: 2.0013572798868322e-06\n",
      "Iteration: 7452 Loss: 1.9992427244185442e-06\n",
      "Iteration: 7453 Loss: 1.9971304031293023e-06\n",
      "Iteration: 7454 Loss: 1.995020313656032e-06\n",
      "Iteration: 7455 Loss: 1.992912453641158e-06\n",
      "Iteration: 7456 Loss: 1.9908068207310612e-06\n",
      "Iteration: 7457 Loss: 1.988703412571553e-06\n",
      "Iteration: 7458 Loss: 1.986602226810556e-06\n",
      "Iteration: 7459 Loss: 1.9845032611034974e-06\n",
      "Iteration: 7460 Loss: 1.982406513099725e-06\n",
      "Iteration: 7461 Loss: 1.980311980459869e-06\n",
      "Iteration: 7462 Loss: 1.9782196609590028e-06\n",
      "Iteration: 7463 Loss: 1.976129552025781e-06\n",
      "Iteration: 7464 Loss: 1.97404165143976e-06\n",
      "Iteration: 7465 Loss: 1.971955956869294e-06\n",
      "Iteration: 7466 Loss: 1.9698724658686182e-06\n",
      "Iteration: 7467 Loss: 1.9677911764520486e-06\n",
      "Iteration: 7468 Loss: 1.9657120859521304e-06\n",
      "Iteration: 7469 Loss: 1.963635192157911e-06\n",
      "Iteration: 7470 Loss: 1.961560492749946e-06\n",
      "Iteration: 7471 Loss: 1.95948798540847e-06\n",
      "Iteration: 7472 Loss: 1.957417667818145e-06\n",
      "Iteration: 7473 Loss: 1.955349537663039e-06\n",
      "Iteration: 7474 Loss: 1.953283592635922e-06\n",
      "Iteration: 7475 Loss: 1.951219830425374e-06\n",
      "Iteration: 7476 Loss: 1.949158248725825e-06\n",
      "Iteration: 7477 Loss: 1.947098845234479e-06\n",
      "Iteration: 7478 Loss: 1.9450416176491475e-06\n",
      "Iteration: 7479 Loss: 1.9429865636684517e-06\n",
      "Iteration: 7480 Loss: 1.9409336810010414e-06\n",
      "Iteration: 7481 Loss: 1.93888296734597e-06\n",
      "Iteration: 7482 Loss: 1.9368344204175645e-06\n",
      "Iteration: 7483 Loss: 1.934788037924871e-06\n",
      "Iteration: 7484 Loss: 1.932743817578397e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7485 Loss: 1.930701757096973e-06\n",
      "Iteration: 7486 Loss: 1.928661854196585e-06\n",
      "Iteration: 7487 Loss: 1.9266241065969154e-06\n",
      "Iteration: 7488 Loss: 1.9245885120237457e-06\n",
      "Iteration: 7489 Loss: 1.922555068085285e-06\n",
      "Iteration: 7490 Loss: 1.920523772739916e-06\n",
      "Iteration: 7491 Loss: 1.918494623599458e-06\n",
      "Iteration: 7492 Loss: 1.916467618401137e-06\n",
      "Iteration: 7493 Loss: 1.9144427548782378e-06\n",
      "Iteration: 7494 Loss: 1.9124200307653194e-06\n",
      "Iteration: 7495 Loss: 1.9103994438028265e-06\n",
      "Iteration: 7496 Loss: 1.908380991736046e-06\n",
      "Iteration: 7497 Loss: 1.9063646723056487e-06\n",
      "Iteration: 7498 Loss: 1.9043504832601962e-06\n",
      "Iteration: 7499 Loss: 1.9023384223483413e-06\n",
      "Iteration: 7500 Loss: 1.900328487320445e-06\n",
      "Iteration: 7501 Loss: 1.89832067593248e-06\n",
      "Iteration: 7502 Loss: 1.8963149859393437e-06\n",
      "Iteration: 7503 Loss: 1.8943114150988657e-06\n",
      "Iteration: 7504 Loss: 1.8923099611717998e-06\n",
      "Iteration: 7505 Loss: 1.8903106219253405e-06\n",
      "Iteration: 7506 Loss: 1.8883133951229879e-06\n",
      "Iteration: 7507 Loss: 1.8863182785303274e-06\n",
      "Iteration: 7508 Loss: 1.884325270034858e-06\n",
      "Iteration: 7509 Loss: 1.8823343671797872e-06\n",
      "Iteration: 7510 Loss: 1.8803455678541457e-06\n",
      "Iteration: 7511 Loss: 1.8783588698365775e-06\n",
      "Iteration: 7512 Loss: 1.8763742709064843e-06\n",
      "Iteration: 7513 Loss: 1.874391768731043e-06\n",
      "Iteration: 7514 Loss: 1.872411361323386e-06\n",
      "Iteration: 7515 Loss: 1.8704330463564106e-06\n",
      "Iteration: 7516 Loss: 1.8684568216203586e-06\n",
      "Iteration: 7517 Loss: 1.86648268490545e-06\n",
      "Iteration: 7518 Loss: 1.8645106340060249e-06\n",
      "Iteration: 7519 Loss: 1.8625406668319387e-06\n",
      "Iteration: 7520 Loss: 1.860572780954723e-06\n",
      "Iteration: 7521 Loss: 1.8586069744880978e-06\n",
      "Iteration: 7522 Loss: 1.8566432448359833e-06\n",
      "Iteration: 7523 Loss: 1.8546815900021592e-06\n",
      "Iteration: 7524 Loss: 1.8527220075943314e-06\n",
      "Iteration: 7525 Loss: 1.850764495711424e-06\n",
      "Iteration: 7526 Loss: 1.8488090521921331e-06\n",
      "Iteration: 7527 Loss: 1.8468556747385363e-06\n",
      "Iteration: 7528 Loss: 1.8449043611655737e-06\n",
      "Iteration: 7529 Loss: 1.8429551092946282e-06\n",
      "Iteration: 7530 Loss: 1.841007916946509e-06\n",
      "Iteration: 7531 Loss: 1.8390627819440859e-06\n",
      "Iteration: 7532 Loss: 1.837119702117339e-06\n",
      "Iteration: 7533 Loss: 1.8351786752879938e-06\n",
      "Iteration: 7534 Loss: 1.8332396992924208e-06\n",
      "Iteration: 7535 Loss: 1.8313027719641282e-06\n",
      "Iteration: 7536 Loss: 1.8293678911355724e-06\n",
      "Iteration: 7537 Loss: 1.827435054646642e-06\n",
      "Iteration: 7538 Loss: 1.8255042603352678e-06\n",
      "Iteration: 7539 Loss: 1.8235755060446677e-06\n",
      "Iteration: 7540 Loss: 1.8216487896215912e-06\n",
      "Iteration: 7541 Loss: 1.8197241089091136e-06\n",
      "Iteration: 7542 Loss: 1.8178014618744864e-06\n",
      "Iteration: 7543 Loss: 1.8158808461369024e-06\n",
      "Iteration: 7544 Loss: 1.813962259665865e-06\n",
      "Iteration: 7545 Loss: 1.812045700317374e-06\n",
      "Iteration: 7546 Loss: 1.8101311659517164e-06\n",
      "Iteration: 7547 Loss: 1.8082186544239347e-06\n",
      "Iteration: 7548 Loss: 1.8063081636020418e-06\n",
      "Iteration: 7549 Loss: 1.804399691350256e-06\n",
      "Iteration: 7550 Loss: 1.8024932355325589e-06\n",
      "Iteration: 7551 Loss: 1.800588794021465e-06\n",
      "Iteration: 7552 Loss: 1.7986863646879882e-06\n",
      "Iteration: 7553 Loss: 1.796785945404811e-06\n",
      "Iteration: 7554 Loss: 1.794887534051018e-06\n",
      "Iteration: 7555 Loss: 1.7929911285019592e-06\n",
      "Iteration: 7556 Loss: 1.7910967266403229e-06\n",
      "Iteration: 7557 Loss: 1.7892043262323043e-06\n",
      "Iteration: 7558 Loss: 1.787313925398197e-06\n",
      "Iteration: 7559 Loss: 1.7854255219030824e-06\n",
      "Iteration: 7560 Loss: 1.7835391136404141e-06\n",
      "Iteration: 7561 Loss: 1.7816546985012146e-06\n",
      "Iteration: 7562 Loss: 1.7797722743816848e-06\n",
      "Iteration: 7563 Loss: 1.7778918391752385e-06\n",
      "Iteration: 7564 Loss: 1.7760133907841254e-06\n",
      "Iteration: 7565 Loss: 1.7741369271052298e-06\n",
      "Iteration: 7566 Loss: 1.772262446044206e-06\n",
      "Iteration: 7567 Loss: 1.770389945502431e-06\n",
      "Iteration: 7568 Loss: 1.7685194233924328e-06\n",
      "Iteration: 7569 Loss: 1.7666508776217802e-06\n",
      "Iteration: 7570 Loss: 1.7647843061011659e-06\n",
      "Iteration: 7571 Loss: 1.7629197067462214e-06\n",
      "Iteration: 7572 Loss: 1.7610570774724993e-06\n",
      "Iteration: 7573 Loss: 1.7591964161996354e-06\n",
      "Iteration: 7574 Loss: 1.757337720845671e-06\n",
      "Iteration: 7575 Loss: 1.7554809893363105e-06\n",
      "Iteration: 7576 Loss: 1.753626219594982e-06\n",
      "Iteration: 7577 Loss: 1.7517734095498196e-06\n",
      "Iteration: 7578 Loss: 1.7499225571285141e-06\n",
      "Iteration: 7579 Loss: 1.7480736602635808e-06\n",
      "Iteration: 7580 Loss: 1.7462267168913595e-06\n",
      "Iteration: 7581 Loss: 1.7443817249447225e-06\n",
      "Iteration: 7582 Loss: 1.7425386823611812e-06\n",
      "Iteration: 7583 Loss: 1.7406975870855763e-06\n",
      "Iteration: 7584 Loss: 1.7388584370565205e-06\n",
      "Iteration: 7585 Loss: 1.737021230219011e-06\n",
      "Iteration: 7586 Loss: 1.7351859645207226e-06\n",
      "Iteration: 7587 Loss: 1.7333526379112321e-06\n",
      "Iteration: 7588 Loss: 1.7315212483416828e-06\n",
      "Iteration: 7589 Loss: 1.7296917937638834e-06\n",
      "Iteration: 7590 Loss: 1.7278642721350666e-06\n",
      "Iteration: 7591 Loss: 1.7260386814135445e-06\n",
      "Iteration: 7592 Loss: 1.7242150195558979e-06\n",
      "Iteration: 7593 Loss: 1.7223932845279016e-06\n",
      "Iteration: 7594 Loss: 1.7205734742914863e-06\n",
      "Iteration: 7595 Loss: 1.7187555868131165e-06\n",
      "Iteration: 7596 Loss: 1.716939620063067e-06\n",
      "Iteration: 7597 Loss: 1.7151255720075618e-06\n",
      "Iteration: 7598 Loss: 1.71331344062541e-06\n",
      "Iteration: 7599 Loss: 1.7115032238900152e-06\n",
      "Iteration: 7600 Loss: 1.70969491977254e-06\n",
      "Iteration: 7601 Loss: 1.707888526259944e-06\n",
      "Iteration: 7602 Loss: 1.706084041327264e-06\n",
      "Iteration: 7603 Loss: 1.7042814629621767e-06\n",
      "Iteration: 7604 Loss: 1.702480789148829e-06\n",
      "Iteration: 7605 Loss: 1.7006820178743925e-06\n",
      "Iteration: 7606 Loss: 1.6988851471295272e-06\n",
      "Iteration: 7607 Loss: 1.6970901749073516e-06\n",
      "Iteration: 7608 Loss: 1.6952970990837926e-06\n",
      "Iteration: 7609 Loss: 1.693505918002683e-06\n",
      "Iteration: 7610 Loss: 1.691716629315663e-06\n",
      "Iteration: 7611 Loss: 1.6899292311389623e-06\n",
      "Iteration: 7612 Loss: 1.6881437214773629e-06\n",
      "Iteration: 7613 Loss: 1.6863600983325553e-06\n",
      "Iteration: 7614 Loss: 1.6845783597090251e-06\n",
      "Iteration: 7615 Loss: 1.6827985036208842e-06\n",
      "Iteration: 7616 Loss: 1.6810205280763683e-06\n",
      "Iteration: 7617 Loss: 1.6792444310890544e-06\n",
      "Iteration: 7618 Loss: 1.6774702106751602e-06\n",
      "Iteration: 7619 Loss: 1.675697864850338e-06\n",
      "Iteration: 7620 Loss: 1.6739273916341616e-06\n",
      "Iteration: 7621 Loss: 1.6721587890475867e-06\n",
      "Iteration: 7622 Loss: 1.6703920551163722e-06\n",
      "Iteration: 7623 Loss: 1.6686271878639135e-06\n",
      "Iteration: 7624 Loss: 1.6668641853184912e-06\n",
      "Iteration: 7625 Loss: 1.6651030455113905e-06\n",
      "Iteration: 7626 Loss: 1.6633437664728601e-06\n",
      "Iteration: 7627 Loss: 1.66158634623747e-06\n",
      "Iteration: 7628 Loss: 1.6598307828416609e-06\n",
      "Iteration: 7629 Loss: 1.6580770743211353e-06\n",
      "Iteration: 7630 Loss: 1.6563252187183546e-06\n",
      "Iteration: 7631 Loss: 1.6545752140776588e-06\n",
      "Iteration: 7632 Loss: 1.6528270584382758e-06\n",
      "Iteration: 7633 Loss: 1.6510807498499293e-06\n",
      "Iteration: 7634 Loss: 1.649336286361458e-06\n",
      "Iteration: 7635 Loss: 1.6475936660204554e-06\n",
      "Iteration: 7636 Loss: 1.6458528868824349e-06\n",
      "Iteration: 7637 Loss: 1.6441139470002752e-06\n",
      "Iteration: 7638 Loss: 1.6423768444314557e-06\n",
      "Iteration: 7639 Loss: 1.640641577234977e-06\n",
      "Iteration: 7640 Loss: 1.6389081434715137e-06\n",
      "Iteration: 7641 Loss: 1.6371765412031692e-06\n",
      "Iteration: 7642 Loss: 1.6354467684953524e-06\n",
      "Iteration: 7643 Loss: 1.6337188234154818e-06\n",
      "Iteration: 7644 Loss: 1.6319927040313171e-06\n",
      "Iteration: 7645 Loss: 1.6302684084149632e-06\n",
      "Iteration: 7646 Loss: 1.6285459348418145e-06\n",
      "Iteration: 7647 Loss: 1.626825280777878e-06\n",
      "Iteration: 7648 Loss: 1.625106444910957e-06\n",
      "Iteration: 7649 Loss: 1.623389425114709e-06\n",
      "Iteration: 7650 Loss: 1.6216742194735465e-06\n",
      "Iteration: 7651 Loss: 1.6199608260650254e-06\n",
      "Iteration: 7652 Loss: 1.6182492429834588e-06\n",
      "Iteration: 7653 Loss: 1.6165394683051611e-06\n",
      "Iteration: 7654 Loss: 1.6148315001280766e-06\n",
      "Iteration: 7655 Loss: 1.6131253365415406e-06\n",
      "Iteration: 7656 Loss: 1.6114209756343602e-06\n",
      "Iteration: 7657 Loss: 1.6097184155077872e-06\n",
      "Iteration: 7658 Loss: 1.608017654255492e-06\n",
      "Iteration: 7659 Loss: 1.6063186899782188e-06\n",
      "Iteration: 7660 Loss: 1.604621520777091e-06\n",
      "Iteration: 7661 Loss: 1.602926144755724e-06\n",
      "Iteration: 7662 Loss: 1.6012325600190065e-06\n",
      "Iteration: 7663 Loss: 1.599540764790659e-06\n",
      "Iteration: 7664 Loss: 1.5978507569463038e-06\n",
      "Iteration: 7665 Loss: 1.59616253460252e-06\n",
      "Iteration: 7666 Loss: 1.5944760960999031e-06\n",
      "Iteration: 7667 Loss: 1.5927914394379952e-06\n",
      "Iteration: 7668 Loss: 1.5911085627361926e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7669 Loss: 1.5894274641116037e-06\n",
      "Iteration: 7670 Loss: 1.5877481416888325e-06\n",
      "Iteration: 7671 Loss: 1.5860705935884277e-06\n",
      "Iteration: 7672 Loss: 1.5843948179355781e-06\n",
      "Iteration: 7673 Loss: 1.5827208128588577e-06\n",
      "Iteration: 7674 Loss: 1.5810485764879232e-06\n",
      "Iteration: 7675 Loss: 1.5793781069515694e-06\n",
      "Iteration: 7676 Loss: 1.577709402386192e-06\n",
      "Iteration: 7677 Loss: 1.5760424609257086e-06\n",
      "Iteration: 7678 Loss: 1.5743772807069874e-06\n",
      "Iteration: 7679 Loss: 1.5727138598685867e-06\n",
      "Iteration: 7680 Loss: 1.5710521965517394e-06\n",
      "Iteration: 7681 Loss: 1.5693922889003102e-06\n",
      "Iteration: 7682 Loss: 1.567734135061056e-06\n",
      "Iteration: 7683 Loss: 1.5660777331755676e-06\n",
      "Iteration: 7684 Loss: 1.56442308139685e-06\n",
      "Iteration: 7685 Loss: 1.562770177876272e-06\n",
      "Iteration: 7686 Loss: 1.5611190207646413e-06\n",
      "Iteration: 7687 Loss: 1.5594696082180611e-06\n",
      "Iteration: 7688 Loss: 1.5578219383920756e-06\n",
      "Iteration: 7689 Loss: 1.5561760094453713e-06\n",
      "Iteration: 7690 Loss: 1.5545318196555159e-06\n",
      "Iteration: 7691 Loss: 1.5528893669538313e-06\n",
      "Iteration: 7692 Loss: 1.5512486496189071e-06\n",
      "Iteration: 7693 Loss: 1.5496096658196373e-06\n",
      "Iteration: 7694 Loss: 1.5479724137217094e-06\n",
      "Iteration: 7695 Loss: 1.5463368914965381e-06\n",
      "Iteration: 7696 Loss: 1.544703097201687e-06\n",
      "Iteration: 7697 Loss: 1.5430710292413635e-06\n",
      "Iteration: 7698 Loss: 1.5414406856772169e-06\n",
      "Iteration: 7699 Loss: 1.5398120646844846e-06\n",
      "Iteration: 7700 Loss: 1.5381851644463062e-06\n",
      "Iteration: 7701 Loss: 1.5365599831451313e-06\n",
      "Iteration: 7702 Loss: 1.5349365189606326e-06\n",
      "Iteration: 7703 Loss: 1.5333147700827522e-06\n",
      "Iteration: 7704 Loss: 1.5316947346965997e-06\n",
      "Iteration: 7705 Loss: 1.530076410992758e-06\n",
      "Iteration: 7706 Loss: 1.528459797162599e-06\n",
      "Iteration: 7707 Loss: 1.5268448913999197e-06\n",
      "Iteration: 7708 Loss: 1.5252316919001954e-06\n",
      "Iteration: 7709 Loss: 1.5236201968594046e-06\n",
      "Iteration: 7710 Loss: 1.5220104044747368e-06\n",
      "Iteration: 7711 Loss: 1.520402312953648e-06\n",
      "Iteration: 7712 Loss: 1.5187959204943841e-06\n",
      "Iteration: 7713 Loss: 1.5171912253026973e-06\n",
      "Iteration: 7714 Loss: 1.5155882255838025e-06\n",
      "Iteration: 7715 Loss: 1.513986919547709e-06\n",
      "Iteration: 7716 Loss: 1.5123873054069323e-06\n",
      "Iteration: 7717 Loss: 1.510789381369915e-06\n",
      "Iteration: 7718 Loss: 1.5091931456558234e-06\n",
      "Iteration: 7719 Loss: 1.507598596474382e-06\n",
      "Iteration: 7720 Loss: 1.5060057320487032e-06\n",
      "Iteration: 7721 Loss: 1.504414550597841e-06\n",
      "Iteration: 7722 Loss: 1.5028250503421277e-06\n",
      "Iteration: 7723 Loss: 1.5012372295050163e-06\n",
      "Iteration: 7724 Loss: 1.4996510863145887e-06\n",
      "Iteration: 7725 Loss: 1.4980666189951052e-06\n",
      "Iteration: 7726 Loss: 1.4964838257817467e-06\n",
      "Iteration: 7727 Loss: 1.4949027048979637e-06\n",
      "Iteration: 7728 Loss: 1.493323254582284e-06\n",
      "Iteration: 7729 Loss: 1.4917454730653149e-06\n",
      "Iteration: 7730 Loss: 1.4901693587040038e-06\n",
      "Iteration: 7731 Loss: 1.4885949095031243e-06\n",
      "Iteration: 7732 Loss: 1.4870221237036769e-06\n",
      "Iteration: 7733 Loss: 1.4854509997804217e-06\n",
      "Iteration: 7734 Loss: 1.483881535859187e-06\n",
      "Iteration: 7735 Loss: 1.4823137301887319e-06\n",
      "Iteration: 7736 Loss: 1.4807475810160955e-06\n",
      "Iteration: 7737 Loss: 1.4791830865908447e-06\n",
      "Iteration: 7738 Loss: 1.477620245165358e-06\n",
      "Iteration: 7739 Loss: 1.476059054991318e-06\n",
      "Iteration: 7740 Loss: 1.4744995143275667e-06\n",
      "Iteration: 7741 Loss: 1.472941621426889e-06\n",
      "Iteration: 7742 Loss: 1.4713853745523653e-06\n",
      "Iteration: 7743 Loss: 1.4698307719611327e-06\n",
      "Iteration: 7744 Loss: 1.4682778119192667e-06\n",
      "Iteration: 7745 Loss: 1.4667264926893347e-06\n",
      "Iteration: 7746 Loss: 1.4651768125374135e-06\n",
      "Iteration: 7747 Loss: 1.4636287697321652e-06\n",
      "Iteration: 7748 Loss: 1.4620823625441362e-06\n",
      "Iteration: 7749 Loss: 1.4605375892460706e-06\n",
      "Iteration: 7750 Loss: 1.4589944481091441e-06\n",
      "Iteration: 7751 Loss: 1.4574529374097326e-06\n",
      "Iteration: 7752 Loss: 1.45591305542784e-06\n",
      "Iteration: 7753 Loss: 1.4543748004375468e-06\n",
      "Iteration: 7754 Loss: 1.452838170725057e-06\n",
      "Iteration: 7755 Loss: 1.4513031645698262e-06\n",
      "Iteration: 7756 Loss: 1.4497697802578327e-06\n",
      "Iteration: 7757 Loss: 1.4482380160745935e-06\n",
      "Iteration: 7758 Loss: 1.446707870307961e-06\n",
      "Iteration: 7759 Loss: 1.4451793412506462e-06\n",
      "Iteration: 7760 Loss: 1.4436524271917521e-06\n",
      "Iteration: 7761 Loss: 1.4421271264257184e-06\n",
      "Iteration: 7762 Loss: 1.4406034372478258e-06\n",
      "Iteration: 7763 Loss: 1.4390813579566e-06\n",
      "Iteration: 7764 Loss: 1.4375608868480625e-06\n",
      "Iteration: 7765 Loss: 1.4360420222264456e-06\n",
      "Iteration: 7766 Loss: 1.4345247623908934e-06\n",
      "Iteration: 7767 Loss: 1.4330091056509497e-06\n",
      "Iteration: 7768 Loss: 1.4314950504238242e-06\n",
      "Iteration: 7769 Loss: 1.4299825947875323e-06\n",
      "Iteration: 7770 Loss: 1.4284717371689793e-06\n",
      "Iteration: 7771 Loss: 1.4269624758763657e-06\n",
      "Iteration: 7772 Loss: 1.425454809227025e-06\n",
      "Iteration: 7773 Loss: 1.4239487354197558e-06\n",
      "Iteration: 7774 Loss: 1.4224442530000939e-06\n",
      "Iteration: 7775 Loss: 1.4209413601726811e-06\n",
      "Iteration: 7776 Loss: 1.4194400552580607e-06\n",
      "Iteration: 7777 Loss: 1.4179403366942913e-06\n",
      "Iteration: 7778 Loss: 1.4164422025738592e-06\n",
      "Iteration: 7779 Loss: 1.4149456513374326e-06\n",
      "Iteration: 7780 Loss: 1.4134506812011555e-06\n",
      "Iteration: 7781 Loss: 1.4119572907196812e-06\n",
      "Iteration: 7782 Loss: 1.4104654782267636e-06\n",
      "Iteration: 7783 Loss: 1.4089752417089505e-06\n",
      "Iteration: 7784 Loss: 1.407486579846516e-06\n",
      "Iteration: 7785 Loss: 1.4059994908609815e-06\n",
      "Iteration: 7786 Loss: 1.4045139730903223e-06\n",
      "Iteration: 7787 Loss: 1.403030024874146e-06\n",
      "Iteration: 7788 Loss: 1.4015476445542061e-06\n",
      "Iteration: 7789 Loss: 1.4000668304760407e-06\n",
      "Iteration: 7790 Loss: 1.398587580981602e-06\n",
      "Iteration: 7791 Loss: 1.3971098944201095e-06\n",
      "Iteration: 7792 Loss: 1.3956337691388622e-06\n",
      "Iteration: 7793 Loss: 1.3941592034888291e-06\n",
      "Iteration: 7794 Loss: 1.3926861958186135e-06\n",
      "Iteration: 7795 Loss: 1.3912147444889896e-06\n",
      "Iteration: 7796 Loss: 1.3897448478503772e-06\n",
      "Iteration: 7797 Loss: 1.3882765042623327e-06\n",
      "Iteration: 7798 Loss: 1.3868097120848198e-06\n",
      "Iteration: 7799 Loss: 1.385344469674554e-06\n",
      "Iteration: 7800 Loss: 1.383880775397925e-06\n",
      "Iteration: 7801 Loss: 1.3824186276193306e-06\n",
      "Iteration: 7802 Loss: 1.3809580247017088e-06\n",
      "Iteration: 7803 Loss: 1.3794989650158031e-06\n",
      "Iteration: 7804 Loss: 1.3780414469274337e-06\n",
      "Iteration: 7805 Loss: 1.3765854688126134e-06\n",
      "Iteration: 7806 Loss: 1.3751310290402504e-06\n",
      "Iteration: 7807 Loss: 1.3736781259881472e-06\n",
      "Iteration: 7808 Loss: 1.3722267580310948e-06\n",
      "Iteration: 7809 Loss: 1.370776923542331e-06\n",
      "Iteration: 7810 Loss: 1.3693286209142893e-06\n",
      "Iteration: 7811 Loss: 1.3678818485158419e-06\n",
      "Iteration: 7812 Loss: 1.3664366047369004e-06\n",
      "Iteration: 7813 Loss: 1.364992887958882e-06\n",
      "Iteration: 7814 Loss: 1.3635506965702845e-06\n",
      "Iteration: 7815 Loss: 1.3621100289593705e-06\n",
      "Iteration: 7816 Loss: 1.360670883514895e-06\n",
      "Iteration: 7817 Loss: 1.3592332586315723e-06\n",
      "Iteration: 7818 Loss: 1.3577971526995552e-06\n",
      "Iteration: 7819 Loss: 1.3563625641162548e-06\n",
      "Iteration: 7820 Loss: 1.3549294912771037e-06\n",
      "Iteration: 7821 Loss: 1.3534979325795047e-06\n",
      "Iteration: 7822 Loss: 1.3520678864261646e-06\n",
      "Iteration: 7823 Loss: 1.3506393512181721e-06\n",
      "Iteration: 7824 Loss: 1.3492123253593436e-06\n",
      "Iteration: 7825 Loss: 1.34778680725418e-06\n",
      "Iteration: 7826 Loss: 1.3463627953095623e-06\n",
      "Iteration: 7827 Loss: 1.3449402879351432e-06\n",
      "Iteration: 7828 Loss: 1.3435192835387088e-06\n",
      "Iteration: 7829 Loss: 1.342099780536737e-06\n",
      "Iteration: 7830 Loss: 1.3406817773382836e-06\n",
      "Iteration: 7831 Loss: 1.3392652723599243e-06\n",
      "Iteration: 7832 Loss: 1.3378502640221767e-06\n",
      "Iteration: 7833 Loss: 1.336436750739733e-06\n",
      "Iteration: 7834 Loss: 1.3350247309348608e-06\n",
      "Iteration: 7835 Loss: 1.3336142030272175e-06\n",
      "Iteration: 7836 Loss: 1.3322051654439074e-06\n",
      "Iteration: 7837 Loss: 1.3307976166088499e-06\n",
      "Iteration: 7838 Loss: 1.329391554948502e-06\n",
      "Iteration: 7839 Loss: 1.3279869788904769e-06\n",
      "Iteration: 7840 Loss: 1.3265838868697513e-06\n",
      "Iteration: 7841 Loss: 1.3251822773115601e-06\n",
      "Iteration: 7842 Loss: 1.3237821486544172e-06\n",
      "Iteration: 7843 Loss: 1.3223834993337227e-06\n",
      "Iteration: 7844 Loss: 1.3209863277841789e-06\n",
      "Iteration: 7845 Loss: 1.3195906324451052e-06\n",
      "Iteration: 7846 Loss: 1.3181964117594717e-06\n",
      "Iteration: 7847 Loss: 1.3168036641630343e-06\n",
      "Iteration: 7848 Loss: 1.3154123881059329e-06\n",
      "Iteration: 7849 Loss: 1.3140225820289092e-06\n",
      "Iteration: 7850 Loss: 1.3126342443800983e-06\n",
      "Iteration: 7851 Loss: 1.3112473736103699e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7852 Loss: 1.3098619682819902e-06\n",
      "Iteration: 7853 Loss: 1.3084780265012806e-06\n",
      "Iteration: 7854 Loss: 1.3070955470690446e-06\n",
      "Iteration: 7855 Loss: 1.3057145283261782e-06\n",
      "Iteration: 7856 Loss: 1.3043349687251089e-06\n",
      "Iteration: 7857 Loss: 1.3029568667274347e-06\n",
      "Iteration: 7858 Loss: 1.3015802207926882e-06\n",
      "Iteration: 7859 Loss: 1.3002050293804846e-06\n",
      "Iteration: 7860 Loss: 1.298831290957308e-06\n",
      "Iteration: 7861 Loss: 1.297459003983253e-06\n",
      "Iteration: 7862 Loss: 1.2960881669318637e-06\n",
      "Iteration: 7863 Loss: 1.2947187782646675e-06\n",
      "Iteration: 7864 Loss: 1.2933508364522152e-06\n",
      "Iteration: 7865 Loss: 1.2919843399689658e-06\n",
      "Iteration: 7866 Loss: 1.2906192872866626e-06\n",
      "Iteration: 7867 Loss: 1.2892556768771647e-06\n",
      "Iteration: 7868 Loss: 1.2878935072217785e-06\n",
      "Iteration: 7869 Loss: 1.2865327767914845e-06\n",
      "Iteration: 7870 Loss: 1.285173484072323e-06\n",
      "Iteration: 7871 Loss: 1.2838156275410683e-06\n",
      "Iteration: 7872 Loss: 1.282459205681529e-06\n",
      "Iteration: 7873 Loss: 1.2811042169781805e-06\n",
      "Iteration: 7874 Loss: 1.2797506599159226e-06\n",
      "Iteration: 7875 Loss: 1.278398532983224e-06\n",
      "Iteration: 7876 Loss: 1.2770478346672249e-06\n",
      "Iteration: 7877 Loss: 1.275698563460308e-06\n",
      "Iteration: 7878 Loss: 1.2743507178548668e-06\n",
      "Iteration: 7879 Loss: 1.2730042963444125e-06\n",
      "Iteration: 7880 Loss: 1.2716592974198085e-06\n",
      "Iteration: 7881 Loss: 1.2703157195849617e-06\n",
      "Iteration: 7882 Loss: 1.2689735613351004e-06\n",
      "Iteration: 7883 Loss: 1.2676328211713737e-06\n",
      "Iteration: 7884 Loss: 1.266293497592831e-06\n",
      "Iteration: 7885 Loss: 1.2649555891049382e-06\n",
      "Iteration: 7886 Loss: 1.2636190942129666e-06\n",
      "Iteration: 7887 Loss: 1.2622840114211317e-06\n",
      "Iteration: 7888 Loss: 1.2609503392410207e-06\n",
      "Iteration: 7889 Loss: 1.2596180761788833e-06\n",
      "Iteration: 7890 Loss: 1.2582872207471238e-06\n",
      "Iteration: 7891 Loss: 1.2569577714582434e-06\n",
      "Iteration: 7892 Loss: 1.255629726826985e-06\n",
      "Iteration: 7893 Loss: 1.2543030853701988e-06\n",
      "Iteration: 7894 Loss: 1.2529778456026073e-06\n",
      "Iteration: 7895 Loss: 1.2516540060445286e-06\n",
      "Iteration: 7896 Loss: 1.2503315652176087e-06\n",
      "Iteration: 7897 Loss: 1.2490105216435322e-06\n",
      "Iteration: 7898 Loss: 1.2476908738445107e-06\n",
      "Iteration: 7899 Loss: 1.2463726203483205e-06\n",
      "Iteration: 7900 Loss: 1.2450557596788822e-06\n",
      "Iteration: 7901 Loss: 1.2437402903659785e-06\n",
      "Iteration: 7902 Loss: 1.2424262110554451e-06\n",
      "Iteration: 7903 Loss: 1.2411135200489166e-06\n",
      "Iteration: 7904 Loss: 1.2398022159923093e-06\n",
      "Iteration: 7905 Loss: 1.2384922974215234e-06\n",
      "Iteration: 7906 Loss: 1.237183762872149e-06\n",
      "Iteration: 7907 Loss: 1.2358766108820609e-06\n",
      "Iteration: 7908 Loss: 1.2345708399910626e-06\n",
      "Iteration: 7909 Loss: 1.2332664487391052e-06\n",
      "Iteration: 7910 Loss: 1.23196343555531e-06\n",
      "Iteration: 7911 Loss: 1.2306617992111835e-06\n",
      "Iteration: 7912 Loss: 1.229361538138936e-06\n",
      "Iteration: 7913 Loss: 1.2280626508835087e-06\n",
      "Iteration: 7914 Loss: 1.2267651359949585e-06\n",
      "Iteration: 7915 Loss: 1.2254689920222826e-06\n",
      "Iteration: 7916 Loss: 1.224174217518169e-06\n",
      "Iteration: 7917 Loss: 1.222880811035306e-06\n",
      "Iteration: 7918 Loss: 1.2215887711284383e-06\n",
      "Iteration: 7919 Loss: 1.220298096352622e-06\n",
      "Iteration: 7920 Loss: 1.2190087854671955e-06\n",
      "Iteration: 7921 Loss: 1.2177208366302643e-06\n",
      "Iteration: 7922 Loss: 1.216434248603194e-06\n",
      "Iteration: 7923 Loss: 1.2151490201462004e-06\n",
      "Iteration: 7924 Loss: 1.2138651492241775e-06\n",
      "Iteration: 7925 Loss: 1.2125826350013682e-06\n",
      "Iteration: 7926 Loss: 1.2113014758476668e-06\n",
      "Iteration: 7927 Loss: 1.2100216701276922e-06\n",
      "Iteration: 7928 Loss: 1.2087432169282082e-06\n",
      "Iteration: 7929 Loss: 1.2074661143912972e-06\n",
      "Iteration: 7930 Loss: 1.2061903612031897e-06\n",
      "Iteration: 7931 Loss: 1.2049159558241028e-06\n",
      "Iteration: 7932 Loss: 1.203642897057986e-06\n",
      "Iteration: 7933 Loss: 1.2023711833705726e-06\n",
      "Iteration: 7934 Loss: 1.2011008133378437e-06\n",
      "Iteration: 7935 Loss: 1.1998317855426626e-06\n",
      "Iteration: 7936 Loss: 1.1985640985640143e-06\n",
      "Iteration: 7937 Loss: 1.1972977509864756e-06\n",
      "Iteration: 7938 Loss: 1.1960327413944056e-06\n",
      "Iteration: 7939 Loss: 1.1947690683748326e-06\n",
      "Iteration: 7940 Loss: 1.1935067305175326e-06\n",
      "Iteration: 7941 Loss: 1.1922457264070377e-06\n",
      "Iteration: 7942 Loss: 1.1909860546375957e-06\n",
      "Iteration: 7943 Loss: 1.1897277135997585e-06\n",
      "Iteration: 7944 Loss: 1.188470702488568e-06\n",
      "Iteration: 7945 Loss: 1.1872150194152465e-06\n",
      "Iteration: 7946 Loss: 1.1859606628297289e-06\n",
      "Iteration: 7947 Loss: 1.1847076316744054e-06\n",
      "Iteration: 7948 Loss: 1.1834559244374141e-06\n",
      "Iteration: 7949 Loss: 1.1822055397168905e-06\n",
      "Iteration: 7950 Loss: 1.1809564761176098e-06\n",
      "Iteration: 7951 Loss: 1.1797087322403617e-06\n",
      "Iteration: 7952 Loss: 1.1784623066932902e-06\n",
      "Iteration: 7953 Loss: 1.1772171980849886e-06\n",
      "Iteration: 7954 Loss: 1.1759734050195853e-06\n",
      "Iteration: 7955 Loss: 1.1747309261097464e-06\n",
      "Iteration: 7956 Loss: 1.1734897599693236e-06\n",
      "Iteration: 7957 Loss: 1.172249905208302e-06\n",
      "Iteration: 7958 Loss: 1.1710113604392707e-06\n",
      "Iteration: 7959 Loss: 1.1697741242832833e-06\n",
      "Iteration: 7960 Loss: 1.1685381953541443e-06\n",
      "Iteration: 7961 Loss: 1.1673035722721573e-06\n",
      "Iteration: 7962 Loss: 1.166070253655004e-06\n",
      "Iteration: 7963 Loss: 1.164838238128746e-06\n",
      "Iteration: 7964 Loss: 1.1636075243122582e-06\n",
      "Iteration: 7965 Loss: 1.1623781108327004e-06\n",
      "Iteration: 7966 Loss: 1.161149996316005e-06\n",
      "Iteration: 7967 Loss: 1.1599231793903024e-06\n",
      "Iteration: 7968 Loss: 1.1586976586812565e-06\n",
      "Iteration: 7969 Loss: 1.1574734328237126e-06\n",
      "Iteration: 7970 Loss: 1.1562505004460277e-06\n",
      "Iteration: 7971 Loss: 1.1550288601841873e-06\n",
      "Iteration: 7972 Loss: 1.153808510671841e-06\n",
      "Iteration: 7973 Loss: 1.152589450545186e-06\n",
      "Iteration: 7974 Loss: 1.1513716784414148e-06\n",
      "Iteration: 7975 Loss: 1.1501551930010901e-06\n",
      "Iteration: 7976 Loss: 1.1489399928639784e-06\n",
      "Iteration: 7977 Loss: 1.1477260766688333e-06\n",
      "Iteration: 7978 Loss: 1.1465134430654412e-06\n",
      "Iteration: 7979 Loss: 1.1453020906949948e-06\n",
      "Iteration: 7980 Loss: 1.1440920182026796e-06\n",
      "Iteration: 7981 Loss: 1.1428832242388754e-06\n",
      "Iteration: 7982 Loss: 1.1416757074506851e-06\n",
      "Iteration: 7983 Loss: 1.1404694664907731e-06\n",
      "Iteration: 7984 Loss: 1.1392645000104738e-06\n",
      "Iteration: 7985 Loss: 1.1380608066602728e-06\n",
      "Iteration: 7986 Loss: 1.1368583850982319e-06\n",
      "Iteration: 7987 Loss: 1.1356572339810528e-06\n",
      "Iteration: 7988 Loss: 1.1344573519648674e-06\n",
      "Iteration: 7989 Loss: 1.1332587377095097e-06\n",
      "Iteration: 7990 Loss: 1.1320613898725282e-06\n",
      "Iteration: 7991 Loss: 1.1308653071210703e-06\n",
      "Iteration: 7992 Loss: 1.1296704882300968e-06\n",
      "Iteration: 7993 Loss: 1.1284769316352014e-06\n",
      "Iteration: 7994 Loss: 1.127284636116256e-06\n",
      "Iteration: 7995 Loss: 1.1260936001400768e-06\n",
      "Iteration: 7996 Loss: 1.124903822983214e-06\n",
      "Iteration: 7997 Loss: 1.1237153027067717e-06\n",
      "Iteration: 7998 Loss: 1.1225280381860846e-06\n",
      "Iteration: 7999 Loss: 1.1213420280974206e-06\n",
      "Iteration: 8000 Loss: 1.1201572709942116e-06\n",
      "Iteration: 8001 Loss: 1.118973765785554e-06\n",
      "Iteration: 8002 Loss: 1.1177915110361682e-06\n",
      "Iteration: 8003 Loss: 1.1166105054220258e-06\n",
      "Iteration: 8004 Loss: 1.1154307476239005e-06\n",
      "Iteration: 8005 Loss: 1.1142522363238516e-06\n",
      "Iteration: 8006 Loss: 1.1130749702034586e-06\n",
      "Iteration: 8007 Loss: 1.1118989479494259e-06\n",
      "Iteration: 8008 Loss: 1.1107241682451678e-06\n",
      "Iteration: 8009 Loss: 1.1095506297793136e-06\n",
      "Iteration: 8010 Loss: 1.1083783312401926e-06\n",
      "Iteration: 8011 Loss: 1.1072072713166632e-06\n",
      "Iteration: 8012 Loss: 1.1060374487022602e-06\n",
      "Iteration: 8013 Loss: 1.10486886208715e-06\n",
      "Iteration: 8014 Loss: 1.1037015102838095e-06\n",
      "Iteration: 8015 Loss: 1.1025353917536365e-06\n",
      "Iteration: 8016 Loss: 1.1013705053101665e-06\n",
      "Iteration: 8017 Loss: 1.1002068496528752e-06\n",
      "Iteration: 8018 Loss: 1.0990444234800276e-06\n",
      "Iteration: 8019 Loss: 1.0978832254932348e-06\n",
      "Iteration: 8020 Loss: 1.0967232543943551e-06\n",
      "Iteration: 8021 Loss: 1.0955645088849654e-06\n",
      "Iteration: 8022 Loss: 1.0944069875590804e-06\n",
      "Iteration: 8023 Loss: 1.093250689352177e-06\n",
      "Iteration: 8024 Loss: 1.0920956128548715e-06\n",
      "Iteration: 8025 Loss: 1.0909417567788661e-06\n",
      "Iteration: 8026 Loss: 1.0897891198321548e-06\n",
      "Iteration: 8027 Loss: 1.088637700728409e-06\n",
      "Iteration: 8028 Loss: 1.0874874981818859e-06\n",
      "Iteration: 8029 Loss: 1.0863385109034555e-06\n",
      "Iteration: 8030 Loss: 1.0851907376135028e-06\n",
      "Iteration: 8031 Loss: 1.0840441770274023e-06\n",
      "Iteration: 8032 Loss: 1.082898827863249e-06\n",
      "Iteration: 8033 Loss: 1.081754688839999e-06\n",
      "Iteration: 8034 Loss: 1.080611758682149e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8035 Loss: 1.0794700361116988e-06\n",
      "Iteration: 8036 Loss: 1.0783295198502141e-06\n",
      "Iteration: 8037 Loss: 1.0771902086277093e-06\n",
      "Iteration: 8038 Loss: 1.0760521012822368e-06\n",
      "Iteration: 8039 Loss: 1.074915196198068e-06\n",
      "Iteration: 8040 Loss: 1.0737794924482961e-06\n",
      "Iteration: 8041 Loss: 1.0726449886513019e-06\n",
      "Iteration: 8042 Loss: 1.0715116835359965e-06\n",
      "Iteration: 8043 Loss: 1.070379575839857e-06\n",
      "Iteration: 8044 Loss: 1.0692486644109156e-06\n",
      "Iteration: 8045 Loss: 1.0681189477550032e-06\n",
      "Iteration: 8046 Loss: 1.0669904247246349e-06\n",
      "Iteration: 8047 Loss: 1.065863094056579e-06\n",
      "Iteration: 8048 Loss: 1.0647369544962814e-06\n",
      "Iteration: 8049 Loss: 1.0636120047807048e-06\n",
      "Iteration: 8050 Loss: 1.0624882436546815e-06\n",
      "Iteration: 8051 Loss: 1.0613656698616845e-06\n",
      "Iteration: 8052 Loss: 1.060244282147942e-06\n",
      "Iteration: 8053 Loss: 1.0591240792602527e-06\n",
      "Iteration: 8054 Loss: 1.058005059944812e-06\n",
      "Iteration: 8055 Loss: 1.056887222953191e-06\n",
      "Iteration: 8056 Loss: 1.0557705670356541e-06\n",
      "Iteration: 8057 Loss: 1.0546550909442753e-06\n",
      "Iteration: 8058 Loss: 1.0535407934315781e-06\n",
      "Iteration: 8059 Loss: 1.0524276732557461e-06\n",
      "Iteration: 8060 Loss: 1.0513157291698252e-06\n",
      "Iteration: 8061 Loss: 1.050204959815021e-06\n",
      "Iteration: 8062 Loss: 1.0490953642969606e-06\n",
      "Iteration: 8063 Loss: 1.0479869410284642e-06\n",
      "Iteration: 8064 Loss: 1.0468796887744129e-06\n",
      "Iteration: 8065 Loss: 1.0457736065258552e-06\n",
      "Iteration: 8066 Loss: 1.0446686929330822e-06\n",
      "Iteration: 8067 Loss: 1.0435649467569346e-06\n",
      "Iteration: 8068 Loss: 1.042462366770669e-06\n",
      "Iteration: 8069 Loss: 1.0413609517355757e-06\n",
      "Iteration: 8070 Loss: 1.0402607004236143e-06\n",
      "Iteration: 8071 Loss: 1.0391616116058955e-06\n",
      "Iteration: 8072 Loss: 1.0380636840530709e-06\n",
      "Iteration: 8073 Loss: 1.0369669165371892e-06\n",
      "Iteration: 8074 Loss: 1.0358713078369305e-06\n",
      "Iteration: 8075 Loss: 1.0347768567223202e-06\n",
      "Iteration: 8076 Loss: 1.0336835619737405e-06\n",
      "Iteration: 8077 Loss: 1.0325914223666643e-06\n",
      "Iteration: 8078 Loss: 1.0315004366860074e-06\n",
      "Iteration: 8079 Loss: 1.0304106037070643e-06\n",
      "Iteration: 8080 Loss: 1.029321922214332e-06\n",
      "Iteration: 8081 Loss: 1.0282343911053212e-06\n",
      "Iteration: 8082 Loss: 1.0271480088209009e-06\n",
      "Iteration: 8083 Loss: 1.0260627744892933e-06\n",
      "Iteration: 8084 Loss: 1.0249786867870828e-06\n",
      "Iteration: 8085 Loss: 1.0238957445000273e-06\n",
      "Iteration: 8086 Loss: 1.0228139464178006e-06\n",
      "Iteration: 8087 Loss: 1.0217332913306245e-06\n",
      "Iteration: 8088 Loss: 1.0206537780328287e-06\n",
      "Iteration: 8089 Loss: 1.0195754053192137e-06\n",
      "Iteration: 8090 Loss: 1.0184981719802322e-06\n",
      "Iteration: 8091 Loss: 1.0174220768155447e-06\n",
      "Iteration: 8092 Loss: 1.0163471186217702e-06\n",
      "Iteration: 8093 Loss: 1.015273296196993e-06\n",
      "Iteration: 8094 Loss: 1.0142006083434977e-06\n",
      "Iteration: 8095 Loss: 1.0131290538577665e-06\n",
      "Iteration: 8096 Loss: 1.012058631749898e-06\n",
      "Iteration: 8097 Loss: 1.01098934041647e-06\n",
      "Iteration: 8098 Loss: 1.0099211786618597e-06\n",
      "Iteration: 8099 Loss: 1.0088541456991185e-06\n",
      "Iteration: 8100 Loss: 1.0077882401315101e-06\n",
      "Iteration: 8101 Loss: 1.0067234607693867e-06\n",
      "Iteration: 8102 Loss: 1.0056598064222028e-06\n",
      "Iteration: 8103 Loss: 1.0045972758986844e-06\n",
      "Iteration: 8104 Loss: 1.0035358680179851e-06\n",
      "Iteration: 8105 Loss: 1.0024755815892524e-06\n",
      "Iteration: 8106 Loss: 1.0014164154238443e-06\n",
      "Iteration: 8107 Loss: 1.0003583683454314e-06\n",
      "Iteration: 8108 Loss: 9.993014392859174e-07\n",
      "Iteration: 8109 Loss: 9.98245626830592e-07\n",
      "Iteration: 8110 Loss: 9.97190929914214e-07\n",
      "Iteration: 8111 Loss: 9.961373473591178e-07\n",
      "Iteration: 8112 Loss: 9.950848779929004e-07\n",
      "Iteration: 8113 Loss: 9.940335206325364e-07\n",
      "Iteration: 8114 Loss: 9.929832741075132e-07\n",
      "Iteration: 8115 Loss: 9.919341372399198e-07\n",
      "Iteration: 8116 Loss: 9.908861088594296e-07\n",
      "Iteration: 8117 Loss: 9.898391876828123e-07\n",
      "Iteration: 8118 Loss: 9.88793372766417e-07\n",
      "Iteration: 8119 Loss: 9.877486628263385e-07\n",
      "Iteration: 8120 Loss: 9.867050566987411e-07\n",
      "Iteration: 8121 Loss: 9.856625534143869e-07\n",
      "Iteration: 8122 Loss: 9.846211512058909e-07\n",
      "Iteration: 8123 Loss: 9.83580849514718e-07\n",
      "Iteration: 8124 Loss: 9.82541646977187e-07\n",
      "Iteration: 8125 Loss: 9.815035424264485e-07\n",
      "Iteration: 8126 Loss: 9.80466534826993e-07\n",
      "Iteration: 8127 Loss: 9.794306227829227e-07\n",
      "Iteration: 8128 Loss: 9.783958052513663e-07\n",
      "Iteration: 8129 Loss: 9.77362081079786e-07\n",
      "Iteration: 8130 Loss: 9.763294491111907e-07\n",
      "Iteration: 8131 Loss: 9.75297908193052e-07\n",
      "Iteration: 8132 Loss: 9.742674571682978e-07\n",
      "Iteration: 8133 Loss: 9.7323809488628e-07\n",
      "Iteration: 8134 Loss: 9.72209820087882e-07\n",
      "Iteration: 8135 Loss: 9.71182631847359e-07\n",
      "Iteration: 8136 Loss: 9.70156528903646e-07\n",
      "Iteration: 8137 Loss: 9.69131510111301e-07\n",
      "Iteration: 8138 Loss: 9.681075743243497e-07\n",
      "Iteration: 8139 Loss: 9.670847203956934e-07\n",
      "Iteration: 8140 Loss: 9.66062947186889e-07\n",
      "Iteration: 8141 Loss: 9.650422535540723e-07\n",
      "Iteration: 8142 Loss: 9.640226383559554e-07\n",
      "Iteration: 8143 Loss: 9.630041004517675e-07\n",
      "Iteration: 8144 Loss: 9.6198663870854e-07\n",
      "Iteration: 8145 Loss: 9.609702519820442e-07\n",
      "Iteration: 8146 Loss: 9.599549391419992e-07\n",
      "Iteration: 8147 Loss: 9.58940699053071e-07\n",
      "Iteration: 8148 Loss: 9.579275305788126e-07\n",
      "Iteration: 8149 Loss: 9.569154325887493e-07\n",
      "Iteration: 8150 Loss: 9.559044039549771e-07\n",
      "Iteration: 8151 Loss: 9.548944435406872e-07\n",
      "Iteration: 8152 Loss: 9.538855502228082e-07\n",
      "Iteration: 8153 Loss: 9.528777230737687e-07\n",
      "Iteration: 8154 Loss: 9.518709605624269e-07\n",
      "Iteration: 8155 Loss: 9.508652615686503e-07\n",
      "Iteration: 8156 Loss: 9.498606253674229e-07\n",
      "Iteration: 8157 Loss: 9.488570506349932e-07\n",
      "Iteration: 8158 Loss: 9.47854536252371e-07\n",
      "Iteration: 8159 Loss: 9.468530810945032e-07\n",
      "Iteration: 8160 Loss: 9.458526840465346e-07\n",
      "Iteration: 8161 Loss: 9.448533439871097e-07\n",
      "Iteration: 8162 Loss: 9.438550598034565e-07\n",
      "Iteration: 8163 Loss: 9.428578303751019e-07\n",
      "Iteration: 8164 Loss: 9.418616545936666e-07\n",
      "Iteration: 8165 Loss: 9.4086653134077e-07\n",
      "Iteration: 8166 Loss: 9.398724595061273e-07\n",
      "Iteration: 8167 Loss: 9.388794379791685e-07\n",
      "Iteration: 8168 Loss: 9.378874657639216e-07\n",
      "Iteration: 8169 Loss: 9.36896541523896e-07\n",
      "Iteration: 8170 Loss: 9.359066642661632e-07\n",
      "Iteration: 8171 Loss: 9.349178327671507e-07\n",
      "Iteration: 8172 Loss: 9.339300461549864e-07\n",
      "Iteration: 8173 Loss: 9.329433032095453e-07\n",
      "Iteration: 8174 Loss: 9.319576028262686e-07\n",
      "Iteration: 8175 Loss: 9.309729439044159e-07\n",
      "Iteration: 8176 Loss: 9.299893255474071e-07\n",
      "Iteration: 8177 Loss: 9.290067464545139e-07\n",
      "Iteration: 8178 Loss: 9.280252053246946e-07\n",
      "Iteration: 8179 Loss: 9.27044701260599e-07\n",
      "Iteration: 8180 Loss: 9.260652331716407e-07\n",
      "Iteration: 8181 Loss: 9.250867997563339e-07\n",
      "Iteration: 8182 Loss: 9.241094001249056e-07\n",
      "Iteration: 8183 Loss: 9.231330331866894e-07\n",
      "Iteration: 8184 Loss: 9.221576980482695e-07\n",
      "Iteration: 8185 Loss: 9.211833934215519e-07\n",
      "Iteration: 8186 Loss: 9.202101182172007e-07\n",
      "Iteration: 8187 Loss: 9.192378713464046e-07\n",
      "Iteration: 8188 Loss: 9.18266651724504e-07\n",
      "Iteration: 8189 Loss: 9.172964582633726e-07\n",
      "Iteration: 8190 Loss: 9.163272898831556e-07\n",
      "Iteration: 8191 Loss: 9.15359145609986e-07\n",
      "Iteration: 8192 Loss: 9.143920241360149e-07\n",
      "Iteration: 8193 Loss: 9.134259244951118e-07\n",
      "Iteration: 8194 Loss: 9.124608456065773e-07\n",
      "Iteration: 8195 Loss: 9.114967863924447e-07\n",
      "Iteration: 8196 Loss: 9.105337457735823e-07\n",
      "Iteration: 8197 Loss: 9.095717226773808e-07\n",
      "Iteration: 8198 Loss: 9.08610716023924e-07\n",
      "Iteration: 8199 Loss: 9.076507247463614e-07\n",
      "Iteration: 8200 Loss: 9.066917476516483e-07\n",
      "Iteration: 8201 Loss: 9.057337838970045e-07\n",
      "Iteration: 8202 Loss: 9.047768323004794e-07\n",
      "Iteration: 8203 Loss: 9.038208917899709e-07\n",
      "Iteration: 8204 Loss: 9.02865961298676e-07\n",
      "Iteration: 8205 Loss: 9.019120397600147e-07\n",
      "Iteration: 8206 Loss: 9.00959126106567e-07\n",
      "Iteration: 8207 Loss: 9.000072192734556e-07\n",
      "Iteration: 8208 Loss: 8.990563181966824e-07\n",
      "Iteration: 8209 Loss: 8.981064218168998e-07\n",
      "Iteration: 8210 Loss: 8.971575290684259e-07\n",
      "Iteration: 8211 Loss: 8.962096388912488e-07\n",
      "Iteration: 8212 Loss: 8.952627502291847e-07\n",
      "Iteration: 8213 Loss: 8.943168620211383e-07\n",
      "Iteration: 8214 Loss: 8.933719732105184e-07\n",
      "Iteration: 8215 Loss: 8.924280827433604e-07\n",
      "Iteration: 8216 Loss: 8.914851895618933e-07\n",
      "Iteration: 8217 Loss: 8.90543292614957e-07\n",
      "Iteration: 8218 Loss: 8.896023908487792e-07\n",
      "Iteration: 8219 Loss: 8.886624832114973e-07\n",
      "Iteration: 8220 Loss: 8.877235686510014e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8221 Loss: 8.867856461242303e-07\n",
      "Iteration: 8222 Loss: 8.858487145779127e-07\n",
      "Iteration: 8223 Loss: 8.849127729651289e-07\n",
      "Iteration: 8224 Loss: 8.839778202427175e-07\n",
      "Iteration: 8225 Loss: 8.830438553630954e-07\n",
      "Iteration: 8226 Loss: 8.82110877283751e-07\n",
      "Iteration: 8227 Loss: 8.811788849604457e-07\n",
      "Iteration: 8228 Loss: 8.802478773551675e-07\n",
      "Iteration: 8229 Loss: 8.793178534242856e-07\n",
      "Iteration: 8230 Loss: 8.783888121301084e-07\n",
      "Iteration: 8231 Loss: 8.774607524325178e-07\n",
      "Iteration: 8232 Loss: 8.765336735003937e-07\n",
      "Iteration: 8233 Loss: 8.756075736889362e-07\n",
      "Iteration: 8234 Loss: 8.746824525661396e-07\n",
      "Iteration: 8235 Loss: 8.737583089028992e-07\n",
      "Iteration: 8236 Loss: 8.728351416592251e-07\n",
      "Iteration: 8237 Loss: 8.71912949809619e-07\n",
      "Iteration: 8238 Loss: 8.70991732320613e-07\n",
      "Iteration: 8239 Loss: 8.700714881631262e-07\n",
      "Iteration: 8240 Loss: 8.691522163081598e-07\n",
      "Iteration: 8241 Loss: 8.682339157305311e-07\n",
      "Iteration: 8242 Loss: 8.673165854019941e-07\n",
      "Iteration: 8243 Loss: 8.664002242978668e-07\n",
      "Iteration: 8244 Loss: 8.654848313938586e-07\n",
      "Iteration: 8245 Loss: 8.645704056685162e-07\n",
      "Iteration: 8246 Loss: 8.636569460967818e-07\n",
      "Iteration: 8247 Loss: 8.627444516603248e-07\n",
      "Iteration: 8248 Loss: 8.61832921339788e-07\n",
      "Iteration: 8249 Loss: 8.609223541137933e-07\n",
      "Iteration: 8250 Loss: 8.600127489687719e-07\n",
      "Iteration: 8251 Loss: 8.59104104885635e-07\n",
      "Iteration: 8252 Loss: 8.581964209636724e-07\n",
      "Iteration: 8253 Loss: 8.572896959595761e-07\n",
      "Iteration: 8254 Loss: 8.563839289751543e-07\n",
      "Iteration: 8255 Loss: 8.554791189965851e-07\n",
      "Iteration: 8256 Loss: 8.54575265015947e-07\n",
      "Iteration: 8257 Loss: 8.536723660189489e-07\n",
      "Iteration: 8258 Loss: 8.527704210000485e-07\n",
      "Iteration: 8259 Loss: 8.518694288346938e-07\n",
      "Iteration: 8260 Loss: 8.509693889467182e-07\n",
      "Iteration: 8261 Loss: 8.500702998126705e-07\n",
      "Iteration: 8262 Loss: 8.491721606320109e-07\n",
      "Iteration: 8263 Loss: 8.482749703990074e-07\n",
      "Iteration: 8264 Loss: 8.473787281113798e-07\n",
      "Iteration: 8265 Loss: 8.464834327660903e-07\n",
      "Iteration: 8266 Loss: 8.455890833636085e-07\n",
      "Iteration: 8267 Loss: 8.446956789045376e-07\n",
      "Iteration: 8268 Loss: 8.438032183918586e-07\n",
      "Iteration: 8269 Loss: 8.429117008263275e-07\n",
      "Iteration: 8270 Loss: 8.420211252140438e-07\n",
      "Iteration: 8271 Loss: 8.41131490555111e-07\n",
      "Iteration: 8272 Loss: 8.402427958600176e-07\n",
      "Iteration: 8273 Loss: 8.393550401342459e-07\n",
      "Iteration: 8274 Loss: 8.384682223831478e-07\n",
      "Iteration: 8275 Loss: 8.375823416210183e-07\n",
      "Iteration: 8276 Loss: 8.366973968521341e-07\n",
      "Iteration: 8277 Loss: 8.358133870899425e-07\n",
      "Iteration: 8278 Loss: 8.349303113479441e-07\n",
      "Iteration: 8279 Loss: 8.340481686366071e-07\n",
      "Iteration: 8280 Loss: 8.331669579739368e-07\n",
      "Iteration: 8281 Loss: 8.32286678370437e-07\n",
      "Iteration: 8282 Loss: 8.314073288466615e-07\n",
      "Iteration: 8283 Loss: 8.305289084160828e-07\n",
      "Iteration: 8284 Loss: 8.296514161012954e-07\n",
      "Iteration: 8285 Loss: 8.287748509177911e-07\n",
      "Iteration: 8286 Loss: 8.278992118858317e-07\n",
      "Iteration: 8287 Loss: 8.270244980316783e-07\n",
      "Iteration: 8288 Loss: 8.261507083751432e-07\n",
      "Iteration: 8289 Loss: 8.252778420509554e-07\n",
      "Iteration: 8290 Loss: 8.244058978603485e-07\n",
      "Iteration: 8291 Loss: 8.23534874937726e-07\n",
      "Iteration: 8292 Loss: 8.226647723123545e-07\n",
      "Iteration: 8293 Loss: 8.217955890141985e-07\n",
      "Iteration: 8294 Loss: 8.209273240698106e-07\n",
      "Iteration: 8295 Loss: 8.200599765074506e-07\n",
      "Iteration: 8296 Loss: 8.191935453599934e-07\n",
      "Iteration: 8297 Loss: 8.183280296580839e-07\n",
      "Iteration: 8298 Loss: 8.174634284344742e-07\n",
      "Iteration: 8299 Loss: 8.165997407250476e-07\n",
      "Iteration: 8300 Loss: 8.1573696544698e-07\n",
      "Iteration: 8301 Loss: 8.148751018656372e-07\n",
      "Iteration: 8302 Loss: 8.140141489065246e-07\n",
      "Iteration: 8303 Loss: 8.131541056047285e-07\n",
      "Iteration: 8304 Loss: 8.122949709993386e-07\n",
      "Iteration: 8305 Loss: 8.114367443308409e-07\n",
      "Iteration: 8306 Loss: 8.105794242411501e-07\n",
      "Iteration: 8307 Loss: 8.097230097657893e-07\n",
      "Iteration: 8308 Loss: 8.08867500356949e-07\n",
      "Iteration: 8309 Loss: 8.080128948527668e-07\n",
      "Iteration: 8310 Loss: 8.071591923019483e-07\n",
      "Iteration: 8311 Loss: 8.063063917473086e-07\n",
      "Iteration: 8312 Loss: 8.054544922352578e-07\n",
      "Iteration: 8313 Loss: 8.046034928163648e-07\n",
      "Iteration: 8314 Loss: 8.037533925397369e-07\n",
      "Iteration: 8315 Loss: 8.029041904550177e-07\n",
      "Iteration: 8316 Loss: 8.020558856109845e-07\n",
      "Iteration: 8317 Loss: 8.012084770595447e-07\n",
      "Iteration: 8318 Loss: 8.003619638587528e-07\n",
      "Iteration: 8319 Loss: 7.99516345057059e-07\n",
      "Iteration: 8320 Loss: 7.986716197121786e-07\n",
      "Iteration: 8321 Loss: 7.978277868790338e-07\n",
      "Iteration: 8322 Loss: 7.969848456145692e-07\n",
      "Iteration: 8323 Loss: 7.961427949787775e-07\n",
      "Iteration: 8324 Loss: 7.953016340284643e-07\n",
      "Iteration: 8325 Loss: 7.944613618242007e-07\n",
      "Iteration: 8326 Loss: 7.936219774268103e-07\n",
      "Iteration: 8327 Loss: 7.927834798996521e-07\n",
      "Iteration: 8328 Loss: 7.919458683040491e-07\n",
      "Iteration: 8329 Loss: 7.911091417047747e-07\n",
      "Iteration: 8330 Loss: 7.90273299164856e-07\n",
      "Iteration: 8331 Loss: 7.894383397534813e-07\n",
      "Iteration: 8332 Loss: 7.886042625342407e-07\n",
      "Iteration: 8333 Loss: 7.877710666925208e-07\n",
      "Iteration: 8334 Loss: 7.869387510676577e-07\n",
      "Iteration: 8335 Loss: 7.861073147267042e-07\n",
      "Iteration: 8336 Loss: 7.85276756971853e-07\n",
      "Iteration: 8337 Loss: 7.844470767609011e-07\n",
      "Iteration: 8338 Loss: 7.836182731658854e-07\n",
      "Iteration: 8339 Loss: 7.827903452617424e-07\n",
      "Iteration: 8340 Loss: 7.81963292120438e-07\n",
      "Iteration: 8341 Loss: 7.811371128186791e-07\n",
      "Iteration: 8342 Loss: 7.803118065505425e-07\n",
      "Iteration: 8343 Loss: 7.794873721618748e-07\n",
      "Iteration: 8344 Loss: 7.786638088461332e-07\n",
      "Iteration: 8345 Loss: 7.778411156841329e-07\n",
      "Iteration: 8346 Loss: 7.770192917548289e-07\n",
      "Iteration: 8347 Loss: 7.761983360260188e-07\n",
      "Iteration: 8348 Loss: 7.753782478111415e-07\n",
      "Iteration: 8349 Loss: 7.745590260760847e-07\n",
      "Iteration: 8350 Loss: 7.737406699056663e-07\n",
      "Iteration: 8351 Loss: 7.729231783892762e-07\n",
      "Iteration: 8352 Loss: 7.7210655060975e-07\n",
      "Iteration: 8353 Loss: 7.712907856539305e-07\n",
      "Iteration: 8354 Loss: 7.704758826129e-07\n",
      "Iteration: 8355 Loss: 7.69661840575849e-07\n",
      "Iteration: 8356 Loss: 7.688486586301415e-07\n",
      "Iteration: 8357 Loss: 7.680363358692001e-07\n",
      "Iteration: 8358 Loss: 7.672248713849904e-07\n",
      "Iteration: 8359 Loss: 7.664142642707249e-07\n",
      "Iteration: 8360 Loss: 7.656045136197252e-07\n",
      "Iteration: 8361 Loss: 7.647956185282135e-07\n",
      "Iteration: 8362 Loss: 7.639875780912433e-07\n",
      "Iteration: 8363 Loss: 7.631803914052486e-07\n",
      "Iteration: 8364 Loss: 7.623740575718248e-07\n",
      "Iteration: 8365 Loss: 7.615685756837993e-07\n",
      "Iteration: 8366 Loss: 7.607639448477699e-07\n",
      "Iteration: 8367 Loss: 7.599601641580217e-07\n",
      "Iteration: 8368 Loss: 7.591572327223549e-07\n",
      "Iteration: 8369 Loss: 7.583551496374504e-07\n",
      "Iteration: 8370 Loss: 7.5755391401489e-07\n",
      "Iteration: 8371 Loss: 7.567535249529955e-07\n",
      "Iteration: 8372 Loss: 7.559539815571317e-07\n",
      "Iteration: 8373 Loss: 7.551552829361913e-07\n",
      "Iteration: 8374 Loss: 7.54357428197596e-07\n",
      "Iteration: 8375 Loss: 7.53560416449126e-07\n",
      "Iteration: 8376 Loss: 7.527642467984946e-07\n",
      "Iteration: 8377 Loss: 7.519689183587844e-07\n",
      "Iteration: 8378 Loss: 7.511744302403688e-07\n",
      "Iteration: 8379 Loss: 7.503807817545695e-07\n",
      "Iteration: 8380 Loss: 7.495879716150734e-07\n",
      "Iteration: 8381 Loss: 7.487959991357375e-07\n",
      "Iteration: 8382 Loss: 7.480048636314486e-07\n",
      "Iteration: 8383 Loss: 7.472145638190745e-07\n",
      "Iteration: 8384 Loss: 7.46425099012769e-07\n",
      "Iteration: 8385 Loss: 7.456364681308997e-07\n",
      "Iteration: 8386 Loss: 7.448486704932397e-07\n",
      "Iteration: 8387 Loss: 7.44061705419094e-07\n",
      "Iteration: 8388 Loss: 7.432755720329645e-07\n",
      "Iteration: 8389 Loss: 7.424902690495415e-07\n",
      "Iteration: 8390 Loss: 7.417057957971214e-07\n",
      "Iteration: 8391 Loss: 7.409221511943084e-07\n",
      "Iteration: 8392 Loss: 7.401393347669389e-07\n",
      "Iteration: 8393 Loss: 7.393573454405528e-07\n",
      "Iteration: 8394 Loss: 7.385761823433947e-07\n",
      "Iteration: 8395 Loss: 7.377958445991048e-07\n",
      "Iteration: 8396 Loss: 7.370163313353835e-07\n",
      "Iteration: 8397 Loss: 7.36237641684767e-07\n",
      "Iteration: 8398 Loss: 7.354597747740628e-07\n",
      "Iteration: 8399 Loss: 7.346827297362151e-07\n",
      "Iteration: 8400 Loss: 7.339065056997111e-07\n",
      "Iteration: 8401 Loss: 7.331311017995179e-07\n",
      "Iteration: 8402 Loss: 7.323565171696722e-07\n",
      "Iteration: 8403 Loss: 7.315827509410353e-07\n",
      "Iteration: 8404 Loss: 7.308098022527953e-07\n",
      "Iteration: 8405 Loss: 7.300376702407859e-07\n",
      "Iteration: 8406 Loss: 7.29266354038084e-07\n",
      "Iteration: 8407 Loss: 7.284958527864044e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8408 Loss: 7.277261657413918e-07\n",
      "Iteration: 8409 Loss: 7.269572918086066e-07\n",
      "Iteration: 8410 Loss: 7.261892304483921e-07\n",
      "Iteration: 8411 Loss: 7.254219801958449e-07\n",
      "Iteration: 8412 Loss: 7.246555407970159e-07\n",
      "Iteration: 8413 Loss: 7.238899110799405e-07\n",
      "Iteration: 8414 Loss: 7.231250904228156e-07\n",
      "Iteration: 8415 Loss: 7.223610778520587e-07\n",
      "Iteration: 8416 Loss: 7.215978725152023e-07\n",
      "Iteration: 8417 Loss: 7.208354735593294e-07\n",
      "Iteration: 8418 Loss: 7.200738801314167e-07\n",
      "Iteration: 8419 Loss: 7.193130913806273e-07\n",
      "Iteration: 8420 Loss: 7.185531064588884e-07\n",
      "Iteration: 8421 Loss: 7.177939245115802e-07\n",
      "Iteration: 8422 Loss: 7.170355446954188e-07\n",
      "Iteration: 8423 Loss: 7.162779661621635e-07\n",
      "Iteration: 8424 Loss: 7.155211880630183e-07\n",
      "Iteration: 8425 Loss: 7.147652095531282e-07\n",
      "Iteration: 8426 Loss: 7.140100297908757e-07\n",
      "Iteration: 8427 Loss: 7.132556479257772e-07\n",
      "Iteration: 8428 Loss: 7.125020631212274e-07\n",
      "Iteration: 8429 Loss: 7.117492745315391e-07\n",
      "Iteration: 8430 Loss: 7.10997281316751e-07\n",
      "Iteration: 8431 Loss: 7.102460826361264e-07\n",
      "Iteration: 8432 Loss: 7.094956776499742e-07\n",
      "Iteration: 8433 Loss: 7.087460655204757e-07\n",
      "Iteration: 8434 Loss: 7.07997245407955e-07\n",
      "Iteration: 8435 Loss: 7.072492164778015e-07\n",
      "Iteration: 8436 Loss: 7.065019778911922e-07\n",
      "Iteration: 8437 Loss: 7.057555288192492e-07\n",
      "Iteration: 8438 Loss: 7.050098684215627e-07\n",
      "Iteration: 8439 Loss: 7.042649958662111e-07\n",
      "Iteration: 8440 Loss: 7.035209103208717e-07\n",
      "Iteration: 8441 Loss: 7.027776109548021e-07\n",
      "Iteration: 8442 Loss: 7.020350969388988e-07\n",
      "Iteration: 8443 Loss: 7.012933674424283e-07\n",
      "Iteration: 8444 Loss: 7.005524216336528e-07\n",
      "Iteration: 8445 Loss: 6.99812258689599e-07\n",
      "Iteration: 8446 Loss: 6.990728777758515e-07\n",
      "Iteration: 8447 Loss: 6.983342780740366e-07\n",
      "Iteration: 8448 Loss: 6.975964587538177e-07\n",
      "Iteration: 8449 Loss: 6.968594189898888e-07\n",
      "Iteration: 8450 Loss: 6.961231579635637e-07\n",
      "Iteration: 8451 Loss: 6.953876748479954e-07\n",
      "Iteration: 8452 Loss: 6.94652968823002e-07\n",
      "Iteration: 8453 Loss: 6.939190390666094e-07\n",
      "Iteration: 8454 Loss: 6.931858847586181e-07\n",
      "Iteration: 8455 Loss: 6.92453505080262e-07\n",
      "Iteration: 8456 Loss: 6.917218992136893e-07\n",
      "Iteration: 8457 Loss: 6.909910663415121e-07\n",
      "Iteration: 8458 Loss: 6.90261005644091e-07\n",
      "Iteration: 8459 Loss: 6.895317163067449e-07\n",
      "Iteration: 8460 Loss: 6.888031975178607e-07\n",
      "Iteration: 8461 Loss: 6.880754484592293e-07\n",
      "Iteration: 8462 Loss: 6.873484683182653e-07\n",
      "Iteration: 8463 Loss: 6.866222562848653e-07\n",
      "Iteration: 8464 Loss: 6.858968115456585e-07\n",
      "Iteration: 8465 Loss: 6.851721332896199e-07\n",
      "Iteration: 8466 Loss: 6.844482207095845e-07\n",
      "Iteration: 8467 Loss: 6.837250727886312e-07\n",
      "Iteration: 8468 Loss: 6.830026893314389e-07\n",
      "Iteration: 8469 Loss: 6.822810689211366e-07\n",
      "Iteration: 8470 Loss: 6.815602109531926e-07\n",
      "Iteration: 8471 Loss: 6.808401146237592e-07\n",
      "Iteration: 8472 Loss: 6.801207791257898e-07\n",
      "Iteration: 8473 Loss: 6.7940220365739e-07\n",
      "Iteration: 8474 Loss: 6.786843874164915e-07\n",
      "Iteration: 8475 Loss: 6.779673295993601e-07\n",
      "Iteration: 8476 Loss: 6.772510294015962e-07\n",
      "Iteration: 8477 Loss: 6.765354860270415e-07\n",
      "Iteration: 8478 Loss: 6.758206986770917e-07\n",
      "Iteration: 8479 Loss: 6.751066665480417e-07\n",
      "Iteration: 8480 Loss: 6.743933888454114e-07\n",
      "Iteration: 8481 Loss: 6.736808647711586e-07\n",
      "Iteration: 8482 Loss: 6.729690935289762e-07\n",
      "Iteration: 8483 Loss: 6.722580743237918e-07\n",
      "Iteration: 8484 Loss: 6.715478063603673e-07\n",
      "Iteration: 8485 Loss: 6.708382888462836e-07\n",
      "Iteration: 8486 Loss: 6.701295209880808e-07\n",
      "Iteration: 8487 Loss: 6.694215019906171e-07\n",
      "Iteration: 8488 Loss: 6.687142310661715e-07\n",
      "Iteration: 8489 Loss: 6.680077074268671e-07\n",
      "Iteration: 8490 Loss: 6.673019302769733e-07\n",
      "Iteration: 8491 Loss: 6.66596898831583e-07\n",
      "Iteration: 8492 Loss: 6.658926123017776e-07\n",
      "Iteration: 8493 Loss: 6.651890698992879e-07\n",
      "Iteration: 8494 Loss: 6.644862708407332e-07\n",
      "Iteration: 8495 Loss: 6.637842143392972e-07\n",
      "Iteration: 8496 Loss: 6.630828996092855e-07\n",
      "Iteration: 8497 Loss: 6.623823258672991e-07\n",
      "Iteration: 8498 Loss: 6.616824923332915e-07\n",
      "Iteration: 8499 Loss: 6.609833982216628e-07\n",
      "Iteration: 8500 Loss: 6.602850427523344e-07\n",
      "Iteration: 8501 Loss: 6.595874251461902e-07\n",
      "Iteration: 8502 Loss: 6.588905446211068e-07\n",
      "Iteration: 8503 Loss: 6.581944003998456e-07\n",
      "Iteration: 8504 Loss: 6.574989917051245e-07\n",
      "Iteration: 8505 Loss: 6.568043177577883e-07\n",
      "Iteration: 8506 Loss: 6.561103777859707e-07\n",
      "Iteration: 8507 Loss: 6.554171710093154e-07\n",
      "Iteration: 8508 Loss: 6.547246966532743e-07\n",
      "Iteration: 8509 Loss: 6.540329539460504e-07\n",
      "Iteration: 8510 Loss: 6.533419421130064e-07\n",
      "Iteration: 8511 Loss: 6.526516603835439e-07\n",
      "Iteration: 8512 Loss: 6.519621079867139e-07\n",
      "Iteration: 8513 Loss: 6.512732841499385e-07\n",
      "Iteration: 8514 Loss: 6.505851881057408e-07\n",
      "Iteration: 8515 Loss: 6.498978190823744e-07\n",
      "Iteration: 8516 Loss: 6.492111763140808e-07\n",
      "Iteration: 8517 Loss: 6.48525259031055e-07\n",
      "Iteration: 8518 Loss: 6.478400664706829e-07\n",
      "Iteration: 8519 Loss: 6.471555978628174e-07\n",
      "Iteration: 8520 Loss: 6.464718524454561e-07\n",
      "Iteration: 8521 Loss: 6.457888294524609e-07\n",
      "Iteration: 8522 Loss: 6.451065281210157e-07\n",
      "Iteration: 8523 Loss: 6.444249476924463e-07\n",
      "Iteration: 8524 Loss: 6.437440873996671e-07\n",
      "Iteration: 8525 Loss: 6.430639464849848e-07\n",
      "Iteration: 8526 Loss: 6.423845241883509e-07\n",
      "Iteration: 8527 Loss: 6.417058197457712e-07\n",
      "Iteration: 8528 Loss: 6.410278324055799e-07\n",
      "Iteration: 8529 Loss: 6.403505614075303e-07\n",
      "Iteration: 8530 Loss: 6.396740059921739e-07\n",
      "Iteration: 8531 Loss: 6.389981654071586e-07\n",
      "Iteration: 8532 Loss: 6.383230388937335e-07\n",
      "Iteration: 8533 Loss: 6.376486256998601e-07\n",
      "Iteration: 8534 Loss: 6.369749250742887e-07\n",
      "Iteration: 8535 Loss: 6.3630193625795e-07\n",
      "Iteration: 8536 Loss: 6.356296585028402e-07\n",
      "Iteration: 8537 Loss: 6.349580910566033e-07\n",
      "Iteration: 8538 Loss: 6.342872331692742e-07\n",
      "Iteration: 8539 Loss: 6.336170840899426e-07\n",
      "Iteration: 8540 Loss: 6.329476430720228e-07\n",
      "Iteration: 8541 Loss: 6.322789093635691e-07\n",
      "Iteration: 8542 Loss: 6.316108822244244e-07\n",
      "Iteration: 8543 Loss: 6.309435609001716e-07\n",
      "Iteration: 8544 Loss: 6.302769446485581e-07\n",
      "Iteration: 8545 Loss: 6.296110327237598e-07\n",
      "Iteration: 8546 Loss: 6.289458243814504e-07\n",
      "Iteration: 8547 Loss: 6.282813188803226e-07\n",
      "Iteration: 8548 Loss: 6.276175154762985e-07\n",
      "Iteration: 8549 Loss: 6.269544134273128e-07\n",
      "Iteration: 8550 Loss: 6.262920119928021e-07\n",
      "Iteration: 8551 Loss: 6.256303106328325e-07\n",
      "Iteration: 8552 Loss: 6.24969308006698e-07\n",
      "Iteration: 8553 Loss: 6.243090039737304e-07\n",
      "Iteration: 8554 Loss: 6.236493976034582e-07\n",
      "Iteration: 8555 Loss: 6.229904881502636e-07\n",
      "Iteration: 8556 Loss: 6.223322748845517e-07\n",
      "Iteration: 8557 Loss: 6.216747570670432e-07\n",
      "Iteration: 8558 Loss: 6.210179339615497e-07\n",
      "Iteration: 8559 Loss: 6.203618049530281e-07\n",
      "Iteration: 8560 Loss: 6.197063690768259e-07\n",
      "Iteration: 8561 Loss: 6.190516256011608e-07\n",
      "Iteration: 8562 Loss: 6.183975741374019e-07\n",
      "Iteration: 8563 Loss: 6.177442134936268e-07\n",
      "Iteration: 8564 Loss: 6.170915432894772e-07\n",
      "Iteration: 8565 Loss: 6.164395626768818e-07\n",
      "Iteration: 8566 Loss: 6.15788270924807e-07\n",
      "Iteration: 8567 Loss: 6.151376673114581e-07\n",
      "Iteration: 8568 Loss: 6.144877511064554e-07\n",
      "Iteration: 8569 Loss: 6.138385215846631e-07\n",
      "Iteration: 8570 Loss: 6.131899780192557e-07\n",
      "Iteration: 8571 Loss: 6.12542119686352e-07\n",
      "Iteration: 8572 Loss: 6.118949458592454e-07\n",
      "Iteration: 8573 Loss: 6.112484558194018e-07\n",
      "Iteration: 8574 Loss: 6.106026489562127e-07\n",
      "Iteration: 8575 Loss: 6.099575243218924e-07\n",
      "Iteration: 8576 Loss: 6.093130813054819e-07\n",
      "Iteration: 8577 Loss: 6.086693191885138e-07\n",
      "Iteration: 8578 Loss: 6.080262371356412e-07\n",
      "Iteration: 8579 Loss: 6.073838346611772e-07\n",
      "Iteration: 8580 Loss: 6.067421109295799e-07\n",
      "Iteration: 8581 Loss: 6.06101065225753e-07\n",
      "Iteration: 8582 Loss: 6.054606968300304e-07\n",
      "Iteration: 8583 Loss: 6.048210050294471e-07\n",
      "Iteration: 8584 Loss: 6.041819891087938e-07\n",
      "Iteration: 8585 Loss: 6.035436483548864e-07\n",
      "Iteration: 8586 Loss: 6.029059820545619e-07\n",
      "Iteration: 8587 Loss: 6.022689894920281e-07\n",
      "Iteration: 8588 Loss: 6.016326699558853e-07\n",
      "Iteration: 8589 Loss: 6.009970228518998e-07\n",
      "Iteration: 8590 Loss: 6.003620471250467e-07\n",
      "Iteration: 8591 Loss: 5.997277424103969e-07\n",
      "Iteration: 8592 Loss: 5.990941078844695e-07\n",
      "Iteration: 8593 Loss: 5.984611428367524e-07\n",
      "Iteration: 8594 Loss: 5.978288465597716e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8595 Loss: 5.971972183529497e-07\n",
      "Iteration: 8596 Loss: 5.965662575023697e-07\n",
      "Iteration: 8597 Loss: 5.959359633069746e-07\n",
      "Iteration: 8598 Loss: 5.953063350628997e-07\n",
      "Iteration: 8599 Loss: 5.94677372066015e-07\n",
      "Iteration: 8600 Loss: 5.940490736126688e-07\n",
      "Iteration: 8601 Loss: 5.934214390012042e-07\n",
      "Iteration: 8602 Loss: 5.927944675305871e-07\n",
      "Iteration: 8603 Loss: 5.921681585007603e-07\n",
      "Iteration: 8604 Loss: 5.915425112106324e-07\n",
      "Iteration: 8605 Loss: 5.909175249579668e-07\n",
      "Iteration: 8606 Loss: 5.902931990524417e-07\n",
      "Iteration: 8607 Loss: 5.896695327882221e-07\n",
      "Iteration: 8608 Loss: 5.890465254713646e-07\n",
      "Iteration: 8609 Loss: 5.884241764072351e-07\n",
      "Iteration: 8610 Loss: 5.878024848970962e-07\n",
      "Iteration: 8611 Loss: 5.871814502506569e-07\n",
      "Iteration: 8612 Loss: 5.865610717691428e-07\n",
      "Iteration: 8613 Loss: 5.859413487632386e-07\n",
      "Iteration: 8614 Loss: 5.853222805389472e-07\n",
      "Iteration: 8615 Loss: 5.847038664015619e-07\n",
      "Iteration: 8616 Loss: 5.840861056662676e-07\n",
      "Iteration: 8617 Loss: 5.834689976361874e-07\n",
      "Iteration: 8618 Loss: 5.828525416226679e-07\n",
      "Iteration: 8619 Loss: 5.822367369426795e-07\n",
      "Iteration: 8620 Loss: 5.816215829021602e-07\n",
      "Iteration: 8621 Loss: 5.810070788158781e-07\n",
      "Iteration: 8622 Loss: 5.80393223995557e-07\n",
      "Iteration: 8623 Loss: 5.797800177586598e-07\n",
      "Iteration: 8624 Loss: 5.791674594145161e-07\n",
      "Iteration: 8625 Loss: 5.785555482831611e-07\n",
      "Iteration: 8626 Loss: 5.779442836812947e-07\n",
      "Iteration: 8627 Loss: 5.773336649191842e-07\n",
      "Iteration: 8628 Loss: 5.767236913250704e-07\n",
      "Iteration: 8629 Loss: 5.761143622084379e-07\n",
      "Iteration: 8630 Loss: 5.755056770896032e-07\n",
      "Iteration: 8631 Loss: 5.748976350945063e-07\n",
      "Iteration: 8632 Loss: 5.742902351328397e-07\n",
      "Iteration: 8633 Loss: 5.736834769345076e-07\n",
      "Iteration: 8634 Loss: 5.730773600180983e-07\n",
      "Iteration: 8635 Loss: 5.724718835091964e-07\n",
      "Iteration: 8636 Loss: 5.718670467273333e-07\n",
      "Iteration: 8637 Loss: 5.712628489991136e-07\n",
      "Iteration: 8638 Loss: 5.706592896509461e-07\n",
      "Iteration: 8639 Loss: 5.700563680040791e-07\n",
      "Iteration: 8640 Loss: 5.694540833873549e-07\n",
      "Iteration: 8641 Loss: 5.688524351254533e-07\n",
      "Iteration: 8642 Loss: 5.682514225487628e-07\n",
      "Iteration: 8643 Loss: 5.676510449827144e-07\n",
      "Iteration: 8644 Loss: 5.670513017608902e-07\n",
      "Iteration: 8645 Loss: 5.664521922084737e-07\n",
      "Iteration: 8646 Loss: 5.65853715658441e-07\n",
      "Iteration: 8647 Loss: 5.652558714417013e-07\n",
      "Iteration: 8648 Loss: 5.646586588872923e-07\n",
      "Iteration: 8649 Loss: 5.640620773326663e-07\n",
      "Iteration: 8650 Loss: 5.634661261064186e-07\n",
      "Iteration: 8651 Loss: 5.628708045488727e-07\n",
      "Iteration: 8652 Loss: 5.622761119878471e-07\n",
      "Iteration: 8653 Loss: 5.616820478773736e-07\n",
      "Iteration: 8654 Loss: 5.610886112088016e-07\n",
      "Iteration: 8655 Loss: 5.604958016609539e-07\n",
      "Iteration: 8656 Loss: 5.599036184600237e-07\n",
      "Iteration: 8657 Loss: 5.593120609426699e-07\n",
      "Iteration: 8658 Loss: 5.587211284457573e-07\n",
      "Iteration: 8659 Loss: 5.581308203135573e-07\n",
      "Iteration: 8660 Loss: 5.575411358823692e-07\n",
      "Iteration: 8661 Loss: 5.569520744944865e-07\n",
      "Iteration: 8662 Loss: 5.56363635490552e-07\n",
      "Iteration: 8663 Loss: 5.557758182180168e-07\n",
      "Iteration: 8664 Loss: 5.551886220130288e-07\n",
      "Iteration: 8665 Loss: 5.5460204622369e-07\n",
      "Iteration: 8666 Loss: 5.540160901931065e-07\n",
      "Iteration: 8667 Loss: 5.534307532658515e-07\n",
      "Iteration: 8668 Loss: 5.528460347871825e-07\n",
      "Iteration: 8669 Loss: 5.522619341068121e-07\n",
      "Iteration: 8670 Loss: 5.51678450685988e-07\n",
      "Iteration: 8671 Loss: 5.510955836392749e-07\n",
      "Iteration: 8672 Loss: 5.505133323201695e-07\n",
      "Iteration: 8673 Loss: 5.499316963045719e-07\n",
      "Iteration: 8674 Loss: 5.493506748284022e-07\n",
      "Iteration: 8675 Loss: 5.487702672435619e-07\n",
      "Iteration: 8676 Loss: 5.481904729017999e-07\n",
      "Iteration: 8677 Loss: 5.476112911522924e-07\n",
      "Iteration: 8678 Loss: 5.470327213480931e-07\n",
      "Iteration: 8679 Loss: 5.464547628452333e-07\n",
      "Iteration: 8680 Loss: 5.458774149963905e-07\n",
      "Iteration: 8681 Loss: 5.453006771570388e-07\n",
      "Iteration: 8682 Loss: 5.447245487964033e-07\n",
      "Iteration: 8683 Loss: 5.441490290428095e-07\n",
      "Iteration: 8684 Loss: 5.435741173628683e-07\n",
      "Iteration: 8685 Loss: 5.429998130064253e-07\n",
      "Iteration: 8686 Loss: 5.424261155579735e-07\n",
      "Iteration: 8687 Loss: 5.418530242606731e-07\n",
      "Iteration: 8688 Loss: 5.412805384733578e-07\n",
      "Iteration: 8689 Loss: 5.407086575602268e-07\n",
      "Iteration: 8690 Loss: 5.401373808780637e-07\n",
      "Iteration: 8691 Loss: 5.395667077918149e-07\n",
      "Iteration: 8692 Loss: 5.389966376607311e-07\n",
      "Iteration: 8693 Loss: 5.384271698503535e-07\n",
      "Iteration: 8694 Loss: 5.378583037219758e-07\n",
      "Iteration: 8695 Loss: 5.372900386413935e-07\n",
      "Iteration: 8696 Loss: 5.36722373973718e-07\n",
      "Iteration: 8697 Loss: 5.361553090832096e-07\n",
      "Iteration: 8698 Loss: 5.355888433371718e-07\n",
      "Iteration: 8699 Loss: 5.35022976102498e-07\n",
      "Iteration: 8700 Loss: 5.344577067451937e-07\n",
      "Iteration: 8701 Loss: 5.338930346359621e-07\n",
      "Iteration: 8702 Loss: 5.333289591439095e-07\n",
      "Iteration: 8703 Loss: 5.327654796369108e-07\n",
      "Iteration: 8704 Loss: 5.322025954856172e-07\n",
      "Iteration: 8705 Loss: 5.316403060625791e-07\n",
      "Iteration: 8706 Loss: 5.310786107358919e-07\n",
      "Iteration: 8707 Loss: 5.305175088833278e-07\n",
      "Iteration: 8708 Loss: 5.299569998704427e-07\n",
      "Iteration: 8709 Loss: 5.293970830773509e-07\n",
      "Iteration: 8710 Loss: 5.288377578768886e-07\n",
      "Iteration: 8711 Loss: 5.282790236421716e-07\n",
      "Iteration: 8712 Loss: 5.277208797493771e-07\n",
      "Iteration: 8713 Loss: 5.271633255756007e-07\n",
      "Iteration: 8714 Loss: 5.266063604976465e-07\n",
      "Iteration: 8715 Loss: 5.260499838926491e-07\n",
      "Iteration: 8716 Loss: 5.254941951392653e-07\n",
      "Iteration: 8717 Loss: 5.249389936152023e-07\n",
      "Iteration: 8718 Loss: 5.243843787019464e-07\n",
      "Iteration: 8719 Loss: 5.238303497786551e-07\n",
      "Iteration: 8720 Loss: 5.232769062257237e-07\n",
      "Iteration: 8721 Loss: 5.227240474257649e-07\n",
      "Iteration: 8722 Loss: 5.221717727580207e-07\n",
      "Iteration: 8723 Loss: 5.216200816097659e-07\n",
      "Iteration: 8724 Loss: 5.210689733624785e-07\n",
      "Iteration: 8725 Loss: 5.20518447398021e-07\n",
      "Iteration: 8726 Loss: 5.199685031042261e-07\n",
      "Iteration: 8727 Loss: 5.194191398660527e-07\n",
      "Iteration: 8728 Loss: 5.188703570696631e-07\n",
      "Iteration: 8729 Loss: 5.183221541005456e-07\n",
      "Iteration: 8730 Loss: 5.177745303464673e-07\n",
      "Iteration: 8731 Loss: 5.172274853102199e-07\n",
      "Iteration: 8732 Loss: 5.166810181511364e-07\n",
      "Iteration: 8733 Loss: 5.161351283764204e-07\n",
      "Iteration: 8734 Loss: 5.155898153687792e-07\n",
      "Iteration: 8735 Loss: 5.150450785248117e-07\n",
      "Iteration: 8736 Loss: 5.145009171182667e-07\n",
      "Iteration: 8737 Loss: 5.139573307721385e-07\n",
      "Iteration: 8738 Loss: 5.134143187632501e-07\n",
      "Iteration: 8739 Loss: 5.128718804841516e-07\n",
      "Iteration: 8740 Loss: 5.123300153316213e-07\n",
      "Iteration: 8741 Loss: 5.117887226968306e-07\n",
      "Iteration: 8742 Loss: 5.112480019744742e-07\n",
      "Iteration: 8743 Loss: 5.107078525653086e-07\n",
      "Iteration: 8744 Loss: 5.101682738615046e-07\n",
      "Iteration: 8745 Loss: 5.096292652592877e-07\n",
      "Iteration: 8746 Loss: 5.090908261569429e-07\n",
      "Iteration: 8747 Loss: 5.08552955955532e-07\n",
      "Iteration: 8748 Loss: 5.080156540529912e-07\n",
      "Iteration: 8749 Loss: 5.074789198464791e-07\n",
      "Iteration: 8750 Loss: 5.069427527379518e-07\n",
      "Iteration: 8751 Loss: 5.064071521265377e-07\n",
      "Iteration: 8752 Loss: 5.05872117418465e-07\n",
      "Iteration: 8753 Loss: 5.053376480105979e-07\n",
      "Iteration: 8754 Loss: 5.048037433069062e-07\n",
      "Iteration: 8755 Loss: 5.042704027127825e-07\n",
      "Iteration: 8756 Loss: 5.037376256293329e-07\n",
      "Iteration: 8757 Loss: 5.032054114628754e-07\n",
      "Iteration: 8758 Loss: 5.026737596193782e-07\n",
      "Iteration: 8759 Loss: 5.021426695034085e-07\n",
      "Iteration: 8760 Loss: 5.016121405209597e-07\n",
      "Iteration: 8761 Loss: 5.010821720819896e-07\n",
      "Iteration: 8762 Loss: 5.005527635893089e-07\n",
      "Iteration: 8763 Loss: 5.000239144554078e-07\n",
      "Iteration: 8764 Loss: 4.994956240902967e-07\n",
      "Iteration: 8765 Loss: 4.989678918971075e-07\n",
      "Iteration: 8766 Loss: 4.984407172934561e-07\n",
      "Iteration: 8767 Loss: 4.979140996871241e-07\n",
      "Iteration: 8768 Loss: 4.973880384891745e-07\n",
      "Iteration: 8769 Loss: 4.968625331114586e-07\n",
      "Iteration: 8770 Loss: 4.96337582968828e-07\n",
      "Iteration: 8771 Loss: 4.958131874703894e-07\n",
      "Iteration: 8772 Loss: 4.9528934603622e-07\n",
      "Iteration: 8773 Loss: 4.947660580761618e-07\n",
      "Iteration: 8774 Loss: 4.942433230063419e-07\n",
      "Iteration: 8775 Loss: 4.937211402449873e-07\n",
      "Iteration: 8776 Loss: 4.931995092054421e-07\n",
      "Iteration: 8777 Loss: 4.926784293062686e-07\n",
      "Iteration: 8778 Loss: 4.921578999656165e-07\n",
      "Iteration: 8779 Loss: 4.916379205988775e-07\n",
      "Iteration: 8780 Loss: 4.911184906282858e-07\n",
      "Iteration: 8781 Loss: 4.905996094724521e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8782 Loss: 4.90081276550931e-07\n",
      "Iteration: 8783 Loss: 4.895634912874952e-07\n",
      "Iteration: 8784 Loss: 4.890462530976213e-07\n",
      "Iteration: 8785 Loss: 4.885295614093281e-07\n",
      "Iteration: 8786 Loss: 4.880134156408038e-07\n",
      "Iteration: 8787 Loss: 4.874978152170897e-07\n",
      "Iteration: 8788 Loss: 4.869827595615808e-07\n",
      "Iteration: 8789 Loss: 4.864682480985281e-07\n",
      "Iteration: 8790 Loss: 4.859542802545202e-07\n",
      "Iteration: 8791 Loss: 4.854408554532179e-07\n",
      "Iteration: 8792 Loss: 4.84927973120202e-07\n",
      "Iteration: 8793 Loss: 4.844156326867786e-07\n",
      "Iteration: 8794 Loss: 4.839038335754096e-07\n",
      "Iteration: 8795 Loss: 4.833925752166045e-07\n",
      "Iteration: 8796 Loss: 4.828818570382291e-07\n",
      "Iteration: 8797 Loss: 4.82371678469173e-07\n",
      "Iteration: 8798 Loss: 4.818620389413594e-07\n",
      "Iteration: 8799 Loss: 4.813529378833729e-07\n",
      "Iteration: 8800 Loss: 4.80844374726613e-07\n",
      "Iteration: 8801 Loss: 4.803363489016147e-07\n",
      "Iteration: 8802 Loss: 4.798288598420523e-07\n",
      "Iteration: 8803 Loss: 4.793219069812852e-07\n",
      "Iteration: 8804 Loss: 4.788154897524702e-07\n",
      "Iteration: 8805 Loss: 4.783096075896137e-07\n",
      "Iteration: 8806 Loss: 4.778042599277197e-07\n",
      "Iteration: 8807 Loss: 4.772994461996168e-07\n",
      "Iteration: 8808 Loss: 4.7679516584449097e-07\n",
      "Iteration: 8809 Loss: 4.7629141829481394e-07\n",
      "Iteration: 8810 Loss: 4.757882029936085e-07\n",
      "Iteration: 8811 Loss: 4.7528551937332206e-07\n",
      "Iteration: 8812 Loss: 4.7478336687315857e-07\n",
      "Iteration: 8813 Loss: 4.742817449342653e-07\n",
      "Iteration: 8814 Loss: 4.7378065299331896e-07\n",
      "Iteration: 8815 Loss: 4.7328009049299787e-07\n",
      "Iteration: 8816 Loss: 4.7278005687157686e-07\n",
      "Iteration: 8817 Loss: 4.722805517710869e-07\n",
      "Iteration: 8818 Loss: 4.7178157423514455e-07\n",
      "Iteration: 8819 Loss: 4.7128312390235274e-07\n",
      "Iteration: 8820 Loss: 4.7078520021495536e-07\n",
      "Iteration: 8821 Loss: 4.702878024227172e-07\n",
      "Iteration: 8822 Loss: 4.697909303653743e-07\n",
      "Iteration: 8823 Loss: 4.6929458340353194e-07\n",
      "Iteration: 8824 Loss: 4.687987607528659e-07\n",
      "Iteration: 8825 Loss: 4.683034619719176e-07\n",
      "Iteration: 8826 Loss: 4.6780868651213915e-07\n",
      "Iteration: 8827 Loss: 4.673144337019529e-07\n",
      "Iteration: 8828 Loss: 4.6682070321959986e-07\n",
      "Iteration: 8829 Loss: 4.6632749439786263e-07\n",
      "Iteration: 8830 Loss: 4.658348066881612e-07\n",
      "Iteration: 8831 Loss: 4.653426395348392e-07\n",
      "Iteration: 8832 Loss: 4.6485099239318834e-07\n",
      "Iteration: 8833 Loss: 4.6435986471138395e-07\n",
      "Iteration: 8834 Loss: 4.6386925594079365e-07\n",
      "Iteration: 8835 Loss: 4.63379165532969e-07\n",
      "Iteration: 8836 Loss: 4.628895929399351e-07\n",
      "Iteration: 8837 Loss: 4.6240053761552153e-07\n",
      "Iteration: 8838 Loss: 4.6191199912706647e-07\n",
      "Iteration: 8839 Loss: 4.6142397670082467e-07\n",
      "Iteration: 8840 Loss: 4.609364699047437e-07\n",
      "Iteration: 8841 Loss: 4.6044947819221976e-07\n",
      "Iteration: 8842 Loss: 4.599630010218981e-07\n",
      "Iteration: 8843 Loss: 4.594770378497893e-07\n",
      "Iteration: 8844 Loss: 4.5899158813155454e-07\n",
      "Iteration: 8845 Loss: 4.585066513252812e-07\n",
      "Iteration: 8846 Loss: 4.5802222689068696e-07\n",
      "Iteration: 8847 Loss: 4.5753831416778116e-07\n",
      "Iteration: 8848 Loss: 4.5705491284800625e-07\n",
      "Iteration: 8849 Loss: 4.5657202227664974e-07\n",
      "Iteration: 8850 Loss: 4.5608964191394757e-07\n",
      "Iteration: 8851 Loss: 4.5560777121895156e-07\n",
      "Iteration: 8852 Loss: 4.551264096550269e-07\n",
      "Iteration: 8853 Loss: 4.546455566838795e-07\n",
      "Iteration: 8854 Loss: 4.541652117679052e-07\n",
      "Iteration: 8855 Loss: 4.536853743706666e-07\n",
      "Iteration: 8856 Loss: 4.5320604395436474e-07\n",
      "Iteration: 8857 Loss: 4.527272199864437e-07\n",
      "Iteration: 8858 Loss: 4.522489019292432e-07\n",
      "Iteration: 8859 Loss: 4.5177108924848607e-07\n",
      "Iteration: 8860 Loss: 4.512937814107737e-07\n",
      "Iteration: 8861 Loss: 4.5081697788354224e-07\n",
      "Iteration: 8862 Loss: 4.503406781322194e-07\n",
      "Iteration: 8863 Loss: 4.4986488162513906e-07\n",
      "Iteration: 8864 Loss: 4.493895878318469e-07\n",
      "Iteration: 8865 Loss: 4.489147962176727e-07\n",
      "Iteration: 8866 Loss: 4.48440506256654e-07\n",
      "Iteration: 8867 Loss: 4.4796671741675875e-07\n",
      "Iteration: 8868 Loss: 4.4749342916836875e-07\n",
      "Iteration: 8869 Loss: 4.4702064098100966e-07\n",
      "Iteration: 8870 Loss: 4.4654835232886387e-07\n",
      "Iteration: 8871 Loss: 4.4607656279827816e-07\n",
      "Iteration: 8872 Loss: 4.4560527163237206e-07\n",
      "Iteration: 8873 Loss: 4.4513447841782447e-07\n",
      "Iteration: 8874 Loss: 4.446641825154972e-07\n",
      "Iteration: 8875 Loss: 4.4419438362761057e-07\n",
      "Iteration: 8876 Loss: 4.4372508111775104e-07\n",
      "Iteration: 8877 Loss: 4.4325627445694097e-07\n",
      "Iteration: 8878 Loss: 4.4278796312497757e-07\n",
      "Iteration: 8879 Loss: 4.42320146596811e-07\n",
      "Iteration: 8880 Loss: 4.4185282435088385e-07\n",
      "Iteration: 8881 Loss: 4.413859959782446e-07\n",
      "Iteration: 8882 Loss: 4.4091966072828665e-07\n",
      "Iteration: 8883 Loss: 4.404538181942514e-07\n",
      "Iteration: 8884 Loss: 4.3998846785611664e-07\n",
      "Iteration: 8885 Loss: 4.395236091921849e-07\n",
      "Iteration: 8886 Loss: 4.3905924168607616e-07\n",
      "Iteration: 8887 Loss: 4.3859536470179617e-07\n",
      "Iteration: 8888 Loss: 4.3813197795056e-07\n",
      "Iteration: 8889 Loss: 4.37669080801493e-07\n",
      "Iteration: 8890 Loss: 4.3720667273587865e-07\n",
      "Iteration: 8891 Loss: 4.367447533519003e-07\n",
      "Iteration: 8892 Loss: 4.3628332190338024e-07\n",
      "Iteration: 8893 Loss: 4.358223779891068e-07\n",
      "Iteration: 8894 Loss: 4.3536192109579983e-07\n",
      "Iteration: 8895 Loss: 4.349019507079092e-07\n",
      "Iteration: 8896 Loss: 4.3444246619608327e-07\n",
      "Iteration: 8897 Loss: 4.3398346727731705e-07\n",
      "Iteration: 8898 Loss: 4.335249533226194e-07\n",
      "Iteration: 8899 Loss: 4.330669238203156e-07\n",
      "Iteration: 8900 Loss: 4.326093782586303e-07\n",
      "Iteration: 8901 Loss: 4.321523161275425e-07\n",
      "Iteration: 8902 Loss: 4.316957369143935e-07\n",
      "Iteration: 8903 Loss: 4.312396401092494e-07\n",
      "Iteration: 8904 Loss: 4.307840252030579e-07\n",
      "Iteration: 8905 Loss: 4.303288916863398e-07\n",
      "Iteration: 8906 Loss: 4.298742390505736e-07\n",
      "Iteration: 8907 Loss: 4.294200667872236e-07\n",
      "Iteration: 8908 Loss: 4.2896637438894936e-07\n",
      "Iteration: 8909 Loss: 4.28513161349025e-07\n",
      "Iteration: 8910 Loss: 4.280604271614879e-07\n",
      "Iteration: 8911 Loss: 4.276081713175874e-07\n",
      "Iteration: 8912 Loss: 4.271563933158866e-07\n",
      "Iteration: 8913 Loss: 4.267050926496407e-07\n",
      "Iteration: 8914 Loss: 4.2625426881341105e-07\n",
      "Iteration: 8915 Loss: 4.258039213061542e-07\n",
      "Iteration: 8916 Loss: 4.253540496214911e-07\n",
      "Iteration: 8917 Loss: 4.2490465325875093e-07\n",
      "Iteration: 8918 Loss: 4.2445573171525776e-07\n",
      "Iteration: 8919 Loss: 4.240072844887594e-07\n",
      "Iteration: 8920 Loss: 4.235593110787189e-07\n",
      "Iteration: 8921 Loss: 4.231118109838263e-07\n",
      "Iteration: 8922 Loss: 4.2266478370438703e-07\n",
      "Iteration: 8923 Loss: 4.2221822874191574e-07\n",
      "Iteration: 8924 Loss: 4.2177214559694355e-07\n",
      "Iteration: 8925 Loss: 4.213265337690217e-07\n",
      "Iteration: 8926 Loss: 4.2088139276257584e-07\n",
      "Iteration: 8927 Loss: 4.204367220785452e-07\n",
      "Iteration: 8928 Loss: 4.1999252122009164e-07\n",
      "Iteration: 8929 Loss: 4.195487896929404e-07\n",
      "Iteration: 8930 Loss: 4.191055269980366e-07\n",
      "Iteration: 8931 Loss: 4.1866273264069935e-07\n",
      "Iteration: 8932 Loss: 4.1822040612968634e-07\n",
      "Iteration: 8933 Loss: 4.1777854696737573e-07\n",
      "Iteration: 8934 Loss: 4.1733715465961383e-07\n",
      "Iteration: 8935 Loss: 4.168962287145132e-07\n",
      "Iteration: 8936 Loss: 4.1645576863862987e-07\n",
      "Iteration: 8937 Loss: 4.16015773939618e-07\n",
      "Iteration: 8938 Loss: 4.155762441276306e-07\n",
      "Iteration: 8939 Loss: 4.1513717870824144e-07\n",
      "Iteration: 8940 Loss: 4.1469857719341315e-07\n",
      "Iteration: 8941 Loss: 4.1426043909233925e-07\n",
      "Iteration: 8942 Loss: 4.138227639157511e-07\n",
      "Iteration: 8943 Loss: 4.1338555117446754e-07\n",
      "Iteration: 8944 Loss: 4.12948800693181e-07\n",
      "Iteration: 8945 Loss: 4.1251251135481457e-07\n",
      "Iteration: 8946 Loss: 4.120766827871266e-07\n",
      "Iteration: 8947 Loss: 4.116413147889465e-07\n",
      "Iteration: 8948 Loss: 4.112064069015322e-07\n",
      "Iteration: 8949 Loss: 4.1077195852545374e-07\n",
      "Iteration: 8950 Loss: 4.1033796917569284e-07\n",
      "Iteration: 8951 Loss: 4.099044383664031e-07\n",
      "Iteration: 8952 Loss: 4.094713656113474e-07\n",
      "Iteration: 8953 Loss: 4.0903875043032803e-07\n",
      "Iteration: 8954 Loss: 4.0860659233741915e-07\n",
      "Iteration: 8955 Loss: 4.0817489084936307e-07\n",
      "Iteration: 8956 Loss: 4.077436454857612e-07\n",
      "Iteration: 8957 Loss: 4.0731285576343746e-07\n",
      "Iteration: 8958 Loss: 4.068825211990865e-07\n",
      "Iteration: 8959 Loss: 4.0645264131677723e-07\n",
      "Iteration: 8960 Loss: 4.060232156296114e-07\n",
      "Iteration: 8961 Loss: 4.0559424366380766e-07\n",
      "Iteration: 8962 Loss: 4.051657249357524e-07\n",
      "Iteration: 8963 Loss: 4.047376589698157e-07\n",
      "Iteration: 8964 Loss: 4.0431004528450476e-07\n",
      "Iteration: 8965 Loss: 4.038828834033315e-07\n",
      "Iteration: 8966 Loss: 4.0345617285052063e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8967 Loss: 4.0302991314662953e-07\n",
      "Iteration: 8968 Loss: 4.0260410381601754e-07\n",
      "Iteration: 8969 Loss: 4.0217874438207127e-07\n",
      "Iteration: 8970 Loss: 4.0175383437166704e-07\n",
      "Iteration: 8971 Loss: 4.013293733081283e-07\n",
      "Iteration: 8972 Loss: 4.0090536072008993e-07\n",
      "Iteration: 8973 Loss: 4.0048179612897575e-07\n",
      "Iteration: 8974 Loss: 4.000586790652065e-07\n",
      "Iteration: 8975 Loss: 3.9963600905262205e-07\n",
      "Iteration: 8976 Loss: 3.9921378562302545e-07\n",
      "Iteration: 8977 Loss: 3.987920084165588e-07\n",
      "Iteration: 8978 Loss: 3.9837067673323937e-07\n",
      "Iteration: 8979 Loss: 3.979497902161444e-07\n",
      "Iteration: 8980 Loss: 3.975293483952651e-07\n",
      "Iteration: 8981 Loss: 3.971093508016695e-07\n",
      "Iteration: 8982 Loss: 3.9668979696562067e-07\n",
      "Iteration: 8983 Loss: 3.9627068630182954e-07\n",
      "Iteration: 8984 Loss: 3.9585201868809865e-07\n",
      "Iteration: 8985 Loss: 3.9543379331167476e-07\n",
      "Iteration: 8986 Loss: 3.950160098215217e-07\n",
      "Iteration: 8987 Loss: 3.9459866774799424e-07\n",
      "Iteration: 8988 Loss: 3.941817666234434e-07\n",
      "Iteration: 8989 Loss: 3.9376530587346077e-07\n",
      "Iteration: 8990 Loss: 3.933492853720736e-07\n",
      "Iteration: 8991 Loss: 3.929337043105484e-07\n",
      "Iteration: 8992 Loss: 3.9251856222569605e-07\n",
      "Iteration: 8993 Loss: 3.921038588813043e-07\n",
      "Iteration: 8994 Loss: 3.916895937019255e-07\n",
      "Iteration: 8995 Loss: 3.9127576622226196e-07\n",
      "Iteration: 8996 Loss: 3.908623759810597e-07\n",
      "Iteration: 8997 Loss: 3.904494225162385e-07\n",
      "Iteration: 8998 Loss: 3.9003690536422863e-07\n",
      "Iteration: 8999 Loss: 3.8962482406836085e-07\n",
      "Iteration: 9000 Loss: 3.892131781645344e-07\n",
      "Iteration: 9001 Loss: 3.8880196719306197e-07\n",
      "Iteration: 9002 Loss: 3.883911906951041e-07\n",
      "Iteration: 9003 Loss: 3.8798084821304415e-07\n",
      "Iteration: 9004 Loss: 3.87570939286178e-07\n",
      "Iteration: 9005 Loss: 3.87161463457997e-07\n",
      "Iteration: 9006 Loss: 3.867524202673774e-07\n",
      "Iteration: 9007 Loss: 3.8634380926275576e-07\n",
      "Iteration: 9008 Loss: 3.85935629982339e-07\n",
      "Iteration: 9009 Loss: 3.8552788197373793e-07\n",
      "Iteration: 9010 Loss: 3.8512056478004427e-07\n",
      "Iteration: 9011 Loss: 3.8471367794606556e-07\n",
      "Iteration: 9012 Loss: 3.843072210156568e-07\n",
      "Iteration: 9013 Loss: 3.839011935360173e-07\n",
      "Iteration: 9014 Loss: 3.8349559505373494e-07\n",
      "Iteration: 9015 Loss: 3.8309042511525146e-07\n",
      "Iteration: 9016 Loss: 3.82685683266554e-07\n",
      "Iteration: 9017 Loss: 3.8228136905787336e-07\n",
      "Iteration: 9018 Loss: 3.8187748183172976e-07\n",
      "Iteration: 9019 Loss: 3.814740217456564e-07\n",
      "Iteration: 9020 Loss: 3.810709877420063e-07\n",
      "Iteration: 9021 Loss: 3.806683795716996e-07\n",
      "Iteration: 9022 Loss: 3.802661967864014e-07\n",
      "Iteration: 9023 Loss: 3.798644390500917e-07\n",
      "Iteration: 9024 Loss: 3.7946310557067455e-07\n",
      "Iteration: 9025 Loss: 3.7906219624103083e-07\n",
      "Iteration: 9026 Loss: 3.7866171050330855e-07\n",
      "Iteration: 9027 Loss: 3.7826164790503394e-07\n",
      "Iteration: 9028 Loss: 3.77862008002073e-07\n",
      "Iteration: 9029 Loss: 3.7746279034738044e-07\n",
      "Iteration: 9030 Loss: 3.770639942926407e-07\n",
      "Iteration: 9031 Loss: 3.766656199965992e-07\n",
      "Iteration: 9032 Loss: 3.7626766641006704e-07\n",
      "Iteration: 9033 Loss: 3.7587013328962253e-07\n",
      "Iteration: 9034 Loss: 3.754730201913668e-07\n",
      "Iteration: 9035 Loss: 3.7507632667127484e-07\n",
      "Iteration: 9036 Loss: 3.746800522851249e-07\n",
      "Iteration: 9037 Loss: 3.742841965927525e-07\n",
      "Iteration: 9038 Loss: 3.7388875914830734e-07\n",
      "Iteration: 9039 Loss: 3.7349373951300503e-07\n",
      "Iteration: 9040 Loss: 3.730991372428255e-07\n",
      "Iteration: 9041 Loss: 3.7270495201380146e-07\n",
      "Iteration: 9042 Loss: 3.7231118303949353e-07\n",
      "Iteration: 9043 Loss: 3.7191783022415237e-07\n",
      "Iteration: 9044 Loss: 3.715248930147954e-07\n",
      "Iteration: 9045 Loss: 3.7113237097025034e-07\n",
      "Iteration: 9046 Loss: 3.707402636529899e-07\n",
      "Iteration: 9047 Loss: 3.703485706252053e-07\n",
      "Iteration: 9048 Loss: 3.699572914485923e-07\n",
      "Iteration: 9049 Loss: 3.6956642568779964e-07\n",
      "Iteration: 9050 Loss: 3.6917597290171975e-07\n",
      "Iteration: 9051 Loss: 3.687859326580029e-07\n",
      "Iteration: 9052 Loss: 3.6839630451883206e-07\n",
      "Iteration: 9053 Loss: 3.680070880507392e-07\n",
      "Iteration: 9054 Loss: 3.6761828281820796e-07\n",
      "Iteration: 9055 Loss: 3.672298883844665e-07\n",
      "Iteration: 9056 Loss: 3.668419043168354e-07\n",
      "Iteration: 9057 Loss: 3.6645433018235803e-07\n",
      "Iteration: 9058 Loss: 3.6606716554746406e-07\n",
      "Iteration: 9059 Loss: 3.656804099796406e-07\n",
      "Iteration: 9060 Loss: 3.652940630456747e-07\n",
      "Iteration: 9061 Loss: 3.6490812431491e-07\n",
      "Iteration: 9062 Loss: 3.6452259335633736e-07\n",
      "Iteration: 9063 Loss: 3.641374697368084e-07\n",
      "Iteration: 9064 Loss: 3.637527530306079e-07\n",
      "Iteration: 9065 Loss: 3.633684428031862e-07\n",
      "Iteration: 9066 Loss: 3.629845386268351e-07\n",
      "Iteration: 9067 Loss: 3.626010400714033e-07\n",
      "Iteration: 9068 Loss: 3.622179467107571e-07\n",
      "Iteration: 9069 Loss: 3.61835258116825e-07\n",
      "Iteration: 9070 Loss: 3.614529738569447e-07\n",
      "Iteration: 9071 Loss: 3.6107109350935324e-07\n",
      "Iteration: 9072 Loss: 3.606896166460252e-07\n",
      "Iteration: 9073 Loss: 3.603085428387079e-07\n",
      "Iteration: 9074 Loss: 3.599278716637432e-07\n",
      "Iteration: 9075 Loss: 3.5954760269504606e-07\n",
      "Iteration: 9076 Loss: 3.591677355062572e-07\n",
      "Iteration: 9077 Loss: 3.587882696749438e-07\n",
      "Iteration: 9078 Loss: 3.584092047768867e-07\n",
      "Iteration: 9079 Loss: 3.5803054038568733e-07\n",
      "Iteration: 9080 Loss: 3.5765227608229065e-07\n",
      "Iteration: 9081 Loss: 3.572744114409327e-07\n",
      "Iteration: 9082 Loss: 3.56896946040791e-07\n",
      "Iteration: 9083 Loss: 3.565198794592631e-07\n",
      "Iteration: 9084 Loss: 3.5614321139103796e-07\n",
      "Iteration: 9085 Loss: 3.557669410683741e-07\n",
      "Iteration: 9086 Loss: 3.5539106841751537e-07\n",
      "Iteration: 9087 Loss: 3.5501559290225755e-07\n",
      "Iteration: 9088 Loss: 3.546405141039503e-07\n",
      "Iteration: 9089 Loss: 3.5426583160280193e-07\n",
      "Iteration: 9090 Loss: 3.5389154498057403e-07\n",
      "Iteration: 9091 Loss: 3.5351765381840304e-07\n",
      "Iteration: 9092 Loss: 3.5314415769827987e-07\n",
      "Iteration: 9093 Loss: 3.527710562048503e-07\n",
      "Iteration: 9094 Loss: 3.5239834891818006e-07\n",
      "Iteration: 9095 Loss: 3.520260354223677e-07\n",
      "Iteration: 9096 Loss: 3.516541153042375e-07\n",
      "Iteration: 9097 Loss: 3.512825881449621e-07\n",
      "Iteration: 9098 Loss: 3.509114535309696e-07\n",
      "Iteration: 9099 Loss: 3.50540711047181e-07\n",
      "Iteration: 9100 Loss: 3.501703602784176e-07\n",
      "Iteration: 9101 Loss: 3.498004008117023e-07\n",
      "Iteration: 9102 Loss: 3.494308322327789e-07\n",
      "Iteration: 9103 Loss: 3.4906165412970266e-07\n",
      "Iteration: 9104 Loss: 3.486928660890463e-07\n",
      "Iteration: 9105 Loss: 3.4832446770060016e-07\n",
      "Iteration: 9106 Loss: 3.4795645854971905e-07\n",
      "Iteration: 9107 Loss: 3.4758883822754907e-07\n",
      "Iteration: 9108 Loss: 3.472216063210239e-07\n",
      "Iteration: 9109 Loss: 3.4685476242205786e-07\n",
      "Iteration: 9110 Loss: 3.464883061182828e-07\n",
      "Iteration: 9111 Loss: 3.461222370038162e-07\n",
      "Iteration: 9112 Loss: 3.457565546671475e-07\n",
      "Iteration: 9113 Loss: 3.453912586986351e-07\n",
      "Iteration: 9114 Loss: 3.450263486914043e-07\n",
      "Iteration: 9115 Loss: 3.4466182423705177e-07\n",
      "Iteration: 9116 Loss: 3.442976849299685e-07\n",
      "Iteration: 9117 Loss: 3.439339303604475e-07\n",
      "Iteration: 9118 Loss: 3.435705601221628e-07\n",
      "Iteration: 9119 Loss: 3.4320757381276194e-07\n",
      "Iteration: 9120 Loss: 3.4284497102482045e-07\n",
      "Iteration: 9121 Loss: 3.4248275134937405e-07\n",
      "Iteration: 9122 Loss: 3.421209143874902e-07\n",
      "Iteration: 9123 Loss: 3.4175945973023235e-07\n",
      "Iteration: 9124 Loss: 3.4139838697543647e-07\n",
      "Iteration: 9125 Loss: 3.410376957203454e-07\n",
      "Iteration: 9126 Loss: 3.406773855596287e-07\n",
      "Iteration: 9127 Loss: 3.403174560958629e-07\n",
      "Iteration: 9128 Loss: 3.3995790692016785e-07\n",
      "Iteration: 9129 Loss: 3.395987376330602e-07\n",
      "Iteration: 9130 Loss: 3.3923994783487154e-07\n",
      "Iteration: 9131 Loss: 3.388815371237738e-07\n",
      "Iteration: 9132 Loss: 3.38523505099006e-07\n",
      "Iteration: 9133 Loss: 3.3816585136092625e-07\n",
      "Iteration: 9134 Loss: 3.378085755077245e-07\n",
      "Iteration: 9135 Loss: 3.3745167714241376e-07\n",
      "Iteration: 9136 Loss: 3.3709515586643336e-07\n",
      "Iteration: 9137 Loss: 3.3673901128067304e-07\n",
      "Iteration: 9138 Loss: 3.3638324298526063e-07\n",
      "Iteration: 9139 Loss: 3.36027850584594e-07\n",
      "Iteration: 9140 Loss: 3.3567283368187046e-07\n",
      "Iteration: 9141 Loss: 3.35318191880696e-07\n",
      "Iteration: 9142 Loss: 3.349639247816414e-07\n",
      "Iteration: 9143 Loss: 3.346100319914436e-07\n",
      "Iteration: 9144 Loss: 3.342565131137318e-07\n",
      "Iteration: 9145 Loss: 3.339033677535271e-07\n",
      "Iteration: 9146 Loss: 3.3355059551687195e-07\n",
      "Iteration: 9147 Loss: 3.3319819601054123e-07\n",
      "Iteration: 9148 Loss: 3.328461688373215e-07\n",
      "Iteration: 9149 Loss: 3.324945136073367e-07\n",
      "Iteration: 9150 Loss: 3.321432299259199e-07\n",
      "Iteration: 9151 Loss: 3.317923173989249e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9152 Loss: 3.314417756374504e-07\n",
      "Iteration: 9153 Loss: 3.3109160436276857e-07\n",
      "Iteration: 9154 Loss: 3.307418028389189e-07\n",
      "Iteration: 9155 Loss: 3.3039237102062404e-07\n",
      "Iteration: 9156 Loss: 3.3004330840024656e-07\n",
      "Iteration: 9157 Loss: 3.2969461458953254e-07\n",
      "Iteration: 9158 Loss: 3.2934628919960787e-07\n",
      "Iteration: 9159 Loss: 3.2899833183974505e-07\n",
      "Iteration: 9160 Loss: 3.2865074212074255e-07\n",
      "Iteration: 9161 Loss: 3.283035196560424e-07\n",
      "Iteration: 9162 Loss: 3.2795666405656825e-07\n",
      "Iteration: 9163 Loss: 3.27610174934305e-07\n",
      "Iteration: 9164 Loss: 3.2726405190163577e-07\n",
      "Iteration: 9165 Loss: 3.269182946890486e-07\n",
      "Iteration: 9166 Loss: 3.2657290267691563e-07\n",
      "Iteration: 9167 Loss: 3.262278755971936e-07\n",
      "Iteration: 9168 Loss: 3.2588321306168376e-07\n",
      "Iteration: 9169 Loss: 3.2553891468743094e-07\n",
      "Iteration: 9170 Loss: 3.2519497997412783e-07\n",
      "Iteration: 9171 Loss: 3.2485140876755656e-07\n",
      "Iteration: 9172 Loss: 3.2450820056665077e-07\n",
      "Iteration: 9173 Loss: 3.241653549909407e-07\n",
      "Iteration: 9174 Loss: 3.2382287165535615e-07\n",
      "Iteration: 9175 Loss: 3.2348075017823564e-07\n",
      "Iteration: 9176 Loss: 3.231389901746936e-07\n",
      "Iteration: 9177 Loss: 3.2279759126670447e-07\n",
      "Iteration: 9178 Loss: 3.2245655307063745e-07\n",
      "Iteration: 9179 Loss: 3.221158752047689e-07\n",
      "Iteration: 9180 Loss: 3.217755572899444e-07\n",
      "Iteration: 9181 Loss: 3.214355989440985e-07\n",
      "Iteration: 9182 Loss: 3.210959997904301e-07\n",
      "Iteration: 9183 Loss: 3.207567594457148e-07\n",
      "Iteration: 9184 Loss: 3.204178775320453e-07\n",
      "Iteration: 9185 Loss: 3.200793536728425e-07\n",
      "Iteration: 9186 Loss: 3.197411874858926e-07\n",
      "Iteration: 9187 Loss: 3.1940337859669457e-07\n",
      "Iteration: 9188 Loss: 3.190659266272119e-07\n",
      "Iteration: 9189 Loss: 3.187288311981612e-07\n",
      "Iteration: 9190 Loss: 3.183920919365871e-07\n",
      "Iteration: 9191 Loss: 3.1805570846175765e-07\n",
      "Iteration: 9192 Loss: 3.1771968039969846e-07\n",
      "Iteration: 9193 Loss: 3.173840073755288e-07\n",
      "Iteration: 9194 Loss: 3.1704868901372734e-07\n",
      "Iteration: 9195 Loss: 3.167137249409745e-07\n",
      "Iteration: 9196 Loss: 3.1637911478065076e-07\n",
      "Iteration: 9197 Loss: 3.160448581589769e-07\n",
      "Iteration: 9198 Loss: 3.157109547046564e-07\n",
      "Iteration: 9199 Loss: 3.153774041579788e-07\n",
      "Iteration: 9200 Loss: 3.150442059136409e-07\n",
      "Iteration: 9201 Loss: 3.147113597184656e-07\n",
      "Iteration: 9202 Loss: 3.14378865198441e-07\n",
      "Iteration: 9203 Loss: 3.1404672198383777e-07\n",
      "Iteration: 9204 Loss: 3.137149297012899e-07\n",
      "Iteration: 9205 Loss: 3.13383487979383e-07\n",
      "Iteration: 9206 Loss: 3.130523963361575e-07\n",
      "Iteration: 9207 Loss: 3.12721654629016e-07\n",
      "Iteration: 9208 Loss: 3.1239126237410193e-07\n",
      "Iteration: 9209 Loss: 3.120612192015505e-07\n",
      "Iteration: 9210 Loss: 3.117315247446817e-07\n",
      "Iteration: 9211 Loss: 3.1140217862997775e-07\n",
      "Iteration: 9212 Loss: 3.1107318049617733e-07\n",
      "Iteration: 9213 Loss: 3.107445299686409e-07\n",
      "Iteration: 9214 Loss: 3.104162266857845e-07\n",
      "Iteration: 9215 Loss: 3.100882702800859e-07\n",
      "Iteration: 9216 Loss: 3.0976066038194616e-07\n",
      "Iteration: 9217 Loss: 3.094333966252781e-07\n",
      "Iteration: 9218 Loss: 3.0910647864871895e-07\n",
      "Iteration: 9219 Loss: 3.0877990608368036e-07\n",
      "Iteration: 9220 Loss: 3.084536785644536e-07\n",
      "Iteration: 9221 Loss: 3.081277957285998e-07\n",
      "Iteration: 9222 Loss: 3.078022572108327e-07\n",
      "Iteration: 9223 Loss: 3.0747706264795806e-07\n",
      "Iteration: 9224 Loss: 3.071522116754315e-07\n",
      "Iteration: 9225 Loss: 3.0682770393147343e-07\n",
      "Iteration: 9226 Loss: 3.065035390527299e-07\n",
      "Iteration: 9227 Loss: 3.061797167915806e-07\n",
      "Iteration: 9228 Loss: 3.058562364430164e-07\n",
      "Iteration: 9229 Loss: 3.0553309798907574e-07\n",
      "Iteration: 9230 Loss: 3.052103009517583e-07\n",
      "Iteration: 9231 Loss: 3.048878449748952e-07\n",
      "Iteration: 9232 Loss: 3.045657296938262e-07\n",
      "Iteration: 9233 Loss: 3.0424395475224506e-07\n",
      "Iteration: 9234 Loss: 3.039225197876604e-07\n",
      "Iteration: 9235 Loss: 3.0360142444124267e-07\n",
      "Iteration: 9236 Loss: 3.032806683555538e-07\n",
      "Iteration: 9237 Loss: 3.029602511712415e-07\n",
      "Iteration: 9238 Loss: 3.0264017253199276e-07\n",
      "Iteration: 9239 Loss: 3.0232043219183205e-07\n",
      "Iteration: 9240 Loss: 3.020010295650543e-07\n",
      "Iteration: 9241 Loss: 3.016819644103136e-07\n",
      "Iteration: 9242 Loss: 3.0136323637060294e-07\n",
      "Iteration: 9243 Loss: 3.010448449745105e-07\n",
      "Iteration: 9244 Loss: 3.007267900956841e-07\n",
      "Iteration: 9245 Loss: 3.0040907126473453e-07\n",
      "Iteration: 9246 Loss: 3.0009168812719446e-07\n",
      "Iteration: 9247 Loss: 2.9977464032435564e-07\n",
      "Iteration: 9248 Loss: 2.9945792750952897e-07\n",
      "Iteration: 9249 Loss: 2.991415493208539e-07\n",
      "Iteration: 9250 Loss: 2.9882550540925586e-07\n",
      "Iteration: 9251 Loss: 2.9850979542052225e-07\n",
      "Iteration: 9252 Loss: 2.9819441900286307e-07\n",
      "Iteration: 9253 Loss: 2.9787937580121283e-07\n",
      "Iteration: 9254 Loss: 2.975646654642862e-07\n",
      "Iteration: 9255 Loss: 2.9725028764238754e-07\n",
      "Iteration: 9256 Loss: 2.9693624198392526e-07\n",
      "Iteration: 9257 Loss: 2.9662252813552604e-07\n",
      "Iteration: 9258 Loss: 2.9630914574861657e-07\n",
      "Iteration: 9259 Loss: 2.9599609458702456e-07\n",
      "Iteration: 9260 Loss: 2.9568337407173005e-07\n",
      "Iteration: 9261 Loss: 2.9537098396783064e-07\n",
      "Iteration: 9262 Loss: 2.950589239251185e-07\n",
      "Iteration: 9263 Loss: 2.9474719348196196e-07\n",
      "Iteration: 9264 Loss: 2.94435792518583e-07\n",
      "Iteration: 9265 Loss: 2.9412472057278825e-07\n",
      "Iteration: 9266 Loss: 2.938139772956448e-07\n",
      "Iteration: 9267 Loss: 2.935035623436802e-07\n",
      "Iteration: 9268 Loss: 2.9319347536401474e-07\n",
      "Iteration: 9269 Loss: 2.9288371601249685e-07\n",
      "Iteration: 9270 Loss: 2.925742839442107e-07\n",
      "Iteration: 9271 Loss: 2.9226517881758647e-07\n",
      "Iteration: 9272 Loss: 2.919564002771758e-07\n",
      "Iteration: 9273 Loss: 2.91647947986371e-07\n",
      "Iteration: 9274 Loss: 2.9133982159455424e-07\n",
      "Iteration: 9275 Loss: 2.9103202076223357e-07\n",
      "Iteration: 9276 Loss: 2.90724545140748e-07\n",
      "Iteration: 9277 Loss: 2.904173943914099e-07\n",
      "Iteration: 9278 Loss: 2.9011056816788954e-07\n",
      "Iteration: 9279 Loss: 2.898040661268591e-07\n",
      "Iteration: 9280 Loss: 2.8949788804307354e-07\n",
      "Iteration: 9281 Loss: 2.8919203334352615e-07\n",
      "Iteration: 9282 Loss: 2.8888650168536623e-07\n",
      "Iteration: 9283 Loss: 2.885812929579958e-07\n",
      "Iteration: 9284 Loss: 2.8827640670608844e-07\n",
      "Iteration: 9285 Loss: 2.8797184258735823e-07\n",
      "Iteration: 9286 Loss: 2.8766760026331217e-07\n",
      "Iteration: 9287 Loss: 2.8736367939217467e-07\n",
      "Iteration: 9288 Loss: 2.870600796323658e-07\n",
      "Iteration: 9289 Loss: 2.867568006530163e-07\n",
      "Iteration: 9290 Loss: 2.8645384221993606e-07\n",
      "Iteration: 9291 Loss: 2.861512037704333e-07\n",
      "Iteration: 9292 Loss: 2.8584888508021086e-07\n",
      "Iteration: 9293 Loss: 2.8554688580934754e-07\n",
      "Iteration: 9294 Loss: 2.852452055084447e-07\n",
      "Iteration: 9295 Loss: 2.8494384406916927e-07\n",
      "Iteration: 9296 Loss: 2.846428010395819e-07\n",
      "Iteration: 9297 Loss: 2.843420760842028e-07\n",
      "Iteration: 9298 Loss: 2.840416688646813e-07\n",
      "Iteration: 9299 Loss: 2.837415790464991e-07\n",
      "Iteration: 9300 Loss: 2.834418062944957e-07\n",
      "Iteration: 9301 Loss: 2.8314235027351714e-07\n",
      "Iteration: 9302 Loss: 2.828432106501697e-07\n",
      "Iteration: 9303 Loss: 2.8254438720386783e-07\n",
      "Iteration: 9304 Loss: 2.822458793704067e-07\n",
      "Iteration: 9305 Loss: 2.819476869319417e-07\n",
      "Iteration: 9306 Loss: 2.8164980955466135e-07\n",
      "Iteration: 9307 Loss: 2.813522469058716e-07\n",
      "Iteration: 9308 Loss: 2.810549986530462e-07\n",
      "Iteration: 9309 Loss: 2.807580644637223e-07\n",
      "Iteration: 9310 Loss: 2.80461443891966e-07\n",
      "Iteration: 9311 Loss: 2.801651368370819e-07\n",
      "Iteration: 9312 Loss: 2.798691428494031e-07\n",
      "Iteration: 9313 Loss: 2.7957346160015076e-07\n",
      "Iteration: 9314 Loss: 2.792780927602225e-07\n",
      "Iteration: 9315 Loss: 2.789830359988518e-07\n",
      "Iteration: 9316 Loss: 2.786882909854195e-07\n",
      "Iteration: 9317 Loss: 2.783938573905164e-07\n",
      "Iteration: 9318 Loss: 2.780997348853211e-07\n",
      "Iteration: 9319 Loss: 2.778059231414942e-07\n",
      "Iteration: 9320 Loss: 2.775124218315569e-07\n",
      "Iteration: 9321 Loss: 2.772192306281944e-07\n",
      "Iteration: 9322 Loss: 2.7692634919929295e-07\n",
      "Iteration: 9323 Loss: 2.766337772216279e-07\n",
      "Iteration: 9324 Loss: 2.7634151436633363e-07\n",
      "Iteration: 9325 Loss: 2.760495603069253e-07\n",
      "Iteration: 9326 Loss: 2.7575791471763075e-07\n",
      "Iteration: 9327 Loss: 2.754665772732928e-07\n",
      "Iteration: 9328 Loss: 2.751755476479275e-07\n",
      "Iteration: 9329 Loss: 2.7488482551587257e-07\n",
      "Iteration: 9330 Loss: 2.7459441055183057e-07\n",
      "Iteration: 9331 Loss: 2.743043024318055e-07\n",
      "Iteration: 9332 Loss: 2.7401450083086793e-07\n",
      "Iteration: 9333 Loss: 2.7372500542824693e-07\n",
      "Iteration: 9334 Loss: 2.7343581589719927e-07\n",
      "Iteration: 9335 Loss: 2.731469319142217e-07\n",
      "Iteration: 9336 Loss: 2.728583531589801e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9337 Loss: 2.725700793079015e-07\n",
      "Iteration: 9338 Loss: 2.7228211003607045e-07\n",
      "Iteration: 9339 Loss: 2.7199444502584003e-07\n",
      "Iteration: 9340 Loss: 2.7170708395384464e-07\n",
      "Iteration: 9341 Loss: 2.7142002649901254e-07\n",
      "Iteration: 9342 Loss: 2.711332723420364e-07\n",
      "Iteration: 9343 Loss: 2.7084682115889764e-07\n",
      "Iteration: 9344 Loss: 2.705606726331996e-07\n",
      "Iteration: 9345 Loss: 2.7027482644344756e-07\n",
      "Iteration: 9346 Loss: 2.6998928227034024e-07\n",
      "Iteration: 9347 Loss: 2.697040397946314e-07\n",
      "Iteration: 9348 Loss: 2.694190986984647e-07\n",
      "Iteration: 9349 Loss: 2.691344586641021e-07\n",
      "Iteration: 9350 Loss: 2.6885011936888246e-07\n",
      "Iteration: 9351 Loss: 2.685660805006534e-07\n",
      "Iteration: 9352 Loss: 2.6828234185302214e-07\n",
      "Iteration: 9353 Loss: 2.679989028817131e-07\n",
      "Iteration: 9354 Loss: 2.67715763384148e-07\n",
      "Iteration: 9355 Loss: 2.6743292292609926e-07\n",
      "Iteration: 9356 Loss: 2.6715038142682166e-07\n",
      "Iteration: 9357 Loss: 2.668681384513926e-07\n",
      "Iteration: 9358 Loss: 2.66586193684358e-07\n",
      "Iteration: 9359 Loss: 2.6630454681393023e-07\n",
      "Iteration: 9360 Loss: 2.660231975229142e-07\n",
      "Iteration: 9361 Loss: 2.6574214549636617e-07\n",
      "Iteration: 9362 Loss: 2.654613904220905e-07\n",
      "Iteration: 9363 Loss: 2.651809321005521e-07\n",
      "Iteration: 9364 Loss: 2.6490076987293535e-07\n",
      "Iteration: 9365 Loss: 2.6462090377228514e-07\n",
      "Iteration: 9366 Loss: 2.643413333679403e-07\n",
      "Iteration: 9367 Loss: 2.640620583521625e-07\n",
      "Iteration: 9368 Loss: 2.6378307840807876e-07\n",
      "Iteration: 9369 Loss: 2.6350439322674177e-07\n",
      "Iteration: 9370 Loss: 2.6322600249562133e-07\n",
      "Iteration: 9371 Loss: 2.629479059056827e-07\n",
      "Iteration: 9372 Loss: 2.626701031434798e-07\n",
      "Iteration: 9373 Loss: 2.623925938980651e-07\n",
      "Iteration: 9374 Loss: 2.6211537786265026e-07\n",
      "Iteration: 9375 Loss: 2.6183845472681893e-07\n",
      "Iteration: 9376 Loss: 2.6156182417847457e-07\n",
      "Iteration: 9377 Loss: 2.612854859097189e-07\n",
      "Iteration: 9378 Loss: 2.610094396107189e-07\n",
      "Iteration: 9379 Loss: 2.6073368497603173e-07\n",
      "Iteration: 9380 Loss: 2.6045822169554327e-07\n",
      "Iteration: 9381 Loss: 2.601830494610229e-07\n",
      "Iteration: 9382 Loss: 2.599081679652319e-07\n",
      "Iteration: 9383 Loss: 2.5963357690140744e-07\n",
      "Iteration: 9384 Loss: 2.5935927596207885e-07\n",
      "Iteration: 9385 Loss: 2.5908526484155534e-07\n",
      "Iteration: 9386 Loss: 2.5881154323314117e-07\n",
      "Iteration: 9387 Loss: 2.58538110830918e-07\n",
      "Iteration: 9388 Loss: 2.582649673297351e-07\n",
      "Iteration: 9389 Loss: 2.5799211242394045e-07\n",
      "Iteration: 9390 Loss: 2.5771954580932574e-07\n",
      "Iteration: 9391 Loss: 2.5744726718054037e-07\n",
      "Iteration: 9392 Loss: 2.5717527623273463e-07\n",
      "Iteration: 9393 Loss: 2.569035726632412e-07\n",
      "Iteration: 9394 Loss: 2.566321561668327e-07\n",
      "Iteration: 9395 Loss: 2.563610264417769e-07\n",
      "Iteration: 9396 Loss: 2.5609018318599254e-07\n",
      "Iteration: 9397 Loss: 2.558196260930269e-07\n",
      "Iteration: 9398 Loss: 2.5554935497881144e-07\n",
      "Iteration: 9399 Loss: 2.5527936919573675e-07\n",
      "Iteration: 9400 Loss: 2.550096687858854e-07\n",
      "Iteration: 9401 Loss: 2.547402533330193e-07\n",
      "Iteration: 9402 Loss: 2.544711225373554e-07\n",
      "Iteration: 9403 Loss: 2.542022760974882e-07\n",
      "Iteration: 9404 Loss: 2.5393371371400644e-07\n",
      "Iteration: 9405 Loss: 2.536654350839659e-07\n",
      "Iteration: 9406 Loss: 2.533974399097723e-07\n",
      "Iteration: 9407 Loss: 2.5312972789060825e-07\n",
      "Iteration: 9408 Loss: 2.528622987292849e-07\n",
      "Iteration: 9409 Loss: 2.525951521250646e-07\n",
      "Iteration: 9410 Loss: 2.523282877823203e-07\n",
      "Iteration: 9411 Loss: 2.520617053985504e-07\n",
      "Iteration: 9412 Loss: 2.517954046779125e-07\n",
      "Iteration: 9413 Loss: 2.5152938532368583e-07\n",
      "Iteration: 9414 Loss: 2.5126364703764244e-07\n",
      "Iteration: 9415 Loss: 2.5099818952185923e-07\n",
      "Iteration: 9416 Loss: 2.5073301248373934e-07\n",
      "Iteration: 9417 Loss: 2.5046811562105825e-07\n",
      "Iteration: 9418 Loss: 2.502034986424309e-07\n",
      "Iteration: 9419 Loss: 2.499391612491712e-07\n",
      "Iteration: 9420 Loss: 2.4967510314664e-07\n",
      "Iteration: 9421 Loss: 2.494113240421996e-07\n",
      "Iteration: 9422 Loss: 2.491478236372687e-07\n",
      "Iteration: 9423 Loss: 2.488846016403579e-07\n",
      "Iteration: 9424 Loss: 2.486216577562959e-07\n",
      "Iteration: 9425 Loss: 2.4835899169050473e-07\n",
      "Iteration: 9426 Loss: 2.4809660315137947e-07\n",
      "Iteration: 9427 Loss: 2.478344918408922e-07\n",
      "Iteration: 9428 Loss: 2.4757265747271637e-07\n",
      "Iteration: 9429 Loss: 2.473110997499535e-07\n",
      "Iteration: 9430 Loss: 2.470498183807831e-07\n",
      "Iteration: 9431 Loss: 2.467888130760657e-07\n",
      "Iteration: 9432 Loss: 2.465280836549175e-07\n",
      "Iteration: 9433 Loss: 2.4626762959760236e-07\n",
      "Iteration: 9434 Loss: 2.460074507305459e-07\n",
      "Iteration: 9435 Loss: 2.4574754675948974e-07\n",
      "Iteration: 9436 Loss: 2.4548791739670224e-07\n",
      "Iteration: 9437 Loss: 2.4522856234976204e-07\n",
      "Iteration: 9438 Loss: 2.4496948132979e-07\n",
      "Iteration: 9439 Loss: 2.447106740472615e-07\n",
      "Iteration: 9440 Loss: 2.4445214021232107e-07\n",
      "Iteration: 9441 Loss: 2.441938795397583e-07\n",
      "Iteration: 9442 Loss: 2.439358917353128e-07\n",
      "Iteration: 9443 Loss: 2.4367817651240424e-07\n",
      "Iteration: 9444 Loss: 2.4342073358445544e-07\n",
      "Iteration: 9445 Loss: 2.4316356254800815e-07\n",
      "Iteration: 9446 Loss: 2.429066633461491e-07\n",
      "Iteration: 9447 Loss: 2.4265003557791924e-07\n",
      "Iteration: 9448 Loss: 2.423936789544925e-07\n",
      "Iteration: 9449 Loss: 2.421375931892147e-07\n",
      "Iteration: 9450 Loss: 2.418817781139095e-07\n",
      "Iteration: 9451 Loss: 2.4162623309427013e-07\n",
      "Iteration: 9452 Loss: 2.413709581896701e-07\n",
      "Iteration: 9453 Loss: 2.4111595300256864e-07\n",
      "Iteration: 9454 Loss: 2.408612172450458e-07\n",
      "Iteration: 9455 Loss: 2.4060675063578536e-07\n",
      "Iteration: 9456 Loss: 2.40352552886634e-07\n",
      "Iteration: 9457 Loss: 2.400986237151781e-07\n",
      "Iteration: 9458 Loss: 2.398449628393194e-07\n",
      "Iteration: 9459 Loss: 2.3959156997480106e-07\n",
      "Iteration: 9460 Loss: 2.3933844483709094e-07\n",
      "Iteration: 9461 Loss: 2.390855871425978e-07\n",
      "Iteration: 9462 Loss: 2.3883299661257364e-07\n",
      "Iteration: 9463 Loss: 2.3858067295915905e-07\n",
      "Iteration: 9464 Loss: 2.3832861590594575e-07\n",
      "Iteration: 9465 Loss: 2.3807682516853591e-07\n",
      "Iteration: 9466 Loss: 2.378253004675103e-07\n",
      "Iteration: 9467 Loss: 2.3757404151648416e-07\n",
      "Iteration: 9468 Loss: 2.3732304804052748e-07\n",
      "Iteration: 9469 Loss: 2.3707231975717396e-07\n",
      "Iteration: 9470 Loss: 2.3682185638316905e-07\n",
      "Iteration: 9471 Loss: 2.3657165764693204e-07\n",
      "Iteration: 9472 Loss: 2.3632172325753128e-07\n",
      "Iteration: 9473 Loss: 2.3607205294343784e-07\n",
      "Iteration: 9474 Loss: 2.3582264642761947e-07\n",
      "Iteration: 9475 Loss: 2.355735034239978e-07\n",
      "Iteration: 9476 Loss: 2.3532462365746404e-07\n",
      "Iteration: 9477 Loss: 2.3507600685007427e-07\n",
      "Iteration: 9478 Loss: 2.3482765272533885e-07\n",
      "Iteration: 9479 Loss: 2.345795610041623e-07\n",
      "Iteration: 9480 Loss: 2.3433173140823625e-07\n",
      "Iteration: 9481 Loss: 2.3408416366278004e-07\n",
      "Iteration: 9482 Loss: 2.3383685749015763e-07\n",
      "Iteration: 9483 Loss: 2.3358981261413258e-07\n",
      "Iteration: 9484 Loss: 2.3334302875819758e-07\n",
      "Iteration: 9485 Loss: 2.3309650564576086e-07\n",
      "Iteration: 9486 Loss: 2.328502430014307e-07\n",
      "Iteration: 9487 Loss: 2.3260424055527725e-07\n",
      "Iteration: 9488 Loss: 2.3235849802723531e-07\n",
      "Iteration: 9489 Loss: 2.3211301513986294e-07\n",
      "Iteration: 9490 Loss: 2.318677916265182e-07\n",
      "Iteration: 9491 Loss: 2.3162282720772648e-07\n",
      "Iteration: 9492 Loss: 2.313781216091051e-07\n",
      "Iteration: 9493 Loss: 2.3113367456402658e-07\n",
      "Iteration: 9494 Loss: 2.3088948578889452e-07\n",
      "Iteration: 9495 Loss: 2.3064555502177535e-07\n",
      "Iteration: 9496 Loss: 2.3040188198186356e-07\n",
      "Iteration: 9497 Loss: 2.3015846640173776e-07\n",
      "Iteration: 9498 Loss: 2.2991530800470943e-07\n",
      "Iteration: 9499 Loss: 2.2967240652280047e-07\n",
      "Iteration: 9500 Loss: 2.2942976179945669e-07\n",
      "Iteration: 9501 Loss: 2.2918737321799068e-07\n",
      "Iteration: 9502 Loss: 2.2894524084981266e-07\n",
      "Iteration: 9503 Loss: 2.287033643130751e-07\n",
      "Iteration: 9504 Loss: 2.2846174333669164e-07\n",
      "Iteration: 9505 Loss: 2.282203776489994e-07\n",
      "Iteration: 9506 Loss: 2.2797926709647623e-07\n",
      "Iteration: 9507 Loss: 2.2773841106495478e-07\n",
      "Iteration: 9508 Loss: 2.274978096280061e-07\n",
      "Iteration: 9509 Loss: 2.2725746240614305e-07\n",
      "Iteration: 9510 Loss: 2.2701736912770143e-07\n",
      "Iteration: 9511 Loss: 2.2677752952414984e-07\n",
      "Iteration: 9512 Loss: 2.2653794332798875e-07\n",
      "Iteration: 9513 Loss: 2.2629861027051352e-07\n",
      "Iteration: 9514 Loss: 2.260595300853643e-07\n",
      "Iteration: 9515 Loss: 2.2582070250724187e-07\n",
      "Iteration: 9516 Loss: 2.2558212726445771e-07\n",
      "Iteration: 9517 Loss: 2.253438040978377e-07\n",
      "Iteration: 9518 Loss: 2.2510573284920642e-07\n",
      "Iteration: 9519 Loss: 2.248679130258515e-07\n",
      "Iteration: 9520 Loss: 2.2463034447379383e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9521 Loss: 2.2439302693062287e-07\n",
      "Iteration: 9522 Loss: 2.241559601296658e-07\n",
      "Iteration: 9523 Loss: 2.2391914380848117e-07\n",
      "Iteration: 9524 Loss: 2.236825776999716e-07\n",
      "Iteration: 9525 Loss: 2.23446261540018e-07\n",
      "Iteration: 9526 Loss: 2.2321019495114017e-07\n",
      "Iteration: 9527 Loss: 2.229743778936353e-07\n",
      "Iteration: 9528 Loss: 2.2273880999878916e-07\n",
      "Iteration: 9529 Loss: 2.2250349099531792e-07\n",
      "Iteration: 9530 Loss: 2.222684206250087e-07\n",
      "Iteration: 9531 Loss: 2.2203359862092755e-07\n",
      "Iteration: 9532 Loss: 2.2179902472428275e-07\n",
      "Iteration: 9533 Loss: 2.2156469867243564e-07\n",
      "Iteration: 9534 Loss: 2.2133062020199733e-07\n",
      "Iteration: 9535 Loss: 2.2109678905102967e-07\n",
      "Iteration: 9536 Loss: 2.2086320496076767e-07\n",
      "Iteration: 9537 Loss: 2.2062986766611982e-07\n",
      "Iteration: 9538 Loss: 2.203967769105599e-07\n",
      "Iteration: 9539 Loss: 2.2016393243157766e-07\n",
      "Iteration: 9540 Loss: 2.1993133397061212e-07\n",
      "Iteration: 9541 Loss: 2.196989812639039e-07\n",
      "Iteration: 9542 Loss: 2.1946687405651166e-07\n",
      "Iteration: 9543 Loss: 2.1923501220144758e-07\n",
      "Iteration: 9544 Loss: 2.1900339520858927e-07\n",
      "Iteration: 9545 Loss: 2.1877202293764223e-07\n",
      "Iteration: 9546 Loss: 2.1854089512672551e-07\n",
      "Iteration: 9547 Loss: 2.1831001151777842e-07\n",
      "Iteration: 9548 Loss: 2.180793715379825e-07\n",
      "Iteration: 9549 Loss: 2.178489757629555e-07\n",
      "Iteration: 9550 Loss: 2.176188232162697e-07\n",
      "Iteration: 9551 Loss: 2.1738891384373613e-07\n",
      "Iteration: 9552 Loss: 2.1715924738617094e-07\n",
      "Iteration: 9553 Loss: 2.1692982358643206e-07\n",
      "Iteration: 9554 Loss: 2.1670064218869115e-07\n",
      "Iteration: 9555 Loss: 2.164717030542216e-07\n",
      "Iteration: 9556 Loss: 2.1624300569443597e-07\n",
      "Iteration: 9557 Loss: 2.1601454996819357e-07\n",
      "Iteration: 9558 Loss: 2.1578633562106022e-07\n",
      "Iteration: 9559 Loss: 2.1555836240257474e-07\n",
      "Iteration: 9560 Loss: 2.1533063005139652e-07\n",
      "Iteration: 9561 Loss: 2.1510313831640833e-07\n",
      "Iteration: 9562 Loss: 2.1487588682498273e-07\n",
      "Iteration: 9563 Loss: 2.1464887576073848e-07\n",
      "Iteration: 9564 Loss: 2.1442210414503408e-07\n",
      "Iteration: 9565 Loss: 2.141955723317907e-07\n",
      "Iteration: 9566 Loss: 2.1396927986459682e-07\n",
      "Iteration: 9567 Loss: 2.137432264931371e-07\n",
      "Iteration: 9568 Loss: 2.1351741196147872e-07\n",
      "Iteration: 9569 Loss: 2.1329183601909336e-07\n",
      "Iteration: 9570 Loss: 2.1306649841377826e-07\n",
      "Iteration: 9571 Loss: 2.1284139909712433e-07\n",
      "Iteration: 9572 Loss: 2.126165372059138e-07\n",
      "Iteration: 9573 Loss: 2.1239191310273235e-07\n",
      "Iteration: 9574 Loss: 2.1216752632987166e-07\n",
      "Iteration: 9575 Loss: 2.119433766379523e-07\n",
      "Iteration: 9576 Loss: 2.1171946377578774e-07\n",
      "Iteration: 9577 Loss: 2.1149578749198786e-07\n",
      "Iteration: 9578 Loss: 2.1127234754010402e-07\n",
      "Iteration: 9579 Loss: 2.110491436671996e-07\n",
      "Iteration: 9580 Loss: 2.108261756253222e-07\n",
      "Iteration: 9581 Loss: 2.1060344316526562e-07\n",
      "Iteration: 9582 Loss: 2.103809460372842e-07\n",
      "Iteration: 9583 Loss: 2.1015868399417725e-07\n",
      "Iteration: 9584 Loss: 2.099366567845251e-07\n",
      "Iteration: 9585 Loss: 2.0971486416591775e-07\n",
      "Iteration: 9586 Loss: 2.0949330588237919e-07\n",
      "Iteration: 9587 Loss: 2.0927198169372436e-07\n",
      "Iteration: 9588 Loss: 2.0905089135067038e-07\n",
      "Iteration: 9589 Loss: 2.0883003460384758e-07\n",
      "Iteration: 9590 Loss: 2.0860941120906565e-07\n",
      "Iteration: 9591 Loss: 2.083890209163396e-07\n",
      "Iteration: 9592 Loss: 2.0816886348348053e-07\n",
      "Iteration: 9593 Loss: 2.0794893866104565e-07\n",
      "Iteration: 9594 Loss: 2.077292462048815e-07\n",
      "Iteration: 9595 Loss: 2.0750978587024374e-07\n",
      "Iteration: 9596 Loss: 2.0729055741144004e-07\n",
      "Iteration: 9597 Loss: 2.0707156058115537e-07\n",
      "Iteration: 9598 Loss: 2.068527951379768e-07\n",
      "Iteration: 9599 Loss: 2.0663426083652732e-07\n",
      "Iteration: 9600 Loss: 2.0641595743120175e-07\n",
      "Iteration: 9601 Loss: 2.0619788467789088e-07\n",
      "Iteration: 9602 Loss: 2.0598004233523782e-07\n",
      "Iteration: 9603 Loss: 2.0576243015877263e-07\n",
      "Iteration: 9604 Loss: 2.055450479039403e-07\n",
      "Iteration: 9605 Loss: 2.0532789532920802e-07\n",
      "Iteration: 9606 Loss: 2.0511097219327317e-07\n",
      "Iteration: 9607 Loss: 2.0489427824934732e-07\n",
      "Iteration: 9608 Loss: 2.0467781325856156e-07\n",
      "Iteration: 9609 Loss: 2.0446157698029706e-07\n",
      "Iteration: 9610 Loss: 2.0424556916667143e-07\n",
      "Iteration: 9611 Loss: 2.0402978958475672e-07\n",
      "Iteration: 9612 Loss: 2.038142379882575e-07\n",
      "Iteration: 9613 Loss: 2.0359891425059676e-07\n",
      "Iteration: 9614 Loss: 2.0338381779008278e-07\n",
      "Iteration: 9615 Loss: 2.0316894870890375e-07\n",
      "Iteration: 9616 Loss: 2.0295430665067854e-07\n",
      "Iteration: 9617 Loss: 2.0273989137723756e-07\n",
      "Iteration: 9618 Loss: 2.025257026499589e-07\n",
      "Iteration: 9619 Loss: 2.023117402291335e-07\n",
      "Iteration: 9620 Loss: 2.0209800387092536e-07\n",
      "Iteration: 9621 Loss: 2.0188449334456239e-07\n",
      "Iteration: 9622 Loss: 2.0167120840422778e-07\n",
      "Iteration: 9623 Loss: 2.01458148816287e-07\n",
      "Iteration: 9624 Loss: 2.0124531454172492e-07\n",
      "Iteration: 9625 Loss: 2.0103270493980694e-07\n",
      "Iteration: 9626 Loss: 2.0082031997684777e-07\n",
      "Iteration: 9627 Loss: 2.0060815941212193e-07\n",
      "Iteration: 9628 Loss: 2.0039622280913606e-07\n",
      "Iteration: 9629 Loss: 2.0018451033261392e-07\n",
      "Iteration: 9630 Loss: 1.9997302134358873e-07\n",
      "Iteration: 9631 Loss: 1.997617562114294e-07\n",
      "Iteration: 9632 Loss: 1.9955071409464794e-07\n",
      "Iteration: 9633 Loss: 1.9933989516081273e-07\n",
      "Iteration: 9634 Loss: 1.9912929876972447e-07\n",
      "Iteration: 9635 Loss: 1.9891892488901468e-07\n",
      "Iteration: 9636 Loss: 1.987087732826377e-07\n",
      "Iteration: 9637 Loss: 1.9849884391959524e-07\n",
      "Iteration: 9638 Loss: 1.9828913595722158e-07\n",
      "Iteration: 9639 Loss: 1.9807964976764535e-07\n",
      "Iteration: 9640 Loss: 1.9787038471430498e-07\n",
      "Iteration: 9641 Loss: 1.9766134096561137e-07\n",
      "Iteration: 9642 Loss: 1.9745251808559097e-07\n",
      "Iteration: 9643 Loss: 1.9724391584342443e-07\n",
      "Iteration: 9644 Loss: 1.970355340019231e-07\n",
      "Iteration: 9645 Loss: 1.9682737233279434e-07\n",
      "Iteration: 9646 Loss: 1.9661943059988244e-07\n",
      "Iteration: 9647 Loss: 1.964117085731064e-07\n",
      "Iteration: 9648 Loss: 1.9620420601943518e-07\n",
      "Iteration: 9649 Loss: 1.9599692270551925e-07\n",
      "Iteration: 9650 Loss: 1.9578985840086297e-07\n",
      "Iteration: 9651 Loss: 1.9558301287490512e-07\n",
      "Iteration: 9652 Loss: 1.9537638589764725e-07\n",
      "Iteration: 9653 Loss: 1.9516997723619955e-07\n",
      "Iteration: 9654 Loss: 1.949637866583184e-07\n",
      "Iteration: 9655 Loss: 1.9475781393563017e-07\n",
      "Iteration: 9656 Loss: 1.9455205883731029e-07\n",
      "Iteration: 9657 Loss: 1.943465211365149e-07\n",
      "Iteration: 9658 Loss: 1.9414120059902914e-07\n",
      "Iteration: 9659 Loss: 1.9393609699543098e-07\n",
      "Iteration: 9660 Loss: 1.9373121010237007e-07\n",
      "Iteration: 9661 Loss: 1.935265396840886e-07\n",
      "Iteration: 9662 Loss: 1.9332208551462144e-07\n",
      "Iteration: 9663 Loss: 1.931178473659498e-07\n",
      "Iteration: 9664 Loss: 1.9291382500964151e-07\n",
      "Iteration: 9665 Loss: 1.9271001821699223e-07\n",
      "Iteration: 9666 Loss: 1.925064267604239e-07\n",
      "Iteration: 9667 Loss: 1.923030504141165e-07\n",
      "Iteration: 9668 Loss: 1.9209988894724806e-07\n",
      "Iteration: 9669 Loss: 1.918969421369564e-07\n",
      "Iteration: 9670 Loss: 1.9169420975310376e-07\n",
      "Iteration: 9671 Loss: 1.9149169157111342e-07\n",
      "Iteration: 9672 Loss: 1.91289387363243e-07\n",
      "Iteration: 9673 Loss: 1.910872969029636e-07\n",
      "Iteration: 9674 Loss: 1.9088541996866922e-07\n",
      "Iteration: 9675 Loss: 1.906837563288129e-07\n",
      "Iteration: 9676 Loss: 1.9048230576093977e-07\n",
      "Iteration: 9677 Loss: 1.902810680424664e-07\n",
      "Iteration: 9678 Loss: 1.9008004294526056e-07\n",
      "Iteration: 9679 Loss: 1.8987923024220197e-07\n",
      "Iteration: 9680 Loss: 1.8967862971566218e-07\n",
      "Iteration: 9681 Loss: 1.8947824113714374e-07\n",
      "Iteration: 9682 Loss: 1.8927806428164566e-07\n",
      "Iteration: 9683 Loss: 1.8907809892859876e-07\n",
      "Iteration: 9684 Loss: 1.8887834485314724e-07\n",
      "Iteration: 9685 Loss: 1.8867880183041872e-07\n",
      "Iteration: 9686 Loss: 1.884794696396975e-07\n",
      "Iteration: 9687 Loss: 1.8828034805961985e-07\n",
      "Iteration: 9688 Loss: 1.8808143686488146e-07\n",
      "Iteration: 9689 Loss: 1.8788273583254546e-07\n",
      "Iteration: 9690 Loss: 1.8768424474165357e-07\n",
      "Iteration: 9691 Loss: 1.874859633721926e-07\n",
      "Iteration: 9692 Loss: 1.8728789149982478e-07\n",
      "Iteration: 9693 Loss: 1.8709002890516255e-07\n",
      "Iteration: 9694 Loss: 1.8689237536676766e-07\n",
      "Iteration: 9695 Loss: 1.8669493066213164e-07\n",
      "Iteration: 9696 Loss: 1.8649769457220578e-07\n",
      "Iteration: 9697 Loss: 1.863006668761194e-07\n",
      "Iteration: 9698 Loss: 1.8610384735365667e-07\n",
      "Iteration: 9699 Loss: 1.8590723578402146e-07\n",
      "Iteration: 9700 Loss: 1.8571083195191242e-07\n",
      "Iteration: 9701 Loss: 1.8551463563183833e-07\n",
      "Iteration: 9702 Loss: 1.8531864660836459e-07\n",
      "Iteration: 9703 Loss: 1.8512286466015018e-07\n",
      "Iteration: 9704 Loss: 1.84927289569669e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9705 Loss: 1.8473192111653256e-07\n",
      "Iteration: 9706 Loss: 1.845367590856245e-07\n",
      "Iteration: 9707 Loss: 1.8434180325693752e-07\n",
      "Iteration: 9708 Loss: 1.84147053414085e-07\n",
      "Iteration: 9709 Loss: 1.8395250933615903e-07\n",
      "Iteration: 9710 Loss: 1.837581708092484e-07\n",
      "Iteration: 9711 Loss: 1.8356403761494694e-07\n",
      "Iteration: 9712 Loss: 1.8337010953573847e-07\n",
      "Iteration: 9713 Loss: 1.8317638635559568e-07\n",
      "Iteration: 9714 Loss: 1.8298286785645504e-07\n",
      "Iteration: 9715 Loss: 1.827895538234853e-07\n",
      "Iteration: 9716 Loss: 1.8259644404257042e-07\n",
      "Iteration: 9717 Loss: 1.8240353829357012e-07\n",
      "Iteration: 9718 Loss: 1.822108363630032e-07\n",
      "Iteration: 9719 Loss: 1.820183380361305e-07\n",
      "Iteration: 9720 Loss: 1.8182604309916548e-07\n",
      "Iteration: 9721 Loss: 1.8163395133420077e-07\n",
      "Iteration: 9722 Loss: 1.8144206252671138e-07\n",
      "Iteration: 9723 Loss: 1.812503764649181e-07\n",
      "Iteration: 9724 Loss: 1.810588929333894e-07\n",
      "Iteration: 9725 Loss: 1.8086761171689964e-07\n",
      "Iteration: 9726 Loss: 1.8067653260297122e-07\n",
      "Iteration: 9727 Loss: 1.8048565537576884e-07\n",
      "Iteration: 9728 Loss: 1.802949798262839e-07\n",
      "Iteration: 9729 Loss: 1.801045057378635e-07\n",
      "Iteration: 9730 Loss: 1.7991423310021533e-07\n",
      "Iteration: 9731 Loss: 1.797241612986025e-07\n",
      "Iteration: 9732 Loss: 1.7953429011758013e-07\n",
      "Iteration: 9733 Loss: 1.793446197530004e-07\n",
      "Iteration: 9734 Loss: 1.79155149788556e-07\n",
      "Iteration: 9735 Loss: 1.789658800090563e-07\n",
      "Iteration: 9736 Loss: 1.7877681020910614e-07\n",
      "Iteration: 9737 Loss: 1.785879401757145e-07\n",
      "Iteration: 9738 Loss: 1.7839926981191264e-07\n",
      "Iteration: 9739 Loss: 1.7821079867567195e-07\n",
      "Iteration: 9740 Loss: 1.7802252655794132e-07\n",
      "Iteration: 9741 Loss: 1.7783445347750964e-07\n",
      "Iteration: 9742 Loss: 1.7764657910979449e-07\n",
      "Iteration: 9743 Loss: 1.7745890324381075e-07\n",
      "Iteration: 9744 Loss: 1.7727142567239575e-07\n",
      "Iteration: 9745 Loss: 1.7708414618335878e-07\n",
      "Iteration: 9746 Loss: 1.768970645678983e-07\n",
      "Iteration: 9747 Loss: 1.76710180618278e-07\n",
      "Iteration: 9748 Loss: 1.7652349412503666e-07\n",
      "Iteration: 9749 Loss: 1.7633700487767798e-07\n",
      "Iteration: 9750 Loss: 1.7615071266987658e-07\n",
      "Iteration: 9751 Loss: 1.7596461729553587e-07\n",
      "Iteration: 9752 Loss: 1.7577871854232193e-07\n",
      "Iteration: 9753 Loss: 1.7559301620350542e-07\n",
      "Iteration: 9754 Loss: 1.7540751007425485e-07\n",
      "Iteration: 9755 Loss: 1.75222199944636e-07\n",
      "Iteration: 9756 Loss: 1.750370856067634e-07\n",
      "Iteration: 9757 Loss: 1.748521668566193e-07\n",
      "Iteration: 9758 Loss: 1.7466744348731767e-07\n",
      "Iteration: 9759 Loss: 1.7448291529000534e-07\n",
      "Iteration: 9760 Loss: 1.7429858205938681e-07\n",
      "Iteration: 9761 Loss: 1.7411444358986804e-07\n",
      "Iteration: 9762 Loss: 1.7393049967610634e-07\n",
      "Iteration: 9763 Loss: 1.737467501126501e-07\n",
      "Iteration: 9764 Loss: 1.7356319469271455e-07\n",
      "Iteration: 9765 Loss: 1.7337983321287083e-07\n",
      "Iteration: 9766 Loss: 1.731966654673684e-07\n",
      "Iteration: 9767 Loss: 1.7301369125110597e-07\n",
      "Iteration: 9768 Loss: 1.7283091036144295e-07\n",
      "Iteration: 9769 Loss: 1.7264832258984103e-07\n",
      "Iteration: 9770 Loss: 1.7246592773786989e-07\n",
      "Iteration: 9771 Loss: 1.7228372559855958e-07\n",
      "Iteration: 9772 Loss: 1.721017159675523e-07\n",
      "Iteration: 9773 Loss: 1.7191989864205502e-07\n",
      "Iteration: 9774 Loss: 1.717382734228609e-07\n",
      "Iteration: 9775 Loss: 1.7155684021548273e-07\n",
      "Iteration: 9776 Loss: 1.7137559859013678e-07\n",
      "Iteration: 9777 Loss: 1.7119454834621804e-07\n",
      "Iteration: 9778 Loss: 1.710136895092822e-07\n",
      "Iteration: 9779 Loss: 1.7083302176315292e-07\n",
      "Iteration: 9780 Loss: 1.706525449032574e-07\n",
      "Iteration: 9781 Loss: 1.704722587311254e-07\n",
      "Iteration: 9782 Loss: 1.702921630433469e-07\n",
      "Iteration: 9783 Loss: 1.7011225775345967e-07\n",
      "Iteration: 9784 Loss: 1.6993254243404353e-07\n",
      "Iteration: 9785 Loss: 1.6975301699671585e-07\n",
      "Iteration: 9786 Loss: 1.6957368132569813e-07\n",
      "Iteration: 9787 Loss: 1.6939453516534236e-07\n",
      "Iteration: 9788 Loss: 1.6921557796621196e-07\n",
      "Iteration: 9789 Loss: 1.69036810051697e-07\n",
      "Iteration: 9790 Loss: 1.6885823101800997e-07\n",
      "Iteration: 9791 Loss: 1.6867984055113202e-07\n",
      "Iteration: 9792 Loss: 1.6850163868174905e-07\n",
      "Iteration: 9793 Loss: 1.6832362509217278e-07\n",
      "Iteration: 9794 Loss: 1.6814579959043894e-07\n",
      "Iteration: 9795 Loss: 1.6796816197199692e-07\n",
      "Iteration: 9796 Loss: 1.6779071204060518e-07\n",
      "Iteration: 9797 Loss: 1.67613449712965e-07\n",
      "Iteration: 9798 Loss: 1.674363744434437e-07\n",
      "Iteration: 9799 Loss: 1.6725948638489454e-07\n",
      "Iteration: 9800 Loss: 1.670827852198674e-07\n",
      "Iteration: 9801 Loss: 1.6690627075043093e-07\n",
      "Iteration: 9802 Loss: 1.6672994278035754e-07\n",
      "Iteration: 9803 Loss: 1.6655380111502537e-07\n",
      "Iteration: 9804 Loss: 1.6637784555838866e-07\n",
      "Iteration: 9805 Loss: 1.662020759085731e-07\n",
      "Iteration: 9806 Loss: 1.6602649197172174e-07\n",
      "Iteration: 9807 Loss: 1.6585109355235044e-07\n",
      "Iteration: 9808 Loss: 1.6567588045225706e-07\n",
      "Iteration: 9809 Loss: 1.6550085247962376e-07\n",
      "Iteration: 9810 Loss: 1.6532600943578052e-07\n",
      "Iteration: 9811 Loss: 1.6515135112380993e-07\n",
      "Iteration: 9812 Loss: 1.649768773545852e-07\n",
      "Iteration: 9813 Loss: 1.6480258792652205e-07\n",
      "Iteration: 9814 Loss: 1.6462848264782124e-07\n",
      "Iteration: 9815 Loss: 1.6445456132375745e-07\n",
      "Iteration: 9816 Loss: 1.642808237611347e-07\n",
      "Iteration: 9817 Loss: 1.64107269763584e-07\n",
      "Iteration: 9818 Loss: 1.6393389913877014e-07\n",
      "Iteration: 9819 Loss: 1.6376071169204123e-07\n",
      "Iteration: 9820 Loss: 1.6358770722880291e-07\n",
      "Iteration: 9821 Loss: 1.634148855592509e-07\n",
      "Iteration: 9822 Loss: 1.6324224648780426e-07\n",
      "Iteration: 9823 Loss: 1.6306978982048163e-07\n",
      "Iteration: 9824 Loss: 1.6289751536670245e-07\n",
      "Iteration: 9825 Loss: 1.6272542293198153e-07\n",
      "Iteration: 9826 Loss: 1.6255351232661785e-07\n",
      "Iteration: 9827 Loss: 1.623817833548185e-07\n",
      "Iteration: 9828 Loss: 1.6221023594465622e-07\n",
      "Iteration: 9829 Loss: 1.6203886966835274e-07\n",
      "Iteration: 9830 Loss: 1.6186768445483644e-07\n",
      "Iteration: 9831 Loss: 1.6169668011066456e-07\n",
      "Iteration: 9832 Loss: 1.6152585644356984e-07\n",
      "Iteration: 9833 Loss: 1.6135521326564587e-07\n",
      "Iteration: 9834 Loss: 1.6118475038115627e-07\n",
      "Iteration: 9835 Loss: 1.6101446760797327e-07\n",
      "Iteration: 9836 Loss: 1.6084436463174992e-07\n",
      "Iteration: 9837 Loss: 1.6067444149718058e-07\n",
      "Iteration: 9838 Loss: 1.6050469789848105e-07\n",
      "Iteration: 9839 Loss: 1.6033513364479864e-07\n",
      "Iteration: 9840 Loss: 1.601657485474504e-07\n",
      "Iteration: 9841 Loss: 1.599965424203765e-07\n",
      "Iteration: 9842 Loss: 1.5982751506999235e-07\n",
      "Iteration: 9843 Loss: 1.5965866630649434e-07\n",
      "Iteration: 9844 Loss: 1.594899959473139e-07\n",
      "Iteration: 9845 Loss: 1.5932150379928731e-07\n",
      "Iteration: 9846 Loss: 1.5915318967494136e-07\n",
      "Iteration: 9847 Loss: 1.5898505338649242e-07\n",
      "Iteration: 9848 Loss: 1.5881709474547625e-07\n",
      "Iteration: 9849 Loss: 1.586493136804983e-07\n",
      "Iteration: 9850 Loss: 1.5848170977222388e-07\n",
      "Iteration: 9851 Loss: 1.5831428295072922e-07\n",
      "Iteration: 9852 Loss: 1.5814703302698415e-07\n",
      "Iteration: 9853 Loss: 1.579799598130505e-07\n",
      "Iteration: 9854 Loss: 1.5781306301088244e-07\n",
      "Iteration: 9855 Loss: 1.576463426617595e-07\n",
      "Iteration: 9856 Loss: 1.5747979846374145e-07\n",
      "Iteration: 9857 Loss: 1.5731343023133947e-07\n",
      "Iteration: 9858 Loss: 1.571472377789869e-07\n",
      "Iteration: 9859 Loss: 1.5698122092153315e-07\n",
      "Iteration: 9860 Loss: 1.5681537947186577e-07\n",
      "Iteration: 9861 Loss: 1.5664971324536028e-07\n",
      "Iteration: 9862 Loss: 1.5648422205677466e-07\n",
      "Iteration: 9863 Loss: 1.5631890560561764e-07\n",
      "Iteration: 9864 Loss: 1.5615376394036584e-07\n",
      "Iteration: 9865 Loss: 1.5598879675694187e-07\n",
      "Iteration: 9866 Loss: 1.5582400387434334e-07\n",
      "Iteration: 9867 Loss: 1.556593851063839e-07\n",
      "Iteration: 9868 Loss: 1.5549494038399532e-07\n",
      "Iteration: 9869 Loss: 1.553306692947202e-07\n",
      "Iteration: 9870 Loss: 1.5516657176891463e-07\n",
      "Iteration: 9871 Loss: 1.550026476255702e-07\n",
      "Iteration: 9872 Loss: 1.548388966791728e-07\n",
      "Iteration: 9873 Loss: 1.5467531874693273e-07\n",
      "Iteration: 9874 Loss: 1.5451191364498354e-07\n",
      "Iteration: 9875 Loss: 1.5434868119483535e-07\n",
      "Iteration: 9876 Loss: 1.5418562120919737e-07\n",
      "Iteration: 9877 Loss: 1.5402273350966082e-07\n",
      "Iteration: 9878 Loss: 1.538600180271069e-07\n",
      "Iteration: 9879 Loss: 1.5369747434872267e-07\n",
      "Iteration: 9880 Loss: 1.535351024108735e-07\n",
      "Iteration: 9881 Loss: 1.533729020288904e-07\n",
      "Iteration: 9882 Loss: 1.5321087290999968e-07\n",
      "Iteration: 9883 Loss: 1.5304901509821936e-07\n",
      "Iteration: 9884 Loss: 1.5288732830472302e-07\n",
      "Iteration: 9885 Loss: 1.527258123422233e-07\n",
      "Iteration: 9886 Loss: 1.5256446703192651e-07\n",
      "Iteration: 9887 Loss: 1.5240329219629374e-07\n",
      "Iteration: 9888 Loss: 1.5224228765339177e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9889 Loss: 1.5208145322328233e-07\n",
      "Iteration: 9890 Loss: 1.519207887249949e-07\n",
      "Iteration: 9891 Loss: 1.5176029398076752e-07\n",
      "Iteration: 9892 Loss: 1.5159996881095898e-07\n",
      "Iteration: 9893 Loss: 1.5143981303372027e-07\n",
      "Iteration: 9894 Loss: 1.512798264748355e-07\n",
      "Iteration: 9895 Loss: 1.5112000895248013e-07\n",
      "Iteration: 9896 Loss: 1.5096036028932613e-07\n",
      "Iteration: 9897 Loss: 1.5080088030535891e-07\n",
      "Iteration: 9898 Loss: 1.5064156882492282e-07\n",
      "Iteration: 9899 Loss: 1.5048242566640269e-07\n",
      "Iteration: 9900 Loss: 1.5032345065306506e-07\n",
      "Iteration: 9901 Loss: 1.5016464360969328e-07\n",
      "Iteration: 9902 Loss: 1.5000600435810016e-07\n",
      "Iteration: 9903 Loss: 1.4984753271964658e-07\n",
      "Iteration: 9904 Loss: 1.4968922851678215e-07\n",
      "Iteration: 9905 Loss: 1.495310915742439e-07\n",
      "Iteration: 9906 Loss: 1.493731217143066e-07\n",
      "Iteration: 9907 Loss: 1.492153187610651e-07\n",
      "Iteration: 9908 Loss: 1.4905768253620184e-07\n",
      "Iteration: 9909 Loss: 1.4890021286489136e-07\n",
      "Iteration: 9910 Loss: 1.487429095753265e-07\n",
      "Iteration: 9911 Loss: 1.4858577248525498e-07\n",
      "Iteration: 9912 Loss: 1.4842880142149292e-07\n",
      "Iteration: 9913 Loss: 1.4827199620857144e-07\n",
      "Iteration: 9914 Loss: 1.481153566729936e-07\n",
      "Iteration: 9915 Loss: 1.4795888263823562e-07\n",
      "Iteration: 9916 Loss: 1.4780257392502033e-07\n",
      "Iteration: 9917 Loss: 1.4764643036884753e-07\n",
      "Iteration: 9918 Loss: 1.4749045178700274e-07\n",
      "Iteration: 9919 Loss: 1.4733463800678524e-07\n",
      "Iteration: 9920 Loss: 1.4717898885615026e-07\n",
      "Iteration: 9921 Loss: 1.4702350415976797e-07\n",
      "Iteration: 9922 Loss: 1.46868183743719e-07\n",
      "Iteration: 9923 Loss: 1.4671302743573393e-07\n",
      "Iteration: 9924 Loss: 1.4655803505969465e-07\n",
      "Iteration: 9925 Loss: 1.4640320644475622e-07\n",
      "Iteration: 9926 Loss: 1.4624854141592015e-07\n",
      "Iteration: 9927 Loss: 1.4609403980268733e-07\n",
      "Iteration: 9928 Loss: 1.4593970143146984e-07\n",
      "Iteration: 9929 Loss: 1.457855261302552e-07\n",
      "Iteration: 9930 Loss: 1.4563151372638774e-07\n",
      "Iteration: 9931 Loss: 1.454776640476543e-07\n",
      "Iteration: 9932 Loss: 1.453239769210397e-07\n",
      "Iteration: 9933 Loss: 1.451704521752829e-07\n",
      "Iteration: 9934 Loss: 1.4501708963974272e-07\n",
      "Iteration: 9935 Loss: 1.4486388914278685e-07\n",
      "Iteration: 9936 Loss: 1.447108505126778e-07\n",
      "Iteration: 9937 Loss: 1.4455797357856238e-07\n",
      "Iteration: 9938 Loss: 1.4440525817024336e-07\n",
      "Iteration: 9939 Loss: 1.4425270411675248e-07\n",
      "Iteration: 9940 Loss: 1.4410031124756162e-07\n",
      "Iteration: 9941 Loss: 1.4394807939108761e-07\n",
      "Iteration: 9942 Loss: 1.437960083792214e-07\n",
      "Iteration: 9943 Loss: 1.4364409804140133e-07\n",
      "Iteration: 9944 Loss: 1.4349234820677582e-07\n",
      "Iteration: 9945 Loss: 1.4334075870649868e-07\n",
      "Iteration: 9946 Loss: 1.4318932937240066e-07\n",
      "Iteration: 9947 Loss: 1.4303806003272898e-07\n",
      "Iteration: 9948 Loss: 1.428869505219955e-07\n",
      "Iteration: 9949 Loss: 1.4273600066671336e-07\n",
      "Iteration: 9950 Loss: 1.4258521030319684e-07\n",
      "Iteration: 9951 Loss: 1.4243457925895618e-07\n",
      "Iteration: 9952 Loss: 1.4228410736641943e-07\n",
      "Iteration: 9953 Loss: 1.4213379445848508e-07\n",
      "Iteration: 9954 Loss: 1.4198364036584954e-07\n",
      "Iteration: 9955 Loss: 1.4183364492465238e-07\n",
      "Iteration: 9956 Loss: 1.416838079631605e-07\n",
      "Iteration: 9957 Loss: 1.415341293140936e-07\n",
      "Iteration: 9958 Loss: 1.4138460881191816e-07\n",
      "Iteration: 9959 Loss: 1.4123524628597473e-07\n",
      "Iteration: 9960 Loss: 1.4108604157519788e-07\n",
      "Iteration: 9961 Loss: 1.4093699450955352e-07\n",
      "Iteration: 9962 Loss: 1.407881049240283e-07\n",
      "Iteration: 9963 Loss: 1.4063937264606397e-07\n",
      "Iteration: 9964 Loss: 1.4049079751943535e-07\n",
      "Iteration: 9965 Loss: 1.4034237936829085e-07\n",
      "Iteration: 9966 Loss: 1.4019411803630576e-07\n",
      "Iteration: 9967 Loss: 1.4004601334969642e-07\n",
      "Iteration: 9968 Loss: 1.398980651475602e-07\n",
      "Iteration: 9969 Loss: 1.39750273263353e-07\n",
      "Iteration: 9970 Loss: 1.3960263753128372e-07\n",
      "Iteration: 9971 Loss: 1.394551577850732e-07\n",
      "Iteration: 9972 Loss: 1.3930783386394885e-07\n",
      "Iteration: 9973 Loss: 1.391606657158638e-07\n",
      "Iteration: 9974 Loss: 1.3901365294466724e-07\n",
      "Iteration: 9975 Loss: 1.3886679550585434e-07\n",
      "Iteration: 9976 Loss: 1.3872009322893372e-07\n",
      "Iteration: 9977 Loss: 1.385735459552488e-07\n",
      "Iteration: 9978 Loss: 1.384271535198643e-07\n",
      "Iteration: 9979 Loss: 1.382809157564459e-07\n",
      "Iteration: 9980 Loss: 1.3813483250420754e-07\n",
      "Iteration: 9981 Loss: 1.379889035997693e-07\n",
      "Iteration: 9982 Loss: 1.378431288797712e-07\n",
      "Iteration: 9983 Loss: 1.3769750818202053e-07\n",
      "Iteration: 9984 Loss: 1.375520413419426e-07\n",
      "Iteration: 9985 Loss: 1.3740672808220333e-07\n",
      "Iteration: 9986 Loss: 1.3726156847040784e-07\n",
      "Iteration: 9987 Loss: 1.371165622320544e-07\n",
      "Iteration: 9988 Loss: 1.3697170920230283e-07\n",
      "Iteration: 9989 Loss: 1.3682700921866306e-07\n",
      "Iteration: 9990 Loss: 1.3668246212192e-07\n",
      "Iteration: 9991 Loss: 1.3653806775082018e-07\n",
      "Iteration: 9992 Loss: 1.363938259386888e-07\n",
      "Iteration: 9993 Loss: 1.3624973653349337e-07\n",
      "Iteration: 9994 Loss: 1.3610579936604224e-07\n",
      "Iteration: 9995 Loss: 1.3596201427869944e-07\n",
      "Iteration: 9996 Loss: 1.3581838111150377e-07\n",
      "Iteration: 9997 Loss: 1.35674899699461e-07\n",
      "Iteration: 9998 Loss: 1.3553156988873415e-07\n",
      "Iteration: 9999 Loss: 1.3538839151708813e-07\n",
      "Iteration: 10000 Loss: 1.3524536442115155e-07\n",
      "r: 1\n"
     ]
    }
   ],
   "source": [
    "adm = time.time()\n",
    "ADMM_list, dual_ADMM_list, iterations_ADMM = admm.ADMM(N, M, (Q1,B1), (Q2,B2), (Q3,B3), Sigma, D, e1, e2, e31, e32, (x1,x2,x3), 1)\n",
    "fin = time.time()\n",
    "\n",
    "Times[\"ADMM\"] = fin - adm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f4b857f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 10000\n",
      "Primal: (x1,x2,x3),(y1,y2,y3),(z1,z2,z3)\n",
      " ((array([[1188.19, 1188.19, 1188.19, 1188.19, 1188.19],\n",
      "       [44.88, 44.88, 44.88, 44.88, 44.88],\n",
      "       [25.89, 25.89, 25.89, 25.89, 25.89]]), array([[186.57, 703.99, 833.33, 1188.19, 714.29],\n",
      "       [8.88, 30.91, 44.88, 42.83, 43.48],\n",
      "       [4.55, 15.09, 25.64, 18.98, 25.89]]), array([[-0.00, -0.00, 96.14, -0.00, 716.34]])), (array([[1188.19, 1188.19, 1188.19, 1188.19, 1188.19],\n",
      "       [44.88, 44.88, 44.88, 44.88, 44.88],\n",
      "       [25.89, 25.89, 25.89, 25.89, 25.89]]), array([[186.57, 704.00, 833.33, 1188.19, 714.29],\n",
      "       [8.88, 30.91, 44.88, 42.83, 43.48],\n",
      "       [4.55, 15.09, 25.64, 18.98, 25.89]]), array([[0.00, 0.00, 96.14, 0.00, 716.34]])), (array([[-5.00, -5.00, -5.00, 29.62, -5.00],\n",
      "       [-50.00, -50.00, 76.14, -50.00, -50.00],\n",
      "       [-95.00, -95.00, -95.00, -95.00, 324.62]]), array([[-1865.66, -6335.90, -10000.00, -8351.93, -10000.00],\n",
      "       [-1865.66, -6335.90, -10000.00, -8351.93, -10000.00],\n",
      "       [-1865.66, -6335.90, -10000.00, -8351.93, -10000.00]]), array([[-10000.00, -10000.00, -10000.00, -10000.00, -10000.00]])), 'infactible')\n",
      "Dual: (Equilibrium)\n",
      " [[1865.66 6335.90 10000.00 8351.93 10000.00]]\n"
     ]
    }
   ],
   "source": [
    "iter_ = -2\n",
    "print(\"Iterations:\",iterations_ADMM)\n",
    "print(\"Primal: (x1,x2,x3),(y1,y2,y3),(z1,z2,z3)\\n\",ADMM_list[iter_])\n",
    "print(\"Dual: (Equilibrium)\\n\",dual_ADMM_list[iter_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1d7890b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_iters_list = max([iterations_DY,iterations_BA,iterations_ADMM])\n",
    "max_iters_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8240e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_adjusted(x_sol, x_teo, sigma):\n",
    "    x_sol_1, x_sol_2, x_sol_3 = x_sol\n",
    "    x_teo_1, x_teo_2, x_teo_3 = x_teo\n",
    "    sigma=sigma[0]\n",
    "    \n",
    "    return LA.norm(x_sol_1-x_teo_1) + sum([sigma[xi]*LA.norm(x_sol_2[:,xi]-x_teo_2[:,xi]) for xi in range(M)]) + sum([sigma[xi]*LA.norm(x_sol_3[:,xi]-x_teo_3[:,xi]) for xi in range(M)])\n",
    "\n",
    "\n",
    "def norm_adjusted_N(x_sol, x_teo, sigma):\n",
    "    x_sol_1, x_sol_2, x_sol_3 = x_sol\n",
    "    x_teo_1, x_teo_2, x_teo_3 = x_teo\n",
    "    sigma=sigma[0]\n",
    "    \n",
    "    return sum([sigma[xi]*LA.norm(x_sol_1[:,xi][:,np.newaxis]-x_teo_1) for xi in range(M)]) + sum([sigma[xi]*LA.norm(x_sol_2[:,xi]-x_teo_2[:,xi]) for xi in range(M)]) + sum([sigma[xi]*LA.norm(x_sol_3[:,xi]-x_teo_3[:,xi]) for xi in range(M)])\n",
    "\n",
    "\n",
    "\n",
    "def generate_list(lista, lista_2, algoritmo, solution, objective_function, Demanda, max_iterations, P):\n",
    "\n",
    "    # unpack solution\n",
    "    x1, x2, x3 = solution\n",
    "\n",
    "    # create a list with index of graphics\n",
    "    iterations = list(range(max_iterations))\n",
    "\n",
    "    # create list to return\n",
    "    x_solution     = []\n",
    "    Fx_solution    = []\n",
    "    Non_anti_sol   = []\n",
    "    equili_solut   = []\n",
    "    capacity_solut = []\n",
    "    demand_solu    = []\n",
    "    dual_solut     = []\n",
    "    \n",
    "    zero_1 = np.zeros((N,1))\n",
    "    zero_1_N = np.zeros((N,M))\n",
    "    zero_2 = np.zeros((N,M))\n",
    "    zero_3 = np.zeros((1,M))\n",
    "    zeroo = (zero_1, zero_2, zero_3)\n",
    "    zeroo_N = (zero_1_N, zero_2, zero_3)\n",
    "    \n",
    "    # create arrays for each graph\n",
    "\n",
    "    if algoritmo == \"DY\":\n",
    "        for x_algo, x_fact in lista:\n",
    "            x1_algo, x2_algo, x3_algo = x_algo\n",
    "            x_solution.append( norm_adjusted(x_algo, (x1,x2,x3), P)/norm_adjusted((x1,x2,x3), zeroo, P) )\n",
    "            Fx_solution.append( abs(objective_function(x1_algo, x2_algo, x3_algo)[0][0] - objective_function(x1, x2, x3)[0][0] )/abs(objective_function(x1, x2, x3)[0][0]) )\n",
    "            Non_anti_sol.append( LA.norm(x1_algo - np.roll(x1_algo, 1, axis=1)) ) #No aplica a DY\n",
    "            equili_solut.append( norm_adjusted((zero_1,zero_2,x2_algo.sum(axis=0) - (D - x3_algo)), zeroo, P) )\n",
    "            capacity_solut.append( norm_adjusted((zero_1,x1_algo - x2_algo,zero_3), zeroo, P) )\n",
    "            demand_solu.append(norm_adjusted((zero_1,zero_2,D - x3_algo), zeroo, P))\n",
    "\n",
    "        for lambda1, lambda2 in lista_2:\n",
    "            dual_solut.append(norm_adjusted((zero_1,zero_2,rho - lambda2),zeroo,P)/norm_adjusted((zero_1,zero_2,rho),zeroo,P))\n",
    "    \n",
    "\n",
    "    elif algoritmo == \"ADMM\":\n",
    "        for elemento in lista:\n",
    "            x1_algo, x2_algo, x3_algo = elemento[0]\n",
    "            x_solution.append( norm_adjusted_N((x1_algo, x2_algo, x3_algo), (x1,x2,x3), P)/norm_adjusted((x1,x2,x3), zeroo, P) )\n",
    "            Fx_solution.append( abs(objective_function(x1_algo, x2_algo, x3_algo)[0][0] - objective_function(x1, x2, x3)[0][0] )/abs(objective_function(x1, x2, x3)[0][0]) )\n",
    "            Non_anti_sol.append( norm_adjusted_N((x1_algo - np.roll(x1_algo, 1, axis=1),zero_2, zero_3), zeroo, P) ) #No aplica a DY\n",
    "            equili_solut.append( norm_adjusted_N((zero_1_N,zero_2,x2_algo.sum(axis=0) - (D - x3_algo)), zeroo, P) )\n",
    "            capacity_solut.append( norm_adjusted_N((zero_1_N,x1_algo - x2_algo,zero_3), zeroo, P) )\n",
    "            demand_solu.append(norm_adjusted_N((zero_1_N,zero_2,D - x3_algo), zeroo, P))\n",
    "\n",
    "        for lambda2 in lista_2:\n",
    "            dual_solut.append(norm_adjusted_N((zero_1_N,zero_2,rho - lambda2),zeroo,P)/norm_adjusted((zero_1,zero_2,rho),zeroo,P))\n",
    "\n",
    "\n",
    "    elif algoritmo == \"BA\":\n",
    "        for x_algo, x_fact in lista:\n",
    "            x1_algo, x2_algo, x3_algo = x_algo\n",
    "            x_solution.append( norm_adjusted_N((x1_algo, x2_algo, x3_algo), (x1,x2,x3), P)/norm_adjusted((x1,x2,x3), zeroo, P) )\n",
    "            Fx_solution.append( abs(objective_function(x1_algo, x2_algo, x3_algo)[0][0] - objective_function(x1, x2, x3)[0][0] )/abs(objective_function(x1, x2, x3)[0][0]) )\n",
    "            Non_anti_sol.append( norm_adjusted_N((x1_algo - np.roll(x1_algo, 1, axis=1),zero_2, zero_3), zeroo, P) ) #No aplica a DY\n",
    "            equili_solut.append( norm_adjusted_N((zero_1_N,zero_2,x2_algo.sum(axis=0) - (D - x3_algo)), zeroo, P) )\n",
    "            capacity_solut.append( norm_adjusted_N((zero_1_N,x1_algo - x2_algo,zero_3), zeroo, P) )\n",
    "            demand_solu.append(norm_adjusted_N((zero_1_N,zero_2,D - x3_algo), zeroo, P))\n",
    "            \n",
    "        \n",
    "        for lambda1, lambda2 in lista_2:\n",
    "            dual_solut.append(norm_adjusted_N((zero_1_N,zero_2,rho - lambda2),zeroo,P)/norm_adjusted((zero_1,zero_2,rho),zeroo,P))\n",
    "            \n",
    "    \n",
    "    print(\"Completando listas\")\n",
    "    \n",
    "    x_solution     = x_solution     + [None]*(max_iterations-len(lista))\n",
    "    Fx_solution    = Fx_solution    + [None]*(max_iterations-len(lista))\n",
    "    Non_anti_sol   = Non_anti_sol   + [None]*(max_iterations-len(lista))\n",
    "    equili_solut   = equili_solut   + [None]*(max_iterations-len(lista))\n",
    "    capacity_solut = capacity_solut + [None]*(max_iterations-len(lista))\n",
    "    demand_solu    = demand_solu    + [None]*(max_iterations-len(lista))\n",
    "    dual_solut     = dual_solut     + [None]*(max_iterations-len(lista))\n",
    "\n",
    "    k = 0\n",
    "\n",
    "    return iterations[k:], x_solution[k:], Fx_solution[k:], Non_anti_sol[k:], equili_solut[k:], capacity_solut[k:], demand_solu[k:], dual_solut[k:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9094918c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DY\n",
      "Completando listas\n",
      "ADMM\n",
      "Completando listas\n",
      "BA\n",
      "Completando listas\n"
     ]
    }
   ],
   "source": [
    "print(\"DY\")\n",
    "iter_DY, x_DY_sol, Fx_DY_sol, Non_anti_DY, equili_DY_solu, capacity_DY_solu, demand_DY_sol, dual_DY_sol = generate_list(DY_list, Dual_DY_list, \"DY\", (x1, x2, x3), objective_function, D, max_iters_list, Sigma)\n",
    "print(\"ADMM\")\n",
    "iter_ADMM, x_ADMM_sol, Fx_ADMM_sol, Non_anti_ADMM, equili_ADMM_solu, capacity_ADMM_solu, demand_ADMM_sol, dual_ADMM_sol  = generate_list(ADMM_list, dual_ADMM_list, \"ADMM\", (x1, x2, x3), objective_function, D, max_iters_list, Sigma)\n",
    "print(\"BA\")\n",
    "iter_BA, x_BA_sol, Fx_BA_sol, Non_anti_BA, equili_BA_solu, capacity_BA_solu, demand_BA_sol, BA_dual_sol = generate_list(x_BA_list, dual_BA_list, \"BA\", (x1, x2, x3), objective_function, D, max_iters_list, Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba168274",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Crear un rango de iteraciones para las gráficas\n",
    "k = 0\n",
    "l = int(max_iters_list / 1) + 1\n",
    "\n",
    "zero_1 = np.zeros((N,1))\n",
    "zero_2 = np.zeros((N,M))\n",
    "zero_3 = np.zeros((1,M))\n",
    "zeroo = (zero_1, zero_2, zero_3)\n",
    "\n",
    "# Definir los niveles teóricos según tu solución teórica\n",
    "nivel_teorico_equilibrio = norm_adjusted((zero_1,zero_2,x2.sum(axis=0) - (D - x3)), zeroo, Sigma)\n",
    "nivel_teorico_capacidad = norm_adjusted((zero_1,x1 - x2,zero_3), zeroo, Sigma)\n",
    "nivel_teorico_demanda = norm_adjusted((zero_1,zero_2,D - x3), zeroo, Sigma)\n",
    "\n",
    "# Variable de sufijo para los nombres de archivo\n",
    "suffix = \"_2\"\n",
    "\n",
    "# Función para configurar y guardar cada gráfico\n",
    "def configurar_grafico(ax, x_data, y_data, labels, colors, title, y_label, width, height, y_lim=None, nivel_teorico=None):\n",
    "    for x, y, label, color in zip(x_data, y_data, labels, colors):\n",
    "        ax.plot(x[k:l], y[k:l], '-', linewidth=1.5, label=label, color=color)\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_xlabel('Iteraciones')\n",
    "    ax.legend()\n",
    "    if y_lim:\n",
    "        ax.set_ylim(y_lim)\n",
    "    if nivel_teorico is not None:\n",
    "        ax.axhline(y=nivel_teorico, color='gray', linestyle='--', linewidth=1)\n",
    "    ax.figure.set_figwidth(width)\n",
    "    ax.figure.set_figheight(height)\n",
    "\n",
    "# Datos para los gráficos\n",
    "iter_data = [iter_DY, iter_ADMM, iter_BA]\n",
    "labels = ['TOPS', 'ADMM', 'FPIS']\n",
    "colors = ['green', 'blue', 'red']\n",
    "\n",
    "# Definir la altura y el ancho deseado para cada gráfico\n",
    "width = 10  # Ajusta esta variable según sea necesario\n",
    "height = 3  # Ajusta esta variable según sea necesario\n",
    "\n",
    "# Crear y guardar cada figura individualmente\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "configurar_grafico(ax, iter_data, [x_DY_sol, x_ADMM_sol, x_BA_sol], labels, colors, 'Distancia al Óptimo', \n",
    "                   r'$\\frac{\\|x^{k} - x^{*}\\|}{\\|x^{*}\\|}$', width, height, y_lim=(0, 1))\n",
    "plt.savefig(f'images/caso{suffix}/distancia_al_optimo{suffix}.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "configurar_grafico(ax, iter_data, [Fx_DY_sol, Fx_ADMM_sol, Fx_BA_sol], labels, colors, 'Diferencia Valor Función Objetivo', \n",
    "                   r'$\\|f(x^{k})-f(x^{*})\\|$', width, height, y_lim=(0, 1))\n",
    "plt.savefig(f'images/caso{suffix}/diferencia_valor_funcion_objetivo{suffix}.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "configurar_grafico(ax, iter_data[1:], [Non_anti_DY, Non_anti_ADMM, Non_anti_BA][1:], labels[1:], colors[1:], 'No-Anticipatividad', \n",
    "                   r'$\\|c^{k}-P_{\\mathcal{N}}(c^{k})\\|$', width, height)#, y_lim=(0, 25))\n",
    "plt.savefig(f'images/caso{suffix}/no_anticipatividad{suffix}.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "configurar_grafico(ax, iter_data, [equili_DY_solu, equili_ADMM_solu, equili_BA_solu], labels, colors, 'Restricción de Equilibrio', \n",
    "                   r'$\\|\\textbf{1}^{T}g^{k}-(D-q^{k})\\|$', width, height, y_lim=(-0.1, 1.0), nivel_teorico=nivel_teorico_equilibrio)\n",
    "plt.savefig(f'images/caso{suffix}/restriccion_de_equilibrio{suffix}.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "configurar_grafico(ax, iter_data, [capacity_DY_solu, capacity_ADMM_solu, capacity_BA_solu], labels, colors, 'Restricción de Capacidad de Producción', \n",
    "                   r'$\\|c^{k} - g^{k}\\|$', width, height, nivel_teorico=nivel_teorico_capacidad)\n",
    "plt.savefig(f'images/caso{suffix}/restriccion_de_capacidad_de_produccion{suffix}.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "configurar_grafico(ax, iter_data, [demand_DY_sol, demand_ADMM_sol, demand_BA_sol], labels, colors, 'Diferencia de Demanda Satisfecha', \n",
    "                   r'$\\|D-q^{k}\\|$', width, height, y_lim=(0, 4000), nivel_teorico=nivel_teorico_demanda)\n",
    "plt.savefig(f'images/caso{suffix}/diferencia_de_demanda_satisfecha{suffix}.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "configurar_grafico(ax, iter_data, [dual_DY_sol, dual_ADMM_sol, BA_dual_sol], labels, colors, 'Diferencia al Precio Óptimo', \n",
    "                   r'$\\frac{\\|\\rho^{k}-\\rho^{*}\\|}{\\|\\rho^{*}\\|}$', width, height, y_lim=(0, 1))\n",
    "plt.savefig(f'images/caso{suffix}/diferencia_al_precio_optimo{suffix}.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0041b318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CP': 0.08461213111877441,\n",
       " 'DY': 1219.4612152576447,\n",
       " 'BA': 632.7192559242249,\n",
       " 'ADMM': 590.8202083110809}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e36f3d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9999,\n",
       " 0.0018707239259118764,\n",
       " 3.838783836264409e-05,\n",
       " 0.0,\n",
       " 9.76295432314967e-06,\n",
       " 466.42247034170043,\n",
       " 777.166041511746,\n",
       " 0.003523146132476917)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_DY[-1], x_DY_sol[-1], Fx_DY_sol[-1], Non_anti_DY[-1], equili_DY_solu[-1], capacity_DY_solu[-1], demand_DY_sol[-1], dual_DY_sol[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1a81cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9999,\n",
       " 0.00034243730719639323,\n",
       " 1.442964268896573e-06,\n",
       " 0.0,\n",
       " 9.75533275911769e-06,\n",
       " 467.80093622220585,\n",
       " 777.9293176951296,\n",
       " 0.00021417750698211665)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_BA[-1], x_BA_sol[-1], Fx_BA_sol[-1], Non_anti_BA[-1], equili_BA_solu[-1], capacity_BA_solu[-1], demand_BA_sol[-1], BA_dual_sol[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30c264f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9999,\n",
       " 2.2936912828630326e-07,\n",
       " 7.704763134959864e-07,\n",
       " 1.0528671680236122e-13,\n",
       " 0.0005252464789091867,\n",
       " 467.9367410229821,\n",
       " 778.1024461603864,\n",
       " 5.447842018551336e-07)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_ADMM[-1], x_ADMM_sol[-1], Fx_ADMM_sol[-1], Non_anti_ADMM[-1], equili_ADMM_solu[-1], capacity_ADMM_solu[-1], demand_ADMM_sol[-1], dual_ADMM_sol[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67346c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9.780116885453774e-06, 467.9362565949723, 778.1024461607899)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nivel_teorico_equilibrio, nivel_teorico_capacidad, nivel_teorico_demanda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2d7184",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
