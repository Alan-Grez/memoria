{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0227457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import centralized as CP\n",
    "import davisyin as DY\n",
    "import admm as admm\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from numpy import linalg as LA\n",
    "from numpy.linalg import inv\n",
    "import matplotlib as plt\n",
    "from matplotlib import rc\n",
    "# Configura el tipo de letra globalmente\n",
    "rc('font', **{'family': 'serif', 'serif': ['Computer Modern Roman']})\n",
    "rc('text', usetex=True)\n",
    "#plt.rcParams['text.usetex'] = True\n",
    "import matplotlib.pyplot as plt\n",
    "import proyecciones as pro\n",
    "import time\n",
    "import briceno as BA\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96d1d966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18, 0.02, 0.34, 0.13, 0.20, 0.13]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caso 1: Caso base\n",
    "    \n",
    "# Cambiar criterio de parada por errores relativos.\n",
    "# Establecer la semilla\n",
    "seed = 40\n",
    "#41\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Seteamos los parámetros:\n",
    "N, M = 3, 6  # Son 2 tecnologías, 10 escenarios\n",
    "\n",
    "_g_ = 1\n",
    "\n",
    "# Probabilidades:\n",
    "inv_, mc_, voll_, d_ = [50.0,  1000.0, 10000.0, 1000.0]\n",
    "                       #[10.0, 2000.0, 10000.0, 1000.0]\n",
    "                       #[50.0,  1000.0, 10000.0, 1000.0]\n",
    "#Sigma = np.ones((1,M))\n",
    "Sigma = np.random.rand(1,M)\n",
    "Sigma /= Sigma.sum()\n",
    "Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1895a45",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      " [[5.0]\n",
      " [50.0]\n",
      " [95.0]]\n",
      "MC\n",
      " [[[1000.0 0.0 0.0]\n",
      "  [0.0 21000.0 0.0]\n",
      "  [0.0 0.0 41000.0]]\n",
      "\n",
      " [[900.0 0.0 0.0]\n",
      "  [0.0 20500.0 0.0]\n",
      "  [0.0 0.0 42000.0]]\n",
      "\n",
      " [[1200.0 0.0 0.0]\n",
      "  [0.0 22000.0 0.0]\n",
      "  [0.0 0.0 39000.0]]\n",
      "\n",
      " [[700.0 0.0 0.0]\n",
      "  [0.0 19500.0 0.0]\n",
      "  [0.0 0.0 44000.0]]\n",
      "\n",
      " [[1400.0 0.0 0.0]\n",
      "  [0.0 23000.0 0.0]\n",
      "  [0.0 0.0 37000.0]]\n",
      "\n",
      " [[88000.0 0.0 0.0]\n",
      "  [0.0 88000.0 0.0]\n",
      "  [0.0 0.0 88000.0]]]\n",
      "VOLL\n",
      " 10000.0\n",
      "D\n",
      " [[200.0 750.0 1000.0 1250.0 1500.0 4000.0]]\n",
      "frobenius_norm: 595.1406870090601\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.1f}\".format(x)})\n",
    "Times = {}\n",
    "r_ = 1\n",
    "\n",
    "\n",
    "# Parámetros funciones:\n",
    "I    = (inv_ * np.ones((N, 1)) + r_*np.array([[-45], [0], [45]])) / _g_\n",
    "print(\"I\\n\",I)\n",
    "aux  = np.array([1/ _g_ + r_*20*i/ _g_ for i in range(N)])\n",
    "\n",
    "mc_11 = 100\n",
    "mc_22 = 500\n",
    "mc_33 = 1000\n",
    "MC   = (np.array([np.diag(mc_*aux/_g_ + r_*np.array([((-1)**m)*mc_11*m/_g_, ((-1)**m)*mc_22*m/_g_, ((-1)**(m+1))*mc_33*m/_g_]))/ _g_ for m in range(M)])) \n",
    "MC[-1]=(np.array([[1,0,0],[0,1,0],[0,0,1]])*88000) / _g_\n",
    "print(\"MC\\n\",MC)\n",
    "\n",
    "MC_pyomo = np.array([mc_*aux + r_*np.array([((-1)**m)*mc_11*m, ((-1)**m)*mc_22*m, ((-1)**(m+1))*mc_33*m]) for m in range(M)]).T.tolist()\n",
    "for i in range(N):\n",
    "    MC_pyomo[i][-1] = MC[-1][i][i]\n",
    "MC_pyomo\n",
    "\n",
    "VOLL = voll_ / _g_\n",
    "print(\"VOLL\\n\",VOLL)\n",
    "D    = (d_*np.ones((1,M)) + r_*np.array([-800, -250, 0, 250, 500, 3000])[np.newaxis])/ _g_\n",
    "print(\"D\\n\",D)\n",
    "\n",
    "e_ = 0\n",
    "\n",
    "e1  = e_\n",
    "e2  = e_\n",
    "e31 = e_*1e2/2\n",
    "e32 = e_\n",
    "\n",
    "Q1, B1 = np.zeros((N,N)), I\n",
    "Q2, B2 = 0.01*MC, np.zeros((N,M))\n",
    "Q3, B3 = np.zeros((1,M)), VOLL*np.ones((1,M))\n",
    "\n",
    "\n",
    "frobenius_norm = (e1+e2)*np.sqrt(N)+e31+e32+np.array([LA.norm(np.einsum('i,ikl->ikl',Sigma[0],0.01*MC)[xi], 'fro') for xi in range(M)]).sum()\n",
    "#frobenius_norm = max([e1*np.sqrt(N),e31+e32,e2*np.sqrt(N)+np.array([LA.norm(np.einsum('i,ikl->ikl',Sigma[0],0.01*MC)[xi], 'fro') for xi in range(M)]).sum()])\n",
    "print(\"frobenius_norm:\",frobenius_norm)\n",
    "\n",
    "def Grad_Phi_1(x1, Q_1 = np.zeros((N,N)), B_1 = I, e1 = e1, N = N):\n",
    "       return np.dot(Q_1,x1)+B_1# - e1*np.dot(np.identity(N),np.maximum(-x1,0))\n",
    "\n",
    "\n",
    "def Grad_Phi_2(x2, Q_2 = 0.01*MC, B_2 = np.zeros((N,M)), e2 = e2, N = N, M = M):\n",
    "\n",
    "    return np.einsum('ijk,ki->ji', Q_2, x2)+B_2# - e2*np.einsum('ijk,ki->ji', np.array([np.diag(np.ones(N)) for m in range(M)]), np.maximum(-x2,0))\n",
    "\n",
    "\n",
    "def Grad_Phi_3(x3, Q_3 = np.zeros((1,M)), B_3 = VOLL*np.ones((1,M)), D=D, e31=e31, e32= e32, M = M):\n",
    "    return Q_3*x3+B_3 #- e31*np.dot(np.maximum(-x3,0),np.identity(M)) - e32*np.dot(np.maximum(-D+x3,0),np.identity(M))\n",
    "\n",
    "\n",
    "def Grad_Phi(x1,x2,x3, P = Sigma):\n",
    "    return Grad_Phi_1(x1), P*Grad_Phi_2(x2), P*Grad_Phi_3(x3)\n",
    "\n",
    "def Grad_Phi_NA(x1,x2,x3, P = Sigma):\n",
    "    return P*Grad_Phi_1(x1), P*Grad_Phi_2(x2), P*Grad_Phi_3(x3)\n",
    "\n",
    "def Phi_1(x1, Q_1 = np.zeros((N,N)), B_1 = I, C_1 = 0.0, e1 = e1):\n",
    "    return 0.5*np.einsum('ij,ji -> i', x1.T,np.dot(Q_1,x1))[:,np.newaxis]+np.dot(x1.T, B_1)+C_1 + e1/2*LA.norm(np.maximum(-x1.flatten(),0))**2\n",
    "\n",
    "def Phi_2_xi(x2, Q_2 = 0.01*MC, B_2 = np.zeros((N,M)), C_2 = np.zeros((M, 1))):\n",
    "    return 0.5*np.einsum('ij,ji -> i', x2.T, np.einsum('ijk,ki -> ji', Q_2, x2))[:,np.newaxis]+np.einsum('ij,ji->i',x2.T,B_2)[:,np.newaxis]+C_2 + e2/2*LA.norm(np.maximum(-x2.flatten(),0))**2\n",
    "\n",
    "def Phi_3_xi(x3, Q_3 = np.zeros((1,M)), B_3 = VOLL*np.ones((1,M)), C_3 = -VOLL*D ):\n",
    "    return (0.5*x3*Q_3*x3+B_3*x3+C_3).T + e31/2*LA.norm(np.maximum(-x3.flatten(),0))**2 + e32/2*LA.norm(np.maximum((D-x3).flatten(),0))**2\n",
    "\n",
    "\n",
    "def objective_function(x1, x2, x3, P = Sigma, NA = True):\n",
    "\n",
    "# NA = True, cumple la funcion que si se impuso \n",
    "#      la condición de no anticipatividad para x1\n",
    "#      entonces, Phi_1(x1).shape == (M,1)\n",
    "    if NA:\n",
    "        return np.dot(P, Phi_1(x1) +Phi_2_xi(x2)+Phi_3_xi(x3))\n",
    "    else:\n",
    "        return Phi_1(x1)+ np.dot(P, Phi_2_xi(x2)+Phi_3_xi(x3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4f57c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = time.time()\n",
    "x1, x2, x3, rho, mu = map(np.array, CP.modelo(Sigma, N, M, \\\n",
    "                                              parametros = [I.T[0].tolist(),\\\n",
    "                                                            MC_pyomo,\\\n",
    "                                                            VOLL,\\\n",
    "                                                            D[0]] , show = 0))\n",
    "fin = time.time()\n",
    "\n",
    "\n",
    "Times[\"CP\"] = fin - cp\n",
    "\n",
    "x1 = x1[:,np.newaxis]\n",
    "x2 = x2.T\n",
    "x3 = x3[np.newaxis,:][0]\n",
    "rho = rho[np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1265a4e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primal:\n",
      "[[1188.2]\n",
      " [44.8]\n",
      " [25.7]]\n",
      "[[186.6 704.0 833.3 1188.2 714.3 11.4]\n",
      " [8.9 30.9 44.8 42.9 43.5 11.4]\n",
      " [4.6 15.1 25.6 19.0 25.7 11.4]]\n",
      "[[0.0 0.0 96.2 0.0 716.5 3965.9]]\n",
      "Dual:\n",
      "[[0.0 0.0 0.0 39.9 0.0 0.0]\n",
      " [0.0 0.0 145.4 0.0 0.0 0.0]\n",
      " [0.0 0.0 0.0 0.0 483.7 0.0]]\n",
      "[[1865.7 6336.1 10000.0 8357.0 10000.0 10000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"primal:\\n{x1}\\n{x2}\\n{x3}\")\n",
    "print(f\"Dual:\\n{mu}\\n{rho}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c84c5f51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta: 0.0016802749699833185\n",
      "Gamma: 0.0031218162324379273\n",
      "Lambda_k: 1\n",
      "Iteration: 1 lambda_k: 1 Loss: 0.8310352033297772\n",
      "Iteration: 2 lambda_k: 1 Loss: 0.7073862252784012\n",
      "Iteration: 3 lambda_k: 1 Loss: 0.6279999442373778\n",
      "Iteration: 4 lambda_k: 1 Loss: 0.5762411807069783\n",
      "Iteration: 5 lambda_k: 1 Loss: 0.5498784584975096\n",
      "Iteration: 6 lambda_k: 1 Loss: 0.5316815468733749\n",
      "Iteration: 7 lambda_k: 1 Loss: 0.516868498733118\n",
      "Iteration: 8 lambda_k: 1 Loss: 0.5073995894005161\n",
      "Iteration: 9 lambda_k: 1 Loss: 0.5011011760404228\n",
      "Iteration: 10 lambda_k: 1 Loss: 0.4965083676440122\n",
      "Iteration: 11 lambda_k: 1 Loss: 0.4930361585580605\n",
      "Iteration: 12 lambda_k: 1 Loss: 0.4904892272916616\n",
      "Iteration: 13 lambda_k: 1 Loss: 0.48840373734393955\n",
      "Iteration: 14 lambda_k: 1 Loss: 0.48644188250347625\n",
      "Iteration: 15 lambda_k: 1 Loss: 0.48458791134431356\n",
      "Iteration: 16 lambda_k: 1 Loss: 0.4828285908045649\n",
      "Iteration: 17 lambda_k: 1 Loss: 0.4812012873159904\n",
      "Iteration: 18 lambda_k: 1 Loss: 0.4797652028790013\n",
      "Iteration: 19 lambda_k: 1 Loss: 0.4784107070493575\n",
      "Iteration: 20 lambda_k: 1 Loss: 0.47707120921904955\n",
      "Iteration: 21 lambda_k: 1 Loss: 0.47575536380071426\n",
      "Iteration: 22 lambda_k: 1 Loss: 0.47447467247513286\n",
      "Iteration: 23 lambda_k: 1 Loss: 0.47322912614308654\n",
      "Iteration: 24 lambda_k: 1 Loss: 0.47201430817719403\n",
      "Iteration: 25 lambda_k: 1 Loss: 0.4708264161145806\n",
      "Iteration: 26 lambda_k: 1 Loss: 0.46966291230678553\n",
      "Iteration: 27 lambda_k: 1 Loss: 0.46852188358680147\n",
      "Iteration: 28 lambda_k: 1 Loss: 0.4674016567155915\n",
      "Iteration: 29 lambda_k: 1 Loss: 0.46630073706071945\n",
      "Iteration: 30 lambda_k: 1 Loss: 0.46521783240890513\n",
      "Iteration: 31 lambda_k: 1 Loss: 0.46415185892722177\n",
      "Iteration: 32 lambda_k: 1 Loss: 0.46311022387840833\n",
      "Iteration: 33 lambda_k: 1 Loss: 0.46220275762241875\n",
      "Iteration: 34 lambda_k: 1 Loss: 0.46131277486195105\n",
      "Iteration: 35 lambda_k: 1 Loss: 0.46043900075074423\n",
      "Iteration: 36 lambda_k: 1 Loss: 0.4595916108316169\n",
      "Iteration: 37 lambda_k: 1 Loss: 0.4587987833777213\n",
      "Iteration: 38 lambda_k: 1 Loss: 0.45802637690277764\n",
      "Iteration: 39 lambda_k: 1 Loss: 0.4572630510785053\n",
      "Iteration: 40 lambda_k: 1 Loss: 0.45650918085801817\n",
      "Iteration: 41 lambda_k: 1 Loss: 0.455765466870404\n",
      "Iteration: 42 lambda_k: 1 Loss: 0.45503135235810593\n",
      "Iteration: 43 lambda_k: 1 Loss: 0.45430604264180174\n",
      "Iteration: 44 lambda_k: 1 Loss: 0.453588916239849\n",
      "Iteration: 45 lambda_k: 1 Loss: 0.45287947133795886\n",
      "Iteration: 46 lambda_k: 1 Loss: 0.4521772500055605\n",
      "Iteration: 47 lambda_k: 1 Loss: 0.45148182277461707\n",
      "Iteration: 48 lambda_k: 1 Loss: 0.4507927926030829\n",
      "Iteration: 49 lambda_k: 1 Loss: 0.4501097956236636\n",
      "Iteration: 50 lambda_k: 1 Loss: 0.4494324982831226\n",
      "Iteration: 51 lambda_k: 1 Loss: 0.4487605940577908\n",
      "Iteration: 52 lambda_k: 1 Loss: 0.4480938008209586\n",
      "Iteration: 53 lambda_k: 1 Loss: 0.4474318586233208\n",
      "Iteration: 54 lambda_k: 1 Loss: 0.4467745277764197\n",
      "Iteration: 55 lambda_k: 1 Loss: 0.4461215870933884\n",
      "Iteration: 56 lambda_k: 1 Loss: 0.4454728322910075\n",
      "Iteration: 57 lambda_k: 1 Loss: 0.44482807453866563\n",
      "Iteration: 58 lambda_k: 1 Loss: 0.44418713913653446\n",
      "Iteration: 59 lambda_k: 1 Loss: 0.44354986434085875\n",
      "Iteration: 60 lambda_k: 1 Loss: 0.44291610024896255\n",
      "Iteration: 61 lambda_k: 1 Loss: 0.4422857078376539\n",
      "Iteration: 62 lambda_k: 1 Loss: 0.44165855805414533\n",
      "Iteration: 63 lambda_k: 1 Loss: 0.4410345310087204\n",
      "Iteration: 64 lambda_k: 1 Loss: 0.44041351519982863\n",
      "Iteration: 65 lambda_k: 1 Loss: 0.4397954069191036\n",
      "Iteration: 66 lambda_k: 1 Loss: 0.43918010953299635\n",
      "Iteration: 67 lambda_k: 1 Loss: 0.43856753298013057\n",
      "Iteration: 68 lambda_k: 1 Loss: 0.43795759321468536\n",
      "Iteration: 69 lambda_k: 1 Loss: 0.4373502118070776\n",
      "Iteration: 70 lambda_k: 1 Loss: 0.4367453154273584\n",
      "Iteration: 71 lambda_k: 1 Loss: 0.436142835595979\n",
      "Iteration: 72 lambda_k: 1 Loss: 0.4355428237914712\n",
      "Iteration: 73 lambda_k: 1 Loss: 0.4349452833681091\n",
      "Iteration: 74 lambda_k: 1 Loss: 0.4343501261299609\n",
      "Iteration: 75 lambda_k: 1 Loss: 0.43375729222107495\n",
      "Iteration: 76 lambda_k: 1 Loss: 0.4331667212495714\n",
      "Iteration: 77 lambda_k: 1 Loss: 0.4325783544364432\n",
      "Iteration: 78 lambda_k: 1 Loss: 0.431992136343453\n",
      "Iteration: 79 lambda_k: 1 Loss: 0.4314080153584338\n",
      "Iteration: 80 lambda_k: 1 Loss: 0.4308259434043576\n",
      "Iteration: 81 lambda_k: 1 Loss: 0.4302458755050315\n",
      "Iteration: 82 lambda_k: 1 Loss: 0.42966776951101043\n",
      "Iteration: 83 lambda_k: 1 Loss: 0.42909158584782353\n",
      "Iteration: 84 lambda_k: 1 Loss: 0.42851728735693045\n",
      "Iteration: 85 lambda_k: 1 Loss: 0.4279448390927045\n",
      "Iteration: 86 lambda_k: 1 Loss: 0.42737420815965194\n",
      "Iteration: 87 lambda_k: 1 Loss: 0.42680536355815735\n",
      "Iteration: 88 lambda_k: 1 Loss: 0.42623827604724973\n",
      "Iteration: 89 lambda_k: 1 Loss: 0.42567291801125007\n",
      "Iteration: 90 lambda_k: 1 Loss: 0.42510926335254307\n",
      "Iteration: 91 lambda_k: 1 Loss: 0.42454728737905456\n",
      "Iteration: 92 lambda_k: 1 Loss: 0.42398696670798935\n",
      "Iteration: 93 lambda_k: 1 Loss: 0.42342827917574777\n",
      "Iteration: 94 lambda_k: 1 Loss: 0.42287120375252424\n",
      "Iteration: 95 lambda_k: 1 Loss: 0.4223157204756089\n",
      "Iteration: 96 lambda_k: 1 Loss: 0.42176181036544574\n",
      "Iteration: 97 lambda_k: 1 Loss: 0.42120945536866466\n",
      "Iteration: 98 lambda_k: 1 Loss: 0.4206586382961206\n",
      "Iteration: 99 lambda_k: 1 Loss: 0.42010934276706646\n",
      "Iteration: 100 lambda_k: 1 Loss: 0.41956155315739924\n",
      "Iteration: 101 lambda_k: 1 Loss: 0.41901525455165467\n",
      "Iteration: 102 lambda_k: 1 Loss: 0.41847043269837586\n",
      "Iteration: 103 lambda_k: 1 Loss: 0.41792707396885187\n",
      "Iteration: 104 lambda_k: 1 Loss: 0.4173851653183367\n",
      "Iteration: 105 lambda_k: 1 Loss: 0.4168446942502293\n",
      "Iteration: 106 lambda_k: 1 Loss: 0.416305648782643\n",
      "Iteration: 107 lambda_k: 1 Loss: 0.4157680174172061\n",
      "Iteration: 108 lambda_k: 1 Loss: 0.4152317891099542\n",
      "Iteration: 109 lambda_k: 1 Loss: 0.4146969532441586\n",
      "Iteration: 110 lambda_k: 1 Loss: 0.4141634996049467\n",
      "Iteration: 111 lambda_k: 1 Loss: 0.4136314183555791\n",
      "Iteration: 112 lambda_k: 1 Loss: 0.4131007000152658\n",
      "Iteration: 113 lambda_k: 1 Loss: 0.41257133543982644\n",
      "Iteration: 114 lambda_k: 1 Loss: 0.41204331579705555\n",
      "Iteration: 115 lambda_k: 1 Loss: 0.4115166325555858\n",
      "Iteration: 116 lambda_k: 1 Loss: 0.41099127746369013\n",
      "Iteration: 117 lambda_k: 1 Loss: 0.4104672425341678\n",
      "Iteration: 118 lambda_k: 1 Loss: 0.40994452002937487\n",
      "Iteration: 119 lambda_k: 1 Loss: 0.40942310244717506\n",
      "Iteration: 120 lambda_k: 1 Loss: 0.40890298250775664\n",
      "Iteration: 121 lambda_k: 1 Loss: 0.4083841531412555\n",
      "Iteration: 122 lambda_k: 1 Loss: 0.40786660747613557\n",
      "Iteration: 123 lambda_k: 1 Loss: 0.4073503388282761\n",
      "Iteration: 124 lambda_k: 1 Loss: 0.4068353406907195\n",
      "Iteration: 125 lambda_k: 1 Loss: 0.40632160672403705\n",
      "Iteration: 126 lambda_k: 1 Loss: 0.4058091307472744\n",
      "Iteration: 127 lambda_k: 1 Loss: 0.40529790672943683\n",
      "Iteration: 128 lambda_k: 1 Loss: 0.40478792878148323\n",
      "Iteration: 129 lambda_k: 1 Loss: 0.40427919114879507\n",
      "Iteration: 130 lambda_k: 1 Loss: 0.40377168820409093\n",
      "Iteration: 131 lambda_k: 1 Loss: 0.4032654144407591\n",
      "Iteration: 132 lambda_k: 1 Loss: 0.4027603644665819\n",
      "Iteration: 133 lambda_k: 1 Loss: 0.4022565329978286\n",
      "Iteration: 134 lambda_k: 1 Loss: 0.4017539148536918\n",
      "Iteration: 135 lambda_k: 1 Loss: 0.40125250495105075\n",
      "Iteration: 136 lambda_k: 1 Loss: 0.40075229829953607\n",
      "Iteration: 137 lambda_k: 1 Loss: 0.40025328999688264\n",
      "Iteration: 138 lambda_k: 1 Loss: 0.39975547522455046\n",
      "Iteration: 139 lambda_k: 1 Loss: 0.3992588492435975\n",
      "Iteration: 140 lambda_k: 1 Loss: 0.3987634073907908\n",
      "Iteration: 141 lambda_k: 1 Loss: 0.3982691450749412\n",
      "Iteration: 142 lambda_k: 1 Loss: 0.39777605777344804\n",
      "Iteration: 143 lambda_k: 1 Loss: 0.3972841410276136\n",
      "Iteration: 144 lambda_k: 1 Loss: 0.3967933904447848\n",
      "Iteration: 145 lambda_k: 1 Loss: 0.3963038016884082\n",
      "Iteration: 146 lambda_k: 1 Loss: 0.39581537048084053\n",
      "Iteration: 147 lambda_k: 1 Loss: 0.39532809259472584\n",
      "Iteration: 148 lambda_k: 1 Loss: 0.39484196385757486\n",
      "Iteration: 149 lambda_k: 1 Loss: 0.3943569801453472\n",
      "Iteration: 150 lambda_k: 1 Loss: 0.3938731373811381\n",
      "Iteration: 151 lambda_k: 1 Loss: 0.39339043153314407\n",
      "Iteration: 152 lambda_k: 1 Loss: 0.3929088586127399\n",
      "Iteration: 153 lambda_k: 1 Loss: 0.3924284146726577\n",
      "Iteration: 154 lambda_k: 1 Loss: 0.39194909580526627\n",
      "Iteration: 155 lambda_k: 1 Loss: 0.3914708981409467\n",
      "Iteration: 156 lambda_k: 1 Loss: 0.390993817846561\n",
      "Iteration: 157 lambda_k: 1 Loss: 0.390517851124002\n",
      "Iteration: 158 lambda_k: 1 Loss: 0.3900429942088263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 159 lambda_k: 1 Loss: 0.3895692433689621\n",
      "Iteration: 160 lambda_k: 1 Loss: 0.38909659490348875\n",
      "Iteration: 161 lambda_k: 1 Loss: 0.3886250451414834\n",
      "Iteration: 162 lambda_k: 1 Loss: 0.3881545904409313\n",
      "Iteration: 163 lambda_k: 1 Loss: 0.3876852271876959\n",
      "Iteration: 164 lambda_k: 1 Loss: 0.3872169517945448\n",
      "Iteration: 165 lambda_k: 1 Loss: 0.38674976070023004\n",
      "Iteration: 166 lambda_k: 1 Loss: 0.38628365036861767\n",
      "Iteration: 167 lambda_k: 1 Loss: 0.38581861728786476\n",
      "Iteration: 168 lambda_k: 1 Loss: 0.3853546579696416\n",
      "Iteration: 169 lambda_k: 1 Loss: 0.3848917689483953\n",
      "Iteration: 170 lambda_k: 1 Loss: 0.38442994678065373\n",
      "Iteration: 171 lambda_k: 1 Loss: 0.3839691880443666\n",
      "Iteration: 172 lambda_k: 1 Loss: 0.3835094893382817\n",
      "Iteration: 173 lambda_k: 1 Loss: 0.38305084728135563\n",
      "Iteration: 174 lambda_k: 1 Loss: 0.382593258512194\n",
      "Iteration: 175 lambda_k: 1 Loss: 0.3821367196885231\n",
      "Iteration: 176 lambda_k: 1 Loss: 0.381681227486689\n",
      "Iteration: 177 lambda_k: 1 Loss: 0.38122677860118254\n",
      "Iteration: 178 lambda_k: 1 Loss: 0.3807733697441905\n",
      "Iteration: 179 lambda_k: 1 Loss: 0.38032099764516775\n",
      "Iteration: 180 lambda_k: 1 Loss: 0.37986965905043635\n",
      "Iteration: 181 lambda_k: 1 Loss: 0.37941935072280103\n",
      "Iteration: 182 lambda_k: 1 Loss: 0.3789700694411855\n",
      "Iteration: 183 lambda_k: 1 Loss: 0.3785218120002877\n",
      "Iteration: 184 lambda_k: 1 Loss: 0.3780745752102544\n",
      "Iteration: 185 lambda_k: 1 Loss: 0.37762835589637017\n",
      "Iteration: 186 lambda_k: 1 Loss: 0.37718315089876236\n",
      "Iteration: 187 lambda_k: 1 Loss: 0.3767389570721222\n",
      "Iteration: 188 lambda_k: 1 Loss: 0.3762957712854382\n",
      "Iteration: 189 lambda_k: 1 Loss: 0.37585359042174377\n",
      "Iteration: 190 lambda_k: 1 Loss: 0.3754124113778769\n",
      "Iteration: 191 lambda_k: 1 Loss: 0.37497223106425154\n",
      "Iteration: 192 lambda_k: 1 Loss: 0.37453304640464047\n",
      "Iteration: 193 lambda_k: 1 Loss: 0.37409485433596795\n",
      "Iteration: 194 lambda_k: 1 Loss: 0.37365765180811256\n",
      "Iteration: 195 lambda_k: 1 Loss: 0.37322143578371975\n",
      "Iteration: 196 lambda_k: 1 Loss: 0.3727862032380224\n",
      "Iteration: 197 lambda_k: 1 Loss: 0.37235195115867026\n",
      "Iteration: 198 lambda_k: 1 Loss: 0.3719186765455669\n",
      "Iteration: 199 lambda_k: 1 Loss: 0.37148637641071414\n",
      "Iteration: 200 lambda_k: 1 Loss: 0.37105504777806336\n",
      "Iteration: 201 lambda_k: 1 Loss: 0.3706246876833741\n",
      "Iteration: 202 lambda_k: 1 Loss: 0.37019529317407707\n",
      "Iteration: 203 lambda_k: 1 Loss: 0.3697668613091455\n",
      "Iteration: 204 lambda_k: 1 Loss: 0.3693393891589696\n",
      "Iteration: 205 lambda_k: 1 Loss: 0.3689128738052381\n",
      "Iteration: 206 lambda_k: 1 Loss: 0.3684873123408237\n",
      "Iteration: 207 lambda_k: 1 Loss: 0.3680627018696734\n",
      "Iteration: 208 lambda_k: 1 Loss: 0.3676390395067032\n",
      "Iteration: 209 lambda_k: 1 Loss: 0.3672163223776973\n",
      "Iteration: 210 lambda_k: 1 Loss: 0.3667945476192107\n",
      "Iteration: 211 lambda_k: 1 Loss: 0.3663737123784755\n",
      "Iteration: 212 lambda_k: 1 Loss: 0.3659538138133114\n",
      "Iteration: 213 lambda_k: 1 Loss: 0.3655348490920386\n",
      "Iteration: 214 lambda_k: 1 Loss: 0.36511681539339425\n",
      "Iteration: 215 lambda_k: 1 Loss: 0.36469970990645195\n",
      "Iteration: 216 lambda_k: 1 Loss: 0.36428352983054374\n",
      "Iteration: 217 lambda_k: 1 Loss: 0.363868272375185\n",
      "Iteration: 218 lambda_k: 1 Loss: 0.3634539347600011\n",
      "Iteration: 219 lambda_k: 1 Loss: 0.3630405142146578\n",
      "Iteration: 220 lambda_k: 1 Loss: 0.3626280079787922\n",
      "Iteration: 221 lambda_k: 1 Loss: 0.3622164133019472\n",
      "Iteration: 222 lambda_k: 1 Loss: 0.36180572744350675\n",
      "Iteration: 223 lambda_k: 1 Loss: 0.3613959476726341\n",
      "Iteration: 224 lambda_k: 1 Loss: 0.36098707126821056\n",
      "Iteration: 225 lambda_k: 1 Loss: 0.36057909551877737\n",
      "Iteration: 226 lambda_k: 1 Loss: 0.36017201772247776\n",
      "Iteration: 227 lambda_k: 1 Loss: 0.35976583518700134\n",
      "Iteration: 228 lambda_k: 1 Loss: 0.3593605452295297\n",
      "Iteration: 229 lambda_k: 1 Loss: 0.3589561451766834\n",
      "Iteration: 230 lambda_k: 1 Loss: 0.35855263236446994\n",
      "Iteration: 231 lambda_k: 1 Loss: 0.358150004138233\n",
      "Iteration: 232 lambda_k: 1 Loss: 0.3577482578526038\n",
      "Iteration: 233 lambda_k: 1 Loss: 0.35734739087145123\n",
      "Iteration: 234 lambda_k: 1 Loss: 0.35694740056783586\n",
      "Iteration: 235 lambda_k: 1 Loss: 0.35654828432396246\n",
      "Iteration: 236 lambda_k: 1 Loss: 0.3561500395311352\n",
      "Iteration: 237 lambda_k: 1 Loss: 0.3557526635897124\n",
      "Iteration: 238 lambda_k: 1 Loss: 0.35535615390906305\n",
      "Iteration: 239 lambda_k: 1 Loss: 0.3549605079075235\n",
      "Iteration: 240 lambda_k: 1 Loss: 0.3545657230123553\n",
      "Iteration: 241 lambda_k: 1 Loss: 0.3541717966597036\n",
      "Iteration: 242 lambda_k: 1 Loss: 0.3537787262945558\n",
      "Iteration: 243 lambda_k: 1 Loss: 0.3533865093707018\n",
      "Iteration: 244 lambda_k: 1 Loss: 0.35299514335069404\n",
      "Iteration: 245 lambda_k: 1 Loss: 0.352604625705808\n",
      "Iteration: 246 lambda_k: 1 Loss: 0.35221495391600405\n",
      "Iteration: 247 lambda_k: 1 Loss: 0.35182612546988945\n",
      "Iteration: 248 lambda_k: 1 Loss: 0.3514381378646803\n",
      "Iteration: 249 lambda_k: 1 Loss: 0.35105098860616474\n",
      "Iteration: 250 lambda_k: 1 Loss: 0.35066467520866623\n",
      "Iteration: 251 lambda_k: 1 Loss: 0.35027919519500744\n",
      "Iteration: 252 lambda_k: 1 Loss: 0.3498945460964744\n",
      "Iteration: 253 lambda_k: 1 Loss: 0.3495107254527807\n",
      "Iteration: 254 lambda_k: 1 Loss: 0.34912773081203535\n",
      "Iteration: 255 lambda_k: 1 Loss: 0.34874555973070476\n",
      "Iteration: 256 lambda_k: 1 Loss: 0.3483642097735748\n",
      "Iteration: 257 lambda_k: 1 Loss: 0.34798367851372425\n",
      "Iteration: 258 lambda_k: 1 Loss: 0.3476039635323427\n",
      "Iteration: 259 lambda_k: 1 Loss: 0.34722506241925827\n",
      "Iteration: 260 lambda_k: 1 Loss: 0.3468469727721038\n",
      "Iteration: 261 lambda_k: 1 Loss: 0.34646969219677554\n",
      "Iteration: 262 lambda_k: 1 Loss: 0.3460932183073024\n",
      "Iteration: 263 lambda_k: 1 Loss: 0.3457175487259677\n",
      "Iteration: 264 lambda_k: 1 Loss: 0.345342681082655\n",
      "Iteration: 265 lambda_k: 1 Loss: 0.34496861301571186\n",
      "Iteration: 266 lambda_k: 1 Loss: 0.344595342171347\n",
      "Iteration: 267 lambda_k: 1 Loss: 0.3442228662037347\n",
      "Iteration: 268 lambda_k: 1 Loss: 0.3438511827749844\n",
      "Iteration: 269 lambda_k: 1 Loss: 0.34348028955511\n",
      "Iteration: 270 lambda_k: 1 Loss: 0.34311018422199874\n",
      "Iteration: 271 lambda_k: 1 Loss: 0.34274086446102964\n",
      "Iteration: 272 lambda_k: 1 Loss: 0.34237232796643396\n",
      "Iteration: 273 lambda_k: 1 Loss: 0.3420045724392032\n",
      "Iteration: 274 lambda_k: 1 Loss: 0.34163759558841716\n",
      "Iteration: 275 lambda_k: 1 Loss: 0.3412713951308794\n",
      "Iteration: 276 lambda_k: 1 Loss: 0.3409059687910877\n",
      "Iteration: 277 lambda_k: 1 Loss: 0.34054131430120477\n",
      "Iteration: 278 lambda_k: 1 Loss: 0.34017742940102924\n",
      "Iteration: 279 lambda_k: 1 Loss: 0.3398143118379668\n",
      "Iteration: 280 lambda_k: 1 Loss: 0.3394519593670013\n",
      "Iteration: 281 lambda_k: 1 Loss: 0.3390903697506663\n",
      "Iteration: 282 lambda_k: 1 Loss: 0.3387295407590165\n",
      "Iteration: 283 lambda_k: 1 Loss: 0.3383694701695998\n",
      "Iteration: 284 lambda_k: 1 Loss: 0.3380101557674289\n",
      "Iteration: 285 lambda_k: 1 Loss: 0.3376515953449533\n",
      "Iteration: 286 lambda_k: 1 Loss: 0.33729378670203175\n",
      "Iteration: 287 lambda_k: 1 Loss: 0.3369367276459045\n",
      "Iteration: 288 lambda_k: 1 Loss: 0.336580415991195\n",
      "Iteration: 289 lambda_k: 1 Loss: 0.3362248495597649\n",
      "Iteration: 290 lambda_k: 1 Loss: 0.3358700261808634\n",
      "Iteration: 291 lambda_k: 1 Loss: 0.3355159436909827\n",
      "Iteration: 292 lambda_k: 1 Loss: 0.33516259993386\n",
      "Iteration: 293 lambda_k: 1 Loss: 0.334809992760451\n",
      "Iteration: 294 lambda_k: 1 Loss: 0.33445812002890296\n",
      "Iteration: 295 lambda_k: 1 Loss: 0.3341069796045284\n",
      "Iteration: 296 lambda_k: 1 Loss: 0.33375656935977854\n",
      "Iteration: 297 lambda_k: 1 Loss: 0.33340688717421696\n",
      "Iteration: 298 lambda_k: 1 Loss: 0.33305793093449315\n",
      "Iteration: 299 lambda_k: 1 Loss: 0.332709698534317\n",
      "Iteration: 300 lambda_k: 1 Loss: 0.33236218787443245\n",
      "Iteration: 301 lambda_k: 1 Loss: 0.33201539686259174\n",
      "Iteration: 302 lambda_k: 1 Loss: 0.3316693234135299\n",
      "Iteration: 303 lambda_k: 1 Loss: 0.331323965448939\n",
      "Iteration: 304 lambda_k: 1 Loss: 0.33097932089744264\n",
      "Iteration: 305 lambda_k: 1 Loss: 0.3306353876945708\n",
      "Iteration: 306 lambda_k: 1 Loss: 0.3302921637827345\n",
      "Iteration: 307 lambda_k: 1 Loss: 0.3299496471112008\n",
      "Iteration: 308 lambda_k: 1 Loss: 0.3296078356360675\n",
      "Iteration: 309 lambda_k: 1 Loss: 0.3292667273202387\n",
      "Iteration: 310 lambda_k: 1 Loss: 0.32892632013339984\n",
      "Iteration: 311 lambda_k: 1 Loss: 0.328586612051993\n",
      "Iteration: 312 lambda_k: 1 Loss: 0.32824760105919243\n",
      "Iteration: 313 lambda_k: 1 Loss: 0.32790928514488\n",
      "Iteration: 314 lambda_k: 1 Loss: 0.32757166230562107\n",
      "Iteration: 315 lambda_k: 1 Loss: 0.3272347305446403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 316 lambda_k: 1 Loss: 0.32689848787179737\n",
      "Iteration: 317 lambda_k: 1 Loss: 0.32656293230356304\n",
      "Iteration: 318 lambda_k: 1 Loss: 0.32622806186299524\n",
      "Iteration: 319 lambda_k: 1 Loss: 0.3258938745797155\n",
      "Iteration: 320 lambda_k: 1 Loss: 0.3255603684898853\n",
      "Iteration: 321 lambda_k: 1 Loss: 0.3252275416332716\n",
      "Iteration: 322 lambda_k: 1 Loss: 0.3248953920638625\n",
      "Iteration: 323 lambda_k: 1 Loss: 0.32456391783573846\n",
      "Iteration: 324 lambda_k: 1 Loss: 0.3242331170107218\n",
      "Iteration: 325 lambda_k: 1 Loss: 0.3239029876571997\n",
      "Iteration: 326 lambda_k: 1 Loss: 0.32357352785012705\n",
      "Iteration: 327 lambda_k: 1 Loss: 0.32324473567082124\n",
      "Iteration: 328 lambda_k: 1 Loss: 0.32291660920692367\n",
      "Iteration: 329 lambda_k: 1 Loss: 0.32258914655239906\n",
      "Iteration: 330 lambda_k: 1 Loss: 0.3222623458075228\n",
      "Iteration: 331 lambda_k: 1 Loss: 0.3219362050788585\n",
      "Iteration: 332 lambda_k: 1 Loss: 0.3216107224792345\n",
      "Iteration: 333 lambda_k: 1 Loss: 0.3212858961277205\n",
      "Iteration: 334 lambda_k: 1 Loss: 0.32096172414960533\n",
      "Iteration: 335 lambda_k: 1 Loss: 0.32063820467637505\n",
      "Iteration: 336 lambda_k: 1 Loss: 0.3203153358456907\n",
      "Iteration: 337 lambda_k: 1 Loss: 0.3199931158013666\n",
      "Iteration: 338 lambda_k: 1 Loss: 0.3196715426933481\n",
      "Iteration: 339 lambda_k: 1 Loss: 0.31935061467769\n",
      "Iteration: 340 lambda_k: 1 Loss: 0.3190303299165352\n",
      "Iteration: 341 lambda_k: 1 Loss: 0.3187106865780922\n",
      "Iteration: 342 lambda_k: 1 Loss: 0.31839168283661473\n",
      "Iteration: 343 lambda_k: 1 Loss: 0.3180733168723792\n",
      "Iteration: 344 lambda_k: 1 Loss: 0.31775558687166433\n",
      "Iteration: 345 lambda_k: 1 Loss: 0.3174384910267294\n",
      "Iteration: 346 lambda_k: 1 Loss: 0.3171220275357932\n",
      "Iteration: 347 lambda_k: 1 Loss: 0.31680619460301346\n",
      "Iteration: 348 lambda_k: 1 Loss: 0.316490990438465\n",
      "Iteration: 349 lambda_k: 1 Loss: 0.31617641325811957\n",
      "Iteration: 350 lambda_k: 1 Loss: 0.3158624612838247\n",
      "Iteration: 351 lambda_k: 1 Loss: 0.3155491327419558\n",
      "Iteration: 352 lambda_k: 1 Loss: 0.31523642586821526\n",
      "Iteration: 353 lambda_k: 1 Loss: 0.3149243389011738\n",
      "Iteration: 354 lambda_k: 1 Loss: 0.31461287008595373\n",
      "Iteration: 355 lambda_k: 1 Loss: 0.3143020176734677\n",
      "Iteration: 356 lambda_k: 1 Loss: 0.3139917799203871\n",
      "Iteration: 357 lambda_k: 1 Loss: 0.3136821550891162\n",
      "Iteration: 358 lambda_k: 1 Loss: 0.31337314144777106\n",
      "Iteration: 359 lambda_k: 1 Loss: 0.3130647372701606\n",
      "Iteration: 360 lambda_k: 1 Loss: 0.31275694083576683\n",
      "Iteration: 361 lambda_k: 1 Loss: 0.3124497504297252\n",
      "Iteration: 362 lambda_k: 1 Loss: 0.3121431643428048\n",
      "Iteration: 363 lambda_k: 1 Loss: 0.3118371808713887\n",
      "Iteration: 364 lambda_k: 1 Loss: 0.3115317983174539\n",
      "Iteration: 365 lambda_k: 1 Loss: 0.31122701498855276\n",
      "Iteration: 366 lambda_k: 1 Loss: 0.3109228291977927\n",
      "Iteration: 367 lambda_k: 1 Loss: 0.3106192392638172\n",
      "Iteration: 368 lambda_k: 1 Loss: 0.31031624351078646\n",
      "Iteration: 369 lambda_k: 1 Loss: 0.3100138402683587\n",
      "Iteration: 370 lambda_k: 1 Loss: 0.30971202787167024\n",
      "Iteration: 371 lambda_k: 1 Loss: 0.30941080466131704\n",
      "Iteration: 372 lambda_k: 1 Loss: 0.3091101689833356\n",
      "Iteration: 373 lambda_k: 1 Loss: 0.3088101191891842\n",
      "Iteration: 374 lambda_k: 1 Loss: 0.3085106536357239\n",
      "Iteration: 375 lambda_k: 1 Loss: 0.3082117706852001\n",
      "Iteration: 376 lambda_k: 1 Loss: 0.3079134687052238\n",
      "Iteration: 377 lambda_k: 1 Loss: 0.30761574606875286\n",
      "Iteration: 378 lambda_k: 1 Loss: 0.3073186011540739\n",
      "Iteration: 379 lambda_k: 1 Loss: 0.30702203234478354\n",
      "Iteration: 380 lambda_k: 1 Loss: 0.3067260380297704\n",
      "Iteration: 381 lambda_k: 1 Loss: 0.3064306166031967\n",
      "Iteration: 382 lambda_k: 1 Loss: 0.3061357664644798\n",
      "Iteration: 383 lambda_k: 1 Loss: 0.3058414860182748\n",
      "Iteration: 384 lambda_k: 1 Loss: 0.30554777367445574\n",
      "Iteration: 385 lambda_k: 1 Loss: 0.3052546278480982\n",
      "Iteration: 386 lambda_k: 1 Loss: 0.3049620469594612\n",
      "Iteration: 387 lambda_k: 1 Loss: 0.3046700294339693\n",
      "Iteration: 388 lambda_k: 1 Loss: 0.3043785737021953\n",
      "Iteration: 389 lambda_k: 1 Loss: 0.3040876781998417\n",
      "Iteration: 390 lambda_k: 1 Loss: 0.3037973413677244\n",
      "Iteration: 391 lambda_k: 1 Loss: 0.3035075616517538\n",
      "Iteration: 392 lambda_k: 1 Loss: 0.30321833750291843\n",
      "Iteration: 393 lambda_k: 1 Loss: 0.3029296673772669\n",
      "Iteration: 394 lambda_k: 1 Loss: 0.3026415497497491\n",
      "Iteration: 395 lambda_k: 1 Loss: 0.30235398304929373\n",
      "Iteration: 396 lambda_k: 1 Loss: 0.3020669657759027\n",
      "Iteration: 397 lambda_k: 1 Loss: 0.30178049640371857\n",
      "Iteration: 398 lambda_k: 1 Loss: 0.30149457341105895\n",
      "Iteration: 399 lambda_k: 1 Loss: 0.30120919528287693\n",
      "Iteration: 400 lambda_k: 1 Loss: 0.3009243605102536\n",
      "Iteration: 401 lambda_k: 1 Loss: 0.300640067589292\n",
      "Iteration: 402 lambda_k: 1 Loss: 0.3003563150208626\n",
      "Iteration: 403 lambda_k: 1 Loss: 0.3000731013106651\n",
      "Iteration: 404 lambda_k: 1 Loss: 0.29979042496927233\n",
      "Iteration: 405 lambda_k: 1 Loss: 0.2995082845121229\n",
      "Iteration: 406 lambda_k: 1 Loss: 0.2992266784595002\n",
      "Iteration: 407 lambda_k: 1 Loss: 0.2989456053365111\n",
      "Iteration: 408 lambda_k: 1 Loss: 0.29866506367306994\n",
      "Iteration: 409 lambda_k: 1 Loss: 0.2983850520038824\n",
      "Iteration: 410 lambda_k: 1 Loss: 0.29810556886842937\n",
      "Iteration: 411 lambda_k: 1 Loss: 0.29782661281095096\n",
      "Iteration: 412 lambda_k: 1 Loss: 0.29754818238043007\n",
      "Iteration: 413 lambda_k: 1 Loss: 0.29727027613057655\n",
      "Iteration: 414 lambda_k: 1 Loss: 0.2969928926198113\n",
      "Iteration: 415 lambda_k: 1 Loss: 0.2967160304150932\n",
      "Iteration: 416 lambda_k: 1 Loss: 0.29643968807775994\n",
      "Iteration: 417 lambda_k: 1 Loss: 0.29616386418240187\n",
      "Iteration: 418 lambda_k: 1 Loss: 0.2958885573064952\n",
      "Iteration: 419 lambda_k: 1 Loss: 0.2956137660319349\n",
      "Iteration: 420 lambda_k: 1 Loss: 0.29533948894501416\n",
      "Iteration: 421 lambda_k: 1 Loss: 0.2950657246366491\n",
      "Iteration: 422 lambda_k: 1 Loss: 0.2947924717023794\n",
      "Iteration: 423 lambda_k: 1 Loss: 0.29451972874232357\n",
      "Iteration: 424 lambda_k: 1 Loss: 0.29424749436115144\n",
      "Iteration: 425 lambda_k: 1 Loss: 0.29397576716806956\n",
      "Iteration: 426 lambda_k: 1 Loss: 0.29370454577680705\n",
      "Iteration: 427 lambda_k: 1 Loss: 0.29343382880560176\n",
      "Iteration: 428 lambda_k: 1 Loss: 0.2931636148771846\n",
      "Iteration: 429 lambda_k: 1 Loss: 0.29289390261876475\n",
      "Iteration: 430 lambda_k: 1 Loss: 0.2926246906620142\n",
      "Iteration: 431 lambda_k: 1 Loss: 0.29235597764305327\n",
      "Iteration: 432 lambda_k: 1 Loss: 0.29208776220243526\n",
      "Iteration: 433 lambda_k: 1 Loss: 0.2918200429851321\n",
      "Iteration: 434 lambda_k: 1 Loss: 0.2915528186405191\n",
      "Iteration: 435 lambda_k: 1 Loss: 0.2912860878223605\n",
      "Iteration: 436 lambda_k: 1 Loss: 0.29101984918879464\n",
      "Iteration: 437 lambda_k: 1 Loss: 0.2907541014023195\n",
      "Iteration: 438 lambda_k: 1 Loss: 0.2904888431297781\n",
      "Iteration: 439 lambda_k: 1 Loss: 0.29022407304234354\n",
      "Iteration: 440 lambda_k: 1 Loss: 0.2899597898155053\n",
      "Iteration: 441 lambda_k: 1 Loss: 0.2896959921290544\n",
      "Iteration: 442 lambda_k: 1 Loss: 0.28943267866706895\n",
      "Iteration: 443 lambda_k: 1 Loss: 0.2891698481179\n",
      "Iteration: 444 lambda_k: 1 Loss: 0.2889074991741573\n",
      "Iteration: 445 lambda_k: 1 Loss: 0.2886456305326952\n",
      "Iteration: 446 lambda_k: 1 Loss: 0.28838424089459813\n",
      "Iteration: 447 lambda_k: 1 Loss: 0.288123328965167\n",
      "Iteration: 448 lambda_k: 1 Loss: 0.2878628934539048\n",
      "Iteration: 449 lambda_k: 1 Loss: 0.28760293307450285\n",
      "Iteration: 450 lambda_k: 1 Loss: 0.2873434465448265\n",
      "Iteration: 451 lambda_k: 1 Loss: 0.2870844325869019\n",
      "Iteration: 452 lambda_k: 1 Loss: 0.28682588992690145\n",
      "Iteration: 453 lambda_k: 1 Loss: 0.2865678172951306\n",
      "Iteration: 454 lambda_k: 1 Loss: 0.2863102134260135\n",
      "Iteration: 455 lambda_k: 1 Loss: 0.28605307705808014\n",
      "Iteration: 456 lambda_k: 1 Loss: 0.2857964069339519\n",
      "Iteration: 457 lambda_k: 1 Loss: 0.2855402018003287\n",
      "Iteration: 458 lambda_k: 1 Loss: 0.2852844604079748\n",
      "Iteration: 459 lambda_k: 1 Loss: 0.28502918151170575\n",
      "Iteration: 460 lambda_k: 1 Loss: 0.2847743638703751\n",
      "Iteration: 461 lambda_k: 1 Loss: 0.2845200062468605\n",
      "Iteration: 462 lambda_k: 1 Loss: 0.2842661074080508\n",
      "Iteration: 463 lambda_k: 1 Loss: 0.2840126661248325\n",
      "Iteration: 464 lambda_k: 1 Loss: 0.28375968117207695\n",
      "Iteration: 465 lambda_k: 1 Loss: 0.2835071513286267\n",
      "Iteration: 466 lambda_k: 1 Loss: 0.2832550753772824\n",
      "Iteration: 467 lambda_k: 1 Loss: 0.28300345210479017\n",
      "Iteration: 468 lambda_k: 1 Loss: 0.2827522803018281\n",
      "Iteration: 469 lambda_k: 1 Loss: 0.2825015587629935\n",
      "Iteration: 470 lambda_k: 1 Loss: 0.2822512862867898\n",
      "Iteration: 471 lambda_k: 1 Loss: 0.28200146167561385\n",
      "Iteration: 472 lambda_k: 1 Loss: 0.281752083735743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 473 lambda_k: 1 Loss: 0.28150315127732217\n",
      "Iteration: 474 lambda_k: 1 Loss: 0.2812546631143513\n",
      "Iteration: 475 lambda_k: 1 Loss: 0.28100661806467253\n",
      "Iteration: 476 lambda_k: 1 Loss: 0.28075901494995764\n",
      "Iteration: 477 lambda_k: 1 Loss: 0.28051185259569517\n",
      "Iteration: 478 lambda_k: 1 Loss: 0.2802651298311784\n",
      "Iteration: 479 lambda_k: 1 Loss: 0.28001884548949224\n",
      "Iteration: 480 lambda_k: 1 Loss: 0.27977299840750103\n",
      "Iteration: 481 lambda_k: 1 Loss: 0.2795275874258363\n",
      "Iteration: 482 lambda_k: 1 Loss: 0.27928261138888383\n",
      "Iteration: 483 lambda_k: 1 Loss: 0.279038069144772\n",
      "Iteration: 484 lambda_k: 1 Loss: 0.2787939595453591\n",
      "Iteration: 485 lambda_k: 1 Loss: 0.27855028144622096\n",
      "Iteration: 486 lambda_k: 1 Loss: 0.2783070337066392\n",
      "Iteration: 487 lambda_k: 1 Loss: 0.27806421518958846\n",
      "Iteration: 488 lambda_k: 1 Loss: 0.27782182476172507\n",
      "Iteration: 489 lambda_k: 1 Loss: 0.27757986129337425\n",
      "Iteration: 490 lambda_k: 1 Loss: 0.27733832365851846\n",
      "Iteration: 491 lambda_k: 1 Loss: 0.27709721073478544\n",
      "Iteration: 492 lambda_k: 1 Loss: 0.27685652140343586\n",
      "Iteration: 493 lambda_k: 1 Loss: 0.27661625454935207\n",
      "Iteration: 494 lambda_k: 1 Loss: 0.2763764090610258\n",
      "Iteration: 495 lambda_k: 1 Loss: 0.27613698383054636\n",
      "Iteration: 496 lambda_k: 1 Loss: 0.2758979777535891\n",
      "Iteration: 497 lambda_k: 1 Loss: 0.27565938972940357\n",
      "Iteration: 498 lambda_k: 1 Loss: 0.27542121866080166\n",
      "Iteration: 499 lambda_k: 1 Loss: 0.2751834634541463\n",
      "Iteration: 500 lambda_k: 1 Loss: 0.2749461230193396\n",
      "Iteration: 501 lambda_k: 1 Loss: 0.27470919626981133\n",
      "Iteration: 502 lambda_k: 1 Loss: 0.2744726821225076\n",
      "Iteration: 503 lambda_k: 1 Loss: 0.2742365794978792\n",
      "Iteration: 504 lambda_k: 1 Loss: 0.27400088731987005\n",
      "Iteration: 505 lambda_k: 1 Loss: 0.2737656045159062\n",
      "Iteration: 506 lambda_k: 1 Loss: 0.27353073001688405\n",
      "Iteration: 507 lambda_k: 1 Loss: 0.2732962627571594\n",
      "Iteration: 508 lambda_k: 1 Loss: 0.2730622016745359\n",
      "Iteration: 509 lambda_k: 1 Loss: 0.272828545710254\n",
      "Iteration: 510 lambda_k: 1 Loss: 0.2725952938089798\n",
      "Iteration: 511 lambda_k: 1 Loss: 0.2723624449187936\n",
      "Iteration: 512 lambda_k: 1 Loss: 0.2721299979911791\n",
      "Iteration: 513 lambda_k: 1 Loss: 0.2718979519810123\n",
      "Iteration: 514 lambda_k: 1 Loss: 0.27166630584655027\n",
      "Iteration: 515 lambda_k: 1 Loss: 0.27143505854942046\n",
      "Iteration: 516 lambda_k: 1 Loss: 0.2712042090546094\n",
      "Iteration: 517 lambda_k: 1 Loss: 0.2709737563304521\n",
      "Iteration: 518 lambda_k: 1 Loss: 0.27074369934862114\n",
      "Iteration: 519 lambda_k: 1 Loss: 0.2705140370841154\n",
      "Iteration: 520 lambda_k: 1 Loss: 0.2702847685152501\n",
      "Iteration: 521 lambda_k: 1 Loss: 0.27005589262364516\n",
      "Iteration: 522 lambda_k: 1 Loss: 0.2698274083942152\n",
      "Iteration: 523 lambda_k: 1 Loss: 0.26959931481515836\n",
      "Iteration: 524 lambda_k: 1 Loss: 0.26937161087794587\n",
      "Iteration: 525 lambda_k: 1 Loss: 0.26914429557731145\n",
      "Iteration: 526 lambda_k: 1 Loss: 0.2689173679112408\n",
      "Iteration: 527 lambda_k: 1 Loss: 0.26869082688096096\n",
      "Iteration: 528 lambda_k: 1 Loss: 0.2684646714909299\n",
      "Iteration: 529 lambda_k: 1 Loss: 0.2682389007488259\n",
      "Iteration: 530 lambda_k: 1 Loss: 0.2680135136655375\n",
      "Iteration: 531 lambda_k: 1 Loss: 0.26778850925515263\n",
      "Iteration: 532 lambda_k: 1 Loss: 0.2675638865349487\n",
      "Iteration: 533 lambda_k: 1 Loss: 0.2673396445253822\n",
      "Iteration: 534 lambda_k: 1 Loss: 0.2671157822500783\n",
      "Iteration: 535 lambda_k: 1 Loss: 0.2668922987358206\n",
      "Iteration: 536 lambda_k: 1 Loss: 0.2666691930125412\n",
      "Iteration: 537 lambda_k: 1 Loss: 0.26644646411331036\n",
      "Iteration: 538 lambda_k: 1 Loss: 0.26622411107432625\n",
      "Iteration: 539 lambda_k: 1 Loss: 0.2660021329349053\n",
      "Iteration: 540 lambda_k: 1 Loss: 0.2657805287374716\n",
      "Iteration: 541 lambda_k: 1 Loss: 0.26555929752754737\n",
      "Iteration: 542 lambda_k: 1 Loss: 0.2653384383537426\n",
      "Iteration: 543 lambda_k: 1 Loss: 0.2651179502677455\n",
      "Iteration: 544 lambda_k: 1 Loss: 0.2648978323243123\n",
      "Iteration: 545 lambda_k: 1 Loss: 0.26467808358125755\n",
      "Iteration: 546 lambda_k: 1 Loss: 0.26445870309944414\n",
      "Iteration: 547 lambda_k: 1 Loss: 0.2642396899427737\n",
      "Iteration: 548 lambda_k: 1 Loss: 0.2640210431781766\n",
      "Iteration: 549 lambda_k: 1 Loss: 0.2638027618756026\n",
      "Iteration: 550 lambda_k: 1 Loss: 0.2635848451080106\n",
      "Iteration: 551 lambda_k: 1 Loss: 0.26336729195135966\n",
      "Iteration: 552 lambda_k: 1 Loss: 0.26315010148459883\n",
      "Iteration: 553 lambda_k: 1 Loss: 0.262933272789658\n",
      "Iteration: 554 lambda_k: 1 Loss: 0.2627168049514378\n",
      "Iteration: 555 lambda_k: 1 Loss: 0.26250069705780077\n",
      "Iteration: 556 lambda_k: 1 Loss: 0.26228494819956133\n",
      "Iteration: 557 lambda_k: 1 Loss: 0.2620695574451084\n",
      "Iteration: 558 lambda_k: 1 Loss: 0.261854523933286\n",
      "Iteration: 559 lambda_k: 1 Loss: 0.26163984675330665\n",
      "Iteration: 560 lambda_k: 1 Loss: 0.2614255250017436\n",
      "Iteration: 561 lambda_k: 1 Loss: 0.26121155778290645\n",
      "Iteration: 562 lambda_k: 1 Loss: 0.2609979442055815\n",
      "Iteration: 563 lambda_k: 1 Loss: 0.2607846833812265\n",
      "Iteration: 564 lambda_k: 1 Loss: 0.26057177442392193\n",
      "Iteration: 565 lambda_k: 1 Loss: 0.26035921645058036\n",
      "Iteration: 566 lambda_k: 1 Loss: 0.26014700858099854\n",
      "Iteration: 567 lambda_k: 1 Loss: 0.25993514993783373\n",
      "Iteration: 568 lambda_k: 1 Loss: 0.2597236396465829\n",
      "Iteration: 569 lambda_k: 1 Loss: 0.25951247683557316\n",
      "Iteration: 570 lambda_k: 1 Loss: 0.25930166063595456\n",
      "Iteration: 571 lambda_k: 1 Loss: 0.259091190181691\n",
      "Iteration: 572 lambda_k: 1 Loss: 0.2588810646095513\n",
      "Iteration: 573 lambda_k: 1 Loss: 0.2586712830590999\n",
      "Iteration: 574 lambda_k: 1 Loss: 0.258461844672688\n",
      "Iteration: 575 lambda_k: 1 Loss: 0.2582527485954446\n",
      "Iteration: 576 lambda_k: 1 Loss: 0.25804399397526745\n",
      "Iteration: 577 lambda_k: 1 Loss: 0.25783557996281437\n",
      "Iteration: 578 lambda_k: 1 Loss: 0.2576275057182672\n",
      "Iteration: 579 lambda_k: 1 Loss: 0.2574197703863156\n",
      "Iteration: 580 lambda_k: 1 Loss: 0.2572123731292515\n",
      "Iteration: 581 lambda_k: 1 Loss: 0.25700531310979785\n",
      "Iteration: 582 lambda_k: 1 Loss: 0.25679858949256723\n",
      "Iteration: 583 lambda_k: 1 Loss: 0.2565922014444766\n",
      "Iteration: 584 lambda_k: 1 Loss: 0.25638614813519306\n",
      "Iteration: 585 lambda_k: 1 Loss: 0.2561804287371419\n",
      "Iteration: 586 lambda_k: 1 Loss: 0.2559750424254427\n",
      "Iteration: 587 lambda_k: 1 Loss: 0.2557699883778823\n",
      "Iteration: 588 lambda_k: 1 Loss: 0.2555652657749086\n",
      "Iteration: 589 lambda_k: 1 Loss: 0.2553608737996254\n",
      "Iteration: 590 lambda_k: 1 Loss: 0.25515681163778436\n",
      "Iteration: 591 lambda_k: 1 Loss: 0.2549530784905289\n",
      "Iteration: 592 lambda_k: 1 Loss: 0.25474967352778266\n",
      "Iteration: 593 lambda_k: 1 Loss: 0.2545465959481239\n",
      "Iteration: 594 lambda_k: 1 Loss: 0.2543438449510721\n",
      "Iteration: 595 lambda_k: 1 Loss: 0.25414141973606696\n",
      "Iteration: 596 lambda_k: 1 Loss: 0.25393931950434384\n",
      "Iteration: 597 lambda_k: 1 Loss: 0.25373754345984195\n",
      "Iteration: 598 lambda_k: 1 Loss: 0.2535360908092104\n",
      "Iteration: 599 lambda_k: 1 Loss: 0.253334960761689\n",
      "Iteration: 600 lambda_k: 1 Loss: 0.2531341525290712\n",
      "Iteration: 601 lambda_k: 1 Loss: 0.25293366532570405\n",
      "Iteration: 602 lambda_k: 1 Loss: 0.2527334983325222\n",
      "Iteration: 603 lambda_k: 1 Loss: 0.2525336508283043\n",
      "Iteration: 604 lambda_k: 1 Loss: 0.2523341220206753\n",
      "Iteration: 605 lambda_k: 1 Loss: 0.25213491112557657\n",
      "Iteration: 606 lambda_k: 1 Loss: 0.2519360173684281\n",
      "Iteration: 607 lambda_k: 1 Loss: 0.2517374399793858\n",
      "Iteration: 608 lambda_k: 1 Loss: 0.25153917819080324\n",
      "Iteration: 609 lambda_k: 1 Loss: 0.2513412312371717\n",
      "Iteration: 610 lambda_k: 1 Loss: 0.2511435983554201\n",
      "Iteration: 611 lambda_k: 1 Loss: 0.2509462787849909\n",
      "Iteration: 612 lambda_k: 1 Loss: 0.2507492717678115\n",
      "Iteration: 613 lambda_k: 1 Loss: 0.2505525765482707\n",
      "Iteration: 614 lambda_k: 1 Loss: 0.25035619237320816\n",
      "Iteration: 615 lambda_k: 1 Loss: 0.2501601185007145\n",
      "Iteration: 616 lambda_k: 1 Loss: 0.2499643541675999\n",
      "Iteration: 617 lambda_k: 1 Loss: 0.24976889863227789\n",
      "Iteration: 618 lambda_k: 1 Loss: 0.249573751152986\n",
      "Iteration: 619 lambda_k: 1 Loss: 0.24937891098910153\n",
      "Iteration: 620 lambda_k: 1 Loss: 0.2491843774018768\n",
      "Iteration: 621 lambda_k: 1 Loss: 0.2489901496550358\n",
      "Iteration: 622 lambda_k: 1 Loss: 0.2487962270147847\n",
      "Iteration: 623 lambda_k: 1 Loss: 0.24860260874973086\n",
      "Iteration: 624 lambda_k: 1 Loss: 0.2484092941308516\n",
      "Iteration: 625 lambda_k: 1 Loss: 0.24821628243149077\n",
      "Iteration: 626 lambda_k: 1 Loss: 0.2480235729273556\n",
      "Iteration: 627 lambda_k: 1 Loss: 0.24783116489650947\n",
      "Iteration: 628 lambda_k: 1 Loss: 0.24763905761936456\n",
      "Iteration: 629 lambda_k: 1 Loss: 0.24744725037867402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 630 lambda_k: 1 Loss: 0.24725574245952506\n",
      "Iteration: 631 lambda_k: 1 Loss: 0.24706453314933136\n",
      "Iteration: 632 lambda_k: 1 Loss: 0.24687362173782598\n",
      "Iteration: 633 lambda_k: 1 Loss: 0.24668300751705435\n",
      "Iteration: 634 lambda_k: 1 Loss: 0.24649268978136682\n",
      "Iteration: 635 lambda_k: 1 Loss: 0.24630266782741161\n",
      "Iteration: 636 lambda_k: 1 Loss: 0.24611294095412764\n",
      "Iteration: 637 lambda_k: 1 Loss: 0.24592350846273733\n",
      "Iteration: 638 lambda_k: 1 Loss: 0.2457343696567399\n",
      "Iteration: 639 lambda_k: 1 Loss: 0.2455455238419038\n",
      "Iteration: 640 lambda_k: 1 Loss: 0.2453569703262602\n",
      "Iteration: 641 lambda_k: 1 Loss: 0.24516870842009544\n",
      "Iteration: 642 lambda_k: 1 Loss: 0.2449807374359448\n",
      "Iteration: 643 lambda_k: 1 Loss: 0.24479305668858503\n",
      "Iteration: 644 lambda_k: 1 Loss: 0.24460566549502757\n",
      "Iteration: 645 lambda_k: 1 Loss: 0.24441856317451222\n",
      "Iteration: 646 lambda_k: 1 Loss: 0.2442317490484995\n",
      "Iteration: 647 lambda_k: 1 Loss: 0.24404522244066487\n",
      "Iteration: 648 lambda_k: 1 Loss: 0.24385898267689154\n",
      "Iteration: 649 lambda_k: 1 Loss: 0.24367302908526395\n",
      "Iteration: 650 lambda_k: 1 Loss: 0.2434873609960615\n",
      "Iteration: 651 lambda_k: 1 Loss: 0.24330197774175197\n",
      "Iteration: 652 lambda_k: 1 Loss: 0.24311687865698528\n",
      "Iteration: 653 lambda_k: 1 Loss: 0.24293206307858733\n",
      "Iteration: 654 lambda_k: 1 Loss: 0.24274753034555405\n",
      "Iteration: 655 lambda_k: 1 Loss: 0.2425632797990456\n",
      "Iteration: 656 lambda_k: 1 Loss: 0.24237931078238048\n",
      "Iteration: 657 lambda_k: 1 Loss: 0.24219562264103056\n",
      "Iteration: 658 lambda_k: 1 Loss: 0.24201221472261547\n",
      "Iteration: 659 lambda_k: 1 Loss: 0.24182908637689815\n",
      "Iteration: 660 lambda_k: 1 Loss: 0.24164623695578064\n",
      "Iteration: 661 lambda_k: 1 Loss: 0.24146366581329967\n",
      "Iteration: 662 lambda_k: 1 Loss: 0.24128137230562358\n",
      "Iteration: 663 lambda_k: 1 Loss: 0.24109935579104982\n",
      "Iteration: 664 lambda_k: 1 Loss: 0.24091761563000272\n",
      "Iteration: 665 lambda_k: 1 Loss: 0.24073615118503267\n",
      "Iteration: 666 lambda_k: 1 Loss: 0.24055496182081632\n",
      "Iteration: 667 lambda_k: 1 Loss: 0.24037404690415803\n",
      "Iteration: 668 lambda_k: 1 Loss: 0.24019340575014858\n",
      "Iteration: 669 lambda_k: 1 Loss: 0.24001303781122976\n",
      "Iteration: 670 lambda_k: 1 Loss: 0.23983294244319842\n",
      "Iteration: 671 lambda_k: 1 Loss: 0.23965311900896166\n",
      "Iteration: 672 lambda_k: 1 Loss: 0.23947356688313604\n",
      "Iteration: 673 lambda_k: 1 Loss: 0.23929428544543546\n",
      "Iteration: 674 lambda_k: 1 Loss: 0.2391152740772459\n",
      "Iteration: 675 lambda_k: 1 Loss: 0.2389365321615168\n",
      "Iteration: 676 lambda_k: 1 Loss: 0.23875805908316008\n",
      "Iteration: 677 lambda_k: 1 Loss: 0.2385798542291546\n",
      "Iteration: 678 lambda_k: 1 Loss: 0.2384019169885124\n",
      "Iteration: 679 lambda_k: 1 Loss: 0.23822424675225165\n",
      "Iteration: 680 lambda_k: 1 Loss: 0.2380468429133923\n",
      "Iteration: 681 lambda_k: 1 Loss: 0.23786970486695536\n",
      "Iteration: 682 lambda_k: 1 Loss: 0.23769283200996108\n",
      "Iteration: 683 lambda_k: 1 Loss: 0.23751622374142625\n",
      "Iteration: 684 lambda_k: 1 Loss: 0.23733987946236332\n",
      "Iteration: 685 lambda_k: 1 Loss: 0.23716379857578299\n",
      "Iteration: 686 lambda_k: 1 Loss: 0.2369879804866992\n",
      "Iteration: 687 lambda_k: 1 Loss: 0.23681242460213925\n",
      "Iteration: 688 lambda_k: 1 Loss: 0.2366371303311602\n",
      "Iteration: 689 lambda_k: 1 Loss: 0.23646209708487442\n",
      "Iteration: 690 lambda_k: 1 Loss: 0.23628732427648622\n",
      "Iteration: 691 lambda_k: 1 Loss: 0.236112811321347\n",
      "Iteration: 692 lambda_k: 1 Loss: 0.23593855763703322\n",
      "Iteration: 693 lambda_k: 1 Loss: 0.2357645626434565\n",
      "Iteration: 694 lambda_k: 1 Loss: 0.23559082576301824\n",
      "Iteration: 695 lambda_k: 1 Loss: 0.23541734642082446\n",
      "Iteration: 696 lambda_k: 1 Loss: 0.23524412404498224\n",
      "Iteration: 697 lambda_k: 1 Loss: 0.235071157822993\n",
      "Iteration: 698 lambda_k: 1 Loss: 0.23489844752790742\n",
      "Iteration: 699 lambda_k: 1 Loss: 0.23472599252192275\n",
      "Iteration: 700 lambda_k: 1 Loss: 0.23455379218879235\n",
      "Iteration: 701 lambda_k: 1 Loss: 0.23438184599828418\n",
      "Iteration: 702 lambda_k: 1 Loss: 0.2342101533138073\n",
      "Iteration: 703 lambda_k: 1 Loss: 0.23403871359378534\n",
      "Iteration: 704 lambda_k: 1 Loss: 0.23386752628936872\n",
      "Iteration: 705 lambda_k: 1 Loss: 0.2336965908452806\n",
      "Iteration: 706 lambda_k: 1 Loss: 0.23352590670727094\n",
      "Iteration: 707 lambda_k: 1 Loss: 0.23335547332688542\n",
      "Iteration: 708 lambda_k: 1 Loss: 0.2331852901645978\n",
      "Iteration: 709 lambda_k: 1 Loss: 0.23301535669530685\n",
      "Iteration: 710 lambda_k: 1 Loss: 0.23284567241894233\n",
      "Iteration: 711 lambda_k: 1 Loss: 0.23267623603997348\n",
      "Iteration: 712 lambda_k: 1 Loss: 0.23250704794582422\n",
      "Iteration: 713 lambda_k: 1 Loss: 0.23233810758747198\n",
      "Iteration: 714 lambda_k: 1 Loss: 0.23216941414417316\n",
      "Iteration: 715 lambda_k: 1 Loss: 0.2320009672614652\n",
      "Iteration: 716 lambda_k: 1 Loss: 0.2318327654031579\n",
      "Iteration: 717 lambda_k: 1 Loss: 0.23166480939404532\n",
      "Iteration: 718 lambda_k: 1 Loss: 0.23149710084852015\n",
      "Iteration: 719 lambda_k: 1 Loss: 0.23132964107914106\n",
      "Iteration: 720 lambda_k: 1 Loss: 0.23116246280018202\n",
      "Iteration: 721 lambda_k: 1 Loss: 0.23099556355879156\n",
      "Iteration: 722 lambda_k: 1 Loss: 0.23082894452847044\n",
      "Iteration: 723 lambda_k: 1 Loss: 0.23066260373547598\n",
      "Iteration: 724 lambda_k: 1 Loss: 0.2304965400618835\n",
      "Iteration: 725 lambda_k: 1 Loss: 0.23033075152364207\n",
      "Iteration: 726 lambda_k: 1 Loss: 0.23016523642698833\n",
      "Iteration: 727 lambda_k: 1 Loss: 0.2299999932170727\n",
      "Iteration: 728 lambda_k: 1 Loss: 0.22983502009996348\n",
      "Iteration: 729 lambda_k: 1 Loss: 0.22967031552837605\n",
      "Iteration: 730 lambda_k: 1 Loss: 0.22950587792362936\n",
      "Iteration: 731 lambda_k: 1 Loss: 0.2293417057706114\n",
      "Iteration: 732 lambda_k: 1 Loss: 0.2291777974635245\n",
      "Iteration: 733 lambda_k: 1 Loss: 0.2290141515248508\n",
      "Iteration: 734 lambda_k: 1 Loss: 0.22885076646707495\n",
      "Iteration: 735 lambda_k: 1 Loss: 0.22868764081987936\n",
      "Iteration: 736 lambda_k: 1 Loss: 0.22852477312986236\n",
      "Iteration: 737 lambda_k: 1 Loss: 0.22836216195073483\n",
      "Iteration: 738 lambda_k: 1 Loss: 0.22819980587270808\n",
      "Iteration: 739 lambda_k: 1 Loss: 0.2280377035093271\n",
      "Iteration: 740 lambda_k: 1 Loss: 0.22787585343382016\n",
      "Iteration: 741 lambda_k: 1 Loss: 0.2277142542733172\n",
      "Iteration: 742 lambda_k: 1 Loss: 0.2275529046609248\n",
      "Iteration: 743 lambda_k: 1 Loss: 0.22739180323870156\n",
      "Iteration: 744 lambda_k: 1 Loss: 0.22723094866749008\n",
      "Iteration: 745 lambda_k: 1 Loss: 0.22707033961451192\n",
      "Iteration: 746 lambda_k: 1 Loss: 0.2269099747529534\n",
      "Iteration: 747 lambda_k: 1 Loss: 0.22674985277973003\n",
      "Iteration: 748 lambda_k: 1 Loss: 0.22658997240849976\n",
      "Iteration: 749 lambda_k: 1 Loss: 0.22643033232631196\n",
      "Iteration: 750 lambda_k: 1 Loss: 0.22627093125846817\n",
      "Iteration: 751 lambda_k: 1 Loss: 0.2261117679352626\n",
      "Iteration: 752 lambda_k: 1 Loss: 0.22595284109649094\n",
      "Iteration: 753 lambda_k: 1 Loss: 0.22579414949098592\n",
      "Iteration: 754 lambda_k: 1 Loss: 0.22563569187672547\n",
      "Iteration: 755 lambda_k: 1 Loss: 0.22547746702072832\n",
      "Iteration: 756 lambda_k: 1 Loss: 0.22531947369891675\n",
      "Iteration: 757 lambda_k: 1 Loss: 0.22516171069599847\n",
      "Iteration: 758 lambda_k: 1 Loss: 0.22500417680536142\n",
      "Iteration: 759 lambda_k: 1 Loss: 0.22484687082897092\n",
      "Iteration: 760 lambda_k: 1 Loss: 0.22468979157726857\n",
      "Iteration: 761 lambda_k: 1 Loss: 0.2245329378690732\n",
      "Iteration: 762 lambda_k: 1 Loss: 0.22437630853148668\n",
      "Iteration: 763 lambda_k: 1 Loss: 0.2242199023998037\n",
      "Iteration: 764 lambda_k: 1 Loss: 0.2240637183191434\n",
      "Iteration: 765 lambda_k: 1 Loss: 0.2239077551380556\n",
      "Iteration: 766 lambda_k: 1 Loss: 0.22375201171699288\n",
      "Iteration: 767 lambda_k: 1 Loss: 0.2235964869232123\n",
      "Iteration: 768 lambda_k: 1 Loss: 0.22344117963174592\n",
      "Iteration: 769 lambda_k: 1 Loss: 0.22328608872533426\n",
      "Iteration: 770 lambda_k: 1 Loss: 0.22313121309436362\n",
      "Iteration: 771 lambda_k: 1 Loss: 0.22297655163679916\n",
      "Iteration: 772 lambda_k: 1 Loss: 0.22282210325811702\n",
      "Iteration: 773 lambda_k: 1 Loss: 0.22266786687123866\n",
      "Iteration: 774 lambda_k: 1 Loss: 0.22251384139646627\n",
      "Iteration: 775 lambda_k: 1 Loss: 0.222360025761421\n",
      "Iteration: 776 lambda_k: 1 Loss: 0.22220641890098128\n",
      "Iteration: 777 lambda_k: 1 Loss: 0.22205301975722352\n",
      "Iteration: 778 lambda_k: 1 Loss: 0.22189982727936272\n",
      "Iteration: 779 lambda_k: 1 Loss: 0.22174684042369505\n",
      "Iteration: 780 lambda_k: 1 Loss: 0.2215940581535412\n",
      "Iteration: 781 lambda_k: 1 Loss: 0.2214414794391906\n",
      "Iteration: 782 lambda_k: 1 Loss: 0.22128910325784665\n",
      "Iteration: 783 lambda_k: 1 Loss: 0.22113692859357276\n",
      "Iteration: 784 lambda_k: 1 Loss: 0.22098495443723937\n",
      "Iteration: 785 lambda_k: 1 Loss: 0.22083317978647177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 786 lambda_k: 1 Loss: 0.22068160364559836\n",
      "Iteration: 787 lambda_k: 1 Loss: 0.22053022502560013\n",
      "Iteration: 788 lambda_k: 1 Loss: 0.2203790429440606\n",
      "Iteration: 789 lambda_k: 1 Loss: 0.2202280564251165\n",
      "Iteration: 790 lambda_k: 1 Loss: 0.22007726449940915\n",
      "Iteration: 791 lambda_k: 1 Loss: 0.21992666620403617\n",
      "Iteration: 792 lambda_k: 1 Loss: 0.21977626058250443\n",
      "Iteration: 793 lambda_k: 1 Loss: 0.21962604668468316\n",
      "Iteration: 794 lambda_k: 1 Loss: 0.2194760235667576\n",
      "Iteration: 795 lambda_k: 1 Loss: 0.21932619029118375\n",
      "Iteration: 796 lambda_k: 1 Loss: 0.2191765459266427\n",
      "Iteration: 797 lambda_k: 1 Loss: 0.21902708954799666\n",
      "Iteration: 798 lambda_k: 1 Loss: 0.2188778202362447\n",
      "Iteration: 799 lambda_k: 1 Loss: 0.21872873707847934\n",
      "Iteration: 800 lambda_k: 1 Loss: 0.21857983916784338\n",
      "Iteration: 801 lambda_k: 1 Loss: 0.21843112560348768\n",
      "Iteration: 802 lambda_k: 1 Loss: 0.21828259549052897\n",
      "Iteration: 803 lambda_k: 1 Loss: 0.21813424794000805\n",
      "Iteration: 804 lambda_k: 1 Loss: 0.21798608206884917\n",
      "Iteration: 805 lambda_k: 1 Loss: 0.21783809699981907\n",
      "Iteration: 806 lambda_k: 1 Loss: 0.21769029186148658\n",
      "Iteration: 807 lambda_k: 1 Loss: 0.2175426657881832\n",
      "Iteration: 808 lambda_k: 1 Loss: 0.2173952179199634\n",
      "Iteration: 809 lambda_k: 1 Loss: 0.21724794740256564\n",
      "Iteration: 810 lambda_k: 1 Loss: 0.21710085338737398\n",
      "Iteration: 811 lambda_k: 1 Loss: 0.21695393503137966\n",
      "Iteration: 812 lambda_k: 1 Loss: 0.21680719149714353\n",
      "Iteration: 813 lambda_k: 1 Loss: 0.21666062195275806\n",
      "Iteration: 814 lambda_k: 1 Loss: 0.21651422557181096\n",
      "Iteration: 815 lambda_k: 1 Loss: 0.21636800153334793\n",
      "Iteration: 816 lambda_k: 1 Loss: 0.21622194902183645\n",
      "Iteration: 817 lambda_k: 1 Loss: 0.2160760672271298\n",
      "Iteration: 818 lambda_k: 1 Loss: 0.21593035534443128\n",
      "Iteration: 819 lambda_k: 1 Loss: 0.21578481257425908\n",
      "Iteration: 820 lambda_k: 1 Loss: 0.21563943812241063\n",
      "Iteration: 821 lambda_k: 1 Loss: 0.21549423119992878\n",
      "Iteration: 822 lambda_k: 1 Loss: 0.21534919102306654\n",
      "Iteration: 823 lambda_k: 1 Loss: 0.21520431681325397\n",
      "Iteration: 824 lambda_k: 1 Loss: 0.215059607797064\n",
      "Iteration: 825 lambda_k: 1 Loss: 0.21491506320617804\n",
      "Iteration: 826 lambda_k: 1 Loss: 0.21477068227735355\n",
      "Iteration: 827 lambda_k: 1 Loss: 0.21462646425239126\n",
      "Iteration: 828 lambda_k: 1 Loss: 0.2144824083781026\n",
      "Iteration: 829 lambda_k: 1 Loss: 0.2143385139062772\n",
      "Iteration: 830 lambda_k: 1 Loss: 0.21419478009098003\n",
      "Iteration: 831 lambda_k: 1 Loss: 0.21405120619795884\n",
      "Iteration: 832 lambda_k: 1 Loss: 0.2139077914924216\n",
      "Iteration: 833 lambda_k: 1 Loss: 0.21376453524580616\n",
      "Iteration: 834 lambda_k: 1 Loss: 0.21362143673417022\n",
      "Iteration: 835 lambda_k: 1 Loss: 0.21347849523852647\n",
      "Iteration: 836 lambda_k: 1 Loss: 0.21333571004466192\n",
      "Iteration: 837 lambda_k: 1 Loss: 0.21319308044306606\n",
      "Iteration: 838 lambda_k: 1 Loss: 0.21305060572891363\n",
      "Iteration: 839 lambda_k: 1 Loss: 0.21290828520204863\n",
      "Iteration: 840 lambda_k: 1 Loss: 0.2127661181669582\n",
      "Iteration: 841 lambda_k: 1 Loss: 0.21262410393274167\n",
      "Iteration: 842 lambda_k: 1 Loss: 0.21248224181308023\n",
      "Iteration: 843 lambda_k: 1 Loss: 0.21234053112620788\n",
      "Iteration: 844 lambda_k: 1 Loss: 0.21219897119488282\n",
      "Iteration: 845 lambda_k: 1 Loss: 0.21205756134635914\n",
      "Iteration: 846 lambda_k: 1 Loss: 0.21191630091235872\n",
      "Iteration: 847 lambda_k: 1 Loss: 0.2117751892290435\n",
      "Iteration: 848 lambda_k: 1 Loss: 0.2116342256369871\n",
      "Iteration: 849 lambda_k: 1 Loss: 0.211493409481148\n",
      "Iteration: 850 lambda_k: 1 Loss: 0.21135274011084165\n",
      "Iteration: 851 lambda_k: 1 Loss: 0.21121221687971364\n",
      "Iteration: 852 lambda_k: 1 Loss: 0.21107183914571273\n",
      "Iteration: 853 lambda_k: 1 Loss: 0.2109316062710641\n",
      "Iteration: 854 lambda_k: 1 Loss: 0.210791517622243\n",
      "Iteration: 855 lambda_k: 1 Loss: 0.2106515725699482\n",
      "Iteration: 856 lambda_k: 1 Loss: 0.21051177048907602\n",
      "Iteration: 857 lambda_k: 1 Loss: 0.21037211075869425\n",
      "Iteration: 858 lambda_k: 1 Loss: 0.21023259276201667\n",
      "Iteration: 859 lambda_k: 1 Loss: 0.21009321588637725\n",
      "Iteration: 860 lambda_k: 1 Loss: 0.20995397952320494\n",
      "Iteration: 861 lambda_k: 1 Loss: 0.20981488306799842\n",
      "Iteration: 862 lambda_k: 1 Loss: 0.20967592592030118\n",
      "Iteration: 863 lambda_k: 1 Loss: 0.20953710748367638\n",
      "Iteration: 864 lambda_k: 1 Loss: 0.2093984271656828\n",
      "Iteration: 865 lambda_k: 1 Loss: 0.20925988437784987\n",
      "Iteration: 866 lambda_k: 1 Loss: 0.20912147853565338\n",
      "Iteration: 867 lambda_k: 1 Loss: 0.20898320905849171\n",
      "Iteration: 868 lambda_k: 1 Loss: 0.20884507536966168\n",
      "Iteration: 869 lambda_k: 1 Loss: 0.20870707689633466\n",
      "Iteration: 870 lambda_k: 1 Loss: 0.2085692130695332\n",
      "Iteration: 871 lambda_k: 1 Loss: 0.2084314833241071\n",
      "Iteration: 872 lambda_k: 1 Loss: 0.20829388709871083\n",
      "Iteration: 873 lambda_k: 1 Loss: 0.20815642383577979\n",
      "Iteration: 874 lambda_k: 1 Loss: 0.2080190929815078\n",
      "Iteration: 875 lambda_k: 1 Loss: 0.20788189398582382\n",
      "Iteration: 876 lambda_k: 1 Loss: 0.2077448263023697\n",
      "Iteration: 877 lambda_k: 1 Loss: 0.20760788938847743\n",
      "Iteration: 878 lambda_k: 1 Loss: 0.20747108270514686\n",
      "Iteration: 879 lambda_k: 1 Loss: 0.20733440571702375\n",
      "Iteration: 880 lambda_k: 1 Loss: 0.20719785789237727\n",
      "Iteration: 881 lambda_k: 1 Loss: 0.20706143870307855\n",
      "Iteration: 882 lambda_k: 1 Loss: 0.20692514762457873\n",
      "Iteration: 883 lambda_k: 1 Loss: 0.20678898413588723\n",
      "Iteration: 884 lambda_k: 1 Loss: 0.2066529477195508\n",
      "Iteration: 885 lambda_k: 1 Loss: 0.20651703786428588\n",
      "Iteration: 886 lambda_k: 1 Loss: 0.2063812540554052\n",
      "Iteration: 887 lambda_k: 1 Loss: 0.20624559578752164\n",
      "Iteration: 888 lambda_k: 1 Loss: 0.20611006255697237\n",
      "Iteration: 889 lambda_k: 1 Loss: 0.2059746538638208\n",
      "Iteration: 890 lambda_k: 1 Loss: 0.2058393692113753\n",
      "Iteration: 891 lambda_k: 1 Loss: 0.20570420810631918\n",
      "Iteration: 892 lambda_k: 1 Loss: 0.2055691700587335\n",
      "Iteration: 893 lambda_k: 1 Loss: 0.20543425458312678\n",
      "Iteration: 894 lambda_k: 1 Loss: 0.20529946119359613\n",
      "Iteration: 895 lambda_k: 1 Loss: 0.2051647894118798\n",
      "Iteration: 896 lambda_k: 1 Loss: 0.20503023876173893\n",
      "Iteration: 897 lambda_k: 1 Loss: 0.20489580876954383\n",
      "Iteration: 898 lambda_k: 1 Loss: 0.20476149896501347\n",
      "Iteration: 899 lambda_k: 1 Loss: 0.2046273088812543\n",
      "Iteration: 900 lambda_k: 1 Loss: 0.20449323805463207\n",
      "Iteration: 901 lambda_k: 1 Loss: 0.20435928602469414\n",
      "Iteration: 902 lambda_k: 1 Loss: 0.20422545233414754\n",
      "Iteration: 903 lambda_k: 1 Loss: 0.2040917365288512\n",
      "Iteration: 904 lambda_k: 1 Loss: 0.20395813815780184\n",
      "Iteration: 905 lambda_k: 1 Loss: 0.20382465677311593\n",
      "Iteration: 906 lambda_k: 1 Loss: 0.20369129193000934\n",
      "Iteration: 907 lambda_k: 1 Loss: 0.20355804318677959\n",
      "Iteration: 908 lambda_k: 1 Loss: 0.2034249101047864\n",
      "Iteration: 909 lambda_k: 1 Loss: 0.2032918922484346\n",
      "Iteration: 910 lambda_k: 1 Loss: 0.2031589891851556\n",
      "Iteration: 911 lambda_k: 1 Loss: 0.20302620048538972\n",
      "Iteration: 912 lambda_k: 1 Loss: 0.20289352572256805\n",
      "Iteration: 913 lambda_k: 1 Loss: 0.20276096447309502\n",
      "Iteration: 914 lambda_k: 1 Loss: 0.20262851631633091\n",
      "Iteration: 915 lambda_k: 1 Loss: 0.20249618083457407\n",
      "Iteration: 916 lambda_k: 1 Loss: 0.20236395761304385\n",
      "Iteration: 917 lambda_k: 1 Loss: 0.202231846239863\n",
      "Iteration: 918 lambda_k: 1 Loss: 0.2020998463060411\n",
      "Iteration: 919 lambda_k: 1 Loss: 0.20196795740545698\n",
      "Iteration: 920 lambda_k: 1 Loss: 0.20183617913484195\n",
      "Iteration: 921 lambda_k: 1 Loss: 0.20170451109376336\n",
      "Iteration: 922 lambda_k: 1 Loss: 0.20157295288460747\n",
      "Iteration: 923 lambda_k: 1 Loss: 0.2014415041125629\n",
      "Iteration: 924 lambda_k: 1 Loss: 0.2013101643856045\n",
      "Iteration: 925 lambda_k: 1 Loss: 0.20117893331447653\n",
      "Iteration: 926 lambda_k: 1 Loss: 0.20104781051267673\n",
      "Iteration: 927 lambda_k: 1 Loss: 0.20091679559643988\n",
      "Iteration: 928 lambda_k: 1 Loss: 0.20078588818472198\n",
      "Iteration: 929 lambda_k: 1 Loss: 0.2006550878991841\n",
      "Iteration: 930 lambda_k: 1 Loss: 0.20052439436417666\n",
      "Iteration: 931 lambda_k: 1 Loss: 0.20039380720672362\n",
      "Iteration: 932 lambda_k: 1 Loss: 0.20026332605650674\n",
      "Iteration: 933 lambda_k: 1 Loss: 0.20013295054585015\n",
      "Iteration: 934 lambda_k: 1 Loss: 0.2000026803097048\n",
      "Iteration: 935 lambda_k: 1 Loss: 0.19987251498563322\n",
      "Iteration: 936 lambda_k: 1 Loss: 0.19974245421379402\n",
      "Iteration: 937 lambda_k: 1 Loss: 0.19961249763692704\n",
      "Iteration: 938 lambda_k: 1 Loss: 0.19948264490033804\n",
      "Iteration: 939 lambda_k: 1 Loss: 0.19935289565188385\n",
      "Iteration: 940 lambda_k: 1 Loss: 0.19922324954195741\n",
      "Iteration: 941 lambda_k: 1 Loss: 0.1990937062234731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 942 lambda_k: 1 Loss: 0.1989642653518519\n",
      "Iteration: 943 lambda_k: 1 Loss: 0.19883492658500698\n",
      "Iteration: 944 lambda_k: 1 Loss: 0.19870568958332901\n",
      "Iteration: 945 lambda_k: 1 Loss: 0.19857655400967195\n",
      "Iteration: 946 lambda_k: 1 Loss: 0.19844751952933848\n",
      "Iteration: 947 lambda_k: 1 Loss: 0.19831858581006606\n",
      "Iteration: 948 lambda_k: 1 Loss: 0.19818975252201254\n",
      "Iteration: 949 lambda_k: 1 Loss: 0.1980610193377423\n",
      "Iteration: 950 lambda_k: 1 Loss: 0.19793238593221207\n",
      "Iteration: 951 lambda_k: 1 Loss: 0.19780385198275757\n",
      "Iteration: 952 lambda_k: 1 Loss: 0.19767541716907888\n",
      "Iteration: 953 lambda_k: 1 Loss: 0.19754708117322747\n",
      "Iteration: 954 lambda_k: 1 Loss: 0.1974188436795925\n",
      "Iteration: 955 lambda_k: 1 Loss: 0.19729070437488683\n",
      "Iteration: 956 lambda_k: 1 Loss: 0.19716266294813414\n",
      "Iteration: 957 lambda_k: 1 Loss: 0.1970347190906552\n",
      "Iteration: 958 lambda_k: 1 Loss: 0.19690687249605493\n",
      "Iteration: 959 lambda_k: 1 Loss: 0.19677912286020874\n",
      "Iteration: 960 lambda_k: 1 Loss: 0.19665146988125004\n",
      "Iteration: 961 lambda_k: 1 Loss: 0.19652391325955665\n",
      "Iteration: 962 lambda_k: 1 Loss: 0.19639645269773826\n",
      "Iteration: 963 lambda_k: 1 Loss: 0.19626908790062345\n",
      "Iteration: 964 lambda_k: 1 Loss: 0.1961418185752468\n",
      "Iteration: 965 lambda_k: 1 Loss: 0.19601464443083627\n",
      "Iteration: 966 lambda_k: 1 Loss: 0.19588756517880096\n",
      "Iteration: 967 lambda_k: 1 Loss: 0.19576058053271783\n",
      "Iteration: 968 lambda_k: 1 Loss: 0.1956336902083199\n",
      "Iteration: 969 lambda_k: 1 Loss: 0.1955068939234835\n",
      "Iteration: 970 lambda_k: 1 Loss: 0.1953801913982161\n",
      "Iteration: 971 lambda_k: 1 Loss: 0.19525358235464435\n",
      "Iteration: 972 lambda_k: 1 Loss: 0.19512706651700143\n",
      "Iteration: 973 lambda_k: 1 Loss: 0.19500064361161523\n",
      "Iteration: 974 lambda_k: 1 Loss: 0.19487431336689656\n",
      "Iteration: 975 lambda_k: 1 Loss: 0.19474807551332693\n",
      "Iteration: 976 lambda_k: 1 Loss: 0.19462192978344672\n",
      "Iteration: 977 lambda_k: 1 Loss: 0.19449587591184378\n",
      "Iteration: 978 lambda_k: 1 Loss: 0.19436991363514125\n",
      "Iteration: 979 lambda_k: 1 Loss: 0.19424404269198636\n",
      "Iteration: 980 lambda_k: 1 Loss: 0.19411826282303846\n",
      "Iteration: 981 lambda_k: 1 Loss: 0.19399257377095805\n",
      "Iteration: 982 lambda_k: 1 Loss: 0.19386697528039495\n",
      "Iteration: 983 lambda_k: 1 Loss: 0.19374146709797715\n",
      "Iteration: 984 lambda_k: 1 Loss: 0.19361604897229964\n",
      "Iteration: 985 lambda_k: 1 Loss: 0.19349072065391298\n",
      "Iteration: 986 lambda_k: 1 Loss: 0.19336548189531247\n",
      "Iteration: 987 lambda_k: 1 Loss: 0.19324033245092684\n",
      "Iteration: 988 lambda_k: 1 Loss: 0.19311527207710735\n",
      "Iteration: 989 lambda_k: 1 Loss: 0.19299030053211713\n",
      "Iteration: 990 lambda_k: 1 Loss: 0.19286541757612002\n",
      "Iteration: 991 lambda_k: 1 Loss: 0.1927406229711698\n",
      "Iteration: 992 lambda_k: 1 Loss: 0.1926159164811998\n",
      "Iteration: 993 lambda_k: 1 Loss: 0.19249129787201186\n",
      "Iteration: 994 lambda_k: 1 Loss: 0.19236676691126606\n",
      "Iteration: 995 lambda_k: 1 Loss: 0.19224232336847003\n",
      "Iteration: 996 lambda_k: 1 Loss: 0.1921179670149686\n",
      "Iteration: 997 lambda_k: 1 Loss: 0.1919936976239335\n",
      "Iteration: 998 lambda_k: 1 Loss: 0.1918695149703529\n",
      "Iteration: 999 lambda_k: 1 Loss: 0.1917454188310212\n",
      "Iteration: 1000 lambda_k: 1 Loss: 0.19162140898452887\n",
      "Iteration: 1001 lambda_k: 1 Loss: 0.19149748521125226\n",
      "Iteration: 1002 lambda_k: 1 Loss: 0.1913736472933438\n",
      "Iteration: 1003 lambda_k: 1 Loss: 0.1912498950147217\n",
      "Iteration: 1004 lambda_k: 1 Loss: 0.19112622816106015\n",
      "Iteration: 1005 lambda_k: 1 Loss: 0.19100264651977944\n",
      "Iteration: 1006 lambda_k: 1 Loss: 0.19087914988003626\n",
      "Iteration: 1007 lambda_k: 1 Loss: 0.19075573803271362\n",
      "Iteration: 1008 lambda_k: 1 Loss: 0.19063241077041157\n",
      "Iteration: 1009 lambda_k: 1 Loss: 0.19050916788743746\n",
      "Iteration: 1010 lambda_k: 1 Loss: 0.19038600917979606\n",
      "Iteration: 1011 lambda_k: 1 Loss: 0.1902629344451805\n",
      "Iteration: 1012 lambda_k: 1 Loss: 0.19013994348296265\n",
      "Iteration: 1013 lambda_k: 1 Loss: 0.1900170360941836\n",
      "Iteration: 1014 lambda_k: 1 Loss: 0.1898942120815445\n",
      "Iteration: 1015 lambda_k: 1 Loss: 0.18977147124939722\n",
      "Iteration: 1016 lambda_k: 1 Loss: 0.18964881340373502\n",
      "Iteration: 1017 lambda_k: 1 Loss: 0.1895262383521838\n",
      "Iteration: 1018 lambda_k: 1 Loss: 0.18940374590399248\n",
      "Iteration: 1019 lambda_k: 1 Loss: 0.18928133587002427\n",
      "Iteration: 1020 lambda_k: 1 Loss: 0.18915900806274755\n",
      "Iteration: 1021 lambda_k: 1 Loss: 0.18903676229622707\n",
      "Iteration: 1022 lambda_k: 1 Loss: 0.1889145983861149\n",
      "Iteration: 1023 lambda_k: 1 Loss: 0.1887925161496417\n",
      "Iteration: 1024 lambda_k: 1 Loss: 0.1886705154056081\n",
      "Iteration: 1025 lambda_k: 1 Loss: 0.18854859597437565\n",
      "Iteration: 1026 lambda_k: 1 Loss: 0.18842675767785844\n",
      "Iteration: 1027 lambda_k: 1 Loss: 0.1883050003395143\n",
      "Iteration: 1028 lambda_k: 1 Loss: 0.18818332378433647\n",
      "Iteration: 1029 lambda_k: 1 Loss: 0.18806172783884495\n",
      "Iteration: 1030 lambda_k: 1 Loss: 0.1879402123310779\n",
      "Iteration: 1031 lambda_k: 1 Loss: 0.18781877709058367\n",
      "Iteration: 1032 lambda_k: 1 Loss: 0.18769742194841205\n",
      "Iteration: 1033 lambda_k: 1 Loss: 0.18757614673710613\n",
      "Iteration: 1034 lambda_k: 1 Loss: 0.187454951290694\n",
      "Iteration: 1035 lambda_k: 1 Loss: 0.18733383544468074\n",
      "Iteration: 1036 lambda_k: 1 Loss: 0.18721279903604018\n",
      "Iteration: 1037 lambda_k: 1 Loss: 0.18709184190320646\n",
      "Iteration: 1038 lambda_k: 1 Loss: 0.1869709638860666\n",
      "Iteration: 1039 lambda_k: 1 Loss: 0.18685016482595213\n",
      "Iteration: 1040 lambda_k: 1 Loss: 0.18672944456563126\n",
      "Iteration: 1041 lambda_k: 1 Loss: 0.18660880294930068\n",
      "Iteration: 1042 lambda_k: 1 Loss: 0.18648823982257845\n",
      "Iteration: 1043 lambda_k: 1 Loss: 0.18636775503249534\n",
      "Iteration: 1044 lambda_k: 1 Loss: 0.18624734842748744\n",
      "Iteration: 1045 lambda_k: 1 Loss: 0.18612701985738875\n",
      "Iteration: 1046 lambda_k: 1 Loss: 0.1860067691734231\n",
      "Iteration: 1047 lambda_k: 1 Loss: 0.18588659622819675\n",
      "Iteration: 1048 lambda_k: 1 Loss: 0.18576650087569055\n",
      "Iteration: 1049 lambda_k: 1 Loss: 0.18564648297125297\n",
      "Iteration: 1050 lambda_k: 1 Loss: 0.1855265423715919\n",
      "Iteration: 1051 lambda_k: 1 Loss: 0.18540667893476798\n",
      "Iteration: 1052 lambda_k: 1 Loss: 0.1852868925201865\n",
      "Iteration: 1053 lambda_k: 1 Loss: 0.18516718298859078\n",
      "Iteration: 1054 lambda_k: 1 Loss: 0.18504755020205416\n",
      "Iteration: 1055 lambda_k: 1 Loss: 0.1849279940239732\n",
      "Iteration: 1056 lambda_k: 1 Loss: 0.18480851431906042\n",
      "Iteration: 1057 lambda_k: 1 Loss: 0.184689110953337\n",
      "Iteration: 1058 lambda_k: 1 Loss: 0.18456978379412586\n",
      "Iteration: 1059 lambda_k: 1 Loss: 0.18445053271004422\n",
      "Iteration: 1060 lambda_k: 1 Loss: 0.18433135757099708\n",
      "Iteration: 1061 lambda_k: 1 Loss: 0.1842122582481698\n",
      "Iteration: 1062 lambda_k: 1 Loss: 0.18409323461402133\n",
      "Iteration: 1063 lambda_k: 1 Loss: 0.18397428654227718\n",
      "Iteration: 1064 lambda_k: 1 Loss: 0.1838554139079229\n",
      "Iteration: 1065 lambda_k: 1 Loss: 0.1837366165871967\n",
      "Iteration: 1066 lambda_k: 1 Loss: 0.18361789445758317\n",
      "Iteration: 1067 lambda_k: 1 Loss: 0.18349924739780624\n",
      "Iteration: 1068 lambda_k: 1 Loss: 0.1833806752878225\n",
      "Iteration: 1069 lambda_k: 1 Loss: 0.1832621780088148\n",
      "Iteration: 1070 lambda_k: 1 Loss: 0.18314375544318526\n",
      "Iteration: 1071 lambda_k: 1 Loss: 0.1830254074745488\n",
      "Iteration: 1072 lambda_k: 1 Loss: 0.1829071339877268\n",
      "Iteration: 1073 lambda_k: 1 Loss: 0.18278893486874026\n",
      "Iteration: 1074 lambda_k: 1 Loss: 0.18267081000480365\n",
      "Iteration: 1075 lambda_k: 1 Loss: 0.1825527592843183\n",
      "Iteration: 1076 lambda_k: 1 Loss: 0.18243478259686596\n",
      "Iteration: 1077 lambda_k: 1 Loss: 0.18231687983320252\n",
      "Iteration: 1078 lambda_k: 1 Loss: 0.18219905088525185\n",
      "Iteration: 1079 lambda_k: 1 Loss: 0.1820812956460991\n",
      "Iteration: 1080 lambda_k: 1 Loss: 0.18196361400998504\n",
      "Iteration: 1081 lambda_k: 1 Loss: 0.18184600587229932\n",
      "Iteration: 1082 lambda_k: 1 Loss: 0.18172847112957452\n",
      "Iteration: 1083 lambda_k: 1 Loss: 0.18161100967948005\n",
      "Iteration: 1084 lambda_k: 1 Loss: 0.181493621420816\n",
      "Iteration: 1085 lambda_k: 1 Loss: 0.1813763062535071\n",
      "Iteration: 1086 lambda_k: 1 Loss: 0.18125906407859668\n",
      "Iteration: 1087 lambda_k: 1 Loss: 0.1811418947982406\n",
      "Iteration: 1088 lambda_k: 1 Loss: 0.18102479831570148\n",
      "Iteration: 1089 lambda_k: 1 Loss: 0.18090777453534262\n",
      "Iteration: 1090 lambda_k: 1 Loss: 0.1807908233626221\n",
      "Iteration: 1091 lambda_k: 1 Loss: 0.18067394470408704\n",
      "Iteration: 1092 lambda_k: 1 Loss: 0.1805571384673677\n",
      "Iteration: 1093 lambda_k: 1 Loss: 0.18044040456117177\n",
      "Iteration: 1094 lambda_k: 1 Loss: 0.1803237428952785\n",
      "Iteration: 1095 lambda_k: 1 Loss: 0.1802071533805331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1096 lambda_k: 1 Loss: 0.18009063592884086\n",
      "Iteration: 1097 lambda_k: 1 Loss: 0.1799741904531618\n",
      "Iteration: 1098 lambda_k: 1 Loss: 0.17985781686750474\n",
      "Iteration: 1099 lambda_k: 1 Loss: 0.17974151508692196\n",
      "Iteration: 1100 lambda_k: 1 Loss: 0.1796252850275035\n",
      "Iteration: 1101 lambda_k: 1 Loss: 0.1795091266063716\n",
      "Iteration: 1102 lambda_k: 1 Loss: 0.17939303974167536\n",
      "Iteration: 1103 lambda_k: 1 Loss: 0.17927702435258516\n",
      "Iteration: 1104 lambda_k: 1 Loss: 0.17916108035928727\n",
      "Iteration: 1105 lambda_k: 1 Loss: 0.1790452076829783\n",
      "Iteration: 1106 lambda_k: 1 Loss: 0.17892940624586015\n",
      "Iteration: 1107 lambda_k: 1 Loss: 0.17881367597113446\n",
      "Iteration: 1108 lambda_k: 1 Loss: 0.17869801678299727\n",
      "Iteration: 1109 lambda_k: 1 Loss: 0.1785824286066338\n",
      "Iteration: 1110 lambda_k: 1 Loss: 0.17846691136821322\n",
      "Iteration: 1111 lambda_k: 1 Loss: 0.17835146499488344\n",
      "Iteration: 1112 lambda_k: 1 Loss: 0.1782360894147659\n",
      "Iteration: 1113 lambda_k: 1 Loss: 0.17812078455695032\n",
      "Iteration: 1114 lambda_k: 1 Loss: 0.17800555035148968\n",
      "Iteration: 1115 lambda_k: 1 Loss: 0.1778903867293952\n",
      "Iteration: 1116 lambda_k: 1 Loss: 0.17777529362263103\n",
      "Iteration: 1117 lambda_k: 1 Loss: 0.17766027096410938\n",
      "Iteration: 1118 lambda_k: 1 Loss: 0.17754531868768544\n",
      "Iteration: 1119 lambda_k: 1 Loss: 0.17743043672815242\n",
      "Iteration: 1120 lambda_k: 1 Loss: 0.1773156250212366\n",
      "Iteration: 1121 lambda_k: 1 Loss: 0.1772008835035923\n",
      "Iteration: 1122 lambda_k: 1 Loss: 0.1770862121127971\n",
      "Iteration: 1123 lambda_k: 1 Loss: 0.17697161078734697\n",
      "Iteration: 1124 lambda_k: 1 Loss: 0.17685707946665125\n",
      "Iteration: 1125 lambda_k: 1 Loss: 0.17674261809102795\n",
      "Iteration: 1126 lambda_k: 1 Loss: 0.1766282266016991\n",
      "Iteration: 1127 lambda_k: 1 Loss: 0.1765139049407857\n",
      "Iteration: 1128 lambda_k: 1 Loss: 0.17639965305130298\n",
      "Iteration: 1129 lambda_k: 1 Loss: 0.1762854708771561\n",
      "Iteration: 1130 lambda_k: 1 Loss: 0.17617135836313497\n",
      "Iteration: 1131 lambda_k: 1 Loss: 0.1760573154549097\n",
      "Iteration: 1132 lambda_k: 1 Loss: 0.1759433420990264\n",
      "Iteration: 1133 lambda_k: 1 Loss: 0.17582943824290187\n",
      "Iteration: 1134 lambda_k: 1 Loss: 0.17571560383481952\n",
      "Iteration: 1135 lambda_k: 1 Loss: 0.17560183882392463\n",
      "Iteration: 1136 lambda_k: 1 Loss: 0.17548814316021982\n",
      "Iteration: 1137 lambda_k: 1 Loss: 0.1753745167945606\n",
      "Iteration: 1138 lambda_k: 1 Loss: 0.17526095967865088\n",
      "Iteration: 1139 lambda_k: 1 Loss: 0.17514747176503834\n",
      "Iteration: 1140 lambda_k: 1 Loss: 0.1750340530071103\n",
      "Iteration: 1141 lambda_k: 1 Loss: 0.1749207033590891\n",
      "Iteration: 1142 lambda_k: 1 Loss: 0.1748074227760277\n",
      "Iteration: 1143 lambda_k: 1 Loss: 0.17469421121380552\n",
      "Iteration: 1144 lambda_k: 1 Loss: 0.17458106862912384\n",
      "Iteration: 1145 lambda_k: 1 Loss: 0.17446799497950172\n",
      "Iteration: 1146 lambda_k: 1 Loss: 0.17435499022327167\n",
      "Iteration: 1147 lambda_k: 1 Loss: 0.17424205431957515\n",
      "Iteration: 1148 lambda_k: 1 Loss: 0.17412918722835885\n",
      "Iteration: 1149 lambda_k: 1 Loss: 0.1740163889103698\n",
      "Iteration: 1150 lambda_k: 1 Loss: 0.1739036593271518\n",
      "Iteration: 1151 lambda_k: 1 Loss: 0.17379099844104093\n",
      "Iteration: 1152 lambda_k: 1 Loss: 0.17367840621516137\n",
      "Iteration: 1153 lambda_k: 1 Loss: 0.1735658826134216\n",
      "Iteration: 1154 lambda_k: 1 Loss: 0.17345342760050997\n",
      "Iteration: 1155 lambda_k: 1 Loss: 0.17334104114189072\n",
      "Iteration: 1156 lambda_k: 1 Loss: 0.17322872320380012\n",
      "Iteration: 1157 lambda_k: 1 Loss: 0.1731164737532422\n",
      "Iteration: 1158 lambda_k: 1 Loss: 0.17300429275798496\n",
      "Iteration: 1159 lambda_k: 1 Loss: 0.17289218018655628\n",
      "Iteration: 1160 lambda_k: 1 Loss: 0.17278013600823985\n",
      "Iteration: 1161 lambda_k: 1 Loss: 0.1726681601930716\n",
      "Iteration: 1162 lambda_k: 1 Loss: 0.17255625271183545\n",
      "Iteration: 1163 lambda_k: 1 Loss: 0.17244441353605958\n",
      "Iteration: 1164 lambda_k: 1 Loss: 0.1723326426380127\n",
      "Iteration: 1165 lambda_k: 1 Loss: 0.17222093999069985\n",
      "Iteration: 1166 lambda_k: 1 Loss: 0.172109305567859\n",
      "Iteration: 1167 lambda_k: 1 Loss: 0.17199773934395696\n",
      "Iteration: 1168 lambda_k: 1 Loss: 0.1718862412941859\n",
      "Iteration: 1169 lambda_k: 1 Loss: 0.17177481139445933\n",
      "Iteration: 1170 lambda_k: 1 Loss: 0.17166344962140853\n",
      "Iteration: 1171 lambda_k: 1 Loss: 0.1715521559523788\n",
      "Iteration: 1172 lambda_k: 1 Loss: 0.17144093036542601\n",
      "Iteration: 1173 lambda_k: 1 Loss: 0.17132977283931256\n",
      "Iteration: 1174 lambda_k: 1 Loss: 0.17121868335350407\n",
      "Iteration: 1175 lambda_k: 1 Loss: 0.17110766188816565\n",
      "Iteration: 1176 lambda_k: 1 Loss: 0.17099670842415846\n",
      "Iteration: 1177 lambda_k: 1 Loss: 0.17088582294303595\n",
      "Iteration: 1178 lambda_k: 1 Loss: 0.17077500542704047\n",
      "Iteration: 1179 lambda_k: 1 Loss: 0.17066425585909978\n",
      "Iteration: 1180 lambda_k: 1 Loss: 0.1705535742228235\n",
      "Iteration: 1181 lambda_k: 1 Loss: 0.17044296050249969\n",
      "Iteration: 1182 lambda_k: 1 Loss: 0.17033241468309135\n",
      "Iteration: 1183 lambda_k: 1 Loss: 0.17022193675023325\n",
      "Iteration: 1184 lambda_k: 1 Loss: 0.1701115266902282\n",
      "Iteration: 1185 lambda_k: 1 Loss: 0.17000118449004378\n",
      "Iteration: 1186 lambda_k: 1 Loss: 0.16989091013730934\n",
      "Iteration: 1187 lambda_k: 1 Loss: 0.16978070362031214\n",
      "Iteration: 1188 lambda_k: 1 Loss: 0.16967056492799454\n",
      "Iteration: 1189 lambda_k: 1 Loss: 0.16956049404995047\n",
      "Iteration: 1190 lambda_k: 1 Loss: 0.1694504909764223\n",
      "Iteration: 1191 lambda_k: 1 Loss: 0.16934055569829748\n",
      "Iteration: 1192 lambda_k: 1 Loss: 0.16923068820710563\n",
      "Iteration: 1193 lambda_k: 1 Loss: 0.16912088849501491\n",
      "Iteration: 1194 lambda_k: 1 Loss: 0.16901115655482937\n",
      "Iteration: 1195 lambda_k: 1 Loss: 0.16890149237998564\n",
      "Iteration: 1196 lambda_k: 1 Loss: 0.16879189596454963\n",
      "Iteration: 1197 lambda_k: 1 Loss: 0.16868236730321384\n",
      "Iteration: 1198 lambda_k: 1 Loss: 0.16857290639129396\n",
      "Iteration: 1199 lambda_k: 1 Loss: 0.16846351322472614\n",
      "Iteration: 1200 lambda_k: 1 Loss: 0.16835418780006375\n",
      "Iteration: 1201 lambda_k: 1 Loss: 0.16824493011447458\n",
      "Iteration: 1202 lambda_k: 1 Loss: 0.1681357401657379\n",
      "Iteration: 1203 lambda_k: 1 Loss: 0.16802661795224139\n",
      "Iteration: 1204 lambda_k: 1 Loss: 0.16791756347297837\n",
      "Iteration: 1205 lambda_k: 1 Loss: 0.16780857672754496\n",
      "Iteration: 1206 lambda_k: 1 Loss: 0.16769965771613712\n",
      "Iteration: 1207 lambda_k: 1 Loss: 0.16759080643954793\n",
      "Iteration: 1208 lambda_k: 1 Loss: 0.16748202289916472\n",
      "Iteration: 1209 lambda_k: 1 Loss: 0.16737330709696638\n",
      "Iteration: 1210 lambda_k: 1 Loss: 0.1672646590355207\n",
      "Iteration: 1211 lambda_k: 1 Loss: 0.16715607871798135\n",
      "Iteration: 1212 lambda_k: 1 Loss: 0.16704756614808564\n",
      "Iteration: 1213 lambda_k: 1 Loss: 0.1669391213301513\n",
      "Iteration: 1214 lambda_k: 1 Loss: 0.16683074426907454\n",
      "Iteration: 1215 lambda_k: 1 Loss: 0.16672243497032688\n",
      "Iteration: 1216 lambda_k: 1 Loss: 0.1666141934399527\n",
      "Iteration: 1217 lambda_k: 1 Loss: 0.1665060196845669\n",
      "Iteration: 1218 lambda_k: 1 Loss: 0.16639791371135226\n",
      "Iteration: 1219 lambda_k: 1 Loss: 0.16628987552805666\n",
      "Iteration: 1220 lambda_k: 1 Loss: 0.16618190873033922\n",
      "Iteration: 1221 lambda_k: 1 Loss: 0.16607471500866214\n",
      "Iteration: 1222 lambda_k: 1 Loss: 0.16596758613834275\n",
      "Iteration: 1223 lambda_k: 1 Loss: 0.16586052214176952\n",
      "Iteration: 1224 lambda_k: 1 Loss: 0.16575352304183222\n",
      "Iteration: 1225 lambda_k: 1 Loss: 0.1656465888619202\n",
      "Iteration: 1226 lambda_k: 1 Loss: 0.16553971962591968\n",
      "Iteration: 1227 lambda_k: 1 Loss: 0.16543291535821225\n",
      "Iteration: 1228 lambda_k: 1 Loss: 0.16532617608367245\n",
      "Iteration: 1229 lambda_k: 1 Loss: 0.16521950182766604\n",
      "Iteration: 1230 lambda_k: 1 Loss: 0.1651128926160477\n",
      "Iteration: 1231 lambda_k: 1 Loss: 0.16500634847515946\n",
      "Iteration: 1232 lambda_k: 1 Loss: 0.16489986943182855\n",
      "Iteration: 1233 lambda_k: 1 Loss: 0.1647934555133654\n",
      "Iteration: 1234 lambda_k: 1 Loss: 0.1646871067475621\n",
      "Iteration: 1235 lambda_k: 1 Loss: 0.1645808231626902\n",
      "Iteration: 1236 lambda_k: 1 Loss: 0.16447460478749923\n",
      "Iteration: 1237 lambda_k: 1 Loss: 0.16436845165121455\n",
      "Iteration: 1238 lambda_k: 1 Loss: 0.16426236378353598\n",
      "Iteration: 1239 lambda_k: 1 Loss: 0.16415634121463565\n",
      "Iteration: 1240 lambda_k: 1 Loss: 0.1640503839751567\n",
      "Iteration: 1241 lambda_k: 1 Loss: 0.16394449209621137\n",
      "Iteration: 1242 lambda_k: 1 Loss: 0.16383866560937935\n",
      "Iteration: 1243 lambda_k: 1 Loss: 0.16373290454670628\n",
      "Iteration: 1244 lambda_k: 1 Loss: 0.163627208940702\n",
      "Iteration: 1245 lambda_k: 1 Loss: 0.16352157882433924\n",
      "Iteration: 1246 lambda_k: 1 Loss: 0.16341601423105168\n",
      "Iteration: 1247 lambda_k: 1 Loss: 0.16331051519473283\n",
      "Iteration: 1248 lambda_k: 1 Loss: 0.16320508174973436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1249 lambda_k: 1 Loss: 0.16309971393086473\n",
      "Iteration: 1250 lambda_k: 1 Loss: 0.16299441177338758\n",
      "Iteration: 1251 lambda_k: 1 Loss: 0.16288917531302055\n",
      "Iteration: 1252 lambda_k: 1 Loss: 0.16278400458593378\n",
      "Iteration: 1253 lambda_k: 1 Loss: 0.16267889962874868\n",
      "Iteration: 1254 lambda_k: 1 Loss: 0.1625738604785364\n",
      "Iteration: 1255 lambda_k: 1 Loss: 0.16246888717281682\n",
      "Iteration: 1256 lambda_k: 1 Loss: 0.16236397974955705\n",
      "Iteration: 1257 lambda_k: 1 Loss: 0.16225913824717028\n",
      "Iteration: 1258 lambda_k: 1 Loss: 0.1621543627045147\n",
      "Iteration: 1259 lambda_k: 1 Loss: 0.16204965316089204\n",
      "Iteration: 1260 lambda_k: 1 Loss: 0.1619450096560466\n",
      "Iteration: 1261 lambda_k: 1 Loss: 0.16184043223016414\n",
      "Iteration: 1262 lambda_k: 1 Loss: 0.16173592092387076\n",
      "Iteration: 1263 lambda_k: 1 Loss: 0.16163147577823164\n",
      "Iteration: 1264 lambda_k: 1 Loss: 0.16152709683475033\n",
      "Iteration: 1265 lambda_k: 1 Loss: 0.16142278413536734\n",
      "Iteration: 1266 lambda_k: 1 Loss: 0.1613185377224595\n",
      "Iteration: 1267 lambda_k: 1 Loss: 0.16121435763883873\n",
      "Iteration: 1268 lambda_k: 1 Loss: 0.1611102439277512\n",
      "Iteration: 1269 lambda_k: 1 Loss: 0.16100619663287635\n",
      "Iteration: 1270 lambda_k: 1 Loss: 0.16090221579832606\n",
      "Iteration: 1271 lambda_k: 1 Loss: 0.1607983014686437\n",
      "Iteration: 1272 lambda_k: 1 Loss: 0.1606944536888034\n",
      "Iteration: 1273 lambda_k: 1 Loss: 0.16059067250420966\n",
      "Iteration: 1274 lambda_k: 1 Loss: 0.16048695796069554\n",
      "Iteration: 1275 lambda_k: 1 Loss: 0.1603833101045222\n",
      "Iteration: 1276 lambda_k: 1 Loss: 0.1602797289823774\n",
      "Iteration: 1277 lambda_k: 1 Loss: 0.16017621464137707\n",
      "Iteration: 1278 lambda_k: 1 Loss: 0.16007276712906324\n",
      "Iteration: 1279 lambda_k: 1 Loss: 0.159969386493403\n",
      "Iteration: 1280 lambda_k: 1 Loss: 0.15986607278278828\n",
      "Iteration: 1281 lambda_k: 1 Loss: 0.15976282604603476\n",
      "Iteration: 1282 lambda_k: 1 Loss: 0.15965964633238183\n",
      "Iteration: 1283 lambda_k: 1 Loss: 0.1595565336914917\n",
      "Iteration: 1284 lambda_k: 1 Loss: 0.15945348817344882\n",
      "Iteration: 1285 lambda_k: 1 Loss: 0.1593505098287593\n",
      "Iteration: 1286 lambda_k: 1 Loss: 0.15924759870835062\n",
      "Iteration: 1287 lambda_k: 1 Loss: 0.15914475486357071\n",
      "Iteration: 1288 lambda_k: 1 Loss: 0.1590419783461876\n",
      "Iteration: 1289 lambda_k: 1 Loss: 0.15893926920838927\n",
      "Iteration: 1290 lambda_k: 1 Loss: 0.1588366275027826\n",
      "Iteration: 1291 lambda_k: 1 Loss: 0.15873405328239337\n",
      "Iteration: 1292 lambda_k: 1 Loss: 0.15863154660066564\n",
      "Iteration: 1293 lambda_k: 1 Loss: 0.1585291075114614\n",
      "Iteration: 1294 lambda_k: 1 Loss: 0.15842673606906016\n",
      "Iteration: 1295 lambda_k: 1 Loss: 0.15832443232815846\n",
      "Iteration: 1296 lambda_k: 1 Loss: 0.15822219634386975\n",
      "Iteration: 1297 lambda_k: 1 Loss: 0.1581203710478692\n",
      "Iteration: 1298 lambda_k: 1 Loss: 0.15801921366706914\n",
      "Iteration: 1299 lambda_k: 1 Loss: 0.15791811826991006\n",
      "Iteration: 1300 lambda_k: 1 Loss: 0.1578170849505985\n",
      "Iteration: 1301 lambda_k: 1 Loss: 0.15771611380352055\n",
      "Iteration: 1302 lambda_k: 1 Loss: 0.15761520492324282\n",
      "Iteration: 1303 lambda_k: 1 Loss: 0.1575143584045136\n",
      "Iteration: 1304 lambda_k: 1 Loss: 0.15741357434226427\n",
      "Iteration: 1305 lambda_k: 1 Loss: 0.1573128528316104\n",
      "Iteration: 1306 lambda_k: 1 Loss: 0.15721219396785285\n",
      "Iteration: 1307 lambda_k: 1 Loss: 0.1571115978464793\n",
      "Iteration: 1308 lambda_k: 1 Loss: 0.15701106456316513\n",
      "Iteration: 1309 lambda_k: 1 Loss: 0.15691059421377493\n",
      "Iteration: 1310 lambda_k: 1 Loss: 0.1568101868943635\n",
      "Iteration: 1311 lambda_k: 1 Loss: 0.15670984270117722\n",
      "Iteration: 1312 lambda_k: 1 Loss: 0.15660956173065518\n",
      "Iteration: 1313 lambda_k: 1 Loss: 0.15650934407943048\n",
      "Iteration: 1314 lambda_k: 1 Loss: 0.156409189845154\n",
      "Iteration: 1315 lambda_k: 1 Loss: 0.15630909912362242\n",
      "Iteration: 1316 lambda_k: 1 Loss: 0.15620907201203527\n",
      "Iteration: 1317 lambda_k: 1 Loss: 0.1561091086081642\n",
      "Iteration: 1318 lambda_k: 1 Loss: 0.1560092090097618\n",
      "Iteration: 1319 lambda_k: 1 Loss: 0.1559093733146349\n",
      "Iteration: 1320 lambda_k: 1 Loss: 0.15580960162077534\n",
      "Iteration: 1321 lambda_k: 1 Loss: 0.15570989402640084\n",
      "Iteration: 1322 lambda_k: 1 Loss: 0.15561025062994707\n",
      "Iteration: 1323 lambda_k: 1 Loss: 0.15551067153005693\n",
      "Iteration: 1324 lambda_k: 1 Loss: 0.15541115682557813\n",
      "Iteration: 1325 lambda_k: 1 Loss: 0.1553117066155658\n",
      "Iteration: 1326 lambda_k: 1 Loss: 0.15521232099928473\n",
      "Iteration: 1327 lambda_k: 1 Loss: 0.15511300007621057\n",
      "Iteration: 1328 lambda_k: 1 Loss: 0.15501374394603104\n",
      "Iteration: 1329 lambda_k: 1 Loss: 0.15491455270864696\n",
      "Iteration: 1330 lambda_k: 1 Loss: 0.15481542646417346\n",
      "Iteration: 1331 lambda_k: 1 Loss: 0.15471636531294092\n",
      "Iteration: 1332 lambda_k: 1 Loss: 0.1546173693554964\n",
      "Iteration: 1333 lambda_k: 1 Loss: 0.15451843869260445\n",
      "Iteration: 1334 lambda_k: 1 Loss: 0.15441957342524854\n",
      "Iteration: 1335 lambda_k: 1 Loss: 0.1543207736546319\n",
      "Iteration: 1336 lambda_k: 1 Loss: 0.1542220394821788\n",
      "Iteration: 1337 lambda_k: 1 Loss: 0.1541233710095356\n",
      "Iteration: 1338 lambda_k: 1 Loss: 0.1540247683385716\n",
      "Iteration: 1339 lambda_k: 1 Loss: 0.15392623157138058\n",
      "Iteration: 1340 lambda_k: 1 Loss: 0.15382776081028135\n",
      "Iteration: 1341 lambda_k: 1 Loss: 0.15372935615781916\n",
      "Iteration: 1342 lambda_k: 1 Loss: 0.15363101771676652\n",
      "Iteration: 1343 lambda_k: 1 Loss: 0.15353274559012048\n",
      "Iteration: 1344 lambda_k: 1 Loss: 0.15343453988110417\n",
      "Iteration: 1345 lambda_k: 1 Loss: 0.15333640069318547\n",
      "Iteration: 1346 lambda_k: 1 Loss: 0.15323832813006152\n",
      "Iteration: 1347 lambda_k: 1 Loss: 0.1531403222956572\n",
      "Iteration: 1348 lambda_k: 1 Loss: 0.1530423832941293\n",
      "Iteration: 1349 lambda_k: 1 Loss: 0.15294451122986946\n",
      "Iteration: 1350 lambda_k: 1 Loss: 0.15284670620749527\n",
      "Iteration: 1351 lambda_k: 1 Loss: 0.15274896833186474\n",
      "Iteration: 1352 lambda_k: 1 Loss: 0.15265129770808833\n",
      "Iteration: 1353 lambda_k: 1 Loss: 0.1525536944415097\n",
      "Iteration: 1354 lambda_k: 1 Loss: 0.15245615863770548\n",
      "Iteration: 1355 lambda_k: 1 Loss: 0.15235869040249023\n",
      "Iteration: 1356 lambda_k: 1 Loss: 0.15226128984191975\n",
      "Iteration: 1357 lambda_k: 1 Loss: 0.15216395706229197\n",
      "Iteration: 1358 lambda_k: 1 Loss: 0.15206669217014673\n",
      "Iteration: 1359 lambda_k: 1 Loss: 0.15196949527180514\n",
      "Iteration: 1360 lambda_k: 1 Loss: 0.15187236647499838\n",
      "Iteration: 1361 lambda_k: 1 Loss: 0.15177530588699684\n",
      "Iteration: 1362 lambda_k: 1 Loss: 0.15167831361511355\n",
      "Iteration: 1363 lambda_k: 1 Loss: 0.1515813897670391\n",
      "Iteration: 1364 lambda_k: 1 Loss: 0.15148453445079307\n",
      "Iteration: 1365 lambda_k: 1 Loss: 0.15138774777465092\n",
      "Iteration: 1366 lambda_k: 1 Loss: 0.15129102984712306\n",
      "Iteration: 1367 lambda_k: 1 Loss: 0.15119438077696126\n",
      "Iteration: 1368 lambda_k: 1 Loss: 0.15109780067316644\n",
      "Iteration: 1369 lambda_k: 1 Loss: 0.15100128964499082\n",
      "Iteration: 1370 lambda_k: 1 Loss: 0.15090484780193797\n",
      "Iteration: 1371 lambda_k: 1 Loss: 0.15080847525376276\n",
      "Iteration: 1372 lambda_k: 1 Loss: 0.15071217211047194\n",
      "Iteration: 1373 lambda_k: 1 Loss: 0.150615938482325\n",
      "Iteration: 1374 lambda_k: 1 Loss: 0.15051977447983403\n",
      "Iteration: 1375 lambda_k: 1 Loss: 0.15042368021376454\n",
      "Iteration: 1376 lambda_k: 1 Loss: 0.15032765579513652\n",
      "Iteration: 1377 lambda_k: 1 Loss: 0.15023170133522454\n",
      "Iteration: 1378 lambda_k: 1 Loss: 0.15013581694555844\n",
      "Iteration: 1379 lambda_k: 1 Loss: 0.15004000273792364\n",
      "Iteration: 1380 lambda_k: 1 Loss: 0.14994425882436166\n",
      "Iteration: 1381 lambda_k: 1 Loss: 0.14984858531717055\n",
      "Iteration: 1382 lambda_k: 1 Loss: 0.14975298232890535\n",
      "Iteration: 1383 lambda_k: 1 Loss: 0.1496574499723785\n",
      "Iteration: 1384 lambda_k: 1 Loss: 0.14956198836066031\n",
      "Iteration: 1385 lambda_k: 1 Loss: 0.1494665976070791\n",
      "Iteration: 1386 lambda_k: 1 Loss: 0.14937127782522194\n",
      "Iteration: 1387 lambda_k: 1 Loss: 0.14927602912893462\n",
      "Iteration: 1388 lambda_k: 1 Loss: 0.14918085163232234\n",
      "Iteration: 1389 lambda_k: 1 Loss: 0.14908574544974973\n",
      "Iteration: 1390 lambda_k: 1 Loss: 0.14899071069584133\n",
      "Iteration: 1391 lambda_k: 1 Loss: 0.14889574748548184\n",
      "Iteration: 1392 lambda_k: 1 Loss: 0.1488008559338164\n",
      "Iteration: 1393 lambda_k: 1 Loss: 0.14870603615625075\n",
      "Iteration: 1394 lambda_k: 1 Loss: 0.1486112882684517\n",
      "Iteration: 1395 lambda_k: 1 Loss: 0.14851661238634709\n",
      "Iteration: 1396 lambda_k: 1 Loss: 0.1484220086261261\n",
      "Iteration: 1397 lambda_k: 1 Loss: 0.14832747710423966\n",
      "Iteration: 1398 lambda_k: 1 Loss: 0.14823301793740015\n",
      "Iteration: 1399 lambda_k: 1 Loss: 0.14813863124258211\n",
      "Iteration: 1400 lambda_k: 1 Loss: 0.14804431713702199\n",
      "Iteration: 1401 lambda_k: 1 Loss: 0.14795007573557398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1402 lambda_k: 1 Loss: 0.14785590716000555\n",
      "Iteration: 1403 lambda_k: 1 Loss: 0.14776181152712775\n",
      "Iteration: 1404 lambda_k: 1 Loss: 0.14766778895525826\n",
      "Iteration: 1405 lambda_k: 1 Loss: 0.14757383956278292\n",
      "Iteration: 1406 lambda_k: 1 Loss: 0.1474799634685276\n",
      "Iteration: 1407 lambda_k: 1 Loss: 0.14738616079159902\n",
      "Iteration: 1408 lambda_k: 1 Loss: 0.14729243165133923\n",
      "Iteration: 1409 lambda_k: 1 Loss: 0.14719877616733834\n",
      "Iteration: 1410 lambda_k: 1 Loss: 0.14710519445944914\n",
      "Iteration: 1411 lambda_k: 1 Loss: 0.14701168664779074\n",
      "Iteration: 1412 lambda_k: 1 Loss: 0.14691825285274687\n",
      "Iteration: 1413 lambda_k: 1 Loss: 0.1468248931949645\n",
      "Iteration: 1414 lambda_k: 1 Loss: 0.14673160779535344\n",
      "Iteration: 1415 lambda_k: 1 Loss: 0.1466383967750863\n",
      "Iteration: 1416 lambda_k: 1 Loss: 0.14654526026231957\n",
      "Iteration: 1417 lambda_k: 1 Loss: 0.14645219836832032\n",
      "Iteration: 1418 lambda_k: 1 Loss: 0.14635921121649534\n",
      "Iteration: 1419 lambda_k: 1 Loss: 0.14626629893083418\n",
      "Iteration: 1420 lambda_k: 1 Loss: 0.14617346163462389\n",
      "Iteration: 1421 lambda_k: 1 Loss: 0.14608069945048965\n",
      "Iteration: 1422 lambda_k: 1 Loss: 0.14598801250123133\n",
      "Iteration: 1423 lambda_k: 1 Loss: 0.1458954009100604\n",
      "Iteration: 1424 lambda_k: 1 Loss: 0.14580286480053156\n",
      "Iteration: 1425 lambda_k: 1 Loss: 0.1457104042964649\n",
      "Iteration: 1426 lambda_k: 1 Loss: 0.14561801952192654\n",
      "Iteration: 1427 lambda_k: 1 Loss: 0.14552571060123604\n",
      "Iteration: 1428 lambda_k: 1 Loss: 0.14543347765897352\n",
      "Iteration: 1429 lambda_k: 1 Loss: 0.1453413208199807\n",
      "Iteration: 1430 lambda_k: 1 Loss: 0.14524924020936006\n",
      "Iteration: 1431 lambda_k: 1 Loss: 0.1451572359524739\n",
      "Iteration: 1432 lambda_k: 1 Loss: 0.14506530817494515\n",
      "Iteration: 1433 lambda_k: 1 Loss: 0.14497345700267603\n",
      "Iteration: 1434 lambda_k: 1 Loss: 0.14488168256009257\n",
      "Iteration: 1435 lambda_k: 1 Loss: 0.14478998497618525\n",
      "Iteration: 1436 lambda_k: 1 Loss: 0.14469836437762576\n",
      "Iteration: 1437 lambda_k: 1 Loss: 0.14460682089056945\n",
      "Iteration: 1438 lambda_k: 1 Loss: 0.1445153546419244\n",
      "Iteration: 1439 lambda_k: 1 Loss: 0.14442396575916477\n",
      "Iteration: 1440 lambda_k: 1 Loss: 0.14433265437005358\n",
      "Iteration: 1441 lambda_k: 1 Loss: 0.14424142060256157\n",
      "Iteration: 1442 lambda_k: 1 Loss: 0.1441502645848891\n",
      "Iteration: 1443 lambda_k: 1 Loss: 0.14405918644549154\n",
      "Iteration: 1444 lambda_k: 1 Loss: 0.14396818631308575\n",
      "Iteration: 1445 lambda_k: 1 Loss: 0.14387726431664646\n",
      "Iteration: 1446 lambda_k: 1 Loss: 0.14378642058540372\n",
      "Iteration: 1447 lambda_k: 1 Loss: 0.14369565524884267\n",
      "Iteration: 1448 lambda_k: 1 Loss: 0.1436049684367031\n",
      "Iteration: 1449 lambda_k: 1 Loss: 0.1435143602789791\n",
      "Iteration: 1450 lambda_k: 1 Loss: 0.14342383090591895\n",
      "Iteration: 1451 lambda_k: 1 Loss: 0.14333338044802457\n",
      "Iteration: 1452 lambda_k: 1 Loss: 0.14324300903605108\n",
      "Iteration: 1453 lambda_k: 1 Loss: 0.14315271680100647\n",
      "Iteration: 1454 lambda_k: 1 Loss: 0.14306250387415112\n",
      "Iteration: 1455 lambda_k: 1 Loss: 0.14297237038699737\n",
      "Iteration: 1456 lambda_k: 1 Loss: 0.14288231647130906\n",
      "Iteration: 1457 lambda_k: 1 Loss: 0.14279234225910128\n",
      "Iteration: 1458 lambda_k: 1 Loss: 0.14270244788263967\n",
      "Iteration: 1459 lambda_k: 1 Loss: 0.14261263347444014\n",
      "Iteration: 1460 lambda_k: 1 Loss: 0.1425228991672684\n",
      "Iteration: 1461 lambda_k: 1 Loss: 0.14243324509413935\n",
      "Iteration: 1462 lambda_k: 1 Loss: 0.1423436713883169\n",
      "Iteration: 1463 lambda_k: 1 Loss: 0.14225417818331318\n",
      "Iteration: 1464 lambda_k: 1 Loss: 0.1421647656128883\n",
      "Iteration: 1465 lambda_k: 1 Loss: 0.14207543381104976\n",
      "Iteration: 1466 lambda_k: 1 Loss: 0.141986182912052\n",
      "Iteration: 1467 lambda_k: 1 Loss: 0.1418970130503959\n",
      "Iteration: 1468 lambda_k: 1 Loss: 0.14180792436082826\n",
      "Iteration: 1469 lambda_k: 1 Loss: 0.1417189169783413\n",
      "Iteration: 1470 lambda_k: 1 Loss: 0.14162999103817223\n",
      "Iteration: 1471 lambda_k: 1 Loss: 0.1415411466758027\n",
      "Iteration: 1472 lambda_k: 1 Loss: 0.1414523840269582\n",
      "Iteration: 1473 lambda_k: 1 Loss: 0.14136370322760766\n",
      "Iteration: 1474 lambda_k: 1 Loss: 0.14127510441396293\n",
      "Iteration: 1475 lambda_k: 1 Loss: 0.1411865877224781\n",
      "Iteration: 1476 lambda_k: 1 Loss: 0.1410981532898492\n",
      "Iteration: 1477 lambda_k: 1 Loss: 0.1410098012530135\n",
      "Iteration: 1478 lambda_k: 1 Loss: 0.140921531749149\n",
      "Iteration: 1479 lambda_k: 1 Loss: 0.14083334491567398\n",
      "Iteration: 1480 lambda_k: 1 Loss: 0.14074524089024626\n",
      "Iteration: 1481 lambda_k: 1 Loss: 0.14065721981076296\n",
      "Iteration: 1482 lambda_k: 1 Loss: 0.14056928181535958\n",
      "Iteration: 1483 lambda_k: 1 Loss: 0.14048142704240976\n",
      "Iteration: 1484 lambda_k: 1 Loss: 0.14039365563052467\n",
      "Iteration: 1485 lambda_k: 1 Loss: 0.14030596771855217\n",
      "Iteration: 1486 lambda_k: 1 Loss: 0.14021836344557664\n",
      "Iteration: 1487 lambda_k: 1 Loss: 0.14013084295091818\n",
      "Iteration: 1488 lambda_k: 1 Loss: 0.14004340637413204\n",
      "Iteration: 1489 lambda_k: 1 Loss: 0.13995605385500823\n",
      "Iteration: 1490 lambda_k: 1 Loss: 0.13986878553357068\n",
      "Iteration: 1491 lambda_k: 1 Loss: 0.13978160155007693\n",
      "Iteration: 1492 lambda_k: 1 Loss: 0.13969450204501735\n",
      "Iteration: 1493 lambda_k: 1 Loss: 0.1396074871591147\n",
      "Iteration: 1494 lambda_k: 1 Loss: 0.1395205570333234\n",
      "Iteration: 1495 lambda_k: 1 Loss: 0.13943371180882905\n",
      "Iteration: 1496 lambda_k: 1 Loss: 0.13934695162704788\n",
      "Iteration: 1497 lambda_k: 1 Loss: 0.13926027662962615\n",
      "Iteration: 1498 lambda_k: 1 Loss: 0.13917368695843937\n",
      "Iteration: 1499 lambda_k: 1 Loss: 0.13908718275559187\n",
      "Iteration: 1500 lambda_k: 1 Loss: 0.1390007641634163\n",
      "Iteration: 1501 lambda_k: 1 Loss: 0.13891443132447284\n",
      "Iteration: 1502 lambda_k: 1 Loss: 0.1388281843815486\n",
      "Iteration: 1503 lambda_k: 1 Loss: 0.13874202347765724\n",
      "Iteration: 1504 lambda_k: 1 Loss: 0.13865594875603815\n",
      "Iteration: 1505 lambda_k: 1 Loss: 0.13856996036015587\n",
      "Iteration: 1506 lambda_k: 1 Loss: 0.1384840584336996\n",
      "Iteration: 1507 lambda_k: 1 Loss: 0.13839824312058258\n",
      "Iteration: 1508 lambda_k: 1 Loss: 0.1383125145649413\n",
      "Iteration: 1509 lambda_k: 1 Loss: 0.13822687291113514\n",
      "Iteration: 1510 lambda_k: 1 Loss: 0.13814131830374551\n",
      "Iteration: 1511 lambda_k: 1 Loss: 0.1380558508875755\n",
      "Iteration: 1512 lambda_k: 1 Loss: 0.13797047080764904\n",
      "Iteration: 1513 lambda_k: 1 Loss: 0.1378851782092104\n",
      "Iteration: 1514 lambda_k: 1 Loss: 0.1377999732377236\n",
      "Iteration: 1515 lambda_k: 1 Loss: 0.13771485603887168\n",
      "Iteration: 1516 lambda_k: 1 Loss: 0.13762982675855615\n",
      "Iteration: 1517 lambda_k: 1 Loss: 0.1375448855428964\n",
      "Iteration: 1518 lambda_k: 1 Loss: 0.13746003253822908\n",
      "Iteration: 1519 lambda_k: 1 Loss: 0.13737526789110746\n",
      "Iteration: 1520 lambda_k: 1 Loss: 0.1372905917483007\n",
      "Iteration: 1521 lambda_k: 1 Loss: 0.13720600425679338\n",
      "Iteration: 1522 lambda_k: 1 Loss: 0.13712150556378488\n",
      "Iteration: 1523 lambda_k: 1 Loss: 0.13703709581668863\n",
      "Iteration: 1524 lambda_k: 1 Loss: 0.13695277516508286\n",
      "Iteration: 1525 lambda_k: 1 Loss: 0.13686854375376023\n",
      "Iteration: 1526 lambda_k: 1 Loss: 0.13678440173075923\n",
      "Iteration: 1527 lambda_k: 1 Loss: 0.13670034924527383\n",
      "Iteration: 1528 lambda_k: 1 Loss: 0.13661638644628843\n",
      "Iteration: 1529 lambda_k: 1 Loss: 0.1365325134808151\n",
      "Iteration: 1530 lambda_k: 1 Loss: 0.13644873049979134\n",
      "Iteration: 1531 lambda_k: 1 Loss: 0.1363650376523918\n",
      "Iteration: 1532 lambda_k: 1 Loss: 0.13628143508729174\n",
      "Iteration: 1533 lambda_k: 1 Loss: 0.13619792295377844\n",
      "Iteration: 1534 lambda_k: 1 Loss: 0.13611450139918002\n",
      "Iteration: 1535 lambda_k: 1 Loss: 0.13603117057725522\n",
      "Iteration: 1536 lambda_k: 1 Loss: 0.13594793063696245\n",
      "Iteration: 1537 lambda_k: 1 Loss: 0.135864781728677\n",
      "Iteration: 1538 lambda_k: 1 Loss: 0.13578172400278096\n",
      "Iteration: 1539 lambda_k: 1 Loss: 0.13569875761004313\n",
      "Iteration: 1540 lambda_k: 1 Loss: 0.1356158827014571\n",
      "Iteration: 1541 lambda_k: 1 Loss: 0.13553309942819217\n",
      "Iteration: 1542 lambda_k: 1 Loss: 0.13545040794160432\n",
      "Iteration: 1543 lambda_k: 1 Loss: 0.1353678083932506\n",
      "Iteration: 1544 lambda_k: 1 Loss: 0.13528530093489238\n",
      "Iteration: 1545 lambda_k: 1 Loss: 0.1352028857184932\n",
      "Iteration: 1546 lambda_k: 1 Loss: 0.13512056289621674\n",
      "Iteration: 1547 lambda_k: 1 Loss: 0.135038332620426\n",
      "Iteration: 1548 lambda_k: 1 Loss: 0.13495619504368245\n",
      "Iteration: 1549 lambda_k: 1 Loss: 0.134874150318746\n",
      "Iteration: 1550 lambda_k: 1 Loss: 0.13479219859857386\n",
      "Iteration: 1551 lambda_k: 1 Loss: 0.13471034003632035\n",
      "Iteration: 1552 lambda_k: 1 Loss: 0.13462857478533596\n",
      "Iteration: 1553 lambda_k: 1 Loss: 0.1345469029991669\n",
      "Iteration: 1554 lambda_k: 1 Loss: 0.13446532483155438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1555 lambda_k: 1 Loss: 0.13438384043643398\n",
      "Iteration: 1556 lambda_k: 1 Loss: 0.13430244996793517\n",
      "Iteration: 1557 lambda_k: 1 Loss: 0.1342211535803805\n",
      "Iteration: 1558 lambda_k: 1 Loss: 0.1341399514282852\n",
      "Iteration: 1559 lambda_k: 1 Loss: 0.13405884366635637\n",
      "Iteration: 1560 lambda_k: 1 Loss: 0.13397783044949266\n",
      "Iteration: 1561 lambda_k: 1 Loss: 0.1338969119350687\n",
      "Iteration: 1562 lambda_k: 1 Loss: 0.13381608827465385\n",
      "Iteration: 1563 lambda_k: 1 Loss: 0.1337353596250781\n",
      "Iteration: 1564 lambda_k: 1 Loss: 0.13365472614190838\n",
      "Iteration: 1565 lambda_k: 1 Loss: 0.13357418798115225\n",
      "Iteration: 1566 lambda_k: 1 Loss: 0.1334937452988265\n",
      "Iteration: 1567 lambda_k: 1 Loss: 0.13341339825111534\n",
      "Iteration: 1568 lambda_k: 1 Loss: 0.1333331469944172\n",
      "Iteration: 1569 lambda_k: 1 Loss: 0.13325299168533206\n",
      "Iteration: 1570 lambda_k: 1 Loss: 0.13317293248064602\n",
      "Iteration: 1571 lambda_k: 1 Loss: 0.13309296953732708\n",
      "Iteration: 1572 lambda_k: 1 Loss: 0.1330131030125259\n",
      "Iteration: 1573 lambda_k: 1 Loss: 0.13293333306357655\n",
      "Iteration: 1574 lambda_k: 1 Loss: 0.1328536598479965\n",
      "Iteration: 1575 lambda_k: 1 Loss: 0.13277408352353787\n",
      "Iteration: 1576 lambda_k: 1 Loss: 0.1326946042480021\n",
      "Iteration: 1577 lambda_k: 1 Loss: 0.1326152221794498\n",
      "Iteration: 1578 lambda_k: 1 Loss: 0.13253593747614945\n",
      "Iteration: 1579 lambda_k: 1 Loss: 0.13245675029654355\n",
      "Iteration: 1580 lambda_k: 1 Loss: 0.13237766079917315\n",
      "Iteration: 1581 lambda_k: 1 Loss: 0.13229866914289007\n",
      "Iteration: 1582 lambda_k: 1 Loss: 0.13221977548665909\n",
      "Iteration: 1583 lambda_k: 1 Loss: 0.1321409799895975\n",
      "Iteration: 1584 lambda_k: 1 Loss: 0.13206228281101232\n",
      "Iteration: 1585 lambda_k: 1 Loss: 0.1319836841103951\n",
      "Iteration: 1586 lambda_k: 1 Loss: 0.13190518404741355\n",
      "Iteration: 1587 lambda_k: 1 Loss: 0.1318267827819079\n",
      "Iteration: 1588 lambda_k: 1 Loss: 0.13174848047388976\n",
      "Iteration: 1589 lambda_k: 1 Loss: 0.13167027728354103\n",
      "Iteration: 1590 lambda_k: 1 Loss: 0.1315921733712209\n",
      "Iteration: 1591 lambda_k: 1 Loss: 0.13151416889742698\n",
      "Iteration: 1592 lambda_k: 1 Loss: 0.13143626402282568\n",
      "Iteration: 1593 lambda_k: 1 Loss: 0.13135845890827008\n",
      "Iteration: 1594 lambda_k: 1 Loss: 0.13128075371461922\n",
      "Iteration: 1595 lambda_k: 1 Loss: 0.1312031486028438\n",
      "Iteration: 1596 lambda_k: 1 Loss: 0.13112564373376717\n",
      "Iteration: 1597 lambda_k: 1 Loss: 0.13104823926559797\n",
      "Iteration: 1598 lambda_k: 1 Loss: 0.13097093535818646\n",
      "Iteration: 1599 lambda_k: 1 Loss: 0.1308937321654344\n",
      "Iteration: 1600 lambda_k: 1 Loss: 0.13081171489100743\n",
      "Iteration: 1601 lambda_k: 1 Loss: 0.1307169151746691\n",
      "Iteration: 1602 lambda_k: 1 Loss: 0.1306184631575779\n",
      "Iteration: 1603 lambda_k: 1 Loss: 0.13052275359073712\n",
      "Iteration: 1604 lambda_k: 1 Loss: 0.1304309108785573\n",
      "Iteration: 1605 lambda_k: 1 Loss: 0.13034444568754316\n",
      "Iteration: 1606 lambda_k: 1 Loss: 0.13026526021970383\n",
      "Iteration: 1607 lambda_k: 1 Loss: 0.13019154356344223\n",
      "Iteration: 1608 lambda_k: 1 Loss: 0.1301198520718698\n",
      "Iteration: 1609 lambda_k: 1 Loss: 0.1300481403430815\n",
      "Iteration: 1610 lambda_k: 1 Loss: 0.12997616306031476\n",
      "Iteration: 1611 lambda_k: 1 Loss: 0.12990381673235982\n",
      "Iteration: 1612 lambda_k: 1 Loss: 0.12983099425934572\n",
      "Iteration: 1613 lambda_k: 1 Loss: 0.12975765765848946\n",
      "Iteration: 1614 lambda_k: 1 Loss: 0.12968383158458488\n",
      "Iteration: 1615 lambda_k: 1 Loss: 0.12960956375326507\n",
      "Iteration: 1616 lambda_k: 1 Loss: 0.12953490523044922\n",
      "Iteration: 1617 lambda_k: 1 Loss: 0.1294599054281919\n",
      "Iteration: 1618 lambda_k: 1 Loss: 0.12938461009028313\n",
      "Iteration: 1619 lambda_k: 1 Loss: 0.1293093289685292\n",
      "Iteration: 1620 lambda_k: 1 Loss: 0.12923681315423732\n",
      "Iteration: 1621 lambda_k: 1 Loss: 0.12916389638562115\n",
      "Iteration: 1622 lambda_k: 1 Loss: 0.1290906218926789\n",
      "Iteration: 1623 lambda_k: 1 Loss: 0.12901702772345752\n",
      "Iteration: 1624 lambda_k: 1 Loss: 0.12894314756531292\n",
      "Iteration: 1625 lambda_k: 1 Loss: 0.12886901139965448\n",
      "Iteration: 1626 lambda_k: 1 Loss: 0.1287946459668466\n",
      "Iteration: 1627 lambda_k: 1 Loss: 0.12872007513926897\n",
      "Iteration: 1628 lambda_k: 1 Loss: 0.1286453202560188\n",
      "Iteration: 1629 lambda_k: 1 Loss: 0.12857040042582676\n",
      "Iteration: 1630 lambda_k: 1 Loss: 0.12849690503572328\n",
      "Iteration: 1631 lambda_k: 1 Loss: 0.12842381295923716\n",
      "Iteration: 1632 lambda_k: 1 Loss: 0.12835046901135586\n",
      "Iteration: 1633 lambda_k: 1 Loss: 0.1282768974128292\n",
      "Iteration: 1634 lambda_k: 1 Loss: 0.12820312018663424\n",
      "Iteration: 1635 lambda_k: 1 Loss: 0.12812915735463587\n",
      "Iteration: 1636 lambda_k: 1 Loss: 0.12805502711711417\n",
      "Iteration: 1637 lambda_k: 1 Loss: 0.12798074601666068\n",
      "Iteration: 1638 lambda_k: 1 Loss: 0.12790632908771757\n",
      "Iteration: 1639 lambda_k: 1 Loss: 0.1278317899929484\n",
      "Iteration: 1640 lambda_k: 1 Loss: 0.1277571411475424\n",
      "Iteration: 1641 lambda_k: 1 Loss: 0.12768239383247368\n",
      "Iteration: 1642 lambda_k: 1 Loss: 0.1276075582976563\n",
      "Iteration: 1643 lambda_k: 1 Loss: 0.1275326438558654\n",
      "Iteration: 1644 lambda_k: 1 Loss: 0.12745765896822217\n",
      "Iteration: 1645 lambda_k: 1 Loss: 0.1273826113219774\n",
      "Iteration: 1646 lambda_k: 1 Loss: 0.12730750790126907\n",
      "Iteration: 1647 lambda_k: 1 Loss: 0.12723235505147137\n",
      "Iteration: 1648 lambda_k: 1 Loss: 0.12715715853770326\n",
      "Iteration: 1649 lambda_k: 1 Loss: 0.12708192359801534\n",
      "Iteration: 1650 lambda_k: 1 Loss: 0.12700665499173\n",
      "Iteration: 1651 lambda_k: 1 Loss: 0.12693135704336994\n",
      "Iteration: 1652 lambda_k: 1 Loss: 0.12685603368257134\n",
      "Iteration: 1653 lambda_k: 1 Loss: 0.12678068848034466\n",
      "Iteration: 1654 lambda_k: 1 Loss: 0.12670532468201273\n",
      "Iteration: 1655 lambda_k: 1 Loss: 0.12662994523712906\n",
      "Iteration: 1656 lambda_k: 1 Loss: 0.12655455282664993\n",
      "Iteration: 1657 lambda_k: 1 Loss: 0.1264791498876112\n",
      "Iteration: 1658 lambda_k: 1 Loss: 0.1264037386355379\n",
      "Iteration: 1659 lambda_k: 1 Loss: 0.1263283210847937\n",
      "Iteration: 1660 lambda_k: 1 Loss: 0.12625289906705964\n",
      "Iteration: 1661 lambda_k: 1 Loss: 0.12617747424811274\n",
      "Iteration: 1662 lambda_k: 1 Loss: 0.12610204814306183\n",
      "Iteration: 1663 lambda_k: 1 Loss: 0.12602662213018187\n",
      "Iteration: 1664 lambda_k: 1 Loss: 0.12595119746347552\n",
      "Iteration: 1665 lambda_k: 1 Loss: 0.12587577528407973\n",
      "Iteration: 1666 lambda_k: 1 Loss: 0.1258003566306233\n",
      "Iteration: 1667 lambda_k: 1 Loss: 0.12572494244863236\n",
      "Iteration: 1668 lambda_k: 1 Loss: 0.12564953359907127\n",
      "Iteration: 1669 lambda_k: 1 Loss: 0.12557413086609892\n",
      "Iteration: 1670 lambda_k: 1 Loss: 0.12549873496411285\n",
      "Iteration: 1671 lambda_k: 1 Loss: 0.12542334654414666\n",
      "Iteration: 1672 lambda_k: 1 Loss: 0.1253479661996802\n",
      "Iteration: 1673 lambda_k: 1 Loss: 0.12527259447191738\n",
      "Iteration: 1674 lambda_k: 1 Loss: 0.12519723185457993\n",
      "Iteration: 1675 lambda_k: 1 Loss: 0.12512187879826223\n",
      "Iteration: 1676 lambda_k: 1 Loss: 0.12504653571438723\n",
      "Iteration: 1677 lambda_k: 1 Loss: 0.12497120297880109\n",
      "Iteration: 1678 lambda_k: 1 Loss: 0.12489588093503858\n",
      "Iteration: 1679 lambda_k: 1 Loss: 0.12482056989729055\n",
      "Iteration: 1680 lambda_k: 1 Loss: 0.12474527015310052\n",
      "Iteration: 1681 lambda_k: 1 Loss: 0.12466998196581537\n",
      "Iteration: 1682 lambda_k: 1 Loss: 0.12459470557681267\n",
      "Iteration: 1683 lambda_k: 1 Loss: 0.12451944120752548\n",
      "Iteration: 1684 lambda_k: 1 Loss: 0.12444418906128256\n",
      "Iteration: 1685 lambda_k: 1 Loss: 0.12436894932498213\n",
      "Iteration: 1686 lambda_k: 1 Loss: 0.12429372217061295\n",
      "Iteration: 1687 lambda_k: 1 Loss: 0.12421850775663826\n",
      "Iteration: 1688 lambda_k: 1 Loss: 0.12414330622925376\n",
      "Iteration: 1689 lambda_k: 1 Loss: 0.12406811772353259\n",
      "Iteration: 1690 lambda_k: 1 Loss: 0.12399294236446627\n",
      "Iteration: 1691 lambda_k: 1 Loss: 0.12391778026791234\n",
      "Iteration: 1692 lambda_k: 1 Loss: 0.12384263154145636\n",
      "Iteration: 1693 lambda_k: 1 Loss: 0.12376749628519665\n",
      "Iteration: 1694 lambda_k: 1 Loss: 0.12369237459245873\n",
      "Iteration: 1695 lambda_k: 1 Loss: 0.12361726655044575\n",
      "Iteration: 1696 lambda_k: 1 Loss: 0.1235421722408307\n",
      "Iteration: 1697 lambda_k: 1 Loss: 0.12346709174029623\n",
      "Iteration: 1698 lambda_k: 1 Loss: 0.12339202512102632\n",
      "Iteration: 1699 lambda_k: 1 Loss: 0.12331697245115442\n",
      "Iteration: 1700 lambda_k: 1 Loss: 0.1232419337951721\n",
      "Iteration: 1701 lambda_k: 1 Loss: 0.12316690921430276\n",
      "Iteration: 1702 lambda_k: 1 Loss: 0.12309189876683926\n",
      "Iteration: 1703 lambda_k: 1 Loss: 0.12301690250845575\n",
      "Iteration: 1704 lambda_k: 1 Loss: 0.12294192049249171\n",
      "Iteration: 1705 lambda_k: 1 Loss: 0.12286695277021051\n",
      "Iteration: 1706 lambda_k: 1 Loss: 0.12279199939103634\n",
      "Iteration: 1707 lambda_k: 1 Loss: 0.12271706040277051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1708 lambda_k: 1 Loss: 0.12264213585179018\n",
      "Iteration: 1709 lambda_k: 1 Loss: 0.12256722578322979\n",
      "Iteration: 1710 lambda_k: 1 Loss: 0.12249233024114788\n",
      "Iteration: 1711 lambda_k: 1 Loss: 0.12241744927087614\n",
      "Iteration: 1712 lambda_k: 1 Loss: 0.12234258291153383\n",
      "Iteration: 1713 lambda_k: 1 Loss: 0.12226773120448739\n",
      "Iteration: 1714 lambda_k: 1 Loss: 0.12219289419219766\n",
      "Iteration: 1715 lambda_k: 1 Loss: 0.1221180719156911\n",
      "Iteration: 1716 lambda_k: 1 Loss: 0.12204326441525658\n",
      "Iteration: 1717 lambda_k: 1 Loss: 0.12196847173099501\n",
      "Iteration: 1718 lambda_k: 1 Loss: 0.12189369390294824\n",
      "Iteration: 1719 lambda_k: 1 Loss: 0.12181893097111518\n",
      "Iteration: 1720 lambda_k: 1 Loss: 0.1217441829754996\n",
      "Iteration: 1721 lambda_k: 1 Loss: 0.12166944995618323\n",
      "Iteration: 1722 lambda_k: 1 Loss: 0.12159473195339701\n",
      "Iteration: 1723 lambda_k: 1 Loss: 0.12152002900758382\n",
      "Iteration: 1724 lambda_k: 1 Loss: 0.12144534115945545\n",
      "Iteration: 1725 lambda_k: 1 Loss: 0.12137066845004674\n",
      "Iteration: 1726 lambda_k: 1 Loss: 0.12129601092076703\n",
      "Iteration: 1727 lambda_k: 1 Loss: 0.12122136861344876\n",
      "Iteration: 1728 lambda_k: 1 Loss: 0.12114674157039433\n",
      "Iteration: 1729 lambda_k: 1 Loss: 0.12107212983442034\n",
      "Iteration: 1730 lambda_k: 1 Loss: 0.12099753344889995\n",
      "Iteration: 1731 lambda_k: 1 Loss: 0.12092295245780381\n",
      "Iteration: 1732 lambda_k: 1 Loss: 0.12084838690573937\n",
      "Iteration: 1733 lambda_k: 1 Loss: 0.12077383683798934\n",
      "Iteration: 1734 lambda_k: 1 Loss: 0.12069930230054833\n",
      "Iteration: 1735 lambda_k: 1 Loss: 0.12062478334015925\n",
      "Iteration: 1736 lambda_k: 1 Loss: 0.1205502800043486\n",
      "Iteration: 1737 lambda_k: 1 Loss: 0.12047579234412659\n",
      "Iteration: 1738 lambda_k: 1 Loss: 0.12040132040474882\n",
      "Iteration: 1739 lambda_k: 1 Loss: 0.12032686423585563\n",
      "Iteration: 1740 lambda_k: 1 Loss: 0.12025242389007126\n",
      "Iteration: 1741 lambda_k: 1 Loss: 0.1201779994197054\n",
      "Iteration: 1742 lambda_k: 1 Loss: 0.12010359087753436\n",
      "Iteration: 1743 lambda_k: 1 Loss: 0.1200291983174052\n",
      "Iteration: 1744 lambda_k: 1 Loss: 0.11995482179431974\n",
      "Iteration: 1745 lambda_k: 1 Loss: 0.11988046136438615\n",
      "Iteration: 1746 lambda_k: 1 Loss: 0.11980611708481784\n",
      "Iteration: 1747 lambda_k: 1 Loss: 0.11973178901397052\n",
      "Iteration: 1748 lambda_k: 1 Loss: 0.1196574772113828\n",
      "Iteration: 1749 lambda_k: 1 Loss: 0.1195831817378112\n",
      "Iteration: 1750 lambda_k: 1 Loss: 0.1195089026552636\n",
      "Iteration: 1751 lambda_k: 1 Loss: 0.11943464002703297\n",
      "Iteration: 1752 lambda_k: 1 Loss: 0.11936039391773243\n",
      "Iteration: 1753 lambda_k: 1 Loss: 0.11928616439333141\n",
      "Iteration: 1754 lambda_k: 1 Loss: 0.11921195152119228\n",
      "Iteration: 1755 lambda_k: 1 Loss: 0.11913775537010902\n",
      "Iteration: 1756 lambda_k: 1 Loss: 0.11906357601034799\n",
      "Iteration: 1757 lambda_k: 1 Loss: 0.11898941351369287\n",
      "Iteration: 1758 lambda_k: 1 Loss: 0.11891526795349897\n",
      "Iteration: 1759 lambda_k: 1 Loss: 0.11884113940768373\n",
      "Iteration: 1760 lambda_k: 1 Loss: 0.11876702795378888\n",
      "Iteration: 1761 lambda_k: 1 Loss: 0.11869293365308131\n",
      "Iteration: 1762 lambda_k: 1 Loss: 0.11861885660967667\n",
      "Iteration: 1763 lambda_k: 1 Loss: 0.11854479693886107\n",
      "Iteration: 1764 lambda_k: 1 Loss: 0.1184707546944758\n",
      "Iteration: 1765 lambda_k: 1 Loss: 0.11839773294662133\n",
      "Iteration: 1766 lambda_k: 1 Loss: 0.1183248783878018\n",
      "Iteration: 1767 lambda_k: 1 Loss: 0.11825204345130132\n",
      "Iteration: 1768 lambda_k: 1 Loss: 0.11817922811990121\n",
      "Iteration: 1769 lambda_k: 1 Loss: 0.1181064323313122\n",
      "Iteration: 1770 lambda_k: 1 Loss: 0.1180336560247219\n",
      "Iteration: 1771 lambda_k: 1 Loss: 0.11796089914076757\n",
      "Iteration: 1772 lambda_k: 1 Loss: 0.11788816162151966\n",
      "Iteration: 1773 lambda_k: 1 Loss: 0.11781544341044248\n",
      "Iteration: 1774 lambda_k: 1 Loss: 0.11774274445234438\n",
      "Iteration: 1775 lambda_k: 1 Loss: 0.11767006469333001\n",
      "Iteration: 1776 lambda_k: 1 Loss: 0.11759740408075633\n",
      "Iteration: 1777 lambda_k: 1 Loss: 0.1175247625631913\n",
      "Iteration: 1778 lambda_k: 1 Loss: 0.11745214009037404\n",
      "Iteration: 1779 lambda_k: 1 Loss: 0.11737953661317635\n",
      "Iteration: 1780 lambda_k: 1 Loss: 0.11730695208356579\n",
      "Iteration: 1781 lambda_k: 1 Loss: 0.11723438645457003\n",
      "Iteration: 1782 lambda_k: 1 Loss: 0.11716183968024296\n",
      "Iteration: 1783 lambda_k: 1 Loss: 0.1170893117156319\n",
      "Iteration: 1784 lambda_k: 1 Loss: 0.117016802516746\n",
      "Iteration: 1785 lambda_k: 1 Loss: 0.11694431204052634\n",
      "Iteration: 1786 lambda_k: 1 Loss: 0.1168718402448166\n",
      "Iteration: 1787 lambda_k: 1 Loss: 0.11679938708833541\n",
      "Iteration: 1788 lambda_k: 1 Loss: 0.11672695253064949\n",
      "Iteration: 1789 lambda_k: 1 Loss: 0.1166545365125564\n",
      "Iteration: 1790 lambda_k: 1 Loss: 0.11658213902727853\n",
      "Iteration: 1791 lambda_k: 1 Loss: 0.11650976003488743\n",
      "Iteration: 1792 lambda_k: 1 Loss: 0.11643739948814201\n",
      "Iteration: 1793 lambda_k: 1 Loss: 0.11636505734803602\n",
      "Iteration: 1794 lambda_k: 1 Loss: 0.11629273357968922\n",
      "Iteration: 1795 lambda_k: 1 Loss: 0.11622042814868398\n",
      "Iteration: 1796 lambda_k: 1 Loss: 0.11614814102061853\n",
      "Iteration: 1797 lambda_k: 1 Loss: 0.11607587216156767\n",
      "Iteration: 1798 lambda_k: 1 Loss: 0.11600362153829132\n",
      "Iteration: 1799 lambda_k: 1 Loss: 0.11593138911820898\n",
      "Iteration: 1800 lambda_k: 1 Loss: 0.11585917486934112\n",
      "Iteration: 1801 lambda_k: 1 Loss: 0.11578697876028163\n",
      "Iteration: 1802 lambda_k: 1 Loss: 0.1157148007601861\n",
      "Iteration: 1803 lambda_k: 1 Loss: 0.11564264083876057\n",
      "Iteration: 1804 lambda_k: 1 Loss: 0.11557049896624791\n",
      "Iteration: 1805 lambda_k: 1 Loss: 0.11549837511341365\n",
      "Iteration: 1806 lambda_k: 1 Loss: 0.11542626925153293\n",
      "Iteration: 1807 lambda_k: 1 Loss: 0.11535418135237768\n",
      "Iteration: 1808 lambda_k: 1 Loss: 0.11528211138820472\n",
      "Iteration: 1809 lambda_k: 1 Loss: 0.1152100593317441\n",
      "Iteration: 1810 lambda_k: 1 Loss: 0.11513802515618776\n",
      "Iteration: 1811 lambda_k: 1 Loss: 0.11506600883517888\n",
      "Iteration: 1812 lambda_k: 1 Loss: 0.11499401034280127\n",
      "Iteration: 1813 lambda_k: 1 Loss: 0.11492202965356936\n",
      "Iteration: 1814 lambda_k: 1 Loss: 0.11485006674241867\n",
      "Iteration: 1815 lambda_k: 1 Loss: 0.11477812158469625\n",
      "Iteration: 1816 lambda_k: 1 Loss: 0.11470619415615192\n",
      "Iteration: 1817 lambda_k: 1 Loss: 0.11463428443292939\n",
      "Iteration: 1818 lambda_k: 1 Loss: 0.11456239239155802\n",
      "Iteration: 1819 lambda_k: 1 Loss: 0.11449051800894484\n",
      "Iteration: 1820 lambda_k: 1 Loss: 0.11441866126236655\n",
      "Iteration: 1821 lambda_k: 1 Loss: 0.11434682212946225\n",
      "Iteration: 1822 lambda_k: 1 Loss: 0.114275000588226\n",
      "Iteration: 1823 lambda_k: 1 Loss: 0.11420319661700011\n",
      "Iteration: 1824 lambda_k: 1 Loss: 0.11413141019446812\n",
      "Iteration: 1825 lambda_k: 1 Loss: 0.11405964129964852\n",
      "Iteration: 1826 lambda_k: 1 Loss: 0.11398788991188835\n",
      "Iteration: 1827 lambda_k: 1 Loss: 0.11391615601085721\n",
      "Iteration: 1828 lambda_k: 1 Loss: 0.11384443957654133\n",
      "Iteration: 1829 lambda_k: 1 Loss: 0.11377274058923807\n",
      "Iteration: 1830 lambda_k: 1 Loss: 0.1137010590295502\n",
      "Iteration: 1831 lambda_k: 1 Loss: 0.11362939487838099\n",
      "Iteration: 1832 lambda_k: 1 Loss: 0.11355774811692868\n",
      "Iteration: 1833 lambda_k: 1 Loss: 0.11348611872668192\n",
      "Iteration: 1834 lambda_k: 1 Loss: 0.11341450668941484\n",
      "Iteration: 1835 lambda_k: 1 Loss: 0.11334291198718227\n",
      "Iteration: 1836 lambda_k: 1 Loss: 0.11327133460231573\n",
      "Iteration: 1837 lambda_k: 1 Loss: 0.11319977451741864\n",
      "Iteration: 1838 lambda_k: 1 Loss: 0.11312823171536228\n",
      "Iteration: 1839 lambda_k: 1 Loss: 0.11305670617928194\n",
      "Iteration: 1840 lambda_k: 1 Loss: 0.11298519789257282\n",
      "Iteration: 1841 lambda_k: 1 Loss: 0.1129137068388861\n",
      "Iteration: 1842 lambda_k: 1 Loss: 0.11284223300212559\n",
      "Iteration: 1843 lambda_k: 1 Loss: 0.11277077636644393\n",
      "Iteration: 1844 lambda_k: 1 Loss: 0.11269933691623915\n",
      "Iteration: 1845 lambda_k: 1 Loss: 0.11262791463615127\n",
      "Iteration: 1846 lambda_k: 1 Loss: 0.11255650951105933\n",
      "Iteration: 1847 lambda_k: 1 Loss: 0.11248512152607786\n",
      "Iteration: 1848 lambda_k: 1 Loss: 0.11241375066655404\n",
      "Iteration: 1849 lambda_k: 1 Loss: 0.11234239691806464\n",
      "Iteration: 1850 lambda_k: 1 Loss: 0.11227106026641312\n",
      "Iteration: 1851 lambda_k: 1 Loss: 0.11219974069762695\n",
      "Iteration: 1852 lambda_k: 1 Loss: 0.11212843819795469\n",
      "Iteration: 1853 lambda_k: 1 Loss: 0.11205715275386346\n",
      "Iteration: 1854 lambda_k: 1 Loss: 0.11198588435203626\n",
      "Iteration: 1855 lambda_k: 1 Loss: 0.11191463297936961\n",
      "Iteration: 1856 lambda_k: 1 Loss: 0.11184339862297102\n",
      "Iteration: 1857 lambda_k: 1 Loss: 0.11177218127015659\n",
      "Iteration: 1858 lambda_k: 1 Loss: 0.1117009809084488\n",
      "Iteration: 1859 lambda_k: 1 Loss: 0.11162979752557398\n",
      "Iteration: 1860 lambda_k: 1 Loss: 0.11155863110946064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1861 lambda_k: 1 Loss: 0.1114874816482368\n",
      "Iteration: 1862 lambda_k: 1 Loss: 0.11141634913022813\n",
      "Iteration: 1863 lambda_k: 1 Loss: 0.11134523354395595\n",
      "Iteration: 1864 lambda_k: 1 Loss: 0.11127413487813526\n",
      "Iteration: 1865 lambda_k: 1 Loss: 0.11120305312167268\n",
      "Iteration: 1866 lambda_k: 1 Loss: 0.11113198826366472\n",
      "Iteration: 1867 lambda_k: 1 Loss: 0.11106094029339572\n",
      "Iteration: 1868 lambda_k: 1 Loss: 0.11098990920033633\n",
      "Iteration: 1869 lambda_k: 1 Loss: 0.11091889497414166\n",
      "Iteration: 1870 lambda_k: 1 Loss: 0.1108478976046493\n",
      "Iteration: 1871 lambda_k: 1 Loss: 0.11077691708187815\n",
      "Iteration: 1872 lambda_k: 1 Loss: 0.11070595339602625\n",
      "Iteration: 1873 lambda_k: 1 Loss: 0.11063500653746965\n",
      "Iteration: 1874 lambda_k: 1 Loss: 0.11056407649676046\n",
      "Iteration: 1875 lambda_k: 1 Loss: 0.11049316326462556\n",
      "Iteration: 1876 lambda_k: 1 Loss: 0.11042226683196495\n",
      "Iteration: 1877 lambda_k: 1 Loss: 0.11035138718985041\n",
      "Iteration: 1878 lambda_k: 1 Loss: 0.11028052432952397\n",
      "Iteration: 1879 lambda_k: 1 Loss: 0.11020967824239658\n",
      "Iteration: 1880 lambda_k: 1 Loss: 0.11013884892004668\n",
      "Iteration: 1881 lambda_k: 1 Loss: 0.11006803635421872\n",
      "Iteration: 1882 lambda_k: 1 Loss: 0.10999724053682221\n",
      "Iteration: 1883 lambda_k: 1 Loss: 0.10992646145992996\n",
      "Iteration: 1884 lambda_k: 1 Loss: 0.10985569911577708\n",
      "Iteration: 1885 lambda_k: 1 Loss: 0.10978495349675974\n",
      "Iteration: 1886 lambda_k: 1 Loss: 0.1097142245954338\n",
      "Iteration: 1887 lambda_k: 1 Loss: 0.10964351240451373\n",
      "Iteration: 1888 lambda_k: 1 Loss: 0.10957281691687139\n",
      "Iteration: 1889 lambda_k: 1 Loss: 0.10950213812553476\n",
      "Iteration: 1890 lambda_k: 1 Loss: 0.109431476023687\n",
      "Iteration: 1891 lambda_k: 1 Loss: 0.10936083060466514\n",
      "Iteration: 1892 lambda_k: 1 Loss: 0.10929020186195922\n",
      "Iteration: 1893 lambda_k: 1 Loss: 0.10921958978921087\n",
      "Iteration: 1894 lambda_k: 1 Loss: 0.10914899438021256\n",
      "Iteration: 1895 lambda_k: 1 Loss: 0.10907841562890637\n",
      "Iteration: 1896 lambda_k: 1 Loss: 0.10900785352938304\n",
      "Iteration: 1897 lambda_k: 1 Loss: 0.10893730807588094\n",
      "Iteration: 1898 lambda_k: 1 Loss: 0.10886677926278507\n",
      "Iteration: 1899 lambda_k: 1 Loss: 0.1087962670846261\n",
      "Iteration: 1900 lambda_k: 1 Loss: 0.10872577153607937\n",
      "Iteration: 1901 lambda_k: 1 Loss: 0.10865529261196398\n",
      "Iteration: 1902 lambda_k: 1 Loss: 0.10858483030724181\n",
      "Iteration: 1903 lambda_k: 1 Loss: 0.10851438461701668\n",
      "Iteration: 1904 lambda_k: 1 Loss: 0.10844395553653338\n",
      "Iteration: 1905 lambda_k: 1 Loss: 0.10837354306117676\n",
      "Iteration: 1906 lambda_k: 1 Loss: 0.10830314718647087\n",
      "Iteration: 1907 lambda_k: 1 Loss: 0.10823276790807826\n",
      "Iteration: 1908 lambda_k: 1 Loss: 0.10816240522179875\n",
      "Iteration: 1909 lambda_k: 1 Loss: 0.10809205912356899\n",
      "Iteration: 1910 lambda_k: 1 Loss: 0.10802172960946141\n",
      "Iteration: 1911 lambda_k: 1 Loss: 0.1079514166756834\n",
      "Iteration: 1912 lambda_k: 1 Loss: 0.10788112031857663\n",
      "Iteration: 1913 lambda_k: 1 Loss: 0.1078108405346161\n",
      "Iteration: 1914 lambda_k: 1 Loss: 0.10774057732040952\n",
      "Iteration: 1915 lambda_k: 1 Loss: 0.10767033067269632\n",
      "Iteration: 1916 lambda_k: 1 Loss: 0.10760010058834717\n",
      "Iteration: 1917 lambda_k: 1 Loss: 0.10752988706436291\n",
      "Iteration: 1918 lambda_k: 1 Loss: 0.10745969009787419\n",
      "Iteration: 1919 lambda_k: 1 Loss: 0.10738950968614035\n",
      "Iteration: 1920 lambda_k: 1 Loss: 0.10731934582654883\n",
      "Iteration: 1921 lambda_k: 1 Loss: 0.10724919851661463\n",
      "Iteration: 1922 lambda_k: 1 Loss: 0.10717906775397942\n",
      "Iteration: 1923 lambda_k: 1 Loss: 0.1071089535364109\n",
      "Iteration: 1924 lambda_k: 1 Loss: 0.10703885586180195\n",
      "Iteration: 1925 lambda_k: 1 Loss: 0.10696877472817035\n",
      "Iteration: 1926 lambda_k: 1 Loss: 0.10689871013365765\n",
      "Iteration: 1927 lambda_k: 1 Loss: 0.10682866207652887\n",
      "Iteration: 1928 lambda_k: 1 Loss: 0.10675863055517161\n",
      "Iteration: 1929 lambda_k: 1 Loss: 0.1066886155680955\n",
      "Iteration: 1930 lambda_k: 1 Loss: 0.10661861711393157\n",
      "Iteration: 1931 lambda_k: 1 Loss: 0.1065486351914316\n",
      "Iteration: 1932 lambda_k: 1 Loss: 0.1064786697994675\n",
      "Iteration: 1933 lambda_k: 1 Loss: 0.10640872093703066\n",
      "Iteration: 1934 lambda_k: 1 Loss: 0.1063387886032314\n",
      "Iteration: 1935 lambda_k: 1 Loss: 0.10626887279729835\n",
      "Iteration: 1936 lambda_k: 1 Loss: 0.10619897351857788\n",
      "Iteration: 1937 lambda_k: 1 Loss: 0.1061290907665334\n",
      "Iteration: 1938 lambda_k: 1 Loss: 0.10605922454074489\n",
      "Iteration: 1939 lambda_k: 1 Loss: 0.10598937484090835\n",
      "Iteration: 1940 lambda_k: 1 Loss: 0.10591954166683507\n",
      "Iteration: 1941 lambda_k: 1 Loss: 0.10584972501845138\n",
      "Iteration: 1942 lambda_k: 1 Loss: 0.10577992489579766\n",
      "Iteration: 1943 lambda_k: 1 Loss: 0.10571014129902816\n",
      "Iteration: 1944 lambda_k: 1 Loss: 0.10564037422841037\n",
      "Iteration: 1945 lambda_k: 1 Loss: 0.10557062368432428\n",
      "Iteration: 1946 lambda_k: 1 Loss: 0.10550088966726216\n",
      "Iteration: 1947 lambda_k: 1 Loss: 0.10543117217782792\n",
      "Iteration: 1948 lambda_k: 1 Loss: 0.10536147121673635\n",
      "Iteration: 1949 lambda_k: 1 Loss: 0.10529178678481307\n",
      "Iteration: 1950 lambda_k: 1 Loss: 0.10522211888299361\n",
      "Iteration: 1951 lambda_k: 1 Loss: 0.10515246751232311\n",
      "Iteration: 1952 lambda_k: 1 Loss: 0.10508283267395585\n",
      "Iteration: 1953 lambda_k: 1 Loss: 0.10501321436915467\n",
      "Iteration: 1954 lambda_k: 1 Loss: 0.10494361259929047\n",
      "Iteration: 1955 lambda_k: 1 Loss: 0.10487402736584188\n",
      "Iteration: 1956 lambda_k: 1 Loss: 0.10480445867039456\n",
      "Iteration: 1957 lambda_k: 1 Loss: 0.10473490651464094\n",
      "Iteration: 1958 lambda_k: 1 Loss: 0.10466537090037975\n",
      "Iteration: 1959 lambda_k: 1 Loss: 0.10459585182951538\n",
      "Iteration: 1960 lambda_k: 1 Loss: 0.10452634930405758\n",
      "Iteration: 1961 lambda_k: 1 Loss: 0.1044568633261209\n",
      "Iteration: 1962 lambda_k: 1 Loss: 0.10438739389792445\n",
      "Iteration: 1963 lambda_k: 1 Loss: 0.1043179410217912\n",
      "Iteration: 1964 lambda_k: 1 Loss: 0.10424850470014782\n",
      "Iteration: 1965 lambda_k: 1 Loss: 0.10417908493552401\n",
      "Iteration: 1966 lambda_k: 1 Loss: 0.10410968173055217\n",
      "Iteration: 1967 lambda_k: 1 Loss: 0.10404029508796714\n",
      "Iteration: 1968 lambda_k: 1 Loss: 0.10397092501060547\n",
      "Iteration: 1969 lambda_k: 1 Loss: 0.10390157150140518\n",
      "Iteration: 1970 lambda_k: 1 Loss: 0.10383223456340548\n",
      "Iteration: 1971 lambda_k: 1 Loss: 0.10376291419974619\n",
      "Iteration: 1972 lambda_k: 1 Loss: 0.10369361041366737\n",
      "Iteration: 1973 lambda_k: 1 Loss: 0.10362432320850899\n",
      "Iteration: 1974 lambda_k: 1 Loss: 0.10355505258771051\n",
      "Iteration: 1975 lambda_k: 1 Loss: 0.1034857985548105\n",
      "Iteration: 1976 lambda_k: 1 Loss: 0.10341656111344621\n",
      "Iteration: 1977 lambda_k: 1 Loss: 0.1033473402673534\n",
      "Iteration: 1978 lambda_k: 1 Loss: 0.10327813602036563\n",
      "Iteration: 1979 lambda_k: 1 Loss: 0.10320894837641424\n",
      "Iteration: 1980 lambda_k: 1 Loss: 0.10313977733952766\n",
      "Iteration: 1981 lambda_k: 1 Loss: 0.10307062291383141\n",
      "Iteration: 1982 lambda_k: 1 Loss: 0.1030014851035475\n",
      "Iteration: 1983 lambda_k: 1 Loss: 0.10293236391299412\n",
      "Iteration: 1984 lambda_k: 1 Loss: 0.10286325934658533\n",
      "Iteration: 1985 lambda_k: 1 Loss: 0.1027941714088307\n",
      "Iteration: 1986 lambda_k: 1 Loss: 0.102725100104335\n",
      "Iteration: 1987 lambda_k: 1 Loss: 0.10265604543779788\n",
      "Iteration: 1988 lambda_k: 1 Loss: 0.10258700741401341\n",
      "Iteration: 1989 lambda_k: 1 Loss: 0.10251798603786999\n",
      "Iteration: 1990 lambda_k: 1 Loss: 0.10244898131434976\n",
      "Iteration: 1991 lambda_k: 1 Loss: 0.10237999324852858\n",
      "Iteration: 1992 lambda_k: 1 Loss: 0.10231102184557533\n",
      "Iteration: 1993 lambda_k: 1 Loss: 0.10224206711075201\n",
      "Iteration: 1994 lambda_k: 1 Loss: 0.10217312904941317\n",
      "Iteration: 1995 lambda_k: 1 Loss: 0.10210420766700569\n",
      "Iteration: 1996 lambda_k: 1 Loss: 0.10203530296906844\n",
      "Iteration: 1997 lambda_k: 1 Loss: 0.10196641496123203\n",
      "Iteration: 1998 lambda_k: 1 Loss: 0.10189754364921852\n",
      "Iteration: 1999 lambda_k: 1 Loss: 0.1018286890388412\n",
      "Iteration: 2000 lambda_k: 1 Loss: 0.10175985113600403\n",
      "Iteration: 2001 lambda_k: 1 Loss: 0.10169102994670175\n",
      "Iteration: 2002 lambda_k: 1 Loss: 0.1016222254770192\n",
      "Iteration: 2003 lambda_k: 1 Loss: 0.10155343773313143\n",
      "Iteration: 2004 lambda_k: 1 Loss: 0.10148466672130306\n",
      "Iteration: 2005 lambda_k: 1 Loss: 0.10141591244788839\n",
      "Iteration: 2006 lambda_k: 1 Loss: 0.10134717491933078\n",
      "Iteration: 2007 lambda_k: 1 Loss: 0.10127845414216266\n",
      "Iteration: 2008 lambda_k: 1 Loss: 0.10120975012300511\n",
      "Iteration: 2009 lambda_k: 1 Loss: 0.10114106286856761\n",
      "Iteration: 2010 lambda_k: 1 Loss: 0.10107239238564793\n",
      "Iteration: 2011 lambda_k: 1 Loss: 0.10100373868113173\n",
      "Iteration: 2012 lambda_k: 1 Loss: 0.10093510176199239\n",
      "Iteration: 2013 lambda_k: 1 Loss: 0.10086648163529073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2014 lambda_k: 1 Loss: 0.10079787830817485\n",
      "Iteration: 2015 lambda_k: 1 Loss: 0.10072929178787973\n",
      "Iteration: 2016 lambda_k: 1 Loss: 0.10066072208172717\n",
      "Iteration: 2017 lambda_k: 1 Loss: 0.10059216919712549\n",
      "Iteration: 2018 lambda_k: 1 Loss: 0.1005236331415693\n",
      "Iteration: 2019 lambda_k: 1 Loss: 0.10045511392263932\n",
      "Iteration: 2020 lambda_k: 1 Loss: 0.10038661154800202\n",
      "Iteration: 2021 lambda_k: 1 Loss: 0.10031812602540965\n",
      "Iteration: 2022 lambda_k: 1 Loss: 0.10024965736269974\n",
      "Iteration: 2023 lambda_k: 1 Loss: 0.10018120556779514\n",
      "Iteration: 2024 lambda_k: 1 Loss: 0.10011277064870358\n",
      "Iteration: 2025 lambda_k: 1 Loss: 0.10004435261351781\n",
      "Iteration: 2026 lambda_k: 1 Loss: 0.0999759514704149\n",
      "Iteration: 2027 lambda_k: 1 Loss: 0.09990756722765647\n",
      "Iteration: 2028 lambda_k: 1 Loss: 0.09983919989358835\n",
      "Iteration: 2029 lambda_k: 1 Loss: 0.09977084947664029\n",
      "Iteration: 2030 lambda_k: 1 Loss: 0.09970251598532597\n",
      "Iteration: 2031 lambda_k: 1 Loss: 0.0996341994282426\n",
      "Iteration: 2032 lambda_k: 1 Loss: 0.09956589981407082\n",
      "Iteration: 2033 lambda_k: 1 Loss: 0.09949761715157465\n",
      "Iteration: 2034 lambda_k: 1 Loss: 0.09942935144960105\n",
      "Iteration: 2035 lambda_k: 1 Loss: 0.09936110271707992\n",
      "Iteration: 2036 lambda_k: 1 Loss: 0.09929287096302397\n",
      "Iteration: 2037 lambda_k: 1 Loss: 0.09922465619652836\n",
      "Iteration: 2038 lambda_k: 1 Loss: 0.09915645842677066\n",
      "Iteration: 2039 lambda_k: 1 Loss: 0.09908827766301072\n",
      "Iteration: 2040 lambda_k: 1 Loss: 0.09902011391459034\n",
      "Iteration: 2041 lambda_k: 1 Loss: 0.09895196719093334\n",
      "Iteration: 2042 lambda_k: 1 Loss: 0.09888383750154521\n",
      "Iteration: 2043 lambda_k: 1 Loss: 0.098815724856013\n",
      "Iteration: 2044 lambda_k: 1 Loss: 0.09874762926400531\n",
      "Iteration: 2045 lambda_k: 1 Loss: 0.0986795507352718\n",
      "Iteration: 2046 lambda_k: 1 Loss: 0.09861148927964346\n",
      "Iteration: 2047 lambda_k: 1 Loss: 0.09854344490703222\n",
      "Iteration: 2048 lambda_k: 1 Loss: 0.09847541762743081\n",
      "Iteration: 2049 lambda_k: 1 Loss: 0.0984074074509127\n",
      "Iteration: 2050 lambda_k: 1 Loss: 0.09833941438763187\n",
      "Iteration: 2051 lambda_k: 1 Loss: 0.09827143844782282\n",
      "Iteration: 2052 lambda_k: 1 Loss: 0.09820347964180032\n",
      "Iteration: 2053 lambda_k: 1 Loss: 0.09813553797995919\n",
      "Iteration: 2054 lambda_k: 1 Loss: 0.09806761347277451\n",
      "Iteration: 2055 lambda_k: 1 Loss: 0.09799970613080107\n",
      "Iteration: 2056 lambda_k: 1 Loss: 0.09793181596467358\n",
      "Iteration: 2057 lambda_k: 1 Loss: 0.09786394298510634\n",
      "Iteration: 2058 lambda_k: 1 Loss: 0.09779608720289328\n",
      "Iteration: 2059 lambda_k: 1 Loss: 0.09772824862890772\n",
      "Iteration: 2060 lambda_k: 1 Loss: 0.09766042727410232\n",
      "Iteration: 2061 lambda_k: 1 Loss: 0.09759262314950894\n",
      "Iteration: 2062 lambda_k: 1 Loss: 0.09752483626623848\n",
      "Iteration: 2063 lambda_k: 1 Loss: 0.097457066635481\n",
      "Iteration: 2064 lambda_k: 1 Loss: 0.09738931426850536\n",
      "Iteration: 2065 lambda_k: 1 Loss: 0.09732157917665918\n",
      "Iteration: 2066 lambda_k: 1 Loss: 0.09725386137136882\n",
      "Iteration: 2067 lambda_k: 1 Loss: 0.0971861608641392\n",
      "Iteration: 2068 lambda_k: 1 Loss: 0.09711847766655383\n",
      "Iteration: 2069 lambda_k: 1 Loss: 0.09705081179027454\n",
      "Iteration: 2070 lambda_k: 1 Loss: 0.09698316324704148\n",
      "Iteration: 2071 lambda_k: 1 Loss: 0.09691553204867308\n",
      "Iteration: 2072 lambda_k: 1 Loss: 0.09684791820706591\n",
      "Iteration: 2073 lambda_k: 1 Loss: 0.0967803217341945\n",
      "Iteration: 2074 lambda_k: 1 Loss: 0.09671274264211153\n",
      "Iteration: 2075 lambda_k: 1 Loss: 0.09664518094294745\n",
      "Iteration: 2076 lambda_k: 1 Loss: 0.09657763664891059\n",
      "Iteration: 2077 lambda_k: 1 Loss: 0.096510109772287\n",
      "Iteration: 2078 lambda_k: 1 Loss: 0.0964426003254404\n",
      "Iteration: 2079 lambda_k: 1 Loss: 0.09637510832081213\n",
      "Iteration: 2080 lambda_k: 1 Loss: 0.0963076337709211\n",
      "Iteration: 2081 lambda_k: 1 Loss: 0.09624017668836363\n",
      "Iteration: 2082 lambda_k: 1 Loss: 0.09617273708581349\n",
      "Iteration: 2083 lambda_k: 1 Loss: 0.09610531497602172\n",
      "Iteration: 2084 lambda_k: 1 Loss: 0.09603791037181676\n",
      "Iteration: 2085 lambda_k: 1 Loss: 0.09597052328610416\n",
      "Iteration: 2086 lambda_k: 1 Loss: 0.09590315373186666\n",
      "Iteration: 2087 lambda_k: 1 Loss: 0.09583580172216423\n",
      "Iteration: 2088 lambda_k: 1 Loss: 0.09576846727013373\n",
      "Iteration: 2089 lambda_k: 1 Loss: 0.09570115038898916\n",
      "Iteration: 2090 lambda_k: 1 Loss: 0.09563385109202138\n",
      "Iteration: 2091 lambda_k: 1 Loss: 0.09556656939259829\n",
      "Iteration: 2092 lambda_k: 1 Loss: 0.09549930530416455\n",
      "Iteration: 2093 lambda_k: 1 Loss: 0.09543205884024177\n",
      "Iteration: 2094 lambda_k: 1 Loss: 0.09536483001442816\n",
      "Iteration: 2095 lambda_k: 1 Loss: 0.0952976188403989\n",
      "Iteration: 2096 lambda_k: 1 Loss: 0.09523042533190583\n",
      "Iteration: 2097 lambda_k: 1 Loss: 0.09516324950277733\n",
      "Iteration: 2098 lambda_k: 1 Loss: 0.0950960913669186\n",
      "Iteration: 2099 lambda_k: 1 Loss: 0.09502895093831139\n",
      "Iteration: 2100 lambda_k: 1 Loss: 0.09496182823101401\n",
      "Iteration: 2101 lambda_k: 1 Loss: 0.09489472325916147\n",
      "Iteration: 2102 lambda_k: 1 Loss: 0.09482763603696515\n",
      "Iteration: 2103 lambda_k: 1 Loss: 0.09476056657871308\n",
      "Iteration: 2104 lambda_k: 1 Loss: 0.09469351489876975\n",
      "Iteration: 2105 lambda_k: 1 Loss: 0.09462648101157603\n",
      "Iteration: 2106 lambda_k: 1 Loss: 0.09455946493164946\n",
      "Iteration: 2107 lambda_k: 1 Loss: 0.09449246667358381\n",
      "Iteration: 2108 lambda_k: 1 Loss: 0.09442548625204941\n",
      "Iteration: 2109 lambda_k: 1 Loss: 0.09435852368179301\n",
      "Iteration: 2110 lambda_k: 1 Loss: 0.09429157897763775\n",
      "Iteration: 2111 lambda_k: 1 Loss: 0.09422465215448313\n",
      "Iteration: 2112 lambda_k: 1 Loss: 0.09415774322730512\n",
      "Iteration: 2113 lambda_k: 1 Loss: 0.09409085221463481\n",
      "Iteration: 2114 lambda_k: 1 Loss: 0.09402397912461066\n",
      "Iteration: 2115 lambda_k: 1 Loss: 0.09395712397594871\n",
      "Iteration: 2116 lambda_k: 1 Loss: 0.09389028678393065\n",
      "Iteration: 2117 lambda_k: 1 Loss: 0.09382346756391438\n",
      "Iteration: 2118 lambda_k: 1 Loss: 0.09375666633133423\n",
      "Iteration: 2119 lambda_k: 1 Loss: 0.0936898831017009\n",
      "Iteration: 2120 lambda_k: 1 Loss: 0.09362311789060158\n",
      "Iteration: 2121 lambda_k: 1 Loss: 0.09355637071369977\n",
      "Iteration: 2122 lambda_k: 1 Loss: 0.0934896415867355\n",
      "Iteration: 2123 lambda_k: 1 Loss: 0.09342293052552526\n",
      "Iteration: 2124 lambda_k: 1 Loss: 0.09335623754596196\n",
      "Iteration: 2125 lambda_k: 1 Loss: 0.09328956266401499\n",
      "Iteration: 2126 lambda_k: 1 Loss: 0.09322290589573025\n",
      "Iteration: 2127 lambda_k: 1 Loss: 0.09315626725723018\n",
      "Iteration: 2128 lambda_k: 1 Loss: 0.09308964676471375\n",
      "Iteration: 2129 lambda_k: 1 Loss: 0.09302304443445654\n",
      "Iteration: 2130 lambda_k: 1 Loss: 0.09295646028281068\n",
      "Iteration: 2131 lambda_k: 1 Loss: 0.09288989432620488\n",
      "Iteration: 2132 lambda_k: 1 Loss: 0.0928233465811446\n",
      "Iteration: 2133 lambda_k: 1 Loss: 0.0927568170642119\n",
      "Iteration: 2134 lambda_k: 1 Loss: 0.09269030579206557\n",
      "Iteration: 2135 lambda_k: 1 Loss: 0.09262381278144119\n",
      "Iteration: 2136 lambda_k: 1 Loss: 0.09255733804915109\n",
      "Iteration: 2137 lambda_k: 1 Loss: 0.09249088161208438\n",
      "Iteration: 2138 lambda_k: 1 Loss: 0.09242444348720712\n",
      "Iteration: 2139 lambda_k: 1 Loss: 0.09235802369156214\n",
      "Iteration: 2140 lambda_k: 1 Loss: 0.09229162224226936\n",
      "Iteration: 2141 lambda_k: 1 Loss: 0.09222523915652554\n",
      "Iteration: 2142 lambda_k: 1 Loss: 0.09215887445160464\n",
      "Iteration: 2143 lambda_k: 1 Loss: 0.09209252814485752\n",
      "Iteration: 2144 lambda_k: 1 Loss: 0.09202620025371226\n",
      "Iteration: 2145 lambda_k: 1 Loss: 0.09195989079567408\n",
      "Iteration: 2146 lambda_k: 1 Loss: 0.09189359978832552\n",
      "Iteration: 2147 lambda_k: 1 Loss: 0.09182732724932628\n",
      "Iteration: 2148 lambda_k: 1 Loss: 0.09176107319641348\n",
      "Iteration: 2149 lambda_k: 1 Loss: 0.0916948376474016\n",
      "Iteration: 2150 lambda_k: 1 Loss: 0.09162862062018262\n",
      "Iteration: 2151 lambda_k: 1 Loss: 0.09156242213272603\n",
      "Iteration: 2152 lambda_k: 1 Loss: 0.0914962422030789\n",
      "Iteration: 2153 lambda_k: 1 Loss: 0.09143008084797564\n",
      "Iteration: 2154 lambda_k: 1 Loss: 0.09136393808842402\n",
      "Iteration: 2155 lambda_k: 1 Loss: 0.09129781394128728\n",
      "Iteration: 2156 lambda_k: 1 Loss: 0.09123170842492322\n",
      "Iteration: 2157 lambda_k: 1 Loss: 0.09116562155776749\n",
      "Iteration: 2158 lambda_k: 1 Loss: 0.09109955335833383\n",
      "Iteration: 2159 lambda_k: 1 Loss: 0.09103350384521387\n",
      "Iteration: 2160 lambda_k: 1 Loss: 0.09096747303707751\n",
      "Iteration: 2161 lambda_k: 1 Loss: 0.09090146095267282\n",
      "Iteration: 2162 lambda_k: 1 Loss: 0.09083546761082621\n",
      "Iteration: 2163 lambda_k: 1 Loss: 0.09076949303044252\n",
      "Iteration: 2164 lambda_k: 1 Loss: 0.09070353723050528\n",
      "Iteration: 2165 lambda_k: 1 Loss: 0.09063760023007686\n",
      "Iteration: 2166 lambda_k: 1 Loss: 0.09057168204828743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2167 lambda_k: 1 Loss: 0.09050578270436041\n",
      "Iteration: 2168 lambda_k: 1 Loss: 0.09043990221759493\n",
      "Iteration: 2169 lambda_k: 1 Loss: 0.09037404060736715\n",
      "Iteration: 2170 lambda_k: 1 Loss: 0.09030819789313216\n",
      "Iteration: 2171 lambda_k: 1 Loss: 0.09024237409442395\n",
      "Iteration: 2172 lambda_k: 1 Loss: 0.09017656923085575\n",
      "Iteration: 2173 lambda_k: 1 Loss: 0.09011078332211982\n",
      "Iteration: 2174 lambda_k: 1 Loss: 0.09004501638798779\n",
      "Iteration: 2175 lambda_k: 1 Loss: 0.08997926844831063\n",
      "Iteration: 2176 lambda_k: 1 Loss: 0.08991353952301875\n",
      "Iteration: 2177 lambda_k: 1 Loss: 0.08984782963212216\n",
      "Iteration: 2178 lambda_k: 1 Loss: 0.08978213879571058\n",
      "Iteration: 2179 lambda_k: 1 Loss: 0.08971646703395335\n",
      "Iteration: 2180 lambda_k: 1 Loss: 0.08965081436709982\n",
      "Iteration: 2181 lambda_k: 1 Loss: 0.0895851808154792\n",
      "Iteration: 2182 lambda_k: 1 Loss: 0.08951956639950089\n",
      "Iteration: 2183 lambda_k: 1 Loss: 0.08945397113965428\n",
      "Iteration: 2184 lambda_k: 1 Loss: 0.08938839505650922\n",
      "Iteration: 2185 lambda_k: 1 Loss: 0.08932283817071597\n",
      "Iteration: 2186 lambda_k: 1 Loss: 0.08925730050300514\n",
      "Iteration: 2187 lambda_k: 1 Loss: 0.08919178207418803\n",
      "Iteration: 2188 lambda_k: 1 Loss: 0.08912628290515667\n",
      "Iteration: 2189 lambda_k: 1 Loss: 0.089060803016884\n",
      "Iteration: 2190 lambda_k: 1 Loss: 0.08899534243042381\n",
      "Iteration: 2191 lambda_k: 1 Loss: 0.08892990116691106\n",
      "Iteration: 2192 lambda_k: 1 Loss: 0.08886447924756183\n",
      "Iteration: 2193 lambda_k: 1 Loss: 0.08879907669367357\n",
      "Iteration: 2194 lambda_k: 1 Loss: 0.0887336935266251\n",
      "Iteration: 2195 lambda_k: 1 Loss: 0.08866832976787685\n",
      "Iteration: 2196 lambda_k: 1 Loss: 0.08860298543897094\n",
      "Iteration: 2197 lambda_k: 1 Loss: 0.08853766056153127\n",
      "Iteration: 2198 lambda_k: 1 Loss: 0.08847235515726373\n",
      "Iteration: 2199 lambda_k: 1 Loss: 0.08840706924795613\n",
      "Iteration: 2200 lambda_k: 1 Loss: 0.08834180285547862\n",
      "Iteration: 2201 lambda_k: 1 Loss: 0.08827655600178363\n",
      "Iteration: 2202 lambda_k: 1 Loss: 0.08821132870890601\n",
      "Iteration: 2203 lambda_k: 1 Loss: 0.08814612099896318\n",
      "Iteration: 2204 lambda_k: 1 Loss: 0.08808093289415542\n",
      "Iteration: 2205 lambda_k: 1 Loss: 0.08801576441676565\n",
      "Iteration: 2206 lambda_k: 1 Loss: 0.08795061558915997\n",
      "Iteration: 2207 lambda_k: 1 Loss: 0.0878854864337875\n",
      "Iteration: 2208 lambda_k: 1 Loss: 0.08782037697318071\n",
      "Iteration: 2209 lambda_k: 1 Loss: 0.08775528722995542\n",
      "Iteration: 2210 lambda_k: 1 Loss: 0.08769021722681107\n",
      "Iteration: 2211 lambda_k: 1 Loss: 0.08762516698653078\n",
      "Iteration: 2212 lambda_k: 1 Loss: 0.08756013653198143\n",
      "Iteration: 2213 lambda_k: 1 Loss: 0.08749512588611405\n",
      "Iteration: 2214 lambda_k: 1 Loss: 0.08743013507196368\n",
      "Iteration: 2215 lambda_k: 1 Loss: 0.08736516411264966\n",
      "Iteration: 2216 lambda_k: 1 Loss: 0.08730021303137582\n",
      "Iteration: 2217 lambda_k: 1 Loss: 0.08723528185143054\n",
      "Iteration: 2218 lambda_k: 1 Loss: 0.08717037059618701\n",
      "Iteration: 2219 lambda_k: 1 Loss: 0.08710547928910323\n",
      "Iteration: 2220 lambda_k: 1 Loss: 0.08704060795372232\n",
      "Iteration: 2221 lambda_k: 1 Loss: 0.08697575661367254\n",
      "Iteration: 2222 lambda_k: 1 Loss: 0.0869109252926676\n",
      "Iteration: 2223 lambda_k: 1 Loss: 0.08684611401450663\n",
      "Iteration: 2224 lambda_k: 1 Loss: 0.08678132280307459\n",
      "Iteration: 2225 lambda_k: 1 Loss: 0.08671655168234214\n",
      "Iteration: 2226 lambda_k: 1 Loss: 0.086651800676366\n",
      "Iteration: 2227 lambda_k: 1 Loss: 0.08658706980928914\n",
      "Iteration: 2228 lambda_k: 1 Loss: 0.08652235910534078\n",
      "Iteration: 2229 lambda_k: 1 Loss: 0.08645766858883673\n",
      "Iteration: 2230 lambda_k: 1 Loss: 0.08639299828417937\n",
      "Iteration: 2231 lambda_k: 1 Loss: 0.08632834821585803\n",
      "Iteration: 2232 lambda_k: 1 Loss: 0.08626371840844896\n",
      "Iteration: 2233 lambda_k: 1 Loss: 0.08619910888661572\n",
      "Iteration: 2234 lambda_k: 1 Loss: 0.08613451967510907\n",
      "Iteration: 2235 lambda_k: 1 Loss: 0.08606995079876748\n",
      "Iteration: 2236 lambda_k: 1 Loss: 0.08600540228251695\n",
      "Iteration: 2237 lambda_k: 1 Loss: 0.08594087415137155\n",
      "Iteration: 2238 lambda_k: 1 Loss: 0.08587636643043328\n",
      "Iteration: 2239 lambda_k: 1 Loss: 0.08581187914489244\n",
      "Iteration: 2240 lambda_k: 1 Loss: 0.08574741232002775\n",
      "Iteration: 2241 lambda_k: 1 Loss: 0.08568296598120652\n",
      "Iteration: 2242 lambda_k: 1 Loss: 0.08561854015388491\n",
      "Iteration: 2243 lambda_k: 1 Loss: 0.08555413486360797\n",
      "Iteration: 2244 lambda_k: 1 Loss: 0.08548975013600996\n",
      "Iteration: 2245 lambda_k: 1 Loss: 0.08542538599681444\n",
      "Iteration: 2246 lambda_k: 1 Loss: 0.08536104247183463\n",
      "Iteration: 2247 lambda_k: 1 Loss: 0.08529671958697331\n",
      "Iteration: 2248 lambda_k: 1 Loss: 0.08523241736822323\n",
      "Iteration: 2249 lambda_k: 1 Loss: 0.08516813584166731\n",
      "Iteration: 2250 lambda_k: 1 Loss: 0.08510387503347878\n",
      "Iteration: 2251 lambda_k: 1 Loss: 0.08503963496992119\n",
      "Iteration: 2252 lambda_k: 1 Loss: 0.08497541567734898\n",
      "Iteration: 2253 lambda_k: 1 Loss: 0.08491121718220732\n",
      "Iteration: 2254 lambda_k: 1 Loss: 0.08484703951103259\n",
      "Iteration: 2255 lambda_k: 1 Loss: 0.08478288269045237\n",
      "Iteration: 2256 lambda_k: 1 Loss: 0.0847187467471857\n",
      "Iteration: 2257 lambda_k: 1 Loss: 0.08465463170804337\n",
      "Iteration: 2258 lambda_k: 1 Loss: 0.08459053759992807\n",
      "Iteration: 2259 lambda_k: 1 Loss: 0.08452646444983453\n",
      "Iteration: 2260 lambda_k: 1 Loss: 0.08446241228484976\n",
      "Iteration: 2261 lambda_k: 1 Loss: 0.08439838113215328\n",
      "Iteration: 2262 lambda_k: 1 Loss: 0.08433437101901729\n",
      "Iteration: 2263 lambda_k: 1 Loss: 0.08427038197280705\n",
      "Iteration: 2264 lambda_k: 1 Loss: 0.08420641402098074\n",
      "Iteration: 2265 lambda_k: 1 Loss: 0.08414246719109005\n",
      "Iteration: 2266 lambda_k: 1 Loss: 0.08407854151078013\n",
      "Iteration: 2267 lambda_k: 1 Loss: 0.08401463700778987\n",
      "Iteration: 2268 lambda_k: 1 Loss: 0.08395075370995218\n",
      "Iteration: 2269 lambda_k: 1 Loss: 0.08388689164519414\n",
      "Iteration: 2270 lambda_k: 1 Loss: 0.0838230508415373\n",
      "Iteration: 2271 lambda_k: 1 Loss: 0.08375923132709773\n",
      "Iteration: 2272 lambda_k: 1 Loss: 0.08369543313008646\n",
      "Iteration: 2273 lambda_k: 1 Loss: 0.08363165627880946\n",
      "Iteration: 2274 lambda_k: 1 Loss: 0.08356790080166814\n",
      "Iteration: 2275 lambda_k: 1 Loss: 0.08350416672715928\n",
      "Iteration: 2276 lambda_k: 1 Loss: 0.08344045408387554\n",
      "Iteration: 2277 lambda_k: 1 Loss: 0.08337676290050536\n",
      "Iteration: 2278 lambda_k: 1 Loss: 0.08331309320583354\n",
      "Iteration: 2279 lambda_k: 1 Loss: 0.08324944502874114\n",
      "Iteration: 2280 lambda_k: 1 Loss: 0.08318581839820602\n",
      "Iteration: 2281 lambda_k: 1 Loss: 0.08312221334330275\n",
      "Iteration: 2282 lambda_k: 1 Loss: 0.08305862989320313\n",
      "Iteration: 2283 lambda_k: 1 Loss: 0.08299506807717631\n",
      "Iteration: 2284 lambda_k: 1 Loss: 0.0829315279245889\n",
      "Iteration: 2285 lambda_k: 1 Loss: 0.08286800946490537\n",
      "Iteration: 2286 lambda_k: 1 Loss: 0.08280451272768824\n",
      "Iteration: 2287 lambda_k: 1 Loss: 0.08274103774259832\n",
      "Iteration: 2288 lambda_k: 1 Loss: 0.08267758453939486\n",
      "Iteration: 2289 lambda_k: 1 Loss: 0.08261415314793594\n",
      "Iteration: 2290 lambda_k: 1 Loss: 0.08255074359817863\n",
      "Iteration: 2291 lambda_k: 1 Loss: 0.08248735592017921\n",
      "Iteration: 2292 lambda_k: 1 Loss: 0.08242399014409343\n",
      "Iteration: 2293 lambda_k: 1 Loss: 0.0823606463001768\n",
      "Iteration: 2294 lambda_k: 1 Loss: 0.08229732441878478\n",
      "Iteration: 2295 lambda_k: 1 Loss: 0.08223402453037303\n",
      "Iteration: 2296 lambda_k: 1 Loss: 0.08217074666549772\n",
      "Iteration: 2297 lambda_k: 1 Loss: 0.08210749085481572\n",
      "Iteration: 2298 lambda_k: 1 Loss: 0.08204425712908477\n",
      "Iteration: 2299 lambda_k: 1 Loss: 0.08198104551916396\n",
      "Iteration: 2300 lambda_k: 1 Loss: 0.08191785605601375\n",
      "Iteration: 2301 lambda_k: 1 Loss: 0.08185468877069632\n",
      "Iteration: 2302 lambda_k: 1 Loss: 0.08179154369437593\n",
      "Iteration: 2303 lambda_k: 1 Loss: 0.08172842085831886\n",
      "Iteration: 2304 lambda_k: 1 Loss: 0.08166532029389414\n",
      "Iteration: 2305 lambda_k: 1 Loss: 0.08160224203257331\n",
      "Iteration: 2306 lambda_k: 1 Loss: 0.08153918610593103\n",
      "Iteration: 2307 lambda_k: 1 Loss: 0.0814761525456452\n",
      "Iteration: 2308 lambda_k: 1 Loss: 0.08141314138349721\n",
      "Iteration: 2309 lambda_k: 1 Loss: 0.08135015265137228\n",
      "Iteration: 2310 lambda_k: 1 Loss: 0.08128718638125967\n",
      "Iteration: 2311 lambda_k: 1 Loss: 0.08122424260525293\n",
      "Iteration: 2312 lambda_k: 1 Loss: 0.08116132135555018\n",
      "Iteration: 2313 lambda_k: 1 Loss: 0.08109842266445448\n",
      "Iteration: 2314 lambda_k: 1 Loss: 0.08103554656437385\n",
      "Iteration: 2315 lambda_k: 1 Loss: 0.08097269308782196\n",
      "Iteration: 2316 lambda_k: 1 Loss: 0.08090986226741795\n",
      "Iteration: 2317 lambda_k: 1 Loss: 0.08084705413588676\n",
      "Iteration: 2318 lambda_k: 1 Loss: 0.08078426872605968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2319 lambda_k: 1 Loss: 0.08072150607087444\n",
      "Iteration: 2320 lambda_k: 1 Loss: 0.08065876620337553\n",
      "Iteration: 2321 lambda_k: 1 Loss: 0.0805960491567145\n",
      "Iteration: 2322 lambda_k: 1 Loss: 0.08053335496415012\n",
      "Iteration: 2323 lambda_k: 1 Loss: 0.08047068365904884\n",
      "Iteration: 2324 lambda_k: 1 Loss: 0.0804080352748849\n",
      "Iteration: 2325 lambda_k: 1 Loss: 0.08034540984524072\n",
      "Iteration: 2326 lambda_k: 1 Loss: 0.0802828074038071\n",
      "Iteration: 2327 lambda_k: 1 Loss: 0.0802202279843836\n",
      "Iteration: 2328 lambda_k: 1 Loss: 0.08015767162087868\n",
      "Iteration: 2329 lambda_k: 1 Loss: 0.08009513834731022\n",
      "Iteration: 2330 lambda_k: 1 Loss: 0.08003262819780557\n",
      "Iteration: 2331 lambda_k: 1 Loss: 0.07997014120660194\n",
      "Iteration: 2332 lambda_k: 1 Loss: 0.07990767740804672\n",
      "Iteration: 2333 lambda_k: 1 Loss: 0.0798452368365976\n",
      "Iteration: 2334 lambda_k: 1 Loss: 0.07978281952682315\n",
      "Iteration: 2335 lambda_k: 1 Loss: 0.07972042551340285\n",
      "Iteration: 2336 lambda_k: 1 Loss: 0.07965805483112752\n",
      "Iteration: 2337 lambda_k: 1 Loss: 0.07959570751489961\n",
      "Iteration: 2338 lambda_k: 1 Loss: 0.07953338359973344\n",
      "Iteration: 2339 lambda_k: 1 Loss: 0.07947108312075546\n",
      "Iteration: 2340 lambda_k: 1 Loss: 0.07940880611320472\n",
      "Iteration: 2341 lambda_k: 1 Loss: 0.07934655261243295\n",
      "Iteration: 2342 lambda_k: 1 Loss: 0.0792843226539051\n",
      "Iteration: 2343 lambda_k: 1 Loss: 0.07922211627319936\n",
      "Iteration: 2344 lambda_k: 1 Loss: 0.07915993350600771\n",
      "Iteration: 2345 lambda_k: 1 Loss: 0.07909777438813617\n",
      "Iteration: 2346 lambda_k: 1 Loss: 0.0790356389555049\n",
      "Iteration: 2347 lambda_k: 1 Loss: 0.07897352724414881\n",
      "Iteration: 2348 lambda_k: 1 Loss: 0.07891143929021768\n",
      "Iteration: 2349 lambda_k: 1 Loss: 0.07884937512997642\n",
      "Iteration: 2350 lambda_k: 1 Loss: 0.07878733479980563\n",
      "Iteration: 2351 lambda_k: 1 Loss: 0.07872531833620165\n",
      "Iteration: 2352 lambda_k: 1 Loss: 0.07866332577577696\n",
      "Iteration: 2353 lambda_k: 1 Loss: 0.07860135715526062\n",
      "Iteration: 2354 lambda_k: 1 Loss: 0.0785394125114983\n",
      "Iteration: 2355 lambda_k: 1 Loss: 0.07847749188145291\n",
      "Iteration: 2356 lambda_k: 1 Loss: 0.07841559530220461\n",
      "Iteration: 2357 lambda_k: 1 Loss: 0.07835372281095143\n",
      "Iteration: 2358 lambda_k: 1 Loss: 0.07829187444500944\n",
      "Iteration: 2359 lambda_k: 1 Loss: 0.07823005024181295\n",
      "Iteration: 2360 lambda_k: 1 Loss: 0.07816825023891505\n",
      "Iteration: 2361 lambda_k: 1 Loss: 0.07810647447398775\n",
      "Iteration: 2362 lambda_k: 1 Loss: 0.07804472298482246\n",
      "Iteration: 2363 lambda_k: 1 Loss: 0.07798299580933024\n",
      "Iteration: 2364 lambda_k: 1 Loss: 0.07792129298554208\n",
      "Iteration: 2365 lambda_k: 1 Loss: 0.0778596145516093\n",
      "Iteration: 2366 lambda_k: 1 Loss: 0.0777979605458038\n",
      "Iteration: 2367 lambda_k: 1 Loss: 0.07773633100651851\n",
      "Iteration: 2368 lambda_k: 1 Loss: 0.07767472597226754\n",
      "Iteration: 2369 lambda_k: 1 Loss: 0.07761314548168674\n",
      "Iteration: 2370 lambda_k: 1 Loss: 0.07755158957353374\n",
      "Iteration: 2371 lambda_k: 1 Loss: 0.07749005828668856\n",
      "Iteration: 2372 lambda_k: 1 Loss: 0.0774285516601538\n",
      "Iteration: 2373 lambda_k: 1 Loss: 0.0773670697330551\n",
      "Iteration: 2374 lambda_k: 1 Loss: 0.0773056125446412\n",
      "Iteration: 2375 lambda_k: 1 Loss: 0.07724418013428454\n",
      "Iteration: 2376 lambda_k: 1 Loss: 0.07718277254148155\n",
      "Iteration: 2377 lambda_k: 1 Loss: 0.0771213898058529\n",
      "Iteration: 2378 lambda_k: 1 Loss: 0.07706003196714391\n",
      "Iteration: 2379 lambda_k: 1 Loss: 0.07699869906522487\n",
      "Iteration: 2380 lambda_k: 1 Loss: 0.07693739114009139\n",
      "Iteration: 2381 lambda_k: 1 Loss: 0.07687610823186473\n",
      "Iteration: 2382 lambda_k: 1 Loss: 0.07681485038079217\n",
      "Iteration: 2383 lambda_k: 1 Loss: 0.07675361762724726\n",
      "Iteration: 2384 lambda_k: 1 Loss: 0.07669241001173041\n",
      "Iteration: 2385 lambda_k: 1 Loss: 0.07663122757486888\n",
      "Iteration: 2386 lambda_k: 1 Loss: 0.07657007035741743\n",
      "Iteration: 2387 lambda_k: 1 Loss: 0.07650893840025859\n",
      "Iteration: 2388 lambda_k: 1 Loss: 0.07644783174440285\n",
      "Iteration: 2389 lambda_k: 1 Loss: 0.07638675043098925\n",
      "Iteration: 2390 lambda_k: 1 Loss: 0.07632569450128555\n",
      "Iteration: 2391 lambda_k: 1 Loss: 0.07626466399668878\n",
      "Iteration: 2392 lambda_k: 1 Loss: 0.07620365895872534\n",
      "Iteration: 2393 lambda_k: 1 Loss: 0.0761426794290516\n",
      "Iteration: 2394 lambda_k: 1 Loss: 0.07608172544945402\n",
      "Iteration: 2395 lambda_k: 1 Loss: 0.07602079706184975\n",
      "Iteration: 2396 lambda_k: 1 Loss: 0.07595989430828679\n",
      "Iteration: 2397 lambda_k: 1 Loss: 0.07589901723094453\n",
      "Iteration: 2398 lambda_k: 1 Loss: 0.07583816587213386\n",
      "Iteration: 2399 lambda_k: 1 Loss: 0.07577734027429792\n",
      "Iteration: 2400 lambda_k: 1 Loss: 0.075716540480012\n",
      "Iteration: 2401 lambda_k: 1 Loss: 0.07565576653198428\n",
      "Iteration: 2402 lambda_k: 1 Loss: 0.07559501847305593\n",
      "Iteration: 2403 lambda_k: 1 Loss: 0.07553429634620173\n",
      "Iteration: 2404 lambda_k: 1 Loss: 0.07547360019453024\n",
      "Iteration: 2405 lambda_k: 1 Loss: 0.07541293006128424\n",
      "Iteration: 2406 lambda_k: 1 Loss: 0.07535228598984106\n",
      "Iteration: 2407 lambda_k: 1 Loss: 0.07529166802371307\n",
      "Iteration: 2408 lambda_k: 1 Loss: 0.07523107620654787\n",
      "Iteration: 2409 lambda_k: 1 Loss: 0.07517051058212883\n",
      "Iteration: 2410 lambda_k: 1 Loss: 0.07510997119437529\n",
      "Iteration: 2411 lambda_k: 1 Loss: 0.07504945808734322\n",
      "Iteration: 2412 lambda_k: 1 Loss: 0.07498897130522525\n",
      "Iteration: 2413 lambda_k: 1 Loss: 0.07492851089235127\n",
      "Iteration: 2414 lambda_k: 1 Loss: 0.07486807689318868\n",
      "Iteration: 2415 lambda_k: 1 Loss: 0.07480766935234298\n",
      "Iteration: 2416 lambda_k: 1 Loss: 0.0747472883145579\n",
      "Iteration: 2417 lambda_k: 1 Loss: 0.07468693382471589\n",
      "Iteration: 2418 lambda_k: 1 Loss: 0.07462660592783851\n",
      "Iteration: 2419 lambda_k: 1 Loss: 0.07456630466908683\n",
      "Iteration: 2420 lambda_k: 1 Loss: 0.07450603009376179\n",
      "Iteration: 2421 lambda_k: 1 Loss: 0.07444578224730455\n",
      "Iteration: 2422 lambda_k: 1 Loss: 0.07438556117529692\n",
      "Iteration: 2423 lambda_k: 1 Loss: 0.07432536692346176\n",
      "Iteration: 2424 lambda_k: 1 Loss: 0.07426519953766325\n",
      "Iteration: 2425 lambda_k: 1 Loss: 0.07420505906390747\n",
      "Iteration: 2426 lambda_k: 1 Loss: 0.07414494554834267\n",
      "Iteration: 2427 lambda_k: 1 Loss: 0.07408485903725966\n",
      "Iteration: 2428 lambda_k: 1 Loss: 0.07402479957709224\n",
      "Iteration: 2429 lambda_k: 1 Loss: 0.07396476721441757\n",
      "Iteration: 2430 lambda_k: 1 Loss: 0.07390476199595655\n",
      "Iteration: 2431 lambda_k: 1 Loss: 0.07384478396857433\n",
      "Iteration: 2432 lambda_k: 1 Loss: 0.07378483317928049\n",
      "Iteration: 2433 lambda_k: 1 Loss: 0.07372490967522956\n",
      "Iteration: 2434 lambda_k: 1 Loss: 0.07366501350372157\n",
      "Iteration: 2435 lambda_k: 1 Loss: 0.07360514471220209\n",
      "Iteration: 2436 lambda_k: 1 Loss: 0.07354530334826301\n",
      "Iteration: 2437 lambda_k: 1 Loss: 0.07348548945964267\n",
      "Iteration: 2438 lambda_k: 1 Loss: 0.0734257030942263\n",
      "Iteration: 2439 lambda_k: 1 Loss: 0.07336594430004666\n",
      "Iteration: 2440 lambda_k: 1 Loss: 0.07330621312528413\n",
      "Iteration: 2441 lambda_k: 1 Loss: 0.0732465096182673\n",
      "Iteration: 2442 lambda_k: 1 Loss: 0.07318683382747324\n",
      "Iteration: 2443 lambda_k: 1 Loss: 0.0731271858015281\n",
      "Iteration: 2444 lambda_k: 1 Loss: 0.07306756558920728\n",
      "Iteration: 2445 lambda_k: 1 Loss: 0.07300797323943616\n",
      "Iteration: 2446 lambda_k: 1 Loss: 0.07294840880129007\n",
      "Iteration: 2447 lambda_k: 1 Loss: 0.07288887232399521\n",
      "Iteration: 2448 lambda_k: 1 Loss: 0.07282936385692854\n",
      "Iteration: 2449 lambda_k: 1 Loss: 0.07276988344961853\n",
      "Iteration: 2450 lambda_k: 1 Loss: 0.07271043115174557\n",
      "Iteration: 2451 lambda_k: 1 Loss: 0.07265100701314224\n",
      "Iteration: 2452 lambda_k: 1 Loss: 0.07259161108379379\n",
      "Iteration: 2453 lambda_k: 1 Loss: 0.07253224341383857\n",
      "Iteration: 2454 lambda_k: 1 Loss: 0.07247290405356843\n",
      "Iteration: 2455 lambda_k: 1 Loss: 0.07241359305342908\n",
      "Iteration: 2456 lambda_k: 1 Loss: 0.0723543104640206\n",
      "Iteration: 2457 lambda_k: 1 Loss: 0.07229505633609792\n",
      "Iteration: 2458 lambda_k: 1 Loss: 0.07223583072057095\n",
      "Iteration: 2459 lambda_k: 1 Loss: 0.07217663366850531\n",
      "Iteration: 2460 lambda_k: 1 Loss: 0.0721174652311227\n",
      "Iteration: 2461 lambda_k: 1 Loss: 0.07205832545980106\n",
      "Iteration: 2462 lambda_k: 1 Loss: 0.07199921440607536\n",
      "Iteration: 2463 lambda_k: 1 Loss: 0.07194013212163781\n",
      "Iteration: 2464 lambda_k: 1 Loss: 0.0718810786583383\n",
      "Iteration: 2465 lambda_k: 1 Loss: 0.07182205406818486\n",
      "Iteration: 2466 lambda_k: 1 Loss: 0.07176305840334415\n",
      "Iteration: 2467 lambda_k: 1 Loss: 0.07170409171614177\n",
      "Iteration: 2468 lambda_k: 1 Loss: 0.07164515405906265\n",
      "Iteration: 2469 lambda_k: 1 Loss: 0.07158624548475173\n",
      "Iteration: 2470 lambda_k: 1 Loss: 0.07152736604601408\n",
      "Iteration: 2471 lambda_k: 1 Loss: 0.0714685157958156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2472 lambda_k: 1 Loss: 0.0714096947872833\n",
      "Iteration: 2473 lambda_k: 1 Loss: 0.07135090307370572\n",
      "Iteration: 2474 lambda_k: 1 Loss: 0.07129214070853336\n",
      "Iteration: 2475 lambda_k: 1 Loss: 0.07123340774537928\n",
      "Iteration: 2476 lambda_k: 1 Loss: 0.07117470423801935\n",
      "Iteration: 2477 lambda_k: 1 Loss: 0.07111603024039265\n",
      "Iteration: 2478 lambda_k: 1 Loss: 0.07105738580660215\n",
      "Iteration: 2479 lambda_k: 1 Loss: 0.070998770990915\n",
      "Iteration: 2480 lambda_k: 1 Loss: 0.07094018584776295\n",
      "Iteration: 2481 lambda_k: 1 Loss: 0.0708816304317427\n",
      "Iteration: 2482 lambda_k: 1 Loss: 0.07082310479761653\n",
      "Iteration: 2483 lambda_k: 1 Loss: 0.07076460900031267\n",
      "Iteration: 2484 lambda_k: 1 Loss: 0.07070614309492565\n",
      "Iteration: 2485 lambda_k: 1 Loss: 0.07064770713671693\n",
      "Iteration: 2486 lambda_k: 1 Loss: 0.07058930118111517\n",
      "Iteration: 2487 lambda_k: 1 Loss: 0.07053092528371668\n",
      "Iteration: 2488 lambda_k: 1 Loss: 0.07047257950028604\n",
      "Iteration: 2489 lambda_k: 1 Loss: 0.07041426388675634\n",
      "Iteration: 2490 lambda_k: 1 Loss: 0.07035597849922964\n",
      "Iteration: 2491 lambda_k: 1 Loss: 0.07029772339397768\n",
      "Iteration: 2492 lambda_k: 1 Loss: 0.07023949862744192\n",
      "Iteration: 2493 lambda_k: 1 Loss: 0.0701813042562342\n",
      "Iteration: 2494 lambda_k: 1 Loss: 0.0701231403371374\n",
      "Iteration: 2495 lambda_k: 1 Loss: 0.07006500692710542\n",
      "Iteration: 2496 lambda_k: 1 Loss: 0.07000690408326402\n",
      "Iteration: 2497 lambda_k: 1 Loss: 0.0699488318629111\n",
      "Iteration: 2498 lambda_k: 1 Loss: 0.06989079032351714\n",
      "Iteration: 2499 lambda_k: 1 Loss: 0.06983277952272574\n",
      "Iteration: 2500 lambda_k: 1 Loss: 0.06977479951835389\n",
      "Iteration: 2501 lambda_k: 1 Loss: 0.06971685036839277\n",
      "Iteration: 2502 lambda_k: 1 Loss: 0.06965893213100778\n",
      "Iteration: 2503 lambda_k: 1 Loss: 0.06960104486453934\n",
      "Iteration: 2504 lambda_k: 1 Loss: 0.06954318862750308\n",
      "Iteration: 2505 lambda_k: 1 Loss: 0.06948536347859048\n",
      "Iteration: 2506 lambda_k: 1 Loss: 0.06942756947666927\n",
      "Iteration: 2507 lambda_k: 1 Loss: 0.06936980668078385\n",
      "Iteration: 2508 lambda_k: 1 Loss: 0.06931207515015574\n",
      "Iteration: 2509 lambda_k: 1 Loss: 0.06925437494418409\n",
      "Iteration: 2510 lambda_k: 1 Loss: 0.06919670612244612\n",
      "Iteration: 2511 lambda_k: 1 Loss: 0.06913906874469761\n",
      "Iteration: 2512 lambda_k: 1 Loss: 0.06908146287087316\n",
      "Iteration: 2513 lambda_k: 1 Loss: 0.06902388856108704\n",
      "Iteration: 2514 lambda_k: 1 Loss: 0.0689663458756332\n",
      "Iteration: 2515 lambda_k: 1 Loss: 0.06890883487498609\n",
      "Iteration: 2516 lambda_k: 1 Loss: 0.06885135561980087\n",
      "Iteration: 2517 lambda_k: 1 Loss: 0.068793908170914\n",
      "Iteration: 2518 lambda_k: 1 Loss: 0.06873649258934375\n",
      "Iteration: 2519 lambda_k: 1 Loss: 0.06867910893629046\n",
      "Iteration: 2520 lambda_k: 1 Loss: 0.06862175727313724\n",
      "Iteration: 2521 lambda_k: 1 Loss: 0.06856443766145026\n",
      "Iteration: 2522 lambda_k: 1 Loss: 0.06850715016297931\n",
      "Iteration: 2523 lambda_k: 1 Loss: 0.0684498948396581\n",
      "Iteration: 2524 lambda_k: 1 Loss: 0.06839267175360499\n",
      "Iteration: 2525 lambda_k: 1 Loss: 0.0683354809671232\n",
      "Iteration: 2526 lambda_k: 1 Loss: 0.06827832254270151\n",
      "Iteration: 2527 lambda_k: 1 Loss: 0.06822119654301453\n",
      "Iteration: 2528 lambda_k: 1 Loss: 0.06816410303092324\n",
      "Iteration: 2529 lambda_k: 1 Loss: 0.06810704206947531\n",
      "Iteration: 2530 lambda_k: 1 Loss: 0.0680500137219059\n",
      "Iteration: 2531 lambda_k: 1 Loss: 0.06799301805163788\n",
      "Iteration: 2532 lambda_k: 1 Loss: 0.06793605512228226\n",
      "Iteration: 2533 lambda_k: 1 Loss: 0.0678791249976388\n",
      "Iteration: 2534 lambda_k: 1 Loss: 0.06782222774169641\n",
      "Iteration: 2535 lambda_k: 1 Loss: 0.06776536341863364\n",
      "Iteration: 2536 lambda_k: 1 Loss: 0.06770853209281898\n",
      "Iteration: 2537 lambda_k: 1 Loss: 0.06765173382881173\n",
      "Iteration: 2538 lambda_k: 1 Loss: 0.067594968691362\n",
      "Iteration: 2539 lambda_k: 1 Loss: 0.06753823674541154\n",
      "Iteration: 2540 lambda_k: 1 Loss: 0.06748153805609385\n",
      "Iteration: 2541 lambda_k: 1 Loss: 0.06742487268873511\n",
      "Iteration: 2542 lambda_k: 1 Loss: 0.0673682407088542\n",
      "Iteration: 2543 lambda_k: 1 Loss: 0.06731164218216337\n",
      "Iteration: 2544 lambda_k: 1 Loss: 0.0672550771745688\n",
      "Iteration: 2545 lambda_k: 1 Loss: 0.06719854575217094\n",
      "Iteration: 2546 lambda_k: 1 Loss: 0.06714204798126489\n",
      "Iteration: 2547 lambda_k: 1 Loss: 0.06708558392834113\n",
      "Iteration: 2548 lambda_k: 1 Loss: 0.06702915366008584\n",
      "Iteration: 2549 lambda_k: 1 Loss: 0.06697275724338123\n",
      "Iteration: 2550 lambda_k: 1 Loss: 0.06691639474530696\n",
      "Iteration: 2551 lambda_k: 1 Loss: 0.06686006623313906\n",
      "Iteration: 2552 lambda_k: 1 Loss: 0.06680377177435057\n",
      "Iteration: 2553 lambda_k: 1 Loss: 0.06674751143661317\n",
      "Iteration: 2554 lambda_k: 1 Loss: 0.066691285287797\n",
      "Iteration: 2555 lambda_k: 1 Loss: 0.06663509339597116\n",
      "Iteration: 2556 lambda_k: 1 Loss: 0.06657893582940431\n",
      "Iteration: 2557 lambda_k: 1 Loss: 0.06652281265656503\n",
      "Iteration: 2558 lambda_k: 1 Loss: 0.06646672394612219\n",
      "Iteration: 2559 lambda_k: 1 Loss: 0.06641066976694564\n",
      "Iteration: 2560 lambda_k: 1 Loss: 0.06635465018810643\n",
      "Iteration: 2561 lambda_k: 1 Loss: 0.06629866527887748\n",
      "Iteration: 2562 lambda_k: 1 Loss: 0.06624271510873403\n",
      "Iteration: 2563 lambda_k: 1 Loss: 0.06618679974735396\n",
      "Iteration: 2564 lambda_k: 1 Loss: 0.0661309192646184\n",
      "Iteration: 2565 lambda_k: 1 Loss: 0.06607507373061214\n",
      "Iteration: 2566 lambda_k: 1 Loss: 0.06601926321562417\n",
      "Iteration: 2567 lambda_k: 1 Loss: 0.06596348779014802\n",
      "Iteration: 2568 lambda_k: 1 Loss: 0.06590774752488247\n",
      "Iteration: 2569 lambda_k: 1 Loss: 0.06585204249073177\n",
      "Iteration: 2570 lambda_k: 1 Loss: 0.06579637275880633\n",
      "Iteration: 2571 lambda_k: 1 Loss: 0.06574073840042308\n",
      "Iteration: 2572 lambda_k: 1 Loss: 0.06568513948710608\n",
      "Iteration: 2573 lambda_k: 1 Loss: 0.06562957609058703\n",
      "Iteration: 2574 lambda_k: 1 Loss: 0.06557404828257635\n",
      "Iteration: 2575 lambda_k: 1 Loss: 0.06551855613609328\n",
      "Iteration: 2576 lambda_k: 1 Loss: 0.06546309972196179\n",
      "Iteration: 2577 lambda_k: 1 Loss: 0.06540767911406979\n",
      "Iteration: 2578 lambda_k: 1 Loss: 0.0653522943847535\n",
      "Iteration: 2579 lambda_k: 1 Loss: 0.06529694560699957\n",
      "Iteration: 2580 lambda_k: 1 Loss: 0.06524163285400686\n",
      "Iteration: 2581 lambda_k: 1 Loss: 0.06518635622471372\n",
      "Iteration: 2582 lambda_k: 1 Loss: 0.06513111572108804\n",
      "Iteration: 2583 lambda_k: 1 Loss: 0.0650759114789068\n",
      "Iteration: 2584 lambda_k: 1 Loss: 0.06502074356107358\n",
      "Iteration: 2585 lambda_k: 1 Loss: 0.06496561203714915\n",
      "Iteration: 2586 lambda_k: 1 Loss: 0.06491051698163837\n",
      "Iteration: 2587 lambda_k: 1 Loss: 0.06485545846918946\n",
      "Iteration: 2588 lambda_k: 1 Loss: 0.06480043657469874\n",
      "Iteration: 2589 lambda_k: 1 Loss: 0.06474545137327817\n",
      "Iteration: 2590 lambda_k: 1 Loss: 0.06469050294025509\n",
      "Iteration: 2591 lambda_k: 1 Loss: 0.06463559135117233\n",
      "Iteration: 2592 lambda_k: 1 Loss: 0.06458071668178897\n",
      "Iteration: 2593 lambda_k: 1 Loss: 0.06452587900808078\n",
      "Iteration: 2594 lambda_k: 1 Loss: 0.06447107840624036\n",
      "Iteration: 2595 lambda_k: 1 Loss: 0.06441631495267805\n",
      "Iteration: 2596 lambda_k: 1 Loss: 0.06436158872402192\n",
      "Iteration: 2597 lambda_k: 1 Loss: 0.06430689979711865\n",
      "Iteration: 2598 lambda_k: 1 Loss: 0.06425224824903364\n",
      "Iteration: 2599 lambda_k: 1 Loss: 0.0641976341570517\n",
      "Iteration: 2600 lambda_k: 1 Loss: 0.06414305759867729\n",
      "Iteration: 2601 lambda_k: 1 Loss: 0.06408851865163513\n",
      "Iteration: 2602 lambda_k: 1 Loss: 0.0640340173938706\n",
      "Iteration: 2603 lambda_k: 1 Loss: 0.06397955390355023\n",
      "Iteration: 2604 lambda_k: 1 Loss: 0.06392512825906199\n",
      "Iteration: 2605 lambda_k: 1 Loss: 0.06387074053901592\n",
      "Iteration: 2606 lambda_k: 1 Loss: 0.06381639082224448\n",
      "Iteration: 2607 lambda_k: 1 Loss: 0.06376207918780287\n",
      "Iteration: 2608 lambda_k: 1 Loss: 0.06370780571496974\n",
      "Iteration: 2609 lambda_k: 1 Loss: 0.0636535704832475\n",
      "Iteration: 2610 lambda_k: 1 Loss: 0.06359937357236273\n",
      "Iteration: 2611 lambda_k: 1 Loss: 0.06354521506226593\n",
      "Iteration: 2612 lambda_k: 1 Loss: 0.06349109503313365\n",
      "Iteration: 2613 lambda_k: 1 Loss: 0.06343701356536796\n",
      "Iteration: 2614 lambda_k: 1 Loss: 0.06338297073959653\n",
      "Iteration: 2615 lambda_k: 1 Loss: 0.0633289666366733\n",
      "Iteration: 2616 lambda_k: 1 Loss: 0.06327500133767919\n",
      "Iteration: 2617 lambda_k: 1 Loss: 0.06322107492392222\n",
      "Iteration: 2618 lambda_k: 1 Loss: 0.06316718747693807\n",
      "Iteration: 2619 lambda_k: 1 Loss: 0.06311333907849057\n",
      "Iteration: 2620 lambda_k: 1 Loss: 0.06305952981057192\n",
      "Iteration: 2621 lambda_k: 1 Loss: 0.06300575975540339\n",
      "Iteration: 2622 lambda_k: 1 Loss: 0.0629520289954355\n",
      "Iteration: 2623 lambda_k: 1 Loss: 0.06289833761334863\n",
      "Iteration: 2624 lambda_k: 1 Loss: 0.06284468569205325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2625 lambda_k: 1 Loss: 0.06279107331469043\n",
      "Iteration: 2626 lambda_k: 1 Loss: 0.06273750056463241\n",
      "Iteration: 2627 lambda_k: 1 Loss: 0.0626839675254827\n",
      "Iteration: 2628 lambda_k: 1 Loss: 0.06263047428107668\n",
      "Iteration: 2629 lambda_k: 1 Loss: 0.0625770209154821\n",
      "Iteration: 2630 lambda_k: 1 Loss: 0.0625236075129993\n",
      "Iteration: 2631 lambda_k: 1 Loss: 0.06247023415816177\n",
      "Iteration: 2632 lambda_k: 1 Loss: 0.06241690093573635\n",
      "Iteration: 2633 lambda_k: 1 Loss: 0.062363607930723895\n",
      "Iteration: 2634 lambda_k: 1 Loss: 0.062310355228359475\n",
      "Iteration: 2635 lambda_k: 1 Loss: 0.062257142914112905\n",
      "Iteration: 2636 lambda_k: 1 Loss: 0.06220397107368906\n",
      "Iteration: 2637 lambda_k: 1 Loss: 0.062150839818514894\n",
      "Iteration: 2638 lambda_k: 1 Loss: 0.062097749188829474\n",
      "Iteration: 2639 lambda_k: 1 Loss: 0.06204469928257466\n",
      "Iteration: 2640 lambda_k: 1 Loss: 0.06199169018500002\n",
      "Iteration: 2641 lambda_k: 1 Loss: 0.06193872200343199\n",
      "Iteration: 2642 lambda_k: 1 Loss: 0.061885794818435785\n",
      "Iteration: 2643 lambda_k: 1 Loss: 0.06183290871376939\n",
      "Iteration: 2644 lambda_k: 1 Loss: 0.06178006377662466\n",
      "Iteration: 2645 lambda_k: 1 Loss: 0.06172726009533044\n",
      "Iteration: 2646 lambda_k: 1 Loss: 0.061674497758143855\n",
      "Iteration: 2647 lambda_k: 1 Loss: 0.06162177685332091\n",
      "Iteration: 2648 lambda_k: 1 Loss: 0.061569097469348626\n",
      "Iteration: 2649 lambda_k: 1 Loss: 0.06151645969499418\n",
      "Iteration: 2650 lambda_k: 1 Loss: 0.06146386361929583\n",
      "Iteration: 2651 lambda_k: 1 Loss: 0.06141130933152981\n",
      "Iteration: 2652 lambda_k: 1 Loss: 0.06135879692120765\n",
      "Iteration: 2653 lambda_k: 1 Loss: 0.06130632647808178\n",
      "Iteration: 2654 lambda_k: 1 Loss: 0.061253898092148165\n",
      "Iteration: 2655 lambda_k: 1 Loss: 0.06120151185364638\n",
      "Iteration: 2656 lambda_k: 1 Loss: 0.061149167853059294\n",
      "Iteration: 2657 lambda_k: 1 Loss: 0.06109686618111306\n",
      "Iteration: 2658 lambda_k: 1 Loss: 0.06104460692877747\n",
      "Iteration: 2659 lambda_k: 1 Loss: 0.06099239018726646\n",
      "Iteration: 2660 lambda_k: 1 Loss: 0.06094021604803818\n",
      "Iteration: 2661 lambda_k: 1 Loss: 0.060888084602795545\n",
      "Iteration: 2662 lambda_k: 1 Loss: 0.06083599594348629\n",
      "Iteration: 2663 lambda_k: 1 Loss: 0.06078395016230343\n",
      "Iteration: 2664 lambda_k: 1 Loss: 0.06073194735168556\n",
      "Iteration: 2665 lambda_k: 1 Loss: 0.0606799876043171\n",
      "Iteration: 2666 lambda_k: 1 Loss: 0.06062807101312856\n",
      "Iteration: 2667 lambda_k: 1 Loss: 0.06057619767129684\n",
      "Iteration: 2668 lambda_k: 1 Loss: 0.060524367672245734\n",
      "Iteration: 2669 lambda_k: 1 Loss: 0.06047258110964587\n",
      "Iteration: 2670 lambda_k: 1 Loss: 0.060420838077415454\n",
      "Iteration: 2671 lambda_k: 1 Loss: 0.060369138669720136\n",
      "Iteration: 2672 lambda_k: 1 Loss: 0.06031748298097512\n",
      "Iteration: 2673 lambda_k: 1 Loss: 0.06026587110584196\n",
      "Iteration: 2674 lambda_k: 1 Loss: 0.060214303139229916\n",
      "Iteration: 2675 lambda_k: 1 Loss: 0.060162779176297906\n",
      "Iteration: 2676 lambda_k: 1 Loss: 0.06011129931245428\n",
      "Iteration: 2677 lambda_k: 1 Loss: 0.060059863643356644\n",
      "Iteration: 2678 lambda_k: 1 Loss: 0.06000847226491231\n",
      "Iteration: 2679 lambda_k: 1 Loss: 0.059957125273278455\n",
      "Iteration: 2680 lambda_k: 1 Loss: 0.05990582276486238\n",
      "Iteration: 2681 lambda_k: 1 Loss: 0.05985456483632204\n",
      "Iteration: 2682 lambda_k: 1 Loss: 0.059803351584565956\n",
      "Iteration: 2683 lambda_k: 1 Loss: 0.05975218310675371\n",
      "Iteration: 2684 lambda_k: 1 Loss: 0.05970105950029611\n",
      "Iteration: 2685 lambda_k: 1 Loss: 0.05964998086285545\n",
      "Iteration: 2686 lambda_k: 1 Loss: 0.059598947292345876\n",
      "Iteration: 2687 lambda_k: 1 Loss: 0.05954795888693347\n",
      "Iteration: 2688 lambda_k: 1 Loss: 0.05949701574503655\n",
      "Iteration: 2689 lambda_k: 1 Loss: 0.05944611796532602\n",
      "Iteration: 2690 lambda_k: 1 Loss: 0.05939526564672544\n",
      "Iteration: 2691 lambda_k: 1 Loss: 0.059344458888411354\n",
      "Iteration: 2692 lambda_k: 1 Loss: 0.05929369778981362\n",
      "Iteration: 2693 lambda_k: 1 Loss: 0.05924298245061525\n",
      "Iteration: 2694 lambda_k: 1 Loss: 0.059192312970753135\n",
      "Iteration: 2695 lambda_k: 1 Loss: 0.05914168945041789\n",
      "Iteration: 2696 lambda_k: 1 Loss: 0.05909111199005425\n",
      "Iteration: 2697 lambda_k: 1 Loss: 0.059040580690361284\n",
      "Iteration: 2698 lambda_k: 1 Loss: 0.058990095652292414\n",
      "Iteration: 2699 lambda_k: 1 Loss: 0.05893965697705579\n",
      "Iteration: 2700 lambda_k: 1 Loss: 0.058889264766114426\n",
      "Iteration: 2701 lambda_k: 1 Loss: 0.05883891912118653\n",
      "Iteration: 2702 lambda_k: 1 Loss: 0.05878862014424529\n",
      "Iteration: 2703 lambda_k: 1 Loss: 0.05873836793751948\n",
      "Iteration: 2704 lambda_k: 1 Loss: 0.05868816260349346\n",
      "Iteration: 2705 lambda_k: 1 Loss: 0.058638004244907393\n",
      "Iteration: 2706 lambda_k: 1 Loss: 0.05858789296475724\n",
      "Iteration: 2707 lambda_k: 1 Loss: 0.058537828866295216\n",
      "Iteration: 2708 lambda_k: 1 Loss: 0.05848781205302969\n",
      "Iteration: 2709 lambda_k: 1 Loss: 0.058437842628725464\n",
      "Iteration: 2710 lambda_k: 1 Loss: 0.05838792069740385\n",
      "Iteration: 2711 lambda_k: 1 Loss: 0.05833804636334291\n",
      "Iteration: 2712 lambda_k: 1 Loss: 0.05828821973107739\n",
      "Iteration: 2713 lambda_k: 1 Loss: 0.058238440905399126\n",
      "Iteration: 2714 lambda_k: 1 Loss: 0.058188709991356904\n",
      "Iteration: 2715 lambda_k: 1 Loss: 0.05813902709425679\n",
      "Iteration: 2716 lambda_k: 1 Loss: 0.05808939231966213\n",
      "Iteration: 2717 lambda_k: 1 Loss: 0.058039805773393724\n",
      "Iteration: 2718 lambda_k: 1 Loss: 0.05799026756152977\n",
      "Iteration: 2719 lambda_k: 1 Loss: 0.057940777790406045\n",
      "Iteration: 2720 lambda_k: 1 Loss: 0.05789133656661619\n",
      "Iteration: 2721 lambda_k: 1 Loss: 0.05784194399701141\n",
      "Iteration: 2722 lambda_k: 1 Loss: 0.05779260018870082\n",
      "Iteration: 2723 lambda_k: 1 Loss: 0.057743305249051635\n",
      "Iteration: 2724 lambda_k: 1 Loss: 0.0576940592856889\n",
      "Iteration: 2725 lambda_k: 1 Loss: 0.05764486240649519\n",
      "Iteration: 2726 lambda_k: 1 Loss: 0.057595714719612646\n",
      "Iteration: 2727 lambda_k: 1 Loss: 0.05754661633344074\n",
      "Iteration: 2728 lambda_k: 1 Loss: 0.05749756735663705\n",
      "Iteration: 2729 lambda_k: 1 Loss: 0.05744856789811745\n",
      "Iteration: 2730 lambda_k: 1 Loss: 0.057399618067056096\n",
      "Iteration: 2731 lambda_k: 1 Loss: 0.057350717972885386\n",
      "Iteration: 2732 lambda_k: 1 Loss: 0.05730186772529601\n",
      "Iteration: 2733 lambda_k: 1 Loss: 0.05725306743423719\n",
      "Iteration: 2734 lambda_k: 1 Loss: 0.05720431720991618\n",
      "Iteration: 2735 lambda_k: 1 Loss: 0.05715561716279866\n",
      "Iteration: 2736 lambda_k: 1 Loss: 0.05710696740360872\n",
      "Iteration: 2737 lambda_k: 1 Loss: 0.057058368043328554\n",
      "Iteration: 2738 lambda_k: 1 Loss: 0.05700981919319878\n",
      "Iteration: 2739 lambda_k: 1 Loss: 0.05696132096471831\n",
      "Iteration: 2740 lambda_k: 1 Loss: 0.05691287346964403\n",
      "Iteration: 2741 lambda_k: 1 Loss: 0.05686447681999118\n",
      "Iteration: 2742 lambda_k: 1 Loss: 0.05681613112803293\n",
      "Iteration: 2743 lambda_k: 1 Loss: 0.05676783650630067\n",
      "Iteration: 2744 lambda_k: 1 Loss: 0.05671959306759519\n",
      "Iteration: 2745 lambda_k: 1 Loss: 0.056671400924966794\n",
      "Iteration: 2746 lambda_k: 1 Loss: 0.056623260191707166\n",
      "Iteration: 2747 lambda_k: 1 Loss: 0.05657517098137598\n",
      "Iteration: 2748 lambda_k: 1 Loss: 0.05652713340779424\n",
      "Iteration: 2749 lambda_k: 1 Loss: 0.05647914758504066\n",
      "Iteration: 2750 lambda_k: 1 Loss: 0.056431213627450565\n",
      "Iteration: 2751 lambda_k: 1 Loss: 0.0563833316496165\n",
      "Iteration: 2752 lambda_k: 1 Loss: 0.056335501766388345\n",
      "Iteration: 2753 lambda_k: 1 Loss: 0.05628772409287297\n",
      "Iteration: 2754 lambda_k: 1 Loss: 0.0562399987444343\n",
      "Iteration: 2755 lambda_k: 1 Loss: 0.05619232583669285\n",
      "Iteration: 2756 lambda_k: 1 Loss: 0.0561447054855258\n",
      "Iteration: 2757 lambda_k: 1 Loss: 0.05609713780706676\n",
      "Iteration: 2758 lambda_k: 1 Loss: 0.056049622917705585\n",
      "Iteration: 2759 lambda_k: 1 Loss: 0.05600216093408817\n",
      "Iteration: 2760 lambda_k: 1 Loss: 0.055954751978530386\n",
      "Iteration: 2761 lambda_k: 1 Loss: 0.05590739615846511\n",
      "Iteration: 2762 lambda_k: 1 Loss: 0.05586009359415972\n",
      "Iteration: 2763 lambda_k: 1 Loss: 0.055812844404805266\n",
      "Iteration: 2764 lambda_k: 1 Loss: 0.05576564870851978\n",
      "Iteration: 2765 lambda_k: 1 Loss: 0.05571850662327111\n",
      "Iteration: 2766 lambda_k: 1 Loss: 0.05567141826740813\n",
      "Iteration: 2767 lambda_k: 1 Loss: 0.05562438375963811\n",
      "Iteration: 2768 lambda_k: 1 Loss: 0.05557740321892385\n",
      "Iteration: 2769 lambda_k: 1 Loss: 0.055530476764461094\n",
      "Iteration: 2770 lambda_k: 1 Loss: 0.05548360451569244\n",
      "Iteration: 2771 lambda_k: 1 Loss: 0.05543678657099528\n",
      "Iteration: 2772 lambda_k: 1 Loss: 0.055390023088483835\n",
      "Iteration: 2773 lambda_k: 1 Loss: 0.05534331417941213\n",
      "Iteration: 2774 lambda_k: 1 Loss: 0.055296659956528\n",
      "Iteration: 2775 lambda_k: 1 Loss: 0.05525006053882836\n",
      "Iteration: 2776 lambda_k: 1 Loss: 0.055203516048131604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2777 lambda_k: 1 Loss: 0.05515702660621541\n",
      "Iteration: 2778 lambda_k: 1 Loss: 0.05511059233458567\n",
      "Iteration: 2779 lambda_k: 1 Loss: 0.055064213354892484\n",
      "Iteration: 2780 lambda_k: 1 Loss: 0.05501788980109634\n",
      "Iteration: 2781 lambda_k: 1 Loss: 0.05497162177389807\n",
      "Iteration: 2782 lambda_k: 1 Loss: 0.05492540940032213\n",
      "Iteration: 2783 lambda_k: 1 Loss: 0.054879252807866805\n",
      "Iteration: 2784 lambda_k: 1 Loss: 0.054833152120768267\n",
      "Iteration: 2785 lambda_k: 1 Loss: 0.054787107461756426\n",
      "Iteration: 2786 lambda_k: 1 Loss: 0.054741118953906674\n",
      "Iteration: 2787 lambda_k: 1 Loss: 0.05469518672087028\n",
      "Iteration: 2788 lambda_k: 1 Loss: 0.05464931088663652\n",
      "Iteration: 2789 lambda_k: 1 Loss: 0.05460349157541602\n",
      "Iteration: 2790 lambda_k: 1 Loss: 0.05455772891164308\n",
      "Iteration: 2791 lambda_k: 1 Loss: 0.054512023019995776\n",
      "Iteration: 2792 lambda_k: 1 Loss: 0.054466374025401304\n",
      "Iteration: 2793 lambda_k: 1 Loss: 0.05442078205303378\n",
      "Iteration: 2794 lambda_k: 1 Loss: 0.05437524722831195\n",
      "Iteration: 2795 lambda_k: 1 Loss: 0.0543297696768983\n",
      "Iteration: 2796 lambda_k: 1 Loss: 0.05428434952469869\n",
      "Iteration: 2797 lambda_k: 1 Loss: 0.05423898689786189\n",
      "Iteration: 2798 lambda_k: 1 Loss: 0.05419368192277884\n",
      "Iteration: 2799 lambda_k: 1 Loss: 0.05414843472608206\n",
      "Iteration: 2800 lambda_k: 1 Loss: 0.054103245434644794\n",
      "Iteration: 2801 lambda_k: 1 Loss: 0.05405811417558082\n",
      "Iteration: 2802 lambda_k: 1 Loss: 0.05401304107624332\n",
      "Iteration: 2803 lambda_k: 1 Loss: 0.05396802626422454\n",
      "Iteration: 2804 lambda_k: 1 Loss: 0.05392306986735504\n",
      "Iteration: 2805 lambda_k: 1 Loss: 0.05387817201370294\n",
      "Iteration: 2806 lambda_k: 1 Loss: 0.05383333283157336\n",
      "Iteration: 2807 lambda_k: 1 Loss: 0.05378855244950775\n",
      "Iteration: 2808 lambda_k: 1 Loss: 0.05374383099628303\n",
      "Iteration: 2809 lambda_k: 1 Loss: 0.05369916860091099\n",
      "Iteration: 2810 lambda_k: 1 Loss: 0.053654565392637715\n",
      "Iteration: 2811 lambda_k: 1 Loss: 0.05361002150094267\n",
      "Iteration: 2812 lambda_k: 1 Loss: 0.053565537055537835\n",
      "Iteration: 2813 lambda_k: 1 Loss: 0.053521112186367396\n",
      "Iteration: 2814 lambda_k: 1 Loss: 0.05347674702360659\n",
      "Iteration: 2815 lambda_k: 1 Loss: 0.053432441697661065\n",
      "Iteration: 2816 lambda_k: 1 Loss: 0.053388196339166145\n",
      "Iteration: 2817 lambda_k: 1 Loss: 0.053344011078985974\n",
      "Iteration: 2818 lambda_k: 1 Loss: 0.05329988604821272\n",
      "Iteration: 2819 lambda_k: 1 Loss: 0.05325582137816578\n",
      "Iteration: 2820 lambda_k: 1 Loss: 0.05321181720039087\n",
      "Iteration: 2821 lambda_k: 1 Loss: 0.0531678736466594\n",
      "Iteration: 2822 lambda_k: 1 Loss: 0.05312399084896719\n",
      "Iteration: 2823 lambda_k: 1 Loss: 0.05308016893953408\n",
      "Iteration: 2824 lambda_k: 1 Loss: 0.05303640805080272\n",
      "Iteration: 2825 lambda_k: 1 Loss: 0.052992708315437884\n",
      "Iteration: 2826 lambda_k: 1 Loss: 0.052949069866325436\n",
      "Iteration: 2827 lambda_k: 1 Loss: 0.05290549283657139\n",
      "Iteration: 2828 lambda_k: 1 Loss: 0.05286197735950101\n",
      "Iteration: 2829 lambda_k: 1 Loss: 0.052818523568657905\n",
      "Iteration: 2830 lambda_k: 1 Loss: 0.05277513159780301\n",
      "Iteration: 2831 lambda_k: 1 Loss: 0.0527318015809136\n",
      "Iteration: 2832 lambda_k: 1 Loss: 0.05268853365218244\n",
      "Iteration: 2833 lambda_k: 1 Loss: 0.05264532794601653\n",
      "Iteration: 2834 lambda_k: 1 Loss: 0.052602184597036306\n",
      "Iteration: 2835 lambda_k: 1 Loss: 0.05255910374007451\n",
      "Iteration: 2836 lambda_k: 1 Loss: 0.05251608551017517\n",
      "Iteration: 2837 lambda_k: 1 Loss: 0.052473130042592395\n",
      "Iteration: 2838 lambda_k: 1 Loss: 0.052430237472789645\n",
      "Iteration: 2839 lambda_k: 1 Loss: 0.052387407936438236\n",
      "Iteration: 2840 lambda_k: 1 Loss: 0.05234464156941659\n",
      "Iteration: 2841 lambda_k: 1 Loss: 0.05230193850780888\n",
      "Iteration: 2842 lambda_k: 1 Loss: 0.05225929888790392\n",
      "Iteration: 2843 lambda_k: 1 Loss: 0.052216722846194205\n",
      "Iteration: 2844 lambda_k: 1 Loss: 0.052174210519374445\n",
      "Iteration: 2845 lambda_k: 1 Loss: 0.05213176204434065\n",
      "Iteration: 2846 lambda_k: 1 Loss: 0.05208937755818883\n",
      "Iteration: 2847 lambda_k: 1 Loss: 0.052047057198213856\n",
      "Iteration: 2848 lambda_k: 1 Loss: 0.05200480110190804\n",
      "Iteration: 2849 lambda_k: 1 Loss: 0.05196260940696018\n",
      "Iteration: 2850 lambda_k: 1 Loss: 0.05192048225125412\n",
      "Iteration: 2851 lambda_k: 1 Loss: 0.0518784197728675\n",
      "Iteration: 2852 lambda_k: 1 Loss: 0.05183642211007042\n",
      "Iteration: 2853 lambda_k: 1 Loss: 0.05179448940132439\n",
      "Iteration: 2854 lambda_k: 1 Loss: 0.05175262178528085\n",
      "Iteration: 2855 lambda_k: 1 Loss: 0.05171081940077968\n",
      "Iteration: 2856 lambda_k: 1 Loss: 0.05166908238684811\n",
      "Iteration: 2857 lambda_k: 1 Loss: 0.05162741088269926\n",
      "Iteration: 2858 lambda_k: 1 Loss: 0.05158580502773071\n",
      "Iteration: 2859 lambda_k: 1 Loss: 0.05154426496152317\n",
      "Iteration: 2860 lambda_k: 1 Loss: 0.05150279082383913\n",
      "Iteration: 2861 lambda_k: 1 Loss: 0.051461382754621365\n",
      "Iteration: 2862 lambda_k: 1 Loss: 0.05142004089399137\n",
      "Iteration: 2863 lambda_k: 1 Loss: 0.051378765382248136\n",
      "Iteration: 2864 lambda_k: 1 Loss: 0.05133755635986654\n",
      "Iteration: 2865 lambda_k: 1 Loss: 0.05129641396749592\n",
      "Iteration: 2866 lambda_k: 1 Loss: 0.051255338345958475\n",
      "Iteration: 2867 lambda_k: 1 Loss: 0.05121432963624786\n",
      "Iteration: 2868 lambda_k: 1 Loss: 0.05117338797952751\n",
      "Iteration: 2869 lambda_k: 1 Loss: 0.0511325135171292\n",
      "Iteration: 2870 lambda_k: 1 Loss: 0.05109170639055145\n",
      "Iteration: 2871 lambda_k: 1 Loss: 0.05105096674145789\n",
      "Iteration: 2872 lambda_k: 1 Loss: 0.05101029471167567\n",
      "Iteration: 2873 lambda_k: 1 Loss: 0.05096969044319388\n",
      "Iteration: 2874 lambda_k: 1 Loss: 0.05092915407816197\n",
      "Iteration: 2875 lambda_k: 1 Loss: 0.05088868575888789\n",
      "Iteration: 2876 lambda_k: 1 Loss: 0.050848285627836554\n",
      "Iteration: 2877 lambda_k: 1 Loss: 0.050807953827628136\n",
      "Iteration: 2878 lambda_k: 1 Loss: 0.050767690501036446\n",
      "Iteration: 2879 lambda_k: 1 Loss: 0.05072749579098698\n",
      "Iteration: 2880 lambda_k: 1 Loss: 0.05068736984055538\n",
      "Iteration: 2881 lambda_k: 1 Loss: 0.05064731279296563\n",
      "Iteration: 2882 lambda_k: 1 Loss: 0.05060732479158825\n",
      "Iteration: 2883 lambda_k: 1 Loss: 0.05056740597993842\n",
      "Iteration: 2884 lambda_k: 1 Loss: 0.05052755650167444\n",
      "Iteration: 2885 lambda_k: 1 Loss: 0.05048777650059569\n",
      "Iteration: 2886 lambda_k: 1 Loss: 0.050448066120640685\n",
      "Iteration: 2887 lambda_k: 1 Loss: 0.05040842550588547\n",
      "Iteration: 2888 lambda_k: 1 Loss: 0.05036885480054153\n",
      "Iteration: 2889 lambda_k: 1 Loss: 0.05032935414895453\n",
      "Iteration: 2890 lambda_k: 1 Loss: 0.050289923695600904\n",
      "Iteration: 2891 lambda_k: 1 Loss: 0.0502505635886618\n",
      "Iteration: 2892 lambda_k: 1 Loss: 0.050211273966537656\n",
      "Iteration: 2893 lambda_k: 1 Loss: 0.05017205497553514\n",
      "Iteration: 2894 lambda_k: 1 Loss: 0.0501329067619456\n",
      "Iteration: 2895 lambda_k: 1 Loss: 0.05009382947112418\n",
      "Iteration: 2896 lambda_k: 1 Loss: 0.05005482324814758\n",
      "Iteration: 2897 lambda_k: 1 Loss: 0.05001588823827273\n",
      "Iteration: 2898 lambda_k: 1 Loss: 0.049977024586958727\n",
      "Iteration: 2899 lambda_k: 1 Loss: 0.04993823243979203\n",
      "Iteration: 2900 lambda_k: 1 Loss: 0.04989951194245959\n",
      "Iteration: 2901 lambda_k: 1 Loss: 0.04986086324075252\n",
      "Iteration: 2902 lambda_k: 1 Loss: 0.04982228648056534\n",
      "Iteration: 2903 lambda_k: 1 Loss: 0.04978378180790845\n",
      "Iteration: 2904 lambda_k: 1 Loss: 0.04974534936889693\n",
      "Iteration: 2905 lambda_k: 1 Loss: 0.04970698930974273\n",
      "Iteration: 2906 lambda_k: 1 Loss: 0.04966870177671738\n",
      "Iteration: 2907 lambda_k: 1 Loss: 0.049630486915340825\n",
      "Iteration: 2908 lambda_k: 1 Loss: 0.049592344873498916\n",
      "Iteration: 2909 lambda_k: 1 Loss: 0.04955427579702488\n",
      "Iteration: 2910 lambda_k: 1 Loss: 0.049516279832886034\n",
      "Iteration: 2911 lambda_k: 1 Loss: 0.04947834781624528\n",
      "Iteration: 2912 lambda_k: 1 Loss: 0.04944042386370612\n",
      "Iteration: 2913 lambda_k: 1 Loss: 0.04940277200347844\n",
      "Iteration: 2914 lambda_k: 1 Loss: 0.04936526520282441\n",
      "Iteration: 2915 lambda_k: 1 Loss: 0.04932781410511002\n",
      "Iteration: 2916 lambda_k: 1 Loss: 0.049290392619420316\n",
      "Iteration: 2917 lambda_k: 1 Loss: 0.04925301392577911\n",
      "Iteration: 2918 lambda_k: 1 Loss: 0.04921569871308687\n",
      "Iteration: 2919 lambda_k: 1 Loss: 0.04917845823408488\n",
      "Iteration: 2920 lambda_k: 1 Loss: 0.049141292869664836\n",
      "Iteration: 2921 lambda_k: 1 Loss: 0.04910419765064271\n",
      "Iteration: 2922 lambda_k: 1 Loss: 0.04906716747391724\n",
      "Iteration: 2923 lambda_k: 1 Loss: 0.049030199306003885\n",
      "Iteration: 2924 lambda_k: 1 Loss: 0.048993291954864175\n",
      "Iteration: 2925 lambda_k: 1 Loss: 0.048956444988785694\n",
      "Iteration: 2926 lambda_k: 1 Loss: 0.04891965791010659\n",
      "Iteration: 2927 lambda_k: 1 Loss: 0.04888292988653222\n",
      "Iteration: 2928 lambda_k: 1 Loss: 0.04884625985165701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2929 lambda_k: 1 Loss: 0.04880964669485737\n",
      "Iteration: 2930 lambda_k: 1 Loss: 0.048773089379629535\n",
      "Iteration: 2931 lambda_k: 1 Loss: 0.04873658695594341\n",
      "Iteration: 2932 lambda_k: 1 Loss: 0.04870013856858581\n",
      "Iteration: 2933 lambda_k: 1 Loss: 0.04866374337447376\n",
      "Iteration: 2934 lambda_k: 1 Loss: 0.048627400540841526\n",
      "Iteration: 2935 lambda_k: 1 Loss: 0.04859110924781335\n",
      "Iteration: 2936 lambda_k: 1 Loss: 0.048554868691495556\n",
      "Iteration: 2937 lambda_k: 1 Loss: 0.04851867808622789\n",
      "Iteration: 2938 lambda_k: 1 Loss: 0.048482536675910105\n",
      "Iteration: 2939 lambda_k: 1 Loss: 0.048446443695427846\n",
      "Iteration: 2940 lambda_k: 1 Loss: 0.04841039841059669\n",
      "Iteration: 2941 lambda_k: 1 Loss: 0.04837440010921986\n",
      "Iteration: 2942 lambda_k: 1 Loss: 0.0483384480877465\n",
      "Iteration: 2943 lambda_k: 1 Loss: 0.04830254165575525\n",
      "Iteration: 2944 lambda_k: 1 Loss: 0.048266680133405404\n",
      "Iteration: 2945 lambda_k: 1 Loss: 0.04823086285232152\n",
      "Iteration: 2946 lambda_k: 1 Loss: 0.04819508915934102\n",
      "Iteration: 2947 lambda_k: 1 Loss: 0.04815935841125174\n",
      "Iteration: 2948 lambda_k: 1 Loss: 0.04812366997489644\n",
      "Iteration: 2949 lambda_k: 1 Loss: 0.04808802322828438\n",
      "Iteration: 2950 lambda_k: 1 Loss: 0.04805241756008309\n",
      "Iteration: 2951 lambda_k: 1 Loss: 0.04801685236888662\n",
      "Iteration: 2952 lambda_k: 1 Loss: 0.047981327062167706\n",
      "Iteration: 2953 lambda_k: 1 Loss: 0.04794584100647771\n",
      "Iteration: 2954 lambda_k: 1 Loss: 0.04791039323160439\n",
      "Iteration: 2955 lambda_k: 1 Loss: 0.047872216283212456\n",
      "Iteration: 2956 lambda_k: 1 Loss: 0.04783586200741825\n",
      "Iteration: 2957 lambda_k: 1 Loss: 0.047801200922934176\n",
      "Iteration: 2958 lambda_k: 1 Loss: 0.047766939523470164\n",
      "Iteration: 2959 lambda_k: 1 Loss: 0.04773256924289333\n",
      "Iteration: 2960 lambda_k: 1 Loss: 0.04769815651579277\n",
      "Iteration: 2961 lambda_k: 1 Loss: 0.04766393258046932\n",
      "Iteration: 2962 lambda_k: 1 Loss: 0.04763003427214949\n",
      "Iteration: 2963 lambda_k: 1 Loss: 0.047596458862982534\n",
      "Iteration: 2964 lambda_k: 1 Loss: 0.04756312811594389\n",
      "Iteration: 2965 lambda_k: 1 Loss: 0.04752995888697521\n",
      "Iteration: 2966 lambda_k: 1 Loss: 0.047496896789256726\n",
      "Iteration: 2967 lambda_k: 1 Loss: 0.04746391551994741\n",
      "Iteration: 2968 lambda_k: 1 Loss: 0.047431002214514094\n",
      "Iteration: 2969 lambda_k: 1 Loss: 0.04739814534570717\n",
      "Iteration: 2970 lambda_k: 1 Loss: 0.04736533050425184\n",
      "Iteration: 2971 lambda_k: 1 Loss: 0.04733254167387603\n",
      "Iteration: 2972 lambda_k: 1 Loss: 0.04729976387505695\n",
      "Iteration: 2973 lambda_k: 1 Loss: 0.047266984739004377\n",
      "Iteration: 2974 lambda_k: 1 Loss: 0.047234194648318927\n",
      "Iteration: 2975 lambda_k: 1 Loss: 0.04720138611659775\n",
      "Iteration: 2976 lambda_k: 1 Loss: 0.04716855311009685\n",
      "Iteration: 2977 lambda_k: 1 Loss: 0.04713569063051659\n",
      "Iteration: 2978 lambda_k: 1 Loss: 0.047102794548149925\n",
      "Iteration: 2979 lambda_k: 1 Loss: 0.047069861551306755\n",
      "Iteration: 2980 lambda_k: 1 Loss: 0.04703688910355585\n",
      "Iteration: 2981 lambda_k: 1 Loss: 0.04700387536928133\n",
      "Iteration: 2982 lambda_k: 1 Loss: 0.04697081911575451\n",
      "Iteration: 2983 lambda_k: 1 Loss: 0.046937719613435604\n",
      "Iteration: 2984 lambda_k: 1 Loss: 0.0469045765493069\n",
      "Iteration: 2985 lambda_k: 1 Loss: 0.04687138995700157\n",
      "Iteration: 2986 lambda_k: 1 Loss: 0.046838160160729454\n",
      "Iteration: 2987 lambda_k: 1 Loss: 0.04680488772844715\n",
      "Iteration: 2988 lambda_k: 1 Loss: 0.046771573431009864\n",
      "Iteration: 2989 lambda_k: 1 Loss: 0.04673821820582307\n",
      "Iteration: 2990 lambda_k: 1 Loss: 0.04670482312462473\n",
      "Iteration: 2991 lambda_k: 1 Loss: 0.04667138936536094\n",
      "Iteration: 2992 lambda_k: 1 Loss: 0.046637918188009615\n",
      "Iteration: 2993 lambda_k: 1 Loss: 0.04660441091399808\n",
      "Iteration: 2994 lambda_k: 1 Loss: 0.04657086890873872\n",
      "Iteration: 2995 lambda_k: 1 Loss: 0.04653729356680329\n",
      "Iteration: 2996 lambda_k: 1 Loss: 0.04650368629932912\n",
      "Iteration: 2997 lambda_k: 1 Loss: 0.046470048523336346\n",
      "Iteration: 2998 lambda_k: 1 Loss: 0.04643638165270923\n",
      "Iteration: 2999 lambda_k: 1 Loss: 0.046402687090636136\n",
      "Iteration: 3000 lambda_k: 1 Loss: 0.046368966223326785\n",
      "Iteration: 3001 lambda_k: 1 Loss: 0.04633522041484185\n",
      "Iteration: 3002 lambda_k: 1 Loss: 0.04630145100288246\n",
      "Iteration: 3003 lambda_k: 1 Loss: 0.04626765929540595\n",
      "Iteration: 3004 lambda_k: 1 Loss: 0.04623384656795013\n",
      "Iteration: 3005 lambda_k: 1 Loss: 0.04620001406156625\n",
      "Iteration: 3006 lambda_k: 1 Loss: 0.04616616298127409\n",
      "Iteration: 3007 lambda_k: 1 Loss: 0.04613229449496468\n",
      "Iteration: 3008 lambda_k: 1 Loss: 0.04609840973268564\n",
      "Iteration: 3009 lambda_k: 1 Loss: 0.046064509786252095\n",
      "Iteration: 3010 lambda_k: 1 Loss: 0.04603059570913289\n",
      "Iteration: 3011 lambda_k: 1 Loss: 0.04599666851656908\n",
      "Iteration: 3012 lambda_k: 1 Loss: 0.045962729185886855\n",
      "Iteration: 3013 lambda_k: 1 Loss: 0.04592877865697266\n",
      "Iteration: 3014 lambda_k: 1 Loss: 0.04589481783288092\n",
      "Iteration: 3015 lambda_k: 1 Loss: 0.04586084758055175\n",
      "Iteration: 3016 lambda_k: 1 Loss: 0.04582686873161621\n",
      "Iteration: 3017 lambda_k: 1 Loss: 0.04579288208327129\n",
      "Iteration: 3018 lambda_k: 1 Loss: 0.045758888399208525\n",
      "Iteration: 3019 lambda_k: 1 Loss: 0.04572488841058317\n",
      "Iteration: 3020 lambda_k: 1 Loss: 0.04569088281701199\n",
      "Iteration: 3021 lambda_k: 1 Loss: 0.045656872287589675\n",
      "Iteration: 3022 lambda_k: 1 Loss: 0.045622857461915294\n",
      "Iteration: 3023 lambda_k: 1 Loss: 0.04558883895112224\n",
      "Iteration: 3024 lambda_k: 1 Loss: 0.0455548173389045\n",
      "Iteration: 3025 lambda_k: 1 Loss: 0.04552079318253498\n",
      "Iteration: 3026 lambda_k: 1 Loss: 0.045486767013871554\n",
      "Iteration: 3027 lambda_k: 1 Loss: 0.04545273934034676\n",
      "Iteration: 3028 lambda_k: 1 Loss: 0.04541871064593852\n",
      "Iteration: 3029 lambda_k: 1 Loss: 0.04538468139211978\n",
      "Iteration: 3030 lambda_k: 1 Loss: 0.045350652018785007\n",
      "Iteration: 3031 lambda_k: 1 Loss: 0.04531662294515187\n",
      "Iteration: 3032 lambda_k: 1 Loss: 0.045282594570637325\n",
      "Iteration: 3033 lambda_k: 1 Loss: 0.04524856727570711\n",
      "Iteration: 3034 lambda_k: 1 Loss: 0.04521454142269838\n",
      "Iteration: 3035 lambda_k: 1 Loss: 0.04518051735661486\n",
      "Iteration: 3036 lambda_k: 1 Loss: 0.04514649540589451\n",
      "Iteration: 3037 lambda_k: 1 Loss: 0.04511247588314999\n",
      "Iteration: 3038 lambda_k: 1 Loss: 0.04507845908588182\n",
      "Iteration: 3039 lambda_k: 1 Loss: 0.04504444529716469\n",
      "Iteration: 3040 lambda_k: 1 Loss: 0.04501043478630727\n",
      "Iteration: 3041 lambda_k: 1 Loss: 0.04497642780948608\n",
      "Iteration: 3042 lambda_k: 1 Loss: 0.04494242461035384\n",
      "Iteration: 3043 lambda_k: 1 Loss: 0.04490842542062308\n",
      "Iteration: 3044 lambda_k: 1 Loss: 0.044874430460625565\n",
      "Iteration: 3045 lambda_k: 1 Loss: 0.044840439939847994\n",
      "Iteration: 3046 lambda_k: 1 Loss: 0.04480645405744539\n",
      "Iteration: 3047 lambda_k: 1 Loss: 0.0447724730027316\n",
      "Iteration: 3048 lambda_k: 1 Loss: 0.044738496955649174\n",
      "Iteration: 3049 lambda_k: 1 Loss: 0.04470452608721805\n",
      "Iteration: 3050 lambda_k: 1 Loss: 0.044670560559964684\n",
      "Iteration: 3051 lambda_k: 1 Loss: 0.044636600528331634\n",
      "Iteration: 3052 lambda_k: 1 Loss: 0.044602646139068974\n",
      "Iteration: 3053 lambda_k: 1 Loss: 0.044568697531607526\n",
      "Iteration: 3054 lambda_k: 1 Loss: 0.04453475483841543\n",
      "Iteration: 3055 lambda_k: 1 Loss: 0.044500818185337916\n",
      "Iteration: 3056 lambda_k: 1 Loss: 0.044466887691921614\n",
      "Iteration: 3057 lambda_k: 1 Loss: 0.044432963471723465\n",
      "Iteration: 3058 lambda_k: 1 Loss: 0.04439904563260542\n",
      "Iteration: 3059 lambda_k: 1 Loss: 0.04436513427701507\n",
      "Iteration: 3060 lambda_k: 1 Loss: 0.04433122950225324\n",
      "Iteration: 3061 lambda_k: 1 Loss: 0.044297331400728446\n",
      "Iteration: 3062 lambda_k: 1 Loss: 0.044263440060199735\n",
      "Iteration: 3063 lambda_k: 1 Loss: 0.04422955556400758\n",
      "Iteration: 3064 lambda_k: 1 Loss: 0.044195677991293654\n",
      "Iteration: 3065 lambda_k: 1 Loss: 0.044161807417210305\n",
      "Iteration: 3066 lambda_k: 1 Loss: 0.04412794391311944\n",
      "Iteration: 3067 lambda_k: 1 Loss: 0.044094087546782346\n",
      "Iteration: 3068 lambda_k: 1 Loss: 0.04406023838253966\n",
      "Iteration: 3069 lambda_k: 1 Loss: 0.044026396481483134\n",
      "Iteration: 3070 lambda_k: 1 Loss: 0.04399256190161844\n",
      "Iteration: 3071 lambda_k: 1 Loss: 0.04395873469802069\n",
      "Iteration: 3072 lambda_k: 1 Loss: 0.04392491492298146\n",
      "Iteration: 3073 lambda_k: 1 Loss: 0.04389110262614947\n",
      "Iteration: 3074 lambda_k: 1 Loss: 0.043857297854663545\n",
      "Iteration: 3075 lambda_k: 1 Loss: 0.0438235006532795\n",
      "Iteration: 3076 lambda_k: 1 Loss: 0.04378971106449075\n",
      "Iteration: 3077 lambda_k: 1 Loss: 0.043755929128642485\n",
      "Iteration: 3078 lambda_k: 1 Loss: 0.04372215488404065\n",
      "Iteration: 3079 lambda_k: 1 Loss: 0.04368838836705526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3080 lambda_k: 1 Loss: 0.04365462961221874\n",
      "Iteration: 3081 lambda_k: 1 Loss: 0.04362087865231904\n",
      "Iteration: 3082 lambda_k: 1 Loss: 0.043587135518488536\n",
      "Iteration: 3083 lambda_k: 1 Loss: 0.04355340024028801\n",
      "Iteration: 3084 lambda_k: 1 Loss: 0.04351967284578687\n",
      "Iteration: 3085 lambda_k: 1 Loss: 0.0434859533616391\n",
      "Iteration: 3086 lambda_k: 1 Loss: 0.04345224181315524\n",
      "Iteration: 3087 lambda_k: 1 Loss: 0.04341853822437115\n",
      "Iteration: 3088 lambda_k: 1 Loss: 0.043384842618112984\n",
      "Iteration: 3089 lambda_k: 1 Loss: 0.04335115501605897\n",
      "Iteration: 3090 lambda_k: 1 Loss: 0.043317475438798304\n",
      "Iteration: 3091 lambda_k: 1 Loss: 0.043283803905886604\n",
      "Iteration: 3092 lambda_k: 1 Loss: 0.043250140435898955\n",
      "Iteration: 3093 lambda_k: 1 Loss: 0.04321648504648018\n",
      "Iteration: 3094 lambda_k: 1 Loss: 0.04318283775439242\n",
      "Iteration: 3095 lambda_k: 1 Loss: 0.04314919857556041\n",
      "Iteration: 3096 lambda_k: 1 Loss: 0.04311556752511454\n",
      "Iteration: 3097 lambda_k: 1 Loss: 0.04308194461743161\n",
      "Iteration: 3098 lambda_k: 1 Loss: 0.0430483298661735\n",
      "Iteration: 3099 lambda_k: 1 Loss: 0.04301472328432394\n",
      "Iteration: 3100 lambda_k: 1 Loss: 0.04298112488422336\n",
      "Iteration: 3101 lambda_k: 1 Loss: 0.042947534677602\n",
      "Iteration: 3102 lambda_k: 1 Loss: 0.04291395267561138\n",
      "Iteration: 3103 lambda_k: 1 Loss: 0.042880378888854015\n",
      "Iteration: 3104 lambda_k: 1 Loss: 0.04284681332741167\n",
      "Iteration: 3105 lambda_k: 1 Loss: 0.04281325600087239\n",
      "Iteration: 3106 lambda_k: 1 Loss: 0.04277970691835567\n",
      "Iteration: 3107 lambda_k: 1 Loss: 0.04274616608853688\n",
      "Iteration: 3108 lambda_k: 1 Loss: 0.04271263351966997\n",
      "Iteration: 3109 lambda_k: 1 Loss: 0.04267910921960944\n",
      "Iteration: 3110 lambda_k: 1 Loss: 0.0426455931958307\n",
      "Iteration: 3111 lambda_k: 1 Loss: 0.04261208545544988\n",
      "Iteration: 3112 lambda_k: 1 Loss: 0.04257858600524232\n",
      "Iteration: 3113 lambda_k: 1 Loss: 0.042545094851659984\n",
      "Iteration: 3114 lambda_k: 1 Loss: 0.04251161200084845\n",
      "Iteration: 3115 lambda_k: 1 Loss: 0.042478137458662656\n",
      "Iteration: 3116 lambda_k: 1 Loss: 0.04244467123068176\n",
      "Iteration: 3117 lambda_k: 1 Loss: 0.042411213322223626\n",
      "Iteration: 3118 lambda_k: 1 Loss: 0.04237776373835806\n",
      "Iteration: 3119 lambda_k: 1 Loss: 0.042344322483919984\n",
      "Iteration: 3120 lambda_k: 1 Loss: 0.04231088956352126\n",
      "Iteration: 3121 lambda_k: 1 Loss: 0.04227746498156235\n",
      "Iteration: 3122 lambda_k: 1 Loss: 0.04224404874224318\n",
      "Iteration: 3123 lambda_k: 1 Loss: 0.04221064084957351\n",
      "Iteration: 3124 lambda_k: 1 Loss: 0.04217724130738272\n",
      "Iteration: 3125 lambda_k: 1 Loss: 0.0421438501193293\n",
      "Iteration: 3126 lambda_k: 1 Loss: 0.04211046728890946\n",
      "Iteration: 3127 lambda_k: 1 Loss: 0.04207709281946544\n",
      "Iteration: 3128 lambda_k: 1 Loss: 0.04204372671419372\n",
      "Iteration: 3129 lambda_k: 1 Loss: 0.0420103689761522\n",
      "Iteration: 3130 lambda_k: 1 Loss: 0.04197701960826752\n",
      "Iteration: 3131 lambda_k: 1 Loss: 0.041943678613341674\n",
      "Iteration: 3132 lambda_k: 1 Loss: 0.04191034599405869\n",
      "Iteration: 3133 lambda_k: 1 Loss: 0.04187702175299023\n",
      "Iteration: 3134 lambda_k: 1 Loss: 0.041843705892601696\n",
      "Iteration: 3135 lambda_k: 1 Loss: 0.04181039841525742\n",
      "Iteration: 3136 lambda_k: 1 Loss: 0.041777099323226013\n",
      "Iteration: 3137 lambda_k: 1 Loss: 0.041743808618685145\n",
      "Iteration: 3138 lambda_k: 1 Loss: 0.04171052630372604\n",
      "Iteration: 3139 lambda_k: 1 Loss: 0.04167725238035813\n",
      "Iteration: 3140 lambda_k: 1 Loss: 0.041643986850512876\n",
      "Iteration: 3141 lambda_k: 1 Loss: 0.0416107297160479\n",
      "Iteration: 3142 lambda_k: 1 Loss: 0.04157748097875067\n",
      "Iteration: 3143 lambda_k: 1 Loss: 0.04154424064034201\n",
      "Iteration: 3144 lambda_k: 1 Loss: 0.04151100870247925\n",
      "Iteration: 3145 lambda_k: 1 Loss: 0.041477785166759706\n",
      "Iteration: 3146 lambda_k: 1 Loss: 0.041444570034723245\n",
      "Iteration: 3147 lambda_k: 1 Loss: 0.04141136330785559\n",
      "Iteration: 3148 lambda_k: 1 Loss: 0.04137816498759052\n",
      "Iteration: 3149 lambda_k: 1 Loss: 0.04134497507531263\n",
      "Iteration: 3150 lambda_k: 1 Loss: 0.04131179357235972\n",
      "Iteration: 3151 lambda_k: 1 Loss: 0.04127862048002505\n",
      "Iteration: 3152 lambda_k: 1 Loss: 0.041245455799559363\n",
      "Iteration: 3153 lambda_k: 1 Loss: 0.04121229953217299\n",
      "Iteration: 3154 lambda_k: 1 Loss: 0.041179151679037726\n",
      "Iteration: 3155 lambda_k: 1 Loss: 0.04114601224128873\n",
      "Iteration: 3156 lambda_k: 1 Loss: 0.04111288122002608\n",
      "Iteration: 3157 lambda_k: 1 Loss: 0.041079758616316416\n",
      "Iteration: 3158 lambda_k: 1 Loss: 0.04104664443119458\n",
      "Iteration: 3159 lambda_k: 1 Loss: 0.0410135386656648\n",
      "Iteration: 3160 lambda_k: 1 Loss: 0.04098044132070246\n",
      "Iteration: 3161 lambda_k: 1 Loss: 0.04094735239725492\n",
      "Iteration: 3162 lambda_k: 1 Loss: 0.04091427189624299\n",
      "Iteration: 3163 lambda_k: 1 Loss: 0.040881199818562045\n",
      "Iteration: 3164 lambda_k: 1 Loss: 0.040848136165083114\n",
      "Iteration: 3165 lambda_k: 1 Loss: 0.0408150809366537\n",
      "Iteration: 3166 lambda_k: 1 Loss: 0.04078203413409911\n",
      "Iteration: 3167 lambda_k: 1 Loss: 0.04074899575822293\n",
      "Iteration: 3168 lambda_k: 1 Loss: 0.0407159658098081\n",
      "Iteration: 3169 lambda_k: 1 Loss: 0.040682944289617905\n",
      "Iteration: 3170 lambda_k: 1 Loss: 0.040649931198396314\n",
      "Iteration: 3171 lambda_k: 1 Loss: 0.040616926536868964\n",
      "Iteration: 3172 lambda_k: 1 Loss: 0.04058393030574376\n",
      "Iteration: 3173 lambda_k: 1 Loss: 0.0405509425057116\n",
      "Iteration: 3174 lambda_k: 1 Loss: 0.040517963137446766\n",
      "Iteration: 3175 lambda_k: 1 Loss: 0.040484992201607624\n",
      "Iteration: 3176 lambda_k: 1 Loss: 0.04045202969883719\n",
      "Iteration: 3177 lambda_k: 1 Loss: 0.04041907562976353\n",
      "Iteration: 3178 lambda_k: 1 Loss: 0.0403861299950002\n",
      "Iteration: 3179 lambda_k: 1 Loss: 0.04035319279514679\n",
      "Iteration: 3180 lambda_k: 1 Loss: 0.04032026403078919\n",
      "Iteration: 3181 lambda_k: 1 Loss: 0.04028734370250023\n",
      "Iteration: 3182 lambda_k: 1 Loss: 0.04025443181083969\n",
      "Iteration: 3183 lambda_k: 1 Loss: 0.040221528356354834\n",
      "Iteration: 3184 lambda_k: 1 Loss: 0.04018863333958082\n",
      "Iteration: 3185 lambda_k: 1 Loss: 0.040155746761040785\n",
      "Iteration: 3186 lambda_k: 1 Loss: 0.04012286862124618\n",
      "Iteration: 3187 lambda_k: 1 Loss: 0.04008999892069723\n",
      "Iteration: 3188 lambda_k: 1 Loss: 0.04005713765988287\n",
      "Iteration: 3189 lambda_k: 1 Loss: 0.04002428483928119\n",
      "Iteration: 3190 lambda_k: 1 Loss: 0.03999144045935946\n",
      "Iteration: 3191 lambda_k: 1 Loss: 0.03995860452057458\n",
      "Iteration: 3192 lambda_k: 1 Loss: 0.03992577702337299\n",
      "Iteration: 3193 lambda_k: 1 Loss: 0.03989295796819101\n",
      "Iteration: 3194 lambda_k: 1 Loss: 0.039860147355455025\n",
      "Iteration: 3195 lambda_k: 1 Loss: 0.039827345185581536\n",
      "Iteration: 3196 lambda_k: 1 Loss: 0.039794551458977236\n",
      "Iteration: 3197 lambda_k: 1 Loss: 0.03976176617603935\n",
      "Iteration: 3198 lambda_k: 1 Loss: 0.03972898933715553\n",
      "Iteration: 3199 lambda_k: 1 Loss: 0.03969622094270415\n",
      "Iteration: 3200 lambda_k: 1 Loss: 0.03966346099305426\n",
      "Iteration: 3201 lambda_k: 1 Loss: 0.0396307094885658\n",
      "Iteration: 3202 lambda_k: 1 Loss: 0.039597966429589515\n",
      "Iteration: 3203 lambda_k: 1 Loss: 0.03956523181646723\n",
      "Iteration: 3204 lambda_k: 1 Loss: 0.03953250564953181\n",
      "Iteration: 3205 lambda_k: 1 Loss: 0.039499787929107286\n",
      "Iteration: 3206 lambda_k: 1 Loss: 0.039467078655508775\n",
      "Iteration: 3207 lambda_k: 1 Loss: 0.0394343778290428\n",
      "Iteration: 3208 lambda_k: 1 Loss: 0.039401685450007023\n",
      "Iteration: 3209 lambda_k: 1 Loss: 0.0393690015186906\n",
      "Iteration: 3210 lambda_k: 1 Loss: 0.03933632603537398\n",
      "Iteration: 3211 lambda_k: 1 Loss: 0.03930365900032902\n",
      "Iteration: 3212 lambda_k: 1 Loss: 0.039271000413819084\n",
      "Iteration: 3213 lambda_k: 1 Loss: 0.03923835027609897\n",
      "Iteration: 3214 lambda_k: 1 Loss: 0.03920570858741499\n",
      "Iteration: 3215 lambda_k: 1 Loss: 0.039173075348005085\n",
      "Iteration: 3216 lambda_k: 1 Loss: 0.039140450558098665\n",
      "Iteration: 3217 lambda_k: 1 Loss: 0.03910783421791674\n",
      "Iteration: 3218 lambda_k: 1 Loss: 0.039075226327671866\n",
      "Iteration: 3219 lambda_k: 1 Loss: 0.039042626887568314\n",
      "Iteration: 3220 lambda_k: 1 Loss: 0.03901003589780187\n",
      "Iteration: 3221 lambda_k: 1 Loss: 0.038977453358559934\n",
      "Iteration: 3222 lambda_k: 1 Loss: 0.03894487927002168\n",
      "Iteration: 3223 lambda_k: 1 Loss: 0.03891231363235779\n",
      "Iteration: 3224 lambda_k: 1 Loss: 0.038879756445730516\n",
      "Iteration: 3225 lambda_k: 1 Loss: 0.038847207710294006\n",
      "Iteration: 3226 lambda_k: 1 Loss: 0.03881466742619389\n",
      "Iteration: 3227 lambda_k: 1 Loss: 0.038782135593567475\n",
      "Iteration: 3228 lambda_k: 1 Loss: 0.03874961221254376\n",
      "Iteration: 3229 lambda_k: 1 Loss: 0.03871709728324333\n",
      "Iteration: 3230 lambda_k: 1 Loss: 0.03868459080577845\n",
      "Iteration: 3231 lambda_k: 1 Loss: 0.03865209278025303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3232 lambda_k: 1 Loss: 0.03861960320676263\n",
      "Iteration: 3233 lambda_k: 1 Loss: 0.038587122085394485\n",
      "Iteration: 3234 lambda_k: 1 Loss: 0.03855464941622745\n",
      "Iteration: 3235 lambda_k: 1 Loss: 0.03852218519933201\n",
      "Iteration: 3236 lambda_k: 1 Loss: 0.03848972943477023\n",
      "Iteration: 3237 lambda_k: 1 Loss: 0.03845728212259583\n",
      "Iteration: 3238 lambda_k: 1 Loss: 0.038424843262854266\n",
      "Iteration: 3239 lambda_k: 1 Loss: 0.03839241285558238\n",
      "Iteration: 3240 lambda_k: 1 Loss: 0.03835999090080882\n",
      "Iteration: 3241 lambda_k: 1 Loss: 0.03832757739855372\n",
      "Iteration: 3242 lambda_k: 1 Loss: 0.03829517234882884\n",
      "Iteration: 3243 lambda_k: 1 Loss: 0.03826277575163764\n",
      "Iteration: 3244 lambda_k: 1 Loss: 0.03823038760697505\n",
      "Iteration: 3245 lambda_k: 1 Loss: 0.038198007914827525\n",
      "Iteration: 3246 lambda_k: 1 Loss: 0.038165636675173226\n",
      "Iteration: 3247 lambda_k: 1 Loss: 0.0381332738879819\n",
      "Iteration: 3248 lambda_k: 1 Loss: 0.038100919553214735\n",
      "Iteration: 3249 lambda_k: 1 Loss: 0.0380685736708246\n",
      "Iteration: 3250 lambda_k: 1 Loss: 0.03803623624075585\n",
      "Iteration: 3251 lambda_k: 1 Loss: 0.03800390726294449\n",
      "Iteration: 3252 lambda_k: 1 Loss: 0.03797158673731791\n",
      "Iteration: 3253 lambda_k: 1 Loss: 0.03793927466379527\n",
      "Iteration: 3254 lambda_k: 1 Loss: 0.03790697104228704\n",
      "Iteration: 3255 lambda_k: 1 Loss: 0.03787467587269542\n",
      "Iteration: 3256 lambda_k: 1 Loss: 0.03784238915491405\n",
      "Iteration: 3257 lambda_k: 1 Loss: 0.03781011088882809\n",
      "Iteration: 3258 lambda_k: 1 Loss: 0.03777784107431435\n",
      "Iteration: 3259 lambda_k: 1 Loss: 0.03774557971124109\n",
      "Iteration: 3260 lambda_k: 1 Loss: 0.037713326799468164\n",
      "Iteration: 3261 lambda_k: 1 Loss: 0.037681082338846865\n",
      "Iteration: 3262 lambda_k: 1 Loss: 0.0376488463292201\n",
      "Iteration: 3263 lambda_k: 1 Loss: 0.03761661877042225\n",
      "Iteration: 3264 lambda_k: 1 Loss: 0.03758439966227935\n",
      "Iteration: 3265 lambda_k: 1 Loss: 0.037552189004608875\n",
      "Iteration: 3266 lambda_k: 1 Loss: 0.03751998679721979\n",
      "Iteration: 3267 lambda_k: 1 Loss: 0.03748779303991268\n",
      "Iteration: 3268 lambda_k: 1 Loss: 0.037455607732479745\n",
      "Iteration: 3269 lambda_k: 1 Loss: 0.03742343087470457\n",
      "Iteration: 3270 lambda_k: 1 Loss: 0.03739126246636244\n",
      "Iteration: 3271 lambda_k: 1 Loss: 0.03735910250722006\n",
      "Iteration: 3272 lambda_k: 1 Loss: 0.037326950997035785\n",
      "Iteration: 3273 lambda_k: 1 Loss: 0.037294807935559546\n",
      "Iteration: 3274 lambda_k: 1 Loss: 0.03726267332253281\n",
      "Iteration: 3275 lambda_k: 1 Loss: 0.037230547157688594\n",
      "Iteration: 3276 lambda_k: 1 Loss: 0.03719842944075147\n",
      "Iteration: 3277 lambda_k: 1 Loss: 0.03716632017143774\n",
      "Iteration: 3278 lambda_k: 1 Loss: 0.03713421934945518\n",
      "Iteration: 3279 lambda_k: 1 Loss: 0.037102126974503165\n",
      "Iteration: 3280 lambda_k: 1 Loss: 0.037070043046272844\n",
      "Iteration: 3281 lambda_k: 1 Loss: 0.03703796756444676\n",
      "Iteration: 3282 lambda_k: 1 Loss: 0.03700590052869922\n",
      "Iteration: 3283 lambda_k: 1 Loss: 0.03697384193869613\n",
      "Iteration: 3284 lambda_k: 1 Loss: 0.036941791794095055\n",
      "Iteration: 3285 lambda_k: 1 Loss: 0.03690975009454523\n",
      "Iteration: 3286 lambda_k: 1 Loss: 0.03687771683968756\n",
      "Iteration: 3287 lambda_k: 1 Loss: 0.03684569202915456\n",
      "Iteration: 3288 lambda_k: 1 Loss: 0.036813675662570565\n",
      "Iteration: 3289 lambda_k: 1 Loss: 0.0367816677395515\n",
      "Iteration: 3290 lambda_k: 1 Loss: 0.03674966825970508\n",
      "Iteration: 3291 lambda_k: 1 Loss: 0.03671767722263082\n",
      "Iteration: 3292 lambda_k: 1 Loss: 0.0366856946279198\n",
      "Iteration: 3293 lambda_k: 1 Loss: 0.03665372047515492\n",
      "Iteration: 3294 lambda_k: 1 Loss: 0.03662175476391091\n",
      "Iteration: 3295 lambda_k: 1 Loss: 0.0365897974937542\n",
      "Iteration: 3296 lambda_k: 1 Loss: 0.036557848664243144\n",
      "Iteration: 3297 lambda_k: 1 Loss: 0.03652590827492776\n",
      "Iteration: 3298 lambda_k: 1 Loss: 0.03649397632534999\n",
      "Iteration: 3299 lambda_k: 1 Loss: 0.0364620528150436\n",
      "Iteration: 3300 lambda_k: 1 Loss: 0.03643013774353428\n",
      "Iteration: 3301 lambda_k: 1 Loss: 0.03639823111033941\n",
      "Iteration: 3302 lambda_k: 1 Loss: 0.036366332914968526\n",
      "Iteration: 3303 lambda_k: 1 Loss: 0.03633444315692288\n",
      "Iteration: 3304 lambda_k: 1 Loss: 0.036302561835695725\n",
      "Iteration: 3305 lambda_k: 1 Loss: 0.03627068895077227\n",
      "Iteration: 3306 lambda_k: 1 Loss: 0.03623882450162966\n",
      "Iteration: 3307 lambda_k: 1 Loss: 0.03620696848773705\n",
      "Iteration: 3308 lambda_k: 1 Loss: 0.0361751209085556\n",
      "Iteration: 3309 lambda_k: 1 Loss: 0.03614328176353842\n",
      "Iteration: 3310 lambda_k: 1 Loss: 0.03611145105213085\n",
      "Iteration: 3311 lambda_k: 1 Loss: 0.036079628773770064\n",
      "Iteration: 3312 lambda_k: 1 Loss: 0.03604781492788541\n",
      "Iteration: 3313 lambda_k: 1 Loss: 0.03601600951389843\n",
      "Iteration: 3314 lambda_k: 1 Loss: 0.03598421253122266\n",
      "Iteration: 3315 lambda_k: 1 Loss: 0.03595242397926379\n",
      "Iteration: 3316 lambda_k: 1 Loss: 0.03592064385741975\n",
      "Iteration: 3317 lambda_k: 1 Loss: 0.03588887216508056\n",
      "Iteration: 3318 lambda_k: 1 Loss: 0.035857108901628555\n",
      "Iteration: 3319 lambda_k: 1 Loss: 0.03582535406643824\n",
      "Iteration: 3320 lambda_k: 1 Loss: 0.03579360765887637\n",
      "Iteration: 3321 lambda_k: 1 Loss: 0.035761869678301965\n",
      "Iteration: 3322 lambda_k: 1 Loss: 0.035730140124066403\n",
      "Iteration: 3323 lambda_k: 1 Loss: 0.0356984189955133\n",
      "Iteration: 3324 lambda_k: 1 Loss: 0.03566670629197864\n",
      "Iteration: 3325 lambda_k: 1 Loss: 0.03563500201279074\n",
      "Iteration: 3326 lambda_k: 1 Loss: 0.035603306157270385\n",
      "Iteration: 3327 lambda_k: 1 Loss: 0.03557161872473069\n",
      "Iteration: 3328 lambda_k: 1 Loss: 0.03553993971447731\n",
      "Iteration: 3329 lambda_k: 1 Loss: 0.035508269125808185\n",
      "Iteration: 3330 lambda_k: 1 Loss: 0.035476606958014\n",
      "Iteration: 3331 lambda_k: 1 Loss: 0.03544495321037766\n",
      "Iteration: 3332 lambda_k: 1 Loss: 0.03541330788217475\n",
      "Iteration: 3333 lambda_k: 1 Loss: 0.03538167097267348\n",
      "Iteration: 3334 lambda_k: 1 Loss: 0.0353500424811345\n",
      "Iteration: 3335 lambda_k: 1 Loss: 0.035318422406811184\n",
      "Iteration: 3336 lambda_k: 1 Loss: 0.035286810748949446\n",
      "Iteration: 3337 lambda_k: 1 Loss: 0.03525520750678783\n",
      "Iteration: 3338 lambda_k: 1 Loss: 0.0352236126795577\n",
      "Iteration: 3339 lambda_k: 1 Loss: 0.035192026266483094\n",
      "Iteration: 3340 lambda_k: 1 Loss: 0.035160448266780636\n",
      "Iteration: 3341 lambda_k: 1 Loss: 0.035128878679659886\n",
      "Iteration: 3342 lambda_k: 1 Loss: 0.03509731750432312\n",
      "Iteration: 3343 lambda_k: 1 Loss: 0.03506576473996547\n",
      "Iteration: 3344 lambda_k: 1 Loss: 0.03503422038577481\n",
      "Iteration: 3345 lambda_k: 1 Loss: 0.035002684440932005\n",
      "Iteration: 3346 lambda_k: 1 Loss: 0.034971156904610695\n",
      "Iteration: 3347 lambda_k: 1 Loss: 0.03493963777597746\n",
      "Iteration: 3348 lambda_k: 1 Loss: 0.03490812705419191\n",
      "Iteration: 3349 lambda_k: 1 Loss: 0.0348766247384066\n",
      "Iteration: 3350 lambda_k: 1 Loss: 0.034845130827767\n",
      "Iteration: 3351 lambda_k: 1 Loss: 0.03481364532141172\n",
      "Iteration: 3352 lambda_k: 1 Loss: 0.034782168218472305\n",
      "Iteration: 3353 lambda_k: 1 Loss: 0.03475069951807351\n",
      "Iteration: 3354 lambda_k: 1 Loss: 0.03471923921933316\n",
      "Iteration: 3355 lambda_k: 1 Loss: 0.0346877873213622\n",
      "Iteration: 3356 lambda_k: 1 Loss: 0.03465634382326464\n",
      "Iteration: 3357 lambda_k: 1 Loss: 0.03462490872413787\n",
      "Iteration: 3358 lambda_k: 1 Loss: 0.03459348202307235\n",
      "Iteration: 3359 lambda_k: 1 Loss: 0.03456206371915185\n",
      "Iteration: 3360 lambda_k: 1 Loss: 0.0345306538114534\n",
      "Iteration: 3361 lambda_k: 1 Loss: 0.034499252299047406\n",
      "Iteration: 3362 lambda_k: 1 Loss: 0.03446785918099748\n",
      "Iteration: 3363 lambda_k: 1 Loss: 0.03443647445636069\n",
      "Iteration: 3364 lambda_k: 1 Loss: 0.03440509812418744\n",
      "Iteration: 3365 lambda_k: 1 Loss: 0.034373730183521564\n",
      "Iteration: 3366 lambda_k: 1 Loss: 0.03434237063340037\n",
      "Iteration: 3367 lambda_k: 1 Loss: 0.03431101947285454\n",
      "Iteration: 3368 lambda_k: 1 Loss: 0.03427967670090836\n",
      "Iteration: 3369 lambda_k: 1 Loss: 0.034248342316579715\n",
      "Iteration: 3370 lambda_k: 1 Loss: 0.03421701631887981\n",
      "Iteration: 3371 lambda_k: 1 Loss: 0.034185698706813594\n",
      "Iteration: 3372 lambda_k: 1 Loss: 0.03415438947937967\n",
      "Iteration: 3373 lambda_k: 1 Loss: 0.03412308863557019\n",
      "Iteration: 3374 lambda_k: 1 Loss: 0.03409179617437099\n",
      "Iteration: 3375 lambda_k: 1 Loss: 0.034060512094761616\n",
      "Iteration: 3376 lambda_k: 1 Loss: 0.03402923639571534\n",
      "Iteration: 3377 lambda_k: 1 Loss: 0.033997969076199265\n",
      "Iteration: 3378 lambda_k: 1 Loss: 0.03396671013517417\n",
      "Iteration: 3379 lambda_k: 1 Loss: 0.033935459571594696\n",
      "Iteration: 3380 lambda_k: 1 Loss: 0.03390421738440929\n",
      "Iteration: 3381 lambda_k: 1 Loss: 0.03387298357256033\n",
      "Iteration: 3382 lambda_k: 1 Loss: 0.03384175813498406\n",
      "Iteration: 3383 lambda_k: 1 Loss: 0.03381054107061068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3384 lambda_k: 1 Loss: 0.03377933237836427\n",
      "Iteration: 3385 lambda_k: 1 Loss: 0.033748132057163024\n",
      "Iteration: 3386 lambda_k: 1 Loss: 0.033716940105919024\n",
      "Iteration: 3387 lambda_k: 1 Loss: 0.03368575652353847\n",
      "Iteration: 3388 lambda_k: 1 Loss: 0.03365458130892162\n",
      "Iteration: 3389 lambda_k: 1 Loss: 0.03362341446096285\n",
      "Iteration: 3390 lambda_k: 1 Loss: 0.03359225597855061\n",
      "Iteration: 3391 lambda_k: 1 Loss: 0.03356110586056754\n",
      "Iteration: 3392 lambda_k: 1 Loss: 0.0335299641058904\n",
      "Iteration: 3393 lambda_k: 1 Loss: 0.033498830713390285\n",
      "Iteration: 3394 lambda_k: 1 Loss: 0.03346770568193252\n",
      "Iteration: 3395 lambda_k: 1 Loss: 0.033436589010376654\n",
      "Iteration: 3396 lambda_k: 1 Loss: 0.03340548069757647\n",
      "Iteration: 3397 lambda_k: 1 Loss: 0.0333743807423802\n",
      "Iteration: 3398 lambda_k: 1 Loss: 0.03334328914363034\n",
      "Iteration: 3399 lambda_k: 1 Loss: 0.03331220590016387\n",
      "Iteration: 3400 lambda_k: 1 Loss: 0.0332811310108121\n",
      "Iteration: 3401 lambda_k: 1 Loss: 0.03325006447440083\n",
      "Iteration: 3402 lambda_k: 1 Loss: 0.033219006289750275\n",
      "Iteration: 3403 lambda_k: 1 Loss: 0.03318795645567518\n",
      "Iteration: 3404 lambda_k: 1 Loss: 0.03315691497098488\n",
      "Iteration: 3405 lambda_k: 1 Loss: 0.033125881834483194\n",
      "Iteration: 3406 lambda_k: 1 Loss: 0.033094857044968465\n",
      "Iteration: 3407 lambda_k: 1 Loss: 0.03306384060123378\n",
      "Iteration: 3408 lambda_k: 1 Loss: 0.03303283250206686\n",
      "Iteration: 3409 lambda_k: 1 Loss: 0.03300183274624993\n",
      "Iteration: 3410 lambda_k: 1 Loss: 0.032970841332560154\n",
      "Iteration: 3411 lambda_k: 1 Loss: 0.032939858259769215\n",
      "Iteration: 3412 lambda_k: 1 Loss: 0.03290888352664363\n",
      "Iteration: 3413 lambda_k: 1 Loss: 0.0328779171319447\n",
      "Iteration: 3414 lambda_k: 1 Loss: 0.03284695907442855\n",
      "Iteration: 3415 lambda_k: 1 Loss: 0.03281600935284605\n",
      "Iteration: 3416 lambda_k: 1 Loss: 0.03278506796594301\n",
      "Iteration: 3417 lambda_k: 1 Loss: 0.03275413491246014\n",
      "Iteration: 3418 lambda_k: 1 Loss: 0.032723210191133056\n",
      "Iteration: 3419 lambda_k: 1 Loss: 0.03269229380069236\n",
      "Iteration: 3420 lambda_k: 1 Loss: 0.0326613857398635\n",
      "Iteration: 3421 lambda_k: 1 Loss: 0.03263048600736704\n",
      "Iteration: 3422 lambda_k: 1 Loss: 0.03259959460191848\n",
      "Iteration: 3423 lambda_k: 1 Loss: 0.03256871152222849\n",
      "Iteration: 3424 lambda_k: 1 Loss: 0.032537836767002816\n",
      "Iteration: 3425 lambda_k: 1 Loss: 0.03250697033494223\n",
      "Iteration: 3426 lambda_k: 1 Loss: 0.03247611222474262\n",
      "Iteration: 3427 lambda_k: 1 Loss: 0.03244526243509512\n",
      "Iteration: 3428 lambda_k: 1 Loss: 0.032414420964686136\n",
      "Iteration: 3429 lambda_k: 1 Loss: 0.03238358781219704\n",
      "Iteration: 3430 lambda_k: 1 Loss: 0.03235276297630469\n",
      "Iteration: 3431 lambda_k: 1 Loss: 0.03232194645568117\n",
      "Iteration: 3432 lambda_k: 1 Loss: 0.03229113824899375\n",
      "Iteration: 3433 lambda_k: 1 Loss: 0.03226033835490515\n",
      "Iteration: 3434 lambda_k: 1 Loss: 0.03222954677207336\n",
      "Iteration: 3435 lambda_k: 1 Loss: 0.03219876349915178\n",
      "Iteration: 3436 lambda_k: 1 Loss: 0.032167988534789256\n",
      "Iteration: 3437 lambda_k: 1 Loss: 0.03213722187762994\n",
      "Iteration: 3438 lambda_k: 1 Loss: 0.03210646352631365\n",
      "Iteration: 3439 lambda_k: 1 Loss: 0.03207571347947547\n",
      "Iteration: 3440 lambda_k: 1 Loss: 0.03204497173574617\n",
      "Iteration: 3441 lambda_k: 1 Loss: 0.03201423829375197\n",
      "Iteration: 3442 lambda_k: 1 Loss: 0.03198351315211465\n",
      "Iteration: 3443 lambda_k: 1 Loss: 0.03195279630945163\n",
      "Iteration: 3444 lambda_k: 1 Loss: 0.03192208776437586\n",
      "Iteration: 3445 lambda_k: 1 Loss: 0.03189138751549605\n",
      "Iteration: 3446 lambda_k: 1 Loss: 0.03186069556141649\n",
      "Iteration: 3447 lambda_k: 1 Loss: 0.031830011900737226\n",
      "Iteration: 3448 lambda_k: 1 Loss: 0.031799336532054\n",
      "Iteration: 3449 lambda_k: 1 Loss: 0.031768669453958204\n",
      "Iteration: 3450 lambda_k: 1 Loss: 0.031738010665037134\n",
      "Iteration: 3451 lambda_k: 1 Loss: 0.03170736016387384\n",
      "Iteration: 3452 lambda_k: 1 Loss: 0.031676717949047146\n",
      "Iteration: 3453 lambda_k: 1 Loss: 0.03164608401913182\n",
      "Iteration: 3454 lambda_k: 1 Loss: 0.03161545837269837\n",
      "Iteration: 3455 lambda_k: 1 Loss: 0.03158484100831328\n",
      "Iteration: 3456 lambda_k: 1 Loss: 0.03155423192453904\n",
      "Iteration: 3457 lambda_k: 1 Loss: 0.03152363111993386\n",
      "Iteration: 3458 lambda_k: 1 Loss: 0.03149303859305215\n",
      "Iteration: 3459 lambda_k: 1 Loss: 0.03146245434244416\n",
      "Iteration: 3460 lambda_k: 1 Loss: 0.03143187836665624\n",
      "Iteration: 3461 lambda_k: 1 Loss: 0.031401310664230825\n",
      "Iteration: 3462 lambda_k: 1 Loss: 0.03137075123370623\n",
      "Iteration: 3463 lambda_k: 1 Loss: 0.03134020007361714\n",
      "Iteration: 3464 lambda_k: 1 Loss: 0.03130965718249408\n",
      "Iteration: 3465 lambda_k: 1 Loss: 0.031279122558863906\n",
      "Iteration: 3466 lambda_k: 1 Loss: 0.031248596201249666\n",
      "Iteration: 3467 lambda_k: 1 Loss: 0.031218078108170455\n",
      "Iteration: 3468 lambda_k: 1 Loss: 0.03118756827814167\n",
      "Iteration: 3469 lambda_k: 1 Loss: 0.03115706670967489\n",
      "Iteration: 3470 lambda_k: 1 Loss: 0.031126573401278036\n",
      "Iteration: 3471 lambda_k: 1 Loss: 0.03109608835145523\n",
      "Iteration: 3472 lambda_k: 1 Loss: 0.031065611558706963\n",
      "Iteration: 3473 lambda_k: 1 Loss: 0.031035143021530067\n",
      "Iteration: 3474 lambda_k: 1 Loss: 0.031004682738417718\n",
      "Iteration: 3475 lambda_k: 1 Loss: 0.03097423070785939\n",
      "Iteration: 3476 lambda_k: 1 Loss: 0.030943786928341124\n",
      "Iteration: 3477 lambda_k: 1 Loss: 0.030913351398345265\n",
      "Iteration: 3478 lambda_k: 1 Loss: 0.030882924116350664\n",
      "Iteration: 3479 lambda_k: 1 Loss: 0.030852505080832596\n",
      "Iteration: 3480 lambda_k: 1 Loss: 0.030822094290262805\n",
      "Iteration: 3481 lambda_k: 1 Loss: 0.03079169174310971\n",
      "Iteration: 3482 lambda_k: 1 Loss: 0.03076129743783816\n",
      "Iteration: 3483 lambda_k: 1 Loss: 0.030730911372909595\n",
      "Iteration: 3484 lambda_k: 1 Loss: 0.03070053354678209\n",
      "Iteration: 3485 lambda_k: 1 Loss: 0.030670163957910175\n",
      "Iteration: 3486 lambda_k: 1 Loss: 0.03063980260474517\n",
      "Iteration: 3487 lambda_k: 1 Loss: 0.030609449485734984\n",
      "Iteration: 3488 lambda_k: 1 Loss: 0.030579104599324163\n",
      "Iteration: 3489 lambda_k: 1 Loss: 0.030548767943954055\n",
      "Iteration: 3490 lambda_k: 1 Loss: 0.030518439518062622\n",
      "Iteration: 3491 lambda_k: 1 Loss: 0.030488119320084748\n",
      "Iteration: 3492 lambda_k: 1 Loss: 0.0304578073484519\n",
      "Iteration: 3493 lambda_k: 1 Loss: 0.030427503601592427\n",
      "Iteration: 3494 lambda_k: 1 Loss: 0.030397208077931396\n",
      "Iteration: 3495 lambda_k: 1 Loss: 0.030366920775890795\n",
      "Iteration: 3496 lambda_k: 1 Loss: 0.030336641693889475\n",
      "Iteration: 3497 lambda_k: 1 Loss: 0.030306370830343096\n",
      "Iteration: 3498 lambda_k: 1 Loss: 0.030276108183664243\n",
      "Iteration: 3499 lambda_k: 1 Loss: 0.030245853752262426\n",
      "Iteration: 3500 lambda_k: 1 Loss: 0.030215607534544067\n",
      "Iteration: 3501 lambda_k: 1 Loss: 0.03018536952891258\n",
      "Iteration: 3502 lambda_k: 1 Loss: 0.030155139733768356\n",
      "Iteration: 3503 lambda_k: 1 Loss: 0.030124918147508752\n",
      "Iteration: 3504 lambda_k: 1 Loss: 0.030094704768528272\n",
      "Iteration: 3505 lambda_k: 1 Loss: 0.03006449959521829\n",
      "Iteration: 3506 lambda_k: 1 Loss: 0.03003430262596731\n",
      "Iteration: 3507 lambda_k: 1 Loss: 0.030004113859160957\n",
      "Iteration: 3508 lambda_k: 1 Loss: 0.029973933293181963\n",
      "Iteration: 3509 lambda_k: 1 Loss: 0.02994376092641019\n",
      "Iteration: 3510 lambda_k: 1 Loss: 0.029913596757222567\n",
      "Iteration: 3511 lambda_k: 1 Loss: 0.02988344078399328\n",
      "Iteration: 3512 lambda_k: 1 Loss: 0.02985329300509367\n",
      "Iteration: 3513 lambda_k: 1 Loss: 0.02982315341889218\n",
      "Iteration: 3514 lambda_k: 1 Loss: 0.029793022023754705\n",
      "Iteration: 3515 lambda_k: 1 Loss: 0.029762898818044208\n",
      "Iteration: 3516 lambda_k: 1 Loss: 0.02973278380012099\n",
      "Iteration: 3517 lambda_k: 1 Loss: 0.0297026769683427\n",
      "Iteration: 3518 lambda_k: 1 Loss: 0.02967257832106416\n",
      "Iteration: 3519 lambda_k: 1 Loss: 0.0296424878566375\n",
      "Iteration: 3520 lambda_k: 1 Loss: 0.029612405573412444\n",
      "Iteration: 3521 lambda_k: 1 Loss: 0.029582331469735816\n",
      "Iteration: 3522 lambda_k: 1 Loss: 0.029552265543951885\n",
      "Iteration: 3523 lambda_k: 1 Loss: 0.029522207794402466\n",
      "Iteration: 3524 lambda_k: 1 Loss: 0.02949215821942661\n",
      "Iteration: 3525 lambda_k: 1 Loss: 0.02946211681736092\n",
      "Iteration: 3526 lambda_k: 1 Loss: 0.029432083586539502\n",
      "Iteration: 3527 lambda_k: 1 Loss: 0.029402058525293765\n",
      "Iteration: 3528 lambda_k: 1 Loss: 0.029372041631952856\n",
      "Iteration: 3529 lambda_k: 1 Loss: 0.029342032904843166\n",
      "Iteration: 3530 lambda_k: 1 Loss: 0.029312032342288925\n",
      "Iteration: 3531 lambda_k: 1 Loss: 0.02928203994261168\n",
      "Iteration: 3532 lambda_k: 1 Loss: 0.02925205570413063\n",
      "Iteration: 3533 lambda_k: 1 Loss: 0.029222079625162652\n",
      "Iteration: 3534 lambda_k: 1 Loss: 0.029192111704022063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3535 lambda_k: 1 Loss: 0.029162151939020994\n",
      "Iteration: 3536 lambda_k: 1 Loss: 0.029132200328469048\n",
      "Iteration: 3537 lambda_k: 1 Loss: 0.02910225687067366\n",
      "Iteration: 3538 lambda_k: 1 Loss: 0.029072321563939817\n",
      "Iteration: 3539 lambda_k: 1 Loss: 0.02904239440657029\n",
      "Iteration: 3540 lambda_k: 1 Loss: 0.029012475396865547\n",
      "Iteration: 3541 lambda_k: 1 Loss: 0.028982564533123766\n",
      "Iteration: 3542 lambda_k: 1 Loss: 0.02895266181364094\n",
      "Iteration: 3543 lambda_k: 1 Loss: 0.028922767236710813\n",
      "Iteration: 3544 lambda_k: 1 Loss: 0.028892880800624843\n",
      "Iteration: 3545 lambda_k: 1 Loss: 0.028863002503672366\n",
      "Iteration: 3546 lambda_k: 1 Loss: 0.028833132344140585\n",
      "Iteration: 3547 lambda_k: 1 Loss: 0.028803270320314482\n",
      "Iteration: 3548 lambda_k: 1 Loss: 0.028773416430476872\n",
      "Iteration: 3549 lambda_k: 1 Loss: 0.028743570672908517\n",
      "Iteration: 3550 lambda_k: 1 Loss: 0.028713733045888073\n",
      "Iteration: 3551 lambda_k: 1 Loss: 0.028683903547692083\n",
      "Iteration: 3552 lambda_k: 1 Loss: 0.028654082176595007\n",
      "Iteration: 3553 lambda_k: 1 Loss: 0.02862426893086922\n",
      "Iteration: 3554 lambda_k: 1 Loss: 0.02859446380878511\n",
      "Iteration: 3555 lambda_k: 1 Loss: 0.028564666808611115\n",
      "Iteration: 3556 lambda_k: 1 Loss: 0.028534877928613516\n",
      "Iteration: 3557 lambda_k: 1 Loss: 0.028505097167056705\n",
      "Iteration: 3558 lambda_k: 1 Loss: 0.028475324522203124\n",
      "Iteration: 3559 lambda_k: 1 Loss: 0.02844555999231315\n",
      "Iteration: 3560 lambda_k: 1 Loss: 0.02841580357564528\n",
      "Iteration: 3561 lambda_k: 1 Loss: 0.028386055270456127\n",
      "Iteration: 3562 lambda_k: 1 Loss: 0.028356315075000325\n",
      "Iteration: 3563 lambda_k: 1 Loss: 0.028326582987530723\n",
      "Iteration: 3564 lambda_k: 1 Loss: 0.028296859006298233\n",
      "Iteration: 3565 lambda_k: 1 Loss: 0.028267143129551885\n",
      "Iteration: 3566 lambda_k: 1 Loss: 0.028237435355538888\n",
      "Iteration: 3567 lambda_k: 1 Loss: 0.028207735682504623\n",
      "Iteration: 3568 lambda_k: 1 Loss: 0.02817804410869269\n",
      "Iteration: 3569 lambda_k: 1 Loss: 0.02814836063234483\n",
      "Iteration: 3570 lambda_k: 1 Loss: 0.028118685251701123\n",
      "Iteration: 3571 lambda_k: 1 Loss: 0.028089017964999703\n",
      "Iteration: 3572 lambda_k: 1 Loss: 0.028059358770477143\n",
      "Iteration: 3573 lambda_k: 1 Loss: 0.028029707666368184\n",
      "Iteration: 3574 lambda_k: 1 Loss: 0.02800006465090588\n",
      "Iteration: 3575 lambda_k: 1 Loss: 0.027970429722321572\n",
      "Iteration: 3576 lambda_k: 1 Loss: 0.027940802878844834\n",
      "Iteration: 3577 lambda_k: 1 Loss: 0.027911184118703754\n",
      "Iteration: 3578 lambda_k: 1 Loss: 0.027881573440124638\n",
      "Iteration: 3579 lambda_k: 1 Loss: 0.027851970841332153\n",
      "Iteration: 3580 lambda_k: 1 Loss: 0.02782237632054937\n",
      "Iteration: 3581 lambda_k: 1 Loss: 0.027792789875997714\n",
      "Iteration: 3582 lambda_k: 1 Loss: 0.02776321150589709\n",
      "Iteration: 3583 lambda_k: 1 Loss: 0.02773364120846567\n",
      "Iteration: 3584 lambda_k: 1 Loss: 0.027704078981920288\n",
      "Iteration: 3585 lambda_k: 1 Loss: 0.027674524824476018\n",
      "Iteration: 3586 lambda_k: 1 Loss: 0.027644978734346456\n",
      "Iteration: 3587 lambda_k: 1 Loss: 0.027615440709743692\n",
      "Iteration: 3588 lambda_k: 1 Loss: 0.027585910748878315\n",
      "Iteration: 3589 lambda_k: 1 Loss: 0.027556388849959357\n",
      "Iteration: 3590 lambda_k: 1 Loss: 0.02752687501119451\n",
      "Iteration: 3591 lambda_k: 1 Loss: 0.027497369230789822\n",
      "Iteration: 3592 lambda_k: 1 Loss: 0.027467871506949966\n",
      "Iteration: 3593 lambda_k: 1 Loss: 0.02743838183787818\n",
      "Iteration: 3594 lambda_k: 1 Loss: 0.02740890022177638\n",
      "Iteration: 3595 lambda_k: 1 Loss: 0.027379426656844882\n",
      "Iteration: 3596 lambda_k: 1 Loss: 0.027349961141282735\n",
      "Iteration: 3597 lambda_k: 1 Loss: 0.027320503673287515\n",
      "Iteration: 3598 lambda_k: 1 Loss: 0.027291054251055498\n",
      "Iteration: 3599 lambda_k: 1 Loss: 0.0272616128727816\n",
      "Iteration: 3600 lambda_k: 1 Loss: 0.02723217953665932\n",
      "Iteration: 3601 lambda_k: 1 Loss: 0.0272027542408809\n",
      "Iteration: 3602 lambda_k: 1 Loss: 0.027173336983637338\n",
      "Iteration: 3603 lambda_k: 1 Loss: 0.027143927763118168\n",
      "Iteration: 3604 lambda_k: 1 Loss: 0.02711452657751172\n",
      "Iteration: 3605 lambda_k: 1 Loss: 0.027085133425005015\n",
      "Iteration: 3606 lambda_k: 1 Loss: 0.02705574830378382\n",
      "Iteration: 3607 lambda_k: 1 Loss: 0.027026371212032718\n",
      "Iteration: 3608 lambda_k: 1 Loss: 0.026997002147934934\n",
      "Iteration: 3609 lambda_k: 1 Loss: 0.026967641109672626\n",
      "Iteration: 3610 lambda_k: 1 Loss: 0.026938288095426557\n",
      "Iteration: 3611 lambda_k: 1 Loss: 0.026908943103376486\n",
      "Iteration: 3612 lambda_k: 1 Loss: 0.026879606131700828\n",
      "Iteration: 3613 lambda_k: 1 Loss: 0.02685027717857691\n",
      "Iteration: 3614 lambda_k: 1 Loss: 0.026820956242180907\n",
      "Iteration: 3615 lambda_k: 1 Loss: 0.026791643320687838\n",
      "Iteration: 3616 lambda_k: 1 Loss: 0.026762338412271533\n",
      "Iteration: 3617 lambda_k: 1 Loss: 0.02673304151510474\n",
      "Iteration: 3618 lambda_k: 1 Loss: 0.02670375262735917\n",
      "Iteration: 3619 lambda_k: 1 Loss: 0.02667447174720527\n",
      "Iteration: 3620 lambda_k: 1 Loss: 0.02664519887281259\n",
      "Iteration: 3621 lambda_k: 1 Loss: 0.026615934002349424\n",
      "Iteration: 3622 lambda_k: 1 Loss: 0.02658667713398316\n",
      "Iteration: 3623 lambda_k: 1 Loss: 0.026557428265880065\n",
      "Iteration: 3624 lambda_k: 1 Loss: 0.026528187396205414\n",
      "Iteration: 3625 lambda_k: 1 Loss: 0.026498954523123414\n",
      "Iteration: 3626 lambda_k: 1 Loss: 0.026469729644797366\n",
      "Iteration: 3627 lambda_k: 1 Loss: 0.026440512759389335\n",
      "Iteration: 3628 lambda_k: 1 Loss: 0.026411303865060687\n",
      "Iteration: 3629 lambda_k: 1 Loss: 0.026382102959971638\n",
      "Iteration: 3630 lambda_k: 1 Loss: 0.02635291004228149\n",
      "Iteration: 3631 lambda_k: 1 Loss: 0.026323725110148563\n",
      "Iteration: 3632 lambda_k: 1 Loss: 0.026294548161730238\n",
      "Iteration: 3633 lambda_k: 1 Loss: 0.026265379195183063\n",
      "Iteration: 3634 lambda_k: 1 Loss: 0.02623621820866254\n",
      "Iteration: 3635 lambda_k: 1 Loss: 0.026207065200323332\n",
      "Iteration: 3636 lambda_k: 1 Loss: 0.02617792016831927\n",
      "Iteration: 3637 lambda_k: 1 Loss: 0.026148783110803157\n",
      "Iteration: 3638 lambda_k: 1 Loss: 0.026119654025927072\n",
      "Iteration: 3639 lambda_k: 1 Loss: 0.026090532911842126\n",
      "Iteration: 3640 lambda_k: 1 Loss: 0.026061419766698655\n",
      "Iteration: 3641 lambda_k: 1 Loss: 0.02603231458864612\n",
      "Iteration: 3642 lambda_k: 1 Loss: 0.026003217375833176\n",
      "Iteration: 3643 lambda_k: 1 Loss: 0.025974128126407627\n",
      "Iteration: 3644 lambda_k: 1 Loss: 0.02594504683851651\n",
      "Iteration: 3645 lambda_k: 1 Loss: 0.02591597351030607\n",
      "Iteration: 3646 lambda_k: 1 Loss: 0.025886908139921797\n",
      "Iteration: 3647 lambda_k: 1 Loss: 0.025857850725508346\n",
      "Iteration: 3648 lambda_k: 1 Loss: 0.02582880126520967\n",
      "Iteration: 3649 lambda_k: 1 Loss: 0.025799759757168944\n",
      "Iteration: 3650 lambda_k: 1 Loss: 0.025770726199528686\n",
      "Iteration: 3651 lambda_k: 1 Loss: 0.025741700590430506\n",
      "Iteration: 3652 lambda_k: 1 Loss: 0.02571268292801549\n",
      "Iteration: 3653 lambda_k: 1 Loss: 0.02568367321042395\n",
      "Iteration: 3654 lambda_k: 1 Loss: 0.025654671435795456\n",
      "Iteration: 3655 lambda_k: 1 Loss: 0.025625677602268925\n",
      "Iteration: 3656 lambda_k: 1 Loss: 0.02559669170798261\n",
      "Iteration: 3657 lambda_k: 1 Loss: 0.025567713751074198\n",
      "Iteration: 3658 lambda_k: 1 Loss: 0.025538743729680524\n",
      "Iteration: 3659 lambda_k: 1 Loss: 0.025509781641937908\n",
      "Iteration: 3660 lambda_k: 1 Loss: 0.025480827485982075\n",
      "Iteration: 3661 lambda_k: 1 Loss: 0.025451881259948038\n",
      "Iteration: 3662 lambda_k: 1 Loss: 0.025422942961970254\n",
      "Iteration: 3663 lambda_k: 1 Loss: 0.025394012590182525\n",
      "Iteration: 3664 lambda_k: 1 Loss: 0.0253650901427181\n",
      "Iteration: 3665 lambda_k: 1 Loss: 0.02533617561770969\n",
      "Iteration: 3666 lambda_k: 1 Loss: 0.025307269013289342\n",
      "Iteration: 3667 lambda_k: 1 Loss: 0.025278370327588666\n",
      "Iteration: 3668 lambda_k: 1 Loss: 0.025249479558738568\n",
      "Iteration: 3669 lambda_k: 1 Loss: 0.02522059670486959\n",
      "Iteration: 3670 lambda_k: 1 Loss: 0.02519172176411162\n",
      "Iteration: 3671 lambda_k: 1 Loss: 0.025162854734594092\n",
      "Iteration: 3672 lambda_k: 1 Loss: 0.02513399561444592\n",
      "Iteration: 3673 lambda_k: 1 Loss: 0.02510514440179545\n",
      "Iteration: 3674 lambda_k: 1 Loss: 0.025076301094770628\n",
      "Iteration: 3675 lambda_k: 1 Loss: 0.025047465691498804\n",
      "Iteration: 3676 lambda_k: 1 Loss: 0.025018638190107027\n",
      "Iteration: 3677 lambda_k: 1 Loss: 0.024989818588721742\n",
      "Iteration: 3678 lambda_k: 1 Loss: 0.024961006885469057\n",
      "Iteration: 3679 lambda_k: 1 Loss: 0.024932203078474505\n",
      "Iteration: 3680 lambda_k: 1 Loss: 0.02490340716586326\n",
      "Iteration: 3681 lambda_k: 1 Loss: 0.024874619145760137\n",
      "Iteration: 3682 lambda_k: 1 Loss: 0.02484583901628949\n",
      "Iteration: 3683 lambda_k: 1 Loss: 0.02481706677557521\n",
      "Iteration: 3684 lambda_k: 1 Loss: 0.024788302421740863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3685 lambda_k: 1 Loss: 0.024759545952909592\n",
      "Iteration: 3686 lambda_k: 1 Loss: 0.024730797367204196\n",
      "Iteration: 3687 lambda_k: 1 Loss: 0.024702056662747073\n",
      "Iteration: 3688 lambda_k: 1 Loss: 0.024673323837660303\n",
      "Iteration: 3689 lambda_k: 1 Loss: 0.02464459889006563\n",
      "Iteration: 3690 lambda_k: 1 Loss: 0.024615881818084367\n",
      "Iteration: 3691 lambda_k: 1 Loss: 0.02458717261983759\n",
      "Iteration: 3692 lambda_k: 1 Loss: 0.024558471293446096\n",
      "Iteration: 3693 lambda_k: 1 Loss: 0.0245297778370302\n",
      "Iteration: 3694 lambda_k: 1 Loss: 0.024501092248710107\n",
      "Iteration: 3695 lambda_k: 1 Loss: 0.024472414526605663\n",
      "Iteration: 3696 lambda_k: 1 Loss: 0.024443744668836285\n",
      "Iteration: 3697 lambda_k: 1 Loss: 0.024415082673521382\n",
      "Iteration: 3698 lambda_k: 1 Loss: 0.02438642853877989\n",
      "Iteration: 3699 lambda_k: 1 Loss: 0.024357782262730623\n",
      "Iteration: 3700 lambda_k: 1 Loss: 0.02432914384349203\n",
      "Iteration: 3701 lambda_k: 1 Loss: 0.024300513279182336\n",
      "Iteration: 3702 lambda_k: 1 Loss: 0.024271890567919632\n",
      "Iteration: 3703 lambda_k: 1 Loss: 0.02424327570782178\n",
      "Iteration: 3704 lambda_k: 1 Loss: 0.02421466869700627\n",
      "Iteration: 3705 lambda_k: 1 Loss: 0.024186069533590593\n",
      "Iteration: 3706 lambda_k: 1 Loss: 0.02415747821569194\n",
      "Iteration: 3707 lambda_k: 1 Loss: 0.02412889474142729\n",
      "Iteration: 3708 lambda_k: 1 Loss: 0.024100319108913484\n",
      "Iteration: 3709 lambda_k: 1 Loss: 0.02407175131626724\n",
      "Iteration: 3710 lambda_k: 1 Loss: 0.024043191361605095\n",
      "Iteration: 3711 lambda_k: 1 Loss: 0.02401463924304338\n",
      "Iteration: 3712 lambda_k: 1 Loss: 0.023986094958698287\n",
      "Iteration: 3713 lambda_k: 1 Loss: 0.023957558506685926\n",
      "Iteration: 3714 lambda_k: 1 Loss: 0.023929029885122285\n",
      "Iteration: 3715 lambda_k: 1 Loss: 0.023900509092123164\n",
      "Iteration: 3716 lambda_k: 1 Loss: 0.023871996125804328\n",
      "Iteration: 3717 lambda_k: 1 Loss: 0.023843490984281433\n",
      "Iteration: 3718 lambda_k: 1 Loss: 0.023814993665670005\n",
      "Iteration: 3719 lambda_k: 1 Loss: 0.023786504168085452\n",
      "Iteration: 3720 lambda_k: 1 Loss: 0.023758022489643215\n",
      "Iteration: 3721 lambda_k: 1 Loss: 0.02372954862845858\n",
      "Iteration: 3722 lambda_k: 1 Loss: 0.02370108258264684\n",
      "Iteration: 3723 lambda_k: 1 Loss: 0.023672624350323114\n",
      "Iteration: 3724 lambda_k: 1 Loss: 0.023644173929602653\n",
      "Iteration: 3725 lambda_k: 1 Loss: 0.02361573131860058\n",
      "Iteration: 3726 lambda_k: 1 Loss: 0.02358729651543202\n",
      "Iteration: 3727 lambda_k: 1 Loss: 0.023558869518212032\n",
      "Iteration: 3728 lambda_k: 1 Loss: 0.023530450325055716\n",
      "Iteration: 3729 lambda_k: 1 Loss: 0.023502038934078123\n",
      "Iteration: 3730 lambda_k: 1 Loss: 0.023473635343394396\n",
      "Iteration: 3731 lambda_k: 1 Loss: 0.02344523955111956\n",
      "Iteration: 3732 lambda_k: 1 Loss: 0.023416851555368805\n",
      "Iteration: 3733 lambda_k: 1 Loss: 0.02338847135425724\n",
      "Iteration: 3734 lambda_k: 1 Loss: 0.02336009894590012\n",
      "Iteration: 3735 lambda_k: 1 Loss: 0.023331734328412627\n",
      "Iteration: 3736 lambda_k: 1 Loss: 0.02330337749991005\n",
      "Iteration: 3737 lambda_k: 1 Loss: 0.0232750284585078\n",
      "Iteration: 3738 lambda_k: 1 Loss: 0.02324668720232131\n",
      "Iteration: 3739 lambda_k: 1 Loss: 0.023218353729466054\n",
      "Iteration: 3740 lambda_k: 1 Loss: 0.023190028038057652\n",
      "Iteration: 3741 lambda_k: 1 Loss: 0.023161710126211762\n",
      "Iteration: 3742 lambda_k: 1 Loss: 0.0231333999920442\n",
      "Iteration: 3743 lambda_k: 1 Loss: 0.023105097633670874\n",
      "Iteration: 3744 lambda_k: 1 Loss: 0.02307680304920777\n",
      "Iteration: 3745 lambda_k: 1 Loss: 0.023048516236771094\n",
      "Iteration: 3746 lambda_k: 1 Loss: 0.02302023719447711\n",
      "Iteration: 3747 lambda_k: 1 Loss: 0.02299196592044225\n",
      "Iteration: 3748 lambda_k: 1 Loss: 0.0229637024127831\n",
      "Iteration: 3749 lambda_k: 1 Loss: 0.022935446669616322\n",
      "Iteration: 3750 lambda_k: 1 Loss: 0.022907198689058866\n",
      "Iteration: 3751 lambda_k: 1 Loss: 0.022878958469227845\n",
      "Iteration: 3752 lambda_k: 1 Loss: 0.02285072600824045\n",
      "Iteration: 3753 lambda_k: 1 Loss: 0.022822501304214107\n",
      "Iteration: 3754 lambda_k: 1 Loss: 0.022794284355266505\n",
      "Iteration: 3755 lambda_k: 1 Loss: 0.022766075159515415\n",
      "Iteration: 3756 lambda_k: 1 Loss: 0.022737873715078932\n",
      "Iteration: 3757 lambda_k: 1 Loss: 0.02270968002007532\n",
      "Iteration: 3758 lambda_k: 1 Loss: 0.022681494072623102\n",
      "Iteration: 3759 lambda_k: 1 Loss: 0.022653315870840962\n",
      "Iteration: 3760 lambda_k: 1 Loss: 0.02262514541284789\n",
      "Iteration: 3761 lambda_k: 1 Loss: 0.022596982696763062\n",
      "Iteration: 3762 lambda_k: 1 Loss: 0.022568827720706\n",
      "Iteration: 3763 lambda_k: 1 Loss: 0.02254068048279641\n",
      "Iteration: 3764 lambda_k: 1 Loss: 0.02251254098115436\n",
      "Iteration: 3765 lambda_k: 1 Loss: 0.02248440921390009\n",
      "Iteration: 3766 lambda_k: 1 Loss: 0.022456285179154194\n",
      "Iteration: 3767 lambda_k: 1 Loss: 0.02242816887503753\n",
      "Iteration: 3768 lambda_k: 1 Loss: 0.02240006029967123\n",
      "Iteration: 3769 lambda_k: 1 Loss: 0.022371959451176864\n",
      "Iteration: 3770 lambda_k: 1 Loss: 0.022343866327676164\n",
      "Iteration: 3771 lambda_k: 1 Loss: 0.022315780927291317\n",
      "Iteration: 3772 lambda_k: 1 Loss: 0.022287703248144733\n",
      "Iteration: 3773 lambda_k: 1 Loss: 0.02225963328835925\n",
      "Iteration: 3774 lambda_k: 1 Loss: 0.02223157104605796\n",
      "Iteration: 3775 lambda_k: 1 Loss: 0.022203516519364353\n",
      "Iteration: 3776 lambda_k: 1 Loss: 0.02217546970640233\n",
      "Iteration: 3777 lambda_k: 1 Loss: 0.022147430605296118\n",
      "Iteration: 3778 lambda_k: 1 Loss: 0.022119399214170315\n",
      "Iteration: 3779 lambda_k: 1 Loss: 0.02209137553114991\n",
      "Iteration: 3780 lambda_k: 1 Loss: 0.022063359554360236\n",
      "Iteration: 3781 lambda_k: 1 Loss: 0.02203535128192706\n",
      "Iteration: 3782 lambda_k: 1 Loss: 0.022007350711976653\n",
      "Iteration: 3783 lambda_k: 1 Loss: 0.021979357842635515\n",
      "Iteration: 3784 lambda_k: 1 Loss: 0.02195137267203076\n",
      "Iteration: 3785 lambda_k: 1 Loss: 0.0219233951982898\n",
      "Iteration: 3786 lambda_k: 1 Loss: 0.02189542541954045\n",
      "Iteration: 3787 lambda_k: 1 Loss: 0.021867463333911152\n",
      "Iteration: 3788 lambda_k: 1 Loss: 0.021839508939530565\n",
      "Iteration: 3789 lambda_k: 1 Loss: 0.021811562234527966\n",
      "Iteration: 3790 lambda_k: 1 Loss: 0.02178362321703305\n",
      "Iteration: 3791 lambda_k: 1 Loss: 0.021755691885175968\n",
      "Iteration: 3792 lambda_k: 1 Loss: 0.021727768237087424\n",
      "Iteration: 3793 lambda_k: 1 Loss: 0.021699852270898528\n",
      "Iteration: 3794 lambda_k: 1 Loss: 0.02167194398474092\n",
      "Iteration: 3795 lambda_k: 1 Loss: 0.02164404337674668\n",
      "Iteration: 3796 lambda_k: 1 Loss: 0.021616150445048467\n",
      "Iteration: 3797 lambda_k: 1 Loss: 0.021588265187779426\n",
      "Iteration: 3798 lambda_k: 1 Loss: 0.021560387603073286\n",
      "Iteration: 3799 lambda_k: 1 Loss: 0.021532517689064265\n",
      "Iteration: 3800 lambda_k: 1 Loss: 0.021504655443887122\n",
      "Iteration: 3801 lambda_k: 1 Loss: 0.021476800865677034\n",
      "Iteration: 3802 lambda_k: 1 Loss: 0.02144895395257\n",
      "Iteration: 3803 lambda_k: 1 Loss: 0.02142111470270232\n",
      "Iteration: 3804 lambda_k: 1 Loss: 0.02139328311421104\n",
      "Iteration: 3805 lambda_k: 1 Loss: 0.02136545918523374\n",
      "Iteration: 3806 lambda_k: 1 Loss: 0.021337642913908442\n",
      "Iteration: 3807 lambda_k: 1 Loss: 0.021309834298374023\n",
      "Iteration: 3808 lambda_k: 1 Loss: 0.021282033336769663\n",
      "Iteration: 3809 lambda_k: 1 Loss: 0.021254240027235406\n",
      "Iteration: 3810 lambda_k: 1 Loss: 0.02122645436791172\n",
      "Iteration: 3811 lambda_k: 1 Loss: 0.021198676356939755\n",
      "Iteration: 3812 lambda_k: 1 Loss: 0.0211709059924613\n",
      "Iteration: 3813 lambda_k: 1 Loss: 0.021143143272618865\n",
      "Iteration: 3814 lambda_k: 1 Loss: 0.021115388195555394\n",
      "Iteration: 3815 lambda_k: 1 Loss: 0.02108764075941471\n",
      "Iteration: 3816 lambda_k: 1 Loss: 0.02105990096234113\n",
      "Iteration: 3817 lambda_k: 1 Loss: 0.02103216880247963\n",
      "Iteration: 3818 lambda_k: 1 Loss: 0.02100444427797598\n",
      "Iteration: 3819 lambda_k: 1 Loss: 0.020976727386976528\n",
      "Iteration: 3820 lambda_k: 1 Loss: 0.02094901812762838\n",
      "Iteration: 3821 lambda_k: 1 Loss: 0.020921316498079247\n",
      "Iteration: 3822 lambda_k: 1 Loss: 0.020893622496477582\n",
      "Iteration: 3823 lambda_k: 1 Loss: 0.02086593612097265\n",
      "Iteration: 3824 lambda_k: 1 Loss: 0.020838257369714216\n",
      "Iteration: 3825 lambda_k: 1 Loss: 0.020810586240852958\n",
      "Iteration: 3826 lambda_k: 1 Loss: 0.020782922732540236\n",
      "Iteration: 3827 lambda_k: 1 Loss: 0.020755266842928104\n",
      "Iteration: 3828 lambda_k: 1 Loss: 0.020727618570169373\n",
      "Iteration: 3829 lambda_k: 1 Loss: 0.020699977912417653\n",
      "Iteration: 3830 lambda_k: 1 Loss: 0.02067234486782725\n",
      "Iteration: 3831 lambda_k: 1 Loss: 0.020644719434553264\n",
      "Iteration: 3832 lambda_k: 1 Loss: 0.02061710161075157\n",
      "Iteration: 3833 lambda_k: 1 Loss: 0.02058949139457883\n",
      "Iteration: 3834 lambda_k: 1 Loss: 0.020561888784192565\n",
      "Iteration: 3835 lambda_k: 1 Loss: 0.020534293777750956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3836 lambda_k: 1 Loss: 0.020506706373413095\n",
      "Iteration: 3837 lambda_k: 1 Loss: 0.020479126569338816\n",
      "Iteration: 3838 lambda_k: 1 Loss: 0.02045155436368891\n",
      "Iteration: 3839 lambda_k: 1 Loss: 0.02042398975462481\n",
      "Iteration: 3840 lambda_k: 1 Loss: 0.020396432740308904\n",
      "Iteration: 3841 lambda_k: 1 Loss: 0.020368883318904386\n",
      "Iteration: 3842 lambda_k: 1 Loss: 0.020341341488575398\n",
      "Iteration: 3843 lambda_k: 1 Loss: 0.02031380724748676\n",
      "Iteration: 3844 lambda_k: 1 Loss: 0.020286280593804235\n",
      "Iteration: 3845 lambda_k: 1 Loss: 0.020258761525694585\n",
      "Iteration: 3846 lambda_k: 1 Loss: 0.02023125004132528\n",
      "Iteration: 3847 lambda_k: 1 Loss: 0.02020374613886479\n",
      "Iteration: 3848 lambda_k: 1 Loss: 0.02017624981648245\n",
      "Iteration: 3849 lambda_k: 1 Loss: 0.020148761072348472\n",
      "Iteration: 3850 lambda_k: 1 Loss: 0.020121279904633978\n",
      "Iteration: 3851 lambda_k: 1 Loss: 0.020093806311511125\n",
      "Iteration: 3852 lambda_k: 1 Loss: 0.02006634029115283\n",
      "Iteration: 3853 lambda_k: 1 Loss: 0.0200388818417331\n",
      "Iteration: 3854 lambda_k: 1 Loss: 0.020011430961426793\n",
      "Iteration: 3855 lambda_k: 1 Loss: 0.019983987648409706\n",
      "Iteration: 3856 lambda_k: 1 Loss: 0.01995655190085877\n",
      "Iteration: 3857 lambda_k: 1 Loss: 0.019929123716951637\n",
      "Iteration: 3858 lambda_k: 1 Loss: 0.019901703094867062\n",
      "Iteration: 3859 lambda_k: 1 Loss: 0.019874290032784826\n",
      "Iteration: 3860 lambda_k: 1 Loss: 0.01984688452888564\n",
      "Iteration: 3861 lambda_k: 1 Loss: 0.019819486581351194\n",
      "Iteration: 3862 lambda_k: 1 Loss: 0.01979209618836425\n",
      "Iteration: 3863 lambda_k: 1 Loss: 0.019764713348108563\n",
      "Iteration: 3864 lambda_k: 1 Loss: 0.01973733805876892\n",
      "Iteration: 3865 lambda_k: 1 Loss: 0.019709970318531132\n",
      "Iteration: 3866 lambda_k: 1 Loss: 0.019682610125582117\n",
      "Iteration: 3867 lambda_k: 1 Loss: 0.019655257478109714\n",
      "Iteration: 3868 lambda_k: 1 Loss: 0.019627912374302888\n",
      "Iteration: 3869 lambda_k: 1 Loss: 0.019600574812351675\n",
      "Iteration: 3870 lambda_k: 1 Loss: 0.019573244790447187\n",
      "Iteration: 3871 lambda_k: 1 Loss: 0.019545922306781614\n",
      "Iteration: 3872 lambda_k: 1 Loss: 0.01951860735954829\n",
      "Iteration: 3873 lambda_k: 1 Loss: 0.019491299946941546\n",
      "Iteration: 3874 lambda_k: 1 Loss: 0.01946400006715689\n",
      "Iteration: 3875 lambda_k: 1 Loss: 0.019436707718390938\n",
      "Iteration: 3876 lambda_k: 1 Loss: 0.019409422898841386\n",
      "Iteration: 3877 lambda_k: 1 Loss: 0.019382145606707146\n",
      "Iteration: 3878 lambda_k: 1 Loss: 0.01935487584018824\n",
      "Iteration: 3879 lambda_k: 1 Loss: 0.019327613597485832\n",
      "Iteration: 3880 lambda_k: 1 Loss: 0.019300358876802214\n",
      "Iteration: 3881 lambda_k: 1 Loss: 0.019273111676340843\n",
      "Iteration: 3882 lambda_k: 1 Loss: 0.0192458719943065\n",
      "Iteration: 3883 lambda_k: 1 Loss: 0.01921863982890492\n",
      "Iteration: 3884 lambda_k: 1 Loss: 0.01919141517834325\n",
      "Iteration: 3885 lambda_k: 1 Loss: 0.019164198040829687\n",
      "Iteration: 3886 lambda_k: 1 Loss: 0.019136988414573727\n",
      "Iteration: 3887 lambda_k: 1 Loss: 0.019109786297786\n",
      "Iteration: 3888 lambda_k: 1 Loss: 0.019082591688678516\n",
      "Iteration: 3889 lambda_k: 1 Loss: 0.01905540458546437\n",
      "Iteration: 3890 lambda_k: 1 Loss: 0.019028224986357922\n",
      "Iteration: 3891 lambda_k: 1 Loss: 0.0190010528895749\n",
      "Iteration: 3892 lambda_k: 1 Loss: 0.01897388829333225\n",
      "Iteration: 3893 lambda_k: 1 Loss: 0.01894673119584812\n",
      "Iteration: 3894 lambda_k: 1 Loss: 0.018919581595341996\n",
      "Iteration: 3895 lambda_k: 1 Loss: 0.018892439490034735\n",
      "Iteration: 3896 lambda_k: 1 Loss: 0.01886530487814836\n",
      "Iteration: 3897 lambda_k: 1 Loss: 0.01883817775790621\n",
      "Iteration: 3898 lambda_k: 1 Loss: 0.018811058127533066\n",
      "Iteration: 3899 lambda_k: 1 Loss: 0.01878394598525499\n",
      "Iteration: 3900 lambda_k: 1 Loss: 0.01875684132929936\n",
      "Iteration: 3901 lambda_k: 1 Loss: 0.01872974415789485\n",
      "Iteration: 3902 lambda_k: 1 Loss: 0.018702654469271645\n",
      "Iteration: 3903 lambda_k: 1 Loss: 0.018675572261661156\n",
      "Iteration: 3904 lambda_k: 1 Loss: 0.01864849753329626\n",
      "Iteration: 3905 lambda_k: 1 Loss: 0.018621430282411186\n",
      "Iteration: 3906 lambda_k: 1 Loss: 0.018594370507241515\n",
      "Iteration: 3907 lambda_k: 1 Loss: 0.01856731820602432\n",
      "Iteration: 3908 lambda_k: 1 Loss: 0.01854027337699803\n",
      "Iteration: 3909 lambda_k: 1 Loss: 0.018513236018402473\n",
      "Iteration: 3910 lambda_k: 1 Loss: 0.018486206128479052\n",
      "Iteration: 3911 lambda_k: 1 Loss: 0.018459183705470443\n",
      "Iteration: 3912 lambda_k: 1 Loss: 0.01843216874762094\n",
      "Iteration: 3913 lambda_k: 1 Loss: 0.018405161253176185\n",
      "Iteration: 3914 lambda_k: 1 Loss: 0.018378161220383396\n",
      "Iteration: 3915 lambda_k: 1 Loss: 0.01835116864749114\n",
      "Iteration: 3916 lambda_k: 1 Loss: 0.018324183532749658\n",
      "Iteration: 3917 lambda_k: 1 Loss: 0.018297205874408896\n",
      "Iteration: 3918 lambda_k: 1 Loss: 0.018270235670721953\n",
      "Iteration: 3919 lambda_k: 1 Loss: 0.018243272919945257\n",
      "Iteration: 3920 lambda_k: 1 Loss: 0.01821631762033503\n",
      "Iteration: 3921 lambda_k: 1 Loss: 0.018189369770149143\n",
      "Iteration: 3922 lambda_k: 1 Loss: 0.018162429367646827\n",
      "Iteration: 3923 lambda_k: 1 Loss: 0.018135496411089132\n",
      "Iteration: 3924 lambda_k: 1 Loss: 0.018108570898738453\n",
      "Iteration: 3925 lambda_k: 1 Loss: 0.018081652828956164\n",
      "Iteration: 3926 lambda_k: 1 Loss: 0.018054742199672548\n",
      "Iteration: 3927 lambda_k: 1 Loss: 0.018027839010253982\n",
      "Iteration: 3928 lambda_k: 1 Loss: 0.018000943256861947\n",
      "Iteration: 3929 lambda_k: 1 Loss: 0.01797405493948221\n",
      "Iteration: 3930 lambda_k: 1 Loss: 0.01794717405594296\n",
      "Iteration: 3931 lambda_k: 1 Loss: 0.0179203006045056\n",
      "Iteration: 3932 lambda_k: 1 Loss: 0.01789343458344039\n",
      "Iteration: 3933 lambda_k: 1 Loss: 0.017866575991021944\n",
      "Iteration: 3934 lambda_k: 1 Loss: 0.017839724825527745\n",
      "Iteration: 3935 lambda_k: 1 Loss: 0.017812881085240116\n",
      "Iteration: 3936 lambda_k: 1 Loss: 0.01778604476843988\n",
      "Iteration: 3937 lambda_k: 1 Loss: 0.017759215873408223\n",
      "Iteration: 3938 lambda_k: 1 Loss: 0.017732394398430874\n",
      "Iteration: 3939 lambda_k: 1 Loss: 0.017705580341795386\n",
      "Iteration: 3940 lambda_k: 1 Loss: 0.017678773701790995\n",
      "Iteration: 3941 lambda_k: 1 Loss: 0.01765197447670879\n",
      "Iteration: 3942 lambda_k: 1 Loss: 0.017625182664841504\n",
      "Iteration: 3943 lambda_k: 1 Loss: 0.017598398264483905\n",
      "Iteration: 3944 lambda_k: 1 Loss: 0.017571621273932372\n",
      "Iteration: 3945 lambda_k: 1 Loss: 0.017544851691485237\n",
      "Iteration: 3946 lambda_k: 1 Loss: 0.017518089515442533\n",
      "Iteration: 3947 lambda_k: 1 Loss: 0.017491334744106263\n",
      "Iteration: 3948 lambda_k: 1 Loss: 0.01746458737578023\n",
      "Iteration: 3949 lambda_k: 1 Loss: 0.017437847408770097\n",
      "Iteration: 3950 lambda_k: 1 Loss: 0.017411114841383454\n",
      "Iteration: 3951 lambda_k: 1 Loss: 0.017384389671929817\n",
      "Iteration: 3952 lambda_k: 1 Loss: 0.017357671898720506\n",
      "Iteration: 3953 lambda_k: 1 Loss: 0.017330961520068813\n",
      "Iteration: 3954 lambda_k: 1 Loss: 0.017304258534289994\n",
      "Iteration: 3955 lambda_k: 1 Loss: 0.01727756293970124\n",
      "Iteration: 3956 lambda_k: 1 Loss: 0.01725087473462166\n",
      "Iteration: 3957 lambda_k: 1 Loss: 0.01722419391737238\n",
      "Iteration: 3958 lambda_k: 1 Loss: 0.017197520486276506\n",
      "Iteration: 3959 lambda_k: 1 Loss: 0.01717085443965915\n",
      "Iteration: 3960 lambda_k: 1 Loss: 0.017144195775847432\n",
      "Iteration: 3961 lambda_k: 1 Loss: 0.017117544493170468\n",
      "Iteration: 3962 lambda_k: 1 Loss: 0.017090900589959483\n",
      "Iteration: 3963 lambda_k: 1 Loss: 0.017064264064547613\n",
      "Iteration: 3964 lambda_k: 1 Loss: 0.01703763491527029\n",
      "Iteration: 3965 lambda_k: 1 Loss: 0.017011013140464805\n",
      "Iteration: 3966 lambda_k: 1 Loss: 0.01698439873847072\n",
      "Iteration: 3967 lambda_k: 1 Loss: 0.01695779170762956\n",
      "Iteration: 3968 lambda_k: 1 Loss: 0.01693119204628514\n",
      "Iteration: 3969 lambda_k: 1 Loss: 0.016904599752783266\n",
      "Iteration: 3970 lambda_k: 1 Loss: 0.016878014825471927\n",
      "Iteration: 3971 lambda_k: 1 Loss: 0.01685143726270129\n",
      "Iteration: 3972 lambda_k: 1 Loss: 0.016824867062823698\n",
      "Iteration: 3973 lambda_k: 1 Loss: 0.0167983042241938\n",
      "Iteration: 3974 lambda_k: 1 Loss: 0.016771748745168232\n",
      "Iteration: 3975 lambda_k: 1 Loss: 0.016745200624106022\n",
      "Iteration: 3976 lambda_k: 1 Loss: 0.016718659859368396\n",
      "Iteration: 3977 lambda_k: 1 Loss: 0.01669212644931885\n",
      "Iteration: 3978 lambda_k: 1 Loss: 0.01666560039232313\n",
      "Iteration: 3979 lambda_k: 1 Loss: 0.01663908168674928\n",
      "Iteration: 3980 lambda_k: 1 Loss: 0.016612570330967618\n",
      "Iteration: 3981 lambda_k: 1 Loss: 0.01658606632335071\n",
      "Iteration: 3982 lambda_k: 1 Loss: 0.016559569662273628\n",
      "Iteration: 3983 lambda_k: 1 Loss: 0.016533080346113668\n",
      "Iteration: 3984 lambda_k: 1 Loss: 0.0165065983732505\n",
      "Iteration: 3985 lambda_k: 1 Loss: 0.01648012374206621\n",
      "Iteration: 3986 lambda_k: 1 Loss: 0.016453656450945153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3987 lambda_k: 1 Loss: 0.0164271964982743\n",
      "Iteration: 3988 lambda_k: 1 Loss: 0.016400743882442864\n",
      "Iteration: 3989 lambda_k: 1 Loss: 0.01637429860184259\n",
      "Iteration: 3990 lambda_k: 1 Loss: 0.016347860654867596\n",
      "Iteration: 3991 lambda_k: 1 Loss: 0.01632143003991467\n",
      "Iteration: 3992 lambda_k: 1 Loss: 0.016295006755382833\n",
      "Iteration: 3993 lambda_k: 1 Loss: 0.016268590799673802\n",
      "Iteration: 3994 lambda_k: 1 Loss: 0.01624218217119178\n",
      "Iteration: 3995 lambda_k: 1 Loss: 0.016215780868343486\n",
      "Iteration: 3996 lambda_k: 1 Loss: 0.016189386889538193\n",
      "Iteration: 3997 lambda_k: 1 Loss: 0.016163000233187782\n",
      "Iteration: 3998 lambda_k: 1 Loss: 0.016136620897706764\n",
      "Iteration: 3999 lambda_k: 1 Loss: 0.01611024888151216\n",
      "Iteration: 4000 lambda_k: 1 Loss: 0.01608388418302454\n",
      "Iteration: 4001 lambda_k: 1 Loss: 0.01605752680066585\n",
      "Iteration: 4002 lambda_k: 1 Loss: 0.016031176732860766\n",
      "Iteration: 4003 lambda_k: 1 Loss: 0.016004833978037095\n",
      "Iteration: 4004 lambda_k: 1 Loss: 0.01597849853462524\n",
      "Iteration: 4005 lambda_k: 1 Loss: 0.01595217040105846\n",
      "Iteration: 4006 lambda_k: 1 Loss: 0.015925849575772647\n",
      "Iteration: 4007 lambda_k: 1 Loss: 0.015899536057206468\n",
      "Iteration: 4008 lambda_k: 1 Loss: 0.01587322984380145\n",
      "Iteration: 4009 lambda_k: 1 Loss: 0.015846930934001986\n",
      "Iteration: 4010 lambda_k: 1 Loss: 0.015820639326255142\n",
      "Iteration: 4011 lambda_k: 1 Loss: 0.015794355019010983\n",
      "Iteration: 4012 lambda_k: 1 Loss: 0.01576807801072233\n",
      "Iteration: 4013 lambda_k: 1 Loss: 0.01574180829984495\n",
      "Iteration: 4014 lambda_k: 1 Loss: 0.015715545884837603\n",
      "Iteration: 4015 lambda_k: 1 Loss: 0.015689290764161836\n",
      "Iteration: 4016 lambda_k: 1 Loss: 0.015663042936282203\n",
      "Iteration: 4017 lambda_k: 1 Loss: 0.015636802399666333\n",
      "Iteration: 4018 lambda_k: 1 Loss: 0.015610569152784701\n",
      "Iteration: 4019 lambda_k: 1 Loss: 0.015584343194110891\n",
      "Iteration: 4020 lambda_k: 1 Loss: 0.015558124522121576\n",
      "Iteration: 4021 lambda_k: 1 Loss: 0.015531913135296395\n",
      "Iteration: 4022 lambda_k: 1 Loss: 0.015505709032118035\n",
      "Iteration: 4023 lambda_k: 1 Loss: 0.01547951221107248\n",
      "Iteration: 4024 lambda_k: 1 Loss: 0.015453322670648641\n",
      "Iteration: 4025 lambda_k: 1 Loss: 0.015427140409338727\n",
      "Iteration: 4026 lambda_k: 1 Loss: 0.015400965425638015\n",
      "Iteration: 4027 lambda_k: 1 Loss: 0.015374797718045046\n",
      "Iteration: 4028 lambda_k: 1 Loss: 0.015348637285061643\n",
      "Iteration: 4029 lambda_k: 1 Loss: 0.015322484125192696\n",
      "Iteration: 4030 lambda_k: 1 Loss: 0.015296338236946531\n",
      "Iteration: 4031 lambda_k: 1 Loss: 0.01527019961883468\n",
      "Iteration: 4032 lambda_k: 1 Loss: 0.015244068269372047\n",
      "Iteration: 4033 lambda_k: 1 Loss: 0.015217944187076804\n",
      "Iteration: 4034 lambda_k: 1 Loss: 0.015191827370470548\n",
      "Iteration: 4035 lambda_k: 1 Loss: 0.015165717818078189\n",
      "Iteration: 4036 lambda_k: 1 Loss: 0.015139615528428125\n",
      "Iteration: 4037 lambda_k: 1 Loss: 0.015113520500052194\n",
      "Iteration: 4038 lambda_k: 1 Loss: 0.015087432731485733\n",
      "Iteration: 4039 lambda_k: 1 Loss: 0.015061352221267444\n",
      "Iteration: 4040 lambda_k: 1 Loss: 0.015035278969300302\n",
      "Iteration: 4041 lambda_k: 1 Loss: 0.015009212971426187\n",
      "Iteration: 4042 lambda_k: 1 Loss: 0.014983154227537602\n",
      "Iteration: 4043 lambda_k: 1 Loss: 0.014957102736187526\n",
      "Iteration: 4044 lambda_k: 1 Loss: 0.014931058495932526\n",
      "Iteration: 4045 lambda_k: 1 Loss: 0.014905021505332907\n",
      "Iteration: 4046 lambda_k: 1 Loss: 0.014878991762952702\n",
      "Iteration: 4047 lambda_k: 1 Loss: 0.014852969267359455\n",
      "Iteration: 4048 lambda_k: 1 Loss: 0.01482695401712454\n",
      "Iteration: 4049 lambda_k: 1 Loss: 0.014800946010823119\n",
      "Iteration: 4050 lambda_k: 1 Loss: 0.014774945247034118\n",
      "Iteration: 4051 lambda_k: 1 Loss: 0.014748951724340214\n",
      "Iteration: 4052 lambda_k: 1 Loss: 0.014722965441328008\n",
      "Iteration: 4053 lambda_k: 1 Loss: 0.014696986396588013\n",
      "Iteration: 4054 lambda_k: 1 Loss: 0.014671014588714528\n",
      "Iteration: 4055 lambda_k: 1 Loss: 0.01464505001630585\n",
      "Iteration: 4056 lambda_k: 1 Loss: 0.014619092677964208\n",
      "Iteration: 4057 lambda_k: 1 Loss: 0.014593142572295884\n",
      "Iteration: 4058 lambda_k: 1 Loss: 0.014567199697911076\n",
      "Iteration: 4059 lambda_k: 1 Loss: 0.014541264053424154\n",
      "Iteration: 4060 lambda_k: 1 Loss: 0.014515335637453539\n",
      "Iteration: 4061 lambda_k: 1 Loss: 0.014489414448621694\n",
      "Iteration: 4062 lambda_k: 1 Loss: 0.014463500485555286\n",
      "Iteration: 4063 lambda_k: 1 Loss: 0.014437593746885215\n",
      "Iteration: 4064 lambda_k: 1 Loss: 0.014411694231246509\n",
      "Iteration: 4065 lambda_k: 1 Loss: 0.01438580193727846\n",
      "Iteration: 4066 lambda_k: 1 Loss: 0.014359916863624632\n",
      "Iteration: 4067 lambda_k: 1 Loss: 0.014334039008932946\n",
      "Iteration: 4068 lambda_k: 1 Loss: 0.014308168371855648\n",
      "Iteration: 4069 lambda_k: 1 Loss: 0.014282304951049363\n",
      "Iteration: 4070 lambda_k: 1 Loss: 0.014256448745175086\n",
      "Iteration: 4071 lambda_k: 1 Loss: 0.014230599752898275\n",
      "Iteration: 4072 lambda_k: 1 Loss: 0.0142047579728889\n",
      "Iteration: 4073 lambda_k: 1 Loss: 0.014178923403821469\n",
      "Iteration: 4074 lambda_k: 1 Loss: 0.014153096044374982\n",
      "Iteration: 4075 lambda_k: 1 Loss: 0.01412727589323308\n",
      "Iteration: 4076 lambda_k: 1 Loss: 0.014101462949084046\n",
      "Iteration: 4077 lambda_k: 1 Loss: 0.014075657210620741\n",
      "Iteration: 4078 lambda_k: 1 Loss: 0.014049858676540789\n",
      "Iteration: 4079 lambda_k: 1 Loss: 0.014024067345546518\n",
      "Iteration: 4080 lambda_k: 1 Loss: 0.013998283216345055\n",
      "Iteration: 4081 lambda_k: 1 Loss: 0.013972506287648317\n",
      "Iteration: 4082 lambda_k: 1 Loss: 0.013946736558173179\n",
      "Iteration: 4083 lambda_k: 1 Loss: 0.013920974026641275\n",
      "Iteration: 4084 lambda_k: 1 Loss: 0.013895218691779157\n",
      "Iteration: 4085 lambda_k: 1 Loss: 0.013869470552318467\n",
      "Iteration: 4086 lambda_k: 1 Loss: 0.013843729606995847\n",
      "Iteration: 4087 lambda_k: 1 Loss: 0.013817995854552827\n",
      "Iteration: 4088 lambda_k: 1 Loss: 0.013792269293736191\n",
      "Iteration: 4089 lambda_k: 1 Loss: 0.013766549923297903\n",
      "Iteration: 4090 lambda_k: 1 Loss: 0.013740837741994949\n",
      "Iteration: 4091 lambda_k: 1 Loss: 0.013715132748589542\n",
      "Iteration: 4092 lambda_k: 1 Loss: 0.01368943494184929\n",
      "Iteration: 4093 lambda_k: 1 Loss: 0.013663744320546992\n",
      "Iteration: 4094 lambda_k: 1 Loss: 0.013638060883460824\n",
      "Iteration: 4095 lambda_k: 1 Loss: 0.013612384629374356\n",
      "Iteration: 4096 lambda_k: 1 Loss: 0.013586715557076627\n",
      "Iteration: 4097 lambda_k: 1 Loss: 0.013561053665362177\n",
      "Iteration: 4098 lambda_k: 1 Loss: 0.01353539895303097\n",
      "Iteration: 4099 lambda_k: 1 Loss: 0.013509751418888665\n",
      "Iteration: 4100 lambda_k: 1 Loss: 0.013484111061746415\n",
      "Iteration: 4101 lambda_k: 1 Loss: 0.013458477880421149\n",
      "Iteration: 4102 lambda_k: 1 Loss: 0.013432851873735453\n",
      "Iteration: 4103 lambda_k: 1 Loss: 0.013407233040517772\n",
      "Iteration: 4104 lambda_k: 1 Loss: 0.0133816213796022\n",
      "Iteration: 4105 lambda_k: 1 Loss: 0.013356016889828832\n",
      "Iteration: 4106 lambda_k: 1 Loss: 0.013330419570043552\n",
      "Iteration: 4107 lambda_k: 1 Loss: 0.013304829419098317\n",
      "Iteration: 4108 lambda_k: 1 Loss: 0.013279246435851074\n",
      "Iteration: 4109 lambda_k: 1 Loss: 0.013253670619165775\n",
      "Iteration: 4110 lambda_k: 1 Loss: 0.013228101967912529\n",
      "Iteration: 4111 lambda_k: 1 Loss: 0.013202540480967678\n",
      "Iteration: 4112 lambda_k: 1 Loss: 0.013176986157213591\n",
      "Iteration: 4113 lambda_k: 1 Loss: 0.013151438995539173\n",
      "Iteration: 4114 lambda_k: 1 Loss: 0.013125898994839413\n",
      "Iteration: 4115 lambda_k: 1 Loss: 0.013100366154015773\n",
      "Iteration: 4116 lambda_k: 1 Loss: 0.013074840471976195\n",
      "Iteration: 4117 lambda_k: 1 Loss: 0.013049321947635105\n",
      "Iteration: 4118 lambda_k: 1 Loss: 0.013023810579913444\n",
      "Iteration: 4119 lambda_k: 1 Loss: 0.012998306367738731\n",
      "Iteration: 4120 lambda_k: 1 Loss: 0.012972809310045236\n",
      "Iteration: 4121 lambda_k: 1 Loss: 0.01294731940577388\n",
      "Iteration: 4122 lambda_k: 1 Loss: 0.012921836653872382\n",
      "Iteration: 4123 lambda_k: 1 Loss: 0.012896361050351525\n",
      "Iteration: 4124 lambda_k: 1 Loss: 0.012870892600051873\n",
      "Iteration: 4125 lambda_k: 1 Loss: 0.012845431299006806\n",
      "Iteration: 4126 lambda_k: 1 Loss: 0.012819977146191744\n",
      "Iteration: 4127 lambda_k: 1 Loss: 0.012794530140589186\n",
      "Iteration: 4128 lambda_k: 1 Loss: 0.012769090281188864\n",
      "Iteration: 4129 lambda_k: 1 Loss: 0.012743657566987648\n",
      "Iteration: 4130 lambda_k: 1 Loss: 0.012718231996989748\n",
      "Iteration: 4131 lambda_k: 1 Loss: 0.012692813570206603\n",
      "Iteration: 4132 lambda_k: 1 Loss: 0.01266740228565723\n",
      "Iteration: 4133 lambda_k: 1 Loss: 0.012641998142367985\n",
      "Iteration: 4134 lambda_k: 1 Loss: 0.012616601139372848\n",
      "Iteration: 4135 lambda_k: 1 Loss: 0.012591211275713343\n",
      "Iteration: 4136 lambda_k: 1 Loss: 0.012565828550438723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4137 lambda_k: 1 Loss: 0.01254045296260597\n",
      "Iteration: 4138 lambda_k: 1 Loss: 0.012515084511279854\n",
      "Iteration: 4139 lambda_k: 1 Loss: 0.012489723195533174\n",
      "Iteration: 4140 lambda_k: 1 Loss: 0.012464369014446443\n",
      "Iteration: 4141 lambda_k: 1 Loss: 0.012439021967108485\n",
      "Iteration: 4142 lambda_k: 1 Loss: 0.012413682052616037\n",
      "Iteration: 4143 lambda_k: 1 Loss: 0.0123883492700741\n",
      "Iteration: 4144 lambda_k: 1 Loss: 0.012363023618595896\n",
      "Iteration: 4145 lambda_k: 1 Loss: 0.012337705097303054\n",
      "Iteration: 4146 lambda_k: 1 Loss: 0.012312393705325506\n",
      "Iteration: 4147 lambda_k: 1 Loss: 0.012287089441801836\n",
      "Iteration: 4148 lambda_k: 1 Loss: 0.012261792305879064\n",
      "Iteration: 4149 lambda_k: 1 Loss: 0.012236502296712894\n",
      "Iteration: 4150 lambda_k: 1 Loss: 0.012211219413467806\n",
      "Iteration: 4151 lambda_k: 1 Loss: 0.012185943655317014\n",
      "Iteration: 4152 lambda_k: 1 Loss: 0.012160675021442672\n",
      "Iteration: 4153 lambda_k: 1 Loss: 0.012135413511035883\n",
      "Iteration: 4154 lambda_k: 1 Loss: 0.012110159123296911\n",
      "Iteration: 4155 lambda_k: 1 Loss: 0.012084911857435106\n",
      "Iteration: 4156 lambda_k: 1 Loss: 0.012059671712669008\n",
      "Iteration: 4157 lambda_k: 1 Loss: 0.01203443868822654\n",
      "Iteration: 4158 lambda_k: 1 Loss: 0.01200921278334504\n",
      "Iteration: 4159 lambda_k: 1 Loss: 0.011983993997271302\n",
      "Iteration: 4160 lambda_k: 1 Loss: 0.011958782329261761\n",
      "Iteration: 4161 lambda_k: 1 Loss: 0.011933577778582574\n",
      "Iteration: 4162 lambda_k: 1 Loss: 0.01190838034450963\n",
      "Iteration: 4163 lambda_k: 1 Loss: 0.011883190026328677\n",
      "Iteration: 4164 lambda_k: 1 Loss: 0.011858006823335371\n",
      "Iteration: 4165 lambda_k: 1 Loss: 0.011832830734835627\n",
      "Iteration: 4166 lambda_k: 1 Loss: 0.011807661760145392\n",
      "Iteration: 4167 lambda_k: 1 Loss: 0.011782499898590894\n",
      "Iteration: 4168 lambda_k: 1 Loss: 0.01175734514950863\n",
      "Iteration: 4169 lambda_k: 1 Loss: 0.011732197512245768\n",
      "Iteration: 4170 lambda_k: 1 Loss: 0.011707056986159963\n",
      "Iteration: 4171 lambda_k: 1 Loss: 0.011681923570619406\n",
      "Iteration: 4172 lambda_k: 1 Loss: 0.011656797265003271\n",
      "Iteration: 4173 lambda_k: 1 Loss: 0.011631678068701452\n",
      "Iteration: 4174 lambda_k: 1 Loss: 0.011606565981114907\n",
      "Iteration: 4175 lambda_k: 1 Loss: 0.01158146100165567\n",
      "Iteration: 4176 lambda_k: 1 Loss: 0.011556363129747015\n",
      "Iteration: 4177 lambda_k: 1 Loss: 0.011531272364823535\n",
      "Iteration: 4178 lambda_k: 1 Loss: 0.011506188706331237\n",
      "Iteration: 4179 lambda_k: 1 Loss: 0.011481112153727704\n",
      "Iteration: 4180 lambda_k: 1 Loss: 0.011456042706482112\n",
      "Iteration: 4181 lambda_k: 1 Loss: 0.011430980364075489\n",
      "Iteration: 4182 lambda_k: 1 Loss: 0.011405925126000806\n",
      "Iteration: 4183 lambda_k: 1 Loss: 0.011380876991762936\n",
      "Iteration: 4184 lambda_k: 1 Loss: 0.011355835960878998\n",
      "Iteration: 4185 lambda_k: 1 Loss: 0.011330802032878265\n",
      "Iteration: 4186 lambda_k: 1 Loss: 0.011305775207302507\n",
      "Iteration: 4187 lambda_k: 1 Loss: 0.011280755483706065\n",
      "Iteration: 4188 lambda_k: 1 Loss: 0.011255742861655732\n",
      "Iteration: 4189 lambda_k: 1 Loss: 0.011230737340731238\n",
      "Iteration: 4190 lambda_k: 1 Loss: 0.011205738920525177\n",
      "Iteration: 4191 lambda_k: 1 Loss: 0.011180747600643158\n",
      "Iteration: 4192 lambda_k: 1 Loss: 0.011155763380703983\n",
      "Iteration: 4193 lambda_k: 1 Loss: 0.011130786260339797\n",
      "Iteration: 4194 lambda_k: 1 Loss: 0.011105816239196206\n",
      "Iteration: 4195 lambda_k: 1 Loss: 0.011080853316932275\n",
      "Iteration: 4196 lambda_k: 1 Loss: 0.011055897493220937\n",
      "Iteration: 4197 lambda_k: 1 Loss: 0.011030948767749035\n",
      "Iteration: 4198 lambda_k: 1 Loss: 0.011006007140217287\n",
      "Iteration: 4199 lambda_k: 1 Loss: 0.010981072610340677\n",
      "Iteration: 4200 lambda_k: 1 Loss: 0.010956145177848408\n",
      "Iteration: 4201 lambda_k: 1 Loss: 0.01093122484248433\n",
      "Iteration: 4202 lambda_k: 1 Loss: 0.010906311604006751\n",
      "Iteration: 4203 lambda_k: 1 Loss: 0.010881405462188876\n",
      "Iteration: 4204 lambda_k: 1 Loss: 0.01085650641681879\n",
      "Iteration: 4205 lambda_k: 1 Loss: 0.01083161446769965\n",
      "Iteration: 4206 lambda_k: 1 Loss: 0.01080672961464984\n",
      "Iteration: 4207 lambda_k: 1 Loss: 0.010781851857503287\n",
      "Iteration: 4208 lambda_k: 1 Loss: 0.01075698119610941\n",
      "Iteration: 4209 lambda_k: 1 Loss: 0.010732117630333476\n",
      "Iteration: 4210 lambda_k: 1 Loss: 0.01070726116005658\n",
      "Iteration: 4211 lambda_k: 1 Loss: 0.010682411785175832\n",
      "Iteration: 4212 lambda_k: 1 Loss: 0.01065756950560487\n",
      "Iteration: 4213 lambda_k: 1 Loss: 0.010632734321273566\n",
      "Iteration: 4214 lambda_k: 1 Loss: 0.010607906232128497\n",
      "Iteration: 4215 lambda_k: 1 Loss: 0.010583085238133024\n",
      "Iteration: 4216 lambda_k: 1 Loss: 0.010558271339267502\n",
      "Iteration: 4217 lambda_k: 1 Loss: 0.010533464535529504\n",
      "Iteration: 4218 lambda_k: 1 Loss: 0.010508664826933934\n",
      "Iteration: 4219 lambda_k: 1 Loss: 0.010483872213513374\n",
      "Iteration: 4220 lambda_k: 1 Loss: 0.010459086695317964\n",
      "Iteration: 4221 lambda_k: 1 Loss: 0.010434308272415926\n",
      "Iteration: 4222 lambda_k: 1 Loss: 0.01040953694489357\n",
      "Iteration: 4223 lambda_k: 1 Loss: 0.01038477271285563\n",
      "Iteration: 4224 lambda_k: 1 Loss: 0.010360015576425356\n",
      "Iteration: 4225 lambda_k: 1 Loss: 0.010335265535744811\n",
      "Iteration: 4226 lambda_k: 1 Loss: 0.01031052259097501\n",
      "Iteration: 4227 lambda_k: 1 Loss: 0.010285786742296105\n",
      "Iteration: 4228 lambda_k: 1 Loss: 0.010261057989907748\n",
      "Iteration: 4229 lambda_k: 1 Loss: 0.010236336334029183\n",
      "Iteration: 4230 lambda_k: 1 Loss: 0.01021162177489949\n",
      "Iteration: 4231 lambda_k: 1 Loss: 0.01018691431277789\n",
      "Iteration: 4232 lambda_k: 1 Loss: 0.010162213947943888\n",
      "Iteration: 4233 lambda_k: 1 Loss: 0.010137520680697495\n",
      "Iteration: 4234 lambda_k: 1 Loss: 0.010112834511359555\n",
      "Iteration: 4235 lambda_k: 1 Loss: 0.010088155440271829\n",
      "Iteration: 4236 lambda_k: 1 Loss: 0.01006348346779748\n",
      "Iteration: 4237 lambda_k: 1 Loss: 0.010038818594321033\n",
      "Iteration: 4238 lambda_k: 1 Loss: 0.010014160820248872\n",
      "Iteration: 4239 lambda_k: 1 Loss: 0.00998951014600936\n",
      "Iteration: 4240 lambda_k: 1 Loss: 0.009964866572053099\n",
      "Iteration: 4241 lambda_k: 1 Loss: 0.009940230098853217\n",
      "Iteration: 4242 lambda_k: 1 Loss: 0.00991560072690562\n",
      "Iteration: 4243 lambda_k: 1 Loss: 0.00989097845672928\n",
      "Iteration: 4244 lambda_k: 1 Loss: 0.009866363288866512\n",
      "Iteration: 4245 lambda_k: 1 Loss: 0.009841755223883279\n",
      "Iteration: 4246 lambda_k: 1 Loss: 0.009817154262369288\n",
      "Iteration: 4247 lambda_k: 1 Loss: 0.009792560404938483\n",
      "Iteration: 4248 lambda_k: 1 Loss: 0.009767973652229329\n",
      "Iteration: 4249 lambda_k: 1 Loss: 0.009743394004904853\n",
      "Iteration: 4250 lambda_k: 1 Loss: 0.009718821463653361\n",
      "Iteration: 4251 lambda_k: 1 Loss: 0.009694256029188279\n",
      "Iteration: 4252 lambda_k: 1 Loss: 0.009669697702248794\n",
      "Iteration: 4253 lambda_k: 1 Loss: 0.009645146483600018\n",
      "Iteration: 4254 lambda_k: 1 Loss: 0.009620602374033262\n",
      "Iteration: 4255 lambda_k: 1 Loss: 0.00959606537436658\n",
      "Iteration: 4256 lambda_k: 1 Loss: 0.009571535485444685\n",
      "Iteration: 4257 lambda_k: 1 Loss: 0.009547012708139808\n",
      "Iteration: 4258 lambda_k: 1 Loss: 0.009522497043351591\n",
      "Iteration: 4259 lambda_k: 1 Loss: 0.00949798849200752\n",
      "Iteration: 4260 lambda_k: 1 Loss: 0.009473487055063467\n",
      "Iteration: 4261 lambda_k: 1 Loss: 0.009448992733503827\n",
      "Iteration: 4262 lambda_k: 1 Loss: 0.009424505528341877\n",
      "Iteration: 4263 lambda_k: 1 Loss: 0.009400025440620373\n",
      "Iteration: 4264 lambda_k: 1 Loss: 0.009375552471411582\n",
      "Iteration: 4265 lambda_k: 1 Loss: 0.009351086621817821\n",
      "Iteration: 4266 lambda_k: 1 Loss: 0.009326627892971884\n",
      "Iteration: 4267 lambda_k: 1 Loss: 0.009302176286037342\n",
      "Iteration: 4268 lambda_k: 1 Loss: 0.009277731802208948\n",
      "Iteration: 4269 lambda_k: 1 Loss: 0.009253294442712886\n",
      "Iteration: 4270 lambda_k: 1 Loss: 0.009228864208807482\n",
      "Iteration: 4271 lambda_k: 1 Loss: 0.009204441101783376\n",
      "Iteration: 4272 lambda_k: 1 Loss: 0.009180025122964027\n",
      "Iteration: 4273 lambda_k: 1 Loss: 0.009155616273705959\n",
      "Iteration: 4274 lambda_k: 1 Loss: 0.009131214555399442\n",
      "Iteration: 4275 lambda_k: 1 Loss: 0.009106819969468776\n",
      "Iteration: 4276 lambda_k: 1 Loss: 0.009082432517372722\n",
      "Iteration: 4277 lambda_k: 1 Loss: 0.009058052200604924\n",
      "Iteration: 4278 lambda_k: 1 Loss: 0.009033679020694444\n",
      "Iteration: 4279 lambda_k: 1 Loss: 0.009009312979206216\n",
      "Iteration: 4280 lambda_k: 1 Loss: 0.008984954077741482\n",
      "Iteration: 4281 lambda_k: 1 Loss: 0.008960602317938172\n",
      "Iteration: 4282 lambda_k: 1 Loss: 0.008936257701471453\n",
      "Iteration: 4283 lambda_k: 1 Loss: 0.008911920230054297\n",
      "Iteration: 4284 lambda_k: 1 Loss: 0.008887589905437969\n",
      "Iteration: 4285 lambda_k: 1 Loss: 0.0088632667294123\n",
      "Iteration: 4286 lambda_k: 1 Loss: 0.008838950703806475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4287 lambda_k: 1 Loss: 0.008814641830489382\n",
      "Iteration: 4288 lambda_k: 1 Loss: 0.008790340111370252\n",
      "Iteration: 4289 lambda_k: 1 Loss: 0.008766045548399066\n",
      "Iteration: 4290 lambda_k: 1 Loss: 0.008741758143567167\n",
      "Iteration: 4291 lambda_k: 1 Loss: 0.008717477898907902\n",
      "Iteration: 4292 lambda_k: 1 Loss: 0.00869320481649699\n",
      "Iteration: 4293 lambda_k: 1 Loss: 0.008668938898453197\n",
      "Iteration: 4294 lambda_k: 1 Loss: 0.008644680146938958\n",
      "Iteration: 4295 lambda_k: 1 Loss: 0.008620428564161006\n",
      "Iteration: 4296 lambda_k: 1 Loss: 0.00859618415237072\n",
      "Iteration: 4297 lambda_k: 1 Loss: 0.008571946913864951\n",
      "Iteration: 4298 lambda_k: 1 Loss: 0.008547716850986617\n",
      "Iteration: 4299 lambda_k: 1 Loss: 0.008523493966125313\n",
      "Iteration: 4300 lambda_k: 1 Loss: 0.00849927826171788\n",
      "Iteration: 4301 lambda_k: 1 Loss: 0.008475069740249183\n",
      "Iteration: 4302 lambda_k: 1 Loss: 0.008450868404252597\n",
      "Iteration: 4303 lambda_k: 1 Loss: 0.008426674256310938\n",
      "Iteration: 4304 lambda_k: 1 Loss: 0.008402487299056797\n",
      "Iteration: 4305 lambda_k: 1 Loss: 0.008378307535173548\n",
      "Iteration: 4306 lambda_k: 1 Loss: 0.008354134967395987\n",
      "Iteration: 4307 lambda_k: 1 Loss: 0.008329969598510768\n",
      "Iteration: 4308 lambda_k: 1 Loss: 0.00830581143135756\n",
      "Iteration: 4309 lambda_k: 1 Loss: 0.008281660468829558\n",
      "Iteration: 4310 lambda_k: 1 Loss: 0.008257516713874155\n",
      "Iteration: 4311 lambda_k: 1 Loss: 0.008233380169493898\n",
      "Iteration: 4312 lambda_k: 1 Loss: 0.008209250838747175\n",
      "Iteration: 4313 lambda_k: 1 Loss: 0.008185128724749075\n",
      "Iteration: 4314 lambda_k: 1 Loss: 0.008161013830672048\n",
      "Iteration: 4315 lambda_k: 1 Loss: 0.008136906159746845\n",
      "Iteration: 4316 lambda_k: 1 Loss: 0.008112805715263393\n",
      "Iteration: 4317 lambda_k: 1 Loss: 0.008088712500571481\n",
      "Iteration: 4318 lambda_k: 1 Loss: 0.008064626519081759\n",
      "Iteration: 4319 lambda_k: 1 Loss: 0.00804054777426659\n",
      "Iteration: 4320 lambda_k: 1 Loss: 0.008016476269660918\n",
      "Iteration: 4321 lambda_k: 1 Loss: 0.007992412008863222\n",
      "Iteration: 4322 lambda_k: 1 Loss: 0.007968354995536386\n",
      "Iteration: 4323 lambda_k: 1 Loss: 0.007944305233408626\n",
      "Iteration: 4324 lambda_k: 1 Loss: 0.007920262726274637\n",
      "Iteration: 4325 lambda_k: 1 Loss: 0.007896227477996242\n",
      "Iteration: 4326 lambda_k: 1 Loss: 0.007872199492503696\n",
      "Iteration: 4327 lambda_k: 1 Loss: 0.007848178773796545\n",
      "Iteration: 4328 lambda_k: 1 Loss: 0.007824165325944677\n",
      "Iteration: 4329 lambda_k: 1 Loss: 0.007800159153089377\n",
      "Iteration: 4330 lambda_k: 1 Loss: 0.007776160259444341\n",
      "Iteration: 4331 lambda_k: 1 Loss: 0.007752168649296819\n",
      "Iteration: 4332 lambda_k: 1 Loss: 0.007728184327008697\n",
      "Iteration: 4333 lambda_k: 1 Loss: 0.007704207297017719\n",
      "Iteration: 4334 lambda_k: 1 Loss: 0.007680237563838462\n",
      "Iteration: 4335 lambda_k: 1 Loss: 0.007656275132063639\n",
      "Iteration: 4336 lambda_k: 1 Loss: 0.007632320006365248\n",
      "Iteration: 4337 lambda_k: 1 Loss: 0.007608372191495673\n",
      "Iteration: 4338 lambda_k: 1 Loss: 0.007584431692289175\n",
      "Iteration: 4339 lambda_k: 1 Loss: 0.007560498513662813\n",
      "Iteration: 4340 lambda_k: 1 Loss: 0.007536572660617988\n",
      "Iteration: 4341 lambda_k: 1 Loss: 0.007512654138241584\n",
      "Iteration: 4342 lambda_k: 1 Loss: 0.007488742951707425\n",
      "Iteration: 4343 lambda_k: 1 Loss: 0.007464839106277593\n",
      "Iteration: 4344 lambda_k: 1 Loss: 0.007440942607303621\n",
      "Iteration: 4345 lambda_k: 1 Loss: 0.007417053460228189\n",
      "Iteration: 4346 lambda_k: 1 Loss: 0.007393171670586265\n",
      "Iteration: 4347 lambda_k: 1 Loss: 0.007369297244006843\n",
      "Iteration: 4348 lambda_k: 1 Loss: 0.007345430186214146\n",
      "Iteration: 4349 lambda_k: 1 Loss: 0.007321570503029385\n",
      "Iteration: 4350 lambda_k: 1 Loss: 0.0072977182003721025\n",
      "Iteration: 4351 lambda_k: 1 Loss: 0.007273873284261931\n",
      "Iteration: 4352 lambda_k: 1 Loss: 0.007250035760819939\n",
      "Iteration: 4353 lambda_k: 1 Loss: 0.007226205636270426\n",
      "Iteration: 4354 lambda_k: 1 Loss: 0.007202382916942619\n",
      "Iteration: 4355 lambda_k: 1 Loss: 0.007178567609272298\n",
      "Iteration: 4356 lambda_k: 1 Loss: 0.00715475971980352\n",
      "Iteration: 4357 lambda_k: 1 Loss: 0.007130959255190422\n",
      "Iteration: 4358 lambda_k: 1 Loss: 0.007107166222198867\n",
      "Iteration: 4359 lambda_k: 1 Loss: 0.0070833806277085\n",
      "Iteration: 4360 lambda_k: 1 Loss: 0.007059602478714414\n",
      "Iteration: 4361 lambda_k: 1 Loss: 0.00703583178232921\n",
      "Iteration: 4362 lambda_k: 1 Loss: 0.007012068545784794\n",
      "Iteration: 4363 lambda_k: 1 Loss: 0.0069883127764345\n",
      "Iteration: 4364 lambda_k: 1 Loss: 0.006964564481754984\n",
      "Iteration: 4365 lambda_k: 1 Loss: 0.006940823669348286\n",
      "Iteration: 4366 lambda_k: 1 Loss: 0.006917090346943987\n",
      "Iteration: 4367 lambda_k: 1 Loss: 0.006893364522401341\n",
      "Iteration: 4368 lambda_k: 1 Loss: 0.006869646203711422\n",
      "Iteration: 4369 lambda_k: 1 Loss: 0.006845935398999428\n",
      "Iteration: 4370 lambda_k: 1 Loss: 0.006822232116526876\n",
      "Iteration: 4371 lambda_k: 1 Loss: 0.006798536364693872\n",
      "Iteration: 4372 lambda_k: 1 Loss: 0.006774848152041664\n",
      "Iteration: 4373 lambda_k: 1 Loss: 0.00675116748725483\n",
      "Iteration: 4374 lambda_k: 1 Loss: 0.006727494379163935\n",
      "Iteration: 4375 lambda_k: 1 Loss: 0.006703828836747886\n",
      "Iteration: 4376 lambda_k: 1 Loss: 0.0066801708691366195\n",
      "Iteration: 4377 lambda_k: 1 Loss: 0.006656520485613774\n",
      "Iteration: 4378 lambda_k: 1 Loss: 0.006632877695619134\n",
      "Iteration: 4379 lambda_k: 1 Loss: 0.006609242508751521\n",
      "Iteration: 4380 lambda_k: 1 Loss: 0.00658561493477163\n",
      "Iteration: 4381 lambda_k: 1 Loss: 0.0065619949836047764\n",
      "Iteration: 4382 lambda_k: 1 Loss: 0.006538382665343742\n",
      "Iteration: 4383 lambda_k: 1 Loss: 0.0065147779902519996\n",
      "Iteration: 4384 lambda_k: 1 Loss: 0.006491180968766368\n",
      "Iteration: 4385 lambda_k: 1 Loss: 0.006467591611500408\n",
      "Iteration: 4386 lambda_k: 1 Loss: 0.006444009929247603\n",
      "Iteration: 4387 lambda_k: 1 Loss: 0.006420435932984299\n",
      "Iteration: 4388 lambda_k: 1 Loss: 0.0063968696338732815\n",
      "Iteration: 4389 lambda_k: 1 Loss: 0.0063733110432669686\n",
      "Iteration: 4390 lambda_k: 1 Loss: 0.00634976017271104\n",
      "Iteration: 4391 lambda_k: 1 Loss: 0.006326217033947708\n",
      "Iteration: 4392 lambda_k: 1 Loss: 0.006302681638919576\n",
      "Iteration: 4393 lambda_k: 1 Loss: 0.006279153999773207\n",
      "Iteration: 4394 lambda_k: 1 Loss: 0.0062556341288627\n",
      "Iteration: 4395 lambda_k: 1 Loss: 0.00623212203875382\n",
      "Iteration: 4396 lambda_k: 1 Loss: 0.00620861774222776\n",
      "Iteration: 4397 lambda_k: 1 Loss: 0.00618512125228521\n",
      "Iteration: 4398 lambda_k: 1 Loss: 0.00616163258215034\n",
      "Iteration: 4399 lambda_k: 1 Loss: 0.00613815174527521\n",
      "Iteration: 4400 lambda_k: 1 Loss: 0.006114678755343773\n",
      "Iteration: 4401 lambda_k: 1 Loss: 0.006091213626276418\n",
      "Iteration: 4402 lambda_k: 1 Loss: 0.006067756372234554\n",
      "Iteration: 4403 lambda_k: 1 Loss: 0.006044307007625013\n",
      "Iteration: 4404 lambda_k: 1 Loss: 0.006020865547104862\n",
      "Iteration: 4405 lambda_k: 1 Loss: 0.005997432005586021\n",
      "Iteration: 4406 lambda_k: 1 Loss: 0.005974006398240285\n",
      "Iteration: 4407 lambda_k: 1 Loss: 0.005950588740504447\n",
      "Iteration: 4408 lambda_k: 1 Loss: 0.0059271790480852375\n",
      "Iteration: 4409 lambda_k: 1 Loss: 0.005903777336964743\n",
      "Iteration: 4410 lambda_k: 1 Loss: 0.005880383623405627\n",
      "Iteration: 4411 lambda_k: 1 Loss: 0.0058569979239567445\n",
      "Iteration: 4412 lambda_k: 1 Loss: 0.0058336202554586\n",
      "Iteration: 4413 lambda_k: 1 Loss: 0.005810250635049374\n",
      "Iteration: 4414 lambda_k: 1 Loss: 0.005786889080170703\n",
      "Iteration: 4415 lambda_k: 1 Loss: 0.005763535608573618\n",
      "Iteration: 4416 lambda_k: 1 Loss: 0.005740190238324743\n",
      "Iteration: 4417 lambda_k: 1 Loss: 0.005716852987812853\n",
      "Iteration: 4418 lambda_k: 1 Loss: 0.005693523875755081\n",
      "Iteration: 4419 lambda_k: 1 Loss: 0.005670202921203661\n",
      "Iteration: 4420 lambda_k: 1 Loss: 0.0056468901435526775\n",
      "Iteration: 4421 lambda_k: 1 Loss: 0.005623585562545182\n",
      "Iteration: 4422 lambda_k: 1 Loss: 0.005600289198280138\n",
      "Iteration: 4423 lambda_k: 1 Loss: 0.005577001071219842\n",
      "Iteration: 4424 lambda_k: 1 Loss: 0.005553721202197446\n",
      "Iteration: 4425 lambda_k: 1 Loss: 0.005530449612424345\n",
      "Iteration: 4426 lambda_k: 1 Loss: 0.0055071863234984674\n",
      "Iteration: 4427 lambda_k: 1 Loss: 0.005483931357411963\n",
      "Iteration: 4428 lambda_k: 1 Loss: 0.005460684736559549\n",
      "Iteration: 4429 lambda_k: 1 Loss: 0.005437446483747194\n",
      "Iteration: 4430 lambda_k: 1 Loss: 0.005414216622200612\n",
      "Iteration: 4431 lambda_k: 1 Loss: 0.005390995175573997\n",
      "Iteration: 4432 lambda_k: 1 Loss: 0.0053677821679595115\n",
      "Iteration: 4433 lambda_k: 1 Loss: 0.005344577623896236\n",
      "Iteration: 4434 lambda_k: 1 Loss: 0.00532138156838021\n",
      "Iteration: 4435 lambda_k: 1 Loss: 0.005298194026873773\n",
      "Iteration: 4436 lambda_k: 1 Loss: 0.0052750150253159786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4437 lambda_k: 1 Loss: 0.005251844590132925\n",
      "Iteration: 4438 lambda_k: 1 Loss: 0.00522868274824823\n",
      "Iteration: 4439 lambda_k: 1 Loss: 0.005205529527094095\n",
      "Iteration: 4440 lambda_k: 1 Loss: 0.005182384954622301\n",
      "Iteration: 4441 lambda_k: 1 Loss: 0.005159249059315792\n",
      "Iteration: 4442 lambda_k: 1 Loss: 0.00513612187020051\n",
      "Iteration: 4443 lambda_k: 1 Loss: 0.005113003416857308\n",
      "Iteration: 4444 lambda_k: 1 Loss: 0.005089893729434466\n",
      "Iteration: 4445 lambda_k: 1 Loss: 0.005066792838660473\n",
      "Iteration: 4446 lambda_k: 1 Loss: 0.00504370077585712\n",
      "Iteration: 4447 lambda_k: 1 Loss: 0.0050206175729527\n",
      "Iteration: 4448 lambda_k: 1 Loss: 0.004997543262496057\n",
      "Iteration: 4449 lambda_k: 1 Loss: 0.00497447787767073\n",
      "Iteration: 4450 lambda_k: 1 Loss: 0.004951421452309464\n",
      "Iteration: 4451 lambda_k: 1 Loss: 0.004928374020909117\n",
      "Iteration: 4452 lambda_k: 1 Loss: 0.00490533561864608\n",
      "Iteration: 4453 lambda_k: 1 Loss: 0.004882306281392041\n",
      "Iteration: 4454 lambda_k: 1 Loss: 0.004859286045730417\n",
      "Iteration: 4455 lambda_k: 1 Loss: 0.004836274948972692\n",
      "Iteration: 4456 lambda_k: 1 Loss: 0.0048132730291758725\n",
      "Iteration: 4457 lambda_k: 1 Loss: 0.004790280325159879\n",
      "Iteration: 4458 lambda_k: 1 Loss: 0.004767296876525992\n",
      "Iteration: 4459 lambda_k: 1 Loss: 0.004744322723675132\n",
      "Iteration: 4460 lambda_k: 1 Loss: 0.00472135790782729\n",
      "Iteration: 4461 lambda_k: 1 Loss: 0.004698402471041183\n",
      "Iteration: 4462 lambda_k: 1 Loss: 0.004675456456234646\n",
      "Iteration: 4463 lambda_k: 1 Loss: 0.004652519907205327\n",
      "Iteration: 4464 lambda_k: 1 Loss: 0.004629592868652303\n",
      "Iteration: 4465 lambda_k: 1 Loss: 0.004606675386198078\n",
      "Iteration: 4466 lambda_k: 1 Loss: 0.004583767506411684\n",
      "Iteration: 4467 lambda_k: 1 Loss: 0.004560869276831534\n",
      "Iteration: 4468 lambda_k: 1 Loss: 0.004537980745990185\n",
      "Iteration: 4469 lambda_k: 1 Loss: 0.0045151019634387555\n",
      "Iteration: 4470 lambda_k: 1 Loss: 0.0044922329797725345\n",
      "Iteration: 4471 lambda_k: 1 Loss: 0.0044693738466575475\n",
      "Iteration: 4472 lambda_k: 1 Loss: 0.004446524616857653\n",
      "Iteration: 4473 lambda_k: 1 Loss: 0.004423685344262261\n",
      "Iteration: 4474 lambda_k: 1 Loss: 0.004400856083915374\n",
      "Iteration: 4475 lambda_k: 1 Loss: 0.004378036892045327\n",
      "Iteration: 4476 lambda_k: 1 Loss: 0.004355227826095285\n",
      "Iteration: 4477 lambda_k: 1 Loss: 0.004332428944754728\n",
      "Iteration: 4478 lambda_k: 1 Loss: 0.004309640307992127\n",
      "Iteration: 4479 lambda_k: 1 Loss: 0.004286861977088482\n",
      "Iteration: 4480 lambda_k: 1 Loss: 0.004264094014671733\n",
      "Iteration: 4481 lambda_k: 1 Loss: 0.004241336484752693\n",
      "Iteration: 4482 lambda_k: 1 Loss: 0.004218589452761697\n",
      "Iteration: 4483 lambda_k: 1 Loss: 0.004195852985586601\n",
      "Iteration: 4484 lambda_k: 1 Loss: 0.004173127151612045\n",
      "Iteration: 4485 lambda_k: 1 Loss: 0.004150412020759774\n",
      "Iteration: 4486 lambda_k: 1 Loss: 0.004127707664530559\n",
      "Iteration: 4487 lambda_k: 1 Loss: 0.004105014156047195\n",
      "Iteration: 4488 lambda_k: 1 Loss: 0.004082331570098996\n",
      "Iteration: 4489 lambda_k: 1 Loss: 0.004059659983187822\n",
      "Iteration: 4490 lambda_k: 1 Loss: 0.004036999473575546\n",
      "Iteration: 4491 lambda_k: 1 Loss: 0.004014350121333156\n",
      "Iteration: 4492 lambda_k: 1 Loss: 0.003991712008391428\n",
      "Iteration: 4493 lambda_k: 1 Loss: 0.003969085218593163\n",
      "Iteration: 4494 lambda_k: 1 Loss: 0.003946469837747487\n",
      "Iteration: 4495 lambda_k: 1 Loss: 0.003923865953685857\n",
      "Iteration: 4496 lambda_k: 1 Loss: 0.003901273656319723\n",
      "Iteration: 4497 lambda_k: 1 Loss: 0.0038786930377006598\n",
      "Iteration: 4498 lambda_k: 1 Loss: 0.003856124192082061\n",
      "Iteration: 4499 lambda_k: 1 Loss: 0.003833567215983305\n",
      "Iteration: 4500 lambda_k: 1 Loss: 0.0038110222082559553\n",
      "Iteration: 4501 lambda_k: 1 Loss: 0.003788489270152412\n",
      "Iteration: 4502 lambda_k: 1 Loss: 0.003765968505396849\n",
      "Iteration: 4503 lambda_k: 1 Loss: 0.0037434600202586503\n",
      "Iteration: 4504 lambda_k: 1 Loss: 0.003720963923628749\n",
      "Iteration: 4505 lambda_k: 1 Loss: 0.0036984803270981616\n",
      "Iteration: 4506 lambda_k: 1 Loss: 0.0036760093450399177\n",
      "Iteration: 4507 lambda_k: 1 Loss: 0.003653551094693445\n",
      "Iteration: 4508 lambda_k: 1 Loss: 0.003631105696252397\n",
      "Iteration: 4509 lambda_k: 1 Loss: 0.003608673272955454\n",
      "Iteration: 4510 lambda_k: 1 Loss: 0.0035862539511806683\n",
      "Iteration: 4511 lambda_k: 1 Loss: 0.003563847860543056\n",
      "Iteration: 4512 lambda_k: 1 Loss: 0.0035414551339959027\n",
      "Iteration: 4513 lambda_k: 1 Loss: 0.0035190759079360002\n",
      "Iteration: 4514 lambda_k: 1 Loss: 0.003496710322312766\n",
      "Iteration: 4515 lambda_k: 1 Loss: 0.00347435852074141\n",
      "Iteration: 4516 lambda_k: 1 Loss: 0.003452020650620377\n",
      "Iteration: 4517 lambda_k: 1 Loss: 0.003429696863253578\n",
      "Iteration: 4518 lambda_k: 1 Loss: 0.0034073873139770228\n",
      "Iteration: 4519 lambda_k: 1 Loss: 0.003385092162290497\n",
      "Iteration: 4520 lambda_k: 1 Loss: 0.0033628115719943263\n",
      "Iteration: 4521 lambda_k: 1 Loss: 0.0033405457113318137\n",
      "Iteration: 4522 lambda_k: 1 Loss: 0.0033182947531366887\n",
      "Iteration: 4523 lambda_k: 1 Loss: 0.0032960588749871332\n",
      "Iteration: 4524 lambda_k: 1 Loss: 0.003273838259365379\n",
      "Iteration: 4525 lambda_k: 1 Loss: 0.003251633093824106\n",
      "Iteration: 4526 lambda_k: 1 Loss: 0.003229443571159693\n",
      "Iteration: 4527 lambda_k: 1 Loss: 0.003207269889592065\n",
      "Iteration: 4528 lambda_k: 1 Loss: 0.003185112252952311\n",
      "Iteration: 4529 lambda_k: 1 Loss: 0.0031629708708781315\n",
      "Iteration: 4530 lambda_k: 1 Loss: 0.003140845959017272\n",
      "Iteration: 4531 lambda_k: 1 Loss: 0.0031187377392392344\n",
      "Iteration: 4532 lambda_k: 1 Loss: 0.0030966464398566556\n",
      "Iteration: 4533 lambda_k: 1 Loss: 0.003074572295855508\n",
      "Iteration: 4534 lambda_k: 1 Loss: 0.0030525155491352776\n",
      "Iteration: 4535 lambda_k: 1 Loss: 0.0030304764487597722\n",
      "Iteration: 4536 lambda_k: 1 Loss: 0.0030084552512186346\n",
      "Iteration: 4537 lambda_k: 1 Loss: 0.002986452220700223\n",
      "Iteration: 4538 lambda_k: 1 Loss: 0.0029644676293768984\n",
      "Iteration: 4539 lambda_k: 1 Loss: 0.0029425017577026603\n",
      "Iteration: 4540 lambda_k: 1 Loss: 0.0029205548947244097\n",
      "Iteration: 4541 lambda_k: 1 Loss: 0.002898627338406923\n",
      "Iteration: 4542 lambda_k: 1 Loss: 0.0028767193959732275\n",
      "Iteration: 4543 lambda_k: 1 Loss: 0.0028548313842598045\n",
      "Iteration: 4544 lambda_k: 1 Loss: 0.002832963630089039\n",
      "Iteration: 4545 lambda_k: 1 Loss: 0.0028111164706588775\n",
      "Iteration: 4546 lambda_k: 1 Loss: 0.0027892902539504326\n",
      "Iteration: 4547 lambda_k: 1 Loss: 0.002767485339155891\n",
      "Iteration: 4548 lambda_k: 1 Loss: 0.002745702097126289\n",
      "Iteration: 4549 lambda_k: 1 Loss: 0.0027239409108419234\n",
      "Iteration: 4550 lambda_k: 1 Loss: 0.0027022021759058024\n",
      "Iteration: 4551 lambda_k: 1 Loss: 0.0026804863010626567\n",
      "Iteration: 4552 lambda_k: 1 Loss: 0.0026587937087447994\n",
      "Iteration: 4553 lambda_k: 1 Loss: 0.0026371248356485545\n",
      "Iteration: 4554 lambda_k: 1 Loss: 0.002615480133344658\n",
      "Iteration: 4555 lambda_k: 1 Loss: 0.0025938600689290837\n",
      "Iteration: 4556 lambda_k: 1 Loss: 0.002572265125726842\n",
      "Iteration: 4557 lambda_k: 1 Loss: 0.002550695804074507\n",
      "Iteration: 4558 lambda_k: 1 Loss: 0.002529152622248238\n",
      "Iteration: 4559 lambda_k: 1 Loss: 0.002507636117749113\n",
      "Iteration: 4560 lambda_k: 1 Loss: 0.002486146850129312\n",
      "Iteration: 4561 lambda_k: 1 Loss: 0.0024646853605676403\n",
      "Iteration: 4562 lambda_k: 1 Loss: 0.0024432523924720543\n",
      "Iteration: 4563 lambda_k: 1 Loss: 0.0024239821860861477\n",
      "Iteration: 4564 lambda_k: 1 Loss: 0.002405735895057713\n",
      "Iteration: 4565 lambda_k: 1 Loss: 0.0023881073919855295\n",
      "Iteration: 4566 lambda_k: 1 Loss: 0.0023710158892364347\n",
      "Iteration: 4567 lambda_k: 1 Loss: 0.0023543382743284515\n",
      "Iteration: 4568 lambda_k: 1 Loss: 0.00233797392302076\n",
      "Iteration: 4569 lambda_k: 1 Loss: 0.0023218716098666256\n",
      "Iteration: 4570 lambda_k: 1 Loss: 0.0023060240522660715\n",
      "Iteration: 4571 lambda_k: 1 Loss: 0.002290447084345167\n",
      "Iteration: 4572 lambda_k: 1 Loss: 0.0022751594210203706\n",
      "Iteration: 4573 lambda_k: 1 Loss: 0.002260171070496189\n",
      "Iteration: 4574 lambda_k: 1 Loss: 0.0022454808955451222\n",
      "Iteration: 4575 lambda_k: 1 Loss: 0.002231079860002348\n",
      "Iteration: 4576 lambda_k: 1 Loss: 0.002216955921925016\n",
      "Iteration: 4577 lambda_k: 1 Loss: 0.0022030978409708145\n",
      "Iteration: 4578 lambda_k: 1 Loss: 0.002189496914422366\n",
      "Iteration: 4579 lambda_k: 1 Loss: 0.002176146950762449\n",
      "Iteration: 4580 lambda_k: 1 Loss: 0.002163043337884558\n",
      "Iteration: 4581 lambda_k: 1 Loss: 0.0021501820058521253\n",
      "Iteration: 4582 lambda_k: 1 Loss: 0.0021375587384538457\n",
      "Iteration: 4583 lambda_k: 1 Loss: 0.0021251689317469172\n",
      "Iteration: 4584 lambda_k: 1 Loss: 0.002113007676308388\n",
      "Iteration: 4585 lambda_k: 1 Loss: 0.0021010699735414537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4586 lambda_k: 1 Loss: 0.0020893509354890467\n",
      "Iteration: 4587 lambda_k: 1 Loss: 0.0020778458962149348\n",
      "Iteration: 4588 lambda_k: 1 Loss: 0.002066550431658127\n",
      "Iteration: 4589 lambda_k: 1 Loss: 0.00205546032202434\n",
      "Iteration: 4590 lambda_k: 1 Loss: 0.0020445714969809458\n",
      "Iteration: 4591 lambda_k: 1 Loss: 0.0020338799915024953\n",
      "Iteration: 4592 lambda_k: 1 Loss: 0.0020233819769540514\n",
      "Iteration: 4593 lambda_k: 1 Loss: 0.0020130736088872134\n",
      "Iteration: 4594 lambda_k: 1 Loss: 0.002002951154481529\n",
      "Iteration: 4595 lambda_k: 1 Loss: 0.0019930109693050177\n",
      "Iteration: 4596 lambda_k: 1 Loss: 0.00198324951277295\n",
      "Iteration: 4597 lambda_k: 1 Loss: 0.0019736633450153423\n",
      "Iteration: 4598 lambda_k: 1 Loss: 0.0019642491175859496\n",
      "Iteration: 4599 lambda_k: 1 Loss: 0.001955003564425514\n",
      "Iteration: 4600 lambda_k: 1 Loss: 0.0019459234955602988\n",
      "Iteration: 4601 lambda_k: 1 Loss: 0.0019370057935552108\n",
      "Iteration: 4602 lambda_k: 1 Loss: 0.0019282474116955458\n",
      "Iteration: 4603 lambda_k: 1 Loss: 0.0019196453727714123\n",
      "Iteration: 4604 lambda_k: 1 Loss: 0.0019111967677045104\n",
      "Iteration: 4605 lambda_k: 1 Loss: 0.0019028987537265535\n",
      "Iteration: 4606 lambda_k: 1 Loss: 0.0018947485521742542\n",
      "Iteration: 4607 lambda_k: 1 Loss: 0.0018867434461241471\n",
      "Iteration: 4608 lambda_k: 1 Loss: 0.0018788807710606974\n",
      "Iteration: 4609 lambda_k: 1 Loss: 0.0018711579208220488\n",
      "Iteration: 4610 lambda_k: 1 Loss: 0.001863572363433217\n",
      "Iteration: 4611 lambda_k: 1 Loss: 0.001856121611115266\n",
      "Iteration: 4612 lambda_k: 1 Loss: 0.0018488032259055182\n",
      "Iteration: 4613 lambda_k: 1 Loss: 0.0018416148179327817\n",
      "Iteration: 4614 lambda_k: 1 Loss: 0.001834554045585817\n",
      "Iteration: 4615 lambda_k: 1 Loss: 0.0018276186152050215\n",
      "Iteration: 4616 lambda_k: 1 Loss: 0.0018208062799924527\n",
      "Iteration: 4617 lambda_k: 1 Loss: 0.0018141148384750979\n",
      "Iteration: 4618 lambda_k: 1 Loss: 0.0018075421328781938\n",
      "Iteration: 4619 lambda_k: 1 Loss: 0.0018010860476166016\n",
      "Iteration: 4620 lambda_k: 1 Loss: 0.0017947445079779674\n",
      "Iteration: 4621 lambda_k: 1 Loss: 0.0017885154789833383\n",
      "Iteration: 4622 lambda_k: 1 Loss: 0.00178239696436947\n",
      "Iteration: 4623 lambda_k: 1 Loss: 0.0017763870056341587\n",
      "Iteration: 4624 lambda_k: 1 Loss: 0.0017704836811060244\n",
      "Iteration: 4625 lambda_k: 1 Loss: 0.001764685105024215\n",
      "Iteration: 4626 lambda_k: 1 Loss: 0.0017589894266328771\n",
      "Iteration: 4627 lambda_k: 1 Loss: 0.0017533948293020829\n",
      "Iteration: 4628 lambda_k: 1 Loss: 0.0017478995296841679\n",
      "Iteration: 4629 lambda_k: 1 Loss: 0.0017425017769095018\n",
      "Iteration: 4630 lambda_k: 1 Loss: 0.0017371998518215066\n",
      "Iteration: 4631 lambda_k: 1 Loss: 0.0017319920662462306\n",
      "Iteration: 4632 lambda_k: 1 Loss: 0.0017268767622919197\n",
      "Iteration: 4633 lambda_k: 1 Loss: 0.0017218523116759945\n",
      "Iteration: 4634 lambda_k: 1 Loss: 0.0017169171150758673\n",
      "Iteration: 4635 lambda_k: 1 Loss: 0.001712069601503333\n",
      "Iteration: 4636 lambda_k: 1 Loss: 0.0017073082277011817\n",
      "Iteration: 4637 lambda_k: 1 Loss: 0.001702631477561752\n",
      "Iteration: 4638 lambda_k: 1 Loss: 0.001698037861566447\n",
      "Iteration: 4639 lambda_k: 1 Loss: 0.0016935259162452455\n",
      "Iteration: 4640 lambda_k: 1 Loss: 0.001689094203654953\n",
      "Iteration: 4641 lambda_k: 1 Loss: 0.0016847413108754597\n",
      "Iteration: 4642 lambda_k: 1 Loss: 0.001680465849522377\n",
      "Iteration: 4643 lambda_k: 1 Loss: 0.0016762664552756946\n",
      "Iteration: 4644 lambda_k: 1 Loss: 0.0016721417874232258\n",
      "Iteration: 4645 lambda_k: 1 Loss: 0.0016680905284180566\n",
      "Iteration: 4646 lambda_k: 1 Loss: 0.0016641113834495371\n",
      "Iteration: 4647 lambda_k: 1 Loss: 0.001660203080026863\n",
      "Iteration: 4648 lambda_k: 1 Loss: 0.0016563643675749304\n",
      "Iteration: 4649 lambda_k: 1 Loss: 0.001652594017041085\n",
      "Iteration: 4650 lambda_k: 1 Loss: 0.0016488908205131963\n",
      "Iteration: 4651 lambda_k: 1 Loss: 0.0016452535908478378\n",
      "Iteration: 4652 lambda_k: 1 Loss: 0.001641681161308002\n",
      "Iteration: 4653 lambda_k: 1 Loss: 0.0016381723852102327\n",
      "Iteration: 4654 lambda_k: 1 Loss: 0.0016347261355803369\n",
      "Iteration: 4655 lambda_k: 1 Loss: 0.0016313413048175728\n",
      "Iteration: 4656 lambda_k: 1 Loss: 0.0016280168043665663\n",
      "Iteration: 4657 lambda_k: 1 Loss: 0.0016247515643970688\n",
      "Iteration: 4658 lambda_k: 1 Loss: 0.001621544533490273\n",
      "Iteration: 4659 lambda_k: 1 Loss: 0.0016183946783234664\n",
      "Iteration: 4660 lambda_k: 1 Loss: 0.0016153009833869104\n",
      "Iteration: 4661 lambda_k: 1 Loss: 0.001612262450683951\n",
      "Iteration: 4662 lambda_k: 1 Loss: 0.0016092780994416606\n",
      "Iteration: 4663 lambda_k: 1 Loss: 0.0016063469658287512\n",
      "Iteration: 4664 lambda_k: 1 Loss: 0.0016034681026782867\n",
      "Iteration: 4665 lambda_k: 1 Loss: 0.001600640579215733\n",
      "Iteration: 4666 lambda_k: 1 Loss: 0.0015978634807923089\n",
      "Iteration: 4667 lambda_k: 1 Loss: 0.0015951359086236502\n",
      "Iteration: 4668 lambda_k: 1 Loss: 0.001592456979532852\n",
      "Iteration: 4669 lambda_k: 1 Loss: 0.0015898258256978018\n",
      "Iteration: 4670 lambda_k: 1 Loss: 0.0015872415944028085\n",
      "Iteration: 4671 lambda_k: 1 Loss: 0.001584703447794247\n",
      "Iteration: 4672 lambda_k: 1 Loss: 0.001582210562640077\n",
      "Iteration: 4673 lambda_k: 1 Loss: 0.0015797621300933716\n",
      "Iteration: 4674 lambda_k: 1 Loss: 0.0015773573554594804\n",
      "Iteration: 4675 lambda_k: 1 Loss: 0.0015749954579669554\n",
      "Iteration: 4676 lambda_k: 1 Loss: 0.001572675670541957\n",
      "Iteration: 4677 lambda_k: 1 Loss: 0.0015703972395859978\n",
      "Iteration: 4678 lambda_k: 1 Loss: 0.0015681594247572367\n",
      "Iteration: 4679 lambda_k: 1 Loss: 0.0015659614987547513\n",
      "Iteration: 4680 lambda_k: 1 Loss: 0.001563802747106324\n",
      "Iteration: 4681 lambda_k: 1 Loss: 0.0015616824679588602\n",
      "Iteration: 4682 lambda_k: 1 Loss: 0.0015595999718722054\n",
      "Iteration: 4683 lambda_k: 1 Loss: 0.0015575545816159586\n",
      "Iteration: 4684 lambda_k: 1 Loss: 0.001555545631968895\n",
      "Iteration: 4685 lambda_k: 1 Loss: 0.0015535724695214675\n",
      "Iteration: 4686 lambda_k: 1 Loss: 0.0015516347028815233\n",
      "Iteration: 4687 lambda_k: 1 Loss: 0.0015497313812493842\n",
      "Iteration: 4688 lambda_k: 1 Loss: 0.0015478619457637156\n",
      "Iteration: 4689 lambda_k: 1 Loss: 0.0015460257639587397\n",
      "Iteration: 4690 lambda_k: 1 Loss: 0.0015442222393419313\n",
      "Iteration: 4691 lambda_k: 1 Loss: 0.0015424507993717928\n",
      "Iteration: 4692 lambda_k: 1 Loss: 0.0015407108836753226\n",
      "Iteration: 4693 lambda_k: 1 Loss: 0.001539001937227103\n",
      "Iteration: 4694 lambda_k: 1 Loss: 0.001537323408624585\n",
      "Iteration: 4695 lambda_k: 1 Loss: 0.001535674751564822\n",
      "Iteration: 4696 lambda_k: 1 Loss: 0.001534055427374644\n",
      "Iteration: 4697 lambda_k: 1 Loss: 0.0015324649070924802\n",
      "Iteration: 4698 lambda_k: 1 Loss: 0.0015309026724705712\n",
      "Iteration: 4699 lambda_k: 1 Loss: 0.001529368215958477\n",
      "Iteration: 4700 lambda_k: 1 Loss: 0.0015278610400862212\n",
      "Iteration: 4701 lambda_k: 1 Loss: 0.0015263806567053944\n",
      "Iteration: 4702 lambda_k: 1 Loss: 0.0015249265863907415\n",
      "Iteration: 4703 lambda_k: 1 Loss: 0.0015234983581026106\n",
      "Iteration: 4704 lambda_k: 1 Loss: 0.0015220955090603347\n",
      "Iteration: 4705 lambda_k: 1 Loss: 0.0015207175847188426\n",
      "Iteration: 4706 lambda_k: 1 Loss: 0.0015193641387500954\n",
      "Iteration: 4707 lambda_k: 1 Loss: 0.0015180347329779302\n",
      "Iteration: 4708 lambda_k: 1 Loss: 0.0015167288837389604\n",
      "Iteration: 4709 lambda_k: 1 Loss: 0.0015154462217474515\n",
      "Iteration: 4710 lambda_k: 1 Loss: 0.0015141863407432553\n",
      "Iteration: 4711 lambda_k: 1 Loss: 0.0015129488383100298\n",
      "Iteration: 4712 lambda_k: 1 Loss: 0.0015117333137294779\n",
      "Iteration: 4713 lambda_k: 1 Loss: 0.0015105393703567888\n",
      "Iteration: 4714 lambda_k: 1 Loss: 0.0015093666185983417\n",
      "Iteration: 4715 lambda_k: 1 Loss: 0.0015082146774658418\n",
      "Iteration: 4716 lambda_k: 1 Loss: 0.0015070831747110494\n",
      "Iteration: 4717 lambda_k: 1 Loss: 0.0015059717461619719\n",
      "Iteration: 4718 lambda_k: 1 Loss: 0.0015048800348530203\n",
      "Iteration: 4719 lambda_k: 1 Loss: 0.0015038076903097998\n",
      "Iteration: 4720 lambda_k: 1 Loss: 0.0015027543681276538\n",
      "Iteration: 4721 lambda_k: 1 Loss: 0.0015017197298234778\n",
      "Iteration: 4722 lambda_k: 1 Loss: 0.0015007034428522\n",
      "Iteration: 4723 lambda_k: 1 Loss: 0.0014997051806654705\n",
      "Iteration: 4724 lambda_k: 1 Loss: 0.001498724622726851\n",
      "Iteration: 4725 lambda_k: 1 Loss: 0.001497761454454343\n",
      "Iteration: 4726 lambda_k: 1 Loss: 0.0014968153671039103\n",
      "Iteration: 4727 lambda_k: 1 Loss: 0.00149588605762632\n",
      "Iteration: 4728 lambda_k: 1 Loss: 0.0014949732285257885\n",
      "Iteration: 4729 lambda_k: 1 Loss: 0.0014940765877356293\n",
      "Iteration: 4730 lambda_k: 1 Loss: 0.001493195848512481\n",
      "Iteration: 4731 lambda_k: 1 Loss: 0.0014923307293434995\n",
      "Iteration: 4732 lambda_k: 1 Loss: 0.001491480953858369\n",
      "Iteration: 4733 lambda_k: 1 Loss: 0.0014906462507414818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4734 lambda_k: 1 Loss: 0.0014898263536417614\n",
      "Iteration: 4735 lambda_k: 1 Loss: 0.0014890210010811654\n",
      "Iteration: 4736 lambda_k: 1 Loss: 0.001488229936362791\n",
      "Iteration: 4737 lambda_k: 1 Loss: 0.001487452907480361\n",
      "Iteration: 4738 lambda_k: 1 Loss: 0.0014866896670298847\n",
      "Iteration: 4739 lambda_k: 1 Loss: 0.0014859399721235801\n",
      "Iteration: 4740 lambda_k: 1 Loss: 0.0014852035843059045\n",
      "Iteration: 4741 lambda_k: 1 Loss: 0.0014844802694713654\n",
      "Iteration: 4742 lambda_k: 1 Loss: 0.0014837697977837293\n",
      "Iteration: 4743 lambda_k: 1 Loss: 0.0014830719435966462\n",
      "Iteration: 4744 lambda_k: 1 Loss: 0.0014823864853753552\n",
      "Iteration: 4745 lambda_k: 1 Loss: 0.001481713205619806\n",
      "Iteration: 4746 lambda_k: 1 Loss: 0.0014810518907891266\n",
      "Iteration: 4747 lambda_k: 1 Loss: 0.0014804023312274769\n",
      "Iteration: 4748 lambda_k: 1 Loss: 0.0014797643210911749\n",
      "Iteration: 4749 lambda_k: 1 Loss: 0.0014791376582773757\n",
      "Iteration: 4750 lambda_k: 1 Loss: 0.001478522144353715\n",
      "Iteration: 4751 lambda_k: 1 Loss: 0.0014779175844896908\n",
      "Iteration: 4752 lambda_k: 1 Loss: 0.0014773237873889746\n",
      "Iteration: 4753 lambda_k: 1 Loss: 0.001476740565223044\n",
      "Iteration: 4754 lambda_k: 1 Loss: 0.001476167733566097\n",
      "Iteration: 4755 lambda_k: 1 Loss: 0.0014756051113310957\n",
      "Iteration: 4756 lambda_k: 1 Loss: 0.0014750525207068823\n",
      "Iteration: 4757 lambda_k: 1 Loss: 0.0014745097870968228\n",
      "Iteration: 4758 lambda_k: 1 Loss: 0.0014739767390582177\n",
      "Iteration: 4759 lambda_k: 1 Loss: 0.001473453176883045\n",
      "Iteration: 4760 lambda_k: 1 Loss: 0.0014729389809281814\n",
      "Iteration: 4761 lambda_k: 1 Loss: 0.0014724339760213252\n",
      "Iteration: 4762 lambda_k: 1 Loss: 0.0014719380052387817\n",
      "Iteration: 4763 lambda_k: 1 Loss: 0.0014714509117819835\n",
      "Iteration: 4764 lambda_k: 1 Loss: 0.0014709725403864262\n",
      "Iteration: 4765 lambda_k: 1 Loss: 0.0014705027384674372\n",
      "Iteration: 4766 lambda_k: 1 Loss: 0.0014700413567073423\n",
      "Iteration: 4767 lambda_k: 1 Loss: 0.0014695882491305725\n",
      "Iteration: 4768 lambda_k: 1 Loss: 0.0014691432728678668\n",
      "Iteration: 4769 lambda_k: 1 Loss: 0.0014687062878240426\n",
      "Iteration: 4770 lambda_k: 1 Loss: 0.0014682771563995045\n",
      "Iteration: 4771 lambda_k: 1 Loss: 0.0014678557433266642\n",
      "Iteration: 4772 lambda_k: 1 Loss: 0.0014674419156125616\n",
      "Iteration: 4773 lambda_k: 1 Loss: 0.0014670355425430383\n",
      "Iteration: 4774 lambda_k: 1 Loss: 0.001466636495701094\n",
      "Iteration: 4775 lambda_k: 1 Loss: 0.0014662446489685013\n",
      "Iteration: 4776 lambda_k: 1 Loss: 0.001465859878501328\n",
      "Iteration: 4777 lambda_k: 1 Loss: 0.0014654820626846569\n",
      "Iteration: 4778 lambda_k: 1 Loss: 0.001465111082078204\n",
      "Iteration: 4779 lambda_k: 1 Loss: 0.0014647468193627704\n",
      "Iteration: 4780 lambda_k: 1 Loss: 0.0014643891592923733\n",
      "Iteration: 4781 lambda_k: 1 Loss: 0.0014640379886535803\n",
      "Iteration: 4782 lambda_k: 1 Loss: 0.0014636931962287163\n",
      "Iteration: 4783 lambda_k: 1 Loss: 0.0014633546727619126\n",
      "Iteration: 4784 lambda_k: 1 Loss: 0.0014630223109246808\n",
      "Iteration: 4785 lambda_k: 1 Loss: 0.0014626960052814772\n",
      "Iteration: 4786 lambda_k: 1 Loss: 0.0014623756522544054\n",
      "Iteration: 4787 lambda_k: 1 Loss: 0.0014620611500881589\n",
      "Iteration: 4788 lambda_k: 1 Loss: 0.001461752398815565\n",
      "Iteration: 4789 lambda_k: 1 Loss: 0.0014614493002236533\n",
      "Iteration: 4790 lambda_k: 1 Loss: 0.001461151757821101\n",
      "Iteration: 4791 lambda_k: 1 Loss: 0.0014608596768062117\n",
      "Iteration: 4792 lambda_k: 1 Loss: 0.0014605729640356137\n",
      "Iteration: 4793 lambda_k: 1 Loss: 0.0014602915279937172\n",
      "Iteration: 4794 lambda_k: 1 Loss: 0.0014600152787627287\n",
      "Iteration: 4795 lambda_k: 1 Loss: 0.0014597441279928835\n",
      "Iteration: 4796 lambda_k: 1 Loss: 0.0014594779888737082\n",
      "Iteration: 4797 lambda_k: 1 Loss: 0.0014592167761054112\n",
      "Iteration: 4798 lambda_k: 1 Loss: 0.0014589604058710692\n",
      "Iteration: 4799 lambda_k: 1 Loss: 0.001458708795809239\n",
      "Iteration: 4800 lambda_k: 1 Loss: 0.0014584618649871504\n",
      "Iteration: 4801 lambda_k: 1 Loss: 0.0014582195338744802\n",
      "Iteration: 4802 lambda_k: 1 Loss: 0.00145798172431756\n",
      "Iteration: 4803 lambda_k: 1 Loss: 0.0014577483595141015\n",
      "Iteration: 4804 lambda_k: 1 Loss: 0.0014575193639884806\n",
      "Iteration: 4805 lambda_k: 1 Loss: 0.0014572946635674098\n",
      "Iteration: 4806 lambda_k: 1 Loss: 0.0014570741853560912\n",
      "Iteration: 4807 lambda_k: 1 Loss: 0.001456857857714884\n",
      "Iteration: 4808 lambda_k: 1 Loss: 0.0014566456102363726\n",
      "Iteration: 4809 lambda_k: 1 Loss: 0.0014564373737229784\n",
      "Iteration: 4810 lambda_k: 1 Loss: 0.0014562330801647701\n",
      "Iteration: 4811 lambda_k: 1 Loss: 0.0014560326627180696\n",
      "Iteration: 4812 lambda_k: 1 Loss: 0.0014558360556841058\n",
      "Iteration: 4813 lambda_k: 1 Loss: 0.0014556431944883533\n",
      "Iteration: 4814 lambda_k: 1 Loss: 0.001455454015660073\n",
      "Iteration: 4815 lambda_k: 1 Loss: 0.0014552684568123065\n",
      "Iteration: 4816 lambda_k: 1 Loss: 0.001455086456622436\n",
      "Iteration: 4817 lambda_k: 1 Loss: 0.001454907954812825\n",
      "Iteration: 4818 lambda_k: 1 Loss: 0.0014547328921320777\n",
      "Iteration: 4819 lambda_k: 1 Loss: 0.0014545612103364753\n",
      "Iteration: 4820 lambda_k: 1 Loss: 0.00145439285217186\n",
      "Iteration: 4821 lambda_k: 1 Loss: 0.0014542277613559516\n",
      "Iteration: 4822 lambda_k: 1 Loss: 0.0014540658825607788\n",
      "Iteration: 4823 lambda_k: 1 Loss: 0.0014539071613956375\n",
      "Iteration: 4824 lambda_k: 1 Loss: 0.0014537515443904055\n",
      "Iteration: 4825 lambda_k: 1 Loss: 0.0014535989789789309\n",
      "Iteration: 4826 lambda_k: 1 Loss: 0.0014534494134829782\n",
      "Iteration: 4827 lambda_k: 1 Loss: 0.0014533027970964937\n",
      "Iteration: 4828 lambda_k: 1 Loss: 0.0014531590798699283\n",
      "Iteration: 4829 lambda_k: 1 Loss: 0.0014530182126950603\n",
      "Iteration: 4830 lambda_k: 1 Loss: 0.0014528801472901067\n",
      "Iteration: 4831 lambda_k: 1 Loss: 0.0014527448361850198\n",
      "Iteration: 4832 lambda_k: 1 Loss: 0.0014526122327071255\n",
      "Iteration: 4833 lambda_k: 1 Loss: 0.0014524822909670499\n",
      "Iteration: 4834 lambda_k: 1 Loss: 0.001452354965844919\n",
      "Iteration: 4835 lambda_k: 1 Loss: 0.001452230212976675\n",
      "Iteration: 4836 lambda_k: 1 Loss: 0.0014521079887349375\n",
      "Iteration: 4837 lambda_k: 1 Loss: 0.0014519882502255592\n",
      "Iteration: 4838 lambda_k: 1 Loss: 0.001451870955276342\n",
      "Iteration: 4839 lambda_k: 1 Loss: 0.0014517560624173083\n",
      "Iteration: 4840 lambda_k: 1 Loss: 0.0014516435308704846\n",
      "Iteration: 4841 lambda_k: 1 Loss: 0.0014515333205373136\n",
      "Iteration: 4842 lambda_k: 1 Loss: 0.0014514253919865289\n",
      "Iteration: 4843 lambda_k: 1 Loss: 0.0014513197064425106\n",
      "Iteration: 4844 lambda_k: 1 Loss: 0.0014512162257741524\n",
      "Iteration: 4845 lambda_k: 1 Loss: 0.0014511149124839169\n",
      "Iteration: 4846 lambda_k: 1 Loss: 0.001451015729696838\n",
      "Iteration: 4847 lambda_k: 1 Loss: 0.0014509186411497679\n",
      "Iteration: 4848 lambda_k: 1 Loss: 0.0014508236111807338\n",
      "Iteration: 4849 lambda_k: 1 Loss: 0.0014507306047185837\n",
      "Iteration: 4850 lambda_k: 1 Loss: 0.0014506395872726353\n",
      "Iteration: 4851 lambda_k: 1 Loss: 0.001450550524922705\n",
      "Iteration: 4852 lambda_k: 1 Loss: 0.0014504633843092079\n",
      "Iteration: 4853 lambda_k: 1 Loss: 0.0014503781326236022\n",
      "Iteration: 4854 lambda_k: 1 Loss: 0.0014502947375988437\n",
      "Iteration: 4855 lambda_k: 1 Loss: 0.0014502131675002273\n",
      "Iteration: 4856 lambda_k: 1 Loss: 0.0014501333911160447\n",
      "Iteration: 4857 lambda_k: 1 Loss: 0.001450055377748787\n",
      "Iteration: 4858 lambda_k: 1 Loss: 0.0014499790972064006\n",
      "Iteration: 4859 lambda_k: 1 Loss: 0.0014499045197935954\n",
      "Iteration: 4860 lambda_k: 1 Loss: 0.001449831616303224\n",
      "Iteration: 4861 lambda_k: 1 Loss: 0.001449760358008231\n",
      "Iteration: 4862 lambda_k: 1 Loss: 0.0014496907166533307\n",
      "Iteration: 4863 lambda_k: 1 Loss: 0.0014496226644469595\n",
      "Iteration: 4864 lambda_k: 1 Loss: 0.0014495561740535436\n",
      "Iteration: 4865 lambda_k: 1 Loss: 0.00144949121858571\n",
      "Iteration: 4866 lambda_k: 1 Loss: 0.0014494277715966402\n",
      "Iteration: 4867 lambda_k: 1 Loss: 0.0014493658070727517\n",
      "Iteration: 4868 lambda_k: 1 Loss: 0.001449305299426299\n",
      "Iteration: 4869 lambda_k: 1 Loss: 0.0014492462234881336\n",
      "Iteration: 4870 lambda_k: 1 Loss: 0.0014491891804155148\n",
      "Iteration: 4871 lambda_k: 1 Loss: 0.0014491332301397066\n",
      "Iteration: 4872 lambda_k: 1 Loss: 0.0014490785897106923\n",
      "Iteration: 4873 lambda_k: 1 Loss: 0.0014490251941676417\n",
      "Iteration: 4874 lambda_k: 1 Loss: 0.0014489730310281136\n",
      "Iteration: 4875 lambda_k: 1 Loss: 0.0014489221106621024\n",
      "Iteration: 4876 lambda_k: 1 Loss: 0.0014488724431966575\n",
      "Iteration: 4877 lambda_k: 1 Loss: 0.001448824027059362\n",
      "Iteration: 4878 lambda_k: 1 Loss: 0.0014487768473920148\n",
      "Iteration: 4879 lambda_k: 1 Loss: 0.0014487308801001447\n",
      "Iteration: 4880 lambda_k: 1 Loss: 0.0014486860974593926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4881 lambda_k: 1 Loss: 0.0014486424725697097\n",
      "Iteration: 4882 lambda_k: 1 Loss: 0.0014485999815665835\n",
      "Iteration: 4883 lambda_k: 1 Loss: 0.0014485586037833048\n",
      "Iteration: 4884 lambda_k: 1 Loss: 0.0014485183207295922\n",
      "Iteration: 4885 lambda_k: 1 Loss: 0.0014484791148175464\n",
      "Iteration: 4886 lambda_k: 1 Loss: 0.0014484409684467293\n",
      "Iteration: 4887 lambda_k: 1 Loss: 0.00144840386364078\n",
      "Iteration: 4888 lambda_k: 1 Loss: 0.0014483677821201886\n",
      "Iteration: 4889 lambda_k: 1 Loss: 0.0014483327055755434\n",
      "Iteration: 4890 lambda_k: 1 Loss: 0.001448298615934807\n",
      "Iteration: 4891 lambda_k: 1 Loss: 0.0014482654955200543\n",
      "Iteration: 4892 lambda_k: 1 Loss: 0.0014482333270822638\n",
      "Iteration: 4893 lambda_k: 1 Loss: 0.0014482020937578604\n",
      "Iteration: 4894 lambda_k: 1 Loss: 0.0014481717790002313\n",
      "Iteration: 4895 lambda_k: 1 Loss: 0.001448142366523922\n",
      "Iteration: 4896 lambda_k: 1 Loss: 0.0014481138402757728\n",
      "Iteration: 4897 lambda_k: 1 Loss: 0.0014480861844296763\n",
      "Iteration: 4898 lambda_k: 1 Loss: 0.0014480593833937569\n",
      "Iteration: 4899 lambda_k: 1 Loss: 0.0014480334218201214\n",
      "Iteration: 4900 lambda_k: 1 Loss: 0.0014480082846101462\n",
      "Iteration: 4901 lambda_k: 1 Loss: 0.0014479839569142689\n",
      "Iteration: 4902 lambda_k: 1 Loss: 0.001447960424127355\n",
      "Iteration: 4903 lambda_k: 1 Loss: 0.0014479376718823534\n",
      "Iteration: 4904 lambda_k: 1 Loss: 0.0014479156860438883\n",
      "Iteration: 4905 lambda_k: 1 Loss: 0.001447894452703146\n",
      "Iteration: 4906 lambda_k: 1 Loss: 0.0014478739581738661\n",
      "Iteration: 4907 lambda_k: 1 Loss: 0.0014478541889891266\n",
      "Iteration: 4908 lambda_k: 1 Loss: 0.001447835131898579\n",
      "Iteration: 4909 lambda_k: 1 Loss: 0.0014478167738655723\n",
      "Iteration: 4910 lambda_k: 1 Loss: 0.0014477991020640803\n",
      "Iteration: 4911 lambda_k: 1 Loss: 0.001447782103875281\n",
      "Iteration: 4912 lambda_k: 1 Loss: 0.0014477657668843856\n",
      "Iteration: 4913 lambda_k: 1 Loss: 0.0014477500788771809\n",
      "Iteration: 4914 lambda_k: 1 Loss: 0.0014477350278368367\n",
      "Iteration: 4915 lambda_k: 1 Loss: 0.0014477206019407726\n",
      "Iteration: 4916 lambda_k: 1 Loss: 0.0014477067895574927\n",
      "Iteration: 4917 lambda_k: 1 Loss: 0.001447693579243727\n",
      "Iteration: 4918 lambda_k: 1 Loss: 0.0014476809597414667\n",
      "Iteration: 4919 lambda_k: 1 Loss: 0.0014476689199750326\n",
      "Iteration: 4920 lambda_k: 1 Loss: 0.0014476574490481639\n",
      "Iteration: 4921 lambda_k: 1 Loss: 0.0014476465362413284\n",
      "Iteration: 4922 lambda_k: 1 Loss: 0.0014476361710087377\n",
      "Iteration: 4923 lambda_k: 1 Loss: 0.0014476263429758257\n",
      "Iteration: 4924 lambda_k: 1 Loss: 0.0014476170419362806\n",
      "Iteration: 4925 lambda_k: 1 Loss: 0.0014476082578496715\n",
      "Iteration: 4926 lambda_k: 1 Loss: 0.0014475999808385449\n",
      "Iteration: 4927 lambda_k: 1 Loss: 0.0014475922011861525\n",
      "Iteration: 4928 lambda_k: 1 Loss: 0.0014475849093337346\n",
      "Iteration: 4929 lambda_k: 1 Loss: 0.0014475780958780936\n",
      "Iteration: 4930 lambda_k: 1 Loss: 0.0014475717515691133\n",
      "Iteration: 4931 lambda_k: 1 Loss: 0.0014475658673072955\n",
      "Iteration: 4932 lambda_k: 1 Loss: 0.001447560434141479\n",
      "Iteration: 4933 lambda_k: 1 Loss: 0.0014475554432665167\n",
      "Iteration: 4934 lambda_k: 1 Loss: 0.0014475508860209102\n",
      "Iteration: 4935 lambda_k: 1 Loss: 0.0014475467538845376\n",
      "Iteration: 4936 lambda_k: 1 Loss: 0.0014475430384764888\n",
      "Iteration: 4937 lambda_k: 1 Loss: 0.0014475397315528839\n",
      "Iteration: 4938 lambda_k: 1 Loss: 0.0014475368250045837\n",
      "Iteration: 4939 lambda_k: 1 Loss: 0.0014475343108552843\n",
      "Iteration: 4940 lambda_k: 1 Loss: 0.0014475321812592004\n",
      "Iteration: 4941 lambda_k: 1 Loss: 0.001447530428499163\n",
      "Iteration: 4942 lambda_k: 1 Loss: 0.0014475290449845676\n",
      "Iteration: 4943 lambda_k: 1 Loss: 0.0014475280232494096\n",
      "Iteration: 4944 lambda_k: 1 Loss: 0.00144752735595022\n",
      "Iteration: 4945 lambda_k: 1 Loss: 0.0014475270358641859\n",
      "Iteration: 4946 lambda_k: 1 Loss: 0.0014475270558873977\n",
      "Iteration: 4947 lambda_k: 1 Loss: 0.001447527409032719\n",
      "Iteration: 4948 lambda_k: 1 Loss: 0.0014475280884282218\n",
      "Iteration: 4949 lambda_k: 1 Loss: 0.0014475290873151529\n",
      "Iteration: 4950 lambda_k: 1 Loss: 0.0014475303990463249\n",
      "Iteration: 4951 lambda_k: 1 Loss: 0.0014475320170843123\n",
      "Iteration: 4952 lambda_k: 1 Loss: 0.0014475339349996304\n",
      "Iteration: 4953 lambda_k: 1 Loss: 0.0014475361464691945\n",
      "Iteration: 4954 lambda_k: 1 Loss: 0.001447538645274537\n",
      "Iteration: 4955 lambda_k: 1 Loss: 0.0014475414253002292\n",
      "Iteration: 4956 lambda_k: 1 Loss: 0.0014475444805322252\n",
      "Iteration: 4957 lambda_k: 1 Loss: 0.00144754780505641\n",
      "Iteration: 4958 lambda_k: 1 Loss: 0.0014475513930567127\n",
      "Iteration: 4959 lambda_k: 1 Loss: 0.0014475552388139642\n",
      "Iteration: 4960 lambda_k: 1 Loss: 0.0014475593367040739\n",
      "Iteration: 4961 lambda_k: 1 Loss: 0.0014475636811967169\n",
      "Iteration: 4962 lambda_k: 1 Loss: 0.001447568266853751\n",
      "Iteration: 4963 lambda_k: 1 Loss: 0.0014475730883278905\n",
      "Iteration: 4964 lambda_k: 1 Loss: 0.0014475781403611235\n",
      "Iteration: 4965 lambda_k: 1 Loss: 0.0014475834177835086\n",
      "Iteration: 4966 lambda_k: 1 Loss: 0.0014475889155116035\n",
      "Iteration: 4967 lambda_k: 1 Loss: 0.0014475946285473003\n",
      "Iteration: 4968 lambda_k: 1 Loss: 0.0014476005519762705\n",
      "Iteration: 4969 lambda_k: 1 Loss: 0.0014476066809668829\n",
      "Iteration: 4970 lambda_k: 1 Loss: 0.001447613010768663\n",
      "Iteration: 4971 lambda_k: 1 Loss: 0.0014476195367112844\n",
      "Iteration: 4972 lambda_k: 1 Loss: 0.001447626254203166\n",
      "Iteration: 4973 lambda_k: 1 Loss: 0.0014476331587301562\n",
      "Iteration: 4974 lambda_k: 1 Loss: 0.0014476402458545408\n",
      "Iteration: 4975 lambda_k: 1 Loss: 0.0014476475112136639\n",
      "Iteration: 4976 lambda_k: 1 Loss: 0.0014476549505189054\n",
      "Iteration: 4977 lambda_k: 1 Loss: 0.0014476625595543295\n",
      "Iteration: 4978 lambda_k: 1 Loss: 0.0014476703341756176\n",
      "Iteration: 4979 lambda_k: 1 Loss: 0.0014476782703091084\n",
      "Iteration: 4980 lambda_k: 1 Loss: 0.0014476863639504988\n",
      "Iteration: 4981 lambda_k: 1 Loss: 0.0014476946111638444\n",
      "Iteration: 4982 lambda_k: 1 Loss: 0.0014477030080804266\n",
      "Iteration: 4983 lambda_k: 1 Loss: 0.0014477115508977202\n",
      "Iteration: 4984 lambda_k: 1 Loss: 0.0014477202358784578\n",
      "Iteration: 4985 lambda_k: 1 Loss: 0.0014477290593494514\n",
      "Iteration: 4986 lambda_k: 1 Loss: 0.001447738017700708\n",
      "Iteration: 4987 lambda_k: 1 Loss: 0.001447747107384383\n",
      "Iteration: 4988 lambda_k: 1 Loss: 0.0014477563249138076\n",
      "Iteration: 4989 lambda_k: 1 Loss: 0.001447765666862509\n",
      "Iteration: 4990 lambda_k: 1 Loss: 0.0014477751298633643\n",
      "Iteration: 4991 lambda_k: 1 Loss: 0.0014477847106075428\n",
      "Iteration: 4992 lambda_k: 1 Loss: 0.0014477944058436923\n",
      "Iteration: 4993 lambda_k: 1 Loss: 0.0014478042123769158\n",
      "Iteration: 4994 lambda_k: 1 Loss: 0.0014478141270680889\n",
      "Iteration: 4995 lambda_k: 1 Loss: 0.001447824146832796\n",
      "Iteration: 4996 lambda_k: 1 Loss: 0.0014478342686405363\n",
      "Iteration: 4997 lambda_k: 1 Loss: 0.001447844489513861\n",
      "Iteration: 4998 lambda_k: 1 Loss: 0.00144785480652761\n",
      "Iteration: 4999 lambda_k: 1 Loss: 0.0014478652168080169\n",
      "Iteration: 5000 lambda_k: 1 Loss: 0.001447875717531875\n",
      "Iteration: 5001 lambda_k: 1 Loss: 0.001447886305925881\n",
      "Iteration: 5002 lambda_k: 1 Loss: 0.0014478969792657006\n",
      "Iteration: 5003 lambda_k: 1 Loss: 0.0014479077348752568\n",
      "Iteration: 5004 lambda_k: 1 Loss: 0.001447918570125973\n",
      "Iteration: 5005 lambda_k: 1 Loss: 0.0014479294824360643\n",
      "Iteration: 5006 lambda_k: 1 Loss: 0.001447940469269682\n",
      "Iteration: 5007 lambda_k: 1 Loss: 0.0014479515281363591\n",
      "Iteration: 5008 lambda_k: 1 Loss: 0.001447962656590198\n",
      "Iteration: 5009 lambda_k: 1 Loss: 0.0014479738522290928\n",
      "Iteration: 5010 lambda_k: 1 Loss: 0.0014479851126942122\n",
      "Iteration: 5011 lambda_k: 1 Loss: 0.0014479964356691895\n",
      "Iteration: 5012 lambda_k: 1 Loss: 0.0014480078188794975\n",
      "Iteration: 5013 lambda_k: 1 Loss: 0.0014480192600917561\n",
      "Iteration: 5014 lambda_k: 1 Loss: 0.0014480307571131709\n",
      "Iteration: 5015 lambda_k: 1 Loss: 0.0014480423077907908\n",
      "Iteration: 5016 lambda_k: 1 Loss: 0.0014480539100108823\n",
      "Iteration: 5017 lambda_k: 1 Loss: 0.001448065561698395\n",
      "Iteration: 5018 lambda_k: 1 Loss: 0.0014480772608162756\n",
      "Iteration: 5019 lambda_k: 1 Loss: 0.0014480890053648928\n",
      "Iteration: 5020 lambda_k: 1 Loss: 0.001448100793381477\n",
      "Iteration: 5021 lambda_k: 1 Loss: 0.0014481126229394258\n",
      "Iteration: 5022 lambda_k: 1 Loss: 0.0014481244921478408\n",
      "Iteration: 5023 lambda_k: 1 Loss: 0.0014481363991509847\n",
      "Iteration: 5024 lambda_k: 1 Loss: 0.0014481483421275492\n",
      "Iteration: 5025 lambda_k: 1 Loss: 0.0014481603192903292\n",
      "Iteration: 5026 lambda_k: 1 Loss: 0.00144817232888549\n",
      "Iteration: 5027 lambda_k: 1 Loss: 0.0014481843691922154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5028 lambda_k: 1 Loss: 0.00144819643852203\n",
      "Iteration: 5029 lambda_k: 1 Loss: 0.0014482085352183233\n",
      "Iteration: 5030 lambda_k: 1 Loss: 0.0014482206576558866\n",
      "Iteration: 5031 lambda_k: 1 Loss: 0.001448232804240473\n",
      "Iteration: 5032 lambda_k: 1 Loss: 0.0014482449734080746\n",
      "Iteration: 5033 lambda_k: 1 Loss: 0.001448257163624657\n",
      "Iteration: 5034 lambda_k: 1 Loss: 0.001448269373385631\n",
      "Iteration: 5035 lambda_k: 1 Loss: 0.0014482816012153109\n",
      "Iteration: 5036 lambda_k: 1 Loss: 0.001448293845666545\n",
      "Iteration: 5037 lambda_k: 1 Loss: 0.001448306105320179\n",
      "Iteration: 5038 lambda_k: 1 Loss: 0.0014483183787846598\n",
      "Iteration: 5039 lambda_k: 1 Loss: 0.00144833066469558\n",
      "Iteration: 5040 lambda_k: 1 Loss: 0.0014483429617151902\n",
      "Iteration: 5041 lambda_k: 1 Loss: 0.0014483552685320914\n",
      "Iteration: 5042 lambda_k: 1 Loss: 0.0014483675838607092\n",
      "Iteration: 5043 lambda_k: 1 Loss: 0.0014483799064409063\n",
      "Iteration: 5044 lambda_k: 1 Loss: 0.0014483922350376041\n",
      "Iteration: 5045 lambda_k: 1 Loss: 0.0014484045684402765\n",
      "Iteration: 5046 lambda_k: 1 Loss: 0.0014484169054626806\n",
      "Iteration: 5047 lambda_k: 1 Loss: 0.0014484292449424942\n",
      "Iteration: 5048 lambda_k: 1 Loss: 0.0014484415857407003\n",
      "Iteration: 5049 lambda_k: 1 Loss: 0.001448453926741424\n",
      "Iteration: 5050 lambda_k: 1 Loss: 0.0014484662668515392\n",
      "Iteration: 5051 lambda_k: 1 Loss: 0.0014484786050001674\n",
      "Iteration: 5052 lambda_k: 1 Loss: 0.0014484909401384337\n",
      "Iteration: 5053 lambda_k: 1 Loss: 0.0014485032712391102\n",
      "Iteration: 5054 lambda_k: 1 Loss: 0.0014485155972961928\n",
      "Iteration: 5055 lambda_k: 1 Loss: 0.0014485279173246507\n",
      "Iteration: 5056 lambda_k: 1 Loss: 0.0014485402303599524\n",
      "Iteration: 5057 lambda_k: 1 Loss: 0.0014485525354579012\n",
      "Iteration: 5058 lambda_k: 1 Loss: 0.001448564831694135\n",
      "Iteration: 5059 lambda_k: 1 Loss: 0.0014485771181638844\n",
      "Iteration: 5060 lambda_k: 1 Loss: 0.0014485893939817334\n",
      "Iteration: 5061 lambda_k: 1 Loss: 0.001448601658281148\n",
      "Iteration: 5062 lambda_k: 1 Loss: 0.0014486139102142333\n",
      "Iteration: 5063 lambda_k: 1 Loss: 0.0014486261489514637\n",
      "Iteration: 5064 lambda_k: 1 Loss: 0.0014486383736813098\n",
      "Iteration: 5065 lambda_k: 1 Loss: 0.0014486505836100068\n",
      "Iteration: 5066 lambda_k: 1 Loss: 0.001448662777961228\n",
      "Iteration: 5067 lambda_k: 1 Loss: 0.001448674955975856\n",
      "Iteration: 5068 lambda_k: 1 Loss: 0.0014486871169115774\n",
      "Iteration: 5069 lambda_k: 1 Loss: 0.001448699260042705\n",
      "Iteration: 5070 lambda_k: 1 Loss: 0.0014487113846598772\n",
      "Iteration: 5071 lambda_k: 1 Loss: 0.0014487234900697596\n",
      "Iteration: 5072 lambda_k: 1 Loss: 0.0014487355755948726\n",
      "Iteration: 5073 lambda_k: 1 Loss: 0.0014487476405731867\n",
      "Iteration: 5074 lambda_k: 1 Loss: 0.0014487596843580195\n",
      "Iteration: 5075 lambda_k: 1 Loss: 0.00144877170631757\n",
      "Iteration: 5076 lambda_k: 1 Loss: 0.0014487837058349573\n",
      "Iteration: 5077 lambda_k: 1 Loss: 0.0014487956823077046\n",
      "Iteration: 5078 lambda_k: 1 Loss: 0.0014488076351476642\n",
      "Iteration: 5079 lambda_k: 1 Loss: 0.001448819563780687\n",
      "Iteration: 5080 lambda_k: 1 Loss: 0.0014488314676464875\n",
      "Iteration: 5081 lambda_k: 1 Loss: 0.0014488433461982623\n",
      "Iteration: 5082 lambda_k: 1 Loss: 0.0014488551989025372\n",
      "Iteration: 5083 lambda_k: 1 Loss: 0.0014488670252390749\n",
      "Iteration: 5084 lambda_k: 1 Loss: 0.0014488788247004102\n",
      "Iteration: 5085 lambda_k: 1 Loss: 0.0014488905967918319\n",
      "Iteration: 5086 lambda_k: 1 Loss: 0.0014489023410310397\n",
      "Iteration: 5087 lambda_k: 1 Loss: 0.0014489140569480155\n",
      "Iteration: 5088 lambda_k: 1 Loss: 0.0014489257440847847\n",
      "Iteration: 5089 lambda_k: 1 Loss: 0.0014489374019951567\n",
      "Iteration: 5090 lambda_k: 1 Loss: 0.0014489490302446461\n",
      "Iteration: 5091 lambda_k: 1 Loss: 0.0014489606284101564\n",
      "Iteration: 5092 lambda_k: 1 Loss: 0.0014489721960798765\n",
      "Iteration: 5093 lambda_k: 1 Loss: 0.0014489837328530084\n",
      "Iteration: 5094 lambda_k: 1 Loss: 0.0014489952383396334\n",
      "Iteration: 5095 lambda_k: 1 Loss: 0.0014490067121604988\n",
      "Iteration: 5096 lambda_k: 1 Loss: 0.0014490181539467872\n",
      "Iteration: 5097 lambda_k: 1 Loss: 0.0014490295633400926\n",
      "Iteration: 5098 lambda_k: 1 Loss: 0.0014490409399921175\n",
      "Iteration: 5099 lambda_k: 1 Loss: 0.0014490522835644416\n",
      "Iteration: 5100 lambda_k: 1 Loss: 0.0014490635937284909\n",
      "Iteration: 5101 lambda_k: 1 Loss: 0.0014490748701653084\n",
      "Iteration: 5102 lambda_k: 1 Loss: 0.0014490861125653562\n",
      "Iteration: 5103 lambda_k: 1 Loss: 0.0014490973206283612\n",
      "Iteration: 5104 lambda_k: 1 Loss: 0.0014491084940632404\n",
      "Iteration: 5105 lambda_k: 1 Loss: 0.0014491196325877845\n",
      "Iteration: 5106 lambda_k: 1 Loss: 0.0014491307359286564\n",
      "Iteration: 5107 lambda_k: 1 Loss: 0.0014491418038211654\n",
      "Iteration: 5108 lambda_k: 1 Loss: 0.0014491528360089964\n",
      "Iteration: 5109 lambda_k: 1 Loss: 0.0014491638322443603\n",
      "Iteration: 5110 lambda_k: 1 Loss: 0.0014491747922875035\n",
      "Iteration: 5111 lambda_k: 1 Loss: 0.001449185715906811\n",
      "Iteration: 5112 lambda_k: 1 Loss: 0.0014491966028786047\n",
      "Iteration: 5113 lambda_k: 1 Loss: 0.0014492074529868925\n",
      "Iteration: 5114 lambda_k: 1 Loss: 0.0014492182660233134\n",
      "Iteration: 5115 lambda_k: 1 Loss: 0.0014492290417870523\n",
      "Iteration: 5116 lambda_k: 1 Loss: 0.001449239780084625\n",
      "Iteration: 5117 lambda_k: 1 Loss: 0.001449250480729806\n",
      "Iteration: 5118 lambda_k: 1 Loss: 0.0014492611435433854\n",
      "Iteration: 5119 lambda_k: 1 Loss: 0.0014492717683531828\n",
      "Iteration: 5120 lambda_k: 1 Loss: 0.001449282354993845\n",
      "Iteration: 5121 lambda_k: 1 Loss: 0.001449292903306742\n",
      "Iteration: 5122 lambda_k: 1 Loss: 0.0014493034131398317\n",
      "Iteration: 5123 lambda_k: 1 Loss: 0.0014493138843475124\n",
      "Iteration: 5124 lambda_k: 1 Loss: 0.0014493243167905674\n",
      "Iteration: 5125 lambda_k: 1 Loss: 0.001449334710336007\n",
      "Iteration: 5126 lambda_k: 1 Loss: 0.001449345064857005\n",
      "Iteration: 5127 lambda_k: 1 Loss: 0.0014493553802327162\n",
      "Iteration: 5128 lambda_k: 1 Loss: 0.001449365656348163\n",
      "Iteration: 5129 lambda_k: 1 Loss: 0.001449375893094191\n",
      "Iteration: 5130 lambda_k: 1 Loss: 0.0014493860903672727\n",
      "Iteration: 5131 lambda_k: 1 Loss: 0.0014493962480695635\n",
      "Iteration: 5132 lambda_k: 1 Loss: 0.0014494063661085938\n",
      "Iteration: 5133 lambda_k: 1 Loss: 0.0014494164443973024\n",
      "Iteration: 5134 lambda_k: 1 Loss: 0.001449426482853855\n",
      "Iteration: 5135 lambda_k: 1 Loss: 0.0014494364814016392\n",
      "Iteration: 5136 lambda_k: 1 Loss: 0.0014494464399690838\n",
      "Iteration: 5137 lambda_k: 1 Loss: 0.0014494563584895944\n",
      "Iteration: 5138 lambda_k: 1 Loss: 0.0014494662369014307\n",
      "Iteration: 5139 lambda_k: 1 Loss: 0.001449476075147674\n",
      "Iteration: 5140 lambda_k: 1 Loss: 0.0014494858731760529\n",
      "Iteration: 5141 lambda_k: 1 Loss: 0.0014494956309389281\n",
      "Iteration: 5142 lambda_k: 1 Loss: 0.0014495053483931473\n",
      "Iteration: 5143 lambda_k: 1 Loss: 0.0014495150255000343\n",
      "Iteration: 5144 lambda_k: 1 Loss: 0.0014495246622251664\n",
      "Iteration: 5145 lambda_k: 1 Loss: 0.00144953425853843\n",
      "Iteration: 5146 lambda_k: 1 Loss: 0.0014495438144138785\n",
      "Iteration: 5147 lambda_k: 1 Loss: 0.0014495533298296498\n",
      "Iteration: 5148 lambda_k: 1 Loss: 0.0014495628047679035\n",
      "Iteration: 5149 lambda_k: 1 Loss: 0.0014495722392146638\n",
      "Iteration: 5150 lambda_k: 1 Loss: 0.0014495816331598597\n",
      "Iteration: 5151 lambda_k: 1 Loss: 0.001449590986597138\n",
      "Iteration: 5152 lambda_k: 1 Loss: 0.0014496002995239485\n",
      "Iteration: 5153 lambda_k: 1 Loss: 0.0014496095719412616\n",
      "Iteration: 5154 lambda_k: 1 Loss: 0.0014496188038535685\n",
      "Iteration: 5155 lambda_k: 1 Loss: 0.0014496279952689215\n",
      "Iteration: 5156 lambda_k: 1 Loss: 0.001449637146198758\n",
      "Iteration: 5157 lambda_k: 1 Loss: 0.0014496462566578\n",
      "Iteration: 5158 lambda_k: 1 Loss: 0.001449655326664091\n",
      "Iteration: 5159 lambda_k: 1 Loss: 0.001449664356238889\n",
      "Iteration: 5160 lambda_k: 1 Loss: 0.0014496733454064654\n",
      "Iteration: 5161 lambda_k: 1 Loss: 0.0014496822941942253\n",
      "Iteration: 5162 lambda_k: 1 Loss: 0.0014496912026325949\n",
      "Iteration: 5163 lambda_k: 1 Loss: 0.0014497000707549327\n",
      "Iteration: 5164 lambda_k: 1 Loss: 0.0014497088985973977\n",
      "Iteration: 5165 lambda_k: 1 Loss: 0.001449717686199108\n",
      "Iteration: 5166 lambda_k: 1 Loss: 0.0014497264336017857\n",
      "Iteration: 5167 lambda_k: 1 Loss: 0.0014497351408499018\n",
      "Iteration: 5168 lambda_k: 1 Loss: 0.0014497438079905992\n",
      "Iteration: 5169 lambda_k: 1 Loss: 0.001449752435073583\n",
      "Iteration: 5170 lambda_k: 1 Loss: 0.0014497610221509782\n",
      "Iteration: 5171 lambda_k: 1 Loss: 0.0014497695692774907\n",
      "Iteration: 5172 lambda_k: 1 Loss: 0.001449778076510235\n",
      "Iteration: 5173 lambda_k: 1 Loss: 0.001449786543908601\n",
      "Iteration: 5174 lambda_k: 1 Loss: 0.001449794971534365\n",
      "Iteration: 5175 lambda_k: 1 Loss: 0.0014498033594514605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5176 lambda_k: 1 Loss: 0.0014498117077260991\n",
      "Iteration: 5177 lambda_k: 1 Loss: 0.00144982001642662\n",
      "Iteration: 5178 lambda_k: 1 Loss: 0.0014498282856234715\n",
      "Iteration: 5179 lambda_k: 1 Loss: 0.0014498365153891284\n",
      "Iteration: 5180 lambda_k: 1 Loss: 0.0014498447057980613\n",
      "Iteration: 5181 lambda_k: 1 Loss: 0.0014498528569267517\n",
      "Iteration: 5182 lambda_k: 1 Loss: 0.0014498609688534901\n",
      "Iteration: 5183 lambda_k: 1 Loss: 0.0014498690416585346\n",
      "Iteration: 5184 lambda_k: 1 Loss: 0.0014498770754239468\n",
      "Iteration: 5185 lambda_k: 1 Loss: 0.0014498850702335238\n",
      "Iteration: 5186 lambda_k: 1 Loss: 0.0014498930261727778\n",
      "Iteration: 5187 lambda_k: 1 Loss: 0.0014499009433289547\n",
      "Iteration: 5188 lambda_k: 1 Loss: 0.0014499088217909374\n",
      "Iteration: 5189 lambda_k: 1 Loss: 0.0014499166616491982\n",
      "Iteration: 5190 lambda_k: 1 Loss: 0.0014499244629958157\n",
      "Iteration: 5191 lambda_k: 1 Loss: 0.0014499322259243534\n",
      "Iteration: 5192 lambda_k: 1 Loss: 0.001449939950529862\n",
      "Iteration: 5193 lambda_k: 1 Loss: 0.0014499476369088818\n",
      "Iteration: 5194 lambda_k: 1 Loss: 0.00144995528515926\n",
      "Iteration: 5195 lambda_k: 1 Loss: 0.0014499628953803098\n",
      "Iteration: 5196 lambda_k: 1 Loss: 0.0014499704676727134\n",
      "Iteration: 5197 lambda_k: 1 Loss: 0.001449978002138417\n",
      "Iteration: 5198 lambda_k: 1 Loss: 0.0014499854988805432\n",
      "Iteration: 5199 lambda_k: 1 Loss: 0.0014499929580035521\n",
      "Iteration: 5200 lambda_k: 1 Loss: 0.0014500003796130726\n",
      "Iteration: 5201 lambda_k: 1 Loss: 0.001450007763815912\n",
      "Iteration: 5202 lambda_k: 1 Loss: 0.0014500151107199593\n",
      "Iteration: 5203 lambda_k: 1 Loss: 0.0014500224204341936\n",
      "Iteration: 5204 lambda_k: 1 Loss: 0.001450029693068711\n",
      "Iteration: 5205 lambda_k: 1 Loss: 0.0014500369287346485\n",
      "Iteration: 5206 lambda_k: 1 Loss: 0.0014500441275441284\n",
      "Iteration: 5207 lambda_k: 1 Loss: 0.001450051289610215\n",
      "Iteration: 5208 lambda_k: 1 Loss: 0.0014500584150469503\n",
      "Iteration: 5209 lambda_k: 1 Loss: 0.0014500655039693053\n",
      "Iteration: 5210 lambda_k: 1 Loss: 0.0014500725564930996\n",
      "Iteration: 5211 lambda_k: 1 Loss: 0.0014500795727350442\n",
      "Iteration: 5212 lambda_k: 1 Loss: 0.0014500865528125947\n",
      "Iteration: 5213 lambda_k: 1 Loss: 0.0014500934968441578\n",
      "Iteration: 5214 lambda_k: 1 Loss: 0.0014501004049487824\n",
      "Iteration: 5215 lambda_k: 1 Loss: 0.0014501072772463695\n",
      "Iteration: 5216 lambda_k: 1 Loss: 0.0014501141138574774\n",
      "Iteration: 5217 lambda_k: 1 Loss: 0.0014501209149033866\n",
      "Iteration: 5218 lambda_k: 1 Loss: 0.0014501276805060298\n",
      "Iteration: 5219 lambda_k: 1 Loss: 0.001450134410788062\n",
      "Iteration: 5220 lambda_k: 1 Loss: 0.0014501411058726703\n",
      "Iteration: 5221 lambda_k: 1 Loss: 0.0014501477658836815\n",
      "Iteration: 5222 lambda_k: 1 Loss: 0.0014501543909455216\n",
      "Iteration: 5223 lambda_k: 1 Loss: 0.0014501609811831534\n",
      "Iteration: 5224 lambda_k: 1 Loss: 0.0014501675367220791\n",
      "Iteration: 5225 lambda_k: 1 Loss: 0.0014501740576883004\n",
      "Iteration: 5226 lambda_k: 1 Loss: 0.0014501805442083667\n",
      "Iteration: 5227 lambda_k: 1 Loss: 0.0014501869964092652\n",
      "Iteration: 5228 lambda_k: 1 Loss: 0.0014501934144183718\n",
      "Iteration: 5229 lambda_k: 1 Loss: 0.001450199798363568\n",
      "Iteration: 5230 lambda_k: 1 Loss: 0.001450206148373054\n",
      "Iteration: 5231 lambda_k: 1 Loss: 0.0014502124645755631\n",
      "Iteration: 5232 lambda_k: 1 Loss: 0.0014502187471001338\n",
      "Iteration: 5233 lambda_k: 1 Loss: 0.0014502249960760953\n",
      "Iteration: 5234 lambda_k: 1 Loss: 0.001450231211633188\n",
      "Iteration: 5235 lambda_k: 1 Loss: 0.0014502373939014014\n",
      "Iteration: 5236 lambda_k: 1 Loss: 0.0014502435430110883\n",
      "Iteration: 5237 lambda_k: 1 Loss: 0.001450249659092879\n",
      "Iteration: 5238 lambda_k: 1 Loss: 0.001450255742277579\n",
      "Iteration: 5239 lambda_k: 1 Loss: 0.0014502617926963547\n",
      "Iteration: 5240 lambda_k: 1 Loss: 0.0014502678104804769\n",
      "Iteration: 5241 lambda_k: 1 Loss: 0.001450273795761558\n",
      "Iteration: 5242 lambda_k: 1 Loss: 0.0014502797486712895\n",
      "Iteration: 5243 lambda_k: 1 Loss: 0.0014502856693416812\n",
      "Iteration: 5244 lambda_k: 1 Loss: 0.0014502915579047925\n",
      "Iteration: 5245 lambda_k: 1 Loss: 0.0014502974144928666\n",
      "Iteration: 5246 lambda_k: 1 Loss: 0.001450303239238273\n",
      "Iteration: 5247 lambda_k: 1 Loss: 0.001450309032273525\n",
      "Iteration: 5248 lambda_k: 1 Loss: 0.0014503147937313058\n",
      "Iteration: 5249 lambda_k: 1 Loss: 0.0014503205237442745\n",
      "Iteration: 5250 lambda_k: 1 Loss: 0.0014503262224452133\n",
      "Iteration: 5251 lambda_k: 1 Loss: 0.0014503318899669873\n",
      "Iteration: 5252 lambda_k: 1 Loss: 0.001450337526442537\n",
      "Iteration: 5253 lambda_k: 1 Loss: 0.0014503431320047595\n",
      "Iteration: 5254 lambda_k: 1 Loss: 0.0014503487067866814\n",
      "Iteration: 5255 lambda_k: 1 Loss: 0.0014503542509212933\n",
      "Iteration: 5256 lambda_k: 1 Loss: 0.0014503597645416017\n",
      "Iteration: 5257 lambda_k: 1 Loss: 0.0014503652477805737\n",
      "Iteration: 5258 lambda_k: 1 Loss: 0.001450370700771253\n",
      "Iteration: 5259 lambda_k: 1 Loss: 0.001450376123646565\n",
      "Iteration: 5260 lambda_k: 1 Loss: 0.001450381516539361\n",
      "Iteration: 5261 lambda_k: 1 Loss: 0.001450386879582542\n",
      "Iteration: 5262 lambda_k: 1 Loss: 0.0014503922129089155\n",
      "Iteration: 5263 lambda_k: 1 Loss: 0.0014503975166511858\n",
      "Iteration: 5264 lambda_k: 1 Loss: 0.0014504027909419705\n",
      "Iteration: 5265 lambda_k: 1 Loss: 0.0014504080359138275\n",
      "Iteration: 5266 lambda_k: 1 Loss: 0.0014504132516991806\n",
      "Iteration: 5267 lambda_k: 1 Loss: 0.0014504184384304163\n",
      "Iteration: 5268 lambda_k: 1 Loss: 0.00145042359623966\n",
      "Iteration: 5269 lambda_k: 1 Loss: 0.001450428725259074\n",
      "Iteration: 5270 lambda_k: 1 Loss: 0.0014504338256205124\n",
      "Iteration: 5271 lambda_k: 1 Loss: 0.0014504388974557866\n",
      "Iteration: 5272 lambda_k: 1 Loss: 0.0014504439408965304\n",
      "Iteration: 5273 lambda_k: 1 Loss: 0.001450448956074218\n",
      "Iteration: 5274 lambda_k: 1 Loss: 0.0014504539431201434\n",
      "Iteration: 5275 lambda_k: 1 Loss: 0.001450458902165366\n",
      "Iteration: 5276 lambda_k: 1 Loss: 0.001450463833340834\n",
      "Iteration: 5277 lambda_k: 1 Loss: 0.0014504687367772415\n",
      "Iteration: 5278 lambda_k: 1 Loss: 0.0014504736126051397\n",
      "Iteration: 5279 lambda_k: 1 Loss: 0.0014504784609547898\n",
      "Iteration: 5280 lambda_k: 1 Loss: 0.0014504832819563342\n",
      "Iteration: 5281 lambda_k: 1 Loss: 0.0014504880757395867\n",
      "Iteration: 5282 lambda_k: 1 Loss: 0.0014504928424341709\n",
      "Iteration: 5283 lambda_k: 1 Loss: 0.001450497582169492\n",
      "Iteration: 5284 lambda_k: 1 Loss: 0.0014505022950746247\n",
      "Iteration: 5285 lambda_k: 1 Loss: 0.001450506981278492\n",
      "Iteration: 5286 lambda_k: 1 Loss: 0.0014505116409097181\n",
      "Iteration: 5287 lambda_k: 1 Loss: 0.0014505162740966475\n",
      "Iteration: 5288 lambda_k: 1 Loss: 0.0014505208809673814\n",
      "Iteration: 5289 lambda_k: 1 Loss: 0.0014505254616497246\n",
      "Iteration: 5290 lambda_k: 1 Loss: 0.0014505300162711578\n",
      "Iteration: 5291 lambda_k: 1 Loss: 0.001450534544959013\n",
      "Iteration: 5292 lambda_k: 1 Loss: 0.0014505390478401314\n",
      "Iteration: 5293 lambda_k: 1 Loss: 0.0014505435250412506\n",
      "Iteration: 5294 lambda_k: 1 Loss: 0.0014505479766886086\n",
      "Iteration: 5295 lambda_k: 1 Loss: 0.0014505524029083524\n",
      "Iteration: 5296 lambda_k: 1 Loss: 0.0014505568038261038\n",
      "Iteration: 5297 lambda_k: 1 Loss: 0.0014505611795673466\n",
      "Iteration: 5298 lambda_k: 1 Loss: 0.0014505655302571366\n",
      "Iteration: 5299 lambda_k: 1 Loss: 0.0014505698560201552\n",
      "Iteration: 5300 lambda_k: 1 Loss: 0.001450574156980888\n",
      "Iteration: 5301 lambda_k: 1 Loss: 0.001450578433263388\n",
      "Iteration: 5302 lambda_k: 1 Loss: 0.0014505826849914119\n",
      "Iteration: 5303 lambda_k: 1 Loss: 0.001450586912288377\n",
      "Iteration: 5304 lambda_k: 1 Loss: 0.0014505911152773486\n",
      "Iteration: 5305 lambda_k: 1 Loss: 0.0014505952940809456\n",
      "Iteration: 5306 lambda_k: 1 Loss: 0.0014505994488215803\n",
      "Iteration: 5307 lambda_k: 1 Loss: 0.0014506035796212017\n",
      "Iteration: 5308 lambda_k: 1 Loss: 0.0014506076866014417\n",
      "Iteration: 5309 lambda_k: 1 Loss: 0.001450611769883557\n",
      "Iteration: 5310 lambda_k: 1 Loss: 0.0014506158295884257\n",
      "Iteration: 5311 lambda_k: 1 Loss: 0.001450619865836587\n",
      "Iteration: 5312 lambda_k: 1 Loss: 0.0014506238787481663\n",
      "Iteration: 5313 lambda_k: 1 Loss: 0.0014506278684429104\n",
      "Iteration: 5314 lambda_k: 1 Loss: 0.0014506318350401856\n",
      "Iteration: 5315 lambda_k: 1 Loss: 0.0014506357786590299\n",
      "Iteration: 5316 lambda_k: 1 Loss: 0.001450639699418049\n",
      "Iteration: 5317 lambda_k: 1 Loss: 0.0014506435974354324\n",
      "Iteration: 5318 lambda_k: 1 Loss: 0.001450647472829022\n",
      "Iteration: 5319 lambda_k: 1 Loss: 0.0014506513257162203\n",
      "Iteration: 5320 lambda_k: 1 Loss: 0.0014506551562140917\n",
      "Iteration: 5321 lambda_k: 1 Loss: 0.0014506589644392688\n",
      "Iteration: 5322 lambda_k: 1 Loss: 0.0014506627505079983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5323 lambda_k: 1 Loss: 0.0014506665145360867\n",
      "Iteration: 5324 lambda_k: 1 Loss: 0.0014506702566389879\n",
      "Iteration: 5325 lambda_k: 1 Loss: 0.0014506739769317158\n",
      "Iteration: 5326 lambda_k: 1 Loss: 0.0014506776755288768\n",
      "Iteration: 5327 lambda_k: 1 Loss: 0.0014506813525446457\n",
      "Iteration: 5328 lambda_k: 1 Loss: 0.001450685008092819\n",
      "Iteration: 5329 lambda_k: 1 Loss: 0.0014506886422867723\n",
      "Iteration: 5330 lambda_k: 1 Loss: 0.0014506922552394953\n",
      "Iteration: 5331 lambda_k: 1 Loss: 0.0014506958470634726\n",
      "Iteration: 5332 lambda_k: 1 Loss: 0.001450699417870858\n",
      "Iteration: 5333 lambda_k: 1 Loss: 0.00145070296777339\n",
      "Iteration: 5334 lambda_k: 1 Loss: 0.0014507064968822794\n",
      "Iteration: 5335 lambda_k: 1 Loss: 0.001450710005308407\n",
      "Iteration: 5336 lambda_k: 1 Loss: 0.0014507134931621894\n",
      "Iteration: 5337 lambda_k: 1 Loss: 0.001450716960553688\n",
      "Iteration: 5338 lambda_k: 1 Loss: 0.0014507204075924216\n",
      "Iteration: 5339 lambda_k: 1 Loss: 0.0014507238343875815\n",
      "Iteration: 5340 lambda_k: 1 Loss: 0.0014507272410478375\n",
      "Iteration: 5341 lambda_k: 1 Loss: 0.0014507306276815384\n",
      "Iteration: 5342 lambda_k: 1 Loss: 0.0014507339943965211\n",
      "Iteration: 5343 lambda_k: 1 Loss: 0.0014507373413002293\n",
      "Iteration: 5344 lambda_k: 1 Loss: 0.0014507406684996693\n",
      "Iteration: 5345 lambda_k: 1 Loss: 0.0014507439761014297\n",
      "Iteration: 5346 lambda_k: 1 Loss: 0.0014507472642116112\n",
      "Iteration: 5347 lambda_k: 1 Loss: 0.0014507505329358964\n",
      "Iteration: 5348 lambda_k: 1 Loss: 0.0014507537823795985\n",
      "Iteration: 5349 lambda_k: 1 Loss: 0.0014507570126475345\n",
      "Iteration: 5350 lambda_k: 1 Loss: 0.0014507602238441604\n",
      "Iteration: 5351 lambda_k: 1 Loss: 0.0014507634160733872\n",
      "Iteration: 5352 lambda_k: 1 Loss: 0.0014507665894387541\n",
      "Iteration: 5353 lambda_k: 1 Loss: 0.0014507697440433104\n",
      "Iteration: 5354 lambda_k: 1 Loss: 0.0014507728799897641\n",
      "Iteration: 5355 lambda_k: 1 Loss: 0.0014507759973803516\n",
      "Iteration: 5356 lambda_k: 1 Loss: 0.0014507790963168346\n",
      "Iteration: 5357 lambda_k: 1 Loss: 0.0014507821769005483\n",
      "Iteration: 5358 lambda_k: 1 Loss: 0.001450785239232412\n",
      "Iteration: 5359 lambda_k: 1 Loss: 0.0014507882834129088\n",
      "Iteration: 5360 lambda_k: 1 Loss: 0.001450791309542075\n",
      "Iteration: 5361 lambda_k: 1 Loss: 0.001450794317719502\n",
      "Iteration: 5362 lambda_k: 1 Loss: 0.0014507973080443593\n",
      "Iteration: 5363 lambda_k: 1 Loss: 0.0014508002806153583\n",
      "Iteration: 5364 lambda_k: 1 Loss: 0.0014508032355308145\n",
      "Iteration: 5365 lambda_k: 1 Loss: 0.001450806172888562\n",
      "Iteration: 5366 lambda_k: 1 Loss: 0.0014508090927860384\n",
      "Iteration: 5367 lambda_k: 1 Loss: 0.0014508119953202442\n",
      "Iteration: 5368 lambda_k: 1 Loss: 0.0014508148805876945\n",
      "Iteration: 5369 lambda_k: 1 Loss: 0.0014508177486844969\n",
      "Iteration: 5370 lambda_k: 1 Loss: 0.0014508205997063324\n",
      "Iteration: 5371 lambda_k: 1 Loss: 0.0014508234337484245\n",
      "Iteration: 5372 lambda_k: 1 Loss: 0.0014508262509056312\n",
      "Iteration: 5373 lambda_k: 1 Loss: 0.001450829051272279\n",
      "Iteration: 5374 lambda_k: 1 Loss: 0.0014508318349423267\n",
      "Iteration: 5375 lambda_k: 1 Loss: 0.0014508346020093113\n",
      "Iteration: 5376 lambda_k: 1 Loss: 0.0014508373525662713\n",
      "Iteration: 5377 lambda_k: 1 Loss: 0.0014508400867058544\n",
      "Iteration: 5378 lambda_k: 1 Loss: 0.0014508428045203164\n",
      "Iteration: 5379 lambda_k: 1 Loss: 0.0014508455061013838\n",
      "Iteration: 5380 lambda_k: 1 Loss: 0.0014508481915404407\n",
      "Iteration: 5381 lambda_k: 1 Loss: 0.0014508508609284074\n",
      "Iteration: 5382 lambda_k: 1 Loss: 0.0014508535143558045\n",
      "Iteration: 5383 lambda_k: 1 Loss: 0.0014508561519126408\n",
      "Iteration: 5384 lambda_k: 1 Loss: 0.0014508587736886183\n",
      "Iteration: 5385 lambda_k: 1 Loss: 0.001450861379772932\n",
      "Iteration: 5386 lambda_k: 1 Loss: 0.0014508639702543649\n",
      "Iteration: 5387 lambda_k: 1 Loss: 0.0014508665452212805\n",
      "Iteration: 5388 lambda_k: 1 Loss: 0.001450869104761616\n",
      "Iteration: 5389 lambda_k: 1 Loss: 0.0014508716489629095\n",
      "Iteration: 5390 lambda_k: 1 Loss: 0.0014508741779122453\n",
      "Iteration: 5391 lambda_k: 1 Loss: 0.0014508766916962938\n",
      "Iteration: 5392 lambda_k: 1 Loss: 0.0014508791904013398\n",
      "Iteration: 5393 lambda_k: 1 Loss: 0.0014508816741131657\n",
      "Iteration: 5394 lambda_k: 1 Loss: 0.0014508841429172236\n",
      "Iteration: 5395 lambda_k: 1 Loss: 0.0014508865968984835\n",
      "Iteration: 5396 lambda_k: 1 Loss: 0.001450889036141577\n",
      "Iteration: 5397 lambda_k: 1 Loss: 0.0014508914607306276\n",
      "Iteration: 5398 lambda_k: 1 Loss: 0.001450893870749391\n",
      "Iteration: 5399 lambda_k: 1 Loss: 0.0014508962662812327\n",
      "Iteration: 5400 lambda_k: 1 Loss: 0.0014508986474090525\n",
      "Iteration: 5401 lambda_k: 1 Loss: 0.0014509010142153514\n",
      "Iteration: 5402 lambda_k: 1 Loss: 0.0014509033667822403\n",
      "Iteration: 5403 lambda_k: 1 Loss: 0.0014509057051914592\n",
      "Iteration: 5404 lambda_k: 1 Loss: 0.0014509080295242434\n",
      "Iteration: 5405 lambda_k: 1 Loss: 0.00145091033986146\n",
      "Iteration: 5406 lambda_k: 1 Loss: 0.0014509126362836397\n",
      "Iteration: 5407 lambda_k: 1 Loss: 0.0014509149188707943\n",
      "Iteration: 5408 lambda_k: 1 Loss: 0.0014509171877026387\n",
      "Iteration: 5409 lambda_k: 1 Loss: 0.0014509194428584003\n",
      "Iteration: 5410 lambda_k: 1 Loss: 0.0014509216844169556\n",
      "Iteration: 5411 lambda_k: 1 Loss: 0.001450923912456762\n",
      "Iteration: 5412 lambda_k: 1 Loss: 0.001450926127055865\n",
      "Iteration: 5413 lambda_k: 1 Loss: 0.0014509283282919058\n",
      "Iteration: 5414 lambda_k: 1 Loss: 0.0014509305162421727\n",
      "Iteration: 5415 lambda_k: 1 Loss: 0.0014509326909835569\n",
      "Iteration: 5416 lambda_k: 1 Loss: 0.001450934852592535\n",
      "Iteration: 5417 lambda_k: 1 Loss: 0.0014509370011451595\n",
      "Iteration: 5418 lambda_k: 1 Loss: 0.0014509391367171294\n",
      "Iteration: 5419 lambda_k: 1 Loss: 0.0014509412593837333\n",
      "Iteration: 5420 lambda_k: 1 Loss: 0.0014509433692198967\n",
      "Iteration: 5421 lambda_k: 1 Loss: 0.001450945466300176\n",
      "Iteration: 5422 lambda_k: 1 Loss: 0.0014509475506986917\n",
      "Iteration: 5423 lambda_k: 1 Loss: 0.001450949622489144\n",
      "Iteration: 5424 lambda_k: 1 Loss: 0.0014509516817449392\n",
      "Iteration: 5425 lambda_k: 1 Loss: 0.0014509537285390802\n",
      "Iteration: 5426 lambda_k: 1 Loss: 0.0014509557629441422\n",
      "Iteration: 5427 lambda_k: 1 Loss: 0.0014509577850323658\n",
      "Iteration: 5428 lambda_k: 1 Loss: 0.001450959794875617\n",
      "Iteration: 5429 lambda_k: 1 Loss: 0.0014509617925453028\n",
      "Iteration: 5430 lambda_k: 1 Loss: 0.001450963778112567\n",
      "Iteration: 5431 lambda_k: 1 Loss: 0.0014509657516480847\n",
      "Iteration: 5432 lambda_k: 1 Loss: 0.0014509677132222363\n",
      "Iteration: 5433 lambda_k: 1 Loss: 0.0014509696629049923\n",
      "Iteration: 5434 lambda_k: 1 Loss: 0.0014509716007659306\n",
      "Iteration: 5435 lambda_k: 1 Loss: 0.0014509735268742835\n",
      "Iteration: 5436 lambda_k: 1 Loss: 0.0014509754412989331\n",
      "Iteration: 5437 lambda_k: 1 Loss: 0.0014509773441083863\n",
      "Iteration: 5438 lambda_k: 1 Loss: 0.0014509792353707663\n",
      "Iteration: 5439 lambda_k: 1 Loss: 0.00145098111515383\n",
      "Iteration: 5440 lambda_k: 1 Loss: 0.0014509829835250023\n",
      "Iteration: 5441 lambda_k: 1 Loss: 0.0014509848405513165\n",
      "Iteration: 5442 lambda_k: 1 Loss: 0.001450986686299428\n",
      "Iteration: 5443 lambda_k: 1 Loss: 0.0014509885208357353\n",
      "Iteration: 5444 lambda_k: 1 Loss: 0.001450990344226156\n",
      "Iteration: 5445 lambda_k: 1 Loss: 0.0014509921565363207\n",
      "Iteration: 5446 lambda_k: 1 Loss: 0.0014509939578314771\n",
      "Iteration: 5447 lambda_k: 1 Loss: 0.0014509957481765486\n",
      "Iteration: 5448 lambda_k: 1 Loss: 0.001450997527636108\n",
      "Iteration: 5449 lambda_k: 1 Loss: 0.0014509992962743667\n",
      "Iteration: 5450 lambda_k: 1 Loss: 0.001451001054155154\n",
      "Iteration: 5451 lambda_k: 1 Loss: 0.0014510028013419852\n",
      "Iteration: 5452 lambda_k: 1 Loss: 0.0014510045378980639\n",
      "Iteration: 5453 lambda_k: 1 Loss: 0.0014510062638862023\n",
      "Iteration: 5454 lambda_k: 1 Loss: 0.0014510079793688894\n",
      "Iteration: 5455 lambda_k: 1 Loss: 0.001451009684408264\n",
      "Iteration: 5456 lambda_k: 1 Loss: 0.0014510113790661262\n",
      "Iteration: 5457 lambda_k: 1 Loss: 0.0014510130634039135\n",
      "Iteration: 5458 lambda_k: 1 Loss: 0.001451014737482801\n",
      "Iteration: 5459 lambda_k: 1 Loss: 0.0014510164013635319\n",
      "Iteration: 5460 lambda_k: 1 Loss: 0.0014510180551065897\n",
      "Iteration: 5461 lambda_k: 1 Loss: 0.0014510196987721122\n",
      "Iteration: 5462 lambda_k: 1 Loss: 0.0014510213324198642\n",
      "Iteration: 5463 lambda_k: 1 Loss: 0.0014510229561093383\n",
      "Iteration: 5464 lambda_k: 1 Loss: 0.0014510245698996756\n",
      "Iteration: 5465 lambda_k: 1 Loss: 0.0014510261738496825\n",
      "Iteration: 5466 lambda_k: 1 Loss: 0.0014510277680178105\n",
      "Iteration: 5467 lambda_k: 1 Loss: 0.0014510293524622288\n",
      "Iteration: 5468 lambda_k: 1 Loss: 0.0014510309272407988\n",
      "Iteration: 5469 lambda_k: 1 Loss: 0.00145103249241104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5470 lambda_k: 1 Loss: 0.0014510340480301713\n",
      "Iteration: 5471 lambda_k: 1 Loss: 0.0014510355941550585\n",
      "Iteration: 5472 lambda_k: 1 Loss: 0.0014510371308422236\n",
      "Iteration: 5473 lambda_k: 1 Loss: 0.0014510386581479703\n",
      "Iteration: 5474 lambda_k: 1 Loss: 0.001451040176128191\n",
      "Iteration: 5475 lambda_k: 1 Loss: 0.0014510416848385176\n",
      "Iteration: 5476 lambda_k: 1 Loss: 0.0014510431843342488\n",
      "Iteration: 5477 lambda_k: 1 Loss: 0.0014510446746704463\n",
      "Iteration: 5478 lambda_k: 1 Loss: 0.001451046155901742\n",
      "Iteration: 5479 lambda_k: 1 Loss: 0.0014510476280825849\n",
      "Iteration: 5480 lambda_k: 1 Loss: 0.0014510490912670107\n",
      "Iteration: 5481 lambda_k: 1 Loss: 0.0014510505455088039\n",
      "Iteration: 5482 lambda_k: 1 Loss: 0.0014510519908614173\n",
      "Iteration: 5483 lambda_k: 1 Loss: 0.0014510534273780664\n",
      "Iteration: 5484 lambda_k: 1 Loss: 0.0014510548551116029\n",
      "Iteration: 5485 lambda_k: 1 Loss: 0.0014510562741146399\n",
      "Iteration: 5486 lambda_k: 1 Loss: 0.0014510576844394465\n",
      "Iteration: 5487 lambda_k: 1 Loss: 0.0014510590861379479\n",
      "Iteration: 5488 lambda_k: 1 Loss: 0.0014510604792618864\n",
      "Iteration: 5489 lambda_k: 1 Loss: 0.0014510618638626733\n",
      "Iteration: 5490 lambda_k: 1 Loss: 0.0014510632399913764\n",
      "Iteration: 5491 lambda_k: 1 Loss: 0.0014510646076988515\n",
      "Iteration: 5492 lambda_k: 1 Loss: 0.0014510659670356045\n",
      "Iteration: 5493 lambda_k: 1 Loss: 0.0014510673180518777\n",
      "Iteration: 5494 lambda_k: 1 Loss: 0.0014510686607976386\n",
      "Iteration: 5495 lambda_k: 1 Loss: 0.0014510699953225472\n",
      "Iteration: 5496 lambda_k: 1 Loss: 0.001451071321676025\n",
      "Iteration: 5497 lambda_k: 1 Loss: 0.0014510726399071412\n",
      "Iteration: 5498 lambda_k: 1 Loss: 0.0014510739500647356\n",
      "Iteration: 5499 lambda_k: 1 Loss: 0.0014510752521973234\n",
      "Iteration: 5500 lambda_k: 1 Loss: 0.0014510765463532126\n",
      "Iteration: 5501 lambda_k: 1 Loss: 0.0014510778325804348\n",
      "Iteration: 5502 lambda_k: 1 Loss: 0.0014510791109266746\n",
      "Iteration: 5503 lambda_k: 1 Loss: 0.0014510803814393519\n",
      "Iteration: 5504 lambda_k: 1 Loss: 0.0014510816441656563\n",
      "Iteration: 5505 lambda_k: 1 Loss: 0.0014510828991525428\n",
      "Iteration: 5506 lambda_k: 1 Loss: 0.001451084146446619\n",
      "Iteration: 5507 lambda_k: 1 Loss: 0.0014510853860942314\n",
      "Iteration: 5508 lambda_k: 1 Loss: 0.0014510866181414825\n",
      "Iteration: 5509 lambda_k: 1 Loss: 0.0014510878426342484\n",
      "Iteration: 5510 lambda_k: 1 Loss: 0.0014510890596180757\n",
      "Iteration: 5511 lambda_k: 1 Loss: 0.00145109026913828\n",
      "Iteration: 5512 lambda_k: 1 Loss: 0.0014510914712398786\n",
      "Iteration: 5513 lambda_k: 1 Loss: 0.0014510926659676883\n",
      "Iteration: 5514 lambda_k: 1 Loss: 0.00145109385336626\n",
      "Iteration: 5515 lambda_k: 1 Loss: 0.001451095033479833\n",
      "Iteration: 5516 lambda_k: 1 Loss: 0.0014510962063524438\n",
      "Iteration: 5517 lambda_k: 1 Loss: 0.0014510973720278425\n",
      "Iteration: 5518 lambda_k: 1 Loss: 0.0014510985305495434\n",
      "Iteration: 5519 lambda_k: 1 Loss: 0.00145109968196081\n",
      "Iteration: 5520 lambda_k: 1 Loss: 0.0014511008263046448\n",
      "Iteration: 5521 lambda_k: 1 Loss: 0.001451101963623771\n",
      "Iteration: 5522 lambda_k: 1 Loss: 0.0014511030939607641\n",
      "Iteration: 5523 lambda_k: 1 Loss: 0.0014511042173578578\n",
      "Iteration: 5524 lambda_k: 1 Loss: 0.00145110533385705\n",
      "Iteration: 5525 lambda_k: 1 Loss: 0.0014511064435001274\n",
      "Iteration: 5526 lambda_k: 1 Loss: 0.001451107546328628\n",
      "Iteration: 5527 lambda_k: 1 Loss: 0.0014511086423838347\n",
      "Iteration: 5528 lambda_k: 1 Loss: 0.0014511097317068\n",
      "Iteration: 5529 lambda_k: 1 Loss: 0.0014511108143383136\n",
      "Iteration: 5530 lambda_k: 1 Loss: 0.0014511118903189715\n",
      "Iteration: 5531 lambda_k: 1 Loss: 0.001451112959689036\n",
      "Iteration: 5532 lambda_k: 1 Loss: 0.001451114022488696\n",
      "Iteration: 5533 lambda_k: 1 Loss: 0.0014511150787577484\n",
      "Iteration: 5534 lambda_k: 1 Loss: 0.0014511161285358496\n",
      "Iteration: 5535 lambda_k: 1 Loss: 0.001451117171862356\n",
      "Iteration: 5536 lambda_k: 1 Loss: 0.0014511182087765379\n",
      "Iteration: 5537 lambda_k: 1 Loss: 0.0014511192393172265\n",
      "Iteration: 5538 lambda_k: 1 Loss: 0.0014511202635231822\n",
      "Iteration: 5539 lambda_k: 1 Loss: 0.0014511212814328875\n",
      "Iteration: 5540 lambda_k: 1 Loss: 0.0014511222930846067\n",
      "Iteration: 5541 lambda_k: 1 Loss: 0.0014511232985163517\n",
      "Iteration: 5542 lambda_k: 1 Loss: 0.0014511242977659337\n",
      "Iteration: 5543 lambda_k: 1 Loss: 0.0014511252908709301\n",
      "Iteration: 5544 lambda_k: 1 Loss: 0.0014511262778687344\n",
      "Iteration: 5545 lambda_k: 1 Loss: 0.0014511272587964768\n",
      "Iteration: 5546 lambda_k: 1 Loss: 0.0014511282336911043\n",
      "Iteration: 5547 lambda_k: 1 Loss: 0.0014511292025892822\n",
      "Iteration: 5548 lambda_k: 1 Loss: 0.0014511301655275538\n",
      "Iteration: 5549 lambda_k: 1 Loss: 0.001451131122542169\n",
      "Iteration: 5550 lambda_k: 1 Loss: 0.0014511320736691734\n",
      "Iteration: 5551 lambda_k: 1 Loss: 0.0014511330189444435\n",
      "Iteration: 5552 lambda_k: 1 Loss: 0.001451133958403642\n",
      "Iteration: 5553 lambda_k: 1 Loss: 0.0014511348920821588\n",
      "Iteration: 5554 lambda_k: 1 Loss: 0.001451135820015248\n",
      "Iteration: 5555 lambda_k: 1 Loss: 0.0014511367422379254\n",
      "Iteration: 5556 lambda_k: 1 Loss: 0.0014511376587849833\n",
      "Iteration: 5557 lambda_k: 1 Loss: 0.0014511385696909696\n",
      "Iteration: 5558 lambda_k: 1 Loss: 0.0014511394749903244\n",
      "Iteration: 5559 lambda_k: 1 Loss: 0.001451140374717222\n",
      "Iteration: 5560 lambda_k: 1 Loss: 0.0014511412689056885\n",
      "Iteration: 5561 lambda_k: 1 Loss: 0.0014511421575894915\n",
      "Iteration: 5562 lambda_k: 1 Loss: 0.0014511430408022362\n",
      "Iteration: 5563 lambda_k: 1 Loss: 0.001451143918577285\n",
      "Iteration: 5564 lambda_k: 1 Loss: 0.0014511447909478275\n",
      "Iteration: 5565 lambda_k: 1 Loss: 0.0014511456579468411\n",
      "Iteration: 5566 lambda_k: 1 Loss: 0.0014511465196071355\n",
      "Iteration: 5567 lambda_k: 1 Loss: 0.001451147375961325\n",
      "Iteration: 5568 lambda_k: 1 Loss: 0.0014511482270418137\n",
      "Iteration: 5569 lambda_k: 1 Loss: 0.001451149072880776\n",
      "Iteration: 5570 lambda_k: 1 Loss: 0.0014511499135102589\n",
      "Iteration: 5571 lambda_k: 1 Loss: 0.0014511507489621532\n",
      "Iteration: 5572 lambda_k: 1 Loss: 0.0014511515792679853\n",
      "Iteration: 5573 lambda_k: 1 Loss: 0.0014511524044592981\n",
      "Iteration: 5574 lambda_k: 1 Loss: 0.0014511532245673454\n",
      "Iteration: 5575 lambda_k: 1 Loss: 0.0014511540396231794\n",
      "Iteration: 5576 lambda_k: 1 Loss: 0.0014511548496577184\n",
      "Iteration: 5577 lambda_k: 1 Loss: 0.0014511556547016989\n",
      "Iteration: 5578 lambda_k: 1 Loss: 0.0014511564547855752\n",
      "Iteration: 5579 lambda_k: 1 Loss: 0.0014511572499397226\n",
      "Iteration: 5580 lambda_k: 1 Loss: 0.0014511580401943055\n",
      "Iteration: 5581 lambda_k: 1 Loss: 0.0014511588255793496\n",
      "Iteration: 5582 lambda_k: 1 Loss: 0.001451159606124614\n",
      "Iteration: 5583 lambda_k: 1 Loss: 0.0014511603818596996\n",
      "Iteration: 5584 lambda_k: 1 Loss: 0.0014511611528140878\n",
      "Iteration: 5585 lambda_k: 1 Loss: 0.0014511619190170548\n",
      "Iteration: 5586 lambda_k: 1 Loss: 0.0014511626804977267\n",
      "Iteration: 5587 lambda_k: 1 Loss: 0.0014511634372849755\n",
      "Iteration: 5588 lambda_k: 1 Loss: 0.0014511641894075617\n",
      "Iteration: 5589 lambda_k: 1 Loss: 0.0014511649368940725\n",
      "Iteration: 5590 lambda_k: 1 Loss: 0.0014511656797728905\n",
      "Iteration: 5591 lambda_k: 1 Loss: 0.0014511664180722507\n",
      "Iteration: 5592 lambda_k: 1 Loss: 0.001451167151820266\n",
      "Iteration: 5593 lambda_k: 1 Loss: 0.0014511678810447994\n",
      "Iteration: 5594 lambda_k: 1 Loss: 0.0014511686057735724\n",
      "Iteration: 5595 lambda_k: 1 Loss: 0.001451169326034153\n",
      "Iteration: 5596 lambda_k: 1 Loss: 0.0014511700418539805\n",
      "Iteration: 5597 lambda_k: 1 Loss: 0.0014511707532602195\n",
      "Iteration: 5598 lambda_k: 1 Loss: 0.0014511714602799867\n",
      "Iteration: 5599 lambda_k: 1 Loss: 0.0014511721629401496\n",
      "Iteration: 5600 lambda_k: 1 Loss: 0.0014511728612675078\n",
      "Iteration: 5601 lambda_k: 1 Loss: 0.0014511735552886105\n",
      "Iteration: 5602 lambda_k: 1 Loss: 0.0014511742450298373\n",
      "Iteration: 5603 lambda_k: 1 Loss: 0.0014511749305175087\n",
      "Iteration: 5604 lambda_k: 1 Loss: 0.0014511756117777162\n",
      "Iteration: 5605 lambda_k: 1 Loss: 0.0014511762888364398\n",
      "Iteration: 5606 lambda_k: 1 Loss: 0.001451176961719429\n",
      "Iteration: 5607 lambda_k: 1 Loss: 0.001451177630452375\n",
      "Iteration: 5608 lambda_k: 1 Loss: 0.0014511782950607079\n",
      "Iteration: 5609 lambda_k: 1 Loss: 0.0014511789555697186\n",
      "Iteration: 5610 lambda_k: 1 Loss: 0.0014511796120047062\n",
      "Iteration: 5611 lambda_k: 1 Loss: 0.0014511802643905895\n",
      "Iteration: 5612 lambda_k: 1 Loss: 0.0014511809127523015\n",
      "Iteration: 5613 lambda_k: 1 Loss: 0.0014511815571145305\n",
      "Iteration: 5614 lambda_k: 1 Loss: 0.0014511821975018342\n",
      "Iteration: 5615 lambda_k: 1 Loss: 0.0014511828339386647\n",
      "Iteration: 5616 lambda_k: 1 Loss: 0.0014511834664493102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5617 lambda_k: 1 Loss: 0.001451184095057895\n",
      "Iteration: 5618 lambda_k: 1 Loss: 0.0014511847197884115\n",
      "Iteration: 5619 lambda_k: 1 Loss: 0.0014511853406646788\n",
      "Iteration: 5620 lambda_k: 1 Loss: 0.0014511859577103587\n",
      "Iteration: 5621 lambda_k: 1 Loss: 0.0014511865709490423\n",
      "Iteration: 5622 lambda_k: 1 Loss: 0.0014511871804041244\n",
      "Iteration: 5623 lambda_k: 1 Loss: 0.0014511877860989453\n",
      "Iteration: 5624 lambda_k: 1 Loss: 0.0014511883880565348\n",
      "Iteration: 5625 lambda_k: 1 Loss: 0.0014511889862999265\n",
      "Iteration: 5626 lambda_k: 1 Loss: 0.0014511895808519855\n",
      "Iteration: 5627 lambda_k: 1 Loss: 0.001451190171735378\n",
      "Iteration: 5628 lambda_k: 1 Loss: 0.00145119075897269\n",
      "Iteration: 5629 lambda_k: 1 Loss: 0.001451191342586332\n",
      "Iteration: 5630 lambda_k: 1 Loss: 0.0014511919225986384\n",
      "Iteration: 5631 lambda_k: 1 Loss: 0.0014511924990317628\n",
      "Iteration: 5632 lambda_k: 1 Loss: 0.0014511930719077212\n",
      "Iteration: 5633 lambda_k: 1 Loss: 0.001451193641248422\n",
      "Iteration: 5634 lambda_k: 1 Loss: 0.0014511942070756173\n",
      "Iteration: 5635 lambda_k: 1 Loss: 0.0014511947694109276\n",
      "Iteration: 5636 lambda_k: 1 Loss: 0.0014511953282758871\n",
      "Iteration: 5637 lambda_k: 1 Loss: 0.0014511958836918538\n",
      "Iteration: 5638 lambda_k: 1 Loss: 0.0014511964356800318\n",
      "Iteration: 5639 lambda_k: 1 Loss: 0.0014511969842615636\n",
      "Iteration: 5640 lambda_k: 1 Loss: 0.0014511975294573948\n",
      "Iteration: 5641 lambda_k: 1 Loss: 0.0014511980712883935\n",
      "Iteration: 5642 lambda_k: 1 Loss: 0.0014511986097753232\n",
      "Iteration: 5643 lambda_k: 1 Loss: 0.0014511991449387516\n",
      "Iteration: 5644 lambda_k: 1 Loss: 0.0014511996767991692\n",
      "Iteration: 5645 lambda_k: 1 Loss: 0.0014512002053768901\n",
      "Iteration: 5646 lambda_k: 1 Loss: 0.0014512007306921439\n",
      "Iteration: 5647 lambda_k: 1 Loss: 0.0014512012527650609\n",
      "Iteration: 5648 lambda_k: 1 Loss: 0.001451201771615606\n",
      "Iteration: 5649 lambda_k: 1 Loss: 0.0014512022872636406\n",
      "Iteration: 5650 lambda_k: 1 Loss: 0.0014512027997289078\n",
      "Iteration: 5651 lambda_k: 1 Loss: 0.0014512033090310495\n",
      "Iteration: 5652 lambda_k: 1 Loss: 0.0014512038151895015\n",
      "Iteration: 5653 lambda_k: 1 Loss: 0.0014512043182236902\n",
      "Iteration: 5654 lambda_k: 1 Loss: 0.0014512048181528345\n",
      "Iteration: 5655 lambda_k: 1 Loss: 0.0014512053149961595\n",
      "Iteration: 5656 lambda_k: 1 Loss: 0.0014512058087726072\n",
      "Iteration: 5657 lambda_k: 1 Loss: 0.0014512062995011173\n",
      "Iteration: 5658 lambda_k: 1 Loss: 0.0014512067872005032\n",
      "Iteration: 5659 lambda_k: 1 Loss: 0.001451207271889438\n",
      "Iteration: 5660 lambda_k: 1 Loss: 0.0014512077535864692\n",
      "Iteration: 5661 lambda_k: 1 Loss: 0.0014512082323100666\n",
      "Iteration: 5662 lambda_k: 1 Loss: 0.0014512087080785809\n",
      "Iteration: 5663 lambda_k: 1 Loss: 0.0014512091809101922\n",
      "Iteration: 5664 lambda_k: 1 Loss: 0.0014512096508230887\n",
      "Iteration: 5665 lambda_k: 1 Loss: 0.0014512101178352208\n",
      "Iteration: 5666 lambda_k: 1 Loss: 0.0014512105819644908\n",
      "Iteration: 5667 lambda_k: 1 Loss: 0.001451211043228714\n",
      "Iteration: 5668 lambda_k: 1 Loss: 0.0014512115016455558\n",
      "Iteration: 5669 lambda_k: 1 Loss: 0.001451211957232542\n",
      "Iteration: 5670 lambda_k: 1 Loss: 0.0014512124100071703\n",
      "Iteration: 5671 lambda_k: 1 Loss: 0.0014512128599868228\n",
      "Iteration: 5672 lambda_k: 1 Loss: 0.0014512133071887482\n",
      "Iteration: 5673 lambda_k: 1 Loss: 0.0014512137516300332\n",
      "Iteration: 5674 lambda_k: 1 Loss: 0.001451214193327796\n",
      "Iteration: 5675 lambda_k: 1 Loss: 0.0014512146322989364\n",
      "Iteration: 5676 lambda_k: 1 Loss: 0.0014512150685602246\n",
      "Iteration: 5677 lambda_k: 1 Loss: 0.0014512155021284754\n",
      "Iteration: 5678 lambda_k: 1 Loss: 0.001451215933020279\n",
      "Iteration: 5679 lambda_k: 1 Loss: 0.0014512163612521495\n",
      "Iteration: 5680 lambda_k: 1 Loss: 0.0014512167868405439\n",
      "Iteration: 5681 lambda_k: 1 Loss: 0.0014512172098017557\n",
      "Iteration: 5682 lambda_k: 1 Loss: 0.0014512176301520363\n",
      "Iteration: 5683 lambda_k: 1 Loss: 0.0014512180479074842\n",
      "Iteration: 5684 lambda_k: 1 Loss: 0.0014512184630841023\n",
      "Iteration: 5685 lambda_k: 1 Loss: 0.0014512188756978513\n",
      "Iteration: 5686 lambda_k: 1 Loss: 0.0014512192857645723\n",
      "Iteration: 5687 lambda_k: 1 Loss: 0.0014512196932999822\n",
      "Iteration: 5688 lambda_k: 1 Loss: 0.0014512200983197083\n",
      "Iteration: 5689 lambda_k: 1 Loss: 0.0014512205008392562\n",
      "Iteration: 5690 lambda_k: 1 Loss: 0.0014512209008741394\n",
      "Iteration: 5691 lambda_k: 1 Loss: 0.001451221298439672\n",
      "Iteration: 5692 lambda_k: 1 Loss: 0.0014512216935510997\n",
      "Iteration: 5693 lambda_k: 1 Loss: 0.0014512220862236103\n",
      "Iteration: 5694 lambda_k: 1 Loss: 0.0014512224764722025\n",
      "Iteration: 5695 lambda_k: 1 Loss: 0.0014512228643119517\n",
      "Iteration: 5696 lambda_k: 1 Loss: 0.0014512232497576865\n",
      "Iteration: 5697 lambda_k: 1 Loss: 0.001451223632824206\n",
      "Iteration: 5698 lambda_k: 1 Loss: 0.0014512240135262108\n",
      "Iteration: 5699 lambda_k: 1 Loss: 0.0014512243918783066\n",
      "Iteration: 5700 lambda_k: 1 Loss: 0.0014512247678950066\n",
      "Iteration: 5701 lambda_k: 1 Loss: 0.0014512251415907554\n",
      "Iteration: 5702 lambda_k: 1 Loss: 0.0014512255129799198\n",
      "Iteration: 5703 lambda_k: 1 Loss: 0.0014512258820767103\n",
      "Iteration: 5704 lambda_k: 1 Loss: 0.001451226248895347\n",
      "Iteration: 5705 lambda_k: 1 Loss: 0.0014512266134499123\n",
      "Iteration: 5706 lambda_k: 1 Loss: 0.0014512269757543674\n",
      "Iteration: 5707 lambda_k: 1 Loss: 0.0014512273358226046\n",
      "Iteration: 5708 lambda_k: 1 Loss: 0.0014512276936684883\n",
      "Iteration: 5709 lambda_k: 1 Loss: 0.0014512280493057255\n",
      "Iteration: 5710 lambda_k: 1 Loss: 0.0014512284027479768\n",
      "Iteration: 5711 lambda_k: 1 Loss: 0.0014512287540088364\n",
      "Iteration: 5712 lambda_k: 1 Loss: 0.0014512291031017273\n",
      "Iteration: 5713 lambda_k: 1 Loss: 0.0014512294500401176\n",
      "Iteration: 5714 lambda_k: 1 Loss: 0.0014512297948372727\n",
      "Iteration: 5715 lambda_k: 1 Loss: 0.001451230137506496\n",
      "Iteration: 5716 lambda_k: 1 Loss: 0.0014512304780608916\n",
      "Iteration: 5717 lambda_k: 1 Loss: 0.0014512308165135906\n",
      "Iteration: 5718 lambda_k: 1 Loss: 0.0014512311528775512\n",
      "Iteration: 5719 lambda_k: 1 Loss: 0.0014512314871656955\n",
      "Iteration: 5720 lambda_k: 1 Loss: 0.0014512318193908595\n",
      "Iteration: 5721 lambda_k: 1 Loss: 0.001451232149565773\n",
      "Iteration: 5722 lambda_k: 1 Loss: 0.0014512324777031448\n",
      "Iteration: 5723 lambda_k: 1 Loss: 0.0014512328038155562\n",
      "Iteration: 5724 lambda_k: 1 Loss: 0.0014512331279155476\n",
      "Iteration: 5725 lambda_k: 1 Loss: 0.0014512334500155664\n",
      "Iteration: 5726 lambda_k: 1 Loss: 0.0014512337701279865\n",
      "Iteration: 5727 lambda_k: 1 Loss: 0.0014512340882651238\n",
      "Iteration: 5728 lambda_k: 1 Loss: 0.0014512344044391307\n",
      "Iteration: 5729 lambda_k: 1 Loss: 0.0014512347186621773\n",
      "Iteration: 5730 lambda_k: 1 Loss: 0.0014512350309463051\n",
      "Iteration: 5731 lambda_k: 1 Loss: 0.001451235341303592\n",
      "Iteration: 5732 lambda_k: 1 Loss: 0.0014512356497458807\n",
      "Iteration: 5733 lambda_k: 1 Loss: 0.0014512359562850435\n",
      "Iteration: 5734 lambda_k: 1 Loss: 0.001451236260932854\n",
      "Iteration: 5735 lambda_k: 1 Loss: 0.0014512365637009976\n",
      "Iteration: 5736 lambda_k: 1 Loss: 0.001451236864601133\n",
      "Iteration: 5737 lambda_k: 1 Loss: 0.0014512371636447804\n",
      "Iteration: 5738 lambda_k: 1 Loss: 0.0014512374608434276\n",
      "Iteration: 5739 lambda_k: 1 Loss: 0.0014512377562085207\n",
      "Iteration: 5740 lambda_k: 1 Loss: 0.001451238049751378\n",
      "Iteration: 5741 lambda_k: 1 Loss: 0.0014512383414832701\n",
      "Iteration: 5742 lambda_k: 1 Loss: 0.0014512386314154183\n",
      "Iteration: 5743 lambda_k: 1 Loss: 0.0014512389195589389\n",
      "Iteration: 5744 lambda_k: 1 Loss: 0.0014512392059249125\n",
      "Iteration: 5745 lambda_k: 1 Loss: 0.0014512394905243228\n",
      "Iteration: 5746 lambda_k: 1 Loss: 0.0014512397733681412\n",
      "Iteration: 5747 lambda_k: 1 Loss: 0.0014512400544671926\n",
      "Iteration: 5748 lambda_k: 1 Loss: 0.0014512403338322669\n",
      "Iteration: 5749 lambda_k: 1 Loss: 0.0014512406114741052\n",
      "Iteration: 5750 lambda_k: 1 Loss: 0.001451240887403371\n",
      "Iteration: 5751 lambda_k: 1 Loss: 0.0014512411616306453\n",
      "Iteration: 5752 lambda_k: 1 Loss: 0.0014512414341664752\n",
      "Iteration: 5753 lambda_k: 1 Loss: 0.001451241705021361\n",
      "Iteration: 5754 lambda_k: 1 Loss: 0.001451241974205649\n",
      "Iteration: 5755 lambda_k: 1 Loss: 0.001451242241729697\n",
      "Iteration: 5756 lambda_k: 1 Loss: 0.0014512425076037673\n",
      "Iteration: 5757 lambda_k: 1 Loss: 0.0014512427718381036\n",
      "Iteration: 5758 lambda_k: 1 Loss: 0.001451243034442807\n",
      "Iteration: 5759 lambda_k: 1 Loss: 0.0014512432954280324\n",
      "Iteration: 5760 lambda_k: 1 Loss: 0.0014512435548037271\n",
      "Iteration: 5761 lambda_k: 1 Loss: 0.0014512438125799344\n",
      "Iteration: 5762 lambda_k: 1 Loss: 0.0014512440687664701\n",
      "Iteration: 5763 lambda_k: 1 Loss: 0.0014512443233731804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5764 lambda_k: 1 Loss: 0.0014512445764098883\n",
      "Iteration: 5765 lambda_k: 1 Loss: 0.0014512448278863152\n",
      "Iteration: 5766 lambda_k: 1 Loss: 0.0014512450778120945\n",
      "Iteration: 5767 lambda_k: 1 Loss: 0.0014512453261968218\n",
      "Iteration: 5768 lambda_k: 1 Loss: 0.0014512455730500064\n",
      "Iteration: 5769 lambda_k: 1 Loss: 0.001451245818381151\n",
      "Iteration: 5770 lambda_k: 1 Loss: 0.0014512460621996979\n",
      "Iteration: 5771 lambda_k: 1 Loss: 0.0014512463045149911\n",
      "Iteration: 5772 lambda_k: 1 Loss: 0.0014512465453363353\n",
      "Iteration: 5773 lambda_k: 1 Loss: 0.0014512467846729772\n",
      "Iteration: 5774 lambda_k: 1 Loss: 0.001451247022534077\n",
      "Iteration: 5775 lambda_k: 1 Loss: 0.0014512472589288008\n",
      "Iteration: 5776 lambda_k: 1 Loss: 0.0014512474938662224\n",
      "Iteration: 5777 lambda_k: 1 Loss: 0.0014512477273553983\n",
      "Iteration: 5778 lambda_k: 1 Loss: 0.0014512479594052297\n",
      "Iteration: 5779 lambda_k: 1 Loss: 0.0014512481900246334\n",
      "Iteration: 5780 lambda_k: 1 Loss: 0.0014512484192224845\n",
      "Iteration: 5781 lambda_k: 1 Loss: 0.0014512486470075637\n",
      "Iteration: 5782 lambda_k: 1 Loss: 0.0014512488733886275\n",
      "Iteration: 5783 lambda_k: 1 Loss: 0.0014512490983743915\n",
      "Iteration: 5784 lambda_k: 1 Loss: 0.001451249321973443\n",
      "Iteration: 5785 lambda_k: 1 Loss: 0.001451249544194397\n",
      "Iteration: 5786 lambda_k: 1 Loss: 0.0014512497650457908\n",
      "Iteration: 5787 lambda_k: 1 Loss: 0.0014512499845360809\n",
      "Iteration: 5788 lambda_k: 1 Loss: 0.0014512502026737134\n",
      "Iteration: 5789 lambda_k: 1 Loss: 0.0014512504194670514\n",
      "Iteration: 5790 lambda_k: 1 Loss: 0.001451250634924385\n",
      "Iteration: 5791 lambda_k: 1 Loss: 0.0014512508490540088\n",
      "Iteration: 5792 lambda_k: 1 Loss: 0.0014512510618641477\n",
      "Iteration: 5793 lambda_k: 1 Loss: 0.001451251273362955\n",
      "Iteration: 5794 lambda_k: 1 Loss: 0.0014512514835586007\n",
      "Iteration: 5795 lambda_k: 1 Loss: 0.0014512516924590681\n",
      "Iteration: 5796 lambda_k: 1 Loss: 0.001451251900072449\n",
      "Iteration: 5797 lambda_k: 1 Loss: 0.001451252106406621\n",
      "Iteration: 5798 lambda_k: 1 Loss: 0.0014512523114695772\n",
      "Iteration: 5799 lambda_k: 1 Loss: 0.00145125251526918\n",
      "Iteration: 5800 lambda_k: 1 Loss: 0.001451252717813249\n",
      "Iteration: 5801 lambda_k: 1 Loss: 0.0014512529191095139\n",
      "Iteration: 5802 lambda_k: 1 Loss: 0.001451253119165722\n",
      "Iteration: 5803 lambda_k: 1 Loss: 0.001451253317989541\n",
      "Iteration: 5804 lambda_k: 1 Loss: 0.0014512535155886581\n",
      "Iteration: 5805 lambda_k: 1 Loss: 0.0014512537119705919\n",
      "Iteration: 5806 lambda_k: 1 Loss: 0.0014512539071428472\n",
      "Iteration: 5807 lambda_k: 1 Loss: 0.0014512541011129775\n",
      "Iteration: 5808 lambda_k: 1 Loss: 0.001451254293888418\n",
      "Iteration: 5809 lambda_k: 1 Loss: 0.0014512544854765736\n",
      "Iteration: 5810 lambda_k: 1 Loss: 0.0014512546758847467\n",
      "Iteration: 5811 lambda_k: 1 Loss: 0.0014512548651202416\n",
      "Iteration: 5812 lambda_k: 1 Loss: 0.0014512550531903647\n",
      "Iteration: 5813 lambda_k: 1 Loss: 0.001451255240102257\n",
      "Iteration: 5814 lambda_k: 1 Loss: 0.001451255425863127\n",
      "Iteration: 5815 lambda_k: 1 Loss: 0.0014512556104801235\n",
      "Iteration: 5816 lambda_k: 1 Loss: 0.0014512557939603216\n",
      "Iteration: 5817 lambda_k: 1 Loss: 0.0014512559763107315\n",
      "Iteration: 5818 lambda_k: 1 Loss: 0.0014512561575383803\n",
      "Iteration: 5819 lambda_k: 1 Loss: 0.0014512563376502\n",
      "Iteration: 5820 lambda_k: 1 Loss: 0.0014512565166530573\n",
      "Iteration: 5821 lambda_k: 1 Loss: 0.0014512566945538677\n",
      "Iteration: 5822 lambda_k: 1 Loss: 0.0014512568713594574\n",
      "Iteration: 5823 lambda_k: 1 Loss: 0.0014512570470765717\n",
      "Iteration: 5824 lambda_k: 1 Loss: 0.0014512572217119238\n",
      "Iteration: 5825 lambda_k: 1 Loss: 0.0014512573952722542\n",
      "Iteration: 5826 lambda_k: 1 Loss: 0.0014512575677641992\n",
      "Iteration: 5827 lambda_k: 1 Loss: 0.0014512577391943573\n",
      "Iteration: 5828 lambda_k: 1 Loss: 0.0014512579095693758\n",
      "Iteration: 5829 lambda_k: 1 Loss: 0.0014512580788956905\n",
      "Iteration: 5830 lambda_k: 1 Loss: 0.0014512582471798332\n",
      "Iteration: 5831 lambda_k: 1 Loss: 0.0014512584144282547\n",
      "Iteration: 5832 lambda_k: 1 Loss: 0.0014512585806473674\n",
      "Iteration: 5833 lambda_k: 1 Loss: 0.0014512587458435226\n",
      "Iteration: 5834 lambda_k: 1 Loss: 0.0014512589100230601\n",
      "Iteration: 5835 lambda_k: 1 Loss: 0.001451259073192308\n",
      "Iteration: 5836 lambda_k: 1 Loss: 0.0014512592353574624\n",
      "Iteration: 5837 lambda_k: 1 Loss: 0.0014512593965247975\n",
      "Iteration: 5838 lambda_k: 1 Loss: 0.0014512595567004606\n",
      "Iteration: 5839 lambda_k: 1 Loss: 0.0014512597158905763\n",
      "Iteration: 5840 lambda_k: 1 Loss: 0.0014512598741012084\n",
      "Iteration: 5841 lambda_k: 1 Loss: 0.0014512600313384585\n",
      "Iteration: 5842 lambda_k: 1 Loss: 0.001451260187608352\n",
      "Iteration: 5843 lambda_k: 1 Loss: 0.0014512603429168876\n",
      "Iteration: 5844 lambda_k: 1 Loss: 0.0014512604972700353\n",
      "Iteration: 5845 lambda_k: 1 Loss: 0.00145126065067362\n",
      "Iteration: 5846 lambda_k: 1 Loss: 0.0014512608031335818\n",
      "Iteration: 5847 lambda_k: 1 Loss: 0.0014512609546557384\n",
      "Iteration: 5848 lambda_k: 1 Loss: 0.00145126110524593\n",
      "Iteration: 5849 lambda_k: 1 Loss: 0.0014512612549098822\n",
      "Iteration: 5850 lambda_k: 1 Loss: 0.0014512614036533583\n",
      "Iteration: 5851 lambda_k: 1 Loss: 0.0014512615514820177\n",
      "Iteration: 5852 lambda_k: 1 Loss: 0.0014512616984015151\n",
      "Iteration: 5853 lambda_k: 1 Loss: 0.001451261844417498\n",
      "Iteration: 5854 lambda_k: 1 Loss: 0.0014512619895355708\n",
      "Iteration: 5855 lambda_k: 1 Loss: 0.001451262133761272\n",
      "Iteration: 5856 lambda_k: 1 Loss: 0.0014512622771001278\n",
      "Iteration: 5857 lambda_k: 1 Loss: 0.0014512624195576367\n",
      "Iteration: 5858 lambda_k: 1 Loss: 0.0014512625611392274\n",
      "Iteration: 5859 lambda_k: 1 Loss: 0.0014512627018503127\n",
      "Iteration: 5860 lambda_k: 1 Loss: 0.0014512628416963056\n",
      "Iteration: 5861 lambda_k: 1 Loss: 0.0014512629806825402\n",
      "Iteration: 5862 lambda_k: 1 Loss: 0.001451263118814371\n",
      "Iteration: 5863 lambda_k: 1 Loss: 0.0014512632560970502\n",
      "Iteration: 5864 lambda_k: 1 Loss: 0.0014512633925358313\n",
      "Iteration: 5865 lambda_k: 1 Loss: 0.0014512635281359388\n",
      "Iteration: 5866 lambda_k: 1 Loss: 0.0014512636629025924\n",
      "Iteration: 5867 lambda_k: 1 Loss: 0.0014512637968409029\n",
      "Iteration: 5868 lambda_k: 1 Loss: 0.001451263929956025\n",
      "Iteration: 5869 lambda_k: 1 Loss: 0.0014512640622530177\n",
      "Iteration: 5870 lambda_k: 1 Loss: 0.0014512641937369885\n",
      "Iteration: 5871 lambda_k: 1 Loss: 0.0014512643244128853\n",
      "Iteration: 5872 lambda_k: 1 Loss: 0.0014512644542857829\n",
      "Iteration: 5873 lambda_k: 1 Loss: 0.0014512645833606212\n",
      "Iteration: 5874 lambda_k: 1 Loss: 0.0014512647116423506\n",
      "Iteration: 5875 lambda_k: 1 Loss: 0.0014512648391358542\n",
      "Iteration: 5876 lambda_k: 1 Loss: 0.0014512649658460367\n",
      "Iteration: 5877 lambda_k: 1 Loss: 0.0014512650917777184\n",
      "Iteration: 5878 lambda_k: 1 Loss: 0.0014512652169356847\n",
      "Iteration: 5879 lambda_k: 1 Loss: 0.0014512653413247894\n",
      "Iteration: 5880 lambda_k: 1 Loss: 0.0014512654649497774\n",
      "Iteration: 5881 lambda_k: 1 Loss: 0.0014512655878153472\n",
      "Iteration: 5882 lambda_k: 1 Loss: 0.0014512657099261974\n",
      "Iteration: 5883 lambda_k: 1 Loss: 0.0014512658312869852\n",
      "Iteration: 5884 lambda_k: 1 Loss: 0.0014512659519023805\n",
      "Iteration: 5885 lambda_k: 1 Loss: 0.001451266071776951\n",
      "Iteration: 5886 lambda_k: 1 Loss: 0.0014512661909153205\n",
      "Iteration: 5887 lambda_k: 1 Loss: 0.0014512663093220637\n",
      "Iteration: 5888 lambda_k: 1 Loss: 0.0014512664270016316\n",
      "Iteration: 5889 lambda_k: 1 Loss: 0.001451266543958574\n",
      "Iteration: 5890 lambda_k: 1 Loss: 0.0014512666601973307\n",
      "Iteration: 5891 lambda_k: 1 Loss: 0.0014512667757223669\n",
      "Iteration: 5892 lambda_k: 1 Loss: 0.0014512668905380507\n",
      "Iteration: 5893 lambda_k: 1 Loss: 0.0014512670046488221\n",
      "Iteration: 5894 lambda_k: 1 Loss: 0.0014512671180590111\n",
      "Iteration: 5895 lambda_k: 1 Loss: 0.0014512672307729216\n",
      "Iteration: 5896 lambda_k: 1 Loss: 0.0014512673427949266\n",
      "Iteration: 5897 lambda_k: 1 Loss: 0.0014512674541292691\n",
      "Iteration: 5898 lambda_k: 1 Loss: 0.0014512675647801764\n",
      "Iteration: 5899 lambda_k: 1 Loss: 0.001451267674751913\n",
      "Iteration: 5900 lambda_k: 1 Loss: 0.0014512677840486335\n",
      "Iteration: 5901 lambda_k: 1 Loss: 0.0014512678926745656\n",
      "Iteration: 5902 lambda_k: 1 Loss: 0.0014512680006338477\n",
      "Iteration: 5903 lambda_k: 1 Loss: 0.0014512681079305936\n",
      "Iteration: 5904 lambda_k: 1 Loss: 0.0014512682145688706\n",
      "Iteration: 5905 lambda_k: 1 Loss: 0.0014512683205527674\n",
      "Iteration: 5906 lambda_k: 1 Loss: 0.0014512684258863057\n",
      "Iteration: 5907 lambda_k: 1 Loss: 0.0014512685305735473\n",
      "Iteration: 5908 lambda_k: 1 Loss: 0.0014512686346184929\n",
      "Iteration: 5909 lambda_k: 1 Loss: 0.0014512687380250605\n",
      "Iteration: 5910 lambda_k: 1 Loss: 0.0014512688407972022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5911 lambda_k: 1 Loss: 0.001451268942938869\n",
      "Iteration: 5912 lambda_k: 1 Loss: 0.0014512690444539508\n",
      "Iteration: 5913 lambda_k: 1 Loss: 0.0014512691453463197\n",
      "Iteration: 5914 lambda_k: 1 Loss: 0.0014512692456198361\n",
      "Iteration: 5915 lambda_k: 1 Loss: 0.0014512693452782886\n",
      "Iteration: 5916 lambda_k: 1 Loss: 0.0014512694443255302\n",
      "Iteration: 5917 lambda_k: 1 Loss: 0.0014512695427652896\n",
      "Iteration: 5918 lambda_k: 1 Loss: 0.0014512696406013656\n",
      "Iteration: 5919 lambda_k: 1 Loss: 0.0014512697378374407\n",
      "Iteration: 5920 lambda_k: 1 Loss: 0.0014512698344772612\n",
      "Iteration: 5921 lambda_k: 1 Loss: 0.001451269930524479\n",
      "Iteration: 5922 lambda_k: 1 Loss: 0.0014512700259827706\n",
      "Iteration: 5923 lambda_k: 1 Loss: 0.0014512701208557936\n",
      "Iteration: 5924 lambda_k: 1 Loss: 0.001451270215147149\n",
      "Iteration: 5925 lambda_k: 1 Loss: 0.001451270308860417\n",
      "Iteration: 5926 lambda_k: 1 Loss: 0.001451270401999184\n",
      "Iteration: 5927 lambda_k: 1 Loss: 0.0014512704945670008\n",
      "Iteration: 5928 lambda_k: 1 Loss: 0.0014512705865673806\n",
      "Iteration: 5929 lambda_k: 1 Loss: 0.0014512706780038303\n",
      "Iteration: 5930 lambda_k: 1 Loss: 0.0014512707688798668\n",
      "Iteration: 5931 lambda_k: 1 Loss: 0.001451270859198922\n",
      "Iteration: 5932 lambda_k: 1 Loss: 0.0014512709489644551\n",
      "Iteration: 5933 lambda_k: 1 Loss: 0.0014512710381798645\n",
      "Iteration: 5934 lambda_k: 1 Loss: 0.0014512711268485441\n",
      "Iteration: 5935 lambda_k: 1 Loss: 0.0014512712149738637\n",
      "Iteration: 5936 lambda_k: 1 Loss: 0.0014512713025591915\n",
      "Iteration: 5937 lambda_k: 1 Loss: 0.0014512713896079003\n",
      "Iteration: 5938 lambda_k: 1 Loss: 0.00145127147612326\n",
      "Iteration: 5939 lambda_k: 1 Loss: 0.001451271562108583\n",
      "Iteration: 5940 lambda_k: 1 Loss: 0.0014512716475671647\n",
      "Iteration: 5941 lambda_k: 1 Loss: 0.0014512717325022037\n",
      "Iteration: 5942 lambda_k: 1 Loss: 0.0014512718169169855\n",
      "Iteration: 5943 lambda_k: 1 Loss: 0.0014512719008146872\n",
      "Iteration: 5944 lambda_k: 1 Loss: 0.0014512719841985125\n",
      "Iteration: 5945 lambda_k: 1 Loss: 0.0014512720670716425\n",
      "Iteration: 5946 lambda_k: 1 Loss: 0.001451272149437213\n",
      "Iteration: 5947 lambda_k: 1 Loss: 0.001451272231298362\n",
      "Iteration: 5948 lambda_k: 1 Loss: 0.0014512723126582344\n",
      "Iteration: 5949 lambda_k: 1 Loss: 0.001451272393519848\n",
      "Iteration: 5950 lambda_k: 1 Loss: 0.0014512724738863735\n",
      "Iteration: 5951 lambda_k: 1 Loss: 0.0014512725537607967\n",
      "Iteration: 5952 lambda_k: 1 Loss: 0.0014512726331461893\n",
      "Iteration: 5953 lambda_k: 1 Loss: 0.0014512727120455737\n",
      "Iteration: 5954 lambda_k: 1 Loss: 0.0014512727904619485\n",
      "Iteration: 5955 lambda_k: 1 Loss: 0.0014512728683982908\n",
      "Iteration: 5956 lambda_k: 1 Loss: 0.0014512729458576019\n",
      "Iteration: 5957 lambda_k: 1 Loss: 0.0014512730228427374\n",
      "Iteration: 5958 lambda_k: 1 Loss: 0.0014512730993567133\n",
      "Iteration: 5959 lambda_k: 1 Loss: 0.0014512731754024164\n",
      "Iteration: 5960 lambda_k: 1 Loss: 0.0014512732509826798\n",
      "Iteration: 5961 lambda_k: 1 Loss: 0.0014512733261004304\n",
      "Iteration: 5962 lambda_k: 1 Loss: 0.0014512734007584996\n",
      "Iteration: 5963 lambda_k: 1 Loss: 0.0014512734749597463\n",
      "Iteration: 5964 lambda_k: 1 Loss: 0.0014512735487069567\n",
      "Iteration: 5965 lambda_k: 1 Loss: 0.0014512736220029874\n",
      "Iteration: 5966 lambda_k: 1 Loss: 0.0014512736948505604\n",
      "Iteration: 5967 lambda_k: 1 Loss: 0.001451273767252467\n",
      "Iteration: 5968 lambda_k: 1 Loss: 0.0014512738392114936\n",
      "Iteration: 5969 lambda_k: 1 Loss: 0.0014512739107303294\n",
      "Iteration: 5970 lambda_k: 1 Loss: 0.0014512739818116703\n",
      "Iteration: 5971 lambda_k: 1 Loss: 0.0014512740524582901\n",
      "Iteration: 5972 lambda_k: 1 Loss: 0.0014512741226728133\n",
      "Iteration: 5973 lambda_k: 1 Loss: 0.0014512741924579177\n",
      "Iteration: 5974 lambda_k: 1 Loss: 0.0014512742618162547\n",
      "Iteration: 5975 lambda_k: 1 Loss: 0.0014512743307504693\n",
      "Iteration: 5976 lambda_k: 1 Loss: 0.0014512743992631722\n",
      "Iteration: 5977 lambda_k: 1 Loss: 0.0014512744673569567\n",
      "Iteration: 5978 lambda_k: 1 Loss: 0.0014512745350344016\n",
      "Iteration: 5979 lambda_k: 1 Loss: 0.001451274602298101\n",
      "Iteration: 5980 lambda_k: 1 Loss: 0.0014512746691505924\n",
      "Iteration: 5981 lambda_k: 1 Loss: 0.0014512747355943968\n",
      "Iteration: 5982 lambda_k: 1 Loss: 0.0014512748016320511\n",
      "Iteration: 5983 lambda_k: 1 Loss: 0.0014512748672660376\n",
      "Iteration: 5984 lambda_k: 1 Loss: 0.0014512749324988553\n",
      "Iteration: 5985 lambda_k: 1 Loss: 0.0014512749973329894\n",
      "Iteration: 5986 lambda_k: 1 Loss: 0.0014512750617708984\n",
      "Iteration: 5987 lambda_k: 1 Loss: 0.001451275125815046\n",
      "Iteration: 5988 lambda_k: 1 Loss: 0.0014512751894678107\n",
      "Iteration: 5989 lambda_k: 1 Loss: 0.001451275252731659\n",
      "Iteration: 5990 lambda_k: 1 Loss: 0.0014512753156089486\n",
      "Iteration: 5991 lambda_k: 1 Loss: 0.001451275378102102\n",
      "Iteration: 5992 lambda_k: 1 Loss: 0.0014512754402134494\n",
      "Iteration: 5993 lambda_k: 1 Loss: 0.0014512755019453448\n",
      "Iteration: 5994 lambda_k: 1 Loss: 0.0014512755633001794\n",
      "Iteration: 5995 lambda_k: 1 Loss: 0.0014512756242801973\n",
      "Iteration: 5996 lambda_k: 1 Loss: 0.0014512756848877929\n",
      "Iteration: 5997 lambda_k: 1 Loss: 0.001451275745125171\n",
      "Iteration: 5998 lambda_k: 1 Loss: 0.0014512758049946889\n",
      "Iteration: 5999 lambda_k: 1 Loss: 0.0014512758644985898\n",
      "Iteration: 6000 lambda_k: 1 Loss: 0.0014512759236391228\n",
      "Iteration: 6001 lambda_k: 1 Loss: 0.001451275982418517\n",
      "Iteration: 6002 lambda_k: 1 Loss: 0.0014512760408390295\n",
      "Iteration: 6003 lambda_k: 1 Loss: 0.001451276098902874\n",
      "Iteration: 6004 lambda_k: 1 Loss: 0.0014512761566122195\n",
      "Iteration: 6005 lambda_k: 1 Loss: 0.001451276213969254\n",
      "Iteration: 6006 lambda_k: 1 Loss: 0.0014512762709761467\n",
      "Iteration: 6007 lambda_k: 1 Loss: 0.0014512763276350643\n",
      "Iteration: 6008 lambda_k: 1 Loss: 0.0014512763839481736\n",
      "Iteration: 6009 lambda_k: 1 Loss: 0.001451276439917562\n",
      "Iteration: 6010 lambda_k: 1 Loss: 0.0014512764955453682\n",
      "Iteration: 6011 lambda_k: 1 Loss: 0.0014512765508337363\n",
      "Iteration: 6012 lambda_k: 1 Loss: 0.0014512766057847041\n",
      "Iteration: 6013 lambda_k: 1 Loss: 0.0014512766604003417\n",
      "Iteration: 6014 lambda_k: 1 Loss: 0.001451276714682755\n",
      "Iteration: 6015 lambda_k: 1 Loss: 0.0014512767686339638\n",
      "Iteration: 6016 lambda_k: 1 Loss: 0.0014512768222560424\n",
      "Iteration: 6017 lambda_k: 1 Loss: 0.0014512768755510189\n",
      "Iteration: 6018 lambda_k: 1 Loss: 0.0014512769285208555\n",
      "Iteration: 6019 lambda_k: 1 Loss: 0.0014512769811675786\n",
      "Iteration: 6020 lambda_k: 1 Loss: 0.0014512770334932082\n",
      "Iteration: 6021 lambda_k: 1 Loss: 0.001451277085499734\n",
      "Iteration: 6022 lambda_k: 1 Loss: 0.0014512771371890851\n",
      "Iteration: 6023 lambda_k: 1 Loss: 0.0014512771885631887\n",
      "Iteration: 6024 lambda_k: 1 Loss: 0.0014512772396240124\n",
      "Iteration: 6025 lambda_k: 1 Loss: 0.0014512772903734915\n",
      "Iteration: 6026 lambda_k: 1 Loss: 0.0014512773408135355\n",
      "Iteration: 6027 lambda_k: 1 Loss: 0.0014512773909460639\n",
      "Iteration: 6028 lambda_k: 1 Loss: 0.001451277440772967\n",
      "Iteration: 6029 lambda_k: 1 Loss: 0.0014512774902961424\n",
      "Iteration: 6030 lambda_k: 1 Loss: 0.0014512775395174294\n",
      "Iteration: 6031 lambda_k: 1 Loss: 0.001451277588438688\n",
      "Iteration: 6032 lambda_k: 1 Loss: 0.001451277637061771\n",
      "Iteration: 6033 lambda_k: 1 Loss: 0.0014512776853885109\n",
      "Iteration: 6034 lambda_k: 1 Loss: 0.0014512777334207852\n",
      "Iteration: 6035 lambda_k: 1 Loss: 0.0014512777811603202\n",
      "Iteration: 6036 lambda_k: 1 Loss: 0.0014512778286089824\n",
      "Iteration: 6037 lambda_k: 1 Loss: 0.001451277875768526\n",
      "Iteration: 6038 lambda_k: 1 Loss: 0.0014512779226407305\n",
      "Iteration: 6039 lambda_k: 1 Loss: 0.0014512779692273674\n",
      "Iteration: 6040 lambda_k: 1 Loss: 0.001451278015530219\n",
      "Iteration: 6041 lambda_k: 1 Loss: 0.0014512780615510016\n",
      "Iteration: 6042 lambda_k: 1 Loss: 0.0014512781072914857\n",
      "Iteration: 6043 lambda_k: 1 Loss: 0.0014512781527533877\n",
      "Iteration: 6044 lambda_k: 1 Loss: 0.0014512781979383808\n",
      "Iteration: 6045 lambda_k: 1 Loss: 0.0014512782428482203\n",
      "Iteration: 6046 lambda_k: 1 Loss: 0.001451278287484579\n",
      "Iteration: 6047 lambda_k: 1 Loss: 0.0014512783318491665\n",
      "Iteration: 6048 lambda_k: 1 Loss: 0.001451278375943592\n",
      "Iteration: 6049 lambda_k: 1 Loss: 0.001451278419769551\n",
      "Iteration: 6050 lambda_k: 1 Loss: 0.001451278463328669\n",
      "Iteration: 6051 lambda_k: 1 Loss: 0.0014512785066226046\n",
      "Iteration: 6052 lambda_k: 1 Loss: 0.0014512785496530098\n",
      "Iteration: 6053 lambda_k: 1 Loss: 0.0014512785924215212\n",
      "Iteration: 6054 lambda_k: 1 Loss: 0.0014512786349296898\n",
      "Iteration: 6055 lambda_k: 1 Loss: 0.0014512786771791943\n",
      "Iteration: 6056 lambda_k: 1 Loss: 0.001451278719171569\n",
      "Iteration: 6057 lambda_k: 1 Loss: 0.0014512787609083902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6058 lambda_k: 1 Loss: 0.0014512788023912575\n",
      "Iteration: 6059 lambda_k: 1 Loss: 0.0014512788436217255\n",
      "Iteration: 6060 lambda_k: 1 Loss: 0.0014512788846013308\n",
      "Iteration: 6061 lambda_k: 1 Loss: 0.0014512789253316278\n",
      "Iteration: 6062 lambda_k: 1 Loss: 0.001451278965814154\n",
      "Iteration: 6063 lambda_k: 1 Loss: 0.001451279006050432\n",
      "Iteration: 6064 lambda_k: 1 Loss: 0.0014512790460419922\n",
      "Iteration: 6065 lambda_k: 1 Loss: 0.0014512790857902926\n",
      "Iteration: 6066 lambda_k: 1 Loss: 0.0014512791252968456\n",
      "Iteration: 6067 lambda_k: 1 Loss: 0.001451279164563165\n",
      "Iteration: 6068 lambda_k: 1 Loss: 0.0014512792035906985\n",
      "Iteration: 6069 lambda_k: 1 Loss: 0.0014512792423808842\n",
      "Iteration: 6070 lambda_k: 1 Loss: 0.0014512792809352525\n",
      "Iteration: 6071 lambda_k: 1 Loss: 0.0014512793192552372\n",
      "Iteration: 6072 lambda_k: 1 Loss: 0.0014512793573422116\n",
      "Iteration: 6073 lambda_k: 1 Loss: 0.0014512793951976918\n",
      "Iteration: 6074 lambda_k: 1 Loss: 0.001451279432823057\n",
      "Iteration: 6075 lambda_k: 1 Loss: 0.0014512794702197342\n",
      "Iteration: 6076 lambda_k: 1 Loss: 0.0014512795073891391\n",
      "Iteration: 6077 lambda_k: 1 Loss: 0.0014512795443326555\n",
      "Iteration: 6078 lambda_k: 1 Loss: 0.0014512795810516692\n",
      "Iteration: 6079 lambda_k: 1 Loss: 0.0014512796175475365\n",
      "Iteration: 6080 lambda_k: 1 Loss: 0.0014512796538217057\n",
      "Iteration: 6081 lambda_k: 1 Loss: 0.0014512796898754608\n",
      "Iteration: 6082 lambda_k: 1 Loss: 0.0014512797257101536\n",
      "Iteration: 6083 lambda_k: 1 Loss: 0.0014512797613271693\n",
      "Iteration: 6084 lambda_k: 1 Loss: 0.0014512797967278328\n",
      "Iteration: 6085 lambda_k: 1 Loss: 0.0014512798319134645\n",
      "Iteration: 6086 lambda_k: 1 Loss: 0.0014512798668854007\n",
      "Iteration: 6087 lambda_k: 1 Loss: 0.0014512799016449438\n",
      "Iteration: 6088 lambda_k: 1 Loss: 0.001451279936193433\n",
      "Iteration: 6089 lambda_k: 1 Loss: 0.0014512799705321167\n",
      "Iteration: 6090 lambda_k: 1 Loss: 0.0014512800046622965\n",
      "Iteration: 6091 lambda_k: 1 Loss: 0.00145128003858523\n",
      "Iteration: 6092 lambda_k: 1 Loss: 0.0014512800723022329\n",
      "Iteration: 6093 lambda_k: 1 Loss: 0.0014512801058145502\n",
      "Iteration: 6094 lambda_k: 1 Loss: 0.0014512801391234277\n",
      "Iteration: 6095 lambda_k: 1 Loss: 0.0014512801722301727\n",
      "Iteration: 6096 lambda_k: 1 Loss: 0.0014512802051359739\n",
      "Iteration: 6097 lambda_k: 1 Loss: 0.0014512802378420526\n",
      "Iteration: 6098 lambda_k: 1 Loss: 0.0014512802703496735\n",
      "Iteration: 6099 lambda_k: 1 Loss: 0.00145128030265999\n",
      "Iteration: 6100 lambda_k: 1 Loss: 0.001451280334774297\n",
      "Iteration: 6101 lambda_k: 1 Loss: 0.0014512803666937367\n",
      "Iteration: 6102 lambda_k: 1 Loss: 0.001451280398419529\n",
      "Iteration: 6103 lambda_k: 1 Loss: 0.0014512804299528755\n",
      "Iteration: 6104 lambda_k: 1 Loss: 0.001451280461294928\n",
      "Iteration: 6105 lambda_k: 1 Loss: 0.0014512804924468711\n",
      "Iteration: 6106 lambda_k: 1 Loss: 0.0014512805234098893\n",
      "Iteration: 6107 lambda_k: 1 Loss: 0.0014512805541851332\n",
      "Iteration: 6108 lambda_k: 1 Loss: 0.0014512805847737553\n",
      "Iteration: 6109 lambda_k: 1 Loss: 0.0014512806151768946\n",
      "Iteration: 6110 lambda_k: 1 Loss: 0.001451280645395678\n",
      "Iteration: 6111 lambda_k: 1 Loss: 0.0014512806754312301\n",
      "Iteration: 6112 lambda_k: 1 Loss: 0.001451280705284717\n",
      "Iteration: 6113 lambda_k: 1 Loss: 0.0014512807349572208\n",
      "Iteration: 6114 lambda_k: 1 Loss: 0.0014512807644498375\n",
      "Iteration: 6115 lambda_k: 1 Loss: 0.0014512807937637256\n",
      "Iteration: 6116 lambda_k: 1 Loss: 0.0014512808228999332\n",
      "Iteration: 6117 lambda_k: 1 Loss: 0.0014512808518595478\n",
      "Iteration: 6118 lambda_k: 1 Loss: 0.0014512808806436972\n",
      "Iteration: 6119 lambda_k: 1 Loss: 0.0014512809092534615\n",
      "Iteration: 6120 lambda_k: 1 Loss: 0.0014512809376898135\n",
      "Iteration: 6121 lambda_k: 1 Loss: 0.0014512809659539155\n",
      "Iteration: 6122 lambda_k: 1 Loss: 0.0014512809940467855\n",
      "Iteration: 6123 lambda_k: 1 Loss: 0.001451281021969483\n",
      "Iteration: 6124 lambda_k: 1 Loss: 0.0014512810497230594\n",
      "Iteration: 6125 lambda_k: 1 Loss: 0.00145128107730851\n",
      "Iteration: 6126 lambda_k: 1 Loss: 0.0014512811047269155\n",
      "Iteration: 6127 lambda_k: 1 Loss: 0.0014512811319792966\n",
      "Iteration: 6128 lambda_k: 1 Loss: 0.0014512811590666264\n",
      "Iteration: 6129 lambda_k: 1 Loss: 0.0014512811859899223\n",
      "Iteration: 6130 lambda_k: 1 Loss: 0.0014512812127502209\n",
      "Iteration: 6131 lambda_k: 1 Loss: 0.0014512812393485323\n",
      "Iteration: 6132 lambda_k: 1 Loss: 0.0014512812657858226\n",
      "Iteration: 6133 lambda_k: 1 Loss: 0.0014512812920630645\n",
      "Iteration: 6134 lambda_k: 1 Loss: 0.0014512813181812973\n",
      "Iteration: 6135 lambda_k: 1 Loss: 0.001451281344141425\n",
      "Iteration: 6136 lambda_k: 1 Loss: 0.0014512813699444304\n",
      "Iteration: 6137 lambda_k: 1 Loss: 0.0014512813955913238\n",
      "Iteration: 6138 lambda_k: 1 Loss: 0.0014512814210830317\n",
      "Iteration: 6139 lambda_k: 1 Loss: 0.0014512814464204703\n",
      "Iteration: 6140 lambda_k: 1 Loss: 0.0014512814716046325\n",
      "Iteration: 6141 lambda_k: 1 Loss: 0.0014512814966364774\n",
      "Iteration: 6142 lambda_k: 1 Loss: 0.0014512815215168746\n",
      "Iteration: 6143 lambda_k: 1 Loss: 0.0014512815462467434\n",
      "Iteration: 6144 lambda_k: 1 Loss: 0.0014512815708270684\n",
      "Iteration: 6145 lambda_k: 1 Loss: 0.0014512815952587217\n",
      "Iteration: 6146 lambda_k: 1 Loss: 0.0014512816195426557\n",
      "Iteration: 6147 lambda_k: 1 Loss: 0.0014512816436797209\n",
      "Iteration: 6148 lambda_k: 1 Loss: 0.0014512816676708728\n",
      "Iteration: 6149 lambda_k: 1 Loss: 0.0014512816915170116\n",
      "Iteration: 6150 lambda_k: 1 Loss: 0.0014512817152189652\n",
      "Iteration: 6151 lambda_k: 1 Loss: 0.0014512817387776593\n",
      "Iteration: 6152 lambda_k: 1 Loss: 0.0014512817621939074\n",
      "Iteration: 6153 lambda_k: 1 Loss: 0.0014512817854686623\n",
      "Iteration: 6154 lambda_k: 1 Loss: 0.0014512818086027447\n",
      "Iteration: 6155 lambda_k: 1 Loss: 0.00145128183159705\n",
      "Iteration: 6156 lambda_k: 1 Loss: 0.0014512818544523986\n",
      "Iteration: 6157 lambda_k: 1 Loss: 0.001451281877169662\n",
      "Iteration: 6158 lambda_k: 1 Loss: 0.0014512818997496377\n",
      "Iteration: 6159 lambda_k: 1 Loss: 0.0014512819221932083\n",
      "Iteration: 6160 lambda_k: 1 Loss: 0.001451281944501225\n",
      "Iteration: 6161 lambda_k: 1 Loss: 0.0014512819666744792\n",
      "Iteration: 6162 lambda_k: 1 Loss: 0.0014512819887138066\n",
      "Iteration: 6163 lambda_k: 1 Loss: 0.0014512820106200612\n",
      "Iteration: 6164 lambda_k: 1 Loss: 0.0014512820323940106\n",
      "Iteration: 6165 lambda_k: 1 Loss: 0.0014512820540364638\n",
      "Iteration: 6166 lambda_k: 1 Loss: 0.0014512820755482872\n",
      "Iteration: 6167 lambda_k: 1 Loss: 0.0014512820969301982\n",
      "Iteration: 6168 lambda_k: 1 Loss: 0.001451282118183051\n",
      "Iteration: 6169 lambda_k: 1 Loss: 0.0014512821393076144\n",
      "Iteration: 6170 lambda_k: 1 Loss: 0.0014512821603046874\n",
      "Iteration: 6171 lambda_k: 1 Loss: 0.0014512821811750458\n",
      "Iteration: 6172 lambda_k: 1 Loss: 0.0014512822019194392\n",
      "Iteration: 6173 lambda_k: 1 Loss: 0.0014512822225386287\n",
      "Iteration: 6174 lambda_k: 1 Loss: 0.001451282243033438\n",
      "Iteration: 6175 lambda_k: 1 Loss: 0.0014512822634045615\n",
      "Iteration: 6176 lambda_k: 1 Loss: 0.001451282283652817\n",
      "Iteration: 6177 lambda_k: 1 Loss: 0.0014512823037788755\n",
      "Iteration: 6178 lambda_k: 1 Loss: 0.0014512823237835509\n",
      "Iteration: 6179 lambda_k: 1 Loss: 0.0014512823436675672\n",
      "Iteration: 6180 lambda_k: 1 Loss: 0.0014512823634316625\n",
      "Iteration: 6181 lambda_k: 1 Loss: 0.0014512823830765678\n",
      "Iteration: 6182 lambda_k: 1 Loss: 0.001451282402603048\n",
      "Iteration: 6183 lambda_k: 1 Loss: 0.0014512824220117488\n",
      "Iteration: 6184 lambda_k: 1 Loss: 0.001451282441303439\n",
      "Iteration: 6185 lambda_k: 1 Loss: 0.001451282460478844\n",
      "Iteration: 6186 lambda_k: 1 Loss: 0.0014512824795386318\n",
      "Iteration: 6187 lambda_k: 1 Loss: 0.00145128249848355\n",
      "Iteration: 6188 lambda_k: 1 Loss: 0.0014512825173142885\n",
      "Iteration: 6189 lambda_k: 1 Loss: 0.0014512825360315056\n",
      "Iteration: 6190 lambda_k: 1 Loss: 0.0014512825546359705\n",
      "Iteration: 6191 lambda_k: 1 Loss: 0.0014512825731283238\n",
      "Iteration: 6192 lambda_k: 1 Loss: 0.001451282591509251\n",
      "Iteration: 6193 lambda_k: 1 Loss: 0.0014512826097794525\n",
      "Iteration: 6194 lambda_k: 1 Loss: 0.0014512826279395807\n",
      "Iteration: 6195 lambda_k: 1 Loss: 0.001451282645990324\n",
      "Iteration: 6196 lambda_k: 1 Loss: 0.0014512826639323773\n",
      "Iteration: 6197 lambda_k: 1 Loss: 0.0014512826817663436\n",
      "Iteration: 6198 lambda_k: 1 Loss: 0.0014512826994928971\n",
      "Iteration: 6199 lambda_k: 1 Loss: 0.001451282717112714\n",
      "Iteration: 6200 lambda_k: 1 Loss: 0.0014512827346264666\n",
      "Iteration: 6201 lambda_k: 1 Loss: 0.0014512827520347421\n",
      "Iteration: 6202 lambda_k: 1 Loss: 0.001451282769338222\n",
      "Iteration: 6203 lambda_k: 1 Loss: 0.0014512827865375657\n",
      "Iteration: 6204 lambda_k: 1 Loss: 0.0014512828036333844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6205 lambda_k: 1 Loss: 0.00145128282062628\n",
      "Iteration: 6206 lambda_k: 1 Loss: 0.0014512828375169166\n",
      "Iteration: 6207 lambda_k: 1 Loss: 0.0014512828543059373\n",
      "Iteration: 6208 lambda_k: 1 Loss: 0.0014512828709939457\n",
      "Iteration: 6209 lambda_k: 1 Loss: 0.0014512828875815364\n",
      "Iteration: 6210 lambda_k: 1 Loss: 0.0014512829040693406\n",
      "Iteration: 6211 lambda_k: 1 Loss: 0.0014512829204579528\n",
      "Iteration: 6212 lambda_k: 1 Loss: 0.001451282936747968\n",
      "Iteration: 6213 lambda_k: 1 Loss: 0.001451282952940054\n",
      "Iteration: 6214 lambda_k: 1 Loss: 0.0014512829690347356\n",
      "Iteration: 6215 lambda_k: 1 Loss: 0.0014512829850326279\n",
      "Iteration: 6216 lambda_k: 1 Loss: 0.0014512830009343327\n",
      "Iteration: 6217 lambda_k: 1 Loss: 0.0014512830167404272\n",
      "Iteration: 6218 lambda_k: 1 Loss: 0.0014512830324515025\n",
      "Iteration: 6219 lambda_k: 1 Loss: 0.0014512830480681131\n",
      "Iteration: 6220 lambda_k: 1 Loss: 0.0014512830635908546\n",
      "Iteration: 6221 lambda_k: 1 Loss: 0.001451283079020339\n",
      "Iteration: 6222 lambda_k: 1 Loss: 0.001451283094357054\n",
      "Iteration: 6223 lambda_k: 1 Loss: 0.0014512831096016219\n",
      "Iteration: 6224 lambda_k: 1 Loss: 0.0014512831247545789\n",
      "Iteration: 6225 lambda_k: 1 Loss: 0.0014512831398165167\n",
      "Iteration: 6226 lambda_k: 1 Loss: 0.0014512831547879698\n",
      "Iteration: 6227 lambda_k: 1 Loss: 0.0014512831696694762\n",
      "Iteration: 6228 lambda_k: 1 Loss: 0.0014512831844616025\n",
      "Iteration: 6229 lambda_k: 1 Loss: 0.0014512831991648541\n",
      "Iteration: 6230 lambda_k: 1 Loss: 0.0014512832137798114\n",
      "Iteration: 6231 lambda_k: 1 Loss: 0.0014512832283070018\n",
      "Iteration: 6232 lambda_k: 1 Loss: 0.0014512832427469684\n",
      "Iteration: 6233 lambda_k: 1 Loss: 0.001451283257100232\n",
      "Iteration: 6234 lambda_k: 1 Loss: 0.0014512832713673373\n",
      "Iteration: 6235 lambda_k: 1 Loss: 0.0014512832855487813\n",
      "Iteration: 6236 lambda_k: 1 Loss: 0.001451283299645139\n",
      "Iteration: 6237 lambda_k: 1 Loss: 0.0014512833136568798\n",
      "Iteration: 6238 lambda_k: 1 Loss: 0.001451283327584536\n",
      "Iteration: 6239 lambda_k: 1 Loss: 0.0014512833414285888\n",
      "Iteration: 6240 lambda_k: 1 Loss: 0.001451283355189628\n",
      "Iteration: 6241 lambda_k: 1 Loss: 0.001451283368868079\n",
      "Iteration: 6242 lambda_k: 1 Loss: 0.0014512833824644779\n",
      "Iteration: 6243 lambda_k: 1 Loss: 0.0014512833959792687\n",
      "Iteration: 6244 lambda_k: 1 Loss: 0.0014512834094130222\n",
      "Iteration: 6245 lambda_k: 1 Loss: 0.0014512834227661951\n",
      "Iteration: 6246 lambda_k: 1 Loss: 0.0014512834360392909\n",
      "Iteration: 6247 lambda_k: 1 Loss: 0.001451283449232825\n",
      "Iteration: 6248 lambda_k: 1 Loss: 0.0014512834623472426\n",
      "Iteration: 6249 lambda_k: 1 Loss: 0.0014512834753830335\n",
      "Iteration: 6250 lambda_k: 1 Loss: 0.0014512834883406687\n",
      "Iteration: 6251 lambda_k: 1 Loss: 0.0014512835012206297\n",
      "Iteration: 6252 lambda_k: 1 Loss: 0.0014512835140234007\n",
      "Iteration: 6253 lambda_k: 1 Loss: 0.0014512835267494301\n",
      "Iteration: 6254 lambda_k: 1 Loss: 0.0014512835393991875\n",
      "Iteration: 6255 lambda_k: 1 Loss: 0.0014512835519731827\n",
      "Iteration: 6256 lambda_k: 1 Loss: 0.00145128356447183\n",
      "Iteration: 6257 lambda_k: 1 Loss: 0.0014512835768956042\n",
      "Iteration: 6258 lambda_k: 1 Loss: 0.0014512835892449628\n",
      "Iteration: 6259 lambda_k: 1 Loss: 0.001451283601520328\n",
      "Iteration: 6260 lambda_k: 1 Loss: 0.0014512836137221456\n",
      "Iteration: 6261 lambda_k: 1 Loss: 0.0014512836258508996\n",
      "Iteration: 6262 lambda_k: 1 Loss: 0.0014512836379070228\n",
      "Iteration: 6263 lambda_k: 1 Loss: 0.0014512836498909868\n",
      "Iteration: 6264 lambda_k: 1 Loss: 0.0014512836618031685\n",
      "Iteration: 6265 lambda_k: 1 Loss: 0.0014512836736440353\n",
      "Iteration: 6266 lambda_k: 1 Loss: 0.0014512836854140665\n",
      "Iteration: 6267 lambda_k: 1 Loss: 0.0014512836971136323\n",
      "Iteration: 6268 lambda_k: 1 Loss: 0.001451283708743159\n",
      "Iteration: 6269 lambda_k: 1 Loss: 0.0014512837203031006\n",
      "Iteration: 6270 lambda_k: 1 Loss: 0.0014512837317938918\n",
      "Iteration: 6271 lambda_k: 1 Loss: 0.0014512837432159233\n",
      "Iteration: 6272 lambda_k: 1 Loss: 0.0014512837545696282\n",
      "Iteration: 6273 lambda_k: 1 Loss: 0.0014512837658554\n",
      "Iteration: 6274 lambda_k: 1 Loss: 0.0014512837770736726\n",
      "Iteration: 6275 lambda_k: 1 Loss: 0.001451283788224837\n",
      "Iteration: 6276 lambda_k: 1 Loss: 0.0014512837993093262\n",
      "Iteration: 6277 lambda_k: 1 Loss: 0.001451283810327546\n",
      "Iteration: 6278 lambda_k: 1 Loss: 0.0014512838212798833\n",
      "Iteration: 6279 lambda_k: 1 Loss: 0.0014512838321667156\n",
      "Iteration: 6280 lambda_k: 1 Loss: 0.0014512838429884551\n",
      "Iteration: 6281 lambda_k: 1 Loss: 0.0014512838537455064\n",
      "Iteration: 6282 lambda_k: 1 Loss: 0.0014512838644382812\n",
      "Iteration: 6283 lambda_k: 1 Loss: 0.0014512838750671405\n",
      "Iteration: 6284 lambda_k: 1 Loss: 0.0014512838856325233\n",
      "Iteration: 6285 lambda_k: 1 Loss: 0.001451283896134751\n",
      "Iteration: 6286 lambda_k: 1 Loss: 0.0014512839065741938\n",
      "Iteration: 6287 lambda_k: 1 Loss: 0.0014512839169513057\n",
      "Iteration: 6288 lambda_k: 1 Loss: 0.0014512839272664425\n",
      "Iteration: 6289 lambda_k: 1 Loss: 0.0014512839375199343\n",
      "Iteration: 6290 lambda_k: 1 Loss: 0.0014512839477122126\n",
      "Iteration: 6291 lambda_k: 1 Loss: 0.001451283957843621\n",
      "Iteration: 6292 lambda_k: 1 Loss: 0.0014512839679145613\n",
      "Iteration: 6293 lambda_k: 1 Loss: 0.0014512839779253692\n",
      "Iteration: 6294 lambda_k: 1 Loss: 0.0014512839878763864\n",
      "Iteration: 6295 lambda_k: 1 Loss: 0.0014512839977680143\n",
      "Iteration: 6296 lambda_k: 1 Loss: 0.0014512840076006017\n",
      "Iteration: 6297 lambda_k: 1 Loss: 0.0014512840173744668\n",
      "Iteration: 6298 lambda_k: 1 Loss: 0.001451284027090056\n",
      "Iteration: 6299 lambda_k: 1 Loss: 0.0014512840367476485\n",
      "Iteration: 6300 lambda_k: 1 Loss: 0.0014512840463476303\n",
      "Iteration: 6301 lambda_k: 1 Loss: 0.0014512840558903214\n",
      "Iteration: 6302 lambda_k: 1 Loss: 0.001451284065376071\n",
      "Iteration: 6303 lambda_k: 1 Loss: 0.0014512840748052895\n",
      "Iteration: 6304 lambda_k: 1 Loss: 0.0014512840841782568\n",
      "Iteration: 6305 lambda_k: 1 Loss: 0.0014512840934953117\n",
      "Iteration: 6306 lambda_k: 1 Loss: 0.0014512841027568084\n",
      "Iteration: 6307 lambda_k: 1 Loss: 0.0014512841119631002\n",
      "Iteration: 6308 lambda_k: 1 Loss: 0.0014512841211145157\n",
      "Iteration: 6309 lambda_k: 1 Loss: 0.0014512841302113466\n",
      "Iteration: 6310 lambda_k: 1 Loss: 0.001451284139253971\n",
      "Iteration: 6311 lambda_k: 1 Loss: 0.0014512841482427056\n",
      "Iteration: 6312 lambda_k: 1 Loss: 0.001451284157177856\n",
      "Iteration: 6313 lambda_k: 1 Loss: 0.0014512841660597594\n",
      "Iteration: 6314 lambda_k: 1 Loss: 0.0014512841748887946\n",
      "Iteration: 6315 lambda_k: 1 Loss: 0.0014512841836652288\n",
      "Iteration: 6316 lambda_k: 1 Loss: 0.0014512841923893468\n",
      "Iteration: 6317 lambda_k: 1 Loss: 0.001451284201061517\n",
      "Iteration: 6318 lambda_k: 1 Loss: 0.0014512842096820223\n",
      "Iteration: 6319 lambda_k: 1 Loss: 0.0014512842182511873\n",
      "Iteration: 6320 lambda_k: 1 Loss: 0.0014512842267693246\n",
      "Iteration: 6321 lambda_k: 1 Loss: 0.0014512842352367508\n",
      "Iteration: 6322 lambda_k: 1 Loss: 0.0014512842436537378\n",
      "Iteration: 6323 lambda_k: 1 Loss: 0.0014512842520206398\n",
      "Iteration: 6324 lambda_k: 1 Loss: 0.001451284260337756\n",
      "Iteration: 6325 lambda_k: 1 Loss: 0.0014512842686053929\n",
      "Iteration: 6326 lambda_k: 1 Loss: 0.0014512842768238069\n",
      "Iteration: 6327 lambda_k: 1 Loss: 0.0014512842849933067\n",
      "Iteration: 6328 lambda_k: 1 Loss: 0.0014512842931142396\n",
      "Iteration: 6329 lambda_k: 1 Loss: 0.0014512843011868614\n",
      "Iteration: 6330 lambda_k: 1 Loss: 0.001451284309211418\n",
      "Iteration: 6331 lambda_k: 1 Loss: 0.0014512843171882632\n",
      "Iteration: 6332 lambda_k: 1 Loss: 0.0014512843251176647\n",
      "Iteration: 6333 lambda_k: 1 Loss: 0.0014512843329999273\n",
      "Iteration: 6334 lambda_k: 1 Loss: 0.0014512843408353113\n",
      "Iteration: 6335 lambda_k: 1 Loss: 0.0014512843486241204\n",
      "Iteration: 6336 lambda_k: 1 Loss: 0.0014512843563666323\n",
      "Iteration: 6337 lambda_k: 1 Loss: 0.001451284364063085\n",
      "Iteration: 6338 lambda_k: 1 Loss: 0.0014512843717138112\n",
      "Iteration: 6339 lambda_k: 1 Loss: 0.0014512843793191004\n",
      "Iteration: 6340 lambda_k: 1 Loss: 0.0014512843868791864\n",
      "Iteration: 6341 lambda_k: 1 Loss: 0.001451284394394323\n",
      "Iteration: 6342 lambda_k: 1 Loss: 0.0014512844018648586\n",
      "Iteration: 6343 lambda_k: 1 Loss: 0.0014512844092909808\n",
      "Iteration: 6344 lambda_k: 1 Loss: 0.0014512844166730348\n",
      "Iteration: 6345 lambda_k: 1 Loss: 0.0014512844240112388\n",
      "Iteration: 6346 lambda_k: 1 Loss: 0.0014512844313058265\n",
      "Iteration: 6347 lambda_k: 1 Loss: 0.0014512844385571161\n",
      "Iteration: 6348 lambda_k: 1 Loss: 0.0014512844457653783\n",
      "Iteration: 6349 lambda_k: 1 Loss: 0.0014512844529308132\n",
      "Iteration: 6350 lambda_k: 1 Loss: 0.0014512844600537424\n",
      "Iteration: 6351 lambda_k: 1 Loss: 0.0014512844671343728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6352 lambda_k: 1 Loss: 0.001451284474172962\n",
      "Iteration: 6353 lambda_k: 1 Loss: 0.0014512844811698305\n",
      "Iteration: 6354 lambda_k: 1 Loss: 0.0014512844881251455\n",
      "Iteration: 6355 lambda_k: 1 Loss: 0.0014512844950392235\n",
      "Iteration: 6356 lambda_k: 1 Loss: 0.0014512845019122498\n",
      "Iteration: 6357 lambda_k: 1 Loss: 0.0014512845087445183\n",
      "Iteration: 6358 lambda_k: 1 Loss: 0.001451284515536301\n",
      "Iteration: 6359 lambda_k: 1 Loss: 0.0014512845222877855\n",
      "Iteration: 6360 lambda_k: 1 Loss: 0.0014512845289992257\n",
      "Iteration: 6361 lambda_k: 1 Loss: 0.0014512845356708696\n",
      "Iteration: 6362 lambda_k: 1 Loss: 0.001451284542302946\n",
      "Iteration: 6363 lambda_k: 1 Loss: 0.0014512845488957376\n",
      "Iteration: 6364 lambda_k: 1 Loss: 0.0014512845554494195\n",
      "Iteration: 6365 lambda_k: 1 Loss: 0.0014512845619642789\n",
      "Iteration: 6366 lambda_k: 1 Loss: 0.001451284568440543\n",
      "Iteration: 6367 lambda_k: 1 Loss: 0.001451284574878435\n",
      "Iteration: 6368 lambda_k: 1 Loss: 0.001451284581278179\n",
      "Iteration: 6369 lambda_k: 1 Loss: 0.001451284587640003\n",
      "Iteration: 6370 lambda_k: 1 Loss: 0.0014512845939641716\n",
      "Iteration: 6371 lambda_k: 1 Loss: 0.0014512846002508563\n",
      "Iteration: 6372 lambda_k: 1 Loss: 0.0014512846065003505\n",
      "Iteration: 6373 lambda_k: 1 Loss: 0.001451284612712841\n",
      "Iteration: 6374 lambda_k: 1 Loss: 0.0014512846188885046\n",
      "Iteration: 6375 lambda_k: 1 Loss: 0.0014512846250276398\n",
      "Iteration: 6376 lambda_k: 1 Loss: 0.0014512846311304348\n",
      "Iteration: 6377 lambda_k: 1 Loss: 0.001451284637197084\n",
      "Iteration: 6378 lambda_k: 1 Loss: 0.001451284643227867\n",
      "Iteration: 6379 lambda_k: 1 Loss: 0.0014512846492229577\n",
      "Iteration: 6380 lambda_k: 1 Loss: 0.0014512846551825466\n",
      "Iteration: 6381 lambda_k: 1 Loss: 0.0014512846611069291\n",
      "Iteration: 6382 lambda_k: 1 Loss: 0.001451284666996275\n",
      "Iteration: 6383 lambda_k: 1 Loss: 0.0014512846728507284\n",
      "Iteration: 6384 lambda_k: 1 Loss: 0.0014512846786705693\n",
      "Iteration: 6385 lambda_k: 1 Loss: 0.001451284684455984\n",
      "Iteration: 6386 lambda_k: 1 Loss: 0.0014512846902072177\n",
      "Iteration: 6387 lambda_k: 1 Loss: 0.0014512846959244291\n",
      "Iteration: 6388 lambda_k: 1 Loss: 0.0014512847016078497\n",
      "Iteration: 6389 lambda_k: 1 Loss: 0.0014512847072576888\n",
      "Iteration: 6390 lambda_k: 1 Loss: 0.0014512847128741182\n",
      "Iteration: 6391 lambda_k: 1 Loss: 0.0014512847184573682\n",
      "Iteration: 6392 lambda_k: 1 Loss: 0.0014512847240076246\n",
      "Iteration: 6393 lambda_k: 1 Loss: 0.0014512847295250885\n",
      "Iteration: 6394 lambda_k: 1 Loss: 0.0014512847350099944\n",
      "Iteration: 6395 lambda_k: 1 Loss: 0.0014512847404624702\n",
      "Iteration: 6396 lambda_k: 1 Loss: 0.0014512847458827776\n",
      "Iteration: 6397 lambda_k: 1 Loss: 0.0014512847512710648\n",
      "Iteration: 6398 lambda_k: 1 Loss: 0.001451284756627531\n",
      "Iteration: 6399 lambda_k: 1 Loss: 0.001451284761952363\n",
      "Iteration: 6400 lambda_k: 1 Loss: 0.0014512847672457823\n",
      "Iteration: 6401 lambda_k: 1 Loss: 0.0014512847725079246\n",
      "Iteration: 6402 lambda_k: 1 Loss: 0.0014512847777390088\n",
      "Iteration: 6403 lambda_k: 1 Loss: 0.0014512847829392225\n",
      "Iteration: 6404 lambda_k: 1 Loss: 0.001451284788108772\n",
      "Iteration: 6405 lambda_k: 1 Loss: 0.0014512847932478096\n",
      "Iteration: 6406 lambda_k: 1 Loss: 0.0014512847983565366\n",
      "Iteration: 6407 lambda_k: 1 Loss: 0.00145128480343513\n",
      "Iteration: 6408 lambda_k: 1 Loss: 0.0014512848084837727\n",
      "Iteration: 6409 lambda_k: 1 Loss: 0.0014512848135026634\n",
      "Iteration: 6410 lambda_k: 1 Loss: 0.0014512848184919417\n",
      "Iteration: 6411 lambda_k: 1 Loss: 0.0014512848234517945\n",
      "Iteration: 6412 lambda_k: 1 Loss: 0.0014512848283824336\n",
      "Iteration: 6413 lambda_k: 1 Loss: 0.001451284833283966\n",
      "Iteration: 6414 lambda_k: 1 Loss: 0.0014512848381566538\n",
      "Iteration: 6415 lambda_k: 1 Loss: 0.001451284843000625\n",
      "Iteration: 6416 lambda_k: 1 Loss: 0.0014512848478160226\n",
      "Iteration: 6417 lambda_k: 1 Loss: 0.0014512848526030723\n",
      "Iteration: 6418 lambda_k: 1 Loss: 0.0014512848573619093\n",
      "Iteration: 6419 lambda_k: 1 Loss: 0.001451284862092722\n",
      "Iteration: 6420 lambda_k: 1 Loss: 0.00145128486679565\n",
      "Iteration: 6421 lambda_k: 1 Loss: 0.0014512848714709043\n",
      "Iteration: 6422 lambda_k: 1 Loss: 0.0014512848761186264\n",
      "Iteration: 6423 lambda_k: 1 Loss: 0.0014512848807389977\n",
      "Iteration: 6424 lambda_k: 1 Loss: 0.001451284885332151\n",
      "Iteration: 6425 lambda_k: 1 Loss: 0.0014512848898982625\n",
      "Iteration: 6426 lambda_k: 1 Loss: 0.0014512848944375203\n",
      "Iteration: 6427 lambda_k: 1 Loss: 0.00145128489895008\n",
      "Iteration: 6428 lambda_k: 1 Loss: 0.0014512849034360605\n",
      "Iteration: 6429 lambda_k: 1 Loss: 0.0014512849078956627\n",
      "Iteration: 6430 lambda_k: 1 Loss: 0.001451284912329048\n",
      "Iteration: 6431 lambda_k: 1 Loss: 0.0014512849167362977\n",
      "Iteration: 6432 lambda_k: 1 Loss: 0.001451284921117651\n",
      "Iteration: 6433 lambda_k: 1 Loss: 0.001451284925473215\n",
      "Iteration: 6434 lambda_k: 1 Loss: 0.001451284929803225\n",
      "Iteration: 6435 lambda_k: 1 Loss: 0.0014512849341077245\n",
      "Iteration: 6436 lambda_k: 1 Loss: 0.0014512849383869432\n",
      "Iteration: 6437 lambda_k: 1 Loss: 0.001451284942641031\n",
      "Iteration: 6438 lambda_k: 1 Loss: 0.0014512849468701235\n",
      "Iteration: 6439 lambda_k: 1 Loss: 0.0014512849510743579\n",
      "Iteration: 6440 lambda_k: 1 Loss: 0.001451284955253865\n",
      "Iteration: 6441 lambda_k: 1 Loss: 0.0014512849594088468\n",
      "Iteration: 6442 lambda_k: 1 Loss: 0.0014512849635393919\n",
      "Iteration: 6443 lambda_k: 1 Loss: 0.001451284967645688\n",
      "Iteration: 6444 lambda_k: 1 Loss: 0.0014512849717278765\n",
      "Iteration: 6445 lambda_k: 1 Loss: 0.0014512849757860786\n",
      "Iteration: 6446 lambda_k: 1 Loss: 0.0014512849798204736\n",
      "Iteration: 6447 lambda_k: 1 Loss: 0.0014512849838311795\n",
      "Iteration: 6448 lambda_k: 1 Loss: 0.0014512849878183546\n",
      "Iteration: 6449 lambda_k: 1 Loss: 0.0014512849917820952\n",
      "Iteration: 6450 lambda_k: 1 Loss: 0.001451284995722603\n",
      "Iteration: 6451 lambda_k: 1 Loss: 0.0014512849996399808\n",
      "Iteration: 6452 lambda_k: 1 Loss: 0.0014512850035343597\n",
      "Iteration: 6453 lambda_k: 1 Loss: 0.0014512850074059186\n",
      "Iteration: 6454 lambda_k: 1 Loss: 0.0014512850112547448\n",
      "Iteration: 6455 lambda_k: 1 Loss: 0.0014512850150809958\n",
      "Iteration: 6456 lambda_k: 1 Loss: 0.0014512850188848213\n",
      "Iteration: 6457 lambda_k: 1 Loss: 0.0014512850226663483\n",
      "Iteration: 6458 lambda_k: 1 Loss: 0.0014512850264256799\n",
      "Iteration: 6459 lambda_k: 1 Loss: 0.001451285030162977\n",
      "Iteration: 6460 lambda_k: 1 Loss: 0.0014512850338783603\n",
      "Iteration: 6461 lambda_k: 1 Loss: 0.0014512850375720137\n",
      "Iteration: 6462 lambda_k: 1 Loss: 0.0014512850412439753\n",
      "Iteration: 6463 lambda_k: 1 Loss: 0.0014512850448944444\n",
      "Iteration: 6464 lambda_k: 1 Loss: 0.0014512850485235394\n",
      "Iteration: 6465 lambda_k: 1 Loss: 0.0014512850521313518\n",
      "Iteration: 6466 lambda_k: 1 Loss: 0.0014512850557180516\n",
      "Iteration: 6467 lambda_k: 1 Loss: 0.0014512850592837284\n",
      "Iteration: 6468 lambda_k: 1 Loss: 0.0014512850628285363\n",
      "Iteration: 6469 lambda_k: 1 Loss: 0.0014512850663525917\n",
      "Iteration: 6470 lambda_k: 1 Loss: 0.0014512850698560186\n",
      "Iteration: 6471 lambda_k: 1 Loss: 0.0014512850733389576\n",
      "Iteration: 6472 lambda_k: 1 Loss: 0.0014512850768014943\n",
      "Iteration: 6473 lambda_k: 1 Loss: 0.0014512850802437915\n",
      "Iteration: 6474 lambda_k: 1 Loss: 0.0014512850836659137\n",
      "Iteration: 6475 lambda_k: 1 Loss: 0.0014512850870680432\n",
      "Iteration: 6476 lambda_k: 1 Loss: 0.0014512850904502713\n",
      "Iteration: 6477 lambda_k: 1 Loss: 0.0014512850938126936\n",
      "Iteration: 6478 lambda_k: 1 Loss: 0.0014512850971554836\n",
      "Iteration: 6479 lambda_k: 1 Loss: 0.0014512851004787326\n",
      "Iteration: 6480 lambda_k: 1 Loss: 0.0014512851037825388\n",
      "Iteration: 6481 lambda_k: 1 Loss: 0.001451285107067001\n",
      "Iteration: 6482 lambda_k: 1 Loss: 0.001451285110332314\n",
      "Iteration: 6483 lambda_k: 1 Loss: 0.0014512851135784907\n",
      "Iteration: 6484 lambda_k: 1 Loss: 0.0014512851168057238\n",
      "Iteration: 6485 lambda_k: 1 Loss: 0.001451285120014087\n",
      "Iteration: 6486 lambda_k: 1 Loss: 0.0014512851232037242\n",
      "Iteration: 6487 lambda_k: 1 Loss: 0.0014512851263747428\n",
      "Iteration: 6488 lambda_k: 1 Loss: 0.001451285129527251\n",
      "Iteration: 6489 lambda_k: 1 Loss: 0.0014512851326613522\n",
      "Iteration: 6490 lambda_k: 1 Loss: 0.001451285135777124\n",
      "Iteration: 6491 lambda_k: 1 Loss: 0.0014512851388746937\n",
      "Iteration: 6492 lambda_k: 1 Loss: 0.0014512851419541909\n",
      "Iteration: 6493 lambda_k: 1 Loss: 0.0014512851450157302\n",
      "Iteration: 6494 lambda_k: 1 Loss: 0.001451285148059406\n",
      "Iteration: 6495 lambda_k: 1 Loss: 0.001451285151085323\n",
      "Iteration: 6496 lambda_k: 1 Loss: 0.001451285154093558\n",
      "Iteration: 6497 lambda_k: 1 Loss: 0.0014512851570842513\n",
      "Iteration: 6498 lambda_k: 1 Loss: 0.0014512851600574895\n",
      "Iteration: 6499 lambda_k: 1 Loss: 0.0014512851630134145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6500 lambda_k: 1 Loss: 0.0014512851659521046\n",
      "Iteration: 6501 lambda_k: 1 Loss: 0.0014512851688736692\n",
      "Iteration: 6502 lambda_k: 1 Loss: 0.0014512851717781673\n",
      "Iteration: 6503 lambda_k: 1 Loss: 0.0014512851746657238\n",
      "Iteration: 6504 lambda_k: 1 Loss: 0.0014512851775364888\n",
      "Iteration: 6505 lambda_k: 1 Loss: 0.001451285180390517\n",
      "Iteration: 6506 lambda_k: 1 Loss: 0.001451285183227897\n",
      "Iteration: 6507 lambda_k: 1 Loss: 0.0014512851860487561\n",
      "Iteration: 6508 lambda_k: 1 Loss: 0.0014512851888531853\n",
      "Iteration: 6509 lambda_k: 1 Loss: 0.00145128519164127\n",
      "Iteration: 6510 lambda_k: 1 Loss: 0.0014512851944131217\n",
      "Iteration: 6511 lambda_k: 1 Loss: 0.0014512851971688295\n",
      "Iteration: 6512 lambda_k: 1 Loss: 0.0014512851999085076\n",
      "Iteration: 6513 lambda_k: 1 Loss: 0.00145128520263221\n",
      "Iteration: 6514 lambda_k: 1 Loss: 0.001451285205340075\n",
      "Iteration: 6515 lambda_k: 1 Loss: 0.0014512852080321922\n",
      "Iteration: 6516 lambda_k: 1 Loss: 0.0014512852107086316\n",
      "Iteration: 6517 lambda_k: 1 Loss: 0.001451285213369509\n",
      "Iteration: 6518 lambda_k: 1 Loss: 0.0014512852160148961\n",
      "Iteration: 6519 lambda_k: 1 Loss: 0.0014512852186449137\n",
      "Iteration: 6520 lambda_k: 1 Loss: 0.001451285221259616\n",
      "Iteration: 6521 lambda_k: 1 Loss: 0.0014512852238591084\n",
      "Iteration: 6522 lambda_k: 1 Loss: 0.0014512852264435016\n",
      "Iteration: 6523 lambda_k: 1 Loss: 0.0014512852290128801\n",
      "Iteration: 6524 lambda_k: 1 Loss: 0.0014512852315673095\n",
      "Iteration: 6525 lambda_k: 1 Loss: 0.0014512852341068895\n",
      "Iteration: 6526 lambda_k: 1 Loss: 0.0014512852366317353\n",
      "Iteration: 6527 lambda_k: 1 Loss: 0.0014512852391418715\n",
      "Iteration: 6528 lambda_k: 1 Loss: 0.0014512852416374278\n",
      "Iteration: 6529 lambda_k: 1 Loss: 0.0014512852441184944\n",
      "Iteration: 6530 lambda_k: 1 Loss: 0.00145128524658516\n",
      "Iteration: 6531 lambda_k: 1 Loss: 0.0014512852490375303\n",
      "Iteration: 6532 lambda_k: 1 Loss: 0.0014512852514756234\n",
      "Iteration: 6533 lambda_k: 1 Loss: 0.0014512852538995972\n",
      "Iteration: 6534 lambda_k: 1 Loss: 0.0014512852563094835\n",
      "Iteration: 6535 lambda_k: 1 Loss: 0.0014512852587053812\n",
      "Iteration: 6536 lambda_k: 1 Loss: 0.0014512852610873825\n",
      "Iteration: 6537 lambda_k: 1 Loss: 0.001451285263455552\n",
      "Iteration: 6538 lambda_k: 1 Loss: 0.0014512852658099924\n",
      "Iteration: 6539 lambda_k: 1 Loss: 0.001451285268150758\n",
      "Iteration: 6540 lambda_k: 1 Loss: 0.0014512852704779654\n",
      "Iteration: 6541 lambda_k: 1 Loss: 0.0014512852727916626\n",
      "Iteration: 6542 lambda_k: 1 Loss: 0.001451285275091922\n",
      "Iteration: 6543 lambda_k: 1 Loss: 0.0014512852773788847\n",
      "Iteration: 6544 lambda_k: 1 Loss: 0.0014512852796525482\n",
      "Iteration: 6545 lambda_k: 1 Loss: 0.0014512852819130821\n",
      "Iteration: 6546 lambda_k: 1 Loss: 0.0014512852841604793\n",
      "Iteration: 6547 lambda_k: 1 Loss: 0.0014512852863948764\n",
      "Iteration: 6548 lambda_k: 1 Loss: 0.0014512852886162954\n",
      "Iteration: 6549 lambda_k: 1 Loss: 0.0014512852908248749\n",
      "Iteration: 6550 lambda_k: 1 Loss: 0.0014512852930206603\n",
      "Iteration: 6551 lambda_k: 1 Loss: 0.0014512852952036968\n",
      "Iteration: 6552 lambda_k: 1 Loss: 0.0014512852973741493\n",
      "Iteration: 6553 lambda_k: 1 Loss: 0.0014512852995319952\n",
      "Iteration: 6554 lambda_k: 1 Loss: 0.001451285301677352\n",
      "Iteration: 6555 lambda_k: 1 Loss: 0.0014512853038102808\n",
      "Iteration: 6556 lambda_k: 1 Loss: 0.0014512853059309096\n",
      "Iteration: 6557 lambda_k: 1 Loss: 0.001451285308039253\n",
      "Iteration: 6558 lambda_k: 1 Loss: 0.0014512853101353881\n",
      "Iteration: 6559 lambda_k: 1 Loss: 0.0014512853122194047\n",
      "Iteration: 6560 lambda_k: 1 Loss: 0.0014512853142913617\n",
      "Iteration: 6561 lambda_k: 1 Loss: 0.0014512853163513313\n",
      "Iteration: 6562 lambda_k: 1 Loss: 0.0014512853183993872\n",
      "Iteration: 6563 lambda_k: 1 Loss: 0.0014512853204356117\n",
      "Iteration: 6564 lambda_k: 1 Loss: 0.0014512853224600824\n",
      "Iteration: 6565 lambda_k: 1 Loss: 0.001451285324472823\n",
      "Iteration: 6566 lambda_k: 1 Loss: 0.0014512853264739259\n",
      "Iteration: 6567 lambda_k: 1 Loss: 0.0014512853284634761\n",
      "Iteration: 6568 lambda_k: 1 Loss: 0.0014512853304415405\n",
      "Iteration: 6569 lambda_k: 1 Loss: 0.0014512853324081824\n",
      "Iteration: 6570 lambda_k: 1 Loss: 0.0014512853343634576\n",
      "Iteration: 6571 lambda_k: 1 Loss: 0.001451285336307424\n",
      "Iteration: 6572 lambda_k: 1 Loss: 0.0014512853382401953\n",
      "Iteration: 6573 lambda_k: 1 Loss: 0.001451285340161802\n",
      "Iteration: 6574 lambda_k: 1 Loss: 0.0014512853420723193\n",
      "Iteration: 6575 lambda_k: 1 Loss: 0.0014512853439717951\n",
      "Iteration: 6576 lambda_k: 1 Loss: 0.0014512853458603073\n",
      "Iteration: 6577 lambda_k: 1 Loss: 0.0014512853477379327\n",
      "Iteration: 6578 lambda_k: 1 Loss: 0.0014512853496047443\n",
      "Iteration: 6579 lambda_k: 1 Loss: 0.0014512853514607681\n",
      "Iteration: 6580 lambda_k: 1 Loss: 0.0014512853533060913\n",
      "Iteration: 6581 lambda_k: 1 Loss: 0.0014512853551407676\n",
      "Iteration: 6582 lambda_k: 1 Loss: 0.0014512853569648722\n",
      "Iteration: 6583 lambda_k: 1 Loss: 0.0014512853587784688\n",
      "Iteration: 6584 lambda_k: 1 Loss: 0.0014512853605816208\n",
      "Iteration: 6585 lambda_k: 1 Loss: 0.0014512853623743398\n",
      "Iteration: 6586 lambda_k: 1 Loss: 0.0014512853641567513\n",
      "Iteration: 6587 lambda_k: 1 Loss: 0.0014512853659288663\n",
      "Iteration: 6588 lambda_k: 1 Loss: 0.0014512853676908104\n",
      "Iteration: 6589 lambda_k: 1 Loss: 0.0014512853694425825\n",
      "Iteration: 6590 lambda_k: 1 Loss: 0.001451285371184267\n",
      "Iteration: 6591 lambda_k: 1 Loss: 0.0014512853729158998\n",
      "Iteration: 6592 lambda_k: 1 Loss: 0.0014512853746376207\n",
      "Iteration: 6593 lambda_k: 1 Loss: 0.0014512853763493767\n",
      "Iteration: 6594 lambda_k: 1 Loss: 0.001451285378051297\n",
      "Iteration: 6595 lambda_k: 1 Loss: 0.001451285379743435\n",
      "Iteration: 6596 lambda_k: 1 Loss: 0.0014512853814258237\n",
      "Iteration: 6597 lambda_k: 1 Loss: 0.0014512853830985373\n",
      "Iteration: 6598 lambda_k: 1 Loss: 0.0014512853847616226\n",
      "Iteration: 6599 lambda_k: 1 Loss: 0.0014512853864151478\n",
      "Iteration: 6600 lambda_k: 1 Loss: 0.001451285388059171\n",
      "Iteration: 6601 lambda_k: 1 Loss: 0.0014512853896937435\n",
      "Iteration: 6602 lambda_k: 1 Loss: 0.0014512853913189196\n",
      "Iteration: 6603 lambda_k: 1 Loss: 0.0014512853929347373\n",
      "Iteration: 6604 lambda_k: 1 Loss: 0.0014512853945412674\n",
      "Iteration: 6605 lambda_k: 1 Loss: 0.001451285396138572\n",
      "Iteration: 6606 lambda_k: 1 Loss: 0.0014512853977267097\n",
      "Iteration: 6607 lambda_k: 1 Loss: 0.0014512853993057287\n",
      "Iteration: 6608 lambda_k: 1 Loss: 0.0014512854008756953\n",
      "Iteration: 6609 lambda_k: 1 Loss: 0.001451285402436628\n",
      "Iteration: 6610 lambda_k: 1 Loss: 0.001451285403988608\n",
      "Iteration: 6611 lambda_k: 1 Loss: 0.0014512854055316441\n",
      "Iteration: 6612 lambda_k: 1 Loss: 0.0014512854070658139\n",
      "Iteration: 6613 lambda_k: 1 Loss: 0.001451285408591215\n",
      "Iteration: 6614 lambda_k: 1 Loss: 0.0014512854101078664\n",
      "Iteration: 6615 lambda_k: 1 Loss: 0.0014512854116157984\n",
      "Iteration: 6616 lambda_k: 1 Loss: 0.0014512854131150737\n",
      "Iteration: 6617 lambda_k: 1 Loss: 0.0014512854146057422\n",
      "Iteration: 6618 lambda_k: 1 Loss: 0.0014512854160878682\n",
      "Iteration: 6619 lambda_k: 1 Loss: 0.001451285417561499\n",
      "Iteration: 6620 lambda_k: 1 Loss: 0.0014512854190267006\n",
      "Iteration: 6621 lambda_k: 1 Loss: 0.0014512854204834745\n",
      "Iteration: 6622 lambda_k: 1 Loss: 0.0014512854219319116\n",
      "Iteration: 6623 lambda_k: 1 Loss: 0.0014512854233720762\n",
      "Iteration: 6624 lambda_k: 1 Loss: 0.0014512854248039633\n",
      "Iteration: 6625 lambda_k: 1 Loss: 0.001451285426227673\n",
      "Iteration: 6626 lambda_k: 1 Loss: 0.001451285427643216\n",
      "Iteration: 6627 lambda_k: 1 Loss: 0.0014512854290506306\n",
      "Iteration: 6628 lambda_k: 1 Loss: 0.0014512854304499904\n",
      "Iteration: 6629 lambda_k: 1 Loss: 0.0014512854318413202\n",
      "Iteration: 6630 lambda_k: 1 Loss: 0.001451285433224672\n",
      "Iteration: 6631 lambda_k: 1 Loss: 0.0014512854346001738\n",
      "Iteration: 6632 lambda_k: 1 Loss: 0.0014512854359677448\n",
      "Iteration: 6633 lambda_k: 1 Loss: 0.001451285437327502\n",
      "Iteration: 6634 lambda_k: 1 Loss: 0.001451285438679502\n",
      "Iteration: 6635 lambda_k: 1 Loss: 0.0014512854400237814\n",
      "Iteration: 6636 lambda_k: 1 Loss: 0.0014512854413603292\n",
      "Iteration: 6637 lambda_k: 1 Loss: 0.0014512854426892616\n",
      "Iteration: 6638 lambda_k: 1 Loss: 0.001451285444010578\n",
      "Iteration: 6639 lambda_k: 1 Loss: 0.0014512854453243584\n",
      "Iteration: 6640 lambda_k: 1 Loss: 0.001451285446630615\n",
      "Iteration: 6641 lambda_k: 1 Loss: 0.0014512854479293968\n",
      "Iteration: 6642 lambda_k: 1 Loss: 0.0014512854492207882\n",
      "Iteration: 6643 lambda_k: 1 Loss: 0.0014512854505047954\n",
      "Iteration: 6644 lambda_k: 1 Loss: 0.0014512854517814576\n",
      "Iteration: 6645 lambda_k: 1 Loss: 0.0014512854530508218\n",
      "Iteration: 6646 lambda_k: 1 Loss: 0.0014512854543129471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6647 lambda_k: 1 Loss: 0.0014512854555678754\n",
      "Iteration: 6648 lambda_k: 1 Loss: 0.001451285456815634\n",
      "Iteration: 6649 lambda_k: 1 Loss: 0.0014512854580562868\n",
      "Iteration: 6650 lambda_k: 1 Loss: 0.0014512854592898348\n",
      "Iteration: 6651 lambda_k: 1 Loss: 0.0014512854605163417\n",
      "Iteration: 6652 lambda_k: 1 Loss: 0.0014512854617358389\n",
      "Iteration: 6653 lambda_k: 1 Loss: 0.0014512854629483902\n",
      "Iteration: 6654 lambda_k: 1 Loss: 0.0014512854641540313\n",
      "Iteration: 6655 lambda_k: 1 Loss: 0.0014512854653527933\n",
      "Iteration: 6656 lambda_k: 1 Loss: 0.0014512854665447157\n",
      "Iteration: 6657 lambda_k: 1 Loss: 0.0014512854677298445\n",
      "Iteration: 6658 lambda_k: 1 Loss: 0.0014512854689082246\n",
      "Iteration: 6659 lambda_k: 1 Loss: 0.0014512854700798898\n",
      "Iteration: 6660 lambda_k: 1 Loss: 0.0014512854712448577\n",
      "Iteration: 6661 lambda_k: 1 Loss: 0.0014512854724031876\n",
      "Iteration: 6662 lambda_k: 1 Loss: 0.001451285473554975\n",
      "Iteration: 6663 lambda_k: 1 Loss: 0.0014512854747001456\n",
      "Iteration: 6664 lambda_k: 1 Loss: 0.0014512854758388124\n",
      "Iteration: 6665 lambda_k: 1 Loss: 0.0014512854769709828\n",
      "Iteration: 6666 lambda_k: 1 Loss: 0.001451285478096731\n",
      "Iteration: 6667 lambda_k: 1 Loss: 0.00145128547921608\n",
      "Iteration: 6668 lambda_k: 1 Loss: 0.0014512854803290225\n",
      "Iteration: 6669 lambda_k: 1 Loss: 0.0014512854814356473\n",
      "Iteration: 6670 lambda_k: 1 Loss: 0.0014512854825360175\n",
      "Iteration: 6671 lambda_k: 1 Loss: 0.0014512854836300883\n",
      "Iteration: 6672 lambda_k: 1 Loss: 0.0014512854847179625\n",
      "Iteration: 6673 lambda_k: 1 Loss: 0.001451285485799634\n",
      "Iteration: 6674 lambda_k: 1 Loss: 0.0014512854868751353\n",
      "Iteration: 6675 lambda_k: 1 Loss: 0.0014512854879445661\n",
      "Iteration: 6676 lambda_k: 1 Loss: 0.0014512854890078885\n",
      "Iteration: 6677 lambda_k: 1 Loss: 0.0014512854900651801\n",
      "Iteration: 6678 lambda_k: 1 Loss: 0.001451285491116446\n",
      "Iteration: 6679 lambda_k: 1 Loss: 0.0014512854921617736\n",
      "Iteration: 6680 lambda_k: 1 Loss: 0.0014512854932011302\n",
      "Iteration: 6681 lambda_k: 1 Loss: 0.0014512854942346195\n",
      "Iteration: 6682 lambda_k: 1 Loss: 0.0014512854952621929\n",
      "Iteration: 6683 lambda_k: 1 Loss: 0.001451285496283944\n",
      "Iteration: 6684 lambda_k: 1 Loss: 0.0014512854972999047\n",
      "Iteration: 6685 lambda_k: 1 Loss: 0.0014512854983100968\n",
      "Iteration: 6686 lambda_k: 1 Loss: 0.0014512854993145657\n",
      "Iteration: 6687 lambda_k: 1 Loss: 0.0014512855003133642\n",
      "Iteration: 6688 lambda_k: 1 Loss: 0.001451285501306468\n",
      "Iteration: 6689 lambda_k: 1 Loss: 0.0014512855022939485\n",
      "Iteration: 6690 lambda_k: 1 Loss: 0.0014512855032758117\n",
      "Iteration: 6691 lambda_k: 1 Loss: 0.0014512855042521163\n",
      "Iteration: 6692 lambda_k: 1 Loss: 0.001451285505222878\n",
      "Iteration: 6693 lambda_k: 1 Loss: 0.0014512855061881488\n",
      "Iteration: 6694 lambda_k: 1 Loss: 0.0014512855071479504\n",
      "Iteration: 6695 lambda_k: 1 Loss: 0.001451285508102288\n",
      "Iteration: 6696 lambda_k: 1 Loss: 0.00145128550905126\n",
      "Iteration: 6697 lambda_k: 1 Loss: 0.0014512855099948268\n",
      "Iteration: 6698 lambda_k: 1 Loss: 0.0014512855109330735\n",
      "Iteration: 6699 lambda_k: 1 Loss: 0.0014512855118659703\n",
      "Iteration: 6700 lambda_k: 1 Loss: 0.0014512855127936006\n",
      "Iteration: 6701 lambda_k: 1 Loss: 0.0014512855137159587\n",
      "Iteration: 6702 lambda_k: 1 Loss: 0.001451285514633107\n",
      "Iteration: 6703 lambda_k: 1 Loss: 0.0014512855155450722\n",
      "Iteration: 6704 lambda_k: 1 Loss: 0.0014512855164518774\n",
      "Iteration: 6705 lambda_k: 1 Loss: 0.0014512855173535516\n",
      "Iteration: 6706 lambda_k: 1 Loss: 0.0014512855182501458\n",
      "Iteration: 6707 lambda_k: 1 Loss: 0.0014512855191416616\n",
      "Iteration: 6708 lambda_k: 1 Loss: 0.0014512855200281675\n",
      "Iteration: 6709 lambda_k: 1 Loss: 0.001451285520909637\n",
      "Iteration: 6710 lambda_k: 1 Loss: 0.0014512855217861307\n",
      "Iteration: 6711 lambda_k: 1 Loss: 0.0014512855226576718\n",
      "Iteration: 6712 lambda_k: 1 Loss: 0.0014512855235242774\n",
      "Iteration: 6713 lambda_k: 1 Loss: 0.0014512855243860047\n",
      "Iteration: 6714 lambda_k: 1 Loss: 0.0014512855252428502\n",
      "Iteration: 6715 lambda_k: 1 Loss: 0.0014512855260948626\n",
      "Iteration: 6716 lambda_k: 1 Loss: 0.0014512855269420619\n",
      "Iteration: 6717 lambda_k: 1 Loss: 0.0014512855277844998\n",
      "Iteration: 6718 lambda_k: 1 Loss: 0.0014512855286221682\n",
      "Iteration: 6719 lambda_k: 1 Loss: 0.0014512855294551\n",
      "Iteration: 6720 lambda_k: 1 Loss: 0.0014512855302833286\n",
      "Iteration: 6721 lambda_k: 1 Loss: 0.0014512855311068639\n",
      "Iteration: 6722 lambda_k: 1 Loss: 0.0014512855319257737\n",
      "Iteration: 6723 lambda_k: 1 Loss: 0.0014512855327400776\n",
      "Iteration: 6724 lambda_k: 1 Loss: 0.0014512855335497975\n",
      "Iteration: 6725 lambda_k: 1 Loss: 0.001451285534354924\n",
      "Iteration: 6726 lambda_k: 1 Loss: 0.0014512855351555185\n",
      "Iteration: 6727 lambda_k: 1 Loss: 0.0014512855359516113\n",
      "Iteration: 6728 lambda_k: 1 Loss: 0.0014512855367432263\n",
      "Iteration: 6729 lambda_k: 1 Loss: 0.0014512855375303636\n",
      "Iteration: 6730 lambda_k: 1 Loss: 0.0014512855383130802\n",
      "Iteration: 6731 lambda_k: 1 Loss: 0.0014512855390913686\n",
      "Iteration: 6732 lambda_k: 1 Loss: 0.0014512855398652505\n",
      "Iteration: 6733 lambda_k: 1 Loss: 0.0014512855406348057\n",
      "Iteration: 6734 lambda_k: 1 Loss: 0.001451285541400029\n",
      "Iteration: 6735 lambda_k: 1 Loss: 0.0014512855421609802\n",
      "Iteration: 6736 lambda_k: 1 Loss: 0.001451285542917608\n",
      "Iteration: 6737 lambda_k: 1 Loss: 0.001451285543669981\n",
      "Iteration: 6738 lambda_k: 1 Loss: 0.0014512855444181179\n",
      "Iteration: 6739 lambda_k: 1 Loss: 0.001451285545162059\n",
      "Iteration: 6740 lambda_k: 1 Loss: 0.0014512855459017833\n",
      "Iteration: 6741 lambda_k: 1 Loss: 0.0014512855466373904\n",
      "Iteration: 6742 lambda_k: 1 Loss: 0.0014512855473688261\n",
      "Iteration: 6743 lambda_k: 1 Loss: 0.0014512855480961794\n",
      "Iteration: 6744 lambda_k: 1 Loss: 0.0014512855488194474\n",
      "Iteration: 6745 lambda_k: 1 Loss: 0.0014512855495386545\n",
      "Iteration: 6746 lambda_k: 1 Loss: 0.0014512855502538202\n",
      "Iteration: 6747 lambda_k: 1 Loss: 0.00145128555096496\n",
      "Iteration: 6748 lambda_k: 1 Loss: 0.0014512855516720965\n",
      "Iteration: 6749 lambda_k: 1 Loss: 0.0014512855523753018\n",
      "Iteration: 6750 lambda_k: 1 Loss: 0.0014512855530745463\n",
      "Iteration: 6751 lambda_k: 1 Loss: 0.001451285553769841\n",
      "Iteration: 6752 lambda_k: 1 Loss: 0.0014512855544612422\n",
      "Iteration: 6753 lambda_k: 1 Loss: 0.0014512855551487675\n",
      "Iteration: 6754 lambda_k: 1 Loss: 0.0014512855558324257\n",
      "Iteration: 6755 lambda_k: 1 Loss: 0.0014512855565122524\n",
      "Iteration: 6756 lambda_k: 1 Loss: 0.0014512855571882654\n",
      "Iteration: 6757 lambda_k: 1 Loss: 0.0014512855578604946\n",
      "Iteration: 6758 lambda_k: 1 Loss: 0.0014512855585289473\n",
      "Iteration: 6759 lambda_k: 1 Loss: 0.0014512855591936723\n",
      "Iteration: 6760 lambda_k: 1 Loss: 0.001451285559854665\n",
      "Iteration: 6761 lambda_k: 1 Loss: 0.001451285560511926\n",
      "Iteration: 6762 lambda_k: 1 Loss: 0.0014512855611655104\n",
      "Iteration: 6763 lambda_k: 1 Loss: 0.0014512855618154473\n",
      "Iteration: 6764 lambda_k: 1 Loss: 0.0014512855624617552\n",
      "Iteration: 6765 lambda_k: 1 Loss: 0.0014512855631044548\n",
      "Iteration: 6766 lambda_k: 1 Loss: 0.001451285563743532\n",
      "Iteration: 6767 lambda_k: 1 Loss: 0.001451285564379049\n",
      "Iteration: 6768 lambda_k: 1 Loss: 0.0014512855650109673\n",
      "Iteration: 6769 lambda_k: 1 Loss: 0.0014512855656393772\n",
      "Iteration: 6770 lambda_k: 1 Loss: 0.0014512855662642866\n",
      "Iteration: 6771 lambda_k: 1 Loss: 0.0014512855668856951\n",
      "Iteration: 6772 lambda_k: 1 Loss: 0.0014512855675036238\n",
      "Iteration: 6773 lambda_k: 1 Loss: 0.001451285568118089\n",
      "Iteration: 6774 lambda_k: 1 Loss: 0.0014512855687291548\n",
      "Iteration: 6775 lambda_k: 1 Loss: 0.0014512855693367625\n",
      "Iteration: 6776 lambda_k: 1 Loss: 0.0014512855699409745\n",
      "Iteration: 6777 lambda_k: 1 Loss: 0.0014512855705418294\n",
      "Iteration: 6778 lambda_k: 1 Loss: 0.001451285571139327\n",
      "Iteration: 6779 lambda_k: 1 Loss: 0.001451285571733512\n",
      "Iteration: 6780 lambda_k: 1 Loss: 0.0014512855723243477\n",
      "Iteration: 6781 lambda_k: 1 Loss: 0.0014512855729118923\n",
      "Iteration: 6782 lambda_k: 1 Loss: 0.0014512855734961413\n",
      "Iteration: 6783 lambda_k: 1 Loss: 0.0014512855740771223\n",
      "Iteration: 6784 lambda_k: 1 Loss: 0.0014512855746548774\n",
      "Iteration: 6785 lambda_k: 1 Loss: 0.0014512855752294325\n",
      "Iteration: 6786 lambda_k: 1 Loss: 0.0014512855758007542\n",
      "Iteration: 6787 lambda_k: 1 Loss: 0.0014512855763689036\n",
      "Iteration: 6788 lambda_k: 1 Loss: 0.0014512855769338831\n",
      "Iteration: 6789 lambda_k: 1 Loss: 0.001451285577495686\n",
      "Iteration: 6790 lambda_k: 1 Loss: 0.0014512855780543735\n",
      "Iteration: 6791 lambda_k: 1 Loss: 0.00145128557860994\n",
      "Iteration: 6792 lambda_k: 1 Loss: 0.0014512855791624227\n",
      "Iteration: 6793 lambda_k: 1 Loss: 0.0014512855797118003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6794 lambda_k: 1 Loss: 0.0014512855802581459\n",
      "Iteration: 6795 lambda_k: 1 Loss: 0.001451285580801455\n",
      "Iteration: 6796 lambda_k: 1 Loss: 0.001451285581341748\n",
      "Iteration: 6797 lambda_k: 1 Loss: 0.001451285581878996\n",
      "Iteration: 6798 lambda_k: 1 Loss: 0.0014512855824132392\n",
      "Iteration: 6799 lambda_k: 1 Loss: 0.0014512855829445304\n",
      "Iteration: 6800 lambda_k: 1 Loss: 0.001451285583472865\n",
      "Iteration: 6801 lambda_k: 1 Loss: 0.0014512855839982895\n",
      "Iteration: 6802 lambda_k: 1 Loss: 0.001451285584520753\n",
      "Iteration: 6803 lambda_k: 1 Loss: 0.0014512855850403168\n",
      "Iteration: 6804 lambda_k: 1 Loss: 0.0014512855855570012\n",
      "Iteration: 6805 lambda_k: 1 Loss: 0.0014512855860708057\n",
      "Iteration: 6806 lambda_k: 1 Loss: 0.0014512855865817466\n",
      "Iteration: 6807 lambda_k: 1 Loss: 0.0014512855870898764\n",
      "Iteration: 6808 lambda_k: 1 Loss: 0.0014512855875951597\n",
      "Iteration: 6809 lambda_k: 1 Loss: 0.001451285588097652\n",
      "Iteration: 6810 lambda_k: 1 Loss: 0.0014512855885973237\n",
      "Iteration: 6811 lambda_k: 1 Loss: 0.0014512855890942368\n",
      "Iteration: 6812 lambda_k: 1 Loss: 0.001451285589588379\n",
      "Iteration: 6813 lambda_k: 1 Loss: 0.0014512855900797998\n",
      "Iteration: 6814 lambda_k: 1 Loss: 0.0014512855905684686\n",
      "Iteration: 6815 lambda_k: 1 Loss: 0.0014512855910544516\n",
      "Iteration: 6816 lambda_k: 1 Loss: 0.001451285591537729\n",
      "Iteration: 6817 lambda_k: 1 Loss: 0.0014512855920183383\n",
      "Iteration: 6818 lambda_k: 1 Loss: 0.0014512855924962746\n",
      "Iteration: 6819 lambda_k: 1 Loss: 0.001451285592971551\n",
      "Iteration: 6820 lambda_k: 1 Loss: 0.0014512855934442164\n",
      "Iteration: 6821 lambda_k: 1 Loss: 0.0014512855939142486\n",
      "Iteration: 6822 lambda_k: 1 Loss: 0.0014512855943816705\n",
      "Iteration: 6823 lambda_k: 1 Loss: 0.001451285594846507\n",
      "Iteration: 6824 lambda_k: 1 Loss: 0.0014512855953087787\n",
      "Iteration: 6825 lambda_k: 1 Loss: 0.001451285595768479\n",
      "Iteration: 6826 lambda_k: 1 Loss: 0.0014512855962256418\n",
      "Iteration: 6827 lambda_k: 1 Loss: 0.001451285596680227\n",
      "Iteration: 6828 lambda_k: 1 Loss: 0.0014512855971322992\n",
      "Iteration: 6829 lambda_k: 1 Loss: 0.0014512855975819224\n",
      "Iteration: 6830 lambda_k: 1 Loss: 0.0014512855980290391\n",
      "Iteration: 6831 lambda_k: 1 Loss: 0.0014512855984736737\n",
      "Iteration: 6832 lambda_k: 1 Loss: 0.0014512855989158501\n",
      "Iteration: 6833 lambda_k: 1 Loss: 0.0014512855993555822\n",
      "Iteration: 6834 lambda_k: 1 Loss: 0.001451285599792882\n",
      "Iteration: 6835 lambda_k: 1 Loss: 0.0014512856002277625\n",
      "Iteration: 6836 lambda_k: 1 Loss: 0.0014512856006602367\n",
      "Iteration: 6837 lambda_k: 1 Loss: 0.001451285601090319\n",
      "Iteration: 6838 lambda_k: 1 Loss: 0.00145128560151803\n",
      "Iteration: 6839 lambda_k: 1 Loss: 0.0014512856019433752\n",
      "Iteration: 6840 lambda_k: 1 Loss: 0.0014512856023663518\n",
      "Iteration: 6841 lambda_k: 1 Loss: 0.0014512856027870213\n",
      "Iteration: 6842 lambda_k: 1 Loss: 0.0014512856032053666\n",
      "Iteration: 6843 lambda_k: 1 Loss: 0.0014512856036214133\n",
      "Iteration: 6844 lambda_k: 1 Loss: 0.0014512856040351286\n",
      "Iteration: 6845 lambda_k: 1 Loss: 0.0014512856044465582\n",
      "Iteration: 6846 lambda_k: 1 Loss: 0.001451285604855729\n",
      "Iteration: 6847 lambda_k: 1 Loss: 0.0014512856052626543\n",
      "Iteration: 6848 lambda_k: 1 Loss: 0.0014512856056673326\n",
      "Iteration: 6849 lambda_k: 1 Loss: 0.0014512856060697635\n",
      "Iteration: 6850 lambda_k: 1 Loss: 0.001451285606469958\n",
      "Iteration: 6851 lambda_k: 1 Loss: 0.001451285606867978\n",
      "Iteration: 6852 lambda_k: 1 Loss: 0.0014512856072638064\n",
      "Iteration: 6853 lambda_k: 1 Loss: 0.0014512856076574357\n",
      "Iteration: 6854 lambda_k: 1 Loss: 0.0014512856080488982\n",
      "Iteration: 6855 lambda_k: 1 Loss: 0.0014512856084382244\n",
      "Iteration: 6856 lambda_k: 1 Loss: 0.0014512856088253776\n",
      "Iteration: 6857 lambda_k: 1 Loss: 0.0014512856092104246\n",
      "Iteration: 6858 lambda_k: 1 Loss: 0.0014512856095933615\n",
      "Iteration: 6859 lambda_k: 1 Loss: 0.0014512856099741715\n",
      "Iteration: 6860 lambda_k: 1 Loss: 0.0014512856103528796\n",
      "Iteration: 6861 lambda_k: 1 Loss: 0.0014512856107294968\n",
      "Iteration: 6862 lambda_k: 1 Loss: 0.001451285611104069\n",
      "Iteration: 6863 lambda_k: 1 Loss: 0.0014512856114765947\n",
      "Iteration: 6864 lambda_k: 1 Loss: 0.0014512856118470301\n",
      "Iteration: 6865 lambda_k: 1 Loss: 0.0014512856122154355\n",
      "Iteration: 6866 lambda_k: 1 Loss: 0.0014512856125818109\n",
      "Iteration: 6867 lambda_k: 1 Loss: 0.0014512856129461668\n",
      "Iteration: 6868 lambda_k: 1 Loss: 0.001451285613308549\n",
      "Iteration: 6869 lambda_k: 1 Loss: 0.001451285613668902\n",
      "Iteration: 6870 lambda_k: 1 Loss: 0.0014512856140273043\n",
      "Iteration: 6871 lambda_k: 1 Loss: 0.0014512856143837393\n",
      "Iteration: 6872 lambda_k: 1 Loss: 0.0014512856147382285\n",
      "Iteration: 6873 lambda_k: 1 Loss: 0.001451285615090754\n",
      "Iteration: 6874 lambda_k: 1 Loss: 0.0014512856154413369\n",
      "Iteration: 6875 lambda_k: 1 Loss: 0.0014512856157900174\n",
      "Iteration: 6876 lambda_k: 1 Loss: 0.0014512856161367777\n",
      "Iteration: 6877 lambda_k: 1 Loss: 0.0014512856164816171\n",
      "Iteration: 6878 lambda_k: 1 Loss: 0.001451285616824593\n",
      "Iteration: 6879 lambda_k: 1 Loss: 0.0014512856171656813\n",
      "Iteration: 6880 lambda_k: 1 Loss: 0.0014512856175048858\n",
      "Iteration: 6881 lambda_k: 1 Loss: 0.001451285617842259\n",
      "Iteration: 6882 lambda_k: 1 Loss: 0.001451285618177768\n",
      "Iteration: 6883 lambda_k: 1 Loss: 0.0014512856185114316\n",
      "Iteration: 6884 lambda_k: 1 Loss: 0.001451285618843272\n",
      "Iteration: 6885 lambda_k: 1 Loss: 0.001451285619173272\n",
      "Iteration: 6886 lambda_k: 1 Loss: 0.0014512856195014955\n",
      "Iteration: 6887 lambda_k: 1 Loss: 0.001451285619827924\n",
      "Iteration: 6888 lambda_k: 1 Loss: 0.001451285620152552\n",
      "Iteration: 6889 lambda_k: 1 Loss: 0.001451285620475404\n",
      "Iteration: 6890 lambda_k: 1 Loss: 0.0014512856207964738\n",
      "Iteration: 6891 lambda_k: 1 Loss: 0.0014512856211157793\n",
      "Iteration: 6892 lambda_k: 1 Loss: 0.0014512856214333341\n",
      "Iteration: 6893 lambda_k: 1 Loss: 0.001451285621749198\n",
      "Iteration: 6894 lambda_k: 1 Loss: 0.0014512856220633393\n",
      "Iteration: 6895 lambda_k: 1 Loss: 0.001451285622375746\n",
      "Iteration: 6896 lambda_k: 1 Loss: 0.0014512856226864498\n",
      "Iteration: 6897 lambda_k: 1 Loss: 0.0014512856229954422\n",
      "Iteration: 6898 lambda_k: 1 Loss: 0.0014512856233027578\n",
      "Iteration: 6899 lambda_k: 1 Loss: 0.0014512856236083918\n",
      "Iteration: 6900 lambda_k: 1 Loss: 0.001451285623912363\n",
      "Iteration: 6901 lambda_k: 1 Loss: 0.0014512856242146737\n",
      "Iteration: 6902 lambda_k: 1 Loss: 0.0014512856245153228\n",
      "Iteration: 6903 lambda_k: 1 Loss: 0.0014512856248143392\n",
      "Iteration: 6904 lambda_k: 1 Loss: 0.0014512856251117175\n",
      "Iteration: 6905 lambda_k: 1 Loss: 0.0014512856254074828\n",
      "Iteration: 6906 lambda_k: 1 Loss: 0.0014512856257016447\n",
      "Iteration: 6907 lambda_k: 1 Loss: 0.0014512856259941977\n",
      "Iteration: 6908 lambda_k: 1 Loss: 0.001451285626285116\n",
      "Iteration: 6909 lambda_k: 1 Loss: 0.001451285626574445\n",
      "Iteration: 6910 lambda_k: 1 Loss: 0.0014512856268622041\n",
      "Iteration: 6911 lambda_k: 1 Loss: 0.0014512856271484045\n",
      "Iteration: 6912 lambda_k: 1 Loss: 0.0014512856274330435\n",
      "Iteration: 6913 lambda_k: 1 Loss: 0.0014512856277161406\n",
      "Iteration: 6914 lambda_k: 1 Loss: 0.0014512856279977095\n",
      "Iteration: 6915 lambda_k: 1 Loss: 0.001451285628277757\n",
      "Iteration: 6916 lambda_k: 1 Loss: 0.0014512856285562445\n",
      "Iteration: 6917 lambda_k: 1 Loss: 0.0014512856288332176\n",
      "Iteration: 6918 lambda_k: 1 Loss: 0.0014512856291086739\n",
      "Iteration: 6919 lambda_k: 1 Loss: 0.0014512856293826656\n",
      "Iteration: 6920 lambda_k: 1 Loss: 0.0014512856296551291\n",
      "Iteration: 6921 lambda_k: 1 Loss: 0.0014512856299261176\n",
      "Iteration: 6922 lambda_k: 1 Loss: 0.0014512856301956416\n",
      "Iteration: 6923 lambda_k: 1 Loss: 0.0014512856304637104\n",
      "Iteration: 6924 lambda_k: 1 Loss: 0.0014512856307302984\n",
      "Iteration: 6925 lambda_k: 1 Loss: 0.0014512856309954413\n",
      "Iteration: 6926 lambda_k: 1 Loss: 0.001451285631259155\n",
      "Iteration: 6927 lambda_k: 1 Loss: 0.001451285631521449\n",
      "Iteration: 6928 lambda_k: 1 Loss: 0.001451285631782299\n",
      "Iteration: 6929 lambda_k: 1 Loss: 0.0014512856320417372\n",
      "Iteration: 6930 lambda_k: 1 Loss: 0.0014512856322997448\n",
      "Iteration: 6931 lambda_k: 1 Loss: 0.001451285632556361\n",
      "Iteration: 6932 lambda_k: 1 Loss: 0.0014512856328116003\n",
      "Iteration: 6933 lambda_k: 1 Loss: 0.0014512856330654892\n",
      "Iteration: 6934 lambda_k: 1 Loss: 0.001451285633317958\n",
      "Iteration: 6935 lambda_k: 1 Loss: 0.0014512856335690842\n",
      "Iteration: 6936 lambda_k: 1 Loss: 0.0014512856338188033\n",
      "Iteration: 6937 lambda_k: 1 Loss: 0.001451285634067184\n",
      "Iteration: 6938 lambda_k: 1 Loss: 0.001451285634314262\n",
      "Iteration: 6939 lambda_k: 1 Loss: 0.0014512856345599485\n",
      "Iteration: 6940 lambda_k: 1 Loss: 0.0014512856348043402\n",
      "Iteration: 6941 lambda_k: 1 Loss: 0.0014512856350473791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6942 lambda_k: 1 Loss: 0.0014512856352891278\n",
      "Iteration: 6943 lambda_k: 1 Loss: 0.0014512856355295349\n",
      "Iteration: 6944 lambda_k: 1 Loss: 0.0014512856357686624\n",
      "Iteration: 6945 lambda_k: 1 Loss: 0.0014512856360064728\n",
      "Iteration: 6946 lambda_k: 1 Loss: 0.0014512856362430258\n",
      "Iteration: 6947 lambda_k: 1 Loss: 0.001451285636478265\n",
      "Iteration: 6948 lambda_k: 1 Loss: 0.0014512856367122579\n",
      "Iteration: 6949 lambda_k: 1 Loss: 0.0014512856369449682\n",
      "Iteration: 6950 lambda_k: 1 Loss: 0.0014512856371764304\n",
      "Iteration: 6951 lambda_k: 1 Loss: 0.001451285637406646\n",
      "Iteration: 6952 lambda_k: 1 Loss: 0.00145128563763562\n",
      "Iteration: 6953 lambda_k: 1 Loss: 0.001451285637863348\n",
      "Iteration: 6954 lambda_k: 1 Loss: 0.0014512856380898165\n",
      "Iteration: 6955 lambda_k: 1 Loss: 0.001451285638315091\n",
      "Iteration: 6956 lambda_k: 1 Loss: 0.0014512856385391353\n",
      "Iteration: 6957 lambda_k: 1 Loss: 0.0014512856387619891\n",
      "Iteration: 6958 lambda_k: 1 Loss: 0.0014512856389836193\n",
      "Iteration: 6959 lambda_k: 1 Loss: 0.0014512856392040357\n",
      "Iteration: 6960 lambda_k: 1 Loss: 0.0014512856394232887\n",
      "Iteration: 6961 lambda_k: 1 Loss: 0.0014512856396413558\n",
      "Iteration: 6962 lambda_k: 1 Loss: 0.0014512856398582643\n",
      "Iteration: 6963 lambda_k: 1 Loss: 0.0014512856400740084\n",
      "Iteration: 6964 lambda_k: 1 Loss: 0.001451285640288556\n",
      "Iteration: 6965 lambda_k: 1 Loss: 0.0014512856405019692\n",
      "Iteration: 6966 lambda_k: 1 Loss: 0.001451285640714234\n",
      "Iteration: 6967 lambda_k: 1 Loss: 0.001451285640925326\n",
      "Iteration: 6968 lambda_k: 1 Loss: 0.0014512856411353183\n",
      "Iteration: 6969 lambda_k: 1 Loss: 0.0014512856413441623\n",
      "Iteration: 6970 lambda_k: 1 Loss: 0.0014512856415518696\n",
      "Iteration: 6971 lambda_k: 1 Loss: 0.0014512856417584416\n",
      "Iteration: 6972 lambda_k: 1 Loss: 0.0014512856419639393\n",
      "Iteration: 6973 lambda_k: 1 Loss: 0.0014512856421683156\n",
      "Iteration: 6974 lambda_k: 1 Loss: 0.0014512856423716035\n",
      "Iteration: 6975 lambda_k: 1 Loss: 0.0014512856425737924\n",
      "Iteration: 6976 lambda_k: 1 Loss: 0.0014512856427749102\n",
      "Iteration: 6977 lambda_k: 1 Loss: 0.0014512856429749314\n",
      "Iteration: 6978 lambda_k: 1 Loss: 0.00145128564317386\n",
      "Iteration: 6979 lambda_k: 1 Loss: 0.0014512856433717015\n",
      "Iteration: 6980 lambda_k: 1 Loss: 0.0014512856435685154\n",
      "Iteration: 6981 lambda_k: 1 Loss: 0.0014512856437642707\n",
      "Iteration: 6982 lambda_k: 1 Loss: 0.0014512856439589622\n",
      "Iteration: 6983 lambda_k: 1 Loss: 0.0014512856441526109\n",
      "Iteration: 6984 lambda_k: 1 Loss: 0.001451285644345196\n",
      "Iteration: 6985 lambda_k: 1 Loss: 0.0014512856445367862\n",
      "Iteration: 6986 lambda_k: 1 Loss: 0.0014512856447273504\n",
      "Iteration: 6987 lambda_k: 1 Loss: 0.0014512856449168665\n",
      "Iteration: 6988 lambda_k: 1 Loss: 0.001451285645105372\n",
      "Iteration: 6989 lambda_k: 1 Loss: 0.0014512856452928605\n",
      "Iteration: 6990 lambda_k: 1 Loss: 0.001451285645479344\n",
      "Iteration: 6991 lambda_k: 1 Loss: 0.001451285645664797\n",
      "Iteration: 6992 lambda_k: 1 Loss: 0.001451285645849259\n",
      "Iteration: 6993 lambda_k: 1 Loss: 0.001451285646032774\n",
      "Iteration: 6994 lambda_k: 1 Loss: 0.00145128564621531\n",
      "Iteration: 6995 lambda_k: 1 Loss: 0.0014512856463968467\n",
      "Iteration: 6996 lambda_k: 1 Loss: 0.0014512856465774173\n",
      "Iteration: 6997 lambda_k: 1 Loss: 0.001451285646757012\n",
      "Iteration: 6998 lambda_k: 1 Loss: 0.0014512856469356449\n",
      "Iteration: 6999 lambda_k: 1 Loss: 0.00145128564711331\n",
      "Iteration: 7000 lambda_k: 1 Loss: 0.0014512856472900363\n",
      "Iteration: 7001 lambda_k: 1 Loss: 0.001451285647465788\n",
      "Iteration: 7002 lambda_k: 1 Loss: 0.0014512856476406135\n",
      "Iteration: 7003 lambda_k: 1 Loss: 0.0014512856478145519\n",
      "Iteration: 7004 lambda_k: 1 Loss: 0.001451285647987497\n",
      "Iteration: 7005 lambda_k: 1 Loss: 0.001451285648159527\n",
      "Iteration: 7006 lambda_k: 1 Loss: 0.0014512856483306733\n",
      "Iteration: 7007 lambda_k: 1 Loss: 0.0014512856485008577\n",
      "Iteration: 7008 lambda_k: 1 Loss: 0.0014512856486701814\n",
      "Iteration: 7009 lambda_k: 1 Loss: 0.0014512856488385674\n",
      "Iteration: 7010 lambda_k: 1 Loss: 0.0014512856490060688\n",
      "Iteration: 7011 lambda_k: 1 Loss: 0.0014512856491726562\n",
      "Iteration: 7012 lambda_k: 1 Loss: 0.0014512856493383848\n",
      "Iteration: 7013 lambda_k: 1 Loss: 0.001451285649503202\n",
      "Iteration: 7014 lambda_k: 1 Loss: 0.0014512856496671142\n",
      "Iteration: 7015 lambda_k: 1 Loss: 0.001451285649830208\n",
      "Iteration: 7016 lambda_k: 1 Loss: 0.0014512856499923927\n",
      "Iteration: 7017 lambda_k: 1 Loss: 0.0014512856501537072\n",
      "Iteration: 7018 lambda_k: 1 Loss: 0.0014512856503141717\n",
      "Iteration: 7019 lambda_k: 1 Loss: 0.0014512856504737836\n",
      "Iteration: 7020 lambda_k: 1 Loss: 0.0014512856506325264\n",
      "Iteration: 7021 lambda_k: 1 Loss: 0.0014512856507904442\n",
      "Iteration: 7022 lambda_k: 1 Loss: 0.0014512856509475201\n",
      "Iteration: 7023 lambda_k: 1 Loss: 0.0014512856511037634\n",
      "Iteration: 7024 lambda_k: 1 Loss: 0.001451285651259169\n",
      "Iteration: 7025 lambda_k: 1 Loss: 0.0014512856514137652\n",
      "Iteration: 7026 lambda_k: 1 Loss: 0.001451285651567537\n",
      "Iteration: 7027 lambda_k: 1 Loss: 0.0014512856517205014\n",
      "Iteration: 7028 lambda_k: 1 Loss: 0.001451285651872636\n",
      "Iteration: 7029 lambda_k: 1 Loss: 0.0014512856520239412\n",
      "Iteration: 7030 lambda_k: 1 Loss: 0.001451285652174433\n",
      "Iteration: 7031 lambda_k: 1 Loss: 0.00145128565232414\n",
      "Iteration: 7032 lambda_k: 1 Loss: 0.0014512856524730498\n",
      "Iteration: 7033 lambda_k: 1 Loss: 0.0014512856526211805\n",
      "Iteration: 7034 lambda_k: 1 Loss: 0.0014512856527685082\n",
      "Iteration: 7035 lambda_k: 1 Loss: 0.00145128565291508\n",
      "Iteration: 7036 lambda_k: 1 Loss: 0.0014512856530608676\n",
      "Iteration: 7037 lambda_k: 1 Loss: 0.0014512856532058504\n",
      "Iteration: 7038 lambda_k: 1 Loss: 0.0014512856533500636\n",
      "Iteration: 7039 lambda_k: 1 Loss: 0.0014512856534935235\n",
      "Iteration: 7040 lambda_k: 1 Loss: 0.0014512856536362296\n",
      "Iteration: 7041 lambda_k: 1 Loss: 0.0014512856537781979\n",
      "Iteration: 7042 lambda_k: 1 Loss: 0.0014512856539193977\n",
      "Iteration: 7043 lambda_k: 1 Loss: 0.0014512856540598318\n",
      "Iteration: 7044 lambda_k: 1 Loss: 0.001451285654199528\n",
      "Iteration: 7045 lambda_k: 1 Loss: 0.0014512856543384578\n",
      "Iteration: 7046 lambda_k: 1 Loss: 0.0014512856544766956\n",
      "Iteration: 7047 lambda_k: 1 Loss: 0.0014512856546141895\n",
      "Iteration: 7048 lambda_k: 1 Loss: 0.0014512856547509371\n",
      "Iteration: 7049 lambda_k: 1 Loss: 0.0014512856548869564\n",
      "Iteration: 7050 lambda_k: 1 Loss: 0.0014512856550222687\n",
      "Iteration: 7051 lambda_k: 1 Loss: 0.0014512856551568763\n",
      "Iteration: 7052 lambda_k: 1 Loss: 0.0014512856552907226\n",
      "Iteration: 7053 lambda_k: 1 Loss: 0.0014512856554238871\n",
      "Iteration: 7054 lambda_k: 1 Loss: 0.0014512856555563404\n",
      "Iteration: 7055 lambda_k: 1 Loss: 0.0014512856556881152\n",
      "Iteration: 7056 lambda_k: 1 Loss: 0.001451285655819158\n",
      "Iteration: 7057 lambda_k: 1 Loss: 0.0014512856559495168\n",
      "Iteration: 7058 lambda_k: 1 Loss: 0.0014512856560792004\n",
      "Iteration: 7059 lambda_k: 1 Loss: 0.0014512856562082061\n",
      "Iteration: 7060 lambda_k: 1 Loss: 0.0014512856563364924\n",
      "Iteration: 7061 lambda_k: 1 Loss: 0.001451285656464123\n",
      "Iteration: 7062 lambda_k: 1 Loss: 0.0014512856565910804\n",
      "Iteration: 7063 lambda_k: 1 Loss: 0.0014512856567173989\n",
      "Iteration: 7064 lambda_k: 1 Loss: 0.0014512856568430154\n",
      "Iteration: 7065 lambda_k: 1 Loss: 0.001451285656967957\n",
      "Iteration: 7066 lambda_k: 1 Loss: 0.001451285657092259\n",
      "Iteration: 7067 lambda_k: 1 Loss: 0.0014512856572158853\n",
      "Iteration: 7068 lambda_k: 1 Loss: 0.0014512856573388614\n",
      "Iteration: 7069 lambda_k: 1 Loss: 0.001451285657461207\n",
      "Iteration: 7070 lambda_k: 1 Loss: 0.001451285657582875\n",
      "Iteration: 7071 lambda_k: 1 Loss: 0.001451285657703914\n",
      "Iteration: 7072 lambda_k: 1 Loss: 0.001451285657824322\n",
      "Iteration: 7073 lambda_k: 1 Loss: 0.0014512856579440812\n",
      "Iteration: 7074 lambda_k: 1 Loss: 0.0014512856580632075\n",
      "Iteration: 7075 lambda_k: 1 Loss: 0.0014512856581817219\n",
      "Iteration: 7076 lambda_k: 1 Loss: 0.0014512856582996516\n",
      "Iteration: 7077 lambda_k: 1 Loss: 0.0014512856584169285\n",
      "Iteration: 7078 lambda_k: 1 Loss: 0.0014512856585335693\n",
      "Iteration: 7079 lambda_k: 1 Loss: 0.0014512856586495868\n",
      "Iteration: 7080 lambda_k: 1 Loss: 0.0014512856587650018\n",
      "Iteration: 7081 lambda_k: 1 Loss: 0.001451285658879825\n",
      "Iteration: 7082 lambda_k: 1 Loss: 0.0014512856589940224\n",
      "Iteration: 7083 lambda_k: 1 Loss: 0.0014512856591076307\n",
      "Iteration: 7084 lambda_k: 1 Loss: 0.0014512856592206577\n",
      "Iteration: 7085 lambda_k: 1 Loss: 0.001451285659333069\n",
      "Iteration: 7086 lambda_k: 1 Loss: 0.0014512856594448942\n",
      "Iteration: 7087 lambda_k: 1 Loss: 0.0014512856595561582\n",
      "Iteration: 7088 lambda_k: 1 Loss: 0.0014512856596668134\n",
      "Iteration: 7089 lambda_k: 1 Loss: 0.0014512856597768925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7090 lambda_k: 1 Loss: 0.00145128565988638\n",
      "Iteration: 7091 lambda_k: 1 Loss: 0.0014512856599952935\n",
      "Iteration: 7092 lambda_k: 1 Loss: 0.0014512856601036754\n",
      "Iteration: 7093 lambda_k: 1 Loss: 0.001451285660211442\n",
      "Iteration: 7094 lambda_k: 1 Loss: 0.001451285660318658\n",
      "Iteration: 7095 lambda_k: 1 Loss: 0.0014512856604252979\n",
      "Iteration: 7096 lambda_k: 1 Loss: 0.001451285660531381\n",
      "Iteration: 7097 lambda_k: 1 Loss: 0.0014512856606369365\n",
      "Iteration: 7098 lambda_k: 1 Loss: 0.0014512856607419135\n",
      "Iteration: 7099 lambda_k: 1 Loss: 0.0014512856608463562\n",
      "Iteration: 7100 lambda_k: 1 Loss: 0.0014512856609502347\n",
      "Iteration: 7101 lambda_k: 1 Loss: 0.0014512856610535713\n",
      "Iteration: 7102 lambda_k: 1 Loss: 0.0014512856611563533\n",
      "Iteration: 7103 lambda_k: 1 Loss: 0.0014512856612586024\n",
      "Iteration: 7104 lambda_k: 1 Loss: 0.0014512856613603047\n",
      "Iteration: 7105 lambda_k: 1 Loss: 0.0014512856614614894\n",
      "Iteration: 7106 lambda_k: 1 Loss: 0.0014512856615621678\n",
      "Iteration: 7107 lambda_k: 1 Loss: 0.0014512856616623008\n",
      "Iteration: 7108 lambda_k: 1 Loss: 0.0014512856617619279\n",
      "Iteration: 7109 lambda_k: 1 Loss: 0.0014512856618609988\n",
      "Iteration: 7110 lambda_k: 1 Loss: 0.0014512856619595764\n",
      "Iteration: 7111 lambda_k: 1 Loss: 0.001451285662057616\n",
      "Iteration: 7112 lambda_k: 1 Loss: 0.0014512856621551632\n",
      "Iteration: 7113 lambda_k: 1 Loss: 0.0014512856622521742\n",
      "Iteration: 7114 lambda_k: 1 Loss: 0.0014512856623486972\n",
      "Iteration: 7115 lambda_k: 1 Loss: 0.0014512856624447092\n",
      "Iteration: 7116 lambda_k: 1 Loss: 0.0014512856625402111\n",
      "Iteration: 7117 lambda_k: 1 Loss: 0.0014512856626352087\n",
      "Iteration: 7118 lambda_k: 1 Loss: 0.00145128566272972\n",
      "Iteration: 7119 lambda_k: 1 Loss: 0.0014512856628237313\n",
      "Iteration: 7120 lambda_k: 1 Loss: 0.0014512856629172626\n",
      "Iteration: 7121 lambda_k: 1 Loss: 0.001451285663010293\n",
      "Iteration: 7122 lambda_k: 1 Loss: 0.0014512856631028464\n",
      "Iteration: 7123 lambda_k: 1 Loss: 0.0014512856631949168\n",
      "Iteration: 7124 lambda_k: 1 Loss: 0.0014512856632865208\n",
      "Iteration: 7125 lambda_k: 1 Loss: 0.0014512856633776175\n",
      "Iteration: 7126 lambda_k: 1 Loss: 0.0014512856634682682\n",
      "Iteration: 7127 lambda_k: 1 Loss: 0.0014512856635584197\n",
      "Iteration: 7128 lambda_k: 1 Loss: 0.001451285663648129\n",
      "Iteration: 7129 lambda_k: 1 Loss: 0.0014512856637373406\n",
      "Iteration: 7130 lambda_k: 1 Loss: 0.001451285663826119\n",
      "Iteration: 7131 lambda_k: 1 Loss: 0.0014512856639144023\n",
      "Iteration: 7132 lambda_k: 1 Loss: 0.0014512856640022638\n",
      "Iteration: 7133 lambda_k: 1 Loss: 0.001451285664089647\n",
      "Iteration: 7134 lambda_k: 1 Loss: 0.0014512856641765908\n",
      "Iteration: 7135 lambda_k: 1 Loss: 0.0014512856642630767\n",
      "Iteration: 7136 lambda_k: 1 Loss: 0.0014512856643490726\n",
      "Iteration: 7137 lambda_k: 1 Loss: 0.0014512856644346413\n",
      "Iteration: 7138 lambda_k: 1 Loss: 0.0014512856645197661\n",
      "Iteration: 7139 lambda_k: 1 Loss: 0.0014512856646044592\n",
      "Iteration: 7140 lambda_k: 1 Loss: 0.0014512856646887145\n",
      "Iteration: 7141 lambda_k: 1 Loss: 0.0014512856647725246\n",
      "Iteration: 7142 lambda_k: 1 Loss: 0.0014512856648558985\n",
      "Iteration: 7143 lambda_k: 1 Loss: 0.0014512856649388436\n",
      "Iteration: 7144 lambda_k: 1 Loss: 0.0014512856650213551\n",
      "Iteration: 7145 lambda_k: 1 Loss: 0.0014512856651034175\n",
      "Iteration: 7146 lambda_k: 1 Loss: 0.0014512856651850742\n",
      "Iteration: 7147 lambda_k: 1 Loss: 0.0014512856652662966\n",
      "Iteration: 7148 lambda_k: 1 Loss: 0.0014512856653471112\n",
      "Iteration: 7149 lambda_k: 1 Loss: 0.0014512856654275055\n",
      "Iteration: 7150 lambda_k: 1 Loss: 0.0014512856655074487\n",
      "Iteration: 7151 lambda_k: 1 Loss: 0.0014512856655870216\n",
      "Iteration: 7152 lambda_k: 1 Loss: 0.0014512856656661468\n",
      "Iteration: 7153 lambda_k: 1 Loss: 0.0014512856657448922\n",
      "Iteration: 7154 lambda_k: 1 Loss: 0.0014512856658232048\n",
      "Iteration: 7155 lambda_k: 1 Loss: 0.0014512856659010993\n",
      "Iteration: 7156 lambda_k: 1 Loss: 0.0014512856659786016\n",
      "Iteration: 7157 lambda_k: 1 Loss: 0.0014512856660557042\n",
      "Iteration: 7158 lambda_k: 1 Loss: 0.0014512856661324303\n",
      "Iteration: 7159 lambda_k: 1 Loss: 0.0014512856662087556\n",
      "Iteration: 7160 lambda_k: 1 Loss: 0.0014512856662846378\n",
      "Iteration: 7161 lambda_k: 1 Loss: 0.0014512856663601746\n",
      "Iteration: 7162 lambda_k: 1 Loss: 0.0014512856664352957\n",
      "Iteration: 7163 lambda_k: 1 Loss: 0.0014512856665100636\n",
      "Iteration: 7164 lambda_k: 1 Loss: 0.0014512856665844212\n",
      "Iteration: 7165 lambda_k: 1 Loss: 0.0014512856666583835\n",
      "Iteration: 7166 lambda_k: 1 Loss: 0.001451285666731967\n",
      "Iteration: 7167 lambda_k: 1 Loss: 0.00145128566680519\n",
      "Iteration: 7168 lambda_k: 1 Loss: 0.001451285666877987\n",
      "Iteration: 7169 lambda_k: 1 Loss: 0.0014512856669504537\n",
      "Iteration: 7170 lambda_k: 1 Loss: 0.0014512856670225224\n",
      "Iteration: 7171 lambda_k: 1 Loss: 0.0014512856670942482\n",
      "Iteration: 7172 lambda_k: 1 Loss: 0.0014512856671655765\n",
      "Iteration: 7173 lambda_k: 1 Loss: 0.00145128566723655\n",
      "Iteration: 7174 lambda_k: 1 Loss: 0.0014512856673071588\n",
      "Iteration: 7175 lambda_k: 1 Loss: 0.0014512856673773989\n",
      "Iteration: 7176 lambda_k: 1 Loss: 0.0014512856674472644\n",
      "Iteration: 7177 lambda_k: 1 Loss: 0.0014512856675167921\n",
      "Iteration: 7178 lambda_k: 1 Loss: 0.0014512856675859532\n",
      "Iteration: 7179 lambda_k: 1 Loss: 0.0014512856676547417\n",
      "Iteration: 7180 lambda_k: 1 Loss: 0.00145128566772318\n",
      "Iteration: 7181 lambda_k: 1 Loss: 0.0014512856677912811\n",
      "Iteration: 7182 lambda_k: 1 Loss: 0.0014512856678589928\n",
      "Iteration: 7183 lambda_k: 1 Loss: 0.0014512856679263927\n",
      "Iteration: 7184 lambda_k: 1 Loss: 0.0014512856679934423\n",
      "Iteration: 7185 lambda_k: 1 Loss: 0.0014512856680601032\n",
      "Iteration: 7186 lambda_k: 1 Loss: 0.0014512856681264752\n",
      "Iteration: 7187 lambda_k: 1 Loss: 0.0014512856681924825\n",
      "Iteration: 7188 lambda_k: 1 Loss: 0.0014512856682581464\n",
      "Iteration: 7189 lambda_k: 1 Loss: 0.001451285668323479\n",
      "Iteration: 7190 lambda_k: 1 Loss: 0.0014512856683884863\n",
      "Iteration: 7191 lambda_k: 1 Loss: 0.0014512856684531393\n",
      "Iteration: 7192 lambda_k: 1 Loss: 0.0014512856685174698\n",
      "Iteration: 7193 lambda_k: 1 Loss: 0.0014512856685814856\n",
      "Iteration: 7194 lambda_k: 1 Loss: 0.001451285668645146\n",
      "Iteration: 7195 lambda_k: 1 Loss: 0.0014512856687084877\n",
      "Iteration: 7196 lambda_k: 1 Loss: 0.0014512856687715074\n",
      "Iteration: 7197 lambda_k: 1 Loss: 0.0014512856688342058\n",
      "Iteration: 7198 lambda_k: 1 Loss: 0.0014512856688966016\n",
      "Iteration: 7199 lambda_k: 1 Loss: 0.0014512856689586457\n",
      "Iteration: 7200 lambda_k: 1 Loss: 0.0014512856690203772\n",
      "Iteration: 7201 lambda_k: 1 Loss: 0.0014512856690817684\n",
      "Iteration: 7202 lambda_k: 1 Loss: 0.0014512856691428799\n",
      "Iteration: 7203 lambda_k: 1 Loss: 0.0014512856692036678\n",
      "Iteration: 7204 lambda_k: 1 Loss: 0.0014512856692641303\n",
      "Iteration: 7205 lambda_k: 1 Loss: 0.0014512856693243027\n",
      "Iteration: 7206 lambda_k: 1 Loss: 0.0014512856693841628\n",
      "Iteration: 7207 lambda_k: 1 Loss: 0.0014512856694437063\n",
      "Iteration: 7208 lambda_k: 1 Loss: 0.0014512856695029657\n",
      "Iteration: 7209 lambda_k: 1 Loss: 0.001451285669561922\n",
      "Iteration: 7210 lambda_k: 1 Loss: 0.0014512856696205644\n",
      "Iteration: 7211 lambda_k: 1 Loss: 0.0014512856696788706\n",
      "Iteration: 7212 lambda_k: 1 Loss: 0.0014512856697369183\n",
      "Iteration: 7213 lambda_k: 1 Loss: 0.0014512856697946653\n",
      "Iteration: 7214 lambda_k: 1 Loss: 0.0014512856698521124\n",
      "Iteration: 7215 lambda_k: 1 Loss: 0.0014512856699092826\n",
      "Iteration: 7216 lambda_k: 1 Loss: 0.0014512856699661614\n",
      "Iteration: 7217 lambda_k: 1 Loss: 0.0014512856700227208\n",
      "Iteration: 7218 lambda_k: 1 Loss: 0.0014512856700789913\n",
      "Iteration: 7219 lambda_k: 1 Loss: 0.0014512856701349992\n",
      "Iteration: 7220 lambda_k: 1 Loss: 0.0014512856701907242\n",
      "Iteration: 7221 lambda_k: 1 Loss: 0.0014512856702461276\n",
      "Iteration: 7222 lambda_k: 1 Loss: 0.0014512856703012616\n",
      "Iteration: 7223 lambda_k: 1 Loss: 0.0014512856703561314\n",
      "Iteration: 7224 lambda_k: 1 Loss: 0.001451285670410698\n",
      "Iteration: 7225 lambda_k: 1 Loss: 0.0014512856704649911\n",
      "Iteration: 7226 lambda_k: 1 Loss: 0.0014512856705189855\n",
      "Iteration: 7227 lambda_k: 1 Loss: 0.0014512856705727396\n",
      "Iteration: 7228 lambda_k: 1 Loss: 0.0014512856706262128\n",
      "Iteration: 7229 lambda_k: 1 Loss: 0.0014512856706794118\n",
      "Iteration: 7230 lambda_k: 1 Loss: 0.0014512856707323226\n",
      "Iteration: 7231 lambda_k: 1 Loss: 0.0014512856707849717\n",
      "Iteration: 7232 lambda_k: 1 Loss: 0.0014512856708373682\n",
      "Iteration: 7233 lambda_k: 1 Loss: 0.0014512856708894751\n",
      "Iteration: 7234 lambda_k: 1 Loss: 0.0014512856709413477\n",
      "Iteration: 7235 lambda_k: 1 Loss: 0.0014512856709929542\n",
      "Iteration: 7236 lambda_k: 1 Loss: 0.0014512856710442881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7237 lambda_k: 1 Loss: 0.0014512856710953528\n",
      "Iteration: 7238 lambda_k: 1 Loss: 0.0014512856711461361\n",
      "Iteration: 7239 lambda_k: 1 Loss: 0.001451285671196707\n",
      "Iteration: 7240 lambda_k: 1 Loss: 0.0014512856712469893\n",
      "Iteration: 7241 lambda_k: 1 Loss: 0.0014512856712970298\n",
      "Iteration: 7242 lambda_k: 1 Loss: 0.0014512856713467756\n",
      "Iteration: 7243 lambda_k: 1 Loss: 0.0014512856713963203\n",
      "Iteration: 7244 lambda_k: 1 Loss: 0.0014512856714455934\n",
      "Iteration: 7245 lambda_k: 1 Loss: 0.0014512856714946171\n",
      "Iteration: 7246 lambda_k: 1 Loss: 0.0014512856715433956\n",
      "Iteration: 7247 lambda_k: 1 Loss: 0.0014512856715918837\n",
      "Iteration: 7248 lambda_k: 1 Loss: 0.0014512856716401754\n",
      "Iteration: 7249 lambda_k: 1 Loss: 0.001451285671688202\n",
      "Iteration: 7250 lambda_k: 1 Loss: 0.0014512856717359866\n",
      "Iteration: 7251 lambda_k: 1 Loss: 0.0014512856717835117\n",
      "Iteration: 7252 lambda_k: 1 Loss: 0.001451285671830826\n",
      "Iteration: 7253 lambda_k: 1 Loss: 0.0014512856718779015\n",
      "Iteration: 7254 lambda_k: 1 Loss: 0.0014512856719247236\n",
      "Iteration: 7255 lambda_k: 1 Loss: 0.0014512856719713187\n",
      "Iteration: 7256 lambda_k: 1 Loss: 0.001451285672017634\n",
      "Iteration: 7257 lambda_k: 1 Loss: 0.0014512856720637593\n",
      "Iteration: 7258 lambda_k: 1 Loss: 0.0014512856721096408\n",
      "Iteration: 7259 lambda_k: 1 Loss: 0.0014512856721552903\n",
      "Iteration: 7260 lambda_k: 1 Loss: 0.0014512856722007086\n",
      "Iteration: 7261 lambda_k: 1 Loss: 0.0014512856722458552\n",
      "Iteration: 7262 lambda_k: 1 Loss: 0.0014512856722908227\n",
      "Iteration: 7263 lambda_k: 1 Loss: 0.001451285672335553\n",
      "Iteration: 7264 lambda_k: 1 Loss: 0.001451285672380056\n",
      "Iteration: 7265 lambda_k: 1 Loss: 0.0014512856724243218\n",
      "Iteration: 7266 lambda_k: 1 Loss: 0.0014512856724683534\n",
      "Iteration: 7267 lambda_k: 1 Loss: 0.001451285672512183\n",
      "Iteration: 7268 lambda_k: 1 Loss: 0.0014512856725558034\n",
      "Iteration: 7269 lambda_k: 1 Loss: 0.0014512856725991878\n",
      "Iteration: 7270 lambda_k: 1 Loss: 0.0014512856726423382\n",
      "Iteration: 7271 lambda_k: 1 Loss: 0.0014512856726852676\n",
      "Iteration: 7272 lambda_k: 1 Loss: 0.0014512856727280042\n",
      "Iteration: 7273 lambda_k: 1 Loss: 0.0014512856727705262\n",
      "Iteration: 7274 lambda_k: 1 Loss: 0.0014512856728128203\n",
      "Iteration: 7275 lambda_k: 1 Loss: 0.0014512856728549173\n",
      "Iteration: 7276 lambda_k: 1 Loss: 0.0014512856728967566\n",
      "Iteration: 7277 lambda_k: 1 Loss: 0.0014512856729384292\n",
      "Iteration: 7278 lambda_k: 1 Loss: 0.0014512856729798954\n",
      "Iteration: 7279 lambda_k: 1 Loss: 0.0014512856730211495\n",
      "Iteration: 7280 lambda_k: 1 Loss: 0.0014512856730621686\n",
      "Iteration: 7281 lambda_k: 1 Loss: 0.0014512856731029747\n",
      "Iteration: 7282 lambda_k: 1 Loss: 0.0014512856731435694\n",
      "Iteration: 7283 lambda_k: 1 Loss: 0.0014512856731839897\n",
      "Iteration: 7284 lambda_k: 1 Loss: 0.0014512856732242132\n",
      "Iteration: 7285 lambda_k: 1 Loss: 0.0014512856732642177\n",
      "Iteration: 7286 lambda_k: 1 Loss: 0.0014512856733040287\n",
      "Iteration: 7287 lambda_k: 1 Loss: 0.0014512856733436066\n",
      "Iteration: 7288 lambda_k: 1 Loss: 0.0014512856733830308\n",
      "Iteration: 7289 lambda_k: 1 Loss: 0.0014512856734222584\n",
      "Iteration: 7290 lambda_k: 1 Loss: 0.0014512856734612653\n",
      "Iteration: 7291 lambda_k: 1 Loss: 0.001451285673500076\n",
      "Iteration: 7292 lambda_k: 1 Loss: 0.001451285673538684\n",
      "Iteration: 7293 lambda_k: 1 Loss: 0.001451285673577094\n",
      "Iteration: 7294 lambda_k: 1 Loss: 0.001451285673615324\n",
      "Iteration: 7295 lambda_k: 1 Loss: 0.0014512856736533808\n",
      "Iteration: 7296 lambda_k: 1 Loss: 0.0014512856736912242\n",
      "Iteration: 7297 lambda_k: 1 Loss: 0.0014512856737288883\n",
      "Iteration: 7298 lambda_k: 1 Loss: 0.0014512856737663412\n",
      "Iteration: 7299 lambda_k: 1 Loss: 0.001451285673803606\n",
      "Iteration: 7300 lambda_k: 1 Loss: 0.0014512856738407044\n",
      "Iteration: 7301 lambda_k: 1 Loss: 0.0014512856738776146\n",
      "Iteration: 7302 lambda_k: 1 Loss: 0.001451285673914336\n",
      "Iteration: 7303 lambda_k: 1 Loss: 0.0014512856739508678\n",
      "Iteration: 7304 lambda_k: 1 Loss: 0.001451285673987222\n",
      "Iteration: 7305 lambda_k: 1 Loss: 0.001451285674023368\n",
      "Iteration: 7306 lambda_k: 1 Loss: 0.0014512856740593355\n",
      "Iteration: 7307 lambda_k: 1 Loss: 0.0014512856740951507\n",
      "Iteration: 7308 lambda_k: 1 Loss: 0.0014512856741307715\n",
      "Iteration: 7309 lambda_k: 1 Loss: 0.0014512856741662273\n",
      "Iteration: 7310 lambda_k: 1 Loss: 0.001451285674201491\n",
      "Iteration: 7311 lambda_k: 1 Loss: 0.0014512856742365618\n",
      "Iteration: 7312 lambda_k: 1 Loss: 0.001451285674271462\n",
      "Iteration: 7313 lambda_k: 1 Loss: 0.0014512856743062095\n",
      "Iteration: 7314 lambda_k: 1 Loss: 0.001451285674340797\n",
      "Iteration: 7315 lambda_k: 1 Loss: 0.0014512856743751851\n",
      "Iteration: 7316 lambda_k: 1 Loss: 0.00145128567440941\n",
      "Iteration: 7317 lambda_k: 1 Loss: 0.0014512856744434417\n",
      "Iteration: 7318 lambda_k: 1 Loss: 0.0014512856744773224\n",
      "Iteration: 7319 lambda_k: 1 Loss: 0.00145128567451101\n",
      "Iteration: 7320 lambda_k: 1 Loss: 0.0014512856745445488\n",
      "Iteration: 7321 lambda_k: 1 Loss: 0.0014512856745779402\n",
      "Iteration: 7322 lambda_k: 1 Loss: 0.001451285674611156\n",
      "Iteration: 7323 lambda_k: 1 Loss: 0.0014512856746441828\n",
      "Iteration: 7324 lambda_k: 1 Loss: 0.0014512856746770502\n",
      "Iteration: 7325 lambda_k: 1 Loss: 0.0014512856747097608\n",
      "Iteration: 7326 lambda_k: 1 Loss: 0.0014512856747422695\n",
      "Iteration: 7327 lambda_k: 1 Loss: 0.0014512856747746763\n",
      "Iteration: 7328 lambda_k: 1 Loss: 0.0014512856748069025\n",
      "Iteration: 7329 lambda_k: 1 Loss: 0.0014512856748389797\n",
      "Iteration: 7330 lambda_k: 1 Loss: 0.0014512856748708691\n",
      "Iteration: 7331 lambda_k: 1 Loss: 0.0014512856749026148\n",
      "Iteration: 7332 lambda_k: 1 Loss: 0.0014512856749341835\n",
      "Iteration: 7333 lambda_k: 1 Loss: 0.001451285674965595\n",
      "Iteration: 7334 lambda_k: 1 Loss: 0.0014512856749968278\n",
      "Iteration: 7335 lambda_k: 1 Loss: 0.0014512856750279563\n",
      "Iteration: 7336 lambda_k: 1 Loss: 0.0014512856750589329\n",
      "Iteration: 7337 lambda_k: 1 Loss: 0.0014512856750897292\n",
      "Iteration: 7338 lambda_k: 1 Loss: 0.0014512856751203787\n",
      "Iteration: 7339 lambda_k: 1 Loss: 0.0014512856751508793\n",
      "Iteration: 7340 lambda_k: 1 Loss: 0.0014512856751811984\n",
      "Iteration: 7341 lambda_k: 1 Loss: 0.0014512856752113767\n",
      "Iteration: 7342 lambda_k: 1 Loss: 0.0014512856752413802\n",
      "Iteration: 7343 lambda_k: 1 Loss: 0.0014512856752712893\n",
      "Iteration: 7344 lambda_k: 1 Loss: 0.0014512856753010563\n",
      "Iteration: 7345 lambda_k: 1 Loss: 0.0014512856753306446\n",
      "Iteration: 7346 lambda_k: 1 Loss: 0.0014512856753600958\n",
      "Iteration: 7347 lambda_k: 1 Loss: 0.0014512856753893827\n",
      "Iteration: 7348 lambda_k: 1 Loss: 0.0014512856754185408\n",
      "Iteration: 7349 lambda_k: 1 Loss: 0.0014512856754475177\n",
      "Iteration: 7350 lambda_k: 1 Loss: 0.0014512856754763613\n",
      "Iteration: 7351 lambda_k: 1 Loss: 0.0014512856755051018\n",
      "Iteration: 7352 lambda_k: 1 Loss: 0.0014512856755336816\n",
      "Iteration: 7353 lambda_k: 1 Loss: 0.0014512856755621345\n",
      "Iteration: 7354 lambda_k: 1 Loss: 0.001451285675590423\n",
      "Iteration: 7355 lambda_k: 1 Loss: 0.0014512856756185761\n",
      "Iteration: 7356 lambda_k: 1 Loss: 0.0014512856756465932\n",
      "Iteration: 7357 lambda_k: 1 Loss: 0.0014512856756744414\n",
      "Iteration: 7358 lambda_k: 1 Loss: 0.001451285675702166\n",
      "Iteration: 7359 lambda_k: 1 Loss: 0.001451285675729722\n",
      "Iteration: 7360 lambda_k: 1 Loss: 0.0014512856757571992\n",
      "Iteration: 7361 lambda_k: 1 Loss: 0.0014512856757845428\n",
      "Iteration: 7362 lambda_k: 1 Loss: 0.00145128567581173\n",
      "Iteration: 7363 lambda_k: 1 Loss: 0.0014512856758387837\n",
      "Iteration: 7364 lambda_k: 1 Loss: 0.0014512856758657007\n",
      "Iteration: 7365 lambda_k: 1 Loss: 0.0014512856758924677\n",
      "Iteration: 7366 lambda_k: 1 Loss: 0.0014512856759191168\n",
      "Iteration: 7367 lambda_k: 1 Loss: 0.0014512856759456103\n",
      "Iteration: 7368 lambda_k: 1 Loss: 0.0014512856759719812\n",
      "Iteration: 7369 lambda_k: 1 Loss: 0.0014512856759982162\n",
      "Iteration: 7370 lambda_k: 1 Loss: 0.0014512856760243425\n",
      "Iteration: 7371 lambda_k: 1 Loss: 0.0014512856760503588\n",
      "Iteration: 7372 lambda_k: 1 Loss: 0.0014512856760762283\n",
      "Iteration: 7373 lambda_k: 1 Loss: 0.0014512856761019744\n",
      "Iteration: 7374 lambda_k: 1 Loss: 0.0014512856761275828\n",
      "Iteration: 7375 lambda_k: 1 Loss: 0.0014512856761530607\n",
      "Iteration: 7376 lambda_k: 1 Loss: 0.001451285676178412\n",
      "Iteration: 7377 lambda_k: 1 Loss: 0.001451285676203633\n",
      "Iteration: 7378 lambda_k: 1 Loss: 0.0014512856762287035\n",
      "Iteration: 7379 lambda_k: 1 Loss: 0.001451285676253702\n",
      "Iteration: 7380 lambda_k: 1 Loss: 0.00145128567627859\n",
      "Iteration: 7381 lambda_k: 1 Loss: 0.0014512856763033528\n",
      "Iteration: 7382 lambda_k: 1 Loss: 0.0014512856763279677\n",
      "Iteration: 7383 lambda_k: 1 Loss: 0.0014512856763524691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7384 lambda_k: 1 Loss: 0.0014512856763768296\n",
      "Iteration: 7385 lambda_k: 1 Loss: 0.0014512856764010752\n",
      "Iteration: 7386 lambda_k: 1 Loss: 0.0014512856764252134\n",
      "Iteration: 7387 lambda_k: 1 Loss: 0.0014512856764491938\n",
      "Iteration: 7388 lambda_k: 1 Loss: 0.0014512856764730773\n",
      "Iteration: 7389 lambda_k: 1 Loss: 0.0014512856764968293\n",
      "Iteration: 7390 lambda_k: 1 Loss: 0.0014512856765204881\n",
      "Iteration: 7391 lambda_k: 1 Loss: 0.0014512856765440509\n",
      "Iteration: 7392 lambda_k: 1 Loss: 0.0014512856765674965\n",
      "Iteration: 7393 lambda_k: 1 Loss: 0.0014512856765908013\n",
      "Iteration: 7394 lambda_k: 1 Loss: 0.001451285676614004\n",
      "Iteration: 7395 lambda_k: 1 Loss: 0.0014512856766370778\n",
      "Iteration: 7396 lambda_k: 1 Loss: 0.0014512856766600464\n",
      "Iteration: 7397 lambda_k: 1 Loss: 0.0014512856766828956\n",
      "Iteration: 7398 lambda_k: 1 Loss: 0.0014512856767056066\n",
      "Iteration: 7399 lambda_k: 1 Loss: 0.0014512856767282224\n",
      "Iteration: 7400 lambda_k: 1 Loss: 0.0014512856767507185\n",
      "Iteration: 7401 lambda_k: 1 Loss: 0.0014512856767731242\n",
      "Iteration: 7402 lambda_k: 1 Loss: 0.0014512856767954457\n",
      "Iteration: 7403 lambda_k: 1 Loss: 0.001451285676817654\n",
      "Iteration: 7404 lambda_k: 1 Loss: 0.001451285676839731\n",
      "Iteration: 7405 lambda_k: 1 Loss: 0.001451285676861709\n",
      "Iteration: 7406 lambda_k: 1 Loss: 0.0014512856768835757\n",
      "Iteration: 7407 lambda_k: 1 Loss: 0.0014512856769053274\n",
      "Iteration: 7408 lambda_k: 1 Loss: 0.0014512856769269594\n",
      "Iteration: 7409 lambda_k: 1 Loss: 0.0014512856769484887\n",
      "Iteration: 7410 lambda_k: 1 Loss: 0.001451285676969908\n",
      "Iteration: 7411 lambda_k: 1 Loss: 0.001451285676991213\n",
      "Iteration: 7412 lambda_k: 1 Loss: 0.0014512856770124117\n",
      "Iteration: 7413 lambda_k: 1 Loss: 0.0014512856770335467\n",
      "Iteration: 7414 lambda_k: 1 Loss: 0.0014512856770545828\n",
      "Iteration: 7415 lambda_k: 1 Loss: 0.0014512856770755097\n",
      "Iteration: 7416 lambda_k: 1 Loss: 0.0014512856770963482\n",
      "Iteration: 7417 lambda_k: 1 Loss: 0.0014512856771170504\n",
      "Iteration: 7418 lambda_k: 1 Loss: 0.001451285677137663\n",
      "Iteration: 7419 lambda_k: 1 Loss: 0.0014512856771581586\n",
      "Iteration: 7420 lambda_k: 1 Loss: 0.0014512856771785546\n",
      "Iteration: 7421 lambda_k: 1 Loss: 0.0014512856771988559\n",
      "Iteration: 7422 lambda_k: 1 Loss: 0.0014512856772190598\n",
      "Iteration: 7423 lambda_k: 1 Loss: 0.001451285677239138\n",
      "Iteration: 7424 lambda_k: 1 Loss: 0.0014512856772591182\n",
      "Iteration: 7425 lambda_k: 1 Loss: 0.0014512856772790016\n",
      "Iteration: 7426 lambda_k: 1 Loss: 0.001451285677298835\n",
      "Iteration: 7427 lambda_k: 1 Loss: 0.0014512856773185752\n",
      "Iteration: 7428 lambda_k: 1 Loss: 0.0014512856773382132\n",
      "Iteration: 7429 lambda_k: 1 Loss: 0.001451285677357738\n",
      "Iteration: 7430 lambda_k: 1 Loss: 0.0014512856773771796\n",
      "Iteration: 7431 lambda_k: 1 Loss: 0.0014512856773965062\n",
      "Iteration: 7432 lambda_k: 1 Loss: 0.0014512856774157486\n",
      "Iteration: 7433 lambda_k: 1 Loss: 0.0014512856774348787\n",
      "Iteration: 7434 lambda_k: 1 Loss: 0.0014512856774539253\n",
      "Iteration: 7435 lambda_k: 1 Loss: 0.0014512856774728871\n",
      "Iteration: 7436 lambda_k: 1 Loss: 0.0014512856774917258\n",
      "Iteration: 7437 lambda_k: 1 Loss: 0.0014512856775104823\n",
      "Iteration: 7438 lambda_k: 1 Loss: 0.001451285677529146\n",
      "Iteration: 7439 lambda_k: 1 Loss: 0.0014512856775476984\n",
      "Iteration: 7440 lambda_k: 1 Loss: 0.0014512856775661682\n",
      "Iteration: 7441 lambda_k: 1 Loss: 0.0014512856775845823\n",
      "Iteration: 7442 lambda_k: 1 Loss: 0.00145128567760292\n",
      "Iteration: 7443 lambda_k: 1 Loss: 0.0014512856776211665\n",
      "Iteration: 7444 lambda_k: 1 Loss: 0.0014512856776393067\n",
      "Iteration: 7445 lambda_k: 1 Loss: 0.0014512856776573743\n",
      "Iteration: 7446 lambda_k: 1 Loss: 0.001451285677675346\n",
      "Iteration: 7447 lambda_k: 1 Loss: 0.0014512856776932024\n",
      "Iteration: 7448 lambda_k: 1 Loss: 0.0014512856777109846\n",
      "Iteration: 7449 lambda_k: 1 Loss: 0.0014512856777286842\n",
      "Iteration: 7450 lambda_k: 1 Loss: 0.0014512856777462763\n",
      "Iteration: 7451 lambda_k: 1 Loss: 0.001451285677763798\n",
      "Iteration: 7452 lambda_k: 1 Loss: 0.0014512856777812264\n",
      "Iteration: 7453 lambda_k: 1 Loss: 0.00145128567779857\n",
      "Iteration: 7454 lambda_k: 1 Loss: 0.0014512856778158005\n",
      "Iteration: 7455 lambda_k: 1 Loss: 0.0014512856778329639\n",
      "Iteration: 7456 lambda_k: 1 Loss: 0.001451285677850047\n",
      "Iteration: 7457 lambda_k: 1 Loss: 0.001451285677867072\n",
      "Iteration: 7458 lambda_k: 1 Loss: 0.0014512856778840346\n",
      "Iteration: 7459 lambda_k: 1 Loss: 0.001451285677900912\n",
      "Iteration: 7460 lambda_k: 1 Loss: 0.0014512856779176834\n",
      "Iteration: 7461 lambda_k: 1 Loss: 0.0014512856779343897\n",
      "Iteration: 7462 lambda_k: 1 Loss: 0.0014512856779509962\n",
      "Iteration: 7463 lambda_k: 1 Loss: 0.0014512856779675333\n",
      "Iteration: 7464 lambda_k: 1 Loss: 0.001451285677983982\n",
      "Iteration: 7465 lambda_k: 1 Loss: 0.0014512856780003382\n",
      "Iteration: 7466 lambda_k: 1 Loss: 0.001451285678016621\n",
      "Iteration: 7467 lambda_k: 1 Loss: 0.001451285678032829\n",
      "Iteration: 7468 lambda_k: 1 Loss: 0.00145128567804894\n",
      "Iteration: 7469 lambda_k: 1 Loss: 0.0014512856780649838\n",
      "Iteration: 7470 lambda_k: 1 Loss: 0.001451285678080946\n",
      "Iteration: 7471 lambda_k: 1 Loss: 0.001451285678096808\n",
      "Iteration: 7472 lambda_k: 1 Loss: 0.0014512856781126102\n",
      "Iteration: 7473 lambda_k: 1 Loss: 0.0014512856781283285\n",
      "Iteration: 7474 lambda_k: 1 Loss: 0.0014512856781440167\n",
      "Iteration: 7475 lambda_k: 1 Loss: 0.0014512856781595718\n",
      "Iteration: 7476 lambda_k: 1 Loss: 0.0014512856781751\n",
      "Iteration: 7477 lambda_k: 1 Loss: 0.0014512856781905138\n",
      "Iteration: 7478 lambda_k: 1 Loss: 0.0014512856782058743\n",
      "Iteration: 7479 lambda_k: 1 Loss: 0.001451285678221181\n",
      "Iteration: 7480 lambda_k: 1 Loss: 0.0014512856782363942\n",
      "Iteration: 7481 lambda_k: 1 Loss: 0.0014512856782515472\n",
      "Iteration: 7482 lambda_k: 1 Loss: 0.0014512856782666207\n",
      "Iteration: 7483 lambda_k: 1 Loss: 0.0014512856782816208\n",
      "Iteration: 7484 lambda_k: 1 Loss: 0.001451285678296528\n",
      "Iteration: 7485 lambda_k: 1 Loss: 0.0014512856783113744\n",
      "Iteration: 7486 lambda_k: 1 Loss: 0.0014512856783261475\n",
      "Iteration: 7487 lambda_k: 1 Loss: 0.0014512856783408488\n",
      "Iteration: 7488 lambda_k: 1 Loss: 0.001451285678355478\n",
      "Iteration: 7489 lambda_k: 1 Loss: 0.001451285678370017\n",
      "Iteration: 7490 lambda_k: 1 Loss: 0.0014512856783844934\n",
      "Iteration: 7491 lambda_k: 1 Loss: 0.0014512856783988996\n",
      "Iteration: 7492 lambda_k: 1 Loss: 0.0014512856784132226\n",
      "Iteration: 7493 lambda_k: 1 Loss: 0.0014512856784274848\n",
      "Iteration: 7494 lambda_k: 1 Loss: 0.0014512856784416738\n",
      "Iteration: 7495 lambda_k: 1 Loss: 0.0014512856784558376\n",
      "Iteration: 7496 lambda_k: 1 Loss: 0.0014512856784698765\n",
      "Iteration: 7497 lambda_k: 1 Loss: 0.0014512856784839032\n",
      "Iteration: 7498 lambda_k: 1 Loss: 0.001451285678497868\n",
      "Iteration: 7499 lambda_k: 1 Loss: 0.0014512856785117618\n",
      "Iteration: 7500 lambda_k: 1 Loss: 0.0014512856785255633\n",
      "Iteration: 7501 lambda_k: 1 Loss: 0.0014512856785393164\n",
      "Iteration: 7502 lambda_k: 1 Loss: 0.0014512856785529964\n",
      "Iteration: 7503 lambda_k: 1 Loss: 0.001451285678566595\n",
      "Iteration: 7504 lambda_k: 1 Loss: 0.0014512856785801438\n",
      "Iteration: 7505 lambda_k: 1 Loss: 0.0014512856785936182\n",
      "Iteration: 7506 lambda_k: 1 Loss: 0.0014512856786070179\n",
      "Iteration: 7507 lambda_k: 1 Loss: 0.0014512856786203716\n",
      "Iteration: 7508 lambda_k: 1 Loss: 0.001451285678633636\n",
      "Iteration: 7509 lambda_k: 1 Loss: 0.0014512856786468515\n",
      "Iteration: 7510 lambda_k: 1 Loss: 0.001451285678659995\n",
      "Iteration: 7511 lambda_k: 1 Loss: 0.0014512856786730636\n",
      "Iteration: 7512 lambda_k: 1 Loss: 0.0014512856786860744\n",
      "Iteration: 7513 lambda_k: 1 Loss: 0.0014512856786990248\n",
      "Iteration: 7514 lambda_k: 1 Loss: 0.001451285678711894\n",
      "Iteration: 7515 lambda_k: 1 Loss: 0.0014512856787247234\n",
      "Iteration: 7516 lambda_k: 1 Loss: 0.0014512856787374793\n",
      "Iteration: 7517 lambda_k: 1 Loss: 0.0014512856787502033\n",
      "Iteration: 7518 lambda_k: 1 Loss: 0.0014512856787628854\n",
      "Iteration: 7519 lambda_k: 1 Loss: 0.0014512856787754608\n",
      "Iteration: 7520 lambda_k: 1 Loss: 0.0014512856787879916\n",
      "Iteration: 7521 lambda_k: 1 Loss: 0.0014512856788004881\n",
      "Iteration: 7522 lambda_k: 1 Loss: 0.0014512856788129178\n",
      "Iteration: 7523 lambda_k: 1 Loss: 0.0014512856788252751\n",
      "Iteration: 7524 lambda_k: 1 Loss: 0.001451285678837588\n",
      "Iteration: 7525 lambda_k: 1 Loss: 0.0014512856788498338\n",
      "Iteration: 7526 lambda_k: 1 Loss: 0.0014512856788620192\n",
      "Iteration: 7527 lambda_k: 1 Loss: 0.0014512856788741258\n",
      "Iteration: 7528 lambda_k: 1 Loss: 0.0014512856788861934\n",
      "Iteration: 7529 lambda_k: 1 Loss: 0.0014512856788981834\n",
      "Iteration: 7530 lambda_k: 1 Loss: 0.0014512856789101274\n",
      "Iteration: 7531 lambda_k: 1 Loss: 0.0014512856789220092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7532 lambda_k: 1 Loss: 0.0014512856789338333\n",
      "Iteration: 7533 lambda_k: 1 Loss: 0.0014512856789455945\n",
      "Iteration: 7534 lambda_k: 1 Loss: 0.0014512856789573036\n",
      "Iteration: 7535 lambda_k: 1 Loss: 0.0014512856789689608\n",
      "Iteration: 7536 lambda_k: 1 Loss: 0.0014512856789805414\n",
      "Iteration: 7537 lambda_k: 1 Loss: 0.0014512856789920783\n",
      "Iteration: 7538 lambda_k: 1 Loss: 0.0014512856790035572\n",
      "Iteration: 7539 lambda_k: 1 Loss: 0.0014512856790149665\n",
      "Iteration: 7540 lambda_k: 1 Loss: 0.0014512856790263344\n",
      "Iteration: 7541 lambda_k: 1 Loss: 0.0014512856790376406\n",
      "Iteration: 7542 lambda_k: 1 Loss: 0.0014512856790488797\n",
      "Iteration: 7543 lambda_k: 1 Loss: 0.001451285679060076\n",
      "Iteration: 7544 lambda_k: 1 Loss: 0.0014512856790712561\n",
      "Iteration: 7545 lambda_k: 1 Loss: 0.0014512856790823462\n",
      "Iteration: 7546 lambda_k: 1 Loss: 0.0014512856790934215\n",
      "Iteration: 7547 lambda_k: 1 Loss: 0.0014512856791044043\n",
      "Iteration: 7548 lambda_k: 1 Loss: 0.001451285679115364\n",
      "Iteration: 7549 lambda_k: 1 Loss: 0.0014512856791262805\n",
      "Iteration: 7550 lambda_k: 1 Loss: 0.00145128567913715\n",
      "Iteration: 7551 lambda_k: 1 Loss: 0.0014512856791479717\n",
      "Iteration: 7552 lambda_k: 1 Loss: 0.0014512856791587198\n",
      "Iteration: 7553 lambda_k: 1 Loss: 0.0014512856791694233\n",
      "Iteration: 7554 lambda_k: 1 Loss: 0.0014512856791800762\n",
      "Iteration: 7555 lambda_k: 1 Loss: 0.0014512856791906643\n",
      "Iteration: 7556 lambda_k: 1 Loss: 0.0014512856792012127\n",
      "Iteration: 7557 lambda_k: 1 Loss: 0.001451285679211713\n",
      "Iteration: 7558 lambda_k: 1 Loss: 0.0014512856792221378\n",
      "Iteration: 7559 lambda_k: 1 Loss: 0.0014512856792325282\n",
      "Iteration: 7560 lambda_k: 1 Loss: 0.0014512856792428675\n",
      "Iteration: 7561 lambda_k: 1 Loss: 0.0014512856792531447\n",
      "Iteration: 7562 lambda_k: 1 Loss: 0.0014512856792633887\n",
      "Iteration: 7563 lambda_k: 1 Loss: 0.0014512856792735747\n",
      "Iteration: 7564 lambda_k: 1 Loss: 0.001451285679283714\n",
      "Iteration: 7565 lambda_k: 1 Loss: 0.0014512856792937901\n",
      "Iteration: 7566 lambda_k: 1 Loss: 0.0014512856793038262\n",
      "Iteration: 7567 lambda_k: 1 Loss: 0.0014512856793138145\n",
      "Iteration: 7568 lambda_k: 1 Loss: 0.0014512856793237408\n",
      "Iteration: 7569 lambda_k: 1 Loss: 0.0014512856793336276\n",
      "Iteration: 7570 lambda_k: 1 Loss: 0.0014512856793434674\n",
      "Iteration: 7571 lambda_k: 1 Loss: 0.001451285679353258\n",
      "Iteration: 7572 lambda_k: 1 Loss: 0.00145128567936299\n",
      "Iteration: 7573 lambda_k: 1 Loss: 0.0014512856793726877\n",
      "Iteration: 7574 lambda_k: 1 Loss: 0.0014512856793823877\n",
      "Iteration: 7575 lambda_k: 1 Loss: 0.0014512856793919852\n",
      "Iteration: 7576 lambda_k: 1 Loss: 0.0014512856794015884\n",
      "Iteration: 7577 lambda_k: 1 Loss: 0.0014512856794110973\n",
      "Iteration: 7578 lambda_k: 1 Loss: 0.0014512856794205876\n",
      "Iteration: 7579 lambda_k: 1 Loss: 0.0014512856794300524\n",
      "Iteration: 7580 lambda_k: 1 Loss: 0.0014512856794394824\n",
      "Iteration: 7581 lambda_k: 1 Loss: 0.001451285679448844\n",
      "Iteration: 7582 lambda_k: 1 Loss: 0.0014512856794581686\n",
      "Iteration: 7583 lambda_k: 1 Loss: 0.001451285679467432\n",
      "Iteration: 7584 lambda_k: 1 Loss: 0.001451285679476668\n",
      "Iteration: 7585 lambda_k: 1 Loss: 0.0014512856794858572\n",
      "Iteration: 7586 lambda_k: 1 Loss: 0.0014512856794949916\n",
      "Iteration: 7587 lambda_k: 1 Loss: 0.0014512856795040948\n",
      "Iteration: 7588 lambda_k: 1 Loss: 0.0014512856795131396\n",
      "Iteration: 7589 lambda_k: 1 Loss: 0.0014512856795221524\n",
      "Iteration: 7590 lambda_k: 1 Loss: 0.0014512856795311213\n",
      "Iteration: 7591 lambda_k: 1 Loss: 0.0014512856795400437\n",
      "Iteration: 7592 lambda_k: 1 Loss: 0.0014512856795489033\n",
      "Iteration: 7593 lambda_k: 1 Loss: 0.0014512856795577441\n",
      "Iteration: 7594 lambda_k: 1 Loss: 0.001451285679566522\n",
      "Iteration: 7595 lambda_k: 1 Loss: 0.0014512856795752787\n",
      "Iteration: 7596 lambda_k: 1 Loss: 0.0014512856795839686\n",
      "Iteration: 7597 lambda_k: 1 Loss: 0.0014512856795926364\n",
      "Iteration: 7598 lambda_k: 1 Loss: 0.001451285679601242\n",
      "Iteration: 7599 lambda_k: 1 Loss: 0.0014512856796098294\n",
      "Iteration: 7600 lambda_k: 1 Loss: 0.001451285679618364\n",
      "Iteration: 7601 lambda_k: 1 Loss: 0.0014512856796268581\n",
      "Iteration: 7602 lambda_k: 1 Loss: 0.0014512856796352976\n",
      "Iteration: 7603 lambda_k: 1 Loss: 0.0014512856796437164\n",
      "Iteration: 7604 lambda_k: 1 Loss: 0.0014512856796520843\n",
      "Iteration: 7605 lambda_k: 1 Loss: 0.0014512856796603981\n",
      "Iteration: 7606 lambda_k: 1 Loss: 0.001451285679668689\n",
      "Iteration: 7607 lambda_k: 1 Loss: 0.001451285679676933\n",
      "Iteration: 7608 lambda_k: 1 Loss: 0.0014512856796851249\n",
      "Iteration: 7609 lambda_k: 1 Loss: 0.0014512856796932993\n",
      "Iteration: 7610 lambda_k: 1 Loss: 0.0014512856797014159\n",
      "Iteration: 7611 lambda_k: 1 Loss: 0.001451285679709508\n",
      "Iteration: 7612 lambda_k: 1 Loss: 0.0014512856797175614\n",
      "Iteration: 7613 lambda_k: 1 Loss: 0.0014512856797255578\n",
      "Iteration: 7614 lambda_k: 1 Loss: 0.0014512856797335287\n",
      "Iteration: 7615 lambda_k: 1 Loss: 0.0014512856797414982\n",
      "Iteration: 7616 lambda_k: 1 Loss: 0.0014512856797494014\n",
      "Iteration: 7617 lambda_k: 1 Loss: 0.001451285679757304\n",
      "Iteration: 7618 lambda_k: 1 Loss: 0.0014512856797651167\n",
      "Iteration: 7619 lambda_k: 1 Loss: 0.0014512856797729396\n",
      "Iteration: 7620 lambda_k: 1 Loss: 0.0014512856797807374\n",
      "Iteration: 7621 lambda_k: 1 Loss: 0.0014512856797884916\n",
      "Iteration: 7622 lambda_k: 1 Loss: 0.0014512856797961906\n",
      "Iteration: 7623 lambda_k: 1 Loss: 0.001451285679803873\n",
      "Iteration: 7624 lambda_k: 1 Loss: 0.0014512856798115231\n",
      "Iteration: 7625 lambda_k: 1 Loss: 0.0014512856798191138\n",
      "Iteration: 7626 lambda_k: 1 Loss: 0.001451285679826687\n",
      "Iteration: 7627 lambda_k: 1 Loss: 0.001451285679834215\n",
      "Iteration: 7628 lambda_k: 1 Loss: 0.0014512856798416928\n",
      "Iteration: 7629 lambda_k: 1 Loss: 0.00145128567984916\n",
      "Iteration: 7630 lambda_k: 1 Loss: 0.0014512856798565828\n",
      "Iteration: 7631 lambda_k: 1 Loss: 0.0014512856798639525\n",
      "Iteration: 7632 lambda_k: 1 Loss: 0.0014512856798713117\n",
      "Iteration: 7633 lambda_k: 1 Loss: 0.0014512856798786099\n",
      "Iteration: 7634 lambda_k: 1 Loss: 0.0014512856798858942\n",
      "Iteration: 7635 lambda_k: 1 Loss: 0.0014512856798931262\n",
      "Iteration: 7636 lambda_k: 1 Loss: 0.0014512856799003407\n",
      "Iteration: 7637 lambda_k: 1 Loss: 0.001451285679907515\n",
      "Iteration: 7638 lambda_k: 1 Loss: 0.001451285679914639\n",
      "Iteration: 7639 lambda_k: 1 Loss: 0.001451285679921745\n",
      "Iteration: 7640 lambda_k: 1 Loss: 0.0014512856799288037\n",
      "Iteration: 7641 lambda_k: 1 Loss: 0.0014512856799358416\n",
      "Iteration: 7642 lambda_k: 1 Loss: 0.001451285679942845\n",
      "Iteration: 7643 lambda_k: 1 Loss: 0.001451285679949804\n",
      "Iteration: 7644 lambda_k: 1 Loss: 0.0014512856799567448\n",
      "Iteration: 7645 lambda_k: 1 Loss: 0.0014512856799636428\n",
      "Iteration: 7646 lambda_k: 1 Loss: 0.001451285679970493\n",
      "Iteration: 7647 lambda_k: 1 Loss: 0.001451285679977336\n",
      "Iteration: 7648 lambda_k: 1 Loss: 0.0014512856799841218\n",
      "Iteration: 7649 lambda_k: 1 Loss: 0.0014512856799908918\n",
      "Iteration: 7650 lambda_k: 1 Loss: 0.0014512856799976338\n",
      "Iteration: 7651 lambda_k: 1 Loss: 0.0014512856800043205\n",
      "Iteration: 7652 lambda_k: 1 Loss: 0.0014512856800109949\n",
      "Iteration: 7653 lambda_k: 1 Loss: 0.001451285680017632\n",
      "Iteration: 7654 lambda_k: 1 Loss: 0.0014512856800242239\n",
      "Iteration: 7655 lambda_k: 1 Loss: 0.0014512856800308037\n",
      "Iteration: 7656 lambda_k: 1 Loss: 0.0014512856800373336\n",
      "Iteration: 7657 lambda_k: 1 Loss: 0.0014512856800438464\n",
      "Iteration: 7658 lambda_k: 1 Loss: 0.0014512856800503132\n",
      "Iteration: 7659 lambda_k: 1 Loss: 0.0014512856800567638\n",
      "Iteration: 7660 lambda_k: 1 Loss: 0.001451285680063181\n",
      "Iteration: 7661 lambda_k: 1 Loss: 0.0014512856800696008\n",
      "Iteration: 7662 lambda_k: 1 Loss: 0.0014512856800759655\n",
      "Iteration: 7663 lambda_k: 1 Loss: 0.0014512856800822913\n",
      "Iteration: 7664 lambda_k: 1 Loss: 0.0014512856800885684\n",
      "Iteration: 7665 lambda_k: 1 Loss: 0.0014512856800948332\n",
      "Iteration: 7666 lambda_k: 1 Loss: 0.0014512856801011003\n",
      "Iteration: 7667 lambda_k: 1 Loss: 0.0014512856801073175\n",
      "Iteration: 7668 lambda_k: 1 Loss: 0.0014512856801135233\n",
      "Iteration: 7669 lambda_k: 1 Loss: 0.0014512856801197167\n",
      "Iteration: 7670 lambda_k: 1 Loss: 0.0014512856801258342\n",
      "Iteration: 7671 lambda_k: 1 Loss: 0.0014512856801319474\n",
      "Iteration: 7672 lambda_k: 1 Loss: 0.0014512856801380484\n",
      "Iteration: 7673 lambda_k: 1 Loss: 0.001451285680144132\n",
      "Iteration: 7674 lambda_k: 1 Loss: 0.0014512856801501192\n",
      "Iteration: 7675 lambda_k: 1 Loss: 0.0014512856801561335\n",
      "Iteration: 7676 lambda_k: 1 Loss: 0.0014512856801621034\n",
      "Iteration: 7677 lambda_k: 1 Loss: 0.0014512856801680608\n",
      "Iteration: 7678 lambda_k: 1 Loss: 0.0014512856801739817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7679 lambda_k: 1 Loss: 0.0014512856801798834\n",
      "Iteration: 7680 lambda_k: 1 Loss: 0.001451285680185764\n",
      "Iteration: 7681 lambda_k: 1 Loss: 0.0014512856801915947\n",
      "Iteration: 7682 lambda_k: 1 Loss: 0.0014512856801974154\n",
      "Iteration: 7683 lambda_k: 1 Loss: 0.0014512856802031925\n",
      "Iteration: 7684 lambda_k: 1 Loss: 0.0014512856802089526\n",
      "Iteration: 7685 lambda_k: 1 Loss: 0.0014512856802146859\n",
      "Iteration: 7686 lambda_k: 1 Loss: 0.0014512856802203766\n",
      "Iteration: 7687 lambda_k: 1 Loss: 0.001451285680226051\n",
      "Iteration: 7688 lambda_k: 1 Loss: 0.0014512856802317059\n",
      "Iteration: 7689 lambda_k: 1 Loss: 0.0014512856802373357\n",
      "Iteration: 7690 lambda_k: 1 Loss: 0.0014512856802429293\n",
      "Iteration: 7691 lambda_k: 1 Loss: 0.0014512856802484924\n",
      "Iteration: 7692 lambda_k: 1 Loss: 0.0014512856802540196\n",
      "Iteration: 7693 lambda_k: 1 Loss: 0.0014512856802595439\n",
      "Iteration: 7694 lambda_k: 1 Loss: 0.0014512856802650202\n",
      "Iteration: 7695 lambda_k: 1 Loss: 0.001451285680270483\n",
      "Iteration: 7696 lambda_k: 1 Loss: 0.00145128568027591\n",
      "Iteration: 7697 lambda_k: 1 Loss: 0.001451285680281324\n",
      "Iteration: 7698 lambda_k: 1 Loss: 0.0014512856802867114\n",
      "Iteration: 7699 lambda_k: 1 Loss: 0.0014512856802920604\n",
      "Iteration: 7700 lambda_k: 1 Loss: 0.001451285680297399\n",
      "Iteration: 7701 lambda_k: 1 Loss: 0.0014512856803027125\n",
      "Iteration: 7702 lambda_k: 1 Loss: 0.0014512856803079828\n",
      "Iteration: 7703 lambda_k: 1 Loss: 0.001451285680313239\n",
      "Iteration: 7704 lambda_k: 1 Loss: 0.001451285680318469\n",
      "Iteration: 7705 lambda_k: 1 Loss: 0.0014512856803236595\n",
      "Iteration: 7706 lambda_k: 1 Loss: 0.0014512856803288487\n",
      "Iteration: 7707 lambda_k: 1 Loss: 0.001451285680334016\n",
      "Iteration: 7708 lambda_k: 1 Loss: 0.0014512856803391321\n",
      "Iteration: 7709 lambda_k: 1 Loss: 0.0014512856803442454\n",
      "Iteration: 7710 lambda_k: 1 Loss: 0.0014512856803493184\n",
      "Iteration: 7711 lambda_k: 1 Loss: 0.001451285680354375\n",
      "Iteration: 7712 lambda_k: 1 Loss: 0.001451285680359416\n",
      "Iteration: 7713 lambda_k: 1 Loss: 0.0014512856803644348\n",
      "Iteration: 7714 lambda_k: 1 Loss: 0.0014512856803694108\n",
      "Iteration: 7715 lambda_k: 1 Loss: 0.001451285680374384\n",
      "Iteration: 7716 lambda_k: 1 Loss: 0.0014512856803793207\n",
      "Iteration: 7717 lambda_k: 1 Loss: 0.0014512856803842384\n",
      "Iteration: 7718 lambda_k: 1 Loss: 0.0014512856803891208\n",
      "Iteration: 7719 lambda_k: 1 Loss: 0.001451285680393997\n",
      "Iteration: 7720 lambda_k: 1 Loss: 0.0014512856803988352\n",
      "Iteration: 7721 lambda_k: 1 Loss: 0.00145128568040367\n",
      "Iteration: 7722 lambda_k: 1 Loss: 0.0014512856804084582\n",
      "Iteration: 7723 lambda_k: 1 Loss: 0.0014512856804132397\n",
      "Iteration: 7724 lambda_k: 1 Loss: 0.0014512856804179887\n",
      "Iteration: 7725 lambda_k: 1 Loss: 0.001451285680422719\n",
      "Iteration: 7726 lambda_k: 1 Loss: 0.0014512856804274287\n",
      "Iteration: 7727 lambda_k: 1 Loss: 0.001451285680432127\n",
      "Iteration: 7728 lambda_k: 1 Loss: 0.0014512856804367782\n",
      "Iteration: 7729 lambda_k: 1 Loss: 0.0014512856804414233\n",
      "Iteration: 7730 lambda_k: 1 Loss: 0.0014512856804460327\n",
      "Iteration: 7731 lambda_k: 1 Loss: 0.0014512856804506334\n",
      "Iteration: 7732 lambda_k: 1 Loss: 0.0014512856804552129\n",
      "Iteration: 7733 lambda_k: 1 Loss: 0.0014512856804597583\n",
      "Iteration: 7734 lambda_k: 1 Loss: 0.0014512856804642967\n",
      "Iteration: 7735 lambda_k: 1 Loss: 0.0014512856804688111\n",
      "Iteration: 7736 lambda_k: 1 Loss: 0.0014512856804732883\n",
      "Iteration: 7737 lambda_k: 1 Loss: 0.001451285680477769\n",
      "Iteration: 7738 lambda_k: 1 Loss: 0.0014512856804822162\n",
      "Iteration: 7739 lambda_k: 1 Loss: 0.0014512856804866255\n",
      "Iteration: 7740 lambda_k: 1 Loss: 0.0014512856804910373\n",
      "Iteration: 7741 lambda_k: 1 Loss: 0.0014512856804954264\n",
      "Iteration: 7742 lambda_k: 1 Loss: 0.0014512856804997753\n",
      "Iteration: 7743 lambda_k: 1 Loss: 0.0014512856805041303\n",
      "Iteration: 7744 lambda_k: 1 Loss: 0.001451285680508442\n",
      "Iteration: 7745 lambda_k: 1 Loss: 0.0014512856805127914\n",
      "Iteration: 7746 lambda_k: 1 Loss: 0.0014512856805170668\n",
      "Iteration: 7747 lambda_k: 1 Loss: 0.0014512856805213772\n",
      "Iteration: 7748 lambda_k: 1 Loss: 0.0014512856805256262\n",
      "Iteration: 7749 lambda_k: 1 Loss: 0.0014512856805298838\n",
      "Iteration: 7750 lambda_k: 1 Loss: 0.0014512856805340962\n",
      "Iteration: 7751 lambda_k: 1 Loss: 0.001451285680538267\n",
      "Iteration: 7752 lambda_k: 1 Loss: 0.001451285680542477\n",
      "Iteration: 7753 lambda_k: 1 Loss: 0.0014512856805466265\n",
      "Iteration: 7754 lambda_k: 1 Loss: 0.0014512856805507803\n",
      "Iteration: 7755 lambda_k: 1 Loss: 0.001451285680554892\n",
      "Iteration: 7756 lambda_k: 1 Loss: 0.001451285680559023\n",
      "Iteration: 7757 lambda_k: 1 Loss: 0.0014512856805631274\n",
      "Iteration: 7758 lambda_k: 1 Loss: 0.0014512856805672294\n",
      "Iteration: 7759 lambda_k: 1 Loss: 0.0014512856805712602\n",
      "Iteration: 7760 lambda_k: 1 Loss: 0.0014512856805752948\n",
      "Iteration: 7761 lambda_k: 1 Loss: 0.0014512856805793323\n",
      "Iteration: 7762 lambda_k: 1 Loss: 0.0014512856805833534\n",
      "Iteration: 7763 lambda_k: 1 Loss: 0.0014512856805873407\n",
      "Iteration: 7764 lambda_k: 1 Loss: 0.001451285680591322\n",
      "Iteration: 7765 lambda_k: 1 Loss: 0.001451285680595272\n",
      "Iteration: 7766 lambda_k: 1 Loss: 0.00145128568059917\n",
      "Iteration: 7767 lambda_k: 1 Loss: 0.0014512856806030816\n",
      "Iteration: 7768 lambda_k: 1 Loss: 0.0014512856806069804\n",
      "Iteration: 7769 lambda_k: 1 Loss: 0.0014512856806108605\n",
      "Iteration: 7770 lambda_k: 1 Loss: 0.0014512856806147307\n",
      "Iteration: 7771 lambda_k: 1 Loss: 0.0014512856806185833\n",
      "Iteration: 7772 lambda_k: 1 Loss: 0.0014512856806224023\n",
      "Iteration: 7773 lambda_k: 1 Loss: 0.0014512856806262176\n",
      "Iteration: 7774 lambda_k: 1 Loss: 0.0014512856806300186\n",
      "Iteration: 7775 lambda_k: 1 Loss: 0.0014512856806337745\n",
      "Iteration: 7776 lambda_k: 1 Loss: 0.0014512856806375373\n",
      "Iteration: 7777 lambda_k: 1 Loss: 0.0014512856806412698\n",
      "Iteration: 7778 lambda_k: 1 Loss: 0.0014512856806449947\n",
      "Iteration: 7779 lambda_k: 1 Loss: 0.0014512856806487044\n",
      "Iteration: 7780 lambda_k: 1 Loss: 0.0014512856806523807\n",
      "Iteration: 7781 lambda_k: 1 Loss: 0.001451285680656049\n",
      "Iteration: 7782 lambda_k: 1 Loss: 0.0014512856806597012\n",
      "Iteration: 7783 lambda_k: 1 Loss: 0.0014512856806633394\n",
      "Iteration: 7784 lambda_k: 1 Loss: 0.0014512856806669573\n",
      "Iteration: 7785 lambda_k: 1 Loss: 0.001451285680670565\n",
      "Iteration: 7786 lambda_k: 1 Loss: 0.0014512856806741371\n",
      "Iteration: 7787 lambda_k: 1 Loss: 0.0014512856806777098\n",
      "Iteration: 7788 lambda_k: 1 Loss: 0.0014512856806812616\n",
      "Iteration: 7789 lambda_k: 1 Loss: 0.0014512856806847838\n",
      "Iteration: 7790 lambda_k: 1 Loss: 0.0014512856806883042\n",
      "Iteration: 7791 lambda_k: 1 Loss: 0.0014512856806917975\n",
      "Iteration: 7792 lambda_k: 1 Loss: 0.0014512856806952865\n",
      "Iteration: 7793 lambda_k: 1 Loss: 0.0014512856806987568\n",
      "Iteration: 7794 lambda_k: 1 Loss: 0.0014512856807021993\n",
      "Iteration: 7795 lambda_k: 1 Loss: 0.0014512856807056386\n",
      "Iteration: 7796 lambda_k: 1 Loss: 0.0014512856807090597\n",
      "Iteration: 7797 lambda_k: 1 Loss: 0.0014512856807124674\n",
      "Iteration: 7798 lambda_k: 1 Loss: 0.0014512856807158418\n",
      "Iteration: 7799 lambda_k: 1 Loss: 0.0014512856807192178\n",
      "Iteration: 7800 lambda_k: 1 Loss: 0.001451285680722566\n",
      "Iteration: 7801 lambda_k: 1 Loss: 0.0014512856807259032\n",
      "Iteration: 7802 lambda_k: 1 Loss: 0.0014512856807292335\n",
      "Iteration: 7803 lambda_k: 1 Loss: 0.0014512856807325294\n",
      "Iteration: 7804 lambda_k: 1 Loss: 0.0014512856807358237\n",
      "Iteration: 7805 lambda_k: 1 Loss: 0.0014512856807391032\n",
      "Iteration: 7806 lambda_k: 1 Loss: 0.0014512856807423567\n",
      "Iteration: 7807 lambda_k: 1 Loss: 0.0014512856807456138\n",
      "Iteration: 7808 lambda_k: 1 Loss: 0.0014512856807488356\n",
      "Iteration: 7809 lambda_k: 1 Loss: 0.0014512856807520605\n",
      "Iteration: 7810 lambda_k: 1 Loss: 0.0014512856807552686\n",
      "Iteration: 7811 lambda_k: 1 Loss: 0.0014512856807584386\n",
      "Iteration: 7812 lambda_k: 1 Loss: 0.0014512856807616158\n",
      "Iteration: 7813 lambda_k: 1 Loss: 0.0014512856807647777\n",
      "Iteration: 7814 lambda_k: 1 Loss: 0.0014512856807679067\n",
      "Iteration: 7815 lambda_k: 1 Loss: 0.001451285680771043\n",
      "Iteration: 7816 lambda_k: 1 Loss: 0.0014512856807741637\n",
      "Iteration: 7817 lambda_k: 1 Loss: 0.0014512856807772458\n",
      "Iteration: 7818 lambda_k: 1 Loss: 0.0014512856807803317\n",
      "Iteration: 7819 lambda_k: 1 Loss: 0.0014512856807834089\n",
      "Iteration: 7820 lambda_k: 1 Loss: 0.0014512856807864518\n",
      "Iteration: 7821 lambda_k: 1 Loss: 0.0014512856807894997\n",
      "Iteration: 7822 lambda_k: 1 Loss: 0.0014512856807925268\n",
      "Iteration: 7823 lambda_k: 1 Loss: 0.0014512856807955233\n",
      "Iteration: 7824 lambda_k: 1 Loss: 0.0014512856807985268\n",
      "Iteration: 7825 lambda_k: 1 Loss: 0.0014512856808015166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7826 lambda_k: 1 Loss: 0.0014512856808044884\n",
      "Iteration: 7827 lambda_k: 1 Loss: 0.001451285680807433\n",
      "Iteration: 7828 lambda_k: 1 Loss: 0.001451285680810381\n",
      "Iteration: 7829 lambda_k: 1 Loss: 0.001451285680813312\n",
      "Iteration: 7830 lambda_k: 1 Loss: 0.001451285680816228\n",
      "Iteration: 7831 lambda_k: 1 Loss: 0.0014512856808191164\n",
      "Iteration: 7832 lambda_k: 1 Loss: 0.0014512856808220067\n",
      "Iteration: 7833 lambda_k: 1 Loss: 0.0014512856808248837\n",
      "Iteration: 7834 lambda_k: 1 Loss: 0.001451285680827734\n",
      "Iteration: 7835 lambda_k: 1 Loss: 0.0014512856808305832\n",
      "Iteration: 7836 lambda_k: 1 Loss: 0.0014512856808334238\n",
      "Iteration: 7837 lambda_k: 1 Loss: 0.0014512856808362327\n",
      "Iteration: 7838 lambda_k: 1 Loss: 0.0014512856808390402\n",
      "Iteration: 7839 lambda_k: 1 Loss: 0.0014512856808418352\n",
      "Iteration: 7840 lambda_k: 1 Loss: 0.0014512856808446045\n",
      "Iteration: 7841 lambda_k: 1 Loss: 0.0014512856808473772\n",
      "Iteration: 7842 lambda_k: 1 Loss: 0.0014512856808501404\n",
      "Iteration: 7843 lambda_k: 1 Loss: 0.001451285680852875\n",
      "Iteration: 7844 lambda_k: 1 Loss: 0.0014512856808556085\n",
      "Iteration: 7845 lambda_k: 1 Loss: 0.001451285680858328\n",
      "Iteration: 7846 lambda_k: 1 Loss: 0.0014512856808610202\n",
      "Iteration: 7847 lambda_k: 1 Loss: 0.0014512856808637188\n",
      "Iteration: 7848 lambda_k: 1 Loss: 0.0014512856808664028\n",
      "Iteration: 7849 lambda_k: 1 Loss: 0.0014512856808690732\n",
      "Iteration: 7850 lambda_k: 1 Loss: 0.0014512856808717126\n",
      "Iteration: 7851 lambda_k: 1 Loss: 0.0014512856808743609\n",
      "Iteration: 7852 lambda_k: 1 Loss: 0.0014512856808769933\n",
      "Iteration: 7853 lambda_k: 1 Loss: 0.0014512856808796119\n",
      "Iteration: 7854 lambda_k: 1 Loss: 0.0014512856808822096\n",
      "Iteration: 7855 lambda_k: 1 Loss: 0.0014512856808848082\n",
      "Iteration: 7856 lambda_k: 1 Loss: 0.0014512856808873904\n",
      "Iteration: 7857 lambda_k: 1 Loss: 0.0014512856808899484\n",
      "Iteration: 7858 lambda_k: 1 Loss: 0.0014512856808925126\n",
      "Iteration: 7859 lambda_k: 1 Loss: 0.0014512856808950633\n",
      "Iteration: 7860 lambda_k: 1 Loss: 0.001451285680897587\n",
      "Iteration: 7861 lambda_k: 1 Loss: 0.0014512856809001115\n",
      "Iteration: 7862 lambda_k: 1 Loss: 0.0014512856809026223\n",
      "Iteration: 7863 lambda_k: 1 Loss: 0.0014512856809051104\n",
      "Iteration: 7864 lambda_k: 1 Loss: 0.0014512856809076058\n",
      "Iteration: 7865 lambda_k: 1 Loss: 0.0014512856809100823\n",
      "Iteration: 7866 lambda_k: 1 Loss: 0.001451285680912537\n",
      "Iteration: 7867 lambda_k: 1 Loss: 0.0014512856809149929\n",
      "Iteration: 7868 lambda_k: 1 Loss: 0.001451285680917433\n",
      "Iteration: 7869 lambda_k: 1 Loss: 0.0014512856809198505\n",
      "Iteration: 7870 lambda_k: 1 Loss: 0.0014512856809222637\n",
      "Iteration: 7871 lambda_k: 1 Loss: 0.0014512856809246757\n",
      "Iteration: 7872 lambda_k: 1 Loss: 0.0014512856809270759\n",
      "Iteration: 7873 lambda_k: 1 Loss: 0.001451285680929462\n",
      "Iteration: 7874 lambda_k: 1 Loss: 0.0014512856809318262\n",
      "Iteration: 7875 lambda_k: 1 Loss: 0.0014512856809341887\n",
      "Iteration: 7876 lambda_k: 1 Loss: 0.0014512856809365305\n",
      "Iteration: 7877 lambda_k: 1 Loss: 0.0014512856809388707\n",
      "Iteration: 7878 lambda_k: 1 Loss: 0.0014512856809412004\n",
      "Iteration: 7879 lambda_k: 1 Loss: 0.001451285680943523\n",
      "Iteration: 7880 lambda_k: 1 Loss: 0.0014512856809458336\n",
      "Iteration: 7881 lambda_k: 1 Loss: 0.0014512856809481196\n",
      "Iteration: 7882 lambda_k: 1 Loss: 0.001451285680950411\n",
      "Iteration: 7883 lambda_k: 1 Loss: 0.0014512856809526802\n",
      "Iteration: 7884 lambda_k: 1 Loss: 0.0014512856809549529\n",
      "Iteration: 7885 lambda_k: 1 Loss: 0.0014512856809571987\n",
      "Iteration: 7886 lambda_k: 1 Loss: 0.0014512856809594475\n",
      "Iteration: 7887 lambda_k: 1 Loss: 0.0014512856809616873\n",
      "Iteration: 7888 lambda_k: 1 Loss: 0.0014512856809638956\n",
      "Iteration: 7889 lambda_k: 1 Loss: 0.0014512856809661202\n",
      "Iteration: 7890 lambda_k: 1 Loss: 0.001451285680968325\n",
      "Iteration: 7891 lambda_k: 1 Loss: 0.001451285680970508\n",
      "Iteration: 7892 lambda_k: 1 Loss: 0.0014512856809726954\n",
      "Iteration: 7893 lambda_k: 1 Loss: 0.0014512856809748614\n",
      "Iteration: 7894 lambda_k: 1 Loss: 0.0014512856809770257\n",
      "Iteration: 7895 lambda_k: 1 Loss: 0.0014512856809792266\n",
      "Iteration: 7896 lambda_k: 1 Loss: 0.0014512856809813632\n",
      "Iteration: 7897 lambda_k: 1 Loss: 0.0014512856809835027\n",
      "Iteration: 7898 lambda_k: 1 Loss: 0.0014512856809856278\n",
      "Iteration: 7899 lambda_k: 1 Loss: 0.0014512856809877287\n",
      "Iteration: 7900 lambda_k: 1 Loss: 0.0014512856809898798\n",
      "Iteration: 7901 lambda_k: 1 Loss: 0.001451285680991979\n",
      "Iteration: 7902 lambda_k: 1 Loss: 0.0014512856809941084\n",
      "Iteration: 7903 lambda_k: 1 Loss: 0.001451285680996193\n",
      "Iteration: 7904 lambda_k: 1 Loss: 0.0014512856809982427\n",
      "Iteration: 7905 lambda_k: 1 Loss: 0.001451285681000304\n",
      "Iteration: 7906 lambda_k: 1 Loss: 0.0014512856810023796\n",
      "Iteration: 7907 lambda_k: 1 Loss: 0.0014512856810044313\n",
      "Iteration: 7908 lambda_k: 1 Loss: 0.001451285681006487\n",
      "Iteration: 7909 lambda_k: 1 Loss: 0.0014512856810085172\n",
      "Iteration: 7910 lambda_k: 1 Loss: 0.0014512856810105564\n",
      "Iteration: 7911 lambda_k: 1 Loss: 0.0014512856810125615\n",
      "Iteration: 7912 lambda_k: 1 Loss: 0.0014512856810145406\n",
      "Iteration: 7913 lambda_k: 1 Loss: 0.001451285681016566\n",
      "Iteration: 7914 lambda_k: 1 Loss: 0.0014512856810185814\n",
      "Iteration: 7915 lambda_k: 1 Loss: 0.0014512856810205961\n",
      "Iteration: 7916 lambda_k: 1 Loss: 0.0014512856810225629\n",
      "Iteration: 7917 lambda_k: 1 Loss: 0.0014512856810244877\n",
      "Iteration: 7918 lambda_k: 1 Loss: 0.0014512856810264653\n",
      "Iteration: 7919 lambda_k: 1 Loss: 0.0014512856810283991\n",
      "Iteration: 7920 lambda_k: 1 Loss: 0.0014512856810303624\n",
      "Iteration: 7921 lambda_k: 1 Loss: 0.0014512856810323072\n",
      "Iteration: 7922 lambda_k: 1 Loss: 0.0014512856810342115\n",
      "Iteration: 7923 lambda_k: 1 Loss: 0.001451285681036135\n",
      "Iteration: 7924 lambda_k: 1 Loss: 0.0014512856810380212\n",
      "Iteration: 7925 lambda_k: 1 Loss: 0.0014512856810399274\n",
      "Iteration: 7926 lambda_k: 1 Loss: 0.0014512856810418044\n",
      "Iteration: 7927 lambda_k: 1 Loss: 0.0014512856810436914\n",
      "Iteration: 7928 lambda_k: 1 Loss: 0.0014512856810455991\n",
      "Iteration: 7929 lambda_k: 1 Loss: 0.0014512856810474264\n",
      "Iteration: 7930 lambda_k: 1 Loss: 0.0014512856810493056\n",
      "Iteration: 7931 lambda_k: 1 Loss: 0.001451285681051116\n",
      "Iteration: 7932 lambda_k: 1 Loss: 0.0014512856810529767\n",
      "Iteration: 7933 lambda_k: 1 Loss: 0.0014512856810548302\n",
      "Iteration: 7934 lambda_k: 1 Loss: 0.0014512856810566593\n",
      "Iteration: 7935 lambda_k: 1 Loss: 0.0014512856810584968\n",
      "Iteration: 7936 lambda_k: 1 Loss: 0.0014512856810603265\n",
      "Iteration: 7937 lambda_k: 1 Loss: 0.0014512856810621289\n",
      "Iteration: 7938 lambda_k: 1 Loss: 0.0014512856810639416\n",
      "Iteration: 7939 lambda_k: 1 Loss: 0.0014512856810657481\n",
      "Iteration: 7940 lambda_k: 1 Loss: 0.0014512856810675258\n",
      "Iteration: 7941 lambda_k: 1 Loss: 0.0014512856810693143\n",
      "Iteration: 7942 lambda_k: 1 Loss: 0.0014512856810710835\n",
      "Iteration: 7943 lambda_k: 1 Loss: 0.0014512856810728644\n",
      "Iteration: 7944 lambda_k: 1 Loss: 0.0014512856810746132\n",
      "Iteration: 7945 lambda_k: 1 Loss: 0.0014512856810763696\n",
      "Iteration: 7946 lambda_k: 1 Loss: 0.0014512856810781193\n",
      "Iteration: 7947 lambda_k: 1 Loss: 0.0014512856810798339\n",
      "Iteration: 7948 lambda_k: 1 Loss: 0.0014512856810815207\n",
      "Iteration: 7949 lambda_k: 1 Loss: 0.0014512856810831886\n",
      "Iteration: 7950 lambda_k: 1 Loss: 0.0014512856810848988\n",
      "Iteration: 7951 lambda_k: 1 Loss: 0.0014512856810865908\n",
      "Iteration: 7952 lambda_k: 1 Loss: 0.0014512856810882952\n",
      "Iteration: 7953 lambda_k: 1 Loss: 0.0014512856810899725\n",
      "Iteration: 7954 lambda_k: 1 Loss: 0.0014512856810916562\n",
      "Iteration: 7955 lambda_k: 1 Loss: 0.0014512856810932901\n",
      "Iteration: 7956 lambda_k: 1 Loss: 0.001451285681094936\n",
      "Iteration: 7957 lambda_k: 1 Loss: 0.0014512856810965972\n",
      "Iteration: 7958 lambda_k: 1 Loss: 0.0014512856810982363\n",
      "Iteration: 7959 lambda_k: 1 Loss: 0.001451285681099883\n",
      "Iteration: 7960 lambda_k: 1 Loss: 0.0014512856811015262\n",
      "Iteration: 7961 lambda_k: 1 Loss: 0.0014512856811031579\n",
      "Iteration: 7962 lambda_k: 1 Loss: 0.0014512856811047215\n",
      "Iteration: 7963 lambda_k: 1 Loss: 0.0014512856811063327\n",
      "Iteration: 7964 lambda_k: 1 Loss: 0.0014512856811079412\n",
      "Iteration: 7965 lambda_k: 1 Loss: 0.0014512856811095445\n",
      "Iteration: 7966 lambda_k: 1 Loss: 0.0014512856811111235\n",
      "Iteration: 7967 lambda_k: 1 Loss: 0.0014512856811126661\n",
      "Iteration: 7968 lambda_k: 1 Loss: 0.0014512856811142378\n",
      "Iteration: 7969 lambda_k: 1 Loss: 0.0014512856811157988\n",
      "Iteration: 7970 lambda_k: 1 Loss: 0.001451285681117367\n",
      "Iteration: 7971 lambda_k: 1 Loss: 0.001451285681118928\n",
      "Iteration: 7972 lambda_k: 1 Loss: 0.0014512856811204238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7973 lambda_k: 1 Loss: 0.0014512856811219606\n",
      "Iteration: 7974 lambda_k: 1 Loss: 0.001451285681123489\n",
      "Iteration: 7975 lambda_k: 1 Loss: 0.0014512856811250275\n",
      "Iteration: 7976 lambda_k: 1 Loss: 0.00145128568112654\n",
      "Iteration: 7977 lambda_k: 1 Loss: 0.001451285681128012\n",
      "Iteration: 7978 lambda_k: 1 Loss: 0.0014512856811295144\n",
      "Iteration: 7979 lambda_k: 1 Loss: 0.001451285681131001\n",
      "Iteration: 7980 lambda_k: 1 Loss: 0.001451285681132507\n",
      "Iteration: 7981 lambda_k: 1 Loss: 0.0014512856811339835\n",
      "Iteration: 7982 lambda_k: 1 Loss: 0.0014512856811354194\n",
      "Iteration: 7983 lambda_k: 1 Loss: 0.0014512856811368879\n",
      "Iteration: 7984 lambda_k: 1 Loss: 0.0014512856811383513\n",
      "Iteration: 7985 lambda_k: 1 Loss: 0.0014512856811398005\n",
      "Iteration: 7986 lambda_k: 1 Loss: 0.0014512856811412609\n",
      "Iteration: 7987 lambda_k: 1 Loss: 0.001451285681142665\n",
      "Iteration: 7988 lambda_k: 1 Loss: 0.00145128568114409\n",
      "Iteration: 7989 lambda_k: 1 Loss: 0.001451285681145527\n",
      "Iteration: 7990 lambda_k: 1 Loss: 0.0014512856811469508\n",
      "Iteration: 7991 lambda_k: 1 Loss: 0.0014512856811483735\n",
      "Iteration: 7992 lambda_k: 1 Loss: 0.001451285681149787\n",
      "Iteration: 7993 lambda_k: 1 Loss: 0.001451285681151153\n",
      "Iteration: 7994 lambda_k: 1 Loss: 0.0014512856811525455\n",
      "Iteration: 7995 lambda_k: 1 Loss: 0.0014512856811539445\n",
      "Iteration: 7996 lambda_k: 1 Loss: 0.0014512856811553245\n",
      "Iteration: 7997 lambda_k: 1 Loss: 0.0014512856811566641\n",
      "Iteration: 7998 lambda_k: 1 Loss: 0.0014512856811580406\n",
      "Iteration: 7999 lambda_k: 1 Loss: 0.0014512856811594156\n",
      "Iteration: 8000 lambda_k: 1 Loss: 0.001451285681160771\n",
      "Iteration: 8001 lambda_k: 1 Loss: 0.0014512856811621328\n",
      "Iteration: 8002 lambda_k: 1 Loss: 0.0014512856811634428\n",
      "Iteration: 8003 lambda_k: 1 Loss: 0.0014512856811647829\n",
      "Iteration: 8004 lambda_k: 1 Loss: 0.001451285681166125\n",
      "Iteration: 8005 lambda_k: 1 Loss: 0.0014512856811674437\n",
      "Iteration: 8006 lambda_k: 1 Loss: 0.0014512856811687788\n",
      "Iteration: 8007 lambda_k: 1 Loss: 0.0014512856811700627\n",
      "Iteration: 8008 lambda_k: 1 Loss: 0.0014512856811713648\n",
      "Iteration: 8009 lambda_k: 1 Loss: 0.0014512856811726778\n",
      "Iteration: 8010 lambda_k: 1 Loss: 0.0014512856811739934\n",
      "Iteration: 8011 lambda_k: 1 Loss: 0.0014512856811752797\n",
      "Iteration: 8012 lambda_k: 1 Loss: 0.0014512856811765347\n",
      "Iteration: 8013 lambda_k: 1 Loss: 0.0014512856811778182\n",
      "Iteration: 8014 lambda_k: 1 Loss: 0.0014512856811791023\n",
      "Iteration: 8015 lambda_k: 1 Loss: 0.0014512856811803644\n",
      "Iteration: 8016 lambda_k: 1 Loss: 0.0014512856811816416\n",
      "Iteration: 8017 lambda_k: 1 Loss: 0.0014512856811829075\n",
      "Iteration: 8018 lambda_k: 1 Loss: 0.0014512856811841127\n",
      "Iteration: 8019 lambda_k: 1 Loss: 0.0014512856811853584\n",
      "Iteration: 8020 lambda_k: 1 Loss: 0.001451285681186597\n",
      "Iteration: 8021 lambda_k: 1 Loss: 0.0014512856811878469\n",
      "Iteration: 8022 lambda_k: 1 Loss: 0.0014512856811890807\n",
      "Iteration: 8023 lambda_k: 1 Loss: 0.001451285681190327\n",
      "Iteration: 8024 lambda_k: 1 Loss: 0.0014512856811914948\n",
      "Iteration: 8025 lambda_k: 1 Loss: 0.0014512856811927071\n",
      "Iteration: 8026 lambda_k: 1 Loss: 0.001451285681193916\n",
      "Iteration: 8027 lambda_k: 1 Loss: 0.0014512856811951318\n",
      "Iteration: 8028 lambda_k: 1 Loss: 0.0014512856811963292\n",
      "Iteration: 8029 lambda_k: 1 Loss: 0.0014512856811975327\n",
      "Iteration: 8030 lambda_k: 1 Loss: 0.0014512856811986841\n",
      "Iteration: 8031 lambda_k: 1 Loss: 0.0014512856811998692\n",
      "Iteration: 8032 lambda_k: 1 Loss: 0.001451285681201036\n",
      "Iteration: 8033 lambda_k: 1 Loss: 0.0014512856812022108\n",
      "Iteration: 8034 lambda_k: 1 Loss: 0.0014512856812033679\n",
      "Iteration: 8035 lambda_k: 1 Loss: 0.0014512856812045306\n",
      "Iteration: 8036 lambda_k: 1 Loss: 0.001451285681205691\n",
      "Iteration: 8037 lambda_k: 1 Loss: 0.0014512856812068046\n",
      "Iteration: 8038 lambda_k: 1 Loss: 0.0014512856812079545\n",
      "Iteration: 8039 lambda_k: 1 Loss: 0.0014512856812090949\n",
      "Iteration: 8040 lambda_k: 1 Loss: 0.0014512856812102337\n",
      "Iteration: 8041 lambda_k: 1 Loss: 0.001451285681211373\n",
      "Iteration: 8042 lambda_k: 1 Loss: 0.0014512856812124964\n",
      "Iteration: 8043 lambda_k: 1 Loss: 0.0014512856812135826\n",
      "Iteration: 8044 lambda_k: 1 Loss: 0.0014512856812147034\n",
      "Iteration: 8045 lambda_k: 1 Loss: 0.0014512856812158067\n",
      "Iteration: 8046 lambda_k: 1 Loss: 0.0014512856812169182\n",
      "Iteration: 8047 lambda_k: 1 Loss: 0.0014512856812180285\n",
      "Iteration: 8048 lambda_k: 1 Loss: 0.0014512856812191161\n",
      "Iteration: 8049 lambda_k: 1 Loss: 0.001451285681220178\n",
      "Iteration: 8050 lambda_k: 1 Loss: 0.001451285681221261\n",
      "Iteration: 8051 lambda_k: 1 Loss: 0.001451285681222337\n",
      "Iteration: 8052 lambda_k: 1 Loss: 0.0014512856812234263\n",
      "Iteration: 8053 lambda_k: 1 Loss: 0.0014512856812244897\n",
      "Iteration: 8054 lambda_k: 1 Loss: 0.0014512856812255637\n",
      "Iteration: 8055 lambda_k: 1 Loss: 0.001451285681226624\n",
      "Iteration: 8056 lambda_k: 1 Loss: 0.0014512856812276455\n",
      "Iteration: 8057 lambda_k: 1 Loss: 0.0014512856812286942\n",
      "Iteration: 8058 lambda_k: 1 Loss: 0.0014512856812297506\n",
      "Iteration: 8059 lambda_k: 1 Loss: 0.00145128568123079\n",
      "Iteration: 8060 lambda_k: 1 Loss: 0.001451285681231839\n",
      "Iteration: 8061 lambda_k: 1 Loss: 0.0014512856812328777\n",
      "Iteration: 8062 lambda_k: 1 Loss: 0.0014512856812338934\n",
      "Iteration: 8063 lambda_k: 1 Loss: 0.0014512856812348835\n",
      "Iteration: 8064 lambda_k: 1 Loss: 0.001451285681235901\n",
      "Iteration: 8065 lambda_k: 1 Loss: 0.0014512856812369263\n",
      "Iteration: 8066 lambda_k: 1 Loss: 0.0014512856812379292\n",
      "Iteration: 8067 lambda_k: 1 Loss: 0.0014512856812389447\n",
      "Iteration: 8068 lambda_k: 1 Loss: 0.0014512856812399536\n",
      "Iteration: 8069 lambda_k: 1 Loss: 0.0014512856812409426\n",
      "Iteration: 8070 lambda_k: 1 Loss: 0.0014512856812419444\n",
      "Iteration: 8071 lambda_k: 1 Loss: 0.0014512856812428962\n",
      "Iteration: 8072 lambda_k: 1 Loss: 0.0014512856812438613\n",
      "Iteration: 8073 lambda_k: 1 Loss: 0.0014512856812448553\n",
      "Iteration: 8074 lambda_k: 1 Loss: 0.0014512856812458187\n",
      "Iteration: 8075 lambda_k: 1 Loss: 0.001451285681246805\n",
      "Iteration: 8076 lambda_k: 1 Loss: 0.001451285681247769\n",
      "Iteration: 8077 lambda_k: 1 Loss: 0.0014512856812487407\n",
      "Iteration: 8078 lambda_k: 1 Loss: 0.0014512856812497075\n",
      "Iteration: 8079 lambda_k: 1 Loss: 0.00145128568125062\n",
      "Iteration: 8080 lambda_k: 1 Loss: 0.001451285681251549\n",
      "Iteration: 8081 lambda_k: 1 Loss: 0.001451285681252511\n",
      "Iteration: 8082 lambda_k: 1 Loss: 0.0014512856812534418\n",
      "Iteration: 8083 lambda_k: 1 Loss: 0.0014512856812543935\n",
      "Iteration: 8084 lambda_k: 1 Loss: 0.0014512856812553283\n",
      "Iteration: 8085 lambda_k: 1 Loss: 0.001451285681256252\n",
      "Iteration: 8086 lambda_k: 1 Loss: 0.0014512856812571868\n",
      "Iteration: 8087 lambda_k: 1 Loss: 0.0014512856812581027\n",
      "Iteration: 8088 lambda_k: 1 Loss: 0.0014512856812589835\n",
      "Iteration: 8089 lambda_k: 1 Loss: 0.0014512856812598977\n",
      "Iteration: 8090 lambda_k: 1 Loss: 0.0014512856812607978\n",
      "Iteration: 8091 lambda_k: 1 Loss: 0.001451285681261712\n",
      "Iteration: 8092 lambda_k: 1 Loss: 0.0014512856812626189\n",
      "Iteration: 8093 lambda_k: 1 Loss: 0.0014512856812635196\n",
      "Iteration: 8094 lambda_k: 1 Loss: 0.0014512856812644065\n",
      "Iteration: 8095 lambda_k: 1 Loss: 0.001451285681265303\n",
      "Iteration: 8096 lambda_k: 1 Loss: 0.0014512856812661826\n",
      "Iteration: 8097 lambda_k: 1 Loss: 0.0014512856812670264\n",
      "Iteration: 8098 lambda_k: 1 Loss: 0.001451285681267899\n",
      "Iteration: 8099 lambda_k: 1 Loss: 0.0014512856812687793\n",
      "Iteration: 8100 lambda_k: 1 Loss: 0.0014512856812696367\n",
      "Iteration: 8101 lambda_k: 1 Loss: 0.001451285681270509\n",
      "Iteration: 8102 lambda_k: 1 Loss: 0.0014512856812713794\n",
      "Iteration: 8103 lambda_k: 1 Loss: 0.0014512856812722277\n",
      "Iteration: 8104 lambda_k: 1 Loss: 0.0014512856812730955\n",
      "Iteration: 8105 lambda_k: 1 Loss: 0.0014512856812739319\n",
      "Iteration: 8106 lambda_k: 1 Loss: 0.0014512856812747919\n",
      "Iteration: 8107 lambda_k: 1 Loss: 0.0014512856812756427\n",
      "Iteration: 8108 lambda_k: 1 Loss: 0.0014512856812764232\n",
      "Iteration: 8109 lambda_k: 1 Loss: 0.0014512856812772632\n",
      "Iteration: 8110 lambda_k: 1 Loss: 0.0014512856812780816\n",
      "Iteration: 8111 lambda_k: 1 Loss: 0.0014512856812789205\n",
      "Iteration: 8112 lambda_k: 1 Loss: 0.001451285681279752\n",
      "Iteration: 8113 lambda_k: 1 Loss: 0.0014512856812805629\n",
      "Iteration: 8114 lambda_k: 1 Loss: 0.0014512856812813938\n",
      "Iteration: 8115 lambda_k: 1 Loss: 0.0014512856812821933\n",
      "Iteration: 8116 lambda_k: 1 Loss: 0.001451285681283014\n",
      "Iteration: 8117 lambda_k: 1 Loss: 0.0014512856812837832\n",
      "Iteration: 8118 lambda_k: 1 Loss: 0.0014512856812845677\n",
      "Iteration: 8119 lambda_k: 1 Loss: 0.0014512856812853808\n",
      "Iteration: 8120 lambda_k: 1 Loss: 0.001451285681286161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8121 lambda_k: 1 Loss: 0.0014512856812869581\n",
      "Iteration: 8122 lambda_k: 1 Loss: 0.001451285681287743\n",
      "Iteration: 8123 lambda_k: 1 Loss: 0.0014512856812885274\n",
      "Iteration: 8124 lambda_k: 1 Loss: 0.0014512856812893076\n",
      "Iteration: 8125 lambda_k: 1 Loss: 0.0014512856812900932\n",
      "Iteration: 8126 lambda_k: 1 Loss: 0.0014512856812908604\n",
      "Iteration: 8127 lambda_k: 1 Loss: 0.001451285681291648\n",
      "Iteration: 8128 lambda_k: 1 Loss: 0.0014512856812923583\n",
      "Iteration: 8129 lambda_k: 1 Loss: 0.001451285681293121\n",
      "Iteration: 8130 lambda_k: 1 Loss: 0.0014512856812938853\n",
      "Iteration: 8131 lambda_k: 1 Loss: 0.001451285681294648\n",
      "Iteration: 8132 lambda_k: 1 Loss: 0.0014512856812953913\n",
      "Iteration: 8133 lambda_k: 1 Loss: 0.0014512856812961556\n",
      "Iteration: 8134 lambda_k: 1 Loss: 0.0014512856812969061\n",
      "Iteration: 8135 lambda_k: 1 Loss: 0.0014512856812976347\n",
      "Iteration: 8136 lambda_k: 1 Loss: 0.0014512856812983774\n",
      "Iteration: 8137 lambda_k: 1 Loss: 0.001451285681299122\n",
      "Iteration: 8138 lambda_k: 1 Loss: 0.00145128568129986\n",
      "Iteration: 8139 lambda_k: 1 Loss: 0.0014512856813005313\n",
      "Iteration: 8140 lambda_k: 1 Loss: 0.0014512856813012616\n",
      "Iteration: 8141 lambda_k: 1 Loss: 0.0014512856813019902\n",
      "Iteration: 8142 lambda_k: 1 Loss: 0.0014512856813027025\n",
      "Iteration: 8143 lambda_k: 1 Loss: 0.001451285681303433\n",
      "Iteration: 8144 lambda_k: 1 Loss: 0.001451285681304155\n",
      "Iteration: 8145 lambda_k: 1 Loss: 0.0014512856813048553\n",
      "Iteration: 8146 lambda_k: 1 Loss: 0.001451285681305574\n",
      "Iteration: 8147 lambda_k: 1 Loss: 0.0014512856813062812\n",
      "Iteration: 8148 lambda_k: 1 Loss: 0.0014512856813069719\n",
      "Iteration: 8149 lambda_k: 1 Loss: 0.0014512856813076794\n",
      "Iteration: 8150 lambda_k: 1 Loss: 0.001451285681308382\n",
      "Iteration: 8151 lambda_k: 1 Loss: 0.0014512856813090143\n",
      "Iteration: 8152 lambda_k: 1 Loss: 0.0014512856813097038\n",
      "Iteration: 8153 lambda_k: 1 Loss: 0.0014512856813103988\n",
      "Iteration: 8154 lambda_k: 1 Loss: 0.0014512856813110877\n",
      "Iteration: 8155 lambda_k: 1 Loss: 0.0014512856813117569\n",
      "Iteration: 8156 lambda_k: 1 Loss: 0.0014512856813124406\n",
      "Iteration: 8157 lambda_k: 1 Loss: 0.001451285681313124\n",
      "Iteration: 8158 lambda_k: 1 Loss: 0.001451285681313792\n",
      "Iteration: 8159 lambda_k: 1 Loss: 0.0014512856813144663\n",
      "Iteration: 8160 lambda_k: 1 Loss: 0.0014512856813150804\n",
      "Iteration: 8161 lambda_k: 1 Loss: 0.0014512856813157415\n",
      "Iteration: 8162 lambda_k: 1 Loss: 0.0014512856813163934\n",
      "Iteration: 8163 lambda_k: 1 Loss: 0.001451285681317058\n",
      "Iteration: 8164 lambda_k: 1 Loss: 0.00145128568131771\n",
      "Iteration: 8165 lambda_k: 1 Loss: 0.0014512856813183584\n",
      "Iteration: 8166 lambda_k: 1 Loss: 0.0014512856813190071\n",
      "Iteration: 8167 lambda_k: 1 Loss: 0.00145128568131966\n",
      "Iteration: 8168 lambda_k: 1 Loss: 0.0014512856813203052\n",
      "Iteration: 8169 lambda_k: 1 Loss: 0.0014512856813209377\n",
      "Iteration: 8170 lambda_k: 1 Loss: 0.0014512856813215839\n",
      "Iteration: 8171 lambda_k: 1 Loss: 0.0014512856813221817\n",
      "Iteration: 8172 lambda_k: 1 Loss: 0.0014512856813227934\n",
      "Iteration: 8173 lambda_k: 1 Loss: 0.001451285681323437\n",
      "Iteration: 8174 lambda_k: 1 Loss: 0.0014512856813240693\n",
      "Iteration: 8175 lambda_k: 1 Loss: 0.0014512856813246851\n",
      "Iteration: 8176 lambda_k: 1 Loss: 0.0014512856813253179\n",
      "Iteration: 8177 lambda_k: 1 Loss: 0.001451285681325921\n",
      "Iteration: 8178 lambda_k: 1 Loss: 0.0014512856813265395\n",
      "Iteration: 8179 lambda_k: 1 Loss: 0.0014512856813271616\n",
      "Iteration: 8180 lambda_k: 1 Loss: 0.001451285681327777\n",
      "Iteration: 8181 lambda_k: 1 Loss: 0.0014512856813283727\n",
      "Iteration: 8182 lambda_k: 1 Loss: 0.0014512856813289868\n",
      "Iteration: 8183 lambda_k: 1 Loss: 0.0014512856813295497\n",
      "Iteration: 8184 lambda_k: 1 Loss: 0.0014512856813301456\n",
      "Iteration: 8185 lambda_k: 1 Loss: 0.0014512856813307287\n",
      "Iteration: 8186 lambda_k: 1 Loss: 0.001451285681331328\n",
      "Iteration: 8187 lambda_k: 1 Loss: 0.001451285681331927\n",
      "Iteration: 8188 lambda_k: 1 Loss: 0.0014512856813325193\n",
      "Iteration: 8189 lambda_k: 1 Loss: 0.0014512856813330987\n",
      "Iteration: 8190 lambda_k: 1 Loss: 0.0014512856813336786\n",
      "Iteration: 8191 lambda_k: 1 Loss: 0.00145128568133426\n",
      "Iteration: 8192 lambda_k: 1 Loss: 0.0014512856813348428\n",
      "Iteration: 8193 lambda_k: 1 Loss: 0.0014512856813354298\n",
      "Iteration: 8194 lambda_k: 1 Loss: 0.0014512856813359938\n",
      "Iteration: 8195 lambda_k: 1 Loss: 0.0014512856813365697\n",
      "Iteration: 8196 lambda_k: 1 Loss: 0.0014512856813371424\n",
      "Iteration: 8197 lambda_k: 1 Loss: 0.0014512856813376498\n",
      "Iteration: 8198 lambda_k: 1 Loss: 0.0014512856813382108\n",
      "Iteration: 8199 lambda_k: 1 Loss: 0.0014512856813387626\n",
      "Iteration: 8200 lambda_k: 1 Loss: 0.0014512856813393234\n",
      "Iteration: 8201 lambda_k: 1 Loss: 0.0014512856813398765\n",
      "Iteration: 8202 lambda_k: 1 Loss: 0.001451285681340436\n",
      "Iteration: 8203 lambda_k: 1 Loss: 0.0014512856813409959\n",
      "Iteration: 8204 lambda_k: 1 Loss: 0.0014512856813415332\n",
      "Iteration: 8205 lambda_k: 1 Loss: 0.0014512856813420885\n",
      "Iteration: 8206 lambda_k: 1 Loss: 0.0014512856813426193\n",
      "Iteration: 8207 lambda_k: 1 Loss: 0.001451285681343165\n",
      "Iteration: 8208 lambda_k: 1 Loss: 0.0014512856813437155\n",
      "Iteration: 8209 lambda_k: 1 Loss: 0.001451285681344246\n",
      "Iteration: 8210 lambda_k: 1 Loss: 0.0014512856813447858\n",
      "Iteration: 8211 lambda_k: 1 Loss: 0.0014512856813453125\n",
      "Iteration: 8212 lambda_k: 1 Loss: 0.0014512856813458492\n",
      "Iteration: 8213 lambda_k: 1 Loss: 0.0014512856813463655\n",
      "Iteration: 8214 lambda_k: 1 Loss: 0.0014512856813468527\n",
      "Iteration: 8215 lambda_k: 1 Loss: 0.0014512856813473562\n",
      "Iteration: 8216 lambda_k: 1 Loss: 0.0014512856813478877\n",
      "Iteration: 8217 lambda_k: 1 Loss: 0.0014512856813484003\n",
      "Iteration: 8218 lambda_k: 1 Loss: 0.0014512856813489264\n",
      "Iteration: 8219 lambda_k: 1 Loss: 0.001451285681349426\n",
      "Iteration: 8220 lambda_k: 1 Loss: 0.0014512856813499462\n",
      "Iteration: 8221 lambda_k: 1 Loss: 0.001451285681350448\n",
      "Iteration: 8222 lambda_k: 1 Loss: 0.0014512856813509532\n",
      "Iteration: 8223 lambda_k: 1 Loss: 0.0014512856813514578\n",
      "Iteration: 8224 lambda_k: 1 Loss: 0.0014512856813519615\n",
      "Iteration: 8225 lambda_k: 1 Loss: 0.0014512856813524695\n",
      "Iteration: 8226 lambda_k: 1 Loss: 0.001451285681352963\n",
      "Iteration: 8227 lambda_k: 1 Loss: 0.0014512856813534596\n",
      "Iteration: 8228 lambda_k: 1 Loss: 0.0014512856813539607\n",
      "Iteration: 8229 lambda_k: 1 Loss: 0.001451285681354464\n",
      "Iteration: 8230 lambda_k: 1 Loss: 0.0014512856813549413\n",
      "Iteration: 8231 lambda_k: 1 Loss: 0.0014512856813554385\n",
      "Iteration: 8232 lambda_k: 1 Loss: 0.0014512856813559305\n",
      "Iteration: 8233 lambda_k: 1 Loss: 0.0014512856813563974\n",
      "Iteration: 8234 lambda_k: 1 Loss: 0.0014512856813568833\n",
      "Iteration: 8235 lambda_k: 1 Loss: 0.0014512856813573708\n",
      "Iteration: 8236 lambda_k: 1 Loss: 0.001451285681357833\n",
      "Iteration: 8237 lambda_k: 1 Loss: 0.0014512856813582756\n",
      "Iteration: 8238 lambda_k: 1 Loss: 0.0014512856813587284\n",
      "Iteration: 8239 lambda_k: 1 Loss: 0.0014512856813592083\n",
      "Iteration: 8240 lambda_k: 1 Loss: 0.0014512856813596686\n",
      "Iteration: 8241 lambda_k: 1 Loss: 0.00145128568136014\n",
      "Iteration: 8242 lambda_k: 1 Loss: 0.0014512856813605937\n",
      "Iteration: 8243 lambda_k: 1 Loss: 0.001451285681361053\n",
      "Iteration: 8244 lambda_k: 1 Loss: 0.001451285681361521\n",
      "Iteration: 8245 lambda_k: 1 Loss: 0.0014512856813619726\n",
      "Iteration: 8246 lambda_k: 1 Loss: 0.0014512856813624277\n",
      "Iteration: 8247 lambda_k: 1 Loss: 0.0014512856813628818\n",
      "Iteration: 8248 lambda_k: 1 Loss: 0.00145128568136334\n",
      "Iteration: 8249 lambda_k: 1 Loss: 0.0014512856813638027\n",
      "Iteration: 8250 lambda_k: 1 Loss: 0.0014512856813642344\n",
      "Iteration: 8251 lambda_k: 1 Loss: 0.0014512856813646859\n",
      "Iteration: 8252 lambda_k: 1 Loss: 0.0014512856813651213\n",
      "Iteration: 8253 lambda_k: 1 Loss: 0.0014512856813655626\n",
      "Iteration: 8254 lambda_k: 1 Loss: 0.001451285681366007\n",
      "Iteration: 8255 lambda_k: 1 Loss: 0.00145128568136645\n",
      "Iteration: 8256 lambda_k: 1 Loss: 0.0014512856813668899\n",
      "Iteration: 8257 lambda_k: 1 Loss: 0.0014512856813673136\n",
      "Iteration: 8258 lambda_k: 1 Loss: 0.0014512856813677527\n",
      "Iteration: 8259 lambda_k: 1 Loss: 0.0014512856813681412\n",
      "Iteration: 8260 lambda_k: 1 Loss: 0.0014512856813685686\n",
      "Iteration: 8261 lambda_k: 1 Loss: 0.0014512856813689795\n",
      "Iteration: 8262 lambda_k: 1 Loss: 0.00145128568136941\n",
      "Iteration: 8263 lambda_k: 1 Loss: 0.0014512856813698241\n",
      "Iteration: 8264 lambda_k: 1 Loss: 0.0014512856813702556\n",
      "Iteration: 8265 lambda_k: 1 Loss: 0.0014512856813706783\n",
      "Iteration: 8266 lambda_k: 1 Loss: 0.001451285681371087\n",
      "Iteration: 8267 lambda_k: 1 Loss: 0.0014512856813715114\n",
      "Iteration: 8268 lambda_k: 1 Loss: 0.0014512856813719297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8269 lambda_k: 1 Loss: 0.0014512856813723315\n",
      "Iteration: 8270 lambda_k: 1 Loss: 0.0014512856813727454\n",
      "Iteration: 8271 lambda_k: 1 Loss: 0.0014512856813731615\n",
      "Iteration: 8272 lambda_k: 1 Loss: 0.0014512856813735549\n",
      "Iteration: 8273 lambda_k: 1 Loss: 0.0014512856813739651\n",
      "Iteration: 8274 lambda_k: 1 Loss: 0.00145128568137437\n",
      "Iteration: 8275 lambda_k: 1 Loss: 0.0014512856813747718\n",
      "Iteration: 8276 lambda_k: 1 Loss: 0.001451285681375159\n",
      "Iteration: 8277 lambda_k: 1 Loss: 0.001451285681375561\n",
      "Iteration: 8278 lambda_k: 1 Loss: 0.00145128568137595\n",
      "Iteration: 8279 lambda_k: 1 Loss: 0.0014512856813763508\n",
      "Iteration: 8280 lambda_k: 1 Loss: 0.001451285681376738\n",
      "Iteration: 8281 lambda_k: 1 Loss: 0.0014512856813771366\n",
      "Iteration: 8282 lambda_k: 1 Loss: 0.0014512856813775224\n",
      "Iteration: 8283 lambda_k: 1 Loss: 0.0014512856813779073\n",
      "Iteration: 8284 lambda_k: 1 Loss: 0.0014512856813782909\n",
      "Iteration: 8285 lambda_k: 1 Loss: 0.0014512856813786753\n",
      "Iteration: 8286 lambda_k: 1 Loss: 0.0014512856813790624\n",
      "Iteration: 8287 lambda_k: 1 Loss: 0.0014512856813794525\n",
      "Iteration: 8288 lambda_k: 1 Loss: 0.001451285681379822\n",
      "Iteration: 8289 lambda_k: 1 Loss: 0.0014512856813802028\n",
      "Iteration: 8290 lambda_k: 1 Loss: 0.0014512856813805404\n",
      "Iteration: 8291 lambda_k: 1 Loss: 0.0014512856813808934\n",
      "Iteration: 8292 lambda_k: 1 Loss: 0.0014512856813812705\n",
      "Iteration: 8293 lambda_k: 1 Loss: 0.001451285681381648\n",
      "Iteration: 8294 lambda_k: 1 Loss: 0.0014512856813820034\n",
      "Iteration: 8295 lambda_k: 1 Loss: 0.0014512856813823733\n",
      "Iteration: 8296 lambda_k: 1 Loss: 0.0014512856813827311\n",
      "Iteration: 8297 lambda_k: 1 Loss: 0.0014512856813831034\n",
      "Iteration: 8298 lambda_k: 1 Loss: 0.0014512856813834738\n",
      "Iteration: 8299 lambda_k: 1 Loss: 0.0014512856813838186\n",
      "Iteration: 8300 lambda_k: 1 Loss: 0.0014512856813841814\n",
      "Iteration: 8301 lambda_k: 1 Loss: 0.0014512856813845472\n",
      "Iteration: 8302 lambda_k: 1 Loss: 0.0014512856813848904\n",
      "Iteration: 8303 lambda_k: 1 Loss: 0.0014512856813852473\n",
      "Iteration: 8304 lambda_k: 1 Loss: 0.0014512856813856056\n",
      "Iteration: 8305 lambda_k: 1 Loss: 0.0014512856813859473\n",
      "Iteration: 8306 lambda_k: 1 Loss: 0.0014512856813863062\n",
      "Iteration: 8307 lambda_k: 1 Loss: 0.0014512856813866462\n",
      "Iteration: 8308 lambda_k: 1 Loss: 0.0014512856813869966\n",
      "Iteration: 8309 lambda_k: 1 Loss: 0.0014512856813873425\n",
      "Iteration: 8310 lambda_k: 1 Loss: 0.0014512856813876805\n",
      "Iteration: 8311 lambda_k: 1 Loss: 0.0014512856813880272\n",
      "Iteration: 8312 lambda_k: 1 Loss: 0.0014512856813883703\n",
      "Iteration: 8313 lambda_k: 1 Loss: 0.0014512856813887142\n",
      "Iteration: 8314 lambda_k: 1 Loss: 0.0014512856813890403\n",
      "Iteration: 8315 lambda_k: 1 Loss: 0.0014512856813893797\n",
      "Iteration: 8316 lambda_k: 1 Loss: 0.0014512856813897132\n",
      "Iteration: 8317 lambda_k: 1 Loss: 0.0014512856813900358\n",
      "Iteration: 8318 lambda_k: 1 Loss: 0.0014512856813903756\n",
      "Iteration: 8319 lambda_k: 1 Loss: 0.0014512856813906998\n",
      "Iteration: 8320 lambda_k: 1 Loss: 0.0014512856813910366\n",
      "Iteration: 8321 lambda_k: 1 Loss: 0.001451285681391365\n",
      "Iteration: 8322 lambda_k: 1 Loss: 0.0014512856813916392\n",
      "Iteration: 8323 lambda_k: 1 Loss: 0.0014512856813919664\n",
      "Iteration: 8324 lambda_k: 1 Loss: 0.001451285681392279\n",
      "Iteration: 8325 lambda_k: 1 Loss: 0.0014512856813926082\n",
      "Iteration: 8326 lambda_k: 1 Loss: 0.0014512856813929426\n",
      "Iteration: 8327 lambda_k: 1 Loss: 0.001451285681393253\n",
      "Iteration: 8328 lambda_k: 1 Loss: 0.0014512856813935807\n",
      "Iteration: 8329 lambda_k: 1 Loss: 0.0014512856813939\n",
      "Iteration: 8330 lambda_k: 1 Loss: 0.0014512856813941994\n",
      "Iteration: 8331 lambda_k: 1 Loss: 0.0014512856813945164\n",
      "Iteration: 8332 lambda_k: 1 Loss: 0.0014512856813948373\n",
      "Iteration: 8333 lambda_k: 1 Loss: 0.0014512856813951377\n",
      "Iteration: 8334 lambda_k: 1 Loss: 0.001451285681395454\n",
      "Iteration: 8335 lambda_k: 1 Loss: 0.0014512856813957515\n",
      "Iteration: 8336 lambda_k: 1 Loss: 0.0014512856813960616\n",
      "Iteration: 8337 lambda_k: 1 Loss: 0.0014512856813963695\n",
      "Iteration: 8338 lambda_k: 1 Loss: 0.0014512856813966753\n",
      "Iteration: 8339 lambda_k: 1 Loss: 0.001451285681396962\n",
      "Iteration: 8340 lambda_k: 1 Loss: 0.0014512856813972685\n",
      "Iteration: 8341 lambda_k: 1 Loss: 0.0014512856813975771\n",
      "Iteration: 8342 lambda_k: 1 Loss: 0.0014512856813978675\n",
      "Iteration: 8343 lambda_k: 1 Loss: 0.0014512856813981678\n",
      "Iteration: 8344 lambda_k: 1 Loss: 0.0014512856813984718\n",
      "Iteration: 8345 lambda_k: 1 Loss: 0.0014512856813987543\n",
      "Iteration: 8346 lambda_k: 1 Loss: 0.0014512856813990497\n",
      "Iteration: 8347 lambda_k: 1 Loss: 0.0014512856813993504\n",
      "Iteration: 8348 lambda_k: 1 Loss: 0.0014512856813996267\n",
      "Iteration: 8349 lambda_k: 1 Loss: 0.0014512856813999222\n",
      "Iteration: 8350 lambda_k: 1 Loss: 0.0014512856814002102\n",
      "Iteration: 8351 lambda_k: 1 Loss: 0.0014512856814004904\n",
      "Iteration: 8352 lambda_k: 1 Loss: 0.00145128568140078\n",
      "Iteration: 8353 lambda_k: 1 Loss: 0.0014512856814010637\n",
      "Iteration: 8354 lambda_k: 1 Loss: 0.0014512856814013397\n",
      "Iteration: 8355 lambda_k: 1 Loss: 0.0014512856814016314\n",
      "Iteration: 8356 lambda_k: 1 Loss: 0.0014512856814019078\n",
      "Iteration: 8357 lambda_k: 1 Loss: 0.001451285681402196\n",
      "Iteration: 8358 lambda_k: 1 Loss: 0.001451285681402478\n",
      "Iteration: 8359 lambda_k: 1 Loss: 0.0014512856814027488\n",
      "Iteration: 8360 lambda_k: 1 Loss: 0.0014512856814030302\n",
      "Iteration: 8361 lambda_k: 1 Loss: 0.0014512856814033138\n",
      "Iteration: 8362 lambda_k: 1 Loss: 0.001451285681403575\n",
      "Iteration: 8363 lambda_k: 1 Loss: 0.0014512856814038603\n",
      "Iteration: 8364 lambda_k: 1 Loss: 0.0014512856814041211\n",
      "Iteration: 8365 lambda_k: 1 Loss: 0.0014512856814043978\n",
      "Iteration: 8366 lambda_k: 1 Loss: 0.0014512856814046754\n",
      "Iteration: 8367 lambda_k: 1 Loss: 0.001451285681404934\n",
      "Iteration: 8368 lambda_k: 1 Loss: 0.0014512856814052056\n",
      "Iteration: 8369 lambda_k: 1 Loss: 0.001451285681405479\n",
      "Iteration: 8370 lambda_k: 1 Loss: 0.0014512856814057388\n",
      "Iteration: 8371 lambda_k: 1 Loss: 0.0014512856814060107\n",
      "Iteration: 8372 lambda_k: 1 Loss: 0.00145128568140628\n",
      "Iteration: 8373 lambda_k: 1 Loss: 0.0014512856814065309\n",
      "Iteration: 8374 lambda_k: 1 Loss: 0.0014512856814067963\n",
      "Iteration: 8375 lambda_k: 1 Loss: 0.0014512856814070624\n",
      "Iteration: 8376 lambda_k: 1 Loss: 0.001451285681407313\n",
      "Iteration: 8377 lambda_k: 1 Loss: 0.0014512856814075381\n",
      "Iteration: 8378 lambda_k: 1 Loss: 0.0014512856814077762\n",
      "Iteration: 8379 lambda_k: 1 Loss: 0.0014512856814080375\n",
      "Iteration: 8380 lambda_k: 1 Loss: 0.0014512856814082923\n",
      "Iteration: 8381 lambda_k: 1 Loss: 0.0014512856814085347\n",
      "Iteration: 8382 lambda_k: 1 Loss: 0.001451285681408794\n",
      "Iteration: 8383 lambda_k: 1 Loss: 0.001451285681409045\n",
      "Iteration: 8384 lambda_k: 1 Loss: 0.001451285681409284\n",
      "Iteration: 8385 lambda_k: 1 Loss: 0.0014512856814095356\n",
      "Iteration: 8386 lambda_k: 1 Loss: 0.001451285681409784\n",
      "Iteration: 8387 lambda_k: 1 Loss: 0.0014512856814100326\n",
      "Iteration: 8388 lambda_k: 1 Loss: 0.0014512856814102768\n",
      "Iteration: 8389 lambda_k: 1 Loss: 0.0014512856814105069\n",
      "Iteration: 8390 lambda_k: 1 Loss: 0.001451285681410758\n",
      "Iteration: 8391 lambda_k: 1 Loss: 0.0014512856814109906\n",
      "Iteration: 8392 lambda_k: 1 Loss: 0.0014512856814112409\n",
      "Iteration: 8393 lambda_k: 1 Loss: 0.0014512856814114868\n",
      "Iteration: 8394 lambda_k: 1 Loss: 0.0014512856814117155\n",
      "Iteration: 8395 lambda_k: 1 Loss: 0.0014512856814119575\n",
      "Iteration: 8396 lambda_k: 1 Loss: 0.0014512856814121974\n",
      "Iteration: 8397 lambda_k: 1 Loss: 0.0014512856814124183\n",
      "Iteration: 8398 lambda_k: 1 Loss: 0.0014512856814126575\n",
      "Iteration: 8399 lambda_k: 1 Loss: 0.0014512856814128893\n",
      "Iteration: 8400 lambda_k: 1 Loss: 0.0014512856814131085\n",
      "Iteration: 8401 lambda_k: 1 Loss: 0.0014512856814133462\n",
      "Iteration: 8402 lambda_k: 1 Loss: 0.0014512856814135782\n",
      "Iteration: 8403 lambda_k: 1 Loss: 0.0014512856814137966\n",
      "Iteration: 8404 lambda_k: 1 Loss: 0.0014512856814140281\n",
      "Iteration: 8405 lambda_k: 1 Loss: 0.0014512856814142604\n",
      "Iteration: 8406 lambda_k: 1 Loss: 0.0014512856814144729\n",
      "Iteration: 8407 lambda_k: 1 Loss: 0.001451285681414694\n",
      "Iteration: 8408 lambda_k: 1 Loss: 0.0014512856814149274\n",
      "Iteration: 8409 lambda_k: 1 Loss: 0.0014512856814151548\n",
      "Iteration: 8410 lambda_k: 1 Loss: 0.0014512856814153218\n",
      "Iteration: 8411 lambda_k: 1 Loss: 0.0014512856814155482\n",
      "Iteration: 8412 lambda_k: 1 Loss: 0.001451285681415756\n",
      "Iteration: 8413 lambda_k: 1 Loss: 0.001451285681415977\n",
      "Iteration: 8414 lambda_k: 1 Loss: 0.001451285681416201\n",
      "Iteration: 8415 lambda_k: 1 Loss: 0.0014512856814164086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8416 lambda_k: 1 Loss: 0.0014512856814166337\n",
      "Iteration: 8417 lambda_k: 1 Loss: 0.001451285681416836\n",
      "Iteration: 8418 lambda_k: 1 Loss: 0.0014512856814170535\n",
      "Iteration: 8419 lambda_k: 1 Loss: 0.0014512856814172734\n",
      "Iteration: 8420 lambda_k: 1 Loss: 0.0014512856814174768\n",
      "Iteration: 8421 lambda_k: 1 Loss: 0.0014512856814176858\n",
      "Iteration: 8422 lambda_k: 1 Loss: 0.0014512856814178994\n",
      "Iteration: 8423 lambda_k: 1 Loss: 0.001451285681418118\n",
      "Iteration: 8424 lambda_k: 1 Loss: 0.0014512856814183303\n",
      "Iteration: 8425 lambda_k: 1 Loss: 0.0014512856814185421\n",
      "Iteration: 8426 lambda_k: 1 Loss: 0.0014512856814187388\n",
      "Iteration: 8427 lambda_k: 1 Loss: 0.001451285681418948\n",
      "Iteration: 8428 lambda_k: 1 Loss: 0.0014512856814191623\n",
      "Iteration: 8429 lambda_k: 1 Loss: 0.0014512856814193596\n",
      "Iteration: 8430 lambda_k: 1 Loss: 0.0014512856814195671\n",
      "Iteration: 8431 lambda_k: 1 Loss: 0.001451285681419778\n",
      "Iteration: 8432 lambda_k: 1 Loss: 0.0014512856814199821\n",
      "Iteration: 8433 lambda_k: 1 Loss: 0.001451285681420178\n",
      "Iteration: 8434 lambda_k: 1 Loss: 0.001451285681420387\n",
      "Iteration: 8435 lambda_k: 1 Loss: 0.0014512856814205904\n",
      "Iteration: 8436 lambda_k: 1 Loss: 0.0014512856814207782\n",
      "Iteration: 8437 lambda_k: 1 Loss: 0.0014512856814209853\n",
      "Iteration: 8438 lambda_k: 1 Loss: 0.0014512856814211902\n",
      "Iteration: 8439 lambda_k: 1 Loss: 0.0014512856814213905\n",
      "Iteration: 8440 lambda_k: 1 Loss: 0.0014512856814215761\n",
      "Iteration: 8441 lambda_k: 1 Loss: 0.0014512856814217837\n",
      "Iteration: 8442 lambda_k: 1 Loss: 0.00145128568142197\n",
      "Iteration: 8443 lambda_k: 1 Loss: 0.0014512856814221644\n",
      "Iteration: 8444 lambda_k: 1 Loss: 0.0014512856814223598\n",
      "Iteration: 8445 lambda_k: 1 Loss: 0.0014512856814225428\n",
      "Iteration: 8446 lambda_k: 1 Loss: 0.0014512856814227369\n",
      "Iteration: 8447 lambda_k: 1 Loss: 0.0014512856814229325\n",
      "Iteration: 8448 lambda_k: 1 Loss: 0.001451285681423122\n",
      "Iteration: 8449 lambda_k: 1 Loss: 0.0014512856814233024\n",
      "Iteration: 8450 lambda_k: 1 Loss: 0.0014512856814235\n",
      "Iteration: 8451 lambda_k: 1 Loss: 0.0014512856814236782\n",
      "Iteration: 8452 lambda_k: 1 Loss: 0.0014512856814238716\n",
      "Iteration: 8453 lambda_k: 1 Loss: 0.001451285681424044\n",
      "Iteration: 8454 lambda_k: 1 Loss: 0.0014512856814242405\n",
      "Iteration: 8455 lambda_k: 1 Loss: 0.0014512856814244115\n",
      "Iteration: 8456 lambda_k: 1 Loss: 0.001451285681424597\n",
      "Iteration: 8457 lambda_k: 1 Loss: 0.0014512856814247847\n",
      "Iteration: 8458 lambda_k: 1 Loss: 0.001451285681424953\n",
      "Iteration: 8459 lambda_k: 1 Loss: 0.0014512856814251367\n",
      "Iteration: 8460 lambda_k: 1 Loss: 0.0014512856814253192\n",
      "Iteration: 8461 lambda_k: 1 Loss: 0.001451285681425484\n",
      "Iteration: 8462 lambda_k: 1 Loss: 0.0014512856814256716\n",
      "Iteration: 8463 lambda_k: 1 Loss: 0.0014512856814258355\n",
      "Iteration: 8464 lambda_k: 1 Loss: 0.0014512856814260225\n",
      "Iteration: 8465 lambda_k: 1 Loss: 0.001451285681426184\n",
      "Iteration: 8466 lambda_k: 1 Loss: 0.0014512856814263625\n",
      "Iteration: 8467 lambda_k: 1 Loss: 0.00145128568142654\n",
      "Iteration: 8468 lambda_k: 1 Loss: 0.0014512856814267038\n",
      "Iteration: 8469 lambda_k: 1 Loss: 0.0014512856814268822\n",
      "Iteration: 8470 lambda_k: 1 Loss: 0.0014512856814270555\n",
      "Iteration: 8471 lambda_k: 1 Loss: 0.0014512856814272142\n",
      "Iteration: 8472 lambda_k: 1 Loss: 0.0014512856814273914\n",
      "Iteration: 8473 lambda_k: 1 Loss: 0.0014512856814275507\n",
      "Iteration: 8474 lambda_k: 1 Loss: 0.0014512856814277322\n",
      "Iteration: 8475 lambda_k: 1 Loss: 0.0014512856814278914\n",
      "Iteration: 8476 lambda_k: 1 Loss: 0.0014512856814280677\n",
      "Iteration: 8477 lambda_k: 1 Loss: 0.0014512856814282197\n",
      "Iteration: 8478 lambda_k: 1 Loss: 0.0014512856814283873\n",
      "Iteration: 8479 lambda_k: 1 Loss: 0.0014512856814285517\n",
      "Iteration: 8480 lambda_k: 1 Loss: 0.0014512856814287015\n",
      "Iteration: 8481 lambda_k: 1 Loss: 0.0014512856814288765\n",
      "Iteration: 8482 lambda_k: 1 Loss: 0.001451285681429043\n",
      "Iteration: 8483 lambda_k: 1 Loss: 0.0014512856814291914\n",
      "Iteration: 8484 lambda_k: 1 Loss: 0.0014512856814293624\n",
      "Iteration: 8485 lambda_k: 1 Loss: 0.0014512856814295134\n",
      "Iteration: 8486 lambda_k: 1 Loss: 0.0014512856814296786\n",
      "Iteration: 8487 lambda_k: 1 Loss: 0.0014512856814298447\n",
      "Iteration: 8488 lambda_k: 1 Loss: 0.0014512856814299906\n",
      "Iteration: 8489 lambda_k: 1 Loss: 0.0014512856814301483\n",
      "Iteration: 8490 lambda_k: 1 Loss: 0.0014512856814303007\n",
      "Iteration: 8491 lambda_k: 1 Loss: 0.0014512856814304649\n",
      "Iteration: 8492 lambda_k: 1 Loss: 0.001451285681430626\n",
      "Iteration: 8493 lambda_k: 1 Loss: 0.0014512856814307665\n",
      "Iteration: 8494 lambda_k: 1 Loss: 0.0014512856814309324\n",
      "Iteration: 8495 lambda_k: 1 Loss: 0.0014512856814310885\n",
      "Iteration: 8496 lambda_k: 1 Loss: 0.0014512856814312323\n",
      "Iteration: 8497 lambda_k: 1 Loss: 0.0014512856814313877\n",
      "Iteration: 8498 lambda_k: 1 Loss: 0.0014512856814315406\n",
      "Iteration: 8499 lambda_k: 1 Loss: 0.0014512856814316844\n",
      "Iteration: 8500 lambda_k: 1 Loss: 0.0014512856814318407\n",
      "Iteration: 8501 lambda_k: 1 Loss: 0.0014512856814319873\n",
      "Iteration: 8502 lambda_k: 1 Loss: 0.0014512856814321402\n",
      "Iteration: 8503 lambda_k: 1 Loss: 0.0014512856814322909\n",
      "Iteration: 8504 lambda_k: 1 Loss: 0.0014512856814324303\n",
      "Iteration: 8505 lambda_k: 1 Loss: 0.001451285681432587\n",
      "Iteration: 8506 lambda_k: 1 Loss: 0.0014512856814327426\n",
      "Iteration: 8507 lambda_k: 1 Loss: 0.0014512856814328753\n",
      "Iteration: 8508 lambda_k: 1 Loss: 0.001451285681433032\n",
      "Iteration: 8509 lambda_k: 1 Loss: 0.0014512856814331819\n",
      "Iteration: 8510 lambda_k: 1 Loss: 0.0014512856814333176\n",
      "Iteration: 8511 lambda_k: 1 Loss: 0.0014512856814334668\n",
      "Iteration: 8512 lambda_k: 1 Loss: 0.0014512856814336051\n",
      "Iteration: 8513 lambda_k: 1 Loss: 0.0014512856814337535\n",
      "Iteration: 8514 lambda_k: 1 Loss: 0.0014512856814339076\n",
      "Iteration: 8515 lambda_k: 1 Loss: 0.0014512856814340354\n",
      "Iteration: 8516 lambda_k: 1 Loss: 0.0014512856814341804\n",
      "Iteration: 8517 lambda_k: 1 Loss: 0.001451285681434318\n",
      "Iteration: 8518 lambda_k: 1 Loss: 0.0014512856814344634\n",
      "Iteration: 8519 lambda_k: 1 Loss: 0.0014512856814345963\n",
      "Iteration: 8520 lambda_k: 1 Loss: 0.0014512856814347384\n",
      "Iteration: 8521 lambda_k: 1 Loss: 0.0014512856814348854\n",
      "Iteration: 8522 lambda_k: 1 Loss: 0.0014512856814350146\n",
      "Iteration: 8523 lambda_k: 1 Loss: 0.0014512856814351569\n",
      "Iteration: 8524 lambda_k: 1 Loss: 0.0014512856814353\n",
      "Iteration: 8525 lambda_k: 1 Loss: 0.001451285681435422\n",
      "Iteration: 8526 lambda_k: 1 Loss: 0.0014512856814355643\n",
      "Iteration: 8527 lambda_k: 1 Loss: 0.0014512856814357044\n",
      "Iteration: 8528 lambda_k: 1 Loss: 0.0014512856814358245\n",
      "Iteration: 8529 lambda_k: 1 Loss: 0.0014512856814359704\n",
      "Iteration: 8530 lambda_k: 1 Loss: 0.0014512856814360947\n",
      "Iteration: 8531 lambda_k: 1 Loss: 0.0014512856814362346\n",
      "Iteration: 8532 lambda_k: 1 Loss: 0.0014512856814363569\n",
      "Iteration: 8533 lambda_k: 1 Loss: 0.0014512856814364904\n",
      "Iteration: 8534 lambda_k: 1 Loss: 0.0014512856814366294\n",
      "Iteration: 8535 lambda_k: 1 Loss: 0.0014512856814367513\n",
      "Iteration: 8536 lambda_k: 1 Loss: 0.0014512856814368881\n",
      "Iteration: 8537 lambda_k: 1 Loss: 0.0014512856814370104\n",
      "Iteration: 8538 lambda_k: 1 Loss: 0.0014512856814371438\n",
      "Iteration: 8539 lambda_k: 1 Loss: 0.0014512856814372802\n",
      "Iteration: 8540 lambda_k: 1 Loss: 0.001451285681437403\n",
      "Iteration: 8541 lambda_k: 1 Loss: 0.0014512856814375397\n",
      "Iteration: 8542 lambda_k: 1 Loss: 0.0014512856814376585\n",
      "Iteration: 8543 lambda_k: 1 Loss: 0.0014512856814377854\n",
      "Iteration: 8544 lambda_k: 1 Loss: 0.0014512856814379196\n",
      "Iteration: 8545 lambda_k: 1 Loss: 0.0014512856814380547\n",
      "Iteration: 8546 lambda_k: 1 Loss: 0.0014512856814381681\n",
      "Iteration: 8547 lambda_k: 1 Loss: 0.0014512856814382989\n",
      "Iteration: 8548 lambda_k: 1 Loss: 0.0014512856814384132\n",
      "Iteration: 8549 lambda_k: 1 Loss: 0.0014512856814385435\n",
      "Iteration: 8550 lambda_k: 1 Loss: 0.001451285681438674\n",
      "Iteration: 8551 lambda_k: 1 Loss: 0.0014512856814387894\n",
      "Iteration: 8552 lambda_k: 1 Loss: 0.001451285681438919\n",
      "Iteration: 8553 lambda_k: 1 Loss: 0.001451285681439033\n",
      "Iteration: 8554 lambda_k: 1 Loss: 0.0014512856814391597\n",
      "Iteration: 8555 lambda_k: 1 Loss: 0.001451285681439287\n",
      "Iteration: 8556 lambda_k: 1 Loss: 0.0014512856814393497\n",
      "Iteration: 8557 lambda_k: 1 Loss: 0.0014512856814394679\n",
      "Iteration: 8558 lambda_k: 1 Loss: 0.0014512856814395912\n",
      "Iteration: 8559 lambda_k: 1 Loss: 0.0014512856814396923\n",
      "Iteration: 8560 lambda_k: 1 Loss: 0.001451285681439812\n",
      "Iteration: 8561 lambda_k: 1 Loss: 0.0014512856814399195\n",
      "Iteration: 8562 lambda_k: 1 Loss: 0.0014512856814400412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8563 lambda_k: 1 Loss: 0.0014512856814401479\n",
      "Iteration: 8564 lambda_k: 1 Loss: 0.001451285681440271\n",
      "Iteration: 8565 lambda_k: 1 Loss: 0.001451285681440387\n",
      "Iteration: 8566 lambda_k: 1 Loss: 0.001451285681440503\n",
      "Iteration: 8567 lambda_k: 1 Loss: 0.0014512856814406039\n",
      "Iteration: 8568 lambda_k: 1 Loss: 0.0014512856814407177\n",
      "Iteration: 8569 lambda_k: 1 Loss: 0.0014512856814408225\n",
      "Iteration: 8570 lambda_k: 1 Loss: 0.0014512856814409322\n",
      "Iteration: 8571 lambda_k: 1 Loss: 0.0014512856814410458\n",
      "Iteration: 8572 lambda_k: 1 Loss: 0.0014512856814411497\n",
      "Iteration: 8573 lambda_k: 1 Loss: 0.0014512856814412672\n",
      "Iteration: 8574 lambda_k: 1 Loss: 0.0014512856814413847\n",
      "Iteration: 8575 lambda_k: 1 Loss: 0.00145128568144148\n",
      "Iteration: 8576 lambda_k: 1 Loss: 0.0014512856814415962\n",
      "Iteration: 8577 lambda_k: 1 Loss: 0.0014512856814417\n",
      "Iteration: 8578 lambda_k: 1 Loss: 0.0014512856814417898\n",
      "Iteration: 8579 lambda_k: 1 Loss: 0.0014512856814419\n",
      "Iteration: 8580 lambda_k: 1 Loss: 0.0014512856814420062\n",
      "Iteration: 8581 lambda_k: 1 Loss: 0.0014512856814421023\n",
      "Iteration: 8582 lambda_k: 1 Loss: 0.0014512856814422083\n",
      "Iteration: 8583 lambda_k: 1 Loss: 0.0014512856814422965\n",
      "Iteration: 8584 lambda_k: 1 Loss: 0.0014512856814424024\n",
      "Iteration: 8585 lambda_k: 1 Loss: 0.0014512856814425008\n",
      "Iteration: 8586 lambda_k: 1 Loss: 0.0014512856814426064\n",
      "Iteration: 8587 lambda_k: 1 Loss: 0.0014512856814427157\n",
      "Iteration: 8588 lambda_k: 1 Loss: 0.0014512856814427998\n",
      "Iteration: 8589 lambda_k: 1 Loss: 0.001451285681442904\n",
      "Iteration: 8590 lambda_k: 1 Loss: 0.0014512856814430086\n",
      "Iteration: 8591 lambda_k: 1 Loss: 0.0014512856814431138\n",
      "Iteration: 8592 lambda_k: 1 Loss: 0.0014512856814431962\n",
      "Iteration: 8593 lambda_k: 1 Loss: 0.0014512856814433007\n",
      "Iteration: 8594 lambda_k: 1 Loss: 0.0014512856814433946\n",
      "Iteration: 8595 lambda_k: 1 Loss: 0.0014512856814434514\n",
      "Iteration: 8596 lambda_k: 1 Loss: 0.0014512856814435325\n",
      "Iteration: 8597 lambda_k: 1 Loss: 0.0014512856814436292\n",
      "Iteration: 8598 lambda_k: 1 Loss: 0.0014512856814437164\n",
      "Iteration: 8599 lambda_k: 1 Loss: 0.0014512856814438112\n",
      "Iteration: 8600 lambda_k: 1 Loss: 0.0014512856814439072\n",
      "Iteration: 8601 lambda_k: 1 Loss: 0.0014512856814439974\n",
      "Iteration: 8602 lambda_k: 1 Loss: 0.001451285681444095\n",
      "Iteration: 8603 lambda_k: 1 Loss: 0.001451285681444199\n",
      "Iteration: 8604 lambda_k: 1 Loss: 0.0014512856814442783\n",
      "Iteration: 8605 lambda_k: 1 Loss: 0.0014512856814443776\n",
      "Iteration: 8606 lambda_k: 1 Loss: 0.0014512856814444751\n",
      "Iteration: 8607 lambda_k: 1 Loss: 0.0014512856814445684\n",
      "Iteration: 8608 lambda_k: 1 Loss: 0.00145128568144465\n",
      "Iteration: 8609 lambda_k: 1 Loss: 0.0014512856814447473\n",
      "Iteration: 8610 lambda_k: 1 Loss: 0.00145128568144483\n",
      "Iteration: 8611 lambda_k: 1 Loss: 0.001451285681444929\n",
      "Iteration: 8612 lambda_k: 1 Loss: 0.0014512856814450155\n",
      "Iteration: 8613 lambda_k: 1 Loss: 0.0014512856814451148\n",
      "Iteration: 8614 lambda_k: 1 Loss: 0.0014512856814451964\n",
      "Iteration: 8615 lambda_k: 1 Loss: 0.0014512856814452892\n",
      "Iteration: 8616 lambda_k: 1 Loss: 0.0014512856814453876\n",
      "Iteration: 8617 lambda_k: 1 Loss: 0.0014512856814454618\n",
      "Iteration: 8618 lambda_k: 1 Loss: 0.0014512856814455526\n",
      "Iteration: 8619 lambda_k: 1 Loss: 0.0014512856814456335\n",
      "Iteration: 8620 lambda_k: 1 Loss: 0.001451285681445724\n",
      "Iteration: 8621 lambda_k: 1 Loss: 0.0014512856814458224\n",
      "Iteration: 8622 lambda_k: 1 Loss: 0.0014512856814458957\n",
      "Iteration: 8623 lambda_k: 1 Loss: 0.0014512856814459883\n",
      "Iteration: 8624 lambda_k: 1 Loss: 0.0014512856814460767\n",
      "Iteration: 8625 lambda_k: 1 Loss: 0.001451285681446149\n",
      "Iteration: 8626 lambda_k: 1 Loss: 0.0014512856814462363\n",
      "Iteration: 8627 lambda_k: 1 Loss: 0.001451285681446329\n",
      "Iteration: 8628 lambda_k: 1 Loss: 0.0014512856814464007\n",
      "Iteration: 8629 lambda_k: 1 Loss: 0.001451285681446493\n",
      "Iteration: 8630 lambda_k: 1 Loss: 0.0014512856814465668\n",
      "Iteration: 8631 lambda_k: 1 Loss: 0.0014512856814466566\n",
      "Iteration: 8632 lambda_k: 1 Loss: 0.0014512856814467394\n",
      "Iteration: 8633 lambda_k: 1 Loss: 0.0014512856814468066\n",
      "Iteration: 8634 lambda_k: 1 Loss: 0.001451285681446894\n",
      "Iteration: 8635 lambda_k: 1 Loss: 0.0014512856814469825\n",
      "Iteration: 8636 lambda_k: 1 Loss: 0.0014512856814470536\n",
      "Iteration: 8637 lambda_k: 1 Loss: 0.0014512856814471442\n",
      "Iteration: 8638 lambda_k: 1 Loss: 0.0014512856814472314\n",
      "Iteration: 8639 lambda_k: 1 Loss: 0.0014512856814472906\n",
      "Iteration: 8640 lambda_k: 1 Loss: 0.0014512856814473776\n",
      "Iteration: 8641 lambda_k: 1 Loss: 0.0014512856814474593\n",
      "Iteration: 8642 lambda_k: 1 Loss: 0.0014512856814475423\n",
      "Iteration: 8643 lambda_k: 1 Loss: 0.001451285681447627\n",
      "Iteration: 8644 lambda_k: 1 Loss: 0.001451285681447693\n",
      "Iteration: 8645 lambda_k: 1 Loss: 0.001451285681447778\n",
      "Iteration: 8646 lambda_k: 1 Loss: 0.0014512856814478652\n",
      "Iteration: 8647 lambda_k: 1 Loss: 0.001451285681447931\n",
      "Iteration: 8648 lambda_k: 1 Loss: 0.001451285681448014\n",
      "Iteration: 8649 lambda_k: 1 Loss: 0.001451285681448084\n",
      "Iteration: 8650 lambda_k: 1 Loss: 0.0014512856814481677\n",
      "Iteration: 8651 lambda_k: 1 Loss: 0.0014512856814482506\n",
      "Iteration: 8652 lambda_k: 1 Loss: 0.0014512856814483176\n",
      "Iteration: 8653 lambda_k: 1 Loss: 0.0014512856814483967\n",
      "Iteration: 8654 lambda_k: 1 Loss: 0.001451285681448485\n",
      "Iteration: 8655 lambda_k: 1 Loss: 0.001451285681448546\n",
      "Iteration: 8656 lambda_k: 1 Loss: 0.0014512856814486274\n",
      "Iteration: 8657 lambda_k: 1 Loss: 0.0014512856814486927\n",
      "Iteration: 8658 lambda_k: 1 Loss: 0.001451285681448775\n",
      "Iteration: 8659 lambda_k: 1 Loss: 0.001451285681448856\n",
      "Iteration: 8660 lambda_k: 1 Loss: 0.0014512856814489217\n",
      "Iteration: 8661 lambda_k: 1 Loss: 0.0014512856814490002\n",
      "Iteration: 8662 lambda_k: 1 Loss: 0.0014512856814490635\n",
      "Iteration: 8663 lambda_k: 1 Loss: 0.0014512856814491433\n",
      "Iteration: 8664 lambda_k: 1 Loss: 0.001451285681449206\n",
      "Iteration: 8665 lambda_k: 1 Loss: 0.001451285681449281\n",
      "Iteration: 8666 lambda_k: 1 Loss: 0.0014512856814493473\n",
      "Iteration: 8667 lambda_k: 1 Loss: 0.0014512856814494245\n",
      "Iteration: 8668 lambda_k: 1 Loss: 0.0014512856814495032\n",
      "Iteration: 8669 lambda_k: 1 Loss: 0.0014512856814495585\n",
      "Iteration: 8670 lambda_k: 1 Loss: 0.00145128568144963\n",
      "Iteration: 8671 lambda_k: 1 Loss: 0.0014512856814497112\n",
      "Iteration: 8672 lambda_k: 1 Loss: 0.0014512856814497684\n",
      "Iteration: 8673 lambda_k: 1 Loss: 0.0014512856814498426\n",
      "Iteration: 8674 lambda_k: 1 Loss: 0.001451285681449921\n",
      "Iteration: 8675 lambda_k: 1 Loss: 0.0014512856814499783\n",
      "Iteration: 8676 lambda_k: 1 Loss: 0.0014512856814500531\n",
      "Iteration: 8677 lambda_k: 1 Loss: 0.0014512856814501273\n",
      "Iteration: 8678 lambda_k: 1 Loss: 0.0014512856814501817\n",
      "Iteration: 8679 lambda_k: 1 Loss: 0.0014512856814502591\n",
      "Iteration: 8680 lambda_k: 1 Loss: 0.0014512856814503127\n",
      "Iteration: 8681 lambda_k: 1 Loss: 0.001451285681450385\n",
      "Iteration: 8682 lambda_k: 1 Loss: 0.0014512856814504552\n",
      "Iteration: 8683 lambda_k: 1 Loss: 0.0014512856814505278\n",
      "Iteration: 8684 lambda_k: 1 Loss: 0.00145128568145058\n",
      "Iteration: 8685 lambda_k: 1 Loss: 0.0014512856814506516\n",
      "Iteration: 8686 lambda_k: 1 Loss: 0.0014512856814507045\n",
      "Iteration: 8687 lambda_k: 1 Loss: 0.0014512856814507765\n",
      "Iteration: 8688 lambda_k: 1 Loss: 0.0014512856814508516\n",
      "Iteration: 8689 lambda_k: 1 Loss: 0.001451285681450903\n",
      "Iteration: 8690 lambda_k: 1 Loss: 0.0014512856814509743\n",
      "Iteration: 8691 lambda_k: 1 Loss: 0.0014512856814510333\n",
      "Iteration: 8692 lambda_k: 1 Loss: 0.001451285681451107\n",
      "Iteration: 8693 lambda_k: 1 Loss: 0.00145128568145116\n",
      "Iteration: 8694 lambda_k: 1 Loss: 0.0014512856814512276\n",
      "Iteration: 8695 lambda_k: 1 Loss: 0.0014512856814512991\n",
      "Iteration: 8696 lambda_k: 1 Loss: 0.0014512856814513525\n",
      "Iteration: 8697 lambda_k: 1 Loss: 0.0014512856814514162\n",
      "Iteration: 8698 lambda_k: 1 Loss: 0.0014512856814514687\n",
      "Iteration: 8699 lambda_k: 1 Loss: 0.0014512856814515337\n",
      "Iteration: 8700 lambda_k: 1 Loss: 0.0014512856814516038\n",
      "Iteration: 8701 lambda_k: 1 Loss: 0.0014512856814516545\n",
      "Iteration: 8702 lambda_k: 1 Loss: 0.0014512856814517313\n",
      "Iteration: 8703 lambda_k: 1 Loss: 0.0014512856814517946\n",
      "Iteration: 8704 lambda_k: 1 Loss: 0.0014512856814518451\n",
      "Iteration: 8705 lambda_k: 1 Loss: 0.001451285681451917\n",
      "Iteration: 8706 lambda_k: 1 Loss: 0.0014512856814519694\n",
      "Iteration: 8707 lambda_k: 1 Loss: 0.0014512856814520375\n",
      "Iteration: 8708 lambda_k: 1 Loss: 0.0014512856814520878\n",
      "Iteration: 8709 lambda_k: 1 Loss: 0.0014512856814521504\n",
      "Iteration: 8710 lambda_k: 1 Loss: 0.0014512856814522196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8711 lambda_k: 1 Loss: 0.001451285681452272\n",
      "Iteration: 8712 lambda_k: 1 Loss: 0.00145128568145234\n",
      "Iteration: 8713 lambda_k: 1 Loss: 0.0014512856814524041\n",
      "Iteration: 8714 lambda_k: 1 Loss: 0.0014512856814524486\n",
      "Iteration: 8715 lambda_k: 1 Loss: 0.0014512856814525145\n",
      "Iteration: 8716 lambda_k: 1 Loss: 0.0014512856814525791\n",
      "Iteration: 8717 lambda_k: 1 Loss: 0.001451285681452628\n",
      "Iteration: 8718 lambda_k: 1 Loss: 0.0014512856814526917\n",
      "Iteration: 8719 lambda_k: 1 Loss: 0.0014512856814527472\n",
      "Iteration: 8720 lambda_k: 1 Loss: 0.0014512856814528088\n",
      "Iteration: 8721 lambda_k: 1 Loss: 0.0014512856814528758\n",
      "Iteration: 8722 lambda_k: 1 Loss: 0.0014512856814529217\n",
      "Iteration: 8723 lambda_k: 1 Loss: 0.0014512856814529833\n",
      "Iteration: 8724 lambda_k: 1 Loss: 0.0014512856814530451\n",
      "Iteration: 8725 lambda_k: 1 Loss: 0.0014512856814531002\n",
      "Iteration: 8726 lambda_k: 1 Loss: 0.0014512856814531416\n",
      "Iteration: 8727 lambda_k: 1 Loss: 0.0014512856814532077\n",
      "Iteration: 8728 lambda_k: 1 Loss: 0.0014512856814532513\n",
      "Iteration: 8729 lambda_k: 1 Loss: 0.001451285681453315\n",
      "Iteration: 8730 lambda_k: 1 Loss: 0.0014512856814533563\n",
      "Iteration: 8731 lambda_k: 1 Loss: 0.0014512856814534142\n",
      "Iteration: 8732 lambda_k: 1 Loss: 0.0014512856814534742\n",
      "Iteration: 8733 lambda_k: 1 Loss: 0.001451285681453515\n",
      "Iteration: 8734 lambda_k: 1 Loss: 0.0014512856814535744\n",
      "Iteration: 8735 lambda_k: 1 Loss: 0.0014512856814536377\n",
      "Iteration: 8736 lambda_k: 1 Loss: 0.0014512856814536763\n",
      "Iteration: 8737 lambda_k: 1 Loss: 0.0014512856814537344\n",
      "Iteration: 8738 lambda_k: 1 Loss: 0.001451285681453796\n",
      "Iteration: 8739 lambda_k: 1 Loss: 0.001451285681453835\n",
      "Iteration: 8740 lambda_k: 1 Loss: 0.0014512856814538895\n",
      "Iteration: 8741 lambda_k: 1 Loss: 0.001451285681453946\n",
      "Iteration: 8742 lambda_k: 1 Loss: 0.0014512856814540027\n",
      "Iteration: 8743 lambda_k: 1 Loss: 0.0014512856814540352\n",
      "Iteration: 8744 lambda_k: 1 Loss: 0.001451285681454088\n",
      "Iteration: 8745 lambda_k: 1 Loss: 0.0014512856814541289\n",
      "Iteration: 8746 lambda_k: 1 Loss: 0.0014512856814541768\n",
      "Iteration: 8747 lambda_k: 1 Loss: 0.0014512856814542338\n",
      "Iteration: 8748 lambda_k: 1 Loss: 0.0014512856814542809\n",
      "Iteration: 8749 lambda_k: 1 Loss: 0.0014512856814543355\n",
      "Iteration: 8750 lambda_k: 1 Loss: 0.001451285681454388\n",
      "Iteration: 8751 lambda_k: 1 Loss: 0.0014512856814544392\n",
      "Iteration: 8752 lambda_k: 1 Loss: 0.0014512856814544956\n",
      "Iteration: 8753 lambda_k: 1 Loss: 0.0014512856814545318\n",
      "Iteration: 8754 lambda_k: 1 Loss: 0.0014512856814545914\n",
      "Iteration: 8755 lambda_k: 1 Loss: 0.0014512856814546296\n",
      "Iteration: 8756 lambda_k: 1 Loss: 0.0014512856814546862\n",
      "Iteration: 8757 lambda_k: 1 Loss: 0.0014512856814547287\n",
      "Iteration: 8758 lambda_k: 1 Loss: 0.001451285681454784\n",
      "Iteration: 8759 lambda_k: 1 Loss: 0.0014512856814548247\n",
      "Iteration: 8760 lambda_k: 1 Loss: 0.00145128568145487\n",
      "Iteration: 8761 lambda_k: 1 Loss: 0.0014512856814549145\n",
      "Iteration: 8762 lambda_k: 1 Loss: 0.0014512856814549696\n",
      "Iteration: 8763 lambda_k: 1 Loss: 0.0014512856814550223\n",
      "Iteration: 8764 lambda_k: 1 Loss: 0.0014512856814550546\n",
      "Iteration: 8765 lambda_k: 1 Loss: 0.0014512856814551162\n",
      "Iteration: 8766 lambda_k: 1 Loss: 0.0014512856814551547\n",
      "Iteration: 8767 lambda_k: 1 Loss: 0.0014512856814551684\n",
      "Iteration: 8768 lambda_k: 1 Loss: 0.0014512856814551942\n",
      "Iteration: 8769 lambda_k: 1 Loss: 0.0014512856814552534\n",
      "Iteration: 8770 lambda_k: 1 Loss: 0.0014512856814552894\n",
      "Iteration: 8771 lambda_k: 1 Loss: 0.0014512856814553445\n",
      "Iteration: 8772 lambda_k: 1 Loss: 0.0014512856814553853\n",
      "Iteration: 8773 lambda_k: 1 Loss: 0.001451285681455445\n",
      "Iteration: 8774 lambda_k: 1 Loss: 0.0014512856814554828\n",
      "Iteration: 8775 lambda_k: 1 Loss: 0.0014512856814555381\n",
      "Iteration: 8776 lambda_k: 1 Loss: 0.0014512856814555704\n",
      "Iteration: 8777 lambda_k: 1 Loss: 0.0014512856814556244\n",
      "Iteration: 8778 lambda_k: 1 Loss: 0.0014512856814556606\n",
      "Iteration: 8779 lambda_k: 1 Loss: 0.0014512856814557164\n",
      "Iteration: 8780 lambda_k: 1 Loss: 0.0014512856814557537\n",
      "Iteration: 8781 lambda_k: 1 Loss: 0.001451285681455801\n",
      "Iteration: 8782 lambda_k: 1 Loss: 0.0014512856814558504\n",
      "Iteration: 8783 lambda_k: 1 Loss: 0.001451285681455888\n",
      "Iteration: 8784 lambda_k: 1 Loss: 0.0014512856814559343\n",
      "Iteration: 8785 lambda_k: 1 Loss: 0.0014512856814559796\n",
      "Iteration: 8786 lambda_k: 1 Loss: 0.0014512856814560126\n",
      "Iteration: 8787 lambda_k: 1 Loss: 0.001451285681456067\n",
      "Iteration: 8788 lambda_k: 1 Loss: 0.0014512856814560989\n",
      "Iteration: 8789 lambda_k: 1 Loss: 0.0014512856814561496\n",
      "Iteration: 8790 lambda_k: 1 Loss: 0.001451285681456186\n",
      "Iteration: 8791 lambda_k: 1 Loss: 0.0014512856814562311\n",
      "Iteration: 8792 lambda_k: 1 Loss: 0.0014512856814562869\n",
      "Iteration: 8793 lambda_k: 1 Loss: 0.0014512856814563218\n",
      "Iteration: 8794 lambda_k: 1 Loss: 0.0014512856814563704\n",
      "Iteration: 8795 lambda_k: 1 Loss: 0.0014512856814564163\n",
      "Iteration: 8796 lambda_k: 1 Loss: 0.0014512856814564638\n",
      "Iteration: 8797 lambda_k: 1 Loss: 0.0014512856814564946\n",
      "Iteration: 8798 lambda_k: 1 Loss: 0.0014512856814565404\n",
      "Iteration: 8799 lambda_k: 1 Loss: 0.001451285681456582\n",
      "Iteration: 8800 lambda_k: 1 Loss: 0.0014512856814566154\n",
      "Iteration: 8801 lambda_k: 1 Loss: 0.0014512856814566687\n",
      "Iteration: 8802 lambda_k: 1 Loss: 0.0014512856814567123\n",
      "Iteration: 8803 lambda_k: 1 Loss: 0.0014512856814567418\n",
      "Iteration: 8804 lambda_k: 1 Loss: 0.001451285681456786\n",
      "Iteration: 8805 lambda_k: 1 Loss: 0.001451285681456832\n",
      "Iteration: 8806 lambda_k: 1 Loss: 0.0014512856814568624\n",
      "Iteration: 8807 lambda_k: 1 Loss: 0.001451285681456907\n",
      "Iteration: 8808 lambda_k: 1 Loss: 0.0014512856814569406\n",
      "Iteration: 8809 lambda_k: 1 Loss: 0.001451285681456978\n",
      "Iteration: 8810 lambda_k: 1 Loss: 0.0014512856814570172\n",
      "Iteration: 8811 lambda_k: 1 Loss: 0.0014512856814570577\n",
      "Iteration: 8812 lambda_k: 1 Loss: 0.0014512856814570957\n",
      "Iteration: 8813 lambda_k: 1 Loss: 0.001451285681457143\n",
      "Iteration: 8814 lambda_k: 1 Loss: 0.001451285681457192\n",
      "Iteration: 8815 lambda_k: 1 Loss: 0.0014512856814572202\n",
      "Iteration: 8816 lambda_k: 1 Loss: 0.0014512856814572655\n",
      "Iteration: 8817 lambda_k: 1 Loss: 0.0014512856814572993\n",
      "Iteration: 8818 lambda_k: 1 Loss: 0.0014512856814573427\n",
      "Iteration: 8819 lambda_k: 1 Loss: 0.0014512856814573852\n",
      "Iteration: 8820 lambda_k: 1 Loss: 0.0014512856814574288\n",
      "Iteration: 8821 lambda_k: 1 Loss: 0.001451285681457473\n",
      "Iteration: 8822 lambda_k: 1 Loss: 0.0014512856814574932\n",
      "Iteration: 8823 lambda_k: 1 Loss: 0.0014512856814575378\n",
      "Iteration: 8824 lambda_k: 1 Loss: 0.0014512856814575797\n",
      "Iteration: 8825 lambda_k: 1 Loss: 0.0014512856814576098\n",
      "Iteration: 8826 lambda_k: 1 Loss: 0.0014512856814576501\n",
      "Iteration: 8827 lambda_k: 1 Loss: 0.0014512856814576844\n",
      "Iteration: 8828 lambda_k: 1 Loss: 0.0014512856814577295\n",
      "Iteration: 8829 lambda_k: 1 Loss: 0.0014512856814577594\n",
      "Iteration: 8830 lambda_k: 1 Loss: 0.0014512856814578065\n",
      "Iteration: 8831 lambda_k: 1 Loss: 0.0014512856814578444\n",
      "Iteration: 8832 lambda_k: 1 Loss: 0.0014512856814578711\n",
      "Iteration: 8833 lambda_k: 1 Loss: 0.001451285681457914\n",
      "Iteration: 8834 lambda_k: 1 Loss: 0.0014512856814579455\n",
      "Iteration: 8835 lambda_k: 1 Loss: 0.0014512856814579919\n",
      "Iteration: 8836 lambda_k: 1 Loss: 0.001451285681458033\n",
      "Iteration: 8837 lambda_k: 1 Loss: 0.0014512856814580578\n",
      "Iteration: 8838 lambda_k: 1 Loss: 0.001451285681458094\n",
      "Iteration: 8839 lambda_k: 1 Loss: 0.0014512856814581257\n",
      "Iteration: 8840 lambda_k: 1 Loss: 0.00145128568145817\n",
      "Iteration: 8841 lambda_k: 1 Loss: 0.0014512856814582133\n",
      "Iteration: 8842 lambda_k: 1 Loss: 0.001451285681458255\n",
      "Iteration: 8843 lambda_k: 1 Loss: 0.0014512856814582935\n",
      "Iteration: 8844 lambda_k: 1 Loss: 0.001451285681458317\n",
      "Iteration: 8845 lambda_k: 1 Loss: 0.0014512856814583627\n",
      "Iteration: 8846 lambda_k: 1 Loss: 0.0014512856814584054\n",
      "Iteration: 8847 lambda_k: 1 Loss: 0.0014512856814584262\n",
      "Iteration: 8848 lambda_k: 1 Loss: 0.0014512856814584666\n",
      "Iteration: 8849 lambda_k: 1 Loss: 0.0014512856814585073\n",
      "Iteration: 8850 lambda_k: 1 Loss: 0.0014512856814585297\n",
      "Iteration: 8851 lambda_k: 1 Loss: 0.0014512856814585696\n",
      "Iteration: 8852 lambda_k: 1 Loss: 0.0014512856814585973\n",
      "Iteration: 8853 lambda_k: 1 Loss: 0.0014512856814586437\n",
      "Iteration: 8854 lambda_k: 1 Loss: 0.001451285681458661\n",
      "Iteration: 8855 lambda_k: 1 Loss: 0.0014512856814586986\n",
      "Iteration: 8856 lambda_k: 1 Loss: 0.0014512856814587252\n",
      "Iteration: 8857 lambda_k: 1 Loss: 0.0014512856814587582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8858 lambda_k: 1 Loss: 0.0014512856814587957\n",
      "Iteration: 8859 lambda_k: 1 Loss: 0.001451285681458833\n",
      "Iteration: 8860 lambda_k: 1 Loss: 0.0014512856814588705\n",
      "Iteration: 8861 lambda_k: 1 Loss: 0.0014512856814588913\n",
      "Iteration: 8862 lambda_k: 1 Loss: 0.0014512856814589306\n",
      "Iteration: 8863 lambda_k: 1 Loss: 0.0014512856814589662\n",
      "Iteration: 8864 lambda_k: 1 Loss: 0.0014512856814589898\n",
      "Iteration: 8865 lambda_k: 1 Loss: 0.001451285681459024\n",
      "Iteration: 8866 lambda_k: 1 Loss: 0.0014512856814590624\n",
      "Iteration: 8867 lambda_k: 1 Loss: 0.0014512856814590802\n",
      "Iteration: 8868 lambda_k: 1 Loss: 0.0014512856814591164\n",
      "Iteration: 8869 lambda_k: 1 Loss: 0.001451285681459148\n",
      "Iteration: 8870 lambda_k: 1 Loss: 0.0014512856814591832\n",
      "Iteration: 8871 lambda_k: 1 Loss: 0.0014512856814592181\n",
      "Iteration: 8872 lambda_k: 1 Loss: 0.0014512856814592324\n",
      "Iteration: 8873 lambda_k: 1 Loss: 0.0014512856814592695\n",
      "Iteration: 8874 lambda_k: 1 Loss: 0.0014512856814593105\n",
      "Iteration: 8875 lambda_k: 1 Loss: 0.0014512856814593307\n",
      "Iteration: 8876 lambda_k: 1 Loss: 0.0014512856814593727\n",
      "Iteration: 8877 lambda_k: 1 Loss: 0.0014512856814593925\n",
      "Iteration: 8878 lambda_k: 1 Loss: 0.0014512856814594304\n",
      "Iteration: 8879 lambda_k: 1 Loss: 0.0014512856814594556\n",
      "Iteration: 8880 lambda_k: 1 Loss: 0.0014512856814594933\n",
      "Iteration: 8881 lambda_k: 1 Loss: 0.0014512856814595304\n",
      "Iteration: 8882 lambda_k: 1 Loss: 0.0014512856814595499\n",
      "Iteration: 8883 lambda_k: 1 Loss: 0.0014512856814595798\n",
      "Iteration: 8884 lambda_k: 1 Loss: 0.0014512856814596132\n",
      "Iteration: 8885 lambda_k: 1 Loss: 0.0014512856814596386\n",
      "Iteration: 8886 lambda_k: 1 Loss: 0.0014512856814596757\n",
      "Iteration: 8887 lambda_k: 1 Loss: 0.0014512856814597062\n",
      "Iteration: 8888 lambda_k: 1 Loss: 0.0014512856814597424\n",
      "Iteration: 8889 lambda_k: 1 Loss: 0.0014512856814597574\n",
      "Iteration: 8890 lambda_k: 1 Loss: 0.0014512856814597995\n",
      "Iteration: 8891 lambda_k: 1 Loss: 0.0014512856814598131\n",
      "Iteration: 8892 lambda_k: 1 Loss: 0.0014512856814598548\n",
      "Iteration: 8893 lambda_k: 1 Loss: 0.0014512856814598886\n",
      "Iteration: 8894 lambda_k: 1 Loss: 0.0014512856814599016\n",
      "Iteration: 8895 lambda_k: 1 Loss: 0.001451285681459931\n",
      "Iteration: 8896 lambda_k: 1 Loss: 0.00145128568145996\n",
      "Iteration: 8897 lambda_k: 1 Loss: 0.001451285681459998\n",
      "Iteration: 8898 lambda_k: 1 Loss: 0.0014512856814600146\n",
      "Iteration: 8899 lambda_k: 1 Loss: 0.0014512856814600506\n",
      "Iteration: 8900 lambda_k: 1 Loss: 0.0014512856814600788\n",
      "Iteration: 8901 lambda_k: 1 Loss: 0.0014512856814601113\n",
      "Iteration: 8902 lambda_k: 1 Loss: 0.001451285681460128\n",
      "Iteration: 8903 lambda_k: 1 Loss: 0.0014512856814601664\n",
      "Iteration: 8904 lambda_k: 1 Loss: 0.0014512856814601811\n",
      "Iteration: 8905 lambda_k: 1 Loss: 0.0014512856814602225\n",
      "Iteration: 8906 lambda_k: 1 Loss: 0.0014512856814602561\n",
      "Iteration: 8907 lambda_k: 1 Loss: 0.0014512856814602648\n",
      "Iteration: 8908 lambda_k: 1 Loss: 0.001451285681460303\n",
      "Iteration: 8909 lambda_k: 1 Loss: 0.001451285681460333\n",
      "Iteration: 8910 lambda_k: 1 Loss: 0.001451285681460342\n",
      "Iteration: 8911 lambda_k: 1 Loss: 0.0014512856814603797\n",
      "Iteration: 8912 lambda_k: 1 Loss: 0.0014512856814604088\n",
      "Iteration: 8913 lambda_k: 1 Loss: 0.0014512856814604229\n",
      "Iteration: 8914 lambda_k: 1 Loss: 0.0014512856814604587\n",
      "Iteration: 8915 lambda_k: 1 Loss: 0.0014512856814604712\n",
      "Iteration: 8916 lambda_k: 1 Loss: 0.0014512856814605025\n",
      "Iteration: 8917 lambda_k: 1 Loss: 0.0014512856814605229\n",
      "Iteration: 8918 lambda_k: 1 Loss: 0.0014512856814605532\n",
      "Iteration: 8919 lambda_k: 1 Loss: 0.0014512856814605753\n",
      "Iteration: 8920 lambda_k: 1 Loss: 0.0014512856814606042\n",
      "Iteration: 8921 lambda_k: 1 Loss: 0.0014512856814606445\n",
      "Iteration: 8922 lambda_k: 1 Loss: 0.0014512856814606753\n",
      "Iteration: 8923 lambda_k: 1 Loss: 0.0014512856814606874\n",
      "Iteration: 8924 lambda_k: 1 Loss: 0.0014512856814607204\n",
      "Iteration: 8925 lambda_k: 1 Loss: 0.001451285681460749\n",
      "Iteration: 8926 lambda_k: 1 Loss: 0.0014512856814607592\n",
      "Iteration: 8927 lambda_k: 1 Loss: 0.001451285681460787\n",
      "Iteration: 8928 lambda_k: 1 Loss: 0.0014512856814608015\n",
      "Iteration: 8929 lambda_k: 1 Loss: 0.001451285681460833\n",
      "Iteration: 8930 lambda_k: 1 Loss: 0.001451285681460861\n",
      "Iteration: 8931 lambda_k: 1 Loss: 0.0014512856814608863\n",
      "Iteration: 8932 lambda_k: 1 Loss: 0.0014512856814609147\n",
      "Iteration: 8933 lambda_k: 1 Loss: 0.0014512856814609264\n",
      "Iteration: 8934 lambda_k: 1 Loss: 0.0014512856814609563\n",
      "Iteration: 8935 lambda_k: 1 Loss: 0.0014512856814609836\n",
      "Iteration: 8936 lambda_k: 1 Loss: 0.001451285681461013\n",
      "Iteration: 8937 lambda_k: 1 Loss: 0.0014512856814610272\n",
      "Iteration: 8938 lambda_k: 1 Loss: 0.0014512856814610548\n",
      "Iteration: 8939 lambda_k: 1 Loss: 0.0014512856814610814\n",
      "Iteration: 8940 lambda_k: 1 Loss: 0.0014512856814610962\n",
      "Iteration: 8941 lambda_k: 1 Loss: 0.0014512856814611337\n",
      "Iteration: 8942 lambda_k: 1 Loss: 0.0014512856814611617\n",
      "Iteration: 8943 lambda_k: 1 Loss: 0.0014512856814611747\n",
      "Iteration: 8944 lambda_k: 1 Loss: 0.001451285681461205\n",
      "Iteration: 8945 lambda_k: 1 Loss: 0.0014512856814612332\n",
      "Iteration: 8946 lambda_k: 1 Loss: 0.0014512856814612432\n",
      "Iteration: 8947 lambda_k: 1 Loss: 0.0014512856814612779\n",
      "Iteration: 8948 lambda_k: 1 Loss: 0.0014512856814613022\n",
      "Iteration: 8949 lambda_k: 1 Loss: 0.0014512856814613104\n",
      "Iteration: 8950 lambda_k: 1 Loss: 0.0014512856814613453\n",
      "Iteration: 8951 lambda_k: 1 Loss: 0.0014512856814613581\n",
      "Iteration: 8952 lambda_k: 1 Loss: 0.0014512856814613896\n",
      "Iteration: 8953 lambda_k: 1 Loss: 0.0014512856814614045\n",
      "Iteration: 8954 lambda_k: 1 Loss: 0.0014512856814614349\n",
      "Iteration: 8955 lambda_k: 1 Loss: 0.001451285681461459\n",
      "Iteration: 8956 lambda_k: 1 Loss: 0.0014512856814614713\n",
      "Iteration: 8957 lambda_k: 1 Loss: 0.0014512856814614975\n",
      "Iteration: 8958 lambda_k: 1 Loss: 0.0014512856814615134\n",
      "Iteration: 8959 lambda_k: 1 Loss: 0.001451285681461545\n",
      "Iteration: 8960 lambda_k: 1 Loss: 0.0014512856814615663\n",
      "Iteration: 8961 lambda_k: 1 Loss: 0.0014512856814615938\n",
      "Iteration: 8962 lambda_k: 1 Loss: 0.0014512856814616032\n",
      "Iteration: 8963 lambda_k: 1 Loss: 0.0014512856814616264\n",
      "Iteration: 8964 lambda_k: 1 Loss: 0.0014512856814616378\n",
      "Iteration: 8965 lambda_k: 1 Loss: 0.0014512856814616734\n",
      "Iteration: 8966 lambda_k: 1 Loss: 0.001451285681461697\n",
      "Iteration: 8967 lambda_k: 1 Loss: 0.0014512856814617057\n",
      "Iteration: 8968 lambda_k: 1 Loss: 0.0014512856814617404\n",
      "Iteration: 8969 lambda_k: 1 Loss: 0.0014512856814617504\n",
      "Iteration: 8970 lambda_k: 1 Loss: 0.001451285681461777\n",
      "Iteration: 8971 lambda_k: 1 Loss: 0.001451285681461787\n",
      "Iteration: 8972 lambda_k: 1 Loss: 0.0014512856814618113\n",
      "Iteration: 8973 lambda_k: 1 Loss: 0.001451285681461824\n",
      "Iteration: 8974 lambda_k: 1 Loss: 0.0014512856814618423\n",
      "Iteration: 8975 lambda_k: 1 Loss: 0.0014512856814618666\n",
      "Iteration: 8976 lambda_k: 1 Loss: 0.001451285681461888\n",
      "Iteration: 8977 lambda_k: 1 Loss: 0.001451285681461908\n",
      "Iteration: 8978 lambda_k: 1 Loss: 0.0014512856814619362\n",
      "Iteration: 8979 lambda_k: 1 Loss: 0.001451285681461949\n",
      "Iteration: 8980 lambda_k: 1 Loss: 0.0014512856814619796\n",
      "Iteration: 8981 lambda_k: 1 Loss: 0.0014512856814619876\n",
      "Iteration: 8982 lambda_k: 1 Loss: 0.001451285681462018\n",
      "Iteration: 8983 lambda_k: 1 Loss: 0.0014512856814620314\n",
      "Iteration: 8984 lambda_k: 1 Loss: 0.001451285681462055\n",
      "Iteration: 8985 lambda_k: 1 Loss: 0.0014512856814620854\n",
      "Iteration: 8986 lambda_k: 1 Loss: 0.0014512856814621084\n",
      "Iteration: 8987 lambda_k: 1 Loss: 0.001451285681462114\n",
      "Iteration: 8988 lambda_k: 1 Loss: 0.0014512856814621405\n",
      "Iteration: 8989 lambda_k: 1 Loss: 0.0014512856814621626\n",
      "Iteration: 8990 lambda_k: 1 Loss: 0.001451285681462173\n",
      "Iteration: 8991 lambda_k: 1 Loss: 0.0014512856814622008\n",
      "Iteration: 8992 lambda_k: 1 Loss: 0.0014512856814622224\n",
      "Iteration: 8993 lambda_k: 1 Loss: 0.001451285681462248\n",
      "Iteration: 8994 lambda_k: 1 Loss: 0.0014512856814622697\n",
      "Iteration: 8995 lambda_k: 1 Loss: 0.0014512856814622738\n",
      "Iteration: 8996 lambda_k: 1 Loss: 0.0014512856814623079\n",
      "Iteration: 8997 lambda_k: 1 Loss: 0.0014512856814623137\n",
      "Iteration: 8998 lambda_k: 1 Loss: 0.00145128568146234\n",
      "Iteration: 8999 lambda_k: 1 Loss: 0.0014512856814623508\n",
      "Iteration: 9000 lambda_k: 1 Loss: 0.0014512856814623792\n",
      "Iteration: 9001 lambda_k: 1 Loss: 0.0014512856814624005\n",
      "Iteration: 9002 lambda_k: 1 Loss: 0.0014512856814624124\n",
      "Iteration: 9003 lambda_k: 1 Loss: 0.001451285681462434\n",
      "Iteration: 9004 lambda_k: 1 Loss: 0.0014512856814624438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9005 lambda_k: 1 Loss: 0.0014512856814624649\n",
      "Iteration: 9006 lambda_k: 1 Loss: 0.0014512856814624766\n",
      "Iteration: 9007 lambda_k: 1 Loss: 0.0014512856814625024\n",
      "Iteration: 9008 lambda_k: 1 Loss: 0.0014512856814625243\n",
      "Iteration: 9009 lambda_k: 1 Loss: 0.0014512856814625488\n",
      "Iteration: 9010 lambda_k: 1 Loss: 0.0014512856814625707\n",
      "Iteration: 9011 lambda_k: 1 Loss: 0.001451285681462576\n",
      "Iteration: 9012 lambda_k: 1 Loss: 0.0014512856814625993\n",
      "Iteration: 9013 lambda_k: 1 Loss: 0.001451285681462619\n",
      "Iteration: 9014 lambda_k: 1 Loss: 0.0014512856814626383\n",
      "Iteration: 9015 lambda_k: 1 Loss: 0.0014512856814626457\n",
      "Iteration: 9016 lambda_k: 1 Loss: 0.0014512856814626696\n",
      "Iteration: 9017 lambda_k: 1 Loss: 0.0014512856814626915\n",
      "Iteration: 9018 lambda_k: 1 Loss: 0.0014512856814626965\n",
      "Iteration: 9019 lambda_k: 1 Loss: 0.0014512856814627173\n",
      "Iteration: 9020 lambda_k: 1 Loss: 0.0014512856814627281\n",
      "Iteration: 9021 lambda_k: 1 Loss: 0.0014512856814627466\n",
      "Iteration: 9022 lambda_k: 1 Loss: 0.0014512856814627648\n",
      "Iteration: 9023 lambda_k: 1 Loss: 0.0014512856814627843\n",
      "Iteration: 9024 lambda_k: 1 Loss: 0.001451285681462804\n",
      "Iteration: 9025 lambda_k: 1 Loss: 0.001451285681462823\n",
      "Iteration: 9026 lambda_k: 1 Loss: 0.0014512856814628407\n",
      "Iteration: 9027 lambda_k: 1 Loss: 0.0014512856814628558\n",
      "Iteration: 9028 lambda_k: 1 Loss: 0.001451285681462875\n",
      "Iteration: 9029 lambda_k: 1 Loss: 0.001451285681462898\n",
      "Iteration: 9030 lambda_k: 1 Loss: 0.0014512856814629226\n",
      "Iteration: 9031 lambda_k: 1 Loss: 0.0014512856814629315\n",
      "Iteration: 9032 lambda_k: 1 Loss: 0.0014512856814629543\n",
      "Iteration: 9033 lambda_k: 1 Loss: 0.0014512856814629632\n",
      "Iteration: 9034 lambda_k: 1 Loss: 0.0014512856814629864\n",
      "Iteration: 9035 lambda_k: 1 Loss: 0.0014512856814630096\n",
      "Iteration: 9036 lambda_k: 1 Loss: 0.0014512856814630176\n",
      "Iteration: 9037 lambda_k: 1 Loss: 0.0014512856814630365\n",
      "Iteration: 9038 lambda_k: 1 Loss: 0.001451285681463063\n",
      "Iteration: 9039 lambda_k: 1 Loss: 0.001451285681463065\n",
      "Iteration: 9040 lambda_k: 1 Loss: 0.0014512856814630922\n",
      "Iteration: 9041 lambda_k: 1 Loss: 0.001451285681463095\n",
      "Iteration: 9042 lambda_k: 1 Loss: 0.0014512856814631239\n",
      "Iteration: 9043 lambda_k: 1 Loss: 0.0014512856814631273\n",
      "Iteration: 9044 lambda_k: 1 Loss: 0.0014512856814631572\n",
      "Iteration: 9045 lambda_k: 1 Loss: 0.0014512856814631807\n",
      "Iteration: 9046 lambda_k: 1 Loss: 0.0014512856814631848\n",
      "Iteration: 9047 lambda_k: 1 Loss: 0.001451285681463212\n",
      "Iteration: 9048 lambda_k: 1 Loss: 0.0014512856814632323\n",
      "Iteration: 9049 lambda_k: 1 Loss: 0.0014512856814632505\n",
      "Iteration: 9050 lambda_k: 1 Loss: 0.001451285681463254\n",
      "Iteration: 9051 lambda_k: 1 Loss: 0.0014512856814632743\n",
      "Iteration: 9052 lambda_k: 1 Loss: 0.0014512856814633006\n",
      "Iteration: 9053 lambda_k: 1 Loss: 0.0014512856814632984\n",
      "Iteration: 9054 lambda_k: 1 Loss: 0.0014512856814633184\n",
      "Iteration: 9055 lambda_k: 1 Loss: 0.00145128568146333\n",
      "Iteration: 9056 lambda_k: 1 Loss: 0.0014512856814633483\n",
      "Iteration: 9057 lambda_k: 1 Loss: 0.001451285681463373\n",
      "Iteration: 9058 lambda_k: 1 Loss: 0.001451285681463383\n",
      "Iteration: 9059 lambda_k: 1 Loss: 0.0014512856814634081\n",
      "Iteration: 9060 lambda_k: 1 Loss: 0.001451285681463435\n",
      "Iteration: 9061 lambda_k: 1 Loss: 0.0014512856814634385\n",
      "Iteration: 9062 lambda_k: 1 Loss: 0.0014512856814634652\n",
      "Iteration: 9063 lambda_k: 1 Loss: 0.0014512856814634706\n",
      "Iteration: 9064 lambda_k: 1 Loss: 0.0014512856814634918\n",
      "Iteration: 9065 lambda_k: 1 Loss: 0.001451285681463499\n",
      "Iteration: 9066 lambda_k: 1 Loss: 0.0014512856814635224\n",
      "Iteration: 9067 lambda_k: 1 Loss: 0.001451285681463544\n",
      "Iteration: 9068 lambda_k: 1 Loss: 0.001451285681463563\n",
      "Iteration: 9069 lambda_k: 1 Loss: 0.0014512856814635656\n",
      "Iteration: 9070 lambda_k: 1 Loss: 0.001451285681463594\n",
      "Iteration: 9071 lambda_k: 1 Loss: 0.0014512856814636083\n",
      "Iteration: 9072 lambda_k: 1 Loss: 0.0014512856814636065\n",
      "Iteration: 9073 lambda_k: 1 Loss: 0.0014512856814636265\n",
      "Iteration: 9074 lambda_k: 1 Loss: 0.0014512856814636343\n",
      "Iteration: 9075 lambda_k: 1 Loss: 0.0014512856814636484\n",
      "Iteration: 9076 lambda_k: 1 Loss: 0.001451285681463671\n",
      "Iteration: 9077 lambda_k: 1 Loss: 0.0014512856814636905\n",
      "Iteration: 9078 lambda_k: 1 Loss: 0.001451285681463695\n",
      "Iteration: 9079 lambda_k: 1 Loss: 0.0014512856814637234\n",
      "Iteration: 9080 lambda_k: 1 Loss: 0.0014512856814637373\n",
      "Iteration: 9081 lambda_k: 1 Loss: 0.0014512856814637358\n",
      "Iteration: 9082 lambda_k: 1 Loss: 0.001451285681463761\n",
      "Iteration: 9083 lambda_k: 1 Loss: 0.0014512856814637618\n",
      "Iteration: 9084 lambda_k: 1 Loss: 0.0014512856814637856\n",
      "Iteration: 9085 lambda_k: 1 Loss: 0.0014512856814638013\n",
      "Iteration: 9086 lambda_k: 1 Loss: 0.0014512856814637974\n",
      "Iteration: 9087 lambda_k: 1 Loss: 0.0014512856814638173\n",
      "Iteration: 9088 lambda_k: 1 Loss: 0.0014512856814638349\n",
      "Iteration: 9089 lambda_k: 1 Loss: 0.0014512856814638355\n",
      "Iteration: 9090 lambda_k: 1 Loss: 0.001451285681463859\n",
      "Iteration: 9091 lambda_k: 1 Loss: 0.0014512856814638663\n",
      "Iteration: 9092 lambda_k: 1 Loss: 0.0014512856814638882\n",
      "Iteration: 9093 lambda_k: 1 Loss: 0.00145128568146389\n",
      "Iteration: 9094 lambda_k: 1 Loss: 0.0014512856814639092\n",
      "Iteration: 9095 lambda_k: 1 Loss: 0.001451285681463924\n",
      "Iteration: 9096 lambda_k: 1 Loss: 0.0014512856814639218\n",
      "Iteration: 9097 lambda_k: 1 Loss: 0.0014512856814639485\n",
      "Iteration: 9098 lambda_k: 1 Loss: 0.001451285681463945\n",
      "Iteration: 9099 lambda_k: 1 Loss: 0.001451285681463968\n",
      "Iteration: 9100 lambda_k: 1 Loss: 0.0014512856814639832\n",
      "Iteration: 9101 lambda_k: 1 Loss: 0.0014512856814639797\n",
      "Iteration: 9102 lambda_k: 1 Loss: 0.0014512856814640077\n",
      "Iteration: 9103 lambda_k: 1 Loss: 0.0014512856814640116\n",
      "Iteration: 9104 lambda_k: 1 Loss: 0.0014512856814640313\n",
      "Iteration: 9105 lambda_k: 1 Loss: 0.0014512856814640348\n",
      "Iteration: 9106 lambda_k: 1 Loss: 0.0014512856814640571\n",
      "Iteration: 9107 lambda_k: 1 Loss: 0.0014512856814640671\n",
      "Iteration: 9108 lambda_k: 1 Loss: 0.0014512856814640808\n",
      "Iteration: 9109 lambda_k: 1 Loss: 0.0014512856814641035\n",
      "Iteration: 9110 lambda_k: 1 Loss: 0.0014512856814641057\n",
      "Iteration: 9111 lambda_k: 1 Loss: 0.0014512856814641267\n",
      "Iteration: 9112 lambda_k: 1 Loss: 0.0014512856814641393\n",
      "Iteration: 9113 lambda_k: 1 Loss: 0.0014512856814641398\n",
      "Iteration: 9114 lambda_k: 1 Loss: 0.0014512856814641608\n",
      "Iteration: 9115 lambda_k: 1 Loss: 0.0014512856814641632\n",
      "Iteration: 9116 lambda_k: 1 Loss: 0.0014512856814641849\n",
      "Iteration: 9117 lambda_k: 1 Loss: 0.0014512856814642015\n",
      "Iteration: 9118 lambda_k: 1 Loss: 0.0014512856814641976\n",
      "Iteration: 9119 lambda_k: 1 Loss: 0.0014512856814642124\n",
      "Iteration: 9120 lambda_k: 1 Loss: 0.0014512856814642339\n",
      "Iteration: 9121 lambda_k: 1 Loss: 0.0014512856814642343\n",
      "Iteration: 9122 lambda_k: 1 Loss: 0.0014512856814642501\n",
      "Iteration: 9123 lambda_k: 1 Loss: 0.0014512856814642506\n",
      "Iteration: 9124 lambda_k: 1 Loss: 0.0014512856814642716\n",
      "Iteration: 9125 lambda_k: 1 Loss: 0.001451285681464289\n",
      "Iteration: 9126 lambda_k: 1 Loss: 0.0014512856814642883\n",
      "Iteration: 9127 lambda_k: 1 Loss: 0.0014512856814643032\n",
      "Iteration: 9128 lambda_k: 1 Loss: 0.0014512856814643202\n",
      "Iteration: 9129 lambda_k: 1 Loss: 0.0014512856814643184\n",
      "Iteration: 9130 lambda_k: 1 Loss: 0.001451285681464336\n",
      "Iteration: 9131 lambda_k: 1 Loss: 0.0014512856814643418\n",
      "Iteration: 9132 lambda_k: 1 Loss: 0.00145128568146436\n",
      "Iteration: 9133 lambda_k: 1 Loss: 0.0014512856814643627\n",
      "Iteration: 9134 lambda_k: 1 Loss: 0.0014512856814643772\n",
      "Iteration: 9135 lambda_k: 1 Loss: 0.0014512856814643785\n",
      "Iteration: 9136 lambda_k: 1 Loss: 0.0014512856814643984\n",
      "Iteration: 9137 lambda_k: 1 Loss: 0.0014512856814644175\n",
      "Iteration: 9138 lambda_k: 1 Loss: 0.0014512856814644119\n",
      "Iteration: 9139 lambda_k: 1 Loss: 0.0014512856814644258\n",
      "Iteration: 9140 lambda_k: 1 Loss: 0.0014512856814644483\n",
      "Iteration: 9141 lambda_k: 1 Loss: 0.0014512856814644433\n",
      "Iteration: 9142 lambda_k: 1 Loss: 0.0014512856814644572\n",
      "Iteration: 9143 lambda_k: 1 Loss: 0.001451285681464463\n",
      "Iteration: 9144 lambda_k: 1 Loss: 0.0014512856814644826\n",
      "Iteration: 9145 lambda_k: 1 Loss: 0.0014512856814644852\n",
      "Iteration: 9146 lambda_k: 1 Loss: 0.001451285681464509\n",
      "Iteration: 9147 lambda_k: 1 Loss: 0.0014512856814645114\n",
      "Iteration: 9148 lambda_k: 1 Loss: 0.0014512856814645281\n",
      "Iteration: 9149 lambda_k: 1 Loss: 0.0014512856814645426\n",
      "Iteration: 9150 lambda_k: 1 Loss: 0.0014512856814645565\n",
      "Iteration: 9151 lambda_k: 1 Loss: 0.0014512856814645509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9152 lambda_k: 1 Loss: 0.0014512856814645648\n",
      "Iteration: 9153 lambda_k: 1 Loss: 0.0014512856814645815\n",
      "Iteration: 9154 lambda_k: 1 Loss: 0.0014512856814645758\n",
      "Iteration: 9155 lambda_k: 1 Loss: 0.0014512856814645888\n",
      "Iteration: 9156 lambda_k: 1 Loss: 0.0014512856814646075\n",
      "Iteration: 9157 lambda_k: 1 Loss: 0.001451285681464605\n",
      "Iteration: 9158 lambda_k: 1 Loss: 0.0014512856814646205\n",
      "Iteration: 9159 lambda_k: 1 Loss: 0.0014512856814646344\n",
      "Iteration: 9160 lambda_k: 1 Loss: 0.0014512856814646272\n",
      "Iteration: 9161 lambda_k: 1 Loss: 0.0014512856814646344\n",
      "Iteration: 9162 lambda_k: 1 Loss: 0.0014512856814646524\n",
      "Iteration: 9163 lambda_k: 1 Loss: 0.0014512856814646482\n",
      "Iteration: 9164 lambda_k: 1 Loss: 0.0014512856814646656\n",
      "Iteration: 9165 lambda_k: 1 Loss: 0.0014512856814646667\n",
      "Iteration: 9166 lambda_k: 1 Loss: 0.0014512856814646892\n",
      "Iteration: 9167 lambda_k: 1 Loss: 0.0014512856814646886\n",
      "Iteration: 9168 lambda_k: 1 Loss: 0.0014512856814647094\n",
      "Iteration: 9169 lambda_k: 1 Loss: 0.0014512856814647083\n",
      "Iteration: 9170 lambda_k: 1 Loss: 0.0014512856814647298\n",
      "Iteration: 9171 lambda_k: 1 Loss: 0.0014512856814647259\n",
      "Iteration: 9172 lambda_k: 1 Loss: 0.0014512856814647376\n",
      "Iteration: 9173 lambda_k: 1 Loss: 0.001451285681464741\n",
      "Iteration: 9174 lambda_k: 1 Loss: 0.0014512856814647556\n",
      "Iteration: 9175 lambda_k: 1 Loss: 0.0014512856814647582\n",
      "Iteration: 9176 lambda_k: 1 Loss: 0.0014512856814647669\n",
      "Iteration: 9177 lambda_k: 1 Loss: 0.0014512856814647734\n",
      "Iteration: 9178 lambda_k: 1 Loss: 0.0014512856814647818\n",
      "Iteration: 9179 lambda_k: 1 Loss: 0.0014512856814647892\n",
      "Iteration: 9180 lambda_k: 1 Loss: 0.001451285681464794\n",
      "Iteration: 9181 lambda_k: 1 Loss: 0.0014512856814647968\n",
      "Iteration: 9182 lambda_k: 1 Loss: 0.0014512856814648033\n",
      "Iteration: 9183 lambda_k: 1 Loss: 0.0014512856814648091\n",
      "Iteration: 9184 lambda_k: 1 Loss: 0.0014512856814648148\n",
      "Iteration: 9185 lambda_k: 1 Loss: 0.0014512856814648239\n",
      "Iteration: 9186 lambda_k: 1 Loss: 0.0014512856814648326\n",
      "Iteration: 9187 lambda_k: 1 Loss: 0.0014512856814648373\n",
      "Iteration: 9188 lambda_k: 1 Loss: 0.0014512856814648456\n",
      "Iteration: 9189 lambda_k: 1 Loss: 0.0014512856814648542\n",
      "Iteration: 9190 lambda_k: 1 Loss: 0.0014512856814648623\n",
      "Iteration: 9191 lambda_k: 1 Loss: 0.0014512856814648664\n",
      "Iteration: 9192 lambda_k: 1 Loss: 0.0014512856814648727\n",
      "Iteration: 9193 lambda_k: 1 Loss: 0.001451285681464879\n",
      "Iteration: 9194 lambda_k: 1 Loss: 0.001451285681464884\n",
      "Iteration: 9195 lambda_k: 1 Loss: 0.0014512856814648922\n",
      "Iteration: 9196 lambda_k: 1 Loss: 0.0014512856814648993\n",
      "Iteration: 9197 lambda_k: 1 Loss: 0.0014512856814649089\n",
      "Iteration: 9198 lambda_k: 1 Loss: 0.001451285681464911\n",
      "Iteration: 9199 lambda_k: 1 Loss: 0.0014512856814649165\n",
      "Iteration: 9200 lambda_k: 1 Loss: 0.001451285681464924\n",
      "Iteration: 9201 lambda_k: 1 Loss: 0.0014512856814649297\n",
      "Iteration: 9202 lambda_k: 1 Loss: 0.001451285681464934\n",
      "Iteration: 9203 lambda_k: 1 Loss: 0.0014512856814649416\n",
      "Iteration: 9204 lambda_k: 1 Loss: 0.0014512856814649492\n",
      "Iteration: 9205 lambda_k: 1 Loss: 0.0014512856814649568\n",
      "Iteration: 9206 lambda_k: 1 Loss: 0.0014512856814649657\n",
      "Iteration: 9207 lambda_k: 1 Loss: 0.0014512856814649735\n",
      "Iteration: 9208 lambda_k: 1 Loss: 0.0014512856814649809\n",
      "Iteration: 9209 lambda_k: 1 Loss: 0.001451285681464989\n",
      "Iteration: 9210 lambda_k: 1 Loss: 0.001451285681464996\n",
      "Iteration: 9211 lambda_k: 1 Loss: 0.001451285681465004\n",
      "Iteration: 9212 lambda_k: 1 Loss: 0.0014512856814650182\n",
      "Iteration: 9213 lambda_k: 1 Loss: 0.0014512856814650201\n",
      "Iteration: 9214 lambda_k: 1 Loss: 0.0014512856814650266\n",
      "Iteration: 9215 lambda_k: 1 Loss: 0.0014512856814650344\n",
      "Iteration: 9216 lambda_k: 1 Loss: 0.0014512856814650422\n",
      "Iteration: 9217 lambda_k: 1 Loss: 0.0014512856814650494\n",
      "Iteration: 9218 lambda_k: 1 Loss: 0.001451285681465056\n",
      "Iteration: 9219 lambda_k: 1 Loss: 0.0014512856814650635\n",
      "Iteration: 9220 lambda_k: 1 Loss: 0.0014512856814650706\n",
      "Iteration: 9221 lambda_k: 1 Loss: 0.0014512856814650782\n",
      "Iteration: 9222 lambda_k: 1 Loss: 0.0014512856814650858\n",
      "Iteration: 9223 lambda_k: 1 Loss: 0.0014512856814650923\n",
      "Iteration: 9224 lambda_k: 1 Loss: 0.0014512856814651001\n",
      "Iteration: 9225 lambda_k: 1 Loss: 0.0014512856814651084\n",
      "Iteration: 9226 lambda_k: 1 Loss: 0.0014512856814651164\n",
      "Iteration: 9227 lambda_k: 1 Loss: 0.0014512856814651181\n",
      "Iteration: 9228 lambda_k: 1 Loss: 0.0014512856814651244\n",
      "Iteration: 9229 lambda_k: 1 Loss: 0.001451285681465131\n",
      "Iteration: 9230 lambda_k: 1 Loss: 0.0014512856814651363\n",
      "Iteration: 9231 lambda_k: 1 Loss: 0.0014512856814651416\n",
      "Iteration: 9232 lambda_k: 1 Loss: 0.0014512856814651491\n",
      "Iteration: 9233 lambda_k: 1 Loss: 0.0014512856814651535\n",
      "Iteration: 9234 lambda_k: 1 Loss: 0.0014512856814651626\n",
      "Iteration: 9235 lambda_k: 1 Loss: 0.0014512856814651708\n",
      "Iteration: 9236 lambda_k: 1 Loss: 0.0014512856814651743\n",
      "Iteration: 9237 lambda_k: 1 Loss: 0.0014512856814651819\n",
      "Iteration: 9238 lambda_k: 1 Loss: 0.0014512856814651884\n",
      "Iteration: 9239 lambda_k: 1 Loss: 0.0014512856814651936\n",
      "Iteration: 9240 lambda_k: 1 Loss: 0.0014512856814652014\n",
      "Iteration: 9241 lambda_k: 1 Loss: 0.0014512856814652094\n",
      "Iteration: 9242 lambda_k: 1 Loss: 0.0014512856814652172\n",
      "Iteration: 9243 lambda_k: 1 Loss: 0.001451285681465223\n",
      "Iteration: 9244 lambda_k: 1 Loss: 0.0014512856814652309\n",
      "Iteration: 9245 lambda_k: 1 Loss: 0.001451285681465239\n",
      "Iteration: 9246 lambda_k: 1 Loss: 0.0014512856814652456\n",
      "Iteration: 9247 lambda_k: 1 Loss: 0.0014512856814652534\n",
      "Iteration: 9248 lambda_k: 1 Loss: 0.001451285681465261\n",
      "Iteration: 9249 lambda_k: 1 Loss: 0.0014512856814652682\n",
      "Iteration: 9250 lambda_k: 1 Loss: 0.0014512856814652751\n",
      "Iteration: 9251 lambda_k: 1 Loss: 0.0014512856814652886\n",
      "Iteration: 9252 lambda_k: 1 Loss: 0.0014512856814653066\n",
      "Iteration: 9253 lambda_k: 1 Loss: 0.0014512856814653014\n",
      "Iteration: 9254 lambda_k: 1 Loss: 0.0014512856814653172\n",
      "Iteration: 9255 lambda_k: 1 Loss: 0.0014512856814653298\n",
      "Iteration: 9256 lambda_k: 1 Loss: 0.0014512856814653252\n",
      "Iteration: 9257 lambda_k: 1 Loss: 0.0014512856814653406\n",
      "Iteration: 9258 lambda_k: 1 Loss: 0.0014512856814653402\n",
      "Iteration: 9259 lambda_k: 1 Loss: 0.001451285681465355\n",
      "Iteration: 9260 lambda_k: 1 Loss: 0.0014512856814653673\n",
      "Iteration: 9261 lambda_k: 1 Loss: 0.0014512856814653625\n",
      "Iteration: 9262 lambda_k: 1 Loss: 0.001451285681465374\n",
      "Iteration: 9263 lambda_k: 1 Loss: 0.00145128568146539\n",
      "Iteration: 9264 lambda_k: 1 Loss: 0.0014512856814653835\n",
      "Iteration: 9265 lambda_k: 1 Loss: 0.0014512856814653983\n",
      "Iteration: 9266 lambda_k: 1 Loss: 0.001451285681465411\n",
      "Iteration: 9267 lambda_k: 1 Loss: 0.0014512856814654057\n",
      "Iteration: 9268 lambda_k: 1 Loss: 0.0014512856814654198\n",
      "Iteration: 9269 lambda_k: 1 Loss: 0.001451285681465432\n",
      "Iteration: 9270 lambda_k: 1 Loss: 0.001451285681465427\n",
      "Iteration: 9271 lambda_k: 1 Loss: 0.00145128568146544\n",
      "Iteration: 9272 lambda_k: 1 Loss: 0.0014512856814654532\n",
      "Iteration: 9273 lambda_k: 1 Loss: 0.001451285681465465\n",
      "Iteration: 9274 lambda_k: 1 Loss: 0.0014512856814654545\n",
      "Iteration: 9275 lambda_k: 1 Loss: 0.0014512856814654707\n",
      "Iteration: 9276 lambda_k: 1 Loss: 0.0014512856814654835\n",
      "Iteration: 9277 lambda_k: 1 Loss: 0.0014512856814654774\n",
      "Iteration: 9278 lambda_k: 1 Loss: 0.0014512856814654874\n",
      "Iteration: 9279 lambda_k: 1 Loss: 0.0014512856814654991\n",
      "Iteration: 9280 lambda_k: 1 Loss: 0.0014512856814654915\n",
      "Iteration: 9281 lambda_k: 1 Loss: 0.0014512856814654993\n",
      "Iteration: 9282 lambda_k: 1 Loss: 0.0014512856814655147\n",
      "Iteration: 9283 lambda_k: 1 Loss: 0.0014512856814655026\n",
      "Iteration: 9284 lambda_k: 1 Loss: 0.0014512856814655084\n",
      "Iteration: 9285 lambda_k: 1 Loss: 0.001451285681465519\n",
      "Iteration: 9286 lambda_k: 1 Loss: 0.0014512856814655082\n",
      "Iteration: 9287 lambda_k: 1 Loss: 0.001451285681465519\n",
      "Iteration: 9288 lambda_k: 1 Loss: 0.0014512856814655126\n",
      "Iteration: 9289 lambda_k: 1 Loss: 0.0014512856814655215\n",
      "Iteration: 9290 lambda_k: 1 Loss: 0.0014512856814655134\n",
      "Iteration: 9291 lambda_k: 1 Loss: 0.001451285681465524\n",
      "Iteration: 9292 lambda_k: 1 Loss: 0.0014512856814655178\n",
      "Iteration: 9293 lambda_k: 1 Loss: 0.0014512856814655351\n",
      "Iteration: 9294 lambda_k: 1 Loss: 0.0014512856814655436\n",
      "Iteration: 9295 lambda_k: 1 Loss: 0.0014512856814655327\n",
      "Iteration: 9296 lambda_k: 1 Loss: 0.001451285681465541\n",
      "Iteration: 9297 lambda_k: 1 Loss: 0.001451285681465551\n",
      "Iteration: 9298 lambda_k: 1 Loss: 0.001451285681465546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9299 lambda_k: 1 Loss: 0.0014512856814655525\n",
      "Iteration: 9300 lambda_k: 1 Loss: 0.0014512856814655666\n",
      "Iteration: 9301 lambda_k: 1 Loss: 0.0014512856814655549\n",
      "Iteration: 9302 lambda_k: 1 Loss: 0.0014512856814655724\n",
      "Iteration: 9303 lambda_k: 1 Loss: 0.0014512856814655759\n",
      "Iteration: 9304 lambda_k: 1 Loss: 0.0014512856814655635\n",
      "Iteration: 9305 lambda_k: 1 Loss: 0.0014512856814655765\n",
      "Iteration: 9306 lambda_k: 1 Loss: 0.0014512856814655748\n",
      "Iteration: 9307 lambda_k: 1 Loss: 0.0014512856814655809\n",
      "Iteration: 9308 lambda_k: 1 Loss: 0.001451285681465596\n",
      "Iteration: 9309 lambda_k: 1 Loss: 0.0014512856814655865\n",
      "Iteration: 9310 lambda_k: 1 Loss: 0.0014512856814655993\n",
      "Iteration: 9311 lambda_k: 1 Loss: 0.0014512856814656058\n",
      "Iteration: 9312 lambda_k: 1 Loss: 0.0014512856814655958\n",
      "Iteration: 9313 lambda_k: 1 Loss: 0.0014512856814656065\n",
      "Iteration: 9314 lambda_k: 1 Loss: 0.0014512856814656114\n",
      "Iteration: 9315 lambda_k: 1 Loss: 0.0014512856814656086\n",
      "Iteration: 9316 lambda_k: 1 Loss: 0.0014512856814656201\n",
      "Iteration: 9317 lambda_k: 1 Loss: 0.0014512856814656201\n",
      "Iteration: 9318 lambda_k: 1 Loss: 0.001451285681465634\n",
      "Iteration: 9319 lambda_k: 1 Loss: 0.0014512856814656264\n",
      "Iteration: 9320 lambda_k: 1 Loss: 0.0014512856814656381\n",
      "Iteration: 9321 lambda_k: 1 Loss: 0.0014512856814656427\n",
      "Iteration: 9322 lambda_k: 1 Loss: 0.001451285681465653\n",
      "Iteration: 9323 lambda_k: 1 Loss: 0.0014512856814656403\n",
      "Iteration: 9324 lambda_k: 1 Loss: 0.00145128568146565\n",
      "Iteration: 9325 lambda_k: 1 Loss: 0.0014512856814656592\n",
      "Iteration: 9326 lambda_k: 1 Loss: 0.0014512856814656464\n",
      "Iteration: 9327 lambda_k: 1 Loss: 0.0014512856814656609\n",
      "Iteration: 9328 lambda_k: 1 Loss: 0.0014512856814656698\n",
      "Iteration: 9329 lambda_k: 1 Loss: 0.0014512856814656602\n",
      "Iteration: 9330 lambda_k: 1 Loss: 0.0014512856814656743\n",
      "Iteration: 9331 lambda_k: 1 Loss: 0.0014512856814656622\n",
      "Iteration: 9332 lambda_k: 1 Loss: 0.0014512856814656735\n",
      "Iteration: 9333 lambda_k: 1 Loss: 0.001451285681465668\n",
      "Iteration: 9334 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9335 lambda_k: 1 Loss: 0.0014512856814656908\n",
      "Iteration: 9336 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9337 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9338 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9339 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9340 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9341 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9342 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9343 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9344 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9345 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9346 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9347 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9348 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9349 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9350 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9351 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9352 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9353 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9354 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9355 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9356 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9357 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9358 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9359 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9360 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9361 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9362 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9363 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9364 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9365 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9366 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9367 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9368 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9369 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9370 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9371 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9372 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9373 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9374 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9375 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9376 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9377 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9378 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9379 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9380 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9381 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9382 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9383 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9384 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9385 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9386 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9387 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9388 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9389 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9390 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9391 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9392 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9393 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9394 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9395 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9396 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9397 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9398 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9399 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9400 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9401 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9402 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9403 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9404 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9405 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9406 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9407 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9408 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9409 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9410 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9411 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9412 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9413 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9414 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9415 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9416 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9417 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9418 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9419 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9420 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9421 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9422 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9423 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9424 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9425 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9426 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9427 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9428 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9429 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9430 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9431 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9432 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9433 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9434 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9435 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9436 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9437 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9438 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9439 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9440 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9441 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9442 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9443 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9444 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9445 lambda_k: 1 Loss: 0.0014512856814656802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9446 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9447 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9448 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9449 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9450 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9451 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9452 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9453 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9454 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9455 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9456 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9457 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9458 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9459 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9460 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9461 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9462 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9463 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9464 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9465 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9466 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9467 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9468 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9469 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9470 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9471 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9472 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9473 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9474 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9475 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9476 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9477 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9478 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9479 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9480 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9481 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9482 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9483 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9484 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9485 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9486 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9487 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9488 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9489 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9490 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9491 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9492 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9493 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9494 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9495 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9496 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9497 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9498 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9499 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9500 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9501 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9502 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9503 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9504 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9505 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9506 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9507 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9508 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9509 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9510 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9511 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9512 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9513 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9514 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9515 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9516 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9517 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9518 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9519 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9520 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9521 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9522 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9523 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9524 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9525 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9526 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9527 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9528 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9529 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9530 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9531 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9532 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9533 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9534 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9535 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9536 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9537 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9538 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9539 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9540 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9541 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9542 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9543 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9544 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9545 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9546 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9547 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9548 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9549 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9550 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9551 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9552 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9553 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9554 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9555 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9556 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9557 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9558 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9559 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9560 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9561 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9562 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9563 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9564 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9565 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9566 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9567 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9568 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9569 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9570 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9571 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9572 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9573 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9574 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9575 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9576 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9577 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9578 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9579 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9580 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9581 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9582 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9583 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9584 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9585 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9586 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9587 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9588 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9589 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9590 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9591 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9592 lambda_k: 1 Loss: 0.001451285681465686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9593 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9594 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9595 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9596 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9597 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9598 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9599 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9600 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9601 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9602 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9603 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9604 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9605 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9606 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9607 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9608 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9609 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9610 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9611 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9612 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9613 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9614 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9615 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9616 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9617 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9618 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9619 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9620 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9621 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9622 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9623 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9624 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9625 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9626 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9627 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9628 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9629 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9630 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9631 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9632 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9633 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9634 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9635 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9636 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9637 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9638 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9639 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9640 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9641 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9642 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9643 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9644 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9645 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9646 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9647 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9648 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9649 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9650 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9651 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9652 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9653 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9654 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9655 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9656 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9657 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9658 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9659 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9660 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9661 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9662 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9663 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9664 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9665 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9666 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9667 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9668 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9669 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9670 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9671 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9672 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9673 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9674 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9675 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9676 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9677 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9678 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9679 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9680 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9681 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9682 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9683 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9684 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9685 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9686 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9687 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9688 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9689 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9690 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9691 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9692 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9693 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9694 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9695 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9696 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9697 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9698 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9699 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9700 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9701 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9702 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9703 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9704 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9705 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9706 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9707 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9708 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9709 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9710 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9711 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9712 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9713 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9714 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9715 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9716 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9717 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9718 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9719 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9720 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9721 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9722 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9723 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9724 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9725 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9726 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9727 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9728 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9729 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9730 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9731 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9732 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9733 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9734 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9735 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9736 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9737 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9738 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9739 lambda_k: 1 Loss: 0.0014512856814656802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9740 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9741 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9742 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9743 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9744 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9745 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9746 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9747 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9748 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9749 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9750 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9751 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9752 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9753 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9754 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9755 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9756 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9757 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9758 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9759 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9760 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9761 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9762 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9763 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9764 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9765 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9766 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9767 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9768 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9769 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9770 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9771 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9772 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9773 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9774 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9775 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9776 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9777 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9778 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9779 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9780 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9781 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9782 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9783 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9784 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9785 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9786 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9787 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9788 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9789 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9790 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9791 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9792 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9793 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9794 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9795 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9796 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9797 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9798 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9799 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9800 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9801 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9802 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9803 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9804 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9805 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9806 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9807 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9808 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9809 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9810 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9811 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9812 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9813 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9814 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9815 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9816 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9817 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9818 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9819 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9820 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9821 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9822 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9823 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9824 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9825 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9826 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9827 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9828 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9829 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9830 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9831 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9832 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9833 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9834 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9835 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9836 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9837 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9838 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9839 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9840 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9841 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9842 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9843 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9844 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9845 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9846 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9847 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9848 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9849 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9850 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9851 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9852 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9853 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9854 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9855 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9856 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9857 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9858 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9859 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9860 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9861 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9862 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9863 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9864 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9865 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9866 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9867 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9868 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9869 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9870 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9871 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9872 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9873 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9874 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9875 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9876 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9877 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9878 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9879 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9880 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9881 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9882 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9883 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9884 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9885 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9886 lambda_k: 1 Loss: 0.0014512856814656815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9887 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9888 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9889 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9890 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9891 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9892 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9893 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9894 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9895 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9896 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9897 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9898 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9899 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9900 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9901 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9902 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9903 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9904 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9905 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9906 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9907 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9908 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9909 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9910 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9911 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9912 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9913 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9914 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9915 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9916 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9917 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9918 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9919 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9920 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9921 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9922 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9923 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9924 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9925 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9926 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9927 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9928 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9929 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9930 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9931 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9932 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9933 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9934 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9935 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9936 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9937 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9938 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9939 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9940 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9941 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9942 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9943 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9944 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9945 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9946 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9947 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9948 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9949 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9950 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9951 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9952 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9953 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9954 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9955 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9956 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9957 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9958 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9959 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9960 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9961 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9962 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9963 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9964 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9965 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9966 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9967 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9968 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9969 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9970 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9971 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9972 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9973 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9974 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9975 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9976 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9977 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9978 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9979 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9980 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9981 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9982 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9983 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9984 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9985 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9986 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9987 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9988 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9989 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9990 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Iteration: 9991 lambda_k: 1 Loss: 0.0014512856814656798\n",
      "Iteration: 9992 lambda_k: 1 Loss: 0.001451285681465686\n",
      "Iteration: 9993 lambda_k: 1 Loss: 0.0014512856814656806\n",
      "Iteration: 9994 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9995 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9996 lambda_k: 1 Loss: 0.0014512856814656815\n",
      "Iteration: 9997 lambda_k: 1 Loss: 0.0014512856814656867\n",
      "Iteration: 9998 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 9999 lambda_k: 1 Loss: 0.0014512856814656802\n",
      "Iteration: 10000 lambda_k: 1 Loss: 0.0014512856814656808\n",
      "Beta: 0.0016802749699833185\n",
      "Gamma: 0.0031218162324379273\n",
      "Lambda_k: 1\n"
     ]
    }
   ],
   "source": [
    "# DY\n",
    "    \n",
    "dy = time.time()\n",
    "DY_list, DY_f_list, DY_z_list, Dual_DY_list, iterations_DY   = DY.Davis_Yin(N, M, frobenius_norm, Grad_Phi, D, (x1,x2,x3), Sigma)\n",
    "fin = time.time()\n",
    "\n",
    "Times[\"DY\"] = fin - dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87f81b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:\n",
      "10000\n",
      "Primal: (x1,x2,x3)\n",
      " ((array([[1186.8],\n",
      "       [43.6],\n",
      "       [24.4]]), array([[186.6, 704.0, 833.3, 1186.8, 714.3, 11.4],\n",
      "       [8.9, 30.9, 43.6, 43.6, 43.5, 11.4],\n",
      "       [4.6, 15.1, 24.4, 19.6, 24.4, 11.4]]), array([[0.0, 0.0, 98.7, 0.0, 717.8, 3965.9]])), 'infactible')\n",
      "Primal: (xf1,xf2,xf3)\n",
      " ((array([[1186.8],\n",
      "       [43.6],\n",
      "       [24.4]]), array([[186.6, 704.0, 833.3, 1186.8, 714.3, 11.4],\n",
      "       [8.9, 30.9, 43.6, 43.6, 43.5, 11.4],\n",
      "       [4.6, 15.1, 24.4, 19.6, 24.4, 11.4]]), array([[0.0, 0.0, 98.7, 0.0, 717.8, 3965.9]])), 'infactible')\n",
      "Primal: (z1,z2,z3)\n",
      " ((array([[1186.8],\n",
      "       [43.5],\n",
      "       [24.1]]), array([[186.6, 704.0, 833.3, 1186.9, 714.3, 11.4],\n",
      "       [8.9, 30.9, 44.0, 43.7, 43.5, 11.4],\n",
      "       [4.6, 15.1, 24.9, 19.6, 25.0, 11.4]]), array([[0.0, 0.0, 98.7, 0.0, 717.8, 3965.9]])), 'infactible')\n",
      "Dual: (Capacity, Equilibrium)\n",
      " (array([[0.0, 0.0, 0.0, 39.9, 0.0, 0.0],\n",
      "       [0.0, 0.0, 139.8, 15.4, 0.0, 0.0],\n",
      "       [0.0, 0.0, 167.1, 0.0, 191.2, 0.0]]), array([[1865.7, 6336.1, 10000.0, 8626.1, 10000.0, 10000.0]]))\n"
     ]
    }
   ],
   "source": [
    "iter_ = -1 # Minimo en 96633\n",
    "print(f\"Iterations:\\n{iterations_DY}\")\n",
    "print(\"Primal: (x1,x2,x3)\\n\",DY_list[iter_])\n",
    "print(\"Primal: (xf1,xf2,xf3)\\n\",DY_f_list[iter_])\n",
    "print(\"Primal: (z1,z2,z3)\\n\",DY_z_list[iter_])\n",
    "print(\"Dual: (Capacity, Equilibrium)\\n\",Dual_DY_list[iter_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "999e17b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta:  0.0016802749699833185\n",
      "gamma: 0.0031709202631556546\n",
      "Iteration: 1 lambda_n: 0.9954164741229811 Loss: 0.5936327346105819\n",
      "Iteration: 2 lambda_n: 0.9814303396086871 Loss: 0.5149427551414544\n",
      "Iteration: 3 lambda_n: 1.0269566995626496 Loss: 0.4781593803207887\n",
      "Iteration: 4 lambda_n: 0.9682333555626561 Loss: 0.4525025159426966\n",
      "Iteration: 5 lambda_n: 1.0011622519448642 Loss: 0.4362423071995406\n",
      "Iteration: 6 lambda_n: 0.9848913499371857 Loss: 0.42455104070545546\n",
      "Iteration: 7 lambda_n: 0.8796698594233883 Loss: 0.41651063133363686\n",
      "Iteration: 8 lambda_n: 1.0151120646595293 Loss: 0.41039462134533444\n",
      "Iteration: 9 lambda_n: 0.9449198016447052 Loss: 0.4048632475527174\n",
      "Iteration: 10 lambda_n: 0.8937882581944954 Loss: 0.400157045837584\n",
      "Iteration: 11 lambda_n: 1.0044576771845004 Loss: 0.3960492493683219\n",
      "Iteration: 12 lambda_n: 0.9066605172962854 Loss: 0.3917624459876942\n",
      "Iteration: 13 lambda_n: 0.9023278938851468 Loss: 0.3881930472818085\n",
      "Iteration: 14 lambda_n: 0.9448027371495705 Loss: 0.3849058814049003\n",
      "Iteration: 15 lambda_n: 0.9788895423443364 Loss: 0.381745144983878\n",
      "Iteration: 16 lambda_n: 1.0233201558368903 Loss: 0.3787235480462993\n",
      "Iteration: 17 lambda_n: 1.004153816339232 Loss: 0.37579802151386815\n",
      "Iteration: 18 lambda_n: 0.9539425275777713 Loss: 0.37313496110320593\n",
      "Iteration: 19 lambda_n: 0.9727454753067405 Loss: 0.3707759803235086\n",
      "Iteration: 20 lambda_n: 1.0044166629172226 Loss: 0.3685196039933208\n",
      "Iteration: 21 lambda_n: 0.9445222793976751 Loss: 0.3663327754731959\n",
      "Iteration: 22 lambda_n: 0.9931787584931737 Loss: 0.3644028079970616\n",
      "Iteration: 23 lambda_n: 0.9738112215523104 Loss: 0.36248772502542326\n",
      "Iteration: 24 lambda_n: 0.9313855139745649 Loss: 0.3607179233866173\n",
      "Iteration: 25 lambda_n: 0.8776677109618325 Loss: 0.3591176253218851\n",
      "Iteration: 26 lambda_n: 0.9591448054067089 Loss: 0.35768578360952064\n",
      "Iteration: 27 lambda_n: 0.924566473259232 Loss: 0.35619325908246846\n",
      "Iteration: 28 lambda_n: 0.8978150221327725 Loss: 0.3548245408711785\n",
      "Iteration: 29 lambda_n: 0.960211843414868 Loss: 0.35355541619028036\n",
      "Iteration: 30 lambda_n: 0.9043792415203116 Loss: 0.3522554356531431\n",
      "Iteration: 31 lambda_n: 0.9370528936393392 Loss: 0.35108408888995335\n",
      "Iteration: 32 lambda_n: 0.9124985101176503 Loss: 0.34991794134653464\n",
      "Iteration: 33 lambda_n: 0.8810787911293553 Loss: 0.3488264559691644\n",
      "Iteration: 34 lambda_n: 0.9632479117676345 Loss: 0.3478106643392719\n",
      "Iteration: 35 lambda_n: 1.0179696673697738 Loss: 0.34673736306886493\n",
      "Iteration: 36 lambda_n: 0.9030775233380443 Loss: 0.3456428688201923\n",
      "Iteration: 37 lambda_n: 0.9840312560492397 Loss: 0.34470605681339933\n",
      "Iteration: 38 lambda_n: 0.9866474825740593 Loss: 0.34371575794878323\n",
      "Iteration: 39 lambda_n: 0.8802150899704101 Loss: 0.3427536893210299\n",
      "Iteration: 40 lambda_n: 0.9654540454049635 Loss: 0.34193074491277947\n",
      "Iteration: 41 lambda_n: 0.9831970247067375 Loss: 0.3410532839213171\n",
      "Iteration: 42 lambda_n: 1.0159750577231792 Loss: 0.34018344482011437\n",
      "Iteration: 43 lambda_n: 1.0247566958526717 Loss: 0.3394000357673234\n",
      "Iteration: 44 lambda_n: 1.0045240063205008 Loss: 0.33863153379215205\n",
      "Iteration: 45 lambda_n: 1.0223503379706855 Loss: 0.3378964497805798\n",
      "Iteration: 46 lambda_n: 0.9536359616228002 Loss: 0.33716528209723867\n",
      "Iteration: 47 lambda_n: 0.9054632809756196 Loss: 0.33649840562024946\n",
      "Iteration: 48 lambda_n: 0.8861752427380085 Loss: 0.3358778866928586\n",
      "Iteration: 49 lambda_n: 1.018437892148968 Loss: 0.3352819526003427\n",
      "Iteration: 50 lambda_n: 0.9358703101215449 Loss: 0.334625310078167\n",
      "Iteration: 51 lambda_n: 0.9384724971355343 Loss: 0.3340457782910348\n",
      "Iteration: 52 lambda_n: 0.9018184604081569 Loss: 0.33348514474902746\n",
      "Iteration: 53 lambda_n: 0.8958615249075487 Loss: 0.33296544353725765\n",
      "Iteration: 54 lambda_n: 0.9542006132057549 Loss: 0.332466241958232\n",
      "Iteration: 55 lambda_n: 0.9280560881784169 Loss: 0.3319511438471721\n",
      "Iteration: 56 lambda_n: 0.946003461274629 Loss: 0.3314657227692473\n",
      "Iteration: 57 lambda_n: 0.8829242902153104 Loss: 0.3309846408156453\n",
      "Iteration: 58 lambda_n: 0.91505704609559 Loss: 0.33054721359194733\n",
      "Iteration: 59 lambda_n: 1.0177114984235107 Loss: 0.33010377514862543\n",
      "Iteration: 60 lambda_n: 0.8890654985359487 Loss: 0.3296208045363905\n",
      "Iteration: 61 lambda_n: 0.9701181971070585 Loss: 0.3292077418736645\n",
      "Iteration: 62 lambda_n: 0.9914853762902244 Loss: 0.3287645398468147\n",
      "Iteration: 63 lambda_n: 0.9477527000375534 Loss: 0.3283191992651184\n",
      "Iteration: 64 lambda_n: 0.973273694497397 Loss: 0.32790026379185205\n",
      "Iteration: 65 lambda_n: 0.9416450248042384 Loss: 0.32747611499152357\n",
      "Iteration: 66 lambda_n: 1.02343499803156 Loss: 0.3270713155886284\n",
      "Iteration: 67 lambda_n: 1.0113410250955992 Loss: 0.32663678400552326\n",
      "Iteration: 68 lambda_n: 0.893676501480778 Loss: 0.326212823358639\n",
      "Iteration: 69 lambda_n: 0.8873809464269704 Loss: 0.3258426034955922\n",
      "Iteration: 70 lambda_n: 1.0137021380501479 Loss: 0.3254786140757161\n",
      "Iteration: 71 lambda_n: 0.8772826703500969 Loss: 0.32506669101185176\n",
      "Iteration: 72 lambda_n: 0.9906223850916751 Loss: 0.32471382312700914\n",
      "Iteration: 73 lambda_n: 0.896074255660998 Loss: 0.3243186932533145\n",
      "Iteration: 74 lambda_n: 0.89824180326291 Loss: 0.3239644912406588\n",
      "Iteration: 75 lambda_n: 0.9458165775621067 Loss: 0.32361218014764775\n",
      "Iteration: 76 lambda_n: 1.0026852178425292 Loss: 0.3232439598384311\n",
      "Iteration: 77 lambda_n: 0.9374723103545446 Loss: 0.32285651300389767\n",
      "Iteration: 78 lambda_n: 0.9771486945689469 Loss: 0.3224969970992599\n",
      "Iteration: 79 lambda_n: 0.9346714662862101 Loss: 0.32212478244244436\n",
      "Iteration: 80 lambda_n: 1.01943656732656 Loss: 0.32177112912466604\n",
      "Iteration: 81 lambda_n: 0.8972613672673488 Loss: 0.3213877605392185\n",
      "Iteration: 82 lambda_n: 1.009282108685759 Loss: 0.32105248778653395\n",
      "Iteration: 83 lambda_n: 0.9516034950386348 Loss: 0.32067737990699263\n",
      "Iteration: 84 lambda_n: 0.8959743852590317 Loss: 0.32032575684792114\n",
      "Iteration: 85 lambda_n: 1.010793123254482 Loss: 0.3199964191647011\n",
      "Iteration: 86 lambda_n: 0.8928290753391791 Loss: 0.31962663541817576\n",
      "Iteration: 87 lambda_n: 0.9564054416526553 Loss: 0.3193016852957875\n",
      "Iteration: 88 lambda_n: 0.9192732271356132 Loss: 0.3189551154094313\n",
      "Iteration: 89 lambda_n: 0.9034160982710137 Loss: 0.31862350436978437\n",
      "Iteration: 90 lambda_n: 0.9784112583343624 Loss: 0.3182989774313186\n",
      "Iteration: 91 lambda_n: 0.955693545121761 Loss: 0.3179489096888713\n",
      "Iteration: 92 lambda_n: 0.9201553435949761 Loss: 0.3176083969113479\n",
      "Iteration: 93 lambda_n: 0.9852194152938917 Loss: 0.3172818383898371\n",
      "Iteration: 94 lambda_n: 1.0246170344651353 Loss: 0.31693347556307616\n",
      "Iteration: 95 lambda_n: 0.8760687611565611 Loss: 0.316572567927066\n",
      "Iteration: 96 lambda_n: 0.909062271950066 Loss: 0.3162651732151209\n",
      "Iteration: 97 lambda_n: 0.9466747201780036 Loss: 0.3159472225353348\n",
      "Iteration: 98 lambda_n: 1.0184858221988022 Loss: 0.31561718908143505\n",
      "Iteration: 99 lambda_n: 0.9378487052816121 Loss: 0.3152632889409404\n",
      "Iteration: 100 lambda_n: 1.015143961290886 Loss: 0.31493853156710505\n",
      "Iteration: 101 lambda_n: 1.0160736047664203 Loss: 0.31458809720361425\n",
      "Iteration: 102 lambda_n: 0.945938563004634 Loss: 0.3142384900737777\n",
      "Iteration: 103 lambda_n: 0.9909492092933772 Loss: 0.3139140550515718\n",
      "Iteration: 104 lambda_n: 0.9136270615291042 Loss: 0.31357517203124513\n",
      "Iteration: 105 lambda_n: 0.9931763715282668 Loss: 0.3132636638470892\n",
      "Iteration: 106 lambda_n: 1.0001774864028485 Loss: 0.3129259471765059\n",
      "Iteration: 107 lambda_n: 1.0253251142496658 Loss: 0.3125868293577204\n",
      "Iteration: 108 lambda_n: 0.9974183111145829 Loss: 0.3122401746211859\n",
      "Iteration: 109 lambda_n: 0.9713176509244358 Loss: 0.31190392054923693\n",
      "Iteration: 110 lambda_n: 0.8830337172849555 Loss: 0.31157736158474486\n",
      "Iteration: 111 lambda_n: 0.967951629795176 Loss: 0.3112812611603645\n",
      "Iteration: 112 lambda_n: 0.9153528945689672 Loss: 0.3109574486633954\n",
      "Iteration: 113 lambda_n: 0.9344159099476045 Loss: 0.310652008896291\n",
      "Iteration: 114 lambda_n: 0.9673317078671715 Loss: 0.3103409463026732\n",
      "Iteration: 115 lambda_n: 0.914743269523937 Loss: 0.3100196948505187\n",
      "Iteration: 116 lambda_n: 0.875482072841567 Loss: 0.3097166484634854\n",
      "Iteration: 117 lambda_n: 0.9071795292094786 Loss: 0.3094272693885985\n",
      "Iteration: 118 lambda_n: 0.9335499097942779 Loss: 0.30912806022618994\n",
      "Iteration: 119 lambda_n: 0.9526058373584957 Loss: 0.30882083501250523\n",
      "Iteration: 120 lambda_n: 0.9429615143066031 Loss: 0.30850804545362237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 121 lambda_n: 0.9408734914113958 Loss: 0.3081991276154623\n",
      "Iteration: 122 lambda_n: 0.9204643949529475 Loss: 0.3078915819833699\n",
      "Iteration: 123 lambda_n: 0.9798168909918029 Loss: 0.3075913714834052\n",
      "Iteration: 124 lambda_n: 0.9664678764547813 Loss: 0.3072724881243514\n",
      "Iteration: 125 lambda_n: 1.0071266401657342 Loss: 0.3069586602145383\n",
      "Iteration: 126 lambda_n: 0.8763204699933627 Loss: 0.3066323536842094\n",
      "Iteration: 127 lambda_n: 0.8882093267318734 Loss: 0.3063490759310524\n",
      "Iteration: 128 lambda_n: 0.955263531643216 Loss: 0.30606252228362724\n",
      "Iteration: 129 lambda_n: 0.9175304164344341 Loss: 0.3057549496682136\n",
      "Iteration: 130 lambda_n: 0.9580818253642019 Loss: 0.30546015406420945\n",
      "Iteration: 131 lambda_n: 0.8934411902095888 Loss: 0.30515295504361145\n",
      "Iteration: 132 lambda_n: 1.0010495443806433 Loss: 0.3048670853901053\n",
      "Iteration: 133 lambda_n: 1.017446569768795 Loss: 0.3045474123751346\n",
      "Iteration: 134 lambda_n: 1.0287079529388399 Loss: 0.30422321135707564\n",
      "Iteration: 135 lambda_n: 0.9127644534662662 Loss: 0.30389614420296746\n",
      "Iteration: 136 lambda_n: 0.9887767502689305 Loss: 0.3036065816048575\n",
      "Iteration: 137 lambda_n: 0.8832495134552885 Loss: 0.3032935197890122\n",
      "Iteration: 138 lambda_n: 0.9197750880071727 Loss: 0.3030144583597689\n",
      "Iteration: 139 lambda_n: 1.0017721151635481 Loss: 0.30272440269651196\n",
      "Iteration: 140 lambda_n: 0.9461966723562169 Loss: 0.30240910533996873\n",
      "Iteration: 141 lambda_n: 0.9393673478571618 Loss: 0.30211192851536567\n",
      "Iteration: 142 lambda_n: 0.9166286851932565 Loss: 0.3018174833648099\n",
      "Iteration: 143 lambda_n: 0.8973400845738957 Loss: 0.30153073085729004\n",
      "Iteration: 144 lambda_n: 0.9846935581936791 Loss: 0.30125054968144094\n",
      "Iteration: 145 lambda_n: 0.9394420504219584 Loss: 0.3009436695017989\n",
      "Iteration: 146 lambda_n: 0.9476471379915838 Loss: 0.3006514901904662\n",
      "Iteration: 147 lambda_n: 0.9359319310288416 Loss: 0.300357332605271\n",
      "Iteration: 148 lambda_n: 0.9240343867915726 Loss: 0.3000673801722285\n",
      "Iteration: 149 lambda_n: 0.927282223300814 Loss: 0.2997816656659529\n",
      "Iteration: 150 lambda_n: 0.8866159535025087 Loss: 0.29949549178946133\n",
      "Iteration: 151 lambda_n: 0.9820881933916795 Loss: 0.2992223883084383\n",
      "Iteration: 152 lambda_n: 0.8944324180874451 Loss: 0.29892042726838447\n",
      "Iteration: 153 lambda_n: 0.969026036489444 Loss: 0.29864596858180975\n",
      "Iteration: 154 lambda_n: 1.0081634870078438 Loss: 0.2983491643763151\n",
      "Iteration: 155 lambda_n: 0.8774201462368384 Loss: 0.29804098282506103\n",
      "Iteration: 156 lambda_n: 0.9775989785062221 Loss: 0.29777331590561695\n",
      "Iteration: 157 lambda_n: 0.908238573044707 Loss: 0.2974756209916123\n",
      "Iteration: 158 lambda_n: 0.9735493926381756 Loss: 0.297199594720564\n",
      "Iteration: 159 lambda_n: 0.9154551151863332 Loss: 0.2969042645799458\n",
      "Iteration: 160 lambda_n: 0.8844347033626715 Loss: 0.29662710347777715\n",
      "Iteration: 161 lambda_n: 0.9204380903407224 Loss: 0.29635982868066246\n",
      "Iteration: 162 lambda_n: 1.0265127305828574 Loss: 0.2960821704618347\n",
      "Iteration: 163 lambda_n: 0.9258410361732574 Loss: 0.29577308986652834\n",
      "Iteration: 164 lambda_n: 0.8939678272404493 Loss: 0.29549489599382317\n",
      "Iteration: 165 lambda_n: 0.9530624041473627 Loss: 0.2952267789973652\n",
      "Iteration: 166 lambda_n: 0.9565377120679265 Loss: 0.2949414527276017\n",
      "Iteration: 167 lambda_n: 1.01336531098834 Loss: 0.2946556339988451\n",
      "Iteration: 168 lambda_n: 0.9956335685483167 Loss: 0.29435341672432286\n",
      "Iteration: 169 lambda_n: 0.8909443901232613 Loss: 0.2940570904773559\n",
      "Iteration: 170 lambda_n: 0.9108856033308329 Loss: 0.2937924498416872\n",
      "Iteration: 171 lambda_n: 0.9568348026215137 Loss: 0.2935223687317002\n",
      "Iteration: 172 lambda_n: 0.9919025196164203 Loss: 0.29323918108261027\n",
      "Iteration: 173 lambda_n: 0.9995074702651132 Loss: 0.2929461767655315\n",
      "Iteration: 174 lambda_n: 1.0135660885666224 Loss: 0.2926515112405225\n",
      "Iteration: 175 lambda_n: 0.895829887081775 Loss: 0.29235329773777197\n",
      "Iteration: 176 lambda_n: 0.9981750860141417 Loss: 0.29209025650953613\n",
      "Iteration: 177 lambda_n: 0.9893954202013785 Loss: 0.2917976891455577\n",
      "Iteration: 178 lambda_n: 0.9158453922891995 Loss: 0.29150827227200926\n",
      "Iteration: 179 lambda_n: 0.9896502408316039 Loss: 0.291240897584418\n",
      "Iteration: 180 lambda_n: 0.9939569499361607 Loss: 0.2909525044572296\n",
      "Iteration: 181 lambda_n: 0.9799746794898583 Loss: 0.290663427424201\n",
      "Iteration: 182 lambda_n: 1.0164006815898472 Loss: 0.2903789808879529\n",
      "Iteration: 183 lambda_n: 0.9442574259743208 Loss: 0.2900845375137089\n",
      "Iteration: 184 lambda_n: 0.9270538551914461 Loss: 0.2898115458764504\n",
      "Iteration: 185 lambda_n: 0.9883534454700548 Loss: 0.2895440312476668\n",
      "Iteration: 186 lambda_n: 0.8889487862260572 Loss: 0.2892593546497383\n",
      "Iteration: 187 lambda_n: 0.9846873979267613 Loss: 0.2890038117681528\n",
      "Iteration: 188 lambda_n: 0.9382079826444972 Loss: 0.2887212490365952\n",
      "Iteration: 189 lambda_n: 0.9645329089209135 Loss: 0.288452550376649\n",
      "Iteration: 190 lambda_n: 0.9131559237013467 Loss: 0.288176827980469\n",
      "Iteration: 191 lambda_n: 0.9150142649737749 Loss: 0.287916292063693\n",
      "Iteration: 192 lambda_n: 0.9220805571228744 Loss: 0.28765569977899463\n",
      "Iteration: 193 lambda_n: 0.8769055623239058 Loss: 0.28739357259862935\n",
      "Iteration: 194 lambda_n: 0.9099986970800373 Loss: 0.2871447438850797\n",
      "Iteration: 195 lambda_n: 0.936411507925694 Loss: 0.2868869750985083\n",
      "Iteration: 196 lambda_n: 0.8879485621542742 Loss: 0.28662220445421127\n",
      "Iteration: 197 lambda_n: 0.8801466982621197 Loss: 0.28637160316436605\n",
      "Iteration: 198 lambda_n: 1.000035957758367 Loss: 0.28612364181337147\n",
      "Iteration: 199 lambda_n: 0.9292046475933601 Loss: 0.2858423986829605\n",
      "Iteration: 200 lambda_n: 0.8758603050004 Loss: 0.28558159369843095\n",
      "Iteration: 201 lambda_n: 1.011932730957259 Loss: 0.2853362142752233\n",
      "Iteration: 202 lambda_n: 0.9285816692521073 Loss: 0.2850532082193486\n",
      "Iteration: 203 lambda_n: 0.9388817717805432 Loss: 0.2847940335572882\n",
      "Iteration: 204 lambda_n: 1.0277901047161746 Loss: 0.2845324678850629\n",
      "Iteration: 205 lambda_n: 0.923125043440194 Loss: 0.2842466902696863\n",
      "Iteration: 206 lambda_n: 1.0026323974925813 Loss: 0.283990558565403\n",
      "Iteration: 207 lambda_n: 0.9628059383388216 Loss: 0.2837128890870448\n",
      "Iteration: 208 lambda_n: 0.9633731675268586 Loss: 0.2834467866976125\n",
      "Iteration: 209 lambda_n: 0.9082170489959664 Loss: 0.28318104032832825\n",
      "Iteration: 210 lambda_n: 0.9377334007864959 Loss: 0.2829309893560268\n",
      "Iteration: 211 lambda_n: 0.981898125025396 Loss: 0.2826732792159677\n",
      "Iteration: 212 lambda_n: 0.9708097957205253 Loss: 0.2824039358228714\n",
      "Iteration: 213 lambda_n: 1.0157842213109003 Loss: 0.2821381543610514\n",
      "Iteration: 214 lambda_n: 0.8804053786632576 Loss: 0.28186059839985\n",
      "Iteration: 215 lambda_n: 0.9067186853778043 Loss: 0.2816205190714584\n",
      "Iteration: 216 lambda_n: 0.9441966858739543 Loss: 0.2813736988439011\n",
      "Iteration: 217 lambda_n: 1.029107907409621 Loss: 0.281117142284656\n",
      "Iteration: 218 lambda_n: 0.8768712209838149 Loss: 0.2808380422728753\n",
      "Iteration: 219 lambda_n: 0.8804061545712238 Loss: 0.28060071700464295\n",
      "Iteration: 220 lambda_n: 0.9756303637034605 Loss: 0.2803628529019118\n",
      "Iteration: 221 lambda_n: 1.006636350782975 Loss: 0.28009972713932424\n",
      "Iteration: 222 lambda_n: 0.9797180258979142 Loss: 0.2798287696619708\n",
      "Iteration: 223 lambda_n: 0.9462831869644026 Loss: 0.2795655888819268\n",
      "Iteration: 224 lambda_n: 0.9028766425300134 Loss: 0.27931188790737876\n",
      "Iteration: 225 lambda_n: 0.9479351283566552 Loss: 0.2790702825235264\n",
      "Iteration: 226 lambda_n: 0.9160942539514937 Loss: 0.2788170789942572\n",
      "Iteration: 227 lambda_n: 1.0030896429955776 Loss: 0.2785728446773235\n",
      "Iteration: 228 lambda_n: 0.9862549814886964 Loss: 0.27830590892953516\n",
      "Iteration: 229 lambda_n: 0.9874898733030779 Loss: 0.27804398014668774\n",
      "Iteration: 230 lambda_n: 0.9465868516131871 Loss: 0.2777822414027789\n",
      "Iteration: 231 lambda_n: 0.9892445990348439 Loss: 0.2775318397817943\n",
      "Iteration: 232 lambda_n: 0.9929679325689368 Loss: 0.27727065040454046\n",
      "Iteration: 233 lambda_n: 0.9015218793025138 Loss: 0.2770089972200231\n",
      "Iteration: 234 lambda_n: 0.9348786913436227 Loss: 0.2767719116687826\n",
      "Iteration: 235 lambda_n: 0.9663418471040669 Loss: 0.2765264978341584\n",
      "Iteration: 236 lambda_n: 0.9767414790032508 Loss: 0.2762732996095965\n",
      "Iteration: 237 lambda_n: 0.9594198068290505 Loss: 0.2760178714258024\n",
      "Iteration: 238 lambda_n: 0.9451473064025835 Loss: 0.2757674630158663\n",
      "Iteration: 239 lambda_n: 0.957162560426799 Loss: 0.2755212528687366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 240 lambda_n: 0.8943708936943343 Loss: 0.2752723841264248\n",
      "Iteration: 241 lambda_n: 0.9244374085093023 Loss: 0.2750402858470387\n",
      "Iteration: 242 lambda_n: 1.0027376737919877 Loss: 0.27480081422079833\n",
      "Iteration: 243 lambda_n: 0.9066134612850706 Loss: 0.27454154014428667\n",
      "Iteration: 244 lambda_n: 0.9530940095204509 Loss: 0.2743075889854086\n",
      "Iteration: 245 lambda_n: 0.9323930360085613 Loss: 0.2740620895124467\n",
      "Iteration: 246 lambda_n: 1.0066085965902851 Loss: 0.2738223790739444\n",
      "Iteration: 247 lambda_n: 0.8877963842084625 Loss: 0.2735640711755758\n",
      "Iteration: 248 lambda_n: 0.8784537410489119 Loss: 0.2733367082398705\n",
      "Iteration: 249 lambda_n: 1.0039563686894608 Loss: 0.273112136378402\n",
      "Iteration: 250 lambda_n: 0.965834143648374 Loss: 0.27285593193620356\n",
      "Iteration: 251 lambda_n: 1.0057028244000121 Loss: 0.2726099491407442\n",
      "Iteration: 252 lambda_n: 0.983237210595136 Loss: 0.2723543062281112\n",
      "Iteration: 253 lambda_n: 0.9732518418443373 Loss: 0.27210487468928557\n",
      "Iteration: 254 lambda_n: 0.9673240456775603 Loss: 0.2718584599292587\n",
      "Iteration: 255 lambda_n: 0.9006588365766437 Loss: 0.27161402081454233\n",
      "Iteration: 256 lambda_n: 0.9436194732937808 Loss: 0.2713868654318651\n",
      "Iteration: 257 lambda_n: 0.9106097163249929 Loss: 0.2711493022580523\n",
      "Iteration: 258 lambda_n: 0.9742216196985999 Loss: 0.27092047974504335\n",
      "Iteration: 259 lambda_n: 1.02402993566081 Loss: 0.2706761169376547\n",
      "Iteration: 260 lambda_n: 0.9566881516254118 Loss: 0.27041975924610206\n",
      "Iteration: 261 lambda_n: 0.9200585214790492 Loss: 0.27018074692903365\n",
      "Iteration: 262 lambda_n: 1.0239538299152546 Loss: 0.26995132264650135\n",
      "Iteration: 263 lambda_n: 0.9704254185269529 Loss: 0.26969645946907284\n",
      "Iteration: 264 lambda_n: 0.9741918478130621 Loss: 0.2694554102772827\n",
      "Iteration: 265 lambda_n: 0.9683333427451848 Loss: 0.26921389195643325\n",
      "Iteration: 266 lambda_n: 0.9493014990236808 Loss: 0.26897429030956754\n",
      "Iteration: 267 lambda_n: 0.9617025615599734 Loss: 0.2687398490463579\n",
      "Iteration: 268 lambda_n: 0.9183707196425162 Loss: 0.2685027926718882\n",
      "Iteration: 269 lambda_n: 0.8762653615873713 Loss: 0.2682768487183105\n",
      "Iteration: 270 lambda_n: 0.9374771874742873 Loss: 0.26806165587941505\n",
      "Iteration: 271 lambda_n: 0.9247714065638398 Loss: 0.26783183124394727\n",
      "Iteration: 272 lambda_n: 0.9329786216796483 Loss: 0.2676055424046351\n",
      "Iteration: 273 lambda_n: 0.9502165521163795 Loss: 0.26737766352990744\n",
      "Iteration: 274 lambda_n: 0.8804877020766938 Loss: 0.2671460032343189\n",
      "Iteration: 275 lambda_n: 0.9580040083624916 Loss: 0.2669317455628358\n",
      "Iteration: 276 lambda_n: 0.980990072743772 Loss: 0.2666990321311988\n",
      "Iteration: 277 lambda_n: 0.9727807733032817 Loss: 0.26646118682350195\n",
      "Iteration: 278 lambda_n: 0.8972124357546732 Loss: 0.26622578920863327\n",
      "Iteration: 279 lambda_n: 0.9279143144950553 Loss: 0.2660090944559172\n",
      "Iteration: 280 lambda_n: 0.8885756400503934 Loss: 0.2657853821483959\n",
      "Iteration: 281 lambda_n: 0.9144586980302215 Loss: 0.265571546158596\n",
      "Iteration: 282 lambda_n: 0.8935093050159467 Loss: 0.26535186768891555\n",
      "Iteration: 283 lambda_n: 0.9519038722383123 Loss: 0.2651376089172485\n",
      "Iteration: 284 lambda_n: 0.9070127092689275 Loss: 0.264909750418596\n",
      "Iteration: 285 lambda_n: 0.95146012385909 Loss: 0.2646930445528771\n",
      "Iteration: 286 lambda_n: 0.9438256805676335 Loss: 0.26446612608852976\n",
      "Iteration: 287 lambda_n: 0.9235339123461823 Loss: 0.26424145024751666\n",
      "Iteration: 288 lambda_n: 0.9621647179502028 Loss: 0.26402201320496094\n",
      "Iteration: 289 lambda_n: 0.9547305875193948 Loss: 0.26379381332991375\n",
      "Iteration: 290 lambda_n: 0.9440900350809114 Loss: 0.26356780523746043\n",
      "Iteration: 291 lambda_n: 0.8841456410547149 Loss: 0.26334473556418453\n",
      "Iteration: 292 lambda_n: 0.9057302292034448 Loss: 0.2631362166465625\n",
      "Iteration: 293 lambda_n: 0.975785795175092 Loss: 0.2629229785545171\n",
      "Iteration: 294 lambda_n: 0.9946902687592186 Loss: 0.2626936566998612\n",
      "Iteration: 295 lambda_n: 0.9390487254542532 Loss: 0.26246034018279213\n",
      "Iteration: 296 lambda_n: 0.8839828598225865 Loss: 0.2622405044299088\n",
      "Iteration: 297 lambda_n: 0.9011010943631879 Loss: 0.2620339404978051\n",
      "Iteration: 298 lambda_n: 1.0266803970139844 Loss: 0.26182374166450484\n",
      "Iteration: 299 lambda_n: 0.9025188625075388 Loss: 0.26158467366857596\n",
      "Iteration: 300 lambda_n: 0.9707812169689978 Loss: 0.26137493872977974\n",
      "Iteration: 301 lambda_n: 1.0269887742535402 Loss: 0.2611497399489832\n",
      "Iteration: 302 lambda_n: 0.9438145353374446 Loss: 0.2609119557426544\n",
      "Iteration: 303 lambda_n: 0.8764231038116639 Loss: 0.2606938674774126\n",
      "Iteration: 304 lambda_n: 0.970320953578674 Loss: 0.26049172451519725\n",
      "Iteration: 305 lambda_n: 0.9519004506011919 Loss: 0.26026830896075\n",
      "Iteration: 306 lambda_n: 0.9367447033603608 Loss: 0.26004955004824276\n",
      "Iteration: 307 lambda_n: 0.903116041545057 Loss: 0.2598346741716872\n",
      "Iteration: 308 lambda_n: 1.0026397679918908 Loss: 0.2596278906978542\n",
      "Iteration: 309 lambda_n: 0.8911268356712112 Loss: 0.2593987253516841\n",
      "Iteration: 310 lambda_n: 0.9333846936520809 Loss: 0.2591954447420082\n",
      "Iteration: 311 lambda_n: 0.9757690542193131 Loss: 0.2589828947782609\n",
      "Iteration: 312 lambda_n: 1.028936009637003 Loss: 0.2587610976927069\n",
      "Iteration: 313 lambda_n: 0.9371688663504383 Loss: 0.2585276606356864\n",
      "Iteration: 314 lambda_n: 0.9237845898165723 Loss: 0.25831546787292825\n",
      "Iteration: 315 lambda_n: 1.0059554867896736 Loss: 0.25810668682561877\n",
      "Iteration: 316 lambda_n: 0.9700946578436733 Loss: 0.2578797440821991\n",
      "Iteration: 317 lambda_n: 1.0067598890499305 Loss: 0.25766131907432005\n",
      "Iteration: 318 lambda_n: 0.9827237873383946 Loss: 0.25743506627152635\n",
      "Iteration: 319 lambda_n: 0.9013325449975365 Loss: 0.25721464670691907\n",
      "Iteration: 320 lambda_n: 0.9272188909714905 Loss: 0.2570128675235419\n",
      "Iteration: 321 lambda_n: 1.0055832593427512 Loss: 0.25680565647775966\n",
      "Iteration: 322 lambda_n: 0.8909858140162691 Loss: 0.2565813378623922\n",
      "Iteration: 323 lambda_n: 0.9820975342725671 Loss: 0.2563829690650402\n",
      "Iteration: 324 lambda_n: 0.9503963790605645 Loss: 0.2561646936302071\n",
      "Iteration: 325 lambda_n: 0.993570337058501 Loss: 0.2559538652641873\n",
      "Iteration: 326 lambda_n: 0.8988307415836809 Loss: 0.25573386543779275\n",
      "Iteration: 327 lambda_n: 0.9364221332548363 Loss: 0.25553522473040274\n",
      "Iteration: 328 lambda_n: 0.9380773252795572 Loss: 0.2553286364128344\n",
      "Iteration: 329 lambda_n: 0.8764604620968367 Loss: 0.25512205747640854\n",
      "Iteration: 330 lambda_n: 0.9822942492194163 Loss: 0.25492939663432795\n",
      "Iteration: 331 lambda_n: 0.9250229772141186 Loss: 0.2547138383004363\n",
      "Iteration: 332 lambda_n: 1.0057463624250247 Loss: 0.2545112319825663\n",
      "Iteration: 333 lambda_n: 0.9149079648355394 Loss: 0.254291338905223\n",
      "Iteration: 334 lambda_n: 0.994157026842793 Loss: 0.2540916933502598\n",
      "Iteration: 335 lambda_n: 0.8959784105955655 Loss: 0.2538751378872082\n",
      "Iteration: 336 lambda_n: 0.9956382310504978 Loss: 0.2536803411952092\n",
      "Iteration: 337 lambda_n: 0.9236245575885844 Loss: 0.2534642516871446\n",
      "Iteration: 338 lambda_n: 0.9727854090963869 Loss: 0.25326417501190157\n",
      "Iteration: 339 lambda_n: 0.9464462808603119 Loss: 0.2530538238679701\n",
      "Iteration: 340 lambda_n: 0.9173418953194682 Loss: 0.25284955056904773\n",
      "Iteration: 341 lambda_n: 0.9703501408868693 Loss: 0.25265191861684405\n",
      "Iteration: 342 lambda_n: 0.9220889777778676 Loss: 0.2524432353768506\n",
      "Iteration: 343 lambda_n: 0.8759005166504185 Loss: 0.2522452999738546\n",
      "Iteration: 344 lambda_n: 0.9088826441317116 Loss: 0.2520576114886904\n",
      "Iteration: 345 lambda_n: 0.8782880823302006 Loss: 0.25186318296389054\n",
      "Iteration: 346 lambda_n: 1.0285054173819514 Loss: 0.2516756261893414\n",
      "Iteration: 347 lambda_n: 0.8930730035112155 Loss: 0.25145636197612314\n",
      "Iteration: 348 lambda_n: 0.9525661248570468 Loss: 0.2512663437943209\n",
      "Iteration: 349 lambda_n: 0.9243486795836596 Loss: 0.2510640142939671\n",
      "Iteration: 350 lambda_n: 1.0146370444174089 Loss: 0.2508680357171957\n",
      "Iteration: 351 lambda_n: 0.9574524236588642 Loss: 0.2506532954899731\n",
      "Iteration: 352 lambda_n: 0.9978637726902656 Loss: 0.25045105018715913\n",
      "Iteration: 353 lambda_n: 0.8909283791554395 Loss: 0.25024065449967653\n",
      "Iteration: 354 lambda_n: 0.9043538294144517 Loss: 0.25005316245153875\n",
      "Iteration: 355 lambda_n: 0.9969635770903442 Loss: 0.24986316865255428\n",
      "Iteration: 356 lambda_n: 0.9522180130460944 Loss: 0.2496540808131972\n",
      "Iteration: 357 lambda_n: 0.8967365127015497 Loss: 0.24945475618420285\n",
      "Iteration: 358 lambda_n: 0.9806540921210937 Loss: 0.24926738522549746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 359 lambda_n: 0.9142597065200994 Loss: 0.2490628305683966\n",
      "Iteration: 360 lambda_n: 0.955514177024377 Loss: 0.24887248031745807\n",
      "Iteration: 361 lambda_n: 0.9619115800317878 Loss: 0.24867388714115332\n",
      "Iteration: 362 lambda_n: 0.9591471574363135 Loss: 0.24847432751063186\n",
      "Iteration: 363 lambda_n: 0.9164377847402481 Loss: 0.24827570500292998\n",
      "Iteration: 364 lambda_n: 0.8859702699121376 Loss: 0.24808627207530834\n",
      "Iteration: 365 lambda_n: 0.9601356541564475 Loss: 0.24790345519033746\n",
      "Iteration: 366 lambda_n: 1.0158438225206248 Loss: 0.24770566824743134\n",
      "Iteration: 367 lambda_n: 0.9650688963097004 Loss: 0.24749678701456632\n",
      "Iteration: 368 lambda_n: 1.0126425504189183 Loss: 0.2472987277751232\n",
      "Iteration: 369 lambda_n: 0.8926329256968469 Loss: 0.2470912854409449\n",
      "Iteration: 370 lambda_n: 0.9531392308002278 Loss: 0.24690877676666725\n",
      "Iteration: 371 lambda_n: 0.9977606812737273 Loss: 0.24671422661435144\n",
      "Iteration: 372 lambda_n: 0.9583597711254358 Loss: 0.24651093607104038\n",
      "Iteration: 373 lambda_n: 1.0132831881273907 Loss: 0.24631604115536224\n",
      "Iteration: 374 lambda_n: 0.996276890804976 Loss: 0.24611035045611585\n",
      "Iteration: 375 lambda_n: 0.9463775666233241 Loss: 0.24590849864425007\n",
      "Iteration: 376 lambda_n: 0.9821953469321753 Loss: 0.24571711669963328\n",
      "Iteration: 377 lambda_n: 0.8769165207137535 Loss: 0.24551884632796137\n",
      "Iteration: 378 lambda_n: 0.9294688162189164 Loss: 0.2453421547797258\n",
      "Iteration: 379 lambda_n: 0.890549807440286 Loss: 0.2451551842544432\n",
      "Iteration: 380 lambda_n: 0.9716909341015655 Loss: 0.24497635578380816\n",
      "Iteration: 381 lambda_n: 0.9267119614438105 Loss: 0.24478156144673305\n",
      "Iteration: 382 lambda_n: 0.8865873670774571 Loss: 0.244596123157412\n",
      "Iteration: 383 lambda_n: 0.8936967431973402 Loss: 0.24441902266947582\n",
      "Iteration: 384 lambda_n: 1.002219650036896 Loss: 0.24424079952008276\n",
      "Iteration: 385 lambda_n: 0.9727546939266685 Loss: 0.24404127104514425\n",
      "Iteration: 386 lambda_n: 1.015103910292175 Loss: 0.2438479727109997\n",
      "Iteration: 387 lambda_n: 0.971609997522044 Loss: 0.2436466276381136\n",
      "Iteration: 388 lambda_n: 0.9015452636761316 Loss: 0.2434542759609145\n",
      "Iteration: 389 lambda_n: 1.022488100576326 Loss: 0.24327611951164763\n",
      "Iteration: 390 lambda_n: 0.8828165654209409 Loss: 0.24307440571552844\n",
      "Iteration: 391 lambda_n: 0.8929615753535112 Loss: 0.24290057811655402\n",
      "Iteration: 392 lambda_n: 0.9799856269515642 Loss: 0.24272504348198834\n",
      "Iteration: 393 lambda_n: 0.9096232184718525 Loss: 0.24253272458687117\n",
      "Iteration: 394 lambda_n: 0.9254953583721057 Loss: 0.2423545404508578\n",
      "Iteration: 395 lambda_n: 0.8941406707061919 Loss: 0.24217355539333438\n",
      "Iteration: 396 lambda_n: 0.9656859227211005 Loss: 0.2419990038001118\n",
      "Iteration: 397 lambda_n: 0.9809161401910536 Loss: 0.24181080062430987\n",
      "Iteration: 398 lambda_n: 0.899512388051953 Loss: 0.24161997374737604\n",
      "Iteration: 399 lambda_n: 0.9897819334321448 Loss: 0.24144530242502513\n",
      "Iteration: 400 lambda_n: 0.9522238823683795 Loss: 0.2412534251934584\n",
      "Iteration: 401 lambda_n: 0.9658769548295548 Loss: 0.24106916883891621\n",
      "Iteration: 402 lambda_n: 0.8848448540949307 Loss: 0.24088260201028253\n",
      "Iteration: 403 lambda_n: 0.970761128669394 Loss: 0.24071199361398976\n",
      "Iteration: 404 lambda_n: 0.976861983193365 Loss: 0.24052512833420836\n",
      "Iteration: 405 lambda_n: 1.0081011839228113 Loss: 0.24033742800091798\n",
      "Iteration: 406 lambda_n: 0.963366928256375 Loss: 0.24014407696495405\n",
      "Iteration: 407 lambda_n: 1.015150733652353 Loss: 0.23995965123628185\n",
      "Iteration: 408 lambda_n: 1.0041799629833306 Loss: 0.23976565997572377\n",
      "Iteration: 409 lambda_n: 0.963093465090696 Loss: 0.2395741263406308\n",
      "Iteration: 410 lambda_n: 1.0062530272574586 Loss: 0.23939077091484576\n",
      "Iteration: 411 lambda_n: 0.8794423122396371 Loss: 0.2391995408856362\n",
      "Iteration: 412 lambda_n: 1.0017211029271291 Loss: 0.23903272048612542\n",
      "Iteration: 413 lambda_n: 1.0239238666753028 Loss: 0.23884301554144707\n",
      "Iteration: 414 lambda_n: 0.9073250283375071 Loss: 0.23864946532308273\n",
      "Iteration: 415 lambda_n: 0.9634782637502431 Loss: 0.23847827915851166\n",
      "Iteration: 416 lambda_n: 1.0228686994262537 Loss: 0.23829680370254408\n",
      "Iteration: 417 lambda_n: 0.9120986558806466 Loss: 0.23810448511411775\n",
      "Iteration: 418 lambda_n: 0.9485829138224746 Loss: 0.23793331615624133\n",
      "Iteration: 419 lambda_n: 0.8872053192661284 Loss: 0.23775560018405187\n",
      "Iteration: 420 lambda_n: 0.9162064938044879 Loss: 0.23758967339375112\n",
      "Iteration: 421 lambda_n: 0.9798144405055627 Loss: 0.2374186031039913\n",
      "Iteration: 422 lambda_n: 0.8860564971525299 Loss: 0.23723596546563996\n",
      "Iteration: 423 lambda_n: 1.0000033565671709 Loss: 0.23707110143060509\n",
      "Iteration: 424 lambda_n: 0.9844006649895214 Loss: 0.23688534023119326\n",
      "Iteration: 425 lambda_n: 0.9923027800140742 Loss: 0.23670281349522473\n",
      "Iteration: 426 lambda_n: 0.8782056456647309 Loss: 0.23651915448016142\n",
      "Iteration: 427 lambda_n: 0.8891695151118072 Loss: 0.23635690830481765\n",
      "Iteration: 428 lambda_n: 0.9649419695726162 Loss: 0.23619290150145786\n",
      "Iteration: 429 lambda_n: 0.9568750234798602 Loss: 0.2360152095529627\n",
      "Iteration: 430 lambda_n: 1.0252959238200974 Loss: 0.23583931485868037\n",
      "Iteration: 431 lambda_n: 0.9504850794429767 Loss: 0.23565117409009997\n",
      "Iteration: 432 lambda_n: 1.0232748537738907 Loss: 0.23547708798587816\n",
      "Iteration: 433 lambda_n: 0.8952002330063076 Loss: 0.23528999689586416\n",
      "Iteration: 434 lambda_n: 0.933029395284168 Loss: 0.23512662785376215\n",
      "Iteration: 435 lambda_n: 1.011554398092939 Loss: 0.23495663429160332\n",
      "Iteration: 436 lambda_n: 1.0094785406433824 Loss: 0.2347726488894949\n",
      "Iteration: 437 lambda_n: 1.0254503638264132 Loss: 0.23458938036151747\n",
      "Iteration: 438 lambda_n: 1.0105211403474113 Loss: 0.23440355549192593\n",
      "Iteration: 439 lambda_n: 0.8912763198776709 Loss: 0.23422077854408324\n",
      "Iteration: 440 lambda_n: 0.9861498217391643 Loss: 0.23405986605556625\n",
      "Iteration: 441 lambda_n: 0.9167090000406554 Loss: 0.23388211504274753\n",
      "Iteration: 442 lambda_n: 0.9126903027407985 Loss: 0.233717176882694\n",
      "Iteration: 443 lambda_n: 0.9423123414734346 Loss: 0.23355323587746443\n",
      "Iteration: 444 lambda_n: 0.9998748117740699 Loss: 0.23338425545995553\n",
      "Iteration: 445 lambda_n: 0.8805639222381887 Loss: 0.23320526050016305\n",
      "Iteration: 446 lambda_n: 0.915688385867761 Loss: 0.23304790991346017\n",
      "Iteration: 447 lambda_n: 0.9177616433461284 Loss: 0.23288454493769856\n",
      "Iteration: 448 lambda_n: 0.9320971906824939 Loss: 0.23272108240877065\n",
      "Iteration: 449 lambda_n: 0.9805995100983776 Loss: 0.23255534328261757\n",
      "Iteration: 450 lambda_n: 0.9822955755560511 Loss: 0.2323812751047532\n",
      "Iteration: 451 lambda_n: 0.8793542009765902 Loss: 0.23220721594671517\n",
      "Iteration: 452 lambda_n: 0.9665398538335588 Loss: 0.23205167425517578\n",
      "Iteration: 453 lambda_n: 0.8818624972315924 Loss: 0.23188098407643923\n",
      "Iteration: 454 lambda_n: 0.9107687297124514 Loss: 0.23172551975045053\n",
      "Iteration: 455 lambda_n: 1.0183595975858084 Loss: 0.23156521597469545\n",
      "Iteration: 456 lambda_n: 0.9832940863122059 Loss: 0.23138627141406107\n",
      "Iteration: 457 lambda_n: 0.9472589249805071 Loss: 0.23121380630889937\n",
      "Iteration: 458 lambda_n: 0.9508747415499414 Loss: 0.23104795648215048\n",
      "Iteration: 459 lambda_n: 0.9728469306264163 Loss: 0.2308817583929613\n",
      "Iteration: 460 lambda_n: 1.0015198132916503 Loss: 0.23071201190677512\n",
      "Iteration: 461 lambda_n: 0.911620903224844 Loss: 0.23053756936647507\n",
      "Iteration: 462 lambda_n: 0.9947333720410116 Loss: 0.23037907124016968\n",
      "Iteration: 463 lambda_n: 0.9612539642477039 Loss: 0.23020640763425773\n",
      "Iteration: 464 lambda_n: 0.9478721967289614 Loss: 0.23003985395686977\n",
      "Iteration: 465 lambda_n: 0.9127149788596178 Loss: 0.2298759029626466\n",
      "Iteration: 466 lambda_n: 0.8795793629362694 Loss: 0.2297183019628762\n",
      "Iteration: 467 lambda_n: 0.9043130892796446 Loss: 0.2295666715952466\n",
      "Iteration: 468 lambda_n: 0.8760468724378558 Loss: 0.22941102400909497\n",
      "Iteration: 469 lambda_n: 0.9139683230306662 Loss: 0.22926048624691964\n",
      "Iteration: 470 lambda_n: 0.9778288421232263 Loss: 0.22910367946690308\n",
      "Iteration: 471 lambda_n: 0.9998516049916893 Loss: 0.22893619200267787\n",
      "Iteration: 472 lambda_n: 1.0263215426569903 Loss: 0.22876523291253548\n",
      "Iteration: 473 lambda_n: 1.021734478802622 Loss: 0.22859006263110243\n",
      "Iteration: 474 lambda_n: 1.0162145481755696 Loss: 0.22841599588297626\n",
      "Iteration: 475 lambda_n: 0.9364664657313639 Loss: 0.22824318622599626\n",
      "Iteration: 476 lambda_n: 0.9287092999394225 Loss: 0.2280842269185631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 477 lambda_n: 0.9756448059988472 Loss: 0.2279268483280556\n",
      "Iteration: 478 lambda_n: 1.0061010239056354 Loss: 0.22776179093176582\n",
      "Iteration: 479 lambda_n: 0.9311537196217373 Loss: 0.2275918779829737\n",
      "Iteration: 480 lambda_n: 1.0242237812536277 Loss: 0.22743490430078528\n",
      "Iteration: 481 lambda_n: 0.9469520458393746 Loss: 0.2272625286072219\n",
      "Iteration: 482 lambda_n: 0.9854353078718106 Loss: 0.22710344825007656\n",
      "Iteration: 483 lambda_n: 0.9746810536707321 Loss: 0.22693818280170144\n",
      "Iteration: 484 lambda_n: 0.8863345126869606 Loss: 0.22677500787500646\n",
      "Iteration: 485 lambda_n: 0.8780772116207363 Loss: 0.22662688031298914\n",
      "Iteration: 486 lambda_n: 0.9090550672879253 Loss: 0.22648036420172504\n",
      "Iteration: 487 lambda_n: 0.9408128508230343 Loss: 0.22632891630496982\n",
      "Iteration: 488 lambda_n: 0.9125359914847923 Loss: 0.22617243119190014\n",
      "Iteration: 489 lambda_n: 1.002439922452238 Loss: 0.2260209029562602\n",
      "Iteration: 490 lambda_n: 0.958157236955331 Loss: 0.2258547165554955\n",
      "Iteration: 491 lambda_n: 0.9956518564187365 Loss: 0.22569615380619695\n",
      "Iteration: 492 lambda_n: 0.9305984320056145 Loss: 0.22553166661946578\n",
      "Iteration: 493 lambda_n: 1.0270061155913148 Loss: 0.22537819762702557\n",
      "Iteration: 494 lambda_n: 1.0216785256034733 Loss: 0.225209109787575\n",
      "Iteration: 495 lambda_n: 0.9929706558570742 Loss: 0.22504120511061732\n",
      "Iteration: 496 lambda_n: 1.0091812930826007 Loss: 0.22487831333368818\n",
      "Iteration: 497 lambda_n: 0.993926331168148 Loss: 0.22471305332198793\n",
      "Iteration: 498 lambda_n: 0.8844451462011405 Loss: 0.2245505818113316\n",
      "Iteration: 499 lambda_n: 1.002309702928145 Loss: 0.22440625983843185\n",
      "Iteration: 500 lambda_n: 0.9572448984317081 Loss: 0.2242429614433658\n",
      "Iteration: 501 lambda_n: 0.9011026082480148 Loss: 0.22408728080417833\n",
      "Iteration: 502 lambda_n: 0.9945256293563138 Loss: 0.22394097801453255\n",
      "Iteration: 503 lambda_n: 0.9637460893750869 Loss: 0.22377976445696873\n",
      "Iteration: 504 lambda_n: 1.0227387961909529 Loss: 0.22362381399377093\n",
      "Iteration: 505 lambda_n: 0.976810547809396 Loss: 0.22345859901884835\n",
      "Iteration: 506 lambda_n: 0.9441429274229272 Loss: 0.22330108725499764\n",
      "Iteration: 507 lambda_n: 0.9206568573936197 Loss: 0.22314910470937002\n",
      "Iteration: 508 lambda_n: 0.9752874590583219 Loss: 0.22300114880744573\n",
      "Iteration: 509 lambda_n: 0.9183885337602986 Loss: 0.22284466751581702\n",
      "Iteration: 510 lambda_n: 0.9944833092588221 Loss: 0.2226975676129119\n",
      "Iteration: 511 lambda_n: 0.8961220213884079 Loss: 0.2225385369951503\n",
      "Iteration: 512 lambda_n: 0.901218986141295 Loss: 0.22239548513758223\n",
      "Iteration: 513 lambda_n: 0.9219668485720114 Loss: 0.22225184587135632\n",
      "Iteration: 514 lambda_n: 1.0207844500924317 Loss: 0.22210513213485247\n",
      "Iteration: 515 lambda_n: 0.9307961360725898 Loss: 0.22194295669353573\n",
      "Iteration: 516 lambda_n: 1.0108679954328599 Loss: 0.22179534184173488\n",
      "Iteration: 517 lambda_n: 0.9705543467249419 Loss: 0.2216352902837168\n",
      "Iteration: 518 lambda_n: 0.960645313110391 Loss: 0.22148189318586178\n",
      "Iteration: 519 lambda_n: 0.903495998909466 Loss: 0.22133031990071086\n",
      "Iteration: 520 lambda_n: 0.9425614990467487 Loss: 0.2211880028213326\n",
      "Iteration: 521 lambda_n: 0.8764466715614083 Loss: 0.221039766864964\n",
      "Iteration: 522 lambda_n: 0.8767682341027557 Loss: 0.22090215520940043\n",
      "Iteration: 523 lambda_n: 0.9006540471415214 Loss: 0.22076470366648687\n",
      "Iteration: 524 lambda_n: 0.8893355248611199 Loss: 0.220623723665203\n",
      "Iteration: 525 lambda_n: 1.0044600002239368 Loss: 0.22048473392225582\n",
      "Iteration: 526 lambda_n: 0.9492663231947919 Loss: 0.220327996122778\n",
      "Iteration: 527 lambda_n: 0.9866167607135657 Loss: 0.22018012966456701\n",
      "Iteration: 528 lambda_n: 0.8880510887170153 Loss: 0.2200266994287535\n",
      "Iteration: 529 lambda_n: 1.0081118780332232 Loss: 0.21988883376838322\n",
      "Iteration: 530 lambda_n: 0.962455794793804 Loss: 0.21973257184146316\n",
      "Iteration: 531 lambda_n: 1.0130592055998944 Loss: 0.21958364794812682\n",
      "Iteration: 532 lambda_n: 0.9827155171615467 Loss: 0.2194271565142075\n",
      "Iteration: 533 lambda_n: 1.0059828240019002 Loss: 0.2192756192351087\n",
      "Iteration: 534 lambda_n: 1.0266495077292102 Loss: 0.21912075881526816\n",
      "Iteration: 535 lambda_n: 0.9381946094403734 Loss: 0.21896299290704957\n",
      "Iteration: 536 lambda_n: 1.0044316842049876 Loss: 0.21881907594296895\n",
      "Iteration: 537 lambda_n: 0.9366219055308794 Loss: 0.21866524929145298\n",
      "Iteration: 538 lambda_n: 0.8897305450691949 Loss: 0.21852205663311988\n",
      "Iteration: 539 lambda_n: 0.9746231247513162 Loss: 0.2183862530420748\n",
      "Iteration: 540 lambda_n: 0.9860782492901213 Loss: 0.218237721458843\n",
      "Iteration: 541 lambda_n: 0.9437459447429446 Loss: 0.21808769745201761\n",
      "Iteration: 542 lambda_n: 0.8978709066653151 Loss: 0.2179443583736278\n",
      "Iteration: 543 lambda_n: 0.9210173931244543 Loss: 0.2178082089483105\n",
      "Iteration: 544 lambda_n: 0.9539557457414224 Loss: 0.2176687663078389\n",
      "Iteration: 545 lambda_n: 0.9510471759452506 Loss: 0.2175245665217889\n",
      "Iteration: 546 lambda_n: 0.9839163652086621 Loss: 0.2173810428888883\n",
      "Iteration: 547 lambda_n: 0.8899754210766566 Loss: 0.21723280256203184\n",
      "Iteration: 548 lambda_n: 0.9906430813750379 Loss: 0.21709894242942993\n",
      "Iteration: 549 lambda_n: 0.9414882785515253 Loss: 0.2169501700074324\n",
      "Iteration: 550 lambda_n: 0.905442646037995 Loss: 0.21680902028202853\n",
      "Iteration: 551 lambda_n: 0.9946382903487356 Loss: 0.21667349419915138\n",
      "Iteration: 552 lambda_n: 0.9508560021707518 Loss: 0.21652484971602717\n",
      "Iteration: 553 lambda_n: 1.0086720769141864 Loss: 0.21638299090007276\n",
      "Iteration: 554 lambda_n: 0.9226005244620443 Loss: 0.2162327526013384\n",
      "Iteration: 555 lambda_n: 0.9240901067121614 Loss: 0.2160955717629407\n",
      "Iteration: 556 lambda_n: 0.9476518987747472 Loss: 0.21595838692756764\n",
      "Iteration: 557 lambda_n: 0.8880342415888316 Loss: 0.2158179273172397\n",
      "Iteration: 558 lambda_n: 0.9229866663595045 Loss: 0.21568651757047472\n",
      "Iteration: 559 lambda_n: 0.9254851805521543 Loss: 0.21555014360228647\n",
      "Iteration: 560 lambda_n: 0.885766251666227 Loss: 0.21541361657612285\n",
      "Iteration: 561 lambda_n: 1.0285264099950187 Loss: 0.21528315557376357\n",
      "Iteration: 562 lambda_n: 0.9630739052035793 Loss: 0.2151318984637648\n",
      "Iteration: 563 lambda_n: 0.948349806452794 Loss: 0.21499051554464313\n",
      "Iteration: 564 lambda_n: 0.9716085118238433 Loss: 0.21485152317547465\n",
      "Iteration: 565 lambda_n: 1.015085226666873 Loss: 0.21470935271975783\n",
      "Iteration: 566 lambda_n: 1.0285054367414073 Loss: 0.2145610671445565\n",
      "Iteration: 567 lambda_n: 1.0090650057112736 Loss: 0.21441108137729942\n",
      "Iteration: 568 lambda_n: 0.8928059363316535 Loss: 0.21426418849207135\n",
      "Iteration: 569 lambda_n: 0.9826636028006157 Loss: 0.21413444261933892\n",
      "Iteration: 570 lambda_n: 0.9329446849006178 Loss: 0.2139918559992775\n",
      "Iteration: 571 lambda_n: 0.943337237546939 Loss: 0.2138567098407823\n",
      "Iteration: 572 lambda_n: 1.0282554484249316 Loss: 0.2137202751783926\n",
      "Iteration: 573 lambda_n: 0.9225223213662157 Loss: 0.21357179792466685\n",
      "Iteration: 574 lambda_n: 1.0087765694626833 Loss: 0.21343882041348505\n",
      "Iteration: 575 lambda_n: 1.0007134059301872 Loss: 0.2132936381459103\n",
      "Iteration: 576 lambda_n: 0.9815721045888303 Loss: 0.21314986292741983\n",
      "Iteration: 577 lambda_n: 0.8861640576909469 Loss: 0.21300907713497808\n",
      "Iteration: 578 lambda_n: 0.9967049579982286 Loss: 0.21288218665514866\n",
      "Iteration: 579 lambda_n: 0.9148154100954727 Loss: 0.2127396828046347\n",
      "Iteration: 580 lambda_n: 1.009992904926273 Loss: 0.21260910749525844\n",
      "Iteration: 581 lambda_n: 1.0158658155972613 Loss: 0.21246517102242402\n",
      "Iteration: 582 lambda_n: 0.9101066514354612 Loss: 0.2123206450837134\n",
      "Iteration: 583 lambda_n: 0.9192945270131354 Loss: 0.2121913872476449\n",
      "Iteration: 584 lambda_n: 0.9007676802498267 Loss: 0.2120610254355163\n",
      "Iteration: 585 lambda_n: 0.8835917356892791 Loss: 0.21193348915690116\n",
      "Iteration: 586 lambda_n: 1.0150102806717223 Loss: 0.21180857497639902\n",
      "Iteration: 587 lambda_n: 0.8933240558091475 Loss: 0.21166529687848326\n",
      "Iteration: 588 lambda_n: 0.9537912541197149 Loss: 0.21153941119736025\n",
      "Iteration: 589 lambda_n: 1.0024065959639616 Loss: 0.21140520740623608\n",
      "Iteration: 590 lambda_n: 0.917518143866621 Loss: 0.21126439019558108\n",
      "Iteration: 591 lambda_n: 1.0149295686017237 Loss: 0.21113571521311347\n",
      "Iteration: 592 lambda_n: 1.0141537598462032 Loss: 0.21099359949308052\n",
      "Iteration: 593 lambda_n: 1.0008123731512162 Loss: 0.21085183493573634\n",
      "Iteration: 594 lambda_n: 0.973428861982108 Loss: 0.21071217385819888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 595 lambda_n: 1.0106463925370959 Loss: 0.21057656244820758\n",
      "Iteration: 596 lambda_n: 0.9346135301210182 Loss: 0.21043599662899432\n",
      "Iteration: 597 lambda_n: 0.9029484841351667 Loss: 0.2103062260125593\n",
      "Iteration: 598 lambda_n: 0.9045121513485505 Loss: 0.2101810485444027\n",
      "Iteration: 599 lambda_n: 0.9961824400509803 Loss: 0.21005584422972642\n",
      "Iteration: 600 lambda_n: 1.0118862090585823 Loss: 0.20991816038933675\n",
      "Iteration: 601 lambda_n: 1.0202082478746881 Loss: 0.20977853966084917\n",
      "Iteration: 602 lambda_n: 0.8796358195625892 Loss: 0.2096380092645567\n",
      "Iteration: 603 lambda_n: 1.0070353224308393 Loss: 0.2095170484505809\n",
      "Iteration: 604 lambda_n: 0.9119840483588817 Loss: 0.2093787731694122\n",
      "Iteration: 605 lambda_n: 0.9254271988579261 Loss: 0.2092537596636075\n",
      "Iteration: 606 lambda_n: 1.0190790797368134 Loss: 0.20912709680886257\n",
      "Iteration: 607 lambda_n: 0.9402713586519241 Loss: 0.20898783203100277\n",
      "Iteration: 608 lambda_n: 1.009918043228389 Loss: 0.2088595551073536\n",
      "Iteration: 609 lambda_n: 0.9444961148559119 Loss: 0.20872199318439513\n",
      "Iteration: 610 lambda_n: 0.9930303959657634 Loss: 0.20859355877607477\n",
      "Iteration: 611 lambda_n: 0.8898008614655867 Loss: 0.20845873745231636\n",
      "Iteration: 612 lambda_n: 0.9887127722674395 Loss: 0.2083381306827233\n",
      "Iteration: 613 lambda_n: 0.9791821192398298 Loss: 0.2082043161625652\n",
      "Iteration: 614 lambda_n: 1.0172388128016188 Loss: 0.20807200955739702\n",
      "Iteration: 615 lambda_n: 0.9254144702003114 Loss: 0.20793478484775854\n",
      "Iteration: 616 lambda_n: 0.8891562366386779 Loss: 0.2078101578077473\n",
      "Iteration: 617 lambda_n: 1.0221949944750157 Loss: 0.20769059766725387\n",
      "Iteration: 618 lambda_n: 0.941614250060846 Loss: 0.20755335225172467\n",
      "Iteration: 619 lambda_n: 1.0235404247090576 Loss: 0.20742714013358707\n",
      "Iteration: 620 lambda_n: 0.9233153859915453 Loss: 0.20729016158880392\n",
      "Iteration: 621 lambda_n: 0.9994641584197529 Loss: 0.2071668051766375\n",
      "Iteration: 622 lambda_n: 0.9671345192454174 Loss: 0.20703347992270457\n",
      "Iteration: 623 lambda_n: 0.8901609615311628 Loss: 0.20690468075083704\n",
      "Iteration: 624 lambda_n: 1.0238150555744425 Loss: 0.20678632204762892\n",
      "Iteration: 625 lambda_n: 0.8847020655283482 Loss: 0.2066503935346218\n",
      "Iteration: 626 lambda_n: 1.0245016739512423 Loss: 0.20653313279573804\n",
      "Iteration: 627 lambda_n: 0.9050433955652522 Loss: 0.20639754202285948\n",
      "Iteration: 628 lambda_n: 0.9589714347307843 Loss: 0.2062779634673591\n",
      "Iteration: 629 lambda_n: 0.9870690749894628 Loss: 0.20615144932895046\n",
      "Iteration: 630 lambda_n: 0.891992951685442 Loss: 0.206021434614711\n",
      "Iteration: 631 lambda_n: 0.9318747158555369 Loss: 0.20590413397518634\n",
      "Iteration: 632 lambda_n: 0.9859090768974377 Loss: 0.20578176914965343\n",
      "Iteration: 633 lambda_n: 0.9695598345761576 Loss: 0.20565250814938932\n",
      "Iteration: 634 lambda_n: 1.016616966835055 Loss: 0.20552559701027406\n",
      "Iteration: 635 lambda_n: 0.929428967154331 Loss: 0.2053927389526063\n",
      "Iteration: 636 lambda_n: 0.9437494635475905 Loss: 0.20527147791682177\n",
      "Iteration: 637 lambda_n: 0.959811871642838 Loss: 0.2051485367740165\n",
      "Iteration: 638 lambda_n: 0.9700445736200864 Loss: 0.20502369723110897\n",
      "Iteration: 639 lambda_n: 1.0122415015097295 Loss: 0.2048977257486049\n",
      "Iteration: 640 lambda_n: 0.9688265538639792 Loss: 0.2047664841045999\n",
      "Iteration: 641 lambda_n: 0.9799321743369706 Loss: 0.20464107983980478\n",
      "Iteration: 642 lambda_n: 1.0100061660438069 Loss: 0.20451443968519728\n",
      "Iteration: 643 lambda_n: 0.9577997445978382 Loss: 0.20438412280120805\n",
      "Iteration: 644 lambda_n: 1.011296666218868 Loss: 0.20426074613462072\n",
      "Iteration: 645 lambda_n: 0.9671944607247924 Loss: 0.2041306829940098\n",
      "Iteration: 646 lambda_n: 0.9768940092773493 Loss: 0.2040064975154351\n",
      "Iteration: 647 lambda_n: 0.9061563056703952 Loss: 0.20388126513685675\n",
      "Iteration: 648 lambda_n: 0.9772634195068419 Loss: 0.20376528613705844\n",
      "Iteration: 649 lambda_n: 1.0101913328977379 Loss: 0.20364039170815446\n",
      "Iteration: 650 lambda_n: 0.9427519319699792 Loss: 0.20351149532678053\n",
      "Iteration: 651 lambda_n: 0.8882449326953584 Loss: 0.2033914019807988\n",
      "Iteration: 652 lambda_n: 0.9229653006390424 Loss: 0.2032784258498249\n",
      "Iteration: 653 lambda_n: 1.0297322719737247 Loss: 0.20316120383054312\n",
      "Iteration: 654 lambda_n: 1.0150298226807302 Loss: 0.20303061908901218\n",
      "Iteration: 655 lambda_n: 0.9608490837100876 Loss: 0.20290211468424826\n",
      "Iteration: 656 lambda_n: 0.9577992585017985 Loss: 0.20278067042044362\n",
      "Iteration: 657 lambda_n: 0.9416412678123808 Loss: 0.2026598009052502\n",
      "Iteration: 658 lambda_n: 0.9033973195855223 Loss: 0.2025411554889322\n",
      "Iteration: 659 lambda_n: 0.9919573103775516 Loss: 0.20242750282033015\n",
      "Iteration: 660 lambda_n: 0.8832852103971982 Loss: 0.20230289241118254\n",
      "Iteration: 661 lambda_n: 0.9459127713211705 Loss: 0.2021921117354994\n",
      "Iteration: 662 lambda_n: 0.997483557167021 Loss: 0.20207364676630377\n",
      "Iteration: 663 lambda_n: 0.9987529134176458 Loss: 0.2019489151039108\n",
      "Iteration: 664 lambda_n: 0.9932912611920413 Loss: 0.2018242267246082\n",
      "Iteration: 665 lambda_n: 0.9889999585824097 Loss: 0.20170042089579132\n",
      "Iteration: 666 lambda_n: 0.943769157891161 Loss: 0.20157734825667442\n",
      "Iteration: 667 lambda_n: 1.0069751199278352 Loss: 0.20146009203631376\n",
      "Iteration: 668 lambda_n: 1.006700875630975 Loss: 0.20133517431860876\n",
      "Iteration: 669 lambda_n: 1.018752301886387 Loss: 0.20121049399603522\n",
      "Iteration: 670 lambda_n: 1.0107994608619404 Loss: 0.2010845264677339\n",
      "Iteration: 671 lambda_n: 1.007340822150364 Loss: 0.20095974795582794\n",
      "Iteration: 672 lambda_n: 0.9611716556382207 Loss: 0.20083559932619036\n",
      "Iteration: 673 lambda_n: 0.9277075318634028 Loss: 0.2007173331112373\n",
      "Iteration: 674 lambda_n: 0.9736670048134541 Loss: 0.20060336125485062\n",
      "Iteration: 675 lambda_n: 0.9074265543510178 Loss: 0.20048392223419448\n",
      "Iteration: 676 lambda_n: 0.9423739683155437 Loss: 0.20037278321674531\n",
      "Iteration: 677 lambda_n: 0.9775514591768693 Loss: 0.20025753276116948\n",
      "Iteration: 678 lambda_n: 1.0218340630903047 Loss: 0.20013816167580945\n",
      "Iteration: 679 lambda_n: 1.0272165966695248 Loss: 0.20001357958920746\n",
      "Iteration: 680 lambda_n: 0.8908313906929441 Loss: 0.1998885470592916\n",
      "Iteration: 681 lambda_n: 0.9461813170146431 Loss: 0.19978029371483536\n",
      "Iteration: 682 lambda_n: 0.9112663642466893 Loss: 0.19966547904237128\n",
      "Iteration: 683 lambda_n: 0.9769449847606588 Loss: 0.19955506895087385\n",
      "Iteration: 684 lambda_n: 0.970879921512374 Loss: 0.19943687454210213\n",
      "Iteration: 685 lambda_n: 0.9451771854514328 Loss: 0.19931959791666393\n",
      "Iteration: 686 lambda_n: 0.9476360822654113 Loss: 0.19920560359747044\n",
      "Iteration: 687 lambda_n: 0.9647541941561998 Loss: 0.1990914857982669\n",
      "Iteration: 688 lambda_n: 0.9204867337784385 Loss: 0.1989754829460443\n",
      "Iteration: 689 lambda_n: 1.0184251483545521 Loss: 0.19886497355671484\n",
      "Iteration: 690 lambda_n: 0.8983982325578255 Loss: 0.19874288663149922\n",
      "Iteration: 691 lambda_n: 0.9471820897310073 Loss: 0.198635363174334\n",
      "Iteration: 692 lambda_n: 0.9631336787369122 Loss: 0.1985221640542272\n",
      "Iteration: 693 lambda_n: 0.9038829157101198 Loss: 0.19840723274367944\n",
      "Iteration: 694 lambda_n: 0.9155986791364165 Loss: 0.1982995374549897\n",
      "Iteration: 695 lambda_n: 0.951333876987652 Loss: 0.19819060364651392\n",
      "Iteration: 696 lambda_n: 0.9780720929402357 Loss: 0.19807758365365194\n",
      "Iteration: 697 lambda_n: 0.9903135565843324 Loss: 0.19796156344177257\n",
      "Iteration: 698 lambda_n: 0.9293769040793441 Loss: 0.19784427423828196\n",
      "Iteration: 699 lambda_n: 0.9770030169954732 Loss: 0.19773437547151956\n",
      "Iteration: 700 lambda_n: 0.9406484071390319 Loss: 0.19761901601422463\n",
      "Iteration: 701 lambda_n: 0.9587943173645521 Loss: 0.19750812157904496\n",
      "Iteration: 702 lambda_n: 0.9635202085599017 Loss: 0.19739525701864266\n",
      "Iteration: 703 lambda_n: 0.927613902260592 Loss: 0.1972820089781823\n",
      "Iteration: 704 lambda_n: 0.9768588895414065 Loss: 0.19717314790947013\n",
      "Iteration: 705 lambda_n: 0.9483195015970111 Loss: 0.19705867667276616\n",
      "Iteration: 706 lambda_n: 0.9376897723792114 Loss: 0.1969477218850305\n",
      "Iteration: 707 lambda_n: 0.9853253583736511 Loss: 0.19683817573782217\n",
      "Iteration: 708 lambda_n: 1.0021978752082383 Loss: 0.1967232358286944\n",
      "Iteration: 709 lambda_n: 0.992705372891795 Loss: 0.19660651027556056\n",
      "Iteration: 710 lambda_n: 0.9088871550510446 Loss: 0.19649107374038624\n",
      "Iteration: 711 lambda_n: 0.9847101897322189 Loss: 0.19638554969035343\n",
      "Iteration: 712 lambda_n: 0.9291544260325667 Loss: 0.19627138708063144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 713 lambda_n: 1.0049375020501843 Loss: 0.1961638328105163\n",
      "Iteration: 714 lambda_n: 0.9726596944910284 Loss: 0.19604767738692663\n",
      "Iteration: 715 lambda_n: 0.9862219179588987 Loss: 0.19593543107568265\n",
      "Iteration: 716 lambda_n: 0.9082582907886192 Loss: 0.19582179445581083\n",
      "Iteration: 717 lambda_n: 0.9061612360303238 Loss: 0.19571730365342016\n",
      "Iteration: 718 lambda_n: 0.9365881851339697 Loss: 0.1956132033988035\n",
      "Iteration: 719 lambda_n: 0.9731714511851997 Loss: 0.19550576146513549\n",
      "Iteration: 720 lambda_n: 0.9381514621956277 Loss: 0.19539428769547224\n",
      "Iteration: 721 lambda_n: 0.8885522233872467 Loss: 0.19528698987258825\n",
      "Iteration: 722 lambda_n: 0.8807784930947795 Loss: 0.19518551463106174\n",
      "Iteration: 723 lambda_n: 0.9698224449119136 Loss: 0.1950850677226399\n",
      "Iteration: 724 lambda_n: 1.0240138993708257 Loss: 0.19497461946193054\n",
      "Iteration: 725 lambda_n: 1.0178361042820947 Loss: 0.19485817756296359\n",
      "Iteration: 726 lambda_n: 0.9401507847957754 Loss: 0.19474262426079617\n",
      "Iteration: 727 lambda_n: 0.9211699350200184 Loss: 0.19463606066733516\n",
      "Iteration: 728 lambda_n: 0.9107173750042367 Loss: 0.1945318024314283\n",
      "Iteration: 729 lambda_n: 0.9021124599177611 Loss: 0.19442887606502163\n",
      "Iteration: 730 lambda_n: 0.9863336256339754 Loss: 0.19432706769300873\n",
      "Iteration: 731 lambda_n: 0.9467307190252866 Loss: 0.1942159121466304\n",
      "Iteration: 732 lambda_n: 0.9915454324065337 Loss: 0.19410938429130042\n",
      "Iteration: 733 lambda_n: 0.8869863267004814 Loss: 0.19399797928582083\n",
      "Iteration: 734 lambda_n: 0.9208133340006682 Loss: 0.19389847618597328\n",
      "Iteration: 735 lambda_n: 1.0161980646836162 Loss: 0.19379532170108008\n",
      "Iteration: 736 lambda_n: 0.9047073772313495 Loss: 0.19368164589755563\n",
      "Iteration: 737 lambda_n: 0.8917081887698078 Loss: 0.19358060207480296\n",
      "Iteration: 738 lambda_n: 0.9488615855154788 Loss: 0.1934811506859343\n",
      "Iteration: 739 lambda_n: 0.9290732114965142 Loss: 0.1933754724814294\n",
      "Iteration: 740 lambda_n: 0.9526747838143026 Loss: 0.19327215121694313\n",
      "Iteration: 741 lambda_n: 0.9842338763391111 Loss: 0.1931663587582876\n",
      "Iteration: 742 lambda_n: 0.9036798254372269 Loss: 0.19305722407006476\n",
      "Iteration: 743 lambda_n: 0.9044865966565467 Loss: 0.19295717471778895\n",
      "Iteration: 744 lambda_n: 0.8988227381224211 Loss: 0.19285717688061166\n",
      "Iteration: 745 lambda_n: 0.954853547701959 Loss: 0.192757945016413\n",
      "Iteration: 746 lambda_n: 0.9295667615773286 Loss: 0.19265267476783765\n",
      "Iteration: 747 lambda_n: 0.9161391564730513 Loss: 0.1925503443009054\n",
      "Iteration: 748 lambda_n: 0.9519162220532235 Loss: 0.1924496375636724\n",
      "Iteration: 749 lambda_n: 0.9842379208318454 Loss: 0.1923451469874379\n",
      "Iteration: 750 lambda_n: 0.8976272112492574 Loss: 0.19223726820877005\n",
      "Iteration: 751 lambda_n: 0.9864330341438958 Loss: 0.19213903239352673\n",
      "Iteration: 752 lambda_n: 0.9905500421295517 Loss: 0.19203122827249114\n",
      "Iteration: 753 lambda_n: 0.9891698229041156 Loss: 0.19192313967970848\n",
      "Iteration: 754 lambda_n: 0.9599940465204159 Loss: 0.19181536725585924\n",
      "Iteration: 755 lambda_n: 0.953761718936761 Loss: 0.19171093362068728\n",
      "Iteration: 756 lambda_n: 0.991680681805388 Loss: 0.1916073320285336\n",
      "Iteration: 757 lambda_n: 0.8878054815720452 Loss: 0.19149977051133898\n",
      "Iteration: 758 lambda_n: 1.0240419709150752 Loss: 0.19140362291069274\n",
      "Iteration: 759 lambda_n: 0.9794471453308071 Loss: 0.1912928737721409\n",
      "Iteration: 760 lambda_n: 0.9186395341953109 Loss: 0.1911871148080977\n",
      "Iteration: 761 lambda_n: 0.9421649927902662 Loss: 0.19108807142915835\n",
      "Iteration: 762 lambda_n: 0.9976468101517202 Loss: 0.1909866356340634\n",
      "Iteration: 763 lambda_n: 0.949585831005096 Loss: 0.1908793827221903\n",
      "Iteration: 764 lambda_n: 0.9289108363466152 Loss: 0.19077745337690474\n",
      "Iteration: 765 lambda_n: 1.0234690201911365 Loss: 0.19067788902020916\n",
      "Iteration: 766 lambda_n: 0.8947710951465759 Loss: 0.19056834673110182\n",
      "Iteration: 767 lambda_n: 0.9873174688485098 Loss: 0.19047272936598741\n",
      "Iteration: 768 lambda_n: 0.8802902047119616 Loss: 0.1903673677553364\n",
      "Iteration: 769 lambda_n: 1.0130182232559921 Loss: 0.19027356973137874\n",
      "Iteration: 770 lambda_n: 0.9892525514746937 Loss: 0.19016577550628438\n",
      "Iteration: 771 lambda_n: 1.0037547337628843 Loss: 0.19006067377197422\n",
      "Iteration: 772 lambda_n: 0.9658766502414778 Loss: 0.18995419318947618\n",
      "Iteration: 773 lambda_n: 0.9315349412923746 Loss: 0.18985188837831862\n",
      "Iteration: 774 lambda_n: 0.9174652338645931 Loss: 0.189753366935006\n",
      "Iteration: 775 lambda_n: 0.950607229886778 Loss: 0.1896564719634181\n",
      "Iteration: 776 lambda_n: 0.9780761485793633 Loss: 0.18955621795998276\n",
      "Iteration: 777 lambda_n: 0.9749165915749075 Loss: 0.189453217159169\n",
      "Iteration: 778 lambda_n: 0.9050433777233815 Loss: 0.18935070267553575\n",
      "Iteration: 779 lambda_n: 0.96093220524725 Loss: 0.189255677114938\n",
      "Iteration: 780 lambda_n: 1.0192764493863986 Loss: 0.18915492319554394\n",
      "Iteration: 781 lambda_n: 0.8897163986105161 Loss: 0.18904820892969562\n",
      "Iteration: 782 lambda_n: 0.8795254867096299 Loss: 0.18895520361066853\n",
      "Iteration: 783 lambda_n: 0.9600622996415813 Loss: 0.18886338835730454\n",
      "Iteration: 784 lambda_n: 0.9168615271942241 Loss: 0.1887633004160826\n",
      "Iteration: 785 lambda_n: 0.9412017872020778 Loss: 0.18866785594931137\n",
      "Iteration: 786 lambda_n: 0.9732174679433464 Loss: 0.18857001461817052\n",
      "Iteration: 787 lambda_n: 1.0298678540527522 Loss: 0.18846899024301855\n",
      "Iteration: 788 lambda_n: 0.8876504104397253 Loss: 0.18836224385104247\n",
      "Iteration: 789 lambda_n: 0.932874542927428 Loss: 0.1882703820630931\n",
      "Iteration: 790 lambda_n: 0.8765306497195506 Loss: 0.1881739705203567\n",
      "Iteration: 791 lambda_n: 0.9515588813959626 Loss: 0.18808351029842926\n",
      "Iteration: 792 lambda_n: 0.9860734443854103 Loss: 0.18798543795839595\n",
      "Iteration: 793 lambda_n: 0.9590367596244271 Loss: 0.18788395530808372\n",
      "Iteration: 794 lambda_n: 0.9545014560735775 Loss: 0.18778540274835068\n",
      "Iteration: 795 lambda_n: 0.9573407987308379 Loss: 0.18768745888990004\n",
      "Iteration: 796 lambda_n: 0.9408166142956611 Loss: 0.1875893658161268\n",
      "Iteration: 797 lambda_n: 1.004233694416504 Loss: 0.18749310564060803\n",
      "Iteration: 798 lambda_n: 0.90939244181661 Loss: 0.18739050349783443\n",
      "Iteration: 799 lambda_n: 0.8975754096056625 Loss: 0.18729773216844833\n",
      "Iteration: 800 lambda_n: 0.977302930632236 Loss: 0.18720629227269064\n",
      "Iteration: 801 lambda_n: 0.9122056837888913 Loss: 0.1871068655695491\n",
      "Iteration: 802 lambda_n: 0.983290012985717 Loss: 0.18701419844802866\n",
      "Iteration: 803 lambda_n: 1.0031017301676817 Loss: 0.18691444803597215\n",
      "Iteration: 804 lambda_n: 0.9870070373456675 Loss: 0.18681283893949638\n",
      "Iteration: 805 lambda_n: 0.9285577935469884 Loss: 0.18671301143449465\n",
      "Iteration: 806 lambda_n: 1.0066064726092363 Loss: 0.18661923518867538\n",
      "Iteration: 807 lambda_n: 1.01996102984646 Loss: 0.18651771925273458\n",
      "Iteration: 808 lambda_n: 0.9307367114644962 Loss: 0.18641501255090243\n",
      "Iteration: 809 lambda_n: 0.9350259002101653 Loss: 0.18632143410349108\n",
      "Iteration: 810 lambda_n: 1.0147869012892086 Loss: 0.18622755610383296\n",
      "Iteration: 811 lambda_n: 0.9683444068740484 Loss: 0.18612581354790128\n",
      "Iteration: 812 lambda_n: 1.0033137923160933 Loss: 0.1860288752897455\n",
      "Iteration: 813 lambda_n: 0.994090947555783 Loss: 0.18592858260517586\n",
      "Iteration: 814 lambda_n: 0.9187613363972222 Loss: 0.18582936155466795\n",
      "Iteration: 815 lambda_n: 0.9921142372197189 Loss: 0.1857377958266587\n",
      "Iteration: 816 lambda_n: 0.8940091413953973 Loss: 0.18563905611604722\n",
      "Iteration: 817 lambda_n: 0.8899845636856853 Loss: 0.1855502123552328\n",
      "Iteration: 818 lambda_n: 0.8871186525855821 Loss: 0.18546188704066122\n",
      "Iteration: 819 lambda_n: 0.9826158411915271 Loss: 0.18537396352499622\n",
      "Iteration: 820 lambda_n: 1.0262198112475556 Loss: 0.18527670483107375\n",
      "Iteration: 821 lambda_n: 0.9852693852148965 Loss: 0.18517527978685194\n",
      "Iteration: 822 lambda_n: 0.9343163194133943 Loss: 0.1850780512821073\n",
      "Iteration: 823 lambda_n: 0.9377098848977135 Loss: 0.18498598661463198\n",
      "Iteration: 824 lambda_n: 0.9339363779348304 Loss: 0.18489371659881293\n",
      "Iteration: 825 lambda_n: 0.9118001216925543 Loss: 0.18480194658255758\n",
      "Iteration: 826 lambda_n: 0.8995401483799773 Loss: 0.1847124765612821\n",
      "Iteration: 827 lambda_n: 0.9694294832147623 Loss: 0.18462432962549033\n",
      "Iteration: 828 lambda_n: 1.018662477675726 Loss: 0.18452946194789321\n",
      "Iteration: 829 lambda_n: 0.9528580379809662 Loss: 0.18442992024700447\n",
      "Iteration: 830 lambda_n: 0.9611032485710862 Loss: 0.18433695021047283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 831 lambda_n: 0.9627085375735823 Loss: 0.1842433086022699\n",
      "Iteration: 832 lambda_n: 1.0005331218562445 Loss: 0.18414964850497167\n",
      "Iteration: 833 lambda_n: 0.8797567157007232 Loss: 0.18405250673999665\n",
      "Iteration: 834 lambda_n: 1.0128251470306822 Loss: 0.18396728199297827\n",
      "Iteration: 835 lambda_n: 0.9269342428709001 Loss: 0.18386936551054964\n",
      "Iteration: 836 lambda_n: 1.0226882808451685 Loss: 0.183779965586954\n",
      "Iteration: 837 lambda_n: 0.9769182433275091 Loss: 0.18368155028470765\n",
      "Iteration: 838 lambda_n: 0.8901885098362569 Loss: 0.1835877737619241\n",
      "Iteration: 839 lambda_n: 0.9558083810639108 Loss: 0.18350252836140984\n",
      "Iteration: 840 lambda_n: 0.9508841148442294 Loss: 0.18341120206251982\n",
      "Iteration: 841 lambda_n: 0.9117225538425608 Loss: 0.1833205634107182\n",
      "Iteration: 842 lambda_n: 0.9458436185484707 Loss: 0.18323386490120985\n",
      "Iteration: 843 lambda_n: 0.9722491217269101 Loss: 0.18314412795266224\n",
      "Iteration: 844 lambda_n: 0.8950035173514248 Loss: 0.18305210531565194\n",
      "Iteration: 845 lambda_n: 0.8855121488891764 Loss: 0.18296760081024113\n",
      "Iteration: 846 lambda_n: 0.9280958589460455 Loss: 0.1828841804372714\n",
      "Iteration: 847 lambda_n: 0.9709995590604349 Loss: 0.1827969428101523\n",
      "Iteration: 848 lambda_n: 1.0207029074601879 Loss: 0.18270588473674132\n",
      "Iteration: 849 lambda_n: 0.9123247621089142 Loss: 0.1826103981158217\n",
      "Iteration: 850 lambda_n: 1.0079577961713604 Loss: 0.1825252672349239\n",
      "Iteration: 851 lambda_n: 0.9435973634262257 Loss: 0.18243142644271335\n",
      "Iteration: 852 lambda_n: 0.940870298260676 Loss: 0.1823437971941769\n",
      "Iteration: 853 lambda_n: 0.8990197045894466 Loss: 0.18225662528239833\n",
      "Iteration: 854 lambda_n: 0.9148946545998629 Loss: 0.18217352420519833\n",
      "Iteration: 855 lambda_n: 1.0207867735229448 Loss: 0.18208914299697843\n",
      "Iteration: 856 lambda_n: 0.9924873049640387 Loss: 0.18199520720731688\n",
      "Iteration: 857 lambda_n: 0.9323047563405015 Loss: 0.18190410391146755\n",
      "Iteration: 858 lambda_n: 0.9909865102203662 Loss: 0.1818187322192026\n",
      "Iteration: 859 lambda_n: 0.9139430447500808 Loss: 0.18172819323305062\n",
      "Iteration: 860 lambda_n: 0.9741565465344535 Loss: 0.18164489387646457\n",
      "Iteration: 861 lambda_n: 0.9970417756622594 Loss: 0.18155630319202753\n",
      "Iteration: 862 lambda_n: 0.9147005767365451 Loss: 0.18146584470529917\n",
      "Iteration: 863 lambda_n: 0.9191765763926935 Loss: 0.18138305579408703\n",
      "Iteration: 864 lambda_n: 0.9527530321786359 Loss: 0.18130004448941445\n",
      "Iteration: 865 lambda_n: 0.982974948390279 Loss: 0.18121419035868835\n",
      "Iteration: 866 lambda_n: 1.0149985437778972 Loss: 0.18112581449747392\n",
      "Iteration: 867 lambda_n: 1.0092631363412041 Loss: 0.18103477319503675\n",
      "Iteration: 868 lambda_n: 1.0271717098345199 Loss: 0.18094446445111856\n",
      "Iteration: 869 lambda_n: 1.0133697769108356 Loss: 0.1808527728522327\n",
      "Iteration: 870 lambda_n: 0.9488970546034641 Loss: 0.18076253248847363\n",
      "Iteration: 871 lambda_n: 1.0086007803532517 Loss: 0.1806782346154566\n",
      "Iteration: 872 lambda_n: 1.0131793566253382 Loss: 0.1805888323558722\n",
      "Iteration: 873 lambda_n: 0.983958660861325 Loss: 0.18049923604447513\n",
      "Iteration: 874 lambda_n: 0.9970373524469053 Loss: 0.18041242912879957\n",
      "Iteration: 875 lambda_n: 0.9728292679689604 Loss: 0.18032466952311646\n",
      "Iteration: 876 lambda_n: 0.8811554440541285 Loss: 0.18023923841152595\n",
      "Iteration: 877 lambda_n: 1.0068774352390306 Loss: 0.18016203141148832\n",
      "Iteration: 878 lambda_n: 0.9796884573444841 Loss: 0.18007398801485414\n",
      "Iteration: 879 lambda_n: 0.985863274571611 Loss: 0.1799885200323569\n",
      "Iteration: 880 lambda_n: 0.8994614346007505 Loss: 0.1799027062311073\n",
      "Iteration: 881 lambda_n: 0.9670124912693914 Loss: 0.1798245890881099\n",
      "Iteration: 882 lambda_n: 0.9914835432186647 Loss: 0.17974077726156748\n",
      "Iteration: 883 lambda_n: 0.9821277685152245 Loss: 0.17965503307634698\n",
      "Iteration: 884 lambda_n: 0.9752996706067601 Loss: 0.17957028838976533\n",
      "Iteration: 885 lambda_n: 0.8878320229380826 Loss: 0.1794863191774317\n",
      "Iteration: 886 lambda_n: 0.9448108410595151 Loss: 0.17941004784680725\n",
      "Iteration: 887 lambda_n: 1.0217816501868762 Loss: 0.17932904323244736\n",
      "Iteration: 888 lambda_n: 0.929578457955407 Loss: 0.17924162458644266\n",
      "Iteration: 889 lambda_n: 0.9280777959034523 Loss: 0.17916227511158603\n",
      "Iteration: 890 lambda_n: 0.9328492740953037 Loss: 0.17908321721762435\n",
      "Iteration: 891 lambda_n: 0.9525191714649297 Loss: 0.1790039161195622\n",
      "Iteration: 892 lambda_n: 0.9825610413100946 Loss: 0.17892310964846372\n",
      "Iteration: 893 lambda_n: 0.9999584341224556 Loss: 0.1788399293789807\n",
      "Iteration: 894 lambda_n: 0.9990869921950294 Loss: 0.17875545880918742\n",
      "Iteration: 895 lambda_n: 0.9200936961230599 Loss: 0.17867124638008228\n",
      "Iteration: 896 lambda_n: 0.8819658676828022 Loss: 0.17859386088298115\n",
      "Iteration: 897 lambda_n: 0.9474349687496867 Loss: 0.178519830329526\n",
      "Iteration: 898 lambda_n: 0.9992451238396632 Loss: 0.17844045654174717\n",
      "Iteration: 899 lambda_n: 0.9339476393929896 Loss: 0.17835691369113663\n",
      "Iteration: 900 lambda_n: 1.0282538066782834 Loss: 0.17827899789342744\n",
      "Iteration: 901 lambda_n: 0.9638987089664947 Loss: 0.1781933867334789\n",
      "Iteration: 902 lambda_n: 0.8899709669592855 Loss: 0.1781133100521207\n",
      "Iteration: 903 lambda_n: 0.8805357756390866 Loss: 0.17803952672542786\n",
      "Iteration: 904 lambda_n: 0.9389503144135433 Loss: 0.17796666369509048\n",
      "Iteration: 905 lambda_n: 0.8826338795991092 Loss: 0.17788911212313246\n",
      "Iteration: 906 lambda_n: 0.9920376846681002 Loss: 0.17781635643304217\n",
      "Iteration: 907 lambda_n: 0.9771388551589343 Loss: 0.177734734973595\n",
      "Iteration: 908 lambda_n: 1.0161978794023558 Loss: 0.17765450682376457\n",
      "Iteration: 909 lambda_n: 1.027786167742403 Loss: 0.17757124255009815\n",
      "Iteration: 910 lambda_n: 0.9711181415887998 Loss: 0.17748720740034607\n",
      "Iteration: 911 lambda_n: 0.9635227461454381 Loss: 0.17740797517062773\n",
      "Iteration: 912 lambda_n: 1.0262422594588227 Loss: 0.17732952085788936\n",
      "Iteration: 913 lambda_n: 1.0271899795181878 Loss: 0.17724612620391497\n",
      "Iteration: 914 lambda_n: 0.9081686790911989 Loss: 0.17716283096124602\n",
      "Iteration: 915 lambda_n: 1.0061959952264468 Loss: 0.17708934211020008\n",
      "Iteration: 916 lambda_n: 1.0058620423725928 Loss: 0.17700807247577835\n",
      "Iteration: 917 lambda_n: 1.0030272090728964 Loss: 0.17692699651186758\n",
      "Iteration: 918 lambda_n: 0.9085720731516289 Loss: 0.17684631431039152\n",
      "Iteration: 919 lambda_n: 0.9690462547857164 Loss: 0.1767733781570883\n",
      "Iteration: 920 lambda_n: 0.9146732120809816 Loss: 0.17669573027351268\n",
      "Iteration: 921 lambda_n: 0.9504492337982251 Loss: 0.1766225819734098\n",
      "Iteration: 922 lambda_n: 1.013259711064866 Loss: 0.17654671216546197\n",
      "Iteration: 923 lambda_n: 0.9614586150686706 Loss: 0.1764659824431886\n",
      "Iteration: 924 lambda_n: 0.9151177976350344 Loss: 0.1763895344451713\n",
      "Iteration: 925 lambda_n: 0.9043009947638639 Loss: 0.17631690996245045\n",
      "Iteration: 926 lambda_n: 0.9060734421147328 Loss: 0.17624527392144726\n",
      "Iteration: 927 lambda_n: 0.8900160945118945 Loss: 0.1761736256051106\n",
      "Iteration: 928 lambda_n: 1.0224703718700021 Loss: 0.17610337247961086\n",
      "Iteration: 929 lambda_n: 1.0012697868244347 Loss: 0.1760228054920754\n",
      "Iteration: 930 lambda_n: 0.9048407432970039 Loss: 0.17594406675443616\n",
      "Iteration: 931 lambda_n: 1.0291261550584723 Loss: 0.17587304967635828\n",
      "Iteration: 932 lambda_n: 0.9960144081248593 Loss: 0.1757924203622842\n",
      "Iteration: 933 lambda_n: 0.9568177583933476 Loss: 0.17571454068260772\n",
      "Iteration: 934 lambda_n: 0.9175701072029492 Loss: 0.1756398695208615\n",
      "Iteration: 935 lambda_n: 0.8973138629427199 Loss: 0.17556839294690196\n",
      "Iteration: 936 lambda_n: 0.9005992427045725 Loss: 0.17549861717653814\n",
      "Iteration: 937 lambda_n: 0.9253932043872729 Loss: 0.1754287060278161\n",
      "Iteration: 938 lambda_n: 0.9262214386305622 Loss: 0.17535699351629916\n",
      "Iteration: 939 lambda_n: 1.025532577135821 Loss: 0.1752853429750908\n",
      "Iteration: 940 lambda_n: 0.9737931816400863 Loss: 0.17520614938270115\n",
      "Iteration: 941 lambda_n: 0.8880067375631892 Loss: 0.17513109657975093\n",
      "Iteration: 942 lambda_n: 0.8819637142021036 Loss: 0.17506278064950065\n",
      "Iteration: 943 lambda_n: 1.010438720404114 Loss: 0.17499504253264955\n",
      "Iteration: 944 lambda_n: 0.982728466164837 Loss: 0.17491756537687325\n",
      "Iteration: 945 lambda_n: 0.9380248856012856 Loss: 0.17484235470525228\n",
      "Iteration: 946 lambda_n: 1.0135485863520952 Loss: 0.17477069614430527\n",
      "Iteration: 947 lambda_n: 0.9312092680361415 Loss: 0.17469340272803469\n",
      "Iteration: 948 lambda_n: 0.989073704916857 Loss: 0.17462252101050066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 949 lambda_n: 0.9786635095730886 Loss: 0.1745473637999956\n",
      "Iteration: 950 lambda_n: 0.8926420467053446 Loss: 0.1744731323394476\n",
      "Iteration: 951 lambda_n: 1.0182609789922648 Loss: 0.17440554630677596\n",
      "Iteration: 952 lambda_n: 0.9888023766356578 Loss: 0.17432857472149496\n",
      "Iteration: 953 lambda_n: 0.9954356257073343 Loss: 0.17425396790044415\n",
      "Iteration: 954 lambda_n: 1.016071693496748 Loss: 0.1741789948417664\n",
      "Iteration: 955 lambda_n: 0.9888298984129656 Loss: 0.17410260482366272\n",
      "Iteration: 956 lambda_n: 0.9812954857416271 Loss: 0.174028398392394\n",
      "Iteration: 957 lambda_n: 1.0085340223176935 Loss: 0.17395488760522637\n",
      "Iteration: 958 lambda_n: 0.985395570205258 Loss: 0.17387946857275127\n",
      "Iteration: 959 lambda_n: 1.0159056490897274 Loss: 0.17380591180391594\n",
      "Iteration: 960 lambda_n: 1.0264129259617976 Loss: 0.17373020994101393\n",
      "Iteration: 961 lambda_n: 0.9008425562484108 Loss: 0.17365386223008042\n",
      "Iteration: 962 lambda_n: 0.9923499709104262 Loss: 0.17358697536161094\n",
      "Iteration: 963 lambda_n: 0.9595773093249736 Loss: 0.17351341070850482\n",
      "Iteration: 964 lambda_n: 0.9912607830203796 Loss: 0.17344239871532988\n",
      "Iteration: 965 lambda_n: 0.9618549385558834 Loss: 0.1733691646363658\n",
      "Iteration: 966 lambda_n: 0.9093682663701234 Loss: 0.17329822513146245\n",
      "Iteration: 967 lambda_n: 0.8804116987699393 Loss: 0.17323126800784297\n",
      "Iteration: 968 lambda_n: 0.883718355721846 Loss: 0.17316654443006504\n",
      "Iteration: 969 lambda_n: 0.9617125579918078 Loss: 0.1731016759887772\n",
      "Iteration: 970 lambda_n: 0.8761741128447481 Loss: 0.17303118950588064\n",
      "Iteration: 971 lambda_n: 0.960209296385864 Loss: 0.17296707751324392\n",
      "Iteration: 972 lambda_n: 1.0093768598603305 Loss: 0.17289692141655064\n",
      "Iteration: 973 lambda_n: 0.9412265149154365 Loss: 0.17282329321517553\n",
      "Iteration: 974 lambda_n: 0.8864020852503404 Loss: 0.17275475309257954\n",
      "Iteration: 975 lambda_n: 0.9961982182917856 Loss: 0.17269030743600206\n",
      "Iteration: 976 lambda_n: 0.9168751861279079 Loss: 0.172617987199278\n",
      "Iteration: 977 lambda_n: 0.9758597921572915 Loss: 0.17255153622862698\n",
      "Iteration: 978 lambda_n: 1.015968323719042 Loss: 0.17248091864038087\n",
      "Iteration: 979 lambda_n: 0.8843907001808389 Loss: 0.1724075180030915\n",
      "Iteration: 980 lambda_n: 0.9576035751277143 Loss: 0.17234373059312036\n",
      "Iteration: 981 lambda_n: 1.0062110625699539 Loss: 0.17227476369686293\n",
      "Iteration: 982 lambda_n: 0.9491183461086774 Loss: 0.1722024104230008\n",
      "Iteration: 983 lambda_n: 0.879496840411712 Loss: 0.17213427493285244\n",
      "Iteration: 984 lambda_n: 0.8801925954712173 Loss: 0.1720712351677878\n",
      "Iteration: 985 lambda_n: 0.8951336278058275 Loss: 0.1720082359074875\n",
      "Iteration: 986 lambda_n: 0.9334137925137812 Loss: 0.17194425886058942\n",
      "Iteration: 987 lambda_n: 0.9338291766420216 Loss: 0.17187764263333388\n",
      "Iteration: 988 lambda_n: 0.8896113186719763 Loss: 0.17181109714198375\n",
      "Iteration: 989 lambda_n: 0.8945000602386776 Loss: 0.17174779773644175\n",
      "Iteration: 990 lambda_n: 0.893827101627188 Loss: 0.17168424125971007\n",
      "Iteration: 991 lambda_n: 0.9399555888017213 Loss: 0.17162082338030607\n",
      "Iteration: 992 lambda_n: 0.884182311794918 Loss: 0.1715542277473319\n",
      "Iteration: 993 lambda_n: 0.9377960683765841 Loss: 0.17149167698386106\n",
      "Iteration: 994 lambda_n: 0.8845523097525864 Loss: 0.1714254263741825\n",
      "Iteration: 995 lambda_n: 0.916693357382644 Loss: 0.17136302948362908\n",
      "Iteration: 996 lambda_n: 0.9070780681251978 Loss: 0.17129845539934763\n",
      "Iteration: 997 lambda_n: 1.0037689628604864 Loss: 0.17123465044543623\n",
      "Iteration: 998 lambda_n: 1.0291807163566418 Loss: 0.17116414455278645\n",
      "Iteration: 999 lambda_n: 0.9811935235233072 Loss: 0.1710919668392059\n",
      "Iteration: 1000 lambda_n: 0.9641348403611619 Loss: 0.17102326430920448\n",
      "Iteration: 1001 lambda_n: 0.9547857365658325 Loss: 0.17095585861736443\n",
      "Iteration: 1002 lambda_n: 1.0285106699135562 Loss: 0.17088920572459465\n",
      "Iteration: 1003 lambda_n: 0.9362941874390452 Loss: 0.17081751171368675\n",
      "Iteration: 1004 lambda_n: 0.9672646350723273 Loss: 0.17075234829157057\n",
      "Iteration: 1005 lambda_n: 1.0215121938948413 Loss: 0.17068512566966335\n",
      "Iteration: 1006 lambda_n: 0.9108478891799953 Loss: 0.17061423756814723\n",
      "Iteration: 1007 lambda_n: 0.9334115156459208 Loss: 0.17055112655440677\n",
      "Iteration: 1008 lambda_n: 0.8832444870679506 Loss: 0.1704865411666055\n",
      "Iteration: 1009 lambda_n: 0.9262462343178075 Loss: 0.1704255126893685\n",
      "Iteration: 1010 lambda_n: 0.8784858310416875 Loss: 0.17036159790742147\n",
      "Iteration: 1011 lambda_n: 1.0093102782529602 Loss: 0.17030106262153819\n",
      "Iteration: 1012 lambda_n: 1.0159254259929507 Loss: 0.170231603918247\n",
      "Iteration: 1013 lambda_n: 0.8967313407227697 Loss: 0.17016179486366262\n",
      "Iteration: 1014 lambda_n: 1.0006751264899942 Loss: 0.17010026854473823\n",
      "Iteration: 1015 lambda_n: 1.0127595964731189 Loss: 0.17003170165290243\n",
      "Iteration: 1016 lambda_n: 1.0068300170646551 Loss: 0.1699624089127882\n",
      "Iteration: 1017 lambda_n: 0.9365668632261902 Loss: 0.16989362411554595\n",
      "Iteration: 1018 lambda_n: 1.0222510335699797 Loss: 0.16982973344291913\n",
      "Iteration: 1019 lambda_n: 0.9615938472933133 Loss: 0.16976009292236532\n",
      "Iteration: 1020 lambda_n: 0.8789279812807341 Loss: 0.16969468160324952\n",
      "Iteration: 1021 lambda_n: 0.889789569220892 Loss: 0.16963497639465058\n",
      "Iteration: 1022 lambda_n: 0.8864234319124454 Loss: 0.16957460992869308\n",
      "Iteration: 1023 lambda_n: 0.9983473610608894 Loss: 0.16951454867238708\n",
      "Iteration: 1024 lambda_n: 1.0058720149567235 Loss: 0.16944699002493763\n",
      "Iteration: 1025 lambda_n: 0.8985700008785703 Loss: 0.16937901919648163\n",
      "Iteration: 1026 lambda_n: 0.9787955512987861 Loss: 0.16931838574338473\n",
      "Iteration: 1027 lambda_n: 0.9341631492007408 Loss: 0.1692524232284793\n",
      "Iteration: 1028 lambda_n: 0.9357207589806261 Loss: 0.16918955547758763\n",
      "Iteration: 1029 lambda_n: 0.971195835239642 Loss: 0.16912666573471874\n",
      "Iteration: 1030 lambda_n: 0.9521304530701619 Loss: 0.16906147754692524\n",
      "Iteration: 1031 lambda_n: 0.8765350643817155 Loss: 0.16899765581838117\n",
      "Iteration: 1032 lambda_n: 0.9270064245817277 Loss: 0.16893897904620805\n",
      "Iteration: 1033 lambda_n: 0.951782137505853 Loss: 0.16887699936876477\n",
      "Iteration: 1034 lambda_n: 0.97430244494671 Loss: 0.168813444981904\n",
      "Iteration: 1035 lambda_n: 0.92168317635283 Loss: 0.16874847239091786\n",
      "Iteration: 1036 lambda_n: 0.9980050591277396 Loss: 0.16868709102881455\n",
      "Iteration: 1037 lambda_n: 0.8826228222178213 Loss: 0.16862071112643756\n",
      "Iteration: 1038 lambda_n: 0.9316070450286713 Loss: 0.16856208534825617\n",
      "Iteration: 1039 lambda_n: 0.9079643701662091 Loss: 0.16850028048628532\n",
      "Iteration: 1040 lambda_n: 0.9384814791369642 Loss: 0.16844012027314995\n",
      "Iteration: 1041 lambda_n: 0.9274113948958274 Loss: 0.16837801457386084\n",
      "Iteration: 1042 lambda_n: 0.9892408463478833 Loss: 0.16831671914829716\n",
      "Iteration: 1043 lambda_n: 0.9545196107518052 Loss: 0.1682514189916526\n",
      "Iteration: 1044 lambda_n: 0.9509806917787741 Loss: 0.1681884942532488\n",
      "Iteration: 1045 lambda_n: 0.9508256176631459 Loss: 0.1681258827531698\n",
      "Iteration: 1046 lambda_n: 0.90263831912871 Loss: 0.1680633607416795\n",
      "Iteration: 1047 lambda_n: 0.9874244603433258 Loss: 0.1680040820694108\n",
      "Iteration: 1048 lambda_n: 1.0064174220893054 Loss: 0.16793931298109593\n",
      "Iteration: 1049 lambda_n: 0.8845521834517783 Loss: 0.16787338409835062\n",
      "Iteration: 1050 lambda_n: 0.9389538884532211 Loss: 0.16781551470543524\n",
      "Iteration: 1051 lambda_n: 1.020821494611744 Loss: 0.16775415756689052\n",
      "Iteration: 1052 lambda_n: 0.8942852601973728 Loss: 0.167687532748152\n",
      "Iteration: 1053 lambda_n: 0.8815031381730919 Loss: 0.16762924358994166\n",
      "Iteration: 1054 lambda_n: 1.024329848321178 Loss: 0.1675718541621481\n",
      "Iteration: 1055 lambda_n: 0.9473835755257324 Loss: 0.16750524263058908\n",
      "Iteration: 1056 lambda_n: 0.9337253362720603 Loss: 0.16744371596071272\n",
      "Iteration: 1057 lambda_n: 0.946628477238993 Loss: 0.16738315005051352\n",
      "Iteration: 1058 lambda_n: 0.9530748130519584 Loss: 0.1673218206177757\n",
      "Iteration: 1059 lambda_n: 0.9422384851024765 Loss: 0.16726014815216025\n",
      "Iteration: 1060 lambda_n: 0.974035758903297 Loss: 0.16719925076867126\n",
      "Iteration: 1061 lambda_n: 0.9457162193847723 Loss: 0.1671363736104589\n",
      "Iteration: 1062 lambda_n: 0.9278988395614971 Loss: 0.16707539961662693\n",
      "Iteration: 1063 lambda_n: 0.9733029607676353 Loss: 0.16701564557568854\n",
      "Iteration: 1064 lambda_n: 0.9806402910080729 Loss: 0.16695304079361958\n",
      "Iteration: 1065 lambda_n: 0.9220442108709449 Loss: 0.1668900409039073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1066 lambda_n: 0.9332471197539874 Loss: 0.16683087769880434\n",
      "Iteration: 1067 lambda_n: 0.9894915945685494 Loss: 0.16677106431715674\n",
      "Iteration: 1068 lambda_n: 1.0015981468749824 Loss: 0.1667077196494668\n",
      "Iteration: 1069 lambda_n: 0.995951291120518 Loss: 0.16664367835034924\n",
      "Iteration: 1070 lambda_n: 0.9635859657160306 Loss: 0.1665800765930264\n",
      "Iteration: 1071 lambda_n: 0.8841972674298477 Loss: 0.1665186167647804\n",
      "Iteration: 1072 lambda_n: 0.9639768092114559 Loss: 0.16646228673202548\n",
      "Iteration: 1073 lambda_n: 0.9630401097835636 Loss: 0.16640094056318538\n",
      "Iteration: 1074 lambda_n: 1.0021197621899964 Loss: 0.16633972576817885\n",
      "Iteration: 1075 lambda_n: 1.0270842014784929 Loss: 0.16627610131647114\n",
      "Iteration: 1076 lambda_n: 0.8908367541550876 Loss: 0.16621097082450828\n",
      "Iteration: 1077 lambda_n: 0.9729607466959267 Loss: 0.16615454956107456\n",
      "Iteration: 1078 lambda_n: 0.9915393059803826 Loss: 0.16609299302609917\n",
      "Iteration: 1079 lambda_n: 0.9316450396873256 Loss: 0.16603033406863593\n",
      "Iteration: 1080 lambda_n: 0.9110047250082389 Loss: 0.1659715293778038\n",
      "Iteration: 1081 lambda_n: 0.9246501535683919 Loss: 0.16591409100728965\n",
      "Iteration: 1082 lambda_n: 1.0060140257704542 Loss: 0.16585585518034293\n",
      "Iteration: 1083 lambda_n: 0.9412647171168479 Loss: 0.16579256431791117\n",
      "Iteration: 1084 lambda_n: 0.983085475166161 Loss: 0.16573341683747456\n",
      "Iteration: 1085 lambda_n: 0.9124891847086797 Loss: 0.16567170969096778\n",
      "Iteration: 1086 lambda_n: 0.9747081800464867 Loss: 0.16561449934481054\n",
      "Iteration: 1087 lambda_n: 0.9623148547528478 Loss: 0.16555345318055892\n",
      "Iteration: 1088 lambda_n: 0.8988831762226221 Loss: 0.1654932513548659\n",
      "Iteration: 1089 lambda_n: 1.0010495472449 Loss: 0.16543708018323625\n",
      "Iteration: 1090 lambda_n: 0.8839247321597016 Loss: 0.16537458982569592\n",
      "Iteration: 1091 lambda_n: 1.019306228671347 Loss: 0.1653194741139781\n",
      "Iteration: 1092 lambda_n: 0.9617665504494148 Loss: 0.16525598174303457\n",
      "Iteration: 1093 lambda_n: 0.9899564638482358 Loss: 0.1651961430752799\n",
      "Iteration: 1094 lambda_n: 0.8875282659892034 Loss: 0.16513461803655663\n",
      "Iteration: 1095 lambda_n: 0.9949569573158328 Loss: 0.1650795204868541\n",
      "Iteration: 1096 lambda_n: 0.9867535238942714 Loss: 0.1650178161550507\n",
      "Iteration: 1097 lambda_n: 0.8834563331293348 Loss: 0.1649566892088182\n",
      "Iteration: 1098 lambda_n: 0.9898029814243151 Loss: 0.16490202165218468\n",
      "Iteration: 1099 lambda_n: 1.0054899802700057 Loss: 0.1648408344259368\n",
      "Iteration: 1100 lambda_n: 0.9451960286308927 Loss: 0.1647787462379596\n",
      "Iteration: 1101 lambda_n: 1.003393674817044 Loss: 0.16472044628916468\n",
      "Iteration: 1102 lambda_n: 0.8994888559550338 Loss: 0.16465862179772528\n",
      "Iteration: 1103 lambda_n: 0.9500939786216167 Loss: 0.16460326063011751\n",
      "Iteration: 1104 lambda_n: 0.8813866019455939 Loss: 0.16454484299686123\n",
      "Iteration: 1105 lambda_n: 0.9609943267325824 Loss: 0.1644907062989673\n",
      "Iteration: 1106 lambda_n: 1.0117116274363653 Loss: 0.16443173718087367\n",
      "Iteration: 1107 lambda_n: 1.011644583777613 Loss: 0.16436972126625543\n",
      "Iteration: 1108 lambda_n: 1.0102002377543555 Loss: 0.1643077777803217\n",
      "Iteration: 1109 lambda_n: 0.9376057461724917 Loss: 0.16424599064198675\n",
      "Iteration: 1110 lambda_n: 0.9670186754763759 Loss: 0.1641887060515404\n",
      "Iteration: 1111 lambda_n: 1.0110258578813118 Loss: 0.16412968424830313\n",
      "Iteration: 1112 lambda_n: 0.9887993534317847 Loss: 0.16406804076255876\n",
      "Iteration: 1113 lambda_n: 0.9789979228344722 Loss: 0.16400781768220898\n",
      "Iteration: 1114 lambda_n: 0.9639748965652245 Loss: 0.16394825448442277\n",
      "Iteration: 1115 lambda_n: 0.9238958901158553 Loss: 0.1638896663654887\n",
      "Iteration: 1116 lambda_n: 0.9050528249570862 Loss: 0.16383357145537042\n",
      "Iteration: 1117 lambda_n: 0.9722281058320299 Loss: 0.16377867424841008\n",
      "Iteration: 1118 lambda_n: 0.9573053628563796 Loss: 0.16371975893475424\n",
      "Iteration: 1119 lambda_n: 1.0298952763860143 Loss: 0.16366180715781162\n",
      "Iteration: 1120 lambda_n: 1.026537428796917 Loss: 0.16359952385697968\n",
      "Iteration: 1121 lambda_n: 0.8909898994311908 Loss: 0.16353751040707082\n",
      "Iteration: 1122 lambda_n: 0.9951073636291274 Loss: 0.16348374254232628\n",
      "Iteration: 1123 lambda_n: 0.9075069975983131 Loss: 0.16342374746409635\n",
      "Iteration: 1124 lambda_n: 0.8755695821582417 Loss: 0.16336908988207607\n",
      "Iteration: 1125 lambda_n: 1.013014250470796 Loss: 0.16331640509976694\n",
      "Iteration: 1126 lambda_n: 0.8816024293919672 Loss: 0.16325550536089428\n",
      "Iteration: 1127 lambda_n: 0.9274248853818454 Loss: 0.16320256041317638\n",
      "Iteration: 1128 lambda_n: 0.9183180161707694 Loss: 0.16314691391788327\n",
      "Iteration: 1129 lambda_n: 0.9939799956687992 Loss: 0.16309186590072897\n",
      "Iteration: 1130 lambda_n: 0.881963055933252 Loss: 0.16303233823502394\n",
      "Iteration: 1131 lambda_n: 0.9309959087134396 Loss: 0.16297957191624637\n",
      "Iteration: 1132 lambda_n: 0.9261174153425012 Loss: 0.16292392182017318\n",
      "Iteration: 1133 lambda_n: 0.9801851421345016 Loss: 0.1628686152258949\n",
      "Iteration: 1134 lambda_n: 0.9716891689243259 Loss: 0.16281013439142927\n",
      "Iteration: 1135 lambda_n: 1.0178197092506103 Loss: 0.16275221731613423\n",
      "Iteration: 1136 lambda_n: 0.8805954475889597 Loss: 0.1626916096478108\n",
      "Iteration: 1137 lambda_n: 0.9370959961134347 Loss: 0.16263922589750907\n",
      "Iteration: 1138 lambda_n: 1.0027763986458476 Loss: 0.16258352997945125\n",
      "Iteration: 1139 lambda_n: 0.9817257398949648 Loss: 0.16252398583523517\n",
      "Iteration: 1140 lambda_n: 0.9135136213476043 Loss: 0.16246574923197796\n",
      "Iteration: 1141 lambda_n: 0.9719739321013032 Loss: 0.16241161109004987\n",
      "Iteration: 1142 lambda_n: 0.9779518792293004 Loss: 0.16235406013399684\n",
      "Iteration: 1143 lambda_n: 1.004641821275488 Loss: 0.16229621023033342\n",
      "Iteration: 1144 lambda_n: 1.021739224001424 Loss: 0.16223683820788168\n",
      "Iteration: 1145 lambda_n: 0.9771829000140837 Loss: 0.16217651473420022\n",
      "Iteration: 1146 lambda_n: 0.9899869329500223 Loss: 0.1621188787734491\n",
      "Iteration: 1147 lambda_n: 0.9822346169374946 Loss: 0.16206054270614362\n",
      "Iteration: 1148 lambda_n: 1.0096077515610744 Loss: 0.1620027185432139\n",
      "Iteration: 1149 lambda_n: 0.9930527918287445 Loss: 0.16194333900139152\n",
      "Iteration: 1150 lambda_n: 0.9305077715085569 Loss: 0.1618849894455351\n",
      "Iteration: 1151 lambda_n: 1.0292652346974471 Loss: 0.16183036643458254\n",
      "Iteration: 1152 lambda_n: 0.8789898451499284 Loss: 0.16176999989147092\n",
      "Iteration: 1153 lambda_n: 0.9798822635175989 Loss: 0.16171849682587425\n",
      "Iteration: 1154 lambda_n: 0.8835052321622229 Loss: 0.16166113012015423\n",
      "Iteration: 1155 lambda_n: 0.9426583896264945 Loss: 0.16160945319081976\n",
      "Iteration: 1156 lambda_n: 1.0226895507522715 Loss: 0.16155436227332187\n",
      "Iteration: 1157 lambda_n: 0.9972739135580408 Loss: 0.16149464720327603\n",
      "Iteration: 1158 lambda_n: 0.9382547292799128 Loss: 0.16143647166491276\n",
      "Iteration: 1159 lambda_n: 1.0022776765776056 Loss: 0.16138178958832888\n",
      "Iteration: 1160 lambda_n: 0.8921680381609193 Loss: 0.1613234272845627\n",
      "Iteration: 1161 lambda_n: 0.9222176198011235 Loss: 0.16127152445574788\n",
      "Iteration: 1162 lambda_n: 1.0231741237714815 Loss: 0.16121791770061167\n",
      "Iteration: 1163 lambda_n: 1.0066860172805934 Loss: 0.16115849334213955\n",
      "Iteration: 1164 lambda_n: 1.0189391473435196 Loss: 0.16110008139266108\n",
      "Iteration: 1165 lambda_n: 1.000557375215188 Loss: 0.1610410129289492\n",
      "Iteration: 1166 lambda_n: 0.924038781620138 Loss: 0.16098306387278152\n",
      "Iteration: 1167 lambda_n: 0.9197367591990391 Loss: 0.1609295949364697\n",
      "Iteration: 1168 lambda_n: 1.0148404092850973 Loss: 0.16087641946999628\n",
      "Iteration: 1169 lambda_n: 0.9496529192008998 Loss: 0.16081779458319076\n",
      "Iteration: 1170 lambda_n: 0.980264228312391 Loss: 0.16076298535739497\n",
      "Iteration: 1171 lambda_n: 0.9317439488357231 Loss: 0.16070645775709216\n",
      "Iteration: 1172 lambda_n: 0.88505480315123 Loss: 0.16065277512065815\n",
      "Iteration: 1173 lambda_n: 1.0097831644660022 Loss: 0.1606018247684976\n",
      "Iteration: 1174 lambda_n: 0.9338804512228952 Loss: 0.16054374037217392\n",
      "Iteration: 1175 lambda_n: 0.9938966431020357 Loss: 0.1604900699483127\n",
      "Iteration: 1176 lambda_n: 0.9521253416855208 Loss: 0.1604329978098871\n",
      "Iteration: 1177 lambda_n: 0.9180876435718408 Loss: 0.1603783721256581\n",
      "Iteration: 1178 lambda_n: 1.0025244059794534 Loss: 0.16032574330216307\n",
      "Iteration: 1179 lambda_n: 0.8843870752033325 Loss: 0.16026832080166684\n",
      "Iteration: 1180 lambda_n: 0.9293269724978971 Loss: 0.1602177090500747\n",
      "Iteration: 1181 lambda_n: 0.9432615555508701 Loss: 0.16016456667334725\n",
      "Iteration: 1182 lambda_n: 0.9502310970504222 Loss: 0.16011067114742844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1183 lambda_n: 0.9333475213066824 Loss: 0.16005642188239197\n",
      "Iteration: 1184 lambda_n: 1.0146567113717064 Loss: 0.16000318028931587\n",
      "Iteration: 1185 lambda_n: 0.9607469446359331 Loss: 0.159945347445501\n",
      "Iteration: 1186 lambda_n: 1.0249357519706943 Loss: 0.15989063497452216\n",
      "Iteration: 1187 lambda_n: 0.9123519619162428 Loss: 0.15983231545448898\n",
      "Iteration: 1188 lambda_n: 0.9224583530266665 Loss: 0.1597804472091196\n",
      "Iteration: 1189 lambda_n: 0.9747588410560417 Loss: 0.15972804526806803\n",
      "Iteration: 1190 lambda_n: 0.8906303402097631 Loss: 0.15967271595552351\n",
      "Iteration: 1191 lambda_n: 0.8839332326405492 Loss: 0.15962220351497597\n",
      "Iteration: 1192 lambda_n: 1.015094989228235 Loss: 0.15957210867545757\n",
      "Iteration: 1193 lambda_n: 0.9976207901006039 Loss: 0.15951462395096297\n",
      "Iteration: 1194 lambda_n: 0.969379135537093 Loss: 0.1594581770199873\n",
      "Iteration: 1195 lambda_n: 0.8919263996782973 Loss: 0.1594033738925925\n",
      "Iteration: 1196 lambda_n: 1.022569719572051 Loss: 0.15935299017802057\n",
      "Iteration: 1197 lambda_n: 0.9649046690179006 Loss: 0.15929527008370106\n",
      "Iteration: 1198 lambda_n: 0.9628962151833941 Loss: 0.15924085114133074\n",
      "Iteration: 1199 lambda_n: 0.899158046826559 Loss: 0.15918658897351506\n",
      "Iteration: 1200 lambda_n: 0.9010181828865321 Loss: 0.15913595884496406\n",
      "Iteration: 1201 lambda_n: 1.0076637687661905 Loss: 0.1590852616503018\n",
      "Iteration: 1202 lambda_n: 1.0034968515066713 Loss: 0.1590286063302965\n",
      "Iteration: 1203 lambda_n: 0.9444832811918006 Loss: 0.15897223198503763\n",
      "Iteration: 1204 lambda_n: 0.9342654492651477 Loss: 0.1589192163183849\n",
      "Iteration: 1205 lambda_n: 0.9126028273145063 Loss: 0.158866814623892\n",
      "Iteration: 1206 lambda_n: 0.9089456459656109 Loss: 0.1588156668461837\n",
      "Iteration: 1207 lambda_n: 0.9247285361530921 Loss: 0.15876476180216148\n",
      "Iteration: 1208 lambda_n: 1.0265029045518128 Loss: 0.15871301104722302\n",
      "Iteration: 1209 lambda_n: 0.9476122838399879 Loss: 0.15865560798706327\n",
      "Iteration: 1210 lambda_n: 0.9643043442503849 Loss: 0.1586026601630362\n",
      "Iteration: 1211 lambda_n: 0.8964938384478336 Loss: 0.15854882077653948\n",
      "Iteration: 1212 lambda_n: 0.9774746720884064 Loss: 0.1584988058979153\n",
      "Iteration: 1213 lambda_n: 0.9955763875550857 Loss: 0.1584443125210783\n",
      "Iteration: 1214 lambda_n: 0.98887937906365 Loss: 0.15838885333067984\n",
      "Iteration: 1215 lambda_n: 0.9625079528482257 Loss: 0.15833381080779302\n",
      "Iteration: 1216 lambda_n: 0.9612427702996894 Loss: 0.15828027809976952\n",
      "Iteration: 1217 lambda_n: 0.967661280256944 Loss: 0.15822685646754317\n",
      "Iteration: 1218 lambda_n: 0.8813992082735146 Loss: 0.1581731189366211\n",
      "Iteration: 1219 lambda_n: 0.9818818885339746 Loss: 0.1581242088391378\n",
      "Iteration: 1220 lambda_n: 1.0117497080157722 Loss: 0.1580697608600932\n",
      "Iteration: 1221 lambda_n: 0.9094909408509785 Loss: 0.15801369987373734\n",
      "Iteration: 1222 lambda_n: 0.955165053501059 Loss: 0.15796334452639285\n",
      "Iteration: 1223 lambda_n: 0.8836931710637013 Loss: 0.15791049797517698\n",
      "Iteration: 1224 lambda_n: 0.9094186284198081 Loss: 0.157861641813627\n",
      "Iteration: 1225 lambda_n: 0.9244423924379614 Loss: 0.15781139789591347\n",
      "Iteration: 1226 lambda_n: 0.914981227958788 Loss: 0.15776035989258463\n",
      "Iteration: 1227 lambda_n: 0.906532976280976 Loss: 0.1577098802139945\n",
      "Iteration: 1228 lambda_n: 0.9152629174067127 Loss: 0.15765990179840633\n",
      "Iteration: 1229 lambda_n: 0.9096153547678996 Loss: 0.15760947721580348\n",
      "Iteration: 1230 lambda_n: 1.0154550010551997 Loss: 0.1575593988636111\n",
      "Iteration: 1231 lambda_n: 0.9702811124139347 Loss: 0.15750353278160023\n",
      "Iteration: 1232 lambda_n: 0.979657012690159 Loss: 0.15745019311003847\n",
      "Iteration: 1233 lambda_n: 0.8998463947491964 Loss: 0.15739637774037712\n",
      "Iteration: 1234 lambda_n: 0.9679175451301942 Loss: 0.1573469830314887\n",
      "Iteration: 1235 lambda_n: 0.983492800742083 Loss: 0.1572938881005528\n",
      "Iteration: 1236 lambda_n: 0.9719364010389303 Loss: 0.1572399782146217\n",
      "Iteration: 1237 lambda_n: 1.0272296402828467 Loss: 0.15718674115106004\n",
      "Iteration: 1238 lambda_n: 1.029236035979883 Loss: 0.1571305166630492\n",
      "Iteration: 1239 lambda_n: 0.8827768930733901 Loss: 0.15707422565679166\n",
      "Iteration: 1240 lambda_n: 0.9911912538748796 Loss: 0.1570259814116608\n",
      "Iteration: 1241 lambda_n: 0.9409939327526419 Loss: 0.15697184822798643\n",
      "Iteration: 1242 lambda_n: 1.0078719466104278 Loss: 0.15692049418143764\n",
      "Iteration: 1243 lambda_n: 0.9218864920906833 Loss: 0.15686552888333255\n",
      "Iteration: 1244 lambda_n: 1.0051524473741962 Loss: 0.15681529003279862\n",
      "Iteration: 1245 lambda_n: 0.9213911245705559 Loss: 0.15676055104844858\n",
      "Iteration: 1246 lambda_n: 0.9617157081387857 Loss: 0.15671041036367736\n",
      "Iteration: 1247 lambda_n: 0.9009849668307536 Loss: 0.1566581107654801\n",
      "Iteration: 1248 lambda_n: 1.0022040721831968 Loss: 0.15660914806460835\n",
      "Iteration: 1249 lambda_n: 0.9487830066846998 Loss: 0.15655472092259345\n",
      "Iteration: 1250 lambda_n: 0.8912636583651999 Loss: 0.15650323234607566\n",
      "Iteration: 1251 lambda_n: 0.928827982646307 Loss: 0.15645489836201423\n",
      "Iteration: 1252 lambda_n: 0.9793601100006667 Loss: 0.15640455987128354\n",
      "Iteration: 1253 lambda_n: 1.0059363932306538 Loss: 0.15635151855700422\n",
      "Iteration: 1254 lambda_n: 0.916210728574262 Loss: 0.15629707645369742\n",
      "Iteration: 1255 lambda_n: 0.96379801980528 Loss: 0.15624752595083094\n",
      "Iteration: 1256 lambda_n: 0.8776903749654971 Loss: 0.15619543625184135\n",
      "Iteration: 1257 lambda_n: 0.9960060426607571 Loss: 0.1561480327963503\n",
      "Iteration: 1258 lambda_n: 0.9111256055615408 Loss: 0.15609427332212997\n",
      "Iteration: 1259 lambda_n: 0.8920819355557043 Loss: 0.15604512988612845\n",
      "Iteration: 1260 lambda_n: 0.9706348594325367 Loss: 0.1559970446988066\n",
      "Iteration: 1261 lambda_n: 1.0082127015084126 Loss: 0.1559447586979073\n",
      "Iteration: 1262 lambda_n: 1.0143871973875958 Loss: 0.15589048588544013\n",
      "Iteration: 1263 lambda_n: 0.8767465933040364 Loss: 0.15583591956744777\n",
      "Iteration: 1264 lambda_n: 0.9351480921845126 Loss: 0.1557887905076991\n",
      "Iteration: 1265 lambda_n: 1.016978359398075 Loss: 0.15573855324730165\n",
      "Iteration: 1266 lambda_n: 0.9148955949714972 Loss: 0.15568395606333651\n",
      "Iteration: 1267 lambda_n: 0.9148129079829915 Loss: 0.1556348738424378\n",
      "Iteration: 1268 lambda_n: 1.009748947740269 Loss: 0.15558582736534884\n",
      "Iteration: 1269 lambda_n: 0.9758759083814553 Loss: 0.15553172581970165\n",
      "Iteration: 1270 lambda_n: 0.9797826345582439 Loss: 0.15547947569473855\n",
      "Iteration: 1271 lambda_n: 0.8882314609827112 Loss: 0.1554270518602417\n",
      "Iteration: 1272 lambda_n: 1.0145381425005602 Loss: 0.15537955839797524\n",
      "Iteration: 1273 lambda_n: 0.9485952073958883 Loss: 0.15532534502693832\n",
      "Iteration: 1274 lambda_n: 0.8875769881187479 Loss: 0.1552746905562368\n",
      "Iteration: 1275 lambda_n: 0.9936402803820503 Loss: 0.15522732507314008\n",
      "Iteration: 1276 lambda_n: 0.9835176276718537 Loss: 0.15517433212492093\n",
      "Iteration: 1277 lambda_n: 0.9847502619799698 Loss: 0.15512191460169014\n",
      "Iteration: 1278 lambda_n: 0.8899960144365322 Loss: 0.15506946656073936\n",
      "Iteration: 1279 lambda_n: 0.8994649776460428 Loss: 0.15502209658086052\n",
      "Iteration: 1280 lambda_n: 0.9486814458756473 Loss: 0.15497425154337216\n",
      "Iteration: 1281 lambda_n: 0.9242880221294597 Loss: 0.15492381943301917\n",
      "Iteration: 1282 lambda_n: 0.9185032122208096 Loss: 0.154874715480608\n",
      "Iteration: 1283 lambda_n: 0.9572646979612423 Loss: 0.1548259492250698\n",
      "Iteration: 1284 lambda_n: 1.0294068538837209 Loss: 0.15477515652082555\n",
      "Iteration: 1285 lambda_n: 0.9486806766023063 Loss: 0.15472057128487446\n",
      "Iteration: 1286 lambda_n: 1.005986539249533 Loss: 0.15467030101513585\n",
      "Iteration: 1287 lambda_n: 0.9199906205236978 Loss: 0.15461702810792494\n",
      "Iteration: 1288 lambda_n: 0.9815188497327205 Loss: 0.15456834156074914\n",
      "Iteration: 1289 lambda_n: 0.947463267053498 Loss: 0.1545164308832478\n",
      "Iteration: 1290 lambda_n: 1.013808775653311 Loss: 0.15446635385027918\n",
      "Iteration: 1291 lambda_n: 0.9792087340298008 Loss: 0.1544128040510052\n",
      "Iteration: 1292 lambda_n: 0.8910203363651109 Loss: 0.15436111634693414\n",
      "Iteration: 1293 lambda_n: 0.8925677076476981 Loss: 0.15431411374145698\n",
      "Iteration: 1294 lambda_n: 1.0138192392909733 Loss: 0.1542670570944621\n",
      "Iteration: 1295 lambda_n: 1.0262075247060063 Loss: 0.1542136397474656\n",
      "Iteration: 1296 lambda_n: 1.022643890901418 Loss: 0.1541596056081413\n",
      "Iteration: 1297 lambda_n: 1.0194069246922253 Loss: 0.15410579519140624\n",
      "Iteration: 1298 lambda_n: 1.0286158484921453 Loss: 0.15405219083730487\n",
      "Iteration: 1299 lambda_n: 0.9425027525664287 Loss: 0.15399813812767532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1300 lambda_n: 0.971038188199809 Loss: 0.1539486433254022\n",
      "Iteration: 1301 lambda_n: 1.0160654785801808 Loss: 0.1538976812130225\n",
      "Iteration: 1302 lambda_n: 0.954466184424994 Loss: 0.15384438958303454\n",
      "Iteration: 1303 lambda_n: 0.9236187114861452 Loss: 0.15379436133997423\n",
      "Iteration: 1304 lambda_n: 0.9129152570072918 Loss: 0.15374597956979483\n",
      "Iteration: 1305 lambda_n: 0.9705465557171961 Loss: 0.1536981867783269\n",
      "Iteration: 1306 lambda_n: 1.0007021808542425 Loss: 0.15364740678764177\n",
      "Iteration: 1307 lambda_n: 0.9172846271997178 Loss: 0.153595081605616\n",
      "Iteration: 1308 lambda_n: 0.8943482315651187 Loss: 0.1535471485250408\n",
      "Iteration: 1309 lambda_n: 0.9101817205092663 Loss: 0.15350044119659778\n",
      "Iteration: 1310 lambda_n: 0.9714386489373482 Loss: 0.15345293400898036\n",
      "Iteration: 1311 lambda_n: 1.0219227786090463 Loss: 0.15340225896204898\n",
      "Iteration: 1312 lambda_n: 0.9333164652612883 Loss: 0.15334898335176644\n",
      "Iteration: 1313 lambda_n: 0.9295852488669091 Loss: 0.15330035810943574\n",
      "Iteration: 1314 lambda_n: 0.9062369555001762 Loss: 0.15325195571274294\n",
      "Iteration: 1315 lambda_n: 1.0262610394424543 Loss: 0.1532047965243909\n",
      "Iteration: 1316 lambda_n: 0.9569182307232276 Loss: 0.1531514222697017\n",
      "Iteration: 1317 lambda_n: 0.9872544235829295 Loss: 0.15310168615578\n",
      "Iteration: 1318 lambda_n: 1.0094827163961948 Loss: 0.1530504040870898\n",
      "Iteration: 1319 lambda_n: 0.9555187762041568 Loss: 0.15299799973675496\n",
      "Iteration: 1320 lambda_n: 0.9907460552511318 Loss: 0.15294842772669398\n",
      "Iteration: 1321 lambda_n: 0.9284907164482327 Loss: 0.15289705874367387\n",
      "Iteration: 1322 lambda_n: 1.0070183935450088 Loss: 0.15284894695826465\n",
      "Iteration: 1323 lambda_n: 0.9168924095860729 Loss: 0.15279679632415807\n",
      "Iteration: 1324 lambda_n: 0.9023329637434568 Loss: 0.15274934225419237\n",
      "Iteration: 1325 lambda_n: 0.9617408765907854 Loss: 0.15270266802346705\n",
      "Iteration: 1326 lambda_n: 1.001500384868588 Loss: 0.15265294863723441\n",
      "Iteration: 1327 lambda_n: 0.980819264558359 Loss: 0.1526012044740473\n",
      "Iteration: 1328 lambda_n: 0.8763995358949638 Loss: 0.15255055982072988\n",
      "Iteration: 1329 lambda_n: 0.9856619859012612 Loss: 0.1525053336567237\n",
      "Iteration: 1330 lambda_n: 0.8980382082342749 Loss: 0.15245449662232813\n",
      "Iteration: 1331 lambda_n: 1.0057502851160423 Loss: 0.15240820640485794\n",
      "Iteration: 1332 lambda_n: 0.9193713039401405 Loss: 0.15235639273146068\n",
      "Iteration: 1333 lambda_n: 0.9994074036850531 Loss: 0.15230905765798125\n",
      "Iteration: 1334 lambda_n: 0.9352476967835447 Loss: 0.15225763072084228\n",
      "Iteration: 1335 lambda_n: 0.9588300033495039 Loss: 0.1522095340895443\n",
      "Iteration: 1336 lambda_n: 0.9353300665839881 Loss: 0.15216025256264035\n",
      "Iteration: 1337 lambda_n: 0.9322514344806323 Loss: 0.1521125563905382\n",
      "Iteration: 1338 lambda_n: 0.9516733695185269 Loss: 0.15206517344747264\n",
      "Iteration: 1339 lambda_n: 0.9111482546850764 Loss: 0.15201682877300446\n",
      "Iteration: 1340 lambda_n: 0.9128032570655242 Loss: 0.15197056733986947\n",
      "Iteration: 1341 lambda_n: 0.9193940230535639 Loss: 0.15192424553315664\n",
      "Iteration: 1342 lambda_n: 0.887060513320997 Loss: 0.15187761309854597\n",
      "Iteration: 1343 lambda_n: 1.006555728014978 Loss: 0.15183264362719265\n",
      "Iteration: 1344 lambda_n: 0.9858655198997488 Loss: 0.15178164201402677\n",
      "Iteration: 1345 lambda_n: 0.9822041387868803 Loss: 0.15173171665207244\n",
      "Iteration: 1346 lambda_n: 0.8976776993440122 Loss: 0.15168200391443828\n",
      "Iteration: 1347 lambda_n: 0.9813881858416262 Loss: 0.15163659379821193\n",
      "Iteration: 1348 lambda_n: 1.0047846157634917 Loss: 0.1515869740341264\n",
      "Iteration: 1349 lambda_n: 0.9593396021831214 Loss: 0.1515361989413448\n",
      "Iteration: 1350 lambda_n: 1.0297261974342835 Loss: 0.15148774701920514\n",
      "Iteration: 1351 lambda_n: 0.9542383482288948 Loss: 0.1514357678976897\n",
      "Iteration: 1352 lambda_n: 0.9323161116440961 Loss: 0.15138762626510627\n",
      "Iteration: 1353 lambda_n: 0.9701292771851066 Loss: 0.15134061513804103\n",
      "Iteration: 1354 lambda_n: 0.9042398912601594 Loss: 0.15129172240345354\n",
      "Iteration: 1355 lambda_n: 0.9120068560739965 Loss: 0.15124617429728013\n",
      "Iteration: 1356 lambda_n: 1.012774896338601 Loss: 0.15120025762662176\n",
      "Iteration: 1357 lambda_n: 0.993109638831038 Loss: 0.15114929328692067\n",
      "Iteration: 1358 lambda_n: 0.899936446462488 Loss: 0.15109934595926575\n",
      "Iteration: 1359 lambda_n: 0.8921035986497848 Loss: 0.15105410873951738\n",
      "Iteration: 1360 lambda_n: 0.9451177081835082 Loss: 0.15100928707461908\n",
      "Iteration: 1361 lambda_n: 0.9239349164678602 Loss: 0.15096182491637583\n",
      "Iteration: 1362 lambda_n: 0.9837696433959016 Loss: 0.15091545011256474\n",
      "Iteration: 1363 lambda_n: 0.9378269419849821 Loss: 0.1508660968314819\n",
      "Iteration: 1364 lambda_n: 0.940935707233911 Loss: 0.15081907311245685\n",
      "Iteration: 1365 lambda_n: 0.9198606285222266 Loss: 0.15077191727836606\n",
      "Iteration: 1366 lambda_n: 0.9459203190379817 Loss: 0.15072584081482834\n",
      "Iteration: 1367 lambda_n: 0.9250075080599207 Loss: 0.15067848241166268\n",
      "Iteration: 1368 lambda_n: 1.008085458140024 Loss: 0.15063219435515965\n",
      "Iteration: 1369 lambda_n: 0.9051963463625069 Loss: 0.1505817742072105\n",
      "Iteration: 1370 lambda_n: 1.0025534753147194 Loss: 0.15053652408826776\n",
      "Iteration: 1371 lambda_n: 0.9561294255268443 Loss: 0.1504864316233666\n",
      "Iteration: 1372 lambda_n: 0.9829224653185529 Loss: 0.15043868397787918\n",
      "Iteration: 1373 lambda_n: 1.029547654463001 Loss: 0.1503896232823667\n",
      "Iteration: 1374 lambda_n: 1.00872685102704 Loss: 0.15033826226347846\n",
      "Iteration: 1375 lambda_n: 1.0263802817162697 Loss: 0.1502879672006037\n",
      "Iteration: 1376 lambda_n: 0.9707581621616704 Loss: 0.150236819204347\n",
      "Iteration: 1377 lambda_n: 0.9334887042503576 Loss: 0.15018846896231527\n",
      "Iteration: 1378 lambda_n: 0.9619204301871734 Loss: 0.15014199855676005\n",
      "Iteration: 1379 lambda_n: 1.018552597972547 Loss: 0.15009413630986024\n",
      "Iteration: 1380 lambda_n: 0.9763773980732758 Loss: 0.15004348194197753\n",
      "Iteration: 1381 lambda_n: 0.9206426548425847 Loss: 0.149994950706236\n",
      "Iteration: 1382 lambda_n: 0.8787448339268065 Loss: 0.14994921290142213\n",
      "Iteration: 1383 lambda_n: 0.9916880783410297 Loss: 0.14990557738366286\n",
      "Iteration: 1384 lambda_n: 0.9486575751326054 Loss: 0.14985635639205278\n",
      "Iteration: 1385 lambda_n: 1.0195999465650465 Loss: 0.14980929524387693\n",
      "Iteration: 1386 lambda_n: 0.9007350112630815 Loss: 0.14975873990795954\n",
      "Iteration: 1387 lambda_n: 1.007760279696598 Loss: 0.14971410147676767\n",
      "Iteration: 1388 lambda_n: 0.9429851618251058 Loss: 0.14966418274231258\n",
      "Iteration: 1389 lambda_n: 0.8981318139064802 Loss: 0.14961749664906776\n",
      "Iteration: 1390 lambda_n: 0.9140694987970253 Loss: 0.149573052632387\n",
      "Iteration: 1391 lambda_n: 1.018794532603737 Loss: 0.14952784087631263\n",
      "Iteration: 1392 lambda_n: 1.0208188904476934 Loss: 0.1494774732399256\n",
      "Iteration: 1393 lambda_n: 0.9098801691252829 Loss: 0.14942703188021192\n",
      "Iteration: 1394 lambda_n: 0.9642162627507805 Loss: 0.14938209540512526\n",
      "Iteration: 1395 lambda_n: 0.894463242513892 Loss: 0.14933449774354962\n",
      "Iteration: 1396 lambda_n: 0.9600552749737831 Loss: 0.14929036485936484\n",
      "Iteration: 1397 lambda_n: 1.0201405916438717 Loss: 0.1492430174656206\n",
      "Iteration: 1398 lambda_n: 0.9459246944726627 Loss: 0.14919273160671537\n",
      "Iteration: 1399 lambda_n: 0.9599230781459194 Loss: 0.1491461279674906\n",
      "Iteration: 1400 lambda_n: 0.9323528649493867 Loss: 0.14909885737908168\n",
      "Iteration: 1401 lambda_n: 0.8773725849957971 Loss: 0.14905296666918247\n",
      "Iteration: 1402 lambda_n: 0.9957239959534392 Loss: 0.14900980226946034\n",
      "Iteration: 1403 lambda_n: 0.9671651315008688 Loss: 0.14896083741415572\n",
      "Iteration: 1404 lambda_n: 0.9676842063020487 Loss: 0.14891330069894865\n",
      "Iteration: 1405 lambda_n: 0.9829962550669663 Loss: 0.14886576160893458\n",
      "Iteration: 1406 lambda_n: 0.971974702528184 Loss: 0.14881749381199377\n",
      "Iteration: 1407 lambda_n: 1.0177344038680671 Loss: 0.14876979068715795\n",
      "Iteration: 1408 lambda_n: 1.0156653324758849 Loss: 0.14871986621772612\n",
      "Iteration: 1409 lambda_n: 0.9136931747401597 Loss: 0.1486700685832731\n",
      "Iteration: 1410 lambda_n: 0.9498187710101279 Loss: 0.14862529296334218\n",
      "Iteration: 1411 lambda_n: 0.8861671841062251 Loss: 0.14857876832069944\n",
      "Iteration: 1412 lambda_n: 0.9308707781389507 Loss: 0.14853538180629935\n",
      "Iteration: 1413 lambda_n: 1.0020969418116359 Loss: 0.1484898268319362\n",
      "Iteration: 1414 lambda_n: 0.877868651098597 Loss: 0.14844080909888624\n",
      "Iteration: 1415 lambda_n: 0.8794005519164297 Loss: 0.14839788892807218\n",
      "Iteration: 1416 lambda_n: 0.9811129321900836 Loss: 0.1483549125332982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1417 lambda_n: 0.9907288459139427 Loss: 0.14830698663625375\n",
      "Iteration: 1418 lambda_n: 1.0268518997764269 Loss: 0.14825861447095864\n",
      "Iteration: 1419 lambda_n: 0.8823521756092151 Loss: 0.14820850320238632\n",
      "Iteration: 1420 lambda_n: 0.902677854698259 Loss: 0.14816546492103708\n",
      "Iteration: 1421 lambda_n: 1.0162614372823036 Loss: 0.1481214543597478\n",
      "Iteration: 1422 lambda_n: 0.9014576314471565 Loss: 0.1480719283199194\n",
      "Iteration: 1423 lambda_n: 0.9474817558604312 Loss: 0.14802801855811487\n",
      "Iteration: 1424 lambda_n: 0.9379328469019268 Loss: 0.14798188747134236\n",
      "Iteration: 1425 lambda_n: 1.0238995816474172 Loss: 0.14793624239250175\n",
      "Iteration: 1426 lambda_n: 1.0116443144911642 Loss: 0.14788643679480137\n",
      "Iteration: 1427 lambda_n: 0.9117654747425984 Loss: 0.14783725181202195\n",
      "Iteration: 1428 lambda_n: 0.9107298338671482 Loss: 0.14779294428611092\n",
      "Iteration: 1429 lambda_n: 0.8782729262597618 Loss: 0.14774870666639195\n",
      "Iteration: 1430 lambda_n: 0.9157909013188021 Loss: 0.14770606432691025\n",
      "Iteration: 1431 lambda_n: 1.0227550162082917 Loss: 0.14766161942114783\n",
      "Iteration: 1432 lambda_n: 0.9999629563909632 Loss: 0.14761200573306957\n",
      "Iteration: 1433 lambda_n: 0.9161470818787137 Loss: 0.14756352154648095\n",
      "Iteration: 1434 lambda_n: 1.0036397963367405 Loss: 0.1475191223806389\n",
      "Iteration: 1435 lambda_n: 0.9335993807522014 Loss: 0.14747050482571408\n",
      "Iteration: 1436 lambda_n: 0.9891446401899902 Loss: 0.14742530168836626\n",
      "Iteration: 1437 lambda_n: 0.9759059319270001 Loss: 0.1473774308086125\n",
      "Iteration: 1438 lambda_n: 0.9460352585361702 Loss: 0.14733022297127224\n",
      "Iteration: 1439 lambda_n: 0.9090099165519172 Loss: 0.14728448134431685\n",
      "Iteration: 1440 lambda_n: 0.9168748317131895 Loss: 0.14724054966857916\n",
      "Iteration: 1441 lambda_n: 0.9483612524789086 Loss: 0.1471962571332791\n",
      "Iteration: 1442 lambda_n: 0.9750960818446016 Loss: 0.14715046367121085\n",
      "Iteration: 1443 lambda_n: 0.9907703737550485 Loss: 0.14710340060964103\n",
      "Iteration: 1444 lambda_n: 0.9590897648398669 Loss: 0.1470556032422634\n",
      "Iteration: 1445 lambda_n: 0.9776861860668005 Loss: 0.14700935587367167\n",
      "Iteration: 1446 lambda_n: 0.9985989604171359 Loss: 0.14696223327607055\n",
      "Iteration: 1447 lambda_n: 0.8812612742398699 Loss: 0.14691412506583765\n",
      "Iteration: 1448 lambda_n: 0.9817449940281144 Loss: 0.14687168933917638\n",
      "Iteration: 1449 lambda_n: 0.9730811263525783 Loss: 0.14682443499613423\n",
      "Iteration: 1450 lambda_n: 0.9191377243465265 Loss: 0.14677761931012145\n",
      "Iteration: 1451 lambda_n: 0.9909222563711785 Loss: 0.14673341895293052\n",
      "Iteration: 1452 lambda_n: 0.9603692531079756 Loss: 0.14668578739517626\n",
      "Iteration: 1453 lambda_n: 0.9579465568626347 Loss: 0.1466396458190596\n",
      "Iteration: 1454 lambda_n: 0.9139052470732362 Loss: 0.1465936413505896\n",
      "Iteration: 1455 lambda_n: 0.9717836331761308 Loss: 0.14654977145633893\n",
      "Iteration: 1456 lambda_n: 0.932626861827421 Loss: 0.14650314337513617\n",
      "Iteration: 1457 lambda_n: 0.9261204412819469 Loss: 0.14645841428023004\n",
      "Iteration: 1458 lambda_n: 0.9902216806069877 Loss: 0.1464140165246516\n",
      "Iteration: 1459 lambda_n: 0.9766247465334623 Loss: 0.14636656648480195\n",
      "Iteration: 1460 lambda_n: 0.9400963756269344 Loss: 0.14631978948982988\n",
      "Iteration: 1461 lambda_n: 1.0236946292273488 Loss: 0.14627478237379848\n",
      "Iteration: 1462 lambda_n: 0.9207039485826749 Loss: 0.14622579466415286\n",
      "Iteration: 1463 lambda_n: 0.9977070116244138 Loss: 0.14618175599786798\n",
      "Iteration: 1464 lambda_n: 0.9528718051266701 Loss: 0.14613405475379307\n",
      "Iteration: 1465 lambda_n: 0.9981854268803907 Loss: 0.14608851795876532\n",
      "Iteration: 1466 lambda_n: 0.9089661306351784 Loss: 0.14604083679815685\n",
      "Iteration: 1467 lambda_n: 0.8822331295066203 Loss: 0.14599743712207053\n",
      "Iteration: 1468 lambda_n: 1.0062339323613319 Loss: 0.14595533137288302\n",
      "Iteration: 1469 lambda_n: 1.0263550877407124 Loss: 0.14590732743423085\n",
      "Iteration: 1470 lambda_n: 0.9148572223223828 Loss: 0.1458583862672812\n",
      "Iteration: 1471 lambda_n: 0.9892147002463731 Loss: 0.14581478196482953\n",
      "Iteration: 1472 lambda_n: 0.9071325073051192 Loss: 0.14576765360844385\n",
      "Iteration: 1473 lambda_n: 0.9806558189673478 Loss: 0.14572445510024135\n",
      "Iteration: 1474 lambda_n: 0.9796210870675776 Loss: 0.14567808808564064\n",
      "Iteration: 1475 lambda_n: 0.9595675908621161 Loss: 0.14563214780300265\n",
      "Iteration: 1476 lambda_n: 0.9472013646261009 Loss: 0.14558716412997277\n",
      "Iteration: 1477 lambda_n: 0.9608974163869819 Loss: 0.1455427758358147\n",
      "Iteration: 1478 lambda_n: 1.0128869022662856 Loss: 0.14549776147210355\n",
      "Iteration: 1479 lambda_n: 0.9224851036597252 Loss: 0.14545032858385704\n",
      "Iteration: 1480 lambda_n: 0.9257152340170558 Loss: 0.1454071449743148\n",
      "Iteration: 1481 lambda_n: 0.8788352673157402 Loss: 0.14536382488436897\n",
      "Iteration: 1482 lambda_n: 0.9042956936894273 Loss: 0.14532271247272274\n",
      "Iteration: 1483 lambda_n: 0.9586538625730102 Loss: 0.14528042277215486\n",
      "Iteration: 1484 lambda_n: 0.8864086035383936 Loss: 0.14523560609526937\n",
      "Iteration: 1485 lambda_n: 0.9316931499083554 Loss: 0.14519418123984107\n",
      "Iteration: 1486 lambda_n: 0.976281519952541 Loss: 0.1451506544292896\n",
      "Iteration: 1487 lambda_n: 0.9822851357713823 Loss: 0.14510506031970571\n",
      "Iteration: 1488 lambda_n: 0.9225269562477983 Loss: 0.14505920231421404\n",
      "Iteration: 1489 lambda_n: 0.8927668558330566 Loss: 0.14501614947187969\n",
      "Iteration: 1490 lambda_n: 0.9599230161330775 Loss: 0.14497449951266345\n",
      "Iteration: 1491 lambda_n: 0.98180088054066 Loss: 0.14492973145347884\n",
      "Iteration: 1492 lambda_n: 0.8897153467715786 Loss: 0.14488395929046677\n",
      "Iteration: 1493 lambda_n: 0.9913288152967585 Loss: 0.14484249487301595\n",
      "Iteration: 1494 lambda_n: 1.00687076246482 Loss: 0.14479631029150364\n",
      "Iteration: 1495 lambda_n: 1.016205429797541 Loss: 0.14474941875263683\n",
      "Iteration: 1496 lambda_n: 0.973648076845833 Loss: 0.14470210999879507\n",
      "Iteration: 1497 lambda_n: 1.0042617078818268 Loss: 0.14465679922120453\n",
      "Iteration: 1498 lambda_n: 1.0008253392900084 Loss: 0.14461008056686342\n",
      "Iteration: 1499 lambda_n: 0.9386979282992048 Loss: 0.14456353890167337\n",
      "Iteration: 1500 lambda_n: 1.0037698897859375 Loss: 0.14451990217055571\n",
      "Iteration: 1501 lambda_n: 0.8841268904127815 Loss: 0.14447325675353193\n",
      "Iteration: 1502 lambda_n: 1.014207416706439 Loss: 0.14443218590797804\n",
      "Iteration: 1503 lambda_n: 0.9961902180554153 Loss: 0.1443850880764729\n",
      "Iteration: 1504 lambda_n: 0.9872725159345287 Loss: 0.14433884404316605\n",
      "Iteration: 1505 lambda_n: 0.8775018480139438 Loss: 0.14429303065725818\n",
      "Iteration: 1506 lambda_n: 1.0223584544311253 Loss: 0.14425232544122946\n",
      "Iteration: 1507 lambda_n: 1.0045658695753108 Loss: 0.14420491641684183\n",
      "Iteration: 1508 lambda_n: 1.0295523816555916 Loss: 0.14415834982972578\n",
      "Iteration: 1509 lambda_n: 0.9744556015220797 Loss: 0.14411064261455625\n",
      "Iteration: 1510 lambda_n: 0.9231902606091884 Loss: 0.144065505270263\n",
      "Iteration: 1511 lambda_n: 0.9994303793704186 Loss: 0.1440227576292292\n",
      "Iteration: 1512 lambda_n: 1.0110410057932657 Loss: 0.1439764956218879\n",
      "Iteration: 1513 lambda_n: 0.90486713038014 Loss: 0.1439297133033415\n",
      "Iteration: 1514 lambda_n: 0.912696275594961 Loss: 0.14388785892586134\n",
      "Iteration: 1515 lambda_n: 1.0060225862495558 Loss: 0.1438456563849639\n",
      "Iteration: 1516 lambda_n: 0.8940433296210215 Loss: 0.14379915431415588\n",
      "Iteration: 1517 lambda_n: 0.9163626367648366 Loss: 0.1437578431598472\n",
      "Iteration: 1518 lambda_n: 0.9826065618529355 Loss: 0.14371551457525705\n",
      "Iteration: 1519 lambda_n: 1.0141422582379096 Loss: 0.14367014145359633\n",
      "Iteration: 1520 lambda_n: 0.9740104581432127 Loss: 0.14362332902018587\n",
      "Iteration: 1521 lambda_n: 0.9395424207528452 Loss: 0.14357838554234015\n",
      "Iteration: 1522 lambda_n: 0.8886409807095641 Loss: 0.1435350477906598\n",
      "Iteration: 1523 lambda_n: 0.8846014649692844 Loss: 0.14349407181749932\n",
      "Iteration: 1524 lambda_n: 0.8904139909823037 Loss: 0.1434532953021647\n",
      "Iteration: 1525 lambda_n: 0.8977732204638205 Loss: 0.14341226409492266\n",
      "Iteration: 1526 lambda_n: 0.9447370054332689 Loss: 0.14337090720301382\n",
      "Iteration: 1527 lambda_n: 0.973167515653816 Loss: 0.14332740125099813\n",
      "Iteration: 1528 lambda_n: 0.9417183699702244 Loss: 0.14328260155484415\n",
      "Iteration: 1529 lambda_n: 1.0117001384844355 Loss: 0.14323926486714766\n",
      "Iteration: 1530 lambda_n: 0.894169122155486 Loss: 0.14319272390109625\n",
      "Iteration: 1531 lambda_n: 0.9984894474178315 Loss: 0.14315160445596817\n",
      "Iteration: 1532 lambda_n: 0.9601733636259586 Loss: 0.1431057030028504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1533 lambda_n: 0.8905776457426056 Loss: 0.14306157886990048\n",
      "Iteration: 1534 lambda_n: 0.9601920576463983 Loss: 0.14302066703116445\n",
      "Iteration: 1535 lambda_n: 0.9670367079218105 Loss: 0.14297657171172914\n",
      "Iteration: 1536 lambda_n: 0.9609051194419345 Loss: 0.14293217757623014\n",
      "Iteration: 1537 lambda_n: 0.930588158606072 Loss: 0.142888080399334\n",
      "Iteration: 1538 lambda_n: 1.0260389693794132 Loss: 0.14284538931116586\n",
      "Iteration: 1539 lambda_n: 1.0210352379779002 Loss: 0.14279833561785252\n",
      "Iteration: 1540 lambda_n: 0.9670505595292701 Loss: 0.14275152881318473\n",
      "Iteration: 1541 lambda_n: 1.021099865321552 Loss: 0.1427072130471192\n",
      "Iteration: 1542 lambda_n: 0.9657947008034415 Loss: 0.1426604370384766\n",
      "Iteration: 1543 lambda_n: 0.9795851266817148 Loss: 0.14261621073078115\n",
      "Iteration: 1544 lambda_n: 0.9436484525528046 Loss: 0.14257136867839917\n",
      "Iteration: 1545 lambda_n: 1.0036868513498582 Loss: 0.1425281869087153\n",
      "Iteration: 1546 lambda_n: 0.9154387461366043 Loss: 0.14248227366959404\n",
      "Iteration: 1547 lambda_n: 0.9014090830913406 Loss: 0.1424404122590785\n",
      "Iteration: 1548 lambda_n: 1.0235997810950688 Loss: 0.14239920602831224\n",
      "Iteration: 1549 lambda_n: 0.9221249672642161 Loss: 0.14235242978128027\n",
      "Iteration: 1550 lambda_n: 1.0150205218628825 Loss: 0.14231030601132866\n",
      "Iteration: 1551 lambda_n: 0.9486887424809431 Loss: 0.1422639544619822\n",
      "Iteration: 1552 lambda_n: 0.9947962904600314 Loss: 0.14222064769906032\n",
      "Iteration: 1553 lambda_n: 1.0288506933881947 Loss: 0.1421752519161104\n",
      "Iteration: 1554 lambda_n: 0.8769371418817231 Loss: 0.14212831913822663\n",
      "Iteration: 1555 lambda_n: 0.9035381565015879 Loss: 0.14208833059371953\n",
      "Iteration: 1556 lambda_n: 0.9021463198499837 Loss: 0.14204714218578132\n",
      "Iteration: 1557 lambda_n: 0.8840301279591267 Loss: 0.14200603065820375\n",
      "Iteration: 1558 lambda_n: 0.8935155853473071 Loss: 0.14196575778575748\n",
      "Iteration: 1559 lambda_n: 0.9505742972780953 Loss: 0.14192506583005585\n",
      "Iteration: 1560 lambda_n: 1.027842554218319 Loss: 0.14188178950282598\n",
      "Iteration: 1561 lambda_n: 0.9023189054558226 Loss: 0.1418350117621239\n",
      "Iteration: 1562 lambda_n: 0.8904369007733698 Loss: 0.14179396154620416\n",
      "Iteration: 1563 lambda_n: 0.9939834410163263 Loss: 0.141753465058888\n",
      "Iteration: 1564 lambda_n: 0.9926990621851394 Loss: 0.14170827422534665\n",
      "Iteration: 1565 lambda_n: 0.8926678490569382 Loss: 0.1416631579785195\n",
      "Iteration: 1566 lambda_n: 0.9976767667795196 Loss: 0.14162260219504427\n",
      "Iteration: 1567 lambda_n: 0.9199523467059623 Loss: 0.14157729059009036\n",
      "Iteration: 1568 lambda_n: 0.9002238219411793 Loss: 0.14153552380694392\n",
      "Iteration: 1569 lambda_n: 1.0252298853695643 Loss: 0.14149466622535806\n",
      "Iteration: 1570 lambda_n: 0.952987929870467 Loss: 0.1414481506549299\n",
      "Iteration: 1571 lambda_n: 0.9945143075177668 Loss: 0.1414049285222553\n",
      "Iteration: 1572 lambda_n: 0.8755961870997875 Loss: 0.1413598386268376\n",
      "Iteration: 1573 lambda_n: 0.9892964573482289 Loss: 0.14132015419990976\n",
      "Iteration: 1574 lambda_n: 0.919549085615639 Loss: 0.14127533109175958\n",
      "Iteration: 1575 lambda_n: 0.8829639833840245 Loss: 0.14123368274327194\n",
      "Iteration: 1576 lambda_n: 0.9452875576867265 Loss: 0.14119370455645885\n",
      "Iteration: 1577 lambda_n: 0.8841734587606837 Loss: 0.1411509183414152\n",
      "Iteration: 1578 lambda_n: 0.927168682051601 Loss: 0.1411109117602411\n",
      "Iteration: 1579 lambda_n: 0.940790429940007 Loss: 0.14106897324835102\n",
      "Iteration: 1580 lambda_n: 1.0104309250987844 Loss: 0.14102643283705382\n",
      "Iteration: 1581 lambda_n: 0.8877902842332275 Loss: 0.14098075915138883\n",
      "Iteration: 1582 lambda_n: 0.9426769553750879 Loss: 0.14094064332703235\n",
      "Iteration: 1583 lambda_n: 0.9940240313449875 Loss: 0.14089806116920034\n",
      "Iteration: 1584 lambda_n: 0.9387245472645019 Loss: 0.14085317498846328\n",
      "Iteration: 1585 lambda_n: 0.9894950399811238 Loss: 0.14081080090319253\n",
      "Iteration: 1586 lambda_n: 0.9795323845661971 Loss: 0.1407661502865492\n",
      "Iteration: 1587 lambda_n: 1.0171006686473183 Loss: 0.14072196493242675\n",
      "Iteration: 1588 lambda_n: 0.977985860933719 Loss: 0.1406761012154094\n",
      "Iteration: 1589 lambda_n: 0.9989173119132265 Loss: 0.1406320172909415\n",
      "Iteration: 1590 lambda_n: 0.9809428003966841 Loss: 0.140587005758414\n",
      "Iteration: 1591 lambda_n: 0.8902713618896665 Loss: 0.14054281997880994\n",
      "Iteration: 1592 lambda_n: 0.974758893166175 Loss: 0.14050273229538898\n",
      "Iteration: 1593 lambda_n: 1.0188231485790489 Loss: 0.14045885457163992\n",
      "Iteration: 1594 lambda_n: 0.885073022412821 Loss: 0.14041300955559638\n",
      "Iteration: 1595 lambda_n: 0.959188257558973 Loss: 0.1403731972193375\n",
      "Iteration: 1596 lambda_n: 1.0030616383797544 Loss: 0.14033006498660966\n",
      "Iteration: 1597 lambda_n: 0.9295560511479685 Loss: 0.1402849755667813\n",
      "Iteration: 1598 lambda_n: 0.9241777398448385 Loss: 0.14024320518462566\n",
      "Iteration: 1599 lambda_n: 1.0265468622141545 Loss: 0.14020169032870977\n",
      "Iteration: 1600 lambda_n: 0.893590143574705 Loss: 0.14015559261203045\n",
      "Iteration: 1601 lambda_n: 0.9607832108709979 Loss: 0.14011547979854744\n",
      "Iteration: 1602 lambda_n: 0.9527259648562707 Loss: 0.14007236476921237\n",
      "Iteration: 1603 lambda_n: 1.0130889071915639 Loss: 0.14002962602740074\n",
      "Iteration: 1604 lambda_n: 0.9475049211275847 Loss: 0.139984195178896\n",
      "Iteration: 1605 lambda_n: 0.9673712597471994 Loss: 0.13994172061610455\n",
      "Iteration: 1606 lambda_n: 1.006590555776578 Loss: 0.1398983702956197\n",
      "Iteration: 1607 lambda_n: 0.8820473175479677 Loss: 0.13985327824559274\n",
      "Iteration: 1608 lambda_n: 0.998180461989571 Loss: 0.13981377924578847\n",
      "Iteration: 1609 lambda_n: 0.8945016067622087 Loss: 0.13976909420366382\n",
      "Iteration: 1610 lambda_n: 0.9688132573355398 Loss: 0.13972906453462594\n",
      "Iteration: 1611 lambda_n: 1.0258200558390191 Loss: 0.13968572350220707\n",
      "Iteration: 1612 lambda_n: 1.01352032559196 Loss: 0.13963984833545512\n",
      "Iteration: 1613 lambda_n: 0.9254509727801739 Loss: 0.13959453984926135\n",
      "Iteration: 1614 lambda_n: 0.9932515998279818 Loss: 0.13955318318977283\n",
      "Iteration: 1615 lambda_n: 1.026656574359771 Loss: 0.13950881159281972\n",
      "Iteration: 1616 lambda_n: 0.9122390122499593 Loss: 0.13946296413361411\n",
      "Iteration: 1617 lambda_n: 0.952112917095298 Loss: 0.13942224087722008\n",
      "Iteration: 1618 lambda_n: 0.9892271184246682 Loss: 0.1393797516290444\n",
      "Iteration: 1619 lambda_n: 0.9713287333909028 Loss: 0.13933562129372098\n",
      "Iteration: 1620 lambda_n: 0.8798216347685486 Loss: 0.13929230472496615\n",
      "Iteration: 1621 lambda_n: 0.9354569606663957 Loss: 0.13925308232465639\n",
      "Iteration: 1622 lambda_n: 0.9875935002722347 Loss: 0.1392113930180911\n",
      "Iteration: 1623 lambda_n: 0.8843988503546841 Loss: 0.13916739512531684\n",
      "Iteration: 1624 lambda_n: 0.986967138338996 Loss: 0.13912800826024538\n",
      "Iteration: 1625 lambda_n: 1.019995927021627 Loss: 0.1390840677484232\n",
      "Iteration: 1626 lambda_n: 0.9281052865751548 Loss: 0.13903867293129285\n",
      "Iteration: 1627 lambda_n: 0.973794359490108 Loss: 0.13899738251059218\n",
      "Iteration: 1628 lambda_n: 0.883705911078109 Loss: 0.13895407397343368\n",
      "Iteration: 1629 lambda_n: 0.9286512694679404 Loss: 0.13891478547776218\n",
      "Iteration: 1630 lambda_n: 0.9433314132307388 Loss: 0.13887351196385062\n",
      "Iteration: 1631 lambda_n: 1.0023680020690449 Loss: 0.13883159998121802\n",
      "Iteration: 1632 lambda_n: 1.0124783030923905 Loss: 0.13878708024799677\n",
      "Iteration: 1633 lambda_n: 0.9171640478117827 Loss: 0.13874212763885563\n",
      "Iteration: 1634 lambda_n: 0.8793599444210586 Loss: 0.13870142132484273\n",
      "Iteration: 1635 lambda_n: 1.0246382319458092 Loss: 0.13866240556839654\n",
      "Iteration: 1636 lambda_n: 0.8890720696818417 Loss: 0.13861695883958572\n",
      "Iteration: 1637 lambda_n: 0.9867770692798413 Loss: 0.13857753907567089\n",
      "Iteration: 1638 lambda_n: 0.9324224046960659 Loss: 0.13853380148812794\n",
      "Iteration: 1639 lambda_n: 0.9242136223943014 Loss: 0.13849248753016044\n",
      "Iteration: 1640 lambda_n: 0.950338020883938 Loss: 0.13845155092718678\n",
      "Iteration: 1641 lambda_n: 0.9812149081156051 Loss: 0.13840947118442604\n",
      "Iteration: 1642 lambda_n: 1.0201049634268253 Loss: 0.1383660391229887\n",
      "Iteration: 1643 lambda_n: 0.9316165955725462 Loss: 0.13832090162774238\n",
      "Iteration: 1644 lambda_n: 0.9633842947291571 Loss: 0.1382796943363078\n",
      "Iteration: 1645 lambda_n: 1.0053905821798688 Loss: 0.13823709619483793\n",
      "Iteration: 1646 lambda_n: 0.9703088330531562 Loss: 0.13819265610923415\n",
      "Iteration: 1647 lambda_n: 0.9933677121085014 Loss: 0.13814978201627082\n",
      "Iteration: 1648 lambda_n: 1.0210273996981034 Loss: 0.13810590435033265\n",
      "Iteration: 1649 lambda_n: 0.9984946006495087 Loss: 0.1380608210532186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1650 lambda_n: 0.9796849293351177 Loss: 0.13801674871670744\n",
      "Iteration: 1651 lambda_n: 0.9505427978672228 Loss: 0.13797352199969518\n",
      "Iteration: 1652 lambda_n: 0.9322729579018952 Loss: 0.1379315957299174\n",
      "Iteration: 1653 lambda_n: 0.9151162243794656 Loss: 0.13789048922522606\n",
      "Iteration: 1654 lambda_n: 0.8871443058161754 Loss: 0.13785015261113334\n",
      "Iteration: 1655 lambda_n: 0.8777250762120001 Loss: 0.13781106166470994\n",
      "Iteration: 1656 lambda_n: 0.9998482391055232 Loss: 0.13777239800860336\n",
      "Iteration: 1657 lambda_n: 0.9292155478327289 Loss: 0.13772836905802732\n",
      "Iteration: 1658 lambda_n: 1.0209480188732563 Loss: 0.13768746488328595\n",
      "Iteration: 1659 lambda_n: 0.9492252428971459 Loss: 0.13764253786502234\n",
      "Iteration: 1660 lambda_n: 0.896680356055081 Loss: 0.13760078205362392\n",
      "Iteration: 1661 lambda_n: 0.976776015470311 Loss: 0.1375613508958763\n",
      "Iteration: 1662 lambda_n: 1.0186933629677748 Loss: 0.13751841158334122\n",
      "Iteration: 1663 lambda_n: 0.9537847755498684 Loss: 0.13747364534899295\n",
      "Iteration: 1664 lambda_n: 0.9565178350156375 Loss: 0.1374317465760376\n",
      "Iteration: 1665 lambda_n: 0.9827301491092728 Loss: 0.1373897420795001\n",
      "Iteration: 1666 lambda_n: 0.9367022929007905 Loss: 0.13734660133600687\n",
      "Iteration: 1667 lambda_n: 1.0163660707930111 Loss: 0.1373054954739811\n",
      "Iteration: 1668 lambda_n: 1.0124282544518053 Loss: 0.13726090887209075\n",
      "Iteration: 1669 lambda_n: 0.9487571968613818 Loss: 0.13721651114233788\n",
      "Iteration: 1670 lambda_n: 0.9817015609329393 Loss: 0.13717492042917995\n",
      "Iteration: 1671 lambda_n: 0.9499776789550142 Loss: 0.13713190023142285\n",
      "Iteration: 1672 lambda_n: 0.9694655336702404 Loss: 0.13709028475488996\n",
      "Iteration: 1673 lambda_n: 0.973957875029727 Loss: 0.13704783006092602\n",
      "Iteration: 1674 lambda_n: 0.9495340670733405 Loss: 0.13700519343503992\n",
      "Iteration: 1675 lambda_n: 0.9349121637291468 Loss: 0.1369636404016456\n",
      "Iteration: 1676 lambda_n: 0.9721091580398981 Loss: 0.13692274108712782\n",
      "Iteration: 1677 lambda_n: 0.9725724839295758 Loss: 0.13688022885817186\n",
      "Iteration: 1678 lambda_n: 1.0096027418186517 Loss: 0.13683771114875423\n",
      "Iteration: 1679 lambda_n: 0.9809958592868314 Loss: 0.13679359005822758\n",
      "Iteration: 1680 lambda_n: 0.9350870768979878 Loss: 0.13675073451781597\n",
      "Iteration: 1681 lambda_n: 0.942703457492667 Loss: 0.13670989872646708\n",
      "Iteration: 1682 lambda_n: 1.0032617057469297 Loss: 0.13666874410813304\n",
      "Iteration: 1683 lambda_n: 0.9430286999721571 Loss: 0.13662496071383567\n",
      "Iteration: 1684 lambda_n: 0.9760211047742674 Loss: 0.13658382053303503\n",
      "Iteration: 1685 lambda_n: 0.9499412921840484 Loss: 0.13654125549571794\n",
      "Iteration: 1686 lambda_n: 0.8987345872024283 Loss: 0.1364998421968232\n",
      "Iteration: 1687 lambda_n: 0.9385246894756067 Loss: 0.13646067444864887\n",
      "Iteration: 1688 lambda_n: 1.0239334126628248 Loss: 0.13641978586798467\n",
      "Iteration: 1689 lambda_n: 0.97201516076511 Loss: 0.13637519154029948\n",
      "Iteration: 1690 lambda_n: 0.8943208721105808 Loss: 0.13633287369401773\n",
      "Iteration: 1691 lambda_n: 0.9080720130412366 Loss: 0.13629395167428626\n",
      "Iteration: 1692 lambda_n: 0.9430251897606315 Loss: 0.13625444386253638\n",
      "Iteration: 1693 lambda_n: 0.9481348504830065 Loss: 0.1362134287551538\n",
      "Iteration: 1694 lambda_n: 0.8877060752568041 Loss: 0.13617220533224206\n",
      "Iteration: 1695 lambda_n: 0.9703307381764058 Loss: 0.13613362217439764\n",
      "Iteration: 1696 lambda_n: 1.029585988833735 Loss: 0.13609146146523274\n",
      "Iteration: 1697 lambda_n: 0.98147667675689 Loss: 0.13604674183803628\n",
      "Iteration: 1698 lambda_n: 0.9608342108031938 Loss: 0.1360041273645038\n",
      "Iteration: 1699 lambda_n: 0.9116134367360834 Loss: 0.13596242373988401\n",
      "Iteration: 1700 lambda_n: 0.9943233851452975 Loss: 0.1359228699261856\n",
      "Iteration: 1701 lambda_n: 1.0272628084697615 Loss: 0.13587974174889564\n",
      "Iteration: 1702 lambda_n: 0.889374477038074 Loss: 0.13583520079343095\n",
      "Iteration: 1703 lambda_n: 1.011805230044606 Loss: 0.13579665230279803\n",
      "Iteration: 1704 lambda_n: 0.9761614258131855 Loss: 0.1357528115868747\n",
      "Iteration: 1705 lambda_n: 0.9765927940023987 Loss: 0.1357105304850276\n",
      "Iteration: 1706 lambda_n: 0.9679246208417782 Loss: 0.13566824546816822\n",
      "Iteration: 1707 lambda_n: 0.933014670814675 Loss: 0.13562635038259155\n",
      "Iteration: 1708 lambda_n: 1.021742958649091 Loss: 0.13558598019796608\n",
      "Iteration: 1709 lambda_n: 0.9647208931605384 Loss: 0.13554178590673294\n",
      "Iteration: 1710 lambda_n: 0.9892564627916123 Loss: 0.13550007311624154\n",
      "Iteration: 1711 lambda_n: 0.8870396564296519 Loss: 0.1354573142883921\n",
      "Iteration: 1712 lambda_n: 1.015584205555955 Loss: 0.1354189868823052\n",
      "Iteration: 1713 lambda_n: 1.0158378703969257 Loss: 0.135375119627099\n",
      "Iteration: 1714 lambda_n: 0.9908125715598671 Loss: 0.13533125735750157\n",
      "Iteration: 1715 lambda_n: 0.908389071727654 Loss: 0.1352884911109876\n",
      "Iteration: 1716 lambda_n: 1.0013175224190296 Loss: 0.13524929615872133\n",
      "Iteration: 1717 lambda_n: 1.0078790356282588 Loss: 0.13520610588417217\n",
      "Iteration: 1718 lambda_n: 0.9453552577377938 Loss: 0.13516264818459237\n",
      "Iteration: 1719 lambda_n: 0.9160339754095308 Loss: 0.1351219008990408\n",
      "Iteration: 1720 lambda_n: 1.0059153694020193 Loss: 0.13508243070988957\n",
      "Iteration: 1721 lambda_n: 1.0020331059800323 Loss: 0.1350391021890726\n",
      "Iteration: 1722 lambda_n: 0.8851112122302603 Loss: 0.13499595641642181\n",
      "Iteration: 1723 lambda_n: 0.9221947721108401 Loss: 0.13495785843780905\n",
      "Iteration: 1724 lambda_n: 1.013406782466511 Loss: 0.1349181769461657\n",
      "Iteration: 1725 lambda_n: 0.937763109956924 Loss: 0.13487458533627597\n",
      "Iteration: 1726 lambda_n: 0.9589568861526429 Loss: 0.13483426194293177\n",
      "Iteration: 1727 lambda_n: 0.8836529952309986 Loss: 0.13479304113058516\n",
      "Iteration: 1728 lambda_n: 0.9779531403742027 Loss: 0.13475507009105425\n",
      "Iteration: 1729 lambda_n: 0.9970644892859676 Loss: 0.1347130605035978\n",
      "Iteration: 1730 lambda_n: 0.9223851153651316 Loss: 0.13467024501032154\n",
      "Iteration: 1731 lambda_n: 0.9868972071208294 Loss: 0.1346306502888665\n",
      "Iteration: 1732 lambda_n: 0.9262875989989721 Loss: 0.13458830047978862\n",
      "Iteration: 1733 lambda_n: 0.9545576041527325 Loss: 0.1345485654251827\n",
      "Iteration: 1734 lambda_n: 1.0119167575199315 Loss: 0.1345076313336659\n",
      "Iteration: 1735 lambda_n: 0.9885545765154247 Loss: 0.1344642525338212\n",
      "Iteration: 1736 lambda_n: 0.969011098478023 Loss: 0.1344218905065018\n",
      "Iteration: 1737 lambda_n: 1.0074397001672273 Loss: 0.13438038060691482\n",
      "Iteration: 1738 lambda_n: 0.9287631680460153 Loss: 0.13433723962298375\n",
      "Iteration: 1739 lambda_n: 0.8930303726420791 Loss: 0.13429748187978954\n",
      "Iteration: 1740 lambda_n: 0.990695371675634 Loss: 0.13425926637456814\n",
      "Iteration: 1741 lambda_n: 0.9730924199738787 Loss: 0.13421688534576776\n",
      "Iteration: 1742 lambda_n: 0.9390785305609627 Loss: 0.1341752720739676\n",
      "Iteration: 1743 lambda_n: 0.9075471908077634 Loss: 0.13413512726958768\n",
      "Iteration: 1744 lambda_n: 0.8766056441714661 Loss: 0.13409634336595624\n",
      "Iteration: 1745 lambda_n: 0.9724360784710672 Loss: 0.1340588938409285\n",
      "Iteration: 1746 lambda_n: 1.0094765122487315 Loss: 0.13401736367420747\n",
      "Iteration: 1747 lambda_n: 0.983599564967577 Loss: 0.1339742667456385\n",
      "Iteration: 1748 lambda_n: 0.9097120484953766 Loss: 0.13393228967486975\n",
      "Iteration: 1749 lambda_n: 0.9617021656571454 Loss: 0.13389347937643722\n",
      "Iteration: 1750 lambda_n: 0.9585864370416234 Loss: 0.1338524645964913\n",
      "Iteration: 1751 lambda_n: 0.9529066175987948 Loss: 0.13381159677718127\n",
      "Iteration: 1752 lambda_n: 1.011961905409817 Loss: 0.13377098504833948\n",
      "Iteration: 1753 lambda_n: 0.9567839813397184 Loss: 0.13372787136965852\n",
      "Iteration: 1754 lambda_n: 0.9654622186474312 Loss: 0.13368712311644695\n",
      "Iteration: 1755 lambda_n: 1.0033546531441389 Loss: 0.13364601939591786\n",
      "Iteration: 1756 lambda_n: 0.9308249406635414 Loss: 0.13360331733983227\n",
      "Iteration: 1757 lambda_n: 1.0139111366903688 Loss: 0.13356371613733808\n",
      "Iteration: 1758 lambda_n: 0.9913362525599168 Loss: 0.13352059475549513\n",
      "Iteration: 1759 lambda_n: 1.0012421076351312 Loss: 0.1334784487356742\n",
      "Iteration: 1760 lambda_n: 0.9516060345829771 Loss: 0.13343589673674\n",
      "Iteration: 1761 lambda_n: 0.9825669544173448 Loss: 0.13339546860615978\n",
      "Iteration: 1762 lambda_n: 0.9722978022490033 Loss: 0.1333537394683215\n",
      "Iteration: 1763 lambda_n: 0.8799668735520096 Loss: 0.13331246097676158\n",
      "Iteration: 1764 lambda_n: 0.8818952756103446 Loss: 0.13327511513255497\n",
      "Iteration: 1765 lambda_n: 0.938967204292841 Loss: 0.13323769926561166\n",
      "Iteration: 1766 lambda_n: 0.8914808511806638 Loss: 0.13319787479563625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1767 lambda_n: 0.9562890993561773 Loss: 0.13316007697224422\n",
      "Iteration: 1768 lambda_n: 0.9575194806729439 Loss: 0.13311954450828795\n",
      "Iteration: 1769 lambda_n: 0.9341428285475112 Loss: 0.13307897382249106\n",
      "Iteration: 1770 lambda_n: 0.9365676572035831 Loss: 0.13303940715044926\n",
      "Iteration: 1771 lambda_n: 0.9579444244469987 Loss: 0.13299975107605597\n",
      "Iteration: 1772 lambda_n: 0.9077830678551939 Loss: 0.13295920356384527\n",
      "Iteration: 1773 lambda_n: 0.92900760906332 Loss: 0.13292079233141485\n",
      "Iteration: 1774 lambda_n: 0.9240583104374147 Loss: 0.1328814958878523\n",
      "Iteration: 1775 lambda_n: 0.95778262139512 Loss: 0.13284242181583186\n",
      "Iteration: 1776 lambda_n: 0.9149588230459098 Loss: 0.13280193523469866\n",
      "Iteration: 1777 lambda_n: 0.8854914367243045 Loss: 0.13276327204167696\n",
      "Iteration: 1778 lambda_n: 0.9306814571170686 Loss: 0.13272586625480123\n",
      "Iteration: 1779 lambda_n: 1.0217590471175673 Loss: 0.13268656413581384\n",
      "Iteration: 1780 lambda_n: 1.017783011469299 Loss: 0.13264343056971462\n",
      "Iteration: 1781 lambda_n: 0.8876324075265043 Loss: 0.13260048060313157\n",
      "Iteration: 1782 lambda_n: 0.9607118492807295 Loss: 0.1325630362647291\n",
      "Iteration: 1783 lambda_n: 0.9914799458248643 Loss: 0.13252252224117642\n",
      "Iteration: 1784 lambda_n: 0.8757607104769222 Loss: 0.1324807252217838\n",
      "Iteration: 1785 lambda_n: 0.8881527671387023 Loss: 0.1324438193192884\n",
      "Iteration: 1786 lambda_n: 0.9756476261820061 Loss: 0.13240640300423112\n",
      "Iteration: 1787 lambda_n: 0.9814842739792207 Loss: 0.1323653140612869\n",
      "Iteration: 1788 lambda_n: 1.0103524807981579 Loss: 0.13232399381609541\n",
      "Iteration: 1789 lambda_n: 0.9136629385894665 Loss: 0.13228147331547382\n",
      "Iteration: 1790 lambda_n: 0.9030269492774889 Loss: 0.1322430356629153\n",
      "Iteration: 1791 lambda_n: 0.8872701290510617 Loss: 0.1322050579074874\n",
      "Iteration: 1792 lambda_n: 0.9462121044912345 Loss: 0.13216775488772411\n",
      "Iteration: 1793 lambda_n: 0.9270072607598263 Loss: 0.13212798665906478\n",
      "Iteration: 1794 lambda_n: 0.9022729369173388 Loss: 0.13208903878310935\n",
      "Iteration: 1795 lambda_n: 1.0275381797843164 Loss: 0.1320511426731983\n",
      "Iteration: 1796 lambda_n: 0.8952547482443778 Loss: 0.13200799972715074\n",
      "Iteration: 1797 lambda_n: 1.0068191315620294 Loss: 0.13197042444673884\n",
      "Iteration: 1798 lambda_n: 0.9959304051707065 Loss: 0.1319281805769965\n",
      "Iteration: 1799 lambda_n: 0.9441182077506998 Loss: 0.13188640866695173\n",
      "Iteration: 1800 lambda_n: 0.9972948623696631 Loss: 0.13184682392502464\n",
      "Iteration: 1801 lambda_n: 1.0214967499578518 Loss: 0.1318050239657241\n",
      "Iteration: 1802 lambda_n: 0.9975692146207206 Loss: 0.13176222505620103\n",
      "Iteration: 1803 lambda_n: 0.9747912517850023 Loss: 0.13172044395013613\n",
      "Iteration: 1804 lambda_n: 0.9432400847795186 Loss: 0.13167963143243513\n",
      "Iteration: 1805 lambda_n: 0.9746735289149566 Loss: 0.13164015365360565\n",
      "Iteration: 1806 lambda_n: 0.9414001755614504 Loss: 0.13159937421575876\n",
      "Iteration: 1807 lambda_n: 0.8860501258600397 Loss: 0.13156000061938206\n",
      "Iteration: 1808 lambda_n: 0.9305243372410711 Loss: 0.13152295441284498\n",
      "Iteration: 1809 lambda_n: 0.9744387258684901 Loss: 0.13148406124858547\n",
      "Iteration: 1810 lambda_n: 0.88426778560978 Loss: 0.1314433463628741\n",
      "Iteration: 1811 lambda_n: 0.8874095147138756 Loss: 0.13140641179292062\n",
      "Iteration: 1812 lambda_n: 0.9082397496420876 Loss: 0.13136935780277897\n",
      "Iteration: 1813 lambda_n: 1.00529216030048 Loss: 0.1313314462131098\n",
      "Iteration: 1814 lambda_n: 0.8952484206784495 Loss: 0.13128949749789698\n",
      "Iteration: 1815 lambda_n: 0.9798447317649728 Loss: 0.13125215389152364\n",
      "Iteration: 1816 lambda_n: 0.9216101058812004 Loss: 0.13121129493800557\n",
      "Iteration: 1817 lambda_n: 0.8857104989166265 Loss: 0.13117287772763983\n",
      "Iteration: 1818 lambda_n: 0.9406512229945134 Loss: 0.1311359691429799\n",
      "Iteration: 1819 lambda_n: 0.9872025002996151 Loss: 0.13109678377632084\n",
      "Iteration: 1820 lambda_n: 0.9973401640854395 Loss: 0.13105567326392858\n",
      "Iteration: 1821 lambda_n: 0.9736348068764852 Loss: 0.1310141553832143\n",
      "Iteration: 1822 lambda_n: 0.9757923550851476 Loss: 0.13097363881336002\n",
      "Iteration: 1823 lambda_n: 0.9216915019773647 Loss: 0.13093304670887126\n",
      "Iteration: 1824 lambda_n: 0.9902621871511825 Loss: 0.13089471847796463\n",
      "Iteration: 1825 lambda_n: 0.9252252784888231 Loss: 0.13085355264260048\n",
      "Iteration: 1826 lambda_n: 0.985090637575672 Loss: 0.13081510397252397\n",
      "Iteration: 1827 lambda_n: 0.9075405487523361 Loss: 0.13077418136766525\n",
      "Iteration: 1828 lambda_n: 1.0058641021277193 Loss: 0.1307364935144803\n",
      "Iteration: 1829 lambda_n: 0.9959807402162953 Loss: 0.1306947365018732\n",
      "Iteration: 1830 lambda_n: 0.944590249859017 Loss: 0.13065340474404352\n",
      "Iteration: 1831 lambda_n: 0.9546573615132805 Loss: 0.13061421954211055\n",
      "Iteration: 1832 lambda_n: 1.0101258552744699 Loss: 0.13057463023447857\n",
      "Iteration: 1833 lambda_n: 1.026633821500747 Loss: 0.1305327552591594\n",
      "Iteration: 1834 lambda_n: 0.8853558539017125 Loss: 0.1304902114969854\n",
      "Iteration: 1835 lambda_n: 0.9137834602205992 Loss: 0.13045353550029318\n",
      "Iteration: 1836 lambda_n: 0.8782068145645695 Loss: 0.13041569405298395\n",
      "Iteration: 1837 lambda_n: 1.016375395018695 Loss: 0.1303793377904074\n",
      "Iteration: 1838 lambda_n: 1.0026044031893624 Loss: 0.13033727534607403\n",
      "Iteration: 1839 lambda_n: 0.946743925710491 Loss: 0.1302957979822672\n",
      "Iteration: 1840 lambda_n: 0.9448211647004694 Loss: 0.13025664555028832\n",
      "Iteration: 1841 lambda_n: 0.8780768841423422 Loss: 0.13021758597593747\n",
      "Iteration: 1842 lambda_n: 0.9856824679842527 Loss: 0.13018129785350124\n",
      "Iteration: 1843 lambda_n: 0.9842202459252019 Loss: 0.13014057596989434\n",
      "Iteration: 1844 lambda_n: 0.9694892304041096 Loss: 0.1300999289569366\n",
      "Iteration: 1845 lambda_n: 0.8799744845807699 Loss: 0.1300599044961703\n",
      "Iteration: 1846 lambda_n: 1.0295241913941118 Loss: 0.1300235880404841\n",
      "Iteration: 1847 lambda_n: 0.8794612399402087 Loss: 0.1299811136562673\n",
      "Iteration: 1848 lambda_n: 0.9381514666312738 Loss: 0.12994484339917636\n",
      "Iteration: 1849 lambda_n: 0.8792506991729532 Loss: 0.129906165128653\n",
      "Iteration: 1850 lambda_n: 1.0180310893470041 Loss: 0.12986992735115155\n",
      "Iteration: 1851 lambda_n: 0.9445947352879234 Loss: 0.12982798358391726\n",
      "Iteration: 1852 lambda_n: 0.9469248474087263 Loss: 0.12978907955574445\n",
      "Iteration: 1853 lambda_n: 0.8914161318922397 Loss: 0.1297500928742978\n",
      "Iteration: 1854 lambda_n: 0.925297267337827 Loss: 0.12971340399456183\n",
      "Iteration: 1855 lambda_n: 1.0122245418648543 Loss: 0.12967533299833425\n",
      "Iteration: 1856 lambda_n: 1.0093217199580713 Loss: 0.12963369960904952\n",
      "Iteration: 1857 lambda_n: 0.9883323509982208 Loss: 0.12959220079138228\n",
      "Iteration: 1858 lambda_n: 0.951874578976587 Loss: 0.12955157972289166\n",
      "Iteration: 1859 lambda_n: 0.9481985047585209 Loss: 0.12951247096073112\n",
      "Iteration: 1860 lambda_n: 0.8901424775673479 Loss: 0.12947352662857323\n",
      "Iteration: 1861 lambda_n: 0.8788535213018679 Loss: 0.12943697914269858\n",
      "Iteration: 1862 lambda_n: 0.9054676576380125 Loss: 0.129400906742044\n",
      "Iteration: 1863 lambda_n: 1.003715422279802 Loss: 0.1293637538535802\n",
      "Iteration: 1864 lambda_n: 0.9768656432719055 Loss: 0.12932258348077078\n",
      "Iteration: 1865 lambda_n: 0.9292550423690874 Loss: 0.12928252889676592\n",
      "Iteration: 1866 lambda_n: 0.9963591416792953 Loss: 0.1292444398254214\n",
      "Iteration: 1867 lambda_n: 1.0269081584015511 Loss: 0.12920361417081588\n",
      "Iteration: 1868 lambda_n: 0.9321864616965703 Loss: 0.12916155203998225\n",
      "Iteration: 1869 lambda_n: 0.9085244720287359 Loss: 0.1291233836229708\n",
      "Iteration: 1870 lambda_n: 1.0216874649931287 Loss: 0.12908619652985723\n",
      "Iteration: 1871 lambda_n: 0.9421400684764619 Loss: 0.1290443916344979\n",
      "Iteration: 1872 lambda_n: 1.0019751807495136 Loss: 0.12900585564485464\n",
      "Iteration: 1873 lambda_n: 0.9921184269421096 Loss: 0.12896488640521012\n",
      "Iteration: 1874 lambda_n: 0.9057388432217475 Loss: 0.12892433487879698\n",
      "Iteration: 1875 lambda_n: 1.020393668881365 Loss: 0.1288873270613368\n",
      "Iteration: 1876 lambda_n: 0.9042808818455914 Loss: 0.1288456485681363\n",
      "Iteration: 1877 lambda_n: 0.9880705325316361 Loss: 0.12880872608626023\n",
      "Iteration: 1878 lambda_n: 0.9202196783971017 Loss: 0.1287683958780479\n",
      "Iteration: 1879 lambda_n: 0.8961972492623257 Loss: 0.12873084839887902\n",
      "Iteration: 1880 lambda_n: 0.8862827147157053 Loss: 0.1286942932252337\n",
      "Iteration: 1881 lambda_n: 0.9886600205833298 Loss: 0.12865815416939522\n",
      "Iteration: 1882 lambda_n: 0.9293013081907375 Loss: 0.1286178538369537\n",
      "Iteration: 1883 lambda_n: 0.8904142734913145 Loss: 0.1285799865214799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1884 lambda_n: 0.950118699968422 Loss: 0.12854371589541652\n",
      "Iteration: 1885 lambda_n: 1.0120963286868347 Loss: 0.12850502590359478\n",
      "Iteration: 1886 lambda_n: 0.9456988114797481 Loss: 0.12846382649722177\n",
      "Iteration: 1887 lambda_n: 0.9988843389035719 Loss: 0.12842534386819565\n",
      "Iteration: 1888 lambda_n: 0.9046455949105026 Loss: 0.12838471110301733\n",
      "Iteration: 1889 lambda_n: 0.9458420302762807 Loss: 0.12834792486381488\n",
      "Iteration: 1890 lambda_n: 1.0154951983130578 Loss: 0.12830947616597255\n",
      "Iteration: 1891 lambda_n: 0.9471128777622234 Loss: 0.1282682104332877\n",
      "Iteration: 1892 lambda_n: 0.8910849976860722 Loss: 0.12822973746945207\n",
      "Iteration: 1893 lambda_n: 1.0012811661506942 Loss: 0.12819355271719243\n",
      "Iteration: 1894 lambda_n: 0.9077766797594222 Loss: 0.12815290666121357\n",
      "Iteration: 1895 lambda_n: 0.9890971513949255 Loss: 0.1281160694610759\n",
      "Iteration: 1896 lambda_n: 0.8928365663816731 Loss: 0.12807594578438292\n",
      "Iteration: 1897 lambda_n: 0.9384712651970469 Loss: 0.12803973975148905\n",
      "Iteration: 1898 lambda_n: 0.9672933065865865 Loss: 0.12800169562423858\n",
      "Iteration: 1899 lambda_n: 1.0208390706895256 Loss: 0.1279624965473648\n",
      "Iteration: 1900 lambda_n: 0.8901416332523765 Loss: 0.12792114225789025\n",
      "Iteration: 1901 lambda_n: 0.8756458254812137 Loss: 0.12788509554614905\n",
      "Iteration: 1902 lambda_n: 0.894091091593677 Loss: 0.12784964727697803\n",
      "Iteration: 1903 lambda_n: 0.9872813614923464 Loss: 0.12781346386543804\n",
      "Iteration: 1904 lambda_n: 0.9264761466831768 Loss: 0.12777352235720252\n",
      "Iteration: 1905 lambda_n: 0.8977606871929295 Loss: 0.12773605406289124\n",
      "Iteration: 1906 lambda_n: 0.9452055256849126 Loss: 0.12769975921857987\n",
      "Iteration: 1907 lambda_n: 0.9893432280277206 Loss: 0.1276615588756134\n",
      "Iteration: 1908 lambda_n: 0.9592073374274862 Loss: 0.12762158859049297\n",
      "Iteration: 1909 lambda_n: 1.0040883175919804 Loss: 0.1275828496730453\n",
      "Iteration: 1910 lambda_n: 0.9531881445803185 Loss: 0.12754231246169223\n",
      "Iteration: 1911 lambda_n: 1.0255234193288394 Loss: 0.12750384410539276\n",
      "Iteration: 1912 lambda_n: 0.9582622687256794 Loss: 0.12746247105568528\n",
      "Iteration: 1913 lambda_n: 0.8940707824204525 Loss: 0.12742382577258818\n",
      "Iteration: 1914 lambda_n: 0.8820161759962445 Loss: 0.12738778163254763\n",
      "Iteration: 1915 lambda_n: 0.8759430247261599 Loss: 0.12735223500820456\n",
      "Iteration: 1916 lambda_n: 0.876829342979688 Loss: 0.12731694445970945\n",
      "Iteration: 1917 lambda_n: 0.9536076233571684 Loss: 0.12728162947130764\n",
      "Iteration: 1918 lambda_n: 0.9128851670793896 Loss: 0.12724323466778426\n",
      "Iteration: 1919 lambda_n: 0.9839567394856824 Loss: 0.12720649210956927\n",
      "Iteration: 1920 lambda_n: 0.9245416829714902 Loss: 0.12716690238985934\n",
      "Iteration: 1921 lambda_n: 0.9078692007148814 Loss: 0.1271297164125906\n",
      "Iteration: 1922 lambda_n: 0.9138550383052377 Loss: 0.12709321327329454\n",
      "Iteration: 1923 lambda_n: 0.949840374410846 Loss: 0.12705648163246253\n",
      "Iteration: 1924 lambda_n: 1.0139912793336243 Loss: 0.12701831641406766\n",
      "Iteration: 1925 lambda_n: 0.918867375747865 Loss: 0.12697758787967053\n",
      "Iteration: 1926 lambda_n: 0.8954146901394092 Loss: 0.12694069351848786\n",
      "Iteration: 1927 lambda_n: 0.8942898235554768 Loss: 0.12690475281170224\n",
      "Iteration: 1928 lambda_n: 1.021196926612528 Loss: 0.12686886897564198\n",
      "Iteration: 1929 lambda_n: 0.9416856351820765 Loss: 0.12682790669827915\n",
      "Iteration: 1930 lambda_n: 0.9081570971238431 Loss: 0.12679014762128885\n",
      "Iteration: 1931 lambda_n: 0.90679276363927 Loss: 0.12675374536836348\n",
      "Iteration: 1932 lambda_n: 0.9093699262282267 Loss: 0.12671740984741173\n",
      "Iteration: 1933 lambda_n: 0.989407604581857 Loss: 0.1266809831284628\n",
      "Iteration: 1934 lambda_n: 1.0127764785639901 Loss: 0.12664136373869134\n",
      "Iteration: 1935 lambda_n: 0.9930142095991119 Loss: 0.1266008233039045\n",
      "Iteration: 1936 lambda_n: 0.9089035144987845 Loss: 0.1265610885768023\n",
      "Iteration: 1937 lambda_n: 0.955385269839307 Loss: 0.12652473243291917\n",
      "Iteration: 1938 lambda_n: 0.8915611093120002 Loss: 0.1264865298494842\n",
      "Iteration: 1939 lambda_n: 0.9484393539355263 Loss: 0.1264508916464515\n",
      "Iteration: 1940 lambda_n: 1.005455221709883 Loss: 0.1264129923844592\n",
      "Iteration: 1941 lambda_n: 1.0184982808044623 Loss: 0.12637282890282786\n",
      "Iteration: 1942 lambda_n: 0.9994660242923783 Loss: 0.12633215940938763\n",
      "Iteration: 1943 lambda_n: 0.9497321245985743 Loss: 0.12629226469925567\n",
      "Iteration: 1944 lambda_n: 1.0258609016708555 Loss: 0.1262543688834736\n",
      "Iteration: 1945 lambda_n: 1.01719311610318 Loss: 0.12621344987946126\n",
      "Iteration: 1946 lambda_n: 0.9990256573749882 Loss: 0.12617289181899172\n",
      "Iteration: 1947 lambda_n: 0.935996531963287 Loss: 0.12613307291857884\n",
      "Iteration: 1948 lambda_n: 0.9406189428099067 Loss: 0.12609577967998764\n",
      "Iteration: 1949 lambda_n: 1.0118684680769692 Loss: 0.12605831513117335\n",
      "Iteration: 1950 lambda_n: 0.8974684269188887 Loss: 0.12601802685093566\n",
      "Iteration: 1951 lambda_n: 0.8866072399825642 Loss: 0.1259823064196877\n",
      "Iteration: 1952 lambda_n: 1.0004048045248 Loss: 0.12594702985571338\n",
      "Iteration: 1953 lambda_n: 0.949324546811539 Loss: 0.1259072387788967\n",
      "Iteration: 1954 lambda_n: 0.8802680190267826 Loss: 0.12586949311774687\n",
      "Iteration: 1955 lambda_n: 0.880547212047112 Loss: 0.12583450517778957\n",
      "Iteration: 1956 lambda_n: 0.8874363995288219 Loss: 0.12579951744409665\n",
      "Iteration: 1957 lambda_n: 0.9746061845474489 Loss: 0.12576426738757127\n",
      "Iteration: 1958 lambda_n: 0.9545666307021655 Loss: 0.12572556771719592\n",
      "Iteration: 1959 lambda_n: 0.8994524542131165 Loss: 0.1256876772796219\n",
      "Iteration: 1960 lambda_n: 0.9294740275416821 Loss: 0.1256519868958002\n",
      "Iteration: 1961 lambda_n: 0.8901885014910317 Loss: 0.12561511752492233\n",
      "Iteration: 1962 lambda_n: 1.0115501807807215 Loss: 0.12557981843810775\n",
      "Iteration: 1963 lambda_n: 0.922039758097906 Loss: 0.12553972044505568\n",
      "Iteration: 1964 lambda_n: 0.9882753868652754 Loss: 0.12550318398815707\n",
      "Iteration: 1965 lambda_n: 0.8811715716623933 Loss: 0.12546403638262396\n",
      "Iteration: 1966 lambda_n: 0.961705059579693 Loss: 0.12542914376761755\n",
      "Iteration: 1967 lambda_n: 0.9270420388763411 Loss: 0.12539107477872108\n",
      "Iteration: 1968 lambda_n: 0.9559803845050318 Loss: 0.12535439080741143\n",
      "Iteration: 1969 lambda_n: 0.9283385686295723 Loss: 0.12531657471348515\n",
      "Iteration: 1970 lambda_n: 0.8940928149635397 Loss: 0.1252798649029935\n",
      "Iteration: 1971 lambda_n: 1.0081162997075745 Loss: 0.12524452128334335\n",
      "Iteration: 1972 lambda_n: 0.9514897936838225 Loss: 0.1252046837892942\n",
      "Iteration: 1973 lambda_n: 0.9986635542566211 Loss: 0.12516709779012974\n",
      "Iteration: 1974 lambda_n: 1.0245051623089403 Loss: 0.12512766230739622\n",
      "Iteration: 1975 lambda_n: 0.9922800089235422 Loss: 0.12508722136739114\n",
      "Iteration: 1976 lambda_n: 1.0116511898196445 Loss: 0.12504806717290853\n",
      "Iteration: 1977 lambda_n: 1.0160320819932631 Loss: 0.12500816329356043\n",
      "Iteration: 1978 lambda_n: 0.9803791855174008 Loss: 0.12496810159342456\n",
      "Iteration: 1979 lambda_n: 0.9806791618353546 Loss: 0.12492946006361942\n",
      "Iteration: 1980 lambda_n: 0.9850133414009636 Loss: 0.12489082070953639\n",
      "Iteration: 1981 lambda_n: 0.9931742464236888 Loss: 0.12485202466405251\n",
      "Iteration: 1982 lambda_n: 0.9336696221105764 Loss: 0.12481292145984245\n",
      "Iteration: 1983 lambda_n: 0.9224119940070044 Loss: 0.12477617439243938\n",
      "Iteration: 1984 lambda_n: 0.9008322271974573 Loss: 0.1247398829048719\n",
      "Iteration: 1985 lambda_n: 0.9637508322151758 Loss: 0.1247044524900976\n",
      "Iteration: 1986 lambda_n: 0.9815105021215159 Loss: 0.12466656027276994\n",
      "Iteration: 1987 lambda_n: 0.9312236874248541 Loss: 0.12462798361915824\n",
      "Iteration: 1988 lambda_n: 1.00967077421041 Loss: 0.12459139656448032\n",
      "Iteration: 1989 lambda_n: 0.9851635777433523 Loss: 0.12455174132872243\n",
      "Iteration: 1990 lambda_n: 1.0181312618257585 Loss: 0.12451306303008665\n",
      "Iteration: 1991 lambda_n: 0.9906615815089848 Loss: 0.12447310510799404\n",
      "Iteration: 1992 lambda_n: 0.9788179653535042 Loss: 0.12443423987143427\n",
      "Iteration: 1993 lambda_n: 0.9465770143640959 Loss: 0.124395853363695\n",
      "Iteration: 1994 lambda_n: 0.8910911049833224 Loss: 0.12435874465234902\n",
      "Iteration: 1995 lambda_n: 0.8934274803350065 Loss: 0.12432382328478377\n",
      "Iteration: 1996 lambda_n: 0.947338569737951 Loss: 0.12428882195914856\n",
      "Iteration: 1997 lambda_n: 1.0173187215406887 Loss: 0.12425172108191121\n",
      "Iteration: 1998 lambda_n: 0.9696994125439118 Loss: 0.12421189382327234\n",
      "Iteration: 1999 lambda_n: 0.9416947851329928 Loss: 0.12417394504916919\n",
      "Iteration: 2000 lambda_n: 0.91700132828841 Loss: 0.12413710544540786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2001 lambda_n: 0.9089459701479734 Loss: 0.1241012443741333\n",
      "Iteration: 2002 lambda_n: 1.0271905816565852 Loss: 0.12406571044728196\n",
      "Iteration: 2003 lambda_n: 0.9514540620799122 Loss: 0.12402556792167209\n",
      "Iteration: 2004 lambda_n: 0.9555713922067436 Loss: 0.1239883991877726\n",
      "Iteration: 2005 lambda_n: 1.002990343256981 Loss: 0.12395108287887224\n",
      "Iteration: 2006 lambda_n: 0.891162421659701 Loss: 0.12391192892938371\n",
      "Iteration: 2007 lambda_n: 0.9050226810761695 Loss: 0.12387715314297494\n",
      "Iteration: 2008 lambda_n: 0.8777069592304787 Loss: 0.12384184829410108\n",
      "Iteration: 2009 lambda_n: 0.9439705418598624 Loss: 0.12380762053992304\n",
      "Iteration: 2010 lambda_n: 0.9360369617469959 Loss: 0.12377082101274833\n",
      "Iteration: 2011 lambda_n: 0.9995724686129591 Loss: 0.12373434363808156\n",
      "Iteration: 2012 lambda_n: 1.0058781469277376 Loss: 0.12369540414972639\n",
      "Iteration: 2013 lambda_n: 0.9003405187830074 Loss: 0.12365623371796707\n",
      "Iteration: 2014 lambda_n: 0.918188170021915 Loss: 0.12362118599732318\n",
      "Iteration: 2015 lambda_n: 1.0259327800134388 Loss: 0.12358545564717421\n",
      "Iteration: 2016 lambda_n: 0.9500004792210376 Loss: 0.12354554665272541\n",
      "Iteration: 2017 lambda_n: 0.9668361018133815 Loss: 0.12350860544700262\n",
      "Iteration: 2018 lambda_n: 0.9922123695119747 Loss: 0.12347102306085067\n",
      "Iteration: 2019 lambda_n: 0.9823635942813106 Loss: 0.12343246837178416\n",
      "Iteration: 2020 lambda_n: 0.9987117032935876 Loss: 0.12339431060366195\n",
      "Iteration: 2021 lambda_n: 1.0003892865829862 Loss: 0.12335553223746068\n",
      "Iteration: 2022 lambda_n: 0.9085349535695203 Loss: 0.12331670336151941\n",
      "Iteration: 2023 lambda_n: 1.017927534405397 Loss: 0.12328145272777447\n",
      "Iteration: 2024 lambda_n: 0.9601339639711509 Loss: 0.123241971638649\n",
      "Iteration: 2025 lambda_n: 0.9640868084229937 Loss: 0.12320474625065624\n",
      "Iteration: 2026 lambda_n: 0.941428952999499 Loss: 0.12316738118522838\n",
      "Iteration: 2027 lambda_n: 0.9703766047187332 Loss: 0.12313090750431324\n",
      "Iteration: 2028 lambda_n: 0.9067299646960874 Loss: 0.12309332579959602\n",
      "Iteration: 2029 lambda_n: 0.9069828653492155 Loss: 0.12305822178101876\n",
      "Iteration: 2030 lambda_n: 0.9237529425555314 Loss: 0.12302312004259637\n",
      "Iteration: 2031 lambda_n: 0.9612202854610692 Loss: 0.12298738162835611\n",
      "Iteration: 2032 lambda_n: 0.8881193698346945 Loss: 0.12295020683120222\n",
      "Iteration: 2033 lambda_n: 0.8781266696231659 Loss: 0.12291587150855231\n",
      "Iteration: 2034 lambda_n: 0.8899275167658853 Loss: 0.12288193394685117\n",
      "Iteration: 2035 lambda_n: 0.9151797632563359 Loss: 0.1228475518392476\n",
      "Iteration: 2036 lambda_n: 1.0116883711462399 Loss: 0.12281220618069087\n",
      "Iteration: 2037 lambda_n: 0.9432976609860715 Loss: 0.12277314717245245\n",
      "Iteration: 2038 lambda_n: 1.0089352468509705 Loss: 0.12273674241385281\n",
      "Iteration: 2039 lambda_n: 0.8807125731393641 Loss: 0.12269781875097849\n",
      "Iteration: 2040 lambda_n: 0.8838642413772142 Loss: 0.12266385450274675\n",
      "Iteration: 2041 lambda_n: 0.9048402849727651 Loss: 0.12262978020642548\n",
      "Iteration: 2042 lambda_n: 1.0112346354459187 Loss: 0.12259490912299435\n",
      "Iteration: 2043 lambda_n: 0.9598108432693052 Loss: 0.12255595165726658\n",
      "Iteration: 2044 lambda_n: 0.9756605586877509 Loss: 0.12251898945162397\n",
      "Iteration: 2045 lambda_n: 0.9372480987062612 Loss: 0.12248143077842567\n",
      "Iteration: 2046 lambda_n: 1.0043828876818464 Loss: 0.12244536422640254\n",
      "Iteration: 2047 lambda_n: 1.0161496505445056 Loss: 0.1224067284127407\n",
      "Iteration: 2048 lambda_n: 1.0196602684533966 Loss: 0.12236765513183004\n",
      "Iteration: 2049 lambda_n: 0.9352552978999258 Loss: 0.12232846223979839\n",
      "Iteration: 2050 lambda_n: 0.9177661238356648 Loss: 0.12229252751939866\n",
      "Iteration: 2051 lambda_n: 0.9609110780538912 Loss: 0.12225727746843706\n",
      "Iteration: 2052 lambda_n: 0.9463983686949521 Loss: 0.1222203835377818\n",
      "Iteration: 2053 lambda_n: 0.8839750338478177 Loss: 0.12218406030032171\n",
      "Iteration: 2054 lambda_n: 0.9249917427288242 Loss: 0.12215014516377962\n",
      "Iteration: 2055 lambda_n: 1.0002171185950337 Loss: 0.12211466867251582\n",
      "Iteration: 2056 lambda_n: 0.9215750924191274 Loss: 0.12207632110348553\n",
      "Iteration: 2057 lambda_n: 1.0041857297513945 Loss: 0.1220410021138559\n",
      "Iteration: 2058 lambda_n: 0.9294553420315081 Loss: 0.12200253122689572\n",
      "Iteration: 2059 lambda_n: 1.0265335095791508 Loss: 0.1219669370124885\n",
      "Iteration: 2060 lambda_n: 0.953782388410491 Loss: 0.12192763975346327\n",
      "Iteration: 2061 lambda_n: 1.0082278391209791 Loss: 0.12189114194215492\n",
      "Iteration: 2062 lambda_n: 0.8802660813739316 Loss: 0.12185257532310671\n",
      "Iteration: 2063 lambda_n: 0.9983857012105178 Loss: 0.12181891640601235\n",
      "Iteration: 2064 lambda_n: 1.0208897181916863 Loss: 0.12178075456231677\n",
      "Iteration: 2065 lambda_n: 0.9704950006032009 Loss: 0.12174174795913136\n",
      "Iteration: 2066 lambda_n: 0.8913163955018902 Loss: 0.1217046816118592\n",
      "Iteration: 2067 lambda_n: 1.0024463393114753 Loss: 0.1216706521450839\n",
      "Iteration: 2068 lambda_n: 0.8759423971500573 Loss: 0.12163239376380537\n",
      "Iteration: 2069 lambda_n: 0.9492230395021574 Loss: 0.1215989762806303\n",
      "Iteration: 2070 lambda_n: 1.0046651160495996 Loss: 0.12156277596278839\n",
      "Iteration: 2071 lambda_n: 1.0231237492312704 Loss: 0.12152447592620227\n",
      "Iteration: 2072 lambda_n: 0.9431432346428378 Loss: 0.121485487881958\n",
      "Iteration: 2073 lambda_n: 0.9104620336654096 Loss: 0.12144956203567668\n",
      "Iteration: 2074 lambda_n: 0.9931072355931563 Loss: 0.12141489403180337\n",
      "Iteration: 2075 lambda_n: 0.9337070417185918 Loss: 0.12137709320873064\n",
      "Iteration: 2076 lambda_n: 0.9632695201459356 Loss: 0.12134156729675752\n",
      "Iteration: 2077 lambda_n: 0.8880016935657307 Loss: 0.12130493044608837\n",
      "Iteration: 2078 lambda_n: 1.0269311440662992 Loss: 0.1212711691687477\n",
      "Iteration: 2079 lambda_n: 0.954282275504326 Loss: 0.12123214041001462\n",
      "Iteration: 2080 lambda_n: 0.9334617202048532 Loss: 0.12119588748195\n",
      "Iteration: 2081 lambda_n: 1.004536317804467 Loss: 0.12116043915743309\n",
      "Iteration: 2082 lambda_n: 0.9693019085235524 Loss: 0.12112230649826305\n",
      "Iteration: 2083 lambda_n: 0.9000028511550255 Loss: 0.12108552626117988\n",
      "Iteration: 2084 lambda_n: 0.9802691112653227 Loss: 0.12105138883627675\n",
      "Iteration: 2085 lambda_n: 1.027757258415448 Loss: 0.12101422087139997\n",
      "Iteration: 2086 lambda_n: 0.8852471228789117 Loss: 0.12097526817964285\n",
      "Iteration: 2087 lambda_n: 0.9326129473365622 Loss: 0.120941730378136\n",
      "Iteration: 2088 lambda_n: 0.9440977809472281 Loss: 0.12090641116384819\n",
      "Iteration: 2089 lambda_n: 0.9877666568243506 Loss: 0.12087067080184292\n",
      "Iteration: 2090 lambda_n: 1.0149396899046934 Loss: 0.1208332920549465\n",
      "Iteration: 2091 lambda_n: 0.9253896068380628 Loss: 0.12079490087619472\n",
      "Iteration: 2092 lambda_n: 0.9888229815626965 Loss: 0.12075991146304826\n",
      "Iteration: 2093 lambda_n: 1.0173243619221481 Loss: 0.12072253827753561\n",
      "Iteration: 2094 lambda_n: 0.9759823381096524 Loss: 0.12068410387461265\n",
      "Iteration: 2095 lambda_n: 0.9787623929977437 Loss: 0.12064724692866138\n",
      "Iteration: 2096 lambda_n: 0.8854786346936188 Loss: 0.12061030016993513\n",
      "Iteration: 2097 lambda_n: 1.0119370388309734 Loss: 0.1205768881817541\n",
      "Iteration: 2098 lambda_n: 0.9362994247521794 Loss: 0.12053871933937992\n",
      "Iteration: 2099 lambda_n: 0.972575154260619 Loss: 0.12050341830115864\n",
      "Iteration: 2100 lambda_n: 1.020085152213372 Loss: 0.12046676432336223\n",
      "Iteration: 2101 lambda_n: 0.8760569654074457 Loss: 0.12042833597255614\n",
      "Iteration: 2102 lambda_n: 0.9269102042557493 Loss: 0.12039534728432656\n",
      "Iteration: 2103 lambda_n: 0.9594972970241135 Loss: 0.1203604570076262\n",
      "Iteration: 2104 lambda_n: 0.9101152334080265 Loss: 0.12032435466374125\n",
      "Iteration: 2105 lambda_n: 0.954226394465055 Loss: 0.12029012439874043\n",
      "Iteration: 2106 lambda_n: 0.9824744231734401 Loss: 0.1202542494738871\n",
      "Iteration: 2107 lambda_n: 0.9740944170400234 Loss: 0.1202177883363603\n",
      "Iteration: 2108 lambda_n: 0.9289190722977438 Loss: 0.12018200532951498\n",
      "Iteration: 2109 lambda_n: 0.9838239099004648 Loss: 0.12014790375455948\n",
      "Iteration: 2110 lambda_n: 1.0044564766682256 Loss: 0.12011180327283613\n",
      "Iteration: 2111 lambda_n: 0.9840062016711032 Loss: 0.12007496343122719\n",
      "Iteration: 2112 lambda_n: 0.9773365591278401 Loss: 0.12003889117515754\n",
      "Iteration: 2113 lambda_n: 0.9397894955799772 Loss: 0.12000308052025602\n",
      "Iteration: 2114 lambda_n: 0.921430362608362 Loss: 0.11996866173046486\n",
      "Iteration: 2115 lambda_n: 0.9074184348266505 Loss: 0.11993493053952053\n",
      "Iteration: 2116 lambda_n: 1.0075280240679396 Loss: 0.11990172698764415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2117 lambda_n: 0.939697495253876 Loss: 0.11986487665874748\n",
      "Iteration: 2118 lambda_n: 0.9101230752463145 Loss: 0.11983052361162204\n",
      "Iteration: 2119 lambda_n: 1.026309464962859 Loss: 0.11979726658046409\n",
      "Iteration: 2120 lambda_n: 0.923104854629976 Loss: 0.11975978057699548\n",
      "Iteration: 2121 lambda_n: 1.0053968423389736 Loss: 0.11972608029044994\n",
      "Iteration: 2122 lambda_n: 1.0025996757606208 Loss: 0.11968939203563186\n",
      "Iteration: 2123 lambda_n: 0.8955885720825943 Loss: 0.11965282326784753\n",
      "Iteration: 2124 lambda_n: 0.8783621877116595 Loss: 0.11962017280789664\n",
      "Iteration: 2125 lambda_n: 0.8963933404392573 Loss: 0.1195881638324234\n",
      "Iteration: 2126 lambda_n: 0.8980152085689042 Loss: 0.11955551131699058\n",
      "Iteration: 2127 lambda_n: 0.9679669758420806 Loss: 0.11952281349506377\n",
      "Iteration: 2128 lambda_n: 0.9825438189820468 Loss: 0.11948758368232427\n",
      "Iteration: 2129 lambda_n: 0.8980159773530735 Loss: 0.11945183957214427\n",
      "Iteration: 2130 lambda_n: 0.8946986216858859 Loss: 0.11941918530407417\n",
      "Iteration: 2131 lambda_n: 0.9245443210270997 Loss: 0.11938666520972174\n",
      "Iteration: 2132 lambda_n: 0.8848502590645281 Loss: 0.11935307439351567\n",
      "Iteration: 2133 lambda_n: 0.8995070174796604 Loss: 0.11932093945407789\n",
      "Iteration: 2134 lambda_n: 0.8802452170398601 Loss: 0.11928828566780407\n",
      "Iteration: 2135 lambda_n: 0.9752626658390943 Loss: 0.11925634437284652\n",
      "Iteration: 2136 lambda_n: 0.9486992786135658 Loss: 0.11922096986834099\n",
      "Iteration: 2137 lambda_n: 0.9951744852912928 Loss: 0.11918657427038477\n",
      "Iteration: 2138 lambda_n: 0.8756045130754628 Loss: 0.11915050961883697\n",
      "Iteration: 2139 lambda_n: 0.8840308896945942 Loss: 0.11911879230415731\n",
      "Iteration: 2140 lambda_n: 0.8934549751899973 Loss: 0.11908678264716367\n",
      "Iteration: 2141 lambda_n: 0.946337749976719 Loss: 0.11905444488494832\n",
      "Iteration: 2142 lambda_n: 1.018506079794767 Loss: 0.11902020723086316\n",
      "Iteration: 2143 lambda_n: 0.9401157638456975 Loss: 0.11898337474031796\n",
      "Iteration: 2144 lambda_n: 1.0291351484705478 Loss: 0.11894939267073885\n",
      "Iteration: 2145 lambda_n: 0.9408857982091507 Loss: 0.11891220904219159\n",
      "Iteration: 2146 lambda_n: 1.0091621632652696 Loss: 0.11887822960292416\n",
      "Iteration: 2147 lambda_n: 0.9142142831881287 Loss: 0.11884180017838891\n",
      "Iteration: 2148 lambda_n: 0.9172518014764494 Loss: 0.11880881308030995\n",
      "Iteration: 2149 lambda_n: 1.0073576494883616 Loss: 0.118775730078914\n",
      "Iteration: 2150 lambda_n: 0.8928030006286125 Loss: 0.11873941250973666\n",
      "Iteration: 2151 lambda_n: 0.964707614277201 Loss: 0.11870723922310444\n",
      "Iteration: 2152 lambda_n: 0.8831645925168986 Loss: 0.11867248895011702\n",
      "Iteration: 2153 lambda_n: 0.9077349460133214 Loss: 0.11864068957463218\n",
      "Iteration: 2154 lambda_n: 0.9282124632763006 Loss: 0.1186080185281123\n",
      "Iteration: 2155 lambda_n: 0.8974274268229712 Loss: 0.1185746241359843\n",
      "Iteration: 2156 lambda_n: 0.9135615252549797 Loss: 0.11854235064496915\n",
      "Iteration: 2157 lambda_n: 0.9747868816972984 Loss: 0.11850951016990878\n",
      "Iteration: 2158 lambda_n: 0.9850941227121379 Loss: 0.11847448326179277\n",
      "Iteration: 2159 lambda_n: 0.9055082628654083 Loss: 0.11843910141087635\n",
      "Iteration: 2160 lambda_n: 1.0292512709862085 Loss: 0.1184065921246762\n",
      "Iteration: 2161 lambda_n: 1.0124032800323264 Loss: 0.11836965552127707\n",
      "Iteration: 2162 lambda_n: 0.8958647256843705 Loss: 0.1183333401066472\n",
      "Iteration: 2163 lambda_n: 0.976235451109111 Loss: 0.11830121911513006\n",
      "Iteration: 2164 lambda_n: 0.9049090789568408 Loss: 0.11826623058164584\n",
      "Iteration: 2165 lambda_n: 0.9755644856177739 Loss: 0.11823381222537178\n",
      "Iteration: 2166 lambda_n: 0.9667177205712905 Loss: 0.11819887682408144\n",
      "Iteration: 2167 lambda_n: 1.0220165668475807 Loss: 0.11816427311385642\n",
      "Iteration: 2168 lambda_n: 0.94739945664533 Loss: 0.11812770573934832\n",
      "Iteration: 2169 lambda_n: 0.9829876629975521 Loss: 0.11809382318264224\n",
      "Iteration: 2170 lambda_n: 0.8877128997621702 Loss: 0.11805868260865543\n",
      "Iteration: 2171 lambda_n: 0.8760307302867595 Loss: 0.11802696143779294\n",
      "Iteration: 2172 lambda_n: 0.9219500248803353 Loss: 0.11799566987218846\n",
      "Iteration: 2173 lambda_n: 0.904233925818067 Loss: 0.11796275084891954\n",
      "Iteration: 2174 lambda_n: 1.019874320937906 Loss: 0.11793047737484182\n",
      "Iteration: 2175 lambda_n: 1.02248759639698 Loss: 0.11789409125987307\n",
      "Iteration: 2176 lambda_n: 1.0054071846415533 Loss: 0.11785762816515817\n",
      "Iteration: 2177 lambda_n: 0.9535885579005311 Loss: 0.1178217901152925\n",
      "Iteration: 2178 lambda_n: 0.9801994451380802 Loss: 0.11778781389556586\n",
      "Iteration: 2179 lambda_n: 1.0110863579340519 Loss: 0.11775290409968597\n",
      "Iteration: 2180 lambda_n: 0.9999179375299092 Loss: 0.11771690969722814\n",
      "Iteration: 2181 lambda_n: 0.8955241826462073 Loss: 0.11768132847583686\n",
      "Iteration: 2182 lambda_n: 0.953597353851256 Loss: 0.11764947555262387\n",
      "Iteration: 2183 lambda_n: 0.9547547334207779 Loss: 0.11761557033959066\n",
      "Iteration: 2184 lambda_n: 0.9339824262580282 Loss: 0.1175816379809581\n",
      "Iteration: 2185 lambda_n: 0.9017370981256234 Loss: 0.1175484575136995\n",
      "Iteration: 2186 lambda_n: 0.9831084098968553 Loss: 0.1175164354044445\n",
      "Iteration: 2187 lambda_n: 0.9577116745939133 Loss: 0.11748153748907696\n",
      "Iteration: 2188 lambda_n: 0.9378826781629314 Loss: 0.11744755541808508\n",
      "Iteration: 2189 lambda_n: 0.9995149043452233 Loss: 0.11741429058670658\n",
      "Iteration: 2190 lambda_n: 0.9413602864858608 Loss: 0.11737885425354247\n",
      "Iteration: 2191 lambda_n: 1.0291138572721827 Loss: 0.11734549386489909\n",
      "Iteration: 2192 lambda_n: 0.9929681961151643 Loss: 0.11730903861110507\n",
      "Iteration: 2193 lambda_n: 1.0208101026179988 Loss: 0.11727387917835858\n",
      "Iteration: 2194 lambda_n: 0.995207780600279 Loss: 0.11723774934881266\n",
      "Iteration: 2195 lambda_n: 0.9742348206700091 Loss: 0.11720254096257295\n",
      "Iteration: 2196 lambda_n: 0.8988336775160791 Loss: 0.11716808914018352\n",
      "Iteration: 2197 lambda_n: 0.9883917648302075 Loss: 0.11713631673102991\n",
      "Iteration: 2198 lambda_n: 0.9034963960402884 Loss: 0.11710139222033182\n",
      "Iteration: 2199 lambda_n: 0.9407639079067465 Loss: 0.11706948066410618\n",
      "Iteration: 2200 lambda_n: 0.8921274389339442 Loss: 0.11703626571747999\n",
      "Iteration: 2201 lambda_n: 0.9806204887470253 Loss: 0.11700478037806065\n",
      "Iteration: 2202 lambda_n: 0.8960835392357263 Loss: 0.11697018526184001\n",
      "Iteration: 2203 lambda_n: 0.9658815661227249 Loss: 0.11693858542686086\n",
      "Iteration: 2204 lambda_n: 0.885140893849152 Loss: 0.1169045373343334\n",
      "Iteration: 2205 lambda_n: 0.987413598680386 Loss: 0.11687334795621017\n",
      "Iteration: 2206 lambda_n: 0.9240416041097621 Loss: 0.11683856815120792\n",
      "Iteration: 2207 lambda_n: 0.9083089024245763 Loss: 0.11680603390764488\n",
      "Iteration: 2208 lambda_n: 0.950268505582885 Loss: 0.11677406601796145\n",
      "Iteration: 2209 lambda_n: 1.011863826599373 Loss: 0.11674063428627099\n",
      "Iteration: 2210 lambda_n: 1.0030791875122267 Loss: 0.11670504998026268\n",
      "Iteration: 2211 lambda_n: 1.0228070351239107 Loss: 0.11666978959811537\n",
      "Iteration: 2212 lambda_n: 0.9531327689406353 Loss: 0.11663385096022075\n",
      "Iteration: 2213 lambda_n: 0.9697563605277778 Loss: 0.11660037467442937\n",
      "Iteration: 2214 lambda_n: 1.022913494464343 Loss: 0.11656632819647374\n",
      "Iteration: 2215 lambda_n: 0.9635604668033542 Loss: 0.11653043022186207\n",
      "Iteration: 2216 lambda_n: 0.8972417761700718 Loss: 0.11649662948996188\n",
      "Iteration: 2217 lambda_n: 0.8814267854326243 Loss: 0.1164651676631497\n",
      "Iteration: 2218 lambda_n: 1.0239735163671722 Loss: 0.11643427193988864\n",
      "Iteration: 2219 lambda_n: 0.9012095713599381 Loss: 0.11639839332048102\n",
      "Iteration: 2220 lambda_n: 0.8916267370057102 Loss: 0.11636682934617633\n",
      "Iteration: 2221 lambda_n: 0.9529149039413237 Loss: 0.11633561270874607\n",
      "Iteration: 2222 lambda_n: 1.0087298266457885 Loss: 0.11630226288226692\n",
      "Iteration: 2223 lambda_n: 0.8825677875384925 Loss: 0.11626697383022895\n",
      "Iteration: 2224 lambda_n: 0.972874040656903 Loss: 0.11623611100944246\n",
      "Iteration: 2225 lambda_n: 1.0229898338883985 Loss: 0.11620210297211328\n",
      "Iteration: 2226 lambda_n: 0.9863682989744805 Loss: 0.11616635766827074\n",
      "Iteration: 2227 lambda_n: 0.880932122104006 Loss: 0.11613190651137624\n",
      "Iteration: 2228 lambda_n: 0.9580279932492329 Loss: 0.1161011502550369\n",
      "Iteration: 2229 lambda_n: 0.8783270789190131 Loss: 0.11606771474456995\n",
      "Iteration: 2230 lambda_n: 0.9367130601935495 Loss: 0.11603707276665211\n",
      "Iteration: 2231 lambda_n: 0.9915623526165815 Loss: 0.11600440591790226\n",
      "Iteration: 2232 lambda_n: 0.9979613124297866 Loss: 0.11596983979970996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2233 lambda_n: 1.0258728737932241 Loss: 0.11593506486800682\n",
      "Iteration: 2234 lambda_n: 1.0018816319806378 Loss: 0.11589933212645406\n",
      "Iteration: 2235 lambda_n: 0.9715262770974288 Loss: 0.11586444971615704\n",
      "Iteration: 2236 lambda_n: 0.9837859295729221 Loss: 0.11583063804868865\n",
      "Iteration: 2237 lambda_n: 0.9504180236822752 Loss: 0.11579641343015902\n",
      "Iteration: 2238 lambda_n: 0.9291537038187638 Loss: 0.11576336291143259\n",
      "Iteration: 2239 lambda_n: 1.0057318962307549 Loss: 0.1157310644051661\n",
      "Iteration: 2240 lambda_n: 0.9719507496542854 Loss: 0.11569611750402223\n",
      "Iteration: 2241 lambda_n: 0.9869814536290688 Loss: 0.11566235825022535\n",
      "Iteration: 2242 lambda_n: 0.9494207126042234 Loss: 0.11562809062081152\n",
      "Iteration: 2243 lambda_n: 0.9553796879824339 Loss: 0.11559514029836072\n",
      "Iteration: 2244 lambda_n: 0.9455842441266703 Loss: 0.11556199605758845\n",
      "Iteration: 2245 lambda_n: 1.024906356736032 Loss: 0.1155292044236738\n",
      "Iteration: 2246 lambda_n: 1.0048309823818151 Loss: 0.11549367596664215\n",
      "Iteration: 2247 lambda_n: 0.9602944899598289 Loss: 0.11545885793248178\n",
      "Iteration: 2248 lambda_n: 0.9086808845746247 Loss: 0.11542559662277437\n",
      "Iteration: 2249 lambda_n: 1.0047174625825739 Loss: 0.1153941352065556\n",
      "Iteration: 2250 lambda_n: 0.8788378034153767 Loss: 0.1153593618524307\n",
      "Iteration: 2251 lambda_n: 0.897711028752619 Loss: 0.1153289573095355\n",
      "Iteration: 2252 lambda_n: 0.9858950449056296 Loss: 0.11529791096783576\n",
      "Iteration: 2253 lambda_n: 0.9385358991992885 Loss: 0.11526382756996158\n",
      "Iteration: 2254 lambda_n: 1.0195393748926094 Loss: 0.11523139428044635\n",
      "Iteration: 2255 lambda_n: 0.9979346033463357 Loss: 0.11519617538445107\n",
      "Iteration: 2256 lambda_n: 0.902692623988349 Loss: 0.11516171698504132\n",
      "Iteration: 2257 lambda_n: 0.9705472924704753 Loss: 0.11513055960939966\n",
      "Iteration: 2258 lambda_n: 0.9114376215479553 Loss: 0.11509707259252458\n",
      "Iteration: 2259 lambda_n: 0.88615679726642 Loss: 0.11506563723601378\n",
      "Iteration: 2260 lambda_n: 0.9298621751104884 Loss: 0.11503508500431964\n",
      "Iteration: 2261 lambda_n: 0.9707853417809369 Loss: 0.11500303752659927\n",
      "Iteration: 2262 lambda_n: 0.8814941145757605 Loss: 0.1149695923164989\n",
      "Iteration: 2263 lambda_n: 0.9803244382824635 Loss: 0.11493923500108191\n",
      "Iteration: 2264 lambda_n: 0.9795689874133928 Loss: 0.11490548640108798\n",
      "Iteration: 2265 lambda_n: 1.0023932289917425 Loss: 0.11487177712296544\n",
      "Iteration: 2266 lambda_n: 1.003721989928401 Loss: 0.11483729607813319\n",
      "Iteration: 2267 lambda_n: 0.9724985812756801 Loss: 0.1148027832514637\n",
      "Iteration: 2268 lambda_n: 0.9371325736721533 Loss: 0.1147693574390241\n",
      "Iteration: 2269 lambda_n: 0.9767183874376599 Loss: 0.11473715967881498\n",
      "Iteration: 2270 lambda_n: 0.9191327365234481 Loss: 0.11470361457704963\n",
      "Iteration: 2271 lambda_n: 0.9359350678529088 Loss: 0.11467205944966125\n",
      "Iteration: 2272 lambda_n: 0.9537600675956311 Loss: 0.11463993936015722\n",
      "Iteration: 2273 lambda_n: 0.9317278180041368 Loss: 0.11460721986272526\n",
      "Iteration: 2274 lambda_n: 0.9720147492154014 Loss: 0.11457526834406516\n",
      "Iteration: 2275 lambda_n: 1.0167794512740418 Loss: 0.11454194781700315\n",
      "Iteration: 2276 lambda_n: 0.9833644833254682 Loss: 0.11450710644342024\n",
      "Iteration: 2277 lambda_n: 0.931588443285762 Loss: 0.11447342367447019\n",
      "Iteration: 2278 lambda_n: 1.0113250056802667 Loss: 0.11444152675625377\n",
      "Iteration: 2279 lambda_n: 0.9365960140297659 Loss: 0.1144069128260037\n",
      "Iteration: 2280 lambda_n: 0.9581213651947528 Loss: 0.11437486931682202\n",
      "Iteration: 2281 lambda_n: 0.9764647865434639 Loss: 0.11434210166326916\n",
      "Iteration: 2282 lambda_n: 0.9708051469739679 Loss: 0.11430871946354827\n",
      "Iteration: 2283 lambda_n: 1.0160481513747777 Loss: 0.11427554363247416\n",
      "Iteration: 2284 lambda_n: 0.9243421756679658 Loss: 0.11424083523318819\n",
      "Iteration: 2285 lambda_n: 0.8936407871648555 Loss: 0.11420927203224024\n",
      "Iteration: 2286 lambda_n: 0.9635132122998962 Loss: 0.11417876830647752\n",
      "Iteration: 2287 lambda_n: 0.8900078359485103 Loss: 0.11414589140854223\n",
      "Iteration: 2288 lambda_n: 0.926070170091619 Loss: 0.11411553407231063\n",
      "Iteration: 2289 lambda_n: 0.9995726352257283 Loss: 0.11408395793127\n",
      "Iteration: 2290 lambda_n: 0.9313694976294893 Loss: 0.11404988830944418\n",
      "Iteration: 2291 lambda_n: 1.0127839878370652 Loss: 0.11401815571924538\n",
      "Iteration: 2292 lambda_n: 1.0024732427783272 Loss: 0.11398366223098227\n",
      "Iteration: 2293 lambda_n: 0.9990677422769787 Loss: 0.11394953355837059\n",
      "Iteration: 2294 lambda_n: 0.9344551669969774 Loss: 0.1139155342969431\n",
      "Iteration: 2295 lambda_n: 0.9794388735441605 Loss: 0.11388374624267279\n",
      "Iteration: 2296 lambda_n: 1.0174967720503985 Loss: 0.11385044037037248\n",
      "Iteration: 2297 lambda_n: 0.909915901505658 Loss: 0.11381585382775301\n",
      "Iteration: 2298 lambda_n: 0.8806996215511362 Loss: 0.11378493627879965\n",
      "Iteration: 2299 lambda_n: 1.0030095281955071 Loss: 0.11375502211355298\n",
      "Iteration: 2300 lambda_n: 0.918158646189143 Loss: 0.11372096570926858\n",
      "Iteration: 2301 lambda_n: 0.9437658431300285 Loss: 0.11368980243581539\n",
      "Iteration: 2302 lambda_n: 0.9315320554130501 Loss: 0.1136577816743519\n",
      "Iteration: 2303 lambda_n: 0.915568516360125 Loss: 0.1136261876902266\n",
      "Iteration: 2304 lambda_n: 0.9276157564428713 Loss: 0.11359514645193317\n",
      "Iteration: 2305 lambda_n: 0.9446009821763257 Loss: 0.11356370812035316\n",
      "Iteration: 2306 lambda_n: 0.961083268895769 Loss: 0.11353170584456838\n",
      "Iteration: 2307 lambda_n: 0.9955264117306141 Loss: 0.11349915728326927\n",
      "Iteration: 2308 lambda_n: 0.9111649420970356 Loss: 0.11346545507241433\n",
      "Iteration: 2309 lambda_n: 0.9544477513098986 Loss: 0.11343462061549484\n",
      "Iteration: 2310 lambda_n: 0.8809442523474063 Loss: 0.11340233310961471\n",
      "Iteration: 2311 lambda_n: 0.9860155971039882 Loss: 0.11337254303288277\n",
      "Iteration: 2312 lambda_n: 0.9003102636689598 Loss: 0.11333921167539684\n",
      "Iteration: 2313 lambda_n: 0.899345133932989 Loss: 0.1133087890312488\n",
      "Iteration: 2314 lambda_n: 0.9011182590124928 Loss: 0.11327840969565292\n",
      "Iteration: 2315 lambda_n: 0.9728901645772526 Loss: 0.11324798117074916\n",
      "Iteration: 2316 lambda_n: 0.9294700734065586 Loss: 0.11321514085907698\n",
      "Iteration: 2317 lambda_n: 0.9908501483775327 Loss: 0.11318377801249592\n",
      "Iteration: 2318 lambda_n: 0.9233745040131007 Loss: 0.1131503563440448\n",
      "Iteration: 2319 lambda_n: 0.9605655781600041 Loss: 0.11311922250553932\n",
      "Iteration: 2320 lambda_n: 0.9199863280037709 Loss: 0.11308684644496601\n",
      "Iteration: 2321 lambda_n: 0.891458419698951 Loss: 0.11305584961511068\n",
      "Iteration: 2322 lambda_n: 1.0216235539327019 Loss: 0.11302582465684417\n",
      "Iteration: 2323 lambda_n: 0.9562359986955801 Loss: 0.11299142797789069\n",
      "Iteration: 2324 lambda_n: 0.9095078370624401 Loss: 0.11295924542581737\n",
      "Iteration: 2325 lambda_n: 0.9503039035085267 Loss: 0.11292864679125653\n",
      "Iteration: 2326 lambda_n: 0.9444967781940549 Loss: 0.11289668707074191\n",
      "Iteration: 2327 lambda_n: 1.0266865414680137 Loss: 0.11286493436400663\n",
      "Iteration: 2328 lambda_n: 0.8885949991596084 Loss: 0.11283043145878412\n",
      "Iteration: 2329 lambda_n: 0.8892653662304649 Loss: 0.11280058081017905\n",
      "Iteration: 2330 lambda_n: 0.8981232233313132 Loss: 0.11277071794379585\n",
      "Iteration: 2331 lambda_n: 0.8877429260885947 Loss: 0.1127405680430782\n",
      "Iteration: 2332 lambda_n: 0.8765127647404969 Loss: 0.11271077695717756\n",
      "Iteration: 2333 lambda_n: 0.906393821557648 Loss: 0.11268137282548825\n",
      "Iteration: 2334 lambda_n: 0.8966962772673794 Loss: 0.1126509766851679\n",
      "Iteration: 2335 lambda_n: 1.0117419728319033 Loss: 0.11262091628172262\n",
      "Iteration: 2336 lambda_n: 0.8831477189280463 Loss: 0.11258701125550843\n",
      "Iteration: 2337 lambda_n: 0.9844171454727204 Loss: 0.1125574268694274\n",
      "Iteration: 2338 lambda_n: 0.9460285831191906 Loss: 0.11252446163299648\n",
      "Iteration: 2339 lambda_n: 0.9773482258957235 Loss: 0.11249279386296608\n",
      "Iteration: 2340 lambda_n: 1.005353609620745 Loss: 0.11246008973201466\n",
      "Iteration: 2341 lambda_n: 0.9771463959572076 Loss: 0.11242646126049752\n",
      "Iteration: 2342 lambda_n: 0.8784446236113419 Loss: 0.11239378890481116\n",
      "Iteration: 2343 lambda_n: 1.0199674882029897 Loss: 0.1123644276082363\n",
      "Iteration: 2344 lambda_n: 0.9362633021982264 Loss: 0.1123303479971091\n",
      "Iteration: 2345 lambda_n: 1.028577140430959 Loss: 0.11229907721608129\n",
      "Iteration: 2346 lambda_n: 0.9435272720740325 Loss: 0.11226473586443898\n",
      "Iteration: 2347 lambda_n: 0.9058977707010237 Loss: 0.11223324633537958\n",
      "Iteration: 2348 lambda_n: 0.9387808819682876 Loss: 0.11220302354342175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2349 lambda_n: 1.0038288871018577 Loss: 0.1121717147039321\n",
      "Iteration: 2350 lambda_n: 0.961761706116939 Loss: 0.1121382487583589\n",
      "Iteration: 2351 lambda_n: 0.9201634399372427 Loss: 0.11210619751576469\n",
      "Iteration: 2352 lambda_n: 0.9489569009631205 Loss: 0.11207554377898013\n",
      "Iteration: 2353 lambda_n: 0.9100697029080139 Loss: 0.11204394208872771\n",
      "Iteration: 2354 lambda_n: 1.0037779729925866 Loss: 0.112013646338895\n",
      "Iteration: 2355 lambda_n: 0.9329565553977995 Loss: 0.11198024303665062\n",
      "Iteration: 2356 lambda_n: 0.9943471415791935 Loss: 0.1119492082614294\n",
      "Iteration: 2357 lambda_n: 0.8921755461832493 Loss: 0.11191614334299745\n",
      "Iteration: 2358 lambda_n: 0.9996150599690065 Loss: 0.11188648696571746\n",
      "Iteration: 2359 lambda_n: 1.0197287641706485 Loss: 0.11185327091742894\n",
      "Iteration: 2360 lambda_n: 1.0148164541411007 Loss: 0.11181939954751806\n",
      "Iteration: 2361 lambda_n: 1.007825881876174 Loss: 0.11178570449041088\n",
      "Iteration: 2362 lambda_n: 0.9803483523656559 Loss: 0.11175225451694228\n",
      "Iteration: 2363 lambda_n: 0.8950730362263206 Loss: 0.11171972899395789\n",
      "Iteration: 2364 lambda_n: 1.021912720385455 Loss: 0.11169004360170308\n",
      "Iteration: 2365 lambda_n: 0.9108828986646264 Loss: 0.11165616350300901\n",
      "Iteration: 2366 lambda_n: 0.9892352290695541 Loss: 0.11162597593978181\n",
      "Iteration: 2367 lambda_n: 0.9122379116219947 Loss: 0.11159320333497552\n",
      "Iteration: 2368 lambda_n: 1.0040001177093534 Loss: 0.11156299279966131\n",
      "Iteration: 2369 lambda_n: 0.9822453555207755 Loss: 0.11152975522566391\n",
      "Iteration: 2370 lambda_n: 0.9587763959499915 Loss: 0.11149725023798684\n",
      "Iteration: 2371 lambda_n: 0.9898007053620886 Loss: 0.1114655337157019\n",
      "Iteration: 2372 lambda_n: 0.8918990345351532 Loss: 0.11143280295936027\n",
      "Iteration: 2373 lambda_n: 0.9246721545819548 Loss: 0.11140332048521363\n",
      "Iteration: 2374 lambda_n: 0.8900212593570697 Loss: 0.11137276514051729\n",
      "Iteration: 2375 lambda_n: 0.9149647726560722 Loss: 0.11134336508201177\n",
      "Iteration: 2376 lambda_n: 1.008193716448205 Loss: 0.11131315137045385\n",
      "Iteration: 2377 lambda_n: 0.9765348187466093 Loss: 0.11127987094477228\n",
      "Iteration: 2378 lambda_n: 0.9090419822728094 Loss: 0.11124764784848752\n",
      "Iteration: 2379 lambda_n: 0.9684111769506171 Loss: 0.11121766279653346\n",
      "Iteration: 2380 lambda_n: 1.0195120400457804 Loss: 0.1111857306370899\n",
      "Iteration: 2381 lambda_n: 1.0034779306474033 Loss: 0.11115212601191926\n",
      "Iteration: 2382 lambda_n: 1.0293038261299474 Loss: 0.11111906265509496\n",
      "Iteration: 2383 lambda_n: 0.8938566189101131 Loss: 0.11108516136565938\n",
      "Iteration: 2384 lambda_n: 0.9635193394178452 Loss: 0.11105573232900873\n",
      "Iteration: 2385 lambda_n: 0.9927244777747389 Loss: 0.11102402069446614\n",
      "Iteration: 2386 lambda_n: 1.009084381294557 Loss: 0.11099135987598982\n",
      "Iteration: 2387 lambda_n: 1.028639099574277 Loss: 0.11095817335694722\n",
      "Iteration: 2388 lambda_n: 0.9551133046687212 Loss: 0.11092435672351447\n",
      "Iteration: 2389 lambda_n: 0.8887037148127255 Loss: 0.11089296928965482\n",
      "Iteration: 2390 lambda_n: 1.0085590535583693 Loss: 0.11086377463309585\n",
      "Iteration: 2391 lambda_n: 0.9641790485613777 Loss: 0.11083065413051539\n",
      "Iteration: 2392 lambda_n: 1.0065863824580388 Loss: 0.11079900300221589\n",
      "Iteration: 2393 lambda_n: 0.9201046139387568 Loss: 0.11076597194588791\n",
      "Iteration: 2394 lambda_n: 0.8970949742446007 Loss: 0.11073579004315998\n",
      "Iteration: 2395 lambda_n: 0.9614340864826741 Loss: 0.11070637309439987\n",
      "Iteration: 2396 lambda_n: 0.9392122676263386 Loss: 0.11067485725039157\n",
      "Iteration: 2397 lambda_n: 0.997083511350834 Loss: 0.11064408096101695\n",
      "Iteration: 2398 lambda_n: 0.977157724743648 Loss: 0.11061142009524015\n",
      "Iteration: 2399 lambda_n: 0.8839082027741146 Loss: 0.11057942392003574\n",
      "Iteration: 2400 lambda_n: 1.0184659501674365 Loss: 0.11055049155241205\n",
      "Iteration: 2401 lambda_n: 0.9543146992707737 Loss: 0.11051716631792187\n",
      "Iteration: 2402 lambda_n: 0.8774626874637831 Loss: 0.11048595199450421\n",
      "Iteration: 2403 lambda_n: 1.0162854881328187 Loss: 0.11045726152564046\n",
      "Iteration: 2404 lambda_n: 1.0239621487226007 Loss: 0.11042404336179362\n",
      "Iteration: 2405 lambda_n: 0.9381803366949589 Loss: 0.11039058712174882\n",
      "Iteration: 2406 lambda_n: 1.016334025853158 Loss: 0.1103599452474874\n",
      "Iteration: 2407 lambda_n: 1.0199965772783148 Loss: 0.11032676276006466\n",
      "Iteration: 2408 lambda_n: 0.9626910589315173 Loss: 0.11029347344763024\n",
      "Iteration: 2409 lambda_n: 0.9319443664033802 Loss: 0.11026206629813846\n",
      "Iteration: 2410 lambda_n: 0.9652580495278814 Loss: 0.1102316731679844\n",
      "Iteration: 2411 lambda_n: 0.9630802670588627 Loss: 0.11020020472134706\n",
      "Iteration: 2412 lambda_n: 0.9379416205115098 Loss: 0.11016881866307733\n",
      "Iteration: 2413 lambda_n: 0.9472460611470855 Loss: 0.1101382628518015\n",
      "Iteration: 2414 lambda_n: 0.927690703272927 Loss: 0.11010741482821797\n",
      "Iteration: 2415 lambda_n: 0.9093863904062927 Loss: 0.11007721434309384\n",
      "Iteration: 2416 lambda_n: 0.9789344809726132 Loss: 0.11004762001178219\n",
      "Iteration: 2417 lambda_n: 0.8894810296754821 Loss: 0.11001577344164826\n",
      "Iteration: 2418 lambda_n: 0.9993557798588995 Loss: 0.10998684735999269\n",
      "Iteration: 2419 lambda_n: 0.9080440170110997 Loss: 0.10995435930173958\n",
      "Iteration: 2420 lambda_n: 1.0013163123512006 Loss: 0.10992485052261222\n",
      "Iteration: 2421 lambda_n: 0.8918303153814461 Loss: 0.10989232202198078\n",
      "Iteration: 2422 lambda_n: 0.9130671330470723 Loss: 0.10986336084641032\n",
      "Iteration: 2423 lambda_n: 0.8783629480979811 Loss: 0.1098337199983992\n",
      "Iteration: 2424 lambda_n: 1.009109349883051 Loss: 0.10980521541789119\n",
      "Iteration: 2425 lambda_n: 0.9497303131237018 Loss: 0.10977247902614228\n",
      "Iteration: 2426 lambda_n: 0.9715576203849597 Loss: 0.10974168042242687\n",
      "Iteration: 2427 lambda_n: 1.0227232311154042 Loss: 0.10971018525207984\n",
      "Iteration: 2428 lambda_n: 1.0156468199869035 Loss: 0.1096770436474883\n",
      "Iteration: 2429 lambda_n: 0.9550928692466116 Loss: 0.10964414393175931\n",
      "Iteration: 2430 lambda_n: 0.9411579760392247 Loss: 0.10961321731901039\n",
      "Iteration: 2431 lambda_n: 0.9700442275102402 Loss: 0.10958275277662777\n",
      "Iteration: 2432 lambda_n: 0.9400180192830931 Loss: 0.10955136434125046\n",
      "Iteration: 2433 lambda_n: 1.006152192742049 Loss: 0.10952095843332822\n",
      "Iteration: 2434 lambda_n: 0.8830947157147659 Loss: 0.10948842497163251\n",
      "Iteration: 2435 lambda_n: 0.9755286700487055 Loss: 0.10945988093223413\n",
      "Iteration: 2436 lambda_n: 0.9934417703058244 Loss: 0.10942835983928104\n",
      "Iteration: 2437 lambda_n: 0.9016012600523169 Loss: 0.10939627168769966\n",
      "Iteration: 2438 lambda_n: 0.9532099130158547 Loss: 0.1093671605414328\n",
      "Iteration: 2439 lambda_n: 0.9897356688280856 Loss: 0.10933639354188276\n",
      "Iteration: 2440 lambda_n: 0.9906469594818824 Loss: 0.10930445905975539\n",
      "Iteration: 2441 lambda_n: 0.9298841010690314 Loss: 0.10927250697865148\n",
      "Iteration: 2442 lambda_n: 0.9509723506648053 Loss: 0.10924252564338974\n",
      "Iteration: 2443 lambda_n: 0.883785628068836 Loss: 0.10921187506983102\n",
      "Iteration: 2444 lambda_n: 0.9916252132575961 Loss: 0.10918339990007828\n",
      "Iteration: 2445 lambda_n: 1.0228600015009643 Loss: 0.1091514610240736\n",
      "Iteration: 2446 lambda_n: 0.9279233887089249 Loss: 0.1091185283843431\n",
      "Iteration: 2447 lambda_n: 0.8977161722367466 Loss: 0.10908866350828665\n",
      "Iteration: 2448 lambda_n: 0.9047012375791581 Loss: 0.10905978074504598\n",
      "Iteration: 2449 lambda_n: 0.9461248509226785 Loss: 0.10903068299399775\n",
      "Iteration: 2450 lambda_n: 0.875633833771294 Loss: 0.10900026330431212\n",
      "Iteration: 2451 lambda_n: 1.0087177381677461 Loss: 0.1089721197649078\n",
      "Iteration: 2452 lambda_n: 0.9592837455230941 Loss: 0.10893970976210683\n",
      "Iteration: 2453 lambda_n: 0.9571094456768662 Loss: 0.10890889949455668\n",
      "Iteration: 2454 lambda_n: 1.0140399274632919 Loss: 0.10887817002118234\n",
      "Iteration: 2455 lambda_n: 0.9845460792560742 Loss: 0.10884562446109346\n",
      "Iteration: 2456 lambda_n: 1.0197815054477983 Loss: 0.1088140373298561\n",
      "Iteration: 2457 lambda_n: 0.9190443626677418 Loss: 0.10878133181796576\n",
      "Iteration: 2458 lambda_n: 0.9824669446838472 Loss: 0.10875186795300616\n",
      "Iteration: 2459 lambda_n: 0.9659369022092936 Loss: 0.10872038174226037\n",
      "Iteration: 2460 lambda_n: 0.923559214801039 Loss: 0.10868943653547153\n",
      "Iteration: 2461 lambda_n: 0.9422633001608962 Loss: 0.10865985945610769\n",
      "Iteration: 2462 lambda_n: 0.9277915484127369 Loss: 0.1086296937723991\n",
      "Iteration: 2463 lambda_n: 0.9655819645511546 Loss: 0.10860000173504976\n",
      "Iteration: 2464 lambda_n: 1.0261373468375072 Loss: 0.10856911103774067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2465 lambda_n: 0.8895270674733571 Loss: 0.10853629500482412\n",
      "Iteration: 2466 lambda_n: 0.9928131257059314 Loss: 0.10850785826000665\n",
      "Iteration: 2467 lambda_n: 0.8993536733110603 Loss: 0.10847613039391403\n",
      "Iteration: 2468 lambda_n: 0.9676928637504639 Loss: 0.10844739959366467\n",
      "Iteration: 2469 lambda_n: 0.999933847608943 Loss: 0.10841649612305664\n",
      "Iteration: 2470 lambda_n: 1.0079135437435847 Loss: 0.10838457457409086\n",
      "Iteration: 2471 lambda_n: 0.9916197755394878 Loss: 0.1083524102263616\n",
      "Iteration: 2472 lambda_n: 0.9422682449504128 Loss: 0.10832077760776361\n",
      "Iteration: 2473 lambda_n: 0.9145847391779957 Loss: 0.10829073020117143\n",
      "Iteration: 2474 lambda_n: 0.9065879892546631 Loss: 0.10826157567606005\n",
      "Iteration: 2475 lambda_n: 0.9250876162981105 Loss: 0.10823268582756912\n",
      "Iteration: 2476 lambda_n: 0.9140091701127463 Loss: 0.10820321639922667\n",
      "Iteration: 2477 lambda_n: 0.8926609975444983 Loss: 0.10817410981970862\n",
      "Iteration: 2478 lambda_n: 0.9636927675021375 Loss: 0.10814569262691534\n",
      "Iteration: 2479 lambda_n: 0.9504756031585537 Loss: 0.10811502451904559\n",
      "Iteration: 2480 lambda_n: 0.9505594994361906 Loss: 0.10808478776951334\n",
      "Iteration: 2481 lambda_n: 1.0020356361524874 Loss: 0.10805455897664346\n",
      "Iteration: 2482 lambda_n: 0.969395839392686 Loss: 0.10802270453476753\n",
      "Iteration: 2483 lambda_n: 0.9337601715235849 Loss: 0.107991899021826\n",
      "Iteration: 2484 lambda_n: 0.9307316507531384 Loss: 0.10796223647122573\n",
      "Iteration: 2485 lambda_n: 0.9793966027416636 Loss: 0.10793268031659049\n",
      "Iteration: 2486 lambda_n: 0.9765130649137701 Loss: 0.1079015895959079\n",
      "Iteration: 2487 lambda_n: 1.0111099396857228 Loss: 0.10787060161460456\n",
      "Iteration: 2488 lambda_n: 0.9380000872106804 Loss: 0.1078385274294127\n",
      "Iteration: 2489 lambda_n: 0.9692979347110852 Loss: 0.1078087833213428\n",
      "Iteration: 2490 lambda_n: 0.8931641737107865 Loss: 0.10777805748212511\n",
      "Iteration: 2491 lambda_n: 0.9922199215142693 Loss: 0.10774975493848264\n",
      "Iteration: 2492 lambda_n: 0.9416653142105713 Loss: 0.10771832416713449\n",
      "Iteration: 2493 lambda_n: 0.9220958799370206 Loss: 0.1076885056021016\n",
      "Iteration: 2494 lambda_n: 0.9767118495610082 Loss: 0.1076593168044206\n",
      "Iteration: 2495 lambda_n: 0.9725829959875524 Loss: 0.10762840981326645\n",
      "Iteration: 2496 lambda_n: 0.9534694322840856 Loss: 0.10759764454364931\n",
      "Iteration: 2497 lambda_n: 0.993519512872415 Loss: 0.10756749464188617\n",
      "Iteration: 2498 lambda_n: 0.9927542946699005 Loss: 0.10753608945972028\n",
      "Iteration: 2499 lambda_n: 0.9812250262325354 Loss: 0.10750471994974935\n",
      "Iteration: 2500 lambda_n: 0.9703412222444632 Loss: 0.10747372605103873\n",
      "Iteration: 2501 lambda_n: 0.8885230246928318 Loss: 0.10744308700214351\n",
      "Iteration: 2502 lambda_n: 0.9496230689102299 Loss: 0.1074150411888267\n",
      "Iteration: 2503 lambda_n: 0.9874735476605877 Loss: 0.10738507676530974\n",
      "Iteration: 2504 lambda_n: 0.998300749784844 Loss: 0.10735392902745504\n",
      "Iteration: 2505 lambda_n: 0.9300490563198641 Loss: 0.10732245122372408\n",
      "Iteration: 2506 lambda_n: 1.0125733923276412 Loss: 0.10729313607668073\n",
      "Iteration: 2507 lambda_n: 0.9408765232917192 Loss: 0.1072612309260776\n",
      "Iteration: 2508 lambda_n: 0.9348109845408905 Loss: 0.10723159571451495\n",
      "Iteration: 2509 lambda_n: 0.8944272136240109 Loss: 0.10720216172305295\n",
      "Iteration: 2510 lambda_n: 0.9108838089993992 Loss: 0.10717400885220661\n",
      "Iteration: 2511 lambda_n: 0.8918592852193983 Loss: 0.10714534746659989\n",
      "Iteration: 2512 lambda_n: 0.992927745577608 Loss: 0.10711729404237973\n",
      "Iteration: 2513 lambda_n: 0.984513025278078 Loss: 0.10708607204037725\n",
      "Iteration: 2514 lambda_n: 0.9662388922928808 Loss: 0.10705512590436643\n",
      "Iteration: 2515 lambda_n: 0.9419067238864608 Loss: 0.10702476511143592\n",
      "Iteration: 2516 lambda_n: 1.0286060825765173 Loss: 0.10699517929851748\n",
      "Iteration: 2517 lambda_n: 0.998165762121282 Loss: 0.10696288166095698\n",
      "Iteration: 2518 lambda_n: 0.9715316300405733 Loss: 0.10693155158161619\n",
      "Iteration: 2519 lambda_n: 1.0136967257906293 Loss: 0.10690106858776932\n",
      "Iteration: 2520 lambda_n: 0.905514805392184 Loss: 0.1068692740761716\n",
      "Iteration: 2521 lambda_n: 0.9908600936583594 Loss: 0.10684088296089363\n",
      "Iteration: 2522 lambda_n: 0.916307265839897 Loss: 0.10680982653620193\n",
      "Iteration: 2523 lambda_n: 1.0176691224989942 Loss: 0.10678111705095696\n",
      "Iteration: 2524 lambda_n: 1.0154437123751165 Loss: 0.10674924274580297\n",
      "Iteration: 2525 lambda_n: 0.9114209963286947 Loss: 0.10671745000717728\n",
      "Iteration: 2526 lambda_n: 0.9149254495975335 Loss: 0.10668892448658343\n",
      "Iteration: 2527 lambda_n: 0.941802387170467 Loss: 0.10666029886236085\n",
      "Iteration: 2528 lambda_n: 0.9456316906965213 Loss: 0.1066308422860101\n",
      "Iteration: 2529 lambda_n: 0.9826754422887491 Loss: 0.10660127616164608\n",
      "Iteration: 2530 lambda_n: 0.8958559751655395 Loss: 0.10657056258104697\n",
      "Iteration: 2531 lambda_n: 0.8881782843640472 Loss: 0.10654257240334074\n",
      "Iteration: 2532 lambda_n: 1.0184808030091397 Loss: 0.10651483119588272\n",
      "Iteration: 2533 lambda_n: 0.9440107182992014 Loss: 0.10648303087898982\n",
      "Iteration: 2534 lambda_n: 0.8988399465931193 Loss: 0.10645356655289422\n",
      "Iteration: 2535 lambda_n: 1.013461920915717 Loss: 0.10642552167038234\n",
      "Iteration: 2536 lambda_n: 0.9676167836409414 Loss: 0.10639391119079941\n",
      "Iteration: 2537 lambda_n: 0.9501873375163438 Loss: 0.10636374171674946\n",
      "Iteration: 2538 lambda_n: 0.9740833019127952 Loss: 0.10633412612301966\n",
      "Iteration: 2539 lambda_n: 0.9391887655182075 Loss: 0.10630377636193428\n",
      "Iteration: 2540 lambda_n: 0.9647578095834822 Loss: 0.10627452415388544\n",
      "Iteration: 2541 lambda_n: 0.9748808497756293 Loss: 0.1062444859635984\n",
      "Iteration: 2542 lambda_n: 0.9621208779959702 Loss: 0.10621414332930468\n",
      "Iteration: 2543 lambda_n: 0.9029472056840295 Loss: 0.10618420848460883\n",
      "Iteration: 2544 lambda_n: 0.9752339012045467 Loss: 0.10615612446314801\n",
      "Iteration: 2545 lambda_n: 1.0160488828724246 Loss: 0.1061258023570359\n",
      "Iteration: 2546 lambda_n: 0.9672218839475649 Loss: 0.10609422261129252\n",
      "Iteration: 2547 lambda_n: 0.9304883931999962 Loss: 0.10606417148271084\n",
      "Iteration: 2548 lambda_n: 0.8852571225237741 Loss: 0.106035271767399\n",
      "Iteration: 2549 lambda_n: 0.9071515095194655 Loss: 0.10600778610757158\n",
      "Iteration: 2550 lambda_n: 1.0182662714967565 Loss: 0.10597962983501526\n",
      "Iteration: 2551 lambda_n: 0.9676226828268992 Loss: 0.10594803557504918\n",
      "Iteration: 2552 lambda_n: 0.8937808293762245 Loss: 0.10591802368478351\n",
      "Iteration: 2553 lambda_n: 0.8897659887311279 Loss: 0.1058903116870905\n",
      "Iteration: 2554 lambda_n: 0.9817162528407467 Loss: 0.1058627331658667\n",
      "Iteration: 2555 lambda_n: 0.995208060032898 Loss: 0.10583231477304796\n",
      "Iteration: 2556 lambda_n: 0.9957632932932505 Loss: 0.10580148943039965\n",
      "Iteration: 2557 lambda_n: 0.8948490408593364 Loss: 0.1057706580978092\n",
      "Iteration: 2558 lambda_n: 0.9501352352447728 Loss: 0.10574296113168495\n",
      "Iteration: 2559 lambda_n: 0.9782862753967069 Loss: 0.1057135627299738\n",
      "Iteration: 2560 lambda_n: 0.9278298312131301 Loss: 0.10568330387490345\n",
      "Iteration: 2561 lambda_n: 0.9813078448637435 Loss: 0.10565461576927074\n",
      "Iteration: 2562 lambda_n: 0.8880653917547531 Loss: 0.1056242845700431\n",
      "Iteration: 2563 lambda_n: 0.8895106193676647 Loss: 0.10559684498237722\n",
      "Iteration: 2564 lambda_n: 0.8890447689389797 Loss: 0.10556936964627053\n",
      "Iteration: 2565 lambda_n: 0.921107335965446 Loss: 0.10554191760678665\n",
      "Iteration: 2566 lambda_n: 0.9409300130288747 Loss: 0.10551348484223325\n",
      "Iteration: 2567 lambda_n: 0.9184975118473734 Loss: 0.10548444999992915\n",
      "Iteration: 2568 lambda_n: 0.9042511038777832 Loss: 0.10545611703343849\n",
      "Iteration: 2569 lambda_n: 0.923520954548447 Loss: 0.10542823282885695\n",
      "Iteration: 2570 lambda_n: 0.887218494052359 Loss: 0.1053997638419234\n",
      "Iteration: 2571 lambda_n: 0.9264853988606297 Loss: 0.10537242304689413\n",
      "Iteration: 2572 lambda_n: 0.8943951140010775 Loss: 0.10534388152764461\n",
      "Iteration: 2573 lambda_n: 0.9755296488937024 Loss: 0.1053163378066002\n",
      "Iteration: 2574 lambda_n: 0.8785426563086138 Loss: 0.10528630549267025\n",
      "Iteration: 2575 lambda_n: 0.9427029874904984 Loss: 0.1052592683494695\n",
      "Iteration: 2576 lambda_n: 1.0134495912689885 Loss: 0.10523026613243888\n",
      "Iteration: 2577 lambda_n: 0.9077024525103539 Loss: 0.10519909831266148\n",
      "Iteration: 2578 lambda_n: 0.9607347411462241 Loss: 0.10517119268303277\n",
      "Iteration: 2579 lambda_n: 0.9045384538925724 Loss: 0.10514166658412767\n",
      "Iteration: 2580 lambda_n: 0.8902707098691226 Loss: 0.10511387713152978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2581 lambda_n: 0.9929900766467659 Loss: 0.10508653498509872\n",
      "Iteration: 2582 lambda_n: 1.0246917317807827 Loss: 0.10505604828342346\n",
      "Iteration: 2583 lambda_n: 0.9002442074432093 Loss: 0.10502459974429612\n",
      "Iteration: 2584 lambda_n: 0.9677704600374543 Loss: 0.10499698056709754\n",
      "Iteration: 2585 lambda_n: 0.963776695002655 Loss: 0.10496729962407629\n",
      "Iteration: 2586 lambda_n: 0.9580931869477519 Loss: 0.10493775156367234\n",
      "Iteration: 2587 lambda_n: 0.9951936793765654 Loss: 0.10490838803447375\n",
      "Iteration: 2588 lambda_n: 1.0250953408108339 Loss: 0.10487789819058202\n",
      "Iteration: 2589 lambda_n: 1.0242126923010406 Loss: 0.1048465037033818\n",
      "Iteration: 2590 lambda_n: 0.880161513987174 Loss: 0.10481514793882243\n",
      "Iteration: 2591 lambda_n: 0.9341199418625795 Loss: 0.10478821189566076\n",
      "Iteration: 2592 lambda_n: 0.9026847740032661 Loss: 0.10475963382928513\n",
      "Iteration: 2593 lambda_n: 0.8754247741879466 Loss: 0.10473202677249571\n",
      "Iteration: 2594 lambda_n: 0.9002607044421553 Loss: 0.10470526213123896\n",
      "Iteration: 2595 lambda_n: 0.8784648899379466 Loss: 0.10467774699587211\n",
      "Iteration: 2596 lambda_n: 0.8760459334710707 Loss: 0.1046509067459088\n",
      "Iteration: 2597 lambda_n: 0.8922030985029807 Loss: 0.1046241489420016\n",
      "Iteration: 2598 lambda_n: 0.9797447127445231 Loss: 0.10459690635271536\n",
      "Iteration: 2599 lambda_n: 1.0275809862469762 Loss: 0.1045670007088896\n",
      "Iteration: 2600 lambda_n: 0.878511540637939 Loss: 0.10453564622179748\n",
      "Iteration: 2601 lambda_n: 1.0208473301362349 Loss: 0.10450884990186736\n",
      "Iteration: 2602 lambda_n: 0.9280442925082424 Loss: 0.10447772242049098\n",
      "Iteration: 2603 lambda_n: 0.9026363502603301 Loss: 0.10444943491204249\n",
      "Iteration: 2604 lambda_n: 0.9224343750586109 Loss: 0.10442193106251274\n",
      "Iteration: 2605 lambda_n: 0.8957247401277241 Loss: 0.10439383322081457\n",
      "Iteration: 2606 lambda_n: 0.9328903548737635 Loss: 0.10436655803663136\n",
      "Iteration: 2607 lambda_n: 0.9752996510770521 Loss: 0.10433816048230102\n",
      "Iteration: 2608 lambda_n: 1.021504904887933 Loss: 0.10430848214595487\n",
      "Iteration: 2609 lambda_n: 0.9561520513791301 Loss: 0.10427740892628236\n",
      "Iteration: 2610 lambda_n: 0.8957032249115102 Loss: 0.10424833427981221\n",
      "Iteration: 2611 lambda_n: 0.8772698018051777 Loss: 0.10422110705234296\n",
      "Iteration: 2612 lambda_n: 0.9468189575423125 Loss: 0.10419444877639521\n",
      "Iteration: 2613 lambda_n: 0.9456445971240862 Loss: 0.10416568640570331\n",
      "Iteration: 2614 lambda_n: 0.9449960829338271 Loss: 0.10413696957367288\n",
      "Iteration: 2615 lambda_n: 1.0224550080637842 Loss: 0.10410828227756858\n",
      "Iteration: 2616 lambda_n: 0.9099184599555061 Loss: 0.10407725442618294\n",
      "Iteration: 2617 lambda_n: 0.9411370544541212 Loss: 0.10404965159788948\n",
      "Iteration: 2618 lambda_n: 0.9971720573351559 Loss: 0.1040211112442461\n",
      "Iteration: 2619 lambda_n: 0.8828219197414257 Loss: 0.10399088208932103\n",
      "Iteration: 2620 lambda_n: 0.9084649698031415 Loss: 0.10396412882784409\n",
      "Iteration: 2621 lambda_n: 0.9779168204532237 Loss: 0.10393660735191124\n",
      "Iteration: 2622 lambda_n: 0.9635375176946959 Loss: 0.10390699182223467\n",
      "Iteration: 2623 lambda_n: 0.9013459188420824 Loss: 0.10387782206301635\n",
      "Iteration: 2624 lambda_n: 1.019290515377667 Loss: 0.10385054443434466\n",
      "Iteration: 2625 lambda_n: 1.0153075274508214 Loss: 0.10381970784876708\n",
      "Iteration: 2626 lambda_n: 0.9019114756103018 Loss: 0.10378900309443474\n",
      "Iteration: 2627 lambda_n: 0.9602353098492051 Loss: 0.10376173738078986\n",
      "Iteration: 2628 lambda_n: 1.0138756883578908 Loss: 0.10373271812629625\n",
      "Iteration: 2629 lambda_n: 0.947096319031854 Loss: 0.103702088624129\n",
      "Iteration: 2630 lambda_n: 0.9971631662292173 Loss: 0.10367348687221443\n",
      "Iteration: 2631 lambda_n: 0.8919788307916152 Loss: 0.10364338360682335\n",
      "Iteration: 2632 lambda_n: 0.9648329902331493 Loss: 0.1036164651937716\n",
      "Iteration: 2633 lambda_n: 1.0007069483193756 Loss: 0.10358735777553241\n",
      "Iteration: 2634 lambda_n: 0.9969765567574764 Loss: 0.10355717875157251\n",
      "Iteration: 2635 lambda_n: 0.9257169023892498 Loss: 0.10352712311141703\n",
      "Iteration: 2636 lambda_n: 0.9596650058302679 Loss: 0.1034992256026368\n",
      "Iteration: 2637 lambda_n: 0.882117825607615 Loss: 0.10347031481702552\n",
      "Iteration: 2638 lambda_n: 1.0212233534091817 Loss: 0.10344374924408004\n",
      "Iteration: 2639 lambda_n: 0.9899959859824418 Loss: 0.10341300464748321\n",
      "Iteration: 2640 lambda_n: 0.9113149091754832 Loss: 0.10338321109770508\n",
      "Iteration: 2641 lambda_n: 0.9280247607162503 Loss: 0.10335579504252908\n",
      "Iteration: 2642 lambda_n: 0.9329361710113931 Loss: 0.10332788553934523\n",
      "Iteration: 2643 lambda_n: 0.9066397817378998 Loss: 0.10329983776499164\n",
      "Iteration: 2644 lambda_n: 0.9463418208367205 Loss: 0.10327258969621071\n",
      "Iteration: 2645 lambda_n: 0.8977662994716087 Loss: 0.10324415786436585\n",
      "Iteration: 2646 lambda_n: 0.9631551156334682 Loss: 0.10321719453955983\n",
      "Iteration: 2647 lambda_n: 1.0067933112231529 Loss: 0.1031882769180699\n",
      "Iteration: 2648 lambda_n: 0.9503732186380768 Loss: 0.10315805976864918\n",
      "Iteration: 2649 lambda_n: 0.8764805348799186 Loss: 0.10312954620080896\n",
      "Iteration: 2650 lambda_n: 0.9131948669041648 Loss: 0.10310325844771755\n",
      "Iteration: 2651 lambda_n: 0.969396659018149 Loss: 0.10307587832221832\n",
      "Iteration: 2652 lambda_n: 1.0139411291193434 Loss: 0.10304682286638583\n",
      "Iteration: 2653 lambda_n: 0.8892983147979332 Loss: 0.10301644307490153\n",
      "Iteration: 2654 lambda_n: 0.9749744486669238 Loss: 0.10298980728755605\n",
      "Iteration: 2655 lambda_n: 0.9685292562521461 Loss: 0.10296061500544872\n",
      "Iteration: 2656 lambda_n: 0.9579858785531664 Loss: 0.10293162590498392\n",
      "Iteration: 2657 lambda_n: 0.9533710536172658 Loss: 0.10290296238765259\n",
      "Iteration: 2658 lambda_n: 0.906836054574325 Loss: 0.10287444681267376\n",
      "Iteration: 2659 lambda_n: 0.9658635773807263 Loss: 0.10284733233438276\n",
      "Iteration: 2660 lambda_n: 0.9395872896267413 Loss: 0.1028184625576714\n",
      "Iteration: 2661 lambda_n: 0.896792599506581 Loss: 0.10279038791243567\n",
      "Iteration: 2662 lambda_n: 0.9179679514817859 Loss: 0.10276360094791583\n",
      "Iteration: 2663 lambda_n: 0.9530818188889025 Loss: 0.10273619042169621\n",
      "Iteration: 2664 lambda_n: 1.0091461381448572 Loss: 0.10270774093059817\n",
      "Iteration: 2665 lambda_n: 0.9835685642184441 Loss: 0.10267762845541946\n",
      "Iteration: 2666 lambda_n: 0.9623271327656231 Loss: 0.10264828982924801\n",
      "Iteration: 2667 lambda_n: 0.8975877630885366 Loss: 0.10261959494690247\n",
      "Iteration: 2668 lambda_n: 0.9961590076314459 Loss: 0.10259283961077857\n",
      "Iteration: 2669 lambda_n: 0.9814699495430813 Loss: 0.10256315596932433\n",
      "Iteration: 2670 lambda_n: 0.9948806419065378 Loss: 0.10253392051266953\n",
      "Iteration: 2671 lambda_n: 0.9362427765158461 Loss: 0.10250429612757583\n",
      "Iteration: 2672 lambda_n: 0.9517943386349844 Loss: 0.10247642764674944\n",
      "Iteration: 2673 lambda_n: 1.0181790189094158 Loss: 0.1024481058731464\n",
      "Iteration: 2674 lambda_n: 1.015569592556244 Loss: 0.10241781935129607\n",
      "Iteration: 2675 lambda_n: 0.9282040023572327 Loss: 0.10238762154464358\n",
      "Iteration: 2676 lambda_n: 0.9002141682547741 Loss: 0.10236003142266283\n",
      "Iteration: 2677 lambda_n: 0.9683513491321015 Loss: 0.10233328216758214\n",
      "Iteration: 2678 lambda_n: 0.9938105693154521 Loss: 0.10230451780266302\n",
      "Iteration: 2679 lambda_n: 0.9518006883612773 Loss: 0.1022750075752517\n",
      "Iteration: 2680 lambda_n: 0.9318047805455058 Loss: 0.10224675480986632\n",
      "Iteration: 2681 lambda_n: 0.9091324200607401 Loss: 0.10221910503830298\n",
      "Iteration: 2682 lambda_n: 0.8815685825643916 Loss: 0.10219213704187564\n",
      "Iteration: 2683 lambda_n: 0.8870619544332261 Loss: 0.1021659951907965\n",
      "Iteration: 2684 lambda_n: 0.9117897859134126 Loss: 0.10213969882074071\n",
      "Iteration: 2685 lambda_n: 0.8792666967693574 Loss: 0.10211267812554294\n",
      "Iteration: 2686 lambda_n: 0.9614582545847953 Loss: 0.10208662973249802\n",
      "Iteration: 2687 lambda_n: 0.9815455528802602 Loss: 0.10205815566696115\n",
      "Iteration: 2688 lambda_n: 1.0114836605563828 Loss: 0.10202909684417845\n",
      "Iteration: 2689 lambda_n: 0.9214700095456292 Loss: 0.10199916238759586\n",
      "Iteration: 2690 lambda_n: 0.9551542410441669 Loss: 0.10197190155624968\n",
      "Iteration: 2691 lambda_n: 0.9519918438616092 Loss: 0.1019436536917266\n",
      "Iteration: 2692 lambda_n: 0.9783128478063254 Loss: 0.10191550904080741\n",
      "Iteration: 2693 lambda_n: 1.022541021349768 Loss: 0.10188659623978162\n",
      "Iteration: 2694 lambda_n: 0.9903979546664479 Loss: 0.10185638712199803\n",
      "Iteration: 2695 lambda_n: 0.9354200947154876 Loss: 0.10182713830718705\n",
      "Iteration: 2696 lambda_n: 0.9382517139769698 Loss: 0.10179952283397316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2697 lambda_n: 1.0278364986696193 Loss: 0.10177183311374564\n",
      "Iteration: 2698 lambda_n: 0.9982748085707208 Loss: 0.10174151009045874\n",
      "Iteration: 2699 lambda_n: 0.8855029263790873 Loss: 0.10171207001552254\n",
      "Iteration: 2700 lambda_n: 0.9298562560861781 Loss: 0.10168596479681315\n",
      "Iteration: 2701 lambda_n: 0.9432820455603455 Loss: 0.10165856088041765\n",
      "Iteration: 2702 lambda_n: 0.9809137641263094 Loss: 0.10163077064330636\n",
      "Iteration: 2703 lambda_n: 1.0262895563517092 Loss: 0.10160188166051533\n",
      "Iteration: 2704 lambda_n: 1.0204994269087564 Loss: 0.10157166712959388\n",
      "Iteration: 2705 lambda_n: 0.9517498855975401 Loss: 0.10154163415586433\n",
      "Iteration: 2706 lambda_n: 0.9111422529737556 Loss: 0.10151363458604107\n",
      "Iteration: 2707 lambda_n: 1.0056155854716828 Loss: 0.10148683872354879\n",
      "Iteration: 2708 lambda_n: 0.9705318142511274 Loss: 0.1014572744473516\n",
      "Iteration: 2709 lambda_n: 1.0166756316495789 Loss: 0.1014287518621614\n",
      "Iteration: 2710 lambda_n: 0.9176148910346028 Loss: 0.10139888375587751\n",
      "Iteration: 2711 lambda_n: 0.9401926609224149 Loss: 0.1013719354846381\n",
      "Iteration: 2712 lambda_n: 1.0168397975755736 Loss: 0.1013443333426575\n",
      "Iteration: 2713 lambda_n: 0.8860173957679146 Loss: 0.10131449133162272\n",
      "Iteration: 2714 lambda_n: 0.8819330500436676 Loss: 0.10128849785617765\n",
      "Iteration: 2715 lambda_n: 0.8775426117183024 Loss: 0.10126263244967289\n",
      "Iteration: 2716 lambda_n: 0.9336277653867605 Loss: 0.10123690396872864\n",
      "Iteration: 2717 lambda_n: 0.947766423181028 Loss: 0.10120953994080462\n",
      "Iteration: 2718 lambda_n: 0.8880779705508297 Loss: 0.1011817708949939\n",
      "Iteration: 2719 lambda_n: 0.9747387495067896 Loss: 0.10115575941394053\n",
      "Iteration: 2720 lambda_n: 1.0269840123796523 Loss: 0.10112721904361562\n",
      "Iteration: 2721 lambda_n: 0.9681684059985369 Loss: 0.10109715963880275\n",
      "Iteration: 2722 lambda_n: 1.0277378737591696 Loss: 0.10106883205433062\n",
      "Iteration: 2723 lambda_n: 0.9154865358958942 Loss: 0.10103877218789095\n",
      "Iteration: 2724 lambda_n: 0.996894826557928 Loss: 0.10101200512037867\n",
      "Iteration: 2725 lambda_n: 0.8968210839922861 Loss: 0.10098286767083993\n",
      "Iteration: 2726 lambda_n: 0.9816079797565894 Loss: 0.10095666434201407\n",
      "Iteration: 2727 lambda_n: 0.996991885455322 Loss: 0.10092799320728757\n",
      "Iteration: 2728 lambda_n: 0.9404663532062492 Loss: 0.10089888306599319\n",
      "Iteration: 2729 lambda_n: 1.0239799820222784 Loss: 0.10087143305359621\n",
      "Iteration: 2730 lambda_n: 0.9094505177412388 Loss: 0.10084155583635131\n",
      "Iteration: 2731 lambda_n: 0.9558849256862011 Loss: 0.10081502978208731\n",
      "Iteration: 2732 lambda_n: 0.9951305516748773 Loss: 0.10078715862182747\n",
      "Iteration: 2733 lambda_n: 0.9542733294772294 Loss: 0.10075815325182312\n",
      "Iteration: 2734 lambda_n: 1.01147115230174 Loss: 0.10073034860472362\n",
      "Iteration: 2735 lambda_n: 0.9496602645244522 Loss: 0.10070088767115154\n",
      "Iteration: 2736 lambda_n: 0.9235979472251011 Loss: 0.10067323698639978\n",
      "Iteration: 2737 lambda_n: 0.8911966299968517 Loss: 0.10064635425852296\n",
      "Iteration: 2738 lambda_n: 0.9053617417376085 Loss: 0.1006204231548675\n",
      "Iteration: 2739 lambda_n: 0.8828896473343268 Loss: 0.10059408837132264\n",
      "Iteration: 2740 lambda_n: 0.9374115772925362 Loss: 0.10056841555175774\n",
      "Iteration: 2741 lambda_n: 0.9104258055858353 Loss: 0.10054116613781033\n",
      "Iteration: 2742 lambda_n: 0.9268611297609622 Loss: 0.10051471001837649\n",
      "Iteration: 2743 lambda_n: 0.899423188091559 Loss: 0.10048778516652226\n",
      "Iteration: 2744 lambda_n: 0.9373230993001045 Loss: 0.10046166601020472\n",
      "Iteration: 2745 lambda_n: 0.9659272763144442 Loss: 0.10043445515112527\n",
      "Iteration: 2746 lambda_n: 1.0173360994879215 Loss: 0.10040642343283664\n",
      "Iteration: 2747 lambda_n: 0.9188137969862683 Loss: 0.10037691020921423\n",
      "Iteration: 2748 lambda_n: 1.0267219466772124 Loss: 0.10035026463438282\n",
      "Iteration: 2749 lambda_n: 0.9869317573512499 Loss: 0.10032049988361935\n",
      "Iteration: 2750 lambda_n: 0.9704420999012515 Loss: 0.10029189910091624\n",
      "Iteration: 2751 lambda_n: 0.8826414689185381 Loss: 0.10026378611190921\n",
      "Iteration: 2752 lambda_n: 0.9642964888875012 Loss: 0.10023822533756437\n",
      "Iteration: 2753 lambda_n: 0.9803431074083477 Loss: 0.10021030897091023\n",
      "Iteration: 2754 lambda_n: 1.0086313770657103 Loss: 0.10018193793810842\n",
      "Iteration: 2755 lambda_n: 1.0028725999553436 Loss: 0.10015275861418799\n",
      "Iteration: 2756 lambda_n: 0.9449215224185665 Loss: 0.1001237563908142\n",
      "Iteration: 2757 lambda_n: 0.9850924118971911 Loss: 0.10009643975419077\n",
      "Iteration: 2758 lambda_n: 0.9872893313936043 Loss: 0.10006797160752941\n",
      "Iteration: 2759 lambda_n: 0.927382074368597 Loss: 0.10003945007943256\n",
      "Iteration: 2760 lambda_n: 0.9296236550559716 Loss: 0.10001266853978154\n",
      "Iteration: 2761 lambda_n: 0.9807513182948251 Loss: 0.09998583121907538\n",
      "Iteration: 2762 lambda_n: 0.9430035684389597 Loss: 0.09995752749653719\n",
      "Iteration: 2763 lambda_n: 0.9984556759534755 Loss: 0.09993032263215564\n",
      "Iteration: 2764 lambda_n: 1.0022359654381527 Loss: 0.09990152794136818\n",
      "Iteration: 2765 lambda_n: 0.9438811535961782 Loss: 0.09987263460999257\n",
      "Iteration: 2766 lambda_n: 0.9175863868751422 Loss: 0.09984543322057501\n",
      "Iteration: 2767 lambda_n: 0.9045069164837167 Loss: 0.09981899850831695\n",
      "Iteration: 2768 lambda_n: 0.9098109994668873 Loss: 0.09979294915763864\n",
      "Iteration: 2769 lambda_n: 0.9446465254841397 Loss: 0.09976675558000907\n",
      "Iteration: 2770 lambda_n: 0.912954401464349 Loss: 0.09973956806711153\n",
      "Iteration: 2771 lambda_n: 0.895513129089647 Loss: 0.09971330150539759\n",
      "Iteration: 2772 lambda_n: 0.9402349127145788 Loss: 0.09968754514948132\n",
      "Iteration: 2773 lambda_n: 1.003396971925741 Loss: 0.09966051134569874\n",
      "Iteration: 2774 lambda_n: 0.995024505434626 Loss: 0.09963167142177058\n",
      "Iteration: 2775 lambda_n: 1.0144664200206137 Loss: 0.09960308242437103\n",
      "Iteration: 2776 lambda_n: 0.9912827651148126 Loss: 0.09957394529640755\n",
      "Iteration: 2777 lambda_n: 0.9350242109076518 Loss: 0.09954548434727209\n",
      "Iteration: 2778 lambda_n: 0.9649771385992744 Loss: 0.09951864805496183\n",
      "Iteration: 2779 lambda_n: 0.9360710872480451 Loss: 0.0994909614620273\n",
      "Iteration: 2780 lambda_n: 0.8927727988655013 Loss: 0.09946411345384297\n",
      "Iteration: 2781 lambda_n: 1.0203018492833578 Loss: 0.09943851580441398\n",
      "Iteration: 2782 lambda_n: 0.936334859521335 Loss: 0.09940927138777796\n",
      "Iteration: 2783 lambda_n: 0.9392454686668451 Loss: 0.09938244328041168\n",
      "Iteration: 2784 lambda_n: 0.9694732102707105 Loss: 0.09935554083290407\n",
      "Iteration: 2785 lambda_n: 0.9837915025362829 Loss: 0.09932778203297231\n",
      "Iteration: 2786 lambda_n: 0.9092163020218142 Loss: 0.09929962310478355\n",
      "Iteration: 2787 lambda_n: 0.8991294005518905 Loss: 0.09927360772741654\n",
      "Iteration: 2788 lambda_n: 1.0208195429295746 Loss: 0.09924788933746252\n",
      "Iteration: 2789 lambda_n: 0.9326999207974714 Loss: 0.09921869994939958\n",
      "Iteration: 2790 lambda_n: 0.985707644199103 Loss: 0.0991920397952385\n",
      "Iteration: 2791 lambda_n: 0.9140035825189525 Loss: 0.09916387406120428\n",
      "Iteration: 2792 lambda_n: 1.0213405472679729 Loss: 0.09913776625894144\n",
      "Iteration: 2793 lambda_n: 1.0255155941220961 Loss: 0.09910860234882497\n",
      "Iteration: 2794 lambda_n: 0.9551516178278253 Loss: 0.09907932996975535\n",
      "Iteration: 2795 lambda_n: 0.977489375368681 Loss: 0.099052075904872\n",
      "Iteration: 2796 lambda_n: 1.011459082692998 Loss: 0.09902419408478987\n",
      "Iteration: 2797 lambda_n: 0.9774299849478211 Loss: 0.09899535353696153\n",
      "Iteration: 2798 lambda_n: 0.9834991241823354 Loss: 0.09896749330885021\n",
      "Iteration: 2799 lambda_n: 1.0187118800798753 Loss: 0.09893946993952737\n",
      "Iteration: 2800 lambda_n: 0.9572150099299273 Loss: 0.09891045358315191\n",
      "Iteration: 2801 lambda_n: 0.9399927808937933 Loss: 0.09888319866207343\n",
      "Iteration: 2802 lambda_n: 0.8768841418076128 Loss: 0.09885644325853601\n",
      "Iteration: 2803 lambda_n: 0.9322265868591771 Loss: 0.09883149240067961\n",
      "Iteration: 2804 lambda_n: 1.0123113805523793 Loss: 0.09880497532486965\n",
      "Iteration: 2805 lambda_n: 0.9300425761954821 Loss: 0.09877619011343672\n",
      "Iteration: 2806 lambda_n: 0.9467090714605944 Loss: 0.09874975361065835\n",
      "Iteration: 2807 lambda_n: 0.9629124629692124 Loss: 0.09872285238755733\n",
      "Iteration: 2808 lambda_n: 0.9849019740557681 Loss: 0.09869550007585975\n",
      "Iteration: 2809 lambda_n: 1.008316963997899 Loss: 0.09866753286222968\n",
      "Iteration: 2810 lambda_n: 1.0198943533220555 Loss: 0.09863891094368453\n",
      "Iteration: 2811 lambda_n: 0.9946328160117412 Loss: 0.0986099708981705\n",
      "Iteration: 2812 lambda_n: 0.9719420343062677 Loss: 0.0985817579158998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2813 lambda_n: 0.9251529122737694 Loss: 0.09855419833590152\n",
      "Iteration: 2814 lambda_n: 1.008606491778811 Loss: 0.09852797449028022\n",
      "Iteration: 2815 lambda_n: 0.9588524091855416 Loss: 0.09849939484998031\n",
      "Iteration: 2816 lambda_n: 0.8817990805793052 Loss: 0.09847223472182186\n",
      "Iteration: 2817 lambda_n: 1.0236196178086276 Loss: 0.09844726557384141\n",
      "Iteration: 2818 lambda_n: 0.9060131286191504 Loss: 0.09841829020348311\n",
      "Iteration: 2819 lambda_n: 1.0205078682374393 Loss: 0.09839265300090594\n",
      "Iteration: 2820 lambda_n: 0.9318638428951087 Loss: 0.0983637856949875\n",
      "Iteration: 2821 lambda_n: 0.9396072503366618 Loss: 0.09833743528884108\n",
      "Iteration: 2822 lambda_n: 1.0171151496095021 Loss: 0.0983108748383537\n",
      "Iteration: 2823 lambda_n: 0.9893620314832305 Loss: 0.09828213331008791\n",
      "Iteration: 2824 lambda_n: 0.9229646335642105 Loss: 0.09825418615976385\n",
      "Iteration: 2825 lambda_n: 0.973494058918212 Loss: 0.09822812365210265\n",
      "Iteration: 2826 lambda_n: 0.9167954633531891 Loss: 0.09820064354561053\n",
      "Iteration: 2827 lambda_n: 0.9497033127770123 Loss: 0.0981747728296206\n",
      "Iteration: 2828 lambda_n: 0.9414725632656142 Loss: 0.0981479824003067\n",
      "Iteration: 2829 lambda_n: 0.9709440714636508 Loss: 0.09812143317654931\n",
      "Iteration: 2830 lambda_n: 1.012297808238296 Loss: 0.09809406219224646\n",
      "Iteration: 2831 lambda_n: 0.9735060072874357 Loss: 0.09806553549979473\n",
      "Iteration: 2832 lambda_n: 0.9125582923662984 Loss: 0.09803811181412217\n",
      "Iteration: 2833 lambda_n: 0.8897637987474931 Loss: 0.09801241383904037\n",
      "Iteration: 2834 lambda_n: 0.9845562781727177 Loss: 0.09798736590482555\n",
      "Iteration: 2835 lambda_n: 1.002955179197611 Loss: 0.09795965855950493\n",
      "Iteration: 2836 lambda_n: 0.9404095023602008 Loss: 0.09793144345008621\n",
      "Iteration: 2837 lambda_n: 0.9857103548526929 Loss: 0.09790499721685242\n",
      "Iteration: 2838 lambda_n: 0.9528774654535501 Loss: 0.09787728650679406\n",
      "Iteration: 2839 lambda_n: 0.9458293923867105 Loss: 0.09785050818492108\n",
      "Iteration: 2840 lambda_n: 0.8824898246381634 Loss: 0.09782393699308038\n",
      "Iteration: 2841 lambda_n: 1.014279674072574 Loss: 0.09779915344464231\n",
      "Iteration: 2842 lambda_n: 0.9831954761305037 Loss: 0.09777067815322922\n",
      "Iteration: 2843 lambda_n: 0.9168703472383575 Loss: 0.09774308548000343\n",
      "Iteration: 2844 lambda_n: 0.915075972096606 Loss: 0.09771736306677169\n",
      "Iteration: 2845 lambda_n: 0.8902050941752331 Loss: 0.09769169942696053\n",
      "Iteration: 2846 lambda_n: 0.9943696011965805 Loss: 0.09766674142534579\n",
      "Iteration: 2847 lambda_n: 0.8814593512115636 Loss: 0.09763887223906928\n",
      "Iteration: 2848 lambda_n: 1.0290036606666082 Loss: 0.09761417610446238\n",
      "Iteration: 2849 lambda_n: 0.9631816162646614 Loss: 0.09758535572167791\n",
      "Iteration: 2850 lambda_n: 0.9875511893452484 Loss: 0.09755838865226178\n",
      "Iteration: 2851 lambda_n: 0.9837085841307248 Loss: 0.0975307489011859\n",
      "Iteration: 2852 lambda_n: 0.9956216055066885 Loss: 0.09750322643218297\n",
      "Iteration: 2853 lambda_n: 1.0212586314913439 Loss: 0.09747538051328919\n",
      "Iteration: 2854 lambda_n: 0.9248884854959398 Loss: 0.09744682783695038\n",
      "Iteration: 2855 lambda_n: 1.0050501044163969 Loss: 0.09742097871475576\n",
      "Iteration: 2856 lambda_n: 0.962261565868796 Loss: 0.09739289875091278\n",
      "Iteration: 2857 lambda_n: 0.9295637746773132 Loss: 0.0973660238152219\n",
      "Iteration: 2858 lambda_n: 1.008323092740021 Loss: 0.09734007095911312\n",
      "Iteration: 2859 lambda_n: 0.9870880564087191 Loss: 0.09731192879932202\n",
      "Iteration: 2860 lambda_n: 0.9273670870527844 Loss: 0.09728438920118733\n",
      "Iteration: 2861 lambda_n: 0.9436431068954031 Loss: 0.09725852480069154\n",
      "Iteration: 2862 lambda_n: 0.9349965891496749 Loss: 0.0972322152507806\n",
      "Iteration: 2863 lambda_n: 1.0095964271754674 Loss: 0.09720615556500319\n",
      "Iteration: 2864 lambda_n: 0.9910757138194317 Loss: 0.09717802632209428\n",
      "Iteration: 2865 lambda_n: 0.9678680721151961 Loss: 0.09715042303797362\n",
      "Iteration: 2866 lambda_n: 0.9262584288243818 Loss: 0.09712347563810213\n",
      "Iteration: 2867 lambda_n: 0.9280670054501826 Loss: 0.09709769556729769\n",
      "Iteration: 2868 lambda_n: 0.9707851756749377 Loss: 0.0970718737375937\n",
      "Iteration: 2869 lambda_n: 0.9366202212554162 Loss: 0.09704487245265533\n",
      "Iteration: 2870 lambda_n: 0.9076174294505285 Loss: 0.09701883039641958\n",
      "Iteration: 2871 lambda_n: 0.9981762037545335 Loss: 0.09699360313519656\n",
      "Iteration: 2872 lambda_n: 0.9719653104116498 Loss: 0.0969658680741325\n",
      "Iteration: 2873 lambda_n: 0.9424691693444066 Loss: 0.09693887088750597\n",
      "Iteration: 2874 lambda_n: 0.8908269210370325 Loss: 0.09691270201826815\n",
      "Iteration: 2875 lambda_n: 0.938183573043363 Loss: 0.09688797528195503\n",
      "Iteration: 2876 lambda_n: 0.8766010317504477 Loss: 0.09686194250479\n",
      "Iteration: 2877 lambda_n: 0.9108860049504877 Loss: 0.09683762654387894\n",
      "Iteration: 2878 lambda_n: 0.8786078683611551 Loss: 0.09681236758126441\n",
      "Iteration: 2879 lambda_n: 0.9337081334879107 Loss: 0.09678801156385826\n",
      "Iteration: 2880 lambda_n: 0.9411724349712172 Loss: 0.0967621364007928\n",
      "Iteration: 2881 lambda_n: 0.9335024576083987 Loss: 0.09673606313056697\n",
      "Iteration: 2882 lambda_n: 1.0114648726424318 Loss: 0.09671021104095238\n",
      "Iteration: 2883 lambda_n: 0.9103996960750877 Loss: 0.09668220948427653\n",
      "Iteration: 2884 lambda_n: 0.9362242884127409 Loss: 0.09665701469888828\n",
      "Iteration: 2885 lambda_n: 0.9843329348003715 Loss: 0.09663111375355571\n",
      "Iteration: 2886 lambda_n: 0.8794757169378794 Loss: 0.09660389113816725\n",
      "Iteration: 2887 lambda_n: 1.0015373054199153 Loss: 0.09657957675382506\n",
      "Iteration: 2888 lambda_n: 0.9719373393885329 Loss: 0.0965518968776923\n",
      "Iteration: 2889 lambda_n: 0.9911068871685921 Loss: 0.09652504462204553\n",
      "Iteration: 2890 lambda_n: 1.023652677965701 Loss: 0.09649767234782121\n",
      "Iteration: 2891 lambda_n: 0.9443418713188859 Loss: 0.0964694113621625\n",
      "Iteration: 2892 lambda_n: 0.9518704790825302 Loss: 0.09644334933165456\n",
      "Iteration: 2893 lambda_n: 0.8888663041583889 Loss: 0.09641708843370914\n",
      "Iteration: 2894 lambda_n: 0.9900455250921472 Loss: 0.09639257394070928\n",
      "Iteration: 2895 lambda_n: 1.0264822485324405 Loss: 0.09636527795941278\n",
      "Iteration: 2896 lambda_n: 0.9139534687306038 Loss: 0.09633698754746897\n",
      "Iteration: 2897 lambda_n: 0.9086883336242075 Loss: 0.09631180745457106\n",
      "Iteration: 2898 lambda_n: 0.9704790892313301 Loss: 0.0962867806025297\n",
      "Iteration: 2899 lambda_n: 0.9902521722291805 Loss: 0.09626006080007071\n",
      "Iteration: 2900 lambda_n: 0.9998534383398798 Loss: 0.09623280612832573\n",
      "Iteration: 2901 lambda_n: 0.9649542430690228 Loss: 0.09620529699070235\n",
      "Iteration: 2902 lambda_n: 0.9262018463581682 Loss: 0.0961787574517516\n",
      "Iteration: 2903 lambda_n: 0.9729663407662457 Loss: 0.0961532924372326\n",
      "Iteration: 2904 lambda_n: 0.9270436405545486 Loss: 0.09612655068555956\n",
      "Iteration: 2905 lambda_n: 1.010880591348984 Loss: 0.09610107986532623\n",
      "Iteration: 2906 lambda_n: 0.9747258551996191 Loss: 0.0960733150665468\n",
      "Iteration: 2907 lambda_n: 0.984597606356742 Loss: 0.09604655288395107\n",
      "Iteration: 2908 lambda_n: 0.9887224997360665 Loss: 0.0960195291280981\n",
      "Iteration: 2909 lambda_n: 1.0139412895837818 Loss: 0.09599240174081083\n",
      "Iteration: 2910 lambda_n: 0.9383841404058577 Loss: 0.09596459235689815\n",
      "Iteration: 2911 lambda_n: 0.9280351062119109 Loss: 0.09593886442204176\n",
      "Iteration: 2912 lambda_n: 0.9381375971576148 Loss: 0.09591342875391128\n",
      "Iteration: 2913 lambda_n: 0.9298444495610235 Loss: 0.09588772477036762\n",
      "Iteration: 2914 lambda_n: 0.876222954981318 Loss: 0.09586225654887313\n",
      "Iteration: 2915 lambda_n: 0.9003858474592942 Loss: 0.09583826486456572\n",
      "Iteration: 2916 lambda_n: 0.9951561077922473 Loss: 0.09581361938131247\n",
      "Iteration: 2917 lambda_n: 0.9348979097608123 Loss: 0.0957863888916756\n",
      "Iteration: 2918 lambda_n: 1.026420335981057 Loss: 0.09576081620436919\n",
      "Iteration: 2919 lambda_n: 0.8922966520055189 Loss: 0.09573274973632573\n",
      "Iteration: 2920 lambda_n: 0.8799392002156342 Loss: 0.09570835937362408\n",
      "Iteration: 2921 lambda_n: 0.9516961280433013 Loss: 0.09568431444966166\n",
      "Iteration: 2922 lambda_n: 0.8809698617633026 Loss: 0.09565831711401727\n",
      "Iteration: 2923 lambda_n: 0.9906985325070339 Loss: 0.09563425981951375\n",
      "Iteration: 2924 lambda_n: 1.0185889605738716 Loss: 0.09560721493464189\n",
      "Iteration: 2925 lambda_n: 0.8860729106514043 Loss: 0.09557941862362011\n",
      "Iteration: 2926 lambda_n: 1.0199502953911286 Loss: 0.09555524702933323\n",
      "Iteration: 2927 lambda_n: 0.8765067527756385 Loss: 0.09552743255761784\n",
      "Iteration: 2928 lambda_n: 0.9005317601083254 Loss: 0.09550353821918547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2929 lambda_n: 1.0242540387960213 Loss: 0.09547899670770137\n",
      "Iteration: 2930 lambda_n: 1.01343319182301 Loss: 0.09545109285684336\n",
      "Iteration: 2931 lambda_n: 1.0039355111962056 Loss: 0.09542349389623016\n",
      "Iteration: 2932 lambda_n: 1.0101987721516947 Loss: 0.0953961634836595\n",
      "Iteration: 2933 lambda_n: 0.8866102434741022 Loss: 0.09536867247175469\n",
      "Iteration: 2934 lambda_n: 1.0027906594641436 Loss: 0.09534455314479721\n",
      "Iteration: 2935 lambda_n: 0.9774365446858182 Loss: 0.0953172822486251\n",
      "Iteration: 2936 lambda_n: 0.9830959035365056 Loss: 0.09529071033315482\n",
      "Iteration: 2937 lambda_n: 1.025546790709095 Loss: 0.0952639939387754\n",
      "Iteration: 2938 lambda_n: 0.9228014890957217 Loss: 0.09523613384823532\n",
      "Iteration: 2939 lambda_n: 1.011667397720933 Loss: 0.09521107387884173\n",
      "Iteration: 2940 lambda_n: 0.8825597404232339 Loss: 0.09518360997195653\n",
      "Iteration: 2941 lambda_n: 0.8781981350547284 Loss: 0.09515965932398954\n",
      "Iteration: 2942 lambda_n: 0.9528355321994679 Loss: 0.09513583456240007\n",
      "Iteration: 2943 lambda_n: 1.0094178794791084 Loss: 0.09510999328920465\n",
      "Iteration: 2944 lambda_n: 0.9624754974823947 Loss: 0.09508262698889224\n",
      "Iteration: 2945 lambda_n: 0.9127509442139841 Loss: 0.09505654264539468\n",
      "Iteration: 2946 lambda_n: 0.9598251637394847 Loss: 0.09503181429833038\n",
      "Iteration: 2947 lambda_n: 0.9788099601439783 Loss: 0.09500581924358197\n",
      "Iteration: 2948 lambda_n: 0.9866293552759055 Loss: 0.09497931918879093\n",
      "Iteration: 2949 lambda_n: 0.9253646646010624 Loss: 0.09495261682085109\n",
      "Iteration: 2950 lambda_n: 0.893023520036841 Loss: 0.0949275812209596\n",
      "Iteration: 2951 lambda_n: 0.9322517883547397 Loss: 0.09490342853003228\n",
      "Iteration: 2952 lambda_n: 0.9669538003964011 Loss: 0.09487822304325026\n",
      "Iteration: 2953 lambda_n: 0.9701874072525201 Loss: 0.09485208813794216\n",
      "Iteration: 2954 lambda_n: 1.02831037131073 Loss: 0.0948258749255686\n",
      "Iteration: 2955 lambda_n: 0.9811742022436206 Loss: 0.09479810113391722\n",
      "Iteration: 2956 lambda_n: 0.8877891753229756 Loss: 0.09477161007720224\n",
      "Iteration: 2957 lambda_n: 0.8864464008364136 Loss: 0.0947476485384439\n",
      "Iteration: 2958 lambda_n: 0.9875697849804392 Loss: 0.0947237308460129\n",
      "Iteration: 2959 lambda_n: 0.9501592986749274 Loss: 0.09469709343858475\n",
      "Iteration: 2960 lambda_n: 0.9539873020157991 Loss: 0.09467147405518607\n",
      "Iteration: 2961 lambda_n: 1.0282060398961776 Loss: 0.09464576022146821\n",
      "Iteration: 2962 lambda_n: 0.9363933903341167 Loss: 0.09461805558286245\n",
      "Iteration: 2963 lambda_n: 0.8865925182310115 Loss: 0.09459283384624723\n",
      "Iteration: 2964 lambda_n: 0.9050066746373279 Loss: 0.09456896137701147\n",
      "Iteration: 2965 lambda_n: 0.9200106840236216 Loss: 0.09454460087372833\n",
      "Iteration: 2966 lambda_n: 1.0267849872023085 Loss: 0.0945198445683317\n",
      "Iteration: 2967 lambda_n: 1.0289079019272027 Loss: 0.09449222452686759\n",
      "Iteration: 2968 lambda_n: 0.9584651869532356 Loss: 0.09446455756623788\n",
      "Iteration: 2969 lambda_n: 0.8771376487863672 Loss: 0.0944387940881255\n",
      "Iteration: 2970 lambda_n: 0.9082279163879683 Loss: 0.09441522457689543\n",
      "Iteration: 2971 lambda_n: 0.9179626707616759 Loss: 0.09439082739104761\n",
      "Iteration: 2972 lambda_n: 1.015446730812702 Loss: 0.09436617675481333\n",
      "Iteration: 2973 lambda_n: 0.908268144668518 Loss: 0.09433891757086287\n",
      "Iteration: 2974 lambda_n: 0.9407855348537363 Loss: 0.09431454413214266\n",
      "Iteration: 2975 lambda_n: 0.9713859991210342 Loss: 0.0942893063899729\n",
      "Iteration: 2976 lambda_n: 0.9054437796632518 Loss: 0.09426325662080041\n",
      "Iteration: 2977 lambda_n: 1.0199331093019015 Loss: 0.09423898350820441\n",
      "Iteration: 2978 lambda_n: 1.0051771465862902 Loss: 0.09421165037619773\n",
      "Iteration: 2979 lambda_n: 1.0101774343732304 Loss: 0.09418472248817\n",
      "Iteration: 2980 lambda_n: 1.0066745964602228 Loss: 0.09415767040622777\n",
      "Iteration: 2981 lambda_n: 0.937686757933707 Loss: 0.0941307218746719\n",
      "Iteration: 2982 lambda_n: 0.91168315713489 Loss: 0.09410562900284297\n",
      "Iteration: 2983 lambda_n: 0.8847121824462065 Loss: 0.09408124012484592\n",
      "Iteration: 2984 lambda_n: 0.9992294877952619 Loss: 0.09405758042165421\n",
      "Iteration: 2985 lambda_n: 0.9795543309153625 Loss: 0.09403086700383126\n",
      "Iteration: 2986 lambda_n: 0.9973635821862716 Loss: 0.09400468890112919\n",
      "Iteration: 2987 lambda_n: 0.9911385697805659 Loss: 0.09397804426084327\n",
      "Iteration: 2988 lambda_n: 0.9556026321447785 Loss: 0.09395157536604434\n",
      "Iteration: 2989 lambda_n: 0.8858774304889269 Loss: 0.09392606444174458\n",
      "Iteration: 2990 lambda_n: 0.9900423996367791 Loss: 0.09390242283092491\n",
      "Iteration: 2991 lambda_n: 0.9805567342990371 Loss: 0.0938760100313638\n",
      "Iteration: 2992 lambda_n: 0.8785421686047347 Loss: 0.09384985954740804\n",
      "Iteration: 2993 lambda_n: 1.003581151089708 Loss: 0.09382643766350017\n",
      "Iteration: 2994 lambda_n: 0.9986620238336995 Loss: 0.09379969103360497\n",
      "Iteration: 2995 lambda_n: 0.9452766276897607 Loss: 0.09377308506093089\n",
      "Iteration: 2996 lambda_n: 0.9202289552326831 Loss: 0.09374791022700729\n",
      "Iteration: 2997 lambda_n: 0.9610901020507644 Loss: 0.09372341070298046\n",
      "Iteration: 2998 lambda_n: 0.9779012770948694 Loss: 0.09369783187185858\n",
      "Iteration: 2999 lambda_n: 1.0215873078092514 Loss: 0.09367181463236929\n",
      "Iteration: 3000 lambda_n: 1.0208571178260222 Loss: 0.09364464477176503\n",
      "Iteration: 3001 lambda_n: 0.9335205172129092 Loss: 0.09361750426823803\n",
      "Iteration: 3002 lambda_n: 0.9779869536942905 Loss: 0.09359269453792425\n",
      "Iteration: 3003 lambda_n: 0.9918531218659935 Loss: 0.09356671188049885\n",
      "Iteration: 3004 lambda_n: 0.9815389635490946 Loss: 0.09354037009841633\n",
      "Iteration: 3005 lambda_n: 0.9856906747405765 Loss: 0.09351431147178975\n",
      "Iteration: 3006 lambda_n: 0.9645839620706006 Loss: 0.09348815183963811\n",
      "Iteration: 3007 lambda_n: 0.9853025855087203 Loss: 0.0934625613364773\n",
      "Iteration: 3008 lambda_n: 1.0088881884343621 Loss: 0.09343643027857161\n",
      "Iteration: 3009 lambda_n: 0.8998076362231096 Loss: 0.09340968322721388\n",
      "Iteration: 3010 lambda_n: 0.9948550826209271 Loss: 0.09338583639549171\n",
      "Iteration: 3011 lambda_n: 0.9310073415603702 Loss: 0.09335947938068014\n",
      "Iteration: 3012 lambda_n: 0.9422554531500665 Loss: 0.09333482252711403\n",
      "Iteration: 3013 lambda_n: 0.9850649356868143 Loss: 0.09330987613573043\n",
      "Iteration: 3014 lambda_n: 0.9661813579602281 Loss: 0.09328380528534919\n",
      "Iteration: 3015 lambda_n: 0.9494357296563893 Loss: 0.09325824318571832\n",
      "Iteration: 3016 lambda_n: 0.8997858834079089 Loss: 0.09323313277536446\n",
      "Iteration: 3017 lambda_n: 0.9863480767187017 Loss: 0.09320934345930307\n",
      "Iteration: 3018 lambda_n: 1.0040337317721393 Loss: 0.09318327419577853\n",
      "Iteration: 3019 lambda_n: 0.906594477641365 Loss: 0.09315674693284265\n",
      "Iteration: 3020 lambda_n: 0.8931126756003438 Loss: 0.09313280244672098\n",
      "Iteration: 3021 lambda_n: 0.8759260909169704 Loss: 0.09310922166567441\n",
      "Iteration: 3022 lambda_n: 0.9279693740349264 Loss: 0.09308610202125017\n",
      "Iteration: 3023 lambda_n: 1.01722604964466 Loss: 0.0930616165507045\n",
      "Iteration: 3024 lambda_n: 1.0240064283283907 Loss: 0.09303478513913573\n",
      "Iteration: 3025 lambda_n: 0.9749817473889595 Loss: 0.09300778475149142\n",
      "Iteration: 3026 lambda_n: 1.0073833249574922 Loss: 0.09298208631630511\n",
      "Iteration: 3027 lambda_n: 0.999755631302587 Loss: 0.09295554323448145\n",
      "Iteration: 3028 lambda_n: 0.8934849062004574 Loss: 0.09292921060043632\n",
      "Iteration: 3029 lambda_n: 0.9861387400772523 Loss: 0.09290568520614019\n",
      "Iteration: 3030 lambda_n: 0.8881930443409165 Loss: 0.09287972882612619\n",
      "Iteration: 3031 lambda_n: 1.01193574346897 Loss: 0.09285635851053901\n",
      "Iteration: 3032 lambda_n: 0.9504937573250759 Loss: 0.09282974109499653\n",
      "Iteration: 3033 lambda_n: 1.0092926461567153 Loss: 0.09280474871869022\n",
      "Iteration: 3034 lambda_n: 0.9367966260131451 Loss: 0.09277821949578492\n",
      "Iteration: 3035 lambda_n: 0.9683686392122596 Loss: 0.09275360454288108\n",
      "Iteration: 3036 lambda_n: 0.9794425192264185 Loss: 0.09272816865061824\n",
      "Iteration: 3037 lambda_n: 0.9933582245352816 Loss: 0.09270245084885999\n",
      "Iteration: 3038 lambda_n: 0.9796781815535067 Loss: 0.0926763768557136\n",
      "Iteration: 3039 lambda_n: 0.9291580026433743 Loss: 0.09265067106252645\n",
      "Iteration: 3040 lambda_n: 0.9030162695731399 Loss: 0.09262629929864839\n",
      "Iteration: 3041 lambda_n: 0.9971596273080544 Loss: 0.09260262105532885\n",
      "Iteration: 3042 lambda_n: 0.9801843470880022 Loss: 0.09257648299304055\n",
      "Iteration: 3043 lambda_n: 1.0187945827461176 Loss: 0.09255079903602433\n",
      "Iteration: 3044 lambda_n: 0.9872873886954884 Loss: 0.09252411286692569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3045 lambda_n: 0.9269609196297333 Loss: 0.09249826135272246\n",
      "Iteration: 3046 lambda_n: 0.8758090991921648 Loss: 0.09247399788296483\n",
      "Iteration: 3047 lambda_n: 0.9474378634894053 Loss: 0.09245108081903655\n",
      "Iteration: 3048 lambda_n: 0.9089503586037703 Loss: 0.09242629744959019\n",
      "Iteration: 3049 lambda_n: 1.0234775371551346 Loss: 0.09240252882710018\n",
      "Iteration: 3050 lambda_n: 1.0218764788440053 Loss: 0.09237577443986046\n",
      "Iteration: 3051 lambda_n: 0.9677389762301011 Loss: 0.0923490717082775\n",
      "Iteration: 3052 lambda_n: 0.9702922013766289 Loss: 0.09232379276612115\n",
      "Iteration: 3053 lambda_n: 0.8837722169629855 Loss: 0.09229845593618444\n",
      "Iteration: 3054 lambda_n: 0.9556810361224432 Loss: 0.09227538617734411\n",
      "Iteration: 3055 lambda_n: 1.013757123562802 Loss: 0.09225044743904631\n",
      "Iteration: 3056 lambda_n: 0.8808819156450686 Loss: 0.09222400243229929\n",
      "Iteration: 3057 lambda_n: 0.964014066891939 Loss: 0.09220103164449527\n",
      "Iteration: 3058 lambda_n: 1.002188099997634 Loss: 0.09217590119493678\n",
      "Iteration: 3059 lambda_n: 0.8901309034002833 Loss: 0.09214978475175889\n",
      "Iteration: 3060 lambda_n: 0.9974165720705973 Loss: 0.09212659650634325\n",
      "Iteration: 3061 lambda_n: 0.8756884982561366 Loss: 0.09210062203901752\n",
      "Iteration: 3062 lambda_n: 0.9263512355252524 Loss: 0.09207782543560665\n",
      "Iteration: 3063 lambda_n: 1.0037792288943375 Loss: 0.092053717655043\n",
      "Iteration: 3064 lambda_n: 0.8801154732190842 Loss: 0.09202760376371029\n",
      "Iteration: 3065 lambda_n: 0.8848435154553677 Loss: 0.09200471498784932\n",
      "Iteration: 3066 lambda_n: 0.9889328614275811 Loss: 0.09198171053265145\n",
      "Iteration: 3067 lambda_n: 1.0014197143304278 Loss: 0.091956008392242\n",
      "Iteration: 3068 lambda_n: 0.9677295373566888 Loss: 0.09192999099577598\n",
      "Iteration: 3069 lambda_n: 0.9295183118786566 Loss: 0.09190485782969143\n",
      "Iteration: 3070 lambda_n: 0.9386025358563685 Loss: 0.09188072533998802\n",
      "Iteration: 3071 lambda_n: 0.8821161270156915 Loss: 0.09185636515774717\n",
      "Iteration: 3072 lambda_n: 0.9456479677518052 Loss: 0.09183347857402636\n",
      "Iteration: 3073 lambda_n: 0.9030302948087681 Loss: 0.091808951594684\n",
      "Iteration: 3074 lambda_n: 0.969512844304857 Loss: 0.09178553781601102\n",
      "Iteration: 3075 lambda_n: 0.9124566177380448 Loss: 0.09176040861378235\n",
      "Iteration: 3076 lambda_n: 0.9375496582565714 Loss: 0.09173676635598159\n",
      "Iteration: 3077 lambda_n: 0.9995397755563641 Loss: 0.09171248195004691\n",
      "Iteration: 3078 lambda_n: 0.922688446930391 Loss: 0.09168660077164226\n",
      "Iteration: 3079 lambda_n: 1.0127817203375273 Loss: 0.09166271788014906\n",
      "Iteration: 3080 lambda_n: 1.0050207853703559 Loss: 0.09163651196249392\n",
      "Iteration: 3081 lambda_n: 1.0199231310488437 Loss: 0.09161051629514742\n",
      "Iteration: 3082 lambda_n: 0.946992044167532 Loss: 0.09158414473183484\n",
      "Iteration: 3083 lambda_n: 0.9672936144547712 Loss: 0.09155966767298777\n",
      "Iteration: 3084 lambda_n: 0.9204061967732677 Loss: 0.09153467443599764\n",
      "Iteration: 3085 lambda_n: 0.9433358191143817 Loss: 0.09151090082692354\n",
      "Iteration: 3086 lambda_n: 1.0141265972176239 Loss: 0.09148654307510289\n",
      "Iteration: 3087 lambda_n: 0.8908589310132486 Loss: 0.09146036652461027\n",
      "Iteration: 3088 lambda_n: 0.8935082968396117 Loss: 0.0914373798052954\n",
      "Iteration: 3089 lambda_n: 0.966219377884115 Loss: 0.09141433210896228\n",
      "Iteration: 3090 lambda_n: 1.0273598470443572 Loss: 0.09138941706232336\n",
      "Iteration: 3091 lambda_n: 1.0250699739215021 Loss: 0.09136293481797465\n",
      "Iteration: 3092 lambda_n: 0.878735632544634 Loss: 0.09133652134479454\n",
      "Iteration: 3093 lambda_n: 1.0216085862945357 Loss: 0.09131388650361012\n",
      "Iteration: 3094 lambda_n: 0.9360363272084988 Loss: 0.09128758021321298\n",
      "Iteration: 3095 lambda_n: 0.968318308062455 Loss: 0.09126348601135807\n",
      "Iteration: 3096 lambda_n: 0.9918225710152966 Loss: 0.0912385693233563\n",
      "Iteration: 3097 lambda_n: 0.9223310987143151 Loss: 0.09121305677309373\n",
      "Iteration: 3098 lambda_n: 0.9028215776816136 Loss: 0.09118934001371144\n",
      "Iteration: 3099 lambda_n: 0.8788055136215076 Loss: 0.09116613256380984\n",
      "Iteration: 3100 lambda_n: 0.88180059858621 Loss: 0.09114354972563057\n",
      "Iteration: 3101 lambda_n: 0.9048197192053273 Loss: 0.09112089708619135\n",
      "Iteration: 3102 lambda_n: 0.9655696533752367 Loss: 0.09109766053367697\n",
      "Iteration: 3103 lambda_n: 0.9474815817775065 Loss: 0.09107287210480133\n",
      "Iteration: 3104 lambda_n: 0.9181679354896365 Loss: 0.09104855643062854\n",
      "Iteration: 3105 lambda_n: 1.0275555615067853 Loss: 0.09102500099296222\n",
      "Iteration: 3106 lambda_n: 0.9705265842139397 Loss: 0.09099864826247152\n",
      "Iteration: 3107 lambda_n: 0.9742642769804329 Loss: 0.09097376713208072\n",
      "Iteration: 3108 lambda_n: 0.8842437346054599 Loss: 0.09094879890430464\n",
      "Iteration: 3109 lambda_n: 1.0175061782839483 Loss: 0.09092614540826008\n",
      "Iteration: 3110 lambda_n: 0.9555632140738952 Loss: 0.09090008653501236\n",
      "Iteration: 3111 lambda_n: 0.958556735414449 Loss: 0.09087562283441819\n",
      "Iteration: 3112 lambda_n: 0.9800530398794004 Loss: 0.09085109093550466\n",
      "Iteration: 3113 lambda_n: 0.9243497020801279 Loss: 0.09082601759800628\n",
      "Iteration: 3114 lambda_n: 0.9456949965887317 Loss: 0.09080237753948495\n",
      "Iteration: 3115 lambda_n: 0.9147118825866701 Loss: 0.09077819967757605\n",
      "Iteration: 3116 lambda_n: 0.9816270284455024 Loss: 0.09075482180808829\n",
      "Iteration: 3117 lambda_n: 0.9186968916211058 Loss: 0.09072974219045331\n",
      "Iteration: 3118 lambda_n: 0.9547259695101193 Loss: 0.09070627849508175\n",
      "Iteration: 3119 lambda_n: 0.9705883235619538 Loss: 0.09068190276762446\n",
      "Iteration: 3120 lambda_n: 1.0120474765893448 Loss: 0.0906571306030382\n",
      "Iteration: 3121 lambda_n: 1.0279189395302233 Loss: 0.09063130942601177\n",
      "Iteration: 3122 lambda_n: 0.8870731183829601 Loss: 0.09060509289810445\n",
      "Iteration: 3123 lambda_n: 1.000228696124428 Loss: 0.09058247657335043\n",
      "Iteration: 3124 lambda_n: 0.9431579816446509 Loss: 0.09055698376566125\n",
      "Iteration: 3125 lambda_n: 0.9537272614213844 Loss: 0.09053295401071183\n",
      "Iteration: 3126 lambda_n: 0.9134224045051551 Loss: 0.09050866324911873\n",
      "Iteration: 3127 lambda_n: 0.9345112212683513 Loss: 0.09048540689907256\n",
      "Iteration: 3128 lambda_n: 0.95356700132322 Loss: 0.09046162149189468\n",
      "Iteration: 3129 lambda_n: 0.876164897959554 Loss: 0.09043735928754174\n",
      "Iteration: 3130 lambda_n: 0.9284253798936302 Loss: 0.09041507392620296\n",
      "Iteration: 3131 lambda_n: 0.9971970451874195 Loss: 0.09039146690110582\n",
      "Iteration: 3132 lambda_n: 0.986687181724412 Loss: 0.09036611989528995\n",
      "Iteration: 3133 lambda_n: 0.9783046125122871 Loss: 0.09034104900373881\n",
      "Iteration: 3134 lambda_n: 0.9151804699798958 Loss: 0.09031619991126678\n",
      "Iteration: 3135 lambda_n: 0.9184758727040181 Loss: 0.09029296220022742\n",
      "Iteration: 3136 lambda_n: 0.9947321505742981 Loss: 0.09026964850688862\n",
      "Iteration: 3137 lambda_n: 0.9688716977569299 Loss: 0.09024440776941668\n",
      "Iteration: 3138 lambda_n: 0.8973243656975236 Loss: 0.09021983195758487\n",
      "Iteration: 3139 lambda_n: 0.9063102132881088 Loss: 0.0901970787251846\n",
      "Iteration: 3140 lambda_n: 1.0022432006097006 Loss: 0.09017410509095274\n",
      "Iteration: 3141 lambda_n: 0.9877429117514349 Loss: 0.09014870826615756\n",
      "Iteration: 3142 lambda_n: 0.9911990712006675 Loss: 0.0901236878723111\n",
      "Iteration: 3143 lambda_n: 1.0192983966217677 Loss: 0.09009858887299603\n",
      "Iteration: 3144 lambda_n: 1.0279376202148371 Loss: 0.09007278764489159\n",
      "Iteration: 3145 lambda_n: 0.9381357432938326 Loss: 0.09004677731378903\n",
      "Iteration: 3146 lambda_n: 0.9182372616270686 Loss: 0.09002304782092149\n",
      "Iteration: 3147 lambda_n: 1.0014676442295511 Loss: 0.089999829442618\n",
      "Iteration: 3148 lambda_n: 0.9201147726536684 Loss: 0.0899745151367391\n",
      "Iteration: 3149 lambda_n: 0.9681956710585743 Loss: 0.0899512653778586\n",
      "Iteration: 3150 lambda_n: 0.888862946894986 Loss: 0.08992680894202276\n",
      "Iteration: 3151 lambda_n: 0.9666159178529078 Loss: 0.0899043640628657\n",
      "Iteration: 3152 lambda_n: 0.8820829405383115 Loss: 0.0898799638636149\n",
      "Iteration: 3153 lambda_n: 0.9702140330601359 Loss: 0.0898577050616037\n",
      "Iteration: 3154 lambda_n: 0.9059190718035464 Loss: 0.08983323036080794\n",
      "Iteration: 3155 lambda_n: 0.9939601395070247 Loss: 0.08981038539118351\n",
      "Iteration: 3156 lambda_n: 0.9092268339957652 Loss: 0.08978532869091956\n",
      "Iteration: 3157 lambda_n: 0.8797359498091009 Loss: 0.08976241601664045\n",
      "Iteration: 3158 lambda_n: 0.9290380984997597 Loss: 0.0897402537086838\n",
      "Iteration: 3159 lambda_n: 1.010978176898651 Loss: 0.08971685693791913\n",
      "Iteration: 3160 lambda_n: 1.0125898757080207 Loss: 0.08969140536963854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3161 lambda_n: 1.0190218599218195 Loss: 0.08966592252418203\n",
      "Iteration: 3162 lambda_n: 1.0229886211231818 Loss: 0.08964028719460498\n",
      "Iteration: 3163 lambda_n: 0.9273524269310851 Loss: 0.08961456154337746\n",
      "Iteration: 3164 lambda_n: 0.9083378132697263 Loss: 0.08959124925248997\n",
      "Iteration: 3165 lambda_n: 0.903652495992761 Loss: 0.08956842254845705\n",
      "Iteration: 3166 lambda_n: 0.9791373741170942 Loss: 0.08954572101542181\n",
      "Iteration: 3167 lambda_n: 1.0228566280579487 Loss: 0.08952113138778524\n",
      "Iteration: 3168 lambda_n: 0.9865166945256991 Loss: 0.08949545301844421\n",
      "Iteration: 3169 lambda_n: 0.949563509505914 Loss: 0.08947069598203611\n",
      "Iteration: 3170 lambda_n: 0.9318480478748349 Loss: 0.08944687468038108\n",
      "Iteration: 3171 lambda_n: 0.8809974604750989 Loss: 0.08942350576071603\n",
      "Iteration: 3172 lambda_n: 1.0124617272177394 Loss: 0.08940141937244205\n",
      "Iteration: 3173 lambda_n: 0.9032944312599147 Loss: 0.08937604566705211\n",
      "Iteration: 3174 lambda_n: 0.9003477022151914 Loss: 0.08935341583047102\n",
      "Iteration: 3175 lambda_n: 0.9352800505123632 Loss: 0.0893308671634255\n",
      "Iteration: 3176 lambda_n: 0.9616909833893513 Loss: 0.08930745134499732\n",
      "Iteration: 3177 lambda_n: 0.9029875218217526 Loss: 0.08928338250195303\n",
      "Iteration: 3178 lambda_n: 0.9710660035290871 Loss: 0.08926079055868816\n",
      "Iteration: 3179 lambda_n: 0.9353496307055174 Loss: 0.08923650346691196\n",
      "Iteration: 3180 lambda_n: 0.932053983544743 Loss: 0.08921311777349779\n",
      "Iteration: 3181 lambda_n: 0.8758321241327276 Loss: 0.08918982234025298\n",
      "Iteration: 3182 lambda_n: 0.882723096953812 Loss: 0.08916793932078572\n",
      "Iteration: 3183 lambda_n: 0.9926426847167082 Loss: 0.08914589112192108\n",
      "Iteration: 3184 lambda_n: 1.0013097922323708 Loss: 0.08912110563332548\n",
      "Iteration: 3185 lambda_n: 0.9918123905905921 Loss: 0.08909611272440912\n",
      "Iteration: 3186 lambda_n: 0.9210223533759891 Loss: 0.08907136580011732\n",
      "Iteration: 3187 lambda_n: 0.9748026972801338 Loss: 0.08904839321698771\n",
      "Iteration: 3188 lambda_n: 0.9842715668435149 Loss: 0.08902408746345888\n",
      "Iteration: 3189 lambda_n: 0.9808191202673822 Loss: 0.08899955428497495\n",
      "Iteration: 3190 lambda_n: 0.989300293325995 Loss: 0.08897511584533763\n",
      "Iteration: 3191 lambda_n: 0.9313223986368334 Loss: 0.08895047484823995\n",
      "Iteration: 3192 lambda_n: 0.9131521217173698 Loss: 0.08892728607202499\n",
      "Iteration: 3193 lambda_n: 0.9176721557248424 Loss: 0.08890455731879007\n",
      "Iteration: 3194 lambda_n: 0.9170099993368922 Loss: 0.08888172361219542\n",
      "Iteration: 3195 lambda_n: 0.8957346559871227 Loss: 0.08885891394981639\n",
      "Iteration: 3196 lambda_n: 0.8848218280382707 Loss: 0.08883664082071482\n",
      "Iteration: 3197 lambda_n: 0.8986952265066341 Loss: 0.08881464614535738\n",
      "Iteration: 3198 lambda_n: 0.8825128549077695 Loss: 0.0887923137932182\n",
      "Iteration: 3199 lambda_n: 1.0148178085064214 Loss: 0.08877039065481268\n",
      "Iteration: 3200 lambda_n: 0.8951350290127406 Loss: 0.08874518927423991\n",
      "Iteration: 3201 lambda_n: 1.0202552445587763 Loss: 0.08872296787021107\n",
      "Iteration: 3202 lambda_n: 0.8951144915002234 Loss: 0.08869764897799239\n",
      "Iteration: 3203 lambda_n: 1.023826995288642 Loss: 0.0886754434786207\n",
      "Iteration: 3204 lambda_n: 0.8971659876933219 Loss: 0.08865005356403337\n",
      "Iteration: 3205 lambda_n: 0.9432725376874728 Loss: 0.08862781262863637\n",
      "Iteration: 3206 lambda_n: 0.9703458519736944 Loss: 0.08860443642187722\n",
      "Iteration: 3207 lambda_n: 0.9371621002637617 Loss: 0.08858039757011739\n",
      "Iteration: 3208 lambda_n: 0.9888924906299564 Loss: 0.0885571888586856\n",
      "Iteration: 3209 lambda_n: 0.9854326883197554 Loss: 0.08853270750838463\n",
      "Iteration: 3210 lambda_n: 0.8880783716860161 Loss: 0.08850832053399088\n",
      "Iteration: 3211 lambda_n: 0.9646142099381283 Loss: 0.08848635042103585\n",
      "Iteration: 3212 lambda_n: 0.9581960821122048 Loss: 0.08846249478364453\n",
      "Iteration: 3213 lambda_n: 0.9672549753308505 Loss: 0.08843880613118678\n",
      "Iteration: 3214 lambda_n: 0.919076816266865 Loss: 0.08841490184774797\n",
      "Iteration: 3215 lambda_n: 1.0110825979978708 Loss: 0.08839219604421106\n",
      "Iteration: 3216 lambda_n: 1.0120036415471578 Loss: 0.08836722582633035\n",
      "Iteration: 3217 lambda_n: 0.9643154275603073 Loss: 0.08834224201957136\n",
      "Iteration: 3218 lambda_n: 0.8809976128180675 Loss: 0.08831844410809225\n",
      "Iteration: 3219 lambda_n: 1.0096712061539774 Loss: 0.08829670974134175\n",
      "Iteration: 3220 lambda_n: 0.8853921840094037 Loss: 0.08827180930888182\n",
      "Iteration: 3221 lambda_n: 0.9768827317061587 Loss: 0.0882499815029315\n",
      "Iteration: 3222 lambda_n: 0.9320242209517406 Loss: 0.08822590614836079\n",
      "Iteration: 3223 lambda_n: 0.882894622397894 Loss: 0.08820294434680931\n",
      "Iteration: 3224 lambda_n: 0.948967600179943 Loss: 0.08818120014820936\n",
      "Iteration: 3225 lambda_n: 0.9993163124534493 Loss: 0.08815783635032888\n",
      "Iteration: 3226 lambda_n: 0.9268686159942674 Loss: 0.08813324157026485\n",
      "Iteration: 3227 lambda_n: 0.9026261113056865 Loss: 0.08811043791421393\n",
      "Iteration: 3228 lambda_n: 0.9569973098019489 Loss: 0.08808823809528221\n",
      "Iteration: 3229 lambda_n: 0.9323500547817705 Loss: 0.08806470889267365\n",
      "Iteration: 3230 lambda_n: 1.000180383540263 Loss: 0.08804179357039385\n",
      "Iteration: 3231 lambda_n: 0.9730044237879814 Loss: 0.0880172196281151\n",
      "Iteration: 3232 lambda_n: 0.9441232276026708 Loss: 0.08799332198436616\n",
      "Iteration: 3233 lambda_n: 0.8845794992279953 Loss: 0.08797014179000529\n",
      "Iteration: 3234 lambda_n: 0.9282643200869174 Loss: 0.08794843080539028\n",
      "Iteration: 3235 lambda_n: 0.9072544701263558 Loss: 0.08792565506210762\n",
      "Iteration: 3236 lambda_n: 1.015842280634191 Loss: 0.08790340225835476\n",
      "Iteration: 3237 lambda_n: 0.9853922543391055 Loss: 0.08787849458512192\n",
      "Iteration: 3238 lambda_n: 0.930408341686045 Loss: 0.08785434234703726\n",
      "Iteration: 3239 lambda_n: 0.9725815835090437 Loss: 0.08783154578989637\n",
      "Iteration: 3240 lambda_n: 0.968419778692296 Loss: 0.08780772409161947\n",
      "Iteration: 3241 lambda_n: 0.8928905318134157 Loss: 0.08778401269756601\n",
      "Iteration: 3242 lambda_n: 0.9575705975169715 Loss: 0.08776215810134483\n",
      "Iteration: 3243 lambda_n: 1.001117678185573 Loss: 0.08773872816704484\n",
      "Iteration: 3244 lambda_n: 0.9196890443524439 Loss: 0.08771424137044075\n",
      "Iteration: 3245 lambda_n: 0.9170549625171189 Loss: 0.08769175423766554\n",
      "Iteration: 3246 lambda_n: 0.9067823702621157 Loss: 0.08766933899859689\n",
      "Iteration: 3247 lambda_n: 0.8998797871241003 Loss: 0.08764718220991287\n",
      "Iteration: 3248 lambda_n: 1.0055446346066792 Loss: 0.08762520131291279\n",
      "Iteration: 3249 lambda_n: 0.8906435261082449 Loss: 0.08760064774570255\n",
      "Iteration: 3250 lambda_n: 1.0143171626330085 Loss: 0.08757890750447837\n",
      "Iteration: 3251 lambda_n: 0.9191320551948425 Loss: 0.08755415683120046\n",
      "Iteration: 3252 lambda_n: 0.9876515584771005 Loss: 0.08753173681628135\n",
      "Iteration: 3253 lambda_n: 1.0222548779422505 Loss: 0.08750765368831095\n",
      "Iteration: 3254 lambda_n: 0.9920010816020189 Loss: 0.08748273584682112\n",
      "Iteration: 3255 lambda_n: 0.9033675953288535 Loss: 0.08745856435906732\n",
      "Iteration: 3256 lambda_n: 0.9905337220915386 Loss: 0.0874365602599597\n",
      "Iteration: 3257 lambda_n: 0.988247956515334 Loss: 0.08741244116939743\n",
      "Iteration: 3258 lambda_n: 0.9221918873262557 Loss: 0.0873883864067817\n",
      "Iteration: 3259 lambda_n: 0.9186153620197072 Loss: 0.08736594739943418\n",
      "Iteration: 3260 lambda_n: 0.9944504061513795 Loss: 0.087343602913234\n",
      "Iteration: 3261 lambda_n: 0.9782183524185264 Loss: 0.08731942212383365\n",
      "Iteration: 3262 lambda_n: 0.9714053920395397 Loss: 0.08729564459757501\n",
      "Iteration: 3263 lambda_n: 0.9082531450729501 Loss: 0.08727204106853576\n",
      "Iteration: 3264 lambda_n: 1.0153856727852506 Loss: 0.08724997967477548\n",
      "Iteration: 3265 lambda_n: 0.9185571894557581 Loss: 0.08722532452556106\n",
      "Iteration: 3266 lambda_n: 1.007772574532404 Loss: 0.08720302851360859\n",
      "Iteration: 3267 lambda_n: 0.9924674896856637 Loss: 0.08717857544529362\n",
      "Iteration: 3268 lambda_n: 0.9297629087561469 Loss: 0.08715450254928161\n",
      "Iteration: 3269 lambda_n: 0.9395881497836591 Loss: 0.08713195857878343\n",
      "Iteration: 3270 lambda_n: 0.969905736103955 Loss: 0.08710918412732982\n",
      "Iteration: 3271 lambda_n: 0.9032484890374433 Loss: 0.08708568295868915\n",
      "Iteration: 3272 lambda_n: 0.9157252873020341 Loss: 0.08706380448421769\n",
      "Iteration: 3273 lambda_n: 0.9443235181862113 Loss: 0.0870416311399706\n",
      "Iteration: 3274 lambda_n: 0.9656171071530365 Loss: 0.08701877303930519\n",
      "Iteration: 3275 lambda_n: 1.0158658188146281 Loss: 0.08699540762637636\n",
      "Iteration: 3276 lambda_n: 0.9295333358952333 Loss: 0.08697083514083644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3277 lambda_n: 0.9597476386266482 Loss: 0.08694835902704198\n",
      "Iteration: 3278 lambda_n: 1.0010136523749724 Loss: 0.08692516029349714\n",
      "Iteration: 3279 lambda_n: 0.9879428767427498 Loss: 0.08690097269286931\n",
      "Iteration: 3280 lambda_n: 0.9804825545009247 Loss: 0.08687710961206255\n",
      "Iteration: 3281 lambda_n: 1.0060161356867658 Loss: 0.08685343525537953\n",
      "Iteration: 3282 lambda_n: 0.9300346349713098 Loss: 0.08682915315245317\n",
      "Iteration: 3283 lambda_n: 1.0142313547561979 Loss: 0.08680671304919164\n",
      "Iteration: 3284 lambda_n: 0.9360957073337463 Loss: 0.0867822499985599\n",
      "Iteration: 3285 lambda_n: 0.9540074741775671 Loss: 0.08675967971701569\n",
      "Iteration: 3286 lambda_n: 0.8885327012254309 Loss: 0.08673668548628204\n",
      "Iteration: 3287 lambda_n: 0.9494235478365581 Loss: 0.08671527667737024\n",
      "Iteration: 3288 lambda_n: 0.8900268672161963 Loss: 0.08669240833542137\n",
      "Iteration: 3289 lambda_n: 0.9391384420737066 Loss: 0.08667097794248041\n",
      "Iteration: 3290 lambda_n: 1.022394257832217 Loss: 0.08664837252308291\n",
      "Iteration: 3291 lambda_n: 0.9776671104557393 Loss: 0.08662377181433667\n",
      "Iteration: 3292 lambda_n: 1.0050705625260532 Loss: 0.08660025599591441\n",
      "Iteration: 3293 lambda_n: 0.9749368725992509 Loss: 0.08657608977572252\n",
      "Iteration: 3294 lambda_n: 1.0112431164572606 Loss: 0.08655265663718466\n",
      "Iteration: 3295 lambda_n: 0.9549078494499249 Loss: 0.08652835964459914\n",
      "Iteration: 3296 lambda_n: 0.9334312085548772 Loss: 0.08650542454836875\n",
      "Iteration: 3297 lambda_n: 0.9561071408543514 Loss: 0.0864830130630094\n",
      "Iteration: 3298 lambda_n: 0.9357800983376818 Loss: 0.0864600650483907\n",
      "Iteration: 3299 lambda_n: 0.906999609328916 Loss: 0.08643761272573568\n",
      "Iteration: 3300 lambda_n: 0.9329210861435469 Loss: 0.08641585832369848\n",
      "Iteration: 3301 lambda_n: 0.9384779589942178 Loss: 0.08639348970618689\n",
      "Iteration: 3302 lambda_n: 0.9267917620249246 Loss: 0.0863709955638182\n",
      "Iteration: 3303 lambda_n: 1.007911355550937 Loss: 0.086348789137432\n",
      "Iteration: 3304 lambda_n: 0.9204188114922701 Loss: 0.086324647495253\n",
      "Iteration: 3305 lambda_n: 0.8888613821978943 Loss: 0.08630260939855776\n",
      "Iteration: 3306 lambda_n: 0.9267153939944377 Loss: 0.08628133400246318\n",
      "Iteration: 3307 lambda_n: 0.9775779776715644 Loss: 0.08625915988950779\n",
      "Iteration: 3308 lambda_n: 0.998204001434231 Loss: 0.08623577685991626\n",
      "Iteration: 3309 lambda_n: 1.0230006131247835 Loss: 0.08621190909981288\n",
      "Iteration: 3310 lambda_n: 0.8791979041484441 Loss: 0.08618745747693227\n",
      "Iteration: 3311 lambda_n: 0.9686180260576631 Loss: 0.08616645052482105\n",
      "Iteration: 3312 lambda_n: 0.9433240672181106 Loss: 0.08614331476318861\n",
      "Iteration: 3313 lambda_n: 0.888516548191201 Loss: 0.08612079110358692\n",
      "Iteration: 3314 lambda_n: 0.9673942673681383 Loss: 0.08609958328683447\n",
      "Iteration: 3315 lambda_n: 1.0018909619636527 Loss: 0.0860765005236107\n",
      "Iteration: 3316 lambda_n: 0.9132005388352901 Loss: 0.0860526032487723\n",
      "Iteration: 3317 lambda_n: 0.9223073304955517 Loss: 0.08603082921553205\n",
      "Iteration: 3318 lambda_n: 1.0018607528673544 Loss: 0.08600884545455949\n",
      "Iteration: 3319 lambda_n: 1.0037157887564117 Loss: 0.08598497383001921\n",
      "Iteration: 3320 lambda_n: 0.9618970715218412 Loss: 0.08596106682702549\n",
      "Iteration: 3321 lambda_n: 0.9156827707120822 Loss: 0.08593816422608384\n",
      "Iteration: 3322 lambda_n: 1.0142441199790144 Loss: 0.0859163695727918\n",
      "Iteration: 3323 lambda_n: 0.9405652794161923 Loss: 0.08589223744717837\n",
      "Iteration: 3324 lambda_n: 1.0194267394207024 Loss: 0.08586986652934281\n",
      "Iteration: 3325 lambda_n: 1.0142912199556358 Loss: 0.08584562856953168\n",
      "Iteration: 3326 lambda_n: 0.9784998488222678 Loss: 0.08582152175485497\n",
      "Iteration: 3327 lambda_n: 0.9529488184660033 Loss: 0.08579827418749925\n",
      "Iteration: 3328 lambda_n: 0.9091388836219527 Loss: 0.08577564176458258\n",
      "Iteration: 3329 lambda_n: 0.991499161955044 Loss: 0.08575405728954644\n",
      "Iteration: 3330 lambda_n: 0.9640845556192869 Loss: 0.08573052558195263\n",
      "Iteration: 3331 lambda_n: 0.9939109429491342 Loss: 0.08570765280722775\n",
      "Iteration: 3332 lambda_n: 0.9756521061427214 Loss: 0.0856840808827462\n",
      "Iteration: 3333 lambda_n: 0.9081825483346037 Loss: 0.08566095042517002\n",
      "Iteration: 3334 lambda_n: 0.8891855424045979 Loss: 0.08563942708536017\n",
      "Iteration: 3335 lambda_n: 0.9362372924180913 Loss: 0.08561836097807554\n",
      "Iteration: 3336 lambda_n: 1.0007762534658484 Loss: 0.08559618756263897\n",
      "Iteration: 3337 lambda_n: 0.9151498055145144 Loss: 0.08557249402796771\n",
      "Iteration: 3338 lambda_n: 0.9227987077177859 Loss: 0.08555083548897713\n",
      "Iteration: 3339 lambda_n: 0.9547317241425951 Loss: 0.08552900333419006\n",
      "Iteration: 3340 lambda_n: 0.9568916224845965 Loss: 0.08550642348546907\n",
      "Iteration: 3341 lambda_n: 0.9391935328076602 Loss: 0.08548380055000554\n",
      "Iteration: 3342 lambda_n: 0.8755127459397245 Loss: 0.08546160384339195\n",
      "Iteration: 3343 lambda_n: 1.022246502483318 Loss: 0.08544091917694006\n",
      "Iteration: 3344 lambda_n: 0.9516919640325586 Loss: 0.08541677608921606\n",
      "Iteration: 3345 lambda_n: 0.9435192640337723 Loss: 0.08539430763818305\n",
      "Iteration: 3346 lambda_n: 1.0214056269268295 Loss: 0.08537203996194068\n",
      "Iteration: 3347 lambda_n: 0.9012657528397043 Loss: 0.08534794277992584\n",
      "Iteration: 3348 lambda_n: 0.9435047626546923 Loss: 0.08532668767748226\n",
      "Iteration: 3349 lambda_n: 0.9011252771684245 Loss: 0.08530444397816554\n",
      "Iteration: 3350 lambda_n: 0.8916942752513521 Loss: 0.08528320671500322\n",
      "Iteration: 3351 lambda_n: 0.910115876675347 Loss: 0.08526219871231622\n",
      "Iteration: 3352 lambda_n: 0.880776349519063 Loss: 0.08524076384427112\n",
      "Iteration: 3353 lambda_n: 0.8791625379395148 Loss: 0.0852200268997838\n",
      "Iteration: 3354 lambda_n: 0.9632397501945027 Loss: 0.08519933471083728\n",
      "Iteration: 3355 lambda_n: 0.9518971562213987 Loss: 0.08517667130208946\n",
      "Iteration: 3356 lambda_n: 0.9506546427341046 Loss: 0.08515428274156574\n",
      "Iteration: 3357 lambda_n: 0.8858500869508301 Loss: 0.0851319313048997\n",
      "Iteration: 3358 lambda_n: 0.8800277466483992 Loss: 0.0851111107093985\n",
      "Iteration: 3359 lambda_n: 0.9984010853658025 Loss: 0.08509043375161811\n",
      "Iteration: 3360 lambda_n: 0.91587834602331 Loss: 0.08506698354813923\n",
      "Iteration: 3361 lambda_n: 1.0233226064793846 Loss: 0.08504547937405604\n",
      "Iteration: 3362 lambda_n: 1.0196611515890492 Loss: 0.08502146100204744\n",
      "Iteration: 3363 lambda_n: 0.949940656580781 Loss: 0.08499753766693219\n",
      "Iteration: 3364 lambda_n: 0.877072702804148 Loss: 0.08497525837245595\n",
      "Iteration: 3365 lambda_n: 0.9906736686536695 Loss: 0.08495469515508564\n",
      "Iteration: 3366 lambda_n: 0.9744694261319056 Loss: 0.08493147645804491\n",
      "Iteration: 3367 lambda_n: 0.9022859254560471 Loss: 0.08490864592338322\n",
      "Iteration: 3368 lambda_n: 0.9626001655589532 Loss: 0.08488751402960862\n",
      "Iteration: 3369 lambda_n: 0.9095249639458818 Loss: 0.08486497731005108\n",
      "Iteration: 3370 lambda_n: 0.9901860067950518 Loss: 0.08484369069949933\n",
      "Iteration: 3371 lambda_n: 0.9240413432234474 Loss: 0.0848205243935456\n",
      "Iteration: 3372 lambda_n: 0.9355946945624622 Loss: 0.08479891340158922\n",
      "Iteration: 3373 lambda_n: 0.8837182679469476 Loss: 0.0847770397837555\n",
      "Iteration: 3374 lambda_n: 0.932191116822375 Loss: 0.08475638607986234\n",
      "Iteration: 3375 lambda_n: 0.8896009900000821 Loss: 0.08473460682603406\n",
      "Iteration: 3376 lambda_n: 1.0296079211101705 Loss: 0.0847138297479699\n",
      "Iteration: 3377 lambda_n: 0.9781388429032479 Loss: 0.08468979117069714\n",
      "Iteration: 3378 lambda_n: 0.9431682424080874 Loss: 0.0846669628926175\n",
      "Iteration: 3379 lambda_n: 0.94430433036714 Loss: 0.08464495872485074\n",
      "Iteration: 3380 lambda_n: 0.9670616347396096 Loss: 0.08462293582705753\n",
      "Iteration: 3381 lambda_n: 0.9522151342403856 Loss: 0.08460039022329512\n",
      "Iteration: 3382 lambda_n: 0.9384311543000702 Loss: 0.08457819873559728\n",
      "Iteration: 3383 lambda_n: 0.9130920875273629 Loss: 0.084556336242894\n",
      "Iteration: 3384 lambda_n: 0.9724766736865885 Loss: 0.08453507147672198\n",
      "Iteration: 3385 lambda_n: 0.9914082675616931 Loss: 0.0845124316433664\n",
      "Iteration: 3386 lambda_n: 0.8997776618126017 Loss: 0.08448935954172213\n",
      "Iteration: 3387 lambda_n: 1.0247558440894822 Loss: 0.08446842740312384\n",
      "Iteration: 3388 lambda_n: 0.9605182316444532 Loss: 0.08444459625109176\n",
      "Iteration: 3389 lambda_n: 1.012404267801105 Loss: 0.08442226737312047\n",
      "Iteration: 3390 lambda_n: 1.0261681481872136 Loss: 0.08439874096330464\n",
      "Iteration: 3391 lambda_n: 0.927385026698781 Loss: 0.08437490381583633\n",
      "Iteration: 3392 lambda_n: 0.9721205526685631 Loss: 0.08435336935245054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3393 lambda_n: 0.9524283449839149 Loss: 0.08433080410341878\n",
      "Iteration: 3394 lambda_n: 0.9005214180890665 Loss: 0.08430870398040162\n",
      "Iteration: 3395 lambda_n: 0.9837047494919591 Loss: 0.0842878156458719\n",
      "Iteration: 3396 lambda_n: 0.973482268429855 Loss: 0.08426500578916399\n",
      "Iteration: 3397 lambda_n: 0.9652195633614382 Loss: 0.0842424412960412\n",
      "Iteration: 3398 lambda_n: 0.8895737779756091 Loss: 0.08422007650245944\n",
      "Iteration: 3399 lambda_n: 0.965766121612939 Loss: 0.08419947176051543\n",
      "Iteration: 3400 lambda_n: 0.8762997224779981 Loss: 0.08417710994100737\n",
      "Iteration: 3401 lambda_n: 0.9672573481195634 Loss: 0.084156826816029\n",
      "Iteration: 3402 lambda_n: 0.9203651416721372 Loss: 0.08413444602724136\n",
      "Iteration: 3403 lambda_n: 0.9064525113342887 Loss: 0.08411315788819071\n",
      "Iteration: 3404 lambda_n: 0.9001837676592988 Loss: 0.08409219879491665\n",
      "Iteration: 3405 lambda_n: 0.9925182666244916 Loss: 0.0840713917556882\n",
      "Iteration: 3406 lambda_n: 0.8873708678565215 Loss: 0.0840484585630145\n",
      "Iteration: 3407 lambda_n: 0.9716422672967769 Loss: 0.08402796231562812\n",
      "Iteration: 3408 lambda_n: 0.9196640572352904 Loss: 0.08400552737484895\n",
      "Iteration: 3409 lambda_n: 0.9642630626626655 Loss: 0.08398430025281774\n",
      "Iteration: 3410 lambda_n: 0.9764010260898942 Loss: 0.0839620516037421\n",
      "Iteration: 3411 lambda_n: 0.9760016912767505 Loss: 0.08393953115475232\n",
      "Iteration: 3412 lambda_n: 0.9801575765801201 Loss: 0.08391702824117833\n",
      "Iteration: 3413 lambda_n: 0.9615103127411317 Loss: 0.08389443788017947\n",
      "Iteration: 3414 lambda_n: 0.9615999018314454 Loss: 0.08387228547529477\n",
      "Iteration: 3415 lambda_n: 0.9516150701036222 Loss: 0.08385013908718013\n",
      "Iteration: 3416 lambda_n: 0.9943751031263084 Loss: 0.08382823062612663\n",
      "Iteration: 3417 lambda_n: 0.9674820096818035 Loss: 0.08380534613091942\n",
      "Iteration: 3418 lambda_n: 1.026122444854382 Loss: 0.0837830888816277\n",
      "Iteration: 3419 lambda_n: 0.9806319390811403 Loss: 0.08375949145760951\n",
      "Iteration: 3420 lambda_n: 0.9132352316435602 Loss: 0.08373694882530137\n",
      "Iteration: 3421 lambda_n: 0.8778932998447468 Loss: 0.08371596313939741\n",
      "Iteration: 3422 lambda_n: 0.9055533092802475 Loss: 0.08369579651084884\n",
      "Iteration: 3423 lambda_n: 1.0048113808161376 Loss: 0.08367500151777478\n",
      "Iteration: 3424 lambda_n: 0.9361527275791135 Loss: 0.08365193545055621\n",
      "Iteration: 3425 lambda_n: 0.914982472881074 Loss: 0.083630453515656\n",
      "Iteration: 3426 lambda_n: 0.8825062626364631 Loss: 0.08360946481073114\n",
      "Iteration: 3427 lambda_n: 1.0065979549914295 Loss: 0.08358922805385953\n",
      "Iteration: 3428 lambda_n: 0.9821800351768489 Loss: 0.08356615391870852\n",
      "Iteration: 3429 lambda_n: 1.0163259390118453 Loss: 0.0835436480947225\n",
      "Iteration: 3430 lambda_n: 0.9138573072220527 Loss: 0.08352036869766466\n",
      "Iteration: 3431 lambda_n: 0.8962769018231307 Loss: 0.0834994442313421\n",
      "Iteration: 3432 lambda_n: 1.0294618232108967 Loss: 0.083478929427354\n",
      "Iteration: 3433 lambda_n: 0.8824060446993276 Loss: 0.08345537468207329\n",
      "Iteration: 3434 lambda_n: 0.9393915029152017 Loss: 0.08343519221980526\n",
      "Iteration: 3435 lambda_n: 0.9439481234023589 Loss: 0.08341371381650892\n",
      "Iteration: 3436 lambda_n: 0.9270407022114028 Loss: 0.08339213901802067\n",
      "Iteration: 3437 lambda_n: 0.8757840879375318 Loss: 0.08337095827851974\n",
      "Iteration: 3438 lambda_n: 0.9694589538657616 Loss: 0.08335095561358598\n",
      "Iteration: 3439 lambda_n: 0.9561796801164144 Loss: 0.0833288211763977\n",
      "Iteration: 3440 lambda_n: 1.0159647173071964 Loss: 0.08330699802215391\n",
      "Iteration: 3441 lambda_n: 0.9692541073087948 Loss: 0.08328381909611787\n",
      "Iteration: 3442 lambda_n: 0.8928139539894575 Loss: 0.0832617143600481\n",
      "Iteration: 3443 lambda_n: 0.9358027463947999 Loss: 0.0832413602947449\n",
      "Iteration: 3444 lambda_n: 0.9968486310021544 Loss: 0.083220033651644\n",
      "Iteration: 3445 lambda_n: 1.0084161509684062 Loss: 0.08319732418122554\n",
      "Iteration: 3446 lambda_n: 1.0298494609786895 Loss: 0.08317436005897307\n",
      "Iteration: 3447 lambda_n: 1.0256444813627528 Loss: 0.08315091704642308\n",
      "Iteration: 3448 lambda_n: 1.0166693628686765 Loss: 0.08312757902777\n",
      "Iteration: 3449 lambda_n: 0.9369334695772277 Loss: 0.0831044543755283\n",
      "Iteration: 3450 lambda_n: 0.8975286830587453 Loss: 0.08308315149802585\n",
      "Iteration: 3451 lambda_n: 0.8813578951306145 Loss: 0.08306275184693068\n",
      "Iteration: 3452 lambda_n: 0.92298109810212 Loss: 0.0830427266542132\n",
      "Iteration: 3453 lambda_n: 0.9866171876075713 Loss: 0.08302176303269493\n",
      "Iteration: 3454 lambda_n: 1.0177168353910075 Loss: 0.08299936226935752\n",
      "Iteration: 3455 lambda_n: 0.9449141487878931 Loss: 0.08297626434849148\n",
      "Iteration: 3456 lambda_n: 0.9519502351301498 Loss: 0.08295482699941269\n",
      "Iteration: 3457 lambda_n: 0.971603672393078 Loss: 0.08293323797373846\n",
      "Iteration: 3458 lambda_n: 0.938815304648336 Loss: 0.08291121145123705\n",
      "Iteration: 3459 lambda_n: 0.8874434629566694 Loss: 0.08288993619818764\n",
      "Iteration: 3460 lambda_n: 0.8999736375069987 Loss: 0.08286983232974152\n",
      "Iteration: 3461 lambda_n: 0.9285985442404094 Loss: 0.08284945169339118\n",
      "Iteration: 3462 lambda_n: 0.9448696346224569 Loss: 0.08282843028831709\n",
      "Iteration: 3463 lambda_n: 1.0068943955221992 Loss: 0.08280704834243281\n",
      "Iteration: 3464 lambda_n: 0.9113056828836102 Loss: 0.08278427141524093\n",
      "Iteration: 3465 lambda_n: 1.0003126206322097 Loss: 0.08276366462792546\n",
      "Iteration: 3466 lambda_n: 1.0081680708598693 Loss: 0.08274105353224602\n",
      "Iteration: 3467 lambda_n: 0.8901660110601428 Loss: 0.08271827382007846\n",
      "Iteration: 3468 lambda_n: 1.010692917600728 Loss: 0.08269816798543778\n",
      "Iteration: 3469 lambda_n: 0.9334064236146293 Loss: 0.08267534821782625\n",
      "Iteration: 3470 lambda_n: 0.8805426694063178 Loss: 0.08265428157355638\n",
      "Iteration: 3471 lambda_n: 0.9255464554803785 Loss: 0.08263441517735524\n",
      "Iteration: 3472 lambda_n: 0.887879026925845 Loss: 0.08261354078726793\n",
      "Iteration: 3473 lambda_n: 0.9066536612678833 Loss: 0.08259352311221056\n",
      "Iteration: 3474 lambda_n: 0.8804578806337924 Loss: 0.08257308935175184\n",
      "Iteration: 3475 lambda_n: 1.0275385121521798 Loss: 0.08255325299087614\n",
      "Iteration: 3476 lambda_n: 0.9490614794873622 Loss: 0.08253011149069127\n",
      "Iteration: 3477 lambda_n: 0.9392188325026337 Loss: 0.08250874581615904\n",
      "Iteration: 3478 lambda_n: 1.0026000380831208 Loss: 0.08248760961674882\n",
      "Iteration: 3479 lambda_n: 0.963313783857518 Loss: 0.08246505566923362\n",
      "Iteration: 3480 lambda_n: 1.0128980106100116 Loss: 0.08244339395192908\n",
      "Iteration: 3481 lambda_n: 0.9646326050273863 Loss: 0.08242062610407515\n",
      "Iteration: 3482 lambda_n: 1.0091147340926399 Loss: 0.0823989517106002\n",
      "Iteration: 3483 lambda_n: 0.9330663938462403 Loss: 0.08237628667018336\n",
      "Iteration: 3484 lambda_n: 0.9243700109774011 Loss: 0.08235533786015273\n",
      "Iteration: 3485 lambda_n: 1.0229707598422626 Loss: 0.08233459196669732\n",
      "Iteration: 3486 lambda_n: 0.891942355490667 Loss: 0.08231164192339267\n",
      "Iteration: 3487 lambda_n: 1.0094066525568826 Loss: 0.08229163922600873\n",
      "Iteration: 3488 lambda_n: 0.9709804439818722 Loss: 0.08226901071294285\n",
      "Iteration: 3489 lambda_n: 0.9324054259548128 Loss: 0.0822472522649717\n",
      "Iteration: 3490 lambda_n: 0.9041407131122019 Loss: 0.08222636621538018\n",
      "Iteration: 3491 lambda_n: 0.9292322864298217 Loss: 0.08220612076236093\n",
      "Iteration: 3492 lambda_n: 0.955850791061162 Loss: 0.0821853210637969\n",
      "Iteration: 3493 lambda_n: 0.9654714667620113 Loss: 0.08216393358895065\n",
      "Iteration: 3494 lambda_n: 0.924960532591584 Loss: 0.08214233915574343\n",
      "Iteration: 3495 lambda_n: 1.0030267866815326 Loss: 0.08212165870823579\n",
      "Iteration: 3496 lambda_n: 0.926298453773861 Loss: 0.08209924143318346\n",
      "Iteration: 3497 lambda_n: 0.9018571621782098 Loss: 0.08207854711851462\n",
      "Iteration: 3498 lambda_n: 1.0106312645237754 Loss: 0.08205840628109691\n",
      "Iteration: 3499 lambda_n: 0.8888771598036702 Loss: 0.0820358448023003\n",
      "Iteration: 3500 lambda_n: 0.9678052902481278 Loss: 0.08201600910429277\n",
      "Iteration: 3501 lambda_n: 0.9483111940615305 Loss: 0.08199442009707168\n",
      "Iteration: 3502 lambda_n: 0.9918835165923137 Loss: 0.08197327416075213\n",
      "Iteration: 3503 lambda_n: 0.9596504069232269 Loss: 0.08195116525897213\n",
      "Iteration: 3504 lambda_n: 0.8938581436077228 Loss: 0.08192978331764207\n",
      "Iteration: 3505 lambda_n: 0.9443397117797231 Loss: 0.08190987484375306\n",
      "Iteration: 3506 lambda_n: 0.8967487912106978 Loss: 0.08188884981262237\n",
      "Iteration: 3507 lambda_n: 0.943204396777493 Loss: 0.0818688918769292\n",
      "Iteration: 3508 lambda_n: 0.9176128847105512 Loss: 0.08184790783946759\n",
      "Iteration: 3509 lambda_n: 1.013070879428829 Loss: 0.08182750091594769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3510 lambda_n: 0.9886155835478214 Loss: 0.0818049798500182\n",
      "Iteration: 3511 lambda_n: 0.946936704045209 Loss: 0.08178301144691674\n",
      "Iteration: 3512 lambda_n: 0.9156348439331689 Loss: 0.08176197757679797\n",
      "Iteration: 3513 lambda_n: 0.8861347325642613 Loss: 0.08174164678665041\n",
      "Iteration: 3514 lambda_n: 0.9059974429254777 Loss: 0.08172197831242073\n",
      "Iteration: 3515 lambda_n: 0.9888224744251971 Loss: 0.08170187634334788\n",
      "Iteration: 3516 lambda_n: 0.9056964403795038 Loss: 0.08167994512842904\n",
      "Iteration: 3517 lambda_n: 0.9662301728961268 Loss: 0.08165986549937251\n",
      "Iteration: 3518 lambda_n: 1.0167224981249932 Loss: 0.08163845198464291\n",
      "Iteration: 3519 lambda_n: 0.9449113370170066 Loss: 0.08161592863155337\n",
      "Iteration: 3520 lambda_n: 0.899950088092238 Loss: 0.08159500465942864\n",
      "Iteration: 3521 lambda_n: 0.9621930160807568 Loss: 0.08157508396548505\n",
      "Iteration: 3522 lambda_n: 0.9971265940176284 Loss: 0.08155379361709131\n",
      "Iteration: 3523 lambda_n: 1.0059654941537557 Loss: 0.08153173924369976\n",
      "Iteration: 3524 lambda_n: 0.9970947568714641 Loss: 0.08150949861782184\n",
      "Iteration: 3525 lambda_n: 0.8789200710216286 Loss: 0.08148746330195875\n",
      "Iteration: 3526 lambda_n: 0.9004403542495164 Loss: 0.08146804727031007\n",
      "Iteration: 3527 lambda_n: 0.9119593946704251 Loss: 0.08144816321410922\n",
      "Iteration: 3528 lambda_n: 0.9330123406534913 Loss: 0.081428032399533\n",
      "Iteration: 3529 lambda_n: 0.9743508888539391 Loss: 0.08140744478134941\n",
      "Iteration: 3530 lambda_n: 0.967860188185271 Loss: 0.08138595354034556\n",
      "Iteration: 3531 lambda_n: 0.9183803855972276 Loss: 0.08136461416733626\n",
      "Iteration: 3532 lambda_n: 0.9363571992764805 Loss: 0.0813443737967314\n",
      "Iteration: 3533 lambda_n: 0.9195516821166732 Loss: 0.0813237452647506\n",
      "Iteration: 3534 lambda_n: 0.9686028165663875 Loss: 0.0813034949066249\n",
      "Iteration: 3535 lambda_n: 0.8932343642354825 Loss: 0.08128217279123194\n",
      "Iteration: 3536 lambda_n: 1.0270678650757044 Loss: 0.08126251759192157\n",
      "Iteration: 3537 lambda_n: 0.9947609920367393 Loss: 0.08123992648629348\n",
      "Iteration: 3538 lambda_n: 0.8827544467307205 Loss: 0.08121805540825001\n",
      "Iteration: 3539 lambda_n: 0.9199867763339145 Loss: 0.08119865478184565\n",
      "Iteration: 3540 lambda_n: 1.0122593291405524 Loss: 0.08117844360265573\n",
      "Iteration: 3541 lambda_n: 0.9805388655334069 Loss: 0.08115621434294347\n",
      "Iteration: 3542 lambda_n: 0.9793137564580144 Loss: 0.08113469085812855\n",
      "Iteration: 3543 lambda_n: 0.9886179870698022 Loss: 0.08111320327451745\n",
      "Iteration: 3544 lambda_n: 0.9127633577913651 Loss: 0.0810915206759755\n",
      "Iteration: 3545 lambda_n: 0.9290949408954154 Loss: 0.0810715099807365\n",
      "Iteration: 3546 lambda_n: 0.8833783424496697 Loss: 0.0810511492926464\n",
      "Iteration: 3547 lambda_n: 1.023978836537895 Loss: 0.08103179806102563\n",
      "Iteration: 3548 lambda_n: 0.8782814976262039 Loss: 0.08100937591782079\n",
      "Iteration: 3549 lambda_n: 0.9879792369017923 Loss: 0.08099015216297974\n",
      "Iteration: 3550 lambda_n: 0.928234790405902 Loss: 0.08096853597395422\n",
      "Iteration: 3551 lambda_n: 0.9742609046967186 Loss: 0.08094823544514862\n",
      "Iteration: 3552 lambda_n: 1.0243165552622386 Loss: 0.08092693708456987\n",
      "Iteration: 3553 lambda_n: 0.9570898078790954 Loss: 0.08090455414493439\n",
      "Iteration: 3554 lambda_n: 1.0242838328098447 Loss: 0.08088364932548897\n",
      "Iteration: 3555 lambda_n: 0.9623475075986481 Loss: 0.0808612864676309\n",
      "Iteration: 3556 lambda_n: 0.8981954541056784 Loss: 0.08084028505290873\n",
      "Iteration: 3557 lambda_n: 0.9443758028846335 Loss: 0.08082069170207819\n",
      "Iteration: 3558 lambda_n: 0.9907145080684507 Loss: 0.08080009927008494\n",
      "Iteration: 3559 lambda_n: 0.9016370983016374 Loss: 0.0807785055721164\n",
      "Iteration: 3560 lambda_n: 0.9563488665059882 Loss: 0.08075886171122394\n",
      "Iteration: 3561 lambda_n: 0.9419589720882614 Loss: 0.08073803436432775\n",
      "Iteration: 3562 lambda_n: 0.9876037365300392 Loss: 0.08071752905123925\n",
      "Iteration: 3563 lambda_n: 0.9065944850154398 Loss: 0.08069603927963925\n",
      "Iteration: 3564 lambda_n: 0.9653945123425391 Loss: 0.08067632063006501\n",
      "Iteration: 3565 lambda_n: 0.9092199635260569 Loss: 0.08065533178112794\n",
      "Iteration: 3566 lambda_n: 0.9154932215492024 Loss: 0.08063557257907737\n",
      "Iteration: 3567 lambda_n: 0.9335362232036157 Loss: 0.08061568518031414\n",
      "Iteration: 3568 lambda_n: 0.9933241769973465 Loss: 0.08059541423920803\n",
      "Iteration: 3569 lambda_n: 0.9892076846280351 Loss: 0.08057385435264754\n",
      "Iteration: 3570 lambda_n: 1.0295903445344206 Loss: 0.0805523934279744\n",
      "Iteration: 3571 lambda_n: 0.952263009238268 Loss: 0.08053006656187109\n",
      "Iteration: 3572 lambda_n: 0.971616303846631 Loss: 0.08050942591620659\n",
      "Iteration: 3573 lambda_n: 1.0056339671193972 Loss: 0.0804883749775841\n",
      "Iteration: 3574 lambda_n: 0.929259061051601 Loss: 0.08046659680426922\n",
      "Iteration: 3575 lambda_n: 0.9859735529423044 Loss: 0.08044648159347464\n",
      "Iteration: 3576 lambda_n: 0.8884820906327179 Loss: 0.08042514802340227\n",
      "Iteration: 3577 lambda_n: 0.9590940380979549 Loss: 0.08040593225109137\n",
      "Iteration: 3578 lambda_n: 0.9652794605012687 Loss: 0.08038519806753222\n",
      "Iteration: 3579 lambda_n: 0.918879886183908 Loss: 0.08036433943262386\n",
      "Iteration: 3580 lambda_n: 0.9644689602717936 Loss: 0.08034449214846502\n",
      "Iteration: 3581 lambda_n: 0.9853839356670706 Loss: 0.0803236692300971\n",
      "Iteration: 3582 lambda_n: 0.8907484412890111 Loss: 0.08030240439262365\n",
      "Iteration: 3583 lambda_n: 1.0009922418110844 Loss: 0.08028319031786707\n",
      "Iteration: 3584 lambda_n: 0.9924575816391916 Loss: 0.08026160765516195\n",
      "Iteration: 3585 lambda_n: 0.898843020729119 Loss: 0.08024021903255914\n",
      "Iteration: 3586 lambda_n: 0.9632038714372376 Loss: 0.08022085662355508\n",
      "Iteration: 3587 lambda_n: 0.9736152718318404 Loss: 0.08020011684491285\n",
      "Iteration: 3588 lambda_n: 0.9873813906793507 Loss: 0.08017916249038885\n",
      "Iteration: 3589 lambda_n: 0.9953887896244316 Loss: 0.08015792174094272\n",
      "Iteration: 3590 lambda_n: 0.9406215591471696 Loss: 0.08013651884010224\n",
      "Iteration: 3591 lambda_n: 1.025039559596399 Loss: 0.08011630295486573\n",
      "Iteration: 3592 lambda_n: 1.0092188647608455 Loss: 0.08009428304613145\n",
      "Iteration: 3593 lambda_n: 1.0160343431319403 Loss: 0.08007261361858814\n",
      "Iteration: 3594 lambda_n: 0.9146042503153706 Loss: 0.08005080850498772\n",
      "Iteration: 3595 lambda_n: 0.9366729586782924 Loss: 0.08003118944773421\n",
      "Iteration: 3596 lambda_n: 0.8849147702537498 Loss: 0.08001110600872101\n",
      "Iteration: 3597 lambda_n: 0.8922847795048662 Loss: 0.07999214079387142\n",
      "Iteration: 3598 lambda_n: 0.9283565142625926 Loss: 0.07997302592417543\n",
      "Iteration: 3599 lambda_n: 1.000615261384794 Loss: 0.07995314715260825\n",
      "Iteration: 3600 lambda_n: 0.9125918012907119 Loss: 0.07993173120536846\n",
      "Iteration: 3601 lambda_n: 0.9610806990047226 Loss: 0.07991220850417392\n",
      "Iteration: 3602 lambda_n: 0.9708953755146581 Loss: 0.07989165799085045\n",
      "Iteration: 3603 lambda_n: 0.949222589672983 Loss: 0.07987090756760812\n",
      "Iteration: 3604 lambda_n: 0.9827257833972465 Loss: 0.07985063007668611\n",
      "Iteration: 3605 lambda_n: 0.949119825543355 Loss: 0.0798296469986153\n",
      "Iteration: 3606 lambda_n: 0.8834541548166708 Loss: 0.07980939133419784\n",
      "Iteration: 3607 lambda_n: 0.9472717327373725 Loss: 0.07979054583942899\n",
      "Iteration: 3608 lambda_n: 1.0251983283569455 Loss: 0.07977034830965565\n",
      "Iteration: 3609 lambda_n: 0.946608183434211 Loss: 0.07974850011178056\n",
      "Iteration: 3610 lambda_n: 0.9370803040659317 Loss: 0.07972833697228415\n",
      "Iteration: 3611 lambda_n: 0.8860300365729974 Loss: 0.07970838640704425\n",
      "Iteration: 3612 lambda_n: 0.9558778590221718 Loss: 0.07968953158287888\n",
      "Iteration: 3613 lambda_n: 0.9425876537312132 Loss: 0.07966919998002585\n",
      "Iteration: 3614 lambda_n: 0.880434052864981 Loss: 0.07964916092042641\n",
      "Iteration: 3615 lambda_n: 0.9280309338321627 Loss: 0.07963045213835775\n",
      "Iteration: 3616 lambda_n: 0.9648914312927349 Loss: 0.07961074120198297\n",
      "Iteration: 3617 lambda_n: 0.9534213111915786 Loss: 0.0795902574861077\n",
      "Iteration: 3618 lambda_n: 0.9130362527621585 Loss: 0.07957002748524357\n",
      "Iteration: 3619 lambda_n: 0.9063604712601233 Loss: 0.07955066396592173\n",
      "Iteration: 3620 lambda_n: 0.9176952570710958 Loss: 0.07953145129866138\n",
      "Iteration: 3621 lambda_n: 0.9859109676085783 Loss: 0.07951200779231778\n",
      "Iteration: 3622 lambda_n: 0.9735118719265987 Loss: 0.07949112953826494\n",
      "Iteration: 3623 lambda_n: 0.909225665362692 Loss: 0.07947052470750704\n",
      "Iteration: 3624 lambda_n: 0.968253478646461 Loss: 0.07945129034659754\n",
      "Iteration: 3625 lambda_n: 0.9014328164037173 Loss: 0.0794308176304102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3626 lambda_n: 0.9834786287200048 Loss: 0.07941176751625441\n",
      "Iteration: 3627 lambda_n: 0.9976549043843257 Loss: 0.07939099414153947\n",
      "Iteration: 3628 lambda_n: 0.9980966086913707 Loss: 0.07936993277536128\n",
      "Iteration: 3629 lambda_n: 0.9238730234318648 Loss: 0.07934887368166825\n",
      "Iteration: 3630 lambda_n: 0.9076947722078826 Loss: 0.07932939109740458\n",
      "Iteration: 3631 lambda_n: 0.9834132127705288 Loss: 0.07931025945466226\n",
      "Iteration: 3632 lambda_n: 0.9742029939237179 Loss: 0.07928954277710218\n",
      "Iteration: 3633 lambda_n: 0.9150357879672907 Loss: 0.07926903142586007\n",
      "Iteration: 3634 lambda_n: 0.8935125608994173 Loss: 0.07924977614407902\n",
      "Iteration: 3635 lambda_n: 0.997343295695304 Loss: 0.07923098345185779\n",
      "Iteration: 3636 lambda_n: 1.012628107335508 Loss: 0.07921001817232531\n",
      "Iteration: 3637 lambda_n: 0.9848455847155902 Loss: 0.07918874384474336\n",
      "Iteration: 3638 lambda_n: 0.9015175551118355 Loss: 0.07916806514300798\n",
      "Iteration: 3639 lambda_n: 1.0270687891344903 Loss: 0.07914914649126964\n",
      "Iteration: 3640 lambda_n: 0.943938380400458 Loss: 0.07912760509899201\n",
      "Iteration: 3641 lambda_n: 0.9513009285617944 Loss: 0.07910781876114013\n",
      "Iteration: 3642 lambda_n: 0.9560790334483172 Loss: 0.07908788922096688\n",
      "Iteration: 3643 lambda_n: 0.8958130611760197 Loss: 0.0790678708929671\n",
      "Iteration: 3644 lambda_n: 1.0179585945995429 Loss: 0.07904912480460989\n",
      "Iteration: 3645 lambda_n: 0.9171650766545324 Loss: 0.07902783475278297\n",
      "Iteration: 3646 lambda_n: 1.0096352896180512 Loss: 0.07900866400594422\n",
      "Iteration: 3647 lambda_n: 1.0045732341279223 Loss: 0.07898757266293485\n",
      "Iteration: 3648 lambda_n: 0.9586694588347199 Loss: 0.07896659995133389\n",
      "Iteration: 3649 lambda_n: 0.8859396200970032 Loss: 0.07894659766935182\n",
      "Iteration: 3650 lambda_n: 0.9398843706176242 Loss: 0.07892812344109539\n",
      "Iteration: 3651 lambda_n: 1.0187367193608203 Loss: 0.07890853537231335\n",
      "Iteration: 3652 lambda_n: 0.901337826611929 Loss: 0.07888731684929429\n",
      "Iteration: 3653 lambda_n: 1.0153050201043559 Loss: 0.07886855495290236\n",
      "Iteration: 3654 lambda_n: 0.9982758123673456 Loss: 0.078847433453115\n",
      "Iteration: 3655 lambda_n: 0.9820534991802967 Loss: 0.07882667952793235\n",
      "Iteration: 3656 lambda_n: 0.893171359865135 Loss: 0.0788062759631068\n",
      "Iteration: 3657 lambda_n: 0.9776594038422499 Loss: 0.07878911024818995\n",
      "Iteration: 3658 lambda_n: 1.0260158545279132 Loss: 0.07877072060768574\n",
      "Iteration: 3659 lambda_n: 0.9672037261047008 Loss: 0.07875136967575348\n",
      "Iteration: 3660 lambda_n: 1.0248551017081515 Loss: 0.07873306783678437\n",
      "Iteration: 3661 lambda_n: 0.9753256639969274 Loss: 0.07871365293663328\n",
      "Iteration: 3662 lambda_n: 0.9268393961153948 Loss: 0.07869517560958866\n",
      "Iteration: 3663 lambda_n: 0.9440475514987691 Loss: 0.07867762891068188\n",
      "Iteration: 3664 lambda_n: 1.0016654760684582 Loss: 0.07865977492166673\n",
      "Iteration: 3665 lambda_n: 0.8855237355721982 Loss: 0.0786408552353166\n",
      "Iteration: 3666 lambda_n: 0.9851987730568958 Loss: 0.07862415350085693\n",
      "Iteration: 3667 lambda_n: 1.0171329277957473 Loss: 0.0786055972830908\n",
      "Iteration: 3668 lambda_n: 0.9278915688879211 Loss: 0.07858646913734545\n",
      "Iteration: 3669 lambda_n: 0.9115180468209412 Loss: 0.07856904699096626\n",
      "Iteration: 3670 lambda_n: 0.9049192321581002 Loss: 0.07855195718648729\n",
      "Iteration: 3671 lambda_n: 0.9854486374072108 Loss: 0.07853501523802016\n",
      "Iteration: 3672 lambda_n: 0.9498513439583628 Loss: 0.07851659165168612\n",
      "Iteration: 3673 lambda_n: 0.9316024702613214 Loss: 0.0784988603834363\n",
      "Iteration: 3674 lambda_n: 0.9268039007267161 Loss: 0.07848149485767303\n",
      "Iteration: 3675 lambda_n: 0.97333651493266 Loss: 0.07846424300714389\n",
      "Iteration: 3676 lambda_n: 0.9623793251990849 Loss: 0.0784461501211934\n",
      "Iteration: 3677 lambda_n: 1.0109006213236738 Loss: 0.07842828662015101\n",
      "Iteration: 3678 lambda_n: 0.9651626231849811 Loss: 0.07840954899857311\n",
      "Iteration: 3679 lambda_n: 0.9173116445778373 Loss: 0.07839168528439962\n",
      "Iteration: 3680 lambda_n: 0.9527859122323136 Loss: 0.07837473065808082\n",
      "Iteration: 3681 lambda_n: 0.9964278986553956 Loss: 0.07835714340506163\n",
      "Iteration: 3682 lambda_n: 0.9622338563797902 Loss: 0.0783387753673159\n",
      "Iteration: 3683 lambda_n: 0.8793554775869596 Loss: 0.07832106229573103\n",
      "Iteration: 3684 lambda_n: 0.9367237560492004 Loss: 0.07830489630475378\n",
      "Iteration: 3685 lambda_n: 0.9793636952370117 Loss: 0.07828769654263887\n",
      "Iteration: 3686 lambda_n: 0.9820315887785904 Loss: 0.07826973684913648\n",
      "Iteration: 3687 lambda_n: 0.9006527526760469 Loss: 0.0782517520393019\n",
      "Iteration: 3688 lambda_n: 0.9432683685948318 Loss: 0.07823527912902208\n",
      "Iteration: 3689 lambda_n: 0.9461898057516307 Loss: 0.07821804745324883\n",
      "Iteration: 3690 lambda_n: 0.9271974613970291 Loss: 0.07820078385483104\n",
      "Iteration: 3691 lambda_n: 0.8987471640325512 Loss: 0.07818388762083028\n",
      "Iteration: 3692 lambda_n: 0.955376290195182 Loss: 0.0781675294247724\n",
      "Iteration: 3693 lambda_n: 0.8940088557746847 Loss: 0.07815016064743571\n",
      "Iteration: 3694 lambda_n: 0.9667652612703156 Loss: 0.0781339271928939\n",
      "Iteration: 3695 lambda_n: 0.9627583221264862 Loss: 0.07811639253627196\n",
      "Iteration: 3696 lambda_n: 0.893439863704513 Loss: 0.07809895167366426\n",
      "Iteration: 3697 lambda_n: 1.027608501980578 Loss: 0.07808278578422141\n",
      "Iteration: 3698 lambda_n: 1.021159993431554 Loss: 0.07806421290962307\n",
      "Iteration: 3699 lambda_n: 0.916886218731441 Loss: 0.07804577972431971\n",
      "Iteration: 3700 lambda_n: 0.9561345191979218 Loss: 0.07802924910458525\n",
      "Iteration: 3701 lambda_n: 0.8902414081258616 Loss: 0.07801202988689417\n",
      "Iteration: 3702 lambda_n: 0.9641132552317765 Loss: 0.07799601549900607\n",
      "Iteration: 3703 lambda_n: 1.010148340595286 Loss: 0.07797869057255476\n",
      "Iteration: 3704 lambda_n: 0.9714604763931062 Loss: 0.07796055896781082\n",
      "Iteration: 3705 lambda_n: 0.9365664234219683 Loss: 0.07794314218809305\n",
      "Iteration: 3706 lambda_n: 0.9967698173348418 Loss: 0.07792636973807426\n",
      "Iteration: 3707 lambda_n: 0.9687610708201564 Loss: 0.07790853831894314\n",
      "Iteration: 3708 lambda_n: 0.9050773273654035 Loss: 0.07789122748492179\n",
      "Iteration: 3709 lambda_n: 0.9158579861611036 Loss: 0.07787507213785133\n",
      "Iteration: 3710 lambda_n: 0.9914286254768673 Loss: 0.07785874086294156\n",
      "Iteration: 3711 lambda_n: 1.000215584626556 Loss: 0.07784108005127047\n",
      "Iteration: 3712 lambda_n: 0.923785606382973 Loss: 0.07782328211240272\n",
      "Iteration: 3713 lambda_n: 0.9613023700975988 Loss: 0.07780686197492766\n",
      "Iteration: 3714 lambda_n: 0.9072692495705882 Loss: 0.07778979207372028\n",
      "Iteration: 3715 lambda_n: 0.9011221829714013 Loss: 0.07777369816689872\n",
      "Iteration: 3716 lambda_n: 0.8756155969088684 Loss: 0.07775772871014479\n",
      "Iteration: 3717 lambda_n: 0.9392489229176295 Loss: 0.07774222599806677\n",
      "Iteration: 3718 lambda_n: 1.0236242663905595 Loss: 0.07772561199225181\n",
      "Iteration: 3719 lambda_n: 0.9553069034538098 Loss: 0.07770752330136069\n",
      "Iteration: 3720 lambda_n: 1.0143539110308617 Loss: 0.07769065960644043\n",
      "Iteration: 3721 lambda_n: 0.9594931860963563 Loss: 0.0776727711579931\n",
      "Iteration: 3722 lambda_n: 0.9000164724331786 Loss: 0.07765586755631956\n",
      "Iteration: 3723 lambda_n: 0.8772842230678625 Loss: 0.07764002702388974\n",
      "Iteration: 3724 lambda_n: 0.9362708477944954 Loss: 0.0776246004500027\n",
      "Iteration: 3725 lambda_n: 1.016191889729742 Loss: 0.07760815102987059\n",
      "Iteration: 3726 lambda_n: 0.9400883652480416 Loss: 0.07759031404180422\n",
      "Iteration: 3727 lambda_n: 1.0152973807173127 Loss: 0.07757382919224208\n",
      "Iteration: 3728 lambda_n: 0.9693740585239945 Loss: 0.07755604185081508\n",
      "Iteration: 3729 lambda_n: 1.0046766349704994 Loss: 0.07753907560403454\n",
      "Iteration: 3730 lambda_n: 1.0253140768866467 Loss: 0.07752150780637015\n",
      "Iteration: 3731 lambda_n: 0.9188487281677955 Loss: 0.07750359624079943\n",
      "Iteration: 3732 lambda_n: 0.9208961105049659 Loss: 0.077487559917121\n",
      "Iteration: 3733 lambda_n: 0.9048927095043421 Loss: 0.07747150164141467\n",
      "Iteration: 3734 lambda_n: 0.9101264011657451 Loss: 0.07745573587078024\n",
      "Iteration: 3735 lambda_n: 0.983327810845826 Loss: 0.07743989211630202\n",
      "Iteration: 3736 lambda_n: 1.007968324672265 Loss: 0.07742278835936146\n",
      "Iteration: 3737 lambda_n: 1.019885036523569 Loss: 0.07740527167138914\n",
      "Iteration: 3738 lambda_n: 1.0246479352935265 Loss: 0.07738756398094696\n",
      "Iteration: 3739 lambda_n: 1.01565100587148 Loss: 0.07736978979971094\n",
      "Iteration: 3740 lambda_n: 0.9673616951383794 Loss: 0.07735218766576744\n",
      "Iteration: 3741 lambda_n: 1.0087565104667098 Loss: 0.07733543734233757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3742 lambda_n: 0.8802694429608391 Loss: 0.0773179850294778\n",
      "Iteration: 3743 lambda_n: 0.948396777726087 Loss: 0.07730276882430313\n",
      "Iteration: 3744 lambda_n: 0.936463056062579 Loss: 0.07728638744739975\n",
      "Iteration: 3745 lambda_n: 1.008558688085314 Loss: 0.07727022527654348\n",
      "Iteration: 3746 lambda_n: 1.0202200650609796 Loss: 0.07725283271273523\n",
      "Iteration: 3747 lambda_n: 0.9437174417214915 Loss: 0.07723525398686605\n",
      "Iteration: 3748 lambda_n: 0.980287927247326 Loss: 0.0772190071989696\n",
      "Iteration: 3749 lambda_n: 0.9059487464702886 Loss: 0.07720214405417881\n",
      "Iteration: 3750 lambda_n: 1.016881607952326 Loss: 0.07718657220616333\n",
      "Iteration: 3751 lambda_n: 1.0298474647562856 Loss: 0.07716910664000728\n",
      "Iteration: 3752 lambda_n: 0.9437767591534572 Loss: 0.07715143297710532\n",
      "Iteration: 3753 lambda_n: 0.9774619550117873 Loss: 0.07713524975341716\n",
      "Iteration: 3754 lambda_n: 0.8769228582750902 Loss: 0.0771185015866355\n",
      "Iteration: 3755 lambda_n: 1.0235551140722747 Loss: 0.07710348765148216\n",
      "Iteration: 3756 lambda_n: 1.0281339283007709 Loss: 0.07708597545108475\n",
      "Iteration: 3757 lambda_n: 0.89055212729048 Loss: 0.07706839899923432\n",
      "Iteration: 3758 lambda_n: 1.0292544856958685 Loss: 0.07705318660784138\n",
      "Iteration: 3759 lambda_n: 1.0007106275571604 Loss: 0.07703561710868259\n",
      "Iteration: 3760 lambda_n: 1.002139494383744 Loss: 0.07701854828678276\n",
      "Iteration: 3761 lambda_n: 0.925658192237281 Loss: 0.07700146808891883\n",
      "Iteration: 3762 lambda_n: 0.8989036664666301 Loss: 0.07698570327379695\n",
      "Iteration: 3763 lambda_n: 1.019963362819481 Loss: 0.07697040469967399\n",
      "Iteration: 3764 lambda_n: 0.9017709148287814 Loss: 0.07695305751203341\n",
      "Iteration: 3765 lambda_n: 0.9230744658605268 Loss: 0.07693773195769069\n",
      "Iteration: 3766 lambda_n: 0.9953416887712446 Loss: 0.07692205474276025\n",
      "Iteration: 3767 lambda_n: 0.9971088925238732 Loss: 0.07690516159761907\n",
      "Iteration: 3768 lambda_n: 0.9537079679553785 Loss: 0.07688825064980466\n",
      "Iteration: 3769 lambda_n: 0.9089061098668905 Loss: 0.07687208733038307\n",
      "Iteration: 3770 lambda_n: 0.8944024599670563 Loss: 0.07685669374965945\n",
      "Iteration: 3771 lambda_n: 0.9824293020689263 Loss: 0.07684155555086625\n",
      "Iteration: 3772 lambda_n: 0.9913764053593828 Loss: 0.07682493799999525\n",
      "Iteration: 3773 lambda_n: 0.9689210486225595 Loss: 0.07680818063961943\n",
      "Iteration: 3774 lambda_n: 1.0107773121817742 Loss: 0.07679181410057771\n",
      "Iteration: 3775 lambda_n: 0.9792320538730689 Loss: 0.07677475198986801\n",
      "Iteration: 3776 lambda_n: 0.9695696307298649 Loss: 0.07675823378272993\n",
      "Iteration: 3777 lambda_n: 0.9035849309598485 Loss: 0.07674188944667394\n",
      "Iteration: 3778 lambda_n: 0.882565245928833 Loss: 0.07672666735472912\n",
      "Iteration: 3779 lambda_n: 1.025267022002373 Loss: 0.07671180836014213\n",
      "Iteration: 3780 lambda_n: 0.8800852817706437 Loss: 0.07669455709373513\n",
      "Iteration: 3781 lambda_n: 0.9490490108934525 Loss: 0.07667975861648837\n",
      "Iteration: 3782 lambda_n: 0.9419686551817309 Loss: 0.07666380981447062\n",
      "Iteration: 3783 lambda_n: 1.0220767285955448 Loss: 0.07664798980717911\n",
      "Iteration: 3784 lambda_n: 0.934701945021284 Loss: 0.07663083497682475\n",
      "Iteration: 3785 lambda_n: 0.9981575952882638 Loss: 0.07661515693883521\n",
      "Iteration: 3786 lambda_n: 0.8824716882517224 Loss: 0.07659842460947666\n",
      "Iteration: 3787 lambda_n: 0.9684157688472923 Loss: 0.07658364085326462\n",
      "Iteration: 3788 lambda_n: 0.8889218248257028 Loss: 0.07656742641424748\n",
      "Iteration: 3789 lambda_n: 0.9770708008942427 Loss: 0.07655255195191396\n",
      "Iteration: 3790 lambda_n: 0.9241184051888917 Loss: 0.07653621161053992\n",
      "Iteration: 3791 lambda_n: 0.9162537457032706 Loss: 0.07652076615127618\n",
      "Iteration: 3792 lambda_n: 0.9658205576465172 Loss: 0.07650546084363179\n",
      "Iteration: 3793 lambda_n: 0.930143314046337 Loss: 0.07648933663886409\n",
      "Iteration: 3794 lambda_n: 0.9254321542033108 Loss: 0.07647381714638815\n",
      "Iteration: 3795 lambda_n: 0.8962316950300634 Loss: 0.0764583849259684\n",
      "Iteration: 3796 lambda_n: 0.9788565614461227 Loss: 0.0764434479182958\n",
      "Iteration: 3797 lambda_n: 0.9749541917327927 Loss: 0.07642714262467301\n",
      "Iteration: 3798 lambda_n: 0.931552321312872 Loss: 0.07641091174783174\n",
      "Iteration: 3799 lambda_n: 0.8963783701823915 Loss: 0.07639541228275565\n",
      "Iteration: 3800 lambda_n: 0.950274524352213 Loss: 0.07638050614907478\n",
      "Iteration: 3801 lambda_n: 0.9086969205647853 Loss: 0.07636471202726626\n",
      "Iteration: 3802 lambda_n: 1.0264741637018415 Loss: 0.07634961720525905\n",
      "Iteration: 3803 lambda_n: 0.9884734856390082 Loss: 0.07633257490561111\n",
      "Iteration: 3804 lambda_n: 0.986712193592856 Loss: 0.07631617309501296\n",
      "Iteration: 3805 lambda_n: 0.9363989114514444 Loss: 0.07629980966909077\n",
      "Iteration: 3806 lambda_n: 0.8763903472587193 Loss: 0.07628428920787472\n",
      "Iteration: 3807 lambda_n: 0.9063401357328119 Loss: 0.07626977092403302\n",
      "Iteration: 3808 lambda_n: 1.0274286992461732 Loss: 0.07625476381624163\n",
      "Iteration: 3809 lambda_n: 0.915309493357736 Loss: 0.07623776033480384\n",
      "Iteration: 3810 lambda_n: 0.9333706705094474 Loss: 0.07622262083437244\n",
      "Iteration: 3811 lambda_n: 0.9160106969930565 Loss: 0.0762071903058754\n",
      "Iteration: 3812 lambda_n: 0.9619860269254397 Loss: 0.07619205441335213\n",
      "Iteration: 3813 lambda_n: 0.9142922813543369 Loss: 0.07617616670406928\n",
      "Iteration: 3814 lambda_n: 0.9299871091643045 Loss: 0.07616107441745915\n",
      "Iteration: 3815 lambda_n: 1.0262500402857162 Loss: 0.07614573052319779\n",
      "Iteration: 3816 lambda_n: 0.944396747548166 Loss: 0.07612880677066344\n",
      "Iteration: 3817 lambda_n: 0.8815262150518254 Loss: 0.07611324118166592\n",
      "Iteration: 3818 lambda_n: 0.9768589167990704 Loss: 0.07609871894022618\n",
      "Iteration: 3819 lambda_n: 0.9523238287028783 Loss: 0.07608263360658268\n",
      "Iteration: 3820 lambda_n: 0.9518041671264804 Loss: 0.07606696014969744\n",
      "Iteration: 3821 lambda_n: 0.8822210179747505 Loss: 0.07605130287909458\n",
      "Iteration: 3822 lambda_n: 0.9875445912372279 Loss: 0.07603679723866717\n",
      "Iteration: 3823 lambda_n: 0.895773597898523 Loss: 0.07602056716228019\n",
      "Iteration: 3824 lambda_n: 0.9224975172427808 Loss: 0.07600585256827645\n",
      "Iteration: 3825 lambda_n: 0.9101090555961545 Loss: 0.07599070578284599\n",
      "Iteration: 3826 lambda_n: 0.996949752913637 Loss: 0.07597576924140279\n",
      "Iteration: 3827 lambda_n: 1.026058971783378 Loss: 0.07595941489361814\n",
      "Iteration: 3828 lambda_n: 0.9578933363481523 Loss: 0.07594259127710452\n",
      "Iteration: 3829 lambda_n: 0.8951108536846042 Loss: 0.07592689313403703\n",
      "Iteration: 3830 lambda_n: 0.9731152858396622 Loss: 0.07591223064456915\n",
      "Iteration: 3831 lambda_n: 0.8835579576573942 Loss: 0.07589629730872832\n",
      "Iteration: 3832 lambda_n: 1.0000857781696637 Loss: 0.07588183702069788\n",
      "Iteration: 3833 lambda_n: 0.9643782122292135 Loss: 0.07586547659465613\n",
      "Iteration: 3834 lambda_n: 0.9153764166110578 Loss: 0.07584970774137086\n",
      "Iteration: 3835 lambda_n: 0.9621515425712276 Loss: 0.0758347468784647\n",
      "Iteration: 3836 lambda_n: 1.0169319329805915 Loss: 0.07581902827319197\n",
      "Iteration: 3837 lambda_n: 0.9044032203296206 Loss: 0.07580242217753683\n",
      "Iteration: 3838 lambda_n: 0.9022359123204566 Loss: 0.07578766048607224\n",
      "Iteration: 3839 lambda_n: 0.9613111657175115 Loss: 0.07577294026898734\n",
      "Iteration: 3840 lambda_n: 0.995542972639394 Loss: 0.07575726270675057\n",
      "Iteration: 3841 lambda_n: 1.0255879384139508 Loss: 0.0757410339648638\n",
      "Iteration: 3842 lambda_n: 0.9045853555771296 Loss: 0.07572432295948502\n",
      "Iteration: 3843 lambda_n: 0.8922232814069884 Loss: 0.07570959026602388\n",
      "Iteration: 3844 lambda_n: 0.885220078528836 Loss: 0.0756950647431975\n",
      "Iteration: 3845 lambda_n: 0.9636727934718218 Loss: 0.07568065891036574\n",
      "Iteration: 3846 lambda_n: 0.9064351421584329 Loss: 0.0756649825086684\n",
      "Iteration: 3847 lambda_n: 1.0170068706374555 Loss: 0.07565024338526184\n",
      "Iteration: 3848 lambda_n: 1.0000669185839979 Loss: 0.0756337128899098\n",
      "Iteration: 3849 lambda_n: 0.964502808958911 Loss: 0.0756174648642491\n",
      "Iteration: 3850 lambda_n: 0.8945452953734286 Loss: 0.07560180134728141\n",
      "Iteration: 3851 lambda_n: 0.9420117238555095 Loss: 0.07558727987256424\n",
      "Iteration: 3852 lambda_n: 1.0106102404264226 Loss: 0.07557199368515351\n",
      "Iteration: 3853 lambda_n: 0.8802146622609455 Loss: 0.0755556008938688\n",
      "Iteration: 3854 lambda_n: 0.9731387475078531 Loss: 0.07554132917875944\n",
      "Iteration: 3855 lambda_n: 0.9863179405243975 Loss: 0.07552555664333135\n",
      "Iteration: 3856 lambda_n: 0.9455763285874041 Loss: 0.07550957695132349\n",
      "Iteration: 3857 lambda_n: 0.8886783802544442 Loss: 0.07549426352324631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3858 lambda_n: 0.9552565697077531 Loss: 0.07547987708136067\n",
      "Iteration: 3859 lambda_n: 0.9941621518699353 Loss: 0.07546441846404803\n",
      "Iteration: 3860 lambda_n: 0.9100573876376837 Loss: 0.07544833649170848\n",
      "Iteration: 3861 lambda_n: 0.9494001257522148 Loss: 0.07543362086868337\n",
      "Iteration: 3862 lambda_n: 1.0026945493296286 Loss: 0.0754182746822831\n",
      "Iteration: 3863 lambda_n: 1.0292703676527886 Loss: 0.07540207319091444\n",
      "Iteration: 3864 lambda_n: 1.0044485214037449 Loss: 0.07538544889628424\n",
      "Iteration: 3865 lambda_n: 0.9191985855957104 Loss: 0.07536923205526462\n",
      "Iteration: 3866 lambda_n: 0.9846296564805918 Loss: 0.0753543973445496\n",
      "Iteration: 3867 lambda_n: 0.962302459161907 Loss: 0.07533851236644865\n",
      "Iteration: 3868 lambda_n: 0.9776052093498093 Loss: 0.07532299347561479\n",
      "Iteration: 3869 lambda_n: 0.974141411178326 Loss: 0.07530723362780972\n",
      "Iteration: 3870 lambda_n: 0.923821548863309 Loss: 0.07529153547101584\n",
      "Iteration: 3871 lambda_n: 0.9218728302840715 Loss: 0.07527665367972043\n",
      "Iteration: 3872 lambda_n: 0.9984373660333041 Loss: 0.07526180844907211\n",
      "Iteration: 3873 lambda_n: 1.013312265592379 Loss: 0.07524573587666412\n",
      "Iteration: 3874 lambda_n: 0.9520269443090665 Loss: 0.07522942993148013\n",
      "Iteration: 3875 lambda_n: 0.8847770139226873 Loss: 0.07521411588741402\n",
      "Iteration: 3876 lambda_n: 0.9411135244423336 Loss: 0.07519988855946728\n",
      "Iteration: 3877 lambda_n: 0.9544880459184268 Loss: 0.0751847602692347\n",
      "Iteration: 3878 lambda_n: 0.9708393519854489 Loss: 0.07516942225269826\n",
      "Iteration: 3879 lambda_n: 0.9259989376234875 Loss: 0.07515382688461272\n",
      "Iteration: 3880 lambda_n: 0.9672428158825214 Loss: 0.07513895699938279\n",
      "Iteration: 3881 lambda_n: 0.9607147246586067 Loss: 0.07512342998684554\n",
      "Iteration: 3882 lambda_n: 1.024766497182932 Loss: 0.0751080130778182\n",
      "Iteration: 3883 lambda_n: 0.9181539079092376 Loss: 0.07509157394440111\n",
      "Iteration: 3884 lambda_n: 1.022452392785419 Loss: 0.07507685032681984\n",
      "Iteration: 3885 lambda_n: 1.0070043415000738 Loss: 0.07506045950284607\n",
      "Iteration: 3886 lambda_n: 0.9671095262122246 Loss: 0.07504432206189111\n",
      "Iteration: 3887 lambda_n: 1.0215898275582482 Loss: 0.07502882932061201\n",
      "Iteration: 3888 lambda_n: 0.8988817741689045 Loss: 0.0750124693047764\n",
      "Iteration: 3889 lambda_n: 0.9095164465757285 Loss: 0.07499807933233578\n",
      "Iteration: 3890 lambda_n: 1.023940890073179 Loss: 0.0749835235695692\n",
      "Iteration: 3891 lambda_n: 0.9815646157839245 Loss: 0.07496714169039095\n",
      "Iteration: 3892 lambda_n: 0.9381088543032723 Loss: 0.07495144316957689\n",
      "Iteration: 3893 lambda_n: 1.0199820691153367 Loss: 0.07493644455525215\n",
      "Iteration: 3894 lambda_n: 0.885140869212717 Loss: 0.07492014208180121\n",
      "Iteration: 3895 lambda_n: 0.9740680781422896 Loss: 0.07490599949078511\n",
      "Iteration: 3896 lambda_n: 0.9085065063588522 Loss: 0.07489044062518985\n",
      "Iteration: 3897 lambda_n: 1.0084525485436004 Loss: 0.07487593357048875\n",
      "Iteration: 3898 lambda_n: 0.9097092686637223 Loss: 0.07485983539878835\n",
      "Iteration: 3899 lambda_n: 0.9668537183027122 Loss: 0.07484531817791372\n",
      "Iteration: 3900 lambda_n: 0.8900169778956782 Loss: 0.07482989359417987\n",
      "Iteration: 3901 lambda_n: 0.9696875518240572 Loss: 0.07481569917232497\n",
      "Iteration: 3902 lambda_n: 0.878871477565474 Loss: 0.07480023855783086\n",
      "Iteration: 3903 lambda_n: 0.9751457247427749 Loss: 0.07478623016602697\n",
      "Iteration: 3904 lambda_n: 0.9071324999484778 Loss: 0.07477069161627667\n",
      "Iteration: 3905 lambda_n: 0.9870605568098373 Loss: 0.07475624121436787\n",
      "Iteration: 3906 lambda_n: 0.9221433466034225 Loss: 0.07474052207602173\n",
      "Iteration: 3907 lambda_n: 0.98679309584144 Loss: 0.07472584122106235\n",
      "Iteration: 3908 lambda_n: 1.0154233049153658 Loss: 0.07471013562994162\n",
      "Iteration: 3909 lambda_n: 0.9341849581847502 Loss: 0.07469397928239266\n",
      "Iteration: 3910 lambda_n: 0.9905449236719915 Loss: 0.07467912007870227\n",
      "Iteration: 3911 lambda_n: 0.966444935163225 Loss: 0.07466336891450767\n",
      "Iteration: 3912 lambda_n: 0.9989714063878051 Loss: 0.07464800556277543\n",
      "Iteration: 3913 lambda_n: 0.9439779522703498 Loss: 0.07463212977744502\n",
      "Iteration: 3914 lambda_n: 0.9900850641913618 Loss: 0.07461713240431998\n",
      "Iteration: 3915 lambda_n: 0.9387850081937725 Loss: 0.07460140697082986\n",
      "Iteration: 3916 lambda_n: 1.0053095743937892 Loss: 0.0745865006540549\n",
      "Iteration: 3917 lambda_n: 0.937464280457518 Loss: 0.07457054248474936\n",
      "Iteration: 3918 lambda_n: 0.9205190058275511 Loss: 0.0745556656226243\n",
      "Iteration: 3919 lambda_n: 0.9055541033686503 Loss: 0.07454106164581746\n",
      "Iteration: 3920 lambda_n: 0.9941910673983 Loss: 0.07452669890016753\n",
      "Iteration: 3921 lambda_n: 0.8836413261528965 Loss: 0.07451093448319673\n",
      "Iteration: 3922 lambda_n: 0.9357946155420387 Loss: 0.07449692693865234\n",
      "Iteration: 3923 lambda_n: 0.9674966884376127 Loss: 0.07448209641958604\n",
      "Iteration: 3924 lambda_n: 0.9824504338648331 Loss: 0.07446676758088891\n",
      "Iteration: 3925 lambda_n: 0.8857744458320778 Loss: 0.07445120608128182\n",
      "Iteration: 3926 lambda_n: 0.9215054499200355 Loss: 0.0744371797012362\n",
      "Iteration: 3927 lambda_n: 0.888251438026841 Loss: 0.07442259114793397\n",
      "Iteration: 3928 lambda_n: 0.9913748386585145 Loss: 0.07440853263388873\n",
      "Iteration: 3929 lambda_n: 0.9041065991484889 Loss: 0.07439284588699806\n",
      "Iteration: 3930 lambda_n: 0.9433956784955456 Loss: 0.0743785438662524\n",
      "Iteration: 3931 lambda_n: 0.9105827930207475 Loss: 0.07436362405397222\n",
      "Iteration: 3932 lambda_n: 0.8782316672763042 Loss: 0.0743492268664293\n",
      "Iteration: 3933 lambda_n: 0.9426207822924378 Loss: 0.07433534459674634\n",
      "Iteration: 3934 lambda_n: 1.0056027135780992 Loss: 0.0743204480952284\n",
      "Iteration: 3935 lambda_n: 1.0005950433218276 Loss: 0.07430456033868954\n",
      "Iteration: 3936 lambda_n: 0.9600946914276806 Loss: 0.07428875595075847\n",
      "Iteration: 3937 lambda_n: 0.9058553173635732 Loss: 0.07427359528175152\n",
      "Iteration: 3938 lambda_n: 0.9408259346918523 Loss: 0.07425929470403106\n",
      "Iteration: 3939 lambda_n: 0.9488195019137559 Loss: 0.07424444561629549\n",
      "Iteration: 3940 lambda_n: 0.9061181017989591 Loss: 0.07422947406565827\n",
      "Iteration: 3941 lambda_n: 0.9041310183230121 Loss: 0.0742151798241606\n",
      "Iteration: 3942 lambda_n: 0.9440971057471275 Loss: 0.07420092028524762\n",
      "Iteration: 3943 lambda_n: 0.9818155693177137 Loss: 0.07418603392248352\n",
      "Iteration: 3944 lambda_n: 1.0243791809599005 Loss: 0.07417055660460647\n",
      "Iteration: 3945 lambda_n: 0.9252851472295396 Loss: 0.07415441240126129\n",
      "Iteration: 3946 lambda_n: 0.9764182648827415 Loss: 0.07413983367110939\n",
      "Iteration: 3947 lambda_n: 1.0187880161649638 Loss: 0.07412445292925594\n",
      "Iteration: 3948 lambda_n: 0.8914899114698727 Loss: 0.07410840874973905\n",
      "Iteration: 3949 lambda_n: 0.9483424520775467 Loss: 0.07409437282323282\n",
      "Iteration: 3950 lambda_n: 0.9063886376184739 Loss: 0.07407944514752798\n",
      "Iteration: 3951 lambda_n: 0.9975309032671618 Loss: 0.07406518120104313\n",
      "Iteration: 3952 lambda_n: 0.9953222217453596 Loss: 0.07404948651323263\n",
      "Iteration: 3953 lambda_n: 0.8763051344542594 Loss: 0.07403383042340633\n",
      "Iteration: 3954 lambda_n: 1.0174225516007165 Loss: 0.07402004973506582\n",
      "Iteration: 3955 lambda_n: 1.0010720327729599 Loss: 0.07400405335336814\n",
      "Iteration: 3956 lambda_n: 1.0051596696237144 Loss: 0.07398831792206496\n",
      "Iteration: 3957 lambda_n: 0.9910711670354112 Loss: 0.07397252206398613\n",
      "Iteration: 3958 lambda_n: 0.9274028259276872 Loss: 0.07395695136025555\n",
      "Iteration: 3959 lambda_n: 0.8853184692466421 Loss: 0.07394238436976901\n",
      "Iteration: 3960 lambda_n: 0.9583506309713408 Loss: 0.0739284814600939\n",
      "Iteration: 3961 lambda_n: 0.9468155377731333 Loss: 0.0739134348643001\n",
      "Iteration: 3962 lambda_n: 0.8924952754866852 Loss: 0.07389857273194048\n",
      "Iteration: 3963 lambda_n: 0.9025041139215687 Loss: 0.07388456635103466\n",
      "Iteration: 3964 lambda_n: 0.9194476517846618 Loss: 0.07387040585825173\n",
      "Iteration: 3965 lambda_n: 0.8869682561661594 Loss: 0.07385598255886597\n",
      "Iteration: 3966 lambda_n: 0.953961314658657 Loss: 0.0738420717105002\n",
      "Iteration: 3967 lambda_n: 0.938939066205805 Loss: 0.0738271132715563\n",
      "Iteration: 3968 lambda_n: 0.9886541897339145 Loss: 0.07381239360694343\n",
      "Iteration: 3969 lambda_n: 0.9312766569514672 Loss: 0.07379689792186331\n",
      "Iteration: 3970 lambda_n: 0.9598064470099289 Loss: 0.07378230480038969\n",
      "Iteration: 3971 lambda_n: 0.9113344879993248 Loss: 0.07376726780871953\n",
      "Iteration: 3972 lambda_n: 0.933103785437215 Loss: 0.07375299327990735\n",
      "Iteration: 3973 lambda_n: 0.9425207221141035 Loss: 0.07373838077683094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3974 lambda_n: 1.0145611528137044 Loss: 0.07372362389069952\n",
      "Iteration: 3975 lambda_n: 0.9233362464488859 Loss: 0.07370774245856482\n",
      "Iteration: 3976 lambda_n: 0.9616393006379712 Loss: 0.07369329221850378\n",
      "Iteration: 3977 lambda_n: 1.01567246227876 Loss: 0.0736782456240181\n",
      "Iteration: 3978 lambda_n: 0.9207336928716676 Loss: 0.073662356974357\n",
      "Iteration: 3979 lambda_n: 0.9807969408117074 Loss: 0.07364795664734083\n",
      "Iteration: 3980 lambda_n: 1.0185100427818796 Loss: 0.0736326200389974\n",
      "Iteration: 3981 lambda_n: 0.9042570123791382 Loss: 0.07361669712608196\n",
      "Iteration: 3982 lambda_n: 0.9809043831611202 Loss: 0.07360256344019335\n",
      "Iteration: 3983 lambda_n: 1.0023941234043496 Loss: 0.0735872347673594\n",
      "Iteration: 3984 lambda_n: 0.9650171520906228 Loss: 0.07357157357423298\n",
      "Iteration: 3985 lambda_n: 0.9303139901853731 Loss: 0.07355649955032179\n",
      "Iteration: 3986 lambda_n: 0.8958781501509896 Loss: 0.07354197056201729\n",
      "Iteration: 3987 lambda_n: 0.949964515377698 Loss: 0.07352798209936566\n",
      "Iteration: 3988 lambda_n: 0.9718223009006824 Loss: 0.07351315194411512\n",
      "Iteration: 3989 lambda_n: 0.9859131690795535 Loss: 0.07349798359272615\n",
      "Iteration: 3990 lambda_n: 0.9455790335890665 Loss: 0.07348259843665587\n",
      "Iteration: 3991 lambda_n: 0.894274262049659 Loss: 0.07346784569309392\n",
      "Iteration: 3992 lambda_n: 1.0195085219685738 Loss: 0.07345389609793178\n",
      "Iteration: 3993 lambda_n: 0.9577637488808389 Loss: 0.07343799600403098\n",
      "Iteration: 3994 lambda_n: 0.9183960155492432 Loss: 0.07342306196066027\n",
      "Iteration: 3995 lambda_n: 0.9237953221232583 Loss: 0.07340874454266771\n",
      "Iteration: 3996 lambda_n: 1.0174702304795418 Loss: 0.07339434564420237\n",
      "Iteration: 3997 lambda_n: 0.9454441385162562 Loss: 0.07337848968879537\n",
      "Iteration: 3998 lambda_n: 0.9617404732949815 Loss: 0.07336375915056464\n",
      "Iteration: 3999 lambda_n: 0.9561885807182298 Loss: 0.07334877756100187\n",
      "Iteration: 4000 lambda_n: 0.9400131168890936 Loss: 0.07333388531867335\n",
      "Iteration: 4001 lambda_n: 0.9555707522210136 Loss: 0.07331924778201356\n",
      "Iteration: 4002 lambda_n: 0.9638534673208123 Loss: 0.07330437077114185\n",
      "Iteration: 4003 lambda_n: 1.0063010496645355 Loss: 0.07328936764693073\n",
      "Iteration: 4004 lambda_n: 0.9468115456630071 Loss: 0.07327370678947355\n",
      "Iteration: 4005 lambda_n: 0.9465458041564627 Loss: 0.07325897462764681\n",
      "Iteration: 4006 lambda_n: 0.8851264424554162 Loss: 0.07324424932178282\n",
      "Iteration: 4007 lambda_n: 0.9359396551324733 Loss: 0.07323048201116966\n",
      "Iteration: 4008 lambda_n: 1.006469235207794 Loss: 0.07321592687145503\n",
      "Iteration: 4009 lambda_n: 0.977757155401947 Loss: 0.07320027776806795\n",
      "Iteration: 4010 lambda_n: 1.0035421069369657 Loss: 0.07318507801815119\n",
      "Iteration: 4011 lambda_n: 0.8931570922973993 Loss: 0.07316948036140078\n",
      "Iteration: 4012 lambda_n: 1.0261851558418345 Loss: 0.07315560097146528\n",
      "Iteration: 4013 lambda_n: 0.960285577860616 Loss: 0.07313965714956376\n",
      "Iteration: 4014 lambda_n: 0.9433081733006741 Loss: 0.07312474006575576\n",
      "Iteration: 4015 lambda_n: 0.9792233605950252 Loss: 0.0731100893490417\n",
      "Iteration: 4016 lambda_n: 1.0212835209182571 Loss: 0.07309488353754154\n",
      "Iteration: 4017 lambda_n: 0.877382946481107 Loss: 0.07307902752426268\n",
      "Iteration: 4018 lambda_n: 1.0254008847700777 Loss: 0.07306540816411433\n",
      "Iteration: 4019 lambda_n: 0.9901765843505684 Loss: 0.07304949384281584\n",
      "Iteration: 4020 lambda_n: 0.9246129568078304 Loss: 0.07303412909560204\n",
      "Iteration: 4021 lambda_n: 0.9736405324341003 Loss: 0.07301978428786929\n",
      "Iteration: 4022 lambda_n: 0.9507084374145679 Loss: 0.07300468143416242\n",
      "Iteration: 4023 lambda_n: 0.9197048250994311 Loss: 0.07298993690493842\n",
      "Iteration: 4024 lambda_n: 0.9368081700149357 Loss: 0.07297567565925925\n",
      "Iteration: 4025 lambda_n: 1.0252400075319665 Loss: 0.07296115163318001\n",
      "Iteration: 4026 lambda_n: 0.9958067641989871 Loss: 0.07294525932492235\n",
      "Iteration: 4027 lambda_n: 0.9766527366776057 Loss: 0.07292982609186933\n",
      "Iteration: 4028 lambda_n: 1.0179472878037266 Loss: 0.07291469240097839\n",
      "Iteration: 4029 lambda_n: 1.0089744476873295 Loss: 0.07289892160557646\n",
      "Iteration: 4030 lambda_n: 0.9408796130899318 Loss: 0.0728832926473589\n",
      "Iteration: 4031 lambda_n: 0.9673105463323297 Loss: 0.07286872104003184\n",
      "Iteration: 4032 lambda_n: 0.9519546200302831 Loss: 0.07285374259255221\n",
      "Iteration: 4033 lambda_n: 0.9288727458262496 Loss: 0.07283900442331678\n",
      "Iteration: 4034 lambda_n: 0.8858294260675562 Loss: 0.07282462599234849\n",
      "Iteration: 4035 lambda_n: 1.0242560775631644 Loss: 0.07281091604755155\n",
      "Iteration: 4036 lambda_n: 0.9239003871967163 Loss: 0.07279506620549295\n",
      "Iteration: 4037 lambda_n: 0.9434586542913458 Loss: 0.07278077179917877\n",
      "Iteration: 4038 lambda_n: 0.8867169105447439 Loss: 0.07276617713041851\n",
      "Iteration: 4039 lambda_n: 0.8933368115364387 Loss: 0.07275246241461851\n",
      "Iteration: 4040 lambda_n: 0.957365326929838 Loss: 0.07273864741387184\n",
      "Iteration: 4041 lambda_n: 0.910171051766288 Loss: 0.07272384454137865\n",
      "Iteration: 4042 lambda_n: 0.9713858316233658 Loss: 0.07270977366038588\n",
      "Iteration: 4043 lambda_n: 0.9613353439416935 Loss: 0.07269475877884461\n",
      "Iteration: 4044 lambda_n: 0.9958544451250476 Loss: 0.07267990168557707\n",
      "Iteration: 4045 lambda_n: 0.931939359458108 Loss: 0.0726645136267354\n",
      "Iteration: 4046 lambda_n: 1.0087433044230112 Loss: 0.07265011556474878\n",
      "Iteration: 4047 lambda_n: 0.9658250676418905 Loss: 0.07263453339246886\n",
      "Iteration: 4048 lambda_n: 0.9599069925031471 Loss: 0.07261961666907671\n",
      "Iteration: 4049 lambda_n: 0.8902930238270083 Loss: 0.07260479372452501\n",
      "Iteration: 4050 lambda_n: 0.9808738546365441 Loss: 0.0725910479124136\n",
      "Iteration: 4051 lambda_n: 0.9446683252364361 Loss: 0.07257590584343714\n",
      "Iteration: 4052 lambda_n: 0.9666846768768057 Loss: 0.07256132502485778\n",
      "Iteration: 4053 lambda_n: 0.9217225852080652 Loss: 0.0725464067118391\n",
      "Iteration: 4054 lambda_n: 0.984647562001246 Loss: 0.07253218449723288\n",
      "Iteration: 4055 lambda_n: 0.9814506278210616 Loss: 0.0725169936668577\n",
      "Iteration: 4056 lambda_n: 1.0220837543944843 Loss: 0.0725018545774832\n",
      "Iteration: 4057 lambda_n: 0.941292844371069 Loss: 0.07248609124303945\n",
      "Iteration: 4058 lambda_n: 0.9013944120684428 Loss: 0.07247157627410232\n",
      "Iteration: 4059 lambda_n: 0.9723060192545866 Loss: 0.07245767863070643\n",
      "Iteration: 4060 lambda_n: 0.9420847864105688 Loss: 0.07244268988355106\n",
      "Iteration: 4061 lambda_n: 0.9774904713686108 Loss: 0.07242816925584761\n",
      "Iteration: 4062 lambda_n: 1.027558172470786 Loss: 0.07241310519121127\n",
      "Iteration: 4063 lambda_n: 0.9407340868860667 Loss: 0.07239727202369041\n",
      "Iteration: 4064 lambda_n: 0.9798728391606382 Loss: 0.07238277899590137\n",
      "Iteration: 4065 lambda_n: 0.884512553135687 Loss: 0.07236768524764538\n",
      "Iteration: 4066 lambda_n: 0.9231196981695896 Loss: 0.07235406245926362\n",
      "Iteration: 4067 lambda_n: 0.9927119356705174 Loss: 0.07233984705806008\n",
      "Iteration: 4068 lambda_n: 0.9232278018492527 Loss: 0.07232456223270024\n",
      "Iteration: 4069 lambda_n: 0.9041470278620575 Loss: 0.07231034941761669\n",
      "Iteration: 4070 lambda_n: 0.9435789094334349 Loss: 0.07229643233072458\n",
      "Iteration: 4071 lambda_n: 0.8815070287474994 Loss: 0.07228191034425194\n",
      "Iteration: 4072 lambda_n: 0.8777390600945676 Loss: 0.07226834560741133\n",
      "Iteration: 4073 lambda_n: 0.9837361076055436 Loss: 0.07225484068305478\n",
      "Iteration: 4074 lambda_n: 0.9991344381416339 Loss: 0.07223970698654587\n",
      "Iteration: 4075 lambda_n: 0.8948668345249571 Loss: 0.07222433872732754\n",
      "Iteration: 4076 lambda_n: 0.9359606722284616 Loss: 0.07221057630970253\n",
      "Iteration: 4077 lambda_n: 0.8962193525172749 Loss: 0.07219618388055912\n",
      "Iteration: 4078 lambda_n: 0.910299253763175 Loss: 0.07218240449544693\n",
      "Iteration: 4079 lambda_n: 0.988420926489135 Loss: 0.07216841053649416\n",
      "Iteration: 4080 lambda_n: 1.011620224232493 Loss: 0.07215321775227777\n",
      "Iteration: 4081 lambda_n: 0.9681824898780724 Loss: 0.07213767070304386\n",
      "Iteration: 4082 lambda_n: 1.0202752580990475 Loss: 0.0721227934566624\n",
      "Iteration: 4083 lambda_n: 1.0075926548326521 Loss: 0.07210711804594944\n",
      "Iteration: 4084 lambda_n: 0.9412195873522403 Loss: 0.07209163983602175\n",
      "Iteration: 4085 lambda_n: 0.9012324236051181 Loss: 0.07207718334775363\n",
      "Iteration: 4086 lambda_n: 0.9224698530925813 Loss: 0.07206334294317285\n",
      "Iteration: 4087 lambda_n: 1.0251229777881727 Loss: 0.07204917828929767\n",
      "Iteration: 4088 lambda_n: 0.9262036191829033 Loss: 0.07203343958594864\n",
      "Iteration: 4089 lambda_n: 0.8901092754237598 Loss: 0.07201922167675086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4090 lambda_n: 0.9785703991928059 Loss: 0.07200555967680705\n",
      "Iteration: 4091 lambda_n: 0.8849934289710979 Loss: 0.07199054192180121\n",
      "Iteration: 4092 lambda_n: 1.0020278220472403 Loss: 0.0719769621439402\n",
      "Iteration: 4093 lambda_n: 0.9379484760777884 Loss: 0.07196158858140178\n",
      "Iteration: 4094 lambda_n: 0.998116984893152 Loss: 0.07194720020650551\n",
      "Iteration: 4095 lambda_n: 0.9864973385894491 Loss: 0.07193189094344073\n",
      "Iteration: 4096 lambda_n: 1.0223859807713065 Loss: 0.0719167620743036\n",
      "Iteration: 4097 lambda_n: 0.9756349947618923 Loss: 0.07190108506484909\n",
      "Iteration: 4098 lambda_n: 0.9117899476341489 Loss: 0.0718861270879074\n",
      "Iteration: 4099 lambda_n: 0.9972638474724933 Loss: 0.07187214986704799\n",
      "Iteration: 4100 lambda_n: 0.9675426830654353 Loss: 0.07185686441930214\n",
      "Iteration: 4101 lambda_n: 1.0095259628282154 Loss: 0.07184203660469454\n",
      "Iteration: 4102 lambda_n: 0.877051506378259 Loss: 0.07182656753542785\n",
      "Iteration: 4103 lambda_n: 0.9836492128994542 Loss: 0.07181313023328294\n",
      "Iteration: 4104 lambda_n: 0.9669469837429998 Loss: 0.07179806167687357\n",
      "Iteration: 4105 lambda_n: 1.009404405141096 Loss: 0.0717832510228556\n",
      "Iteration: 4106 lambda_n: 0.931497162801817 Loss: 0.07176779217621868\n",
      "Iteration: 4107 lambda_n: 1.0268170555264717 Loss: 0.07175352843603726\n",
      "Iteration: 4108 lambda_n: 0.9471457401488846 Loss: 0.07173780719556105\n",
      "Iteration: 4109 lambda_n: 0.9942108176719111 Loss: 0.07172330780224667\n",
      "Iteration: 4110 lambda_n: 0.9096003429063388 Loss: 0.07170808994356664\n",
      "Iteration: 4111 lambda_n: 0.9860858129207097 Loss: 0.07169416904297035\n",
      "Iteration: 4112 lambda_n: 0.8924961425757464 Loss: 0.07167907952053977\n",
      "Iteration: 4113 lambda_n: 0.9839180616098615 Loss: 0.0716654239538866\n",
      "Iteration: 4114 lambda_n: 0.9626622421054727 Loss: 0.07165037149663299\n",
      "Iteration: 4115 lambda_n: 0.9049218320246761 Loss: 0.07163564619680628\n",
      "Iteration: 4116 lambda_n: 0.9016701143402461 Loss: 0.07162180591082258\n",
      "Iteration: 4117 lambda_n: 1.02510436341995 Loss: 0.07160801706010056\n",
      "Iteration: 4118 lambda_n: 0.91456223646808 Loss: 0.07159234258669823\n",
      "Iteration: 4119 lambda_n: 0.9436352453107045 Loss: 0.07157836025596705\n",
      "Iteration: 4120 lambda_n: 0.9334662343120234 Loss: 0.07156393524686824\n",
      "Iteration: 4121 lambda_n: 0.9854117093260603 Loss: 0.07154966750330717\n",
      "Iteration: 4122 lambda_n: 0.9563940663750718 Loss: 0.07153460771839634\n",
      "Iteration: 4123 lambda_n: 1.0252518761240474 Loss: 0.07151999332461528\n",
      "Iteration: 4124 lambda_n: 0.9311798099875458 Loss: 0.07150432879027331\n",
      "Iteration: 4125 lambda_n: 0.8945461376003199 Loss: 0.07149010345706565\n",
      "Iteration: 4126 lambda_n: 0.9589679859563905 Loss: 0.07147643944557311\n",
      "Iteration: 4127 lambda_n: 0.9276434308827138 Loss: 0.0714617931897824\n",
      "Iteration: 4128 lambda_n: 0.9836998215557262 Loss: 0.07144762714245965\n",
      "Iteration: 4129 lambda_n: 0.9963205081778789 Loss: 0.07143260694295651\n",
      "Iteration: 4130 lambda_n: 0.9343994962140021 Loss: 0.0714173960283316\n",
      "Iteration: 4131 lambda_n: 0.8915396055287329 Loss: 0.07140313231077232\n",
      "Iteration: 4132 lambda_n: 0.9635343538440976 Loss: 0.07138952450834414\n",
      "Iteration: 4133 lambda_n: 0.924031838892199 Loss: 0.07137481960063508\n",
      "Iteration: 4134 lambda_n: 0.9504807977559578 Loss: 0.07136071932005604\n",
      "Iteration: 4135 lambda_n: 0.9880735563275285 Loss: 0.07134621721380122\n",
      "Iteration: 4136 lambda_n: 0.8917585322826624 Loss: 0.07133114342815751\n",
      "Iteration: 4137 lambda_n: 0.9476068541310064 Loss: 0.07131754069992456\n",
      "Iteration: 4138 lambda_n: 0.9393493187502905 Loss: 0.07130308778340377\n",
      "Iteration: 4139 lambda_n: 0.9656917760635327 Loss: 0.07128876257116958\n",
      "Iteration: 4140 lambda_n: 0.970538031993017 Loss: 0.07127403744341704\n",
      "Iteration: 4141 lambda_n: 0.9557797889307744 Loss: 0.07125924027201085\n",
      "Iteration: 4142 lambda_n: 0.9881791003809217 Loss: 0.07124466992861027\n",
      "Iteration: 4143 lambda_n: 0.9758491393384341 Loss: 0.07122960755189613\n",
      "Iteration: 4144 lambda_n: 0.9408968138630348 Loss: 0.07121473501207222\n",
      "Iteration: 4145 lambda_n: 0.8840898250312287 Loss: 0.07120039693847462\n",
      "Iteration: 4146 lambda_n: 1.0264400736545485 Loss: 0.07118692612138483\n",
      "Iteration: 4147 lambda_n: 0.9280606002266304 Loss: 0.07117128818155036\n",
      "Iteration: 4148 lambda_n: 1.0147058821501784 Loss: 0.07115715085909302\n",
      "Iteration: 4149 lambda_n: 0.9666811843368167 Loss: 0.071141695534799\n",
      "Iteration: 4150 lambda_n: 0.89965627523867 Loss: 0.07112697356076028\n",
      "Iteration: 4151 lambda_n: 0.9782307545964849 Loss: 0.07111327397823662\n",
      "Iteration: 4152 lambda_n: 0.8846953810397693 Loss: 0.07109837964251893\n",
      "Iteration: 4153 lambda_n: 0.9796961025083613 Loss: 0.07108491107020562\n",
      "Iteration: 4154 lambda_n: 0.9840425433711425 Loss: 0.07106999793364999\n",
      "Iteration: 4155 lambda_n: 0.9853068861417638 Loss: 0.07105502048569116\n",
      "Iteration: 4156 lambda_n: 1.0040722295829143 Loss: 0.07104002564992487\n",
      "Iteration: 4157 lambda_n: 0.8828010350475619 Loss: 0.07102474713580413\n",
      "Iteration: 4158 lambda_n: 0.9704783310999542 Loss: 0.0710113155720477\n",
      "Iteration: 4159 lambda_n: 0.9468977038372657 Loss: 0.07099655170647753\n",
      "Iteration: 4160 lambda_n: 1.011743346702511 Loss: 0.07098214830266991\n",
      "Iteration: 4161 lambda_n: 0.954599113096704 Loss: 0.07096676037854256\n",
      "Iteration: 4162 lambda_n: 0.9302649066018851 Loss: 0.07095224337078042\n",
      "Iteration: 4163 lambda_n: 0.9552081485635638 Loss: 0.07093809808538878\n",
      "Iteration: 4164 lambda_n: 0.8920640131205699 Loss: 0.07092357521051722\n",
      "Iteration: 4165 lambda_n: 0.9500580997897133 Loss: 0.07091001393805313\n",
      "Iteration: 4166 lambda_n: 0.9265601175391899 Loss: 0.07089557265740144\n",
      "Iteration: 4167 lambda_n: 0.9639466066754232 Loss: 0.07088149019113187\n",
      "Iteration: 4168 lambda_n: 1.0180966502816948 Loss: 0.07086684119361064\n",
      "Iteration: 4169 lambda_n: 1.0133975987924642 Loss: 0.07085137115043644\n",
      "Iteration: 4170 lambda_n: 0.9217831571645559 Loss: 0.07083597442899338\n",
      "Iteration: 4171 lambda_n: 1.0098593178377935 Loss: 0.07082197130461398\n",
      "Iteration: 4172 lambda_n: 0.941572439024044 Loss: 0.07080663196956614\n",
      "Iteration: 4173 lambda_n: 0.9556574926421387 Loss: 0.07079233160412962\n",
      "Iteration: 4174 lambda_n: 0.9584049968164208 Loss: 0.07077781899123789\n",
      "Iteration: 4175 lambda_n: 0.880304345292206 Loss: 0.07076326634748109\n",
      "Iteration: 4176 lambda_n: 1.0039001413354363 Loss: 0.07074990111316891\n",
      "Iteration: 4177 lambda_n: 0.9415602993170643 Loss: 0.07073466108912899\n",
      "Iteration: 4178 lambda_n: 0.8830792413962733 Loss: 0.07072036913370386\n",
      "Iteration: 4179 lambda_n: 0.9043708623375308 Loss: 0.0707069663546269\n",
      "Iteration: 4180 lambda_n: 0.9156388002887748 Loss: 0.07069324190008643\n",
      "Iteration: 4181 lambda_n: 0.9150083264805031 Loss: 0.07067934792948757\n",
      "Iteration: 4182 lambda_n: 0.9849512829399476 Loss: 0.07066546506835457\n",
      "Iteration: 4183 lambda_n: 0.8869711810662247 Loss: 0.0706505227015412\n",
      "Iteration: 4184 lambda_n: 0.9527390138072979 Loss: 0.07063706829753688\n",
      "Iteration: 4185 lambda_n: 0.9133031618800814 Loss: 0.07062261784426727\n",
      "Iteration: 4186 lambda_n: 1.007658300818052 Loss: 0.07060876708233808\n",
      "Iteration: 4187 lambda_n: 0.8959992508823095 Loss: 0.07059348709981748\n",
      "Iteration: 4188 lambda_n: 0.8792771560450608 Loss: 0.07057990186731702\n",
      "Iteration: 4189 lambda_n: 0.9780189120089976 Loss: 0.07056657158963889\n",
      "Iteration: 4190 lambda_n: 0.8909521717545124 Loss: 0.07055174595012444\n",
      "Iteration: 4191 lambda_n: 0.9782072093454556 Loss: 0.0705382416601989\n",
      "Iteration: 4192 lambda_n: 0.8847140525650489 Loss: 0.07052341645411517\n",
      "Iteration: 4193 lambda_n: 0.9499508043520124 Loss: 0.07051000967804748\n",
      "Iteration: 4194 lambda_n: 0.9053146754634966 Loss: 0.07049561586442701\n",
      "Iteration: 4195 lambda_n: 0.9774915041866559 Loss: 0.07048189989808978\n",
      "Iteration: 4196 lambda_n: 0.8985901846016833 Loss: 0.0704670920438308\n",
      "Iteration: 4197 lambda_n: 0.9286468444537754 Loss: 0.07045348097357454\n",
      "Iteration: 4198 lambda_n: 0.9430243607967234 Loss: 0.07043941613533768\n",
      "Iteration: 4199 lambda_n: 0.8963130539481022 Loss: 0.07042513510860188\n",
      "Iteration: 4200 lambda_n: 0.9361675344607732 Loss: 0.07041156294536943\n",
      "Iteration: 4201 lambda_n: 0.9695077021837948 Loss: 0.0703973888094627\n",
      "Iteration: 4202 lambda_n: 0.9773263607487294 Loss: 0.07038271151200771\n",
      "Iteration: 4203 lambda_n: 1.0081368078213608 Loss: 0.07036791752937778\n",
      "Iteration: 4204 lambda_n: 1.0183446261926903 Loss: 0.07035265892327636\n",
      "Iteration: 4205 lambda_n: 0.8945688231624612 Loss: 0.07033724763464454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4206 lambda_n: 0.9766303902984491 Loss: 0.07032371106011479\n",
      "Iteration: 4207 lambda_n: 1.0169031865280502 Loss: 0.07030893432216599\n",
      "Iteration: 4208 lambda_n: 0.8990475741384257 Loss: 0.07029355001450886\n",
      "Iteration: 4209 lambda_n: 1.0034841310576 Loss: 0.07027995023006985\n",
      "Iteration: 4210 lambda_n: 0.8793114231252454 Loss: 0.07026477229474014\n",
      "Iteration: 4211 lambda_n: 0.9169831449775244 Loss: 0.07025147397323661\n",
      "Iteration: 4212 lambda_n: 0.8787370531276205 Loss: 0.07023760735200568\n",
      "Iteration: 4213 lambda_n: 0.8916647967906223 Loss: 0.07022432047208625\n",
      "Iteration: 4214 lambda_n: 0.9998471976337298 Loss: 0.0702108394919838\n",
      "Iteration: 4215 lambda_n: 1.0235315797819604 Loss: 0.07019572453666692\n",
      "Iteration: 4216 lambda_n: 0.9333376256012883 Loss: 0.07018025333319047\n",
      "Iteration: 4217 lambda_n: 0.8766427613768532 Loss: 0.07016614706088214\n",
      "Iteration: 4218 lambda_n: 0.9667993307432848 Loss: 0.07015289904903772\n",
      "Iteration: 4219 lambda_n: 0.9107967365942787 Loss: 0.07013829018098651\n",
      "Iteration: 4220 lambda_n: 0.8951825476210428 Loss: 0.07012452900086533\n",
      "Iteration: 4221 lambda_n: 0.9654476301203013 Loss: 0.07011100511592196\n",
      "Iteration: 4222 lambda_n: 0.9037455262561943 Loss: 0.0700964212316404\n",
      "Iteration: 4223 lambda_n: 0.9605527258259698 Loss: 0.07008277086816951\n",
      "Iteration: 4224 lambda_n: 0.9893788513358304 Loss: 0.0700682640025541\n",
      "Iteration: 4225 lambda_n: 0.9332045450734231 Loss: 0.07005332343692236\n",
      "Iteration: 4226 lambda_n: 0.8839182040131701 Loss: 0.07003923270679381\n",
      "Iteration: 4227 lambda_n: 0.9418581821545278 Loss: 0.07002588755137935\n",
      "Iteration: 4228 lambda_n: 1.0033503814396487 Loss: 0.0700116690962946\n",
      "Iteration: 4229 lambda_n: 1.0046568345442641 Loss: 0.06999652400257203\n",
      "Iteration: 4230 lambda_n: 0.9818118177644143 Loss: 0.06998136091319498\n",
      "Iteration: 4231 lambda_n: 1.0218245526780305 Loss: 0.06996654428924688\n",
      "Iteration: 4232 lambda_n: 0.9891560067525619 Loss: 0.06995112556765708\n",
      "Iteration: 4233 lambda_n: 0.9940879977921484 Loss: 0.06993620149610237\n",
      "Iteration: 4234 lambda_n: 0.9917596754291873 Loss: 0.06992120468931269\n",
      "Iteration: 4235 lambda_n: 0.8919447440296041 Loss: 0.06990624468241857\n",
      "Iteration: 4236 lambda_n: 0.9621158069283253 Loss: 0.06989279175800174\n",
      "Iteration: 4237 lambda_n: 0.9287558479766154 Loss: 0.06987828186554294\n",
      "Iteration: 4238 lambda_n: 0.966127285069023 Loss: 0.06986427661500003\n",
      "Iteration: 4239 lambda_n: 0.9574277054342419 Loss: 0.06984970938601291\n",
      "Iteration: 4240 lambda_n: 0.9539964288821231 Loss: 0.06983527490348908\n",
      "Iteration: 4241 lambda_n: 0.9257083216026598 Loss: 0.06982089370230937\n",
      "Iteration: 4242 lambda_n: 0.9580694260732695 Loss: 0.06980694041410751\n",
      "Iteration: 4243 lambda_n: 0.9587210031452285 Loss: 0.06979250086310679\n",
      "Iteration: 4244 lambda_n: 0.8877841073951558 Loss: 0.0697780530414104\n",
      "Iteration: 4245 lambda_n: 0.9389800385328027 Loss: 0.06976467562238654\n",
      "Iteration: 4246 lambda_n: 0.9041216488413595 Loss: 0.06975052820038324\n",
      "Iteration: 4247 lambda_n: 0.8885611495365311 Loss: 0.06973690738787518\n",
      "Iteration: 4248 lambda_n: 0.9042528118271098 Loss: 0.06972352233706372\n",
      "Iteration: 4249 lambda_n: 0.9297786179498616 Loss: 0.06970990226779898\n",
      "Iteration: 4250 lambda_n: 0.9022882415766807 Loss: 0.0696958991470619\n",
      "Iteration: 4251 lambda_n: 0.9954984220363848 Loss: 0.06968231143829597\n",
      "Iteration: 4252 lambda_n: 0.9690594046614516 Loss: 0.06966732162598205\n",
      "Iteration: 4253 lambda_n: 0.9107306708395321 Loss: 0.06965273151643848\n",
      "Iteration: 4254 lambda_n: 0.9272042060020005 Loss: 0.06963902103906684\n",
      "Iteration: 4255 lambda_n: 0.9379123569274412 Loss: 0.06962506398155865\n",
      "Iteration: 4256 lambda_n: 1.0119802861945375 Loss: 0.06961094719188161\n",
      "Iteration: 4257 lambda_n: 0.9282467634246117 Loss: 0.06959571721508022\n",
      "Iteration: 4258 lambda_n: 0.8785599918245668 Loss: 0.0695817489114768\n",
      "Iteration: 4259 lambda_n: 0.9850119823908845 Loss: 0.06956852962312321\n",
      "Iteration: 4260 lambda_n: 0.9379329036588263 Loss: 0.06955371010829614\n",
      "Iteration: 4261 lambda_n: 0.9789276573664745 Loss: 0.06953960040226952\n",
      "Iteration: 4262 lambda_n: 0.8997249650147003 Loss: 0.0695248755449943\n",
      "Iteration: 4263 lambda_n: 0.8877492707201771 Loss: 0.0695113434520409\n",
      "Iteration: 4264 lambda_n: 0.8932941780409013 Loss: 0.06949799279171198\n",
      "Iteration: 4265 lambda_n: 1.025104416336607 Loss: 0.06948456005796597\n",
      "Iteration: 4266 lambda_n: 1.0273497867433403 Loss: 0.06946914685878536\n",
      "Iteration: 4267 lambda_n: 0.9346025480122657 Loss: 0.06945370164011211\n",
      "Iteration: 4268 lambda_n: 0.923797062056421 Loss: 0.06943965231353559\n",
      "Iteration: 4269 lambda_n: 0.8888834809718883 Loss: 0.06942576683655599\n",
      "Iteration: 4270 lambda_n: 0.9283494865762395 Loss: 0.06941240747344248\n",
      "Iteration: 4271 lambda_n: 0.9186772368162833 Loss: 0.06939845634460012\n",
      "Iteration: 4272 lambda_n: 0.9829555134019099 Loss: 0.0693846519667006\n",
      "Iteration: 4273 lambda_n: 0.9397722306244076 Loss: 0.06936988324785723\n",
      "Iteration: 4274 lambda_n: 0.9192127745912951 Loss: 0.06935576484019838\n",
      "Iteration: 4275 lambda_n: 0.9125891206423763 Loss: 0.06934195670760973\n",
      "Iteration: 4276 lambda_n: 0.889006761950469 Loss: 0.0693282494460166\n",
      "Iteration: 4277 lambda_n: 0.9450840621765901 Loss: 0.06931489771215982\n",
      "Iteration: 4278 lambda_n: 1.013594725187325 Loss: 0.06930070518257135\n",
      "Iteration: 4279 lambda_n: 0.8789996227457363 Loss: 0.06928548542984284\n",
      "Iteration: 4280 lambda_n: 1.0081895955494744 Loss: 0.06927228808383407\n",
      "Iteration: 4281 lambda_n: 0.9310935520838922 Loss: 0.06925715261092084\n",
      "Iteration: 4282 lambda_n: 1.0150522239730013 Loss: 0.06924317603003735\n",
      "Iteration: 4283 lambda_n: 0.897336645352964 Loss: 0.06922794075325163\n",
      "Iteration: 4284 lambda_n: 0.9030677245879655 Loss: 0.06921447372610695\n",
      "Iteration: 4285 lambda_n: 0.9489954290032616 Loss: 0.06920092201374171\n",
      "Iteration: 4286 lambda_n: 0.9762577589610125 Loss: 0.0691866825231467\n",
      "Iteration: 4287 lambda_n: 0.9473790466271749 Loss: 0.06917203549451195\n",
      "Iteration: 4288 lambda_n: 1.0286315449075516 Loss: 0.06915782322765976\n",
      "Iteration: 4289 lambda_n: 1.0181649661206826 Loss: 0.06914239368094162\n",
      "Iteration: 4290 lambda_n: 0.9563502058127943 Loss: 0.06912712283001318\n",
      "Iteration: 4291 lambda_n: 1.0035208309372918 Loss: 0.06911278064437867\n",
      "Iteration: 4292 lambda_n: 0.9034171893781126 Loss: 0.06909773264226086\n",
      "Iteration: 4293 lambda_n: 0.8782143941312104 Loss: 0.06908418712499471\n",
      "Iteration: 4294 lambda_n: 0.9012725786237676 Loss: 0.06907102076176302\n",
      "Iteration: 4295 lambda_n: 1.0023535677772932 Loss: 0.06905751000364242\n",
      "Iteration: 4296 lambda_n: 0.9011163963986876 Loss: 0.06904248549900446\n",
      "Iteration: 4297 lambda_n: 1.0126921950313719 Loss: 0.06902897986209108\n",
      "Iteration: 4298 lambda_n: 0.9754786511497096 Loss: 0.06901380351912509\n",
      "Iteration: 4299 lambda_n: 1.0074571674280521 Loss: 0.06899918643738912\n",
      "Iteration: 4300 lambda_n: 0.9166329730209481 Loss: 0.06898409178485317\n",
      "Iteration: 4301 lambda_n: 0.9878394274643422 Loss: 0.06897035937998063\n",
      "Iteration: 4302 lambda_n: 0.939251482312491 Loss: 0.06895556171760017\n",
      "Iteration: 4303 lambda_n: 1.0107299295696794 Loss: 0.06894149336139391\n",
      "Iteration: 4304 lambda_n: 0.9915451285467698 Loss: 0.06892635596466083\n",
      "Iteration: 4305 lambda_n: 0.9531076884995852 Loss: 0.06891150749702302\n",
      "Iteration: 4306 lambda_n: 0.9761612057487534 Loss: 0.0688972361320995\n",
      "Iteration: 4307 lambda_n: 0.954783537650753 Loss: 0.06888262109062183\n",
      "Iteration: 4308 lambda_n: 0.8822830733628347 Loss: 0.06886832760237088\n",
      "Iteration: 4309 lambda_n: 0.9292072946950055 Loss: 0.06885512078733555\n",
      "Iteration: 4310 lambda_n: 0.9144457882466082 Loss: 0.06884121291792643\n",
      "Iteration: 4311 lambda_n: 0.9481093687194493 Loss: 0.06882752734878167\n",
      "Iteration: 4312 lambda_n: 0.9726697141020154 Loss: 0.06881333938825322\n",
      "Iteration: 4313 lambda_n: 1.022069674174154 Loss: 0.0687987853944686\n",
      "Iteration: 4314 lambda_n: 0.906920730125533 Loss: 0.06878349386527656\n",
      "Iteration: 4315 lambda_n: 1.0084588892889326 Loss: 0.06876992653504518\n",
      "Iteration: 4316 lambda_n: 1.0286133019280945 Loss: 0.06875484175676018\n",
      "Iteration: 4317 lambda_n: 0.9011200706145298 Loss: 0.0687394571835345\n",
      "Iteration: 4318 lambda_n: 0.956889094545694 Loss: 0.0687259808849991\n",
      "Iteration: 4319 lambda_n: 0.8850363236386969 Loss: 0.06871167197803438\n",
      "Iteration: 4320 lambda_n: 0.952676417807063 Loss: 0.06869843884008854\n",
      "Iteration: 4321 lambda_n: 0.9219880679576917 Loss: 0.06868419573814379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4322 lambda_n: 1.0235949228652965 Loss: 0.06867041283374702\n",
      "Iteration: 4323 lambda_n: 0.9338771854742397 Loss: 0.06865511258125426\n",
      "Iteration: 4324 lambda_n: 0.9397933915279283 Loss: 0.06864115486275991\n",
      "Iteration: 4325 lambda_n: 0.8938389497582201 Loss: 0.06862711012945752\n",
      "Iteration: 4326 lambda_n: 0.9049481100590013 Loss: 0.06861375347732333\n",
      "Iteration: 4327 lambda_n: 0.8814004564048197 Loss: 0.06860023212174175\n",
      "Iteration: 4328 lambda_n: 0.9019886932493926 Loss: 0.06858706383230843\n",
      "Iteration: 4329 lambda_n: 0.8936895919077197 Loss: 0.06857358924755251\n",
      "Iteration: 4330 lambda_n: 0.9501720872326641 Loss: 0.0685602399326309\n",
      "Iteration: 4331 lambda_n: 0.8877862929457946 Loss: 0.06854604832150668\n",
      "Iteration: 4332 lambda_n: 0.9614424683492899 Loss: 0.06853278980630761\n",
      "Iteration: 4333 lambda_n: 0.9622714624686014 Loss: 0.06851843269858778\n",
      "Iteration: 4334 lambda_n: 0.9460868605835969 Loss: 0.06850406469060466\n",
      "Iteration: 4335 lambda_n: 0.8796965897906642 Loss: 0.06848993978353865\n",
      "Iteration: 4336 lambda_n: 1.0219614045643461 Loss: 0.06847680735852167\n",
      "Iteration: 4337 lambda_n: 0.9708272502804226 Loss: 0.06846155269011682\n",
      "Iteration: 4338 lambda_n: 0.927078471766688 Loss: 0.06844706284117591\n",
      "Iteration: 4339 lambda_n: 1.0222537312160336 Loss: 0.06843322736050032\n",
      "Iteration: 4340 lambda_n: 0.9092418374948625 Loss: 0.06841797308792025\n",
      "Iteration: 4341 lambda_n: 0.9999476198046168 Loss: 0.0684044066098372\n",
      "Iteration: 4342 lambda_n: 0.8830070305367564 Loss: 0.06838948825512636\n",
      "Iteration: 4343 lambda_n: 0.9832162416439191 Loss: 0.06837631588637919\n",
      "Iteration: 4344 lambda_n: 0.8971306038304692 Loss: 0.06836165008622007\n",
      "Iteration: 4345 lambda_n: 0.8781334712809565 Loss: 0.06834826970196989\n",
      "Iteration: 4346 lambda_n: 0.8820330305872097 Loss: 0.06833517389531793\n",
      "Iteration: 4347 lambda_n: 1.0110504917407195 Loss: 0.06832202116865456\n",
      "Iteration: 4348 lambda_n: 0.9494257463057842 Loss: 0.06830694606691111\n",
      "Iteration: 4349 lambda_n: 0.9745708721162407 Loss: 0.0682927912964051\n",
      "Iteration: 4350 lambda_n: 0.9413894431380586 Loss: 0.0682782631320766\n",
      "Iteration: 4351 lambda_n: 0.8778229787935405 Loss: 0.06826423104815857\n",
      "Iteration: 4352 lambda_n: 0.941990421836948 Loss: 0.06825114774215571\n",
      "Iteration: 4353 lambda_n: 0.8986287739806458 Loss: 0.06823710942504967\n",
      "Iteration: 4354 lambda_n: 0.9291904072444699 Loss: 0.06822371863665747\n",
      "Iteration: 4355 lambda_n: 0.9518649759832323 Loss: 0.06820987378559486\n",
      "Iteration: 4356 lambda_n: 0.9704636651510279 Loss: 0.06819569250628245\n",
      "Iteration: 4357 lambda_n: 0.9830650656045575 Loss: 0.06818123561642171\n",
      "Iteration: 4358 lambda_n: 0.8848644578585243 Loss: 0.06816659252919638\n",
      "Iteration: 4359 lambda_n: 0.9901476234542077 Loss: 0.06815341349286669\n",
      "Iteration: 4360 lambda_n: 0.8993026218379199 Loss: 0.06813866785086214\n",
      "Iteration: 4361 lambda_n: 0.8998245965272271 Loss: 0.0681252764614822\n",
      "Iteration: 4362 lambda_n: 0.9684862254139289 Loss: 0.06811187858446409\n",
      "Iteration: 4363 lambda_n: 0.9803934401258831 Loss: 0.06809745980573632\n",
      "Iteration: 4364 lambda_n: 0.9947872260513153 Loss: 0.06808286526840486\n",
      "Iteration: 4365 lambda_n: 0.919374649897979 Loss: 0.06806805801742286\n",
      "Iteration: 4366 lambda_n: 0.9195066050359064 Loss: 0.06805437467241558\n",
      "Iteration: 4367 lambda_n: 0.8829982604009775 Loss: 0.06804069070491071\n",
      "Iteration: 4368 lambda_n: 0.9813079871492734 Loss: 0.06802755131444976\n",
      "Iteration: 4369 lambda_n: 0.9309414163857466 Loss: 0.06801295047722557\n",
      "Iteration: 4370 lambda_n: 0.9644101800344843 Loss: 0.06799910045741421\n",
      "Iteration: 4371 lambda_n: 0.9205244395552442 Loss: 0.06798475395554421\n",
      "Iteration: 4372 lambda_n: 0.9159744274275228 Loss: 0.06797106167401644\n",
      "Iteration: 4373 lambda_n: 0.9011029410902943 Loss: 0.06795743840554863\n",
      "Iteration: 4374 lambda_n: 0.975243173573487 Loss: 0.06794403761962672\n",
      "Iteration: 4375 lambda_n: 0.8874907156524606 Loss: 0.0679295357000954\n",
      "Iteration: 4376 lambda_n: 0.9414108535001707 Loss: 0.06791633998067685\n",
      "Iteration: 4377 lambda_n: 0.9678963647386126 Loss: 0.06790234390712785\n",
      "Iteration: 4378 lambda_n: 0.9700543686778258 Loss: 0.06788795553333263\n",
      "Iteration: 4379 lambda_n: 0.9584638003506373 Loss: 0.06787353656957493\n",
      "Iteration: 4380 lambda_n: 0.9251772603001502 Loss: 0.06785929135490731\n",
      "Iteration: 4381 lambda_n: 0.9834145468032421 Loss: 0.06784554224652986\n",
      "Iteration: 4382 lambda_n: 0.881538624183093 Loss: 0.06783092915449519\n",
      "Iteration: 4383 lambda_n: 1.025402733125323 Loss: 0.06781783120144468\n",
      "Iteration: 4384 lambda_n: 0.9129006598890947 Loss: 0.06780259724671238\n",
      "Iteration: 4385 lambda_n: 1.0139947997180603 Loss: 0.06778903609490255\n",
      "Iteration: 4386 lambda_n: 0.9592807707940446 Loss: 0.06777397472993978\n",
      "Iteration: 4387 lambda_n: 0.956945828821765 Loss: 0.06775972756321284\n",
      "Iteration: 4388 lambda_n: 0.9873152634842993 Loss: 0.06774551652834047\n",
      "Iteration: 4389 lambda_n: 0.909669520583347 Loss: 0.06773085601443207\n",
      "Iteration: 4390 lambda_n: 0.924872030204168 Loss: 0.06771734982416384\n",
      "Iteration: 4391 lambda_n: 1.0074951368722596 Loss: 0.06770361926058414\n",
      "Iteration: 4392 lambda_n: 0.9805901102638295 Loss: 0.06768866361976886\n",
      "Iteration: 4393 lambda_n: 0.9011266376923437 Loss: 0.06767410891426741\n",
      "Iteration: 4394 lambda_n: 1.0000002347720711 Loss: 0.06766073501793854\n",
      "Iteration: 4395 lambda_n: 0.8982114039315636 Loss: 0.06764589520893105\n",
      "Iteration: 4396 lambda_n: 0.9046931284865302 Loss: 0.06763256728356738\n",
      "Iteration: 4397 lambda_n: 0.9009204101462028 Loss: 0.06761914447298073\n",
      "Iteration: 4398 lambda_n: 1.0061056578521732 Loss: 0.06760577892739181\n",
      "Iteration: 4399 lambda_n: 0.9223488024338603 Loss: 0.0675908544274806\n",
      "Iteration: 4400 lambda_n: 0.9512879668340338 Loss: 0.06757717378565826\n",
      "Iteration: 4401 lambda_n: 0.8781963209596072 Loss: 0.06756306531813373\n",
      "Iteration: 4402 lambda_n: 0.8860377257318237 Loss: 0.0675500421440125\n",
      "Iteration: 4403 lambda_n: 0.8877974286217906 Loss: 0.06753690392511168\n",
      "Iteration: 4404 lambda_n: 0.9984492767391331 Loss: 0.06752374086184414\n",
      "Iteration: 4405 lambda_n: 0.9156794280731695 Loss: 0.06750893868912372\n",
      "Iteration: 4406 lambda_n: 0.900991479526668 Loss: 0.06749536498731673\n",
      "Iteration: 4407 lambda_n: 0.9669075168565059 Loss: 0.06748201031328349\n",
      "Iteration: 4408 lambda_n: 0.974523507311175 Loss: 0.06746768004755085\n",
      "Iteration: 4409 lambda_n: 1.004561369610115 Loss: 0.06745323840773644\n",
      "Iteration: 4410 lambda_n: 0.9876277120222711 Loss: 0.0674383532069248\n",
      "Iteration: 4411 lambda_n: 0.926493540805102 Loss: 0.06742372048449093\n",
      "Iteration: 4412 lambda_n: 0.9080233659954586 Loss: 0.06740999493814347\n",
      "Iteration: 4413 lambda_n: 0.8921099200225829 Loss: 0.06739654434113701\n",
      "Iteration: 4414 lambda_n: 1.016799565927411 Loss: 0.06738333074606598\n",
      "Iteration: 4415 lambda_n: 1.0087659259368615 Loss: 0.06736827182864988\n",
      "Iteration: 4416 lambda_n: 1.004022485462288 Loss: 0.06735333351281729\n",
      "Iteration: 4417 lambda_n: 0.8912524769859146 Loss: 0.0673384670449007\n",
      "Iteration: 4418 lambda_n: 0.9767616707116912 Loss: 0.06732527169862969\n",
      "Iteration: 4419 lambda_n: 0.9988033643423617 Loss: 0.06731081179923563\n",
      "Iteration: 4420 lambda_n: 0.9724475093974406 Loss: 0.06729602716234474\n",
      "Iteration: 4421 lambda_n: 0.997385758156482 Loss: 0.0672816341780021\n",
      "Iteration: 4422 lambda_n: 0.9563877631044111 Loss: 0.06726687364749785\n",
      "Iteration: 4423 lambda_n: 1.0259936111555277 Loss: 0.06725272134193944\n",
      "Iteration: 4424 lambda_n: 1.0152509859555203 Loss: 0.06723754064488273\n",
      "Iteration: 4425 lambda_n: 0.9479434556939655 Loss: 0.06722252054430415\n",
      "Iteration: 4426 lambda_n: 0.9409217487377098 Loss: 0.06720849770547395\n",
      "Iteration: 4427 lambda_n: 0.955493924303452 Loss: 0.06719458015131406\n",
      "Iteration: 4428 lambda_n: 1.027991833324886 Loss: 0.06718044849423391\n",
      "Iteration: 4429 lambda_n: 0.899158980257226 Loss: 0.06716524621829792\n",
      "Iteration: 4430 lambda_n: 0.9388331686549959 Loss: 0.06715195054823137\n",
      "Iteration: 4431 lambda_n: 0.8823873672001615 Loss: 0.06713806959529905\n",
      "Iteration: 4432 lambda_n: 0.9643082507017495 Loss: 0.06712502449282082\n",
      "Iteration: 4433 lambda_n: 0.9211046127143009 Loss: 0.06711076969562556\n",
      "Iteration: 4434 lambda_n: 1.0201622897609968 Loss: 0.06709715493627663\n",
      "Iteration: 4435 lambda_n: 0.9931709531733781 Loss: 0.06708207758647529\n",
      "Iteration: 4436 lambda_n: 0.8885809488017112 Loss: 0.0670674007447734\n",
      "Iteration: 4437 lambda_n: 0.9095956769310176 Loss: 0.06705427084491128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4438 lambda_n: 0.9007834784337113 Loss: 0.06704083172781362\n",
      "Iteration: 4439 lambda_n: 0.9454757558588142 Loss: 0.06702752410881749\n",
      "Iteration: 4440 lambda_n: 0.8970623197177622 Loss: 0.06701355762253619\n",
      "Iteration: 4441 lambda_n: 0.9155289702397211 Loss: 0.06700030761450715\n",
      "Iteration: 4442 lambda_n: 0.9190584779906297 Loss: 0.06698678616776199\n",
      "Iteration: 4443 lambda_n: 0.9790716176933794 Loss: 0.06697321393771742\n",
      "Iteration: 4444 lambda_n: 0.955163452444221 Loss: 0.06695875694059791\n",
      "Iteration: 4445 lambda_n: 0.9250458931092592 Loss: 0.06694465444655366\n",
      "Iteration: 4446 lambda_n: 1.0289126795415666 Loss: 0.06693099801134346\n",
      "Iteration: 4447 lambda_n: 0.9582126379749867 Loss: 0.06691580979210066\n",
      "Iteration: 4448 lambda_n: 1.0168480927999495 Loss: 0.06690166672890539\n",
      "Iteration: 4449 lambda_n: 0.9381161391356495 Loss: 0.06688665981646029\n",
      "Iteration: 4450 lambda_n: 0.880067254861954 Loss: 0.06687281631845302\n",
      "Iteration: 4451 lambda_n: 0.8852427361369043 Loss: 0.06685983071041532\n",
      "Iteration: 4452 lambda_n: 0.8821972288255465 Loss: 0.06684676998491718\n",
      "Iteration: 4453 lambda_n: 0.9776438063296204 Loss: 0.06683375543815055\n",
      "Iteration: 4454 lambda_n: 1.0294416181465447 Loss: 0.06681933427217122\n",
      "Iteration: 4455 lambda_n: 1.00966219510907 Loss: 0.06680415068906527\n",
      "Iteration: 4456 lambda_n: 0.9723012714769355 Loss: 0.06678926048629434\n",
      "Iteration: 4457 lambda_n: 1.0224377996237317 Loss: 0.0667749228141385\n",
      "Iteration: 4458 lambda_n: 0.8965225151608657 Loss: 0.0667598474520642\n",
      "Iteration: 4459 lambda_n: 0.880071842820265 Loss: 0.0667466300324715\n",
      "Iteration: 4460 lambda_n: 1.028797110592091 Loss: 0.066733656396821\n",
      "Iteration: 4461 lambda_n: 0.9662166550745248 Loss: 0.06671849188406682\n",
      "Iteration: 4462 lambda_n: 0.9720937710093303 Loss: 0.06670425135581308\n",
      "Iteration: 4463 lambda_n: 0.9076931297984362 Loss: 0.06668992571698827\n",
      "Iteration: 4464 lambda_n: 0.9426877924242354 Loss: 0.06667655051261186\n",
      "Iteration: 4465 lambda_n: 0.927340858516145 Loss: 0.06666266104510879\n",
      "Iteration: 4466 lambda_n: 0.9429811990714229 Loss: 0.0666489990884102\n",
      "Iteration: 4467 lambda_n: 1.0231854073058544 Loss: 0.06663510812549951\n",
      "Iteration: 4468 lambda_n: 0.8762989603123819 Loss: 0.06662003729267851\n",
      "Iteration: 4469 lambda_n: 0.9034306118070592 Loss: 0.066607131338647\n",
      "Iteration: 4470 lambda_n: 0.9728228568664653 Loss: 0.06659382708443747\n",
      "Iteration: 4471 lambda_n: 0.915690474419718 Loss: 0.06657950239687048\n",
      "Iteration: 4472 lambda_n: 0.9395112404742678 Loss: 0.06656602036645759\n",
      "Iteration: 4473 lambda_n: 0.9353849284769158 Loss: 0.0665521890136047\n",
      "Iteration: 4474 lambda_n: 0.9085867239782992 Loss: 0.06653841981676697\n",
      "Iteration: 4475 lambda_n: 0.9717891795350916 Loss: 0.06652504644618588\n",
      "Iteration: 4476 lambda_n: 0.9873362831079188 Loss: 0.06651074427345131\n",
      "Iteration: 4477 lambda_n: 0.9408776811136096 Loss: 0.06649621484314415\n",
      "Iteration: 4478 lambda_n: 0.9117727437263853 Loss: 0.06648237054846898\n",
      "Iteration: 4479 lambda_n: 0.8921455409304769 Loss: 0.06646895587062916\n",
      "Iteration: 4480 lambda_n: 1.0184724572923012 Loss: 0.0664558312585534\n",
      "Iteration: 4481 lambda_n: 0.9777804313430299 Loss: 0.06644084977655287\n",
      "Iteration: 4482 lambda_n: 0.8980319976228046 Loss: 0.06642646843757477\n",
      "Iteration: 4483 lambda_n: 0.8766789006673537 Loss: 0.06641326140974789\n",
      "Iteration: 4484 lambda_n: 0.9913305870513216 Loss: 0.06640036966838331\n",
      "Iteration: 4485 lambda_n: 0.9535263397173404 Loss: 0.06638579343940931\n",
      "Iteration: 4486 lambda_n: 0.9381228248039295 Loss: 0.06637177456924377\n",
      "Iteration: 4487 lambda_n: 0.9443581505514574 Loss: 0.06635798359535013\n",
      "Iteration: 4488 lambda_n: 0.9410527847897528 Loss: 0.06634410239245948\n",
      "Iteration: 4489 lambda_n: 1.0158576198671718 Loss: 0.06633027120746235\n",
      "Iteration: 4490 lambda_n: 0.9719759188931406 Loss: 0.06631534217592767\n",
      "Iteration: 4491 lambda_n: 1.0189551934740402 Loss: 0.06630105959113979\n",
      "Iteration: 4492 lambda_n: 1.0281528841874559 Loss: 0.06628608831236805\n",
      "Iteration: 4493 lambda_n: 0.9169693555862245 Loss: 0.06627098359471045\n",
      "Iteration: 4494 lambda_n: 0.8946809925820073 Loss: 0.06625751373286534\n",
      "Iteration: 4495 lambda_n: 0.9100846583436931 Loss: 0.06624437258834127\n",
      "Iteration: 4496 lambda_n: 1.0048093086179481 Loss: 0.06623100652185517\n",
      "Iteration: 4497 lambda_n: 0.9113692708515849 Loss: 0.0662162508246572\n",
      "Iteration: 4498 lambda_n: 1.0054534121791887 Loss: 0.06620286871752984\n",
      "Iteration: 4499 lambda_n: 0.9356986668125415 Loss: 0.06618810668205295\n",
      "Iteration: 4500 lambda_n: 0.9528606672236032 Loss: 0.06617437025698866\n",
      "Iteration: 4501 lambda_n: 0.9595953294727075 Loss: 0.06616038334482881\n",
      "Iteration: 4502 lambda_n: 0.957185098172313 Loss: 0.06614629906303218\n",
      "Iteration: 4503 lambda_n: 1.012119695178833 Loss: 0.06613225164492804\n",
      "Iteration: 4504 lambda_n: 0.951528595361895 Loss: 0.06611739963525803\n",
      "Iteration: 4505 lambda_n: 0.9074085043607224 Loss: 0.06610343826738695\n",
      "Iteration: 4506 lambda_n: 0.9435000901055111 Loss: 0.066090125625177\n",
      "Iteration: 4507 lambda_n: 0.9422448222274141 Loss: 0.06607628489853233\n",
      "Iteration: 4508 lambda_n: 1.0147927376549173 Loss: 0.06606246402972162\n",
      "Iteration: 4509 lambda_n: 1.0029954601715139 Loss: 0.06604758063841709\n",
      "Iteration: 4510 lambda_n: 0.9364436651149158 Loss: 0.06603287191697624\n",
      "Iteration: 4511 lambda_n: 0.9077407224304102 Loss: 0.06601914064288524\n",
      "Iteration: 4512 lambda_n: 0.9740197040963974 Loss: 0.06600583160905071\n",
      "Iteration: 4513 lambda_n: 1.0065925487634788 Loss: 0.06599155230193424\n",
      "Iteration: 4514 lambda_n: 0.9079932016465753 Loss: 0.06597679709349308\n",
      "Iteration: 4515 lambda_n: 1.02755222505205 Loss: 0.06596348862920476\n",
      "Iteration: 4516 lambda_n: 0.9070270869100417 Loss: 0.06594842940469922\n",
      "Iteration: 4517 lambda_n: 0.9397033614393893 Loss: 0.06593513796198017\n",
      "Iteration: 4518 lambda_n: 1.0022738428724567 Loss: 0.06592136910036174\n",
      "Iteration: 4519 lambda_n: 0.9651961305449871 Loss: 0.065906685020549\n",
      "Iteration: 4520 lambda_n: 0.897300803065798 Loss: 0.06589254570855675\n",
      "Iteration: 4521 lambda_n: 0.9119629354219049 Loss: 0.06587940237223132\n",
      "Iteration: 4522 lambda_n: 1.0025186446733265 Loss: 0.06586604561770826\n",
      "Iteration: 4523 lambda_n: 1.0160017987284249 Loss: 0.06585136413582544\n",
      "Iteration: 4524 lambda_n: 0.9295156680438551 Loss: 0.06583648687476147\n",
      "Iteration: 4525 lambda_n: 0.8810261666054512 Loss: 0.06582287750659102\n",
      "Iteration: 4526 lambda_n: 0.9631548092811018 Loss: 0.06580997939652025\n",
      "Iteration: 4527 lambda_n: 0.9148612650958026 Loss: 0.06579588038542437\n",
      "Iteration: 4528 lambda_n: 1.0269418594707067 Loss: 0.06578248972071588\n",
      "Iteration: 4529 lambda_n: 0.9618768605818968 Loss: 0.06576746018246113\n",
      "Iteration: 4530 lambda_n: 0.99743010681274 Loss: 0.06575338445514255\n",
      "Iteration: 4531 lambda_n: 0.9804599809838768 Loss: 0.0657387900568939\n",
      "Iteration: 4532 lambda_n: 0.9618343293158754 Loss: 0.06572444555646101\n",
      "Iteration: 4533 lambda_n: 0.898734566581926 Loss: 0.06571037508965652\n",
      "Iteration: 4534 lambda_n: 1.0035189084984608 Loss: 0.0656972290696223\n",
      "Iteration: 4535 lambda_n: 0.9846128135857348 Loss: 0.06568255190735323\n",
      "Iteration: 4536 lambda_n: 0.9487332724893512 Loss: 0.06566815286854047\n",
      "Iteration: 4537 lambda_n: 0.9255695820567126 Loss: 0.06565428004272347\n",
      "Iteration: 4538 lambda_n: 0.9199967985744385 Loss: 0.06564074735425507\n",
      "Iteration: 4539 lambda_n: 0.9360926649889507 Loss: 0.06562729754186145\n",
      "Iteration: 4540 lambda_n: 0.8927680645519124 Loss: 0.06561361384682074\n",
      "Iteration: 4541 lambda_n: 0.9675946889056546 Loss: 0.06560056481088046\n",
      "Iteration: 4542 lambda_n: 1.0253329180089583 Loss: 0.0655864235615786\n",
      "Iteration: 4543 lambda_n: 0.9566902163156684 Loss: 0.06557144015921515\n",
      "Iteration: 4544 lambda_n: 0.8852281572078925 Loss: 0.0655574614114623\n",
      "Iteration: 4545 lambda_n: 1.028852885880433 Loss: 0.06554452818280596\n",
      "Iteration: 4546 lambda_n: 0.9296159212085995 Loss: 0.06552949821101955\n",
      "Iteration: 4547 lambda_n: 0.9961341777378407 Loss: 0.06551591944478162\n",
      "Iteration: 4548 lambda_n: 0.9770443432737649 Loss: 0.06550137063858906\n",
      "Iteration: 4549 lambda_n: 0.9118353122698757 Loss: 0.06548710223769003\n",
      "Iteration: 4550 lambda_n: 0.8976868805485533 Loss: 0.0654737875506578\n",
      "Iteration: 4551 lambda_n: 1.0157450641902213 Loss: 0.06546068080351491\n",
      "Iteration: 4552 lambda_n: 1.001800941883064 Loss: 0.06544585194391934\n",
      "Iteration: 4553 lambda_n: 1.0096800194538214 Loss: 0.06543122832761818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4554 lambda_n: 0.9103823552478109 Loss: 0.06541649137935059\n",
      "Iteration: 4555 lambda_n: 0.9831977593217699 Loss: 0.0654032051961445\n",
      "Iteration: 4556 lambda_n: 1.011452841767346 Loss: 0.0653888578813778\n",
      "Iteration: 4557 lambda_n: 0.9168883058646524 Loss: 0.06537409992651896\n",
      "Iteration: 4558 lambda_n: 0.8906118040388286 Loss: 0.06536072321690993\n",
      "Iteration: 4559 lambda_n: 0.88627134498554 Loss: 0.06534773119810403\n",
      "Iteration: 4560 lambda_n: 1.0278300989241471 Loss: 0.0653348038043808\n",
      "Iteration: 4561 lambda_n: 0.9425178929337059 Loss: 0.06531981322761703\n",
      "Iteration: 4562 lambda_n: 0.9381245358371845 Loss: 0.06530606844727425\n",
      "Iteration: 4563 lambda_n: 0.8941010412236181 Loss: 0.06529238920237018\n",
      "Iteration: 4564 lambda_n: 0.8887343339173549 Loss: 0.0652793532485954\n",
      "Iteration: 4565 lambda_n: 0.9048446038214084 Loss: 0.06526639685977019\n",
      "Iteration: 4566 lambda_n: 0.8810658922480452 Loss: 0.06525320695862134\n",
      "Iteration: 4567 lambda_n: 0.9784557889904347 Loss: 0.06524036499018639\n",
      "Iteration: 4568 lambda_n: 0.9225546234234365 Loss: 0.06522610503037721\n",
      "Iteration: 4569 lambda_n: 0.9313932537736791 Loss: 0.06521266123401297\n",
      "Iteration: 4570 lambda_n: 0.8871532305436536 Loss: 0.06519909007794766\n",
      "Iteration: 4571 lambda_n: 1.0249056314007825 Loss: 0.06518616488182923\n",
      "Iteration: 4572 lambda_n: 1.0101614254172713 Loss: 0.06517123436378941\n",
      "Iteration: 4573 lambda_n: 1.0253016812880207 Loss: 0.06515652035215834\n",
      "Iteration: 4574 lambda_n: 0.9887087266745693 Loss: 0.0651415875511877\n",
      "Iteration: 4575 lambda_n: 1.0239815320690782 Loss: 0.06512718936642202\n",
      "Iteration: 4576 lambda_n: 0.8920069347544821 Loss: 0.06511227924063079\n",
      "Iteration: 4577 lambda_n: 1.0297103937382754 Loss: 0.06509929222040177\n",
      "Iteration: 4578 lambda_n: 1.0091757861536619 Loss: 0.06508430198483714\n",
      "Iteration: 4579 lambda_n: 0.9707563648786374 Loss: 0.06506961241034705\n",
      "Iteration: 4580 lambda_n: 0.879890258462569 Loss: 0.06505548368094001\n",
      "Iteration: 4581 lambda_n: 0.8862208964439424 Loss: 0.06504267881558215\n",
      "Iteration: 4582 lambda_n: 1.000279765192232 Loss: 0.06502978313462221\n",
      "Iteration: 4583 lambda_n: 1.007593718616835 Loss: 0.06501522932870969\n",
      "Iteration: 4584 lambda_n: 0.8888152449857154 Loss: 0.06500057080534352\n",
      "Iteration: 4585 lambda_n: 0.9213695643061034 Loss: 0.06498764169480704\n",
      "Iteration: 4586 lambda_n: 0.9971226422281969 Loss: 0.06497424054858339\n",
      "Iteration: 4587 lambda_n: 0.9904110279353825 Loss: 0.06495973914409364\n",
      "Iteration: 4588 lambda_n: 0.9163804585186023 Loss: 0.06494533682513476\n",
      "Iteration: 4589 lambda_n: 0.9346658966995096 Loss: 0.06493201256105405\n",
      "Iteration: 4590 lambda_n: 0.9457022547970687 Loss: 0.06491842392416614\n",
      "Iteration: 4591 lambda_n: 0.9972528915345603 Loss: 0.06490467633935672\n",
      "Iteration: 4592 lambda_n: 0.9047072212553289 Loss: 0.06489018100384342\n",
      "Iteration: 4593 lambda_n: 0.9738818460303674 Loss: 0.06487703229877792\n",
      "Iteration: 4594 lambda_n: 0.9055081113453758 Loss: 0.06486287977720928\n",
      "Iteration: 4595 lambda_n: 1.005168939367598 Loss: 0.06484972230606881\n",
      "Iteration: 4596 lambda_n: 0.9241814640468433 Loss: 0.06483511833590032\n",
      "Iteration: 4597 lambda_n: 0.985780434457604 Loss: 0.0648216925305976\n",
      "Iteration: 4598 lambda_n: 0.9531482636007802 Loss: 0.0648073734534581\n",
      "Iteration: 4599 lambda_n: 0.8890506377242539 Loss: 0.06479352994296785\n",
      "Iteration: 4600 lambda_n: 1.0246720941845795 Loss: 0.06478061877252075\n",
      "Iteration: 4601 lambda_n: 0.9081863053494694 Loss: 0.06476573970824535\n",
      "Iteration: 4602 lambda_n: 0.9435060738026767 Loss: 0.06475255360034604\n",
      "Iteration: 4603 lambda_n: 0.9188965893747596 Loss: 0.06473885615935494\n",
      "Iteration: 4604 lambda_n: 1.0272427145027325 Loss: 0.06472551744071225\n",
      "Iteration: 4605 lambda_n: 0.9757807740389638 Loss: 0.06471060766262746\n",
      "Iteration: 4606 lambda_n: 0.9588405266069304 Loss: 0.0646964464821981\n",
      "Iteration: 4607 lambda_n: 0.8813766070277951 Loss: 0.06468253272591193\n",
      "Iteration: 4608 lambda_n: 0.9353517988111979 Loss: 0.06466974442974519\n",
      "Iteration: 4609 lambda_n: 0.9863356947718905 Loss: 0.06465617442704763\n",
      "Iteration: 4610 lambda_n: 0.8887993595635524 Loss: 0.06464186636624412\n",
      "Iteration: 4611 lambda_n: 0.9812101885371638 Loss: 0.0646289746145745\n",
      "Iteration: 4612 lambda_n: 0.9732961992149779 Loss: 0.06461474403461354\n",
      "Iteration: 4613 lambda_n: 0.9671773380460711 Loss: 0.06460062985297817\n",
      "Iteration: 4614 lambda_n: 0.8959162681111502 Loss: 0.06458660600364187\n",
      "Iteration: 4615 lambda_n: 0.9576839720219669 Loss: 0.06457361684809283\n",
      "Iteration: 4616 lambda_n: 0.9103754255006913 Loss: 0.06455973368736252\n",
      "Iteration: 4617 lambda_n: 1.0090886725993096 Loss: 0.06454653779232869\n",
      "Iteration: 4618 lambda_n: 0.903065564934343 Loss: 0.06453191270176527\n",
      "Iteration: 4619 lambda_n: 0.9743935390874207 Loss: 0.06451882572005897\n",
      "Iteration: 4620 lambda_n: 0.9124743581634659 Loss: 0.06450470663619941\n",
      "Iteration: 4621 lambda_n: 0.9008810677727509 Loss: 0.0644914862428934\n",
      "Iteration: 4622 lambda_n: 0.9362414481032824 Loss: 0.06447843521770666\n",
      "Iteration: 4623 lambda_n: 0.9829732137632387 Loss: 0.06446487340095791\n",
      "Iteration: 4624 lambda_n: 0.9060191936292187 Loss: 0.06445063627272418\n",
      "Iteration: 4625 lambda_n: 0.9888474697396128 Loss: 0.0644375151948068\n",
      "Iteration: 4626 lambda_n: 0.8996406486915006 Loss: 0.06442319619427939\n",
      "Iteration: 4627 lambda_n: 0.9386304965516428 Loss: 0.06441017041190411\n",
      "Iteration: 4628 lambda_n: 0.8768736430360496 Loss: 0.06439658158177497\n",
      "Iteration: 4629 lambda_n: 0.9337214435161488 Loss: 0.0643838881922898\n",
      "Iteration: 4630 lambda_n: 0.8895742583908599 Loss: 0.06437037334180248\n",
      "Iteration: 4631 lambda_n: 0.9837095499657229 Loss: 0.06435749888101994\n",
      "Iteration: 4632 lambda_n: 0.9756996806141 Loss: 0.06434326362153725\n",
      "Iteration: 4633 lambda_n: 0.9638062674383842 Loss: 0.06432914591899373\n",
      "Iteration: 4634 lambda_n: 0.9377704479515796 Loss: 0.06431520191642698\n",
      "Iteration: 4635 lambda_n: 1.00595554867017 Loss: 0.06430163612805621\n",
      "Iteration: 4636 lambda_n: 0.9843001436195795 Loss: 0.06428708565937423\n",
      "Iteration: 4637 lambda_n: 0.9697225308080936 Loss: 0.06427285011216718\n",
      "Iteration: 4638 lambda_n: 1.0061679997632504 Loss: 0.0642588270313753\n",
      "Iteration: 4639 lambda_n: 0.9447793882000693 Loss: 0.06424427863249942\n",
      "Iteration: 4640 lambda_n: 0.9195600510600613 Loss: 0.06423061945852232\n",
      "Iteration: 4641 lambda_n: 0.9011926878062135 Loss: 0.06421732637663025\n",
      "Iteration: 4642 lambda_n: 0.9075938901682858 Loss: 0.06420430023167603\n",
      "Iteration: 4643 lambda_n: 0.9754783683761875 Loss: 0.06419118298269572\n",
      "Iteration: 4644 lambda_n: 0.9927812907660545 Loss: 0.06417708620516042\n",
      "Iteration: 4645 lambda_n: 0.9591242973699224 Loss: 0.06416274107399124\n",
      "Iteration: 4646 lambda_n: 0.9773646795634131 Loss: 0.06414888389124274\n",
      "Iteration: 4647 lambda_n: 0.9730241296405768 Loss: 0.06413476481806342\n",
      "Iteration: 4648 lambda_n: 0.9374930545162596 Loss: 0.06412071009629619\n",
      "Iteration: 4649 lambda_n: 0.9423046212550908 Loss: 0.06410717015468118\n",
      "Iteration: 4650 lambda_n: 0.8868227976760361 Loss: 0.06409356226037641\n",
      "Iteration: 4651 lambda_n: 0.9683476149674611 Loss: 0.06408075699441737\n",
      "Iteration: 4652 lambda_n: 0.9073663507092703 Loss: 0.06406677611402462\n",
      "Iteration: 4653 lambda_n: 0.8908789252499312 Loss: 0.06405367715551438\n",
      "Iteration: 4654 lambda_n: 1.0093380117615882 Loss: 0.06404081760885189\n",
      "Iteration: 4655 lambda_n: 0.9347672875916744 Loss: 0.0640262498140061\n",
      "Iteration: 4656 lambda_n: 1.008030804049158 Loss: 0.06401275988506917\n",
      "Iteration: 4657 lambda_n: 1.0292841446479715 Loss: 0.06399821437438416\n",
      "Iteration: 4658 lambda_n: 0.9439993852633216 Loss: 0.06398336401612233\n",
      "Iteration: 4659 lambda_n: 0.8841357231941469 Loss: 0.06396974576146511\n",
      "Iteration: 4660 lambda_n: 0.9879951162587167 Loss: 0.06395699252061426\n",
      "Iteration: 4661 lambda_n: 0.9310676722390245 Loss: 0.06394274277327094\n",
      "Iteration: 4662 lambda_n: 1.024036964204668 Loss: 0.06392931564805295\n",
      "Iteration: 4663 lambda_n: 1.0051562639130593 Loss: 0.06391454954450766\n",
      "Iteration: 4664 lambda_n: 0.9302319620190993 Loss: 0.06390005747795731\n",
      "Iteration: 4665 lambda_n: 1.028118986412775 Loss: 0.06388664722788734\n",
      "Iteration: 4666 lambda_n: 0.9784636573084821 Loss: 0.06387182760032008\n",
      "Iteration: 4667 lambda_n: 0.9204008722780915 Loss: 0.06385772544330189\n",
      "Iteration: 4668 lambda_n: 0.896756289126491 Loss: 0.06384446165397765\n",
      "Iteration: 4669 lambda_n: 1.0210362307240681 Loss: 0.06383154003557233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4670 lambda_n: 0.9038118053933575 Loss: 0.06381682935166486\n",
      "Iteration: 4671 lambda_n: 1.0291303278413084 Loss: 0.06380380912152402\n",
      "Iteration: 4672 lambda_n: 0.9590769314894657 Loss: 0.06378898531336591\n",
      "Iteration: 4673 lambda_n: 0.9859020231968869 Loss: 0.0637751722483919\n",
      "Iteration: 4674 lambda_n: 0.9887955815384556 Loss: 0.06376097452516467\n",
      "Iteration: 4675 lambda_n: 0.9003541207523622 Loss: 0.06374673685360861\n",
      "Iteration: 4676 lambda_n: 1.0103339022084514 Loss: 0.0637337741518395\n",
      "Iteration: 4677 lambda_n: 1.0000724666343597 Loss: 0.0637192297366174\n",
      "Iteration: 4678 lambda_n: 0.9193983051271515 Loss: 0.0637048348168184\n",
      "Iteration: 4679 lambda_n: 0.9326003665521735 Loss: 0.0636916026707681\n",
      "Iteration: 4680 lambda_n: 0.9316103278441826 Loss: 0.06367818204497962\n",
      "Iteration: 4681 lambda_n: 0.991996905799319 Loss: 0.06366477720250516\n",
      "Iteration: 4682 lambda_n: 0.9225984885331149 Loss: 0.06365050515219368\n",
      "Iteration: 4683 lambda_n: 1.0174004813718218 Loss: 0.06363723311497603\n",
      "Iteration: 4684 lambda_n: 0.9988357126792987 Loss: 0.06362259905201065\n",
      "Iteration: 4685 lambda_n: 0.9872040639726912 Loss: 0.0636082338061464\n",
      "Iteration: 4686 lambda_n: 0.9178416863392086 Loss: 0.06359403758589915\n",
      "Iteration: 4687 lambda_n: 0.894025665250328 Loss: 0.06358084036505632\n",
      "Iteration: 4688 lambda_n: 1.0068465815174426 Loss: 0.06356798702254592\n",
      "Iteration: 4689 lambda_n: 0.9729309696001519 Loss: 0.06355351336163195\n",
      "Iteration: 4690 lambda_n: 0.9983823356835012 Loss: 0.06353952895868559\n",
      "Iteration: 4691 lambda_n: 0.9280884675419728 Loss: 0.06352518048184326\n",
      "Iteration: 4692 lambda_n: 1.0122299884531292 Loss: 0.06351184384132019\n",
      "Iteration: 4693 lambda_n: 1.0269971456485951 Loss: 0.06349729983505045\n",
      "Iteration: 4694 lambda_n: 1.0128969532321848 Loss: 0.06348254551654206\n",
      "Iteration: 4695 lambda_n: 0.8913562097628464 Loss: 0.06346799561075353\n",
      "Iteration: 4696 lambda_n: 0.990070346420763 Loss: 0.06345519311017772\n",
      "Iteration: 4697 lambda_n: 1.0065673879048014 Loss: 0.06344097444628327\n",
      "Iteration: 4698 lambda_n: 0.9299941729319596 Loss: 0.06342652065937912\n",
      "Iteration: 4699 lambda_n: 0.9605364651986266 Loss: 0.06341316803475194\n",
      "Iteration: 4700 lambda_n: 0.9547613390848241 Loss: 0.06339937851556296\n",
      "Iteration: 4701 lambda_n: 0.9765290777504657 Loss: 0.06338567354109896\n",
      "Iteration: 4702 lambda_n: 0.9196404447372312 Loss: 0.06337165779374265\n",
      "Iteration: 4703 lambda_n: 0.8956850114349514 Loss: 0.06335846011002841\n",
      "Iteration: 4704 lambda_n: 0.9547801472272818 Loss: 0.0633456076666725\n",
      "Iteration: 4705 lambda_n: 0.9786922923654209 Loss: 0.0633319088346238\n",
      "Iteration: 4706 lambda_n: 1.0188036436882129 Loss: 0.06331786861828567\n",
      "Iteration: 4707 lambda_n: 0.9247056402986595 Loss: 0.06330325479619675\n",
      "Iteration: 4708 lambda_n: 0.9047185551338919 Loss: 0.06328999234105862\n",
      "Iteration: 4709 lambda_n: 0.9234847551970905 Loss: 0.06327701803617314\n",
      "Iteration: 4710 lambda_n: 0.9586666457655977 Loss: 0.06326377612956448\n",
      "Iteration: 4711 lambda_n: 0.8865449980425328 Loss: 0.06325003137167766\n",
      "Iteration: 4712 lambda_n: 0.9278927580175532 Loss: 0.06323732212244261\n",
      "Iteration: 4713 lambda_n: 0.9276815424800962 Loss: 0.06322402164093577\n",
      "Iteration: 4714 lambda_n: 0.9448610501385541 Loss: 0.06321072573930686\n",
      "Iteration: 4715 lambda_n: 0.9423345622886333 Loss: 0.0631971852104448\n",
      "Iteration: 4716 lambda_n: 1.026451329574118 Loss: 0.06318368249352081\n",
      "Iteration: 4717 lambda_n: 0.9140236873787995 Loss: 0.06316897629200036\n",
      "Iteration: 4718 lambda_n: 0.9717599300211578 Loss: 0.06315588247078402\n",
      "Iteration: 4719 lambda_n: 0.9550654375193158 Loss: 0.06314196320768443\n",
      "Iteration: 4720 lambda_n: 0.8794931019897964 Loss: 0.0631282847381338\n",
      "Iteration: 4721 lambda_n: 0.9312141303607862 Loss: 0.06311569007883006\n",
      "Iteration: 4722 lambda_n: 0.9622169187107583 Loss: 0.06310235628311109\n",
      "Iteration: 4723 lambda_n: 0.893038082990972 Loss: 0.0630885802185823\n",
      "Iteration: 4724 lambda_n: 0.9874746449142862 Loss: 0.06307579609109223\n",
      "Iteration: 4725 lambda_n: 0.8900800293147738 Loss: 0.06306166175867521\n",
      "Iteration: 4726 lambda_n: 0.9939782899144988 Loss: 0.06304892301359717\n",
      "Iteration: 4727 lambda_n: 0.9214566297361969 Loss: 0.06303469898707971\n",
      "Iteration: 4728 lambda_n: 0.965811105457913 Loss: 0.06302151436513734\n",
      "Iteration: 4729 lambda_n: 0.9234538348010046 Loss: 0.06300769675716554\n",
      "Iteration: 4730 lambda_n: 0.9728536187892739 Loss: 0.06299448673208372\n",
      "Iteration: 4731 lambda_n: 0.9759725537819346 Loss: 0.06298057172224289\n",
      "Iteration: 4732 lambda_n: 1.0168189816558086 Loss: 0.06296661383487843\n",
      "Iteration: 4733 lambda_n: 0.9378811872774266 Loss: 0.06295207362939126\n",
      "Iteration: 4734 lambda_n: 0.9972689391409004 Loss: 0.06293866388403253\n",
      "Iteration: 4735 lambda_n: 0.9069098877678299 Loss: 0.06292440677950288\n",
      "Iteration: 4736 lambda_n: 0.9995053115024967 Loss: 0.0629114430396866\n",
      "Iteration: 4737 lambda_n: 0.890726586581145 Loss: 0.0628971574446064\n",
      "Iteration: 4738 lambda_n: 0.9382983060701051 Loss: 0.06288442812820558\n",
      "Iteration: 4739 lambda_n: 0.9328367291579963 Loss: 0.06287102053799168\n",
      "Iteration: 4740 lambda_n: 0.9899417169032572 Loss: 0.06285769258829071\n",
      "Iteration: 4741 lambda_n: 0.9908480714672794 Loss: 0.06284355049231741\n",
      "Iteration: 4742 lambda_n: 0.9596041632797901 Loss: 0.06282939724805293\n",
      "Iteration: 4743 lambda_n: 1.0176840665461844 Loss: 0.06281569200833648\n",
      "Iteration: 4744 lambda_n: 0.9294772296996022 Loss: 0.06280115910811854\n",
      "Iteration: 4745 lambda_n: 0.977485362662235 Loss: 0.06278788749688471\n",
      "Iteration: 4746 lambda_n: 0.9260214632083417 Loss: 0.06277393211112652\n",
      "Iteration: 4747 lambda_n: 0.8928867434129067 Loss: 0.06276071308795905\n",
      "Iteration: 4748 lambda_n: 0.9378413816819965 Loss: 0.06274796856019006\n",
      "Iteration: 4749 lambda_n: 0.881551812157656 Loss: 0.06273458395785537\n",
      "Iteration: 4750 lambda_n: 1.0043633507668634 Loss: 0.0627220041821862\n",
      "Iteration: 4751 lambda_n: 1.0000810155915285 Loss: 0.06270767362718646\n",
      "Iteration: 4752 lambda_n: 0.8855014103214627 Loss: 0.06269340602318083\n",
      "Iteration: 4753 lambda_n: 0.8855030773522995 Loss: 0.06268077460502455\n",
      "Iteration: 4754 lambda_n: 1.0151712191852704 Loss: 0.06266814461159011\n",
      "Iteration: 4755 lambda_n: 1.00665819915539 Loss: 0.06265366693478784\n",
      "Iteration: 4756 lambda_n: 0.8923315410095356 Loss: 0.06263931254709253\n",
      "Iteration: 4757 lambda_n: 0.9949513713235146 Loss: 0.06262658996271453\n",
      "Iteration: 4758 lambda_n: 0.9552730705913723 Loss: 0.062612405994989\n",
      "Iteration: 4759 lambda_n: 0.9301581370568599 Loss: 0.06259878940500986\n",
      "Iteration: 4760 lambda_n: 0.9824237468011867 Loss: 0.06258553243260113\n",
      "Iteration: 4761 lambda_n: 0.9698157370717786 Loss: 0.06257153229329922\n",
      "Iteration: 4762 lambda_n: 0.894079404388813 Loss: 0.06255771358329199\n",
      "Iteration: 4763 lambda_n: 1.0099712143388855 Loss: 0.06254497557321069\n",
      "Iteration: 4764 lambda_n: 0.8965335739896713 Loss: 0.06253058823121434\n",
      "Iteration: 4765 lambda_n: 0.9649784707239109 Loss: 0.06251781843226167\n",
      "Iteration: 4766 lambda_n: 0.9394908650696774 Loss: 0.06250407540777592\n",
      "Iteration: 4767 lambda_n: 0.9857032127769522 Loss: 0.06249069703847247\n",
      "Iteration: 4768 lambda_n: 1.0224272460064268 Loss: 0.06247666237206395\n",
      "Iteration: 4769 lambda_n: 0.9682638591036042 Loss: 0.06246210673432191\n",
      "Iteration: 4770 lambda_n: 0.8985942074091697 Loss: 0.06244832398405478\n",
      "Iteration: 4771 lambda_n: 0.9964334553561591 Loss: 0.06243553451241773\n",
      "Iteration: 4772 lambda_n: 0.9262491005066406 Loss: 0.06242135428179988\n",
      "Iteration: 4773 lambda_n: 0.9465916834210497 Loss: 0.062408174508347035\n",
      "Iteration: 4774 lambda_n: 0.9247030832068615 Loss: 0.06239470693412194\n",
      "Iteration: 4775 lambda_n: 0.8897118897646875 Loss: 0.06238155239776518\n",
      "Iteration: 4776 lambda_n: 0.9289046498339898 Loss: 0.062368897146666304\n",
      "Iteration: 4777 lambda_n: 0.9625090449159756 Loss: 0.062355686000590146\n",
      "Iteration: 4778 lambda_n: 0.9301037201704355 Loss: 0.0623419986295526\n",
      "Iteration: 4779 lambda_n: 0.9561624262038213 Loss: 0.06232877372999349\n",
      "Iteration: 4780 lambda_n: 0.907495366426424 Loss: 0.062315180001110124\n",
      "Iteration: 4781 lambda_n: 1.0008243007229087 Loss: 0.06230227975857234\n",
      "Iteration: 4782 lambda_n: 1.0018618824992622 Loss: 0.06228805461931073\n",
      "Iteration: 4783 lambda_n: 0.9676699168076027 Loss: 0.06227381661897851\n",
      "Iteration: 4784 lambda_n: 0.9452662016751591 Loss: 0.06226006633240811\n",
      "Iteration: 4785 lambda_n: 0.9960010090205994 Loss: 0.06224663609805986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4786 lambda_n: 0.8844001797857667 Loss: 0.062232486850581865\n",
      "Iteration: 4787 lambda_n: 1.0267028485555925 Loss: 0.062219924578903026\n",
      "Iteration: 4788 lambda_n: 0.949959485454014 Loss: 0.062205342849747436\n",
      "Iteration: 4789 lambda_n: 1.0217608978607 Loss: 0.06219185283938472\n",
      "Iteration: 4790 lambda_n: 1.0139357284283854 Loss: 0.06217734510704574\n",
      "Iteration: 4791 lambda_n: 0.9559788240455709 Loss: 0.062162950433010795\n",
      "Iteration: 4792 lambda_n: 1.014804709087224 Loss: 0.062149380344411254\n",
      "Iteration: 4793 lambda_n: 0.9005735001629287 Loss: 0.062134977116516964\n",
      "Iteration: 4794 lambda_n: 0.9625551793270612 Loss: 0.062122196817631994\n",
      "Iteration: 4795 lambda_n: 0.9986321544934176 Loss: 0.06210853861770277\n",
      "Iteration: 4796 lambda_n: 1.013813804773495 Loss: 0.062094370359266564\n",
      "Iteration: 4797 lambda_n: 0.9279735086448668 Loss: 0.06207998864409876\n",
      "Iteration: 4798 lambda_n: 0.9046577065800891 Loss: 0.062066826349240246\n",
      "Iteration: 4799 lambda_n: 1.0072145562497807 Loss: 0.06205399633852578\n",
      "Iteration: 4800 lambda_n: 0.8881093430040059 Loss: 0.062039713678830245\n",
      "Iteration: 4801 lambda_n: 0.9297184394041571 Loss: 0.06202712157453832\n",
      "Iteration: 4802 lambda_n: 0.8797977669235179 Loss: 0.06201394112166909\n",
      "Iteration: 4803 lambda_n: 0.9369021186699611 Loss: 0.062001469901332165\n",
      "Iteration: 4804 lambda_n: 1.0095447660027057 Loss: 0.061988190842612596\n",
      "Iteration: 4805 lambda_n: 0.9860076400352348 Loss: 0.06197388406597481\n",
      "Iteration: 4806 lambda_n: 0.9442125935992255 Loss: 0.06195991272314149\n",
      "Iteration: 4807 lambda_n: 0.9333741111187965 Loss: 0.06194653533966727\n",
      "Iteration: 4808 lambda_n: 0.9643177183106361 Loss: 0.061933313187246646\n",
      "Iteration: 4809 lambda_n: 0.9837871816831705 Loss: 0.06191965443744844\n",
      "Iteration: 4810 lambda_n: 0.9196136305819004 Loss: 0.061905721751908956\n",
      "Iteration: 4811 lambda_n: 0.9548420284072182 Loss: 0.061892699586507276\n",
      "Iteration: 4812 lambda_n: 0.954628902641023 Loss: 0.06187918028403462\n",
      "Iteration: 4813 lambda_n: 0.9287533005429749 Loss: 0.06186566574573393\n",
      "Iteration: 4814 lambda_n: 1.000632622731017 Loss: 0.06185251920161758\n",
      "Iteration: 4815 lambda_n: 0.9095077319603475 Loss: 0.061838357054446735\n",
      "Iteration: 4816 lambda_n: 0.9118594851610288 Loss: 0.061825486283537956\n",
      "Iteration: 4817 lambda_n: 0.9807837323656761 Loss: 0.0618125838274704\n",
      "Iteration: 4818 lambda_n: 0.9780575669000376 Loss: 0.061798707904151026\n",
      "Iteration: 4819 lambda_n: 0.9499557796208128 Loss: 0.061784872392693535\n",
      "Iteration: 4820 lambda_n: 0.9400594770931633 Loss: 0.0617714361692516\n",
      "Iteration: 4821 lambda_n: 0.8796628881681193 Loss: 0.06175814163064887\n",
      "Iteration: 4822 lambda_n: 1.0128439120363257 Loss: 0.06174570277721832\n",
      "Iteration: 4823 lambda_n: 0.8823849493533449 Loss: 0.061731382528234534\n",
      "Iteration: 4824 lambda_n: 0.8860000096907013 Loss: 0.06171890840711326\n",
      "Iteration: 4825 lambda_n: 1.0230218578835304 Loss: 0.06170638469305377\n",
      "Iteration: 4826 lambda_n: 0.8923477081437982 Loss: 0.061691926045170896\n",
      "Iteration: 4827 lambda_n: 0.888767867079964 Loss: 0.06167931590322771\n",
      "Iteration: 4828 lambda_n: 0.9565856450517142 Loss: 0.0616667578805732\n",
      "Iteration: 4829 lambda_n: 0.9024597511168108 Loss: 0.061653243321236374\n",
      "Iteration: 4830 lambda_n: 0.9123741987571765 Loss: 0.06164049507261873\n",
      "Iteration: 4831 lambda_n: 0.8759086146488991 Loss: 0.06162760837574737\n",
      "Iteration: 4832 lambda_n: 0.9381574634124008 Loss: 0.06161523825045302\n",
      "Iteration: 4833 lambda_n: 0.9999463299571041 Loss: 0.061601990659093615\n",
      "Iteration: 4834 lambda_n: 0.9736832263353031 Loss: 0.06158787243677898\n",
      "Iteration: 4835 lambda_n: 0.9263121467758323 Loss: 0.06157412688939662\n",
      "Iteration: 4836 lambda_n: 0.94266840910057 Loss: 0.061561051793307314\n",
      "Iteration: 4837 lambda_n: 0.9758483416647663 Loss: 0.06154774753851363\n",
      "Iteration: 4838 lambda_n: 0.8967550319034435 Loss: 0.06153397682401366\n",
      "Iteration: 4839 lambda_n: 1.0226260643345153 Loss: 0.061521323872720156\n",
      "Iteration: 4840 lambda_n: 0.9050454714228178 Loss: 0.06150689683020542\n",
      "Iteration: 4841 lambda_n: 0.9635093741220517 Loss: 0.061494130297006275\n",
      "Iteration: 4842 lambda_n: 1.0241349643950348 Loss: 0.06148054083086567\n",
      "Iteration: 4843 lambda_n: 0.9840404378678276 Loss: 0.0614660982804129\n",
      "Iteration: 4844 lambda_n: 0.9947878980599942 Loss: 0.06145222308144665\n",
      "Iteration: 4845 lambda_n: 0.9449442911202804 Loss: 0.061438198265175104\n",
      "Iteration: 4846 lambda_n: 0.9223939997803858 Loss: 0.06142487795210227\n",
      "Iteration: 4847 lambda_n: 0.9195280017472522 Loss: 0.061411877203025184\n",
      "Iteration: 4848 lambda_n: 0.9401753495175355 Loss: 0.06139891850788845\n",
      "Iteration: 4849 lambda_n: 0.9528148640014725 Loss: 0.0613856705480114\n",
      "Iteration: 4850 lambda_n: 0.8958068914106752 Loss: 0.061372246254221594\n",
      "Iteration: 4851 lambda_n: 1.0163277750068356 Loss: 0.0613596267760261\n",
      "Iteration: 4852 lambda_n: 0.9738786795303473 Loss: 0.06134531139487437\n",
      "Iteration: 4853 lambda_n: 0.9766627083521469 Loss: 0.061331595830647054\n",
      "Iteration: 4854 lambda_n: 1.025716451813655 Loss: 0.061317842930734515\n",
      "Iteration: 4855 lambda_n: 0.8783484802328859 Loss: 0.06130340129992303\n",
      "Iteration: 4856 lambda_n: 0.8816303692413298 Loss: 0.06129103619135564\n",
      "Iteration: 4857 lambda_n: 0.950115857174142 Loss: 0.06127862640993727\n",
      "Iteration: 4858 lambda_n: 0.9892039781138126 Loss: 0.06126525434581154\n",
      "Iteration: 4859 lambda_n: 0.9418563644157941 Loss: 0.061251334042023015\n",
      "Iteration: 4860 lambda_n: 0.9815108391657795 Loss: 0.061238081819812676\n",
      "Iteration: 4861 lambda_n: 0.9943669503989334 Loss: 0.06122427351066091\n",
      "Iteration: 4862 lambda_n: 1.0223148485614288 Loss: 0.061210286277867156\n",
      "Iteration: 4863 lambda_n: 0.9573543223912414 Loss: 0.06119590795528456\n",
      "Iteration: 4864 lambda_n: 1.008558918377573 Loss: 0.06118244514403666\n",
      "Iteration: 4865 lambda_n: 0.9393266639464463 Loss: 0.06116826423080422\n",
      "Iteration: 4866 lambda_n: 1.0242897641471362 Loss: 0.06115505857585759\n",
      "Iteration: 4867 lambda_n: 0.9215952779433103 Loss: 0.061140660449495066\n",
      "Iteration: 4868 lambda_n: 0.94829109434631 Loss: 0.0611277076475419\n",
      "Iteration: 4869 lambda_n: 0.9142818015706052 Loss: 0.061114381402698224\n",
      "Iteration: 4870 lambda_n: 0.9250251747227939 Loss: 0.06110153477907012\n",
      "Iteration: 4871 lambda_n: 0.9935757112351775 Loss: 0.06108853889101779\n",
      "Iteration: 4872 lambda_n: 0.9309822504567322 Loss: 0.061074581816983844\n",
      "Iteration: 4873 lambda_n: 1.000113859875463 Loss: 0.0610615057967178\n",
      "Iteration: 4874 lambda_n: 0.9776317493090961 Loss: 0.06104746071860224\n",
      "Iteration: 4875 lambda_n: 0.887981642231145 Loss: 0.06103373329460593\n",
      "Iteration: 4876 lambda_n: 0.9096967836579732 Loss: 0.06102126634561585\n",
      "Iteration: 4877 lambda_n: 1.016165473527898 Loss: 0.061008496155574556\n",
      "Iteration: 4878 lambda_n: 1.0091842970236466 Loss: 0.06099423332756969\n",
      "Iteration: 4879 lambda_n: 1.0139223855818957 Loss: 0.060980070529198946\n",
      "Iteration: 4880 lambda_n: 0.9083341328071788 Loss: 0.06096584328794891\n",
      "Iteration: 4881 lambda_n: 0.8832634952164269 Loss: 0.060953099396003844\n",
      "Iteration: 4882 lambda_n: 0.9344842810994278 Loss: 0.06094070882845192\n",
      "Iteration: 4883 lambda_n: 1.0119535530307555 Loss: 0.0609276014287531\n",
      "Iteration: 4884 lambda_n: 0.9779606193968879 Loss: 0.060913409392533535\n",
      "Iteration: 4885 lambda_n: 1.0195187087268707 Loss: 0.06089969600819743\n",
      "Iteration: 4886 lambda_n: 0.9749268538329369 Loss: 0.06088540193657676\n",
      "Iteration: 4887 lambda_n: 0.9858871509492534 Loss: 0.0608717350226549\n",
      "Iteration: 4888 lambda_n: 1.0271249860652487 Loss: 0.06085791641070226\n",
      "Iteration: 4889 lambda_n: 0.955597732309214 Loss: 0.060843521874174494\n",
      "Iteration: 4890 lambda_n: 0.9082609176920003 Loss: 0.06083013165762704\n",
      "Iteration: 4891 lambda_n: 0.9484121706203241 Loss: 0.060817406449212644\n",
      "Iteration: 4892 lambda_n: 0.9389601765872377 Loss: 0.0608041204757666\n",
      "Iteration: 4893 lambda_n: 1.0271133409553845 Loss: 0.060790968699332176\n",
      "Iteration: 4894 lambda_n: 1.0103717184343877 Loss: 0.060776584221617545\n",
      "Iteration: 4895 lambda_n: 0.9879573234188984 Loss: 0.060762436285177954\n",
      "Iteration: 4896 lambda_n: 0.987736304679638 Loss: 0.06074860420589401\n",
      "Iteration: 4897 lambda_n: 0.9903181713116878 Loss: 0.060734777194286646\n",
      "Iteration: 4898 lambda_n: 0.9991593242140665 Loss: 0.06072091602208615\n",
      "Iteration: 4899 lambda_n: 0.9371167790409018 Loss: 0.06070693311639659\n",
      "Iteration: 4900 lambda_n: 0.9498496968030645 Loss: 0.06069382034361517\n",
      "Iteration: 4901 lambda_n: 0.9251635812783157 Loss: 0.060680531207516895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4902 lambda_n: 1.018405804000934 Loss: 0.060667589200950556\n",
      "Iteration: 4903 lambda_n: 1.0208567358774325 Loss: 0.06065334484360938\n",
      "Iteration: 4904 lambda_n: 0.9113140144476146 Loss: 0.06063906831647335\n",
      "Iteration: 4905 lambda_n: 0.9389191637303579 Loss: 0.06062632551600654\n",
      "Iteration: 4906 lambda_n: 0.9584468988699384 Loss: 0.06061319848196777\n",
      "Iteration: 4907 lambda_n: 0.9767879407658047 Loss: 0.060599800280913524\n",
      "Iteration: 4908 lambda_n: 0.9706744657511095 Loss: 0.0605861476139919\n",
      "Iteration: 4909 lambda_n: 0.9944715157068077 Loss: 0.06057258232219284\n",
      "Iteration: 4910 lambda_n: 0.9272679554513802 Loss: 0.060558686456740535\n",
      "Iteration: 4911 lambda_n: 0.8826956708988076 Loss: 0.06054573145303429\n",
      "Iteration: 4912 lambda_n: 0.9553690251778818 Loss: 0.060533400807136324\n",
      "Iteration: 4913 lambda_n: 1.020250958923041 Loss: 0.06052005675913178\n",
      "Iteration: 4914 lambda_n: 0.886681876497642 Loss: 0.06050580853821417\n",
      "Iteration: 4915 lambda_n: 0.8810487566822416 Loss: 0.06049342739421153\n",
      "Iteration: 4916 lambda_n: 1.002407831287073 Loss: 0.060481126502543954\n",
      "Iteration: 4917 lambda_n: 0.9561404501144544 Loss: 0.06046713317222458\n",
      "Iteration: 4918 lambda_n: 0.8827242598646896 Loss: 0.060453787641185495\n",
      "Iteration: 4919 lambda_n: 0.9188173626239515 Loss: 0.06044146849714355\n",
      "Iteration: 4920 lambda_n: 0.8889328536013548 Loss: 0.060428647343034325\n",
      "Iteration: 4921 lambda_n: 0.9821661727009782 Loss: 0.060416244847024445\n",
      "Iteration: 4922 lambda_n: 1.0293010559509408 Loss: 0.06040254343716483\n",
      "Iteration: 4923 lambda_n: 0.8994914133497779 Loss: 0.06038818661548284\n",
      "Iteration: 4924 lambda_n: 0.9856719100254163 Loss: 0.06037564218063261\n",
      "Iteration: 4925 lambda_n: 0.9654735900736877 Loss: 0.060361897773554836\n",
      "Iteration: 4926 lambda_n: 0.9373714419965573 Loss: 0.060348436956928354\n",
      "Iteration: 4927 lambda_n: 0.9891919761224821 Loss: 0.060335369784893726\n",
      "Iteration: 4928 lambda_n: 0.9642847842205787 Loss: 0.06032158218889639\n",
      "Iteration: 4929 lambda_n: 0.8989661366713171 Loss: 0.06030814370012706\n",
      "Iteration: 4930 lambda_n: 0.9672949881113695 Loss: 0.060295617237156915\n",
      "Iteration: 4931 lambda_n: 0.9494714513727638 Loss: 0.06028214052557311\n",
      "Iteration: 4932 lambda_n: 1.01214803577061 Loss: 0.060268914020654284\n",
      "Iteration: 4933 lambda_n: 0.8975513323961509 Loss: 0.06025481646168498\n",
      "Iteration: 4934 lambda_n: 0.9143284265137614 Loss: 0.06024231682201369\n",
      "Iteration: 4935 lambda_n: 0.9838631700263804 Loss: 0.06022958525517502\n",
      "Iteration: 4936 lambda_n: 0.9651904688526465 Loss: 0.060215887388496106\n",
      "Iteration: 4937 lambda_n: 0.902302902686884 Loss: 0.06020245144540646\n",
      "Iteration: 4938 lambda_n: 1.000366286310143 Loss: 0.06018989267920555\n",
      "Iteration: 4939 lambda_n: 0.9457504173914653 Loss: 0.06017597098863502\n",
      "Iteration: 4940 lambda_n: 0.9840007552437692 Loss: 0.060162811278780454\n",
      "Iteration: 4941 lambda_n: 0.9294338178931513 Loss: 0.06014912130768416\n",
      "Iteration: 4942 lambda_n: 0.9991406216304176 Loss: 0.060136192354195546\n",
      "Iteration: 4943 lambda_n: 0.8968991100021839 Loss: 0.060122295746903726\n",
      "Iteration: 4944 lambda_n: 0.9668633376061718 Loss: 0.06010982294486317\n",
      "Iteration: 4945 lambda_n: 1.0041070982231135 Loss: 0.06009637905929373\n",
      "Iteration: 4946 lambda_n: 0.8979286418276352 Loss: 0.060082419378616726\n",
      "Iteration: 4947 lambda_n: 0.8804196321674641 Loss: 0.060069937636780674\n",
      "Iteration: 4948 lambda_n: 0.9624267553772546 Loss: 0.06005770091717909\n",
      "Iteration: 4949 lambda_n: 0.9831080691739619 Loss: 0.060044326257213114\n",
      "Iteration: 4950 lambda_n: 0.9762693259664437 Loss: 0.06003066619477707\n",
      "Iteration: 4951 lambda_n: 0.9221368525107013 Loss: 0.060017103158493325\n",
      "Iteration: 4952 lambda_n: 0.9031783664319981 Loss: 0.06000429400417618\n",
      "Iteration: 4953 lambda_n: 0.9721524679252957 Loss: 0.05999174992591484\n",
      "Iteration: 4954 lambda_n: 0.8774413988391989 Loss: 0.05997824979200837\n",
      "Iteration: 4955 lambda_n: 0.9177653171180294 Loss: 0.05996606660052276\n",
      "Iteration: 4956 lambda_n: 0.9474782687522989 Loss: 0.059953325246222065\n",
      "Iteration: 4957 lambda_n: 1.0045114841793386 Loss: 0.0599401732443681\n",
      "Iteration: 4958 lambda_n: 0.9216323845356744 Loss: 0.059926231623800306\n",
      "Iteration: 4959 lambda_n: 0.8825892792551981 Loss: 0.05991344215159432\n",
      "Iteration: 4960 lambda_n: 0.9303835379887355 Loss: 0.059901196157144156\n",
      "Iteration: 4961 lambda_n: 0.9876777271475431 Loss: 0.05988828879153483\n",
      "Iteration: 4962 lambda_n: 1.0015671465301685 Loss: 0.05987458857214114\n",
      "Iteration: 4963 lambda_n: 0.9398142974274218 Loss: 0.059860697793412965\n",
      "Iteration: 4964 lambda_n: 0.9612423893776245 Loss: 0.059847665394709634\n",
      "Iteration: 4965 lambda_n: 0.9877726542688631 Loss: 0.05983433778403018\n",
      "Iteration: 4966 lambda_n: 0.9315338658754511 Loss: 0.059820644367588424\n",
      "Iteration: 4967 lambda_n: 0.9609970633587315 Loss: 0.05980773247725668\n",
      "Iteration: 4968 lambda_n: 0.9827185637424867 Loss: 0.05979441412661944\n",
      "Iteration: 4969 lambda_n: 1.012013652406521 Loss: 0.05978079676417762\n",
      "Iteration: 4970 lambda_n: 0.9235867448893724 Loss: 0.05976677560524699\n",
      "Iteration: 4971 lambda_n: 0.9887415502773816 Loss: 0.0597539814727434\n",
      "Iteration: 4972 lambda_n: 0.9523404126254175 Loss: 0.05974028678047877\n",
      "Iteration: 4973 lambda_n: 1.0087656336446762 Loss: 0.05972709823099361\n",
      "Iteration: 4974 lambda_n: 0.8904965877891603 Loss: 0.059713130376368766\n",
      "Iteration: 4975 lambda_n: 0.8827151483399795 Loss: 0.05970080193138895\n",
      "Iteration: 4976 lambda_n: 0.9815200356204985 Loss: 0.059688582882588166\n",
      "Iteration: 4977 lambda_n: 1.0096641093815513 Loss: 0.059674998069025154\n",
      "Iteration: 4978 lambda_n: 0.9094582516298249 Loss: 0.05966102586797895\n",
      "Iteration: 4979 lambda_n: 0.9883584539762005 Loss: 0.059648442223994486\n",
      "Iteration: 4980 lambda_n: 0.9226191213669922 Loss: 0.05963476888592626\n",
      "Iteration: 4981 lambda_n: 0.9304971933655696 Loss: 0.05962200689475706\n",
      "Iteration: 4982 lambda_n: 0.8992509097112213 Loss: 0.05960913777425405\n",
      "Iteration: 4983 lambda_n: 0.9277870355092935 Loss: 0.05959670256114435\n",
      "Iteration: 4984 lambda_n: 0.9248458423804414 Loss: 0.05958387455266909\n",
      "Iteration: 4985 lambda_n: 0.9350757527602077 Loss: 0.05957108904522409\n",
      "Iteration: 4986 lambda_n: 1.0196577803661286 Loss: 0.05955816397817003\n",
      "Iteration: 4987 lambda_n: 0.9203749368173395 Loss: 0.0595440719146391\n",
      "Iteration: 4988 lambda_n: 0.9910091817076476 Loss: 0.059531353894147356\n",
      "Iteration: 4989 lambda_n: 1.0242231505374582 Loss: 0.05951766186184371\n",
      "Iteration: 4990 lambda_n: 0.9684232625502281 Loss: 0.0595035131545264\n",
      "Iteration: 4991 lambda_n: 0.993325302188838 Loss: 0.05949013734643855\n",
      "Iteration: 4992 lambda_n: 1.0064939527634007 Loss: 0.05947641968922443\n",
      "Iteration: 4993 lambda_n: 0.9699406773526343 Loss: 0.05946252234210503\n",
      "Iteration: 4994 lambda_n: 0.9303890472748488 Loss: 0.05944913177625447\n",
      "Iteration: 4995 lambda_n: 0.932073012737724 Loss: 0.059436289148556906\n",
      "Iteration: 4996 lambda_n: 0.926481016575967 Loss: 0.05942342514891348\n",
      "Iteration: 4997 lambda_n: 1.0165001062544707 Loss: 0.05941064018590592\n",
      "Iteration: 4998 lambda_n: 0.9873704861946099 Loss: 0.059396615138910425\n",
      "Iteration: 4999 lambda_n: 0.9419389790737771 Loss: 0.059382994143267254\n",
      "Iteration: 5000 lambda_n: 0.9295886492687389 Loss: 0.05937000185107252\n",
      "Iteration: 5001 lambda_n: 0.9340616051876366 Loss: 0.05935718179168824\n",
      "Iteration: 5002 lambda_n: 0.9263081513972605 Loss: 0.05934430193061413\n",
      "Iteration: 5003 lambda_n: 1.0232536789408186 Loss: 0.05933153085023516\n",
      "Iteration: 5004 lambda_n: 0.931757055891974 Loss: 0.05931742533787316\n",
      "Iteration: 5005 lambda_n: 0.9951401569936763 Loss: 0.059304583079786845\n",
      "Iteration: 5006 lambda_n: 0.9428095946446321 Loss: 0.059290869304841586\n",
      "Iteration: 5007 lambda_n: 0.9951812373038993 Loss: 0.059277878669642244\n",
      "Iteration: 5008 lambda_n: 0.9551322945902395 Loss: 0.059264168521521055\n",
      "Iteration: 5009 lambda_n: 0.9928985952418639 Loss: 0.05925101213601363\n",
      "Iteration: 5010 lambda_n: 0.9687410719954394 Loss: 0.059237337647968916\n",
      "Iteration: 5011 lambda_n: 0.9473076299260637 Loss: 0.05922399793495842\n",
      "Iteration: 5012 lambda_n: 0.9047760784750462 Loss: 0.059210955342789924\n",
      "Iteration: 5013 lambda_n: 0.9393423252089792 Loss: 0.05919850015606075\n",
      "Iteration: 5014 lambda_n: 0.8856233018431688 Loss: 0.059185571020035625\n",
      "Iteration: 5015 lambda_n: 0.880997855186998 Loss: 0.05917338303993302\n",
      "Iteration: 5016 lambda_n: 1.0216775302835974 Loss: 0.05916126041671673\n",
      "Iteration: 5017 lambda_n: 0.9898399725325178 Loss: 0.05914720414987887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5018 lambda_n: 0.8984748655626154 Loss: 0.05913358805394821\n",
      "Iteration: 5019 lambda_n: 0.9262679778409264 Loss: 0.05912123062513086\n",
      "Iteration: 5020 lambda_n: 0.9688263275858426 Loss: 0.05910849279920731\n",
      "Iteration: 5021 lambda_n: 0.8764750993814643 Loss: 0.05909517174139781\n",
      "Iteration: 5022 lambda_n: 0.9513887304455634 Loss: 0.059083122263145944\n",
      "Iteration: 5023 lambda_n: 0.9053780111199697 Loss: 0.05907004481005629\n",
      "Iteration: 5024 lambda_n: 0.9638176915956156 Loss: 0.05905760165282081\n",
      "Iteration: 5025 lambda_n: 0.9723453152734393 Loss: 0.05904435730138359\n",
      "Iteration: 5026 lambda_n: 0.9155246678901783 Loss: 0.0590309978411682\n",
      "Iteration: 5027 lambda_n: 0.9252160448884632 Loss: 0.059018420968085746\n",
      "Iteration: 5028 lambda_n: 0.9277203238293672 Loss: 0.05900571283830768\n",
      "Iteration: 5029 lambda_n: 1.0082831631935414 Loss: 0.05899297223949591\n",
      "Iteration: 5030 lambda_n: 0.9525816853390539 Loss: 0.05897912739131972\n",
      "Iteration: 5031 lambda_n: 0.9697569941888485 Loss: 0.05896604943888425\n",
      "Iteration: 5032 lambda_n: 0.9224645930338621 Loss: 0.05895273774127579\n",
      "Iteration: 5033 lambda_n: 1.0003353236306305 Loss: 0.05894007714515685\n",
      "Iteration: 5034 lambda_n: 1.0038345725474223 Loss: 0.05892634991833279\n",
      "Iteration: 5035 lambda_n: 0.9717998573218655 Loss: 0.05891257689830878\n",
      "Iteration: 5036 lambda_n: 0.9330637279080481 Loss: 0.05889924553366679\n",
      "Iteration: 5037 lambda_n: 1.0092654631621274 Loss: 0.05888644752957089\n",
      "Iteration: 5038 lambda_n: 0.8971234021642168 Loss: 0.05887260650827073\n",
      "Iteration: 5039 lambda_n: 0.9561527282385869 Loss: 0.058860305296415706\n",
      "Iteration: 5040 lambda_n: 0.947923108071263 Loss: 0.05884719665194754\n",
      "Iteration: 5041 lambda_n: 1.0074317856420563 Loss: 0.05883420283969892\n",
      "Iteration: 5042 lambda_n: 0.9222254805322926 Loss: 0.0588203954931477\n",
      "Iteration: 5043 lambda_n: 1.0131910589801914 Loss: 0.05880775792126976\n",
      "Iteration: 5044 lambda_n: 1.0150687302791408 Loss: 0.05879387600056542\n",
      "Iteration: 5045 lambda_n: 0.9017252996661345 Loss: 0.058779970647840074\n",
      "Iteration: 5046 lambda_n: 0.8958158691885425 Loss: 0.058767619906021765\n",
      "Iteration: 5047 lambda_n: 0.8934803125516708 Loss: 0.05875535190125672\n",
      "Iteration: 5048 lambda_n: 0.966067587411869 Loss: 0.058743117666464506\n",
      "Iteration: 5049 lambda_n: 0.9700723697871769 Loss: 0.05872989151669727\n",
      "Iteration: 5050 lambda_n: 0.8860931531763565 Loss: 0.05871661263842991\n",
      "Iteration: 5051 lambda_n: 0.8874779267115578 Loss: 0.058704485153417515\n",
      "Iteration: 5052 lambda_n: 0.9946238818782036 Loss: 0.05869234047774307\n",
      "Iteration: 5053 lambda_n: 0.8830801947719149 Loss: 0.058678731662015295\n",
      "Iteration: 5054 lambda_n: 0.9108442268325887 Loss: 0.05866665088736601\n",
      "Iteration: 5055 lambda_n: 0.959188144964678 Loss: 0.05865419212594678\n",
      "Iteration: 5056 lambda_n: 1.0272601192736568 Loss: 0.058641074117072006\n",
      "Iteration: 5057 lambda_n: 1.003344113262319 Loss: 0.058627027436795355\n",
      "Iteration: 5058 lambda_n: 1.0143460726104083 Loss: 0.05861331007199418\n",
      "Iteration: 5059 lambda_n: 1.004536455234571 Loss: 0.058599444594043815\n",
      "Iteration: 5060 lambda_n: 1.0022626539274964 Loss: 0.05858571548988251\n",
      "Iteration: 5061 lambda_n: 1.0229216771265408 Loss: 0.058572019727229016\n",
      "Iteration: 5062 lambda_n: 0.9994797019319219 Loss: 0.058558043996980336\n",
      "Iteration: 5063 lambda_n: 0.99248077266605 Loss: 0.05854439082388057\n",
      "Iteration: 5064 lambda_n: 0.9232897047003653 Loss: 0.05853083548935698\n",
      "Iteration: 5065 lambda_n: 1.0022755629019549 Loss: 0.058518227166364914\n",
      "Iteration: 5066 lambda_n: 0.9280837344065537 Loss: 0.0585045424039224\n",
      "Iteration: 5067 lambda_n: 0.9150841798483887 Loss: 0.05849187265997126\n",
      "Iteration: 5068 lambda_n: 0.9548363645454573 Loss: 0.0584793822882536\n",
      "Iteration: 5069 lambda_n: 0.888122988526238 Loss: 0.058466351343922064\n",
      "Iteration: 5070 lambda_n: 0.9887631950308882 Loss: 0.05845423271213848\n",
      "Iteration: 5071 lambda_n: 1.0030020978472711 Loss: 0.05844074292693584\n",
      "Iteration: 5072 lambda_n: 0.976289961784773 Loss: 0.05842706114575924\n",
      "Iteration: 5073 lambda_n: 1.0233007589271608 Loss: 0.058413745934306406\n",
      "Iteration: 5074 lambda_n: 0.9230847870677544 Loss: 0.058399791887033495\n",
      "Iteration: 5075 lambda_n: 0.9981909634866896 Loss: 0.058387206458723855\n",
      "Iteration: 5076 lambda_n: 0.9517559326046307 Loss: 0.05837359920738217\n",
      "Iteration: 5077 lambda_n: 0.9945675616435316 Loss: 0.058360627067246124\n",
      "Iteration: 5078 lambda_n: 0.9391814544743039 Loss: 0.0583470736231525\n",
      "Iteration: 5079 lambda_n: 1.0019805014460588 Loss: 0.0583342770223552\n",
      "Iteration: 5080 lambda_n: 0.9889559789487476 Loss: 0.05832062698649399\n",
      "Iteration: 5081 lambda_n: 1.0066850061139383 Loss: 0.05830715663221611\n",
      "Iteration: 5082 lambda_n: 0.97056512639956 Loss: 0.058293447089797036\n",
      "Iteration: 5083 lambda_n: 1.0244058769323998 Loss: 0.05828023163994366\n",
      "Iteration: 5084 lambda_n: 1.0146316379881732 Loss: 0.05826628541931998\n",
      "Iteration: 5085 lambda_n: 0.8789028007033155 Loss: 0.05825247463326506\n",
      "Iteration: 5086 lambda_n: 0.916888709024191 Loss: 0.05824051324392775\n",
      "Iteration: 5087 lambda_n: 1.0205576847680031 Loss: 0.058228036774506046\n",
      "Iteration: 5088 lambda_n: 0.9301212244910197 Loss: 0.058214151908156354\n",
      "Iteration: 5089 lambda_n: 0.9230403250386949 Loss: 0.05820149952840414\n",
      "Iteration: 5090 lambda_n: 0.9311202861522967 Loss: 0.05818894543446468\n",
      "Iteration: 5091 lambda_n: 0.9009978232646264 Loss: 0.05817628343086628\n",
      "Iteration: 5092 lambda_n: 0.9418547572907364 Loss: 0.058164032951452296\n",
      "Iteration: 5093 lambda_n: 0.9518708916567742 Loss: 0.05815122895543479\n",
      "Iteration: 5094 lambda_n: 0.9807981764700697 Loss: 0.058138290871471064\n",
      "Iteration: 5095 lambda_n: 0.8847616987303671 Loss: 0.05812496178451473\n",
      "Iteration: 5096 lambda_n: 0.8910505361496465 Loss: 0.05811293974058142\n",
      "Iteration: 5097 lambda_n: 0.9784999259671425 Loss: 0.05810083407046345\n",
      "Iteration: 5098 lambda_n: 0.9602477427483321 Loss: 0.05808754243910834\n",
      "Iteration: 5099 lambda_n: 0.9469994310862948 Loss: 0.05807450089058185\n",
      "Iteration: 5100 lambda_n: 0.9360101445764651 Loss: 0.058061641361471635\n",
      "Iteration: 5101 lambda_n: 0.9044596596197616 Loss: 0.05804893309761779\n",
      "Iteration: 5102 lambda_n: 0.9185443923356905 Loss: 0.05803665512367539\n",
      "Iteration: 5103 lambda_n: 0.911594161473134 Loss: 0.0580241878903341\n",
      "Iteration: 5104 lambda_n: 0.9315491449121148 Loss: 0.05801181692511452\n",
      "Iteration: 5105 lambda_n: 0.940192354416995 Loss: 0.05799917714849799\n",
      "Iteration: 5106 lambda_n: 0.9586140993828879 Loss: 0.05798642213858347\n",
      "Iteration: 5107 lambda_n: 0.9078067394428583 Loss: 0.05797341932631714\n",
      "Iteration: 5108 lambda_n: 0.9884377662501022 Loss: 0.05796110764335158\n",
      "Iteration: 5109 lambda_n: 0.9009164351577512 Loss: 0.05794770462148269\n",
      "Iteration: 5110 lambda_n: 1.0214483179265685 Loss: 0.05793549035258918\n",
      "Iteration: 5111 lambda_n: 0.912732009069417 Loss: 0.05792164424672538\n",
      "Iteration: 5112 lambda_n: 0.937680244252721 Loss: 0.05790927388751556\n",
      "Iteration: 5113 lambda_n: 0.9049608706455596 Loss: 0.05789656742537123\n",
      "Iteration: 5114 lambda_n: 1.0159879534383232 Loss: 0.05788430628773204\n",
      "Iteration: 5115 lambda_n: 0.9462543258516339 Loss: 0.057870543144671896\n",
      "Iteration: 5116 lambda_n: 0.944116094407862 Loss: 0.05785772682200151\n",
      "Iteration: 5117 lambda_n: 0.9561537467458003 Loss: 0.057844941546847724\n",
      "Iteration: 5118 lambda_n: 0.9204516572009757 Loss: 0.057831995382923836\n",
      "Iteration: 5119 lambda_n: 1.0107302020198954 Loss: 0.0578195346417113\n",
      "Iteration: 5120 lambda_n: 0.9671836690201394 Loss: 0.05780585402898583\n",
      "Iteration: 5121 lambda_n: 0.9738232242209577 Loss: 0.05779276507767329\n",
      "Iteration: 5122 lambda_n: 1.0240616637970705 Loss: 0.05777958849039732\n",
      "Iteration: 5123 lambda_n: 0.9998068652631146 Loss: 0.057765734539768555\n",
      "Iteration: 5124 lambda_n: 1.012147963712516 Loss: 0.05775221109578305\n",
      "Iteration: 5125 lambda_n: 1.0253880280548686 Loss: 0.0577385231194699\n",
      "Iteration: 5126 lambda_n: 0.9937793652662578 Loss: 0.05772465854658556\n",
      "Iteration: 5127 lambda_n: 0.967328641165448 Loss: 0.05771122372613779\n",
      "Iteration: 5128 lambda_n: 0.9174704073059636 Loss: 0.057698148725804144\n",
      "Iteration: 5129 lambda_n: 0.9230235356840625 Loss: 0.05768574967826742\n",
      "Iteration: 5130 lambda_n: 0.8992320900034575 Loss: 0.05767327758772837\n",
      "Iteration: 5131 lambda_n: 0.9498750123686075 Loss: 0.0576611289066237\n",
      "Iteration: 5132 lambda_n: 0.907175484186784 Loss: 0.05764829811123044\n",
      "Iteration: 5133 lambda_n: 0.9736250925192924 Loss: 0.057636046087125324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5134 lambda_n: 0.886367247474124 Loss: 0.0576228987815804\n",
      "Iteration: 5135 lambda_n: 0.9457935930193704 Loss: 0.05761093170995008\n",
      "Iteration: 5136 lambda_n: 1.022056621947351 Loss: 0.05759816436021471\n",
      "Iteration: 5137 lambda_n: 0.9820564537753896 Loss: 0.057584369912784844\n",
      "Iteration: 5138 lambda_n: 0.9318589110394107 Loss: 0.057571117671932764\n",
      "Iteration: 5139 lambda_n: 0.9452119812714923 Loss: 0.0575585449322844\n",
      "Iteration: 5140 lambda_n: 0.9569473156781664 Loss: 0.05754579413857572\n",
      "Iteration: 5141 lambda_n: 0.9219469016117811 Loss: 0.057532887199738474\n",
      "Iteration: 5142 lambda_n: 1.0040297765927442 Loss: 0.05752045439294804\n",
      "Iteration: 5143 lambda_n: 0.8825696783535167 Loss: 0.05750691696816502\n",
      "Iteration: 5144 lambda_n: 0.9037198796726278 Loss: 0.05749501918358062\n",
      "Iteration: 5145 lambda_n: 0.9185761781613222 Loss: 0.057482838199854266\n",
      "Iteration: 5146 lambda_n: 0.9972701879587015 Loss: 0.05747045896802674\n",
      "Iteration: 5147 lambda_n: 0.9853348225503911 Loss: 0.05745702149237158\n",
      "Iteration: 5148 lambda_n: 1.0131761961422197 Loss: 0.05744374716914268\n",
      "Iteration: 5149 lambda_n: 0.9059606080328337 Loss: 0.057430100189026494\n",
      "Iteration: 5150 lambda_n: 0.9141951624553071 Loss: 0.05741789942824811\n",
      "Iteration: 5151 lambda_n: 0.9623231163649067 Loss: 0.05740558976162599\n",
      "Iteration: 5152 lambda_n: 0.9685988871286277 Loss: 0.05739263421229289\n",
      "Iteration: 5153 lambda_n: 0.9025685699960652 Loss: 0.05737959641399711\n",
      "Iteration: 5154 lambda_n: 1.0238167742008455 Loss: 0.05736744943936668\n",
      "Iteration: 5155 lambda_n: 0.9017334693639997 Loss: 0.05735367304430864\n",
      "Iteration: 5156 lambda_n: 0.9302924811046575 Loss: 0.05734154147639683\n",
      "Iteration: 5157 lambda_n: 0.9492726043852243 Loss: 0.0573290277338084\n",
      "Iteration: 5158 lambda_n: 1.0008171082016883 Loss: 0.05731626082628148\n",
      "Iteration: 5159 lambda_n: 0.9789784157699216 Loss: 0.057302803036375236\n",
      "Iteration: 5160 lambda_n: 0.9154717724343888 Loss: 0.05728964123970433\n",
      "Iteration: 5161 lambda_n: 0.9699435436877565 Loss: 0.0572773353417305\n",
      "Iteration: 5162 lambda_n: 0.9564659462555363 Loss: 0.05726429943059262\n",
      "Iteration: 5163 lambda_n: 0.9281675478224282 Loss: 0.05725144687859666\n",
      "Iteration: 5164 lambda_n: 0.8860271327480458 Loss: 0.057238976698494065\n",
      "Iteration: 5165 lambda_n: 0.910006816128005 Loss: 0.05722707462724399\n",
      "Iteration: 5166 lambda_n: 0.9470261418049308 Loss: 0.05721485240993451\n",
      "Iteration: 5167 lambda_n: 0.947170721036982 Loss: 0.05720213511588549\n",
      "Iteration: 5168 lambda_n: 0.8783049127450386 Loss: 0.05718941805105435\n",
      "Iteration: 5169 lambda_n: 0.92974098457925 Loss: 0.05717762754521791\n",
      "Iteration: 5170 lambda_n: 0.9700933932455237 Loss: 0.05716514858981099\n",
      "Iteration: 5171 lambda_n: 0.9958003135151142 Loss: 0.05715213025981465\n",
      "Iteration: 5172 lambda_n: 1.01146228890909 Loss: 0.05713876932630879\n",
      "Iteration: 5173 lambda_n: 0.8858763740076799 Loss: 0.05712520071629458\n",
      "Iteration: 5174 lambda_n: 0.8946481473132666 Loss: 0.05711331886371299\n",
      "Iteration: 5175 lambda_n: 0.9440852653404777 Loss: 0.057101321295696124\n",
      "Iteration: 5176 lambda_n: 1.0130479168644073 Loss: 0.05708866286880847\n",
      "Iteration: 5177 lambda_n: 0.8784003833272891 Loss: 0.057075082194284865\n",
      "Iteration: 5178 lambda_n: 0.9401092570856726 Loss: 0.05706330859555583\n",
      "Iteration: 5179 lambda_n: 0.9475662047483766 Loss: 0.05705070996842228\n",
      "Iteration: 5180 lambda_n: 0.996598936337824 Loss: 0.057038013590421474\n",
      "Iteration: 5181 lambda_n: 0.884577976609654 Loss: 0.057024662590626926\n",
      "Iteration: 5182 lambda_n: 1.0288225002653553 Loss: 0.05701281431879987\n",
      "Iteration: 5183 lambda_n: 0.8971861660827938 Loss: 0.0569990364032365\n",
      "Iteration: 5184 lambda_n: 0.9257021203608712 Loss: 0.05698702346494628\n",
      "Iteration: 5185 lambda_n: 0.9381861845304564 Loss: 0.05697463077515096\n",
      "Iteration: 5186 lambda_n: 0.8860469166962135 Loss: 0.05696207309814853\n",
      "Iteration: 5187 lambda_n: 0.9044061558610529 Loss: 0.05695021528878887\n",
      "Iteration: 5188 lambda_n: 0.9567564786388453 Loss: 0.05693811376632619\n",
      "Iteration: 5189 lambda_n: 0.9456013978846652 Loss: 0.05692531394828102\n",
      "Iteration: 5190 lambda_n: 0.9967171438326338 Loss: 0.056912665575166624\n",
      "Iteration: 5191 lambda_n: 0.9428223205355128 Loss: 0.056899335855654626\n",
      "Iteration: 5192 lambda_n: 0.8783804963183687 Loss: 0.056886729153231265\n",
      "Iteration: 5193 lambda_n: 1.0035829849740199 Loss: 0.05687498608573161\n",
      "Iteration: 5194 lambda_n: 1.0219755181039363 Loss: 0.05686157151126637\n",
      "Iteration: 5195 lambda_n: 1.016328090200318 Loss: 0.056847913639599686\n",
      "Iteration: 5196 lambda_n: 0.9221628370063462 Loss: 0.05683433379485102\n",
      "Iteration: 5197 lambda_n: 0.9019884623596786 Loss: 0.05682201436066119\n",
      "Iteration: 5198 lambda_n: 0.9658604215257112 Loss: 0.056809966472659304\n",
      "Iteration: 5199 lambda_n: 0.884734953496953 Loss: 0.05679706767328134\n",
      "Iteration: 5200 lambda_n: 1.0221050300473224 Loss: 0.05678525430583387\n",
      "Iteration: 5201 lambda_n: 0.9489587038420956 Loss: 0.05677160912367449\n",
      "Iteration: 5202 lambda_n: 1.0264199864087211 Loss: 0.05675894276570912\n",
      "Iteration: 5203 lambda_n: 0.9277948835623543 Loss: 0.0567452449937269\n",
      "Iteration: 5204 lambda_n: 1.0286939642046442 Loss: 0.05673286563985973\n",
      "Iteration: 5205 lambda_n: 0.9409909976885251 Loss: 0.056719142508902455\n",
      "Iteration: 5206 lambda_n: 0.8862685941769346 Loss: 0.05670659166600377\n",
      "Iteration: 5207 lambda_n: 0.9342038612010907 Loss: 0.0566947727162566\n",
      "Iteration: 5208 lambda_n: 0.9573135756314044 Loss: 0.0566823166330612\n",
      "Iteration: 5209 lambda_n: 0.9155396359027436 Loss: 0.05666955467136009\n",
      "Iteration: 5210 lambda_n: 0.8852754860717374 Loss: 0.05665735173239726\n",
      "Iteration: 5211 lambda_n: 1.025285765616652 Loss: 0.05664555415985432\n",
      "Iteration: 5212 lambda_n: 0.8785924628087848 Loss: 0.056631893189188674\n",
      "Iteration: 5213 lambda_n: 1.0293394383118823 Loss: 0.056620188854784856\n",
      "Iteration: 5214 lambda_n: 0.9821250516770855 Loss: 0.05660647876659762\n",
      "Iteration: 5215 lambda_n: 0.914452036857312 Loss: 0.05659340000787404\n",
      "Iteration: 5216 lambda_n: 0.9441416414781987 Loss: 0.05658122460378101\n",
      "Iteration: 5217 lambda_n: 0.9920204695270217 Loss: 0.05656865609355038\n",
      "Iteration: 5218 lambda_n: 0.9812099538025681 Loss: 0.0565554526184408\n",
      "Iteration: 5219 lambda_n: 0.9098843398715225 Loss: 0.05654239545181882\n",
      "Iteration: 5220 lambda_n: 0.972368142822954 Loss: 0.05653028958579019\n",
      "Iteration: 5221 lambda_n: 0.8804384782818651 Loss: 0.05651735467720657\n",
      "Iteration: 5222 lambda_n: 0.8969658132554283 Loss: 0.05650564470763719\n",
      "Iteration: 5223 lambda_n: 0.9492475459763572 Loss: 0.05649371692318721\n",
      "Iteration: 5224 lambda_n: 0.9563593214808469 Loss: 0.056481096101466385\n",
      "Iteration: 5225 lambda_n: 0.9794121874123606 Loss: 0.05646838301505048\n",
      "Iteration: 5226 lambda_n: 0.9220155142151492 Loss: 0.05645536586666204\n",
      "Iteration: 5227 lambda_n: 0.9419396734872936 Loss: 0.056443113771192086\n",
      "Iteration: 5228 lambda_n: 0.8784113307151988 Loss: 0.05643059912715273\n",
      "Iteration: 5229 lambda_n: 0.9804958344190099 Loss: 0.05641893053816981\n",
      "Iteration: 5230 lambda_n: 0.9978668685481874 Loss: 0.056405908183599265\n",
      "Iteration: 5231 lambda_n: 0.9545949888877558 Loss: 0.056392657608734076\n",
      "Iteration: 5232 lambda_n: 1.0179919163972468 Loss: 0.056379983990565825\n",
      "Iteration: 5233 lambda_n: 0.9213874411588293 Loss: 0.05636647122487778\n",
      "Iteration: 5234 lambda_n: 0.9316093292802233 Loss: 0.05635424304114023\n",
      "Iteration: 5235 lambda_n: 0.9610069421967737 Loss: 0.05634188138223031\n",
      "Iteration: 5236 lambda_n: 0.9018708438196836 Loss: 0.05632913194529314\n",
      "Iteration: 5237 lambda_n: 1.0260367244917183 Loss: 0.0563171691809521\n",
      "Iteration: 5238 lambda_n: 0.9477157195949967 Loss: 0.056303561940454175\n",
      "Iteration: 5239 lambda_n: 0.987590217854022 Loss: 0.056290995762131056\n",
      "Iteration: 5240 lambda_n: 0.9942919744107566 Loss: 0.056277903297185454\n",
      "Iteration: 5241 lambda_n: 0.8921951581569024 Loss: 0.05626472449102344\n",
      "Iteration: 5242 lambda_n: 0.9069442888971928 Loss: 0.05625290106340031\n",
      "Iteration: 5243 lambda_n: 0.9646081631918371 Loss: 0.05624088425541728\n",
      "Iteration: 5244 lambda_n: 1.0251826285292758 Loss: 0.05622810571249134\n",
      "Iteration: 5245 lambda_n: 0.9142173742156828 Loss: 0.056214527314717745\n",
      "Iteration: 5246 lambda_n: 0.922741596313101 Loss: 0.05620242089637367\n",
      "Iteration: 5247 lambda_n: 0.920446534370966 Loss: 0.05619020375946072\n",
      "Iteration: 5248 lambda_n: 0.8841218533335382 Loss: 0.05617801917507198\n",
      "Iteration: 5249 lambda_n: 0.9207168748218956 Loss: 0.0561663174835576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5250 lambda_n: 0.9446566597043545 Loss: 0.05615413356700921\n",
      "Iteration: 5251 lambda_n: 0.939385501093858 Loss: 0.05614163510735231\n",
      "Iteration: 5252 lambda_n: 0.9462565562890995 Loss: 0.05612920865401056\n",
      "Iteration: 5253 lambda_n: 0.9510870109833179 Loss: 0.05611669359371556\n",
      "Iteration: 5254 lambda_n: 0.9487478760059005 Loss: 0.05610411695923548\n",
      "Iteration: 5255 lambda_n: 0.9351515755446507 Loss: 0.05609157356779501\n",
      "Iteration: 5256 lambda_n: 0.9324511632538 Loss: 0.05607921219390586\n",
      "Iteration: 5257 lambda_n: 0.9712278122868228 Loss: 0.056066888752006064\n",
      "Iteration: 5258 lambda_n: 0.9275514067929826 Loss: 0.056054055206918356\n",
      "Iteration: 5259 lambda_n: 0.8933796372669203 Loss: 0.05604180105499884\n",
      "Iteration: 5260 lambda_n: 0.9581891662992356 Loss: 0.056030000449595996\n",
      "Iteration: 5261 lambda_n: 0.9109328825067305 Loss: 0.056017346062907654\n",
      "Iteration: 5262 lambda_n: 0.9090469493236302 Loss: 0.056005317963165414\n",
      "Iteration: 5263 lambda_n: 0.9685297662040769 Loss: 0.05599331689867465\n",
      "Iteration: 5264 lambda_n: 1.0267907896364714 Loss: 0.05598053289951089\n",
      "Iteration: 5265 lambda_n: 1.0273856797208218 Loss: 0.05596698253561156\n",
      "Iteration: 5266 lambda_n: 0.9624379257422965 Loss: 0.055953427047673907\n",
      "Iteration: 5267 lambda_n: 0.921355956245353 Loss: 0.05594073096660037\n",
      "Iteration: 5268 lambda_n: 0.9776559694024077 Loss: 0.055928579067343866\n",
      "Iteration: 5269 lambda_n: 0.9666091148013481 Loss: 0.05591568702255845\n",
      "Iteration: 5270 lambda_n: 0.9427187082047056 Loss: 0.055902943083962646\n",
      "Iteration: 5271 lambda_n: 1.0255632385236975 Loss: 0.055890516454047794\n",
      "Iteration: 5272 lambda_n: 1.0065362236655795 Loss: 0.05587700041160663\n",
      "Iteration: 5273 lambda_n: 0.9336515328611806 Loss: 0.05586373778426116\n",
      "Iteration: 5274 lambda_n: 0.959163512084782 Loss: 0.0558514378756034\n",
      "Iteration: 5275 lambda_n: 0.9535137111847075 Loss: 0.05583880423282179\n",
      "Iteration: 5276 lambda_n: 0.8875075656590846 Loss: 0.05582624737906959\n",
      "Iteration: 5277 lambda_n: 0.9566346341467129 Loss: 0.05581456188944345\n",
      "Iteration: 5278 lambda_n: 1.0170440881200122 Loss: 0.05580196852688552\n",
      "Iteration: 5279 lambda_n: 0.9359227431300383 Loss: 0.05578858253668055\n",
      "Iteration: 5280 lambda_n: 0.9610340708988248 Loss: 0.05577626662242754\n",
      "Iteration: 5281 lambda_n: 0.9629788959786932 Loss: 0.055763622644981845\n",
      "Iteration: 5282 lambda_n: 0.9702288813123262 Loss: 0.0557509555002787\n",
      "Iteration: 5283 lambda_n: 0.9024450536548199 Loss: 0.05573819544001555\n",
      "Iteration: 5284 lambda_n: 0.9400146633696844 Loss: 0.05572632905586669\n",
      "Iteration: 5285 lambda_n: 0.938906362112114 Loss: 0.055713970930163496\n",
      "Iteration: 5286 lambda_n: 0.9744324493293788 Loss: 0.05570162968549293\n",
      "Iteration: 5287 lambda_n: 0.9248034131993669 Loss: 0.05568882391959529\n",
      "Iteration: 5288 lambda_n: 0.9272263468489308 Loss: 0.05567667267050778\n",
      "Iteration: 5289 lambda_n: 1.0224722710186123 Loss: 0.05566449183944812\n",
      "Iteration: 5290 lambda_n: 0.9382750783098709 Loss: 0.05565106239515835\n",
      "Iteration: 5291 lambda_n: 0.9343745883570004 Loss: 0.055638741238615855\n",
      "Iteration: 5292 lambda_n: 0.8759752247831155 Loss: 0.055626473603171855\n",
      "Iteration: 5293 lambda_n: 0.987384738252575 Loss: 0.05561497479421795\n",
      "Iteration: 5294 lambda_n: 0.9064987802053068 Loss: 0.05560201595020666\n",
      "Iteration: 5295 lambda_n: 0.9280355726039413 Loss: 0.05559012094894536\n",
      "Iteration: 5296 lambda_n: 0.9143372152109651 Loss: 0.05557794558847166\n",
      "Iteration: 5297 lambda_n: 0.9997515589035564 Loss: 0.055565952166108816\n",
      "Iteration: 5298 lambda_n: 0.9497978634894552 Loss: 0.055552840884541196\n",
      "Iteration: 5299 lambda_n: 0.9924381334291117 Loss: 0.05554038716905273\n",
      "Iteration: 5300 lambda_n: 0.9674831548669756 Loss: 0.05552737690423116\n",
      "Iteration: 5301 lambda_n: 0.9322621788647267 Loss: 0.05551469629274774\n",
      "Iteration: 5302 lambda_n: 0.8888043869203126 Loss: 0.05550247966036654\n",
      "Iteration: 5303 lambda_n: 0.9729343100180572 Loss: 0.0554908346556534\n",
      "Iteration: 5304 lambda_n: 0.9439565322771646 Loss: 0.05547808979232598\n",
      "Iteration: 5305 lambda_n: 0.9694051837994279 Loss: 0.055465726921024056\n",
      "Iteration: 5306 lambda_n: 0.878526020057552 Loss: 0.055453033214313475\n",
      "Iteration: 5307 lambda_n: 0.9797486205945769 Loss: 0.0554415316653849\n",
      "Iteration: 5308 lambda_n: 0.8823938652110226 Loss: 0.055428707343256986\n",
      "Iteration: 5309 lambda_n: 1.0218133489871135 Loss: 0.055417159522009166\n",
      "Iteration: 5310 lambda_n: 0.9282404729083467 Loss: 0.05540378971831821\n",
      "Iteration: 5311 lambda_n: 1.0123953168807274 Loss: 0.055391646669412174\n",
      "Iteration: 5312 lambda_n: 0.947500952796735 Loss: 0.055378405342860065\n",
      "Iteration: 5313 lambda_n: 0.8977887526972214 Loss: 0.05536601525975583\n",
      "Iteration: 5314 lambda_n: 0.9576031445307863 Loss: 0.05535427745383065\n",
      "Iteration: 5315 lambda_n: 0.9807948408742141 Loss: 0.055341759999651595\n",
      "Iteration: 5316 lambda_n: 0.9648297451207403 Loss: 0.05532894193243095\n",
      "Iteration: 5317 lambda_n: 0.9462699009484964 Loss: 0.05531633502442675\n",
      "Iteration: 5318 lambda_n: 0.8803257600015 Loss: 0.05530397304794868\n",
      "Iteration: 5319 lambda_n: 0.9188339603253274 Loss: 0.0552924747125238\n",
      "Iteration: 5320 lambda_n: 0.9715872965708767 Loss: 0.05528047561914507\n",
      "Iteration: 5321 lambda_n: 0.8965135240787683 Loss: 0.05526779008040066\n",
      "Iteration: 5322 lambda_n: 1.008007809661941 Loss: 0.055256086990233294\n",
      "Iteration: 5323 lambda_n: 0.925403681442757 Loss: 0.05524293103095209\n",
      "Iteration: 5324 lambda_n: 1.0141984584405506 Loss: 0.05523085557888559\n",
      "Iteration: 5325 lambda_n: 0.907492452892789 Loss: 0.055217624102102056\n",
      "Iteration: 5326 lambda_n: 0.9816125531287591 Loss: 0.055205787083823954\n",
      "Iteration: 5327 lambda_n: 0.9063026874129579 Loss: 0.055192985765163904\n",
      "Iteration: 5328 lambda_n: 0.9320188695477506 Loss: 0.0551811688760178\n",
      "Iteration: 5329 lambda_n: 1.0163974935336137 Loss: 0.055169018994519974\n",
      "Iteration: 5330 lambda_n: 1.0284532522301641 Loss: 0.05515577181733012\n",
      "Iteration: 5331 lambda_n: 0.9778522221475283 Loss: 0.055142370350985404\n",
      "Iteration: 5332 lambda_n: 0.8966427987234205 Loss: 0.05512963090195124\n",
      "Iteration: 5333 lambda_n: 0.9008226582944371 Loss: 0.05511795172070196\n",
      "Iteration: 5334 lambda_n: 1.0108452147818332 Loss: 0.055106220285316546\n",
      "Iteration: 5335 lambda_n: 0.8983881908900035 Loss: 0.05509305863937274\n",
      "Iteration: 5336 lambda_n: 0.920901944089749 Loss: 0.05508136355642857\n",
      "Iteration: 5337 lambda_n: 0.9092756622544064 Loss: 0.05506937766381404\n",
      "Iteration: 5338 lambda_n: 0.9115817198781501 Loss: 0.05505754534893305\n",
      "Iteration: 5339 lambda_n: 1.0095650596320773 Loss: 0.055045685278422976\n",
      "Iteration: 5340 lambda_n: 0.9672171363593693 Loss: 0.055032553036698975\n",
      "Iteration: 5341 lambda_n: 0.9290333972538163 Loss: 0.05501997424777512\n",
      "Iteration: 5342 lambda_n: 0.9137354979008788 Loss: 0.055007894439481016\n",
      "Iteration: 5343 lambda_n: 0.9783175203673607 Loss: 0.05499601583454881\n",
      "Iteration: 5344 lambda_n: 0.9766698830973045 Loss: 0.05498330018109435\n",
      "Iteration: 5345 lambda_n: 0.9349268351465737 Loss: 0.054970608544837575\n",
      "Iteration: 5346 lambda_n: 0.9256262087286516 Loss: 0.05495846178867225\n",
      "Iteration: 5347 lambda_n: 0.9164426118546181 Loss: 0.054946438218258994\n",
      "Iteration: 5348 lambda_n: 1.0261482110949207 Loss: 0.05493453624489373\n",
      "Iteration: 5349 lambda_n: 0.9617165503204247 Loss: 0.05492121223286485\n",
      "Iteration: 5350 lambda_n: 0.9098425665614807 Loss: 0.054908727447317916\n",
      "Iteration: 5351 lambda_n: 0.9017428585817832 Loss: 0.054896918408100105\n",
      "Iteration: 5352 lambda_n: 0.9089647526540672 Loss: 0.054885216733498166\n",
      "Iteration: 5353 lambda_n: 0.8921072594641882 Loss: 0.05487342359738608\n",
      "Iteration: 5354 lambda_n: 0.9893084936896722 Loss: 0.054861851377489654\n",
      "Iteration: 5355 lambda_n: 1.0194868300313993 Loss: 0.054849020838037124\n",
      "Iteration: 5356 lambda_n: 0.9732121430384526 Loss: 0.05483580172126486\n",
      "Iteration: 5357 lambda_n: 0.9619585923738914 Loss: 0.05482318528685352\n",
      "Iteration: 5358 lambda_n: 0.9689419579041076 Loss: 0.054810717299418825\n",
      "Iteration: 5359 lambda_n: 0.9269633581832609 Loss: 0.054798161374214054\n",
      "Iteration: 5360 lambda_n: 0.892769946616719 Loss: 0.054786151843402804\n",
      "Iteration: 5361 lambda_n: 1.0005900299694113 Loss: 0.05477458755286673\n",
      "Iteration: 5362 lambda_n: 0.916340411626458 Loss: 0.05476162925119931\n",
      "Iteration: 5363 lambda_n: 1.0092496737526486 Loss: 0.05474976446080104\n",
      "Iteration: 5364 lambda_n: 0.9073445270261525 Loss: 0.054736699362438926\n",
      "Iteration: 5365 lambda_n: 1.0253973222566164 Loss: 0.05472495586483324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5366 lambda_n: 0.9164969763864269 Loss: 0.05471168718351856\n",
      "Iteration: 5367 lambda_n: 0.9118659255426124 Loss: 0.054699830138713185\n",
      "Iteration: 5368 lambda_n: 0.9304343536893633 Loss: 0.05468803531515294\n",
      "Iteration: 5369 lambda_n: 0.9762769079551542 Loss: 0.05467600268647752\n",
      "Iteration: 5370 lambda_n: 0.9773818262410601 Loss: 0.05466337978949134\n",
      "Iteration: 5371 lambda_n: 0.9664097524563234 Loss: 0.05465074525465405\n",
      "Iteration: 5372 lambda_n: 0.9042142027579029 Loss: 0.05463825516209891\n",
      "Iteration: 5373 lambda_n: 0.9143335744471187 Loss: 0.05462657124769028\n",
      "Iteration: 5374 lambda_n: 0.9191502785037224 Loss: 0.05461475888542503\n",
      "Iteration: 5375 lambda_n: 0.9066654993319884 Loss: 0.054602886639320884\n",
      "Iteration: 5376 lambda_n: 0.8879068421258369 Loss: 0.05459117795704666\n",
      "Iteration: 5377 lambda_n: 0.933416837264375 Loss: 0.054579713742934556\n",
      "Iteration: 5378 lambda_n: 0.9133238784916619 Loss: 0.05456766429507175\n",
      "Iteration: 5379 lambda_n: 0.893981677450665 Loss: 0.05455587657825162\n",
      "Iteration: 5380 lambda_n: 1.0160172147638655 Loss: 0.05454434075366661\n",
      "Iteration: 5381 lambda_n: 0.9616790748909428 Loss: 0.05453123290748756\n",
      "Iteration: 5382 lambda_n: 0.9197515069558841 Loss: 0.05451882874588368\n",
      "Iteration: 5383 lambda_n: 0.973501925348968 Loss: 0.05450696780380133\n",
      "Iteration: 5384 lambda_n: 1.0040786601276142 Loss: 0.05449441628489216\n",
      "Iteration: 5385 lambda_n: 0.8798369156328638 Loss: 0.054481473314951866\n",
      "Iteration: 5386 lambda_n: 0.9957349664562724 Loss: 0.05447013419214283\n",
      "Iteration: 5387 lambda_n: 0.9672975934030924 Loss: 0.054457304021676825\n",
      "Iteration: 5388 lambda_n: 0.9910656595378002 Loss: 0.054444842934033115\n",
      "Iteration: 5389 lambda_n: 0.9362882872431781 Loss: 0.05443207838186987\n",
      "Iteration: 5390 lambda_n: 0.989349213824337 Loss: 0.05442002187657203\n",
      "Iteration: 5391 lambda_n: 0.9911162355079024 Loss: 0.05440728478827587\n",
      "Iteration: 5392 lambda_n: 0.9533452642348226 Loss: 0.054394527712137895\n",
      "Iteration: 5393 lambda_n: 0.9970366797078272 Loss: 0.054382259411615515\n",
      "Iteration: 5394 lambda_n: 0.8768659827049043 Loss: 0.0543694315992837\n",
      "Iteration: 5395 lambda_n: 0.8783884977717703 Loss: 0.05435815221193469\n",
      "Iteration: 5396 lambda_n: 0.9136508744542513 Loss: 0.05434685541476877\n",
      "Iteration: 5397 lambda_n: 0.948092147169855 Loss: 0.05433510742540958\n",
      "Iteration: 5398 lambda_n: 1.0116200072282355 Loss: 0.05432291907305104\n",
      "Iteration: 5399 lambda_n: 0.9518366157930537 Loss: 0.054309916829837414\n",
      "Iteration: 5400 lambda_n: 0.8991421920960502 Loss: 0.05429768561923092\n",
      "Iteration: 5401 lambda_n: 0.9975055223807369 Loss: 0.054286133893417535\n",
      "Iteration: 5402 lambda_n: 1.00067442432194 Loss: 0.054273321124420966\n",
      "Iteration: 5403 lambda_n: 0.9951307793744917 Loss: 0.0542604704851664\n",
      "Iteration: 5404 lambda_n: 0.9546068709801305 Loss: 0.054247693853928046\n",
      "Iteration: 5405 lambda_n: 0.9588107237105415 Loss: 0.0542354401565792\n",
      "Iteration: 5406 lambda_n: 1.0082181698068706 Loss: 0.054223135102420675\n",
      "Iteration: 5407 lambda_n: 1.0291583935146873 Loss: 0.05421019878814463\n",
      "Iteration: 5408 lambda_n: 0.9339270894707618 Loss: 0.054196996774590445\n",
      "Iteration: 5409 lambda_n: 0.9556927861442021 Loss: 0.054185018994610486\n",
      "Iteration: 5410 lambda_n: 0.8893731312308787 Loss: 0.054172764637597876\n",
      "Iteration: 5411 lambda_n: 0.92210109963897 Loss: 0.054161363001922765\n",
      "Iteration: 5412 lambda_n: 0.9419393719483281 Loss: 0.054149544180385714\n",
      "Iteration: 5413 lambda_n: 0.9573460255223387 Loss: 0.054137473591778236\n",
      "Iteration: 5414 lambda_n: 0.87567585053139 Loss: 0.05412520816921609\n",
      "Iteration: 5415 lambda_n: 0.8909022776470107 Loss: 0.054113991390374316\n",
      "Iteration: 5416 lambda_n: 1.0060193866848808 Loss: 0.05410258182220286\n",
      "Iteration: 5417 lambda_n: 1.0155310473083319 Loss: 0.054089700707605666\n",
      "Iteration: 5418 lambda_n: 1.0052506922934625 Loss: 0.054076700744549996\n",
      "Iteration: 5419 lambda_n: 0.9681321613323042 Loss: 0.05406383529231586\n",
      "Iteration: 5420 lambda_n: 0.9470093313076978 Loss: 0.0540514476316234\n",
      "Iteration: 5421 lambda_n: 0.9655072033384415 Loss: 0.05403933284864806\n",
      "Iteration: 5422 lambda_n: 0.9292139100920227 Loss: 0.05402698407957217\n",
      "Iteration: 5423 lambda_n: 0.9783585750674548 Loss: 0.05401510202864242\n",
      "Iteration: 5424 lambda_n: 0.9799746467723766 Loss: 0.054002594237847035\n",
      "Iteration: 5425 lambda_n: 1.0041903618014068 Loss: 0.05399006854744026\n",
      "Iteration: 5426 lambda_n: 0.9819781452720988 Loss: 0.05397723620886548\n",
      "Iteration: 5427 lambda_n: 0.9301241971063223 Loss: 0.05396469052548609\n",
      "Iteration: 5428 lambda_n: 0.9256198443660794 Loss: 0.053952809888556434\n",
      "Iteration: 5429 lambda_n: 0.9378501196292611 Loss: 0.05394098926461716\n",
      "Iteration: 5430 lambda_n: 0.9408181753206638 Loss: 0.05392901497698998\n",
      "Iteration: 5431 lambda_n: 0.9821571856521955 Loss: 0.05391700534712023\n",
      "Iteration: 5432 lambda_n: 0.9785702026485762 Loss: 0.05390447075123416\n",
      "Iteration: 5433 lambda_n: 0.9155335426481203 Loss: 0.05389198470911422\n",
      "Iteration: 5434 lambda_n: 0.9083195353451902 Loss: 0.05388030549194655\n",
      "Iteration: 5435 lambda_n: 0.9531703198713491 Loss: 0.05386872070165097\n",
      "Iteration: 5436 lambda_n: 0.9200888674660026 Loss: 0.05385656645220915\n",
      "Iteration: 5437 lambda_n: 0.8909604867326602 Loss: 0.053844836537318086\n",
      "Iteration: 5438 lambda_n: 0.9749695646295325 Loss: 0.0538334803126644\n",
      "Iteration: 5439 lambda_n: 0.9278919505806903 Loss: 0.053821055946492245\n",
      "Iteration: 5440 lambda_n: 0.9729194130822443 Loss: 0.05380923407231751\n",
      "Iteration: 5441 lambda_n: 1.0155405610609654 Loss: 0.0537968412120489\n",
      "Iteration: 5442 lambda_n: 0.9699484973554772 Loss: 0.053783908390640255\n",
      "Iteration: 5443 lambda_n: 0.92912753111012 Loss: 0.05377155898491431\n",
      "Iteration: 5444 lambda_n: 0.9216651782706579 Loss: 0.05375973188413388\n",
      "Iteration: 5445 lambda_n: 0.8998845695367285 Loss: 0.053748002261200335\n",
      "Iteration: 5446 lambda_n: 0.9505341840436169 Loss: 0.05373655222244818\n",
      "Iteration: 5447 lambda_n: 0.9477839310921112 Loss: 0.05372446029163527\n",
      "Iteration: 5448 lambda_n: 0.9320662840177567 Loss: 0.05371240597636627\n",
      "Iteration: 5449 lambda_n: 0.9098153800217106 Loss: 0.05370055412676512\n",
      "Iteration: 5450 lambda_n: 0.9543436332064088 Loss: 0.05368898766433568\n",
      "Iteration: 5451 lambda_n: 0.9213237597125825 Loss: 0.0536768577200305\n",
      "Iteration: 5452 lambda_n: 0.9433542497815575 Loss: 0.05366514999839026\n",
      "Iteration: 5453 lambda_n: 0.9156595629306274 Loss: 0.053653164903258915\n",
      "Iteration: 5454 lambda_n: 1.0228183905707833 Loss: 0.05364153415994361\n",
      "Iteration: 5455 lambda_n: 0.9932746637665062 Loss: 0.05362854519182011\n",
      "Iteration: 5456 lambda_n: 0.90580285688409 Loss: 0.053615934347122814\n",
      "Iteration: 5457 lambda_n: 1.022466692031264 Loss: 0.05360443659360443\n",
      "Iteration: 5458 lambda_n: 1.009390724373677 Loss: 0.053591460875284366\n",
      "Iteration: 5459 lambda_n: 1.0162154976800108 Loss: 0.0535786541182159\n",
      "Iteration: 5460 lambda_n: 0.9252265323620308 Loss: 0.05356576380367975\n",
      "Iteration: 5461 lambda_n: 0.9902387215452614 Loss: 0.05355403029843954\n",
      "Iteration: 5462 lambda_n: 0.9752774674489432 Loss: 0.053541475122219094\n",
      "Iteration: 5463 lambda_n: 0.9416071748759705 Loss: 0.053529112468818134\n",
      "Iteration: 5464 lambda_n: 0.9645024940789669 Loss: 0.05351717928782718\n",
      "Iteration: 5465 lambda_n: 0.9948796469110671 Loss: 0.05350495866763031\n",
      "Iteration: 5466 lambda_n: 0.9175372745904035 Loss: 0.05349235604095428\n",
      "Iteration: 5467 lambda_n: 0.9743789888454583 Loss: 0.05348073574555993\n",
      "Iteration: 5468 lambda_n: 0.9862512539032515 Loss: 0.053468398300196375\n",
      "Iteration: 5469 lambda_n: 1.0093897110131669 Loss: 0.053455913396443434\n",
      "Iteration: 5470 lambda_n: 1.0244224513387905 Loss: 0.05344313857241122\n",
      "Iteration: 5471 lambda_n: 0.8769319588423746 Loss: 0.053430176587186816\n",
      "Iteration: 5472 lambda_n: 0.8918257401212581 Loss: 0.053419083271508204\n",
      "Iteration: 5473 lambda_n: 0.950829930084432 Loss: 0.0534078038922567\n",
      "Iteration: 5474 lambda_n: 0.9067822119193601 Loss: 0.05339578086282189\n",
      "Iteration: 5475 lambda_n: 0.9149534739268639 Loss: 0.053384317314087555\n",
      "Iteration: 5476 lambda_n: 0.9780160062539359 Loss: 0.0533727529468207\n",
      "Iteration: 5477 lambda_n: 0.9052376555852477 Loss: 0.053360394272926034\n",
      "Iteration: 5478 lambda_n: 0.8855100962341449 Loss: 0.0533489578033729\n",
      "Iteration: 5479 lambda_n: 0.96671308350325 Loss: 0.053337772931776876\n",
      "Iteration: 5480 lambda_n: 1.027486529620136 Loss: 0.05332556505987549\n",
      "Iteration: 5481 lambda_n: 0.9227103616726993 Loss: 0.05331259278921964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5482 lambda_n: 1.0243415693603404 Loss: 0.053300946034544404\n",
      "Iteration: 5483 lambda_n: 0.9659634339098021 Loss: 0.05328801944195398\n",
      "Iteration: 5484 lambda_n: 0.9165255248117402 Loss: 0.05327583242650021\n",
      "Iteration: 5485 lambda_n: 0.9307282249650806 Loss: 0.05326427172683994\n",
      "Iteration: 5486 lambda_n: 0.961737219802472 Loss: 0.05325253445803559\n",
      "Iteration: 5487 lambda_n: 0.9483954315109259 Loss: 0.05324040887079073\n",
      "Iteration: 5488 lambda_n: 0.877956466144107 Loss: 0.05322845421713421\n",
      "Iteration: 5489 lambda_n: 1.006864177459539 Loss: 0.05321738986560355\n",
      "Iteration: 5490 lambda_n: 0.974224720908044 Loss: 0.05320470382206734\n",
      "Iteration: 5491 lambda_n: 0.9559506568843407 Loss: 0.05319243192525371\n",
      "Iteration: 5492 lambda_n: 0.9265718311074191 Loss: 0.05318039299733029\n",
      "Iteration: 5493 lambda_n: 0.8902160951945083 Loss: 0.053168726684822844\n",
      "Iteration: 5494 lambda_n: 0.9857504818671438 Loss: 0.053157520559837634\n",
      "Iteration: 5495 lambda_n: 1.0194281768677036 Loss: 0.05314511462885974\n",
      "Iteration: 5496 lambda_n: 0.8909875509987669 Loss: 0.05313228794130796\n",
      "Iteration: 5497 lambda_n: 1.004667162371043 Loss: 0.053121079895841174\n",
      "Iteration: 5498 lambda_n: 0.9793047198034431 Loss: 0.05310844471390071\n",
      "Iteration: 5499 lambda_n: 0.9433829355400222 Loss: 0.05309613144173508\n",
      "Iteration: 5500 lambda_n: 1.0119866372485113 Loss: 0.05308427257755759\n",
      "Iteration: 5501 lambda_n: 1.014572081056889 Loss: 0.05307155432339309\n",
      "Iteration: 5502 lambda_n: 1.0128448363397606 Loss: 0.05305880669352274\n",
      "Iteration: 5503 lambda_n: 0.929883148277497 Loss: 0.05304608388109227\n",
      "Iteration: 5504 lambda_n: 0.8767492705024944 Loss: 0.05303440593147412\n",
      "Iteration: 5505 lambda_n: 0.8847913713962421 Loss: 0.053023397670767955\n",
      "Iteration: 5506 lambda_n: 0.8977935546038796 Loss: 0.05301229080469729\n",
      "Iteration: 5507 lambda_n: 0.8892377578334641 Loss: 0.05300102315551574\n",
      "Iteration: 5508 lambda_n: 0.9521075712030104 Loss: 0.05298986530364594\n",
      "Iteration: 5509 lambda_n: 0.9331346528894865 Loss: 0.05297792125310126\n",
      "Iteration: 5510 lambda_n: 0.9316783680168895 Loss: 0.052966217896281476\n",
      "Iteration: 5511 lambda_n: 0.9362239966055222 Loss: 0.0529545354539714\n",
      "Iteration: 5512 lambda_n: 0.9690103183769576 Loss: 0.052942798682332745\n",
      "Iteration: 5513 lambda_n: 0.9648259635082087 Loss: 0.05293065371149644\n",
      "Iteration: 5514 lambda_n: 0.9776651314026231 Loss: 0.05291856403617321\n",
      "Iteration: 5515 lambda_n: 0.9535965517337787 Loss: 0.05290631638497794\n",
      "Iteration: 5516 lambda_n: 0.9748455910024449 Loss: 0.05289437307008429\n",
      "Iteration: 5517 lambda_n: 0.9933550450270788 Loss: 0.05288216650066476\n",
      "Iteration: 5518 lambda_n: 0.94414370270111 Loss: 0.05286973116052852\n",
      "Iteration: 5519 lambda_n: 0.8795420430301331 Loss: 0.052857914679087174\n",
      "Iteration: 5520 lambda_n: 0.882040336441759 Loss: 0.0528469091846593\n",
      "Iteration: 5521 lambda_n: 0.8947600889841681 Loss: 0.05283587481561854\n",
      "Iteration: 5522 lambda_n: 0.9165582913329626 Loss: 0.05282468376461191\n",
      "Iteration: 5523 lambda_n: 0.9470130868181187 Loss: 0.052813222629043144\n",
      "Iteration: 5524 lambda_n: 0.977403447860294 Loss: 0.05280138338561947\n",
      "Iteration: 5525 lambda_n: 0.9907377528538993 Loss: 0.05278916710778123\n",
      "Iteration: 5526 lambda_n: 1.0254037221188328 Loss: 0.05277678717246948\n",
      "Iteration: 5527 lambda_n: 0.9705438317987333 Loss: 0.0527639772498165\n",
      "Iteration: 5528 lambda_n: 0.9847586193661071 Loss: 0.05275185565658063\n",
      "Iteration: 5529 lambda_n: 0.8925164269905059 Loss: 0.05273955950075566\n",
      "Iteration: 5530 lambda_n: 1.019324616191004 Loss: 0.05272841771257495\n",
      "Iteration: 5531 lambda_n: 0.9226737164628234 Loss: 0.05271569591878546\n",
      "Iteration: 5532 lambda_n: 0.9548942867501893 Loss: 0.052704183159328497\n",
      "Iteration: 5533 lambda_n: 1.0156943393885822 Loss: 0.052692271139879114\n",
      "Iteration: 5534 lambda_n: 0.8879258393989924 Loss: 0.05267960375876109\n",
      "Iteration: 5535 lambda_n: 0.889387416426595 Loss: 0.05266853248159374\n",
      "Iteration: 5536 lambda_n: 0.999931561595864 Loss: 0.05265744543259616\n",
      "Iteration: 5537 lambda_n: 1.0052399027864931 Loss: 0.05264498327964596\n",
      "Iteration: 5538 lambda_n: 0.8959776544983865 Loss: 0.05263245809989532\n",
      "Iteration: 5539 lambda_n: 0.9867812180943549 Loss: 0.05262129696377075\n",
      "Iteration: 5540 lambda_n: 0.9220561891152684 Loss: 0.052609007583410375\n",
      "Iteration: 5541 lambda_n: 0.8948045566246942 Loss: 0.05259752702862796\n",
      "Iteration: 5542 lambda_n: 0.9715081079793578 Loss: 0.05258638831711658\n",
      "Iteration: 5543 lambda_n: 0.8775627291964783 Loss: 0.052574297609971114\n",
      "Iteration: 5544 lambda_n: 1.0191190034585993 Loss: 0.05256337861176213\n",
      "Iteration: 5545 lambda_n: 1.0173522212064607 Loss: 0.05255070132860918\n",
      "Iteration: 5546 lambda_n: 0.9802531788080007 Loss: 0.05253804925896099\n",
      "Iteration: 5547 lambda_n: 0.9856693758059165 Loss: 0.05252586162351983\n",
      "Iteration: 5548 lambda_n: 0.9741195150508276 Loss: 0.052513609678334065\n",
      "Iteration: 5549 lambda_n: 1.0172387358581458 Loss: 0.05250150428667345\n",
      "Iteration: 5550 lambda_n: 1.0286584376340246 Loss: 0.05248886622472259\n",
      "Iteration: 5551 lambda_n: 0.9007986645193835 Loss: 0.052476089584147255\n",
      "Iteration: 5552 lambda_n: 0.8788788977788985 Loss: 0.052464903775320634\n",
      "Iteration: 5553 lambda_n: 0.9102380835501956 Loss: 0.0524539926134883\n",
      "Iteration: 5554 lambda_n: 0.9304317326818994 Loss: 0.052442694689194755\n",
      "Iteration: 5555 lambda_n: 1.0251399449674237 Loss: 0.052431148811605294\n",
      "Iteration: 5556 lambda_n: 0.9783164401895857 Loss: 0.05241843083709269\n",
      "Iteration: 5557 lambda_n: 0.9793320903120398 Loss: 0.052406296843417154\n",
      "Iteration: 5558 lambda_n: 1.018678900854387 Loss: 0.05239415327184624\n",
      "Iteration: 5559 lambda_n: 0.9919931133751002 Loss: 0.052381525013167056\n",
      "Iteration: 5560 lambda_n: 0.9000833339251832 Loss: 0.05236923071570731\n",
      "Iteration: 5561 lambda_n: 0.9424751541977866 Loss: 0.052358078192507734\n",
      "Iteration: 5562 lambda_n: 0.9167694540792194 Loss: 0.0523464031538709\n",
      "Iteration: 5563 lambda_n: 0.902767808872736 Loss: 0.05233504924153053\n",
      "Iteration: 5564 lambda_n: 0.9991087607152926 Loss: 0.05232387133270703\n",
      "Iteration: 5565 lambda_n: 1.0277506917940449 Loss: 0.052311503554180995\n",
      "Iteration: 5566 lambda_n: 0.9335112249098656 Loss: 0.052298784520996816\n",
      "Iteration: 5567 lambda_n: 0.8940176182298001 Loss: 0.0522872346589458\n",
      "Iteration: 5568 lambda_n: 0.9744606155974526 Loss: 0.052276176021915616\n",
      "Iteration: 5569 lambda_n: 0.8868476487943242 Loss: 0.05226412522638248\n",
      "Iteration: 5570 lambda_n: 1.011393767580981 Loss: 0.052253160528932203\n",
      "Iteration: 5571 lambda_n: 0.9649157126522817 Loss: 0.05224065903378778\n",
      "Iteration: 5572 lambda_n: 1.0083968309527749 Loss: 0.052228735069912074\n",
      "Iteration: 5573 lambda_n: 0.9934835217293967 Loss: 0.052216276953169244\n",
      "Iteration: 5574 lambda_n: 0.9900043812616277 Loss: 0.05220400624753036\n",
      "Iteration: 5575 lambda_n: 0.8898519601417607 Loss: 0.05219178164199541\n",
      "Iteration: 5576 lambda_n: 0.9901195973139685 Loss: 0.05218079638870988\n",
      "Iteration: 5577 lambda_n: 1.0184436150328016 Loss: 0.052168576297656456\n",
      "Iteration: 5578 lambda_n: 0.9695594534792743 Loss: 0.052156009896405765\n",
      "Iteration: 5579 lambda_n: 0.9551553960801065 Loss: 0.05214404974787597\n",
      "Iteration: 5580 lambda_n: 0.9979352171803343 Loss: 0.05213227022194889\n",
      "Iteration: 5581 lambda_n: 0.9622990601648567 Loss: 0.05211996622888406\n",
      "Iteration: 5582 lambda_n: 0.8956494050140952 Loss: 0.052108104630029795\n",
      "Iteration: 5583 lambda_n: 1.0191019633731018 Loss: 0.05209706724159086\n",
      "Iteration: 5584 lambda_n: 0.8982166752446824 Loss: 0.05208451163409115\n",
      "Iteration: 5585 lambda_n: 1.0223044920705755 Loss: 0.052073448128275135\n",
      "Iteration: 5586 lambda_n: 0.9176309502413067 Loss: 0.052060859361062674\n",
      "Iteration: 5587 lambda_n: 0.9859125363727212 Loss: 0.05204956241475782\n",
      "Iteration: 5588 lambda_n: 1.0254889546512789 Loss: 0.05203742787139351\n",
      "Iteration: 5589 lambda_n: 0.9103611483775953 Loss: 0.05202480954235596\n",
      "Iteration: 5590 lambda_n: 0.9232429402007534 Loss: 0.05201361066284559\n",
      "Iteration: 5591 lambda_n: 0.9488338206866644 Loss: 0.052002256043948765\n",
      "Iteration: 5592 lambda_n: 0.8865390618932703 Loss: 0.05199058955567957\n",
      "Iteration: 5593 lambda_n: 0.9223477414547916 Loss: 0.05197969164387409\n",
      "Iteration: 5594 lambda_n: 0.9298723854092293 Loss: 0.05196835624153416\n",
      "Iteration: 5595 lambda_n: 1.0222158249521391 Loss: 0.05195693114511255\n",
      "Iteration: 5596 lambda_n: 0.9372317046385479 Loss: 0.05194437467430701\n",
      "Iteration: 5597 lambda_n: 0.987035320371194 Loss: 0.05193286508295484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5598 lambda_n: 0.9100887583038051 Loss: 0.05192074695664099\n",
      "Iteration: 5599 lambda_n: 0.9696677465239196 Loss: 0.051909576322383404\n",
      "Iteration: 5600 lambda_n: 0.9246667511576705 Loss: 0.05189767735622198\n",
      "Iteration: 5601 lambda_n: 1.0196644762901705 Loss: 0.051886333445968914\n",
      "Iteration: 5602 lambda_n: 1.002029532237026 Loss: 0.05187382731089288\n",
      "Iteration: 5603 lambda_n: 0.8904394665147848 Loss: 0.05186154075740013\n",
      "Iteration: 5604 lambda_n: 0.9355748005308042 Loss: 0.05185062522276058\n",
      "Iteration: 5605 lambda_n: 0.9386603589758753 Loss: 0.05183915917059707\n",
      "Iteration: 5606 lambda_n: 0.889746056544533 Loss: 0.05182765816578702\n",
      "Iteration: 5607 lambda_n: 0.8870597452049787 Loss: 0.05181675913604862\n",
      "Iteration: 5608 lambda_n: 0.910970915102053 Loss: 0.051805895580660025\n",
      "Iteration: 5609 lambda_n: 0.9102667142870846 Loss: 0.05179474186287924\n",
      "Iteration: 5610 lambda_n: 0.8930656740022606 Loss: 0.05178359947184103\n",
      "Iteration: 5611 lambda_n: 0.9586595200070794 Loss: 0.051772670264430075\n",
      "Iteration: 5612 lambda_n: 0.9007335958471637 Loss: 0.051760941229011904\n",
      "Iteration: 5613 lambda_n: 0.9463386585945059 Loss: 0.05174992364513622\n",
      "Iteration: 5614 lambda_n: 0.9425121221735403 Loss: 0.05173835108894246\n",
      "Iteration: 5615 lambda_n: 0.9096063103040475 Loss: 0.05172682824057131\n",
      "Iteration: 5616 lambda_n: 0.8756138117925036 Loss: 0.05171571044741571\n",
      "Iteration: 5617 lambda_n: 0.9945101734625371 Loss: 0.05170501069442785\n",
      "Iteration: 5618 lambda_n: 1.0198384881244793 Loss: 0.05169286111182823\n",
      "Iteration: 5619 lambda_n: 0.9504670306515094 Loss: 0.05168040547358536\n",
      "Iteration: 5620 lambda_n: 0.906003614617953 Loss: 0.0516688001684484\n",
      "Iteration: 5621 lambda_n: 0.9069092988727645 Loss: 0.05165774053068298\n",
      "Iteration: 5622 lambda_n: 1.0198809372022923 Loss: 0.05164667254104975\n",
      "Iteration: 5623 lambda_n: 0.9792750297355952 Loss: 0.051634229070861105\n",
      "Iteration: 5624 lambda_n: 0.8850681805011327 Loss: 0.051622284253431565\n",
      "Iteration: 5625 lambda_n: 0.911716434091043 Loss: 0.05161149125382469\n",
      "Iteration: 5626 lambda_n: 0.9351699490966262 Loss: 0.05160037599233396\n",
      "Iteration: 5627 lambda_n: 0.9463757747658151 Loss: 0.051588977645417\n",
      "Iteration: 5628 lambda_n: 1.0013420793400614 Loss: 0.0515774456560975\n",
      "Iteration: 5629 lambda_n: 0.9326098844754374 Loss: 0.05156524710161059\n",
      "Iteration: 5630 lambda_n: 0.9925905544324356 Loss: 0.05155388883895999\n",
      "Iteration: 5631 lambda_n: 0.9029843153235265 Loss: 0.05154180323306326\n",
      "Iteration: 5632 lambda_n: 0.9201258100146835 Loss: 0.05153081149076899\n",
      "Iteration: 5633 lambda_n: 0.9500068127403609 Loss: 0.05151961386972841\n",
      "Iteration: 5634 lambda_n: 0.970742978319077 Loss: 0.05150805555240194\n",
      "Iteration: 5635 lambda_n: 0.9814479107106373 Loss: 0.05149624804050512\n",
      "Iteration: 5636 lambda_n: 0.9990752369353579 Loss: 0.05148431350126937\n",
      "Iteration: 5637 lambda_n: 0.9828615248529284 Loss: 0.051472167898564895\n",
      "Iteration: 5638 lambda_n: 0.9403591090792047 Loss: 0.05146022264188166\n",
      "Iteration: 5639 lambda_n: 1.0107869252051416 Loss: 0.051448796949123556\n",
      "Iteration: 5640 lambda_n: 0.8978042264456715 Loss: 0.05143651881698161\n",
      "Iteration: 5641 lambda_n: 0.9238281413438183 Loss: 0.0514256159518341\n",
      "Iteration: 5642 lambda_n: 0.9197161138344007 Loss: 0.05141439985967024\n",
      "Iteration: 5643 lambda_n: 0.9608821037637856 Loss: 0.05140323651950624\n",
      "Iteration: 5644 lambda_n: 1.0028179022384585 Loss: 0.0513915765306813\n",
      "Iteration: 5645 lambda_n: 0.89644316678049 Loss: 0.05137941095404473\n",
      "Iteration: 5646 lambda_n: 0.9855835206559983 Loss: 0.05136853869683398\n",
      "Iteration: 5647 lambda_n: 0.9777450753224569 Loss: 0.051356588428643914\n",
      "Iteration: 5648 lambda_n: 0.913077837242718 Loss: 0.0513447364151857\n",
      "Iteration: 5649 lambda_n: 0.9377096642349203 Loss: 0.051333671175642304\n",
      "Iteration: 5650 lambda_n: 0.9989386513842947 Loss: 0.05132231034123016\n",
      "Iteration: 5651 lambda_n: 0.9734474982366014 Loss: 0.05131021093083939\n",
      "Iteration: 5652 lambda_n: 0.9580813244678588 Loss: 0.05129842349851329\n",
      "Iteration: 5653 lambda_n: 0.9658686205762584 Loss: 0.051286825242546616\n",
      "Iteration: 5654 lambda_n: 1.007212681575563 Loss: 0.05127513583899429\n",
      "Iteration: 5655 lambda_n: 1.0173066798667958 Loss: 0.05126294941223731\n",
      "Iteration: 5656 lambda_n: 0.929960318607614 Loss: 0.05125064432298037\n",
      "Iteration: 5657 lambda_n: 0.8776709902522389 Loss: 0.05123939880369542\n",
      "Iteration: 5658 lambda_n: 0.9163776691806459 Loss: 0.05122878826593405\n",
      "Iteration: 5659 lambda_n: 0.9087120038515196 Loss: 0.051217712558989395\n",
      "Iteration: 5660 lambda_n: 0.9337243756049601 Loss: 0.0512067323008629\n",
      "Iteration: 5661 lambda_n: 0.952531396807122 Loss: 0.05119545271526278\n",
      "Iteration: 5662 lambda_n: 0.9234281117616029 Loss: 0.05118394897361594\n",
      "Iteration: 5663 lambda_n: 0.9852068355098529 Loss: 0.051172799642607236\n",
      "Iteration: 5664 lambda_n: 0.884635509577158 Loss: 0.05116090757890824\n",
      "Iteration: 5665 lambda_n: 0.8778196785153214 Loss: 0.051150232278885946\n",
      "Iteration: 5666 lambda_n: 0.9248587920573134 Loss: 0.05113964184965832\n",
      "Iteration: 5667 lambda_n: 1.0285912032055726 Loss: 0.05112848674186458\n",
      "Iteration: 5668 lambda_n: 0.9628419124481532 Loss: 0.05111608388301333\n",
      "Iteration: 5669 lambda_n: 0.950145948534097 Loss: 0.05110447709221165\n",
      "Iteration: 5670 lambda_n: 1.012919467463375 Loss: 0.05109302643621445\n",
      "Iteration: 5671 lambda_n: 0.8792521177502994 Loss: 0.05108082264775865\n",
      "Iteration: 5672 lambda_n: 1.028277173443258 Loss: 0.05107023213176545\n",
      "Iteration: 5673 lambda_n: 0.9976099183809348 Loss: 0.05105784996097142\n",
      "Iteration: 5674 lambda_n: 1.0184313743685578 Loss: 0.051045840517818915\n",
      "Iteration: 5675 lambda_n: 0.9256395165263563 Loss: 0.0510335839373745\n",
      "Iteration: 5676 lambda_n: 0.91803335573394 Loss: 0.05102244714321775\n",
      "Iteration: 5677 lambda_n: 0.9803761611022579 Loss: 0.0510114047487002\n",
      "Iteration: 5678 lambda_n: 0.9472510196371553 Loss: 0.050999615653841444\n",
      "Iteration: 5679 lambda_n: 0.9220153779292609 Loss: 0.05098822801183576\n",
      "Iteration: 5680 lambda_n: 0.9260041754739207 Loss: 0.050977146694313684\n",
      "Iteration: 5681 lambda_n: 0.9680549086404842 Loss: 0.050966020366166875\n",
      "Iteration: 5682 lambda_n: 0.9559060320294674 Loss: 0.05095439192150068\n",
      "Iteration: 5683 lambda_n: 0.8867910346671031 Loss: 0.05094291256362786\n",
      "Iteration: 5684 lambda_n: 1.0263584506774002 Loss: 0.050932266001965716\n",
      "Iteration: 5685 lambda_n: 0.9721749469926414 Loss: 0.05091994720310969\n",
      "Iteration: 5686 lambda_n: 0.9933276432139201 Loss: 0.05090828207475193\n",
      "Iteration: 5687 lambda_n: 0.879884854682811 Loss: 0.05089636649085218\n",
      "Iteration: 5688 lambda_n: 0.9016975277837674 Loss: 0.05088581455863366\n",
      "Iteration: 5689 lambda_n: 0.8990797594180315 Loss: 0.050875003804762016\n",
      "Iteration: 5690 lambda_n: 1.0253939314227416 Loss: 0.050864227224272696\n",
      "Iteration: 5691 lambda_n: 0.9837379619921712 Loss: 0.050851940012975425\n",
      "Iteration: 5692 lambda_n: 0.9564112380783804 Loss: 0.05084015536986557\n",
      "Iteration: 5693 lambda_n: 0.9890431551367467 Loss: 0.05082870128787912\n",
      "Iteration: 5694 lambda_n: 1.0128424971234695 Loss: 0.05081685972509769\n",
      "Iteration: 5695 lambda_n: 0.9695710641375512 Loss: 0.050804736722404834\n",
      "Iteration: 5696 lambda_n: 0.8828463665618055 Loss: 0.05079313497151511\n",
      "Iteration: 5697 lambda_n: 0.943004831587507 Loss: 0.05078257378592598\n",
      "Iteration: 5698 lambda_n: 0.90568304637542 Loss: 0.05077129592665418\n",
      "Iteration: 5699 lambda_n: 0.9388807049535858 Loss: 0.05076046731790849\n",
      "Iteration: 5700 lambda_n: 0.9775176540737235 Loss: 0.050749244790858274\n",
      "Iteration: 5701 lambda_n: 0.8881737459949338 Loss: 0.050737563682799997\n",
      "Iteration: 5702 lambda_n: 0.9481093747589998 Loss: 0.05072695309032641\n",
      "Iteration: 5703 lambda_n: 1.0142439736378999 Loss: 0.0507156294996342\n",
      "Iteration: 5704 lambda_n: 0.9378827371288891 Loss: 0.05070351950094838\n",
      "Iteration: 5705 lambda_n: 0.9840915258967377 Loss: 0.050692324434722096\n",
      "Iteration: 5706 lambda_n: 1.0257085441482514 Loss: 0.05068058108834114\n",
      "Iteration: 5707 lambda_n: 0.9268541872149598 Loss: 0.05066834470940925\n",
      "Iteration: 5708 lambda_n: 0.956752318868077 Loss: 0.050657290786448955\n",
      "Iteration: 5709 lambda_n: 0.957776469624344 Loss: 0.05064588343362486\n",
      "Iteration: 5710 lambda_n: 0.9612743009573728 Loss: 0.050634467070673574\n",
      "Iteration: 5711 lambda_n: 1.0024870656493072 Loss: 0.05062301223707176\n",
      "Iteration: 5712 lambda_n: 0.9024989271302445 Loss: 0.05061106974087593\n",
      "Iteration: 5713 lambda_n: 0.9156795673333277 Loss: 0.05060032139739144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5714 lambda_n: 1.0060472776115785 Loss: 0.05058941899262746\n",
      "Iteration: 5715 lambda_n: 0.9932740811125594 Loss: 0.05057744402415583\n",
      "Iteration: 5716 lambda_n: 1.017033540036561 Loss: 0.05056562457549284\n",
      "Iteration: 5717 lambda_n: 0.9643575293736141 Loss: 0.050553525987540315\n",
      "Iteration: 5718 lambda_n: 1.020663330496046 Loss: 0.05054205738489957\n",
      "Iteration: 5719 lambda_n: 0.9390768966872928 Loss: 0.0505299227252013\n",
      "Iteration: 5720 lambda_n: 0.9270453100437978 Loss: 0.05051876128072638\n",
      "Iteration: 5721 lambda_n: 0.8934770099641474 Loss: 0.05050774588099722\n",
      "Iteration: 5722 lambda_n: 0.9484316055419597 Loss: 0.05049713221152978\n",
      "Iteration: 5723 lambda_n: 0.9382899554319399 Loss: 0.05048586880952496\n",
      "Iteration: 5724 lambda_n: 0.9914697063191543 Loss: 0.050474728967674916\n",
      "Iteration: 5725 lambda_n: 0.9176537324354721 Loss: 0.05046296112361805\n",
      "Iteration: 5726 lambda_n: 0.8882843787766495 Loss: 0.0504520724996124\n",
      "Iteration: 5727 lambda_n: 1.0063213094525385 Loss: 0.05044153519691939\n",
      "Iteration: 5728 lambda_n: 0.9687173187131342 Loss: 0.05042960104619559\n",
      "Iteration: 5729 lambda_n: 0.8780433328722951 Loss: 0.05041811623118524\n",
      "Iteration: 5730 lambda_n: 1.0229771724274477 Loss: 0.05040770928824368\n",
      "Iteration: 5731 lambda_n: 0.882862355430809 Loss: 0.05039558797108628\n",
      "Iteration: 5732 lambda_n: 1.001843445395196 Loss: 0.05038512986385833\n",
      "Iteration: 5733 lambda_n: 0.8821755592860803 Loss: 0.0503732656921079\n",
      "Iteration: 5734 lambda_n: 1.0118639038072597 Loss: 0.05036282161695905\n",
      "Iteration: 5735 lambda_n: 0.9882000957713997 Loss: 0.05035084556417007\n",
      "Iteration: 5736 lambda_n: 1.016393065391133 Loss: 0.05033915309890876\n",
      "Iteration: 5737 lambda_n: 0.8940704008723659 Loss: 0.050327130674344564\n",
      "Iteration: 5738 lambda_n: 0.9657468326085568 Loss: 0.05031655818449969\n",
      "Iteration: 5739 lambda_n: 0.9530840603652081 Loss: 0.05030514130978389\n",
      "Iteration: 5740 lambda_n: 0.9667772826985233 Loss: 0.050293877389857994\n",
      "Iteration: 5741 lambda_n: 0.9464963305801994 Loss: 0.05028245494683165\n",
      "Iteration: 5742 lambda_n: 0.8811808158666738 Loss: 0.050271275353032684\n",
      "Iteration: 5743 lambda_n: 0.9899677083941669 Loss: 0.050260870111902396\n",
      "Iteration: 5744 lambda_n: 0.9123719736162422 Loss: 0.0502491835920839\n",
      "Iteration: 5745 lambda_n: 0.8858660628626238 Loss: 0.050238416188433326\n",
      "Iteration: 5746 lambda_n: 1.0045311167525264 Loss: 0.05022796444496233\n",
      "Iteration: 5747 lambda_n: 0.9241981302529841 Loss: 0.05021611605062714\n",
      "Iteration: 5748 lambda_n: 0.9404690006413088 Loss: 0.05020521837209687\n",
      "Iteration: 5749 lambda_n: 0.9728052801982335 Loss: 0.050194131978294296\n",
      "Iteration: 5750 lambda_n: 1.0172070391429462 Loss: 0.05018266773722934\n",
      "Iteration: 5751 lambda_n: 0.965981703664571 Loss: 0.050170683866508366\n",
      "Iteration: 5752 lambda_n: 1.0272240241854207 Loss: 0.05015930692993536\n",
      "Iteration: 5753 lambda_n: 0.8833958112994321 Loss: 0.05014721238632029\n",
      "Iteration: 5754 lambda_n: 0.9628555382932645 Loss: 0.05013681431151214\n",
      "Iteration: 5755 lambda_n: 0.9653525966530531 Loss: 0.05012548414962992\n",
      "Iteration: 5756 lambda_n: 0.8846501475873793 Loss: 0.050114127956555767\n",
      "Iteration: 5757 lambda_n: 0.8787643245023465 Loss: 0.05010372407853866\n",
      "Iteration: 5758 lambda_n: 0.9817841824406492 Loss: 0.0500933922141563\n",
      "Iteration: 5759 lambda_n: 0.9301490546422873 Loss: 0.050081852411811276\n",
      "Iteration: 5760 lambda_n: 1.0138145480628025 Loss: 0.05007092273260372\n",
      "Iteration: 5761 lambda_n: 0.9911569104420375 Loss: 0.05005901351702007\n",
      "Iteration: 5762 lambda_n: 0.9530946266321572 Loss: 0.050047374048721255\n",
      "Iteration: 5763 lambda_n: 0.9429688981072167 Loss: 0.05003618490909163\n",
      "Iteration: 5764 lambda_n: 0.9456757426765906 Loss: 0.050025117878140395\n",
      "Iteration: 5765 lambda_n: 1.0084825344990722 Loss: 0.050014022313026575\n",
      "Iteration: 5766 lambda_n: 0.8974238877258947 Loss: 0.050002193411347776\n",
      "Iteration: 5767 lambda_n: 0.9207347442140628 Loss: 0.04999167026456139\n",
      "Iteration: 5768 lambda_n: 0.8798976116819438 Loss: 0.049980876814929596\n",
      "Iteration: 5769 lambda_n: 0.887653147436903 Loss: 0.04997056496280064\n",
      "Iteration: 5770 lambda_n: 0.9116530025247687 Loss: 0.04996016507260587\n",
      "Iteration: 5771 lambda_n: 0.9161676332323031 Loss: 0.0499494869797522\n",
      "Iteration: 5772 lambda_n: 1.0259243178764745 Loss: 0.04993875905532307\n",
      "Iteration: 5773 lambda_n: 0.8894688754112113 Loss: 0.049926749556561754\n",
      "Iteration: 5774 lambda_n: 1.0038722699464269 Loss: 0.04991634051389404\n",
      "Iteration: 5775 lambda_n: 0.9298380000981147 Loss: 0.04990459612744873\n",
      "Iteration: 5776 lambda_n: 1.0043482532764592 Loss: 0.04989372115539103\n",
      "Iteration: 5777 lambda_n: 0.9076945696737818 Loss: 0.04988197829197498\n",
      "Iteration: 5778 lambda_n: 0.9772443457243678 Loss: 0.04987136867718784\n",
      "Iteration: 5779 lambda_n: 0.9388555267771169 Loss: 0.0498599494961749\n",
      "Iteration: 5780 lambda_n: 0.914781889114799 Loss: 0.04984898218349256\n",
      "Iteration: 5781 lambda_n: 0.8861691538385785 Loss: 0.04983829919393631\n",
      "Iteration: 5782 lambda_n: 0.9187564593597889 Loss: 0.04982795327515163\n",
      "Iteration: 5783 lambda_n: 0.8875988905951078 Loss: 0.04981722993150413\n",
      "Iteration: 5784 lambda_n: 0.9456447222022071 Loss: 0.04980687320449844\n",
      "Iteration: 5785 lambda_n: 0.9383343194693554 Loss: 0.049795842360775625\n",
      "Iteration: 5786 lambda_n: 0.9386185832952214 Loss: 0.049784900037549075\n",
      "Iteration: 5787 lambda_n: 0.8760797734794623 Loss: 0.049773957637413206\n",
      "Iteration: 5788 lambda_n: 0.970706664644226 Loss: 0.04976374723774025\n",
      "Iteration: 5789 lambda_n: 0.9420452336353771 Loss: 0.049752437293136935\n",
      "Iteration: 5790 lambda_n: 0.9019724616294285 Loss: 0.04974146460808213\n",
      "Iteration: 5791 lambda_n: 0.8872737412049178 Loss: 0.04973096174523204\n",
      "Iteration: 5792 lambda_n: 1.0278994924902223 Loss: 0.04972063296708004\n",
      "Iteration: 5793 lambda_n: 0.9312093658251956 Loss: 0.04970867079371741\n",
      "Iteration: 5794 lambda_n: 0.8755206225389641 Loss: 0.049697837219645866\n",
      "Iteration: 5795 lambda_n: 1.0111787716008376 Loss: 0.04968765444404909\n",
      "Iteration: 5796 lambda_n: 0.9918471813384598 Loss: 0.0496758974190468\n",
      "Iteration: 5797 lambda_n: 0.9685951295307164 Loss: 0.04966436883979386\n",
      "Iteration: 5798 lambda_n: 0.9184636967575164 Loss: 0.04965311404342683\n",
      "Iteration: 5799 lambda_n: 0.919269200410633 Loss: 0.04964244497130789\n",
      "Iteration: 5800 lambda_n: 1.004874876880801 Loss: 0.04963176967461986\n",
      "Iteration: 5801 lambda_n: 0.9629495872198689 Loss: 0.04962010384313457\n",
      "Iteration: 5802 lambda_n: 1.0027926831308551 Loss: 0.049608928250550334\n",
      "Iteration: 5803 lambda_n: 0.9843643807906153 Loss: 0.04959729391800037\n",
      "Iteration: 5804 lambda_n: 0.8841080972694219 Loss: 0.04958587702615043\n",
      "Iteration: 5805 lambda_n: 0.9612348677178786 Loss: 0.04957562600383067\n",
      "Iteration: 5806 lambda_n: 1.0263146741242255 Loss: 0.04956448401727696\n",
      "Iteration: 5807 lambda_n: 1.00461114395193 Loss: 0.04955259146957298\n",
      "Iteration: 5808 lambda_n: 0.8903771006504994 Loss: 0.04954095421776912\n",
      "Iteration: 5809 lambda_n: 0.9815950760674234 Loss: 0.049530643382091136\n",
      "Iteration: 5810 lambda_n: 0.8847208807302692 Loss: 0.049519279644923864\n",
      "Iteration: 5811 lambda_n: 0.9999425489154783 Loss: 0.04950904048599413\n",
      "Iteration: 5812 lambda_n: 0.9904879318684959 Loss: 0.049497471352323956\n",
      "Iteration: 5813 lambda_n: 1.0221063378940254 Loss: 0.049486015294314574\n",
      "Iteration: 5814 lambda_n: 0.961770471068572 Loss: 0.04947419738591154\n",
      "Iteration: 5815 lambda_n: 0.9651176375369728 Loss: 0.04946308067336769\n",
      "Iteration: 5816 lambda_n: 0.9754936512513672 Loss: 0.04945192875809847\n",
      "Iteration: 5817 lambda_n: 0.9347384764564226 Loss: 0.049440660498990076\n",
      "Iteration: 5818 lambda_n: 1.026502712087981 Loss: 0.04942986636821848\n",
      "Iteration: 5819 lambda_n: 1.0005989239964277 Loss: 0.049418016347893175\n",
      "Iteration: 5820 lambda_n: 0.941103142742052 Loss: 0.049406469175087786\n",
      "Iteration: 5821 lambda_n: 0.9905002282106289 Loss: 0.0493956120361279\n",
      "Iteration: 5822 lambda_n: 1.0200926987915404 Loss: 0.049384188623361844\n",
      "Iteration: 5823 lambda_n: 0.8921761708406961 Loss: 0.04937242778438721\n",
      "Iteration: 5824 lambda_n: 0.9781667272187804 Loss: 0.04936214493446061\n",
      "Iteration: 5825 lambda_n: 0.8819883114193042 Loss: 0.049350874443804475\n",
      "Iteration: 5826 lambda_n: 0.8993049556784843 Loss: 0.049340715222418056\n",
      "Iteration: 5827 lambda_n: 0.9407299347242986 Loss: 0.049330359563690515\n",
      "Iteration: 5828 lambda_n: 1.0054839950953023 Loss: 0.04931953015984521\n",
      "Iteration: 5829 lambda_n: 1.0211447468118382 Loss: 0.0493079590268247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5830 lambda_n: 0.9177564943348092 Loss: 0.04929621158549591\n",
      "Iteration: 5831 lambda_n: 0.9073044170967471 Loss: 0.049285656911555176\n",
      "Iteration: 5832 lambda_n: 1.0087920995348265 Loss: 0.04927522557951541\n",
      "Iteration: 5833 lambda_n: 0.891197939499723 Loss: 0.049263631102663685\n",
      "Iteration: 5834 lambda_n: 0.9772007278469228 Loss: 0.0492533913982188\n",
      "Iteration: 5835 lambda_n: 0.9276070666376091 Loss: 0.04924216700364746\n",
      "Iteration: 5836 lambda_n: 0.9145039855371019 Loss: 0.04923151561229798\n",
      "Iteration: 5837 lambda_n: 0.8969882719387957 Loss: 0.04922101788174404\n",
      "Iteration: 5838 lambda_n: 0.9861737921521588 Loss: 0.04921072430780321\n",
      "Iteration: 5839 lambda_n: 1.028035989846084 Loss: 0.04919941080220016\n",
      "Iteration: 5840 lambda_n: 1.0213851224282275 Loss: 0.049187620992762304\n",
      "Iteration: 5841 lambda_n: 0.9005769748905983 Loss: 0.04917591144758155\n",
      "Iteration: 5842 lambda_n: 0.9224581100707219 Loss: 0.04916559019440809\n",
      "Iteration: 5843 lambda_n: 0.9068862612352303 Loss: 0.04915502137786642\n",
      "Iteration: 5844 lambda_n: 0.8788094693668391 Loss: 0.049144634140355956\n",
      "Iteration: 5845 lambda_n: 0.9822803848540878 Loss: 0.04913457148632962\n",
      "Iteration: 5846 lambda_n: 0.9149840799468975 Loss: 0.04912332755291978\n",
      "Iteration: 5847 lambda_n: 0.9705246912192067 Loss: 0.04911285726670947\n",
      "Iteration: 5848 lambda_n: 0.965412033568776 Loss: 0.049101754926143014\n",
      "Iteration: 5849 lambda_n: 1.0070060640147969 Loss: 0.049090714653457095\n",
      "Iteration: 5850 lambda_n: 0.9998176468060056 Loss: 0.049079202528005496\n",
      "Iteration: 5851 lambda_n: 0.9017953544988991 Loss: 0.049067776431063366\n",
      "Iteration: 5852 lambda_n: 1.0233734279783642 Loss: 0.049057473843595926\n",
      "Iteration: 5853 lambda_n: 0.8957093452354005 Loss: 0.049045786069681296\n",
      "Iteration: 5854 lambda_n: 1.0153350974895101 Loss: 0.0490355596307993\n",
      "Iteration: 5855 lambda_n: 1.0199812870426466 Loss: 0.04902397114151911\n",
      "Iteration: 5856 lambda_n: 0.9691223393854949 Loss: 0.04901233362039839\n",
      "Iteration: 5857 lambda_n: 1.0035081971902067 Loss: 0.04900128009099807\n",
      "Iteration: 5858 lambda_n: 1.023331625620251 Loss: 0.04898983818337856\n",
      "Iteration: 5859 lambda_n: 0.9884504297480986 Loss: 0.04897817425297113\n",
      "Iteration: 5860 lambda_n: 1.0239826601561497 Loss: 0.04896691173799962\n",
      "Iteration: 5861 lambda_n: 0.9182994311025546 Loss: 0.04895524834663817\n",
      "Iteration: 5862 lambda_n: 0.9828142253769104 Loss: 0.04894479215962214\n",
      "Iteration: 5863 lambda_n: 0.9566291271464438 Loss: 0.04893360499222859\n",
      "Iteration: 5864 lambda_n: 0.9863183527079834 Loss: 0.04892271947658208\n",
      "Iteration: 5865 lambda_n: 1.0150181379491936 Loss: 0.04891149983922705\n",
      "Iteration: 5866 lambda_n: 0.9503327987395601 Loss: 0.04889995767267281\n",
      "Iteration: 5867 lambda_n: 0.8909418779015226 Loss: 0.0488891546920974\n",
      "Iteration: 5868 lambda_n: 1.0292531687851914 Loss: 0.04887903002731606\n",
      "Iteration: 5869 lambda_n: 0.946020387638729 Loss: 0.04886733743251762\n",
      "Iteration: 5870 lambda_n: 0.9698760376691955 Loss: 0.048856594017508574\n",
      "Iteration: 5871 lambda_n: 0.9618822169686463 Loss: 0.04884558330244823\n",
      "Iteration: 5872 lambda_n: 0.9793831119056481 Loss: 0.04883466695616742\n",
      "Iteration: 5873 lambda_n: 0.8763738173586042 Loss: 0.048823555696992156\n",
      "Iteration: 5874 lambda_n: 1.0018945705801583 Loss: 0.048813616265168135\n",
      "Iteration: 5875 lambda_n: 0.9809318816167238 Loss: 0.04880225690505249\n",
      "Iteration: 5876 lambda_n: 0.8890183948988964 Loss: 0.048791139013979634\n",
      "Iteration: 5877 lambda_n: 1.0076065265428906 Loss: 0.04878106611836945\n",
      "Iteration: 5878 lambda_n: 0.946551449015797 Loss: 0.04876965331220868\n",
      "Iteration: 5879 lambda_n: 0.9520273430763523 Loss: 0.04875893567332587\n",
      "Iteration: 5880 lambda_n: 0.8842400348724484 Loss: 0.04874815956948069\n",
      "Iteration: 5881 lambda_n: 0.9924589350877724 Loss: 0.0487381539377284\n",
      "Iteration: 5882 lambda_n: 0.9426512631408116 Loss: 0.04872692740350186\n",
      "Iteration: 5883 lambda_n: 0.9127793361598658 Loss: 0.04871626786280319\n",
      "Iteration: 5884 lambda_n: 0.9261265285517071 Loss: 0.04870594943858446\n",
      "Iteration: 5885 lambda_n: 0.950700859179418 Loss: 0.04869548347653228\n",
      "Iteration: 5886 lambda_n: 0.9954326437437301 Loss: 0.04868474331111423\n",
      "Iteration: 5887 lambda_n: 0.8900206719012203 Loss: 0.048673501615122815\n",
      "Iteration: 5888 lambda_n: 0.9619940258825526 Loss: 0.04866345366732152\n",
      "Iteration: 5889 lambda_n: 0.9021287540041759 Loss: 0.04865259667924249\n",
      "Iteration: 5890 lambda_n: 0.9808519584455986 Loss: 0.048642418638924996\n",
      "Iteration: 5891 lambda_n: 1.0274571894554037 Loss: 0.04863135606492821\n",
      "Iteration: 5892 lambda_n: 0.9338576790820753 Loss: 0.04861977192282512\n",
      "Iteration: 5893 lambda_n: 0.9703967028041878 Loss: 0.04860924669106957\n",
      "Iteration: 5894 lambda_n: 0.9428099420570232 Loss: 0.04859831328926271\n",
      "Iteration: 5895 lambda_n: 0.9805275756762825 Loss: 0.04858769427151601\n",
      "Iteration: 5896 lambda_n: 0.9939168177395891 Loss: 0.04857665416455894\n",
      "Iteration: 5897 lambda_n: 1.0108049879856225 Loss: 0.048565467187927674\n",
      "Iteration: 5898 lambda_n: 0.9497391193608249 Loss: 0.04855409414134182\n",
      "Iteration: 5899 lambda_n: 1.00534929834355 Loss: 0.04854341186670097\n",
      "Iteration: 5900 lambda_n: 1.0063532857429947 Loss: 0.048532108010189694\n",
      "Iteration: 5901 lambda_n: 0.9166401746308057 Loss: 0.04852079688336769\n",
      "Iteration: 5902 lambda_n: 0.9871292826329752 Loss: 0.04851049760765669\n",
      "Iteration: 5903 lambda_n: 0.9276046678848913 Loss: 0.04849941005868454\n",
      "Iteration: 5904 lambda_n: 1.0283251892210747 Loss: 0.048488994629498905\n",
      "Iteration: 5905 lambda_n: 0.9179678896884637 Loss: 0.04847745228254878\n",
      "Iteration: 5906 lambda_n: 0.9884486519032377 Loss: 0.04846715218980215\n",
      "Iteration: 5907 lambda_n: 1.0180470818771825 Loss: 0.04845606502057728\n",
      "Iteration: 5908 lambda_n: 0.9568188545171902 Loss: 0.04844464992733839\n",
      "Iteration: 5909 lambda_n: 1.0024991384727733 Loss: 0.04843392514109602\n",
      "Iteration: 5910 lambda_n: 1.0182094772818087 Loss: 0.04842269225661\n",
      "Iteration: 5911 lambda_n: 1.0250969237859577 Loss: 0.048411287451472884\n",
      "Iteration: 5912 lambda_n: 0.9423099484501709 Loss: 0.04839980969003721\n",
      "Iteration: 5913 lambda_n: 0.9206984956124349 Loss: 0.04838926259210921\n",
      "Iteration: 5914 lambda_n: 0.9065488726781857 Loss: 0.04837896081649487\n",
      "Iteration: 5915 lambda_n: 0.9960742839561626 Loss: 0.048368820679840156\n",
      "Iteration: 5916 lambda_n: 0.8982838261340355 Loss: 0.04835768296272914\n",
      "Iteration: 5917 lambda_n: 1.0244889296214315 Loss: 0.048347642111753075\n",
      "Iteration: 5918 lambda_n: 0.8773333952208658 Loss: 0.04833619452015558\n",
      "Iteration: 5919 lambda_n: 0.9944293340020527 Loss: 0.048326394588553145\n",
      "Iteration: 5920 lambda_n: 0.9217663506738848 Loss: 0.04831529042229625\n",
      "Iteration: 5921 lambda_n: 1.009004336377264 Loss: 0.04830500119228529\n",
      "Iteration: 5922 lambda_n: 0.9685846268723649 Loss: 0.048293742089945864\n",
      "Iteration: 5923 lambda_n: 0.930306530760162 Loss: 0.048282937875595115\n",
      "Iteration: 5924 lambda_n: 1.0196931538293128 Loss: 0.048272564201394655\n",
      "Iteration: 5925 lambda_n: 0.9516764985297007 Loss: 0.04826119780518456\n",
      "Iteration: 5926 lambda_n: 0.9135719624525215 Loss: 0.0482505933697412\n",
      "Iteration: 5927 lambda_n: 1.0216738127447482 Loss: 0.04824041697156681\n",
      "Iteration: 5928 lambda_n: 0.9464892780440937 Loss: 0.048229040409357465\n",
      "Iteration: 5929 lambda_n: 0.9380486164465207 Loss: 0.04821850479586904\n",
      "Iteration: 5930 lambda_n: 0.9691372949925356 Loss: 0.04820806672218642\n",
      "Iteration: 5931 lambda_n: 0.9938737981292584 Loss: 0.048197286454212056\n",
      "Iteration: 5932 lambda_n: 0.9639059788504817 Loss: 0.048186235024402425\n",
      "Iteration: 5933 lambda_n: 0.9601389934001194 Loss: 0.04817552063764361\n",
      "Iteration: 5934 lambda_n: 0.9916781927227193 Loss: 0.0481648518647812\n",
      "Iteration: 5935 lambda_n: 0.9362523520528584 Loss: 0.04815383656407935\n",
      "Iteration: 5936 lambda_n: 0.9276070155969083 Loss: 0.0481434405845244\n",
      "Iteration: 5937 lambda_n: 1.0122338479747677 Loss: 0.04813314411569382\n",
      "Iteration: 5938 lambda_n: 0.8848436405828296 Loss: 0.04812191228104411\n",
      "Iteration: 5939 lambda_n: 0.9393826965782919 Loss: 0.04811209739684825\n",
      "Iteration: 5940 lambda_n: 0.9713653781001343 Loss: 0.048101681044762507\n",
      "Iteration: 5941 lambda_n: 0.93569548396649 Loss: 0.0480909138367225\n",
      "Iteration: 5942 lambda_n: 0.9380289879903401 Loss: 0.04808054565633891\n",
      "Iteration: 5943 lambda_n: 1.019588973272142 Loss: 0.04807015520746464\n",
      "Iteration: 5944 lambda_n: 0.8883757746710448 Loss: 0.048058865404982665\n",
      "Iteration: 5945 lambda_n: 0.907970927471224 Loss: 0.048049031977782285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5946 lambda_n: 0.9252933284213198 Loss: 0.04803898498854501\n",
      "Iteration: 5947 lambda_n: 0.9892718299604388 Loss: 0.0480287497937277\n",
      "Iteration: 5948 lambda_n: 0.9807888771856539 Loss: 0.048017810775975694\n",
      "Iteration: 5949 lambda_n: 0.9384380770339705 Loss: 0.0480069695201783\n",
      "Iteration: 5950 lambda_n: 0.8780948846684055 Loss: 0.04799660008767711\n",
      "Iteration: 5951 lambda_n: 0.985153858271588 Loss: 0.04798690070139979\n",
      "Iteration: 5952 lambda_n: 0.9549344541128408 Loss: 0.047976022517577696\n",
      "Iteration: 5953 lambda_n: 0.9344065211577748 Loss: 0.047965481827283565\n",
      "Iteration: 5954 lambda_n: 1.0244994330389552 Loss: 0.04795517135742913\n",
      "Iteration: 5955 lambda_n: 0.9064647443200936 Loss: 0.04794387091019735\n",
      "Iteration: 5956 lambda_n: 1.0208809841796722 Loss: 0.04793387598051881\n",
      "Iteration: 5957 lambda_n: 0.9165238276002642 Loss: 0.047922623539997516\n",
      "Iteration: 5958 lambda_n: 0.9995787814011001 Loss: 0.04791252502305508\n",
      "Iteration: 5959 lambda_n: 0.8899447020781478 Loss: 0.047901515339777004\n",
      "Iteration: 5960 lambda_n: 0.8763869446433744 Loss: 0.04789171667619296\n",
      "Iteration: 5961 lambda_n: 0.9042729342762794 Loss: 0.04788207048836522\n",
      "Iteration: 5962 lambda_n: 0.9890224717500522 Loss: 0.04787212070373941\n",
      "Iteration: 5963 lambda_n: 0.9251499663499844 Loss: 0.04786124229235337\n",
      "Iteration: 5964 lambda_n: 0.9243521533711809 Loss: 0.04785107008490791\n",
      "Iteration: 5965 lambda_n: 0.9593635056372157 Loss: 0.04784091018758532\n",
      "Iteration: 5966 lambda_n: 1.0003081587809206 Loss: 0.04783036921108974\n",
      "Iteration: 5967 lambda_n: 0.8869291640185037 Loss: 0.0478193824207632\n",
      "Iteration: 5968 lambda_n: 1.001732349765031 Loss: 0.04780964439102083\n",
      "Iteration: 5969 lambda_n: 0.9734547724494911 Loss: 0.04779864980950838\n",
      "Iteration: 5970 lambda_n: 1.0234944355408948 Loss: 0.04778796958590364\n",
      "Iteration: 5971 lambda_n: 0.9896385437237334 Loss: 0.04777674460414847\n",
      "Iteration: 5972 lambda_n: 0.8820162280189512 Loss: 0.047765895076321875\n",
      "Iteration: 5973 lambda_n: 0.9652892818026383 Loss: 0.047756228862681716\n",
      "Iteration: 5974 lambda_n: 0.9785215840885799 Loss: 0.04774565375665326\n",
      "Iteration: 5975 lambda_n: 0.9040581650444964 Loss: 0.04773493765177638\n",
      "Iteration: 5976 lambda_n: 0.9127539735748631 Loss: 0.04772504057087544\n",
      "Iteration: 5977 lambda_n: 0.996646073254708 Loss: 0.047715051755778726\n",
      "Iteration: 5978 lambda_n: 0.9479047202081001 Loss: 0.04770414883473824\n",
      "Iteration: 5979 lambda_n: 0.8926238242679266 Loss: 0.04769378297859333\n",
      "Iteration: 5980 lambda_n: 0.9597390250976519 Loss: 0.04768402508608047\n",
      "Iteration: 5981 lambda_n: 0.9812224558416544 Loss: 0.0476735372319708\n",
      "Iteration: 5982 lambda_n: 0.9405587433077534 Loss: 0.047662818600099806\n",
      "Iteration: 5983 lambda_n: 0.9491143793684595 Loss: 0.047652547957263806\n",
      "Iteration: 5984 lambda_n: 0.8943927626192889 Loss: 0.04764218765098569\n",
      "Iteration: 5985 lambda_n: 0.9705252849664524 Loss: 0.04763242813330849\n",
      "Iteration: 5986 lambda_n: 0.8937967325609985 Loss: 0.047621841666846035\n",
      "Iteration: 5987 lambda_n: 1.0169880819332244 Loss: 0.047612095655294714\n",
      "Iteration: 5988 lambda_n: 0.9392829488705557 Loss: 0.04760101044445574\n",
      "Iteration: 5989 lambda_n: 0.9666741874439037 Loss: 0.04759077608917697\n",
      "Iteration: 5990 lambda_n: 0.9633620815007652 Loss: 0.04758024716056738\n",
      "Iteration: 5991 lambda_n: 0.8985910559057325 Loss: 0.047569758224772066\n",
      "Iteration: 5992 lambda_n: 0.9427720309116777 Loss: 0.047559978033424335\n",
      "Iteration: 5993 lambda_n: 0.9569749091098997 Loss: 0.047549720642489274\n",
      "Iteration: 5994 lambda_n: 0.9896748266473507 Loss: 0.047539312561811944\n",
      "Iteration: 5995 lambda_n: 0.8868791167445497 Loss: 0.04752855290598249\n",
      "Iteration: 5996 lambda_n: 0.990592657250501 Loss: 0.04751891435399852\n",
      "Iteration: 5997 lambda_n: 0.9597038651740325 Loss: 0.04750815258318216\n",
      "Iteration: 5998 lambda_n: 0.9877404009351493 Loss: 0.04749773034913494\n",
      "Iteration: 5999 lambda_n: 0.8825387035859176 Loss: 0.04748700771701399\n",
      "Iteration: 6000 lambda_n: 0.9235671572354306 Loss: 0.047477430623354286\n",
      "Iteration: 6001 lambda_n: 0.9248592081625109 Loss: 0.047467411836671435\n",
      "Iteration: 6002 lambda_n: 1.0186746523627892 Loss: 0.04745738266185316\n",
      "Iteration: 6003 lambda_n: 0.9440475815122499 Loss: 0.04744634035651713\n",
      "Iteration: 6004 lambda_n: 0.962890235172742 Loss: 0.04743611093666054\n",
      "Iteration: 6005 lambda_n: 1.0038253651562972 Loss: 0.04742568124787097\n",
      "Iteration: 6006 lambda_n: 0.9550287698292057 Loss: 0.04741481236503807\n",
      "Iteration: 6007 lambda_n: 0.9727719362188639 Loss: 0.04740447580907407\n",
      "Iteration: 6008 lambda_n: 1.0104494859620334 Loss: 0.04739395120946805\n",
      "Iteration: 6009 lambda_n: 0.9215254680468349 Loss: 0.04738302324190056\n",
      "Iteration: 6010 lambda_n: 0.8807817431530647 Loss: 0.04737306078204345\n",
      "Iteration: 6011 lambda_n: 0.9265260114752693 Loss: 0.04736354218482156\n",
      "Iteration: 6012 lambda_n: 0.9539480996984036 Loss: 0.04735353280700167\n",
      "Iteration: 6013 lambda_n: 1.0100223044161374 Loss: 0.04734323101945664\n",
      "Iteration: 6014 lambda_n: 0.9456004214044424 Loss: 0.04733232792365096\n",
      "Iteration: 6015 lambda_n: 1.0010777044126917 Loss: 0.04732212421415873\n",
      "Iteration: 6016 lambda_n: 0.9698595176719306 Loss: 0.047311326038894035\n",
      "Iteration: 6017 lambda_n: 0.9087290738581582 Loss: 0.04730086869722998\n",
      "Iteration: 6018 lambda_n: 0.926340588098599 Loss: 0.04729107414533062\n",
      "Iteration: 6019 lambda_n: 0.883527233780376 Loss: 0.04728109341953934\n",
      "Iteration: 6020 lambda_n: 0.9903995800071616 Loss: 0.04727157741420163\n",
      "Iteration: 6021 lambda_n: 0.9105822537708009 Loss: 0.047260914331552537\n",
      "Iteration: 6022 lambda_n: 0.9610097945944803 Loss: 0.04725111431852097\n",
      "Iteration: 6023 lambda_n: 0.9708211050421502 Loss: 0.04724077545602699\n",
      "Iteration: 6024 lambda_n: 0.9514030331497491 Loss: 0.047230335078448066\n",
      "Iteration: 6025 lambda_n: 0.9787608439509417 Loss: 0.0472201074663942\n",
      "Iteration: 6026 lambda_n: 0.9593085240403139 Loss: 0.04720958982993752\n",
      "Iteration: 6027 lambda_n: 0.9929005066809985 Loss: 0.047199285236500556\n",
      "Iteration: 6028 lambda_n: 0.9448305334584647 Loss: 0.04718862399288306\n",
      "Iteration: 6029 lambda_n: 1.0297790366082937 Loss: 0.04717848285429125\n",
      "Iteration: 6030 lambda_n: 0.9385050163024413 Loss: 0.04716743433442887\n",
      "Iteration: 6031 lambda_n: 0.9749333353851132 Loss: 0.0471573690910943\n",
      "Iteration: 6032 lambda_n: 0.9699832022238255 Loss: 0.047146917200574893\n",
      "Iteration: 6033 lambda_n: 0.9220677879010938 Loss: 0.047136522464716016\n",
      "Iteration: 6034 lambda_n: 0.9989975961833598 Loss: 0.047126644991196974\n",
      "Iteration: 6035 lambda_n: 0.9204412564168711 Loss: 0.04711594758417517\n",
      "Iteration: 6036 lambda_n: 0.9706060826706083 Loss: 0.04710609520369651\n",
      "Iteration: 6037 lambda_n: 0.9041806582696295 Loss: 0.04709570984613508\n",
      "Iteration: 6038 lambda_n: 0.9336659800772276 Loss: 0.04708603891549947\n",
      "Iteration: 6039 lambda_n: 1.0230219964623852 Loss: 0.047076056347095205\n",
      "Iteration: 6040 lambda_n: 1.0123474649765256 Loss: 0.047065122757653594\n",
      "Iteration: 6041 lambda_n: 1.004196937995218 Loss: 0.04705430773908368\n",
      "Iteration: 6042 lambda_n: 0.9711085057944618 Loss: 0.04704358420564269\n",
      "Iteration: 6043 lambda_n: 0.9690093730189362 Loss: 0.04703321819652789\n",
      "Iteration: 6044 lambda_n: 0.9939428088034665 Loss: 0.04702287869615695\n",
      "Iteration: 6045 lambda_n: 0.9494807849104356 Loss: 0.04701227741129517\n",
      "Iteration: 6046 lambda_n: 0.9500396150334116 Loss: 0.04700215438480099\n",
      "Iteration: 6047 lambda_n: 0.879334948720153 Loss: 0.046992029345414216\n",
      "Iteration: 6048 lambda_n: 0.9075594466929932 Loss: 0.0469826613593603\n",
      "Iteration: 6049 lambda_n: 0.9713849954045111 Loss: 0.04697299623366131\n",
      "Iteration: 6050 lambda_n: 0.8823887555306322 Loss: 0.046962655390390955\n",
      "Iteration: 6051 lambda_n: 1.0292305496365282 Loss: 0.04695326553824622\n",
      "Iteration: 6052 lambda_n: 0.939066779502895 Loss: 0.04694231739861326\n",
      "Iteration: 6053 lambda_n: 0.9180632231200011 Loss: 0.04693233240601102\n",
      "Iteration: 6054 lambda_n: 1.0022885933121242 Loss: 0.046922574485908725\n",
      "Iteration: 6055 lambda_n: 0.913674993077514 Loss: 0.046911925579724276\n",
      "Iteration: 6056 lambda_n: 0.8816377294487835 Loss: 0.046902222005974915\n",
      "Iteration: 6057 lambda_n: 1.0192796862447941 Loss: 0.04689286216264346\n",
      "Iteration: 6058 lambda_n: 1.003177738043135 Loss: 0.04688204531927449\n",
      "Iteration: 6059 lambda_n: 0.9150063112622745 Loss: 0.04687140382339428\n",
      "Iteration: 6060 lambda_n: 0.9721711912281256 Loss: 0.04686170150024012\n",
      "Iteration: 6061 lambda_n: 0.8846770980157109 Loss: 0.04685139707298111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6062 lambda_n: 0.9580467791461053 Loss: 0.04684202365621909\n",
      "Iteration: 6063 lambda_n: 0.976889200827735 Loss: 0.04683187676524013\n",
      "Iteration: 6064 lambda_n: 0.9329464984650491 Loss: 0.04682153448760825\n",
      "Iteration: 6065 lambda_n: 0.904362368459204 Loss: 0.04681166136980789\n",
      "Iteration: 6066 lambda_n: 0.9869887521383065 Loss: 0.04680209442722099\n",
      "Iteration: 6067 lambda_n: 1.0033070160468283 Loss: 0.04679165754200015\n",
      "Iteration: 6068 lambda_n: 0.8973423344082494 Loss: 0.046781052524679234\n",
      "Iteration: 6069 lambda_n: 0.9006978224709135 Loss: 0.046771571342420756\n",
      "Iteration: 6070 lambda_n: 1.029010422570112 Loss: 0.04676205829989562\n",
      "Iteration: 6071 lambda_n: 0.9972544142586192 Loss: 0.04675119444655975\n",
      "Iteration: 6072 lambda_n: 0.9673580806945726 Loss: 0.04674067034898967\n",
      "Iteration: 6073 lambda_n: 0.8976313238788916 Loss: 0.046730465974728956\n",
      "Iteration: 6074 lambda_n: 1.0101900214694846 Loss: 0.04672100085161322\n",
      "Iteration: 6075 lambda_n: 0.9354292782890593 Loss: 0.046710353137350995\n",
      "Iteration: 6076 lambda_n: 1.0185159134272423 Loss: 0.04670049747844651\n",
      "Iteration: 6077 lambda_n: 0.9407821085248088 Loss: 0.04668977085691953\n",
      "Iteration: 6078 lambda_n: 0.9230820327187745 Loss: 0.04667986700960445\n",
      "Iteration: 6079 lambda_n: 0.9251735276094022 Loss: 0.04667015333545124\n",
      "Iteration: 6080 lambda_n: 1.0163575876426343 Loss: 0.04666042147125427\n",
      "Iteration: 6081 lambda_n: 0.9548877360915373 Loss: 0.04664973485541062\n",
      "Iteration: 6082 lambda_n: 0.9695365721961643 Loss: 0.046639698781195026\n",
      "Iteration: 6083 lambda_n: 0.9929273183436426 Loss: 0.04662951291979582\n",
      "Iteration: 6084 lambda_n: 0.9086018360741058 Loss: 0.04661908568073219\n",
      "Iteration: 6085 lambda_n: 1.0133910267709847 Loss: 0.04660954785813421\n",
      "Iteration: 6086 lambda_n: 0.9733289380671829 Loss: 0.04659891440412756\n",
      "Iteration: 6087 lambda_n: 0.9099866797910499 Loss: 0.04658870565792938\n",
      "Iteration: 6088 lambda_n: 0.952162059238505 Loss: 0.046579165123731815\n",
      "Iteration: 6089 lambda_n: 1.0234718741842928 Loss: 0.046569186395311254\n",
      "Iteration: 6090 lambda_n: 0.9671860221353332 Loss: 0.04655846488029683\n",
      "Iteration: 6091 lambda_n: 0.9991570097073499 Loss: 0.0465483373262502\n",
      "Iteration: 6092 lambda_n: 1.0154008319553312 Loss: 0.046537879421643016\n",
      "Iteration: 6093 lambda_n: 0.9964826897019342 Loss: 0.0465272561051592\n",
      "Iteration: 6094 lambda_n: 0.999789103406841 Loss: 0.0465168352328889\n",
      "Iteration: 6095 lambda_n: 1.003543253254901 Loss: 0.04650638428517189\n",
      "Iteration: 6096 lambda_n: 1.028356594343384 Loss: 0.04649589863260263\n",
      "Iteration: 6097 lambda_n: 1.0276147147564563 Loss: 0.04648515843419166\n",
      "Iteration: 6098 lambda_n: 1.0064636289541538 Loss: 0.04647443075935464\n",
      "Iteration: 6099 lambda_n: 0.98901488626138 Loss: 0.04646392851949449\n",
      "Iteration: 6100 lambda_n: 0.9243190779469175 Loss: 0.046453612820538864\n",
      "Iteration: 6101 lambda_n: 0.9165063871270586 Loss: 0.0464439759222371\n",
      "Iteration: 6102 lambda_n: 0.9091484056934461 Loss: 0.04643442430215089\n",
      "Iteration: 6103 lambda_n: 1.011688657251282 Loss: 0.04642495312918899\n",
      "Iteration: 6104 lambda_n: 0.9173958251049148 Loss: 0.0464144181393193\n",
      "Iteration: 6105 lambda_n: 0.939945214359739 Loss: 0.0464048690648051\n",
      "Iteration: 6106 lambda_n: 0.9764459708563789 Loss: 0.04639508924272025\n",
      "Iteration: 6107 lambda_n: 0.9425474718000896 Loss: 0.04638493389657444\n",
      "Iteration: 6108 lambda_n: 0.9938181323990958 Loss: 0.046375135220399176\n",
      "Iteration: 6109 lambda_n: 1.0186695725651413 Loss: 0.04636480791788119\n",
      "Iteration: 6110 lambda_n: 0.9043539008831402 Loss: 0.046354227040639136\n",
      "Iteration: 6111 lambda_n: 0.8926684022884961 Loss: 0.046344837519656366\n",
      "Iteration: 6112 lambda_n: 0.9299665973533802 Loss: 0.046335572983197064\n",
      "Iteration: 6113 lambda_n: 0.9176207630963231 Loss: 0.0463259252169221\n",
      "Iteration: 6114 lambda_n: 1.0167655222207208 Loss: 0.046316409402066355\n",
      "Iteration: 6115 lambda_n: 1.0095729550994699 Loss: 0.046305869941229885\n",
      "Iteration: 6116 lambda_n: 0.9060246567349497 Loss: 0.04629540971428148\n",
      "Iteration: 6117 lambda_n: 1.001446788096745 Loss: 0.04628602632695758\n",
      "Iteration: 6118 lambda_n: 0.9807271102272359 Loss: 0.046275659059232324\n",
      "Iteration: 6119 lambda_n: 0.9643357508177538 Loss: 0.04626551074173223\n",
      "Iteration: 6120 lambda_n: 0.9273700900632778 Loss: 0.04625553633841079\n",
      "Iteration: 6121 lambda_n: 0.8944722273575769 Loss: 0.04624594830641046\n",
      "Iteration: 6122 lambda_n: 0.9272749190081783 Loss: 0.04623670414473717\n",
      "Iteration: 6123 lambda_n: 0.9660417223160916 Loss: 0.04622712485529279\n",
      "Iteration: 6124 lambda_n: 0.9108607819384843 Loss: 0.046217149286282656\n",
      "Iteration: 6125 lambda_n: 0.9524801272878682 Loss: 0.04620774746039337\n",
      "Iteration: 6126 lambda_n: 0.9748974651857024 Loss: 0.04619792012782314\n",
      "Iteration: 6127 lambda_n: 0.8996121487939993 Loss: 0.04618786582872566\n",
      "Iteration: 6128 lambda_n: 0.9989868076536483 Loss: 0.04617859184704624\n",
      "Iteration: 6129 lambda_n: 0.953323758518066 Loss: 0.04616829779871892\n",
      "Iteration: 6130 lambda_n: 1.0132270102262093 Loss: 0.04615847857913529\n",
      "Iteration: 6131 lambda_n: 0.9157870325606581 Loss: 0.046148046957568516\n",
      "Iteration: 6132 lambda_n: 0.8910053770840037 Loss: 0.04613862260519933\n",
      "Iteration: 6133 lambda_n: 0.9956549991895033 Loss: 0.04612945700223131\n",
      "Iteration: 6134 lambda_n: 1.0106425166816266 Loss: 0.04611921923388467\n",
      "Iteration: 6135 lambda_n: 0.9342026837204731 Loss: 0.04610883205075447\n",
      "Iteration: 6136 lambda_n: 0.905554580530541 Loss: 0.04609923470948677\n",
      "Iteration: 6137 lambda_n: 0.9656680468473756 Loss: 0.04608993554009979\n",
      "Iteration: 6138 lambda_n: 0.978857871685012 Loss: 0.046080023254385066\n",
      "Iteration: 6139 lambda_n: 0.9326217426992449 Loss: 0.046069979996453465\n",
      "Iteration: 6140 lambda_n: 0.901002520028874 Loss: 0.04606041526929976\n",
      "Iteration: 6141 lambda_n: 0.9169406278188039 Loss: 0.04605117865964533\n",
      "Iteration: 6142 lambda_n: 0.8784839261684679 Loss: 0.04604178253660985\n",
      "Iteration: 6143 lambda_n: 0.9480376956612813 Loss: 0.046032784158626334\n",
      "Iteration: 6144 lambda_n: 0.9368162320089245 Loss: 0.046023077367140106\n",
      "Iteration: 6145 lambda_n: 0.8984324302375142 Loss: 0.046013489583362946\n",
      "Iteration: 6146 lambda_n: 0.9981386505262145 Loss: 0.04600429847923228\n",
      "Iteration: 6147 lambda_n: 1.0227476523236425 Loss: 0.04599409178002255\n",
      "Iteration: 6148 lambda_n: 0.923565523422927 Loss: 0.045983638259401875\n",
      "Iteration: 6149 lambda_n: 1.0143732013189366 Loss: 0.045974202678866066\n",
      "Iteration: 6150 lambda_n: 0.9228303944359469 Loss: 0.04596384395816621\n",
      "Iteration: 6151 lambda_n: 1.020050790899485 Loss: 0.04595442424770358\n",
      "Iteration: 6152 lambda_n: 0.8850432212723329 Loss: 0.04594401680592699\n",
      "Iteration: 6153 lambda_n: 1.0271280264087395 Loss: 0.04593499077615855\n",
      "Iteration: 6154 lambda_n: 1.0070727739542427 Loss: 0.045924520309562566\n",
      "Iteration: 6155 lambda_n: 0.918978522820569 Loss: 0.04591425908757267\n",
      "Iteration: 6156 lambda_n: 0.9817602706784964 Loss: 0.045904899624132356\n",
      "Iteration: 6157 lambda_n: 1.0147050749160251 Loss: 0.04589490513199563\n",
      "Iteration: 6158 lambda_n: 1.0011190783713908 Loss: 0.04588458001474872\n",
      "Iteration: 6159 lambda_n: 0.9270918161968259 Loss: 0.045874397885385214\n",
      "Iteration: 6160 lambda_n: 0.9182361290952756 Loss: 0.04586497287342898\n",
      "Iteration: 6161 lambda_n: 0.969046072731729 Loss: 0.04585564187842903\n",
      "Iteration: 6162 lambda_n: 0.9373663378439988 Loss: 0.04584579886668655\n",
      "Iteration: 6163 lambda_n: 0.8811100914568907 Loss: 0.04583628185074454\n",
      "Iteration: 6164 lambda_n: 0.9622189885460379 Loss: 0.04582733977891317\n",
      "Iteration: 6165 lambda_n: 0.9581397914977768 Loss: 0.04581757874693085\n",
      "Iteration: 6166 lambda_n: 0.8806408903246726 Loss: 0.04580786343972928\n",
      "Iteration: 6167 lambda_n: 0.9893987223671592 Loss: 0.045798937778352\n",
      "Iteration: 6168 lambda_n: 1.0150116880719402 Loss: 0.045788914185016294\n",
      "Iteration: 6169 lambda_n: 0.9004841533493537 Loss: 0.04577863591945389\n",
      "Iteration: 6170 lambda_n: 0.887038754051702 Loss: 0.04576952147154288\n",
      "Iteration: 6171 lambda_n: 0.9985100961792942 Loss: 0.04576054686968311\n",
      "Iteration: 6172 lambda_n: 1.0016280998606584 Loss: 0.04575044892014246\n",
      "Iteration: 6173 lambda_n: 0.9547471552575433 Loss: 0.04574032418919477\n",
      "Iteration: 6174 lambda_n: 0.8840061847400872 Loss: 0.04573067777613328\n",
      "Iteration: 6175 lambda_n: 0.8984235237538337 Loss: 0.045721749963479906\n",
      "Iteration: 6176 lambda_n: 1.0254159339460645 Loss: 0.04571268035114038\n",
      "Iteration: 6177 lambda_n: 0.9877639088768877 Loss: 0.045702333436725424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6178 lambda_n: 0.9575427401135796 Loss: 0.04569237117916268\n",
      "Iteration: 6179 lambda_n: 1.0108832100388243 Loss: 0.0456827181565502\n",
      "Iteration: 6180 lambda_n: 0.9203050283517371 Loss: 0.04567253214682607\n",
      "Iteration: 6181 lambda_n: 0.9258426086442134 Loss: 0.04566326307069207\n",
      "Iteration: 6182 lambda_n: 0.8920445938477606 Loss: 0.04565394229829272\n",
      "Iteration: 6183 lambda_n: 0.9819106426372445 Loss: 0.045644965652218795\n",
      "Iteration: 6184 lambda_n: 0.8840755398031127 Loss: 0.045635089078187904\n",
      "Iteration: 6185 lambda_n: 0.9305051755879157 Loss: 0.04562620052309166\n",
      "Iteration: 6186 lambda_n: 0.9328555123630884 Loss: 0.04561684919850881\n",
      "Iteration: 6187 lambda_n: 0.9825191817462071 Loss: 0.04560747841241117\n",
      "Iteration: 6188 lambda_n: 0.8894337609568563 Loss: 0.04559761324655401\n",
      "Iteration: 6189 lambda_n: 0.8864502897516335 Loss: 0.04558868671053269\n",
      "Iteration: 6190 lambda_n: 0.9102432797311807 Loss: 0.04557979389057595\n",
      "Iteration: 6191 lambda_n: 0.8824203174706893 Loss: 0.045570666303027745\n",
      "Iteration: 6192 lambda_n: 1.0233631217915182 Loss: 0.04556182151018134\n",
      "Iteration: 6193 lambda_n: 0.9453818183951027 Loss: 0.04555156868389322\n",
      "Iteration: 6194 lambda_n: 0.9641098270974752 Loss: 0.045542101605448554\n",
      "Iteration: 6195 lambda_n: 1.0014287346138808 Loss: 0.04553245141055365\n",
      "Iteration: 6196 lambda_n: 1.0206977822393113 Loss: 0.04552243241012114\n",
      "Iteration: 6197 lambda_n: 1.0255557198567087 Loss: 0.04551222559753703\n",
      "Iteration: 6198 lambda_n: 0.9394186263852926 Loss: 0.045501975261995675\n",
      "Iteration: 6199 lambda_n: 0.8900324869560582 Loss: 0.04549259030880938\n",
      "Iteration: 6200 lambda_n: 0.9231678873692191 Loss: 0.045483702659371364\n",
      "Iteration: 6201 lambda_n: 0.8979151571607779 Loss: 0.04547448816806639\n",
      "Iteration: 6202 lambda_n: 0.9453548441181647 Loss: 0.04546552968307218\n",
      "Iteration: 6203 lambda_n: 0.9217292891870525 Loss: 0.04545610210412509\n",
      "Iteration: 6204 lambda_n: 0.969556379622677 Loss: 0.045446914293096885\n",
      "Iteration: 6205 lambda_n: 0.979912920056503 Loss: 0.04543725417775101\n",
      "Iteration: 6206 lambda_n: 0.9557700144235688 Loss: 0.04542749550074159\n",
      "Iteration: 6207 lambda_n: 1.0168677916442928 Loss: 0.04541798173823903\n",
      "Iteration: 6208 lambda_n: 1.0204489747367949 Loss: 0.04540786466951043\n",
      "Iteration: 6209 lambda_n: 0.9578271868756094 Loss: 0.045397717013984075\n",
      "Iteration: 6210 lambda_n: 1.006431859926788 Loss: 0.04538819668823197\n",
      "Iteration: 6211 lambda_n: 0.9535324732466681 Loss: 0.04537819805816025\n",
      "Iteration: 6212 lambda_n: 0.9409984917907014 Loss: 0.045368729511802215\n",
      "Iteration: 6213 lambda_n: 0.9674865417741718 Loss: 0.04535938976339078\n",
      "Iteration: 6214 lambda_n: 0.8918168694309497 Loss: 0.04534979160519134\n",
      "Iteration: 6215 lambda_n: 1.0152796177703614 Loss: 0.04534094818215742\n",
      "Iteration: 6216 lambda_n: 0.9891854311671838 Loss: 0.04533088519746387\n",
      "Iteration: 6217 lambda_n: 0.8942253593628563 Loss: 0.04532108568092366\n",
      "Iteration: 6218 lambda_n: 0.9399269363805839 Loss: 0.04531223100997483\n",
      "Iteration: 6219 lambda_n: 0.9295828372906835 Loss: 0.045302928008173406\n",
      "Iteration: 6220 lambda_n: 0.8858927785365415 Loss: 0.045293731633207636\n",
      "Iteration: 6221 lambda_n: 1.0007420524151502 Loss: 0.04528497141576111\n",
      "Iteration: 6222 lambda_n: 0.9301325846373575 Loss: 0.04527508012039488\n",
      "Iteration: 6223 lambda_n: 0.9776737873718175 Loss: 0.04526589112156308\n",
      "Iteration: 6224 lambda_n: 0.9619142739223377 Loss: 0.045256237019680526\n",
      "Iteration: 6225 lambda_n: 1.0078178110959004 Loss: 0.04524674310791395\n",
      "Iteration: 6226 lambda_n: 0.8921817080723664 Loss: 0.04523680100448489\n",
      "Iteration: 6227 lambda_n: 0.9484236117852268 Loss: 0.04522800380811838\n",
      "Iteration: 6228 lambda_n: 0.9757877930178519 Loss: 0.04521865633439935\n",
      "Iteration: 6229 lambda_n: 1.015398871010811 Loss: 0.04520904377733033\n",
      "Iteration: 6230 lambda_n: 0.9697219766988897 Loss: 0.04519904597868939\n",
      "Iteration: 6231 lambda_n: 0.9093425539073925 Loss: 0.04518950265800378\n",
      "Iteration: 6232 lambda_n: 1.0128534406182268 Loss: 0.045180557754776196\n",
      "Iteration: 6233 lambda_n: 1.0121965524672134 Loss: 0.0451705994440476\n",
      "Iteration: 6234 lambda_n: 0.9703967961181781 Loss: 0.04516065264334316\n",
      "Iteration: 6235 lambda_n: 0.9061429501059964 Loss: 0.04515112135108482\n",
      "Iteration: 6236 lambda_n: 1.0118801979661523 Loss: 0.045142225359749835\n",
      "Iteration: 6237 lambda_n: 0.9467138820990199 Loss: 0.0451322960930588\n",
      "Iteration: 6238 lambda_n: 0.8812155690326571 Loss: 0.045123010864551\n",
      "Iteration: 6239 lambda_n: 1.024183401313599 Loss: 0.04511437201607341\n",
      "Iteration: 6240 lambda_n: 0.9319299579087024 Loss: 0.04510433643412634\n",
      "Iteration: 6241 lambda_n: 0.9731605618593512 Loss: 0.045095209320905015\n",
      "Iteration: 6242 lambda_n: 0.9015663611793658 Loss: 0.04508568299652418\n",
      "Iteration: 6243 lambda_n: 1.0148510491599756 Loss: 0.04507686170092707\n",
      "Iteration: 6244 lambda_n: 0.9863254032764813 Loss: 0.04506693680474156\n",
      "Iteration: 6245 lambda_n: 0.9380685618275705 Loss: 0.045057295777801945\n",
      "Iteration: 6246 lambda_n: 0.963862377869091 Loss: 0.045048130929286806\n",
      "Iteration: 6247 lambda_n: 0.9944301567136199 Loss: 0.04503871863245367\n",
      "Iteration: 6248 lambda_n: 0.8898394256492124 Loss: 0.04502901267733375\n",
      "Iteration: 6249 lambda_n: 0.9740523274215995 Loss: 0.04502033173244619\n",
      "Iteration: 6250 lambda_n: 0.9680036744288133 Loss: 0.04501083375716372\n",
      "Iteration: 6251 lambda_n: 1.0063891213770553 Loss: 0.04500139944499969\n",
      "Iteration: 6252 lambda_n: 0.9386521134563607 Loss: 0.044991595975078014\n",
      "Iteration: 6253 lambda_n: 0.9053260193769581 Loss: 0.044982456900620485\n",
      "Iteration: 6254 lambda_n: 0.9998144732705153 Loss: 0.0449736464679743\n",
      "Iteration: 6255 lambda_n: 1.006813371730296 Loss: 0.044963921251239006\n",
      "Iteration: 6256 lambda_n: 1.0227375769600324 Loss: 0.04495413300437569\n",
      "Iteration: 6257 lambda_n: 0.9269095245539374 Loss: 0.044944195132426125\n",
      "Iteration: 6258 lambda_n: 0.8987856974636291 Loss: 0.044935192936748206\n",
      "Iteration: 6259 lambda_n: 0.8874240241661114 Loss: 0.04492646798937418\n",
      "Iteration: 6260 lambda_n: 0.9706674707091708 Loss: 0.04491785730610382\n",
      "Iteration: 6261 lambda_n: 0.8998373791109102 Loss: 0.04490844343174282\n",
      "Iteration: 6262 lambda_n: 0.8855810844923605 Loss: 0.044899720713966425\n",
      "Iteration: 6263 lambda_n: 0.9264528504634831 Loss: 0.04489114015982787\n",
      "Iteration: 6264 lambda_n: 0.9520072232985322 Loss: 0.044882167807263826\n",
      "Iteration: 6265 lambda_n: 0.9524710009132538 Loss: 0.04487295246328324\n",
      "Iteration: 6266 lambda_n: 1.0111827296732327 Loss: 0.04486373719044463\n",
      "Iteration: 6267 lambda_n: 0.8952142023624828 Loss: 0.04485395886965947\n",
      "Iteration: 6268 lambda_n: 0.9061821991441623 Loss: 0.04484530628162242\n",
      "Iteration: 6269 lambda_n: 0.904333063726512 Loss: 0.04483655179533148\n",
      "Iteration: 6270 lambda_n: 0.9558128053445842 Loss: 0.044827819299685025\n",
      "Iteration: 6271 lambda_n: 0.9617870514171829 Loss: 0.04481859418467607\n",
      "Iteration: 6272 lambda_n: 0.9155367078893598 Loss: 0.04480931606246943\n",
      "Iteration: 6273 lambda_n: 0.8811056710803232 Loss: 0.04480048844551714\n",
      "Iteration: 6274 lambda_n: 0.8907746013557643 Loss: 0.04479199681237896\n",
      "Iteration: 6275 lambda_n: 0.9363015582712496 Loss: 0.044783415984960405\n",
      "Iteration: 6276 lambda_n: 0.9090508306336713 Loss: 0.04477440092345808\n",
      "Iteration: 6277 lambda_n: 0.9682240398863029 Loss: 0.04476565248762241\n",
      "Iteration: 6278 lambda_n: 0.9103237048809705 Loss: 0.044756339189198316\n",
      "Iteration: 6279 lambda_n: 0.9306920876755391 Loss: 0.04474758716432423\n",
      "Iteration: 6280 lambda_n: 1.021845618404913 Loss: 0.044738643658164545\n",
      "Iteration: 6281 lambda_n: 0.8858240777750082 Loss: 0.04472882927188847\n",
      "Iteration: 6282 lambda_n: 0.9632963407366516 Loss: 0.0447203256035779\n",
      "Iteration: 6283 lambda_n: 0.9372202421293766 Loss: 0.04471108274733755\n",
      "Iteration: 6284 lambda_n: 1.0110785654101602 Loss: 0.04470209461917119\n",
      "Iteration: 6285 lambda_n: 0.9895490579100716 Loss: 0.04469240318487713\n",
      "Iteration: 6286 lambda_n: 1.0151928171525628 Loss: 0.04468292315421462\n",
      "Iteration: 6287 lambda_n: 1.0075059457193567 Loss: 0.04467320263504773\n",
      "Iteration: 6288 lambda_n: 1.01660224392503 Loss: 0.044663560910849026\n",
      "Iteration: 6289 lambda_n: 0.894849242587863 Loss: 0.04465383738282768\n",
      "Iteration: 6290 lambda_n: 0.9912768156230849 Loss: 0.04464528275372877\n",
      "Iteration: 6291 lambda_n: 0.9229115925297291 Loss: 0.04463581106429422\n",
      "Iteration: 6292 lambda_n: 1.017472601130613 Loss: 0.04462699712022703\n",
      "Iteration: 6293 lambda_n: 1.0011939681092918 Loss: 0.04461728515095302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6294 lambda_n: 0.9729041912130777 Loss: 0.044607733733794355\n",
      "Iteration: 6295 lambda_n: 0.9203416622209435 Loss: 0.044598457117516366\n",
      "Iteration: 6296 lambda_n: 0.9021475843818307 Loss: 0.04458968614599432\n",
      "Iteration: 6297 lambda_n: 0.8879411733231155 Loss: 0.04458109277955535\n",
      "Iteration: 6298 lambda_n: 0.9267665301383031 Loss: 0.044572638811187816\n",
      "Iteration: 6299 lambda_n: 0.9837415735405323 Loss: 0.04456381950658477\n",
      "Iteration: 6300 lambda_n: 1.0108201119067988 Loss: 0.04455446284022398\n",
      "Iteration: 6301 lambda_n: 0.9568469344521054 Loss: 0.04454485380047814\n",
      "Iteration: 6302 lambda_n: 0.9093115068631894 Loss: 0.04453576267885973\n",
      "Iteration: 6303 lambda_n: 0.9962861634297111 Loss: 0.04452712756156688\n",
      "Iteration: 6304 lambda_n: 0.9683317394648882 Loss: 0.04451767139051598\n",
      "Iteration: 6305 lambda_n: 0.8774654425760415 Loss: 0.04450848544548462\n",
      "Iteration: 6306 lambda_n: 0.9771919919595328 Loss: 0.044500165664190676\n",
      "Iteration: 6307 lambda_n: 0.9258951815942804 Loss: 0.04449090498769892\n",
      "Iteration: 6308 lambda_n: 0.9262118531059989 Loss: 0.0444821349876943\n",
      "Iteration: 6309 lambda_n: 0.9966064485284505 Loss: 0.04447336641627361\n",
      "Iteration: 6310 lambda_n: 0.9137507463810614 Loss: 0.044463936359346354\n",
      "Iteration: 6311 lambda_n: 0.8969228130185692 Loss: 0.04445529480830386\n",
      "Iteration: 6312 lambda_n: 0.9560243863910102 Loss: 0.044446816602636406\n",
      "Iteration: 6313 lambda_n: 0.9660618044084789 Loss: 0.04443778432022364\n",
      "Iteration: 6314 lambda_n: 0.9807654141956135 Loss: 0.04442866201430598\n",
      "Iteration: 6315 lambda_n: 0.9257747990809607 Loss: 0.04441940581208947\n",
      "Iteration: 6316 lambda_n: 0.9184464017233152 Loss: 0.04441067317205742\n",
      "Iteration: 6317 lambda_n: 0.9732240628818989 Loss: 0.04440201405275731\n",
      "Iteration: 6318 lambda_n: 0.9256925750047733 Loss: 0.044392843267453\n",
      "Iteration: 6319 lambda_n: 0.9724816000057048 Loss: 0.044384124940966996\n",
      "Iteration: 6320 lambda_n: 0.9900181634330477 Loss: 0.04437497074443679\n",
      "Iteration: 6321 lambda_n: 1.0264599729413089 Loss: 0.044365656523999375\n",
      "Iteration: 6322 lambda_n: 0.9395727748255671 Loss: 0.044356004839561806\n",
      "Iteration: 6323 lambda_n: 0.9571535292006065 Loss: 0.04434717495457112\n",
      "Iteration: 6324 lambda_n: 0.9090826701971643 Loss: 0.04433818457916161\n",
      "Iteration: 6325 lambda_n: 0.9489622755641544 Loss: 0.04432965014721495\n",
      "Iteration: 6326 lambda_n: 1.0284074231211568 Loss: 0.0443207459259163\n",
      "Iteration: 6327 lambda_n: 1.0209754486026272 Loss: 0.04431110156885415\n",
      "Iteration: 6328 lambda_n: 0.8861517887401799 Loss: 0.04430153237263656\n",
      "Iteration: 6329 lambda_n: 0.9528756427135333 Loss: 0.04429323124101031\n",
      "Iteration: 6330 lambda_n: 1.0231851828799967 Loss: 0.04428430964766136\n",
      "Iteration: 6331 lambda_n: 1.0246700085111413 Loss: 0.04427473504923377\n",
      "Iteration: 6332 lambda_n: 0.999385106867721 Loss: 0.044265152049755875\n",
      "Iteration: 6333 lambda_n: 0.9325609286007165 Loss: 0.04425581082074085\n",
      "Iteration: 6334 lambda_n: 0.9238515476814232 Loss: 0.044247098918383486\n",
      "Iteration: 6335 lambda_n: 1.0252880015424892 Loss: 0.044238472876640754\n",
      "Iteration: 6336 lambda_n: 0.9577358415183466 Loss: 0.04422890496316857\n",
      "Iteration: 6337 lambda_n: 0.9664157220687055 Loss: 0.044219972429066\n",
      "Iteration: 6338 lambda_n: 0.9865899350043632 Loss: 0.04421096382648978\n",
      "Iteration: 6339 lambda_n: 0.9933227660397275 Loss: 0.04420177223295527\n",
      "Iteration: 6340 lambda_n: 0.9409200073534523 Loss: 0.0441925230871598\n",
      "Iteration: 6341 lambda_n: 0.9519070371977637 Loss: 0.04418376667148598\n",
      "Iteration: 6342 lambda_n: 0.8995492878769664 Loss: 0.044174912754355714\n",
      "Iteration: 6343 lambda_n: 0.9804236962904426 Loss: 0.04416655021913468\n",
      "Iteration: 6344 lambda_n: 0.9316401937413531 Loss: 0.04415744070730801\n",
      "Iteration: 6345 lambda_n: 0.9696063027205498 Loss: 0.04414878916355531\n",
      "Iteration: 6346 lambda_n: 0.998127735484703 Loss: 0.04413978992068753\n",
      "Iteration: 6347 lambda_n: 1.0170797758672563 Loss: 0.04413053115013016\n",
      "Iteration: 6348 lambda_n: 1.0114284926982227 Loss: 0.04412110199669611\n",
      "Iteration: 6349 lambda_n: 0.9213948618140593 Loss: 0.0441117306630252\n",
      "Iteration: 6350 lambda_n: 1.0150657744686977 Loss: 0.0441031982452098\n",
      "Iteration: 6351 lambda_n: 0.9875792176031338 Loss: 0.04409380361070772\n",
      "Iteration: 6352 lambda_n: 0.9693031930376388 Loss: 0.04408466861170871\n",
      "Iteration: 6353 lambda_n: 0.9124497347599422 Loss: 0.044075707694483615\n",
      "Iteration: 6354 lambda_n: 1.0054865309125862 Loss: 0.04406727692686964\n",
      "Iteration: 6355 lambda_n: 0.9979435955470431 Loss: 0.044057991647278964\n",
      "Iteration: 6356 lambda_n: 0.9769068129681908 Loss: 0.04404878133587771\n",
      "Iteration: 6357 lambda_n: 1.0006901628403595 Loss: 0.04403977030820828\n",
      "Iteration: 6358 lambda_n: 0.9023053865731823 Loss: 0.04403054516659398\n",
      "Iteration: 6359 lambda_n: 1.0207425383537727 Loss: 0.04402223158322764\n",
      "Iteration: 6360 lambda_n: 0.9451872922454717 Loss: 0.04401283198225014\n",
      "Iteration: 6361 lambda_n: 0.9973999284695269 Loss: 0.04400413309098234\n",
      "Iteration: 6362 lambda_n: 0.9261080105278948 Loss: 0.04399495883550505\n",
      "Iteration: 6363 lambda_n: 0.9478109848833995 Loss: 0.04398644508877671\n",
      "Iteration: 6364 lambda_n: 0.9300332434040578 Loss: 0.04397773656807305\n",
      "Iteration: 6365 lambda_n: 1.0283472573999037 Loss: 0.04396919605573486\n",
      "Iteration: 6366 lambda_n: 0.888547332701459 Loss: 0.0439597581076255\n",
      "Iteration: 6367 lambda_n: 0.9142781355358083 Loss: 0.04395160776882603\n",
      "Iteration: 6368 lambda_n: 0.979960875136801 Loss: 0.0439432258217395\n",
      "Iteration: 6369 lambda_n: 1.004550894691474 Loss: 0.043934246677517705\n",
      "Iteration: 6370 lambda_n: 0.9755173194686343 Loss: 0.04392504756283059\n",
      "Iteration: 6371 lambda_n: 1.0295973941194274 Loss: 0.04391611950050412\n",
      "Iteration: 6372 lambda_n: 1.018013294819865 Loss: 0.04390670202913456\n",
      "Iteration: 6373 lambda_n: 0.9197762131989519 Loss: 0.04389739611081936\n",
      "Iteration: 6374 lambda_n: 1.026632039822264 Loss: 0.04388899299058587\n",
      "Iteration: 6375 lambda_n: 0.9866538260024642 Loss: 0.04387961900161152\n",
      "Iteration: 6376 lambda_n: 0.9708108981399005 Loss: 0.04387061538915038\n",
      "Iteration: 6377 lambda_n: 0.9288783441667264 Loss: 0.043861761464183184\n",
      "Iteration: 6378 lambda_n: 0.8867711017360593 Loss: 0.043853294721642025\n",
      "Iteration: 6379 lambda_n: 0.9210964554376186 Loss: 0.0438452161257212\n",
      "Iteration: 6380 lambda_n: 0.88420009269436 Loss: 0.04383682931106681\n",
      "Iteration: 6381 lambda_n: 0.9831374709579029 Loss: 0.04382878275306103\n",
      "Iteration: 6382 lambda_n: 0.978209163916271 Loss: 0.04381984078242186\n",
      "Iteration: 6383 lambda_n: 0.8891713066615557 Loss: 0.043810948818102866\n",
      "Iteration: 6384 lambda_n: 0.9174494110954547 Loss: 0.04380287069887815\n",
      "Iteration: 6385 lambda_n: 0.9565289485859345 Loss: 0.04379454015485933\n",
      "Iteration: 6386 lambda_n: 0.9739997823497891 Loss: 0.043785859613862575\n",
      "Iteration: 6387 lambda_n: 0.9330416984152978 Loss: 0.04377702561454528\n",
      "Iteration: 6388 lambda_n: 0.9349117762193175 Loss: 0.043768567917271906\n",
      "Iteration: 6389 lambda_n: 1.008434141933573 Loss: 0.04376009800160678\n",
      "Iteration: 6390 lambda_n: 0.9620306663662115 Loss: 0.04375096731826299\n",
      "Iteration: 6391 lambda_n: 0.9622931974625768 Loss: 0.04374226193071984\n",
      "Iteration: 6392 lambda_n: 0.8962722684519022 Loss: 0.043733559195503195\n",
      "Iteration: 6393 lambda_n: 0.9277051330456714 Loss: 0.043725458062425365\n",
      "Iteration: 6394 lambda_n: 0.9450258869977592 Loss: 0.04371707741705413\n",
      "Iteration: 6395 lambda_n: 0.8988806615816709 Loss: 0.04370854511425193\n",
      "Iteration: 6396 lambda_n: 0.9954166612308813 Loss: 0.04370043395108867\n",
      "Iteration: 6397 lambda_n: 0.9308317363536021 Loss: 0.04369145681792857\n",
      "Iteration: 6398 lambda_n: 0.9655135724636569 Loss: 0.043683067027314434\n",
      "Iteration: 6399 lambda_n: 0.9864015475368051 Loss: 0.043674369633529796\n",
      "Iteration: 6400 lambda_n: 0.9689932184090149 Loss: 0.04366548933223589\n",
      "Iteration: 6401 lambda_n: 1.0086484665095792 Loss: 0.04365677092536062\n",
      "Iteration: 6402 lambda_n: 0.9730061698549756 Loss: 0.043647701173161295\n",
      "Iteration: 6403 lambda_n: 0.9176628385076327 Loss: 0.04363895718561414\n",
      "Iteration: 6404 lambda_n: 1.0051451695366973 Loss: 0.04363071528933083\n",
      "Iteration: 6405 lambda_n: 0.9302505186300609 Loss: 0.043621692967697766\n",
      "Iteration: 6406 lambda_n: 0.9213572866575165 Loss: 0.043613347840070246\n",
      "Iteration: 6407 lambda_n: 0.8897659745089389 Loss: 0.043605087165762335\n",
      "Iteration: 6408 lambda_n: 0.8779337136724222 Loss: 0.043597114148924566\n",
      "Iteration: 6409 lambda_n: 0.9883935700168628 Loss: 0.04358925141490355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6410 lambda_n: 0.886426909321273 Loss: 0.04358040446950083\n",
      "Iteration: 6411 lambda_n: 0.9859514961187319 Loss: 0.04357247477366296\n",
      "Iteration: 6412 lambda_n: 0.9715721314385611 Loss: 0.04356365983398693\n",
      "Iteration: 6413 lambda_n: 0.9199723325414306 Loss: 0.04355497868173634\n",
      "Iteration: 6414 lambda_n: 0.9906134345356379 Loss: 0.04354676336838018\n",
      "Iteration: 6415 lambda_n: 0.9606229094440479 Loss: 0.043537922441976885\n",
      "Iteration: 6416 lambda_n: 0.8926678851656658 Loss: 0.04352935433375448\n",
      "Iteration: 6417 lambda_n: 0.9402607018308107 Loss: 0.043521396897028115\n",
      "Iteration: 6418 lambda_n: 0.9819788406578446 Loss: 0.04351301995907192\n",
      "Iteration: 6419 lambda_n: 1.0057199292066255 Loss: 0.04350427655449971\n",
      "Iteration: 6420 lambda_n: 0.8976323066002104 Loss: 0.04349532728054175\n",
      "Iteration: 6421 lambda_n: 0.9259540448169259 Loss: 0.04348734452965587\n",
      "Iteration: 6422 lambda_n: 1.0060885734941751 Loss: 0.04347911457656518\n",
      "Iteration: 6423 lambda_n: 0.907317588241638 Loss: 0.043470177756314475\n",
      "Iteration: 6424 lambda_n: 0.9015309290529576 Loss: 0.043462123096022875\n",
      "Iteration: 6425 lambda_n: 0.9378475972640193 Loss: 0.043454124320912745\n",
      "Iteration: 6426 lambda_n: 0.9003166960861566 Loss: 0.04344580810676104\n",
      "Iteration: 6427 lambda_n: 0.8870205786520923 Loss: 0.043437829278486925\n",
      "Iteration: 6428 lambda_n: 0.9512022616729168 Loss: 0.04342997268004046\n",
      "Iteration: 6429 lambda_n: 0.9660809175252755 Loss: 0.04342155245722866\n",
      "Iteration: 6430 lambda_n: 0.950962808723804 Loss: 0.04341300566790194\n",
      "Iteration: 6431 lambda_n: 1.0249069065762637 Loss: 0.04340459769051355\n",
      "Iteration: 6432 lambda_n: 1.0248839967907757 Loss: 0.04339554156196943\n",
      "Iteration: 6433 lambda_n: 0.9452803529756886 Loss: 0.043386491478567574\n",
      "Iteration: 6434 lambda_n: 0.964425320605275 Loss: 0.04337814950586266\n",
      "Iteration: 6435 lambda_n: 0.8970654068357964 Loss: 0.04336964370989678\n",
      "Iteration: 6436 lambda_n: 0.9174294644491551 Loss: 0.043361736650512316\n",
      "Iteration: 6437 lambda_n: 0.9828432048542601 Loss: 0.04335365473562246\n",
      "Iteration: 6438 lambda_n: 0.879701268287395 Loss: 0.04334500178077763\n",
      "Iteration: 6439 lambda_n: 0.9517975102086771 Loss: 0.04333726146075852\n",
      "Iteration: 6440 lambda_n: 0.9926995497041226 Loss: 0.04332889164792808\n",
      "Iteration: 6441 lambda_n: 0.9499993360874667 Loss: 0.043320167548734856\n",
      "Iteration: 6442 lambda_n: 0.9722086299956354 Loss: 0.04331182386980276\n",
      "Iteration: 6443 lambda_n: 0.9420048725133444 Loss: 0.04330329035833299\n",
      "Iteration: 6444 lambda_n: 0.9842777664618522 Loss: 0.04329502700608981\n",
      "Iteration: 6445 lambda_n: 1.010975531200643 Loss: 0.04328639814236064\n",
      "Iteration: 6446 lambda_n: 0.9249463718215922 Loss: 0.043277540880329014\n",
      "Iteration: 6447 lambda_n: 0.9422441584243634 Loss: 0.04326944234976928\n",
      "Iteration: 6448 lambda_n: 0.9635660894955235 Loss: 0.04326119730162857\n",
      "Iteration: 6449 lambda_n: 0.9740320717685351 Loss: 0.04325277083257976\n",
      "Iteration: 6450 lambda_n: 0.9471502775035704 Loss: 0.04324425813893356\n",
      "Iteration: 6451 lambda_n: 1.0061115423321516 Loss: 0.043235985496940854\n",
      "Iteration: 6452 lambda_n: 0.9559940313202872 Loss: 0.043227203399563416\n",
      "Iteration: 6453 lambda_n: 0.8931778799955448 Loss: 0.043218864043774265\n",
      "Iteration: 6454 lambda_n: 0.8815513883579382 Loss: 0.043211077298046296\n",
      "Iteration: 6455 lambda_n: 1.0139773918529797 Loss: 0.043203396319806936\n",
      "Iteration: 6456 lambda_n: 1.026607300452293 Loss: 0.04319456692832117\n",
      "Iteration: 6457 lambda_n: 0.9027708091284671 Loss: 0.043185633468030254\n",
      "Iteration: 6458 lambda_n: 0.901289295366396 Loss: 0.04317778253913848\n",
      "Iteration: 6459 lambda_n: 0.8823644905725526 Loss: 0.04316994908549036\n",
      "Iteration: 6460 lambda_n: 0.9154578164549921 Loss: 0.04316228456109186\n",
      "Iteration: 6461 lambda_n: 0.9119644506603403 Loss: 0.04315433722897855\n",
      "Iteration: 6462 lambda_n: 0.9123957007305674 Loss: 0.043146424937242016\n",
      "Iteration: 6463 lambda_n: 0.8890575541498164 Loss: 0.04313851361441482\n",
      "Iteration: 6464 lambda_n: 0.9580352605412076 Loss: 0.04313080918995487\n",
      "Iteration: 6465 lambda_n: 0.9886071261478895 Loss: 0.04312251202947127\n",
      "Iteration: 6466 lambda_n: 0.9979371741282091 Loss: 0.043113955553466574\n",
      "Iteration: 6467 lambda_n: 0.923735997867174 Loss: 0.04310532394801541\n",
      "Iteration: 6468 lambda_n: 0.9274849017727423 Loss: 0.04309733917955266\n",
      "Iteration: 6469 lambda_n: 0.9131070711407697 Loss: 0.04308932688117753\n",
      "Iteration: 6470 lambda_n: 0.9765185442666813 Loss: 0.04308144356420709\n",
      "Iteration: 6471 lambda_n: 1.008145392410598 Loss: 0.04307301802957024\n",
      "Iteration: 6472 lambda_n: 1.0066880715342432 Loss: 0.043064325305552494\n",
      "Iteration: 6473 lambda_n: 0.9064042587356055 Loss: 0.04305565092025896\n",
      "Iteration: 6474 lambda_n: 0.9538872187719953 Loss: 0.04304784559441208\n",
      "Iteration: 6475 lambda_n: 0.9906109158755411 Loss: 0.04303963643468534\n",
      "Iteration: 6476 lambda_n: 0.923204342492825 Loss: 0.043031116722579775\n",
      "Iteration: 6477 lambda_n: 1.0014695880455369 Loss: 0.04302318177861922\n",
      "Iteration: 6478 lambda_n: 0.9565152130960246 Loss: 0.0430145796466316\n",
      "Iteration: 6479 lambda_n: 0.8805732714065984 Loss: 0.043006368999696794\n",
      "Iteration: 6480 lambda_n: 0.8836393633598928 Loss: 0.04299881485500242\n",
      "Iteration: 6481 lambda_n: 1.0140193538320805 Loss: 0.04299123886476477\n",
      "Iteration: 6482 lambda_n: 0.9746052325559529 Loss: 0.042982550550824644\n",
      "Iteration: 6483 lambda_n: 0.9666553764210669 Loss: 0.04297420549259175\n",
      "Iteration: 6484 lambda_n: 1.0037881522566754 Loss: 0.04296593387977178\n",
      "Iteration: 6485 lambda_n: 0.9441623623855976 Loss: 0.042957350192344736\n",
      "Iteration: 6486 lambda_n: 0.9047950431169441 Loss: 0.04294928162706998\n",
      "Iteration: 6487 lambda_n: 1.0204521622411031 Loss: 0.042941554296074685\n",
      "Iteration: 6488 lambda_n: 0.8948578370616544 Loss: 0.04293284485256874\n",
      "Iteration: 6489 lambda_n: 0.9065147316904677 Loss: 0.04292521227763375\n",
      "Iteration: 6490 lambda_n: 0.9054227543451194 Loss: 0.04291748496977315\n",
      "Iteration: 6491 lambda_n: 0.9895955445953396 Loss: 0.04290977168758027\n",
      "Iteration: 6492 lambda_n: 1.0164203488968977 Loss: 0.042901346734176296\n",
      "Iteration: 6493 lambda_n: 0.8884426638538483 Loss: 0.04289269927667453\n",
      "Iteration: 6494 lambda_n: 0.9048693824595958 Loss: 0.04288514551882368\n",
      "Iteration: 6495 lambda_n: 0.95128480965755 Loss: 0.042877456765349466\n",
      "Iteration: 6496 lambda_n: 0.968204464728315 Loss: 0.04286937870183116\n",
      "Iteration: 6497 lambda_n: 0.9585557614139519 Loss: 0.04286116231898949\n",
      "Iteration: 6498 lambda_n: 0.9222260134982748 Loss: 0.042853033116147146\n",
      "Iteration: 6499 lambda_n: 0.9996624301782614 Loss: 0.042845217035191174\n",
      "Iteration: 6500 lambda_n: 0.91546598658723 Loss: 0.04283675022231838\n",
      "Iteration: 6501 lambda_n: 1.0125952285673474 Loss: 0.0428290016078136\n",
      "Iteration: 6502 lambda_n: 0.953242238245533 Loss: 0.042820436551520805\n",
      "Iteration: 6503 lambda_n: 0.9872442651532961 Loss: 0.04281237894804822\n",
      "Iteration: 6504 lambda_n: 0.9916335305564882 Loss: 0.04280403947333827\n",
      "Iteration: 6505 lambda_n: 0.972939079169376 Loss: 0.04279566860320219\n",
      "Iteration: 6506 lambda_n: 0.9657621253398934 Loss: 0.04278746107790013\n",
      "Iteration: 6507 lambda_n: 0.9386245675680486 Loss: 0.04277931950997587\n",
      "Iteration: 6508 lambda_n: 0.9512133690316261 Loss: 0.042771411405454825\n",
      "Iteration: 6509 lambda_n: 0.8892991199942213 Loss: 0.04276195559642581\n",
      "Iteration: 6510 lambda_n: 1.0011131249318956 Loss: 0.04275234344313593\n",
      "Iteration: 6511 lambda_n: 0.9169282833236583 Loss: 0.04274117070068748\n",
      "Iteration: 6512 lambda_n: 0.9522468880773228 Loss: 0.04273079787886411\n",
      "Iteration: 6513 lambda_n: 0.9533344078030207 Loss: 0.04272002157542502\n",
      "Iteration: 6514 lambda_n: 0.9524918563511149 Loss: 0.04270928417637161\n",
      "Iteration: 6515 lambda_n: 0.966664803630666 Loss: 0.04269863510835395\n",
      "Iteration: 6516 lambda_n: 0.9289014170504216 Loss: 0.04268791949663876\n",
      "Iteration: 6517 lambda_n: 0.9488967414381654 Loss: 0.04267771520239519\n",
      "Iteration: 6518 lambda_n: 0.9159778499503067 Loss: 0.04266738336004534\n",
      "Iteration: 6519 lambda_n: 0.8827351792066186 Loss: 0.04265749885888361\n",
      "Iteration: 6520 lambda_n: 0.9994639843642781 Loss: 0.042648054204573765\n",
      "Iteration: 6521 lambda_n: 0.895932289278196 Loss: 0.042637449990925703\n",
      "Iteration: 6522 lambda_n: 0.8838309274958113 Loss: 0.04262802966435601\n",
      "Iteration: 6523 lambda_n: 1.0132271701752056 Loss: 0.04261881177325815\n",
      "Iteration: 6524 lambda_n: 0.9195397866034882 Loss: 0.04260833066457031\n",
      "Iteration: 6525 lambda_n: 0.9161097910382017 Loss: 0.04259890283928941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6526 lambda_n: 0.9875502856682422 Loss: 0.04258958619889948\n",
      "Iteration: 6527 lambda_n: 1.0259854014033087 Loss: 0.04257962478575745\n",
      "Iteration: 6528 lambda_n: 0.8969851814084174 Loss: 0.04256936489455552\n",
      "Iteration: 6529 lambda_n: 1.0044404448662179 Loss: 0.04256047113491389\n",
      "Iteration: 6530 lambda_n: 0.9073417602221876 Loss: 0.04255058947843868\n",
      "Iteration: 6531 lambda_n: 1.0259757293785936 Loss: 0.04254173554670635\n",
      "Iteration: 6532 lambda_n: 0.9110650908480692 Loss: 0.042531800710410975\n",
      "Iteration: 6533 lambda_n: 1.0231254748924792 Loss: 0.0425230486332202\n",
      "Iteration: 6534 lambda_n: 0.9178589163259173 Loss: 0.042513292517245796\n",
      "Iteration: 6535 lambda_n: 0.9008680593358457 Loss: 0.04250460622716304\n",
      "Iteration: 6536 lambda_n: 0.94938343097376 Loss: 0.0424961381988504\n",
      "Iteration: 6537 lambda_n: 0.9890670197611856 Loss: 0.0424872727156037\n",
      "Iteration: 6538 lambda_n: 0.8943907444091671 Loss: 0.042478098231348065\n",
      "Iteration: 6539 lambda_n: 0.8774671697332569 Loss: 0.0424698548238817\n",
      "Iteration: 6540 lambda_n: 0.9645886212995715 Loss: 0.042461813240457925\n",
      "Iteration: 6541 lambda_n: 0.8865938271375228 Loss: 0.04245302226548834\n",
      "Iteration: 6542 lambda_n: 0.9980068535268385 Loss: 0.04244498624588812\n",
      "Iteration: 6543 lambda_n: 1.0201483406435023 Loss: 0.042435986775877364\n",
      "Iteration: 6544 lambda_n: 0.9847622834628795 Loss: 0.04242683608989184\n",
      "Iteration: 6545 lambda_n: 0.9584216143552122 Loss: 0.04241804651264952\n",
      "Iteration: 6546 lambda_n: 0.9072476475751108 Loss: 0.04240953036634037\n",
      "Iteration: 6547 lambda_n: 0.9867270047525997 Loss: 0.042401501437521624\n",
      "Iteration: 6548 lambda_n: 0.8858516316350471 Loss: 0.042392802006547825\n",
      "Iteration: 6549 lambda_n: 1.0252591493347583 Loss: 0.042385019551329985\n",
      "Iteration: 6550 lambda_n: 1.00642112471758 Loss: 0.042376041796408546\n",
      "Iteration: 6551 lambda_n: 0.8796089429258854 Loss: 0.04236725775548742\n",
      "Iteration: 6552 lambda_n: 0.9799032544304271 Loss: 0.042359602107582425\n",
      "Iteration: 6553 lambda_n: 0.9413645484910389 Loss: 0.04235109493741272\n",
      "Iteration: 6554 lambda_n: 1.0274295150123671 Loss: 0.04234294205015518\n",
      "Iteration: 6555 lambda_n: 0.8784583629800958 Loss: 0.04233406377661078\n",
      "Iteration: 6556 lambda_n: 0.9070222091066314 Loss: 0.042326488075170525\n",
      "Iteration: 6557 lambda_n: 1.0010709094952772 Loss: 0.042318679427289486\n",
      "Iteration: 6558 lambda_n: 0.977685079187842 Loss: 0.04231007553666839\n",
      "Iteration: 6559 lambda_n: 0.9832652873885303 Loss: 0.042301685966931506\n",
      "Iteration: 6560 lambda_n: 0.9307122355788188 Loss: 0.042293260535255\n",
      "Iteration: 6561 lambda_n: 1.0067841581558428 Loss: 0.04228529546991694\n",
      "Iteration: 6562 lambda_n: 0.8994498134236538 Loss: 0.04227668937083529\n",
      "Iteration: 6563 lambda_n: 0.9284074203193394 Loss: 0.042269008658126876\n",
      "Iteration: 6564 lambda_n: 0.9094362728554645 Loss: 0.042261087772074726\n",
      "Iteration: 6565 lambda_n: 0.935837052253886 Loss: 0.04225333506090989\n",
      "Iteration: 6566 lambda_n: 0.9100459352406925 Loss: 0.042245363221631534\n",
      "Iteration: 6567 lambda_n: 0.958697186057911 Loss: 0.04223761625827032\n",
      "Iteration: 6568 lambda_n: 0.9171520856836801 Loss: 0.042229460170689216\n",
      "Iteration: 6569 lambda_n: 0.990603047838762 Loss: 0.042221661709175115\n",
      "Iteration: 6570 lambda_n: 0.9343366370371712 Loss: 0.04221324311261477\n",
      "Iteration: 6571 lambda_n: 1.0014654437134876 Loss: 0.04220530615500484\n",
      "Iteration: 6572 lambda_n: 0.9423491331276298 Loss: 0.0421968025864147\n",
      "Iteration: 6573 lambda_n: 0.9090643947505329 Loss: 0.04218878218954407\n",
      "Iteration: 6574 lambda_n: 0.9370029832055602 Loss: 0.042181022696013606\n",
      "Iteration: 6575 lambda_n: 0.9033015042939124 Loss: 0.04217300461728382\n",
      "Iteration: 6576 lambda_n: 0.9214303015382779 Loss: 0.04216525766768508\n",
      "Iteration: 6577 lambda_n: 0.8769238930035904 Loss: 0.0421573416529179\n",
      "Iteration: 6578 lambda_n: 0.9681497496700626 Loss: 0.04214979742172523\n",
      "Iteration: 6579 lambda_n: 1.0096448350893605 Loss: 0.04214146009304276\n",
      "Iteration: 6580 lambda_n: 0.9604675271531846 Loss: 0.04213275812325428\n",
      "Iteration: 6581 lambda_n: 0.9400445203028038 Loss: 0.042124474817177945\n",
      "Iteration: 6582 lambda_n: 1.0080283871974276 Loss: 0.04211636463344062\n",
      "Iteration: 6583 lambda_n: 1.0170340641692361 Loss: 0.04210766629573458\n",
      "Iteration: 6584 lambda_n: 0.9494921180798297 Loss: 0.042098889594212666\n",
      "Iteration: 6585 lambda_n: 0.9950760951652017 Loss: 0.04209069604701518\n",
      "Iteration: 6586 lambda_n: 0.9072928155794113 Loss: 0.04208211028682211\n",
      "Iteration: 6587 lambda_n: 0.9387303665427362 Loss: 0.042074283438414475\n",
      "Iteration: 6588 lambda_n: 0.9400842548351708 Loss: 0.04206618728472325\n",
      "Iteration: 6589 lambda_n: 1.0236753430641252 Loss: 0.04205808165749638\n",
      "Iteration: 6590 lambda_n: 0.9948861375435295 Loss: 0.0420492579653428\n",
      "Iteration: 6591 lambda_n: 0.9636688163888728 Loss: 0.04204068533936821\n",
      "Iteration: 6592 lambda_n: 0.8928906251624247 Loss: 0.04203238455911966\n",
      "Iteration: 6593 lambda_n: 0.9701872045656528 Loss: 0.04202469604234573\n",
      "Iteration: 6594 lambda_n: 0.9946593203356752 Loss: 0.04201634463902353\n",
      "Iteration: 6595 lambda_n: 0.9489464337569589 Loss: 0.04200778554361199\n",
      "Iteration: 6596 lambda_n: 1.0177096900005227 Loss: 0.04199962265268225\n",
      "Iteration: 6597 lambda_n: 0.8987656166355874 Loss: 0.04199087115943087\n",
      "Iteration: 6598 lambda_n: 0.9341545375082111 Loss: 0.04198314512510054\n",
      "Iteration: 6599 lambda_n: 0.918208750640924 Loss: 0.04197511727946137\n",
      "Iteration: 6600 lambda_n: 0.8871702871541316 Loss: 0.04196722884390416\n",
      "Iteration: 6601 lambda_n: 1.0241626788317764 Loss: 0.041959609254694455\n",
      "Iteration: 6602 lambda_n: 0.8818230649676181 Loss: 0.041950815502007834\n",
      "Iteration: 6603 lambda_n: 0.9859248725506714 Loss: 0.041943246185517064\n",
      "Iteration: 6604 lambda_n: 0.957318305835936 Loss: 0.041934785448296444\n",
      "Iteration: 6605 lambda_n: 0.9647316318228351 Loss: 0.0419265724428938\n",
      "Iteration: 6606 lambda_n: 0.9192017106799191 Loss: 0.04191829796627779\n",
      "Iteration: 6607 lambda_n: 0.8766061681972104 Loss: 0.041910415967715865\n",
      "Iteration: 6608 lambda_n: 1.0096754563581591 Loss: 0.0419029009505282\n",
      "Iteration: 6609 lambda_n: 0.9997202544915775 Loss: 0.041894247020761596\n",
      "Iteration: 6610 lambda_n: 1.0219601568952281 Loss: 0.04188568046242288\n",
      "Iteration: 6611 lambda_n: 0.9239573917175652 Loss: 0.041876925338212016\n",
      "Iteration: 6612 lambda_n: 0.8838482451248045 Loss: 0.04186901158387236\n",
      "Iteration: 6613 lambda_n: 0.984323516732379 Loss: 0.041861442864739654\n",
      "Iteration: 6614 lambda_n: 0.9733989918876925 Loss: 0.041853015306149086\n",
      "Iteration: 6615 lambda_n: 0.9062771601470517 Loss: 0.041844682947441286\n",
      "Iteration: 6616 lambda_n: 1.0002668248416766 Loss: 0.041836926641804884\n",
      "Iteration: 6617 lambda_n: 1.0264899913178103 Loss: 0.04182836743491801\n",
      "Iteration: 6618 lambda_n: 0.9606181743966621 Loss: 0.041819585489803655\n",
      "Iteration: 6619 lambda_n: 0.9323988737945548 Loss: 0.04181136863511944\n",
      "Iteration: 6620 lambda_n: 0.971718359223976 Loss: 0.04180339452344214\n",
      "Iteration: 6621 lambda_n: 0.9645777944741033 Loss: 0.041795085494604284\n",
      "Iteration: 6622 lambda_n: 0.9362235030145791 Loss: 0.04178683888929129\n",
      "Iteration: 6623 lambda_n: 0.8897465580480564 Loss: 0.0417788359832832\n",
      "Iteration: 6624 lambda_n: 0.9678134411067084 Loss: 0.04177123152619717\n",
      "Iteration: 6625 lambda_n: 0.9256480445511408 Loss: 0.04176296103584346\n",
      "Iteration: 6626 lambda_n: 0.9310874154399068 Loss: 0.041755052075193456\n",
      "Iteration: 6627 lambda_n: 0.9340388059693918 Loss: 0.0417470977793751\n",
      "Iteration: 6628 lambda_n: 0.9771977555269974 Loss: 0.041739119400945164\n",
      "Iteration: 6629 lambda_n: 0.9876086870386023 Loss: 0.04173077353798626\n",
      "Iteration: 6630 lambda_n: 0.9964430737494023 Loss: 0.0417223399758186\n",
      "Iteration: 6631 lambda_n: 0.9386362899290368 Loss: 0.04171383219465699\n",
      "Iteration: 6632 lambda_n: 0.9952818717592491 Loss: 0.041705819115834694\n",
      "Iteration: 6633 lambda_n: 0.9106682346890914 Loss: 0.04169732358595066\n",
      "Iteration: 6634 lambda_n: 0.8795800928767177 Loss: 0.04168955137411246\n",
      "Iteration: 6635 lambda_n: 1.0246520292217676 Loss: 0.041682045426104174\n",
      "Iteration: 6636 lambda_n: 0.9329495186550458 Loss: 0.04167330255236185\n",
      "Iteration: 6637 lambda_n: 1.0200623741648394 Loss: 0.041665343224751225\n",
      "Iteration: 6638 lambda_n: 0.9683446133145673 Loss: 0.04165664179000844\n",
      "Iteration: 6639 lambda_n: 0.8842209356814372 Loss: 0.041648382628380616\n",
      "Iteration: 6640 lambda_n: 0.8860484713484809 Loss: 0.041640841916711885\n",
      "Iteration: 6641 lambda_n: 0.9628232110100682 Loss: 0.04163328648060653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6642 lambda_n: 0.927247199776369 Loss: 0.041625077310705694\n",
      "Iteration: 6643 lambda_n: 0.979967311373367 Loss: 0.04161717242943833\n",
      "Iteration: 6644 lambda_n: 0.9154627904493167 Loss: 0.04160881907897567\n",
      "Iteration: 6645 lambda_n: 0.8759141023501896 Loss: 0.041601016521662766\n",
      "Iteration: 6646 lambda_n: 0.9201108325642752 Loss: 0.041593551883299495\n",
      "Iteration: 6647 lambda_n: 0.9713433639813822 Loss: 0.04158571143970288\n",
      "Iteration: 6648 lambda_n: 0.9807646382383277 Loss: 0.04157743536364249\n",
      "Iteration: 6649 lambda_n: 0.8945830711975744 Loss: 0.04156907999809743\n",
      "Iteration: 6650 lambda_n: 0.9492680683234229 Loss: 0.04156145972741786\n",
      "Iteration: 6651 lambda_n: 1.0109857140652443 Loss: 0.04155337450200585\n",
      "Iteration: 6652 lambda_n: 0.8803912786757759 Loss: 0.04154476457889936\n",
      "Iteration: 6653 lambda_n: 0.90058808411715 Loss: 0.04153726773281791\n",
      "Iteration: 6654 lambda_n: 0.985261221297164 Loss: 0.041529599694436176\n",
      "Iteration: 6655 lambda_n: 1.0261160496568598 Loss: 0.04152121159117763\n",
      "Iteration: 6656 lambda_n: 0.8890549750515396 Loss: 0.04151247666460371\n",
      "Iteration: 6657 lambda_n: 0.9854725295612133 Loss: 0.04150490937322133\n",
      "Iteration: 6658 lambda_n: 0.9135103264846068 Loss: 0.041496522269807365\n",
      "Iteration: 6659 lambda_n: 1.0178791759369687 Loss: 0.041488748486739314\n",
      "Iteration: 6660 lambda_n: 1.027714821598994 Loss: 0.0414800874462395\n",
      "Iteration: 6661 lambda_n: 0.9184944204191818 Loss: 0.04147134371740646\n",
      "Iteration: 6662 lambda_n: 0.9093639323418246 Loss: 0.041463530123067624\n",
      "Iteration: 6663 lambda_n: 1.011578050932653 Loss: 0.041455794992216424\n",
      "Iteration: 6664 lambda_n: 0.9928201193092263 Loss: 0.041447191290525376\n",
      "Iteration: 6665 lambda_n: 0.9663698947710566 Loss: 0.04143874807076474\n",
      "Iteration: 6666 lambda_n: 0.9565868194205264 Loss: 0.041430530685552934\n",
      "Iteration: 6667 lambda_n: 0.9532406451266571 Loss: 0.04142239734748776\n",
      "Iteration: 6668 lambda_n: 0.8943075334731517 Loss: 0.04141429330290022\n",
      "Iteration: 6669 lambda_n: 0.9693530061989977 Loss: 0.04140669106462776\n",
      "Iteration: 6670 lambda_n: 1.027099843380295 Loss: 0.041398451684497295\n",
      "Iteration: 6671 lambda_n: 0.918175485447159 Loss: 0.041389722373916735\n",
      "Iteration: 6672 lambda_n: 0.8775390401982742 Loss: 0.04138191966236692\n",
      "Iteration: 6673 lambda_n: 0.8941579244615039 Loss: 0.041374463008436356\n",
      "Iteration: 6674 lambda_n: 1.0112361166252917 Loss: 0.04136686584663937\n",
      "Iteration: 6675 lambda_n: 0.9027644059190424 Loss: 0.041358274751862496\n",
      "Iteration: 6676 lambda_n: 0.8958859939645881 Loss: 0.04135060600427058\n",
      "Iteration: 6677 lambda_n: 1.0253957239005504 Loss: 0.04134299640549414\n",
      "Iteration: 6678 lambda_n: 0.9299129333560266 Loss: 0.041334287577304514\n",
      "Iteration: 6679 lambda_n: 0.9537919837272372 Loss: 0.041326390534972515\n",
      "Iteration: 6680 lambda_n: 1.0124767405946922 Loss: 0.04131829145348743\n",
      "Iteration: 6681 lambda_n: 0.9598507561681275 Loss: 0.041309694947089824\n",
      "Iteration: 6682 lambda_n: 0.8771901652892976 Loss: 0.041301546103943834\n",
      "Iteration: 6683 lambda_n: 1.0060421846334184 Loss: 0.041294099748324674\n",
      "Iteration: 6684 lambda_n: 0.889948798198899 Loss: 0.04128556034929768\n",
      "Iteration: 6685 lambda_n: 1.0199194146810209 Loss: 0.04127800712858444\n",
      "Iteration: 6686 lambda_n: 0.882582164827613 Loss: 0.04126935159598828\n",
      "Iteration: 6687 lambda_n: 0.932672118817974 Loss: 0.041261862336787455\n",
      "Iteration: 6688 lambda_n: 0.9405434295911889 Loss: 0.04125394873386904\n",
      "Iteration: 6689 lambda_n: 0.8982114010515468 Loss: 0.04124596908632003\n",
      "Iteration: 6690 lambda_n: 0.9107446905947382 Loss: 0.0412383492976907\n",
      "Iteration: 6691 lambda_n: 0.9398170955912102 Loss: 0.04123062387272803\n",
      "Iteration: 6692 lambda_n: 0.9747316785343251 Loss: 0.04122265255699269\n",
      "Iteration: 6693 lambda_n: 1.0022284325740711 Loss: 0.04121438586829188\n",
      "Iteration: 6694 lambda_n: 0.9588057128885829 Loss: 0.04120588679180284\n",
      "Iteration: 6695 lambda_n: 0.9600150340744829 Loss: 0.04119775674050733\n",
      "Iteration: 6696 lambda_n: 0.9634984381988843 Loss: 0.04118961719273921\n",
      "Iteration: 6697 lambda_n: 0.9616246713883325 Loss: 0.04118144886926922\n",
      "Iteration: 6698 lambda_n: 0.9649479016183505 Loss: 0.04117329718779157\n",
      "Iteration: 6699 lambda_n: 0.9870292259799768 Loss: 0.04116511809025868\n",
      "Iteration: 6700 lambda_n: 0.9099545907289216 Loss: 0.04115675259936238\n",
      "Iteration: 6701 lambda_n: 0.9109274609721075 Loss: 0.04114904107079772\n",
      "Iteration: 6702 lambda_n: 0.9623586422484727 Loss: 0.04114132196374656\n",
      "Iteration: 6703 lambda_n: 0.8819890180733438 Loss: 0.04113316773785692\n",
      "Iteration: 6704 lambda_n: 0.9906699105571641 Loss: 0.04112569517163902\n",
      "Iteration: 6705 lambda_n: 0.9524835338166184 Loss: 0.04111730251539215\n",
      "Iteration: 6706 lambda_n: 0.987207972484286 Loss: 0.04110923410819701\n",
      "Iteration: 6707 lambda_n: 0.9783740914263473 Loss: 0.04110087229583415\n",
      "Iteration: 6708 lambda_n: 0.9279836721891525 Loss: 0.04109258606603542\n",
      "Iteration: 6709 lambda_n: 0.891774127807559 Loss: 0.041084727320571536\n",
      "Iteration: 6710 lambda_n: 0.8787661999758123 Loss: 0.04107717586355479\n",
      "Iteration: 6711 lambda_n: 0.9072551737589577 Loss: 0.041069735164071204\n",
      "Iteration: 6712 lambda_n: 0.8828197057819892 Loss: 0.04106205385973968\n",
      "Iteration: 6713 lambda_n: 0.9174320468665009 Loss: 0.04105458005521488\n",
      "Iteration: 6714 lambda_n: 0.9007853795447835 Loss: 0.04104681385109327\n",
      "Iteration: 6715 lambda_n: 0.8962434387945374 Loss: 0.04103918919489206\n",
      "Iteration: 6716 lambda_n: 1.0144567528970423 Loss: 0.0410316035985144\n",
      "Iteration: 6717 lambda_n: 0.9558714096978806 Loss: 0.041023018166476105\n",
      "Iteration: 6718 lambda_n: 1.0223915633294722 Loss: 0.041014929278105086\n",
      "Iteration: 6719 lambda_n: 1.0208746790155327 Loss: 0.04100627821324472\n",
      "Iteration: 6720 lambda_n: 0.9087879755932494 Loss: 0.04099764076622388\n",
      "Iteration: 6721 lambda_n: 0.9583717413565007 Loss: 0.040989952354692376\n",
      "Iteration: 6722 lambda_n: 0.9495742317527268 Loss: 0.0409818451102407\n",
      "Iteration: 6723 lambda_n: 1.029303078101153 Loss: 0.04097381296112859\n",
      "Iteration: 6724 lambda_n: 0.8782490501859189 Loss: 0.040965107134896234\n",
      "Iteration: 6725 lambda_n: 0.9549877330011648 Loss: 0.04095767958017108\n",
      "Iteration: 6726 lambda_n: 0.995677002208261 Loss: 0.04094960364503573\n",
      "Iteration: 6727 lambda_n: 1.0012012939134356 Loss: 0.04094118431223636\n",
      "Iteration: 6728 lambda_n: 0.9398211955342636 Loss: 0.04093271899060537\n",
      "Iteration: 6729 lambda_n: 0.8913981237132518 Loss: 0.040924773325992814\n",
      "Iteration: 6730 lambda_n: 0.9460513649938783 Loss: 0.040917237653048215\n",
      "Iteration: 6731 lambda_n: 0.9452640403780473 Loss: 0.04090924056092881\n",
      "Iteration: 6732 lambda_n: 0.8848303379946207 Loss: 0.0409012507630329\n",
      "Iteration: 6733 lambda_n: 1.029052674915465 Loss: 0.040893772371259955\n",
      "Iteration: 6734 lambda_n: 0.9220244628180027 Loss: 0.04088507569472459\n",
      "Iteration: 6735 lambda_n: 0.8960347388352788 Loss: 0.04087728419582205\n",
      "Iteration: 6736 lambda_n: 1.0172136986536378 Loss: 0.040869712901126495\n",
      "Iteration: 6737 lambda_n: 0.8840173975611317 Loss: 0.04086111831390308\n",
      "Iteration: 6738 lambda_n: 1.0225160272850229 Loss: 0.04085364974475996\n",
      "Iteration: 6739 lambda_n: 0.8889066908086177 Loss: 0.04084501171112313\n",
      "Iteration: 6740 lambda_n: 0.9987547356359755 Loss: 0.04083750301010226\n",
      "Iteration: 6741 lambda_n: 0.9193221417602874 Loss: 0.04082906702523679\n",
      "Iteration: 6742 lambda_n: 0.9516036667912348 Loss: 0.04082130259563014\n",
      "Iteration: 6743 lambda_n: 0.8971175897124902 Loss: 0.040813266121142416\n",
      "Iteration: 6744 lambda_n: 1.0214503817578386 Loss: 0.04080569037143359\n",
      "Iteration: 6745 lambda_n: 0.9704664191855428 Loss: 0.04079706531385727\n",
      "Iteration: 6746 lambda_n: 0.9038573647394174 Loss: 0.040788871429196474\n",
      "Iteration: 6747 lambda_n: 0.8900355750156992 Loss: 0.04078124052877377\n",
      "Iteration: 6748 lambda_n: 0.9069720367548435 Loss: 0.04077372685844893\n",
      "Iteration: 6749 lambda_n: 0.912170823518359 Loss: 0.04076607074981253\n",
      "Iteration: 6750 lambda_n: 0.978053935056968 Loss: 0.04075837130617884\n",
      "Iteration: 6751 lambda_n: 0.9906661691258137 Loss: 0.04075011634989518\n",
      "Iteration: 6752 lambda_n: 0.9262019392049422 Loss: 0.040741755583406265\n",
      "Iteration: 6753 lambda_n: 0.8936142018113387 Loss: 0.040733939465711154\n",
      "Iteration: 6754 lambda_n: 0.9424026525014405 Loss: 0.04072639889252323\n",
      "Iteration: 6755 lambda_n: 0.9732980613836445 Loss: 0.0407184471784802\n",
      "Iteration: 6756 lambda_n: 1.0185842120793747 Loss: 0.0407102353742672\n",
      "Iteration: 6757 lambda_n: 0.9704160230486616 Loss: 0.040701642128870306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6758 lambda_n: 1.0047531430046888 Loss: 0.040693455886756144\n",
      "Iteration: 6759 lambda_n: 1.011252252978333 Loss: 0.04068498060911295\n",
      "Iteration: 6760 lambda_n: 1.0179498126296609 Loss: 0.04067645115860384\n",
      "Iteration: 6761 lambda_n: 0.9199680376124791 Loss: 0.0406678658712221\n",
      "Iteration: 6762 lambda_n: 0.9246503154902543 Loss: 0.04066010754126074\n",
      "Iteration: 6763 lambda_n: 0.9711731435882618 Loss: 0.040652310260053795\n",
      "Iteration: 6764 lambda_n: 0.9655583202665283 Loss: 0.04064412123129995\n",
      "Iteration: 6765 lambda_n: 0.9786824232702886 Loss: 0.04063598013264845\n",
      "Iteration: 6766 lambda_n: 0.8841986683095767 Loss: 0.040627728966242345\n",
      "Iteration: 6767 lambda_n: 0.9877374094770773 Loss: 0.04062027491504126\n",
      "Iteration: 6768 lambda_n: 0.8812251017699091 Loss: 0.040611948544318224\n",
      "Iteration: 6769 lambda_n: 0.9397183411695379 Loss: 0.040604520575989844\n",
      "Iteration: 6770 lambda_n: 0.9450988364918711 Loss: 0.040596600068381186\n",
      "Iteration: 6771 lambda_n: 0.9062647241862969 Loss: 0.040588634751897945\n",
      "Iteration: 6772 lambda_n: 0.9907173470280497 Loss: 0.04058099724858982\n",
      "Iteration: 6773 lambda_n: 0.9984617658501239 Loss: 0.0405726485701353\n",
      "Iteration: 6774 lambda_n: 0.8817838943006193 Loss: 0.040564235225668877\n",
      "Iteration: 6775 lambda_n: 0.951220667713595 Loss: 0.040556805568433746\n",
      "Iteration: 6776 lambda_n: 0.8803623796563824 Loss: 0.04054879135954235\n",
      "Iteration: 6777 lambda_n: 0.8932809721572237 Loss: 0.04054137464004671\n",
      "Iteration: 6778 lambda_n: 1.0066259594240345 Loss: 0.040533849551960925\n",
      "Iteration: 6779 lambda_n: 0.9834681039308919 Loss: 0.04052537016833088\n",
      "Iteration: 6780 lambda_n: 0.9898983013801342 Loss: 0.040517086436791194\n",
      "Iteration: 6781 lambda_n: 0.9141516866606628 Loss: 0.040508749112947086\n",
      "Iteration: 6782 lambda_n: 1.0011118642963714 Loss: 0.04050105028177074\n",
      "Iteration: 6783 lambda_n: 1.0203086101863093 Loss: 0.04049261961984918\n",
      "Iteration: 6784 lambda_n: 0.8974156560869361 Loss: 0.0404840278863603\n",
      "Iteration: 6785 lambda_n: 0.9516060847529594 Loss: 0.04047647152206908\n",
      "Iteration: 6786 lambda_n: 0.9114247238418087 Loss: 0.04046845935691699\n",
      "Iteration: 6787 lambda_n: 0.9037289783449559 Loss: 0.04046078599630229\n",
      "Iteration: 6788 lambda_n: 0.9890788712650023 Loss: 0.04045317789361421\n",
      "Iteration: 6789 lambda_n: 0.9813510888467404 Loss: 0.04044485177490236\n",
      "Iteration: 6790 lambda_n: 0.9234551438166135 Loss: 0.040436591254726074\n",
      "Iteration: 6791 lambda_n: 0.9157736361226754 Loss: 0.04042881857908122\n",
      "Iteration: 6792 lambda_n: 0.9360908713955514 Loss: 0.04042111102951012\n",
      "Iteration: 6793 lambda_n: 0.9294623163572484 Loss: 0.04041323295783566\n",
      "Iteration: 6794 lambda_n: 0.9731515121452218 Loss: 0.040405411152431916\n",
      "Iteration: 6795 lambda_n: 0.9426208940010631 Loss: 0.0403972221840667\n",
      "Iteration: 6796 lambda_n: 0.9926224155240378 Loss: 0.040389290629598305\n",
      "Iteration: 6797 lambda_n: 0.910990385337895 Loss: 0.04038093885665612\n",
      "Iteration: 6798 lambda_n: 0.9942874767295315 Loss: 0.040373274412039376\n",
      "Iteration: 6799 lambda_n: 1.0033710977424106 Loss: 0.04036490965618392\n",
      "Iteration: 6800 lambda_n: 0.9177739959865854 Loss: 0.04035646901944944\n",
      "Iteration: 6801 lambda_n: 1.0216125775756602 Loss: 0.04034874894063546\n",
      "Iteration: 6802 lambda_n: 1.002339600703616 Loss: 0.04034015590277949\n",
      "Iteration: 6803 lambda_n: 1.026087686771325 Loss: 0.04033172551864481\n",
      "Iteration: 6804 lambda_n: 1.0113190520181778 Loss: 0.040323095941607955\n",
      "Iteration: 6805 lambda_n: 0.9003187095926581 Loss: 0.040314591117671265\n",
      "Iteration: 6806 lambda_n: 1.0298948312977558 Loss: 0.04030702024018964\n",
      "Iteration: 6807 lambda_n: 0.9907187543141938 Loss: 0.040298360231770886\n",
      "Iteration: 6808 lambda_n: 0.877239978443681 Loss: 0.0402900301698111\n",
      "Iteration: 6809 lambda_n: 0.988591793404854 Loss: 0.04028265469504124\n",
      "Iteration: 6810 lambda_n: 0.9481959117383774 Loss: 0.04027434347033032\n",
      "Iteration: 6811 lambda_n: 1.0278127828054555 Loss: 0.04026637233890891\n",
      "Iteration: 6812 lambda_n: 0.9505203990568071 Loss: 0.040257732398574546\n",
      "Iteration: 6813 lambda_n: 0.9702973630252058 Loss: 0.04024974268333664\n",
      "Iteration: 6814 lambda_n: 0.9173223085446318 Loss: 0.04024158719791082\n",
      "Iteration: 6815 lambda_n: 1.0268555098361938 Loss: 0.0402338774221628\n",
      "Iteration: 6816 lambda_n: 0.9400175684738176 Loss: 0.040225247534300394\n",
      "Iteration: 6817 lambda_n: 0.9592541996712207 Loss: 0.04021734792839237\n",
      "Iteration: 6818 lambda_n: 0.9341757033210016 Loss: 0.04020928711302187\n",
      "Iteration: 6819 lambda_n: 0.9945963172139008 Loss: 0.040201437480420016\n",
      "Iteration: 6820 lambda_n: 1.0259475714216943 Loss: 0.04019308060931925\n",
      "Iteration: 6821 lambda_n: 0.911546138031745 Loss: 0.040184460818271026\n",
      "Iteration: 6822 lambda_n: 0.9721543699617409 Loss: 0.04017680265683764\n",
      "Iteration: 6823 lambda_n: 0.8777430854818726 Loss: 0.0401686357410697\n",
      "Iteration: 6824 lambda_n: 0.9002885465398396 Loss: 0.04016126236998921\n",
      "Iteration: 6825 lambda_n: 1.0200688246566314 Loss: 0.04015369999015145\n",
      "Iteration: 6826 lambda_n: 0.9129438390809677 Loss: 0.04014513190677373\n",
      "Iteration: 6827 lambda_n: 0.9779643910016347 Loss: 0.04013746406320288\n",
      "Iteration: 6828 lambda_n: 0.9605788650487287 Loss: 0.040129250536909036\n",
      "Iteration: 6829 lambda_n: 1.0086287977906603 Loss: 0.04012118346907628\n",
      "Iteration: 6830 lambda_n: 0.9167988551562919 Loss: 0.0401127133301014\n",
      "Iteration: 6831 lambda_n: 0.9534163625228822 Loss: 0.040105014780654584\n",
      "Iteration: 6832 lambda_n: 0.9943319588925813 Loss: 0.040097009156031184\n",
      "Iteration: 6833 lambda_n: 1.0147843859705223 Loss: 0.04008866041469905\n",
      "Iteration: 6834 lambda_n: 0.9164141261136161 Loss: 0.04008014041606029\n",
      "Iteration: 6835 lambda_n: 0.931255121275126 Loss: 0.040072446747180704\n",
      "Iteration: 6836 lambda_n: 1.0009386231041968 Loss: 0.040064628874168104\n",
      "Iteration: 6837 lambda_n: 0.9777879722626118 Loss: 0.040056226437358254\n",
      "Iteration: 6838 lambda_n: 0.913171272262044 Loss: 0.04004801878446384\n",
      "Iteration: 6839 lambda_n: 1.0215234316594013 Loss: 0.04004035393311032\n",
      "Iteration: 6840 lambda_n: 0.945766862355428 Loss: 0.04003178003433749\n",
      "Iteration: 6841 lambda_n: 0.8953671784254632 Loss: 0.04002384241035566\n",
      "Iteration: 6842 lambda_n: 0.8813489270736503 Loss: 0.04001632815709028\n",
      "Iteration: 6843 lambda_n: 0.905424325629181 Loss: 0.040008931900616995\n",
      "Iteration: 6844 lambda_n: 0.9066651967169665 Loss: 0.04000133395803199\n",
      "Iteration: 6845 lambda_n: 0.8776940174137393 Loss: 0.039993725964518834\n",
      "Iteration: 6846 lambda_n: 0.945067592144557 Loss: 0.03998636142196622\n",
      "Iteration: 6847 lambda_n: 0.9463439121443914 Loss: 0.0399784319267924\n",
      "Iteration: 6848 lambda_n: 0.9676407331253603 Loss: 0.039970492112101706\n",
      "Iteration: 6849 lambda_n: 0.9846520214891242 Loss: 0.03996237401477936\n",
      "Iteration: 6850 lambda_n: 0.9092676332499119 Loss: 0.03995411361146118\n",
      "Iteration: 6851 lambda_n: 0.9346745283111475 Loss: 0.039946486002055324\n",
      "Iteration: 6852 lambda_n: 1.0080896179737817 Loss: 0.039938645624792775\n",
      "Iteration: 6853 lambda_n: 0.8765193654893887 Loss: 0.039930189819352285\n",
      "Iteration: 6854 lambda_n: 0.9373260124847679 Loss: 0.039922837989251905\n",
      "Iteration: 6855 lambda_n: 0.9369085447274338 Loss: 0.03991497648992639\n",
      "Iteration: 6856 lambda_n: 0.9603412333626163 Loss: 0.03990711886063171\n",
      "Iteration: 6857 lambda_n: 0.9075916694415138 Loss: 0.03989906508369554\n",
      "Iteration: 6858 lambda_n: 0.9509656552008072 Loss: 0.03989145404523741\n",
      "Iteration: 6859 lambda_n: 1.0119411400725644 Loss: 0.03988347963249061\n",
      "Iteration: 6860 lambda_n: 0.978669435016712 Loss: 0.03987499430277411\n",
      "Iteration: 6861 lambda_n: 0.9914663383486053 Loss: 0.039866788368449146\n",
      "Iteration: 6862 lambda_n: 1.023543355963799 Loss: 0.039858475531547415\n",
      "Iteration: 6863 lambda_n: 0.9460440465776829 Loss: 0.03984989416210876\n",
      "Iteration: 6864 lambda_n: 0.9877379140608238 Loss: 0.03984196293498441\n",
      "Iteration: 6865 lambda_n: 0.9673770143531518 Loss: 0.03983368254185874\n",
      "Iteration: 6866 lambda_n: 0.9834076650900553 Loss: 0.03982557322030177\n",
      "Iteration: 6867 lambda_n: 1.0276279767390488 Loss: 0.03981732989702612\n",
      "Iteration: 6868 lambda_n: 0.9610832283586848 Loss: 0.0398087163036732\n",
      "Iteration: 6869 lambda_n: 0.8777851064858764 Loss: 0.039800718891409015\n",
      "Iteration: 6870 lambda_n: 0.9546122090699388 Loss: 0.03979343276016775\n",
      "Iteration: 6871 lambda_n: 0.9215236558493378 Loss: 0.03978550895904967\n",
      "Iteration: 6872 lambda_n: 0.8782225189678886 Loss: 0.039777859849468314\n",
      "Iteration: 6873 lambda_n: 1.0031470568842207 Loss: 0.03977057019593508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6874 lambda_n: 1.0095991756914349 Loss: 0.039762243653597575\n",
      "Iteration: 6875 lambda_n: 1.0113437316383824 Loss: 0.03975386360049166\n",
      "Iteration: 6876 lambda_n: 1.0187058816667252 Loss: 0.03974546911126151\n",
      "Iteration: 6877 lambda_n: 0.958109164703432 Loss: 0.03973701355825075\n",
      "Iteration: 6878 lambda_n: 0.9282529286808177 Loss: 0.03972906101530876\n",
      "Iteration: 6879 lambda_n: 0.930109797823528 Loss: 0.03972135632320648\n",
      "Iteration: 6880 lambda_n: 0.9857738717177393 Loss: 0.03971363625487311\n",
      "Iteration: 6881 lambda_n: 0.9306612625048727 Loss: 0.03970545420499252\n",
      "Iteration: 6882 lambda_n: 0.9690988994345932 Loss: 0.03969772963302921\n",
      "Iteration: 6883 lambda_n: 0.9201694554333919 Loss: 0.03968968606306666\n",
      "Iteration: 6884 lambda_n: 0.9630615329135559 Loss: 0.03968204864461855\n",
      "Iteration: 6885 lambda_n: 0.9808252684857007 Loss: 0.03967405525802478\n",
      "Iteration: 6886 lambda_n: 0.8917848596571573 Loss: 0.03966591447080953\n",
      "Iteration: 6887 lambda_n: 0.9838470465409859 Loss: 0.039658512745304145\n",
      "Iteration: 6888 lambda_n: 0.9739945067719328 Loss: 0.039650346949583755\n",
      "Iteration: 6889 lambda_n: 1.0283051565690968 Loss: 0.03964226296522213\n",
      "Iteration: 6890 lambda_n: 0.9440283948273147 Loss: 0.03963372825177347\n",
      "Iteration: 6891 lambda_n: 0.985598336930214 Loss: 0.03962589305180531\n",
      "Iteration: 6892 lambda_n: 0.9430184886112628 Loss: 0.03961771286765281\n",
      "Iteration: 6893 lambda_n: 0.9967760451216691 Loss: 0.0396098861172837\n",
      "Iteration: 6894 lambda_n: 0.8799212718303366 Loss: 0.03960161323244271\n",
      "Iteration: 6895 lambda_n: 0.9225099986885238 Loss: 0.03959431022929252\n",
      "Iteration: 6896 lambda_n: 1.0256859240600205 Loss: 0.03958665378658871\n",
      "Iteration: 6897 lambda_n: 0.9181402447946444 Loss: 0.03957814106400795\n",
      "Iteration: 6898 lambda_n: 0.8885432514549653 Loss: 0.03957052095153894\n",
      "Iteration: 6899 lambda_n: 1.0196581281375983 Loss: 0.03956314650720416\n",
      "Iteration: 6900 lambda_n: 0.8903792941179056 Loss: 0.03955468391295453\n",
      "Iteration: 6901 lambda_n: 0.8842247010042099 Loss: 0.03954729428869601\n",
      "Iteration: 6902 lambda_n: 1.026260226055291 Loss: 0.03953995577034172\n",
      "Iteration: 6903 lambda_n: 0.9858996563682054 Loss: 0.03953143847932563\n",
      "Iteration: 6904 lambda_n: 0.877963483616914 Loss: 0.03952325618704314\n",
      "Iteration: 6905 lambda_n: 0.9124882450469606 Loss: 0.03951596971673688\n",
      "Iteration: 6906 lambda_n: 0.8815538379719169 Loss: 0.03950839674230012\n",
      "Iteration: 6907 lambda_n: 0.890387347730009 Loss: 0.03950108052541372\n",
      "Iteration: 6908 lambda_n: 0.937452817143629 Loss: 0.03949369102224922\n",
      "Iteration: 6909 lambda_n: 0.9776921530448337 Loss: 0.039485910940645935\n",
      "Iteration: 6910 lambda_n: 0.8903306516732097 Loss: 0.03947779693534039\n",
      "Iteration: 6911 lambda_n: 0.9448659296901395 Loss: 0.039470407979983975\n",
      "Iteration: 6912 lambda_n: 0.9076684555136856 Loss: 0.03946256645699475\n",
      "Iteration: 6913 lambda_n: 0.9674652597705509 Loss: 0.03945503366374769\n",
      "Iteration: 6914 lambda_n: 0.9595306520366836 Loss: 0.03944700464086639\n",
      "Iteration: 6915 lambda_n: 0.8970168490582452 Loss: 0.039439041494440906\n",
      "Iteration: 6916 lambda_n: 0.9835923648688082 Loss: 0.03943159717349819\n",
      "Iteration: 6917 lambda_n: 0.9044794248083794 Loss: 0.03942343439189507\n",
      "Iteration: 6918 lambda_n: 1.0236640361698108 Loss: 0.03941592818759631\n",
      "Iteration: 6919 lambda_n: 0.966627543627686 Loss: 0.039407432908488\n",
      "Iteration: 6920 lambda_n: 0.9029146955107532 Loss: 0.03939941099489302\n",
      "Iteration: 6921 lambda_n: 0.9602851622112472 Loss: 0.03939191784790261\n",
      "Iteration: 6922 lambda_n: 0.974208794737848 Loss: 0.039383948617088896\n",
      "Iteration: 6923 lambda_n: 0.8999124421592204 Loss: 0.03937586386167389\n",
      "Iteration: 6924 lambda_n: 0.9630152491393633 Loss: 0.039368395697297665\n",
      "Iteration: 6925 lambda_n: 1.0128749540251156 Loss: 0.03936040388126449\n",
      "Iteration: 6926 lambda_n: 0.9944561333394981 Loss: 0.03935199831844036\n",
      "Iteration: 6927 lambda_n: 0.9408012261579957 Loss: 0.03934374563295769\n",
      "Iteration: 6928 lambda_n: 0.9930229151139695 Loss: 0.039335938234844325\n",
      "Iteration: 6929 lambda_n: 0.9523409572226147 Loss: 0.03932769749028173\n",
      "Iteration: 6930 lambda_n: 0.8793982785752802 Loss: 0.03931979437255213\n",
      "Iteration: 6931 lambda_n: 0.9958920620815824 Loss: 0.03931249659673393\n",
      "Iteration: 6932 lambda_n: 1.0115175665208662 Loss: 0.03930423210880556\n",
      "Iteration: 6933 lambda_n: 0.9775956024061229 Loss: 0.039295837974979514\n",
      "Iteration: 6934 lambda_n: 0.9001097849589994 Loss: 0.03928772536599255\n",
      "Iteration: 6935 lambda_n: 0.8991648885638043 Loss: 0.03928025579347216\n",
      "Iteration: 6936 lambda_n: 1.0071783860457852 Loss: 0.03927279407995326\n",
      "Iteration: 6937 lambda_n: 1.0051251106630874 Loss: 0.03926443603950006\n",
      "Iteration: 6938 lambda_n: 0.9959860523932602 Loss: 0.03925609505963713\n",
      "Iteration: 6939 lambda_n: 0.8763200062934593 Loss: 0.039247829940570966\n",
      "Iteration: 6940 lambda_n: 0.890747976625683 Loss: 0.03924055787708521\n",
      "Iteration: 6941 lambda_n: 1.0205149159210278 Loss: 0.03923316610055219\n",
      "Iteration: 6942 lambda_n: 0.9846800177415951 Loss: 0.039224697488411384\n",
      "Iteration: 6943 lambda_n: 0.9122553822203833 Loss: 0.03921652626651362\n",
      "Iteration: 6944 lambda_n: 0.9754727954387051 Loss: 0.039208956065568026\n",
      "Iteration: 6945 lambda_n: 0.8799279369973443 Loss: 0.03920086128375797\n",
      "Iteration: 6946 lambda_n: 0.9504603355251845 Loss: 0.03919355937746515\n",
      "Iteration: 6947 lambda_n: 1.008579129393714 Loss: 0.03918567218913907\n",
      "Iteration: 6948 lambda_n: 0.9203763859731069 Loss: 0.0391773027333886\n",
      "Iteration: 6949 lambda_n: 0.9091638158274431 Loss: 0.03916966522186392\n",
      "Iteration: 6950 lambda_n: 0.9090933705933617 Loss: 0.039162120769398526\n",
      "Iteration: 6951 lambda_n: 0.9688206965484435 Loss: 0.03915457691568531\n",
      "Iteration: 6952 lambda_n: 0.9239573848393647 Loss: 0.0391465374478815\n",
      "Iteration: 6953 lambda_n: 0.9785648711281597 Loss: 0.03913887027866115\n",
      "Iteration: 6954 lambda_n: 0.950620471497612 Loss: 0.03913074998234467\n",
      "Iteration: 6955 lambda_n: 0.9135707843510443 Loss: 0.03912286158758068\n",
      "Iteration: 6956 lambda_n: 0.9601108148081479 Loss: 0.039115280649608744\n",
      "Iteration: 6957 lambda_n: 0.9648070269375374 Loss: 0.03910731353042489\n",
      "Iteration: 6958 lambda_n: 0.8795939186765318 Loss: 0.03909930745550732\n",
      "Iteration: 6959 lambda_n: 0.8908528419029528 Loss: 0.039092008499139116\n",
      "Iteration: 6960 lambda_n: 1.022616013456363 Loss: 0.039084616126656396\n",
      "Iteration: 6961 lambda_n: 0.894673463824149 Loss: 0.039076130388101295\n",
      "Iteration: 6962 lambda_n: 0.9972135112502393 Loss: 0.03906870633580144\n",
      "Iteration: 6963 lambda_n: 0.9434193937094199 Loss: 0.03906043141459557\n",
      "Iteration: 6964 lambda_n: 1.001318162221229 Loss: 0.03905260289066375\n",
      "Iteration: 6965 lambda_n: 0.9695639821315066 Loss: 0.03904429393442799\n",
      "Iteration: 6966 lambda_n: 0.895641976695564 Loss: 0.03903624848655507\n",
      "Iteration: 6967 lambda_n: 0.9503345926217478 Loss: 0.03902881645321329\n",
      "Iteration: 6968 lambda_n: 0.9876826263014521 Loss: 0.03902093059216908\n",
      "Iteration: 6969 lambda_n: 0.9768128405027156 Loss: 0.039012734829597946\n",
      "Iteration: 6970 lambda_n: 1.004575505629655 Loss: 0.03900462927505198\n",
      "Iteration: 6971 lambda_n: 0.9103061121398001 Loss: 0.03899629335857528\n",
      "Iteration: 6972 lambda_n: 0.969044028660379 Loss: 0.03898873969286401\n",
      "Iteration: 6973 lambda_n: 0.9088666705192938 Loss: 0.038980698633994185\n",
      "Iteration: 6974 lambda_n: 0.9547357482927122 Loss: 0.03897315693053009\n",
      "Iteration: 6975 lambda_n: 0.9043646381893478 Loss: 0.03896523461874071\n",
      "Iteration: 6976 lambda_n: 1.0010279741253094 Loss: 0.03895773028940269\n",
      "Iteration: 6977 lambda_n: 1.013717242291051 Loss: 0.03894942386777068\n",
      "Iteration: 6978 lambda_n: 0.9748368490372247 Loss: 0.03894101216159037\n",
      "Iteration: 6979 lambda_n: 0.9417307473667889 Loss: 0.03893292308837405\n",
      "Iteration: 6980 lambda_n: 0.9392665453314565 Loss: 0.038925108732712155\n",
      "Iteration: 6981 lambda_n: 0.9067567679704831 Loss: 0.038917314832027865\n",
      "Iteration: 6982 lambda_n: 1.0228916092939173 Loss: 0.03890979069917469\n",
      "Iteration: 6983 lambda_n: 1.0090181256803952 Loss: 0.03890130290634797\n",
      "Iteration: 6984 lambda_n: 0.8991548222569766 Loss: 0.03889293024102276\n",
      "Iteration: 6985 lambda_n: 0.8831958067705123 Loss: 0.03888546920783239\n",
      "Iteration: 6986 lambda_n: 0.915483722685807 Loss: 0.03887814060512581\n",
      "Iteration: 6987 lambda_n: 0.9560564265110258 Loss: 0.03887054408906492\n",
      "Iteration: 6988 lambda_n: 0.8812598491452388 Loss: 0.03886261091462489\n",
      "Iteration: 6989 lambda_n: 0.8882155316191052 Loss: 0.03885529839195314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6990 lambda_n: 0.9138206422955372 Loss: 0.03884792815721809\n",
      "Iteration: 6991 lambda_n: 0.9311864230455295 Loss: 0.038840345461547056\n",
      "Iteration: 6992 lambda_n: 0.9386071823695513 Loss: 0.03883261867322598\n",
      "Iteration: 6993 lambda_n: 0.8808366132597542 Loss: 0.03882483031376875\n",
      "Iteration: 6994 lambda_n: 0.8765584639177658 Loss: 0.03881752132519829\n",
      "Iteration: 6995 lambda_n: 0.9881839642895092 Loss: 0.038810247839408465\n",
      "Iteration: 6996 lambda_n: 0.9603539665186664 Loss: 0.038802048116268854\n",
      "Iteration: 6997 lambda_n: 0.9923414329217395 Loss: 0.03879407932364339\n",
      "Iteration: 6998 lambda_n: 0.8990562913095734 Loss: 0.03878584511097413\n",
      "Iteration: 6999 lambda_n: 0.9146422644113703 Loss: 0.038778384958013924\n",
      "Iteration: 7000 lambda_n: 1.026072762346178 Loss: 0.03877079547960177\n",
      "Iteration: 7001 lambda_n: 0.8921953924084408 Loss: 0.03876228138334481\n",
      "Iteration: 7002 lambda_n: 0.9417748017769746 Loss: 0.03875487816880146\n",
      "Iteration: 7003 lambda_n: 0.9758375817822574 Loss: 0.03874706355995411\n",
      "Iteration: 7004 lambda_n: 0.9956515566346439 Loss: 0.0387389663097972\n",
      "Iteration: 7005 lambda_n: 0.943384512106769 Loss: 0.03873070465102512\n",
      "Iteration: 7006 lambda_n: 0.9412573756299293 Loss: 0.03872287669177387\n",
      "Iteration: 7007 lambda_n: 0.9303706204344923 Loss: 0.038715066384600275\n",
      "Iteration: 7008 lambda_n: 0.9656322626020207 Loss: 0.038707346414153423\n",
      "Iteration: 7009 lambda_n: 0.9353545464527965 Loss: 0.038699333853783616\n",
      "Iteration: 7010 lambda_n: 0.9722581806310964 Loss: 0.03869157253046904\n",
      "Iteration: 7011 lambda_n: 1.0185759747013279 Loss: 0.03868350499213611\n",
      "Iteration: 7012 lambda_n: 1.0082631485738391 Loss: 0.03867505312270699\n",
      "Iteration: 7013 lambda_n: 1.026830702372197 Loss: 0.038666686826687655\n",
      "Iteration: 7014 lambda_n: 0.9365768702391293 Loss: 0.038658166462727205\n",
      "Iteration: 7015 lambda_n: 0.9958563250087173 Loss: 0.038650394999297955\n",
      "Iteration: 7016 lambda_n: 1.005133978584595 Loss: 0.03864213165170509\n",
      "Iteration: 7017 lambda_n: 0.9102682948829292 Loss: 0.03863379132039456\n",
      "Iteration: 7018 lambda_n: 0.9643928933513212 Loss: 0.03862623815697391\n",
      "Iteration: 7019 lambda_n: 0.9910932951832192 Loss: 0.0386182358822099\n",
      "Iteration: 7020 lambda_n: 0.9142627397546615 Loss: 0.03861001205406871\n",
      "Iteration: 7021 lambda_n: 1.002113397654403 Loss: 0.038602425743092014\n",
      "Iteration: 7022 lambda_n: 0.922429235011717 Loss: 0.0385941104706237\n",
      "Iteration: 7023 lambda_n: 0.9105705244787202 Loss: 0.038586456393533024\n",
      "Iteration: 7024 lambda_n: 0.9198847821881426 Loss: 0.03857890071515349\n",
      "Iteration: 7025 lambda_n: 1.0084429820864869 Loss: 0.03857126774785385\n",
      "Iteration: 7026 lambda_n: 0.9733275100209303 Loss: 0.03856289994669219\n",
      "Iteration: 7027 lambda_n: 0.9159674636079963 Loss: 0.038554823521697305\n",
      "Iteration: 7028 lambda_n: 0.9384733286256214 Loss: 0.03854722305257151\n",
      "Iteration: 7029 lambda_n: 0.9882137669878283 Loss: 0.03853943583323008\n",
      "Iteration: 7030 lambda_n: 0.9563203629442605 Loss: 0.038531235877932835\n",
      "Iteration: 7031 lambda_n: 0.9209867232649172 Loss: 0.03852330056266761\n",
      "Iteration: 7032 lambda_n: 0.9304037517301027 Loss: 0.03851565843380902\n",
      "Iteration: 7033 lambda_n: 0.9976039321240843 Loss: 0.0385079381616525\n",
      "Iteration: 7034 lambda_n: 0.8959795922283983 Loss: 0.03849966027557671\n",
      "Iteration: 7035 lambda_n: 0.879206571919595 Loss: 0.03849222563966532\n",
      "Iteration: 7036 lambda_n: 0.893992607486033 Loss: 0.0384849301788963\n",
      "Iteration: 7037 lambda_n: 0.9849413511306401 Loss: 0.03847751202359824\n",
      "Iteration: 7038 lambda_n: 0.9978687209131146 Loss: 0.03846933919277763\n",
      "Iteration: 7039 lambda_n: 0.9675639739848539 Loss: 0.03846105908889901\n",
      "Iteration: 7040 lambda_n: 0.9641082832306029 Loss: 0.03845303044214764\n",
      "Iteration: 7041 lambda_n: 0.9893617629040997 Loss: 0.03844503046512037\n",
      "Iteration: 7042 lambda_n: 0.9011148206620423 Loss: 0.038436820935031535\n",
      "Iteration: 7043 lambda_n: 0.925884845622426 Loss: 0.03842934365467252\n",
      "Iteration: 7044 lambda_n: 1.021491064051667 Loss: 0.03842166083288651\n",
      "Iteration: 7045 lambda_n: 0.8925648353977127 Loss: 0.03841318468415001\n",
      "Iteration: 7046 lambda_n: 1.0226825080380608 Loss: 0.038405778334831456\n",
      "Iteration: 7047 lambda_n: 0.9135966695993383 Loss: 0.03839729228765746\n",
      "Iteration: 7048 lambda_n: 0.9545296827275519 Loss: 0.038389711408979905\n",
      "Iteration: 7049 lambda_n: 0.9296691290131551 Loss: 0.03838179086951973\n",
      "Iteration: 7050 lambda_n: 1.0215772927804745 Loss: 0.038374076612758844\n",
      "Iteration: 7051 lambda_n: 0.9345535701229832 Loss: 0.03836559971038712\n",
      "Iteration: 7052 lambda_n: 1.0240528433483147 Loss: 0.03835784491052402\n",
      "Iteration: 7053 lambda_n: 0.9541541316999791 Loss: 0.038349347452024916\n",
      "Iteration: 7054 lambda_n: 0.9553953643761739 Loss: 0.0383414299958452\n",
      "Iteration: 7055 lambda_n: 0.9194161135577258 Loss: 0.038333502233149\n",
      "Iteration: 7056 lambda_n: 0.9069424886706495 Loss: 0.03832587301494124\n",
      "Iteration: 7057 lambda_n: 0.9842477072187037 Loss: 0.03831834729480611\n",
      "Iteration: 7058 lambda_n: 0.8974230778767328 Loss: 0.038310180097329456\n",
      "Iteration: 7059 lambda_n: 0.8925380013907815 Loss: 0.038302733354389856\n",
      "Iteration: 7060 lambda_n: 0.9686951183174269 Loss: 0.03829532714064713\n",
      "Iteration: 7061 lambda_n: 0.9643329823656739 Loss: 0.038287288974425925\n",
      "Iteration: 7062 lambda_n: 0.9407250981028112 Loss: 0.03827928699675409\n",
      "Iteration: 7063 lambda_n: 1.0027487273844538 Loss: 0.0382714809076065\n",
      "Iteration: 7064 lambda_n: 1.0252914551814454 Loss: 0.03826316014185938\n",
      "Iteration: 7065 lambda_n: 0.9266765634962273 Loss: 0.03825465230843462\n",
      "Iteration: 7066 lambda_n: 1.0178001291255974 Loss: 0.038246962768029406\n",
      "Iteration: 7067 lambda_n: 1.0198652034999844 Loss: 0.038238517078684094\n",
      "Iteration: 7068 lambda_n: 0.9587996167885319 Loss: 0.03823005424346415\n",
      "Iteration: 7069 lambda_n: 0.9624078727789124 Loss: 0.0382220981198659\n",
      "Iteration: 7070 lambda_n: 0.938911728537115 Loss: 0.03821411204587399\n",
      "Iteration: 7071 lambda_n: 0.9809203450885671 Loss: 0.03820632093389643\n",
      "Iteration: 7072 lambda_n: 0.9348999989735742 Loss: 0.038198181224566294\n",
      "Iteration: 7073 lambda_n: 0.905635909259673 Loss: 0.03819042338362706\n",
      "Iteration: 7074 lambda_n: 0.9279193433592446 Loss: 0.03818290836823732\n",
      "Iteration: 7075 lambda_n: 0.9356456654155123 Loss: 0.03817520843509667\n",
      "Iteration: 7076 lambda_n: 1.021329791002817 Loss: 0.03816744437926376\n",
      "Iteration: 7077 lambda_n: 0.8825779106225653 Loss: 0.03815896930102294\n",
      "Iteration: 7078 lambda_n: 1.024252519812615 Loss: 0.03815164558594785\n",
      "Iteration: 7079 lambda_n: 1.0187374330127448 Loss: 0.03814314623323518\n",
      "Iteration: 7080 lambda_n: 0.9345686861749404 Loss: 0.03813469263352496\n",
      "Iteration: 7081 lambda_n: 0.9109122046261805 Loss: 0.0381269374640051\n",
      "Iteration: 7082 lambda_n: 0.9271697526823234 Loss: 0.038119378588971485\n",
      "Iteration: 7083 lambda_n: 0.9853548314690468 Loss: 0.038111684796958874\n",
      "Iteration: 7084 lambda_n: 0.9054469080612431 Loss: 0.03810350816650217\n",
      "Iteration: 7085 lambda_n: 0.9896381867315014 Loss: 0.038095994613297796\n",
      "Iteration: 7086 lambda_n: 0.9763305817796293 Loss: 0.03808778241670833\n",
      "Iteration: 7087 lambda_n: 0.9885271363155989 Loss: 0.038079680637240834\n",
      "Iteration: 7088 lambda_n: 1.0079659282393165 Loss: 0.03807147763679538\n",
      "Iteration: 7089 lambda_n: 0.9388103206078204 Loss: 0.0380631133172421\n",
      "Iteration: 7090 lambda_n: 0.8855305128308016 Loss: 0.038055322853449175\n",
      "Iteration: 7091 lambda_n: 0.9703243637246305 Loss: 0.03804797450682946\n",
      "Iteration: 7092 lambda_n: 0.9225418264573279 Loss: 0.0380399225103338\n",
      "Iteration: 7093 lambda_n: 0.9389006883235375 Loss: 0.03803226701352383\n",
      "Iteration: 7094 lambda_n: 0.9088468956481471 Loss: 0.038024475755576985\n",
      "Iteration: 7095 lambda_n: 0.9422411941753006 Loss: 0.03801693388099096\n",
      "Iteration: 7096 lambda_n: 0.9317948062161462 Loss: 0.03800911488000359\n",
      "Iteration: 7097 lambda_n: 0.9305884053984442 Loss: 0.038001382554638474\n",
      "Iteration: 7098 lambda_n: 0.9356479068462713 Loss: 0.037993660228860866\n",
      "Iteration: 7099 lambda_n: 0.9623172324293985 Loss: 0.037985895906112246\n",
      "Iteration: 7100 lambda_n: 0.9756993306218212 Loss: 0.03797791026042664\n",
      "Iteration: 7101 lambda_n: 1.0122093056193993 Loss: 0.03796981355280257\n",
      "Iteration: 7102 lambda_n: 0.904171019663486 Loss: 0.03796141385901792\n",
      "Iteration: 7103 lambda_n: 0.9742635190026385 Loss: 0.03795391069402753\n",
      "Iteration: 7104 lambda_n: 1.0040341954697396 Loss: 0.037945825862516824\n",
      "Iteration: 7105 lambda_n: 1.0139844732484191 Loss: 0.037937493968574695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7106 lambda_n: 0.9000070699546573 Loss: 0.03792907948887591\n",
      "Iteration: 7107 lambda_n: 0.9770879344459309 Loss: 0.037921610828835814\n",
      "Iteration: 7108 lambda_n: 1.01940198776878 Loss: 0.03791350250561263\n",
      "Iteration: 7109 lambda_n: 0.9489864527552233 Loss: 0.037905043027132324\n",
      "Iteration: 7110 lambda_n: 1.0223050357440093 Loss: 0.03789716787538504\n",
      "Iteration: 7111 lambda_n: 0.9995468188193717 Loss: 0.037888684276955575\n",
      "Iteration: 7112 lambda_n: 0.9969969430787042 Loss: 0.03788038952245334\n",
      "Iteration: 7113 lambda_n: 0.9173444476125727 Loss: 0.03787211591346515\n",
      "Iteration: 7114 lambda_n: 1.0014357817861235 Loss: 0.03786450328890325\n",
      "Iteration: 7115 lambda_n: 0.9931696246524753 Loss: 0.037856192815701836\n",
      "Iteration: 7116 lambda_n: 0.9274122505802491 Loss: 0.037847950924742524\n",
      "Iteration: 7117 lambda_n: 0.9218686904999779 Loss: 0.03784025471177683\n",
      "Iteration: 7118 lambda_n: 1.0166167945600169 Loss: 0.03783260448957304\n",
      "Iteration: 7119 lambda_n: 1.0123336449582263 Loss: 0.03782416797709945\n",
      "Iteration: 7120 lambda_n: 1.0043526710755653 Loss: 0.03781576699308313\n",
      "Iteration: 7121 lambda_n: 0.9058007088173394 Loss: 0.03780743222454754\n",
      "Iteration: 7122 lambda_n: 0.9291469967106192 Loss: 0.03779991528916714\n",
      "Iteration: 7123 lambda_n: 0.8785544906693408 Loss: 0.03779220459793928\n",
      "Iteration: 7124 lambda_n: 0.8791002653595906 Loss: 0.03778491374450787\n",
      "Iteration: 7125 lambda_n: 0.9727245946265517 Loss: 0.037777618349787816\n",
      "Iteration: 7126 lambda_n: 0.9604864230203042 Loss: 0.037769545981620044\n",
      "Iteration: 7127 lambda_n: 0.9572233426216504 Loss: 0.03776157515976034\n",
      "Iteration: 7128 lambda_n: 0.9894778452369752 Loss: 0.037753631402742294\n",
      "Iteration: 7129 lambda_n: 0.8809953132125757 Loss: 0.03774541995886684\n",
      "Iteration: 7130 lambda_n: 1.0194600531865494 Loss: 0.0377381087712832\n",
      "Iteration: 7131 lambda_n: 0.896396463719649 Loss: 0.03772964848182521\n",
      "Iteration: 7132 lambda_n: 0.9834177429763205 Loss: 0.03772220945617852\n",
      "Iteration: 7133 lambda_n: 0.9577493078340507 Loss: 0.037714048243735296\n",
      "Iteration: 7134 lambda_n: 0.9187774919770757 Loss: 0.037706100033665986\n",
      "Iteration: 7135 lambda_n: 0.9620197551910071 Loss: 0.03769847522990315\n",
      "Iteration: 7136 lambda_n: 0.892433744794946 Loss: 0.03769049155061366\n",
      "Iteration: 7137 lambda_n: 1.024080515331399 Loss: 0.03768308534216535\n",
      "Iteration: 7138 lambda_n: 0.9837270480294832 Loss: 0.03767458659779334\n",
      "Iteration: 7139 lambda_n: 0.9939807443321921 Loss: 0.037666422725938765\n",
      "Iteration: 7140 lambda_n: 0.9157843439358313 Loss: 0.03765817374332593\n",
      "Iteration: 7141 lambda_n: 0.9090224179643219 Loss: 0.03765057369192704\n",
      "Iteration: 7142 lambda_n: 0.8885585640086459 Loss: 0.0376430297434786\n",
      "Iteration: 7143 lambda_n: 0.9403337188131438 Loss: 0.03763565561023578\n",
      "Iteration: 7144 lambda_n: 0.9163619448731364 Loss: 0.037627851782133355\n",
      "Iteration: 7145 lambda_n: 0.8982217842759382 Loss: 0.03762024688105226\n",
      "Iteration: 7146 lambda_n: 0.9150154407516845 Loss: 0.0376127925113871\n",
      "Iteration: 7147 lambda_n: 0.8873649584939389 Loss: 0.037605198756834594\n",
      "Iteration: 7148 lambda_n: 1.0237568074888204 Loss: 0.037597834460898365\n",
      "Iteration: 7149 lambda_n: 0.8863778100098233 Loss: 0.037589338226200884\n",
      "Iteration: 7150 lambda_n: 1.0122220812626412 Loss: 0.037581982093731835\n",
      "Iteration: 7151 lambda_n: 0.8807166953662475 Loss: 0.037573581553389065\n",
      "Iteration: 7152 lambda_n: 0.9787211195096812 Loss: 0.03756627237428969\n",
      "Iteration: 7153 lambda_n: 0.979442445770691 Loss: 0.03755814982990903\n",
      "Iteration: 7154 lambda_n: 1.0057738735118966 Loss: 0.03755002128257219\n",
      "Iteration: 7155 lambda_n: 1.012612220550177 Loss: 0.037541674189671206\n",
      "Iteration: 7156 lambda_n: 0.8842011124164245 Loss: 0.03753327032648139\n",
      "Iteration: 7157 lambda_n: 1.020047874547932 Loss: 0.03752593215535351\n",
      "Iteration: 7158 lambda_n: 0.9777914242846566 Loss: 0.03751746654854796\n",
      "Iteration: 7159 lambda_n: 0.8908801672055973 Loss: 0.03750935161979317\n",
      "Iteration: 7160 lambda_n: 0.8921031409318335 Loss: 0.03750195797289411\n",
      "Iteration: 7161 lambda_n: 0.9475789813855985 Loss: 0.037494554162257325\n",
      "Iteration: 7162 lambda_n: 1.022432572546229 Loss: 0.03748668992774066\n",
      "Iteration: 7163 lambda_n: 0.9917611147477968 Loss: 0.03747820444482472\n",
      "Iteration: 7164 lambda_n: 0.9184026766618727 Loss: 0.03746997349565003\n",
      "Iteration: 7165 lambda_n: 0.9555312014925715 Loss: 0.037462351355517086\n",
      "Iteration: 7166 lambda_n: 0.8873681224289894 Loss: 0.037454421057613686\n",
      "Iteration: 7167 lambda_n: 0.9163159100052388 Loss: 0.03744705645420841\n",
      "Iteration: 7168 lambda_n: 0.9839298907338829 Loss: 0.037439451587853737\n",
      "Iteration: 7169 lambda_n: 0.9296992766208733 Loss: 0.03743128555078585\n",
      "Iteration: 7170 lambda_n: 0.9316775978216578 Loss: 0.03742356957911815\n",
      "Iteration: 7171 lambda_n: 0.9271832053412203 Loss: 0.03741583717301401\n",
      "Iteration: 7172 lambda_n: 0.9699079735473514 Loss: 0.03740814205234579\n",
      "Iteration: 7173 lambda_n: 1.0277701072926337 Loss: 0.03740009232323129\n",
      "Iteration: 7174 lambda_n: 0.9448601169129295 Loss: 0.03739156235107215\n",
      "Iteration: 7175 lambda_n: 0.9932568167008048 Loss: 0.037383720471927714\n",
      "Iteration: 7176 lambda_n: 0.8963592677733466 Loss: 0.037375476907069806\n",
      "Iteration: 7177 lambda_n: 0.9409237523661147 Loss: 0.03736803752972641\n",
      "Iteration: 7178 lambda_n: 0.9180783125631268 Loss: 0.03736022827225433\n",
      "Iteration: 7179 lambda_n: 0.8812058093563755 Loss: 0.037352608606205186\n",
      "Iteration: 7180 lambda_n: 0.8783269299154473 Loss: 0.03734529495164302\n",
      "Iteration: 7181 lambda_n: 0.9083622332734678 Loss: 0.03733800517654929\n",
      "Iteration: 7182 lambda_n: 0.8802561791470103 Loss: 0.03733046610565484\n",
      "Iteration: 7183 lambda_n: 0.9812333402131965 Loss: 0.03732316028992445\n",
      "Iteration: 7184 lambda_n: 0.9172168103834946 Loss: 0.03731501638419363\n",
      "Iteration: 7185 lambda_n: 0.9733839830305105 Loss: 0.03730740377731058\n",
      "Iteration: 7186 lambda_n: 0.9515727645264126 Loss: 0.03729932498480597\n",
      "Iteration: 7187 lambda_n: 0.9172558417060294 Loss: 0.037291427201758084\n",
      "Iteration: 7188 lambda_n: 1.0177532840190764 Loss: 0.03728381422324853\n",
      "Iteration: 7189 lambda_n: 0.9825045551027501 Loss: 0.03727536712625495\n",
      "Iteration: 7190 lambda_n: 0.8810170423581981 Loss: 0.037267212566338324\n",
      "Iteration: 7191 lambda_n: 0.8934235209848609 Loss: 0.03725990031286815\n",
      "Iteration: 7192 lambda_n: 0.9138107992144737 Loss: 0.03725248507387202\n",
      "Iteration: 7193 lambda_n: 1.0297706151918444 Loss: 0.037244900609576456\n",
      "Iteration: 7194 lambda_n: 0.8831407272962621 Loss: 0.03723635368309684\n",
      "Iteration: 7195 lambda_n: 1.0096782462947058 Loss: 0.03722902374311176\n",
      "Iteration: 7196 lambda_n: 0.9070382028949829 Loss: 0.0372206435442934\n",
      "Iteration: 7197 lambda_n: 1.0180170987565231 Loss: 0.03721311522719029\n",
      "Iteration: 7198 lambda_n: 1.0231507732586727 Loss: 0.0372046657810195\n",
      "Iteration: 7199 lambda_n: 0.8884460845455996 Loss: 0.03719617370662295\n",
      "Iteration: 7200 lambda_n: 0.9780446404406612 Loss: 0.03718879965360873\n",
      "Iteration: 7201 lambda_n: 0.992751389720631 Loss: 0.03718068192194594\n",
      "Iteration: 7202 lambda_n: 1.0170014685509343 Loss: 0.037172442106961655\n",
      "Iteration: 7203 lambda_n: 0.9255168128560082 Loss: 0.037164000998285845\n",
      "Iteration: 7204 lambda_n: 1.0235467613213818 Loss: 0.037156319194100285\n",
      "Iteration: 7205 lambda_n: 0.94679108179634 Loss: 0.03714782372273163\n",
      "Iteration: 7206 lambda_n: 1.0105776884401882 Loss: 0.03713996530769399\n",
      "Iteration: 7207 lambda_n: 0.92799974773059 Loss: 0.03713157744308935\n",
      "Iteration: 7208 lambda_n: 0.9600911921858587 Loss: 0.0371238749633488\n",
      "Iteration: 7209 lambda_n: 0.9955653433837327 Loss: 0.03711590610541355\n",
      "Iteration: 7210 lambda_n: 0.962800382963286 Loss: 0.037107642790724926\n",
      "Iteration: 7211 lambda_n: 0.9198883107738507 Loss: 0.037099651411305834\n",
      "Iteration: 7212 lambda_n: 0.9426743276453939 Loss: 0.03709201619154592\n",
      "Iteration: 7213 lambda_n: 0.9145800596334425 Loss: 0.03708419182821758\n",
      "Iteration: 7214 lambda_n: 0.9082358799242526 Loss: 0.03707660063615861\n",
      "Iteration: 7215 lambda_n: 0.9391876668442646 Loss: 0.0370690620865825\n",
      "Iteration: 7216 lambda_n: 0.9922572272511673 Loss: 0.037061266614937374\n",
      "Iteration: 7217 lambda_n: 0.9485256234246467 Loss: 0.03705303063678112\n",
      "Iteration: 7218 lambda_n: 0.9483312133184058 Loss: 0.03704515762400102\n",
      "Iteration: 7219 lambda_n: 1.0212747290033288 Loss: 0.037037286208179046\n",
      "Iteration: 7220 lambda_n: 0.9080682379390814 Loss: 0.03702880932300155\n",
      "Iteration: 7221 lambda_n: 0.9339411989839684 Loss: 0.03702127206784864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7222 lambda_n: 0.9827093862041031 Loss: 0.037013520043257336\n",
      "Iteration: 7223 lambda_n: 1.0242968473705576 Loss: 0.03700536320955865\n",
      "Iteration: 7224 lambda_n: 0.9937610227754468 Loss: 0.036996861166798976\n",
      "Iteration: 7225 lambda_n: 0.8947062064611471 Loss: 0.03698861256366957\n",
      "Iteration: 7226 lambda_n: 0.9982521806591514 Loss: 0.03698118613720247\n",
      "Iteration: 7227 lambda_n: 0.8773592539524045 Loss: 0.03697290022077624\n",
      "Iteration: 7228 lambda_n: 0.9077588276674792 Loss: 0.03696561775024612\n",
      "Iteration: 7229 lambda_n: 0.9153509619552179 Loss: 0.03695808293514183\n",
      "Iteration: 7230 lambda_n: 0.9588978701663198 Loss: 0.03695048508647451\n",
      "Iteration: 7231 lambda_n: 0.9987580098376678 Loss: 0.036942525761654146\n",
      "Iteration: 7232 lambda_n: 0.8919949274631768 Loss: 0.0369342355604594\n",
      "Iteration: 7233 lambda_n: 1.0192086792066934 Loss: 0.036926831530477704\n",
      "Iteration: 7234 lambda_n: 0.8912390341403074 Loss: 0.03691837154271514\n",
      "Iteration: 7235 lambda_n: 0.9278288576899698 Loss: 0.03691097375554877\n",
      "Iteration: 7236 lambda_n: 0.8933539547086021 Loss: 0.03690327223703869\n",
      "Iteration: 7237 lambda_n: 0.9637744401078299 Loss: 0.036895856864824834\n",
      "Iteration: 7238 lambda_n: 0.9282005171131537 Loss: 0.03688785694481753\n",
      "Iteration: 7239 lambda_n: 0.9383477639801184 Loss: 0.03688015229359683\n",
      "Iteration: 7240 lambda_n: 0.9966775275341265 Loss: 0.03687236339778298\n",
      "Iteration: 7241 lambda_n: 0.9773426041620903 Loss: 0.036864090310024855\n",
      "Iteration: 7242 lambda_n: 0.9528676158779248 Loss: 0.036855977697000865\n",
      "Iteration: 7243 lambda_n: 0.8794854058667055 Loss: 0.036848068225912446\n",
      "Iteration: 7244 lambda_n: 0.953088313703844 Loss: 0.03684076786310678\n",
      "Iteration: 7245 lambda_n: 0.9631045973549928 Loss: 0.03683285652795198\n",
      "Iteration: 7246 lambda_n: 0.8969901164057901 Loss: 0.03682486203343041\n",
      "Iteration: 7247 lambda_n: 0.9843355855907151 Loss: 0.03681741632289328\n",
      "Iteration: 7248 lambda_n: 0.915302152676688 Loss: 0.036809245561620865\n",
      "Iteration: 7249 lambda_n: 0.9176518402384783 Loss: 0.036801647815519194\n",
      "Iteration: 7250 lambda_n: 1.0069379312932112 Loss: 0.036794030549735665\n",
      "Iteration: 7251 lambda_n: 1.0092531474925845 Loss: 0.03678567211927512\n",
      "Iteration: 7252 lambda_n: 0.9318498430492203 Loss: 0.036777294451989344\n",
      "Iteration: 7253 lambda_n: 0.9622975410846721 Loss: 0.03676955928112688\n",
      "Iteration: 7254 lambda_n: 0.9223950153233071 Loss: 0.036761571351392484\n",
      "Iteration: 7255 lambda_n: 1.0148978064890097 Loss: 0.036753914631971765\n",
      "Iteration: 7256 lambda_n: 0.9982795101908503 Loss: 0.036745490038304955\n",
      "Iteration: 7257 lambda_n: 0.8854291678593665 Loss: 0.036737203373409036\n",
      "Iteration: 7258 lambda_n: 0.9930255178989236 Loss: 0.036729853434097876\n",
      "Iteration: 7259 lambda_n: 0.9092560198245283 Loss: 0.0367216103325103\n",
      "Iteration: 7260 lambda_n: 0.9865328497862037 Loss: 0.03671406259106336\n",
      "Iteration: 7261 lambda_n: 1.013899207915391 Loss: 0.03670587336184289\n",
      "Iteration: 7262 lambda_n: 0.9468763043816175 Loss: 0.036697456948547\n",
      "Iteration: 7263 lambda_n: 0.9648756807926594 Loss: 0.03668959687864464\n",
      "Iteration: 7264 lambda_n: 0.8845487109299583 Loss: 0.03668158737926455\n",
      "Iteration: 7265 lambda_n: 0.9370897978492915 Loss: 0.03667424466433336\n",
      "Iteration: 7266 lambda_n: 1.013885535255347 Loss: 0.03666646578669035\n",
      "Iteration: 7267 lambda_n: 0.9008726883362693 Loss: 0.03665804940301783\n",
      "Iteration: 7268 lambda_n: 0.9991012961461763 Loss: 0.03665057113560642\n",
      "Iteration: 7269 lambda_n: 0.9677135227233753 Loss: 0.03664227744315974\n",
      "Iteration: 7270 lambda_n: 0.8918407244613493 Loss: 0.0366342442879062\n",
      "Iteration: 7271 lambda_n: 1.0073975133503517 Loss: 0.03662684094998091\n",
      "Iteration: 7272 lambda_n: 0.909226452526822 Loss: 0.03661847833755919\n",
      "Iteration: 7273 lambda_n: 0.9532615031161072 Loss: 0.03661093064645448\n",
      "Iteration: 7274 lambda_n: 1.0155339565410801 Loss: 0.036603017395180404\n",
      "Iteration: 7275 lambda_n: 1.0036983056346147 Loss: 0.03659458718825671\n",
      "Iteration: 7276 lambda_n: 1.019993148332869 Loss: 0.03658625521384451\n",
      "Iteration: 7277 lambda_n: 0.9118419909401926 Loss: 0.03657778795325984\n",
      "Iteration: 7278 lambda_n: 0.9823367807049397 Loss: 0.036570218470132504\n",
      "Iteration: 7279 lambda_n: 0.9259107704179652 Loss: 0.03656206377215425\n",
      "Iteration: 7280 lambda_n: 1.017058081165942 Loss: 0.03655437746859393\n",
      "Iteration: 7281 lambda_n: 0.9710022179826748 Loss: 0.03654593450342782\n",
      "Iteration: 7282 lambda_n: 0.9125025139920064 Loss: 0.03653787384694883\n",
      "Iteration: 7283 lambda_n: 0.9069478877751727 Loss: 0.036530298802821236\n",
      "Iteration: 7284 lambda_n: 1.0193481007588703 Loss: 0.03652276985522042\n",
      "Iteration: 7285 lambda_n: 1.0162962946046323 Loss: 0.03651430781120313\n",
      "Iteration: 7286 lambda_n: 0.876693480175709 Loss: 0.03650587108328455\n",
      "Iteration: 7287 lambda_n: 0.9481854783394937 Loss: 0.03649859324449483\n",
      "Iteration: 7288 lambda_n: 0.9382700532861984 Loss: 0.036490721903089815\n",
      "Iteration: 7289 lambda_n: 0.9141492579329737 Loss: 0.03648293285875959\n",
      "Iteration: 7290 lambda_n: 0.9831544798716364 Loss: 0.03647534403803324\n",
      "Iteration: 7291 lambda_n: 1.0229103044333498 Loss: 0.036467182354118576\n",
      "Iteration: 7292 lambda_n: 0.9997659258692885 Loss: 0.03645869061868779\n",
      "Iteration: 7293 lambda_n: 0.9400626105733655 Loss: 0.036450390999473524\n",
      "Iteration: 7294 lambda_n: 0.9593421644660427 Loss: 0.036442586994575976\n",
      "Iteration: 7295 lambda_n: 0.9999670759446784 Loss: 0.03643462292330881\n",
      "Iteration: 7296 lambda_n: 1.0025316313651125 Loss: 0.03642632158389855\n",
      "Iteration: 7297 lambda_n: 0.9970398463354514 Loss: 0.036417998937197285\n",
      "Iteration: 7298 lambda_n: 0.9495847984005082 Loss: 0.036409721863979436\n",
      "Iteration: 7299 lambda_n: 0.9376546898660699 Loss: 0.03640183872940001\n",
      "Iteration: 7300 lambda_n: 0.9587668924154759 Loss: 0.036394054619224944\n",
      "Iteration: 7301 lambda_n: 0.8914508907098465 Loss: 0.03638609522687115\n",
      "Iteration: 7302 lambda_n: 1.0280667804007497 Loss: 0.036378694656771106\n",
      "Iteration: 7303 lambda_n: 0.8781115235209217 Loss: 0.036370159925279553\n",
      "Iteration: 7304 lambda_n: 1.0144568329153898 Loss: 0.036362870065908826\n",
      "Iteration: 7305 lambda_n: 0.9506100483304047 Loss: 0.036354448287425144\n",
      "Iteration: 7306 lambda_n: 1.0192869024487567 Loss: 0.03634655653313221\n",
      "Iteration: 7307 lambda_n: 1.006632790673388 Loss: 0.036338094622456095\n",
      "Iteration: 7308 lambda_n: 0.9828929433752064 Loss: 0.03632973774615976\n",
      "Iteration: 7309 lambda_n: 1.0246331382939509 Loss: 0.03632157793678806\n",
      "Iteration: 7310 lambda_n: 0.9020963700673444 Loss: 0.036313071590492046\n",
      "Iteration: 7311 lambda_n: 0.9428790273089624 Loss: 0.03630558250969951\n",
      "Iteration: 7312 lambda_n: 0.9989674768256691 Loss: 0.03629775484260094\n",
      "Iteration: 7313 lambda_n: 0.8800362156479611 Loss: 0.03628946152027412\n",
      "Iteration: 7314 lambda_n: 0.9585487586174202 Loss: 0.036282155537711094\n",
      "Iteration: 7315 lambda_n: 0.992447212703257 Loss: 0.03627419773699443\n",
      "Iteration: 7316 lambda_n: 0.8768406372132576 Loss: 0.03626595849796861\n",
      "Iteration: 7317 lambda_n: 0.8998328789178406 Loss: 0.036258679003262295\n",
      "Iteration: 7318 lambda_n: 0.9255030986058709 Loss: 0.036251208614801717\n",
      "Iteration: 7319 lambda_n: 0.8996992858419428 Loss: 0.036243525099049295\n",
      "Iteration: 7320 lambda_n: 0.9578507420531756 Loss: 0.03623605579236246\n",
      "Iteration: 7321 lambda_n: 0.8837567705141515 Loss: 0.036228103697893976\n",
      "Iteration: 7322 lambda_n: 0.9647054672935615 Loss: 0.03622076671879674\n",
      "Iteration: 7323 lambda_n: 0.8818743216183266 Loss: 0.03621275768689837\n",
      "Iteration: 7324 lambda_n: 0.8877461627665093 Loss: 0.036205436309043305\n",
      "Iteration: 7325 lambda_n: 0.9380283928458716 Loss: 0.03619806616990497\n",
      "Iteration: 7326 lambda_n: 0.9309334897878414 Loss: 0.036190278570105434\n",
      "Iteration: 7327 lambda_n: 0.9555796111119674 Loss: 0.03618254985852159\n",
      "Iteration: 7328 lambda_n: 0.9930682987497798 Loss: 0.03617461651762538\n",
      "Iteration: 7329 lambda_n: 1.0294302267436142 Loss: 0.03616637192554018\n",
      "Iteration: 7330 lambda_n: 0.9348356629156773 Loss: 0.0361578254350335\n",
      "Iteration: 7331 lambda_n: 0.9373658972283606 Loss: 0.036150064267547195\n",
      "Iteration: 7332 lambda_n: 0.9083507701875344 Loss: 0.03614228207939447\n",
      "Iteration: 7333 lambda_n: 0.9372590173530942 Loss: 0.036134740766445426\n",
      "Iteration: 7334 lambda_n: 0.993316179837211 Loss: 0.03612695943762644\n",
      "Iteration: 7335 lambda_n: 0.9433379263380436 Loss: 0.036118712695087624\n",
      "Iteration: 7336 lambda_n: 0.9909874086549093 Loss: 0.03611088086848727\n",
      "Iteration: 7337 lambda_n: 0.9278216518932966 Loss: 0.036102653429064345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7338 lambda_n: 0.9643358147319298 Loss: 0.036094950393603144\n",
      "Iteration: 7339 lambda_n: 0.9594498204946893 Loss: 0.036086944193066774\n",
      "Iteration: 7340 lambda_n: 0.8852177555274574 Loss: 0.03607897854271157\n",
      "Iteration: 7341 lambda_n: 1.0245728299946548 Loss: 0.036071629176366665\n",
      "Iteration: 7342 lambda_n: 0.9875944223608677 Loss: 0.03606312282458525\n",
      "Iteration: 7343 lambda_n: 0.939008212734459 Loss: 0.036054923463988316\n",
      "Iteration: 7344 lambda_n: 0.9046238103668761 Loss: 0.03604712746864737\n",
      "Iteration: 7345 lambda_n: 0.8957811130660304 Loss: 0.036039616931930404\n",
      "Iteration: 7346 lambda_n: 0.9680732197304966 Loss: 0.03603217979791973\n",
      "Iteration: 7347 lambda_n: 0.9674843742213907 Loss: 0.03602414245232031\n",
      "Iteration: 7348 lambda_n: 0.9143922923156338 Loss: 0.03601610998085527\n",
      "Iteration: 7349 lambda_n: 0.9644125224621871 Loss: 0.0360085182887593\n",
      "Iteration: 7350 lambda_n: 0.9964365914973631 Loss: 0.03600051129272596\n",
      "Iteration: 7351 lambda_n: 0.8932822408032234 Loss: 0.035992238403213854\n",
      "Iteration: 7352 lambda_n: 1.0035027607998501 Loss: 0.03598482193608823\n",
      "Iteration: 7353 lambda_n: 0.9583210650434962 Loss: 0.0359764903504191\n",
      "Iteration: 7354 lambda_n: 1.02710468583546 Loss: 0.035968533870994995\n",
      "Iteration: 7355 lambda_n: 1.0169531746277591 Loss: 0.035960006299104556\n",
      "Iteration: 7356 lambda_n: 1.0002726575674907 Loss: 0.035951562994361384\n",
      "Iteration: 7357 lambda_n: 0.9123425666289907 Loss: 0.035943258164775414\n",
      "Iteration: 7358 lambda_n: 1.0154286849277623 Loss: 0.035935683366437605\n",
      "Iteration: 7359 lambda_n: 0.9991922108619948 Loss: 0.03592725267319269\n",
      "Iteration: 7360 lambda_n: 0.8938996276690009 Loss: 0.03591895676929751\n",
      "Iteration: 7361 lambda_n: 1.0223854622464592 Loss: 0.03591153505498327\n",
      "Iteration: 7362 lambda_n: 0.9334851930404362 Loss: 0.03590304655713149\n",
      "Iteration: 7363 lambda_n: 0.9060460270942622 Loss: 0.03589529615157009\n",
      "Iteration: 7364 lambda_n: 0.9105084375907251 Loss: 0.0358877735511111\n",
      "Iteration: 7365 lambda_n: 1.0131637664045743 Loss: 0.03588021388829044\n",
      "Iteration: 7366 lambda_n: 0.9442287727931934 Loss: 0.035871801897080634\n",
      "Iteration: 7367 lambda_n: 0.9760858589374258 Loss: 0.03586396223773625\n",
      "Iteration: 7368 lambda_n: 0.9798366773794552 Loss: 0.035855858064418517\n",
      "Iteration: 7369 lambda_n: 0.8957534993250824 Loss: 0.035847722734789866\n",
      "Iteration: 7370 lambda_n: 0.8916943958903618 Loss: 0.03584028551277692\n",
      "Iteration: 7371 lambda_n: 0.9194496552656177 Loss: 0.0358328819806202\n",
      "Iteration: 7372 lambda_n: 0.9749704439756341 Loss: 0.03582524799083533\n",
      "Iteration: 7373 lambda_n: 0.8936616836664851 Loss: 0.03581715301085673\n",
      "Iteration: 7374 lambda_n: 0.9295313764575993 Loss: 0.03580973310790185\n",
      "Iteration: 7375 lambda_n: 0.9973476641083587 Loss: 0.035802015373512956\n",
      "Iteration: 7376 lambda_n: 0.8846135832600103 Loss: 0.03579373455895364\n",
      "Iteration: 7377 lambda_n: 0.994160803736047 Loss: 0.03578638974396582\n",
      "Iteration: 7378 lambda_n: 0.9333308688747383 Loss: 0.035778135361972545\n",
      "Iteration: 7379 lambda_n: 1.026232778699909 Loss: 0.035770386029043956\n",
      "Iteration: 7380 lambda_n: 0.876309693068762 Loss: 0.03576186532900565\n",
      "Iteration: 7381 lambda_n: 0.8958454488276604 Loss: 0.03575458941095073\n",
      "Iteration: 7382 lambda_n: 0.944016633548992 Loss: 0.03574715127798049\n",
      "Iteration: 7383 lambda_n: 0.9411034666818792 Loss: 0.03573931317135825\n",
      "Iteration: 7384 lambda_n: 0.9506455978293576 Loss: 0.03573149923975365\n",
      "Iteration: 7385 lambda_n: 0.8984822330036561 Loss: 0.03572360606749119\n",
      "Iteration: 7386 lambda_n: 0.9711586585981415 Loss: 0.03571614599324748\n",
      "Iteration: 7387 lambda_n: 0.9481391621980132 Loss: 0.035708082476159655\n",
      "Iteration: 7388 lambda_n: 0.9179873793013955 Loss: 0.03570021007646754\n",
      "Iteration: 7389 lambda_n: 0.9438122689732592 Loss: 0.03569258801461319\n",
      "Iteration: 7390 lambda_n: 0.9006222758470295 Loss: 0.035684751516068806\n",
      "Iteration: 7391 lambda_n: 0.9967335131197884 Loss: 0.035677273613109656\n",
      "Iteration: 7392 lambda_n: 0.8907533923643054 Loss: 0.03566899768210768\n",
      "Iteration: 7393 lambda_n: 0.9489666252965367 Loss: 0.035661601697026875\n",
      "Iteration: 7394 lambda_n: 0.9350058235820183 Loss: 0.03565372235175315\n",
      "Iteration: 7395 lambda_n: 0.9457286538237308 Loss: 0.03564595891166886\n",
      "Iteration: 7396 lambda_n: 0.9504698892621656 Loss: 0.03563810642657414\n",
      "Iteration: 7397 lambda_n: 0.9082615981444877 Loss: 0.03563021456196495\n",
      "Iteration: 7398 lambda_n: 0.9518362042465176 Loss: 0.03562267314576787\n",
      "Iteration: 7399 lambda_n: 1.010893423276545 Loss: 0.035614769911898936\n",
      "Iteration: 7400 lambda_n: 0.9151129633067945 Loss: 0.03560637630412907\n",
      "Iteration: 7401 lambda_n: 1.0287288641326193 Loss: 0.03559877796378373\n",
      "Iteration: 7402 lambda_n: 0.9099030142245962 Loss: 0.03559023623823011\n",
      "Iteration: 7403 lambda_n: 1.0146748414090379 Loss: 0.03558268113266544\n",
      "Iteration: 7404 lambda_n: 1.0013066138857962 Loss: 0.03557425607326268\n",
      "Iteration: 7405 lambda_n: 0.9645750190612731 Loss: 0.0355659419991838\n",
      "Iteration: 7406 lambda_n: 1.024206979675098 Loss: 0.03555793290261021\n",
      "Iteration: 7407 lambda_n: 0.9820799310083863 Loss: 0.03554942865427981\n",
      "Iteration: 7408 lambda_n: 1.0206356379154942 Loss: 0.03554127418377242\n",
      "Iteration: 7409 lambda_n: 0.9900536344633303 Loss: 0.03553279956149155\n",
      "Iteration: 7410 lambda_n: 0.8775239516533604 Loss: 0.035524578856473955\n",
      "Iteration: 7411 lambda_n: 1.0129910616820987 Loss: 0.0355172925065783\n",
      "Iteration: 7412 lambda_n: 0.8812906290499446 Loss: 0.035508881320042734\n",
      "Iteration: 7413 lambda_n: 0.8954275622383864 Loss: 0.035501563671997034\n",
      "Iteration: 7414 lambda_n: 0.9591826819118028 Loss: 0.03549412862981504\n",
      "Iteration: 7415 lambda_n: 0.9761675706070831 Loss: 0.03548616419565417\n",
      "Iteration: 7416 lambda_n: 0.9147627126259172 Loss: 0.03547805871754457\n",
      "Iteration: 7417 lambda_n: 0.9043741112231882 Loss: 0.03547046309468883\n",
      "Iteration: 7418 lambda_n: 0.9741447893659265 Loss: 0.03546295372143503\n",
      "Iteration: 7419 lambda_n: 0.9448163852834276 Loss: 0.03545486500333687\n",
      "Iteration: 7420 lambda_n: 1.0239004304759352 Loss: 0.03544701979874943\n",
      "Iteration: 7421 lambda_n: 1.0289513122560447 Loss: 0.03543851791377229\n",
      "Iteration: 7422 lambda_n: 1.0140657923936116 Loss: 0.035429974075427154\n",
      "Iteration: 7423 lambda_n: 1.0048894254102785 Loss: 0.035421553824571884\n",
      "Iteration: 7424 lambda_n: 0.9809406287778134 Loss: 0.03541320975606745\n",
      "Iteration: 7425 lambda_n: 0.9171822896931175 Loss: 0.0354050645329081\n",
      "Iteration: 7426 lambda_n: 0.9759800378903144 Loss: 0.03539744871435351\n",
      "Iteration: 7427 lambda_n: 1.0070551191654047 Loss: 0.03538934465754494\n",
      "Iteration: 7428 lambda_n: 0.983451528325057 Loss: 0.03538098255602924\n",
      "Iteration: 7429 lambda_n: 0.8785819174516214 Loss: 0.03537281643471726\n",
      "Iteration: 7430 lambda_n: 0.9419607818227508 Loss: 0.035365521090507765\n",
      "Iteration: 7431 lambda_n: 0.9646274191457221 Loss: 0.03535769946661037\n",
      "Iteration: 7432 lambda_n: 1.002332131341114 Loss: 0.03534968961753984\n",
      "Iteration: 7433 lambda_n: 0.9662132801374363 Loss: 0.03534136667266315\n",
      "Iteration: 7434 lambda_n: 0.9077682001942583 Loss: 0.03533334363133196\n",
      "Iteration: 7435 lambda_n: 0.890681641067064 Loss: 0.03532580588307804\n",
      "Iteration: 7436 lambda_n: 0.917800625716064 Loss: 0.035318410004727116\n",
      "Iteration: 7437 lambda_n: 0.9331833582780031 Loss: 0.03531078893055914\n",
      "Iteration: 7438 lambda_n: 0.9409907370850414 Loss: 0.03530304011326785\n",
      "Iteration: 7439 lambda_n: 0.9700142122871935 Loss: 0.035295226455432496\n",
      "Iteration: 7440 lambda_n: 0.9713316473496844 Loss: 0.035287171785539126\n",
      "Iteration: 7441 lambda_n: 0.8822064245159487 Loss: 0.035279106164484056\n",
      "Iteration: 7442 lambda_n: 0.8819227578727052 Loss: 0.03527178059959578\n",
      "Iteration: 7443 lambda_n: 0.997672132541141 Loss: 0.03526445738064195\n",
      "Iteration: 7444 lambda_n: 0.9878050041347278 Loss: 0.03525617300332644\n",
      "Iteration: 7445 lambda_n: 1.0167992724137294 Loss: 0.035247970539475454\n",
      "Iteration: 7446 lambda_n: 0.9182404754603396 Loss: 0.03523952730651595\n",
      "Iteration: 7447 lambda_n: 0.9625693061638666 Loss: 0.035231902470459764\n",
      "Iteration: 7448 lambda_n: 0.9606946618123615 Loss: 0.035223909529514416\n",
      "Iteration: 7449 lambda_n: 0.8908273973131047 Loss: 0.03521593214474137\n",
      "Iteration: 7450 lambda_n: 0.9934120213103518 Loss: 0.035208534911529435\n",
      "Iteration: 7451 lambda_n: 0.9185398050549218 Loss: 0.03520028582803855\n",
      "Iteration: 7452 lambda_n: 0.9359497250713392 Loss: 0.03519265845679042\n",
      "Iteration: 7453 lambda_n: 0.9302831627737292 Loss: 0.035184886506868226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7454 lambda_n: 0.9429359538930524 Loss: 0.03517716160068695\n",
      "Iteration: 7455 lambda_n: 0.9749736273171437 Loss: 0.03516933161759607\n",
      "Iteration: 7456 lambda_n: 0.9509342533413861 Loss: 0.03516123558815644\n",
      "Iteration: 7457 lambda_n: 0.981059740107294 Loss: 0.035153339166994514\n",
      "Iteration: 7458 lambda_n: 0.9290836067440364 Loss: 0.035145192577177874\n",
      "Iteration: 7459 lambda_n: 0.9885506678449361 Loss: 0.035137477579555654\n",
      "Iteration: 7460 lambda_n: 0.9207181233580423 Loss: 0.035129268763974854\n",
      "Iteration: 7461 lambda_n: 0.9919440190895439 Loss: 0.035121623211710494\n",
      "Iteration: 7462 lambda_n: 0.9797055354763816 Loss: 0.035113386196091376\n",
      "Iteration: 7463 lambda_n: 0.8902062059875803 Loss: 0.03510525079650386\n",
      "Iteration: 7464 lambda_n: 0.956955274804161 Loss: 0.03509785858236884\n",
      "Iteration: 7465 lambda_n: 1.0177652662984686 Loss: 0.03508991207857364\n",
      "Iteration: 7466 lambda_n: 0.9173929345942083 Loss: 0.03508146060083486\n",
      "Iteration: 7467 lambda_n: 0.9174035842053796 Loss: 0.03507384259976834\n",
      "Iteration: 7468 lambda_n: 0.9002336098493545 Loss: 0.03506622450067992\n",
      "Iteration: 7469 lambda_n: 0.9700752403808651 Loss: 0.03505874897128933\n",
      "Iteration: 7470 lambda_n: 0.9170002166622016 Loss: 0.03505069346796462\n",
      "Iteration: 7471 lambda_n: 0.9660828536270905 Loss: 0.03504307868948292\n",
      "Iteration: 7472 lambda_n: 0.9621015555943708 Loss: 0.03503505631837521\n",
      "Iteration: 7473 lambda_n: 0.9852377206700788 Loss: 0.03502706699761144\n",
      "Iteration: 7474 lambda_n: 0.9603273660756205 Loss: 0.03501888554283299\n",
      "Iteration: 7475 lambda_n: 0.9099976409613424 Loss: 0.03501091093410317\n",
      "Iteration: 7476 lambda_n: 1.0199637536180755 Loss: 0.035003354256304714\n",
      "Iteration: 7477 lambda_n: 0.9048185280551946 Loss: 0.03499488440274874\n",
      "Iteration: 7478 lambda_n: 0.963078486735207 Loss: 0.034987370713305245\n",
      "Iteration: 7479 lambda_n: 0.9266038613029203 Loss: 0.03497937321868081\n",
      "Iteration: 7480 lambda_n: 0.9283512734684218 Loss: 0.03497167860296979\n",
      "Iteration: 7481 lambda_n: 0.9531047097177937 Loss: 0.03496396946714968\n",
      "Iteration: 7482 lambda_n: 0.8815465966441249 Loss: 0.03495605476630271\n",
      "Iteration: 7483 lambda_n: 0.909051786280234 Loss: 0.034948734283811116\n",
      "Iteration: 7484 lambda_n: 0.9068518910130932 Loss: 0.03494118538581649\n",
      "Iteration: 7485 lambda_n: 0.9744304910611961 Loss: 0.034933654747155096\n",
      "Iteration: 7486 lambda_n: 0.9385562794248162 Loss: 0.034925562915830835\n",
      "Iteration: 7487 lambda_n: 0.9287852323812665 Loss: 0.034917768980020074\n",
      "Iteration: 7488 lambda_n: 0.9493711956401973 Loss: 0.034910056175349966\n",
      "Iteration: 7489 lambda_n: 0.8909832760948952 Loss: 0.034902172411616465\n",
      "Iteration: 7490 lambda_n: 0.99969564201559 Loss: 0.034894773503529264\n",
      "Iteration: 7491 lambda_n: 0.8816372535105123 Loss: 0.03488647181622947\n",
      "Iteration: 7492 lambda_n: 0.9746840012159442 Loss: 0.03487915050175893\n",
      "Iteration: 7493 lambda_n: 0.9684424034313136 Loss: 0.03487105649710158\n",
      "Iteration: 7494 lambda_n: 0.9571640595134492 Loss: 0.034863014314203244\n",
      "Iteration: 7495 lambda_n: 0.943020272966043 Loss: 0.03485506577970915\n",
      "Iteration: 7496 lambda_n: 0.9363234425205984 Loss: 0.03484723468938483\n",
      "Iteration: 7497 lambda_n: 1.0101369471028898 Loss: 0.03483945920211265\n",
      "Iteration: 7498 lambda_n: 0.9752659581765961 Loss: 0.034831070737417764\n",
      "Iteration: 7499 lambda_n: 0.920110311382461 Loss: 0.03482297184112209\n",
      "Iteration: 7500 lambda_n: 0.8899017601409426 Loss: 0.034815330964308396\n",
      "Iteration: 7501 lambda_n: 0.9045591559511507 Loss: 0.03480794094008847\n",
      "Iteration: 7502 lambda_n: 0.8898206910289072 Loss: 0.03480042918799382\n",
      "Iteration: 7503 lambda_n: 0.9830567323310729 Loss: 0.03479303982064892\n",
      "Iteration: 7504 lambda_n: 1.005711968721315 Loss: 0.03478487618129346\n",
      "Iteration: 7505 lambda_n: 0.929172623825967 Loss: 0.0347765243950686\n",
      "Iteration: 7506 lambda_n: 0.9428949388633407 Loss: 0.0347688082090103\n",
      "Iteration: 7507 lambda_n: 0.9565272725758488 Loss: 0.03476097805903889\n",
      "Iteration: 7508 lambda_n: 1.028372815943966 Loss: 0.03475303469203215\n",
      "Iteration: 7509 lambda_n: 0.9872681830331353 Loss: 0.034744494682417575\n",
      "Iteration: 7510 lambda_n: 1.0121717126338048 Loss: 0.034736296011563375\n",
      "Iteration: 7511 lambda_n: 0.9639343188541133 Loss: 0.034727890521867526\n",
      "Iteration: 7512 lambda_n: 0.9046572733354815 Loss: 0.03471988560559612\n",
      "Iteration: 7513 lambda_n: 0.9606965212864967 Loss: 0.03471237294220566\n",
      "Iteration: 7514 lambda_n: 0.9248694762446489 Loss: 0.03470439489622885\n",
      "Iteration: 7515 lambda_n: 0.9705995431352303 Loss: 0.034696714365022564\n",
      "Iteration: 7516 lambda_n: 0.9326475834739508 Loss: 0.03468865406203688\n",
      "Iteration: 7517 lambda_n: 1.004662056301734 Loss: 0.03468090892064068\n",
      "Iteration: 7518 lambda_n: 0.9524879855009195 Loss: 0.03467256572831106\n",
      "Iteration: 7519 lambda_n: 1.0086044064769566 Loss: 0.034664655805010866\n",
      "Iteration: 7520 lambda_n: 1.006216413491027 Loss: 0.034656279854397336\n",
      "Iteration: 7521 lambda_n: 0.9693240857935166 Loss: 0.034647923725072735\n",
      "Iteration: 7522 lambda_n: 0.9615328018738009 Loss: 0.03463987395888409\n",
      "Iteration: 7523 lambda_n: 0.9892269522697797 Loss: 0.034631888886599155\n",
      "Iteration: 7524 lambda_n: 1.0251704389801164 Loss: 0.03462367381850778\n",
      "Iteration: 7525 lambda_n: 0.893275653738501 Loss: 0.03461516024690308\n",
      "Iteration: 7526 lambda_n: 0.9566447949855771 Loss: 0.03460774199243687\n",
      "Iteration: 7527 lambda_n: 0.9266504668374199 Loss: 0.03459979747756697\n",
      "Iteration: 7528 lambda_n: 0.9701097774933191 Loss: 0.034592102044059575\n",
      "Iteration: 7529 lambda_n: 1.0174522700986741 Loss: 0.03458404569940695\n",
      "Iteration: 7530 lambda_n: 0.9821399622015063 Loss: 0.034575596182923905\n",
      "Iteration: 7531 lambda_n: 0.9887938636541759 Loss: 0.03456743990886579\n",
      "Iteration: 7532 lambda_n: 0.9576718257514019 Loss: 0.03455922836656148\n",
      "Iteration: 7533 lambda_n: 0.9733079387057045 Loss: 0.034551275270971946\n",
      "Iteration: 7534 lambda_n: 0.9619472573736988 Loss: 0.03454319231447945\n",
      "Iteration: 7535 lambda_n: 1.0232067560254074 Loss: 0.03453520369532597\n",
      "Iteration: 7536 lambda_n: 0.9868809462367825 Loss: 0.03452670632937647\n",
      "Iteration: 7537 lambda_n: 0.9310645268725227 Loss: 0.034518510626978326\n",
      "Iteration: 7538 lambda_n: 1.025013836831826 Loss: 0.03451077845209468\n",
      "Iteration: 7539 lambda_n: 1.0053010690073858 Loss: 0.03450226605145559\n",
      "Iteration: 7540 lambda_n: 0.9450999541101714 Loss: 0.03449391734951502\n",
      "Iteration: 7541 lambda_n: 0.9612329073685724 Loss: 0.03448606858990009\n",
      "Iteration: 7542 lambda_n: 0.9137803537092337 Loss: 0.03447808584301625\n",
      "Iteration: 7543 lambda_n: 1.0127951744941806 Loss: 0.03447049716729447\n",
      "Iteration: 7544 lambda_n: 0.8947228077185315 Loss: 0.03446208619454699\n",
      "Iteration: 7545 lambda_n: 0.9956708042126728 Loss: 0.03445465577078075\n",
      "Iteration: 7546 lambda_n: 1.0059437361277839 Loss: 0.0344463869941646\n",
      "Iteration: 7547 lambda_n: 0.972902970747744 Loss: 0.0344380328947958\n",
      "Iteration: 7548 lambda_n: 1.0097012260751712 Loss: 0.0344299531817178\n",
      "Iteration: 7549 lambda_n: 0.973989399020951 Loss: 0.03442156785984096\n",
      "Iteration: 7550 lambda_n: 0.8948661362391527 Loss: 0.034413479107358536\n",
      "Iteration: 7551 lambda_n: 1.0269568402789977 Loss: 0.03440604744732396\n",
      "Iteration: 7552 lambda_n: 0.9234152093368228 Loss: 0.03439751879606065\n",
      "Iteration: 7553 lambda_n: 0.9719531984374181 Loss: 0.03438985002709754\n",
      "Iteration: 7554 lambda_n: 0.8983187805250546 Loss: 0.03438177815263152\n",
      "Iteration: 7555 lambda_n: 1.0109993707858196 Loss: 0.03437431778957446\n",
      "Iteration: 7556 lambda_n: 0.9296587483211414 Loss: 0.03436592162835319\n",
      "Iteration: 7557 lambda_n: 0.9050067884534598 Loss: 0.034358200977789334\n",
      "Iteration: 7558 lambda_n: 0.9164902653027613 Loss: 0.03435068505020815\n",
      "Iteration: 7559 lambda_n: 0.8876847894169312 Loss: 0.034343073747277686\n",
      "Iteration: 7560 lambda_n: 0.9878901541445465 Loss: 0.034335701662270665\n",
      "Iteration: 7561 lambda_n: 1.0215985551200406 Loss: 0.034327497379838603\n",
      "Iteration: 7562 lambda_n: 0.8813450991672205 Loss: 0.0343190131456331\n",
      "Iteration: 7563 lambda_n: 0.9214315478793583 Loss: 0.03431169368934718\n",
      "Iteration: 7564 lambda_n: 0.9901903055637633 Loss: 0.03430404131356745\n",
      "Iteration: 7565 lambda_n: 0.9014803950155454 Loss: 0.034295817897193666\n",
      "Iteration: 7566 lambda_n: 1.0277620451850626 Loss: 0.03428833119899075\n",
      "Iteration: 7567 lambda_n: 0.9624989939507641 Loss: 0.03427979573758907\n",
      "Iteration: 7568 lambda_n: 0.8792804752190182 Loss: 0.03427180227117393\n",
      "Iteration: 7569 lambda_n: 0.9259117199389942 Loss: 0.03426449992001771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7570 lambda_n: 0.9788413113665972 Loss: 0.03425681029361944\n",
      "Iteration: 7571 lambda_n: 0.8944842113447923 Loss: 0.03424868108362724\n",
      "Iteration: 7572 lambda_n: 0.9274657102564852 Loss: 0.034241252446445186\n",
      "Iteration: 7573 lambda_n: 0.8994216075776608 Loss: 0.034233549893214324\n",
      "Iteration: 7574 lambda_n: 0.9491096850488433 Loss: 0.03422608023803127\n",
      "Iteration: 7575 lambda_n: 0.9898778335198889 Loss: 0.03421819791880088\n",
      "Iteration: 7576 lambda_n: 0.9481720578023496 Loss: 0.03420997701416323\n",
      "Iteration: 7577 lambda_n: 1.0010916673042147 Loss: 0.03420210246719862\n",
      "Iteration: 7578 lambda_n: 1.0070649324987273 Loss: 0.03419378841657118\n",
      "Iteration: 7579 lambda_n: 0.9502375062158669 Loss: 0.03418542475008421\n",
      "Iteration: 7580 lambda_n: 0.9954123216863905 Loss: 0.03417753302736665\n",
      "Iteration: 7581 lambda_n: 1.0291308115358464 Loss: 0.03416926612036765\n",
      "Iteration: 7582 lambda_n: 0.8848652952821947 Loss: 0.03416071917303003\n",
      "Iteration: 7583 lambda_n: 1.003079396484989 Loss: 0.034153370345898486\n",
      "Iteration: 7584 lambda_n: 1.022468962540771 Loss: 0.03414503974099324\n",
      "Iteration: 7585 lambda_n: 0.8861963540766002 Loss: 0.03413654809720302\n",
      "Iteration: 7586 lambda_n: 0.9864291001381156 Loss: 0.034129188195602365\n",
      "Iteration: 7587 lambda_n: 0.9486757019226495 Loss: 0.03412099584971634\n",
      "Iteration: 7588 lambda_n: 0.9012560516833277 Loss: 0.03411311704061336\n",
      "Iteration: 7589 lambda_n: 0.9141261840070491 Loss: 0.0341056320480306\n",
      "Iteration: 7590 lambda_n: 0.9085694246570504 Loss: 0.03409804016185741\n",
      "Iteration: 7591 lambda_n: 1.0202312428133473 Loss: 0.034090494418689485\n",
      "Iteration: 7592 lambda_n: 0.9298289774970365 Loss: 0.03408202133365896\n",
      "Iteration: 7593 lambda_n: 0.9503162810369408 Loss: 0.03407429902800331\n",
      "Iteration: 7594 lambda_n: 0.9690046037568754 Loss: 0.03406640656122676\n",
      "Iteration: 7595 lambda_n: 0.9179421862692849 Loss: 0.03405835887553553\n",
      "Iteration: 7596 lambda_n: 0.9289643724772102 Loss: 0.0340507352598023\n",
      "Iteration: 7597 lambda_n: 0.9179192845951225 Loss: 0.03404302009600135\n",
      "Iteration: 7598 lambda_n: 0.9023492713655392 Loss: 0.0340353966560076\n",
      "Iteration: 7599 lambda_n: 0.954594082698472 Loss: 0.034027902520539687\n",
      "Iteration: 7600 lambda_n: 0.9054988328043234 Loss: 0.034019974478189545\n",
      "Iteration: 7601 lambda_n: 1.0109228357570452 Loss: 0.03401245414833004\n",
      "Iteration: 7602 lambda_n: 0.8863853837906172 Loss: 0.0340040582487436\n",
      "Iteration: 7603 lambda_n: 1.0212246771807074 Loss: 0.03399669665878925\n",
      "Iteration: 7604 lambda_n: 0.957735307882119 Loss: 0.033988215203440274\n",
      "Iteration: 7605 lambda_n: 0.9785616144850104 Loss: 0.03398026106815311\n",
      "Iteration: 7606 lambda_n: 0.8941443978752699 Loss: 0.03397213394911431\n",
      "Iteration: 7607 lambda_n: 0.9673591621734864 Loss: 0.033964707916091706\n",
      "Iteration: 7608 lambda_n: 0.8868596475935927 Loss: 0.033956673810905356\n",
      "Iteration: 7609 lambda_n: 1.010479509647616 Loss: 0.03394930823893992\n",
      "Iteration: 7610 lambda_n: 0.9951360081313811 Loss: 0.033940915977446105\n",
      "Iteration: 7611 lambda_n: 0.8959010361566339 Loss: 0.03393265114596391\n",
      "Iteration: 7612 lambda_n: 1.0298352304085527 Loss: 0.03392521048037129\n",
      "Iteration: 7613 lambda_n: 0.9836108923139847 Loss: 0.033916657455744265\n",
      "Iteration: 7614 lambda_n: 0.8835961244311719 Loss: 0.033908488329370205\n",
      "Iteration: 7615 lambda_n: 0.9518423770689166 Loss: 0.033901149844380175\n",
      "Iteration: 7616 lambda_n: 0.9850844631876631 Loss: 0.03389324455182966\n",
      "Iteration: 7617 lambda_n: 0.8877541541887287 Loss: 0.0338850631690305\n",
      "Iteration: 7618 lambda_n: 0.92687671027645 Loss: 0.033877690133809944\n",
      "Iteration: 7619 lambda_n: 0.8968280103109483 Loss: 0.03386999216969065\n",
      "Iteration: 7620 lambda_n: 0.9466864117608728 Loss: 0.03386254376252225\n",
      "Iteration: 7621 lambda_n: 0.9221765840301052 Loss: 0.033854681285334046\n",
      "Iteration: 7622 lambda_n: 1.0202686573721418 Loss: 0.033847022353256616\n",
      "Iteration: 7623 lambda_n: 0.9271504486195303 Loss: 0.03383854872658073\n",
      "Iteration: 7624 lambda_n: 0.9366708134280252 Loss: 0.033830848463149904\n",
      "Iteration: 7625 lambda_n: 1.023731975050515 Loss: 0.03382306912223028\n",
      "Iteration: 7626 lambda_n: 0.9275991163072573 Loss: 0.033814566703573294\n",
      "Iteration: 7627 lambda_n: 0.9830743677037351 Loss: 0.033806862691597224\n",
      "Iteration: 7628 lambda_n: 0.9988974360670925 Loss: 0.033798697933161266\n",
      "Iteration: 7629 lambda_n: 0.9455965443018082 Loss: 0.033790401752152296\n",
      "Iteration: 7630 lambda_n: 0.9975980666070217 Loss: 0.03378254824672668\n",
      "Iteration: 7631 lambda_n: 0.9253449301603858 Loss: 0.033774262844495744\n",
      "Iteration: 7632 lambda_n: 0.9148038133557164 Loss: 0.03376657752387458\n",
      "Iteration: 7633 lambda_n: 0.8830136529244985 Loss: 0.033758979745487264\n",
      "Iteration: 7634 lambda_n: 0.9403227794080471 Loss: 0.03375164599070461\n",
      "Iteration: 7635 lambda_n: 0.9617629654628506 Loss: 0.03374383625708814\n",
      "Iteration: 7636 lambda_n: 0.9998354937361448 Loss: 0.03373584844885757\n",
      "Iteration: 7637 lambda_n: 0.9224782965318172 Loss: 0.033727544427589295\n",
      "Iteration: 7638 lambda_n: 0.9917740358183845 Loss: 0.0337198828819064\n",
      "Iteration: 7639 lambda_n: 0.8784564740576635 Loss: 0.033711645801977316\n",
      "Iteration: 7640 lambda_n: 0.8852058879990599 Loss: 0.033704349864146776\n",
      "Iteration: 7641 lambda_n: 0.9125395817162918 Loss: 0.033696997864766584\n",
      "Iteration: 7642 lambda_n: 0.9725271906711821 Loss: 0.033689418842718054\n",
      "Iteration: 7643 lambda_n: 1.0189069385909117 Loss: 0.03368134159296604\n",
      "Iteration: 7644 lambda_n: 0.9741029149680315 Loss: 0.033672879133569256\n",
      "Iteration: 7645 lambda_n: 0.8766644779743274 Loss: 0.03366478878459277\n",
      "Iteration: 7646 lambda_n: 1.0274957574848635 Loss: 0.033657507698929605\n",
      "Iteration: 7647 lambda_n: 0.8974618488002633 Loss: 0.033648973887291975\n",
      "Iteration: 7648 lambda_n: 0.9817723488187796 Loss: 0.03364152005966243\n",
      "Iteration: 7649 lambda_n: 0.8788550119579666 Loss: 0.03363336598966173\n",
      "Iteration: 7650 lambda_n: 0.9358718937714022 Loss: 0.03362606669005377\n",
      "Iteration: 7651 lambda_n: 0.905786243566758 Loss: 0.03361829383368222\n",
      "Iteration: 7652 lambda_n: 0.9889291846732872 Loss: 0.03361077084759775\n",
      "Iteration: 7653 lambda_n: 0.9626215532500849 Loss: 0.03360255731434294\n",
      "Iteration: 7654 lambda_n: 0.9636674250397692 Loss: 0.03359456227284963\n",
      "Iteration: 7655 lambda_n: 0.9688478773868623 Loss: 0.033586558539263234\n",
      "Iteration: 7656 lambda_n: 0.9814349592650137 Loss: 0.03357851177383013\n",
      "Iteration: 7657 lambda_n: 0.8969448171356571 Loss: 0.03357036046068125\n",
      "Iteration: 7658 lambda_n: 0.9047357921836587 Loss: 0.033562910875563234\n",
      "Iteration: 7659 lambda_n: 1.022930793065488 Loss: 0.03355539657757356\n",
      "Iteration: 7660 lambda_n: 0.9952235425217194 Loss: 0.033546900603572856\n",
      "Iteration: 7661 lambda_n: 0.9848431574165933 Loss: 0.03353863474670744\n",
      "Iteration: 7662 lambda_n: 0.9018533623698409 Loss: 0.03353045509863121\n",
      "Iteration: 7663 lambda_n: 0.8912885414956625 Loss: 0.03352296471986426\n",
      "Iteration: 7664 lambda_n: 0.888460272189882 Loss: 0.03351556208292333\n",
      "Iteration: 7665 lambda_n: 0.9479750289583014 Loss: 0.033508182931675165\n",
      "Iteration: 7666 lambda_n: 0.8987631332819643 Loss: 0.0335003094727416\n",
      "Iteration: 7667 lambda_n: 0.8972911501337333 Loss: 0.033492844740976746\n",
      "Iteration: 7668 lambda_n: 0.9161365787817828 Loss: 0.03348539223019623\n",
      "Iteration: 7669 lambda_n: 0.9878790844520114 Loss: 0.03347778319273348\n",
      "Iteration: 7670 lambda_n: 0.9090508149822926 Loss: 0.03346957828760489\n",
      "Iteration: 7671 lambda_n: 0.9733304747545652 Loss: 0.03346202809153906\n",
      "Iteration: 7672 lambda_n: 0.9539158996918966 Loss: 0.03345394401045489\n",
      "Iteration: 7673 lambda_n: 0.9428411424054833 Loss: 0.03344602117354121\n",
      "Iteration: 7674 lambda_n: 0.9807226986176322 Loss: 0.033438190313961044\n",
      "Iteration: 7675 lambda_n: 0.9790899854961361 Loss: 0.0334300448201925\n",
      "Iteration: 7676 lambda_n: 1.010152621398912 Loss: 0.03342191288169826\n",
      "Iteration: 7677 lambda_n: 0.938043596754812 Loss: 0.03341352294356833\n",
      "Iteration: 7678 lambda_n: 0.9542939928937418 Loss: 0.03340573190989998\n",
      "Iteration: 7679 lambda_n: 1.029236078602547 Loss: 0.033397805901647294\n",
      "Iteration: 7680 lambda_n: 1.0172919902797006 Loss: 0.03338925744709177\n",
      "Iteration: 7681 lambda_n: 0.9175290777587293 Loss: 0.033380808189947035\n",
      "Iteration: 7682 lambda_n: 0.9684533575098987 Loss: 0.03337318752212133\n",
      "Iteration: 7683 lambda_n: 0.9716816771707248 Loss: 0.03336514389061639\n",
      "Iteration: 7684 lambda_n: 0.939822056465227 Loss: 0.03335707344070029\n",
      "Iteration: 7685 lambda_n: 0.9898235659440326 Loss: 0.033349267600746565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7686 lambda_n: 0.9347289590221003 Loss: 0.033341046460362005\n",
      "Iteration: 7687 lambda_n: 0.9903109741581027 Loss: 0.03333328291219089\n",
      "Iteration: 7688 lambda_n: 0.9120990477672706 Loss: 0.03332505771330454\n",
      "Iteration: 7689 lambda_n: 0.910982329674643 Loss: 0.03331748211224518\n",
      "Iteration: 7690 lambda_n: 0.8981537445559828 Loss: 0.033309915781858324\n",
      "Iteration: 7691 lambda_n: 1.00181740963211 Loss: 0.0333024559972854\n",
      "Iteration: 7692 lambda_n: 0.982352270483911 Loss: 0.03329413520992569\n",
      "Iteration: 7693 lambda_n: 0.9088130135897108 Loss: 0.03328597608883432\n",
      "Iteration: 7694 lambda_n: 0.9684005317800397 Loss: 0.03327842775789026\n",
      "Iteration: 7695 lambda_n: 1.0037333967098894 Loss: 0.0332703845061192\n",
      "Iteration: 7696 lambda_n: 1.009172377031931 Loss: 0.03326204778483331\n",
      "Iteration: 7697 lambda_n: 0.9559243666993305 Loss: 0.03325366588367669\n",
      "Iteration: 7698 lambda_n: 1.020141678160644 Loss: 0.033245726240479154\n",
      "Iteration: 7699 lambda_n: 0.8948065570167824 Loss: 0.03323725322104627\n",
      "Iteration: 7700 lambda_n: 0.9506357913179129 Loss: 0.03322982119635564\n",
      "Iteration: 7701 lambda_n: 0.9868657404375651 Loss: 0.033221925464583246\n",
      "Iteration: 7702 lambda_n: 0.9349468620297253 Loss: 0.03321372881156778\n",
      "Iteration: 7703 lambda_n: 0.9661183222713171 Loss: 0.03320596337871157\n",
      "Iteration: 7704 lambda_n: 0.9729761184374263 Loss: 0.033197939038999785\n",
      "Iteration: 7705 lambda_n: 0.9696080378869303 Loss: 0.03318985773539143\n",
      "Iteration: 7706 lambda_n: 0.8846851904653659 Loss: 0.033181804401504304\n",
      "Iteration: 7707 lambda_n: 0.8964035106373704 Loss: 0.03317445641230275\n",
      "Iteration: 7708 lambda_n: 0.9319532182108647 Loss: 0.03316701108951016\n",
      "Iteration: 7709 lambda_n: 0.9267976770436253 Loss: 0.03315927049481993\n",
      "Iteration: 7710 lambda_n: 1.0266256263887388 Loss: 0.03315157271662363\n",
      "Iteration: 7711 lambda_n: 1.0236851531113327 Loss: 0.03314304578469467\n",
      "Iteration: 7712 lambda_n: 1.0102703952247405 Loss: 0.033134543270539586\n",
      "Iteration: 7713 lambda_n: 0.8804884257920909 Loss: 0.0331261521714858\n",
      "Iteration: 7714 lambda_n: 0.9622875632934013 Loss: 0.033118839010551106\n",
      "Iteration: 7715 lambda_n: 1.0211449205715755 Loss: 0.03311084643821635\n",
      "Iteration: 7716 lambda_n: 0.9649010999845691 Loss: 0.033102365003430216\n",
      "Iteration: 7717 lambda_n: 0.938171563568846 Loss: 0.03309435071432626\n",
      "Iteration: 7718 lambda_n: 1.0238681495656727 Loss: 0.033086558431431684\n",
      "Iteration: 7719 lambda_n: 0.927977512726269 Loss: 0.033078054363643986\n",
      "Iteration: 7720 lambda_n: 0.9427173421345814 Loss: 0.03307034674199205\n",
      "Iteration: 7721 lambda_n: 0.9479488111593445 Loss: 0.03306251668969924\n",
      "Iteration: 7722 lambda_n: 0.9801733728046119 Loss: 0.033054643181474073\n",
      "Iteration: 7723 lambda_n: 1.0210481722990128 Loss: 0.03304650201691853\n",
      "Iteration: 7724 lambda_n: 0.8818598058507525 Loss: 0.033038021348065764\n",
      "Iteration: 7725 lambda_n: 0.9845959974722573 Loss: 0.03303069675212479\n",
      "Iteration: 7726 lambda_n: 0.8846452647537226 Loss: 0.033022518840670693\n",
      "Iteration: 7727 lambda_n: 0.9762673568903804 Loss: 0.03301517110144514\n",
      "Iteration: 7728 lambda_n: 0.924487692565815 Loss: 0.03300706235801277\n",
      "Iteration: 7729 lambda_n: 0.921940120842588 Loss: 0.032999383685242197\n",
      "Iteration: 7730 lambda_n: 1.0198605970677264 Loss: 0.03299172616835255\n",
      "Iteration: 7731 lambda_n: 1.0254070205224646 Loss: 0.03298325533220001\n",
      "Iteration: 7732 lambda_n: 0.891731140995744 Loss: 0.032974738423372604\n",
      "Iteration: 7733 lambda_n: 0.9913343935589507 Loss: 0.032967331806351016\n",
      "Iteration: 7734 lambda_n: 1.0150505315652254 Loss: 0.03295909789209807\n",
      "Iteration: 7735 lambda_n: 1.0285283848514304 Loss: 0.03295066698968316\n",
      "Iteration: 7736 lambda_n: 0.9491001806091596 Loss: 0.032942124136958385\n",
      "Iteration: 7737 lambda_n: 0.9683095502898755 Loss: 0.03293424100249404\n",
      "Iteration: 7738 lambda_n: 0.9867970863704681 Loss: 0.03292619831276902\n",
      "Iteration: 7739 lambda_n: 0.9944944819005873 Loss: 0.03291800206303873\n",
      "Iteration: 7740 lambda_n: 0.9407797062518685 Loss: 0.03290974184127916\n",
      "Iteration: 7741 lambda_n: 0.8958904679295034 Loss: 0.03290192778132801\n",
      "Iteration: 7742 lambda_n: 0.9083669713875946 Loss: 0.03289448657247587\n",
      "Iteration: 7743 lambda_n: 0.8924601104426876 Loss: 0.03288694173549364\n",
      "Iteration: 7744 lambda_n: 0.8897350363998794 Loss: 0.03287952901912072\n",
      "Iteration: 7745 lambda_n: 1.0274081793400962 Loss: 0.032872138935255826\n",
      "Iteration: 7746 lambda_n: 1.0098936387744706 Loss: 0.03286360534403332\n",
      "Iteration: 7747 lambda_n: 0.9532290209543461 Loss: 0.03285521722382079\n",
      "Iteration: 7748 lambda_n: 1.0230791247318332 Loss: 0.03284729975295281\n",
      "Iteration: 7749 lambda_n: 0.996772920983147 Loss: 0.032838802106703015\n",
      "Iteration: 7750 lambda_n: 0.9741683042275976 Loss: 0.032830522954201864\n",
      "Iteration: 7751 lambda_n: 0.987932942861028 Loss: 0.03282243155050848\n",
      "Iteration: 7752 lambda_n: 0.9345033704218925 Loss: 0.03281422581414284\n",
      "Iteration: 7753 lambda_n: 0.94726470730025 Loss: 0.03280646385797977\n",
      "Iteration: 7754 lambda_n: 0.9232618360102228 Loss: 0.03279859590277683\n",
      "Iteration: 7755 lambda_n: 0.9461823165786964 Loss: 0.032790927311069876\n",
      "Iteration: 7756 lambda_n: 1.0298174875452433 Loss: 0.03278306833869399\n",
      "Iteration: 7757 lambda_n: 0.91546130009782 Loss: 0.032774514690093605\n",
      "Iteration: 7758 lambda_n: 1.0140566948104859 Loss: 0.03276691087833201\n",
      "Iteration: 7759 lambda_n: 1.0031157352917555 Loss: 0.03275848813043291\n",
      "Iteration: 7760 lambda_n: 0.9583874431986429 Loss: 0.03275015625386894\n",
      "Iteration: 7761 lambda_n: 0.8964858737711464 Loss: 0.03274219588642737\n",
      "Iteration: 7762 lambda_n: 1.0209424970325771 Loss: 0.03273474966999438\n",
      "Iteration: 7763 lambda_n: 0.9283300411153976 Loss: 0.03272626971247183\n",
      "Iteration: 7764 lambda_n: 0.9418764958332767 Loss: 0.03271855899099385\n",
      "Iteration: 7765 lambda_n: 0.9179107661311994 Loss: 0.032710735748961245\n",
      "Iteration: 7766 lambda_n: 1.02679758222117 Loss: 0.03270311156321064\n",
      "Iteration: 7767 lambda_n: 1.0086313197663024 Loss: 0.0326945829575579\n",
      "Iteration: 7768 lambda_n: 0.9263812831638766 Loss: 0.032686205237196314\n",
      "Iteration: 7769 lambda_n: 1.023520196154946 Loss: 0.032678510684263085\n",
      "Iteration: 7770 lambda_n: 0.9444465681970865 Loss: 0.0326700092886346\n",
      "Iteration: 7771 lambda_n: 0.8813812930325704 Loss: 0.03266216467760264\n",
      "Iteration: 7772 lambda_n: 0.9036167399816227 Loss: 0.032654843886009234\n",
      "Iteration: 7773 lambda_n: 0.9543465373396861 Loss: 0.03264733840271416\n",
      "Iteration: 7774 lambda_n: 0.926735110842749 Loss: 0.03263941155197815\n",
      "Iteration: 7775 lambda_n: 0.9779980856634062 Loss: 0.03263171403972405\n",
      "Iteration: 7776 lambda_n: 0.9497093225395321 Loss: 0.03262359073091754\n",
      "Iteration: 7777 lambda_n: 0.9056555677540091 Loss: 0.0326157023866461\n",
      "Iteration: 7778 lambda_n: 0.9337048008249987 Loss: 0.032608179952286646\n",
      "Iteration: 7779 lambda_n: 0.9488936346719054 Loss: 0.03260042453593936\n",
      "Iteration: 7780 lambda_n: 0.9067725996393448 Loss: 0.032592542956726256\n",
      "Iteration: 7781 lambda_n: 0.9559278361030236 Loss: 0.03258501123461936\n",
      "Iteration: 7782 lambda_n: 0.9205768284148086 Loss: 0.032577071222116664\n",
      "Iteration: 7783 lambda_n: 0.9625845723253426 Loss: 0.03256942483458185\n",
      "Iteration: 7784 lambda_n: 0.969341742423094 Loss: 0.03256142952392454\n",
      "Iteration: 7785 lambda_n: 0.9673429347206148 Loss: 0.03255337808414845\n",
      "Iteration: 7786 lambda_n: 0.9051777569570999 Loss: 0.03254534324316501\n",
      "Iteration: 7787 lambda_n: 0.951561419394573 Loss: 0.032537824748724936\n",
      "Iteration: 7788 lambda_n: 1.0100885979440464 Loss: 0.03252992098390236\n",
      "Iteration: 7789 lambda_n: 0.964815963343074 Loss: 0.03252153108292229\n",
      "Iteration: 7790 lambda_n: 0.910796927331853 Loss: 0.032513517217586836\n",
      "Iteration: 7791 lambda_n: 0.9993085678797287 Loss: 0.032505952036979195\n",
      "Iteration: 7792 lambda_n: 0.8917538842097873 Loss: 0.03249765166553757\n",
      "Iteration: 7793 lambda_n: 0.8814495715813568 Loss: 0.032490244652391614\n",
      "Iteration: 7794 lambda_n: 0.8962398938534395 Loss: 0.03248292322525169\n",
      "Iteration: 7795 lambda_n: 0.9104038102589463 Loss: 0.032475478945064085\n",
      "Iteration: 7796 lambda_n: 1.010959795131008 Loss: 0.0324679170147043\n",
      "Iteration: 7797 lambda_n: 0.9896807273321988 Loss: 0.032459519850211026\n",
      "Iteration: 7798 lambda_n: 0.8822760731822127 Loss: 0.03245129942888791\n",
      "Iteration: 7799 lambda_n: 0.9841872642162268 Loss: 0.032443971121985056\n",
      "Iteration: 7800 lambda_n: 0.9684256993510041 Loss: 0.03243579632360091\n",
      "Iteration: 7801 lambda_n: 1.005979588753894 Loss: 0.032427752439653165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7802 lambda_n: 0.9328273614642544 Loss: 0.03241939662426233\n",
      "Iteration: 7803 lambda_n: 0.876781137079733 Loss: 0.03241164841882905\n",
      "Iteration: 7804 lambda_n: 0.9848418657086769 Loss: 0.032404365738989575\n",
      "Iteration: 7805 lambda_n: 0.9101936985263336 Loss: 0.03239618548703142\n",
      "Iteration: 7806 lambda_n: 0.9967991781644733 Loss: 0.03238862527143997\n",
      "Iteration: 7807 lambda_n: 0.9181590430870763 Loss: 0.03238034569366387\n",
      "Iteration: 7808 lambda_n: 0.9605840584734144 Loss: 0.03237271931064098\n",
      "Iteration: 7809 lambda_n: 0.909160353852257 Loss: 0.03236474053532273\n",
      "Iteration: 7810 lambda_n: 0.9434836357756707 Loss: 0.03235718889111708\n",
      "Iteration: 7811 lambda_n: 0.9929130009027546 Loss: 0.03234935214885797\n",
      "Iteration: 7812 lambda_n: 0.8920058562843719 Loss: 0.03234110483438127\n",
      "Iteration: 7813 lambda_n: 0.9550355142244764 Loss: 0.03233369566987399\n",
      "Iteration: 7814 lambda_n: 0.8863175640586314 Loss: 0.032325762966605366\n",
      "Iteration: 7815 lambda_n: 1.029838540690703 Loss: 0.032318401044638956\n",
      "Iteration: 7816 lambda_n: 0.9099235063917226 Loss: 0.03230984700716999\n",
      "Iteration: 7817 lambda_n: 0.9776298730885803 Loss: 0.03230228900399437\n",
      "Iteration: 7818 lambda_n: 0.9851843373641604 Loss: 0.03229416861552767\n",
      "Iteration: 7819 lambda_n: 0.9780777929710033 Loss: 0.032285985475018035\n",
      "Iteration: 7820 lambda_n: 0.9812705301347672 Loss: 0.032277861359757035\n",
      "Iteration: 7821 lambda_n: 1.029605700026714 Loss: 0.03226971072184239\n",
      "Iteration: 7822 lambda_n: 1.0008090815474384 Loss: 0.03226115859864192\n",
      "Iteration: 7823 lambda_n: 0.9813559821890635 Loss: 0.03225284566293166\n",
      "Iteration: 7824 lambda_n: 0.9330370936636242 Loss: 0.03224469430569511\n",
      "Iteration: 7825 lambda_n: 1.0054757501576503 Loss: 0.032236944292810134\n",
      "Iteration: 7826 lambda_n: 0.9155280962759054 Loss: 0.03222859258543042\n",
      "Iteration: 7827 lambda_n: 0.8768780841941712 Loss: 0.0322209880005543\n",
      "Iteration: 7828 lambda_n: 0.9799964938002883 Loss: 0.032213704456199116\n",
      "Iteration: 7829 lambda_n: 0.9141923281661251 Loss: 0.032205564381665414\n",
      "Iteration: 7830 lambda_n: 0.9904819478162267 Loss: 0.03219797088666828\n",
      "Iteration: 7831 lambda_n: 0.9698743806457318 Loss: 0.03218974370818004\n",
      "Iteration: 7832 lambda_n: 0.9186207031063405 Loss: 0.03218168769721764\n",
      "Iteration: 7833 lambda_n: 0.9372527223223991 Loss: 0.03217405740847567\n",
      "Iteration: 7834 lambda_n: 0.9768381809545523 Loss: 0.03216627235471297\n",
      "Iteration: 7835 lambda_n: 0.9987878297922252 Loss: 0.03215815849134407\n",
      "Iteration: 7836 lambda_n: 0.9107808933541723 Loss: 0.03214986230560852\n",
      "Iteration: 7837 lambda_n: 0.9966455262476434 Loss: 0.03214229712506024\n",
      "Iteration: 7838 lambda_n: 0.8821187638113119 Loss: 0.032134018753884434\n",
      "Iteration: 7839 lambda_n: 0.9906868677886229 Loss: 0.03212669165639054\n",
      "Iteration: 7840 lambda_n: 0.8912151034626041 Loss: 0.032118462756760044\n",
      "Iteration: 7841 lambda_n: 0.9744071446668771 Loss: 0.03211106008869299\n",
      "Iteration: 7842 lambda_n: 0.9127213090760169 Loss: 0.032102966400758944\n",
      "Iteration: 7843 lambda_n: 0.9016409430522274 Loss: 0.03209538508793566\n",
      "Iteration: 7844 lambda_n: 0.952758401064535 Loss: 0.032087895808503\n",
      "Iteration: 7845 lambda_n: 0.9514120802520657 Loss: 0.032079981930280894\n",
      "Iteration: 7846 lambda_n: 0.990899986236596 Loss: 0.0320720792320276\n",
      "Iteration: 7847 lambda_n: 0.9228770049806566 Loss: 0.03206384853314815\n",
      "Iteration: 7848 lambda_n: 0.9724293833612806 Loss: 0.03205618284990928\n",
      "Iteration: 7849 lambda_n: 0.939357446599674 Loss: 0.032048105567691935\n",
      "Iteration: 7850 lambda_n: 1.0147788625547043 Loss: 0.03204030298801831\n",
      "Iteration: 7851 lambda_n: 1.0241400369628053 Loss: 0.032031873933142045\n",
      "Iteration: 7852 lambda_n: 0.9899761061420562 Loss: 0.03202336711863132\n",
      "Iteration: 7853 lambda_n: 0.9357432351935118 Loss: 0.03201514407712521\n",
      "Iteration: 7854 lambda_n: 1.002617278793404 Loss: 0.03200737150767199\n",
      "Iteration: 7855 lambda_n: 0.9293957176529423 Loss: 0.031999043459332306\n",
      "Iteration: 7856 lambda_n: 0.9621151921792204 Loss: 0.031991323609250215\n",
      "Iteration: 7857 lambda_n: 0.9211684228429543 Loss: 0.03198333197853147\n",
      "Iteration: 7858 lambda_n: 0.8908811082587555 Loss: 0.0319756804620565\n",
      "Iteration: 7859 lambda_n: 0.8801050638805142 Loss: 0.03196828051931944\n",
      "Iteration: 7860 lambda_n: 0.8938658006361976 Loss: 0.03196097008367016\n",
      "Iteration: 7861 lambda_n: 0.9349836500579143 Loss: 0.03195354534474092\n",
      "Iteration: 7862 lambda_n: 0.9609770184456078 Loss: 0.03194577906532899\n",
      "Iteration: 7863 lambda_n: 0.9568542235511581 Loss: 0.03193779687404368\n",
      "Iteration: 7864 lambda_n: 0.881012338452711 Loss: 0.03192984892552845\n",
      "Iteration: 7865 lambda_n: 0.9682835789256152 Loss: 0.03192253094255859\n",
      "Iteration: 7866 lambda_n: 0.8919199924558161 Loss: 0.03191448805317404\n",
      "Iteration: 7867 lambda_n: 1.0264453414163548 Loss: 0.03190707946310172\n",
      "Iteration: 7868 lambda_n: 0.9011452999541744 Loss: 0.0318985534573671\n",
      "Iteration: 7869 lambda_n: 0.999963630629987 Loss: 0.031891068234095855\n",
      "Iteration: 7870 lambda_n: 0.9132007303839558 Loss: 0.03188276218931524\n",
      "Iteration: 7871 lambda_n: 1.011671820073505 Loss: 0.031875176824836114\n",
      "Iteration: 7872 lambda_n: 0.9588616863920205 Loss: 0.03186677352260679\n",
      "Iteration: 7873 lambda_n: 0.9882779209046347 Loss: 0.03185880887736022\n",
      "Iteration: 7874 lambda_n: 0.9358407365819797 Loss: 0.031850599887932485\n",
      "Iteration: 7875 lambda_n: 0.9045829158590051 Loss: 0.03184282645805177\n",
      "Iteration: 7876 lambda_n: 0.8940583283229837 Loss: 0.031835312664655675\n",
      "Iteration: 7877 lambda_n: 0.8803151061840421 Loss: 0.03182788629018278\n",
      "Iteration: 7878 lambda_n: 1.023372421766364 Loss: 0.03182057406987467\n",
      "Iteration: 7879 lambda_n: 0.9089287075579514 Loss: 0.03181207356067424\n",
      "Iteration: 7880 lambda_n: 0.9367589492360984 Loss: 0.03180452366081369\n",
      "Iteration: 7881 lambda_n: 0.9442083114143973 Loss: 0.03179674259042495\n",
      "Iteration: 7882 lambda_n: 0.8976665334188156 Loss: 0.03178889964057782\n",
      "Iteration: 7883 lambda_n: 0.8837815940486395 Loss: 0.03178144328210093\n",
      "Iteration: 7884 lambda_n: 0.8763780288087318 Loss: 0.031774102255173846\n",
      "Iteration: 7885 lambda_n: 0.9723041700365203 Loss: 0.03176682272311848\n",
      "Iteration: 7886 lambda_n: 0.9101949284567413 Loss: 0.03175874638958912\n",
      "Iteration: 7887 lambda_n: 0.9541885555881613 Loss: 0.03175118595715114\n",
      "Iteration: 7888 lambda_n: 0.9222340724705963 Loss: 0.031743260094389526\n",
      "Iteration: 7889 lambda_n: 0.9545059783195994 Loss: 0.031735599655843785\n",
      "Iteration: 7890 lambda_n: 0.938384349039463 Loss: 0.03172767115197842\n",
      "Iteration: 7891 lambda_n: 0.9563525971206535 Loss: 0.03171987655851569\n",
      "Iteration: 7892 lambda_n: 0.9500839274859706 Loss: 0.03171193271142488\n",
      "Iteration: 7893 lambda_n: 0.8809472070446899 Loss: 0.03170404093217502\n",
      "Iteration: 7894 lambda_n: 0.8996388809504847 Loss: 0.03169672342828112\n",
      "Iteration: 7895 lambda_n: 0.9537267078284831 Loss: 0.031689250661847475\n",
      "Iteration: 7896 lambda_n: 0.9403620984838146 Loss: 0.031681328617839954\n",
      "Iteration: 7897 lambda_n: 0.9848870227737968 Loss: 0.031673517583568477\n",
      "Iteration: 7898 lambda_n: 1.0266470649798263 Loss: 0.031665336704732094\n",
      "Iteration: 7899 lambda_n: 0.9888412131809055 Loss: 0.031656808947287715\n",
      "Iteration: 7900 lambda_n: 0.923914581246522 Loss: 0.03164859521853725\n",
      "Iteration: 7901 lambda_n: 0.9241829569451747 Loss: 0.031640920795370245\n",
      "Iteration: 7902 lambda_n: 0.9941111351906807 Loss: 0.031633244140933485\n",
      "Iteration: 7903 lambda_n: 0.9327846423785866 Loss: 0.03162498663130592\n",
      "Iteration: 7904 lambda_n: 0.9107645091622658 Loss: 0.03161723852340667\n",
      "Iteration: 7905 lambda_n: 0.8918235715780283 Loss: 0.03160967332214999\n",
      "Iteration: 7906 lambda_n: 0.95495595450489 Loss: 0.03160226545055731\n",
      "Iteration: 7907 lambda_n: 0.8888972030556611 Loss: 0.031594333172153974\n",
      "Iteration: 7908 lambda_n: 1.0085595014039168 Loss: 0.03158694960442348\n",
      "Iteration: 7909 lambda_n: 0.8948662591944448 Loss: 0.03157857206739889\n",
      "Iteration: 7910 lambda_n: 1.0227362711411072 Loss: 0.03157113891416653\n",
      "Iteration: 7911 lambda_n: 0.9834839830329244 Loss: 0.031562643614205674\n",
      "Iteration: 7912 lambda_n: 0.9101860068820327 Loss: 0.031554474358816045\n",
      "Iteration: 7913 lambda_n: 0.9382746373536391 Loss: 0.03154691394697804\n",
      "Iteration: 7914 lambda_n: 0.9807648766379561 Loss: 0.03153912021648346\n",
      "Iteration: 7915 lambda_n: 0.8786364023193948 Loss: 0.03153097354090199\n",
      "Iteration: 7916 lambda_n: 0.8804565430105681 Loss: 0.03152367518856719\n",
      "Iteration: 7917 lambda_n: 0.9300204057598093 Loss: 0.031516361715584765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7918 lambda_n: 0.893753522418526 Loss: 0.03150863654076055\n",
      "Iteration: 7919 lambda_n: 0.9061095937195237 Loss: 0.03150121261340548\n",
      "Iteration: 7920 lambda_n: 0.9298174374983529 Loss: 0.031493686049050826\n",
      "Iteration: 7921 lambda_n: 0.9405196244653591 Loss: 0.03148596255454693\n",
      "Iteration: 7922 lambda_n: 0.880230576978755 Loss: 0.031478150160794846\n",
      "Iteration: 7923 lambda_n: 0.9031014709054975 Loss: 0.03147083855411633\n",
      "Iteration: 7924 lambda_n: 1.0038820645098858 Loss: 0.031463336969365485\n",
      "Iteration: 7925 lambda_n: 0.8934637781957799 Loss: 0.03145499825172531\n",
      "Iteration: 7926 lambda_n: 0.8864365589149471 Loss: 0.031447576718474736\n",
      "Iteration: 7927 lambda_n: 0.8891007146780479 Loss: 0.03144021355492573\n",
      "Iteration: 7928 lambda_n: 0.9848022866199856 Loss: 0.03143282825993735\n",
      "Iteration: 7929 lambda_n: 0.8963013764885686 Loss: 0.031424648020130466\n",
      "Iteration: 7930 lambda_n: 0.9415420698262005 Loss: 0.03141720290941577\n",
      "Iteration: 7931 lambda_n: 1.0120164219895447 Loss: 0.03140938200592966\n",
      "Iteration: 7932 lambda_n: 1.0151856295633503 Loss: 0.031400975706396425\n",
      "Iteration: 7933 lambda_n: 0.9584694102722012 Loss: 0.03139254307971919\n",
      "Iteration: 7934 lambda_n: 0.9282171286536317 Loss: 0.03138458156355931\n",
      "Iteration: 7935 lambda_n: 0.8991845148242018 Loss: 0.031376871335799\n",
      "Iteration: 7936 lambda_n: 0.9438522749673774 Loss: 0.031369402265460866\n",
      "Iteration: 7937 lambda_n: 0.984527590138718 Loss: 0.03136156216090141\n",
      "Iteration: 7938 lambda_n: 1.0216008133385968 Loss: 0.03135338418510286\n",
      "Iteration: 7939 lambda_n: 0.9994104249320632 Loss: 0.031344898258602864\n",
      "Iteration: 7940 lambda_n: 0.8904166146617193 Loss: 0.031336596654454194\n",
      "Iteration: 7941 lambda_n: 0.9660911942961313 Loss: 0.03132920040572895\n",
      "Iteration: 7942 lambda_n: 0.9849481230896203 Loss: 0.03132117556409365\n",
      "Iteration: 7943 lambda_n: 0.9919938933724545 Loss: 0.03131299408535001\n",
      "Iteration: 7944 lambda_n: 1.0114135576288064 Loss: 0.03130475407889109\n",
      "Iteration: 7945 lambda_n: 0.8767119404415454 Loss: 0.031296352760793435\n",
      "Iteration: 7946 lambda_n: 0.8774299732342973 Loss: 0.03128907034141657\n",
      "Iteration: 7947 lambda_n: 1.025855448701142 Loss: 0.031281781956156274\n",
      "Iteration: 7948 lambda_n: 0.8912668763699754 Loss: 0.03127326067066641\n",
      "Iteration: 7949 lambda_n: 0.8801156132251666 Loss: 0.03126585734559736\n",
      "Iteration: 7950 lambda_n: 0.9220607259294598 Loss: 0.031258546647158235\n",
      "Iteration: 7951 lambda_n: 0.9714727746186843 Loss: 0.031250887529184056\n",
      "Iteration: 7952 lambda_n: 0.9427125878511905 Loss: 0.031242817967173084\n",
      "Iteration: 7953 lambda_n: 0.9562189397995252 Loss: 0.031234987300560055\n",
      "Iteration: 7954 lambda_n: 0.8938042663718471 Loss: 0.031227044441332792\n",
      "Iteration: 7955 lambda_n: 0.885601478116517 Loss: 0.031219620029672396\n",
      "Iteration: 7956 lambda_n: 0.9752089851677092 Loss: 0.031212263753190504\n",
      "Iteration: 7957 lambda_n: 0.9859491499466344 Loss: 0.03120416314748102\n",
      "Iteration: 7958 lambda_n: 0.8777667881228142 Loss: 0.03119597332639814\n",
      "Iteration: 7959 lambda_n: 0.9420858761478115 Loss: 0.031188682124235537\n",
      "Iteration: 7960 lambda_n: 1.027508555601475 Loss: 0.031180856651630437\n",
      "Iteration: 7961 lambda_n: 0.8945440741957281 Loss: 0.03117232161042038\n",
      "Iteration: 7962 lambda_n: 1.0019387062411707 Loss: 0.031164891042298828\n",
      "Iteration: 7963 lambda_n: 0.9065881589144342 Loss: 0.03115656839445253\n",
      "Iteration: 7964 lambda_n: 0.9293995388432943 Loss: 0.03114903777841495\n",
      "Iteration: 7965 lambda_n: 0.9373454543526453 Loss: 0.031141317677038472\n",
      "Iteration: 7966 lambda_n: 0.9045442497014861 Loss: 0.031133531570923016\n",
      "Iteration: 7967 lambda_n: 0.8992388479572828 Loss: 0.03112601792806117\n",
      "Iteration: 7968 lambda_n: 0.9552766701096965 Loss: 0.03111854835329283\n",
      "Iteration: 7969 lambda_n: 0.9730969505859871 Loss: 0.031110613295835195\n",
      "Iteration: 7970 lambda_n: 0.934967310120669 Loss: 0.031102530211554704\n",
      "Iteration: 7971 lambda_n: 0.9032568167567135 Loss: 0.03109476385161184\n",
      "Iteration: 7972 lambda_n: 0.8757396161412692 Loss: 0.03108726089518307\n",
      "Iteration: 7973 lambda_n: 0.9681947682040561 Loss: 0.031079986510589203\n",
      "Iteration: 7974 lambda_n: 1.0188473213223062 Loss: 0.031071944140106698\n",
      "Iteration: 7975 lambda_n: 0.9232419311304854 Loss: 0.0310634810192534\n",
      "Iteration: 7976 lambda_n: 0.8825805748941921 Loss: 0.03105581204903838\n",
      "Iteration: 7977 lambda_n: 0.9673921533346134 Loss: 0.031048480833604626\n",
      "Iteration: 7978 lambda_n: 1.0275713268081244 Loss: 0.031040445123520225\n",
      "Iteration: 7979 lambda_n: 0.9007282451536619 Loss: 0.031031909529196853\n",
      "Iteration: 7980 lambda_n: 0.9232585522975353 Loss: 0.03102442756432751\n",
      "Iteration: 7981 lambda_n: 0.9220119804495175 Loss: 0.031016758448366773\n",
      "Iteration: 7982 lambda_n: 0.915574914478375 Loss: 0.031009099685668503\n",
      "Iteration: 7983 lambda_n: 0.9527404661315764 Loss: 0.031001494391487926\n",
      "Iteration: 7984 lambda_n: 0.9736567397594094 Loss: 0.030993580377261652\n",
      "Iteration: 7985 lambda_n: 1.000934842108058 Loss: 0.030985492593860516\n",
      "Iteration: 7986 lambda_n: 0.9303007582840167 Loss: 0.030977178231041392\n",
      "Iteration: 7987 lambda_n: 0.9436779976221745 Loss: 0.030969450601827878\n",
      "Iteration: 7988 lambda_n: 0.8858189465949544 Loss: 0.0309616118553787\n",
      "Iteration: 7989 lambda_n: 0.8995385513620365 Loss: 0.03095425372100123\n",
      "Iteration: 7990 lambda_n: 0.9089709666125884 Loss: 0.03094678162331052\n",
      "Iteration: 7991 lambda_n: 0.9715139783119785 Loss: 0.03093923117374922\n",
      "Iteration: 7992 lambda_n: 0.9677721788308958 Loss: 0.030931161203957485\n",
      "Iteration: 7993 lambda_n: 0.9018834841064163 Loss: 0.03092312231443576\n",
      "Iteration: 7994 lambda_n: 0.8998757947339185 Loss: 0.03091563073412391\n",
      "Iteration: 7995 lambda_n: 0.9917502944874249 Loss: 0.030908155829547885\n",
      "Iteration: 7996 lambda_n: 0.8952916372311746 Loss: 0.030899917759140555\n",
      "Iteration: 7997 lambda_n: 0.9092529409037569 Loss: 0.030892480930466173\n",
      "Iteration: 7998 lambda_n: 0.9002518718520994 Loss: 0.030884928129465375\n",
      "Iteration: 7999 lambda_n: 0.9538103357489492 Loss: 0.030877450095371954\n",
      "Iteration: 8000 lambda_n: 1.0179586151310958 Loss: 0.030869527170992502\n",
      "Iteration: 8001 lambda_n: 0.9411688871580718 Loss: 0.030861071390669216\n",
      "Iteration: 8002 lambda_n: 0.9586226220725217 Loss: 0.030853253470715998\n",
      "Iteration: 8003 lambda_n: 0.9865690273471937 Loss: 0.030845290567969395\n",
      "Iteration: 8004 lambda_n: 0.9109868014799672 Loss: 0.0308370955238378\n",
      "Iteration: 8005 lambda_n: 0.9898411612305265 Loss: 0.030829528310323734\n",
      "Iteration: 8006 lambda_n: 0.9135094250880407 Loss: 0.030821306082823623\n",
      "Iteration: 8007 lambda_n: 1.0015761561640135 Loss: 0.030813717912072845\n",
      "Iteration: 8008 lambda_n: 0.9778846387424562 Loss: 0.030805398203473066\n",
      "Iteration: 8009 lambda_n: 1.0259019420376247 Loss: 0.030797275314604747\n",
      "Iteration: 8010 lambda_n: 0.9026964961084303 Loss: 0.030788753553019403\n",
      "Iteration: 8011 lambda_n: 0.8810855037756911 Loss: 0.030781255202664743\n",
      "Iteration: 8012 lambda_n: 1.0221129926290016 Loss: 0.030773936362096473\n",
      "Iteration: 8013 lambda_n: 0.927774336365997 Loss: 0.030765446056476003\n",
      "Iteration: 8014 lambda_n: 0.8933188162909881 Loss: 0.030757739383452883\n",
      "Iteration: 8015 lambda_n: 0.9801941855204942 Loss: 0.03075031891754779\n",
      "Iteration: 8016 lambda_n: 0.9469867043980034 Loss: 0.030742176808654702\n",
      "Iteration: 8017 lambda_n: 0.9633747286543969 Loss: 0.03073431054034023\n",
      "Iteration: 8018 lambda_n: 0.9893748296744167 Loss: 0.030726308141306075\n",
      "Iteration: 8019 lambda_n: 1.0181342780351432 Loss: 0.03071808976755875\n",
      "Iteration: 8020 lambda_n: 1.00463046398752 Loss: 0.030709632498138704\n",
      "Iteration: 8021 lambda_n: 0.9026848776090495 Loss: 0.030701287398484396\n",
      "Iteration: 8022 lambda_n: 0.9513825502794914 Loss: 0.03069378912241118\n",
      "Iteration: 8023 lambda_n: 0.9344724953710931 Loss: 0.030685886331193468\n",
      "Iteration: 8024 lambda_n: 0.9405979871496707 Loss: 0.030678124004422228\n",
      "Iteration: 8025 lambda_n: 0.9695558136974901 Loss: 0.030670310794138192\n",
      "Iteration: 8026 lambda_n: 0.9137264521046351 Loss: 0.030662257040266875\n",
      "Iteration: 8027 lambda_n: 1.0221840727733467 Loss: 0.03065466703969695\n",
      "Iteration: 8028 lambda_n: 0.9794975132742719 Loss: 0.030646176118926016\n",
      "Iteration: 8029 lambda_n: 0.95486211360659 Loss: 0.030638039778847773\n",
      "Iteration: 8030 lambda_n: 0.8890523401299455 Loss: 0.030630108075007562\n",
      "Iteration: 8031 lambda_n: 0.905061914456234 Loss: 0.030622723028610407\n",
      "Iteration: 8032 lambda_n: 0.9341809353169853 Loss: 0.03061520499514683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8033 lambda_n: 0.8867767597526143 Loss: 0.03060744507894455\n",
      "Iteration: 8034 lambda_n: 0.9440035012612953 Loss: 0.030600078931577412\n",
      "Iteration: 8035 lambda_n: 0.916135253868558 Loss: 0.03059223742031562\n",
      "Iteration: 8036 lambda_n: 0.9711452951903053 Loss: 0.030584627399754\n",
      "Iteration: 8037 lambda_n: 0.9898136150186633 Loss: 0.030576560428450857\n",
      "Iteration: 8038 lambda_n: 0.9020830406373646 Loss: 0.030568338384471688\n",
      "Iteration: 8039 lambda_n: 1.0155525031392336 Loss: 0.03056084508720124\n",
      "Iteration: 8040 lambda_n: 1.0179069739142614 Loss: 0.030552409236400267\n",
      "Iteration: 8041 lambda_n: 0.9645105835720024 Loss: 0.03054395382638411\n",
      "Iteration: 8042 lambda_n: 0.8897043832229584 Loss: 0.030535941960831196\n",
      "Iteration: 8043 lambda_n: 1.013317966128115 Loss: 0.030528551484093072\n",
      "Iteration: 8044 lambda_n: 1.0194767741288706 Loss: 0.03052013418944007\n",
      "Iteration: 8045 lambda_n: 1.0069814143006985 Loss: 0.03051166573422061\n",
      "Iteration: 8046 lambda_n: 0.9814597492152064 Loss: 0.030503301072421708\n",
      "Iteration: 8047 lambda_n: 0.9928933027410144 Loss: 0.03049514840933027\n",
      "Iteration: 8048 lambda_n: 0.9856717037698001 Loss: 0.030486900770167885\n",
      "Iteration: 8049 lambda_n: 0.9327240694527946 Loss: 0.030478713117155085\n",
      "Iteration: 8050 lambda_n: 0.9090401126849601 Loss: 0.03047096528163083\n",
      "Iteration: 8051 lambda_n: 0.9525242641871874 Loss: 0.030463414179911375\n",
      "Iteration: 8052 lambda_n: 0.9600916922879617 Loss: 0.03045550186829182\n",
      "Iteration: 8053 lambda_n: 0.980540669700862 Loss: 0.03044752669528877\n",
      "Iteration: 8054 lambda_n: 0.9644273871787181 Loss: 0.03043938165797397\n",
      "Iteration: 8055 lambda_n: 0.8866310610182252 Loss: 0.030431370467304625\n",
      "Iteration: 8056 lambda_n: 0.9622854160188982 Loss: 0.030424005504791043\n",
      "Iteration: 8057 lambda_n: 0.9363854811986343 Loss: 0.030416012104486412\n",
      "Iteration: 8058 lambda_n: 0.9342313585114496 Loss: 0.03040823384558058\n",
      "Iteration: 8059 lambda_n: 0.9922310162767366 Loss: 0.030400473479170933\n",
      "Iteration: 8060 lambda_n: 1.012220334435157 Loss: 0.030392231326641237\n",
      "Iteration: 8061 lambda_n: 0.8800041095218327 Loss: 0.0303838231278238\n",
      "Iteration: 8062 lambda_n: 0.9433970805537603 Loss: 0.030376513206846544\n",
      "Iteration: 8063 lambda_n: 0.9342667171199812 Loss: 0.03036867669908982\n",
      "Iteration: 8064 lambda_n: 1.0116458533481092 Loss: 0.03036091603328715\n",
      "Iteration: 8065 lambda_n: 0.9359504042891349 Loss: 0.030352512575918197\n",
      "Iteration: 8066 lambda_n: 0.9106633194384426 Loss: 0.030344737932745933\n",
      "Iteration: 8067 lambda_n: 0.9604025072255845 Loss: 0.030337173336513715\n",
      "Iteration: 8068 lambda_n: 0.9930869673419465 Loss: 0.030329195568751716\n",
      "Iteration: 8069 lambda_n: 0.9481284921571138 Loss: 0.03032094629827358\n",
      "Iteration: 8070 lambda_n: 1.0056955728100903 Loss: 0.03031307048185852\n",
      "Iteration: 8071 lambda_n: 0.9005012709681356 Loss: 0.03030471647127172\n",
      "Iteration: 8072 lambda_n: 0.925258618715307 Loss: 0.030297236276593847\n",
      "Iteration: 8073 lambda_n: 0.9468044830278417 Loss: 0.030289550428803252\n",
      "Iteration: 8074 lambda_n: 1.0262357797408972 Loss: 0.030281685604731948\n",
      "Iteration: 8075 lambda_n: 0.9518577054056556 Loss: 0.030273160967169255\n",
      "Iteration: 8076 lambda_n: 0.9337610003960869 Loss: 0.030265254165078727\n",
      "Iteration: 8077 lambda_n: 1.0032358178380625 Loss: 0.030257497685909587\n",
      "Iteration: 8078 lambda_n: 0.9226794383713328 Loss: 0.030249164098652068\n",
      "Iteration: 8079 lambda_n: 0.9734499798699291 Loss: 0.03024149966863514\n",
      "Iteration: 8080 lambda_n: 0.9689979660523785 Loss: 0.030233413501421235\n",
      "Iteration: 8081 lambda_n: 0.998518122448059 Loss: 0.03022536431469032\n",
      "Iteration: 8082 lambda_n: 0.9390193029884764 Loss: 0.030217069911401488\n",
      "Iteration: 8083 lambda_n: 0.9373281133227499 Loss: 0.03020926974662833\n",
      "Iteration: 8084 lambda_n: 0.9988229174699761 Loss: 0.030201483629064833\n",
      "Iteration: 8085 lambda_n: 0.9321164188908011 Loss: 0.03019318669060551\n",
      "Iteration: 8086 lambda_n: 1.0252023104365302 Loss: 0.030185443863024696\n",
      "Iteration: 8087 lambda_n: 0.929636826606545 Loss: 0.030176927796165126\n",
      "Iteration: 8088 lambda_n: 0.9934882575617551 Loss: 0.0301692055637774\n",
      "Iteration: 8089 lambda_n: 0.9642519893509927 Loss: 0.030160952934412316\n",
      "Iteration: 8090 lambda_n: 1.0261302140398583 Loss: 0.030152943161474138\n",
      "Iteration: 8091 lambda_n: 1.0200654212748423 Loss: 0.030144419382228713\n",
      "Iteration: 8092 lambda_n: 1.0246167212180786 Loss: 0.030135945980360675\n",
      "Iteration: 8093 lambda_n: 0.9400356275816719 Loss: 0.030127434770932233\n",
      "Iteration: 8094 lambda_n: 0.9521559950663939 Loss: 0.030119626152324133\n",
      "Iteration: 8095 lambda_n: 1.011082867827757 Loss: 0.03011171685214975\n",
      "Iteration: 8096 lambda_n: 0.9631047450042934 Loss: 0.030103318061452\n",
      "Iteration: 8097 lambda_n: 0.9665512277523292 Loss: 0.030095317810915272\n",
      "Iteration: 8098 lambda_n: 1.006760603555169 Loss: 0.03008728893035989\n",
      "Iteration: 8099 lambda_n: 1.029646736401092 Loss: 0.030078926040295904\n",
      "Iteration: 8100 lambda_n: 1.021991539017256 Loss: 0.030070373040144933\n",
      "Iteration: 8101 lambda_n: 0.9115999558288254 Loss: 0.03006188362853421\n",
      "Iteration: 8102 lambda_n: 1.0235360430374278 Loss: 0.030054311209409865\n",
      "Iteration: 8103 lambda_n: 0.8992974836125798 Loss: 0.030045808965889508\n",
      "Iteration: 8104 lambda_n: 0.9306106093202378 Loss: 0.030038338738278488\n",
      "Iteration: 8105 lambda_n: 0.9808644577910212 Loss: 0.030030608399880615\n",
      "Iteration: 8106 lambda_n: 0.8793254484503658 Loss: 0.030022460614969864\n",
      "Iteration: 8107 lambda_n: 0.9274373887691817 Loss: 0.03001515628718378\n",
      "Iteration: 8108 lambda_n: 0.8778376058791664 Loss: 0.03000745230516728\n",
      "Iteration: 8109 lambda_n: 0.9306743927352501 Loss: 0.030000160334839786\n",
      "Iteration: 8110 lambda_n: 0.9411143710507037 Loss: 0.029992429462115503\n",
      "Iteration: 8111 lambda_n: 0.9686285635294629 Loss: 0.029984611866267948\n",
      "Iteration: 8112 lambda_n: 1.0105562847361653 Loss: 0.029976565716133233\n",
      "Iteration: 8113 lambda_n: 1.0158846690969716 Loss: 0.029968171282116547\n",
      "Iteration: 8114 lambda_n: 0.9092682968371115 Loss: 0.02995973258551279\n",
      "Iteration: 8115 lambda_n: 0.9702650756395922 Loss: 0.029952179523160954\n",
      "Iteration: 8116 lambda_n: 0.9545658973313612 Loss: 0.0299441197750586\n",
      "Iteration: 8117 lambda_n: 1.017040450190817 Loss: 0.029936190435145473\n",
      "Iteration: 8118 lambda_n: 0.9858756834574746 Loss: 0.029927742133783946\n",
      "Iteration: 8119 lambda_n: 0.9514535577817713 Loss: 0.029919552709334937\n",
      "Iteration: 8120 lambda_n: 0.8900507979725957 Loss: 0.02991164921996419\n",
      "Iteration: 8121 lambda_n: 0.9740296148962282 Loss: 0.02990425578727561\n",
      "Iteration: 8122 lambda_n: 1.0017187744992775 Loss: 0.02989616476238243\n",
      "Iteration: 8123 lambda_n: 0.9432477793223916 Loss: 0.02988784370379194\n",
      "Iteration: 8124 lambda_n: 0.937777499647732 Loss: 0.02988000836045778\n",
      "Iteration: 8125 lambda_n: 0.9421623696643845 Loss: 0.029872218462512725\n",
      "Iteration: 8126 lambda_n: 0.977202386216853 Loss: 0.029864392143253527\n",
      "Iteration: 8127 lambda_n: 0.9058135840622161 Loss: 0.02985627475631667\n",
      "Iteration: 8128 lambda_n: 0.892731583801509 Loss: 0.029848750379580086\n",
      "Iteration: 8129 lambda_n: 0.9155032275232219 Loss: 0.02984133467177223\n",
      "Iteration: 8130 lambda_n: 1.0102609496387154 Loss: 0.02983372982833551\n",
      "Iteration: 8131 lambda_n: 0.976019421149093 Loss: 0.029825337846787675\n",
      "Iteration: 8132 lambda_n: 0.9792281368198906 Loss: 0.029817230293210595\n",
      "Iteration: 8133 lambda_n: 1.0189722740090712 Loss: 0.02980909608077259\n",
      "Iteration: 8134 lambda_n: 0.9296830621677561 Loss: 0.029800631719931907\n",
      "Iteration: 8135 lambda_n: 0.9225424579286755 Loss: 0.029792909037322037\n",
      "Iteration: 8136 lambda_n: 0.8924882921541972 Loss: 0.029785245678133936\n",
      "Iteration: 8137 lambda_n: 0.9563024501299733 Loss: 0.0297778319769216\n",
      "Iteration: 8138 lambda_n: 0.9839504597295978 Loss: 0.029769888188450572\n",
      "Iteration: 8139 lambda_n: 0.917886881435665 Loss: 0.029761714735841743\n",
      "Iteration: 8140 lambda_n: 1.0103199818953046 Loss: 0.029754090059377455\n",
      "Iteration: 8141 lambda_n: 0.9256374552854517 Loss: 0.029745697562164483\n",
      "Iteration: 8142 lambda_n: 0.9667385239798216 Loss: 0.029738008526155924\n",
      "Iteration: 8143 lambda_n: 0.9141140271008662 Loss: 0.029729978063385686\n",
      "Iteration: 8144 lambda_n: 1.0201488442054847 Loss: 0.02972238473268575\n",
      "Iteration: 8145 lambda_n: 0.8938356680531785 Loss: 0.029713910590988334\n",
      "Iteration: 8146 lambda_n: 0.9666289114497989 Loss: 0.02970648570064712\n",
      "Iteration: 8147 lambda_n: 0.8923659013100335 Loss: 0.02969845613131449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8148 lambda_n: 0.9147913133499945 Loss: 0.029691043446618784\n",
      "Iteration: 8149 lambda_n: 0.9897159553002964 Loss: 0.029683444477917587\n",
      "Iteration: 8150 lambda_n: 0.9190973710707534 Loss: 0.029675223125812382\n",
      "Iteration: 8151 lambda_n: 0.9340609393925803 Loss: 0.029667588385890828\n",
      "Iteration: 8152 lambda_n: 0.9760690829776955 Loss: 0.02965982934653896\n",
      "Iteration: 8153 lambda_n: 0.9388663246454437 Loss: 0.02965172132843278\n",
      "Iteration: 8154 lambda_n: 0.933643473434497 Loss: 0.02964392235566781\n",
      "Iteration: 8155 lambda_n: 0.9926720459111077 Loss: 0.029636166773208527\n",
      "Iteration: 8156 lambda_n: 0.91475298950776 Loss: 0.02962792085574461\n",
      "Iteration: 8157 lambda_n: 1.0141476261264393 Loss: 0.029620322197026636\n",
      "Iteration: 8158 lambda_n: 0.8812919911540408 Loss: 0.02961189788877606\n",
      "Iteration: 8159 lambda_n: 0.8792675629729275 Loss: 0.029604577184009006\n",
      "Iteration: 8160 lambda_n: 0.9757170438685808 Loss: 0.02959727329543693\n",
      "Iteration: 8161 lambda_n: 0.9803330664232671 Loss: 0.029589168221038242\n",
      "Iteration: 8162 lambda_n: 0.9086200527045876 Loss: 0.02958102480162909\n",
      "Iteration: 8163 lambda_n: 0.8814013555943503 Loss: 0.029573477086336637\n",
      "Iteration: 8164 lambda_n: 0.9870255916264007 Loss: 0.029566155470344437\n",
      "Iteration: 8165 lambda_n: 1.0122752495360654 Loss: 0.02955795645524774\n",
      "Iteration: 8166 lambda_n: 0.9968851730766208 Loss: 0.029549547695653315\n",
      "Iteration: 8167 lambda_n: 1.01802831115824 Loss: 0.029541266777337538\n",
      "Iteration: 8168 lambda_n: 0.8904773091544494 Loss: 0.029532810226480872\n",
      "Iteration: 8169 lambda_n: 0.9473303855184254 Loss: 0.02952541321466645\n",
      "Iteration: 8170 lambda_n: 0.9779350562642146 Loss: 0.02951754393532535\n",
      "Iteration: 8171 lambda_n: 0.8919500677646489 Loss: 0.02950942042847595\n",
      "Iteration: 8172 lambda_n: 0.8790765214481587 Loss: 0.029502011180640716\n",
      "Iteration: 8173 lambda_n: 1.0260448049492785 Loss: 0.029494708870086563\n",
      "Iteration: 8174 lambda_n: 0.9889830802274154 Loss: 0.029486185723036788\n",
      "Iteration: 8175 lambda_n: 1.0206875366128845 Loss: 0.029477970439421745\n",
      "Iteration: 8176 lambda_n: 0.8808180817435061 Loss: 0.029469491792442043\n",
      "Iteration: 8177 lambda_n: 1.0244740794376714 Loss: 0.029462175012289784\n",
      "Iteration: 8178 lambda_n: 0.91046970809632 Loss: 0.029453664909689897\n",
      "Iteration: 8179 lambda_n: 0.9459428988399745 Loss: 0.029446101818003754\n",
      "Iteration: 8180 lambda_n: 0.9525417157324874 Loss: 0.029438244056859228\n",
      "Iteration: 8181 lambda_n: 0.8979982623279217 Loss: 0.02943033147993275\n",
      "Iteration: 8182 lambda_n: 1.0207033900394007 Loss: 0.02942287198408769\n",
      "Iteration: 8183 lambda_n: 1.0038176365369404 Loss: 0.029414393200041848\n",
      "Iteration: 8184 lambda_n: 0.9394611162324402 Loss: 0.029406054681860486\n",
      "Iteration: 8185 lambda_n: 0.8777097278786923 Loss: 0.02939825076006558\n",
      "Iteration: 8186 lambda_n: 0.9963189208798903 Loss: 0.029390959794474727\n",
      "Iteration: 8187 lambda_n: 0.8954861735388869 Loss: 0.02938268356450097\n",
      "Iteration: 8188 lambda_n: 0.9837900567261004 Loss: 0.029375244932116136\n",
      "Iteration: 8189 lambda_n: 0.9977763818422419 Loss: 0.029367072775591602\n",
      "Iteration: 8190 lambda_n: 0.9458899982727385 Loss: 0.029358784436585794\n",
      "Iteration: 8191 lambda_n: 0.9627173519109534 Loss: 0.02935092710720681\n",
      "Iteration: 8192 lambda_n: 0.9780184288359571 Loss: 0.02934292999549866\n",
      "Iteration: 8193 lambda_n: 0.9700405624767507 Loss: 0.029334805779922257\n",
      "Iteration: 8194 lambda_n: 0.9577992760953734 Loss: 0.02932674783427799\n",
      "Iteration: 8195 lambda_n: 0.8933197076290209 Loss: 0.02931879157402245\n",
      "Iteration: 8196 lambda_n: 0.9236936401631622 Loss: 0.029311370932904528\n",
      "Iteration: 8197 lambda_n: 0.8820146626861005 Loss: 0.029303697980556032\n",
      "Iteration: 8198 lambda_n: 0.9609260974474587 Loss: 0.02929637124717302\n",
      "Iteration: 8199 lambda_n: 0.9548155616592144 Loss: 0.02928838901041458\n",
      "Iteration: 8200 lambda_n: 0.8785992600309153 Loss: 0.029280457532088567\n",
      "Iteration: 8201 lambda_n: 0.8905192973999294 Loss: 0.029273159168039766\n",
      "Iteration: 8202 lambda_n: 0.9400445348061892 Loss: 0.029265761785847257\n",
      "Iteration: 8203 lambda_n: 0.9324397419138521 Loss: 0.029257953005894375\n",
      "Iteration: 8204 lambda_n: 0.929250642972226 Loss: 0.02925020739695354\n",
      "Iteration: 8205 lambda_n: 0.9904114091668235 Loss: 0.02924248827866553\n",
      "Iteration: 8206 lambda_n: 0.9492539737610907 Loss: 0.02923426110821833\n",
      "Iteration: 8207 lambda_n: 0.917140363736685 Loss: 0.029226375824561418\n",
      "Iteration: 8208 lambda_n: 1.0201342488373717 Loss: 0.02921875730232913\n",
      "Iteration: 8209 lambda_n: 0.9998115794014836 Loss: 0.029210283227516703\n",
      "Iteration: 8210 lambda_n: 0.9527246177355148 Loss: 0.029201977968813176\n",
      "Iteration: 8211 lambda_n: 0.9875943604445243 Loss: 0.029194063852544735\n",
      "Iteration: 8212 lambda_n: 1.0182587829331473 Loss: 0.02918586007878935\n",
      "Iteration: 8213 lambda_n: 0.9369670769107462 Loss: 0.02917740158034221\n",
      "Iteration: 8214 lambda_n: 0.9393504665057117 Loss: 0.02916961835728993\n",
      "Iteration: 8215 lambda_n: 0.971140150011365 Loss: 0.02916181533523189\n",
      "Iteration: 8216 lambda_n: 0.9097841174189863 Loss: 0.029153748241143494\n",
      "Iteration: 8217 lambda_n: 1.0149634585726444 Loss: 0.029146190820458745\n",
      "Iteration: 8218 lambda_n: 0.8928296705751814 Loss: 0.029137759692396605\n",
      "Iteration: 8219 lambda_n: 0.9233350888443073 Loss: 0.02913034310823687\n",
      "Iteration: 8220 lambda_n: 1.0038173750780788 Loss: 0.029122673120206048\n",
      "Iteration: 8221 lambda_n: 1.006586352680294 Loss: 0.029114334578851344\n",
      "Iteration: 8222 lambda_n: 0.9533580946462031 Loss: 0.029105973035396023\n",
      "Iteration: 8223 lambda_n: 1.0091032993992701 Loss: 0.029098053649488333\n",
      "Iteration: 8224 lambda_n: 0.9644965948277668 Loss: 0.029089671196874223\n",
      "Iteration: 8225 lambda_n: 0.964255008554799 Loss: 0.029081659284067977\n",
      "Iteration: 8226 lambda_n: 0.8941578816094669 Loss: 0.02907364937747115\n",
      "Iteration: 8227 lambda_n: 0.9514441736389361 Loss: 0.02906622175553278\n",
      "Iteration: 8228 lambda_n: 1.0091856900303648 Loss: 0.02905831826527091\n",
      "Iteration: 8229 lambda_n: 0.9946530683931855 Loss: 0.02904993512510995\n",
      "Iteration: 8230 lambda_n: 0.9948418316286376 Loss: 0.02904167270441029\n",
      "Iteration: 8231 lambda_n: 0.9148029123278157 Loss: 0.02903340871505193\n",
      "Iteration: 8232 lambda_n: 0.9829300010916665 Loss: 0.029025809595403428\n",
      "Iteration: 8233 lambda_n: 0.9564596690106899 Loss: 0.02901764455449089\n",
      "Iteration: 8234 lambda_n: 0.9535357321259508 Loss: 0.0290096993977613\n",
      "Iteration: 8235 lambda_n: 0.9910235211027366 Loss: 0.029001778529131345\n",
      "Iteration: 8236 lambda_n: 0.8935472478454878 Loss: 0.02899354625484939\n",
      "Iteration: 8237 lambda_n: 0.8795083485799329 Loss: 0.02898612369986474\n",
      "Iteration: 8238 lambda_n: 0.9653622036507767 Loss: 0.028978817763296614\n",
      "Iteration: 8239 lambda_n: 0.9925657083474609 Loss: 0.02897079865182535\n",
      "Iteration: 8240 lambda_n: 0.9864072236632585 Loss: 0.028962553564542866\n",
      "Iteration: 8241 lambda_n: 0.9581756661439333 Loss: 0.02895435963422285\n",
      "Iteration: 8242 lambda_n: 0.9544253403794868 Loss: 0.028946400218451303\n",
      "Iteration: 8243 lambda_n: 0.9014221394896903 Loss: 0.028938471955493716\n",
      "Iteration: 8244 lambda_n: 0.9899766237744511 Loss: 0.028930983981365437\n",
      "Iteration: 8245 lambda_n: 0.9978553636384662 Loss: 0.028922760398309925\n",
      "Iteration: 8246 lambda_n: 0.9745948859211914 Loss: 0.028914471367183513\n",
      "Iteration: 8247 lambda_n: 0.9323499951983364 Loss: 0.028906375556687184\n",
      "Iteration: 8248 lambda_n: 1.00148145672422 Loss: 0.02889863066748746\n",
      "Iteration: 8249 lambda_n: 0.8832629319465689 Loss: 0.028890311513226935\n",
      "Iteration: 8250 lambda_n: 0.9382425349447355 Loss: 0.02888297438176136\n",
      "Iteration: 8251 lambda_n: 0.959012531228279 Loss: 0.028875180542568515\n",
      "Iteration: 8252 lambda_n: 0.9749313935338441 Loss: 0.028867214169621833\n",
      "Iteration: 8253 lambda_n: 0.962284620185959 Loss: 0.028859115560532164\n",
      "Iteration: 8254 lambda_n: 0.960218565820019 Loss: 0.028851122005748527\n",
      "Iteration: 8255 lambda_n: 0.917893640707966 Loss: 0.028843145612834496\n",
      "Iteration: 8256 lambda_n: 1.0116741977422197 Loss: 0.0288355208062737\n",
      "Iteration: 8257 lambda_n: 1.0026994026173874 Loss: 0.0288271169779782\n",
      "Iteration: 8258 lambda_n: 1.0195332518420475 Loss: 0.028818787701400565\n",
      "Iteration: 8259 lambda_n: 0.9480233897058679 Loss: 0.028810318587925963\n",
      "Iteration: 8260 lambda_n: 0.9573865382396445 Loss: 0.028802443495855805\n",
      "Iteration: 8261 lambda_n: 0.927941373424826 Loss: 0.028794490624961512\n",
      "Iteration: 8262 lambda_n: 0.8825305785618147 Loss: 0.02878678235027283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8263 lambda_n: 0.9930538868882444 Loss: 0.028779451296025174\n",
      "Iteration: 8264 lambda_n: 0.9961285069953277 Loss: 0.02877120214010472\n",
      "Iteration: 8265 lambda_n: 0.9899328883417213 Loss: 0.02876292744320669\n",
      "Iteration: 8266 lambda_n: 1.0149305109552222 Loss: 0.02875470421187971\n",
      "Iteration: 8267 lambda_n: 0.9287834035010694 Loss: 0.02874627332831281\n",
      "Iteration: 8268 lambda_n: 0.9030957331218522 Loss: 0.028738558056010985\n",
      "Iteration: 8269 lambda_n: 0.9831800571858555 Loss: 0.028731056167093358\n",
      "Iteration: 8270 lambda_n: 0.9160382018903475 Loss: 0.02872288902850467\n",
      "Iteration: 8271 lambda_n: 1.0259300033718415 Loss: 0.028715279627385085\n",
      "Iteration: 8272 lambda_n: 1.0162828588099957 Loss: 0.02870675736994583\n",
      "Iteration: 8273 lambda_n: 0.8784005387533084 Loss: 0.028698315249429916\n",
      "Iteration: 8274 lambda_n: 0.9968130328825536 Loss: 0.02869101849771201\n",
      "Iteration: 8275 lambda_n: 1.0222078031865995 Loss: 0.028682738109325524\n",
      "Iteration: 8276 lambda_n: 0.9751467566882563 Loss: 0.0286742467695433\n",
      "Iteration: 8277 lambda_n: 0.9166412409537353 Loss: 0.028666146358881203\n",
      "Iteration: 8278 lambda_n: 0.9995992683000597 Loss: 0.028658531945061006\n",
      "Iteration: 8279 lambda_n: 0.9035807562077806 Loss: 0.02865022840973739\n",
      "Iteration: 8280 lambda_n: 0.9695017618667775 Loss: 0.028642722486677093\n",
      "Iteration: 8281 lambda_n: 1.0008140212113634 Loss: 0.028634668966288544\n",
      "Iteration: 8282 lambda_n: 0.9392122428741119 Loss: 0.02862635533868178\n",
      "Iteration: 8283 lambda_n: 0.9854829026973002 Loss: 0.02861855342828568\n",
      "Iteration: 8284 lambda_n: 1.0150244207349037 Loss: 0.028610367153236064\n",
      "Iteration: 8285 lambda_n: 1.0010880891625078 Loss: 0.028601935480224078\n",
      "Iteration: 8286 lambda_n: 0.8888268854128278 Loss: 0.02859361957394916\n",
      "Iteration: 8287 lambda_n: 0.9721921667839619 Loss: 0.028586236206185504\n",
      "Iteration: 8288 lambda_n: 0.8897708856805046 Loss: 0.028578160333583433\n",
      "Iteration: 8289 lambda_n: 0.9811031992263184 Loss: 0.028570769123299318\n",
      "Iteration: 8290 lambda_n: 0.8936132585416988 Loss: 0.028562619227004194\n",
      "Iteration: 8291 lambda_n: 1.0010639106739925 Loss: 0.028555196097795672\n",
      "Iteration: 8292 lambda_n: 0.9896288321465408 Loss: 0.028546880389539606\n",
      "Iteration: 8293 lambda_n: 0.9003709796800816 Loss: 0.028538659670509357\n",
      "Iteration: 8294 lambda_n: 0.9774354546319223 Loss: 0.02853118040450417\n",
      "Iteration: 8295 lambda_n: 0.8893398851938266 Loss: 0.028523060973378873\n",
      "Iteration: 8296 lambda_n: 0.9331586663902541 Loss: 0.02851567334043961\n",
      "Iteration: 8297 lambda_n: 0.9283353208550802 Loss: 0.02850792171007271\n",
      "Iteration: 8298 lambda_n: 1.0294716506053783 Loss: 0.028500210146203568\n",
      "Iteration: 8299 lambda_n: 0.8901327178444576 Loss: 0.02849165845519828\n",
      "Iteration: 8300 lambda_n: 0.8926372211352916 Loss: 0.02848426423467096\n",
      "Iteration: 8301 lambda_n: 0.896891157505387 Loss: 0.02847684920916921\n",
      "Iteration: 8302 lambda_n: 0.9864033871601969 Loss: 0.02846939884636926\n",
      "Iteration: 8303 lambda_n: 1.0288517368821282 Loss: 0.028461204916239952\n",
      "Iteration: 8304 lambda_n: 1.0020218053463479 Loss: 0.028452658372474026\n",
      "Iteration: 8305 lambda_n: 1.0279180534802985 Loss: 0.028444334701135377\n",
      "Iteration: 8306 lambda_n: 0.9884164185345504 Loss: 0.02843579591237962\n",
      "Iteration: 8307 lambda_n: 0.9122313593634679 Loss: 0.028427585258368547\n",
      "Iteration: 8308 lambda_n: 0.9195591243646 Loss: 0.028420007463883998\n",
      "Iteration: 8309 lambda_n: 0.8850417375930947 Loss: 0.02841236879816201\n",
      "Iteration: 8310 lambda_n: 0.9281481264053553 Loss: 0.028405016863794944\n",
      "Iteration: 8311 lambda_n: 0.9853195619576762 Loss: 0.02839730684950709\n",
      "Iteration: 8312 lambda_n: 1.024157709442441 Loss: 0.028389121918596744\n",
      "Iteration: 8313 lambda_n: 0.9900016639407798 Loss: 0.028380614363413662\n",
      "Iteration: 8314 lambda_n: 1.0065571951029544 Loss: 0.028372390537940932\n",
      "Iteration: 8315 lambda_n: 0.9843877965144316 Loss: 0.02836402918719959\n",
      "Iteration: 8316 lambda_n: 0.8878856103105327 Loss: 0.028355851994565886\n",
      "Iteration: 8317 lambda_n: 0.9316450959704927 Loss: 0.028348476433751435\n",
      "Iteration: 8318 lambda_n: 1.0002956429487446 Loss: 0.028340737367693145\n",
      "Iteration: 8319 lambda_n: 0.8967366548407977 Loss: 0.02833242802921466\n",
      "Iteration: 8320 lambda_n: 0.9550015175177119 Loss: 0.028324978942695696\n",
      "Iteration: 8321 lambda_n: 0.9195429587788965 Loss: 0.028317045856399277\n",
      "Iteration: 8322 lambda_n: 0.9014137875613086 Loss: 0.02830940731983167\n",
      "Iteration: 8323 lambda_n: 0.9508677196849759 Loss: 0.028301919379821405\n",
      "Iteration: 8324 lambda_n: 0.9291246003390209 Loss: 0.028294020631347328\n",
      "Iteration: 8325 lambda_n: 1.0296417916505773 Loss: 0.028286302500074384\n",
      "Iteration: 8326 lambda_n: 1.0004563654689451 Loss: 0.028277749383632606\n",
      "Iteration: 8327 lambda_n: 0.9692619266508282 Loss: 0.028269438706740133\n",
      "Iteration: 8328 lambda_n: 0.9291990711054762 Loss: 0.028261387158076617\n",
      "Iteration: 8329 lambda_n: 1.0116105875199832 Loss: 0.028253668406616427\n",
      "Iteration: 8330 lambda_n: 0.9983625906015331 Loss: 0.028245265071622887\n",
      "Iteration: 8331 lambda_n: 0.9254359583625543 Loss: 0.028236971785817445\n",
      "Iteration: 8332 lambda_n: 0.8863880469216718 Loss: 0.028229284292958224\n",
      "Iteration: 8333 lambda_n: 0.9570558946647049 Loss: 0.02822192116638188\n",
      "Iteration: 8334 lambda_n: 0.9053066969693329 Loss: 0.028213971009527553\n",
      "Iteration: 8335 lambda_n: 0.9011301185738598 Loss: 0.02820645072714121\n",
      "Iteration: 8336 lambda_n: 0.990745003034107 Loss: 0.028198965138790607\n",
      "Iteration: 8337 lambda_n: 0.8938091359216136 Loss: 0.028190735129126156\n",
      "Iteration: 8338 lambda_n: 1.014871992226585 Loss: 0.02818331035467322\n",
      "Iteration: 8339 lambda_n: 1.013085167424447 Loss: 0.02817487992398128\n",
      "Iteration: 8340 lambda_n: 0.9303083022428277 Loss: 0.028166464335828412\n",
      "Iteration: 8341 lambda_n: 0.9216605741514626 Loss: 0.028158736365696588\n",
      "Iteration: 8342 lambda_n: 0.9463740191720008 Loss: 0.028151080230957186\n",
      "Iteration: 8343 lambda_n: 1.029089083593557 Loss: 0.028143218803951042\n",
      "Iteration: 8344 lambda_n: 1.0104468138832727 Loss: 0.028134670271427218\n",
      "Iteration: 8345 lambda_n: 0.9191813940690554 Loss: 0.028126276597820986\n",
      "Iteration: 8346 lambda_n: 0.9445278689000864 Loss: 0.02811864105592383\n",
      "Iteration: 8347 lambda_n: 0.9956954494106418 Loss: 0.02811079496323592\n",
      "Iteration: 8348 lambda_n: 1.0226448457860813 Loss: 0.028102523826537006\n",
      "Iteration: 8349 lambda_n: 0.8769252128061309 Loss: 0.028094028823665688\n",
      "Iteration: 8350 lambda_n: 0.9958550157052591 Loss: 0.0280867442980686\n",
      "Iteration: 8351 lambda_n: 0.8761687452903364 Loss: 0.028078471834757285\n",
      "Iteration: 8352 lambda_n: 0.9338870908406735 Loss: 0.028071193592299058\n",
      "Iteration: 8353 lambda_n: 0.9779992039616338 Loss: 0.02806343588924231\n",
      "Iteration: 8354 lambda_n: 0.9651999402245193 Loss: 0.028055311751107398\n",
      "Iteration: 8355 lambda_n: 0.999188038161628 Loss: 0.02804729393478579\n",
      "Iteration: 8356 lambda_n: 0.9612291423562945 Loss: 0.028038993782487594\n",
      "Iteration: 8357 lambda_n: 0.8937809726573367 Loss: 0.02803100895048005\n",
      "Iteration: 8358 lambda_n: 0.991458347443773 Loss: 0.028023584403190315\n",
      "Iteration: 8359 lambda_n: 1.024557152854843 Loss: 0.028015348459592995\n",
      "Iteration: 8360 lambda_n: 1.0219054224389728 Loss: 0.028006837567211496\n",
      "Iteration: 8361 lambda_n: 0.9439887237385702 Loss: 0.027998348702098348\n",
      "Iteration: 8362 lambda_n: 1.012210741000249 Loss: 0.02799050708277372\n",
      "Iteration: 8363 lambda_n: 1.028286720968369 Loss: 0.027982098749666014\n",
      "Iteration: 8364 lambda_n: 0.9954205946677487 Loss: 0.027973556874622885\n",
      "Iteration: 8365 lambda_n: 0.8858491912202654 Loss: 0.02796528801483221\n",
      "Iteration: 8366 lambda_n: 0.9324311790852037 Loss: 0.027957929353460757\n",
      "Iteration: 8367 lambda_n: 1.0070581634185896 Loss: 0.02795018373983648\n",
      "Iteration: 8368 lambda_n: 0.9754729885546641 Loss: 0.027941818206891633\n",
      "Iteration: 8369 lambda_n: 0.9222820963935856 Loss: 0.027933715048528332\n",
      "Iteration: 8370 lambda_n: 1.0280873402698094 Loss: 0.027926053741361867\n",
      "Iteration: 8371 lambda_n: 0.9841787055846569 Loss: 0.02791751352004501\n",
      "Iteration: 8372 lambda_n: 0.99301444015092 Loss: 0.027909338043115847\n",
      "Iteration: 8373 lambda_n: 0.8906626039878424 Loss: 0.027901089168251406\n",
      "Iteration: 8374 lambda_n: 0.9010100414306718 Loss: 0.027893690519873258\n",
      "Iteration: 8375 lambda_n: 0.8921611248895448 Loss: 0.027886205916048002\n",
      "Iteration: 8376 lambda_n: 0.9152791021381196 Loss: 0.027878794819040657\n",
      "Iteration: 8377 lambda_n: 1.0279005155615393 Loss: 0.027871191682926136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8378 lambda_n: 1.026066080606131 Loss: 0.027862653011156736\n",
      "Iteration: 8379 lambda_n: 0.9013843300062642 Loss: 0.02785412957749913\n",
      "Iteration: 8380 lambda_n: 1.009553526785498 Loss: 0.027846641863004545\n",
      "Iteration: 8381 lambda_n: 1.0101150911727854 Loss: 0.027838255596897354\n",
      "Iteration: 8382 lambda_n: 0.8976192645247026 Loss: 0.02782986466557945\n",
      "Iteration: 8383 lambda_n: 0.9487117783648377 Loss: 0.027822408226233157\n",
      "Iteration: 8384 lambda_n: 0.8911215981032318 Loss: 0.02781452736586109\n",
      "Iteration: 8385 lambda_n: 0.9454724260639495 Loss: 0.027807124893990395\n",
      "Iteration: 8386 lambda_n: 0.8960851918377798 Loss: 0.027799270937135523\n",
      "Iteration: 8387 lambda_n: 1.0294451285593782 Loss: 0.027791827237414975\n",
      "Iteration: 8388 lambda_n: 0.8873721504539414 Loss: 0.027783275729625998\n",
      "Iteration: 8389 lambda_n: 0.9605277893373653 Loss: 0.027775904409770683\n",
      "Iteration: 8390 lambda_n: 0.9734718049384871 Loss: 0.02776792539281898\n",
      "Iteration: 8391 lambda_n: 0.9963189911671002 Loss: 0.027759838851084382\n",
      "Iteration: 8392 lambda_n: 0.9971682716274575 Loss: 0.02775156251969386\n",
      "Iteration: 8393 lambda_n: 0.9937277454250733 Loss: 0.027743279133163067\n",
      "Iteration: 8394 lambda_n: 1.0063554161471935 Loss: 0.02773502432648002\n",
      "Iteration: 8395 lambda_n: 0.8944494645114787 Loss: 0.027726664622556237\n",
      "Iteration: 8396 lambda_n: 0.9034750137194091 Loss: 0.027719234511007818\n",
      "Iteration: 8397 lambda_n: 0.8861866065039655 Loss: 0.027711729424759014\n",
      "Iteration: 8398 lambda_n: 0.9868687607199502 Loss: 0.027704367951495227\n",
      "Iteration: 8399 lambda_n: 0.8878004068748071 Loss: 0.027696170120245046\n",
      "Iteration: 8400 lambda_n: 0.9317212803993712 Loss: 0.027688795240729903\n",
      "Iteration: 8401 lambda_n: 0.9534596669213437 Loss: 0.02768105551412762\n",
      "Iteration: 8402 lambda_n: 0.9927950418400802 Loss: 0.02767313520837317\n",
      "Iteration: 8403 lambda_n: 1.0224021036831896 Loss: 0.027664888146805965\n",
      "Iteration: 8404 lambda_n: 0.9133366767150715 Loss: 0.027656395141640273\n",
      "Iteration: 8405 lambda_n: 0.9452178446229276 Loss: 0.02764880813317298\n",
      "Iteration: 8406 lambda_n: 1.0097171424214055 Loss: 0.027640956290339896\n",
      "Iteration: 8407 lambda_n: 0.9809015921240604 Loss: 0.02763256865712326\n",
      "Iteration: 8408 lambda_n: 0.9716427741692559 Loss: 0.027624420391889254\n",
      "Iteration: 8409 lambda_n: 0.9469760681456353 Loss: 0.027616349038565653\n",
      "Iteration: 8410 lambda_n: 0.942573838994444 Loss: 0.027608482589173946\n",
      "Iteration: 8411 lambda_n: 0.9580293048469488 Loss: 0.027600652708451447\n",
      "Iteration: 8412 lambda_n: 1.0013420590335886 Loss: 0.027592694440214165\n",
      "Iteration: 8413 lambda_n: 1.0276579245660058 Loss: 0.02758437637630907\n",
      "Iteration: 8414 lambda_n: 0.910939132254934 Loss: 0.02757583970842244\n",
      "Iteration: 8415 lambda_n: 0.9985474009679763 Loss: 0.02756827261343926\n",
      "Iteration: 8416 lambda_n: 0.9256531841584459 Loss: 0.02755997776364412\n",
      "Iteration: 8417 lambda_n: 0.9789199389227438 Loss: 0.02755228843974083\n",
      "Iteration: 8418 lambda_n: 0.9503388058188312 Loss: 0.027544156633075353\n",
      "Iteration: 8419 lambda_n: 0.9925031577197166 Loss: 0.027536262247236563\n",
      "Iteration: 8420 lambda_n: 0.9204308285104652 Loss: 0.027528017605339364\n",
      "Iteration: 8421 lambda_n: 0.964541628664809 Loss: 0.027520371662072277\n",
      "Iteration: 8422 lambda_n: 1.022840007843856 Loss: 0.0275123592937557\n",
      "Iteration: 8423 lambda_n: 0.9242566418786693 Loss: 0.027503862645293524\n",
      "Iteration: 8424 lambda_n: 0.9743695574224016 Loss: 0.027496184920534277\n",
      "Iteration: 8425 lambda_n: 1.0286160464224885 Loss: 0.02748809091160395\n",
      "Iteration: 8426 lambda_n: 0.8979510295119407 Loss: 0.027479546281201823\n",
      "Iteration: 8427 lambda_n: 0.9457004832565155 Loss: 0.027472087074277292\n",
      "Iteration: 8428 lambda_n: 0.9484722927162961 Loss: 0.027464231216244712\n",
      "Iteration: 8429 lambda_n: 0.9566918864487121 Loss: 0.027456352332761443\n",
      "Iteration: 8430 lambda_n: 0.8795254442040489 Loss: 0.02744840516951668\n",
      "Iteration: 8431 lambda_n: 0.9063470472827188 Loss: 0.0274410990215222\n",
      "Iteration: 8432 lambda_n: 0.9606219247508525 Loss: 0.027433570068380774\n",
      "Iteration: 8433 lambda_n: 0.9076214707201736 Loss: 0.02742559025789043\n",
      "Iteration: 8434 lambda_n: 0.9131221759761987 Loss: 0.027418050717747413\n",
      "Iteration: 8435 lambda_n: 0.91421097450966 Loss: 0.027410465483451652\n",
      "Iteration: 8436 lambda_n: 0.8998797054583293 Loss: 0.027402871204364184\n",
      "Iteration: 8437 lambda_n: 1.023531726632626 Loss: 0.027395395973784636\n",
      "Iteration: 8438 lambda_n: 0.889230888813867 Loss: 0.0273868935752537\n",
      "Iteration: 8439 lambda_n: 0.9890430719338368 Loss: 0.02737950680310227\n",
      "Iteration: 8440 lambda_n: 0.9015645142948129 Loss: 0.027371290898637428\n",
      "Iteration: 8441 lambda_n: 0.9120080139016311 Loss: 0.027363801671560696\n",
      "Iteration: 8442 lambda_n: 0.9587391601251632 Loss: 0.027356225690917318\n",
      "Iteration: 8443 lambda_n: 0.991772644548952 Loss: 0.027348261517993935\n",
      "Iteration: 8444 lambda_n: 1.0047864259762846 Loss: 0.027340022938189556\n",
      "Iteration: 8445 lambda_n: 0.9688480753311532 Loss: 0.027331676253632465\n",
      "Iteration: 8446 lambda_n: 0.9256875948268367 Loss: 0.027323628105970933\n",
      "Iteration: 8447 lambda_n: 0.9333432712008916 Loss: 0.027315938488921786\n",
      "Iteration: 8448 lambda_n: 0.9013398112612891 Loss: 0.027308185276522276\n",
      "Iteration: 8449 lambda_n: 0.8872769523587863 Loss: 0.027300697914241587\n",
      "Iteration: 8450 lambda_n: 0.9702698643437168 Loss: 0.027293327370870434\n",
      "Iteration: 8451 lambda_n: 1.0149058035985847 Loss: 0.027285267411358217\n",
      "Iteration: 8452 lambda_n: 0.9000970227168448 Loss: 0.02727683666417142\n",
      "Iteration: 8453 lambda_n: 0.9900716397503203 Loss: 0.02726935962477403\n",
      "Iteration: 8454 lambda_n: 1.012883183641635 Loss: 0.0272611351726625\n",
      "Iteration: 8455 lambda_n: 0.993897657871509 Loss: 0.02725272122648534\n",
      "Iteration: 8456 lambda_n: 1.024711097259774 Loss: 0.02724446499142314\n",
      "Iteration: 8457 lambda_n: 0.9608888887892737 Loss: 0.02723595279112232\n",
      "Iteration: 8458 lambda_n: 1.007249025751062 Loss: 0.027227970757004494\n",
      "Iteration: 8459 lambda_n: 0.9896463367628976 Loss: 0.027219603612361334\n",
      "Iteration: 8460 lambda_n: 0.9807140976362478 Loss: 0.027211382691733996\n",
      "Iteration: 8461 lambda_n: 0.927524477004971 Loss: 0.027203235970640463\n",
      "Iteration: 8462 lambda_n: 0.9534059701893809 Loss: 0.027195531091327777\n",
      "Iteration: 8463 lambda_n: 0.9406268713094769 Loss: 0.027187611216043656\n",
      "Iteration: 8464 lambda_n: 1.0065341587270673 Loss: 0.02717979749571342\n",
      "Iteration: 8465 lambda_n: 0.9777180244715044 Loss: 0.027171436288095922\n",
      "Iteration: 8466 lambda_n: 0.9456166428986909 Loss: 0.027163314453835397\n",
      "Iteration: 8467 lambda_n: 0.8941828544848148 Loss: 0.027155459283230148\n",
      "Iteration: 8468 lambda_n: 0.9077324825479233 Loss: 0.027148031369238015\n",
      "Iteration: 8469 lambda_n: 0.9145382201947626 Loss: 0.027140490899221055\n",
      "Iteration: 8470 lambda_n: 0.9898941417591898 Loss: 0.027132893894201218\n",
      "Iteration: 8471 lambda_n: 1.0066236689765304 Loss: 0.027124670912582326\n",
      "Iteration: 8472 lambda_n: 0.9928227126440404 Loss: 0.027116308959700942\n",
      "Iteration: 8473 lambda_n: 0.8886504832656952 Loss: 0.02710806165016439\n",
      "Iteration: 8474 lambda_n: 0.9846393642723607 Loss: 0.027100679691916638\n",
      "Iteration: 8475 lambda_n: 0.9394822201256221 Loss: 0.0270925003604551\n",
      "Iteration: 8476 lambda_n: 0.8773067278289723 Loss: 0.027084696146070907\n",
      "Iteration: 8477 lambda_n: 0.8911733287923618 Loss: 0.027077408419050205\n",
      "Iteration: 8478 lambda_n: 0.9102189753861277 Loss: 0.027070005502946547\n",
      "Iteration: 8479 lambda_n: 0.9309839937630129 Loss: 0.02706244437577243\n",
      "Iteration: 8480 lambda_n: 0.9620621976735401 Loss: 0.027054710754812195\n",
      "Iteration: 8481 lambda_n: 0.9026708754207566 Loss: 0.0270467189691182\n",
      "Iteration: 8482 lambda_n: 0.9009574882555449 Loss: 0.027039220542933486\n",
      "Iteration: 8483 lambda_n: 1.029665093249758 Loss: 0.027031736349560652\n",
      "Iteration: 8484 lambda_n: 1.0182421560907782 Loss: 0.027023182990549535\n",
      "Iteration: 8485 lambda_n: 0.9304654465000889 Loss: 0.02701472452088243\n",
      "Iteration: 8486 lambda_n: 0.9488038056871411 Loss: 0.027006995206278826\n",
      "Iteration: 8487 lambda_n: 0.9034958468361373 Loss: 0.02699911355595225\n",
      "Iteration: 8488 lambda_n: 0.962318162155997 Loss: 0.026991608275650364\n",
      "Iteration: 8489 lambda_n: 1.0134999448983222 Loss: 0.02698361436207386\n",
      "Iteration: 8490 lambda_n: 0.9161664292278633 Loss: 0.02697519528458987\n",
      "Iteration: 8491 lambda_n: 1.0263275465117878 Loss: 0.02696758475002859\n",
      "Iteration: 8492 lambda_n: 1.0045416145471624 Loss: 0.026959059114075885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8493 lambda_n: 0.9628002836138693 Loss: 0.026950714452218492\n",
      "Iteration: 8494 lambda_n: 0.8797397633163191 Loss: 0.026942716566606326\n",
      "Iteration: 8495 lambda_n: 0.9160083448860882 Loss: 0.026935408643441894\n",
      "Iteration: 8496 lambda_n: 0.9328835925949714 Loss: 0.026927799432551212\n",
      "Iteration: 8497 lambda_n: 0.9557531848284491 Loss: 0.026920050034942508\n",
      "Iteration: 8498 lambda_n: 0.9419118469195288 Loss: 0.026912110657736497\n",
      "Iteration: 8499 lambda_n: 0.9070629514049768 Loss: 0.02690428625737998\n",
      "Iteration: 8500 lambda_n: 0.9550694590484202 Loss: 0.02689675134325256\n",
      "Iteration: 8501 lambda_n: 0.9179605690937669 Loss: 0.02688881764133382\n",
      "Iteration: 8502 lambda_n: 0.9056606785472293 Loss: 0.02688119220012331\n",
      "Iteration: 8503 lambda_n: 0.9808953396084089 Loss: 0.026873668933035347\n",
      "Iteration: 8504 lambda_n: 0.9297122148758196 Loss: 0.02686552069606632\n",
      "Iteration: 8505 lambda_n: 0.942699641048278 Loss: 0.026857797633985603\n",
      "Iteration: 8506 lambda_n: 0.971234073114058 Loss: 0.02684996668602818\n",
      "Iteration: 8507 lambda_n: 1.0037031171428077 Loss: 0.02684189870417683\n",
      "Iteration: 8508 lambda_n: 0.8963348442720247 Loss: 0.02683356100384281\n",
      "Iteration: 8509 lambda_n: 1.0255295693086999 Loss: 0.02682611520505725\n",
      "Iteration: 8510 lambda_n: 0.9803266374812355 Loss: 0.026817596193458013\n",
      "Iteration: 8511 lambda_n: 0.9711483508160406 Loss: 0.02680945267971157\n",
      "Iteration: 8512 lambda_n: 0.9291728796127282 Loss: 0.026801385409289984\n",
      "Iteration: 8513 lambda_n: 1.0095050129901504 Loss: 0.026793666826418542\n",
      "Iteration: 8514 lambda_n: 0.9127728305494429 Loss: 0.026785280929225583\n",
      "Iteration: 8515 lambda_n: 0.9663676750027967 Loss: 0.02677769858027823\n",
      "Iteration: 8516 lambda_n: 0.9775761600318523 Loss: 0.026769671022035365\n",
      "Iteration: 8517 lambda_n: 0.9836845221944114 Loss: 0.026761550355417815\n",
      "Iteration: 8518 lambda_n: 1.0039240073484696 Loss: 0.02675337894683393\n",
      "Iteration: 8519 lambda_n: 0.9203682592285094 Loss: 0.02674503940988186\n",
      "Iteration: 8520 lambda_n: 1.0110397259763835 Loss: 0.026737393965390103\n",
      "Iteration: 8521 lambda_n: 0.9170149946589663 Loss: 0.02672899531824163\n",
      "Iteration: 8522 lambda_n: 0.9944749246088139 Loss: 0.026721377728803827\n",
      "Iteration: 8523 lambda_n: 0.9394749267570034 Loss: 0.026713116684136968\n",
      "Iteration: 8524 lambda_n: 1.0139727011664799 Loss: 0.02670531252104767\n",
      "Iteration: 8525 lambda_n: 0.9322349373940637 Loss: 0.026696889509147725\n",
      "Iteration: 8526 lambda_n: 0.8897936039010819 Loss: 0.026689145487899465\n",
      "Iteration: 8527 lambda_n: 0.9652561534202242 Loss: 0.02668175402418469\n",
      "Iteration: 8528 lambda_n: 0.8859500206699238 Loss: 0.026673735697307528\n",
      "Iteration: 8529 lambda_n: 0.9868850372283826 Loss: 0.026666376161722564\n",
      "Iteration: 8530 lambda_n: 0.9352739138547944 Loss: 0.026658178164637884\n",
      "Iteration: 8531 lambda_n: 0.8851787845568078 Loss: 0.026650408898014823\n",
      "Iteration: 8532 lambda_n: 1.0022121401946038 Loss: 0.02664305576860414\n",
      "Iteration: 8533 lambda_n: 1.0066580342517017 Loss: 0.02663473044966313\n",
      "Iteration: 8534 lambda_n: 0.9391491497818838 Loss: 0.026626368198757674\n",
      "Iteration: 8535 lambda_n: 0.9148017363162789 Loss: 0.026618566740141777\n",
      "Iteration: 8536 lambda_n: 0.9391914140188548 Loss: 0.02661096753395642\n",
      "Iteration: 8537 lambda_n: 0.9909899904049664 Loss: 0.026603165723951747\n",
      "Iteration: 8538 lambda_n: 0.8905912313104233 Loss: 0.026594933625939714\n",
      "Iteration: 8539 lambda_n: 0.915461910588037 Loss: 0.0265875355346102\n",
      "Iteration: 8540 lambda_n: 0.9561031768590975 Loss: 0.026579930843805586\n",
      "Iteration: 8541 lambda_n: 0.9016306653609062 Loss: 0.026571988548129796\n",
      "Iteration: 8542 lambda_n: 0.9984262107365006 Loss: 0.02656449875241708\n",
      "Iteration: 8543 lambda_n: 1.0016154506815016 Loss: 0.026556204881349053\n",
      "Iteration: 8544 lambda_n: 0.9643836347709787 Loss: 0.02654788451727342\n",
      "Iteration: 8545 lambda_n: 1.004108306801628 Loss: 0.026539873435668243\n",
      "Iteration: 8546 lambda_n: 1.0261035088119272 Loss: 0.02653153236324435\n",
      "Iteration: 8547 lambda_n: 0.9977633768978356 Loss: 0.026523008577717003\n",
      "Iteration: 8548 lambda_n: 1.0080701997517558 Loss: 0.026514720211940217\n",
      "Iteration: 8549 lambda_n: 0.8825665100663486 Loss: 0.02650634622778417\n",
      "Iteration: 8550 lambda_n: 0.9468909772107426 Loss: 0.026499014795785102\n",
      "Iteration: 8551 lambda_n: 0.9867449804650467 Loss: 0.02649114902379964\n",
      "Iteration: 8552 lambda_n: 0.9796105571585161 Loss: 0.026482952186619103\n",
      "Iteration: 8553 lambda_n: 1.010722305237189 Loss: 0.02647481461455037\n",
      "Iteration: 8554 lambda_n: 0.9694426725282386 Loss: 0.026466418598708595\n",
      "Iteration: 8555 lambda_n: 1.0240809214049955 Loss: 0.026458365490398466\n",
      "Iteration: 8556 lambda_n: 1.028558875231585 Loss: 0.026449858504925552\n",
      "Iteration: 8557 lambda_n: 0.9549264871869021 Loss: 0.026441314321161903\n",
      "Iteration: 8558 lambda_n: 0.9440332850193578 Loss: 0.02643338179756577\n",
      "Iteration: 8559 lambda_n: 0.8818497558474172 Loss: 0.026425539763079063\n",
      "Iteration: 8560 lambda_n: 0.9756416859343156 Loss: 0.026418214283743868\n",
      "Iteration: 8561 lambda_n: 0.92465172781932 Loss: 0.026410109679666272\n",
      "Iteration: 8562 lambda_n: 0.9726602717436112 Loss: 0.026402428646343273\n",
      "Iteration: 8563 lambda_n: 0.8792668173546485 Loss: 0.026394348808428992\n",
      "Iteration: 8564 lambda_n: 0.9701266325018323 Loss: 0.02638704478491442\n",
      "Iteration: 8565 lambda_n: 1.0113028648410634 Loss: 0.02637898599353174\n",
      "Iteration: 8566 lambda_n: 1.018785900190306 Loss: 0.026370585153182746\n",
      "Iteration: 8567 lambda_n: 1.0011558120603217 Loss: 0.02636212215149094\n",
      "Iteration: 8568 lambda_n: 0.9569399492744113 Loss: 0.026353805601872993\n",
      "Iteration: 8569 lambda_n: 0.9012398952793875 Loss: 0.026345856350998426\n",
      "Iteration: 8570 lambda_n: 1.0252747893649479 Loss: 0.02633836979747383\n",
      "Iteration: 8571 lambda_n: 0.9266906631390731 Loss: 0.026329852892316593\n",
      "Iteration: 8572 lambda_n: 1.0088832616319023 Loss: 0.02632215492030397\n",
      "Iteration: 8573 lambda_n: 0.8960923024619183 Loss: 0.026313774178435004\n",
      "Iteration: 8574 lambda_n: 0.9034096327661285 Loss: 0.02630633038518499\n",
      "Iteration: 8575 lambda_n: 0.8830764248010888 Loss: 0.02629882580712474\n",
      "Iteration: 8576 lambda_n: 0.9202727596449193 Loss: 0.026291490135876224\n",
      "Iteration: 8577 lambda_n: 0.9828764384090094 Loss: 0.026283845476434958\n",
      "Iteration: 8578 lambda_n: 0.9650674390641267 Loss: 0.026275680771257814\n",
      "Iteration: 8579 lambda_n: 0.9213885057720602 Loss: 0.026267664004405927\n",
      "Iteration: 8580 lambda_n: 0.9912083928259583 Loss: 0.026260010076135674\n",
      "Iteration: 8581 lambda_n: 1.0113584165422809 Loss: 0.026251776157422444\n",
      "Iteration: 8582 lambda_n: 1.0034683886434739 Loss: 0.026243374853323627\n",
      "Iteration: 8583 lambda_n: 0.9558859951595834 Loss: 0.026235039091150176\n",
      "Iteration: 8584 lambda_n: 0.9887511554340523 Loss: 0.026227098593425365\n",
      "Iteration: 8585 lambda_n: 0.9726047579682743 Loss: 0.0262188850863054\n",
      "Iteration: 8586 lambda_n: 0.9575733756027281 Loss: 0.026210805706378605\n",
      "Iteration: 8587 lambda_n: 1.0252536932373904 Loss: 0.02620285119127641\n",
      "Iteration: 8588 lambda_n: 0.9917283571014837 Loss: 0.026194334458956577\n",
      "Iteration: 8589 lambda_n: 0.9016789713953748 Loss: 0.026186096219824682\n",
      "Iteration: 8590 lambda_n: 0.9262361493741547 Loss: 0.02617860601642793\n",
      "Iteration: 8591 lambda_n: 0.9187089565663161 Loss: 0.026170911817632838\n",
      "Iteration: 8592 lambda_n: 0.9878175233296163 Loss: 0.026163280146747438\n",
      "Iteration: 8593 lambda_n: 0.9652972327642687 Loss: 0.02615507439421626\n",
      "Iteration: 8594 lambda_n: 0.9941507934268966 Loss: 0.026147055716523745\n",
      "Iteration: 8595 lambda_n: 0.922164082992972 Loss: 0.026138797353561304\n",
      "Iteration: 8596 lambda_n: 1.0096875197989177 Loss: 0.026131136980626808\n",
      "Iteration: 8597 lambda_n: 0.9575662726596499 Loss: 0.02612274955456491\n",
      "Iteration: 8598 lambda_n: 1.0044685191579166 Loss: 0.026114795097087973\n",
      "Iteration: 8599 lambda_n: 1.0086597696765738 Loss: 0.026106451024753553\n",
      "Iteration: 8600 lambda_n: 0.9022090918478504 Loss: 0.026098072135766596\n",
      "Iteration: 8601 lambda_n: 0.9077974152845355 Loss: 0.02609057752740444\n",
      "Iteration: 8602 lambda_n: 0.8808564323049906 Loss: 0.026083036496996365\n",
      "Iteration: 8603 lambda_n: 0.9253576569909776 Loss: 0.02607571926396213\n",
      "Iteration: 8604 lambda_n: 0.8930936343819285 Loss: 0.02606803236123825\n",
      "Iteration: 8605 lambda_n: 0.9161179263302925 Loss: 0.026060613474132754\n",
      "Iteration: 8606 lambda_n: 0.9823739458393431 Loss: 0.026053003325204712\n",
      "Iteration: 8607 lambda_n: 0.9379514447066026 Loss: 0.02604484279049524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8608 lambda_n: 1.0162118510657085 Loss: 0.026037051271320655\n",
      "Iteration: 8609 lambda_n: 0.9075605927179219 Loss: 0.02602860964645788\n",
      "Iteration: 8610 lambda_n: 0.9491147432235952 Loss: 0.02602107058245803\n",
      "Iteration: 8611 lambda_n: 0.8866473519345959 Loss: 0.026013186329938046\n",
      "Iteration: 8612 lambda_n: 1.019767127341617 Loss: 0.026005820991059123\n",
      "Iteration: 8613 lambda_n: 0.9760907701358317 Loss: 0.025997349832200936\n",
      "Iteration: 8614 lambda_n: 0.9236695217796778 Loss: 0.02598924149071981\n",
      "Iteration: 8615 lambda_n: 0.9560858411567442 Loss: 0.025981568610045087\n",
      "Iteration: 8616 lambda_n: 1.0120515647929804 Loss: 0.02597362644837269\n",
      "Iteration: 8617 lambda_n: 0.8977581159722643 Loss: 0.025965219381857314\n",
      "Iteration: 8618 lambda_n: 0.8903475443489627 Loss: 0.025957761745734213\n",
      "Iteration: 8619 lambda_n: 1.0274599456832174 Loss: 0.025950365668798162\n",
      "Iteration: 8620 lambda_n: 1.025406326249238 Loss: 0.025941830605212508\n",
      "Iteration: 8621 lambda_n: 0.9120026678773264 Loss: 0.025933312600824793\n",
      "Iteration: 8622 lambda_n: 0.9009951143893445 Loss: 0.02592573663543158\n",
      "Iteration: 8623 lambda_n: 1.0024327437722755 Loss: 0.025918252109195464\n",
      "Iteration: 8624 lambda_n: 1.0123269687409486 Loss: 0.02590992494498938\n",
      "Iteration: 8625 lambda_n: 0.9871306879250138 Loss: 0.025901515589777185\n",
      "Iteration: 8626 lambda_n: 1.007202761581497 Loss: 0.025893315538834515\n",
      "Iteration: 8627 lambda_n: 0.9781374153242977 Loss: 0.02588494874994834\n",
      "Iteration: 8628 lambda_n: 1.0252791775404098 Loss: 0.025876823405496123\n",
      "Iteration: 8629 lambda_n: 0.9991291617475915 Loss: 0.025868306456384726\n",
      "Iteration: 8630 lambda_n: 0.9585037701954746 Loss: 0.025860006734188413\n",
      "Iteration: 8631 lambda_n: 0.9467384173956558 Loss: 0.025852044485230975\n",
      "Iteration: 8632 lambda_n: 1.0111624708197506 Loss: 0.025844179970441968\n",
      "Iteration: 8633 lambda_n: 0.8835338193112922 Loss: 0.02583578028773958\n",
      "Iteration: 8634 lambda_n: 0.9069233020141034 Loss: 0.025828440779306714\n",
      "Iteration: 8635 lambda_n: 1.0073580933959128 Loss: 0.02582090701871062\n",
      "Iteration: 8636 lambda_n: 0.9943607412835955 Loss: 0.025812538911555494\n",
      "Iteration: 8637 lambda_n: 1.017687477679211 Loss: 0.025804278784870592\n",
      "Iteration: 8638 lambda_n: 0.9842138131633816 Loss: 0.025795824890651776\n",
      "Iteration: 8639 lambda_n: 0.9664519896189642 Loss: 0.025787649065036545\n",
      "Iteration: 8640 lambda_n: 0.9119378469213518 Loss: 0.02577962078830591\n",
      "Iteration: 8641 lambda_n: 0.9423426988174406 Loss: 0.025772045359332023\n",
      "Iteration: 8642 lambda_n: 0.9891343239032638 Loss: 0.025764217392271502\n",
      "Iteration: 8643 lambda_n: 0.9385800929451517 Loss: 0.025756000716962327\n",
      "Iteration: 8644 lambda_n: 0.92142109034282 Loss: 0.02574820398365575\n",
      "Iteration: 8645 lambda_n: 0.927929084432874 Loss: 0.025740549759703777\n",
      "Iteration: 8646 lambda_n: 0.8785591032517364 Loss: 0.02573284148069628\n",
      "Iteration: 8647 lambda_n: 0.9046079533033126 Loss: 0.0257255433207328\n",
      "Iteration: 8648 lambda_n: 0.9225620879402465 Loss: 0.025718028776390487\n",
      "Iteration: 8649 lambda_n: 0.9194717799866058 Loss: 0.025710365089410686\n",
      "Iteration: 8650 lambda_n: 0.9180812125417412 Loss: 0.02570272707454759\n",
      "Iteration: 8651 lambda_n: 0.993788876152597 Loss: 0.025695100611679955\n",
      "Iteration: 8652 lambda_n: 1.0242016374944414 Loss: 0.025686845248722782\n",
      "Iteration: 8653 lambda_n: 0.9691190844197544 Loss: 0.025678337248407512\n",
      "Iteration: 8654 lambda_n: 0.9997888548760321 Loss: 0.02567028681660984\n",
      "Iteration: 8655 lambda_n: 0.8798633868372857 Loss: 0.02566198161227253\n",
      "Iteration: 8656 lambda_n: 1.0238160021745346 Loss: 0.025654672623727354\n",
      "Iteration: 8657 lambda_n: 0.9796159161816663 Loss: 0.025646167826703113\n",
      "Iteration: 8658 lambda_n: 1.0041249206042935 Loss: 0.025638030197843303\n",
      "Iteration: 8659 lambda_n: 1.012878921562304 Loss: 0.025629688973580915\n",
      "Iteration: 8660 lambda_n: 0.9367433636548874 Loss: 0.025621275030071472\n",
      "Iteration: 8661 lambda_n: 0.9535580259813783 Loss: 0.025613493541394045\n",
      "Iteration: 8662 lambda_n: 0.9025244595103384 Loss: 0.025605572373902222\n",
      "Iteration: 8663 lambda_n: 0.9831509385709298 Loss: 0.02559807514005916\n",
      "Iteration: 8664 lambda_n: 0.8831293477403596 Loss: 0.025589908145246514\n",
      "Iteration: 8665 lambda_n: 0.9220800651887391 Loss: 0.025582572025619706\n",
      "Iteration: 8666 lambda_n: 0.9607191408944648 Loss: 0.02557491234387699\n",
      "Iteration: 8667 lambda_n: 0.8938366295003938 Loss: 0.02556693168880375\n",
      "Iteration: 8668 lambda_n: 0.9673805617319751 Loss: 0.02555950662395895\n",
      "Iteration: 8669 lambda_n: 0.9815691478270128 Loss: 0.0255514706325451\n",
      "Iteration: 8670 lambda_n: 0.9289048642898309 Loss: 0.02554331677702027\n",
      "Iteration: 8671 lambda_n: 0.9103570024534132 Loss: 0.025535600401493136\n",
      "Iteration: 8672 lambda_n: 1.0262788951332418 Loss: 0.025528038102227202\n",
      "Iteration: 8673 lambda_n: 0.8886726905465537 Loss: 0.025519512844335967\n",
      "Iteration: 8674 lambda_n: 1.021665021577237 Loss: 0.025512130675613143\n",
      "Iteration: 8675 lambda_n: 0.928471492302684 Loss: 0.02550364374479412\n",
      "Iteration: 8676 lambda_n: 0.928209337284025 Loss: 0.025495930968834794\n",
      "Iteration: 8677 lambda_n: 1.0171147345595473 Loss: 0.02548822037050403\n",
      "Iteration: 8678 lambda_n: 0.8970954573468644 Loss: 0.025479771238457412\n",
      "Iteration: 8679 lambda_n: 0.956707801535286 Loss: 0.025472319101733115\n",
      "Iteration: 8680 lambda_n: 1.0189228528106136 Loss: 0.02546437176752681\n",
      "Iteration: 8681 lambda_n: 1.023076898376521 Loss: 0.025455907615237295\n",
      "Iteration: 8682 lambda_n: 0.970117757954019 Loss: 0.02544740895535663\n",
      "Iteration: 8683 lambda_n: 0.920903082493731 Loss: 0.025439350224897087\n",
      "Iteration: 8684 lambda_n: 0.9771115632550907 Loss: 0.02543170031874868\n",
      "Iteration: 8685 lambda_n: 0.9124814950621749 Loss: 0.025423583490851047\n",
      "Iteration: 8686 lambda_n: 0.9790253700484787 Loss: 0.025416003542344218\n",
      "Iteration: 8687 lambda_n: 1.0117017772317898 Loss: 0.0254078708163596\n",
      "Iteration: 8688 lambda_n: 1.0226037207903396 Loss: 0.025399466648631852\n",
      "Iteration: 8689 lambda_n: 0.9951296517457936 Loss: 0.025390971918785785\n",
      "Iteration: 8690 lambda_n: 0.9227369598115477 Loss: 0.025382705414885454\n",
      "Iteration: 8691 lambda_n: 0.9129426788112873 Loss: 0.025375040274222254\n",
      "Iteration: 8692 lambda_n: 0.9444472893360066 Loss: 0.025367456494202613\n",
      "Iteration: 8693 lambda_n: 0.899155648590883 Loss: 0.025359611006509134\n",
      "Iteration: 8694 lambda_n: 0.92921422384511 Loss: 0.02535214175467735\n",
      "Iteration: 8695 lambda_n: 0.9642263222696097 Loss: 0.025344422807329726\n",
      "Iteration: 8696 lambda_n: 0.9107895146059339 Loss: 0.025336413015729763\n",
      "Iteration: 8697 lambda_n: 1.0066710689764218 Loss: 0.02532884712159453\n",
      "Iteration: 8698 lambda_n: 1.0145433642667938 Loss: 0.025320484742926565\n",
      "Iteration: 8699 lambda_n: 0.9434969295899432 Loss: 0.02531205696931087\n",
      "Iteration: 8700 lambda_n: 1.003212526267147 Loss: 0.025304219375677495\n",
      "Iteration: 8701 lambda_n: 0.9968752682022514 Loss: 0.02529588572674095\n",
      "Iteration: 8702 lambda_n: 1.0159341707413745 Loss: 0.02528760472108602\n",
      "Iteration: 8703 lambda_n: 0.9927450501592735 Loss: 0.0252791653937543\n",
      "Iteration: 8704 lambda_n: 0.9302529515560287 Loss: 0.025270918697499818\n",
      "Iteration: 8705 lambda_n: 0.9538269171534024 Loss: 0.02526319112071025\n",
      "Iteration: 8706 lambda_n: 0.8956208235644795 Loss: 0.02525526771578928\n",
      "Iteration: 8707 lambda_n: 0.9829307478558552 Loss: 0.025247827826664498\n",
      "Iteration: 8708 lambda_n: 0.9139092407975434 Loss: 0.02523966265714988\n",
      "Iteration: 8709 lambda_n: 0.9836522862280984 Loss: 0.02523207084667855\n",
      "Iteration: 8710 lambda_n: 0.8957157457283506 Loss: 0.025223899683220124\n",
      "Iteration: 8711 lambda_n: 1.0226557859903846 Loss: 0.02521645900530652\n",
      "Iteration: 8712 lambda_n: 0.9446901924427384 Loss: 0.025207963841052426\n",
      "Iteration: 8713 lambda_n: 0.9337953246937266 Loss: 0.02520011633405826\n",
      "Iteration: 8714 lambda_n: 0.960425818100989 Loss: 0.025192359330263432\n",
      "Iteration: 8715 lambda_n: 0.9623863719298604 Loss: 0.025184381107858023\n",
      "Iteration: 8716 lambda_n: 0.9733532414348927 Loss: 0.02517638659913063\n",
      "Iteration: 8717 lambda_n: 0.9168085371644462 Loss: 0.0251683009889425\n",
      "Iteration: 8718 lambda_n: 0.9965797741150875 Loss: 0.0251606850934989\n",
      "Iteration: 8719 lambda_n: 0.9482458669359254 Loss: 0.025152406541197676\n",
      "Iteration: 8720 lambda_n: 0.9525579284907132 Loss: 0.025144529496850098\n",
      "Iteration: 8721 lambda_n: 0.9048468309586091 Loss: 0.02513661663229303\n",
      "Iteration: 8722 lambda_n: 0.9085096531587389 Loss: 0.025129100102046557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8723 lambda_n: 0.971462099631971 Loss: 0.02512155314480484\n",
      "Iteration: 8724 lambda_n: 0.9706458126003185 Loss: 0.02511348324377242\n",
      "Iteration: 8725 lambda_n: 0.9219657858723262 Loss: 0.025105420123536483\n",
      "Iteration: 8726 lambda_n: 0.9239389745150266 Loss: 0.025097761386483754\n",
      "Iteration: 8727 lambda_n: 0.8898808737609214 Loss: 0.02509008625815972\n",
      "Iteration: 8728 lambda_n: 1.005233737132377 Loss: 0.02508269404921091\n",
      "Iteration: 8729 lambda_n: 0.8968134569687086 Loss: 0.025074343608040284\n",
      "Iteration: 8730 lambda_n: 0.9757100969836288 Loss: 0.025066893810243192\n",
      "Iteration: 8731 lambda_n: 1.0125136109074202 Loss: 0.025058788620773327\n",
      "Iteration: 8732 lambda_n: 0.9862381475845539 Loss: 0.0250503777057359\n",
      "Iteration: 8733 lambda_n: 0.9529922858145204 Loss: 0.02504218505997807\n",
      "Iteration: 8734 lambda_n: 0.980452188504317 Loss: 0.025034268586362176\n",
      "Iteration: 8735 lambda_n: 0.9372746246321523 Loss: 0.025026124004227674\n",
      "Iteration: 8736 lambda_n: 0.9990416099287415 Loss: 0.02501833809654479\n",
      "Iteration: 8737 lambda_n: 0.9554184377474864 Loss: 0.0250100390925992\n",
      "Iteration: 8738 lambda_n: 0.9166420392876548 Loss: 0.025002102464761995\n",
      "Iteration: 8739 lambda_n: 0.8984383579255302 Loss: 0.024994487951060882\n",
      "Iteration: 8740 lambda_n: 0.9602341547913761 Loss: 0.024987024654653484\n",
      "Iteration: 8741 lambda_n: 0.8833172330316019 Loss: 0.02497904802263655\n",
      "Iteration: 8742 lambda_n: 0.9889125480106618 Loss: 0.024971710336778204\n",
      "Iteration: 8743 lambda_n: 0.8837918785294999 Loss: 0.024963495474212987\n",
      "Iteration: 8744 lambda_n: 0.889279351521684 Loss: 0.02495615384537792\n",
      "Iteration: 8745 lambda_n: 0.8866189006979641 Loss: 0.024948766632239557\n",
      "Iteration: 8746 lambda_n: 0.8873016189957209 Loss: 0.02494140151932186\n",
      "Iteration: 8747 lambda_n: 0.9140146292824344 Loss: 0.024934030735033655\n",
      "Iteration: 8748 lambda_n: 0.9030324031815011 Loss: 0.024926438046625858\n",
      "Iteration: 8749 lambda_n: 0.9685194036356468 Loss: 0.02491893658714131\n",
      "Iteration: 8750 lambda_n: 1.0023740572825606 Loss: 0.024910891129311878\n",
      "Iteration: 8751 lambda_n: 0.9352274084495288 Loss: 0.024902564441959207\n",
      "Iteration: 8752 lambda_n: 1.015253065430728 Loss: 0.02489479553948279\n",
      "Iteration: 8753 lambda_n: 0.9487574836684479 Loss: 0.024886361866516715\n",
      "Iteration: 8754 lambda_n: 0.8868026069817446 Loss: 0.02487848057004217\n",
      "Iteration: 8755 lambda_n: 0.8911659703031908 Loss: 0.024871113930587652\n",
      "Iteration: 8756 lambda_n: 0.9537671600631598 Loss: 0.024863711044769834\n",
      "Iteration: 8757 lambda_n: 0.9709353097930066 Loss: 0.02485578813291484\n",
      "Iteration: 8758 lambda_n: 0.9515186357578884 Loss: 0.02484772260575614\n",
      "Iteration: 8759 lambda_n: 1.0123207666024274 Loss: 0.024839818372200843\n",
      "Iteration: 8760 lambda_n: 0.958159104072729 Loss: 0.02483140905731322\n",
      "Iteration: 8761 lambda_n: 0.9986221021854971 Loss: 0.02482344966149092\n",
      "Iteration: 8762 lambda_n: 1.0116799849871962 Loss: 0.024815154140829108\n",
      "Iteration: 8763 lambda_n: 0.8810248223041592 Loss: 0.024806750148705578\n",
      "Iteration: 8764 lambda_n: 1.0065216763794647 Loss: 0.02479943150463476\n",
      "Iteration: 8765 lambda_n: 0.8886839132392566 Loss: 0.024791070362292635\n",
      "Iteration: 8766 lambda_n: 0.9681993740662974 Loss: 0.02478368809430395\n",
      "Iteration: 8767 lambda_n: 0.8978523086929017 Loss: 0.02477564529395013\n",
      "Iteration: 8768 lambda_n: 0.8981477184929005 Loss: 0.024768186864301482\n",
      "Iteration: 8769 lambda_n: 0.9484197510044341 Loss: 0.024760725980644816\n",
      "Iteration: 8770 lambda_n: 0.9510595345026875 Loss: 0.0247528474888113\n",
      "Iteration: 8771 lambda_n: 1.0116567969980779 Loss: 0.024744947068328915\n",
      "Iteration: 8772 lambda_n: 1.0238651561271286 Loss: 0.02473654326831006\n",
      "Iteration: 8773 lambda_n: 0.980263361544155 Loss: 0.024728038053789106\n",
      "Iteration: 8774 lambda_n: 1.0209797547784625 Loss: 0.02471989503789721\n",
      "Iteration: 8775 lambda_n: 0.8857476677015106 Loss: 0.024711413792191668\n",
      "Iteration: 8776 lambda_n: 0.9499471015579789 Loss: 0.024704055914993757\n",
      "Iteration: 8777 lambda_n: 0.9473848139348965 Loss: 0.024696164735129256\n",
      "Iteration: 8778 lambda_n: 0.8844486878853814 Loss: 0.02468829484005356\n",
      "Iteration: 8779 lambda_n: 0.9120799170470942 Loss: 0.024680947753297714\n",
      "Iteration: 8780 lambda_n: 0.95675428670022 Loss: 0.024673371134765785\n",
      "Iteration: 8781 lambda_n: 0.8981366862926278 Loss: 0.02466542340763317\n",
      "Iteration: 8782 lambda_n: 0.9009377089681039 Loss: 0.024657962614975933\n",
      "Iteration: 8783 lambda_n: 1.0256478956346347 Loss: 0.024650478554267906\n",
      "Iteration: 8784 lambda_n: 0.8785818999475333 Loss: 0.024641958529980846\n",
      "Iteration: 8785 lambda_n: 0.8788225663117 Loss: 0.02463466017816865\n",
      "Iteration: 8786 lambda_n: 0.9329043948391275 Loss: 0.024627359827106152\n",
      "Iteration: 8787 lambda_n: 0.9704548968226092 Loss: 0.024619610219972336\n",
      "Iteration: 8788 lambda_n: 0.9797041787072388 Loss: 0.024611548681963843\n",
      "Iteration: 8789 lambda_n: 0.9992720839800364 Loss: 0.024603410310412986\n",
      "Iteration: 8790 lambda_n: 0.8831411847741707 Loss: 0.024595109388840793\n",
      "Iteration: 8791 lambda_n: 0.941087481049064 Loss: 0.02458777316292443\n",
      "Iteration: 8792 lambda_n: 0.8841970224045761 Loss: 0.02457995557891198\n",
      "Iteration: 8793 lambda_n: 1.0179333457767734 Loss: 0.024572610582100016\n",
      "Iteration: 8794 lambda_n: 0.9808875734498556 Loss: 0.024564154641816015\n",
      "Iteration: 8795 lambda_n: 0.9769498219139573 Loss: 0.024556006439544774\n",
      "Iteration: 8796 lambda_n: 0.975367104800051 Loss: 0.024547890948001826\n",
      "Iteration: 8797 lambda_n: 0.8834882591460529 Loss: 0.02453978860399083\n",
      "Iteration: 8798 lambda_n: 0.9207187631741981 Loss: 0.024532449494629856\n",
      "Iteration: 8799 lambda_n: 0.9104264477975752 Loss: 0.02452480111259377\n",
      "Iteration: 8800 lambda_n: 0.9904606038198759 Loss: 0.024517238228457482\n",
      "Iteration: 8801 lambda_n: 0.9684844372959861 Loss: 0.024509010503035392\n",
      "Iteration: 8802 lambda_n: 0.9782661257987091 Loss: 0.02450096533289626\n",
      "Iteration: 8803 lambda_n: 0.9489923635500234 Loss: 0.024492838906526985\n",
      "Iteration: 8804 lambda_n: 0.998807015301606 Loss: 0.024484955656346265\n",
      "Iteration: 8805 lambda_n: 0.9311999624366072 Loss: 0.024476658597349047\n",
      "Iteration: 8806 lambda_n: 0.9223367835462413 Loss: 0.024468923148003432\n",
      "Iteration: 8807 lambda_n: 0.9155046166645611 Loss: 0.024461261324768994\n",
      "Iteration: 8808 lambda_n: 0.9624144640294322 Loss: 0.024453656256092946\n",
      "Iteration: 8809 lambda_n: 0.9159169761192116 Loss: 0.02444566150871591\n",
      "Iteration: 8810 lambda_n: 0.9121987218736443 Loss: 0.024438053014499122\n",
      "Iteration: 8811 lambda_n: 0.9607808850995748 Loss: 0.02443047540766603\n",
      "Iteration: 8812 lambda_n: 0.912749453229708 Loss: 0.02442249423024993\n",
      "Iteration: 8813 lambda_n: 0.8949776999014767 Loss: 0.02441491204842672\n",
      "Iteration: 8814 lambda_n: 0.8977287140089296 Loss: 0.024407477495976698\n",
      "Iteration: 8815 lambda_n: 0.9934238919030932 Loss: 0.02440002009089859\n",
      "Iteration: 8816 lambda_n: 0.9557286140754495 Loss: 0.024391767748851906\n",
      "Iteration: 8817 lambda_n: 0.9799905480825497 Loss: 0.024383828540286836\n",
      "Iteration: 8818 lambda_n: 0.9574608169060009 Loss: 0.024375687788529735\n",
      "Iteration: 8819 lambda_n: 0.9506443605899404 Loss: 0.024367734190522742\n",
      "Iteration: 8820 lambda_n: 0.9078395402128162 Loss: 0.024359837216570195\n",
      "Iteration: 8821 lambda_n: 1.0235906616337351 Loss: 0.024352295820926358\n",
      "Iteration: 8822 lambda_n: 0.9179629628988665 Loss: 0.024343792884171267\n",
      "Iteration: 8823 lambda_n: 0.9224325770678671 Loss: 0.02433616739348087\n",
      "Iteration: 8824 lambda_n: 0.9186793606886752 Loss: 0.02432850477380232\n",
      "Iteration: 8825 lambda_n: 1.0137204055684925 Loss: 0.024320873331941575\n",
      "Iteration: 8826 lambda_n: 0.8998032483409056 Loss: 0.024312452386934556\n",
      "Iteration: 8827 lambda_n: 0.884041561043595 Loss: 0.024304977748295296\n",
      "Iteration: 8828 lambda_n: 0.9241844937192397 Loss: 0.024297634041485532\n",
      "Iteration: 8829 lambda_n: 0.9041402118895306 Loss: 0.02428995686849917\n",
      "Iteration: 8830 lambda_n: 0.9810646673184872 Loss: 0.024282446202726987\n",
      "Iteration: 8831 lambda_n: 1.0023108345816862 Loss: 0.02427429652776031\n",
      "Iteration: 8832 lambda_n: 0.9617995161942029 Loss: 0.024265970361473082\n",
      "Iteration: 8833 lambda_n: 1.0066109924546565 Loss: 0.024257980721461292\n",
      "Iteration: 8834 lambda_n: 0.96035965330803 Loss: 0.024249618833806597\n",
      "Iteration: 8835 lambda_n: 0.940679401092438 Loss: 0.02424164115461323\n",
      "Iteration: 8836 lambda_n: 0.9427119702271685 Loss: 0.024233826958654153\n",
      "Iteration: 8837 lambda_n: 1.0032236040142157 Loss: 0.024225995878166512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8838 lambda_n: 1.029474026716994 Loss: 0.024217662129285913\n",
      "Iteration: 8839 lambda_n: 0.9409675096691261 Loss: 0.0242091103188761\n",
      "Iteration: 8840 lambda_n: 0.954175876465585 Loss: 0.024201293729455696\n",
      "Iteration: 8841 lambda_n: 0.9195136906191007 Loss: 0.02419336741848452\n",
      "Iteration: 8842 lambda_n: 0.9752397872114394 Loss: 0.024185729045237116\n",
      "Iteration: 8843 lambda_n: 1.023370618896511 Loss: 0.02417762775690482\n",
      "Iteration: 8844 lambda_n: 0.9223000683407696 Loss: 0.02416912664712473\n",
      "Iteration: 8845 lambda_n: 0.9962277816254024 Loss: 0.02416146512741197\n",
      "Iteration: 8846 lambda_n: 0.9788932922090974 Loss: 0.024153189492309855\n",
      "Iteration: 8847 lambda_n: 0.9028301676881445 Loss: 0.024145057854266978\n",
      "Iteration: 8848 lambda_n: 0.9226452773203588 Loss: 0.0241375580703473\n",
      "Iteration: 8849 lambda_n: 0.9562629750943427 Loss: 0.024129893682856635\n",
      "Iteration: 8850 lambda_n: 0.9907481777064652 Loss: 0.02412195003409433\n",
      "Iteration: 8851 lambda_n: 0.9622024898982011 Loss: 0.02411371991771941\n",
      "Iteration: 8852 lambda_n: 0.943618593905676 Loss: 0.024105726929508672\n",
      "Iteration: 8853 lambda_n: 0.892940218136065 Loss: 0.02409788831714915\n",
      "Iteration: 8854 lambda_n: 1.0200842933645657 Loss: 0.02409047068855897\n",
      "Iteration: 8855 lambda_n: 0.9115803993637319 Loss: 0.024081996877776634\n",
      "Iteration: 8856 lambda_n: 0.9130167026063464 Loss: 0.024074424405674772\n",
      "Iteration: 8857 lambda_n: 0.8951481010364081 Loss: 0.024066840002212545\n",
      "Iteration: 8858 lambda_n: 0.8767165812129797 Loss: 0.02405940403267903\n",
      "Iteration: 8859 lambda_n: 1.0276599313334058 Loss: 0.024052121173221894\n",
      "Iteration: 8860 lambda_n: 0.8813507470955528 Loss: 0.024043584431659465\n",
      "Iteration: 8861 lambda_n: 0.8988681662093212 Loss: 0.02403626307625878\n",
      "Iteration: 8862 lambda_n: 0.9349171271054152 Loss: 0.024028796204131224\n",
      "Iteration: 8863 lambda_n: 0.912967133114782 Loss: 0.024021029874287635\n",
      "Iteration: 8864 lambda_n: 0.9830268158142399 Loss: 0.02401344588238616\n",
      "Iteration: 8865 lambda_n: 0.9347993498684849 Loss: 0.024005279906670212\n",
      "Iteration: 8866 lambda_n: 0.902385100849811 Loss: 0.02399751455510557\n",
      "Iteration: 8867 lambda_n: 0.9940725383680581 Loss: 0.023990018467754448\n",
      "Iteration: 8868 lambda_n: 1.0266856872039325 Loss: 0.023981760735438864\n",
      "Iteration: 8869 lambda_n: 0.958337635876323 Loss: 0.023973232086588234\n",
      "Iteration: 8870 lambda_n: 0.9720862837850082 Loss: 0.023965271203025135\n",
      "Iteration: 8871 lambda_n: 1.0021702505111014 Loss: 0.02395719610980227\n",
      "Iteration: 8872 lambda_n: 0.9084130814915071 Loss: 0.02394887110988741\n",
      "Iteration: 8873 lambda_n: 0.9103992024892364 Loss: 0.02394132494809164\n",
      "Iteration: 8874 lambda_n: 0.9951003126116573 Loss: 0.02393376228761738\n",
      "Iteration: 8875 lambda_n: 0.9482506760481997 Loss: 0.023925496017383113\n",
      "Iteration: 8876 lambda_n: 0.9974695738537721 Loss: 0.023917618925727257\n",
      "Iteration: 8877 lambda_n: 0.9026766043835542 Loss: 0.02390933297404316\n",
      "Iteration: 8878 lambda_n: 0.9430152266561077 Loss: 0.02390183446485943\n",
      "Iteration: 8879 lambda_n: 0.9385891060101148 Loss: 0.023894000863846982\n",
      "Iteration: 8880 lambda_n: 0.9069295286000593 Loss: 0.0238862040304661\n",
      "Iteration: 8881 lambda_n: 0.8881460711147166 Loss: 0.023878670192279555\n",
      "Iteration: 8882 lambda_n: 0.8826505598491439 Loss: 0.023871292387722513\n",
      "Iteration: 8883 lambda_n: 0.8931931292729939 Loss: 0.023863960234199084\n",
      "Iteration: 8884 lambda_n: 0.994496773459493 Loss: 0.023856540503821626\n",
      "Iteration: 8885 lambda_n: 0.9713562079306539 Loss: 0.023848279246874538\n",
      "Iteration: 8886 lambda_n: 1.0225430216602105 Loss: 0.02384021021792959\n",
      "Iteration: 8887 lambda_n: 0.9404108596366626 Loss: 0.02383171598151964\n",
      "Iteration: 8888 lambda_n: 0.9493900484408768 Loss: 0.023823904014662917\n",
      "Iteration: 8889 lambda_n: 0.9062726283832623 Loss: 0.023816017457907575\n",
      "Iteration: 8890 lambda_n: 0.8870519791341466 Loss: 0.023808489076336578\n",
      "Iteration: 8891 lambda_n: 0.9631478104286761 Loss: 0.023801120360141294\n",
      "Iteration: 8892 lambda_n: 0.9560523146374611 Loss: 0.023793119517960554\n",
      "Iteration: 8893 lambda_n: 0.9461721446452175 Loss: 0.02378517761783959\n",
      "Iteration: 8894 lambda_n: 0.8925960973408277 Loss: 0.02377731779199078\n",
      "Iteration: 8895 lambda_n: 0.896945826361907 Loss: 0.023769903020860905\n",
      "Iteration: 8896 lambda_n: 0.8942996534266786 Loss: 0.023762452116629067\n",
      "Iteration: 8897 lambda_n: 1.0268309868973404 Loss: 0.023755023194059713\n",
      "Iteration: 8898 lambda_n: 0.9237892454771268 Loss: 0.023746493337331295\n",
      "Iteration: 8899 lambda_n: 0.9954905401685679 Loss: 0.0237388194454834\n",
      "Iteration: 8900 lambda_n: 1.0126909175968473 Loss: 0.023730549932927718\n",
      "Iteration: 8901 lambda_n: 0.9006349069485897 Loss: 0.02372213753728142\n",
      "Iteration: 8902 lambda_n: 1.0221041879271167 Loss: 0.023714655987812687\n",
      "Iteration: 8903 lambda_n: 1.016655331067307 Loss: 0.023706165396334158\n",
      "Iteration: 8904 lambda_n: 1.0177184788356102 Loss: 0.023697720068331755\n",
      "Iteration: 8905 lambda_n: 1.0019585739509065 Loss: 0.02368926590876181\n",
      "Iteration: 8906 lambda_n: 1.0245499104578357 Loss: 0.023680942666263062\n",
      "Iteration: 8907 lambda_n: 0.941019641192199 Loss: 0.023672431758121875\n",
      "Iteration: 8908 lambda_n: 0.9302670579098608 Loss: 0.023664614733621282\n",
      "Iteration: 8909 lambda_n: 0.9111883065785515 Loss: 0.023656887030513652\n",
      "Iteration: 8910 lambda_n: 0.9176665758156937 Loss: 0.023649317814051225\n",
      "Iteration: 8911 lambda_n: 0.9234104334682499 Loss: 0.023641694782760833\n",
      "Iteration: 8912 lambda_n: 0.9467269821330924 Loss: 0.023634024037379225\n",
      "Iteration: 8913 lambda_n: 0.8924522921714936 Loss: 0.023626159602039093\n",
      "Iteration: 8914 lambda_n: 0.9943668857705115 Loss: 0.023618746025053185\n",
      "Iteration: 8915 lambda_n: 0.8952405613386017 Loss: 0.023610485846280664\n",
      "Iteration: 8916 lambda_n: 0.9830221826475242 Loss: 0.02360304910717578\n",
      "Iteration: 8917 lambda_n: 0.9479647964752984 Loss: 0.02359488316849798\n",
      "Iteration: 8918 lambda_n: 0.9060337813180418 Loss: 0.023587008450555336\n",
      "Iteration: 8919 lambda_n: 1.00650997727223 Loss: 0.023579482052400867\n",
      "Iteration: 8920 lambda_n: 1.0216024758528108 Loss: 0.023571121001178384\n",
      "Iteration: 8921 lambda_n: 0.9823808910561916 Loss: 0.02356263457695257\n",
      "Iteration: 8922 lambda_n: 1.0067345510222558 Loss: 0.023554473965349834\n",
      "Iteration: 8923 lambda_n: 0.8797264757026613 Loss: 0.023546111048526226\n",
      "Iteration: 8924 lambda_n: 0.9086932050545318 Loss: 0.023538803184344596\n",
      "Iteration: 8925 lambda_n: 0.9879660442001066 Loss: 0.023531254694302405\n",
      "Iteration: 8926 lambda_n: 0.9442427162556075 Loss: 0.02352304768689472\n",
      "Iteration: 8927 lambda_n: 0.9088434602244571 Loss: 0.023515203887976555\n",
      "Iteration: 8928 lambda_n: 1.0223932532254207 Loss: 0.023507654149708773\n",
      "Iteration: 8929 lambda_n: 0.8776548727896332 Loss: 0.023499161156333208\n",
      "Iteration: 8930 lambda_n: 0.9642577661509178 Loss: 0.023491870500786598\n",
      "Iteration: 8931 lambda_n: 0.9958396003798613 Loss: 0.023483860437303487\n",
      "Iteration: 8932 lambda_n: 1.0118898074277427 Loss: 0.023475588024346565\n",
      "Iteration: 8933 lambda_n: 0.9162740030229931 Loss: 0.02346718228272621\n",
      "Iteration: 8934 lambda_n: 0.9609164391899617 Loss: 0.023459570819019743\n",
      "Iteration: 8935 lambda_n: 0.8901811201462487 Loss: 0.02345158851176771\n",
      "Iteration: 8936 lambda_n: 0.8810610970274012 Loss: 0.023444193800906867\n",
      "Iteration: 8937 lambda_n: 1.0267833996475557 Loss: 0.023436874849818292\n",
      "Iteration: 8938 lambda_n: 0.8774049916198884 Loss: 0.02342834538742125\n",
      "Iteration: 8939 lambda_n: 0.9546403194315977 Loss: 0.023421056807467394\n",
      "Iteration: 8940 lambda_n: 0.9569304793151142 Loss: 0.023413126635680376\n",
      "Iteration: 8941 lambda_n: 0.9481763751611114 Loss: 0.02340517743957699\n",
      "Iteration: 8942 lambda_n: 0.9892335263304229 Loss: 0.02339730096356569\n",
      "Iteration: 8943 lambda_n: 0.9643784178312521 Loss: 0.02338908342686914\n",
      "Iteration: 8944 lambda_n: 0.9292192952577555 Loss: 0.023381072360880793\n",
      "Iteration: 8945 lambda_n: 0.9659652624947421 Loss: 0.02337335336077459\n",
      "Iteration: 8946 lambda_n: 0.9952875703900255 Loss: 0.023365329112872833\n",
      "Iteration: 8947 lambda_n: 0.8910706244808521 Loss: 0.023357061285315582\n",
      "Iteration: 8948 lambda_n: 0.8819818273600601 Loss: 0.023349659185156937\n",
      "Iteration: 8949 lambda_n: 0.9030611032976726 Loss: 0.023342332585380658\n",
      "Iteration: 8950 lambda_n: 0.9971197109887433 Loss: 0.023334830880599858\n",
      "Iteration: 8951 lambda_n: 1.021515291523671 Loss: 0.023326547833427688\n",
      "Iteration: 8952 lambda_n: 0.9696033395219325 Loss: 0.02331806213279139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8953 lambda_n: 0.9282465591022844 Loss: 0.023310007663354667\n",
      "Iteration: 8954 lambda_n: 1.0162518030143777 Loss: 0.02330229674358785\n",
      "Iteration: 8955 lambda_n: 0.9003445115327158 Loss: 0.023293854766556255\n",
      "Iteration: 8956 lambda_n: 0.919243779017713 Loss: 0.023286375628332488\n",
      "Iteration: 8957 lambda_n: 0.9324973180816319 Loss: 0.02327873949437481\n",
      "Iteration: 8958 lambda_n: 0.925171552298002 Loss: 0.023270993263600347\n",
      "Iteration: 8959 lambda_n: 0.9483660523504192 Loss: 0.023263307887754613\n",
      "Iteration: 8960 lambda_n: 0.9738028836480159 Loss: 0.023255429835789116\n",
      "Iteration: 8961 lambda_n: 1.0049471427871912 Loss: 0.02324734048071592\n",
      "Iteration: 8962 lambda_n: 0.9796648334052916 Loss: 0.023238992411080038\n",
      "Iteration: 8963 lambda_n: 0.960923490860302 Loss: 0.023230854360909752\n",
      "Iteration: 8964 lambda_n: 0.9704079675721415 Loss: 0.02322287199456585\n",
      "Iteration: 8965 lambda_n: 0.9652039242367817 Loss: 0.023214810840905733\n",
      "Iteration: 8966 lambda_n: 1.0246027070558936 Loss: 0.023206792917081883\n",
      "Iteration: 8967 lambda_n: 0.9634132239603824 Loss: 0.02319828156910231\n",
      "Iteration: 8968 lambda_n: 1.0136957277259346 Loss: 0.023190278520546937\n",
      "Iteration: 8969 lambda_n: 0.9132300372059627 Loss: 0.023181857776526905\n",
      "Iteration: 8970 lambda_n: 0.9121686900743947 Loss: 0.023174271598366696\n",
      "Iteration: 8971 lambda_n: 0.9027860134138447 Loss: 0.023166694236776014\n",
      "Iteration: 8972 lambda_n: 0.9237491082688304 Loss: 0.023159194816823138\n",
      "Iteration: 8973 lambda_n: 0.9497985304612871 Loss: 0.023151521256972993\n",
      "Iteration: 8974 lambda_n: 1.0105964346445362 Loss: 0.02314363130523564\n",
      "Iteration: 8975 lambda_n: 0.9692229527280822 Loss: 0.023135236306871972\n",
      "Iteration: 8976 lambda_n: 0.9936976601914097 Loss: 0.023127184996933742\n",
      "Iteration: 8977 lambda_n: 0.9109411152848077 Loss: 0.02311893037622065\n",
      "Iteration: 8978 lambda_n: 0.9600856764775608 Loss: 0.023111363211967256\n",
      "Iteration: 8979 lambda_n: 1.001519700246662 Loss: 0.02310338780510404\n",
      "Iteration: 8980 lambda_n: 0.9244985612512426 Loss: 0.0230950682068639\n",
      "Iteration: 8981 lambda_n: 0.9676007132377128 Loss: 0.02308738842121863\n",
      "Iteration: 8982 lambda_n: 0.9120009112816325 Loss: 0.02307935058709851\n",
      "Iteration: 8983 lambda_n: 0.9861036986070882 Loss: 0.023071774619081685\n",
      "Iteration: 8984 lambda_n: 0.9442364711571831 Loss: 0.023063583081110183\n",
      "Iteration: 8985 lambda_n: 1.0070530091880479 Loss: 0.023055739333101885\n",
      "Iteration: 8986 lambda_n: 0.9009695808179463 Loss: 0.023047373769719232\n",
      "Iteration: 8987 lambda_n: 1.010927564592899 Loss: 0.02303988943862739\n",
      "Iteration: 8988 lambda_n: 0.9795411822222067 Loss: 0.023031491689386088\n",
      "Iteration: 8989 lambda_n: 1.0218421531958959 Loss: 0.023023354666001132\n",
      "Iteration: 8990 lambda_n: 0.9448022677348122 Loss: 0.02301486624952627\n",
      "Iteration: 8991 lambda_n: 0.9601739780340359 Loss: 0.023007017801384484\n",
      "Iteration: 8992 lambda_n: 0.9023681169176246 Loss: 0.02299904166082887\n",
      "Iteration: 8993 lambda_n: 0.9017292899301892 Loss: 0.022991545712064146\n",
      "Iteration: 8994 lambda_n: 1.004075286657031 Loss: 0.02298405507000841\n",
      "Iteration: 8995 lambda_n: 0.8786078894259927 Loss: 0.022975714242375458\n",
      "Iteration: 8996 lambda_n: 1.0144483547196839 Loss: 0.0229684156691788\n",
      "Iteration: 8997 lambda_n: 0.9793716944588576 Loss: 0.022959988672712102\n",
      "Iteration: 8998 lambda_n: 0.9728750780751915 Loss: 0.022951853057150237\n",
      "Iteration: 8999 lambda_n: 0.9600593101929008 Loss: 0.022943771408802598\n",
      "Iteration: 9000 lambda_n: 0.946763452096467 Loss: 0.022935796220699148\n",
      "Iteration: 9001 lambda_n: 0.9131973536506439 Loss: 0.022927931480937584\n",
      "Iteration: 9002 lambda_n: 1.0056162372371533 Loss: 0.022920345573886235\n",
      "Iteration: 9003 lambda_n: 1.0036438908760144 Loss: 0.022911991945523767\n",
      "Iteration: 9004 lambda_n: 0.9430962833610521 Loss: 0.022903654701380515\n",
      "Iteration: 9005 lambda_n: 0.9350820403545613 Loss: 0.02289582042465382\n",
      "Iteration: 9006 lambda_n: 0.9633297067511766 Loss: 0.022888052722029074\n",
      "Iteration: 9007 lambda_n: 0.9086356477344621 Loss: 0.02288005036675138\n",
      "Iteration: 9008 lambda_n: 0.9038631336128387 Loss: 0.022872502353615504\n",
      "Iteration: 9009 lambda_n: 0.9795873554098813 Loss: 0.022864993985623742\n",
      "Iteration: 9010 lambda_n: 0.943200016009192 Loss: 0.022856856578444527\n",
      "Iteration: 9011 lambda_n: 0.8953846806684005 Loss: 0.022849021439955732\n",
      "Iteration: 9012 lambda_n: 0.8956397361573915 Loss: 0.02284158350222951\n",
      "Iteration: 9013 lambda_n: 0.9535979798377897 Loss: 0.022834143445755618\n",
      "Iteration: 9014 lambda_n: 0.9600279852852743 Loss: 0.022826221931619588\n",
      "Iteration: 9015 lambda_n: 0.9993500623729136 Loss: 0.022818247003582863\n",
      "Iteration: 9016 lambda_n: 0.9586082822124664 Loss: 0.022809945419677617\n",
      "Iteration: 9017 lambda_n: 0.942122610912675 Loss: 0.02280198228048024\n",
      "Iteration: 9018 lambda_n: 0.9321161754187804 Loss: 0.0227941560893685\n",
      "Iteration: 9019 lambda_n: 0.9418578327551625 Loss: 0.022786413022647924\n",
      "Iteration: 9020 lambda_n: 1.0255315006977124 Loss: 0.02277858903291384\n",
      "Iteration: 9021 lambda_n: 1.021864684059291 Loss: 0.022770069968545727\n",
      "Iteration: 9022 lambda_n: 0.977562956514967 Loss: 0.022761581364610817\n",
      "Iteration: 9023 lambda_n: 0.9855703431475633 Loss: 0.022753460774128754\n",
      "Iteration: 9024 lambda_n: 0.9574053752756252 Loss: 0.022745273666541135\n",
      "Iteration: 9025 lambda_n: 0.9608585443678066 Loss: 0.022737320524630865\n",
      "Iteration: 9026 lambda_n: 0.9797188568266911 Loss: 0.022729338697324165\n",
      "Iteration: 9027 lambda_n: 0.99597504555241 Loss: 0.022721200197865943\n",
      "Iteration: 9028 lambda_n: 0.9259185500513603 Loss: 0.022712926658639967\n",
      "Iteration: 9029 lambda_n: 0.94152672169615 Loss: 0.022705235076905514\n",
      "Iteration: 9030 lambda_n: 0.8877479452914581 Loss: 0.022697413838466223\n",
      "Iteration: 9031 lambda_n: 0.9823950889331012 Loss: 0.02269003933892942\n",
      "Iteration: 9032 lambda_n: 0.8763849291556602 Loss: 0.022681878607966503\n",
      "Iteration: 9033 lambda_n: 0.9509225307433822 Loss: 0.02267459850068081\n",
      "Iteration: 9034 lambda_n: 0.9080268696856699 Loss: 0.02266669921142272\n",
      "Iteration: 9035 lambda_n: 0.8866887573128512 Loss: 0.02265915625531454\n",
      "Iteration: 9036 lambda_n: 0.9199165390911022 Loss: 0.02265179055435072\n",
      "Iteration: 9037 lambda_n: 0.9878394059616467 Loss: 0.022644148831037346\n",
      "Iteration: 9038 lambda_n: 0.958146337154703 Loss: 0.022635942874183766\n",
      "Iteration: 9039 lambda_n: 0.9517168114684516 Loss: 0.022627983576886897\n",
      "Iteration: 9040 lambda_n: 0.8853667377409452 Loss: 0.022620077689486373\n",
      "Iteration: 9041 lambda_n: 0.8877901408894858 Loss: 0.022612722970455114\n",
      "Iteration: 9042 lambda_n: 0.9683987866205656 Loss: 0.02260534812026756\n",
      "Iteration: 9043 lambda_n: 0.9882899458238912 Loss: 0.022597303656096506\n",
      "Iteration: 9044 lambda_n: 0.9464530654349639 Loss: 0.02258909395656169\n",
      "Iteration: 9045 lambda_n: 0.9451922378193728 Loss: 0.022581231794923558\n",
      "Iteration: 9046 lambda_n: 0.8948716748239107 Loss: 0.022573380106940773\n",
      "Iteration: 9047 lambda_n: 0.9393595680101906 Loss: 0.022565946430591153\n",
      "Iteration: 9048 lambda_n: 0.9639404643241797 Loss: 0.022558143194431744\n",
      "Iteration: 9049 lambda_n: 0.9772785550468217 Loss: 0.022550135765381445\n",
      "Iteration: 9050 lambda_n: 1.0217849993314743 Loss: 0.022542017537141943\n",
      "Iteration: 9051 lambda_n: 0.9495339378926767 Loss: 0.022533529594988704\n",
      "Iteration: 9052 lambda_n: 0.8815667393915374 Loss: 0.022525641840568805\n",
      "Iteration: 9053 lambda_n: 0.9811778486737157 Loss: 0.02251831868794281\n",
      "Iteration: 9054 lambda_n: 0.93503574357086 Loss: 0.022510168068344685\n",
      "Iteration: 9055 lambda_n: 0.9873085705692024 Loss: 0.022502400750041812\n",
      "Iteration: 9056 lambda_n: 0.9543027990085443 Loss: 0.022494199202680403\n",
      "Iteration: 9057 lambda_n: 0.9256315744202672 Loss: 0.022486271833423995\n",
      "Iteration: 9058 lambda_n: 0.9568383394671438 Loss: 0.022478582635301055\n",
      "Iteration: 9059 lambda_n: 1.0226045075944563 Loss: 0.02247063420336394\n",
      "Iteration: 9060 lambda_n: 0.9841493615295328 Loss: 0.02246213945352323\n",
      "Iteration: 9061 lambda_n: 0.9439804881693794 Loss: 0.02245396414960483\n",
      "Iteration: 9062 lambda_n: 1.0236663404469393 Loss: 0.022446122527498856\n",
      "Iteration: 9063 lambda_n: 0.8812889710743281 Loss: 0.02243761895702364\n",
      "Iteration: 9064 lambda_n: 1.0166363700488212 Loss: 0.02243029811175973\n",
      "Iteration: 9065 lambda_n: 1.0218681540728958 Loss: 0.022421852939061692\n",
      "Iteration: 9066 lambda_n: 0.9886689525208271 Loss: 0.02241336430606073\n",
      "Iteration: 9067 lambda_n: 0.9051590687015947 Loss: 0.022405151457985342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9068 lambda_n: 0.9990371958486727 Loss: 0.02239763232440578\n",
      "Iteration: 9069 lambda_n: 0.9605746902868282 Loss: 0.022389333347585933\n",
      "Iteration: 9070 lambda_n: 0.9042183395472777 Loss: 0.022381353877826447\n",
      "Iteration: 9071 lambda_n: 1.0271877162661074 Loss: 0.0223738425588491\n",
      "Iteration: 9072 lambda_n: 0.9380306921739502 Loss: 0.02236530973635316\n",
      "Iteration: 9073 lambda_n: 0.9499295034968447 Loss: 0.022357517539006923\n",
      "Iteration: 9074 lambda_n: 0.9293158465438233 Loss: 0.02234962649853124\n",
      "Iteration: 9075 lambda_n: 0.9835268218298779 Loss: 0.022341906695181518\n",
      "Iteration: 9076 lambda_n: 0.9694558543310199 Loss: 0.022333736562621512\n",
      "Iteration: 9077 lambda_n: 0.9103425154341818 Loss: 0.02232568331723095\n",
      "Iteration: 9078 lambda_n: 0.9393774167955469 Loss: 0.022318121124855785\n",
      "Iteration: 9079 lambda_n: 1.0209775414176347 Loss: 0.02231031774028282\n",
      "Iteration: 9080 lambda_n: 0.9903045929001555 Loss: 0.022301836505524773\n",
      "Iteration: 9081 lambda_n: 0.8955006656038249 Loss: 0.02229361007017519\n",
      "Iteration: 9082 lambda_n: 0.9957398372262406 Loss: 0.02228617116866213\n",
      "Iteration: 9083 lambda_n: 0.9238437680792206 Loss: 0.022277899582869087\n",
      "Iteration: 9084 lambda_n: 0.8841266233283267 Loss: 0.022270225235911677\n",
      "Iteration: 9085 lambda_n: 0.8890147004156812 Loss: 0.02226288081827477\n",
      "Iteration: 9086 lambda_n: 0.9383128345373326 Loss: 0.022255495795502366\n",
      "Iteration: 9087 lambda_n: 0.9480399043155235 Loss: 0.022247701254367164\n",
      "Iteration: 9088 lambda_n: 0.9400438974130109 Loss: 0.022239825910705724\n",
      "Iteration: 9089 lambda_n: 0.9268201548228598 Loss: 0.022232016989670575\n",
      "Iteration: 9090 lambda_n: 1.0128650364642031 Loss: 0.022224317917930832\n",
      "Iteration: 9091 lambda_n: 1.0267303558638887 Loss: 0.022215904073519994\n",
      "Iteration: 9092 lambda_n: 0.8919768811556386 Loss: 0.022207375050247297\n",
      "Iteration: 9093 lambda_n: 0.8988403363828635 Loss: 0.022199965420700924\n",
      "Iteration: 9094 lambda_n: 0.8993637142022249 Loss: 0.022192498776603026\n",
      "Iteration: 9095 lambda_n: 0.9570046317443446 Loss: 0.02218502779228423\n",
      "Iteration: 9096 lambda_n: 0.895627001569764 Loss: 0.022177077983719158\n",
      "Iteration: 9097 lambda_n: 0.933172701218482 Loss: 0.02216963803530295\n",
      "Iteration: 9098 lambda_n: 0.907065091484072 Loss: 0.022161886194632434\n",
      "Iteration: 9099 lambda_n: 1.0006754270375713 Loss: 0.022154351228426704\n",
      "Iteration: 9100 lambda_n: 0.8824311392709688 Loss: 0.022146038643106995\n",
      "Iteration: 9101 lambda_n: 1.011217570223642 Loss: 0.02213870830979757\n",
      "Iteration: 9102 lambda_n: 0.9415436598883838 Loss: 0.02213030815069361\n",
      "Iteration: 9103 lambda_n: 0.9614441367926145 Loss: 0.02212248677093698\n",
      "Iteration: 9104 lambda_n: 1.0033828993962315 Loss: 0.02211450007838194\n",
      "Iteration: 9105 lambda_n: 0.9817696996500274 Loss: 0.022106165001560513\n",
      "Iteration: 9106 lambda_n: 0.9943882899037063 Loss: 0.022098009465058983\n",
      "Iteration: 9107 lambda_n: 0.9836876855267519 Loss: 0.02208974910625566\n",
      "Iteration: 9108 lambda_n: 0.9477270204252755 Loss: 0.022081577637123102\n",
      "Iteration: 9109 lambda_n: 1.0108639277897067 Loss: 0.02207370489235622\n",
      "Iteration: 9110 lambda_n: 0.9175029477506077 Loss: 0.022065307670887022\n",
      "Iteration: 9111 lambda_n: 0.9101152162189797 Loss: 0.022057685996765712\n",
      "Iteration: 9112 lambda_n: 0.9886670876849655 Loss: 0.02205012569235625\n",
      "Iteration: 9113 lambda_n: 1.00695166252828 Loss: 0.022041912859519314\n",
      "Iteration: 9114 lambda_n: 0.9160566305791932 Loss: 0.022033548137184653\n",
      "Iteration: 9115 lambda_n: 0.9107736329165464 Loss: 0.022025938477618957\n",
      "Iteration: 9116 lambda_n: 0.8755163311089511 Loss: 0.02201837270378872\n",
      "Iteration: 9117 lambda_n: 1.0152852577159421 Loss: 0.02201109981148892\n",
      "Iteration: 9118 lambda_n: 0.8772480347566807 Loss: 0.022002665862212133\n",
      "Iteration: 9119 lambda_n: 0.9621871937965759 Loss: 0.02199537858470191\n",
      "Iteration: 9120 lambda_n: 0.8963459002098857 Loss: 0.021987385719723872\n",
      "Iteration: 9121 lambda_n: 0.9143785448234172 Loss: 0.021979939796729513\n",
      "Iteration: 9122 lambda_n: 1.0279422505792077 Loss: 0.021972344077010184\n",
      "Iteration: 9123 lambda_n: 0.9713921513776618 Loss: 0.02196380498642979\n",
      "Iteration: 9124 lambda_n: 1.020030751303055 Loss: 0.021955735656112738\n",
      "Iteration: 9125 lambda_n: 1.029765233614539 Loss: 0.021947262286165704\n",
      "Iteration: 9126 lambda_n: 0.9425354946302303 Loss: 0.021938708052120868\n",
      "Iteration: 9127 lambda_n: 0.9386472128810865 Loss: 0.021930878433338654\n",
      "Iteration: 9128 lambda_n: 1.0145903856488638 Loss: 0.021923081114418284\n",
      "Iteration: 9129 lambda_n: 0.8864869697092117 Loss: 0.021914652937464776\n",
      "Iteration: 9130 lambda_n: 0.9148466663205171 Loss: 0.02190728891238456\n",
      "Iteration: 9131 lambda_n: 0.9292462659444032 Loss: 0.021899689304016725\n",
      "Iteration: 9132 lambda_n: 0.9998558056048477 Loss: 0.021891970078537795\n",
      "Iteration: 9133 lambda_n: 0.8985566973395449 Loss: 0.021883664301383252\n",
      "Iteration: 9134 lambda_n: 0.8864548092359762 Loss: 0.021876200013387855\n",
      "Iteration: 9135 lambda_n: 0.8998593534056486 Loss: 0.021868836255476427\n",
      "Iteration: 9136 lambda_n: 0.8995853075430139 Loss: 0.021861361146354757\n",
      "Iteration: 9137 lambda_n: 0.9980860618670152 Loss: 0.02185388831372782\n",
      "Iteration: 9138 lambda_n: 0.9865618230011804 Loss: 0.021845597237804763\n",
      "Iteration: 9139 lambda_n: 0.904743738235501 Loss: 0.02183740189344859\n",
      "Iteration: 9140 lambda_n: 0.9165007015803187 Loss: 0.021829886209875863\n",
      "Iteration: 9141 lambda_n: 0.9289740406839753 Loss: 0.02182227286150609\n",
      "Iteration: 9142 lambda_n: 0.9738220095770144 Loss: 0.02181455589742398\n",
      "Iteration: 9143 lambda_n: 0.9961716459916071 Loss: 0.021806466382391605\n",
      "Iteration: 9144 lambda_n: 0.9274207810849824 Loss: 0.021798191209493323\n",
      "Iteration: 9145 lambda_n: 1.015604704174075 Loss: 0.02179048714830975\n",
      "Iteration: 9146 lambda_n: 0.8982694783461037 Loss: 0.02178205054549183\n",
      "Iteration: 9147 lambda_n: 1.016900183297191 Loss: 0.021774588643456026\n",
      "Iteration: 9148 lambda_n: 1.0082921472692195 Loss: 0.02176614127913323\n",
      "Iteration: 9149 lambda_n: 0.910233659945034 Loss: 0.021757765421553995\n",
      "Iteration: 9150 lambda_n: 0.89325885601647 Loss: 0.021750204133372628\n",
      "Iteration: 9151 lambda_n: 1.001474240922506 Loss: 0.021742783854465025\n",
      "Iteration: 9152 lambda_n: 0.9514996418641395 Loss: 0.021734464633074733\n",
      "Iteration: 9153 lambda_n: 0.908367349735633 Loss: 0.021726560549428602\n",
      "Iteration: 9154 lambda_n: 0.9271962073681377 Loss: 0.021719014764654248\n",
      "Iteration: 9155 lambda_n: 0.890624545480036 Loss: 0.021711312569036183\n",
      "Iteration: 9156 lambda_n: 0.9848380497354811 Loss: 0.021703914173298842\n",
      "Iteration: 9157 lambda_n: 0.9493109462779069 Loss: 0.021695733148352235\n",
      "Iteration: 9158 lambda_n: 1.0204807466734962 Loss: 0.021687847246166018\n",
      "Iteration: 9159 lambda_n: 0.9834827078929271 Loss: 0.021679370138241016\n",
      "Iteration: 9160 lambda_n: 0.9660403671516398 Loss: 0.0216712003720993\n",
      "Iteration: 9161 lambda_n: 0.9644036353254967 Loss: 0.021663175499048497\n",
      "Iteration: 9162 lambda_n: 0.901938896999021 Loss: 0.02165516422229264\n",
      "Iteration: 9163 lambda_n: 0.9419465904860318 Loss: 0.021647671838551905\n",
      "Iteration: 9164 lambda_n: 0.8932413660527657 Loss: 0.02163984711191215\n",
      "Iteration: 9165 lambda_n: 0.9951604872805498 Loss: 0.02163242697835115\n",
      "Iteration: 9166 lambda_n: 0.9566679974187092 Loss: 0.02162416020521953\n",
      "Iteration: 9167 lambda_n: 1.0070802587818084 Loss: 0.021616213188237858\n",
      "Iteration: 9168 lambda_n: 0.9409548548230584 Loss: 0.021607847397875714\n",
      "Iteration: 9169 lambda_n: 0.9739020783378104 Loss: 0.021600030909585087\n",
      "Iteration: 9170 lambda_n: 0.9040533886249255 Loss: 0.02159194072954434\n",
      "Iteration: 9171 lambda_n: 0.9580273668904118 Loss: 0.021584430780815598\n",
      "Iteration: 9172 lambda_n: 0.9495439376964432 Loss: 0.02157647247161612\n",
      "Iteration: 9173 lambda_n: 0.9194072822893977 Loss: 0.021568584634058335\n",
      "Iteration: 9174 lambda_n: 0.9628409590546112 Loss: 0.02156094714095364\n",
      "Iteration: 9175 lambda_n: 0.999847174678227 Loss: 0.021552948845502466\n",
      "Iteration: 9176 lambda_n: 0.9639777374784094 Loss: 0.02154464316642741\n",
      "Iteration: 9177 lambda_n: 0.9371059274926512 Loss: 0.02153663544206635\n",
      "Iteration: 9178 lambda_n: 0.967426490889881 Loss: 0.021528850934665757\n",
      "Iteration: 9179 lambda_n: 0.9513926808826532 Loss: 0.021520814518340373\n",
      "Iteration: 9180 lambda_n: 0.9976155151587599 Loss: 0.021512911306218824\n",
      "Iteration: 9181 lambda_n: 0.922393468669754 Loss: 0.02150462412860579\n",
      "Iteration: 9182 lambda_n: 0.907816993097095 Loss: 0.02149696182383351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9183 lambda_n: 0.9219467561954471 Loss: 0.021489420607951563\n",
      "Iteration: 9184 lambda_n: 1.0197652298889506 Loss: 0.02148176201791133\n",
      "Iteration: 9185 lambda_n: 0.9865978448092946 Loss: 0.021473290853122034\n",
      "Iteration: 9186 lambda_n: 0.9624588558998661 Loss: 0.02146509520962616\n",
      "Iteration: 9187 lambda_n: 0.9124201296564518 Loss: 0.02145710008841715\n",
      "Iteration: 9188 lambda_n: 0.9051075107064753 Loss: 0.021449520637774173\n",
      "Iteration: 9189 lambda_n: 0.9734974188961928 Loss: 0.021442001932925826\n",
      "Iteration: 9190 lambda_n: 0.9867222543594173 Loss: 0.021433915114884176\n",
      "Iteration: 9191 lambda_n: 0.950789303525577 Loss: 0.021425718438467686\n",
      "Iteration: 9192 lambda_n: 0.8756928691869896 Loss: 0.021417820256130652\n",
      "Iteration: 9193 lambda_n: 0.8931221374080648 Loss: 0.021410545897923756\n",
      "Iteration: 9194 lambda_n: 0.9764915689955396 Loss: 0.021403126755212772\n",
      "Iteration: 9195 lambda_n: 0.9738092803440651 Loss: 0.0213950150647501\n",
      "Iteration: 9196 lambda_n: 0.9272555544500469 Loss: 0.021386925655971112\n",
      "Iteration: 9197 lambda_n: 1.0109799277743168 Loss: 0.021379222967786514\n",
      "Iteration: 9198 lambda_n: 0.9595948926017424 Loss: 0.021370824783361615\n",
      "Iteration: 9199 lambda_n: 0.902421333727991 Loss: 0.021362853453100353\n",
      "Iteration: 9200 lambda_n: 0.9015700461460505 Loss: 0.02135535706212545\n",
      "Iteration: 9201 lambda_n: 1.008378420504263 Loss: 0.02134786774277078\n",
      "Iteration: 9202 lambda_n: 0.9283126653911813 Loss: 0.021339491168975707\n",
      "Iteration: 9203 lambda_n: 0.9081355827358972 Loss: 0.02133177969936224\n",
      "Iteration: 9204 lambda_n: 0.9975614804889308 Loss: 0.02132423584025928\n",
      "Iteration: 9205 lambda_n: 1.0123929070614843 Loss: 0.021315949122508695\n",
      "Iteration: 9206 lambda_n: 0.9070148502276173 Loss: 0.021307539200478676\n",
      "Iteration: 9207 lambda_n: 0.8794895370737057 Loss: 0.02130000465127819\n",
      "Iteration: 9208 lambda_n: 0.8848320345061617 Loss: 0.02129269875415416\n",
      "Iteration: 9209 lambda_n: 1.0105426687206402 Loss: 0.02128534847704398\n",
      "Iteration: 9210 lambda_n: 1.0234130394097962 Loss: 0.02127695392491209\n",
      "Iteration: 9211 lambda_n: 0.9879543798098347 Loss: 0.021268452458945448\n",
      "Iteration: 9212 lambda_n: 0.9669954917727719 Loss: 0.02126024554716357\n",
      "Iteration: 9213 lambda_n: 0.9659520814505002 Loss: 0.02125221274033467\n",
      "Iteration: 9214 lambda_n: 1.019478332944094 Loss: 0.021244188601094734\n",
      "Iteration: 9215 lambda_n: 0.9365968283560413 Loss: 0.021235719820659606\n",
      "Iteration: 9216 lambda_n: 0.8870335892458958 Loss: 0.021227939534769015\n",
      "Iteration: 9217 lambda_n: 0.9184843472163448 Loss: 0.02122057096944321\n",
      "Iteration: 9218 lambda_n: 0.9124313698314961 Loss: 0.02121294117524104\n",
      "Iteration: 9219 lambda_n: 0.9983043866592506 Loss: 0.021205361650346748\n",
      "Iteration: 9220 lambda_n: 0.8980644468446909 Loss: 0.021197068773713797\n",
      "Iteration: 9221 lambda_n: 0.9993016835588026 Loss: 0.021189608581116744\n",
      "Iteration: 9222 lambda_n: 0.9632547178562395 Loss: 0.02118130741090674\n",
      "Iteration: 9223 lambda_n: 1.0046891647900809 Loss: 0.021173305679739737\n",
      "Iteration: 9224 lambda_n: 0.981535958512953 Loss: 0.021164959752592447\n",
      "Iteration: 9225 lambda_n: 0.9062247765378197 Loss: 0.02115680615791723\n",
      "Iteration: 9226 lambda_n: 0.9987010568832662 Loss: 0.021149278171091832\n",
      "Iteration: 9227 lambda_n: 0.9273119108525057 Loss: 0.021140981985999615\n",
      "Iteration: 9228 lambda_n: 0.9508982593058626 Loss: 0.0211332788287773\n",
      "Iteration: 9229 lambda_n: 0.9111148327004506 Loss: 0.021125379740380815\n",
      "Iteration: 9230 lambda_n: 0.9777639728683564 Loss: 0.021117811131988504\n",
      "Iteration: 9231 lambda_n: 0.9038895731153851 Loss: 0.021109688870906414\n",
      "Iteration: 9232 lambda_n: 0.9707405885273003 Loss: 0.021102180282698977\n",
      "Iteration: 9233 lambda_n: 1.0099293316107643 Loss: 0.021094116364832795\n",
      "Iteration: 9234 lambda_n: 0.9810153754301456 Loss: 0.02108572690711954\n",
      "Iteration: 9235 lambda_n: 0.9479600035834681 Loss: 0.021077577636974848\n",
      "Iteration: 9236 lambda_n: 0.9435121557551305 Loss: 0.021069702957022863\n",
      "Iteration: 9237 lambda_n: 0.9350786621860285 Loss: 0.021061865225270877\n",
      "Iteration: 9238 lambda_n: 1.0033482880699294 Loss: 0.021054097550372304\n",
      "Iteration: 9239 lambda_n: 0.997847644677137 Loss: 0.0210457627614439\n",
      "Iteration: 9240 lambda_n: 0.9575755179194034 Loss: 0.021037473666252138\n",
      "Iteration: 9241 lambda_n: 0.9294800569692728 Loss: 0.02102951911062629\n",
      "Iteration: 9242 lambda_n: 0.9517878255178653 Loss: 0.021021797943306046\n",
      "Iteration: 9243 lambda_n: 0.924963492334793 Loss: 0.0210138914659368\n",
      "Iteration: 9244 lambda_n: 0.9662526683951151 Loss: 0.021006207817642012\n",
      "Iteration: 9245 lambda_n: 0.9107507356662133 Loss: 0.020998181181225317\n",
      "Iteration: 9246 lambda_n: 0.9117682281380782 Loss: 0.020990615597971694\n",
      "Iteration: 9247 lambda_n: 0.9471655974400207 Loss: 0.020983041562447922\n",
      "Iteration: 9248 lambda_n: 0.9869315912355616 Loss: 0.020975173481890128\n",
      "Iteration: 9249 lambda_n: 0.9223643069019913 Loss: 0.02096697506624671\n",
      "Iteration: 9250 lambda_n: 1.0065407820618957 Loss: 0.02095931300940577\n",
      "Iteration: 9251 lambda_n: 0.9535057512010525 Loss: 0.020950951700740385\n",
      "Iteration: 9252 lambda_n: 0.9362147250167603 Loss: 0.02094303095273942\n",
      "Iteration: 9253 lambda_n: 0.951297866156277 Loss: 0.020935253840864435\n",
      "Iteration: 9254 lambda_n: 0.937551173988454 Loss: 0.020927351433773903\n",
      "Iteration: 9255 lambda_n: 0.9425260989134471 Loss: 0.020919563220239884\n",
      "Iteration: 9256 lambda_n: 0.9327564078226497 Loss: 0.020911733680216208\n",
      "Iteration: 9257 lambda_n: 0.8797156427809724 Loss: 0.02090398529681745\n",
      "Iteration: 9258 lambda_n: 0.8960140105887042 Loss: 0.02089667752171553\n",
      "Iteration: 9259 lambda_n: 0.9278150420982794 Loss: 0.020889234356513874\n",
      "Iteration: 9260 lambda_n: 0.9700404003679088 Loss: 0.020881526996850336\n",
      "Iteration: 9261 lambda_n: 0.9108809776558402 Loss: 0.020873468881085534\n",
      "Iteration: 9262 lambda_n: 0.8808225380748138 Loss: 0.02086590220823566\n",
      "Iteration: 9263 lambda_n: 0.9952620187217175 Loss: 0.020858585233757353\n",
      "Iteration: 9264 lambda_n: 1.0080696283979536 Loss: 0.02085031761522787\n",
      "Iteration: 9265 lambda_n: 0.9743798174110123 Loss: 0.020841943605844997\n",
      "Iteration: 9266 lambda_n: 0.9807339978374524 Loss: 0.020833849457794915\n",
      "Iteration: 9267 lambda_n: 0.8861795997588712 Loss: 0.020825702526195818\n",
      "Iteration: 9268 lambda_n: 0.9466437260363121 Loss: 0.020818341055710024\n",
      "Iteration: 9269 lambda_n: 0.8890231756449957 Loss: 0.020810477311411422\n",
      "Iteration: 9270 lambda_n: 1.018719449776451 Loss: 0.02080309221951989\n",
      "Iteration: 9271 lambda_n: 0.9069925472944673 Loss: 0.020794629744117745\n",
      "Iteration: 9272 lambda_n: 1.0177413074930763 Loss: 0.020787095381097933\n",
      "Iteration: 9273 lambda_n: 1.0155092848828415 Loss: 0.020778641031033187\n",
      "Iteration: 9274 lambda_n: 0.9703065285442742 Loss: 0.020770205222282883\n",
      "Iteration: 9275 lambda_n: 0.9967015140006892 Loss: 0.020762144911599896\n",
      "Iteration: 9276 lambda_n: 1.0116855898986148 Loss: 0.020753865338444238\n",
      "Iteration: 9277 lambda_n: 0.9250582649490166 Loss: 0.020745461292944373\n",
      "Iteration: 9278 lambda_n: 0.9785308035852484 Loss: 0.02073777685833186\n",
      "Iteration: 9279 lambda_n: 0.9450074003417379 Loss: 0.02072964822874354\n",
      "Iteration: 9280 lambda_n: 0.9983379388483152 Loss: 0.020721798077179304\n",
      "Iteration: 9281 lambda_n: 0.9770717176878264 Loss: 0.020713504910237247\n",
      "Iteration: 9282 lambda_n: 0.8883333163259848 Loss: 0.02070538840123416\n",
      "Iteration: 9283 lambda_n: 0.9824821525908989 Loss: 0.02069800903979307\n",
      "Iteration: 9284 lambda_n: 0.9454618227795841 Loss: 0.02068984758645531\n",
      "Iteration: 9285 lambda_n: 0.9728466092140299 Loss: 0.020681993660025523\n",
      "Iteration: 9286 lambda_n: 0.9465145067776208 Loss: 0.020673912248903067\n",
      "Iteration: 9287 lambda_n: 0.8971563826020956 Loss: 0.02066604957786756\n",
      "Iteration: 9288 lambda_n: 0.9674804360975668 Loss: 0.020658596923473934\n",
      "Iteration: 9289 lambda_n: 1.0202629049919687 Loss: 0.02065056008903257\n",
      "Iteration: 9290 lambda_n: 0.9941472292452396 Loss: 0.020642084792025884\n",
      "Iteration: 9291 lambda_n: 1.0002092514180345 Loss: 0.020633826437258244\n",
      "Iteration: 9292 lambda_n: 0.8850664145542055 Loss: 0.02062551772544359\n",
      "Iteration: 9293 lambda_n: 1.0078904966407458 Loss: 0.02061816550214042\n",
      "Iteration: 9294 lambda_n: 1.0065428647914365 Loss: 0.020609792982447032\n",
      "Iteration: 9295 lambda_n: 0.8959887438547316 Loss: 0.020601431657508124\n",
      "Iteration: 9296 lambda_n: 0.8856867397704062 Loss: 0.02059398870273608\n",
      "Iteration: 9297 lambda_n: 0.9276823704031204 Loss: 0.020586631326449265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9298 lambda_n: 0.895685380267815 Loss: 0.020578925093582272\n",
      "Iteration: 9299 lambda_n: 0.9397378076824235 Loss: 0.020571484658875042\n",
      "Iteration: 9300 lambda_n: 0.8992381313365603 Loss: 0.02056367828183292\n",
      "Iteration: 9301 lambda_n: 0.9879001787591922 Loss: 0.020556208334538955\n",
      "Iteration: 9302 lambda_n: 0.9093479933261149 Loss: 0.02054800187398299\n",
      "Iteration: 9303 lambda_n: 0.9793877104306086 Loss: 0.020540447944357415\n",
      "Iteration: 9304 lambda_n: 0.9620070947133776 Loss: 0.020532312196675496\n",
      "Iteration: 9305 lambda_n: 1.0202619433383846 Loss: 0.020524320829319428\n",
      "Iteration: 9306 lambda_n: 0.9085757477523485 Loss: 0.0205158455404994\n",
      "Iteration: 9307 lambda_n: 0.9378931841640986 Loss: 0.02050829802594681\n",
      "Iteration: 9308 lambda_n: 0.9042594210810192 Loss: 0.020500506972242857\n",
      "Iteration: 9309 lambda_n: 1.0105233930279272 Loss: 0.020492995313325015\n",
      "Iteration: 9310 lambda_n: 0.9667425301043578 Loss: 0.020484600922438017\n",
      "Iteration: 9311 lambda_n: 0.9865863100203794 Loss: 0.020476570218025982\n",
      "Iteration: 9312 lambda_n: 0.9219302165528852 Loss: 0.02046837467187769\n",
      "Iteration: 9313 lambda_n: 0.9964730948684458 Loss: 0.02046071622218509\n",
      "Iteration: 9314 lambda_n: 0.8968292259310429 Loss: 0.020452438546807193\n",
      "Iteration: 9315 lambda_n: 1.009766183245686 Loss: 0.020444988610398823\n",
      "Iteration: 9316 lambda_n: 0.8822211016904774 Loss: 0.020436600509719627\n",
      "Iteration: 9317 lambda_n: 0.8965563866302191 Loss: 0.020429271922633982\n",
      "Iteration: 9318 lambda_n: 0.8874427197985308 Loss: 0.02042182425273204\n",
      "Iteration: 9319 lambda_n: 0.9698598692736504 Loss: 0.02041445228982849\n",
      "Iteration: 9320 lambda_n: 1.027901189827856 Loss: 0.02040639568988582\n",
      "Iteration: 9321 lambda_n: 0.9575772604072339 Loss: 0.020397856942264727\n",
      "Iteration: 9322 lambda_n: 1.015207040104039 Loss: 0.020389902373653998\n",
      "Iteration: 9323 lambda_n: 0.9402216850082082 Loss: 0.02038146907602521\n",
      "Iteration: 9324 lambda_n: 0.9383800718577139 Loss: 0.020373658679742213\n",
      "Iteration: 9325 lambda_n: 0.9314459955077935 Loss: 0.020365863581703943\n",
      "Iteration: 9326 lambda_n: 0.8865913909549517 Loss: 0.02035812608486516\n",
      "Iteration: 9327 lambda_n: 0.9248379151813666 Loss: 0.020350761194032806\n",
      "Iteration: 9328 lambda_n: 1.0269255139295193 Loss: 0.02034307859036603\n",
      "Iteration: 9329 lambda_n: 0.9380949163717753 Loss: 0.020334547947780696\n",
      "Iteration: 9330 lambda_n: 0.9996386546675656 Loss: 0.0203267552185911\n",
      "Iteration: 9331 lambda_n: 0.9335410253481726 Loss: 0.02031845124724173\n",
      "Iteration: 9332 lambda_n: 0.9787368046318108 Loss: 0.020310696347131056\n",
      "Iteration: 9333 lambda_n: 0.934700160123319 Loss: 0.020302566006916444\n",
      "Iteration: 9334 lambda_n: 0.9040098383802416 Loss: 0.0202948014779333\n",
      "Iteration: 9335 lambda_n: 0.9136602072400787 Loss: 0.020287291892637265\n",
      "Iteration: 9336 lambda_n: 1.0147738675354865 Loss: 0.020279702142001487\n",
      "Iteration: 9337 lambda_n: 1.0193495439534492 Loss: 0.02027127244294015\n",
      "Iteration: 9338 lambda_n: 1.0192785839400844 Loss: 0.02026280473387564\n",
      "Iteration: 9339 lambda_n: 0.9351723546984297 Loss: 0.02025433761429115\n",
      "Iteration: 9340 lambda_n: 1.001089986778902 Loss: 0.02024656916288935\n",
      "Iteration: 9341 lambda_n: 0.8950069359560433 Loss: 0.020238253135520943\n",
      "Iteration: 9342 lambda_n: 0.8976446970741283 Loss: 0.020230818337192313\n",
      "Iteration: 9343 lambda_n: 0.971161521638262 Loss: 0.020223361627067095\n",
      "Iteration: 9344 lambda_n: 1.0221932906882263 Loss: 0.020215294214691867\n",
      "Iteration: 9345 lambda_n: 0.9553407514250425 Loss: 0.02020680288281492\n",
      "Iteration: 9346 lambda_n: 0.9734476159204719 Loss: 0.02019886689317995\n",
      "Iteration: 9347 lambda_n: 1.028285723630723 Loss: 0.02019078049033015\n",
      "Iteration: 9348 lambda_n: 0.9262407846315666 Loss: 0.020182238548829917\n",
      "Iteration: 9349 lambda_n: 0.9919240682957677 Loss: 0.020174544291873823\n",
      "Iteration: 9350 lambda_n: 0.8936990392890876 Loss: 0.020166304405686886\n",
      "Iteration: 9351 lambda_n: 0.9689849147261042 Loss: 0.020158880472151603\n",
      "Iteration: 9352 lambda_n: 1.0240373433234726 Loss: 0.020150831140919066\n",
      "Iteration: 9353 lambda_n: 1.0088678578849835 Loss: 0.020142324490683015\n",
      "Iteration: 9354 lambda_n: 1.0025362999261875 Loss: 0.020133943852965883\n",
      "Iteration: 9355 lambda_n: 1.0118428661010899 Loss: 0.020125615811345195\n",
      "Iteration: 9356 lambda_n: 1.0183169046362015 Loss: 0.02011721046035176\n",
      "Iteration: 9357 lambda_n: 0.9931113360141868 Loss: 0.020108751329715596\n",
      "Iteration: 9358 lambda_n: 0.9421314922057057 Loss: 0.020100501581065144\n",
      "Iteration: 9359 lambda_n: 0.9674391689222436 Loss: 0.020092675320596033\n",
      "Iteration: 9360 lambda_n: 1.0113850115342096 Loss: 0.020084638829966955\n",
      "Iteration: 9361 lambda_n: 0.8962606826695729 Loss: 0.02007623728244716\n",
      "Iteration: 9362 lambda_n: 0.9489505029137368 Loss: 0.020068792069581082\n",
      "Iteration: 9363 lambda_n: 0.8905095035822425 Loss: 0.020060909163841772\n",
      "Iteration: 9364 lambda_n: 0.9876030509821087 Loss: 0.02005351172589184\n",
      "Iteration: 9365 lambda_n: 0.9436142834889483 Loss: 0.0200453077345316\n",
      "Iteration: 9366 lambda_n: 0.94829938862132 Loss: 0.020037469156668472\n",
      "Iteration: 9367 lambda_n: 0.9913349179289535 Loss: 0.020029591659782384\n",
      "Iteration: 9368 lambda_n: 1.0167909288411732 Loss: 0.020021356667958085\n",
      "Iteration: 9369 lambda_n: 0.9157614610546928 Loss: 0.020012910213771936\n",
      "Iteration: 9370 lambda_n: 0.882659938308977 Loss: 0.020005303008602617\n",
      "Iteration: 9371 lambda_n: 1.0043075059706938 Loss: 0.019997970776883214\n",
      "Iteration: 9372 lambda_n: 0.9843876820389983 Loss: 0.019989628022198298\n",
      "Iteration: 9373 lambda_n: 0.9564876025196619 Loss: 0.019981450740958154\n",
      "Iteration: 9374 lambda_n: 0.8834225072984073 Loss: 0.01997350522492387\n",
      "Iteration: 9375 lambda_n: 1.0208259326017701 Loss: 0.01996616665862782\n",
      "Iteration: 9376 lambda_n: 0.9875333864982659 Loss: 0.019957686685902674\n",
      "Iteration: 9377 lambda_n: 0.8945411958872913 Loss: 0.01994948327345372\n",
      "Iteration: 9378 lambda_n: 1.0063443171254962 Loss: 0.019942052344568344\n",
      "Iteration: 9379 lambda_n: 0.942911069688963 Loss: 0.0199336926702765\n",
      "Iteration: 9380 lambda_n: 1.0084249754907944 Loss: 0.01992585993422659\n",
      "Iteration: 9381 lambda_n: 0.9719312978125635 Loss: 0.019917482976000964\n",
      "Iteration: 9382 lambda_n: 0.9444109232642874 Loss: 0.019909409169759192\n",
      "Iteration: 9383 lambda_n: 1.0024284330264925 Loss: 0.0199015639745202\n",
      "Iteration: 9384 lambda_n: 1.012335937253044 Loss: 0.01989323682946234\n",
      "Iteration: 9385 lambda_n: 0.9021361528795212 Loss: 0.01988482738306273\n",
      "Iteration: 9386 lambda_n: 0.9987717363089866 Loss: 0.01987733336321526\n",
      "Iteration: 9387 lambda_n: 0.9644637902934944 Loss: 0.019869036594291338\n",
      "Iteration: 9388 lambda_n: 0.9073045510533487 Loss: 0.019861024820535166\n",
      "Iteration: 9389 lambda_n: 1.0186321065728599 Loss: 0.01985348786699862\n",
      "Iteration: 9390 lambda_n: 1.0133502463880093 Loss: 0.019845026118590854\n",
      "Iteration: 9391 lambda_n: 0.9688747578904797 Loss: 0.019836608246468244\n",
      "Iteration: 9392 lambda_n: 0.9221392464537809 Loss: 0.019828559831002318\n",
      "Iteration: 9393 lambda_n: 0.8921664190689709 Loss: 0.019820899646137796\n",
      "Iteration: 9394 lambda_n: 0.8773885403512153 Loss: 0.01981348844472622\n",
      "Iteration: 9395 lambda_n: 0.8882372457290951 Loss: 0.01980620000275426\n",
      "Iteration: 9396 lambda_n: 0.9991141369362457 Loss: 0.01979882144090739\n",
      "Iteration: 9397 lambda_n: 0.8879273230922198 Loss: 0.019790521827859855\n",
      "Iteration: 9398 lambda_n: 0.8975925174742235 Loss: 0.019783145863515296\n",
      "Iteration: 9399 lambda_n: 0.9788663405013892 Loss: 0.019775689602003075\n",
      "Iteration: 9400 lambda_n: 0.9918974551680456 Loss: 0.019767558196040643\n",
      "Iteration: 9401 lambda_n: 0.9892766984280877 Loss: 0.019759318536591797\n",
      "Iteration: 9402 lambda_n: 0.983944713198758 Loss: 0.019751100644960554\n",
      "Iteration: 9403 lambda_n: 0.9346450819950929 Loss: 0.01974292704462401\n",
      "Iteration: 9404 lambda_n: 0.9019595995591413 Loss: 0.019735162974236935\n",
      "Iteration: 9405 lambda_n: 0.9431350538064525 Loss: 0.019727670420930446\n",
      "Iteration: 9406 lambda_n: 0.9609496821119763 Loss: 0.01971983582413649\n",
      "Iteration: 9407 lambda_n: 0.9829142695477546 Loss: 0.019711853241673745\n",
      "Iteration: 9408 lambda_n: 0.9877869137416712 Loss: 0.019703688199999195\n",
      "Iteration: 9409 lambda_n: 0.9488407863250138 Loss: 0.019695482681427998\n",
      "Iteration: 9410 lambda_n: 0.9417763390891631 Loss: 0.01968760068726351\n",
      "Iteration: 9411 lambda_n: 0.8873738680530939 Loss: 0.019679777377318707\n",
      "Iteration: 9412 lambda_n: 1.0140880790007691 Loss: 0.019672405987242847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9413 lambda_n: 0.993311134411076 Loss: 0.019663981985791393\n",
      "Iteration: 9414 lambda_n: 0.9844046456357743 Loss: 0.019655730577898208\n",
      "Iteration: 9415 lambda_n: 0.9200247015279606 Loss: 0.019647553156012795\n",
      "Iteration: 9416 lambda_n: 0.9938400672126121 Loss: 0.01963991053657047\n",
      "Iteration: 9417 lambda_n: 0.9310363697643039 Loss: 0.019631654734987528\n",
      "Iteration: 9418 lambda_n: 0.8908673907476138 Loss: 0.01962392060974439\n",
      "Iteration: 9419 lambda_n: 0.998573430141911 Loss: 0.019616520180835758\n",
      "Iteration: 9420 lambda_n: 1.004483519158099 Loss: 0.019608225055126872\n",
      "Iteration: 9421 lambda_n: 0.9449317073702566 Loss: 0.01959988083702151\n",
      "Iteration: 9422 lambda_n: 1.0236181370373563 Loss: 0.01959203134092571\n",
      "Iteration: 9423 lambda_n: 1.0155179779943897 Loss: 0.01958352819063144\n",
      "Iteration: 9424 lambda_n: 0.9643607039847732 Loss: 0.019575092294504253\n",
      "Iteration: 9425 lambda_n: 0.9724743993766343 Loss: 0.019567081368238808\n",
      "Iteration: 9426 lambda_n: 0.875888263727415 Loss: 0.019559003045703405\n",
      "Iteration: 9427 lambda_n: 0.9838214358691559 Loss: 0.019551727064249895\n",
      "Iteration: 9428 lambda_n: 1.001547875507254 Loss: 0.019543554486081288\n",
      "Iteration: 9429 lambda_n: 1.0022242544785607 Loss: 0.0195352346558654\n",
      "Iteration: 9430 lambda_n: 0.9351384835638064 Loss: 0.019526909207582103\n",
      "Iteration: 9431 lambda_n: 0.9228945838485887 Loss: 0.019519141039188188\n",
      "Iteration: 9432 lambda_n: 0.9419753935326975 Loss: 0.019511474580662953\n",
      "Iteration: 9433 lambda_n: 1.0088964939206408 Loss: 0.019503649618482423\n",
      "Iteration: 9434 lambda_n: 0.8982236233708037 Loss: 0.019495268744708926\n",
      "Iteration: 9435 lambda_n: 1.02287826126616 Loss: 0.01948780722725825\n",
      "Iteration: 9436 lambda_n: 1.0160108969679253 Loss: 0.019479310207367758\n",
      "Iteration: 9437 lambda_n: 1.0092767850271986 Loss: 0.019470870234468616\n",
      "Iteration: 9438 lambda_n: 0.8824274094131163 Loss: 0.01946248620163652\n",
      "Iteration: 9439 lambda_n: 0.9716363961348624 Loss: 0.019455155902866117\n",
      "Iteration: 9440 lambda_n: 0.9603538909219171 Loss: 0.01944708454764455\n",
      "Iteration: 9441 lambda_n: 0.9699111238752299 Loss: 0.019439106915867685\n",
      "Iteration: 9442 lambda_n: 0.8808022135286565 Loss: 0.019431049892441116\n",
      "Iteration: 9443 lambda_n: 0.9867783103884766 Loss: 0.019423733094141284\n",
      "Iteration: 9444 lambda_n: 1.0120398301125901 Loss: 0.019415535955506994\n",
      "Iteration: 9445 lambda_n: 0.961007068036517 Loss: 0.01940712897017952\n",
      "Iteration: 9446 lambda_n: 0.9041739985906009 Loss: 0.019399145912532115\n",
      "Iteration: 9447 lambda_n: 0.9298621800998925 Loss: 0.0193916349655484\n",
      "Iteration: 9448 lambda_n: 0.910471295594447 Loss: 0.01938391062761014\n",
      "Iteration: 9449 lambda_n: 0.9415036583973251 Loss: 0.01937634736920409\n",
      "Iteration: 9450 lambda_n: 0.918691895274524 Loss: 0.019368526350186745\n",
      "Iteration: 9451 lambda_n: 0.9215460999753561 Loss: 0.019360894817375976\n",
      "Iteration: 9452 lambda_n: 1.0057743478568841 Loss: 0.01935323956912277\n",
      "Iteration: 9453 lambda_n: 0.9927442315413779 Loss: 0.019344884601536706\n",
      "Iteration: 9454 lambda_n: 0.9082988098857482 Loss: 0.019336637921299287\n",
      "Iteration: 9455 lambda_n: 0.9094303508880914 Loss: 0.019329092695149614\n",
      "Iteration: 9456 lambda_n: 0.9142573956002679 Loss: 0.019321538098723497\n",
      "Iteration: 9457 lambda_n: 0.976662172594327 Loss: 0.01931394339845961\n",
      "Iteration: 9458 lambda_n: 0.9507464764262279 Loss: 0.019305830274288543\n",
      "Iteration: 9459 lambda_n: 0.8962196479770802 Loss: 0.019297932439466113\n",
      "Iteration: 9460 lambda_n: 0.9400786271970669 Loss: 0.019290487562595885\n",
      "Iteration: 9461 lambda_n: 0.9486907674208427 Loss: 0.01928267835293925\n",
      "Iteration: 9462 lambda_n: 1.0176267588431671 Loss: 0.019274797604332705\n",
      "Iteration: 9463 lambda_n: 0.930120412805387 Loss: 0.01926634420755582\n",
      "Iteration: 9464 lambda_n: 0.9347575864521892 Loss: 0.01925861772425902\n",
      "Iteration: 9465 lambda_n: 0.9966064804025001 Loss: 0.019250852720469343\n",
      "Iteration: 9466 lambda_n: 0.9805485098539036 Loss: 0.01924257393996708\n",
      "Iteration: 9467 lambda_n: 0.8974304088154755 Loss: 0.019234428552683036\n",
      "Iteration: 9468 lambda_n: 0.931207673296047 Loss: 0.019226973625047668\n",
      "Iteration: 9469 lambda_n: 1.013307964235582 Loss: 0.01921923811070913\n",
      "Iteration: 9470 lambda_n: 1.0094978265351067 Loss: 0.019210820591718306\n",
      "Iteration: 9471 lambda_n: 0.9892928913347296 Loss: 0.019202434723431906\n",
      "Iteration: 9472 lambda_n: 0.9993035077509519 Loss: 0.0191942166969394\n",
      "Iteration: 9473 lambda_n: 0.9880698211659799 Loss: 0.019185915512556967\n",
      "Iteration: 9474 lambda_n: 0.9396002947867192 Loss: 0.01917770764607668\n",
      "Iteration: 9475 lambda_n: 0.9761546498974494 Loss: 0.019169902414508763\n",
      "Iteration: 9476 lambda_n: 0.9086521412145083 Loss: 0.019161793527013988\n",
      "Iteration: 9477 lambda_n: 0.9583614000295226 Loss: 0.019154245380849932\n",
      "Iteration: 9478 lambda_n: 0.9709934632263981 Loss: 0.019146284301371785\n",
      "Iteration: 9479 lambda_n: 0.9733961712907402 Loss: 0.019138218287736843\n",
      "Iteration: 9480 lambda_n: 0.8787730025196039 Loss: 0.019130132314893734\n",
      "Iteration: 9481 lambda_n: 0.9082696874328345 Loss: 0.01912283237389416\n",
      "Iteration: 9482 lambda_n: 0.9948099802521578 Loss: 0.01911528740483275\n",
      "Iteration: 9483 lambda_n: 0.9696545827050822 Loss: 0.01910702354817318\n",
      "Iteration: 9484 lambda_n: 0.9265835134205966 Loss: 0.01909896865666645\n",
      "Iteration: 9485 lambda_n: 1.0193569157516094 Loss: 0.01909127155525833\n",
      "Iteration: 9486 lambda_n: 0.9420934238150206 Loss: 0.019082803788006964\n",
      "Iteration: 9487 lambda_n: 0.9069596114912513 Loss: 0.01907497784628141\n",
      "Iteration: 9488 lambda_n: 1.0229756870125637 Loss: 0.019067443760097523\n",
      "Iteration: 9489 lambda_n: 0.9698721269856605 Loss: 0.01905894593189174\n",
      "Iteration: 9490 lambda_n: 0.8808581893596137 Loss: 0.019050889233382755\n",
      "Iteration: 9491 lambda_n: 0.8908280727168044 Loss: 0.019043571970989676\n",
      "Iteration: 9492 lambda_n: 1.0050837087129552 Loss: 0.01903617188909698\n",
      "Iteration: 9493 lambda_n: 0.9076367315631358 Loss: 0.01902782268911281\n",
      "Iteration: 9494 lambda_n: 1.018093945857907 Loss: 0.01902028297824622\n",
      "Iteration: 9495 lambda_n: 0.9599891677472471 Loss: 0.0190118257026715\n",
      "Iteration: 9496 lambda_n: 0.9081503112250067 Loss: 0.019003851101750138\n",
      "Iteration: 9497 lambda_n: 0.9323490238160553 Loss: 0.018996307124655408\n",
      "Iteration: 9498 lambda_n: 0.9837608010942083 Loss: 0.01898856212960894\n",
      "Iteration: 9499 lambda_n: 0.981948926812815 Loss: 0.01898039005851306\n",
      "Iteration: 9500 lambda_n: 0.9090168303671361 Loss: 0.01897223303862888\n",
      "Iteration: 9501 lambda_n: 1.015668983042638 Loss: 0.01896468186347942\n",
      "Iteration: 9502 lambda_n: 1.0087897554224343 Loss: 0.018956244732176688\n",
      "Iteration: 9503 lambda_n: 0.9341872862266448 Loss: 0.01894786474645156\n",
      "Iteration: 9504 lambda_n: 0.9772477098645279 Loss: 0.018940104481172483\n",
      "Iteration: 9505 lambda_n: 0.8828029887140145 Loss: 0.018931986514272317\n",
      "Iteration: 9506 lambda_n: 0.911573911999515 Loss: 0.018924653119621132\n",
      "Iteration: 9507 lambda_n: 1.0096852421751676 Loss: 0.018917080717639714\n",
      "Iteration: 9508 lambda_n: 0.8755966070777967 Loss: 0.018908693276643383\n",
      "Iteration: 9509 lambda_n: 0.8908914393537313 Loss: 0.018901419713837814\n",
      "Iteration: 9510 lambda_n: 0.9069268860871543 Loss: 0.018894019100327248\n",
      "Iteration: 9511 lambda_n: 0.9747199744886047 Loss: 0.018886485282908136\n",
      "Iteration: 9512 lambda_n: 0.9110672246619976 Loss: 0.018878388311652446\n",
      "Iteration: 9513 lambda_n: 0.9903232219000792 Loss: 0.018870820102976596\n",
      "Iteration: 9514 lambda_n: 0.8811345686318518 Loss: 0.018862593517670827\n",
      "Iteration: 9515 lambda_n: 0.9508595084166892 Loss: 0.01885527395960223\n",
      "Iteration: 9516 lambda_n: 0.9789385482694897 Loss: 0.01884737519879219\n",
      "Iteration: 9517 lambda_n: 0.9978886525733002 Loss: 0.018839243211702206\n",
      "Iteration: 9518 lambda_n: 1.020412555914128 Loss: 0.01883095379642062\n",
      "Iteration: 9519 lambda_n: 1.0097545791275988 Loss: 0.018822477269237695\n",
      "Iteration: 9520 lambda_n: 0.9958893172474615 Loss: 0.018814089273308243\n",
      "Iteration: 9521 lambda_n: 0.9111321743552938 Loss: 0.018805816453304702\n",
      "Iteration: 9522 lambda_n: 1.0094065003134636 Loss: 0.018798247706946217\n",
      "Iteration: 9523 lambda_n: 0.9762443597051341 Loss: 0.018789862598213924\n",
      "Iteration: 9524 lambda_n: 0.9881799193933538 Loss: 0.018781752965998932\n",
      "Iteration: 9525 lambda_n: 1.0266372846792926 Loss: 0.01877354418530748\n",
      "Iteration: 9526 lambda_n: 1.0180193738123324 Loss: 0.01876501594041267\n",
      "Iteration: 9527 lambda_n: 1.0046551496098641 Loss: 0.018756559284274457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9528 lambda_n: 0.8821425967896117 Loss: 0.018748213644406462\n",
      "Iteration: 9529 lambda_n: 0.8882374109667437 Loss: 0.018740885712665623\n",
      "Iteration: 9530 lambda_n: 1.0167620343933537 Loss: 0.018733507151549585\n",
      "Iteration: 9531 lambda_n: 0.9443689945230141 Loss: 0.018725060940371695\n",
      "Iteration: 9532 lambda_n: 0.9739913470520044 Loss: 0.018717216096033083\n",
      "Iteration: 9533 lambda_n: 0.8962961259295597 Loss: 0.018709125179779348\n",
      "Iteration: 9534 lambda_n: 0.9740872788914086 Loss: 0.01870167967539688\n",
      "Iteration: 9535 lambda_n: 0.9523561345354777 Loss: 0.018693587962346447\n",
      "Iteration: 9536 lambda_n: 1.0219626910442707 Loss: 0.018685676769291226\n",
      "Iteration: 9537 lambda_n: 0.9026627704174977 Loss: 0.01867718735676446\n",
      "Iteration: 9538 lambda_n: 0.986630711744589 Loss: 0.018669688965035514\n",
      "Iteration: 9539 lambda_n: 0.9427799458265058 Loss: 0.018661493054252732\n",
      "Iteration: 9540 lambda_n: 0.8962189064745846 Loss: 0.018653661410461975\n",
      "Iteration: 9541 lambda_n: 0.9494737057418606 Loss: 0.018646216547814006\n",
      "Iteration: 9542 lambda_n: 0.9472144530222928 Loss: 0.018638329299233296\n",
      "Iteration: 9543 lambda_n: 0.9732721611890321 Loss: 0.018630460818227116\n",
      "Iteration: 9544 lambda_n: 0.9525132849250653 Loss: 0.018622375876682182\n",
      "Iteration: 9545 lambda_n: 0.9847206187182445 Loss: 0.018614463378507584\n",
      "Iteration: 9546 lambda_n: 0.9476496062297428 Loss: 0.0186062833350468\n",
      "Iteration: 9547 lambda_n: 0.9921151385909631 Loss: 0.018598411239360606\n",
      "Iteration: 9548 lambda_n: 1.0235705466576186 Loss: 0.018590169769916905\n",
      "Iteration: 9549 lambda_n: 0.9405348952494448 Loss: 0.018581667001414844\n",
      "Iteration: 9550 lambda_n: 0.8912205405014225 Loss: 0.018573854007500203\n",
      "Iteration: 9551 lambda_n: 0.9636422482474222 Loss: 0.01856645066641161\n",
      "Iteration: 9552 lambda_n: 0.905746718288278 Loss: 0.018558445720495396\n",
      "Iteration: 9553 lambda_n: 1.0068052215488394 Loss: 0.018550921710956212\n",
      "Iteration: 9554 lambda_n: 0.8906508162795339 Loss: 0.018542558211634683\n",
      "Iteration: 9555 lambda_n: 0.9508517935169545 Loss: 0.018535159603332074\n",
      "Iteration: 9556 lambda_n: 0.9703976638366378 Loss: 0.018527260907433157\n",
      "Iteration: 9557 lambda_n: 0.962151381127873 Loss: 0.018519199844614895\n",
      "Iteration: 9558 lambda_n: 1.0194925959887426 Loss: 0.018511207283513062\n",
      "Iteration: 9559 lambda_n: 0.9151716142405322 Loss: 0.01850273836383766\n",
      "Iteration: 9560 lambda_n: 0.8970092898226525 Loss: 0.01849513604873249\n",
      "Iteration: 9561 lambda_n: 0.9968834250939984 Loss: 0.018487684635905676\n",
      "Iteration: 9562 lambda_n: 1.0088497704958717 Loss: 0.01847940356758332\n",
      "Iteration: 9563 lambda_n: 1.0059618895533984 Loss: 0.01847102309099682\n",
      "Iteration: 9564 lambda_n: 0.8898403284139883 Loss: 0.018462666601247096\n",
      "Iteration: 9565 lambda_n: 0.9525545916366358 Loss: 0.01845527472777289\n",
      "Iteration: 9566 lambda_n: 0.9599513893457373 Loss: 0.01844736188825267\n",
      "Iteration: 9567 lambda_n: 0.9198177965680866 Loss: 0.01843938760325305\n",
      "Iteration: 9568 lambda_n: 0.8914894822310835 Loss: 0.018431746706413808\n",
      "Iteration: 9569 lambda_n: 0.8884454692812761 Loss: 0.01842434113182623\n",
      "Iteration: 9570 lambda_n: 0.8927982039679014 Loss: 0.01841696084368883\n",
      "Iteration: 9571 lambda_n: 0.9451902255679324 Loss: 0.01840954439749761\n",
      "Iteration: 9572 lambda_n: 1.0101786700862607 Loss: 0.01840169273245542\n",
      "Iteration: 9573 lambda_n: 0.9452491110978332 Loss: 0.018393301210497863\n",
      "Iteration: 9574 lambda_n: 0.8995797101577127 Loss: 0.018385449056342235\n",
      "Iteration: 9575 lambda_n: 0.9200023163119142 Loss: 0.018377976276471728\n",
      "Iteration: 9576 lambda_n: 0.9563437583836552 Loss: 0.018370333846698443\n",
      "Iteration: 9577 lambda_n: 0.9783721991785888 Loss: 0.018362389529765863\n",
      "Iteration: 9578 lambda_n: 1.0006872490128909 Loss: 0.018354262223319623\n",
      "Iteration: 9579 lambda_n: 0.9911855706454998 Loss: 0.018345949546509453\n",
      "Iteration: 9580 lambda_n: 1.0010559376328976 Loss: 0.018337715799873613\n",
      "Iteration: 9581 lambda_n: 0.9997803239484542 Loss: 0.018329400060453725\n",
      "Iteration: 9582 lambda_n: 0.9398022450235375 Loss: 0.01832109491755164\n",
      "Iteration: 9583 lambda_n: 0.9227825469098506 Loss: 0.018313288010648687\n",
      "Iteration: 9584 lambda_n: 0.9659724311684261 Loss: 0.018305622485858087\n",
      "Iteration: 9585 lambda_n: 0.9210996561387296 Loss: 0.018297598184125876\n",
      "Iteration: 9586 lambda_n: 0.9514279197831985 Loss: 0.01828994663911316\n",
      "Iteration: 9587 lambda_n: 0.9446511500770354 Loss: 0.018282043158226062\n",
      "Iteration: 9588 lambda_n: 0.889695357101713 Loss: 0.018274195971774636\n",
      "Iteration: 9589 lambda_n: 0.9121722691677526 Loss: 0.01826680530133884\n",
      "Iteration: 9590 lambda_n: 0.9801203445306959 Loss: 0.018259227915951186\n",
      "Iteration: 9591 lambda_n: 0.932272238818999 Loss: 0.01825108608814025\n",
      "Iteration: 9592 lambda_n: 0.9507999339011173 Loss: 0.018243341733015228\n",
      "Iteration: 9593 lambda_n: 1.0137036916385422 Loss: 0.018235443468959292\n",
      "Iteration: 9594 lambda_n: 0.9504431090676715 Loss: 0.0182270226654703\n",
      "Iteration: 9595 lambda_n: 0.9712921430940201 Loss: 0.018219127365606314\n",
      "Iteration: 9596 lambda_n: 0.9942308797191587 Loss: 0.018211058873527437\n",
      "Iteration: 9597 lambda_n: 0.9298556559888523 Loss: 0.018202799830144497\n",
      "Iteration: 9598 lambda_n: 0.9556476978622582 Loss: 0.018195075549666966\n",
      "Iteration: 9599 lambda_n: 0.8854180276255688 Loss: 0.01818713701557115\n",
      "Iteration: 9600 lambda_n: 0.880986777978843 Loss: 0.018179781877069503\n",
      "Iteration: 9601 lambda_n: 0.9418826866910005 Loss: 0.018172463548838072\n",
      "Iteration: 9602 lambda_n: 0.9126209066279984 Loss: 0.018164639360320322\n",
      "Iteration: 9603 lambda_n: 0.9814313831701342 Loss: 0.018157058248474696\n",
      "Iteration: 9604 lambda_n: 1.0241863640520115 Loss: 0.01814890553029406\n",
      "Iteration: 9605 lambda_n: 0.8987823489661951 Loss: 0.01814039764792805\n",
      "Iteration: 9606 lambda_n: 0.9171115633852447 Loss: 0.018132931492609267\n",
      "Iteration: 9607 lambda_n: 1.026787140149038 Loss: 0.01812531307713772\n",
      "Iteration: 9608 lambda_n: 0.9471221071276157 Loss: 0.018116783590303838\n",
      "Iteration: 9609 lambda_n: 0.9701229374272605 Loss: 0.018108915878295974\n",
      "Iteration: 9610 lambda_n: 0.8850796101256049 Loss: 0.0181008570991817\n",
      "Iteration: 9611 lambda_n: 0.9666799580878828 Loss: 0.018093504772197355\n",
      "Iteration: 9612 lambda_n: 0.9480372166478453 Loss: 0.018085474593856104\n",
      "Iteration: 9613 lambda_n: 1.0001032118209348 Loss: 0.018077599280178727\n",
      "Iteration: 9614 lambda_n: 0.9320504473633043 Loss: 0.01806929145603988\n",
      "Iteration: 9615 lambda_n: 0.9405564808717581 Loss: 0.01806154894398248\n",
      "Iteration: 9616 lambda_n: 0.8754734723303219 Loss: 0.01805373577261622\n",
      "Iteration: 9617 lambda_n: 0.8952384562046826 Loss: 0.018046463243660375\n",
      "Iteration: 9618 lambda_n: 0.9356482943143286 Loss: 0.018039026527667958\n",
      "Iteration: 9619 lambda_n: 0.9448960602241246 Loss: 0.018031254128526193\n",
      "Iteration: 9620 lambda_n: 0.9430837869218924 Loss: 0.018023404908530657\n",
      "Iteration: 9621 lambda_n: 0.878097930692421 Loss: 0.018015570743057863\n",
      "Iteration: 9622 lambda_n: 0.9640214249276026 Loss: 0.018008276412945067\n",
      "Iteration: 9623 lambda_n: 0.9890287627773731 Loss: 0.018000268319271297\n",
      "Iteration: 9624 lambda_n: 0.9874908931946714 Loss: 0.017992052490511556\n",
      "Iteration: 9625 lambda_n: 0.893550036945315 Loss: 0.017983849436814765\n",
      "Iteration: 9626 lambda_n: 0.9698839951974835 Loss: 0.01797642674668443\n",
      "Iteration: 9627 lambda_n: 0.8891402380722123 Loss: 0.0179683699529577\n",
      "Iteration: 9628 lambda_n: 0.9695998359767449 Loss: 0.01796098389493275\n",
      "Iteration: 9629 lambda_n: 0.9545194939850394 Loss: 0.01795292946176634\n",
      "Iteration: 9630 lambda_n: 0.8763905922565635 Loss: 0.01794500030052295\n",
      "Iteration: 9631 lambda_n: 0.9213806185099149 Loss: 0.017937720153457577\n",
      "Iteration: 9632 lambda_n: 1.0131909575045595 Loss: 0.017930066275790222\n",
      "Iteration: 9633 lambda_n: 0.9643971819413121 Loss: 0.017921649732771276\n",
      "Iteration: 9634 lambda_n: 0.9967956745013984 Loss: 0.017913638485313178\n",
      "Iteration: 9635 lambda_n: 0.9237641497573836 Loss: 0.017905358117627803\n",
      "Iteration: 9636 lambda_n: 0.9211393419607982 Loss: 0.017897684430127344\n",
      "Iteration: 9637 lambda_n: 1.010672784603596 Loss: 0.01789003255145039\n",
      "Iteration: 9638 lambda_n: 0.8763878707610528 Loss: 0.0178816369240876\n",
      "Iteration: 9639 lambda_n: 1.0108240942672582 Loss: 0.017874356799115206\n",
      "Iteration: 9640 lambda_n: 1.0032035853510428 Loss: 0.01786595991788256\n",
      "Iteration: 9641 lambda_n: 0.9523910505543037 Loss: 0.01785762634061815\n",
      "Iteration: 9642 lambda_n: 0.9518995409092982 Loss: 0.017849714861613727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9643 lambda_n: 0.9067417365679865 Loss: 0.017841807465682716\n",
      "Iteration: 9644 lambda_n: 0.9224073138042483 Loss: 0.01783427519407362\n",
      "Iteration: 9645 lambda_n: 0.8975005968864807 Loss: 0.017826612789060017\n",
      "Iteration: 9646 lambda_n: 0.8943856088809089 Loss: 0.017819157283240412\n",
      "Iteration: 9647 lambda_n: 0.9765904501081923 Loss: 0.017811727653487062\n",
      "Iteration: 9648 lambda_n: 1.02120312717652 Loss: 0.017803615150983138\n",
      "Iteration: 9649 lambda_n: 1.021686008829544 Loss: 0.01779513205251057\n",
      "Iteration: 9650 lambda_n: 0.9328395354242219 Loss: 0.017786644942731984\n",
      "Iteration: 9651 lambda_n: 0.8764305539244551 Loss: 0.017778895877469437\n",
      "Iteration: 9652 lambda_n: 1.0159354464413577 Loss: 0.017771615399627252\n",
      "Iteration: 9653 lambda_n: 0.9323386596731803 Loss: 0.017763176059540788\n",
      "Iteration: 9654 lambda_n: 0.9012911733904638 Loss: 0.01775543115502802\n",
      "Iteration: 9655 lambda_n: 1.0252031942919244 Loss: 0.01774794416089955\n",
      "Iteration: 9656 lambda_n: 0.9110301394999273 Loss: 0.017739427833977403\n",
      "Iteration: 9657 lambda_n: 0.9356171498336645 Loss: 0.0177318599386225\n",
      "Iteration: 9658 lambda_n: 0.9289258052739968 Loss: 0.017724087799852118\n",
      "Iteration: 9659 lambda_n: 1.0259219171368197 Loss: 0.01771637124586282\n",
      "Iteration: 9660 lambda_n: 0.9408216423817385 Loss: 0.017707848948606553\n",
      "Iteration: 9661 lambda_n: 0.910716249764216 Loss: 0.01770003357635844\n",
      "Iteration: 9662 lambda_n: 0.891374923964026 Loss: 0.017692468288567208\n",
      "Iteration: 9663 lambda_n: 1.0002950717679273 Loss: 0.017685063668508458\n",
      "Iteration: 9664 lambda_n: 0.9703794947725118 Loss: 0.017676754252648835\n",
      "Iteration: 9665 lambda_n: 1.02456317825183 Loss: 0.017668693344457774\n",
      "Iteration: 9666 lambda_n: 1.0104718554503653 Loss: 0.01766018233435081\n",
      "Iteration: 9667 lambda_n: 0.9905148639327737 Loss: 0.017651788380393923\n",
      "Iteration: 9668 lambda_n: 1.0206165233699056 Loss: 0.01764356020848868\n",
      "Iteration: 9669 lambda_n: 0.9658878925051806 Loss: 0.017635081983195015\n",
      "Iteration: 9670 lambda_n: 0.9279391284372249 Loss: 0.017627058386727498\n",
      "Iteration: 9671 lambda_n: 1.0199190401129161 Loss: 0.017619350029324028\n",
      "Iteration: 9672 lambda_n: 0.8839692472389166 Loss: 0.01761087759808999\n",
      "Iteration: 9673 lambda_n: 0.9882608923458629 Loss: 0.01760353449698169\n",
      "Iteration: 9674 lambda_n: 0.9634975240161604 Loss: 0.01759532504891476\n",
      "Iteration: 9675 lambda_n: 0.8820347430492136 Loss: 0.01758732130929701\n",
      "Iteration: 9676 lambda_n: 0.9019608775802755 Loss: 0.01757999427812521\n",
      "Iteration: 9677 lambda_n: 0.9743933502036355 Loss: 0.017572501721289806\n",
      "Iteration: 9678 lambda_n: 1.028641263255308 Loss: 0.017564407470517598\n",
      "Iteration: 9679 lambda_n: 1.0115462203258794 Loss: 0.01755586258430173\n",
      "Iteration: 9680 lambda_n: 0.9349599422388165 Loss: 0.017547459706029937\n",
      "Iteration: 9681 lambda_n: 0.9656817527496032 Loss: 0.01753969302726029\n",
      "Iteration: 9682 lambda_n: 0.9703454621804324 Loss: 0.017531671143540515\n",
      "Iteration: 9683 lambda_n: 1.0148278441474325 Loss: 0.017523610518584316\n",
      "Iteration: 9684 lambda_n: 1.0272786092119166 Loss: 0.017515180380109466\n",
      "Iteration: 9685 lambda_n: 0.9856911719404455 Loss: 0.017506646813610673\n",
      "Iteration: 9686 lambda_n: 0.8997226934361036 Loss: 0.017498458712491796\n",
      "Iteration: 9687 lambda_n: 0.9315272689430072 Loss: 0.017490984748460267\n",
      "Iteration: 9688 lambda_n: 0.9810598245457613 Loss: 0.017483246584993086\n",
      "Iteration: 9689 lambda_n: 0.998656025622231 Loss: 0.017475096956402102\n",
      "Iteration: 9690 lambda_n: 0.9586685250493872 Loss: 0.017466801156836204\n",
      "Iteration: 9691 lambda_n: 1.028472626552999 Loss: 0.017458837532025814\n",
      "Iteration: 9692 lambda_n: 0.9865251453249568 Loss: 0.017450294047099954\n",
      "Iteration: 9693 lambda_n: 0.9287788785804671 Loss: 0.017442099018417423\n",
      "Iteration: 9694 lambda_n: 1.0005521974637877 Loss: 0.01743438368591292\n",
      "Iteration: 9695 lambda_n: 0.9137503617899215 Loss: 0.017426072135080763\n",
      "Iteration: 9696 lambda_n: 0.9679046277682061 Loss: 0.01741848164398026\n",
      "Iteration: 9697 lambda_n: 0.9429054466416369 Loss: 0.017410441295388216\n",
      "Iteration: 9698 lambda_n: 1.0199026722301172 Loss: 0.01740260861411649\n",
      "Iteration: 9699 lambda_n: 0.9614372056813983 Loss: 0.01739413631972479\n",
      "Iteration: 9700 lambda_n: 1.0281448171234282 Loss: 0.017386149695869015\n",
      "Iteration: 9701 lambda_n: 0.9272787515487337 Loss: 0.01737760893434935\n",
      "Iteration: 9702 lambda_n: 0.895319909141665 Loss: 0.01736990606359073\n",
      "Iteration: 9703 lambda_n: 0.90994636160081 Loss: 0.017362468673798016\n",
      "Iteration: 9704 lambda_n: 0.9704127711822601 Loss: 0.01735490978262711\n",
      "Iteration: 9705 lambda_n: 0.9551135260118386 Loss: 0.017346848599233185\n",
      "Iteration: 9706 lambda_n: 0.8828860408521725 Loss: 0.017338914506141046\n",
      "Iteration: 9707 lambda_n: 0.9177060228669216 Loss: 0.017331580404151763\n",
      "Iteration: 9708 lambda_n: 0.9429516798949518 Loss: 0.017323957053876536\n",
      "Iteration: 9709 lambda_n: 0.9623941587117277 Loss: 0.017316123988884817\n",
      "Iteration: 9710 lambda_n: 0.9865952009362902 Loss: 0.017308129415967217\n",
      "Iteration: 9711 lambda_n: 0.8777287520863217 Loss: 0.01729993380591462\n",
      "Iteration: 9712 lambda_n: 0.9252563615899193 Loss: 0.017292642545477587\n",
      "Iteration: 9713 lambda_n: 0.9377663576285583 Loss: 0.017284956474967066\n",
      "Iteration: 9714 lambda_n: 0.9720918881719696 Loss: 0.017277166484410994\n",
      "Iteration: 9715 lambda_n: 0.9029379101654199 Loss: 0.01726909135297106\n",
      "Iteration: 9716 lambda_n: 0.9007311880707326 Loss: 0.01726159068110405\n",
      "Iteration: 9717 lambda_n: 0.8853254134519518 Loss: 0.017254108340424654\n",
      "Iteration: 9718 lambda_n: 0.9316443505147188 Loss: 0.01724675397497344\n",
      "Iteration: 9719 lambda_n: 0.9876385845050859 Loss: 0.01723901483985599\n",
      "Iteration: 9720 lambda_n: 0.9102648487274994 Loss: 0.017230810562744222\n",
      "Iteration: 9721 lambda_n: 0.965736929590412 Loss: 0.017223249026418107\n",
      "Iteration: 9722 lambda_n: 0.9556495597059695 Loss: 0.01721522668560675\n",
      "Iteration: 9723 lambda_n: 0.9685981867673347 Loss: 0.017207288140235213\n",
      "Iteration: 9724 lambda_n: 0.9571014123146797 Loss: 0.017199242031133342\n",
      "Iteration: 9725 lambda_n: 0.9051322362915944 Loss: 0.01719129142534218\n",
      "Iteration: 9726 lambda_n: 0.9370551685586012 Loss: 0.017183772525591206\n",
      "Iteration: 9727 lambda_n: 0.9433260932808201 Loss: 0.017175988443262832\n",
      "Iteration: 9728 lambda_n: 0.9991169892365247 Loss: 0.017168152268628722\n",
      "Iteration: 9729 lambda_n: 0.9025144012440002 Loss: 0.017159852641140234\n",
      "Iteration: 9730 lambda_n: 1.0177156907997313 Loss: 0.0171523554877707\n",
      "Iteration: 9731 lambda_n: 0.9575831091578806 Loss: 0.01714390136163394\n",
      "Iteration: 9732 lambda_n: 0.9538418506011609 Loss: 0.01713594675463559\n",
      "Iteration: 9733 lambda_n: 1.0289413728223096 Loss: 0.01712802322616505\n",
      "Iteration: 9734 lambda_n: 0.9394754847230381 Loss: 0.017119475848818032\n",
      "Iteration: 9735 lambda_n: 0.8852139410412148 Loss: 0.017111671661277646\n",
      "Iteration: 9736 lambda_n: 1.0204405875715876 Loss: 0.017104318222370667\n",
      "Iteration: 9737 lambda_n: 0.8883726403473035 Loss: 0.017095841460833986\n",
      "Iteration: 9738 lambda_n: 0.9390972492123822 Loss: 0.017088461782792903\n",
      "Iteration: 9739 lambda_n: 0.9779140482571917 Loss: 0.017080660737369448\n",
      "Iteration: 9740 lambda_n: 1.0284608269427338 Loss: 0.01707253724229524\n",
      "Iteration: 9741 lambda_n: 0.9616986903771254 Loss: 0.01706399385707726\n",
      "Iteration: 9742 lambda_n: 0.8819403244050152 Loss: 0.01705600506243974\n",
      "Iteration: 9743 lambda_n: 0.9318793945066668 Loss: 0.017048678817560958\n",
      "Iteration: 9744 lambda_n: 1.025738790622658 Loss: 0.01704093773074933\n",
      "Iteration: 9745 lambda_n: 0.9904600022097209 Loss: 0.01703241695752898\n",
      "Iteration: 9746 lambda_n: 0.9227615942097637 Loss: 0.017024189243902785\n",
      "Iteration: 9747 lambda_n: 0.8968313469557558 Loss: 0.01701652389841394\n",
      "Iteration: 9748 lambda_n: 0.9753814836933639 Loss: 0.017009073954533917\n",
      "Iteration: 9749 lambda_n: 0.9649553723216824 Loss: 0.01700097149769073\n",
      "Iteration: 9750 lambda_n: 1.0122825766087078 Loss: 0.016992955650192838\n",
      "Iteration: 9751 lambda_n: 0.9185551888842698 Loss: 0.016984546657450562\n",
      "Iteration: 9752 lambda_n: 0.9235871129499162 Loss: 0.016976916254575442\n",
      "Iteration: 9753 lambda_n: 0.9057193538528534 Loss: 0.016969244051731402\n",
      "Iteration: 9754 lambda_n: 0.9577643270261397 Loss: 0.01696172027571053\n",
      "Iteration: 9755 lambda_n: 1.016933402299884 Loss: 0.016953764164122583\n",
      "Iteration: 9756 lambda_n: 0.9854825571553328 Loss: 0.01694531653733034\n",
      "Iteration: 9757 lambda_n: 0.8956521407280688 Loss: 0.016937130171540613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9758 lambda_n: 0.941223395516556 Loss: 0.016929690023605403\n",
      "Iteration: 9759 lambda_n: 0.9887582649982256 Loss: 0.01692187131703988\n",
      "Iteration: 9760 lambda_n: 0.9698026326996834 Loss: 0.016913657740177376\n",
      "Iteration: 9761 lambda_n: 1.0276682901703738 Loss: 0.01690560162705935\n",
      "Iteration: 9762 lambda_n: 0.9493722645055489 Loss: 0.016897064826192874\n",
      "Iteration: 9763 lambda_n: 1.029151245729085 Loss: 0.01688917842742899\n",
      "Iteration: 9764 lambda_n: 0.9052627147065854 Loss: 0.016880629307786145\n",
      "Iteration: 9765 lambda_n: 1.0159972035340363 Loss: 0.016873109325418405\n",
      "Iteration: 9766 lambda_n: 0.9679982053793551 Loss: 0.01686466947597156\n",
      "Iteration: 9767 lambda_n: 1.007347768948936 Loss: 0.016856628352381737\n",
      "Iteration: 9768 lambda_n: 0.8969782117799032 Loss: 0.016848260353530466\n",
      "Iteration: 9769 lambda_n: 0.9020138453792064 Loss: 0.016840809190343472\n",
      "Iteration: 9770 lambda_n: 0.9217595551790436 Loss: 0.016833316196375236\n",
      "Iteration: 9771 lambda_n: 0.9857500834604114 Loss: 0.01682565917561058\n",
      "Iteration: 9772 lambda_n: 0.9962203401544106 Loss: 0.01681747059602511\n",
      "Iteration: 9773 lambda_n: 0.9356654238200979 Loss: 0.01680919503715888\n",
      "Iteration: 9774 lambda_n: 0.9130558951471246 Loss: 0.01680142250341106\n",
      "Iteration: 9775 lambda_n: 0.9295138304211381 Loss: 0.01679383778503699\n",
      "Iteration: 9776 lambda_n: 0.9921882608757177 Loss: 0.016786116350644222\n",
      "Iteration: 9777 lambda_n: 0.9739375387214084 Loss: 0.016777874281862455\n",
      "Iteration: 9778 lambda_n: 0.905560525348074 Loss: 0.016769783820888857\n",
      "Iteration: 9779 lambda_n: 0.9900969287193123 Loss: 0.01676226136499682\n",
      "Iteration: 9780 lambda_n: 0.8776551393810786 Loss: 0.016754036668484018\n",
      "Iteration: 9781 lambda_n: 0.9359078884675088 Loss: 0.016746746021532137\n",
      "Iteration: 9782 lambda_n: 1.0213300099253058 Loss: 0.01673897147130147\n",
      "Iteration: 9783 lambda_n: 0.9325281375175857 Loss: 0.01673048732289651\n",
      "Iteration: 9784 lambda_n: 0.8944804761400822 Loss: 0.01672274084821894\n",
      "Iteration: 9785 lambda_n: 1.0066523153555853 Loss: 0.01671531043401939\n",
      "Iteration: 9786 lambda_n: 0.946818869985972 Loss: 0.016706948212808572\n",
      "Iteration: 9787 lambda_n: 0.8864555813292593 Loss: 0.016699083025728187\n",
      "Iteration: 9788 lambda_n: 1.0062187724179612 Loss: 0.016691719274154665\n",
      "Iteration: 9789 lambda_n: 0.9938701809740954 Loss: 0.016683360654522698\n",
      "Iteration: 9790 lambda_n: 0.8851593148820551 Loss: 0.016675104614203323\n",
      "Iteration: 9791 lambda_n: 0.9714687069084659 Loss: 0.016667751630794266\n",
      "Iteration: 9792 lambda_n: 0.9313389845910998 Loss: 0.016659681678723625\n",
      "Iteration: 9793 lambda_n: 0.9186033855582364 Loss: 0.01665194508270888\n",
      "Iteration: 9794 lambda_n: 1.007941962833956 Loss: 0.016644314280849816\n",
      "Iteration: 9795 lambda_n: 0.9722098261355615 Loss: 0.016635941372748132\n",
      "Iteration: 9796 lambda_n: 1.0091051640280566 Loss: 0.016627865278490316\n",
      "Iteration: 9797 lambda_n: 0.9749634020852872 Loss: 0.016619482690249557\n",
      "Iteration: 9798 lambda_n: 0.961571655558857 Loss: 0.016611383712106832\n",
      "Iteration: 9799 lambda_n: 0.9314620636405591 Loss: 0.01660339597648945\n",
      "Iteration: 9800 lambda_n: 1.0182536014657795 Loss: 0.01659565835884183\n",
      "Iteration: 9801 lambda_n: 0.9299109224886353 Loss: 0.01658719976674078\n",
      "Iteration: 9802 lambda_n: 1.0087759504161553 Loss: 0.01657947503341351\n",
      "Iteration: 9803 lambda_n: 0.9003960181394346 Loss: 0.016571095171289792\n",
      "Iteration: 9804 lambda_n: 0.9006668861467199 Loss: 0.016563615616978906\n",
      "Iteration: 9805 lambda_n: 0.9730718877048663 Loss: 0.016556133812617987\n",
      "Iteration: 9806 lambda_n: 0.9931985767206334 Loss: 0.01654805054283032\n",
      "Iteration: 9807 lambda_n: 0.9417894813229668 Loss: 0.0165398000815203\n",
      "Iteration: 9808 lambda_n: 0.9375491356339939 Loss: 0.01653197667362253\n",
      "Iteration: 9809 lambda_n: 0.8999033378035199 Loss: 0.01652418849019213\n",
      "Iteration: 9810 lambda_n: 0.8773560605740264 Loss: 0.016516713028984793\n",
      "Iteration: 9811 lambda_n: 0.9794556248764522 Loss: 0.016509424867178917\n",
      "Iteration: 9812 lambda_n: 0.9007234099488461 Loss: 0.016501288568435343\n",
      "Iteration: 9813 lambda_n: 0.8928494634535012 Loss: 0.01649380629512069\n",
      "Iteration: 9814 lambda_n: 0.949170163925055 Loss: 0.01648638943042121\n",
      "Iteration: 9815 lambda_n: 0.940572573901098 Loss: 0.016478504711978738\n",
      "Iteration: 9816 lambda_n: 0.978088895794787 Loss: 0.016470691413428194\n",
      "Iteration: 9817 lambda_n: 0.9624174754531828 Loss: 0.016462566468357084\n",
      "Iteration: 9818 lambda_n: 0.9718511063704913 Loss: 0.01645457170519843\n",
      "Iteration: 9819 lambda_n: 0.8994612687679374 Loss: 0.016446498577301155\n",
      "Iteration: 9820 lambda_n: 0.988973663034791 Loss: 0.016439026788908153\n",
      "Iteration: 9821 lambda_n: 0.8892321299482168 Loss: 0.01643081142472183\n",
      "Iteration: 9822 lambda_n: 0.9858575422648076 Loss: 0.016423424609426987\n",
      "Iteration: 9823 lambda_n: 1.0114914623783846 Loss: 0.016415235130774385\n",
      "Iteration: 9824 lambda_n: 0.9310071459543315 Loss: 0.016406832712225313\n",
      "Iteration: 9825 lambda_n: 0.9909353907253363 Loss: 0.016399098873667552\n",
      "Iteration: 9826 lambda_n: 0.9792666886674277 Loss: 0.01639086721365816\n",
      "Iteration: 9827 lambda_n: 0.916462569002621 Loss: 0.016382732485128698\n",
      "Iteration: 9828 lambda_n: 0.9602503395050135 Loss: 0.01637511946790774\n",
      "Iteration: 9829 lambda_n: 0.9614078080711949 Loss: 0.016367142707505312\n",
      "Iteration: 9830 lambda_n: 0.8822128138385686 Loss: 0.016359156332103962\n",
      "Iteration: 9831 lambda_n: 0.9508876490843546 Loss: 0.0163518278263256\n",
      "Iteration: 9832 lambda_n: 0.9136073593271035 Loss: 0.01634392884154078\n",
      "Iteration: 9833 lambda_n: 0.9962211229095915 Loss: 0.0163363395426387\n",
      "Iteration: 9834 lambda_n: 0.9775831754504862 Loss: 0.01632806397463335\n",
      "Iteration: 9835 lambda_n: 0.8970434316504788 Loss: 0.016319943231339756\n",
      "Iteration: 9836 lambda_n: 1.016826777578111 Loss: 0.016312491528431176\n",
      "Iteration: 9837 lambda_n: 0.9061841085600748 Loss: 0.016304044790239062\n",
      "Iteration: 9838 lambda_n: 0.9513572621005179 Loss: 0.016296517156186786\n",
      "Iteration: 9839 lambda_n: 0.9672814788633747 Loss: 0.016288614270651713\n",
      "Iteration: 9840 lambda_n: 0.8840838555915794 Loss: 0.01628057910334934\n",
      "Iteration: 9841 lambda_n: 1.0239965725292348 Loss: 0.016273235055307273\n",
      "Iteration: 9842 lambda_n: 0.9136291465725949 Loss: 0.01626472875815604\n",
      "Iteration: 9843 lambda_n: 0.9102467400222187 Loss: 0.016257139278687348\n",
      "Iteration: 9844 lambda_n: 0.9923904129062813 Loss: 0.01624957789676952\n",
      "Iteration: 9845 lambda_n: 0.9122844847871147 Loss: 0.01624133415081308\n",
      "Iteration: 9846 lambda_n: 0.9768462462333497 Loss: 0.01623375584151732\n",
      "Iteration: 9847 lambda_n: 0.9847568791103857 Loss: 0.016225641220396995\n",
      "Iteration: 9848 lambda_n: 1.003514675417865 Loss: 0.016217460886024913\n",
      "Iteration: 9849 lambda_n: 0.8788853409768759 Loss: 0.01620912473146826\n",
      "Iteration: 9850 lambda_n: 1.0094754981296985 Loss: 0.01620182386763496\n",
      "Iteration: 9851 lambda_n: 0.9013022353343103 Loss: 0.016193438196864238\n",
      "Iteration: 9852 lambda_n: 0.9160908686141771 Loss: 0.016185951116907887\n",
      "Iteration: 9853 lambda_n: 0.9186415766714952 Loss: 0.016178341188433813\n",
      "Iteration: 9854 lambda_n: 0.9232244833736105 Loss: 0.016170710071375435\n",
      "Iteration: 9855 lambda_n: 1.0181000967221783 Loss: 0.016163040884344327\n",
      "Iteration: 9856 lambda_n: 0.9708485385052277 Loss: 0.016154583569623248\n",
      "Iteration: 9857 lambda_n: 0.8946218764822892 Loss: 0.016146518771657487\n",
      "Iteration: 9858 lambda_n: 0.9421679487244728 Loss: 0.016139087185406797\n",
      "Iteration: 9859 lambda_n: 0.9379302721558403 Loss: 0.016131260635978372\n",
      "Iteration: 9860 lambda_n: 0.985729254560218 Loss: 0.016123469288793373\n",
      "Iteration: 9861 lambda_n: 0.8802550435920383 Loss: 0.016115280877524996\n",
      "Iteration: 9862 lambda_n: 0.913975178646617 Loss: 0.01610796863611067\n",
      "Iteration: 9863 lambda_n: 0.8945026203229414 Loss: 0.016100376283000668\n",
      "Iteration: 9864 lambda_n: 0.8974434834135755 Loss: 0.01609294568764763\n",
      "Iteration: 9865 lambda_n: 0.8941846673228142 Loss: 0.01608549066270884\n",
      "Iteration: 9866 lambda_n: 0.9869447361278167 Loss: 0.016078062708656228\n",
      "Iteration: 9867 lambda_n: 0.9322119007948925 Loss: 0.016069864200697236\n",
      "Iteration: 9868 lambda_n: 0.9768358143839112 Loss: 0.016062120356118065\n",
      "Iteration: 9869 lambda_n: 1.0272795284121907 Loss: 0.01605400582263391\n",
      "Iteration: 9870 lambda_n: 0.97540328389468 Loss: 0.016045472255417418\n",
      "Iteration: 9871 lambda_n: 0.900106987697628 Loss: 0.016037369621999066\n",
      "Iteration: 9872 lambda_n: 0.9362720904700483 Loss: 0.016029892471738832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9873 lambda_n: 0.9826696292720619 Loss: 0.016022114899558588\n",
      "Iteration: 9874 lambda_n: 0.9391434062084781 Loss: 0.016013951905055987\n",
      "Iteration: 9875 lambda_n: 0.8867526083968388 Loss: 0.016006150481067915\n",
      "Iteration: 9876 lambda_n: 1.0058560547116153 Loss: 0.015998784265229923\n",
      "Iteration: 9877 lambda_n: 0.9518186208501489 Loss: 0.015990428662225158\n",
      "Iteration: 9878 lambda_n: 1.0057745175453363 Loss: 0.015982521945906672\n",
      "Iteration: 9879 lambda_n: 0.8919205461413878 Loss: 0.015974167020325416\n",
      "Iteration: 9880 lambda_n: 0.891708857740311 Loss: 0.015966757874821144\n",
      "Iteration: 9881 lambda_n: 0.8935586763550694 Loss: 0.015959350487842888\n",
      "Iteration: 9882 lambda_n: 0.8783908985033047 Loss: 0.015951927734541054\n",
      "Iteration: 9883 lambda_n: 0.9897890523752323 Loss: 0.015944630979353107\n",
      "Iteration: 9884 lambda_n: 0.9234044859977665 Loss: 0.01593640884456704\n",
      "Iteration: 9885 lambda_n: 0.9551335951536275 Loss: 0.015928738163542705\n",
      "Iteration: 9886 lambda_n: 1.0139583387825037 Loss: 0.015920803910229438\n",
      "Iteration: 9887 lambda_n: 0.8799257984080757 Loss: 0.015912381002374434\n",
      "Iteration: 9888 lambda_n: 0.886858648446018 Loss: 0.015905071497049384\n",
      "Iteration: 9889 lambda_n: 0.8864996271652084 Loss: 0.0158977044008801\n",
      "Iteration: 9890 lambda_n: 1.0172960019323831 Loss: 0.015890340287124556\n",
      "Iteration: 9891 lambda_n: 0.95724975162135 Loss: 0.015881889653634726\n",
      "Iteration: 9892 lambda_n: 0.9413427964941332 Loss: 0.01587393782177338\n",
      "Iteration: 9893 lambda_n: 0.9212653704853926 Loss: 0.01586611812833845\n",
      "Iteration: 9894 lambda_n: 0.9881312656362232 Loss: 0.015858465217246605\n",
      "Iteration: 9895 lambda_n: 0.9704288811899018 Loss: 0.015850256854139474\n",
      "Iteration: 9896 lambda_n: 0.8891327278484069 Loss: 0.015842195544012224\n",
      "Iteration: 9897 lambda_n: 0.9791247734791231 Loss: 0.015834809557505013\n",
      "Iteration: 9898 lambda_n: 0.9595736541483751 Loss: 0.015826676011074972\n",
      "Iteration: 9899 lambda_n: 0.9331017332008901 Loss: 0.01581870487498074\n",
      "Iteration: 9900 lambda_n: 1.001109613643732 Loss: 0.015810953640013326\n",
      "Iteration: 9901 lambda_n: 0.9646726161029958 Loss: 0.01580263746663025\n",
      "Iteration: 9902 lambda_n: 0.9209276369687728 Loss: 0.015794623973826385\n",
      "Iteration: 9903 lambda_n: 0.9417135639692715 Loss: 0.015786973868676096\n",
      "Iteration: 9904 lambda_n: 0.9347489702487071 Loss: 0.01577915109579451\n",
      "Iteration: 9905 lambda_n: 0.9969581960086835 Loss: 0.01577138617752966\n",
      "Iteration: 9906 lambda_n: 1.0211556543604616 Loss: 0.015763104490033264\n",
      "Iteration: 9907 lambda_n: 0.9841696143328487 Loss: 0.015754621795377125\n",
      "Iteration: 9908 lambda_n: 0.9582311233742552 Loss: 0.01574644634216334\n",
      "Iteration: 9909 lambda_n: 0.9884596725452511 Loss: 0.015738486358888398\n",
      "Iteration: 9910 lambda_n: 1.0027884657387742 Loss: 0.01573027526845124\n",
      "Iteration: 9911 lambda_n: 1.0103169891078507 Loss: 0.015721945149419474\n",
      "Iteration: 9912 lambda_n: 1.0049410760951432 Loss: 0.015713552491332838\n",
      "Iteration: 9913 lambda_n: 0.9870926971364786 Loss: 0.01570520449076803\n",
      "Iteration: 9914 lambda_n: 0.8825603014661754 Loss: 0.01569700475593998\n",
      "Iteration: 9915 lambda_n: 0.9784211157752921 Loss: 0.01568967336708432\n",
      "Iteration: 9916 lambda_n: 1.0183336660816114 Loss: 0.015681545666789462\n",
      "Iteration: 9917 lambda_n: 1.0204965855498283 Loss: 0.015673086414783686\n",
      "Iteration: 9918 lambda_n: 0.9613080417685813 Loss: 0.015664609195557535\n",
      "Iteration: 9919 lambda_n: 1.022608022552154 Loss: 0.015656623652951746\n",
      "Iteration: 9920 lambda_n: 0.8991466448493816 Loss: 0.015648128894219744\n",
      "Iteration: 9921 lambda_n: 0.9125433226610844 Loss: 0.015640659723630674\n",
      "Iteration: 9922 lambda_n: 0.9783508415006696 Loss: 0.015633079267487266\n",
      "Iteration: 9923 lambda_n: 0.929100225843667 Loss: 0.015624952151307445\n",
      "Iteration: 9924 lambda_n: 0.9092377948310908 Loss: 0.01561723415781057\n",
      "Iteration: 9925 lambda_n: 0.8807407141423812 Loss: 0.015609681160672717\n",
      "Iteration: 9926 lambda_n: 0.9860234748594311 Loss: 0.015602364887533847\n",
      "Iteration: 9927 lambda_n: 0.9370643822293915 Loss: 0.01559417403532882\n",
      "Iteration: 9928 lambda_n: 0.8834709904378714 Loss: 0.015586389884129607\n",
      "Iteration: 9929 lambda_n: 0.9660973314488952 Loss: 0.015579050930840981\n",
      "Iteration: 9930 lambda_n: 1.0132104465245946 Loss: 0.015571025604347479\n",
      "Iteration: 9931 lambda_n: 0.9024792109433877 Loss: 0.015562608911408081\n",
      "Iteration: 9932 lambda_n: 0.9317096681460635 Loss: 0.01555511205783538\n",
      "Iteration: 9933 lambda_n: 0.9319222160078074 Loss: 0.015547372388235831\n",
      "Iteration: 9934 lambda_n: 0.9453557523464815 Loss: 0.015539630953057387\n",
      "Iteration: 9935 lambda_n: 1.0185273923122995 Loss: 0.015531777926155206\n",
      "Iteration: 9936 lambda_n: 1.0207819979383568 Loss: 0.015523317065847461\n",
      "Iteration: 9937 lambda_n: 1.0148632008343885 Loss: 0.01551483747669018\n",
      "Iteration: 9938 lambda_n: 0.9794593621542627 Loss: 0.015506407054764096\n",
      "Iteration: 9939 lambda_n: 0.9924545713440169 Loss: 0.015498270730950033\n",
      "Iteration: 9940 lambda_n: 0.9537505984870709 Loss: 0.015490026456584535\n",
      "Iteration: 9941 lambda_n: 0.8939205212193131 Loss: 0.015482103694387244\n",
      "Iteration: 9942 lambda_n: 0.8852420503473222 Loss: 0.015474677937924897\n",
      "Iteration: 9943 lambda_n: 0.9623736405341523 Loss: 0.015467324273161815\n",
      "Iteration: 9944 lambda_n: 1.0043503571427586 Loss: 0.015459329879895265\n",
      "Iteration: 9945 lambda_n: 0.91655417515858 Loss: 0.015450986788041384\n",
      "Iteration: 9946 lambda_n: 0.9626042219126785 Loss: 0.015443373015048894\n",
      "Iteration: 9947 lambda_n: 0.9067085434213613 Loss: 0.015435376706504661\n",
      "Iteration: 9948 lambda_n: 0.9710625036248367 Loss: 0.015427844720810787\n",
      "Iteration: 9949 lambda_n: 0.9695819166348215 Loss: 0.0154197781498128\n",
      "Iteration: 9950 lambda_n: 0.9317871121390481 Loss: 0.015411723878033527\n",
      "Iteration: 9951 lambda_n: 0.9348706356681695 Loss: 0.015403983565981778\n",
      "Iteration: 9952 lambda_n: 0.9578592137257299 Loss: 0.015396217639291922\n",
      "Iteration: 9953 lambda_n: 1.0056212875112427 Loss: 0.01538826074760809\n",
      "Iteration: 9954 lambda_n: 0.9916323509624514 Loss: 0.015379907098665798\n",
      "Iteration: 9955 lambda_n: 0.9178052111316622 Loss: 0.015371669655218963\n",
      "Iteration: 9956 lambda_n: 1.00518181941829 Loss: 0.015364045490409747\n",
      "Iteration: 9957 lambda_n: 0.9648258491509489 Loss: 0.015355695492270194\n",
      "Iteration: 9958 lambda_n: 0.9522781672332006 Loss: 0.015347680729331967\n",
      "Iteration: 9959 lambda_n: 0.9438471176254063 Loss: 0.015339770199447775\n",
      "Iteration: 9960 lambda_n: 0.9158680978193455 Loss: 0.01533192970594532\n",
      "Iteration: 9961 lambda_n: 0.9585167568560213 Loss: 0.01532432163288855\n",
      "Iteration: 9962 lambda_n: 0.9244547968517909 Loss: 0.015316359279484387\n",
      "Iteration: 9963 lambda_n: 0.9877553668966634 Loss: 0.015308679877220788\n",
      "Iteration: 9964 lambda_n: 0.934487943668269 Loss: 0.015300474640168198\n",
      "Iteration: 9965 lambda_n: 0.9383155461009568 Loss: 0.015292711893127362\n",
      "Iteration: 9966 lambda_n: 0.8846080961391795 Loss: 0.015284917350423652\n",
      "Iteration: 9967 lambda_n: 0.9150618878553355 Loss: 0.015277568953002786\n",
      "Iteration: 9968 lambda_n: 0.9109316814184545 Loss: 0.015269967577427561\n",
      "Iteration: 9969 lambda_n: 0.8948357286057507 Loss: 0.015262400511327696\n",
      "Iteration: 9970 lambda_n: 0.9795884064551023 Loss: 0.015254967153587071\n",
      "Iteration: 9971 lambda_n: 0.9892825674867685 Loss: 0.015246829759443711\n",
      "Iteration: 9972 lambda_n: 0.9424972066558689 Loss: 0.015238611836422509\n",
      "Iteration: 9973 lambda_n: 0.9153304557559837 Loss: 0.015230782557210106\n",
      "Iteration: 9974 lambda_n: 0.9993026446725857 Loss: 0.015223178950946294\n",
      "Iteration: 9975 lambda_n: 1.0288809888221124 Loss: 0.015214877791784752\n",
      "Iteration: 9976 lambda_n: 0.9104851493515492 Loss: 0.015206330926795858\n",
      "Iteration: 9977 lambda_n: 0.9266561230178121 Loss: 0.015198767570412984\n",
      "Iteration: 9978 lambda_n: 0.8964176431690785 Loss: 0.01519106988257761\n",
      "Iteration: 9979 lambda_n: 0.9368004062594726 Loss: 0.015183623384386426\n",
      "Iteration: 9980 lambda_n: 0.912400721235184 Loss: 0.015175841428576234\n",
      "Iteration: 9981 lambda_n: 0.974514591319127 Loss: 0.015168262159821588\n",
      "Iteration: 9982 lambda_n: 0.9486172812893027 Loss: 0.015160166914198877\n",
      "Iteration: 9983 lambda_n: 0.9496585998368526 Loss: 0.015152286796331743\n",
      "Iteration: 9984 lambda_n: 1.0057668174518917 Loss: 0.015144398028333738\n",
      "Iteration: 9985 lambda_n: 0.976510957501324 Loss: 0.015136043172145983\n",
      "Iteration: 9986 lambda_n: 0.9700685243946102 Loss: 0.015127931343024306\n",
      "Iteration: 9987 lambda_n: 0.9772410273070617 Loss: 0.015119873030936353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9988 lambda_n: 1.016513306597131 Loss: 0.01511175513727043\n",
      "Iteration: 9989 lambda_n: 0.9691491060702523 Loss: 0.015103311010750057\n",
      "Iteration: 9990 lambda_n: 0.8908094737402832 Loss: 0.015095260336392174\n",
      "Iteration: 9991 lambda_n: 0.9591352958037874 Loss: 0.015087860425587614\n",
      "Iteration: 9992 lambda_n: 0.8834548548488964 Loss: 0.015079892935566877\n",
      "Iteration: 9993 lambda_n: 0.9130907009778597 Loss: 0.015072554119317528\n",
      "Iteration: 9994 lambda_n: 0.9820577530385227 Loss: 0.015064969119592514\n",
      "Iteration: 9995 lambda_n: 0.9415327798376738 Loss: 0.015056811213998883\n",
      "Iteration: 9996 lambda_n: 0.943776710539524 Loss: 0.015048989947423387\n",
      "Iteration: 9997 lambda_n: 1.000261872789275 Loss: 0.015041150040677971\n",
      "Iteration: 9998 lambda_n: 0.9139306894823265 Loss: 0.015032840914523338\n",
      "Iteration: 9999 lambda_n: 0.9402095185925612 Loss: 0.015025248937311657\n",
      "Iteration: 10000 lambda_n: 0.8886600175206348 Loss: 0.015017438663212644\n",
      "beta:  0.0016802749699833185\n",
      "gamma: 0.0031709202631556546\n"
     ]
    }
   ],
   "source": [
    "# BA    \n",
    "ba = time.time()\n",
    "x_BA_list, z_BA_list, dual_BA_list, iterations_BA   = BA.Briceno_Arias(N, M, frobenius_norm, Grad_Phi_NA, Sigma, D, (x1,x2,x3),gamma=1e-3, lambdan=1e-3)\n",
    "fin = time.time()\n",
    "\n",
    "Times[\"BA\"] = fin - ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05a2ff12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:\n",
      " 10000\n",
      "Primal: (x1,x2,x3)\n",
      " ((array([[1188.1, 1188.1, 1188.1, 1188.1, 1188.1, 1188.1],\n",
      "       [108.0, 108.0, 108.0, 108.0, 108.0, 108.0],\n",
      "       [25.5, 25.5, 25.5, 25.5, 25.5, 25.5]]), array([[186.6, 704.0, 833.3, 1188.1, 714.3, 11.4],\n",
      "       [8.9, 30.9, 45.5, 42.9, 43.5, 11.4],\n",
      "       [4.6, 15.1, 25.5, 19.0, 25.5, 11.4]]), array([[0.0, 0.0, 95.7, 0.0, 716.7, 3965.9]])), 'infactible')\n",
      "Primal: (z1,z2,z3)\n",
      " (array([[1188.0, 1187.9, 1188.0, 1188.2, 1188.0, 1188.0],\n",
      "       [108.0, 108.0, 108.0, 108.0, 108.0, 108.0],\n",
      "       [25.1, 22.7, 25.5, 25.0, 26.9, 25.0]]), array([[186.6, 704.0, 833.3, 1188.1, 714.3, 11.4],\n",
      "       [8.9, 30.9, 45.5, 42.9, 43.5, 11.4],\n",
      "       [4.6, 15.1, 25.5, 19.0, 25.5, 11.4]]), array([[0.0, 0.0, 95.7, 0.0, 716.7, 3965.9]]))\n",
      "Dual: (Capacity, Equilibrium)\n",
      " (array([[0.0, 0.0, 0.0, 53.2, 0.0, 0.0],\n",
      "       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
      "       [0.0, 0.0, 49.0, 0.0, 559.3, 0.0]]), array([[1865.7, 6336.1, 10000.0, 8369.6, 10000.0, 10000.0]]))\n"
     ]
    }
   ],
   "source": [
    "iter_ = -2\n",
    "print(\"Iterations:\\n\",iterations_BA)\n",
    "print(\"Primal: (x1,x2,x3)\\n\", x_BA_list[iter_])\n",
    "print(\"Primal: (z1,z2,z3)\\n\", z_BA_list[iter_])\n",
    "print(\"Dual: (Capacity, Equilibrium)\\n\",dual_BA_list[iter_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9a848c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r: 1\n",
      "Iteration: 1 Loss: 2.951141367240721\n",
      "Iteration: 2 Loss: 2.4144878463937385\n",
      "Iteration: 3 Loss: 2.0226249400148935\n",
      "Iteration: 4 Loss: 1.7288328119535512\n",
      "Iteration: 5 Loss: 1.5048955490495726\n",
      "Iteration: 6 Loss: 1.3315893156949994\n",
      "Iteration: 7 Loss: 1.195523415098084\n",
      "Iteration: 8 Loss: 1.0872509336471832\n",
      "Iteration: 9 Loss: 1.0000231999127684\n",
      "Iteration: 10 Loss: 0.9289544828016859\n",
      "Iteration: 11 Loss: 0.8704582576637436\n",
      "Iteration: 12 Loss: 0.821841813261459\n",
      "Iteration: 13 Loss: 0.7809942011582729\n",
      "Iteration: 14 Loss: 0.7464205700598974\n",
      "Iteration: 15 Loss: 0.7169655923990533\n",
      "Iteration: 16 Loss: 0.6917179454117967\n",
      "Iteration: 17 Loss: 0.6699491848847713\n",
      "Iteration: 18 Loss: 0.6510708845524743\n",
      "Iteration: 19 Loss: 0.6346037373868514\n",
      "Iteration: 20 Loss: 0.6201547900732642\n",
      "Iteration: 21 Loss: 0.6074002946177856\n",
      "Iteration: 22 Loss: 0.5960725130497345\n",
      "Iteration: 23 Loss: 0.5859493696673027\n",
      "Iteration: 24 Loss: 0.5768462088723516\n",
      "Iteration: 25 Loss: 0.5686091518648382\n",
      "Iteration: 26 Loss: 0.5611096972570248\n",
      "Iteration: 27 Loss: 0.5542403090219851\n",
      "Iteration: 28 Loss: 0.5479107998140852\n",
      "Iteration: 29 Loss: 0.5420453612846919\n",
      "Iteration: 30 Loss: 0.5365801235618731\n",
      "Iteration: 31 Loss: 0.5314611484849084\n",
      "Iteration: 32 Loss: 0.5266427784290841\n",
      "Iteration: 33 Loss: 0.5220862763472905\n",
      "Iteration: 34 Loss: 0.5176998711076639\n",
      "Iteration: 35 Loss: 0.5134616916149936\n",
      "Iteration: 36 Loss: 0.5093704267645852\n",
      "Iteration: 37 Loss: 0.5054254310059363\n",
      "Iteration: 38 Loss: 0.5016223370453636\n",
      "Iteration: 39 Loss: 0.4979542847235881\n",
      "Iteration: 40 Loss: 0.4944164399404148\n",
      "Iteration: 41 Loss: 0.4909997709785119\n",
      "Iteration: 42 Loss: 0.48769371383680243\n",
      "Iteration: 43 Loss: 0.4844873698862678\n",
      "Iteration: 44 Loss: 0.4813701900112269\n",
      "Iteration: 45 Loss: 0.47833242579907154\n",
      "Iteration: 46 Loss: 0.4753654054100972\n",
      "Iteration: 47 Loss: 0.47246166207358403\n",
      "Iteration: 48 Loss: 0.46961494617651894\n",
      "Iteration: 49 Loss: 0.46682016351648636\n",
      "Iteration: 50 Loss: 0.46407341156902143\n",
      "Iteration: 51 Loss: 0.4613800177478545\n",
      "Iteration: 52 Loss: 0.4556786817221871\n",
      "Iteration: 53 Loss: 0.4462840019497981\n",
      "Iteration: 54 Loss: 0.4344170950815401\n",
      "Iteration: 55 Loss: 0.4210683891764644\n",
      "Iteration: 56 Loss: 0.4070233764537677\n",
      "Iteration: 57 Loss: 0.3928887561242744\n",
      "Iteration: 58 Loss: 0.37911784966833084\n",
      "Iteration: 59 Loss: 0.36603463713326645\n",
      "Iteration: 60 Loss: 0.353855976394789\n",
      "Iteration: 61 Loss: 0.3427117356335731\n",
      "Iteration: 62 Loss: 0.33266270416160865\n",
      "Iteration: 63 Loss: 0.3237162526414314\n",
      "Iteration: 64 Loss: 0.31583979444540256\n",
      "Iteration: 65 Loss: 0.30897215900234726\n",
      "Iteration: 66 Loss: 0.3030330288768797\n",
      "Iteration: 67 Loss: 0.30035694478570707\n",
      "Iteration: 68 Loss: 0.3004278682200635\n",
      "Iteration: 69 Loss: 0.299906339167198\n",
      "Iteration: 70 Loss: 0.2982797791672963\n",
      "Iteration: 71 Loss: 0.29561774532875057\n",
      "Iteration: 72 Loss: 0.29305715723761494\n",
      "Iteration: 73 Loss: 0.29058725047786427\n",
      "Iteration: 74 Loss: 0.2881986469824657\n",
      "Iteration: 75 Loss: 0.28588318015421654\n",
      "Iteration: 76 Loss: 0.28363326141892853\n",
      "Iteration: 77 Loss: 0.28144084457786367\n",
      "Iteration: 78 Loss: 0.2793030218762261\n",
      "Iteration: 79 Loss: 0.2772154514629878\n",
      "Iteration: 80 Loss: 0.27517256462890555\n",
      "Iteration: 81 Loss: 0.27317156447531765\n",
      "Iteration: 82 Loss: 0.27120967031626403\n",
      "Iteration: 83 Loss: 0.269284225834899\n",
      "Iteration: 84 Loss: 0.26739255933389117\n",
      "Iteration: 85 Loss: 0.2655325604974475\n",
      "Iteration: 86 Loss: 0.26370240236222026\n",
      "Iteration: 87 Loss: 0.2619003891279004\n",
      "Iteration: 88 Loss: 0.2601250031350997\n",
      "Iteration: 89 Loss: 0.25837489353708853\n",
      "Iteration: 90 Loss: 0.2566488607118219\n",
      "Iteration: 91 Loss: 0.25494583944097027\n",
      "Iteration: 92 Loss: 0.2532648824369633\n",
      "Iteration: 93 Loss: 0.25160514506718407\n",
      "Iteration: 94 Loss: 0.24996587165574413\n",
      "Iteration: 95 Loss: 0.24834638346040777\n",
      "Iteration: 96 Loss: 0.24674606826562384\n",
      "Iteration: 97 Loss: 0.2451643714470534\n",
      "Iteration: 98 Loss: 0.24360078833265314\n",
      "Iteration: 99 Loss: 0.24205485767846935\n",
      "Iteration: 100 Loss: 0.2405261560940127\n",
      "Iteration: 101 Loss: 0.2390142932640721\n",
      "Iteration: 102 Loss: 0.23751890784809218\n",
      "Iteration: 103 Loss: 0.23603966394670375\n",
      "Iteration: 104 Loss: 0.23457624804867844\n",
      "Iteration: 105 Loss: 0.23312836638481668\n",
      "Iteration: 106 Loss: 0.23169574254395633\n",
      "Iteration: 107 Loss: 0.23025354841048817\n",
      "Iteration: 108 Loss: 0.22879589952884907\n",
      "Iteration: 109 Loss: 0.22733013805658495\n",
      "Iteration: 110 Loss: 0.22586475330163303\n",
      "Iteration: 111 Loss: 0.2244064566766298\n",
      "Iteration: 112 Loss: 0.22296041205579778\n",
      "Iteration: 113 Loss: 0.22153045361568666\n",
      "Iteration: 114 Loss: 0.22011929239049055\n",
      "Iteration: 115 Loss: 0.21872870766669283\n",
      "Iteration: 116 Loss: 0.2173597207738495\n",
      "Iteration: 117 Loss: 0.216012750005332\n",
      "Iteration: 118 Loss: 0.21468774633512294\n",
      "Iteration: 119 Loss: 0.2133843103046004\n",
      "Iteration: 120 Loss: 0.21210179097421036\n",
      "Iteration: 121 Loss: 0.21083936819726673\n",
      "Iteration: 122 Loss: 0.20960201827865863\n",
      "Iteration: 123 Loss: 0.20841702088612066\n",
      "Iteration: 124 Loss: 0.20723841358151526\n",
      "Iteration: 125 Loss: 0.20606687797312118\n",
      "Iteration: 126 Loss: 0.2049030527574452\n",
      "Iteration: 127 Loss: 0.20374751839831187\n",
      "Iteration: 128 Loss: 0.20260078775484774\n",
      "Iteration: 129 Loss: 0.20146330146998842\n",
      "Iteration: 130 Loss: 0.20033542707573118\n",
      "Iteration: 131 Loss: 0.19921746091608722\n",
      "Iteration: 132 Loss: 0.1981096321304232\n",
      "Iteration: 133 Loss: 0.19701173041413647\n",
      "Iteration: 134 Loss: 0.19592362216021691\n",
      "Iteration: 135 Loss: 0.19484603460337316\n",
      "Iteration: 136 Loss: 0.19377911244288473\n",
      "Iteration: 137 Loss: 0.19272285202458922\n",
      "Iteration: 138 Loss: 0.1916771944170182\n",
      "Iteration: 139 Loss: 0.19064206821342805\n",
      "Iteration: 140 Loss: 0.18959008178646408\n",
      "Iteration: 141 Loss: 0.18851387823657573\n",
      "Iteration: 142 Loss: 0.18742817787947347\n",
      "Iteration: 143 Loss: 0.18634983658711096\n",
      "Iteration: 144 Loss: 0.18544412784588565\n",
      "Iteration: 145 Loss: 0.18323808208365322\n",
      "Iteration: 146 Loss: 0.17962931668584714\n",
      "Iteration: 147 Loss: 0.17517493070064183\n",
      "Iteration: 148 Loss: 0.17031978579192134\n",
      "Iteration: 149 Loss: 0.16540475247951422\n",
      "Iteration: 150 Loss: 0.16064814057799096\n",
      "Iteration: 151 Loss: 0.15617891140737938\n",
      "Iteration: 152 Loss: 0.1520645176645959\n",
      "Iteration: 153 Loss: 0.148405790003358\n",
      "Iteration: 154 Loss: 0.14525167297096744\n",
      "Iteration: 155 Loss: 0.14552070276033802\n",
      "Iteration: 156 Loss: 0.145954400068558\n",
      "Iteration: 157 Loss: 0.145976536153238\n",
      "Iteration: 158 Loss: 0.14565290649726978\n",
      "Iteration: 159 Loss: 0.14505039599810168\n",
      "Iteration: 160 Loss: 0.14423263734777073\n",
      "Iteration: 161 Loss: 0.14325528777876856\n",
      "Iteration: 162 Loss: 0.14216639862169964\n",
      "Iteration: 163 Loss: 0.14100879068075226\n",
      "Iteration: 164 Loss: 0.13981960571771035\n",
      "Iteration: 165 Loss: 0.13862943290305202\n",
      "Iteration: 166 Loss: 0.13746215210004256\n",
      "Iteration: 167 Loss: 0.1363352920652677\n",
      "Iteration: 168 Loss: 0.13526008784032645\n",
      "Iteration: 169 Loss: 0.13424327541334144\n",
      "Iteration: 170 Loss: 0.1332884748608264\n",
      "Iteration: 171 Loss: 0.13239530007147016\n",
      "Iteration: 172 Loss: 0.13170429675468706\n",
      "Iteration: 173 Loss: 0.13118012323574207\n",
      "Iteration: 174 Loss: 0.13062105132296956\n",
      "Iteration: 175 Loss: 0.13003321839635468\n",
      "Iteration: 176 Loss: 0.12942286110272855\n",
      "Iteration: 177 Loss: 0.12879600198005356\n",
      "Iteration: 178 Loss: 0.12815821987142087\n",
      "Iteration: 179 Loss: 0.12751449508421142\n",
      "Iteration: 180 Loss: 0.126869119345897\n",
      "Iteration: 181 Loss: 0.12622565971727692\n",
      "Iteration: 182 Loss: 0.12553883556376705\n",
      "Iteration: 183 Loss: 0.12479508201225731\n",
      "Iteration: 184 Loss: 0.12400232790971695\n",
      "Iteration: 185 Loss: 0.12316681321284166\n",
      "Iteration: 186 Loss: 0.12229521031945301\n",
      "Iteration: 187 Loss: 0.12139414224890534\n",
      "Iteration: 188 Loss: 0.12046995544222346\n",
      "Iteration: 189 Loss: 0.11952859363413522\n",
      "Iteration: 190 Loss: 0.11859464705369277\n",
      "Iteration: 191 Loss: 0.11764874245682312\n",
      "Iteration: 192 Loss: 0.1166945802429248\n",
      "Iteration: 193 Loss: 0.11573726857864569\n",
      "Iteration: 194 Loss: 0.11478140285958822\n",
      "Iteration: 195 Loss: 0.11383103328754576\n",
      "Iteration: 196 Loss: 0.11288967615675995\n",
      "Iteration: 197 Loss: 0.11196031801459295\n",
      "Iteration: 198 Loss: 0.11104543525531775\n",
      "Iteration: 199 Loss: 0.11014702127422557\n",
      "Iteration: 200 Loss: 0.10926661911458742\n",
      "Iteration: 201 Loss: 0.1084073630532764\n",
      "Iteration: 202 Loss: 0.10757187575400637\n",
      "Iteration: 203 Loss: 0.10675586058653427\n",
      "Iteration: 204 Loss: 0.10595940831469829\n",
      "Iteration: 205 Loss: 0.10518240221899719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 206 Loss: 0.10442454231066539\n",
      "Iteration: 207 Loss: 0.10368536916413304\n",
      "Iteration: 208 Loss: 0.10296428720068716\n",
      "Iteration: 209 Loss: 0.10226058724528092\n",
      "Iteration: 210 Loss: 0.101573468177657\n",
      "Iteration: 211 Loss: 0.1009020575072942\n",
      "Iteration: 212 Loss: 0.10024543071781422\n",
      "Iteration: 213 Loss: 0.09960262924888291\n",
      "Iteration: 214 Loss: 0.09897267701088841\n",
      "Iteration: 215 Loss: 0.09835439730856523\n",
      "Iteration: 216 Loss: 0.09774700193482434\n",
      "Iteration: 217 Loss: 0.09714957096723162\n",
      "Iteration: 218 Loss: 0.0965611947272244\n",
      "Iteration: 219 Loss: 0.09598100332852497\n",
      "Iteration: 220 Loss: 0.09540817515123809\n",
      "Iteration: 221 Loss: 0.09484194408983986\n",
      "Iteration: 222 Loss: 0.09428160627562118\n",
      "Iteration: 223 Loss: 0.0937265279122431\n",
      "Iteration: 224 Loss: 0.09317615832770682\n",
      "Iteration: 225 Loss: 0.0926300609815388\n",
      "Iteration: 226 Loss: 0.09208801322732436\n",
      "Iteration: 227 Loss: 0.09155046667856333\n",
      "Iteration: 228 Loss: 0.09102259553354396\n",
      "Iteration: 229 Loss: 0.09055805600175666\n",
      "Iteration: 230 Loss: 0.08968952699056885\n",
      "Iteration: 231 Loss: 0.08812592265842197\n",
      "Iteration: 232 Loss: 0.08619392785375997\n",
      "Iteration: 233 Loss: 0.08415086729715625\n",
      "Iteration: 234 Loss: 0.08218461165749033\n",
      "Iteration: 235 Loss: 0.08042005360822083\n",
      "Iteration: 236 Loss: 0.08028898534822117\n",
      "Iteration: 237 Loss: 0.08052921577821232\n",
      "Iteration: 238 Loss: 0.08048629804393595\n",
      "Iteration: 239 Loss: 0.08019751449238322\n",
      "Iteration: 240 Loss: 0.07971260992186589\n",
      "Iteration: 241 Loss: 0.0790823347302093\n",
      "Iteration: 242 Loss: 0.07835495319131018\n",
      "Iteration: 243 Loss: 0.07757624908426605\n",
      "Iteration: 244 Loss: 0.07678631721565696\n",
      "Iteration: 245 Loss: 0.07601693992014458\n",
      "Iteration: 246 Loss: 0.07529067754019726\n",
      "Iteration: 247 Loss: 0.0746588147475717\n",
      "Iteration: 248 Loss: 0.07438814118612097\n",
      "Iteration: 249 Loss: 0.07406103414812518\n",
      "Iteration: 250 Loss: 0.07368352604967401\n",
      "Iteration: 251 Loss: 0.0732643852699966\n",
      "Iteration: 252 Loss: 0.07281374851192374\n",
      "Iteration: 253 Loss: 0.07234199817584239\n",
      "Iteration: 254 Loss: 0.07185888106660972\n",
      "Iteration: 255 Loss: 0.07137287427872764\n",
      "Iteration: 256 Loss: 0.07089079846528189\n",
      "Iteration: 257 Loss: 0.07041765894061588\n",
      "Iteration: 258 Loss: 0.06995667700402414\n",
      "Iteration: 259 Loss: 0.06958007974158839\n",
      "Iteration: 260 Loss: 0.06919732743532987\n",
      "Iteration: 261 Loss: 0.06880666114066691\n",
      "Iteration: 262 Loss: 0.06840954722743142\n",
      "Iteration: 263 Loss: 0.06800783447956447\n",
      "Iteration: 264 Loss: 0.06760350424793965\n",
      "Iteration: 265 Loss: 0.06719847189307473\n",
      "Iteration: 266 Loss: 0.06679444293649346\n",
      "Iteration: 267 Loss: 0.066392822232232\n",
      "Iteration: 268 Loss: 0.06599467061759576\n",
      "Iteration: 269 Loss: 0.06560070094256983\n",
      "Iteration: 270 Loss: 0.06521130406635871\n",
      "Iteration: 271 Loss: 0.06482659519450673\n",
      "Iteration: 272 Loss: 0.06444877197302504\n",
      "Iteration: 273 Loss: 0.06407630672624112\n",
      "Iteration: 274 Loss: 0.06370475084855606\n",
      "Iteration: 275 Loss: 0.06333444623373398\n",
      "Iteration: 276 Loss: 0.06296573726476673\n",
      "Iteration: 277 Loss: 0.06260317012759303\n",
      "Iteration: 278 Loss: 0.06224331931687808\n",
      "Iteration: 279 Loss: 0.06188567011544477\n",
      "Iteration: 280 Loss: 0.06153002835304213\n",
      "Iteration: 281 Loss: 0.06117626387405349\n",
      "Iteration: 282 Loss: 0.06082430320723595\n",
      "Iteration: 283 Loss: 0.06047411944042208\n",
      "Iteration: 284 Loss: 0.06012572085376721\n",
      "Iteration: 285 Loss: 0.05977913957348767\n",
      "Iteration: 286 Loss: 0.05943442118593725\n",
      "Iteration: 287 Loss: 0.059091615933500034\n",
      "Iteration: 288 Loss: 0.058750771824386726\n",
      "Iteration: 289 Loss: 0.05841192974455363\n",
      "Iteration: 290 Loss: 0.058075120470336465\n",
      "Iteration: 291 Loss: 0.05774036334707142\n",
      "Iteration: 292 Loss: 0.05740766631890422\n",
      "Iteration: 293 Loss: 0.0570770269614834\n",
      "Iteration: 294 Loss: 0.05674843417329494\n",
      "Iteration: 295 Loss: 0.05642187021330326\n",
      "Iteration: 296 Loss: 0.05609731282257824\n",
      "Iteration: 297 Loss: 0.055774737226942386\n",
      "Iteration: 298 Loss: 0.05545411787905479\n",
      "Iteration: 299 Loss: 0.05513542985606856\n",
      "Iteration: 300 Loss: 0.054818649879173335\n",
      "Iteration: 301 Loss: 0.05450375696165967\n",
      "Iteration: 302 Loss: 0.054190732721718995\n",
      "Iteration: 303 Loss: 0.05387956141528844\n",
      "Iteration: 304 Loss: 0.05357022975391081\n",
      "Iteration: 305 Loss: 0.05326272657437024\n",
      "Iteration: 306 Loss: 0.05295704242264055\n",
      "Iteration: 307 Loss: 0.05265316910629979\n",
      "Iteration: 308 Loss: 0.052351099258787434\n",
      "Iteration: 309 Loss: 0.05205082594718757\n",
      "Iteration: 310 Loss: 0.05175234234384492\n",
      "Iteration: 311 Loss: 0.051455641471896275\n",
      "Iteration: 312 Loss: 0.051160716026322\n",
      "Iteration: 313 Loss: 0.050867558265621775\n",
      "Iteration: 314 Loss: 0.05057615996476318\n",
      "Iteration: 315 Loss: 0.0502865124174827\n",
      "Iteration: 316 Loss: 0.04999860647506964\n",
      "Iteration: 317 Loss: 0.049712432609125834\n",
      "Iteration: 318 Loss: 0.049427980987071705\n",
      "Iteration: 319 Loss: 0.04914524155105743\n",
      "Iteration: 320 Loss: 0.04886420409310915\n",
      "Iteration: 321 Loss: 0.04858485832153808\n",
      "Iteration: 322 Loss: 0.04830719391569402\n",
      "Iteration: 323 Loss: 0.04803120056789827\n",
      "Iteration: 324 Loss: 0.0477568680127906\n",
      "Iteration: 325 Loss: 0.047484186045341964\n",
      "Iteration: 326 Loss: 0.04721314452941938\n",
      "Iteration: 327 Loss: 0.04694373339911302\n",
      "Iteration: 328 Loss: 0.04667594265506677\n",
      "Iteration: 329 Loss: 0.04640976235789573\n",
      "Iteration: 330 Loss: 0.04614518262047373\n",
      "Iteration: 331 Loss: 0.04588219360048554\n",
      "Iteration: 332 Loss: 0.04562078549424871\n",
      "Iteration: 333 Loss: 0.04536094853240584\n",
      "Iteration: 334 Loss: 0.045102672977759294\n",
      "Iteration: 335 Loss: 0.044845949125227484\n",
      "Iteration: 336 Loss: 0.04459076730370075\n",
      "Iteration: 337 Loss: 0.04433711787943652\n",
      "Iteration: 338 Loss: 0.04408499126055677\n",
      "Iteration: 339 Loss: 0.04383437790219802\n",
      "Iteration: 340 Loss: 0.04358526831188669\n",
      "Iteration: 341 Loss: 0.043337653054773766\n",
      "Iteration: 342 Loss: 0.04309152275842834\n",
      "Iteration: 343 Loss: 0.042846868116983634\n",
      "Iteration: 344 Loss: 0.04260367989450065\n",
      "Iteration: 345 Loss: 0.04236194892749373\n",
      "Iteration: 346 Loss: 0.04212166612662118\n",
      "Iteration: 347 Loss: 0.04188282247759661\n",
      "Iteration: 348 Loss: 0.04164540904140578\n",
      "Iteration: 349 Loss: 0.041409416953938405\n",
      "Iteration: 350 Loss: 0.041174837425150476\n",
      "Iteration: 351 Loss: 0.04094166173786934\n",
      "Iteration: 352 Loss: 0.04070988124635167\n",
      "Iteration: 353 Loss: 0.04047948737468332\n",
      "Iteration: 354 Loss: 0.04025047161510151\n",
      "Iteration: 355 Loss: 0.04002282552629499\n",
      "Iteration: 356 Loss: 0.039796540731730036\n",
      "Iteration: 357 Loss: 0.0395716089180297\n",
      "Iteration: 358 Loss: 0.03934802183342353\n",
      "Iteration: 359 Loss: 0.039125771286278956\n",
      "Iteration: 360 Loss: 0.0389048491437121\n",
      "Iteration: 361 Loss: 0.038685247330277374\n",
      "Iteration: 362 Loss: 0.03846695782672903\n",
      "Iteration: 363 Loss: 0.03824997266884913\n",
      "Iteration: 364 Loss: 0.038034283946335685\n",
      "Iteration: 365 Loss: 0.03781988380174576\n",
      "Iteration: 366 Loss: 0.03760676442948966\n",
      "Iteration: 367 Loss: 0.03739491807487592\n",
      "Iteration: 368 Loss: 0.037184337033202015\n",
      "Iteration: 369 Loss: 0.03697501364889627\n",
      "Iteration: 370 Loss: 0.03676694031470783\n",
      "Iteration: 371 Loss: 0.03656010947094632\n",
      "Iteration: 372 Loss: 0.03635451360477353\n",
      "Iteration: 373 Loss: 0.036150145249548175\n",
      "Iteration: 374 Loss: 0.03594699698422085\n",
      "Iteration: 375 Loss: 0.03574506143278239\n",
      "Iteration: 376 Loss: 0.035544331263761705\n",
      "Iteration: 377 Loss: 0.03534479918977023\n",
      "Iteration: 378 Loss: 0.035146457967095264\n",
      "Iteration: 379 Loss: 0.03494930039533239\n",
      "Iteration: 380 Loss: 0.03475331931705723\n",
      "Iteration: 381 Loss: 0.03455850761753278\n",
      "Iteration: 382 Loss: 0.03436485822444361\n",
      "Iteration: 383 Loss: 0.034172364107661134\n",
      "Iteration: 384 Loss: 0.03398101827902541\n",
      "Iteration: 385 Loss: 0.03379081379214851\n",
      "Iteration: 386 Loss: 0.03360174374223092\n",
      "Iteration: 387 Loss: 0.03341380126588825\n",
      "Iteration: 388 Loss: 0.03322697954098807\n",
      "Iteration: 389 Loss: 0.033041271786488496\n",
      "Iteration: 390 Loss: 0.03285667126228427\n",
      "Iteration: 391 Loss: 0.03267317126904912\n",
      "Iteration: 392 Loss: 0.03249076514808109\n",
      "Iteration: 393 Loss: 0.03230944628114512\n",
      "Iteration: 394 Loss: 0.03212920809031063\n",
      "Iteration: 395 Loss: 0.03195004403778645\n",
      "Iteration: 396 Loss: 0.031771947625752736\n",
      "Iteration: 397 Loss: 0.03159491239618478\n",
      "Iteration: 398 Loss: 0.031418931930673404\n",
      "Iteration: 399 Loss: 0.03124399985023909\n",
      "Iteration: 400 Loss: 0.03107010981513943\n",
      "Iteration: 401 Loss: 0.03089725552467382\n",
      "Iteration: 402 Loss: 0.030725430716978672\n",
      "Iteration: 403 Loss: 0.030554629168820884\n",
      "Iteration: 404 Loss: 0.030384844695384258\n",
      "Iteration: 405 Loss: 0.030216071150050323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 406 Loss: 0.03004830242417773\n",
      "Iteration: 407 Loss: 0.029881532446874606\n",
      "Iteration: 408 Loss: 0.029715755184769616\n",
      "Iteration: 409 Loss: 0.029550964641778123\n",
      "Iteration: 410 Loss: 0.02938715485886687\n",
      "Iteration: 411 Loss: 0.029224319913815185\n",
      "Iteration: 412 Loss: 0.029062453920977036\n",
      "Iteration: 413 Loss: 0.02890155103103867\n",
      "Iteration: 414 Loss: 0.028741605430776865\n",
      "Iteration: 415 Loss: 0.02858261134281721\n",
      "Iteration: 416 Loss: 0.0284245630253918\n",
      "Iteration: 417 Loss: 0.028267454772096613\n",
      "Iteration: 418 Loss: 0.02811128091165178\n",
      "Iteration: 419 Loss: 0.027956035807659686\n",
      "Iteration: 420 Loss: 0.027801713858366845\n",
      "Iteration: 421 Loss: 0.027648309496427\n",
      "Iteration: 422 Loss: 0.027495817188664607\n",
      "Iteration: 423 Loss: 0.027344231435841384\n",
      "Iteration: 424 Loss: 0.02719354677242516\n",
      "Iteration: 425 Loss: 0.027043757766360515\n",
      "Iteration: 426 Loss: 0.02689485901884128\n",
      "Iteration: 427 Loss: 0.026746845164086534\n",
      "Iteration: 428 Loss: 0.02659971086911803\n",
      "Iteration: 429 Loss: 0.026453450833541226\n",
      "Iteration: 430 Loss: 0.026308059789327775\n",
      "Iteration: 431 Loss: 0.026163532500600394\n",
      "Iteration: 432 Loss: 0.02601986376342226\n",
      "Iteration: 433 Loss: 0.0258770484055864\n",
      "Iteration: 434 Loss: 0.025735081286407953\n",
      "Iteration: 435 Loss: 0.025593957296521662\n",
      "Iteration: 436 Loss: 0.025453671357676982\n",
      "Iteration: 437 Loss: 0.025314218422539343\n",
      "Iteration: 438 Loss: 0.025175593474491224\n",
      "Iteration: 439 Loss: 0.02503779152743747\n",
      "Iteration: 440 Loss: 0.02490080762560979\n",
      "Iteration: 441 Loss: 0.024764636843376105\n",
      "Iteration: 442 Loss: 0.024629274285048904\n",
      "Iteration: 443 Loss: 0.024494715084699147\n",
      "Iteration: 444 Loss: 0.02436095440596805\n",
      "Iteration: 445 Loss: 0.024227987441882388\n",
      "Iteration: 446 Loss: 0.024095809414672265\n",
      "Iteration: 447 Loss: 0.023964415575588278\n",
      "Iteration: 448 Loss: 0.02383380120472192\n",
      "Iteration: 449 Loss: 0.02370396161082577\n",
      "Iteration: 450 Loss: 0.02357489213113668\n",
      "Iteration: 451 Loss: 0.023446588131198882\n",
      "Iteration: 452 Loss: 0.02331904500468984\n",
      "Iteration: 453 Loss: 0.023192258173245828\n",
      "Iteration: 454 Loss: 0.02306622308628955\n",
      "Iteration: 455 Loss: 0.02294093522085838\n",
      "Iteration: 456 Loss: 0.02281639008143454\n",
      "Iteration: 457 Loss: 0.022692583199774994\n",
      "Iteration: 458 Loss: 0.022569510134745382\n",
      "Iteration: 459 Loss: 0.022447166472148068\n",
      "Iteration: 460 Loss: 0.022325547824561365\n",
      "Iteration: 461 Loss: 0.0222046498311719\n",
      "Iteration: 462 Loss: 0.022084468157610657\n",
      "Iteration: 463 Loss: 0.021964998495789315\n",
      "Iteration: 464 Loss: 0.02184623656373942\n",
      "Iteration: 465 Loss: 0.021728178105450864\n",
      "Iteration: 466 Loss: 0.02161081889071029\n",
      "Iteration: 467 Loss: 0.021494154714943793\n",
      "Iteration: 468 Loss: 0.0213781813990576\n",
      "Iteration: 469 Loss: 0.021262894789280628\n",
      "Iteration: 470 Loss: 0.02114829075700879\n",
      "Iteration: 471 Loss: 0.021034365198648885\n",
      "Iteration: 472 Loss: 0.020921114035465278\n",
      "Iteration: 473 Loss: 0.020808533213425417\n",
      "Iteration: 474 Loss: 0.020696618703047488\n",
      "Iteration: 475 Loss: 0.02058536649924897\n",
      "Iteration: 476 Loss: 0.020474772621197405\n",
      "Iteration: 477 Loss: 0.020364833112158268\n",
      "Iteration: 478 Loss: 0.020255544039348587\n",
      "Iteration: 479 Loss: 0.020146901493786847\n",
      "Iteration: 480 Loss: 0.020038901590149018\n",
      "Iteration: 481 Loss: 0.019931540466621045\n",
      "Iteration: 482 Loss: 0.019824814284753887\n",
      "Iteration: 483 Loss: 0.019718719229320558\n",
      "Iteration: 484 Loss: 0.01961325150817183\n",
      "Iteration: 485 Loss: 0.019508407352095896\n",
      "Iteration: 486 Loss: 0.019404183014675763\n",
      "Iteration: 487 Loss: 0.019300574772150348\n",
      "Iteration: 488 Loss: 0.0191975789232737\n",
      "Iteration: 489 Loss: 0.019095191789178134\n",
      "Iteration: 490 Loss: 0.01899340971323502\n",
      "Iteration: 491 Loss: 0.01889222906092102\n",
      "Iteration: 492 Loss: 0.018791646219679256\n",
      "Iteration: 493 Loss: 0.018691657598787152\n",
      "Iteration: 494 Loss: 0.01859225962922085\n",
      "Iteration: 495 Loss: 0.01849344876352353\n",
      "Iteration: 496 Loss: 0.018395221475671724\n",
      "Iteration: 497 Loss: 0.01829757426094577\n",
      "Iteration: 498 Loss: 0.018200503635798188\n",
      "Iteration: 499 Loss: 0.01810400613772492\n",
      "Iteration: 500 Loss: 0.01800807832513623\n",
      "Iteration: 501 Loss: 0.017912716777228506\n",
      "Iteration: 502 Loss: 0.017817918093858024\n",
      "Iteration: 503 Loss: 0.017723678895413725\n",
      "Iteration: 504 Loss: 0.01762999582269276\n",
      "Iteration: 505 Loss: 0.01753686553677553\n",
      "Iteration: 506 Loss: 0.01744428471890123\n",
      "Iteration: 507 Loss: 0.017352250070346357\n",
      "Iteration: 508 Loss: 0.0172607583123005\n",
      "Iteration: 509 Loss: 0.017169806185747274\n",
      "Iteration: 510 Loss: 0.0170793904513407\n",
      "Iteration: 511 Loss: 0.016989507889288647\n",
      "Iteration: 512 Loss: 0.01690015529923024\n",
      "Iteration: 513 Loss: 0.016811329500120105\n",
      "Iteration: 514 Loss: 0.016723027330110003\n",
      "Iteration: 515 Loss: 0.01663524564643124\n",
      "Iteration: 516 Loss: 0.016547981325279022\n",
      "Iteration: 517 Loss: 0.016461231261697178\n",
      "Iteration: 518 Loss: 0.0163749923694631\n",
      "Iteration: 519 Loss: 0.016289261580973758\n",
      "Iteration: 520 Loss: 0.016204035847132335\n",
      "Iteration: 521 Loss: 0.016119312137237253\n",
      "Iteration: 522 Loss: 0.016035087438867265\n",
      "Iteration: 523 Loss: 0.015951358757772663\n",
      "Iteration: 524 Loss: 0.015868123117764393\n",
      "Iteration: 525 Loss: 0.015785377560604212\n",
      "Iteration: 526 Loss: 0.01570311914589426\n",
      "Iteration: 527 Loss: 0.015621344950971176\n",
      "Iteration: 528 Loss: 0.01554005207079615\n",
      "Iteration: 529 Loss: 0.015459237617849342\n",
      "Iteration: 530 Loss: 0.015378898722022296\n",
      "Iteration: 531 Loss: 0.015299032530513108\n",
      "Iteration: 532 Loss: 0.015219636207720792\n",
      "Iteration: 533 Loss: 0.015140706935141695\n",
      "Iteration: 534 Loss: 0.015062241911263909\n",
      "Iteration: 535 Loss: 0.014984238351466663\n",
      "Iteration: 536 Loss: 0.014906693487915493\n",
      "Iteration: 537 Loss: 0.014829604569462447\n",
      "Iteration: 538 Loss: 0.01475296886154256\n",
      "Iteration: 539 Loss: 0.014676783646075635\n",
      "Iteration: 540 Loss: 0.014601046221364804\n",
      "Iteration: 541 Loss: 0.014525753901997899\n",
      "Iteration: 542 Loss: 0.014450904018747705\n",
      "Iteration: 543 Loss: 0.014376493918475296\n",
      "Iteration: 544 Loss: 0.01430252096403144\n",
      "Iteration: 545 Loss: 0.014228982534159771\n",
      "Iteration: 546 Loss: 0.01415587602340071\n",
      "Iteration: 547 Loss: 0.014083198841996035\n",
      "Iteration: 548 Loss: 0.014010948415793954\n",
      "Iteration: 549 Loss: 0.01393912218615322\n",
      "Iteration: 550 Loss: 0.013867717609850498\n",
      "Iteration: 551 Loss: 0.013796732158986507\n",
      "Iteration: 552 Loss: 0.013726163320893365\n",
      "Iteration: 553 Loss: 0.013656008598042191\n",
      "Iteration: 554 Loss: 0.01358626550795141\n",
      "Iteration: 555 Loss: 0.013516931583095908\n",
      "Iteration: 556 Loss: 0.013448004370816181\n",
      "Iteration: 557 Loss: 0.01337948143322928\n",
      "Iteration: 558 Loss: 0.013311360347138044\n",
      "Iteration: 559 Loss: 0.013243638703942993\n",
      "Iteration: 560 Loss: 0.013176314109554476\n",
      "Iteration: 561 Loss: 0.013109384184303198\n",
      "Iteration: 562 Loss: 0.013042846562854985\n",
      "Iteration: 563 Loss: 0.012976698894123517\n",
      "Iteration: 564 Loss: 0.012910938841182575\n",
      "Iteration: 565 Loss: 0.012845564081182677\n",
      "Iteration: 566 Loss: 0.012780572305265093\n",
      "Iteration: 567 Loss: 0.01271596121847659\n",
      "Iteration: 568 Loss: 0.012651728539686226\n",
      "Iteration: 569 Loss: 0.012587872001501391\n",
      "Iteration: 570 Loss: 0.01252438935018498\n",
      "Iteration: 571 Loss: 0.012461278345571867\n",
      "Iteration: 572 Loss: 0.01239853676098895\n",
      "Iteration: 573 Loss: 0.012336162383171954\n",
      "Iteration: 574 Loss: 0.01227415301218458\n",
      "Iteration: 575 Loss: 0.012212506461338266\n",
      "Iteration: 576 Loss: 0.012151220557112844\n",
      "Iteration: 577 Loss: 0.012090293139075383\n",
      "Iteration: 578 Loss: 0.012029722059802619\n",
      "Iteration: 579 Loss: 0.011969505184801296\n",
      "Iteration: 580 Loss: 0.01190964039243086\n",
      "Iteration: 581 Loss: 0.011850125573825546\n",
      "Iteration: 582 Loss: 0.011790958632816964\n",
      "Iteration: 583 Loss: 0.011732137485857615\n",
      "Iteration: 584 Loss: 0.011673660061945274\n",
      "Iteration: 585 Loss: 0.011615524302545704\n",
      "Iteration: 586 Loss: 0.011557728161519561\n",
      "Iteration: 587 Loss: 0.011500269605045922\n",
      "Iteration: 588 Loss: 0.011443146611547338\n",
      "Iteration: 589 Loss: 0.0113863571716181\n",
      "Iteration: 590 Loss: 0.011329899287949193\n",
      "Iteration: 591 Loss: 0.011273770975256241\n",
      "Iteration: 592 Loss: 0.011217970260205523\n",
      "Iteration: 593 Loss: 0.011162495181343051\n",
      "Iteration: 594 Loss: 0.011107343789022808\n",
      "Iteration: 595 Loss: 0.011052514145334851\n",
      "Iteration: 596 Loss: 0.010998004324035282\n",
      "Iteration: 597 Loss: 0.010943812410474622\n",
      "Iteration: 598 Loss: 0.010889936501529624\n",
      "Iteration: 599 Loss: 0.01083637470553238\n",
      "Iteration: 600 Loss: 0.010783125142201437\n",
      "Iteration: 601 Loss: 0.010730185942573666\n",
      "Iteration: 602 Loss: 0.010677555248934916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 603 Loss: 0.010625231214753762\n",
      "Iteration: 604 Loss: 0.01057321200461261\n",
      "Iteration: 605 Loss: 0.010521495794140912\n",
      "Iteration: 606 Loss: 0.010470080769949097\n",
      "Iteration: 607 Loss: 0.010418965129562278\n",
      "Iteration: 608 Loss: 0.010368147081353923\n",
      "Iteration: 609 Loss: 0.01031762484448041\n",
      "Iteration: 610 Loss: 0.010267396648817153\n",
      "Iteration: 611 Loss: 0.01021746073489226\n",
      "Iteration: 612 Loss: 0.010167815353823523\n",
      "Iteration: 613 Loss: 0.010118458767233099\n",
      "Iteration: 614 Loss: 0.010069389247267007\n",
      "Iteration: 615 Loss: 0.010020605076410725\n",
      "Iteration: 616 Loss: 0.009972104547503444\n",
      "Iteration: 617 Loss: 0.009923885963659486\n",
      "Iteration: 618 Loss: 0.009875947638204841\n",
      "Iteration: 619 Loss: 0.009828287894616419\n",
      "Iteration: 620 Loss: 0.009780905066460056\n",
      "Iteration: 621 Loss: 0.009733797497330166\n",
      "Iteration: 622 Loss: 0.009686963540788851\n",
      "Iteration: 623 Loss: 0.009640401560306017\n",
      "Iteration: 624 Loss: 0.009594109929200456\n",
      "Iteration: 625 Loss: 0.00954808703057905\n",
      "Iteration: 626 Loss: 0.009502331257278807\n",
      "Iteration: 627 Loss: 0.00945684101180824\n",
      "Iteration: 628 Loss: 0.009411614706287954\n",
      "Iteration: 629 Loss: 0.009366650762394166\n",
      "Iteration: 630 Loss: 0.009321947611300178\n",
      "Iteration: 631 Loss: 0.009277503693619587\n",
      "Iteration: 632 Loss: 0.009233317459349286\n",
      "Iteration: 633 Loss: 0.009189387367812957\n",
      "Iteration: 634 Loss: 0.009145711887604609\n",
      "Iteration: 635 Loss: 0.009102289496533064\n",
      "Iteration: 636 Loss: 0.009059118681566343\n",
      "Iteration: 637 Loss: 0.009016197938776786\n",
      "Iteration: 638 Loss: 0.008973525773285132\n",
      "Iteration: 639 Loss: 0.008931100699207238\n",
      "Iteration: 640 Loss: 0.008888921239599565\n",
      "Iteration: 641 Loss: 0.008846985926404658\n",
      "Iteration: 642 Loss: 0.008805293300398335\n",
      "Iteration: 643 Loss: 0.008763841911136352\n",
      "Iteration: 644 Loss: 0.00872263031690129\n",
      "Iteration: 645 Loss: 0.008681657084650223\n",
      "Iteration: 646 Loss: 0.008640920789962413\n",
      "Iteration: 647 Loss: 0.008600420016987013\n",
      "Iteration: 648 Loss: 0.008560153358392162\n",
      "Iteration: 649 Loss: 0.008520119415312938\n",
      "Iteration: 650 Loss: 0.008480316797301084\n",
      "Iteration: 651 Loss: 0.00844074412227354\n",
      "Iteration: 652 Loss: 0.00840140001646283\n",
      "Iteration: 653 Loss: 0.008362283114366435\n",
      "Iteration: 654 Loss: 0.008323392058696477\n",
      "Iteration: 655 Loss: 0.008284725500332572\n",
      "Iteration: 656 Loss: 0.008246282098269704\n",
      "Iteration: 657 Loss: 0.008208060519571169\n",
      "Iteration: 658 Loss: 0.008170059439319923\n",
      "Iteration: 659 Loss: 0.008132277540568871\n",
      "Iteration: 660 Loss: 0.008094713514294352\n",
      "Iteration: 661 Loss: 0.008057324652693473\n",
      "Iteration: 662 Loss: 0.008019681883339552\n",
      "Iteration: 663 Loss: 0.007982242367801257\n",
      "Iteration: 664 Loss: 0.007945080985443213\n",
      "Iteration: 665 Loss: 0.007908229768095847\n",
      "Iteration: 666 Loss: 0.00787168248383948\n",
      "Iteration: 667 Loss: 0.007835412383914562\n",
      "Iteration: 668 Loss: 0.007799389237294779\n",
      "Iteration: 669 Loss: 0.0077635895292273525\n",
      "Iteration: 670 Loss: 0.007727999546470509\n",
      "Iteration: 671 Loss: 0.0076926138456462866\n",
      "Iteration: 672 Loss: 0.007657431952384945\n",
      "Iteration: 673 Loss: 0.0076224552653259775\n",
      "Iteration: 674 Loss: 0.007587685031073056\n",
      "Iteration: 675 Loss: 0.007553121443225013\n",
      "Iteration: 676 Loss: 0.007518763529258721\n",
      "Iteration: 677 Loss: 0.0074846094269731735\n",
      "Iteration: 678 Loss: 0.007450656756859818\n",
      "Iteration: 679 Loss: 0.007416902937854296\n",
      "Iteration: 680 Loss: 0.007383345401866382\n",
      "Iteration: 681 Loss: 0.007349981719319387\n",
      "Iteration: 682 Loss: 0.007316809664508941\n",
      "Iteration: 683 Loss: 0.007283827244958363\n",
      "Iteration: 684 Loss: 0.007251032708623889\n",
      "Iteration: 685 Loss: 0.007218424534944587\n",
      "Iteration: 686 Loss: 0.007186001412412536\n",
      "Iteration: 687 Loss: 0.007153762205306773\n",
      "Iteration: 688 Loss: 0.0071217059133909235\n",
      "Iteration: 689 Loss: 0.007089831629242248\n",
      "Iteration: 690 Loss: 0.007058138497755409\n",
      "Iteration: 691 Loss: 0.00702662568140601\n",
      "Iteration: 692 Loss: 0.006995292333427744\n",
      "Iteration: 693 Loss: 0.006964137579621322\n",
      "Iteration: 694 Loss: 0.006933160508340621\n",
      "Iteration: 695 Loss: 0.006902360167433816\n",
      "Iteration: 696 Loss: 0.006871735566521265\n",
      "Iteration: 697 Loss: 0.006841285682915963\n",
      "Iteration: 698 Loss: 0.006811009469614422\n",
      "Iteration: 699 Loss: 0.006780905864026052\n",
      "Iteration: 700 Loss: 0.006750973796425381\n",
      "Iteration: 701 Loss: 0.0067212121974009114\n",
      "Iteration: 702 Loss: 0.006691620003876598\n",
      "Iteration: 703 Loss: 0.006662196163520932\n",
      "Iteration: 704 Loss: 0.006632939637554092\n",
      "Iteration: 705 Loss: 0.006603849402117888\n",
      "Iteration: 706 Loss: 0.006574924448447491\n",
      "Iteration: 707 Loss: 0.006546163782143112\n",
      "Iteration: 708 Loss: 0.0065175664218389264\n",
      "Iteration: 709 Loss: 0.006489131397543291\n",
      "Iteration: 710 Loss: 0.006460857748884194\n",
      "Iteration: 711 Loss: 0.006432744523442665\n",
      "Iteration: 712 Loss: 0.006404790775296663\n",
      "Iteration: 713 Loss: 0.006376995563853956\n",
      "Iteration: 714 Loss: 0.0063493579529990685\n",
      "Iteration: 715 Loss: 0.006321877010551371\n",
      "Iteration: 716 Loss: 0.006294551807998752\n",
      "Iteration: 717 Loss: 0.006267381420460098\n",
      "Iteration: 718 Loss: 0.006240364926820601\n",
      "Iteration: 719 Loss: 0.006213501409980147\n",
      "Iteration: 720 Loss: 0.006186789957165964\n",
      "Iteration: 721 Loss: 0.00616022966026139\n",
      "Iteration: 722 Loss: 0.006133819616117541\n",
      "Iteration: 723 Loss: 0.006107558926823304\n",
      "Iteration: 724 Loss: 0.006081446699914598\n",
      "Iteration: 725 Loss: 0.00605548204852503\n",
      "Iteration: 726 Loss: 0.006029664091469764\n",
      "Iteration: 727 Loss: 0.00600399195327202\n",
      "Iteration: 728 Loss: 0.005978464764145015\n",
      "Iteration: 729 Loss: 0.0059530816599314765\n",
      "Iteration: 730 Loss: 0.005927841782021203\n",
      "Iteration: 731 Loss: 0.005902744277252107\n",
      "Iteration: 732 Loss: 0.005877788297803583\n",
      "Iteration: 733 Loss: 0.005852973001093358\n",
      "Iteration: 734 Loss: 0.00582829754968158\n",
      "Iteration: 735 Loss: 0.0058037611111831046\n",
      "Iteration: 736 Loss: 0.005779362858195267\n",
      "Iteration: 737 Loss: 0.005755101968236385\n",
      "Iteration: 738 Loss: 0.005730977623697697\n",
      "Iteration: 739 Loss: 0.005706989011804412\n",
      "Iteration: 740 Loss: 0.005683135324587522\n",
      "Iteration: 741 Loss: 0.005659415758860743\n",
      "Iteration: 742 Loss: 0.005635829516202396\n",
      "Iteration: 743 Loss: 0.005612375802938964\n",
      "Iteration: 744 Loss: 0.005589053830130758\n",
      "Iteration: 745 Loss: 0.005565862813555595\n",
      "Iteration: 746 Loss: 0.005542801973693126\n",
      "Iteration: 747 Loss: 0.005519870535705396\n",
      "Iteration: 748 Loss: 0.005497067729415586\n",
      "Iteration: 749 Loss: 0.005474392789286233\n",
      "Iteration: 750 Loss: 0.005451844954392624\n",
      "Iteration: 751 Loss: 0.005429423468398053\n",
      "Iteration: 752 Loss: 0.005407127579524626\n",
      "Iteration: 753 Loss: 0.005384956540525822\n",
      "Iteration: 754 Loss: 0.005362909608655442\n",
      "Iteration: 755 Loss: 0.005340986045640389\n",
      "Iteration: 756 Loss: 0.005319185117650066\n",
      "Iteration: 757 Loss: 0.005297506095268322\n",
      "Iteration: 758 Loss: 0.005275948253464659\n",
      "Iteration: 759 Loss: 0.005254510871566735\n",
      "Iteration: 760 Loss: 0.005233193233232338\n",
      "Iteration: 761 Loss: 0.005211994626422578\n",
      "Iteration: 762 Loss: 0.005190914343376783\n",
      "Iteration: 763 Loss: 0.0051699516805856415\n",
      "Iteration: 764 Loss: 0.005149105938765739\n",
      "Iteration: 765 Loss: 0.005128376422835077\n",
      "Iteration: 766 Loss: 0.005107762441887387\n",
      "Iteration: 767 Loss: 0.00508726330916803\n",
      "Iteration: 768 Loss: 0.005066878342048801\n",
      "Iteration: 769 Loss: 0.0050466068620036286\n",
      "Iteration: 770 Loss: 0.005026448194584142\n",
      "Iteration: 771 Loss: 0.005006401669395892\n",
      "Iteration: 772 Loss: 0.004986466620073437\n",
      "Iteration: 773 Loss: 0.004966642384256192\n",
      "Iteration: 774 Loss: 0.004946928303564613\n",
      "Iteration: 775 Loss: 0.004927323723575992\n",
      "Iteration: 776 Loss: 0.004907827993801543\n",
      "Iteration: 777 Loss: 0.0048884404676611945\n",
      "Iteration: 778 Loss: 0.004869160502461006\n",
      "Iteration: 779 Loss: 0.004849987459368928\n",
      "Iteration: 780 Loss: 0.004830920703392875\n",
      "Iteration: 781 Loss: 0.004811959603355777\n",
      "Iteration: 782 Loss: 0.004793103531874332\n",
      "Iteration: 783 Loss: 0.004774351865334857\n",
      "Iteration: 784 Loss: 0.004755703983871115\n",
      "Iteration: 785 Loss: 0.004737159271341655\n",
      "Iteration: 786 Loss: 0.004718717115307823\n",
      "Iteration: 787 Loss: 0.004700376907010413\n",
      "Iteration: 788 Loss: 0.004682138041349545\n",
      "Iteration: 789 Loss: 0.004663999916859757\n",
      "Iteration: 790 Loss: 0.004645961935691082\n",
      "Iteration: 791 Loss: 0.004628023503585612\n",
      "Iteration: 792 Loss: 0.004610184029856008\n",
      "Iteration: 793 Loss: 0.004592442927364766\n",
      "Iteration: 794 Loss: 0.0045747996125019355\n",
      "Iteration: 795 Loss: 0.0045572535051653495\n",
      "Iteration: 796 Loss: 0.004539804028737702\n",
      "Iteration: 797 Loss: 0.004522450610066358\n",
      "Iteration: 798 Loss: 0.004505192679443086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 799 Loss: 0.004488029670582538\n",
      "Iteration: 800 Loss: 0.004470961020602415\n",
      "Iteration: 801 Loss: 0.0044539861700022745\n",
      "Iteration: 802 Loss: 0.004437104562643262\n",
      "Iteration: 803 Loss: 0.004420315645728338\n",
      "Iteration: 804 Loss: 0.00440361886978181\n",
      "Iteration: 805 Loss: 0.0043870136886293935\n",
      "Iteration: 806 Loss: 0.0043704995593786\n",
      "Iteration: 807 Loss: 0.0043540759423988645\n",
      "Iteration: 808 Loss: 0.00433774230130111\n",
      "Iteration: 809 Loss: 0.004321498102920187\n",
      "Iteration: 810 Loss: 0.004305342817293652\n",
      "Iteration: 811 Loss: 0.0042892759176430435\n",
      "Iteration: 812 Loss: 0.004273296880355155\n",
      "Iteration: 813 Loss: 0.004257405184962389\n",
      "Iteration: 814 Loss: 0.004241600314124218\n",
      "Iteration: 815 Loss: 0.00422588175360818\n",
      "Iteration: 816 Loss: 0.004210248992271618\n",
      "Iteration: 817 Loss: 0.004194701522042752\n",
      "Iteration: 818 Loss: 0.004179238837901784\n",
      "Iteration: 819 Loss: 0.004163860437863392\n",
      "Iteration: 820 Loss: 0.0041485658229578485\n",
      "Iteration: 821 Loss: 0.00413335449721311\n",
      "Iteration: 822 Loss: 0.0041182259676363015\n",
      "Iteration: 823 Loss: 0.004103179744197425\n",
      "Iteration: 824 Loss: 0.004088215339809648\n",
      "Iteration: 825 Loss: 0.004073332270312506\n",
      "Iteration: 826 Loss: 0.004058530054453576\n",
      "Iteration: 827 Loss: 0.004043808213872307\n",
      "Iteration: 828 Loss: 0.004029166273081248\n",
      "Iteration: 829 Loss: 0.004014603759449544\n",
      "Iteration: 830 Loss: 0.004000120203186099\n",
      "Iteration: 831 Loss: 0.003985715137321421\n",
      "Iteration: 832 Loss: 0.003971388097691525\n",
      "Iteration: 833 Loss: 0.003957138622920371\n",
      "Iteration: 834 Loss: 0.003942966254403644\n",
      "Iteration: 835 Loss: 0.0039288705362923405\n",
      "Iteration: 836 Loss: 0.003914851015474811\n",
      "Iteration: 837 Loss: 0.0039009072415624956\n",
      "Iteration: 838 Loss: 0.0038870387668710447\n",
      "Iteration: 839 Loss: 0.0038732451464063462\n",
      "Iteration: 840 Loss: 0.0038595259378465304\n",
      "Iteration: 841 Loss: 0.003845880701527482\n",
      "Iteration: 842 Loss: 0.003832309000425154\n",
      "Iteration: 843 Loss: 0.0038188104001413486\n",
      "Iteration: 844 Loss: 0.0038053844688868497\n",
      "Iteration: 845 Loss: 0.003792030777465952\n",
      "Iteration: 846 Loss: 0.0037787488992611544\n",
      "Iteration: 847 Loss: 0.0037655384102176256\n",
      "Iteration: 848 Loss: 0.0037523988888272094\n",
      "Iteration: 849 Loss: 0.0037393299161140136\n",
      "Iteration: 850 Loss: 0.003726331075617914\n",
      "Iteration: 851 Loss: 0.0037134019533810615\n",
      "Iteration: 852 Loss: 0.003700542137931434\n",
      "Iteration: 853 Loss: 0.0036877512202693288\n",
      "Iteration: 854 Loss: 0.0036750287938498837\n",
      "Iteration: 855 Loss: 0.003662374454571792\n",
      "Iteration: 856 Loss: 0.0036497878007599725\n",
      "Iteration: 857 Loss: 0.003637268433152648\n",
      "Iteration: 858 Loss: 0.003624815954884703\n",
      "Iteration: 859 Loss: 0.0036124299714752625\n",
      "Iteration: 860 Loss: 0.0036001100908136367\n",
      "Iteration: 861 Loss: 0.0035878559231427194\n",
      "Iteration: 862 Loss: 0.003575667081046599\n",
      "Iteration: 863 Loss: 0.0035635431794367716\n",
      "Iteration: 864 Loss: 0.003551483835537423\n",
      "Iteration: 865 Loss: 0.0035394886688707283\n",
      "Iteration: 866 Loss: 0.003527557301244936\n",
      "Iteration: 867 Loss: 0.0035156893567386666\n",
      "Iteration: 868 Loss: 0.0035038844616890824\n",
      "Iteration: 869 Loss: 0.003492142244676935\n",
      "Iteration: 870 Loss: 0.0034804623365138187\n",
      "Iteration: 871 Loss: 0.003468844370228107\n",
      "Iteration: 872 Loss: 0.003457287981052518\n",
      "Iteration: 873 Loss: 0.0034457928064096743\n",
      "Iteration: 874 Loss: 0.003434358485900713\n",
      "Iteration: 875 Loss: 0.0034229846612898055\n",
      "Iteration: 876 Loss: 0.0034116709764931094\n",
      "Iteration: 877 Loss: 0.0034004170775649967\n",
      "Iteration: 878 Loss: 0.003389222612686084\n",
      "Iteration: 879 Loss: 0.00337808723214886\n",
      "Iteration: 880 Loss: 0.0033670105883460808\n",
      "Iteration: 881 Loss: 0.003355992335757482\n",
      "Iteration: 882 Loss: 0.0033450321309386306\n",
      "Iteration: 883 Loss: 0.0033341296325069963\n",
      "Iteration: 884 Loss: 0.0033232845011299805\n",
      "Iteration: 885 Loss: 0.0033124963995125227\n",
      "Iteration: 886 Loss: 0.003301764992385415\n",
      "Iteration: 887 Loss: 0.0032910899464923776\n",
      "Iteration: 888 Loss: 0.0032804709305781538\n",
      "Iteration: 889 Loss: 0.003269907615377189\n",
      "Iteration: 890 Loss: 0.0032593996736007013\n",
      "Iteration: 891 Loss: 0.0032489467799245676\n",
      "Iteration: 892 Loss: 0.0032385486109788165\n",
      "Iteration: 893 Loss: 0.0032282048453350374\n",
      "Iteration: 894 Loss: 0.003217915163494877\n",
      "Iteration: 895 Loss: 0.003207679247878851\n",
      "Iteration: 896 Loss: 0.0031974967828134982\n",
      "Iteration: 897 Loss: 0.0031873674545212557\n",
      "Iteration: 898 Loss: 0.0031772909511081785\n",
      "Iteration: 899 Loss: 0.003167266962553736\n",
      "Iteration: 900 Loss: 0.0031572951806983158\n",
      "Iteration: 901 Loss: 0.0031473752992326894\n",
      "Iteration: 902 Loss: 0.0031375070136873515\n",
      "Iteration: 903 Loss: 0.0031276900214198306\n",
      "Iteration: 904 Loss: 0.0031179240216053943\n",
      "Iteration: 905 Loss: 0.0031082087152254235\n",
      "Iteration: 906 Loss: 0.00309854380505669\n",
      "Iteration: 907 Loss: 0.003088928995660308\n",
      "Iteration: 908 Loss: 0.003079363993371629\n",
      "Iteration: 909 Loss: 0.0030698485062882544\n",
      "Iteration: 910 Loss: 0.0030603822442611625\n",
      "Iteration: 911 Loss: 0.0030509649188826525\n",
      "Iteration: 912 Loss: 0.003041596243477238\n",
      "Iteration: 913 Loss: 0.0030322759330904435\n",
      "Iteration: 914 Loss: 0.003023003704478316\n",
      "Iteration: 915 Loss: 0.003013779276097499\n",
      "Iteration: 916 Loss: 0.0030046023680945916\n",
      "Iteration: 917 Loss: 0.0029954727022968625\n",
      "Iteration: 918 Loss: 0.0029863900022014185\n",
      "Iteration: 919 Loss: 0.0029773539929652446\n",
      "Iteration: 920 Loss: 0.0029683644013953165\n",
      "Iteration: 921 Loss: 0.0029594209559390024\n",
      "Iteration: 922 Loss: 0.0029505233866733535\n",
      "Iteration: 923 Loss: 0.002941671425296631\n",
      "Iteration: 924 Loss: 0.0029328648051167292\n",
      "Iteration: 925 Loss: 0.00292410326104354\n",
      "Iteration: 926 Loss: 0.0029153865295779677\n",
      "Iteration: 927 Loss: 0.002906714348803135\n",
      "Iteration: 928 Loss: 0.0028980864583736774\n",
      "Iteration: 929 Loss: 0.002889502599507861\n",
      "Iteration: 930 Loss: 0.002880962514976896\n",
      "Iteration: 931 Loss: 0.0028724659490965278\n",
      "Iteration: 932 Loss: 0.002864012647717066\n",
      "Iteration: 933 Loss: 0.00285560235821477\n",
      "Iteration: 934 Loss: 0.00284723482948233\n",
      "Iteration: 935 Loss: 0.0028389098119197846\n",
      "Iteration: 936 Loss: 0.002830627057425101\n",
      "Iteration: 937 Loss: 0.0028223863193857363\n",
      "Iteration: 938 Loss: 0.0028141873526698296\n",
      "Iteration: 939 Loss: 0.002806029913616715\n",
      "Iteration: 940 Loss: 0.0027979137600286116\n",
      "Iteration: 941 Loss: 0.0027898386511607974\n",
      "Iteration: 942 Loss: 0.002781804347714578\n",
      "Iteration: 943 Loss: 0.0027738106118271864\n",
      "Iteration: 944 Loss: 0.002765857207063348\n",
      "Iteration: 945 Loss: 0.002757943898407864\n",
      "Iteration: 946 Loss: 0.002750070452255228\n",
      "Iteration: 947 Loss: 0.0027422366364028656\n",
      "Iteration: 948 Loss: 0.0027344422200415155\n",
      "Iteration: 949 Loss: 0.002726686973747791\n",
      "Iteration: 950 Loss: 0.0027189706694747085\n",
      "Iteration: 951 Loss: 0.002711293080544387\n",
      "Iteration: 952 Loss: 0.0027036539816395182\n",
      "Iteration: 953 Loss: 0.0026960531487956125\n",
      "Iteration: 954 Loss: 0.0026884903593912607\n",
      "Iteration: 955 Loss: 0.0026809653921428577\n",
      "Iteration: 956 Loss: 0.0026734780270936077\n",
      "Iteration: 957 Loss: 0.002666028045607077\n",
      "Iteration: 958 Loss: 0.0026586152303598783\n",
      "Iteration: 959 Loss: 0.0026512393653323916\n",
      "Iteration: 960 Loss: 0.0026439002358016484\n",
      "Iteration: 961 Loss: 0.002636597628333305\n",
      "Iteration: 962 Loss: 0.002629331330774535\n",
      "Iteration: 963 Loss: 0.0026221011322442346\n",
      "Iteration: 964 Loss: 0.0026149068231289788\n",
      "Iteration: 965 Loss: 0.0026077481950724686\n",
      "Iteration: 966 Loss: 0.0026006250409688564\n",
      "Iteration: 967 Loss: 0.0025935371549554494\n",
      "Iteration: 968 Loss: 0.0025864843324047513\n",
      "Iteration: 969 Loss: 0.0025794663699175414\n",
      "Iteration: 970 Loss: 0.0025724830653153915\n",
      "Iteration: 971 Loss: 0.0025655342176328522\n",
      "Iteration: 972 Loss: 0.002558619627111176\n",
      "Iteration: 973 Loss: 0.0025517390951901127\n",
      "Iteration: 974 Loss: 0.0025448924245008654\n",
      "Iteration: 975 Loss: 0.0025380794188591066\n",
      "Iteration: 976 Loss: 0.002531299883257979\n",
      "Iteration: 977 Loss: 0.0025245536238612002\n",
      "Iteration: 978 Loss: 0.002517840447995719\n",
      "Iteration: 979 Loss: 0.0025111601641441164\n",
      "Iteration: 980 Loss: 0.0025045125819389315\n",
      "Iteration: 981 Loss: 0.0024978975121549975\n",
      "Iteration: 982 Loss: 0.0024913147667022695\n",
      "Iteration: 983 Loss: 0.0024847641586196846\n",
      "Iteration: 984 Loss: 0.0024782455020683153\n",
      "Iteration: 985 Loss: 0.002471758612324464\n",
      "Iteration: 986 Loss: 0.002465303305772505\n",
      "Iteration: 987 Loss: 0.0024588793998995906\n",
      "Iteration: 988 Loss: 0.0024524867132862454\n",
      "Iteration: 989 Loss: 0.0024461250656037914\n",
      "Iteration: 990 Loss: 0.0024397942776043756\n",
      "Iteration: 991 Loss: 0.0024334941711160393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 992 Loss: 0.002427224569036083\n",
      "Iteration: 993 Loss: 0.0024209852953241753\n",
      "Iteration: 994 Loss: 0.002414776174996589\n",
      "Iteration: 995 Loss: 0.002408597034119622\n",
      "Iteration: 996 Loss: 0.002402447699803379\n",
      "Iteration: 997 Loss: 0.0023963280001951014\n",
      "Iteration: 998 Loss: 0.002390237764473156\n",
      "Iteration: 999 Loss: 0.0023841768228408757\n",
      "Iteration: 1000 Loss: 0.002378145006520704\n",
      "Iteration: 1001 Loss: 0.002372142147747674\n",
      "Iteration: 1002 Loss: 0.002366168079763287\n",
      "Iteration: 1003 Loss: 0.0023602226368099207\n",
      "Iteration: 1004 Loss: 0.002354305654124271\n",
      "Iteration: 1005 Loss: 0.002348416967931777\n",
      "Iteration: 1006 Loss: 0.0023425564154402794\n",
      "Iteration: 1007 Loss: 0.0023367238348340432\n",
      "Iteration: 1008 Loss: 0.002330919065269548\n",
      "Iteration: 1009 Loss: 0.0023251419468671296\n",
      "Iteration: 1010 Loss: 0.0023193923207061175\n",
      "Iteration: 1011 Loss: 0.002313670028820249\n",
      "Iteration: 1012 Loss: 0.0023079749141904416\n",
      "Iteration: 1013 Loss: 0.0023023068207401283\n",
      "Iteration: 1014 Loss: 0.00229666559332846\n",
      "Iteration: 1015 Loss: 0.002291051077746199\n",
      "Iteration: 1016 Loss: 0.0022854631207084608\n",
      "Iteration: 1017 Loss: 0.0022799015698501263\n",
      "Iteration: 1018 Loss: 0.002274366273720635\n",
      "Iteration: 1019 Loss: 0.0022688570817773856\n",
      "Iteration: 1020 Loss: 0.002263373844381531\n",
      "Iteration: 1021 Loss: 0.002257916412791976\n",
      "Iteration: 1022 Loss: 0.0022524846391587406\n",
      "Iteration: 1023 Loss: 0.0022470783765202633\n",
      "Iteration: 1024 Loss: 0.0022416974787960215\n",
      "Iteration: 1025 Loss: 0.002236341800782077\n",
      "Iteration: 1026 Loss: 0.0022310111981445387\n",
      "Iteration: 1027 Loss: 0.0022257055274166795\n",
      "Iteration: 1028 Loss: 0.002220424645991511\n",
      "Iteration: 1029 Loss: 0.0022151684121179565\n",
      "Iteration: 1030 Loss: 0.0022099366848945972\n",
      "Iteration: 1031 Loss: 0.00220472932426576\n",
      "Iteration: 1032 Loss: 0.002199546191015994\n",
      "Iteration: 1033 Loss: 0.002194387146764265\n",
      "Iteration: 1034 Loss: 0.002189252053960475\n",
      "Iteration: 1035 Loss: 0.0021841407758785886\n",
      "Iteration: 1036 Loss: 0.0021790531766136797\n",
      "Iteration: 1037 Loss: 0.00217398912107496\n",
      "Iteration: 1038 Loss: 0.002168948474982363\n",
      "Iteration: 1039 Loss: 0.002163931104861712\n",
      "Iteration: 1040 Loss: 0.002158936878038601\n",
      "Iteration: 1041 Loss: 0.002153965662634384\n",
      "Iteration: 1042 Loss: 0.002149017327561836\n",
      "Iteration: 1043 Loss: 0.0021440917425197517\n",
      "Iteration: 1044 Loss: 0.0021391887779883705\n",
      "Iteration: 1045 Loss: 0.0021343083052246014\n",
      "Iteration: 1046 Loss: 0.0021294501962573634\n",
      "Iteration: 1047 Loss: 0.0021246143238834825\n",
      "Iteration: 1048 Loss: 0.002119800561661947\n",
      "Iteration: 1049 Loss: 0.002115008783910612\n",
      "Iteration: 1050 Loss: 0.0021102388657009\n",
      "Iteration: 1051 Loss: 0.0021054906828528877\n",
      "Iteration: 1052 Loss: 0.0021007641119315966\n",
      "Iteration: 1053 Loss: 0.0020960590302421473\n",
      "Iteration: 1054 Loss: 0.0020913753158256043\n",
      "Iteration: 1055 Loss: 0.0020867128474543546\n",
      "Iteration: 1056 Loss: 0.002082071504627248\n",
      "Iteration: 1057 Loss: 0.0020774511675659573\n",
      "Iteration: 1058 Loss: 0.002072851717210298\n",
      "Iteration: 1059 Loss: 0.0020682730352135155\n",
      "Iteration: 1060 Loss: 0.0020637150039390308\n",
      "Iteration: 1061 Loss: 0.002059177506454931\n",
      "Iteration: 1062 Loss: 0.002054660426530888\n",
      "Iteration: 1063 Loss: 0.0020501636486329957\n",
      "Iteration: 1064 Loss: 0.00204568705792059\n",
      "Iteration: 1065 Loss: 0.0020412305402399966\n",
      "Iteration: 1066 Loss: 0.002036793982123486\n",
      "Iteration: 1067 Loss: 0.0020323772707823502\n",
      "Iteration: 1068 Loss: 0.002027980294104613\n",
      "Iteration: 1069 Loss: 0.002023602940650029\n",
      "Iteration: 1070 Loss: 0.0020192450996462026\n",
      "Iteration: 1071 Loss: 0.002014906660985018\n",
      "Iteration: 1072 Loss: 0.0020105875152179966\n",
      "Iteration: 1073 Loss: 0.0020062875535528527\n",
      "Iteration: 1074 Loss: 0.002002006667849024\n",
      "Iteration: 1075 Loss: 0.0019977447506138854\n",
      "Iteration: 1076 Loss: 0.0019935016949994876\n",
      "Iteration: 1077 Loss: 0.0019892773947979255\n",
      "Iteration: 1078 Loss: 0.0019850717444374302\n",
      "Iteration: 1079 Loss: 0.00198088463897982\n",
      "Iteration: 1080 Loss: 0.001976715974114521\n",
      "Iteration: 1081 Loss: 0.001972565646156329\n",
      "Iteration: 1082 Loss: 0.001968433552041198\n",
      "Iteration: 1083 Loss: 0.001964319589322767\n",
      "Iteration: 1084 Loss: 0.001960223656168545\n",
      "Iteration: 1085 Loss: 0.0019561456513558194\n",
      "Iteration: 1086 Loss: 0.0019520854742681643\n",
      "Iteration: 1087 Loss: 0.0019480430248920216\n",
      "Iteration: 1088 Loss: 0.001944018203812559\n",
      "Iteration: 1089 Loss: 0.001940010912210857\n",
      "Iteration: 1090 Loss: 0.0019360210518596208\n",
      "Iteration: 1091 Loss: 0.00193204852511974\n",
      "Iteration: 1092 Loss: 0.0019280932349366184\n",
      "Iteration: 1093 Loss: 0.001924155084837851\n",
      "Iteration: 1094 Loss: 0.0019202339789274679\n",
      "Iteration: 1095 Loss: 0.0019163298218845622\n",
      "Iteration: 1096 Loss: 0.0019124425189580498\n",
      "Iteration: 1097 Loss: 0.0019085719759656217\n",
      "Iteration: 1098 Loss: 0.0019047180992874525\n",
      "Iteration: 1099 Loss: 0.0019008807958648586\n",
      "Iteration: 1100 Loss: 0.0018970599731955532\n",
      "Iteration: 1101 Loss: 0.0018932555393311283\n",
      "Iteration: 1102 Loss: 0.0018894674028738544\n",
      "Iteration: 1103 Loss: 0.001885695472972779\n",
      "Iteration: 1104 Loss: 0.0018819396593208518\n",
      "Iteration: 1105 Loss: 0.0018781998721511424\n",
      "Iteration: 1106 Loss: 0.0018744760222341873\n",
      "Iteration: 1107 Loss: 0.0018707680208738134\n",
      "Iteration: 1108 Loss: 0.0018670757799046894\n",
      "Iteration: 1109 Loss: 0.0018633992116893516\n",
      "Iteration: 1110 Loss: 0.0018597382291140577\n",
      "Iteration: 1111 Loss: 0.001856092745586744\n",
      "Iteration: 1112 Loss: 0.0018524626750322341\n",
      "Iteration: 1113 Loss: 0.0018488479318913198\n",
      "Iteration: 1114 Loss: 0.001845248431115247\n",
      "Iteration: 1115 Loss: 0.0018416640881647021\n",
      "Iteration: 1116 Loss: 0.001838094819005003\n",
      "Iteration: 1117 Loss: 0.0018345405401048564\n",
      "Iteration: 1118 Loss: 0.0018310011684310845\n",
      "Iteration: 1119 Loss: 0.0018274766214478268\n",
      "Iteration: 1120 Loss: 0.0018239668171120623\n",
      "Iteration: 1121 Loss: 0.001820471673871807\n",
      "Iteration: 1122 Loss: 0.0018169911106608921\n",
      "Iteration: 1123 Loss: 0.0018135250468983212\n",
      "Iteration: 1124 Loss: 0.0018100734024847959\n",
      "Iteration: 1125 Loss: 0.0018066360977991725\n",
      "Iteration: 1126 Loss: 0.0018032130536958424\n",
      "Iteration: 1127 Loss: 0.0017998041915019782\n",
      "Iteration: 1128 Loss: 0.0017964094330146306\n",
      "Iteration: 1129 Loss: 0.0017930287004973402\n",
      "Iteration: 1130 Loss: 0.0017896619166783384\n",
      "Iteration: 1131 Loss: 0.0017863090047464242\n",
      "Iteration: 1132 Loss: 0.0017829698883496295\n",
      "Iteration: 1133 Loss: 0.0017796444915913273\n",
      "Iteration: 1134 Loss: 0.0017763327390271277\n",
      "Iteration: 1135 Loss: 0.0017730345556638042\n",
      "Iteration: 1136 Loss: 0.0017697498669552\n",
      "Iteration: 1137 Loss: 0.0017664785987993013\n",
      "Iteration: 1138 Loss: 0.0017632206775362847\n",
      "Iteration: 1139 Loss: 0.0017599760299456067\n",
      "Iteration: 1140 Loss: 0.0017567445832434422\n",
      "Iteration: 1141 Loss: 0.0017535262650801727\n",
      "Iteration: 1142 Loss: 0.0017503210035366154\n",
      "Iteration: 1143 Loss: 0.0017471287271220914\n",
      "Iteration: 1144 Loss: 0.0017439493647724467\n",
      "Iteration: 1145 Loss: 0.001740782845846762\n",
      "Iteration: 1146 Loss: 0.001737629100124778\n",
      "Iteration: 1147 Loss: 0.001734488057804338\n",
      "Iteration: 1148 Loss: 0.0017313596494992862\n",
      "Iteration: 1149 Loss: 0.0017282438062360714\n",
      "Iteration: 1150 Loss: 0.0017251404594519977\n",
      "Iteration: 1151 Loss: 0.0017220495409923458\n",
      "Iteration: 1152 Loss: 0.0017189709831075834\n",
      "Iteration: 1153 Loss: 0.0017159047184517027\n",
      "Iteration: 1154 Loss: 0.0017128506800791379\n",
      "Iteration: 1155 Loss: 0.0017098088014419765\n",
      "Iteration: 1156 Loss: 0.0017067790163885783\n",
      "Iteration: 1157 Loss: 0.0017037612591596365\n",
      "Iteration: 1158 Loss: 0.0017007554643875297\n",
      "Iteration: 1159 Loss: 0.0016977615670928196\n",
      "Iteration: 1160 Loss: 0.0016947795026814845\n",
      "Iteration: 1161 Loss: 0.0016918092069438152\n",
      "Iteration: 1162 Loss: 0.001688850616050975\n",
      "Iteration: 1163 Loss: 0.0016859036665527248\n",
      "Iteration: 1164 Loss: 0.0016829682953760823\n",
      "Iteration: 1165 Loss: 0.0016800444398218561\n",
      "Iteration: 1166 Loss: 0.0016771320375627654\n",
      "Iteration: 1167 Loss: 0.0016742310266411923\n",
      "Iteration: 1168 Loss: 0.0016713413454670363\n",
      "Iteration: 1169 Loss: 0.001668462932814768\n",
      "Iteration: 1170 Loss: 0.0016655957278219646\n",
      "Iteration: 1171 Loss: 0.001662739669987011\n",
      "Iteration: 1172 Loss: 0.0016598946991658436\n",
      "Iteration: 1173 Loss: 0.0016570607555711741\n",
      "Iteration: 1174 Loss: 0.001654237779769318\n",
      "Iteration: 1175 Loss: 0.0016514257126786095\n",
      "Iteration: 1176 Loss: 0.0016486244955660617\n",
      "Iteration: 1177 Loss: 0.0016458340700467205\n",
      "Iteration: 1178 Loss: 0.0016430543780804028\n",
      "Iteration: 1179 Loss: 0.0016402853619702908\n",
      "Iteration: 1180 Loss: 0.0016375269643604357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1181 Loss: 0.0016347791282332975\n",
      "Iteration: 1182 Loss: 0.0016320417969077803\n",
      "Iteration: 1183 Loss: 0.0016293149140381245\n",
      "Iteration: 1184 Loss: 0.0016265984236103906\n",
      "Iteration: 1185 Loss: 0.0016238922699412515\n",
      "Iteration: 1186 Loss: 0.0016211963976755272\n",
      "Iteration: 1187 Loss: 0.0016185107517841558\n",
      "Iteration: 1188 Loss: 0.0016158352775626442\n",
      "Iteration: 1189 Loss: 0.0016131699206287974\n",
      "Iteration: 1190 Loss: 0.001610514626920055\n",
      "Iteration: 1191 Loss: 0.0016078693426923665\n",
      "Iteration: 1192 Loss: 0.001605234014517795\n",
      "Iteration: 1193 Loss: 0.0016026085892822549\n",
      "Iteration: 1194 Loss: 0.0015999930141841123\n",
      "Iteration: 1195 Loss: 0.001597387236732454\n",
      "Iteration: 1196 Loss: 0.001594791204743721\n",
      "Iteration: 1197 Loss: 0.0015922048663410285\n",
      "Iteration: 1198 Loss: 0.0015896281699523441\n",
      "Iteration: 1199 Loss: 0.0015870610643076522\n",
      "Iteration: 1200 Loss: 0.0015845034984373952\n",
      "Iteration: 1201 Loss: 0.0015819554216713527\n",
      "Iteration: 1202 Loss: 0.0015794167836353952\n",
      "Iteration: 1203 Loss: 0.001576887534250443\n",
      "Iteration: 1204 Loss: 0.00157436762373117\n",
      "Iteration: 1205 Loss: 0.0015718570025825683\n",
      "Iteration: 1206 Loss: 0.001569355621599385\n",
      "Iteration: 1207 Loss: 0.001566863431863907\n",
      "Iteration: 1208 Loss: 0.00156438038474397\n",
      "Iteration: 1209 Loss: 0.0015619064318909495\n",
      "Iteration: 1210 Loss: 0.0015594415252391128\n",
      "Iteration: 1211 Loss: 0.0015569856170024698\n",
      "Iteration: 1212 Loss: 0.0015545386596737612\n",
      "Iteration: 1213 Loss: 0.0015521006060220041\n",
      "Iteration: 1214 Loss: 0.0015496714090917298\n",
      "Iteration: 1215 Loss: 0.0015472510222003554\n",
      "Iteration: 1216 Loss: 0.0015448393989369229\n",
      "Iteration: 1217 Loss: 0.0015424364931602207\n",
      "Iteration: 1218 Loss: 0.0015400422589970755\n",
      "Iteration: 1219 Loss: 0.001537656650839756\n",
      "Iteration: 1220 Loss: 0.0015352796233463614\n",
      "Iteration: 1221 Loss: 0.001532911131437371\n",
      "Iteration: 1222 Loss: 0.001530551130294411\n",
      "Iteration: 1223 Loss: 0.0015281995753585126\n",
      "Iteration: 1224 Loss: 0.0015258564223289085\n",
      "Iteration: 1225 Loss: 0.0015235216271606048\n",
      "Iteration: 1226 Loss: 0.0015211951460638237\n",
      "Iteration: 1227 Loss: 0.001518876935500795\n",
      "Iteration: 1228 Loss: 0.001516566952186067\n",
      "Iteration: 1229 Loss: 0.0015142651530831005\n",
      "Iteration: 1230 Loss: 0.0015119714954036612\n",
      "Iteration: 1231 Loss: 0.001509685936606064\n",
      "Iteration: 1232 Loss: 0.0015074084343932967\n",
      "Iteration: 1233 Loss: 0.0015051389467120298\n",
      "Iteration: 1234 Loss: 0.0015028774317502609\n",
      "Iteration: 1235 Loss: 0.0015006238479359272\n",
      "Iteration: 1236 Loss: 0.0014983781539362067\n",
      "Iteration: 1237 Loss: 0.0014961403086550277\n",
      "Iteration: 1238 Loss: 0.0014939102712317504\n",
      "Iteration: 1239 Loss: 0.001491688001039942\n",
      "Iteration: 1240 Loss: 0.0014894734576852624\n",
      "Iteration: 1241 Loss: 0.00148726660100478\n",
      "Iteration: 1242 Loss: 0.001485067391064716\n",
      "Iteration: 1243 Loss: 0.0014828757881596873\n",
      "Iteration: 1244 Loss: 0.001480691752810266\n",
      "Iteration: 1245 Loss: 0.0014785152457619766\n",
      "Iteration: 1246 Loss: 0.0014763462279848585\n",
      "Iteration: 1247 Loss: 0.0014741846606698508\n",
      "Iteration: 1248 Loss: 0.0014720305052289197\n",
      "Iteration: 1249 Loss: 0.00146988372329361\n",
      "Iteration: 1250 Loss: 0.0014677442767124997\n",
      "Iteration: 1251 Loss: 0.0014656121275513623\n",
      "Iteration: 1252 Loss: 0.0014634872380905712\n",
      "Iteration: 1253 Loss: 0.001461369570823523\n",
      "Iteration: 1254 Loss: 0.0014592590884566804\n",
      "Iteration: 1255 Loss: 0.001457155753906615\n",
      "Iteration: 1256 Loss: 0.0014550595302992443\n",
      "Iteration: 1257 Loss: 0.0014529703809690105\n",
      "Iteration: 1258 Loss: 0.0014508882694562381\n",
      "Iteration: 1259 Loss: 0.0014488131595071306\n",
      "Iteration: 1260 Loss: 0.0014467450150712658\n",
      "Iteration: 1261 Loss: 0.0014446838003013697\n",
      "Iteration: 1262 Loss: 0.001442629479550904\n",
      "Iteration: 1263 Loss: 0.0014405820173737447\n",
      "Iteration: 1264 Loss: 0.0014385413785223562\n",
      "Iteration: 1265 Loss: 0.0014365075279463703\n",
      "Iteration: 1266 Loss: 0.0014344804307913402\n",
      "Iteration: 1267 Loss: 0.0014324600523973412\n",
      "Iteration: 1268 Loss: 0.0014304463582987311\n",
      "Iteration: 1269 Loss: 0.0014284393142210592\n",
      "Iteration: 1270 Loss: 0.0014264388860816934\n",
      "Iteration: 1271 Loss: 0.0014244450399871222\n",
      "Iteration: 1272 Loss: 0.001422457742232158\n",
      "Iteration: 1273 Loss: 0.0014204769592995397\n",
      "Iteration: 1274 Loss: 0.0014185026578570285\n",
      "Iteration: 1275 Loss: 0.0014165348047574676\n",
      "Iteration: 1276 Loss: 0.0014145733670373807\n",
      "Iteration: 1277 Loss: 0.001412618311915492\n",
      "Iteration: 1278 Loss: 0.001410669606791457\n",
      "Iteration: 1279 Loss: 0.0014087272192453476\n",
      "Iteration: 1280 Loss: 0.001406791117035787\n",
      "Iteration: 1281 Loss: 0.0014048612680982587\n",
      "Iteration: 1282 Loss: 0.0014029376405451089\n",
      "Iteration: 1283 Loss: 0.0014010202026644347\n",
      "Iteration: 1284 Loss: 0.0013991089229174404\n",
      "Iteration: 1285 Loss: 0.0013972037699392123\n",
      "Iteration: 1286 Loss: 0.0013953047125359345\n",
      "Iteration: 1287 Loss: 0.0013934117196847404\n",
      "Iteration: 1288 Loss: 0.0013915247605327816\n",
      "Iteration: 1289 Loss: 0.0013896438043945539\n",
      "Iteration: 1290 Loss: 0.0013877688207527068\n",
      "Iteration: 1291 Loss: 0.001385899779256313\n",
      "Iteration: 1292 Loss: 0.0013840366497193015\n",
      "Iteration: 1293 Loss: 0.0013821794021189667\n",
      "Iteration: 1294 Loss: 0.0013803280065967994\n",
      "Iteration: 1295 Loss: 0.00137848243345524\n",
      "Iteration: 1296 Loss: 0.0013766426531583917\n",
      "Iteration: 1297 Loss: 0.00137480863632986\n",
      "Iteration: 1298 Loss: 0.0013729803537516527\n",
      "Iteration: 1299 Loss: 0.001371157776364642\n",
      "Iteration: 1300 Loss: 0.0013693408752651168\n",
      "Iteration: 1301 Loss: 0.0013675296217060139\n",
      "Iteration: 1302 Loss: 0.0013657239870945661\n",
      "Iteration: 1303 Loss: 0.0013639239429917237\n",
      "Iteration: 1304 Loss: 0.0013621294611108235\n",
      "Iteration: 1305 Loss: 0.001360340513317668\n",
      "Iteration: 1306 Loss: 0.0013585570716278998\n",
      "Iteration: 1307 Loss: 0.0013567791082078281\n",
      "Iteration: 1308 Loss: 0.00135500659537163\n",
      "Iteration: 1309 Loss: 0.0013532395055826597\n",
      "Iteration: 1310 Loss: 0.0013514778114496009\n",
      "Iteration: 1311 Loss: 0.0013497214857283695\n",
      "Iteration: 1312 Loss: 0.0013479705013190923\n",
      "Iteration: 1313 Loss: 0.0013462248312661907\n",
      "Iteration: 1314 Loss: 0.0013444844487572416\n",
      "Iteration: 1315 Loss: 0.0013427493271221164\n",
      "Iteration: 1316 Loss: 0.0013410194398321136\n",
      "Iteration: 1317 Loss: 0.0013392947604985216\n",
      "Iteration: 1318 Loss: 0.0013375752628728703\n",
      "Iteration: 1319 Loss: 0.0013358609208447253\n",
      "Iteration: 1320 Loss: 0.0013341517084420537\n",
      "Iteration: 1321 Loss: 0.0013324475998294522\n",
      "Iteration: 1322 Loss: 0.0013307485693072492\n",
      "Iteration: 1323 Loss: 0.0013290545913113155\n",
      "Iteration: 1324 Loss: 0.0013273656404118239\n",
      "Iteration: 1325 Loss: 0.0013256816913127\n",
      "Iteration: 1326 Loss: 0.0013240027188496945\n",
      "Iteration: 1327 Loss: 0.0013223286979917642\n",
      "Iteration: 1328 Loss: 0.0013206596038379517\n",
      "Iteration: 1329 Loss: 0.0013189954116177116\n",
      "Iteration: 1330 Loss: 0.0013173360966901382\n",
      "Iteration: 1331 Loss: 0.0013156816345426226\n",
      "Iteration: 1332 Loss: 0.0013140320007905002\n",
      "Iteration: 1333 Loss: 0.001312387171176351\n",
      "Iteration: 1334 Loss: 0.001310747121568848\n",
      "Iteration: 1335 Loss: 0.0013091118279622176\n",
      "Iteration: 1336 Loss: 0.0013074812664752545\n",
      "Iteration: 1337 Loss: 0.0013058554133511173\n",
      "Iteration: 1338 Loss: 0.0013042342449562621\n",
      "Iteration: 1339 Loss: 0.0013026177377788344\n",
      "Iteration: 1340 Loss: 0.0013010058684294592\n",
      "Iteration: 1341 Loss: 0.001299398613639848\n",
      "Iteration: 1342 Loss: 0.0012977959502616142\n",
      "Iteration: 1343 Loss: 0.001296197855265729\n",
      "Iteration: 1344 Loss: 0.00129460430574281\n",
      "Iteration: 1345 Loss: 0.0012930152789013783\n",
      "Iteration: 1346 Loss: 0.0012914307520671406\n",
      "Iteration: 1347 Loss: 0.0012898507026828927\n",
      "Iteration: 1348 Loss: 0.0012882751083072351\n",
      "Iteration: 1349 Loss: 0.001286703946614854\n",
      "Iteration: 1350 Loss: 0.001285137195394438\n",
      "Iteration: 1351 Loss: 0.001283574832549105\n",
      "Iteration: 1352 Loss: 0.0012820168360952924\n",
      "Iteration: 1353 Loss: 0.0012804631841623105\n",
      "Iteration: 1354 Loss: 0.0012789138549917137\n",
      "Iteration: 1355 Loss: 0.001277368826936394\n",
      "Iteration: 1356 Loss: 0.0012758280784601623\n",
      "Iteration: 1357 Loss: 0.0012742915881369143\n",
      "Iteration: 1358 Loss: 0.0012727593346509576\n",
      "Iteration: 1359 Loss: 0.0012712312967946972\n",
      "Iteration: 1360 Loss: 0.001269707453469066\n",
      "Iteration: 1361 Loss: 0.0012681877836829922\n",
      "Iteration: 1362 Loss: 0.0012666722665526998\n",
      "Iteration: 1363 Loss: 0.001265160881300629\n",
      "Iteration: 1364 Loss: 0.0012636536072556607\n",
      "Iteration: 1365 Loss: 0.001262150423851947\n",
      "Iteration: 1366 Loss: 0.0012606513106285704\n",
      "Iteration: 1367 Loss: 0.001259156247228796\n",
      "Iteration: 1368 Loss: 0.001257665213400012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1369 Loss: 0.0012561781889920333\n",
      "Iteration: 1370 Loss: 0.0012546951539576295\n",
      "Iteration: 1371 Loss: 0.0012532160883520567\n",
      "Iteration: 1372 Loss: 0.0012517409723315131\n",
      "Iteration: 1373 Loss: 0.0012502697861532612\n",
      "Iteration: 1374 Loss: 0.0012488025101750854\n",
      "Iteration: 1375 Loss: 0.0012473391248555997\n",
      "Iteration: 1376 Loss: 0.001245879610751164\n",
      "Iteration: 1377 Loss: 0.0012444239485185004\n",
      "Iteration: 1378 Loss: 0.001242972118911992\n",
      "Iteration: 1379 Loss: 0.001241524102783979\n",
      "Iteration: 1380 Loss: 0.0012400798810843008\n",
      "Iteration: 1381 Loss: 0.0012386394348594943\n",
      "Iteration: 1382 Loss: 0.0012372027452524953\n",
      "Iteration: 1383 Loss: 0.0012357697935025003\n",
      "Iteration: 1384 Loss: 0.0012343405609438452\n",
      "Iteration: 1385 Loss: 0.0012329150290057826\n",
      "Iteration: 1386 Loss: 0.0012314931792119178\n",
      "Iteration: 1387 Loss: 0.001230074993180022\n",
      "Iteration: 1388 Loss: 0.0012286604526216072\n",
      "Iteration: 1389 Loss: 0.0012272495393408336\n",
      "Iteration: 1390 Loss: 0.0012258422352347728\n",
      "Iteration: 1391 Loss: 0.0012244385222923962\n",
      "Iteration: 1392 Loss: 0.0012230383825948415\n",
      "Iteration: 1393 Loss: 0.0012216417983141193\n",
      "Iteration: 1394 Loss: 0.0012202487517129802\n",
      "Iteration: 1395 Loss: 0.0012188592251451051\n",
      "Iteration: 1396 Loss: 0.0012174732010533495\n",
      "Iteration: 1397 Loss: 0.0012160906619709461\n",
      "Iteration: 1398 Loss: 0.0012147115905196096\n",
      "Iteration: 1399 Loss: 0.0012133359694096095\n",
      "Iteration: 1400 Loss: 0.0012119637814405923\n",
      "Iteration: 1401 Loss: 0.0012105950094987118\n",
      "Iteration: 1402 Loss: 0.0012092296365584045\n",
      "Iteration: 1403 Loss: 0.0012078676456805902\n",
      "Iteration: 1404 Loss: 0.0012065090200133145\n",
      "Iteration: 1405 Loss: 0.001205153742790329\n",
      "Iteration: 1406 Loss: 0.0012038017973322241\n",
      "Iteration: 1407 Loss: 0.0012024531670435813\n",
      "Iteration: 1408 Loss: 0.0012011078354150248\n",
      "Iteration: 1409 Loss: 0.0011997657860219728\n",
      "Iteration: 1410 Loss: 0.001198427002523398\n",
      "Iteration: 1411 Loss: 0.0011970914686625647\n",
      "Iteration: 1412 Loss: 0.0011957591682661653\n",
      "Iteration: 1413 Loss: 0.001194430085244335\n",
      "Iteration: 1414 Loss: 0.001193104203589699\n",
      "Iteration: 1415 Loss: 0.0011917815073771174\n",
      "Iteration: 1416 Loss: 0.0011904619807637292\n",
      "Iteration: 1417 Loss: 0.0011891456079888237\n",
      "Iteration: 1418 Loss: 0.0011878323733719538\n",
      "Iteration: 1419 Loss: 0.0011865222613142558\n",
      "Iteration: 1420 Loss: 0.0011852152562971628\n",
      "Iteration: 1421 Loss: 0.0011839113428826172\n",
      "Iteration: 1422 Loss: 0.0011826105057124111\n",
      "Iteration: 1423 Loss: 0.0011813127295077356\n",
      "Iteration: 1424 Loss: 0.001180017999069098\n",
      "Iteration: 1425 Loss: 0.0011787262992759757\n",
      "Iteration: 1426 Loss: 0.0011774376150857225\n",
      "Iteration: 1427 Loss: 0.0011761519315346893\n",
      "Iteration: 1428 Loss: 0.0011748692337364957\n",
      "Iteration: 1429 Loss: 0.0011735895068823946\n",
      "Iteration: 1430 Loss: 0.00117231273624063\n",
      "Iteration: 1431 Loss: 0.0011710389071568673\n",
      "Iteration: 1432 Loss: 0.0011697680050522365\n",
      "Iteration: 1433 Loss: 0.0011685000154246004\n",
      "Iteration: 1434 Loss: 0.0011672349238477549\n",
      "Iteration: 1435 Loss: 0.001165972715970809\n",
      "Iteration: 1436 Loss: 0.0011647133775182482\n",
      "Iteration: 1437 Loss: 0.0011634568942892618\n",
      "Iteration: 1438 Loss: 0.001162203252157363\n",
      "Iteration: 1439 Loss: 0.0011609524370704656\n",
      "Iteration: 1440 Loss: 0.0011597044350506501\n",
      "Iteration: 1441 Loss: 0.0011584592321931479\n",
      "Iteration: 1442 Loss: 0.0011572168146669968\n",
      "Iteration: 1443 Loss: 0.0011559771687133093\n",
      "Iteration: 1444 Loss: 0.001154740280646601\n",
      "Iteration: 1445 Loss: 0.0011535061368534594\n",
      "Iteration: 1446 Loss: 0.0011522747237924333\n",
      "Iteration: 1447 Loss: 0.0011510460279940487\n",
      "Iteration: 1448 Loss: 0.001149820036059857\n",
      "Iteration: 1449 Loss: 0.0011485967346627423\n",
      "Iteration: 1450 Loss: 0.0011473761105466101\n",
      "Iteration: 1451 Loss: 0.0011461581505257728\n",
      "Iteration: 1452 Loss: 0.0011449428414840907\n",
      "Iteration: 1453 Loss: 0.0011437301703763599\n",
      "Iteration: 1454 Loss: 0.0011425201242258882\n",
      "Iteration: 1455 Loss: 0.0011413126901263578\n",
      "Iteration: 1456 Loss: 0.001140107855239919\n",
      "Iteration: 1457 Loss: 0.0011389056067973202\n",
      "Iteration: 1458 Loss: 0.0011377059320978402\n",
      "Iteration: 1459 Loss: 0.0011365088185090473\n",
      "Iteration: 1460 Loss: 0.0011353142534660228\n",
      "Iteration: 1461 Loss: 0.0011341222244718857\n",
      "Iteration: 1462 Loss: 0.001132932719096774\n",
      "Iteration: 1463 Loss: 0.0011317457249774895\n",
      "Iteration: 1464 Loss: 0.00113056122981794\n",
      "Iteration: 1465 Loss: 0.001129379221388224\n",
      "Iteration: 1466 Loss: 0.001128199687524564\n",
      "Iteration: 1467 Loss: 0.0011270226161294358\n",
      "Iteration: 1468 Loss: 0.001125847995169899\n",
      "Iteration: 1469 Loss: 0.0011246758126794256\n",
      "Iteration: 1470 Loss: 0.00112350605675545\n",
      "Iteration: 1471 Loss: 0.0011223387155608687\n",
      "Iteration: 1472 Loss: 0.0011211737773221167\n",
      "Iteration: 1473 Loss: 0.001120011230330582\n",
      "Iteration: 1474 Loss: 0.0011188510629409707\n",
      "Iteration: 1475 Loss: 0.0011176932635721265\n",
      "Iteration: 1476 Loss: 0.001116537820706043\n",
      "Iteration: 1477 Loss: 0.0011153847228872668\n",
      "Iteration: 1478 Loss: 0.0011142339587232033\n",
      "Iteration: 1479 Loss: 0.0011130855168842162\n",
      "Iteration: 1480 Loss: 0.0011119393861021374\n",
      "Iteration: 1481 Loss: 0.001110795555171959\n",
      "Iteration: 1482 Loss: 0.0011096540129489098\n",
      "Iteration: 1483 Loss: 0.0011085147483504588\n",
      "Iteration: 1484 Loss: 0.0011073777503548714\n",
      "Iteration: 1485 Loss: 0.0011062430080011627\n",
      "Iteration: 1486 Loss: 0.0011051105103898834\n",
      "Iteration: 1487 Loss: 0.0011039802466803082\n",
      "Iteration: 1488 Loss: 0.001102852206092802\n",
      "Iteration: 1489 Loss: 0.0011017263779078959\n",
      "Iteration: 1490 Loss: 0.0011006027514644677\n",
      "Iteration: 1491 Loss: 0.0010994813161615703\n",
      "Iteration: 1492 Loss: 0.0010983620614565491\n",
      "Iteration: 1493 Loss: 0.0010972449768662882\n",
      "Iteration: 1494 Loss: 0.001096130051965378\n",
      "Iteration: 1495 Loss: 0.0010950172763867182\n",
      "Iteration: 1496 Loss: 0.0010939066398219317\n",
      "Iteration: 1497 Loss: 0.0010927981320192947\n",
      "Iteration: 1498 Loss: 0.0010916917427853192\n",
      "Iteration: 1499 Loss: 0.0010905874619832796\n",
      "Iteration: 1500 Loss: 0.0010894852795334175\n",
      "Iteration: 1501 Loss: 0.001088385185412679\n",
      "Iteration: 1502 Loss: 0.0010872871696546006\n",
      "Iteration: 1503 Loss: 0.0010861912223484296\n",
      "Iteration: 1504 Loss: 0.001085097333639887\n",
      "Iteration: 1505 Loss: 0.0010840054937299507\n",
      "Iteration: 1506 Loss: 0.0010829156928747377\n",
      "Iteration: 1507 Loss: 0.001081827921386211\n",
      "Iteration: 1508 Loss: 0.0010807421696310668\n",
      "Iteration: 1509 Loss: 0.0010796584280301173\n",
      "Iteration: 1510 Loss: 0.0010785766870592271\n",
      "Iteration: 1511 Loss: 0.0010774969372483608\n",
      "Iteration: 1512 Loss: 0.0010764191691810649\n",
      "Iteration: 1513 Loss: 0.0010753433734945158\n",
      "Iteration: 1514 Loss: 0.001074269540879816\n",
      "Iteration: 1515 Loss: 0.0010731976620811123\n",
      "Iteration: 1516 Loss: 0.0010721277278952764\n",
      "Iteration: 1517 Loss: 0.0010710597291717684\n",
      "Iteration: 1518 Loss: 0.001069993656812845\n",
      "Iteration: 1519 Loss: 0.0010689295017727843\n",
      "Iteration: 1520 Loss: 0.0010678672550583708\n",
      "Iteration: 1521 Loss: 0.0010668069077271977\n",
      "Iteration: 1522 Loss: 0.001065748450889491\n",
      "Iteration: 1523 Loss: 0.0010646918757062401\n",
      "Iteration: 1524 Loss: 0.0010636371733891866\n",
      "Iteration: 1525 Loss: 0.001062584335201075\n",
      "Iteration: 1526 Loss: 0.0010615333524559255\n",
      "Iteration: 1527 Loss: 0.0010604842165172549\n",
      "Iteration: 1528 Loss: 0.0010594369187993328\n",
      "Iteration: 1529 Loss: 0.0010583914507660288\n",
      "Iteration: 1530 Loss: 0.0010573478039306269\n",
      "Iteration: 1531 Loss: 0.0010563059698566676\n",
      "Iteration: 1532 Loss: 0.0010552659401559757\n",
      "Iteration: 1533 Loss: 0.0010542277064903834\n",
      "Iteration: 1534 Loss: 0.0010531912605693129\n",
      "Iteration: 1535 Loss: 0.0010521565941517133\n",
      "Iteration: 1536 Loss: 0.001051123699044737\n",
      "Iteration: 1537 Loss: 0.0010500925671037525\n",
      "Iteration: 1538 Loss: 0.0010490631902315977\n",
      "Iteration: 1539 Loss: 0.0010480355603786143\n",
      "Iteration: 1540 Loss: 0.0010470096695438093\n",
      "Iteration: 1541 Loss: 0.0010459855097718906\n",
      "Iteration: 1542 Loss: 0.0010449630731558812\n",
      "Iteration: 1543 Loss: 0.0010439423518350277\n",
      "Iteration: 1544 Loss: 0.0010429233379948633\n",
      "Iteration: 1545 Loss: 0.0010419060238682093\n",
      "Iteration: 1546 Loss: 0.0010408904017334248\n",
      "Iteration: 1547 Loss: 0.0010398764639152488\n",
      "Iteration: 1548 Loss: 0.0010388642027836504\n",
      "Iteration: 1549 Loss: 0.0010378536107550581\n",
      "Iteration: 1550 Loss: 0.0010368446802899508\n",
      "Iteration: 1551 Loss: 0.00103583740389522\n",
      "Iteration: 1552 Loss: 0.0010348317741218185\n",
      "Iteration: 1553 Loss: 0.0010338277835663485\n",
      "Iteration: 1554 Loss: 0.0010328254248687142\n",
      "Iteration: 1555 Loss: 0.0010318246907143214\n",
      "Iteration: 1556 Loss: 0.0010308255738321562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1557 Loss: 0.001029828066995022\n",
      "Iteration: 1558 Loss: 0.001028832163019848\n",
      "Iteration: 1559 Loss: 0.001027837854766779\n",
      "Iteration: 1560 Loss: 0.0010268451351393402\n",
      "Iteration: 1561 Loss: 0.0010258539970843476\n",
      "Iteration: 1562 Loss: 0.001024864433591287\n",
      "Iteration: 1563 Loss: 0.001023876437692288\n",
      "Iteration: 1564 Loss: 0.0010228900024626681\n",
      "Iteration: 1565 Loss: 0.0010219051210191336\n",
      "Iteration: 1566 Loss: 0.001020921786521887\n",
      "Iteration: 1567 Loss: 0.0010199399921715794\n",
      "Iteration: 1568 Loss: 0.0010189597312118444\n",
      "Iteration: 1569 Loss: 0.0010179809969272972\n",
      "Iteration: 1570 Loss: 0.001017003782643869\n",
      "Iteration: 1571 Loss: 0.0010160280817291887\n",
      "Iteration: 1572 Loss: 0.00101505388759137\n",
      "Iteration: 1573 Loss: 0.0010140811936797049\n",
      "Iteration: 1574 Loss: 0.0010131099934834287\n",
      "Iteration: 1575 Loss: 0.0010121402805332717\n",
      "Iteration: 1576 Loss: 0.0010111720483995674\n",
      "Iteration: 1577 Loss: 0.0010102052906929213\n",
      "Iteration: 1578 Loss: 0.0010092400010635921\n",
      "Iteration: 1579 Loss: 0.0010082761732014794\n",
      "Iteration: 1580 Loss: 0.0010073138008366883\n",
      "Iteration: 1581 Loss: 0.0010063528777378772\n",
      "Iteration: 1582 Loss: 0.0010053933977133332\n",
      "Iteration: 1583 Loss: 0.001004435354610057\n",
      "Iteration: 1584 Loss: 0.001003478742313378\n",
      "Iteration: 1585 Loss: 0.001002523554748429\n",
      "Iteration: 1586 Loss: 0.0010015697858778278\n",
      "Iteration: 1587 Loss: 0.001000617429702855\n",
      "Iteration: 1588 Loss: 0.000999666480262262\n",
      "Iteration: 1589 Loss: 0.0009987169316334535\n",
      "Iteration: 1590 Loss: 0.0009977687779308957\n",
      "Iteration: 1591 Loss: 0.0009968220133071814\n",
      "Iteration: 1592 Loss: 0.000995876631951652\n",
      "Iteration: 1593 Loss: 0.0009949326280914864\n",
      "Iteration: 1594 Loss: 0.0009939899959901482\n",
      "Iteration: 1595 Loss: 0.000993048729948075\n",
      "Iteration: 1596 Loss: 0.0009921088243026728\n",
      "Iteration: 1597 Loss: 0.0009911702734277574\n",
      "Iteration: 1598 Loss: 0.0009902330717329103\n",
      "Iteration: 1599 Loss: 0.0009892972136646995\n",
      "Iteration: 1600 Loss: 0.0009883626937047434\n",
      "Iteration: 1601 Loss: 0.0009874295063707875\n",
      "Iteration: 1602 Loss: 0.0009864976462162\n",
      "Iteration: 1603 Loss: 0.000985567107829983\n",
      "Iteration: 1604 Loss: 0.0009846378858360493\n",
      "Iteration: 1605 Loss: 0.0009837099748933396\n",
      "Iteration: 1606 Loss: 0.000982783369695994\n",
      "Iteration: 1607 Loss: 0.0009818580649728571\n",
      "Iteration: 1608 Loss: 0.00098093405548704\n",
      "Iteration: 1609 Loss: 0.0009800113360356577\n",
      "Iteration: 1610 Loss: 0.0009790899014514027\n",
      "Iteration: 1611 Loss: 0.0009781697465994737\n",
      "Iteration: 1612 Loss: 0.000977250866379784\n",
      "Iteration: 1613 Loss: 0.0009763332557258503\n",
      "Iteration: 1614 Loss: 0.0009754169096043954\n",
      "Iteration: 1615 Loss: 0.0009745018230158523\n",
      "Iteration: 1616 Loss: 0.0009735879909935054\n",
      "Iteration: 1617 Loss: 0.0009726754086037152\n",
      "Iteration: 1618 Loss: 0.0009717640709461223\n",
      "Iteration: 1619 Loss: 0.0009708539731524722\n",
      "Iteration: 1620 Loss: 0.0009699451103874874\n",
      "Iteration: 1621 Loss: 0.000969037477848093\n",
      "Iteration: 1622 Loss: 0.0009681310707630807\n",
      "Iteration: 1623 Loss: 0.0009672258843936064\n",
      "Iteration: 1624 Loss: 0.0009663219140322756\n",
      "Iteration: 1625 Loss: 0.0009654191550039191\n",
      "Iteration: 1626 Loss: 0.0009645176026638505\n",
      "Iteration: 1627 Loss: 0.0009636172523997256\n",
      "Iteration: 1628 Loss: 0.0009627180996302677\n",
      "Iteration: 1629 Loss: 0.0009618201398045494\n",
      "Iteration: 1630 Loss: 0.0009609233684028437\n",
      "Iteration: 1631 Loss: 0.000960027780936424\n",
      "Iteration: 1632 Loss: 0.0009591333729463823\n",
      "Iteration: 1633 Loss: 0.0009582401400048499\n",
      "Iteration: 1634 Loss: 0.0009573480777134244\n",
      "Iteration: 1635 Loss: 0.0009564571817038779\n",
      "Iteration: 1636 Loss: 0.0009555674476380546\n",
      "Iteration: 1637 Loss: 0.0009546788712073682\n",
      "Iteration: 1638 Loss: 0.0009537914481331294\n",
      "Iteration: 1639 Loss: 0.0009529051741651406\n",
      "Iteration: 1640 Loss: 0.0009520200450826068\n",
      "Iteration: 1641 Loss: 0.0009511360566942584\n",
      "Iteration: 1642 Loss: 0.0009502532048371918\n",
      "Iteration: 1643 Loss: 0.0009493714853772152\n",
      "Iteration: 1644 Loss: 0.0009484908942088717\n",
      "Iteration: 1645 Loss: 0.0009476114272551509\n",
      "Iteration: 1646 Loss: 0.0009467330804664024\n",
      "Iteration: 1647 Loss: 0.0009458558498216548\n",
      "Iteration: 1648 Loss: 0.0009449797313281332\n",
      "Iteration: 1649 Loss: 0.0009441047210199735\n",
      "Iteration: 1650 Loss: 0.0009432308149589992\n",
      "Iteration: 1651 Loss: 0.0009423580092349062\n",
      "Iteration: 1652 Loss: 0.0009414862999641498\n",
      "Iteration: 1653 Loss: 0.0009406156832902566\n",
      "Iteration: 1654 Loss: 0.0009397461553837487\n",
      "Iteration: 1655 Loss: 0.0009388777124419488\n",
      "Iteration: 1656 Loss: 0.0009380103506884268\n",
      "Iteration: 1657 Loss: 0.0009371440663734204\n",
      "Iteration: 1658 Loss: 0.0009362788557733872\n",
      "Iteration: 1659 Loss: 0.0009354147151905346\n",
      "Iteration: 1660 Loss: 0.0009345516409532042\n",
      "Iteration: 1661 Loss: 0.00093368962941581\n",
      "Iteration: 1662 Loss: 0.0009328286769575357\n",
      "Iteration: 1663 Loss: 0.0009319687799836936\n",
      "Iteration: 1664 Loss: 0.000931109934924996\n",
      "Iteration: 1665 Loss: 0.0009302521382368291\n",
      "Iteration: 1666 Loss: 0.0009293953863993145\n",
      "Iteration: 1667 Loss: 0.0009285396759178466\n",
      "Iteration: 1668 Loss: 0.0009276850033226036\n",
      "Iteration: 1669 Loss: 0.000926831365167433\n",
      "Iteration: 1670 Loss: 0.0009259787580313825\n",
      "Iteration: 1671 Loss: 0.0009251271785171042\n",
      "Iteration: 1672 Loss: 0.0009242766232512395\n",
      "Iteration: 1673 Loss: 0.0009234270888845781\n",
      "Iteration: 1674 Loss: 0.0009225785720920063\n",
      "Iteration: 1675 Loss: 0.0009217310695710497\n",
      "Iteration: 1676 Loss: 0.0009208845780427319\n",
      "Iteration: 1677 Loss: 0.0009200390942520077\n",
      "Iteration: 1678 Loss: 0.0009191946149660579\n",
      "Iteration: 1679 Loss: 0.0009183511369756163\n",
      "Iteration: 1680 Loss: 0.0009175086570943908\n",
      "Iteration: 1681 Loss: 0.0009166671721576347\n",
      "Iteration: 1682 Loss: 0.0009158266790239577\n",
      "Iteration: 1683 Loss: 0.0009149871745738484\n",
      "Iteration: 1684 Loss: 0.0009141486557102096\n",
      "Iteration: 1685 Loss: 0.0009133111193581886\n",
      "Iteration: 1686 Loss: 0.0009124745624639844\n",
      "Iteration: 1687 Loss: 0.0009116389819960346\n",
      "Iteration: 1688 Loss: 0.0009108043749443073\n",
      "Iteration: 1689 Loss: 0.0009099707383200393\n",
      "Iteration: 1690 Loss: 0.0009091380691559625\n",
      "Iteration: 1691 Loss: 0.0009083063645053325\n",
      "Iteration: 1692 Loss: 0.0009074756214429505\n",
      "Iteration: 1693 Loss: 0.0009066458370641979\n",
      "Iteration: 1694 Loss: 0.0009058170084852371\n",
      "Iteration: 1695 Loss: 0.0009049891328423232\n",
      "Iteration: 1696 Loss: 0.0009041622072922653\n",
      "Iteration: 1697 Loss: 0.000903336229012522\n",
      "Iteration: 1698 Loss: 0.00090251119520026\n",
      "Iteration: 1699 Loss: 0.0009016871030725557\n",
      "Iteration: 1700 Loss: 0.0009008639498662975\n",
      "Iteration: 1701 Loss: 0.0009000417328376058\n",
      "Iteration: 1702 Loss: 0.0008992204492630805\n",
      "Iteration: 1703 Loss: 0.0008984000964379213\n",
      "Iteration: 1704 Loss: 0.0008975806716766035\n",
      "Iteration: 1705 Loss: 0.0008967621723125965\n",
      "Iteration: 1706 Loss: 0.0008959445956991352\n",
      "Iteration: 1707 Loss: 0.0008951279392068766\n",
      "Iteration: 1708 Loss: 0.0008943122002257017\n",
      "Iteration: 1709 Loss: 0.0008934973761644165\n",
      "Iteration: 1710 Loss: 0.0008926834644498884\n",
      "Iteration: 1711 Loss: 0.0008918704625272894\n",
      "Iteration: 1712 Loss: 0.0008910583678592782\n",
      "Iteration: 1713 Loss: 0.0008902471779267948\n",
      "Iteration: 1714 Loss: 0.0008894368902294122\n",
      "Iteration: 1715 Loss: 0.0008886275022834055\n",
      "Iteration: 1716 Loss: 0.0008878190116229441\n",
      "Iteration: 1717 Loss: 0.0008870114157997084\n",
      "Iteration: 1718 Loss: 0.0008862047123823853\n",
      "Iteration: 1719 Loss: 0.0008853988989567532\n",
      "Iteration: 1720 Loss: 0.0008845939731258526\n",
      "Iteration: 1721 Loss: 0.0008837899325092542\n",
      "Iteration: 1722 Loss: 0.0008829867747438113\n",
      "Iteration: 1723 Loss: 0.0008821844974827997\n",
      "Iteration: 1724 Loss: 0.0008813830983952572\n",
      "Iteration: 1725 Loss: 0.000880582575167571\n",
      "Iteration: 1726 Loss: 0.0008797829255021887\n",
      "Iteration: 1727 Loss: 0.0008789841471170162\n",
      "Iteration: 1728 Loss: 0.0008781862377465462\n",
      "Iteration: 1729 Loss: 0.0008773891951408886\n",
      "Iteration: 1730 Loss: 0.0008765930170657645\n",
      "Iteration: 1731 Loss: 0.0008757977013026441\n",
      "Iteration: 1732 Loss: 0.0008750032456482145\n",
      "Iteration: 1733 Loss: 0.0008742096479149636\n",
      "Iteration: 1734 Loss: 0.0008734169059304108\n",
      "Iteration: 1735 Loss: 0.0008726250175363872\n",
      "Iteration: 1736 Loss: 0.0008718339805911412\n",
      "Iteration: 1737 Loss: 0.0008710437929666921\n",
      "Iteration: 1738 Loss: 0.0008702544525498829\n",
      "Iteration: 1739 Loss: 0.000869465957242517\n",
      "Iteration: 1740 Loss: 0.0008686783049608275\n",
      "Iteration: 1741 Loss: 0.000867891493634911\n",
      "Iteration: 1742 Loss: 0.0008671055212099129\n",
      "Iteration: 1743 Loss: 0.0008663203856441534\n",
      "Iteration: 1744 Loss: 0.0008655360849109751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1745 Loss: 0.0008647526169976022\n",
      "Iteration: 1746 Loss: 0.0008639699799038255\n",
      "Iteration: 1747 Loss: 0.0008631881716445284\n",
      "Iteration: 1748 Loss: 0.0008624071902469791\n",
      "Iteration: 1749 Loss: 0.0008616270337525418\n",
      "Iteration: 1750 Loss: 0.0008608477002156977\n",
      "Iteration: 1751 Loss: 0.0008600691877044674\n",
      "Iteration: 1752 Loss: 0.0008592914942995469\n",
      "Iteration: 1753 Loss: 0.00085851461809505\n",
      "Iteration: 1754 Loss: 0.0008577385571974364\n",
      "Iteration: 1755 Loss: 0.0008569633097262966\n",
      "Iteration: 1756 Loss: 0.000856188873814205\n",
      "Iteration: 1757 Loss: 0.0008554152476056581\n",
      "Iteration: 1758 Loss: 0.0008546424292577954\n",
      "Iteration: 1759 Loss: 0.0008538704169405085\n",
      "Iteration: 1760 Loss: 0.0008530992088352241\n",
      "Iteration: 1761 Loss: 0.0008523288031362301\n",
      "Iteration: 1762 Loss: 0.0008515591980493083\n",
      "Iteration: 1763 Loss: 0.0008507903917924173\n",
      "Iteration: 1764 Loss: 0.000850022382595506\n",
      "Iteration: 1765 Loss: 0.000849255168699767\n",
      "Iteration: 1766 Loss: 0.0008484887483584567\n",
      "Iteration: 1767 Loss: 0.0008477231198360919\n",
      "Iteration: 1768 Loss: 0.000846958281408704\n",
      "Iteration: 1769 Loss: 0.0008461942313638781\n",
      "Iteration: 1770 Loss: 0.0008454309679998372\n",
      "Iteration: 1771 Loss: 0.0008446684896266204\n",
      "Iteration: 1772 Loss: 0.0008439067945650025\n",
      "Iteration: 1773 Loss: 0.0008431458811468943\n",
      "Iteration: 1774 Loss: 0.000842385747714672\n",
      "Iteration: 1775 Loss: 0.0008416263926215493\n",
      "Iteration: 1776 Loss: 0.0008408678142313721\n",
      "Iteration: 1777 Loss: 0.0008401100109192534\n",
      "Iteration: 1778 Loss: 0.0008393529810697758\n",
      "Iteration: 1779 Loss: 0.0008385967230780798\n",
      "Iteration: 1780 Loss: 0.0008378412353500061\n",
      "Iteration: 1781 Loss: 0.000837086516301426\n",
      "Iteration: 1782 Loss: 0.0008363325643583389\n",
      "Iteration: 1783 Loss: 0.0008355793779566854\n",
      "Iteration: 1784 Loss: 0.0008348269555420201\n",
      "Iteration: 1785 Loss: 0.0008340752955704173\n",
      "Iteration: 1786 Loss: 0.0008333243965074239\n",
      "Iteration: 1787 Loss: 0.000832574256828182\n",
      "Iteration: 1788 Loss: 0.0008318248750172192\n",
      "Iteration: 1789 Loss: 0.0008310762495684355\n",
      "Iteration: 1790 Loss: 0.0008303283789860358\n",
      "Iteration: 1791 Loss: 0.000829581261782669\n",
      "Iteration: 1792 Loss: 0.0008288348964807829\n",
      "Iteration: 1793 Loss: 0.0008280892816113877\n",
      "Iteration: 1794 Loss: 0.0008273444157149885\n",
      "Iteration: 1795 Loss: 0.0008266002973412022\n",
      "Iteration: 1796 Loss: 0.0008258569250481355\n",
      "Iteration: 1797 Loss: 0.0008251142974032773\n",
      "Iteration: 1798 Loss: 0.0008243724129824269\n",
      "Iteration: 1799 Loss: 0.0008236312703701255\n",
      "Iteration: 1800 Loss: 0.0008228908681595763\n",
      "Iteration: 1801 Loss: 0.000822151204952534\n",
      "Iteration: 1802 Loss: 0.0008214122793592051\n",
      "Iteration: 1803 Loss: 0.0008206740899981435\n",
      "Iteration: 1804 Loss: 0.0008199366354961857\n",
      "Iteration: 1805 Loss: 0.0008191999144884905\n",
      "Iteration: 1806 Loss: 0.0008184639256181309\n",
      "Iteration: 1807 Loss: 0.0008177286675363953\n",
      "Iteration: 1808 Loss: 0.000816994138902838\n",
      "Iteration: 1809 Loss: 0.0008162603383841171\n",
      "Iteration: 1810 Loss: 0.0008155272646557224\n",
      "Iteration: 1811 Loss: 0.0008147949164005196\n",
      "Iteration: 1812 Loss: 0.0008140632923093147\n",
      "Iteration: 1813 Loss: 0.0008133323910800012\n",
      "Iteration: 1814 Loss: 0.0008126022114182229\n",
      "Iteration: 1815 Loss: 0.0008118727520373577\n",
      "Iteration: 1816 Loss: 0.0008111440116583355\n",
      "Iteration: 1817 Loss: 0.0008104159890092753\n",
      "Iteration: 1818 Loss: 0.0008096886828254885\n",
      "Iteration: 1819 Loss: 0.0008089620918493022\n",
      "Iteration: 1820 Loss: 0.0008082362148310402\n",
      "Iteration: 1821 Loss: 0.0008075110505274288\n",
      "Iteration: 1822 Loss: 0.0008067865977025693\n",
      "Iteration: 1823 Loss: 0.0008060628551274054\n",
      "Iteration: 1824 Loss: 0.0008053398215793511\n",
      "Iteration: 1825 Loss: 0.0008046174958436263\n",
      "Iteration: 1826 Loss: 0.0008038958767116123\n",
      "Iteration: 1827 Loss: 0.0008031749629814295\n",
      "Iteration: 1828 Loss: 0.0008024547534581755\n",
      "Iteration: 1829 Loss: 0.0008017352469531773\n",
      "Iteration: 1830 Loss: 0.0008010164422843975\n",
      "Iteration: 1831 Loss: 0.000800298338276744\n",
      "Iteration: 1832 Loss: 0.0007995809337607007\n",
      "Iteration: 1833 Loss: 0.0007988642275738396\n",
      "Iteration: 1834 Loss: 0.0007981482185598746\n",
      "Iteration: 1835 Loss: 0.0007974329055685529\n",
      "Iteration: 1836 Loss: 0.0007967182874562767\n",
      "Iteration: 1837 Loss: 0.0007960043630848923\n",
      "Iteration: 1838 Loss: 0.0007952911313227499\n",
      "Iteration: 1839 Loss: 0.0007945785910442837\n",
      "Iteration: 1840 Loss: 0.0007938667411298833\n",
      "Iteration: 1841 Loss: 0.0007931555804654046\n",
      "Iteration: 1842 Loss: 0.0007924451079431467\n",
      "Iteration: 1843 Loss: 0.0007917353224613869\n",
      "Iteration: 1844 Loss: 0.0007910262229234894\n",
      "Iteration: 1845 Loss: 0.0007903178082387325\n",
      "Iteration: 1846 Loss: 0.0007896100773221774\n",
      "Iteration: 1847 Loss: 0.0007889030290944903\n",
      "Iteration: 1848 Loss: 0.0007881966624820192\n",
      "Iteration: 1849 Loss: 0.00078749097641621\n",
      "Iteration: 1850 Loss: 0.0007867859698342478\n",
      "Iteration: 1851 Loss: 0.0007860816416783866\n",
      "Iteration: 1852 Loss: 0.000785377990896827\n",
      "Iteration: 1853 Loss: 0.0007846750164425459\n",
      "Iteration: 1854 Loss: 0.0007839727172739592\n",
      "Iteration: 1855 Loss: 0.000783271092354907\n",
      "Iteration: 1856 Loss: 0.0007825701406542979\n",
      "Iteration: 1857 Loss: 0.0007818698611457031\n",
      "Iteration: 1858 Loss: 0.0007811702528081368\n",
      "Iteration: 1859 Loss: 0.0007804713146257107\n",
      "Iteration: 1860 Loss: 0.0007797730455876392\n",
      "Iteration: 1861 Loss: 0.0007790754446874743\n",
      "Iteration: 1862 Loss: 0.0007783785109239621\n",
      "Iteration: 1863 Loss: 0.000777682243300965\n",
      "Iteration: 1864 Loss: 0.0007769866408267236\n",
      "Iteration: 1865 Loss: 0.0007762917025144243\n",
      "Iteration: 1866 Loss: 0.0007755974273823367\n",
      "Iteration: 1867 Loss: 0.0007749038144525519\n",
      "Iteration: 1868 Loss: 0.0007742108627524231\n",
      "Iteration: 1869 Loss: 0.0007735185713138301\n",
      "Iteration: 1870 Loss: 0.000772826939172522\n",
      "Iteration: 1871 Loss: 0.0007721359653696367\n",
      "Iteration: 1872 Loss: 0.0007714456489505479\n",
      "Iteration: 1873 Loss: 0.0007707559889645639\n",
      "Iteration: 1874 Loss: 0.000770066984466594\n",
      "Iteration: 1875 Loss: 0.0007693786345144099\n",
      "Iteration: 1876 Loss: 0.0007686909381707541\n",
      "Iteration: 1877 Loss: 0.0007680038945026286\n",
      "Iteration: 1878 Loss: 0.0007673175025813154\n",
      "Iteration: 1879 Loss: 0.000766631761482012\n",
      "Iteration: 1880 Loss: 0.0007659466702844528\n",
      "Iteration: 1881 Loss: 0.0007652622280720831\n",
      "Iteration: 1882 Loss: 0.0007645784339326704\n",
      "Iteration: 1883 Loss: 0.000763895286958173\n",
      "Iteration: 1884 Loss: 0.0007632127862440946\n",
      "Iteration: 1885 Loss: 0.0007625309308903129\n",
      "Iteration: 1886 Loss: 0.0007618497200003082\n",
      "Iteration: 1887 Loss: 0.0007611691526815575\n",
      "Iteration: 1888 Loss: 0.0007604892280457005\n",
      "Iteration: 1889 Loss: 0.0007598099452081568\n",
      "Iteration: 1890 Loss: 0.0007591313032874651\n",
      "Iteration: 1891 Loss: 0.0007584533014066103\n",
      "Iteration: 1892 Loss: 0.0007577759386924318\n",
      "Iteration: 1893 Loss: 0.0007570992142746062\n",
      "Iteration: 1894 Loss: 0.0007564231272873278\n",
      "Iteration: 1895 Loss: 0.0007557476768680331\n",
      "Iteration: 1896 Loss: 0.0007550728621579706\n",
      "Iteration: 1897 Loss: 0.0007543986823016848\n",
      "Iteration: 1898 Loss: 0.0007537251364472098\n",
      "Iteration: 1899 Loss: 0.0007530522237465737\n",
      "Iteration: 1900 Loss: 0.0007523799433544555\n",
      "Iteration: 1901 Loss: 0.000751708294429642\n",
      "Iteration: 1902 Loss: 0.0007510372761340533\n",
      "Iteration: 1903 Loss: 0.000750366887633684\n",
      "Iteration: 1904 Loss: 0.0007496971280965332\n",
      "Iteration: 1905 Loss: 0.0007490279966950623\n",
      "Iteration: 1906 Loss: 0.0007483594926043319\n",
      "Iteration: 1907 Loss: 0.0007476916150033646\n",
      "Iteration: 1908 Loss: 0.000747024363073583\n",
      "Iteration: 1909 Loss: 0.0007463577360003334\n",
      "Iteration: 1910 Loss: 0.0007456917329713522\n",
      "Iteration: 1911 Loss: 0.0007450263531785913\n",
      "Iteration: 1912 Loss: 0.0007443615958165517\n",
      "Iteration: 1913 Loss: 0.0007436974600823969\n",
      "Iteration: 1914 Loss: 0.00074303394517723\n",
      "Iteration: 1915 Loss: 0.0007423710503040044\n",
      "Iteration: 1916 Loss: 0.0007417087746705084\n",
      "Iteration: 1917 Loss: 0.0007410471174859495\n",
      "Iteration: 1918 Loss: 0.000740386077962957\n",
      "Iteration: 1919 Loss: 0.0007397256553171788\n",
      "Iteration: 1920 Loss: 0.0007390658487670358\n",
      "Iteration: 1921 Loss: 0.0007384066575340967\n",
      "Iteration: 1922 Loss: 0.0007377480808425591\n",
      "Iteration: 1923 Loss: 0.0007370901179198637\n",
      "Iteration: 1924 Loss: 0.0007364327679955725\n",
      "Iteration: 1925 Loss: 0.000735776030302556\n",
      "Iteration: 1926 Loss: 0.0007351199040761929\n",
      "Iteration: 1927 Loss: 0.0007344643885550774\n",
      "Iteration: 1928 Loss: 0.0007338094829798174\n",
      "Iteration: 1929 Loss: 0.000733155186593977\n",
      "Iteration: 1930 Loss: 0.0007325014986439714\n",
      "Iteration: 1931 Loss: 0.0007318484183789639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1932 Loss: 0.0007311959450504135\n",
      "Iteration: 1933 Loss: 0.0007305440779126723\n",
      "Iteration: 1934 Loss: 0.0007298928162224563\n",
      "Iteration: 1935 Loss: 0.0007292421592392789\n",
      "Iteration: 1936 Loss: 0.0007285921062249325\n",
      "Iteration: 1937 Loss: 0.0007279426564438543\n",
      "Iteration: 1938 Loss: 0.0007272938091628428\n",
      "Iteration: 1939 Loss: 0.0007266455636515597\n",
      "Iteration: 1940 Loss: 0.0007259979191818054\n",
      "Iteration: 1941 Loss: 0.0007253508750280208\n",
      "Iteration: 1942 Loss: 0.0007247044304667077\n",
      "Iteration: 1943 Loss: 0.0007240585847771773\n",
      "Iteration: 1944 Loss: 0.0007234133372408429\n",
      "Iteration: 1945 Loss: 0.0007227686871414404\n",
      "Iteration: 1946 Loss: 0.0007221246337651044\n",
      "Iteration: 1947 Loss: 0.00072148117640044\n",
      "Iteration: 1948 Loss: 0.0007208383143384827\n",
      "Iteration: 1949 Loss: 0.0007201960468718372\n",
      "Iteration: 1950 Loss: 0.0007195543732963305\n",
      "Iteration: 1951 Loss: 0.0007189132929093864\n",
      "Iteration: 1952 Loss: 0.0007182728050111078\n",
      "Iteration: 1953 Loss: 0.0007176329089032378\n",
      "Iteration: 1954 Loss: 0.0007169936038899629\n",
      "Iteration: 1955 Loss: 0.000716354889277999\n",
      "Iteration: 1956 Loss: 0.0007157167643755311\n",
      "Iteration: 1957 Loss: 0.0007150792284934287\n",
      "Iteration: 1958 Loss: 0.0007144422809445987\n",
      "Iteration: 1959 Loss: 0.0007138059210436664\n",
      "Iteration: 1960 Loss: 0.0007131701481079741\n",
      "Iteration: 1961 Loss: 0.0007125349614562898\n",
      "Iteration: 1962 Loss: 0.0007119003604101536\n",
      "Iteration: 1963 Loss: 0.0007112663442926658\n",
      "Iteration: 1964 Loss: 0.0007106329124286729\n",
      "Iteration: 1965 Loss: 0.0007100000641454458\n",
      "Iteration: 1966 Loss: 0.0007093677987725481\n",
      "Iteration: 1967 Loss: 0.0007087361156407673\n",
      "Iteration: 1968 Loss: 0.0007081050140831037\n",
      "Iteration: 1969 Loss: 0.0007074744934350474\n",
      "Iteration: 1970 Loss: 0.0007068445530333099\n",
      "Iteration: 1971 Loss: 0.0007062151922166577\n",
      "Iteration: 1972 Loss: 0.0007055864103259345\n",
      "Iteration: 1973 Loss: 0.0007049582067039227\n",
      "Iteration: 1974 Loss: 0.0007043305806950721\n",
      "Iteration: 1975 Loss: 0.000703703531645806\n",
      "Iteration: 1976 Loss: 0.0007030770589045261\n",
      "Iteration: 1977 Loss: 0.0007024511618209391\n",
      "Iteration: 1978 Loss: 0.000701825839747199\n",
      "Iteration: 1979 Loss: 0.0007012010920370139\n",
      "Iteration: 1980 Loss: 0.0007005769180458607\n",
      "Iteration: 1981 Loss: 0.0006999533171304453\n",
      "Iteration: 1982 Loss: 0.0006993302886500453\n",
      "Iteration: 1983 Loss: 0.0006987078319656293\n",
      "Iteration: 1984 Loss: 0.0006980859464393935\n",
      "Iteration: 1985 Loss: 0.0006974646314353449\n",
      "Iteration: 1986 Loss: 0.0006968438863195687\n",
      "Iteration: 1987 Loss: 0.0006962237104596216\n",
      "Iteration: 1988 Loss: 0.0006956041032245086\n",
      "Iteration: 1989 Loss: 0.0006949850639852331\n",
      "Iteration: 1990 Loss: 0.0006943665921146553\n",
      "Iteration: 1991 Loss: 0.0006937486869866181\n",
      "Iteration: 1992 Loss: 0.0006931313479769862\n",
      "Iteration: 1993 Loss: 0.000692514574462939\n",
      "Iteration: 1994 Loss: 0.0006918983658238188\n",
      "Iteration: 1995 Loss: 0.0006912827214404356\n",
      "Iteration: 1996 Loss: 0.0006906676406944427\n",
      "Iteration: 1997 Loss: 0.0006900531229703409\n",
      "Iteration: 1998 Loss: 0.0006894391676529634\n",
      "Iteration: 1999 Loss: 0.0006888257741291032\n",
      "Iteration: 2000 Loss: 0.00068821294178748\n",
      "Iteration: 2001 Loss: 0.000687600670017673\n",
      "Iteration: 2002 Loss: 0.0006869889582110904\n",
      "Iteration: 2003 Loss: 0.000686377805761331\n",
      "Iteration: 2004 Loss: 0.0006857672120623537\n",
      "Iteration: 2005 Loss: 0.0006851571765101783\n",
      "Iteration: 2006 Loss: 0.0006845476985021614\n",
      "Iteration: 2007 Loss: 0.0006839387774371197\n",
      "Iteration: 2008 Loss: 0.000683330412715553\n",
      "Iteration: 2009 Loss: 0.0006827226037388909\n",
      "Iteration: 2010 Loss: 0.0006821153499105188\n",
      "Iteration: 2011 Loss: 0.0006815086506350321\n",
      "Iteration: 2012 Loss: 0.0006809025053180905\n",
      "Iteration: 2013 Loss: 0.0006802969133674449\n",
      "Iteration: 2014 Loss: 0.0006796918741918043\n",
      "Iteration: 2015 Loss: 0.000679087387201341\n",
      "Iteration: 2016 Loss: 0.0006784834518073662\n",
      "Iteration: 2017 Loss: 0.0006778800674227539\n",
      "Iteration: 2018 Loss: 0.0006772772334621907\n",
      "Iteration: 2019 Loss: 0.00067667494934094\n",
      "Iteration: 2020 Loss: 0.0006760732144755693\n",
      "Iteration: 2021 Loss: 0.0006754720282851434\n",
      "Iteration: 2022 Loss: 0.0006748713901883206\n",
      "Iteration: 2023 Loss: 0.0006742712996064876\n",
      "Iteration: 2024 Loss: 0.0006736717559613938\n",
      "Iteration: 2025 Loss: 0.0006730727586770378\n",
      "Iteration: 2026 Loss: 0.0006724743071780065\n",
      "Iteration: 2027 Loss: 0.0006718764008898578\n",
      "Iteration: 2028 Loss: 0.0006712790392398371\n",
      "Iteration: 2029 Loss: 0.0006706822216568475\n",
      "Iteration: 2030 Loss: 0.0006700859475706531\n",
      "Iteration: 2031 Loss: 0.0006694902164119112\n",
      "Iteration: 2032 Loss: 0.0006688950276131042\n",
      "Iteration: 2033 Loss: 0.0006683003806073681\n",
      "Iteration: 2034 Loss: 0.0006677062748291748\n",
      "Iteration: 2035 Loss: 0.0006671127097146119\n",
      "Iteration: 2036 Loss: 0.0006665196847008731\n",
      "Iteration: 2037 Loss: 0.0006659271992261288\n",
      "Iteration: 2038 Loss: 0.0006653352527296221\n",
      "Iteration: 2039 Loss: 0.00066474384465228\n",
      "Iteration: 2040 Loss: 0.0006641529744354413\n",
      "Iteration: 2041 Loss: 0.0006635626415219693\n",
      "Iteration: 2042 Loss: 0.0006629728453563787\n",
      "Iteration: 2043 Loss: 0.0006623835853835722\n",
      "Iteration: 2044 Loss: 0.0006617948610498001\n",
      "Iteration: 2045 Loss: 0.0006612066718026948\n",
      "Iteration: 2046 Loss: 0.0006606190170907255\n",
      "Iteration: 2047 Loss: 0.0006600318963636896\n",
      "Iteration: 2048 Loss: 0.0006594453090720765\n",
      "Iteration: 2049 Loss: 0.0006588592546680042\n",
      "Iteration: 2050 Loss: 0.0006582737326045668\n",
      "Iteration: 2051 Loss: 0.0006576887423361233\n",
      "Iteration: 2052 Loss: 0.0006571042833170104\n",
      "Iteration: 2053 Loss: 0.0006565203550040104\n",
      "Iteration: 2054 Loss: 0.0006559369568543753\n",
      "Iteration: 2055 Loss: 0.0006553540883265473\n",
      "Iteration: 2056 Loss: 0.0006547717488798941\n",
      "Iteration: 2057 Loss: 0.0006541899379749286\n",
      "Iteration: 2058 Loss: 0.0006536086550728836\n",
      "Iteration: 2059 Loss: 0.0006530278996369236\n",
      "Iteration: 2060 Loss: 0.0006524476711299612\n",
      "Iteration: 2061 Loss: 0.0006518679690172362\n",
      "Iteration: 2062 Loss: 0.000651288792763597\n",
      "Iteration: 2063 Loss: 0.0006507101418360845\n",
      "Iteration: 2064 Loss: 0.0006501320157023696\n",
      "Iteration: 2065 Loss: 0.0006495544138308619\n",
      "Iteration: 2066 Loss: 0.0006489773356913322\n",
      "Iteration: 2067 Loss: 0.0006484007807545075\n",
      "Iteration: 2068 Loss: 0.0006478247484914804\n",
      "Iteration: 2069 Loss: 0.0006472492383754867\n",
      "Iteration: 2070 Loss: 0.0006466742498797053\n",
      "Iteration: 2071 Loss: 0.0006460997824785936\n",
      "Iteration: 2072 Loss: 0.0006455258356478897\n",
      "Iteration: 2073 Loss: 0.0006449524088639377\n",
      "Iteration: 2074 Loss: 0.0006443795016039394\n",
      "Iteration: 2075 Loss: 0.0006438071133462973\n",
      "Iteration: 2076 Loss: 0.0006432352435703553\n",
      "Iteration: 2077 Loss: 0.0006426638917557291\n",
      "Iteration: 2078 Loss: 0.0006420930573838843\n",
      "Iteration: 2079 Loss: 0.0006415227399367815\n",
      "Iteration: 2080 Loss: 0.0006409529388979351\n",
      "Iteration: 2081 Loss: 0.0006403836537505892\n",
      "Iteration: 2082 Loss: 0.0006398148839795087\n",
      "Iteration: 2083 Loss: 0.0006392466290707483\n",
      "Iteration: 2084 Loss: 0.0006386788885102639\n",
      "Iteration: 2085 Loss: 0.0006381116617859895\n",
      "Iteration: 2086 Loss: 0.0006375449483854765\n",
      "Iteration: 2087 Loss: 0.0006369787477989642\n",
      "Iteration: 2088 Loss: 0.0006364130595158683\n",
      "Iteration: 2089 Loss: 0.0006358478830278264\n",
      "Iteration: 2090 Loss: 0.0006352832178258548\n",
      "Iteration: 2091 Loss: 0.0006347190634029801\n",
      "Iteration: 2092 Loss: 0.0006341554192526872\n",
      "Iteration: 2093 Loss: 0.0006335922848697657\n",
      "Iteration: 2094 Loss: 0.0006330296597492729\n",
      "Iteration: 2095 Loss: 0.0006324675433870614\n",
      "Iteration: 2096 Loss: 0.0006319059352803251\n",
      "Iteration: 2097 Loss: 0.0006313448349266688\n",
      "Iteration: 2098 Loss: 0.0006307842418249157\n",
      "Iteration: 2099 Loss: 0.0006302241554742345\n",
      "Iteration: 2100 Loss: 0.0006296645753750528\n",
      "Iteration: 2101 Loss: 0.0006291055010283384\n",
      "Iteration: 2102 Loss: 0.000628546931935979\n",
      "Iteration: 2103 Loss: 0.0006279888676008035\n",
      "Iteration: 2104 Loss: 0.0006274313075263879\n",
      "Iteration: 2105 Loss: 0.0006268742512165246\n",
      "Iteration: 2106 Loss: 0.0006263176981767712\n",
      "Iteration: 2107 Loss: 0.0006257616479132094\n",
      "Iteration: 2108 Loss: 0.0006252060999317972\n",
      "Iteration: 2109 Loss: 0.0006246510537410426\n",
      "Iteration: 2110 Loss: 0.000624096508848456\n",
      "Iteration: 2111 Loss: 0.0006235424647632915\n",
      "Iteration: 2112 Loss: 0.0006229889209954154\n",
      "Iteration: 2113 Loss: 0.0006224358770553898\n",
      "Iteration: 2114 Loss: 0.0006218833324546843\n",
      "Iteration: 2115 Loss: 0.0006213312867052931\n",
      "Iteration: 2116 Loss: 0.0006207797393199568\n",
      "Iteration: 2117 Loss: 0.0006202286898125939\n",
      "Iteration: 2118 Loss: 0.0006196781376971931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2119 Loss: 0.0006191280824892853\n",
      "Iteration: 2120 Loss: 0.0006185785237047794\n",
      "Iteration: 2121 Loss: 0.0006180294608601946\n",
      "Iteration: 2122 Loss: 0.0006174808934727549\n",
      "Iteration: 2123 Loss: 0.0006169328210608767\n",
      "Iteration: 2124 Loss: 0.0006163852431430104\n",
      "Iteration: 2125 Loss: 0.0006158381592392568\n",
      "Iteration: 2126 Loss: 0.0006152915688694131\n",
      "Iteration: 2127 Loss: 0.0006147454715547368\n",
      "Iteration: 2128 Loss: 0.0006141998668165122\n",
      "Iteration: 2129 Loss: 0.0006136547541777289\n",
      "Iteration: 2130 Loss: 0.0006131101331614369\n",
      "Iteration: 2131 Loss: 0.0006125660032912958\n",
      "Iteration: 2132 Loss: 0.0006120223640921189\n",
      "Iteration: 2133 Loss: 0.0006114792150889068\n",
      "Iteration: 2134 Loss: 0.0006109365558079735\n",
      "Iteration: 2135 Loss: 0.0006103943857755375\n",
      "Iteration: 2136 Loss: 0.0006098527045191868\n",
      "Iteration: 2137 Loss: 0.000609311511567213\n",
      "Iteration: 2138 Loss: 0.0006087708064480763\n",
      "Iteration: 2139 Loss: 0.0006082305886917005\n",
      "Iteration: 2140 Loss: 0.0006076908578275303\n",
      "Iteration: 2141 Loss: 0.0006071516133867954\n",
      "Iteration: 2142 Loss: 0.0006066128549008052\n",
      "Iteration: 2143 Loss: 0.000606074581901716\n",
      "Iteration: 2144 Loss: 0.000605536793922245\n",
      "Iteration: 2145 Loss: 0.0006049994904961356\n",
      "Iteration: 2146 Loss: 0.0006044626711571767\n",
      "Iteration: 2147 Loss: 0.0006039263354400928\n",
      "Iteration: 2148 Loss: 0.0006033904828807586\n",
      "Iteration: 2149 Loss: 0.0006028551130150231\n",
      "Iteration: 2150 Loss: 0.0006023202253793582\n",
      "Iteration: 2151 Loss: 0.0006017858195118193\n",
      "Iteration: 2152 Loss: 0.0006012518949500874\n",
      "Iteration: 2153 Loss: 0.0006007184512328197\n",
      "Iteration: 2154 Loss: 0.0006001854878997404\n",
      "Iteration: 2155 Loss: 0.0005996530044905953\n",
      "Iteration: 2156 Loss: 0.0005991210005461026\n",
      "Iteration: 2157 Loss: 0.0005985894756075298\n",
      "Iteration: 2158 Loss: 0.00059805842921681\n",
      "Iteration: 2159 Loss: 0.0005975278609162715\n",
      "Iteration: 2160 Loss: 0.0005969977702491375\n",
      "Iteration: 2161 Loss: 0.0005964681567591453\n",
      "Iteration: 2162 Loss: 0.0005959390199909042\n",
      "Iteration: 2163 Loss: 0.0005954103594890106\n",
      "Iteration: 2164 Loss: 0.0005948821747995507\n",
      "Iteration: 2165 Loss: 0.0005943544654686056\n",
      "Iteration: 2166 Loss: 0.0005938272310430833\n",
      "Iteration: 2167 Loss: 0.0005933004710704328\n",
      "Iteration: 2168 Loss: 0.0005927741850988182\n",
      "Iteration: 2169 Loss: 0.0005922483726764819\n",
      "Iteration: 2170 Loss: 0.0005917230333531184\n",
      "Iteration: 2171 Loss: 0.000591198166678727\n",
      "Iteration: 2172 Loss: 0.0005906737722038577\n",
      "Iteration: 2173 Loss: 0.0005901498494791967\n",
      "Iteration: 2174 Loss: 0.0005896263980562978\n",
      "Iteration: 2175 Loss: 0.0005891034174881676\n",
      "Iteration: 2176 Loss: 0.0005885809073273799\n",
      "Iteration: 2177 Loss: 0.0005880588671273031\n",
      "Iteration: 2178 Loss: 0.0005875372964415338\n",
      "Iteration: 2179 Loss: 0.0005870161948254262\n",
      "Iteration: 2180 Loss: 0.0005864955618337443\n",
      "Iteration: 2181 Loss: 0.0005859753970227209\n",
      "Iteration: 2182 Loss: 0.000585455699948202\n",
      "Iteration: 2183 Loss: 0.0005849364701672443\n",
      "Iteration: 2184 Loss: 0.0005844177072377255\n",
      "Iteration: 2185 Loss: 0.0005838994107171018\n",
      "Iteration: 2186 Loss: 0.0005833815801644422\n",
      "Iteration: 2187 Loss: 0.0005828642151386116\n",
      "Iteration: 2188 Loss: 0.0005823473151997674\n",
      "Iteration: 2189 Loss: 0.0005818308799082633\n",
      "Iteration: 2190 Loss: 0.0005813149088250161\n",
      "Iteration: 2191 Loss: 0.0005807994015112807\n",
      "Iteration: 2192 Loss: 0.0005802843575292618\n",
      "Iteration: 2193 Loss: 0.0005797697764415552\n",
      "Iteration: 2194 Loss: 0.0005792556578111685\n",
      "Iteration: 2195 Loss: 0.0005787420012019608\n",
      "Iteration: 2196 Loss: 0.0005782288061780877\n",
      "Iteration: 2197 Loss: 0.0005777160723043177\n",
      "Iteration: 2198 Loss: 0.0005772037991456257\n",
      "Iteration: 2199 Loss: 0.0005766919862682556\n",
      "Iteration: 2200 Loss: 0.0005761806332384387\n",
      "Iteration: 2201 Loss: 0.0005756697396234573\n",
      "Iteration: 2202 Loss: 0.0005751593049906221\n",
      "Iteration: 2203 Loss: 0.0005746493289078611\n",
      "Iteration: 2204 Loss: 0.0005741398109438738\n",
      "Iteration: 2205 Loss: 0.0005736307506679084\n",
      "Iteration: 2206 Loss: 0.0005731221476491493\n",
      "Iteration: 2207 Loss: 0.0005726140014583941\n",
      "Iteration: 2208 Loss: 0.0005721063116658492\n",
      "Iteration: 2209 Loss: 0.0005715990778429189\n",
      "Iteration: 2210 Loss: 0.0005710922995612235\n",
      "Iteration: 2211 Loss: 0.0005705859763932622\n",
      "Iteration: 2212 Loss: 0.0005700801079116126\n",
      "Iteration: 2213 Loss: 0.0005695746936895444\n",
      "Iteration: 2214 Loss: 0.000569069733301175\n",
      "Iteration: 2215 Loss: 0.0005685652263208838\n",
      "Iteration: 2216 Loss: 0.0005680611723233531\n",
      "Iteration: 2217 Loss: 0.0005675575708840348\n",
      "Iteration: 2218 Loss: 0.0005670544215786096\n",
      "Iteration: 2219 Loss: 0.0005665517239836832\n",
      "Iteration: 2220 Loss: 0.0005660494776763818\n",
      "Iteration: 2221 Loss: 0.0005655476822337166\n",
      "Iteration: 2222 Loss: 0.0005650463372338356\n",
      "Iteration: 2223 Loss: 0.000564545442255026\n",
      "Iteration: 2224 Loss: 0.0005640449968760458\n",
      "Iteration: 2225 Loss: 0.0005635450006770424\n",
      "Iteration: 2226 Loss: 0.0005630454532372106\n",
      "Iteration: 2227 Loss: 0.000562546354137254\n",
      "Iteration: 2228 Loss: 0.0005620477029586031\n",
      "Iteration: 2229 Loss: 0.0005615494992822585\n",
      "Iteration: 2230 Loss: 0.0005610517426900402\n",
      "Iteration: 2231 Loss: 0.0005605544327645665\n",
      "Iteration: 2232 Loss: 0.0005600575690885418\n",
      "Iteration: 2233 Loss: 0.0005595611512453883\n",
      "Iteration: 2234 Loss: 0.0005590651788194125\n",
      "Iteration: 2235 Loss: 0.0005585696513944952\n",
      "Iteration: 2236 Loss: 0.0005580745685552887\n",
      "Iteration: 2237 Loss: 0.0005575799298878263\n",
      "Iteration: 2238 Loss: 0.0005570857349778247\n",
      "Iteration: 2239 Loss: 0.0005565919834113026\n",
      "Iteration: 2240 Loss: 0.0005560986747751567\n",
      "Iteration: 2241 Loss: 0.0005556058086563901\n",
      "Iteration: 2242 Loss: 0.000555113384643091\n",
      "Iteration: 2243 Loss: 0.0005546214023238813\n",
      "Iteration: 2244 Loss: 0.000554129861287109\n",
      "Iteration: 2245 Loss: 0.0005536387611219323\n",
      "Iteration: 2246 Loss: 0.0005531481014181676\n",
      "Iteration: 2247 Loss: 0.0005526578817657581\n",
      "Iteration: 2248 Loss: 0.0005521681017551775\n",
      "Iteration: 2249 Loss: 0.0005516787609778157\n",
      "Iteration: 2250 Loss: 0.0005511898590248627\n",
      "Iteration: 2251 Loss: 0.000550701395488398\n",
      "Iteration: 2252 Loss: 0.0005502133699609848\n",
      "Iteration: 2253 Loss: 0.0005497257820358111\n",
      "Iteration: 2254 Loss: 0.0005492386313058114\n",
      "Iteration: 2255 Loss: 0.0005487519173652471\n",
      "Iteration: 2256 Loss: 0.0005482656398081158\n",
      "Iteration: 2257 Loss: 0.0005477797982292286\n",
      "Iteration: 2258 Loss: 0.0005472943922239286\n",
      "Iteration: 2259 Loss: 0.0005468094213879436\n",
      "Iteration: 2260 Loss: 0.0005463248853175555\n",
      "Iteration: 2261 Loss: 0.0005458407836093175\n",
      "Iteration: 2262 Loss: 0.0005453571158599649\n",
      "Iteration: 2263 Loss: 0.0005448738816674194\n",
      "Iteration: 2264 Loss: 0.000544391080629391\n",
      "Iteration: 2265 Loss: 0.0005439087123446255\n",
      "Iteration: 2266 Loss: 0.000543426776411537\n",
      "Iteration: 2267 Loss: 0.0005429452724297504\n",
      "Iteration: 2268 Loss: 0.0005424641999986586\n",
      "Iteration: 2269 Loss: 0.0005419835587189714\n",
      "Iteration: 2270 Loss: 0.0005415033481907723\n",
      "Iteration: 2271 Loss: 0.0005410235680157745\n",
      "Iteration: 2272 Loss: 0.000540544217795536\n",
      "Iteration: 2273 Loss: 0.0005400652971314548\n",
      "Iteration: 2274 Loss: 0.0005395868056261408\n",
      "Iteration: 2275 Loss: 0.0005391087428825609\n",
      "Iteration: 2276 Loss: 0.0005386311085041859\n",
      "Iteration: 2277 Loss: 0.0005381539020945534\n",
      "Iteration: 2278 Loss: 0.0005376771232577093\n",
      "Iteration: 2279 Loss: 0.0005372007715982956\n",
      "Iteration: 2280 Loss: 0.0005367248467214285\n",
      "Iteration: 2281 Loss: 0.0005362493482328454\n",
      "Iteration: 2282 Loss: 0.0005357742757382735\n",
      "Iteration: 2283 Loss: 0.0005352996288441736\n",
      "Iteration: 2284 Loss: 0.0005348254071569301\n",
      "Iteration: 2285 Loss: 0.0005343516102836743\n",
      "Iteration: 2286 Loss: 0.0005338782378325666\n",
      "Iteration: 2287 Loss: 0.0005334052894115795\n",
      "Iteration: 2288 Loss: 0.0005329327646289247\n",
      "Iteration: 2289 Loss: 0.0005324606630937702\n",
      "Iteration: 2290 Loss: 0.0005319889844153551\n",
      "Iteration: 2291 Loss: 0.0005315177282033923\n",
      "Iteration: 2292 Loss: 0.0005310468940681299\n",
      "Iteration: 2293 Loss: 0.0005305764816200828\n",
      "Iteration: 2294 Loss: 0.0005301064904701996\n",
      "Iteration: 2295 Loss: 0.0005296369202306475\n",
      "Iteration: 2296 Loss: 0.0005291677705121183\n",
      "Iteration: 2297 Loss: 0.0005286990409275522\n",
      "Iteration: 2298 Loss: 0.0005282307310894911\n",
      "Iteration: 2299 Loss: 0.0005277628406114926\n",
      "Iteration: 2300 Loss: 0.0005272953691065483\n",
      "Iteration: 2301 Loss: 0.0005268283161890836\n",
      "Iteration: 2302 Loss: 0.0005263616814729099\n",
      "Iteration: 2303 Loss: 0.0005258954645729875\n",
      "Iteration: 2304 Loss: 0.0005254296651046952\n",
      "Iteration: 2305 Loss: 0.0005249642826833613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2306 Loss: 0.0005244993169253553\n",
      "Iteration: 2307 Loss: 0.0005240347674465358\n",
      "Iteration: 2308 Loss: 0.0005235706338641739\n",
      "Iteration: 2309 Loss: 0.000523106915795695\n",
      "Iteration: 2310 Loss: 0.0005226436128583877\n",
      "Iteration: 2311 Loss: 0.0005221807246702422\n",
      "Iteration: 2312 Loss: 0.0005217182508496314\n",
      "Iteration: 2313 Loss: 0.0005212561910156129\n",
      "Iteration: 2314 Loss: 0.0005207945447875037\n",
      "Iteration: 2315 Loss: 0.0005203333117849181\n",
      "Iteration: 2316 Loss: 0.0005198724916277955\n",
      "Iteration: 2317 Loss: 0.0005194120839370236\n",
      "Iteration: 2318 Loss: 0.0005189520883326313\n",
      "Iteration: 2319 Loss: 0.0005184925044366375\n",
      "Iteration: 2320 Loss: 0.0005180333318703058\n",
      "Iteration: 2321 Loss: 0.0005175745702558287\n",
      "Iteration: 2322 Loss: 0.0005171162192160113\n",
      "Iteration: 2323 Loss: 0.00051665827837311\n",
      "Iteration: 2324 Loss: 0.0005162007473506238\n",
      "Iteration: 2325 Loss: 0.0005157436257721822\n",
      "Iteration: 2326 Loss: 0.0005152869132620151\n",
      "Iteration: 2327 Loss: 0.0005148306094440861\n",
      "Iteration: 2328 Loss: 0.0005143747139438998\n",
      "Iteration: 2329 Loss: 0.0005139192263862599\n",
      "Iteration: 2330 Loss: 0.0005134641463963945\n",
      "Iteration: 2331 Loss: 0.0005130094736008204\n",
      "Iteration: 2332 Loss: 0.0005125552076257493\n",
      "Iteration: 2333 Loss: 0.0005121013480981169\n",
      "Iteration: 2334 Loss: 0.0005116478946879464\n",
      "Iteration: 2335 Loss: 0.0005111948469382843\n",
      "Iteration: 2336 Loss: 0.0005107422045160478\n",
      "Iteration: 2337 Loss: 0.0005102899670524546\n",
      "Iteration: 2338 Loss: 0.0005098381341756866\n",
      "Iteration: 2339 Loss: 0.0005093867055153476\n",
      "Iteration: 2340 Loss: 0.0005089356807005401\n",
      "Iteration: 2341 Loss: 0.0005084850593187684\n",
      "Iteration: 2342 Loss: 0.0005080348411282148\n",
      "Iteration: 2343 Loss: 0.0005075850256334271\n",
      "Iteration: 2344 Loss: 0.0005071356125048121\n",
      "Iteration: 2345 Loss: 0.0005066866013696608\n",
      "Iteration: 2346 Loss: 0.0005062379918681698\n",
      "Iteration: 2347 Loss: 0.0005057897835871154\n",
      "Iteration: 2348 Loss: 0.0005053419762428023\n",
      "Iteration: 2349 Loss: 0.0005048945694284195\n",
      "Iteration: 2350 Loss: 0.0005044475627754737\n",
      "Iteration: 2351 Loss: 0.0005040009559815512\n",
      "Iteration: 2352 Loss: 0.0005035547485329918\n",
      "Iteration: 2353 Loss: 0.0005031089401218594\n",
      "Iteration: 2354 Loss: 0.0005026635304942593\n",
      "Iteration: 2355 Loss: 0.0005022185191180417\n",
      "Iteration: 2356 Loss: 0.0005017739057499828\n",
      "Iteration: 2357 Loss: 0.0005013296899884855\n",
      "Iteration: 2358 Loss: 0.0005008858714679491\n",
      "Iteration: 2359 Loss: 0.0005004424498248163\n",
      "Iteration: 2360 Loss: 0.0004999994246960477\n",
      "Iteration: 2361 Loss: 0.0004995567957191355\n",
      "Iteration: 2362 Loss: 0.0004991145625744041\n",
      "Iteration: 2363 Loss: 0.000498672724815914\n",
      "Iteration: 2364 Loss: 0.0004982312821209508\n",
      "Iteration: 2365 Loss: 0.0004977902340882461\n",
      "Iteration: 2366 Loss: 0.0004973495804416225\n",
      "Iteration: 2367 Loss: 0.000496909320824121\n",
      "Iteration: 2368 Loss: 0.0004964694547888528\n",
      "Iteration: 2369 Loss: 0.0004960299820150328\n",
      "Iteration: 2370 Loss: 0.000495590902145967\n",
      "Iteration: 2371 Loss: 0.0004951522148225126\n",
      "Iteration: 2372 Loss: 0.0004947139196864731\n",
      "Iteration: 2373 Loss: 0.0004942760163792791\n",
      "Iteration: 2374 Loss: 0.0004938385045431306\n",
      "Iteration: 2375 Loss: 0.0004934013838204134\n",
      "Iteration: 2376 Loss: 0.0004929646538538552\n",
      "Iteration: 2377 Loss: 0.0004925283142867012\n",
      "Iteration: 2378 Loss: 0.0004920923647629948\n",
      "Iteration: 2379 Loss: 0.000491656804926141\n",
      "Iteration: 2380 Loss: 0.0004912216344204549\n",
      "Iteration: 2381 Loss: 0.0004907868528906656\n",
      "Iteration: 2382 Loss: 0.0004903524599816446\n",
      "Iteration: 2383 Loss: 0.0004899184553387043\n",
      "Iteration: 2384 Loss: 0.0004894848386075352\n",
      "Iteration: 2385 Loss: 0.000489051609434046\n",
      "Iteration: 2386 Loss: 0.0004886187674649328\n",
      "Iteration: 2387 Loss: 0.0004881863123468992\n",
      "Iteration: 2388 Loss: 0.0004877542437269655\n",
      "Iteration: 2389 Loss: 0.00048732256125273525\n",
      "Iteration: 2390 Loss: 0.00048689126457196675\n",
      "Iteration: 2391 Loss: 0.0004864603533327823\n",
      "Iteration: 2392 Loss: 0.00048602982718349284\n",
      "Iteration: 2393 Loss: 0.0004855996857729924\n",
      "Iteration: 2394 Loss: 0.00048516992875083874\n",
      "Iteration: 2395 Loss: 0.00048474055576647076\n",
      "Iteration: 2396 Loss: 0.0004843115664695514\n",
      "Iteration: 2397 Loss: 0.0004838829605107936\n",
      "Iteration: 2398 Loss: 0.000483454737540432\n",
      "Iteration: 2399 Loss: 0.0004830268972094328\n",
      "Iteration: 2400 Loss: 0.0004825994391692294\n",
      "Iteration: 2401 Loss: 0.000482172363071272\n",
      "Iteration: 2402 Loss: 0.0004817456685676721\n",
      "Iteration: 2403 Loss: 0.00048131935531095635\n",
      "Iteration: 2404 Loss: 0.0004808934229533923\n",
      "Iteration: 2405 Loss: 0.00048046787114806085\n",
      "Iteration: 2406 Loss: 0.00048004269954881587\n",
      "Iteration: 2407 Loss: 0.00047961790780919235\n",
      "Iteration: 2408 Loss: 0.0004791934955828062\n",
      "Iteration: 2409 Loss: 0.0004787694625247189\n",
      "Iteration: 2410 Loss: 0.00047834580828910045\n",
      "Iteration: 2411 Loss: 0.00047792253253111023\n",
      "Iteration: 2412 Loss: 0.0004774996349063495\n",
      "Iteration: 2413 Loss: 0.0004770771150704518\n",
      "Iteration: 2414 Loss: 0.0004766549726795637\n",
      "Iteration: 2415 Loss: 0.0004762332073489022\n",
      "Iteration: 2416 Loss: 0.0004758118188165576\n",
      "Iteration: 2417 Loss: 0.0004753908067004143\n",
      "Iteration: 2418 Loss: 0.0004749701706580756\n",
      "Iteration: 2419 Loss: 0.00047454991034634995\n",
      "Iteration: 2420 Loss: 0.000474130025423225\n",
      "Iteration: 2421 Loss: 0.00047371051558991236\n",
      "Iteration: 2422 Loss: 0.00047329138042088114\n",
      "Iteration: 2423 Loss: 0.00047287261961453226\n",
      "Iteration: 2424 Loss: 0.000472454232832522\n",
      "Iteration: 2425 Loss: 0.00047203621973485613\n",
      "Iteration: 2426 Loss: 0.0004716185799816761\n",
      "Iteration: 2427 Loss: 0.0004712013132333431\n",
      "Iteration: 2428 Loss: 0.0004707844191508561\n",
      "Iteration: 2429 Loss: 0.00047036789739526684\n",
      "Iteration: 2430 Loss: 0.00046995174762783646\n",
      "Iteration: 2431 Loss: 0.00046953596951047584\n",
      "Iteration: 2432 Loss: 0.00046912056270520747\n",
      "Iteration: 2433 Loss: 0.0004687055268745964\n",
      "Iteration: 2434 Loss: 0.000468290861681077\n",
      "Iteration: 2435 Loss: 0.0004678765667880878\n",
      "Iteration: 2436 Loss: 0.00046746264185918747\n",
      "Iteration: 2437 Loss: 0.0004670490865576567\n",
      "Iteration: 2438 Loss: 0.00046663590054794765\n",
      "Iteration: 2439 Loss: 0.0004662230834944966\n",
      "Iteration: 2440 Loss: 0.00046581063506147003\n",
      "Iteration: 2441 Loss: 0.00046539855491485664\n",
      "Iteration: 2442 Loss: 0.00046498684271928034\n",
      "Iteration: 2443 Loss: 0.00046457549814062787\n",
      "Iteration: 2444 Loss: 0.0004641645208450586\n",
      "Iteration: 2445 Loss: 0.00046375391049895625\n",
      "Iteration: 2446 Loss: 0.000463343666768866\n",
      "Iteration: 2447 Loss: 0.0004629337893219582\n",
      "Iteration: 2448 Loss: 0.0004625242778255986\n",
      "Iteration: 2449 Loss: 0.0004621151319472858\n",
      "Iteration: 2450 Loss: 0.00046170635135501994\n",
      "Iteration: 2451 Loss: 0.00046129793571729043\n",
      "Iteration: 2452 Loss: 0.0004608898847022457\n",
      "Iteration: 2453 Loss: 0.0004604821979791179\n",
      "Iteration: 2454 Loss: 0.00046007487521759896\n",
      "Iteration: 2455 Loss: 0.00045966791608652383\n",
      "Iteration: 2456 Loss: 0.0004592613202561237\n",
      "Iteration: 2457 Loss: 0.00045885508739697755\n",
      "Iteration: 2458 Loss: 0.00045844921717888505\n",
      "Iteration: 2459 Loss: 0.00045804370927339484\n",
      "Iteration: 2460 Loss: 0.0004576385633511283\n",
      "Iteration: 2461 Loss: 0.0004572337790841257\n",
      "Iteration: 2462 Loss: 0.000456829356144013\n",
      "Iteration: 2463 Loss: 0.0004564252942023685\n",
      "Iteration: 2464 Loss: 0.00045602159293198013\n",
      "Iteration: 2465 Loss: 0.00045561825200604133\n",
      "Iteration: 2466 Loss: 0.00045521527109709895\n",
      "Iteration: 2467 Loss: 0.00045481264987898957\n",
      "Iteration: 2468 Loss: 0.0004544103880250056\n",
      "Iteration: 2469 Loss: 0.000454008485209091\n",
      "Iteration: 2470 Loss: 0.0004536069411062098\n",
      "Iteration: 2471 Loss: 0.00045320575539028936\n",
      "Iteration: 2472 Loss: 0.0004528049277365354\n",
      "Iteration: 2473 Loss: 0.0004524044578201989\n",
      "Iteration: 2474 Loss: 0.0004520043453169779\n",
      "Iteration: 2475 Loss: 0.00045160458990257084\n",
      "Iteration: 2476 Loss: 0.0004512051912535168\n",
      "Iteration: 2477 Loss: 0.00045080614904617074\n",
      "Iteration: 2478 Loss: 0.0004504074629570778\n",
      "Iteration: 2479 Loss: 0.00045000913266369244\n",
      "Iteration: 2480 Loss: 0.00044961115784316116\n",
      "Iteration: 2481 Loss: 0.00044921353817334645\n",
      "Iteration: 2482 Loss: 0.0004488162733326473\n",
      "Iteration: 2483 Loss: 0.0004484193629988484\n",
      "Iteration: 2484 Loss: 0.00044802280685090696\n",
      "Iteration: 2485 Loss: 0.0004476266045678576\n",
      "Iteration: 2486 Loss: 0.00044723075582892637\n",
      "Iteration: 2487 Loss: 0.0004468352603140647\n",
      "Iteration: 2488 Loss: 0.00044644011770263864\n",
      "Iteration: 2489 Loss: 0.0004460453276755189\n",
      "Iteration: 2490 Loss: 0.000445650889912425\n",
      "Iteration: 2491 Loss: 0.00044525680409439707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2492 Loss: 0.00044486306990273186\n",
      "Iteration: 2493 Loss: 0.00044446968701873906\n",
      "Iteration: 2494 Loss: 0.0004440766551244839\n",
      "Iteration: 2495 Loss: 0.00044368397390169415\n",
      "Iteration: 2496 Loss: 0.0004432916430329361\n",
      "Iteration: 2497 Loss: 0.0004428996622005485\n",
      "Iteration: 2498 Loss: 0.00044250803108732557\n",
      "Iteration: 2499 Loss: 0.00044211674937687657\n",
      "Iteration: 2500 Loss: 0.0004417258167527269\n",
      "Iteration: 2501 Loss: 0.00044133523289850555\n",
      "Iteration: 2502 Loss: 0.0004409449974981848\n",
      "Iteration: 2503 Loss: 0.000440555110236718\n",
      "Iteration: 2504 Loss: 0.00044016557079872557\n",
      "Iteration: 2505 Loss: 0.00043977637886867673\n",
      "Iteration: 2506 Loss: 0.00043938753409007264\n",
      "Iteration: 2507 Loss: 0.00043899903623237634\n",
      "Iteration: 2508 Loss: 0.00043861088494251394\n",
      "Iteration: 2509 Loss: 0.0004382230799040127\n",
      "Iteration: 2510 Loss: 0.0004378356208038306\n",
      "Iteration: 2511 Loss: 0.0004374485073281754\n",
      "Iteration: 2512 Loss: 0.000437061739164719\n",
      "Iteration: 2513 Loss: 0.0004366753160006658\n",
      "Iteration: 2514 Loss: 0.00043628923752424373\n",
      "Iteration: 2515 Loss: 0.0004359035034229816\n",
      "Iteration: 2516 Loss: 0.00043551811338526784\n",
      "Iteration: 2517 Loss: 0.00043513306709979294\n",
      "Iteration: 2518 Loss: 0.0004347483642552684\n",
      "Iteration: 2519 Loss: 0.0004343640045411574\n",
      "Iteration: 2520 Loss: 0.00043397998764690125\n",
      "Iteration: 2521 Loss: 0.00043359631326215294\n",
      "Iteration: 2522 Loss: 0.00043321298107717717\n",
      "Iteration: 2523 Loss: 0.00043282999078221276\n",
      "Iteration: 2524 Loss: 0.0004324473420683186\n",
      "Iteration: 2525 Loss: 0.0004320650346261046\n",
      "Iteration: 2526 Loss: 0.00043168306814697556\n",
      "Iteration: 2527 Loss: 0.0004313014423225705\n",
      "Iteration: 2528 Loss: 0.00043092015684463125\n",
      "Iteration: 2529 Loss: 0.0004305392114051189\n",
      "Iteration: 2530 Loss: 0.00043015860569654667\n",
      "Iteration: 2531 Loss: 0.0004297783394116031\n",
      "Iteration: 2532 Loss: 0.0004293984122433561\n",
      "Iteration: 2533 Loss: 0.0004290188238850543\n",
      "Iteration: 2534 Loss: 0.00042863957403081455\n",
      "Iteration: 2535 Loss: 0.0004282606623736194\n",
      "Iteration: 2536 Loss: 0.00042788208860822057\n",
      "Iteration: 2537 Loss: 0.0004275038524286826\n",
      "Iteration: 2538 Loss: 0.0004271259535302804\n",
      "Iteration: 2539 Loss: 0.0004267483916074664\n",
      "Iteration: 2540 Loss: 0.00042637116635562897\n",
      "Iteration: 2541 Loss: 0.0004259942774706591\n",
      "Iteration: 2542 Loss: 0.00042561772464825784\n",
      "Iteration: 2543 Loss: 0.0004252415075846303\n",
      "Iteration: 2544 Loss: 0.00042486562597628596\n",
      "Iteration: 2545 Loss: 0.00042449007952018045\n",
      "Iteration: 2546 Loss: 0.0004241148679129518\n",
      "Iteration: 2547 Loss: 0.0004237399908520201\n",
      "Iteration: 2548 Loss: 0.0004233654480350583\n",
      "Iteration: 2549 Loss: 0.00042299123916010963\n",
      "Iteration: 2550 Loss: 0.00042261736396753566\n",
      "Iteration: 2551 Loss: 0.0004222438220722946\n",
      "Iteration: 2552 Loss: 0.0004218706132117282\n",
      "Iteration: 2553 Loss: 0.0004214977370875171\n",
      "Iteration: 2554 Loss: 0.00042112519339901805\n",
      "Iteration: 2555 Loss: 0.0004207529818459659\n",
      "Iteration: 2556 Loss: 0.0004203811021275809\n",
      "Iteration: 2557 Loss: 0.0004200095539444151\n",
      "Iteration: 2558 Loss: 0.00041963833699677826\n",
      "Iteration: 2559 Loss: 0.00041926745098526516\n",
      "Iteration: 2560 Loss: 0.00041889689561084\n",
      "Iteration: 2561 Loss: 0.00041852667057535697\n",
      "Iteration: 2562 Loss: 0.00041815677557994086\n",
      "Iteration: 2563 Loss: 0.00041778721032633776\n",
      "Iteration: 2564 Loss: 0.00041741797451672546\n",
      "Iteration: 2565 Loss: 0.0004170490678533925\n",
      "Iteration: 2566 Loss: 0.0004166804900391128\n",
      "Iteration: 2567 Loss: 0.0004163122407767555\n",
      "Iteration: 2568 Loss: 0.00041594431976990263\n",
      "Iteration: 2569 Loss: 0.0004155767267219012\n",
      "Iteration: 2570 Loss: 0.00041520946133634607\n",
      "Iteration: 2571 Loss: 0.00041484252331695727\n",
      "Iteration: 2572 Loss: 0.00041447591236874336\n",
      "Iteration: 2573 Loss: 0.0004141096281961048\n",
      "Iteration: 2574 Loss: 0.00041374367050350244\n",
      "Iteration: 2575 Loss: 0.0004133780389965398\n",
      "Iteration: 2576 Loss: 0.00041301273338043243\n",
      "Iteration: 2577 Loss: 0.0004126477533612338\n",
      "Iteration: 2578 Loss: 0.0004122830986446995\n",
      "Iteration: 2579 Loss: 0.0004119187689370388\n",
      "Iteration: 2580 Loss: 0.00041155476394499874\n",
      "Iteration: 2581 Loss: 0.0004111910833751503\n",
      "Iteration: 2582 Loss: 0.0004108277269347348\n",
      "Iteration: 2583 Loss: 0.0004104646943306978\n",
      "Iteration: 2584 Loss: 0.00041010198527116805\n",
      "Iteration: 2585 Loss: 0.00040973959946392815\n",
      "Iteration: 2586 Loss: 0.0004093775366173964\n",
      "Iteration: 2587 Loss: 0.0004090157964396227\n",
      "Iteration: 2588 Loss: 0.0004086543786395246\n",
      "Iteration: 2589 Loss: 0.00040829328292594884\n",
      "Iteration: 2590 Loss: 0.0004079325090083198\n",
      "Iteration: 2591 Loss: 0.0004075720565961475\n",
      "Iteration: 2592 Loss: 0.0004072119253988477\n",
      "Iteration: 2593 Loss: 0.0004068521151267471\n",
      "Iteration: 2594 Loss: 0.0004064926254906904\n",
      "Iteration: 2595 Loss: 0.0004061334562006868\n",
      "Iteration: 2596 Loss: 0.0004057746069678228\n",
      "Iteration: 2597 Loss: 0.0004054160774607217\n",
      "Iteration: 2598 Loss: 0.00040505786747477973\n",
      "Iteration: 2599 Loss: 0.0004046999766829305\n",
      "Iteration: 2600 Loss: 0.00040434240479438147\n",
      "Iteration: 2601 Loss: 0.00040398515156405464\n",
      "Iteration: 2602 Loss: 0.0004036282166199144\n",
      "Iteration: 2603 Loss: 0.0004032715997139593\n",
      "Iteration: 2604 Loss: 0.00040291530056229315\n",
      "Iteration: 2605 Loss: 0.00040255931887783124\n",
      "Iteration: 2606 Loss: 0.0004022036543744829\n",
      "Iteration: 2607 Loss: 0.00040184830676580986\n",
      "Iteration: 2608 Loss: 0.0004014932757658589\n",
      "Iteration: 2609 Loss: 0.0004011385610889739\n",
      "Iteration: 2610 Loss: 0.00040078416245010925\n",
      "Iteration: 2611 Loss: 0.00040043007956364936\n",
      "Iteration: 2612 Loss: 0.00040007631214489015\n",
      "Iteration: 2613 Loss: 0.0003997228599095138\n",
      "Iteration: 2614 Loss: 0.0003993697225727215\n",
      "Iteration: 2615 Loss: 0.00039901689985077487\n",
      "Iteration: 2616 Loss: 0.0003986643914599771\n",
      "Iteration: 2617 Loss: 0.00039831219711685306\n",
      "Iteration: 2618 Loss: 0.0003979603165382744\n",
      "Iteration: 2619 Loss: 0.00039760874944091295\n",
      "Iteration: 2620 Loss: 0.0003972574955420336\n",
      "Iteration: 2621 Loss: 0.00039690655455971324\n",
      "Iteration: 2622 Loss: 0.0003965559262113276\n",
      "Iteration: 2623 Loss: 0.0003962056102145236\n",
      "Iteration: 2624 Loss: 0.0003958556062879886\n",
      "Iteration: 2625 Loss: 0.00039550591414998443\n",
      "Iteration: 2626 Loss: 0.00039515653347704967\n",
      "Iteration: 2627 Loss: 0.00039480746407254385\n",
      "Iteration: 2628 Loss: 0.00039445870561662913\n",
      "Iteration: 2629 Loss: 0.00039411025782634033\n",
      "Iteration: 2630 Loss: 0.0003937621204217346\n",
      "Iteration: 2631 Loss: 0.0003934142931228471\n",
      "Iteration: 2632 Loss: 0.00039306677565033933\n",
      "Iteration: 2633 Loss: 0.00039271956772425317\n",
      "Iteration: 2634 Loss: 0.0003923726690662164\n",
      "Iteration: 2635 Loss: 0.0003920260793972532\n",
      "Iteration: 2636 Loss: 0.00039167979843801116\n",
      "Iteration: 2637 Loss: 0.00039133382591129075\n",
      "Iteration: 2638 Loss: 0.00039098816153856187\n",
      "Iteration: 2639 Loss: 0.0003906428050420624\n",
      "Iteration: 2640 Loss: 0.0003902977561441681\n",
      "Iteration: 2641 Loss: 0.00038995301456774465\n",
      "Iteration: 2642 Loss: 0.00038960858003574773\n",
      "Iteration: 2643 Loss: 0.0003892644522713034\n",
      "Iteration: 2644 Loss: 0.0003889206309982293\n",
      "Iteration: 2645 Loss: 0.0003885771159399071\n",
      "Iteration: 2646 Loss: 0.00038823390682065387\n",
      "Iteration: 2647 Loss: 0.0003878910033645348\n",
      "Iteration: 2648 Loss: 0.0003875484052960569\n",
      "Iteration: 2649 Loss: 0.00038720611234009325\n",
      "Iteration: 2650 Loss: 0.00038686412422129373\n",
      "Iteration: 2651 Loss: 0.00038652244066540037\n",
      "Iteration: 2652 Loss: 0.0003861810613975674\n",
      "Iteration: 2653 Loss: 0.0003858399861440704\n",
      "Iteration: 2654 Loss: 0.0003854992146306698\n",
      "Iteration: 2655 Loss: 0.0003851587465834794\n",
      "Iteration: 2656 Loss: 0.00038481858172929905\n",
      "Iteration: 2657 Loss: 0.00038447871979509305\n",
      "Iteration: 2658 Loss: 0.00038413916050766323\n",
      "Iteration: 2659 Loss: 0.0003837999035941107\n",
      "Iteration: 2660 Loss: 0.00038346094878226816\n",
      "Iteration: 2661 Loss: 0.0003831222957995461\n",
      "Iteration: 2662 Loss: 0.00038278394437439887\n",
      "Iteration: 2663 Loss: 0.0003824458942351588\n",
      "Iteration: 2664 Loss: 0.00038210814511027057\n",
      "Iteration: 2665 Loss: 0.00038177069672843815\n",
      "Iteration: 2666 Loss: 0.0003814335488188826\n",
      "Iteration: 2667 Loss: 0.00038109670111099927\n",
      "Iteration: 2668 Loss: 0.00038076015333433477\n",
      "Iteration: 2669 Loss: 0.0003804239052185144\n",
      "Iteration: 2670 Loss: 0.00038008795649339\n",
      "Iteration: 2671 Loss: 0.0003797523068894845\n",
      "Iteration: 2672 Loss: 0.0003794169561372262\n",
      "Iteration: 2673 Loss: 0.0003790819039677926\n",
      "Iteration: 2674 Loss: 0.0003787471501119825\n",
      "Iteration: 2675 Loss: 0.00037841269430110506\n",
      "Iteration: 2676 Loss: 0.00037807853626671355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2677 Loss: 0.00037774467574071865\n",
      "Iteration: 2678 Loss: 0.0003774111124548688\n",
      "Iteration: 2679 Loss: 0.0003770778461419237\n",
      "Iteration: 2680 Loss: 0.00037674487653364545\n",
      "Iteration: 2681 Loss: 0.0003764122033634633\n",
      "Iteration: 2682 Loss: 0.0003760798263640776\n",
      "Iteration: 2683 Loss: 0.0003757477452691974\n",
      "Iteration: 2684 Loss: 0.0003754159598119294\n",
      "Iteration: 2685 Loss: 0.0003750844697261839\n",
      "Iteration: 2686 Loss: 0.0003747532747459717\n",
      "Iteration: 2687 Loss: 0.0003744223746054321\n",
      "Iteration: 2688 Loss: 0.00037409176903926733\n",
      "Iteration: 2689 Loss: 0.00037376145778176336\n",
      "Iteration: 2690 Loss: 0.00037343144056854196\n",
      "Iteration: 2691 Loss: 0.00037310171713461155\n",
      "Iteration: 2692 Loss: 0.0003727722872153857\n",
      "Iteration: 2693 Loss: 0.0003724431505463221\n",
      "Iteration: 2694 Loss: 0.00037211430686357215\n",
      "Iteration: 2695 Loss: 0.0003717857559036757\n",
      "Iteration: 2696 Loss: 0.00037145749740250244\n",
      "Iteration: 2697 Loss: 0.00037112953109712493\n",
      "Iteration: 2698 Loss: 0.00037080185672425336\n",
      "Iteration: 2699 Loss: 0.0003704744740211832\n",
      "Iteration: 2700 Loss: 0.0003701473827253119\n",
      "Iteration: 2701 Loss: 0.00036982058257442455\n",
      "Iteration: 2702 Loss: 0.00036949407330632303\n",
      "Iteration: 2703 Loss: 0.0003691678546588433\n",
      "Iteration: 2704 Loss: 0.00036884192637068504\n",
      "Iteration: 2705 Loss: 0.0003685162881805376\n",
      "Iteration: 2706 Loss: 0.000368190939827206\n",
      "Iteration: 2707 Loss: 0.00036786588104963303\n",
      "Iteration: 2708 Loss: 0.0003675411115872136\n",
      "Iteration: 2709 Loss: 0.0003672166311795591\n",
      "Iteration: 2710 Loss: 0.00036689243956647633\n",
      "Iteration: 2711 Loss: 0.0003665685364878749\n",
      "Iteration: 2712 Loss: 0.00036624492168427084\n",
      "Iteration: 2713 Loss: 0.0003659215948963183\n",
      "Iteration: 2714 Loss: 0.0003655985558645032\n",
      "Iteration: 2715 Loss: 0.00036527580433008135\n",
      "Iteration: 2716 Loss: 0.0003649533400336821\n",
      "Iteration: 2717 Loss: 0.0003646311627175384\n",
      "Iteration: 2718 Loss: 0.00036430927212288667\n",
      "Iteration: 2719 Loss: 0.00036398766799190015\n",
      "Iteration: 2720 Loss: 0.00036366635006673516\n",
      "Iteration: 2721 Loss: 0.0003633453180898376\n",
      "Iteration: 2722 Loss: 0.0003630245718037757\n",
      "Iteration: 2723 Loss: 0.00036270411095178437\n",
      "Iteration: 2724 Loss: 0.0003623839352765455\n",
      "Iteration: 2725 Loss: 0.00036206404452171313\n",
      "Iteration: 2726 Loss: 0.0003617444384308083\n",
      "Iteration: 2727 Loss: 0.0003614251167476428\n",
      "Iteration: 2728 Loss: 0.00036110607921637606\n",
      "Iteration: 2729 Loss: 0.0003607873255813022\n",
      "Iteration: 2730 Loss: 0.00036046885558711734\n",
      "Iteration: 2731 Loss: 0.0003601506689781949\n",
      "Iteration: 2732 Loss: 0.0003598327655003574\n",
      "Iteration: 2733 Loss: 0.0003595151448981236\n",
      "Iteration: 2734 Loss: 0.00035919780691704984\n",
      "Iteration: 2735 Loss: 0.0003588807513030737\n",
      "Iteration: 2736 Loss: 0.0003585639778019932\n",
      "Iteration: 2737 Loss: 0.0003582474861601921\n",
      "Iteration: 2738 Loss: 0.0003579312761235341\n",
      "Iteration: 2739 Loss: 0.0003576153474391191\n",
      "Iteration: 2740 Loss: 0.0003572996998537712\n",
      "Iteration: 2741 Loss: 0.00035698433311455186\n",
      "Iteration: 2742 Loss: 0.00035666924696905414\n",
      "Iteration: 2743 Loss: 0.00035635444116465453\n",
      "Iteration: 2744 Loss: 0.00035603991544934003\n",
      "Iteration: 2745 Loss: 0.00035572566957100784\n",
      "Iteration: 2746 Loss: 0.000355411703277957\n",
      "Iteration: 2747 Loss: 0.00035509801631844435\n",
      "Iteration: 2748 Loss: 0.00035478460844151426\n",
      "Iteration: 2749 Loss: 0.00035447147939582656\n",
      "Iteration: 2750 Loss: 0.00035415862893087676\n",
      "Iteration: 2751 Loss: 0.0003538460567958365\n",
      "Iteration: 2752 Loss: 0.0003535337627404781\n",
      "Iteration: 2753 Loss: 0.0003532217465146709\n",
      "Iteration: 2754 Loss: 0.0003529100078687944\n",
      "Iteration: 2755 Loss: 0.0003525985465527444\n",
      "Iteration: 2756 Loss: 0.00035228736231733977\n",
      "Iteration: 2757 Loss: 0.00035197645491304614\n",
      "Iteration: 2758 Loss: 0.000351665824091255\n",
      "Iteration: 2759 Loss: 0.0003513554696032835\n",
      "Iteration: 2760 Loss: 0.0003510453912002028\n",
      "Iteration: 2761 Loss: 0.00035073558863390103\n",
      "Iteration: 2762 Loss: 0.0003504260616565035\n",
      "Iteration: 2763 Loss: 0.0003501168100198625\n",
      "Iteration: 2764 Loss: 0.0003498078334767845\n",
      "Iteration: 2765 Loss: 0.0003494991317795595\n",
      "Iteration: 2766 Loss: 0.0003491907046807772\n",
      "Iteration: 2767 Loss: 0.000348882551933917\n",
      "Iteration: 2768 Loss: 0.0003485746732918611\n",
      "Iteration: 2769 Loss: 0.0003482670685085674\n",
      "Iteration: 2770 Loss: 0.0003479597373378312\n",
      "Iteration: 2771 Loss: 0.00034765267953345896\n",
      "Iteration: 2772 Loss: 0.0003473458948495048\n",
      "Iteration: 2773 Loss: 0.00034703938304035393\n",
      "Iteration: 2774 Loss: 0.000346733143860713\n",
      "Iteration: 2775 Loss: 0.0003464271770654435\n",
      "Iteration: 2776 Loss: 0.0003461214824094551\n",
      "Iteration: 2777 Loss: 0.0003458160596484856\n",
      "Iteration: 2778 Loss: 0.00034551090853768957\n",
      "Iteration: 2779 Loss: 0.0003452060288330612\n",
      "Iteration: 2780 Loss: 0.00034490142029051924\n",
      "Iteration: 2781 Loss: 0.0003445970826663005\n",
      "Iteration: 2782 Loss: 0.0003442930157168205\n",
      "Iteration: 2783 Loss: 0.00034398921919875957\n",
      "Iteration: 2784 Loss: 0.00034368569286864265\n",
      "Iteration: 2785 Loss: 0.00034338243648408297\n",
      "Iteration: 2786 Loss: 0.000343079449802034\n",
      "Iteration: 2787 Loss: 0.00034277673258001665\n",
      "Iteration: 2788 Loss: 0.00034247428457586017\n",
      "Iteration: 2789 Loss: 0.0003421721055475327\n",
      "Iteration: 2790 Loss: 0.0003418701952534687\n",
      "Iteration: 2791 Loss: 0.00034156855345214993\n",
      "Iteration: 2792 Loss: 0.00034126717990194303\n",
      "Iteration: 2793 Loss: 0.0003409660743615705\n",
      "Iteration: 2794 Loss: 0.00034066523659056797\n",
      "Iteration: 2795 Loss: 0.00034036466634791674\n",
      "Iteration: 2796 Loss: 0.0003400643633931448\n",
      "Iteration: 2797 Loss: 0.00033976432748616573\n",
      "Iteration: 2798 Loss: 0.00033946455838658636\n",
      "Iteration: 2799 Loss: 0.00033916505585506775\n",
      "Iteration: 2800 Loss: 0.00033886581965144035\n",
      "Iteration: 2801 Loss: 0.0003385668495366372\n",
      "Iteration: 2802 Loss: 0.00033826814527194527\n",
      "Iteration: 2803 Loss: 0.00033796970661770427\n",
      "Iteration: 2804 Loss: 0.00033767153333547794\n",
      "Iteration: 2805 Loss: 0.0003373736251864851\n",
      "Iteration: 2806 Loss: 0.00033707598193286645\n",
      "Iteration: 2807 Loss: 0.0003367786033364905\n",
      "Iteration: 2808 Loss: 0.0003364814891593918\n",
      "Iteration: 2809 Loss: 0.00033618463916379964\n",
      "Iteration: 2810 Loss: 0.0003358880531120473\n",
      "Iteration: 2811 Loss: 0.00033559173076780645\n",
      "Iteration: 2812 Loss: 0.0003352956718934235\n",
      "Iteration: 2813 Loss: 0.00033499987625217607\n",
      "Iteration: 2814 Loss: 0.00033470434360776953\n",
      "Iteration: 2815 Loss: 0.00033440907372355187\n",
      "Iteration: 2816 Loss: 0.0003341140663634244\n",
      "Iteration: 2817 Loss: 0.00033381932129168196\n",
      "Iteration: 2818 Loss: 0.0003335248382724229\n",
      "Iteration: 2819 Loss: 0.00033323061707018413\n",
      "Iteration: 2820 Loss: 0.00033293665744983214\n",
      "Iteration: 2821 Loss: 0.0003326429591762266\n",
      "Iteration: 2822 Loss: 0.0003323495220144232\n",
      "Iteration: 2823 Loss: 0.00033205634572985615\n",
      "Iteration: 2824 Loss: 0.00033176343008848566\n",
      "Iteration: 2825 Loss: 0.00033147077485567837\n",
      "Iteration: 2826 Loss: 0.00033117837979759627\n",
      "Iteration: 2827 Loss: 0.00033088624468028213\n",
      "Iteration: 2828 Loss: 0.0003305943692704989\n",
      "Iteration: 2829 Loss: 0.00033030275333488377\n",
      "Iteration: 2830 Loss: 0.0003300113966400358\n",
      "Iteration: 2831 Loss: 0.00032972029895337945\n",
      "Iteration: 2832 Loss: 0.0003294294600414479\n",
      "Iteration: 2833 Loss: 0.0003291388796726673\n",
      "Iteration: 2834 Loss: 0.0003288485576143398\n",
      "Iteration: 2835 Loss: 0.0003285584936340533\n",
      "Iteration: 2836 Loss: 0.0003282686875006262\n",
      "Iteration: 2837 Loss: 0.00032797913898227637\n",
      "Iteration: 2838 Loss: 0.00032768984784726656\n",
      "Iteration: 2839 Loss: 0.0003274008138643056\n",
      "Iteration: 2840 Loss: 0.00032711203680279503\n",
      "Iteration: 2841 Loss: 0.00032682351643153107\n",
      "Iteration: 2842 Loss: 0.0003265352525202381\n",
      "Iteration: 2843 Loss: 0.0003262472448382332\n",
      "Iteration: 2844 Loss: 0.00032595949315547356\n",
      "Iteration: 2845 Loss: 0.00032567199724192964\n",
      "Iteration: 2846 Loss: 0.0003253847568674761\n",
      "Iteration: 2847 Loss: 0.00032509777180345014\n",
      "Iteration: 2848 Loss: 0.0003248110418199314\n",
      "Iteration: 2849 Loss: 0.0003245245666879217\n",
      "Iteration: 2850 Loss: 0.0003242383461785015\n",
      "Iteration: 2851 Loss: 0.00032395238006294695\n",
      "Iteration: 2852 Loss: 0.0003236666681126215\n",
      "Iteration: 2853 Loss: 0.00032338121009939364\n",
      "Iteration: 2854 Loss: 0.00032309600579524263\n",
      "Iteration: 2855 Loss: 0.0003228110549719694\n",
      "Iteration: 2856 Loss: 0.0003225263574020922\n",
      "Iteration: 2857 Loss: 0.00032224191285826393\n",
      "Iteration: 2858 Loss: 0.00032195772111316256\n",
      "Iteration: 2859 Loss: 0.00032167378193959177\n",
      "Iteration: 2860 Loss: 0.0003213900951106909\n",
      "Iteration: 2861 Loss: 0.0003211066603998742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2862 Loss: 0.00032082347758127463\n",
      "Iteration: 2863 Loss: 0.00032054054642760305\n",
      "Iteration: 2864 Loss: 0.0003202578667133629\n",
      "Iteration: 2865 Loss: 0.00031997543821287594\n",
      "Iteration: 2866 Loss: 0.0003196932607004072\n",
      "Iteration: 2867 Loss: 0.0003194113339505917\n",
      "Iteration: 2868 Loss: 0.00031912965773796097\n",
      "Iteration: 2869 Loss: 0.00031884823183763193\n",
      "Iteration: 2870 Loss: 0.0003185670560248914\n",
      "Iteration: 2871 Loss: 0.0003182861300751841\n",
      "Iteration: 2872 Loss: 0.00031800545376419376\n",
      "Iteration: 2873 Loss: 0.0003177250268675966\n",
      "Iteration: 2874 Loss: 0.0003174448491612772\n",
      "Iteration: 2875 Loss: 0.00031716492042150126\n",
      "Iteration: 2876 Loss: 0.00031688524042492393\n",
      "Iteration: 2877 Loss: 0.000316605808948134\n",
      "Iteration: 2878 Loss: 0.00031632662576806125\n",
      "Iteration: 2879 Loss: 0.00031604769066151666\n",
      "Iteration: 2880 Loss: 0.00031576900340569676\n",
      "Iteration: 2881 Loss: 0.00031549056377822027\n",
      "Iteration: 2882 Loss: 0.0003152123715568655\n",
      "Iteration: 2883 Loss: 0.00031493442651930865\n",
      "Iteration: 2884 Loss: 0.00031465672844373563\n",
      "Iteration: 2885 Loss: 0.00031437927710809455\n",
      "Iteration: 2886 Loss: 0.0003141020722911505\n",
      "Iteration: 2887 Loss: 0.0003138251137712609\n",
      "Iteration: 2888 Loss: 0.00031354840132752197\n",
      "Iteration: 2889 Loss: 0.0003132719347388672\n",
      "Iteration: 2890 Loss: 0.00031299571378451256\n",
      "Iteration: 2891 Loss: 0.0003127197382442794\n",
      "Iteration: 2892 Loss: 0.00031244400789746056\n",
      "Iteration: 2893 Loss: 0.0003121685225238414\n",
      "Iteration: 2894 Loss: 0.0003118932819036818\n",
      "Iteration: 2895 Loss: 0.0003116182858170849\n",
      "Iteration: 2896 Loss: 0.00031134353404475303\n",
      "Iteration: 2897 Loss: 0.0003110690263672122\n",
      "Iteration: 2898 Loss: 0.0003107947625654064\n",
      "Iteration: 2899 Loss: 0.00031052074242021325\n",
      "Iteration: 2900 Loss: 0.00031024696571289086\n",
      "Iteration: 2901 Loss: 0.0003099734322252526\n",
      "Iteration: 2902 Loss: 0.0003097001417385048\n",
      "Iteration: 2903 Loss: 0.0003094270940351161\n",
      "Iteration: 2904 Loss: 0.000309154288896365\n",
      "Iteration: 2905 Loss: 0.0003088817261049278\n",
      "Iteration: 2906 Loss: 0.0003086094054431307\n",
      "Iteration: 2907 Loss: 0.00030833732669365027\n",
      "Iteration: 2908 Loss: 0.0003080654896393879\n",
      "Iteration: 2909 Loss: 0.00030779389406315104\n",
      "Iteration: 2910 Loss: 0.0003075225397484952\n",
      "Iteration: 2911 Loss: 0.0003072514264786482\n",
      "Iteration: 2912 Loss: 0.00030698055403730084\n",
      "Iteration: 2913 Loss: 0.00030670992220834567\n",
      "Iteration: 2914 Loss: 0.00030643953077581104\n",
      "Iteration: 2915 Loss: 0.000306169379523892\n",
      "Iteration: 2916 Loss: 0.00030589946823684436\n",
      "Iteration: 2917 Loss: 0.00030562979669923124\n",
      "Iteration: 2918 Loss: 0.0003053603646958704\n",
      "Iteration: 2919 Loss: 0.00030509117201209934\n",
      "Iteration: 2920 Loss: 0.0003048222184330553\n",
      "Iteration: 2921 Loss: 0.0003045535037439095\n",
      "Iteration: 2922 Loss: 0.00030428502773011893\n",
      "Iteration: 2923 Loss: 0.00030401679017761485\n",
      "Iteration: 2924 Loss: 0.00030374879087236045\n",
      "Iteration: 2925 Loss: 0.0003034810296008331\n",
      "Iteration: 2926 Loss: 0.0003032135061488548\n",
      "Iteration: 2927 Loss: 0.0003029462203034219\n",
      "Iteration: 2928 Loss: 0.0003026791718515142\n",
      "Iteration: 2929 Loss: 0.0003024123605793079\n",
      "Iteration: 2930 Loss: 0.00030214578627440136\n",
      "Iteration: 2931 Loss: 0.0003018794487240612\n",
      "Iteration: 2932 Loss: 0.0003016133477156955\n",
      "Iteration: 2933 Loss: 0.00030134748303726154\n",
      "Iteration: 2934 Loss: 0.0003010818544764241\n",
      "Iteration: 2935 Loss: 0.0003008164618215804\n",
      "Iteration: 2936 Loss: 0.00030055130486095384\n",
      "Iteration: 2937 Loss: 0.00030028638338254287\n",
      "Iteration: 2938 Loss: 0.00030002169717538815\n",
      "Iteration: 2939 Loss: 0.0002997572460284972\n",
      "Iteration: 2940 Loss: 0.0002994930297308321\n",
      "Iteration: 2941 Loss: 0.00029922904807180444\n",
      "Iteration: 2942 Loss: 0.00029896530084037566\n",
      "Iteration: 2943 Loss: 0.00029870178782661174\n",
      "Iteration: 2944 Loss: 0.00029843850882010816\n",
      "Iteration: 2945 Loss: 0.000298175463610825\n",
      "Iteration: 2946 Loss: 0.000297912651989427\n",
      "Iteration: 2947 Loss: 0.0002976500737457192\n",
      "Iteration: 2948 Loss: 0.00029738772867062143\n",
      "Iteration: 2949 Loss: 0.0002971256165548396\n",
      "Iteration: 2950 Loss: 0.0002968637371892611\n",
      "Iteration: 2951 Loss: 0.0002966020903651538\n",
      "Iteration: 2952 Loss: 0.00029634067587360483\n",
      "Iteration: 2953 Loss: 0.00029607949350665653\n",
      "Iteration: 2954 Loss: 0.0002958185430559255\n",
      "Iteration: 2955 Loss: 0.00029555782431312183\n",
      "Iteration: 2956 Loss: 0.0002952973370701354\n",
      "Iteration: 2957 Loss: 0.0002950370811196239\n",
      "Iteration: 2958 Loss: 0.00029477705625396294\n",
      "Iteration: 2959 Loss: 0.0002945172622659484\n",
      "Iteration: 2960 Loss: 0.0002942576989481365\n",
      "Iteration: 2961 Loss: 0.00029399836609375553\n",
      "Iteration: 2962 Loss: 0.00029373926349619237\n",
      "Iteration: 2963 Loss: 0.00029348039094841165\n",
      "Iteration: 2964 Loss: 0.0002932217482443638\n",
      "Iteration: 2965 Loss: 0.00029296333517777636\n",
      "Iteration: 2966 Loss: 0.0002927051515425958\n",
      "Iteration: 2967 Loss: 0.00029244719713304436\n",
      "Iteration: 2968 Loss: 0.00029218947174324304\n",
      "Iteration: 2969 Loss: 0.0002919319751681892\n",
      "Iteration: 2970 Loss: 0.0002916747072022212\n",
      "Iteration: 2971 Loss: 0.00029141766764027974\n",
      "Iteration: 2972 Loss: 0.0002911608562774561\n",
      "Iteration: 2973 Loss: 0.00029090427290911076\n",
      "Iteration: 2974 Loss: 0.00029064791733070147\n",
      "Iteration: 2975 Loss: 0.00029039178933787515\n",
      "Iteration: 2976 Loss: 0.0002901358887265654\n",
      "Iteration: 2977 Loss: 0.00028988021529275455\n",
      "Iteration: 2978 Loss: 0.0002896247688326094\n",
      "Iteration: 2979 Loss: 0.0002893695491425417\n",
      "Iteration: 2980 Loss: 0.00028911455601902676\n",
      "Iteration: 2981 Loss: 0.00028885978925942976\n",
      "Iteration: 2982 Loss: 0.00028860524866010964\n",
      "Iteration: 2983 Loss: 0.0002883509340179505\n",
      "Iteration: 2984 Loss: 0.00028809684513069015\n",
      "Iteration: 2985 Loss: 0.0002878429817959015\n",
      "Iteration: 2986 Loss: 0.0002875893438109866\n",
      "Iteration: 2987 Loss: 0.00028733593097368047\n",
      "Iteration: 2988 Loss: 0.00028708274308257103\n",
      "Iteration: 2989 Loss: 0.00028682977993565735\n",
      "Iteration: 2990 Loss: 0.0002865770413312408\n",
      "Iteration: 2991 Loss: 0.00028632452706780527\n",
      "Iteration: 2992 Loss: 0.0002860722369442665\n",
      "Iteration: 2993 Loss: 0.0002858201707595544\n",
      "Iteration: 2994 Loss: 0.00028556832831320576\n",
      "Iteration: 2995 Loss: 0.0002853167094037763\n",
      "Iteration: 2996 Loss: 0.00028506531383144004\n",
      "Iteration: 2997 Loss: 0.0002848141413958619\n",
      "Iteration: 2998 Loss: 0.00028456319189670154\n",
      "Iteration: 2999 Loss: 0.0002843124651336482\n",
      "Iteration: 3000 Loss: 0.00028406196090747295\n",
      "Iteration: 3001 Loss: 0.0002838116790185159\n",
      "Iteration: 3002 Loss: 0.0002835616192672181\n",
      "Iteration: 3003 Loss: 0.00028331178145448376\n",
      "Iteration: 3004 Loss: 0.00028306216538104176\n",
      "Iteration: 3005 Loss: 0.0002828127708480753\n",
      "Iteration: 3006 Loss: 0.00028256359765696146\n",
      "Iteration: 3007 Loss: 0.000282314645609199\n",
      "Iteration: 3008 Loss: 0.00028206591450687275\n",
      "Iteration: 3009 Loss: 0.0002818174041512624\n",
      "Iteration: 3010 Loss: 0.0002815691143446103\n",
      "Iteration: 3011 Loss: 0.00028132104488901304\n",
      "Iteration: 3012 Loss: 0.00028107319558729154\n",
      "Iteration: 3013 Loss: 0.000280825566241598\n",
      "Iteration: 3014 Loss: 0.0002805781566545355\n",
      "Iteration: 3015 Loss: 0.00028033096662962657\n",
      "Iteration: 3016 Loss: 0.0002800839959696545\n",
      "Iteration: 3017 Loss: 0.0002798372444779659\n",
      "Iteration: 3018 Loss: 0.00027959071195761463\n",
      "Iteration: 3019 Loss: 0.00027934439821290444\n",
      "Iteration: 3020 Loss: 0.000279098303047132\n",
      "Iteration: 3021 Loss: 0.00027885242626436295\n",
      "Iteration: 3022 Loss: 0.00027860676766920667\n",
      "Iteration: 3023 Loss: 0.00027836132706588677\n",
      "Iteration: 3024 Loss: 0.00027811610425857813\n",
      "Iteration: 3025 Loss: 0.0002778710990519274\n",
      "Iteration: 3026 Loss: 0.0002776263112513282\n",
      "Iteration: 3027 Loss: 0.00027738174066138096\n",
      "Iteration: 3028 Loss: 0.00027713738708738694\n",
      "Iteration: 3029 Loss: 0.000276893250334902\n",
      "Iteration: 3030 Loss: 0.00027664933020971495\n",
      "Iteration: 3031 Loss: 0.00027640562651725476\n",
      "Iteration: 3032 Loss: 0.0002761621390638171\n",
      "Iteration: 3033 Loss: 0.0002759188676553501\n",
      "Iteration: 3034 Loss: 0.00027567581209797043\n",
      "Iteration: 3035 Loss: 0.00027543297219818506\n",
      "Iteration: 3036 Loss: 0.0002751903477628663\n",
      "Iteration: 3037 Loss: 0.0002749479385986956\n",
      "Iteration: 3038 Loss: 0.0002747057445127593\n",
      "Iteration: 3039 Loss: 0.0002744637653117867\n",
      "Iteration: 3040 Loss: 0.000274222000803417\n",
      "Iteration: 3041 Loss: 0.00027398045079538345\n",
      "Iteration: 3042 Loss: 0.0002737391150950348\n",
      "Iteration: 3043 Loss: 0.00027349799351040766\n",
      "Iteration: 3044 Loss: 0.0002732570858497689\n",
      "Iteration: 3045 Loss: 0.00027301639192110874\n",
      "Iteration: 3046 Loss: 0.0002727759115327741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3047 Loss: 0.00027253564449328786\n",
      "Iteration: 3048 Loss: 0.0002722955906113466\n",
      "Iteration: 3049 Loss: 0.0002720557496959564\n",
      "Iteration: 3050 Loss: 0.00027181612155629875\n",
      "Iteration: 3051 Loss: 0.00027157670600139574\n",
      "Iteration: 3052 Loss: 0.00027133750284055405\n",
      "Iteration: 3053 Loss: 0.00027109851188400296\n",
      "Iteration: 3054 Loss: 0.00027085973294090326\n",
      "Iteration: 3055 Loss: 0.00027062116582188343\n",
      "Iteration: 3056 Loss: 0.00027038281033623697\n",
      "Iteration: 3057 Loss: 0.00027014466629465454\n",
      "Iteration: 3058 Loss: 0.0002699067335076652\n",
      "Iteration: 3059 Loss: 0.0002696690117855085\n",
      "Iteration: 3060 Loss: 0.0002694315009390501\n",
      "Iteration: 3061 Loss: 0.00026919420077972596\n",
      "Iteration: 3062 Loss: 0.00026895711111818877\n",
      "Iteration: 3063 Loss: 0.000268720231766141\n",
      "Iteration: 3064 Loss: 0.00026848356253487114\n",
      "Iteration: 3065 Loss: 0.0002682471032361252\n",
      "Iteration: 3066 Loss: 0.0002680108536813765\n",
      "Iteration: 3067 Loss: 0.0002677748136829864\n",
      "Iteration: 3068 Loss: 0.0002675389830532359\n",
      "Iteration: 3069 Loss: 0.0002673033616042411\n",
      "Iteration: 3070 Loss: 0.00026706794914865904\n",
      "Iteration: 3071 Loss: 0.00026683274549886056\n",
      "Iteration: 3072 Loss: 0.00026659775046803844\n",
      "Iteration: 3073 Loss: 0.0002663629638687451\n",
      "Iteration: 3074 Loss: 0.000266128385514303\n",
      "Iteration: 3075 Loss: 0.0002658940152183006\n",
      "Iteration: 3076 Loss: 0.000265659852794154\n",
      "Iteration: 3077 Loss: 0.0002654258980552678\n",
      "Iteration: 3078 Loss: 0.00026519215081616975\n",
      "Iteration: 3079 Loss: 0.00026495861089038177\n",
      "Iteration: 3080 Loss: 0.00026472527809217065\n",
      "Iteration: 3081 Loss: 0.000264492152235851\n",
      "Iteration: 3082 Loss: 0.0002642592331363097\n",
      "Iteration: 3083 Loss: 0.000264026520607986\n",
      "Iteration: 3084 Loss: 0.0002637940144660529\n",
      "Iteration: 3085 Loss: 0.000263561714524875\n",
      "Iteration: 3086 Loss: 0.00026332962059989756\n",
      "Iteration: 3087 Loss: 0.0002630977325065254\n",
      "Iteration: 3088 Loss: 0.0002628660500602976\n",
      "Iteration: 3089 Loss: 0.00026263457307708116\n",
      "Iteration: 3090 Loss: 0.00026240330137261895\n",
      "Iteration: 3091 Loss: 0.00026217223476287903\n",
      "Iteration: 3092 Loss: 0.00026194137306418075\n",
      "Iteration: 3093 Loss: 0.0002617107160928221\n",
      "Iteration: 3094 Loss: 0.0002614802636658256\n",
      "Iteration: 3095 Loss: 0.00026125001559938575\n",
      "Iteration: 3096 Loss: 0.00026101997171039116\n",
      "Iteration: 3097 Loss: 0.0002607901318155055\n",
      "Iteration: 3098 Loss: 0.00026056049573224113\n",
      "Iteration: 3099 Loss: 0.00026033106327792905\n",
      "Iteration: 3100 Loss: 0.000260101834270365\n",
      "Iteration: 3101 Loss: 0.0002598728085273621\n",
      "Iteration: 3102 Loss: 0.00025964398586642506\n",
      "Iteration: 3103 Loss: 0.0002594153661054807\n",
      "Iteration: 3104 Loss: 0.00025918694906266804\n",
      "Iteration: 3105 Loss: 0.00025895873455654303\n",
      "Iteration: 3106 Loss: 0.00025873072240565156\n",
      "Iteration: 3107 Loss: 0.0002585029124288502\n",
      "Iteration: 3108 Loss: 0.0002582753044447936\n",
      "Iteration: 3109 Loss: 0.0002580478982721921\n",
      "Iteration: 3110 Loss: 0.0002578206937304357\n",
      "Iteration: 3111 Loss: 0.0002575936906388255\n",
      "Iteration: 3112 Loss: 0.00025736688881704387\n",
      "Iteration: 3113 Loss: 0.00025714028808464153\n",
      "Iteration: 3114 Loss: 0.0002569138882615534\n",
      "Iteration: 3115 Loss: 0.000256687689167497\n",
      "Iteration: 3116 Loss: 0.0002564616906227846\n",
      "Iteration: 3117 Loss: 0.00025623589244771084\n",
      "Iteration: 3118 Loss: 0.00025601029446252455\n",
      "Iteration: 3119 Loss: 0.000255784896488053\n",
      "Iteration: 3120 Loss: 0.0002555596983453148\n",
      "Iteration: 3121 Loss: 0.0002553346998550432\n",
      "Iteration: 3122 Loss: 0.0002551099008383973\n",
      "Iteration: 3123 Loss: 0.0002548853011165414\n",
      "Iteration: 3124 Loss: 0.0002546609005109386\n",
      "Iteration: 3125 Loss: 0.0002544366988432202\n",
      "Iteration: 3126 Loss: 0.0002542126959354295\n",
      "Iteration: 3127 Loss: 0.0002539888916090881\n",
      "Iteration: 3128 Loss: 0.0002537652856864024\n",
      "Iteration: 3129 Loss: 0.0002535418779898352\n",
      "Iteration: 3130 Loss: 0.00025331866834170147\n",
      "Iteration: 3131 Loss: 0.00025309565656486265\n",
      "Iteration: 3132 Loss: 0.0002528728424812705\n",
      "Iteration: 3133 Loss: 0.00025265022591427176\n",
      "Iteration: 3134 Loss: 0.00025242780668660955\n",
      "Iteration: 3135 Loss: 0.0002522055846219808\n",
      "Iteration: 3136 Loss: 0.00025198355954373405\n",
      "Iteration: 3137 Loss: 0.00025176173127492893\n",
      "Iteration: 3138 Loss: 0.0002515400996395078\n",
      "Iteration: 3139 Loss: 0.0002513186644613509\n",
      "Iteration: 3140 Loss: 0.00025109742556481655\n",
      "Iteration: 3141 Loss: 0.0002508763827736919\n",
      "Iteration: 3142 Loss: 0.00025065553591204425\n",
      "Iteration: 3143 Loss: 0.00025043488480513195\n",
      "Iteration: 3144 Loss: 0.0002502144292769633\n",
      "Iteration: 3145 Loss: 0.0002499941691524976\n",
      "Iteration: 3146 Loss: 0.00024977410425678346\n",
      "Iteration: 3147 Loss: 0.000249554234414229\n",
      "Iteration: 3148 Loss: 0.0002493345594509518\n",
      "Iteration: 3149 Loss: 0.00024911507919227226\n",
      "Iteration: 3150 Loss: 0.0002488957934636035\n",
      "Iteration: 3151 Loss: 0.00024867670209077585\n",
      "Iteration: 3152 Loss: 0.0002484578048997138\n",
      "Iteration: 3153 Loss: 0.00024823910171644693\n",
      "Iteration: 3154 Loss: 0.00024802059236754017\n",
      "Iteration: 3155 Loss: 0.00024780227667893674\n",
      "Iteration: 3156 Loss: 0.0002475841544770617\n",
      "Iteration: 3157 Loss: 0.00024736622558864304\n",
      "Iteration: 3158 Loss: 0.00024714848984082436\n",
      "Iteration: 3159 Loss: 0.00024693094706045894\n",
      "Iteration: 3160 Loss: 0.000246713597074822\n",
      "Iteration: 3161 Loss: 0.00024649643971088805\n",
      "Iteration: 3162 Loss: 0.00024627947479644697\n",
      "Iteration: 3163 Loss: 0.000246062702159581\n",
      "Iteration: 3164 Loss: 0.00024584612162723035\n",
      "Iteration: 3165 Loss: 0.00024562973302772357\n",
      "Iteration: 3166 Loss: 0.00024541353618896594\n",
      "Iteration: 3167 Loss: 0.0002451975309395907\n",
      "Iteration: 3168 Loss: 0.0002449817171077562\n",
      "Iteration: 3169 Loss: 0.000244766094521776\n",
      "Iteration: 3170 Loss: 0.0002445506630107447\n",
      "Iteration: 3171 Loss: 0.0002443354224034786\n",
      "Iteration: 3172 Loss: 0.000244120372528867\n",
      "Iteration: 3173 Loss: 0.00024390551321629077\n",
      "Iteration: 3174 Loss: 0.00024369084429469887\n",
      "Iteration: 3175 Loss: 0.00024347636559375095\n",
      "Iteration: 3176 Loss: 0.00024326207694321225\n",
      "Iteration: 3177 Loss: 0.00024304797817308773\n",
      "Iteration: 3178 Loss: 0.00024283406911266995\n",
      "Iteration: 3179 Loss: 0.0002426203495925769\n",
      "Iteration: 3180 Loss: 0.00024240681944327316\n",
      "Iteration: 3181 Loss: 0.00024219347849473868\n",
      "Iteration: 3182 Loss: 0.00024198032657754928\n",
      "Iteration: 3183 Loss: 0.0002417673635226225\n",
      "Iteration: 3184 Loss: 0.0002415545891607175\n",
      "Iteration: 3185 Loss: 0.0002413420033229529\n",
      "Iteration: 3186 Loss: 0.0002411296058402395\n",
      "Iteration: 3187 Loss: 0.00024091739654417404\n",
      "Iteration: 3188 Loss: 0.00024070537526606979\n",
      "Iteration: 3189 Loss: 0.00024049354183764757\n",
      "Iteration: 3190 Loss: 0.00024028189609054314\n",
      "Iteration: 3191 Loss: 0.0002400704378569939\n",
      "Iteration: 3192 Loss: 0.0002398591669689993\n",
      "Iteration: 3193 Loss: 0.00023964808325857857\n",
      "Iteration: 3194 Loss: 0.00023943718655830578\n",
      "Iteration: 3195 Loss: 0.00023922647670066954\n",
      "Iteration: 3196 Loss: 0.00023901595351849816\n",
      "Iteration: 3197 Loss: 0.0002388056168447392\n",
      "Iteration: 3198 Loss: 0.00023859546651180992\n",
      "Iteration: 3199 Loss: 0.00023838550235354608\n",
      "Iteration: 3200 Loss: 0.00023817572420293254\n",
      "Iteration: 3201 Loss: 0.00023796613189316102\n",
      "Iteration: 3202 Loss: 0.00023775672525782948\n",
      "Iteration: 3203 Loss: 0.0002375475041313217\n",
      "Iteration: 3204 Loss: 0.0002373384683470336\n",
      "Iteration: 3205 Loss: 0.00023712961773922535\n",
      "Iteration: 3206 Loss: 0.00023692095214177883\n",
      "Iteration: 3207 Loss: 0.00023671247138897932\n",
      "Iteration: 3208 Loss: 0.00023650417531589552\n",
      "Iteration: 3209 Loss: 0.00023629606375683953\n",
      "Iteration: 3210 Loss: 0.00023608813654679166\n",
      "Iteration: 3211 Loss: 0.00023588039352066833\n",
      "Iteration: 3212 Loss: 0.00023567283451322705\n",
      "Iteration: 3213 Loss: 0.00023546545935993106\n",
      "Iteration: 3214 Loss: 0.00023525826789614856\n",
      "Iteration: 3215 Loss: 0.00023505125995738337\n",
      "Iteration: 3216 Loss: 0.00023484443537963363\n",
      "Iteration: 3217 Loss: 0.00023463779399834304\n",
      "Iteration: 3218 Loss: 0.00023443133564949036\n",
      "Iteration: 3219 Loss: 0.0002342250601694694\n",
      "Iteration: 3220 Loss: 0.00023401896739444884\n",
      "Iteration: 3221 Loss: 0.00023381305716082095\n",
      "Iteration: 3222 Loss: 0.00023360732930561299\n",
      "Iteration: 3223 Loss: 0.0002334017836649538\n",
      "Iteration: 3224 Loss: 0.00023319642007595535\n",
      "Iteration: 3225 Loss: 0.00023299123837549444\n",
      "Iteration: 3226 Loss: 0.0002327862384008766\n",
      "Iteration: 3227 Loss: 0.00023258141998923855\n",
      "Iteration: 3228 Loss: 0.00023237678297838527\n",
      "Iteration: 3229 Loss: 0.0002321723272056719\n",
      "Iteration: 3230 Loss: 0.00023196805250868012\n",
      "Iteration: 3231 Loss: 0.00023176395872557604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3232 Loss: 0.00023156004569451226\n",
      "Iteration: 3233 Loss: 0.00023135631325359055\n",
      "Iteration: 3234 Loss: 0.00023115276124117032\n",
      "Iteration: 3235 Loss: 0.00023094938949535395\n",
      "Iteration: 3236 Loss: 0.00023074619785500694\n",
      "Iteration: 3237 Loss: 0.00023054318615904894\n",
      "Iteration: 3238 Loss: 0.00023034035424606252\n",
      "Iteration: 3239 Loss: 0.00023013770195558247\n",
      "Iteration: 3240 Loss: 0.0002299352291268454\n",
      "Iteration: 3241 Loss: 0.00022973293559874433\n",
      "Iteration: 3242 Loss: 0.00022953082121109872\n",
      "Iteration: 3243 Loss: 0.0002293288858035885\n",
      "Iteration: 3244 Loss: 0.00022912712921565942\n",
      "Iteration: 3245 Loss: 0.00022892555128757198\n",
      "Iteration: 3246 Loss: 0.00022872415185933742\n",
      "Iteration: 3247 Loss: 0.00022852293077099572\n",
      "Iteration: 3248 Loss: 0.00022832188786324568\n",
      "Iteration: 3249 Loss: 0.00022812102297636635\n",
      "Iteration: 3250 Loss: 0.00022792033595118094\n",
      "Iteration: 3251 Loss: 0.00022771982662845178\n",
      "Iteration: 3252 Loss: 0.00022751949484918352\n",
      "Iteration: 3253 Loss: 0.00022731934045414285\n",
      "Iteration: 3254 Loss: 0.00022711936328473359\n",
      "Iteration: 3255 Loss: 0.00022691956318240865\n",
      "Iteration: 3256 Loss: 0.00022671993998857047\n",
      "Iteration: 3257 Loss: 0.00022652049354512753\n",
      "Iteration: 3258 Loss: 0.00022632122369360807\n",
      "Iteration: 3259 Loss: 0.00022612213027601276\n",
      "Iteration: 3260 Loss: 0.00022592321313456433\n",
      "Iteration: 3261 Loss: 0.00022572447211145397\n",
      "Iteration: 3262 Loss: 0.00022552590704902738\n",
      "Iteration: 3263 Loss: 0.0002253275177898726\n",
      "Iteration: 3264 Loss: 0.000225129304176604\n",
      "Iteration: 3265 Loss: 0.00022493126605162915\n",
      "Iteration: 3266 Loss: 0.0002247334032582397\n",
      "Iteration: 3267 Loss: 0.00022453571563999276\n",
      "Iteration: 3268 Loss: 0.00022433820303934064\n",
      "Iteration: 3269 Loss: 0.00022414086529987915\n",
      "Iteration: 3270 Loss: 0.00022394370226499984\n",
      "Iteration: 3271 Loss: 0.00022374671377863508\n",
      "Iteration: 3272 Loss: 0.00022354989968482105\n",
      "Iteration: 3273 Loss: 0.0002233532598270051\n",
      "Iteration: 3274 Loss: 0.00022315679404945782\n",
      "Iteration: 3275 Loss: 0.00022296050219630613\n",
      "Iteration: 3276 Loss: 0.00022276438411223927\n",
      "Iteration: 3277 Loss: 0.0002225684396414862\n",
      "Iteration: 3278 Loss: 0.00022237266862849435\n",
      "Iteration: 3279 Loss: 0.00022217707091823154\n",
      "Iteration: 3280 Loss: 0.00022198164635540765\n",
      "Iteration: 3281 Loss: 0.00022178639478559627\n",
      "Iteration: 3282 Loss: 0.00022159131605372937\n",
      "Iteration: 3283 Loss: 0.0002213964100048373\n",
      "Iteration: 3284 Loss: 0.00022120167648435952\n",
      "Iteration: 3285 Loss: 0.00022100711533867842\n",
      "Iteration: 3286 Loss: 0.00022081272641326922\n",
      "Iteration: 3287 Loss: 0.00022061850955355393\n",
      "Iteration: 3288 Loss: 0.00022042446460574379\n",
      "Iteration: 3289 Loss: 0.00022023059141590263\n",
      "Iteration: 3290 Loss: 0.00022003688983082267\n",
      "Iteration: 3291 Loss: 0.00021984335969644695\n",
      "Iteration: 3292 Loss: 0.00021965000085971562\n",
      "Iteration: 3293 Loss: 0.00021945681316706822\n",
      "Iteration: 3294 Loss: 0.00021926379646565694\n",
      "Iteration: 3295 Loss: 0.00021907095060215884\n",
      "Iteration: 3296 Loss: 0.00021887827542385187\n",
      "Iteration: 3297 Loss: 0.00021868577077792446\n",
      "Iteration: 3298 Loss: 0.00021849343651227595\n",
      "Iteration: 3299 Loss: 0.00021830127247397037\n",
      "Iteration: 3300 Loss: 0.00021810927851092184\n",
      "Iteration: 3301 Loss: 0.00021791745447072766\n",
      "Iteration: 3302 Loss: 0.00021772580020156768\n",
      "Iteration: 3303 Loss: 0.00021753431555145752\n",
      "Iteration: 3304 Loss: 0.00021734300036859924\n",
      "Iteration: 3305 Loss: 0.00021715185450143568\n",
      "Iteration: 3306 Loss: 0.00021696087779826716\n",
      "Iteration: 3307 Loss: 0.00021677007010827072\n",
      "Iteration: 3308 Loss: 0.00021657943127986775\n",
      "Iteration: 3309 Loss: 0.00021638896116191517\n",
      "Iteration: 3310 Loss: 0.00021619865960348722\n",
      "Iteration: 3311 Loss: 0.0002160085264543095\n",
      "Iteration: 3312 Loss: 0.00021581856156309333\n",
      "Iteration: 3313 Loss: 0.00021562876477940825\n",
      "Iteration: 3314 Loss: 0.00021543913595298235\n",
      "Iteration: 3315 Loss: 0.00021524967493340415\n",
      "Iteration: 3316 Loss: 0.00021506038157051161\n",
      "Iteration: 3317 Loss: 0.0002148712557143587\n",
      "Iteration: 3318 Loss: 0.00021468229721505888\n",
      "Iteration: 3319 Loss: 0.00021449350592337335\n",
      "Iteration: 3320 Loss: 0.0002143048816893114\n",
      "Iteration: 3321 Loss: 0.00021411642436325876\n",
      "Iteration: 3322 Loss: 0.00021392813379634323\n",
      "Iteration: 3323 Loss: 0.00021374000983903778\n",
      "Iteration: 3324 Loss: 0.0002135520523425292\n",
      "Iteration: 3325 Loss: 0.00021336426115758217\n",
      "Iteration: 3326 Loss: 0.00021317663613582103\n",
      "Iteration: 3327 Loss: 0.0002129891771283381\n",
      "Iteration: 3328 Loss: 0.0002128018839869015\n",
      "Iteration: 3329 Loss: 0.00021261475656272013\n",
      "Iteration: 3330 Loss: 0.0002124277947079776\n",
      "Iteration: 3331 Loss: 0.00021224099827412687\n",
      "Iteration: 3332 Loss: 0.00021205436711336985\n",
      "Iteration: 3333 Loss: 0.00021186790107778974\n",
      "Iteration: 3334 Loss: 0.00021168160001994497\n",
      "Iteration: 3335 Loss: 0.0002114954637922411\n",
      "Iteration: 3336 Loss: 0.00021130949224711445\n",
      "Iteration: 3337 Loss: 0.0002111236852372443\n",
      "Iteration: 3338 Loss: 0.0002109380426154135\n",
      "Iteration: 3339 Loss: 0.0002107525642345149\n",
      "Iteration: 3340 Loss: 0.00021056724994790487\n",
      "Iteration: 3341 Loss: 0.00021038209960837944\n",
      "Iteration: 3342 Loss: 0.0002101971130697878\n",
      "Iteration: 3343 Loss: 0.0002100122901852248\n",
      "Iteration: 3344 Loss: 0.0002098276308086455\n",
      "Iteration: 3345 Loss: 0.0002096431347934024\n",
      "Iteration: 3346 Loss: 0.0002094588019938552\n",
      "Iteration: 3347 Loss: 0.00020927463226410312\n",
      "Iteration: 3348 Loss: 0.00020909062545775164\n",
      "Iteration: 3349 Loss: 0.00020890678142895724\n",
      "Iteration: 3350 Loss: 0.00020872310003270322\n",
      "Iteration: 3351 Loss: 0.00020853958112335028\n",
      "Iteration: 3352 Loss: 0.0002083562245553455\n",
      "Iteration: 3353 Loss: 0.00020817303018376827\n",
      "Iteration: 3354 Loss: 0.00020798999786344644\n",
      "Iteration: 3355 Loss: 0.0002078071274492159\n",
      "Iteration: 3356 Loss: 0.00020762441879690035\n",
      "Iteration: 3357 Loss: 0.00020744187176154058\n",
      "Iteration: 3358 Loss: 0.0002072594861982721\n",
      "Iteration: 3359 Loss: 0.0002070772619629218\n",
      "Iteration: 3360 Loss: 0.000206895198911125\n",
      "Iteration: 3361 Loss: 0.00020671329689907295\n",
      "Iteration: 3362 Loss: 0.00020653155578210058\n",
      "Iteration: 3363 Loss: 0.00020634997541660823\n",
      "Iteration: 3364 Loss: 0.00020616855565900327\n",
      "Iteration: 3365 Loss: 0.00020598729636580143\n",
      "Iteration: 3366 Loss: 0.00020580619739289633\n",
      "Iteration: 3367 Loss: 0.0002056252585975506\n",
      "Iteration: 3368 Loss: 0.00020544447983606622\n",
      "Iteration: 3369 Loss: 0.0002052638609653212\n",
      "Iteration: 3370 Loss: 0.00020508340184272438\n",
      "Iteration: 3371 Loss: 0.00020490310232494893\n",
      "Iteration: 3372 Loss: 0.00020472296226969842\n",
      "Iteration: 3373 Loss: 0.00020454298153418517\n",
      "Iteration: 3374 Loss: 0.0002043631599761102\n",
      "Iteration: 3375 Loss: 0.00020418349745258754\n",
      "Iteration: 3376 Loss: 0.00020400399382161275\n",
      "Iteration: 3377 Loss: 0.0002038246489418548\n",
      "Iteration: 3378 Loss: 0.00020364546267027283\n",
      "Iteration: 3379 Loss: 0.0002034664348655872\n",
      "Iteration: 3380 Loss: 0.0002032875653861689\n",
      "Iteration: 3381 Loss: 0.00020310885409011387\n",
      "Iteration: 3382 Loss: 0.00020293030083619223\n",
      "Iteration: 3383 Loss: 0.00020275190548313938\n",
      "Iteration: 3384 Loss: 0.00020257366788995444\n",
      "Iteration: 3385 Loss: 0.00020239558791473614\n",
      "Iteration: 3386 Loss: 0.00020221766541737924\n",
      "Iteration: 3387 Loss: 0.00020203990025667554\n",
      "Iteration: 3388 Loss: 0.0002018622922922372\n",
      "Iteration: 3389 Loss: 0.00020168484138344983\n",
      "Iteration: 3390 Loss: 0.0002015075473897058\n",
      "Iteration: 3391 Loss: 0.00020133041017065455\n",
      "Iteration: 3392 Loss: 0.00020115342958616265\n",
      "Iteration: 3393 Loss: 0.0002009766054961698\n",
      "Iteration: 3394 Loss: 0.00020079993776100816\n",
      "Iteration: 3395 Loss: 0.0002006234262406724\n",
      "Iteration: 3396 Loss: 0.00020044707079550184\n",
      "Iteration: 3397 Loss: 0.00020027087128580753\n",
      "Iteration: 3398 Loss: 0.00020009482757232176\n",
      "Iteration: 3399 Loss: 0.00019991893951575067\n",
      "Iteration: 3400 Loss: 0.00019974320697674691\n",
      "Iteration: 3401 Loss: 0.00019956762981666868\n",
      "Iteration: 3402 Loss: 0.00019939220789633568\n",
      "Iteration: 3403 Loss: 0.00019921694107711895\n",
      "Iteration: 3404 Loss: 0.00019904182921993928\n",
      "Iteration: 3405 Loss: 0.00019886687218688347\n",
      "Iteration: 3406 Loss: 0.0001986920698387581\n",
      "Iteration: 3407 Loss: 0.00019851742203796184\n",
      "Iteration: 3408 Loss: 0.0001983429286460002\n",
      "Iteration: 3409 Loss: 0.00019816858952479677\n",
      "Iteration: 3410 Loss: 0.000197994404536746\n",
      "Iteration: 3411 Loss: 0.00019782037354371603\n",
      "Iteration: 3412 Loss: 0.0001976464964081339\n",
      "Iteration: 3413 Loss: 0.00019747277299237085\n",
      "Iteration: 3414 Loss: 0.00019729920315917683\n",
      "Iteration: 3415 Loss: 0.00019712578677125143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3416 Loss: 0.00019695252369125786\n",
      "Iteration: 3417 Loss: 0.00019677941378184324\n",
      "Iteration: 3418 Loss: 0.00019660645690658724\n",
      "Iteration: 3419 Loss: 0.0001964336529285076\n",
      "Iteration: 3420 Loss: 0.00019626100171123145\n",
      "Iteration: 3421 Loss: 0.0001960885031178357\n",
      "Iteration: 3422 Loss: 0.00019591615701196246\n",
      "Iteration: 3423 Loss: 0.00019574396325725875\n",
      "Iteration: 3424 Loss: 0.00019557192171784838\n",
      "Iteration: 3425 Loss: 0.00019540003225712658\n",
      "Iteration: 3426 Loss: 0.00019522829473975323\n",
      "Iteration: 3427 Loss: 0.00019505670902946625\n",
      "Iteration: 3428 Loss: 0.0001948852749905943\n",
      "Iteration: 3429 Loss: 0.00019471399248730873\n",
      "Iteration: 3430 Loss: 0.00019454286138464387\n",
      "Iteration: 3431 Loss: 0.0001943718815473503\n",
      "Iteration: 3432 Loss: 0.000194201052839609\n",
      "Iteration: 3433 Loss: 0.0001940303751270587\n",
      "Iteration: 3434 Loss: 0.00019385984827419335\n",
      "Iteration: 3435 Loss: 0.000193689472146431\n",
      "Iteration: 3436 Loss: 0.0001935192466088742\n",
      "Iteration: 3437 Loss: 0.00019334917152722674\n",
      "Iteration: 3438 Loss: 0.00019317924676658482\n",
      "Iteration: 3439 Loss: 0.0001930094721927778\n",
      "Iteration: 3440 Loss: 0.00019283984767176163\n",
      "Iteration: 3441 Loss: 0.00019267037306909041\n",
      "Iteration: 3442 Loss: 0.0001925010482510752\n",
      "Iteration: 3443 Loss: 0.00019233187308372352\n",
      "Iteration: 3444 Loss: 0.00019216284743321635\n",
      "Iteration: 3445 Loss: 0.00019199397116590462\n",
      "Iteration: 3446 Loss: 0.00019182524414842995\n",
      "Iteration: 3447 Loss: 0.00019165666624724808\n",
      "Iteration: 3448 Loss: 0.0001914882373290039\n",
      "Iteration: 3449 Loss: 0.0001913199572605368\n",
      "Iteration: 3450 Loss: 0.00019115182590890965\n",
      "Iteration: 3451 Loss: 0.00019098384314121053\n",
      "Iteration: 3452 Loss: 0.0001908160088246711\n",
      "Iteration: 3453 Loss: 0.00019064832282659116\n",
      "Iteration: 3454 Loss: 0.00019048078501427202\n",
      "Iteration: 3455 Loss: 0.00019031339525519306\n",
      "Iteration: 3456 Loss: 0.00019014615341768454\n",
      "Iteration: 3457 Loss: 0.0001899790593691039\n",
      "Iteration: 3458 Loss: 0.00018981211297763441\n",
      "Iteration: 3459 Loss: 0.00018964531411099373\n",
      "Iteration: 3460 Loss: 0.00018947866263740293\n",
      "Iteration: 3461 Loss: 0.00018931215842531255\n",
      "Iteration: 3462 Loss: 0.00018914580134273145\n",
      "Iteration: 3463 Loss: 0.0001889795912583882\n",
      "Iteration: 3464 Loss: 0.00018881352804059519\n",
      "Iteration: 3465 Loss: 0.00018864761155867898\n",
      "Iteration: 3466 Loss: 0.0001884818416814646\n",
      "Iteration: 3467 Loss: 0.00018831621827778257\n",
      "Iteration: 3468 Loss: 0.00018815074121656452\n",
      "Iteration: 3469 Loss: 0.00018798541036734918\n",
      "Iteration: 3470 Loss: 0.00018782022559925884\n",
      "Iteration: 3471 Loss: 0.00018765518678185718\n",
      "Iteration: 3472 Loss: 0.00018749029378483894\n",
      "Iteration: 3473 Loss: 0.0001873255464776389\n",
      "Iteration: 3474 Loss: 0.00018716094473012992\n",
      "Iteration: 3475 Loss: 0.0001869964884122125\n",
      "Iteration: 3476 Loss: 0.00018683217739397023\n",
      "Iteration: 3477 Loss: 0.00018666801154535289\n",
      "Iteration: 3478 Loss: 0.0001865039907369936\n",
      "Iteration: 3479 Loss: 0.00018634011483911956\n",
      "Iteration: 3480 Loss: 0.00018617638372250966\n",
      "Iteration: 3481 Loss: 0.00018601279725782147\n",
      "Iteration: 3482 Loss: 0.00018584935531537137\n",
      "Iteration: 3483 Loss: 0.0001856860577661686\n",
      "Iteration: 3484 Loss: 0.00018552290448123874\n",
      "Iteration: 3485 Loss: 0.0001853598953316439\n",
      "Iteration: 3486 Loss: 0.00018519703018888528\n",
      "Iteration: 3487 Loss: 0.00018503430892405397\n",
      "Iteration: 3488 Loss: 0.00018487173140857297\n",
      "Iteration: 3489 Loss: 0.0001847092975140312\n",
      "Iteration: 3490 Loss: 0.00018454700711202182\n",
      "Iteration: 3491 Loss: 0.00018438486007445968\n",
      "Iteration: 3492 Loss: 0.00018422285627342342\n",
      "Iteration: 3493 Loss: 0.00018406099558053888\n",
      "Iteration: 3494 Loss: 0.00018389927786822806\n",
      "Iteration: 3495 Loss: 0.00018373770300864276\n",
      "Iteration: 3496 Loss: 0.00018357627087408804\n",
      "Iteration: 3497 Loss: 0.00018341498133710065\n",
      "Iteration: 3498 Loss: 0.00018325383427034741\n",
      "Iteration: 3499 Loss: 0.00018309282954648355\n",
      "Iteration: 3500 Loss: 0.0001829319670383496\n",
      "Iteration: 3501 Loss: 0.0001827712466186545\n",
      "Iteration: 3502 Loss: 0.00018261066816071476\n",
      "Iteration: 3503 Loss: 0.0001824502315376728\n",
      "Iteration: 3504 Loss: 0.00018228993662264525\n",
      "Iteration: 3505 Loss: 0.00018212978328930603\n",
      "Iteration: 3506 Loss: 0.00018196977141101103\n",
      "Iteration: 3507 Loss: 0.0001818099008614062\n",
      "Iteration: 3508 Loss: 0.00018165017151446655\n",
      "Iteration: 3509 Loss: 0.00018149058324361532\n",
      "Iteration: 3510 Loss: 0.0001813311359231848\n",
      "Iteration: 3511 Loss: 0.00018117182942740902\n",
      "Iteration: 3512 Loss: 0.00018101266363006259\n",
      "Iteration: 3513 Loss: 0.00018085363840539292\n",
      "Iteration: 3514 Loss: 0.00018069475362805332\n",
      "Iteration: 3515 Loss: 0.00018053600917258033\n",
      "Iteration: 3516 Loss: 0.00018037740491356024\n",
      "Iteration: 3517 Loss: 0.0001802189407257965\n",
      "Iteration: 3518 Loss: 0.00018006061648422666\n",
      "Iteration: 3519 Loss: 0.0001799024320635666\n",
      "Iteration: 3520 Loss: 0.00017974438733899212\n",
      "Iteration: 3521 Loss: 0.00017958648218583238\n",
      "Iteration: 3522 Loss: 0.00017942871647943506\n",
      "Iteration: 3523 Loss: 0.00017927109009556496\n",
      "Iteration: 3524 Loss: 0.00017911360290943066\n",
      "Iteration: 3525 Loss: 0.00017895625479658783\n",
      "Iteration: 3526 Loss: 0.0001787990456328069\n",
      "Iteration: 3527 Loss: 0.00017864197529407936\n",
      "Iteration: 3528 Loss: 0.00017848504365648902\n",
      "Iteration: 3529 Loss: 0.0001783282505958894\n",
      "Iteration: 3530 Loss: 0.0001781715959886999\n",
      "Iteration: 3531 Loss: 0.00017801507971108627\n",
      "Iteration: 3532 Loss: 0.0001778587016397216\n",
      "Iteration: 3533 Loss: 0.00017770246165087187\n",
      "Iteration: 3534 Loss: 0.00017754635962138535\n",
      "Iteration: 3535 Loss: 0.000177390395428094\n",
      "Iteration: 3536 Loss: 0.00017723456894777788\n",
      "Iteration: 3537 Loss: 0.0001770788800575898\n",
      "Iteration: 3538 Loss: 0.00017692332863435644\n",
      "Iteration: 3539 Loss: 0.00017676791455547604\n",
      "Iteration: 3540 Loss: 0.00017661263769842627\n",
      "Iteration: 3541 Loss: 0.00017645749794064245\n",
      "Iteration: 3542 Loss: 0.00017630249515924074\n",
      "Iteration: 3543 Loss: 0.00017614762923251232\n",
      "Iteration: 3544 Loss: 0.0001759929000378655\n",
      "Iteration: 3545 Loss: 0.00017583830745293653\n",
      "Iteration: 3546 Loss: 0.00017568385135626223\n",
      "Iteration: 3547 Loss: 0.00017552953162554134\n",
      "Iteration: 3548 Loss: 0.00017537534813899657\n",
      "Iteration: 3549 Loss: 0.00017522130077484566\n",
      "Iteration: 3550 Loss: 0.00017506738941198502\n",
      "Iteration: 3551 Loss: 0.00017491361392860949\n",
      "Iteration: 3552 Loss: 0.00017475997420347201\n",
      "Iteration: 3553 Loss: 0.00017460647011558833\n",
      "Iteration: 3554 Loss: 0.00017445310154317966\n",
      "Iteration: 3555 Loss: 0.00017429986836581924\n",
      "Iteration: 3556 Loss: 0.00017414677046253803\n",
      "Iteration: 3557 Loss: 0.00017399380771225702\n",
      "Iteration: 3558 Loss: 0.00017384097999430763\n",
      "Iteration: 3559 Loss: 0.00017368828718822014\n",
      "Iteration: 3560 Loss: 0.00017353572917363114\n",
      "Iteration: 3561 Loss: 0.0001733833058298027\n",
      "Iteration: 3562 Loss: 0.00017323101703662128\n",
      "Iteration: 3563 Loss: 0.0001730788626741629\n",
      "Iteration: 3564 Loss: 0.0001729268426220847\n",
      "Iteration: 3565 Loss: 0.00017277495676057376\n",
      "Iteration: 3566 Loss: 0.0001726232049696739\n",
      "Iteration: 3567 Loss: 0.0001724715871301034\n",
      "Iteration: 3568 Loss: 0.000172320103121889\n",
      "Iteration: 3569 Loss: 0.00017216875282560393\n",
      "Iteration: 3570 Loss: 0.00017201753612180762\n",
      "Iteration: 3571 Loss: 0.00017186645289130666\n",
      "Iteration: 3572 Loss: 0.0001717155030146407\n",
      "Iteration: 3573 Loss: 0.00017156468637292171\n",
      "Iteration: 3574 Loss: 0.0001714140028472433\n",
      "Iteration: 3575 Loss: 0.00017126345231881015\n",
      "Iteration: 3576 Loss: 0.00017111303466870787\n",
      "Iteration: 3577 Loss: 0.00017096274977815534\n",
      "Iteration: 3578 Loss: 0.0001708125975288762\n",
      "Iteration: 3579 Loss: 0.000170662577802312\n",
      "Iteration: 3580 Loss: 0.0001705126904800545\n",
      "Iteration: 3581 Loss: 0.0001703629354440978\n",
      "Iteration: 3582 Loss: 0.00017021331257645978\n",
      "Iteration: 3583 Loss: 0.0001700638217585617\n",
      "Iteration: 3584 Loss: 0.00016991446287307303\n",
      "Iteration: 3585 Loss: 0.00016976523580170515\n",
      "Iteration: 3586 Loss: 0.00016961614042701137\n",
      "Iteration: 3587 Loss: 0.0001694671766314534\n",
      "Iteration: 3588 Loss: 0.0001693183442979963\n",
      "Iteration: 3589 Loss: 0.00016916964330821298\n",
      "Iteration: 3590 Loss: 0.0001690210735456456\n",
      "Iteration: 3591 Loss: 0.00016887263489283203\n",
      "Iteration: 3592 Loss: 0.0001687243272329568\n",
      "Iteration: 3593 Loss: 0.00016857615044900667\n",
      "Iteration: 3594 Loss: 0.0001684281044236424\n",
      "Iteration: 3595 Loss: 0.00016828018904064458\n",
      "Iteration: 3596 Loss: 0.00016813240418312643\n",
      "Iteration: 3597 Loss: 0.00016798474973483792\n",
      "Iteration: 3598 Loss: 0.00016783722557899678\n",
      "Iteration: 3599 Loss: 0.00016768983159948974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3600 Loss: 0.00016754256768009666\n",
      "Iteration: 3601 Loss: 0.00016739543370490598\n",
      "Iteration: 3602 Loss: 0.0001672484295575462\n",
      "Iteration: 3603 Loss: 0.00016710155512226878\n",
      "Iteration: 3604 Loss: 0.00016695481028325025\n",
      "Iteration: 3605 Loss: 0.00016680819492475402\n",
      "Iteration: 3606 Loss: 0.00016666170893144547\n",
      "Iteration: 3607 Loss: 0.00016651535218766524\n",
      "Iteration: 3608 Loss: 0.00016636912457801151\n",
      "Iteration: 3609 Loss: 0.00016622302598709633\n",
      "Iteration: 3610 Loss: 0.00016607705629996574\n",
      "Iteration: 3611 Loss: 0.00016593121540171144\n",
      "Iteration: 3612 Loss: 0.00016578550317680727\n",
      "Iteration: 3613 Loss: 0.00016563991951075275\n",
      "Iteration: 3614 Loss: 0.0001654944642887297\n",
      "Iteration: 3615 Loss: 0.00016534913739604435\n",
      "Iteration: 3616 Loss: 0.00016520393871822733\n",
      "Iteration: 3617 Loss: 0.00016505886814046567\n",
      "Iteration: 3618 Loss: 0.0001649139255489254\n",
      "Iteration: 3619 Loss: 0.00016476911082938524\n",
      "Iteration: 3620 Loss: 0.00016462442386745108\n",
      "Iteration: 3621 Loss: 0.00016447986454885992\n",
      "Iteration: 3622 Loss: 0.00016433543275987383\n",
      "Iteration: 3623 Loss: 0.00016419112838702662\n",
      "Iteration: 3624 Loss: 0.00016404695131598857\n",
      "Iteration: 3625 Loss: 0.00016390290143354863\n",
      "Iteration: 3626 Loss: 0.00016375897862627657\n",
      "Iteration: 3627 Loss: 0.00016361518278029465\n",
      "Iteration: 3628 Loss: 0.00016347151378253994\n",
      "Iteration: 3629 Loss: 0.00016332797151978658\n",
      "Iteration: 3630 Loss: 0.00016318455587863333\n",
      "Iteration: 3631 Loss: 0.00016304126674647628\n",
      "Iteration: 3632 Loss: 0.0001628981040104621\n",
      "Iteration: 3633 Loss: 0.0001627550675577327\n",
      "Iteration: 3634 Loss: 0.00016261215727518178\n",
      "Iteration: 3635 Loss: 0.0001624693730501156\n",
      "Iteration: 3636 Loss: 0.00016232671477056589\n",
      "Iteration: 3637 Loss: 0.00016218418232384648\n",
      "Iteration: 3638 Loss: 0.0001620417755978487\n",
      "Iteration: 3639 Loss: 0.0001618994944803491\n",
      "Iteration: 3640 Loss: 0.00016175733885908131\n",
      "Iteration: 3641 Loss: 0.0001616153086218566\n",
      "Iteration: 3642 Loss: 0.00016147340365697604\n",
      "Iteration: 3643 Loss: 0.0001613316238527801\n",
      "Iteration: 3644 Loss: 0.00016118996909764312\n",
      "Iteration: 3645 Loss: 0.0001610484392797842\n",
      "Iteration: 3646 Loss: 0.00016090703428761253\n",
      "Iteration: 3647 Loss: 0.0001607657540098879\n",
      "Iteration: 3648 Loss: 0.0001606245983351418\n",
      "Iteration: 3649 Loss: 0.00016048356715232773\n",
      "Iteration: 3650 Loss: 0.00016034266035043775\n",
      "Iteration: 3651 Loss: 0.00016020187781805347\n",
      "Iteration: 3652 Loss: 0.0001600612194446954\n",
      "Iteration: 3653 Loss: 0.00015992068511931317\n",
      "Iteration: 3654 Loss: 0.00015978027473135205\n",
      "Iteration: 3655 Loss: 0.0001596399881700024\n",
      "Iteration: 3656 Loss: 0.00015949982532514269\n",
      "Iteration: 3657 Loss: 0.00015935978608603256\n",
      "Iteration: 3658 Loss: 0.00015921987034262468\n",
      "Iteration: 3659 Loss: 0.0001590800779844164\n",
      "Iteration: 3660 Loss: 0.00015894040890156546\n",
      "Iteration: 3661 Loss: 0.0001588008629838741\n",
      "Iteration: 3662 Loss: 0.00015866144012155646\n",
      "Iteration: 3663 Loss: 0.00015852214020492384\n",
      "Iteration: 3664 Loss: 0.00015838296312416575\n",
      "Iteration: 3665 Loss: 0.0001582439087694832\n",
      "Iteration: 3666 Loss: 0.00015810497703171972\n",
      "Iteration: 3667 Loss: 0.00015796616780120273\n",
      "Iteration: 3668 Loss: 0.00015782748096859766\n",
      "Iteration: 3669 Loss: 0.00015768891642487523\n",
      "Iteration: 3670 Loss: 0.00015755047406090117\n",
      "Iteration: 3671 Loss: 0.00015741215376772452\n",
      "Iteration: 3672 Loss: 0.00015727395543613457\n",
      "Iteration: 3673 Loss: 0.00015713587895742717\n",
      "Iteration: 3674 Loss: 0.0001569979242226853\n",
      "Iteration: 3675 Loss: 0.0001568600911236013\n",
      "Iteration: 3676 Loss: 0.00015672237955161129\n",
      "Iteration: 3677 Loss: 0.00015658478939839767\n",
      "Iteration: 3678 Loss: 0.0001564473205554534\n",
      "Iteration: 3679 Loss: 0.00015630997291468137\n",
      "Iteration: 3680 Loss: 0.00015617274636765948\n",
      "Iteration: 3681 Loss: 0.00015603564080657033\n",
      "Iteration: 3682 Loss: 0.00015589865612316827\n",
      "Iteration: 3683 Loss: 0.0001557617922099887\n",
      "Iteration: 3684 Loss: 0.0001556250489591639\n",
      "Iteration: 3685 Loss: 0.00015548842626317145\n",
      "Iteration: 3686 Loss: 0.00015535192401404794\n",
      "Iteration: 3687 Loss: 0.00015521554210470008\n",
      "Iteration: 3688 Loss: 0.00015507928042762597\n",
      "Iteration: 3689 Loss: 0.0001549431388755962\n",
      "Iteration: 3690 Loss: 0.00015480711734106972\n",
      "Iteration: 3691 Loss: 0.00015467121571772645\n",
      "Iteration: 3692 Loss: 0.0001545354338978828\n",
      "Iteration: 3693 Loss: 0.00015439977177504282\n",
      "Iteration: 3694 Loss: 0.00015426422924247996\n",
      "Iteration: 3695 Loss: 0.00015412880619336365\n",
      "Iteration: 3696 Loss: 0.0001539935025212177\n",
      "Iteration: 3697 Loss: 0.00015385831811935523\n",
      "Iteration: 3698 Loss: 0.0001537232528813526\n",
      "Iteration: 3699 Loss: 0.00015358830670120268\n",
      "Iteration: 3700 Loss: 0.00015345347947264964\n",
      "Iteration: 3701 Loss: 0.00015331877108940438\n",
      "Iteration: 3702 Loss: 0.00015318418144542854\n",
      "Iteration: 3703 Loss: 0.0001530497104348545\n",
      "Iteration: 3704 Loss: 0.0001529153579521455\n",
      "Iteration: 3705 Loss: 0.00015278112389084065\n",
      "Iteration: 3706 Loss: 0.00015264700814585896\n",
      "Iteration: 3707 Loss: 0.00015251301061168274\n",
      "Iteration: 3708 Loss: 0.00015237913118283883\n",
      "Iteration: 3709 Loss: 0.00015224536975354158\n",
      "Iteration: 3710 Loss: 0.00015211172621866715\n",
      "Iteration: 3711 Loss: 0.00015197820047336798\n",
      "Iteration: 3712 Loss: 0.00015184479241238144\n",
      "Iteration: 3713 Loss: 0.0001517115019311453\n",
      "Iteration: 3714 Loss: 0.00015157832892424044\n",
      "Iteration: 3715 Loss: 0.0001514452732871653\n",
      "Iteration: 3716 Loss: 0.00015131233491506117\n",
      "Iteration: 3717 Loss: 0.00015117951370376676\n",
      "Iteration: 3718 Loss: 0.0001510468095482777\n",
      "Iteration: 3719 Loss: 0.00015091422234436667\n",
      "Iteration: 3720 Loss: 0.0001507817519878181\n",
      "Iteration: 3721 Loss: 0.00015064939837429624\n",
      "Iteration: 3722 Loss: 0.00015051716139946093\n",
      "Iteration: 3723 Loss: 0.00015038504095963624\n",
      "Iteration: 3724 Loss: 0.0001502530369505849\n",
      "Iteration: 3725 Loss: 0.00015012114926848017\n",
      "Iteration: 3726 Loss: 0.00014998937780989397\n",
      "Iteration: 3727 Loss: 0.00014985772247118556\n",
      "Iteration: 3728 Loss: 0.00014972618314843997\n",
      "Iteration: 3729 Loss: 0.0001495947597382015\n",
      "Iteration: 3730 Loss: 0.00014946345213740774\n",
      "Iteration: 3731 Loss: 0.0001493322602424979\n",
      "Iteration: 3732 Loss: 0.00014920118395042665\n",
      "Iteration: 3733 Loss: 0.0001490702231580944\n",
      "Iteration: 3734 Loss: 0.00014893937776222797\n",
      "Iteration: 3735 Loss: 0.0001488086476602067\n",
      "Iteration: 3736 Loss: 0.00014867803274902364\n",
      "Iteration: 3737 Loss: 0.00014854753292620735\n",
      "Iteration: 3738 Loss: 0.00014841714808896576\n",
      "Iteration: 3739 Loss: 0.0001482868781344039\n",
      "Iteration: 3740 Loss: 0.00014815672296057087\n",
      "Iteration: 3741 Loss: 0.0001480266824650924\n",
      "Iteration: 3742 Loss: 0.00014789675654538359\n",
      "Iteration: 3743 Loss: 0.0001477669450994511\n",
      "Iteration: 3744 Loss: 0.00014763724802503936\n",
      "Iteration: 3745 Loss: 0.00014750766522077458\n",
      "Iteration: 3746 Loss: 0.0001473781965840632\n",
      "Iteration: 3747 Loss: 0.00014724884201303202\n",
      "Iteration: 3748 Loss: 0.00014711960140656599\n",
      "Iteration: 3749 Loss: 0.0001469904746627221\n",
      "Iteration: 3750 Loss: 0.00014686146168002478\n",
      "Iteration: 3751 Loss: 0.00014673256235681596\n",
      "Iteration: 3752 Loss: 0.0001466037765921873\n",
      "Iteration: 3753 Loss: 0.00014647510428450475\n",
      "Iteration: 3754 Loss: 0.00014634654533293096\n",
      "Iteration: 3755 Loss: 0.00014621809963610057\n",
      "Iteration: 3756 Loss: 0.0001460897670925564\n",
      "Iteration: 3757 Loss: 0.00014596154760221082\n",
      "Iteration: 3758 Loss: 0.00014583344106352988\n",
      "Iteration: 3759 Loss: 0.0001457054473765608\n",
      "Iteration: 3760 Loss: 0.000145577566440461\n",
      "Iteration: 3761 Loss: 0.00014544979815436345\n",
      "Iteration: 3762 Loss: 0.00014532214241803608\n",
      "Iteration: 3763 Loss: 0.0001451945991314101\n",
      "Iteration: 3764 Loss: 0.00014506716819359306\n",
      "Iteration: 3765 Loss: 0.00014493984950480015\n",
      "Iteration: 3766 Loss: 0.00014481264296496694\n",
      "Iteration: 3767 Loss: 0.00014468554847390835\n",
      "Iteration: 3768 Loss: 0.00014455856593197024\n",
      "Iteration: 3769 Loss: 0.00014443169523936186\n",
      "Iteration: 3770 Loss: 0.00014430493629603307\n",
      "Iteration: 3771 Loss: 0.00014417828900257915\n",
      "Iteration: 3772 Loss: 0.0001440517532593864\n",
      "Iteration: 3773 Loss: 0.0001439253289671487\n",
      "Iteration: 3774 Loss: 0.00014379901602614833\n",
      "Iteration: 3775 Loss: 0.00014367281433776428\n",
      "Iteration: 3776 Loss: 0.00014354672380239184\n",
      "Iteration: 3777 Loss: 0.0001434207443206882\n",
      "Iteration: 3778 Loss: 0.00014329487579361252\n",
      "Iteration: 3779 Loss: 0.00014316911812279663\n",
      "Iteration: 3780 Loss: 0.0001430434712090916\n",
      "Iteration: 3781 Loss: 0.00014291793495371685\n",
      "Iteration: 3782 Loss: 0.00014279250925817563\n",
      "Iteration: 3783 Loss: 0.00014266719402357338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3784 Loss: 0.000142541989151898\n",
      "Iteration: 3785 Loss: 0.00014241689454421797\n",
      "Iteration: 3786 Loss: 0.00014229191010293522\n",
      "Iteration: 3787 Loss: 0.00014216703572935614\n",
      "Iteration: 3788 Loss: 0.00014204227132533795\n",
      "Iteration: 3789 Loss: 0.00014191761679318898\n",
      "Iteration: 3790 Loss: 0.00014179307203491588\n",
      "Iteration: 3791 Loss: 0.0001416686369523606\n",
      "Iteration: 3792 Loss: 0.00014154431144792757\n",
      "Iteration: 3793 Loss: 0.00014142009542352654\n",
      "Iteration: 3794 Loss: 0.00014129598878203706\n",
      "Iteration: 3795 Loss: 0.0001411719914261065\n",
      "Iteration: 3796 Loss: 0.00014104810325811162\n",
      "Iteration: 3797 Loss: 0.00014092432418044327\n",
      "Iteration: 3798 Loss: 0.00014080065409623663\n",
      "Iteration: 3799 Loss: 0.00014067709290802627\n",
      "Iteration: 3800 Loss: 0.00014055364051873695\n",
      "Iteration: 3801 Loss: 0.00014043029683171598\n",
      "Iteration: 3802 Loss: 0.00014030706175015775\n",
      "Iteration: 3803 Loss: 0.00014018393517656547\n",
      "Iteration: 3804 Loss: 0.00014006091701486255\n",
      "Iteration: 3805 Loss: 0.00013993800716819495\n",
      "Iteration: 3806 Loss: 0.00013981520554032857\n",
      "Iteration: 3807 Loss: 0.00013969251203416763\n",
      "Iteration: 3808 Loss: 0.00013956992655372958\n",
      "Iteration: 3809 Loss: 0.00013944744900259268\n",
      "Iteration: 3810 Loss: 0.00013932507928497167\n",
      "Iteration: 3811 Loss: 0.00013920281730426002\n",
      "Iteration: 3812 Loss: 0.00013908066296442777\n",
      "Iteration: 3813 Loss: 0.00013895861616975593\n",
      "Iteration: 3814 Loss: 0.00013883667682450338\n",
      "Iteration: 3815 Loss: 0.0001387148448327581\n",
      "Iteration: 3816 Loss: 0.000138593120098407\n",
      "Iteration: 3817 Loss: 0.00013847150252652488\n",
      "Iteration: 3818 Loss: 0.0001383499920211707\n",
      "Iteration: 3819 Loss: 0.0001382285884872591\n",
      "Iteration: 3820 Loss: 0.00013810729182912743\n",
      "Iteration: 3821 Loss: 0.0001379861019519622\n",
      "Iteration: 3822 Loss: 0.00013786501876021442\n",
      "Iteration: 3823 Loss: 0.00013774404215904215\n",
      "Iteration: 3824 Loss: 0.0001376231720532764\n",
      "Iteration: 3825 Loss: 0.0001375024083481601\n",
      "Iteration: 3826 Loss: 0.00013738175094861683\n",
      "Iteration: 3827 Loss: 0.0001372611997598682\n",
      "Iteration: 3828 Loss: 0.00013714075468750816\n",
      "Iteration: 3829 Loss: 0.00013702041563698613\n",
      "Iteration: 3830 Loss: 0.00013690018251356746\n",
      "Iteration: 3831 Loss: 0.00013678005522288456\n",
      "Iteration: 3832 Loss: 0.0001366600336710251\n",
      "Iteration: 3833 Loss: 0.0001365401177636327\n",
      "Iteration: 3834 Loss: 0.00013642030740642938\n",
      "Iteration: 3835 Loss: 0.00013630060250538717\n",
      "Iteration: 3836 Loss: 0.00013618100296631708\n",
      "Iteration: 3837 Loss: 0.00013606150869550403\n",
      "Iteration: 3838 Loss: 0.0001359421195992167\n",
      "Iteration: 3839 Loss: 0.00013582283558356\n",
      "Iteration: 3840 Loss: 0.00013570365655500612\n",
      "Iteration: 3841 Loss: 0.0001355845824198535\n",
      "Iteration: 3842 Loss: 0.00013546561308475753\n",
      "Iteration: 3843 Loss: 0.0001353467484561054\n",
      "Iteration: 3844 Loss: 0.0001352279884410913\n",
      "Iteration: 3845 Loss: 0.00013510933294640171\n",
      "Iteration: 3846 Loss: 0.000134990781878779\n",
      "Iteration: 3847 Loss: 0.00013487233514504273\n",
      "Iteration: 3848 Loss: 0.00013475399265237522\n",
      "Iteration: 3849 Loss: 0.00013463575430780128\n",
      "Iteration: 3850 Loss: 0.00013451762001884978\n",
      "Iteration: 3851 Loss: 0.00013439958969235677\n",
      "Iteration: 3852 Loss: 0.0001342816632356091\n",
      "Iteration: 3853 Loss: 0.00013416384055615216\n",
      "Iteration: 3854 Loss: 0.00013404612156170544\n",
      "Iteration: 3855 Loss: 0.00013392850616000866\n",
      "Iteration: 3856 Loss: 0.00013381099425848955\n",
      "Iteration: 3857 Loss: 0.0001336935857650795\n",
      "Iteration: 3858 Loss: 0.00013357628058748575\n",
      "Iteration: 3859 Loss: 0.00013345907863375912\n",
      "Iteration: 3860 Loss: 0.00013334197981200641\n",
      "Iteration: 3861 Loss: 0.00013322498403032937\n",
      "Iteration: 3862 Loss: 0.00013310809119676276\n",
      "Iteration: 3863 Loss: 0.00013299130121990202\n",
      "Iteration: 3864 Loss: 0.00013287461400776353\n",
      "Iteration: 3865 Loss: 0.00013275802946895868\n",
      "Iteration: 3866 Loss: 0.0001326415475121341\n",
      "Iteration: 3867 Loss: 0.00013252516804562603\n",
      "Iteration: 3868 Loss: 0.00013240889097820634\n",
      "Iteration: 3869 Loss: 0.00013229271621868275\n",
      "Iteration: 3870 Loss: 0.00013217664367599666\n",
      "Iteration: 3871 Loss: 0.0001320606732589517\n",
      "Iteration: 3872 Loss: 0.00013194480487667805\n",
      "Iteration: 3873 Loss: 0.0001318290384383701\n",
      "Iteration: 3874 Loss: 0.00013171337385260904\n",
      "Iteration: 3875 Loss: 0.0001315978110293944\n",
      "Iteration: 3876 Loss: 0.00013148234987787067\n",
      "Iteration: 3877 Loss: 0.00013136699030725306\n",
      "Iteration: 3878 Loss: 0.00013125173222688718\n",
      "Iteration: 3879 Loss: 0.00013113657554693776\n",
      "Iteration: 3880 Loss: 0.00013102152017697077\n",
      "Iteration: 3881 Loss: 0.000130906566026312\n",
      "Iteration: 3882 Loss: 0.0001307917130048859\n",
      "Iteration: 3883 Loss: 0.00013067696102273056\n",
      "Iteration: 3884 Loss: 0.00013056230998988869\n",
      "Iteration: 3885 Loss: 0.00013044775981655845\n",
      "Iteration: 3886 Loss: 0.00013033331041308762\n",
      "Iteration: 3887 Loss: 0.000130218961688804\n",
      "Iteration: 3888 Loss: 0.0001301047135544724\n",
      "Iteration: 3889 Loss: 0.0001299905659210875\n",
      "Iteration: 3890 Loss: 0.00012987651869841285\n",
      "Iteration: 3891 Loss: 0.00012976257179726696\n",
      "Iteration: 3892 Loss: 0.00012964872512840508\n",
      "Iteration: 3893 Loss: 0.0001295349786026822\n",
      "Iteration: 3894 Loss: 0.00012942133213039578\n",
      "Iteration: 3895 Loss: 0.00012930778562283746\n",
      "Iteration: 3896 Loss: 0.00012919433899069116\n",
      "Iteration: 3897 Loss: 0.00012908099214504118\n",
      "Iteration: 3898 Loss: 0.0001289677449970591\n",
      "Iteration: 3899 Loss: 0.00012885459745789\n",
      "Iteration: 3900 Loss: 0.00012874154943903619\n",
      "Iteration: 3901 Loss: 0.00012862860085127777\n",
      "Iteration: 3902 Loss: 0.00012851575160684238\n",
      "Iteration: 3903 Loss: 0.0001284030016167169\n",
      "Iteration: 3904 Loss: 0.00012829035079283194\n",
      "Iteration: 3905 Loss: 0.00012817779904665343\n",
      "Iteration: 3906 Loss: 0.00012806534629011324\n",
      "Iteration: 3907 Loss: 0.0001279529924348\n",
      "Iteration: 3908 Loss: 0.00012784073739262867\n",
      "Iteration: 3909 Loss: 0.00012772858107597628\n",
      "Iteration: 3910 Loss: 0.0001276165233967838\n",
      "Iteration: 3911 Loss: 0.0001275045642669113\n",
      "Iteration: 3912 Loss: 0.00012739270359884305\n",
      "Iteration: 3913 Loss: 0.00012728094130473513\n",
      "Iteration: 3914 Loss: 0.00012716927729709184\n",
      "Iteration: 3915 Loss: 0.000127057711488308\n",
      "Iteration: 3916 Loss: 0.00012694624379097492\n",
      "Iteration: 3917 Loss: 0.0001268348741174933\n",
      "Iteration: 3918 Loss: 0.00012672360238117082\n",
      "Iteration: 3919 Loss: 0.0001266124284941366\n",
      "Iteration: 3920 Loss: 0.0001265013523697737\n",
      "Iteration: 3921 Loss: 0.00012639037392056987\n",
      "Iteration: 3922 Loss: 0.00012627949305954139\n",
      "Iteration: 3923 Loss: 0.00012616870970030413\n",
      "Iteration: 3924 Loss: 0.00012605802375554098\n",
      "Iteration: 3925 Loss: 0.000125947435138666\n",
      "Iteration: 3926 Loss: 0.00012583694376292463\n",
      "Iteration: 3927 Loss: 0.00012572654954210282\n",
      "Iteration: 3928 Loss: 0.0001256162523893727\n",
      "Iteration: 3929 Loss: 0.00012550605221833995\n",
      "Iteration: 3930 Loss: 0.000125395948942699\n",
      "Iteration: 3931 Loss: 0.00012528594247575701\n",
      "Iteration: 3932 Loss: 0.00012517603273176224\n",
      "Iteration: 3933 Loss: 0.00012506621962444738\n",
      "Iteration: 3934 Loss: 0.00012495650306814545\n",
      "Iteration: 3935 Loss: 0.0001248468829760293\n",
      "Iteration: 3936 Loss: 0.00012473735926315632\n",
      "Iteration: 3937 Loss: 0.00012462793184288504\n",
      "Iteration: 3938 Loss: 0.00012451860063011463\n",
      "Iteration: 3939 Loss: 0.00012440936553898634\n",
      "Iteration: 3940 Loss: 0.00012430022648404093\n",
      "Iteration: 3941 Loss: 0.00012419118337962648\n",
      "Iteration: 3942 Loss: 0.0001240822361401473\n",
      "Iteration: 3943 Loss: 0.00012397338468022765\n",
      "Iteration: 3944 Loss: 0.00012386462891479275\n",
      "Iteration: 3945 Loss: 0.00012375596875857146\n",
      "Iteration: 3946 Loss: 0.00012364740412628848\n",
      "Iteration: 3947 Loss: 0.00012353893493326362\n",
      "Iteration: 3948 Loss: 0.00012343056109423092\n",
      "Iteration: 3949 Loss: 0.00012332228252426137\n",
      "Iteration: 3950 Loss: 0.00012321409913867458\n",
      "Iteration: 3951 Loss: 0.0001231060108528432\n",
      "Iteration: 3952 Loss: 0.00012299801758190325\n",
      "Iteration: 3953 Loss: 0.00012289011924129334\n",
      "Iteration: 3954 Loss: 0.00012278231574663323\n",
      "Iteration: 3955 Loss: 0.00012267460701328642\n",
      "Iteration: 3956 Loss: 0.0001225669929570523\n",
      "Iteration: 3957 Loss: 0.0001224594734935046\n",
      "Iteration: 3958 Loss: 0.00012235204853847217\n",
      "Iteration: 3959 Loss: 0.00012224471800773693\n",
      "Iteration: 3960 Loss: 0.0001221374818176193\n",
      "Iteration: 3961 Loss: 0.00012203033988379161\n",
      "Iteration: 3962 Loss: 0.00012192329212233791\n",
      "Iteration: 3963 Loss: 0.00012181633844953167\n",
      "Iteration: 3964 Loss: 0.00012170947878152759\n",
      "Iteration: 3965 Loss: 0.00012160271303472906\n",
      "Iteration: 3966 Loss: 0.0001214960411256068\n",
      "Iteration: 3967 Loss: 0.00012138946297028849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3968 Loss: 0.00012128297848574937\n",
      "Iteration: 3969 Loss: 0.0001211765875882896\n",
      "Iteration: 3970 Loss: 0.0001210702901947418\n",
      "Iteration: 3971 Loss: 0.00012096408622180421\n",
      "Iteration: 3972 Loss: 0.00012085797558623457\n",
      "Iteration: 3973 Loss: 0.00012075195820524117\n",
      "Iteration: 3974 Loss: 0.00012064603399577635\n",
      "Iteration: 3975 Loss: 0.000120540202874274\n",
      "Iteration: 3976 Loss: 0.00012043446475876503\n",
      "Iteration: 3977 Loss: 0.0001203288195659604\n",
      "Iteration: 3978 Loss: 0.00012022326721325804\n",
      "Iteration: 3979 Loss: 0.0001201178076178913\n",
      "Iteration: 3980 Loss: 0.00012001244069758616\n",
      "Iteration: 3981 Loss: 0.00011990716636954207\n",
      "Iteration: 3982 Loss: 0.0001198019845514004\n",
      "Iteration: 3983 Loss: 0.00011969689516090275\n",
      "Iteration: 3984 Loss: 0.00011959189811575551\n",
      "Iteration: 3985 Loss: 0.00011948699333406634\n",
      "Iteration: 3986 Loss: 0.00011938218073302496\n",
      "Iteration: 3987 Loss: 0.00011927746023161721\n",
      "Iteration: 3988 Loss: 0.00011917283174682279\n",
      "Iteration: 3989 Loss: 0.00011906829519715325\n",
      "Iteration: 3990 Loss: 0.00011896385050098674\n",
      "Iteration: 3991 Loss: 0.00011885949757635338\n",
      "Iteration: 3992 Loss: 0.00011875523634156117\n",
      "Iteration: 3993 Loss: 0.00011865106671544485\n",
      "Iteration: 3994 Loss: 0.00011854698861581194\n",
      "Iteration: 3995 Loss: 0.00011844300196135925\n",
      "Iteration: 3996 Loss: 0.00011833910667084186\n",
      "Iteration: 3997 Loss: 0.00011823530266298594\n",
      "Iteration: 3998 Loss: 0.00011813158985651347\n",
      "Iteration: 3999 Loss: 0.00011802796817004241\n",
      "Iteration: 4000 Loss: 0.00011792443752294975\n",
      "Iteration: 4001 Loss: 0.00011782099783407113\n",
      "Iteration: 4002 Loss: 0.00011771764902196345\n",
      "Iteration: 4003 Loss: 0.00011761439100609319\n",
      "Iteration: 4004 Loss: 0.00011751122370577013\n",
      "Iteration: 4005 Loss: 0.00011740814704020695\n",
      "Iteration: 4006 Loss: 0.00011730516092880417\n",
      "Iteration: 4007 Loss: 0.00011720226529091315\n",
      "Iteration: 4008 Loss: 0.00011709946004587589\n",
      "Iteration: 4009 Loss: 0.00011699674511351558\n",
      "Iteration: 4010 Loss: 0.00011689412041339229\n",
      "Iteration: 4011 Loss: 0.00011679158586510909\n",
      "Iteration: 4012 Loss: 0.0001166891413882965\n",
      "Iteration: 4013 Loss: 0.00011658678690301248\n",
      "Iteration: 4014 Loss: 0.00011648452232916087\n",
      "Iteration: 4015 Loss: 0.00011638234758691187\n",
      "Iteration: 4016 Loss: 0.00011628026259586788\n",
      "Iteration: 4017 Loss: 0.00011617826727655317\n",
      "Iteration: 4018 Loss: 0.00011607636154919361\n",
      "Iteration: 4019 Loss: 0.00011597454533387948\n",
      "Iteration: 4020 Loss: 0.0001158728185510683\n",
      "Iteration: 4021 Loss: 0.00011577118112104455\n",
      "Iteration: 4022 Loss: 0.00011566963296449887\n",
      "Iteration: 4023 Loss: 0.00011556817400195057\n",
      "Iteration: 4024 Loss: 0.00011546680415362509\n",
      "Iteration: 4025 Loss: 0.00011536552334086609\n",
      "Iteration: 4026 Loss: 0.00011526433148389488\n",
      "Iteration: 4027 Loss: 0.00011516322850388916\n",
      "Iteration: 4028 Loss: 0.00011506221432174845\n",
      "Iteration: 4029 Loss: 0.0001149612888585932\n",
      "Iteration: 4030 Loss: 0.00011486045203542048\n",
      "Iteration: 4031 Loss: 0.00011475970377344808\n",
      "Iteration: 4032 Loss: 0.00011465904399339648\n",
      "Iteration: 4033 Loss: 0.00011455847261693453\n",
      "Iteration: 4034 Loss: 0.00011445798956506558\n",
      "Iteration: 4035 Loss: 0.00011435759475952204\n",
      "Iteration: 4036 Loss: 0.00011425728812185595\n",
      "Iteration: 4037 Loss: 0.00011415706957349899\n",
      "Iteration: 4038 Loss: 0.0001140569390362461\n",
      "Iteration: 4039 Loss: 0.0001139568964314674\n",
      "Iteration: 4040 Loss: 0.00011385694168145626\n",
      "Iteration: 4041 Loss: 0.00011375707470730472\n",
      "Iteration: 4042 Loss: 0.00011365729543153423\n",
      "Iteration: 4043 Loss: 0.00011355760377573938\n",
      "Iteration: 4044 Loss: 0.00011345799966243707\n",
      "Iteration: 4045 Loss: 0.00011335848301336293\n",
      "Iteration: 4046 Loss: 0.00011325905375061024\n",
      "Iteration: 4047 Loss: 0.00011315971179659075\n",
      "Iteration: 4048 Loss: 0.00011306045707404477\n",
      "Iteration: 4049 Loss: 0.00011296128950459329\n",
      "Iteration: 4050 Loss: 0.00011286220901118292\n",
      "Iteration: 4051 Loss: 0.00011276321551640544\n",
      "Iteration: 4052 Loss: 0.00011266430894272639\n",
      "Iteration: 4053 Loss: 0.0001125654892128681\n",
      "Iteration: 4054 Loss: 0.00011246675624965765\n",
      "Iteration: 4055 Loss: 0.00011236810997566446\n",
      "Iteration: 4056 Loss: 0.00011226955031402642\n",
      "Iteration: 4057 Loss: 0.00011217107718763562\n",
      "Iteration: 4058 Loss: 0.00011207269051930959\n",
      "Iteration: 4059 Loss: 0.00011197439023243138\n",
      "Iteration: 4060 Loss: 0.00011187617625007532\n",
      "Iteration: 4061 Loss: 0.00011177804849538146\n",
      "Iteration: 4062 Loss: 0.00011168000689160227\n",
      "Iteration: 4063 Loss: 0.0001115820513621151\n",
      "Iteration: 4064 Loss: 0.00011148418183064369\n",
      "Iteration: 4065 Loss: 0.00011138639822029233\n",
      "Iteration: 4066 Loss: 0.00011128870045491337\n",
      "Iteration: 4067 Loss: 0.00011119108845820235\n",
      "Iteration: 4068 Loss: 0.00011109356215383802\n",
      "Iteration: 4069 Loss: 0.00011099612146522796\n",
      "Iteration: 4070 Loss: 0.00011089876631691312\n",
      "Iteration: 4071 Loss: 0.000110801496632463\n",
      "Iteration: 4072 Loss: 0.00011070431233544818\n",
      "Iteration: 4073 Loss: 0.00011060721335031676\n",
      "Iteration: 4074 Loss: 0.00011051019960121759\n",
      "Iteration: 4075 Loss: 0.00011041327101226234\n",
      "Iteration: 4076 Loss: 0.00011031642750768274\n",
      "Iteration: 4077 Loss: 0.00011021966901164923\n",
      "Iteration: 4078 Loss: 0.00011012299544892608\n",
      "Iteration: 4079 Loss: 0.00011002640674387035\n",
      "Iteration: 4080 Loss: 0.00010992990282096649\n",
      "Iteration: 4081 Loss: 0.00010983348360476007\n",
      "Iteration: 4082 Loss: 0.00010973714901999092\n",
      "Iteration: 4083 Loss: 0.00010964089899135201\n",
      "Iteration: 4084 Loss: 0.00010954473344345248\n",
      "Iteration: 4085 Loss: 0.00010944865230178465\n",
      "Iteration: 4086 Loss: 0.00010935265549092148\n",
      "Iteration: 4087 Loss: 0.00010925674293595809\n",
      "Iteration: 4088 Loss: 0.00010916091456165992\n",
      "Iteration: 4089 Loss: 0.00010906517029304018\n",
      "Iteration: 4090 Loss: 0.0001089695100558247\n",
      "Iteration: 4091 Loss: 0.0001088739337749327\n",
      "Iteration: 4092 Loss: 0.00010877844137543679\n",
      "Iteration: 4093 Loss: 0.00010868303278363794\n",
      "Iteration: 4094 Loss: 0.00010858770792415487\n",
      "Iteration: 4095 Loss: 0.00010849246672322841\n",
      "Iteration: 4096 Loss: 0.0001083973091061311\n",
      "Iteration: 4097 Loss: 0.00010830223499858042\n",
      "Iteration: 4098 Loss: 0.00010820724432606558\n",
      "Iteration: 4099 Loss: 0.00010811233701471774\n",
      "Iteration: 4100 Loss: 0.00010801751299015913\n",
      "Iteration: 4101 Loss: 0.00010792277217834674\n",
      "Iteration: 4102 Loss: 0.00010782811450534379\n",
      "Iteration: 4103 Loss: 0.00010773353989731955\n",
      "Iteration: 4104 Loss: 0.00010763904828017022\n",
      "Iteration: 4105 Loss: 0.00010754463958038006\n",
      "Iteration: 4106 Loss: 0.00010745031372409176\n",
      "Iteration: 4107 Loss: 0.00010735607063801319\n",
      "Iteration: 4108 Loss: 0.00010726191024785509\n",
      "Iteration: 4109 Loss: 0.00010716783248059718\n",
      "Iteration: 4110 Loss: 0.00010707383726271642\n",
      "Iteration: 4111 Loss: 0.00010697992452088775\n",
      "Iteration: 4112 Loss: 0.00010688609418135364\n",
      "Iteration: 4113 Loss: 0.00010679234617132587\n",
      "Iteration: 4114 Loss: 0.0001066986804171999\n",
      "Iteration: 4115 Loss: 0.00010660509684619868\n",
      "Iteration: 4116 Loss: 0.00010651159538498496\n",
      "Iteration: 4117 Loss: 0.00010641817596068226\n",
      "Iteration: 4118 Loss: 0.00010632483850052327\n",
      "Iteration: 4119 Loss: 0.00010623158293140243\n",
      "Iteration: 4120 Loss: 0.0001061384091802725\n",
      "Iteration: 4121 Loss: 0.00010604531717479402\n",
      "Iteration: 4122 Loss: 0.00010595230684237783\n",
      "Iteration: 4123 Loss: 0.00010585937811002771\n",
      "Iteration: 4124 Loss: 0.00010576653090555865\n",
      "Iteration: 4125 Loss: 0.00010567376515636422\n",
      "Iteration: 4126 Loss: 0.00010558108078962074\n",
      "Iteration: 4127 Loss: 0.00010548847773365157\n",
      "Iteration: 4128 Loss: 0.00010539595591581126\n",
      "Iteration: 4129 Loss: 0.00010530351526392711\n",
      "Iteration: 4130 Loss: 0.00010521115570582969\n",
      "Iteration: 4131 Loss: 0.00010511887716923944\n",
      "Iteration: 4132 Loss: 0.00010502667958245282\n",
      "Iteration: 4133 Loss: 0.0001049345628735595\n",
      "Iteration: 4134 Loss: 0.00010484252697046143\n",
      "Iteration: 4135 Loss: 0.0001047505718013878\n",
      "Iteration: 4136 Loss: 0.00010465869729428575\n",
      "Iteration: 4137 Loss: 0.00010456690337803782\n",
      "Iteration: 4138 Loss: 0.00010447518998058669\n",
      "Iteration: 4139 Loss: 0.0001043835570302554\n",
      "Iteration: 4140 Loss: 0.00010429200445592906\n",
      "Iteration: 4141 Loss: 0.00010420053218571329\n",
      "Iteration: 4142 Loss: 0.00010410914014851365\n",
      "Iteration: 4143 Loss: 0.00010401782827281208\n",
      "Iteration: 4144 Loss: 0.00010392659648788095\n",
      "Iteration: 4145 Loss: 0.00010383544472248964\n",
      "Iteration: 4146 Loss: 0.00010374437290449098\n",
      "Iteration: 4147 Loss: 0.00010365338096380748\n",
      "Iteration: 4148 Loss: 0.00010356246882911581\n",
      "Iteration: 4149 Loss: 0.00010347163642933871\n",
      "Iteration: 4150 Loss: 0.00010338088369394317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4151 Loss: 0.00010329021055175963\n",
      "Iteration: 4152 Loss: 0.0001031996169324919\n",
      "Iteration: 4153 Loss: 0.00010310910276484707\n",
      "Iteration: 4154 Loss: 0.00010301866797889039\n",
      "Iteration: 4155 Loss: 0.00010292831250380162\n",
      "Iteration: 4156 Loss: 0.00010283803626895222\n",
      "Iteration: 4157 Loss: 0.0001027478392037394\n",
      "Iteration: 4158 Loss: 0.0001026577212379045\n",
      "Iteration: 4159 Loss: 0.00010256768230152501\n",
      "Iteration: 4160 Loss: 0.00010247772232407821\n",
      "Iteration: 4161 Loss: 0.00010238784123565794\n",
      "Iteration: 4162 Loss: 0.0001022980389659873\n",
      "Iteration: 4163 Loss: 0.00010220831544487319\n",
      "Iteration: 4164 Loss: 0.0001021186706023934\n",
      "Iteration: 4165 Loss: 0.00010202910436864732\n",
      "Iteration: 4166 Loss: 0.00010193961667376026\n",
      "Iteration: 4167 Loss: 0.000101850207448032\n",
      "Iteration: 4168 Loss: 0.0001017608766214573\n",
      "Iteration: 4169 Loss: 0.00010167162412476815\n",
      "Iteration: 4170 Loss: 0.00010158244988813455\n",
      "Iteration: 4171 Loss: 0.00010149335384191926\n",
      "Iteration: 4172 Loss: 0.00010140433591688339\n",
      "Iteration: 4173 Loss: 0.00010131539604338266\n",
      "Iteration: 4174 Loss: 0.00010122653415199055\n",
      "Iteration: 4175 Loss: 0.00010113775017353595\n",
      "Iteration: 4176 Loss: 0.00010104904403877312\n",
      "Iteration: 4177 Loss: 0.00010096041567863093\n",
      "Iteration: 4178 Loss: 0.00010087186502384326\n",
      "Iteration: 4179 Loss: 0.00010078339200556023\n",
      "Iteration: 4180 Loss: 0.00010069499655454831\n",
      "Iteration: 4181 Loss: 0.00010060667860189259\n",
      "Iteration: 4182 Loss: 0.00010051843807888109\n",
      "Iteration: 4183 Loss: 0.00010043027491673498\n",
      "Iteration: 4184 Loss: 0.00010034218904653556\n",
      "Iteration: 4185 Loss: 0.00010025418040001417\n",
      "Iteration: 4186 Loss: 0.00010016624890801885\n",
      "Iteration: 4187 Loss: 0.00010007839450258992\n",
      "Iteration: 4188 Loss: 9.999061711447813e-05\n",
      "Iteration: 4189 Loss: 9.990291667580036e-05\n",
      "Iteration: 4190 Loss: 9.981529311785118e-05\n",
      "Iteration: 4191 Loss: 9.972774637273707e-05\n",
      "Iteration: 4192 Loss: 9.964027637148432e-05\n",
      "Iteration: 4193 Loss: 9.955288304657247e-05\n",
      "Iteration: 4194 Loss: 9.946556632975288e-05\n",
      "Iteration: 4195 Loss: 9.937832615290865e-05\n",
      "Iteration: 4196 Loss: 9.929116244819389e-05\n",
      "Iteration: 4197 Loss: 9.920407514711098e-05\n",
      "Iteration: 4198 Loss: 9.911706418240412e-05\n",
      "Iteration: 4199 Loss: 9.903012948583985e-05\n",
      "Iteration: 4200 Loss: 9.894327098981016e-05\n",
      "Iteration: 4201 Loss: 9.885648862658275e-05\n",
      "Iteration: 4202 Loss: 9.876978232849676e-05\n",
      "Iteration: 4203 Loss: 9.868315202817351e-05\n",
      "Iteration: 4204 Loss: 9.859659765824725e-05\n",
      "Iteration: 4205 Loss: 9.851011915048297e-05\n",
      "Iteration: 4206 Loss: 9.842371643821358e-05\n",
      "Iteration: 4207 Loss: 9.833738945398178e-05\n",
      "Iteration: 4208 Loss: 9.825113813019282e-05\n",
      "Iteration: 4209 Loss: 9.816496239985357e-05\n",
      "Iteration: 4210 Loss: 9.80788621959761e-05\n",
      "Iteration: 4211 Loss: 9.799283745145475e-05\n",
      "Iteration: 4212 Loss: 9.79068880983701e-05\n",
      "Iteration: 4213 Loss: 9.782101407076087e-05\n",
      "Iteration: 4214 Loss: 9.773521530151858e-05\n",
      "Iteration: 4215 Loss: 9.764949172333651e-05\n",
      "Iteration: 4216 Loss: 9.756384326990886e-05\n",
      "Iteration: 4217 Loss: 9.747826987431618e-05\n",
      "Iteration: 4218 Loss: 9.739277146995116e-05\n",
      "Iteration: 4219 Loss: 9.730734799011031e-05\n",
      "Iteration: 4220 Loss: 9.722199936835859e-05\n",
      "Iteration: 4221 Loss: 9.713672553821286e-05\n",
      "Iteration: 4222 Loss: 9.705152643320519e-05\n",
      "Iteration: 4223 Loss: 9.696640198676223e-05\n",
      "Iteration: 4224 Loss: 9.688135213289178e-05\n",
      "Iteration: 4225 Loss: 9.679637680502986e-05\n",
      "Iteration: 4226 Loss: 9.671147593703999e-05\n",
      "Iteration: 4227 Loss: 9.662664946281132e-05\n",
      "Iteration: 4228 Loss: 9.654189731634423e-05\n",
      "Iteration: 4229 Loss: 9.645721943156081e-05\n",
      "Iteration: 4230 Loss: 9.637261574261956e-05\n",
      "Iteration: 4231 Loss: 9.628808618331848e-05\n",
      "Iteration: 4232 Loss: 9.620363068805965e-05\n",
      "Iteration: 4233 Loss: 9.61192491910559e-05\n",
      "Iteration: 4234 Loss: 9.603494162665043e-05\n",
      "Iteration: 4235 Loss: 9.595070792854294e-05\n",
      "Iteration: 4236 Loss: 9.586654803197147e-05\n",
      "Iteration: 4237 Loss: 9.57824618711118e-05\n",
      "Iteration: 4238 Loss: 9.569844938010267e-05\n",
      "Iteration: 4239 Loss: 9.561451049401254e-05\n",
      "Iteration: 4240 Loss: 9.553064514725635e-05\n",
      "Iteration: 4241 Loss: 9.544685327466069e-05\n",
      "Iteration: 4242 Loss: 9.536313481102882e-05\n",
      "Iteration: 4243 Loss: 9.527948969067979e-05\n",
      "Iteration: 4244 Loss: 9.519591784923837e-05\n",
      "Iteration: 4245 Loss: 9.511241922089793e-05\n",
      "Iteration: 4246 Loss: 9.502899374126206e-05\n",
      "Iteration: 4247 Loss: 9.494564134481671e-05\n",
      "Iteration: 4248 Loss: 9.486236196697599e-05\n",
      "Iteration: 4249 Loss: 9.477915554239564e-05\n",
      "Iteration: 4250 Loss: 9.469602200685973e-05\n",
      "Iteration: 4251 Loss: 9.461296129524827e-05\n",
      "Iteration: 4252 Loss: 9.452997334330231e-05\n",
      "Iteration: 4253 Loss: 9.444705808646261e-05\n",
      "Iteration: 4254 Loss: 9.436421545965409e-05\n",
      "Iteration: 4255 Loss: 9.428144539871029e-05\n",
      "Iteration: 4256 Loss: 9.419874783902554e-05\n",
      "Iteration: 4257 Loss: 9.411612271614534e-05\n",
      "Iteration: 4258 Loss: 9.40335699661269e-05\n",
      "Iteration: 4259 Loss: 9.39510895243111e-05\n",
      "Iteration: 4260 Loss: 9.386868132665874e-05\n",
      "Iteration: 4261 Loss: 9.378634530921589e-05\n",
      "Iteration: 4262 Loss: 9.370408140769135e-05\n",
      "Iteration: 4263 Loss: 9.362188955809052e-05\n",
      "Iteration: 4264 Loss: 9.35397696964028e-05\n",
      "Iteration: 4265 Loss: 9.345772175858902e-05\n",
      "Iteration: 4266 Loss: 9.337574568101136e-05\n",
      "Iteration: 4267 Loss: 9.329384139972442e-05\n",
      "Iteration: 4268 Loss: 9.321200885130538e-05\n",
      "Iteration: 4269 Loss: 9.31302479715126e-05\n",
      "Iteration: 4270 Loss: 9.304855869685844e-05\n",
      "Iteration: 4271 Loss: 9.296694096390711e-05\n",
      "Iteration: 4272 Loss: 9.288539470916987e-05\n",
      "Iteration: 4273 Loss: 9.28039198689571e-05\n",
      "Iteration: 4274 Loss: 9.272251637991831e-05\n",
      "Iteration: 4275 Loss: 9.2641184178932e-05\n",
      "Iteration: 4276 Loss: 9.255992320275424e-05\n",
      "Iteration: 4277 Loss: 9.247873338788035e-05\n",
      "Iteration: 4278 Loss: 9.239761467113427e-05\n",
      "Iteration: 4279 Loss: 9.231656698967147e-05\n",
      "Iteration: 4280 Loss: 9.223559027985928e-05\n",
      "Iteration: 4281 Loss: 9.215468447908348e-05\n",
      "Iteration: 4282 Loss: 9.207384952448567e-05\n",
      "Iteration: 4283 Loss: 9.199308535295826e-05\n",
      "Iteration: 4284 Loss: 9.191239190178083e-05\n",
      "Iteration: 4285 Loss: 9.183176910801935e-05\n",
      "Iteration: 4286 Loss: 9.175121690912671e-05\n",
      "Iteration: 4287 Loss: 9.167073524252769e-05\n",
      "Iteration: 4288 Loss: 9.159032404521827e-05\n",
      "Iteration: 4289 Loss: 9.150998325488452e-05\n",
      "Iteration: 4290 Loss: 9.142971280897498e-05\n",
      "Iteration: 4291 Loss: 9.134951264508497e-05\n",
      "Iteration: 4292 Loss: 9.126938270116569e-05\n",
      "Iteration: 4293 Loss: 9.118932291419519e-05\n",
      "Iteration: 4294 Loss: 9.110933322219112e-05\n",
      "Iteration: 4295 Loss: 9.102941356288421e-05\n",
      "Iteration: 4296 Loss: 9.09495638742327e-05\n",
      "Iteration: 4297 Loss: 9.086978409404767e-05\n",
      "Iteration: 4298 Loss: 9.079007416019698e-05\n",
      "Iteration: 4299 Loss: 9.07104340107903e-05\n",
      "Iteration: 4300 Loss: 9.063086358351034e-05\n",
      "Iteration: 4301 Loss: 9.055136281718121e-05\n",
      "Iteration: 4302 Loss: 9.04719316495044e-05\n",
      "Iteration: 4303 Loss: 9.039257001891529e-05\n",
      "Iteration: 4304 Loss: 9.031327786355791e-05\n",
      "Iteration: 4305 Loss: 9.023405512180071e-05\n",
      "Iteration: 4306 Loss: 9.0154901732379e-05\n",
      "Iteration: 4307 Loss: 9.007581763276602e-05\n",
      "Iteration: 4308 Loss: 8.999680276242574e-05\n",
      "Iteration: 4309 Loss: 8.991785705935503e-05\n",
      "Iteration: 4310 Loss: 8.983898046212642e-05\n",
      "Iteration: 4311 Loss: 8.976017290974388e-05\n",
      "Iteration: 4312 Loss: 8.968143434091605e-05\n",
      "Iteration: 4313 Loss: 8.960276469394551e-05\n",
      "Iteration: 4314 Loss: 8.95241639084844e-05\n",
      "Iteration: 4315 Loss: 8.94456319227959e-05\n",
      "Iteration: 4316 Loss: 8.936716867588412e-05\n",
      "Iteration: 4317 Loss: 8.928877410679244e-05\n",
      "Iteration: 4318 Loss: 8.921044815479117e-05\n",
      "Iteration: 4319 Loss: 8.913219075876039e-05\n",
      "Iteration: 4320 Loss: 8.905400185777461e-05\n",
      "Iteration: 4321 Loss: 8.897588139120894e-05\n",
      "Iteration: 4322 Loss: 8.889782929841172e-05\n",
      "Iteration: 4323 Loss: 8.881984551842144e-05\n",
      "Iteration: 4324 Loss: 8.874192999060008e-05\n",
      "Iteration: 4325 Loss: 8.866408265454086e-05\n",
      "Iteration: 4326 Loss: 8.858630344935665e-05\n",
      "Iteration: 4327 Loss: 8.850859231532812e-05\n",
      "Iteration: 4328 Loss: 8.843094919160798e-05\n",
      "Iteration: 4329 Loss: 8.835337401762508e-05\n",
      "Iteration: 4330 Loss: 8.82758667335422e-05\n",
      "Iteration: 4331 Loss: 8.819842727880194e-05\n",
      "Iteration: 4332 Loss: 8.812105559340768e-05\n",
      "Iteration: 4333 Loss: 8.804375161699522e-05\n",
      "Iteration: 4334 Loss: 8.796651528984899e-05\n",
      "Iteration: 4335 Loss: 8.788934655169915e-05\n",
      "Iteration: 4336 Loss: 8.781224534247021e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4337 Loss: 8.773521160221585e-05\n",
      "Iteration: 4338 Loss: 8.76582452713931e-05\n",
      "Iteration: 4339 Loss: 8.758134628993241e-05\n",
      "Iteration: 4340 Loss: 8.750451459782882e-05\n",
      "Iteration: 4341 Loss: 8.742775013601957e-05\n",
      "Iteration: 4342 Loss: 8.735105284428988e-05\n",
      "Iteration: 4343 Loss: 8.727442266326945e-05\n",
      "Iteration: 4344 Loss: 8.71978595334697e-05\n",
      "Iteration: 4345 Loss: 8.712136339521692e-05\n",
      "Iteration: 4346 Loss: 8.704493418908372e-05\n",
      "Iteration: 4347 Loss: 8.696857185595183e-05\n",
      "Iteration: 4348 Loss: 8.689227633606247e-05\n",
      "Iteration: 4349 Loss: 8.681604757030714e-05\n",
      "Iteration: 4350 Loss: 8.673988549946754e-05\n",
      "Iteration: 4351 Loss: 8.666379006433971e-05\n",
      "Iteration: 4352 Loss: 8.658776120596521e-05\n",
      "Iteration: 4353 Loss: 8.65117988650736e-05\n",
      "Iteration: 4354 Loss: 8.643590298282054e-05\n",
      "Iteration: 4355 Loss: 8.636007350012174e-05\n",
      "Iteration: 4356 Loss: 8.628431035806641e-05\n",
      "Iteration: 4357 Loss: 8.620861349777821e-05\n",
      "Iteration: 4358 Loss: 8.613298286065022e-05\n",
      "Iteration: 4359 Loss: 8.605741838771309e-05\n",
      "Iteration: 4360 Loss: 8.598192002029548e-05\n",
      "Iteration: 4361 Loss: 8.590648769983428e-05\n",
      "Iteration: 4362 Loss: 8.583112136772614e-05\n",
      "Iteration: 4363 Loss: 8.575582096537632e-05\n",
      "Iteration: 4364 Loss: 8.568058643450982e-05\n",
      "Iteration: 4365 Loss: 8.560541771647995e-05\n",
      "Iteration: 4366 Loss: 8.553031475272606e-05\n",
      "Iteration: 4367 Loss: 8.545527748505203e-05\n",
      "Iteration: 4368 Loss: 8.538030585523378e-05\n",
      "Iteration: 4369 Loss: 8.53053998049554e-05\n",
      "Iteration: 4370 Loss: 8.523055927646681e-05\n",
      "Iteration: 4371 Loss: 8.51557842109284e-05\n",
      "Iteration: 4372 Loss: 8.508107455083509e-05\n",
      "Iteration: 4373 Loss: 8.500643023783888e-05\n",
      "Iteration: 4374 Loss: 8.493185121401594e-05\n",
      "Iteration: 4375 Loss: 8.485733742164693e-05\n",
      "Iteration: 4376 Loss: 8.478288880240066e-05\n",
      "Iteration: 4377 Loss: 8.470850529894292e-05\n",
      "Iteration: 4378 Loss: 8.463418685342213e-05\n",
      "Iteration: 4379 Loss: 8.455993340838055e-05\n",
      "Iteration: 4380 Loss: 8.44857449055876e-05\n",
      "Iteration: 4381 Loss: 8.441162128770949e-05\n",
      "Iteration: 4382 Loss: 8.433756249714602e-05\n",
      "Iteration: 4383 Loss: 8.426356847636288e-05\n",
      "Iteration: 4384 Loss: 8.418963916819032e-05\n",
      "Iteration: 4385 Loss: 8.411577451481053e-05\n",
      "Iteration: 4386 Loss: 8.404197445901525e-05\n",
      "Iteration: 4387 Loss: 8.396823894382808e-05\n",
      "Iteration: 4388 Loss: 8.38945679116435e-05\n",
      "Iteration: 4389 Loss: 8.38209613056447e-05\n",
      "Iteration: 4390 Loss: 8.374741906801791e-05\n",
      "Iteration: 4391 Loss: 8.367394114234197e-05\n",
      "Iteration: 4392 Loss: 8.36005274713315e-05\n",
      "Iteration: 4393 Loss: 8.352717799797733e-05\n",
      "Iteration: 4394 Loss: 8.345389266516342e-05\n",
      "Iteration: 4395 Loss: 8.338067141624151e-05\n",
      "Iteration: 4396 Loss: 8.330751419440866e-05\n",
      "Iteration: 4397 Loss: 8.323442094281032e-05\n",
      "Iteration: 4398 Loss: 8.316139160450876e-05\n",
      "Iteration: 4399 Loss: 8.308842612307612e-05\n",
      "Iteration: 4400 Loss: 8.301552444181406e-05\n",
      "Iteration: 4401 Loss: 8.294268650415905e-05\n",
      "Iteration: 4402 Loss: 8.286991225349533e-05\n",
      "Iteration: 4403 Loss: 8.279720163321457e-05\n",
      "Iteration: 4404 Loss: 8.272455458730677e-05\n",
      "Iteration: 4405 Loss: 8.265197105888627e-05\n",
      "Iteration: 4406 Loss: 8.257945099196588e-05\n",
      "Iteration: 4407 Loss: 8.250699432996579e-05\n",
      "Iteration: 4408 Loss: 8.243460101706172e-05\n",
      "Iteration: 4409 Loss: 8.23622709968057e-05\n",
      "Iteration: 4410 Loss: 8.229000421330314e-05\n",
      "Iteration: 4411 Loss: 8.221780061018325e-05\n",
      "Iteration: 4412 Loss: 8.214566013176576e-05\n",
      "Iteration: 4413 Loss: 8.207358272155013e-05\n",
      "Iteration: 4414 Loss: 8.200156832416051e-05\n",
      "Iteration: 4415 Loss: 8.192961688336708e-05\n",
      "Iteration: 4416 Loss: 8.185772834365637e-05\n",
      "Iteration: 4417 Loss: 8.178590264891765e-05\n",
      "Iteration: 4418 Loss: 8.171413974359508e-05\n",
      "Iteration: 4419 Loss: 8.164243957193613e-05\n",
      "Iteration: 4420 Loss: 8.157080207829303e-05\n",
      "Iteration: 4421 Loss: 8.149922720713573e-05\n",
      "Iteration: 4422 Loss: 8.142771490321428e-05\n",
      "Iteration: 4423 Loss: 8.135626511047913e-05\n",
      "Iteration: 4424 Loss: 8.128487777387705e-05\n",
      "Iteration: 4425 Loss: 8.121355283814478e-05\n",
      "Iteration: 4426 Loss: 8.114229024757335e-05\n",
      "Iteration: 4427 Loss: 8.107108994723908e-05\n",
      "Iteration: 4428 Loss: 8.099995188180774e-05\n",
      "Iteration: 4429 Loss: 8.092887599608112e-05\n",
      "Iteration: 4430 Loss: 8.085786223498749e-05\n",
      "Iteration: 4431 Loss: 8.078691054314302e-05\n",
      "Iteration: 4432 Loss: 8.071602086551743e-05\n",
      "Iteration: 4433 Loss: 8.064519314746949e-05\n",
      "Iteration: 4434 Loss: 8.057442733400232e-05\n",
      "Iteration: 4435 Loss: 8.050372337009443e-05\n",
      "Iteration: 4436 Loss: 8.043308120083475e-05\n",
      "Iteration: 4437 Loss: 8.036250077172281e-05\n",
      "Iteration: 4438 Loss: 8.0291982027608e-05\n",
      "Iteration: 4439 Loss: 8.022152491433317e-05\n",
      "Iteration: 4440 Loss: 8.015112937721029e-05\n",
      "Iteration: 4441 Loss: 8.008079536124462e-05\n",
      "Iteration: 4442 Loss: 8.001052281225852e-05\n",
      "Iteration: 4443 Loss: 7.994031167561554e-05\n",
      "Iteration: 4444 Loss: 7.987016189654335e-05\n",
      "Iteration: 4445 Loss: 7.980007342085467e-05\n",
      "Iteration: 4446 Loss: 7.973004619451583e-05\n",
      "Iteration: 4447 Loss: 7.966008016285844e-05\n",
      "Iteration: 4448 Loss: 7.959017527169222e-05\n",
      "Iteration: 4449 Loss: 7.952033146732883e-05\n",
      "Iteration: 4450 Loss: 7.945054869488592e-05\n",
      "Iteration: 4451 Loss: 7.938082690048132e-05\n",
      "Iteration: 4452 Loss: 7.931116603038859e-05\n",
      "Iteration: 4453 Loss: 7.924156603017111e-05\n",
      "Iteration: 4454 Loss: 7.917202684625346e-05\n",
      "Iteration: 4455 Loss: 7.910254842442657e-05\n",
      "Iteration: 4456 Loss: 7.903313071113593e-05\n",
      "Iteration: 4457 Loss: 7.896377365251423e-05\n",
      "Iteration: 4458 Loss: 7.889447719432868e-05\n",
      "Iteration: 4459 Loss: 7.882524128313338e-05\n",
      "Iteration: 4460 Loss: 7.875606586532081e-05\n",
      "Iteration: 4461 Loss: 7.868695088757607e-05\n",
      "Iteration: 4462 Loss: 7.86178962957771e-05\n",
      "Iteration: 4463 Loss: 7.854890203639971e-05\n",
      "Iteration: 4464 Loss: 7.847996805657146e-05\n",
      "Iteration: 4465 Loss: 7.841109430239233e-05\n",
      "Iteration: 4466 Loss: 7.83422807206147e-05\n",
      "Iteration: 4467 Loss: 7.8273527257888e-05\n",
      "Iteration: 4468 Loss: 7.820483386073598e-05\n",
      "Iteration: 4469 Loss: 7.813620047624961e-05\n",
      "Iteration: 4470 Loss: 7.806762705139149e-05\n",
      "Iteration: 4471 Loss: 7.799911353268467e-05\n",
      "Iteration: 4472 Loss: 7.793065986685101e-05\n",
      "Iteration: 4473 Loss: 7.786226600105622e-05\n",
      "Iteration: 4474 Loss: 7.779393188214628e-05\n",
      "Iteration: 4475 Loss: 7.772565745726396e-05\n",
      "Iteration: 4476 Loss: 7.765744267388415e-05\n",
      "Iteration: 4477 Loss: 7.75892874788255e-05\n",
      "Iteration: 4478 Loss: 7.752119181922873e-05\n",
      "Iteration: 4479 Loss: 7.745315564253691e-05\n",
      "Iteration: 4480 Loss: 7.738517889575986e-05\n",
      "Iteration: 4481 Loss: 7.731726152626502e-05\n",
      "Iteration: 4482 Loss: 7.724940348135044e-05\n",
      "Iteration: 4483 Loss: 7.71816047089726e-05\n",
      "Iteration: 4484 Loss: 7.711386515610396e-05\n",
      "Iteration: 4485 Loss: 7.704618477045028e-05\n",
      "Iteration: 4486 Loss: 7.697856349971145e-05\n",
      "Iteration: 4487 Loss: 7.69110012912767e-05\n",
      "Iteration: 4488 Loss: 7.684349809298044e-05\n",
      "Iteration: 4489 Loss: 7.677605385227053e-05\n",
      "Iteration: 4490 Loss: 7.670866851691875e-05\n",
      "Iteration: 4491 Loss: 7.664134203462633e-05\n",
      "Iteration: 4492 Loss: 7.657407435367831e-05\n",
      "Iteration: 4493 Loss: 7.650686542191026e-05\n",
      "Iteration: 4494 Loss: 7.64397151870768e-05\n",
      "Iteration: 4495 Loss: 7.637262359717453e-05\n",
      "Iteration: 4496 Loss: 7.630559060030664e-05\n",
      "Iteration: 4497 Loss: 7.623861614470303e-05\n",
      "Iteration: 4498 Loss: 7.617170017811118e-05\n",
      "Iteration: 4499 Loss: 7.610484264881763e-05\n",
      "Iteration: 4500 Loss: 7.60380435050082e-05\n",
      "Iteration: 4501 Loss: 7.597130269551634e-05\n",
      "Iteration: 4502 Loss: 7.59046201680102e-05\n",
      "Iteration: 4503 Loss: 7.583799587107388e-05\n",
      "Iteration: 4504 Loss: 7.577142975295434e-05\n",
      "Iteration: 4505 Loss: 7.570492176227994e-05\n",
      "Iteration: 4506 Loss: 7.563847184744318e-05\n",
      "Iteration: 4507 Loss: 7.557207995690789e-05\n",
      "Iteration: 4508 Loss: 7.550574603982337e-05\n",
      "Iteration: 4509 Loss: 7.543947004408497e-05\n",
      "Iteration: 4510 Loss: 7.537325191886839e-05\n",
      "Iteration: 4511 Loss: 7.530709161242898e-05\n",
      "Iteration: 4512 Loss: 7.524098907433025e-05\n",
      "Iteration: 4513 Loss: 7.517494425277114e-05\n",
      "Iteration: 4514 Loss: 7.510895709663965e-05\n",
      "Iteration: 4515 Loss: 7.504302755497397e-05\n",
      "Iteration: 4516 Loss: 7.49771555769109e-05\n",
      "Iteration: 4517 Loss: 7.49113411112559e-05\n",
      "Iteration: 4518 Loss: 7.484558410715622e-05\n",
      "Iteration: 4519 Loss: 7.4779884513508e-05\n",
      "Iteration: 4520 Loss: 7.471424227961956e-05\n",
      "Iteration: 4521 Loss: 7.464865735466761e-05\n",
      "Iteration: 4522 Loss: 7.458312968788562e-05\n",
      "Iteration: 4523 Loss: 7.451765922806769e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4524 Loss: 7.445224592525427e-05\n",
      "Iteration: 4525 Loss: 7.438688972867077e-05\n",
      "Iteration: 4526 Loss: 7.432159058731501e-05\n",
      "Iteration: 4527 Loss: 7.425634845125018e-05\n",
      "Iteration: 4528 Loss: 7.419116326967402e-05\n",
      "Iteration: 4529 Loss: 7.412603499187053e-05\n",
      "Iteration: 4530 Loss: 7.406096356784321e-05\n",
      "Iteration: 4531 Loss: 7.399594894709514e-05\n",
      "Iteration: 4532 Loss: 7.393099107961656e-05\n",
      "Iteration: 4533 Loss: 7.386608991474652e-05\n",
      "Iteration: 4534 Loss: 7.38012454019427e-05\n",
      "Iteration: 4535 Loss: 7.373645749162747e-05\n",
      "Iteration: 4536 Loss: 7.367172613368864e-05\n",
      "Iteration: 4537 Loss: 7.360705127765676e-05\n",
      "Iteration: 4538 Loss: 7.354243287371011e-05\n",
      "Iteration: 4539 Loss: 7.347787087170987e-05\n",
      "Iteration: 4540 Loss: 7.341336522176158e-05\n",
      "Iteration: 4541 Loss: 7.334891587384576e-05\n",
      "Iteration: 4542 Loss: 7.32845227783538e-05\n",
      "Iteration: 4543 Loss: 7.322018588511976e-05\n",
      "Iteration: 4544 Loss: 7.315590514497435e-05\n",
      "Iteration: 4545 Loss: 7.309168050784789e-05\n",
      "Iteration: 4546 Loss: 7.302751192403404e-05\n",
      "Iteration: 4547 Loss: 7.296339934374176e-05\n",
      "Iteration: 4548 Loss: 7.289934271768662e-05\n",
      "Iteration: 4549 Loss: 7.283534199613384e-05\n",
      "Iteration: 4550 Loss: 7.277139712964965e-05\n",
      "Iteration: 4551 Loss: 7.270750806881012e-05\n",
      "Iteration: 4552 Loss: 7.264367476407106e-05\n",
      "Iteration: 4553 Loss: 7.257989716643984e-05\n",
      "Iteration: 4554 Loss: 7.251617522601367e-05\n",
      "Iteration: 4555 Loss: 7.245250889385921e-05\n",
      "Iteration: 4556 Loss: 7.238889812038875e-05\n",
      "Iteration: 4557 Loss: 7.232534285700836e-05\n",
      "Iteration: 4558 Loss: 7.226184305412521e-05\n",
      "Iteration: 4559 Loss: 7.219839866303672e-05\n",
      "Iteration: 4560 Loss: 7.213500963470728e-05\n",
      "Iteration: 4561 Loss: 7.207167591958538e-05\n",
      "Iteration: 4562 Loss: 7.200839746896854e-05\n",
      "Iteration: 4563 Loss: 7.194517423425547e-05\n",
      "Iteration: 4564 Loss: 7.188200616632267e-05\n",
      "Iteration: 4565 Loss: 7.181889321648395e-05\n",
      "Iteration: 4566 Loss: 7.175583533587174e-05\n",
      "Iteration: 4567 Loss: 7.169283247613388e-05\n",
      "Iteration: 4568 Loss: 7.16298845884308e-05\n",
      "Iteration: 4569 Loss: 7.156699162417454e-05\n",
      "Iteration: 4570 Loss: 7.150415353437511e-05\n",
      "Iteration: 4571 Loss: 7.144137027131878e-05\n",
      "Iteration: 4572 Loss: 7.137864178613356e-05\n",
      "Iteration: 4573 Loss: 7.131596803084295e-05\n",
      "Iteration: 4574 Loss: 7.125334895683176e-05\n",
      "Iteration: 4575 Loss: 7.119078451613435e-05\n",
      "Iteration: 4576 Loss: 7.112827466059218e-05\n",
      "Iteration: 4577 Loss: 7.106581934235755e-05\n",
      "Iteration: 4578 Loss: 7.100341851354287e-05\n",
      "Iteration: 4579 Loss: 7.094107212636285e-05\n",
      "Iteration: 4580 Loss: 7.08787801335604e-05\n",
      "Iteration: 4581 Loss: 7.081654248770957e-05\n",
      "Iteration: 4582 Loss: 7.075435914219761e-05\n",
      "Iteration: 4583 Loss: 7.069223005043642e-05\n",
      "Iteration: 4584 Loss: 7.063015516653929e-05\n",
      "Iteration: 4585 Loss: 7.056813444611335e-05\n",
      "Iteration: 4586 Loss: 7.050616784554189e-05\n",
      "Iteration: 4587 Loss: 7.044425532406528e-05\n",
      "Iteration: 4588 Loss: 7.038239684503103e-05\n",
      "Iteration: 4589 Loss: 7.03205923792956e-05\n",
      "Iteration: 4590 Loss: 7.025884191357571e-05\n",
      "Iteration: 4591 Loss: 7.01971454708327e-05\n",
      "Iteration: 4592 Loss: 7.013550317521255e-05\n",
      "Iteration: 4593 Loss: 7.007391552914972e-05\n",
      "Iteration: 4594 Loss: 7.00123857506608e-05\n",
      "Iteration: 4595 Loss: 6.98472073751911e-05\n",
      "Iteration: 4596 Loss: 6.91928712353401e-05\n",
      "Iteration: 4597 Loss: 6.820932272791791e-05\n",
      "Iteration: 4598 Loss: 6.698443477821923e-05\n",
      "Iteration: 4599 Loss: 6.560339787259677e-05\n",
      "Iteration: 4600 Loss: 6.414403761788336e-05\n",
      "Iteration: 4601 Loss: 6.267394206320779e-05\n",
      "Iteration: 4602 Loss: 6.124908340943944e-05\n",
      "Iteration: 4603 Loss: 5.991345258005273e-05\n",
      "Iteration: 4604 Loss: 5.869937615854547e-05\n",
      "Iteration: 4605 Loss: 5.7628360428036136e-05\n",
      "Iteration: 4606 Loss: 5.671226541054068e-05\n",
      "Iteration: 4607 Loss: 5.712125927574776e-05\n",
      "Iteration: 4608 Loss: 5.7615890860893995e-05\n",
      "Iteration: 4609 Loss: 5.796466878516296e-05\n",
      "Iteration: 4610 Loss: 5.81797222207923e-05\n",
      "Iteration: 4611 Loss: 5.8276531601580595e-05\n",
      "Iteration: 4612 Loss: 5.8272160386158226e-05\n",
      "Iteration: 4613 Loss: 5.818456032742881e-05\n",
      "Iteration: 4614 Loss: 5.803166777131383e-05\n",
      "Iteration: 4615 Loss: 5.782984576531423e-05\n",
      "Iteration: 4616 Loss: 5.759349449154001e-05\n",
      "Iteration: 4617 Loss: 5.7336283660610945e-05\n",
      "Iteration: 4618 Loss: 5.7070064245801097e-05\n",
      "Iteration: 4619 Loss: 5.6804737724418555e-05\n",
      "Iteration: 4620 Loss: 5.6548146084336564e-05\n",
      "Iteration: 4621 Loss: 5.630634955131339e-05\n",
      "Iteration: 4622 Loss: 5.608366952742477e-05\n",
      "Iteration: 4623 Loss: 5.588286196107748e-05\n",
      "Iteration: 4624 Loss: 5.571921542871437e-05\n",
      "Iteration: 4625 Loss: 5.567075444651561e-05\n",
      "Iteration: 4626 Loss: 5.569122905860128e-05\n",
      "Iteration: 4627 Loss: 5.569446460570512e-05\n",
      "Iteration: 4628 Loss: 5.568200051437867e-05\n",
      "Iteration: 4629 Loss: 5.565322575093647e-05\n",
      "Iteration: 4630 Loss: 5.5610063398038946e-05\n",
      "Iteration: 4631 Loss: 5.5555126796483895e-05\n",
      "Iteration: 4632 Loss: 5.549104772921062e-05\n",
      "Iteration: 4633 Loss: 5.542008004458344e-05\n",
      "Iteration: 4634 Loss: 5.5344303999140874e-05\n",
      "Iteration: 4635 Loss: 5.52656601649772e-05\n",
      "Iteration: 4636 Loss: 5.518581803490102e-05\n",
      "Iteration: 4637 Loss: 5.510616529550355e-05\n",
      "Iteration: 4638 Loss: 5.502841437943212e-05\n",
      "Iteration: 4639 Loss: 5.495349975001274e-05\n",
      "Iteration: 4640 Loss: 5.4881207253840316e-05\n",
      "Iteration: 4641 Loss: 5.481188244343103e-05\n",
      "Iteration: 4642 Loss: 5.4749668821208994e-05\n",
      "Iteration: 4643 Loss: 5.4708825827805125e-05\n",
      "Iteration: 4644 Loss: 5.466905541675485e-05\n",
      "Iteration: 4645 Loss: 5.46263952605022e-05\n",
      "Iteration: 4646 Loss: 5.4581130933896136e-05\n",
      "Iteration: 4647 Loss: 5.4533594615116146e-05\n",
      "Iteration: 4648 Loss: 5.448414480829967e-05\n",
      "Iteration: 4649 Loss: 5.443314961191367e-05\n",
      "Iteration: 4650 Loss: 5.438098438179856e-05\n",
      "Iteration: 4651 Loss: 5.4327943683401784e-05\n",
      "Iteration: 4652 Loss: 5.427432528482037e-05\n",
      "Iteration: 4653 Loss: 5.4220396977948987e-05\n",
      "Iteration: 4654 Loss: 5.4166387480032424e-05\n",
      "Iteration: 4655 Loss: 5.411248546372341e-05\n",
      "Iteration: 4656 Loss: 5.4058840611223834e-05\n",
      "Iteration: 4657 Loss: 5.400556521502878e-05\n",
      "Iteration: 4658 Loss: 5.395273718617811e-05\n",
      "Iteration: 4659 Loss: 5.390040381212394e-05\n",
      "Iteration: 4660 Loss: 5.384941895296768e-05\n",
      "Iteration: 4661 Loss: 5.380108585456789e-05\n",
      "Iteration: 4662 Loss: 5.375307847965934e-05\n",
      "Iteration: 4663 Loss: 5.370472506646022e-05\n",
      "Iteration: 4664 Loss: 5.365606616399973e-05\n",
      "Iteration: 4665 Loss: 5.360714792840639e-05\n",
      "Iteration: 4666 Loss: 5.3558019452115873e-05\n",
      "Iteration: 4667 Loss: 5.3508731671566924e-05\n",
      "Iteration: 4668 Loss: 5.345933286113061e-05\n",
      "Iteration: 4669 Loss: 5.340986256980208e-05\n",
      "Iteration: 4670 Loss: 5.336036081722675e-05\n",
      "Iteration: 4671 Loss: 5.3310862720833516e-05\n",
      "Iteration: 4672 Loss: 5.32613981052247e-05\n",
      "Iteration: 4673 Loss: 5.321199139195416e-05\n",
      "Iteration: 4674 Loss: 5.3162661719297084e-05\n",
      "Iteration: 4675 Loss: 5.31134232450196e-05\n",
      "Iteration: 4676 Loss: 5.30642855939581e-05\n",
      "Iteration: 4677 Loss: 5.3015254653032954e-05\n",
      "Iteration: 4678 Loss: 5.296652364575975e-05\n",
      "Iteration: 4679 Loss: 5.291818624440972e-05\n",
      "Iteration: 4680 Loss: 5.2869849911394624e-05\n",
      "Iteration: 4681 Loss: 5.282150089425769e-05\n",
      "Iteration: 4682 Loss: 5.277314524108607e-05\n",
      "Iteration: 4683 Loss: 5.272478982955866e-05\n",
      "Iteration: 4684 Loss: 5.2676441907347136e-05\n",
      "Iteration: 4685 Loss: 5.262810873406252e-05\n",
      "Iteration: 4686 Loss: 5.257979786254407e-05\n",
      "Iteration: 4687 Loss: 5.253151516100208e-05\n",
      "Iteration: 4688 Loss: 5.248326638849415e-05\n",
      "Iteration: 4689 Loss: 5.243505666694547e-05\n",
      "Iteration: 4690 Loss: 5.238689032218187e-05\n",
      "Iteration: 4691 Loss: 5.23387708773013e-05\n",
      "Iteration: 4692 Loss: 5.229070108369516e-05\n",
      "Iteration: 4693 Loss: 5.224268298485213e-05\n",
      "Iteration: 4694 Loss: 5.219471808577971e-05\n",
      "Iteration: 4695 Loss: 5.2146808555853915e-05\n",
      "Iteration: 4696 Loss: 5.209897608287374e-05\n",
      "Iteration: 4697 Loss: 5.205122869443787e-05\n",
      "Iteration: 4698 Loss: 5.200352738094613e-05\n",
      "Iteration: 4699 Loss: 5.19558632721963e-05\n",
      "Iteration: 4700 Loss: 5.1908236774303836e-05\n",
      "Iteration: 4701 Loss: 5.1860648635882556e-05\n",
      "Iteration: 4702 Loss: 5.181309968443529e-05\n",
      "Iteration: 4703 Loss: 5.176559084489e-05\n",
      "Iteration: 4704 Loss: 5.17181230100981e-05\n",
      "Iteration: 4705 Loss: 5.167069700951558e-05\n",
      "Iteration: 4706 Loss: 5.1623313593163376e-05\n",
      "Iteration: 4707 Loss: 5.157597342555334e-05\n",
      "Iteration: 4708 Loss: 5.1528677096614256e-05\n",
      "Iteration: 4709 Loss: 5.148142516218472e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4710 Loss: 5.1434218254674624e-05\n",
      "Iteration: 4711 Loss: 5.138705732691635e-05\n",
      "Iteration: 4712 Loss: 5.1339943668662144e-05\n",
      "Iteration: 4713 Loss: 5.129287741191194e-05\n",
      "Iteration: 4714 Loss: 5.124585661990964e-05\n",
      "Iteration: 4715 Loss: 5.119887944138877e-05\n",
      "Iteration: 4716 Loss: 5.1151945050751365e-05\n",
      "Iteration: 4717 Loss: 5.110505317113602e-05\n",
      "Iteration: 4718 Loss: 5.105820374276919e-05\n",
      "Iteration: 4719 Loss: 5.1011396795468014e-05\n",
      "Iteration: 4720 Loss: 5.09646323976247e-05\n",
      "Iteration: 4721 Loss: 5.091791063259343e-05\n",
      "Iteration: 4722 Loss: 5.08712315850328e-05\n",
      "Iteration: 4723 Loss: 5.082459533305006e-05\n",
      "Iteration: 4724 Loss: 5.0778001942653545e-05\n",
      "Iteration: 4725 Loss: 5.0731451463977e-05\n",
      "Iteration: 4726 Loss: 5.0684943928564955e-05\n",
      "Iteration: 4727 Loss: 5.063847934856124e-05\n",
      "Iteration: 4728 Loss: 5.059205771581243e-05\n",
      "Iteration: 4729 Loss: 5.0545679003443754e-05\n",
      "Iteration: 4730 Loss: 5.04993431683861e-05\n",
      "Iteration: 4731 Loss: 5.0453050156388414e-05\n",
      "Iteration: 4732 Loss: 5.040679990479259e-05\n",
      "Iteration: 4733 Loss: 5.036059234909009e-05\n",
      "Iteration: 4734 Loss: 5.031442742462523e-05\n",
      "Iteration: 4735 Loss: 5.026830506960554e-05\n",
      "Iteration: 4736 Loss: 5.022222522561346e-05\n",
      "Iteration: 4737 Loss: 5.0176187837111935e-05\n",
      "Iteration: 4738 Loss: 5.013019285146252e-05\n",
      "Iteration: 4739 Loss: 5.0084240217700885e-05\n",
      "Iteration: 4740 Loss: 5.003832988595666e-05\n",
      "Iteration: 4741 Loss: 4.999246180721925e-05\n",
      "Iteration: 4742 Loss: 4.99466359330223e-05\n",
      "Iteration: 4743 Loss: 4.990085221558149e-05\n",
      "Iteration: 4744 Loss: 4.9855110607269955e-05\n",
      "Iteration: 4745 Loss: 4.9809411061671806e-05\n",
      "Iteration: 4746 Loss: 4.9763753533003706e-05\n",
      "Iteration: 4747 Loss: 4.97181379764682e-05\n",
      "Iteration: 4748 Loss: 4.9672564348107066e-05\n",
      "Iteration: 4749 Loss: 4.9627032604949696e-05\n",
      "Iteration: 4750 Loss: 4.958154270500144e-05\n",
      "Iteration: 4751 Loss: 4.9536094606749424e-05\n",
      "Iteration: 4752 Loss: 4.949068826972238e-05\n",
      "Iteration: 4753 Loss: 4.944532365373304e-05\n",
      "Iteration: 4754 Loss: 4.9400000719187765e-05\n",
      "Iteration: 4755 Loss: 4.935471942669914e-05\n",
      "Iteration: 4756 Loss: 4.9309479736962434e-05\n",
      "Iteration: 4757 Loss: 4.926428161077637e-05\n",
      "Iteration: 4758 Loss: 4.921912500887057e-05\n",
      "Iteration: 4759 Loss: 4.917400989210652e-05\n",
      "Iteration: 4760 Loss: 4.912893622134502e-05\n",
      "Iteration: 4761 Loss: 4.908390395715324e-05\n",
      "Iteration: 4762 Loss: 4.9038913060264075e-05\n",
      "Iteration: 4763 Loss: 4.899396349092329e-05\n",
      "Iteration: 4764 Loss: 4.894905520952351e-05\n",
      "Iteration: 4765 Loss: 4.8904188176614907e-05\n",
      "Iteration: 4766 Loss: 4.885936235238041e-05\n",
      "Iteration: 4767 Loss: 4.881457769724931e-05\n",
      "Iteration: 4768 Loss: 4.876983417104304e-05\n",
      "Iteration: 4769 Loss: 4.872513173449702e-05\n",
      "Iteration: 4770 Loss: 4.8680470347629064e-05\n",
      "Iteration: 4771 Loss: 4.8635849971033665e-05\n",
      "Iteration: 4772 Loss: 4.85912705649429e-05\n",
      "Iteration: 4773 Loss: 4.854673208982796e-05\n",
      "Iteration: 4774 Loss: 4.8502234506305925e-05\n",
      "Iteration: 4775 Loss: 4.845777777496966e-05\n",
      "Iteration: 4776 Loss: 4.8413361856486804e-05\n",
      "Iteration: 4777 Loss: 4.836898671182963e-05\n",
      "Iteration: 4778 Loss: 4.832465230182834e-05\n",
      "Iteration: 4779 Loss: 4.828035858732923e-05\n",
      "Iteration: 4780 Loss: 4.823610552965879e-05\n",
      "Iteration: 4781 Loss: 4.819189308988143e-05\n",
      "Iteration: 4782 Loss: 4.8147721229292805e-05\n",
      "Iteration: 4783 Loss: 4.810358990940295e-05\n",
      "Iteration: 4784 Loss: 4.805949909156276e-05\n",
      "Iteration: 4785 Loss: 4.8015448737304734e-05\n",
      "Iteration: 4786 Loss: 4.797143880831981e-05\n",
      "Iteration: 4787 Loss: 4.7927469266429436e-05\n",
      "Iteration: 4788 Loss: 4.788354007324422e-05\n",
      "Iteration: 4789 Loss: 4.783965119067926e-05\n",
      "Iteration: 4790 Loss: 4.779580258061517e-05\n",
      "Iteration: 4791 Loss: 4.775199420507086e-05\n",
      "Iteration: 4792 Loss: 4.770822602618999e-05\n",
      "Iteration: 4793 Loss: 4.7664498005927576e-05\n",
      "Iteration: 4794 Loss: 4.7620810106508136e-05\n",
      "Iteration: 4795 Loss: 4.757716229015776e-05\n",
      "Iteration: 4796 Loss: 4.753355451909203e-05\n",
      "Iteration: 4797 Loss: 4.7489986755718434e-05\n",
      "Iteration: 4798 Loss: 4.744645896239739e-05\n",
      "Iteration: 4799 Loss: 4.7402971101612045e-05\n",
      "Iteration: 4800 Loss: 4.735952313590549e-05\n",
      "Iteration: 4801 Loss: 4.731611502770847e-05\n",
      "Iteration: 4802 Loss: 4.727274673975961e-05\n",
      "Iteration: 4803 Loss: 4.7229418234609805e-05\n",
      "Iteration: 4804 Loss: 4.718612947492401e-05\n",
      "Iteration: 4805 Loss: 4.7142880423666755e-05\n",
      "Iteration: 4806 Loss: 4.7099671043409354e-05\n",
      "Iteration: 4807 Loss: 4.7056501297272056e-05\n",
      "Iteration: 4808 Loss: 4.701337114796208e-05\n",
      "Iteration: 4809 Loss: 4.697028055837296e-05\n",
      "Iteration: 4810 Loss: 4.692722949178244e-05\n",
      "Iteration: 4811 Loss: 4.6884217911187996e-05\n",
      "Iteration: 4812 Loss: 4.6841245779751485e-05\n",
      "Iteration: 4813 Loss: 4.679831306052698e-05\n",
      "Iteration: 4814 Loss: 4.675541971682668e-05\n",
      "Iteration: 4815 Loss: 4.671256571192568e-05\n",
      "Iteration: 4816 Loss: 4.666975100920548e-05\n",
      "Iteration: 4817 Loss: 4.662697557187136e-05\n",
      "Iteration: 4818 Loss: 4.658423936349028e-05\n",
      "Iteration: 4819 Loss: 4.654154234749233e-05\n",
      "Iteration: 4820 Loss: 4.649888448747539e-05\n",
      "Iteration: 4821 Loss: 4.645626574689207e-05\n",
      "Iteration: 4822 Loss: 4.641368608939257e-05\n",
      "Iteration: 4823 Loss: 4.6371145478652576e-05\n",
      "Iteration: 4824 Loss: 4.632864387831784e-05\n",
      "Iteration: 4825 Loss: 4.6286181252260403e-05\n",
      "Iteration: 4826 Loss: 4.6243757564006756e-05\n",
      "Iteration: 4827 Loss: 4.620137277771948e-05\n",
      "Iteration: 4828 Loss: 4.615902685711831e-05\n",
      "Iteration: 4829 Loss: 4.611671976621769e-05\n",
      "Iteration: 4830 Loss: 4.607445146894799e-05\n",
      "Iteration: 4831 Loss: 4.603222192932893e-05\n",
      "Iteration: 4832 Loss: 4.599003111138346e-05\n",
      "Iteration: 4833 Loss: 4.594787897923869e-05\n",
      "Iteration: 4834 Loss: 4.5905765497123557e-05\n",
      "Iteration: 4835 Loss: 4.586369062914122e-05\n",
      "Iteration: 4836 Loss: 4.5821654339437404e-05\n",
      "Iteration: 4837 Loss: 4.577965659244735e-05\n",
      "Iteration: 4838 Loss: 4.57376973524315e-05\n",
      "Iteration: 4839 Loss: 4.569577658379697e-05\n",
      "Iteration: 4840 Loss: 4.5653894250907765e-05\n",
      "Iteration: 4841 Loss: 4.561205031806756e-05\n",
      "Iteration: 4842 Loss: 4.557024474990121e-05\n",
      "Iteration: 4843 Loss: 4.552847751089431e-05\n",
      "Iteration: 4844 Loss: 4.54867485655997e-05\n",
      "Iteration: 4845 Loss: 4.544505787869618e-05\n",
      "Iteration: 4846 Loss: 4.540340541471144e-05\n",
      "Iteration: 4847 Loss: 4.536179113828371e-05\n",
      "Iteration: 4848 Loss: 4.532021501424281e-05\n",
      "Iteration: 4849 Loss: 4.527867700731048e-05\n",
      "Iteration: 4850 Loss: 4.523717708228149e-05\n",
      "Iteration: 4851 Loss: 4.5195715204063565e-05\n",
      "Iteration: 4852 Loss: 4.515429133734754e-05\n",
      "Iteration: 4853 Loss: 4.511290544726583e-05\n",
      "Iteration: 4854 Loss: 4.507155749854565e-05\n",
      "Iteration: 4855 Loss: 4.503024745632159e-05\n",
      "Iteration: 4856 Loss: 4.498897528558434e-05\n",
      "Iteration: 4857 Loss: 4.4947740951398166e-05\n",
      "Iteration: 4858 Loss: 4.490654441886087e-05\n",
      "Iteration: 4859 Loss: 4.486538565311391e-05\n",
      "Iteration: 4860 Loss: 4.482426461932903e-05\n",
      "Iteration: 4861 Loss: 4.4783181282684523e-05\n",
      "Iteration: 4862 Loss: 4.474213560850146e-05\n",
      "Iteration: 4863 Loss: 4.4701127562030774e-05\n",
      "Iteration: 4864 Loss: 4.466015710855396e-05\n",
      "Iteration: 4865 Loss: 4.4619224213454214e-05\n",
      "Iteration: 4866 Loss: 4.4578328842209e-05\n",
      "Iteration: 4867 Loss: 4.453747096011494e-05\n",
      "Iteration: 4868 Loss: 4.449665053270548e-05\n",
      "Iteration: 4869 Loss: 4.445586752549771e-05\n",
      "Iteration: 4870 Loss: 4.4415121904085046e-05\n",
      "Iteration: 4871 Loss: 4.4374413633874926e-05\n",
      "Iteration: 4872 Loss: 4.433374268054163e-05\n",
      "Iteration: 4873 Loss: 4.429310900982721e-05\n",
      "Iteration: 4874 Loss: 4.42525125874037e-05\n",
      "Iteration: 4875 Loss: 4.4211953378873556e-05\n",
      "Iteration: 4876 Loss: 4.4171431350115526e-05\n",
      "Iteration: 4877 Loss: 4.4130946466695435e-05\n",
      "Iteration: 4878 Loss: 4.409049869469146e-05\n",
      "Iteration: 4879 Loss: 4.4050087999881924e-05\n",
      "Iteration: 4880 Loss: 4.400971434800757e-05\n",
      "Iteration: 4881 Loss: 4.396937770514598e-05\n",
      "Iteration: 4882 Loss: 4.3929078037278685e-05\n",
      "Iteration: 4883 Loss: 4.388881531022428e-05\n",
      "Iteration: 4884 Loss: 4.3848589490151094e-05\n",
      "Iteration: 4885 Loss: 4.3808400543081224e-05\n",
      "Iteration: 4886 Loss: 4.37682484351096e-05\n",
      "Iteration: 4887 Loss: 4.3728133132359905e-05\n",
      "Iteration: 4888 Loss: 4.368805460094002e-05\n",
      "Iteration: 4889 Loss: 4.3648012807158525e-05\n",
      "Iteration: 4890 Loss: 4.36080077171468e-05\n",
      "Iteration: 4891 Loss: 4.35680392971838e-05\n",
      "Iteration: 4892 Loss: 4.352810751355146e-05\n",
      "Iteration: 4893 Loss: 4.348821233271495e-05\n",
      "Iteration: 4894 Loss: 4.344835372091484e-05\n",
      "Iteration: 4895 Loss: 4.3408531644510725e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4896 Loss: 4.336874606998573e-05\n",
      "Iteration: 4897 Loss: 4.332899696379309e-05\n",
      "Iteration: 4898 Loss: 4.3289284292502075e-05\n",
      "Iteration: 4899 Loss: 4.3249608022457454e-05\n",
      "Iteration: 4900 Loss: 4.320996812035872e-05\n",
      "Iteration: 4901 Loss: 4.317036455283163e-05\n",
      "Iteration: 4902 Loss: 4.313079728631688e-05\n",
      "Iteration: 4903 Loss: 4.309126628761698e-05\n",
      "Iteration: 4904 Loss: 4.305177152338273e-05\n",
      "Iteration: 4905 Loss: 4.301231296033455e-05\n",
      "Iteration: 4906 Loss: 4.297289056518568e-05\n",
      "Iteration: 4907 Loss: 4.293350430483423e-05\n",
      "Iteration: 4908 Loss: 4.289415414598269e-05\n",
      "Iteration: 4909 Loss: 4.285484005552e-05\n",
      "Iteration: 4910 Loss: 4.2815562000323024e-05\n",
      "Iteration: 4911 Loss: 4.277631994730426e-05\n",
      "Iteration: 4912 Loss: 4.2737113863408856e-05\n",
      "Iteration: 4913 Loss: 4.269794371568969e-05\n",
      "Iteration: 4914 Loss: 4.265880947103562e-05\n",
      "Iteration: 4915 Loss: 4.2619711096454675e-05\n",
      "Iteration: 4916 Loss: 4.258064855920195e-05\n",
      "Iteration: 4917 Loss: 4.254162182622708e-05\n",
      "Iteration: 4918 Loss: 4.25026308646318e-05\n",
      "Iteration: 4919 Loss: 4.24636756417599e-05\n",
      "Iteration: 4920 Loss: 4.2424756124591784e-05\n",
      "Iteration: 4921 Loss: 4.238587228057115e-05\n",
      "Iteration: 4922 Loss: 4.2347024076737206e-05\n",
      "Iteration: 4923 Loss: 4.2308211480600497e-05\n",
      "Iteration: 4924 Loss: 4.226943445933594e-05\n",
      "Iteration: 4925 Loss: 4.2230692980255265e-05\n",
      "Iteration: 4926 Loss: 4.219198701081828e-05\n",
      "Iteration: 4927 Loss: 4.2153316518470296e-05\n",
      "Iteration: 4928 Loss: 4.2114681470622313e-05\n",
      "Iteration: 4929 Loss: 4.207608183471306e-05\n",
      "Iteration: 4930 Loss: 4.2037517578320776e-05\n",
      "Iteration: 4931 Loss: 4.199898866889357e-05\n",
      "Iteration: 4932 Loss: 4.196049507411657e-05\n",
      "Iteration: 4933 Loss: 4.192203676145477e-05\n",
      "Iteration: 4934 Loss: 4.1883613698671564e-05\n",
      "Iteration: 4935 Loss: 4.1845225853281976e-05\n",
      "Iteration: 4936 Loss: 4.180687319313409e-05\n",
      "Iteration: 4937 Loss: 4.1768555685772316e-05\n",
      "Iteration: 4938 Loss: 4.173027329913926e-05\n",
      "Iteration: 4939 Loss: 4.1692026000871425e-05\n",
      "Iteration: 4940 Loss: 4.1653813758793046e-05\n",
      "Iteration: 4941 Loss: 4.161563654077892e-05\n",
      "Iteration: 4942 Loss: 4.1577494314697016e-05\n",
      "Iteration: 4943 Loss: 4.1539387048521695e-05\n",
      "Iteration: 4944 Loss: 4.150131470999605e-05\n",
      "Iteration: 4945 Loss: 4.1463277267291394e-05\n",
      "Iteration: 4946 Loss: 4.1425274688133504e-05\n",
      "Iteration: 4947 Loss: 4.138730694078706e-05\n",
      "Iteration: 4948 Loss: 4.1349373993303896e-05\n",
      "Iteration: 4949 Loss: 4.131147581365492e-05\n",
      "Iteration: 4950 Loss: 4.1273612370058907e-05\n",
      "Iteration: 4951 Loss: 4.12357836304751e-05\n",
      "Iteration: 4952 Loss: 4.119798956327271e-05\n",
      "Iteration: 4953 Loss: 4.116023013665377e-05\n",
      "Iteration: 4954 Loss: 4.112250531842034e-05\n",
      "Iteration: 4955 Loss: 4.1084815077504034e-05\n",
      "Iteration: 4956 Loss: 4.1047159381595604e-05\n",
      "Iteration: 4957 Loss: 4.100953819934933e-05\n",
      "Iteration: 4958 Loss: 4.0971951499055054e-05\n",
      "Iteration: 4959 Loss: 4.093439924909453e-05\n",
      "Iteration: 4960 Loss: 4.0896881417925405e-05\n",
      "Iteration: 4961 Loss: 4.085939797380308e-05\n",
      "Iteration: 4962 Loss: 4.0821948885337127e-05\n",
      "Iteration: 4963 Loss: 4.0784534121043775e-05\n",
      "Iteration: 4964 Loss: 4.0747153649335376e-05\n",
      "Iteration: 4965 Loss: 4.070980743887677e-05\n",
      "Iteration: 4966 Loss: 4.067249545813655e-05\n",
      "Iteration: 4967 Loss: 4.063521767580226e-05\n",
      "Iteration: 4968 Loss: 4.059797406049662e-05\n",
      "Iteration: 4969 Loss: 4.056076458094497e-05\n",
      "Iteration: 4970 Loss: 4.052358920567953e-05\n",
      "Iteration: 4971 Loss: 4.048644790357695e-05\n",
      "Iteration: 4972 Loss: 4.0449340643357026e-05\n",
      "Iteration: 4973 Loss: 4.041226739380323e-05\n",
      "Iteration: 4974 Loss: 4.0375228123805144e-05\n",
      "Iteration: 4975 Loss: 4.033822280202509e-05\n",
      "Iteration: 4976 Loss: 4.030125139747953e-05\n",
      "Iteration: 4977 Loss: 4.0264313879032526e-05\n",
      "Iteration: 4978 Loss: 4.022741021561778e-05\n",
      "Iteration: 4979 Loss: 4.019054037626579e-05\n",
      "Iteration: 4980 Loss: 4.015370432977984e-05\n",
      "Iteration: 4981 Loss: 4.0116902045323955e-05\n",
      "Iteration: 4982 Loss: 4.008013349190636e-05\n",
      "Iteration: 4983 Loss: 4.0043398638599496e-05\n",
      "Iteration: 4984 Loss: 4.0006697454504906e-05\n",
      "Iteration: 4985 Loss: 3.997002990890247e-05\n",
      "Iteration: 4986 Loss: 3.9933395970653644e-05\n",
      "Iteration: 4987 Loss: 3.9896795609015585e-05\n",
      "Iteration: 4988 Loss: 3.986022879318189e-05\n",
      "Iteration: 4989 Loss: 3.9823695492719454e-05\n",
      "Iteration: 4990 Loss: 3.978719567660909e-05\n",
      "Iteration: 4991 Loss: 3.975072931422204e-05\n",
      "Iteration: 4992 Loss: 3.9714296374953936e-05\n",
      "Iteration: 4993 Loss: 3.9677896828021256e-05\n",
      "Iteration: 4994 Loss: 3.964153064290051e-05\n",
      "Iteration: 4995 Loss: 3.9605197788978214e-05\n",
      "Iteration: 4996 Loss: 3.956889823572669e-05\n",
      "Iteration: 4997 Loss: 3.953263195259758e-05\n",
      "Iteration: 4998 Loss: 3.949639890917347e-05\n",
      "Iteration: 4999 Loss: 3.946019907493592e-05\n",
      "Iteration: 5000 Loss: 3.9424032419151166e-05\n",
      "Iteration: 5001 Loss: 3.938789891185229e-05\n",
      "Iteration: 5002 Loss: 3.9351798522499944e-05\n",
      "Iteration: 5003 Loss: 3.93157312205948e-05\n",
      "Iteration: 5004 Loss: 3.9279696975901196e-05\n",
      "Iteration: 5005 Loss: 3.9243695758110735e-05\n",
      "Iteration: 5006 Loss: 3.920772753694604e-05\n",
      "Iteration: 5007 Loss: 3.9171792282160595e-05\n",
      "Iteration: 5008 Loss: 3.913588996349544e-05\n",
      "Iteration: 5009 Loss: 3.9100020550792386e-05\n",
      "Iteration: 5010 Loss: 3.906418401392623e-05\n",
      "Iteration: 5011 Loss: 3.902838032271985e-05\n",
      "Iteration: 5012 Loss: 3.8992609447064804e-05\n",
      "Iteration: 5013 Loss: 3.8956871356820646e-05\n",
      "Iteration: 5014 Loss: 3.892116602214277e-05\n",
      "Iteration: 5015 Loss: 3.888549341275774e-05\n",
      "Iteration: 5016 Loss: 3.884985349875965e-05\n",
      "Iteration: 5017 Loss: 3.881424625015902e-05\n",
      "Iteration: 5018 Loss: 3.877867163710804e-05\n",
      "Iteration: 5019 Loss: 3.874312962940789e-05\n",
      "Iteration: 5020 Loss: 3.870762019752189e-05\n",
      "Iteration: 5021 Loss: 3.867214331159118e-05\n",
      "Iteration: 5022 Loss: 3.863669894148368e-05\n",
      "Iteration: 5023 Loss: 3.860128705754144e-05\n",
      "Iteration: 5024 Loss: 3.856590762998798e-05\n",
      "Iteration: 5025 Loss: 3.853056062906931e-05\n",
      "Iteration: 5026 Loss: 3.849524602506052e-05\n",
      "Iteration: 5027 Loss: 3.8459963788267516e-05\n",
      "Iteration: 5028 Loss: 3.842471388901849e-05\n",
      "Iteration: 5029 Loss: 3.8389496297743483e-05\n",
      "Iteration: 5030 Loss: 3.8354310984588374e-05\n",
      "Iteration: 5031 Loss: 3.831915792019233e-05\n",
      "Iteration: 5032 Loss: 3.828403707500791e-05\n",
      "Iteration: 5033 Loss: 3.824894841939311e-05\n",
      "Iteration: 5034 Loss: 3.821389192387912e-05\n",
      "Iteration: 5035 Loss: 3.817886755898436e-05\n",
      "Iteration: 5036 Loss: 3.814387529525785e-05\n",
      "Iteration: 5037 Loss: 3.810891510342195e-05\n",
      "Iteration: 5038 Loss: 3.807398695363705e-05\n",
      "Iteration: 5039 Loss: 3.803909081699315e-05\n",
      "Iteration: 5040 Loss: 3.800422666392727e-05\n",
      "Iteration: 5041 Loss: 3.796939446526776e-05\n",
      "Iteration: 5042 Loss: 3.7934594191579674e-05\n",
      "Iteration: 5043 Loss: 3.789982581379085e-05\n",
      "Iteration: 5044 Loss: 3.7865089302304854e-05\n",
      "Iteration: 5045 Loss: 3.7830384628219264e-05\n",
      "Iteration: 5046 Loss: 3.7795711762063276e-05\n",
      "Iteration: 5047 Loss: 3.776107067523887e-05\n",
      "Iteration: 5048 Loss: 3.772646133811745e-05\n",
      "Iteration: 5049 Loss: 3.769188372168487e-05\n",
      "Iteration: 5050 Loss: 3.765733779705978e-05\n",
      "Iteration: 5051 Loss: 3.762282353494297e-05\n",
      "Iteration: 5052 Loss: 3.758834090646705e-05\n",
      "Iteration: 5053 Loss: 3.755388988267584e-05\n",
      "Iteration: 5054 Loss: 3.7519470434410364e-05\n",
      "Iteration: 5055 Loss: 3.748508253287663e-05\n",
      "Iteration: 5056 Loss: 3.745072614912693e-05\n",
      "Iteration: 5057 Loss: 3.741640125427044e-05\n",
      "Iteration: 5058 Loss: 3.738210781944154e-05\n",
      "Iteration: 5059 Loss: 3.734784581582766e-05\n",
      "Iteration: 5060 Loss: 3.73136152146183e-05\n",
      "Iteration: 5061 Loss: 3.727941598710759e-05\n",
      "Iteration: 5062 Loss: 3.7245248104268514e-05\n",
      "Iteration: 5063 Loss: 3.7211111537659934e-05\n",
      "Iteration: 5064 Loss: 3.7177006258351076e-05\n",
      "Iteration: 5065 Loss: 3.7142932237816915e-05\n",
      "Iteration: 5066 Loss: 3.7108889447362257e-05\n",
      "Iteration: 5067 Loss: 3.707487785837078e-05\n",
      "Iteration: 5068 Loss: 3.704089744223908e-05\n",
      "Iteration: 5069 Loss: 3.7006948170357236e-05\n",
      "Iteration: 5070 Loss: 3.697303001422037e-05\n",
      "Iteration: 5071 Loss: 3.6939142945286873e-05\n",
      "Iteration: 5072 Loss: 3.6905286935137644e-05\n",
      "Iteration: 5073 Loss: 3.68714619552005e-05\n",
      "Iteration: 5074 Loss: 3.68376679770978e-05\n",
      "Iteration: 5075 Loss: 3.680390497239319e-05\n",
      "Iteration: 5076 Loss: 3.677017291270334e-05\n",
      "Iteration: 5077 Loss: 3.673647176974497e-05\n",
      "Iteration: 5078 Loss: 3.670280151507115e-05\n",
      "Iteration: 5079 Loss: 3.6669162120335525e-05\n",
      "Iteration: 5080 Loss: 3.663555355728659e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5081 Loss: 3.660197579772515e-05\n",
      "Iteration: 5082 Loss: 3.6568428813389134e-05\n",
      "Iteration: 5083 Loss: 3.653491257605996e-05\n",
      "Iteration: 5084 Loss: 3.650142705758394e-05\n",
      "Iteration: 5085 Loss: 3.6467972229900274e-05\n",
      "Iteration: 5086 Loss: 3.643454806469795e-05\n",
      "Iteration: 5087 Loss: 3.640115453394789e-05\n",
      "Iteration: 5088 Loss: 3.636779160960687e-05\n",
      "Iteration: 5089 Loss: 3.633445926354754e-05\n",
      "Iteration: 5090 Loss: 3.630115746778121e-05\n",
      "Iteration: 5091 Loss: 3.6267886194302427e-05\n",
      "Iteration: 5092 Loss: 3.623464541514142e-05\n",
      "Iteration: 5093 Loss: 3.620143510234886e-05\n",
      "Iteration: 5094 Loss: 3.616825522793989e-05\n",
      "Iteration: 5095 Loss: 3.613510576417098e-05\n",
      "Iteration: 5096 Loss: 3.610198668303554e-05\n",
      "Iteration: 5097 Loss: 3.606889795672413e-05\n",
      "Iteration: 5098 Loss: 3.6035839557375555e-05\n",
      "Iteration: 5099 Loss: 3.600281145727174e-05\n",
      "Iteration: 5100 Loss: 3.596981362864129e-05\n",
      "Iteration: 5101 Loss: 3.5936846043662424e-05\n",
      "Iteration: 5102 Loss: 3.590390867465364e-05\n",
      "Iteration: 5103 Loss: 3.5871001493977653e-05\n",
      "Iteration: 5104 Loss: 3.583812447370436e-05\n",
      "Iteration: 5105 Loss: 3.580527758654111e-05\n",
      "Iteration: 5106 Loss: 3.57724608046853e-05\n",
      "Iteration: 5107 Loss: 3.5739674100620726e-05\n",
      "Iteration: 5108 Loss: 3.5706917446923336e-05\n",
      "Iteration: 5109 Loss: 3.567419081568327e-05\n",
      "Iteration: 5110 Loss: 3.564149417956432e-05\n",
      "Iteration: 5111 Loss: 3.5608827511068344e-05\n",
      "Iteration: 5112 Loss: 3.5576190782752145e-05\n",
      "Iteration: 5113 Loss: 3.554358396716026e-05\n",
      "Iteration: 5114 Loss: 3.551100703693024e-05\n",
      "Iteration: 5115 Loss: 3.547845996454983e-05\n",
      "Iteration: 5116 Loss: 3.544594272273535e-05\n",
      "Iteration: 5117 Loss: 3.541345528413226e-05\n",
      "Iteration: 5118 Loss: 3.5380997621275266e-05\n",
      "Iteration: 5119 Loss: 3.5348569707166656e-05\n",
      "Iteration: 5120 Loss: 3.531617151445199e-05\n",
      "Iteration: 5121 Loss: 3.528380301577897e-05\n",
      "Iteration: 5122 Loss: 3.525146418398457e-05\n",
      "Iteration: 5123 Loss: 3.5219154991824614e-05\n",
      "Iteration: 5124 Loss: 3.518687541224469e-05\n",
      "Iteration: 5125 Loss: 3.515462541804767e-05\n",
      "Iteration: 5126 Loss: 3.51224049822628e-05\n",
      "Iteration: 5127 Loss: 3.5090214077467014e-05\n",
      "Iteration: 5128 Loss: 3.505805267678183e-05\n",
      "Iteration: 5129 Loss: 3.502592075314591e-05\n",
      "Iteration: 5130 Loss: 3.4993818279650014e-05\n",
      "Iteration: 5131 Loss: 3.496174522932249e-05\n",
      "Iteration: 5132 Loss: 3.492970157501355e-05\n",
      "Iteration: 5133 Loss: 3.489768728973508e-05\n",
      "Iteration: 5134 Loss: 3.486570234680152e-05\n",
      "Iteration: 5135 Loss: 3.483374671922223e-05\n",
      "Iteration: 5136 Loss: 3.480182037997724e-05\n",
      "Iteration: 5137 Loss: 3.4769923302396065e-05\n",
      "Iteration: 5138 Loss: 3.473805545955954e-05\n",
      "Iteration: 5139 Loss: 3.4706216824730035e-05\n",
      "Iteration: 5140 Loss: 3.467440737127385e-05\n",
      "Iteration: 5141 Loss: 3.4642627072195843e-05\n",
      "Iteration: 5142 Loss: 3.461087590098499e-05\n",
      "Iteration: 5143 Loss: 3.457915383075072e-05\n",
      "Iteration: 5144 Loss: 3.454746083492257e-05\n",
      "Iteration: 5145 Loss: 3.4515796886886154e-05\n",
      "Iteration: 5146 Loss: 3.448416195992176e-05\n",
      "Iteration: 5147 Loss: 3.445255602754861e-05\n",
      "Iteration: 5148 Loss: 3.442097906313215e-05\n",
      "Iteration: 5149 Loss: 3.438943104016075e-05\n",
      "Iteration: 5150 Loss: 3.435791193203371e-05\n",
      "Iteration: 5151 Loss: 3.432642171228452e-05\n",
      "Iteration: 5152 Loss: 3.429496035449647e-05\n",
      "Iteration: 5153 Loss: 3.426352783209939e-05\n",
      "Iteration: 5154 Loss: 3.4232124118687e-05\n",
      "Iteration: 5155 Loss: 3.4200749187931126e-05\n",
      "Iteration: 5156 Loss: 3.416940301342216e-05\n",
      "Iteration: 5157 Loss: 3.413808556881641e-05\n",
      "Iteration: 5158 Loss: 3.410679682771795e-05\n",
      "Iteration: 5159 Loss: 3.4075536763859226e-05\n",
      "Iteration: 5160 Loss: 3.4044305350958644e-05\n",
      "Iteration: 5161 Loss: 3.401310256279058e-05\n",
      "Iteration: 5162 Loss: 3.398192837309794e-05\n",
      "Iteration: 5163 Loss: 3.395078275558633e-05\n",
      "Iteration: 5164 Loss: 3.3919665684065784e-05\n",
      "Iteration: 5165 Loss: 3.3888577132530166e-05\n",
      "Iteration: 5166 Loss: 3.385751707475077e-05\n",
      "Iteration: 5167 Loss: 3.38264854846976e-05\n",
      "Iteration: 5168 Loss: 3.3795482336112366e-05\n",
      "Iteration: 5169 Loss: 3.376450760297133e-05\n",
      "Iteration: 5170 Loss: 3.3733561259212924e-05\n",
      "Iteration: 5171 Loss: 3.370264327886018e-05\n",
      "Iteration: 5172 Loss: 3.367175363605858e-05\n",
      "Iteration: 5173 Loss: 3.364089230474174e-05\n",
      "Iteration: 5174 Loss: 3.361005925891345e-05\n",
      "Iteration: 5175 Loss: 3.3579254472623425e-05\n",
      "Iteration: 5176 Loss: 3.354847792006314e-05\n",
      "Iteration: 5177 Loss: 3.3517729575234385e-05\n",
      "Iteration: 5178 Loss: 3.348700941236544e-05\n",
      "Iteration: 5179 Loss: 3.345631740567451e-05\n",
      "Iteration: 5180 Loss: 3.342565352919601e-05\n",
      "Iteration: 5181 Loss: 3.3395017757340155e-05\n",
      "Iteration: 5182 Loss: 3.3364410064197314e-05\n",
      "Iteration: 5183 Loss: 3.333383042408655e-05\n",
      "Iteration: 5184 Loss: 3.330327881128367e-05\n",
      "Iteration: 5185 Loss: 3.327275520009073e-05\n",
      "Iteration: 5186 Loss: 3.3242259564905144e-05\n",
      "Iteration: 5187 Loss: 3.321179187999189e-05\n",
      "Iteration: 5188 Loss: 3.3181352119846105e-05\n",
      "Iteration: 5189 Loss: 3.315094025877882e-05\n",
      "Iteration: 5190 Loss: 3.3120556271336626e-05\n",
      "Iteration: 5191 Loss: 3.309020013192831e-05\n",
      "Iteration: 5192 Loss: 3.305987181488208e-05\n",
      "Iteration: 5193 Loss: 3.302957129484933e-05\n",
      "Iteration: 5194 Loss: 3.2999298546536085e-05\n",
      "Iteration: 5195 Loss: 3.296905354382378e-05\n",
      "Iteration: 5196 Loss: 3.293883626191103e-05\n",
      "Iteration: 5197 Loss: 3.2908646675280955e-05\n",
      "Iteration: 5198 Loss: 3.287848475839905e-05\n",
      "Iteration: 5199 Loss: 3.284835048607141e-05\n",
      "Iteration: 5200 Loss: 3.281824383285373e-05\n",
      "Iteration: 5201 Loss: 3.2788164773411724e-05\n",
      "Iteration: 5202 Loss: 3.275811328249096e-05\n",
      "Iteration: 5203 Loss: 3.272808933488143e-05\n",
      "Iteration: 5204 Loss: 3.269809290528126e-05\n",
      "Iteration: 5205 Loss: 3.2668123968408414e-05\n",
      "Iteration: 5206 Loss: 3.263818249911404e-05\n",
      "Iteration: 5207 Loss: 3.2608268472306964e-05\n",
      "Iteration: 5208 Loss: 3.2578381862651856e-05\n",
      "Iteration: 5209 Loss: 3.254852264518467e-05\n",
      "Iteration: 5210 Loss: 3.2518690794746576e-05\n",
      "Iteration: 5211 Loss: 3.2488886286255884e-05\n",
      "Iteration: 5212 Loss: 3.245910909470118e-05\n",
      "Iteration: 5213 Loss: 3.2429359194928295e-05\n",
      "Iteration: 5214 Loss: 3.239963656200565e-05\n",
      "Iteration: 5215 Loss: 3.2369941170926724e-05\n",
      "Iteration: 5216 Loss: 3.23402729967432e-05\n",
      "Iteration: 5217 Loss: 3.231063201443206e-05\n",
      "Iteration: 5218 Loss: 3.228101819916644e-05\n",
      "Iteration: 5219 Loss: 3.225143152604862e-05\n",
      "Iteration: 5220 Loss: 3.222187197016128e-05\n",
      "Iteration: 5221 Loss: 3.2192339506690176e-05\n",
      "Iteration: 5222 Loss: 3.2162834110653216e-05\n",
      "Iteration: 5223 Loss: 3.213335575738919e-05\n",
      "Iteration: 5224 Loss: 3.210390442215636e-05\n",
      "Iteration: 5225 Loss: 3.207448008000009e-05\n",
      "Iteration: 5226 Loss: 3.2045082706330204e-05\n",
      "Iteration: 5227 Loss: 3.201571227639239e-05\n",
      "Iteration: 5228 Loss: 3.1986368765489805e-05\n",
      "Iteration: 5229 Loss: 3.195705214895644e-05\n",
      "Iteration: 5230 Loss: 3.192776240204032e-05\n",
      "Iteration: 5231 Loss: 3.1898499500122964e-05\n",
      "Iteration: 5232 Loss: 3.18692634188218e-05\n",
      "Iteration: 5233 Loss: 3.184005413343648e-05\n",
      "Iteration: 5234 Loss: 3.181087161937785e-05\n",
      "Iteration: 5235 Loss: 3.178171585214124e-05\n",
      "Iteration: 5236 Loss: 3.175258680718971e-05\n",
      "Iteration: 5237 Loss: 3.172348446011525e-05\n",
      "Iteration: 5238 Loss: 3.169440878648524e-05\n",
      "Iteration: 5239 Loss: 3.166535976158562e-05\n",
      "Iteration: 5240 Loss: 3.16363373611463e-05\n",
      "Iteration: 5241 Loss: 3.160734156070702e-05\n",
      "Iteration: 5242 Loss: 3.1578372335924525e-05\n",
      "Iteration: 5243 Loss: 3.1549429662555256e-05\n",
      "Iteration: 5244 Loss: 3.152051351610986e-05\n",
      "Iteration: 5245 Loss: 3.149162387233619e-05\n",
      "Iteration: 5246 Loss: 3.146276070688167e-05\n",
      "Iteration: 5247 Loss: 3.1433923995598474e-05\n",
      "Iteration: 5248 Loss: 3.1405113714235106e-05\n",
      "Iteration: 5249 Loss: 3.137632983845668e-05\n",
      "Iteration: 5250 Loss: 3.134757234406234e-05\n",
      "Iteration: 5251 Loss: 3.131884120698167e-05\n",
      "Iteration: 5252 Loss: 3.129013640300331e-05\n",
      "Iteration: 5253 Loss: 3.1261457907995204e-05\n",
      "Iteration: 5254 Loss: 3.12328056978946e-05\n",
      "Iteration: 5255 Loss: 3.120417974850133e-05\n",
      "Iteration: 5256 Loss: 3.117558003580213e-05\n",
      "Iteration: 5257 Loss: 3.11470065358256e-05\n",
      "Iteration: 5258 Loss: 3.1118459224506806e-05\n",
      "Iteration: 5259 Loss: 3.1089938077549206e-05\n",
      "Iteration: 5260 Loss: 3.1061443071322103e-05\n",
      "Iteration: 5261 Loss: 3.1032974181830504e-05\n",
      "Iteration: 5262 Loss: 3.100453138508247e-05\n",
      "Iteration: 5263 Loss: 3.097611465716114e-05\n",
      "Iteration: 5264 Loss: 3.094772397417527e-05\n",
      "Iteration: 5265 Loss: 3.091935931225408e-05\n",
      "Iteration: 5266 Loss: 3.089102064755029e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5267 Loss: 3.086270795614721e-05\n",
      "Iteration: 5268 Loss: 3.0834421214493863e-05\n",
      "Iteration: 5269 Loss: 3.080616039855581e-05\n",
      "Iteration: 5270 Loss: 3.07779254846527e-05\n",
      "Iteration: 5271 Loss: 3.074971644912178e-05\n",
      "Iteration: 5272 Loss: 3.072153326813214e-05\n",
      "Iteration: 5273 Loss: 3.069337591808022e-05\n",
      "Iteration: 5274 Loss: 3.066524437511936e-05\n",
      "Iteration: 5275 Loss: 3.063713861577005e-05\n",
      "Iteration: 5276 Loss: 3.0609058616398307e-05\n",
      "Iteration: 5277 Loss: 3.058100435320647e-05\n",
      "Iteration: 5278 Loss: 3.055297580277414e-05\n",
      "Iteration: 5279 Loss: 3.052497294150008e-05\n",
      "Iteration: 5280 Loss: 3.049699574583777e-05\n",
      "Iteration: 5281 Loss: 3.0469044192260856e-05\n",
      "Iteration: 5282 Loss: 3.044111825721729e-05\n",
      "Iteration: 5283 Loss: 3.0413217917186732e-05\n",
      "Iteration: 5284 Loss: 3.038534314909808e-05\n",
      "Iteration: 5285 Loss: 3.0357493929129352e-05\n",
      "Iteration: 5286 Loss: 3.0329670233954315e-05\n",
      "Iteration: 5287 Loss: 3.0301872040064766e-05\n",
      "Iteration: 5288 Loss: 3.0274099324259777e-05\n",
      "Iteration: 5289 Loss: 3.024635206313143e-05\n",
      "Iteration: 5290 Loss: 3.021863023334595e-05\n",
      "Iteration: 5291 Loss: 3.0190933811596947e-05\n",
      "Iteration: 5292 Loss: 3.0163262774598182e-05\n",
      "Iteration: 5293 Loss: 3.0135617099024565e-05\n",
      "Iteration: 5294 Loss: 3.0107996761745474e-05\n",
      "Iteration: 5295 Loss: 3.0080401739539923e-05\n",
      "Iteration: 5296 Loss: 3.0052832009089408e-05\n",
      "Iteration: 5297 Loss: 3.0025287547214295e-05\n",
      "Iteration: 5298 Loss: 2.9997768330868632e-05\n",
      "Iteration: 5299 Loss: 2.9970274336855166e-05\n",
      "Iteration: 5300 Loss: 2.994280554211689e-05\n",
      "Iteration: 5301 Loss: 2.9915361923443113e-05\n",
      "Iteration: 5302 Loss: 2.988794345781469e-05\n",
      "Iteration: 5303 Loss: 2.9860550122178285e-05\n",
      "Iteration: 5304 Loss: 2.9833181893444138e-05\n",
      "Iteration: 5305 Loss: 2.9805838748716006e-05\n",
      "Iteration: 5306 Loss: 2.9778520665094107e-05\n",
      "Iteration: 5307 Loss: 2.9751227619369303e-05\n",
      "Iteration: 5308 Loss: 2.972395958848246e-05\n",
      "Iteration: 5309 Loss: 2.9696716549911262e-05\n",
      "Iteration: 5310 Loss: 2.96694984804904e-05\n",
      "Iteration: 5311 Loss: 2.964230535733513e-05\n",
      "Iteration: 5312 Loss: 2.9615137157638725e-05\n",
      "Iteration: 5313 Loss: 2.9587993858615384e-05\n",
      "Iteration: 5314 Loss: 2.9560875437382456e-05\n",
      "Iteration: 5315 Loss: 2.9533781871055188e-05\n",
      "Iteration: 5316 Loss: 2.950671313702545e-05\n",
      "Iteration: 5317 Loss: 2.9479669212446248e-05\n",
      "Iteration: 5318 Loss: 2.9452650074515786e-05\n",
      "Iteration: 5319 Loss: 2.942565570064244e-05\n",
      "Iteration: 5320 Loss: 2.9398686068066892e-05\n",
      "Iteration: 5321 Loss: 2.937174115431586e-05\n",
      "Iteration: 5322 Loss: 2.934482093632652e-05\n",
      "Iteration: 5323 Loss: 2.9317925391668205e-05\n",
      "Iteration: 5324 Loss: 2.9291054497781547e-05\n",
      "Iteration: 5325 Loss: 2.9264208231907184e-05\n",
      "Iteration: 5326 Loss: 2.9237386571583675e-05\n",
      "Iteration: 5327 Loss: 2.9210589494316195e-05\n",
      "Iteration: 5328 Loss: 2.9183816977570777e-05\n",
      "Iteration: 5329 Loss: 2.915706899872913e-05\n",
      "Iteration: 5330 Loss: 2.913034553549934e-05\n",
      "Iteration: 5331 Loss: 2.9103646564975495e-05\n",
      "Iteration: 5332 Loss: 2.907697206509122e-05\n",
      "Iteration: 5333 Loss: 2.9050322013385152e-05\n",
      "Iteration: 5334 Loss: 2.9023696387449066e-05\n",
      "Iteration: 5335 Loss: 2.89970951647832e-05\n",
      "Iteration: 5336 Loss: 2.897051832307826e-05\n",
      "Iteration: 5337 Loss: 2.8943965839986724e-05\n",
      "Iteration: 5338 Loss: 2.891743769318427e-05\n",
      "Iteration: 5339 Loss: 2.8890933860365817e-05\n",
      "Iteration: 5340 Loss: 2.886445431918843e-05\n",
      "Iteration: 5341 Loss: 2.883799904756037e-05\n",
      "Iteration: 5342 Loss: 2.8811568023010317e-05\n",
      "Iteration: 5343 Loss: 2.878516122348347e-05\n",
      "Iteration: 5344 Loss: 2.8758778626722882e-05\n",
      "Iteration: 5345 Loss: 2.8732420210541055e-05\n",
      "Iteration: 5346 Loss: 2.8706085952724643e-05\n",
      "Iteration: 5347 Loss: 2.8679775831242018e-05\n",
      "Iteration: 5348 Loss: 2.8653489823766757e-05\n",
      "Iteration: 5349 Loss: 2.8627227908642563e-05\n",
      "Iteration: 5350 Loss: 2.8600990063286682e-05\n",
      "Iteration: 5351 Loss: 2.8574776265898635e-05\n",
      "Iteration: 5352 Loss: 2.8548586494383056e-05\n",
      "Iteration: 5353 Loss: 2.8522420726717573e-05\n",
      "Iteration: 5354 Loss: 2.849627894090288e-05\n",
      "Iteration: 5355 Loss: 2.8470161114955375e-05\n",
      "Iteration: 5356 Loss: 2.844406722677492e-05\n",
      "Iteration: 5357 Loss: 2.841799725470969e-05\n",
      "Iteration: 5358 Loss: 2.8391951176695197e-05\n",
      "Iteration: 5359 Loss: 2.836592897083214e-05\n",
      "Iteration: 5360 Loss: 2.8339930615238957e-05\n",
      "Iteration: 5361 Loss: 2.8313956088056105e-05\n",
      "Iteration: 5362 Loss: 2.8288005367504022e-05\n",
      "Iteration: 5363 Loss: 2.8262078431794938e-05\n",
      "Iteration: 5364 Loss: 2.8236175258888533e-05\n",
      "Iteration: 5365 Loss: 2.82102958271555e-05\n",
      "Iteration: 5366 Loss: 2.8184440114688427e-05\n",
      "Iteration: 5367 Loss: 2.815860810019177e-05\n",
      "Iteration: 5368 Loss: 2.8132799761499035e-05\n",
      "Iteration: 5369 Loss: 2.8107015077063286e-05\n",
      "Iteration: 5370 Loss: 2.8081254025199385e-05\n",
      "Iteration: 5371 Loss: 2.8055516584307064e-05\n",
      "Iteration: 5372 Loss: 2.8029802732632758e-05\n",
      "Iteration: 5373 Loss: 2.8004112448610277e-05\n",
      "Iteration: 5374 Loss: 2.7978445710641357e-05\n",
      "Iteration: 5375 Loss: 2.7952802497086743e-05\n",
      "Iteration: 5376 Loss: 2.792718278650204e-05\n",
      "Iteration: 5377 Loss: 2.790158655734223e-05\n",
      "Iteration: 5378 Loss: 2.78760137879739e-05\n",
      "Iteration: 5379 Loss: 2.7850464456952217e-05\n",
      "Iteration: 5380 Loss: 2.782493854279379e-05\n",
      "Iteration: 5381 Loss: 2.779943602383602e-05\n",
      "Iteration: 5382 Loss: 2.7773956879097754e-05\n",
      "Iteration: 5383 Loss: 2.774850108683804e-05\n",
      "Iteration: 5384 Loss: 2.7723068625857965e-05\n",
      "Iteration: 5385 Loss: 2.7697659474424037e-05\n",
      "Iteration: 5386 Loss: 2.7672273611431304e-05\n",
      "Iteration: 5387 Loss: 2.7646911015419596e-05\n",
      "Iteration: 5388 Loss: 2.7621571665184193e-05\n",
      "Iteration: 5389 Loss: 2.7596255539356095e-05\n",
      "Iteration: 5390 Loss: 2.7570962616651248e-05\n",
      "Iteration: 5391 Loss: 2.7545692875803937e-05\n",
      "Iteration: 5392 Loss: 2.752044629556598e-05\n",
      "Iteration: 5393 Loss: 2.749522285471194e-05\n",
      "Iteration: 5394 Loss: 2.7470022532031453e-05\n",
      "Iteration: 5395 Loss: 2.7444845306336843e-05\n",
      "Iteration: 5396 Loss: 2.7419691156461026e-05\n",
      "Iteration: 5397 Loss: 2.7394560061249773e-05\n",
      "Iteration: 5398 Loss: 2.736945199957456e-05\n",
      "Iteration: 5399 Loss: 2.7344366950326073e-05\n",
      "Iteration: 5400 Loss: 2.731930489241232e-05\n",
      "Iteration: 5401 Loss: 2.7294265804757978e-05\n",
      "Iteration: 5402 Loss: 2.7269249666313888e-05\n",
      "Iteration: 5403 Loss: 2.724425645604229e-05\n",
      "Iteration: 5404 Loss: 2.7219286152931674e-05\n",
      "Iteration: 5405 Loss: 2.7194338735986534e-05\n",
      "Iteration: 5406 Loss: 2.7169414184228923e-05\n",
      "Iteration: 5407 Loss: 2.71445124767621e-05\n",
      "Iteration: 5408 Loss: 2.7119633592588425e-05\n",
      "Iteration: 5409 Loss: 2.7094777510734708e-05\n",
      "Iteration: 5410 Loss: 2.7069944210359243e-05\n",
      "Iteration: 5411 Loss: 2.7045133670578644e-05\n",
      "Iteration: 5412 Loss: 2.702034587053301e-05\n",
      "Iteration: 5413 Loss: 2.699558078938508e-05\n",
      "Iteration: 5414 Loss: 2.6970838406307874e-05\n",
      "Iteration: 5415 Loss: 2.6946118700348334e-05\n",
      "Iteration: 5416 Loss: 2.6921421651023533e-05\n",
      "Iteration: 5417 Loss: 2.6896747237564893e-05\n",
      "Iteration: 5418 Loss: 2.6872095438926777e-05\n",
      "Iteration: 5419 Loss: 2.684746623447833e-05\n",
      "Iteration: 5420 Loss: 2.682285960362385e-05\n",
      "Iteration: 5421 Loss: 2.6798275525612867e-05\n",
      "Iteration: 5422 Loss: 2.6773713979778466e-05\n",
      "Iteration: 5423 Loss: 2.674917494547073e-05\n",
      "Iteration: 5424 Loss: 2.6724658401995007e-05\n",
      "Iteration: 5425 Loss: 2.670016432885567e-05\n",
      "Iteration: 5426 Loss: 2.6675692705398072e-05\n",
      "Iteration: 5427 Loss: 2.6651243511049828e-05\n",
      "Iteration: 5428 Loss: 2.6626816725251203e-05\n",
      "Iteration: 5429 Loss: 2.6602412327464473e-05\n",
      "Iteration: 5430 Loss: 2.6578030297170214e-05\n",
      "Iteration: 5431 Loss: 2.6553670613868264e-05\n",
      "Iteration: 5432 Loss: 2.6529333257077458e-05\n",
      "Iteration: 5433 Loss: 2.6505018206332137e-05\n",
      "Iteration: 5434 Loss: 2.648072544119003e-05\n",
      "Iteration: 5435 Loss: 2.6456454941225397e-05\n",
      "Iteration: 5436 Loss: 2.6432206686031016e-05\n",
      "Iteration: 5437 Loss: 2.640798065522095e-05\n",
      "Iteration: 5438 Loss: 2.6383776828422414e-05\n",
      "Iteration: 5439 Loss: 2.6359595185139424e-05\n",
      "Iteration: 5440 Loss: 2.6335435705390127e-05\n",
      "Iteration: 5441 Loss: 2.6311298368600432e-05\n",
      "Iteration: 5442 Loss: 2.628718315447592e-05\n",
      "Iteration: 5443 Loss: 2.626309004285467e-05\n",
      "Iteration: 5444 Loss: 2.6239019013568393e-05\n",
      "Iteration: 5445 Loss: 2.6214970046082763e-05\n",
      "Iteration: 5446 Loss: 2.6190943120326134e-05\n",
      "Iteration: 5447 Loss: 2.6166938216094818e-05\n",
      "Iteration: 5448 Loss: 2.6142955313263074e-05\n",
      "Iteration: 5449 Loss: 2.611899439157978e-05\n",
      "Iteration: 5450 Loss: 2.609505543090022e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5451 Loss: 2.6071138411124333e-05\n",
      "Iteration: 5452 Loss: 2.6047243311993566e-05\n",
      "Iteration: 5453 Loss: 2.602337011371298e-05\n",
      "Iteration: 5454 Loss: 2.5999518796062716e-05\n",
      "Iteration: 5455 Loss: 2.597568933898708e-05\n",
      "Iteration: 5456 Loss: 2.5951881722450273e-05\n",
      "Iteration: 5457 Loss: 2.5928095926434235e-05\n",
      "Iteration: 5458 Loss: 2.5904331930941654e-05\n",
      "Iteration: 5459 Loss: 2.5880589715989802e-05\n",
      "Iteration: 5460 Loss: 2.5856869261617007e-05\n",
      "Iteration: 5461 Loss: 2.5833170547793656e-05\n",
      "Iteration: 5462 Loss: 2.5809493554763553e-05\n",
      "Iteration: 5463 Loss: 2.578583826253269e-05\n",
      "Iteration: 5464 Loss: 2.5762204651214423e-05\n",
      "Iteration: 5465 Loss: 2.5738592700935765e-05\n",
      "Iteration: 5466 Loss: 2.5715002391842922e-05\n",
      "Iteration: 5467 Loss: 2.5691433704101365e-05\n",
      "Iteration: 5468 Loss: 2.5667886617895675e-05\n",
      "Iteration: 5469 Loss: 2.564436111342521e-05\n",
      "Iteration: 5470 Loss: 2.5620857170910357e-05\n",
      "Iteration: 5471 Loss: 2.5597374770590255e-05\n",
      "Iteration: 5472 Loss: 2.557391389271921e-05\n",
      "Iteration: 5473 Loss: 2.5550474517570925e-05\n",
      "Iteration: 5474 Loss: 2.5527056625437847e-05\n",
      "Iteration: 5475 Loss: 2.5503660196834792e-05\n",
      "Iteration: 5476 Loss: 2.5480285211680648e-05\n",
      "Iteration: 5477 Loss: 2.5456931650470063e-05\n",
      "Iteration: 5478 Loss: 2.5433599493735993e-05\n",
      "Iteration: 5479 Loss: 2.5410288721691312e-05\n",
      "Iteration: 5480 Loss: 2.538699931479434e-05\n",
      "Iteration: 5481 Loss: 2.5363731253459767e-05\n",
      "Iteration: 5482 Loss: 2.534048451812536e-05\n",
      "Iteration: 5483 Loss: 2.5317259089244435e-05\n",
      "Iteration: 5484 Loss: 2.5294054947289654e-05\n",
      "Iteration: 5485 Loss: 2.5270872072750446e-05\n",
      "Iteration: 5486 Loss: 2.5247710446136725e-05\n",
      "Iteration: 5487 Loss: 2.5224570047969577e-05\n",
      "Iteration: 5488 Loss: 2.520145085873948e-05\n",
      "Iteration: 5489 Loss: 2.5178352859119014e-05\n",
      "Iteration: 5490 Loss: 2.515527602963062e-05\n",
      "Iteration: 5491 Loss: 2.51322203509297e-05\n",
      "Iteration: 5492 Loss: 2.510918580351554e-05\n",
      "Iteration: 5493 Loss: 2.5086172368078012e-05\n",
      "Iteration: 5494 Loss: 2.5063180025211454e-05\n",
      "Iteration: 5495 Loss: 2.504020875569398e-05\n",
      "Iteration: 5496 Loss: 2.5017258540154862e-05\n",
      "Iteration: 5497 Loss: 2.4994329359301495e-05\n",
      "Iteration: 5498 Loss: 2.497142119370365e-05\n",
      "Iteration: 5499 Loss: 2.494853402439687e-05\n",
      "Iteration: 5500 Loss: 2.4925667831991106e-05\n",
      "Iteration: 5501 Loss: 2.4902822597254554e-05\n",
      "Iteration: 5502 Loss: 2.4879998301134487e-05\n",
      "Iteration: 5503 Loss: 2.485719492413728e-05\n",
      "Iteration: 5504 Loss: 2.4834412447240474e-05\n",
      "Iteration: 5505 Loss: 2.4811650851291004e-05\n",
      "Iteration: 5506 Loss: 2.4788910117146374e-05\n",
      "Iteration: 5507 Loss: 2.4766190225687852e-05\n",
      "Iteration: 5508 Loss: 2.474349115781245e-05\n",
      "Iteration: 5509 Loss: 2.472081289443604e-05\n",
      "Iteration: 5510 Loss: 2.4698155416488384e-05\n",
      "Iteration: 5511 Loss: 2.4675518704917658e-05\n",
      "Iteration: 5512 Loss: 2.4652902740696144e-05\n",
      "Iteration: 5513 Loss: 2.46303075048015e-05\n",
      "Iteration: 5514 Loss: 2.460773297824082e-05\n",
      "Iteration: 5515 Loss: 2.4585179142030307e-05\n",
      "Iteration: 5516 Loss: 2.456264597720792e-05\n",
      "Iteration: 5517 Loss: 2.4540133464825812e-05\n",
      "Iteration: 5518 Loss: 2.4517641585957226e-05\n",
      "Iteration: 5519 Loss: 2.4495170321690994e-05\n",
      "Iteration: 5520 Loss: 2.44727196531321e-05\n",
      "Iteration: 5521 Loss: 2.445028956140242e-05\n",
      "Iteration: 5522 Loss: 2.4427880027646063e-05\n",
      "Iteration: 5523 Loss: 2.4405491033018585e-05\n",
      "Iteration: 5524 Loss: 2.438312255869471e-05\n",
      "Iteration: 5525 Loss: 2.4360774585925447e-05\n",
      "Iteration: 5526 Loss: 2.4338447095862316e-05\n",
      "Iteration: 5527 Loss: 2.4316140069674286e-05\n",
      "Iteration: 5528 Loss: 2.4293853488665792e-05\n",
      "Iteration: 5529 Loss: 2.427158733409471e-05\n",
      "Iteration: 5530 Loss: 2.424934158724163e-05\n",
      "Iteration: 5531 Loss: 2.422711622934481e-05\n",
      "Iteration: 5532 Loss: 2.4204911241886293e-05\n",
      "Iteration: 5533 Loss: 2.4182726606027475e-05\n",
      "Iteration: 5534 Loss: 2.416056230316933e-05\n",
      "Iteration: 5535 Loss: 2.4138418314622543e-05\n",
      "Iteration: 5536 Loss: 2.4116294621823575e-05\n",
      "Iteration: 5537 Loss: 2.4094191206225783e-05\n",
      "Iteration: 5538 Loss: 2.4072108049190464e-05\n",
      "Iteration: 5539 Loss: 2.4050045132147418e-05\n",
      "Iteration: 5540 Loss: 2.4028002436547923e-05\n",
      "Iteration: 5541 Loss: 2.400597994385668e-05\n",
      "Iteration: 5542 Loss: 2.3983977635559276e-05\n",
      "Iteration: 5543 Loss: 2.3961995493154496e-05\n",
      "Iteration: 5544 Loss: 2.394003349815962e-05\n",
      "Iteration: 5545 Loss: 2.39180916321081e-05\n",
      "Iteration: 5546 Loss: 2.3896169876552013e-05\n",
      "Iteration: 5547 Loss: 2.387426821305993e-05\n",
      "Iteration: 5548 Loss: 2.3852386623217377e-05\n",
      "Iteration: 5549 Loss: 2.3830525088622323e-05\n",
      "Iteration: 5550 Loss: 2.380868359089819e-05\n",
      "Iteration: 5551 Loss: 2.378686211167488e-05\n",
      "Iteration: 5552 Loss: 2.376506063266837e-05\n",
      "Iteration: 5553 Loss: 2.374327913537186e-05\n",
      "Iteration: 5554 Loss: 2.3721517601645086e-05\n",
      "Iteration: 5555 Loss: 2.3699776013131674e-05\n",
      "Iteration: 5556 Loss: 2.3678054351552724e-05\n",
      "Iteration: 5557 Loss: 2.3656352598642248e-05\n",
      "Iteration: 5558 Loss: 2.3634670736157944e-05\n",
      "Iteration: 5559 Loss: 2.3613008745864333e-05\n",
      "Iteration: 5560 Loss: 2.359136660940377e-05\n",
      "Iteration: 5561 Loss: 2.3569744308872556e-05\n",
      "Iteration: 5562 Loss: 2.3548141825943614e-05\n",
      "Iteration: 5563 Loss: 2.3526559142510865e-05\n",
      "Iteration: 5564 Loss: 2.3504996240312478e-05\n",
      "Iteration: 5565 Loss: 2.348345310127488e-05\n",
      "Iteration: 5566 Loss: 2.3461929707228302e-05\n",
      "Iteration: 5567 Loss: 2.3440426040189658e-05\n",
      "Iteration: 5568 Loss: 2.3418942082169723e-05\n",
      "Iteration: 5569 Loss: 2.339747781465961e-05\n",
      "Iteration: 5570 Loss: 2.3376033220206884e-05\n",
      "Iteration: 5571 Loss: 2.3354608280334712e-05\n",
      "Iteration: 5572 Loss: 2.3333202977174553e-05\n",
      "Iteration: 5573 Loss: 2.331181729273185e-05\n",
      "Iteration: 5574 Loss: 2.3290451209023492e-05\n",
      "Iteration: 5575 Loss: 2.326910470808756e-05\n",
      "Iteration: 5576 Loss: 2.3247777771969702e-05\n",
      "Iteration: 5577 Loss: 2.322647038274502e-05\n",
      "Iteration: 5578 Loss: 2.320518252249473e-05\n",
      "Iteration: 5579 Loss: 2.3183914173322116e-05\n",
      "Iteration: 5580 Loss: 2.3162665317340543e-05\n",
      "Iteration: 5581 Loss: 2.3141435936687452e-05\n",
      "Iteration: 5582 Loss: 2.3120226013511877e-05\n",
      "Iteration: 5583 Loss: 2.3099035529979353e-05\n",
      "Iteration: 5584 Loss: 2.307786446827325e-05\n",
      "Iteration: 5585 Loss: 2.30567128105935e-05\n",
      "Iteration: 5586 Loss: 2.3035580539007866e-05\n",
      "Iteration: 5587 Loss: 2.3014467636190784e-05\n",
      "Iteration: 5588 Loss: 2.2993374083797662e-05\n",
      "Iteration: 5589 Loss: 2.2972299864540818e-05\n",
      "Iteration: 5590 Loss: 2.295124496069675e-05\n",
      "Iteration: 5591 Loss: 2.293020935426799e-05\n",
      "Iteration: 5592 Loss: 2.29091930278631e-05\n",
      "Iteration: 5593 Loss: 2.288819596336957e-05\n",
      "Iteration: 5594 Loss: 2.2867218143574683e-05\n",
      "Iteration: 5595 Loss: 2.284625955069341e-05\n",
      "Iteration: 5596 Loss: 2.2825320167099947e-05\n",
      "Iteration: 5597 Loss: 2.280439997519367e-05\n",
      "Iteration: 5598 Loss: 2.2783498957383774e-05\n",
      "Iteration: 5599 Loss: 2.2762617096091993e-05\n",
      "Iteration: 5600 Loss: 2.2741754373763988e-05\n",
      "Iteration: 5601 Loss: 2.2720910772709257e-05\n",
      "Iteration: 5602 Loss: 2.2700086275850416e-05\n",
      "Iteration: 5603 Loss: 2.267928086522671e-05\n",
      "Iteration: 5604 Loss: 2.2658494523500002e-05\n",
      "Iteration: 5605 Loss: 2.2637727233188535e-05\n",
      "Iteration: 5606 Loss: 2.2616978976832368e-05\n",
      "Iteration: 5607 Loss: 2.259624973698299e-05\n",
      "Iteration: 5608 Loss: 2.2575539496217675e-05\n",
      "Iteration: 5609 Loss: 2.2554848237062934e-05\n",
      "Iteration: 5610 Loss: 2.2534175942292573e-05\n",
      "Iteration: 5611 Loss: 2.2513522594351853e-05\n",
      "Iteration: 5612 Loss: 2.249288817593369e-05\n",
      "Iteration: 5613 Loss: 2.247227266968886e-05\n",
      "Iteration: 5614 Loss: 2.2451676058284264e-05\n",
      "Iteration: 5615 Loss: 2.2431098324402245e-05\n",
      "Iteration: 5616 Loss: 2.2410539450738403e-05\n",
      "Iteration: 5617 Loss: 2.238999942001052e-05\n",
      "Iteration: 5618 Loss: 2.2369478214945394e-05\n",
      "Iteration: 5619 Loss: 2.2348975818290088e-05\n",
      "Iteration: 5620 Loss: 2.232849221280379e-05\n",
      "Iteration: 5621 Loss: 2.230802738126732e-05\n",
      "Iteration: 5622 Loss: 2.2287581306471616e-05\n",
      "Iteration: 5623 Loss: 2.226715397122527e-05\n",
      "Iteration: 5624 Loss: 2.2246745358353055e-05\n",
      "Iteration: 5625 Loss: 2.2226355450696224e-05\n",
      "Iteration: 5626 Loss: 2.2205984231110578e-05\n",
      "Iteration: 5627 Loss: 2.2185631682466112e-05\n",
      "Iteration: 5628 Loss: 2.2165297787652225e-05\n",
      "Iteration: 5629 Loss: 2.2144982529569132e-05\n",
      "Iteration: 5630 Loss: 2.212468589114226e-05\n",
      "Iteration: 5631 Loss: 2.2104407855297685e-05\n",
      "Iteration: 5632 Loss: 2.2084148405046777e-05\n",
      "Iteration: 5633 Loss: 2.206390752324077e-05\n",
      "Iteration: 5634 Loss: 2.2043685192918333e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5635 Loss: 2.2023481397019248e-05\n",
      "Iteration: 5636 Loss: 2.2003296118727838e-05\n",
      "Iteration: 5637 Loss: 2.1983129340841087e-05\n",
      "Iteration: 5638 Loss: 2.1962981046576534e-05\n",
      "Iteration: 5639 Loss: 2.1942851218932462e-05\n",
      "Iteration: 5640 Loss: 2.192273984098906e-05\n",
      "Iteration: 5641 Loss: 2.190264689568503e-05\n",
      "Iteration: 5642 Loss: 2.1882572366424975e-05\n",
      "Iteration: 5643 Loss: 2.18625162361803e-05\n",
      "Iteration: 5644 Loss: 2.184247848808438e-05\n",
      "Iteration: 5645 Loss: 2.182245910544119e-05\n",
      "Iteration: 5646 Loss: 2.180245807112153e-05\n",
      "Iteration: 5647 Loss: 2.1782475368458846e-05\n",
      "Iteration: 5648 Loss: 2.1762510980646626e-05\n",
      "Iteration: 5649 Loss: 2.174256489090143e-05\n",
      "Iteration: 5650 Loss: 2.1722637082452588e-05\n",
      "Iteration: 5651 Loss: 2.1702727538545735e-05\n",
      "Iteration: 5652 Loss: 2.1682836242437862e-05\n",
      "Iteration: 5653 Loss: 2.1662963177257925e-05\n",
      "Iteration: 5654 Loss: 2.164310832659044e-05\n",
      "Iteration: 5655 Loss: 2.1623271673743316e-05\n",
      "Iteration: 5656 Loss: 2.1603453201741187e-05\n",
      "Iteration: 5657 Loss: 2.1583652894069615e-05\n",
      "Iteration: 5658 Loss: 2.1563870734079125e-05\n",
      "Iteration: 5659 Loss: 2.154410670519389e-05\n",
      "Iteration: 5660 Loss: 2.152436079062412e-05\n",
      "Iteration: 5661 Loss: 2.1504632973794587e-05\n",
      "Iteration: 5662 Loss: 2.148492323835129e-05\n",
      "Iteration: 5663 Loss: 2.1465231567577764e-05\n",
      "Iteration: 5664 Loss: 2.144555794491677e-05\n",
      "Iteration: 5665 Loss: 2.1425902353824927e-05\n",
      "Iteration: 5666 Loss: 2.140626477777735e-05\n",
      "Iteration: 5667 Loss: 2.1386645200260536e-05\n",
      "Iteration: 5668 Loss: 2.1367043604926006e-05\n",
      "Iteration: 5669 Loss: 2.1347459975001655e-05\n",
      "Iteration: 5670 Loss: 2.1327894294016964e-05\n",
      "Iteration: 5671 Loss: 2.1308346545818575e-05\n",
      "Iteration: 5672 Loss: 2.1288816713819492e-05\n",
      "Iteration: 5673 Loss: 2.1269304781602655e-05\n",
      "Iteration: 5674 Loss: 2.124981073276086e-05\n",
      "Iteration: 5675 Loss: 2.1230334550904173e-05\n",
      "Iteration: 5676 Loss: 2.1210876219656004e-05\n",
      "Iteration: 5677 Loss: 2.1191435722802763e-05\n",
      "Iteration: 5678 Loss: 2.1172013043705408e-05\n",
      "Iteration: 5679 Loss: 2.1152608166176602e-05\n",
      "Iteration: 5680 Loss: 2.113322107390753e-05\n",
      "Iteration: 5681 Loss: 2.1113851750590976e-05\n",
      "Iteration: 5682 Loss: 2.1094500179943618e-05\n",
      "Iteration: 5683 Loss: 2.1075166345749645e-05\n",
      "Iteration: 5684 Loss: 2.105585023164259e-05\n",
      "Iteration: 5685 Loss: 2.1036551821437008e-05\n",
      "Iteration: 5686 Loss: 2.1017271098905178e-05\n",
      "Iteration: 5687 Loss: 2.0998008047835274e-05\n",
      "Iteration: 5688 Loss: 2.0978762652034114e-05\n",
      "Iteration: 5689 Loss: 2.0959534895316513e-05\n",
      "Iteration: 5690 Loss: 2.0940324761519376e-05\n",
      "Iteration: 5691 Loss: 2.092113223454322e-05\n",
      "Iteration: 5692 Loss: 2.0901957298140113e-05\n",
      "Iteration: 5693 Loss: 2.0882799936243338e-05\n",
      "Iteration: 5694 Loss: 2.086366013274636e-05\n",
      "Iteration: 5695 Loss: 2.084453787155593e-05\n",
      "Iteration: 5696 Loss: 2.08254331365916e-05\n",
      "Iteration: 5697 Loss: 2.0806345911735047e-05\n",
      "Iteration: 5698 Loss: 2.078727618099595e-05\n",
      "Iteration: 5699 Loss: 2.0768223928393218e-05\n",
      "Iteration: 5700 Loss: 2.0749189137852767e-05\n",
      "Iteration: 5701 Loss: 2.0730171793372445e-05\n",
      "Iteration: 5702 Loss: 2.0711171878959337e-05\n",
      "Iteration: 5703 Loss: 2.0692189378694205e-05\n",
      "Iteration: 5704 Loss: 2.067322427644988e-05\n",
      "Iteration: 5705 Loss: 2.0654276556447145e-05\n",
      "Iteration: 5706 Loss: 2.0635346202699224e-05\n",
      "Iteration: 5707 Loss: 2.0616433199288135e-05\n",
      "Iteration: 5708 Loss: 2.0597537530456316e-05\n",
      "Iteration: 5709 Loss: 2.0578659180031155e-05\n",
      "Iteration: 5710 Loss: 2.0559798132224903e-05\n",
      "Iteration: 5711 Loss: 2.0540954371289218e-05\n",
      "Iteration: 5712 Loss: 2.0522127881326623e-05\n",
      "Iteration: 5713 Loss: 2.050331864642231e-05\n",
      "Iteration: 5714 Loss: 2.0484526650931103e-05\n",
      "Iteration: 5715 Loss: 2.046575187902449e-05\n",
      "Iteration: 5716 Loss: 2.044699431466349e-05\n",
      "Iteration: 5717 Loss: 2.04282539426426e-05\n",
      "Iteration: 5718 Loss: 2.040953074660443e-05\n",
      "Iteration: 5719 Loss: 2.0390824711031542e-05\n",
      "Iteration: 5720 Loss: 2.0372135820206577e-05\n",
      "Iteration: 5721 Loss: 2.0353464058410693e-05\n",
      "Iteration: 5722 Loss: 2.033480940988546e-05\n",
      "Iteration: 5723 Loss: 2.031617185906356e-05\n",
      "Iteration: 5724 Loss: 2.0297551390213628e-05\n",
      "Iteration: 5725 Loss: 2.0278947987681198e-05\n",
      "Iteration: 5726 Loss: 2.0260361635739582e-05\n",
      "Iteration: 5727 Loss: 2.0241792318930874e-05\n",
      "Iteration: 5728 Loss: 2.022324002141045e-05\n",
      "Iteration: 5729 Loss: 2.0204704728021485e-05\n",
      "Iteration: 5730 Loss: 2.018618642273526e-05\n",
      "Iteration: 5731 Loss: 2.0167685090129512e-05\n",
      "Iteration: 5732 Loss: 2.0149200714648982e-05\n",
      "Iteration: 5733 Loss: 2.0130733280751795e-05\n",
      "Iteration: 5734 Loss: 2.0112282772908875e-05\n",
      "Iteration: 5735 Loss: 2.009384917561119e-05\n",
      "Iteration: 5736 Loss: 2.0075432473353082e-05\n",
      "Iteration: 5737 Loss: 2.0057032650652538e-05\n",
      "Iteration: 5738 Loss: 2.003864969209721e-05\n",
      "Iteration: 5739 Loss: 2.002028358211923e-05\n",
      "Iteration: 5740 Loss: 2.0001934305383796e-05\n",
      "Iteration: 5741 Loss: 1.9983601846351105e-05\n",
      "Iteration: 5742 Loss: 1.996528618966737e-05\n",
      "Iteration: 5743 Loss: 1.9946987319928056e-05\n",
      "Iteration: 5744 Loss: 1.9928705221750578e-05\n",
      "Iteration: 5745 Loss: 1.9910439879762522e-05\n",
      "Iteration: 5746 Loss: 1.989219127860282e-05\n",
      "Iteration: 5747 Loss: 1.9873959402846838e-05\n",
      "Iteration: 5748 Loss: 1.985574423727766e-05\n",
      "Iteration: 5749 Loss: 1.983754576660794e-05\n",
      "Iteration: 5750 Loss: 1.981936397553613e-05\n",
      "Iteration: 5751 Loss: 1.980119884866066e-05\n",
      "Iteration: 5752 Loss: 1.978305037070915e-05\n",
      "Iteration: 5753 Loss: 1.976491852659273e-05\n",
      "Iteration: 5754 Loss: 1.974680330089529e-05\n",
      "Iteration: 5755 Loss: 1.9728704678442434e-05\n",
      "Iteration: 5756 Loss: 1.9710622644014692e-05\n",
      "Iteration: 5757 Loss: 1.969255718241187e-05\n",
      "Iteration: 5758 Loss: 1.9674508278441646e-05\n",
      "Iteration: 5759 Loss: 1.965647591693066e-05\n",
      "Iteration: 5760 Loss: 1.9638460082714945e-05\n",
      "Iteration: 5761 Loss: 1.9620460760674693e-05\n",
      "Iteration: 5762 Loss: 1.960247793562251e-05\n",
      "Iteration: 5763 Loss: 1.9584511592407808e-05\n",
      "Iteration: 5764 Loss: 1.9566561716093973e-05\n",
      "Iteration: 5765 Loss: 1.9548628291245585e-05\n",
      "Iteration: 5766 Loss: 1.9530711303279008e-05\n",
      "Iteration: 5767 Loss: 1.951281073683191e-05\n",
      "Iteration: 5768 Loss: 1.949492657682769e-05\n",
      "Iteration: 5769 Loss: 1.9477058808290618e-05\n",
      "Iteration: 5770 Loss: 1.9459207416192265e-05\n",
      "Iteration: 5771 Loss: 1.944137238561019e-05\n",
      "Iteration: 5772 Loss: 1.9423553701378954e-05\n",
      "Iteration: 5773 Loss: 1.9405751348602856e-05\n",
      "Iteration: 5774 Loss: 1.9387965312312593e-05\n",
      "Iteration: 5775 Loss: 1.9370195577549502e-05\n",
      "Iteration: 5776 Loss: 1.9352442129377794e-05\n",
      "Iteration: 5777 Loss: 1.933470495286891e-05\n",
      "Iteration: 5778 Loss: 1.931698403310741e-05\n",
      "Iteration: 5779 Loss: 1.929927935519663e-05\n",
      "Iteration: 5780 Loss: 1.9281590904247046e-05\n",
      "Iteration: 5781 Loss: 1.9263918665305975e-05\n",
      "Iteration: 5782 Loss: 1.9246262623760475e-05\n",
      "Iteration: 5783 Loss: 1.9228622764519152e-05\n",
      "Iteration: 5784 Loss: 1.9210999072831284e-05\n",
      "Iteration: 5785 Loss: 1.9193391533880716e-05\n",
      "Iteration: 5786 Loss: 1.9175800132859456e-05\n",
      "Iteration: 5787 Loss: 1.9158224854775328e-05\n",
      "Iteration: 5788 Loss: 1.9140665685315573e-05\n",
      "Iteration: 5789 Loss: 1.91231226093992e-05\n",
      "Iteration: 5790 Loss: 1.910559561247609e-05\n",
      "Iteration: 5791 Loss: 1.9088084679519572e-05\n",
      "Iteration: 5792 Loss: 1.9070589795951517e-05\n",
      "Iteration: 5793 Loss: 1.9053110946977424e-05\n",
      "Iteration: 5794 Loss: 1.9035648118069324e-05\n",
      "Iteration: 5795 Loss: 1.90182012944614e-05\n",
      "Iteration: 5796 Loss: 1.9000770461483093e-05\n",
      "Iteration: 5797 Loss: 1.8983355604422506e-05\n",
      "Iteration: 5798 Loss: 1.8965956708778618e-05\n",
      "Iteration: 5799 Loss: 1.8948573759809e-05\n",
      "Iteration: 5800 Loss: 1.8931206742927203e-05\n",
      "Iteration: 5801 Loss: 1.8913855643527812e-05\n",
      "Iteration: 5802 Loss: 1.8896520447025418e-05\n",
      "Iteration: 5803 Loss: 1.887920113884344e-05\n",
      "Iteration: 5804 Loss: 1.886189770441781e-05\n",
      "Iteration: 5805 Loss: 1.8844610129203576e-05\n",
      "Iteration: 5806 Loss: 1.882733839866003e-05\n",
      "Iteration: 5807 Loss: 1.8810082498269017e-05\n",
      "Iteration: 5808 Loss: 1.8792842413576145e-05\n",
      "Iteration: 5809 Loss: 1.877561812997346e-05\n",
      "Iteration: 5810 Loss: 1.8758409633036088e-05\n",
      "Iteration: 5811 Loss: 1.8741216908087655e-05\n",
      "Iteration: 5812 Loss: 1.8724039941142156e-05\n",
      "Iteration: 5813 Loss: 1.8706878717436068e-05\n",
      "Iteration: 5814 Loss: 1.868973322254038e-05\n",
      "Iteration: 5815 Loss: 1.8672603442153854e-05\n",
      "Iteration: 5816 Loss: 1.8655489361815153e-05\n",
      "Iteration: 5817 Loss: 1.863839096713703e-05\n",
      "Iteration: 5818 Loss: 1.862130824374056e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5819 Loss: 1.86042411772631e-05\n",
      "Iteration: 5820 Loss: 1.8587189753356457e-05\n",
      "Iteration: 5821 Loss: 1.8570153957681693e-05\n",
      "Iteration: 5822 Loss: 1.8553133776061446e-05\n",
      "Iteration: 5823 Loss: 1.853612919389186e-05\n",
      "Iteration: 5824 Loss: 1.8519140197025067e-05\n",
      "Iteration: 5825 Loss: 1.850216677117194e-05\n",
      "Iteration: 5826 Loss: 1.848520890206382e-05\n",
      "Iteration: 5827 Loss: 1.8468266575587996e-05\n",
      "Iteration: 5828 Loss: 1.8451339777058787e-05\n",
      "Iteration: 5829 Loss: 1.8434428492833375e-05\n",
      "Iteration: 5830 Loss: 1.841753270815857e-05\n",
      "Iteration: 5831 Loss: 1.8400652409159533e-05\n",
      "Iteration: 5832 Loss: 1.8383787581606082e-05\n",
      "Iteration: 5833 Loss: 1.8366938211206418e-05\n",
      "Iteration: 5834 Loss: 1.835010428384956e-05\n",
      "Iteration: 5835 Loss: 1.8333285785382808e-05\n",
      "Iteration: 5836 Loss: 1.831648270157865e-05\n",
      "Iteration: 5837 Loss: 1.8299695018478803e-05\n",
      "Iteration: 5838 Loss: 1.8282922721883996e-05\n",
      "Iteration: 5839 Loss: 1.8266165797718268e-05\n",
      "Iteration: 5840 Loss: 1.82494242318391e-05\n",
      "Iteration: 5841 Loss: 1.8232698010047637e-05\n",
      "Iteration: 5842 Loss: 1.8215987118578715e-05\n",
      "Iteration: 5843 Loss: 1.8199291543434972e-05\n",
      "Iteration: 5844 Loss: 1.8182611270170965e-05\n",
      "Iteration: 5845 Loss: 1.8165946284967146e-05\n",
      "Iteration: 5846 Loss: 1.814929657380976e-05\n",
      "Iteration: 5847 Loss: 1.8132662122645908e-05\n",
      "Iteration: 5848 Loss: 1.8116042917543706e-05\n",
      "Iteration: 5849 Loss: 1.809943894473335e-05\n",
      "Iteration: 5850 Loss: 1.8082850189755878e-05\n",
      "Iteration: 5851 Loss: 1.8066276639107268e-05\n",
      "Iteration: 5852 Loss: 1.804971827870145e-05\n",
      "Iteration: 5853 Loss: 1.8033175094618893e-05\n",
      "Iteration: 5854 Loss: 1.8016647072947577e-05\n",
      "Iteration: 5855 Loss: 1.800013419979373e-05\n",
      "Iteration: 5856 Loss: 1.7983636461270196e-05\n",
      "Iteration: 5857 Loss: 1.7967153843506872e-05\n",
      "Iteration: 5858 Loss: 1.7950686332646158e-05\n",
      "Iteration: 5859 Loss: 1.7934233914840187e-05\n",
      "Iteration: 5860 Loss: 1.7917796576255354e-05\n",
      "Iteration: 5861 Loss: 1.790137430307306e-05\n",
      "Iteration: 5862 Loss: 1.7884967081483118e-05\n",
      "Iteration: 5863 Loss: 1.7868574897691622e-05\n",
      "Iteration: 5864 Loss: 1.7852197737918173e-05\n",
      "Iteration: 5865 Loss: 1.7835835588388515e-05\n",
      "Iteration: 5866 Loss: 1.7819488435345958e-05\n",
      "Iteration: 5867 Loss: 1.7803156265047355e-05\n",
      "Iteration: 5868 Loss: 1.7786839063759413e-05\n",
      "Iteration: 5869 Loss: 1.7770536817760746e-05\n",
      "Iteration: 5870 Loss: 1.7754249513498062e-05\n",
      "Iteration: 5871 Loss: 1.7737977136827584e-05\n",
      "Iteration: 5872 Loss: 1.772171967451296e-05\n",
      "Iteration: 5873 Loss: 1.7705477112738465e-05\n",
      "Iteration: 5874 Loss: 1.7689249437843207e-05\n",
      "Iteration: 5875 Loss: 1.7673036636186908e-05\n",
      "Iteration: 5876 Loss: 1.7656838694134526e-05\n",
      "Iteration: 5877 Loss: 1.7640655598124962e-05\n",
      "Iteration: 5878 Loss: 1.7624487334440136e-05\n",
      "Iteration: 5879 Loss: 1.760833388959432e-05\n",
      "Iteration: 5880 Loss: 1.7592195249838328e-05\n",
      "Iteration: 5881 Loss: 1.757607140177376e-05\n",
      "Iteration: 5882 Loss: 1.755996233178527e-05\n",
      "Iteration: 5883 Loss: 1.754386802632727e-05\n",
      "Iteration: 5884 Loss: 1.7527788471812855e-05\n",
      "Iteration: 5885 Loss: 1.7511723654833325e-05\n",
      "Iteration: 5886 Loss: 1.7495673561827774e-05\n",
      "Iteration: 5887 Loss: 1.747963817929993e-05\n",
      "Iteration: 5888 Loss: 1.746361749376399e-05\n",
      "Iteration: 5889 Loss: 1.7447611491755764e-05\n",
      "Iteration: 5890 Loss: 1.7431620159811626e-05\n",
      "Iteration: 5891 Loss: 1.7415643484488718e-05\n",
      "Iteration: 5892 Loss: 1.7399681452351583e-05\n",
      "Iteration: 5893 Loss: 1.7383734049898193e-05\n",
      "Iteration: 5894 Loss: 1.736780126388673e-05\n",
      "Iteration: 5895 Loss: 1.7351883080834373e-05\n",
      "Iteration: 5896 Loss: 1.7335979487445642e-05\n",
      "Iteration: 5897 Loss: 1.7320090470177123e-05\n",
      "Iteration: 5898 Loss: 1.7304216015753695e-05\n",
      "Iteration: 5899 Loss: 1.7288356110829394e-05\n",
      "Iteration: 5900 Loss: 1.727251074206843e-05\n",
      "Iteration: 5901 Loss: 1.7256679896202962e-05\n",
      "Iteration: 5902 Loss: 1.7240863559754135e-05\n",
      "Iteration: 5903 Loss: 1.722506171959251e-05\n",
      "Iteration: 5904 Loss: 1.7209274362376294e-05\n",
      "Iteration: 5905 Loss: 1.7193501474829143e-05\n",
      "Iteration: 5906 Loss: 1.7177743043693904e-05\n",
      "Iteration: 5907 Loss: 1.7161999055714822e-05\n",
      "Iteration: 5908 Loss: 1.7146269497658532e-05\n",
      "Iteration: 5909 Loss: 1.7130554356352554e-05\n",
      "Iteration: 5910 Loss: 1.711485361841732e-05\n",
      "Iteration: 5911 Loss: 1.709916727081828e-05\n",
      "Iteration: 5912 Loss: 1.7083495300311435e-05\n",
      "Iteration: 5913 Loss: 1.7067837693719137e-05\n",
      "Iteration: 5914 Loss: 1.7052194437873688e-05\n",
      "Iteration: 5915 Loss: 1.7036565519629874e-05\n",
      "Iteration: 5916 Loss: 1.7020950925838946e-05\n",
      "Iteration: 5917 Loss: 1.7005350643374546e-05\n",
      "Iteration: 5918 Loss: 1.6989764659121377e-05\n",
      "Iteration: 5919 Loss: 1.6974192959972365e-05\n",
      "Iteration: 5920 Loss: 1.6958635532835433e-05\n",
      "Iteration: 5921 Loss: 1.694309236463164e-05\n",
      "Iteration: 5922 Loss: 1.6927563442288766e-05\n",
      "Iteration: 5923 Loss: 1.6912048752752378e-05\n",
      "Iteration: 5924 Loss: 1.689654828297697e-05\n",
      "Iteration: 5925 Loss: 1.68810620199301e-05\n",
      "Iteration: 5926 Loss: 1.686558995058967e-05\n",
      "Iteration: 5927 Loss: 1.6850132061946732e-05\n",
      "Iteration: 5928 Loss: 1.6834688341006456e-05\n",
      "Iteration: 5929 Loss: 1.681925877478111e-05\n",
      "Iteration: 5930 Loss: 1.680384335029782e-05\n",
      "Iteration: 5931 Loss: 1.6788442054596323e-05\n",
      "Iteration: 5932 Loss: 1.6773054874873468e-05\n",
      "Iteration: 5933 Loss: 1.675768179774975e-05\n",
      "Iteration: 5934 Loss: 1.6742322810741445e-05\n",
      "Iteration: 5935 Loss: 1.6726977900787455e-05\n",
      "Iteration: 5936 Loss: 1.6711647054985662e-05\n",
      "Iteration: 5937 Loss: 1.6696330260444393e-05\n",
      "Iteration: 5938 Loss: 1.6681027504287105e-05\n",
      "Iteration: 5939 Loss: 1.666573877364755e-05\n",
      "Iteration: 5940 Loss: 1.665046405566775e-05\n",
      "Iteration: 5941 Loss: 1.663520333750805e-05\n",
      "Iteration: 5942 Loss: 1.661995660633529e-05\n",
      "Iteration: 5943 Loss: 1.6604723849330016e-05\n",
      "Iteration: 5944 Loss: 1.6589505053681893e-05\n",
      "Iteration: 5945 Loss: 1.65743002066018e-05\n",
      "Iteration: 5946 Loss: 1.655910929530069e-05\n",
      "Iteration: 5947 Loss: 1.6543932306920373e-05\n",
      "Iteration: 5948 Loss: 1.6528769228871127e-05\n",
      "Iteration: 5949 Loss: 1.6513620048401996e-05\n",
      "Iteration: 5950 Loss: 1.6498484752665353e-05\n",
      "Iteration: 5951 Loss: 1.64833633288477e-05\n",
      "Iteration: 5952 Loss: 1.6468255764463406e-05\n",
      "Iteration: 5953 Loss: 1.645316204663822e-05\n",
      "Iteration: 5954 Loss: 1.6438082162682386e-05\n",
      "Iteration: 5955 Loss: 1.6423016100028352e-05\n",
      "Iteration: 5956 Loss: 1.6407963845953907e-05\n",
      "Iteration: 5957 Loss: 1.6392925388006513e-05\n",
      "Iteration: 5958 Loss: 1.6377900713076128e-05\n",
      "Iteration: 5959 Loss: 1.6362889808847325e-05\n",
      "Iteration: 5960 Loss: 1.63478926626418e-05\n",
      "Iteration: 5961 Loss: 1.6332909261851398e-05\n",
      "Iteration: 5962 Loss: 1.6317939593878067e-05\n",
      "Iteration: 5963 Loss: 1.6302983646132433e-05\n",
      "Iteration: 5964 Loss: 1.6288041406042312e-05\n",
      "Iteration: 5965 Loss: 1.6273112861042037e-05\n",
      "Iteration: 5966 Loss: 1.6258197998582173e-05\n",
      "Iteration: 5967 Loss: 1.624329680611989e-05\n",
      "Iteration: 5968 Loss: 1.6228409271041676e-05\n",
      "Iteration: 5969 Loss: 1.621353538093642e-05\n",
      "Iteration: 5970 Loss: 1.619867512334177e-05\n",
      "Iteration: 5971 Loss: 1.6183828485697155e-05\n",
      "Iteration: 5972 Loss: 1.6168995455576783e-05\n",
      "Iteration: 5973 Loss: 1.615417602033935e-05\n",
      "Iteration: 5974 Loss: 1.6139370167751227e-05\n",
      "Iteration: 5975 Loss: 1.6124577885136286e-05\n",
      "Iteration: 5976 Loss: 1.6109799160227842e-05\n",
      "Iteration: 5977 Loss: 1.609503398054157e-05\n",
      "Iteration: 5978 Loss: 1.6080282333666356e-05\n",
      "Iteration: 5979 Loss: 1.6065544207195434e-05\n",
      "Iteration: 5980 Loss: 1.6050819588737637e-05\n",
      "Iteration: 5981 Loss: 1.6036108465987667e-05\n",
      "Iteration: 5982 Loss: 1.6021410826466394e-05\n",
      "Iteration: 5983 Loss: 1.6006726657774028e-05\n",
      "Iteration: 5984 Loss: 1.5992055947762836e-05\n",
      "Iteration: 5985 Loss: 1.5977398683890697e-05\n",
      "Iteration: 5986 Loss: 1.5962754853918278e-05\n",
      "Iteration: 5987 Loss: 1.5948124445535454e-05\n",
      "Iteration: 5988 Loss: 1.593350744643756e-05\n",
      "Iteration: 5989 Loss: 1.591890384433499e-05\n",
      "Iteration: 5990 Loss: 1.5904313627006217e-05\n",
      "Iteration: 5991 Loss: 1.5889736782015694e-05\n",
      "Iteration: 5992 Loss: 1.587517329727306e-05\n",
      "Iteration: 5993 Loss: 1.586062316047906e-05\n",
      "Iteration: 5994 Loss: 1.5846086359399683e-05\n",
      "Iteration: 5995 Loss: 1.5831562881812576e-05\n",
      "Iteration: 5996 Loss: 1.581705271550508e-05\n",
      "Iteration: 5997 Loss: 1.580255584827771e-05\n",
      "Iteration: 5998 Loss: 1.5788072267941372e-05\n",
      "Iteration: 5999 Loss: 1.5773601962317303e-05\n",
      "Iteration: 6000 Loss: 1.5759144919155417e-05\n",
      "Iteration: 6001 Loss: 1.574470112652656e-05\n",
      "Iteration: 6002 Loss: 1.5730270572029195e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6003 Loss: 1.5715853243700897e-05\n",
      "Iteration: 6004 Loss: 1.5701449129364434e-05\n",
      "Iteration: 6005 Loss: 1.568705821690738e-05\n",
      "Iteration: 6006 Loss: 1.567268049422997e-05\n",
      "Iteration: 6007 Loss: 1.5658315949242563e-05\n",
      "Iteration: 6008 Loss: 1.564396456986852e-05\n",
      "Iteration: 6009 Loss: 1.5629626344039922e-05\n",
      "Iteration: 6010 Loss: 1.561530125970365e-05\n",
      "Iteration: 6011 Loss: 1.5600989304811604e-05\n",
      "Iteration: 6012 Loss: 1.558669046733248e-05\n",
      "Iteration: 6013 Loss: 1.5572404735326877e-05\n",
      "Iteration: 6014 Loss: 1.5558132096614936e-05\n",
      "Iteration: 6015 Loss: 1.5543872539280977e-05\n",
      "Iteration: 6016 Loss: 1.552962605133438e-05\n",
      "Iteration: 6017 Loss: 1.551539262085334e-05\n",
      "Iteration: 6018 Loss: 1.5501172235702913e-05\n",
      "Iteration: 6019 Loss: 1.5486964884008094e-05\n",
      "Iteration: 6020 Loss: 1.5472770553939595e-05\n",
      "Iteration: 6021 Loss: 1.54585892334748e-05\n",
      "Iteration: 6022 Loss: 1.544442091069136e-05\n",
      "Iteration: 6023 Loss: 1.543026557367553e-05\n",
      "Iteration: 6024 Loss: 1.541612321052629e-05\n",
      "Iteration: 6025 Loss: 1.5401993809409045e-05\n",
      "Iteration: 6026 Loss: 1.5387877358328397e-05\n",
      "Iteration: 6027 Loss: 1.5373773845420845e-05\n",
      "Iteration: 6028 Loss: 1.535968325893733e-05\n",
      "Iteration: 6029 Loss: 1.5345605586970262e-05\n",
      "Iteration: 6030 Loss: 1.5331540817691428e-05\n",
      "Iteration: 6031 Loss: 1.531748893926865e-05\n",
      "Iteration: 6032 Loss: 1.530344993988723e-05\n",
      "Iteration: 6033 Loss: 1.5289423807747064e-05\n",
      "Iteration: 6034 Loss: 1.5275410531051203e-05\n",
      "Iteration: 6035 Loss: 1.5261410098019615e-05\n",
      "Iteration: 6036 Loss: 1.5247422496878232e-05\n",
      "Iteration: 6037 Loss: 1.5233447715865785e-05\n",
      "Iteration: 6038 Loss: 1.5219485743237523e-05\n",
      "Iteration: 6039 Loss: 1.5205536567246495e-05\n",
      "Iteration: 6040 Loss: 1.5191600176170138e-05\n",
      "Iteration: 6041 Loss: 1.5177676558290596e-05\n",
      "Iteration: 6042 Loss: 1.5163765701895097e-05\n",
      "Iteration: 6043 Loss: 1.514986759529231e-05\n",
      "Iteration: 6044 Loss: 1.5135982226795445e-05\n",
      "Iteration: 6045 Loss: 1.5122109584644792e-05\n",
      "Iteration: 6046 Loss: 1.5108249657342563e-05\n",
      "Iteration: 6047 Loss: 1.5094402433157695e-05\n",
      "Iteration: 6048 Loss: 1.5080567900441245e-05\n",
      "Iteration: 6049 Loss: 1.5066746047645824e-05\n",
      "Iteration: 6050 Loss: 1.5052936862984048e-05\n",
      "Iteration: 6051 Loss: 1.5039140334927342e-05\n",
      "Iteration: 6052 Loss: 1.5025356451874937e-05\n",
      "Iteration: 6053 Loss: 1.5011585202238731e-05\n",
      "Iteration: 6054 Loss: 1.4997826574439324e-05\n",
      "Iteration: 6055 Loss: 1.4984080556907991e-05\n",
      "Iteration: 6056 Loss: 1.4970347138003383e-05\n",
      "Iteration: 6057 Loss: 1.4956626306403945e-05\n",
      "Iteration: 6058 Loss: 1.4942918050316855e-05\n",
      "Iteration: 6059 Loss: 1.4929222358386827e-05\n",
      "Iteration: 6060 Loss: 1.4915539219127854e-05\n",
      "Iteration: 6061 Loss: 1.4901868620864708e-05\n",
      "Iteration: 6062 Loss: 1.4888210552189999e-05\n",
      "Iteration: 6063 Loss: 1.487456500161375e-05\n",
      "Iteration: 6064 Loss: 1.4860931957670067e-05\n",
      "Iteration: 6065 Loss: 1.4847311408889691e-05\n",
      "Iteration: 6066 Loss: 1.4833703343882178e-05\n",
      "Iteration: 6067 Loss: 1.4820107751089535e-05\n",
      "Iteration: 6068 Loss: 1.480652461913763e-05\n",
      "Iteration: 6069 Loss: 1.4792953936548873e-05\n",
      "Iteration: 6070 Loss: 1.4779395692026994e-05\n",
      "Iteration: 6071 Loss: 1.4765849874117094e-05\n",
      "Iteration: 6072 Loss: 1.4752316471424926e-05\n",
      "Iteration: 6073 Loss: 1.4738795472578077e-05\n",
      "Iteration: 6074 Loss: 1.4725286866201855e-05\n",
      "Iteration: 6075 Loss: 1.4711790640997876e-05\n",
      "Iteration: 6076 Loss: 1.469830678535586e-05\n",
      "Iteration: 6077 Loss: 1.4684835288349642e-05\n",
      "Iteration: 6078 Loss: 1.467137613838827e-05\n",
      "Iteration: 6079 Loss: 1.4657929324156861e-05\n",
      "Iteration: 6080 Loss: 1.4644494834404986e-05\n",
      "Iteration: 6081 Loss: 1.4631072657893986e-05\n",
      "Iteration: 6082 Loss: 1.4617662783284631e-05\n",
      "Iteration: 6083 Loss: 1.4604265199296286e-05\n",
      "Iteration: 6084 Loss: 1.4590879894814998e-05\n",
      "Iteration: 6085 Loss: 1.4577506858207165e-05\n",
      "Iteration: 6086 Loss: 1.4564146078540917e-05\n",
      "Iteration: 6087 Loss: 1.4550797544505008e-05\n",
      "Iteration: 6088 Loss: 1.4537461244871647e-05\n",
      "Iteration: 6089 Loss: 1.4524137168511787e-05\n",
      "Iteration: 6090 Loss: 1.4510825304053425e-05\n",
      "Iteration: 6091 Loss: 1.449752564038917e-05\n",
      "Iteration: 6092 Loss: 1.4484238166336307e-05\n",
      "Iteration: 6093 Loss: 1.4470962870724578e-05\n",
      "Iteration: 6094 Loss: 1.4457699742241117e-05\n",
      "Iteration: 6095 Loss: 1.444444877008899e-05\n",
      "Iteration: 6096 Loss: 1.4431209943009095e-05\n",
      "Iteration: 6097 Loss: 1.4417983249580467e-05\n",
      "Iteration: 6098 Loss: 1.4404768678938387e-05\n",
      "Iteration: 6099 Loss: 1.4391566219917602e-05\n",
      "Iteration: 6100 Loss: 1.4378375861417892e-05\n",
      "Iteration: 6101 Loss: 1.4365197592347046e-05\n",
      "Iteration: 6102 Loss: 1.4352031401624686e-05\n",
      "Iteration: 6103 Loss: 1.4338877278180771e-05\n",
      "Iteration: 6104 Loss: 1.4325735210957286e-05\n",
      "Iteration: 6105 Loss: 1.431260518890245e-05\n",
      "Iteration: 6106 Loss: 1.4299487200975884e-05\n",
      "Iteration: 6107 Loss: 1.4286381236148966e-05\n",
      "Iteration: 6108 Loss: 1.4273287283402058e-05\n",
      "Iteration: 6109 Loss: 1.4260205331723028e-05\n",
      "Iteration: 6110 Loss: 1.4247135370118553e-05\n",
      "Iteration: 6111 Loss: 1.4234077387592248e-05\n",
      "Iteration: 6112 Loss: 1.4221031373171437e-05\n",
      "Iteration: 6113 Loss: 1.4207997315880592e-05\n",
      "Iteration: 6114 Loss: 1.4194975204766666e-05\n",
      "Iteration: 6115 Loss: 1.418196502887588e-05\n",
      "Iteration: 6116 Loss: 1.416896677726891e-05\n",
      "Iteration: 6117 Loss: 1.4155980439079157e-05\n",
      "Iteration: 6118 Loss: 1.414300600332543e-05\n",
      "Iteration: 6119 Loss: 1.4130043459044167e-05\n",
      "Iteration: 6120 Loss: 1.4117092795392085e-05\n",
      "Iteration: 6121 Loss: 1.410415400147902e-05\n",
      "Iteration: 6122 Loss: 1.4091227066431097e-05\n",
      "Iteration: 6123 Loss: 1.4078311979372782e-05\n",
      "Iteration: 6124 Loss: 1.406540872944959e-05\n",
      "Iteration: 6125 Loss: 1.4052517305754675e-05\n",
      "Iteration: 6126 Loss: 1.4039637697560136e-05\n",
      "Iteration: 6127 Loss: 1.4026769893980586e-05\n",
      "Iteration: 6128 Loss: 1.4013913884199134e-05\n",
      "Iteration: 6129 Loss: 1.4001069657200633e-05\n",
      "Iteration: 6130 Loss: 1.398823720259412e-05\n",
      "Iteration: 6131 Loss: 1.3975416509383426e-05\n",
      "Iteration: 6132 Loss: 1.3962607566941913e-05\n",
      "Iteration: 6133 Loss: 1.3949810364199443e-05\n",
      "Iteration: 6134 Loss: 1.393702489060161e-05\n",
      "Iteration: 6135 Loss: 1.3924251135283516e-05\n",
      "Iteration: 6136 Loss: 1.3911489087566158e-05\n",
      "Iteration: 6137 Loss: 1.3898738736715952e-05\n",
      "Iteration: 6138 Loss: 1.388600007195777e-05\n",
      "Iteration: 6139 Loss: 1.387327308269034e-05\n",
      "Iteration: 6140 Loss: 1.3860557758158359e-05\n",
      "Iteration: 6141 Loss: 1.3847854087522355e-05\n",
      "Iteration: 6142 Loss: 1.3835162060397224e-05\n",
      "Iteration: 6143 Loss: 1.3822481666168143e-05\n",
      "Iteration: 6144 Loss: 1.3809812893763698e-05\n",
      "Iteration: 6145 Loss: 1.3797155732737168e-05\n",
      "Iteration: 6146 Loss: 1.3784510172444035e-05\n",
      "Iteration: 6147 Loss: 1.3771876202255605e-05\n",
      "Iteration: 6148 Loss: 1.3759253811488037e-05\n",
      "Iteration: 6149 Loss: 1.3746642989645985e-05\n",
      "Iteration: 6150 Loss: 1.3734043726064349e-05\n",
      "Iteration: 6151 Loss: 1.3721456010216361e-05\n",
      "Iteration: 6152 Loss: 1.3708879831336579e-05\n",
      "Iteration: 6153 Loss: 1.3696315179026182e-05\n",
      "Iteration: 6154 Loss: 1.3683762042662299e-05\n",
      "Iteration: 6155 Loss: 1.3671220411693471e-05\n",
      "Iteration: 6156 Loss: 1.365869027557249e-05\n",
      "Iteration: 6157 Loss: 1.3646171623761626e-05\n",
      "Iteration: 6158 Loss: 1.3633664445740475e-05\n",
      "Iteration: 6159 Loss: 1.3621168731135733e-05\n",
      "Iteration: 6160 Loss: 1.360868446914713e-05\n",
      "Iteration: 6161 Loss: 1.359621164942407e-05\n",
      "Iteration: 6162 Loss: 1.3583750261541201e-05\n",
      "Iteration: 6163 Loss: 1.3571300294958217e-05\n",
      "Iteration: 6164 Loss: 1.3558861739154416e-05\n",
      "Iteration: 6165 Loss: 1.354643458372562e-05\n",
      "Iteration: 6166 Loss: 1.3534018818222603e-05\n",
      "Iteration: 6167 Loss: 1.3521614432060325e-05\n",
      "Iteration: 6168 Loss: 1.3509221415104996e-05\n",
      "Iteration: 6169 Loss: 1.3496839756843042e-05\n",
      "Iteration: 6170 Loss: 1.3484469446750892e-05\n",
      "Iteration: 6171 Loss: 1.3472110474427816e-05\n",
      "Iteration: 6172 Loss: 1.3459762829597478e-05\n",
      "Iteration: 6173 Loss: 1.3447426501762876e-05\n",
      "Iteration: 6174 Loss: 1.343510148060861e-05\n",
      "Iteration: 6175 Loss: 1.3422787755826939e-05\n",
      "Iteration: 6176 Loss: 1.3410485317010702e-05\n",
      "Iteration: 6177 Loss: 1.3398194153813364e-05\n",
      "Iteration: 6178 Loss: 1.3385914255900595e-05\n",
      "Iteration: 6179 Loss: 1.3373645612948137e-05\n",
      "Iteration: 6180 Loss: 1.3361388214643103e-05\n",
      "Iteration: 6181 Loss: 1.3349142050671382e-05\n",
      "Iteration: 6182 Loss: 1.3336907110746478e-05\n",
      "Iteration: 6183 Loss: 1.3324683384571823e-05\n",
      "Iteration: 6184 Loss: 1.3312470861877938e-05\n",
      "Iteration: 6185 Loss: 1.3300269532449472e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6186 Loss: 1.3288079385912134e-05\n",
      "Iteration: 6187 Loss: 1.3275900412073981e-05\n",
      "Iteration: 6188 Loss: 1.326373260069677e-05\n",
      "Iteration: 6189 Loss: 1.3251575941399016e-05\n",
      "Iteration: 6190 Loss: 1.3239430424404616e-05\n",
      "Iteration: 6191 Loss: 1.3227296039059278e-05\n",
      "Iteration: 6192 Loss: 1.3215172775250171e-05\n",
      "Iteration: 6193 Loss: 1.3203060622952358e-05\n",
      "Iteration: 6194 Loss: 1.3190959571872156e-05\n",
      "Iteration: 6195 Loss: 1.3178869611774037e-05\n",
      "Iteration: 6196 Loss: 1.3166790732552204e-05\n",
      "Iteration: 6197 Loss: 1.3154722923901917e-05\n",
      "Iteration: 6198 Loss: 1.3142666175913695e-05\n",
      "Iteration: 6199 Loss: 1.3130620478422344e-05\n",
      "Iteration: 6200 Loss: 1.3118585821234184e-05\n",
      "Iteration: 6201 Loss: 1.3106562194180796e-05\n",
      "Iteration: 6202 Loss: 1.3094549587260316e-05\n",
      "Iteration: 6203 Loss: 1.308254799031962e-05\n",
      "Iteration: 6204 Loss: 1.3070557393413514e-05\n",
      "Iteration: 6205 Loss: 1.3058577786164292e-05\n",
      "Iteration: 6206 Loss: 1.304660915864896e-05\n",
      "Iteration: 6207 Loss: 1.3034651500805628e-05\n",
      "Iteration: 6208 Loss: 1.3022704802633678e-05\n",
      "Iteration: 6209 Loss: 1.3010769054122759e-05\n",
      "Iteration: 6210 Loss: 1.2998844245000442e-05\n",
      "Iteration: 6211 Loss: 1.2986930365154589e-05\n",
      "Iteration: 6212 Loss: 1.297502740496906e-05\n",
      "Iteration: 6213 Loss: 1.2963135354388452e-05\n",
      "Iteration: 6214 Loss: 1.2951254203233174e-05\n",
      "Iteration: 6215 Loss: 1.2939383941574659e-05\n",
      "Iteration: 6216 Loss: 1.2927524559575781e-05\n",
      "Iteration: 6217 Loss: 1.2915676046973383e-05\n",
      "Iteration: 6218 Loss: 1.290383839389302e-05\n",
      "Iteration: 6219 Loss: 1.2892011590554711e-05\n",
      "Iteration: 6220 Loss: 1.288019562670067e-05\n",
      "Iteration: 6221 Loss: 1.286839049279425e-05\n",
      "Iteration: 6222 Loss: 1.2856596178593038e-05\n",
      "Iteration: 6223 Loss: 1.2844812674498134e-05\n",
      "Iteration: 6224 Loss: 1.283303997026203e-05\n",
      "Iteration: 6225 Loss: 1.282127805612213e-05\n",
      "Iteration: 6226 Loss: 1.2809526922136602e-05\n",
      "Iteration: 6227 Loss: 1.279778655859337e-05\n",
      "Iteration: 6228 Loss: 1.2786056955450399e-05\n",
      "Iteration: 6229 Loss: 1.2774338102847089e-05\n",
      "Iteration: 6230 Loss: 1.2762629990985968e-05\n",
      "Iteration: 6231 Loss: 1.2750932610080369e-05\n",
      "Iteration: 6232 Loss: 1.273924595023692e-05\n",
      "Iteration: 6233 Loss: 1.2727570001629982e-05\n",
      "Iteration: 6234 Loss: 1.271590475449908e-05\n",
      "Iteration: 6235 Loss: 1.270425019892325e-05\n",
      "Iteration: 6236 Loss: 1.2692606325159605e-05\n",
      "Iteration: 6237 Loss: 1.2680973123415483e-05\n",
      "Iteration: 6238 Loss: 1.2669350583856283e-05\n",
      "Iteration: 6239 Loss: 1.2657738696822464e-05\n",
      "Iteration: 6240 Loss: 1.2646137452491904e-05\n",
      "Iteration: 6241 Loss: 1.263454684116852e-05\n",
      "Iteration: 6242 Loss: 1.26229668530513e-05\n",
      "Iteration: 6243 Loss: 1.2611397478344548e-05\n",
      "Iteration: 6244 Loss: 1.2599838707320553e-05\n",
      "Iteration: 6245 Loss: 1.2588290530376662e-05\n",
      "Iteration: 6246 Loss: 1.2576752937744645e-05\n",
      "Iteration: 6247 Loss: 1.256522591972265e-05\n",
      "Iteration: 6248 Loss: 1.2553709466622293e-05\n",
      "Iteration: 6249 Loss: 1.2542203568756415e-05\n",
      "Iteration: 6250 Loss: 1.2530708216453214e-05\n",
      "Iteration: 6251 Loss: 1.2519223399987496e-05\n",
      "Iteration: 6252 Loss: 1.2507749109821594e-05\n",
      "Iteration: 6253 Loss: 1.2496285336246239e-05\n",
      "Iteration: 6254 Loss: 1.248483206962338e-05\n",
      "Iteration: 6255 Loss: 1.247338930017817e-05\n",
      "Iteration: 6256 Loss: 1.2461957018583003e-05\n",
      "Iteration: 6257 Loss: 1.2450535215134034e-05\n",
      "Iteration: 6258 Loss: 1.2439123880112757e-05\n",
      "Iteration: 6259 Loss: 1.2427723003985808e-05\n",
      "Iteration: 6260 Loss: 1.2416332577309919e-05\n",
      "Iteration: 6261 Loss: 1.2404952590273147e-05\n",
      "Iteration: 6262 Loss: 1.2393583033337355e-05\n",
      "Iteration: 6263 Loss: 1.2382223897005856e-05\n",
      "Iteration: 6264 Loss: 1.2370875171722557e-05\n",
      "Iteration: 6265 Loss: 1.2359536847946192e-05\n",
      "Iteration: 6266 Loss: 1.2348208915941315e-05\n",
      "Iteration: 6267 Loss: 1.2336891366737147e-05\n",
      "Iteration: 6268 Loss: 1.2325584190317885e-05\n",
      "Iteration: 6269 Loss: 1.231428737747225e-05\n",
      "Iteration: 6270 Loss: 1.2303000918352095e-05\n",
      "Iteration: 6271 Loss: 1.2291724803579427e-05\n",
      "Iteration: 6272 Loss: 1.2280459023911454e-05\n",
      "Iteration: 6273 Loss: 1.2269203569730875e-05\n",
      "Iteration: 6274 Loss: 1.2257958431569577e-05\n",
      "Iteration: 6275 Loss: 1.2246723600030576e-05\n",
      "Iteration: 6276 Loss: 1.2235499065554203e-05\n",
      "Iteration: 6277 Loss: 1.2224284818704476e-05\n",
      "Iteration: 6278 Loss: 1.2213080850165589e-05\n",
      "Iteration: 6279 Loss: 1.2201887150459232e-05\n",
      "Iteration: 6280 Loss: 1.2190703710175411e-05\n",
      "Iteration: 6281 Loss: 1.2179530519907913e-05\n",
      "Iteration: 6282 Loss: 1.2168367570265147e-05\n",
      "Iteration: 6283 Loss: 1.2157214851858356e-05\n",
      "Iteration: 6284 Loss: 1.2146072355163589e-05\n",
      "Iteration: 6285 Loss: 1.2134940071170182e-05\n",
      "Iteration: 6286 Loss: 1.2123817990191113e-05\n",
      "Iteration: 6287 Loss: 1.2112706103103539e-05\n",
      "Iteration: 6288 Loss: 1.2101604400400718e-05\n",
      "Iteration: 6289 Loss: 1.2090512872852312e-05\n",
      "Iteration: 6290 Loss: 1.2079431511028146e-05\n",
      "Iteration: 6291 Loss: 1.2068360305660728e-05\n",
      "Iteration: 6292 Loss: 1.2057299247595136e-05\n",
      "Iteration: 6293 Loss: 1.2046248327174532e-05\n",
      "Iteration: 6294 Loss: 1.203520753536756e-05\n",
      "Iteration: 6295 Loss: 1.2024176862836853e-05\n",
      "Iteration: 6296 Loss: 1.2013156300363194e-05\n",
      "Iteration: 6297 Loss: 1.2002145838569302e-05\n",
      "Iteration: 6298 Loss: 1.199114546825434e-05\n",
      "Iteration: 6299 Loss: 1.1980155180163968e-05\n",
      "Iteration: 6300 Loss: 1.1969174965006323e-05\n",
      "Iteration: 6301 Loss: 1.19582048136031e-05\n",
      "Iteration: 6302 Loss: 1.1947244716784852e-05\n",
      "Iteration: 6303 Loss: 1.193629466534352e-05\n",
      "Iteration: 6304 Loss: 1.192535464995303e-05\n",
      "Iteration: 6305 Loss: 1.1914424661526612e-05\n",
      "Iteration: 6306 Loss: 1.1903504690769548e-05\n",
      "Iteration: 6307 Loss: 1.1892594728549962e-05\n",
      "Iteration: 6308 Loss: 1.1881694765693795e-05\n",
      "Iteration: 6309 Loss: 1.1870804793039461e-05\n",
      "Iteration: 6310 Loss: 1.1859924801429485e-05\n",
      "Iteration: 6311 Loss: 1.1849054781719167e-05\n",
      "Iteration: 6312 Loss: 1.1838194724763607e-05\n",
      "Iteration: 6313 Loss: 1.1827344621430993e-05\n",
      "Iteration: 6314 Loss: 1.181650446260412e-05\n",
      "Iteration: 6315 Loss: 1.1805674239018608e-05\n",
      "Iteration: 6316 Loss: 1.1794853941863006e-05\n",
      "Iteration: 6317 Loss: 1.1784043561892055e-05\n",
      "Iteration: 6318 Loss: 1.177324309001611e-05\n",
      "Iteration: 6319 Loss: 1.1762452517097538e-05\n",
      "Iteration: 6320 Loss: 1.1751671834232372e-05\n",
      "Iteration: 6321 Loss: 1.1740901032189125e-05\n",
      "Iteration: 6322 Loss: 1.173014010190584e-05\n",
      "Iteration: 6323 Loss: 1.1719389034604054e-05\n",
      "Iteration: 6324 Loss: 1.1708647820940638e-05\n",
      "Iteration: 6325 Loss: 1.1697916451924945e-05\n",
      "Iteration: 6326 Loss: 1.1687194918586629e-05\n",
      "Iteration: 6327 Loss: 1.167648321191452e-05\n",
      "Iteration: 6328 Loss: 1.1665781322841421e-05\n",
      "Iteration: 6329 Loss: 1.1655089242484651e-05\n",
      "Iteration: 6330 Loss: 1.1644406961798359e-05\n",
      "Iteration: 6331 Loss: 1.1633734471739834e-05\n",
      "Iteration: 6332 Loss: 1.1623071763454913e-05\n",
      "Iteration: 6333 Loss: 1.1612418827919278e-05\n",
      "Iteration: 6334 Loss: 1.160177565617273e-05\n",
      "Iteration: 6335 Loss: 1.1591142239269626e-05\n",
      "Iteration: 6336 Loss: 1.1580518568121497e-05\n",
      "Iteration: 6337 Loss: 1.1569904634091087e-05\n",
      "Iteration: 6338 Loss: 1.1559300428253678e-05\n",
      "Iteration: 6339 Loss: 1.1548705941399191e-05\n",
      "Iteration: 6340 Loss: 1.1538121164765652e-05\n",
      "Iteration: 6341 Loss: 1.152754608945188e-05\n",
      "Iteration: 6342 Loss: 1.1516980706571146e-05\n",
      "Iteration: 6343 Loss: 1.1506425007235563e-05\n",
      "Iteration: 6344 Loss: 1.1495878982569235e-05\n",
      "Iteration: 6345 Loss: 1.1485342623709441e-05\n",
      "Iteration: 6346 Loss: 1.147481592179339e-05\n",
      "Iteration: 6347 Loss: 1.146429886803232e-05\n",
      "Iteration: 6348 Loss: 1.1453791453518358e-05\n",
      "Iteration: 6349 Loss: 1.1443293669363885e-05\n",
      "Iteration: 6350 Loss: 1.1432805506800553e-05\n",
      "Iteration: 6351 Loss: 1.142232695701106e-05\n",
      "Iteration: 6352 Loss: 1.1411858011180715e-05\n",
      "Iteration: 6353 Loss: 1.1401398660450734e-05\n",
      "Iteration: 6354 Loss: 1.1390948896085629e-05\n",
      "Iteration: 6355 Loss: 1.1380508709356506e-05\n",
      "Iteration: 6356 Loss: 1.1370078091427227e-05\n",
      "Iteration: 6357 Loss: 1.1359657033525845e-05\n",
      "Iteration: 6358 Loss: 1.134924552689418e-05\n",
      "Iteration: 6359 Loss: 1.1338843562776605e-05\n",
      "Iteration: 6360 Loss: 1.132845113242528e-05\n",
      "Iteration: 6361 Loss: 1.1318068227103521e-05\n",
      "Iteration: 6362 Loss: 1.1307694838082278e-05\n",
      "Iteration: 6363 Loss: 1.1297330956637264e-05\n",
      "Iteration: 6364 Loss: 1.1286976574055977e-05\n",
      "Iteration: 6365 Loss: 1.1276631681632095e-05\n",
      "Iteration: 6366 Loss: 1.1266296270669651e-05\n",
      "Iteration: 6367 Loss: 1.1255970332472734e-05\n",
      "Iteration: 6368 Loss: 1.1245653858365578e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6369 Loss: 1.123534683966947e-05\n",
      "Iteration: 6370 Loss: 1.1225049267723565e-05\n",
      "Iteration: 6371 Loss: 1.1214761133919087e-05\n",
      "Iteration: 6372 Loss: 1.1204482429440376e-05\n",
      "Iteration: 6373 Loss: 1.1194213145872146e-05\n",
      "Iteration: 6374 Loss: 1.118395327446273e-05\n",
      "Iteration: 6375 Loss: 1.117370280653238e-05\n",
      "Iteration: 6376 Loss: 1.1163461733462804e-05\n",
      "Iteration: 6377 Loss: 1.115323004669738e-05\n",
      "Iteration: 6378 Loss: 1.1143007737688252e-05\n",
      "Iteration: 6379 Loss: 1.1132794797789125e-05\n",
      "Iteration: 6380 Loss: 1.1122591218409202e-05\n",
      "Iteration: 6381 Loss: 1.1112396990973265e-05\n",
      "Iteration: 6382 Loss: 1.1102212106904123e-05\n",
      "Iteration: 6383 Loss: 1.1092036557639206e-05\n",
      "Iteration: 6384 Loss: 1.1081870334627473e-05\n",
      "Iteration: 6385 Loss: 1.1071713429316628e-05\n",
      "Iteration: 6386 Loss: 1.1061565833171123e-05\n",
      "Iteration: 6387 Loss: 1.1051427537651718e-05\n",
      "Iteration: 6388 Loss: 1.1041298534242054e-05\n",
      "Iteration: 6389 Loss: 1.1031178814420312e-05\n",
      "Iteration: 6390 Loss: 1.102106836967767e-05\n",
      "Iteration: 6391 Loss: 1.1010967191516926e-05\n",
      "Iteration: 6392 Loss: 1.100087527149832e-05\n",
      "Iteration: 6393 Loss: 1.0990792601025353e-05\n",
      "Iteration: 6394 Loss: 1.0980719171472291e-05\n",
      "Iteration: 6395 Loss: 1.0970654974775184e-05\n",
      "Iteration: 6396 Loss: 1.0960600002272208e-05\n",
      "Iteration: 6397 Loss: 1.0950554245503647e-05\n",
      "Iteration: 6398 Loss: 1.094051769603017e-05\n",
      "Iteration: 6399 Loss: 1.093049034555128e-05\n",
      "Iteration: 6400 Loss: 1.0920472185348952e-05\n",
      "Iteration: 6401 Loss: 1.0910463207146351e-05\n",
      "Iteration: 6402 Loss: 1.0900463402522273e-05\n",
      "Iteration: 6403 Loss: 1.089047276307258e-05\n",
      "Iteration: 6404 Loss: 1.0880491280398357e-05\n",
      "Iteration: 6405 Loss: 1.0870518946103992e-05\n",
      "Iteration: 6406 Loss: 1.0860555751892737e-05\n",
      "Iteration: 6407 Loss: 1.0850601689215314e-05\n",
      "Iteration: 6408 Loss: 1.0840656749787301e-05\n",
      "Iteration: 6409 Loss: 1.0830720925248794e-05\n",
      "Iteration: 6410 Loss: 1.082079420709685e-05\n",
      "Iteration: 6411 Loss: 1.081087658734311e-05\n",
      "Iteration: 6412 Loss: 1.0800968057227416e-05\n",
      "Iteration: 6413 Loss: 1.0791068608779244e-05\n",
      "Iteration: 6414 Loss: 1.0781178233520908e-05\n",
      "Iteration: 6415 Loss: 1.0771296923290883e-05\n",
      "Iteration: 6416 Loss: 1.0761424669483128e-05\n",
      "Iteration: 6417 Loss: 1.0751561463944535e-05\n",
      "Iteration: 6418 Loss: 1.0741707298381362e-05\n",
      "Iteration: 6419 Loss: 1.0731862164509715e-05\n",
      "Iteration: 6420 Loss: 1.0722026054052802e-05\n",
      "Iteration: 6421 Loss: 1.0712198958736235e-05\n",
      "Iteration: 6422 Loss: 1.0702380870301084e-05\n",
      "Iteration: 6423 Loss: 1.0692571780489228e-05\n",
      "Iteration: 6424 Loss: 1.0682771681055626e-05\n",
      "Iteration: 6425 Loss: 1.0672980563760677e-05\n",
      "Iteration: 6426 Loss: 1.0663198420368248e-05\n",
      "Iteration: 6427 Loss: 1.0653425242714259e-05\n",
      "Iteration: 6428 Loss: 1.0643661022466291e-05\n",
      "Iteration: 6429 Loss: 1.0633905751468122e-05\n",
      "Iteration: 6430 Loss: 1.0624159421579921e-05\n",
      "Iteration: 6431 Loss: 1.061442202443391e-05\n",
      "Iteration: 6432 Loss: 1.0604693551955416e-05\n",
      "Iteration: 6433 Loss: 1.0594973996022772e-05\n",
      "Iteration: 6434 Loss: 1.0585263348405208e-05\n",
      "Iteration: 6435 Loss: 1.057556160094075e-05\n",
      "Iteration: 6436 Loss: 1.0565868745471368e-05\n",
      "Iteration: 6437 Loss: 1.0556184773696047e-05\n",
      "Iteration: 6438 Loss: 1.0546509677771764e-05\n",
      "Iteration: 6439 Loss: 1.05368434494158e-05\n",
      "Iteration: 6440 Loss: 1.0527186080553612e-05\n",
      "Iteration: 6441 Loss: 1.0517537562899857e-05\n",
      "Iteration: 6442 Loss: 1.0507897888509859e-05\n",
      "Iteration: 6443 Loss: 1.0498267049221614e-05\n",
      "Iteration: 6444 Loss: 1.0488645036995036e-05\n",
      "Iteration: 6445 Loss: 1.0479031843682257e-05\n",
      "Iteration: 6446 Loss: 1.0469427461142587e-05\n",
      "Iteration: 6447 Loss: 1.0459831881360127e-05\n",
      "Iteration: 6448 Loss: 1.045024509620815e-05\n",
      "Iteration: 6449 Loss: 1.0440667097684816e-05\n",
      "Iteration: 6450 Loss: 1.0431097877938407e-05\n",
      "Iteration: 6451 Loss: 1.0421537428629291e-05\n",
      "Iteration: 6452 Loss: 1.0411985741754408e-05\n",
      "Iteration: 6453 Loss: 1.0402442809336695e-05\n",
      "Iteration: 6454 Loss: 1.0392908623352666e-05\n",
      "Iteration: 6455 Loss: 1.0383383175845681e-05\n",
      "Iteration: 6456 Loss: 1.037386645869018e-05\n",
      "Iteration: 6457 Loss: 1.0364358463941543e-05\n",
      "Iteration: 6458 Loss: 1.0354859183605695e-05\n",
      "Iteration: 6459 Loss: 1.0345368609696577e-05\n",
      "Iteration: 6460 Loss: 1.0335886734177703e-05\n",
      "Iteration: 6461 Loss: 1.0326413549392978e-05\n",
      "Iteration: 6462 Loss: 1.0316949046912119e-05\n",
      "Iteration: 6463 Loss: 1.030749321883493e-05\n",
      "Iteration: 6464 Loss: 1.0298046057505983e-05\n",
      "Iteration: 6465 Loss: 1.0288607554832753e-05\n",
      "Iteration: 6466 Loss: 1.0279177702883061e-05\n",
      "Iteration: 6467 Loss: 1.0269756493723793e-05\n",
      "Iteration: 6468 Loss: 1.0260343919436637e-05\n",
      "Iteration: 6469 Loss: 1.025093997210566e-05\n",
      "Iteration: 6470 Loss: 1.0241544643825066e-05\n",
      "Iteration: 6471 Loss: 1.023215792669501e-05\n",
      "Iteration: 6472 Loss: 1.022277981282097e-05\n",
      "Iteration: 6473 Loss: 1.0213410294321322e-05\n",
      "Iteration: 6474 Loss: 1.0204049363258728e-05\n",
      "Iteration: 6475 Loss: 1.0194697011819614e-05\n",
      "Iteration: 6476 Loss: 1.0185353232256738e-05\n",
      "Iteration: 6477 Loss: 1.0176018016595328e-05\n",
      "Iteration: 6478 Loss: 1.016669135687692e-05\n",
      "Iteration: 6479 Loss: 1.0157373245428196e-05\n",
      "Iteration: 6480 Loss: 1.0148063674299854e-05\n",
      "Iteration: 6481 Loss: 1.013876263578097e-05\n",
      "Iteration: 6482 Loss: 1.0129470121990893e-05\n",
      "Iteration: 6483 Loss: 1.012018612511749e-05\n",
      "Iteration: 6484 Loss: 1.0110910637412609e-05\n",
      "Iteration: 6485 Loss: 1.0101643650907148e-05\n",
      "Iteration: 6486 Loss: 1.009238515797957e-05\n",
      "Iteration: 6487 Loss: 1.0083135150785294e-05\n",
      "Iteration: 6488 Loss: 1.0073893621551756e-05\n",
      "Iteration: 6489 Loss: 1.0064660562563054e-05\n",
      "Iteration: 6490 Loss: 1.0055435965882128e-05\n",
      "Iteration: 6491 Loss: 1.0046219823928422e-05\n",
      "Iteration: 6492 Loss: 1.0037012128895702e-05\n",
      "Iteration: 6493 Loss: 1.002781287303996e-05\n",
      "Iteration: 6494 Loss: 1.001862204862709e-05\n",
      "Iteration: 6495 Loss: 1.0009439647929174e-05\n",
      "Iteration: 6496 Loss: 1.0000265663226346e-05\n",
      "Iteration: 6497 Loss: 9.991100086803016e-06\n",
      "Iteration: 6498 Loss: 9.981942910956115e-06\n",
      "Iteration: 6499 Loss: 9.972794128040357e-06\n",
      "Iteration: 6500 Loss: 9.963653730193443e-06\n",
      "Iteration: 6501 Loss: 9.954521709901793e-06\n",
      "Iteration: 6502 Loss: 9.945398059429811e-06\n",
      "Iteration: 6503 Loss: 9.936282771103981e-06\n",
      "Iteration: 6504 Loss: 9.927175837261418e-06\n",
      "Iteration: 6505 Loss: 9.918077250303745e-06\n",
      "Iteration: 6506 Loss: 9.908987002464899e-06\n",
      "Iteration: 6507 Loss: 9.89990508610203e-06\n",
      "Iteration: 6508 Loss: 9.89083149369398e-06\n",
      "Iteration: 6509 Loss: 9.881766217555528e-06\n",
      "Iteration: 6510 Loss: 9.872709250061759e-06\n",
      "Iteration: 6511 Loss: 9.863660583598773e-06\n",
      "Iteration: 6512 Loss: 9.85462021055597e-06\n",
      "Iteration: 6513 Loss: 9.845588123251671e-06\n",
      "Iteration: 6514 Loss: 9.83656431425949e-06\n",
      "Iteration: 6515 Loss: 9.82754877590632e-06\n",
      "Iteration: 6516 Loss: 9.818541500612953e-06\n",
      "Iteration: 6517 Loss: 9.809542480804263e-06\n",
      "Iteration: 6518 Loss: 9.800551709001353e-06\n",
      "Iteration: 6519 Loss: 9.791569177323964e-06\n",
      "Iteration: 6520 Loss: 9.782594878751463e-06\n",
      "Iteration: 6521 Loss: 9.773628805290761e-06\n",
      "Iteration: 6522 Loss: 9.764670949556558e-06\n",
      "Iteration: 6523 Loss: 9.755721304010714e-06\n",
      "Iteration: 6524 Loss: 9.746779861132052e-06\n",
      "Iteration: 6525 Loss: 9.737846613458553e-06\n",
      "Iteration: 6526 Loss: 9.728921553364764e-06\n",
      "Iteration: 6527 Loss: 9.720004673404293e-06\n",
      "Iteration: 6528 Loss: 9.711095966024961e-06\n",
      "Iteration: 6529 Loss: 9.702195423899847e-06\n",
      "Iteration: 6530 Loss: 9.69330303943692e-06\n",
      "Iteration: 6531 Loss: 9.684418805046187e-06\n",
      "Iteration: 6532 Loss: 9.675542713427444e-06\n",
      "Iteration: 6533 Loss: 9.66667475705789e-06\n",
      "Iteration: 6534 Loss: 9.657814928487694e-06\n",
      "Iteration: 6535 Loss: 9.648963220259411e-06\n",
      "Iteration: 6536 Loss: 9.640119624877648e-06\n",
      "Iteration: 6537 Loss: 9.631284134874727e-06\n",
      "Iteration: 6538 Loss: 9.622456743056539e-06\n",
      "Iteration: 6539 Loss: 9.613637441855931e-06\n",
      "Iteration: 6540 Loss: 9.604826223855127e-06\n",
      "Iteration: 6541 Loss: 9.59602308164784e-06\n",
      "Iteration: 6542 Loss: 9.587228007830293e-06\n",
      "Iteration: 6543 Loss: 9.578440995010097e-06\n",
      "Iteration: 6544 Loss: 9.56966203579672e-06\n",
      "Iteration: 6545 Loss: 9.56089112286637e-06\n",
      "Iteration: 6546 Loss: 9.552128248674145e-06\n",
      "Iteration: 6547 Loss: 9.543373406023905e-06\n",
      "Iteration: 6548 Loss: 9.534626587408702e-06\n",
      "Iteration: 6549 Loss: 9.525887785787956e-06\n",
      "Iteration: 6550 Loss: 9.517156993651983e-06\n",
      "Iteration: 6551 Loss: 9.508434203390759e-06\n",
      "Iteration: 6552 Loss: 9.499719407876603e-06\n",
      "Iteration: 6553 Loss: 9.491012599779341e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6554 Loss: 9.482313771720869e-06\n",
      "Iteration: 6555 Loss: 9.473622916503756e-06\n",
      "Iteration: 6556 Loss: 9.464940026762042e-06\n",
      "Iteration: 6557 Loss: 9.456265095195091e-06\n",
      "Iteration: 6558 Loss: 9.447598114510838e-06\n",
      "Iteration: 6559 Loss: 9.438939077419181e-06\n",
      "Iteration: 6560 Loss: 9.430287976583756e-06\n",
      "Iteration: 6561 Loss: 9.421644804902946e-06\n",
      "Iteration: 6562 Loss: 9.413009554879951e-06\n",
      "Iteration: 6563 Loss: 9.404382219424619e-06\n",
      "Iteration: 6564 Loss: 9.395762791227378e-06\n",
      "Iteration: 6565 Loss: 9.38715126289331e-06\n",
      "Iteration: 6566 Loss: 9.378547627534514e-06\n",
      "Iteration: 6567 Loss: 9.369951877711159e-06\n",
      "Iteration: 6568 Loss: 9.361364006195165e-06\n",
      "Iteration: 6569 Loss: 9.352784005653134e-06\n",
      "Iteration: 6570 Loss: 9.344211869041368e-06\n",
      "Iteration: 6571 Loss: 9.335647589095529e-06\n",
      "Iteration: 6572 Loss: 9.327091158707225e-06\n",
      "Iteration: 6573 Loss: 9.318542570440366e-06\n",
      "Iteration: 6574 Loss: 9.310001817313099e-06\n",
      "Iteration: 6575 Loss: 9.30146889208911e-06\n",
      "Iteration: 6576 Loss: 9.292943787591712e-06\n",
      "Iteration: 6577 Loss: 9.284426496654069e-06\n",
      "Iteration: 6578 Loss: 9.275917011966179e-06\n",
      "Iteration: 6579 Loss: 9.267415326817954e-06\n",
      "Iteration: 6580 Loss: 9.25892143361702e-06\n",
      "Iteration: 6581 Loss: 9.250435325283692e-06\n",
      "Iteration: 6582 Loss: 9.241956994705486e-06\n",
      "Iteration: 6583 Loss: 9.23348643496454e-06\n",
      "Iteration: 6584 Loss: 9.225023638789977e-06\n",
      "Iteration: 6585 Loss: 9.216568599068641e-06\n",
      "Iteration: 6586 Loss: 9.208121308688596e-06\n",
      "Iteration: 6587 Loss: 9.199681760689659e-06\n",
      "Iteration: 6588 Loss: 9.191249947784628e-06\n",
      "Iteration: 6589 Loss: 9.182825862900163e-06\n",
      "Iteration: 6590 Loss: 9.174409498930356e-06\n",
      "Iteration: 6591 Loss: 9.16600084885562e-06\n",
      "Iteration: 6592 Loss: 9.157599905605503e-06\n",
      "Iteration: 6593 Loss: 9.149206662115584e-06\n",
      "Iteration: 6594 Loss: 9.140821111332262e-06\n",
      "Iteration: 6595 Loss: 9.13244324620036e-06\n",
      "Iteration: 6596 Loss: 9.124073059679982e-06\n",
      "Iteration: 6597 Loss: 9.11571054467206e-06\n",
      "Iteration: 6598 Loss: 9.107355694318181e-06\n",
      "Iteration: 6599 Loss: 9.099008501367787e-06\n",
      "Iteration: 6600 Loss: 9.090668958969988e-06\n",
      "Iteration: 6601 Loss: 9.082337060057373e-06\n",
      "Iteration: 6602 Loss: 9.074012797624986e-06\n",
      "Iteration: 6603 Loss: 9.065696164730906e-06\n",
      "Iteration: 6604 Loss: 9.057387154209507e-06\n",
      "Iteration: 6605 Loss: 9.049085759303799e-06\n",
      "Iteration: 6606 Loss: 9.040791972920807e-06\n",
      "Iteration: 6607 Loss: 9.032505788027439e-06\n",
      "Iteration: 6608 Loss: 9.024227197659692e-06\n",
      "Iteration: 6609 Loss: 9.0159561950257e-06\n",
      "Iteration: 6610 Loss: 9.007692773000033e-06\n",
      "Iteration: 6611 Loss: 8.999436924694277e-06\n",
      "Iteration: 6612 Loss: 8.991188643163432e-06\n",
      "Iteration: 6613 Loss: 8.982947921475009e-06\n",
      "Iteration: 6614 Loss: 8.9747147526426e-06\n",
      "Iteration: 6615 Loss: 8.96648913000396e-06\n",
      "Iteration: 6616 Loss: 8.958271046235896e-06\n",
      "Iteration: 6617 Loss: 8.950060494686526e-06\n",
      "Iteration: 6618 Loss: 8.941857468398444e-06\n",
      "Iteration: 6619 Loss: 8.933661960327736e-06\n",
      "Iteration: 6620 Loss: 8.925473963728003e-06\n",
      "Iteration: 6621 Loss: 8.917293472012418e-06\n",
      "Iteration: 6622 Loss: 8.909120477858854e-06\n",
      "Iteration: 6623 Loss: 8.900954974541114e-06\n",
      "Iteration: 6624 Loss: 8.892796955198128e-06\n",
      "Iteration: 6625 Loss: 8.884646412966445e-06\n",
      "Iteration: 6626 Loss: 8.8765033409961e-06\n",
      "Iteration: 6627 Loss: 8.868367732439011e-06\n",
      "Iteration: 6628 Loss: 8.860239580452719e-06\n",
      "Iteration: 6629 Loss: 8.852118878204934e-06\n",
      "Iteration: 6630 Loss: 8.844005618869008e-06\n",
      "Iteration: 6631 Loss: 8.83589979562222e-06\n",
      "Iteration: 6632 Loss: 8.827801401703154e-06\n",
      "Iteration: 6633 Loss: 8.819710430194175e-06\n",
      "Iteration: 6634 Loss: 8.811626874287945e-06\n",
      "Iteration: 6635 Loss: 8.803550727358915e-06\n",
      "Iteration: 6636 Loss: 8.795481982392968e-06\n",
      "Iteration: 6637 Loss: 8.78742063277071e-06\n",
      "Iteration: 6638 Loss: 8.779366671661812e-06\n",
      "Iteration: 6639 Loss: 8.77132009235015e-06\n",
      "Iteration: 6640 Loss: 8.763280887954706e-06\n",
      "Iteration: 6641 Loss: 8.755249051717488e-06\n",
      "Iteration: 6642 Loss: 8.747224577055282e-06\n",
      "Iteration: 6643 Loss: 8.739207457108213e-06\n",
      "Iteration: 6644 Loss: 8.731197685076887e-06\n",
      "Iteration: 6645 Loss: 8.72319525428571e-06\n",
      "Iteration: 6646 Loss: 8.715200158004803e-06\n",
      "Iteration: 6647 Loss: 8.707212389455634e-06\n",
      "Iteration: 6648 Loss: 8.699231941976851e-06\n",
      "Iteration: 6649 Loss: 8.691258808918951e-06\n",
      "Iteration: 6650 Loss: 8.683292983369165e-06\n",
      "Iteration: 6651 Loss: 8.675334459013972e-06\n",
      "Iteration: 6652 Loss: 8.667383228842869e-06\n",
      "Iteration: 6653 Loss: 8.659439286253938e-06\n",
      "Iteration: 6654 Loss: 8.651502624568845e-06\n",
      "Iteration: 6655 Loss: 8.643573237263618e-06\n",
      "Iteration: 6656 Loss: 8.635651117226712e-06\n",
      "Iteration: 6657 Loss: 8.627736258386984e-06\n",
      "Iteration: 6658 Loss: 8.619828653647373e-06\n",
      "Iteration: 6659 Loss: 8.611928296506114e-06\n",
      "Iteration: 6660 Loss: 8.604035180379749e-06\n",
      "Iteration: 6661 Loss: 8.596149298458689e-06\n",
      "Iteration: 6662 Loss: 8.588270644137076e-06\n",
      "Iteration: 6663 Loss: 8.580399211029267e-06\n",
      "Iteration: 6664 Loss: 8.572534992420421e-06\n",
      "Iteration: 6665 Loss: 8.564677981794925e-06\n",
      "Iteration: 6666 Loss: 8.556828172341725e-06\n",
      "Iteration: 6667 Loss: 8.548985557284223e-06\n",
      "Iteration: 6668 Loss: 8.541150130196808e-06\n",
      "Iteration: 6669 Loss: 8.533321884645492e-06\n",
      "Iteration: 6670 Loss: 8.525500814014805e-06\n",
      "Iteration: 6671 Loss: 8.517686911671917e-06\n",
      "Iteration: 6672 Loss: 8.509880171045622e-06\n",
      "Iteration: 6673 Loss: 8.502080585573138e-06\n",
      "Iteration: 6674 Loss: 8.494288148639903e-06\n",
      "Iteration: 6675 Loss: 8.486502853807081e-06\n",
      "Iteration: 6676 Loss: 8.478724694471295e-06\n",
      "Iteration: 6677 Loss: 8.470953664151263e-06\n",
      "Iteration: 6678 Loss: 8.463189756254728e-06\n",
      "Iteration: 6679 Loss: 8.455432964140388e-06\n",
      "Iteration: 6680 Loss: 8.447683281399711e-06\n",
      "Iteration: 6681 Loss: 8.439940701573883e-06\n",
      "Iteration: 6682 Loss: 8.432205217948905e-06\n",
      "Iteration: 6683 Loss: 8.424476824314956e-06\n",
      "Iteration: 6684 Loss: 8.41675551402626e-06\n",
      "Iteration: 6685 Loss: 8.409041280592096e-06\n",
      "Iteration: 6686 Loss: 8.401334117580963e-06\n",
      "Iteration: 6687 Loss: 8.393634018401549e-06\n",
      "Iteration: 6688 Loss: 8.38594097669037e-06\n",
      "Iteration: 6689 Loss: 8.378254985811339e-06\n",
      "Iteration: 6690 Loss: 8.370576039471209e-06\n",
      "Iteration: 6691 Loss: 8.362904131099484e-06\n",
      "Iteration: 6692 Loss: 8.35523925435958e-06\n",
      "Iteration: 6693 Loss: 8.347581402807134e-06\n",
      "Iteration: 6694 Loss: 8.339930570093773e-06\n",
      "Iteration: 6695 Loss: 8.332286749432463e-06\n",
      "Iteration: 6696 Loss: 8.324649934605218e-06\n",
      "Iteration: 6697 Loss: 8.317020119187719e-06\n",
      "Iteration: 6698 Loss: 8.309397296764743e-06\n",
      "Iteration: 6699 Loss: 8.301781460872357e-06\n",
      "Iteration: 6700 Loss: 8.294172605070132e-06\n",
      "Iteration: 6701 Loss: 8.286570723203423e-06\n",
      "Iteration: 6702 Loss: 8.278975808784632e-06\n",
      "Iteration: 6703 Loss: 8.271387855260955e-06\n",
      "Iteration: 6704 Loss: 8.263806856363625e-06\n",
      "Iteration: 6705 Loss: 8.25623280577798e-06\n",
      "Iteration: 6706 Loss: 8.248665697074059e-06\n",
      "Iteration: 6707 Loss: 8.241105524042283e-06\n",
      "Iteration: 6708 Loss: 8.233552280027707e-06\n",
      "Iteration: 6709 Loss: 8.22600595882853e-06\n",
      "Iteration: 6710 Loss: 8.218466554157277e-06\n",
      "Iteration: 6711 Loss: 8.210934059501599e-06\n",
      "Iteration: 6712 Loss: 8.203408468756826e-06\n",
      "Iteration: 6713 Loss: 8.195889775482169e-06\n",
      "Iteration: 6714 Loss: 8.188377973298571e-06\n",
      "Iteration: 6715 Loss: 8.18087305594842e-06\n",
      "Iteration: 6716 Loss: 8.173375017118975e-06\n",
      "Iteration: 6717 Loss: 8.165883850507784e-06\n",
      "Iteration: 6718 Loss: 8.1583995498148e-06\n",
      "Iteration: 6719 Loss: 8.150922108690257e-06\n",
      "Iteration: 6720 Loss: 8.143451521019122e-06\n",
      "Iteration: 6721 Loss: 8.135987780348409e-06\n",
      "Iteration: 6722 Loss: 8.128530880459124e-06\n",
      "Iteration: 6723 Loss: 8.121080815083257e-06\n",
      "Iteration: 6724 Loss: 8.11363757789782e-06\n",
      "Iteration: 6725 Loss: 8.106201162703247e-06\n",
      "Iteration: 6726 Loss: 8.098771563300424e-06\n",
      "Iteration: 6727 Loss: 8.091348773388658e-06\n",
      "Iteration: 6728 Loss: 8.083932786726612e-06\n",
      "Iteration: 6729 Loss: 8.07652359713591e-06\n",
      "Iteration: 6730 Loss: 8.069121198214417e-06\n",
      "Iteration: 6731 Loss: 8.061725583910962e-06\n",
      "Iteration: 6732 Loss: 8.054336747948233e-06\n",
      "Iteration: 6733 Loss: 8.046954684114681e-06\n",
      "Iteration: 6734 Loss: 8.03957938620317e-06\n",
      "Iteration: 6735 Loss: 8.032210848013726e-06\n",
      "Iteration: 6736 Loss: 8.02484906334977e-06\n",
      "Iteration: 6737 Loss: 8.017494026021809e-06\n",
      "Iteration: 6738 Loss: 8.010145729843116e-06\n",
      "Iteration: 6739 Loss: 8.00280416863874e-06\n",
      "Iteration: 6740 Loss: 7.995469336234977e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6741 Loss: 7.988141226611008e-06\n",
      "Iteration: 6742 Loss: 7.980819833163948e-06\n",
      "Iteration: 6743 Loss: 7.97350515031999e-06\n",
      "Iteration: 6744 Loss: 7.96619717150274e-06\n",
      "Iteration: 6745 Loss: 7.958895890563482e-06\n",
      "Iteration: 6746 Loss: 7.951601301647245e-06\n",
      "Iteration: 6747 Loss: 7.944313398479972e-06\n",
      "Iteration: 6748 Loss: 7.937032174932817e-06\n",
      "Iteration: 6749 Loss: 7.92975762494092e-06\n",
      "Iteration: 6750 Loss: 7.922489742331105e-06\n",
      "Iteration: 6751 Loss: 7.915228520936045e-06\n",
      "Iteration: 6752 Loss: 7.907973954706548e-06\n",
      "Iteration: 6753 Loss: 7.900726037543648e-06\n",
      "Iteration: 6754 Loss: 7.893484763353549e-06\n",
      "Iteration: 6755 Loss: 7.886250126045575e-06\n",
      "Iteration: 6756 Loss: 7.879022119537285e-06\n",
      "Iteration: 6757 Loss: 7.87180073775563e-06\n",
      "Iteration: 6758 Loss: 7.8645859746232e-06\n",
      "Iteration: 6759 Loss: 7.857377824076561e-06\n",
      "Iteration: 6760 Loss: 7.85017627999655e-06\n",
      "Iteration: 6761 Loss: 7.842981336388141e-06\n",
      "Iteration: 6762 Loss: 7.835792987311409e-06\n",
      "Iteration: 6763 Loss: 7.828611226555108e-06\n",
      "Iteration: 6764 Loss: 7.82143604807874e-06\n",
      "Iteration: 6765 Loss: 7.814267445963274e-06\n",
      "Iteration: 6766 Loss: 7.807105414125977e-06\n",
      "Iteration: 6767 Loss: 7.799949946543742e-06\n",
      "Iteration: 6768 Loss: 7.792801037255418e-06\n",
      "Iteration: 6769 Loss: 7.785658680197424e-06\n",
      "Iteration: 6770 Loss: 7.778522869359626e-06\n",
      "Iteration: 6771 Loss: 7.771393598690853e-06\n",
      "Iteration: 6772 Loss: 7.764270862304617e-06\n",
      "Iteration: 6773 Loss: 7.757154654103038e-06\n",
      "Iteration: 6774 Loss: 7.750044968155888e-06\n",
      "Iteration: 6775 Loss: 7.742941798487807e-06\n",
      "Iteration: 6776 Loss: 7.735845139011928e-06\n",
      "Iteration: 6777 Loss: 7.728754983930938e-06\n",
      "Iteration: 6778 Loss: 7.721671327229424e-06\n",
      "Iteration: 6779 Loss: 7.714594163003768e-06\n",
      "Iteration: 6780 Loss: 7.707523485251613e-06\n",
      "Iteration: 6781 Loss: 7.700459288024715e-06\n",
      "Iteration: 6782 Loss: 7.693401565383485e-06\n",
      "Iteration: 6783 Loss: 7.686350311394956e-06\n",
      "Iteration: 6784 Loss: 7.679305520128667e-06\n",
      "Iteration: 6785 Loss: 7.672267185663118e-06\n",
      "Iteration: 6786 Loss: 7.665235302080662e-06\n",
      "Iteration: 6787 Loss: 7.658209863524045e-06\n",
      "Iteration: 6788 Loss: 7.651190863915223e-06\n",
      "Iteration: 6789 Loss: 7.644178297582058e-06\n",
      "Iteration: 6790 Loss: 7.637172158513473e-06\n",
      "Iteration: 6791 Loss: 7.630172440764397e-06\n",
      "Iteration: 6792 Loss: 7.62317913844279e-06\n",
      "Iteration: 6793 Loss: 7.616192245846546e-06\n",
      "Iteration: 6794 Loss: 7.6092117569263955e-06\n",
      "Iteration: 6795 Loss: 7.602237665871192e-06\n",
      "Iteration: 6796 Loss: 7.5952699668180446e-06\n",
      "Iteration: 6797 Loss: 7.5883086539048165e-06\n",
      "Iteration: 6798 Loss: 7.581353721281893e-06\n",
      "Iteration: 6799 Loss: 7.574405163043423e-06\n",
      "Iteration: 6800 Loss: 7.56746297351722e-06\n",
      "Iteration: 6801 Loss: 7.560527146697358e-06\n",
      "Iteration: 6802 Loss: 7.553597676749336e-06\n",
      "Iteration: 6803 Loss: 7.546674557905193e-06\n",
      "Iteration: 6804 Loss: 7.539757784402674e-06\n",
      "Iteration: 6805 Loss: 7.5328473503666375e-06\n",
      "Iteration: 6806 Loss: 7.52594325004442e-06\n",
      "Iteration: 6807 Loss: 7.519045477571798e-06\n",
      "Iteration: 6808 Loss: 7.512154027097276e-06\n",
      "Iteration: 6809 Loss: 7.505268892880211e-06\n",
      "Iteration: 6810 Loss: 7.498390069131411e-06\n",
      "Iteration: 6811 Loss: 7.49151755000992e-06\n",
      "Iteration: 6812 Loss: 7.484651329852195e-06\n",
      "Iteration: 6813 Loss: 7.4777914027722956e-06\n",
      "Iteration: 6814 Loss: 7.470937763170715e-06\n",
      "Iteration: 6815 Loss: 7.464090405056242e-06\n",
      "Iteration: 6816 Loss: 7.457249322905195e-06\n",
      "Iteration: 6817 Loss: 7.45041451084466e-06\n",
      "Iteration: 6818 Loss: 7.443585963074715e-06\n",
      "Iteration: 6819 Loss: 7.436763673909353e-06\n",
      "Iteration: 6820 Loss: 7.429947637613097e-06\n",
      "Iteration: 6821 Loss: 7.423137848455485e-06\n",
      "Iteration: 6822 Loss: 7.416334300710593e-06\n",
      "Iteration: 6823 Loss: 7.409536988655458e-06\n",
      "Iteration: 6824 Loss: 7.402745906580468e-06\n",
      "Iteration: 6825 Loss: 7.395961048713075e-06\n",
      "Iteration: 6826 Loss: 7.389182409521039e-06\n",
      "Iteration: 6827 Loss: 7.382409983135723e-06\n",
      "Iteration: 6828 Loss: 7.3756437639158745e-06\n",
      "Iteration: 6829 Loss: 7.368883746120729e-06\n",
      "Iteration: 6830 Loss: 7.3621299242322614e-06\n",
      "Iteration: 6831 Loss: 7.355382292404998e-06\n",
      "Iteration: 6832 Loss: 7.34864084502069e-06\n",
      "Iteration: 6833 Loss: 7.341905576410646e-06\n",
      "Iteration: 6834 Loss: 7.3351764809125585e-06\n",
      "Iteration: 6835 Loss: 7.328453552867671e-06\n",
      "Iteration: 6836 Loss: 7.321736786566698e-06\n",
      "Iteration: 6837 Loss: 7.315026176475168e-06\n",
      "Iteration: 6838 Loss: 7.308321716811882e-06\n",
      "Iteration: 6839 Loss: 7.301623402191159e-06\n",
      "Iteration: 6840 Loss: 7.294931226784798e-06\n",
      "Iteration: 6841 Loss: 7.288245184937206e-06\n",
      "Iteration: 6842 Loss: 7.2815652710821256e-06\n",
      "Iteration: 6843 Loss: 7.274891479605129e-06\n",
      "Iteration: 6844 Loss: 7.268223804895018e-06\n",
      "Iteration: 6845 Loss: 7.2615622413417285e-06\n",
      "Iteration: 6846 Loss: 7.254906783290229e-06\n",
      "Iteration: 6847 Loss: 7.248257425202319e-06\n",
      "Iteration: 6848 Loss: 7.241614161600174e-06\n",
      "Iteration: 6849 Loss: 7.234976986725121e-06\n",
      "Iteration: 6850 Loss: 7.2283458951125224e-06\n",
      "Iteration: 6851 Loss: 7.221720881072755e-06\n",
      "Iteration: 6852 Loss: 7.215101939093777e-06\n",
      "Iteration: 6853 Loss: 7.208489063607513e-06\n",
      "Iteration: 6854 Loss: 7.201882249054772e-06\n",
      "Iteration: 6855 Loss: 7.195281489882996e-06\n",
      "Iteration: 6856 Loss: 7.188686780481381e-06\n",
      "Iteration: 6857 Loss: 7.1820981154787924e-06\n",
      "Iteration: 6858 Loss: 7.175515489163809e-06\n",
      "Iteration: 6859 Loss: 7.168938896056786e-06\n",
      "Iteration: 6860 Loss: 7.162368330629677e-06\n",
      "Iteration: 6861 Loss: 7.155803787302128e-06\n",
      "Iteration: 6862 Loss: 7.149245260610393e-06\n",
      "Iteration: 6863 Loss: 7.142692745096982e-06\n",
      "Iteration: 6864 Loss: 7.136146235195603e-06\n",
      "Iteration: 6865 Loss: 7.129605725401143e-06\n",
      "Iteration: 6866 Loss: 7.123071210214819e-06\n",
      "Iteration: 6867 Loss: 7.116542684200059e-06\n",
      "Iteration: 6868 Loss: 7.110020141809103e-06\n",
      "Iteration: 6869 Loss: 7.10350357750169e-06\n",
      "Iteration: 6870 Loss: 7.096992985857391e-06\n",
      "Iteration: 6871 Loss: 7.090488361400107e-06\n",
      "Iteration: 6872 Loss: 7.0839896986019626e-06\n",
      "Iteration: 6873 Loss: 7.07749699211496e-06\n",
      "Iteration: 6874 Loss: 7.071010236368564e-06\n",
      "Iteration: 6875 Loss: 7.064529426075134e-06\n",
      "Iteration: 6876 Loss: 7.058054555558756e-06\n",
      "Iteration: 6877 Loss: 7.0515856195472375e-06\n",
      "Iteration: 6878 Loss: 7.045122612601111e-06\n",
      "Iteration: 6879 Loss: 7.0386655291719314e-06\n",
      "Iteration: 6880 Loss: 7.032214363944985e-06\n",
      "Iteration: 6881 Loss: 7.025769111382309e-06\n",
      "Iteration: 6882 Loss: 7.019329766121421e-06\n",
      "Iteration: 6883 Loss: 7.012896322747236e-06\n",
      "Iteration: 6884 Loss: 7.006468775852067e-06\n",
      "Iteration: 6885 Loss: 7.000047120029847e-06\n",
      "Iteration: 6886 Loss: 6.993631349881732e-06\n",
      "Iteration: 6887 Loss: 6.987221460014897e-06\n",
      "Iteration: 6888 Loss: 6.9808174448908266e-06\n",
      "Iteration: 6889 Loss: 6.97441929941829e-06\n",
      "Iteration: 6890 Loss: 6.968027018074562e-06\n",
      "Iteration: 6891 Loss: 6.961640595481184e-06\n",
      "Iteration: 6892 Loss: 6.955260026270356e-06\n",
      "Iteration: 6893 Loss: 6.948885305075893e-06\n",
      "Iteration: 6894 Loss: 6.94251642668676e-06\n",
      "Iteration: 6895 Loss: 6.9361533853929635e-06\n",
      "Iteration: 6896 Loss: 6.929796176054266e-06\n",
      "Iteration: 6897 Loss: 6.923444793435242e-06\n",
      "Iteration: 6898 Loss: 6.9170992319676425e-06\n",
      "Iteration: 6899 Loss: 6.910759486491364e-06\n",
      "Iteration: 6900 Loss: 6.9044255516133615e-06\n",
      "Iteration: 6901 Loss: 6.898097422010769e-06\n",
      "Iteration: 6902 Loss: 6.891775092361985e-06\n",
      "Iteration: 6903 Loss: 6.885458557205816e-06\n",
      "Iteration: 6904 Loss: 6.879147811522905e-06\n",
      "Iteration: 6905 Loss: 6.8728428498624925e-06\n",
      "Iteration: 6906 Loss: 6.866543666977994e-06\n",
      "Iteration: 6907 Loss: 6.860250257404482e-06\n",
      "Iteration: 6908 Loss: 6.853962616019904e-06\n",
      "Iteration: 6909 Loss: 6.84768073768318e-06\n",
      "Iteration: 6910 Loss: 6.841404616764074e-06\n",
      "Iteration: 6911 Loss: 6.83513424807487e-06\n",
      "Iteration: 6912 Loss: 6.828869626398564e-06\n",
      "Iteration: 6913 Loss: 6.822610746414496e-06\n",
      "Iteration: 6914 Loss: 6.816357603029787e-06\n",
      "Iteration: 6915 Loss: 6.8101101908124945e-06\n",
      "Iteration: 6916 Loss: 6.803868504569014e-06\n",
      "Iteration: 6917 Loss: 6.7976325390527e-06\n",
      "Iteration: 6918 Loss: 6.791402289019768e-06\n",
      "Iteration: 6919 Loss: 6.7851777492293735e-06\n",
      "Iteration: 6920 Loss: 6.778958914449767e-06\n",
      "Iteration: 6921 Loss: 6.772745779452776e-06\n",
      "Iteration: 6922 Loss: 6.766538338955843e-06\n",
      "Iteration: 6923 Loss: 6.760336587798767e-06\n",
      "Iteration: 6924 Loss: 6.754140520820179e-06\n",
      "Iteration: 6925 Loss: 6.747950132671942e-06\n",
      "Iteration: 6926 Loss: 6.741765418319318e-06\n",
      "Iteration: 6927 Loss: 6.735586372475248e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6928 Loss: 6.729412989944201e-06\n",
      "Iteration: 6929 Loss: 6.723245265620159e-06\n",
      "Iteration: 6930 Loss: 6.717083194150091e-06\n",
      "Iteration: 6931 Loss: 6.710926770435605e-06\n",
      "Iteration: 6932 Loss: 6.704775989300365e-06\n",
      "Iteration: 6933 Loss: 6.69863084563218e-06\n",
      "Iteration: 6934 Loss: 6.692491334203924e-06\n",
      "Iteration: 6935 Loss: 6.686357449797705e-06\n",
      "Iteration: 6936 Loss: 6.680229187314113e-06\n",
      "Iteration: 6937 Loss: 6.674106541543168e-06\n",
      "Iteration: 6938 Loss: 6.667989507392034e-06\n",
      "Iteration: 6939 Loss: 6.661878079775362e-06\n",
      "Iteration: 6940 Loss: 6.65577225349906e-06\n",
      "Iteration: 6941 Loss: 6.649672023485076e-06\n",
      "Iteration: 6942 Loss: 6.643577384547081e-06\n",
      "Iteration: 6943 Loss: 6.637488331503972e-06\n",
      "Iteration: 6944 Loss: 6.631404859291676e-06\n",
      "Iteration: 6945 Loss: 6.625326962799943e-06\n",
      "Iteration: 6946 Loss: 6.619254636911501e-06\n",
      "Iteration: 6947 Loss: 6.613187876526888e-06\n",
      "Iteration: 6948 Loss: 6.6071266764838485e-06\n",
      "Iteration: 6949 Loss: 6.6010710317446985e-06\n",
      "Iteration: 6950 Loss: 6.5950209373604655e-06\n",
      "Iteration: 6951 Loss: 6.588976388016856e-06\n",
      "Iteration: 6952 Loss: 6.5829373787143604e-06\n",
      "Iteration: 6953 Loss: 6.576903904380119e-06\n",
      "Iteration: 6954 Loss: 6.5708759599350665e-06\n",
      "Iteration: 6955 Loss: 6.564853540373075e-06\n",
      "Iteration: 6956 Loss: 6.558836640457901e-06\n",
      "Iteration: 6957 Loss: 6.552825255215392e-06\n",
      "Iteration: 6958 Loss: 6.546819379677242e-06\n",
      "Iteration: 6959 Loss: 6.540819008903643e-06\n",
      "Iteration: 6960 Loss: 6.534824137542704e-06\n",
      "Iteration: 6961 Loss: 6.528834760608002e-06\n",
      "Iteration: 6962 Loss: 6.522850873231522e-06\n",
      "Iteration: 6963 Loss: 6.516872470301586e-06\n",
      "Iteration: 6964 Loss: 6.510899546731487e-06\n",
      "Iteration: 6965 Loss: 6.504932097669709e-06\n",
      "Iteration: 6966 Loss: 6.498970118012998e-06\n",
      "Iteration: 6967 Loss: 6.493013602638089e-06\n",
      "Iteration: 6968 Loss: 6.487062546618976e-06\n",
      "Iteration: 6969 Loss: 6.481116944954221e-06\n",
      "Iteration: 6970 Loss: 6.475176792642532e-06\n",
      "Iteration: 6971 Loss: 6.4692420846910065e-06\n",
      "Iteration: 6972 Loss: 6.46331281605193e-06\n",
      "Iteration: 6973 Loss: 6.457388981852125e-06\n",
      "Iteration: 6974 Loss: 6.451470577058107e-06\n",
      "Iteration: 6975 Loss: 6.445557596746255e-06\n",
      "Iteration: 6976 Loss: 6.439650035834269e-06\n",
      "Iteration: 6977 Loss: 6.433747889410171e-06\n",
      "Iteration: 6978 Loss: 6.4278511525089064e-06\n",
      "Iteration: 6979 Loss: 6.421959820178982e-06\n",
      "Iteration: 6980 Loss: 6.41607388746117e-06\n",
      "Iteration: 6981 Loss: 6.410193349409713e-06\n",
      "Iteration: 6982 Loss: 6.404318201078379e-06\n",
      "Iteration: 6983 Loss: 6.398448437529064e-06\n",
      "Iteration: 6984 Loss: 6.3925840537683835e-06\n",
      "Iteration: 6985 Loss: 6.386725044978784e-06\n",
      "Iteration: 6986 Loss: 6.38087140623516e-06\n",
      "Iteration: 6987 Loss: 6.37502313241635e-06\n",
      "Iteration: 6988 Loss: 6.369180218830918e-06\n",
      "Iteration: 6989 Loss: 6.3633426604834854e-06\n",
      "Iteration: 6990 Loss: 6.357510452407287e-06\n",
      "Iteration: 6991 Loss: 6.351683589839778e-06\n",
      "Iteration: 6992 Loss: 6.345862067769239e-06\n",
      "Iteration: 6993 Loss: 6.340045881330932e-06\n",
      "Iteration: 6994 Loss: 6.334235025688421e-06\n",
      "Iteration: 6995 Loss: 6.328429495902167e-06\n",
      "Iteration: 6996 Loss: 6.3226292870313905e-06\n",
      "Iteration: 6997 Loss: 6.316834394257269e-06\n",
      "Iteration: 6998 Loss: 6.311044812706659e-06\n",
      "Iteration: 6999 Loss: 6.30526053745534e-06\n",
      "Iteration: 7000 Loss: 6.299481563695526e-06\n",
      "Iteration: 7001 Loss: 6.293707886625918e-06\n",
      "Iteration: 7002 Loss: 6.287939501336692e-06\n",
      "Iteration: 7003 Loss: 6.282176402976128e-06\n",
      "Iteration: 7004 Loss: 6.2764185866997416e-06\n",
      "Iteration: 7005 Loss: 6.270666047662749e-06\n",
      "Iteration: 7006 Loss: 6.264918781090549e-06\n",
      "Iteration: 7007 Loss: 6.2591767820329614e-06\n",
      "Iteration: 7008 Loss: 6.253440045663829e-06\n",
      "Iteration: 7009 Loss: 6.2477085672732e-06\n",
      "Iteration: 7010 Loss: 6.241982341985297e-06\n",
      "Iteration: 7011 Loss: 6.236261364836575e-06\n",
      "Iteration: 7012 Loss: 6.230545631369456e-06\n",
      "Iteration: 7013 Loss: 6.224835136576183e-06\n",
      "Iteration: 7014 Loss: 6.2191298755945e-06\n",
      "Iteration: 7015 Loss: 6.213429843629251e-06\n",
      "Iteration: 7016 Loss: 6.207735035945562e-06\n",
      "Iteration: 7017 Loss: 6.202045447809136e-06\n",
      "Iteration: 7018 Loss: 6.196361074382768e-06\n",
      "Iteration: 7019 Loss: 6.190681910885426e-06\n",
      "Iteration: 7020 Loss: 6.185007952688997e-06\n",
      "Iteration: 7021 Loss: 6.17933919472825e-06\n",
      "Iteration: 7022 Loss: 6.173675632384912e-06\n",
      "Iteration: 7023 Loss: 6.168017260895758e-06\n",
      "Iteration: 7024 Loss: 6.162364075505571e-06\n",
      "Iteration: 7025 Loss: 6.156716071515008e-06\n",
      "Iteration: 7026 Loss: 6.1510732441199214e-06\n",
      "Iteration: 7027 Loss: 6.145435588518425e-06\n",
      "Iteration: 7028 Loss: 6.1398030999723315e-06\n",
      "Iteration: 7029 Loss: 6.134175773855568e-06\n",
      "Iteration: 7030 Loss: 6.128553605381615e-06\n",
      "Iteration: 7031 Loss: 6.1229365897679285e-06\n",
      "Iteration: 7032 Loss: 6.1173247224049575e-06\n",
      "Iteration: 7033 Loss: 6.111717998516128e-06\n",
      "Iteration: 7034 Loss: 6.10611641344386e-06\n",
      "Iteration: 7035 Loss: 6.100519962423696e-06\n",
      "Iteration: 7036 Loss: 6.094928640690325e-06\n",
      "Iteration: 7037 Loss: 6.089342443601636e-06\n",
      "Iteration: 7038 Loss: 6.083761366459088e-06\n",
      "Iteration: 7039 Loss: 6.078185404571768e-06\n",
      "Iteration: 7040 Loss: 6.072614553250771e-06\n",
      "Iteration: 7041 Loss: 6.06704880781067e-06\n",
      "Iteration: 7042 Loss: 6.06148816357378e-06\n",
      "Iteration: 7043 Loss: 6.0559326158629954e-06\n",
      "Iteration: 7044 Loss: 6.050382160008377e-06\n",
      "Iteration: 7045 Loss: 6.044836791489428e-06\n",
      "Iteration: 7046 Loss: 6.0392965052038015e-06\n",
      "Iteration: 7047 Loss: 6.033761296931655e-06\n",
      "Iteration: 7048 Loss: 6.028231161872842e-06\n",
      "Iteration: 7049 Loss: 6.022706095323318e-06\n",
      "Iteration: 7050 Loss: 6.017186092805121e-06\n",
      "Iteration: 7051 Loss: 6.011671149507859e-06\n",
      "Iteration: 7052 Loss: 6.006161260850455e-06\n",
      "Iteration: 7053 Loss: 6.0006564222002815e-06\n",
      "Iteration: 7054 Loss: 5.9951566289303145e-06\n",
      "Iteration: 7055 Loss: 5.989661876266637e-06\n",
      "Iteration: 7056 Loss: 5.984172160033268e-06\n",
      "Iteration: 7057 Loss: 5.97868747503012e-06\n",
      "Iteration: 7058 Loss: 5.973207817074962e-06\n",
      "Iteration: 7059 Loss: 5.967733181364928e-06\n",
      "Iteration: 7060 Loss: 5.962263563408348e-06\n",
      "Iteration: 7061 Loss: 5.956798958630751e-06\n",
      "Iteration: 7062 Loss: 5.951339362274931e-06\n",
      "Iteration: 7063 Loss: 5.945884769832478e-06\n",
      "Iteration: 7064 Loss: 5.940435176715151e-06\n",
      "Iteration: 7065 Loss: 5.9349905783437694e-06\n",
      "Iteration: 7066 Loss: 5.9295509701388225e-06\n",
      "Iteration: 7067 Loss: 5.924116347526783e-06\n",
      "Iteration: 7068 Loss: 5.9186867059376165e-06\n",
      "Iteration: 7069 Loss: 5.9132620408066466e-06\n",
      "Iteration: 7070 Loss: 5.907842347574123e-06\n",
      "Iteration: 7071 Loss: 5.902427621681823e-06\n",
      "Iteration: 7072 Loss: 5.89701785863386e-06\n",
      "Iteration: 7073 Loss: 5.89161305371068e-06\n",
      "Iteration: 7074 Loss: 5.8862132025954175e-06\n",
      "Iteration: 7075 Loss: 5.88081830063487e-06\n",
      "Iteration: 7076 Loss: 5.8754283432361745e-06\n",
      "Iteration: 7077 Loss: 5.870043325922285e-06\n",
      "Iteration: 7078 Loss: 5.864663244112272e-06\n",
      "Iteration: 7079 Loss: 5.859288093451909e-06\n",
      "Iteration: 7080 Loss: 5.8539178692463006e-06\n",
      "Iteration: 7081 Loss: 5.848552567042827e-06\n",
      "Iteration: 7082 Loss: 5.843192182328154e-06\n",
      "Iteration: 7083 Loss: 5.837836710596568e-06\n",
      "Iteration: 7084 Loss: 5.83248614734303e-06\n",
      "Iteration: 7085 Loss: 5.827140488068679e-06\n",
      "Iteration: 7086 Loss: 5.82179972828115e-06\n",
      "Iteration: 7087 Loss: 5.816463863489876e-06\n",
      "Iteration: 7088 Loss: 5.811132889121296e-06\n",
      "Iteration: 7089 Loss: 5.805806800862481e-06\n",
      "Iteration: 7090 Loss: 5.8004855941541285e-06\n",
      "Iteration: 7091 Loss: 5.795169264461044e-06\n",
      "Iteration: 7092 Loss: 5.789857807428082e-06\n",
      "Iteration: 7093 Loss: 5.784551218534954e-06\n",
      "Iteration: 7094 Loss: 5.779249493316322e-06\n",
      "Iteration: 7095 Loss: 5.7739526273736545e-06\n",
      "Iteration: 7096 Loss: 5.768660616139154e-06\n",
      "Iteration: 7097 Loss: 5.763373455219819e-06\n",
      "Iteration: 7098 Loss: 5.758091140170177e-06\n",
      "Iteration: 7099 Loss: 5.752813666550023e-06\n",
      "Iteration: 7100 Loss: 5.747541029919452e-06\n",
      "Iteration: 7101 Loss: 5.7422732258458545e-06\n",
      "Iteration: 7102 Loss: 5.737010249901006e-06\n",
      "Iteration: 7103 Loss: 5.731752097660963e-06\n",
      "Iteration: 7104 Loss: 5.72649876470232e-06\n",
      "Iteration: 7105 Loss: 5.721250246609403e-06\n",
      "Iteration: 7106 Loss: 5.716006538967155e-06\n",
      "Iteration: 7107 Loss: 5.710767637313623e-06\n",
      "Iteration: 7108 Loss: 5.70553353741008e-06\n",
      "Iteration: 7109 Loss: 5.70030423468745e-06\n",
      "Iteration: 7110 Loss: 5.69507972489182e-06\n",
      "Iteration: 7111 Loss: 5.689860003458614e-06\n",
      "Iteration: 7112 Loss: 5.6846450660844055e-06\n",
      "Iteration: 7113 Loss: 5.679434908384057e-06\n",
      "Iteration: 7114 Loss: 5.674229525978236e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7115 Loss: 5.669028914488723e-06\n",
      "Iteration: 7116 Loss: 5.663833069542955e-06\n",
      "Iteration: 7117 Loss: 5.658641986773589e-06\n",
      "Iteration: 7118 Loss: 5.653455661814828e-06\n",
      "Iteration: 7119 Loss: 5.648274090304151e-06\n",
      "Iteration: 7120 Loss: 5.643097267889268e-06\n",
      "Iteration: 7121 Loss: 5.6379251902140714e-06\n",
      "Iteration: 7122 Loss: 5.63275785293035e-06\n",
      "Iteration: 7123 Loss: 5.627595251693094e-06\n",
      "Iteration: 7124 Loss: 5.6224373821643956e-06\n",
      "Iteration: 7125 Loss: 5.617284240003579e-06\n",
      "Iteration: 7126 Loss: 5.612135820881351e-06\n",
      "Iteration: 7127 Loss: 5.606992120464275e-06\n",
      "Iteration: 7128 Loss: 5.601853134433301e-06\n",
      "Iteration: 7129 Loss: 5.596718858315454e-06\n",
      "Iteration: 7130 Loss: 5.591589288238499e-06\n",
      "Iteration: 7131 Loss: 5.586464419445626e-06\n",
      "Iteration: 7132 Loss: 5.581344247776137e-06\n",
      "Iteration: 7133 Loss: 5.576228768924705e-06\n",
      "Iteration: 7134 Loss: 5.57111797853517e-06\n",
      "Iteration: 7135 Loss: 5.566011872477729e-06\n",
      "Iteration: 7136 Loss: 5.560910446290489e-06\n",
      "Iteration: 7137 Loss: 5.5558136957398626e-06\n",
      "Iteration: 7138 Loss: 5.550721616485734e-06\n",
      "Iteration: 7139 Loss: 5.545634204301457e-06\n",
      "Iteration: 7140 Loss: 5.540551454965273e-06\n",
      "Iteration: 7141 Loss: 5.535473364150934e-06\n",
      "Iteration: 7142 Loss: 5.530399927584886e-06\n",
      "Iteration: 7143 Loss: 5.525331141001926e-06\n",
      "Iteration: 7144 Loss: 5.520267000198588e-06\n",
      "Iteration: 7145 Loss: 5.515207500802517e-06\n",
      "Iteration: 7146 Loss: 5.510152638820184e-06\n",
      "Iteration: 7147 Loss: 5.505102409593383e-06\n",
      "Iteration: 7148 Loss: 5.500056809079953e-06\n",
      "Iteration: 7149 Loss: 5.49501583289215e-06\n",
      "Iteration: 7150 Loss: 5.489979477082724e-06\n",
      "Iteration: 7151 Loss: 5.484947737270354e-06\n",
      "Iteration: 7152 Loss: 5.479920609227441e-06\n",
      "Iteration: 7153 Loss: 5.474898088723532e-06\n",
      "Iteration: 7154 Loss: 5.469880171536949e-06\n",
      "Iteration: 7155 Loss: 5.464866853450066e-06\n",
      "Iteration: 7156 Loss: 5.459858130244732e-06\n",
      "Iteration: 7157 Loss: 5.454853997713168e-06\n",
      "Iteration: 7158 Loss: 5.44985445164363e-06\n",
      "Iteration: 7159 Loss: 5.444859487836002e-06\n",
      "Iteration: 7160 Loss: 5.439869102088113e-06\n",
      "Iteration: 7161 Loss: 5.4348832901494424e-06\n",
      "Iteration: 7162 Loss: 5.429902047994531e-06\n",
      "Iteration: 7163 Loss: 5.424925371270318e-06\n",
      "Iteration: 7164 Loss: 5.419953255845908e-06\n",
      "Iteration: 7165 Loss: 5.414985697539796e-06\n",
      "Iteration: 7166 Loss: 5.4100226921760385e-06\n",
      "Iteration: 7167 Loss: 5.405064235526262e-06\n",
      "Iteration: 7168 Loss: 5.4001103234766064e-06\n",
      "Iteration: 7169 Loss: 5.395160951919728e-06\n",
      "Iteration: 7170 Loss: 5.390216116635222e-06\n",
      "Iteration: 7171 Loss: 5.3852758135237156e-06\n",
      "Iteration: 7172 Loss: 5.380340038376453e-06\n",
      "Iteration: 7173 Loss: 5.375408786984814e-06\n",
      "Iteration: 7174 Loss: 5.370482055257193e-06\n",
      "Iteration: 7175 Loss: 5.365559839054014e-06\n",
      "Iteration: 7176 Loss: 5.360642134236021e-06\n",
      "Iteration: 7177 Loss: 5.355728936668453e-06\n",
      "Iteration: 7178 Loss: 5.350820242218858e-06\n",
      "Iteration: 7179 Loss: 5.3459160467636145e-06\n",
      "Iteration: 7180 Loss: 5.341016346173734e-06\n",
      "Iteration: 7181 Loss: 5.33612113633337e-06\n",
      "Iteration: 7182 Loss: 5.331230413126855e-06\n",
      "Iteration: 7183 Loss: 5.3263441723840455e-06\n",
      "Iteration: 7184 Loss: 5.32146241016585e-06\n",
      "Iteration: 7185 Loss: 5.316585122199241e-06\n",
      "Iteration: 7186 Loss: 5.311712304438542e-06\n",
      "Iteration: 7187 Loss: 5.306843952730691e-06\n",
      "Iteration: 7188 Loss: 5.3019800630972e-06\n",
      "Iteration: 7189 Loss: 5.2971206314477844e-06\n",
      "Iteration: 7190 Loss: 5.2922656535235205e-06\n",
      "Iteration: 7191 Loss: 5.2874151253587855e-06\n",
      "Iteration: 7192 Loss: 5.2825690429316754e-06\n",
      "Iteration: 7193 Loss: 5.2777274021090805e-06\n",
      "Iteration: 7194 Loss: 5.2728901988224006e-06\n",
      "Iteration: 7195 Loss: 5.26805742900225e-06\n",
      "Iteration: 7196 Loss: 5.263229088587721e-06\n",
      "Iteration: 7197 Loss: 5.25840517351706e-06\n",
      "Iteration: 7198 Loss: 5.253585679735702e-06\n",
      "Iteration: 7199 Loss: 5.248770603189602e-06\n",
      "Iteration: 7200 Loss: 5.2439599398318894e-06\n",
      "Iteration: 7201 Loss: 5.239153685672185e-06\n",
      "Iteration: 7202 Loss: 5.234351836617158e-06\n",
      "Iteration: 7203 Loss: 5.229554388567737e-06\n",
      "Iteration: 7204 Loss: 5.224761337492204e-06\n",
      "Iteration: 7205 Loss: 5.2199726794186965e-06\n",
      "Iteration: 7206 Loss: 5.215188410374849e-06\n",
      "Iteration: 7207 Loss: 5.21040852628276e-06\n",
      "Iteration: 7208 Loss: 5.205633023122166e-06\n",
      "Iteration: 7209 Loss: 5.200861896936963e-06\n",
      "Iteration: 7210 Loss: 5.196095143658559e-06\n",
      "Iteration: 7211 Loss: 5.191332759220264e-06\n",
      "Iteration: 7212 Loss: 5.186574739731124e-06\n",
      "Iteration: 7213 Loss: 5.181821081136937e-06\n",
      "Iteration: 7214 Loss: 5.177071779380198e-06\n",
      "Iteration: 7215 Loss: 5.1723268305288515e-06\n",
      "Iteration: 7216 Loss: 5.16758623053103e-06\n",
      "Iteration: 7217 Loss: 5.162849975461775e-06\n",
      "Iteration: 7218 Loss: 5.15811806139415e-06\n",
      "Iteration: 7219 Loss: 5.1533904842932516e-06\n",
      "Iteration: 7220 Loss: 5.148667240182726e-06\n",
      "Iteration: 7221 Loss: 5.143948325093054e-06\n",
      "Iteration: 7222 Loss: 5.139233735055518e-06\n",
      "Iteration: 7223 Loss: 5.134523466050709e-06\n",
      "Iteration: 7224 Loss: 5.129817514286606e-06\n",
      "Iteration: 7225 Loss: 5.125115875579617e-06\n",
      "Iteration: 7226 Loss: 5.120418546090749e-06\n",
      "Iteration: 7227 Loss: 5.115725521926672e-06\n",
      "Iteration: 7228 Loss: 5.111036799085505e-06\n",
      "Iteration: 7229 Loss: 5.106352373624825e-06\n",
      "Iteration: 7230 Loss: 5.101672241604841e-06\n",
      "Iteration: 7231 Loss: 5.096996399147081e-06\n",
      "Iteration: 7232 Loss: 5.092324842151744e-06\n",
      "Iteration: 7233 Loss: 5.087657566861374e-06\n",
      "Iteration: 7234 Loss: 5.082994569347368e-06\n",
      "Iteration: 7235 Loss: 5.0783358455812624e-06\n",
      "Iteration: 7236 Loss: 5.07368139169924e-06\n",
      "Iteration: 7237 Loss: 5.0690312037037165e-06\n",
      "Iteration: 7238 Loss: 5.0643852779109035e-06\n",
      "Iteration: 7239 Loss: 5.059743610217899e-06\n",
      "Iteration: 7240 Loss: 5.05510619677642e-06\n",
      "Iteration: 7241 Loss: 5.050473033686466e-06\n",
      "Iteration: 7242 Loss: 5.045844117141166e-06\n",
      "Iteration: 7243 Loss: 5.04121944307588e-06\n",
      "Iteration: 7244 Loss: 5.036599007687531e-06\n",
      "Iteration: 7245 Loss: 5.031982807090738e-06\n",
      "Iteration: 7246 Loss: 5.027370837405843e-06\n",
      "Iteration: 7247 Loss: 5.022763094755237e-06\n",
      "Iteration: 7248 Loss: 5.01815957526209e-06\n",
      "Iteration: 7249 Loss: 5.0135602750569415e-06\n",
      "Iteration: 7250 Loss: 5.008965190272913e-06\n",
      "Iteration: 7251 Loss: 5.004374317047153e-06\n",
      "Iteration: 7252 Loss: 4.9997876515187045e-06\n",
      "Iteration: 7253 Loss: 4.995205189831293e-06\n",
      "Iteration: 7254 Loss: 4.990626928131978e-06\n",
      "Iteration: 7255 Loss: 4.9860528625704275e-06\n",
      "Iteration: 7256 Loss: 4.981482989302399e-06\n",
      "Iteration: 7257 Loss: 4.976917304483883e-06\n",
      "Iteration: 7258 Loss: 4.972355804277947e-06\n",
      "Iteration: 7259 Loss: 4.967798484847196e-06\n",
      "Iteration: 7260 Loss: 4.96324534230281e-06\n",
      "Iteration: 7261 Loss: 4.958696372933982e-06\n",
      "Iteration: 7262 Loss: 4.954151572796758e-06\n",
      "Iteration: 7263 Loss: 4.9496109381880156e-06\n",
      "Iteration: 7264 Loss: 4.945074465287146e-06\n",
      "Iteration: 7265 Loss: 4.94054215016716e-06\n",
      "Iteration: 7266 Loss: 4.936013989074591e-06\n",
      "Iteration: 7267 Loss: 4.9314899782018395e-06\n",
      "Iteration: 7268 Loss: 4.926970113801057e-06\n",
      "Iteration: 7269 Loss: 4.9224543919580994e-06\n",
      "Iteration: 7270 Loss: 4.917942808990788e-06\n",
      "Iteration: 7271 Loss: 4.913435361198084e-06\n",
      "Iteration: 7272 Loss: 4.908932044287007e-06\n",
      "Iteration: 7273 Loss: 4.904432854973966e-06\n",
      "Iteration: 7274 Loss: 4.899937789325698e-06\n",
      "Iteration: 7275 Loss: 4.895446843562922e-06\n",
      "Iteration: 7276 Loss: 4.890960013911768e-06\n",
      "Iteration: 7277 Loss: 4.886477296598373e-06\n",
      "Iteration: 7278 Loss: 4.881998687853373e-06\n",
      "Iteration: 7279 Loss: 4.877524183912881e-06\n",
      "Iteration: 7280 Loss: 4.873053781011559e-06\n",
      "Iteration: 7281 Loss: 4.868587475393462e-06\n",
      "Iteration: 7282 Loss: 4.864125263301838e-06\n",
      "Iteration: 7283 Loss: 4.859667140985444e-06\n",
      "Iteration: 7284 Loss: 4.855213104695276e-06\n",
      "Iteration: 7285 Loss: 4.850763150686952e-06\n",
      "Iteration: 7286 Loss: 4.846317275217695e-06\n",
      "Iteration: 7287 Loss: 4.841875474437641e-06\n",
      "Iteration: 7288 Loss: 4.8374377448395555e-06\n",
      "Iteration: 7289 Loss: 4.833004082574593e-06\n",
      "Iteration: 7290 Loss: 4.828574483920861e-06\n",
      "Iteration: 7291 Loss: 4.824148945149146e-06\n",
      "Iteration: 7292 Loss: 4.819727462541085e-06\n",
      "Iteration: 7293 Loss: 4.8153100323198475e-06\n",
      "Iteration: 7294 Loss: 4.810896650828325e-06\n",
      "Iteration: 7295 Loss: 4.806487314416903e-06\n",
      "Iteration: 7296 Loss: 4.802082019315734e-06\n",
      "Iteration: 7297 Loss: 4.797680761823176e-06\n",
      "Iteration: 7298 Loss: 4.793283538295451e-06\n",
      "Iteration: 7299 Loss: 4.788890344923238e-06\n",
      "Iteration: 7300 Loss: 4.784501178067261e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7301 Loss: 4.780116034092787e-06\n",
      "Iteration: 7302 Loss: 4.77573490920224e-06\n",
      "Iteration: 7303 Loss: 4.771357799683698e-06\n",
      "Iteration: 7304 Loss: 4.766984702024254e-06\n",
      "Iteration: 7305 Loss: 4.76261561254717e-06\n",
      "Iteration: 7306 Loss: 4.758250527411033e-06\n",
      "Iteration: 7307 Loss: 4.753889443030309e-06\n",
      "Iteration: 7308 Loss: 4.749532355736268e-06\n",
      "Iteration: 7309 Loss: 4.745179261959689e-06\n",
      "Iteration: 7310 Loss: 4.740830157910118e-06\n",
      "Iteration: 7311 Loss: 4.736485039764992e-06\n",
      "Iteration: 7312 Loss: 4.73214390422276e-06\n",
      "Iteration: 7313 Loss: 4.7278067473995606e-06\n",
      "Iteration: 7314 Loss: 4.723473565902361e-06\n",
      "Iteration: 7315 Loss: 4.719144355835153e-06\n",
      "Iteration: 7316 Loss: 4.714819113641796e-06\n",
      "Iteration: 7317 Loss: 4.710497835684855e-06\n",
      "Iteration: 7318 Loss: 4.70618051833074e-06\n",
      "Iteration: 7319 Loss: 4.701867157953178e-06\n",
      "Iteration: 7320 Loss: 4.697557750920083e-06\n",
      "Iteration: 7321 Loss: 4.69325229361323e-06\n",
      "Iteration: 7322 Loss: 4.68895078241057e-06\n",
      "Iteration: 7323 Loss: 4.684653213692381e-06\n",
      "Iteration: 7324 Loss: 4.680359583849601e-06\n",
      "Iteration: 7325 Loss: 4.6760698892693255e-06\n",
      "Iteration: 7326 Loss: 4.671784126288949e-06\n",
      "Iteration: 7327 Loss: 4.667502291417e-06\n",
      "Iteration: 7328 Loss: 4.66322438100185e-06\n",
      "Iteration: 7329 Loss: 4.658950391383034e-06\n",
      "Iteration: 7330 Loss: 4.654680319197681e-06\n",
      "Iteration: 7331 Loss: 4.6504141605692986e-06\n",
      "Iteration: 7332 Loss: 4.646151912027043e-06\n",
      "Iteration: 7333 Loss: 4.641893569930926e-06\n",
      "Iteration: 7334 Loss: 4.637639130754455e-06\n",
      "Iteration: 7335 Loss: 4.633388590978231e-06\n",
      "Iteration: 7336 Loss: 4.6291419470291855e-06\n",
      "Iteration: 7337 Loss: 4.6248991952208834e-06\n",
      "Iteration: 7338 Loss: 4.620660332104428e-06\n",
      "Iteration: 7339 Loss: 4.61642535399684e-06\n",
      "Iteration: 7340 Loss: 4.612194257397272e-06\n",
      "Iteration: 7341 Loss: 4.607967038746104e-06\n",
      "Iteration: 7342 Loss: 4.603743694491487e-06\n",
      "Iteration: 7343 Loss: 4.599524221080088e-06\n",
      "Iteration: 7344 Loss: 4.59530861496585e-06\n",
      "Iteration: 7345 Loss: 4.591096872601369e-06\n",
      "Iteration: 7346 Loss: 4.586888990449628e-06\n",
      "Iteration: 7347 Loss: 4.582684964913673e-06\n",
      "Iteration: 7348 Loss: 4.57848479257197e-06\n",
      "Iteration: 7349 Loss: 4.574288469779518e-06\n",
      "Iteration: 7350 Loss: 4.57009599317897e-06\n",
      "Iteration: 7351 Loss: 4.5659073591316026e-06\n",
      "Iteration: 7352 Loss: 4.5617225640590085e-06\n",
      "Iteration: 7353 Loss: 4.55754160441348e-06\n",
      "Iteration: 7354 Loss: 4.553364476849661e-06\n",
      "Iteration: 7355 Loss: 4.549191177770123e-06\n",
      "Iteration: 7356 Loss: 4.5450217036684066e-06\n",
      "Iteration: 7357 Loss: 4.540856051034878e-06\n",
      "Iteration: 7358 Loss: 4.53669421637213e-06\n",
      "Iteration: 7359 Loss: 4.532536196175632e-06\n",
      "Iteration: 7360 Loss: 4.528381986895117e-06\n",
      "Iteration: 7361 Loss: 4.524231585151961e-06\n",
      "Iteration: 7362 Loss: 4.520084987338697e-06\n",
      "Iteration: 7363 Loss: 4.515942190171852e-06\n",
      "Iteration: 7364 Loss: 4.511803189940411e-06\n",
      "Iteration: 7365 Loss: 4.507667983307076e-06\n",
      "Iteration: 7366 Loss: 4.503536566679159e-06\n",
      "Iteration: 7367 Loss: 4.49940893661201e-06\n",
      "Iteration: 7368 Loss: 4.495285089634147e-06\n",
      "Iteration: 7369 Loss: 4.491165022424295e-06\n",
      "Iteration: 7370 Loss: 4.487048731340797e-06\n",
      "Iteration: 7371 Loss: 4.482936213070593e-06\n",
      "Iteration: 7372 Loss: 4.478827463927096e-06\n",
      "Iteration: 7373 Loss: 4.474722480654252e-06\n",
      "Iteration: 7374 Loss: 4.470621259799522e-06\n",
      "Iteration: 7375 Loss: 4.466523797802001e-06\n",
      "Iteration: 7376 Loss: 4.462430091216482e-06\n",
      "Iteration: 7377 Loss: 4.458340136771444e-06\n",
      "Iteration: 7378 Loss: 4.454253930856521e-06\n",
      "Iteration: 7379 Loss: 4.450171470094485e-06\n",
      "Iteration: 7380 Loss: 4.446092750994345e-06\n",
      "Iteration: 7381 Loss: 4.44201777018639e-06\n",
      "Iteration: 7382 Loss: 4.437946524296204e-06\n",
      "Iteration: 7383 Loss: 4.4338790099890274e-06\n",
      "Iteration: 7384 Loss: 4.429815223420158e-06\n",
      "Iteration: 7385 Loss: 4.425755161654167e-06\n",
      "Iteration: 7386 Loss: 4.421698820963384e-06\n",
      "Iteration: 7387 Loss: 4.417646198223905e-06\n",
      "Iteration: 7388 Loss: 4.4135972896830425e-06\n",
      "Iteration: 7389 Loss: 4.409552092113221e-06\n",
      "Iteration: 7390 Loss: 4.405510602164952e-06\n",
      "Iteration: 7391 Loss: 4.4014728163849805e-06\n",
      "Iteration: 7392 Loss: 4.39743873137938e-06\n",
      "Iteration: 7393 Loss: 4.3934083437528205e-06\n",
      "Iteration: 7394 Loss: 4.389381650120353e-06\n",
      "Iteration: 7395 Loss: 4.385358647096353e-06\n",
      "Iteration: 7396 Loss: 4.381339331351387e-06\n",
      "Iteration: 7397 Loss: 4.37732369945158e-06\n",
      "Iteration: 7398 Loss: 4.373311747963772e-06\n",
      "Iteration: 7399 Loss: 4.369303473572265e-06\n",
      "Iteration: 7400 Loss: 4.365298872846777e-06\n",
      "Iteration: 7401 Loss: 4.36129794259633e-06\n",
      "Iteration: 7402 Loss: 4.357300679279762e-06\n",
      "Iteration: 7403 Loss: 4.3533070795985315e-06\n",
      "Iteration: 7404 Loss: 4.3493171401908365e-06\n",
      "Iteration: 7405 Loss: 4.3453308576471986e-06\n",
      "Iteration: 7406 Loss: 4.341348228728139e-06\n",
      "Iteration: 7407 Loss: 4.337369250030617e-06\n",
      "Iteration: 7408 Loss: 4.333393918150561e-06\n",
      "Iteration: 7409 Loss: 4.329422229857772e-06\n",
      "Iteration: 7410 Loss: 4.325454181759746e-06\n",
      "Iteration: 7411 Loss: 4.321489770517048e-06\n",
      "Iteration: 7412 Loss: 4.31752899279785e-06\n",
      "Iteration: 7413 Loss: 4.3135718452709005e-06\n",
      "Iteration: 7414 Loss: 4.309618324609684e-06\n",
      "Iteration: 7415 Loss: 4.305668427489478e-06\n",
      "Iteration: 7416 Loss: 4.301722150589004e-06\n",
      "Iteration: 7417 Loss: 4.297779490508058e-06\n",
      "Iteration: 7418 Loss: 4.2938404440974436e-06\n",
      "Iteration: 7419 Loss: 4.289905007814463e-06\n",
      "Iteration: 7420 Loss: 4.2859731787022174e-06\n",
      "Iteration: 7421 Loss: 4.282044953136977e-06\n",
      "Iteration: 7422 Loss: 4.278120327985807e-06\n",
      "Iteration: 7423 Loss: 4.2741992998915545e-06\n",
      "Iteration: 7424 Loss: 4.270281865558496e-06\n",
      "Iteration: 7425 Loss: 4.266368021691947e-06\n",
      "Iteration: 7426 Loss: 4.2624577650866085e-06\n",
      "Iteration: 7427 Loss: 4.258551092433295e-06\n",
      "Iteration: 7428 Loss: 4.254648000234302e-06\n",
      "Iteration: 7429 Loss: 4.2507484853579166e-06\n",
      "Iteration: 7430 Loss: 4.246852544525358e-06\n",
      "Iteration: 7431 Loss: 4.242960174517332e-06\n",
      "Iteration: 7432 Loss: 4.239071372003004e-06\n",
      "Iteration: 7433 Loss: 4.235186133712595e-06\n",
      "Iteration: 7434 Loss: 4.231304456326292e-06\n",
      "Iteration: 7435 Loss: 4.227426336632201e-06\n",
      "Iteration: 7436 Loss: 4.22355177142935e-06\n",
      "Iteration: 7437 Loss: 4.219680757346469e-06\n",
      "Iteration: 7438 Loss: 4.215813291184258e-06\n",
      "Iteration: 7439 Loss: 4.211949369690265e-06\n",
      "Iteration: 7440 Loss: 4.208088989619067e-06\n",
      "Iteration: 7441 Loss: 4.204232147720688e-06\n",
      "Iteration: 7442 Loss: 4.200378840754651e-06\n",
      "Iteration: 7443 Loss: 4.196529065480966e-06\n",
      "Iteration: 7444 Loss: 4.1926828186604145e-06\n",
      "Iteration: 7445 Loss: 4.188840097063084e-06\n",
      "Iteration: 7446 Loss: 4.1850008973411695e-06\n",
      "Iteration: 7447 Loss: 4.181165216494327e-06\n",
      "Iteration: 7448 Loss: 4.177333051186989e-06\n",
      "Iteration: 7449 Loss: 4.173504398137291e-06\n",
      "Iteration: 7450 Loss: 4.169679254180516e-06\n",
      "Iteration: 7451 Loss: 4.165857616162099e-06\n",
      "Iteration: 7452 Loss: 4.1620394808087955e-06\n",
      "Iteration: 7453 Loss: 4.158224844912144e-06\n",
      "Iteration: 7454 Loss: 4.1544137052631355e-06\n",
      "Iteration: 7455 Loss: 4.150606058659031e-06\n",
      "Iteration: 7456 Loss: 4.146801901898181e-06\n",
      "Iteration: 7457 Loss: 4.143001231781768e-06\n",
      "Iteration: 7458 Loss: 4.1392040451128505e-06\n",
      "Iteration: 7459 Loss: 4.135410338699048e-06\n",
      "Iteration: 7460 Loss: 4.131620109351859e-06\n",
      "Iteration: 7461 Loss: 4.1278333538838135e-06\n",
      "Iteration: 7462 Loss: 4.124050069110228e-06\n",
      "Iteration: 7463 Loss: 4.120270251850355e-06\n",
      "Iteration: 7464 Loss: 4.116493898928052e-06\n",
      "Iteration: 7465 Loss: 4.112721007166255e-06\n",
      "Iteration: 7466 Loss: 4.108951573449039e-06\n",
      "Iteration: 7467 Loss: 4.105185594552169e-06\n",
      "Iteration: 7468 Loss: 4.101423067250704e-06\n",
      "Iteration: 7469 Loss: 4.097663988438396e-06\n",
      "Iteration: 7470 Loss: 4.093908354953724e-06\n",
      "Iteration: 7471 Loss: 4.090156163639441e-06\n",
      "Iteration: 7472 Loss: 4.0864074114001556e-06\n",
      "Iteration: 7473 Loss: 4.082662094967781e-06\n",
      "Iteration: 7474 Loss: 4.078920211248733e-06\n",
      "Iteration: 7475 Loss: 4.07518175709969e-06\n",
      "Iteration: 7476 Loss: 4.071446729225831e-06\n",
      "Iteration: 7477 Loss: 4.067715124844145e-06\n",
      "Iteration: 7478 Loss: 4.063986940699625e-06\n",
      "Iteration: 7479 Loss: 4.060262173419511e-06\n",
      "Iteration: 7480 Loss: 4.056540820019096e-06\n",
      "Iteration: 7481 Loss: 4.052822877374053e-06\n",
      "Iteration: 7482 Loss: 4.04910834235144e-06\n",
      "Iteration: 7483 Loss: 4.045397211832313e-06\n",
      "Iteration: 7484 Loss: 4.041689482696512e-06\n",
      "Iteration: 7485 Loss: 4.0379851518240955e-06\n",
      "Iteration: 7486 Loss: 4.034284216102356e-06\n",
      "Iteration: 7487 Loss: 4.030586672361366e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7488 Loss: 4.0268925176080886e-06\n",
      "Iteration: 7489 Loss: 4.023201748620068e-06\n",
      "Iteration: 7490 Loss: 4.019514362260597e-06\n",
      "Iteration: 7491 Loss: 4.015830355670747e-06\n",
      "Iteration: 7492 Loss: 4.012149725601422e-06\n",
      "Iteration: 7493 Loss: 4.0084724689612455e-06\n",
      "Iteration: 7494 Loss: 4.0047985826546585e-06\n",
      "Iteration: 7495 Loss: 4.001128063595198e-06\n",
      "Iteration: 7496 Loss: 3.997460908640249e-06\n",
      "Iteration: 7497 Loss: 3.993797114817918e-06\n",
      "Iteration: 7498 Loss: 3.990136679050412e-06\n",
      "Iteration: 7499 Loss: 3.9864795982907065e-06\n",
      "Iteration: 7500 Loss: 3.982825869227869e-06\n",
      "Iteration: 7501 Loss: 3.97917548893718e-06\n",
      "Iteration: 7502 Loss: 3.975528454348084e-06\n",
      "Iteration: 7503 Loss: 3.971884762395125e-06\n",
      "Iteration: 7504 Loss: 3.968244409960193e-06\n",
      "Iteration: 7505 Loss: 3.964607394147073e-06\n",
      "Iteration: 7506 Loss: 3.960973711732286e-06\n",
      "Iteration: 7507 Loss: 3.95734335971508e-06\n",
      "Iteration: 7508 Loss: 3.953716335044667e-06\n",
      "Iteration: 7509 Loss: 3.950092634669946e-06\n",
      "Iteration: 7510 Loss: 3.946472255544046e-06\n",
      "Iteration: 7511 Loss: 3.942855194624496e-06\n",
      "Iteration: 7512 Loss: 3.939241448867905e-06\n",
      "Iteration: 7513 Loss: 3.93563101523781e-06\n",
      "Iteration: 7514 Loss: 3.932023890697752e-06\n",
      "Iteration: 7515 Loss: 3.928420072216129e-06\n",
      "Iteration: 7516 Loss: 3.924819556758185e-06\n",
      "Iteration: 7517 Loss: 3.92122234130201e-06\n",
      "Iteration: 7518 Loss: 3.917628422819346e-06\n",
      "Iteration: 7519 Loss: 3.9140377982927035e-06\n",
      "Iteration: 7520 Loss: 3.910450464697898e-06\n",
      "Iteration: 7521 Loss: 3.9068664190215156e-06\n",
      "Iteration: 7522 Loss: 3.9032856582495205e-06\n",
      "Iteration: 7523 Loss: 3.899708179371118e-06\n",
      "Iteration: 7524 Loss: 3.896133979379817e-06\n",
      "Iteration: 7525 Loss: 3.892563055267884e-06\n",
      "Iteration: 7526 Loss: 3.888995404117275e-06\n",
      "Iteration: 7527 Loss: 3.885431022762648e-06\n",
      "Iteration: 7528 Loss: 3.881869908289239e-06\n",
      "Iteration: 7529 Loss: 3.878312057702678e-06\n",
      "Iteration: 7530 Loss: 3.874757467954145e-06\n",
      "Iteration: 7531 Loss: 3.8712061361141315e-06\n",
      "Iteration: 7532 Loss: 3.867658059251698e-06\n",
      "Iteration: 7533 Loss: 3.864113234325887e-06\n",
      "Iteration: 7534 Loss: 3.86057165835896e-06\n",
      "Iteration: 7535 Loss: 3.857033328371328e-06\n",
      "Iteration: 7536 Loss: 3.853498241386435e-06\n",
      "Iteration: 7537 Loss: 3.849966394203239e-06\n",
      "Iteration: 7538 Loss: 3.84643778431335e-06\n",
      "Iteration: 7539 Loss: 3.842912408634728e-06\n",
      "Iteration: 7540 Loss: 3.839390263971528e-06\n",
      "Iteration: 7541 Loss: 3.8358713474796234e-06\n",
      "Iteration: 7542 Loss: 3.832355656199384e-06\n",
      "Iteration: 7543 Loss: 3.828843187234791e-06\n",
      "Iteration: 7544 Loss: 3.825333937511412e-06\n",
      "Iteration: 7545 Loss: 3.821827904139718e-06\n",
      "Iteration: 7546 Loss: 3.818325084170644e-06\n",
      "Iteration: 7547 Loss: 3.814825474602446e-06\n",
      "Iteration: 7548 Loss: 3.8113290726928935e-06\n",
      "Iteration: 7549 Loss: 3.8078358752163767e-06\n",
      "Iteration: 7550 Loss: 3.8043458794371163e-06\n",
      "Iteration: 7551 Loss: 3.800859082360696e-06\n",
      "Iteration: 7552 Loss: 3.7973754811138338e-06\n",
      "Iteration: 7553 Loss: 3.793895072654715e-06\n",
      "Iteration: 7554 Loss: 3.7904178541121903e-06\n",
      "Iteration: 7555 Loss: 3.7869438226215494e-06\n",
      "Iteration: 7556 Loss: 3.7834729751450347e-06\n",
      "Iteration: 7557 Loss: 3.7800053087392188e-06\n",
      "Iteration: 7558 Loss: 3.776540820740138e-06\n",
      "Iteration: 7559 Loss: 3.773079507982704e-06\n",
      "Iteration: 7560 Loss: 3.769621367641903e-06\n",
      "Iteration: 7561 Loss: 3.7661663968702214e-06\n",
      "Iteration: 7562 Loss: 3.762714592637748e-06\n",
      "Iteration: 7563 Loss: 3.7592659521078086e-06\n",
      "Iteration: 7564 Loss: 3.7558204722307308e-06\n",
      "Iteration: 7565 Loss: 3.7523781504052015e-06\n",
      "Iteration: 7566 Loss: 3.7489389835881596e-06\n",
      "Iteration: 7567 Loss: 3.745502968888023e-06\n",
      "Iteration: 7568 Loss: 3.742070103419487e-06\n",
      "Iteration: 7569 Loss: 3.7386403842921404e-06\n",
      "Iteration: 7570 Loss: 3.7352138086803715e-06\n",
      "Iteration: 7571 Loss: 3.7317903736743027e-06\n",
      "Iteration: 7572 Loss: 3.7283700762823597e-06\n",
      "Iteration: 7573 Loss: 3.7249529137701085e-06\n",
      "Iteration: 7574 Loss: 3.7215388831534807e-06\n",
      "Iteration: 7575 Loss: 3.718127981557211e-06\n",
      "Iteration: 7576 Loss: 3.7147202061733606e-06\n",
      "Iteration: 7577 Loss: 3.7113155541925685e-06\n",
      "Iteration: 7578 Loss: 3.7079140226963195e-06\n",
      "Iteration: 7579 Loss: 3.704515608675851e-06\n",
      "Iteration: 7580 Loss: 3.701120309570172e-06\n",
      "Iteration: 7581 Loss: 3.6977281225236035e-06\n",
      "Iteration: 7582 Loss: 3.6943390443862882e-06\n",
      "Iteration: 7583 Loss: 3.69095307246197e-06\n",
      "Iteration: 7584 Loss: 3.6875702038982523e-06\n",
      "Iteration: 7585 Loss: 3.684190435854137e-06\n",
      "Iteration: 7586 Loss: 3.680813765488173e-06\n",
      "Iteration: 7587 Loss: 3.677440189900687e-06\n",
      "Iteration: 7588 Loss: 3.674069706428574e-06\n",
      "Iteration: 7589 Loss: 3.670702312068079e-06\n",
      "Iteration: 7590 Loss: 3.667338004133914e-06\n",
      "Iteration: 7591 Loss: 3.6639767794105502e-06\n",
      "Iteration: 7592 Loss: 3.660618635574002e-06\n",
      "Iteration: 7593 Loss: 3.657263569593734e-06\n",
      "Iteration: 7594 Loss: 3.653911578649583e-06\n",
      "Iteration: 7595 Loss: 3.65056265992335e-06\n",
      "Iteration: 7596 Loss: 3.6472168105735626e-06\n",
      "Iteration: 7597 Loss: 3.643874027980411e-06\n",
      "Iteration: 7598 Loss: 3.6405343090250925e-06\n",
      "Iteration: 7599 Loss: 3.6371976510406882e-06\n",
      "Iteration: 7600 Loss: 3.633864051218631e-06\n",
      "Iteration: 7601 Loss: 3.6305335067619692e-06\n",
      "Iteration: 7602 Loss: 3.6272060148645887e-06\n",
      "Iteration: 7603 Loss: 3.6238815727334274e-06\n",
      "Iteration: 7604 Loss: 3.6205601775699168e-06\n",
      "Iteration: 7605 Loss: 3.6172418265813862e-06\n",
      "Iteration: 7606 Loss: 3.61392651697901e-06\n",
      "Iteration: 7607 Loss: 3.6106142459759756e-06\n",
      "Iteration: 7608 Loss: 3.6073050107847933e-06\n",
      "Iteration: 7609 Loss: 3.603998808625737e-06\n",
      "Iteration: 7610 Loss: 3.6006956367173975e-06\n",
      "Iteration: 7611 Loss: 3.5973954922823985e-06\n",
      "Iteration: 7612 Loss: 3.5940983725475395e-06\n",
      "Iteration: 7613 Loss: 3.59080427473712e-06\n",
      "Iteration: 7614 Loss: 3.587513196086164e-06\n",
      "Iteration: 7615 Loss: 3.584225133823009e-06\n",
      "Iteration: 7616 Loss: 3.580940085187741e-06\n",
      "Iteration: 7617 Loss: 3.5776580474130938e-06\n",
      "Iteration: 7618 Loss: 3.574379017742628e-06\n",
      "Iteration: 7619 Loss: 3.5711029934192476e-06\n",
      "Iteration: 7620 Loss: 3.5678299716877712e-06\n",
      "Iteration: 7621 Loss: 3.564559949796074e-06\n",
      "Iteration: 7622 Loss: 3.5612929249948278e-06\n",
      "Iteration: 7623 Loss: 3.558028894538152e-06\n",
      "Iteration: 7624 Loss: 3.5547678556794303e-06\n",
      "Iteration: 7625 Loss: 3.5515098056805292e-06\n",
      "Iteration: 7626 Loss: 3.548254741795278e-06\n",
      "Iteration: 7627 Loss: 3.545002661293406e-06\n",
      "Iteration: 7628 Loss: 3.5417535614380585e-06\n",
      "Iteration: 7629 Loss: 3.538507439497832e-06\n",
      "Iteration: 7630 Loss: 3.5352642927432044e-06\n",
      "Iteration: 7631 Loss: 3.532024118447051e-06\n",
      "Iteration: 7632 Loss: 3.5287869138277933e-06\n",
      "Iteration: 7633 Loss: 3.525552676277675e-06\n",
      "Iteration: 7634 Loss: 3.5223214030782683e-06\n",
      "Iteration: 7635 Loss: 3.519093091396059e-06\n",
      "Iteration: 7636 Loss: 3.515867738576326e-06\n",
      "Iteration: 7637 Loss: 3.512645341906971e-06\n",
      "Iteration: 7638 Loss: 3.5094258986782887e-06\n",
      "Iteration: 7639 Loss: 3.5062094061264023e-06\n",
      "Iteration: 7640 Loss: 3.5029958616590173e-06\n",
      "Iteration: 7641 Loss: 3.4997852625193655e-06\n",
      "Iteration: 7642 Loss: 3.49657760595042e-06\n",
      "Iteration: 7643 Loss: 3.493372889367836e-06\n",
      "Iteration: 7644 Loss: 3.490171110022643e-06\n",
      "Iteration: 7645 Loss: 3.4869722652222465e-06\n",
      "Iteration: 7646 Loss: 3.483776352273728e-06\n",
      "Iteration: 7647 Loss: 3.480583368491745e-06\n",
      "Iteration: 7648 Loss: 3.4773933111952186e-06\n",
      "Iteration: 7649 Loss: 3.474206177697329e-06\n",
      "Iteration: 7650 Loss: 3.471021965321025e-06\n",
      "Iteration: 7651 Loss: 3.4678406713852055e-06\n",
      "Iteration: 7652 Loss: 3.464662293277068e-06\n",
      "Iteration: 7653 Loss: 3.46148682815024e-06\n",
      "Iteration: 7654 Loss: 3.4583142735622185e-06\n",
      "Iteration: 7655 Loss: 3.455144626619027e-06\n",
      "Iteration: 7656 Loss: 3.45197788488376e-06\n",
      "Iteration: 7657 Loss: 3.4488140454655555e-06\n",
      "Iteration: 7658 Loss: 3.445653105872586e-06\n",
      "Iteration: 7659 Loss: 3.4424950633955975e-06\n",
      "Iteration: 7660 Loss: 3.4393399153741005e-06\n",
      "Iteration: 7661 Loss: 3.4361876591587683e-06\n",
      "Iteration: 7662 Loss: 3.4330382920958515e-06\n",
      "Iteration: 7663 Loss: 3.4298918115390272e-06\n",
      "Iteration: 7664 Loss: 3.4267482148432488e-06\n",
      "Iteration: 7665 Loss: 3.423607499423401e-06\n",
      "Iteration: 7666 Loss: 3.420469662578274e-06\n",
      "Iteration: 7667 Loss: 3.417334701672988e-06\n",
      "Iteration: 7668 Loss: 3.414202613955454e-06\n",
      "Iteration: 7669 Loss: 3.411073396964328e-06\n",
      "Iteration: 7670 Loss: 3.4079470480674387e-06\n",
      "Iteration: 7671 Loss: 3.4048235645236276e-06\n",
      "Iteration: 7672 Loss: 3.401702943761464e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7673 Loss: 3.3985851832160316e-06\n",
      "Iteration: 7674 Loss: 3.395470280151546e-06\n",
      "Iteration: 7675 Loss: 3.392358232006133e-06\n",
      "Iteration: 7676 Loss: 3.3892490361608186e-06\n",
      "Iteration: 7677 Loss: 3.3861426900063575e-06\n",
      "Iteration: 7678 Loss: 3.3830391909258774e-06\n",
      "Iteration: 7679 Loss: 3.379938536313194e-06\n",
      "Iteration: 7680 Loss: 3.3768407235584044e-06\n",
      "Iteration: 7681 Loss: 3.373745750059463e-06\n",
      "Iteration: 7682 Loss: 3.3706536132129326e-06\n",
      "Iteration: 7683 Loss: 3.3675643104188803e-06\n",
      "Iteration: 7684 Loss: 3.364477839080017e-06\n",
      "Iteration: 7685 Loss: 3.361394196601062e-06\n",
      "Iteration: 7686 Loss: 3.358313380390451e-06\n",
      "Iteration: 7687 Loss: 3.3552353878543655e-06\n",
      "Iteration: 7688 Loss: 3.352160216409405e-06\n",
      "Iteration: 7689 Loss: 3.349087863466279e-06\n",
      "Iteration: 7690 Loss: 3.3460183263872055e-06\n",
      "Iteration: 7691 Loss: 3.342951602646295e-06\n",
      "Iteration: 7692 Loss: 3.3398876896667705e-06\n",
      "Iteration: 7693 Loss: 3.3368265849283035e-06\n",
      "Iteration: 7694 Loss: 3.3337682857445705e-06\n",
      "Iteration: 7695 Loss: 3.330712789656196e-06\n",
      "Iteration: 7696 Loss: 3.327660094037324e-06\n",
      "Iteration: 7697 Loss: 3.324610196324058e-06\n",
      "Iteration: 7698 Loss: 3.3215630939462446e-06\n",
      "Iteration: 7699 Loss: 3.318518784402931e-06\n",
      "Iteration: 7700 Loss: 3.3154772650215064e-06\n",
      "Iteration: 7701 Loss: 3.3124385333562194e-06\n",
      "Iteration: 7702 Loss: 3.3094025867405083e-06\n",
      "Iteration: 7703 Loss: 3.3063694226744323e-06\n",
      "Iteration: 7704 Loss: 3.3033390386133164e-06\n",
      "Iteration: 7705 Loss: 3.300311432003511e-06\n",
      "Iteration: 7706 Loss: 3.2972866003044845e-06\n",
      "Iteration: 7707 Loss: 3.294264540969294e-06\n",
      "Iteration: 7708 Loss: 3.2912452514571224e-06\n",
      "Iteration: 7709 Loss: 3.288228729232851e-06\n",
      "Iteration: 7710 Loss: 3.28521497169904e-06\n",
      "Iteration: 7711 Loss: 3.282203976495531e-06\n",
      "Iteration: 7712 Loss: 3.27919574089096e-06\n",
      "Iteration: 7713 Loss: 3.2761902624669063e-06\n",
      "Iteration: 7714 Loss: 3.273187538671316e-06\n",
      "Iteration: 7715 Loss: 3.2701875670609376e-06\n",
      "Iteration: 7716 Loss: 3.2671903449474326e-06\n",
      "Iteration: 7717 Loss: 3.26419586989437e-06\n",
      "Iteration: 7718 Loss: 3.26120413938171e-06\n",
      "Iteration: 7719 Loss: 3.2582151508963244e-06\n",
      "Iteration: 7720 Loss: 3.2552289019240527e-06\n",
      "Iteration: 7721 Loss: 3.252245389955772e-06\n",
      "Iteration: 7722 Loss: 3.2492646125385295e-06\n",
      "Iteration: 7723 Loss: 3.246286567050233e-06\n",
      "Iteration: 7724 Loss: 3.2433112510476708e-06\n",
      "Iteration: 7725 Loss: 3.2403386620256647e-06\n",
      "Iteration: 7726 Loss: 3.2373687974876736e-06\n",
      "Iteration: 7727 Loss: 3.2344016549362985e-06\n",
      "Iteration: 7728 Loss: 3.231437231875985e-06\n",
      "Iteration: 7729 Loss: 3.228475525812394e-06\n",
      "Iteration: 7730 Loss: 3.225516534258003e-06\n",
      "Iteration: 7731 Loss: 3.22256025472372e-06\n",
      "Iteration: 7732 Loss: 3.2196066846098008e-06\n",
      "Iteration: 7733 Loss: 3.216655821604217e-06\n",
      "Iteration: 7734 Loss: 3.2137076632841658e-06\n",
      "Iteration: 7735 Loss: 3.2107622069974526e-06\n",
      "Iteration: 7736 Loss: 3.207819450324811e-06\n",
      "Iteration: 7737 Loss: 3.204879390735359e-06\n",
      "Iteration: 7738 Loss: 3.2019420258142833e-06\n",
      "Iteration: 7739 Loss: 3.199007353150595e-06\n",
      "Iteration: 7740 Loss: 3.196075370217866e-06\n",
      "Iteration: 7741 Loss: 3.193146074606452e-06\n",
      "Iteration: 7742 Loss: 3.190219463798741e-06\n",
      "Iteration: 7743 Loss: 3.1872955352761738e-06\n",
      "Iteration: 7744 Loss: 3.1843742866369028e-06\n",
      "Iteration: 7745 Loss: 3.18145571536946e-06\n",
      "Iteration: 7746 Loss: 3.1785398190753917e-06\n",
      "Iteration: 7747 Loss: 3.175626595416292e-06\n",
      "Iteration: 7748 Loss: 3.1727160417732655e-06\n",
      "Iteration: 7749 Loss: 3.169808155754628e-06\n",
      "Iteration: 7750 Loss: 3.1669029348601725e-06\n",
      "Iteration: 7751 Loss: 3.1640003767614197e-06\n",
      "Iteration: 7752 Loss: 3.161100478956554e-06\n",
      "Iteration: 7753 Loss: 3.1582032390123863e-06\n",
      "Iteration: 7754 Loss: 3.1553086544908958e-06\n",
      "Iteration: 7755 Loss: 3.1524167229578905e-06\n",
      "Iteration: 7756 Loss: 3.1495274419834243e-06\n",
      "Iteration: 7757 Loss: 3.1466408091927667e-06\n",
      "Iteration: 7758 Loss: 3.1437568221034292e-06\n",
      "Iteration: 7759 Loss: 3.1408754782342233e-06\n",
      "Iteration: 7760 Loss: 3.137996775217252e-06\n",
      "Iteration: 7761 Loss: 3.1351207106332226e-06\n",
      "Iteration: 7762 Loss: 3.1322472820646003e-06\n",
      "Iteration: 7763 Loss: 3.1293764870946936e-06\n",
      "Iteration: 7764 Loss: 3.1265083233102502e-06\n",
      "Iteration: 7765 Loss: 3.1236427882979543e-06\n",
      "Iteration: 7766 Loss: 3.1207798796503678e-06\n",
      "Iteration: 7767 Loss: 3.117919594959124e-06\n",
      "Iteration: 7768 Loss: 3.115061931820422e-06\n",
      "Iteration: 7769 Loss: 3.112206887774014e-06\n",
      "Iteration: 7770 Loss: 3.109354460589228e-06\n",
      "Iteration: 7771 Loss: 3.106504647699593e-06\n",
      "Iteration: 7772 Loss: 3.1036574467622712e-06\n",
      "Iteration: 7773 Loss: 3.1008128554425795e-06\n",
      "Iteration: 7774 Loss: 3.097970871236084e-06\n",
      "Iteration: 7775 Loss: 3.0951314918914105e-06\n",
      "Iteration: 7776 Loss: 3.0922947148531495e-06\n",
      "Iteration: 7777 Loss: 3.0894605378235855e-06\n",
      "Iteration: 7778 Loss: 3.0866289583592915e-06\n",
      "Iteration: 7779 Loss: 3.083799974195475e-06\n",
      "Iteration: 7780 Loss: 3.0809735828924563e-06\n",
      "Iteration: 7781 Loss: 3.0781497819948295e-06\n",
      "Iteration: 7782 Loss: 3.0753285692961644e-06\n",
      "Iteration: 7783 Loss: 3.0725099423350575e-06\n",
      "Iteration: 7784 Loss: 3.0696938987487785e-06\n",
      "Iteration: 7785 Loss: 3.066880436163077e-06\n",
      "Iteration: 7786 Loss: 3.0640695522153455e-06\n",
      "Iteration: 7787 Loss: 3.06126124454312e-06\n",
      "Iteration: 7788 Loss: 3.0584555107275473e-06\n",
      "Iteration: 7789 Loss: 3.0556523484653707e-06\n",
      "Iteration: 7790 Loss: 3.052851755514735e-06\n",
      "Iteration: 7791 Loss: 3.050053729292742e-06\n",
      "Iteration: 7792 Loss: 3.0472582676170207e-06\n",
      "Iteration: 7793 Loss: 3.044465368081501e-06\n",
      "Iteration: 7794 Loss: 3.0416750283358194e-06\n",
      "Iteration: 7795 Loss: 3.0388872461477568e-06\n",
      "Iteration: 7796 Loss: 3.0361020189504306e-06\n",
      "Iteration: 7797 Loss: 3.033319344509804e-06\n",
      "Iteration: 7798 Loss: 3.030539220431917e-06\n",
      "Iteration: 7799 Loss: 3.0277616444935795e-06\n",
      "Iteration: 7800 Loss: 3.0249866143560484e-06\n",
      "Iteration: 7801 Loss: 3.0222141275771968e-06\n",
      "Iteration: 7802 Loss: 3.019444181879615e-06\n",
      "Iteration: 7803 Loss: 3.0166767749366826e-06\n",
      "Iteration: 7804 Loss: 3.013911904361702e-06\n",
      "Iteration: 7805 Loss: 3.0111495679444678e-06\n",
      "Iteration: 7806 Loss: 3.008389763364815e-06\n",
      "Iteration: 7807 Loss: 3.005632488241575e-06\n",
      "Iteration: 7808 Loss: 3.0028777402031814e-06\n",
      "Iteration: 7809 Loss: 3.00012551704393e-06\n",
      "Iteration: 7810 Loss: 2.997375816338607e-06\n",
      "Iteration: 7811 Loss: 2.994628635831306e-06\n",
      "Iteration: 7812 Loss: 2.9918839732128745e-06\n",
      "Iteration: 7813 Loss: 2.9891418261719026e-06\n",
      "Iteration: 7814 Loss: 2.986402192408555e-06\n",
      "Iteration: 7815 Loss: 2.9836650696151883e-06\n",
      "Iteration: 7816 Loss: 2.980930455490588e-06\n",
      "Iteration: 7817 Loss: 2.9781983477380704e-06\n",
      "Iteration: 7818 Loss: 2.9754687440576125e-06\n",
      "Iteration: 7819 Loss: 2.9727416420419917e-06\n",
      "Iteration: 7820 Loss: 2.9700170397091784e-06\n",
      "Iteration: 7821 Loss: 2.967294934402437e-06\n",
      "Iteration: 7822 Loss: 2.964575324086071e-06\n",
      "Iteration: 7823 Loss: 2.9618582063887917e-06\n",
      "Iteration: 7824 Loss: 2.95914357902622e-06\n",
      "Iteration: 7825 Loss: 2.9564314397138925e-06\n",
      "Iteration: 7826 Loss: 2.953721786230373e-06\n",
      "Iteration: 7827 Loss: 2.951014616239502e-06\n",
      "Iteration: 7828 Loss: 2.948309927409662e-06\n",
      "Iteration: 7829 Loss: 2.945607717523906e-06\n",
      "Iteration: 7830 Loss: 2.9429079843054557e-06\n",
      "Iteration: 7831 Loss: 2.940210725491648e-06\n",
      "Iteration: 7832 Loss: 2.9375159388086245e-06\n",
      "Iteration: 7833 Loss: 2.9348236219941523e-06\n",
      "Iteration: 7834 Loss: 2.9321337727816284e-06\n",
      "Iteration: 7835 Loss: 2.9294463889125835e-06\n",
      "Iteration: 7836 Loss: 2.92676146812505e-06\n",
      "Iteration: 7837 Loss: 2.924079008246424e-06\n",
      "Iteration: 7838 Loss: 2.921399006852532e-06\n",
      "Iteration: 7839 Loss: 2.9187214617744616e-06\n",
      "Iteration: 7840 Loss: 2.9160463707609524e-06\n",
      "Iteration: 7841 Loss: 2.9133737315615575e-06\n",
      "Iteration: 7842 Loss: 2.9107035419327438e-06\n",
      "Iteration: 7843 Loss: 2.908035799543918e-06\n",
      "Iteration: 7844 Loss: 2.905370502259676e-06\n",
      "Iteration: 7845 Loss: 2.902707647816755e-06\n",
      "Iteration: 7846 Loss: 2.900047234028844e-06\n",
      "Iteration: 7847 Loss: 2.8973892585494256e-06\n",
      "Iteration: 7848 Loss: 2.894733719253389e-06\n",
      "Iteration: 7849 Loss: 2.8920806138524757e-06\n",
      "Iteration: 7850 Loss: 2.8894299400619787e-06\n",
      "Iteration: 7851 Loss: 2.8867816957628224e-06\n",
      "Iteration: 7852 Loss: 2.8841358786720063e-06\n",
      "Iteration: 7853 Loss: 2.8814924865670847e-06\n",
      "Iteration: 7854 Loss: 2.8788515172256787e-06\n",
      "Iteration: 7855 Loss: 2.8762129684247473e-06\n",
      "Iteration: 7856 Loss: 2.8735768379472273e-06\n",
      "Iteration: 7857 Loss: 2.8709431236333767e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7858 Loss: 2.868311823210471e-06\n",
      "Iteration: 7859 Loss: 2.8656829344130214e-06\n",
      "Iteration: 7860 Loss: 2.8630564550820045e-06\n",
      "Iteration: 7861 Loss: 2.8604323829571368e-06\n",
      "Iteration: 7862 Loss: 2.85781071579652e-06\n",
      "Iteration: 7863 Loss: 2.8551914516896947e-06\n",
      "Iteration: 7864 Loss: 2.8525745881751628e-06\n",
      "Iteration: 7865 Loss: 2.8499601231080472e-06\n",
      "Iteration: 7866 Loss: 2.8473480542911103e-06\n",
      "Iteration: 7867 Loss: 2.844738379674135e-06\n",
      "Iteration: 7868 Loss: 2.842131096768861e-06\n",
      "Iteration: 7869 Loss: 2.839526203474014e-06\n",
      "Iteration: 7870 Loss: 2.8369236977123e-06\n",
      "Iteration: 7871 Loss: 2.8343235771832628e-06\n",
      "Iteration: 7872 Loss: 2.831725839811299e-06\n",
      "Iteration: 7873 Loss: 2.8291304833584697e-06\n",
      "Iteration: 7874 Loss: 2.826537505697332e-06\n",
      "Iteration: 7875 Loss: 2.8239469045929265e-06\n",
      "Iteration: 7876 Loss: 2.821358677811455e-06\n",
      "Iteration: 7877 Loss: 2.818772823227446e-06\n",
      "Iteration: 7878 Loss: 2.8161893386721244e-06\n",
      "Iteration: 7879 Loss: 2.81360822197125e-06\n",
      "Iteration: 7880 Loss: 2.8110294709560412e-06\n",
      "Iteration: 7881 Loss: 2.8084530834562857e-06\n",
      "Iteration: 7882 Loss: 2.805879057306401e-06\n",
      "Iteration: 7883 Loss: 2.8033073903438008e-06\n",
      "Iteration: 7884 Loss: 2.800738080346863e-06\n",
      "Iteration: 7885 Loss: 2.7981711252130706e-06\n",
      "Iteration: 7886 Loss: 2.7956065228418492e-06\n",
      "Iteration: 7887 Loss: 2.793044271019602e-06\n",
      "Iteration: 7888 Loss: 2.790484367647685e-06\n",
      "Iteration: 7889 Loss: 2.787926810403404e-06\n",
      "Iteration: 7890 Loss: 2.785371597308895e-06\n",
      "Iteration: 7891 Loss: 2.7828187261559445e-06\n",
      "Iteration: 7892 Loss: 2.7802681948015095e-06\n",
      "Iteration: 7893 Loss: 2.7777200010973975e-06\n",
      "Iteration: 7894 Loss: 2.7751741429600328e-06\n",
      "Iteration: 7895 Loss: 2.7726306181359236e-06\n",
      "Iteration: 7896 Loss: 2.770089424597874e-06\n",
      "Iteration: 7897 Loss: 2.76755056009775e-06\n",
      "Iteration: 7898 Loss: 2.7650140225545982e-06\n",
      "Iteration: 7899 Loss: 2.7624798098401903e-06\n",
      "Iteration: 7900 Loss: 2.7599479198186738e-06\n",
      "Iteration: 7901 Loss: 2.757418350364741e-06\n",
      "Iteration: 7902 Loss: 2.7548910993516352e-06\n",
      "Iteration: 7903 Loss: 2.752366164653339e-06\n",
      "Iteration: 7904 Loss: 2.7498435441466107e-06\n",
      "Iteration: 7905 Loss: 2.747323235653018e-06\n",
      "Iteration: 7906 Loss: 2.7448052371686208e-06\n",
      "Iteration: 7907 Loss: 2.742289546370177e-06\n",
      "Iteration: 7908 Loss: 2.7397761614966318e-06\n",
      "Iteration: 7909 Loss: 2.7372650801716144e-06\n",
      "Iteration: 7910 Loss: 2.7347563003429924e-06\n",
      "Iteration: 7911 Loss: 2.7322498198981593e-06\n",
      "Iteration: 7912 Loss: 2.7297456367318338e-06\n",
      "Iteration: 7913 Loss: 2.727243748737181e-06\n",
      "Iteration: 7914 Loss: 2.7247441538120318e-06\n",
      "Iteration: 7915 Loss: 2.722246850000769e-06\n",
      "Iteration: 7916 Loss: 2.7197518349084714e-06\n",
      "Iteration: 7917 Loss: 2.7172591065866757e-06\n",
      "Iteration: 7918 Loss: 2.71476866293859e-06\n",
      "Iteration: 7919 Loss: 2.712280501870411e-06\n",
      "Iteration: 7920 Loss: 2.7097946213737683e-06\n",
      "Iteration: 7921 Loss: 2.7073110191894255e-06\n",
      "Iteration: 7922 Loss: 2.704829693314843e-06\n",
      "Iteration: 7923 Loss: 2.702350641606723e-06\n",
      "Iteration: 7924 Loss: 2.699873862149766e-06\n",
      "Iteration: 7925 Loss: 2.697399352691149e-06\n",
      "Iteration: 7926 Loss: 2.6949271112095848e-06\n",
      "Iteration: 7927 Loss: 2.6924571355661866e-06\n",
      "Iteration: 7928 Loss: 2.68998942374509e-06\n",
      "Iteration: 7929 Loss: 2.687523973724902e-06\n",
      "Iteration: 7930 Loss: 2.6850607834319335e-06\n",
      "Iteration: 7931 Loss: 2.682599850740504e-06\n",
      "Iteration: 7932 Loss: 2.680141173468761e-06\n",
      "Iteration: 7933 Loss: 2.6776847497740503e-06\n",
      "Iteration: 7934 Loss: 2.6752305774214634e-06\n",
      "Iteration: 7935 Loss: 2.6727786543485006e-06\n",
      "Iteration: 7936 Loss: 2.670328978662952e-06\n",
      "Iteration: 7937 Loss: 2.6678815480497495e-06\n",
      "Iteration: 7938 Loss: 2.6654363606790855e-06\n",
      "Iteration: 7939 Loss: 2.6629934144070974e-06\n",
      "Iteration: 7940 Loss: 2.660552707182071e-06\n",
      "Iteration: 7941 Loss: 2.658114236953178e-06\n",
      "Iteration: 7942 Loss: 2.655678001666291e-06\n",
      "Iteration: 7943 Loss: 2.653243999275774e-06\n",
      "Iteration: 7944 Loss: 2.650812227733986e-06\n",
      "Iteration: 7945 Loss: 2.6483826849981976e-06\n",
      "Iteration: 7946 Loss: 2.6459553689660496e-06\n",
      "Iteration: 7947 Loss: 2.643530277655788e-06\n",
      "Iteration: 7948 Loss: 2.6411074090828636e-06\n",
      "Iteration: 7949 Loss: 2.6386867611557566e-06\n",
      "Iteration: 7950 Loss: 2.6362683318370715e-06\n",
      "Iteration: 7951 Loss: 2.633852119094775e-06\n",
      "Iteration: 7952 Loss: 2.631438120896833e-06\n",
      "Iteration: 7953 Loss: 2.629026335270544e-06\n",
      "Iteration: 7954 Loss: 2.6266167601300333e-06\n",
      "Iteration: 7955 Loss: 2.624209393395661e-06\n",
      "Iteration: 7956 Loss: 2.6218042330953425e-06\n",
      "Iteration: 7957 Loss: 2.619401277210779e-06\n",
      "Iteration: 7958 Loss: 2.6170005237198768e-06\n",
      "Iteration: 7959 Loss: 2.61460197054716e-06\n",
      "Iteration: 7960 Loss: 2.6122056157898784e-06\n",
      "Iteration: 7961 Loss: 2.609811457433371e-06\n",
      "Iteration: 7962 Loss: 2.607419493293971e-06\n",
      "Iteration: 7963 Loss: 2.6050297214757434e-06\n",
      "Iteration: 7964 Loss: 2.6026421400222857e-06\n",
      "Iteration: 7965 Loss: 2.600256746874049e-06\n",
      "Iteration: 7966 Loss: 2.597873540021662e-06\n",
      "Iteration: 7967 Loss: 2.595492517517774e-06\n",
      "Iteration: 7968 Loss: 2.5931136773063165e-06\n",
      "Iteration: 7969 Loss: 2.590737017329239e-06\n",
      "Iteration: 7970 Loss: 2.588362535645714e-06\n",
      "Iteration: 7971 Loss: 2.5859902302017127e-06\n",
      "Iteration: 7972 Loss: 2.583620099057995e-06\n",
      "Iteration: 7973 Loss: 2.5812521402804194e-06\n",
      "Iteration: 7974 Loss: 2.5788863518204998e-06\n",
      "Iteration: 7975 Loss: 2.576522731689373e-06\n",
      "Iteration: 7976 Loss: 2.5741612779569968e-06\n",
      "Iteration: 7977 Loss: 2.5718019885235368e-06\n",
      "Iteration: 7978 Loss: 2.569444861461392e-06\n",
      "Iteration: 7979 Loss: 2.5670898947346473e-06\n",
      "Iteration: 7980 Loss: 2.5647370865299684e-06\n",
      "Iteration: 7981 Loss: 2.562386434644107e-06\n",
      "Iteration: 7982 Loss: 2.5600379372696084e-06\n",
      "Iteration: 7983 Loss: 2.5576915923766947e-06\n",
      "Iteration: 7984 Loss: 2.5553473979912113e-06\n",
      "Iteration: 7985 Loss: 2.553005352143026e-06\n",
      "Iteration: 7986 Loss: 2.550665452920209e-06\n",
      "Iteration: 7987 Loss: 2.548327698183768e-06\n",
      "Iteration: 7988 Loss: 2.5459920861381995e-06\n",
      "Iteration: 7989 Loss: 2.543658614763008e-06\n",
      "Iteration: 7990 Loss: 2.5413272821549187e-06\n",
      "Iteration: 7991 Loss: 2.538998086295718e-06\n",
      "Iteration: 7992 Loss: 2.5366710251689674e-06\n",
      "Iteration: 7993 Loss: 2.534346096875872e-06\n",
      "Iteration: 7994 Loss: 2.5320232994074084e-06\n",
      "Iteration: 7995 Loss: 2.529702630861598e-06\n",
      "Iteration: 7996 Loss: 2.527384089405792e-06\n",
      "Iteration: 7997 Loss: 2.5250676728619634e-06\n",
      "Iteration: 7998 Loss: 2.522753379450084e-06\n",
      "Iteration: 7999 Loss: 2.5204412071720783e-06\n",
      "Iteration: 8000 Loss: 2.5181311540791165e-06\n",
      "Iteration: 8001 Loss: 2.5158232182346504e-06\n",
      "Iteration: 8002 Loss: 2.513517397694162e-06\n",
      "Iteration: 8003 Loss: 2.5112136905181306e-06\n",
      "Iteration: 8004 Loss: 2.508912094773159e-06\n",
      "Iteration: 8005 Loss: 2.506612608520241e-06\n",
      "Iteration: 8006 Loss: 2.5043152298273826e-06\n",
      "Iteration: 8007 Loss: 2.502019956763284e-06\n",
      "Iteration: 8008 Loss: 2.4997267873718744e-06\n",
      "Iteration: 8009 Loss: 2.49743571991917e-06\n",
      "Iteration: 8010 Loss: 2.495146752170381e-06\n",
      "Iteration: 8011 Loss: 2.4928598823404774e-06\n",
      "Iteration: 8012 Loss: 2.490575108507911e-06\n",
      "Iteration: 8013 Loss: 2.488292428751311e-06\n",
      "Iteration: 8014 Loss: 2.4860118410950456e-06\n",
      "Iteration: 8015 Loss: 2.483733343733171e-06\n",
      "Iteration: 8016 Loss: 2.4814569346947107e-06\n",
      "Iteration: 8017 Loss: 2.479182612067817e-06\n",
      "Iteration: 8018 Loss: 2.4769103739934737e-06\n",
      "Iteration: 8019 Loss: 2.4746402184481363e-06\n",
      "Iteration: 8020 Loss: 2.4723721435803278e-06\n",
      "Iteration: 8021 Loss: 2.470106147428031e-06\n",
      "Iteration: 8022 Loss: 2.467842228197074e-06\n",
      "Iteration: 8023 Loss: 2.465580383871494e-06\n",
      "Iteration: 8024 Loss: 2.4633206126622934e-06\n",
      "Iteration: 8025 Loss: 2.461062912613886e-06\n",
      "Iteration: 8026 Loss: 2.4588072818267746e-06\n",
      "Iteration: 8027 Loss: 2.456553718405742e-06\n",
      "Iteration: 8028 Loss: 2.454302220511724e-06\n",
      "Iteration: 8029 Loss: 2.4520527861384177e-06\n",
      "Iteration: 8030 Loss: 2.449805413591479e-06\n",
      "Iteration: 8031 Loss: 2.4475601007583725e-06\n",
      "Iteration: 8032 Loss: 2.4453168456340987e-06\n",
      "Iteration: 8033 Loss: 2.443075646673847e-06\n",
      "Iteration: 8034 Loss: 2.4408365018512306e-06\n",
      "Iteration: 8035 Loss: 2.438599409342311e-06\n",
      "Iteration: 8036 Loss: 2.4363643671502307e-06\n",
      "Iteration: 8037 Loss: 2.434131373452747e-06\n",
      "Iteration: 8038 Loss: 2.431900426373312e-06\n",
      "Iteration: 8039 Loss: 2.4296715240340586e-06\n",
      "Iteration: 8040 Loss: 2.4274446645647673e-06\n",
      "Iteration: 8041 Loss: 2.425219846089814e-06\n",
      "Iteration: 8042 Loss: 2.4229970668252342e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8043 Loss: 2.4207763247307657e-06\n",
      "Iteration: 8044 Loss: 2.4185576180251584e-06\n",
      "Iteration: 8045 Loss: 2.416340944842787e-06\n",
      "Iteration: 8046 Loss: 2.4141263033751273e-06\n",
      "Iteration: 8047 Loss: 2.411913691564462e-06\n",
      "Iteration: 8048 Loss: 2.4097031077756487e-06\n",
      "Iteration: 8049 Loss: 2.4074945500637157e-06\n",
      "Iteration: 8050 Loss: 2.4052880165755803e-06\n",
      "Iteration: 8051 Loss: 2.4030835054499017e-06\n",
      "Iteration: 8052 Loss: 2.400881014606524e-06\n",
      "Iteration: 8053 Loss: 2.3986805426553544e-06\n",
      "Iteration: 8054 Loss: 2.3964820875148865e-06\n",
      "Iteration: 8055 Loss: 2.3942856474822076e-06\n",
      "Iteration: 8056 Loss: 2.3920912204184015e-06\n",
      "Iteration: 8057 Loss: 2.389898804624341e-06\n",
      "Iteration: 8058 Loss: 2.3877083982554298e-06\n",
      "Iteration: 8059 Loss: 2.385519999474001e-06\n",
      "Iteration: 8060 Loss: 2.383333606435233e-06\n",
      "Iteration: 8061 Loss: 2.381149217302637e-06\n",
      "Iteration: 8062 Loss: 2.3789668302415575e-06\n",
      "Iteration: 8063 Loss: 2.3767864434132097e-06\n",
      "Iteration: 8064 Loss: 2.3746080549877724e-06\n",
      "Iteration: 8065 Loss: 2.372431663132716e-06\n",
      "Iteration: 8066 Loss: 2.370257266016136e-06\n",
      "Iteration: 8067 Loss: 2.3680848618128e-06\n",
      "Iteration: 8068 Loss: 2.365914448694878e-06\n",
      "Iteration: 8069 Loss: 2.363746024773373e-06\n",
      "Iteration: 8070 Loss: 2.3615795884990617e-06\n",
      "Iteration: 8071 Loss: 2.3594151376362234e-06\n",
      "Iteration: 8072 Loss: 2.357252670683252e-06\n",
      "Iteration: 8073 Loss: 2.355092185649927e-06\n",
      "Iteration: 8074 Loss: 2.352933680778717e-06\n",
      "Iteration: 8075 Loss: 2.350777154252718e-06\n",
      "Iteration: 8076 Loss: 2.3486226042586564e-06\n",
      "Iteration: 8077 Loss: 2.346470028987291e-06\n",
      "Iteration: 8078 Loss: 2.3443194266259435e-06\n",
      "Iteration: 8079 Loss: 2.342170795368301e-06\n",
      "Iteration: 8080 Loss: 2.3400241334066726e-06\n",
      "Iteration: 8081 Loss: 2.3378794388234272e-06\n",
      "Iteration: 8082 Loss: 2.3357367100413465e-06\n",
      "Iteration: 8083 Loss: 2.333595945089863e-06\n",
      "Iteration: 8084 Loss: 2.3314571422251725e-06\n",
      "Iteration: 8085 Loss: 2.3293202997047546e-06\n",
      "Iteration: 8086 Loss: 2.3271854156771694e-06\n",
      "Iteration: 8087 Loss: 2.325052488345221e-06\n",
      "Iteration: 8088 Loss: 2.3229215159159306e-06\n",
      "Iteration: 8089 Loss: 2.320792496655069e-06\n",
      "Iteration: 8090 Loss: 2.318665428603744e-06\n",
      "Iteration: 8091 Loss: 2.3165403101986005e-06\n",
      "Iteration: 8092 Loss: 2.3144171395389635e-06\n",
      "Iteration: 8093 Loss: 2.312295914782647e-06\n",
      "Iteration: 8094 Loss: 2.310176634204963e-06\n",
      "Iteration: 8095 Loss: 2.308059296023031e-06\n",
      "Iteration: 8096 Loss: 2.3059438984559134e-06\n",
      "Iteration: 8097 Loss: 2.303830439724291e-06\n",
      "Iteration: 8098 Loss: 2.301718918053034e-06\n",
      "Iteration: 8099 Loss: 2.299609331665624e-06\n",
      "Iteration: 8100 Loss: 2.2975016787895503e-06\n",
      "Iteration: 8101 Loss: 2.29539595765066e-06\n",
      "Iteration: 8102 Loss: 2.293292166480489e-06\n",
      "Iteration: 8103 Loss: 2.291190303509231e-06\n",
      "Iteration: 8104 Loss: 2.289090366969563e-06\n",
      "Iteration: 8105 Loss: 2.2869923550955735e-06\n",
      "Iteration: 8106 Loss: 2.284896266124709e-06\n",
      "Iteration: 8107 Loss: 2.282802098291182e-06\n",
      "Iteration: 8108 Loss: 2.280709849838141e-06\n",
      "Iteration: 8109 Loss: 2.2786195190043494e-06\n",
      "Iteration: 8110 Loss: 2.2765311040327576e-06\n",
      "Iteration: 8111 Loss: 2.274444603166821e-06\n",
      "Iteration: 8112 Loss: 2.2723600145683363e-06\n",
      "Iteration: 8113 Loss: 2.2702773366539146e-06\n",
      "Iteration: 8114 Loss: 2.268196567587358e-06\n",
      "Iteration: 8115 Loss: 2.2661177056452794e-06\n",
      "Iteration: 8116 Loss: 2.26404074908357e-06\n",
      "Iteration: 8117 Loss: 2.2619656960696996e-06\n",
      "Iteration: 8118 Loss: 2.2598925448582174e-06\n",
      "Iteration: 8119 Loss: 2.257821293820278e-06\n",
      "Iteration: 8120 Loss: 2.255751941156064e-06\n",
      "Iteration: 8121 Loss: 2.2536844851842916e-06\n",
      "Iteration: 8122 Loss: 2.2516189240522075e-06\n",
      "Iteration: 8123 Loss: 2.2495552560225822e-06\n",
      "Iteration: 8124 Loss: 2.247493479472405e-06\n",
      "Iteration: 8125 Loss: 2.245433592559203e-06\n",
      "Iteration: 8126 Loss: 2.2433755936620027e-06\n",
      "Iteration: 8127 Loss: 2.2413194809920886e-06\n",
      "Iteration: 8128 Loss: 2.2392652528220595e-06\n",
      "Iteration: 8129 Loss: 2.237212907426226e-06\n",
      "Iteration: 8130 Loss: 2.235162443077112e-06\n",
      "Iteration: 8131 Loss: 2.2331138580492456e-06\n",
      "Iteration: 8132 Loss: 2.231067150623597e-06\n",
      "Iteration: 8133 Loss: 2.229022319077073e-06\n",
      "Iteration: 8134 Loss: 2.2269793616897898e-06\n",
      "Iteration: 8135 Loss: 2.2249382767467625e-06\n",
      "Iteration: 8136 Loss: 2.2228990625298053e-06\n",
      "Iteration: 8137 Loss: 2.220861717324323e-06\n",
      "Iteration: 8138 Loss: 2.2188262394192477e-06\n",
      "Iteration: 8139 Loss: 2.216792627155795e-06\n",
      "Iteration: 8140 Loss: 2.2147608787715485e-06\n",
      "Iteration: 8141 Loss: 2.2127309924428138e-06\n",
      "Iteration: 8142 Loss: 2.2107029665770033e-06\n",
      "Iteration: 8143 Loss: 2.2086767995232723e-06\n",
      "Iteration: 8144 Loss: 2.2066524895244048e-06\n",
      "Iteration: 8145 Loss: 2.2046300348776654e-06\n",
      "Iteration: 8146 Loss: 2.2026094338822802e-06\n",
      "Iteration: 8147 Loss: 2.2005906848367028e-06\n",
      "Iteration: 8148 Loss: 2.1985737860475613e-06\n",
      "Iteration: 8149 Loss: 2.196558735818727e-06\n",
      "Iteration: 8150 Loss: 2.1945455324527297e-06\n",
      "Iteration: 8151 Loss: 2.1925341742580506e-06\n",
      "Iteration: 8152 Loss: 2.190524659547433e-06\n",
      "Iteration: 8153 Loss: 2.188516986624648e-06\n",
      "Iteration: 8154 Loss: 2.1865111538085343e-06\n",
      "Iteration: 8155 Loss: 2.1845071594061377e-06\n",
      "Iteration: 8156 Loss: 2.1825050017915905e-06\n",
      "Iteration: 8157 Loss: 2.180504679228823e-06\n",
      "Iteration: 8158 Loss: 2.1785061899711532e-06\n",
      "Iteration: 8159 Loss: 2.1765095324038226e-06\n",
      "Iteration: 8160 Loss: 2.174514704841009e-06\n",
      "Iteration: 8161 Loss: 2.172521705609839e-06\n",
      "Iteration: 8162 Loss: 2.1705305330308772e-06\n",
      "Iteration: 8163 Loss: 2.1685411854341013e-06\n",
      "Iteration: 8164 Loss: 2.166553661058901e-06\n",
      "Iteration: 8165 Loss: 2.1645679584873692e-06\n",
      "Iteration: 8166 Loss: 2.162584075801047e-06\n",
      "Iteration: 8167 Loss: 2.1606020114116774e-06\n",
      "Iteration: 8168 Loss: 2.1586217636541e-06\n",
      "Iteration: 8169 Loss: 2.1566433308647517e-06\n",
      "Iteration: 8170 Loss: 2.154666711320416e-06\n",
      "Iteration: 8171 Loss: 2.1526919034182743e-06\n",
      "Iteration: 8172 Loss: 2.150718905553786e-06\n",
      "Iteration: 8173 Loss: 2.148747716070132e-06\n",
      "Iteration: 8174 Loss: 2.146778333135485e-06\n",
      "Iteration: 8175 Loss: 2.144810755268547e-06\n",
      "Iteration: 8176 Loss: 2.1428449807549174e-06\n",
      "Iteration: 8177 Loss: 2.1408810079432516e-06\n",
      "Iteration: 8178 Loss: 2.1389188351802056e-06\n",
      "Iteration: 8179 Loss: 2.1369584608769244e-06\n",
      "Iteration: 8180 Loss: 2.1349998833264075e-06\n",
      "Iteration: 8181 Loss: 2.1330431008837326e-06\n",
      "Iteration: 8182 Loss: 2.1310881118400487e-06\n",
      "Iteration: 8183 Loss: 2.1291349146147184e-06\n",
      "Iteration: 8184 Loss: 2.127183507563107e-06\n",
      "Iteration: 8185 Loss: 2.125233889044886e-06\n",
      "Iteration: 8186 Loss: 2.1232860574217748e-06\n",
      "Iteration: 8187 Loss: 2.1213400110547318e-06\n",
      "Iteration: 8188 Loss: 2.1193957483079624e-06\n",
      "Iteration: 8189 Loss: 2.117453267545011e-06\n",
      "Iteration: 8190 Loss: 2.1155125671377202e-06\n",
      "Iteration: 8191 Loss: 2.1135736455047727e-06\n",
      "Iteration: 8192 Loss: 2.1116365009061153e-06\n",
      "Iteration: 8193 Loss: 2.1097011317696902e-06\n",
      "Iteration: 8194 Loss: 2.1077675364674444e-06\n",
      "Iteration: 8195 Loss: 2.105835713371779e-06\n",
      "Iteration: 8196 Loss: 2.1039056608616477e-06\n",
      "Iteration: 8197 Loss: 2.1019773773115615e-06\n",
      "Iteration: 8198 Loss: 2.100050861104176e-06\n",
      "Iteration: 8199 Loss: 2.098126110501519e-06\n",
      "Iteration: 8200 Loss: 2.0962031241145733e-06\n",
      "Iteration: 8201 Loss: 2.0942819001271744e-06\n",
      "Iteration: 8202 Loss: 2.0923624370954793e-06\n",
      "Iteration: 8203 Loss: 2.090444733316486e-06\n",
      "Iteration: 8204 Loss: 2.088528787183692e-06\n",
      "Iteration: 8205 Loss: 2.0866145970803737e-06\n",
      "Iteration: 8206 Loss: 2.084702161347777e-06\n",
      "Iteration: 8207 Loss: 2.0827914784841437e-06\n",
      "Iteration: 8208 Loss: 2.0808825468287136e-06\n",
      "Iteration: 8209 Loss: 2.0789753648025636e-06\n",
      "Iteration: 8210 Loss: 2.0770699307515583e-06\n",
      "Iteration: 8211 Loss: 2.075166243096893e-06\n",
      "Iteration: 8212 Loss: 2.073264300293872e-06\n",
      "Iteration: 8213 Loss: 2.071364100634396e-06\n",
      "Iteration: 8214 Loss: 2.069465642519633e-06\n",
      "Iteration: 8215 Loss: 2.0675689244640693e-06\n",
      "Iteration: 8216 Loss: 2.0656739448169704e-06\n",
      "Iteration: 8217 Loss: 2.0637807019859826e-06\n",
      "Iteration: 8218 Loss: 2.061889194378825e-06\n",
      "Iteration: 8219 Loss: 2.0599994204050056e-06\n",
      "Iteration: 8220 Loss: 2.0581113784753564e-06\n",
      "Iteration: 8221 Loss: 2.0562250670027396e-06\n",
      "Iteration: 8222 Loss: 2.0543404844022736e-06\n",
      "Iteration: 8223 Loss: 2.0524576291451397e-06\n",
      "Iteration: 8224 Loss: 2.0505764995923016e-06\n",
      "Iteration: 8225 Loss: 2.0486970941032108e-06\n",
      "Iteration: 8226 Loss: 2.046819411154589e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8227 Loss: 2.0449434491689467e-06\n",
      "Iteration: 8228 Loss: 2.043069206571006e-06\n",
      "Iteration: 8229 Loss: 2.041196681780904e-06\n",
      "Iteration: 8230 Loss: 2.039325873226065e-06\n",
      "Iteration: 8231 Loss: 2.037456779333592e-06\n",
      "Iteration: 8232 Loss: 2.0355893985315804e-06\n",
      "Iteration: 8233 Loss: 2.0337237292496466e-06\n",
      "Iteration: 8234 Loss: 2.031859769862909e-06\n",
      "Iteration: 8235 Loss: 2.0299975189751646e-06\n",
      "Iteration: 8236 Loss: 2.0281369748485116e-06\n",
      "Iteration: 8237 Loss: 2.026278135976153e-06\n",
      "Iteration: 8238 Loss: 2.0244210007952725e-06\n",
      "Iteration: 8239 Loss: 2.0225655677450435e-06\n",
      "Iteration: 8240 Loss: 2.0207118352649225e-06\n",
      "Iteration: 8241 Loss: 2.0188598017959175e-06\n",
      "Iteration: 8242 Loss: 2.0170094657225676e-06\n",
      "Iteration: 8243 Loss: 2.0151608257461605e-06\n",
      "Iteration: 8244 Loss: 2.0133138799726927e-06\n",
      "Iteration: 8245 Loss: 2.011468626991282e-06\n",
      "Iteration: 8246 Loss: 2.0096250652497015e-06\n",
      "Iteration: 8247 Loss: 2.0077831931974688e-06\n",
      "Iteration: 8248 Loss: 2.0059430092856595e-06\n",
      "Iteration: 8249 Loss: 2.0041045119674903e-06\n",
      "Iteration: 8250 Loss: 2.0022676996968156e-06\n",
      "Iteration: 8251 Loss: 2.0004325709267313e-06\n",
      "Iteration: 8252 Loss: 1.9985991241208322e-06\n",
      "Iteration: 8253 Loss: 1.9967673577329822e-06\n",
      "Iteration: 8254 Loss: 1.9949372701383807e-06\n",
      "Iteration: 8255 Loss: 1.9931088599690528e-06\n",
      "Iteration: 8256 Loss: 1.9912821256021694e-06\n",
      "Iteration: 8257 Loss: 1.989457065500486e-06\n",
      "Iteration: 8258 Loss: 1.9876336781324135e-06\n",
      "Iteration: 8259 Loss: 1.9858119619608807e-06\n",
      "Iteration: 8260 Loss: 1.983991915460006e-06\n",
      "Iteration: 8261 Loss: 1.982173537093718e-06\n",
      "Iteration: 8262 Loss: 1.9803568253366783e-06\n",
      "Iteration: 8263 Loss: 1.978541778602417e-06\n",
      "Iteration: 8264 Loss: 1.9767283955344742e-06\n",
      "Iteration: 8265 Loss: 1.9749166744410992e-06\n",
      "Iteration: 8266 Loss: 1.9731066138556134e-06\n",
      "Iteration: 8267 Loss: 1.971298212197832e-06\n",
      "Iteration: 8268 Loss: 1.969491468115582e-06\n",
      "Iteration: 8269 Loss: 1.967686379923407e-06\n",
      "Iteration: 8270 Loss: 1.9658829460977072e-06\n",
      "Iteration: 8271 Loss: 1.964081165186871e-06\n",
      "Iteration: 8272 Loss: 1.962281035784085e-06\n",
      "Iteration: 8273 Loss: 1.9604825562642907e-06\n",
      "Iteration: 8274 Loss: 1.9586857250545373e-06\n",
      "Iteration: 8275 Loss: 1.956890540559232e-06\n",
      "Iteration: 8276 Loss: 1.955097001561356e-06\n",
      "Iteration: 8277 Loss: 1.953305106405665e-06\n",
      "Iteration: 8278 Loss: 1.9515148535833324e-06\n",
      "Iteration: 8279 Loss: 1.9497262417394855e-06\n",
      "Iteration: 8280 Loss: 1.9479392690743386e-06\n",
      "Iteration: 8281 Loss: 1.94615393423234e-06\n",
      "Iteration: 8282 Loss: 1.9443702357694376e-06\n",
      "Iteration: 8283 Loss: 1.942588172015659e-06\n",
      "Iteration: 8284 Loss: 1.9408077416437354e-06\n",
      "Iteration: 8285 Loss: 1.9390289430994676e-06\n",
      "Iteration: 8286 Loss: 1.9372517748291638e-06\n",
      "Iteration: 8287 Loss: 1.935476235451251e-06\n",
      "Iteration: 8288 Loss: 1.933702323422008e-06\n",
      "Iteration: 8289 Loss: 1.93193003732766e-06\n",
      "Iteration: 8290 Loss: 1.9301593755104393e-06\n",
      "Iteration: 8291 Loss: 1.9283903365151203e-06\n",
      "Iteration: 8292 Loss: 1.926622918958851e-06\n",
      "Iteration: 8293 Loss: 1.9248571213062797e-06\n",
      "Iteration: 8294 Loss: 1.923092942067887e-06\n",
      "Iteration: 8295 Loss: 1.921330379762636e-06\n",
      "Iteration: 8296 Loss: 1.919569432907191e-06\n",
      "Iteration: 8297 Loss: 1.9178101000204833e-06\n",
      "Iteration: 8298 Loss: 1.9160523797409856e-06\n",
      "Iteration: 8299 Loss: 1.914296270302095e-06\n",
      "Iteration: 8300 Loss: 1.912541770460109e-06\n",
      "Iteration: 8301 Loss: 1.9107888786167083e-06\n",
      "Iteration: 8302 Loss: 1.90903759341885e-06\n",
      "Iteration: 8303 Loss: 1.907287913452169e-06\n",
      "Iteration: 8304 Loss: 1.9055398370144739e-06\n",
      "Iteration: 8305 Loss: 1.9037933627504205e-06\n",
      "Iteration: 8306 Loss: 1.9020484891912746e-06\n",
      "Iteration: 8307 Loss: 1.9003052148679296e-06\n",
      "Iteration: 8308 Loss: 1.8985635383221893e-06\n",
      "Iteration: 8309 Loss: 1.8968234580809998e-06\n",
      "Iteration: 8310 Loss: 1.8950849726865433e-06\n",
      "Iteration: 8311 Loss: 1.893348080526496e-06\n",
      "Iteration: 8312 Loss: 1.8916127804379791e-06\n",
      "Iteration: 8313 Loss: 1.8898790708112363e-06\n",
      "Iteration: 8314 Loss: 1.8881469501932879e-06\n",
      "Iteration: 8315 Loss: 1.8864164171228505e-06\n",
      "Iteration: 8316 Loss: 1.884687470064378e-06\n",
      "Iteration: 8317 Loss: 1.88296010773085e-06\n",
      "Iteration: 8318 Loss: 1.8812343285841748e-06\n",
      "Iteration: 8319 Loss: 1.879510131325761e-06\n",
      "Iteration: 8320 Loss: 1.8777875142028736e-06\n",
      "Iteration: 8321 Loss: 1.8760664758691373e-06\n",
      "Iteration: 8322 Loss: 1.8743470149776087e-06\n",
      "Iteration: 8323 Loss: 1.8726291300940705e-06\n",
      "Iteration: 8324 Loss: 1.8709128196549519e-06\n",
      "Iteration: 8325 Loss: 1.8691980823593966e-06\n",
      "Iteration: 8326 Loss: 1.8674849167456728e-06\n",
      "Iteration: 8327 Loss: 1.865773321159778e-06\n",
      "Iteration: 8328 Loss: 1.864063294163727e-06\n",
      "Iteration: 8329 Loss: 1.8623548345313709e-06\n",
      "Iteration: 8330 Loss: 1.8606479408483163e-06\n",
      "Iteration: 8331 Loss: 1.8589426115922511e-06\n",
      "Iteration: 8332 Loss: 1.8572388453285217e-06\n",
      "Iteration: 8333 Loss: 1.8555366406309502e-06\n",
      "Iteration: 8334 Loss: 1.8538359960630356e-06\n",
      "Iteration: 8335 Loss: 1.852136910195289e-06\n",
      "Iteration: 8336 Loss: 1.8504393816549927e-06\n",
      "Iteration: 8337 Loss: 1.848743408961373e-06\n",
      "Iteration: 8338 Loss: 1.8470489906293862e-06\n",
      "Iteration: 8339 Loss: 1.845356125292218e-06\n",
      "Iteration: 8340 Loss: 1.8436648115253963e-06\n",
      "Iteration: 8341 Loss: 1.841975047759657e-06\n",
      "Iteration: 8342 Loss: 1.840286832869872e-06\n",
      "Iteration: 8343 Loss: 1.8386001652871854e-06\n",
      "Iteration: 8344 Loss: 1.8369150437448164e-06\n",
      "Iteration: 8345 Loss: 1.8352314665271923e-06\n",
      "Iteration: 8346 Loss: 1.8335494323690775e-06\n",
      "Iteration: 8347 Loss: 1.8318689398525281e-06\n",
      "Iteration: 8348 Loss: 1.8301899875682997e-06\n",
      "Iteration: 8349 Loss: 1.8285125741021001e-06\n",
      "Iteration: 8350 Loss: 1.8268366980457713e-06\n",
      "Iteration: 8351 Loss: 1.8251623579865524e-06\n",
      "Iteration: 8352 Loss: 1.8234895525216569e-06\n",
      "Iteration: 8353 Loss: 1.821818280241477e-06\n",
      "Iteration: 8354 Loss: 1.8201485397418528e-06\n",
      "Iteration: 8355 Loss: 1.818480329618235e-06\n",
      "Iteration: 8356 Loss: 1.8168136484662314e-06\n",
      "Iteration: 8357 Loss: 1.8151484947395508e-06\n",
      "Iteration: 8358 Loss: 1.8134848673331282e-06\n",
      "Iteration: 8359 Loss: 1.8118227647008526e-06\n",
      "Iteration: 8360 Loss: 1.8101621854439964e-06\n",
      "Iteration: 8361 Loss: 1.8085031281662519e-06\n",
      "Iteration: 8362 Loss: 1.806845591472244e-06\n",
      "Iteration: 8363 Loss: 1.8051895741182752e-06\n",
      "Iteration: 8364 Loss: 1.8035350744141431e-06\n",
      "Iteration: 8365 Loss: 1.8018820911179618e-06\n",
      "Iteration: 8366 Loss: 1.8002306228362738e-06\n",
      "Iteration: 8367 Loss: 1.7985806681879225e-06\n",
      "Iteration: 8368 Loss: 1.796932225780921e-06\n",
      "Iteration: 8369 Loss: 1.7952852942288664e-06\n",
      "Iteration: 8370 Loss: 1.793639872147928e-06\n",
      "Iteration: 8371 Loss: 1.7919959581532324e-06\n",
      "Iteration: 8372 Loss: 1.7903535508646043e-06\n",
      "Iteration: 8373 Loss: 1.7887126488984388e-06\n",
      "Iteration: 8374 Loss: 1.7870732508805773e-06\n",
      "Iteration: 8375 Loss: 1.7854353554264702e-06\n",
      "Iteration: 8376 Loss: 1.783798961160877e-06\n",
      "Iteration: 8377 Loss: 1.7821640667094758e-06\n",
      "Iteration: 8378 Loss: 1.7805306705834353e-06\n",
      "Iteration: 8379 Loss: 1.778898771636304e-06\n",
      "Iteration: 8380 Loss: 1.7772683683805818e-06\n",
      "Iteration: 8381 Loss: 1.7756394594482065e-06\n",
      "Iteration: 8382 Loss: 1.7740120434699792e-06\n",
      "Iteration: 8383 Loss: 1.772386119072794e-06\n",
      "Iteration: 8384 Loss: 1.7707616848960027e-06\n",
      "Iteration: 8385 Loss: 1.769138739568187e-06\n",
      "Iteration: 8386 Loss: 1.7675172817247526e-06\n",
      "Iteration: 8387 Loss: 1.7658973100101935e-06\n",
      "Iteration: 8388 Loss: 1.7642788229971437e-06\n",
      "Iteration: 8389 Loss: 1.762661819442292e-06\n",
      "Iteration: 8390 Loss: 1.7610462979265207e-06\n",
      "Iteration: 8391 Loss: 1.7594322571519624e-06\n",
      "Iteration: 8392 Loss: 1.757819695646661e-06\n",
      "Iteration: 8393 Loss: 1.7562086121082994e-06\n",
      "Iteration: 8394 Loss: 1.7545990051312344e-06\n",
      "Iteration: 8395 Loss: 1.7529908734133798e-06\n",
      "Iteration: 8396 Loss: 1.7513842157193333e-06\n",
      "Iteration: 8397 Loss: 1.7497790304672593e-06\n",
      "Iteration: 8398 Loss: 1.7481753164868037e-06\n",
      "Iteration: 8399 Loss: 1.7465730723352608e-06\n",
      "Iteration: 8400 Loss: 1.7449722968681953e-06\n",
      "Iteration: 8401 Loss: 1.7433729884264681e-06\n",
      "Iteration: 8402 Loss: 1.7417751458058849e-06\n",
      "Iteration: 8403 Loss: 1.740178767665631e-06\n",
      "Iteration: 8404 Loss: 1.738583852663129e-06\n",
      "Iteration: 8405 Loss: 1.7369903994544008e-06\n",
      "Iteration: 8406 Loss: 1.7353984067041206e-06\n",
      "Iteration: 8407 Loss: 1.7338078730676706e-06\n",
      "Iteration: 8408 Loss: 1.7322187971601117e-06\n",
      "Iteration: 8409 Loss: 1.7306311776931852e-06\n",
      "Iteration: 8410 Loss: 1.7290450133939135e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8411 Loss: 1.7274603028695329e-06\n",
      "Iteration: 8412 Loss: 1.7258770448451047e-06\n",
      "Iteration: 8413 Loss: 1.724295237819208e-06\n",
      "Iteration: 8414 Loss: 1.7227148806886177e-06\n",
      "Iteration: 8415 Loss: 1.7211359718981976e-06\n",
      "Iteration: 8416 Loss: 1.7195585102894981e-06\n",
      "Iteration: 8417 Loss: 1.717982494481414e-06\n",
      "Iteration: 8418 Loss: 1.7164079232017089e-06\n",
      "Iteration: 8419 Loss: 1.7148347950178802e-06\n",
      "Iteration: 8420 Loss: 1.713263108604213e-06\n",
      "Iteration: 8421 Loss: 1.7116928626038523e-06\n",
      "Iteration: 8422 Loss: 1.7101240559402363e-06\n",
      "Iteration: 8423 Loss: 1.7085566873457052e-06\n",
      "Iteration: 8424 Loss: 1.7069907551532985e-06\n",
      "Iteration: 8425 Loss: 1.7054262581348123e-06\n",
      "Iteration: 8426 Loss: 1.703863194978132e-06\n",
      "Iteration: 8427 Loss: 1.7023015644222802e-06\n",
      "Iteration: 8428 Loss: 1.7007413652705655e-06\n",
      "Iteration: 8429 Loss: 1.6991825960969052e-06\n",
      "Iteration: 8430 Loss: 1.6976252555313955e-06\n",
      "Iteration: 8431 Loss: 1.6960693423260161e-06\n",
      "Iteration: 8432 Loss: 1.6945148551696779e-06\n",
      "Iteration: 8433 Loss: 1.6929617927545937e-06\n",
      "Iteration: 8434 Loss: 1.6914101537765436e-06\n",
      "Iteration: 8435 Loss: 1.6898599368736775e-06\n",
      "Iteration: 8436 Loss: 1.6883111409133255e-06\n",
      "Iteration: 8437 Loss: 1.686763764421232e-06\n",
      "Iteration: 8438 Loss: 1.6852178060091935e-06\n",
      "Iteration: 8439 Loss: 1.6836732646684143e-06\n",
      "Iteration: 8440 Loss: 1.6821301391024013e-06\n",
      "Iteration: 8441 Loss: 1.6805884277169832e-06\n",
      "Iteration: 8442 Loss: 1.6790481293641354e-06\n",
      "Iteration: 8443 Loss: 1.6775092428055968e-06\n",
      "Iteration: 8444 Loss: 1.6759717666095412e-06\n",
      "Iteration: 8445 Loss: 1.6744356995910287e-06\n",
      "Iteration: 8446 Loss: 1.672901040432153e-06\n",
      "Iteration: 8447 Loss: 1.6713677878417595e-06\n",
      "Iteration: 8448 Loss: 1.669835940531355e-06\n",
      "Iteration: 8449 Loss: 1.6683054972152672e-06\n",
      "Iteration: 8450 Loss: 1.6667764566863944e-06\n",
      "Iteration: 8451 Loss: 1.665248817496099e-06\n",
      "Iteration: 8452 Loss: 1.6637225784390195e-06\n",
      "Iteration: 8453 Loss: 1.6621977381481952e-06\n",
      "Iteration: 8454 Loss: 1.6606742954301778e-06\n",
      "Iteration: 8455 Loss: 1.6591522491675828e-06\n",
      "Iteration: 8456 Loss: 1.6576315978295222e-06\n",
      "Iteration: 8457 Loss: 1.6561123402252277e-06\n",
      "Iteration: 8458 Loss: 1.654594475070209e-06\n",
      "Iteration: 8459 Loss: 1.653078001096135e-06\n",
      "Iteration: 8460 Loss: 1.6515629170216161e-06\n",
      "Iteration: 8461 Loss: 1.6500492215759585e-06\n",
      "Iteration: 8462 Loss: 1.648536913484698e-06\n",
      "Iteration: 8463 Loss: 1.6470259915605266e-06\n",
      "Iteration: 8464 Loss: 1.645516454369211e-06\n",
      "Iteration: 8465 Loss: 1.644008300721012e-06\n",
      "Iteration: 8466 Loss: 1.6425015293462428e-06\n",
      "Iteration: 8467 Loss: 1.6409961389834457e-06\n",
      "Iteration: 8468 Loss: 1.6394921284467093e-06\n",
      "Iteration: 8469 Loss: 1.6379894963058014e-06\n",
      "Iteration: 8470 Loss: 1.6364882412941455e-06\n",
      "Iteration: 8471 Loss: 1.6349883623216255e-06\n",
      "Iteration: 8472 Loss: 1.6334898580413364e-06\n",
      "Iteration: 8473 Loss: 1.6319927271916865e-06\n",
      "Iteration: 8474 Loss: 1.6304969685166474e-06\n",
      "Iteration: 8475 Loss: 1.6290025807551803e-06\n",
      "Iteration: 8476 Loss: 1.627509562655245e-06\n",
      "Iteration: 8477 Loss: 1.6260179129584455e-06\n",
      "Iteration: 8478 Loss: 1.6245276304112946e-06\n",
      "Iteration: 8479 Loss: 1.6230387137602647e-06\n",
      "Iteration: 8480 Loss: 1.6215511617546249e-06\n",
      "Iteration: 8481 Loss: 1.6200649731423422e-06\n",
      "Iteration: 8482 Loss: 1.6185801466749587e-06\n",
      "Iteration: 8483 Loss: 1.6170966811034616e-06\n",
      "Iteration: 8484 Loss: 1.6156145751803908e-06\n",
      "Iteration: 8485 Loss: 1.614133827659336e-06\n",
      "Iteration: 8486 Loss: 1.6126544372972122e-06\n",
      "Iteration: 8487 Loss: 1.6111764028460198e-06\n",
      "Iteration: 8488 Loss: 1.6096997230674511e-06\n",
      "Iteration: 8489 Loss: 1.60822439671771e-06\n",
      "Iteration: 8490 Loss: 1.6067504225560042e-06\n",
      "Iteration: 8491 Loss: 1.6052777993455818e-06\n",
      "Iteration: 8492 Loss: 1.6038065258452259e-06\n",
      "Iteration: 8493 Loss: 1.6023366008181789e-06\n",
      "Iteration: 8494 Loss: 1.6008680230299711e-06\n",
      "Iteration: 8495 Loss: 1.5994007912442272e-06\n",
      "Iteration: 8496 Loss: 1.5979349041721361e-06\n",
      "Iteration: 8497 Loss: 1.5964703606376806e-06\n",
      "Iteration: 8498 Loss: 1.5950071594664136e-06\n",
      "Iteration: 8499 Loss: 1.5935452993136336e-06\n",
      "Iteration: 8500 Loss: 1.592084779006451e-06\n",
      "Iteration: 8501 Loss: 1.5906255973751164e-06\n",
      "Iteration: 8502 Loss: 1.5891677531351824e-06\n",
      "Iteration: 8503 Loss: 1.587711245061657e-06\n",
      "Iteration: 8504 Loss: 1.5862560719269191e-06\n",
      "Iteration: 8505 Loss: 1.5848022325123384e-06\n",
      "Iteration: 8506 Loss: 1.5833497255923583e-06\n",
      "Iteration: 8507 Loss: 1.5818985500583081e-06\n",
      "Iteration: 8508 Loss: 1.5804487046141406e-06\n",
      "Iteration: 8509 Loss: 1.5790001877076092e-06\n",
      "Iteration: 8510 Loss: 1.5775529985643288e-06\n",
      "Iteration: 8511 Loss: 1.5761071358257061e-06\n",
      "Iteration: 8512 Loss: 1.5746625982688978e-06\n",
      "Iteration: 8513 Loss: 1.5732193846815948e-06\n",
      "Iteration: 8514 Loss: 1.5717774938504625e-06\n",
      "Iteration: 8515 Loss: 1.5703369245634236e-06\n",
      "Iteration: 8516 Loss: 1.5688976755504144e-06\n",
      "Iteration: 8517 Loss: 1.5674597457760505e-06\n",
      "Iteration: 8518 Loss: 1.5660231338576567e-06\n",
      "Iteration: 8519 Loss: 1.5645878386447079e-06\n",
      "Iteration: 8520 Loss: 1.563153858929786e-06\n",
      "Iteration: 8521 Loss: 1.5617211934516343e-06\n",
      "Iteration: 8522 Loss: 1.5602898410620977e-06\n",
      "Iteration: 8523 Loss: 1.5588598006144486e-06\n",
      "Iteration: 8524 Loss: 1.5574310708497076e-06\n",
      "Iteration: 8525 Loss: 1.5560036505665025e-06\n",
      "Iteration: 8526 Loss: 1.5545775385622644e-06\n",
      "Iteration: 8527 Loss: 1.5531527336415326e-06\n",
      "Iteration: 8528 Loss: 1.5517292346045662e-06\n",
      "Iteration: 8529 Loss: 1.550307040255275e-06\n",
      "Iteration: 8530 Loss: 1.5488861493962869e-06\n",
      "Iteration: 8531 Loss: 1.54746656083421e-06\n",
      "Iteration: 8532 Loss: 1.5460482733769e-06\n",
      "Iteration: 8533 Loss: 1.5446312858294743e-06\n",
      "Iteration: 8534 Loss: 1.5432155969989681e-06\n",
      "Iteration: 8535 Loss: 1.5418012055532426e-06\n",
      "Iteration: 8536 Loss: 1.5403881105923778e-06\n",
      "Iteration: 8537 Loss: 1.5389763107835538e-06\n",
      "Iteration: 8538 Loss: 1.5375658051433862e-06\n",
      "Iteration: 8539 Loss: 1.5361565921345045e-06\n",
      "Iteration: 8540 Loss: 1.5347486706616577e-06\n",
      "Iteration: 8541 Loss: 1.5333420395980679e-06\n",
      "Iteration: 8542 Loss: 1.531936697763462e-06\n",
      "Iteration: 8543 Loss: 1.5305326439723618e-06\n",
      "Iteration: 8544 Loss: 1.529129877045859e-06\n",
      "Iteration: 8545 Loss: 1.5277283958060482e-06\n",
      "Iteration: 8546 Loss: 1.526328199073084e-06\n",
      "Iteration: 8547 Loss: 1.524929285668967e-06\n",
      "Iteration: 8548 Loss: 1.5235316544202337e-06\n",
      "Iteration: 8549 Loss: 1.5221353041501296e-06\n",
      "Iteration: 8550 Loss: 1.5207402336837623e-06\n",
      "Iteration: 8551 Loss: 1.5193464418482142e-06\n",
      "Iteration: 8552 Loss: 1.5179539274702394e-06\n",
      "Iteration: 8553 Loss: 1.5165626893847676e-06\n",
      "Iteration: 8554 Loss: 1.5151727264147737e-06\n",
      "Iteration: 8555 Loss: 1.5137840373976869e-06\n",
      "Iteration: 8556 Loss: 1.5123966211614136e-06\n",
      "Iteration: 8557 Loss: 1.511010476541556e-06\n",
      "Iteration: 8558 Loss: 1.50962560231557e-06\n",
      "Iteration: 8559 Loss: 1.5082419974321835e-06\n",
      "Iteration: 8560 Loss: 1.506859660613091e-06\n",
      "Iteration: 8561 Loss: 1.5054785908142679e-06\n",
      "Iteration: 8562 Loss: 1.504098786813485e-06\n",
      "Iteration: 8563 Loss: 1.5027202474519824e-06\n",
      "Iteration: 8564 Loss: 1.5013429715701792e-06\n",
      "Iteration: 8565 Loss: 1.4999669580102157e-06\n",
      "Iteration: 8566 Loss: 1.4985922056158693e-06\n",
      "Iteration: 8567 Loss: 1.4972187132297156e-06\n",
      "Iteration: 8568 Loss: 1.4958464797563258e-06\n",
      "Iteration: 8569 Loss: 1.4944755041292972e-06\n",
      "Iteration: 8570 Loss: 1.4931057846985391e-06\n",
      "Iteration: 8571 Loss: 1.4917373208126994e-06\n",
      "Iteration: 8572 Loss: 1.4903701111698687e-06\n",
      "Iteration: 8573 Loss: 1.4890041546827148e-06\n",
      "Iteration: 8574 Loss: 1.4876394501410025e-06\n",
      "Iteration: 8575 Loss: 1.4862759963436438e-06\n",
      "Iteration: 8576 Loss: 1.4849137921985143e-06\n",
      "Iteration: 8577 Loss: 1.4835528365636169e-06\n",
      "Iteration: 8578 Loss: 1.4821931282905848e-06\n",
      "Iteration: 8579 Loss: 1.480834666153808e-06\n",
      "Iteration: 8580 Loss: 1.4794774491820714e-06\n",
      "Iteration: 8581 Loss: 1.4781214761455724e-06\n",
      "Iteration: 8582 Loss: 1.4767667460541553e-06\n",
      "Iteration: 8583 Loss: 1.475413257473456e-06\n",
      "Iteration: 8584 Loss: 1.4740610094970238e-06\n",
      "Iteration: 8585 Loss: 1.4727100008189074e-06\n",
      "Iteration: 8586 Loss: 1.471360230241649e-06\n",
      "Iteration: 8587 Loss: 1.4700116969245396e-06\n",
      "Iteration: 8588 Loss: 1.4686643995845734e-06\n",
      "Iteration: 8589 Loss: 1.4673183370908066e-06\n",
      "Iteration: 8590 Loss: 1.4659735083103235e-06\n",
      "Iteration: 8591 Loss: 1.4646299121137963e-06\n",
      "Iteration: 8592 Loss: 1.463287547370159e-06\n",
      "Iteration: 8593 Loss: 1.4619464129518443e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8594 Loss: 1.46060650772882e-06\n",
      "Iteration: 8595 Loss: 1.459267830577337e-06\n",
      "Iteration: 8596 Loss: 1.4579303803718114e-06\n",
      "Iteration: 8597 Loss: 1.4565941559856146e-06\n",
      "Iteration: 8598 Loss: 1.4552591562966635e-06\n",
      "Iteration: 8599 Loss: 1.4539253801807049e-06\n",
      "Iteration: 8600 Loss: 1.4525928265181816e-06\n",
      "Iteration: 8601 Loss: 1.4512614941897306e-06\n",
      "Iteration: 8602 Loss: 1.4499313820745942e-06\n",
      "Iteration: 8603 Loss: 1.4486024890515987e-06\n",
      "Iteration: 8604 Loss: 1.4472748141539908e-06\n",
      "Iteration: 8605 Loss: 1.445948355824885e-06\n",
      "Iteration: 8606 Loss: 1.4446231133879464e-06\n",
      "Iteration: 8607 Loss: 1.4432990855821691e-06\n",
      "Iteration: 8608 Loss: 1.4419762712946848e-06\n",
      "Iteration: 8609 Loss: 1.4406546694113057e-06\n",
      "Iteration: 8610 Loss: 1.4393342788256248e-06\n",
      "Iteration: 8611 Loss: 1.438015098423694e-06\n",
      "Iteration: 8612 Loss: 1.4366971270968148e-06\n",
      "Iteration: 8613 Loss: 1.435380363737229e-06\n",
      "Iteration: 8614 Loss: 1.434064807237214e-06\n",
      "Iteration: 8615 Loss: 1.4327504564924903e-06\n",
      "Iteration: 8616 Loss: 1.4314373103958539e-06\n",
      "Iteration: 8617 Loss: 1.4301253678448132e-06\n",
      "Iteration: 8618 Loss: 1.4288146277330566e-06\n",
      "Iteration: 8619 Loss: 1.4275050889647387e-06\n",
      "Iteration: 8620 Loss: 1.4261967504315873e-06\n",
      "Iteration: 8621 Loss: 1.4248896110384176e-06\n",
      "Iteration: 8622 Loss: 1.4235836696838867e-06\n",
      "Iteration: 8623 Loss: 1.4222789252713297e-06\n",
      "Iteration: 8624 Loss: 1.420975376704382e-06\n",
      "Iteration: 8625 Loss: 1.4196730228839675e-06\n",
      "Iteration: 8626 Loss: 1.4183718626596992e-06\n",
      "Iteration: 8627 Loss: 1.4170718951095328e-06\n",
      "Iteration: 8628 Loss: 1.4157731189680392e-06\n",
      "Iteration: 8629 Loss: 1.4144755332013449e-06\n",
      "Iteration: 8630 Loss: 1.4131791367180607e-06\n",
      "Iteration: 8631 Loss: 1.4118839284277202e-06\n",
      "Iteration: 8632 Loss: 1.4105899072398704e-06\n",
      "Iteration: 8633 Loss: 1.4092970719869722e-06\n",
      "Iteration: 8634 Loss: 1.4080054217457352e-06\n",
      "Iteration: 8635 Loss: 1.4067149553461658e-06\n",
      "Iteration: 8636 Loss: 1.4054256717916747e-06\n",
      "Iteration: 8637 Loss: 1.4041375698258104e-06\n",
      "Iteration: 8638 Loss: 1.4028506484507607e-06\n",
      "Iteration: 8639 Loss: 1.4015649065838143e-06\n",
      "Iteration: 8640 Loss: 1.4002803431459572e-06\n",
      "Iteration: 8641 Loss: 1.3989969570546098e-06\n",
      "Iteration: 8642 Loss: 1.3977147472346957e-06\n",
      "Iteration: 8643 Loss: 1.3964337126019325e-06\n",
      "Iteration: 8644 Loss: 1.3951538520842327e-06\n",
      "Iteration: 8645 Loss: 1.393875164604176e-06\n",
      "Iteration: 8646 Loss: 1.3925976490850578e-06\n",
      "Iteration: 8647 Loss: 1.3913213044559448e-06\n",
      "Iteration: 8648 Loss: 1.3900461296407745e-06\n",
      "Iteration: 8649 Loss: 1.3887721235666677e-06\n",
      "Iteration: 8650 Loss: 1.387499285167711e-06\n",
      "Iteration: 8651 Loss: 1.3862276133684111e-06\n",
      "Iteration: 8652 Loss: 1.3849571071011171e-06\n",
      "Iteration: 8653 Loss: 1.3836877652973619e-06\n",
      "Iteration: 8654 Loss: 1.3824195867761223e-06\n",
      "Iteration: 8655 Loss: 1.381152570616034e-06\n",
      "Iteration: 8656 Loss: 1.3798867158032611e-06\n",
      "Iteration: 8657 Loss: 1.3786220212213248e-06\n",
      "Iteration: 8658 Loss: 1.3773584856902872e-06\n",
      "Iteration: 8659 Loss: 1.376096108291177e-06\n",
      "Iteration: 8660 Loss: 1.3748348879069186e-06\n",
      "Iteration: 8661 Loss: 1.3735748234724878e-06\n",
      "Iteration: 8662 Loss: 1.372315913935162e-06\n",
      "Iteration: 8663 Loss: 1.371058158231695e-06\n",
      "Iteration: 8664 Loss: 1.3698015554194457e-06\n",
      "Iteration: 8665 Loss: 1.3685461042144694e-06\n",
      "Iteration: 8666 Loss: 1.3672918036731505e-06\n",
      "Iteration: 8667 Loss: 1.3660386527441326e-06\n",
      "Iteration: 8668 Loss: 1.3647866503736102e-06\n",
      "Iteration: 8669 Loss: 1.363535795363305e-06\n",
      "Iteration: 8670 Loss: 1.3622860870076516e-06\n",
      "Iteration: 8671 Loss: 1.3610375239936017e-06\n",
      "Iteration: 8672 Loss: 1.3597901052761595e-06\n",
      "Iteration: 8673 Loss: 1.3585438299187028e-06\n",
      "Iteration: 8674 Loss: 1.3572986968732055e-06\n",
      "Iteration: 8675 Loss: 1.35605470500638e-06\n",
      "Iteration: 8676 Loss: 1.354811853332671e-06\n",
      "Iteration: 8677 Loss: 1.3535701407188925e-06\n",
      "Iteration: 8678 Loss: 1.3523295661792403e-06\n",
      "Iteration: 8679 Loss: 1.3510901285850118e-06\n",
      "Iteration: 8680 Loss: 1.34985182712088e-06\n",
      "Iteration: 8681 Loss: 1.348614660573609e-06\n",
      "Iteration: 8682 Loss: 1.3473786279056257e-06\n",
      "Iteration: 8683 Loss: 1.3461437281053895e-06\n",
      "Iteration: 8684 Loss: 1.3449099601356735e-06\n",
      "Iteration: 8685 Loss: 1.3436773229552698e-06\n",
      "Iteration: 8686 Loss: 1.3424458155323773e-06\n",
      "Iteration: 8687 Loss: 1.3412154368292112e-06\n",
      "Iteration: 8688 Loss: 1.3399861858111773e-06\n",
      "Iteration: 8689 Loss: 1.33875806144594e-06\n",
      "Iteration: 8690 Loss: 1.33753106264242e-06\n",
      "Iteration: 8691 Loss: 1.3363051884268449e-06\n",
      "Iteration: 8692 Loss: 1.3350804378827583e-06\n",
      "Iteration: 8693 Loss: 1.3338568098077441e-06\n",
      "Iteration: 8694 Loss: 1.33263430323227e-06\n",
      "Iteration: 8695 Loss: 1.3314129171839324e-06\n",
      "Iteration: 8696 Loss: 1.3301926505209848e-06\n",
      "Iteration: 8697 Loss: 1.3289735022766273e-06\n",
      "Iteration: 8698 Loss: 1.3277554714247533e-06\n",
      "Iteration: 8699 Loss: 1.3265385569409962e-06\n",
      "Iteration: 8700 Loss: 1.325322757801231e-06\n",
      "Iteration: 8701 Loss: 1.3241080729021911e-06\n",
      "Iteration: 8702 Loss: 1.3228945013890707e-06\n",
      "Iteration: 8703 Loss: 1.3216820421541226e-06\n",
      "Iteration: 8704 Loss: 1.3204706942670522e-06\n",
      "Iteration: 8705 Loss: 1.3192604565389272e-06\n",
      "Iteration: 8706 Loss: 1.3180513280363025e-06\n",
      "Iteration: 8707 Loss: 1.31684330774208e-06\n",
      "Iteration: 8708 Loss: 1.3156363946421205e-06\n",
      "Iteration: 8709 Loss: 1.3144305876628333e-06\n",
      "Iteration: 8710 Loss: 1.3132258859607151e-06\n",
      "Iteration: 8711 Loss: 1.3120222883560847e-06\n",
      "Iteration: 8712 Loss: 1.3108197938890163e-06\n",
      "Iteration: 8713 Loss: 1.3096184015540566e-06\n",
      "Iteration: 8714 Loss: 1.3084181102792995e-06\n",
      "Iteration: 8715 Loss: 1.3072189192240011e-06\n",
      "Iteration: 8716 Loss: 1.3060208272173242e-06\n",
      "Iteration: 8717 Loss: 1.3048238333013997e-06\n",
      "Iteration: 8718 Loss: 1.303627936473748e-06\n",
      "Iteration: 8719 Loss: 1.3024331356460475e-06\n",
      "Iteration: 8720 Loss: 1.301239430117799e-06\n",
      "Iteration: 8721 Loss: 1.3000468185779884e-06\n",
      "Iteration: 8722 Loss: 1.2988552999705578e-06\n",
      "Iteration: 8723 Loss: 1.2976648735132154e-06\n",
      "Iteration: 8724 Loss: 1.296475538125253e-06\n",
      "Iteration: 8725 Loss: 1.295287292888733e-06\n",
      "Iteration: 8726 Loss: 1.2941001366391555e-06\n",
      "Iteration: 8727 Loss: 1.2929140684583231e-06\n",
      "Iteration: 8728 Loss: 1.2917290873512174e-06\n",
      "Iteration: 8729 Loss: 1.2905451923202805e-06\n",
      "Iteration: 8730 Loss: 1.2893623823727214e-06\n",
      "Iteration: 8731 Loss: 1.2881806565101124e-06\n",
      "Iteration: 8732 Loss: 1.287000013743204e-06\n",
      "Iteration: 8733 Loss: 1.2858204530751981e-06\n",
      "Iteration: 8734 Loss: 1.2846419735163621e-06\n",
      "Iteration: 8735 Loss: 1.2834645740773266e-06\n",
      "Iteration: 8736 Loss: 1.2822882537659008e-06\n",
      "Iteration: 8737 Loss: 1.2811130114803585e-06\n",
      "Iteration: 8738 Loss: 1.279938846398551e-06\n",
      "Iteration: 8739 Loss: 1.2787657575427534e-06\n",
      "Iteration: 8740 Loss: 1.2775937438612859e-06\n",
      "Iteration: 8741 Loss: 1.2764228043728812e-06\n",
      "Iteration: 8742 Loss: 1.275252938034839e-06\n",
      "Iteration: 8743 Loss: 1.2740841439761282e-06\n",
      "Iteration: 8744 Loss: 1.2729164211575999e-06\n",
      "Iteration: 8745 Loss: 1.2717497687490538e-06\n",
      "Iteration: 8746 Loss: 1.2705841854691917e-06\n",
      "Iteration: 8747 Loss: 1.2694196703412652e-06\n",
      "Iteration: 8748 Loss: 1.2682562226802433e-06\n",
      "Iteration: 8749 Loss: 1.2670938413602303e-06\n",
      "Iteration: 8750 Loss: 1.2659325254032376e-06\n",
      "Iteration: 8751 Loss: 1.2647722738326152e-06\n",
      "Iteration: 8752 Loss: 1.2636130856735932e-06\n",
      "Iteration: 8753 Loss: 1.262454959950879e-06\n",
      "Iteration: 8754 Loss: 1.2612978956927082e-06\n",
      "Iteration: 8755 Loss: 1.260141892038283e-06\n",
      "Iteration: 8756 Loss: 1.2589869478466612e-06\n",
      "Iteration: 8757 Loss: 1.25783306214317e-06\n",
      "Iteration: 8758 Loss: 1.256680234072527e-06\n",
      "Iteration: 8759 Loss: 1.2555284624652614e-06\n",
      "Iteration: 8760 Loss: 1.2543777465286273e-06\n",
      "Iteration: 8761 Loss: 1.25322808532158e-06\n",
      "Iteration: 8762 Loss: 1.2520794778198818e-06\n",
      "Iteration: 8763 Loss: 1.2509319230563265e-06\n",
      "Iteration: 8764 Loss: 1.2497854200690935e-06\n",
      "Iteration: 8765 Loss: 1.2486399680391566e-06\n",
      "Iteration: 8766 Loss: 1.2474955657079883e-06\n",
      "Iteration: 8767 Loss: 1.2463522122618513e-06\n",
      "Iteration: 8768 Loss: 1.2452099067420243e-06\n",
      "Iteration: 8769 Loss: 1.2440686481835275e-06\n",
      "Iteration: 8770 Loss: 1.242928435630415e-06\n",
      "Iteration: 8771 Loss: 1.2417892680640714e-06\n",
      "Iteration: 8772 Loss: 1.2406511446991277e-06\n",
      "Iteration: 8773 Loss: 1.2395140644075899e-06\n",
      "Iteration: 8774 Loss: 1.238378026290456e-06\n",
      "Iteration: 8775 Loss: 1.2372430293923447e-06\n",
      "Iteration: 8776 Loss: 1.236109072761098e-06\n",
      "Iteration: 8777 Loss: 1.2349761553229536e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8778 Loss: 1.2338442763604403e-06\n",
      "Iteration: 8779 Loss: 1.2327134348039726e-06\n",
      "Iteration: 8780 Loss: 1.2315836297036796e-06\n",
      "Iteration: 8781 Loss: 1.2304548601089637e-06\n",
      "Iteration: 8782 Loss: 1.2293271250735998e-06\n",
      "Iteration: 8783 Loss: 1.2282004236461103e-06\n",
      "Iteration: 8784 Loss: 1.2270747548790914e-06\n",
      "Iteration: 8785 Loss: 1.22595011783142e-06\n",
      "Iteration: 8786 Loss: 1.2248265115484005e-06\n",
      "Iteration: 8787 Loss: 1.2237039350911846e-06\n",
      "Iteration: 8788 Loss: 1.222582387459214e-06\n",
      "Iteration: 8789 Loss: 1.2214618677646922e-06\n",
      "Iteration: 8790 Loss: 1.2203423751809282e-06\n",
      "Iteration: 8791 Loss: 1.2192239085365482e-06\n",
      "Iteration: 8792 Loss: 1.2181064670644627e-06\n",
      "Iteration: 8793 Loss: 1.216990049767298e-06\n",
      "Iteration: 8794 Loss: 1.2158746557599846e-06\n",
      "Iteration: 8795 Loss: 1.2147602840528726e-06\n",
      "Iteration: 8796 Loss: 1.2136469336497505e-06\n",
      "Iteration: 8797 Loss: 1.2125346036735548e-06\n",
      "Iteration: 8798 Loss: 1.2114232931828226e-06\n",
      "Iteration: 8799 Loss: 1.2103130013062949e-06\n",
      "Iteration: 8800 Loss: 1.2092037269959182e-06\n",
      "Iteration: 8801 Loss: 1.2080954693721993e-06\n",
      "Iteration: 8802 Loss: 1.2069882275086336e-06\n",
      "Iteration: 8803 Loss: 1.2058820004683812e-06\n",
      "Iteration: 8804 Loss: 1.2047767873819655e-06\n",
      "Iteration: 8805 Loss: 1.2036725872061788e-06\n",
      "Iteration: 8806 Loss: 1.2025693990698559e-06\n",
      "Iteration: 8807 Loss: 1.2014672220434917e-06\n",
      "Iteration: 8808 Loss: 1.200366055199969e-06\n",
      "Iteration: 8809 Loss: 1.1992658976162732e-06\n",
      "Iteration: 8810 Loss: 1.1981667483659999e-06\n",
      "Iteration: 8811 Loss: 1.1970686065231032e-06\n",
      "Iteration: 8812 Loss: 1.1959714711681564e-06\n",
      "Iteration: 8813 Loss: 1.194875341375604e-06\n",
      "Iteration: 8814 Loss: 1.1937802162278427e-06\n",
      "Iteration: 8815 Loss: 1.1926860947954075e-06\n",
      "Iteration: 8816 Loss: 1.1915929761666161e-06\n",
      "Iteration: 8817 Loss: 1.1905008594201465e-06\n",
      "Iteration: 8818 Loss: 1.1894097436360308e-06\n",
      "Iteration: 8819 Loss: 1.1883196278986995e-06\n",
      "Iteration: 8820 Loss: 1.1872305112895596e-06\n",
      "Iteration: 8821 Loss: 1.1861423928981508e-06\n",
      "Iteration: 8822 Loss: 1.1850552718018157e-06\n",
      "Iteration: 8823 Loss: 1.1839691469779678e-06\n",
      "Iteration: 8824 Loss: 1.1828840176822646e-06\n",
      "Iteration: 8825 Loss: 1.181799882945923e-06\n",
      "Iteration: 8826 Loss: 1.1807167419136591e-06\n",
      "Iteration: 8827 Loss: 1.179634593621246e-06\n",
      "Iteration: 8828 Loss: 1.1785534371527027e-06\n",
      "Iteration: 8829 Loss: 1.1774732716044807e-06\n",
      "Iteration: 8830 Loss: 1.1763940960638934e-06\n",
      "Iteration: 8831 Loss: 1.1753159096861232e-06\n",
      "Iteration: 8832 Loss: 1.1742387114452808e-06\n",
      "Iteration: 8833 Loss: 1.1731625005502474e-06\n",
      "Iteration: 8834 Loss: 1.1720872759852677e-06\n",
      "Iteration: 8835 Loss: 1.1710130370429137e-06\n",
      "Iteration: 8836 Loss: 1.1699397825361093e-06\n",
      "Iteration: 8837 Loss: 1.1688675117626322e-06\n",
      "Iteration: 8838 Loss: 1.167796223707438e-06\n",
      "Iteration: 8839 Loss: 1.1667259175250235e-06\n",
      "Iteration: 8840 Loss: 1.1656565923154067e-06\n",
      "Iteration: 8841 Loss: 1.164588247180373e-06\n",
      "Iteration: 8842 Loss: 1.163520881278938e-06\n",
      "Iteration: 8843 Loss: 1.1624544935998853e-06\n",
      "Iteration: 8844 Loss: 1.1613890833000819e-06\n",
      "Iteration: 8845 Loss: 1.1603246494308883e-06\n",
      "Iteration: 8846 Loss: 1.1592611912097169e-06\n",
      "Iteration: 8847 Loss: 1.158198707742567e-06\n",
      "Iteration: 8848 Loss: 1.1571371980234077e-06\n",
      "Iteration: 8849 Loss: 1.1560766612127672e-06\n",
      "Iteration: 8850 Loss: 1.1550170964221804e-06\n",
      "Iteration: 8851 Loss: 1.1539585027618434e-06\n",
      "Iteration: 8852 Loss: 1.1529008793387225e-06\n",
      "Iteration: 8853 Loss: 1.1518442252649864e-06\n",
      "Iteration: 8854 Loss: 1.1507885396528215e-06\n",
      "Iteration: 8855 Loss: 1.1497338215573934e-06\n",
      "Iteration: 8856 Loss: 1.1486800702021495e-06\n",
      "Iteration: 8857 Loss: 1.1476272845673339e-06\n",
      "Iteration: 8858 Loss: 1.146575463928789e-06\n",
      "Iteration: 8859 Loss: 1.1455246073221816e-06\n",
      "Iteration: 8860 Loss: 1.144474713863088e-06\n",
      "Iteration: 8861 Loss: 1.143425782724009e-06\n",
      "Iteration: 8862 Loss: 1.1423778129115263e-06\n",
      "Iteration: 8863 Loss: 1.1413308035997055e-06\n",
      "Iteration: 8864 Loss: 1.1402847538507935e-06\n",
      "Iteration: 8865 Loss: 1.139239662903569e-06\n",
      "Iteration: 8866 Loss: 1.1381955298174727e-06\n",
      "Iteration: 8867 Loss: 1.137152353772604e-06\n",
      "Iteration: 8868 Loss: 1.1361101337789798e-06\n",
      "Iteration: 8869 Loss: 1.1350688689323873e-06\n",
      "Iteration: 8870 Loss: 1.134028558471059e-06\n",
      "Iteration: 8871 Loss: 1.1329892016290531e-06\n",
      "Iteration: 8872 Loss: 1.1319507973101706e-06\n",
      "Iteration: 8873 Loss: 1.1309133447276656e-06\n",
      "Iteration: 8874 Loss: 1.1298768430056285e-06\n",
      "Iteration: 8875 Loss: 1.1288412912746943e-06\n",
      "Iteration: 8876 Loss: 1.1278066887210357e-06\n",
      "Iteration: 8877 Loss: 1.1267730343586754e-06\n",
      "Iteration: 8878 Loss: 1.125740327320529e-06\n",
      "Iteration: 8879 Loss: 1.1247085668529094e-06\n",
      "Iteration: 8880 Loss: 1.1236777520290703e-06\n",
      "Iteration: 8881 Loss: 1.1226478820420824e-06\n",
      "Iteration: 8882 Loss: 1.1216189558510422e-06\n",
      "Iteration: 8883 Loss: 1.1205909727655994e-06\n",
      "Iteration: 8884 Loss: 1.1195639318625002e-06\n",
      "Iteration: 8885 Loss: 1.1185378322806489e-06\n",
      "Iteration: 8886 Loss: 1.1175126731522735e-06\n",
      "Iteration: 8887 Loss: 1.1164884536198288e-06\n",
      "Iteration: 8888 Loss: 1.1154651728201723e-06\n",
      "Iteration: 8889 Loss: 1.1144428300424387e-06\n",
      "Iteration: 8890 Loss: 1.113421423923942e-06\n",
      "Iteration: 8891 Loss: 1.112400954110163e-06\n",
      "Iteration: 8892 Loss: 1.1113814197048518e-06\n",
      "Iteration: 8893 Loss: 1.1103628196825943e-06\n",
      "Iteration: 8894 Loss: 1.1093451532998201e-06\n",
      "Iteration: 8895 Loss: 1.1083284196456467e-06\n",
      "Iteration: 8896 Loss: 1.1073126178051956e-06\n",
      "Iteration: 8897 Loss: 1.106297746927462e-06\n",
      "Iteration: 8898 Loss: 1.105283806157025e-06\n",
      "Iteration: 8899 Loss: 1.104270794811832e-06\n",
      "Iteration: 8900 Loss: 1.1032587118723354e-06\n",
      "Iteration: 8901 Loss: 1.1022475565981788e-06\n",
      "Iteration: 8902 Loss: 1.1012373280276957e-06\n",
      "Iteration: 8903 Loss: 1.1002280253389572e-06\n",
      "Iteration: 8904 Loss: 1.0992196477968859e-06\n",
      "Iteration: 8905 Loss: 1.098212194466845e-06\n",
      "Iteration: 8906 Loss: 1.0972056646196434e-06\n",
      "Iteration: 8907 Loss: 1.0962000572601802e-06\n",
      "Iteration: 8908 Loss: 1.0951953714938091e-06\n",
      "Iteration: 8909 Loss: 1.0941916064993612e-06\n",
      "Iteration: 8910 Loss: 1.0931887614906178e-06\n",
      "Iteration: 8911 Loss: 1.092186835682209e-06\n",
      "Iteration: 8912 Loss: 1.0911858281736886e-06\n",
      "Iteration: 8913 Loss: 1.090185738124958e-06\n",
      "Iteration: 8914 Loss: 1.0891865646936637e-06\n",
      "Iteration: 8915 Loss: 1.0881883071534642e-06\n",
      "Iteration: 8916 Loss: 1.08719096443839e-06\n",
      "Iteration: 8917 Loss: 1.0861945357652568e-06\n",
      "Iteration: 8918 Loss: 1.085199020356063e-06\n",
      "Iteration: 8919 Loss: 1.0842044174270606e-06\n",
      "Iteration: 8920 Loss: 1.0832107260869304e-06\n",
      "Iteration: 8921 Loss: 1.0822179455011515e-06\n",
      "Iteration: 8922 Loss: 1.0812260748896578e-06\n",
      "Iteration: 8923 Loss: 1.0802351133622938e-06\n",
      "Iteration: 8924 Loss: 1.0792450600310257e-06\n",
      "Iteration: 8925 Loss: 1.0782559141157675e-06\n",
      "Iteration: 8926 Loss: 1.077267674791502e-06\n",
      "Iteration: 8927 Loss: 1.0762803412776812e-06\n",
      "Iteration: 8928 Loss: 1.0752939126330625e-06\n",
      "Iteration: 8929 Loss: 1.0743083881427413e-06\n",
      "Iteration: 8930 Loss: 1.073323766864415e-06\n",
      "Iteration: 8931 Loss: 1.0723400480814635e-06\n",
      "Iteration: 8932 Loss: 1.0713572308561683e-06\n",
      "Iteration: 8933 Loss: 1.0703753144162057e-06\n",
      "Iteration: 8934 Loss: 1.0693942978816915e-06\n",
      "Iteration: 8935 Loss: 1.0684141805415935e-06\n",
      "Iteration: 8936 Loss: 1.0674349615102067e-06\n",
      "Iteration: 8937 Loss: 1.066456639970002e-06\n",
      "Iteration: 8938 Loss: 1.0654792150947888e-06\n",
      "Iteration: 8939 Loss: 1.0645026861218448e-06\n",
      "Iteration: 8940 Loss: 1.0635270520579148e-06\n",
      "Iteration: 8941 Loss: 1.0625523122537819e-06\n",
      "Iteration: 8942 Loss: 1.0615784658900337e-06\n",
      "Iteration: 8943 Loss: 1.0606055119764977e-06\n",
      "Iteration: 8944 Loss: 1.059633449869702e-06\n",
      "Iteration: 8945 Loss: 1.0586622786894417e-06\n",
      "Iteration: 8946 Loss: 1.0576919976213179e-06\n",
      "Iteration: 8947 Loss: 1.056722605765714e-06\n",
      "Iteration: 8948 Loss: 1.055754102536364e-06\n",
      "Iteration: 8949 Loss: 1.0547864868561753e-06\n",
      "Iteration: 8950 Loss: 1.0538197581467947e-06\n",
      "Iteration: 8951 Loss: 1.0528539153633368e-06\n",
      "Iteration: 8952 Loss: 1.0518889580082164e-06\n",
      "Iteration: 8953 Loss: 1.0509248849305223e-06\n",
      "Iteration: 8954 Loss: 1.0499616954565651e-06\n",
      "Iteration: 8955 Loss: 1.0489993887810395e-06\n",
      "Iteration: 8956 Loss: 1.0480379640363032e-06\n",
      "Iteration: 8957 Loss: 1.047077420528884e-06\n",
      "Iteration: 8958 Loss: 1.0461177573944477e-06\n",
      "Iteration: 8959 Loss: 1.0451589738221932e-06\n",
      "Iteration: 8960 Loss: 1.0442010690115942e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8961 Loss: 1.0432440421542141e-06\n",
      "Iteration: 8962 Loss: 1.0422878924447157e-06\n",
      "Iteration: 8963 Loss: 1.0413326191403493e-06\n",
      "Iteration: 8964 Loss: 1.040378221258617e-06\n",
      "Iteration: 8965 Loss: 1.0394246981765625e-06\n",
      "Iteration: 8966 Loss: 1.038472049088274e-06\n",
      "Iteration: 8967 Loss: 1.0375202730219068e-06\n",
      "Iteration: 8968 Loss: 1.0365693693505644e-06\n",
      "Iteration: 8969 Loss: 1.0356193372158924e-06\n",
      "Iteration: 8970 Loss: 1.0346701758175504e-06\n",
      "Iteration: 8971 Loss: 1.033721884359361e-06\n",
      "Iteration: 8972 Loss: 1.0327744620457938e-06\n",
      "Iteration: 8973 Loss: 1.0318279080766836e-06\n",
      "Iteration: 8974 Loss: 1.030882221658664e-06\n",
      "Iteration: 8975 Loss: 1.0299374019944469e-06\n",
      "Iteration: 8976 Loss: 1.0289934482893255e-06\n",
      "Iteration: 8977 Loss: 1.0280503597543948e-06\n",
      "Iteration: 8978 Loss: 1.0271081355920774e-06\n",
      "Iteration: 8979 Loss: 1.0261667750123363e-06\n",
      "Iteration: 8980 Loss: 1.0252262772216838e-06\n",
      "Iteration: 8981 Loss: 1.0242866414307383e-06\n",
      "Iteration: 8982 Loss: 1.0233478668501037e-06\n",
      "Iteration: 8983 Loss: 1.0224099526888072e-06\n",
      "Iteration: 8984 Loss: 1.02147289815911e-06\n",
      "Iteration: 8985 Loss: 1.0205367024723496e-06\n",
      "Iteration: 8986 Loss: 1.019601364841657e-06\n",
      "Iteration: 8987 Loss: 1.0186668844821392e-06\n",
      "Iteration: 8988 Loss: 1.0177332606635681e-06\n",
      "Iteration: 8989 Loss: 1.0168004924864206e-06\n",
      "Iteration: 8990 Loss: 1.0158685792254807e-06\n",
      "Iteration: 8991 Loss: 1.0149375200958936e-06\n",
      "Iteration: 8992 Loss: 1.0140073142598526e-06\n",
      "Iteration: 8993 Loss: 1.0130779610419968e-06\n",
      "Iteration: 8994 Loss: 1.0121494596682336e-06\n",
      "Iteration: 8995 Loss: 1.0112218091881323e-06\n",
      "Iteration: 8996 Loss: 1.0102950089864626e-06\n",
      "Iteration: 8997 Loss: 1.009369058116778e-06\n",
      "Iteration: 8998 Loss: 1.0084439560276784e-06\n",
      "Iteration: 8999 Loss: 1.0075197018270562e-06\n",
      "Iteration: 9000 Loss: 1.006596294737847e-06\n",
      "Iteration: 9001 Loss: 1.0056737339844425e-06\n",
      "Iteration: 9002 Loss: 1.0047520187885207e-06\n",
      "Iteration: 9003 Loss: 1.0038311483794156e-06\n",
      "Iteration: 9004 Loss: 1.0029111219805302e-06\n",
      "Iteration: 9005 Loss: 1.0019919388165873e-06\n",
      "Iteration: 9006 Loss: 1.0010735980616487e-06\n",
      "Iteration: 9007 Loss: 1.0001560991105874e-06\n",
      "Iteration: 9008 Loss: 9.992394410252466e-07\n",
      "Iteration: 9009 Loss: 9.983236230866613e-07\n",
      "Iteration: 9010 Loss: 9.97408644585317e-07\n",
      "Iteration: 9011 Loss: 9.964945045817673e-07\n",
      "Iteration: 9012 Loss: 9.955812023269755e-07\n",
      "Iteration: 9013 Loss: 9.94668737295207e-07\n",
      "Iteration: 9014 Loss: 9.93757108568734e-07\n",
      "Iteration: 9015 Loss: 9.928463153820587e-07\n",
      "Iteration: 9016 Loss: 9.91936356970484e-07\n",
      "Iteration: 9017 Loss: 9.910272324835406e-07\n",
      "Iteration: 9018 Loss: 9.9011894132528e-07\n",
      "Iteration: 9019 Loss: 9.892114826465071e-07\n",
      "Iteration: 9020 Loss: 9.883048556875128e-07\n",
      "Iteration: 9021 Loss: 9.873990596828828e-07\n",
      "Iteration: 9022 Loss: 9.864940939301617e-07\n",
      "Iteration: 9023 Loss: 9.85589957554363e-07\n",
      "Iteration: 9024 Loss: 9.84686649934047e-07\n",
      "Iteration: 9025 Loss: 9.83784170142844e-07\n",
      "Iteration: 9026 Loss: 9.828825175076213e-07\n",
      "Iteration: 9027 Loss: 9.81981691267904e-07\n",
      "Iteration: 9028 Loss: 9.81081690815093e-07\n",
      "Iteration: 9029 Loss: 9.801825150978754e-07\n",
      "Iteration: 9030 Loss: 9.792841635052625e-07\n",
      "Iteration: 9031 Loss: 9.78386635343409e-07\n",
      "Iteration: 9032 Loss: 9.774899297370963e-07\n",
      "Iteration: 9033 Loss: 9.765940459959007e-07\n",
      "Iteration: 9034 Loss: 9.756989833617913e-07\n",
      "Iteration: 9035 Loss: 9.748047410861332e-07\n",
      "Iteration: 9036 Loss: 9.739113185618396e-07\n",
      "Iteration: 9037 Loss: 9.73018714596604e-07\n",
      "Iteration: 9038 Loss: 9.72126928878703e-07\n",
      "Iteration: 9039 Loss: 9.712359605178909e-07\n",
      "Iteration: 9040 Loss: 9.70345808842905e-07\n",
      "Iteration: 9041 Loss: 9.694564729397787e-07\n",
      "Iteration: 9042 Loss: 9.685679521441147e-07\n",
      "Iteration: 9043 Loss: 9.676802457113248e-07\n",
      "Iteration: 9044 Loss: 9.667933528908646e-07\n",
      "Iteration: 9045 Loss: 9.659072729388562e-07\n",
      "Iteration: 9046 Loss: 9.650220051108645e-07\n",
      "Iteration: 9047 Loss: 9.641375486625905e-07\n",
      "Iteration: 9048 Loss: 9.63253902848917e-07\n",
      "Iteration: 9049 Loss: 9.623710668449716e-07\n",
      "Iteration: 9050 Loss: 9.614890400742885e-07\n",
      "Iteration: 9051 Loss: 9.60607821599234e-07\n",
      "Iteration: 9052 Loss: 9.59727410906311e-07\n",
      "Iteration: 9053 Loss: 9.58847807138988e-07\n",
      "Iteration: 9054 Loss: 9.5796900956135e-07\n",
      "Iteration: 9055 Loss: 9.570910174321198e-07\n",
      "Iteration: 9056 Loss: 9.56213830097959e-07\n",
      "Iteration: 9057 Loss: 9.553374465679043e-07\n",
      "Iteration: 9058 Loss: 9.54461866360157e-07\n",
      "Iteration: 9059 Loss: 9.535870885946956e-07\n",
      "Iteration: 9060 Loss: 9.527131126499151e-07\n",
      "Iteration: 9061 Loss: 9.518399377374171e-07\n",
      "Iteration: 9062 Loss: 9.509675631181042e-07\n",
      "Iteration: 9063 Loss: 9.50095988061231e-07\n",
      "Iteration: 9064 Loss: 9.492252118911057e-07\n",
      "Iteration: 9065 Loss: 9.483552337614777e-07\n",
      "Iteration: 9066 Loss: 9.474860529411831e-07\n",
      "Iteration: 9067 Loss: 9.466176688110643e-07\n",
      "Iteration: 9068 Loss: 9.457500806426444e-07\n",
      "Iteration: 9069 Loss: 9.448832875946122e-07\n",
      "Iteration: 9070 Loss: 9.440172889912849e-07\n",
      "Iteration: 9071 Loss: 9.431520840484556e-07\n",
      "Iteration: 9072 Loss: 9.422876720992435e-07\n",
      "Iteration: 9073 Loss: 9.414240524689141e-07\n",
      "Iteration: 9074 Loss: 9.405612243780836e-07\n",
      "Iteration: 9075 Loss: 9.396991872114764e-07\n",
      "Iteration: 9076 Loss: 9.388379400757655e-07\n",
      "Iteration: 9077 Loss: 9.379774822485526e-07\n",
      "Iteration: 9078 Loss: 9.371178130601072e-07\n",
      "Iteration: 9079 Loss: 9.362589317892564e-07\n",
      "Iteration: 9080 Loss: 9.354008377135162e-07\n",
      "Iteration: 9081 Loss: 9.34543530109692e-07\n",
      "Iteration: 9082 Loss: 9.336870082608138e-07\n",
      "Iteration: 9083 Loss: 9.328312714437884e-07\n",
      "Iteration: 9084 Loss: 9.319763189379138e-07\n",
      "Iteration: 9085 Loss: 9.311221500855381e-07\n",
      "Iteration: 9086 Loss: 9.302687640515437e-07\n",
      "Iteration: 9087 Loss: 9.294161601770115e-07\n",
      "Iteration: 9088 Loss: 9.285643377414507e-07\n",
      "Iteration: 9089 Loss: 9.27713296089668e-07\n",
      "Iteration: 9090 Loss: 9.268630343340426e-07\n",
      "Iteration: 9091 Loss: 9.260135519871015e-07\n",
      "Iteration: 9092 Loss: 9.251648481631501e-07\n",
      "Iteration: 9093 Loss: 9.243169221495749e-07\n",
      "Iteration: 9094 Loss: 9.234697733474499e-07\n",
      "Iteration: 9095 Loss: 9.226234009874865e-07\n",
      "Iteration: 9096 Loss: 9.217778043574441e-07\n",
      "Iteration: 9097 Loss: 9.209329827468089e-07\n",
      "Iteration: 9098 Loss: 9.200889353586084e-07\n",
      "Iteration: 9099 Loss: 9.192456617399309e-07\n",
      "Iteration: 9100 Loss: 9.184031609267809e-07\n",
      "Iteration: 9101 Loss: 9.175614322966919e-07\n",
      "Iteration: 9102 Loss: 9.167204751385429e-07\n",
      "Iteration: 9103 Loss: 9.158802887486527e-07\n",
      "Iteration: 9104 Loss: 9.150408724190645e-07\n",
      "Iteration: 9105 Loss: 9.142022254447091e-07\n",
      "Iteration: 9106 Loss: 9.133643471180726e-07\n",
      "Iteration: 9107 Loss: 9.125272367385603e-07\n",
      "Iteration: 9108 Loss: 9.116908935984676e-07\n",
      "Iteration: 9109 Loss: 9.108553169409518e-07\n",
      "Iteration: 9110 Loss: 9.100205061759577e-07\n",
      "Iteration: 9111 Loss: 9.091864605427021e-07\n",
      "Iteration: 9112 Loss: 9.083531793998243e-07\n",
      "Iteration: 9113 Loss: 9.075206619318919e-07\n",
      "Iteration: 9114 Loss: 9.066889074937949e-07\n",
      "Iteration: 9115 Loss: 9.058579153344406e-07\n",
      "Iteration: 9116 Loss: 9.050276849194805e-07\n",
      "Iteration: 9117 Loss: 9.041982153843675e-07\n",
      "Iteration: 9118 Loss: 9.033695060875606e-07\n",
      "Iteration: 9119 Loss: 9.025415563321614e-07\n",
      "Iteration: 9120 Loss: 9.017143654233128e-07\n",
      "Iteration: 9121 Loss: 9.008879326616763e-07\n",
      "Iteration: 9122 Loss: 9.000622573547496e-07\n",
      "Iteration: 9123 Loss: 8.992373388096226e-07\n",
      "Iteration: 9124 Loss: 8.98413176332301e-07\n",
      "Iteration: 9125 Loss: 8.975897692284282e-07\n",
      "Iteration: 9126 Loss: 8.967671168053439e-07\n",
      "Iteration: 9127 Loss: 8.959452184297157e-07\n",
      "Iteration: 9128 Loss: 8.951240732393034e-07\n",
      "Iteration: 9129 Loss: 8.943036807141385e-07\n",
      "Iteration: 9130 Loss: 8.934840401076934e-07\n",
      "Iteration: 9131 Loss: 8.926651507298042e-07\n",
      "Iteration: 9132 Loss: 8.918470120428373e-07\n",
      "Iteration: 9133 Loss: 8.910296230592715e-07\n",
      "Iteration: 9134 Loss: 8.902129833004243e-07\n",
      "Iteration: 9135 Loss: 8.893970919619702e-07\n",
      "Iteration: 9136 Loss: 8.885819484172133e-07\n",
      "Iteration: 9137 Loss: 8.877675518347787e-07\n",
      "Iteration: 9138 Loss: 8.869539018207528e-07\n",
      "Iteration: 9139 Loss: 8.861409975463809e-07\n",
      "Iteration: 9140 Loss: 8.853288382707002e-07\n",
      "Iteration: 9141 Loss: 8.845174234246572e-07\n",
      "Iteration: 9142 Loss: 8.837067522686499e-07\n",
      "Iteration: 9143 Loss: 8.828968241212755e-07\n",
      "Iteration: 9144 Loss: 8.820876383024604e-07\n",
      "Iteration: 9145 Loss: 8.812791941289437e-07\n",
      "Iteration: 9146 Loss: 8.804714909253182e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9147 Loss: 8.796645280670313e-07\n",
      "Iteration: 9148 Loss: 8.788583047614342e-07\n",
      "Iteration: 9149 Loss: 8.780528203862306e-07\n",
      "Iteration: 9150 Loss: 8.772480742115445e-07\n",
      "Iteration: 9151 Loss: 8.764440656706719e-07\n",
      "Iteration: 9152 Loss: 8.756407940324656e-07\n",
      "Iteration: 9153 Loss: 8.748382586199852e-07\n",
      "Iteration: 9154 Loss: 8.740364588193823e-07\n",
      "Iteration: 9155 Loss: 8.732353938360613e-07\n",
      "Iteration: 9156 Loss: 8.72435063059119e-07\n",
      "Iteration: 9157 Loss: 8.716354658141203e-07\n",
      "Iteration: 9158 Loss: 8.708366013697945e-07\n",
      "Iteration: 9159 Loss: 8.700384691714625e-07\n",
      "Iteration: 9160 Loss: 8.692410684894078e-07\n",
      "Iteration: 9161 Loss: 8.684443986521402e-07\n",
      "Iteration: 9162 Loss: 8.676484590477258e-07\n",
      "Iteration: 9163 Loss: 8.668532488935861e-07\n",
      "Iteration: 9164 Loss: 8.66058767578133e-07\n",
      "Iteration: 9165 Loss: 8.652650143754396e-07\n",
      "Iteration: 9166 Loss: 8.64471988731961e-07\n",
      "Iteration: 9167 Loss: 8.636796899240885e-07\n",
      "Iteration: 9168 Loss: 8.628881172887035e-07\n",
      "Iteration: 9169 Loss: 8.620972702146276e-07\n",
      "Iteration: 9170 Loss: 8.613071478615572e-07\n",
      "Iteration: 9171 Loss: 8.60517749744237e-07\n",
      "Iteration: 9172 Loss: 8.597290751933423e-07\n",
      "Iteration: 9173 Loss: 8.589411234341328e-07\n",
      "Iteration: 9174 Loss: 8.581538938605406e-07\n",
      "Iteration: 9175 Loss: 8.573673858138818e-07\n",
      "Iteration: 9176 Loss: 8.565815985671088e-07\n",
      "Iteration: 9177 Loss: 8.557965315811113e-07\n",
      "Iteration: 9178 Loss: 8.550121841952132e-07\n",
      "Iteration: 9179 Loss: 8.542285555764703e-07\n",
      "Iteration: 9180 Loss: 8.534456452369044e-07\n",
      "Iteration: 9181 Loss: 8.526634524633178e-07\n",
      "Iteration: 9182 Loss: 8.518819765989185e-07\n",
      "Iteration: 9183 Loss: 8.511012169834561e-07\n",
      "Iteration: 9184 Loss: 8.503211729587791e-07\n",
      "Iteration: 9185 Loss: 8.495418438747362e-07\n",
      "Iteration: 9186 Loss: 8.487632290719962e-07\n",
      "Iteration: 9187 Loss: 8.479853278993117e-07\n",
      "Iteration: 9188 Loss: 8.47208139757355e-07\n",
      "Iteration: 9189 Loss: 8.464316638194033e-07\n",
      "Iteration: 9190 Loss: 8.456558996076431e-07\n",
      "Iteration: 9191 Loss: 8.448808464109586e-07\n",
      "Iteration: 9192 Loss: 8.441065035789333e-07\n",
      "Iteration: 9193 Loss: 8.433328704575588e-07\n",
      "Iteration: 9194 Loss: 8.425599463991646e-07\n",
      "Iteration: 9195 Loss: 8.417877307534095e-07\n",
      "Iteration: 9196 Loss: 8.410162228696052e-07\n",
      "Iteration: 9197 Loss: 8.402454221012111e-07\n",
      "Iteration: 9198 Loss: 8.394753277991643e-07\n",
      "Iteration: 9199 Loss: 8.387059393143126e-07\n",
      "Iteration: 9200 Loss: 8.379372560017177e-07\n",
      "Iteration: 9201 Loss: 8.371692772720437e-07\n",
      "Iteration: 9202 Loss: 8.364020023649291e-07\n",
      "Iteration: 9203 Loss: 8.356354306913506e-07\n",
      "Iteration: 9204 Loss: 8.348695615525821e-07\n",
      "Iteration: 9205 Loss: 8.34104394415508e-07\n",
      "Iteration: 9206 Loss: 8.333399285821895e-07\n",
      "Iteration: 9207 Loss: 8.32576163490659e-07\n",
      "Iteration: 9208 Loss: 8.318130983351393e-07\n",
      "Iteration: 9209 Loss: 8.310507325538439e-07\n",
      "Iteration: 9210 Loss: 8.30289065509388e-07\n",
      "Iteration: 9211 Loss: 8.295280964755145e-07\n",
      "Iteration: 9212 Loss: 8.287678249807903e-07\n",
      "Iteration: 9213 Loss: 8.280082503009598e-07\n",
      "Iteration: 9214 Loss: 8.272493717984093e-07\n",
      "Iteration: 9215 Loss: 8.264911888353428e-07\n",
      "Iteration: 9216 Loss: 8.257337008307641e-07\n",
      "Iteration: 9217 Loss: 8.249769069737038e-07\n",
      "Iteration: 9218 Loss: 8.24220806805443e-07\n",
      "Iteration: 9219 Loss: 8.234653996286021e-07\n",
      "Iteration: 9220 Loss: 8.227106848088556e-07\n",
      "Iteration: 9221 Loss: 8.219566617686854e-07\n",
      "Iteration: 9222 Loss: 8.212033297618385e-07\n",
      "Iteration: 9223 Loss: 8.204506882071557e-07\n",
      "Iteration: 9224 Loss: 8.196987364776188e-07\n",
      "Iteration: 9225 Loss: 8.189474739386972e-07\n",
      "Iteration: 9226 Loss: 8.181968999011394e-07\n",
      "Iteration: 9227 Loss: 8.174470138476428e-07\n",
      "Iteration: 9228 Loss: 8.166978150912003e-07\n",
      "Iteration: 9229 Loss: 8.159493030019363e-07\n",
      "Iteration: 9230 Loss: 8.152014769498539e-07\n",
      "Iteration: 9231 Loss: 8.144543363076757e-07\n",
      "Iteration: 9232 Loss: 8.137078805037041e-07\n",
      "Iteration: 9233 Loss: 8.129621087951843e-07\n",
      "Iteration: 9234 Loss: 8.122170206122799e-07\n",
      "Iteration: 9235 Loss: 8.114726153298392e-07\n",
      "Iteration: 9236 Loss: 8.107288922648728e-07\n",
      "Iteration: 9237 Loss: 8.099858509053512e-07\n",
      "Iteration: 9238 Loss: 8.092434905670109e-07\n",
      "Iteration: 9239 Loss: 8.085018106289382e-07\n",
      "Iteration: 9240 Loss: 8.077608105243758e-07\n",
      "Iteration: 9241 Loss: 8.070204895150155e-07\n",
      "Iteration: 9242 Loss: 8.062808469801377e-07\n",
      "Iteration: 9243 Loss: 8.055418824674045e-07\n",
      "Iteration: 9244 Loss: 8.048035951856566e-07\n",
      "Iteration: 9245 Loss: 8.040659845680734e-07\n",
      "Iteration: 9246 Loss: 8.033290499981317e-07\n",
      "Iteration: 9247 Loss: 8.02592790854221e-07\n",
      "Iteration: 9248 Loss: 8.01857206461033e-07\n",
      "Iteration: 9249 Loss: 8.011222963141532e-07\n",
      "Iteration: 9250 Loss: 8.00388059737547e-07\n",
      "Iteration: 9251 Loss: 7.99654496115553e-07\n",
      "Iteration: 9252 Loss: 7.989216048301912e-07\n",
      "Iteration: 9253 Loss: 7.981893852655507e-07\n",
      "Iteration: 9254 Loss: 7.97457836807267e-07\n",
      "Iteration: 9255 Loss: 7.967269588367019e-07\n",
      "Iteration: 9256 Loss: 7.959967508009603e-07\n",
      "Iteration: 9257 Loss: 7.95267211911129e-07\n",
      "Iteration: 9258 Loss: 7.945383417251875e-07\n",
      "Iteration: 9259 Loss: 7.938101395779196e-07\n",
      "Iteration: 9260 Loss: 7.930826048505653e-07\n",
      "Iteration: 9261 Loss: 7.923557368227741e-07\n",
      "Iteration: 9262 Loss: 7.916295351089445e-07\n",
      "Iteration: 9263 Loss: 7.909039989821538e-07\n",
      "Iteration: 9264 Loss: 7.901791278353538e-07\n",
      "Iteration: 9265 Loss: 7.894549210609565e-07\n",
      "Iteration: 9266 Loss: 7.887313780463273e-07\n",
      "Iteration: 9267 Loss: 7.880084981842112e-07\n",
      "Iteration: 9268 Loss: 7.872862808681795e-07\n",
      "Iteration: 9269 Loss: 7.86564725489033e-07\n",
      "Iteration: 9270 Loss: 7.858438314415685e-07\n",
      "Iteration: 9271 Loss: 7.851235981180569e-07\n",
      "Iteration: 9272 Loss: 7.844040248551758e-07\n",
      "Iteration: 9273 Loss: 7.836851111668399e-07\n",
      "Iteration: 9274 Loss: 7.829668564430149e-07\n",
      "Iteration: 9275 Loss: 7.822492599687548e-07\n",
      "Iteration: 9276 Loss: 7.815323211977911e-07\n",
      "Iteration: 9277 Loss: 7.808160395251682e-07\n",
      "Iteration: 9278 Loss: 7.801004143506624e-07\n",
      "Iteration: 9279 Loss: 7.793854450714267e-07\n",
      "Iteration: 9280 Loss: 7.786711310868568e-07\n",
      "Iteration: 9281 Loss: 7.779574717980372e-07\n",
      "Iteration: 9282 Loss: 7.772444666009403e-07\n",
      "Iteration: 9283 Loss: 7.765321148987931e-07\n",
      "Iteration: 9284 Loss: 7.758204160946529e-07\n",
      "Iteration: 9285 Loss: 7.75109369642408e-07\n",
      "Iteration: 9286 Loss: 7.743989748875958e-07\n",
      "Iteration: 9287 Loss: 7.736892311798166e-07\n",
      "Iteration: 9288 Loss: 7.729801379771399e-07\n",
      "Iteration: 9289 Loss: 7.722716946819058e-07\n",
      "Iteration: 9290 Loss: 7.715639007006377e-07\n",
      "Iteration: 9291 Loss: 7.708567554409955e-07\n",
      "Iteration: 9292 Loss: 7.701502583021554e-07\n",
      "Iteration: 9293 Loss: 7.694444086920662e-07\n",
      "Iteration: 9294 Loss: 7.687392060187717e-07\n",
      "Iteration: 9295 Loss: 7.680346497479182e-07\n",
      "Iteration: 9296 Loss: 7.67330739167792e-07\n",
      "Iteration: 9297 Loss: 7.666274736896876e-07\n",
      "Iteration: 9298 Loss: 7.659248528383343e-07\n",
      "Iteration: 9299 Loss: 7.652228759613978e-07\n",
      "Iteration: 9300 Loss: 7.645215425299437e-07\n",
      "Iteration: 9301 Loss: 7.638208518358586e-07\n",
      "Iteration: 9302 Loss: 7.631208033523883e-07\n",
      "Iteration: 9303 Loss: 7.624213964871992e-07\n",
      "Iteration: 9304 Loss: 7.617226305962948e-07\n",
      "Iteration: 9305 Loss: 7.610245052047682e-07\n",
      "Iteration: 9306 Loss: 7.603270196717866e-07\n",
      "Iteration: 9307 Loss: 7.596301734634662e-07\n",
      "Iteration: 9308 Loss: 7.589339658271344e-07\n",
      "Iteration: 9309 Loss: 7.582383963469928e-07\n",
      "Iteration: 9310 Loss: 7.575434644638247e-07\n",
      "Iteration: 9311 Loss: 7.568491694264667e-07\n",
      "Iteration: 9312 Loss: 7.561555107347544e-07\n",
      "Iteration: 9313 Loss: 7.554624879554408e-07\n",
      "Iteration: 9314 Loss: 7.547701001485659e-07\n",
      "Iteration: 9315 Loss: 7.540783468501126e-07\n",
      "Iteration: 9316 Loss: 7.533872277162775e-07\n",
      "Iteration: 9317 Loss: 7.52696742015532e-07\n",
      "Iteration: 9318 Loss: 7.520068891710654e-07\n",
      "Iteration: 9319 Loss: 7.513176685998441e-07\n",
      "Iteration: 9320 Loss: 7.506290797262009e-07\n",
      "Iteration: 9321 Loss: 7.499411220230776e-07\n",
      "Iteration: 9322 Loss: 7.492537948011434e-07\n",
      "Iteration: 9323 Loss: 7.485670975381737e-07\n",
      "Iteration: 9324 Loss: 7.478810296586731e-07\n",
      "Iteration: 9325 Loss: 7.471955905830597e-07\n",
      "Iteration: 9326 Loss: 7.465107797960646e-07\n",
      "Iteration: 9327 Loss: 7.458265964635919e-07\n",
      "Iteration: 9328 Loss: 7.451430403485429e-07\n",
      "Iteration: 9329 Loss: 7.444601107383416e-07\n",
      "Iteration: 9330 Loss: 7.437778070562713e-07\n",
      "Iteration: 9331 Loss: 7.430961289348478e-07\n",
      "Iteration: 9332 Loss: 7.424150751874177e-07\n",
      "Iteration: 9333 Loss: 7.417346458538613e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9334 Loss: 7.410548401573825e-07\n",
      "Iteration: 9335 Loss: 7.403756575258292e-07\n",
      "Iteration: 9336 Loss: 7.396970972765096e-07\n",
      "Iteration: 9337 Loss: 7.390191590653218e-07\n",
      "Iteration: 9338 Loss: 7.383418422084847e-07\n",
      "Iteration: 9339 Loss: 7.376651461363199e-07\n",
      "Iteration: 9340 Loss: 7.369890702795991e-07\n",
      "Iteration: 9341 Loss: 7.363136140702965e-07\n",
      "Iteration: 9342 Loss: 7.356387769407381e-07\n",
      "Iteration: 9343 Loss: 7.349645582665528e-07\n",
      "Iteration: 9344 Loss: 7.342909575938919e-07\n",
      "Iteration: 9345 Loss: 7.336179742426686e-07\n",
      "Iteration: 9346 Loss: 7.329456077626177e-07\n",
      "Iteration: 9347 Loss: 7.322738575303326e-07\n",
      "Iteration: 9348 Loss: 7.316027229816801e-07\n",
      "Iteration: 9349 Loss: 7.309322035488125e-07\n",
      "Iteration: 9350 Loss: 7.302622986731366e-07\n",
      "Iteration: 9351 Loss: 7.295930077864157e-07\n",
      "Iteration: 9352 Loss: 7.289243304436011e-07\n",
      "Iteration: 9353 Loss: 7.282562659106622e-07\n",
      "Iteration: 9354 Loss: 7.275888136821979e-07\n",
      "Iteration: 9355 Loss: 7.269219731390965e-07\n",
      "Iteration: 9356 Loss: 7.262557437820459e-07\n",
      "Iteration: 9357 Loss: 7.255901250464267e-07\n",
      "Iteration: 9358 Loss: 7.249251163724232e-07\n",
      "Iteration: 9359 Loss: 7.242607172051605e-07\n",
      "Iteration: 9360 Loss: 7.235969269800756e-07\n",
      "Iteration: 9361 Loss: 7.22933745144814e-07\n",
      "Iteration: 9362 Loss: 7.222711711359559e-07\n",
      "Iteration: 9363 Loss: 7.216092044610148e-07\n",
      "Iteration: 9364 Loss: 7.209478443826728e-07\n",
      "Iteration: 9365 Loss: 7.202870905255928e-07\n",
      "Iteration: 9366 Loss: 7.196269422704622e-07\n",
      "Iteration: 9367 Loss: 7.189673990652875e-07\n",
      "Iteration: 9368 Loss: 7.183084603544372e-07\n",
      "Iteration: 9369 Loss: 7.17650125584634e-07\n",
      "Iteration: 9370 Loss: 7.169923941722127e-07\n",
      "Iteration: 9371 Loss: 7.163352655661446e-07\n",
      "Iteration: 9372 Loss: 7.156787393563331e-07\n",
      "Iteration: 9373 Loss: 7.150228148177396e-07\n",
      "Iteration: 9374 Loss: 7.143674914589612e-07\n",
      "Iteration: 9375 Loss: 7.137127686683892e-07\n",
      "Iteration: 9376 Loss: 7.130586460686238e-07\n",
      "Iteration: 9377 Loss: 7.124051229957751e-07\n",
      "Iteration: 9378 Loss: 7.117521988434422e-07\n",
      "Iteration: 9379 Loss: 7.110998731750112e-07\n",
      "Iteration: 9380 Loss: 7.104481453298352e-07\n",
      "Iteration: 9381 Loss: 7.097970148165952e-07\n",
      "Iteration: 9382 Loss: 7.09146481086881e-07\n",
      "Iteration: 9383 Loss: 7.084965435938201e-07\n",
      "Iteration: 9384 Loss: 7.078472017941293e-07\n",
      "Iteration: 9385 Loss: 7.071984551352799e-07\n",
      "Iteration: 9386 Loss: 7.06550303077236e-07\n",
      "Iteration: 9387 Loss: 7.059027450745059e-07\n",
      "Iteration: 9388 Loss: 7.052557805809514e-07\n",
      "Iteration: 9389 Loss: 7.046094089684544e-07\n",
      "Iteration: 9390 Loss: 7.03963629863795e-07\n",
      "Iteration: 9391 Loss: 7.033184426368723e-07\n",
      "Iteration: 9392 Loss: 7.026738468045848e-07\n",
      "Iteration: 9393 Loss: 7.020298416546417e-07\n",
      "Iteration: 9394 Loss: 7.013864268116743e-07\n",
      "Iteration: 9395 Loss: 7.007436016826284e-07\n",
      "Iteration: 9396 Loss: 7.001013657256502e-07\n",
      "Iteration: 9397 Loss: 6.994597184017149e-07\n",
      "Iteration: 9398 Loss: 6.988186591695042e-07\n",
      "Iteration: 9399 Loss: 6.981781874902298e-07\n",
      "Iteration: 9400 Loss: 6.975383028276205e-07\n",
      "Iteration: 9401 Loss: 6.968990046395643e-07\n",
      "Iteration: 9402 Loss: 6.962602924492907e-07\n",
      "Iteration: 9403 Loss: 6.956221655472775e-07\n",
      "Iteration: 9404 Loss: 6.949846235666904e-07\n",
      "Iteration: 9405 Loss: 6.943476659171907e-07\n",
      "Iteration: 9406 Loss: 6.937112920620996e-07\n",
      "Iteration: 9407 Loss: 6.930755014678515e-07\n",
      "Iteration: 9408 Loss: 6.924402936502922e-07\n",
      "Iteration: 9409 Loss: 6.918056679136046e-07\n",
      "Iteration: 9410 Loss: 6.911716238871716e-07\n",
      "Iteration: 9411 Loss: 6.905381609858063e-07\n",
      "Iteration: 9412 Loss: 6.899052786773119e-07\n",
      "Iteration: 9413 Loss: 6.892729764252925e-07\n",
      "Iteration: 9414 Loss: 6.886412537030783e-07\n",
      "Iteration: 9415 Loss: 6.880101099754464e-07\n",
      "Iteration: 9416 Loss: 6.873795447133861e-07\n",
      "Iteration: 9417 Loss: 6.867495572734163e-07\n",
      "Iteration: 9418 Loss: 6.861201472960535e-07\n",
      "Iteration: 9419 Loss: 6.85491314194733e-07\n",
      "Iteration: 9420 Loss: 6.848630575544285e-07\n",
      "Iteration: 9421 Loss: 6.842353767363105e-07\n",
      "Iteration: 9422 Loss: 6.836082710926721e-07\n",
      "Iteration: 9423 Loss: 6.829817402720505e-07\n",
      "Iteration: 9424 Loss: 6.823557836886044e-07\n",
      "Iteration: 9425 Loss: 6.817304007586665e-07\n",
      "Iteration: 9426 Loss: 6.811055910717452e-07\n",
      "Iteration: 9427 Loss: 6.804813540450247e-07\n",
      "Iteration: 9428 Loss: 6.798576891553244e-07\n",
      "Iteration: 9429 Loss: 6.792345958748091e-07\n",
      "Iteration: 9430 Loss: 6.786120736817396e-07\n",
      "Iteration: 9431 Loss: 6.779901221664733e-07\n",
      "Iteration: 9432 Loss: 6.77368740518946e-07\n",
      "Iteration: 9433 Loss: 6.767479283916437e-07\n",
      "Iteration: 9434 Loss: 6.761276853171721e-07\n",
      "Iteration: 9435 Loss: 6.75508010718041e-07\n",
      "Iteration: 9436 Loss: 6.748889040711441e-07\n",
      "Iteration: 9437 Loss: 6.742703648565246e-07\n",
      "Iteration: 9438 Loss: 6.736523925561933e-07\n",
      "Iteration: 9439 Loss: 6.730349866514249e-07\n",
      "Iteration: 9440 Loss: 6.724181466186882e-07\n",
      "Iteration: 9441 Loss: 6.718018719419582e-07\n",
      "Iteration: 9442 Loss: 6.711861622499554e-07\n",
      "Iteration: 9443 Loss: 6.70571016730111e-07\n",
      "Iteration: 9444 Loss: 6.699564350134627e-07\n",
      "Iteration: 9445 Loss: 6.693424165817642e-07\n",
      "Iteration: 9446 Loss: 6.68728960920984e-07\n",
      "Iteration: 9447 Loss: 6.681160675123942e-07\n",
      "Iteration: 9448 Loss: 6.675037356954393e-07\n",
      "Iteration: 9449 Loss: 6.66891965251726e-07\n",
      "Iteration: 9450 Loss: 6.662807555147493e-07\n",
      "Iteration: 9451 Loss: 6.656701060297008e-07\n",
      "Iteration: 9452 Loss: 6.650600161441524e-07\n",
      "Iteration: 9453 Loss: 6.644504853965665e-07\n",
      "Iteration: 9454 Loss: 6.638415134201922e-07\n",
      "Iteration: 9455 Loss: 6.632330996710638e-07\n",
      "Iteration: 9456 Loss: 6.626252434727471e-07\n",
      "Iteration: 9457 Loss: 6.620179443374433e-07\n",
      "Iteration: 9458 Loss: 6.614112018142818e-07\n",
      "Iteration: 9459 Loss: 6.608050153914697e-07\n",
      "Iteration: 9460 Loss: 6.601993845589234e-07\n",
      "Iteration: 9461 Loss: 6.595943088096656e-07\n",
      "Iteration: 9462 Loss: 6.58989787633762e-07\n",
      "Iteration: 9463 Loss: 6.583858205238156e-07\n",
      "Iteration: 9464 Loss: 6.577824069683034e-07\n",
      "Iteration: 9465 Loss: 6.571795464649069e-07\n",
      "Iteration: 9466 Loss: 6.565772385037729e-07\n",
      "Iteration: 9467 Loss: 6.559754825795087e-07\n",
      "Iteration: 9468 Loss: 6.553742781847939e-07\n",
      "Iteration: 9469 Loss: 6.54773624816489e-07\n",
      "Iteration: 9470 Loss: 6.541735219678321e-07\n",
      "Iteration: 9471 Loss: 6.535739691339435e-07\n",
      "Iteration: 9472 Loss: 6.529749658104593e-07\n",
      "Iteration: 9473 Loss: 6.523765114976799e-07\n",
      "Iteration: 9474 Loss: 6.517786056867313e-07\n",
      "Iteration: 9475 Loss: 6.511812478789579e-07\n",
      "Iteration: 9476 Loss: 6.505844375687297e-07\n",
      "Iteration: 9477 Loss: 6.499881742589431e-07\n",
      "Iteration: 9478 Loss: 6.493924574457247e-07\n",
      "Iteration: 9479 Loss: 6.487972866246736e-07\n",
      "Iteration: 9480 Loss: 6.482026613009293e-07\n",
      "Iteration: 9481 Loss: 6.476085809747453e-07\n",
      "Iteration: 9482 Loss: 6.470150451394781e-07\n",
      "Iteration: 9483 Loss: 6.464220533017766e-07\n",
      "Iteration: 9484 Loss: 6.458296049645479e-07\n",
      "Iteration: 9485 Loss: 6.452376996252337e-07\n",
      "Iteration: 9486 Loss: 6.446463367885098e-07\n",
      "Iteration: 9487 Loss: 6.440555159565174e-07\n",
      "Iteration: 9488 Loss: 6.43465236632998e-07\n",
      "Iteration: 9489 Loss: 6.428754983203027e-07\n",
      "Iteration: 9490 Loss: 6.422863005246628e-07\n",
      "Iteration: 9491 Loss: 6.416976427501677e-07\n",
      "Iteration: 9492 Loss: 6.411095245015813e-07\n",
      "Iteration: 9493 Loss: 6.405219452812896e-07\n",
      "Iteration: 9494 Loss: 6.39934904600334e-07\n",
      "Iteration: 9495 Loss: 6.393484019635917e-07\n",
      "Iteration: 9496 Loss: 6.387624368754969e-07\n",
      "Iteration: 9497 Loss: 6.381770088467117e-07\n",
      "Iteration: 9498 Loss: 6.375921175864513e-07\n",
      "Iteration: 9499 Loss: 6.370077620476192e-07\n",
      "Iteration: 9500 Loss: 6.364239422414199e-07\n",
      "Iteration: 9501 Loss: 6.358406574666965e-07\n",
      "Iteration: 9502 Loss: 6.35257907350694e-07\n",
      "Iteration: 9503 Loss: 6.346756913459495e-07\n",
      "Iteration: 9504 Loss: 6.340940089635608e-07\n",
      "Iteration: 9505 Loss: 6.335128597130399e-07\n",
      "Iteration: 9506 Loss: 6.329322431050349e-07\n",
      "Iteration: 9507 Loss: 6.323521586547337e-07\n",
      "Iteration: 9508 Loss: 6.317726059274848e-07\n",
      "Iteration: 9509 Loss: 6.311935843267947e-07\n",
      "Iteration: 9510 Loss: 6.30615093359962e-07\n",
      "Iteration: 9511 Loss: 6.300371326578128e-07\n",
      "Iteration: 9512 Loss: 6.294597016785538e-07\n",
      "Iteration: 9513 Loss: 6.288827999331619e-07\n",
      "Iteration: 9514 Loss: 6.28306426940815e-07\n",
      "Iteration: 9515 Loss: 6.277305822135927e-07\n",
      "Iteration: 9516 Loss: 6.271552652685926e-07\n",
      "Iteration: 9517 Loss: 6.265804755652301e-07\n",
      "Iteration: 9518 Loss: 6.260062127897554e-07\n",
      "Iteration: 9519 Loss: 6.254324762909215e-07\n",
      "Iteration: 9520 Loss: 6.248592656401734e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9521 Loss: 6.242865803566437e-07\n",
      "Iteration: 9522 Loss: 6.237144199617812e-07\n",
      "Iteration: 9523 Loss: 6.231427839707865e-07\n",
      "Iteration: 9524 Loss: 6.225716717916713e-07\n",
      "Iteration: 9525 Loss: 6.220010831690981e-07\n",
      "Iteration: 9526 Loss: 6.214310175126818e-07\n",
      "Iteration: 9527 Loss: 6.20861474342012e-07\n",
      "Iteration: 9528 Loss: 6.202924531759663e-07\n",
      "Iteration: 9529 Loss: 6.197239535949992e-07\n",
      "Iteration: 9530 Loss: 6.191559750645192e-07\n",
      "Iteration: 9531 Loss: 6.185885170501517e-07\n",
      "Iteration: 9532 Loss: 6.180215791297623e-07\n",
      "Iteration: 9533 Loss: 6.174551608294691e-07\n",
      "Iteration: 9534 Loss: 6.168892617549164e-07\n",
      "Iteration: 9535 Loss: 6.163238812647284e-07\n",
      "Iteration: 9536 Loss: 6.157590189627849e-07\n",
      "Iteration: 9537 Loss: 6.151946743786246e-07\n",
      "Iteration: 9538 Loss: 6.146308470076097e-07\n",
      "Iteration: 9539 Loss: 6.140675364320387e-07\n",
      "Iteration: 9540 Loss: 6.13504742094136e-07\n",
      "Iteration: 9541 Loss: 6.129424636330671e-07\n",
      "Iteration: 9542 Loss: 6.12380700521817e-07\n",
      "Iteration: 9543 Loss: 6.118194522843542e-07\n",
      "Iteration: 9544 Loss: 6.11258718453785e-07\n",
      "Iteration: 9545 Loss: 6.106984985526352e-07\n",
      "Iteration: 9546 Loss: 6.101387921133441e-07\n",
      "Iteration: 9547 Loss: 6.095795987216637e-07\n",
      "Iteration: 9548 Loss: 6.090209177362553e-07\n",
      "Iteration: 9549 Loss: 6.084627489175005e-07\n",
      "Iteration: 9550 Loss: 6.079050915650243e-07\n",
      "Iteration: 9551 Loss: 6.073479453831155e-07\n",
      "Iteration: 9552 Loss: 6.067913098437609e-07\n",
      "Iteration: 9553 Loss: 6.062351844828071e-07\n",
      "Iteration: 9554 Loss: 6.056795689749465e-07\n",
      "Iteration: 9555 Loss: 6.05124462562055e-07\n",
      "Iteration: 9556 Loss: 6.045698648337187e-07\n",
      "Iteration: 9557 Loss: 6.040157755042336e-07\n",
      "Iteration: 9558 Loss: 6.034621939624005e-07\n",
      "Iteration: 9559 Loss: 6.029091198504889e-07\n",
      "Iteration: 9560 Loss: 6.023565526535285e-07\n",
      "Iteration: 9561 Loss: 6.018044919041471e-07\n",
      "Iteration: 9562 Loss: 6.012529371900583e-07\n",
      "Iteration: 9563 Loss: 6.007018878866653e-07\n",
      "Iteration: 9564 Loss: 6.00151343695897e-07\n",
      "Iteration: 9565 Loss: 5.996013040957162e-07\n",
      "Iteration: 9566 Loss: 5.990517686270237e-07\n",
      "Iteration: 9567 Loss: 5.985027368271446e-07\n",
      "Iteration: 9568 Loss: 5.979542082346539e-07\n",
      "Iteration: 9569 Loss: 5.974061824455473e-07\n",
      "Iteration: 9570 Loss: 5.968586588840935e-07\n",
      "Iteration: 9571 Loss: 5.963116371460825e-07\n",
      "Iteration: 9572 Loss: 5.957651168587786e-07\n",
      "Iteration: 9573 Loss: 5.952190973347573e-07\n",
      "Iteration: 9574 Loss: 5.94673578519005e-07\n",
      "Iteration: 9575 Loss: 5.941285594299804e-07\n",
      "Iteration: 9576 Loss: 5.93584039924459e-07\n",
      "Iteration: 9577 Loss: 5.930400194908404e-07\n",
      "Iteration: 9578 Loss: 5.924964976739399e-07\n",
      "Iteration: 9579 Loss: 5.919534737786387e-07\n",
      "Iteration: 9580 Loss: 5.914109478172926e-07\n",
      "Iteration: 9581 Loss: 5.908689191003676e-07\n",
      "Iteration: 9582 Loss: 5.903273871720289e-07\n",
      "Iteration: 9583 Loss: 5.897863515759077e-07\n",
      "Iteration: 9584 Loss: 5.892458118578192e-07\n",
      "Iteration: 9585 Loss: 5.887057675645491e-07\n",
      "Iteration: 9586 Loss: 5.881662181269634e-07\n",
      "Iteration: 9587 Loss: 5.876271633200157e-07\n",
      "Iteration: 9588 Loss: 5.870886025735798e-07\n",
      "Iteration: 9589 Loss: 5.865505354380253e-07\n",
      "Iteration: 9590 Loss: 5.860129615443382e-07\n",
      "Iteration: 9591 Loss: 5.854758802732901e-07\n",
      "Iteration: 9592 Loss: 5.849392913108401e-07\n",
      "Iteration: 9593 Loss: 5.844031940676449e-07\n",
      "Iteration: 9594 Loss: 5.83867588204652e-07\n",
      "Iteration: 9595 Loss: 5.833324731283977e-07\n",
      "Iteration: 9596 Loss: 5.827978486191387e-07\n",
      "Iteration: 9597 Loss: 5.822637141105088e-07\n",
      "Iteration: 9598 Loss: 5.817300692124843e-07\n",
      "Iteration: 9599 Loss: 5.811969133356114e-07\n",
      "Iteration: 9600 Loss: 5.806642461413006e-07\n",
      "Iteration: 9601 Loss: 5.801320671566563e-07\n",
      "Iteration: 9602 Loss: 5.796003759321764e-07\n",
      "Iteration: 9603 Loss: 5.790691720205365e-07\n",
      "Iteration: 9604 Loss: 5.785384549787273e-07\n",
      "Iteration: 9605 Loss: 5.780082244373541e-07\n",
      "Iteration: 9606 Loss: 5.774784797904136e-07\n",
      "Iteration: 9607 Loss: 5.769492206726139e-07\n",
      "Iteration: 9608 Loss: 5.764204465835888e-07\n",
      "Iteration: 9609 Loss: 5.758921572451066e-07\n",
      "Iteration: 9610 Loss: 5.753643520471952e-07\n",
      "Iteration: 9611 Loss: 5.748370306012024e-07\n",
      "Iteration: 9612 Loss: 5.743101924659458e-07\n",
      "Iteration: 9613 Loss: 5.7378383719296e-07\n",
      "Iteration: 9614 Loss: 5.732579643458395e-07\n",
      "Iteration: 9615 Loss: 5.727325734802958e-07\n",
      "Iteration: 9616 Loss: 5.722076641523454e-07\n",
      "Iteration: 9617 Loss: 5.716832359223154e-07\n",
      "Iteration: 9618 Loss: 5.71159288406477e-07\n",
      "Iteration: 9619 Loss: 5.706358209948659e-07\n",
      "Iteration: 9620 Loss: 5.701128334143801e-07\n",
      "Iteration: 9621 Loss: 5.695903251709828e-07\n",
      "Iteration: 9622 Loss: 5.690682958814786e-07\n",
      "Iteration: 9623 Loss: 5.685467449382799e-07\n",
      "Iteration: 9624 Loss: 5.680256720691889e-07\n",
      "Iteration: 9625 Loss: 5.675050767839225e-07\n",
      "Iteration: 9626 Loss: 5.66984958527813e-07\n",
      "Iteration: 9627 Loss: 5.664653170924e-07\n",
      "Iteration: 9628 Loss: 5.659461519283919e-07\n",
      "Iteration: 9629 Loss: 5.654274625953135e-07\n",
      "Iteration: 9630 Loss: 5.649092486591537e-07\n",
      "Iteration: 9631 Loss: 5.643915096858873e-07\n",
      "Iteration: 9632 Loss: 5.638742452934994e-07\n",
      "Iteration: 9633 Loss: 5.633574549368307e-07\n",
      "Iteration: 9634 Loss: 5.628411382936792e-07\n",
      "Iteration: 9635 Loss: 5.62325294815751e-07\n",
      "Iteration: 9636 Loss: 5.618099241859328e-07\n",
      "Iteration: 9637 Loss: 5.612950257931894e-07\n",
      "Iteration: 9638 Loss: 5.607805993836548e-07\n",
      "Iteration: 9639 Loss: 5.602666445180352e-07\n",
      "Iteration: 9640 Loss: 5.597531605990733e-07\n",
      "Iteration: 9641 Loss: 5.59240147361362e-07\n",
      "Iteration: 9642 Loss: 5.587276043193215e-07\n",
      "Iteration: 9643 Loss: 5.58215531098302e-07\n",
      "Iteration: 9644 Loss: 5.577039270959191e-07\n",
      "Iteration: 9645 Loss: 5.571927921100458e-07\n",
      "Iteration: 9646 Loss: 5.566821255422331e-07\n",
      "Iteration: 9647 Loss: 5.561719270170117e-07\n",
      "Iteration: 9648 Loss: 5.556621961061745e-07\n",
      "Iteration: 9649 Loss: 5.551529323269488e-07\n",
      "Iteration: 9650 Loss: 5.546441353604482e-07\n",
      "Iteration: 9651 Loss: 5.541358047265414e-07\n",
      "Iteration: 9652 Loss: 5.53627939994169e-07\n",
      "Iteration: 9653 Loss: 5.531205407941583e-07\n",
      "Iteration: 9654 Loss: 5.526136065890733e-07\n",
      "Iteration: 9655 Loss: 5.521071369492739e-07\n",
      "Iteration: 9656 Loss: 5.516011315618994e-07\n",
      "Iteration: 9657 Loss: 5.510955899470983e-07\n",
      "Iteration: 9658 Loss: 5.505905116800151e-07\n",
      "Iteration: 9659 Loss: 5.500858963337505e-07\n",
      "Iteration: 9660 Loss: 5.49581743485376e-07\n",
      "Iteration: 9661 Loss: 5.49078052711238e-07\n",
      "Iteration: 9662 Loss: 5.485748235877094e-07\n",
      "Iteration: 9663 Loss: 5.480720556912827e-07\n",
      "Iteration: 9664 Loss: 5.475697486573235e-07\n",
      "Iteration: 9665 Loss: 5.470679019489071e-07\n",
      "Iteration: 9666 Loss: 5.465665151975175e-07\n",
      "Iteration: 9667 Loss: 5.460655879306724e-07\n",
      "Iteration: 9668 Loss: 5.455651198377519e-07\n",
      "Iteration: 9669 Loss: 5.450651104408859e-07\n",
      "Iteration: 9670 Loss: 5.445655593782443e-07\n",
      "Iteration: 9671 Loss: 5.440664662035367e-07\n",
      "Iteration: 9672 Loss: 5.435678302855936e-07\n",
      "Iteration: 9673 Loss: 5.430696514192748e-07\n",
      "Iteration: 9674 Loss: 5.425719292669593e-07\n",
      "Iteration: 9675 Loss: 5.420746632332854e-07\n",
      "Iteration: 9676 Loss: 5.415778529060155e-07\n",
      "Iteration: 9677 Loss: 5.410814979804268e-07\n",
      "Iteration: 9678 Loss: 5.405855980395218e-07\n",
      "Iteration: 9679 Loss: 5.400901524947405e-07\n",
      "Iteration: 9680 Loss: 5.395951610986826e-07\n",
      "Iteration: 9681 Loss: 5.391006233790135e-07\n",
      "Iteration: 9682 Loss: 5.386065389227038e-07\n",
      "Iteration: 9683 Loss: 5.381129073108666e-07\n",
      "Iteration: 9684 Loss: 5.376197281308487e-07\n",
      "Iteration: 9685 Loss: 5.371270009652045e-07\n",
      "Iteration: 9686 Loss: 5.366347254027285e-07\n",
      "Iteration: 9687 Loss: 5.361429010289006e-07\n",
      "Iteration: 9688 Loss: 5.356515274857299e-07\n",
      "Iteration: 9689 Loss: 5.351606042743284e-07\n",
      "Iteration: 9690 Loss: 5.346701309832112e-07\n",
      "Iteration: 9691 Loss: 5.341801072861437e-07\n",
      "Iteration: 9692 Loss: 5.336905326556624e-07\n",
      "Iteration: 9693 Loss: 5.332014066822715e-07\n",
      "Iteration: 9694 Loss: 5.327127290640432e-07\n",
      "Iteration: 9695 Loss: 5.322244993382451e-07\n",
      "Iteration: 9696 Loss: 5.317367171488093e-07\n",
      "Iteration: 9697 Loss: 5.312493819729124e-07\n",
      "Iteration: 9698 Loss: 5.307624933984848e-07\n",
      "Iteration: 9699 Loss: 5.302760511335199e-07\n",
      "Iteration: 9700 Loss: 5.297900547068284e-07\n",
      "Iteration: 9701 Loss: 5.29304503714882e-07\n",
      "Iteration: 9702 Loss: 5.288193977471953e-07\n",
      "Iteration: 9703 Loss: 5.28334736313452e-07\n",
      "Iteration: 9704 Loss: 5.27850519172883e-07\n",
      "Iteration: 9705 Loss: 5.273667458322212e-07\n",
      "Iteration: 9706 Loss: 5.268834159452986e-07\n",
      "Iteration: 9707 Loss: 5.264005289336397e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9708 Loss: 5.259180846184761e-07\n",
      "Iteration: 9709 Loss: 5.254360823669187e-07\n",
      "Iteration: 9710 Loss: 5.249545219434671e-07\n",
      "Iteration: 9711 Loss: 5.244734028886926e-07\n",
      "Iteration: 9712 Loss: 5.239927247956854e-07\n",
      "Iteration: 9713 Loss: 5.235124872600237e-07\n",
      "Iteration: 9714 Loss: 5.230326898822945e-07\n",
      "Iteration: 9715 Loss: 5.225533322534755e-07\n",
      "Iteration: 9716 Loss: 5.220744139721721e-07\n",
      "Iteration: 9717 Loss: 5.215959346383298e-07\n",
      "Iteration: 9718 Loss: 5.211178939025456e-07\n",
      "Iteration: 9719 Loss: 5.206402911959312e-07\n",
      "Iteration: 9720 Loss: 5.201631262856163e-07\n",
      "Iteration: 9721 Loss: 5.196863987129872e-07\n",
      "Iteration: 9722 Loss: 5.192101080781954e-07\n",
      "Iteration: 9723 Loss: 5.187342541870375e-07\n",
      "Iteration: 9724 Loss: 5.18258836077095e-07\n",
      "Iteration: 9725 Loss: 5.177838538549033e-07\n",
      "Iteration: 9726 Loss: 5.173093069709134e-07\n",
      "Iteration: 9727 Loss: 5.168351950240093e-07\n",
      "Iteration: 9728 Loss: 5.163615176194718e-07\n",
      "Iteration: 9729 Loss: 5.158882742993162e-07\n",
      "Iteration: 9730 Loss: 5.154154647786646e-07\n",
      "Iteration: 9731 Loss: 5.149430886063979e-07\n",
      "Iteration: 9732 Loss: 5.144711453834621e-07\n",
      "Iteration: 9733 Loss: 5.139996347145388e-07\n",
      "Iteration: 9734 Loss: 5.135285561991015e-07\n",
      "Iteration: 9735 Loss: 5.130579094437456e-07\n",
      "Iteration: 9736 Loss: 5.12587694053782e-07\n",
      "Iteration: 9737 Loss: 5.121179096336692e-07\n",
      "Iteration: 9738 Loss: 5.116485557860989e-07\n",
      "Iteration: 9739 Loss: 5.111796321189833e-07\n",
      "Iteration: 9740 Loss: 5.107111382370525e-07\n",
      "Iteration: 9741 Loss: 5.102430737447651e-07\n",
      "Iteration: 9742 Loss: 5.097754383986491e-07\n",
      "Iteration: 9743 Loss: 5.093082313624659e-07\n",
      "Iteration: 9744 Loss: 5.088414526846087e-07\n",
      "Iteration: 9745 Loss: 5.08375101829238e-07\n",
      "Iteration: 9746 Loss: 5.079091783962167e-07\n",
      "Iteration: 9747 Loss: 5.074436820579869e-07\n",
      "Iteration: 9748 Loss: 5.069786123043593e-07\n",
      "Iteration: 9749 Loss: 5.065139688048744e-07\n",
      "Iteration: 9750 Loss: 5.060497511677627e-07\n",
      "Iteration: 9751 Loss: 5.055859590028074e-07\n",
      "Iteration: 9752 Loss: 5.051225918624043e-07\n",
      "Iteration: 9753 Loss: 5.04659649470865e-07\n",
      "Iteration: 9754 Loss: 5.04197131382912e-07\n",
      "Iteration: 9755 Loss: 5.037350372934595e-07\n",
      "Iteration: 9756 Loss: 5.032733665629531e-07\n",
      "Iteration: 9757 Loss: 5.028121190505914e-07\n",
      "Iteration: 9758 Loss: 5.023512942888108e-07\n",
      "Iteration: 9759 Loss: 5.018908918912515e-07\n",
      "Iteration: 9760 Loss: 5.014309114673312e-07\n",
      "Iteration: 9761 Loss: 5.009713526312733e-07\n",
      "Iteration: 9762 Loss: 5.005122149969898e-07\n",
      "Iteration: 9763 Loss: 5.00053498180076e-07\n",
      "Iteration: 9764 Loss: 4.995952017917401e-07\n",
      "Iteration: 9765 Loss: 4.99137325450885e-07\n",
      "Iteration: 9766 Loss: 4.98679868768422e-07\n",
      "Iteration: 9767 Loss: 4.982228313592706e-07\n",
      "Iteration: 9768 Loss: 4.977662128418322e-07\n",
      "Iteration: 9769 Loss: 4.973100128335589e-07\n",
      "Iteration: 9770 Loss: 4.96854230946428e-07\n",
      "Iteration: 9771 Loss: 4.963988668001895e-07\n",
      "Iteration: 9772 Loss: 4.959439200098863e-07\n",
      "Iteration: 9773 Loss: 4.954893901972301e-07\n",
      "Iteration: 9774 Loss: 4.95035277032292e-07\n",
      "Iteration: 9775 Loss: 4.94581579965649e-07\n",
      "Iteration: 9776 Loss: 4.94128298782477e-07\n",
      "Iteration: 9777 Loss: 4.936754331071684e-07\n",
      "Iteration: 9778 Loss: 4.932229823845835e-07\n",
      "Iteration: 9779 Loss: 4.927709464068597e-07\n",
      "Iteration: 9780 Loss: 4.923193247351926e-07\n",
      "Iteration: 9781 Loss: 4.91868116990075e-07\n",
      "Iteration: 9782 Loss: 4.914173227935563e-07\n",
      "Iteration: 9783 Loss: 4.909669417655208e-07\n",
      "Iteration: 9784 Loss: 4.905169735281302e-07\n",
      "Iteration: 9785 Loss: 4.900674177019962e-07\n",
      "Iteration: 9786 Loss: 4.896182739095424e-07\n",
      "Iteration: 9787 Loss: 4.891695417745384e-07\n",
      "Iteration: 9788 Loss: 4.887212209179656e-07\n",
      "Iteration: 9789 Loss: 4.882733109607852e-07\n",
      "Iteration: 9790 Loss: 4.878258115319571e-07\n",
      "Iteration: 9791 Loss: 4.873787222523013e-07\n",
      "Iteration: 9792 Loss: 4.869320427439102e-07\n",
      "Iteration: 9793 Loss: 4.864857726326032e-07\n",
      "Iteration: 9794 Loss: 4.860399115443593e-07\n",
      "Iteration: 9795 Loss: 4.855944589569647e-07\n",
      "Iteration: 9796 Loss: 4.851494147899068e-07\n",
      "Iteration: 9797 Loss: 4.847047785205414e-07\n",
      "Iteration: 9798 Loss: 4.842605497770228e-07\n",
      "Iteration: 9799 Loss: 4.838167282471495e-07\n",
      "Iteration: 9800 Loss: 4.833733134319858e-07\n",
      "Iteration: 9801 Loss: 4.829303050224266e-07\n",
      "Iteration: 9802 Loss: 4.824877026476066e-07\n",
      "Iteration: 9803 Loss: 4.82045505930738e-07\n",
      "Iteration: 9804 Loss: 4.816037145046405e-07\n",
      "Iteration: 9805 Loss: 4.811623279951243e-07\n",
      "Iteration: 9806 Loss: 4.807213460322672e-07\n",
      "Iteration: 9807 Loss: 4.802807682452512e-07\n",
      "Iteration: 9808 Loss: 4.798405942628149e-07\n",
      "Iteration: 9809 Loss: 4.794008237150386e-07\n",
      "Iteration: 9810 Loss: 4.789614562891551e-07\n",
      "Iteration: 9811 Loss: 4.785224915042077e-07\n",
      "Iteration: 9812 Loss: 4.780839290439405e-07\n",
      "Iteration: 9813 Loss: 4.776457685430795e-07\n",
      "Iteration: 9814 Loss: 4.772080097146083e-07\n",
      "Iteration: 9815 Loss: 4.7677065211509323e-07\n",
      "Iteration: 9816 Loss: 4.7633369527828425e-07\n",
      "Iteration: 9817 Loss: 4.7589713878114825e-07\n",
      "Iteration: 9818 Loss: 4.754609825509099e-07\n",
      "Iteration: 9819 Loss: 4.750252260721081e-07\n",
      "Iteration: 9820 Loss: 4.745898689805955e-07\n",
      "Iteration: 9821 Loss: 4.7415491096543753e-07\n",
      "Iteration: 9822 Loss: 4.7372035154936904e-07\n",
      "Iteration: 9823 Loss: 4.7328619041917026e-07\n",
      "Iteration: 9824 Loss: 4.7285242715801613e-07\n",
      "Iteration: 9825 Loss: 4.724190615143068e-07\n",
      "Iteration: 9826 Loss: 4.7198609306219037e-07\n",
      "Iteration: 9827 Loss: 4.7155352144337026e-07\n",
      "Iteration: 9828 Loss: 4.7112134643958576e-07\n",
      "Iteration: 9829 Loss: 4.7068956738960525e-07\n",
      "Iteration: 9830 Loss: 4.702581840821571e-07\n",
      "Iteration: 9831 Loss: 4.69827196095766e-07\n",
      "Iteration: 9832 Loss: 4.6939660303388494e-07\n",
      "Iteration: 9833 Loss: 4.689664047176264e-07\n",
      "Iteration: 9834 Loss: 4.685366007487953e-07\n",
      "Iteration: 9835 Loss: 4.681071906540213e-07\n",
      "Iteration: 9836 Loss: 4.676781742419805e-07\n",
      "Iteration: 9837 Loss: 4.6724955098298847e-07\n",
      "Iteration: 9838 Loss: 4.6682132051434844e-07\n",
      "Iteration: 9839 Loss: 4.663934825343044e-07\n",
      "Iteration: 9840 Loss: 4.6596603673938554e-07\n",
      "Iteration: 9841 Loss: 4.6553898279887233e-07\n",
      "Iteration: 9842 Loss: 4.6511232018370515e-07\n",
      "Iteration: 9843 Loss: 4.646860486199102e-07\n",
      "Iteration: 9844 Loss: 4.642601676641122e-07\n",
      "Iteration: 9845 Loss: 4.638346771289941e-07\n",
      "Iteration: 9846 Loss: 4.6340957657006624e-07\n",
      "Iteration: 9847 Loss: 4.6298486563019156e-07\n",
      "Iteration: 9848 Loss: 4.6256054395508406e-07\n",
      "Iteration: 9849 Loss: 4.6213661118292753e-07\n",
      "Iteration: 9850 Loss: 4.617130670179217e-07\n",
      "Iteration: 9851 Loss: 4.612899110463909e-07\n",
      "Iteration: 9852 Loss: 4.608671428554524e-07\n",
      "Iteration: 9853 Loss: 4.6044476214676384e-07\n",
      "Iteration: 9854 Loss: 4.600227685641346e-07\n",
      "Iteration: 9855 Loss: 4.5960116175507147e-07\n",
      "Iteration: 9856 Loss: 4.591799413620103e-07\n",
      "Iteration: 9857 Loss: 4.587591069765899e-07\n",
      "Iteration: 9858 Loss: 4.583386584126993e-07\n",
      "Iteration: 9859 Loss: 4.5791859514986633e-07\n",
      "Iteration: 9860 Loss: 4.574989168889096e-07\n",
      "Iteration: 9861 Loss: 4.5707962333529383e-07\n",
      "Iteration: 9862 Loss: 4.566607140788967e-07\n",
      "Iteration: 9863 Loss: 4.562421887104594e-07\n",
      "Iteration: 9864 Loss: 4.558240469364721e-07\n",
      "Iteration: 9865 Loss: 4.5540628828840943e-07\n",
      "Iteration: 9866 Loss: 4.5498891258920803e-07\n",
      "Iteration: 9867 Loss: 4.545719195436247e-07\n",
      "Iteration: 9868 Loss: 4.5415530862850836e-07\n",
      "Iteration: 9869 Loss: 4.5373907970013386e-07\n",
      "Iteration: 9870 Loss: 4.5332323211376125e-07\n",
      "Iteration: 9871 Loss: 4.529077656645047e-07\n",
      "Iteration: 9872 Loss: 4.5249268015416093e-07\n",
      "Iteration: 9873 Loss: 4.520779748784845e-07\n",
      "Iteration: 9874 Loss: 4.5166364969693067e-07\n",
      "Iteration: 9875 Loss: 4.5124970442795467e-07\n",
      "Iteration: 9876 Loss: 4.5083613844255555e-07\n",
      "Iteration: 9877 Loss: 4.50422951502396e-07\n",
      "Iteration: 9878 Loss: 4.5001014320857136e-07\n",
      "Iteration: 9879 Loss: 4.4959771326626096e-07\n",
      "Iteration: 9880 Loss: 4.4918566124147236e-07\n",
      "Iteration: 9881 Loss: 4.4877398687473947e-07\n",
      "Iteration: 9882 Loss: 4.483626900269533e-07\n",
      "Iteration: 9883 Loss: 4.4795177009175077e-07\n",
      "Iteration: 9884 Loss: 4.475412267209031e-07\n",
      "Iteration: 9885 Loss: 4.4713105979858005e-07\n",
      "Iteration: 9886 Loss: 4.467212686957107e-07\n",
      "Iteration: 9887 Loss: 4.463118532359231e-07\n",
      "Iteration: 9888 Loss: 4.4590281287359217e-07\n",
      "Iteration: 9889 Loss: 4.4549414750191293e-07\n",
      "Iteration: 9890 Loss: 4.4508585668650535e-07\n",
      "Iteration: 9891 Loss: 4.4467794008389245e-07\n",
      "Iteration: 9892 Loss: 4.442703973518451e-07\n",
      "Iteration: 9893 Loss: 4.438632282023185e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9894 Loss: 4.434564321836836e-07\n",
      "Iteration: 9895 Loss: 4.430500090045273e-07\n",
      "Iteration: 9896 Loss: 4.4264395827216827e-07\n",
      "Iteration: 9897 Loss: 4.422382797561753e-07\n",
      "Iteration: 9898 Loss: 4.418329730567447e-07\n",
      "Iteration: 9899 Loss: 4.414280378380046e-07\n",
      "Iteration: 9900 Loss: 4.4102347375595e-07\n",
      "Iteration: 9901 Loss: 4.4061928047013433e-07\n",
      "Iteration: 9902 Loss: 4.4021545764340177e-07\n",
      "Iteration: 9903 Loss: 4.3981200493173167e-07\n",
      "Iteration: 9904 Loss: 4.394089219440441e-07\n",
      "Iteration: 9905 Loss: 4.390062085096715e-07\n",
      "Iteration: 9906 Loss: 4.38603864177569e-07\n",
      "Iteration: 9907 Loss: 4.382018885488084e-07\n",
      "Iteration: 9908 Loss: 4.378002813457038e-07\n",
      "Iteration: 9909 Loss: 4.3739904217368003e-07\n",
      "Iteration: 9910 Loss: 4.369981708089525e-07\n",
      "Iteration: 9911 Loss: 4.3659766685589406e-07\n",
      "Iteration: 9912 Loss: 4.361975298652503e-07\n",
      "Iteration: 9913 Loss: 4.357977597272932e-07\n",
      "Iteration: 9914 Loss: 4.353983560495635e-07\n",
      "Iteration: 9915 Loss: 4.349993183850773e-07\n",
      "Iteration: 9916 Loss: 4.3460064645136067e-07\n",
      "Iteration: 9917 Loss: 4.342023399691773e-07\n",
      "Iteration: 9918 Loss: 4.338043984947201e-07\n",
      "Iteration: 9919 Loss: 4.3340682174683596e-07\n",
      "Iteration: 9920 Loss: 4.3300960939325034e-07\n",
      "Iteration: 9921 Loss: 4.326127610979688e-07\n",
      "Iteration: 9922 Loss: 4.322162765278364e-07\n",
      "Iteration: 9923 Loss: 4.318201553503367e-07\n",
      "Iteration: 9924 Loss: 4.314243972302923e-07\n",
      "Iteration: 9925 Loss: 4.3102900183837155e-07\n",
      "Iteration: 9926 Loss: 4.3063396884100253e-07\n",
      "Iteration: 9927 Loss: 4.302392979033e-07\n",
      "Iteration: 9928 Loss: 4.298449886961933e-07\n",
      "Iteration: 9929 Loss: 4.29451040886989e-07\n",
      "Iteration: 9930 Loss: 4.2905745420345163e-07\n",
      "Iteration: 9931 Loss: 4.286642281405528e-07\n",
      "Iteration: 9932 Loss: 4.282713625402505e-07\n",
      "Iteration: 9933 Loss: 4.2787885690274127e-07\n",
      "Iteration: 9934 Loss: 4.274867111796226e-07\n",
      "Iteration: 9935 Loss: 4.2709492481579576e-07\n",
      "Iteration: 9936 Loss: 4.2670349753728534e-07\n",
      "Iteration: 9937 Loss: 4.2631242907181514e-07\n",
      "Iteration: 9938 Loss: 4.259217189795165e-07\n",
      "Iteration: 9939 Loss: 4.255313669863041e-07\n",
      "Iteration: 9940 Loss: 4.251413727638658e-07\n",
      "Iteration: 9941 Loss: 4.247517359847878e-07\n",
      "Iteration: 9942 Loss: 4.2436245632234965e-07\n",
      "Iteration: 9943 Loss: 4.239735334485029e-07\n",
      "Iteration: 9944 Loss: 4.2358496703589557e-07\n",
      "Iteration: 9945 Loss: 4.2319675675789116e-07\n",
      "Iteration: 9946 Loss: 4.2280890234580485e-07\n",
      "Iteration: 9947 Loss: 4.224214033584565e-07\n",
      "Iteration: 9948 Loss: 4.220342594721501e-07\n",
      "Iteration: 9949 Loss: 4.2164747047091517e-07\n",
      "Iteration: 9950 Loss: 4.212610359764618e-07\n",
      "Iteration: 9951 Loss: 4.2087495572201174e-07\n",
      "Iteration: 9952 Loss: 4.204892292657152e-07\n",
      "Iteration: 9953 Loss: 4.2010385633991064e-07\n",
      "Iteration: 9954 Loss: 4.197188365658243e-07\n",
      "Iteration: 9955 Loss: 4.193341696776663e-07\n",
      "Iteration: 9956 Loss: 4.1894985546090576e-07\n",
      "Iteration: 9957 Loss: 4.18565893371068e-07\n",
      "Iteration: 9958 Loss: 4.1818228319450695e-07\n",
      "Iteration: 9959 Loss: 4.177990246676522e-07\n",
      "Iteration: 9960 Loss: 4.174161174092409e-07\n",
      "Iteration: 9961 Loss: 4.17033561100846e-07\n",
      "Iteration: 9962 Loss: 4.166513554173755e-07\n",
      "Iteration: 9963 Loss: 4.1626950003969206e-07\n",
      "Iteration: 9964 Loss: 4.158879945880543e-07\n",
      "Iteration: 9965 Loss: 4.155068388015362e-07\n",
      "Iteration: 9966 Loss: 4.151260324118298e-07\n",
      "Iteration: 9967 Loss: 4.1474557504698854e-07\n",
      "Iteration: 9968 Loss: 4.143654665319156e-07\n",
      "Iteration: 9969 Loss: 4.1398570625159007e-07\n",
      "Iteration: 9970 Loss: 4.136062940352744e-07\n",
      "Iteration: 9971 Loss: 4.132272296206671e-07\n",
      "Iteration: 9972 Loss: 4.1284851251938975e-07\n",
      "Iteration: 9973 Loss: 4.1247014243320523e-07\n",
      "Iteration: 9974 Loss: 4.120921192840392e-07\n",
      "Iteration: 9975 Loss: 4.11714442607115e-07\n",
      "Iteration: 9976 Loss: 4.113371120841928e-07\n",
      "Iteration: 9977 Loss: 4.109601273983566e-07\n",
      "Iteration: 9978 Loss: 4.1058348828843787e-07\n",
      "Iteration: 9979 Loss: 4.102071943242996e-07\n",
      "Iteration: 9980 Loss: 4.0983124530543884e-07\n",
      "Iteration: 9981 Loss: 4.094556407992419e-07\n",
      "Iteration: 9982 Loss: 4.090803804910933e-07\n",
      "Iteration: 9983 Loss: 4.0870546415156513e-07\n",
      "Iteration: 9984 Loss: 4.083308914642462e-07\n",
      "Iteration: 9985 Loss: 4.079566621415709e-07\n",
      "Iteration: 9986 Loss: 4.075827758140316e-07\n",
      "Iteration: 9987 Loss: 4.072092321938666e-07\n",
      "Iteration: 9988 Loss: 4.068360308550141e-07\n",
      "Iteration: 9989 Loss: 4.064631714559417e-07\n",
      "Iteration: 9990 Loss: 4.06090653908858e-07\n",
      "Iteration: 9991 Loss: 4.057184777859923e-07\n",
      "Iteration: 9992 Loss: 4.0534664277662996e-07\n",
      "Iteration: 9993 Loss: 4.049751485667958e-07\n",
      "Iteration: 9994 Loss: 4.0460399484647035e-07\n",
      "Iteration: 9995 Loss: 4.0423318129818727e-07\n",
      "Iteration: 9996 Loss: 4.038627075601194e-07\n",
      "Iteration: 9997 Loss: 4.0349257342893885e-07\n",
      "Iteration: 9998 Loss: 4.031227785390458e-07\n",
      "Iteration: 9999 Loss: 4.0275332258128193e-07\n",
      "Iteration: 10000 Loss: 4.0238420518367104e-07\n",
      "r: 1\n"
     ]
    }
   ],
   "source": [
    "adm = time.time()\n",
    "ADMM_list, dual_ADMM_list, iterations_ADMM = admm.ADMM(N, M, (Q1,B1), (Q2,B2), (Q3,B3), Sigma, D, e1, e2, e31, e32, (x1,x2,x3), 1)\n",
    "fin = time.time()\n",
    "\n",
    "Times[\"ADMM\"] = fin - adm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f4b857f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 10000\n",
      "Primal: (x1,x2,x3),(y1,y2,y3),(z1,z2,z3)\n",
      " ((array([[1188.2, 1188.2, 1188.2, 1188.2, 1188.2, 1188.2],\n",
      "       [44.8, 44.8, 44.8, 44.8, 44.8, 44.8],\n",
      "       [25.7, 25.7, 25.7, 25.7, 25.7, 25.7]]), array([[186.6, 703.9, 833.3, 1188.2, 714.3, 11.4],\n",
      "       [8.9, 30.9, 44.8, 42.9, 43.5, 11.4],\n",
      "       [4.6, 15.1, 25.6, 19.0, 25.7, 11.4]]), array([[-0.0, -0.0, 96.2, -0.0, 716.5, 3965.9]])), (array([[1188.2, 1188.2, 1188.2, 1188.2, 1188.2, 1188.2],\n",
      "       [44.8, 44.8, 44.8, 44.8, 44.8, 44.8],\n",
      "       [25.7, 25.7, 25.7, 25.7, 25.7, 25.7]]), array([[186.6, 704.0, 833.3, 1188.2, 714.3, 11.4],\n",
      "       [8.9, 30.9, 44.8, 42.9, 43.5, 11.4],\n",
      "       [4.6, 15.1, 25.6, 19.0, 25.7, 11.4]]), array([[0.0, 0.0, 96.2, 0.0, 716.5, 3965.9]])), (array([[-5.0, -5.0, -5.0, 34.9, -5.0, -5.0],\n",
      "       [-50.0, -50.0, 95.4, -50.0, -50.0, -50.0],\n",
      "       [-95.0, -95.0, -95.0, -95.0, 388.7, -95.0]]), array([[-1865.7, -6335.4, -10000.0, -8357.0, -10000.0, -10000.0],\n",
      "       [-1865.7, -6335.4, -10000.0, -8357.0, -10000.0, -10000.0],\n",
      "       [-1865.7, -6335.4, -10000.0, -8357.0, -10000.0, -10000.0]]), array([[-10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0]])), 'infactible')\n",
      "Dual: (Equilibrium)\n",
      " [[1865.7 6335.4 10000.0 8357.0 10000.0 10000.0]]\n"
     ]
    }
   ],
   "source": [
    "iter_ = -1\n",
    "print(\"Iterations:\",iterations_ADMM)\n",
    "print(\"Primal: (x1,x2,x3),(y1,y2,y3),(z1,z2,z3)\\n\",ADMM_list[iter_])\n",
    "print(\"Dual: (Equilibrium)\\n\",dual_ADMM_list[iter_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1d7890b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_iters_list = max([iterations_DY,iterations_BA,iterations_ADMM])\n",
    "max_iters_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8240e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_adjusted(x_sol, x_teo, sigma):\n",
    "    x_sol_1, x_sol_2, x_sol_3 = x_sol\n",
    "    x_teo_1, x_teo_2, x_teo_3 = x_teo\n",
    "    sigma=sigma[0]\n",
    "    \n",
    "    return LA.norm(x_sol_1-x_teo_1) + sum([sigma[xi]*LA.norm(x_sol_2[:,xi]-x_teo_2[:,xi]) for xi in range(M)]) + sum([sigma[xi]*LA.norm(x_sol_3[:,xi]-x_teo_3[:,xi]) for xi in range(M)])\n",
    "\n",
    "\n",
    "def norm_adjusted_N(x_sol, x_teo, sigma):\n",
    "    x_sol_1, x_sol_2, x_sol_3 = x_sol\n",
    "    x_teo_1, x_teo_2, x_teo_3 = x_teo\n",
    "    sigma=sigma[0]\n",
    "    \n",
    "    return sum([sigma[xi]*LA.norm(x_sol_1[:,xi][:,np.newaxis]-x_teo_1) for xi in range(M)]) + sum([sigma[xi]*LA.norm(x_sol_2[:,xi]-x_teo_2[:,xi]) for xi in range(M)]) + sum([sigma[xi]*LA.norm(x_sol_3[:,xi]-x_teo_3[:,xi]) for xi in range(M)])\n",
    "\n",
    "\n",
    "\n",
    "def generate_list(lista, lista_2, algoritmo, solution, objective_function, Demanda, max_iterations, P):\n",
    "\n",
    "    # unpack solution\n",
    "    x1, x2, x3 = solution\n",
    "\n",
    "    # create a list with index of graphics\n",
    "    iterations = list(range(max_iterations))\n",
    "\n",
    "    # create list to return\n",
    "    x_solution     = []\n",
    "    Fx_solution    = []\n",
    "    Non_anti_sol   = []\n",
    "    equili_solut   = []\n",
    "    capacity_solut = []\n",
    "    demand_solu    = []\n",
    "    dual_solut     = []\n",
    "    \n",
    "    zero_1 = np.zeros((N,1))\n",
    "    zero_1_N = np.zeros((N,M))\n",
    "    zero_2 = np.zeros((N,M))\n",
    "    zero_3 = np.zeros((1,M))\n",
    "    zeroo = (zero_1, zero_2, zero_3)\n",
    "    zeroo_N = (zero_1_N, zero_2, zero_3)\n",
    "    \n",
    "    # create arrays for each graph\n",
    "\n",
    "    if algoritmo == \"DY\":\n",
    "        for x_algo, x_fact in lista:\n",
    "            x1_algo, x2_algo, x3_algo = x_algo\n",
    "            x_solution.append( norm_adjusted(x_algo, (x1,x2,x3), P)/norm_adjusted((x1,x2,x3), zeroo, P) )\n",
    "            Fx_solution.append( abs(objective_function(x1_algo, x2_algo, x3_algo)[0][0] - objective_function(x1, x2, x3)[0][0] )/abs(objective_function(x1, x2, x3)[0][0]) )\n",
    "            Non_anti_sol.append( LA.norm(x1_algo - np.roll(x1_algo, 1, axis=1)) ) #No aplica a DY\n",
    "            equili_solut.append( norm_adjusted((zero_1,zero_2,x2_algo.sum(axis=0) - (D - x3_algo)), zeroo, P) )\n",
    "            capacity_solut.append( norm_adjusted((zero_1,x1_algo - x2_algo,zero_3), zeroo, P) )\n",
    "            demand_solu.append(norm_adjusted((zero_1,zero_2,D - x3_algo), zeroo, P))\n",
    "\n",
    "        for lambda1, lambda2 in lista_2:\n",
    "            dual_solut.append(norm_adjusted((zero_1,zero_2,rho - lambda2),zeroo,P)/norm_adjusted((zero_1,zero_2,rho),zeroo,P))\n",
    "    \n",
    "\n",
    "    elif algoritmo == \"ADMM\":\n",
    "        for elemento in lista:\n",
    "            x1_algo, x2_algo, x3_algo = elemento[0]\n",
    "            x_solution.append( norm_adjusted_N((x1_algo, x2_algo, x3_algo), (x1,x2,x3), P)/norm_adjusted((x1,x2,x3), zeroo, P) )\n",
    "            Fx_solution.append( abs(objective_function(x1_algo, x2_algo, x3_algo)[0][0] - objective_function(x1, x2, x3)[0][0] )/abs(objective_function(x1, x2, x3)[0][0]) )\n",
    "            Non_anti_sol.append( norm_adjusted_N((x1_algo - np.roll(x1_algo, 1, axis=1),zero_2, zero_3), zeroo, P) ) #No aplica a DY\n",
    "            equili_solut.append( norm_adjusted_N((zero_1_N,zero_2,x2_algo.sum(axis=0) - (D - x3_algo)), zeroo, P) )\n",
    "            capacity_solut.append( norm_adjusted_N((zero_1_N,x1_algo - x2_algo,zero_3), zeroo, P) )\n",
    "            demand_solu.append(norm_adjusted_N((zero_1_N,zero_2,D - x3_algo), zeroo, P))\n",
    "\n",
    "        for lambda2 in lista_2:\n",
    "            dual_solut.append(norm_adjusted_N((zero_1_N,zero_2,rho - lambda2),zeroo,P)/norm_adjusted((zero_1,zero_2,rho),zeroo,P))\n",
    "\n",
    "\n",
    "    elif algoritmo == \"BA\":\n",
    "        for x_algo, x_fact in lista:\n",
    "            x1_algo, x2_algo, x3_algo = x_algo\n",
    "            x_solution.append( norm_adjusted_N((x1_algo, x2_algo, x3_algo), (x1,x2,x3), P)/norm_adjusted((x1,x2,x3), zeroo, P) )\n",
    "            Fx_solution.append( abs(objective_function(x1_algo, x2_algo, x3_algo)[0][0] - objective_function(x1, x2, x3)[0][0] )/abs(objective_function(x1, x2, x3)[0][0]) )\n",
    "            Non_anti_sol.append( norm_adjusted_N((x1_algo - np.roll(x1_algo, 1, axis=1),zero_2, zero_3), zeroo, P) ) #No aplica a DY\n",
    "            equili_solut.append( norm_adjusted_N((zero_1_N,zero_2,x2_algo.sum(axis=0) - (D - x3_algo)), zeroo, P) )\n",
    "            capacity_solut.append( norm_adjusted_N((zero_1_N,x1_algo - x2_algo,zero_3), zeroo, P) )\n",
    "            demand_solu.append(norm_adjusted_N((zero_1_N,zero_2,D - x3_algo), zeroo, P))\n",
    "            \n",
    "        \n",
    "        for lambda1, lambda2 in lista_2:\n",
    "            dual_solut.append(norm_adjusted_N((zero_1_N,zero_2,rho - lambda2),zeroo,P)/norm_adjusted((zero_1,zero_2,rho),zeroo,P))\n",
    "            \n",
    "    \n",
    "    print(\"Completando listas\")\n",
    "    \n",
    "    x_solution     = x_solution     + [None]*(max_iterations-len(lista))\n",
    "    Fx_solution    = Fx_solution    + [None]*(max_iterations-len(lista))\n",
    "    Non_anti_sol   = Non_anti_sol   + [None]*(max_iterations-len(lista))\n",
    "    equili_solut   = equili_solut   + [None]*(max_iterations-len(lista))\n",
    "    capacity_solut = capacity_solut + [None]*(max_iterations-len(lista))\n",
    "    demand_solu    = demand_solu    + [None]*(max_iterations-len(lista))\n",
    "    dual_solut     = dual_solut     + [None]*(max_iterations-len(lista))\n",
    "\n",
    "    k = 0\n",
    "\n",
    "    return iterations[k:], x_solution[k:], Fx_solution[k:], Non_anti_sol[k:], equili_solut[k:], capacity_solut[k:], demand_solu[k:], dual_solut[k:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9094918c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DY\n",
      "Completando listas\n",
      "ADMM\n",
      "Completando listas\n",
      "BA\n",
      "Completando listas\n"
     ]
    }
   ],
   "source": [
    "print(\"DY\")\n",
    "iter_DY, x_DY_sol, Fx_DY_sol, Non_anti_DY, equili_DY_solu, capacity_DY_solu, demand_DY_sol, dual_DY_sol = generate_list(DY_list, Dual_DY_list, \"DY\", (x1, x2, x3), objective_function, D, max_iters_list, Sigma)\n",
    "print(\"ADMM\")\n",
    "iter_ADMM, x_ADMM_sol, Fx_ADMM_sol, Non_anti_ADMM, equili_ADMM_solu, capacity_ADMM_solu, demand_ADMM_sol, dual_ADMM_sol  = generate_list(ADMM_list, dual_ADMM_list, \"ADMM\", (x1, x2, x3), objective_function, D, max_iters_list, Sigma)\n",
    "print(\"BA\")\n",
    "iter_BA, x_BA_sol, Fx_BA_sol, Non_anti_BA, equili_BA_solu, capacity_BA_solu, demand_BA_sol, BA_dual_sol = generate_list(x_BA_list, dual_BA_list, \"BA\", (x1, x2, x3), objective_function, D, max_iters_list, Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba168274",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1UAAAE2CAYAAACJALgbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABJAUlEQVR4nO3de3BjeX3n/Y/ku7vbPpb7NsP0MH0MgTBhwqhlAmEhkJYZLksIYHcnSwIkwdJDamtrM1u0cD3PU2Qq2TU2qWxVtpbF6iEhSzbQljIUl0DA6tmBQFhwWzwMMFwyOj3Qw3RPd1uW1W637+f5Q63TPrZ8ld06br9fVSpL53vO0U9HP0vnq9/l+GzbtgUAAAAA2BR/pQsAAAAAADsZSRUAAAAAlIGkCgAAAADKQFIFAAAAAGUgqQIAAACAMpBUAQAAAEAZSKoAAAAAoAwkVQAAAABQBpIqAAAAAChDdaULAAA7kWVZGhgYUH9/v0zTVDQalSSNjo5Kktra2hSJRNa1r1QqpUQiIUnq6OhQZ2fn9hTag6LRqAYHB5VIJBQOhze8/Va+D15T7rFZqr+/37k/OjqqaDSqZDKpU6dOlb3vjdiq+p5OpxWLxZTNZjUyMrKVRQSADfPZtm1XuhAAsFN1dHTINE0NDAy4lkejUWWzWefkUZJisZgsy3ItkySfz6exsTGdO3dOkrbkBHorrVTurXLs2DH19fWV9bo38j7sJFtxbKTCcYhGowoGg86yrq4uSdrQsYnH48uS1I3Wj62s76lUStFoVJlMZtP7AICtQPc/ANgGAwMDyuVyisfjzrKOjg6dPHnStV46nZZpmjIMQ+Fw2HMJlVS63DtFqfdhNxocHHQlVJJ0+vTpDe9naGho2bKN1I+tru+BQKCs7QFgq9D9DwC2SVdXl2KxmPPL/konkIZh3MZSbZwXE72NWPo+7Ea5XE6WZck0TWeZYRhqb29f9z7i8bgsy1q2fKP1w+v1HQA2g6QKALbJiRMnFI1GlU6nJd3qJlXsqpROpzUwMCDLspwxQcXxJcXHxRPhzs5OpVIpxWIxZ+xQsdWgr69vzW2kQsuEZVmyLEujo6POdkVLW3MikYgzbmVxuSU5rT+maWpoaGhZ17LV9Pf3yzCMVVsZSr2WzVr8PhTLuN5jVRyvMzAwoHg8rkAgoDNnzqinp8f1elc7Hht5D9Y6Nps97sFgUB0dHRoYGHAlQYvHU631GoaGhpy6Wtx2pfpR3E8xmTMMQ6FQaEP1ffG+FiuVHKfT6RWPazl1FQDWzQYAbFo4HLYjkciKcUn2wMCAbdu2PTIyYpum6YqXWtbZ2WknEgnXc4yMjNi2bduJRMIOBoP20NCQPTIyYp86dWrNbYaGhmzTNO2hoSEnbpqmE7dt2+7r63P2VXye4v5KlfHUqVN2JpNx7W9sbGzF47B4u+LxsG3bHhsbsyW5yrbaa1nJRt6HzRyrpccmGAwue12rHY/1vAfrOTabPe6ZTMY2TdOWZEuyw+Gwa7/rfQ1LX7dtL68fiUTC9Toymcyq/wOrvR+r1cvi/gzDWPO4buaYAcBGMKYKADzEsiwlk0nXL/VdXV3OBAyGYSidTiscDisYDKqvr2/NbQKBgCzLcrVQFFsFpMIv+bFYTD09PU78zJkzJbt6LS5nKpVy7W/x41JyuZz6+/tdLQ2GYbhaDdZ6LeXa7LFaLBgMLjs2ax2P9bwHax2b9TzPSkzTVCaT0dDQkE6dOqVsNquOjg4lk8my911KIpFQLpdz9hMKhUqut9r7sd56mcvlVjyuW/26AGAldP8DgG2y+KRyvVKplAzDcJ30ZTIZ10ni0v1tZhvDMJTNZiVJ586dk2EYrrEua83kVowXu3dls1lnf2u9tvWss9pr2ajF78Nmj1VbW9uqz7Ge47Hae7CeY7Pe51nN4skhYrGYuru7nYSm3H0XdXZ2amBgQC0tLQoGgzp58uSK07av9n6st16udly38nUBwGpIqgBgmxSnjF7pV/pScrmcTNN0/fK+dCKApSff69lmrefcqHQ6rd7eXnV0dOjEiRPrThzXmq2t3NdSyuL3oTj73FbuX9r88VhsPTPZbeZ5crmcUqnUsnFpfX196u/vVy6Xc1pAN7LvpRNfLDY0NKR0Oq1UKuW0ApZKrFZ7vxe3opVjK94bAFgL3f8AYJsMDAyor69vQ7OdlepaJq2e+Gxmm6Xbl1p3pe1zuZyOHz+unp4eRSIRGYbhrLtai9JK5VzPOptJ/IoWvw/bsf/NHo/F1nNsynme4eHhksuL05tvZt/FCViWKk4sEQwGderUKY2MjOjMmTMl113t/dhovSxlK94bAFgPkioA2AbFFoCVuj2tJBwOKxQKLfuVfnBwcEu3WXxiWpxtrTirWzG+0vaWZTknvUXF7lQrnWgXnycSibhmc8vlckqn0055NvNaVrP0fSj3WJWy2eOx9D1Y69hs9nmkQqKzdBzR4tar9ex78Vgly7JWnEGv1HXBVmodWu392Gi9XLxOUTnHDAA2wmfbtl3pQgDATmNZlgYGBpypoKPRqCRpdHRUuVxObW1troSq2AUpmUyqr6/PmY66uOzUqVM6efKkc/IXi8XU1tbmdAkrTvnd19enc+fOqaenR52dna6T1VLblHre/v5+9fb2yjRNZz/F7VtbW2WaprLZrDOl+tLti+tKhQu/SoWT5lgsppMnT645/XnxmBVb8Ir76uvrc433WfpatuJ92OyxCoVCisViCgQCrvesp6dHhmGsejxM01z3e7DWsdnMcS8mIqZpLkskFh+b9ey7uE5bW9uK9aOYUBWPrWVZikQisixrQ/V9cbmW1kup9P9UqeNaTl0FgPUiqQIAAACAMtD9DwAAAADKQFIFAAAAAGXw1JTqxYv8FfuSFy8KWBwcW5y5p5wYAAAAAGwlz4ypSqfTOnbsmMbGxpwE6NixYxoZGZFUSJRisZhzEb/NxgAAAABgK3mm+9/SiwguvX6EaZrOdLCbjQEAAADAVvNE979kMuma9lQqXD9j6dXlA4GA0um0zp07t6nY0mtqTE9Pa3p62nm8sLCgbDar1tZW+Xy+rXp5AAAAAHYY27Z17do13X333fL7V2+LqnhSlcvlSo53Wulii9lsdtOxpXp7e/XII4+ss6QAAAAAdpsLFy7onnvuWXWdiidVg4ODzoX81mO1K9tvNNbT06OHH37YeTw+Pq57771XFy5cUFNT07rLtB2++13p9a+X7r5b+tGPKloUAAAAYNfJ5/M6cuSI9u3bt+a6FU2qUqmUTpw4UTJmGMay1qVsNivDMDYdW6qurk51dXXLljc1NVU8qSq+dz6fVOGiAAAAALvWeoYFVXyiisHBQcXjccXjcVmWpd7eXqXTaYXD4ZLrh0KhTcd2kuJ75425GQEAAACspKItVUsToGg0qmg06poFsMiyLIVCIac1ajOxnYR5MgAAAICdoeJjqqTCeKd4PC5J6uvrUzQaVTAYVCKRUCwWU3t7u4aHh13XmtpsbKehpQoAAADwNs9c/NcL8vm8mpubNT4+XvExVd/7nvSKV0iHD0sXL1a0KAAAALgN5ufnNTs7W+li7Bo1NTWqqqpaMb6R3MATLVVYjjFVAAAAu4Nt27p06dKqM1ljexiGocOHD5d9jVqSKo9iTBUAAMDuUEyoDh48qMbGxrJP8LE227Y1OTmpy5cvS5LuuuuusvZHUuVxtFQBAADcuebn552EqrW1tdLF2VUaGhokSZcvX9bBgwdX7Qq4lopPqY7S6P4HAABw5yuOoWpsbKxwSXan4nEvdywbSZVHkVQBAADsHnT5q4ytOu4kVR5FUgUAAADsDIyp8ih+rAAAAIAXpVIpRaNRRaNRGYahgYEBSVI0GlUmk1EymVQikVAwGHS26e/vl2EYCgQCsixLpmmqs7PTiafTaQ0MDCgej+vUqVNqa2tTJpORZVmKRqMKh8OSJMuylEwmZRiGJMk0TVmWpUgkcvsOQAkkVR5HSxUAAAC8JJfLaWhoSKZpSpKGhoYUCAScxObkyZOyLMtJqo4dO6bTp0+7kqxYLKbh4WH19fVJkoLBoPr6+hSPx9XT0+MkTblcTi0tLRoZGVEwGFRXV5dGRkac/fT392t0dPR2vOxVkVR5FN3/AAAAdh/btjU5O1mR526sWd907tls1kmoSgkGgzp37pykQvJkmqYroZKkvr4+tbS06OTJk8tiixmGIdM0debMGSfRWuzUqVPq7+9fs8zbjaTKo0iqAAAAdp/J2Unt7d1bkeee6JnQnto9a6534sSJda/T39/vdA9cKhwOq7e3V4lEYtV9ZbNZtbW1OV394vG4q7tfpbv+SUxU4VmMqQIAAIAXlWoxKrWOZVmSpFAoVHId0zSVTqdX3Ecul1MsFlM4HHYSp9OnTysajcrn86mjo0OpVGpd5dlutFR5HC1VAAAAu0djTaMmeiYq9tzbIZvNbmj9eDzudC+MRqOuroadnZ3KZDJKpVIaGhpSR0eHEomEa9KLSiCp8ii6/wEAAOw+Pp9vXV3wdoJiMlRssVoqnU6XHE8ViURKtj7lcjlnjFUkElEkElE8Hldvb2/Fkyq6/3kUSRUAAAB2ulOnTq04ZurcuXOKRqPr3pdlWcu6C544cUK5XK6cIm4JkiqPIqkCAADATtfX16dsNqtUKuVaHo1GdeLECef6U4ut1l0wFou5HqdSqYq3Ukl0//MsJqoAAACAl6VSKVfrUTweVygUWtalb2RkRLFYTJZlORf/7ejoWHbx3zNnzkgqJGLRaLRk18Curi7nQsKSlMlknGtdVZLPtmkLKcrn82pubtb4+LiampoqWhbLktrapD17pInKjFUEAADANpuamtL58+d19OhR1dfXV7o4u85qx38juQHd/zyK7n8AAADAzlDx7n/F/pW5XE7Dw8OuqyoXmxKDwaAsy1Iul3NilmUpmUw6FwFbPEvIarGdgqQKAAAA2BkqnlR1dXXp7NmzCofDymaz6urqUiaTkSQNDAwoHo9LKlxxefHMIV1dXRoZGZFUSKK6u7ud+GqxnYIxVQAAAMDOUPGkKpFIuAahLW5ROnbsmMbGxpYtXzrXvWmaTovXarGdiJYqAAAAwNsqnlQtnkYxkUgsm6u+VLe9VCqlQCDgWhYIBJROp3Xu3LkVY0tnEJmentb09LTzOJ/Pb/ZlbDm6/wEAAAA7Q8WTKunWFIodHR2KRCLO8lwup2QyKUkaHh5WNBqVaZorXuArm82uGluqt7dXjzzySNnl3w4kVQAAAMDO4ImkKhgMyjRNxWIxJZNJZ876xRNMmKapjo4OZ7xVKatdTblUrKenRw8//LDzOJ/P68iRI5t6DVuNpAoAAADYGTwzpbphGOrq6lJXV5eTAC0eH1Wcyc+yLBmGsazlKZvNyjCMVWNL1dXVqampyXXzCiaqAAAAAHaGiiZVqVRKLS0tzmPTNCXJuTLz8ePHl20TCARc47AWC4VCq8Z2IlqqAAAA4GXpdFqxWKzk8mg0Kp/Pp1gspng8rv7+fkWjUWeIT6l1i7N/L9XV1aWWlhb19/dvepvtUtHuf0sTpHQ6LcMwFAwGlcvl1NfX58RSqZQ6Ozud1qjFLMtSKBRaM7aT0P0PAAAAO8HAwIAGBwdd5+5SYYhPX1+f4vG4enp6XOfjXV1dsixLp06dcq2bzWY1MDDgmmdBKgzlCQQCCoVCZW2zXSqaVAWDQZ08edLJLIeGhpzrSxmGoVAopP7+fhmGoUwm47rWVCKRUCwWU3t7u4aHh9cd2ylIqgAAALATGIahXC6nVCq1Yq+xpU6fPq2WlhbXHAqSdPLkSXV3d8uyLKcXmySdO3dOx44dW3b5pM1us9UqPlFFcVIKScuyy2AwuGwa9CLTNJ1sePE+1ortFIypAgAA2H1sW5qcrMxzNzZu/Bw0lUrp5MmTSqfTSiQS606qir3T4vG4qxXJMAydOHFCyWRy3a1Lm9lmq3lmogqURksVAADA7jE5Ke3dW5nbZpK54rVgo9GoBgcHN7StaZoaHh5etjwajWpgYMD1HGvNj7CZbbYSSZVH0f0PAAAAO0VnZ6fTBXAjSl32qNhTLZ1OS1p5Ju9yt9lKFe/+h9Lo/gcAALD7NDZKExOVe+6NSKVSymQyzvwIpmluqAugZVkrrtvZ2amBgQFX69NaNrPNViGp8iiSKgAAgN3H55P27Kl0KdYnnU67EphAIKDu7u51JzWWZSkajZaMRaNRHTt2TF1dXetO0jazzVah+98OQBdAAAAAeN1GugBGo1FFIhHXbH3Sre6ApmnKNE0NDQ2tua/NbLPVaKnyqMUtVbZNyxUAAAC8IZVKOdeHCofDznimeDwuwzAUi8UUjUYVCoWcVqve3l61tbUpl8spk8moo6PDNUt3Op1Wb2+vM/15Z2enotGok3Qlk0klEgmdO3dO8XhckUhkU9tsF59t0w5SlM/n1dzcrPHxcTU1NVW0LKOj0v79hftzc1JVVUWLAwAAgG0wNTWl8+fP6+jRo6qvr690cXad1Y7/RnIDuv951NKWKgAAAADeRFLlUXT3AwAAAHYGkqodgJYqAAAAwLtIqjyK7n8AAADAzkBS5VEkVQAAAMDOQFLlUYypAgAAAHYGkqodgJYqAAAAwLtIqjyK7n8AAADAzkBS5VEkVQAAAMDOUF3pAqA0kioAAAB4UTqd1sDAgOLxuE6dOqW2tjblcjllMhnF43GNjY3Jsqxl62QyGVmWpWg0qnA4rFQqpUQi4azT0dGhcDgsy7KUTCZlGIYkyTRNWZalSCRS2Re+Cp9tV/aUPZVKSZJyuZyGh4d18uRJBYNBSXIO6OIDWTy4m42tJp/Pq7m5WePj42pqatqOl7tuk5PSnj2F+xMTt+4DAADgzjE1NaXz58/r6NGjqq+vr3Rx1s2yLLW1tWlsbMx1nh2PxxUKhRQMBpXL5dTS0uJap7hsZGREwWCw5H6OHTumkZERZ5/9/f0aHR1VX1/flr+O1Y7/RnKDirdUdXV16ezZswqHw8pms+rq6lImk3FixQNqWZa6u7uVSCTKiu1EtFQBAADsErZd+HW9Ehob1z0FdSAQKLn8xIkTOnfu3IrbGYYh0zR15swZBYPBZfuxLGvZNqdOnVJ/f/+6ylUpFU+qEomE0zIlydXatJhpmk6r1mZjOwnd/wAAAHahyUlp797KPHcZ3aPS6bRM03SSptVks1m1tbWVjBV7msXjcVd3Py93/ZM8MFFFOBx27icSCUWjUUmFboFLM9dAIKB0Or3p2E5CUgUAAICd4syZM879lZKqXC6nWCymcDi8apJ0+vRpRaNR+Xw+dXR0KJVKrWsoTyVVvKVKKmS2Z86cUUdHh3OAc7lcyXWz2eymY0tNT09renraeZzP5zdU7u3ExX8BAAB2ocbGQotRpZ57g+LxuKRCg0hPT8+K6xQTrWg0umZLVmdnpzKZjFKplIaGhtTR0aFEIqHOzs4Nl+928URSFQwGZZqmYrGYksnkqgdspaRpM7He3l498sgjGyhpZdBSBQAAsEv4fDtqhrLihHCLh/OstM565HI5pwthJBJRJBJRPB5Xb2+vp5Oqinf/KzIMQ11dXerq6nIO5tLWpWw2K8MwNh1bqqenR+Pj487twoULW/66Nsu/6J1ZWKhcOQAAAIC1hMPhLemiZ1nWsmE7J06cWLXxxAsqmlSlUim1tLQ4j4tNgZZlucZaLRYKhTYdW6qurk5NTU2um1csTqpoqQIAAICXlBpas5l1S8VisZjrcSqV8nQrlVTh7n+BQMCVBKXT6RWbDy3LUigUclqjNhPbSRaPqaKlCgAAAF5RvPivVEiAOjo6liU9xTkTJKmvr0/RaHTZOX7x4r9SYVjOyZMnJRUuj9Tf3++cv2cymW25RtVWqvjFf5PJpJOhDg0Nqa+vz9ViNTAwoPb2dg0PD6unp8c15fpmYqvx0sV/bftWa9Xly9KBAxUtDgAAALbBTr34751iqy7+W/Gkyku8lFRJhaTKtqVLl6RDhypdGgAAAGw1kqrK2qqkyjMTVWC5YksV3f8AAAAA7yKp8jCSKgAAAMD7SKo8jKQKAAAA8D6SKg8jqQIAANgdFjjhq4itOu4VnVIdqytOq87/GAAAwJ2ptrZWfr9fzz33nA4cOKDa2lr5Fl9bB9vCtm3NzMzoypUr8vv9qq2tLWt/JFUeRksVAADAnc3v9+vo0aO6ePGinnvuuUoXZ9dpbGzUvffeK7+/vA58JFUeVnxvmfQeAADgzlVbW6t7771Xc3Nzmp+fr3Rxdo2qqipVV1dvScsgSZWH0VIFAACwO/h8PtXU1KimpqbSRcEmMFGFh5FUAQAAAN5HUuVhJFUAAACA95FUeRhJFQAAAOB9JFUeRlIFAAAAeB9JlYdxnSoAAADA+0iqPIwp1QEAAADvI6nyMLr/AQAAAN5HUuVhJFUAAACA95FUeRhJFQAAAOB9JFUeRlIFAAAAeF91pQuQTqeVSqUkScPDwzp9+rQMw3BikhQMBmVZlnK5nILBoCTJsiwlk0mZpinLshSJRJztVovtJCRVAAAAgPdVPKlKpVI6deqUJKm/v1/Hjx/XyMiIJGlgYEDxeFySFA6HlUgknO26urqc9SzLUnd3txNfLbaTMKU6AAAA4H0V7f6XTqfV29vrPO7s7FQ6nZZlWZKkY8eOaWxsTGNjYxoaGnK1RC1mmqbT2rVabKehpQoAAADwvoomVcFgUKdPn3Ye53I5SVIgEHCWGYaxrOteKpVyrVPcptiVcKXYTsN1qgAAAADvq3j3v87OTuf+mTNnFA6HnSQql8spmUxKKoy3ikajMk3TSb6Wymazq8aWmp6e1vT0tPM4n89v7kVsE1qqAAAAAO+reFJVVEygimOhJLkmmDBNUx0dHcpkMqvuYyOx3t5ePfLII5st8rYjqQIAAAC8zzNTqsdiMde4Kck9Pqo4k59lWTIMY1nLUzabdboKrhRbqqenR+Pj487twoULW/qaykVSBQAAAHifJ5Kq/v5+xWIxp2tfLpdTOp3W8ePHl60bCAQUDodL7icUCq0aW6qurk5NTU2um5eQVAEAAADeV/GkKplMKhgMOgnV4OCgDMOQaZrq6+tz1kulUurs7HRii1mWpVAotGZspyGpAgAAALxv3WOqxsfHFY/H5fP5ZK9zOjqfz6dIJLJiC5BlWerq6nItMwzDGUsVCoXU398vwzCUyWRc15pKJBKKxWJqb2/X8PDwumM7CdepAgAAALzPZ683Q9oF8vm8mpubNT4+7omugK96lfTtb0uf/7z0trdVujQAAADA7rGR3GBDLVVnz57dcGHC4bAnEpSdiO5/AAAAgPetO6lqbm7Wgw8+uOEnIKHaPJIqAAAAwPs2dJ2qo0ePblc5UAJJFQAAAOB9FZ/9DysjqQIAAAC8b0NJ1Xe/+1099thjkqTz588rn89vS6FQQFIFAAAAeN+GkqpsNutc7+no0aMaHBzcjjLhJpIqAAAAwPs2lFSFQiEFAgF997vfVSgUUiaT2a5yQVynCgAAANgJ1jVRxYte9CK1tbWpo6NDpmlqaGhI586d2+6y7XrFliquJAYAAAB417paqoaGhvSVr3xFDz74oL7zne8ok8nooYce0l/8xV9sd/l2Nbr/AQAAAN63rpaq4lTqx48f1/Hjx53l58+f355SQRJJFQAAALATlDWluq846AfbgqQKAAAA8L6ykqpEIqFAIKCTJ0/q0Ucf1TPPPOOKM+V6eUiqAAAAAO8rK6kyTVPnz59XJBLR008/rXA4rNbWVifJisViW1XOXYmkCgAAAPC+srv/NTc36/jx4/rIRz6ip59+Wh/60IecJCuVSm1VOXclplQHAAAAvG9dE1WsJJPJ6NFHH9X73/9+Z1lbW5szoUVbW1vZBdzNii1V8/OVLQcAAACAlZXVUvXBD35QTz/9tFpbW/XQQw/pAx/4gIaHh514d3d32QXczapvprwkVQAAAIB3ldVSJUkf+chHFI1GlU6nJUnvete7yi4UCmpqCn9nZytbDgAAAAArKzupkgrXsSpeywpbp9hSNTdX2XIAAAAAWNmWJFXlSKfTzoQWw8PDOn36tAzDkCRZlqVkMinTNGVZliKRSNmxnYSWKgAAAMD7Kp5UpVIpnTp1SpLU39+v48ePa2RkRJLU1dXl3LcsS93d3UokEmXFdhJaqgAAAADvK2uiiqLHH398U9ul02n19vY6jzs7O5VOp2VZlizLcq1rmqbTorXZ2E5DSxUAAADgfVuSVA0NDW1qu2AwqNOnTzuPc7mcJCkQCCiVSikQCLjWDwQCTnfBzcR2GlqqAAAAAO/bku5/tm1vetvOzk7n/pkzZxQOh2UYhpNgLZXNZjcdW2p6elrT09PO43w+v+5y3w60VAEAAADetyUtVT6fr+x95HI5JZPJNcc+rZQ0bSbW29ur5uZm53bkyJF1lvb2oKUKAAAA8L4tSaq2QiwW09DQkDNLn2EYy1qXstmsDMPYdGypnp4ejY+PO7cLFy5s6WsqFy1VAAAAgPd5Iqnq7+9XLBaTaZrK5XLK5XIKh8Ml1w2FQpuOLVVXV6empibXzUtoqQIAAAC8r+JJVTKZVDAYdBKqwcFBGYYh0zRd61mWpVAoVFZsp6GlCgAAAPC+ik5UYVmWurq6XMsMw1AkEpEkJRIJxWIxtbe3a3h42DXearOxnYSWKgAAAMD7fHY5U/fddPr0aXV3d29FeSoqn8+rublZ4+PjnugK+N/+m/Qf/oN04oR05kylSwMAAADsHhvJDbak+9+dkFB5ES1VAAAAgPdVfEwVVsaYKgAAAMD7SKo8jJYqAAAAwPtIqjyMlioAAADA+0iqPIyWKgAAAMD7NpxUffSjH1UgEFBVVZWqqqr00EMP6bOf/ex2lG3XK7ZUzcxUthwAAAAAVrahpOqjH/2oMpmMzp49q2w2q69+9asKh8P64Ac/qIceekj5fH67yrkrNTQU/k5NVbYcAAAAAFa2oaQqk8no4x//uB588EE1Nzfr+PHj+uAHP6inn35a3d3dTK2+xRobC38nJytbDgAAAAAr21BS1dbWtmKss7NTH/rQh/QXf/EXZRcKBSRVAAAAgPdtKKlqaWlZNf7ggw/q6tWrZRUItxS7/924UdlyAAAAAFjZhpKqkZGRNddpbW3ddGHgRksVAAAA4H0bSqoGBgZUVVWlF7/4xfrABz6gxx57bNnkFGu1ZmH9FidVtl3ZsgAAAAAobUNJVV9fn7LZrD7+8Y+rublZ/+W//BcZhqHW1ladPHlSjz766Lpas7A+xaTKtqXp6cqWBQAAAEBpPtsuvw3k7NmzSqfTGhoa0tmzZzU/P78VZbvt8vm8mpubNT4+rqampkoXR7OzUm1t4f7oqBQIVLY8AAAAwG6xkdxgwxf/LaU4tfpXv/pVfeQjH9mKXUKFi/9WVxfuX79e2bIAAAAAKG1LkqrFOjs7t3qXu1pxiFo2W9lyAAAAAChty5Oqo0ePbvUud7UDBwp/makeAAAA8KYtT6qwtYpJ1ZUrlS0HAAAAgNKq17vi+Pi44vG4fD6f1ju3hc/nUyQSWXVgVzqdVnd397JZA9PptCQpGAzKsizlcjkFg0FJkmVZSiaTMk1TlmUpEonIMIw1YzsRSRUAAADgbetOqpqbm/XBD35wS5+8mPwUE6jFBgYGFI/HJUnhcFiJRMKJdXV1OUmYZVnq7u524qvFdiKSKgAAAMDbNtRSdfbs2Q0/QTgcXrGlarVJLY4dO6axsTFJcrU0WZblWs80TaVSqTVjO9W99xb+ZjKVLQcAAACA0jbUUvXggw9u+AnKud5TqW57qVRKgSUXbAoEAkqn0zp37tyKsWLXwcWmp6c1veiquvl8ftNl3S4ve1nh71NPVbYcAAAAAEpbd1Il3d6Z/XK5nJLJpCRpeHhY0WhUpmkql8uVXD+bza4aK6W3t1ePPPLIVhR329x/f+HvU09Jk5NSY2NlywMAAADAbUNJ1e20eIIJ0zTV0dGhzCp94FZKqFaL9fT06OGHH3Ye5/N5HTlyZDPF3TamWegC+POfS2fPSm97W6VLBAAAAGAxz06pvnh8VHEmP8uyZBjGspanbDYrwzBWjZVSV1enpqYm181rfD7pHe8o3P/YxypbFgAAAADLeTKpSqfTOn78+LLlgUBA4XC45DahUGjV2E72R39U+PvEExUtBgAAAIASPNP9L5fLubr79fX1ObFUKqXOzk6nNWoxy7IUCoXWjO1kL3hB4e/UlDQ7K9XUVLY8AAAAAG6paFKVSqU0NDQkqTBpRHt7u5M8hUIh9ff3yzAMZTIZ17WmEomEYrGY2tvbNTw8vO7YTrVv3637+bzU2lq5sgAAAABw89m2bVe6EF6Rz+fV3Nys8fHxio+v+uZ//r80lz6nln/3h3rgXX+s2tpCK9WFC9I991S0aAAAAMAdbyO5gSfHVEHy/+OX9BuPjWjsXx6XJNXXF5ZPTVWwUAAAAACWIanyKNvnK/y1FyRJDQ2F5SRVAAAAgLeQVHnVzaRKN3tnFluqbtyoUHkAAAAAlERS5VVOTuVOqmipAgAAALyFpMqrii1VCyRVAAAAgJeRVHmV0/2vMKaKpAoAAADwJpIqr1phTBVJFQAAAOAtJFVetSSpKs7+x0QVAAAAgLeQVHnU0inVaakCAAAAvImkyqN8dP8DAAAAdgSSKo+ipQoAAADYGUiqvIop1QEAAIAdgaTKq5ioAgAAANgRSKq86mZOxZgqAAAAwNtIqrzKV3hrGFMFAAAAeBtJldfRUgUAAAB4GkmVV/lvvjUkVQAAAICnkVR51QpjqpioAgAAAPCW6koXIJ1Oq7u7WyMjI67llmUpmUzKNE1ZlqVIJCLDMMqK7Sg+d0tVcfY/WqoAAAAAb6loUlVMftLp9LJYV1eXk2hZlqXu7m4lEomyYjtK8eK/C0xUAQAAAHhZRZOqzs7Okssty3I9Nk1TqVSqrNhO4/M5/f8kkVQBAAAAXlXx7n+lpFIpBQIB17JAIKB0Oq1z585tKhYMBpc9z/T0tKanp53H+Xx+C19FeexiUrVAUgUAAAB4mScnqsjlciWXZ7PZTcdK6e3tVXNzs3M7cuTIJkq7TYpJFRNVAAAAAJ7myaRqJSslTZuN9fT0aHx83LlduHChvAJuId+SpIqJKgAAAABv8mT3P8MwlrUuZbNZGYax6VgpdXV1qqur29Kyb5niRBVcpwoAAADwNE+2VIXD4ZLLQ6HQpmM7zs2kykdSBQAAAHiaZ1qqcrmc06JkmqYrZlmWQqGQ0xq1mdiO47RUuadUz+eluTmp2jPvHAAAALC7VfTUPJVKaWhoSFJh0oj29nZnmvVEIqFYLKb29nYNDw+7rjW12diO4oypKvwpjqmSpD//c+lP//S2lwgAAABACT67OGgHyufzam5u1vj4uJqamipaln858Wr9euL/6KvveoXemPyuFhakqqpb8S99SXrTm27lXgAAAAC2zkZyA0+OqYKWTanu90uZjHT33YXFb3mLdOiQ9KpXSd3d0sWLFSonAAAAsMuRVHmV7+Zbs6gh0TSln/5U+o//UWpslK5ckb79benRR6X/8T8qU0wAAABgtyOp8qolLVVFe/ZI//W/Ss8/L42MSK95TWF5JnObywcAAABAEkmVZy29+O9Se/dKwaD0gQ8UHl+6dJsKBgAAAMCFpMqzVk+qig4dKvx9/vltLg4AAACAkkiqPMrnJ6kCAAAAdgKSKq9ao/tfUTGpGh0tXBQYAAAAwO1FUuVVJWb/K6W1tTDdum0XZgMEAAAAcHtVV7oAKM2uLlzp1z+3sOp6VVXSgQOF7n+PPSbdf78UCEj19ZJhSAcP3obCAgAAALsYSZVHzdXVSJKqZ+fXXPfQoUJS9e///fLYpz8t/c7vbHXpAAAAABSRVHnUQm0hqaqZWXug1NGj0pNPFu7fdVdhbFWxK+Cf/VkhvnfvrWFa2207n+d2vQZgU2xbmp+XFhbksxekhYXCMtsuPL55X7ZdWEe37q+0nuvx4vW0ynYeYtu2bNmy7QUt3Czbgr0g216QLcmW7X5s31xXtmyPvRZJW/YhxGfZcrYHD4pP3ivT5twpr8ODPFhv7wh+n451/ptKl2JDSKpKuHTpkq5fv+48rq+vV0tLi+bm5nSlxMClu+66S5J09epVzc7OumKGYaihoUHXr19XPp93xWpra9Xa2qqFhQU9v2T6vvGGfZr3+1U9O69sNqvp6WlXfN++fdq7d69u3Lih//Sfcqqrk975Tul1r5Oqq6t1/vwB/dqvSdnsRb3jHe7yXrmyX3NzNWpuzqmx8YYrNjGxR9euNam2dlqtrVlXbH7er8uXCzNjHDz4vKqq3F0TR0cDmpmp0759ee3de90Vm5xs0Pi4oerqWR04cNUVs23p0qXCMdy//4pqatyJ5NiYoampBu3ZM6Gmpmuu2NRUncbGAvL753Xo0GUtdenSIdm2X4HAqOrqZlyx8fEmTU7uUUPDDRlGzhWbmanR6Oh+SdJdd11ctt8rVw5obq5ahjGmhoYpV+zatb2amNinurppBQLuYzg3V6UrVwp9Mg8del5+/9Jj2KqZmVo1NeW1Z8/SY9io8fFm1dTMav/+pcfQp0uXDkuSDhy4ourqpcewRVNT9dq7d0L79hWPoS2/5jU3VaOJsb2qrZrWoYOXVaU5VWle1ZpXleZ17dJeVdvzam7Nqa522llepTnNjVdLkz7VNU6rvvmG/FqQXwuq0oI0Y2t+tFpVvjk1HJ6SX/Py33xOvxZkX/apan5B1S2zqqq/FavSgvzX5lU1sSB//bz8LbZr26q5edVdmZFfC5o/7JffV3g+383nbbg6qZrZOc01V8lu9DvP59eC6q9PqzE/qYVan6Zb653n82lB1QtzCjw/VnjNB/bJrlahLDe3b8mOqWF6SlN763RjX+OtsmpejTduqDU3qoXqKl05cGBZfbnrYqEOXW1t1WxtrStm5HJquHFD1xsblW9udsVqp6fVms1qwefT84cPL9vvweefV9XCgrItLZqur3fF9uXz2nv9um7U1yvX0uKKVc/O6sDVQh26ePjwspOC/VeuqGZuTrnmZt1obHTF9kxMqOnaNU3X1irb2uqK+efndehy4f/w+YMHtVBV5YoHRkdVNzOj/L59ur53ryvWMDkpY3xcs9XVurr0GNq27rp5Mb4r+/drrqbGFTbGxtQwNaWJPXt0ranJFaubmlJgbEzzfr8uF2f2WeTQpUvy27ZGAwHN1NW5Yk3j49ozOakbDQ3KGYYrVjMzo/2jo5Kkize/AxY7cOWKqufmNGYYmmpocMX2XrumfRMTmq6rUzYQcMWq5uZ08Ob3zPOHDmnB7x763Do6qtqZGeWbmnR9zx5XrHFyUs3j45qtqdHV/ftdMZ9t63DxGB44oLlq99d/y9iY6qemNLF3r67t2+eK1U9NqWVsTPNVVbpcok/54UuX5LNtjba2amZJ/W4eH1fj5KQmGxs1vrR+z8yodXRUts+nS6Xq9+XLqpqf11hLi6aW1u9r17R3YkJT9fUaW1q/5+Z04OYxvHT48LJkbf/Vq6qZndV4c7Mml9bv69fVlM9rprZWo0vr98KCDt38rr584IDmlxzDQDaruulpXdu7VxNLj+GNG2rJ5TRXXc1nBJ8RkviMKFrpM2JG1Rp9w3fV2toq27Z1qcQFWQ8ePKiqqiqNjY1pasp9LlY8T56amtLY2JgrVl1drQM369ClS5eW/YC3f/9+1dTUaHx8fNn5+WpIqkr4m7/5G9Uv+vB5+ctfrne+853K5/OKx+PL1v/whz8sSfrc5z6nZ5991hV7xzveoQceeEA//OEP9eUvf9kVa2tr0+/93u9pdnZ2+X4bXqTj9fWqmZnXV77yFf30pz91hd/4xjfq1a9+tSzL0uOPJ/Wyl0k//nHhdvjwYf3hH0Z15Ij03vd+QtXV7i6Ef//3H1A2e1BveMPXdf/933XFRkZeo299K6wXvOCi3vGOv3XFJib26ZOffFiS9J73/C/t3etOcD772ffq2Wfv06tf/R2FQt90xX74wwf1+OO/pUBgTO9+t/u1zs9X6WMf+38kSSdOPKaDB93/OF/+cqeefvp+veIV39drX/tVV8yyfklf/OLvqqFhSt3dy9+bj3/8Q5qZqdPb3/5lvfCFGVfsiSferCeffKVe8pJ/1UMPfdYVu3jxBRpM/KFkS9Ho8v1+6a9/W9Nj9Wp/4zd1z8suuLf95t0a/eYBNd83piMnfu6KLYz5ND1Qp1p7RlXvnZca3V9UzX+bVcOzU7r2mr26/mvuD/7Wc1f0wn86r5mDdfpB96+6YtXTs3pL3xdVrTmlut6o/EH3F++bP/1Fvfgn/6r0K47pG+HXuWIv++EP1ZVIKL+nSf81+vCy1/p//9mfqXp+Xp982/v0s/vuc8Xe9vnPK5hOK/3SoL7wW7/lir3wmWf0vk9+UnP+Kv3n6P+7bL9/8pd/qaZ8Xolwl566//5FEZ/ekHpCr/3GN/STF75En/nd33Vtd+DyZUU/VnhPev+gZ9mXXGRgQHddvKh/fM1bdO6Vr3TFXvWtb+l1X/lnXTh4j/76/e93xRqvX9cHP/pRSdJf/e5/0NiSL7J3f+pTujfzrJ4IvV5fe/3rXbGXP/mk3vnYY8o1NSkejS57rR/+0z+VJH3ut39bzx454oq947HH9MCTT+oH9/+K/umtb3HF7nvaUtffJTRVW1dyv3/UH1f95JS++Ka36WcvOeqK/dpXvqkHvvWkLNPU2RMPuWKtF6/onQNJSdIn3t+thWr3ic27/vtnFLgypq//xuv1k+Avu2K/+s9pvfLst/Xc3XfrH9/3dldsT35C/+4vPyVJ+vvf+31db3KfFL31k5/T3c88p++88tf0vdcGXbGXpH+k133+CWVbWvQPUXefZf/cvP7ozwvv+WPv6tToXe4TquODX5H5lKUnH3hA337oNa7YvT95Rg99+su6UV+vv4v+gZZ6b++jqp2e1Zfe8lb94kX3umK//o9f1/3DP9S/vvjFeuKdYVfs4IVL+u1PPCZJJd+bk3/1d2rO5vX4b4b19AO/5IoFnxhW6IlhXbjniL78+29zxZqy4/qdv/pfkqT/+Z73aWqP+2Tr7Y/+gw49+7y+9epf1/df/QpX7GXf+b7+zZf+WVf379dj0ROuWM30jP6g91FJUqLrdzR20F2/3/jpL+m+nzyj774iqOHwq1yxoz98Wh2Jr2pizx79ffS9y17rH/3Zx1U1v6AvvO3tunjfC1yx133+f+ul6R/pxy/9ZX39t97git31zC/0tk9+TvN+vz5R4hi++y//Vnvz1zUUfqOs+1/kir0y9X/04DfSeuaF9+krv+v+v2m5nNWJj31GkvTXf/CHmq1zn8S9c2BQBy5e1T+/5rV66pUvd8Ve/q3/T7/+lX/RpYOH9Ln3v8sVq79+Q+/96N9Ikj79u+9WPuD+nH3Lp76gI5kLOhdq18jr212xFz35Ux1/LKXxpiZ9Jvp7y15r9E8/Jkn67G+/U5ePuJOjNzyW0i89+VP94P5f0Tff6v78vufpn+utf/dFzdTW6G+i3cv2+57+v1bj5A19+U1vWfYZ8eqvfEO/+q3vKWO2aejEm1yx/RevqHNgUJL0aInPiBP//dMKXMnqid94g34cfJkr9uA/j+jXzv4f/eLuu/WF97l/1d2Tn9Dv/2Xh3OJTv/eeZZ8Rb/vkZ/WCZ57Tt1/5Kn33tcdcsZemn9LrP/+/lW0JaDDq/l7wz80r8ucflyQl33VCV5d8RnQM/pPansroew/8qr71kLv144U/Oa83f/pLulFfr7+N/pGW+sPeuGqnZ/XFt7xNzy75jHjtP35NvzL8ff30xb+ks+98oyt26MIlvfMThc/ZUp8R/+6vPqXm7LhSv9mhf33gJa5Y6InvqP2J7+jn99yrf/x993drUzand//V30mSPlniM+IdjyZ1+NlL+uar/42eXPIZcf93ntTrvvR1Xdl/QMnoSVesZnpG7+8tfM5+put3NHbQnRC/6dNf1NGfPKP0K47p2+FXu2LmD5/WQ4l/0sSePfpUic/ZyJ99TFXzC/rc296h55Z8RvzG5x/Xy9JP6amXvkxf+63fdMVe+IUv6H3ve5/m5+dLnn//yZ/8iZqampRKpfTUU0+5Yr/5m7+p1772tfrZz36mz3zmM67YgQMH9Md//MeSCuf8MzPuH90jkYjuuusufeMb39A3vvGNZc+7Ep/tyf4VlZHP59Xc3Kyf/OQn2rcoC69ES9W3Tn9Yb3/kE/rh/ft1zxM/WrWlKpfLuWLFDPzZZ6Xvfe+iHnzQ/SNTMQPP5XK6ccPdUrVnzx41NTVpenpa2ay7lcXv9+vQzV9xnn/+eS0suFtZAoGA6urqlM/nXS19ktTQ0CDDMDQ7O6urV2+1sizYC5qanVJ9S72uzVzTxecv6sb0DU3PTWtmfkbT89Oar5vXrH9WN8avaS6b18Lkddk3bsi+MSnfjQlVTeXkn5yWZutUPT2jqulZVc3OyT8zr4b886qZmdV81R75VK3q2XlVzy2oam5B+8bHtXdiQj5/rWYaDdXOSTULUvWC1Dg1o0NXRlWzIF09tLt+YZqXNO+/efNJLVcuad5v62prq6bqajXvk+Zuxmonx1U9M6kb9Y263tSseZ9k+wox3/yM6q6Nas7v00TgsOb90oLv1q322hXJt6DpvQHN1dVrwecr/KLs98k3Oyn/3KTma+s129gi2+fTQpXv5qyY86qaGZft92t2z37Z/irJ75P8ftl+n/zzE5LP1nzNHi1UN0h+n+ybMWlG0owWqmpkV++V/FWyb8blk1R1Q3aVX7Zvr2yfX76qKtn+Ktl+yVc9J1X7ZKtO8tUUylLlL+yj2pavTpKvSvZcrXx+v3z+Kvmq/JJ88u3zye+vkqb8ku0rxHx++fx++Rv8qqqrlm/WJ01LPp9PPvnk9/nlr/GrZk+NfPJp/tq8/PIX4r5CvL65Xn6/X3PX56Q5uWK1jbWqa6jT/My8Zq/PFvZ3c1bR6ppq7TX2yu/za2J0Qn6fXz7d2nZfyz7V1NToxrUbmpuZcz1vY2Oj9uzbo/nZeU3kJpxt/D6//FV+7T+wX36fX2NXx2Qv2IV9+3zyy6+WQIvq6+t1feK6piYLvyr6Vdi2sbGx8Dk7O6fRm7/uLlb8nL1y5Yrmllw/ovg5OzExoWvX3D/21NXVKRAIaH5+XpcvL2/NPnTokPx+v0ZHR5d9sTY1NWnPnj0lP2dramq0/+b/4cWLy1uzDxw4oOrq6pK/oO7du1f79u0r+TlbVVWlgzd/6S31Odva2qra2tqSn7ONjY1qbm5e9jkrFerG4ZstGaWOYUtL4b0pdQyL34ErHcPDhw/L5/OVPIbNzc1qbGzU5OSkxsfHXbHid2Alf4WenJx0xYrfgTMzM8vq4eLvwMuXL2t+3v2DZfE78Nq1a5qYmHDFKnEeId06hmv1eFnpPEIqXb+9dB5RxGdEAZ8RBVvZUvWSl7xE4+PjalrSyrkUSdUixaRqPQduu325P6I3x07rxy9p1Ut/vPzDw2vmF+Z1+fplXZm8oquTV3V18qquXF90/+by3FRO12au6cZkXvXZvPaOTerQdenQhHR4Qjp0XWqdlIypwq1l6tb9fTNrl+N2WPBJM1XSXJVPs1U+zVb7NFfl01y1f9Ffv+Zq/Jqv8mu+2q/5mirNV9+62dVVmq+pkqoK9+2bf1VVXXhcXS27uloq3q+qkqqrpOLyqkJMNTWyq6vkq66RqqtdN19Nzc2/tc59f3WtE/PV1MhfXYj5amsL96urVVVdI7/Prypflar8Vc59v89f8vFGYz76nwMAgB1gI7kB3f88auFmd4X1zP633RbsBT137Tllshn9fPzn+sW1X+gX+V8U/l77hZ7NP6tLE5e0YBd+LdkzLd2Tl47kpSPjhfsPLLp/14TUemONJ13FfJVfM/XVmqmv1VxDreYa6m7e6rXQUK/5hnrZ9XVSfb3sujr56uul+nr5Ghrkqyv89Tc0yl/foKqGPapq2CN/Y6OqG/aouq5R1Q2Nqq5rUE39Hvnq6qSaGqm2tnCrqZG/qkr1JAYAAAC4iaTKo4pJVe3M7UuqLl+/rB9c/oGeuvKUns4+rcxYRplsRudz5zU1d6tZ1bdQSI7MMelXxqS3Zwv328aktpxPgcn1NX7aVVVaOLBfvkOH5Dt8l3yHDxfmh9+/X2ppKVxoa+mtqUlVtbVqkNSw2s4BAACA24SkyqNm9xQmymicnF1jzY2bW5jTDy//UMPPDevJ55/UDy7/QD+4/ANdmXT3866el15yVXr7Zelloz4Fxxv10lGf7r10Y5Vk72ZCtW+fdORI4XbPPe77d98tHTokX2urqpaMKQIAAAB2Gk8nVel0WpIUDAZlWZZyuZyCwcKMUZZlKZlMyjRNWZalSCQi4+Z0lqvFdoob+w1JkjE+XbgWTRnJx/MTz+vrP/u6vv2Lb+s7v/iORi6OaHJ2yeDcaenXn5c68vv166N79MvPTumun42q2rlOli1p0YDH6mrpvvuktjbJNAt/i/fvu0+q8Jg0AAAA4HbxdFI1MDDgTKEYDoeVSCScWFdXl0ZGRiQVkqju7m4nvlpsp5huNbQgqXrelq5elUrM+7+S7I2svvbM1/T4+cf1+DOP66kr7mkma+ak41cbdSJ7l155qUpHf5ZX08+fl8+2JV29ebtp3z7pgQekX/5l6SUvkV760sLtvvsKiRUAAACwy3n6rPjYsWPONIiLW5osy3KtZ5qmUqnUmrEdpaZaV/YUZsPTc8+tmVRdvHZRn/3xZ5V8Kqmv/exrzqQRklQ3K717sk2/fTmgY/96XYe/b8l/Y1KS+7pNuvtu6cEHpVe84tbfo0fLaiUDAAAA7nSeTqokley2l0qlFFhy/Z9AIKB0Oq1z586tGCt2HSyanp52Xbdh6fUfKqmlvkXP7buZVF28WEhwlvhZ7md67EeP6R9+9A/6lwv/IvvmeKbaOenduSM6ceWgjj09qf3fz8g3k5EriWptlV77WulVr7qVQG2gNQwAAABAgaeTqlwup2SycDXq4eFhRaNRmaa57AJrRdlsdtXYUr29vXrkkUe2qrhb6pUveKW+Y0gPXpImvj+ivW9+syTp6ezT+oen/kHJHyV17rlzzvqHrkn//upRnTi/Ry8aseS/fkHShVs7PHxY+o3fKNxe97pCdz5aoAAAAICyeTqpWjzBhGma6ujoUCaTWXH9lRKqlWI9PT16+OGHncf5fF5HjhzZbHG31AuaXqCLLz4s/fiSvv35j+nTL31G3/j5N/ST0Z9IKkxrHnrepz++dK/e/K+2Dj/1c0nnb+3grrukcPhWEvWiF0lcWwkAAADYcp5OqizLcrrsFWfysyxLhmEsa3nKZrMyDGPV2FJ1dXWqq6vbtvKX62Vv+n3pCx/VL33/oga/9Qkdui699xd+/c6Vw3rtj65rz9VxST+7tUF7u/Rv/6301rcWuvTREgUAAABsO59t2+u7Uuttlk6ndfz4cWeiilwup5aWFo2NjSmbzbpm+JOklpYWnT9/ftXYWtOq5/N5NTc3a3x8XE1emBL8xg1Nv+Cw6sZWGOu1Z4/0xjcWEqm3vKXQxQ8AAABA2TaSG3i2pco0TfX19TmPU6mUOjs7ndaoxSzLUigUWjO24zQ0qO4Tn5Te8x5pYkJqaJBe/nLp9a+Xjh8vdO3zcEsbAAAAsBt4tqVKKrRWpVIpGYahTCbjSrIsy9LAwIDa29s1PDysnp4e18V/V4qtxnMtVUXT09KNG1JzM+OiAAAAgNtgI7mBp5Oq282zSRUAAACA22ojuQEzGQAAAABAGUiqAAAAAKAMJFUAAAAAUAaSKgAAAAAoA0kVAAAAAJSBpAoAAAAAykBSBQAAAABlIKkCAAAAgDKQVAEAAABAGUiqAAAAAKAMJFUAAAAAUAaSKgAAAAAoA0kVAAAAAJSBpAoAAAAAykBSBQAAAABlIKkCAAAAgDKQVAEAAABAGUiqAAAAAKAM1ZUuwHawLEvJZFKmacqyLEUiERmGUeliAQAAALgD3ZFJVVdXl0ZGRiQVEqzu7m4lEokKlwoAAADAneiO6/5nWZbrsWmaSqVSFSoNAAAAgDvdHddSlUqlFAgEXMsCgYDS6bSCwaBr+fT0tKanp53H4+PjkqR8Pr/9BQUAAADgWcWcwLbtNde945KqXC5Xcnk2m122rLe3V4888siy5UeOHNnqYgEAAADYga5du6bm5uZV17njkqqVlEq2enp69PDDDzuPFxYWlM1m1draKp/PdxtLt1w+n9eRI0d04cIFNTU1VbQs2BmoM9go6gw2ijqDjaLOYKO8VGds29a1a9d09913r7nuHZdUGYaxrFUqm82WnP2vrq5OdXV1y7b3kqampopXKOws1BlsFHUGG0WdwUZRZ7BRXqkza7VQFd1xE1WEw+GSy0Oh0G0uCQAAAIDd4I5LqkzTdD22LEuhUMhzLVAAAAAA7gx3XPc/SUokEorFYmpvb9fw8PCOvEZVXV2dPvzhDy/rngishDqDjaLOYKOoM9go6gw2aqfWGZ+9njkCAQAAAAAl3XHd/wAAAADgdiKpAgAAAIAykFQBAAAAQBnuyIkqdjrLspRMJmWapizLUiQSYfbCXSidTiuVSkmShoeHdfr0aacerFZHNhvDnSUWi6mnp4c6gzWlUilZluXMnlu8NAl1BqVYlqVUKqVAICDLstTZ2enUHeoMitLptLq7uzUyMuJavh11xDP1x4bnBINB534mk7E7OzsrWBpUSl9fn+v+4nqxWh3ZbAx3jpGREVuSPTY25iyjzqCUoaEhOxKJ2LZdeH9N03Ri1BmUsvi7ybZtp/7YNnUGBYlEwvkeWmo76ohX6g/d/zzGsizXY9M0ndYK7B7pdFq9vb3O487OTqXTaVmWtWod2WwMd5bFrQ7Fx4tRZ1AUjUbV19cnqfD+Dg0NSaLOYGVnzpwpuZw6g6LOzk4Fg8Fly7ejjnip/pBUeUyxSX2xQCCgdDpdoRKhEoLBoE6fPu08zuVykgp1YbU6stkY7hzJZFKdnZ2uZdQZlGJZlrLZrAzDUDqdVi6Xc5Jx6gxWEggEdOzYMacbYEdHhyTqDNa2HXXES/WHpMpjiifPS2Wz2dtbEFTc4hPjM2fOKBwOyzCMVevIZmO4M+RyuZL9yKkzKCWdTisQCDhjEeLxuJLJpCTqDFaWSCQkSW1tbUokEs53FXUGa9mOOuKl+sNEFTvESpUGd75cLqdkMrlssGep9bY6hp1lcHBQkUhk3etTZ3a3bDYry7KcH2wikYhaWlpk2/aK21BnkEql1NfXJ8uyFI1GJUkDAwMrrk+dwVq2o45Uov7QUuUxhmEsy66L3TOwO8ViMQ0NDTl1YLU6stkYdr5UKqUTJ06UjFFnUIppms77LMn5m06nqTMoybIsDQ8PKxwOKxKJKJPJaHBwUJZlUWewpu2oI16qPyRVHlOcynapUCh0m0sCL+jv71csFpNpmsrlcsrlcqvWkc3GcGcYHBxUPB5XPB6XZVnq7e1VOp2mzqCkxZOZLEWdQSnpdFrt7e3OY9M01dPTw3cT1mU76oiX6g/d/zxm6ZecZVkKhUL8YrMLJZNJBYNBJ6Eqdu1aWhcW15HNxrDzLf1iiUajikajJU+cqTOQCt83oVDIGYtXnDVypVm7qDMIBoMaGBhwjfkdHR2lzmBFi8f6rnaOeyec2/js1TpPoyIsy9LAwIDa29s1PDzsuoAndgfLstTW1uZaZhiGxsbGnPhKdWSzMdwZcrmc4vG4YrGYIpGIotGogsEgdQYl5XI5xWIxHTt2TCMjI07LuMTnDEpLpVJOF1Gp8IMOdQaLpVIpDQ0Nqb+/X6dOnVJ7e7uTiG9HHfFK/SGpAgAAAIAyMKYKAAAAAMpAUgUAAAAAZSCpAgAAAIAykFQBAAAAQBlIqgAAAACgDCRVAAAAAFAGkioAgOekUilFo1H5fD7FYjGlUqmKleXYsWNKJpMVe34AgPdxnSoAgCcVL4I9NjbmupBjLpe7rRd2TKVSCoVCXIwUALAiWqoAAJ4UCASWLbMsS4ODg7e1HOFwmIQKALAqkioAwI7R19dX6SIAALBMdaULAADAeqRSKZ07d07ZbFZSoQXJNE2lUiml02mZpqnh4WH19fU5Y7JisZgkaWBgQCMjI0omkzIMQ5ZlKZPJuJI0y7I0MDCg9vZ2ZbNZnThxQpZlqbu7W9FoVJFIRJKUTqeVSqVkmqYsy1JnZ6dTjlgspmg06sSGhoaUSCRcr2FpWXO5nAYHB2WapnK5nLMcALBzkFQBAHaEcDiscDistrY2J8GxLEuxWEwjIyOSpGw2q/7+fp06dUrhcFgjIyMaGBhwuhJ2dXUpk8koHA4rGo0qmUyqs7NTuVxOHR0dGhkZkWEYisViisfjOnXqlE6ePOmUofh8Q0NDzrJjx47p7NmzTvkWJ1KJRELpdFrBYHDFskpSMBhUOBx2lgMAdhaSKgDAjlVMmBbPDjg8PCxJMgxDra2tkqTOzk5Jcia9sCxL2WxWlmVJktNSVBw71dPTs+LzBYNB1zLTNDU4OKhIJKLW1lbnOYtlKCZJK5W1r69Px44dk2maOnnypJMwAgB2DpIqAMCOlMvlJLlbeSS5khLTNF3b9Pb2qrW11emyt3hfiyej2K6JKUqVNZfLaWxsTOl0WmfOnFFXV5erJQwA4H1MVAEA8KS1usGlUimdPHly2TWsFj9evI/ieKZTp04545eKyzs7O5VOp1fcT3HdUs+XTqd14sSJNV/PSmXt7e2VZVkKBoPq6+tjpkEA2IG4ThUAwHNSqZQSiYRrXFNxXFKxu9ziiSqGhobU3t4uqTD26ty5c4rFYgoEAorFYgqHw8rlcs6kE0UDAwM6efKkOjs7S+6nOFFFIBDQwMBAyYkximVLp9Pq7u6WJJ0+fdoZQ1VMllYqazwel2EYCgQCymazCgQCTndFAMDOQFIFAAAAAGWg+x8AAAAAlIGkCgAAAADKQFIFAAAAAGUgqQIAAACAMpBUAQAAAEAZSKoAAAAAoAwkVQAAAABQBpIqAAAAACgDSRUAAAAAlIGkCgAAAADKQFIFAAAAAGUgqQIAAACAMvz/YKJFTcmGyrgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1IAAAE6CAYAAAAcHmMZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA8iElEQVR4nO3da3Bj6WHm9+cAIMG+sdHg3Gd67D6ULMmyViOQ9HVdltVglJS27I0GbG42m3U2ngaqnNpKlUpDiJWt2PqQosCZfIg/bBmgnXJsl1JNQLUbZ52sB+iRamVrbZOEtLZUllfGmbFaF2tGDZ4mu9nNC3DyAcRpgARIgjcckv9fFQTg3PDy8NU0Hr43w3EcRwAAAACAPfN1uwAAAAAAcNIQpAAAAACgQwQpAAAAAOgQQQoAAAAAOkSQAgAAAIAOEaQAAAAAoEMEKQAAAADoEEEKAAAAADpEkAIA4BQbHR1VsVjsdjEA4NQhSAEAcEolk0mFQiFFIpFuFwUATh2CFAAAp1C9FSqbzXa5JABwOhmO4zjdLgQAADhclmXJNM1uFwMATi2CFAB0mWVZSqfTmp6elmmaSiQSkqS7d+9KkgYHBxWPx/d0rUKh4LZAjI6OKhaLHU2hPa5YLCqZTMqyLJVKpbbHtbv3pVJJ5XJZ4+PjJ+4e2ratqakpjYyMSJLK5bIk7bkOAQD2hiAFAB4xOjoq0zSVTqebticSCZXL5aYuWvWQsLXblmEYWlxc1Pz8vCQpGo0efcE70K7cR6FQKCiRSOwYpOra3fvBwUElEglNTEwcSRkP+37UA2Q2m1UoFHK353I5pdNp5fP5fV03k8lsC2LH+bsEAC9ijBQAeFw6nZZt28pkMu620dFRjY+PNx1XLBZlmqZCoZCi0ajnQpTUutxHJRwOH/gaiURCyWTyEErT2mHfj+vXr7sTTDSKxWJNLW6dahXAjvN3CQBeFOh2AQAAuxsbG1MymXRbBdqFpK1foL3Gi+FuJ/X7adv2kdzbw7wfyWRSpmm2vWYymdTg4KB73F5lMhlZlrVt+0n7XQLAYSNIAcAJcOPGDSUSCXcmtq3jf4rFotLptCzLcsf71Mf21N/XJx+IxWIqFAruF+pEIuG2OKRSqV3PkaSZmRlZliXLsnT37l33vLrG1jOpNj6n3bilemubaZrK5/NKJBJ7mq57v+d1YmFhQZFIRKFQaF/3rK6T+9F4Lak2jmsvXQtzudyOP3/9erlcThMTE+7PEw6HNTY2Jql2Txt/n4VCQfl83q1XkjQxMdGy7K3qR7lc1sLCgtLptDKZjMLhsG7duqXJycltZd3PzwwAXeUAADwhGo068Xi87X5JTjqddhzHcRYWFhzTNJv2t9oWi8WcbDbb9BkLCwuO4zhONpt1IpGIk8/nnYWFBWdiYmLXc/L5vGOappPP5939pmm6+x3HcVKplHut+ufUr9eqjBMTE06pVGq63uLiYtv7sNfzWn1WO1vv/eLiopNKpZxIJNJ0zf3cs07vRywWa7q/pVLJiUaju/4MkpxUKrXjMaZpOrFYrKkskpru48TERNO9yOfzTiQS2XatVmVvVz+2/vxbr7ffnxkAuokxUgBwSlmWpVwu19QyMjY25k6oEAqFVCwWFY1GFYlElEqldj0nHA7Lsqymbl31Vhip1qKRTCY1OTnp7r9161bLrmGN5SwUCk3Xa3x/2Oe1Mz8/r0wmo0wmo9nZWUWjUS0sLDR16ev0nnV6P4rFogqFwrb7Wy6X9/Sz1Wd63Kv6Yr2NXf0mJyfbdufbTbv60SgSiTRd+6A/MwB0C137AOAEsG1b0vYvpTspFApul7S6UqnU9CV26/X2c04oFHKn2J6fn1coFGoKH7vN6lbfb9u22x2sfr2jOK+d4eHhPU0R3sk96/R+zM/Pt/wd17sv7jQuqTHQtmNZ1q4TTtTLW5+8pFOt6sfg4GDb4w/yMwNANxGkAOAEqE9nPjw8vOdzbNveNvnA1i+lWydQ2Ms5u31mp4rFoqampjQ6OqobN27s+cv7fs87qE7uWS6X6+ja+7l/jZ+5UwtOfXzdQYPJYS/0e5CfGQC6ia59AHACpNNppVKpjmaO29qFqm6nL677OWfr+a2ObXe+bdu6fv26JicnFY/HFQqF3GN3al3Z73lHYad71un9iEajLa9lWZa7wG47qVRK5XK5bXirz/q424Qctm27ZW+lHsgOy0F+ZgDoJoIUAHjc9PS0bNvueBazaDSq4eHhbV+sZ2dnD/WcxlBQn62uPsNbfX+78y3L2valvd49b6cv7Ps97yjsdM86vR+RSGRby1L952kcg9VKKBRSNpvV1NTUtmBSnzVw6+yK9es3/g6npqYUj8fdVqfGLoOWZXU8M+JuIfwgPzMAdFW3Z7sAgLOuVCo5ExMTjiTHNE0nlUq5M73F4/FtM7EtLCw4sVisaZa2xm0TExNNs+hNTEw46XS6aba4fD7vRKNRJxQKOalUqmnWtnbntPrcVCrlhEIhJxKJNM1aNzEx4aRSKSebzTbNNLj1/PqxExMTTj6fd/L5vFMqlbbNgtfKTudtvR873fv6z1C/9+1mDNzPPdvP/Wi8Vjqd3rH8rSwuLjadX3+0+5nqv7t8Pu/Wva3q93qnsu9UP6LRqDvTYePvpfFeH+RnBoBuMBzHcboV4gAAQPfU135aWFjodlEA4MShax8AAAAAdIggBQAAAAAdIkgBAHAGFQoFpVIpFYvFpskwAAB74+kxUsViUTdv3ty173Z9Vfn6zEL1qXABAAAA4Ch4NkjVg9HQ0JB2K+LQ0JAbtizLUjKZ3HHleAAAAAA4CM8GqTrDMHYMUpZlaWxsrKnV6sqVK1pcXDyO4gEAAAA4gwLdLsBBFQoFhcPhpm3hcFjFYrHtooGrq6taXV1131erVZXLZQ0MDMgwjCMtLwAAAADvchxHy8vLeu655+TztZ9S4sQHqXYrptdXuG9lampKn/3sZ4+oRAAAAABOujt37uiFF15ou//EB6l22gUsSZqcnNSnPvUp9/29e/f04osv6s6dO+rv7z+G0rX3rbvf0n89Payv/5akYFB6552ulgcAAAA4S5aWlnT16lVdunRpx+NOfJAKhULbWp/K5fKOs/YFg0EFg8Ft2/v7+7sepC6tX5L6JLcUXS4PAAAAcBbtNuTnxK8jFY1GW24fHh4+5pIcDkOGnPrvrFrtalkAAAAAtHYigtTWbnrFYlGWZUmSTNNs2mdZloaHh0/sOlKGYcido9DbEyoCAAAAZ5Zng1ShUFAymZRUmxwil8u5+7a+z2azSiaTyuVySqfTJ3oNKUOGqvUWKYIUAAAA4EmeX0fqOCwtLeny5cu6d+9e18dIWYuWfu5/HdT3/zdJhkH3PgAAgFOsWq1qbW2t28U4U3p6euT3+9vu32s2OPGTTZw2hujaBwAAcBasra3prbfeUpU/nB+7UCikZ5555kBryBKkPMYwGrr2SbUwxSLBAAAAp4rjOPr+978vv9+vq1ev7rjwKw6P4zhaWVnRO5tLDD377LP7vhZBymOaZu2TCFIAAACn0MbGhlZWVvTcc8/p/Pnz3S7OmXLu3DlJ0jvvvKOnnnpqx25+OyH6ekzTrH0S3fsAAABOoUqlIknq7e3tcknOpnp4XV9f3/c1CFIe4jjSwwc+VR+FmjcCAADgVDrIGB3s32Hcd7r2ecyPX31e/VVLUri2gcGHAAAAgOcQpDzEMKRgnyNnpSEh0yIFAAAADygUCkokEkokEgqFQkqn05KkRCKhUqmkXC6nbDarSCTinjM9Pa1QKKRwOCzLsmSapmKxmLu/WCwqnU4rk8loYmJCg4ODKpVKsixLiURC0WhUkmRZlnK5nEKhkCTJNE1ZlqV4PH58N2ALgpTHBINSZaWhxyVBCgAAAB5g27by+bxM05Qk5fN5hcNhN8yMj4/Lsiw3SA0NDWlmZqYpWCWTSc3NzSmVSkmSIpGIUqmUMpmMJicn3aBk27auXLmihYUFRSIRjY2NaWFhwb3O9PS07t69exw/dlsEKY/pO+fo/mJDixRd+wAAAE49x3G0sr7Slc8+33N+T2OGyuWyG6JaiUQimp+fl1QLTKZpNoUoSUqlUrpy5YrGx8e37WsUCoVkmqZu3brlhqtGExMTmp6e3rXMR4kg5TF9QUfLomsfAADAWbKyvqKLUxe78tn3J+/rQu+FXY+7cePGno+Znp52u/5tFY1GNTU1pWw2u+O1yuWyBgcH3W58mUymqStfN7v1Scza5znBPkdV0bUPAAAA3tKqZajVMZZlSZKGh4dbHmOaporFYttr2LatZDKpaDTqhqWZmRklEgkZhqHR0VEVCoU9leco0SLlMcE+yRFd+wAAAM6S8z3ndX/yftc++yiUy+WOjs9kMm7XwUQi0dSNMBaLqVQqqVAoKJ/Pa3R0VNlstmniiuNGkPKYc31Oc5CiRQoAAODUMwxjT93rToJ6AKq3TG1VLBZbjo+Kx+MtW5ls23bHTMXjccXjcWUyGU1NTXU1SNG1z2OCfaJrHwAAAE60iYmJtmOg5ufnlUgk9nwty7K2dQW8ceOGbNs+SBEPjCDlMX107QMAAMAJl0qlVC6XVSgUmrYnEgnduHHDXR+q0U5dAZPJZNP7QqHQ1dYoia59ntPX56gq/+MNtEgBAADAQwqFQlMrUSaT0fDw8LbuegsLC0omk7Isy12Qd3R0dNuCvLdu3ZJUC1+JRKJlt7+xsTF3cV9JKpVK7lpU3WI4Dt/Ul5aWdPnyZd27d0/9/f1dLcv4f7uq7Od7Hoepd96Rnnyyq2UCAADA4Xr06JHeeustXbt2TX19fd0uzpmz0/3fazaga5/H9AXFZBMAAACAxxGkPKYWiAlSAAAAgJcRpDzm3Lnac7UepphsAgAAAPAcgpTH1Ltouu1QtEgBAAAAnkOQ8ph6kHLXkiJIAQAAAJ5DkPKYYF8tODn1YVJ07QMAAAA8hyDlMec3x0i5M/fRIgUAAAB4DkHKY/r6agGKrn0AAACAdxGkPObxZBPM2gcAAAB4FUHKY7YFKVqkAAAA4EHFYlHJZLLl9kQiIcMwlEwmlclkND09rUQioVwu1/bYTCbT8nPGxsZ05coVTU9P7/uco2A4Dt/Ul5aWdPnyZd27d0/9/f1dLcv/8/+u6Zc+0asl44IuOSvS3/6tNDjY1TIBAADgcD169EhvvfWWrl27pr76X9JPmEQiodnZWS0uLm7bZ9u2rly5osXFRYVCIXf72NiYRkZGNDEx0XTszZs3ZVmWFhYWtl0nmUzKsizl8/kDndNop/u/12xAi5TH0LUPAADg7HEc6cGD7jz226wSCoVk27YKhcKez5mZmVEymZRt203bx8fHZVmWLMtq2j4/P6+hoaGW19rPOYeJIOUx5zYnm3AcuvYBAACcFSsr0sWL3XmsrHRe3kKhoPHxcUWjUWWz2T2fFwqFFIlEtnXJC4VCunHjxrauf7tdq9NzDhNBymPObU5/XpW/9oIgBQAAAI8pFouKRCJu975OmKapubm5bdsTiYTS6XTTZwwPD+94rf2cc1gIUh5z7txmixRd+wAAAM6M8+el+/e78zh/fv/ljsViHXfvk7Sta58kRSIRSbUwJEnlcrlpfFUr+znnsASO5VOwZ27XPtaRAgAAODMMQ7pwodul2JtCoaBSqeR2zzNNU9lsVtFodE/nW5bV9thYLKZ0Ot3UyrSb/ZxzGAhSHlNvkWJBXgAAAHhRsVhsCi3hcFg3b97cc5CxLEuJRKLlvkQioaGhIY2Nje05mO3nnMNA1z6POU/XPgAAAJwgnXTvSyQSisfjMk2zaXu9q59pmjJNs+205Qc95zB5ukXKsizlcjmZpinLshSPx9v2ebQsS4VCQeFwWJZlKRaLbfsFnQS0SAEAAMCLCoWCUqmUyuWyotGoOz4pk8koFAopmUwqkUhoeHjYbZ2amprS4OCgbNtWqVTS6OioYrGYe81isaipqSl3CvNYLKZEIuF+j8/lcspms5qfn1cmk1E8Ht/XOUfB0wvyDg0NuQtsWZalZDLZdnrF6enppoW9ts7gsRMvLcjrOJLPX9H3nBf0rP5e+upXpZde6mqZAAAAcLhOw4K8J9mpXpB368Japmnu2Fx469atoy7SsTAMSYFHj7v2eTfnAgAAAGeWZ4NUvZteo3A47E5tuFU4HNbQ0JDbxW90dLTttVdXV7W0tNT08JTAQ7r2AQAAAB7m2SDVam55qTY3fCv1Ln+Dg4PKZrNNfS+3mpqa0uXLl93H1atXD1zeQ9XziMkmAAAAAA/zbJBqp13Aqg9+S6fTymQybadUlKTJyUndu3fPfdy5c+eISrtPdO0DAAAAPM2zQSoUCm1rfWq3UrFlWZqbm1M0GlU8HlepVNLs7Oy2cVZ1wWBQ/f39TQ9PCTyiax8AAADgYZ4NUu0W0xoeHt62rVgsamRkxH1vmqYmJyfbtl55nRGgax8AAADgZZ4NUlvXgLIsS8PDw26LVLFYdFucIpGI5ubmmo6/e/euO7f9idND1z4AAADAyzy9IG82m1UymdTIyIjm5uaa1pCamprSyMiIJiYmZJqmRkdHNT097QatncZIeV5g9XHXPlqkAAAAAM/xdJAyTVOpVEqSts3Ct3Vh3mg02rY74Elj9DwkSAEAAMBTisWiO7HbxMSEBgcHZdu2SqWSMpmMFhcXZVnWtmNKpZIsy1IikVA0GlWhUFA2m3WPGR0dVTQalWVZyuVybsOIaZqyLEvxeLy7P3gbhuPQd2yvqxcfl8CHvqC/+vq/0gf0TelLX5J+4Re6XSQAAAAcokePHumtt97StWvX1NfX1+3i7JllWRocHNTi4mLTJHCZTEbDw8OKRCKybVtXrlxpOqa+bWFhQZFIpOV1hoaGtLCw4F5zenpad+/edRtWDtNO93+v2cDTLVJnFl37AAAAzhbHkVZWuvPZ589LhrGnQ8PhcMvtN27c0Pz8fNvzQqGQTNPUrVu3FIlEtl2n1WzbExMTmp6e3lO5uoEg5UEGQQoAAOBsWVmRLl7szmffvy9duLCvU4vFokzTdIPSTsrlsgYHB1vuq3fjy2QyTV35vNqtT/LwrH1nGWOkAAAAcBLcunXLfd0uSNm2rWQy6a752s7MzIwSiYQMw9Do6KgKhULLNWS9ghYpL+pZVUX+2utKpbtlAQAAwNE7f77WMtStz+5QJpORJBUKBU1OTrY9ph6uEonEri1WsVhMpVJJhUJB+Xxeo6Ojymaz2yad8wqClAfRtQ8AAOCMMYx9d6/rhng8rlAotOO6rfVj9sK2bbd7YDweVzweVyaT0dTUlGeDFF37PMjoIUgBAADA+6LR6KF0v7MsS8VisWnbjRs3ZNv2ga99VAhSHmQEHhGkAAAA4DnlcvlQjm21L5lMNr0vFAqebY2S6NrnSU0tUoyRAgAAgAfUF+SVaqFndHR0W9ApFovuBBSpVEqJRGJb97/6grySNDU1pfHxcUnS2NiYpqen3RauUql0JGtIHRYW5JX3FuQ1/vF/r//wf5f08/oTKZeTXn6520UCAADAITqpC/KeFoexIC9d+7yI6c8BAAAATyNIeRFjpAAAAABPI0h5EUEKAAAA8DSClBcFHrEgLwAAAOBhBCkP+vAL76NFCgAA4Axg3rfuqB7Cd2ymP/cgf+86QQoAAOAU6+npkWEYevfdd/Xkk0/KMIxuF+lMcBxHa2trevfdd+Xz+dTb27vvaxGkPCjQu0GQAgAAOMX8fr9eeOEFfec739Hbb7/d7eKcOefPn9eLL74on2//HfQIUh4U6N14PEaKIAUAAHAqXbx4Ue9973u1vr7e7aKcKX6/X4FA4MCtgAQpD+ppaJGqrlcYyAYAAHBK+f1++f3+bhcD+8B3dA9q7Nq3sUaLFAAAAOA1BCkPIkgBAAAA3kaQ8qCeXqmqWp/N9VWCFAAAAOA1BCkP8hk+VTbHvq2vsiAvAAAA4DUEKQ/yGT5VfbXF2TZokQIAAAA8hyDlQX7Dr6qxGaQYIwUAAAB4DkHKg2otUrUARZACAAAAvIcg5UE+w6fK5m+GIAUAAAB4D0HKg/w+v9siVVljsgkAAADAawhSHkTXPgAAAMDbCFIe5DN8qhoEKQAAAMCrCFIe5Df8qmxOf15ZJ0gBAAAAXkOQ8iCf4VPVvzlGiiAFAAAAeA5ByoNqY6Rqk0xU1plsAgAAAPAagpQH+Y3GWftokQIAAAC8hiDlQY2z9lU2CFIAAACA1wS6XYCdWJalXC4n0zRlWZbi8bhCoVDb4wuFgizLkmmakqRoNHpMJT1cPsOnyuYYqSpjpAAAAADP8XSQGhsb08LCgqRaqLp586ay2WzLYwuFgrLZrNLptCzL0ujoqEql0nEW99DUFuTdkMRkEwAAAIAXeTZIWZbV9N40TRUKhbbHJxIJN3SZpql8Pn+k5TtKjbP2VZlsAgAAAPAcz46RKhQKCofDTdvC4bCKxeK2Yy3LUrlcVigUUrFYlG3bbve+k6hx1r4qY6QAAAAAz/FskLJtu+X2crm8bVuxWFQ4HHbHU2UyGeVyubbXXl1d1dLSUtPDS2pjpAhSAAAAgFd5tmtfO60CVrlclmVZikajCoVCisfjunLlihzHaXmNqakpffaznz3iku6f3/Cr6q+NkSJIAQAAAN5zJC1Sfr//wNcIhULbWp/q3fe2Mk1ToVDI3Vd/btUNUJImJyd1794993Hnzp0Dl/cw1cZI1VqknA3GSAEAAABe03GL1G7d4BzHadsS1IloNKp0Or1t+/Dw8LZtnY6HCgaDCgaD+y7bUfMZPlV8tEgBAAAAXtVxkNppNjzDMOQ4jgzDOFChpO3hyLIsDQ8PN7U2hUIhmaYp0zQ1PDws27YVCoXctaQikciBy9ENfp9f6/WufRWCFAAAAOA1HQepl19++SjK0VI2m1UymdTIyIjm5uaa1pCamprSyMiIJiYmmo4dGhrSwsLCiZ7+3G/4VQnUu/YRpAAAAACv6ThIvfbaazu2OB1Gt7460zSVSqUkSbFYrGnf1oV5Q6FQy66AJ1HAF1DVvy6Jrn0AAACAF3UcpF599dVdj/nMZz6zr8KgJuALqBqoBSmnwmQTAAAAgNd4dh2ps6zWIrUmiRYpAAAAwIs6DlK3b9/W66+/rrfffvsIigOpNtlExW2RqqpKlgIAAAA8peMgZVmW3njjjbZrNOHgal37ai1SPlX16FGXCwQAAACgScdByjRNjY2NndipxU+CrUHqwYMuFwgAAABAk44nm7h+/bquX79+FGXBpoAvoOpmxPWropWV7pYHAAAAQLOOg1SjN998U5L0sY99rOX+paWlXa/R399/kCKcSgFfQJXNIOVTlSAFAAAAeMy+g9Rrr72mUCikUqmkVCqlVCqll156qemY3RbFNQxDn/zkJ/dbhFMr4AuourlUF137AAAAAO/Zd5AyTVMvv/yy+35mZmZbkGrcj73bGqRokQIAAAC85UBd+yYnJzU+Pq6XXnpJAwMD2/a/9tprMgyj5bmO48gwDH36058+SBFOpYAvoMrmbQtogyAFAAAAeMy+g9TLL7+sSCSidDqtV155RbZta25uTjdu3HCPefXVVw+lkGdNwBfQur/2ukfrKtO1DwAAAPCUA7VIXbt2TZ/73Ofc97dv39b8/PyBC3XWBXwBrW9ONtGjdVqkAAAAAI/peB2pnVy/fl03b95s2nb79m29/vrrevvttw/zo061gC+gtc0WqV6tEaQAAAAAjzlwkNotIFmWpTfeeEPFYvGgH3VmbO3aR5ACAAAAvOXAQSqXy+243zRNjY2N6SMf+chBP+rM2Nq1j+nPAQAAAG850BgpqTb73k6uX7+u69evH/RjzhS/4adFCgAAAPCwA7dItZvefKs333xTb7755kE/7kxgsgkAAADA2w51sol2XnvtNZVKJb3xxhv6+Mc/rq997WvH8bEn1tYxUnTtAwAAALzlyLv2SbVxUi+//LL7fmZmRi+99NJBP/rUokUKAAAA8LYDt0iZprmn4yYnJ92WqIGBgYN+7Km2dfpzWqQAAAAAbzlwi1RjS9NOx0QiEaXTab3yyiuybVtzc3MaHx+nZaqFgC+gymbE9amq5eXulgcAAABAswMHqb26du2aPve5z7nvb9++rfn5eYJUCwFfQNXNOTz8qmhxsbvlAQAAANCs4659v/3bvy1p94V4d3P9+nW98sorB7rGaRXwBVTZDFI+VWXbXS0OAAAAgC06DlLXrl2TJGWz2V2PXVpa2vWB7ba2SBGkAAAAAG/Zc9e+119/XZFIRAMDA3r99dc1Ojq66zn5fH7H/YZh6JOf/ORei3BmbB0jZdtStSr5jmWyegAAAAC72XOQunbtmhYXFzU7O6v5+XndvXt31/FNe5mIAtttbZFyHGl5Wbp8ubvlAgAAAFCz5yBVD0WdhKPXXntNhmG03Oc4jgzD0Kc//ek9X++saBwj5VdVkmTbBCkAAADAKw40a9+bb74pSfrYxz7Wcv+rr756kMufWY0tUpJkqKrFRZ9+5Ee6VyYAAAAAj+171M1rr72mUqmkN954Qx//+MfdxXZxcI1jpCQmnAAAAAC8Zt8tUqZpNnXzm5mZYU2oQ+L3+d2ufRJToAMAAABec6B54CYnJ92WqIGBgcMoDyT5DX9T1z5apAAAAABv2XeL1Msvv6xIJKJ0Oq1XXnlFtm1rbm5ON27cOMzynUmGYUh+n7Q50QQtUgAAAIC3HGiyiWvXrulzn/uc+/727duan58/cKEg+fwBSWuSaJECAAAAvOZQl3i9fv26bt68eZiXPLMM/+OMS4sUAAAA4C0dt0gtLS3tuN9xnH0XBo/5GoKUXxUtLnaxMAAAAACadByk8vl8232GYRxqkLIsS7lcTqZpyrIsxeNxhUKhXc9LJpOanJzc07Fe5Q/0uK9pkQIAAAC8peMg1Tjl+VEbGxvTwsKCpFqounnzprLZ7I7nFItFTU9Pa3Jy8jiKeGQCvoAqhuR3GCMFAAAAeE3HQeq1116rzSrXxmG1SFmW1fTeNE0VCoU9nWea5qGUoZsIUgAAAIB3dRykXn311V2PSSaT+ypMo0KhoHA43LQtHA6rWCwqEom0PCeXyykWix3K53dbwBdw15Kiax8AAADgLQea/rydarV64GvYbZJDuVxue/xex0Strq5qdXXVfb/bBBrdEPAFVPFJqtRapMp2t0sEAAAAoO5Qpz8/Du0C1uzsrKLR6J6uMTU1pcuXL7uPq1evHmIJD0e9a59UC1JLS1Kl0t0yAQAAAKjxbJAKhULbWp/K5XLLVqdCoaAbN27s+dqTk5O6d++e+7hz585Bi3voAr6ANjZ/O37VEpQHG84AAACAM+lIuvYdhmg0qnQ6vW378PBwy+NnZ2fd15ZlaWpqSuPj4y3HUwWDQQWDwcMr7BFoDFL9fevSI2lxUbpypbvlAgAAAODhILV15j3LsjQ8POy2SBWLRYVCIZmmua1LXyKRUCKRONGz9/l9fq37a6+vXNqQHokJJwAAAACP8GzXPknKZrNKJpPK5XJKp9NNa0hNTU0pl8s1HW/btqanpyVJqVRKxWLxWMt7mBpbpK5cXJdEkAIAAAC8wnAOa+GnE2xpaUmXL1/WvXv31N/f3+3iSJJ+7v/4Of2fn/6K3rMoxT/0Fc381c/oC1+QPvnJbpcMAAAAOL32mg083SJ1ljW1SF2otUgtLnaxQAAAAABcBCmPCvgC7hip0MUNSVKbJbQAAAAAHDOClEc1tkiFL9VapH74wy4WCAAAAICLIOVRAV9A61smmyBIAQAAAN5AkPKoxhapy5td+whSAAAAgDcQpDyqcYzU5XO1Fqm7d7tYIAAAAAAugpRHNbZI9V+gRQoAAADwEoKURzWOkervY4wUAAAA4CUEKY9qbJG6dO7x9OeVShcLBQAAAEASQcqzAsbjMVIXg7UWKceRbLt7ZQIAAABQQ5DyqMYWKb+zocuXa6/p3gcAAAB0H0HKo/w+vztGSuvreuKJ2kuCFAAAANB9BCmPuth70W2R0sYGQQoAAADwEIKUR13sveiOkWpskWItKQAAAKD7CFIeFfQHm1qkBgZqL2mRAgAAALqPIOVRvf5exkgBAAAAHkWQ8qhgIMgYKQAAAMCjCFIeFfQHGSMFAAAAeBRByqN6/b2PW6TW1xkjBQAAAHgIQcqjgoHg4zFSDV373n23a0UCAAAAsIkg5VFNs/atr+vJJ2svCVIAAABA9xGkPKrX3/t4jNTGhp55pvbStqVHj7pVKgAAAAASQcqzmmbtW19XKCT19tbevvNOt0oFAAAAQCJIeVbQ3zxGyjCkp56qvf3BD7pWLAAAAAAiSHnW1ln7JOnpp2tvCVIAAABAdxGkPCoYCDaNkZIIUgAAAIBXEKQ8qlWLVH3Cib//++6UCQAAAEANQcqjto6RkmiRAgAAALyCIOVRW2ftkwhSAAAAgFcQpDyqcR0pZ4MgBQAAAHgJQcqjgv7HLVLVtTVJj4MUY6QAAACA7iJIeVQw8HiMlLNlsglapAAAAIDuIkh5VI+vx22RctabW6RsW1pd7U65AAAAABCkPMswDKknUHuzGaSuXJF6emqb3nmnSwUDAAAAQJDyMiNQS03Oem36c8OQnnqqto9xUgAAAED3EKS8rN78tNkiJTFzHwAAAOAFgW4XYCeWZSmXy8k0TVmWpXg8rlAo1PLYYrGoQqEgSZqbm9PMzEzbY08KX29v7cXmgrzS4wknvv/9LhQIAAAAgCSPB6mxsTEtLCxIqoWqmzdvKpvNtjy2UChoYmJCkjQ9Pa3r16+7555URs9mkFp/HKSef772/N3vdqFAAAAAACR5uGufZVlN703TdFuctioWi5qamnLfx2IxFYvFbdc4aXz1IFUhSAEAAABe4tkgVSgUFA6Hm7aFw2EVi8Vtx0YiEc3MzLjvbdt2j29ldXVVS0tLTQ8vMja79hkNLVIvvFB7JkgBAAAA3ePZIFUPQ1uVy+WW22OxmPv61q1bikajbcdITU1N6fLly+7j6tWrBy3ukfD1BCVJxkbF3UaLFAAAANB9ng1S7bQLWI37c7lc27FUkjQ5Oal79+65jzt37hxyKQ9HvWufsUHXPgAAAMBLPDvZRCgU2tb6VC6Xd52JL5lMKp/P73hcMBhUMBg8hFIeLX+wT1LrIHX3rvTokdTX142SAQAAAGebZ1ukotFoy+3Dw8Ntz5menlYymZRpmrJte9fWK8/bTEm+qiOtr0uSrlx5HJ6+971uFQwAAAA42zwbpEzTbHpvWZaGh4fdlqats/LlcjlFIhE3RM3Ozp74daR07tzj1w8fSpIMg+59AAAAQLd5tmufJGWzWSWTSY2MjGhubq5p3NPU1JRGRkY0MTEhy7I0NjbWdG4oFFI8Hj/uIh8q37nzj988fCj190uqBalSiSAFAAAAdIung5RpmkqlUpKaZ+WT1BSqTNOU4zjHWrbjcCnYr4cB6dyG3BYpiRYpAAAAoNs827UP0qXgJa30bL5pCFL1taS+853jLxMAAAAAgpSn9W+2SEmSVlbc7bRIAQAAAN1FkPKwS72X9LBFixRBCgAAAOgugpSHNbVINQSpF1+sPb/99rEXCQAAAIAIUp52Kdi6Reo976k9f+970oMHx18uAAAA4KwjSHnYpd7Wk02Ew7WHJH3rW8dfLgAAAOCsI0h5WKgv1LJrnyR98IO152LxeMsEAAAAgCDlaT828GN60Ft7vWrfbdr3cz9Xe/7TPz3mQgEAAAAgSHnZkxee1IP+c5Kku3f+pmnfT/907fkv/uK4SwUAAACAIOVx/qefliQt3fnbpu0/9VO15298Q1pePu5SAQAAAGcbQcrj+p6rzXW+9v3vNG1/5pnaNOiOIy0sdKNkAAAAwNlFkPK4Ky++T5Lk+8E72/b95E/Wnv/8z4+zRAAAAAAIUh734j/4eUlS+AdLqjrVpn31cVJMOAEAAAAcL4KUx5kj/4Uk6bklR3/97ea5zj/60drzl74kra8fb7kAAACAs4wg5XGBJ57S/XO1xaT++s//qGnfRz5SW5h3eVmam+tG6QAAAICziSDldYahxR95SpL0w7/4YtMun0+6fr32Op8/7oIBAAAAZxdB6gSoDEckSb3zX5XjOE37RkdrzwQpAAAA4PgQpE6AZ67/siTp/aUlffOH32zaVw9Sf/Znkm0fc8EAAACAM4ogdQL0/cOPSpKGvi/98fytpn0/+qPSBz4gVSrSH/3R9nMBAAAAHD6C1EkwOKiy+ayCFenu//Xb27r3vfxy7fkLX+hC2QAAAIAziCB1EhiGzv3zX5UkfeLN7+or325eOOqTn6w9//t/Lz14cNyFAwAAAM4egtQJcS7+a1rt9eunvyvd/t//p6Z9L70kXbsmPXxYC1MAAAAAjhZB6qR49lk9+B/jkqR/8TtFvfkfP+/uMgwpFqu9/oM/6EbhAAAAgLOFIHWChH/jc/rBC1d0dUl6KvYreuebC+6+X/mV2vO/+3fSD37QpQICAAAAZwRB6iTp79flN/6D7l706ye+tyHfT/207v3+70iOow9+UPqpn5I2NqTf//1uFxQAAAA43QhSJ0zfB35Cy18u6BvPBvTE0oYu//NXdD/yIenWLb3y361Kkv71v5YWF7tcUAAAAOAUI0idQD/60kcV+LO/0G/+l1e0EpAufu0b0j/5J/oX//PT+oNzr+jaW7f1sV+o6O/+rtslBQAAAE4nw9m6KNEZtLS0pMuXL+vevXvq7+/vdnH2zH5ka/Lz/4Oe/r1/o1eK0gvLj/d9T8/q3wbH9b5f/290/TMjtRkpAAAAAOxor9mAIKWTG6TqvvjWF/W/3P5X8v3pV/RP/0oa+7pP4dWqu/97501d+uWoLn38Z6Wf+Rnpve8lWAEAAAAtEKQ6cNKDVN1X7nxFM8UZ/eFf5vSzX3+of/rFH9cvvVvSBa00HbcW6tf6Sx9S3098WP73/7j0/vdL73uf9PzzBCwAAACcaQSpDpyWIFX3cP2h8lZe+VJeX/zSt/Se3/2H+tl3l/Sz+oqGNa8+rbY8b73Hr/tP9OvRM0+o8uzTqj73rHxPPqWeJ59R8KnndO6ZF9T79HMyQiHp4sXaIxA43h8OAAAAOEIEqQ6ctiC11Q/uv6Pfnr2j33rtWf3gm0/oJX1NP6Gv6/39b+j95/6j3rf2HQ3aVQX2URMe9RhaCfr0MOjXoz6/NnoCqvT4Ve3tkdPbK/X2yhfsU8+5C+o9f0nB8/0K9od1PvSEgv1hGZcuSc89J734onT1qvTkkye6VazqVPWJz39CQX9Q6X+U1tMXn+52kQAAANABglQHTnuQqnMcqVCoTY/+h3/oqFqtBRafv6oPfPgtfejH/kQvDnxZV/RVnX/3rvp/uKwLy490cXlNl+9vaOChNLAiXV6Veqq7fNg+rfb49MOBcyo/cUFL4fN6eLFPqxfPae3iOa1dOq/1/guqXDivSrBHG70BVXp7ao/N99WAX/L55PgMyfBJfp/k88vxGarIUcVwVJWjarUip1qVU63UXlcqcpxq6+eGY51qVY5TkapVqVKRUWl+nv/2n2n54T35Hclflcz+F3XB36dexy+jWpVRqdaeN1/7qo6MSlW//N5/pCeDA7rgD+qCr09BI6Aex6eA41PAkYzNz+vosbFRew4GpV/7tVpgvXRJ8p2uyTqz38jqtxZ+S37Dr3dX3tXF3ov61Y/8qn7y+Z/UC/0v6FzgnHr8Pd0u5oFs/c+0o+3/2d7tmKO4xn4+Zy/X6PX3qi/Qt+04AACOA0GqA2clSDX69relz39emp2VvvrV5n2hUG1x36Gh2iMSkV64WtXKxn0trS5pdWNVqyvLWr9X1saSrY0lW9Wle6rcX1b14Yo2Hj3Q2sP72nj4QOsPH2h1ZUkrD+5pdeWe1h4sq7pyXz0P13VhXepflV5Ykq7ek56735VbceY86PPpQZ9f6wGf1v2G1nuM2nPA0FrAp42AoXWfIRmSY0hVw5AjR45h1N5LkiFVN/c7MlTdzGZO/X8dR4az+d5xZDjum4bXzfsMbT47ta/V7bc3b6tWq6q3Ydb2bX/2STJktN3f8rnlPqfp/U6f2fm1Hz+v+qXf+Kg0M1S712eN3/Drd//x7+qf/YN/1u2iAADOIIJUB85ikGpUKkl/9EfS7dvSl74kLS1tP+b8eek975F+7Mdqk/4NDtbmpnj++VpDRzjcWY+8B2sPZD+ytby2rOXVZS2vLevBclmV73xbvjvf1bnvv6veu4vqWV5RYOmBepcfKHj/oYLLD9W7sqrAekWB9Yp61ioKrG2oZ72i3rXKod2T3VR9hpzNlq+q31d77T77FQyel+MztG44qvoMVX2G5PfL8ftqLWZ+v+T3aWljRffW72vNqGjN2dCqKlp11rWmqio+qWLoQM8f/nvp578tXVqVeo+oFRFH528GpN99SZr9oGSFu12a42XI0K9+5Ff1Gx/9DT3f/3y3iwMAOENORZCyLEu5XE6macqyLMXjcYVCoQMfu9VZD1KNNjZqLVQLC48fX/+6tL6+83l9fbVA9eSTtVA1MFB7bnz09z+eo6LxceGC1Nt7CIV3nFqXu/qjUml+37jN56slv06f648jVnWqWqusaXVjVY82Hmm1Unt+tPFIa5U1VZ2q+6hUK49fO5WW+wzDkG91TYH7K+q5/1A99x/KWF+Xf6Mi3/qGfGvrtef6642K2+JTaxnabImpv64+bhGSU23eZ9TvmSFDteBoSJJhyPD5JMMno77fePzeMIzH+zaPrV/L8PlkaOt2n4zNLpyX+y4rGAg2/Y4cSevVjVowrazVGsEa9xvGZjNQ4zY9Lv/mMUbj773Nw9isH07tB226hvvwbTmn6XP0eFulovO/9Tvqm/2CjJWHbp2oXPsRrV3/Ra1Hf1EbQx9R9dlnaue0aLIyttRRrx/TuH+tsqZ/+f/9S/3ef/o9SVKPr0e/9L5f0vgHx/WL135RT5x/Ytv1AQA4TKciSA0NDWlhYUFSLSglk0lls9kDH7sVQWpn6+vS3/2d9J//c+3xrW9Jb70lffe70ve+J/3whwf/jJ6ex6Gqr682rKfxudW2rc89PbVHIPD49WFt8/sfP07ZECN41fKylMtJv//70pe/XPsrR6OBAenDH6493v/+2mQt9cfly90p8yH6yp2v6DOFz+jL3/6yu82QofcOvFcffPKD+sATH9Dz/c/rmYvP6OkLT+ty32Vd6LmgC70XdKHngs71nJPP4P+sAIDOnfggZVmWxsbG3HAkSVeuXNHi4uKBjm2FIHUwq6u1QFUPVeXy48fdu49fLy9L9+83P9bWul36/WkMVjs9toaww3g0NpLVH1vf73XbcZ7X2JjXrnFnP/sO+3rHta9uawNny33Ly+r5ky+q54t/rJ4vf1G+v/2b2gQkbTiXLqn67PNywgNyroSl0BU5V8Jy+i/X+umeOyfn3Pna6/Pn5Zw/LwX7ZPTW/nLgBGrP9fdNf2Xw1VsJG1rX/M0tcO4+n6/pOHdfBz/7X/7gL/V7/+n39MelP9bX3/l625+5FZ/hk9/wK+ALyO/zy2/43eeAL7AtaHXScnYY+0+9hq8X7pjCht07bWva3uI659Yd9a07WgsY2vDVulu7XZt9hio+ba9gADzvE+/9hH7zv/rNbhdjz9nAs4sAFQoFhcPNgwLC4bCKxaIikci+j5Wk1dVVra4+Xkvp3r17kmo3DfszMFB7dGp9XXrw4PGjHq5WVx8/Hj2qbXv0qPW2+vP6eu2P9hsbtdf1962e93LsTn9iqE+MBxyfj24+pKAe6gP6a31Q39CH9Ff6Ub2t5/VdvaDvKCy79leL5W92say7q9Sm7nAf7Twn6TObj/39k+VIWt98tDui+1+4vVAGo2H2xPrrvW7zYttfVbvf127uP+i1gdOo8NRfaelb3f8+Xs8Eu7U3eTZI2bbdcnu5XD7QsZI0NTWlz372s9u2X716dc/lA4BuWZX0tc3HyeVI8mSHCJwqu9Ux6iDgKe98yVPd05eXl3V5h/J4Nki10y40dXLs5OSkPvWpT7nvq9WqyuWyBgYGut71YmlpSVevXtWdO3foZog9oc6gU9QZdIo6g05RZ9ApL9UZx3G0vLys5557bsfjPBukQqHQthalcrnccia+To6VpGAwqGAwuO0aXtLf39/1SoSThTqDTlFn0CnqDDpFnUGnvFJndmqJqvNit2ZJUjQabbl9eHj4QMcCAAAAwEF5NkiZptn03rIsDQ8Puy1HxWJRlmXt6VgAAAAAOEye7donSdlsVslkUiMjI5qbm2taF2pqakojIyOamJjY9diTJBgM6td//de3dT0E2qHOoFPUGXSKOoNOUWfQqZNYZzy7jhQAAAAAeJVnu/YBAAAAgFcRpAAAAACgQwQpAAAAAOiQpyebOGssy1Iul5NpmrIsS/F4nJkHz6BisahCoSBJmpub08zMjFsPdqoj+92H0yWZTGpycpI6g10VCgVZluXOfFtfSoQ6g1Ysy1KhUFA4HJZlWYrFYm7doc5Aqn1/uXnzphYWFpq2H0X98EzdceAZkUjEfV0qlZxYLNbF0qBbUqlU0+vGerFTHdnvPpweCwsLjiRncXHR3UadQSv5fN6Jx+OO49R+v6ZpuvuoM2il8d8mx3Hc+uM41Bk4Tjabdf8N2uoo6odX6g5d+zyiviZWnWmabqsEzo5isaipqSn3fSwWc9dM26mO7HcfTpfG1oX6+0bUGdQlEgmlUilJtd9vPp+XRJ1Be7du3Wq5nToDqfZ9JRKJbNt+FPXDS3WHIOUR9ebyRuFwWMVisUslQjdEIhHNzMy4723bllSrCzvVkf3uw+mRy+UUi8WatlFn0IplWSqXywqFQioWi7Jt2w3g1Bm0Ew6HNTQ05HbxGx0dlUSdwc6Oon54qe4QpDyi/oV5q3K5fLwFQdc1fhm+deuWotGoQqHQjnVkv/twOti23bJvOHUGrRSLRYXDYXd8QSaTUS6Xk0SdQXvZbFaSNDg4qGw26/5bRZ3BTo6ifnip7jDZhMe1qyw4/WzbVi6X2zZos9Vxh70PJ8vs7Kzi8fiej6fOnG3lclmWZbl/pInH47py5Yocx2l7DnUGhUJBqVRKlmUpkUhIktLpdNvjqTPYyVHUj27UHVqkPCIUCm1L0vWuFzibksmk8vm8Wwd2qiP73YeTr1Ao6MaNGy33UWfQimma7u9ZkvtcLBapM2jJsizNzc0pGo0qHo+rVCppdnZWlmVRZ7Cjo6gfXqo7BCmPqE87u9Xw8PAxlwReMD09rWQyKdM0Zdu2bNvesY7sdx9Oh9nZWWUyGWUyGVmWpampKRWLReoMWmqckGQr6gxaKRaLGhkZcd+bpqnJyUn+bcKujqJ+eKnu0LXPI7b+w2ZZloaHh/nLzBmUy+UUiUTcEFXvtrW1LjTWkf3uw8m39R+URCKhRCLR8ssydQZS7d+b4eFhd2xdfbbHdjNuUWcQiUSUTqebxvDevXuXOoOWGsft7vT99jR8rzGcnTpF41hZlqV0Oq2RkRHNzc01LaqJs8GyLA0ODjZtC4VCWlxcdPe3qyP73YfTwbZtZTIZJZNJxeNxJRIJRSIR6gxasm1byWRSQ0NDWlhYcFvAJf47g9YKhYLb/VOq/RGHOoO6QqGgfD6v6elpTUxMaGRkxA3eR1E/vFJ3CFIAAAAA0CHGSAEAAABAhwhSAAAAANAhghQAAAAAdIggBQAAAAAdIkgBAAAAQIcIUgAAAADQIYIUAMAzCoWCEomEDMNQMplUoVDoWlmGhoaUy+W69vkAAG9jHSkAgKfUF6ZeXFxsWmDRtu1jXXCxUChoeHiYBUIBAC3RIgUA8JRwOLxtm2VZmp2dPdZyRKNRQhQAoC2CFADA81KpVLeLAABAk0C3CwAAwE4KhYLm5+dVLpcl1VqKTNNUoVBQsViUaZqam5tTKpVyx1glk0lJUjqd1sLCgnK5nEKhkCzLUqlUagpmlmUpnU5rZGRE5XJZN27ckGVZunnzphKJhOLxuCSpWCyqUCjINE1ZlqVYLOaWI5lMKpFIuPvy+byy2WzTz7C1rLZta3Z2VqZpyrZtdzsA4GQgSAEAPC0ajSoajWpwcNANNZZlKZlMamFhQZJULpc1PT2tiYkJRaNRLSwsKJ1Ou90Ex8bGVCqVFI1GlUgklMvlFIvFZNu2RkdHtbCwoFAopGQyqUwmo4mJCY2Pj7tlqH9ePp93tw0NDen27dtu+RrDUzabVbFYVCQSaVtWSYpEIopGo+52AMDJQZACAJw49ZDUOKvf3NycJCkUCmlgYECSFIvFJMmduMKyLJXLZVmWJUlui1B9LNTk5GTbz4tEIk3bTNPU7Oys4vG4BgYG3M+sl6EejNqVNZVKaWhoSKZpanx83A2JAICTgSAFADhRbNuW1NyaI6kpiJim2XTO1NSUBgYG3O54jddqnFDiqCaXaFVW27a1uLioYrGoW7duaWxsrKnFCwDgbUw2AQDwlN26uBUKBY2Pj29bY6rxfeM16uOTJiYm3PFI9e2xWEzFYrHtderHtvq8YrGoGzdu7PrztCvr1NSULMtSJBJRKpVihkAAOGFYRwoA4BmFQkHZbLZpnFJ9nFG9K1zjZBP5fF4jIyOSamOp5ufnlUwmFQ6HlUwmFY1GZdu2O3FEXTqd1vj4uGKxWMvr1CebCIfDSqfTLSe3qJetWCzq5s2bkqSZmRl3TFQ9ILUrayaTUSgUUjgcVrlcVjgcdrsiAgC8jyAFAAAAAB2iax8AAAAAdIggBQAAAAAdIkgBAAAAQIcIUgAAAADQIYIUAAAAAHSIIAUAAAAAHSJIAQAAAECHCFIAAAAA0CGCFAAAAAB0iCAFAAAAAB0iSAEAAABAhwhSAAAAANCh/x+X/6vF3IMM2QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Crear un rango de iteraciones para las gráficas\n",
    "k = 0\n",
    "l = int(max_iters_list / 1) + 1\n",
    "\n",
    "zero_1 = np.zeros((N,1))\n",
    "zero_2 = np.zeros((N,M))\n",
    "zero_3 = np.zeros((1,M))\n",
    "zeroo = (zero_1, zero_2, zero_3)\n",
    "\n",
    "# Definir los niveles teóricos según tu solución teórica\n",
    "nivel_teorico_equilibrio = norm_adjusted((zero_1,zero_2,x2.sum(axis=0) - (D - x3)), zeroo, Sigma)\n",
    "nivel_teorico_capacidad = norm_adjusted((zero_1,x1 - x2,zero_3), zeroo, Sigma)\n",
    "nivel_teorico_demanda = norm_adjusted((zero_1,zero_2,D - x3), zeroo, Sigma)\n",
    "\n",
    "# Variable de sufijo para los nombres de archivo\n",
    "suffix = \"_3\"\n",
    "\n",
    "# Función para configurar y guardar cada gráfico\n",
    "def configurar_grafico(ax, x_data, y_data, labels, colors, title, y_label, width, height, y_lim=None, nivel_teorico=None):\n",
    "    for x, y, label, color in zip(x_data, y_data, labels, colors):\n",
    "        ax.plot(x[k:l], y[k:l], '-', linewidth=1.5, label=label, color=color)\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_xlabel('Iteraciones')\n",
    "    ax.legend()\n",
    "    if y_lim:\n",
    "        ax.set_ylim(y_lim)\n",
    "    if nivel_teorico is not None:\n",
    "        ax.axhline(y=nivel_teorico, color='gray', linestyle='--', linewidth=1)\n",
    "    ax.figure.set_figwidth(width)\n",
    "    ax.figure.set_figheight(height)\n",
    "\n",
    "# Datos para los gráficos\n",
    "iter_data = [iter_DY, iter_ADMM, iter_BA]\n",
    "labels = ['TOPS', 'ADMM', 'FPIS']\n",
    "colors = ['green', 'blue', 'red']\n",
    "\n",
    "# Definir la altura y el ancho deseado para cada gráfico\n",
    "width = 10  # Ajusta esta variable según sea necesario\n",
    "height = 3  # Ajusta esta variable según sea necesario\n",
    "\n",
    "# Crear y guardar cada figura individualmente\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "configurar_grafico(ax, iter_data, [x_DY_sol, x_ADMM_sol, x_BA_sol], labels, colors, 'Distancia al Óptimo', \n",
    "                   r'$\\frac{\\|x^{k} - x^{*}\\|}{\\|x^{*}\\|}$', width, height, y_lim=(0, 1))\n",
    "plt.savefig(f'images/caso{suffix}/distancia_al_optimo{suffix}.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "configurar_grafico(ax, iter_data, [Fx_DY_sol, Fx_ADMM_sol, Fx_BA_sol], labels, colors, 'Diferencia Valor Función Objetivo', \n",
    "                   r'$\\|f(x^{k})-f(x^{*})\\|$', width, height, y_lim=(0, 1))\n",
    "plt.savefig(f'images/caso{suffix}/diferencia_valor_funcion_objetivo{suffix}.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "configurar_grafico(ax, iter_data[1:], [Non_anti_DY, Non_anti_ADMM, Non_anti_BA][1:], labels[1:], colors[1:], 'No-Anticipatividad', \n",
    "                   r'$\\|c^{k}-P_{\\mathcal{N}}(c^{k})\\|$', width, height)#, y_lim=(0, 25))\n",
    "plt.savefig(f'images/caso{suffix}/no_anticipatividad{suffix}.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "configurar_grafico(ax, iter_data, [equili_DY_solu, equili_ADMM_solu, equili_BA_solu], labels, colors, 'Restricción de Equilibrio', \n",
    "                   r'$\\|\\textbf{1}^{T}g^{k}-(D-q^{k})\\|$', width, height, y_lim=(-0.1, 1.0), nivel_teorico=nivel_teorico_equilibrio)\n",
    "plt.savefig(f'images/caso{suffix}/restriccion_de_equilibrio{suffix}.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "configurar_grafico(ax, iter_data, [capacity_DY_solu, capacity_ADMM_solu, capacity_BA_solu], labels, colors, 'Restricción de Capacidad de Producción', \n",
    "                   r'$\\|c^{k} - g^{k}\\|$', width, height, nivel_teorico=nivel_teorico_capacidad)\n",
    "plt.savefig(f'images/caso{suffix}/restriccion_de_capacidad_de_produccion{suffix}.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "configurar_grafico(ax, iter_data, [demand_DY_sol, demand_ADMM_sol, demand_BA_sol], labels, colors, 'Diferencia de Demanda Satisfecha', \n",
    "                   r'$\\|D-q^{k}\\|$', width, height, y_lim=(0, 4000), nivel_teorico=nivel_teorico_demanda)\n",
    "plt.savefig(f'images/caso{suffix}/diferencia_de_demanda_satisfecha{suffix}.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "configurar_grafico(ax, iter_data, [dual_DY_sol, dual_ADMM_sol, BA_dual_sol], labels, colors, 'Diferencia al Precio Óptimo', \n",
    "                   r'$\\frac{\\|\\rho^{k}-\\rho^{*}\\|}{\\|\\rho^{*}\\|}$', width, height, y_lim=(0, 1))\n",
    "plt.savefig(f'images/caso{suffix}/diferencia_al_precio_optimo{suffix}.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0041b318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CP': 0.07350301742553711,\n",
       " 'DY': 1221.2837600708008,\n",
       " 'BA': 639.2064185142517,\n",
       " 'ADMM': 590.0728597640991}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e36f3d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9999,\n",
       " 0.0017463588316155116,\n",
       " 6.464275022264534e-05,\n",
       " 0.0,\n",
       " 1.3737724853769713e-05,\n",
       " 560.4113616876273,\n",
       " 678.3377271662825,\n",
       " 0.004081785799062849)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_DY[-1], x_DY_sol[-1], Fx_DY_sol[-1], Non_anti_DY[-1], equili_DY_solu[-1], capacity_DY_solu[-1], demand_DY_sol[-1], dual_DY_sol[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb25dc82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9999,\n",
       " 0.025297524525941043,\n",
       " 0.0008506170720444077,\n",
       " 0.0,\n",
       " 1.3738394881800364e-05,\n",
       " 573.2305208441029,\n",
       " 679.5783041766642,\n",
       " 0.00019198954744443415)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_BA[-1], x_BA_sol[-1], Fx_BA_sol[-1], Non_anti_BA[-1], equili_BA_solu[-1], capacity_BA_solu[-1], demand_BA_sol[-1], BA_dual_sol[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1a81cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9999,\n",
       " 6.778335858578146e-07,\n",
       " 3.1113836785228017e-06,\n",
       " 1.5293280691890615e-13,\n",
       " 0.0018299672255976131,\n",
       " 561.9013253367126,\n",
       " 679.434344407426,\n",
       " 1.8575322539374224e-06)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_ADMM[-1], x_ADMM_sol[-1], Fx_ADMM_sol[-1], Non_anti_ADMM[-1], equili_ADMM_solu[-1], capacity_ADMM_solu[-1], demand_ADMM_sol[-1], dual_ADMM_sol[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30c264f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.3785159814039763e-05, 561.8996186150616, 679.434344407606)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nivel_teorico_equilibrio, nivel_teorico_capacidad, nivel_teorico_demanda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c622cb5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0405fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
