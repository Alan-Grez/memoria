{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0227457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import centralized as CP\n",
    "import davisyin as DY\n",
    "import admm as admm\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from numpy import linalg as LA\n",
    "from numpy.linalg import inv\n",
    "import matplotlib as plt\n",
    "from matplotlib import rc\n",
    "# Configura el tipo de letra globalmente\n",
    "rc('font', **{'family': 'serif', 'serif': ['Computer Modern Roman']})\n",
    "rc('text', usetex=True)\n",
    "#plt.rcParams['text.usetex'] = True\n",
    "import matplotlib.pyplot as plt\n",
    "import proyecciones as pro\n",
    "import time\n",
    "import briceno as BA\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.1f}\".format(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa1b9f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.26.4'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96d1d966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2, 0.2, 0.2, 0.2, 0.2]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caso 1: Caso base\n",
    "    \n",
    "# Cambiar criterio de parada por errores relativos.\n",
    "\n",
    "# Seteamos los parámetros:\n",
    "N, M = 3, 5  # Son 3 tecnologías, 5 escenarios\n",
    "\n",
    "# Probabilidades:\n",
    "inv_, mc_, voll_, d_ = [50.0,  1000.0, 10000.0, 1000.0]\n",
    "                       #[10.0, 2000.0, 10000.0, 1000.0]\n",
    "                       #[50.0,  1000.0, 10000.0, 1000.0]\n",
    "Sigma = np.ones((1,M))\n",
    "#Sigma = np.random.rand(1,M)\n",
    "Sigma /= Sigma.sum()\n",
    "Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1895a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      " [[5.0]\n",
      " [50.0]\n",
      " [95.0]]\n",
      "MC\n",
      " [[[1000.0 0.0 0.0]\n",
      "  [0.0 21000.0 0.0]\n",
      "  [0.0 0.0 41000.0]]\n",
      "\n",
      " [[900.0 0.0 0.0]\n",
      "  [0.0 20500.0 0.0]\n",
      "  [0.0 0.0 42000.0]]\n",
      "\n",
      " [[1200.0 0.0 0.0]\n",
      "  [0.0 22000.0 0.0]\n",
      "  [0.0 0.0 39000.0]]\n",
      "\n",
      " [[700.0 0.0 0.0]\n",
      "  [0.0 19500.0 0.0]\n",
      "  [0.0 0.0 44000.0]]\n",
      "\n",
      " [[1400.0 0.0 0.0]\n",
      "  [0.0 23000.0 0.0]\n",
      "  [0.0 0.0 37000.0]]]\n",
      "VOLL\n",
      " 10000.0\n",
      "D\n",
      " [[200.0 750.0 1000.0 1250.0 1500.0]]\n",
      "frobenius_norm: 458.6700171031756\n"
     ]
    }
   ],
   "source": [
    "Times = {}\n",
    "r_ = 1\n",
    "\n",
    "\n",
    "# Parámetros funciones:\n",
    "I    = inv_ * np.ones((N, 1)) + r_*np.array([[-45], [0], [45]])\n",
    "print(\"I\\n\",I)\n",
    "aux  = np.array([1 + r_*20*i for i in range(N)])\n",
    "\n",
    "mc_11 = 100\n",
    "mc_22 = 500\n",
    "mc_33 = 1000\n",
    "MC   = np.array([np.diag(mc_*aux + r_*np.array([((-1)**m)*mc_11*m, ((-1)**m)*mc_22*m, ((-1)**(m+1))*mc_33*m])) for m in range(M)])\n",
    "print(\"MC\\n\",MC)\n",
    "VOLL = voll_\n",
    "print(\"VOLL\\n\",VOLL)\n",
    "D    = d_*np.ones((1,M)) + r_*np.array([-800, -250, 0, 250, 500])[np.newaxis]\n",
    "print(\"D\\n\",D)\n",
    "\n",
    "e_ = 0\n",
    "\n",
    "e1  = e_\n",
    "e2  = e_\n",
    "e31 = e_*1e2/2\n",
    "e32 = e_\n",
    "\n",
    "Q1, B1 = np.zeros((N,N)), I\n",
    "Q2, B2 = 0.01*MC, np.zeros((N,M))\n",
    "Q3, B3 = np.zeros((1,M)), VOLL*np.ones((1,M))\n",
    "\n",
    "\n",
    "frobenius_norm = (e1+e2)*np.sqrt(N)+e31+e32+np.array([LA.norm(np.einsum('i,ikl->ikl',Sigma[0],0.01*MC)[xi], 'fro') for xi in range(M)]).sum()\n",
    "#frobenius_norm = max([e1*np.sqrt(N),e31+e32,e2*np.sqrt(N)+np.array([LA.norm(np.einsum('i,ikl->ikl',Sigma[0],0.01*MC)[xi], 'fro') for xi in range(M)]).sum()])\n",
    "print(\"frobenius_norm:\",frobenius_norm)\n",
    "\n",
    "def Grad_Phi_1(x1, Q_1 = np.zeros((N,N)), B_1 = I, e1 = e1, N = N):\n",
    "       return np.dot(Q_1,x1)+B_1# - e1*np.dot(np.identity(N),np.maximum(-x1,0))\n",
    "\n",
    "\n",
    "def Grad_Phi_2(x2, Q_2 = 0.01*MC, B_2 = np.zeros((N,M)), e2 = e2, N = N, M = M):\n",
    "\n",
    "    return np.einsum('ijk,ki->ji', Q_2, x2)+B_2# - e2*np.einsum('ijk,ki->ji', np.array([np.diag(np.ones(N)) for m in range(M)]), np.maximum(-x2,0))\n",
    "\n",
    "\n",
    "def Grad_Phi_3(x3, Q_3 = np.zeros((1,M)), B_3 = VOLL*np.ones((1,M)), D=D, e31=e31, e32= e32, M = M):\n",
    "    return Q_3*x3+B_3 #- e31*np.dot(np.maximum(-x3,0),np.identity(M)) - e32*np.dot(np.maximum(-D+x3,0),np.identity(M))\n",
    "\n",
    "\n",
    "def Grad_Phi(x1,x2,x3, P = Sigma):\n",
    "    return Grad_Phi_1(x1), P*Grad_Phi_2(x2), P*Grad_Phi_3(x3)\n",
    "\n",
    "def Grad_Phi_NA(x1,x2,x3, P = Sigma):\n",
    "    return P*Grad_Phi_1(x1), P*Grad_Phi_2(x2), P*Grad_Phi_3(x3)\n",
    "\n",
    "def Phi_1(x1, Q_1 = np.zeros((N,N)), B_1 = I, C_1 = 0.0, e1 = e1):\n",
    "    return 0.5*np.einsum('ij,ji -> i', x1.T,np.dot(Q_1,x1))[:,np.newaxis]+np.dot(x1.T, B_1)+C_1 + e1/2*LA.norm(np.maximum(-x1.flatten(),0))**2\n",
    "\n",
    "def Phi_2_xi(x2, Q_2 = 0.01*MC, B_2 = np.zeros((N,M)), C_2 = np.zeros((M, 1))):\n",
    "    return 0.5*np.einsum('ij,ji -> i', x2.T, np.einsum('ijk,ki -> ji', Q_2, x2))[:,np.newaxis]+np.einsum('ij,ji->i',x2.T,B_2)[:,np.newaxis]+C_2 + e2/2*LA.norm(np.maximum(-x2.flatten(),0))**2\n",
    "\n",
    "def Phi_3_xi(x3, Q_3 = np.zeros((1,M)), B_3 = VOLL*np.ones((1,M)), C_3 = -VOLL*D ):\n",
    "    return (0.5*x3*Q_3*x3+B_3*x3+C_3).T + e31/2*LA.norm(np.maximum(-x3.flatten(),0))**2 + e32/2*LA.norm(np.maximum((D-x3).flatten(),0))**2\n",
    "\n",
    "\n",
    "def objective_function(x1, x2, x3, P = Sigma, NA = True):\n",
    "\n",
    "# NA = True, cumple la funcion que si se impuso \n",
    "#      la condición de no anticipatividad para x1\n",
    "#      entonces, Phi_1(x1).shape == (M,1)\n",
    "    if NA:\n",
    "        return np.dot(P, Phi_1(x1) +Phi_2_xi(x2)+Phi_3_xi(x3))\n",
    "    else:\n",
    "        return Phi_1(x1)+ np.dot(P, Phi_2_xi(x2)+Phi_3_xi(x3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4f57c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = time.time()\n",
    "x1, x2, x3, rho, mu = map(np.array, CP.modelo(Sigma, N, M, \\\n",
    "                                              parametros = [I.T[0].tolist(),\\\n",
    "                                                            np.array([mc_*aux + r_*np.array([((-1)**m)*mc_11*m, ((-1)**m)*mc_22*m, ((-1)**(m+1))*mc_33*m]) for m in range(M)]).T.tolist(),\\\n",
    "                                                            VOLL,\\\n",
    "                                                            D[0]] , show = 0))\n",
    "fin = time.time()\n",
    "\n",
    "\n",
    "Times[\"CP\"] = fin - cp\n",
    "\n",
    "x1 = x1[:,np.newaxis]\n",
    "x2 = x2.T\n",
    "x3 = x3[np.newaxis,:][0]\n",
    "rho = rho[np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1265a4e1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primal:\n",
      "[[1188.3]\n",
      " [44.3]\n",
      " [25.7]]\n",
      "[[186.6 704.0 833.3 1188.3 714.3]\n",
      " [8.9 30.9 44.3 42.8 43.5]\n",
      " [4.6 15.1 25.6 19.0 25.7]]\n",
      "[[0.0 0.0 96.7 0.0 716.5]]\n",
      "Dual:\n",
      "[[0.0 0.0 0.0 25.0 0.0]\n",
      " [0.0 0.0 250.0 0.0 0.0]\n",
      " [0.0 0.0 0.0 0.0 475.0]]\n",
      "[[1865.7 6336.1 10000.0 8342.8 10000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"primal:\\n{x1}\\n{x2}\\n{x3}\")\n",
    "print(f\"Dual:\\n{mu}\\n{rho}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c84c5f51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta: 0.002180216632244036\n",
      "Gamma: 0.004319263595931534\n",
      "Lambda_k: 1\n",
      "Iteration: 1 lambda_k: 1 Loss: 0.9458180934095898\n",
      "Iteration: 2 lambda_k: 1 Loss: 0.8658731937796782\n",
      "Iteration: 3 lambda_k: 1 Loss: 0.8294835994119643\n",
      "Iteration: 4 lambda_k: 1 Loss: 0.8386076066020618\n",
      "Iteration: 5 lambda_k: 1 Loss: 0.8298076405565921\n",
      "Iteration: 6 lambda_k: 1 Loss: 0.8185526618231237\n",
      "Iteration: 7 lambda_k: 1 Loss: 0.8113598780208611\n",
      "Iteration: 8 lambda_k: 1 Loss: 0.806304167610319\n",
      "Iteration: 9 lambda_k: 1 Loss: 0.8022859025545978\n",
      "Iteration: 10 lambda_k: 1 Loss: 0.798999821956594\n",
      "Iteration: 11 lambda_k: 1 Loss: 0.7957905225048221\n",
      "Iteration: 12 lambda_k: 1 Loss: 0.7926571299979063\n",
      "Iteration: 13 lambda_k: 1 Loss: 0.7896163032065262\n",
      "Iteration: 14 lambda_k: 1 Loss: 0.7866691336583628\n",
      "Iteration: 15 lambda_k: 1 Loss: 0.7837984237796184\n",
      "Iteration: 16 lambda_k: 1 Loss: 0.7809878371174725\n",
      "Iteration: 17 lambda_k: 1 Loss: 0.7782142692925936\n",
      "Iteration: 18 lambda_k: 1 Loss: 0.7754760395417101\n",
      "Iteration: 19 lambda_k: 1 Loss: 0.7727725198451455\n",
      "Iteration: 20 lambda_k: 1 Loss: 0.7701019749545155\n",
      "Iteration: 21 lambda_k: 1 Loss: 0.7675235604217548\n",
      "Iteration: 22 lambda_k: 1 Loss: 0.7652217192716854\n",
      "Iteration: 23 lambda_k: 1 Loss: 0.7629595380695015\n",
      "Iteration: 24 lambda_k: 1 Loss: 0.7607329386428514\n",
      "Iteration: 25 lambda_k: 1 Loss: 0.7585385041132262\n",
      "Iteration: 26 lambda_k: 1 Loss: 0.7563733138874289\n",
      "Iteration: 27 lambda_k: 1 Loss: 0.7542348383235497\n",
      "Iteration: 28 lambda_k: 1 Loss: 0.7521208670020443\n",
      "Iteration: 29 lambda_k: 1 Loss: 0.750029455873062\n",
      "Iteration: 30 lambda_k: 1 Loss: 0.7479588861461218\n",
      "Iteration: 31 lambda_k: 1 Loss: 0.7459076314952674\n",
      "Iteration: 32 lambda_k: 1 Loss: 0.7438743315139186\n",
      "Iteration: 33 lambda_k: 1 Loss: 0.7418577698982767\n",
      "Iteration: 34 lambda_k: 1 Loss: 0.739856856232004\n",
      "Iteration: 35 lambda_k: 1 Loss: 0.7378706105800529\n",
      "Iteration: 36 lambda_k: 1 Loss: 0.7358981503399047\n",
      "Iteration: 37 lambda_k: 1 Loss: 0.7339386789479371\n",
      "Iteration: 38 lambda_k: 1 Loss: 0.7319914761303519\n",
      "Iteration: 39 lambda_k: 1 Loss: 0.7300558894495764\n",
      "Iteration: 40 lambda_k: 1 Loss: 0.7281313269487317\n",
      "Iteration: 41 lambda_k: 1 Loss: 0.7262172507188166\n",
      "Iteration: 42 lambda_k: 1 Loss: 0.7243131712658386\n",
      "Iteration: 43 lambda_k: 1 Loss: 0.72241864255453\n",
      "Iteration: 44 lambda_k: 1 Loss: 0.7205332576374007\n",
      "Iteration: 45 lambda_k: 1 Loss: 0.7186566447886636\n",
      "Iteration: 46 lambda_k: 1 Loss: 0.7167884640752789\n",
      "Iteration: 47 lambda_k: 1 Loss: 0.7149284043076952\n",
      "Iteration: 48 lambda_k: 1 Loss: 0.7130761803214883\n",
      "Iteration: 49 lambda_k: 1 Loss: 0.7112315305482445\n",
      "Iteration: 50 lambda_k: 1 Loss: 0.709394214840045\n",
      "Iteration: 51 lambda_k: 1 Loss: 0.7075640125169574\n",
      "Iteration: 52 lambda_k: 1 Loss: 0.7057407206111952\n",
      "Iteration: 53 lambda_k: 1 Loss: 0.7039241522852099\n",
      "Iteration: 54 lambda_k: 1 Loss: 0.7021141354040324\n",
      "Iteration: 55 lambda_k: 1 Loss: 0.7003105112448179\n",
      "Iteration: 56 lambda_k: 1 Loss: 0.6985131333283082\n",
      "Iteration: 57 lambda_k: 1 Loss: 0.6967218663602128\n",
      "Iteration: 58 lambda_k: 1 Loss: 0.6949365852695236\n",
      "Iteration: 59 lambda_k: 1 Loss: 0.6931571743357055\n",
      "Iteration: 60 lambda_k: 1 Loss: 0.6913835263932928\n",
      "Iteration: 61 lambda_k: 1 Loss: 0.6896155421091291\n",
      "Iteration: 62 lambda_k: 1 Loss: 0.6878531293229295\n",
      "Iteration: 63 lambda_k: 1 Loss: 0.6860962024460363\n",
      "Iteration: 64 lambda_k: 1 Loss: 0.6843446819127038\n",
      "Iteration: 65 lambda_k: 1 Loss: 0.682598493678981\n",
      "Iteration: 66 lambda_k: 1 Loss: 0.6808575687647929\n",
      "Iteration: 67 lambda_k: 1 Loss: 0.6791218428352788\n",
      "Iteration: 68 lambda_k: 1 Loss: 0.6773912558184543\n",
      "Iteration: 69 lambda_k: 1 Loss: 0.6756657515522251\n",
      "Iteration: 70 lambda_k: 1 Loss: 0.6739452774676832\n",
      "Iteration: 71 lambda_k: 1 Loss: 0.6722297842922504\n",
      "Iteration: 72 lambda_k: 1 Loss: 0.6705192257792196\n",
      "Iteration: 73 lambda_k: 1 Loss: 0.6688135584703027\n",
      "Iteration: 74 lambda_k: 1 Loss: 0.6671127414554715\n",
      "Iteration: 75 lambda_k: 1 Loss: 0.6654167361758916\n",
      "Iteration: 76 lambda_k: 1 Loss: 0.6637255062309765\n",
      "Iteration: 77 lambda_k: 1 Loss: 0.6620390172040667\n",
      "Iteration: 78 lambda_k: 1 Loss: 0.6603572365027027\n",
      "Iteration: 79 lambda_k: 1 Loss: 0.6586801332129706\n",
      "Iteration: 80 lambda_k: 1 Loss: 0.6570076779615777\n",
      "Iteration: 81 lambda_k: 1 Loss: 0.6553398427973273\n",
      "Iteration: 82 lambda_k: 1 Loss: 0.6536766010745626\n",
      "Iteration: 83 lambda_k: 1 Loss: 0.6520179273500751\n",
      "Iteration: 84 lambda_k: 1 Loss: 0.650363797287812\n",
      "Iteration: 85 lambda_k: 1 Loss: 0.6487141875706522\n",
      "Iteration: 86 lambda_k: 1 Loss: 0.6470690758209813\n",
      "Iteration: 87 lambda_k: 1 Loss: 0.645428440526074\n",
      "Iteration: 88 lambda_k: 1 Loss: 0.6437922609705532\n",
      "Iteration: 89 lambda_k: 1 Loss: 0.6421605171741585\n",
      "Iteration: 90 lambda_k: 1 Loss: 0.6405331898345834\n",
      "Iteration: 91 lambda_k: 1 Loss: 0.6389102602749875\n",
      "Iteration: 92 lambda_k: 1 Loss: 0.6372917103958025\n",
      "Iteration: 93 lambda_k: 1 Loss: 0.6356775226304742\n",
      "Iteration: 94 lambda_k: 1 Loss: 0.6340676799048184\n",
      "Iteration: 95 lambda_k: 1 Loss: 0.6324621655996906\n",
      "Iteration: 96 lambda_k: 1 Loss: 0.6308609635167017\n",
      "Iteration: 97 lambda_k: 1 Loss: 0.6292640578467286\n",
      "Iteration: 98 lambda_k: 1 Loss: 0.6276714331409927\n",
      "Iteration: 99 lambda_k: 1 Loss: 0.6260830742844956\n",
      "Iteration: 100 lambda_k: 1 Loss: 0.6244989664716206\n",
      "Iteration: 101 lambda_k: 1 Loss: 0.6229190951837236\n",
      "Iteration: 102 lambda_k: 1 Loss: 0.6213434461625205\n",
      "Iteration: 103 lambda_k: 1 Loss: 0.6197720054152348\n",
      "Iteration: 104 lambda_k: 1 Loss: 0.6182047591596304\n",
      "Iteration: 105 lambda_k: 1 Loss: 0.6166416938371332\n",
      "Iteration: 106 lambda_k: 1 Loss: 0.615082796083976\n",
      "Iteration: 107 lambda_k: 1 Loss: 0.6135280527265896\n",
      "Iteration: 108 lambda_k: 1 Loss: 0.6119774507629385\n",
      "Iteration: 109 lambda_k: 1 Loss: 0.610430977353282\n",
      "Iteration: 110 lambda_k: 1 Loss: 0.6088886198097799\n",
      "Iteration: 111 lambda_k: 1 Loss: 0.6073503655867919\n",
      "Iteration: 112 lambda_k: 1 Loss: 0.6058162022719974\n",
      "Iteration: 113 lambda_k: 1 Loss: 0.6042861175783232\n",
      "Iteration: 114 lambda_k: 1 Loss: 0.6027600993366223\n",
      "Iteration: 115 lambda_k: 1 Loss: 0.6012381354827027\n",
      "Iteration: 116 lambda_k: 1 Loss: 0.5997202140755837\n",
      "Iteration: 117 lambda_k: 1 Loss: 0.5982063232485186\n",
      "Iteration: 118 lambda_k: 1 Loss: 0.5966964512438382\n",
      "Iteration: 119 lambda_k: 1 Loss: 0.5951905863888549\n",
      "Iteration: 120 lambda_k: 1 Loss: 0.5936887170959718\n",
      "Iteration: 121 lambda_k: 1 Loss: 0.592190831865112\n",
      "Iteration: 122 lambda_k: 1 Loss: 0.5906969192155052\n",
      "Iteration: 123 lambda_k: 1 Loss: 0.5892069678583663\n",
      "Iteration: 124 lambda_k: 1 Loss: 0.5877209664458019\n",
      "Iteration: 125 lambda_k: 1 Loss: 0.58623890382072\n",
      "Iteration: 126 lambda_k: 1 Loss: 0.5847607879274601\n",
      "Iteration: 127 lambda_k: 1 Loss: 0.5832866036920553\n",
      "Iteration: 128 lambda_k: 1 Loss: 0.581816331426727\n",
      "Iteration: 129 lambda_k: 1 Loss: 0.5803499623013922\n",
      "Iteration: 130 lambda_k: 1 Loss: 0.5788874824975598\n",
      "Iteration: 131 lambda_k: 1 Loss: 0.5774288778152797\n",
      "Iteration: 132 lambda_k: 1 Loss: 0.5759741341190844\n",
      "Iteration: 133 lambda_k: 1 Loss: 0.5745232375145217\n",
      "Iteration: 134 lambda_k: 1 Loss: 0.573076174309969\n",
      "Iteration: 135 lambda_k: 1 Loss: 0.571632930950875\n",
      "Iteration: 136 lambda_k: 1 Loss: 0.5701934940292743\n",
      "Iteration: 137 lambda_k: 1 Loss: 0.5687578502536412\n",
      "Iteration: 138 lambda_k: 1 Loss: 0.5673259864539366\n",
      "Iteration: 139 lambda_k: 1 Loss: 0.5658978895609986\n",
      "Iteration: 140 lambda_k: 1 Loss: 0.5644735466134839\n",
      "Iteration: 141 lambda_k: 1 Loss: 0.5630529447431605\n",
      "Iteration: 142 lambda_k: 1 Loss: 0.5616360711732353\n",
      "Iteration: 143 lambda_k: 1 Loss: 0.5602229132135506\n",
      "Iteration: 144 lambda_k: 1 Loss: 0.5588134582569159\n",
      "Iteration: 145 lambda_k: 1 Loss: 0.5574076937758505\n",
      "Iteration: 146 lambda_k: 1 Loss: 0.5560056073197215\n",
      "Iteration: 147 lambda_k: 1 Loss: 0.5546071865122223\n",
      "Iteration: 148 lambda_k: 1 Loss: 0.5532124190491372\n",
      "Iteration: 149 lambda_k: 1 Loss: 0.5518212926963548\n",
      "Iteration: 150 lambda_k: 1 Loss: 0.5504337952880949\n",
      "Iteration: 151 lambda_k: 1 Loss: 0.5490499147253258\n",
      "Iteration: 152 lambda_k: 1 Loss: 0.5476696389743441\n",
      "Iteration: 153 lambda_k: 1 Loss: 0.5462929560655012\n",
      "Iteration: 154 lambda_k: 1 Loss: 0.5449198540920556\n",
      "Iteration: 155 lambda_k: 1 Loss: 0.5435503212091378\n",
      "Iteration: 156 lambda_k: 1 Loss: 0.5421843456328157\n",
      "Iteration: 157 lambda_k: 1 Loss: 0.5408219156392459\n",
      "Iteration: 158 lambda_k: 1 Loss: 0.5394630195639056\n",
      "Iteration: 159 lambda_k: 1 Loss: 0.538107645800894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 160 lambda_k: 1 Loss: 0.5367557828022943\n",
      "Iteration: 161 lambda_k: 1 Loss: 0.5354074190775947\n",
      "Iteration: 162 lambda_k: 1 Loss: 0.5340625431931559\n",
      "Iteration: 163 lambda_k: 1 Loss: 0.532721143771727\n",
      "Iteration: 164 lambda_k: 1 Loss: 0.5313832094919748\n",
      "Iteration: 165 lambda_k: 1 Loss: 0.530048729088173\n",
      "Iteration: 166 lambda_k: 1 Loss: 0.5287176913496899\n",
      "Iteration: 167 lambda_k: 1 Loss: 0.5273900851207222\n",
      "Iteration: 168 lambda_k: 1 Loss: 0.5260658992999756\n",
      "Iteration: 169 lambda_k: 1 Loss: 0.5247451228403629\n",
      "Iteration: 170 lambda_k: 1 Loss: 0.5234277447487365\n",
      "Iteration: 171 lambda_k: 1 Loss: 0.5221137540856451\n",
      "Iteration: 172 lambda_k: 1 Loss: 0.5208031399651074\n",
      "Iteration: 173 lambda_k: 1 Loss: 0.5194958915544067\n",
      "Iteration: 174 lambda_k: 1 Loss: 0.5181919980739014\n",
      "Iteration: 175 lambda_k: 1 Loss: 0.5168914487968533\n",
      "Iteration: 176 lambda_k: 1 Loss: 0.5155942330492725\n",
      "Iteration: 177 lambda_k: 1 Loss: 0.5143003402097753\n",
      "Iteration: 178 lambda_k: 1 Loss: 0.5130097597094606\n",
      "Iteration: 179 lambda_k: 1 Loss: 0.5117224810317965\n",
      "Iteration: 180 lambda_k: 1 Loss: 0.5104384937125246\n",
      "Iteration: 181 lambda_k: 1 Loss: 0.5091577873395746\n",
      "Iteration: 182 lambda_k: 1 Loss: 0.5078803515529956\n",
      "Iteration: 183 lambda_k: 1 Loss: 0.5066061760448973\n",
      "Iteration: 184 lambda_k: 1 Loss: 0.5053352505594086\n",
      "Iteration: 185 lambda_k: 1 Loss: 0.5040675648926459\n",
      "Iteration: 186 lambda_k: 1 Loss: 0.5028031088926969\n",
      "Iteration: 187 lambda_k: 1 Loss: 0.501541872459618\n",
      "Iteration: 188 lambda_k: 1 Loss: 0.5002838455454446\n",
      "Iteration: 189 lambda_k: 1 Loss: 0.49902901815421735\n",
      "Iteration: 190 lambda_k: 1 Loss: 0.49777738034202035\n",
      "Iteration: 191 lambda_k: 1 Loss: 0.4965289222170371\n",
      "Iteration: 192 lambda_k: 1 Loss: 0.49528363393962005\n",
      "Iteration: 193 lambda_k: 1 Loss: 0.4940415057223762\n",
      "Iteration: 194 lambda_k: 1 Loss: 0.4928025278302705\n",
      "Iteration: 195 lambda_k: 1 Loss: 0.49156669058074426\n",
      "Iteration: 196 lambda_k: 1 Loss: 0.4903339843438535\n",
      "Iteration: 197 lambda_k: 1 Loss: 0.48910439954242463\n",
      "Iteration: 198 lambda_k: 1 Loss: 0.48787792665223\n",
      "Iteration: 199 lambda_k: 1 Loss: 0.4866545562021829\n",
      "Iteration: 200 lambda_k: 1 Loss: 0.48543427877455536\n",
      "Iteration: 201 lambda_k: 1 Loss: 0.4842170850052171\n",
      "Iteration: 202 lambda_k: 1 Loss: 0.4830029655838981\n",
      "Iteration: 203 lambda_k: 1 Loss: 0.4817919112544753\n",
      "Iteration: 204 lambda_k: 1 Loss: 0.4805839128152869\n",
      "Iteration: 205 lambda_k: 1 Loss: 0.479378961120588\n",
      "Iteration: 206 lambda_k: 1 Loss: 0.47817704707626435\n",
      "Iteration: 207 lambda_k: 1 Loss: 0.47697816164769974\n",
      "Iteration: 208 lambda_k: 1 Loss: 0.4757822958547444\n",
      "Iteration: 209 lambda_k: 1 Loss: 0.4745894407691616\n",
      "Iteration: 210 lambda_k: 1 Loss: 0.4733995875332799\n",
      "Iteration: 211 lambda_k: 1 Loss: 0.4722127273328088\n",
      "Iteration: 212 lambda_k: 1 Loss: 0.471028851418477\n",
      "Iteration: 213 lambda_k: 1 Loss: 0.46984795109501026\n",
      "Iteration: 214 lambda_k: 1 Loss: 0.4686700177311711\n",
      "Iteration: 215 lambda_k: 1 Loss: 0.4674950427540111\n",
      "Iteration: 216 lambda_k: 1 Loss: 0.46632301765121154\n",
      "Iteration: 217 lambda_k: 1 Loss: 0.4651539339722917\n",
      "Iteration: 218 lambda_k: 1 Loss: 0.46398778332942997\n",
      "Iteration: 219 lambda_k: 1 Loss: 0.4628245573982812\n",
      "Iteration: 220 lambda_k: 1 Loss: 0.461664247918958\n",
      "Iteration: 221 lambda_k: 1 Loss: 0.4605068466971234\n",
      "Iteration: 222 lambda_k: 1 Loss: 0.4593523456051501\n",
      "Iteration: 223 lambda_k: 1 Loss: 0.458200736583345\n",
      "Iteration: 224 lambda_k: 1 Loss: 0.4570520116412577\n",
      "Iteration: 225 lambda_k: 1 Loss: 0.45590616285907726\n",
      "Iteration: 226 lambda_k: 1 Loss: 0.45476318238912933\n",
      "Iteration: 227 lambda_k: 1 Loss: 0.45362306245747414\n",
      "Iteration: 228 lambda_k: 1 Loss: 0.45248579536561634\n",
      "Iteration: 229 lambda_k: 1 Loss: 0.45135137349233256\n",
      "Iteration: 230 lambda_k: 1 Loss: 0.45021978928791645\n",
      "Iteration: 231 lambda_k: 1 Loss: 0.4490910353080198\n",
      "Iteration: 232 lambda_k: 1 Loss: 0.44796510416547064\n",
      "Iteration: 233 lambda_k: 1 Loss: 0.4468419885691\n",
      "Iteration: 234 lambda_k: 1 Loss: 0.44572168131740597\n",
      "Iteration: 235 lambda_k: 1 Loss: 0.4446041752991888\n",
      "Iteration: 236 lambda_k: 1 Loss: 0.44348946349888074\n",
      "Iteration: 237 lambda_k: 1 Loss: 0.4423775389922919\n",
      "Iteration: 238 lambda_k: 1 Loss: 0.4412683949619913\n",
      "Iteration: 239 lambda_k: 1 Loss: 0.4401620246930328\n",
      "Iteration: 240 lambda_k: 1 Loss: 0.43905842157826247\n",
      "Iteration: 241 lambda_k: 1 Loss: 0.43795757912297395\n",
      "Iteration: 242 lambda_k: 1 Loss: 0.4368594909493692\n",
      "Iteration: 243 lambda_k: 1 Loss: 0.43576415080123343\n",
      "Iteration: 244 lambda_k: 1 Loss: 0.43467155254906675\n",
      "Iteration: 245 lambda_k: 1 Loss: 0.43358169019565485\n",
      "Iteration: 246 lambda_k: 1 Loss: 0.43249455788205793\n",
      "Iteration: 247 lambda_k: 1 Loss: 0.43141014989404947\n",
      "Iteration: 248 lambda_k: 1 Loss: 0.43032846066905556\n",
      "Iteration: 249 lambda_k: 1 Loss: 0.4292494848036481\n",
      "Iteration: 250 lambda_k: 1 Loss: 0.42817321706164857\n",
      "Iteration: 251 lambda_k: 1 Loss: 0.4270996523829073\n",
      "Iteration: 252 lambda_k: 1 Loss: 0.42602878588253473\n",
      "Iteration: 253 lambda_k: 1 Loss: 0.42496061289933557\n",
      "Iteration: 254 lambda_k: 1 Loss: 0.4238951289487632\n",
      "Iteration: 255 lambda_k: 1 Loss: 0.4228323297757153\n",
      "Iteration: 256 lambda_k: 1 Loss: 0.42177221135811394\n",
      "Iteration: 257 lambda_k: 1 Loss: 0.42071476991818724\n",
      "Iteration: 258 lambda_k: 1 Loss: 0.4196600019381347\n",
      "Iteration: 259 lambda_k: 1 Loss: 0.4186079041778628\n",
      "Iteration: 260 lambda_k: 1 Loss: 0.41755847369445726\n",
      "Iteration: 261 lambda_k: 1 Loss: 0.41651170781853825\n",
      "Iteration: 262 lambda_k: 1 Loss: 0.41546760432319946\n",
      "Iteration: 263 lambda_k: 1 Loss: 0.4144261612429954\n",
      "Iteration: 264 lambda_k: 1 Loss: 0.4133873770065174\n",
      "Iteration: 265 lambda_k: 1 Loss: 0.41235125054471056\n",
      "Iteration: 266 lambda_k: 1 Loss: 0.4113177809420749\n",
      "Iteration: 267 lambda_k: 1 Loss: 0.41028696825016037\n",
      "Iteration: 268 lambda_k: 1 Loss: 0.4092588206294453\n",
      "Iteration: 269 lambda_k: 1 Loss: 0.40823335061702765\n",
      "Iteration: 270 lambda_k: 1 Loss: 0.4072105399706497\n",
      "Iteration: 271 lambda_k: 1 Loss: 0.4061903954674806\n",
      "Iteration: 272 lambda_k: 1 Loss: 0.40517292130412574\n",
      "Iteration: 273 lambda_k: 1 Loss: 0.40415811985659844\n",
      "Iteration: 274 lambda_k: 1 Loss: 0.4031459936961058\n",
      "Iteration: 275 lambda_k: 1 Loss: 0.40213654639531154\n",
      "Iteration: 276 lambda_k: 1 Loss: 0.4011297828266984\n",
      "Iteration: 277 lambda_k: 1 Loss: 0.4001257090235993\n",
      "Iteration: 278 lambda_k: 1 Loss: 0.39912433228252003\n",
      "Iteration: 279 lambda_k: 1 Loss: 0.39812566127386134\n",
      "Iteration: 280 lambda_k: 1 Loss: 0.3971297061427233\n",
      "Iteration: 281 lambda_k: 1 Loss: 0.3961364786873026\n",
      "Iteration: 282 lambda_k: 1 Loss: 0.3951459924864064\n",
      "Iteration: 283 lambda_k: 1 Loss: 0.39415826308799307\n",
      "Iteration: 284 lambda_k: 1 Loss: 0.39317330817996404\n",
      "Iteration: 285 lambda_k: 1 Loss: 0.3921911478351062\n",
      "Iteration: 286 lambda_k: 1 Loss: 0.3912118047255312\n",
      "Iteration: 287 lambda_k: 1 Loss: 0.3902353043971392\n",
      "Iteration: 288 lambda_k: 1 Loss: 0.3892616755631067\n",
      "Iteration: 289 lambda_k: 1 Loss: 0.3882909504297652\n",
      "Iteration: 290 lambda_k: 1 Loss: 0.3873231650569329\n",
      "Iteration: 291 lambda_k: 1 Loss: 0.38635835975532123\n",
      "Iteration: 292 lambda_k: 1 Loss: 0.3853965795220757\n",
      "Iteration: 293 lambda_k: 1 Loss: 0.38443787452297246\n",
      "Iteration: 294 lambda_k: 1 Loss: 0.3834823006020976\n",
      "Iteration: 295 lambda_k: 1 Loss: 0.38252991988265367\n",
      "Iteration: 296 lambda_k: 1 Loss: 0.3816502017691508\n",
      "Iteration: 297 lambda_k: 1 Loss: 0.38079117726959205\n",
      "Iteration: 298 lambda_k: 1 Loss: 0.37994283841698767\n",
      "Iteration: 299 lambda_k: 1 Loss: 0.3791049849537397\n",
      "Iteration: 300 lambda_k: 1 Loss: 0.37827702639456107\n",
      "Iteration: 301 lambda_k: 1 Loss: 0.3774585971972489\n",
      "Iteration: 302 lambda_k: 1 Loss: 0.3766495261331491\n",
      "Iteration: 303 lambda_k: 1 Loss: 0.37584970037881144\n",
      "Iteration: 304 lambda_k: 1 Loss: 0.3750590017902991\n",
      "Iteration: 305 lambda_k: 1 Loss: 0.37427730090617106\n",
      "Iteration: 306 lambda_k: 1 Loss: 0.37350446638259743\n",
      "Iteration: 307 lambda_k: 1 Loss: 0.3727403706686226\n",
      "Iteration: 308 lambda_k: 1 Loss: 0.3719848906401877\n",
      "Iteration: 309 lambda_k: 1 Loss: 0.3712379065557846\n",
      "Iteration: 310 lambda_k: 1 Loss: 0.37049930125120006\n",
      "Iteration: 311 lambda_k: 1 Loss: 0.3697689598585576\n",
      "Iteration: 312 lambda_k: 1 Loss: 0.36904676989210267\n",
      "Iteration: 313 lambda_k: 1 Loss: 0.3683326212484718\n",
      "Iteration: 314 lambda_k: 1 Loss: 0.3676255943650864\n",
      "Iteration: 315 lambda_k: 1 Loss: 0.3669254818139925\n",
      "Iteration: 316 lambda_k: 1 Loss: 0.3662315382837981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 317 lambda_k: 1 Loss: 0.3655433628960216\n",
      "Iteration: 318 lambda_k: 1 Loss: 0.3648606055876625\n",
      "Iteration: 319 lambda_k: 1 Loss: 0.3641829424733414\n",
      "Iteration: 320 lambda_k: 1 Loss: 0.3635100752563182\n",
      "Iteration: 321 lambda_k: 1 Loss: 0.3628417295684364\n",
      "Iteration: 322 lambda_k: 1 Loss: 0.3621776528756251\n",
      "Iteration: 323 lambda_k: 1 Loss: 0.36151761253303466\n",
      "Iteration: 324 lambda_k: 1 Loss: 0.3608613941318136\n",
      "Iteration: 325 lambda_k: 1 Loss: 0.3602088550908019\n",
      "Iteration: 326 lambda_k: 1 Loss: 0.35955991581964847\n",
      "Iteration: 327 lambda_k: 1 Loss: 0.3589142128069257\n",
      "Iteration: 328 lambda_k: 1 Loss: 0.3582715972645974\n",
      "Iteration: 329 lambda_k: 1 Loss: 0.3576319288691434\n",
      "Iteration: 330 lambda_k: 1 Loss: 0.356995074495328\n",
      "Iteration: 331 lambda_k: 1 Loss: 0.3563609112321692\n",
      "Iteration: 332 lambda_k: 1 Loss: 0.3557293272170159\n",
      "Iteration: 333 lambda_k: 1 Loss: 0.3551002201494715\n",
      "Iteration: 334 lambda_k: 1 Loss: 0.3544734957023179\n",
      "Iteration: 335 lambda_k: 1 Loss: 0.3538490666611202\n",
      "Iteration: 336 lambda_k: 1 Loss: 0.35322685245481233\n",
      "Iteration: 337 lambda_k: 1 Loss: 0.3526067787210964\n",
      "Iteration: 338 lambda_k: 1 Loss: 0.3519887768312529\n",
      "Iteration: 339 lambda_k: 1 Loss: 0.35137278342226225\n",
      "Iteration: 340 lambda_k: 1 Loss: 0.35075873986236156\n",
      "Iteration: 341 lambda_k: 1 Loss: 0.3501465921980256\n",
      "Iteration: 342 lambda_k: 1 Loss: 0.34953629033299244\n",
      "Iteration: 343 lambda_k: 1 Loss: 0.34892778807487773\n",
      "Iteration: 344 lambda_k: 1 Loss: 0.348321042427611\n",
      "Iteration: 345 lambda_k: 1 Loss: 0.34771601473082453\n",
      "Iteration: 346 lambda_k: 1 Loss: 0.3471126748146842\n",
      "Iteration: 347 lambda_k: 1 Loss: 0.3465109956279644\n",
      "Iteration: 348 lambda_k: 1 Loss: 0.3459109405137897\n",
      "Iteration: 349 lambda_k: 1 Loss: 0.3453124807384692\n",
      "Iteration: 350 lambda_k: 1 Loss: 0.34471558759454973\n",
      "Iteration: 351 lambda_k: 1 Loss: 0.34412023425371185\n",
      "Iteration: 352 lambda_k: 1 Loss: 0.34352639557187825\n",
      "Iteration: 353 lambda_k: 1 Loss: 0.34293404806705\n",
      "Iteration: 354 lambda_k: 1 Loss: 0.3423431698341882\n",
      "Iteration: 355 lambda_k: 1 Loss: 0.3417537403726223\n",
      "Iteration: 356 lambda_k: 1 Loss: 0.34116574049916015\n",
      "Iteration: 357 lambda_k: 1 Loss: 0.3405791522152055\n",
      "Iteration: 358 lambda_k: 1 Loss: 0.3399939586378752\n",
      "Iteration: 359 lambda_k: 1 Loss: 0.339410143897396\n",
      "Iteration: 360 lambda_k: 1 Loss: 0.33882769306876115\n",
      "Iteration: 361 lambda_k: 1 Loss: 0.3382465920917972\n",
      "Iteration: 362 lambda_k: 1 Loss: 0.33766682769511275\n",
      "Iteration: 363 lambda_k: 1 Loss: 0.3370883873530296\n",
      "Iteration: 364 lambda_k: 1 Loss: 0.33651125921375924\n",
      "Iteration: 365 lambda_k: 1 Loss: 0.33593543205259085\n",
      "Iteration: 366 lambda_k: 1 Loss: 0.3353608952223102\n",
      "Iteration: 367 lambda_k: 1 Loss: 0.334787638607924\n",
      "Iteration: 368 lambda_k: 1 Loss: 0.33421565258989955\n",
      "Iteration: 369 lambda_k: 1 Loss: 0.3336449280035889\n",
      "Iteration: 370 lambda_k: 1 Loss: 0.3330754560984322\n",
      "Iteration: 371 lambda_k: 1 Loss: 0.33250722851887654\n",
      "Iteration: 372 lambda_k: 1 Loss: 0.3319402372658229\n",
      "Iteration: 373 lambda_k: 1 Loss: 0.33137447467319786\n",
      "Iteration: 374 lambda_k: 1 Loss: 0.33080993338247383\n",
      "Iteration: 375 lambda_k: 1 Loss: 0.3302466063200889\n",
      "Iteration: 376 lambda_k: 1 Loss: 0.3296844866762696\n",
      "Iteration: 377 lambda_k: 1 Loss: 0.32912356788592784\n",
      "Iteration: 378 lambda_k: 1 Loss: 0.32856384361086866\n",
      "Iteration: 379 lambda_k: 1 Loss: 0.3280053077234861\n",
      "Iteration: 380 lambda_k: 1 Loss: 0.32744795429174733\n",
      "Iteration: 381 lambda_k: 1 Loss: 0.3268917775660329\n",
      "Iteration: 382 lambda_k: 1 Loss: 0.32633677196384847\n",
      "Iteration: 383 lambda_k: 1 Loss: 0.32578293206194786\n",
      "Iteration: 384 lambda_k: 1 Loss: 0.3252302525828207\n",
      "Iteration: 385 lambda_k: 1 Loss: 0.3246787283857694\n",
      "Iteration: 386 lambda_k: 1 Loss: 0.3241283544575963\n",
      "Iteration: 387 lambda_k: 1 Loss: 0.3235791259041992\n",
      "Iteration: 388 lambda_k: 1 Loss: 0.3230310379428294\n",
      "Iteration: 389 lambda_k: 1 Loss: 0.3224840858949664\n",
      "Iteration: 390 lambda_k: 1 Loss: 0.3219382651795157\n",
      "Iteration: 391 lambda_k: 1 Loss: 0.32139357130754187\n",
      "Iteration: 392 lambda_k: 1 Loss: 0.32084999987569895\n",
      "Iteration: 393 lambda_k: 1 Loss: 0.32030754656173493\n",
      "Iteration: 394 lambda_k: 1 Loss: 0.31976620711955145\n",
      "Iteration: 395 lambda_k: 1 Loss: 0.31922597737487785\n",
      "Iteration: 396 lambda_k: 1 Loss: 0.3186868532212644\n",
      "Iteration: 397 lambda_k: 1 Loss: 0.3181488306163768\n",
      "Iteration: 398 lambda_k: 1 Loss: 0.317611905578586\n",
      "Iteration: 399 lambda_k: 1 Loss: 0.31707607418382233\n",
      "Iteration: 400 lambda_k: 1 Loss: 0.3165413325626733\n",
      "Iteration: 401 lambda_k: 1 Loss: 0.31600767689770715\n",
      "Iteration: 402 lambda_k: 1 Loss: 0.31547510342100255\n",
      "Iteration: 403 lambda_k: 1 Loss: 0.31494360841187063\n",
      "Iteration: 404 lambda_k: 1 Loss: 0.314413188194749\n",
      "Iteration: 405 lambda_k: 1 Loss: 0.31388383913726\n",
      "Iteration: 406 lambda_k: 1 Loss: 0.3133555576484154\n",
      "Iteration: 407 lambda_k: 1 Loss: 0.31282834017695826\n",
      "Iteration: 408 lambda_k: 1 Loss: 0.3123021832098306\n",
      "Iteration: 409 lambda_k: 1 Loss: 0.3117770832707557\n",
      "Iteration: 410 lambda_k: 1 Loss: 0.3112530369189278\n",
      "Iteration: 411 lambda_k: 1 Loss: 0.3107300407477992\n",
      "Iteration: 412 lambda_k: 1 Loss: 0.3102080913839566\n",
      "Iteration: 413 lambda_k: 1 Loss: 0.3096871854860818\n",
      "Iteration: 414 lambda_k: 1 Loss: 0.30916731974399697\n",
      "Iteration: 415 lambda_k: 1 Loss: 0.30864849087773016\n",
      "Iteration: 416 lambda_k: 1 Loss: 0.3081306956367584\n",
      "Iteration: 417 lambda_k: 1 Loss: 0.30761393079917465\n",
      "Iteration: 418 lambda_k: 1 Loss: 0.3070981931710062\n",
      "Iteration: 419 lambda_k: 1 Loss: 0.3065834795855331\n",
      "Iteration: 420 lambda_k: 1 Loss: 0.3060697869026826\n",
      "Iteration: 421 lambda_k: 1 Loss: 0.3055571120084493\n",
      "Iteration: 422 lambda_k: 1 Loss: 0.3050454518143599\n",
      "Iteration: 423 lambda_k: 1 Loss: 0.30453480325697324\n",
      "Iteration: 424 lambda_k: 1 Loss: 0.3040251632974147\n",
      "Iteration: 425 lambda_k: 1 Loss: 0.30351652892094116\n",
      "Iteration: 426 lambda_k: 1 Loss: 0.30300889713653384\n",
      "Iteration: 427 lambda_k: 1 Loss: 0.3025022649765175\n",
      "Iteration: 428 lambda_k: 1 Loss: 0.30199662949620376\n",
      "Iteration: 429 lambda_k: 1 Loss: 0.3014919877735563\n",
      "Iteration: 430 lambda_k: 1 Loss: 0.3009883369088768\n",
      "Iteration: 431 lambda_k: 1 Loss: 0.3004856740245098\n",
      "Iteration: 432 lambda_k: 1 Loss: 0.2999839962645643\n",
      "Iteration: 433 lambda_k: 1 Loss: 0.299483300794652\n",
      "Iteration: 434 lambda_k: 1 Loss: 0.2989835848016404\n",
      "Iteration: 435 lambda_k: 1 Loss: 0.29848484549341864\n",
      "Iteration: 436 lambda_k: 1 Loss: 0.29798708009867725\n",
      "Iteration: 437 lambda_k: 1 Loss: 0.297490285866698\n",
      "Iteration: 438 lambda_k: 1 Loss: 0.29699446006715585\n",
      "Iteration: 439 lambda_k: 1 Loss: 0.2964995999899297\n",
      "Iteration: 440 lambda_k: 1 Loss: 0.29600570294492307\n",
      "Iteration: 441 lambda_k: 1 Loss: 0.2955127662618929\n",
      "Iteration: 442 lambda_k: 1 Loss: 0.2950207872902859\n",
      "Iteration: 443 lambda_k: 1 Loss: 0.29452976339908266\n",
      "Iteration: 444 lambda_k: 1 Loss: 0.2940396919766478\n",
      "Iteration: 445 lambda_k: 1 Loss: 0.29355057043058724\n",
      "Iteration: 446 lambda_k: 1 Loss: 0.2930623961876099\n",
      "Iteration: 447 lambda_k: 1 Loss: 0.29257516669339595\n",
      "Iteration: 448 lambda_k: 1 Loss: 0.2920888794124691\n",
      "Iteration: 449 lambda_k: 1 Loss: 0.2916035318280738\n",
      "Iteration: 450 lambda_k: 1 Loss: 0.2911191214420567\n",
      "Iteration: 451 lambda_k: 1 Loss: 0.29063564577475187\n",
      "Iteration: 452 lambda_k: 1 Loss: 0.2901531023648698\n",
      "Iteration: 453 lambda_k: 1 Loss: 0.2896714887693896\n",
      "Iteration: 454 lambda_k: 1 Loss: 0.2891908025634546\n",
      "Iteration: 455 lambda_k: 1 Loss: 0.28871104134027054\n",
      "Iteration: 456 lambda_k: 1 Loss: 0.28823220271100686\n",
      "Iteration: 457 lambda_k: 1 Loss: 0.2877542843047004\n",
      "Iteration: 458 lambda_k: 1 Loss: 0.28727728376816136\n",
      "Iteration: 459 lambda_k: 1 Loss: 0.2868011987658811\n",
      "Iteration: 460 lambda_k: 1 Loss: 0.2863260269799432\n",
      "Iteration: 461 lambda_k: 1 Loss: 0.2858517661099354\n",
      "Iteration: 462 lambda_k: 1 Loss: 0.2853784138728635\n",
      "Iteration: 463 lambda_k: 1 Loss: 0.2849059680030674\n",
      "Iteration: 464 lambda_k: 1 Loss: 0.2844344262521381\n",
      "Iteration: 465 lambda_k: 1 Loss: 0.283963786388837\n",
      "Iteration: 466 lambda_k: 1 Loss: 0.2834940461990158\n",
      "Iteration: 467 lambda_k: 1 Loss: 0.28302520348553856\n",
      "Iteration: 468 lambda_k: 1 Loss: 0.28255725606820403\n",
      "Iteration: 469 lambda_k: 1 Loss: 0.2820902017836701\n",
      "Iteration: 470 lambda_k: 1 Loss: 0.28162403848537876\n",
      "Iteration: 471 lambda_k: 1 Loss: 0.28115876404348233\n",
      "Iteration: 472 lambda_k: 1 Loss: 0.28069437634477123\n",
      "Iteration: 473 lambda_k: 1 Loss: 0.280230873292601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 474 lambda_k: 1 Loss: 0.27976825280682277\n",
      "Iteration: 475 lambda_k: 1 Loss: 0.2793065128237121\n",
      "Iteration: 476 lambda_k: 1 Loss: 0.27884565129590083\n",
      "Iteration: 477 lambda_k: 1 Loss: 0.2783856661923076\n",
      "Iteration: 478 lambda_k: 1 Loss: 0.27792655549807105\n",
      "Iteration: 479 lambda_k: 1 Loss: 0.27746831721448234\n",
      "Iteration: 480 lambda_k: 1 Loss: 0.27701094935891923\n",
      "Iteration: 481 lambda_k: 1 Loss: 0.2765544499647803\n",
      "Iteration: 482 lambda_k: 1 Loss: 0.2760988170814202\n",
      "Iteration: 483 lambda_k: 1 Loss: 0.2756440487740852\n",
      "Iteration: 484 lambda_k: 1 Loss: 0.27519014312384943\n",
      "Iteration: 485 lambda_k: 1 Loss: 0.274737098227552\n",
      "Iteration: 486 lambda_k: 1 Loss: 0.27428491219773393\n",
      "Iteration: 487 lambda_k: 1 Loss: 0.27383358316257667\n",
      "Iteration: 488 lambda_k: 1 Loss: 0.2733831092658401\n",
      "Iteration: 489 lambda_k: 1 Loss: 0.2729334886668016\n",
      "Iteration: 490 lambda_k: 1 Loss: 0.2724847195401959\n",
      "Iteration: 491 lambda_k: 1 Loss: 0.27203680007615444\n",
      "Iteration: 492 lambda_k: 1 Loss: 0.2715897284801461\n",
      "Iteration: 493 lambda_k: 1 Loss: 0.27114350297291817\n",
      "Iteration: 494 lambda_k: 1 Loss: 0.27069812179043723\n",
      "Iteration: 495 lambda_k: 1 Loss: 0.2702535831838314\n",
      "Iteration: 496 lambda_k: 1 Loss: 0.2698098854193321\n",
      "Iteration: 497 lambda_k: 1 Loss: 0.26936702677821617\n",
      "Iteration: 498 lambda_k: 1 Loss: 0.26892500555675053\n",
      "Iteration: 499 lambda_k: 1 Loss: 0.26848382006613436\n",
      "Iteration: 500 lambda_k: 1 Loss: 0.26804346863244266\n",
      "Iteration: 501 lambda_k: 1 Loss: 0.26760394959657047\n",
      "Iteration: 502 lambda_k: 1 Loss: 0.2671652613141778\n",
      "Iteration: 503 lambda_k: 1 Loss: 0.2667274021556342\n",
      "Iteration: 504 lambda_k: 1 Loss: 0.26629037050596427\n",
      "Iteration: 505 lambda_k: 1 Loss: 0.265854164764793\n",
      "Iteration: 506 lambda_k: 1 Loss: 0.2654187833462918\n",
      "Iteration: 507 lambda_k: 1 Loss: 0.2649842246791249\n",
      "Iteration: 508 lambda_k: 1 Loss: 0.2645504872063955\n",
      "Iteration: 509 lambda_k: 1 Loss: 0.2641175693855937\n",
      "Iteration: 510 lambda_k: 1 Loss: 0.2636854696885425\n",
      "Iteration: 511 lambda_k: 1 Loss: 0.26325418660134653\n",
      "Iteration: 512 lambda_k: 1 Loss: 0.2628237186243394\n",
      "Iteration: 513 lambda_k: 1 Loss: 0.26239406427203193\n",
      "Iteration: 514 lambda_k: 1 Loss: 0.26196522207306083\n",
      "Iteration: 515 lambda_k: 1 Loss: 0.26153719057013747\n",
      "Iteration: 516 lambda_k: 1 Loss: 0.26110996831999717\n",
      "Iteration: 517 lambda_k: 1 Loss: 0.2606835538933484\n",
      "Iteration: 518 lambda_k: 1 Loss: 0.2602579458748228\n",
      "Iteration: 519 lambda_k: 1 Loss: 0.2598331428629249\n",
      "Iteration: 520 lambda_k: 1 Loss: 0.25940914346998256\n",
      "Iteration: 521 lambda_k: 1 Loss: 0.25898594632209765\n",
      "Iteration: 522 lambda_k: 1 Loss: 0.2585635500590968\n",
      "Iteration: 523 lambda_k: 1 Loss: 0.2581419533344824\n",
      "Iteration: 524 lambda_k: 1 Loss: 0.2577211548153844\n",
      "Iteration: 525 lambda_k: 1 Loss: 0.25730115318251134\n",
      "Iteration: 526 lambda_k: 1 Loss: 0.25688194713010315\n",
      "Iteration: 527 lambda_k: 1 Loss: 0.25646353536588257\n",
      "Iteration: 528 lambda_k: 1 Loss: 0.25604591661100773\n",
      "Iteration: 529 lambda_k: 1 Loss: 0.25562908960002534\n",
      "Iteration: 530 lambda_k: 1 Loss: 0.2552130530808229\n",
      "Iteration: 531 lambda_k: 1 Loss: 0.2547978058145826\n",
      "Iteration: 532 lambda_k: 1 Loss: 0.25438334657573436\n",
      "Iteration: 533 lambda_k: 1 Loss: 0.25396967415190924\n",
      "Iteration: 534 lambda_k: 1 Loss: 0.2535567873438941\n",
      "Iteration: 535 lambda_k: 1 Loss: 0.2531446849655848\n",
      "Iteration: 536 lambda_k: 1 Loss: 0.2527333658439415\n",
      "Iteration: 537 lambda_k: 1 Loss: 0.25232282881894286\n",
      "Iteration: 538 lambda_k: 1 Loss: 0.2519130727435403\n",
      "Iteration: 539 lambda_k: 1 Loss: 0.2515040964836139\n",
      "Iteration: 540 lambda_k: 1 Loss: 0.25109589891792744\n",
      "Iteration: 541 lambda_k: 1 Loss: 0.25068847893807594\n",
      "Iteration: 542 lambda_k: 1 Loss: 0.25028183544846716\n",
      "Iteration: 543 lambda_k: 1 Loss: 0.24987596736624967\n",
      "Iteration: 544 lambda_k: 1 Loss: 0.2494708736212742\n",
      "Iteration: 545 lambda_k: 1 Loss: 0.24906655315605697\n",
      "Iteration: 546 lambda_k: 1 Loss: 0.248663004925735\n",
      "Iteration: 547 lambda_k: 1 Loss: 0.248260227898021\n",
      "Iteration: 548 lambda_k: 1 Loss: 0.24785822105316066\n",
      "Iteration: 549 lambda_k: 1 Loss: 0.24745698338388936\n",
      "Iteration: 550 lambda_k: 1 Loss: 0.2470565138953897\n",
      "Iteration: 551 lambda_k: 1 Loss: 0.24665681160524874\n",
      "Iteration: 552 lambda_k: 1 Loss: 0.2462578755434158\n",
      "Iteration: 553 lambda_k: 1 Loss: 0.24585970475215976\n",
      "Iteration: 554 lambda_k: 1 Loss: 0.24546229828602784\n",
      "Iteration: 555 lambda_k: 1 Loss: 0.2450656552118028\n",
      "Iteration: 556 lambda_k: 1 Loss: 0.2446697746084611\n",
      "Iteration: 557 lambda_k: 1 Loss: 0.24427465556713174\n",
      "Iteration: 558 lambda_k: 1 Loss: 0.24388029719105428\n",
      "Iteration: 559 lambda_k: 1 Loss: 0.24348669859555416\n",
      "Iteration: 560 lambda_k: 1 Loss: 0.24309385890793245\n",
      "Iteration: 561 lambda_k: 1 Loss: 0.24270177726744874\n",
      "Iteration: 562 lambda_k: 1 Loss: 0.24231045282554767\n",
      "Iteration: 563 lambda_k: 1 Loss: 0.24191988474510923\n",
      "Iteration: 564 lambda_k: 1 Loss: 0.24153007220137138\n",
      "Iteration: 565 lambda_k: 1 Loss: 0.24114101438059143\n",
      "Iteration: 566 lambda_k: 1 Loss: 0.24075244356270276\n",
      "Iteration: 567 lambda_k: 1 Loss: 0.2403632830598407\n",
      "Iteration: 568 lambda_k: 1 Loss: 0.2399753114429876\n",
      "Iteration: 569 lambda_k: 1 Loss: 0.2395888386068918\n",
      "Iteration: 570 lambda_k: 1 Loss: 0.2392037528204783\n",
      "Iteration: 571 lambda_k: 1 Loss: 0.23881989477283155\n",
      "Iteration: 572 lambda_k: 1 Loss: 0.2384371841487814\n",
      "Iteration: 573 lambda_k: 1 Loss: 0.23805556290863497\n",
      "Iteration: 574 lambda_k: 1 Loss: 0.23767476514683267\n",
      "Iteration: 575 lambda_k: 1 Loss: 0.23729482211413142\n",
      "Iteration: 576 lambda_k: 1 Loss: 0.23691566470993253\n",
      "Iteration: 577 lambda_k: 1 Loss: 0.23653721757619084\n",
      "Iteration: 578 lambda_k: 1 Loss: 0.23615943021155028\n",
      "Iteration: 579 lambda_k: 1 Loss: 0.23578226901619917\n",
      "Iteration: 580 lambda_k: 1 Loss: 0.23540570725310697\n",
      "Iteration: 581 lambda_k: 1 Loss: 0.23502972171648037\n",
      "Iteration: 582 lambda_k: 1 Loss: 0.23465429273170624\n",
      "Iteration: 583 lambda_k: 1 Loss: 0.23427940414544837\n",
      "Iteration: 584 lambda_k: 1 Loss: 0.2339050427568576\n",
      "Iteration: 585 lambda_k: 1 Loss: 0.23353119767229638\n",
      "Iteration: 586 lambda_k: 1 Loss: 0.23315785983896542\n",
      "Iteration: 587 lambda_k: 1 Loss: 0.23278502172516893\n",
      "Iteration: 588 lambda_k: 1 Loss: 0.2324126770676154\n",
      "Iteration: 589 lambda_k: 1 Loss: 0.23204082065388534\n",
      "Iteration: 590 lambda_k: 1 Loss: 0.23166944813757256\n",
      "Iteration: 591 lambda_k: 1 Loss: 0.23129855588555\n",
      "Iteration: 592 lambda_k: 1 Loss: 0.23092814085250524\n",
      "Iteration: 593 lambda_k: 1 Loss: 0.2305582004769768\n",
      "Iteration: 594 lambda_k: 1 Loss: 0.2301887325945573\n",
      "Iteration: 595 lambda_k: 1 Loss: 0.2298197353653086\n",
      "Iteration: 596 lambda_k: 1 Loss: 0.22945120721313098\n",
      "Iteration: 597 lambda_k: 1 Loss: 0.2290831467751694\n",
      "Iteration: 598 lambda_k: 1 Loss: 0.22871555285962064\n",
      "Iteration: 599 lambda_k: 1 Loss: 0.22834842441057732\n",
      "Iteration: 600 lambda_k: 1 Loss: 0.22798176047878216\n",
      "Iteration: 601 lambda_k: 1 Loss: 0.22761556019735857\n",
      "Iteration: 602 lambda_k: 1 Loss: 0.22724982276173228\n",
      "Iteration: 603 lambda_k: 1 Loss: 0.2268845474130832\n",
      "Iteration: 604 lambda_k: 1 Loss: 0.2265197334247694\n",
      "Iteration: 605 lambda_k: 1 Loss: 0.22615538009125072\n",
      "Iteration: 606 lambda_k: 1 Loss: 0.22579148671911378\n",
      "Iteration: 607 lambda_k: 1 Loss: 0.22542805261985976\n",
      "Iteration: 608 lambda_k: 1 Loss: 0.2250650771041688\n",
      "Iteration: 609 lambda_k: 1 Loss: 0.2247025594773981\n",
      "Iteration: 610 lambda_k: 1 Loss: 0.22434049903610906\n",
      "Iteration: 611 lambda_k: 1 Loss: 0.22397889506544796\n",
      "Iteration: 612 lambda_k: 1 Loss: 0.22361774683515512\n",
      "Iteration: 613 lambda_k: 1 Loss: 0.22325705360642148\n",
      "Iteration: 614 lambda_k: 1 Loss: 0.22289681461873126\n",
      "Iteration: 615 lambda_k: 1 Loss: 0.22253702909807394\n",
      "Iteration: 616 lambda_k: 1 Loss: 0.2221776962540011\n",
      "Iteration: 617 lambda_k: 1 Loss: 0.2218188152799969\n",
      "Iteration: 618 lambda_k: 1 Loss: 0.2214603853538421\n",
      "Iteration: 619 lambda_k: 1 Loss: 0.2211024056380677\n",
      "Iteration: 620 lambda_k: 1 Loss: 0.2207448752805149\n",
      "Iteration: 621 lambda_k: 1 Loss: 0.22038779341499196\n",
      "Iteration: 622 lambda_k: 1 Loss: 0.220031159162\n",
      "Iteration: 623 lambda_k: 1 Loss: 0.21967497162950256\n",
      "Iteration: 624 lambda_k: 1 Loss: 0.21931922991619035\n",
      "Iteration: 625 lambda_k: 1 Loss: 0.218963933102078\n",
      "Iteration: 626 lambda_k: 1 Loss: 0.21860908026567705\n",
      "Iteration: 627 lambda_k: 1 Loss: 0.2182546704721941\n",
      "Iteration: 628 lambda_k: 1 Loss: 0.21790070277887202\n",
      "Iteration: 629 lambda_k: 1 Loss: 0.21754717623528236\n",
      "Iteration: 630 lambda_k: 1 Loss: 0.21719408988394653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 631 lambda_k: 1 Loss: 0.21684144276104636\n",
      "Iteration: 632 lambda_k: 1 Loss: 0.2164892338971327\n",
      "Iteration: 633 lambda_k: 1 Loss: 0.2161374623177939\n",
      "Iteration: 634 lambda_k: 1 Loss: 0.21578612704427916\n",
      "Iteration: 635 lambda_k: 1 Loss: 0.21543522709409318\n",
      "Iteration: 636 lambda_k: 1 Loss: 0.21508476148156822\n",
      "Iteration: 637 lambda_k: 1 Loss: 0.21473472921839315\n",
      "Iteration: 638 lambda_k: 1 Loss: 0.2143851293141188\n",
      "Iteration: 639 lambda_k: 1 Loss: 0.21403596077403583\n",
      "Iteration: 640 lambda_k: 1 Loss: 0.21368722261025833\n",
      "Iteration: 641 lambda_k: 1 Loss: 0.21333891382542233\n",
      "Iteration: 642 lambda_k: 1 Loss: 0.2129910334254758\n",
      "Iteration: 643 lambda_k: 1 Loss: 0.21264358041589249\n",
      "Iteration: 644 lambda_k: 1 Loss: 0.21229655380236295\n",
      "Iteration: 645 lambda_k: 1 Loss: 0.21194995259123264\n",
      "Iteration: 646 lambda_k: 1 Loss: 0.2116037757898068\n",
      "Iteration: 647 lambda_k: 1 Loss: 0.21125802240660863\n",
      "Iteration: 648 lambda_k: 1 Loss: 0.21091269145163025\n",
      "Iteration: 649 lambda_k: 1 Loss: 0.21056778193657502\n",
      "Iteration: 650 lambda_k: 1 Loss: 0.21022329287508335\n",
      "Iteration: 651 lambda_k: 1 Loss: 0.20987922328294165\n",
      "Iteration: 652 lambda_k: 1 Loss: 0.2095355721782755\n",
      "Iteration: 653 lambda_k: 1 Loss: 0.20919233858172845\n",
      "Iteration: 654 lambda_k: 1 Loss: 0.20884952151662736\n",
      "Iteration: 655 lambda_k: 1 Loss: 0.20850712000913596\n",
      "Iteration: 656 lambda_k: 1 Loss: 0.20816513308839577\n",
      "Iteration: 657 lambda_k: 1 Loss: 0.20782355978665687\n",
      "Iteration: 658 lambda_k: 1 Loss: 0.20748239913939767\n",
      "Iteration: 659 lambda_k: 1 Loss: 0.20714165018543557\n",
      "Iteration: 660 lambda_k: 1 Loss: 0.20680131196702844\n",
      "Iteration: 661 lambda_k: 1 Loss: 0.20646138352996787\n",
      "Iteration: 662 lambda_k: 1 Loss: 0.2061218639236644\n",
      "Iteration: 663 lambda_k: 1 Loss: 0.2057827522012258\n",
      "Iteration: 664 lambda_k: 1 Loss: 0.20544404741952796\n",
      "Iteration: 665 lambda_k: 1 Loss: 0.20510574863928036\n",
      "Iteration: 666 lambda_k: 1 Loss: 0.20476785492508465\n",
      "Iteration: 667 lambda_k: 1 Loss: 0.20443036534548847\n",
      "Iteration: 668 lambda_k: 1 Loss: 0.20409327897303406\n",
      "Iteration: 669 lambda_k: 1 Loss: 0.20375659488430162\n",
      "Iteration: 670 lambda_k: 1 Loss: 0.20342031215994885\n",
      "Iteration: 671 lambda_k: 1 Loss: 0.2030844298847462\n",
      "Iteration: 672 lambda_k: 1 Loss: 0.20274894714760813\n",
      "Iteration: 673 lambda_k: 1 Loss: 0.2024138630416209\n",
      "Iteration: 674 lambda_k: 1 Loss: 0.20207917666406736\n",
      "Iteration: 675 lambda_k: 1 Loss: 0.20174488711644803\n",
      "Iteration: 676 lambda_k: 1 Loss: 0.2014109935045002\n",
      "Iteration: 677 lambda_k: 1 Loss: 0.20107749493821367\n",
      "Iteration: 678 lambda_k: 1 Loss: 0.2007443905318445\n",
      "Iteration: 679 lambda_k: 1 Loss: 0.20041167940392599\n",
      "Iteration: 680 lambda_k: 1 Loss: 0.2000793606772784\n",
      "Iteration: 681 lambda_k: 1 Loss: 0.19974743347901586\n",
      "Iteration: 682 lambda_k: 1 Loss: 0.19941589694055184\n",
      "Iteration: 683 lambda_k: 1 Loss: 0.19908475019760327\n",
      "Iteration: 684 lambda_k: 1 Loss: 0.19875399239019229\n",
      "Iteration: 685 lambda_k: 1 Loss: 0.1984236226626478\n",
      "Iteration: 686 lambda_k: 1 Loss: 0.19809364016360437\n",
      "Iteration: 687 lambda_k: 1 Loss: 0.197764044046001\n",
      "Iteration: 688 lambda_k: 1 Loss: 0.19743483346707788\n",
      "Iteration: 689 lambda_k: 1 Loss: 0.19710600758837296\n",
      "Iteration: 690 lambda_k: 1 Loss: 0.19677756557571702\n",
      "Iteration: 691 lambda_k: 1 Loss: 0.19644950659922794\n",
      "Iteration: 692 lambda_k: 1 Loss: 0.19612182983330428\n",
      "Iteration: 693 lambda_k: 1 Loss: 0.19579453445661815\n",
      "Iteration: 694 lambda_k: 1 Loss: 0.1954676196521072\n",
      "Iteration: 695 lambda_k: 1 Loss: 0.1951410846069662\n",
      "Iteration: 696 lambda_k: 1 Loss: 0.19481492851263768\n",
      "Iteration: 697 lambda_k: 1 Loss: 0.19448915056480295\n",
      "Iteration: 698 lambda_k: 1 Loss: 0.19416374996337135\n",
      "Iteration: 699 lambda_k: 1 Loss: 0.19383872591246998\n",
      "Iteration: 700 lambda_k: 1 Loss: 0.19351407762043293\n",
      "Iteration: 701 lambda_k: 1 Loss: 0.19318980429978977\n",
      "Iteration: 702 lambda_k: 1 Loss: 0.19286590516725413\n",
      "Iteration: 703 lambda_k: 1 Loss: 0.1925423794437117\n",
      "Iteration: 704 lambda_k: 1 Loss: 0.19221922635420804\n",
      "Iteration: 705 lambda_k: 1 Loss: 0.19189644512793644\n",
      "Iteration: 706 lambda_k: 1 Loss: 0.1915740349982249\n",
      "Iteration: 707 lambda_k: 1 Loss: 0.1912519952025238\n",
      "Iteration: 708 lambda_k: 1 Loss: 0.19093032498239243\n",
      "Iteration: 709 lambda_k: 1 Loss: 0.19060902358348628\n",
      "Iteration: 710 lambda_k: 1 Loss: 0.19028809025554377\n",
      "Iteration: 711 lambda_k: 1 Loss: 0.18996752425237273\n",
      "Iteration: 712 lambda_k: 1 Loss: 0.18964732483183702\n",
      "Iteration: 713 lambda_k: 1 Loss: 0.18932749125584317\n",
      "Iteration: 714 lambda_k: 1 Loss: 0.18900802279032652\n",
      "Iteration: 715 lambda_k: 1 Loss: 0.18868891870523793\n",
      "Iteration: 716 lambda_k: 1 Loss: 0.1883701782745298\n",
      "Iteration: 717 lambda_k: 1 Loss: 0.18805180077614253\n",
      "Iteration: 718 lambda_k: 1 Loss: 0.18773378549199082\n",
      "Iteration: 719 lambda_k: 1 Loss: 0.18741613170795007\n",
      "Iteration: 720 lambda_k: 1 Loss: 0.18709883871384222\n",
      "Iteration: 721 lambda_k: 1 Loss: 0.18678190580342274\n",
      "Iteration: 722 lambda_k: 1 Loss: 0.18646533227436615\n",
      "Iteration: 723 lambda_k: 1 Loss: 0.18614911742825305\n",
      "Iteration: 724 lambda_k: 1 Loss: 0.18583326057055602\n",
      "Iteration: 725 lambda_k: 1 Loss: 0.1855177610106264\n",
      "Iteration: 726 lambda_k: 1 Loss: 0.1852026180616803\n",
      "Iteration: 727 lambda_k: 1 Loss: 0.18488783104078574\n",
      "Iteration: 728 lambda_k: 1 Loss: 0.18457339926884847\n",
      "Iteration: 729 lambda_k: 1 Loss: 0.18425932207059917\n",
      "Iteration: 730 lambda_k: 1 Loss: 0.1839455987745799\n",
      "Iteration: 731 lambda_k: 1 Loss: 0.18363222871313079\n",
      "Iteration: 732 lambda_k: 1 Loss: 0.18331921122237707\n",
      "Iteration: 733 lambda_k: 1 Loss: 0.18300654564221583\n",
      "Iteration: 734 lambda_k: 1 Loss: 0.18269423131630297\n",
      "Iteration: 735 lambda_k: 1 Loss: 0.1823822675920404\n",
      "Iteration: 736 lambda_k: 1 Loss: 0.182070653820563\n",
      "Iteration: 737 lambda_k: 1 Loss: 0.1817593893567261\n",
      "Iteration: 738 lambda_k: 1 Loss: 0.18144847355909255\n",
      "Iteration: 739 lambda_k: 1 Loss: 0.18113790578992034\n",
      "Iteration: 740 lambda_k: 1 Loss: 0.18082768541515007\n",
      "Iteration: 741 lambda_k: 1 Loss: 0.18051781180439236\n",
      "Iteration: 742 lambda_k: 1 Loss: 0.18020828433091585\n",
      "Iteration: 743 lambda_k: 1 Loss: 0.1798991023716348\n",
      "Iteration: 744 lambda_k: 1 Loss: 0.1795902653070971\n",
      "Iteration: 745 lambda_k: 1 Loss: 0.17928177252147226\n",
      "Iteration: 746 lambda_k: 1 Loss: 0.1789736234025395\n",
      "Iteration: 747 lambda_k: 1 Loss: 0.17866581734167603\n",
      "Iteration: 748 lambda_k: 1 Loss: 0.1783583537338453\n",
      "Iteration: 749 lambda_k: 1 Loss: 0.17805123197758538\n",
      "Iteration: 750 lambda_k: 1 Loss: 0.1777444514749978\n",
      "Iteration: 751 lambda_k: 1 Loss: 0.17743801163173603\n",
      "Iteration: 752 lambda_k: 1 Loss: 0.17713191185699395\n",
      "Iteration: 753 lambda_k: 1 Loss: 0.17682615156349524\n",
      "Iteration: 754 lambda_k: 1 Loss: 0.17652073016748207\n",
      "Iteration: 755 lambda_k: 1 Loss: 0.1762156470887044\n",
      "Iteration: 756 lambda_k: 1 Loss: 0.17591090175040888\n",
      "Iteration: 757 lambda_k: 1 Loss: 0.17560649357932873\n",
      "Iteration: 758 lambda_k: 1 Loss: 0.17530242200567236\n",
      "Iteration: 759 lambda_k: 1 Loss: 0.17499868646311387\n",
      "Iteration: 760 lambda_k: 1 Loss: 0.1746952863887823\n",
      "Iteration: 761 lambda_k: 1 Loss: 0.17439222122325115\n",
      "Iteration: 762 lambda_k: 1 Loss: 0.1740894904105289\n",
      "Iteration: 763 lambda_k: 1 Loss: 0.1737870933980486\n",
      "Iteration: 764 lambda_k: 1 Loss: 0.1734850296366582\n",
      "Iteration: 765 lambda_k: 1 Loss: 0.17318329858061093\n",
      "Iteration: 766 lambda_k: 1 Loss: 0.1728818996875555\n",
      "Iteration: 767 lambda_k: 1 Loss: 0.1725808324185267\n",
      "Iteration: 768 lambda_k: 1 Loss: 0.17228009623793616\n",
      "Iteration: 769 lambda_k: 1 Loss: 0.17197969061356286\n",
      "Iteration: 770 lambda_k: 1 Loss: 0.17167961501654433\n",
      "Iteration: 771 lambda_k: 1 Loss: 0.17137986892136747\n",
      "Iteration: 772 lambda_k: 1 Loss: 0.17108045180585965\n",
      "Iteration: 773 lambda_k: 1 Loss: 0.17078136315118012\n",
      "Iteration: 774 lambda_k: 1 Loss: 0.17048260244181115\n",
      "Iteration: 775 lambda_k: 1 Loss: 0.17018416916554982\n",
      "Iteration: 776 lambda_k: 1 Loss: 0.16988606281349947\n",
      "Iteration: 777 lambda_k: 1 Loss: 0.1695882828800613\n",
      "Iteration: 778 lambda_k: 1 Loss: 0.16929082886292665\n",
      "Iteration: 779 lambda_k: 1 Loss: 0.16899370026306856\n",
      "Iteration: 780 lambda_k: 1 Loss: 0.16869689658473427\n",
      "Iteration: 781 lambda_k: 1 Loss: 0.16840041733543698\n",
      "Iteration: 782 lambda_k: 1 Loss: 0.1681042620259487\n",
      "Iteration: 783 lambda_k: 1 Loss: 0.1678084301702925\n",
      "Iteration: 784 lambda_k: 1 Loss: 0.16751292128573517\n",
      "Iteration: 785 lambda_k: 1 Loss: 0.16721773489277997\n",
      "Iteration: 786 lambda_k: 1 Loss: 0.16692287051515944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 787 lambda_k: 1 Loss: 0.1666283276798284\n",
      "Iteration: 788 lambda_k: 1 Loss: 0.1663341059169572\n",
      "Iteration: 789 lambda_k: 1 Loss: 0.16604020475992468\n",
      "Iteration: 790 lambda_k: 1 Loss: 0.16574662374531185\n",
      "Iteration: 791 lambda_k: 1 Loss: 0.1654533624168001\n",
      "Iteration: 792 lambda_k: 1 Loss: 0.1651604203089772\n",
      "Iteration: 793 lambda_k: 1 Loss: 0.1648677969727363\n",
      "Iteration: 794 lambda_k: 1 Loss: 0.16457549195707233\n",
      "Iteration: 795 lambda_k: 1 Loss: 0.16428350481452283\n",
      "Iteration: 796 lambda_k: 1 Loss: 0.1639918351007665\n",
      "Iteration: 797 lambda_k: 1 Loss: 0.16370048237462798\n",
      "Iteration: 798 lambda_k: 1 Loss: 0.16340944619808329\n",
      "Iteration: 799 lambda_k: 1 Loss: 0.1631187261362559\n",
      "Iteration: 800 lambda_k: 1 Loss: 0.16282832175741094\n",
      "Iteration: 801 lambda_k: 1 Loss: 0.1625382326329501\n",
      "Iteration: 802 lambda_k: 1 Loss: 0.1622484583374064\n",
      "Iteration: 803 lambda_k: 1 Loss: 0.161958998448475\n",
      "Iteration: 804 lambda_k: 1 Loss: 0.1616698525468403\n",
      "Iteration: 805 lambda_k: 1 Loss: 0.16138102021647194\n",
      "Iteration: 806 lambda_k: 1 Loss: 0.1610925010443789\n",
      "Iteration: 807 lambda_k: 1 Loss: 0.16080429462067672\n",
      "Iteration: 808 lambda_k: 1 Loss: 0.1605164005385894\n",
      "Iteration: 809 lambda_k: 1 Loss: 0.16022881839444192\n",
      "Iteration: 810 lambda_k: 1 Loss: 0.15994154778765443\n",
      "Iteration: 811 lambda_k: 1 Loss: 0.1596545883207386\n",
      "Iteration: 812 lambda_k: 1 Loss: 0.15936793959929357\n",
      "Iteration: 813 lambda_k: 1 Loss: 0.1590816012320027\n",
      "Iteration: 814 lambda_k: 1 Loss: 0.15879557283062948\n",
      "Iteration: 815 lambda_k: 1 Loss: 0.15850985401001444\n",
      "Iteration: 816 lambda_k: 1 Loss: 0.15822444438807148\n",
      "Iteration: 817 lambda_k: 1 Loss: 0.15793934358578468\n",
      "Iteration: 818 lambda_k: 1 Loss: 0.1576545512272052\n",
      "Iteration: 819 lambda_k: 1 Loss: 0.15737006693944813\n",
      "Iteration: 820 lambda_k: 1 Loss: 0.15708589035268972\n",
      "Iteration: 821 lambda_k: 1 Loss: 0.15680202110053898\n",
      "Iteration: 822 lambda_k: 1 Loss: 0.15651845881823273\n",
      "Iteration: 823 lambda_k: 1 Loss: 0.15623520314586115\n",
      "Iteration: 824 lambda_k: 1 Loss: 0.15595225372578803\n",
      "Iteration: 825 lambda_k: 1 Loss: 0.15566961020343323\n",
      "Iteration: 826 lambda_k: 1 Loss: 0.1553872722272796\n",
      "Iteration: 827 lambda_k: 1 Loss: 0.1551052394488624\n",
      "Iteration: 828 lambda_k: 1 Loss: 0.15482351152275922\n",
      "Iteration: 829 lambda_k: 1 Loss: 0.154542088106586\n",
      "Iteration: 830 lambda_k: 1 Loss: 0.15426096886099644\n",
      "Iteration: 831 lambda_k: 1 Loss: 0.15398015344968166\n",
      "Iteration: 832 lambda_k: 1 Loss: 0.15369964153936908\n",
      "Iteration: 833 lambda_k: 1 Loss: 0.15341943279982095\n",
      "Iteration: 834 lambda_k: 1 Loss: 0.15313952690383295\n",
      "Iteration: 835 lambda_k: 1 Loss: 0.1528599235272331\n",
      "Iteration: 836 lambda_k: 1 Loss: 0.1525806223488808\n",
      "Iteration: 837 lambda_k: 1 Loss: 0.15230162305066605\n",
      "Iteration: 838 lambda_k: 1 Loss: 0.1520229253175086\n",
      "Iteration: 839 lambda_k: 1 Loss: 0.15174452883735748\n",
      "Iteration: 840 lambda_k: 1 Loss: 0.15146643330119053\n",
      "Iteration: 841 lambda_k: 1 Loss: 0.15118863840301386\n",
      "Iteration: 842 lambda_k: 1 Loss: 0.15091114383986165\n",
      "Iteration: 843 lambda_k: 1 Loss: 0.15063394931179616\n",
      "Iteration: 844 lambda_k: 1 Loss: 0.1503570545219077\n",
      "Iteration: 845 lambda_k: 1 Loss: 0.1500804591763143\n",
      "Iteration: 846 lambda_k: 1 Loss: 0.1498041629841627\n",
      "Iteration: 847 lambda_k: 1 Loss: 0.14952816565762778\n",
      "Iteration: 848 lambda_k: 1 Loss: 0.14925246691191385\n",
      "Iteration: 849 lambda_k: 1 Loss: 0.14897706646525455\n",
      "Iteration: 850 lambda_k: 1 Loss: 0.14870196403891392\n",
      "Iteration: 851 lambda_k: 1 Loss: 0.14842715935718717\n",
      "Iteration: 852 lambda_k: 1 Loss: 0.14815265214740111\n",
      "Iteration: 853 lambda_k: 1 Loss: 0.14787844213991597\n",
      "Iteration: 854 lambda_k: 1 Loss: 0.14760452906812577\n",
      "Iteration: 855 lambda_k: 1 Loss: 0.14733091266846013\n",
      "Iteration: 856 lambda_k: 1 Loss: 0.14705759268038512\n",
      "Iteration: 857 lambda_k: 1 Loss: 0.1467845688464051\n",
      "Iteration: 858 lambda_k: 1 Loss: 0.14651184091206423\n",
      "Iteration: 859 lambda_k: 1 Loss: 0.1462394086259481\n",
      "Iteration: 860 lambda_k: 1 Loss: 0.14596727173968546\n",
      "Iteration: 861 lambda_k: 1 Loss: 0.14569543000795027\n",
      "Iteration: 862 lambda_k: 1 Loss: 0.14542388318846372\n",
      "Iteration: 863 lambda_k: 1 Loss: 0.1451526310419962\n",
      "Iteration: 864 lambda_k: 1 Loss: 0.14488167333236968\n",
      "Iteration: 865 lambda_k: 1 Loss: 0.1446110098264601\n",
      "Iteration: 866 lambda_k: 1 Loss: 0.14434064029419977\n",
      "Iteration: 867 lambda_k: 1 Loss: 0.14407056450857986\n",
      "Iteration: 868 lambda_k: 1 Loss: 0.14380078224565318\n",
      "Iteration: 869 lambda_k: 1 Loss: 0.14353129328453706\n",
      "Iteration: 870 lambda_k: 1 Loss: 0.14326209740741597\n",
      "Iteration: 871 lambda_k: 1 Loss: 0.14299319439954486\n",
      "Iteration: 872 lambda_k: 1 Loss: 0.1427245840492519\n",
      "Iteration: 873 lambda_k: 1 Loss: 0.14245626614794216\n",
      "Iteration: 874 lambda_k: 1 Loss: 0.14218824049010143\n",
      "Iteration: 875 lambda_k: 1 Loss: 0.14192050687329735\n",
      "Iteration: 876 lambda_k: 1 Loss: 0.14165306509818418\n",
      "Iteration: 877 lambda_k: 1 Loss: 0.1413859149685079\n",
      "Iteration: 878 lambda_k: 1 Loss: 0.14111905629110869\n",
      "Iteration: 879 lambda_k: 1 Loss: 0.14085248887592494\n",
      "Iteration: 880 lambda_k: 1 Loss: 0.1405862125359974\n",
      "Iteration: 881 lambda_k: 1 Loss: 0.14032022708747272\n",
      "Iteration: 882 lambda_k: 1 Loss: 0.14005453234960816\n",
      "Iteration: 883 lambda_k: 1 Loss: 0.1397891281447758\n",
      "Iteration: 884 lambda_k: 1 Loss: 0.13952401429846642\n",
      "Iteration: 885 lambda_k: 1 Loss: 0.13925919063929487\n",
      "Iteration: 886 lambda_k: 1 Loss: 0.13899465699900376\n",
      "Iteration: 887 lambda_k: 1 Loss: 0.13873041321246884\n",
      "Iteration: 888 lambda_k: 1 Loss: 0.13846645911770336\n",
      "Iteration: 889 lambda_k: 1 Loss: 0.13820279455586434\n",
      "Iteration: 890 lambda_k: 1 Loss: 0.137939419371254\n",
      "Iteration: 891 lambda_k: 1 Loss: 0.1376763334113287\n",
      "Iteration: 892 lambda_k: 1 Loss: 0.13741353652670263\n",
      "Iteration: 893 lambda_k: 1 Loss: 0.1371510285711534\n",
      "Iteration: 894 lambda_k: 1 Loss: 0.13688880940162745\n",
      "Iteration: 895 lambda_k: 1 Loss: 0.1366268788782452\n",
      "Iteration: 896 lambda_k: 1 Loss: 0.1363652368643073\n",
      "Iteration: 897 lambda_k: 1 Loss: 0.1361038832262998\n",
      "Iteration: 898 lambda_k: 1 Loss: 0.13584281783390037\n",
      "Iteration: 899 lambda_k: 1 Loss: 0.13558204055998424\n",
      "Iteration: 900 lambda_k: 1 Loss: 0.13532155128063\n",
      "Iteration: 901 lambda_k: 1 Loss: 0.13506134987512608\n",
      "Iteration: 902 lambda_k: 1 Loss: 0.13480143622597685\n",
      "Iteration: 903 lambda_k: 1 Loss: 0.13454181021890893\n",
      "Iteration: 904 lambda_k: 1 Loss: 0.13428247174287797\n",
      "Iteration: 905 lambda_k: 1 Loss: 0.13402342069007483\n",
      "Iteration: 906 lambda_k: 1 Loss: 0.13376465695593262\n",
      "Iteration: 907 lambda_k: 1 Loss: 0.13350618043913554\n",
      "Iteration: 908 lambda_k: 1 Loss: 0.13324799104162047\n",
      "Iteration: 909 lambda_k: 1 Loss: 0.1329900886685854\n",
      "Iteration: 910 lambda_k: 1 Loss: 0.13273247322850087\n",
      "Iteration: 911 lambda_k: 1 Loss: 0.1324751446331147\n",
      "Iteration: 912 lambda_k: 1 Loss: 0.13221810279745924\n",
      "Iteration: 913 lambda_k: 1 Loss: 0.1319613476398584\n",
      "Iteration: 914 lambda_k: 1 Loss: 0.13170487908193576\n",
      "Iteration: 915 lambda_k: 1 Loss: 0.13144869704862194\n",
      "Iteration: 916 lambda_k: 1 Loss: 0.13119280146816248\n",
      "Iteration: 917 lambda_k: 1 Loss: 0.13093719227212566\n",
      "Iteration: 918 lambda_k: 1 Loss: 0.13068186939541024\n",
      "Iteration: 919 lambda_k: 1 Loss: 0.13042683277625383\n",
      "Iteration: 920 lambda_k: 1 Loss: 0.13017208235624106\n",
      "Iteration: 921 lambda_k: 1 Loss: 0.1299176180803117\n",
      "Iteration: 922 lambda_k: 1 Loss: 0.1296634398967692\n",
      "Iteration: 923 lambda_k: 1 Loss: 0.12940954775728933\n",
      "Iteration: 924 lambda_k: 1 Loss: 0.12915594161693694\n",
      "Iteration: 925 lambda_k: 1 Loss: 0.12890262143414638\n",
      "Iteration: 926 lambda_k: 1 Loss: 0.12864958717076158\n",
      "Iteration: 927 lambda_k: 1 Loss: 0.12839683879203487\n",
      "Iteration: 928 lambda_k: 1 Loss: 0.1281443762666349\n",
      "Iteration: 929 lambda_k: 1 Loss: 0.1278921995666556\n",
      "Iteration: 930 lambda_k: 1 Loss: 0.12764030866762568\n",
      "Iteration: 931 lambda_k: 1 Loss: 0.12738870354851814\n",
      "Iteration: 932 lambda_k: 1 Loss: 0.12713738419175966\n",
      "Iteration: 933 lambda_k: 1 Loss: 0.12688635058324035\n",
      "Iteration: 934 lambda_k: 1 Loss: 0.12663560271232366\n",
      "Iteration: 935 lambda_k: 1 Loss: 0.12638514057185568\n",
      "Iteration: 936 lambda_k: 1 Loss: 0.12613496415817585\n",
      "Iteration: 937 lambda_k: 1 Loss: 0.12588507347112654\n",
      "Iteration: 938 lambda_k: 1 Loss: 0.12563546851406343\n",
      "Iteration: 939 lambda_k: 1 Loss: 0.12538614929386585\n",
      "Iteration: 940 lambda_k: 1 Loss: 0.12513711582094725\n",
      "Iteration: 941 lambda_k: 1 Loss: 0.12488836810926571\n",
      "Iteration: 942 lambda_k: 1 Loss: 0.1246399061763346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 943 lambda_k: 1 Loss: 0.12439173004323352\n",
      "Iteration: 944 lambda_k: 1 Loss: 0.12414383973461905\n",
      "Iteration: 945 lambda_k: 1 Loss: 0.12389623527873597\n",
      "Iteration: 946 lambda_k: 1 Loss: 0.12364891670742822\n",
      "Iteration: 947 lambda_k: 1 Loss: 0.1234018840561499\n",
      "Iteration: 948 lambda_k: 1 Loss: 0.12315513736397765\n",
      "Iteration: 949 lambda_k: 1 Loss: 0.12290867667362204\n",
      "Iteration: 950 lambda_k: 1 Loss: 0.12266250203143818\n",
      "Iteration: 951 lambda_k: 1 Loss: 0.12241661348743818\n",
      "Iteration: 952 lambda_k: 1 Loss: 0.12217101109530289\n",
      "Iteration: 953 lambda_k: 1 Loss: 0.12192569491239398\n",
      "Iteration: 954 lambda_k: 1 Loss: 0.12168066499976635\n",
      "Iteration: 955 lambda_k: 1 Loss: 0.12143592142217996\n",
      "Iteration: 956 lambda_k: 1 Loss: 0.12119146424811253\n",
      "Iteration: 957 lambda_k: 1 Loss: 0.12094729354977203\n",
      "Iteration: 958 lambda_k: 1 Loss: 0.12070340940310931\n",
      "Iteration: 959 lambda_k: 1 Loss: 0.12045981188783092\n",
      "Iteration: 960 lambda_k: 1 Loss: 0.120216501087412\n",
      "Iteration: 961 lambda_k: 1 Loss: 0.11997347708910935\n",
      "Iteration: 962 lambda_k: 1 Loss: 0.11973073998397461\n",
      "Iteration: 963 lambda_k: 1 Loss: 0.11948828986686756\n",
      "Iteration: 964 lambda_k: 1 Loss: 0.11924612683646962\n",
      "Iteration: 965 lambda_k: 1 Loss: 0.11900425099529738\n",
      "Iteration: 966 lambda_k: 1 Loss: 0.11876266244971628\n",
      "Iteration: 967 lambda_k: 1 Loss: 0.11852136130995448\n",
      "Iteration: 968 lambda_k: 1 Loss: 0.11828034769011693\n",
      "Iteration: 969 lambda_k: 1 Loss: 0.11803962170820306\n",
      "Iteration: 970 lambda_k: 1 Loss: 0.11779918348611622\n",
      "Iteration: 971 lambda_k: 1 Loss: 0.11755903314966874\n",
      "Iteration: 972 lambda_k: 1 Loss: 0.11731917082861407\n",
      "Iteration: 973 lambda_k: 1 Loss: 0.11707959665665553\n",
      "Iteration: 974 lambda_k: 1 Loss: 0.11684031077145973\n",
      "Iteration: 975 lambda_k: 1 Loss: 0.11660131331467165\n",
      "Iteration: 976 lambda_k: 1 Loss: 0.11636260443192958\n",
      "Iteration: 977 lambda_k: 1 Loss: 0.1161241842728804\n",
      "Iteration: 978 lambda_k: 1 Loss: 0.11588605299119527\n",
      "Iteration: 979 lambda_k: 1 Loss: 0.11564821074458463\n",
      "Iteration: 980 lambda_k: 1 Loss: 0.11541065769481434\n",
      "Iteration: 981 lambda_k: 1 Loss: 0.115173394007721\n",
      "Iteration: 982 lambda_k: 1 Loss: 0.11493641985322824\n",
      "Iteration: 983 lambda_k: 1 Loss: 0.11469973540536249\n",
      "Iteration: 984 lambda_k: 1 Loss: 0.11446334084226943\n",
      "Iteration: 985 lambda_k: 1 Loss: 0.11422723634623018\n",
      "Iteration: 986 lambda_k: 1 Loss: 0.11399142210367788\n",
      "Iteration: 987 lambda_k: 1 Loss: 0.11375589830532427\n",
      "Iteration: 988 lambda_k: 1 Loss: 0.11352066514583992\n",
      "Iteration: 989 lambda_k: 1 Loss: 0.11328572282417092\n",
      "Iteration: 990 lambda_k: 1 Loss: 0.11305107154353244\n",
      "Iteration: 991 lambda_k: 1 Loss: 0.1128167115113826\n",
      "Iteration: 992 lambda_k: 1 Loss: 0.11258264293943805\n",
      "Iteration: 993 lambda_k: 1 Loss: 0.11234886604366306\n",
      "Iteration: 994 lambda_k: 1 Loss: 0.11211538104432082\n",
      "Iteration: 995 lambda_k: 1 Loss: 0.11188218816598312\n",
      "Iteration: 996 lambda_k: 1 Loss: 0.1116492876375461\n",
      "Iteration: 997 lambda_k: 1 Loss: 0.11141667969224803\n",
      "Iteration: 998 lambda_k: 1 Loss: 0.11118436456768764\n",
      "Iteration: 999 lambda_k: 1 Loss: 0.11095234250584296\n",
      "Iteration: 1000 lambda_k: 1 Loss: 0.11072061375308997\n",
      "Iteration: 1001 lambda_k: 1 Loss: 0.11048917856022138\n",
      "Iteration: 1002 lambda_k: 1 Loss: 0.11025803718246517\n",
      "Iteration: 1003 lambda_k: 1 Loss: 0.11002718987950376\n",
      "Iteration: 1004 lambda_k: 1 Loss: 0.10979663691549357\n",
      "Iteration: 1005 lambda_k: 1 Loss: 0.10956637855908388\n",
      "Iteration: 1006 lambda_k: 1 Loss: 0.10933641508343649\n",
      "Iteration: 1007 lambda_k: 1 Loss: 0.10910674676624522\n",
      "Iteration: 1008 lambda_k: 1 Loss: 0.10887737388975573\n",
      "Iteration: 1009 lambda_k: 1 Loss: 0.10864829674078529\n",
      "Iteration: 1010 lambda_k: 1 Loss: 0.10841951561074291\n",
      "Iteration: 1011 lambda_k: 1 Loss: 0.10819103079564943\n",
      "Iteration: 1012 lambda_k: 1 Loss: 0.10796284259615786\n",
      "Iteration: 1013 lambda_k: 1 Loss: 0.10773495131757396\n",
      "Iteration: 1014 lambda_k: 1 Loss: 0.10750735726987663\n",
      "Iteration: 1015 lambda_k: 1 Loss: 0.10728006076773888\n",
      "Iteration: 1016 lambda_k: 1 Loss: 0.10705306213054845\n",
      "Iteration: 1017 lambda_k: 1 Loss: 0.10682636168242916\n",
      "Iteration: 1018 lambda_k: 1 Loss: 0.10659995975226196\n",
      "Iteration: 1019 lambda_k: 1 Loss: 0.10637385667370614\n",
      "Iteration: 1020 lambda_k: 1 Loss: 0.10614805278522095\n",
      "Iteration: 1021 lambda_k: 1 Loss: 0.10592254843008729\n",
      "Iteration: 1022 lambda_k: 1 Loss: 0.1056973439564291\n",
      "Iteration: 1023 lambda_k: 1 Loss: 0.10547243971723574\n",
      "Iteration: 1024 lambda_k: 1 Loss: 0.1052478360703835\n",
      "Iteration: 1025 lambda_k: 1 Loss: 0.10502353337865827\n",
      "Iteration: 1026 lambda_k: 1 Loss: 0.10479953200977757\n",
      "Iteration: 1027 lambda_k: 1 Loss: 0.10457583233641293\n",
      "Iteration: 1028 lambda_k: 1 Loss: 0.10435243473622123\n",
      "Iteration: 1029 lambda_k: 1 Loss: 0.10412933959188253\n",
      "Iteration: 1030 lambda_k: 1 Loss: 0.10390654729102071\n",
      "Iteration: 1031 lambda_k: 1 Loss: 0.10368405822634283\n",
      "Iteration: 1032 lambda_k: 1 Loss: 0.10346187279563351\n",
      "Iteration: 1033 lambda_k: 1 Loss: 0.10323999140177065\n",
      "Iteration: 1034 lambda_k: 1 Loss: 0.103018414452746\n",
      "Iteration: 1035 lambda_k: 1 Loss: 0.10279714236168887\n",
      "Iteration: 1036 lambda_k: 1 Loss: 0.1025761755468906\n",
      "Iteration: 1037 lambda_k: 1 Loss: 0.10235551443182869\n",
      "Iteration: 1038 lambda_k: 1 Loss: 0.10213515944529722\n",
      "Iteration: 1039 lambda_k: 1 Loss: 0.10191511102132127\n",
      "Iteration: 1040 lambda_k: 1 Loss: 0.10169536959872517\n",
      "Iteration: 1041 lambda_k: 1 Loss: 0.1014759356219382\n",
      "Iteration: 1042 lambda_k: 1 Loss: 0.10125680954077372\n",
      "Iteration: 1043 lambda_k: 1 Loss: 0.10103799181038088\n",
      "Iteration: 1044 lambda_k: 1 Loss: 0.10081948289124687\n",
      "Iteration: 1045 lambda_k: 1 Loss: 0.10060128324929252\n",
      "Iteration: 1046 lambda_k: 1 Loss: 0.10038339335575189\n",
      "Iteration: 1047 lambda_k: 1 Loss: 0.10016581368722147\n",
      "Iteration: 1048 lambda_k: 1 Loss: 0.0999485447258334\n",
      "Iteration: 1049 lambda_k: 1 Loss: 0.09973158695921618\n",
      "Iteration: 1050 lambda_k: 1 Loss: 0.09951494088049964\n",
      "Iteration: 1051 lambda_k: 1 Loss: 0.09929860698842007\n",
      "Iteration: 1052 lambda_k: 1 Loss: 0.09908258578723947\n",
      "Iteration: 1053 lambda_k: 1 Loss: 0.09886687778657263\n",
      "Iteration: 1054 lambda_k: 1 Loss: 0.09865148350175892\n",
      "Iteration: 1055 lambda_k: 1 Loss: 0.09843640345381219\n",
      "Iteration: 1056 lambda_k: 1 Loss: 0.09822163816941547\n",
      "Iteration: 1057 lambda_k: 1 Loss: 0.09800718818093208\n",
      "Iteration: 1058 lambda_k: 1 Loss: 0.0977930540264322\n",
      "Iteration: 1059 lambda_k: 1 Loss: 0.09757923624972221\n",
      "Iteration: 1060 lambda_k: 1 Loss: 0.09736573540037138\n",
      "Iteration: 1061 lambda_k: 1 Loss: 0.09715255203374275\n",
      "Iteration: 1062 lambda_k: 1 Loss: 0.09693968671104933\n",
      "Iteration: 1063 lambda_k: 1 Loss: 0.09672713999926492\n",
      "Iteration: 1064 lambda_k: 1 Loss: 0.09651491247126288\n",
      "Iteration: 1065 lambda_k: 1 Loss: 0.09630300469641773\n",
      "Iteration: 1066 lambda_k: 1 Loss: 0.09609141728565004\n",
      "Iteration: 1067 lambda_k: 1 Loss: 0.09588015080785646\n",
      "Iteration: 1068 lambda_k: 1 Loss: 0.09566920586262612\n",
      "Iteration: 1069 lambda_k: 1 Loss: 0.09545848058927402\n",
      "Iteration: 1070 lambda_k: 1 Loss: 0.09524827637917661\n",
      "Iteration: 1071 lambda_k: 1 Loss: 0.09503869229906488\n",
      "Iteration: 1072 lambda_k: 1 Loss: 0.0948294933435799\n",
      "Iteration: 1073 lambda_k: 1 Loss: 0.09462057360128624\n",
      "Iteration: 1074 lambda_k: 1 Loss: 0.09441192568254383\n",
      "Iteration: 1075 lambda_k: 1 Loss: 0.09420357487815326\n",
      "Iteration: 1076 lambda_k: 1 Loss: 0.09399553881374831\n",
      "Iteration: 1077 lambda_k: 1 Loss: 0.09378781890343717\n",
      "Iteration: 1078 lambda_k: 1 Loss: 0.09358040703811861\n",
      "Iteration: 1079 lambda_k: 1 Loss: 0.09337329339557829\n",
      "Iteration: 1080 lambda_k: 1 Loss: 0.0931664701768781\n",
      "Iteration: 1081 lambda_k: 1 Loss: 0.09295993186882842\n",
      "Iteration: 1082 lambda_k: 1 Loss: 0.09275367418979402\n",
      "Iteration: 1083 lambda_k: 1 Loss: 0.09254769317843428\n",
      "Iteration: 1084 lambda_k: 1 Loss: 0.09234198483093876\n",
      "Iteration: 1085 lambda_k: 1 Loss: 0.09213654511424021\n",
      "Iteration: 1086 lambda_k: 1 Loss: 0.09193137008843402\n",
      "Iteration: 1087 lambda_k: 1 Loss: 0.09172645599005924\n",
      "Iteration: 1088 lambda_k: 1 Loss: 0.09152179925118872\n",
      "Iteration: 1089 lambda_k: 1 Loss: 0.09131739647991793\n",
      "Iteration: 1090 lambda_k: 1 Loss: 0.0911132444367978\n",
      "Iteration: 1091 lambda_k: 1 Loss: 0.0909093400132068\n",
      "Iteration: 1092 lambda_k: 1 Loss: 0.09070568024420778\n",
      "Iteration: 1093 lambda_k: 1 Loss: 0.09050226227352562\n",
      "Iteration: 1094 lambda_k: 1 Loss: 0.09029908336672478\n",
      "Iteration: 1095 lambda_k: 1 Loss: 0.09009614090518944\n",
      "Iteration: 1096 lambda_k: 1 Loss: 0.08989343238122177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1097 lambda_k: 1 Loss: 0.08969095539635055\n",
      "Iteration: 1098 lambda_k: 1 Loss: 0.08948870763741446\n",
      "Iteration: 1099 lambda_k: 1 Loss: 0.08928668690122438\n",
      "Iteration: 1100 lambda_k: 1 Loss: 0.089084891071626\n",
      "Iteration: 1101 lambda_k: 1 Loss: 0.08888331811701632\n",
      "Iteration: 1102 lambda_k: 1 Loss: 0.08868196611023012\n",
      "Iteration: 1103 lambda_k: 1 Loss: 0.08848083316614541\n",
      "Iteration: 1104 lambda_k: 1 Loss: 0.08827991749750559\n",
      "Iteration: 1105 lambda_k: 1 Loss: 0.0880792173923202\n",
      "Iteration: 1106 lambda_k: 1 Loss: 0.08787873117681612\n",
      "Iteration: 1107 lambda_k: 1 Loss: 0.08767845725502713\n",
      "Iteration: 1108 lambda_k: 1 Loss: 0.08747803106103158\n",
      "Iteration: 1109 lambda_k: 1 Loss: 0.08727699462470528\n",
      "Iteration: 1110 lambda_k: 1 Loss: 0.0870764883220064\n",
      "Iteration: 1111 lambda_k: 1 Loss: 0.08687635200801742\n",
      "Iteration: 1112 lambda_k: 1 Loss: 0.08667652092867088\n",
      "Iteration: 1113 lambda_k: 1 Loss: 0.08647698474115446\n",
      "Iteration: 1114 lambda_k: 1 Loss: 0.08627774648234718\n",
      "Iteration: 1115 lambda_k: 1 Loss: 0.08607880590709342\n",
      "Iteration: 1116 lambda_k: 1 Loss: 0.0858801572883055\n",
      "Iteration: 1117 lambda_k: 1 Loss: 0.0856817916222188\n",
      "Iteration: 1118 lambda_k: 1 Loss: 0.08548369889040101\n",
      "Iteration: 1119 lambda_k: 1 Loss: 0.0852858696030785\n",
      "Iteration: 1120 lambda_k: 1 Loss: 0.085088295582613\n",
      "Iteration: 1121 lambda_k: 1 Loss: 0.0848909699964668\n",
      "Iteration: 1122 lambda_k: 1 Loss: 0.08469388692110796\n",
      "Iteration: 1123 lambda_k: 1 Loss: 0.08449704090702852\n",
      "Iteration: 1124 lambda_k: 1 Loss: 0.08430042671494907\n",
      "Iteration: 1125 lambda_k: 1 Loss: 0.08410403931206603\n",
      "Iteration: 1126 lambda_k: 1 Loss: 0.08390787395806905\n",
      "Iteration: 1127 lambda_k: 1 Loss: 0.0837119262763693\n",
      "Iteration: 1128 lambda_k: 1 Loss: 0.08351619226673586\n",
      "Iteration: 1129 lambda_k: 1 Loss: 0.08332066827088053\n",
      "Iteration: 1130 lambda_k: 1 Loss: 0.08312535092288055\n",
      "Iteration: 1131 lambda_k: 1 Loss: 0.08293023710856767\n",
      "Iteration: 1132 lambda_k: 1 Loss: 0.08273532394281133\n",
      "Iteration: 1133 lambda_k: 1 Loss: 0.08254060875487755\n",
      "Iteration: 1134 lambda_k: 1 Loss: 0.08234608906856943\n",
      "Iteration: 1135 lambda_k: 1 Loss: 0.0821517626106816\n",
      "Iteration: 1136 lambda_k: 1 Loss: 0.08195762728780105\n",
      "Iteration: 1137 lambda_k: 1 Loss: 0.0817636811699978\n",
      "Iteration: 1138 lambda_k: 1 Loss: 0.08156992247738135\n",
      "Iteration: 1139 lambda_k: 1 Loss: 0.08137634956823236\n",
      "Iteration: 1140 lambda_k: 1 Loss: 0.08118296092840979\n",
      "Iteration: 1141 lambda_k: 1 Loss: 0.080989755161718\n",
      "Iteration: 1142 lambda_k: 1 Loss: 0.08079673098089492\n",
      "Iteration: 1143 lambda_k: 1 Loss: 0.08060388719905154\n",
      "Iteration: 1144 lambda_k: 1 Loss: 0.08041122272157483\n",
      "Iteration: 1145 lambda_k: 1 Loss: 0.08021873653857862\n",
      "Iteration: 1146 lambda_k: 1 Loss: 0.08002642771794621\n",
      "Iteration: 1147 lambda_k: 1 Loss: 0.07983429539894532\n",
      "Iteration: 1148 lambda_k: 1 Loss: 0.07964233878635527\n",
      "Iteration: 1149 lambda_k: 1 Loss: 0.0794505571450455\n",
      "Iteration: 1150 lambda_k: 1 Loss: 0.07925894979495786\n",
      "Iteration: 1151 lambda_k: 1 Loss: 0.07906751610646487\n",
      "Iteration: 1152 lambda_k: 1 Loss: 0.07887625549608215\n",
      "Iteration: 1153 lambda_k: 1 Loss: 0.07868516742251598\n",
      "Iteration: 1154 lambda_k: 1 Loss: 0.07849425138302425\n",
      "Iteration: 1155 lambda_k: 1 Loss: 0.07830350691006797\n",
      "Iteration: 1156 lambda_k: 1 Loss: 0.07811293356823032\n",
      "Iteration: 1157 lambda_k: 1 Loss: 0.07792253095138264\n",
      "Iteration: 1158 lambda_k: 1 Loss: 0.07773229868007872\n",
      "Iteration: 1159 lambda_k: 1 Loss: 0.0775422363991597\n",
      "Iteration: 1160 lambda_k: 1 Loss: 0.07735234377555505\n",
      "Iteration: 1161 lambda_k: 1 Loss: 0.07716262049626373\n",
      "Iteration: 1162 lambda_k: 1 Loss: 0.07697306626650337\n",
      "Iteration: 1163 lambda_k: 1 Loss: 0.07678368080801264\n",
      "Iteration: 1164 lambda_k: 1 Loss: 0.07659446385749634\n",
      "Iteration: 1165 lambda_k: 1 Loss: 0.07640541516520155\n",
      "Iteration: 1166 lambda_k: 1 Loss: 0.07621653449361422\n",
      "Iteration: 1167 lambda_k: 1 Loss: 0.07602782161626716\n",
      "Iteration: 1168 lambda_k: 1 Loss: 0.07583927631665062\n",
      "Iteration: 1169 lambda_k: 1 Loss: 0.07565089838721697\n",
      "Iteration: 1170 lambda_k: 1 Loss: 0.07546268762847243\n",
      "Iteration: 1171 lambda_k: 1 Loss: 0.07527464384814812\n",
      "Iteration: 1172 lambda_k: 1 Loss: 0.07508676686044459\n",
      "Iteration: 1173 lambda_k: 1 Loss: 0.07489905648534412\n",
      "Iteration: 1174 lambda_k: 1 Loss: 0.07471151254798411\n",
      "Iteration: 1175 lambda_k: 1 Loss: 0.07452413487808793\n",
      "Iteration: 1176 lambda_k: 1 Loss: 0.07433692330944737\n",
      "Iteration: 1177 lambda_k: 1 Loss: 0.07414987767945305\n",
      "Iteration: 1178 lambda_k: 1 Loss: 0.07396299782866864\n",
      "Iteration: 1179 lambda_k: 1 Loss: 0.0737762836004448\n",
      "Iteration: 1180 lambda_k: 1 Loss: 0.07358973484057076\n",
      "Iteration: 1181 lambda_k: 1 Loss: 0.07340335139695821\n",
      "Iteration: 1182 lambda_k: 1 Loss: 0.07321713311935721\n",
      "Iteration: 1183 lambda_k: 1 Loss: 0.07303107985909914\n",
      "Iteration: 1184 lambda_k: 1 Loss: 0.07284519146886619\n",
      "Iteration: 1185 lambda_k: 1 Loss: 0.07265946780248371\n",
      "Iteration: 1186 lambda_k: 1 Loss: 0.07247390871473415\n",
      "Iteration: 1187 lambda_k: 1 Loss: 0.07228851406119098\n",
      "Iteration: 1188 lambda_k: 1 Loss: 0.0721032836980695\n",
      "Iteration: 1189 lambda_k: 1 Loss: 0.07191821748209472\n",
      "Iteration: 1190 lambda_k: 1 Loss: 0.07173331527038343\n",
      "Iteration: 1191 lambda_k: 1 Loss: 0.07154857692034018\n",
      "Iteration: 1192 lambda_k: 1 Loss: 0.07136400228956502\n",
      "Iteration: 1193 lambda_k: 1 Loss: 0.07117959123577262\n",
      "Iteration: 1194 lambda_k: 1 Loss: 0.07099534361672122\n",
      "Iteration: 1195 lambda_k: 1 Loss: 0.07081125929015103\n",
      "Iteration: 1196 lambda_k: 1 Loss: 0.07062733811373043\n",
      "Iteration: 1197 lambda_k: 1 Loss: 0.07044357994501022\n",
      "Iteration: 1198 lambda_k: 1 Loss: 0.07025998464138378\n",
      "Iteration: 1199 lambda_k: 1 Loss: 0.07007655206005431\n",
      "Iteration: 1200 lambda_k: 1 Loss: 0.06989328205800668\n",
      "Iteration: 1201 lambda_k: 1 Loss: 0.069710174491985\n",
      "Iteration: 1202 lambda_k: 1 Loss: 0.06952722921847364\n",
      "Iteration: 1203 lambda_k: 1 Loss: 0.06934444609368337\n",
      "Iteration: 1204 lambda_k: 1 Loss: 0.06916182497353995\n",
      "Iteration: 1205 lambda_k: 1 Loss: 0.06897936571367698\n",
      "Iteration: 1206 lambda_k: 1 Loss: 0.06879706816943049\n",
      "Iteration: 1207 lambda_k: 1 Loss: 0.06861493219583699\n",
      "Iteration: 1208 lambda_k: 1 Loss: 0.06843295764763307\n",
      "Iteration: 1209 lambda_k: 1 Loss: 0.06825114437925733\n",
      "Iteration: 1210 lambda_k: 1 Loss: 0.06806949224485374\n",
      "Iteration: 1211 lambda_k: 1 Loss: 0.06788800109827708\n",
      "Iteration: 1212 lambda_k: 1 Loss: 0.0677066707930991\n",
      "Iteration: 1213 lambda_k: 1 Loss: 0.06752550118261597\n",
      "Iteration: 1214 lambda_k: 1 Loss: 0.06734449211985687\n",
      "Iteration: 1215 lambda_k: 1 Loss: 0.0671636434575934\n",
      "Iteration: 1216 lambda_k: 1 Loss: 0.06698295504834967\n",
      "Iteration: 1217 lambda_k: 1 Loss: 0.06680242674441281\n",
      "Iteration: 1218 lambda_k: 1 Loss: 0.066622058397844\n",
      "Iteration: 1219 lambda_k: 1 Loss: 0.0664418498604905\n",
      "Iteration: 1220 lambda_k: 1 Loss: 0.06626180098399681\n",
      "Iteration: 1221 lambda_k: 1 Loss: 0.06608191161981747\n",
      "Iteration: 1222 lambda_k: 1 Loss: 0.06590218161922849\n",
      "Iteration: 1223 lambda_k: 1 Loss: 0.0657226108333405\n",
      "Iteration: 1224 lambda_k: 1 Loss: 0.06554319911311042\n",
      "Iteration: 1225 lambda_k: 1 Loss: 0.06536394630935438\n",
      "Iteration: 1226 lambda_k: 1 Loss: 0.0651848522727598\n",
      "Iteration: 1227 lambda_k: 1 Loss: 0.06500591685389756\n",
      "Iteration: 1228 lambda_k: 1 Loss: 0.06482713990323459\n",
      "Iteration: 1229 lambda_k: 1 Loss: 0.06464852127114531\n",
      "Iteration: 1230 lambda_k: 1 Loss: 0.06447006080792402\n",
      "Iteration: 1231 lambda_k: 1 Loss: 0.06429175836379614\n",
      "Iteration: 1232 lambda_k: 1 Loss: 0.06411361378892998\n",
      "Iteration: 1233 lambda_k: 1 Loss: 0.06393562693344794\n",
      "Iteration: 1234 lambda_k: 1 Loss: 0.06375779764743725\n",
      "Iteration: 1235 lambda_k: 1 Loss: 0.06358012578096112\n",
      "Iteration: 1236 lambda_k: 1 Loss: 0.06340261118406908\n",
      "Iteration: 1237 lambda_k: 1 Loss: 0.06322525370680736\n",
      "Iteration: 1238 lambda_k: 1 Loss: 0.0630480531992286\n",
      "Iteration: 1239 lambda_k: 1 Loss: 0.0628710095114021\n",
      "Iteration: 1240 lambda_k: 1 Loss: 0.06269412249342288\n",
      "Iteration: 1241 lambda_k: 1 Loss: 0.06251739199542095\n",
      "Iteration: 1242 lambda_k: 1 Loss: 0.06234081786757033\n",
      "Iteration: 1243 lambda_k: 1 Loss: 0.0621643999600979\n",
      "Iteration: 1244 lambda_k: 1 Loss: 0.06198813812329134\n",
      "Iteration: 1245 lambda_k: 1 Loss: 0.06181203220750774\n",
      "Iteration: 1246 lambda_k: 1 Loss: 0.061636082063181254\n",
      "Iteration: 1247 lambda_k: 1 Loss: 0.06146028754083066\n",
      "Iteration: 1248 lambda_k: 1 Loss: 0.06128464849106675\n",
      "Iteration: 1249 lambda_k: 1 Loss: 0.06110916476459959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1250 lambda_k: 1 Loss: 0.06093383621224525\n",
      "Iteration: 1251 lambda_k: 1 Loss: 0.06075866268493251\n",
      "Iteration: 1252 lambda_k: 1 Loss: 0.060583644033709125\n",
      "Iteration: 1253 lambda_k: 1 Loss: 0.06040878010974805\n",
      "Iteration: 1254 lambda_k: 1 Loss: 0.060234070764353435\n",
      "Iteration: 1255 lambda_k: 1 Loss: 0.06005951584896631\n",
      "Iteration: 1256 lambda_k: 1 Loss: 0.05988511521516995\n",
      "Iteration: 1257 lambda_k: 1 Loss: 0.0597108687146953\n",
      "Iteration: 1258 lambda_k: 1 Loss: 0.059536776199426135\n",
      "Iteration: 1259 lambda_k: 1 Loss: 0.0593628375214037\n",
      "Iteration: 1260 lambda_k: 1 Loss: 0.05918905253283182\n",
      "Iteration: 1261 lambda_k: 1 Loss: 0.059015421086080766\n",
      "Iteration: 1262 lambda_k: 1 Loss: 0.05884194303369219\n",
      "Iteration: 1263 lambda_k: 1 Loss: 0.05866861822838292\n",
      "Iteration: 1264 lambda_k: 1 Loss: 0.05849544652304929\n",
      "Iteration: 1265 lambda_k: 1 Loss: 0.05832242777077071\n",
      "Iteration: 1266 lambda_k: 1 Loss: 0.05814956182481343\n",
      "Iteration: 1267 lambda_k: 1 Loss: 0.057976848538633864\n",
      "Iteration: 1268 lambda_k: 1 Loss: 0.057804287765882326\n",
      "Iteration: 1269 lambda_k: 1 Loss: 0.05763187936040608\n",
      "Iteration: 1270 lambda_k: 1 Loss: 0.05745962317625211\n",
      "Iteration: 1271 lambda_k: 1 Loss: 0.05728751906767074\n",
      "Iteration: 1272 lambda_k: 1 Loss: 0.05711556688911792\n",
      "Iteration: 1273 lambda_k: 1 Loss: 0.056943766495258175\n",
      "Iteration: 1274 lambda_k: 1 Loss: 0.0567721177409673\n",
      "Iteration: 1275 lambda_k: 1 Loss: 0.056600620481334366\n",
      "Iteration: 1276 lambda_k: 1 Loss: 0.05642927457166472\n",
      "Iteration: 1277 lambda_k: 1 Loss: 0.05625807986748182\n",
      "Iteration: 1278 lambda_k: 1 Loss: 0.05608703622452962\n",
      "Iteration: 1279 lambda_k: 1 Loss: 0.05591614349877443\n",
      "Iteration: 1280 lambda_k: 1 Loss: 0.05574540154640727\n",
      "Iteration: 1281 lambda_k: 1 Loss: 0.055574810223845215\n",
      "Iteration: 1282 lambda_k: 1 Loss: 0.055404369387733646\n",
      "Iteration: 1283 lambda_k: 1 Loss: 0.05523407889494791\n",
      "Iteration: 1284 lambda_k: 1 Loss: 0.055063938602594745\n",
      "Iteration: 1285 lambda_k: 1 Loss: 0.05489394836801411\n",
      "Iteration: 1286 lambda_k: 1 Loss: 0.05472410804878045\n",
      "Iteration: 1287 lambda_k: 1 Loss: 0.0545544175027044\n",
      "Iteration: 1288 lambda_k: 1 Loss: 0.05438487658783382\n",
      "Iteration: 1289 lambda_k: 1 Loss: 0.054215485162455425\n",
      "Iteration: 1290 lambda_k: 1 Loss: 0.05404624308509598\n",
      "Iteration: 1291 lambda_k: 1 Loss: 0.053877150214523155\n",
      "Iteration: 1292 lambda_k: 1 Loss: 0.05370820640974707\n",
      "Iteration: 1293 lambda_k: 1 Loss: 0.05353941153002121\n",
      "Iteration: 1294 lambda_k: 1 Loss: 0.0533707654348436\n",
      "Iteration: 1295 lambda_k: 1 Loss: 0.0532022679839575\n",
      "Iteration: 1296 lambda_k: 1 Loss: 0.053033919037352564\n",
      "Iteration: 1297 lambda_k: 1 Loss: 0.052865718455265824\n",
      "Iteration: 1298 lambda_k: 1 Loss: 0.05269766609818255\n",
      "Iteration: 1299 lambda_k: 1 Loss: 0.0525297618268367\n",
      "Iteration: 1300 lambda_k: 1 Loss: 0.05236200550221235\n",
      "Iteration: 1301 lambda_k: 1 Loss: 0.05219439698554396\n",
      "Iteration: 1302 lambda_k: 1 Loss: 0.05202693613831742\n",
      "Iteration: 1303 lambda_k: 1 Loss: 0.051859622822270475\n",
      "Iteration: 1304 lambda_k: 1 Loss: 0.05169245689939366\n",
      "Iteration: 1305 lambda_k: 1 Loss: 0.05152543823193082\n",
      "Iteration: 1306 lambda_k: 1 Loss: 0.051358566682379876\n",
      "Iteration: 1307 lambda_k: 1 Loss: 0.05119184211349327\n",
      "Iteration: 1308 lambda_k: 1 Loss: 0.05102526438827867\n",
      "Iteration: 1309 lambda_k: 1 Loss: 0.050858833369999655\n",
      "Iteration: 1310 lambda_k: 1 Loss: 0.05069254892217588\n",
      "Iteration: 1311 lambda_k: 1 Loss: 0.05052641090858418\n",
      "Iteration: 1312 lambda_k: 1 Loss: 0.05036041919325866\n",
      "Iteration: 1313 lambda_k: 1 Loss: 0.05019457364049138\n",
      "Iteration: 1314 lambda_k: 1 Loss: 0.05002887411483298\n",
      "Iteration: 1315 lambda_k: 1 Loss: 0.04986332048109296\n",
      "Iteration: 1316 lambda_k: 1 Loss: 0.049697912604340165\n",
      "Iteration: 1317 lambda_k: 1 Loss: 0.04953265034990347\n",
      "Iteration: 1318 lambda_k: 1 Loss: 0.04936753358337225\n",
      "Iteration: 1319 lambda_k: 1 Loss: 0.04920256217059676\n",
      "Iteration: 1320 lambda_k: 1 Loss: 0.04903773597768851\n",
      "Iteration: 1321 lambda_k: 1 Loss: 0.04887305487102096\n",
      "Iteration: 1322 lambda_k: 1 Loss: 0.04870851871722983\n",
      "Iteration: 1323 lambda_k: 1 Loss: 0.04854412738321352\n",
      "Iteration: 1324 lambda_k: 1 Loss: 0.048379880736133966\n",
      "Iteration: 1325 lambda_k: 1 Loss: 0.048215778643416585\n",
      "Iteration: 1326 lambda_k: 1 Loss: 0.04805182097275099\n",
      "Iteration: 1327 lambda_k: 1 Loss: 0.04788800759209174\n",
      "Iteration: 1328 lambda_k: 1 Loss: 0.04772433836965827\n",
      "Iteration: 1329 lambda_k: 1 Loss: 0.047560813173935845\n",
      "Iteration: 1330 lambda_k: 1 Loss: 0.04739743187367597\n",
      "Iteration: 1331 lambda_k: 1 Loss: 0.04723419433789665\n",
      "Iteration: 1332 lambda_k: 1 Loss: 0.04707110043588321\n",
      "Iteration: 1333 lambda_k: 1 Loss: 0.046908150037188706\n",
      "Iteration: 1334 lambda_k: 1 Loss: 0.04674534301163436\n",
      "Iteration: 1335 lambda_k: 1 Loss: 0.04658267922931032\n",
      "Iteration: 1336 lambda_k: 1 Loss: 0.046420158560576165\n",
      "Iteration: 1337 lambda_k: 1 Loss: 0.046257780876061305\n",
      "Iteration: 1338 lambda_k: 1 Loss: 0.046095546046665645\n",
      "Iteration: 1339 lambda_k: 1 Loss: 0.0459334539435605\n",
      "Iteration: 1340 lambda_k: 1 Loss: 0.045771504438188886\n",
      "Iteration: 1341 lambda_k: 1 Loss: 0.04560969740226615\n",
      "Iteration: 1342 lambda_k: 1 Loss: 0.045448032707780785\n",
      "Iteration: 1343 lambda_k: 1 Loss: 0.04528651022699501\n",
      "Iteration: 1344 lambda_k: 1 Loss: 0.04512512983244556\n",
      "Iteration: 1345 lambda_k: 1 Loss: 0.04496389139694433\n",
      "Iteration: 1346 lambda_k: 1 Loss: 0.04480279479357913\n",
      "Iteration: 1347 lambda_k: 1 Loss: 0.04464183989571454\n",
      "Iteration: 1348 lambda_k: 1 Loss: 0.044481026576992364\n",
      "Iteration: 1349 lambda_k: 1 Loss: 0.044320354711332915\n",
      "Iteration: 1350 lambda_k: 1 Loss: 0.04415982417293546\n",
      "Iteration: 1351 lambda_k: 1 Loss: 0.043999434836279355\n",
      "Iteration: 1352 lambda_k: 1 Loss: 0.043839186576124595\n",
      "Iteration: 1353 lambda_k: 1 Loss: 0.04367907926751303\n",
      "Iteration: 1354 lambda_k: 1 Loss: 0.04351911278576926\n",
      "Iteration: 1355 lambda_k: 1 Loss: 0.043359287006501476\n",
      "Iteration: 1356 lambda_k: 1 Loss: 0.043199601805602436\n",
      "Iteration: 1357 lambda_k: 1 Loss: 0.04304005705925069\n",
      "Iteration: 1358 lambda_k: 1 Loss: 0.04288065264391149\n",
      "Iteration: 1359 lambda_k: 1 Loss: 0.0427213884363379\n",
      "Iteration: 1360 lambda_k: 1 Loss: 0.042562264313571865\n",
      "Iteration: 1361 lambda_k: 1 Loss: 0.04240328015294552\n",
      "Iteration: 1362 lambda_k: 1 Loss: 0.0422444358320823\n",
      "Iteration: 1363 lambda_k: 1 Loss: 0.042085731228898075\n",
      "Iteration: 1364 lambda_k: 1 Loss: 0.04192716622160256\n",
      "Iteration: 1365 lambda_k: 1 Loss: 0.0417687406887006\n",
      "Iteration: 1366 lambda_k: 1 Loss: 0.04161045450899335\n",
      "Iteration: 1367 lambda_k: 1 Loss: 0.04145230756157996\n",
      "Iteration: 1368 lambda_k: 1 Loss: 0.04129429972585858\n",
      "Iteration: 1369 lambda_k: 1 Loss: 0.04113643088152831\n",
      "Iteration: 1370 lambda_k: 1 Loss: 0.04097870090859037\n",
      "Iteration: 1371 lambda_k: 1 Loss: 0.040821109687349734\n",
      "Iteration: 1372 lambda_k: 1 Loss: 0.04066365709841672\n",
      "Iteration: 1373 lambda_k: 1 Loss: 0.04050634302270873\n",
      "Iteration: 1374 lambda_k: 1 Loss: 0.040349167341451754\n",
      "Iteration: 1375 lambda_k: 1 Loss: 0.040192129936182354\n",
      "Iteration: 1376 lambda_k: 1 Loss: 0.040035230688749285\n",
      "Iteration: 1377 lambda_k: 1 Loss: 0.039878469481315466\n",
      "Iteration: 1378 lambda_k: 1 Loss: 0.03972184619635958\n",
      "Iteration: 1379 lambda_k: 1 Loss: 0.03956536071667845\n",
      "Iteration: 1380 lambda_k: 1 Loss: 0.03940901292538886\n",
      "Iteration: 1381 lambda_k: 1 Loss: 0.03925280270592938\n",
      "Iteration: 1382 lambda_k: 1 Loss: 0.03909672994206285\n",
      "Iteration: 1383 lambda_k: 1 Loss: 0.03894079451787845\n",
      "Iteration: 1384 lambda_k: 1 Loss: 0.03878499631779376\n",
      "Iteration: 1385 lambda_k: 1 Loss: 0.03862933522655731\n",
      "Iteration: 1386 lambda_k: 1 Loss: 0.03847381112925095\n",
      "Iteration: 1387 lambda_k: 1 Loss: 0.038318423911291914\n",
      "Iteration: 1388 lambda_k: 1 Loss: 0.03816317345843579\n",
      "Iteration: 1389 lambda_k: 1 Loss: 0.038008059656778864\n",
      "Iteration: 1390 lambda_k: 1 Loss: 0.037853082392760876\n",
      "Iteration: 1391 lambda_k: 1 Loss: 0.03769824155316742\n",
      "Iteration: 1392 lambda_k: 1 Loss: 0.03754353702513319\n",
      "Iteration: 1393 lambda_k: 1 Loss: 0.037388968696144644\n",
      "Iteration: 1394 lambda_k: 1 Loss: 0.03723453645404269\n",
      "Iteration: 1395 lambda_k: 1 Loss: 0.03708024018702626\n",
      "Iteration: 1396 lambda_k: 1 Loss: 0.03692607978365486\n",
      "Iteration: 1397 lambda_k: 1 Loss: 0.036772055132852165\n",
      "Iteration: 1398 lambda_k: 1 Loss: 0.03661816612390919\n",
      "Iteration: 1399 lambda_k: 1 Loss: 0.036464412646487504\n",
      "Iteration: 1400 lambda_k: 1 Loss: 0.03631079459062284\n",
      "Iteration: 1401 lambda_k: 1 Loss: 0.03615731184672874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1402 lambda_k: 1 Loss: 0.03600396430559999\n",
      "Iteration: 1403 lambda_k: 1 Loss: 0.03585075185841653\n",
      "Iteration: 1404 lambda_k: 1 Loss: 0.035697674396747155\n",
      "Iteration: 1405 lambda_k: 1 Loss: 0.035544731812553715\n",
      "Iteration: 1406 lambda_k: 1 Loss: 0.035391923998194914\n",
      "Iteration: 1407 lambda_k: 1 Loss: 0.03523925084643062\n",
      "Iteration: 1408 lambda_k: 1 Loss: 0.0350867122504263\n",
      "Iteration: 1409 lambda_k: 1 Loss: 0.034934308103757174\n",
      "Iteration: 1410 lambda_k: 1 Loss: 0.03478203830041269\n",
      "Iteration: 1411 lambda_k: 1 Loss: 0.03462990273480151\n",
      "Iteration: 1412 lambda_k: 1 Loss: 0.034477901301755966\n",
      "Iteration: 1413 lambda_k: 1 Loss: 0.03432603389653709\n",
      "Iteration: 1414 lambda_k: 1 Loss: 0.034174300414839764\n",
      "Iteration: 1415 lambda_k: 1 Loss: 0.034022700752797584\n",
      "Iteration: 1416 lambda_k: 1 Loss: 0.03387123480698848\n",
      "Iteration: 1417 lambda_k: 1 Loss: 0.03371990247443993\n",
      "Iteration: 1418 lambda_k: 1 Loss: 0.033568703652634764\n",
      "Iteration: 1419 lambda_k: 1 Loss: 0.033417638239516746\n",
      "Iteration: 1420 lambda_k: 1 Loss: 0.03326670613349671\n",
      "Iteration: 1421 lambda_k: 1 Loss: 0.03311590723345823\n",
      "Iteration: 1422 lambda_k: 1 Loss: 0.0329652414387644\n",
      "Iteration: 1423 lambda_k: 1 Loss: 0.03281470864926387\n",
      "Iteration: 1424 lambda_k: 1 Loss: 0.032664308765297384\n",
      "Iteration: 1425 lambda_k: 1 Loss: 0.032514041687704894\n",
      "Iteration: 1426 lambda_k: 1 Loss: 0.03236390731783218\n",
      "Iteration: 1427 lambda_k: 1 Loss: 0.03221390555753825\n",
      "Iteration: 1428 lambda_k: 1 Loss: 0.03206403630920242\n",
      "Iteration: 1429 lambda_k: 1 Loss: 0.03191429947573211\n",
      "Iteration: 1430 lambda_k: 1 Loss: 0.03176469496057038\n",
      "Iteration: 1431 lambda_k: 1 Loss: 0.031615222667704064\n",
      "Iteration: 1432 lambda_k: 1 Loss: 0.0314658825016718\n",
      "Iteration: 1433 lambda_k: 1 Loss: 0.03131667436757257\n",
      "Iteration: 1434 lambda_k: 1 Loss: 0.03116759817107444\n",
      "Iteration: 1435 lambda_k: 1 Loss: 0.031018653818423184\n",
      "Iteration: 1436 lambda_k: 1 Loss: 0.030869841216451675\n",
      "Iteration: 1437 lambda_k: 1 Loss: 0.030721160272589094\n",
      "Iteration: 1438 lambda_k: 1 Loss: 0.03057261089487078\n",
      "Iteration: 1439 lambda_k: 1 Loss: 0.030424192991948144\n",
      "Iteration: 1440 lambda_k: 1 Loss: 0.030275906473098645\n",
      "Iteration: 1441 lambda_k: 1 Loss: 0.030127751248236526\n",
      "Iteration: 1442 lambda_k: 1 Loss: 0.029979727227923803\n",
      "Iteration: 1443 lambda_k: 1 Loss: 0.029831834323380976\n",
      "Iteration: 1444 lambda_k: 1 Loss: 0.02968407244649868\n",
      "Iteration: 1445 lambda_k: 1 Loss: 0.02953644150984959\n",
      "Iteration: 1446 lambda_k: 1 Loss: 0.029388941426700153\n",
      "Iteration: 1447 lambda_k: 1 Loss: 0.029241572111023545\n",
      "Iteration: 1448 lambda_k: 1 Loss: 0.02909433347751203\n",
      "Iteration: 1449 lambda_k: 1 Loss: 0.028947225441590535\n",
      "Iteration: 1450 lambda_k: 1 Loss: 0.02880024791942991\n",
      "Iteration: 1451 lambda_k: 1 Loss: 0.028653400827961154\n",
      "Iteration: 1452 lambda_k: 1 Loss: 0.028506684084889693\n",
      "Iteration: 1453 lambda_k: 1 Loss: 0.028360097608710246\n",
      "Iteration: 1454 lambda_k: 1 Loss: 0.0282136413187221\n",
      "Iteration: 1455 lambda_k: 1 Loss: 0.028067315135044655\n",
      "Iteration: 1456 lambda_k: 1 Loss: 0.027921118978634012\n",
      "Iteration: 1457 lambda_k: 1 Loss: 0.02777505277129904\n",
      "Iteration: 1458 lambda_k: 1 Loss: 0.027629116435719223\n",
      "Iteration: 1459 lambda_k: 1 Loss: 0.027483309895461855\n",
      "Iteration: 1460 lambda_k: 1 Loss: 0.02733763307500043\n",
      "Iteration: 1461 lambda_k: 1 Loss: 0.02719208589973359\n",
      "Iteration: 1462 lambda_k: 1 Loss: 0.027046668296004064\n",
      "Iteration: 1463 lambda_k: 1 Loss: 0.026901380191119244\n",
      "Iteration: 1464 lambda_k: 1 Loss: 0.026756221513371165\n",
      "Iteration: 1465 lambda_k: 1 Loss: 0.02661119219205793\n",
      "Iteration: 1466 lambda_k: 1 Loss: 0.026466292157505872\n",
      "Iteration: 1467 lambda_k: 1 Loss: 0.02632152134109172\n",
      "Iteration: 1468 lambda_k: 1 Loss: 0.02617687967526605\n",
      "Iteration: 1469 lambda_k: 1 Loss: 0.02603236709357712\n",
      "Iteration: 1470 lambda_k: 1 Loss: 0.025887983530695775\n",
      "Iteration: 1471 lambda_k: 1 Loss: 0.0257437289224411\n",
      "Iteration: 1472 lambda_k: 1 Loss: 0.02559960320580651\n",
      "Iteration: 1473 lambda_k: 1 Loss: 0.025455606318987134\n",
      "Iteration: 1474 lambda_k: 1 Loss: 0.025311738201407957\n",
      "Iteration: 1475 lambda_k: 1 Loss: 0.025167998793752602\n",
      "Iteration: 1476 lambda_k: 1 Loss: 0.025024388037993592\n",
      "Iteration: 1477 lambda_k: 1 Loss: 0.024880905877423\n",
      "Iteration: 1478 lambda_k: 1 Loss: 0.024737552256684423\n",
      "Iteration: 1479 lambda_k: 1 Loss: 0.024594327121806064\n",
      "Iteration: 1480 lambda_k: 1 Loss: 0.024451230420234973\n",
      "Iteration: 1481 lambda_k: 1 Loss: 0.02430826210087193\n",
      "Iteration: 1482 lambda_k: 1 Loss: 0.024165422114108173\n",
      "Iteration: 1483 lambda_k: 1 Loss: 0.024022710411862807\n",
      "Iteration: 1484 lambda_k: 1 Loss: 0.023880126947622003\n",
      "Iteration: 1485 lambda_k: 1 Loss: 0.023737671676478958\n",
      "Iteration: 1486 lambda_k: 1 Loss: 0.02359534455517572\n",
      "Iteration: 1487 lambda_k: 1 Loss: 0.02345314554214614\n",
      "Iteration: 1488 lambda_k: 1 Loss: 0.023311074597560398\n",
      "Iteration: 1489 lambda_k: 1 Loss: 0.023169131683371282\n",
      "Iteration: 1490 lambda_k: 1 Loss: 0.023027316763361682\n",
      "Iteration: 1491 lambda_k: 1 Loss: 0.022885629803194016\n",
      "Iteration: 1492 lambda_k: 1 Loss: 0.022744070770461533\n",
      "Iteration: 1493 lambda_k: 1 Loss: 0.02260263963474126\n",
      "Iteration: 1494 lambda_k: 1 Loss: 0.022461336367648466\n",
      "Iteration: 1495 lambda_k: 1 Loss: 0.022320160942894005\n",
      "Iteration: 1496 lambda_k: 1 Loss: 0.022179113336342933\n",
      "Iteration: 1497 lambda_k: 1 Loss: 0.022038193526075344\n",
      "Iteration: 1498 lambda_k: 1 Loss: 0.02189740149245001\n",
      "Iteration: 1499 lambda_k: 1 Loss: 0.021756737218169703\n",
      "Iteration: 1500 lambda_k: 1 Loss: 0.021616200688349286\n",
      "Iteration: 1501 lambda_k: 1 Loss: 0.021475791890586158\n",
      "Iteration: 1502 lambda_k: 1 Loss: 0.02133551081503341\n",
      "Iteration: 1503 lambda_k: 1 Loss: 0.021195357454476104\n",
      "Iteration: 1504 lambda_k: 1 Loss: 0.02105533180440946\n",
      "Iteration: 1505 lambda_k: 1 Loss: 0.020915433863121245\n",
      "Iteration: 1506 lambda_k: 1 Loss: 0.02077566363177618\n",
      "Iteration: 1507 lambda_k: 1 Loss: 0.020636021114504806\n",
      "Iteration: 1508 lambda_k: 1 Loss: 0.020496506318494447\n",
      "Iteration: 1509 lambda_k: 1 Loss: 0.02035711925408494\n",
      "Iteration: 1510 lambda_k: 1 Loss: 0.020217859934867355\n",
      "Iteration: 1511 lambda_k: 1 Loss: 0.020078728377786837\n",
      "Iteration: 1512 lambda_k: 1 Loss: 0.01993972460324975\n",
      "Iteration: 1513 lambda_k: 1 Loss: 0.01980084863523495\n",
      "Iteration: 1514 lambda_k: 1 Loss: 0.019662100501409232\n",
      "Iteration: 1515 lambda_k: 1 Loss: 0.01952348023324805\n",
      "Iteration: 1516 lambda_k: 1 Loss: 0.019384987866160802\n",
      "Iteration: 1517 lambda_k: 1 Loss: 0.01924662343962126\n",
      "Iteration: 1518 lambda_k: 1 Loss: 0.01910838699730363\n",
      "Iteration: 1519 lambda_k: 1 Loss: 0.018970278587223947\n",
      "Iteration: 1520 lambda_k: 1 Loss: 0.01883229826188769\n",
      "Iteration: 1521 lambda_k: 1 Loss: 0.018694446078443623\n",
      "Iteration: 1522 lambda_k: 1 Loss: 0.018556722098843802\n",
      "Iteration: 1523 lambda_k: 1 Loss: 0.01841912639001087\n",
      "Iteration: 1524 lambda_k: 1 Loss: 0.01828165902401238\n",
      "Iteration: 1525 lambda_k: 1 Loss: 0.018144320078242644\n",
      "Iteration: 1526 lambda_k: 1 Loss: 0.018007109635612757\n",
      "Iteration: 1527 lambda_k: 1 Loss: 0.017870027784748367\n",
      "Iteration: 1528 lambda_k: 1 Loss: 0.017733074620197284\n",
      "Iteration: 1529 lambda_k: 1 Loss: 0.01759625024264523\n",
      "Iteration: 1530 lambda_k: 1 Loss: 0.01745955475914195\n",
      "Iteration: 1531 lambda_k: 1 Loss: 0.017322988283337386\n",
      "Iteration: 1532 lambda_k: 1 Loss: 0.017186550935728632\n",
      "Iteration: 1533 lambda_k: 1 Loss: 0.01705024284391848\n",
      "Iteration: 1534 lambda_k: 1 Loss: 0.01691406414288534\n",
      "Iteration: 1535 lambda_k: 1 Loss: 0.016778014975266264\n",
      "Iteration: 1536 lambda_k: 1 Loss: 0.01664209549165323\n",
      "Iteration: 1537 lambda_k: 1 Loss: 0.01650630585090342\n",
      "Iteration: 1538 lambda_k: 1 Loss: 0.016370646220463962\n",
      "Iteration: 1539 lambda_k: 1 Loss: 0.01623511677671297\n",
      "Iteration: 1540 lambda_k: 1 Loss: 0.016099717705316447\n",
      "Iteration: 1541 lambda_k: 1 Loss: 0.015964449201602976\n",
      "Iteration: 1542 lambda_k: 1 Loss: 0.015829311470956887\n",
      "Iteration: 1543 lambda_k: 1 Loss: 0.01569430472923099\n",
      "Iteration: 1544 lambda_k: 1 Loss: 0.015559429203179475\n",
      "Iteration: 1545 lambda_k: 1 Loss: 0.015424685130913333\n",
      "Iteration: 1546 lambda_k: 1 Loss: 0.015290072762378431\n",
      "Iteration: 1547 lambda_k: 1 Loss: 0.01515559235985828\n",
      "Iteration: 1548 lambda_k: 1 Loss: 0.015021244198503059\n",
      "Iteration: 1549 lambda_k: 1 Loss: 0.014887028566885802\n",
      "Iteration: 1550 lambda_k: 1 Loss: 0.014752945767587893\n",
      "Iteration: 1551 lambda_k: 1 Loss: 0.014618996117815933\n",
      "Iteration: 1552 lambda_k: 1 Loss: 0.014485179950051255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1553 lambda_k: 1 Loss: 0.014351497612734684\n",
      "Iteration: 1554 lambda_k: 1 Loss: 0.01421794947098844\n",
      "Iteration: 1555 lambda_k: 1 Loss: 0.014084535907377838\n",
      "Iteration: 1556 lambda_k: 1 Loss: 0.013951257322714766\n",
      "Iteration: 1557 lambda_k: 1 Loss: 0.013818114136906761\n",
      "Iteration: 1558 lambda_k: 1 Loss: 0.0136851067898533\n",
      "Iteration: 1559 lambda_k: 1 Loss: 0.013552235742393216\n",
      "Iteration: 1560 lambda_k: 1 Loss: 0.01341950147730703\n",
      "Iteration: 1561 lambda_k: 1 Loss: 0.013286904500376534\n",
      "Iteration: 1562 lambda_k: 1 Loss: 0.013154445341507144\n",
      "Iteration: 1563 lambda_k: 1 Loss: 0.013022124555915825\n",
      "Iteration: 1564 lambda_k: 1 Loss: 0.012889942725390546\n",
      "Iteration: 1565 lambda_k: 1 Loss: 0.012757900459624952\n",
      "Iteration: 1566 lambda_k: 1 Loss: 0.012625998397634026\n",
      "Iteration: 1567 lambda_k: 1 Loss: 0.01249423720925737\n",
      "Iteration: 1568 lambda_k: 1 Loss: 0.012362617596754813\n",
      "Iteration: 1569 lambda_k: 1 Loss: 0.012231140296502184\n",
      "Iteration: 1570 lambda_k: 1 Loss: 0.012099806080794145\n",
      "Iteration: 1571 lambda_k: 1 Loss: 0.011968615759762503\n",
      "Iteration: 1572 lambda_k: 1 Loss: 0.01183757018341735\n",
      "Iteration: 1573 lambda_k: 1 Loss: 0.011706670243822017\n",
      "Iteration: 1574 lambda_k: 1 Loss: 0.011575916877410932\n",
      "Iteration: 1575 lambda_k: 1 Loss: 0.011445311067461387\n",
      "Iteration: 1576 lambda_k: 1 Loss: 0.011314853846731442\n",
      "Iteration: 1577 lambda_k: 1 Loss: 0.011184546300277511\n",
      "Iteration: 1578 lambda_k: 1 Loss: 0.011054389568465009\n",
      "Iteration: 1579 lambda_k: 1 Loss: 0.010924384850187737\n",
      "Iteration: 1580 lambda_k: 1 Loss: 0.010794533406313839\n",
      "Iteration: 1581 lambda_k: 1 Loss: 0.010664836563375569\n",
      "Iteration: 1582 lambda_k: 1 Loss: 0.010535295717884585\n",
      "Iteration: 1583 lambda_k: 1 Loss: 0.010405912322006126\n",
      "Iteration: 1584 lambda_k: 1 Loss: 0.010276687955148118\n",
      "Iteration: 1585 lambda_k: 1 Loss: 0.010147624244362734\n",
      "Iteration: 1586 lambda_k: 1 Loss: 0.010018722902915774\n",
      "Iteration: 1587 lambda_k: 1 Loss: 0.009889985741675714\n",
      "Iteration: 1588 lambda_k: 1 Loss: 0.009761414675038814\n",
      "Iteration: 1589 lambda_k: 1 Loss: 0.009633011725703352\n",
      "Iteration: 1590 lambda_k: 1 Loss: 0.009504779030347856\n",
      "Iteration: 1591 lambda_k: 1 Loss: 0.009376718846725848\n",
      "Iteration: 1592 lambda_k: 1 Loss: 0.009248833561991451\n",
      "Iteration: 1593 lambda_k: 1 Loss: 0.009121125701918266\n",
      "Iteration: 1594 lambda_k: 1 Loss: 0.00899359794079872\n",
      "Iteration: 1595 lambda_k: 1 Loss: 0.008866253112009087\n",
      "Iteration: 1596 lambda_k: 1 Loss: 0.008739094219360704\n",
      "Iteration: 1597 lambda_k: 1 Loss: 0.00861212444939995\n",
      "Iteration: 1598 lambda_k: 1 Loss: 0.008485347184802797\n",
      "Iteration: 1599 lambda_k: 1 Loss: 0.008358766018983301\n",
      "Iteration: 1600 lambda_k: 1 Loss: 0.008232384772020521\n",
      "Iteration: 1601 lambda_k: 1 Loss: 0.0081062075080328\n",
      "Iteration: 1602 lambda_k: 1 Loss: 0.007980238554137839\n",
      "Iteration: 1603 lambda_k: 1 Loss: 0.007854482521171031\n",
      "Iteration: 1604 lambda_k: 1 Loss: 0.007728944326354246\n",
      "Iteration: 1605 lambda_k: 1 Loss: 0.00760362921813291\n",
      "Iteration: 1606 lambda_k: 1 Loss: 0.0074785428034208935\n",
      "Iteration: 1607 lambda_k: 1 Loss: 0.007353691077525203\n",
      "Iteration: 1608 lambda_k: 1 Loss: 0.007229080457053708\n",
      "Iteration: 1609 lambda_k: 1 Loss: 0.0071047178161506835\n",
      "Iteration: 1610 lambda_k: 1 Loss: 0.0069806105264487266\n",
      "Iteration: 1611 lambda_k: 1 Loss: 0.006856766501177883\n",
      "Iteration: 1612 lambda_k: 1 Loss: 0.006733194243931962\n",
      "Iteration: 1613 lambda_k: 1 Loss: 0.006609902902659008\n",
      "Iteration: 1614 lambda_k: 1 Loss: 0.006486902329520283\n",
      "Iteration: 1615 lambda_k: 1 Loss: 0.006364203147352613\n",
      "Iteration: 1616 lambda_k: 1 Loss: 0.006241816823569905\n",
      "Iteration: 1617 lambda_k: 1 Loss: 0.006119755752459426\n",
      "Iteration: 1618 lambda_k: 1 Loss: 0.005998033346965478\n",
      "Iteration: 1619 lambda_k: 1 Loss: 0.005876664141209586\n",
      "Iteration: 1620 lambda_k: 1 Loss: 0.00575566390518014\n",
      "Iteration: 1621 lambda_k: 1 Loss: 0.005635049773234837\n",
      "Iteration: 1622 lambda_k: 1 Loss: 0.005514840388303845\n",
      "Iteration: 1623 lambda_k: 1 Loss: 0.005395056063964741\n",
      "Iteration: 1624 lambda_k: 1 Loss: 0.005275718966885553\n",
      "Iteration: 1625 lambda_k: 1 Loss: 0.005156853322513572\n",
      "Iteration: 1626 lambda_k: 1 Loss: 0.005038485647327463\n",
      "Iteration: 1627 lambda_k: 1 Loss: 0.004920645011518074\n",
      "Iteration: 1628 lambda_k: 1 Loss: 0.004803363335934206\n",
      "Iteration: 1629 lambda_k: 1 Loss: 0.004686675730073401\n",
      "Iteration: 1630 lambda_k: 1 Loss: 0.00457062087434477\n",
      "Iteration: 1631 lambda_k: 1 Loss: 0.004455241455855349\n",
      "Iteration: 1632 lambda_k: 1 Loss: 0.004340584661688587\n",
      "Iteration: 1633 lambda_k: 1 Loss: 0.0042267027506118085\n",
      "Iteration: 1634 lambda_k: 1 Loss: 0.004113653688352834\n",
      "Iteration: 1635 lambda_k: 1 Loss: 0.004013044166576094\n",
      "Iteration: 1636 lambda_k: 1 Loss: 0.003923097584095514\n",
      "Iteration: 1637 lambda_k: 1 Loss: 0.0038376677620893668\n",
      "Iteration: 1638 lambda_k: 1 Loss: 0.0037571783325098277\n",
      "Iteration: 1639 lambda_k: 1 Loss: 0.0036814047141456308\n",
      "Iteration: 1640 lambda_k: 1 Loss: 0.0036098418713151523\n",
      "Iteration: 1641 lambda_k: 1 Loss: 0.003542004595858785\n",
      "Iteration: 1642 lambda_k: 1 Loss: 0.003477549038752778\n",
      "Iteration: 1643 lambda_k: 1 Loss: 0.003416264015123607\n",
      "Iteration: 1644 lambda_k: 1 Loss: 0.003358015613208094\n",
      "Iteration: 1645 lambda_k: 1 Loss: 0.0033026983550336303\n",
      "Iteration: 1646 lambda_k: 1 Loss: 0.0032502094983546045\n",
      "Iteration: 1647 lambda_k: 1 Loss: 0.0032004423362113035\n",
      "Iteration: 1648 lambda_k: 1 Loss: 0.0031532889865226957\n",
      "Iteration: 1649 lambda_k: 1 Loss: 0.003108644771828009\n",
      "Iteration: 1650 lambda_k: 1 Loss: 0.0030664110085990137\n",
      "Iteration: 1651 lambda_k: 1 Loss: 0.003026495611678624\n",
      "Iteration: 1652 lambda_k: 1 Loss: 0.002988812319206226\n",
      "Iteration: 1653 lambda_k: 1 Loss: 0.0029532795055786814\n",
      "Iteration: 1654 lambda_k: 1 Loss: 0.0029198190663177314\n",
      "Iteration: 1655 lambda_k: 1 Loss: 0.002888355696484481\n",
      "Iteration: 1656 lambda_k: 1 Loss: 0.0028588163932365307\n",
      "Iteration: 1657 lambda_k: 1 Loss: 0.0028311301713376687\n",
      "Iteration: 1658 lambda_k: 1 Loss: 0.002805227899487036\n",
      "Iteration: 1659 lambda_k: 1 Loss: 0.0027810421515760647\n",
      "Iteration: 1660 lambda_k: 1 Loss: 0.0027585071382291914\n",
      "Iteration: 1661 lambda_k: 1 Loss: 0.0027375585072106547\n",
      "Iteration: 1662 lambda_k: 1 Loss: 0.002717798061535898\n",
      "Iteration: 1663 lambda_k: 1 Loss: 0.0026991328850608843\n",
      "Iteration: 1664 lambda_k: 1 Loss: 0.0026818384309491294\n",
      "Iteration: 1665 lambda_k: 1 Loss: 0.0026659823391010484\n",
      "Iteration: 1666 lambda_k: 1 Loss: 0.0026515500688727156\n",
      "Iteration: 1667 lambda_k: 1 Loss: 0.002638454437839696\n",
      "Iteration: 1668 lambda_k: 1 Loss: 0.002626581416763644\n",
      "Iteration: 1669 lambda_k: 1 Loss: 0.0026158262091047434\n",
      "Iteration: 1670 lambda_k: 1 Loss: 0.002606106573074553\n",
      "Iteration: 1671 lambda_k: 1 Loss: 0.0025973594739973596\n",
      "Iteration: 1672 lambda_k: 1 Loss: 0.0025895317832743923\n",
      "Iteration: 1673 lambda_k: 1 Loss: 0.002582572782160614\n",
      "Iteration: 1674 lambda_k: 1 Loss: 0.002576431074782769\n",
      "Iteration: 1675 lambda_k: 1 Loss: 0.0025710548358423195\n",
      "Iteration: 1676 lambda_k: 1 Loss: 0.002566393291183928\n",
      "Iteration: 1677 lambda_k: 1 Loss: 0.0025623979963019087\n",
      "Iteration: 1678 lambda_k: 1 Loss: 0.0025590234400531433\n",
      "Iteration: 1679 lambda_k: 1 Loss: 0.002556227092795647\n",
      "Iteration: 1680 lambda_k: 1 Loss: 0.0025539691055031276\n",
      "Iteration: 1681 lambda_k: 1 Loss: 0.002552212030902268\n",
      "Iteration: 1682 lambda_k: 1 Loss: 0.0025509205836478335\n",
      "Iteration: 1683 lambda_k: 1 Loss: 0.002550061498827334\n",
      "Iteration: 1684 lambda_k: 1 Loss: 0.002549603465700917\n",
      "Iteration: 1685 lambda_k: 1 Loss: 0.0025495170879988086\n",
      "Iteration: 1686 lambda_k: 1 Loss: 0.002549774818232406\n",
      "Iteration: 1687 lambda_k: 1 Loss: 0.002550350934358856\n",
      "Iteration: 1688 lambda_k: 1 Loss: 0.0025512214109756175\n",
      "Iteration: 1689 lambda_k: 1 Loss: 0.0025523638188470767\n",
      "Iteration: 1690 lambda_k: 1 Loss: 0.002553757237648283\n",
      "Iteration: 1691 lambda_k: 1 Loss: 0.0025553821678391334\n",
      "Iteration: 1692 lambda_k: 1 Loss: 0.002557220446211224\n",
      "Iteration: 1693 lambda_k: 1 Loss: 0.0025592551639095546\n",
      "Iteration: 1694 lambda_k: 1 Loss: 0.0025614705860947163\n",
      "Iteration: 1695 lambda_k: 1 Loss: 0.0025638520727487724\n",
      "Iteration: 1696 lambda_k: 1 Loss: 0.002566386000708789\n",
      "Iteration: 1697 lambda_k: 1 Loss: 0.002569059687444903\n",
      "Iteration: 1698 lambda_k: 1 Loss: 0.002571861317194945\n",
      "Iteration: 1699 lambda_k: 1 Loss: 0.002574779869916611\n",
      "Iteration: 1700 lambda_k: 1 Loss: 0.0025778050533239874\n",
      "Iteration: 1701 lambda_k: 1 Loss: 0.0025809272381791667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1702 lambda_k: 1 Loss: 0.002584137397028881\n",
      "Iteration: 1703 lambda_k: 1 Loss: 0.002587427046649826\n",
      "Iteration: 1704 lambda_k: 1 Loss: 0.0025907881945269324\n",
      "Iteration: 1705 lambda_k: 1 Loss: 0.0025942132896995362\n",
      "Iteration: 1706 lambda_k: 1 Loss: 0.0025976951782716693\n",
      "Iteration: 1707 lambda_k: 1 Loss: 0.002601227063809907\n",
      "Iteration: 1708 lambda_k: 1 Loss: 0.0026048024727626145\n",
      "Iteration: 1709 lambda_k: 1 Loss: 0.0026084152249357543\n",
      "Iteration: 1710 lambda_k: 1 Loss: 0.0026120594089557698\n",
      "Iteration: 1711 lambda_k: 1 Loss: 0.002615729362540463\n",
      "Iteration: 1712 lambda_k: 1 Loss: 0.0026194196572887855\n",
      "Iteration: 1713 lambda_k: 1 Loss: 0.0026231250875959587\n",
      "Iteration: 1714 lambda_k: 1 Loss: 0.0026268406632094605\n",
      "Iteration: 1715 lambda_k: 1 Loss: 0.0026305616048723024\n",
      "Iteration: 1716 lambda_k: 1 Loss: 0.0026342833424558704\n",
      "Iteration: 1717 lambda_k: 1 Loss: 0.002638001514969707\n",
      "Iteration: 1718 lambda_k: 1 Loss: 0.0026417119718476624\n",
      "Iteration: 1719 lambda_k: 1 Loss: 0.0026454107749471693\n",
      "Iteration: 1720 lambda_k: 1 Loss: 0.002649094200756065\n",
      "Iteration: 1721 lambda_k: 1 Loss: 0.0026527587423737656\n",
      "Iteration: 1722 lambda_k: 1 Loss: 0.0026564011109155455\n",
      "Iteration: 1723 lambda_k: 1 Loss: 0.0026600182360736148\n",
      "Iteration: 1724 lambda_k: 1 Loss: 0.0026636072656521757\n",
      "Iteration: 1725 lambda_k: 1 Loss: 0.0026671655639710526\n",
      "Iteration: 1726 lambda_k: 1 Loss: 0.0026706907091010074\n",
      "Iteration: 1727 lambda_k: 1 Loss: 0.0026741804889514774\n",
      "Iteration: 1728 lambda_k: 1 Loss: 0.0026776328962772006\n",
      "Iteration: 1729 lambda_k: 1 Loss: 0.0026810461227043266\n",
      "Iteration: 1730 lambda_k: 1 Loss: 0.0026844185518999\n",
      "Iteration: 1731 lambda_k: 1 Loss: 0.0026877487520218964\n",
      "Iteration: 1732 lambda_k: 1 Loss: 0.0026910354675924466\n",
      "Iteration: 1733 lambda_k: 1 Loss: 0.0026942776109354534\n",
      "Iteration: 1734 lambda_k: 1 Loss: 0.0026974742533135312\n",
      "Iteration: 1735 lambda_k: 1 Loss: 0.002700624615889168\n",
      "Iteration: 1736 lambda_k: 1 Loss: 0.002703728060622694\n",
      "Iteration: 1737 lambda_k: 1 Loss: 0.0027067840812061987\n",
      "Iteration: 1738 lambda_k: 1 Loss: 0.002709792294118332\n",
      "Iteration: 1739 lambda_k: 1 Loss: 0.0027127524298713062\n",
      "Iteration: 1740 lambda_k: 1 Loss: 0.002715664324507865\n",
      "Iteration: 1741 lambda_k: 1 Loss: 0.002718527911394442\n",
      "Iteration: 1742 lambda_k: 1 Loss: 0.0027213432133449283\n",
      "Iteration: 1743 lambda_k: 1 Loss: 0.002724110335100352\n",
      "Iteration: 1744 lambda_k: 1 Loss: 0.0027268294561809915\n",
      "Iteration: 1745 lambda_k: 1 Loss: 0.002729500824120086\n",
      "Iteration: 1746 lambda_k: 1 Loss: 0.0027321247480824444\n",
      "Iteration: 1747 lambda_k: 1 Loss: 0.0027347015928661828\n",
      "Iteration: 1748 lambda_k: 1 Loss: 0.002737231773281486\n",
      "Iteration: 1749 lambda_k: 1 Loss: 0.0027397157488975405\n",
      "Iteration: 1750 lambda_k: 1 Loss: 0.002742154019145865\n",
      "Iteration: 1751 lambda_k: 1 Loss: 0.0027445471187666466\n",
      "Iteration: 1752 lambda_k: 1 Loss: 0.0027468956135833647\n",
      "Iteration: 1753 lambda_k: 1 Loss: 0.0027492000965901172\n",
      "Iteration: 1754 lambda_k: 1 Loss: 0.002751461184335427\n",
      "Iteration: 1755 lambda_k: 1 Loss: 0.0027536795135864526\n",
      "Iteration: 1756 lambda_k: 1 Loss: 0.0027558557382572164\n",
      "Iteration: 1757 lambda_k: 1 Loss: 0.0027579905265849225\n",
      "Iteration: 1758 lambda_k: 1 Loss: 0.002760084558538755\n",
      "Iteration: 1759 lambda_k: 1 Loss: 0.0027621385234460915\n",
      "Iteration: 1760 lambda_k: 1 Loss: 0.0027641531178216067\n",
      "Iteration: 1761 lambda_k: 1 Loss: 0.0027661290433852984\n",
      "Iteration: 1762 lambda_k: 1 Loss: 0.0027680670052564822\n",
      "Iteration: 1763 lambda_k: 1 Loss: 0.0027699677103109038\n",
      "Iteration: 1764 lambda_k: 1 Loss: 0.002771831865689431\n",
      "Iteration: 1765 lambda_k: 1 Loss: 0.002773660177446908\n",
      "Iteration: 1766 lambda_k: 1 Loss: 0.0027754533493310547\n",
      "Iteration: 1767 lambda_k: 1 Loss: 0.002777212081681225\n",
      "Iteration: 1768 lambda_k: 1 Loss: 0.0027789370704382195\n",
      "Iteration: 1769 lambda_k: 1 Loss: 0.0027806290062562765\n",
      "Iteration: 1770 lambda_k: 1 Loss: 0.0027822885737094454\n",
      "Iteration: 1771 lambda_k: 1 Loss: 0.0027839164505848125\n",
      "Iteration: 1772 lambda_k: 1 Loss: 0.002785513307255821\n",
      "Iteration: 1773 lambda_k: 1 Loss: 0.0027870798061289557\n",
      "Iteration: 1774 lambda_k: 1 Loss: 0.002788616601158239\n",
      "Iteration: 1775 lambda_k: 1 Loss: 0.002790124337421588\n",
      "Iteration: 1776 lambda_k: 1 Loss: 0.0027916036507542184\n",
      "Iteration: 1777 lambda_k: 1 Loss: 0.00279305516743426\n",
      "Iteration: 1778 lambda_k: 1 Loss: 0.0027944795039160724\n",
      "Iteration: 1779 lambda_k: 1 Loss: 0.002795877266607365\n",
      "Iteration: 1780 lambda_k: 1 Loss: 0.0027972490516863066\n",
      "Iteration: 1781 lambda_k: 1 Loss: 0.0027985954449550672\n",
      "Iteration: 1782 lambda_k: 1 Loss: 0.0027999170217266932\n",
      "Iteration: 1783 lambda_k: 1 Loss: 0.002801214346742388\n",
      "Iteration: 1784 lambda_k: 1 Loss: 0.0028024879741161976\n",
      "Iteration: 1785 lambda_k: 1 Loss: 0.0028037384473049453\n",
      "Iteration: 1786 lambda_k: 1 Loss: 0.002804966299100846\n",
      "Iteration: 1787 lambda_k: 1 Loss: 0.00280617205164468\n",
      "Iteration: 1788 lambda_k: 1 Loss: 0.0028073562164576083\n",
      "Iteration: 1789 lambda_k: 1 Loss: 0.0028085192944897895\n",
      "Iteration: 1790 lambda_k: 1 Loss: 0.002809661776184078\n",
      "Iteration: 1791 lambda_k: 1 Loss: 0.0028107841415533105\n",
      "Iteration: 1792 lambda_k: 1 Loss: 0.0028118868602696823\n",
      "Iteration: 1793 lambda_k: 1 Loss: 0.0028129703917649782\n",
      "Iteration: 1794 lambda_k: 1 Loss: 0.002814035185340365\n",
      "Iteration: 1795 lambda_k: 1 Loss: 0.0028150816802848264\n",
      "Iteration: 1796 lambda_k: 1 Loss: 0.0028161103060009626\n",
      "Iteration: 1797 lambda_k: 1 Loss: 0.002817121482137341\n",
      "Iteration: 1798 lambda_k: 1 Loss: 0.0028181156187266298\n",
      "Iteration: 1799 lambda_k: 1 Loss: 0.002819093116328546\n",
      "Iteration: 1800 lambda_k: 1 Loss: 0.0028200543661770494\n",
      "Iteration: 1801 lambda_k: 1 Loss: 0.0028209998246686383\n",
      "Iteration: 1802 lambda_k: 1 Loss: 0.002821929734467024\n",
      "Iteration: 1803 lambda_k: 1 Loss: 0.0028228445204374437\n",
      "Iteration: 1804 lambda_k: 1 Loss: 0.002823744533578588\n",
      "Iteration: 1805 lambda_k: 1 Loss: 0.0028246301186683432\n",
      "Iteration: 1806 lambda_k: 1 Loss: 0.002825501612590162\n",
      "Iteration: 1807 lambda_k: 1 Loss: 0.002826359344853984\n",
      "Iteration: 1808 lambda_k: 1 Loss: 0.0028272036381009977\n",
      "Iteration: 1809 lambda_k: 1 Loss: 0.002828034807827545\n",
      "Iteration: 1810 lambda_k: 1 Loss: 0.0028288531617897713\n",
      "Iteration: 1811 lambda_k: 1 Loss: 0.002829658999688827\n",
      "Iteration: 1812 lambda_k: 1 Loss: 0.0028304526133032717\n",
      "Iteration: 1813 lambda_k: 1 Loss: 0.002831234286877007\n",
      "Iteration: 1814 lambda_k: 1 Loss: 0.0028320042975112513\n",
      "Iteration: 1815 lambda_k: 1 Loss: 0.002832762915434623\n",
      "Iteration: 1816 lambda_k: 1 Loss: 0.0028335104041587447\n",
      "Iteration: 1817 lambda_k: 1 Loss: 0.0028342470205838326\n",
      "Iteration: 1818 lambda_k: 1 Loss: 0.002834973015106886\n",
      "Iteration: 1819 lambda_k: 1 Loss: 0.0028356886317517853\n",
      "Iteration: 1820 lambda_k: 1 Loss: 0.0028363941083162938\n",
      "Iteration: 1821 lambda_k: 1 Loss: 0.002837089676523897\n",
      "Iteration: 1822 lambda_k: 1 Loss: 0.0028377755621726486\n",
      "Iteration: 1823 lambda_k: 1 Loss: 0.002838451985278313\n",
      "Iteration: 1824 lambda_k: 1 Loss: 0.002839119160212745\n",
      "Iteration: 1825 lambda_k: 1 Loss: 0.0028397772958389284\n",
      "Iteration: 1826 lambda_k: 1 Loss: 0.0028404265956437393\n",
      "Iteration: 1827 lambda_k: 1 Loss: 0.0028410672578684706\n",
      "Iteration: 1828 lambda_k: 1 Loss: 0.002841699475637188\n",
      "Iteration: 1829 lambda_k: 1 Loss: 0.0028423234370827523\n",
      "Iteration: 1830 lambda_k: 1 Loss: 0.00284293932547037\n",
      "Iteration: 1831 lambda_k: 1 Loss: 0.002843547319318739\n",
      "Iteration: 1832 lambda_k: 1 Loss: 0.0028441475925187216\n",
      "Iteration: 1833 lambda_k: 1 Loss: 0.002844740314449603\n",
      "Iteration: 1834 lambda_k: 1 Loss: 0.002845325650092878\n",
      "Iteration: 1835 lambda_k: 1 Loss: 0.002845903760143666\n",
      "Iteration: 1836 lambda_k: 1 Loss: 0.002846474801119745\n",
      "Iteration: 1837 lambda_k: 1 Loss: 0.0028470389254681575\n",
      "Iteration: 1838 lambda_k: 1 Loss: 0.0028475962816695176\n",
      "Iteration: 1839 lambda_k: 1 Loss: 0.002848147014339997\n",
      "Iteration: 1840 lambda_k: 1 Loss: 0.0028486912643310856\n",
      "Iteration: 1841 lambda_k: 1 Loss: 0.002849229168826975\n",
      "Iteration: 1842 lambda_k: 1 Loss: 0.002849760861439764\n",
      "Iteration: 1843 lambda_k: 1 Loss: 0.0028502864723024836\n",
      "Iteration: 1844 lambda_k: 1 Loss: 0.0028508061281599523\n",
      "Iteration: 1845 lambda_k: 1 Loss: 0.0028513199524574768\n",
      "Iteration: 1846 lambda_k: 1 Loss: 0.0028518280654273925\n",
      "Iteration: 1847 lambda_k: 1 Loss: 0.002852330584173702\n",
      "Iteration: 1848 lambda_k: 1 Loss: 0.0028528276227545086\n",
      "Iteration: 1849 lambda_k: 1 Loss: 0.0028533192922625484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1850 lambda_k: 1 Loss: 0.0028538057009037544\n",
      "Iteration: 1851 lambda_k: 1 Loss: 0.0028542869540738682\n",
      "Iteration: 1852 lambda_k: 1 Loss: 0.002854763154433167\n",
      "Iteration: 1853 lambda_k: 1 Loss: 0.0028552344019794\n",
      "Iteration: 1854 lambda_k: 1 Loss: 0.002855700794118765\n",
      "Iteration: 1855 lambda_k: 1 Loss: 0.002856162425735326\n",
      "Iteration: 1856 lambda_k: 1 Loss: 0.002856619389258488\n",
      "Iteration: 1857 lambda_k: 1 Loss: 0.0028570717747288116\n",
      "Iteration: 1858 lambda_k: 1 Loss: 0.0028575196698622856\n",
      "Iteration: 1859 lambda_k: 1 Loss: 0.002857963160112768\n",
      "Iteration: 1860 lambda_k: 1 Loss: 0.0028584023287329904\n",
      "Iteration: 1861 lambda_k: 1 Loss: 0.0028588372568338757\n",
      "Iteration: 1862 lambda_k: 1 Loss: 0.0028592680234424198\n",
      "Iteration: 1863 lambda_k: 1 Loss: 0.0028596947055580394\n",
      "Iteration: 1864 lambda_k: 1 Loss: 0.0028601173782073957\n",
      "Iteration: 1865 lambda_k: 1 Loss: 0.0028605361144978816\n",
      "Iteration: 1866 lambda_k: 1 Loss: 0.0028609509856697296\n",
      "Iteration: 1867 lambda_k: 1 Loss: 0.0028613620611466235\n",
      "Iteration: 1868 lambda_k: 1 Loss: 0.0028617694085850734\n",
      "Iteration: 1869 lambda_k: 1 Loss: 0.0028621730939224865\n",
      "Iteration: 1870 lambda_k: 1 Loss: 0.0028625731814239715\n",
      "Iteration: 1871 lambda_k: 1 Loss: 0.002862969733727833\n",
      "Iteration: 1872 lambda_k: 1 Loss: 0.0028633628118899246\n",
      "Iteration: 1873 lambda_k: 1 Loss: 0.002863752475426834\n",
      "Iteration: 1874 lambda_k: 1 Loss: 0.0028641387823578786\n",
      "Iteration: 1875 lambda_k: 1 Loss: 0.002864521789245936\n",
      "Iteration: 1876 lambda_k: 1 Loss: 0.0028649015512372815\n",
      "Iteration: 1877 lambda_k: 1 Loss: 0.002865278122100309\n",
      "Iteration: 1878 lambda_k: 1 Loss: 0.0028656515542631635\n",
      "Iteration: 1879 lambda_k: 1 Loss: 0.002866021898850396\n",
      "Iteration: 1880 lambda_k: 1 Loss: 0.002866389205718645\n",
      "Iteration: 1881 lambda_k: 1 Loss: 0.002866753523491375\n",
      "Iteration: 1882 lambda_k: 1 Loss: 0.0028671148995925793\n",
      "Iteration: 1883 lambda_k: 1 Loss: 0.002867473380279631\n",
      "Iteration: 1884 lambda_k: 1 Loss: 0.00286782901067525\n",
      "Iteration: 1885 lambda_k: 1 Loss: 0.0028681818347985927\n",
      "Iteration: 1886 lambda_k: 1 Loss: 0.0028685318955954686\n",
      "Iteration: 1887 lambda_k: 1 Loss: 0.0028688792349677488\n",
      "Iteration: 1888 lambda_k: 1 Loss: 0.002869223893801932\n",
      "Iteration: 1889 lambda_k: 1 Loss: 0.0028695659119970067\n",
      "Iteration: 1890 lambda_k: 1 Loss: 0.0028699053284915437\n",
      "Iteration: 1891 lambda_k: 1 Loss: 0.0028702421812899083\n",
      "Iteration: 1892 lambda_k: 1 Loss: 0.0028705765074879095\n",
      "Iteration: 1893 lambda_k: 1 Loss: 0.0028709083432976916\n",
      "Iteration: 1894 lambda_k: 1 Loss: 0.0028712377240718723\n",
      "Iteration: 1895 lambda_k: 1 Loss: 0.0028715646843271614\n",
      "Iteration: 1896 lambda_k: 1 Loss: 0.0028718892577671716\n",
      "Iteration: 1897 lambda_k: 1 Loss: 0.0028722114773047053\n",
      "Iteration: 1898 lambda_k: 1 Loss: 0.0028725313750833695\n",
      "Iteration: 1899 lambda_k: 1 Loss: 0.002872848982498603\n",
      "Iteration: 1900 lambda_k: 1 Loss: 0.0028731643302181767\n",
      "Iteration: 1901 lambda_k: 1 Loss: 0.0028734774482020234\n",
      "Iteration: 1902 lambda_k: 1 Loss: 0.0028737883657215866\n",
      "Iteration: 1903 lambda_k: 1 Loss: 0.0028740971113786437\n",
      "Iteration: 1904 lambda_k: 1 Loss: 0.0028744037131235214\n",
      "Iteration: 1905 lambda_k: 1 Loss: 0.002874708198272879\n",
      "Iteration: 1906 lambda_k: 1 Loss: 0.0028750105935270694\n",
      "Iteration: 1907 lambda_k: 1 Loss: 0.0028753109249867862\n",
      "Iteration: 1908 lambda_k: 1 Loss: 0.0028756092181694975\n",
      "Iteration: 1909 lambda_k: 1 Loss: 0.002875905498025208\n",
      "Iteration: 1910 lambda_k: 1 Loss: 0.0028761997889520086\n",
      "Iteration: 1911 lambda_k: 1 Loss: 0.002876492114810945\n",
      "Iteration: 1912 lambda_k: 1 Loss: 0.002876782498940679\n",
      "Iteration: 1913 lambda_k: 1 Loss: 0.0028770709641715926\n",
      "Iteration: 1914 lambda_k: 1 Loss: 0.0028773575328396423\n",
      "Iteration: 1915 lambda_k: 1 Loss: 0.0028776422267996382\n",
      "Iteration: 1916 lambda_k: 1 Loss: 0.002877925067438339\n",
      "Iteration: 1917 lambda_k: 1 Loss: 0.002878206075687056\n",
      "Iteration: 1918 lambda_k: 1 Loss: 0.002878485272033939\n",
      "Iteration: 1919 lambda_k: 1 Loss: 0.0028787626765359718\n",
      "Iteration: 1920 lambda_k: 1 Loss: 0.002879038308830553\n",
      "Iteration: 1921 lambda_k: 1 Loss: 0.002879312188146758\n",
      "Iteration: 1922 lambda_k: 1 Loss: 0.0028795843333163317\n",
      "Iteration: 1923 lambda_k: 1 Loss: 0.0028798547627843893\n",
      "Iteration: 1924 lambda_k: 1 Loss: 0.0028801234946197385\n",
      "Iteration: 1925 lambda_k: 1 Loss: 0.002880390546524943\n",
      "Iteration: 1926 lambda_k: 1 Loss: 0.0028806559358461113\n",
      "Iteration: 1927 lambda_k: 1 Loss: 0.0028809196795824867\n",
      "Iteration: 1928 lambda_k: 1 Loss: 0.002881181794395563\n",
      "Iteration: 1929 lambda_k: 1 Loss: 0.0028814422966182036\n",
      "Iteration: 1930 lambda_k: 1 Loss: 0.0028817012022632972\n",
      "Iteration: 1931 lambda_k: 1 Loss: 0.002881958527032282\n",
      "Iteration: 1932 lambda_k: 1 Loss: 0.002882214286323373\n",
      "Iteration: 1933 lambda_k: 1 Loss: 0.0028824684952396097\n",
      "Iteration: 1934 lambda_k: 1 Loss: 0.0028827211685966063\n",
      "Iteration: 1935 lambda_k: 1 Loss: 0.0028829723209301593\n",
      "Iteration: 1936 lambda_k: 1 Loss: 0.0028832219665036168\n",
      "Iteration: 1937 lambda_k: 1 Loss: 0.0028834701193150105\n",
      "Iteration: 1938 lambda_k: 1 Loss: 0.0028837167931040236\n",
      "Iteration: 1939 lambda_k: 1 Loss: 0.0028839620013587325\n",
      "Iteration: 1940 lambda_k: 1 Loss: 0.002884205757322187\n",
      "Iteration: 1941 lambda_k: 1 Loss: 0.002884448073998829\n",
      "Iteration: 1942 lambda_k: 1 Loss: 0.0028846889641605936\n",
      "Iteration: 1943 lambda_k: 1 Loss: 0.002884928440353039\n",
      "Iteration: 1944 lambda_k: 1 Loss: 0.002885166514901178\n",
      "Iteration: 1945 lambda_k: 1 Loss: 0.002885403199915166\n",
      "Iteration: 1946 lambda_k: 1 Loss: 0.0028856385072958215\n",
      "Iteration: 1947 lambda_k: 1 Loss: 0.0028858724487400463\n",
      "Iteration: 1948 lambda_k: 1 Loss: 0.002886105035745992\n",
      "Iteration: 1949 lambda_k: 1 Loss: 0.0028863362796181908\n",
      "Iteration: 1950 lambda_k: 1 Loss: 0.002886566191472507\n",
      "Iteration: 1951 lambda_k: 1 Loss: 0.0028867947822408843\n",
      "Iteration: 1952 lambda_k: 1 Loss: 0.0028870220626760587\n",
      "Iteration: 1953 lambda_k: 1 Loss: 0.0028872480433560365\n",
      "Iteration: 1954 lambda_k: 1 Loss: 0.002887472734688555\n",
      "Iteration: 1955 lambda_k: 1 Loss: 0.0028876961469153574\n",
      "Iteration: 1956 lambda_k: 1 Loss: 0.002887918290116339\n",
      "Iteration: 1957 lambda_k: 1 Loss: 0.002888139174213606\n",
      "Iteration: 1958 lambda_k: 1 Loss: 0.0028883588089754\n",
      "Iteration: 1959 lambda_k: 1 Loss: 0.002888577204019891\n",
      "Iteration: 1960 lambda_k: 1 Loss: 0.0028887943688189793\n",
      "Iteration: 1961 lambda_k: 1 Loss: 0.002889010312701822\n",
      "Iteration: 1962 lambda_k: 1 Loss: 0.0028892250448583736\n",
      "Iteration: 1963 lambda_k: 1 Loss: 0.002889438574342793\n",
      "Iteration: 1964 lambda_k: 1 Loss: 0.00288965091007679\n",
      "Iteration: 1965 lambda_k: 1 Loss: 0.002889862060852819\n",
      "Iteration: 1966 lambda_k: 1 Loss: 0.002890072035337181\n",
      "Iteration: 1967 lambda_k: 1 Loss: 0.0028902808420731524\n",
      "Iteration: 1968 lambda_k: 1 Loss: 0.0028904884894838776\n",
      "Iteration: 1969 lambda_k: 1 Loss: 0.002890694985875288\n",
      "Iteration: 1970 lambda_k: 1 Loss: 0.0028909003394388504\n",
      "Iteration: 1971 lambda_k: 1 Loss: 0.0028911045582543458\n",
      "Iteration: 1972 lambda_k: 1 Loss: 0.002891307650292508\n",
      "Iteration: 1973 lambda_k: 1 Loss: 0.0028915096234175155\n",
      "Iteration: 1974 lambda_k: 1 Loss: 0.0028917104853895923\n",
      "Iteration: 1975 lambda_k: 1 Loss: 0.0028919102438673835\n",
      "Iteration: 1976 lambda_k: 1 Loss: 0.002892108906410352\n",
      "Iteration: 1977 lambda_k: 1 Loss: 0.002892306480480994\n",
      "Iteration: 1978 lambda_k: 1 Loss: 0.002892502973447163\n",
      "Iteration: 1979 lambda_k: 1 Loss: 0.0028926983925842147\n",
      "Iteration: 1980 lambda_k: 1 Loss: 0.0028928927450771126\n",
      "Iteration: 1981 lambda_k: 1 Loss: 0.0028930860380224368\n",
      "Iteration: 1982 lambda_k: 1 Loss: 0.002893278278430456\n",
      "Iteration: 1983 lambda_k: 1 Loss: 0.00289346947322704\n",
      "Iteration: 1984 lambda_k: 1 Loss: 0.002893659629255502\n",
      "Iteration: 1985 lambda_k: 1 Loss: 0.00289384875327849\n",
      "Iteration: 1986 lambda_k: 1 Loss: 0.0028940368519797294\n",
      "Iteration: 1987 lambda_k: 1 Loss: 0.0028942239319658314\n",
      "Iteration: 1988 lambda_k: 1 Loss: 0.002894409999767869\n",
      "Iteration: 1989 lambda_k: 1 Loss: 0.0028945950618431042\n",
      "Iteration: 1990 lambda_k: 1 Loss: 0.0028947791245765475\n",
      "Iteration: 1991 lambda_k: 1 Loss: 0.0028949621942825216\n",
      "Iteration: 1992 lambda_k: 1 Loss: 0.0028951442772061665\n",
      "Iteration: 1993 lambda_k: 1 Loss: 0.0028953253795248796\n",
      "Iteration: 1994 lambda_k: 1 Loss: 0.002895505507349832\n",
      "Iteration: 1995 lambda_k: 1 Loss: 0.0028956846667272424\n",
      "Iteration: 1996 lambda_k: 1 Loss: 0.0028958628636398054\n",
      "Iteration: 1997 lambda_k: 1 Loss: 0.002896040104007976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1998 lambda_k: 1 Loss: 0.002896216393691263\n",
      "Iteration: 1999 lambda_k: 1 Loss: 0.002896391738489462\n",
      "Iteration: 2000 lambda_k: 1 Loss: 0.0028965661441438823\n",
      "Iteration: 2001 lambda_k: 1 Loss: 0.002896739616338477\n",
      "Iteration: 2002 lambda_k: 1 Loss: 0.00289691216070104\n",
      "Iteration: 2003 lambda_k: 1 Loss: 0.002897083782804313\n",
      "Iteration: 2004 lambda_k: 1 Loss: 0.002897254488167034\n",
      "Iteration: 2005 lambda_k: 1 Loss: 0.002897424282255051\n",
      "Iteration: 2006 lambda_k: 1 Loss: 0.0028975931704822867\n",
      "Iteration: 2007 lambda_k: 1 Loss: 0.0028977611582117744\n",
      "Iteration: 2008 lambda_k: 1 Loss: 0.00289792825075661\n",
      "Iteration: 2009 lambda_k: 1 Loss: 0.0028980944533809473\n",
      "Iteration: 2010 lambda_k: 1 Loss: 0.0028982597713008533\n",
      "Iteration: 2011 lambda_k: 1 Loss: 0.0028984242096852385\n",
      "Iteration: 2012 lambda_k: 1 Loss: 0.00289858777365672\n",
      "Iteration: 2013 lambda_k: 1 Loss: 0.002898750468292491\n",
      "Iteration: 2014 lambda_k: 1 Loss: 0.0028989122986251316\n",
      "Iteration: 2015 lambda_k: 1 Loss: 0.0028990732696433895\n",
      "Iteration: 2016 lambda_k: 1 Loss: 0.0028992333862930286\n",
      "Iteration: 2017 lambda_k: 1 Loss: 0.0028993926534775557\n",
      "Iteration: 2018 lambda_k: 1 Loss: 0.0028995510760589224\n",
      "Iteration: 2019 lambda_k: 1 Loss: 0.0028997086588583086\n",
      "Iteration: 2020 lambda_k: 1 Loss: 0.002899865406656815\n",
      "Iteration: 2021 lambda_k: 1 Loss: 0.0029000213241961565\n",
      "Iteration: 2022 lambda_k: 1 Loss: 0.0029001764161792894\n",
      "Iteration: 2023 lambda_k: 1 Loss: 0.002900330687271098\n",
      "Iteration: 2024 lambda_k: 1 Loss: 0.002900484142099005\n",
      "Iteration: 2025 lambda_k: 1 Loss: 0.0029006367852536415\n",
      "Iteration: 2026 lambda_k: 1 Loss: 0.0029007886212893902\n",
      "Iteration: 2027 lambda_k: 1 Loss: 0.0029009396547249964\n",
      "Iteration: 2028 lambda_k: 1 Loss: 0.0029010898900441503\n",
      "Iteration: 2029 lambda_k: 1 Loss: 0.0029012393316960204\n",
      "Iteration: 2030 lambda_k: 1 Loss: 0.002901387984095816\n",
      "Iteration: 2031 lambda_k: 1 Loss: 0.0029015358516252866\n",
      "Iteration: 2032 lambda_k: 1 Loss: 0.0029016829386332987\n",
      "Iteration: 2033 lambda_k: 1 Loss: 0.0029018292494362493\n",
      "Iteration: 2034 lambda_k: 1 Loss: 0.002901974788318661\n",
      "Iteration: 2035 lambda_k: 1 Loss: 0.0029021195595335613\n",
      "Iteration: 2036 lambda_k: 1 Loss: 0.0029022635673030288\n",
      "Iteration: 2037 lambda_k: 1 Loss: 0.0029024068158185782\n",
      "Iteration: 2038 lambda_k: 1 Loss: 0.00290254930924164\n",
      "Iteration: 2039 lambda_k: 1 Loss: 0.0029026910517040037\n",
      "Iteration: 2040 lambda_k: 1 Loss: 0.002902832047308208\n",
      "Iteration: 2041 lambda_k: 1 Loss: 0.0029029723001279743\n",
      "Iteration: 2042 lambda_k: 1 Loss: 0.002903111814208614\n",
      "Iteration: 2043 lambda_k: 1 Loss: 0.00290325059356738\n",
      "Iteration: 2044 lambda_k: 1 Loss: 0.0029033886421938834\n",
      "Iteration: 2045 lambda_k: 1 Loss: 0.002903525964050487\n",
      "Iteration: 2046 lambda_k: 1 Loss: 0.0029036625630725873\n",
      "Iteration: 2047 lambda_k: 1 Loss: 0.0029037984431690617\n",
      "Iteration: 2048 lambda_k: 1 Loss: 0.0029039336082225613\n",
      "Iteration: 2049 lambda_k: 1 Loss: 0.002904068062089856\n",
      "Iteration: 2050 lambda_k: 1 Loss: 0.0029042018086021734\n",
      "Iteration: 2051 lambda_k: 1 Loss: 0.002904334851565534\n",
      "Iteration: 2052 lambda_k: 1 Loss: 0.0029044671947610775\n",
      "Iteration: 2053 lambda_k: 1 Loss: 0.0029045988419452605\n",
      "Iteration: 2054 lambda_k: 1 Loss: 0.0029047297968503053\n",
      "Iteration: 2055 lambda_k: 1 Loss: 0.002904860063184412\n",
      "Iteration: 2056 lambda_k: 1 Loss: 0.002904989644632064\n",
      "Iteration: 2057 lambda_k: 1 Loss: 0.002905118544854301\n",
      "Iteration: 2058 lambda_k: 1 Loss: 0.002905246767488986\n",
      "Iteration: 2059 lambda_k: 1 Loss: 0.002905374316151107\n",
      "Iteration: 2060 lambda_k: 1 Loss: 0.0029055011944330218\n",
      "Iteration: 2061 lambda_k: 1 Loss: 0.00290562740590469\n",
      "Iteration: 2062 lambda_k: 1 Loss: 0.002905752954113939\n",
      "Iteration: 2063 lambda_k: 1 Loss: 0.0029058778425867316\n",
      "Iteration: 2064 lambda_k: 1 Loss: 0.0029060020748273773\n",
      "Iteration: 2065 lambda_k: 1 Loss: 0.002906125654318763\n",
      "Iteration: 2066 lambda_k: 1 Loss: 0.002906248584522605\n",
      "Iteration: 2067 lambda_k: 1 Loss: 0.002906370868879658\n",
      "Iteration: 2068 lambda_k: 1 Loss: 0.0029064925108099418\n",
      "Iteration: 2069 lambda_k: 1 Loss: 0.0029066135137129725\n",
      "Iteration: 2070 lambda_k: 1 Loss: 0.0029067338809678845\n",
      "Iteration: 2071 lambda_k: 1 Loss: 0.002906853615933778\n",
      "Iteration: 2072 lambda_k: 1 Loss: 0.0029069727219498097\n",
      "Iteration: 2073 lambda_k: 1 Loss: 0.002907091202335446\n",
      "Iteration: 2074 lambda_k: 1 Loss: 0.0029072090603905996\n",
      "Iteration: 2075 lambda_k: 1 Loss: 0.0029073262993958918\n",
      "Iteration: 2076 lambda_k: 1 Loss: 0.0029074429226127875\n",
      "Iteration: 2077 lambda_k: 1 Loss: 0.002907558933283804\n",
      "Iteration: 2078 lambda_k: 1 Loss: 0.002907674334632662\n",
      "Iteration: 2079 lambda_k: 1 Loss: 0.0029077891298644825\n",
      "Iteration: 2080 lambda_k: 1 Loss: 0.00290790332216595\n",
      "Iteration: 2081 lambda_k: 1 Loss: 0.0029080169147054917\n",
      "Iteration: 2082 lambda_k: 1 Loss: 0.002908129910633404\n",
      "Iteration: 2083 lambda_k: 1 Loss: 0.0029082423130820543\n",
      "Iteration: 2084 lambda_k: 1 Loss: 0.0029083541251660244\n",
      "Iteration: 2085 lambda_k: 1 Loss: 0.0029084653499822735\n",
      "Iteration: 2086 lambda_k: 1 Loss: 0.002908575990610301\n",
      "Iteration: 2087 lambda_k: 1 Loss: 0.0029086860501122197\n",
      "Iteration: 2088 lambda_k: 1 Loss: 0.002908795531533039\n",
      "Iteration: 2089 lambda_k: 1 Loss: 0.0029089044379006735\n",
      "Iteration: 2090 lambda_k: 1 Loss: 0.0029090127722261774\n",
      "Iteration: 2091 lambda_k: 1 Loss: 0.002909120537503836\n",
      "Iteration: 2092 lambda_k: 1 Loss: 0.0029092277367113286\n",
      "Iteration: 2093 lambda_k: 1 Loss: 0.002909334372809839\n",
      "Iteration: 2094 lambda_k: 1 Loss: 0.0029094404487441943\n",
      "Iteration: 2095 lambda_k: 1 Loss: 0.002909545967443006\n",
      "Iteration: 2096 lambda_k: 1 Loss: 0.002909650931818792\n",
      "Iteration: 2097 lambda_k: 1 Loss: 0.002909755344768105\n",
      "Iteration: 2098 lambda_k: 1 Loss: 0.0029098592091716495\n",
      "Iteration: 2099 lambda_k: 1 Loss: 0.002909962527894384\n",
      "Iteration: 2100 lambda_k: 1 Loss: 0.002910065303785696\n",
      "Iteration: 2101 lambda_k: 1 Loss: 0.0029101675396794547\n",
      "Iteration: 2102 lambda_k: 1 Loss: 0.002910269238394165\n",
      "Iteration: 2103 lambda_k: 1 Loss: 0.0029103704027330815\n",
      "Iteration: 2104 lambda_k: 1 Loss: 0.0029104710354842943\n",
      "Iteration: 2105 lambda_k: 1 Loss: 0.002910571139420849\n",
      "Iteration: 2106 lambda_k: 1 Loss: 0.0029106707173009055\n",
      "Iteration: 2107 lambda_k: 1 Loss: 0.0029107697718677578\n",
      "Iteration: 2108 lambda_k: 1 Loss: 0.0029108683058499786\n",
      "Iteration: 2109 lambda_k: 1 Loss: 0.0029109663219615473\n",
      "Iteration: 2110 lambda_k: 1 Loss: 0.0029110638229019213\n",
      "Iteration: 2111 lambda_k: 1 Loss: 0.0029111608113561362\n",
      "Iteration: 2112 lambda_k: 1 Loss: 0.0029112572899949157\n",
      "Iteration: 2113 lambda_k: 1 Loss: 0.0029113532614747826\n",
      "Iteration: 2114 lambda_k: 1 Loss: 0.002911448728438129\n",
      "Iteration: 2115 lambda_k: 1 Loss: 0.0029115436935133283\n",
      "Iteration: 2116 lambda_k: 1 Loss: 0.002911638159314799\n",
      "Iteration: 2117 lambda_k: 1 Loss: 0.002911732128443161\n",
      "Iteration: 2118 lambda_k: 1 Loss: 0.002911825603485238\n",
      "Iteration: 2119 lambda_k: 1 Loss: 0.002911918587014228\n",
      "Iteration: 2120 lambda_k: 1 Loss: 0.002912011081589747\n",
      "Iteration: 2121 lambda_k: 1 Loss: 0.0029121030897579295\n",
      "Iteration: 2122 lambda_k: 1 Loss: 0.002912194614051519\n",
      "Iteration: 2123 lambda_k: 1 Loss: 0.002912285656989945\n",
      "Iteration: 2124 lambda_k: 1 Loss: 0.0029123762210793835\n",
      "Iteration: 2125 lambda_k: 1 Loss: 0.0029124663088129195\n",
      "Iteration: 2126 lambda_k: 1 Loss: 0.0029125559226705474\n",
      "Iteration: 2127 lambda_k: 1 Loss: 0.002912645065119266\n",
      "Iteration: 2128 lambda_k: 1 Loss: 0.002912733738613208\n",
      "Iteration: 2129 lambda_k: 1 Loss: 0.002912821945593625\n",
      "Iteration: 2130 lambda_k: 1 Loss: 0.0029129096884891047\n",
      "Iteration: 2131 lambda_k: 1 Loss: 0.002912996969715525\n",
      "Iteration: 2132 lambda_k: 1 Loss: 0.0029130837916761664\n",
      "Iteration: 2133 lambda_k: 1 Loss: 0.0029131701567618063\n",
      "Iteration: 2134 lambda_k: 1 Loss: 0.0029132560673507846\n",
      "Iteration: 2135 lambda_k: 1 Loss: 0.0029133415258090927\n",
      "Iteration: 2136 lambda_k: 1 Loss: 0.0029134265344903944\n",
      "Iteration: 2137 lambda_k: 1 Loss: 0.002913511095736154\n",
      "Iteration: 2138 lambda_k: 1 Loss: 0.0029135952118757094\n",
      "Iteration: 2139 lambda_k: 1 Loss: 0.0029136788852262784\n",
      "Iteration: 2140 lambda_k: 1 Loss: 0.0029137621180931037\n",
      "Iteration: 2141 lambda_k: 1 Loss: 0.002913844912769467\n",
      "Iteration: 2142 lambda_k: 1 Loss: 0.002913927271536799\n",
      "Iteration: 2143 lambda_k: 1 Loss: 0.0029140091966647153\n",
      "Iteration: 2144 lambda_k: 1 Loss: 0.002914090690411105\n",
      "Iteration: 2145 lambda_k: 1 Loss: 0.0029141717550222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2146 lambda_k: 1 Loss: 0.002914252392732622\n",
      "Iteration: 2147 lambda_k: 1 Loss: 0.002914332605765442\n",
      "Iteration: 2148 lambda_k: 1 Loss: 0.002914412396332273\n",
      "Iteration: 2149 lambda_k: 1 Loss: 0.0029144917666333067\n",
      "Iteration: 2150 lambda_k: 1 Loss: 0.0029145707188574384\n",
      "Iteration: 2151 lambda_k: 1 Loss: 0.002914649255182222\n",
      "Iteration: 2152 lambda_k: 1 Loss: 0.00291472737777403\n",
      "Iteration: 2153 lambda_k: 1 Loss: 0.0029148050887880883\n",
      "Iteration: 2154 lambda_k: 1 Loss: 0.00291488239036847\n",
      "Iteration: 2155 lambda_k: 1 Loss: 0.002914959284648259\n",
      "Iteration: 2156 lambda_k: 1 Loss: 0.0029150357737495617\n",
      "Iteration: 2157 lambda_k: 1 Loss: 0.0029151118597835718\n",
      "Iteration: 2158 lambda_k: 1 Loss: 0.002915187544850641\n",
      "Iteration: 2159 lambda_k: 1 Loss: 0.0029152628310402744\n",
      "Iteration: 2160 lambda_k: 1 Loss: 0.0029153377204312898\n",
      "Iteration: 2161 lambda_k: 1 Loss: 0.002915412215091799\n",
      "Iteration: 2162 lambda_k: 1 Loss: 0.002915486317079291\n",
      "Iteration: 2163 lambda_k: 1 Loss: 0.0029155600284407135\n",
      "Iteration: 2164 lambda_k: 1 Loss: 0.002915633351212461\n",
      "Iteration: 2165 lambda_k: 1 Loss: 0.0029157062874204825\n",
      "Iteration: 2166 lambda_k: 1 Loss: 0.0029157788390803624\n",
      "Iteration: 2167 lambda_k: 1 Loss: 0.0029158510081973327\n",
      "Iteration: 2168 lambda_k: 1 Loss: 0.00291592279676632\n",
      "Iteration: 2169 lambda_k: 1 Loss: 0.002915994206772017\n",
      "Iteration: 2170 lambda_k: 1 Loss: 0.0029160652401889536\n",
      "Iteration: 2171 lambda_k: 1 Loss: 0.0029161358989815123\n",
      "Iteration: 2172 lambda_k: 1 Loss: 0.002916206185103997\n",
      "Iteration: 2173 lambda_k: 1 Loss: 0.0029162761005007126\n",
      "Iteration: 2174 lambda_k: 1 Loss: 0.0029163456471060045\n",
      "Iteration: 2175 lambda_k: 1 Loss: 0.0029164148268442662\n",
      "Iteration: 2176 lambda_k: 1 Loss: 0.002916483641630034\n",
      "Iteration: 2177 lambda_k: 1 Loss: 0.0029165520933680414\n",
      "Iteration: 2178 lambda_k: 1 Loss: 0.0029166201839532515\n",
      "Iteration: 2179 lambda_k: 1 Loss: 0.0029166879152709166\n",
      "Iteration: 2180 lambda_k: 1 Loss: 0.0029167552891966337\n",
      "Iteration: 2181 lambda_k: 1 Loss: 0.002916822307596363\n",
      "Iteration: 2182 lambda_k: 1 Loss: 0.0029168889723265173\n",
      "Iteration: 2183 lambda_k: 1 Loss: 0.0029169552852340067\n",
      "Iteration: 2184 lambda_k: 1 Loss: 0.002917021248156271\n",
      "Iteration: 2185 lambda_k: 1 Loss: 0.0029170868629213144\n",
      "Iteration: 2186 lambda_k: 1 Loss: 0.0029171521313477843\n",
      "Iteration: 2187 lambda_k: 1 Loss: 0.0029172170552450105\n",
      "Iteration: 2188 lambda_k: 1 Loss: 0.0029172816364130575\n",
      "Iteration: 2189 lambda_k: 1 Loss: 0.002917345876642715\n",
      "Iteration: 2190 lambda_k: 1 Loss: 0.0029174097777156676\n",
      "Iteration: 2191 lambda_k: 1 Loss: 0.002917473341404424\n",
      "Iteration: 2192 lambda_k: 1 Loss: 0.0029175365694723964\n",
      "Iteration: 2193 lambda_k: 1 Loss: 0.0029175994636739984\n",
      "Iteration: 2194 lambda_k: 1 Loss: 0.0029176620257546156\n",
      "Iteration: 2195 lambda_k: 1 Loss: 0.0029177242574506884\n",
      "Iteration: 2196 lambda_k: 1 Loss: 0.002917786160489786\n",
      "Iteration: 2197 lambda_k: 1 Loss: 0.0029178477365905875\n",
      "Iteration: 2198 lambda_k: 1 Loss: 0.002917908987462958\n",
      "Iteration: 2199 lambda_k: 1 Loss: 0.0029179699148080165\n",
      "Iteration: 2200 lambda_k: 1 Loss: 0.0029180305203181456\n",
      "Iteration: 2201 lambda_k: 1 Loss: 0.002918090805677042\n",
      "Iteration: 2202 lambda_k: 1 Loss: 0.0029181507725597887\n",
      "Iteration: 2203 lambda_k: 1 Loss: 0.002918210422632842\n",
      "Iteration: 2204 lambda_k: 1 Loss: 0.002918269757554151\n",
      "Iteration: 2205 lambda_k: 1 Loss: 0.0029183287789731134\n",
      "Iteration: 2206 lambda_k: 1 Loss: 0.002918387488530693\n",
      "Iteration: 2207 lambda_k: 1 Loss: 0.002918445887859403\n",
      "Iteration: 2208 lambda_k: 1 Loss: 0.0029185039785834324\n",
      "Iteration: 2209 lambda_k: 1 Loss: 0.002918561762318575\n",
      "Iteration: 2210 lambda_k: 1 Loss: 0.002918619240672366\n",
      "Iteration: 2211 lambda_k: 1 Loss: 0.0029186764152440703\n",
      "Iteration: 2212 lambda_k: 1 Loss: 0.002918733287624755\n",
      "Iteration: 2213 lambda_k: 1 Loss: 0.0029187898593972857\n",
      "Iteration: 2214 lambda_k: 1 Loss: 0.002918846132136437\n",
      "Iteration: 2215 lambda_k: 1 Loss: 0.0029189021074088706\n",
      "Iteration: 2216 lambda_k: 1 Loss: 0.002918957786773209\n",
      "Iteration: 2217 lambda_k: 1 Loss: 0.002919013171780061\n",
      "Iteration: 2218 lambda_k: 1 Loss: 0.002919068263972078\n",
      "Iteration: 2219 lambda_k: 1 Loss: 0.0029191230648839646\n",
      "Iteration: 2220 lambda_k: 1 Loss: 0.0029191775760425333\n",
      "Iteration: 2221 lambda_k: 1 Loss: 0.00291923179896678\n",
      "Iteration: 2222 lambda_k: 1 Loss: 0.00291928573516787\n",
      "Iteration: 2223 lambda_k: 1 Loss: 0.0029193393861491877\n",
      "Iteration: 2224 lambda_k: 1 Loss: 0.002919392753406395\n",
      "Iteration: 2225 lambda_k: 1 Loss: 0.0029194458384274573\n",
      "Iteration: 2226 lambda_k: 1 Loss: 0.0029194986426927146\n",
      "Iteration: 2227 lambda_k: 1 Loss: 0.002919551167674832\n",
      "Iteration: 2228 lambda_k: 1 Loss: 0.002919603414838929\n",
      "Iteration: 2229 lambda_k: 1 Loss: 0.0029196553856425957\n",
      "Iteration: 2230 lambda_k: 1 Loss: 0.0029197070815359196\n",
      "Iteration: 2231 lambda_k: 1 Loss: 0.002919758503961475\n",
      "Iteration: 2232 lambda_k: 1 Loss: 0.0029198096543544406\n",
      "Iteration: 2233 lambda_k: 1 Loss: 0.002919860534142603\n",
      "Iteration: 2234 lambda_k: 1 Loss: 0.002919911144746399\n",
      "Iteration: 2235 lambda_k: 1 Loss: 0.0029199614875789425\n",
      "Iteration: 2236 lambda_k: 1 Loss: 0.0029200115640460912\n",
      "Iteration: 2237 lambda_k: 1 Loss: 0.002920061375546387\n",
      "Iteration: 2238 lambda_k: 1 Loss: 0.0029201109234712193\n",
      "Iteration: 2239 lambda_k: 1 Loss: 0.002920160209204797\n",
      "Iteration: 2240 lambda_k: 1 Loss: 0.0029202092341241628\n",
      "Iteration: 2241 lambda_k: 1 Loss: 0.0029202579995993064\n",
      "Iteration: 2242 lambda_k: 1 Loss: 0.0029203065069930913\n",
      "Iteration: 2243 lambda_k: 1 Loss: 0.0029203547576614293\n",
      "Iteration: 2244 lambda_k: 1 Loss: 0.0029204027529531667\n",
      "Iteration: 2245 lambda_k: 1 Loss: 0.0029204504942102073\n",
      "Iteration: 2246 lambda_k: 1 Loss: 0.0029204979827675494\n",
      "Iteration: 2247 lambda_k: 1 Loss: 0.002920545219953264\n",
      "Iteration: 2248 lambda_k: 1 Loss: 0.0029205922070886098\n",
      "Iteration: 2249 lambda_k: 1 Loss: 0.0029206389454879363\n",
      "Iteration: 2250 lambda_k: 1 Loss: 0.002920685436458929\n",
      "Iteration: 2251 lambda_k: 1 Loss: 0.0029207316813024253\n",
      "Iteration: 2252 lambda_k: 1 Loss: 0.002920777681312583\n",
      "Iteration: 2253 lambda_k: 1 Loss: 0.002920823437776833\n",
      "Iteration: 2254 lambda_k: 1 Loss: 0.0029208689519759803\n",
      "Iteration: 2255 lambda_k: 1 Loss: 0.002920914225184226\n",
      "Iteration: 2256 lambda_k: 1 Loss: 0.002920959258669147\n",
      "Iteration: 2257 lambda_k: 1 Loss: 0.0029210040536917776\n",
      "Iteration: 2258 lambda_k: 1 Loss: 0.002921048611506664\n",
      "Iteration: 2259 lambda_k: 1 Loss: 0.0029210929333617955\n",
      "Iteration: 2260 lambda_k: 1 Loss: 0.0029211370204987493\n",
      "Iteration: 2261 lambda_k: 1 Loss: 0.002921180874152662\n",
      "Iteration: 2262 lambda_k: 1 Loss: 0.002921224495552304\n",
      "Iteration: 2263 lambda_k: 1 Loss: 0.0029212678859200456\n",
      "Iteration: 2264 lambda_k: 1 Loss: 0.0029213110464719532\n",
      "Iteration: 2265 lambda_k: 1 Loss: 0.0029213539784177845\n",
      "Iteration: 2266 lambda_k: 1 Loss: 0.0029213966829610355\n",
      "Iteration: 2267 lambda_k: 1 Loss: 0.0029214391612989708\n",
      "Iteration: 2268 lambda_k: 1 Loss: 0.0029214814146226393\n",
      "Iteration: 2269 lambda_k: 1 Loss: 0.0029215234441169304\n",
      "Iteration: 2270 lambda_k: 1 Loss: 0.0029215652509606027\n",
      "Iteration: 2271 lambda_k: 1 Loss: 0.0029216068363262927\n",
      "Iteration: 2272 lambda_k: 1 Loss: 0.002921648201380528\n",
      "Iteration: 2273 lambda_k: 1 Loss: 0.002921689347283832\n",
      "Iteration: 2274 lambda_k: 1 Loss: 0.0029217302751906827\n",
      "Iteration: 2275 lambda_k: 1 Loss: 0.0029217709862495917\n",
      "Iteration: 2276 lambda_k: 1 Loss: 0.0029218114816030806\n",
      "Iteration: 2277 lambda_k: 1 Loss: 0.0029218517623877493\n",
      "Iteration: 2278 lambda_k: 1 Loss: 0.0029218918297343453\n",
      "Iteration: 2279 lambda_k: 1 Loss: 0.002921931684767685\n",
      "Iteration: 2280 lambda_k: 1 Loss: 0.002921971328606781\n",
      "Iteration: 2281 lambda_k: 1 Loss: 0.0029220107623648068\n",
      "Iteration: 2282 lambda_k: 1 Loss: 0.0029220499871491726\n",
      "Iteration: 2283 lambda_k: 1 Loss: 0.0029220890040615407\n",
      "Iteration: 2284 lambda_k: 1 Loss: 0.002922127814197864\n",
      "Iteration: 2285 lambda_k: 1 Loss: 0.0029221664186483555\n",
      "Iteration: 2286 lambda_k: 1 Loss: 0.002922204818497613\n",
      "Iteration: 2287 lambda_k: 1 Loss: 0.0029222430148245536\n",
      "Iteration: 2288 lambda_k: 1 Loss: 0.00292228100870251\n",
      "Iteration: 2289 lambda_k: 1 Loss: 0.0029223188011992377\n",
      "Iteration: 2290 lambda_k: 1 Loss: 0.002922356393376901\n",
      "Iteration: 2291 lambda_k: 1 Loss: 0.0029223937862921947\n",
      "Iteration: 2292 lambda_k: 1 Loss: 0.0029224309809962787\n",
      "Iteration: 2293 lambda_k: 1 Loss: 0.0029224679785348653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2294 lambda_k: 1 Loss: 0.0029225047799482052\n",
      "Iteration: 2295 lambda_k: 1 Loss: 0.0029225413862711547\n",
      "Iteration: 2296 lambda_k: 1 Loss: 0.0029225777985331567\n",
      "Iteration: 2297 lambda_k: 1 Loss: 0.002922614017758312\n",
      "Iteration: 2298 lambda_k: 1 Loss: 0.002922650044965408\n",
      "Iteration: 2299 lambda_k: 1 Loss: 0.002922685881167895\n",
      "Iteration: 2300 lambda_k: 1 Loss: 0.002922721527373959\n",
      "Iteration: 2301 lambda_k: 1 Loss: 0.0029227569845865463\n",
      "Iteration: 2302 lambda_k: 1 Loss: 0.0029227922538033395\n",
      "Iteration: 2303 lambda_k: 1 Loss: 0.002922827336016847\n",
      "Iteration: 2304 lambda_k: 1 Loss: 0.002922862232214406\n",
      "Iteration: 2305 lambda_k: 1 Loss: 0.0029228969433782127\n",
      "Iteration: 2306 lambda_k: 1 Loss: 0.002922931470485332\n",
      "Iteration: 2307 lambda_k: 1 Loss: 0.0029229658145077226\n",
      "Iteration: 2308 lambda_k: 1 Loss: 0.0029229999764122934\n",
      "Iteration: 2309 lambda_k: 1 Loss: 0.002923033957160904\n",
      "Iteration: 2310 lambda_k: 1 Loss: 0.0029230677577104045\n",
      "Iteration: 2311 lambda_k: 1 Loss: 0.0029231013790126394\n",
      "Iteration: 2312 lambda_k: 1 Loss: 0.002923134822014468\n",
      "Iteration: 2313 lambda_k: 1 Loss: 0.0029231680876578496\n",
      "Iteration: 2314 lambda_k: 1 Loss: 0.002923201176879824\n",
      "Iteration: 2315 lambda_k: 1 Loss: 0.0029232340906124843\n",
      "Iteration: 2316 lambda_k: 1 Loss: 0.0029232668297831305\n",
      "Iteration: 2317 lambda_k: 1 Loss: 0.0029232993953141853\n",
      "Iteration: 2318 lambda_k: 1 Loss: 0.0029233317881232268\n",
      "Iteration: 2319 lambda_k: 1 Loss: 0.002923364009123085\n",
      "Iteration: 2320 lambda_k: 1 Loss: 0.0029233960592217915\n",
      "Iteration: 2321 lambda_k: 1 Loss: 0.0029234279393226654\n",
      "Iteration: 2322 lambda_k: 1 Loss: 0.0029234596503242647\n",
      "Iteration: 2323 lambda_k: 1 Loss: 0.0029234911931204765\n",
      "Iteration: 2324 lambda_k: 1 Loss: 0.0029235225686005174\n",
      "Iteration: 2325 lambda_k: 1 Loss: 0.0029235537776489307\n",
      "Iteration: 2326 lambda_k: 1 Loss: 0.0029235848211456783\n",
      "Iteration: 2327 lambda_k: 1 Loss: 0.002923615699966061\n",
      "Iteration: 2328 lambda_k: 1 Loss: 0.0029236464149808464\n",
      "Iteration: 2329 lambda_k: 1 Loss: 0.0029236769670562455\n",
      "Iteration: 2330 lambda_k: 1 Loss: 0.0029237073570539114\n",
      "Iteration: 2331 lambda_k: 1 Loss: 0.00292373758583101\n",
      "Iteration: 2332 lambda_k: 1 Loss: 0.002923767654240211\n",
      "Iteration: 2333 lambda_k: 1 Loss: 0.002923797563129736\n",
      "Iteration: 2334 lambda_k: 1 Loss: 0.0029238273133433577\n",
      "Iteration: 2335 lambda_k: 1 Loss: 0.0029238569057204195\n",
      "Iteration: 2336 lambda_k: 1 Loss: 0.0029238863410958843\n",
      "Iteration: 2337 lambda_k: 1 Loss: 0.002923915620300336\n",
      "Iteration: 2338 lambda_k: 1 Loss: 0.0029239447441600105\n",
      "Iteration: 2339 lambda_k: 1 Loss: 0.0029239737134968266\n",
      "Iteration: 2340 lambda_k: 1 Loss: 0.002924002529128372\n",
      "Iteration: 2341 lambda_k: 1 Loss: 0.00292403119186797\n",
      "Iteration: 2342 lambda_k: 1 Loss: 0.0029240597025246896\n",
      "Iteration: 2343 lambda_k: 1 Loss: 0.0029240880619033635\n",
      "Iteration: 2344 lambda_k: 1 Loss: 0.002924116270804579\n",
      "Iteration: 2345 lambda_k: 1 Loss: 0.002924144330024739\n",
      "Iteration: 2346 lambda_k: 1 Loss: 0.0029241722403560663\n",
      "Iteration: 2347 lambda_k: 1 Loss: 0.0029242000025866224\n",
      "Iteration: 2348 lambda_k: 1 Loss: 0.0029242276175003756\n",
      "Iteration: 2349 lambda_k: 1 Loss: 0.0029242550858771563\n",
      "Iteration: 2350 lambda_k: 1 Loss: 0.002924282408492683\n",
      "Iteration: 2351 lambda_k: 1 Loss: 0.0029243095861186405\n",
      "Iteration: 2352 lambda_k: 1 Loss: 0.0029243366195226364\n",
      "Iteration: 2353 lambda_k: 1 Loss: 0.0029243635094682863\n",
      "Iteration: 2354 lambda_k: 1 Loss: 0.0029243902567151676\n",
      "Iteration: 2355 lambda_k: 1 Loss: 0.0029244168620188873\n",
      "Iteration: 2356 lambda_k: 1 Loss: 0.0029244433261310676\n",
      "Iteration: 2357 lambda_k: 1 Loss: 0.0029244696497994194\n",
      "Iteration: 2358 lambda_k: 1 Loss: 0.002924495833767695\n",
      "Iteration: 2359 lambda_k: 1 Loss: 0.0029245218787757395\n",
      "Iteration: 2360 lambda_k: 1 Loss: 0.0029245477855595514\n",
      "Iteration: 2361 lambda_k: 1 Loss: 0.0029245735548512177\n",
      "Iteration: 2362 lambda_k: 1 Loss: 0.0029245991873790085\n",
      "Iteration: 2363 lambda_k: 1 Loss: 0.0029246246838673605\n",
      "Iteration: 2364 lambda_k: 1 Loss: 0.0029246500450368995\n",
      "Iteration: 2365 lambda_k: 1 Loss: 0.0029246752716044857\n",
      "Iteration: 2366 lambda_k: 1 Loss: 0.0029247003642831767\n",
      "Iteration: 2367 lambda_k: 1 Loss: 0.002924725323782331\n",
      "Iteration: 2368 lambda_k: 1 Loss: 0.0029247501508075104\n",
      "Iteration: 2369 lambda_k: 1 Loss: 0.0029247748460606324\n",
      "Iteration: 2370 lambda_k: 1 Loss: 0.0029247994102399168\n",
      "Iteration: 2371 lambda_k: 1 Loss: 0.00292482384403986\n",
      "Iteration: 2372 lambda_k: 1 Loss: 0.0029248481481513483\n",
      "Iteration: 2373 lambda_k: 1 Loss: 0.002924872323261626\n",
      "Iteration: 2374 lambda_k: 1 Loss: 0.002924896370054348\n",
      "Iteration: 2375 lambda_k: 1 Loss: 0.002924920289209544\n",
      "Iteration: 2376 lambda_k: 1 Loss: 0.0029249440814036673\n",
      "Iteration: 2377 lambda_k: 1 Loss: 0.002924967747309618\n",
      "Iteration: 2378 lambda_k: 1 Loss: 0.0029249912875967572\n",
      "Iteration: 2379 lambda_k: 1 Loss: 0.002925014702930904\n",
      "Iteration: 2380 lambda_k: 1 Loss: 0.002925037993974405\n",
      "Iteration: 2381 lambda_k: 1 Loss: 0.0029250611613861116\n",
      "Iteration: 2382 lambda_k: 1 Loss: 0.002925084205821399\n",
      "Iteration: 2383 lambda_k: 1 Loss: 0.0029251071279322097\n",
      "Iteration: 2384 lambda_k: 1 Loss: 0.0029251299283670163\n",
      "Iteration: 2385 lambda_k: 1 Loss: 0.0029251526077709207\n",
      "Iteration: 2386 lambda_k: 1 Loss: 0.0029251751667856134\n",
      "Iteration: 2387 lambda_k: 1 Loss: 0.002925197606049374\n",
      "Iteration: 2388 lambda_k: 1 Loss: 0.002925219926197159\n",
      "Iteration: 2389 lambda_k: 1 Loss: 0.002925242127860579\n",
      "Iteration: 2390 lambda_k: 1 Loss: 0.002925264211667898\n",
      "Iteration: 2391 lambda_k: 1 Loss: 0.002925286178244068\n",
      "Iteration: 2392 lambda_k: 1 Loss: 0.0029253080282107577\n",
      "Iteration: 2393 lambda_k: 1 Loss: 0.002925329762186382\n",
      "Iteration: 2394 lambda_k: 1 Loss: 0.002925351380786064\n",
      "Iteration: 2395 lambda_k: 1 Loss: 0.002925372884621696\n",
      "Iteration: 2396 lambda_k: 1 Loss: 0.00292539427430196\n",
      "Iteration: 2397 lambda_k: 1 Loss: 0.0029254155504323063\n",
      "Iteration: 2398 lambda_k: 1 Loss: 0.002925436713614998\n",
      "Iteration: 2399 lambda_k: 1 Loss: 0.002925457764449129\n",
      "Iteration: 2400 lambda_k: 1 Loss: 0.002925478703530629\n",
      "Iteration: 2401 lambda_k: 1 Loss: 0.0029254995314523264\n",
      "Iteration: 2402 lambda_k: 1 Loss: 0.002925520248803842\n",
      "Iteration: 2403 lambda_k: 1 Loss: 0.0029255408561717545\n",
      "Iteration: 2404 lambda_k: 1 Loss: 0.0029255613541395156\n",
      "Iteration: 2405 lambda_k: 1 Loss: 0.0029255817432875153\n",
      "Iteration: 2406 lambda_k: 1 Loss: 0.002925602024193071\n",
      "Iteration: 2407 lambda_k: 1 Loss: 0.002925622197430456\n",
      "Iteration: 2408 lambda_k: 1 Loss: 0.0029256422635709316\n",
      "Iteration: 2409 lambda_k: 1 Loss: 0.002925662223182732\n",
      "Iteration: 2410 lambda_k: 1 Loss: 0.0029256820768310643\n",
      "Iteration: 2411 lambda_k: 1 Loss: 0.0029257018250781986\n",
      "Iteration: 2412 lambda_k: 1 Loss: 0.0029257214684834253\n",
      "Iteration: 2413 lambda_k: 1 Loss: 0.0029257410076030453\n",
      "Iteration: 2414 lambda_k: 1 Loss: 0.0029257604429904827\n",
      "Iteration: 2415 lambda_k: 1 Loss: 0.0029257797751962\n",
      "Iteration: 2416 lambda_k: 1 Loss: 0.0029257990047677454\n",
      "Iteration: 2417 lambda_k: 1 Loss: 0.0029258181322498224\n",
      "Iteration: 2418 lambda_k: 1 Loss: 0.0029258371581842363\n",
      "Iteration: 2419 lambda_k: 1 Loss: 0.0029258560831098923\n",
      "Iteration: 2420 lambda_k: 1 Loss: 0.0029258749075628913\n",
      "Iteration: 2421 lambda_k: 1 Loss: 0.0029258936320764994\n",
      "Iteration: 2422 lambda_k: 1 Loss: 0.0029259122571811584\n",
      "Iteration: 2423 lambda_k: 1 Loss: 0.0029259307834045194\n",
      "Iteration: 2424 lambda_k: 1 Loss: 0.0029259492112714376\n",
      "Iteration: 2425 lambda_k: 1 Loss: 0.002925967541303975\n",
      "Iteration: 2426 lambda_k: 1 Loss: 0.0029259857740214427\n",
      "Iteration: 2427 lambda_k: 1 Loss: 0.0029260039099404406\n",
      "Iteration: 2428 lambda_k: 1 Loss: 0.002926021949574819\n",
      "Iteration: 2429 lambda_k: 1 Loss: 0.0029260398934356888\n",
      "Iteration: 2430 lambda_k: 1 Loss: 0.002926057742031518\n",
      "Iteration: 2431 lambda_k: 1 Loss: 0.0029260754958680183\n",
      "Iteration: 2432 lambda_k: 1 Loss: 0.0029260931554482795\n",
      "Iteration: 2433 lambda_k: 1 Loss: 0.0029261107212727026\n",
      "Iteration: 2434 lambda_k: 1 Loss: 0.0029261281938390608\n",
      "Iteration: 2435 lambda_k: 1 Loss: 0.0029261455736424727\n",
      "Iteration: 2436 lambda_k: 1 Loss: 0.0029261628611754516\n",
      "Iteration: 2437 lambda_k: 1 Loss: 0.002926180056927939\n",
      "Iteration: 2438 lambda_k: 1 Loss: 0.0029261971613872115\n",
      "Iteration: 2439 lambda_k: 1 Loss: 0.0029262141750380547\n",
      "Iteration: 2440 lambda_k: 1 Loss: 0.002926231098362616\n",
      "Iteration: 2441 lambda_k: 1 Loss: 0.002926247931840534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2442 lambda_k: 1 Loss: 0.0029262646759489136\n",
      "Iteration: 2443 lambda_k: 1 Loss: 0.00292628133116231\n",
      "Iteration: 2444 lambda_k: 1 Loss: 0.00292629789795278\n",
      "Iteration: 2445 lambda_k: 1 Loss: 0.002926314376789889\n",
      "Iteration: 2446 lambda_k: 1 Loss: 0.0029263307681407116\n",
      "Iteration: 2447 lambda_k: 1 Loss: 0.002926347072469856\n",
      "Iteration: 2448 lambda_k: 1 Loss: 0.0029263632902394765\n",
      "Iteration: 2449 lambda_k: 1 Loss: 0.0029263794219092434\n",
      "Iteration: 2450 lambda_k: 1 Loss: 0.002926395467936444\n",
      "Iteration: 2451 lambda_k: 1 Loss: 0.002926411428775924\n",
      "Iteration: 2452 lambda_k: 1 Loss: 0.002926427304880124\n",
      "Iteration: 2453 lambda_k: 1 Loss: 0.002926443096699079\n",
      "Iteration: 2454 lambda_k: 1 Loss: 0.0029264588046804705\n",
      "Iteration: 2455 lambda_k: 1 Loss: 0.0029264744292695514\n",
      "Iteration: 2456 lambda_k: 1 Loss: 0.0029264899709092987\n",
      "Iteration: 2457 lambda_k: 1 Loss: 0.0029265054300402792\n",
      "Iteration: 2458 lambda_k: 1 Loss: 0.002926520807100777\n",
      "Iteration: 2459 lambda_k: 1 Loss: 0.002926536102526716\n",
      "Iteration: 2460 lambda_k: 1 Loss: 0.0029265513167517025\n",
      "Iteration: 2461 lambda_k: 1 Loss: 0.0029265664502070767\n",
      "Iteration: 2462 lambda_k: 1 Loss: 0.0029265815033219017\n",
      "Iteration: 2463 lambda_k: 1 Loss: 0.0029265964765229423\n",
      "Iteration: 2464 lambda_k: 1 Loss: 0.0029266113702347265\n",
      "Iteration: 2465 lambda_k: 1 Loss: 0.0029266261848795065\n",
      "Iteration: 2466 lambda_k: 1 Loss: 0.0029266409208773196\n",
      "Iteration: 2467 lambda_k: 1 Loss: 0.002926655578645959\n",
      "Iteration: 2468 lambda_k: 1 Loss: 0.0029266701586010075\n",
      "Iteration: 2469 lambda_k: 1 Loss: 0.0029266846611558607\n",
      "Iteration: 2470 lambda_k: 1 Loss: 0.0029266990867217327\n",
      "Iteration: 2471 lambda_k: 1 Loss: 0.002926713435707623\n",
      "Iteration: 2472 lambda_k: 1 Loss: 0.0029267277085203896\n",
      "Iteration: 2473 lambda_k: 1 Loss: 0.0029267419055647452\n",
      "Iteration: 2474 lambda_k: 1 Loss: 0.002926756027243229\n",
      "Iteration: 2475 lambda_k: 1 Loss: 0.002926770073956246\n",
      "Iteration: 2476 lambda_k: 1 Loss: 0.00292678404610209\n",
      "Iteration: 2477 lambda_k: 1 Loss: 0.0029267979440769695\n",
      "Iteration: 2478 lambda_k: 1 Loss: 0.0029268117682749654\n",
      "Iteration: 2479 lambda_k: 1 Loss: 0.002926825519088073\n",
      "Iteration: 2480 lambda_k: 1 Loss: 0.00292683919690621\n",
      "Iteration: 2481 lambda_k: 1 Loss: 0.0029268528021172257\n",
      "Iteration: 2482 lambda_k: 1 Loss: 0.0029268663351068947\n",
      "Iteration: 2483 lambda_k: 1 Loss: 0.002926879796258976\n",
      "Iteration: 2484 lambda_k: 1 Loss: 0.002926893185955199\n",
      "Iteration: 2485 lambda_k: 1 Loss: 0.0029269065045752435\n",
      "Iteration: 2486 lambda_k: 1 Loss: 0.0029269197524967846\n",
      "Iteration: 2487 lambda_k: 1 Loss: 0.0029269329300955094\n",
      "Iteration: 2488 lambda_k: 1 Loss: 0.0029269460377451\n",
      "Iteration: 2489 lambda_k: 1 Loss: 0.0029269590758172386\n",
      "Iteration: 2490 lambda_k: 1 Loss: 0.0029269720446816715\n",
      "Iteration: 2491 lambda_k: 1 Loss: 0.002926984944706174\n",
      "Iteration: 2492 lambda_k: 1 Loss: 0.002926997776256541\n",
      "Iteration: 2493 lambda_k: 1 Loss: 0.0029270105396966527\n",
      "Iteration: 2494 lambda_k: 1 Loss: 0.002927023235388481\n",
      "Iteration: 2495 lambda_k: 1 Loss: 0.0029270358636920334\n",
      "Iteration: 2496 lambda_k: 1 Loss: 0.002927048424965429\n",
      "Iteration: 2497 lambda_k: 1 Loss: 0.0029270609195648787\n",
      "Iteration: 2498 lambda_k: 1 Loss: 0.002927073347844717\n",
      "Iteration: 2499 lambda_k: 1 Loss: 0.0029270857101573937\n",
      "Iteration: 2500 lambda_k: 1 Loss: 0.0029270980068534795\n",
      "Iteration: 2501 lambda_k: 1 Loss: 0.002927110238281707\n",
      "Iteration: 2502 lambda_k: 1 Loss: 0.0029271224047889193\n",
      "Iteration: 2503 lambda_k: 1 Loss: 0.0029271345067201685\n",
      "Iteration: 2504 lambda_k: 1 Loss: 0.0029271465444186237\n",
      "Iteration: 2505 lambda_k: 1 Loss: 0.002927158518225676\n",
      "Iteration: 2506 lambda_k: 1 Loss: 0.0029271704284808847\n",
      "Iteration: 2507 lambda_k: 1 Loss: 0.002927182275522008\n",
      "Iteration: 2508 lambda_k: 1 Loss: 0.0029271940596850223\n",
      "Iteration: 2509 lambda_k: 1 Loss: 0.0029272057813041078\n",
      "Iteration: 2510 lambda_k: 1 Loss: 0.0029272174407116752\n",
      "Iteration: 2511 lambda_k: 1 Loss: 0.0029272290382383707\n",
      "Iteration: 2512 lambda_k: 1 Loss: 0.002927240574213086\n",
      "Iteration: 2513 lambda_k: 1 Loss: 0.00292725204896297\n",
      "Iteration: 2514 lambda_k: 1 Loss: 0.002927263462813418\n",
      "Iteration: 2515 lambda_k: 1 Loss: 0.002927274816088121\n",
      "Iteration: 2516 lambda_k: 1 Loss: 0.002927286109109036\n",
      "Iteration: 2517 lambda_k: 1 Loss: 0.002927297342196426\n",
      "Iteration: 2518 lambda_k: 1 Loss: 0.002927308515668831\n",
      "Iteration: 2519 lambda_k: 1 Loss: 0.002927319629843132\n",
      "Iteration: 2520 lambda_k: 1 Loss: 0.002927330685034489\n",
      "Iteration: 2521 lambda_k: 1 Loss: 0.002927341681556414\n",
      "Iteration: 2522 lambda_k: 1 Loss: 0.0029273526197207502\n",
      "Iteration: 2523 lambda_k: 1 Loss: 0.002927363499837669\n",
      "Iteration: 2524 lambda_k: 1 Loss: 0.0029273743222157276\n",
      "Iteration: 2525 lambda_k: 1 Loss: 0.0029273850871618197\n",
      "Iteration: 2526 lambda_k: 1 Loss: 0.0029273957949812095\n",
      "Iteration: 2527 lambda_k: 1 Loss: 0.0029274064459775323\n",
      "Iteration: 2528 lambda_k: 1 Loss: 0.0029274170404528462\n",
      "Iteration: 2529 lambda_k: 1 Loss: 0.0029274275787075877\n",
      "Iteration: 2530 lambda_k: 1 Loss: 0.0029274380610405776\n",
      "Iteration: 2531 lambda_k: 1 Loss: 0.0029274484877490612\n",
      "Iteration: 2532 lambda_k: 1 Loss: 0.002927458859128704\n",
      "Iteration: 2533 lambda_k: 1 Loss: 0.002927469175473621\n",
      "Iteration: 2534 lambda_k: 1 Loss: 0.0029274794370763354\n",
      "Iteration: 2535 lambda_k: 1 Loss: 0.002927489644227838\n",
      "Iteration: 2536 lambda_k: 1 Loss: 0.002927499797217545\n",
      "Iteration: 2537 lambda_k: 1 Loss: 0.0029275098963333643\n",
      "Iteration: 2538 lambda_k: 1 Loss: 0.002927519941861671\n",
      "Iteration: 2539 lambda_k: 1 Loss: 0.002927529934087306\n",
      "Iteration: 2540 lambda_k: 1 Loss: 0.002927539873293621\n",
      "Iteration: 2541 lambda_k: 1 Loss: 0.0029275497597624238\n",
      "Iteration: 2542 lambda_k: 1 Loss: 0.002927559593774047\n",
      "Iteration: 2543 lambda_k: 1 Loss: 0.002927569375607344\n",
      "Iteration: 2544 lambda_k: 1 Loss: 0.0029275791055396686\n",
      "Iteration: 2545 lambda_k: 1 Loss: 0.002927588783846902\n",
      "Iteration: 2546 lambda_k: 1 Loss: 0.0029275984108034676\n",
      "Iteration: 2547 lambda_k: 1 Loss: 0.0029276079866823383\n",
      "Iteration: 2548 lambda_k: 1 Loss: 0.002927617511755009\n",
      "Iteration: 2549 lambda_k: 1 Loss: 0.0029276269862915675\n",
      "Iteration: 2550 lambda_k: 1 Loss: 0.002927636410560617\n",
      "Iteration: 2551 lambda_k: 1 Loss: 0.0029276457848293838\n",
      "Iteration: 2552 lambda_k: 1 Loss: 0.002927655109363647\n",
      "Iteration: 2553 lambda_k: 1 Loss: 0.002927664384427785\n",
      "Iteration: 2554 lambda_k: 1 Loss: 0.002927673610284765\n",
      "Iteration: 2555 lambda_k: 1 Loss: 0.00292768278719615\n",
      "Iteration: 2556 lambda_k: 1 Loss: 0.002927691915422123\n",
      "Iteration: 2557 lambda_k: 1 Loss: 0.0029277009952214717\n",
      "Iteration: 2558 lambda_k: 1 Loss: 0.0029277100268516264\n",
      "Iteration: 2559 lambda_k: 1 Loss: 0.0029277190105686367\n",
      "Iteration: 2560 lambda_k: 1 Loss: 0.002927727946627208\n",
      "Iteration: 2561 lambda_k: 1 Loss: 0.002927736835280647\n",
      "Iteration: 2562 lambda_k: 1 Loss: 0.0029277456767809623\n",
      "Iteration: 2563 lambda_k: 1 Loss: 0.0029277544713788117\n",
      "Iteration: 2564 lambda_k: 1 Loss: 0.0029277632193235137\n",
      "Iteration: 2565 lambda_k: 1 Loss: 0.002927771920863053\n",
      "Iteration: 2566 lambda_k: 1 Loss: 0.0029277805762441054\n",
      "Iteration: 2567 lambda_k: 1 Loss: 0.002927789185712032\n",
      "Iteration: 2568 lambda_k: 1 Loss: 0.002927797749510895\n",
      "Iteration: 2569 lambda_k: 1 Loss: 0.002927806267883445\n",
      "Iteration: 2570 lambda_k: 1 Loss: 0.00292781474107116\n",
      "Iteration: 2571 lambda_k: 1 Loss: 0.002927823169314235\n",
      "Iteration: 2572 lambda_k: 1 Loss: 0.0029278315528515637\n",
      "Iteration: 2573 lambda_k: 1 Loss: 0.002927839891920799\n",
      "Iteration: 2574 lambda_k: 1 Loss: 0.0029278481867583035\n",
      "Iteration: 2575 lambda_k: 1 Loss: 0.002927856437599193\n",
      "Iteration: 2576 lambda_k: 1 Loss: 0.002927864644677352\n",
      "Iteration: 2577 lambda_k: 1 Loss: 0.0029278728082253857\n",
      "Iteration: 2578 lambda_k: 1 Loss: 0.0029278809284746846\n",
      "Iteration: 2579 lambda_k: 1 Loss: 0.002927889005655428\n",
      "Iteration: 2580 lambda_k: 1 Loss: 0.0029278970399965365\n",
      "Iteration: 2581 lambda_k: 1 Loss: 0.0029279050317257208\n",
      "Iteration: 2582 lambda_k: 1 Loss: 0.002927912981069471\n",
      "Iteration: 2583 lambda_k: 1 Loss: 0.0029279208882531152\n",
      "Iteration: 2584 lambda_k: 1 Loss: 0.0029279287535007325\n",
      "Iteration: 2585 lambda_k: 1 Loss: 0.002927936577035249\n",
      "Iteration: 2586 lambda_k: 1 Loss: 0.0029279443590783916\n",
      "Iteration: 2587 lambda_k: 1 Loss: 0.002927952099850695\n",
      "Iteration: 2588 lambda_k: 1 Loss: 0.0029279597995715187\n",
      "Iteration: 2589 lambda_k: 1 Loss: 0.002927967458459089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2590 lambda_k: 1 Loss: 0.00292797507673046\n",
      "Iteration: 2591 lambda_k: 1 Loss: 0.002927982654601489\n",
      "Iteration: 2592 lambda_k: 1 Loss: 0.0029279901922869265\n",
      "Iteration: 2593 lambda_k: 1 Loss: 0.002927997690000381\n",
      "Iteration: 2594 lambda_k: 1 Loss: 0.0029280051479543207\n",
      "Iteration: 2595 lambda_k: 1 Loss: 0.0029280125663600806\n",
      "Iteration: 2596 lambda_k: 1 Loss: 0.002928019945427854\n",
      "Iteration: 2597 lambda_k: 1 Loss: 0.002928027285366729\n",
      "Iteration: 2598 lambda_k: 1 Loss: 0.002928034586384701\n",
      "Iteration: 2599 lambda_k: 1 Loss: 0.0029280418486886347\n",
      "Iteration: 2600 lambda_k: 1 Loss: 0.0029280490724843\n",
      "Iteration: 2601 lambda_k: 1 Loss: 0.0029280562579763725\n",
      "Iteration: 2602 lambda_k: 1 Loss: 0.0029280634053684513\n",
      "Iteration: 2603 lambda_k: 1 Loss: 0.0029280705148630333\n",
      "Iteration: 2604 lambda_k: 1 Loss: 0.0029280775866615577\n",
      "Iteration: 2605 lambda_k: 1 Loss: 0.002928084620964371\n",
      "Iteration: 2606 lambda_k: 1 Loss: 0.002928091617970754\n",
      "Iteration: 2607 lambda_k: 1 Loss: 0.00292809857787895\n",
      "Iteration: 2608 lambda_k: 1 Loss: 0.002928105500886117\n",
      "Iteration: 2609 lambda_k: 1 Loss: 0.002928112387188391\n",
      "Iteration: 2610 lambda_k: 1 Loss: 0.002928119236980858\n",
      "Iteration: 2611 lambda_k: 1 Loss: 0.0029281260504575683\n",
      "Iteration: 2612 lambda_k: 1 Loss: 0.0029281328278115294\n",
      "Iteration: 2613 lambda_k: 1 Loss: 0.0029281395692347264\n",
      "Iteration: 2614 lambda_k: 1 Loss: 0.002928146274918135\n",
      "Iteration: 2615 lambda_k: 1 Loss: 0.0029281529450516865\n",
      "Iteration: 2616 lambda_k: 1 Loss: 0.0029281595798243284\n",
      "Iteration: 2617 lambda_k: 1 Loss: 0.0029281661794239877\n",
      "Iteration: 2618 lambda_k: 1 Loss: 0.002928172744037598\n",
      "Iteration: 2619 lambda_k: 1 Loss: 0.0029281792738511093\n",
      "Iteration: 2620 lambda_k: 1 Loss: 0.0029281857690494537\n",
      "Iteration: 2621 lambda_k: 1 Loss: 0.0029281922298165985\n",
      "Iteration: 2622 lambda_k: 1 Loss: 0.0029281986563355357\n",
      "Iteration: 2623 lambda_k: 1 Loss: 0.002928205048788279\n",
      "Iteration: 2624 lambda_k: 1 Loss: 0.002928211407355858\n",
      "Iteration: 2625 lambda_k: 1 Loss: 0.0029282177322183622\n",
      "Iteration: 2626 lambda_k: 1 Loss: 0.0029282240235548943\n",
      "Iteration: 2627 lambda_k: 1 Loss: 0.0029282302815436495\n",
      "Iteration: 2628 lambda_k: 1 Loss: 0.002928236506361842\n",
      "Iteration: 2629 lambda_k: 1 Loss: 0.002928242698185753\n",
      "Iteration: 2630 lambda_k: 1 Loss: 0.002928248857190708\n",
      "Iteration: 2631 lambda_k: 1 Loss: 0.0029282549835511224\n",
      "Iteration: 2632 lambda_k: 1 Loss: 0.0029282610774404765\n",
      "Iteration: 2633 lambda_k: 1 Loss: 0.00292826713903132\n",
      "Iteration: 2634 lambda_k: 1 Loss: 0.0029282731684952914\n",
      "Iteration: 2635 lambda_k: 1 Loss: 0.0029282791660031116\n",
      "Iteration: 2636 lambda_k: 1 Loss: 0.0029282851317245892\n",
      "Iteration: 2637 lambda_k: 1 Loss: 0.0029282910658286385\n",
      "Iteration: 2638 lambda_k: 1 Loss: 0.0029282969684832952\n",
      "Iteration: 2639 lambda_k: 1 Loss: 0.0029283028398556475\n",
      "Iteration: 2640 lambda_k: 1 Loss: 0.002928308680111923\n",
      "Iteration: 2641 lambda_k: 1 Loss: 0.002928314489417482\n",
      "Iteration: 2642 lambda_k: 1 Loss: 0.0029283202679367933\n",
      "Iteration: 2643 lambda_k: 1 Loss: 0.002928326015833433\n",
      "Iteration: 2644 lambda_k: 1 Loss: 0.0029283317332701327\n",
      "Iteration: 2645 lambda_k: 1 Loss: 0.00292833742040874\n",
      "Iteration: 2646 lambda_k: 1 Loss: 0.002928343077410236\n",
      "Iteration: 2647 lambda_k: 1 Loss: 0.0029283487044347565\n",
      "Iteration: 2648 lambda_k: 1 Loss: 0.002928354301641594\n",
      "Iteration: 2649 lambda_k: 1 Loss: 0.002928359869189188\n",
      "Iteration: 2650 lambda_k: 1 Loss: 0.0029283654072351155\n",
      "Iteration: 2651 lambda_k: 1 Loss: 0.00292837091593613\n",
      "Iteration: 2652 lambda_k: 1 Loss: 0.0029283763954481494\n",
      "Iteration: 2653 lambda_k: 1 Loss: 0.002928381845926278\n",
      "Iteration: 2654 lambda_k: 1 Loss: 0.0029283872675247556\n",
      "Iteration: 2655 lambda_k: 1 Loss: 0.0029283926603970428\n",
      "Iteration: 2656 lambda_k: 1 Loss: 0.00292839802469575\n",
      "Iteration: 2657 lambda_k: 1 Loss: 0.0029284033605726854\n",
      "Iteration: 2658 lambda_k: 1 Loss: 0.0029284086681788646\n",
      "Iteration: 2659 lambda_k: 1 Loss: 0.002928413947664496\n",
      "Iteration: 2660 lambda_k: 1 Loss: 0.0029284191991789787\n",
      "Iteration: 2661 lambda_k: 1 Loss: 0.002928424422870922\n",
      "Iteration: 2662 lambda_k: 1 Loss: 0.0029284296188881232\n",
      "Iteration: 2663 lambda_k: 1 Loss: 0.0029284347873776303\n",
      "Iteration: 2664 lambda_k: 1 Loss: 0.0029284399284856795\n",
      "Iteration: 2665 lambda_k: 1 Loss: 0.002928445042357753\n",
      "Iteration: 2666 lambda_k: 1 Loss: 0.002928450129138535\n",
      "Iteration: 2667 lambda_k: 1 Loss: 0.0029284551889719552\n",
      "Iteration: 2668 lambda_k: 1 Loss: 0.0029284602220011617\n",
      "Iteration: 2669 lambda_k: 1 Loss: 0.0029284652283685505\n",
      "Iteration: 2670 lambda_k: 1 Loss: 0.0029284702082157613\n",
      "Iteration: 2671 lambda_k: 1 Loss: 0.0029284751616836923\n",
      "Iteration: 2672 lambda_k: 1 Loss: 0.0029284800889124696\n",
      "Iteration: 2673 lambda_k: 1 Loss: 0.0029284849900414645\n",
      "Iteration: 2674 lambda_k: 1 Loss: 0.002928489865209346\n",
      "Iteration: 2675 lambda_k: 1 Loss: 0.002928494714554015\n",
      "Iteration: 2676 lambda_k: 1 Loss: 0.0029284995382126626\n",
      "Iteration: 2677 lambda_k: 1 Loss: 0.0029285043363216894\n",
      "Iteration: 2678 lambda_k: 1 Loss: 0.0029285091090168543\n",
      "Iteration: 2679 lambda_k: 1 Loss: 0.002928513856433141\n",
      "Iteration: 2680 lambda_k: 1 Loss: 0.0029285185787048165\n",
      "Iteration: 2681 lambda_k: 1 Loss: 0.0029285232759654494\n",
      "Iteration: 2682 lambda_k: 1 Loss: 0.002928527948347909\n",
      "Iteration: 2683 lambda_k: 1 Loss: 0.0029285325959843137\n",
      "Iteration: 2684 lambda_k: 1 Loss: 0.002928537219006125\n",
      "Iteration: 2685 lambda_k: 1 Loss: 0.0029285418175440735\n",
      "Iteration: 2686 lambda_k: 1 Loss: 0.0029285463917282095\n",
      "Iteration: 2687 lambda_k: 1 Loss: 0.002928550941687881\n",
      "Iteration: 2688 lambda_k: 1 Loss: 0.002928555467551756\n",
      "Iteration: 2689 lambda_k: 1 Loss: 0.0029285599694478075\n",
      "Iteration: 2690 lambda_k: 1 Loss: 0.0029285644475033653\n",
      "Iteration: 2691 lambda_k: 1 Loss: 0.0029285689018450183\n",
      "Iteration: 2692 lambda_k: 1 Loss: 0.00292857333259874\n",
      "Iteration: 2693 lambda_k: 1 Loss: 0.002928577739889794\n",
      "Iteration: 2694 lambda_k: 1 Loss: 0.002928582123842791\n",
      "Iteration: 2695 lambda_k: 1 Loss: 0.0029285864845816755\n",
      "Iteration: 2696 lambda_k: 1 Loss: 0.0029285908222297513\n",
      "Iteration: 2697 lambda_k: 1 Loss: 0.00292859513690963\n",
      "Iteration: 2698 lambda_k: 1 Loss: 0.0029285994287433005\n",
      "Iteration: 2699 lambda_k: 1 Loss: 0.0029286036978520944\n",
      "Iteration: 2700 lambda_k: 1 Loss: 0.0029286079443566824\n",
      "Iteration: 2701 lambda_k: 1 Loss: 0.002928612168377121\n",
      "Iteration: 2702 lambda_k: 1 Loss: 0.0029286163700327924\n",
      "Iteration: 2703 lambda_k: 1 Loss: 0.0029286205494424827\n",
      "Iteration: 2704 lambda_k: 1 Loss: 0.002928624706724339\n",
      "Iteration: 2705 lambda_k: 1 Loss: 0.0029286288419958497\n",
      "Iteration: 2706 lambda_k: 1 Loss: 0.002928632955373895\n",
      "Iteration: 2707 lambda_k: 1 Loss: 0.0029286370469747453\n",
      "Iteration: 2708 lambda_k: 1 Loss: 0.00292864111691403\n",
      "Iteration: 2709 lambda_k: 1 Loss: 0.0029286451653067723\n",
      "Iteration: 2710 lambda_k: 1 Loss: 0.002928649192267376\n",
      "Iteration: 2711 lambda_k: 1 Loss: 0.002928653197909658\n",
      "Iteration: 2712 lambda_k: 1 Loss: 0.0029286571823468055\n",
      "Iteration: 2713 lambda_k: 1 Loss: 0.002928661145691448\n",
      "Iteration: 2714 lambda_k: 1 Loss: 0.002928665088055551\n",
      "Iteration: 2715 lambda_k: 1 Loss: 0.002928669009550523\n",
      "Iteration: 2716 lambda_k: 1 Loss: 0.002928672910287155\n",
      "Iteration: 2717 lambda_k: 1 Loss: 0.0029286767903756753\n",
      "Iteration: 2718 lambda_k: 1 Loss: 0.002928680649925724\n",
      "Iteration: 2719 lambda_k: 1 Loss: 0.0029286844890463377\n",
      "Iteration: 2720 lambda_k: 1 Loss: 0.0029286883078459935\n",
      "Iteration: 2721 lambda_k: 1 Loss: 0.0029286921064325794\n",
      "Iteration: 2722 lambda_k: 1 Loss: 0.002928695884913396\n",
      "Iteration: 2723 lambda_k: 1 Loss: 0.002928699643395206\n",
      "Iteration: 2724 lambda_k: 1 Loss: 0.0029287033819841678\n",
      "Iteration: 2725 lambda_k: 1 Loss: 0.0029287071007859174\n",
      "Iteration: 2726 lambda_k: 1 Loss: 0.002928710799905495\n",
      "Iteration: 2727 lambda_k: 1 Loss: 0.0029287144794473675\n",
      "Iteration: 2728 lambda_k: 1 Loss: 0.002928718139515484\n",
      "Iteration: 2729 lambda_k: 1 Loss: 0.002928721780213229\n",
      "Iteration: 2730 lambda_k: 1 Loss: 0.0029287254016434425\n",
      "Iteration: 2731 lambda_k: 1 Loss: 0.002928729003908397\n",
      "Iteration: 2732 lambda_k: 1 Loss: 0.0029287325871098346\n",
      "Iteration: 2733 lambda_k: 1 Loss: 0.0029287361513489593\n",
      "Iteration: 2734 lambda_k: 1 Loss: 0.002928739696726421\n",
      "Iteration: 2735 lambda_k: 1 Loss: 0.002928743223342339\n",
      "Iteration: 2736 lambda_k: 1 Loss: 0.0029287467312962994\n",
      "Iteration: 2737 lambda_k: 1 Loss: 0.002928750220687369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2738 lambda_k: 1 Loss: 0.0029287536916140925\n",
      "Iteration: 2739 lambda_k: 1 Loss: 0.002928757144174465\n",
      "Iteration: 2740 lambda_k: 1 Loss: 0.002928760578465986\n",
      "Iteration: 2741 lambda_k: 1 Loss: 0.002928763994585611\n",
      "Iteration: 2742 lambda_k: 1 Loss: 0.002928767392629822\n",
      "Iteration: 2743 lambda_k: 1 Loss: 0.0029287707726945323\n",
      "Iteration: 2744 lambda_k: 1 Loss: 0.002928774134875173\n",
      "Iteration: 2745 lambda_k: 1 Loss: 0.002928777479266672\n",
      "Iteration: 2746 lambda_k: 1 Loss: 0.0029287808059634463\n",
      "Iteration: 2747 lambda_k: 1 Loss: 0.002928784115059398\n",
      "Iteration: 2748 lambda_k: 1 Loss: 0.0029287874066479544\n",
      "Iteration: 2749 lambda_k: 1 Loss: 0.002928790680822016\n",
      "Iteration: 2750 lambda_k: 1 Loss: 0.00292879393767402\n",
      "Iteration: 2751 lambda_k: 1 Loss: 0.0029287971772958906\n",
      "Iteration: 2752 lambda_k: 1 Loss: 0.002928800399779064\n",
      "Iteration: 2753 lambda_k: 1 Loss: 0.002928803605214517\n",
      "Iteration: 2754 lambda_k: 1 Loss: 0.0029288067936926944\n",
      "Iteration: 2755 lambda_k: 1 Loss: 0.0029288099653036\n",
      "Iteration: 2756 lambda_k: 1 Loss: 0.00292881312013673\n",
      "Iteration: 2757 lambda_k: 1 Loss: 0.0029288162582811338\n",
      "Iteration: 2758 lambda_k: 1 Loss: 0.0029288193798253635\n",
      "Iteration: 2759 lambda_k: 1 Loss: 0.002928822484857516\n",
      "Iteration: 2760 lambda_k: 1 Loss: 0.002928825573465198\n",
      "Iteration: 2761 lambda_k: 1 Loss: 0.002928828645735588\n",
      "Iteration: 2762 lambda_k: 1 Loss: 0.0029288317017553698\n",
      "Iteration: 2763 lambda_k: 1 Loss: 0.002928834741610771\n",
      "Iteration: 2764 lambda_k: 1 Loss: 0.0029288377653875616\n",
      "Iteration: 2765 lambda_k: 1 Loss: 0.0029288407731710396\n",
      "Iteration: 2766 lambda_k: 1 Loss: 0.002928843765046078\n",
      "Iteration: 2767 lambda_k: 1 Loss: 0.0029288467410970897\n",
      "Iteration: 2768 lambda_k: 1 Loss: 0.0029288497014080357\n",
      "Iteration: 2769 lambda_k: 1 Loss: 0.0029288526460624228\n",
      "Iteration: 2770 lambda_k: 1 Loss: 0.0029288555751433125\n",
      "Iteration: 2771 lambda_k: 1 Loss: 0.002928858488733342\n",
      "Iteration: 2772 lambda_k: 1 Loss: 0.0029288613869147017\n",
      "Iteration: 2773 lambda_k: 1 Loss: 0.0029288642697691395\n",
      "Iteration: 2774 lambda_k: 1 Loss: 0.0029288671373779497\n",
      "Iteration: 2775 lambda_k: 1 Loss: 0.002928869989822051\n",
      "Iteration: 2776 lambda_k: 1 Loss: 0.0029288728271818732\n",
      "Iteration: 2777 lambda_k: 1 Loss: 0.0029288756495374478\n",
      "Iteration: 2778 lambda_k: 1 Loss: 0.0029288784569683752\n",
      "Iteration: 2779 lambda_k: 1 Loss: 0.0029288812495538224\n",
      "Iteration: 2780 lambda_k: 1 Loss: 0.002928884027372545\n",
      "Iteration: 2781 lambda_k: 1 Loss: 0.0029288867905028775\n",
      "Iteration: 2782 lambda_k: 1 Loss: 0.002928889539022737\n",
      "Iteration: 2783 lambda_k: 1 Loss: 0.002928892273009626\n",
      "Iteration: 2784 lambda_k: 1 Loss: 0.0029288949925406504\n",
      "Iteration: 2785 lambda_k: 1 Loss: 0.0029288976976924734\n",
      "Iteration: 2786 lambda_k: 1 Loss: 0.0029289003885413743\n",
      "Iteration: 2787 lambda_k: 1 Loss: 0.0029289030651632204\n",
      "Iteration: 2788 lambda_k: 1 Loss: 0.002928905727633478\n",
      "Iteration: 2789 lambda_k: 1 Loss: 0.002928908376027204\n",
      "Iteration: 2790 lambda_k: 1 Loss: 0.002928911010419068\n",
      "Iteration: 2791 lambda_k: 1 Loss: 0.0029289136308833443\n",
      "Iteration: 2792 lambda_k: 1 Loss: 0.0029289162374938954\n",
      "Iteration: 2793 lambda_k: 1 Loss: 0.0029289188303241874\n",
      "Iteration: 2794 lambda_k: 1 Loss: 0.002928921409447321\n",
      "Iteration: 2795 lambda_k: 1 Loss: 0.0029289239749359926\n",
      "Iteration: 2796 lambda_k: 1 Loss: 0.0029289265268625194\n",
      "Iteration: 2797 lambda_k: 1 Loss: 0.002928929065298813\n",
      "Iteration: 2798 lambda_k: 1 Loss: 0.002928931590316433\n",
      "Iteration: 2799 lambda_k: 1 Loss: 0.002928934101986553\n",
      "Iteration: 2800 lambda_k: 1 Loss: 0.0029289366003799385\n",
      "Iteration: 2801 lambda_k: 1 Loss: 0.0029289390855670025\n",
      "Iteration: 2802 lambda_k: 1 Loss: 0.002928941557617773\n",
      "Iteration: 2803 lambda_k: 1 Loss: 0.002928944016601917\n",
      "Iteration: 2804 lambda_k: 1 Loss: 0.0029289464625887243\n",
      "Iteration: 2805 lambda_k: 1 Loss: 0.0029289488956471296\n",
      "Iteration: 2806 lambda_k: 1 Loss: 0.0029289513158456994\n",
      "Iteration: 2807 lambda_k: 1 Loss: 0.002928953723252588\n",
      "Iteration: 2808 lambda_k: 1 Loss: 0.0029289561179356464\n",
      "Iteration: 2809 lambda_k: 1 Loss: 0.0029289584999623356\n",
      "Iteration: 2810 lambda_k: 1 Loss: 0.0029289608693997765\n",
      "Iteration: 2811 lambda_k: 1 Loss: 0.002928963226314706\n",
      "Iteration: 2812 lambda_k: 1 Loss: 0.002928965570773538\n",
      "Iteration: 2813 lambda_k: 1 Loss: 0.0029289679028423163\n",
      "Iteration: 2814 lambda_k: 1 Loss: 0.002928970222586729\n",
      "Iteration: 2815 lambda_k: 1 Loss: 0.0029289725300721157\n",
      "Iteration: 2816 lambda_k: 1 Loss: 0.0029289748253634768\n",
      "Iteration: 2817 lambda_k: 1 Loss: 0.00292897710852545\n",
      "Iteration: 2818 lambda_k: 1 Loss: 0.0029289793796223773\n",
      "Iteration: 2819 lambda_k: 1 Loss: 0.002928981638718194\n",
      "Iteration: 2820 lambda_k: 1 Loss: 0.0029289838858765414\n",
      "Iteration: 2821 lambda_k: 1 Loss: 0.0029289861211606928\n",
      "Iteration: 2822 lambda_k: 1 Loss: 0.002928988344633604\n",
      "Iteration: 2823 lambda_k: 1 Loss: 0.0029289905563578858\n",
      "Iteration: 2824 lambda_k: 1 Loss: 0.0029289927563958154\n",
      "Iteration: 2825 lambda_k: 1 Loss: 0.002928994944809363\n",
      "Iteration: 2826 lambda_k: 1 Loss: 0.0029289971216601355\n",
      "Iteration: 2827 lambda_k: 1 Loss: 0.0029289992870094356\n",
      "Iteration: 2828 lambda_k: 1 Loss: 0.0029290014409182165\n",
      "Iteration: 2829 lambda_k: 1 Loss: 0.002929003583447134\n",
      "Iteration: 2830 lambda_k: 1 Loss: 0.002929005714656519\n",
      "Iteration: 2831 lambda_k: 1 Loss: 0.002929007834606366\n",
      "Iteration: 2832 lambda_k: 1 Loss: 0.00292900994335635\n",
      "Iteration: 2833 lambda_k: 1 Loss: 0.0029290120409658548\n",
      "Iteration: 2834 lambda_k: 1 Loss: 0.0029290141274938937\n",
      "Iteration: 2835 lambda_k: 1 Loss: 0.0029290162029992355\n",
      "Iteration: 2836 lambda_k: 1 Loss: 0.002929018267540278\n",
      "Iteration: 2837 lambda_k: 1 Loss: 0.002929020321175139\n",
      "Iteration: 2838 lambda_k: 1 Loss: 0.0029290223639616398\n",
      "Iteration: 2839 lambda_k: 1 Loss: 0.002929024395957269\n",
      "Iteration: 2840 lambda_k: 1 Loss: 0.002929026417219196\n",
      "Iteration: 2841 lambda_k: 1 Loss: 0.002929028427804319\n",
      "Iteration: 2842 lambda_k: 1 Loss: 0.002929030427769241\n",
      "Iteration: 2843 lambda_k: 1 Loss: 0.002929032417170213\n",
      "Iteration: 2844 lambda_k: 1 Loss: 0.0029290343960632318\n",
      "Iteration: 2845 lambda_k: 1 Loss: 0.002929036364503986\n",
      "Iteration: 2846 lambda_k: 1 Loss: 0.0029290383225478566\n",
      "Iteration: 2847 lambda_k: 1 Loss: 0.0029290402702499517\n",
      "Iteration: 2848 lambda_k: 1 Loss: 0.002929042207665077\n",
      "Iteration: 2849 lambda_k: 1 Loss: 0.0029290441348477324\n",
      "Iteration: 2850 lambda_k: 1 Loss: 0.0029290460518521504\n",
      "Iteration: 2851 lambda_k: 1 Loss: 0.002929047958732243\n",
      "Iteration: 2852 lambda_k: 1 Loss: 0.00292904985554167\n",
      "Iteration: 2853 lambda_k: 1 Loss: 0.0029290517423337954\n",
      "Iteration: 2854 lambda_k: 1 Loss: 0.0029290536191616946\n",
      "Iteration: 2855 lambda_k: 1 Loss: 0.002929055486078167\n",
      "Iteration: 2856 lambda_k: 1 Loss: 0.0029290573431357244\n",
      "Iteration: 2857 lambda_k: 1 Loss: 0.002929059190386609\n",
      "Iteration: 2858 lambda_k: 1 Loss: 0.002929061027882778\n",
      "Iteration: 2859 lambda_k: 1 Loss: 0.002929062855675919\n",
      "Iteration: 2860 lambda_k: 1 Loss: 0.002929064673817422\n",
      "Iteration: 2861 lambda_k: 1 Loss: 0.002929066482358415\n",
      "Iteration: 2862 lambda_k: 1 Loss: 0.0029290682813497802\n",
      "Iteration: 2863 lambda_k: 1 Loss: 0.0029290700708421023\n",
      "Iteration: 2864 lambda_k: 1 Loss: 0.0029290718508857105\n",
      "Iteration: 2865 lambda_k: 1 Loss: 0.0029290736215306476\n",
      "Iteration: 2866 lambda_k: 1 Loss: 0.002929075382826707\n",
      "Iteration: 2867 lambda_k: 1 Loss: 0.002929077134823432\n",
      "Iteration: 2868 lambda_k: 1 Loss: 0.0029290788775700643\n",
      "Iteration: 2869 lambda_k: 1 Loss: 0.0029290806111156144\n",
      "Iteration: 2870 lambda_k: 1 Loss: 0.00292908233550883\n",
      "Iteration: 2871 lambda_k: 1 Loss: 0.0029290840507981758\n",
      "Iteration: 2872 lambda_k: 1 Loss: 0.0029290857570318835\n",
      "Iteration: 2873 lambda_k: 1 Loss: 0.0029290874542579224\n",
      "Iteration: 2874 lambda_k: 1 Loss: 0.002929089142523993\n",
      "Iteration: 2875 lambda_k: 1 Loss: 0.002929090821877568\n",
      "Iteration: 2876 lambda_k: 1 Loss: 0.002929092492365838\n",
      "Iteration: 2877 lambda_k: 1 Loss: 0.002929094154035775\n",
      "Iteration: 2878 lambda_k: 1 Loss: 0.0029290958069340765\n",
      "Iteration: 2879 lambda_k: 1 Loss: 0.002929097451107195\n",
      "Iteration: 2880 lambda_k: 1 Loss: 0.0029290990866013386\n",
      "Iteration: 2881 lambda_k: 1 Loss: 0.0029291007134624875\n",
      "Iteration: 2882 lambda_k: 1 Loss: 0.0029291023317363583\n",
      "Iteration: 2883 lambda_k: 1 Loss: 0.002929103941468428\n",
      "Iteration: 2884 lambda_k: 1 Loss: 0.002929105542703926\n",
      "Iteration: 2885 lambda_k: 1 Loss: 0.00292910713548786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2886 lambda_k: 1 Loss: 0.0029291087198649824\n",
      "Iteration: 2887 lambda_k: 1 Loss: 0.0029291102958798076\n",
      "Iteration: 2888 lambda_k: 1 Loss: 0.0029291118635766285\n",
      "Iteration: 2889 lambda_k: 1 Loss: 0.0029291134229994676\n",
      "Iteration: 2890 lambda_k: 1 Loss: 0.002929114974192171\n",
      "Iteration: 2891 lambda_k: 1 Loss: 0.002929116517198309\n",
      "Iteration: 2892 lambda_k: 1 Loss: 0.002929118052061219\n",
      "Iteration: 2893 lambda_k: 1 Loss: 0.0029291195788240473\n",
      "Iteration: 2894 lambda_k: 1 Loss: 0.0029291210975296573\n",
      "Iteration: 2895 lambda_k: 1 Loss: 0.002929122608220718\n",
      "Iteration: 2896 lambda_k: 1 Loss: 0.0029291241109396745\n",
      "Iteration: 2897 lambda_k: 1 Loss: 0.002929125605728714\n",
      "Iteration: 2898 lambda_k: 1 Loss: 0.0029291270926298384\n",
      "Iteration: 2899 lambda_k: 1 Loss: 0.0029291285716848145\n",
      "Iteration: 2900 lambda_k: 1 Loss: 0.002929130042935177\n",
      "Iteration: 2901 lambda_k: 1 Loss: 0.0029291315064222567\n",
      "Iteration: 2902 lambda_k: 1 Loss: 0.00292913296218715\n",
      "Iteration: 2903 lambda_k: 1 Loss: 0.0029291344102707314\n",
      "Iteration: 2904 lambda_k: 1 Loss: 0.0029291358507136708\n",
      "Iteration: 2905 lambda_k: 1 Loss: 0.0029291372835564115\n",
      "Iteration: 2906 lambda_k: 1 Loss: 0.0029291387088391883\n",
      "Iteration: 2907 lambda_k: 1 Loss: 0.0029291401266020307\n",
      "Iteration: 2908 lambda_k: 1 Loss: 0.002929141536884759\n",
      "Iteration: 2909 lambda_k: 1 Loss: 0.0029291429397269335\n",
      "Iteration: 2910 lambda_k: 1 Loss: 0.00292914433516798\n",
      "Iteration: 2911 lambda_k: 1 Loss: 0.0029291457232470436\n",
      "Iteration: 2912 lambda_k: 1 Loss: 0.0029291471040031112\n",
      "Iteration: 2913 lambda_k: 1 Loss: 0.0029291484774749445\n",
      "Iteration: 2914 lambda_k: 1 Loss: 0.002929149843701104\n",
      "Iteration: 2915 lambda_k: 1 Loss: 0.0029291512027199273\n",
      "Iteration: 2916 lambda_k: 1 Loss: 0.0029291525545695725\n",
      "Iteration: 2917 lambda_k: 1 Loss: 0.0029291538992879665\n",
      "Iteration: 2918 lambda_k: 1 Loss: 0.0029291552369128744\n",
      "Iteration: 2919 lambda_k: 1 Loss: 0.002929156567481834\n",
      "Iteration: 2920 lambda_k: 1 Loss: 0.002929157891032203\n",
      "Iteration: 2921 lambda_k: 1 Loss: 0.0029291592076011066\n",
      "Iteration: 2922 lambda_k: 1 Loss: 0.0029291605172254895\n",
      "Iteration: 2923 lambda_k: 1 Loss: 0.0029291618199421216\n",
      "Iteration: 2924 lambda_k: 1 Loss: 0.0029291631157875382\n",
      "Iteration: 2925 lambda_k: 1 Loss: 0.0029291644047981236\n",
      "Iteration: 2926 lambda_k: 1 Loss: 0.002929165687010051\n",
      "Iteration: 2927 lambda_k: 1 Loss: 0.002929166962459288\n",
      "Iteration: 2928 lambda_k: 1 Loss: 0.0029291682311816183\n",
      "Iteration: 2929 lambda_k: 1 Loss: 0.0029291694932126557\n",
      "Iteration: 2930 lambda_k: 1 Loss: 0.002929170748587791\n",
      "Iteration: 2931 lambda_k: 1 Loss: 0.002929171997342249\n",
      "Iteration: 2932 lambda_k: 1 Loss: 0.0029291732395110683\n",
      "Iteration: 2933 lambda_k: 1 Loss: 0.002929174475129097\n",
      "Iteration: 2934 lambda_k: 1 Loss: 0.0029291757042309813\n",
      "Iteration: 2935 lambda_k: 1 Loss: 0.0029291769268511986\n",
      "Iteration: 2936 lambda_k: 1 Loss: 0.0029291781430240637\n",
      "Iteration: 2937 lambda_k: 1 Loss: 0.0029291793527836635\n",
      "Iteration: 2938 lambda_k: 1 Loss: 0.002929180556163949\n",
      "Iteration: 2939 lambda_k: 1 Loss: 0.002929181753198662\n",
      "Iteration: 2940 lambda_k: 1 Loss: 0.002929182943921398\n",
      "Iteration: 2941 lambda_k: 1 Loss: 0.002929184128365527\n",
      "Iteration: 2942 lambda_k: 1 Loss: 0.002929185306564294\n",
      "Iteration: 2943 lambda_k: 1 Loss: 0.002929186478550711\n",
      "Iteration: 2944 lambda_k: 1 Loss: 0.0029291876443576508\n",
      "Iteration: 2945 lambda_k: 1 Loss: 0.0029291888040178043\n",
      "Iteration: 2946 lambda_k: 1 Loss: 0.0029291899575636803\n",
      "Iteration: 2947 lambda_k: 1 Loss: 0.0029291911050276435\n",
      "Iteration: 2948 lambda_k: 1 Loss: 0.002929192246441862\n",
      "Iteration: 2949 lambda_k: 1 Loss: 0.0029291933818383197\n",
      "Iteration: 2950 lambda_k: 1 Loss: 0.002929194511248859\n",
      "Iteration: 2951 lambda_k: 1 Loss: 0.0029291956347051447\n",
      "Iteration: 2952 lambda_k: 1 Loss: 0.002929196752238678\n",
      "Iteration: 2953 lambda_k: 1 Loss: 0.0029291978638807665\n",
      "Iteration: 2954 lambda_k: 1 Loss: 0.0029291989696626037\n",
      "Iteration: 2955 lambda_k: 1 Loss: 0.002929200069615182\n",
      "Iteration: 2956 lambda_k: 1 Loss: 0.002929201163769311\n",
      "Iteration: 2957 lambda_k: 1 Loss: 0.0029292022521556628\n",
      "Iteration: 2958 lambda_k: 1 Loss: 0.002929203334804767\n",
      "Iteration: 2959 lambda_k: 1 Loss: 0.0029292044117469294\n",
      "Iteration: 2960 lambda_k: 1 Loss: 0.0029292054830123617\n",
      "Iteration: 2961 lambda_k: 1 Loss: 0.002929206548631079\n",
      "Iteration: 2962 lambda_k: 1 Loss: 0.0029292076086329497\n",
      "Iteration: 2963 lambda_k: 1 Loss: 0.0029292086630476593\n",
      "Iteration: 2964 lambda_k: 1 Loss: 0.0029292097119047854\n",
      "Iteration: 2965 lambda_k: 1 Loss: 0.002929210755233699\n",
      "Iteration: 2966 lambda_k: 1 Loss: 0.0029292117930636314\n",
      "Iteration: 2967 lambda_k: 1 Loss: 0.00292921282542367\n",
      "Iteration: 2968 lambda_k: 1 Loss: 0.0029292138523427315\n",
      "Iteration: 2969 lambda_k: 1 Loss: 0.002929214873849607\n",
      "Iteration: 2970 lambda_k: 1 Loss: 0.002929215889972899\n",
      "Iteration: 2971 lambda_k: 1 Loss: 0.0029292169007410717\n",
      "Iteration: 2972 lambda_k: 1 Loss: 0.0029292179061824524\n",
      "Iteration: 2973 lambda_k: 1 Loss: 0.002929218906325202\n",
      "Iteration: 2974 lambda_k: 1 Loss: 0.0029292199011973427\n",
      "Iteration: 2975 lambda_k: 1 Loss: 0.0029292208908267164\n",
      "Iteration: 2976 lambda_k: 1 Loss: 0.0029292218752410785\n",
      "Iteration: 2977 lambda_k: 1 Loss: 0.00292922285446799\n",
      "Iteration: 2978 lambda_k: 1 Loss: 0.002929223828534861\n",
      "Iteration: 2979 lambda_k: 1 Loss: 0.002929224797468988\n",
      "Iteration: 2980 lambda_k: 1 Loss: 0.002929225761297521\n",
      "Iteration: 2981 lambda_k: 1 Loss: 0.0029292267200474098\n",
      "Iteration: 2982 lambda_k: 1 Loss: 0.0029292276737455354\n",
      "Iteration: 2983 lambda_k: 1 Loss: 0.002929228622418598\n",
      "Iteration: 2984 lambda_k: 1 Loss: 0.002929229566093166\n",
      "Iteration: 2985 lambda_k: 1 Loss: 0.002929230504795653\n",
      "Iteration: 2986 lambda_k: 1 Loss: 0.0029292314385523524\n",
      "Iteration: 2987 lambda_k: 1 Loss: 0.0029292323673894196\n",
      "Iteration: 2988 lambda_k: 1 Loss: 0.0029292332913328435\n",
      "Iteration: 2989 lambda_k: 1 Loss: 0.0029292342104084985\n",
      "Iteration: 2990 lambda_k: 1 Loss: 0.0029292351246421144\n",
      "Iteration: 2991 lambda_k: 1 Loss: 0.0029292360340592793\n",
      "Iteration: 2992 lambda_k: 1 Loss: 0.0029292369386854523\n",
      "Iteration: 2993 lambda_k: 1 Loss: 0.0029292378385459565\n",
      "Iteration: 2994 lambda_k: 1 Loss: 0.002929238733665988\n",
      "Iteration: 2995 lambda_k: 1 Loss: 0.0029292396240706026\n",
      "Iteration: 2996 lambda_k: 1 Loss: 0.002929240509784718\n",
      "Iteration: 2997 lambda_k: 1 Loss: 0.0029292413908331358\n",
      "Iteration: 2998 lambda_k: 1 Loss: 0.002929242267240492\n",
      "Iteration: 2999 lambda_k: 1 Loss: 0.0029292431390313353\n",
      "Iteration: 3000 lambda_k: 1 Loss: 0.002929244006230048\n",
      "Iteration: 3001 lambda_k: 1 Loss: 0.0029292448688609174\n",
      "Iteration: 3002 lambda_k: 1 Loss: 0.002929245726948068\n",
      "Iteration: 3003 lambda_k: 1 Loss: 0.002929246580515532\n",
      "Iteration: 3004 lambda_k: 1 Loss: 0.002929247429587166\n",
      "Iteration: 3005 lambda_k: 1 Loss: 0.002929248274186743\n",
      "Iteration: 3006 lambda_k: 1 Loss: 0.002929249114337894\n",
      "Iteration: 3007 lambda_k: 1 Loss: 0.0029292499500641225\n",
      "Iteration: 3008 lambda_k: 1 Loss: 0.00292925078138883\n",
      "Iteration: 3009 lambda_k: 1 Loss: 0.0029292516083352545\n",
      "Iteration: 3010 lambda_k: 1 Loss: 0.0029292524309265378\n",
      "Iteration: 3011 lambda_k: 1 Loss: 0.0029292532491856827\n",
      "Iteration: 3012 lambda_k: 1 Loss: 0.0029292540631355945\n",
      "Iteration: 3013 lambda_k: 1 Loss: 0.0029292548727990414\n",
      "Iteration: 3014 lambda_k: 1 Loss: 0.0029292556781986647\n",
      "Iteration: 3015 lambda_k: 1 Loss: 0.0029292564793569793\n",
      "Iteration: 3016 lambda_k: 1 Loss: 0.0029292572762964204\n",
      "Iteration: 3017 lambda_k: 1 Loss: 0.0029292580690392536\n",
      "Iteration: 3018 lambda_k: 1 Loss: 0.0029292588576076742\n",
      "Iteration: 3019 lambda_k: 1 Loss: 0.002929259642023723\n",
      "Iteration: 3020 lambda_k: 1 Loss: 0.0029292604223093354\n",
      "Iteration: 3021 lambda_k: 1 Loss: 0.0029292611984863353\n",
      "Iteration: 3022 lambda_k: 1 Loss: 0.0029292619705764255\n",
      "Iteration: 3023 lambda_k: 1 Loss: 0.002929262738601196\n",
      "Iteration: 3024 lambda_k: 1 Loss: 0.0029292635025821257\n",
      "Iteration: 3025 lambda_k: 1 Loss: 0.002929264262540566\n",
      "Iteration: 3026 lambda_k: 1 Loss: 0.0029292650184977813\n",
      "Iteration: 3027 lambda_k: 1 Loss: 0.002929265770474896\n",
      "Iteration: 3028 lambda_k: 1 Loss: 0.002929266518492951\n",
      "Iteration: 3029 lambda_k: 1 Loss: 0.002929267262572846\n",
      "Iteration: 3030 lambda_k: 1 Loss: 0.002929268002735372\n",
      "Iteration: 3031 lambda_k: 1 Loss: 0.0029292687390012344\n",
      "Iteration: 3032 lambda_k: 1 Loss: 0.0029292694713910095\n",
      "Iteration: 3033 lambda_k: 1 Loss: 0.002929270199925183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3034 lambda_k: 1 Loss: 0.002929270924624093\n",
      "Iteration: 3035 lambda_k: 1 Loss: 0.0029292716455080366\n",
      "Iteration: 3036 lambda_k: 1 Loss: 0.002929272362597131\n",
      "Iteration: 3037 lambda_k: 1 Loss: 0.002929273075911435\n",
      "Iteration: 3038 lambda_k: 1 Loss: 0.002929273785470874\n",
      "Iteration: 3039 lambda_k: 1 Loss: 0.0029292744912952765\n",
      "Iteration: 3040 lambda_k: 1 Loss: 0.002929275193404377\n",
      "Iteration: 3041 lambda_k: 1 Loss: 0.002929275891817791\n",
      "Iteration: 3042 lambda_k: 1 Loss: 0.0029292765865550187\n",
      "Iteration: 3043 lambda_k: 1 Loss: 0.0029292772776354915\n",
      "Iteration: 3044 lambda_k: 1 Loss: 0.0029292779650785194\n",
      "Iteration: 3045 lambda_k: 1 Loss: 0.002929278648903314\n",
      "Iteration: 3046 lambda_k: 1 Loss: 0.0029292793291289827\n",
      "Iteration: 3047 lambda_k: 1 Loss: 0.0029292800057745174\n",
      "Iteration: 3048 lambda_k: 1 Loss: 0.0029292806788588288\n",
      "Iteration: 3049 lambda_k: 1 Loss: 0.002929281348400713\n",
      "Iteration: 3050 lambda_k: 1 Loss: 0.0029292820144188815\n",
      "Iteration: 3051 lambda_k: 1 Loss: 0.002929282676931941\n",
      "Iteration: 3052 lambda_k: 1 Loss: 0.0029292833359584078\n",
      "Iteration: 3053 lambda_k: 1 Loss: 0.002929283991516691\n",
      "Iteration: 3054 lambda_k: 1 Loss: 0.0029292846436250867\n",
      "Iteration: 3055 lambda_k: 1 Loss: 0.0029292852923018175\n",
      "Iteration: 3056 lambda_k: 1 Loss: 0.002929285937564992\n",
      "Iteration: 3057 lambda_k: 1 Loss: 0.0029292865794326396\n",
      "Iteration: 3058 lambda_k: 1 Loss: 0.002929287217922687\n",
      "Iteration: 3059 lambda_k: 1 Loss: 0.0029292878530529577\n",
      "Iteration: 3060 lambda_k: 1 Loss: 0.002929288484841206\n",
      "Iteration: 3061 lambda_k: 1 Loss: 0.0029292891133050575\n",
      "Iteration: 3062 lambda_k: 1 Loss: 0.002929289738462068\n",
      "Iteration: 3063 lambda_k: 1 Loss: 0.002929290360329704\n",
      "Iteration: 3064 lambda_k: 1 Loss: 0.0029292909789253143\n",
      "Iteration: 3065 lambda_k: 1 Loss: 0.002929291594266182\n",
      "Iteration: 3066 lambda_k: 1 Loss: 0.0029292922063694775\n",
      "Iteration: 3067 lambda_k: 1 Loss: 0.0029292928152522962\n",
      "Iteration: 3068 lambda_k: 1 Loss: 0.0029292934209316215\n",
      "Iteration: 3069 lambda_k: 1 Loss: 0.0029292940234244046\n",
      "Iteration: 3070 lambda_k: 1 Loss: 0.002929294622747429\n",
      "Iteration: 3071 lambda_k: 1 Loss: 0.0029292952189174395\n",
      "Iteration: 3072 lambda_k: 1 Loss: 0.0029292958119510695\n",
      "Iteration: 3073 lambda_k: 1 Loss: 0.002929296401864867\n",
      "Iteration: 3074 lambda_k: 1 Loss: 0.002929296988675299\n",
      "Iteration: 3075 lambda_k: 1 Loss: 0.0029292975723987594\n",
      "Iteration: 3076 lambda_k: 1 Loss: 0.0029292981530515327\n",
      "Iteration: 3077 lambda_k: 1 Loss: 0.002929298730649838\n",
      "Iteration: 3078 lambda_k: 1 Loss: 0.002929299305209769\n",
      "Iteration: 3079 lambda_k: 1 Loss: 0.002929299876747381\n",
      "Iteration: 3080 lambda_k: 1 Loss: 0.002929300445278621\n",
      "Iteration: 3081 lambda_k: 1 Loss: 0.0029293010108193338\n",
      "Iteration: 3082 lambda_k: 1 Loss: 0.0029293015733853163\n",
      "Iteration: 3083 lambda_k: 1 Loss: 0.002929302132992261\n",
      "Iteration: 3084 lambda_k: 1 Loss: 0.002929302689655803\n",
      "Iteration: 3085 lambda_k: 1 Loss: 0.0029293032433914527\n",
      "Iteration: 3086 lambda_k: 1 Loss: 0.0029293037942146813\n",
      "Iteration: 3087 lambda_k: 1 Loss: 0.0029293043421408237\n",
      "Iteration: 3088 lambda_k: 1 Loss: 0.0029293048871851836\n",
      "Iteration: 3089 lambda_k: 1 Loss: 0.0029293054293629698\n",
      "Iteration: 3090 lambda_k: 1 Loss: 0.002929305968689307\n",
      "Iteration: 3091 lambda_k: 1 Loss: 0.0029293065051792396\n",
      "Iteration: 3092 lambda_k: 1 Loss: 0.0029293070388477275\n",
      "Iteration: 3093 lambda_k: 1 Loss: 0.002929307569709678\n",
      "Iteration: 3094 lambda_k: 1 Loss: 0.0029293080977798496\n",
      "Iteration: 3095 lambda_k: 1 Loss: 0.0029293086230730075\n",
      "Iteration: 3096 lambda_k: 1 Loss: 0.0029293091456037965\n",
      "Iteration: 3097 lambda_k: 1 Loss: 0.002929309665386796\n",
      "Iteration: 3098 lambda_k: 1 Loss: 0.0029293101824365017\n",
      "Iteration: 3099 lambda_k: 1 Loss: 0.0029293106967673297\n",
      "Iteration: 3100 lambda_k: 1 Loss: 0.002929311208393599\n",
      "Iteration: 3101 lambda_k: 1 Loss: 0.002929311717329625\n",
      "Iteration: 3102 lambda_k: 1 Loss: 0.0029293122235895604\n",
      "Iteration: 3103 lambda_k: 1 Loss: 0.0029293127271875375\n",
      "Iteration: 3104 lambda_k: 1 Loss: 0.0029293132281375945\n",
      "Iteration: 3105 lambda_k: 1 Loss: 0.0029293137264536978\n",
      "Iteration: 3106 lambda_k: 1 Loss: 0.0029293142221497367\n",
      "Iteration: 3107 lambda_k: 1 Loss: 0.0029293147152395483\n",
      "Iteration: 3108 lambda_k: 1 Loss: 0.0029293152057368664\n",
      "Iteration: 3109 lambda_k: 1 Loss: 0.002929315693655354\n",
      "Iteration: 3110 lambda_k: 1 Loss: 0.002929316179008648\n",
      "Iteration: 3111 lambda_k: 1 Loss: 0.0029293166618102446\n",
      "Iteration: 3112 lambda_k: 1 Loss: 0.002929317142073613\n",
      "Iteration: 3113 lambda_k: 1 Loss: 0.002929317619812133\n",
      "Iteration: 3114 lambda_k: 1 Loss: 0.00292931809503914\n",
      "Iteration: 3115 lambda_k: 1 Loss: 0.0029293185677678633\n",
      "Iteration: 3116 lambda_k: 1 Loss: 0.002929319038011493\n",
      "Iteration: 3117 lambda_k: 1 Loss: 0.0029293195057831147\n",
      "Iteration: 3118 lambda_k: 1 Loss: 0.002929319971095787\n",
      "Iteration: 3119 lambda_k: 1 Loss: 0.00292932043396245\n",
      "Iteration: 3120 lambda_k: 1 Loss: 0.002929320894396021\n",
      "Iteration: 3121 lambda_k: 1 Loss: 0.00292932135240934\n",
      "Iteration: 3122 lambda_k: 1 Loss: 0.0029293218080151456\n",
      "Iteration: 3123 lambda_k: 1 Loss: 0.002929322261226148\n",
      "Iteration: 3124 lambda_k: 1 Loss: 0.0029293227120549816\n",
      "Iteration: 3125 lambda_k: 1 Loss: 0.002929323160514187\n",
      "Iteration: 3126 lambda_k: 1 Loss: 0.00292932360661627\n",
      "Iteration: 3127 lambda_k: 1 Loss: 0.0029293240503736593\n",
      "Iteration: 3128 lambda_k: 1 Loss: 0.002929324491798712\n",
      "Iteration: 3129 lambda_k: 1 Loss: 0.0029293249309037313\n",
      "Iteration: 3130 lambda_k: 1 Loss: 0.002929325367700952\n",
      "Iteration: 3131 lambda_k: 1 Loss: 0.0029293258022025518\n",
      "Iteration: 3132 lambda_k: 1 Loss: 0.002929326234420618\n",
      "Iteration: 3133 lambda_k: 1 Loss: 0.0029293266643671853\n",
      "Iteration: 3134 lambda_k: 1 Loss: 0.002929327092054251\n",
      "Iteration: 3135 lambda_k: 1 Loss: 0.0029293275174936996\n",
      "Iteration: 3136 lambda_k: 1 Loss: 0.002929327940697412\n",
      "Iteration: 3137 lambda_k: 1 Loss: 0.0029293283616771424\n",
      "Iteration: 3138 lambda_k: 1 Loss: 0.0029293287804446413\n",
      "Iteration: 3139 lambda_k: 1 Loss: 0.0029293291970115564\n",
      "Iteration: 3140 lambda_k: 1 Loss: 0.0029293296113894834\n",
      "Iteration: 3141 lambda_k: 1 Loss: 0.0029293300235899676\n",
      "Iteration: 3142 lambda_k: 1 Loss: 0.0029293304336244913\n",
      "Iteration: 3143 lambda_k: 1 Loss: 0.0029293308415044854\n",
      "Iteration: 3144 lambda_k: 1 Loss: 0.0029293312472412803\n",
      "Iteration: 3145 lambda_k: 1 Loss: 0.0029293316508461923\n",
      "Iteration: 3146 lambda_k: 1 Loss: 0.0029293320523304358\n",
      "Iteration: 3147 lambda_k: 1 Loss: 0.0029293324517052147\n",
      "Iteration: 3148 lambda_k: 1 Loss: 0.0029293328489816296\n",
      "Iteration: 3149 lambda_k: 1 Loss: 0.002929333244170762\n",
      "Iteration: 3150 lambda_k: 1 Loss: 0.002929333637283591\n",
      "Iteration: 3151 lambda_k: 1 Loss: 0.0029293340283310662\n",
      "Iteration: 3152 lambda_k: 1 Loss: 0.002929334417324073\n",
      "Iteration: 3153 lambda_k: 1 Loss: 0.0029293348042734317\n",
      "Iteration: 3154 lambda_k: 1 Loss: 0.0029293351891899184\n",
      "Iteration: 3155 lambda_k: 1 Loss: 0.0029293355720842653\n",
      "Iteration: 3156 lambda_k: 1 Loss: 0.002929335952967102\n",
      "Iteration: 3157 lambda_k: 1 Loss: 0.002929336331849039\n",
      "Iteration: 3158 lambda_k: 1 Loss: 0.002929336708740625\n",
      "Iteration: 3159 lambda_k: 1 Loss: 0.002929337083652331\n",
      "Iteration: 3160 lambda_k: 1 Loss: 0.0029293374565946078\n",
      "Iteration: 3161 lambda_k: 1 Loss: 0.0029293378275778363\n",
      "Iteration: 3162 lambda_k: 1 Loss: 0.002929338196612333\n",
      "Iteration: 3163 lambda_k: 1 Loss: 0.0029293385637083704\n",
      "Iteration: 3164 lambda_k: 1 Loss: 0.0029293389288761592\n",
      "Iteration: 3165 lambda_k: 1 Loss: 0.0029293392921258487\n",
      "Iteration: 3166 lambda_k: 1 Loss: 0.0029293396534675726\n",
      "Iteration: 3167 lambda_k: 1 Loss: 0.0029293400129113576\n",
      "Iteration: 3168 lambda_k: 1 Loss: 0.0029293403704672176\n",
      "Iteration: 3169 lambda_k: 1 Loss: 0.002929340726145102\n",
      "Iteration: 3170 lambda_k: 1 Loss: 0.0029293410799549006\n",
      "Iteration: 3171 lambda_k: 1 Loss: 0.0029293414319064535\n",
      "Iteration: 3172 lambda_k: 1 Loss: 0.0029293417820095463\n",
      "Iteration: 3173 lambda_k: 1 Loss: 0.0029293421302739367\n",
      "Iteration: 3174 lambda_k: 1 Loss: 0.0029293424767092924\n",
      "Iteration: 3175 lambda_k: 1 Loss: 0.0029293428213252592\n",
      "Iteration: 3176 lambda_k: 1 Loss: 0.002929343164131411\n",
      "Iteration: 3177 lambda_k: 1 Loss: 0.0029293435051372806\n",
      "Iteration: 3178 lambda_k: 1 Loss: 0.0029293438443523737\n",
      "Iteration: 3179 lambda_k: 1 Loss: 0.002929344181786112\n",
      "Iteration: 3180 lambda_k: 1 Loss: 0.0029293445174478723\n",
      "Iteration: 3181 lambda_k: 1 Loss: 0.0029293448513469994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3182 lambda_k: 1 Loss: 0.0029293451834927794\n",
      "Iteration: 3183 lambda_k: 1 Loss: 0.0029293455138944316\n",
      "Iteration: 3184 lambda_k: 1 Loss: 0.002929345842561155\n",
      "Iteration: 3185 lambda_k: 1 Loss: 0.002929346169502085\n",
      "Iteration: 3186 lambda_k: 1 Loss: 0.002929346494726307\n",
      "Iteration: 3187 lambda_k: 1 Loss: 0.0029293468182428803\n",
      "Iteration: 3188 lambda_k: 1 Loss: 0.002929347140060777\n",
      "Iteration: 3189 lambda_k: 1 Loss: 0.002929347460188943\n",
      "Iteration: 3190 lambda_k: 1 Loss: 0.002929347778636289\n",
      "Iteration: 3191 lambda_k: 1 Loss: 0.002929348095411697\n",
      "Iteration: 3192 lambda_k: 1 Loss: 0.0029293484105239356\n",
      "Iteration: 3193 lambda_k: 1 Loss: 0.0029293487239817656\n",
      "Iteration: 3194 lambda_k: 1 Loss: 0.0029293490357939008\n",
      "Iteration: 3195 lambda_k: 1 Loss: 0.00292934934596902\n",
      "Iteration: 3196 lambda_k: 1 Loss: 0.0029293496545157362\n",
      "Iteration: 3197 lambda_k: 1 Loss: 0.0029293499614426333\n",
      "Iteration: 3198 lambda_k: 1 Loss: 0.0029293502667582455\n",
      "Iteration: 3199 lambda_k: 1 Loss: 0.002929350570471021\n",
      "Iteration: 3200 lambda_k: 1 Loss: 0.00292935087258944\n",
      "Iteration: 3201 lambda_k: 1 Loss: 0.002929351173121882\n",
      "Iteration: 3202 lambda_k: 1 Loss: 0.0029293514720766956\n",
      "Iteration: 3203 lambda_k: 1 Loss: 0.0029293517694622007\n",
      "Iteration: 3204 lambda_k: 1 Loss: 0.002929352065286648\n",
      "Iteration: 3205 lambda_k: 1 Loss: 0.0029293523595582643\n",
      "Iteration: 3206 lambda_k: 1 Loss: 0.002929352652285219\n",
      "Iteration: 3207 lambda_k: 1 Loss: 0.002929352943475661\n",
      "Iteration: 3208 lambda_k: 1 Loss: 0.002929353233137671\n",
      "Iteration: 3209 lambda_k: 1 Loss: 0.0029293535212792745\n",
      "Iteration: 3210 lambda_k: 1 Loss: 0.0029293538079085035\n",
      "Iteration: 3211 lambda_k: 1 Loss: 0.0029293540930333236\n",
      "Iteration: 3212 lambda_k: 1 Loss: 0.0029293543766616367\n",
      "Iteration: 3213 lambda_k: 1 Loss: 0.002929354658801331\n",
      "Iteration: 3214 lambda_k: 1 Loss: 0.0029293549394602547\n",
      "Iteration: 3215 lambda_k: 1 Loss: 0.002929355218646181\n",
      "Iteration: 3216 lambda_k: 1 Loss: 0.0029293554963668767\n",
      "Iteration: 3217 lambda_k: 1 Loss: 0.0029293557726300544\n",
      "Iteration: 3218 lambda_k: 1 Loss: 0.0029293560474433662\n",
      "Iteration: 3219 lambda_k: 1 Loss: 0.0029293563208144737\n",
      "Iteration: 3220 lambda_k: 1 Loss: 0.0029293565927509513\n",
      "Iteration: 3221 lambda_k: 1 Loss: 0.0029293568632603657\n",
      "Iteration: 3222 lambda_k: 1 Loss: 0.0029293571323502255\n",
      "Iteration: 3223 lambda_k: 1 Loss: 0.0029293574000280043\n",
      "Iteration: 3224 lambda_k: 1 Loss: 0.002929357666301131\n",
      "Iteration: 3225 lambda_k: 1 Loss: 0.002929357931176995\n",
      "Iteration: 3226 lambda_k: 1 Loss: 0.0029293581946629485\n",
      "Iteration: 3227 lambda_k: 1 Loss: 0.00292935845676631\n",
      "Iteration: 3228 lambda_k: 1 Loss: 0.0029293587174943642\n",
      "Iteration: 3229 lambda_k: 1 Loss: 0.0029293589768543443\n",
      "Iteration: 3230 lambda_k: 1 Loss: 0.002929359234853435\n",
      "Iteration: 3231 lambda_k: 1 Loss: 0.00292935949149883\n",
      "Iteration: 3232 lambda_k: 1 Loss: 0.0029293597467976327\n",
      "Iteration: 3233 lambda_k: 1 Loss: 0.002929360000756934\n",
      "Iteration: 3234 lambda_k: 1 Loss: 0.0029293602533837855\n",
      "Iteration: 3235 lambda_k: 1 Loss: 0.0029293605046851827\n",
      "Iteration: 3236 lambda_k: 1 Loss: 0.002929360754668142\n",
      "Iteration: 3237 lambda_k: 1 Loss: 0.0029293610033395683\n",
      "Iteration: 3238 lambda_k: 1 Loss: 0.0029293612507063685\n",
      "Iteration: 3239 lambda_k: 1 Loss: 0.0029293614967754083\n",
      "Iteration: 3240 lambda_k: 1 Loss: 0.00292936174155352\n",
      "Iteration: 3241 lambda_k: 1 Loss: 0.002929361985047493\n",
      "Iteration: 3242 lambda_k: 1 Loss: 0.0029293622272641086\n",
      "Iteration: 3243 lambda_k: 1 Loss: 0.0029293624682100493\n",
      "Iteration: 3244 lambda_k: 1 Loss: 0.002929362707892012\n",
      "Iteration: 3245 lambda_k: 1 Loss: 0.0029293629463166618\n",
      "Iteration: 3246 lambda_k: 1 Loss: 0.0029293631834906167\n",
      "Iteration: 3247 lambda_k: 1 Loss: 0.0029293634194204462\n",
      "Iteration: 3248 lambda_k: 1 Loss: 0.002929363654112681\n",
      "Iteration: 3249 lambda_k: 1 Loss: 0.0029293638875738433\n",
      "Iteration: 3250 lambda_k: 1 Loss: 0.0029293641198104127\n",
      "Iteration: 3251 lambda_k: 1 Loss: 0.0029293643508288515\n",
      "Iteration: 3252 lambda_k: 1 Loss: 0.0029293645806355594\n",
      "Iteration: 3253 lambda_k: 1 Loss: 0.002929364809236895\n",
      "Iteration: 3254 lambda_k: 1 Loss: 0.0029293650366392027\n",
      "Iteration: 3255 lambda_k: 1 Loss: 0.002929365262848806\n",
      "Iteration: 3256 lambda_k: 1 Loss: 0.0029293654878719657\n",
      "Iteration: 3257 lambda_k: 1 Loss: 0.0029293657117149263\n",
      "Iteration: 3258 lambda_k: 1 Loss: 0.0029293659343839007\n",
      "Iteration: 3259 lambda_k: 1 Loss: 0.002929366155885056\n",
      "Iteration: 3260 lambda_k: 1 Loss: 0.0029293663762245347\n",
      "Iteration: 3261 lambda_k: 1 Loss: 0.002929366595408461\n",
      "Iteration: 3262 lambda_k: 1 Loss: 0.002929366813442909\n",
      "Iteration: 3263 lambda_k: 1 Loss: 0.002929367030333923\n",
      "Iteration: 3264 lambda_k: 1 Loss: 0.0029293672460875145\n",
      "Iteration: 3265 lambda_k: 1 Loss: 0.0029293674607096727\n",
      "Iteration: 3266 lambda_k: 1 Loss: 0.0029293676742063405\n",
      "Iteration: 3267 lambda_k: 1 Loss: 0.0029293678865834364\n",
      "Iteration: 3268 lambda_k: 1 Loss: 0.002929368097846854\n",
      "Iteration: 3269 lambda_k: 1 Loss: 0.0029293683080024546\n",
      "Iteration: 3270 lambda_k: 1 Loss: 0.002929368517056058\n",
      "Iteration: 3271 lambda_k: 1 Loss: 0.002929368725013463\n",
      "Iteration: 3272 lambda_k: 1 Loss: 0.0029293689318804347\n",
      "Iteration: 3273 lambda_k: 1 Loss: 0.0029293691376627187\n",
      "Iteration: 3274 lambda_k: 1 Loss: 0.0029293693423659917\n",
      "Iteration: 3275 lambda_k: 1 Loss: 0.002929369545995966\n",
      "Iteration: 3276 lambda_k: 1 Loss: 0.002929369748558257\n",
      "Iteration: 3277 lambda_k: 1 Loss: 0.002929369950058488\n",
      "Iteration: 3278 lambda_k: 1 Loss: 0.002929370150502244\n",
      "Iteration: 3279 lambda_k: 1 Loss: 0.0029293703498950822\n",
      "Iteration: 3280 lambda_k: 1 Loss: 0.0029293705482425305\n",
      "Iteration: 3281 lambda_k: 1 Loss: 0.002929370745550067\n",
      "Iteration: 3282 lambda_k: 1 Loss: 0.0029293709418231925\n",
      "Iteration: 3283 lambda_k: 1 Loss: 0.0029293711370673224\n",
      "Iteration: 3284 lambda_k: 1 Loss: 0.002929371331287868\n",
      "Iteration: 3285 lambda_k: 1 Loss: 0.002929371524490233\n",
      "Iteration: 3286 lambda_k: 1 Loss: 0.0029293717166797573\n",
      "Iteration: 3287 lambda_k: 1 Loss: 0.0029293719078617585\n",
      "Iteration: 3288 lambda_k: 1 Loss: 0.0029293720980415367\n",
      "Iteration: 3289 lambda_k: 1 Loss: 0.002929372287224367\n",
      "Iteration: 3290 lambda_k: 1 Loss: 0.002929372475415487\n",
      "Iteration: 3291 lambda_k: 1 Loss: 0.002929372662620115\n",
      "Iteration: 3292 lambda_k: 1 Loss: 0.002929372848843448\n",
      "Iteration: 3293 lambda_k: 1 Loss: 0.002929373034090618\n",
      "Iteration: 3294 lambda_k: 1 Loss: 0.0029293732183667677\n",
      "Iteration: 3295 lambda_k: 1 Loss: 0.002929373401677025\n",
      "Iteration: 3296 lambda_k: 1 Loss: 0.0029293735840264343\n",
      "Iteration: 3297 lambda_k: 1 Loss: 0.0029293737654200657\n",
      "Iteration: 3298 lambda_k: 1 Loss: 0.0029293739458629326\n",
      "Iteration: 3299 lambda_k: 1 Loss: 0.00292937412536005\n",
      "Iteration: 3300 lambda_k: 1 Loss: 0.0029293743039163693\n",
      "Iteration: 3301 lambda_k: 1 Loss: 0.0029293744815368497\n",
      "Iteration: 3302 lambda_k: 1 Loss: 0.0029293746582264063\n",
      "Iteration: 3303 lambda_k: 1 Loss: 0.0029293748339899394\n",
      "Iteration: 3304 lambda_k: 1 Loss: 0.0029293750088323013\n",
      "Iteration: 3305 lambda_k: 1 Loss: 0.002929375182758352\n",
      "Iteration: 3306 lambda_k: 1 Loss: 0.0029293753557728874\n",
      "Iteration: 3307 lambda_k: 1 Loss: 0.0029293755278807132\n",
      "Iteration: 3308 lambda_k: 1 Loss: 0.002929375699086593\n",
      "Iteration: 3309 lambda_k: 1 Loss: 0.0029293758693952608\n",
      "Iteration: 3310 lambda_k: 1 Loss: 0.002929376038811441\n",
      "Iteration: 3311 lambda_k: 1 Loss: 0.002929376207339807\n",
      "Iteration: 3312 lambda_k: 1 Loss: 0.002929376374985047\n",
      "Iteration: 3313 lambda_k: 1 Loss: 0.0029293765417517923\n",
      "Iteration: 3314 lambda_k: 1 Loss: 0.0029293767076446573\n",
      "Iteration: 3315 lambda_k: 1 Loss: 0.002929376872668245\n",
      "Iteration: 3316 lambda_k: 1 Loss: 0.0029293770368271177\n",
      "Iteration: 3317 lambda_k: 1 Loss: 0.002929377200125804\n",
      "Iteration: 3318 lambda_k: 1 Loss: 0.0029293773625688516\n",
      "Iteration: 3319 lambda_k: 1 Loss: 0.0029293775241607573\n",
      "Iteration: 3320 lambda_k: 1 Loss: 0.0029293776849059643\n",
      "Iteration: 3321 lambda_k: 1 Loss: 0.002929377844808944\n",
      "Iteration: 3322 lambda_k: 1 Loss: 0.002929378003874112\n",
      "Iteration: 3323 lambda_k: 1 Loss: 0.0029293781621058924\n",
      "Iteration: 3324 lambda_k: 1 Loss: 0.002929378319508644\n",
      "Iteration: 3325 lambda_k: 1 Loss: 0.0029293784760867094\n",
      "Iteration: 3326 lambda_k: 1 Loss: 0.002929378631844442\n",
      "Iteration: 3327 lambda_k: 1 Loss: 0.0029293787867861487\n",
      "Iteration: 3328 lambda_k: 1 Loss: 0.0029293789409161304\n",
      "Iteration: 3329 lambda_k: 1 Loss: 0.0029293790942386292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3330 lambda_k: 1 Loss: 0.0029293792467579055\n",
      "Iteration: 3331 lambda_k: 1 Loss: 0.002929379398478179\n",
      "Iteration: 3332 lambda_k: 1 Loss: 0.002929379549403635\n",
      "Iteration: 3333 lambda_k: 1 Loss: 0.0029293796995384694\n",
      "Iteration: 3334 lambda_k: 1 Loss: 0.002929379848886811\n",
      "Iteration: 3335 lambda_k: 1 Loss: 0.0029293799974528002\n",
      "Iteration: 3336 lambda_k: 1 Loss: 0.0029293801452405363\n",
      "Iteration: 3337 lambda_k: 1 Loss: 0.0029293802922541463\n",
      "Iteration: 3338 lambda_k: 1 Loss: 0.002929380438497656\n",
      "Iteration: 3339 lambda_k: 1 Loss: 0.002929380583975123\n",
      "Iteration: 3340 lambda_k: 1 Loss: 0.002929380728690587\n",
      "Iteration: 3341 lambda_k: 1 Loss: 0.0029293808726480255\n",
      "Iteration: 3342 lambda_k: 1 Loss: 0.0029293810158514478\n",
      "Iteration: 3343 lambda_k: 1 Loss: 0.0029293811583047885\n",
      "Iteration: 3344 lambda_k: 1 Loss: 0.002929381300011999\n",
      "Iteration: 3345 lambda_k: 1 Loss: 0.0029293814409770075\n",
      "Iteration: 3346 lambda_k: 1 Loss: 0.0029293815812036963\n",
      "Iteration: 3347 lambda_k: 1 Loss: 0.002929381720695954\n",
      "Iteration: 3348 lambda_k: 1 Loss: 0.0029293818594576465\n",
      "Iteration: 3349 lambda_k: 1 Loss: 0.0029293819974925877\n",
      "Iteration: 3350 lambda_k: 1 Loss: 0.002929382134804596\n",
      "Iteration: 3351 lambda_k: 1 Loss: 0.0029293822713974964\n",
      "Iteration: 3352 lambda_k: 1 Loss: 0.002929382407275047\n",
      "Iteration: 3353 lambda_k: 1 Loss: 0.0029293825424410004\n",
      "Iteration: 3354 lambda_k: 1 Loss: 0.0029293826768991005\n",
      "Iteration: 3355 lambda_k: 1 Loss: 0.0029293828106530637\n",
      "Iteration: 3356 lambda_k: 1 Loss: 0.0029293829437065993\n",
      "Iteration: 3357 lambda_k: 1 Loss: 0.0029293830760633785\n",
      "Iteration: 3358 lambda_k: 1 Loss: 0.0029293832077270632\n",
      "Iteration: 3359 lambda_k: 1 Loss: 0.0029293833387012847\n",
      "Iteration: 3360 lambda_k: 1 Loss: 0.002929383468989662\n",
      "Iteration: 3361 lambda_k: 1 Loss: 0.002929383598595788\n",
      "Iteration: 3362 lambda_k: 1 Loss: 0.002929383727523262\n",
      "Iteration: 3363 lambda_k: 1 Loss: 0.0029293838557756507\n",
      "Iteration: 3364 lambda_k: 1 Loss: 0.002929383983356497\n",
      "Iteration: 3365 lambda_k: 1 Loss: 0.0029293841102693116\n",
      "Iteration: 3366 lambda_k: 1 Loss: 0.00292938423651762\n",
      "Iteration: 3367 lambda_k: 1 Loss: 0.002929384362104914\n",
      "Iteration: 3368 lambda_k: 1 Loss: 0.0029293844870346567\n",
      "Iteration: 3369 lambda_k: 1 Loss: 0.0029293846113102808\n",
      "Iteration: 3370 lambda_k: 1 Loss: 0.002929384734935253\n",
      "Iteration: 3371 lambda_k: 1 Loss: 0.002929384857912966\n",
      "Iteration: 3372 lambda_k: 1 Loss: 0.0029293849802468305\n",
      "Iteration: 3373 lambda_k: 1 Loss: 0.002929385101940234\n",
      "Iteration: 3374 lambda_k: 1 Loss: 0.002929385222996523\n",
      "Iteration: 3375 lambda_k: 1 Loss: 0.0029293853434190433\n",
      "Iteration: 3376 lambda_k: 1 Loss: 0.00292938546321113\n",
      "Iteration: 3377 lambda_k: 1 Loss: 0.0029293855823761044\n",
      "Iteration: 3378 lambda_k: 1 Loss: 0.0029293857009172405\n",
      "Iteration: 3379 lambda_k: 1 Loss: 0.0029293858188378227\n",
      "Iteration: 3380 lambda_k: 1 Loss: 0.0029293859361410987\n",
      "Iteration: 3381 lambda_k: 1 Loss: 0.0029293860528303195\n",
      "Iteration: 3382 lambda_k: 1 Loss: 0.002929386168908694\n",
      "Iteration: 3383 lambda_k: 1 Loss: 0.00292938628437944\n",
      "Iteration: 3384 lambda_k: 1 Loss: 0.0029293863992457454\n",
      "Iteration: 3385 lambda_k: 1 Loss: 0.0029293865135107877\n",
      "Iteration: 3386 lambda_k: 1 Loss: 0.0029293866271777255\n",
      "Iteration: 3387 lambda_k: 1 Loss: 0.002929386740249689\n",
      "Iteration: 3388 lambda_k: 1 Loss: 0.0029293868527297973\n",
      "Iteration: 3389 lambda_k: 1 Loss: 0.0029293869646211633\n",
      "Iteration: 3390 lambda_k: 1 Loss: 0.0029293870759268792\n",
      "Iteration: 3391 lambda_k: 1 Loss: 0.0029293871866500182\n",
      "Iteration: 3392 lambda_k: 1 Loss: 0.0029293872967936416\n",
      "Iteration: 3393 lambda_k: 1 Loss: 0.0029293874063607847\n",
      "Iteration: 3394 lambda_k: 1 Loss: 0.0029293875153544773\n",
      "Iteration: 3395 lambda_k: 1 Loss: 0.0029293876237777294\n",
      "Iteration: 3396 lambda_k: 1 Loss: 0.0029293877316335384\n",
      "Iteration: 3397 lambda_k: 1 Loss: 0.0029293878389248657\n",
      "Iteration: 3398 lambda_k: 1 Loss: 0.002929387945654686\n",
      "Iteration: 3399 lambda_k: 1 Loss: 0.002929388051825941\n",
      "Iteration: 3400 lambda_k: 1 Loss: 0.002929388157441566\n",
      "Iteration: 3401 lambda_k: 1 Loss: 0.0029293882625044875\n",
      "Iteration: 3402 lambda_k: 1 Loss: 0.00292938836701759\n",
      "Iteration: 3403 lambda_k: 1 Loss: 0.0029293884709837665\n",
      "Iteration: 3404 lambda_k: 1 Loss: 0.0029293885744058872\n",
      "Iteration: 3405 lambda_k: 1 Loss: 0.00292938867728681\n",
      "Iteration: 3406 lambda_k: 1 Loss: 0.0029293887796293682\n",
      "Iteration: 3407 lambda_k: 1 Loss: 0.0029293888814363833\n",
      "Iteration: 3408 lambda_k: 1 Loss: 0.002929388982710665\n",
      "Iteration: 3409 lambda_k: 1 Loss: 0.002929389083455007\n",
      "Iteration: 3410 lambda_k: 1 Loss: 0.0029293891836721995\n",
      "Iteration: 3411 lambda_k: 1 Loss: 0.0029293892833650113\n",
      "Iteration: 3412 lambda_k: 1 Loss: 0.0029293893825361864\n",
      "Iteration: 3413 lambda_k: 1 Loss: 0.0029293894811884677\n",
      "Iteration: 3414 lambda_k: 1 Loss: 0.0029293895793245676\n",
      "Iteration: 3415 lambda_k: 1 Loss: 0.002929389676947203\n",
      "Iteration: 3416 lambda_k: 1 Loss: 0.0029293897740590618\n",
      "Iteration: 3417 lambda_k: 1 Loss: 0.0029293898706628193\n",
      "Iteration: 3418 lambda_k: 1 Loss: 0.002929389966761162\n",
      "Iteration: 3419 lambda_k: 1 Loss: 0.0029293900623567275\n",
      "Iteration: 3420 lambda_k: 1 Loss: 0.0029293901574521672\n",
      "Iteration: 3421 lambda_k: 1 Loss: 0.0029293902520500983\n",
      "Iteration: 3422 lambda_k: 1 Loss: 0.002929390346153104\n",
      "Iteration: 3423 lambda_k: 1 Loss: 0.0029293904397638213\n",
      "Iteration: 3424 lambda_k: 1 Loss: 0.002929390532884801\n",
      "Iteration: 3425 lambda_k: 1 Loss: 0.0029293906255186243\n",
      "Iteration: 3426 lambda_k: 1 Loss: 0.002929390717667848\n",
      "Iteration: 3427 lambda_k: 1 Loss: 0.0029293908093350153\n",
      "Iteration: 3428 lambda_k: 1 Loss: 0.002929390900522648\n",
      "Iteration: 3429 lambda_k: 1 Loss: 0.0029293909912332704\n",
      "Iteration: 3430 lambda_k: 1 Loss: 0.0029293910814693807\n",
      "Iteration: 3431 lambda_k: 1 Loss: 0.002929391171233467\n",
      "Iteration: 3432 lambda_k: 1 Loss: 0.0029293912605280145\n",
      "Iteration: 3433 lambda_k: 1 Loss: 0.0029293913493554664\n",
      "Iteration: 3434 lambda_k: 1 Loss: 0.0029293914377182826\n",
      "Iteration: 3435 lambda_k: 1 Loss: 0.002929391525618917\n",
      "Iteration: 3436 lambda_k: 1 Loss: 0.002929391613059765\n",
      "Iteration: 3437 lambda_k: 1 Loss: 0.0029293917000432464\n",
      "Iteration: 3438 lambda_k: 1 Loss: 0.002929391786571772\n",
      "Iteration: 3439 lambda_k: 1 Loss: 0.002929391872647738\n",
      "Iteration: 3440 lambda_k: 1 Loss: 0.0029293919582734887\n",
      "Iteration: 3441 lambda_k: 1 Loss: 0.0029293920434513994\n",
      "Iteration: 3442 lambda_k: 1 Loss: 0.002929392128183811\n",
      "Iteration: 3443 lambda_k: 1 Loss: 0.002929392212473062\n",
      "Iteration: 3444 lambda_k: 1 Loss: 0.0029293922963214967\n",
      "Iteration: 3445 lambda_k: 1 Loss: 0.002929392379731396\n",
      "Iteration: 3446 lambda_k: 1 Loss: 0.0029293924627050746\n",
      "Iteration: 3447 lambda_k: 1 Loss: 0.0029293925452448175\n",
      "Iteration: 3448 lambda_k: 1 Loss: 0.0029293926273529034\n",
      "Iteration: 3449 lambda_k: 1 Loss: 0.0029293927090315894\n",
      "Iteration: 3450 lambda_k: 1 Loss: 0.0029293927902831387\n",
      "Iteration: 3451 lambda_k: 1 Loss: 0.0029293928711097857\n",
      "Iteration: 3452 lambda_k: 1 Loss: 0.0029293929515137604\n",
      "Iteration: 3453 lambda_k: 1 Loss: 0.002929393031497264\n",
      "Iteration: 3454 lambda_k: 1 Loss: 0.002929393111062521\n",
      "Iteration: 3455 lambda_k: 1 Loss: 0.002929393190211715\n",
      "Iteration: 3456 lambda_k: 1 Loss: 0.0029293932689470332\n",
      "Iteration: 3457 lambda_k: 1 Loss: 0.00292939334727064\n",
      "Iteration: 3458 lambda_k: 1 Loss: 0.002929393425184694\n",
      "Iteration: 3459 lambda_k: 1 Loss: 0.0029293935026913566\n",
      "Iteration: 3460 lambda_k: 1 Loss: 0.0029293935797927413\n",
      "Iteration: 3461 lambda_k: 1 Loss: 0.002929393656490987\n",
      "Iteration: 3462 lambda_k: 1 Loss: 0.0029293937327881936\n",
      "Iteration: 3463 lambda_k: 1 Loss: 0.002929393808686468\n",
      "Iteration: 3464 lambda_k: 1 Loss: 0.0029293938841879114\n",
      "Iteration: 3465 lambda_k: 1 Loss: 0.0029293939592945992\n",
      "Iteration: 3466 lambda_k: 1 Loss: 0.0029293940340086094\n",
      "Iteration: 3467 lambda_k: 1 Loss: 0.0029293941083319945\n",
      "Iteration: 3468 lambda_k: 1 Loss: 0.002929394182266783\n",
      "Iteration: 3469 lambda_k: 1 Loss: 0.0029293942558150363\n",
      "Iteration: 3470 lambda_k: 1 Loss: 0.0029293943289787658\n",
      "Iteration: 3471 lambda_k: 1 Loss: 0.002929394401760009\n",
      "Iteration: 3472 lambda_k: 1 Loss: 0.0029293944741607507\n",
      "Iteration: 3473 lambda_k: 1 Loss: 0.002929394546182997\n",
      "Iteration: 3474 lambda_k: 1 Loss: 0.002929394617828722\n",
      "Iteration: 3475 lambda_k: 1 Loss: 0.002929394689099912\n",
      "Iteration: 3476 lambda_k: 1 Loss: 0.0029293947599985186\n",
      "Iteration: 3477 lambda_k: 1 Loss: 0.0029293948305264927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3478 lambda_k: 1 Loss: 0.002929394900685787\n",
      "Iteration: 3479 lambda_k: 1 Loss: 0.002929394970478325\n",
      "Iteration: 3480 lambda_k: 1 Loss: 0.0029293950399060416\n",
      "Iteration: 3481 lambda_k: 1 Loss: 0.002929395108970849\n",
      "Iteration: 3482 lambda_k: 1 Loss: 0.0029293951776746385\n",
      "Iteration: 3483 lambda_k: 1 Loss: 0.002929395246019304\n",
      "Iteration: 3484 lambda_k: 1 Loss: 0.0029293953140067156\n",
      "Iteration: 3485 lambda_k: 1 Loss: 0.0029293953816387553\n",
      "Iteration: 3486 lambda_k: 1 Loss: 0.002929395448917292\n",
      "Iteration: 3487 lambda_k: 1 Loss: 0.0029293955158441747\n",
      "Iteration: 3488 lambda_k: 1 Loss: 0.002929395582421249\n",
      "Iteration: 3489 lambda_k: 1 Loss: 0.0029293956486503457\n",
      "Iteration: 3490 lambda_k: 1 Loss: 0.0029293957145333028\n",
      "Iteration: 3491 lambda_k: 1 Loss: 0.0029293957800719124\n",
      "Iteration: 3492 lambda_k: 1 Loss: 0.002929395845267982\n",
      "Iteration: 3493 lambda_k: 1 Loss: 0.0029293959101233122\n",
      "Iteration: 3494 lambda_k: 1 Loss: 0.002929395974639693\n",
      "Iteration: 3495 lambda_k: 1 Loss: 0.0029293960388188916\n",
      "Iteration: 3496 lambda_k: 1 Loss: 0.0029293961026626825\n",
      "Iteration: 3497 lambda_k: 1 Loss: 0.0029293961661728176\n",
      "Iteration: 3498 lambda_k: 1 Loss: 0.002929396229351049\n",
      "Iteration: 3499 lambda_k: 1 Loss: 0.002929396292199118\n",
      "Iteration: 3500 lambda_k: 1 Loss: 0.002929396354718742\n",
      "Iteration: 3501 lambda_k: 1 Loss: 0.0029293964169116627\n",
      "Iteration: 3502 lambda_k: 1 Loss: 0.002929396478779573\n",
      "Iteration: 3503 lambda_k: 1 Loss: 0.002929396540324171\n",
      "Iteration: 3504 lambda_k: 1 Loss: 0.0029293966015471597\n",
      "Iteration: 3505 lambda_k: 1 Loss: 0.002929396662450228\n",
      "Iteration: 3506 lambda_k: 1 Loss: 0.002929396723035051\n",
      "Iteration: 3507 lambda_k: 1 Loss: 0.002929396783303305\n",
      "Iteration: 3508 lambda_k: 1 Loss: 0.002929396843256636\n",
      "Iteration: 3509 lambda_k: 1 Loss: 0.0029293969028966944\n",
      "Iteration: 3510 lambda_k: 1 Loss: 0.0029293969622251246\n",
      "Iteration: 3511 lambda_k: 1 Loss: 0.002929397021243554\n",
      "Iteration: 3512 lambda_k: 1 Loss: 0.002929397079953618\n",
      "Iteration: 3513 lambda_k: 1 Loss: 0.0029293971383569117\n",
      "Iteration: 3514 lambda_k: 1 Loss: 0.002929397196455072\n",
      "Iteration: 3515 lambda_k: 1 Loss: 0.002929397254249679\n",
      "Iteration: 3516 lambda_k: 1 Loss: 0.0029293973117423253\n",
      "Iteration: 3517 lambda_k: 1 Loss: 0.0029293973689345815\n",
      "Iteration: 3518 lambda_k: 1 Loss: 0.002929397425828043\n",
      "Iteration: 3519 lambda_k: 1 Loss: 0.002929397482424256\n",
      "Iteration: 3520 lambda_k: 1 Loss: 0.0029293975387247764\n",
      "Iteration: 3521 lambda_k: 1 Loss: 0.0029293975947311704\n",
      "Iteration: 3522 lambda_k: 1 Loss: 0.002929397650444972\n",
      "Iteration: 3523 lambda_k: 1 Loss: 0.0029293977058677113\n",
      "Iteration: 3524 lambda_k: 1 Loss: 0.0029293977610008984\n",
      "Iteration: 3525 lambda_k: 1 Loss: 0.0029293978158460706\n",
      "Iteration: 3526 lambda_k: 1 Loss: 0.0029293978704047355\n",
      "Iteration: 3527 lambda_k: 1 Loss: 0.0029293979246783845\n",
      "Iteration: 3528 lambda_k: 1 Loss: 0.002929397978668529\n",
      "Iteration: 3529 lambda_k: 1 Loss: 0.002929398032376635\n",
      "Iteration: 3530 lambda_k: 1 Loss: 0.0029293980858041743\n",
      "Iteration: 3531 lambda_k: 1 Loss: 0.002929398138952626\n",
      "Iteration: 3532 lambda_k: 1 Loss: 0.0029293981918234614\n",
      "Iteration: 3533 lambda_k: 1 Loss: 0.0029293982444181183\n",
      "Iteration: 3534 lambda_k: 1 Loss: 0.002929398296738051\n",
      "Iteration: 3535 lambda_k: 1 Loss: 0.0029293983487846853\n",
      "Iteration: 3536 lambda_k: 1 Loss: 0.002929398400559466\n",
      "Iteration: 3537 lambda_k: 1 Loss: 0.002929398452063821\n",
      "Iteration: 3538 lambda_k: 1 Loss: 0.0029293985032991577\n",
      "Iteration: 3539 lambda_k: 1 Loss: 0.0029293985542668977\n",
      "Iteration: 3540 lambda_k: 1 Loss: 0.0029293986049684284\n",
      "Iteration: 3541 lambda_k: 1 Loss: 0.0029293986554051457\n",
      "Iteration: 3542 lambda_k: 1 Loss: 0.002929398705578427\n",
      "Iteration: 3543 lambda_k: 1 Loss: 0.002929398755489665\n",
      "Iteration: 3544 lambda_k: 1 Loss: 0.0029293988051402407\n",
      "Iteration: 3545 lambda_k: 1 Loss: 0.0029293988545314943\n",
      "Iteration: 3546 lambda_k: 1 Loss: 0.0029293989036647933\n",
      "Iteration: 3547 lambda_k: 1 Loss: 0.0029293989525414925\n",
      "Iteration: 3548 lambda_k: 1 Loss: 0.002929399001162945\n",
      "Iteration: 3549 lambda_k: 1 Loss: 0.002929399049530479\n",
      "Iteration: 3550 lambda_k: 1 Loss: 0.002929399097645423\n",
      "Iteration: 3551 lambda_k: 1 Loss: 0.0029293991455090814\n",
      "Iteration: 3552 lambda_k: 1 Loss: 0.002929399193122797\n",
      "Iteration: 3553 lambda_k: 1 Loss: 0.0029293992404878753\n",
      "Iteration: 3554 lambda_k: 1 Loss: 0.0029293992876055965\n",
      "Iteration: 3555 lambda_k: 1 Loss: 0.002929399334477273\n",
      "Iteration: 3556 lambda_k: 1 Loss: 0.0029293993811041956\n",
      "Iteration: 3557 lambda_k: 1 Loss: 0.0029293994274876244\n",
      "Iteration: 3558 lambda_k: 1 Loss: 0.002929399473628863\n",
      "Iteration: 3559 lambda_k: 1 Loss: 0.0029293995195291654\n",
      "Iteration: 3560 lambda_k: 1 Loss: 0.0029293995651897943\n",
      "Iteration: 3561 lambda_k: 1 Loss: 0.0029293996106119972\n",
      "Iteration: 3562 lambda_k: 1 Loss: 0.002929399655797041\n",
      "Iteration: 3563 lambda_k: 1 Loss: 0.002929399700746141\n",
      "Iteration: 3564 lambda_k: 1 Loss: 0.0029293997454605566\n",
      "Iteration: 3565 lambda_k: 1 Loss: 0.0029293997899414955\n",
      "Iteration: 3566 lambda_k: 1 Loss: 0.00292939983419019\n",
      "Iteration: 3567 lambda_k: 1 Loss: 0.0029293998782078546\n",
      "Iteration: 3568 lambda_k: 1 Loss: 0.0029293999219957047\n",
      "Iteration: 3569 lambda_k: 1 Loss: 0.0029293999655549373\n",
      "Iteration: 3570 lambda_k: 1 Loss: 0.0029294000088867476\n",
      "Iteration: 3571 lambda_k: 1 Loss: 0.0029294000519923405\n",
      "Iteration: 3572 lambda_k: 1 Loss: 0.002929400094872884\n",
      "Iteration: 3573 lambda_k: 1 Loss: 0.002929400137529548\n",
      "Iteration: 3574 lambda_k: 1 Loss: 0.002929400179963526\n",
      "Iteration: 3575 lambda_k: 1 Loss: 0.0029294002221759686\n",
      "Iteration: 3576 lambda_k: 1 Loss: 0.002929400264168035\n",
      "Iteration: 3577 lambda_k: 1 Loss: 0.002929400305940891\n",
      "Iteration: 3578 lambda_k: 1 Loss: 0.002929400347495687\n",
      "Iteration: 3579 lambda_k: 1 Loss: 0.002929400388833549\n",
      "Iteration: 3580 lambda_k: 1 Loss: 0.002929400429955608\n",
      "Iteration: 3581 lambda_k: 1 Loss: 0.002929400470863001\n",
      "Iteration: 3582 lambda_k: 1 Loss: 0.002929400511556853\n",
      "Iteration: 3583 lambda_k: 1 Loss: 0.0029294005520382783\n",
      "Iteration: 3584 lambda_k: 1 Loss: 0.0029294005923083967\n",
      "Iteration: 3585 lambda_k: 1 Loss: 0.0029294006323683113\n",
      "Iteration: 3586 lambda_k: 1 Loss: 0.0029294006722191154\n",
      "Iteration: 3587 lambda_k: 1 Loss: 0.0029294007118619066\n",
      "Iteration: 3588 lambda_k: 1 Loss: 0.002929400751297784\n",
      "Iteration: 3589 lambda_k: 1 Loss: 0.002929400790527814\n",
      "Iteration: 3590 lambda_k: 1 Loss: 0.0029294008295530864\n",
      "Iteration: 3591 lambda_k: 1 Loss: 0.0029294008683746573\n",
      "Iteration: 3592 lambda_k: 1 Loss: 0.0029294009069936048\n",
      "Iteration: 3593 lambda_k: 1 Loss: 0.002929400945410985\n",
      "Iteration: 3594 lambda_k: 1 Loss: 0.0029294009836278597\n",
      "Iteration: 3595 lambda_k: 1 Loss: 0.002929401021645272\n",
      "Iteration: 3596 lambda_k: 1 Loss: 0.0029294010594642663\n",
      "Iteration: 3597 lambda_k: 1 Loss: 0.0029294010970858674\n",
      "Iteration: 3598 lambda_k: 1 Loss: 0.0029294011345111235\n",
      "Iteration: 3599 lambda_k: 1 Loss: 0.0029294011717410687\n",
      "Iteration: 3600 lambda_k: 1 Loss: 0.0029294012087767115\n",
      "Iteration: 3601 lambda_k: 1 Loss: 0.0029294012456190658\n",
      "Iteration: 3602 lambda_k: 1 Loss: 0.0029294012822691567\n",
      "Iteration: 3603 lambda_k: 1 Loss: 0.00292940131872798\n",
      "Iteration: 3604 lambda_k: 1 Loss: 0.002929401354996537\n",
      "Iteration: 3605 lambda_k: 1 Loss: 0.002929401391075823\n",
      "Iteration: 3606 lambda_k: 1 Loss: 0.002929401426966845\n",
      "Iteration: 3607 lambda_k: 1 Loss: 0.002929401462670568\n",
      "Iteration: 3608 lambda_k: 1 Loss: 0.0029294014981879647\n",
      "Iteration: 3609 lambda_k: 1 Loss: 0.0029294015335200306\n",
      "Iteration: 3610 lambda_k: 1 Loss: 0.0029294015686677187\n",
      "Iteration: 3611 lambda_k: 1 Loss: 0.0029294016036320023\n",
      "Iteration: 3612 lambda_k: 1 Loss: 0.002929401638413845\n",
      "Iteration: 3613 lambda_k: 1 Loss: 0.0029294016730141937\n",
      "Iteration: 3614 lambda_k: 1 Loss: 0.002929401707434001\n",
      "Iteration: 3615 lambda_k: 1 Loss: 0.0029294017416742115\n",
      "Iteration: 3616 lambda_k: 1 Loss: 0.0029294017757357615\n",
      "Iteration: 3617 lambda_k: 1 Loss: 0.002929401809619582\n",
      "Iteration: 3618 lambda_k: 1 Loss: 0.002929401843326617\n",
      "Iteration: 3619 lambda_k: 1 Loss: 0.0029294018768577794\n",
      "Iteration: 3620 lambda_k: 1 Loss: 0.0029294019102140086\n",
      "Iteration: 3621 lambda_k: 1 Loss: 0.0029294019433961935\n",
      "Iteration: 3622 lambda_k: 1 Loss: 0.002929401976405247\n",
      "Iteration: 3623 lambda_k: 1 Loss: 0.002929402009242072\n",
      "Iteration: 3624 lambda_k: 1 Loss: 0.00292940204190759\n",
      "Iteration: 3625 lambda_k: 1 Loss: 0.002929402074402667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3626 lambda_k: 1 Loss: 0.0029294021067282276\n",
      "Iteration: 3627 lambda_k: 1 Loss: 0.0029294021388851536\n",
      "Iteration: 3628 lambda_k: 1 Loss: 0.0029294021708743106\n",
      "Iteration: 3629 lambda_k: 1 Loss: 0.0029294022026965724\n",
      "Iteration: 3630 lambda_k: 1 Loss: 0.0029294022343528377\n",
      "Iteration: 3631 lambda_k: 1 Loss: 0.0029294022658439413\n",
      "Iteration: 3632 lambda_k: 1 Loss: 0.002929402297170763\n",
      "Iteration: 3633 lambda_k: 1 Loss: 0.0029294023283341552\n",
      "Iteration: 3634 lambda_k: 1 Loss: 0.002929402359334983\n",
      "Iteration: 3635 lambda_k: 1 Loss: 0.00292940239017411\n",
      "Iteration: 3636 lambda_k: 1 Loss: 0.002929402420852351\n",
      "Iteration: 3637 lambda_k: 1 Loss: 0.00292940245137056\n",
      "Iteration: 3638 lambda_k: 1 Loss: 0.002929402481729575\n",
      "Iteration: 3639 lambda_k: 1 Loss: 0.002929402511930239\n",
      "Iteration: 3640 lambda_k: 1 Loss: 0.002929402541973362\n",
      "Iteration: 3641 lambda_k: 1 Loss: 0.002929402571859776\n",
      "Iteration: 3642 lambda_k: 1 Loss: 0.0029294026015902965\n",
      "Iteration: 3643 lambda_k: 1 Loss: 0.002929402631165758\n",
      "Iteration: 3644 lambda_k: 1 Loss: 0.0029294026605869512\n",
      "Iteration: 3645 lambda_k: 1 Loss: 0.0029294026898546766\n",
      "Iteration: 3646 lambda_k: 1 Loss: 0.0029294027189697563\n",
      "Iteration: 3647 lambda_k: 1 Loss: 0.0029294027479329706\n",
      "Iteration: 3648 lambda_k: 1 Loss: 0.0029294027767451346\n",
      "Iteration: 3649 lambda_k: 1 Loss: 0.0029294028054070017\n",
      "Iteration: 3650 lambda_k: 1 Loss: 0.0029294028339193977\n",
      "Iteration: 3651 lambda_k: 1 Loss: 0.002929402862283095\n",
      "Iteration: 3652 lambda_k: 1 Loss: 0.0029294028904988494\n",
      "Iteration: 3653 lambda_k: 1 Loss: 0.0029294029185674454\n",
      "Iteration: 3654 lambda_k: 1 Loss: 0.0029294029464896534\n",
      "Iteration: 3655 lambda_k: 1 Loss: 0.002929402974266253\n",
      "Iteration: 3656 lambda_k: 1 Loss: 0.0029294030018979893\n",
      "Iteration: 3657 lambda_k: 1 Loss: 0.002929403029385623\n",
      "Iteration: 3658 lambda_k: 1 Loss: 0.0029294030567299134\n",
      "Iteration: 3659 lambda_k: 1 Loss: 0.002929403083931608\n",
      "Iteration: 3660 lambda_k: 1 Loss: 0.0029294031109914445\n",
      "Iteration: 3661 lambda_k: 1 Loss: 0.002929403137910153\n",
      "Iteration: 3662 lambda_k: 1 Loss: 0.002929403164688488\n",
      "Iteration: 3663 lambda_k: 1 Loss: 0.002929403191327192\n",
      "Iteration: 3664 lambda_k: 1 Loss: 0.0029294032178269857\n",
      "Iteration: 3665 lambda_k: 1 Loss: 0.002929403244188589\n",
      "Iteration: 3666 lambda_k: 1 Loss: 0.0029294032704127353\n",
      "Iteration: 3667 lambda_k: 1 Loss: 0.002929403296500139\n",
      "Iteration: 3668 lambda_k: 1 Loss: 0.0029294033224515083\n",
      "Iteration: 3669 lambda_k: 1 Loss: 0.00292940334826756\n",
      "Iteration: 3670 lambda_k: 1 Loss: 0.002929403373949012\n",
      "Iteration: 3671 lambda_k: 1 Loss: 0.0029294033994965354\n",
      "Iteration: 3672 lambda_k: 1 Loss: 0.0029294034249108584\n",
      "Iteration: 3673 lambda_k: 1 Loss: 0.002929403450192672\n",
      "Iteration: 3674 lambda_k: 1 Loss: 0.0029294034753426664\n",
      "Iteration: 3675 lambda_k: 1 Loss: 0.002929403500361529\n",
      "Iteration: 3676 lambda_k: 1 Loss: 0.002929403525249935\n",
      "Iteration: 3677 lambda_k: 1 Loss: 0.002929403550008596\n",
      "Iteration: 3678 lambda_k: 1 Loss: 0.002929403574638156\n",
      "Iteration: 3679 lambda_k: 1 Loss: 0.0029294035991393125\n",
      "Iteration: 3680 lambda_k: 1 Loss: 0.0029294036235127186\n",
      "Iteration: 3681 lambda_k: 1 Loss: 0.002929403647759063\n",
      "Iteration: 3682 lambda_k: 1 Loss: 0.002929403671879001\n",
      "Iteration: 3683 lambda_k: 1 Loss: 0.0029294036958731873\n",
      "Iteration: 3684 lambda_k: 1 Loss: 0.002929403719742289\n",
      "Iteration: 3685 lambda_k: 1 Loss: 0.002929403743486972\n",
      "Iteration: 3686 lambda_k: 1 Loss: 0.0029294037671078518\n",
      "Iteration: 3687 lambda_k: 1 Loss: 0.002929403790605592\n",
      "Iteration: 3688 lambda_k: 1 Loss: 0.0029294038139808312\n",
      "Iteration: 3689 lambda_k: 1 Loss: 0.0029294038372342152\n",
      "Iteration: 3690 lambda_k: 1 Loss: 0.00292940386036639\n",
      "Iteration: 3691 lambda_k: 1 Loss: 0.002929403883377972\n",
      "Iteration: 3692 lambda_k: 1 Loss: 0.002929403906269596\n",
      "Iteration: 3693 lambda_k: 1 Loss: 0.0029294039290418874\n",
      "Iteration: 3694 lambda_k: 1 Loss: 0.0029294039516954786\n",
      "Iteration: 3695 lambda_k: 1 Loss: 0.002929403974230987\n",
      "Iteration: 3696 lambda_k: 1 Loss: 0.0029294039966490325\n",
      "Iteration: 3697 lambda_k: 1 Loss: 0.0029294040189502177\n",
      "Iteration: 3698 lambda_k: 1 Loss: 0.002929404041135176\n",
      "Iteration: 3699 lambda_k: 1 Loss: 0.00292940406320449\n",
      "Iteration: 3700 lambda_k: 1 Loss: 0.00292940408515874\n",
      "Iteration: 3701 lambda_k: 1 Loss: 0.0029294041069985663\n",
      "Iteration: 3702 lambda_k: 1 Loss: 0.0029294041287245757\n",
      "Iteration: 3703 lambda_k: 1 Loss: 0.002929404150337346\n",
      "Iteration: 3704 lambda_k: 1 Loss: 0.0029294041718374584\n",
      "Iteration: 3705 lambda_k: 1 Loss: 0.002929404193225511\n",
      "Iteration: 3706 lambda_k: 1 Loss: 0.0029294042145020923\n",
      "Iteration: 3707 lambda_k: 1 Loss: 0.002929404235667787\n",
      "Iteration: 3708 lambda_k: 1 Loss: 0.002929404256723173\n",
      "Iteration: 3709 lambda_k: 1 Loss: 0.002929404277668824\n",
      "Iteration: 3710 lambda_k: 1 Loss: 0.0029294042985053095\n",
      "Iteration: 3711 lambda_k: 1 Loss: 0.002929404319233197\n",
      "Iteration: 3712 lambda_k: 1 Loss: 0.0029294043398530602\n",
      "Iteration: 3713 lambda_k: 1 Loss: 0.0029294043603654597\n",
      "Iteration: 3714 lambda_k: 1 Loss: 0.0029294043807709556\n",
      "Iteration: 3715 lambda_k: 1 Loss: 0.002929404401070111\n",
      "Iteration: 3716 lambda_k: 1 Loss: 0.002929404421263479\n",
      "Iteration: 3717 lambda_k: 1 Loss: 0.002929404441351626\n",
      "Iteration: 3718 lambda_k: 1 Loss: 0.002929404461335092\n",
      "Iteration: 3719 lambda_k: 1 Loss: 0.002929404481214414\n",
      "Iteration: 3720 lambda_k: 1 Loss: 0.002929404500990142\n",
      "Iteration: 3721 lambda_k: 1 Loss: 0.0029294045206628284\n",
      "Iteration: 3722 lambda_k: 1 Loss: 0.0029294045402329977\n",
      "Iteration: 3723 lambda_k: 1 Loss: 0.002929404559701194\n",
      "Iteration: 3724 lambda_k: 1 Loss: 0.0029294045790679377\n",
      "Iteration: 3725 lambda_k: 1 Loss: 0.0029294045983337615\n",
      "Iteration: 3726 lambda_k: 1 Loss: 0.0029294046174992076\n",
      "Iteration: 3727 lambda_k: 1 Loss: 0.002929404636564784\n",
      "Iteration: 3728 lambda_k: 1 Loss: 0.0029294046555310206\n",
      "Iteration: 3729 lambda_k: 1 Loss: 0.0029294046743984286\n",
      "Iteration: 3730 lambda_k: 1 Loss: 0.0029294046931675195\n",
      "Iteration: 3731 lambda_k: 1 Loss: 0.0029294047118388323\n",
      "Iteration: 3732 lambda_k: 1 Loss: 0.002929404730412863\n",
      "Iteration: 3733 lambda_k: 1 Loss: 0.002929404748890115\n",
      "Iteration: 3734 lambda_k: 1 Loss: 0.0029294047672710983\n",
      "Iteration: 3735 lambda_k: 1 Loss: 0.0029294047855563127\n",
      "Iteration: 3736 lambda_k: 1 Loss: 0.002929404803746258\n",
      "Iteration: 3737 lambda_k: 1 Loss: 0.002929404821841427\n",
      "Iteration: 3738 lambda_k: 1 Loss: 0.0029294048398423194\n",
      "Iteration: 3739 lambda_k: 1 Loss: 0.002929404857749442\n",
      "Iteration: 3740 lambda_k: 1 Loss: 0.002929404875563267\n",
      "Iteration: 3741 lambda_k: 1 Loss: 0.0029294048932842843\n",
      "Iteration: 3742 lambda_k: 1 Loss: 0.00292940491091297\n",
      "Iteration: 3743 lambda_k: 1 Loss: 0.002929404928449836\n",
      "Iteration: 3744 lambda_k: 1 Loss: 0.0029294049458953282\n",
      "Iteration: 3745 lambda_k: 1 Loss: 0.002929404963249937\n",
      "Iteration: 3746 lambda_k: 1 Loss: 0.0029294049805141365\n",
      "Iteration: 3747 lambda_k: 1 Loss: 0.002929404997688396\n",
      "Iteration: 3748 lambda_k: 1 Loss: 0.0029294050147731896\n",
      "Iteration: 3749 lambda_k: 1 Loss: 0.0029294050317689863\n",
      "Iteration: 3750 lambda_k: 1 Loss: 0.0029294050486762422\n",
      "Iteration: 3751 lambda_k: 1 Loss: 0.0029294050654954224\n",
      "Iteration: 3752 lambda_k: 1 Loss: 0.0029294050822270086\n",
      "Iteration: 3753 lambda_k: 1 Loss: 0.0029294050988714227\n",
      "Iteration: 3754 lambda_k: 1 Loss: 0.0029294051154291414\n",
      "Iteration: 3755 lambda_k: 1 Loss: 0.0029294051319006106\n",
      "Iteration: 3756 lambda_k: 1 Loss: 0.002929405148286277\n",
      "Iteration: 3757 lambda_k: 1 Loss: 0.002929405164586613\n",
      "Iteration: 3758 lambda_k: 1 Loss: 0.0029294051808020404\n",
      "Iteration: 3759 lambda_k: 1 Loss: 0.002929405196933\n",
      "Iteration: 3760 lambda_k: 1 Loss: 0.002929405212979944\n",
      "Iteration: 3761 lambda_k: 1 Loss: 0.0029294052289433083\n",
      "Iteration: 3762 lambda_k: 1 Loss: 0.0029294052448235286\n",
      "Iteration: 3763 lambda_k: 1 Loss: 0.002929405260621033\n",
      "Iteration: 3764 lambda_k: 1 Loss: 0.0029294052763362537\n",
      "Iteration: 3765 lambda_k: 1 Loss: 0.0029294052919696363\n",
      "Iteration: 3766 lambda_k: 1 Loss: 0.0029294053075215893\n",
      "Iteration: 3767 lambda_k: 1 Loss: 0.002929405322992542\n",
      "Iteration: 3768 lambda_k: 1 Loss: 0.002929405338382917\n",
      "Iteration: 3769 lambda_k: 1 Loss: 0.0029294053536931346\n",
      "Iteration: 3770 lambda_k: 1 Loss: 0.0029294053689236264\n",
      "Iteration: 3771 lambda_k: 1 Loss: 0.002929405384074802\n",
      "Iteration: 3772 lambda_k: 1 Loss: 0.0029294053991470556\n",
      "Iteration: 3773 lambda_k: 1 Loss: 0.0029294054141408305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3774 lambda_k: 1 Loss: 0.0029294054290565207\n",
      "Iteration: 3775 lambda_k: 1 Loss: 0.002929405443894537\n",
      "Iteration: 3776 lambda_k: 1 Loss: 0.002929405458655281\n",
      "Iteration: 3777 lambda_k: 1 Loss: 0.00292940547333916\n",
      "Iteration: 3778 lambda_k: 1 Loss: 0.0029294054879465637\n",
      "Iteration: 3779 lambda_k: 1 Loss: 0.002929405502477905\n",
      "Iteration: 3780 lambda_k: 1 Loss: 0.002929405516933574\n",
      "Iteration: 3781 lambda_k: 1 Loss: 0.0029294055313139763\n",
      "Iteration: 3782 lambda_k: 1 Loss: 0.002929405545619492\n",
      "Iteration: 3783 lambda_k: 1 Loss: 0.0029294055598505214\n",
      "Iteration: 3784 lambda_k: 1 Loss: 0.0029294055740074453\n",
      "Iteration: 3785 lambda_k: 1 Loss: 0.002929405588090648\n",
      "Iteration: 3786 lambda_k: 1 Loss: 0.0029294056021005165\n",
      "Iteration: 3787 lambda_k: 1 Loss: 0.00292940561603744\n",
      "Iteration: 3788 lambda_k: 1 Loss: 0.002929405629901795\n",
      "Iteration: 3789 lambda_k: 1 Loss: 0.0029294056436939557\n",
      "Iteration: 3790 lambda_k: 1 Loss: 0.002929405657414297\n",
      "Iteration: 3791 lambda_k: 1 Loss: 0.002929405671063206\n",
      "Iteration: 3792 lambda_k: 1 Loss: 0.0029294056846410534\n",
      "Iteration: 3793 lambda_k: 1 Loss: 0.0029294056981481976\n",
      "Iteration: 3794 lambda_k: 1 Loss: 0.0029294057115850187\n",
      "Iteration: 3795 lambda_k: 1 Loss: 0.0029294057249518854\n",
      "Iteration: 3796 lambda_k: 1 Loss: 0.0029294057382491545\n",
      "Iteration: 3797 lambda_k: 1 Loss: 0.002929405751477197\n",
      "Iteration: 3798 lambda_k: 1 Loss: 0.0029294057646363533\n",
      "Iteration: 3799 lambda_k: 1 Loss: 0.0029294057777270134\n",
      "Iteration: 3800 lambda_k: 1 Loss: 0.002929405790749521\n",
      "Iteration: 3801 lambda_k: 1 Loss: 0.0029294058037042255\n",
      "Iteration: 3802 lambda_k: 1 Loss: 0.0029294058165914927\n",
      "Iteration: 3803 lambda_k: 1 Loss: 0.0029294058294116717\n",
      "Iteration: 3804 lambda_k: 1 Loss: 0.0029294058421650976\n",
      "Iteration: 3805 lambda_k: 1 Loss: 0.002929405854852139\n",
      "Iteration: 3806 lambda_k: 1 Loss: 0.00292940586747314\n",
      "Iteration: 3807 lambda_k: 1 Loss: 0.0029294058800284225\n",
      "Iteration: 3808 lambda_k: 1 Loss: 0.0029294058925183487\n",
      "Iteration: 3809 lambda_k: 1 Loss: 0.0029294059049432652\n",
      "Iteration: 3810 lambda_k: 1 Loss: 0.0029294059173035005\n",
      "Iteration: 3811 lambda_k: 1 Loss: 0.002929405929599395\n",
      "Iteration: 3812 lambda_k: 1 Loss: 0.0029294059418312835\n",
      "Iteration: 3813 lambda_k: 1 Loss: 0.0029294059539994957\n",
      "Iteration: 3814 lambda_k: 1 Loss: 0.002929405966104375\n",
      "Iteration: 3815 lambda_k: 1 Loss: 0.0029294059781462467\n",
      "Iteration: 3816 lambda_k: 1 Loss: 0.002929405990125431\n",
      "Iteration: 3817 lambda_k: 1 Loss: 0.0029294060020422625\n",
      "Iteration: 3818 lambda_k: 1 Loss: 0.002929406013897068\n",
      "Iteration: 3819 lambda_k: 1 Loss: 0.002929406025690166\n",
      "Iteration: 3820 lambda_k: 1 Loss: 0.0029294060374218845\n",
      "Iteration: 3821 lambda_k: 1 Loss: 0.0029294060490925324\n",
      "Iteration: 3822 lambda_k: 1 Loss: 0.0029294060607024325\n",
      "Iteration: 3823 lambda_k: 1 Loss: 0.0029294060722519174\n",
      "Iteration: 3824 lambda_k: 1 Loss: 0.0029294060837412824\n",
      "Iteration: 3825 lambda_k: 1 Loss: 0.002929406095170839\n",
      "Iteration: 3826 lambda_k: 1 Loss: 0.00292940610654092\n",
      "Iteration: 3827 lambda_k: 1 Loss: 0.002929406117851817\n",
      "Iteration: 3828 lambda_k: 1 Loss: 0.0029294061291038616\n",
      "Iteration: 3829 lambda_k: 1 Loss: 0.0029294061402973352\n",
      "Iteration: 3830 lambda_k: 1 Loss: 0.0029294061514325456\n",
      "Iteration: 3831 lambda_k: 1 Loss: 0.0029294061625097937\n",
      "Iteration: 3832 lambda_k: 1 Loss: 0.0029294061735293977\n",
      "Iteration: 3833 lambda_k: 1 Loss: 0.0029294061844916583\n",
      "Iteration: 3834 lambda_k: 1 Loss: 0.0029294061953968635\n",
      "Iteration: 3835 lambda_k: 1 Loss: 0.00292940620624533\n",
      "Iteration: 3836 lambda_k: 1 Loss: 0.002929406217037329\n",
      "Iteration: 3837 lambda_k: 1 Loss: 0.0029294062277731613\n",
      "Iteration: 3838 lambda_k: 1 Loss: 0.00292940623845313\n",
      "Iteration: 3839 lambda_k: 1 Loss: 0.002929406249077522\n",
      "Iteration: 3840 lambda_k: 1 Loss: 0.0029294062596466313\n",
      "Iteration: 3841 lambda_k: 1 Loss: 0.002929406270160735\n",
      "Iteration: 3842 lambda_k: 1 Loss: 0.0029294062806201207\n",
      "Iteration: 3843 lambda_k: 1 Loss: 0.0029294062910250906\n",
      "Iteration: 3844 lambda_k: 1 Loss: 0.002929406301375916\n",
      "Iteration: 3845 lambda_k: 1 Loss: 0.0029294063116728757\n",
      "Iteration: 3846 lambda_k: 1 Loss: 0.0029294063219162682\n",
      "Iteration: 3847 lambda_k: 1 Loss: 0.002929406332106349\n",
      "Iteration: 3848 lambda_k: 1 Loss: 0.002929406342243401\n",
      "Iteration: 3849 lambda_k: 1 Loss: 0.0029294063523277085\n",
      "Iteration: 3850 lambda_k: 1 Loss: 0.002929406362359546\n",
      "Iteration: 3851 lambda_k: 1 Loss: 0.0029294063723391862\n",
      "Iteration: 3852 lambda_k: 1 Loss: 0.002929406382266894\n",
      "Iteration: 3853 lambda_k: 1 Loss: 0.002929406392142954\n",
      "Iteration: 3854 lambda_k: 1 Loss: 0.0029294064019676352\n",
      "Iteration: 3855 lambda_k: 1 Loss: 0.0029294064117411935\n",
      "Iteration: 3856 lambda_k: 1 Loss: 0.0029294064214638945\n",
      "Iteration: 3857 lambda_k: 1 Loss: 0.002929406431135996\n",
      "Iteration: 3858 lambda_k: 1 Loss: 0.002929406440757778\n",
      "Iteration: 3859 lambda_k: 1 Loss: 0.0029294064503295096\n",
      "Iteration: 3860 lambda_k: 1 Loss: 0.0029294064598514467\n",
      "Iteration: 3861 lambda_k: 1 Loss: 0.002929406469323839\n",
      "Iteration: 3862 lambda_k: 1 Loss: 0.0029294064787469495\n",
      "Iteration: 3863 lambda_k: 1 Loss: 0.002929406488121042\n",
      "Iteration: 3864 lambda_k: 1 Loss: 0.002929406497446359\n",
      "Iteration: 3865 lambda_k: 1 Loss: 0.002929406506723163\n",
      "Iteration: 3866 lambda_k: 1 Loss: 0.002929406515951699\n",
      "Iteration: 3867 lambda_k: 1 Loss: 0.00292940652513223\n",
      "Iteration: 3868 lambda_k: 1 Loss: 0.0029294065342650075\n",
      "Iteration: 3869 lambda_k: 1 Loss: 0.00292940654335027\n",
      "Iteration: 3870 lambda_k: 1 Loss: 0.002929406552388278\n",
      "Iteration: 3871 lambda_k: 1 Loss: 0.0029294065613792567\n",
      "Iteration: 3872 lambda_k: 1 Loss: 0.0029294065703234676\n",
      "Iteration: 3873 lambda_k: 1 Loss: 0.002929406579221154\n",
      "Iteration: 3874 lambda_k: 1 Loss: 0.0029294065880725523\n",
      "Iteration: 3875 lambda_k: 1 Loss: 0.002929406596877908\n",
      "Iteration: 3876 lambda_k: 1 Loss: 0.0029294066056374576\n",
      "Iteration: 3877 lambda_k: 1 Loss: 0.002929406614351454\n",
      "Iteration: 3878 lambda_k: 1 Loss: 0.0029294066230201136\n",
      "Iteration: 3879 lambda_k: 1 Loss: 0.0029294066316436825\n",
      "Iteration: 3880 lambda_k: 1 Loss: 0.0029294066402223797\n",
      "Iteration: 3881 lambda_k: 1 Loss: 0.0029294066487564694\n",
      "Iteration: 3882 lambda_k: 1 Loss: 0.0029294066572461753\n",
      "Iteration: 3883 lambda_k: 1 Loss: 0.0029294066656917264\n",
      "Iteration: 3884 lambda_k: 1 Loss: 0.0029294066740933418\n",
      "Iteration: 3885 lambda_k: 1 Loss: 0.002929406682451248\n",
      "Iteration: 3886 lambda_k: 1 Loss: 0.0029294066907656915\n",
      "Iteration: 3887 lambda_k: 1 Loss: 0.0029294066990368764\n",
      "Iteration: 3888 lambda_k: 1 Loss: 0.0029294067072650554\n",
      "Iteration: 3889 lambda_k: 1 Loss: 0.0029294067154504323\n",
      "Iteration: 3890 lambda_k: 1 Loss: 0.0029294067235932355\n",
      "Iteration: 3891 lambda_k: 1 Loss: 0.002929406731693694\n",
      "Iteration: 3892 lambda_k: 1 Loss: 0.0029294067397520164\n",
      "Iteration: 3893 lambda_k: 1 Loss: 0.0029294067477684217\n",
      "Iteration: 3894 lambda_k: 1 Loss: 0.002929406755743147\n",
      "Iteration: 3895 lambda_k: 1 Loss: 0.002929406763676389\n",
      "Iteration: 3896 lambda_k: 1 Loss: 0.0029294067715683846\n",
      "Iteration: 3897 lambda_k: 1 Loss: 0.002929406779419325\n",
      "Iteration: 3898 lambda_k: 1 Loss: 0.002929406787229444\n",
      "Iteration: 3899 lambda_k: 1 Loss: 0.0029294067949989424\n",
      "Iteration: 3900 lambda_k: 1 Loss: 0.002929406802728043\n",
      "Iteration: 3901 lambda_k: 1 Loss: 0.0029294068104169465\n",
      "Iteration: 3902 lambda_k: 1 Loss: 0.0029294068180658536\n",
      "Iteration: 3903 lambda_k: 1 Loss: 0.002929406825674988\n",
      "Iteration: 3904 lambda_k: 1 Loss: 0.002929406833244552\n",
      "Iteration: 3905 lambda_k: 1 Loss: 0.0029294068407747656\n",
      "Iteration: 3906 lambda_k: 1 Loss: 0.0029294068482658008\n",
      "Iteration: 3907 lambda_k: 1 Loss: 0.002929406855717892\n",
      "Iteration: 3908 lambda_k: 1 Loss: 0.002929406863131226\n",
      "Iteration: 3909 lambda_k: 1 Loss: 0.0029294068705060156\n",
      "Iteration: 3910 lambda_k: 1 Loss: 0.0029294068778424496\n",
      "Iteration: 3911 lambda_k: 1 Loss: 0.0029294068851407344\n",
      "Iteration: 3912 lambda_k: 1 Loss: 0.002929406892401079\n",
      "Iteration: 3913 lambda_k: 1 Loss: 0.002929406899623675\n",
      "Iteration: 3914 lambda_k: 1 Loss: 0.0029294069068087148\n",
      "Iteration: 3915 lambda_k: 1 Loss: 0.0029294069139563944\n",
      "Iteration: 3916 lambda_k: 1 Loss: 0.002929406921066899\n",
      "Iteration: 3917 lambda_k: 1 Loss: 0.002929406928140438\n",
      "Iteration: 3918 lambda_k: 1 Loss: 0.0029294069351771925\n",
      "Iteration: 3919 lambda_k: 1 Loss: 0.002929406942177352\n",
      "Iteration: 3920 lambda_k: 1 Loss: 0.00292940694914112\n",
      "Iteration: 3921 lambda_k: 1 Loss: 0.002929406956068681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3922 lambda_k: 1 Loss: 0.0029294069629602205\n",
      "Iteration: 3923 lambda_k: 1 Loss: 0.0029294069698159444\n",
      "Iteration: 3924 lambda_k: 1 Loss: 0.002929406976636012\n",
      "Iteration: 3925 lambda_k: 1 Loss: 0.002929406983420626\n",
      "Iteration: 3926 lambda_k: 1 Loss: 0.0029294069901699663\n",
      "Iteration: 3927 lambda_k: 1 Loss: 0.0029294069968842152\n",
      "Iteration: 3928 lambda_k: 1 Loss: 0.0029294070035635667\n",
      "Iteration: 3929 lambda_k: 1 Loss: 0.0029294070102081882\n",
      "Iteration: 3930 lambda_k: 1 Loss: 0.0029294070168182523\n",
      "Iteration: 3931 lambda_k: 1 Loss: 0.002929407023393955\n",
      "Iteration: 3932 lambda_k: 1 Loss: 0.0029294070299354757\n",
      "Iteration: 3933 lambda_k: 1 Loss: 0.002929407036442996\n",
      "Iteration: 3934 lambda_k: 1 Loss: 0.0029294070429166747\n",
      "Iteration: 3935 lambda_k: 1 Loss: 0.0029294070493567003\n",
      "Iteration: 3936 lambda_k: 1 Loss: 0.0029294070557632527\n",
      "Iteration: 3937 lambda_k: 1 Loss: 0.0029294070621365\n",
      "Iteration: 3938 lambda_k: 1 Loss: 0.002929407068476604\n",
      "Iteration: 3939 lambda_k: 1 Loss: 0.0029294070747837666\n",
      "Iteration: 3940 lambda_k: 1 Loss: 0.0029294070810581413\n",
      "Iteration: 3941 lambda_k: 1 Loss: 0.0029294070872998897\n",
      "Iteration: 3942 lambda_k: 1 Loss: 0.0029294070935091967\n",
      "Iteration: 3943 lambda_k: 1 Loss: 0.0029294070996862317\n",
      "Iteration: 3944 lambda_k: 1 Loss: 0.0029294071058311613\n",
      "Iteration: 3945 lambda_k: 1 Loss: 0.00292940711194414\n",
      "Iteration: 3946 lambda_k: 1 Loss: 0.002929407118025348\n",
      "Iteration: 3947 lambda_k: 1 Loss: 0.0029294071240749445\n",
      "Iteration: 3948 lambda_k: 1 Loss: 0.002929407130093106\n",
      "Iteration: 3949 lambda_k: 1 Loss: 0.002929407136079978\n",
      "Iteration: 3950 lambda_k: 1 Loss: 0.0029294071420357245\n",
      "Iteration: 3951 lambda_k: 1 Loss: 0.0029294071479605294\n",
      "Iteration: 3952 lambda_k: 1 Loss: 0.002929407153854533\n",
      "Iteration: 3953 lambda_k: 1 Loss: 0.0029294071597178845\n",
      "Iteration: 3954 lambda_k: 1 Loss: 0.002929407165550778\n",
      "Iteration: 3955 lambda_k: 1 Loss: 0.0029294071713533596\n",
      "Iteration: 3956 lambda_k: 1 Loss: 0.0029294071771257834\n",
      "Iteration: 3957 lambda_k: 1 Loss: 0.0029294071828682025\n",
      "Iteration: 3958 lambda_k: 1 Loss: 0.0029294071885807705\n",
      "Iteration: 3959 lambda_k: 1 Loss: 0.0029294071942636557\n",
      "Iteration: 3960 lambda_k: 1 Loss: 0.002929407199917\n",
      "Iteration: 3961 lambda_k: 1 Loss: 0.0029294072055409616\n",
      "Iteration: 3962 lambda_k: 1 Loss: 0.002929407211135701\n",
      "Iteration: 3963 lambda_k: 1 Loss: 0.002929407216701361\n",
      "Iteration: 3964 lambda_k: 1 Loss: 0.002929407222238098\n",
      "Iteration: 3965 lambda_k: 1 Loss: 0.0029294072277460514\n",
      "Iteration: 3966 lambda_k: 1 Loss: 0.002929407233225388\n",
      "Iteration: 3967 lambda_k: 1 Loss: 0.002929407238676234\n",
      "Iteration: 3968 lambda_k: 1 Loss: 0.0029294072440987657\n",
      "Iteration: 3969 lambda_k: 1 Loss: 0.0029294072494931156\n",
      "Iteration: 3970 lambda_k: 1 Loss: 0.0029294072548594462\n",
      "Iteration: 3971 lambda_k: 1 Loss: 0.002929407260197883\n",
      "Iteration: 3972 lambda_k: 1 Loss: 0.002929407265508576\n",
      "Iteration: 3973 lambda_k: 1 Loss: 0.002929407270791666\n",
      "Iteration: 3974 lambda_k: 1 Loss: 0.0029294072760472963\n",
      "Iteration: 3975 lambda_k: 1 Loss: 0.0029294072812756267\n",
      "Iteration: 3976 lambda_k: 1 Loss: 0.002929407286476781\n",
      "Iteration: 3977 lambda_k: 1 Loss: 0.0029294072916509154\n",
      "Iteration: 3978 lambda_k: 1 Loss: 0.002929407296798164\n",
      "Iteration: 3979 lambda_k: 1 Loss: 0.002929407301918655\n",
      "Iteration: 3980 lambda_k: 1 Loss: 0.002929407307012546\n",
      "Iteration: 3981 lambda_k: 1 Loss: 0.002929407312079978\n",
      "Iteration: 3982 lambda_k: 1 Loss: 0.0029294073171210766\n",
      "Iteration: 3983 lambda_k: 1 Loss: 0.0029294073221359765\n",
      "Iteration: 3984 lambda_k: 1 Loss: 0.0029294073271248296\n",
      "Iteration: 3985 lambda_k: 1 Loss: 0.0029294073320877616\n",
      "Iteration: 3986 lambda_k: 1 Loss: 0.0029294073370248956\n",
      "Iteration: 3987 lambda_k: 1 Loss: 0.00292940734193638\n",
      "Iteration: 3988 lambda_k: 1 Loss: 0.002929407346822341\n",
      "Iteration: 3989 lambda_k: 1 Loss: 0.0029294073516829104\n",
      "Iteration: 3990 lambda_k: 1 Loss: 0.0029294073565182335\n",
      "Iteration: 3991 lambda_k: 1 Loss: 0.002929407361328428\n",
      "Iteration: 3992 lambda_k: 1 Loss: 0.0029294073661136435\n",
      "Iteration: 3993 lambda_k: 1 Loss: 0.0029294073708739927\n",
      "Iteration: 3994 lambda_k: 1 Loss: 0.0029294073756096096\n",
      "Iteration: 3995 lambda_k: 1 Loss: 0.0029294073803206354\n",
      "Iteration: 3996 lambda_k: 1 Loss: 0.0029294073850071787\n",
      "Iteration: 3997 lambda_k: 1 Loss: 0.00292940738966938\n",
      "Iteration: 3998 lambda_k: 1 Loss: 0.0029294073943073426\n",
      "Iteration: 3999 lambda_k: 1 Loss: 0.0029294073989212205\n",
      "Iteration: 4000 lambda_k: 1 Loss: 0.002929407403511127\n",
      "Iteration: 4001 lambda_k: 1 Loss: 0.0029294074080771825\n",
      "Iteration: 4002 lambda_k: 1 Loss: 0.002929407412619519\n",
      "Iteration: 4003 lambda_k: 1 Loss: 0.0029294074171382543\n",
      "Iteration: 4004 lambda_k: 1 Loss: 0.0029294074216335278\n",
      "Iteration: 4005 lambda_k: 1 Loss: 0.002929407426105429\n",
      "Iteration: 4006 lambda_k: 1 Loss: 0.0029294074305541025\n",
      "Iteration: 4007 lambda_k: 1 Loss: 0.0029294074349796766\n",
      "Iteration: 4008 lambda_k: 1 Loss: 0.0029294074393822543\n",
      "Iteration: 4009 lambda_k: 1 Loss: 0.0029294074437619566\n",
      "Iteration: 4010 lambda_k: 1 Loss: 0.002929407448118911\n",
      "Iteration: 4011 lambda_k: 1 Loss: 0.002929407452453237\n",
      "Iteration: 4012 lambda_k: 1 Loss: 0.0029294074567650435\n",
      "Iteration: 4013 lambda_k: 1 Loss: 0.0029294074610544383\n",
      "Iteration: 4014 lambda_k: 1 Loss: 0.0029294074653215666\n",
      "Iteration: 4015 lambda_k: 1 Loss: 0.0029294074695665265\n",
      "Iteration: 4016 lambda_k: 1 Loss: 0.002929407473789434\n",
      "Iteration: 4017 lambda_k: 1 Loss: 0.0029294074779904126\n",
      "Iteration: 4018 lambda_k: 1 Loss: 0.002929407482169568\n",
      "Iteration: 4019 lambda_k: 1 Loss: 0.002929407486327018\n",
      "Iteration: 4020 lambda_k: 1 Loss: 0.002929407490462876\n",
      "Iteration: 4021 lambda_k: 1 Loss: 0.0029294074945772513\n",
      "Iteration: 4022 lambda_k: 1 Loss: 0.0029294074986702555\n",
      "Iteration: 4023 lambda_k: 1 Loss: 0.0029294075027419936\n",
      "Iteration: 4024 lambda_k: 1 Loss: 0.0029294075067925933\n",
      "Iteration: 4025 lambda_k: 1 Loss: 0.002929407510822151\n",
      "Iteration: 4026 lambda_k: 1 Loss: 0.0029294075148307793\n",
      "Iteration: 4027 lambda_k: 1 Loss: 0.002929407518818599\n",
      "Iteration: 4028 lambda_k: 1 Loss: 0.002929407522785698\n",
      "Iteration: 4029 lambda_k: 1 Loss: 0.002929407526732192\n",
      "Iteration: 4030 lambda_k: 1 Loss: 0.002929407530658195\n",
      "Iteration: 4031 lambda_k: 1 Loss: 0.002929407534563804\n",
      "Iteration: 4032 lambda_k: 1 Loss: 0.0029294075384491344\n",
      "Iteration: 4033 lambda_k: 1 Loss: 0.0029294075423142853\n",
      "Iteration: 4034 lambda_k: 1 Loss: 0.0029294075461593575\n",
      "Iteration: 4035 lambda_k: 1 Loss: 0.002929407549984457\n",
      "Iteration: 4036 lambda_k: 1 Loss: 0.002929407553789703\n",
      "Iteration: 4037 lambda_k: 1 Loss: 0.002929407557575182\n",
      "Iteration: 4038 lambda_k: 1 Loss: 0.0029294075613410047\n",
      "Iteration: 4039 lambda_k: 1 Loss: 0.002929407565087265\n",
      "Iteration: 4040 lambda_k: 1 Loss: 0.0029294075688140768\n",
      "Iteration: 4041 lambda_k: 1 Loss: 0.0029294075725215182\n",
      "Iteration: 4042 lambda_k: 1 Loss: 0.002929407576209726\n",
      "Iteration: 4043 lambda_k: 1 Loss: 0.002929407579878767\n",
      "Iteration: 4044 lambda_k: 1 Loss: 0.0029294075835287676\n",
      "Iteration: 4045 lambda_k: 1 Loss: 0.0029294075871598027\n",
      "Iteration: 4046 lambda_k: 1 Loss: 0.0029294075907719884\n",
      "Iteration: 4047 lambda_k: 1 Loss: 0.0029294075943654173\n",
      "Iteration: 4048 lambda_k: 1 Loss: 0.002929407597940173\n",
      "Iteration: 4049 lambda_k: 1 Loss: 0.002929407601496367\n",
      "Iteration: 4050 lambda_k: 1 Loss: 0.002929407605034095\n",
      "Iteration: 4051 lambda_k: 1 Loss: 0.0029294076085534526\n",
      "Iteration: 4052 lambda_k: 1 Loss: 0.002929407612054547\n",
      "Iteration: 4053 lambda_k: 1 Loss: 0.002929407615537466\n",
      "Iteration: 4054 lambda_k: 1 Loss: 0.0029294076190022936\n",
      "Iteration: 4055 lambda_k: 1 Loss: 0.0029294076224491298\n",
      "Iteration: 4056 lambda_k: 1 Loss: 0.002929407625878072\n",
      "Iteration: 4057 lambda_k: 1 Loss: 0.0029294076292892043\n",
      "Iteration: 4058 lambda_k: 1 Loss: 0.00292940763268264\n",
      "Iteration: 4059 lambda_k: 1 Loss: 0.0029294076360584518\n",
      "Iteration: 4060 lambda_k: 1 Loss: 0.002929407639416724\n",
      "Iteration: 4061 lambda_k: 1 Loss: 0.002929407642757559\n",
      "Iteration: 4062 lambda_k: 1 Loss: 0.0029294076460810643\n",
      "Iteration: 4063 lambda_k: 1 Loss: 0.0029294076493873093\n",
      "Iteration: 4064 lambda_k: 1 Loss: 0.0029294076526763984\n",
      "Iteration: 4065 lambda_k: 1 Loss: 0.0029294076559483895\n",
      "Iteration: 4066 lambda_k: 1 Loss: 0.002929407659203403\n",
      "Iteration: 4067 lambda_k: 1 Loss: 0.002929407662441506\n",
      "Iteration: 4068 lambda_k: 1 Loss: 0.0029294076656628\n",
      "Iteration: 4069 lambda_k: 1 Loss: 0.002929407668867374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4070 lambda_k: 1 Loss: 0.0029294076720552996\n",
      "Iteration: 4071 lambda_k: 1 Loss: 0.0029294076752266786\n",
      "Iteration: 4072 lambda_k: 1 Loss: 0.002929407678381588\n",
      "Iteration: 4073 lambda_k: 1 Loss: 0.002929407681520132\n",
      "Iteration: 4074 lambda_k: 1 Loss: 0.002929407684642374\n",
      "Iteration: 4075 lambda_k: 1 Loss: 0.0029294076877484083\n",
      "Iteration: 4076 lambda_k: 1 Loss: 0.002929407690838327\n",
      "Iteration: 4077 lambda_k: 1 Loss: 0.002929407693912205\n",
      "Iteration: 4078 lambda_k: 1 Loss: 0.002929407696970121\n",
      "Iteration: 4079 lambda_k: 1 Loss: 0.0029294077000121595\n",
      "Iteration: 4080 lambda_k: 1 Loss: 0.0029294077030384003\n",
      "Iteration: 4081 lambda_k: 1 Loss: 0.0029294077060489283\n",
      "Iteration: 4082 lambda_k: 1 Loss: 0.0029294077090438464\n",
      "Iteration: 4083 lambda_k: 1 Loss: 0.0029294077120232145\n",
      "Iteration: 4084 lambda_k: 1 Loss: 0.0029294077149871135\n",
      "Iteration: 4085 lambda_k: 1 Loss: 0.0029294077179356365\n",
      "Iteration: 4086 lambda_k: 1 Loss: 0.0029294077208688388\n",
      "Iteration: 4087 lambda_k: 1 Loss: 0.002929407723786836\n",
      "Iteration: 4088 lambda_k: 1 Loss: 0.002929407726689673\n",
      "Iteration: 4089 lambda_k: 1 Loss: 0.0029294077295774507\n",
      "Iteration: 4090 lambda_k: 1 Loss: 0.002929407732450232\n",
      "Iteration: 4091 lambda_k: 1 Loss: 0.0029294077353081128\n",
      "Iteration: 4092 lambda_k: 1 Loss: 0.0029294077381511424\n",
      "Iteration: 4093 lambda_k: 1 Loss: 0.0029294077409794204\n",
      "Iteration: 4094 lambda_k: 1 Loss: 0.0029294077437930226\n",
      "Iteration: 4095 lambda_k: 1 Loss: 0.0029294077465920137\n",
      "Iteration: 4096 lambda_k: 1 Loss: 0.002929407749376472\n",
      "Iteration: 4097 lambda_k: 1 Loss: 0.0029294077521464772\n",
      "Iteration: 4098 lambda_k: 1 Loss: 0.0029294077549021063\n",
      "Iteration: 4099 lambda_k: 1 Loss: 0.0029294077576434304\n",
      "Iteration: 4100 lambda_k: 1 Loss: 0.002929407760370539\n",
      "Iteration: 4101 lambda_k: 1 Loss: 0.0029294077630834894\n",
      "Iteration: 4102 lambda_k: 1 Loss: 0.00292940776578236\n",
      "Iteration: 4103 lambda_k: 1 Loss: 0.0029294077684672236\n",
      "Iteration: 4104 lambda_k: 1 Loss: 0.0029294077711381544\n",
      "Iteration: 4105 lambda_k: 1 Loss: 0.002929407773795221\n",
      "Iteration: 4106 lambda_k: 1 Loss: 0.002929407776438494\n",
      "Iteration: 4107 lambda_k: 1 Loss: 0.002929407779068034\n",
      "Iteration: 4108 lambda_k: 1 Loss: 0.00292940778168395\n",
      "Iteration: 4109 lambda_k: 1 Loss: 0.0029294077842862832\n",
      "Iteration: 4110 lambda_k: 1 Loss: 0.002929407786875105\n",
      "Iteration: 4111 lambda_k: 1 Loss: 0.002929407789450504\n",
      "Iteration: 4112 lambda_k: 1 Loss: 0.002929407792012536\n",
      "Iteration: 4113 lambda_k: 1 Loss: 0.002929407794561258\n",
      "Iteration: 4114 lambda_k: 1 Loss: 0.0029294077970967625\n",
      "Iteration: 4115 lambda_k: 1 Loss: 0.002929407799619097\n",
      "Iteration: 4116 lambda_k: 1 Loss: 0.0029294078021283544\n",
      "Iteration: 4117 lambda_k: 1 Loss: 0.0029294078046245824\n",
      "Iteration: 4118 lambda_k: 1 Loss: 0.0029294078071078547\n",
      "Iteration: 4119 lambda_k: 1 Loss: 0.0029294078095782362\n",
      "Iteration: 4120 lambda_k: 1 Loss: 0.0029294078120358077\n",
      "Iteration: 4121 lambda_k: 1 Loss: 0.002929407814480632\n",
      "Iteration: 4122 lambda_k: 1 Loss: 0.0029294078169127746\n",
      "Iteration: 4123 lambda_k: 1 Loss: 0.0029294078193322767\n",
      "Iteration: 4124 lambda_k: 1 Loss: 0.00292940782173923\n",
      "Iteration: 4125 lambda_k: 1 Loss: 0.0029294078241337037\n",
      "Iteration: 4126 lambda_k: 1 Loss: 0.002929407826515748\n",
      "Iteration: 4127 lambda_k: 1 Loss: 0.0029294078288854422\n",
      "Iteration: 4128 lambda_k: 1 Loss: 0.0029294078312428226\n",
      "Iteration: 4129 lambda_k: 1 Loss: 0.0029294078335879645\n",
      "Iteration: 4130 lambda_k: 1 Loss: 0.002929407835920936\n",
      "Iteration: 4131 lambda_k: 1 Loss: 0.0029294078382418073\n",
      "Iteration: 4132 lambda_k: 1 Loss: 0.002929407840550661\n",
      "Iteration: 4133 lambda_k: 1 Loss: 0.0029294078428475255\n",
      "Iteration: 4134 lambda_k: 1 Loss: 0.002929407845132473\n",
      "Iteration: 4135 lambda_k: 1 Loss: 0.002929407847405568\n",
      "Iteration: 4136 lambda_k: 1 Loss: 0.002929407849666865\n",
      "Iteration: 4137 lambda_k: 1 Loss: 0.002929407851916432\n",
      "Iteration: 4138 lambda_k: 1 Loss: 0.0029294078541543186\n",
      "Iteration: 4139 lambda_k: 1 Loss: 0.0029294078563806107\n",
      "Iteration: 4140 lambda_k: 1 Loss: 0.002929407858595347\n",
      "Iteration: 4141 lambda_k: 1 Loss: 0.0029294078607985854\n",
      "Iteration: 4142 lambda_k: 1 Loss: 0.002929407862990382\n",
      "Iteration: 4143 lambda_k: 1 Loss: 0.002929407865170814\n",
      "Iteration: 4144 lambda_k: 1 Loss: 0.002929407867339933\n",
      "Iteration: 4145 lambda_k: 1 Loss: 0.002929407869497812\n",
      "Iteration: 4146 lambda_k: 1 Loss: 0.0029294078716444846\n",
      "Iteration: 4147 lambda_k: 1 Loss: 0.002929407873780009\n",
      "Iteration: 4148 lambda_k: 1 Loss: 0.0029294078759044646\n",
      "Iteration: 4149 lambda_k: 1 Loss: 0.002929407878017905\n",
      "Iteration: 4150 lambda_k: 1 Loss: 0.0029294078801203726\n",
      "Iteration: 4151 lambda_k: 1 Loss: 0.002929407882211946\n",
      "Iteration: 4152 lambda_k: 1 Loss: 0.002929407884292653\n",
      "Iteration: 4153 lambda_k: 1 Loss: 0.0029294078863625677\n",
      "Iteration: 4154 lambda_k: 1 Loss: 0.00292940788842176\n",
      "Iteration: 4155 lambda_k: 1 Loss: 0.002929407890470249\n",
      "Iteration: 4156 lambda_k: 1 Loss: 0.0029294078925081158\n",
      "Iteration: 4157 lambda_k: 1 Loss: 0.00292940789453541\n",
      "Iteration: 4158 lambda_k: 1 Loss: 0.0029294078965521964\n",
      "Iteration: 4159 lambda_k: 1 Loss: 0.00292940789855853\n",
      "Iteration: 4160 lambda_k: 1 Loss: 0.0029294079005544355\n",
      "Iteration: 4161 lambda_k: 1 Loss: 0.0029294079025399926\n",
      "Iteration: 4162 lambda_k: 1 Loss: 0.002929407904515258\n",
      "Iteration: 4163 lambda_k: 1 Loss: 0.0029294079064802725\n",
      "Iteration: 4164 lambda_k: 1 Loss: 0.002929407908435091\n",
      "Iteration: 4165 lambda_k: 1 Loss: 0.0029294079103797726\n",
      "Iteration: 4166 lambda_k: 1 Loss: 0.002929407912314363\n",
      "Iteration: 4167 lambda_k: 1 Loss: 0.0029294079142389287\n",
      "Iteration: 4168 lambda_k: 1 Loss: 0.002929407916153504\n",
      "Iteration: 4169 lambda_k: 1 Loss: 0.0029294079180581475\n",
      "Iteration: 4170 lambda_k: 1 Loss: 0.0029294079199529096\n",
      "Iteration: 4171 lambda_k: 1 Loss: 0.0029294079218378415\n",
      "Iteration: 4172 lambda_k: 1 Loss: 0.0029294079237129934\n",
      "Iteration: 4173 lambda_k: 1 Loss: 0.0029294079255784327\n",
      "Iteration: 4174 lambda_k: 1 Loss: 0.0029294079274341934\n",
      "Iteration: 4175 lambda_k: 1 Loss: 0.002929407929280327\n",
      "Iteration: 4176 lambda_k: 1 Loss: 0.0029294079311168756\n",
      "Iteration: 4177 lambda_k: 1 Loss: 0.0029294079329439107\n",
      "Iteration: 4178 lambda_k: 1 Loss: 0.0029294079347614676\n",
      "Iteration: 4179 lambda_k: 1 Loss: 0.002929407936569593\n",
      "Iteration: 4180 lambda_k: 1 Loss: 0.0029294079383683476\n",
      "Iteration: 4181 lambda_k: 1 Loss: 0.002929407940157774\n",
      "Iteration: 4182 lambda_k: 1 Loss: 0.0029294079419379076\n",
      "Iteration: 4183 lambda_k: 1 Loss: 0.0029294079437088187\n",
      "Iteration: 4184 lambda_k: 1 Loss: 0.0029294079454705462\n",
      "Iteration: 4185 lambda_k: 1 Loss: 0.00292940794722313\n",
      "Iteration: 4186 lambda_k: 1 Loss: 0.0029294079489666225\n",
      "Iteration: 4187 lambda_k: 1 Loss: 0.0029294079507010858\n",
      "Iteration: 4188 lambda_k: 1 Loss: 0.002929407952426542\n",
      "Iteration: 4189 lambda_k: 1 Loss: 0.002929407954143057\n",
      "Iteration: 4190 lambda_k: 1 Loss: 0.0029294079558506787\n",
      "Iteration: 4191 lambda_k: 1 Loss: 0.0029294079575494267\n",
      "Iteration: 4192 lambda_k: 1 Loss: 0.002929407959239384\n",
      "Iteration: 4193 lambda_k: 1 Loss: 0.0029294079609205608\n",
      "Iteration: 4194 lambda_k: 1 Loss: 0.002929407962593024\n",
      "Iteration: 4195 lambda_k: 1 Loss: 0.002929407964256823\n",
      "Iteration: 4196 lambda_k: 1 Loss: 0.0029294079659119765\n",
      "Iteration: 4197 lambda_k: 1 Loss: 0.0029294079675585465\n",
      "Iteration: 4198 lambda_k: 1 Loss: 0.002929407969196584\n",
      "Iteration: 4199 lambda_k: 1 Loss: 0.002929407970826138\n",
      "Iteration: 4200 lambda_k: 1 Loss: 0.0029294079724472457\n",
      "Iteration: 4201 lambda_k: 1 Loss: 0.002929407974059927\n",
      "Iteration: 4202 lambda_k: 1 Loss: 0.0029294079756642475\n",
      "Iteration: 4203 lambda_k: 1 Loss: 0.0029294079772602473\n",
      "Iteration: 4204 lambda_k: 1 Loss: 0.0029294079788479742\n",
      "Iteration: 4205 lambda_k: 1 Loss: 0.0029294079804274764\n",
      "Iteration: 4206 lambda_k: 1 Loss: 0.0029294079819987837\n",
      "Iteration: 4207 lambda_k: 1 Loss: 0.002929407983561944\n",
      "Iteration: 4208 lambda_k: 1 Loss: 0.002929407985117011\n",
      "Iteration: 4209 lambda_k: 1 Loss: 0.002929407986664002\n",
      "Iteration: 4210 lambda_k: 1 Loss: 0.0029294079882029706\n",
      "Iteration: 4211 lambda_k: 1 Loss: 0.0029294079897339573\n",
      "Iteration: 4212 lambda_k: 1 Loss: 0.0029294079912570107\n",
      "Iteration: 4213 lambda_k: 1 Loss: 0.002929407992772166\n",
      "Iteration: 4214 lambda_k: 1 Loss: 0.0029294079942794628\n",
      "Iteration: 4215 lambda_k: 1 Loss: 0.0029294079957789365\n",
      "Iteration: 4216 lambda_k: 1 Loss: 0.0029294079972706534\n",
      "Iteration: 4217 lambda_k: 1 Loss: 0.0029294079987546378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4218 lambda_k: 1 Loss: 0.0029294080002309174\n",
      "Iteration: 4219 lambda_k: 1 Loss: 0.002929408001699555\n",
      "Iteration: 4220 lambda_k: 1 Loss: 0.0029294080031605557\n",
      "Iteration: 4221 lambda_k: 1 Loss: 0.0029294080046139994\n",
      "Iteration: 4222 lambda_k: 1 Loss: 0.002929408006059907\n",
      "Iteration: 4223 lambda_k: 1 Loss: 0.0029294080074983115\n",
      "Iteration: 4224 lambda_k: 1 Loss: 0.002929408008929249\n",
      "Iteration: 4225 lambda_k: 1 Loss: 0.0029294080103527825\n",
      "Iteration: 4226 lambda_k: 1 Loss: 0.002929408011768932\n",
      "Iteration: 4227 lambda_k: 1 Loss: 0.0029294080131777255\n",
      "Iteration: 4228 lambda_k: 1 Loss: 0.002929408014579232\n",
      "Iteration: 4229 lambda_k: 1 Loss: 0.002929408015973469\n",
      "Iteration: 4230 lambda_k: 1 Loss: 0.0029294080173604916\n",
      "Iteration: 4231 lambda_k: 1 Loss: 0.0029294080187403067\n",
      "Iteration: 4232 lambda_k: 1 Loss: 0.0029294080201129795\n",
      "Iteration: 4233 lambda_k: 1 Loss: 0.002929408021478527\n",
      "Iteration: 4234 lambda_k: 1 Loss: 0.0029294080228369837\n",
      "Iteration: 4235 lambda_k: 1 Loss: 0.002929408024188414\n",
      "Iteration: 4236 lambda_k: 1 Loss: 0.002929408025532846\n",
      "Iteration: 4237 lambda_k: 1 Loss: 0.0029294080268702986\n",
      "Iteration: 4238 lambda_k: 1 Loss: 0.002929408028200825\n",
      "Iteration: 4239 lambda_k: 1 Loss: 0.002929408029524445\n",
      "Iteration: 4240 lambda_k: 1 Loss: 0.0029294080308412103\n",
      "Iteration: 4241 lambda_k: 1 Loss: 0.0029294080321511386\n",
      "Iteration: 4242 lambda_k: 1 Loss: 0.0029294080334542733\n",
      "Iteration: 4243 lambda_k: 1 Loss: 0.002929408034750666\n",
      "Iteration: 4244 lambda_k: 1 Loss: 0.0029294080360403223\n",
      "Iteration: 4245 lambda_k: 1 Loss: 0.0029294080373233004\n",
      "Iteration: 4246 lambda_k: 1 Loss: 0.0029294080385996344\n",
      "Iteration: 4247 lambda_k: 1 Loss: 0.0029294080398693527\n",
      "Iteration: 4248 lambda_k: 1 Loss: 0.0029294080411324786\n",
      "Iteration: 4249 lambda_k: 1 Loss: 0.002929408042389064\n",
      "Iteration: 4250 lambda_k: 1 Loss: 0.0029294080436391406\n",
      "Iteration: 4251 lambda_k: 1 Loss: 0.0029294080448827404\n",
      "Iteration: 4252 lambda_k: 1 Loss: 0.0029294080461198793\n",
      "Iteration: 4253 lambda_k: 1 Loss: 0.0029294080473506044\n",
      "Iteration: 4254 lambda_k: 1 Loss: 0.0029294080485749593\n",
      "Iteration: 4255 lambda_k: 1 Loss: 0.0029294080497929676\n",
      "Iteration: 4256 lambda_k: 1 Loss: 0.002929408051004672\n",
      "Iteration: 4257 lambda_k: 1 Loss: 0.0029294080522100983\n",
      "Iteration: 4258 lambda_k: 1 Loss: 0.0029294080534092676\n",
      "Iteration: 4259 lambda_k: 1 Loss: 0.00292940805460221\n",
      "Iteration: 4260 lambda_k: 1 Loss: 0.002929408055788973\n",
      "Iteration: 4261 lambda_k: 1 Loss: 0.002929408056969589\n",
      "Iteration: 4262 lambda_k: 1 Loss: 0.0029294080581440843\n",
      "Iteration: 4263 lambda_k: 1 Loss: 0.002929408059312499\n",
      "Iteration: 4264 lambda_k: 1 Loss: 0.002929408060474843\n",
      "Iteration: 4265 lambda_k: 1 Loss: 0.002929408061631166\n",
      "Iteration: 4266 lambda_k: 1 Loss: 0.002929408062781509\n",
      "Iteration: 4267 lambda_k: 1 Loss: 0.0029294080639258938\n",
      "Iteration: 4268 lambda_k: 1 Loss: 0.002929408065064354\n",
      "Iteration: 4269 lambda_k: 1 Loss: 0.0029294080661969027\n",
      "Iteration: 4270 lambda_k: 1 Loss: 0.0029294080673235904\n",
      "Iteration: 4271 lambda_k: 1 Loss: 0.0029294080684444373\n",
      "Iteration: 4272 lambda_k: 1 Loss: 0.002929408069559468\n",
      "Iteration: 4273 lambda_k: 1 Loss: 0.0029294080706687158\n",
      "Iteration: 4274 lambda_k: 1 Loss: 0.002929408071772216\n",
      "Iteration: 4275 lambda_k: 1 Loss: 0.0029294080728699897\n",
      "Iteration: 4276 lambda_k: 1 Loss: 0.002929408073962093\n",
      "Iteration: 4277 lambda_k: 1 Loss: 0.0029294080750485335\n",
      "Iteration: 4278 lambda_k: 1 Loss: 0.0029294080761293386\n",
      "Iteration: 4279 lambda_k: 1 Loss: 0.002929408077204536\n",
      "Iteration: 4280 lambda_k: 1 Loss: 0.0029294080782741785\n",
      "Iteration: 4281 lambda_k: 1 Loss: 0.0029294080793382765\n",
      "Iteration: 4282 lambda_k: 1 Loss: 0.002929408080396852\n",
      "Iteration: 4283 lambda_k: 1 Loss: 0.002929408081449944\n",
      "Iteration: 4284 lambda_k: 1 Loss: 0.0029294080824975805\n",
      "Iteration: 4285 lambda_k: 1 Loss: 0.00292940808353978\n",
      "Iteration: 4286 lambda_k: 1 Loss: 0.002929408084576572\n",
      "Iteration: 4287 lambda_k: 1 Loss: 0.002929408085608017\n",
      "Iteration: 4288 lambda_k: 1 Loss: 0.0029294080866341145\n",
      "Iteration: 4289 lambda_k: 1 Loss: 0.0029294080876548795\n",
      "Iteration: 4290 lambda_k: 1 Loss: 0.0029294080886703663\n",
      "Iteration: 4291 lambda_k: 1 Loss: 0.002929408089680577\n",
      "Iteration: 4292 lambda_k: 1 Loss: 0.002929408090685546\n",
      "Iteration: 4293 lambda_k: 1 Loss: 0.0029294080916853343\n",
      "Iteration: 4294 lambda_k: 1 Loss: 0.0029294080926799224\n",
      "Iteration: 4295 lambda_k: 1 Loss: 0.002929408093669374\n",
      "Iteration: 4296 lambda_k: 1 Loss: 0.002929408094653678\n",
      "Iteration: 4297 lambda_k: 1 Loss: 0.0029294080956328936\n",
      "Iteration: 4298 lambda_k: 1 Loss: 0.00292940809660702\n",
      "Iteration: 4299 lambda_k: 1 Loss: 0.002929408097576116\n",
      "Iteration: 4300 lambda_k: 1 Loss: 0.0029294080985401787\n",
      "Iteration: 4301 lambda_k: 1 Loss: 0.0029294080994992614\n",
      "Iteration: 4302 lambda_k: 1 Loss: 0.002929408100453359\n",
      "Iteration: 4303 lambda_k: 1 Loss: 0.0029294081014025245\n",
      "Iteration: 4304 lambda_k: 1 Loss: 0.002929408102346753\n",
      "Iteration: 4305 lambda_k: 1 Loss: 0.0029294081032861102\n",
      "Iteration: 4306 lambda_k: 1 Loss: 0.0029294081042206027\n",
      "Iteration: 4307 lambda_k: 1 Loss: 0.0029294081051502415\n",
      "Iteration: 4308 lambda_k: 1 Loss: 0.00292940810607508\n",
      "Iteration: 4309 lambda_k: 1 Loss: 0.002929408106995102\n",
      "Iteration: 4310 lambda_k: 1 Loss: 0.002929408107910374\n",
      "Iteration: 4311 lambda_k: 1 Loss: 0.0029294081088209138\n",
      "Iteration: 4312 lambda_k: 1 Loss: 0.0029294081097267126\n",
      "Iteration: 4313 lambda_k: 1 Loss: 0.0029294081106278225\n",
      "Iteration: 4314 lambda_k: 1 Loss: 0.0029294081115242778\n",
      "Iteration: 4315 lambda_k: 1 Loss: 0.0029294081124160705\n",
      "Iteration: 4316 lambda_k: 1 Loss: 0.0029294081133032454\n",
      "Iteration: 4317 lambda_k: 1 Loss: 0.0029294081141858323\n",
      "Iteration: 4318 lambda_k: 1 Loss: 0.0029294081150638552\n",
      "Iteration: 4319 lambda_k: 1 Loss: 0.0029294081159373145\n",
      "Iteration: 4320 lambda_k: 1 Loss: 0.0029294081168062466\n",
      "Iteration: 4321 lambda_k: 1 Loss: 0.0029294081176706793\n",
      "Iteration: 4322 lambda_k: 1 Loss: 0.00292940811853065\n",
      "Iteration: 4323 lambda_k: 1 Loss: 0.0029294081193861456\n",
      "Iteration: 4324 lambda_k: 1 Loss: 0.002929408120237213\n",
      "Iteration: 4325 lambda_k: 1 Loss: 0.002929408121083861\n",
      "Iteration: 4326 lambda_k: 1 Loss: 0.002929408121926133\n",
      "Iteration: 4327 lambda_k: 1 Loss: 0.0029294081227640486\n",
      "Iteration: 4328 lambda_k: 1 Loss: 0.002929408123597631\n",
      "Iteration: 4329 lambda_k: 1 Loss: 0.0029294081244268977\n",
      "Iteration: 4330 lambda_k: 1 Loss: 0.0029294081252518485\n",
      "Iteration: 4331 lambda_k: 1 Loss: 0.0029294081260725284\n",
      "Iteration: 4332 lambda_k: 1 Loss: 0.0029294081268889612\n",
      "Iteration: 4333 lambda_k: 1 Loss: 0.0029294081277011627\n",
      "Iteration: 4334 lambda_k: 1 Loss: 0.002929408128509159\n",
      "Iteration: 4335 lambda_k: 1 Loss: 0.0029294081293129723\n",
      "Iteration: 4336 lambda_k: 1 Loss: 0.002929408130112629\n",
      "Iteration: 4337 lambda_k: 1 Loss: 0.0029294081309081434\n",
      "Iteration: 4338 lambda_k: 1 Loss: 0.0029294081316995333\n",
      "Iteration: 4339 lambda_k: 1 Loss: 0.002929408132486827\n",
      "Iteration: 4340 lambda_k: 1 Loss: 0.002929408133270046\n",
      "Iteration: 4341 lambda_k: 1 Loss: 0.002929408134049218\n",
      "Iteration: 4342 lambda_k: 1 Loss: 0.0029294081348243497\n",
      "Iteration: 4343 lambda_k: 1 Loss: 0.002929408135595461\n",
      "Iteration: 4344 lambda_k: 1 Loss: 0.0029294081363625797\n",
      "Iteration: 4345 lambda_k: 1 Loss: 0.0029294081371257267\n",
      "Iteration: 4346 lambda_k: 1 Loss: 0.0029294081378849154\n",
      "Iteration: 4347 lambda_k: 1 Loss: 0.00292940813864017\n",
      "Iteration: 4348 lambda_k: 1 Loss: 0.0029294081393915193\n",
      "Iteration: 4349 lambda_k: 1 Loss: 0.0029294081401389812\n",
      "Iteration: 4350 lambda_k: 1 Loss: 0.0029294081408825622\n",
      "Iteration: 4351 lambda_k: 1 Loss: 0.002929408141622286\n",
      "Iteration: 4352 lambda_k: 1 Loss: 0.0029294081423581714\n",
      "Iteration: 4353 lambda_k: 1 Loss: 0.0029294081430902463\n",
      "Iteration: 4354 lambda_k: 1 Loss: 0.0029294081438185496\n",
      "Iteration: 4355 lambda_k: 1 Loss: 0.0029294081445430803\n",
      "Iteration: 4356 lambda_k: 1 Loss: 0.0029294081452638544\n",
      "Iteration: 4357 lambda_k: 1 Loss: 0.002929408145980895\n",
      "Iteration: 4358 lambda_k: 1 Loss: 0.002929408146694213\n",
      "Iteration: 4359 lambda_k: 1 Loss: 0.002929408147403835\n",
      "Iteration: 4360 lambda_k: 1 Loss: 0.0029294081481097975\n",
      "Iteration: 4361 lambda_k: 1 Loss: 0.0029294081488121042\n",
      "Iteration: 4362 lambda_k: 1 Loss: 0.0029294081495107567\n",
      "Iteration: 4363 lambda_k: 1 Loss: 0.002929408150205795\n",
      "Iteration: 4364 lambda_k: 1 Loss: 0.0029294081508972293\n",
      "Iteration: 4365 lambda_k: 1 Loss: 0.0029294081515850974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4366 lambda_k: 1 Loss: 0.0029294081522694007\n",
      "Iteration: 4367 lambda_k: 1 Loss: 0.0029294081529501535\n",
      "Iteration: 4368 lambda_k: 1 Loss: 0.0029294081536273726\n",
      "Iteration: 4369 lambda_k: 1 Loss: 0.0029294081543011124\n",
      "Iteration: 4370 lambda_k: 1 Loss: 0.002929408154971351\n",
      "Iteration: 4371 lambda_k: 1 Loss: 0.002929408155638109\n",
      "Iteration: 4372 lambda_k: 1 Loss: 0.002929408156301413\n",
      "Iteration: 4373 lambda_k: 1 Loss: 0.002929408156961293\n",
      "Iteration: 4374 lambda_k: 1 Loss: 0.0029294081576177492\n",
      "Iteration: 4375 lambda_k: 1 Loss: 0.002929408158270792\n",
      "Iteration: 4376 lambda_k: 1 Loss: 0.0029294081589204732\n",
      "Iteration: 4377 lambda_k: 1 Loss: 0.002929408159566783\n",
      "Iteration: 4378 lambda_k: 1 Loss: 0.002929408160209735\n",
      "Iteration: 4379 lambda_k: 1 Loss: 0.002929408160849373\n",
      "Iteration: 4380 lambda_k: 1 Loss: 0.002929408161485692\n",
      "Iteration: 4381 lambda_k: 1 Loss: 0.0029294081621187257\n",
      "Iteration: 4382 lambda_k: 1 Loss: 0.0029294081627484767\n",
      "Iteration: 4383 lambda_k: 1 Loss: 0.002929408163374974\n",
      "Iteration: 4384 lambda_k: 1 Loss: 0.0029294081639982266\n",
      "Iteration: 4385 lambda_k: 1 Loss: 0.002929408164618248\n",
      "Iteration: 4386 lambda_k: 1 Loss: 0.0029294081652350705\n",
      "Iteration: 4387 lambda_k: 1 Loss: 0.0029294081658486877\n",
      "Iteration: 4388 lambda_k: 1 Loss: 0.002929408166459122\n",
      "Iteration: 4389 lambda_k: 1 Loss: 0.002929408167066414\n",
      "Iteration: 4390 lambda_k: 1 Loss: 0.002929408167670551\n",
      "Iteration: 4391 lambda_k: 1 Loss: 0.0029294081682715704\n",
      "Iteration: 4392 lambda_k: 1 Loss: 0.002929408168869461\n",
      "Iteration: 4393 lambda_k: 1 Loss: 0.0029294081694642747\n",
      "Iteration: 4394 lambda_k: 1 Loss: 0.0029294081700559907\n",
      "Iteration: 4395 lambda_k: 1 Loss: 0.002929408170644632\n",
      "Iteration: 4396 lambda_k: 1 Loss: 0.0029294081712302475\n",
      "Iteration: 4397 lambda_k: 1 Loss: 0.0029294081718128327\n",
      "Iteration: 4398 lambda_k: 1 Loss: 0.0029294081723923856\n",
      "Iteration: 4399 lambda_k: 1 Loss: 0.0029294081729689305\n",
      "Iteration: 4400 lambda_k: 1 Loss: 0.0029294081735425016\n",
      "Iteration: 4401 lambda_k: 1 Loss: 0.002929408174113115\n",
      "Iteration: 4402 lambda_k: 1 Loss: 0.002929408174680765\n",
      "Iteration: 4403 lambda_k: 1 Loss: 0.0029294081752454905\n",
      "Iteration: 4404 lambda_k: 1 Loss: 0.0029294081758072734\n",
      "Iteration: 4405 lambda_k: 1 Loss: 0.0029294081763661536\n",
      "Iteration: 4406 lambda_k: 1 Loss: 0.0029294081769221285\n",
      "Iteration: 4407 lambda_k: 1 Loss: 0.0029294081774752317\n",
      "Iteration: 4408 lambda_k: 1 Loss: 0.002929408178025482\n",
      "Iteration: 4409 lambda_k: 1 Loss: 0.0029294081785728693\n",
      "Iteration: 4410 lambda_k: 1 Loss: 0.002929408179117438\n",
      "Iteration: 4411 lambda_k: 1 Loss: 0.0029294081796591817\n",
      "Iteration: 4412 lambda_k: 1 Loss: 0.002929408180198121\n",
      "Iteration: 4413 lambda_k: 1 Loss: 0.002929408180734281\n",
      "Iteration: 4414 lambda_k: 1 Loss: 0.002929408181267645\n",
      "Iteration: 4415 lambda_k: 1 Loss: 0.002929408181798257\n",
      "Iteration: 4416 lambda_k: 1 Loss: 0.0029294081823261087\n",
      "Iteration: 4417 lambda_k: 1 Loss: 0.0029294081828512416\n",
      "Iteration: 4418 lambda_k: 1 Loss: 0.002929408183373657\n",
      "Iteration: 4419 lambda_k: 1 Loss: 0.002929408183893359\n",
      "Iteration: 4420 lambda_k: 1 Loss: 0.0029294081844103664\n",
      "Iteration: 4421 lambda_k: 1 Loss: 0.0029294081849247093\n",
      "Iteration: 4422 lambda_k: 1 Loss: 0.00292940818543637\n",
      "Iteration: 4423 lambda_k: 1 Loss: 0.002929408185945395\n",
      "Iteration: 4424 lambda_k: 1 Loss: 0.002929408186451788\n",
      "Iteration: 4425 lambda_k: 1 Loss: 0.002929408186955564\n",
      "Iteration: 4426 lambda_k: 1 Loss: 0.0029294081874567156\n",
      "Iteration: 4427 lambda_k: 1 Loss: 0.0029294081879552803\n",
      "Iteration: 4428 lambda_k: 1 Loss: 0.002929408188451278\n",
      "Iteration: 4429 lambda_k: 1 Loss: 0.0029294081889447003\n",
      "Iteration: 4430 lambda_k: 1 Loss: 0.002929408189435563\n",
      "Iteration: 4431 lambda_k: 1 Loss: 0.0029294081899238857\n",
      "Iteration: 4432 lambda_k: 1 Loss: 0.0029294081904096872\n",
      "Iteration: 4433 lambda_k: 1 Loss: 0.0029294081908929604\n",
      "Iteration: 4434 lambda_k: 1 Loss: 0.0029294081913737386\n",
      "Iteration: 4435 lambda_k: 1 Loss: 0.0029294081918520287\n",
      "Iteration: 4436 lambda_k: 1 Loss: 0.002929408192327851\n",
      "Iteration: 4437 lambda_k: 1 Loss: 0.0029294081928012148\n",
      "Iteration: 4438 lambda_k: 1 Loss: 0.0029294081932721133\n",
      "Iteration: 4439 lambda_k: 1 Loss: 0.002929408193740565\n",
      "Iteration: 4440 lambda_k: 1 Loss: 0.002929408194206606\n",
      "Iteration: 4441 lambda_k: 1 Loss: 0.0029294081946702475\n",
      "Iteration: 4442 lambda_k: 1 Loss: 0.002929408195131494\n",
      "Iteration: 4443 lambda_k: 1 Loss: 0.002929408195590334\n",
      "Iteration: 4444 lambda_k: 1 Loss: 0.0029294081960468004\n",
      "Iteration: 4445 lambda_k: 1 Loss: 0.0029294081965009\n",
      "Iteration: 4446 lambda_k: 1 Loss: 0.0029294081969526565\n",
      "Iteration: 4447 lambda_k: 1 Loss: 0.0029294081974020796\n",
      "Iteration: 4448 lambda_k: 1 Loss: 0.002929408197849184\n",
      "Iteration: 4449 lambda_k: 1 Loss: 0.0029294081982939512\n",
      "Iteration: 4450 lambda_k: 1 Loss: 0.002929408198736423\n",
      "Iteration: 4451 lambda_k: 1 Loss: 0.0029294081991766115\n",
      "Iteration: 4452 lambda_k: 1 Loss: 0.0029294081996145186\n",
      "Iteration: 4453 lambda_k: 1 Loss: 0.00292940820005017\n",
      "Iteration: 4454 lambda_k: 1 Loss: 0.0029294082004835643\n",
      "Iteration: 4455 lambda_k: 1 Loss: 0.0029294082009147177\n",
      "Iteration: 4456 lambda_k: 1 Loss: 0.0029294082013436424\n",
      "Iteration: 4457 lambda_k: 1 Loss: 0.0029294082017703262\n",
      "Iteration: 4458 lambda_k: 1 Loss: 0.0029294082021948057\n",
      "Iteration: 4459 lambda_k: 1 Loss: 0.002929408202617086\n",
      "Iteration: 4460 lambda_k: 1 Loss: 0.0029294082030371714\n",
      "Iteration: 4461 lambda_k: 1 Loss: 0.002929408203455101\n",
      "Iteration: 4462 lambda_k: 1 Loss: 0.0029294082038708695\n",
      "Iteration: 4463 lambda_k: 1 Loss: 0.00292940820428448\n",
      "Iteration: 4464 lambda_k: 1 Loss: 0.002929408204695946\n",
      "Iteration: 4465 lambda_k: 1 Loss: 0.002929408205105287\n",
      "Iteration: 4466 lambda_k: 1 Loss: 0.0029294082055125077\n",
      "Iteration: 4467 lambda_k: 1 Loss: 0.0029294082059176272\n",
      "Iteration: 4468 lambda_k: 1 Loss: 0.002929408206320653\n",
      "Iteration: 4469 lambda_k: 1 Loss: 0.002929408206721599\n",
      "Iteration: 4470 lambda_k: 1 Loss: 0.002929408207120479\n",
      "Iteration: 4471 lambda_k: 1 Loss: 0.0029294082075172886\n",
      "Iteration: 4472 lambda_k: 1 Loss: 0.0029294082079120453\n",
      "Iteration: 4473 lambda_k: 1 Loss: 0.002929408208304754\n",
      "Iteration: 4474 lambda_k: 1 Loss: 0.0029294082086954378\n",
      "Iteration: 4475 lambda_k: 1 Loss: 0.0029294082090841004\n",
      "Iteration: 4476 lambda_k: 1 Loss: 0.0029294082094707538\n",
      "Iteration: 4477 lambda_k: 1 Loss: 0.0029294082098554075\n",
      "Iteration: 4478 lambda_k: 1 Loss: 0.002929408210238065\n",
      "Iteration: 4479 lambda_k: 1 Loss: 0.002929408210618744\n",
      "Iteration: 4480 lambda_k: 1 Loss: 0.002929408210997452\n",
      "Iteration: 4481 lambda_k: 1 Loss: 0.0029294082113742047\n",
      "Iteration: 4482 lambda_k: 1 Loss: 0.002929408211749009\n",
      "Iteration: 4483 lambda_k: 1 Loss: 0.002929408212121879\n",
      "Iteration: 4484 lambda_k: 1 Loss: 0.0029294082124928047\n",
      "Iteration: 4485 lambda_k: 1 Loss: 0.0029294082128618086\n",
      "Iteration: 4486 lambda_k: 1 Loss: 0.002929408213228904\n",
      "Iteration: 4487 lambda_k: 1 Loss: 0.002929408213594106\n",
      "Iteration: 4488 lambda_k: 1 Loss: 0.0029294082139574113\n",
      "Iteration: 4489 lambda_k: 1 Loss: 0.0029294082143188297\n",
      "Iteration: 4490 lambda_k: 1 Loss: 0.0029294082146783754\n",
      "Iteration: 4491 lambda_k: 1 Loss: 0.0029294082150360533\n",
      "Iteration: 4492 lambda_k: 1 Loss: 0.002929408215391883\n",
      "Iteration: 4493 lambda_k: 1 Loss: 0.0029294082157458887\n",
      "Iteration: 4494 lambda_k: 1 Loss: 0.00292940821609806\n",
      "Iteration: 4495 lambda_k: 1 Loss: 0.0029294082164484127\n",
      "Iteration: 4496 lambda_k: 1 Loss: 0.0029294082167969538\n",
      "Iteration: 4497 lambda_k: 1 Loss: 0.00292940821714369\n",
      "Iteration: 4498 lambda_k: 1 Loss: 0.002929408217488626\n",
      "Iteration: 4499 lambda_k: 1 Loss: 0.002929408217831778\n",
      "Iteration: 4500 lambda_k: 1 Loss: 0.0029294082181731463\n",
      "Iteration: 4501 lambda_k: 1 Loss: 0.002929408218512764\n",
      "Iteration: 4502 lambda_k: 1 Loss: 0.0029294082188506313\n",
      "Iteration: 4503 lambda_k: 1 Loss: 0.0029294082191867465\n",
      "Iteration: 4504 lambda_k: 1 Loss: 0.0029294082195211184\n",
      "Iteration: 4505 lambda_k: 1 Loss: 0.002929408219853754\n",
      "Iteration: 4506 lambda_k: 1 Loss: 0.002929408220184673\n",
      "Iteration: 4507 lambda_k: 1 Loss: 0.0029294082205138685\n",
      "Iteration: 4508 lambda_k: 1 Loss: 0.0029294082208413544\n",
      "Iteration: 4509 lambda_k: 1 Loss: 0.002929408221167166\n",
      "Iteration: 4510 lambda_k: 1 Loss: 0.002929408221491291\n",
      "Iteration: 4511 lambda_k: 1 Loss: 0.00292940822181374\n",
      "Iteration: 4512 lambda_k: 1 Loss: 0.002929408222134513\n",
      "Iteration: 4513 lambda_k: 1 Loss: 0.0029294082224536272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4514 lambda_k: 1 Loss: 0.002929408222771079\n",
      "Iteration: 4515 lambda_k: 1 Loss: 0.002929408223086901\n",
      "Iteration: 4516 lambda_k: 1 Loss: 0.0029294082234010838\n",
      "Iteration: 4517 lambda_k: 1 Loss: 0.0029294082237136467\n",
      "Iteration: 4518 lambda_k: 1 Loss: 0.0029294082240245846\n",
      "Iteration: 4519 lambda_k: 1 Loss: 0.002929408224333908\n",
      "Iteration: 4520 lambda_k: 1 Loss: 0.002929408224641647\n",
      "Iteration: 4521 lambda_k: 1 Loss: 0.0029294082249477975\n",
      "Iteration: 4522 lambda_k: 1 Loss: 0.0029294082252523538\n",
      "Iteration: 4523 lambda_k: 1 Loss: 0.0029294082255553193\n",
      "Iteration: 4524 lambda_k: 1 Loss: 0.002929408225856746\n",
      "Iteration: 4525 lambda_k: 1 Loss: 0.002929408226156607\n",
      "Iteration: 4526 lambda_k: 1 Loss: 0.002929408226454909\n",
      "Iteration: 4527 lambda_k: 1 Loss: 0.0029294082267516623\n",
      "Iteration: 4528 lambda_k: 1 Loss: 0.002929408227046895\n",
      "Iteration: 4529 lambda_k: 1 Loss: 0.002929408227340605\n",
      "Iteration: 4530 lambda_k: 1 Loss: 0.00292940822763279\n",
      "Iteration: 4531 lambda_k: 1 Loss: 0.0029294082279234528\n",
      "Iteration: 4532 lambda_k: 1 Loss: 0.0029294082282126286\n",
      "Iteration: 4533 lambda_k: 1 Loss: 0.0029294082285003095\n",
      "Iteration: 4534 lambda_k: 1 Loss: 0.0029294082287865003\n",
      "Iteration: 4535 lambda_k: 1 Loss: 0.002929408229071202\n",
      "Iteration: 4536 lambda_k: 1 Loss: 0.0029294082293544487\n",
      "Iteration: 4537 lambda_k: 1 Loss: 0.002929408229636224\n",
      "Iteration: 4538 lambda_k: 1 Loss: 0.0029294082299165317\n",
      "Iteration: 4539 lambda_k: 1 Loss: 0.0029294082301954006\n",
      "Iteration: 4540 lambda_k: 1 Loss: 0.00292940823047282\n",
      "Iteration: 4541 lambda_k: 1 Loss: 0.0029294082307488004\n",
      "Iteration: 4542 lambda_k: 1 Loss: 0.0029294082310233447\n",
      "Iteration: 4543 lambda_k: 1 Loss: 0.0029294082312964864\n",
      "Iteration: 4544 lambda_k: 1 Loss: 0.002929408231568215\n",
      "Iteration: 4545 lambda_k: 1 Loss: 0.00292940823183853\n",
      "Iteration: 4546 lambda_k: 1 Loss: 0.0029294082321074614\n",
      "Iteration: 4547 lambda_k: 1 Loss: 0.002929408232374995\n",
      "Iteration: 4548 lambda_k: 1 Loss: 0.0029294082326411365\n",
      "Iteration: 4549 lambda_k: 1 Loss: 0.002929408232905893\n",
      "Iteration: 4550 lambda_k: 1 Loss: 0.0029294082331692978\n",
      "Iteration: 4551 lambda_k: 1 Loss: 0.0029294082334313408\n",
      "Iteration: 4552 lambda_k: 1 Loss: 0.002929408233692008\n",
      "Iteration: 4553 lambda_k: 1 Loss: 0.0029294082339513484\n",
      "Iteration: 4554 lambda_k: 1 Loss: 0.002929408234209335\n",
      "Iteration: 4555 lambda_k: 1 Loss: 0.0029294082344660023\n",
      "Iteration: 4556 lambda_k: 1 Loss: 0.0029294082347213254\n",
      "Iteration: 4557 lambda_k: 1 Loss: 0.002929408234975333\n",
      "Iteration: 4558 lambda_k: 1 Loss: 0.002929408235228035\n",
      "Iteration: 4559 lambda_k: 1 Loss: 0.002929408235479424\n",
      "Iteration: 4560 lambda_k: 1 Loss: 0.0029294082357295068\n",
      "Iteration: 4561 lambda_k: 1 Loss: 0.002929408235978314\n",
      "Iteration: 4562 lambda_k: 1 Loss: 0.0029294082362258263\n",
      "Iteration: 4563 lambda_k: 1 Loss: 0.002929408236472051\n",
      "Iteration: 4564 lambda_k: 1 Loss: 0.0029294082367170146\n",
      "Iteration: 4565 lambda_k: 1 Loss: 0.0029294082369607055\n",
      "Iteration: 4566 lambda_k: 1 Loss: 0.0029294082372031314\n",
      "Iteration: 4567 lambda_k: 1 Loss: 0.0029294082374443104\n",
      "Iteration: 4568 lambda_k: 1 Loss: 0.002929408237684239\n",
      "Iteration: 4569 lambda_k: 1 Loss: 0.002929408237922936\n",
      "Iteration: 4570 lambda_k: 1 Loss: 0.0029294082381603956\n",
      "Iteration: 4571 lambda_k: 1 Loss: 0.0029294082383966133\n",
      "Iteration: 4572 lambda_k: 1 Loss: 0.002929408238631621\n",
      "Iteration: 4573 lambda_k: 1 Loss: 0.0029294082388654085\n",
      "Iteration: 4574 lambda_k: 1 Loss: 0.0029294082390979945\n",
      "Iteration: 4575 lambda_k: 1 Loss: 0.0029294082393293694\n",
      "Iteration: 4576 lambda_k: 1 Loss: 0.002929408239559532\n",
      "Iteration: 4577 lambda_k: 1 Loss: 0.0029294082397885246\n",
      "Iteration: 4578 lambda_k: 1 Loss: 0.0029294082400163216\n",
      "Iteration: 4579 lambda_k: 1 Loss: 0.0029294082402429515\n",
      "Iteration: 4580 lambda_k: 1 Loss: 0.0029294082404684014\n",
      "Iteration: 4581 lambda_k: 1 Loss: 0.0029294082406926747\n",
      "Iteration: 4582 lambda_k: 1 Loss: 0.002929408240915805\n",
      "Iteration: 4583 lambda_k: 1 Loss: 0.002929408241137768\n",
      "Iteration: 4584 lambda_k: 1 Loss: 0.0029294082413585886\n",
      "Iteration: 4585 lambda_k: 1 Loss: 0.0029294082415782644\n",
      "Iteration: 4586 lambda_k: 1 Loss: 0.0029294082417968235\n",
      "Iteration: 4587 lambda_k: 1 Loss: 0.002929408242014236\n",
      "Iteration: 4588 lambda_k: 1 Loss: 0.0029294082422305417\n",
      "Iteration: 4589 lambda_k: 1 Loss: 0.00292940824244572\n",
      "Iteration: 4590 lambda_k: 1 Loss: 0.0029294082426598055\n",
      "Iteration: 4591 lambda_k: 1 Loss: 0.0029294082428727632\n",
      "Iteration: 4592 lambda_k: 1 Loss: 0.002929408243084613\n",
      "Iteration: 4593 lambda_k: 1 Loss: 0.00292940824329538\n",
      "Iteration: 4594 lambda_k: 1 Loss: 0.002929408243505047\n",
      "Iteration: 4595 lambda_k: 1 Loss: 0.0029294082437136375\n",
      "Iteration: 4596 lambda_k: 1 Loss: 0.002929408243921143\n",
      "Iteration: 4597 lambda_k: 1 Loss: 0.0029294082441275833\n",
      "Iteration: 4598 lambda_k: 1 Loss: 0.0029294082443329554\n",
      "Iteration: 4599 lambda_k: 1 Loss: 0.0029294082445372677\n",
      "Iteration: 4600 lambda_k: 1 Loss: 0.002929408244740523\n",
      "Iteration: 4601 lambda_k: 1 Loss: 0.0029294082449427346\n",
      "Iteration: 4602 lambda_k: 1 Loss: 0.0029294082451438814\n",
      "Iteration: 4603 lambda_k: 1 Loss: 0.0029294082453439983\n",
      "Iteration: 4604 lambda_k: 1 Loss: 0.0029294082455430773\n",
      "Iteration: 4605 lambda_k: 1 Loss: 0.0029294082457411363\n",
      "Iteration: 4606 lambda_k: 1 Loss: 0.00292940824593816\n",
      "Iteration: 4607 lambda_k: 1 Loss: 0.0029294082461341782\n",
      "Iteration: 4608 lambda_k: 1 Loss: 0.0029294082463291776\n",
      "Iteration: 4609 lambda_k: 1 Loss: 0.0029294082465231735\n",
      "Iteration: 4610 lambda_k: 1 Loss: 0.0029294082467161554\n",
      "Iteration: 4611 lambda_k: 1 Loss: 0.002929408246908136\n",
      "Iteration: 4612 lambda_k: 1 Loss: 0.0029294082470991273\n",
      "Iteration: 4613 lambda_k: 1 Loss: 0.0029294082472891437\n",
      "Iteration: 4614 lambda_k: 1 Loss: 0.0029294082474781674\n",
      "Iteration: 4615 lambda_k: 1 Loss: 0.002929408247666227\n",
      "Iteration: 4616 lambda_k: 1 Loss: 0.0029294082478533\n",
      "Iteration: 4617 lambda_k: 1 Loss: 0.0029294082480394172\n",
      "Iteration: 4618 lambda_k: 1 Loss: 0.00292940824822456\n",
      "Iteration: 4619 lambda_k: 1 Loss: 0.0029294082484087594\n",
      "Iteration: 4620 lambda_k: 1 Loss: 0.0029294082485919895\n",
      "Iteration: 4621 lambda_k: 1 Loss: 0.0029294082487742856\n",
      "Iteration: 4622 lambda_k: 1 Loss: 0.0029294082489556383\n",
      "Iteration: 4623 lambda_k: 1 Loss: 0.0029294082491360483\n",
      "Iteration: 4624 lambda_k: 1 Loss: 0.0029294082493155154\n",
      "Iteration: 4625 lambda_k: 1 Loss: 0.002929408249494061\n",
      "Iteration: 4626 lambda_k: 1 Loss: 0.002929408249671693\n",
      "Iteration: 4627 lambda_k: 1 Loss: 0.002929408249848403\n",
      "Iteration: 4628 lambda_k: 1 Loss: 0.0029294082500241868\n",
      "Iteration: 4629 lambda_k: 1 Loss: 0.002929408250199079\n",
      "Iteration: 4630 lambda_k: 1 Loss: 0.002929408250373063\n",
      "Iteration: 4631 lambda_k: 1 Loss: 0.00292940825054614\n",
      "Iteration: 4632 lambda_k: 1 Loss: 0.002929408250718337\n",
      "Iteration: 4633 lambda_k: 1 Loss: 0.002929408250889624\n",
      "Iteration: 4634 lambda_k: 1 Loss: 0.0029294082510600353\n",
      "Iteration: 4635 lambda_k: 1 Loss: 0.0029294082512295624\n",
      "Iteration: 4636 lambda_k: 1 Loss: 0.002929408251398209\n",
      "Iteration: 4637 lambda_k: 1 Loss: 0.0029294082515660008\n",
      "Iteration: 4638 lambda_k: 1 Loss: 0.0029294082517329068\n",
      "Iteration: 4639 lambda_k: 1 Loss: 0.0029294082518989614\n",
      "Iteration: 4640 lambda_k: 1 Loss: 0.002929408252064145\n",
      "Iteration: 4641 lambda_k: 1 Loss: 0.002929408252228478\n",
      "Iteration: 4642 lambda_k: 1 Loss: 0.0029294082523919703\n",
      "Iteration: 4643 lambda_k: 1 Loss: 0.0029294082525546093\n",
      "Iteration: 4644 lambda_k: 1 Loss: 0.0029294082527164195\n",
      "Iteration: 4645 lambda_k: 1 Loss: 0.0029294082528773806\n",
      "Iteration: 4646 lambda_k: 1 Loss: 0.002929408253037518\n",
      "Iteration: 4647 lambda_k: 1 Loss: 0.0029294082531968186\n",
      "Iteration: 4648 lambda_k: 1 Loss: 0.0029294082533552995\n",
      "Iteration: 4649 lambda_k: 1 Loss: 0.0029294082535129724\n",
      "Iteration: 4650 lambda_k: 1 Loss: 0.002929408253669812\n",
      "Iteration: 4651 lambda_k: 1 Loss: 0.0029294082538258476\n",
      "Iteration: 4652 lambda_k: 1 Loss: 0.002929408253981095\n",
      "Iteration: 4653 lambda_k: 1 Loss: 0.0029294082541355274\n",
      "Iteration: 4654 lambda_k: 1 Loss: 0.002929408254289167\n",
      "Iteration: 4655 lambda_k: 1 Loss: 0.002929408254442005\n",
      "Iteration: 4656 lambda_k: 1 Loss: 0.00292940825459405\n",
      "Iteration: 4657 lambda_k: 1 Loss: 0.0029294082547452996\n",
      "Iteration: 4658 lambda_k: 1 Loss: 0.0029294082548957812\n",
      "Iteration: 4659 lambda_k: 1 Loss: 0.002929408255045489\n",
      "Iteration: 4660 lambda_k: 1 Loss: 0.0029294082551944143\n",
      "Iteration: 4661 lambda_k: 1 Loss: 0.0029294082553425822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4662 lambda_k: 1 Loss: 0.0029294082554899917\n",
      "Iteration: 4663 lambda_k: 1 Loss: 0.0029294082556366187\n",
      "Iteration: 4664 lambda_k: 1 Loss: 0.002929408255782497\n",
      "Iteration: 4665 lambda_k: 1 Loss: 0.00292940825592761\n",
      "Iteration: 4666 lambda_k: 1 Loss: 0.0029294082560719793\n",
      "Iteration: 4667 lambda_k: 1 Loss: 0.002929408256215612\n",
      "Iteration: 4668 lambda_k: 1 Loss: 0.002929408256358487\n",
      "Iteration: 4669 lambda_k: 1 Loss: 0.00292940825650063\n",
      "Iteration: 4670 lambda_k: 1 Loss: 0.0029294082566420467\n",
      "Iteration: 4671 lambda_k: 1 Loss: 0.0029294082567827145\n",
      "Iteration: 4672 lambda_k: 1 Loss: 0.0029294082569226716\n",
      "Iteration: 4673 lambda_k: 1 Loss: 0.0029294082570619053\n",
      "Iteration: 4674 lambda_k: 1 Loss: 0.002929408257200411\n",
      "Iteration: 4675 lambda_k: 1 Loss: 0.002929408257338202\n",
      "Iteration: 4676 lambda_k: 1 Loss: 0.0029294082574752925\n",
      "Iteration: 4677 lambda_k: 1 Loss: 0.002929408257611661\n",
      "Iteration: 4678 lambda_k: 1 Loss: 0.0029294082577473353\n",
      "Iteration: 4679 lambda_k: 1 Loss: 0.0029294082578822933\n",
      "Iteration: 4680 lambda_k: 1 Loss: 0.002929408258016557\n",
      "Iteration: 4681 lambda_k: 1 Loss: 0.0029294082581501307\n",
      "Iteration: 4682 lambda_k: 1 Loss: 0.0029294082582830205\n",
      "Iteration: 4683 lambda_k: 1 Loss: 0.002929408258415214\n",
      "Iteration: 4684 lambda_k: 1 Loss: 0.0029294082585467366\n",
      "Iteration: 4685 lambda_k: 1 Loss: 0.0029294082586775664\n",
      "Iteration: 4686 lambda_k: 1 Loss: 0.002929408258807724\n",
      "Iteration: 4687 lambda_k: 1 Loss: 0.002929408258937217\n",
      "Iteration: 4688 lambda_k: 1 Loss: 0.0029294082590660252\n",
      "Iteration: 4689 lambda_k: 1 Loss: 0.0029294082591941723\n",
      "Iteration: 4690 lambda_k: 1 Loss: 0.0029294082593216675\n",
      "Iteration: 4691 lambda_k: 1 Loss: 0.002929408259448487\n",
      "Iteration: 4692 lambda_k: 1 Loss: 0.0029294082595746583\n",
      "Iteration: 4693 lambda_k: 1 Loss: 0.002929408259700181\n",
      "Iteration: 4694 lambda_k: 1 Loss: 0.0029294082598250656\n",
      "Iteration: 4695 lambda_k: 1 Loss: 0.002929408259949292\n",
      "Iteration: 4696 lambda_k: 1 Loss: 0.002929408260072884\n",
      "Iteration: 4697 lambda_k: 1 Loss: 0.002929408260195822\n",
      "Iteration: 4698 lambda_k: 1 Loss: 0.0029294082603181356\n",
      "Iteration: 4699 lambda_k: 1 Loss: 0.0029294082604398248\n",
      "Iteration: 4700 lambda_k: 1 Loss: 0.0029294082605608807\n",
      "Iteration: 4701 lambda_k: 1 Loss: 0.002929408260681303\n",
      "Iteration: 4702 lambda_k: 1 Loss: 0.0029294082608011056\n",
      "Iteration: 4703 lambda_k: 1 Loss: 0.0029294082609202968\n",
      "Iteration: 4704 lambda_k: 1 Loss: 0.002929408261038858\n",
      "Iteration: 4705 lambda_k: 1 Loss: 0.0029294082611568085\n",
      "Iteration: 4706 lambda_k: 1 Loss: 0.002929408261274159\n",
      "Iteration: 4707 lambda_k: 1 Loss: 0.002929408261390894\n",
      "Iteration: 4708 lambda_k: 1 Loss: 0.0029294082615070327\n",
      "Iteration: 4709 lambda_k: 1 Loss: 0.002929408261622571\n",
      "Iteration: 4710 lambda_k: 1 Loss: 0.0029294082617375076\n",
      "Iteration: 4711 lambda_k: 1 Loss: 0.002929408261851843\n",
      "Iteration: 4712 lambda_k: 1 Loss: 0.0029294082619656017\n",
      "Iteration: 4713 lambda_k: 1 Loss: 0.002929408262078775\n",
      "Iteration: 4714 lambda_k: 1 Loss: 0.0029294082621913473\n",
      "Iteration: 4715 lambda_k: 1 Loss: 0.002929408262303337\n",
      "Iteration: 4716 lambda_k: 1 Loss: 0.0029294082624147584\n",
      "Iteration: 4717 lambda_k: 1 Loss: 0.002929408262525616\n",
      "Iteration: 4718 lambda_k: 1 Loss: 0.0029294082626358845\n",
      "Iteration: 4719 lambda_k: 1 Loss: 0.0029294082627455966\n",
      "Iteration: 4720 lambda_k: 1 Loss: 0.0029294082628547415\n",
      "Iteration: 4721 lambda_k: 1 Loss: 0.0029294082629633352\n",
      "Iteration: 4722 lambda_k: 1 Loss: 0.0029294082630713456\n",
      "Iteration: 4723 lambda_k: 1 Loss: 0.0029294082631788083\n",
      "Iteration: 4724 lambda_k: 1 Loss: 0.0029294082632857284\n",
      "Iteration: 4725 lambda_k: 1 Loss: 0.002929408263392076\n",
      "Iteration: 4726 lambda_k: 1 Loss: 0.0029294082634978742\n",
      "Iteration: 4727 lambda_k: 1 Loss: 0.0029294082636031372\n",
      "Iteration: 4728 lambda_k: 1 Loss: 0.002929408263707844\n",
      "Iteration: 4729 lambda_k: 1 Loss: 0.0029294082638120075\n",
      "Iteration: 4730 lambda_k: 1 Loss: 0.0029294082639156373\n",
      "Iteration: 4731 lambda_k: 1 Loss: 0.002929408264018734\n",
      "Iteration: 4732 lambda_k: 1 Loss: 0.002929408264121306\n",
      "Iteration: 4733 lambda_k: 1 Loss: 0.002929408264223333\n",
      "Iteration: 4734 lambda_k: 1 Loss: 0.002929408264324845\n",
      "Iteration: 4735 lambda_k: 1 Loss: 0.0029294082644258286\n",
      "Iteration: 4736 lambda_k: 1 Loss: 0.002929408264526277\n",
      "Iteration: 4737 lambda_k: 1 Loss: 0.002929408264626214\n",
      "Iteration: 4738 lambda_k: 1 Loss: 0.002929408264725634\n",
      "Iteration: 4739 lambda_k: 1 Loss: 0.0029294082648245487\n",
      "Iteration: 4740 lambda_k: 1 Loss: 0.002929408264922945\n",
      "Iteration: 4741 lambda_k: 1 Loss: 0.002929408265020834\n",
      "Iteration: 4742 lambda_k: 1 Loss: 0.002929408265118215\n",
      "Iteration: 4743 lambda_k: 1 Loss: 0.0029294082652151056\n",
      "Iteration: 4744 lambda_k: 1 Loss: 0.002929408265311478\n",
      "Iteration: 4745 lambda_k: 1 Loss: 0.0029294082654073555\n",
      "Iteration: 4746 lambda_k: 1 Loss: 0.0029294082655027466\n",
      "Iteration: 4747 lambda_k: 1 Loss: 0.002929408265597645\n",
      "Iteration: 4748 lambda_k: 1 Loss: 0.002929408265692061\n",
      "Iteration: 4749 lambda_k: 1 Loss: 0.00292940826578597\n",
      "Iteration: 4750 lambda_k: 1 Loss: 0.0029294082658794054\n",
      "Iteration: 4751 lambda_k: 1 Loss: 0.00292940826597236\n",
      "Iteration: 4752 lambda_k: 1 Loss: 0.002929408266064832\n",
      "Iteration: 4753 lambda_k: 1 Loss: 0.0029294082661568167\n",
      "Iteration: 4754 lambda_k: 1 Loss: 0.002929408266248334\n",
      "Iteration: 4755 lambda_k: 1 Loss: 0.0029294082663393834\n",
      "Iteration: 4756 lambda_k: 1 Loss: 0.002929408266429974\n",
      "Iteration: 4757 lambda_k: 1 Loss: 0.0029294082665200756\n",
      "Iteration: 4758 lambda_k: 1 Loss: 0.0029294082666097144\n",
      "Iteration: 4759 lambda_k: 1 Loss: 0.0029294082666988966\n",
      "Iteration: 4760 lambda_k: 1 Loss: 0.002929408266787625\n",
      "Iteration: 4761 lambda_k: 1 Loss: 0.0029294082668758744\n",
      "Iteration: 4762 lambda_k: 1 Loss: 0.0029294082669636696\n",
      "Iteration: 4763 lambda_k: 1 Loss: 0.0029294082670510172\n",
      "Iteration: 4764 lambda_k: 1 Loss: 0.0029294082671379226\n",
      "Iteration: 4765 lambda_k: 1 Loss: 0.0029294082672243803\n",
      "Iteration: 4766 lambda_k: 1 Loss: 0.002929408267310379\n",
      "Iteration: 4767 lambda_k: 1 Loss: 0.0029294082673959354\n",
      "Iteration: 4768 lambda_k: 1 Loss: 0.002929408267481051\n",
      "Iteration: 4769 lambda_k: 1 Loss: 0.0029294082675657357\n",
      "Iteration: 4770 lambda_k: 1 Loss: 0.0029294082676499856\n",
      "Iteration: 4771 lambda_k: 1 Loss: 0.0029294082677337827\n",
      "Iteration: 4772 lambda_k: 1 Loss: 0.002929408267817155\n",
      "Iteration: 4773 lambda_k: 1 Loss: 0.0029294082679001067\n",
      "Iteration: 4774 lambda_k: 1 Loss: 0.0029294082679826297\n",
      "Iteration: 4775 lambda_k: 1 Loss: 0.0029294082680647077\n",
      "Iteration: 4776 lambda_k: 1 Loss: 0.0029294082681463676\n",
      "Iteration: 4777 lambda_k: 1 Loss: 0.002929408268227612\n",
      "Iteration: 4778 lambda_k: 1 Loss: 0.002929408268308451\n",
      "Iteration: 4779 lambda_k: 1 Loss: 0.002929408268388843\n",
      "Iteration: 4780 lambda_k: 1 Loss: 0.0029294082684688263\n",
      "Iteration: 4781 lambda_k: 1 Loss: 0.002929408268548394\n",
      "Iteration: 4782 lambda_k: 1 Loss: 0.0029294082686275565\n",
      "Iteration: 4783 lambda_k: 1 Loss: 0.00292940826870631\n",
      "Iteration: 4784 lambda_k: 1 Loss: 0.0029294082687846626\n",
      "Iteration: 4785 lambda_k: 1 Loss: 0.002929408268862595\n",
      "Iteration: 4786 lambda_k: 1 Loss: 0.002929408268940126\n",
      "Iteration: 4787 lambda_k: 1 Loss: 0.0029294082690172647\n",
      "Iteration: 4788 lambda_k: 1 Loss: 0.0029294082690940124\n",
      "Iteration: 4789 lambda_k: 1 Loss: 0.002929408269170368\n",
      "Iteration: 4790 lambda_k: 1 Loss: 0.0029294082692463115\n",
      "Iteration: 4791 lambda_k: 1 Loss: 0.0029294082693218683\n",
      "Iteration: 4792 lambda_k: 1 Loss: 0.002929408269397034\n",
      "Iteration: 4793 lambda_k: 1 Loss: 0.0029294082694718217\n",
      "Iteration: 4794 lambda_k: 1 Loss: 0.0029294082695462283\n",
      "Iteration: 4795 lambda_k: 1 Loss: 0.0029294082696202334\n",
      "Iteration: 4796 lambda_k: 1 Loss: 0.00292940826969386\n",
      "Iteration: 4797 lambda_k: 1 Loss: 0.0029294082697671007\n",
      "Iteration: 4798 lambda_k: 1 Loss: 0.002929408269839979\n",
      "Iteration: 4799 lambda_k: 1 Loss: 0.002929408269912487\n",
      "Iteration: 4800 lambda_k: 1 Loss: 0.0029294082699846164\n",
      "Iteration: 4801 lambda_k: 1 Loss: 0.0029294082700563576\n",
      "Iteration: 4802 lambda_k: 1 Loss: 0.002929408270127729\n",
      "Iteration: 4803 lambda_k: 1 Loss: 0.0029294082701987446\n",
      "Iteration: 4804 lambda_k: 1 Loss: 0.0029294082702693994\n",
      "Iteration: 4805 lambda_k: 1 Loss: 0.0029294082703396882\n",
      "Iteration: 4806 lambda_k: 1 Loss: 0.0029294082704095967\n",
      "Iteration: 4807 lambda_k: 1 Loss: 0.002929408270479146\n",
      "Iteration: 4808 lambda_k: 1 Loss: 0.0029294082705483403\n",
      "Iteration: 4809 lambda_k: 1 Loss: 0.0029294082706171785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4810 lambda_k: 1 Loss: 0.0029294082706856676\n",
      "Iteration: 4811 lambda_k: 1 Loss: 0.0029294082707537828\n",
      "Iteration: 4812 lambda_k: 1 Loss: 0.0029294082708215484\n",
      "Iteration: 4813 lambda_k: 1 Loss: 0.0029294082708889637\n",
      "Iteration: 4814 lambda_k: 1 Loss: 0.0029294082709560394\n",
      "Iteration: 4815 lambda_k: 1 Loss: 0.00292940827102277\n",
      "Iteration: 4816 lambda_k: 1 Loss: 0.0029294082710891595\n",
      "Iteration: 4817 lambda_k: 1 Loss: 0.0029294082711551948\n",
      "Iteration: 4818 lambda_k: 1 Loss: 0.0029294082712208874\n",
      "Iteration: 4819 lambda_k: 1 Loss: 0.002929408271286241\n",
      "Iteration: 4820 lambda_k: 1 Loss: 0.0029294082713512597\n",
      "Iteration: 4821 lambda_k: 1 Loss: 0.0029294082714159445\n",
      "Iteration: 4822 lambda_k: 1 Loss: 0.0029294082714803027\n",
      "Iteration: 4823 lambda_k: 1 Loss: 0.00292940827154431\n",
      "Iteration: 4824 lambda_k: 1 Loss: 0.00292940827160799\n",
      "Iteration: 4825 lambda_k: 1 Loss: 0.0029294082716713543\n",
      "Iteration: 4826 lambda_k: 1 Loss: 0.002929408271734384\n",
      "Iteration: 4827 lambda_k: 1 Loss: 0.002929408271797092\n",
      "Iteration: 4828 lambda_k: 1 Loss: 0.00292940827185949\n",
      "Iteration: 4829 lambda_k: 1 Loss: 0.0029294082719215657\n",
      "Iteration: 4830 lambda_k: 1 Loss: 0.002929408271983302\n",
      "Iteration: 4831 lambda_k: 1 Loss: 0.0029294082720447176\n",
      "Iteration: 4832 lambda_k: 1 Loss: 0.002929408272105817\n",
      "Iteration: 4833 lambda_k: 1 Loss: 0.002929408272166613\n",
      "Iteration: 4834 lambda_k: 1 Loss: 0.002929408272227101\n",
      "Iteration: 4835 lambda_k: 1 Loss: 0.0029294082722872857\n",
      "Iteration: 4836 lambda_k: 1 Loss: 0.002929408272347135\n",
      "Iteration: 4837 lambda_k: 1 Loss: 0.0029294082724066754\n",
      "Iteration: 4838 lambda_k: 1 Loss: 0.002929408272465914\n",
      "Iteration: 4839 lambda_k: 1 Loss: 0.002929408272524848\n",
      "Iteration: 4840 lambda_k: 1 Loss: 0.002929408272583483\n",
      "Iteration: 4841 lambda_k: 1 Loss: 0.0029294082726418155\n",
      "Iteration: 4842 lambda_k: 1 Loss: 0.002929408272699845\n",
      "Iteration: 4843 lambda_k: 1 Loss: 0.002929408272757573\n",
      "Iteration: 4844 lambda_k: 1 Loss: 0.0029294082728149994\n",
      "Iteration: 4845 lambda_k: 1 Loss: 0.0029294082728721278\n",
      "Iteration: 4846 lambda_k: 1 Loss: 0.0029294082729289616\n",
      "Iteration: 4847 lambda_k: 1 Loss: 0.0029294082729855037\n",
      "Iteration: 4848 lambda_k: 1 Loss: 0.0029294082730417633\n",
      "Iteration: 4849 lambda_k: 1 Loss: 0.0029294082730977338\n",
      "Iteration: 4850 lambda_k: 1 Loss: 0.0029294082731533976\n",
      "Iteration: 4851 lambda_k: 1 Loss: 0.0029294082732087747\n",
      "Iteration: 4852 lambda_k: 1 Loss: 0.002929408273263872\n",
      "Iteration: 4853 lambda_k: 1 Loss: 0.00292940827331869\n",
      "Iteration: 4854 lambda_k: 1 Loss: 0.0029294082733732256\n",
      "Iteration: 4855 lambda_k: 1 Loss: 0.002929408273427476\n",
      "Iteration: 4856 lambda_k: 1 Loss: 0.002929408273481452\n",
      "Iteration: 4857 lambda_k: 1 Loss: 0.002929408273535139\n",
      "Iteration: 4858 lambda_k: 1 Loss: 0.0029294082735885534\n",
      "Iteration: 4859 lambda_k: 1 Loss: 0.002929408273641688\n",
      "Iteration: 4860 lambda_k: 1 Loss: 0.0029294082736945498\n",
      "Iteration: 4861 lambda_k: 1 Loss: 0.0029294082737471427\n",
      "Iteration: 4862 lambda_k: 1 Loss: 0.0029294082737994667\n",
      "Iteration: 4863 lambda_k: 1 Loss: 0.002929408273851525\n",
      "Iteration: 4864 lambda_k: 1 Loss: 0.0029294082739032973\n",
      "Iteration: 4865 lambda_k: 1 Loss: 0.0029294082739548103\n",
      "Iteration: 4866 lambda_k: 1 Loss: 0.002929408274006056\n",
      "Iteration: 4867 lambda_k: 1 Loss: 0.0029294082740570345\n",
      "Iteration: 4868 lambda_k: 1 Loss: 0.00292940827410775\n",
      "Iteration: 4869 lambda_k: 1 Loss: 0.0029294082741582062\n",
      "Iteration: 4870 lambda_k: 1 Loss: 0.0029294082742084113\n",
      "Iteration: 4871 lambda_k: 1 Loss: 0.002929408274258361\n",
      "Iteration: 4872 lambda_k: 1 Loss: 0.0029294082743080317\n",
      "Iteration: 4873 lambda_k: 1 Loss: 0.0029294082743574553\n",
      "Iteration: 4874 lambda_k: 1 Loss: 0.002929408274406619\n",
      "Iteration: 4875 lambda_k: 1 Loss: 0.002929408274455526\n",
      "Iteration: 4876 lambda_k: 1 Loss: 0.0029294082745041933\n",
      "Iteration: 4877 lambda_k: 1 Loss: 0.0029294082745526164\n",
      "Iteration: 4878 lambda_k: 1 Loss: 0.0029294082746007897\n",
      "Iteration: 4879 lambda_k: 1 Loss: 0.0029294082746487114\n",
      "Iteration: 4880 lambda_k: 1 Loss: 0.0029294082746963777\n",
      "Iteration: 4881 lambda_k: 1 Loss: 0.002929408274743785\n",
      "Iteration: 4882 lambda_k: 1 Loss: 0.00292940827479096\n",
      "Iteration: 4883 lambda_k: 1 Loss: 0.002929408274837887\n",
      "Iteration: 4884 lambda_k: 1 Loss: 0.002929408274884572\n",
      "Iteration: 4885 lambda_k: 1 Loss: 0.0029294082749310247\n",
      "Iteration: 4886 lambda_k: 1 Loss: 0.002929408274977236\n",
      "Iteration: 4887 lambda_k: 1 Loss: 0.0029294082750232165\n",
      "Iteration: 4888 lambda_k: 1 Loss: 0.0029294082750689465\n",
      "Iteration: 4889 lambda_k: 1 Loss: 0.0029294082751144326\n",
      "Iteration: 4890 lambda_k: 1 Loss: 0.0029294082751596864\n",
      "Iteration: 4891 lambda_k: 1 Loss: 0.0029294082752047037\n",
      "Iteration: 4892 lambda_k: 1 Loss: 0.0029294082752494934\n",
      "Iteration: 4893 lambda_k: 1 Loss: 0.002929408275294056\n",
      "Iteration: 4894 lambda_k: 1 Loss: 0.002929408275338394\n",
      "Iteration: 4895 lambda_k: 1 Loss: 0.002929408275382504\n",
      "Iteration: 4896 lambda_k: 1 Loss: 0.002929408275426385\n",
      "Iteration: 4897 lambda_k: 1 Loss: 0.002929408275470029\n",
      "Iteration: 4898 lambda_k: 1 Loss: 0.002929408275513449\n",
      "Iteration: 4899 lambda_k: 1 Loss: 0.002929408275556639\n",
      "Iteration: 4900 lambda_k: 1 Loss: 0.002929408275599609\n",
      "Iteration: 4901 lambda_k: 1 Loss: 0.0029294082756423654\n",
      "Iteration: 4902 lambda_k: 1 Loss: 0.0029294082756848965\n",
      "Iteration: 4903 lambda_k: 1 Loss: 0.002929408275727209\n",
      "Iteration: 4904 lambda_k: 1 Loss: 0.00292940827576932\n",
      "Iteration: 4905 lambda_k: 1 Loss: 0.0029294082758112064\n",
      "Iteration: 4906 lambda_k: 1 Loss: 0.002929408275852861\n",
      "Iteration: 4907 lambda_k: 1 Loss: 0.0029294082758943014\n",
      "Iteration: 4908 lambda_k: 1 Loss: 0.0029294082759355245\n",
      "Iteration: 4909 lambda_k: 1 Loss: 0.0029294082759765373\n",
      "Iteration: 4910 lambda_k: 1 Loss: 0.002929408276017343\n",
      "Iteration: 4911 lambda_k: 1 Loss: 0.002929408276057938\n",
      "Iteration: 4912 lambda_k: 1 Loss: 0.0029294082760983283\n",
      "Iteration: 4913 lambda_k: 1 Loss: 0.0029294082761385128\n",
      "Iteration: 4914 lambda_k: 1 Loss: 0.0029294082761784934\n",
      "Iteration: 4915 lambda_k: 1 Loss: 0.0029294082762182714\n",
      "Iteration: 4916 lambda_k: 1 Loss: 0.002929408276257828\n",
      "Iteration: 4917 lambda_k: 1 Loss: 0.002929408276297183\n",
      "Iteration: 4918 lambda_k: 1 Loss: 0.002929408276336327\n",
      "Iteration: 4919 lambda_k: 1 Loss: 0.002929408276375278\n",
      "Iteration: 4920 lambda_k: 1 Loss: 0.002929408276414024\n",
      "Iteration: 4921 lambda_k: 1 Loss: 0.00292940827645257\n",
      "Iteration: 4922 lambda_k: 1 Loss: 0.0029294082764909205\n",
      "Iteration: 4923 lambda_k: 1 Loss: 0.002929408276529075\n",
      "Iteration: 4924 lambda_k: 1 Loss: 0.0029294082765670385\n",
      "Iteration: 4925 lambda_k: 1 Loss: 0.002929408276604822\n",
      "Iteration: 4926 lambda_k: 1 Loss: 0.002929408276642399\n",
      "Iteration: 4927 lambda_k: 1 Loss: 0.002929408276679766\n",
      "Iteration: 4928 lambda_k: 1 Loss: 0.002929408276716939\n",
      "Iteration: 4929 lambda_k: 1 Loss: 0.0029294082767539263\n",
      "Iteration: 4930 lambda_k: 1 Loss: 0.0029294082767907237\n",
      "Iteration: 4931 lambda_k: 1 Loss: 0.002929408276827333\n",
      "Iteration: 4932 lambda_k: 1 Loss: 0.0029294082768637534\n",
      "Iteration: 4933 lambda_k: 1 Loss: 0.002929408276899985\n",
      "Iteration: 4934 lambda_k: 1 Loss: 0.0029294082769360354\n",
      "Iteration: 4935 lambda_k: 1 Loss: 0.0029294082769719056\n",
      "Iteration: 4936 lambda_k: 1 Loss: 0.0029294082770075958\n",
      "Iteration: 4937 lambda_k: 1 Loss: 0.0029294082770430717\n",
      "Iteration: 4938 lambda_k: 1 Loss: 0.002929408277078373\n",
      "Iteration: 4939 lambda_k: 1 Loss: 0.0029294082771134902\n",
      "Iteration: 4940 lambda_k: 1 Loss: 0.002929408277148434\n",
      "Iteration: 4941 lambda_k: 1 Loss: 0.0029294082771831875\n",
      "Iteration: 4942 lambda_k: 1 Loss: 0.0029294082772177675\n",
      "Iteration: 4943 lambda_k: 1 Loss: 0.0029294082772521757\n",
      "Iteration: 4944 lambda_k: 1 Loss: 0.002929408277286408\n",
      "Iteration: 4945 lambda_k: 1 Loss: 0.0029294082773204688\n",
      "Iteration: 4946 lambda_k: 1 Loss: 0.0029294082773543514\n",
      "Iteration: 4947 lambda_k: 1 Loss: 0.002929408277388064\n",
      "Iteration: 4948 lambda_k: 1 Loss: 0.0029294082774215836\n",
      "Iteration: 4949 lambda_k: 1 Loss: 0.002929408277454926\n",
      "Iteration: 4950 lambda_k: 1 Loss: 0.0029294082774880973\n",
      "Iteration: 4951 lambda_k: 1 Loss: 0.0029294082775211026\n",
      "Iteration: 4952 lambda_k: 1 Loss: 0.002929408277553938\n",
      "Iteration: 4953 lambda_k: 1 Loss: 0.002929408277586604\n",
      "Iteration: 4954 lambda_k: 1 Loss: 0.002929408277619101\n",
      "Iteration: 4955 lambda_k: 1 Loss: 0.0029294082776514354\n",
      "Iteration: 4956 lambda_k: 1 Loss: 0.0029294082776836023\n",
      "Iteration: 4957 lambda_k: 1 Loss: 0.0029294082777156115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4958 lambda_k: 1 Loss: 0.0029294082777474527\n",
      "Iteration: 4959 lambda_k: 1 Loss: 0.0029294082777791336\n",
      "Iteration: 4960 lambda_k: 1 Loss: 0.0029294082778106327\n",
      "Iteration: 4961 lambda_k: 1 Loss: 0.0029294082778419787\n",
      "Iteration: 4962 lambda_k: 1 Loss: 0.002929408277873154\n",
      "Iteration: 4963 lambda_k: 1 Loss: 0.002929408277904166\n",
      "Iteration: 4964 lambda_k: 1 Loss: 0.0029294082779350206\n",
      "Iteration: 4965 lambda_k: 1 Loss: 0.002929408277965717\n",
      "Iteration: 4966 lambda_k: 1 Loss: 0.0029294082779962563\n",
      "Iteration: 4967 lambda_k: 1 Loss: 0.002929408278026637\n",
      "Iteration: 4968 lambda_k: 1 Loss: 0.002929408278056865\n",
      "Iteration: 4969 lambda_k: 1 Loss: 0.0029294082780869386\n",
      "Iteration: 4970 lambda_k: 1 Loss: 0.002929408278116859\n",
      "Iteration: 4971 lambda_k: 1 Loss: 0.0029294082781466295\n",
      "Iteration: 4972 lambda_k: 1 Loss: 0.0029294082781762413\n",
      "Iteration: 4973 lambda_k: 1 Loss: 0.002929408278205693\n",
      "Iteration: 4974 lambda_k: 1 Loss: 0.002929408278234987\n",
      "Iteration: 4975 lambda_k: 1 Loss: 0.00292940827826413\n",
      "Iteration: 4976 lambda_k: 1 Loss: 0.0029294082782931248\n",
      "Iteration: 4977 lambda_k: 1 Loss: 0.002929408278321977\n",
      "Iteration: 4978 lambda_k: 1 Loss: 0.002929408278350673\n",
      "Iteration: 4979 lambda_k: 1 Loss: 0.0029294082783792317\n",
      "Iteration: 4980 lambda_k: 1 Loss: 0.002929408278407637\n",
      "Iteration: 4981 lambda_k: 1 Loss: 0.0029294082784358934\n",
      "Iteration: 4982 lambda_k: 1 Loss: 0.0029294082784640076\n",
      "Iteration: 4983 lambda_k: 1 Loss: 0.002929408278491994\n",
      "Iteration: 4984 lambda_k: 1 Loss: 0.0029294082785198306\n",
      "Iteration: 4985 lambda_k: 1 Loss: 0.0029294082785475233\n",
      "Iteration: 4986 lambda_k: 1 Loss: 0.002929408278575075\n",
      "Iteration: 4987 lambda_k: 1 Loss: 0.002929408278602474\n",
      "Iteration: 4988 lambda_k: 1 Loss: 0.002929408278629724\n",
      "Iteration: 4989 lambda_k: 1 Loss: 0.002929408278656841\n",
      "Iteration: 4990 lambda_k: 1 Loss: 0.0029294082786838135\n",
      "Iteration: 4991 lambda_k: 1 Loss: 0.0029294082787106454\n",
      "Iteration: 4992 lambda_k: 1 Loss: 0.002929408278737339\n",
      "Iteration: 4993 lambda_k: 1 Loss: 0.0029294082787638988\n",
      "Iteration: 4994 lambda_k: 1 Loss: 0.00292940827879032\n",
      "Iteration: 4995 lambda_k: 1 Loss: 0.0029294082788166084\n",
      "Iteration: 4996 lambda_k: 1 Loss: 0.0029294082788427663\n",
      "Iteration: 4997 lambda_k: 1 Loss: 0.002929408278868789\n",
      "Iteration: 4998 lambda_k: 1 Loss: 0.002929408278894682\n",
      "Iteration: 4999 lambda_k: 1 Loss: 0.002929408278920437\n",
      "Iteration: 5000 lambda_k: 1 Loss: 0.002929408278946069\n",
      "Iteration: 5001 lambda_k: 1 Loss: 0.0029294082789715777\n",
      "Iteration: 5002 lambda_k: 1 Loss: 0.0029294082789969316\n",
      "Iteration: 5003 lambda_k: 1 Loss: 0.0029294082790221497\n",
      "Iteration: 5004 lambda_k: 1 Loss: 0.0029294082790472372\n",
      "Iteration: 5005 lambda_k: 1 Loss: 0.0029294082790721943\n",
      "Iteration: 5006 lambda_k: 1 Loss: 0.002929408279097015\n",
      "Iteration: 5007 lambda_k: 1 Loss: 0.0029294082791217146\n",
      "Iteration: 5008 lambda_k: 1 Loss: 0.0029294082791462895\n",
      "Iteration: 5009 lambda_k: 1 Loss: 0.0029294082791707383\n",
      "Iteration: 5010 lambda_k: 1 Loss: 0.002929408279195058\n",
      "Iteration: 5011 lambda_k: 1 Loss: 0.0029294082792192607\n",
      "Iteration: 5012 lambda_k: 1 Loss: 0.002929408279243348\n",
      "Iteration: 5013 lambda_k: 1 Loss: 0.002929408279267315\n",
      "Iteration: 5014 lambda_k: 1 Loss: 0.0029294082792911506\n",
      "Iteration: 5015 lambda_k: 1 Loss: 0.002929408279314866\n",
      "Iteration: 5016 lambda_k: 1 Loss: 0.0029294082793384552\n",
      "Iteration: 5017 lambda_k: 1 Loss: 0.0029294082793619313\n",
      "Iteration: 5018 lambda_k: 1 Loss: 0.002929408279385289\n",
      "Iteration: 5019 lambda_k: 1 Loss: 0.002929408279408506\n",
      "Iteration: 5020 lambda_k: 1 Loss: 0.0029294082794316\n",
      "Iteration: 5021 lambda_k: 1 Loss: 0.002929408279454572\n",
      "Iteration: 5022 lambda_k: 1 Loss: 0.0029294082794774313\n",
      "Iteration: 5023 lambda_k: 1 Loss: 0.002929408279500169\n",
      "Iteration: 5024 lambda_k: 1 Loss: 0.0029294082795227913\n",
      "Iteration: 5025 lambda_k: 1 Loss: 0.002929408279545306\n",
      "Iteration: 5026 lambda_k: 1 Loss: 0.0029294082795676968\n",
      "Iteration: 5027 lambda_k: 1 Loss: 0.002929408279589978\n",
      "Iteration: 5028 lambda_k: 1 Loss: 0.002929408279612139\n",
      "Iteration: 5029 lambda_k: 1 Loss: 0.002929408279634192\n",
      "Iteration: 5030 lambda_k: 1 Loss: 0.002929408279656132\n",
      "Iteration: 5031 lambda_k: 1 Loss: 0.0029294082796779627\n",
      "Iteration: 5032 lambda_k: 1 Loss: 0.0029294082796996832\n",
      "Iteration: 5033 lambda_k: 1 Loss: 0.0029294082797212987\n",
      "Iteration: 5034 lambda_k: 1 Loss: 0.0029294082797428015\n",
      "Iteration: 5035 lambda_k: 1 Loss: 0.0029294082797641794\n",
      "Iteration: 5036 lambda_k: 1 Loss: 0.002929408279785441\n",
      "Iteration: 5037 lambda_k: 1 Loss: 0.002929408279806595\n",
      "Iteration: 5038 lambda_k: 1 Loss: 0.0029294082798276304\n",
      "Iteration: 5039 lambda_k: 1 Loss: 0.0029294082798485594\n",
      "Iteration: 5040 lambda_k: 1 Loss: 0.002929408279869377\n",
      "Iteration: 5041 lambda_k: 1 Loss: 0.0029294082798900904\n",
      "Iteration: 5042 lambda_k: 1 Loss: 0.002929408279910698\n",
      "Iteration: 5043 lambda_k: 1 Loss: 0.0029294082799312\n",
      "Iteration: 5044 lambda_k: 1 Loss: 0.002929408279951601\n",
      "Iteration: 5045 lambda_k: 1 Loss: 0.0029294082799718978\n",
      "Iteration: 5046 lambda_k: 1 Loss: 0.002929408279992088\n",
      "Iteration: 5047 lambda_k: 1 Loss: 0.002929408280012178\n",
      "Iteration: 5048 lambda_k: 1 Loss: 0.002929408280032168\n",
      "Iteration: 5049 lambda_k: 1 Loss: 0.002929408280052058\n",
      "Iteration: 5050 lambda_k: 1 Loss: 0.002929408280071847\n",
      "Iteration: 5051 lambda_k: 1 Loss: 0.0029294082800915373\n",
      "Iteration: 5052 lambda_k: 1 Loss: 0.0029294082801111267\n",
      "Iteration: 5053 lambda_k: 1 Loss: 0.0029294082801306163\n",
      "Iteration: 5054 lambda_k: 1 Loss: 0.0029294082801499858\n",
      "Iteration: 5055 lambda_k: 1 Loss: 0.002929408280169257\n",
      "Iteration: 5056 lambda_k: 1 Loss: 0.0029294082801884476\n",
      "Iteration: 5057 lambda_k: 1 Loss: 0.0029294082802075196\n",
      "Iteration: 5058 lambda_k: 1 Loss: 0.002929408280226491\n",
      "Iteration: 5059 lambda_k: 1 Loss: 0.0029294082802453635\n",
      "Iteration: 5060 lambda_k: 1 Loss: 0.002929408280264134\n",
      "Iteration: 5061 lambda_k: 1 Loss: 0.0029294082802828227\n",
      "Iteration: 5062 lambda_k: 1 Loss: 0.0029294082803014085\n",
      "Iteration: 5063 lambda_k: 1 Loss: 0.002929408280319898\n",
      "Iteration: 5064 lambda_k: 1 Loss: 0.002929408280338291\n",
      "Iteration: 5065 lambda_k: 1 Loss: 0.002929408280356591\n",
      "Iteration: 5066 lambda_k: 1 Loss: 0.002929408280374792\n",
      "Iteration: 5067 lambda_k: 1 Loss: 0.002929408280392911\n",
      "Iteration: 5068 lambda_k: 1 Loss: 0.002929408280410935\n",
      "Iteration: 5069 lambda_k: 1 Loss: 0.0029294082804288686\n",
      "Iteration: 5070 lambda_k: 1 Loss: 0.002929408280446708\n",
      "Iteration: 5071 lambda_k: 1 Loss: 0.0029294082804644586\n",
      "Iteration: 5072 lambda_k: 1 Loss: 0.0029294082804821216\n",
      "Iteration: 5073 lambda_k: 1 Loss: 0.0029294082804997004\n",
      "Iteration: 5074 lambda_k: 1 Loss: 0.0029294082805171886\n",
      "Iteration: 5075 lambda_k: 1 Loss: 0.002929408280534589\n",
      "Iteration: 5076 lambda_k: 1 Loss: 0.0029294082805518804\n",
      "Iteration: 5077 lambda_k: 1 Loss: 0.0029294082805690776\n",
      "Iteration: 5078 lambda_k: 1 Loss: 0.0029294082805861954\n",
      "Iteration: 5079 lambda_k: 1 Loss: 0.0029294082806032382\n",
      "Iteration: 5080 lambda_k: 1 Loss: 0.0029294082806201652\n",
      "Iteration: 5081 lambda_k: 1 Loss: 0.002929408280637012\n",
      "Iteration: 5082 lambda_k: 1 Loss: 0.0029294082806537733\n",
      "Iteration: 5083 lambda_k: 1 Loss: 0.0029294082806704414\n",
      "Iteration: 5084 lambda_k: 1 Loss: 0.0029294082806870245\n",
      "Iteration: 5085 lambda_k: 1 Loss: 0.002929408280703532\n",
      "Iteration: 5086 lambda_k: 1 Loss: 0.002929408280719942\n",
      "Iteration: 5087 lambda_k: 1 Loss: 0.002929408280736273\n",
      "Iteration: 5088 lambda_k: 1 Loss: 0.0029294082807525233\n",
      "Iteration: 5089 lambda_k: 1 Loss: 0.0029294082807686957\n",
      "Iteration: 5090 lambda_k: 1 Loss: 0.0029294082807847783\n",
      "Iteration: 5091 lambda_k: 1 Loss: 0.002929408280800786\n",
      "Iteration: 5092 lambda_k: 1 Loss: 0.002929408280816714\n",
      "Iteration: 5093 lambda_k: 1 Loss: 0.0029294082808325513\n",
      "Iteration: 5094 lambda_k: 1 Loss: 0.002929408280848314\n",
      "Iteration: 5095 lambda_k: 1 Loss: 0.002929408280864\n",
      "Iteration: 5096 lambda_k: 1 Loss: 0.0029294082808796014\n",
      "Iteration: 5097 lambda_k: 1 Loss: 0.0029294082808951297\n",
      "Iteration: 5098 lambda_k: 1 Loss: 0.0029294082809105835\n",
      "Iteration: 5099 lambda_k: 1 Loss: 0.0029294082809259584\n",
      "Iteration: 5100 lambda_k: 1 Loss: 0.0029294082809412252\n",
      "Iteration: 5101 lambda_k: 1 Loss: 0.00292940828095641\n",
      "Iteration: 5102 lambda_k: 1 Loss: 0.002929408280971522\n",
      "Iteration: 5103 lambda_k: 1 Loss: 0.0029294082809865796\n",
      "Iteration: 5104 lambda_k: 1 Loss: 0.0029294082810015372\n",
      "Iteration: 5105 lambda_k: 1 Loss: 0.0029294082810164142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5106 lambda_k: 1 Loss: 0.002929408281031216\n",
      "Iteration: 5107 lambda_k: 1 Loss: 0.002929408281045942\n",
      "Iteration: 5108 lambda_k: 1 Loss: 0.0029294082810605968\n",
      "Iteration: 5109 lambda_k: 1 Loss: 0.0029294082810751732\n",
      "Iteration: 5110 lambda_k: 1 Loss: 0.00292940828108967\n",
      "Iteration: 5111 lambda_k: 1 Loss: 0.0029294082811040932\n",
      "Iteration: 5112 lambda_k: 1 Loss: 0.002929408281118441\n",
      "Iteration: 5113 lambda_k: 1 Loss: 0.00292940828113272\n",
      "Iteration: 5114 lambda_k: 1 Loss: 0.002929408281146922\n",
      "Iteration: 5115 lambda_k: 1 Loss: 0.002929408281161056\n",
      "Iteration: 5116 lambda_k: 1 Loss: 0.0029294082811751176\n",
      "Iteration: 5117 lambda_k: 1 Loss: 0.002929408281189105\n",
      "Iteration: 5118 lambda_k: 1 Loss: 0.0029294082812030223\n",
      "Iteration: 5119 lambda_k: 1 Loss: 0.002929408281216873\n",
      "Iteration: 5120 lambda_k: 1 Loss: 0.0029294082812306617\n",
      "Iteration: 5121 lambda_k: 1 Loss: 0.002929408281244373\n",
      "Iteration: 5122 lambda_k: 1 Loss: 0.002929408281258014\n",
      "Iteration: 5123 lambda_k: 1 Loss: 0.0029294082812715873\n",
      "Iteration: 5124 lambda_k: 1 Loss: 0.002929408281285093\n",
      "Iteration: 5125 lambda_k: 1 Loss: 0.0029294082812985323\n",
      "Iteration: 5126 lambda_k: 1 Loss: 0.0029294082813119083\n",
      "Iteration: 5127 lambda_k: 1 Loss: 0.0029294082813252167\n",
      "Iteration: 5128 lambda_k: 1 Loss: 0.002929408281338434\n",
      "Iteration: 5129 lambda_k: 1 Loss: 0.0029294082813515745\n",
      "Iteration: 5130 lambda_k: 1 Loss: 0.0029294082813646656\n",
      "Iteration: 5131 lambda_k: 1 Loss: 0.0029294082813776747\n",
      "Iteration: 5132 lambda_k: 1 Loss: 0.0029294082813906184\n",
      "Iteration: 5133 lambda_k: 1 Loss: 0.002929408281403499\n",
      "Iteration: 5134 lambda_k: 1 Loss: 0.0029294082814163023\n",
      "Iteration: 5135 lambda_k: 1 Loss: 0.002929408281429037\n",
      "Iteration: 5136 lambda_k: 1 Loss: 0.0029294082814417064\n",
      "Iteration: 5137 lambda_k: 1 Loss: 0.0029294082814543244\n",
      "Iteration: 5138 lambda_k: 1 Loss: 0.002929408281466867\n",
      "Iteration: 5139 lambda_k: 1 Loss: 0.0029294082814793417\n",
      "Iteration: 5140 lambda_k: 1 Loss: 0.0029294082814917497\n",
      "Iteration: 5141 lambda_k: 1 Loss: 0.0029294082815040983\n",
      "Iteration: 5142 lambda_k: 1 Loss: 0.002929408281516392\n",
      "Iteration: 5143 lambda_k: 1 Loss: 0.0029294082815286182\n",
      "Iteration: 5144 lambda_k: 1 Loss: 0.0029294082815407877\n",
      "Iteration: 5145 lambda_k: 1 Loss: 0.0029294082815528896\n",
      "Iteration: 5146 lambda_k: 1 Loss: 0.0029294082815649277\n",
      "Iteration: 5147 lambda_k: 1 Loss: 0.0029294082815769134\n",
      "Iteration: 5148 lambda_k: 1 Loss: 0.002929408281588833\n",
      "Iteration: 5149 lambda_k: 1 Loss: 0.002929408281600694\n",
      "Iteration: 5150 lambda_k: 1 Loss: 0.00292940828161249\n",
      "Iteration: 5151 lambda_k: 1 Loss: 0.002929408281624227\n",
      "Iteration: 5152 lambda_k: 1 Loss: 0.0029294082816359057\n",
      "Iteration: 5153 lambda_k: 1 Loss: 0.002929408281647532\n",
      "Iteration: 5154 lambda_k: 1 Loss: 0.002929408281659096\n",
      "Iteration: 5155 lambda_k: 1 Loss: 0.0029294082816706\n",
      "Iteration: 5156 lambda_k: 1 Loss: 0.0029294082816820484\n",
      "Iteration: 5157 lambda_k: 1 Loss: 0.0029294082816934412\n",
      "Iteration: 5158 lambda_k: 1 Loss: 0.002929408281704777\n",
      "Iteration: 5159 lambda_k: 1 Loss: 0.0029294082817160304\n",
      "Iteration: 5160 lambda_k: 1 Loss: 0.002929408281727261\n",
      "Iteration: 5161 lambda_k: 1 Loss: 0.0029294082817384105\n",
      "Iteration: 5162 lambda_k: 1 Loss: 0.0029294082817494897\n",
      "Iteration: 5163 lambda_k: 1 Loss: 0.002929408281760509\n",
      "Iteration: 5164 lambda_k: 1 Loss: 0.0029294082817714695\n",
      "Iteration: 5165 lambda_k: 1 Loss: 0.002929408281782369\n",
      "Iteration: 5166 lambda_k: 1 Loss: 0.0029294082817932178\n",
      "Iteration: 5167 lambda_k: 1 Loss: 0.0029294082818040043\n",
      "Iteration: 5168 lambda_k: 1 Loss: 0.002929408281814739\n",
      "Iteration: 5169 lambda_k: 1 Loss: 0.0029294082818254177\n",
      "Iteration: 5170 lambda_k: 1 Loss: 0.0029294082818360485\n",
      "Iteration: 5171 lambda_k: 1 Loss: 0.002929408281846623\n",
      "Iteration: 5172 lambda_k: 1 Loss: 0.0029294082818571423\n",
      "Iteration: 5173 lambda_k: 1 Loss: 0.002929408281867609\n",
      "Iteration: 5174 lambda_k: 1 Loss: 0.0029294082818780228\n",
      "Iteration: 5175 lambda_k: 1 Loss: 0.0029294082818883834\n",
      "Iteration: 5176 lambda_k: 1 Loss: 0.0029294082818986855\n",
      "Iteration: 5177 lambda_k: 1 Loss: 0.00292940828190893\n",
      "Iteration: 5178 lambda_k: 1 Loss: 0.00292940828191913\n",
      "Iteration: 5179 lambda_k: 1 Loss: 0.00292940828192928\n",
      "Iteration: 5180 lambda_k: 1 Loss: 0.002929408281939374\n",
      "Iteration: 5181 lambda_k: 1 Loss: 0.002929408281949424\n",
      "Iteration: 5182 lambda_k: 1 Loss: 0.0029294082819594225\n",
      "Iteration: 5183 lambda_k: 1 Loss: 0.0029294082819693755\n",
      "Iteration: 5184 lambda_k: 1 Loss: 0.0029294082819792755\n",
      "Iteration: 5185 lambda_k: 1 Loss: 0.00292940828198912\n",
      "Iteration: 5186 lambda_k: 1 Loss: 0.0029294082819989187\n",
      "Iteration: 5187 lambda_k: 1 Loss: 0.002929408282008665\n",
      "Iteration: 5188 lambda_k: 1 Loss: 0.0029294082820183723\n",
      "Iteration: 5189 lambda_k: 1 Loss: 0.0029294082820280226\n",
      "Iteration: 5190 lambda_k: 1 Loss: 0.0029294082820376256\n",
      "Iteration: 5191 lambda_k: 1 Loss: 0.0029294082820471756\n",
      "Iteration: 5192 lambda_k: 1 Loss: 0.0029294082820566815\n",
      "Iteration: 5193 lambda_k: 1 Loss: 0.0029294082820661375\n",
      "Iteration: 5194 lambda_k: 1 Loss: 0.002929408282075545\n",
      "Iteration: 5195 lambda_k: 1 Loss: 0.0029294082820849046\n",
      "Iteration: 5196 lambda_k: 1 Loss: 0.0029294082820942265\n",
      "Iteration: 5197 lambda_k: 1 Loss: 0.002929408282103478\n",
      "Iteration: 5198 lambda_k: 1 Loss: 0.002929408282112677\n",
      "Iteration: 5199 lambda_k: 1 Loss: 0.002929408282121832\n",
      "Iteration: 5200 lambda_k: 1 Loss: 0.0029294082821309324\n",
      "Iteration: 5201 lambda_k: 1 Loss: 0.0029294082821399816\n",
      "Iteration: 5202 lambda_k: 1 Loss: 0.0029294082821490056\n",
      "Iteration: 5203 lambda_k: 1 Loss: 0.002929408282157962\n",
      "Iteration: 5204 lambda_k: 1 Loss: 0.0029294082821668703\n",
      "Iteration: 5205 lambda_k: 1 Loss: 0.002929408282175728\n",
      "Iteration: 5206 lambda_k: 1 Loss: 0.0029294082821845506\n",
      "Iteration: 5207 lambda_k: 1 Loss: 0.0029294082821933226\n",
      "Iteration: 5208 lambda_k: 1 Loss: 0.0029294082822020466\n",
      "Iteration: 5209 lambda_k: 1 Loss: 0.002929408282210732\n",
      "Iteration: 5210 lambda_k: 1 Loss: 0.0029294082822193704\n",
      "Iteration: 5211 lambda_k: 1 Loss: 0.0029294082822279594\n",
      "Iteration: 5212 lambda_k: 1 Loss: 0.002929408282236506\n",
      "Iteration: 5213 lambda_k: 1 Loss: 0.002929408282245005\n",
      "Iteration: 5214 lambda_k: 1 Loss: 0.0029294082822534633\n",
      "Iteration: 5215 lambda_k: 1 Loss: 0.0029294082822618837\n",
      "Iteration: 5216 lambda_k: 1 Loss: 0.002929408282270261\n",
      "Iteration: 5217 lambda_k: 1 Loss: 0.002929408282278593\n",
      "Iteration: 5218 lambda_k: 1 Loss: 0.0029294082822868823\n",
      "Iteration: 5219 lambda_k: 1 Loss: 0.002929408282295128\n",
      "Iteration: 5220 lambda_k: 1 Loss: 0.0029294082823033384\n",
      "Iteration: 5221 lambda_k: 1 Loss: 0.0029294082823115085\n",
      "Iteration: 5222 lambda_k: 1 Loss: 0.002929408282319633\n",
      "Iteration: 5223 lambda_k: 1 Loss: 0.002929408282327715\n",
      "Iteration: 5224 lambda_k: 1 Loss: 0.002929408282335756\n",
      "Iteration: 5225 lambda_k: 1 Loss: 0.0029294082823437587\n",
      "Iteration: 5226 lambda_k: 1 Loss: 0.0029294082823517176\n",
      "Iteration: 5227 lambda_k: 1 Loss: 0.0029294082823596327\n",
      "Iteration: 5228 lambda_k: 1 Loss: 0.0029294082823675136\n",
      "Iteration: 5229 lambda_k: 1 Loss: 0.002929408282375358\n",
      "Iteration: 5230 lambda_k: 1 Loss: 0.0029294082823831604\n",
      "Iteration: 5231 lambda_k: 1 Loss: 0.002929408282390931\n",
      "Iteration: 5232 lambda_k: 1 Loss: 0.002929408282398662\n",
      "Iteration: 5233 lambda_k: 1 Loss: 0.0029294082824063523\n",
      "Iteration: 5234 lambda_k: 1 Loss: 0.002929408282414003\n",
      "Iteration: 5235 lambda_k: 1 Loss: 0.002929408282421623\n",
      "Iteration: 5236 lambda_k: 1 Loss: 0.0029294082824292017\n",
      "Iteration: 5237 lambda_k: 1 Loss: 0.0029294082824367317\n",
      "Iteration: 5238 lambda_k: 1 Loss: 0.0029294082824442252\n",
      "Iteration: 5239 lambda_k: 1 Loss: 0.0029294082824516837\n",
      "Iteration: 5240 lambda_k: 1 Loss: 0.002929408282459108\n",
      "Iteration: 5241 lambda_k: 1 Loss: 0.0029294082824664934\n",
      "Iteration: 5242 lambda_k: 1 Loss: 0.0029294082824738287\n",
      "Iteration: 5243 lambda_k: 1 Loss: 0.002929408282481141\n",
      "Iteration: 5244 lambda_k: 1 Loss: 0.0029294082824884117\n",
      "Iteration: 5245 lambda_k: 1 Loss: 0.002929408282495627\n",
      "Iteration: 5246 lambda_k: 1 Loss: 0.0029294082825028272\n",
      "Iteration: 5247 lambda_k: 1 Loss: 0.002929408282509967\n",
      "Iteration: 5248 lambda_k: 1 Loss: 0.002929408282517067\n",
      "Iteration: 5249 lambda_k: 1 Loss: 0.0029294082825241487\n",
      "Iteration: 5250 lambda_k: 1 Loss: 0.002929408282531172\n",
      "Iteration: 5251 lambda_k: 1 Loss: 0.0029294082825381596\n",
      "Iteration: 5252 lambda_k: 1 Loss: 0.002929408282545122\n",
      "Iteration: 5253 lambda_k: 1 Loss: 0.0029294082825520613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5254 lambda_k: 1 Loss: 0.0029294082825589442\n",
      "Iteration: 5255 lambda_k: 1 Loss: 0.002929408282565792\n",
      "Iteration: 5256 lambda_k: 1 Loss: 0.0029294082825726\n",
      "Iteration: 5257 lambda_k: 1 Loss: 0.0029294082825793793\n",
      "Iteration: 5258 lambda_k: 1 Loss: 0.0029294082825861187\n",
      "Iteration: 5259 lambda_k: 1 Loss: 0.002929408282592821\n",
      "Iteration: 5260 lambda_k: 1 Loss: 0.0029294082825994895\n",
      "Iteration: 5261 lambda_k: 1 Loss: 0.0029294082826061283\n",
      "Iteration: 5262 lambda_k: 1 Loss: 0.0029294082826127224\n",
      "Iteration: 5263 lambda_k: 1 Loss: 0.0029294082826192905\n",
      "Iteration: 5264 lambda_k: 1 Loss: 0.00292940828262582\n",
      "Iteration: 5265 lambda_k: 1 Loss: 0.00292940828263232\n",
      "Iteration: 5266 lambda_k: 1 Loss: 0.002929408282638784\n",
      "Iteration: 5267 lambda_k: 1 Loss: 0.002929408282645225\n",
      "Iteration: 5268 lambda_k: 1 Loss: 0.0029294082826516288\n",
      "Iteration: 5269 lambda_k: 1 Loss: 0.002929408282657999\n",
      "Iteration: 5270 lambda_k: 1 Loss: 0.0029294082826643378\n",
      "Iteration: 5271 lambda_k: 1 Loss: 0.0029294082826706418\n",
      "Iteration: 5272 lambda_k: 1 Loss: 0.00292940828267691\n",
      "Iteration: 5273 lambda_k: 1 Loss: 0.0029294082826831504\n",
      "Iteration: 5274 lambda_k: 1 Loss: 0.0029294082826893555\n",
      "Iteration: 5275 lambda_k: 1 Loss: 0.0029294082826955277\n",
      "Iteration: 5276 lambda_k: 1 Loss: 0.0029294082827016677\n",
      "Iteration: 5277 lambda_k: 1 Loss: 0.002929408282707777\n",
      "Iteration: 5278 lambda_k: 1 Loss: 0.0029294082827138607\n",
      "Iteration: 5279 lambda_k: 1 Loss: 0.0029294082827199205\n",
      "Iteration: 5280 lambda_k: 1 Loss: 0.002929408282725948\n",
      "Iteration: 5281 lambda_k: 1 Loss: 0.002929408282731932\n",
      "Iteration: 5282 lambda_k: 1 Loss: 0.0029294082827378883\n",
      "Iteration: 5283 lambda_k: 1 Loss: 0.002929408282743814\n",
      "Iteration: 5284 lambda_k: 1 Loss: 0.0029294082827497135\n",
      "Iteration: 5285 lambda_k: 1 Loss: 0.0029294082827555894\n",
      "Iteration: 5286 lambda_k: 1 Loss: 0.0029294082827614316\n",
      "Iteration: 5287 lambda_k: 1 Loss: 0.0029294082827672407\n",
      "Iteration: 5288 lambda_k: 1 Loss: 0.002929408282773024\n",
      "Iteration: 5289 lambda_k: 1 Loss: 0.002929408282778774\n",
      "Iteration: 5290 lambda_k: 1 Loss: 0.0029294082827844977\n",
      "Iteration: 5291 lambda_k: 1 Loss: 0.0029294082827901946\n",
      "Iteration: 5292 lambda_k: 1 Loss: 0.0029294082827958636\n",
      "Iteration: 5293 lambda_k: 1 Loss: 0.002929408282801503\n",
      "Iteration: 5294 lambda_k: 1 Loss: 0.0029294082828071146\n",
      "Iteration: 5295 lambda_k: 1 Loss: 0.002929408282812707\n",
      "Iteration: 5296 lambda_k: 1 Loss: 0.0029294082828182654\n",
      "Iteration: 5297 lambda_k: 1 Loss: 0.0029294082828238083\n",
      "Iteration: 5298 lambda_k: 1 Loss: 0.0029294082828293117\n",
      "Iteration: 5299 lambda_k: 1 Loss: 0.0029294082828347904\n",
      "Iteration: 5300 lambda_k: 1 Loss: 0.0029294082828402452\n",
      "Iteration: 5301 lambda_k: 1 Loss: 0.0029294082828456745\n",
      "Iteration: 5302 lambda_k: 1 Loss: 0.002929408282851076\n",
      "Iteration: 5303 lambda_k: 1 Loss: 0.0029294082828564454\n",
      "Iteration: 5304 lambda_k: 1 Loss: 0.0029294082828617844\n",
      "Iteration: 5305 lambda_k: 1 Loss: 0.0029294082828670758\n",
      "Iteration: 5306 lambda_k: 1 Loss: 0.002929408282872364\n",
      "Iteration: 5307 lambda_k: 1 Loss: 0.0029294082828776246\n",
      "Iteration: 5308 lambda_k: 1 Loss: 0.002929408282882854\n",
      "Iteration: 5309 lambda_k: 1 Loss: 0.0029294082828880317\n",
      "Iteration: 5310 lambda_k: 1 Loss: 0.0029294082828932077\n",
      "Iteration: 5311 lambda_k: 1 Loss: 0.002929408282898334\n",
      "Iteration: 5312 lambda_k: 1 Loss: 0.0029294082829034373\n",
      "Iteration: 5313 lambda_k: 1 Loss: 0.002929408282908532\n",
      "Iteration: 5314 lambda_k: 1 Loss: 0.0029294082829135863\n",
      "Iteration: 5315 lambda_k: 1 Loss: 0.0029294082829186096\n",
      "Iteration: 5316 lambda_k: 1 Loss: 0.0029294082829236278\n",
      "Iteration: 5317 lambda_k: 1 Loss: 0.002929408282928598\n",
      "Iteration: 5318 lambda_k: 1 Loss: 0.002929408282933555\n",
      "Iteration: 5319 lambda_k: 1 Loss: 0.00292940828293847\n",
      "Iteration: 5320 lambda_k: 1 Loss: 0.002929408282943359\n",
      "Iteration: 5321 lambda_k: 1 Loss: 0.0029294082829482253\n",
      "Iteration: 5322 lambda_k: 1 Loss: 0.002929408282953087\n",
      "Iteration: 5323 lambda_k: 1 Loss: 0.0029294082829579037\n",
      "Iteration: 5324 lambda_k: 1 Loss: 0.0029294082829626924\n",
      "Iteration: 5325 lambda_k: 1 Loss: 0.0029294082829674512\n",
      "Iteration: 5326 lambda_k: 1 Loss: 0.00292940828297219\n",
      "Iteration: 5327 lambda_k: 1 Loss: 0.0029294082829769102\n",
      "Iteration: 5328 lambda_k: 1 Loss: 0.0029294082829816287\n",
      "Iteration: 5329 lambda_k: 1 Loss: 0.0029294082829862894\n",
      "Iteration: 5330 lambda_k: 1 Loss: 0.0029294082829909316\n",
      "Iteration: 5331 lambda_k: 1 Loss: 0.002929408282995548\n",
      "Iteration: 5332 lambda_k: 1 Loss: 0.0029294082830001434\n",
      "Iteration: 5333 lambda_k: 1 Loss: 0.0029294082830047135\n",
      "Iteration: 5334 lambda_k: 1 Loss: 0.00292940828300926\n",
      "Iteration: 5335 lambda_k: 1 Loss: 0.0029294082830137835\n",
      "Iteration: 5336 lambda_k: 1 Loss: 0.0029294082830182877\n",
      "Iteration: 5337 lambda_k: 1 Loss: 0.0029294082830227707\n",
      "Iteration: 5338 lambda_k: 1 Loss: 0.002929408283027223\n",
      "Iteration: 5339 lambda_k: 1 Loss: 0.002929408283031655\n",
      "Iteration: 5340 lambda_k: 1 Loss: 0.002929408283036065\n",
      "Iteration: 5341 lambda_k: 1 Loss: 0.00292940828304045\n",
      "Iteration: 5342 lambda_k: 1 Loss: 0.0029294082830448108\n",
      "Iteration: 5343 lambda_k: 1 Loss: 0.002929408283049146\n",
      "Iteration: 5344 lambda_k: 1 Loss: 0.0029294082830534627\n",
      "Iteration: 5345 lambda_k: 1 Loss: 0.0029294082830577557\n",
      "Iteration: 5346 lambda_k: 1 Loss: 0.002929408283062031\n",
      "Iteration: 5347 lambda_k: 1 Loss: 0.0029294082830662893\n",
      "Iteration: 5348 lambda_k: 1 Loss: 0.0029294082830705185\n",
      "Iteration: 5349 lambda_k: 1 Loss: 0.0029294082830747287\n",
      "Iteration: 5350 lambda_k: 1 Loss: 0.0029294082830789133\n",
      "Iteration: 5351 lambda_k: 1 Loss: 0.002929408283083077\n",
      "Iteration: 5352 lambda_k: 1 Loss: 0.002929408283087222\n",
      "Iteration: 5353 lambda_k: 1 Loss: 0.002929408283091348\n",
      "Iteration: 5354 lambda_k: 1 Loss: 0.002929408283095456\n",
      "Iteration: 5355 lambda_k: 1 Loss: 0.0029294082830995352\n",
      "Iteration: 5356 lambda_k: 1 Loss: 0.0029294082831035962\n",
      "Iteration: 5357 lambda_k: 1 Loss: 0.002929408283107636\n",
      "Iteration: 5358 lambda_k: 1 Loss: 0.0029294082831116605\n",
      "Iteration: 5359 lambda_k: 1 Loss: 0.0029294082831156716\n",
      "Iteration: 5360 lambda_k: 1 Loss: 0.002929408283119652\n",
      "Iteration: 5361 lambda_k: 1 Loss: 0.0029294082831236115\n",
      "Iteration: 5362 lambda_k: 1 Loss: 0.0029294082831275645\n",
      "Iteration: 5363 lambda_k: 1 Loss: 0.002929408283131493\n",
      "Iteration: 5364 lambda_k: 1 Loss: 0.002929408283135401\n",
      "Iteration: 5365 lambda_k: 1 Loss: 0.002929408283139282\n",
      "Iteration: 5366 lambda_k: 1 Loss: 0.002929408283143152\n",
      "Iteration: 5367 lambda_k: 1 Loss: 0.0029294082831470077\n",
      "Iteration: 5368 lambda_k: 1 Loss: 0.0029294082831508345\n",
      "Iteration: 5369 lambda_k: 1 Loss: 0.0029294082831546405\n",
      "Iteration: 5370 lambda_k: 1 Loss: 0.002929408283158435\n",
      "Iteration: 5371 lambda_k: 1 Loss: 0.002929408283162203\n",
      "Iteration: 5372 lambda_k: 1 Loss: 0.002929408283165953\n",
      "Iteration: 5373 lambda_k: 1 Loss: 0.0029294082831696853\n",
      "Iteration: 5374 lambda_k: 1 Loss: 0.0029294082831734024\n",
      "Iteration: 5375 lambda_k: 1 Loss: 0.0029294082831770956\n",
      "Iteration: 5376 lambda_k: 1 Loss: 0.0029294082831807762\n",
      "Iteration: 5377 lambda_k: 1 Loss: 0.00292940828318444\n",
      "Iteration: 5378 lambda_k: 1 Loss: 0.0029294082831880825\n",
      "Iteration: 5379 lambda_k: 1 Loss: 0.0029294082831917176\n",
      "Iteration: 5380 lambda_k: 1 Loss: 0.0029294082831953236\n",
      "Iteration: 5381 lambda_k: 1 Loss: 0.002929408283198908\n",
      "Iteration: 5382 lambda_k: 1 Loss: 0.0029294082832024733\n",
      "Iteration: 5383 lambda_k: 1 Loss: 0.002929408283206023\n",
      "Iteration: 5384 lambda_k: 1 Loss: 0.0029294082832095566\n",
      "Iteration: 5385 lambda_k: 1 Loss: 0.0029294082832130725\n",
      "Iteration: 5386 lambda_k: 1 Loss: 0.002929408283216576\n",
      "Iteration: 5387 lambda_k: 1 Loss: 0.002929408283220068\n",
      "Iteration: 5388 lambda_k: 1 Loss: 0.002929408283223533\n",
      "Iteration: 5389 lambda_k: 1 Loss: 0.0029294082832269828\n",
      "Iteration: 5390 lambda_k: 1 Loss: 0.0029294082832304114\n",
      "Iteration: 5391 lambda_k: 1 Loss: 0.0029294082832338254\n",
      "Iteration: 5392 lambda_k: 1 Loss: 0.002929408283237222\n",
      "Iteration: 5393 lambda_k: 1 Loss: 0.002929408283240603\n",
      "Iteration: 5394 lambda_k: 1 Loss: 0.0029294082832439644\n",
      "Iteration: 5395 lambda_k: 1 Loss: 0.0029294082832472994\n",
      "Iteration: 5396 lambda_k: 1 Loss: 0.0029294082832506275\n",
      "Iteration: 5397 lambda_k: 1 Loss: 0.0029294082832539365\n",
      "Iteration: 5398 lambda_k: 1 Loss: 0.0029294082832572402\n",
      "Iteration: 5399 lambda_k: 1 Loss: 0.0029294082832604933\n",
      "Iteration: 5400 lambda_k: 1 Loss: 0.0029294082832637494\n",
      "Iteration: 5401 lambda_k: 1 Loss: 0.002929408283266986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5402 lambda_k: 1 Loss: 0.0029294082832701943\n",
      "Iteration: 5403 lambda_k: 1 Loss: 0.0029294082832734074\n",
      "Iteration: 5404 lambda_k: 1 Loss: 0.002929408283276585\n",
      "Iteration: 5405 lambda_k: 1 Loss: 0.0029294082832797583\n",
      "Iteration: 5406 lambda_k: 1 Loss: 0.0029294082832829133\n",
      "Iteration: 5407 lambda_k: 1 Loss: 0.0029294082832860393\n",
      "Iteration: 5408 lambda_k: 1 Loss: 0.0029294082832891383\n",
      "Iteration: 5409 lambda_k: 1 Loss: 0.00292940828329225\n",
      "Iteration: 5410 lambda_k: 1 Loss: 0.0029294082832953226\n",
      "Iteration: 5411 lambda_k: 1 Loss: 0.0029294082832984018\n",
      "Iteration: 5412 lambda_k: 1 Loss: 0.0029294082833014406\n",
      "Iteration: 5413 lambda_k: 1 Loss: 0.0029294082833044794\n",
      "Iteration: 5414 lambda_k: 1 Loss: 0.0029294082833074835\n",
      "Iteration: 5415 lambda_k: 1 Loss: 0.002929408283310469\n",
      "Iteration: 5416 lambda_k: 1 Loss: 0.0029294082833134405\n",
      "Iteration: 5417 lambda_k: 1 Loss: 0.0029294082833164234\n",
      "Iteration: 5418 lambda_k: 1 Loss: 0.0029294082833193655\n",
      "Iteration: 5419 lambda_k: 1 Loss: 0.002929408283322317\n",
      "Iteration: 5420 lambda_k: 1 Loss: 0.0029294082833252245\n",
      "Iteration: 5421 lambda_k: 1 Loss: 0.0029294082833281184\n",
      "Iteration: 5422 lambda_k: 1 Loss: 0.0029294082833309985\n",
      "Iteration: 5423 lambda_k: 1 Loss: 0.002929408283333888\n",
      "Iteration: 5424 lambda_k: 1 Loss: 0.0029294082833367413\n",
      "Iteration: 5425 lambda_k: 1 Loss: 0.0029294082833395845\n",
      "Iteration: 5426 lambda_k: 1 Loss: 0.0029294082833424264\n",
      "Iteration: 5427 lambda_k: 1 Loss: 0.002929408283345234\n",
      "Iteration: 5428 lambda_k: 1 Loss: 0.0029294082833480404\n",
      "Iteration: 5429 lambda_k: 1 Loss: 0.002929408283350822\n",
      "Iteration: 5430 lambda_k: 1 Loss: 0.002929408283353588\n",
      "Iteration: 5431 lambda_k: 1 Loss: 0.0029294082833563554\n",
      "Iteration: 5432 lambda_k: 1 Loss: 0.002929408283359094\n",
      "Iteration: 5433 lambda_k: 1 Loss: 0.002929408283361815\n",
      "Iteration: 5434 lambda_k: 1 Loss: 0.0029294082833645403\n",
      "Iteration: 5435 lambda_k: 1 Loss: 0.0029294082833672334\n",
      "Iteration: 5436 lambda_k: 1 Loss: 0.0029294082833699166\n",
      "Iteration: 5437 lambda_k: 1 Loss: 0.002929408283372583\n",
      "Iteration: 5438 lambda_k: 1 Loss: 0.00292940828337525\n",
      "Iteration: 5439 lambda_k: 1 Loss: 0.002929408283377889\n",
      "Iteration: 5440 lambda_k: 1 Loss: 0.002929408283380513\n",
      "Iteration: 5441 lambda_k: 1 Loss: 0.0029294082833831252\n",
      "Iteration: 5442 lambda_k: 1 Loss: 0.0029294082833857225\n",
      "Iteration: 5443 lambda_k: 1 Loss: 0.002929408283388305\n",
      "Iteration: 5444 lambda_k: 1 Loss: 0.0029294082833908716\n",
      "Iteration: 5445 lambda_k: 1 Loss: 0.0029294082833934308\n",
      "Iteration: 5446 lambda_k: 1 Loss: 0.002929408283396005\n",
      "Iteration: 5447 lambda_k: 1 Loss: 0.0029294082833985343\n",
      "Iteration: 5448 lambda_k: 1 Loss: 0.0029294082834010558\n",
      "Iteration: 5449 lambda_k: 1 Loss: 0.0029294082834035616\n",
      "Iteration: 5450 lambda_k: 1 Loss: 0.002929408283406063\n",
      "Iteration: 5451 lambda_k: 1 Loss: 0.0029294082834085424\n",
      "Iteration: 5452 lambda_k: 1 Loss: 0.002929408283411007\n",
      "Iteration: 5453 lambda_k: 1 Loss: 0.002929408283413457\n",
      "Iteration: 5454 lambda_k: 1 Loss: 0.002929408283415901\n",
      "Iteration: 5455 lambda_k: 1 Loss: 0.0029294082834183306\n",
      "Iteration: 5456 lambda_k: 1 Loss: 0.002929408283420753\n",
      "Iteration: 5457 lambda_k: 1 Loss: 0.002929408283423158\n",
      "Iteration: 5458 lambda_k: 1 Loss: 0.0029294082834255553\n",
      "Iteration: 5459 lambda_k: 1 Loss: 0.0029294082834279496\n",
      "Iteration: 5460 lambda_k: 1 Loss: 0.0029294082834303145\n",
      "Iteration: 5461 lambda_k: 1 Loss: 0.0029294082834326724\n",
      "Iteration: 5462 lambda_k: 1 Loss: 0.0029294082834350177\n",
      "Iteration: 5463 lambda_k: 1 Loss: 0.0029294082834373627\n",
      "Iteration: 5464 lambda_k: 1 Loss: 0.0029294082834396807\n",
      "Iteration: 5465 lambda_k: 1 Loss: 0.002929408283441994\n",
      "Iteration: 5466 lambda_k: 1 Loss: 0.002929408283444291\n",
      "Iteration: 5467 lambda_k: 1 Loss: 0.002929408283446581\n",
      "Iteration: 5468 lambda_k: 1 Loss: 0.0029294082834488513\n",
      "Iteration: 5469 lambda_k: 1 Loss: 0.0029294082834511116\n",
      "Iteration: 5470 lambda_k: 1 Loss: 0.0029294082834533607\n",
      "Iteration: 5471 lambda_k: 1 Loss: 0.0029294082834555937\n",
      "Iteration: 5472 lambda_k: 1 Loss: 0.0029294082834578185\n",
      "Iteration: 5473 lambda_k: 1 Loss: 0.0029294082834600416\n",
      "Iteration: 5474 lambda_k: 1 Loss: 0.0029294082834622473\n",
      "Iteration: 5475 lambda_k: 1 Loss: 0.002929408283464433\n",
      "Iteration: 5476 lambda_k: 1 Loss: 0.002929408283466612\n",
      "Iteration: 5477 lambda_k: 1 Loss: 0.002929408283468782\n",
      "Iteration: 5478 lambda_k: 1 Loss: 0.002929408283470944\n",
      "Iteration: 5479 lambda_k: 1 Loss: 0.0029294082834730906\n",
      "Iteration: 5480 lambda_k: 1 Loss: 0.0029294082834752273\n",
      "Iteration: 5481 lambda_k: 1 Loss: 0.0029294082834773532\n",
      "Iteration: 5482 lambda_k: 1 Loss: 0.0029294082834794653\n",
      "Iteration: 5483 lambda_k: 1 Loss: 0.002929408283481569\n",
      "Iteration: 5484 lambda_k: 1 Loss: 0.0029294082834836715\n",
      "Iteration: 5485 lambda_k: 1 Loss: 0.0029294082834857597\n",
      "Iteration: 5486 lambda_k: 1 Loss: 0.0029294082834878314\n",
      "Iteration: 5487 lambda_k: 1 Loss: 0.002929408283489895\n",
      "Iteration: 5488 lambda_k: 1 Loss: 0.0029294082834919475\n",
      "Iteration: 5489 lambda_k: 1 Loss: 0.002929408283493986\n",
      "Iteration: 5490 lambda_k: 1 Loss: 0.0029294082834960236\n",
      "Iteration: 5491 lambda_k: 1 Loss: 0.002929408283498047\n",
      "Iteration: 5492 lambda_k: 1 Loss: 0.0029294082835000586\n",
      "Iteration: 5493 lambda_k: 1 Loss: 0.0029294082835020613\n",
      "Iteration: 5494 lambda_k: 1 Loss: 0.002929408283504054\n",
      "Iteration: 5495 lambda_k: 1 Loss: 0.0029294082835060378\n",
      "Iteration: 5496 lambda_k: 1 Loss: 0.002929408283508015\n",
      "Iteration: 5497 lambda_k: 1 Loss: 0.002929408283509973\n",
      "Iteration: 5498 lambda_k: 1 Loss: 0.0029294082835119323\n",
      "Iteration: 5499 lambda_k: 1 Loss: 0.0029294082835138744\n",
      "Iteration: 5500 lambda_k: 1 Loss: 0.0029294082835158103\n",
      "Iteration: 5501 lambda_k: 1 Loss: 0.0029294082835177276\n",
      "Iteration: 5502 lambda_k: 1 Loss: 0.002929408283519638\n",
      "Iteration: 5503 lambda_k: 1 Loss: 0.0029294082835215423\n",
      "Iteration: 5504 lambda_k: 1 Loss: 0.0029294082835234422\n",
      "Iteration: 5505 lambda_k: 1 Loss: 0.0029294082835253335\n",
      "Iteration: 5506 lambda_k: 1 Loss: 0.002929408283527214\n",
      "Iteration: 5507 lambda_k: 1 Loss: 0.002929408283529082\n",
      "Iteration: 5508 lambda_k: 1 Loss: 0.0029294082835309367\n",
      "Iteration: 5509 lambda_k: 1 Loss: 0.002929408283532783\n",
      "Iteration: 5510 lambda_k: 1 Loss: 0.002929408283534621\n",
      "Iteration: 5511 lambda_k: 1 Loss: 0.0029294082835364527\n",
      "Iteration: 5512 lambda_k: 1 Loss: 0.002929408283538282\n",
      "Iteration: 5513 lambda_k: 1 Loss: 0.002929408283540094\n",
      "Iteration: 5514 lambda_k: 1 Loss: 0.0029294082835418928\n",
      "Iteration: 5515 lambda_k: 1 Loss: 0.0029294082835436873\n",
      "Iteration: 5516 lambda_k: 1 Loss: 0.0029294082835454763\n",
      "Iteration: 5517 lambda_k: 1 Loss: 0.0029294082835472574\n",
      "Iteration: 5518 lambda_k: 1 Loss: 0.0029294082835490363\n",
      "Iteration: 5519 lambda_k: 1 Loss: 0.0029294082835507936\n",
      "Iteration: 5520 lambda_k: 1 Loss: 0.0029294082835525427\n",
      "Iteration: 5521 lambda_k: 1 Loss: 0.002929408283554281\n",
      "Iteration: 5522 lambda_k: 1 Loss: 0.0029294082835560125\n",
      "Iteration: 5523 lambda_k: 1 Loss: 0.0029294082835577494\n",
      "Iteration: 5524 lambda_k: 1 Loss: 0.0029294082835594776\n",
      "Iteration: 5525 lambda_k: 1 Loss: 0.0029294082835611877\n",
      "Iteration: 5526 lambda_k: 1 Loss: 0.0029294082835628807\n",
      "Iteration: 5527 lambda_k: 1 Loss: 0.00292940828356457\n",
      "Iteration: 5528 lambda_k: 1 Loss: 0.002929408283566265\n",
      "Iteration: 5529 lambda_k: 1 Loss: 0.00292940828356794\n",
      "Iteration: 5530 lambda_k: 1 Loss: 0.002929408283569613\n",
      "Iteration: 5531 lambda_k: 1 Loss: 0.0029294082835712707\n",
      "Iteration: 5532 lambda_k: 1 Loss: 0.002929408283572927\n",
      "Iteration: 5533 lambda_k: 1 Loss: 0.002929408283574569\n",
      "Iteration: 5534 lambda_k: 1 Loss: 0.0029294082835762047\n",
      "Iteration: 5535 lambda_k: 1 Loss: 0.0029294082835778284\n",
      "Iteration: 5536 lambda_k: 1 Loss: 0.002929408283579445\n",
      "Iteration: 5537 lambda_k: 1 Loss: 0.0029294082835810576\n",
      "Iteration: 5538 lambda_k: 1 Loss: 0.002929408283582663\n",
      "Iteration: 5539 lambda_k: 1 Loss: 0.0029294082835842564\n",
      "Iteration: 5540 lambda_k: 1 Loss: 0.002929408283585849\n",
      "Iteration: 5541 lambda_k: 1 Loss: 0.0029294082835874297\n",
      "Iteration: 5542 lambda_k: 1 Loss: 0.0029294082835890083\n",
      "Iteration: 5543 lambda_k: 1 Loss: 0.0029294082835905743\n",
      "Iteration: 5544 lambda_k: 1 Loss: 0.002929408283592126\n",
      "Iteration: 5545 lambda_k: 1 Loss: 0.0029294082835936703\n",
      "Iteration: 5546 lambda_k: 1 Loss: 0.0029294082835952056\n",
      "Iteration: 5547 lambda_k: 1 Loss: 0.002929408283596739\n",
      "Iteration: 5548 lambda_k: 1 Loss: 0.0029294082835982635\n",
      "Iteration: 5549 lambda_k: 1 Loss: 0.002929408283599782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5550 lambda_k: 1 Loss: 0.002929408283601298\n",
      "Iteration: 5551 lambda_k: 1 Loss: 0.002929408283602807\n",
      "Iteration: 5552 lambda_k: 1 Loss: 0.002929408283604302\n",
      "Iteration: 5553 lambda_k: 1 Loss: 0.002929408283605794\n",
      "Iteration: 5554 lambda_k: 1 Loss: 0.0029294082836072736\n",
      "Iteration: 5555 lambda_k: 1 Loss: 0.002929408283608741\n",
      "Iteration: 5556 lambda_k: 1 Loss: 0.0029294082836102044\n",
      "Iteration: 5557 lambda_k: 1 Loss: 0.0029294082836116633\n",
      "Iteration: 5558 lambda_k: 1 Loss: 0.0029294082836130914\n",
      "Iteration: 5559 lambda_k: 1 Loss: 0.002929408283614537\n",
      "Iteration: 5560 lambda_k: 1 Loss: 0.0029294082836159785\n",
      "Iteration: 5561 lambda_k: 1 Loss: 0.0029294082836174044\n",
      "Iteration: 5562 lambda_k: 1 Loss: 0.0029294082836188204\n",
      "Iteration: 5563 lambda_k: 1 Loss: 0.002929408283620212\n",
      "Iteration: 5564 lambda_k: 1 Loss: 0.002929408283621626\n",
      "Iteration: 5565 lambda_k: 1 Loss: 0.002929408283623023\n",
      "Iteration: 5566 lambda_k: 1 Loss: 0.002929408283624411\n",
      "Iteration: 5567 lambda_k: 1 Loss: 0.002929408283625775\n",
      "Iteration: 5568 lambda_k: 1 Loss: 0.0029294082836271622\n",
      "Iteration: 5569 lambda_k: 1 Loss: 0.002929408283628538\n",
      "Iteration: 5570 lambda_k: 1 Loss: 0.002929408283629901\n",
      "Iteration: 5571 lambda_k: 1 Loss: 0.00292940828363123\n",
      "Iteration: 5572 lambda_k: 1 Loss: 0.0029294082836325776\n",
      "Iteration: 5573 lambda_k: 1 Loss: 0.002929408283633911\n",
      "Iteration: 5574 lambda_k: 1 Loss: 0.00292940828363522\n",
      "Iteration: 5575 lambda_k: 1 Loss: 0.002929408283636542\n",
      "Iteration: 5576 lambda_k: 1 Loss: 0.002929408283637836\n",
      "Iteration: 5577 lambda_k: 1 Loss: 0.002929408283639142\n",
      "Iteration: 5578 lambda_k: 1 Loss: 0.0029294082836404463\n",
      "Iteration: 5579 lambda_k: 1 Loss: 0.0029294082836417213\n",
      "Iteration: 5580 lambda_k: 1 Loss: 0.00292940828364301\n",
      "Iteration: 5581 lambda_k: 1 Loss: 0.00292940828364428\n",
      "Iteration: 5582 lambda_k: 1 Loss: 0.0029294082836455546\n",
      "Iteration: 5583 lambda_k: 1 Loss: 0.0029294082836468227\n",
      "Iteration: 5584 lambda_k: 1 Loss: 0.002929408283648065\n",
      "Iteration: 5585 lambda_k: 1 Loss: 0.0029294082836493\n",
      "Iteration: 5586 lambda_k: 1 Loss: 0.0029294082836505493\n",
      "Iteration: 5587 lambda_k: 1 Loss: 0.0029294082836517944\n",
      "Iteration: 5588 lambda_k: 1 Loss: 0.002929408283653011\n",
      "Iteration: 5589 lambda_k: 1 Loss: 0.002929408283654242\n",
      "Iteration: 5590 lambda_k: 1 Loss: 0.002929408283655464\n",
      "Iteration: 5591 lambda_k: 1 Loss: 0.002929408283656666\n",
      "Iteration: 5592 lambda_k: 1 Loss: 0.002929408283657881\n",
      "Iteration: 5593 lambda_k: 1 Loss: 0.0029294082836590616\n",
      "Iteration: 5594 lambda_k: 1 Loss: 0.002929408283660234\n",
      "Iteration: 5595 lambda_k: 1 Loss: 0.0029294082836614304\n",
      "Iteration: 5596 lambda_k: 1 Loss: 0.0029294082836626256\n",
      "Iteration: 5597 lambda_k: 1 Loss: 0.0029294082836637853\n",
      "Iteration: 5598 lambda_k: 1 Loss: 0.0029294082836649354\n",
      "Iteration: 5599 lambda_k: 1 Loss: 0.0029294082836661033\n",
      "Iteration: 5600 lambda_k: 1 Loss: 0.0029294082836672604\n",
      "Iteration: 5601 lambda_k: 1 Loss: 0.002929408283668395\n",
      "Iteration: 5602 lambda_k: 1 Loss: 0.0029294082836695446\n",
      "Iteration: 5603 lambda_k: 1 Loss: 0.0029294082836706682\n",
      "Iteration: 5604 lambda_k: 1 Loss: 0.0029294082836718123\n",
      "Iteration: 5605 lambda_k: 1 Loss: 0.0029294082836729407\n",
      "Iteration: 5606 lambda_k: 1 Loss: 0.0029294082836740457\n",
      "Iteration: 5607 lambda_k: 1 Loss: 0.002929408283675144\n",
      "Iteration: 5608 lambda_k: 1 Loss: 0.00292940828367626\n",
      "Iteration: 5609 lambda_k: 1 Loss: 0.0029294082836773495\n",
      "Iteration: 5610 lambda_k: 1 Loss: 0.002929408283678465\n",
      "Iteration: 5611 lambda_k: 1 Loss: 0.0029294082836795452\n",
      "Iteration: 5612 lambda_k: 1 Loss: 0.0029294082836806325\n",
      "Iteration: 5613 lambda_k: 1 Loss: 0.0029294082836816954\n",
      "Iteration: 5614 lambda_k: 1 Loss: 0.002929408283682761\n",
      "Iteration: 5615 lambda_k: 1 Loss: 0.0029294082836838356\n",
      "Iteration: 5616 lambda_k: 1 Loss: 0.002929408283684908\n",
      "Iteration: 5617 lambda_k: 1 Loss: 0.0029294082836859524\n",
      "Iteration: 5618 lambda_k: 1 Loss: 0.002929408283686991\n",
      "Iteration: 5619 lambda_k: 1 Loss: 0.002929408283688043\n",
      "Iteration: 5620 lambda_k: 1 Loss: 0.002929408283689093\n",
      "Iteration: 5621 lambda_k: 1 Loss: 0.0029294082836901123\n",
      "Iteration: 5622 lambda_k: 1 Loss: 0.002929408283691137\n",
      "Iteration: 5623 lambda_k: 1 Loss: 0.0029294082836921666\n",
      "Iteration: 5624 lambda_k: 1 Loss: 0.0029294082836931754\n",
      "Iteration: 5625 lambda_k: 1 Loss: 0.0029294082836941776\n",
      "Iteration: 5626 lambda_k: 1 Loss: 0.0029294082836952015\n",
      "Iteration: 5627 lambda_k: 1 Loss: 0.002929408283696209\n",
      "Iteration: 5628 lambda_k: 1 Loss: 0.0029294082836971965\n",
      "Iteration: 5629 lambda_k: 1 Loss: 0.002929408283698176\n",
      "Iteration: 5630 lambda_k: 1 Loss: 0.0029294082836991723\n",
      "Iteration: 5631 lambda_k: 1 Loss: 0.0029294082837001433\n",
      "Iteration: 5632 lambda_k: 1 Loss: 0.002929408283701132\n",
      "Iteration: 5633 lambda_k: 1 Loss: 0.002929408283702092\n",
      "Iteration: 5634 lambda_k: 1 Loss: 0.0029294082837030546\n",
      "Iteration: 5635 lambda_k: 1 Loss: 0.0029294082837040256\n",
      "Iteration: 5636 lambda_k: 1 Loss: 0.0029294082837049724\n",
      "Iteration: 5637 lambda_k: 1 Loss: 0.0029294082837059334\n",
      "Iteration: 5638 lambda_k: 1 Loss: 0.0029294082837068966\n",
      "Iteration: 5639 lambda_k: 1 Loss: 0.0029294082837078264\n",
      "Iteration: 5640 lambda_k: 1 Loss: 0.0029294082837087554\n",
      "Iteration: 5641 lambda_k: 1 Loss: 0.0029294082837096735\n",
      "Iteration: 5642 lambda_k: 1 Loss: 0.002929408283710605\n",
      "Iteration: 5643 lambda_k: 1 Loss: 0.0029294082837115223\n",
      "Iteration: 5644 lambda_k: 1 Loss: 0.0029294082837124555\n",
      "Iteration: 5645 lambda_k: 1 Loss: 0.0029294082837133567\n",
      "Iteration: 5646 lambda_k: 1 Loss: 0.002929408283714252\n",
      "Iteration: 5647 lambda_k: 1 Loss: 0.002929408283715164\n",
      "Iteration: 5648 lambda_k: 1 Loss: 0.0029294082837160564\n",
      "Iteration: 5649 lambda_k: 1 Loss: 0.0029294082837169667\n",
      "Iteration: 5650 lambda_k: 1 Loss: 0.002929408283717864\n",
      "Iteration: 5651 lambda_k: 1 Loss: 0.002929408283718745\n",
      "Iteration: 5652 lambda_k: 1 Loss: 0.002929408283719617\n",
      "Iteration: 5653 lambda_k: 1 Loss: 0.0029294082837204856\n",
      "Iteration: 5654 lambda_k: 1 Loss: 0.002929408283721367\n",
      "Iteration: 5655 lambda_k: 1 Loss: 0.002929408283722223\n",
      "Iteration: 5656 lambda_k: 1 Loss: 0.0029294082837230894\n",
      "Iteration: 5657 lambda_k: 1 Loss: 0.0029294082837239355\n",
      "Iteration: 5658 lambda_k: 1 Loss: 0.0029294082837247803\n",
      "Iteration: 5659 lambda_k: 1 Loss: 0.00292940828372564\n",
      "Iteration: 5660 lambda_k: 1 Loss: 0.002929408283726469\n",
      "Iteration: 5661 lambda_k: 1 Loss: 0.0029294082837272983\n",
      "Iteration: 5662 lambda_k: 1 Loss: 0.0029294082837281435\n",
      "Iteration: 5663 lambda_k: 1 Loss: 0.0029294082837289658\n",
      "Iteration: 5664 lambda_k: 1 Loss: 0.0029294082837298136\n",
      "Iteration: 5665 lambda_k: 1 Loss: 0.002929408283730627\n",
      "Iteration: 5666 lambda_k: 1 Loss: 0.002929408283731439\n",
      "Iteration: 5667 lambda_k: 1 Loss: 0.002929408283732242\n",
      "Iteration: 5668 lambda_k: 1 Loss: 0.002929408283733061\n",
      "Iteration: 5669 lambda_k: 1 Loss: 0.0029294082837338555\n",
      "Iteration: 5670 lambda_k: 1 Loss: 0.002929408283734648\n",
      "Iteration: 5671 lambda_k: 1 Loss: 0.0029294082837354528\n",
      "Iteration: 5672 lambda_k: 1 Loss: 0.002929408283736241\n",
      "Iteration: 5673 lambda_k: 1 Loss: 0.0029294082837370236\n",
      "Iteration: 5674 lambda_k: 1 Loss: 0.0029294082837378215\n",
      "Iteration: 5675 lambda_k: 1 Loss: 0.0029294082837385935\n",
      "Iteration: 5676 lambda_k: 1 Loss: 0.002929408283739367\n",
      "Iteration: 5677 lambda_k: 1 Loss: 0.0029294082837401513\n",
      "Iteration: 5678 lambda_k: 1 Loss: 0.0029294082837409176\n",
      "Iteration: 5679 lambda_k: 1 Loss: 0.0029294082837416917\n",
      "Iteration: 5680 lambda_k: 1 Loss: 0.002929408283742447\n",
      "Iteration: 5681 lambda_k: 1 Loss: 0.0029294082837432018\n",
      "Iteration: 5682 lambda_k: 1 Loss: 0.002929408283743956\n",
      "Iteration: 5683 lambda_k: 1 Loss: 0.0029294082837447296\n",
      "Iteration: 5684 lambda_k: 1 Loss: 0.002929408283745466\n",
      "Iteration: 5685 lambda_k: 1 Loss: 0.002929408283746201\n",
      "Iteration: 5686 lambda_k: 1 Loss: 0.002929408283746958\n",
      "Iteration: 5687 lambda_k: 1 Loss: 0.002929408283747683\n",
      "Iteration: 5688 lambda_k: 1 Loss: 0.002929408283748404\n",
      "Iteration: 5689 lambda_k: 1 Loss: 0.002929408283749145\n",
      "Iteration: 5690 lambda_k: 1 Loss: 0.0029294082837498583\n",
      "Iteration: 5691 lambda_k: 1 Loss: 0.0029294082837505704\n",
      "Iteration: 5692 lambda_k: 1 Loss: 0.0029294082837513003\n",
      "Iteration: 5693 lambda_k: 1 Loss: 0.0029294082837520046\n",
      "Iteration: 5694 lambda_k: 1 Loss: 0.002929408283752701\n",
      "Iteration: 5695 lambda_k: 1 Loss: 0.002929408283753415\n",
      "Iteration: 5696 lambda_k: 1 Loss: 0.00292940828375411\n",
      "Iteration: 5697 lambda_k: 1 Loss: 0.002929408283754808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5698 lambda_k: 1 Loss: 0.0029294082837554927\n",
      "Iteration: 5699 lambda_k: 1 Loss: 0.0029294082837562027\n",
      "Iteration: 5700 lambda_k: 1 Loss: 0.0029294082837568853\n",
      "Iteration: 5701 lambda_k: 1 Loss: 0.0029294082837575623\n",
      "Iteration: 5702 lambda_k: 1 Loss: 0.0029294082837582583\n",
      "Iteration: 5703 lambda_k: 1 Loss: 0.00292940828375893\n",
      "Iteration: 5704 lambda_k: 1 Loss: 0.002929408283759596\n",
      "Iteration: 5705 lambda_k: 1 Loss: 0.002929408283760276\n",
      "Iteration: 5706 lambda_k: 1 Loss: 0.0029294082837609307\n",
      "Iteration: 5707 lambda_k: 1 Loss: 0.002929408283761586\n",
      "Iteration: 5708 lambda_k: 1 Loss: 0.002929408283762239\n",
      "Iteration: 5709 lambda_k: 1 Loss: 0.002929408283762911\n",
      "Iteration: 5710 lambda_k: 1 Loss: 0.0029294082837635553\n",
      "Iteration: 5711 lambda_k: 1 Loss: 0.0029294082837642006\n",
      "Iteration: 5712 lambda_k: 1 Loss: 0.0029294082837648607\n",
      "Iteration: 5713 lambda_k: 1 Loss: 0.0029294082837654964\n",
      "Iteration: 5714 lambda_k: 1 Loss: 0.002929408283766126\n",
      "Iteration: 5715 lambda_k: 1 Loss: 0.0029294082837667532\n",
      "Iteration: 5716 lambda_k: 1 Loss: 0.002929408283767402\n",
      "Iteration: 5717 lambda_k: 1 Loss: 0.002929408283768025\n",
      "Iteration: 5718 lambda_k: 1 Loss: 0.0029294082837686437\n",
      "Iteration: 5719 lambda_k: 1 Loss: 0.002929408283769281\n",
      "Iteration: 5720 lambda_k: 1 Loss: 0.0029294082837698966\n",
      "Iteration: 5721 lambda_k: 1 Loss: 0.002929408283770515\n",
      "Iteration: 5722 lambda_k: 1 Loss: 0.002929408283771124\n",
      "Iteration: 5723 lambda_k: 1 Loss: 0.0029294082837717506\n",
      "Iteration: 5724 lambda_k: 1 Loss: 0.002929408283772351\n",
      "Iteration: 5725 lambda_k: 1 Loss: 0.002929408283772945\n",
      "Iteration: 5726 lambda_k: 1 Loss: 0.0029294082837735564\n",
      "Iteration: 5727 lambda_k: 1 Loss: 0.0029294082837741453\n",
      "Iteration: 5728 lambda_k: 1 Loss: 0.0029294082837747356\n",
      "Iteration: 5729 lambda_k: 1 Loss: 0.002929408283775323\n",
      "Iteration: 5730 lambda_k: 1 Loss: 0.0029294082837759204\n",
      "Iteration: 5731 lambda_k: 1 Loss: 0.002929408283776499\n",
      "Iteration: 5732 lambda_k: 1 Loss: 0.002929408283777074\n",
      "Iteration: 5733 lambda_k: 1 Loss: 0.0029294082837776473\n",
      "Iteration: 5734 lambda_k: 1 Loss: 0.0029294082837782167\n",
      "Iteration: 5735 lambda_k: 1 Loss: 0.0029294082837788\n",
      "Iteration: 5736 lambda_k: 1 Loss: 0.002929408283779361\n",
      "Iteration: 5737 lambda_k: 1 Loss: 0.002929408283779922\n",
      "Iteration: 5738 lambda_k: 1 Loss: 0.0029294082837804845\n",
      "Iteration: 5739 lambda_k: 1 Loss: 0.0029294082837810595\n",
      "Iteration: 5740 lambda_k: 1 Loss: 0.0029294082837816064\n",
      "Iteration: 5741 lambda_k: 1 Loss: 0.0029294082837821515\n",
      "Iteration: 5742 lambda_k: 1 Loss: 0.0029294082837827023\n",
      "Iteration: 5743 lambda_k: 1 Loss: 0.0029294082837832474\n",
      "Iteration: 5744 lambda_k: 1 Loss: 0.0029294082837837835\n",
      "Iteration: 5745 lambda_k: 1 Loss: 0.0029294082837843204\n",
      "Iteration: 5746 lambda_k: 1 Loss: 0.0029294082837848573\n",
      "Iteration: 5747 lambda_k: 1 Loss: 0.0029294082837853937\n",
      "Iteration: 5748 lambda_k: 1 Loss: 0.002929408283785916\n",
      "Iteration: 5749 lambda_k: 1 Loss: 0.0029294082837864424\n",
      "Iteration: 5750 lambda_k: 1 Loss: 0.0029294082837869606\n",
      "Iteration: 5751 lambda_k: 1 Loss: 0.0029294082837874737\n",
      "Iteration: 5752 lambda_k: 1 Loss: 0.0029294082837879928\n",
      "Iteration: 5753 lambda_k: 1 Loss: 0.0029294082837885075\n",
      "Iteration: 5754 lambda_k: 1 Loss: 0.0029294082837890085\n",
      "Iteration: 5755 lambda_k: 1 Loss: 0.0029294082837895124\n",
      "Iteration: 5756 lambda_k: 1 Loss: 0.0029294082837900237\n",
      "Iteration: 5757 lambda_k: 1 Loss: 0.0029294082837905437\n",
      "Iteration: 5758 lambda_k: 1 Loss: 0.0029294082837910355\n",
      "Iteration: 5759 lambda_k: 1 Loss: 0.002929408283791529\n",
      "Iteration: 5760 lambda_k: 1 Loss: 0.002929408283792023\n",
      "Iteration: 5761 lambda_k: 1 Loss: 0.0029294082837925226\n",
      "Iteration: 5762 lambda_k: 1 Loss: 0.0029294082837930105\n",
      "Iteration: 5763 lambda_k: 1 Loss: 0.0029294082837935135\n",
      "Iteration: 5764 lambda_k: 1 Loss: 0.0029294082837939958\n",
      "Iteration: 5765 lambda_k: 1 Loss: 0.002929408283794477\n",
      "Iteration: 5766 lambda_k: 1 Loss: 0.0029294082837949586\n",
      "Iteration: 5767 lambda_k: 1 Loss: 0.0029294082837954417\n",
      "Iteration: 5768 lambda_k: 1 Loss: 0.002929408283795931\n",
      "Iteration: 5769 lambda_k: 1 Loss: 0.002929408283796405\n",
      "Iteration: 5770 lambda_k: 1 Loss: 0.0029294082837968837\n",
      "Iteration: 5771 lambda_k: 1 Loss: 0.002929408283797351\n",
      "Iteration: 5772 lambda_k: 1 Loss: 0.0029294082837978243\n",
      "Iteration: 5773 lambda_k: 1 Loss: 0.0029294082837982858\n",
      "Iteration: 5774 lambda_k: 1 Loss: 0.002929408283798742\n",
      "Iteration: 5775 lambda_k: 1 Loss: 0.002929408283799219\n",
      "Iteration: 5776 lambda_k: 1 Loss: 0.0029294082837996787\n",
      "Iteration: 5777 lambda_k: 1 Loss: 0.0029294082838001315\n",
      "Iteration: 5778 lambda_k: 1 Loss: 0.002929408283800601\n",
      "Iteration: 5779 lambda_k: 1 Loss: 0.0029294082838010487\n",
      "Iteration: 5780 lambda_k: 1 Loss: 0.0029294082838015015\n",
      "Iteration: 5781 lambda_k: 1 Loss: 0.002929408283801945\n",
      "Iteration: 5782 lambda_k: 1 Loss: 0.002929408283802388\n",
      "Iteration: 5783 lambda_k: 1 Loss: 0.0029294082838028385\n",
      "Iteration: 5784 lambda_k: 1 Loss: 0.0029294082838033\n",
      "Iteration: 5785 lambda_k: 1 Loss: 0.0029294082838037406\n",
      "Iteration: 5786 lambda_k: 1 Loss: 0.0029294082838041764\n",
      "Iteration: 5787 lambda_k: 1 Loss: 0.002929408283804608\n",
      "Iteration: 5788 lambda_k: 1 Loss: 0.002929408283805041\n",
      "Iteration: 5789 lambda_k: 1 Loss: 0.0029294082838054636\n",
      "Iteration: 5790 lambda_k: 1 Loss: 0.002929408283805895\n",
      "Iteration: 5791 lambda_k: 1 Loss: 0.0029294082838063366\n",
      "Iteration: 5792 lambda_k: 1 Loss: 0.0029294082838067607\n",
      "Iteration: 5793 lambda_k: 1 Loss: 0.0029294082838071853\n",
      "Iteration: 5794 lambda_k: 1 Loss: 0.0029294082838075973\n",
      "Iteration: 5795 lambda_k: 1 Loss: 0.0029294082838080084\n",
      "Iteration: 5796 lambda_k: 1 Loss: 0.002929408283808442\n",
      "Iteration: 5797 lambda_k: 1 Loss: 0.0029294082838088554\n",
      "Iteration: 5798 lambda_k: 1 Loss: 0.0029294082838092752\n",
      "Iteration: 5799 lambda_k: 1 Loss: 0.0029294082838096937\n",
      "Iteration: 5800 lambda_k: 1 Loss: 0.002929408283810098\n",
      "Iteration: 5801 lambda_k: 1 Loss: 0.0029294082838105012\n",
      "Iteration: 5802 lambda_k: 1 Loss: 0.0029294082838109007\n",
      "Iteration: 5803 lambda_k: 1 Loss: 0.00292940828381132\n",
      "Iteration: 5804 lambda_k: 1 Loss: 0.0029294082838117142\n",
      "Iteration: 5805 lambda_k: 1 Loss: 0.0029294082838121076\n",
      "Iteration: 5806 lambda_k: 1 Loss: 0.002929408283812503\n",
      "Iteration: 5807 lambda_k: 1 Loss: 0.0029294082838129116\n",
      "Iteration: 5808 lambda_k: 1 Loss: 0.0029294082838133015\n",
      "Iteration: 5809 lambda_k: 1 Loss: 0.002929408283813692\n",
      "Iteration: 5810 lambda_k: 1 Loss: 0.002929408283814082\n",
      "Iteration: 5811 lambda_k: 1 Loss: 0.002929408283814472\n",
      "Iteration: 5812 lambda_k: 1 Loss: 0.0029294082838148697\n",
      "Iteration: 5813 lambda_k: 1 Loss: 0.002929408283815247\n",
      "Iteration: 5814 lambda_k: 1 Loss: 0.002929408283815623\n",
      "Iteration: 5815 lambda_k: 1 Loss: 0.0029294082838160077\n",
      "Iteration: 5816 lambda_k: 1 Loss: 0.002929408283816379\n",
      "Iteration: 5817 lambda_k: 1 Loss: 0.0029294082838167493\n",
      "Iteration: 5818 lambda_k: 1 Loss: 0.0029294082838171153\n",
      "Iteration: 5819 lambda_k: 1 Loss: 0.002929408283817502\n",
      "Iteration: 5820 lambda_k: 1 Loss: 0.0029294082838178708\n",
      "Iteration: 5821 lambda_k: 1 Loss: 0.0029294082838182338\n",
      "Iteration: 5822 lambda_k: 1 Loss: 0.002929408283818593\n",
      "Iteration: 5823 lambda_k: 1 Loss: 0.0029294082838189446\n",
      "Iteration: 5824 lambda_k: 1 Loss: 0.0029294082838193214\n",
      "Iteration: 5825 lambda_k: 1 Loss: 0.002929408283819675\n",
      "Iteration: 5826 lambda_k: 1 Loss: 0.002929408283820028\n",
      "Iteration: 5827 lambda_k: 1 Loss: 0.002929408283820381\n",
      "Iteration: 5828 lambda_k: 1 Loss: 0.0029294082838207287\n",
      "Iteration: 5829 lambda_k: 1 Loss: 0.0029294082838210787\n",
      "Iteration: 5830 lambda_k: 1 Loss: 0.0029294082838214478\n",
      "Iteration: 5831 lambda_k: 1 Loss: 0.002929408283821797\n",
      "Iteration: 5832 lambda_k: 1 Loss: 0.002929408283822141\n",
      "Iteration: 5833 lambda_k: 1 Loss: 0.0029294082838224847\n",
      "Iteration: 5834 lambda_k: 1 Loss: 0.0029294082838228265\n",
      "Iteration: 5835 lambda_k: 1 Loss: 0.002929408283823166\n",
      "Iteration: 5836 lambda_k: 1 Loss: 0.0029294082838235047\n",
      "Iteration: 5837 lambda_k: 1 Loss: 0.002929408283823841\n",
      "Iteration: 5838 lambda_k: 1 Loss: 0.002929408283824195\n",
      "Iteration: 5839 lambda_k: 1 Loss: 0.002929408283824524\n",
      "Iteration: 5840 lambda_k: 1 Loss: 0.0029294082838248483\n",
      "Iteration: 5841 lambda_k: 1 Loss: 0.0029294082838251766\n",
      "Iteration: 5842 lambda_k: 1 Loss: 0.002929408283825508\n",
      "Iteration: 5843 lambda_k: 1 Loss: 0.002929408283825833\n",
      "Iteration: 5844 lambda_k: 1 Loss: 0.0029294082838261515\n",
      "Iteration: 5845 lambda_k: 1 Loss: 0.002929408283826471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5846 lambda_k: 1 Loss: 0.0029294082838268146\n",
      "Iteration: 5847 lambda_k: 1 Loss: 0.0029294082838271325\n",
      "Iteration: 5848 lambda_k: 1 Loss: 0.002929408283827449\n",
      "Iteration: 5849 lambda_k: 1 Loss: 0.0029294082838277695\n",
      "Iteration: 5850 lambda_k: 1 Loss: 0.0029294082838280775\n",
      "Iteration: 5851 lambda_k: 1 Loss: 0.0029294082838283897\n",
      "Iteration: 5852 lambda_k: 1 Loss: 0.0029294082838287176\n",
      "Iteration: 5853 lambda_k: 1 Loss: 0.002929408283829023\n",
      "Iteration: 5854 lambda_k: 1 Loss: 0.002929408283829325\n",
      "Iteration: 5855 lambda_k: 1 Loss: 0.0029294082838296257\n",
      "Iteration: 5856 lambda_k: 1 Loss: 0.0029294082838299297\n",
      "Iteration: 5857 lambda_k: 1 Loss: 0.0029294082838302355\n",
      "Iteration: 5858 lambda_k: 1 Loss: 0.002929408283830539\n",
      "Iteration: 5859 lambda_k: 1 Loss: 0.0029294082838308543\n",
      "Iteration: 5860 lambda_k: 1 Loss: 0.002929408283831148\n",
      "Iteration: 5861 lambda_k: 1 Loss: 0.002929408283831442\n",
      "Iteration: 5862 lambda_k: 1 Loss: 0.0029294082838317325\n",
      "Iteration: 5863 lambda_k: 1 Loss: 0.0029294082838320352\n",
      "Iteration: 5864 lambda_k: 1 Loss: 0.0029294082838323297\n",
      "Iteration: 5865 lambda_k: 1 Loss: 0.002929408283832622\n",
      "Iteration: 5866 lambda_k: 1 Loss: 0.0029294082838329143\n",
      "Iteration: 5867 lambda_k: 1 Loss: 0.002929408283833199\n",
      "Iteration: 5868 lambda_k: 1 Loss: 0.0029294082838334824\n",
      "Iteration: 5869 lambda_k: 1 Loss: 0.002929408283833791\n",
      "Iteration: 5870 lambda_k: 1 Loss: 0.002929408283834081\n",
      "Iteration: 5871 lambda_k: 1 Loss: 0.0029294082838343645\n",
      "Iteration: 5872 lambda_k: 1 Loss: 0.002929408283834644\n",
      "Iteration: 5873 lambda_k: 1 Loss: 0.002929408283834918\n",
      "Iteration: 5874 lambda_k: 1 Loss: 0.0029294082838351907\n",
      "Iteration: 5875 lambda_k: 1 Loss: 0.002929408283835491\n",
      "Iteration: 5876 lambda_k: 1 Loss: 0.002929408283835765\n",
      "Iteration: 5877 lambda_k: 1 Loss: 0.0029294082838360346\n",
      "Iteration: 5878 lambda_k: 1 Loss: 0.0029294082838363027\n",
      "Iteration: 5879 lambda_k: 1 Loss: 0.0029294082838365737\n",
      "Iteration: 5880 lambda_k: 1 Loss: 0.0029294082838368487\n",
      "Iteration: 5881 lambda_k: 1 Loss: 0.002929408283837136\n",
      "Iteration: 5882 lambda_k: 1 Loss: 0.002929408283837399\n",
      "Iteration: 5883 lambda_k: 1 Loss: 0.0029294082838376627\n",
      "Iteration: 5884 lambda_k: 1 Loss: 0.002929408283837935\n",
      "Iteration: 5885 lambda_k: 1 Loss: 0.0029294082838382082\n",
      "Iteration: 5886 lambda_k: 1 Loss: 0.002929408283838468\n",
      "Iteration: 5887 lambda_k: 1 Loss: 0.002929408283838722\n",
      "Iteration: 5888 lambda_k: 1 Loss: 0.002929408283838987\n",
      "Iteration: 5889 lambda_k: 1 Loss: 0.0029294082838392504\n",
      "Iteration: 5890 lambda_k: 1 Loss: 0.0029294082838395028\n",
      "Iteration: 5891 lambda_k: 1 Loss: 0.002929408283839771\n",
      "Iteration: 5892 lambda_k: 1 Loss: 0.002929408283840016\n",
      "Iteration: 5893 lambda_k: 1 Loss: 0.0029294082838402635\n",
      "Iteration: 5894 lambda_k: 1 Loss: 0.0029294082838405137\n",
      "Iteration: 5895 lambda_k: 1 Loss: 0.0029294082838407596\n",
      "Iteration: 5896 lambda_k: 1 Loss: 0.0029294082838410094\n",
      "Iteration: 5897 lambda_k: 1 Loss: 0.002929408283841262\n",
      "Iteration: 5898 lambda_k: 1 Loss: 0.0029294082838415073\n",
      "Iteration: 5899 lambda_k: 1 Loss: 0.002929408283841745\n",
      "Iteration: 5900 lambda_k: 1 Loss: 0.002929408283841978\n",
      "Iteration: 5901 lambda_k: 1 Loss: 0.002929408283842217\n",
      "Iteration: 5902 lambda_k: 1 Loss: 0.002929408283842479\n",
      "Iteration: 5903 lambda_k: 1 Loss: 0.0029294082838427194\n",
      "Iteration: 5904 lambda_k: 1 Loss: 0.002929408283842957\n",
      "Iteration: 5905 lambda_k: 1 Loss: 0.002929408283843193\n",
      "Iteration: 5906 lambda_k: 1 Loss: 0.002929408283843433\n",
      "Iteration: 5907 lambda_k: 1 Loss: 0.0029294082838436635\n",
      "Iteration: 5908 lambda_k: 1 Loss: 0.002929408283843895\n",
      "Iteration: 5909 lambda_k: 1 Loss: 0.0029294082838441267\n",
      "Iteration: 5910 lambda_k: 1 Loss: 0.0029294082838443626\n",
      "Iteration: 5911 lambda_k: 1 Loss: 0.0029294082838445938\n",
      "Iteration: 5912 lambda_k: 1 Loss: 0.002929408283844835\n",
      "Iteration: 5913 lambda_k: 1 Loss: 0.0029294082838450595\n",
      "Iteration: 5914 lambda_k: 1 Loss: 0.0029294082838452955\n",
      "Iteration: 5915 lambda_k: 1 Loss: 0.0029294082838455266\n",
      "Iteration: 5916 lambda_k: 1 Loss: 0.0029294082838457656\n",
      "Iteration: 5917 lambda_k: 1 Loss: 0.0029294082838459876\n",
      "Iteration: 5918 lambda_k: 1 Loss: 0.0029294082838462027\n",
      "Iteration: 5919 lambda_k: 1 Loss: 0.0029294082838464226\n",
      "Iteration: 5920 lambda_k: 1 Loss: 0.002929408283846644\n",
      "Iteration: 5921 lambda_k: 1 Loss: 0.0029294082838468636\n",
      "Iteration: 5922 lambda_k: 1 Loss: 0.0029294082838470844\n",
      "Iteration: 5923 lambda_k: 1 Loss: 0.002929408283847305\n",
      "Iteration: 5924 lambda_k: 1 Loss: 0.0029294082838475163\n",
      "Iteration: 5925 lambda_k: 1 Loss: 0.0029294082838477314\n",
      "Iteration: 5926 lambda_k: 1 Loss: 0.002929408283847943\n",
      "Iteration: 5927 lambda_k: 1 Loss: 0.0029294082838481703\n",
      "Iteration: 5928 lambda_k: 1 Loss: 0.002929408283848378\n",
      "Iteration: 5929 lambda_k: 1 Loss: 0.002929408283848589\n",
      "Iteration: 5930 lambda_k: 1 Loss: 0.0029294082838487944\n",
      "Iteration: 5931 lambda_k: 1 Loss: 0.002929408283849008\n",
      "Iteration: 5932 lambda_k: 1 Loss: 0.0029294082838492133\n",
      "Iteration: 5933 lambda_k: 1 Loss: 0.0029294082838494107\n",
      "Iteration: 5934 lambda_k: 1 Loss: 0.0029294082838496106\n",
      "Iteration: 5935 lambda_k: 1 Loss: 0.0029294082838498096\n",
      "Iteration: 5936 lambda_k: 1 Loss: 0.00292940828385001\n",
      "Iteration: 5937 lambda_k: 1 Loss: 0.002929408283850212\n",
      "Iteration: 5938 lambda_k: 1 Loss: 0.0029294082838504103\n",
      "Iteration: 5939 lambda_k: 1 Loss: 0.0029294082838506098\n",
      "Iteration: 5940 lambda_k: 1 Loss: 0.0029294082838508015\n",
      "Iteration: 5941 lambda_k: 1 Loss: 0.0029294082838509953\n",
      "Iteration: 5942 lambda_k: 1 Loss: 0.00292940828385119\n",
      "Iteration: 5943 lambda_k: 1 Loss: 0.002929408283851386\n",
      "Iteration: 5944 lambda_k: 1 Loss: 0.002929408283851599\n",
      "Iteration: 5945 lambda_k: 1 Loss: 0.0029294082838517855\n",
      "Iteration: 5946 lambda_k: 1 Loss: 0.0029294082838519763\n",
      "Iteration: 5947 lambda_k: 1 Loss: 0.002929408283852164\n",
      "Iteration: 5948 lambda_k: 1 Loss: 0.002929408283852351\n",
      "Iteration: 5949 lambda_k: 1 Loss: 0.0029294082838525475\n",
      "Iteration: 5950 lambda_k: 1 Loss: 0.002929408283852736\n",
      "Iteration: 5951 lambda_k: 1 Loss: 0.0029294082838529144\n",
      "Iteration: 5952 lambda_k: 1 Loss: 0.002929408283853104\n",
      "Iteration: 5953 lambda_k: 1 Loss: 0.002929408283853296\n",
      "Iteration: 5954 lambda_k: 1 Loss: 0.0029294082838534916\n",
      "Iteration: 5955 lambda_k: 1 Loss: 0.0029294082838536724\n",
      "Iteration: 5956 lambda_k: 1 Loss: 0.0029294082838538637\n",
      "Iteration: 5957 lambda_k: 1 Loss: 0.00292940828385405\n",
      "Iteration: 5958 lambda_k: 1 Loss: 0.002929408283854231\n",
      "Iteration: 5959 lambda_k: 1 Loss: 0.0029294082838544075\n",
      "Iteration: 5960 lambda_k: 1 Loss: 0.002929408283854582\n",
      "Iteration: 5961 lambda_k: 1 Loss: 0.002929408283854756\n",
      "Iteration: 5962 lambda_k: 1 Loss: 0.0029294082838549344\n",
      "Iteration: 5963 lambda_k: 1 Loss: 0.0029294082838551305\n",
      "Iteration: 5964 lambda_k: 1 Loss: 0.0029294082838553144\n",
      "Iteration: 5965 lambda_k: 1 Loss: 0.0029294082838554917\n",
      "Iteration: 5966 lambda_k: 1 Loss: 0.0029294082838556648\n",
      "Iteration: 5967 lambda_k: 1 Loss: 0.002929408283855836\n",
      "Iteration: 5968 lambda_k: 1 Loss: 0.002929408283856003\n",
      "Iteration: 5969 lambda_k: 1 Loss: 0.0029294082838561782\n",
      "Iteration: 5970 lambda_k: 1 Loss: 0.0029294082838563413\n",
      "Iteration: 5971 lambda_k: 1 Loss: 0.002929408283856506\n",
      "Iteration: 5972 lambda_k: 1 Loss: 0.0029294082838566735\n",
      "Iteration: 5973 lambda_k: 1 Loss: 0.002929408283856834\n",
      "Iteration: 5974 lambda_k: 1 Loss: 0.0029294082838570053\n",
      "Iteration: 5975 lambda_k: 1 Loss: 0.0029294082838571653\n",
      "Iteration: 5976 lambda_k: 1 Loss: 0.002929408283857321\n",
      "Iteration: 5977 lambda_k: 1 Loss: 0.0029294082838574793\n",
      "Iteration: 5978 lambda_k: 1 Loss: 0.002929408283857637\n",
      "Iteration: 5979 lambda_k: 1 Loss: 0.0029294082838577933\n",
      "Iteration: 5980 lambda_k: 1 Loss: 0.0029294082838579485\n",
      "Iteration: 5981 lambda_k: 1 Loss: 0.002929408283858101\n",
      "Iteration: 5982 lambda_k: 1 Loss: 0.0029294082838582803\n",
      "Iteration: 5983 lambda_k: 1 Loss: 0.002929408283858438\n",
      "Iteration: 5984 lambda_k: 1 Loss: 0.002929408283858601\n",
      "Iteration: 5985 lambda_k: 1 Loss: 0.002929408283858751\n",
      "Iteration: 5986 lambda_k: 1 Loss: 0.002929408283858904\n",
      "Iteration: 5987 lambda_k: 1 Loss: 0.0029294082838590653\n",
      "Iteration: 5988 lambda_k: 1 Loss: 0.0029294082838592227\n",
      "Iteration: 5989 lambda_k: 1 Loss: 0.0029294082838593723\n",
      "Iteration: 5990 lambda_k: 1 Loss: 0.002929408283859543\n",
      "Iteration: 5991 lambda_k: 1 Loss: 0.0029294082838597028\n",
      "Iteration: 5992 lambda_k: 1 Loss: 0.0029294082838598537\n",
      "Iteration: 5993 lambda_k: 1 Loss: 0.0029294082838600102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5994 lambda_k: 1 Loss: 0.002929408283860159\n",
      "Iteration: 5995 lambda_k: 1 Loss: 0.002929408283860304\n",
      "Iteration: 5996 lambda_k: 1 Loss: 0.0029294082838604517\n",
      "Iteration: 5997 lambda_k: 1 Loss: 0.0029294082838605957\n",
      "Iteration: 5998 lambda_k: 1 Loss: 0.0029294082838607414\n",
      "Iteration: 5999 lambda_k: 1 Loss: 0.002929408283860885\n",
      "Iteration: 6000 lambda_k: 1 Loss: 0.0029294082838610316\n",
      "Iteration: 6001 lambda_k: 1 Loss: 0.0029294082838611825\n",
      "Iteration: 6002 lambda_k: 1 Loss: 0.00292940828386133\n",
      "Iteration: 6003 lambda_k: 1 Loss: 0.002929408283861473\n",
      "Iteration: 6004 lambda_k: 1 Loss: 0.0029294082838616166\n",
      "Iteration: 6005 lambda_k: 1 Loss: 0.002929408283861756\n",
      "Iteration: 6006 lambda_k: 1 Loss: 0.0029294082838618994\n",
      "Iteration: 6007 lambda_k: 1 Loss: 0.0029294082838620355\n",
      "Iteration: 6008 lambda_k: 1 Loss: 0.0029294082838621743\n",
      "Iteration: 6009 lambda_k: 1 Loss: 0.0029294082838623088\n",
      "Iteration: 6010 lambda_k: 1 Loss: 0.002929408283862442\n",
      "Iteration: 6011 lambda_k: 1 Loss: 0.0029294082838625733\n",
      "Iteration: 6012 lambda_k: 1 Loss: 0.002929408283862711\n",
      "Iteration: 6013 lambda_k: 1 Loss: 0.0029294082838628483\n",
      "Iteration: 6014 lambda_k: 1 Loss: 0.002929408283862982\n",
      "Iteration: 6015 lambda_k: 1 Loss: 0.0029294082838631124\n",
      "Iteration: 6016 lambda_k: 1 Loss: 0.00292940828386324\n",
      "Iteration: 6017 lambda_k: 1 Loss: 0.0029294082838633695\n",
      "Iteration: 6018 lambda_k: 1 Loss: 0.0029294082838635027\n",
      "Iteration: 6019 lambda_k: 1 Loss: 0.0029294082838636354\n",
      "Iteration: 6020 lambda_k: 1 Loss: 0.002929408283863785\n",
      "Iteration: 6021 lambda_k: 1 Loss: 0.002929408283863917\n",
      "Iteration: 6022 lambda_k: 1 Loss: 0.0029294082838640582\n",
      "Iteration: 6023 lambda_k: 1 Loss: 0.0029294082838641953\n",
      "Iteration: 6024 lambda_k: 1 Loss: 0.002929408283864322\n",
      "Iteration: 6025 lambda_k: 1 Loss: 0.0029294082838644472\n",
      "Iteration: 6026 lambda_k: 1 Loss: 0.0029294082838645786\n",
      "Iteration: 6027 lambda_k: 1 Loss: 0.002929408283864703\n",
      "Iteration: 6028 lambda_k: 1 Loss: 0.0029294082838648254\n",
      "Iteration: 6029 lambda_k: 1 Loss: 0.0029294082838649577\n",
      "Iteration: 6030 lambda_k: 1 Loss: 0.002929408283865081\n",
      "Iteration: 6031 lambda_k: 1 Loss: 0.0029294082838652214\n",
      "Iteration: 6032 lambda_k: 1 Loss: 0.00292940828386534\n",
      "Iteration: 6033 lambda_k: 1 Loss: 0.0029294082838654694\n",
      "Iteration: 6034 lambda_k: 1 Loss: 0.0029294082838655956\n",
      "Iteration: 6035 lambda_k: 1 Loss: 0.0029294082838657145\n",
      "Iteration: 6036 lambda_k: 1 Loss: 0.00292940828386583\n",
      "Iteration: 6037 lambda_k: 1 Loss: 0.00292940828386595\n",
      "Iteration: 6038 lambda_k: 1 Loss: 0.0029294082838660688\n",
      "Iteration: 6039 lambda_k: 1 Loss: 0.002929408283866186\n",
      "Iteration: 6040 lambda_k: 1 Loss: 0.002929408283866305\n",
      "Iteration: 6041 lambda_k: 1 Loss: 0.0029294082838664235\n",
      "Iteration: 6042 lambda_k: 1 Loss: 0.0029294082838665415\n",
      "Iteration: 6043 lambda_k: 1 Loss: 0.002929408283866654\n",
      "Iteration: 6044 lambda_k: 1 Loss: 0.0029294082838667635\n",
      "Iteration: 6045 lambda_k: 1 Loss: 0.002929408283866873\n",
      "Iteration: 6046 lambda_k: 1 Loss: 0.0029294082838669812\n",
      "Iteration: 6047 lambda_k: 1 Loss: 0.002929408283867096\n",
      "Iteration: 6048 lambda_k: 1 Loss: 0.0029294082838672076\n",
      "Iteration: 6049 lambda_k: 1 Loss: 0.0029294082838673143\n",
      "Iteration: 6050 lambda_k: 1 Loss: 0.0029294082838674197\n",
      "Iteration: 6051 lambda_k: 1 Loss: 0.0029294082838675233\n",
      "Iteration: 6052 lambda_k: 1 Loss: 0.0029294082838676335\n",
      "Iteration: 6053 lambda_k: 1 Loss: 0.0029294082838677436\n",
      "Iteration: 6054 lambda_k: 1 Loss: 0.002929408283867853\n",
      "Iteration: 6055 lambda_k: 1 Loss: 0.0029294082838679783\n",
      "Iteration: 6056 lambda_k: 1 Loss: 0.002929408283868083\n",
      "Iteration: 6057 lambda_k: 1 Loss: 0.0029294082838681877\n",
      "Iteration: 6058 lambda_k: 1 Loss: 0.0029294082838682905\n",
      "Iteration: 6059 lambda_k: 1 Loss: 0.002929408283868396\n",
      "Iteration: 6060 lambda_k: 1 Loss: 0.0029294082838685095\n",
      "Iteration: 6061 lambda_k: 1 Loss: 0.0029294082838686375\n",
      "Iteration: 6062 lambda_k: 1 Loss: 0.0029294082838687433\n",
      "Iteration: 6063 lambda_k: 1 Loss: 0.002929408283868846\n",
      "Iteration: 6064 lambda_k: 1 Loss: 0.0029294082838689497\n",
      "Iteration: 6065 lambda_k: 1 Loss: 0.0029294082838690508\n",
      "Iteration: 6066 lambda_k: 1 Loss: 0.002929408283869149\n",
      "Iteration: 6067 lambda_k: 1 Loss: 0.0029294082838692503\n",
      "Iteration: 6068 lambda_k: 1 Loss: 0.002929408283869358\n",
      "Iteration: 6069 lambda_k: 1 Loss: 0.002929408283869462\n",
      "Iteration: 6070 lambda_k: 1 Loss: 0.00292940828386956\n",
      "Iteration: 6071 lambda_k: 1 Loss: 0.002929408283869659\n",
      "Iteration: 6072 lambda_k: 1 Loss: 0.002929408283869761\n",
      "Iteration: 6073 lambda_k: 1 Loss: 0.002929408283869859\n",
      "Iteration: 6074 lambda_k: 1 Loss: 0.002929408283869964\n",
      "Iteration: 6075 lambda_k: 1 Loss: 0.0029294082838700647\n",
      "Iteration: 6076 lambda_k: 1 Loss: 0.0029294082838701597\n",
      "Iteration: 6077 lambda_k: 1 Loss: 0.0029294082838702486\n",
      "Iteration: 6078 lambda_k: 1 Loss: 0.0029294082838703414\n",
      "Iteration: 6079 lambda_k: 1 Loss: 0.002929408283870449\n",
      "Iteration: 6080 lambda_k: 1 Loss: 0.0029294082838705444\n",
      "Iteration: 6081 lambda_k: 1 Loss: 0.0029294082838706324\n",
      "Iteration: 6082 lambda_k: 1 Loss: 0.002929408283870732\n",
      "Iteration: 6083 lambda_k: 1 Loss: 0.002929408283870824\n",
      "Iteration: 6084 lambda_k: 1 Loss: 0.0029294082838709156\n",
      "Iteration: 6085 lambda_k: 1 Loss: 0.00292940828387101\n",
      "Iteration: 6086 lambda_k: 1 Loss: 0.0029294082838711008\n",
      "Iteration: 6087 lambda_k: 1 Loss: 0.0029294082838712\n",
      "Iteration: 6088 lambda_k: 1 Loss: 0.0029294082838712946\n",
      "Iteration: 6089 lambda_k: 1 Loss: 0.002929408283871387\n",
      "Iteration: 6090 lambda_k: 1 Loss: 0.0029294082838714677\n",
      "Iteration: 6091 lambda_k: 1 Loss: 0.0029294082838715535\n",
      "Iteration: 6092 lambda_k: 1 Loss: 0.00292940828387164\n",
      "Iteration: 6093 lambda_k: 1 Loss: 0.0029294082838717344\n",
      "Iteration: 6094 lambda_k: 1 Loss: 0.002929408283871817\n",
      "Iteration: 6095 lambda_k: 1 Loss: 0.002929408283871906\n",
      "Iteration: 6096 lambda_k: 1 Loss: 0.002929408283871987\n",
      "Iteration: 6097 lambda_k: 1 Loss: 0.0029294082838720696\n",
      "Iteration: 6098 lambda_k: 1 Loss: 0.0029294082838721516\n",
      "Iteration: 6099 lambda_k: 1 Loss: 0.002929408283872238\n",
      "Iteration: 6100 lambda_k: 1 Loss: 0.002929408283872325\n",
      "Iteration: 6101 lambda_k: 1 Loss: 0.002929408283872434\n",
      "Iteration: 6102 lambda_k: 1 Loss: 0.0029294082838725167\n",
      "Iteration: 6103 lambda_k: 1 Loss: 0.0029294082838726056\n",
      "Iteration: 6104 lambda_k: 1 Loss: 0.002929408283872691\n",
      "Iteration: 6105 lambda_k: 1 Loss: 0.0029294082838727748\n",
      "Iteration: 6106 lambda_k: 1 Loss: 0.002929408283872858\n",
      "Iteration: 6107 lambda_k: 1 Loss: 0.002929408283872943\n",
      "Iteration: 6108 lambda_k: 1 Loss: 0.002929408283873026\n",
      "Iteration: 6109 lambda_k: 1 Loss: 0.002929408283873107\n",
      "Iteration: 6110 lambda_k: 1 Loss: 0.002929408283873194\n",
      "Iteration: 6111 lambda_k: 1 Loss: 0.002929408283873285\n",
      "Iteration: 6112 lambda_k: 1 Loss: 0.0029294082838733633\n",
      "Iteration: 6113 lambda_k: 1 Loss: 0.0029294082838734426\n",
      "Iteration: 6114 lambda_k: 1 Loss: 0.0029294082838735324\n",
      "Iteration: 6115 lambda_k: 1 Loss: 0.0029294082838736144\n",
      "Iteration: 6116 lambda_k: 1 Loss: 0.002929408283873703\n",
      "Iteration: 6117 lambda_k: 1 Loss: 0.002929408283873786\n",
      "Iteration: 6118 lambda_k: 1 Loss: 0.0029294082838738846\n",
      "Iteration: 6119 lambda_k: 1 Loss: 0.0029294082838739652\n",
      "Iteration: 6120 lambda_k: 1 Loss: 0.0029294082838740446\n",
      "Iteration: 6121 lambda_k: 1 Loss: 0.002929408283874122\n",
      "Iteration: 6122 lambda_k: 1 Loss: 0.0029294082838741964\n",
      "Iteration: 6123 lambda_k: 1 Loss: 0.00292940828387428\n",
      "Iteration: 6124 lambda_k: 1 Loss: 0.002929408283874355\n",
      "Iteration: 6125 lambda_k: 1 Loss: 0.002929408283874429\n",
      "Iteration: 6126 lambda_k: 1 Loss: 0.0029294082838745065\n",
      "Iteration: 6127 lambda_k: 1 Loss: 0.0029294082838745863\n",
      "Iteration: 6128 lambda_k: 1 Loss: 0.002929408283874663\n",
      "Iteration: 6129 lambda_k: 1 Loss: 0.0029294082838747333\n",
      "Iteration: 6130 lambda_k: 1 Loss: 0.0029294082838748022\n",
      "Iteration: 6131 lambda_k: 1 Loss: 0.0029294082838748742\n",
      "Iteration: 6132 lambda_k: 1 Loss: 0.0029294082838749475\n",
      "Iteration: 6133 lambda_k: 1 Loss: 0.0029294082838750273\n",
      "Iteration: 6134 lambda_k: 1 Loss: 0.0029294082838750967\n",
      "Iteration: 6135 lambda_k: 1 Loss: 0.0029294082838751644\n",
      "Iteration: 6136 lambda_k: 1 Loss: 0.0029294082838752346\n",
      "Iteration: 6137 lambda_k: 1 Loss: 0.002929408283875309\n",
      "Iteration: 6138 lambda_k: 1 Loss: 0.0029294082838753825\n",
      "Iteration: 6139 lambda_k: 1 Loss: 0.002929408283875453\n",
      "Iteration: 6140 lambda_k: 1 Loss: 0.00292940828387552\n",
      "Iteration: 6141 lambda_k: 1 Loss: 0.0029294082838755885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6142 lambda_k: 1 Loss: 0.002929408283875656\n",
      "Iteration: 6143 lambda_k: 1 Loss: 0.00292940828387573\n",
      "Iteration: 6144 lambda_k: 1 Loss: 0.002929408283875795\n",
      "Iteration: 6145 lambda_k: 1 Loss: 0.002929408283875868\n",
      "Iteration: 6146 lambda_k: 1 Loss: 0.0029294082838759363\n",
      "Iteration: 6147 lambda_k: 1 Loss: 0.0029294082838760022\n",
      "Iteration: 6148 lambda_k: 1 Loss: 0.002929408283876067\n",
      "Iteration: 6149 lambda_k: 1 Loss: 0.002929408283876129\n",
      "Iteration: 6150 lambda_k: 1 Loss: 0.0029294082838761957\n",
      "Iteration: 6151 lambda_k: 1 Loss: 0.0029294082838762655\n",
      "Iteration: 6152 lambda_k: 1 Loss: 0.002929408283876328\n",
      "Iteration: 6153 lambda_k: 1 Loss: 0.0029294082838763895\n",
      "Iteration: 6154 lambda_k: 1 Loss: 0.0029294082838764576\n",
      "Iteration: 6155 lambda_k: 1 Loss: 0.002929408283876523\n",
      "Iteration: 6156 lambda_k: 1 Loss: 0.002929408283876588\n",
      "Iteration: 6157 lambda_k: 1 Loss: 0.0029294082838766584\n",
      "Iteration: 6158 lambda_k: 1 Loss: 0.0029294082838767156\n",
      "Iteration: 6159 lambda_k: 1 Loss: 0.002929408283876777\n",
      "Iteration: 6160 lambda_k: 1 Loss: 0.0029294082838768436\n",
      "Iteration: 6161 lambda_k: 1 Loss: 0.0029294082838769346\n",
      "Iteration: 6162 lambda_k: 1 Loss: 0.002929408283877001\n",
      "Iteration: 6163 lambda_k: 1 Loss: 0.0029294082838770647\n",
      "Iteration: 6164 lambda_k: 1 Loss: 0.002929408283877121\n",
      "Iteration: 6165 lambda_k: 1 Loss: 0.002929408283877185\n",
      "Iteration: 6166 lambda_k: 1 Loss: 0.0029294082838772564\n",
      "Iteration: 6167 lambda_k: 1 Loss: 0.002929408283877325\n",
      "Iteration: 6168 lambda_k: 1 Loss: 0.0029294082838773874\n",
      "Iteration: 6169 lambda_k: 1 Loss: 0.002929408283877456\n",
      "Iteration: 6170 lambda_k: 1 Loss: 0.002929408283877522\n",
      "Iteration: 6171 lambda_k: 1 Loss: 0.002929408283877588\n",
      "Iteration: 6172 lambda_k: 1 Loss: 0.0029294082838776554\n",
      "Iteration: 6173 lambda_k: 1 Loss: 0.002929408283877713\n",
      "Iteration: 6174 lambda_k: 1 Loss: 0.002929408283877778\n",
      "Iteration: 6175 lambda_k: 1 Loss: 0.0029294082838778393\n",
      "Iteration: 6176 lambda_k: 1 Loss: 0.002929408283877898\n",
      "Iteration: 6177 lambda_k: 1 Loss: 0.002929408283877955\n",
      "Iteration: 6178 lambda_k: 1 Loss: 0.0029294082838780097\n",
      "Iteration: 6179 lambda_k: 1 Loss: 0.002929408283878068\n",
      "Iteration: 6180 lambda_k: 1 Loss: 0.0029294082838781273\n",
      "Iteration: 6181 lambda_k: 1 Loss: 0.0029294082838781867\n",
      "Iteration: 6182 lambda_k: 1 Loss: 0.002929408283878249\n",
      "Iteration: 6183 lambda_k: 1 Loss: 0.002929408283878304\n",
      "Iteration: 6184 lambda_k: 1 Loss: 0.0029294082838783563\n",
      "Iteration: 6185 lambda_k: 1 Loss: 0.0029294082838784126\n",
      "Iteration: 6186 lambda_k: 1 Loss: 0.0029294082838784686\n",
      "Iteration: 6187 lambda_k: 1 Loss: 0.0029294082838785184\n",
      "Iteration: 6188 lambda_k: 1 Loss: 0.00292940828387857\n",
      "Iteration: 6189 lambda_k: 1 Loss: 0.002929408283878621\n",
      "Iteration: 6190 lambda_k: 1 Loss: 0.0029294082838786724\n",
      "Iteration: 6191 lambda_k: 1 Loss: 0.0029294082838787205\n",
      "Iteration: 6192 lambda_k: 1 Loss: 0.0029294082838787695\n",
      "Iteration: 6193 lambda_k: 1 Loss: 0.00292940828387882\n",
      "Iteration: 6194 lambda_k: 1 Loss: 0.002929408283878868\n",
      "Iteration: 6195 lambda_k: 1 Loss: 0.0029294082838789135\n",
      "Iteration: 6196 lambda_k: 1 Loss: 0.0029294082838789612\n",
      "Iteration: 6197 lambda_k: 1 Loss: 0.002929408283879008\n",
      "Iteration: 6198 lambda_k: 1 Loss: 0.00292940828387906\n",
      "Iteration: 6199 lambda_k: 1 Loss: 0.002929408283879111\n",
      "Iteration: 6200 lambda_k: 1 Loss: 0.0029294082838791607\n",
      "Iteration: 6201 lambda_k: 1 Loss: 0.002929408283879209\n",
      "Iteration: 6202 lambda_k: 1 Loss: 0.002929408283879262\n",
      "Iteration: 6203 lambda_k: 1 Loss: 0.00292940828387931\n",
      "Iteration: 6204 lambda_k: 1 Loss: 0.002929408283879358\n",
      "Iteration: 6205 lambda_k: 1 Loss: 0.002929408283879429\n",
      "Iteration: 6206 lambda_k: 1 Loss: 0.002929408283879478\n",
      "Iteration: 6207 lambda_k: 1 Loss: 0.002929408283879529\n",
      "Iteration: 6208 lambda_k: 1 Loss: 0.002929408283879578\n",
      "Iteration: 6209 lambda_k: 1 Loss: 0.00292940828387963\n",
      "Iteration: 6210 lambda_k: 1 Loss: 0.002929408283879676\n",
      "Iteration: 6211 lambda_k: 1 Loss: 0.002929408283879727\n",
      "Iteration: 6212 lambda_k: 1 Loss: 0.002929408283879777\n",
      "Iteration: 6213 lambda_k: 1 Loss: 0.0029294082838798234\n",
      "Iteration: 6214 lambda_k: 1 Loss: 0.0029294082838798694\n",
      "Iteration: 6215 lambda_k: 1 Loss: 0.0029294082838799136\n",
      "Iteration: 6216 lambda_k: 1 Loss: 0.0029294082838799587\n",
      "Iteration: 6217 lambda_k: 1 Loss: 0.0029294082838800042\n",
      "Iteration: 6218 lambda_k: 1 Loss: 0.00292940828388005\n",
      "Iteration: 6219 lambda_k: 1 Loss: 0.0029294082838800927\n",
      "Iteration: 6220 lambda_k: 1 Loss: 0.0029294082838801326\n",
      "Iteration: 6221 lambda_k: 1 Loss: 0.0029294082838801747\n",
      "Iteration: 6222 lambda_k: 1 Loss: 0.0029294082838802163\n",
      "Iteration: 6223 lambda_k: 1 Loss: 0.0029294082838802575\n",
      "Iteration: 6224 lambda_k: 1 Loss: 0.0029294082838803\n",
      "Iteration: 6225 lambda_k: 1 Loss: 0.002929408283880345\n",
      "Iteration: 6226 lambda_k: 1 Loss: 0.002929408283880386\n",
      "Iteration: 6227 lambda_k: 1 Loss: 0.002929408283880431\n",
      "Iteration: 6228 lambda_k: 1 Loss: 0.002929408283880475\n",
      "Iteration: 6229 lambda_k: 1 Loss: 0.002929408283880523\n",
      "Iteration: 6230 lambda_k: 1 Loss: 0.0029294082838805667\n",
      "Iteration: 6231 lambda_k: 1 Loss: 0.0029294082838806283\n",
      "Iteration: 6232 lambda_k: 1 Loss: 0.0029294082838806678\n",
      "Iteration: 6233 lambda_k: 1 Loss: 0.002929408283880707\n",
      "Iteration: 6234 lambda_k: 1 Loss: 0.0029294082838807463\n",
      "Iteration: 6235 lambda_k: 1 Loss: 0.002929408283880789\n",
      "Iteration: 6236 lambda_k: 1 Loss: 0.002929408283880827\n",
      "Iteration: 6237 lambda_k: 1 Loss: 0.0029294082838808634\n",
      "Iteration: 6238 lambda_k: 1 Loss: 0.002929408283880901\n",
      "Iteration: 6239 lambda_k: 1 Loss: 0.00292940828388094\n",
      "Iteration: 6240 lambda_k: 1 Loss: 0.0029294082838809774\n",
      "Iteration: 6241 lambda_k: 1 Loss: 0.0029294082838810178\n",
      "Iteration: 6242 lambda_k: 1 Loss: 0.002929408283881056\n",
      "Iteration: 6243 lambda_k: 1 Loss: 0.002929408283881097\n",
      "Iteration: 6244 lambda_k: 1 Loss: 0.002929408283881135\n",
      "Iteration: 6245 lambda_k: 1 Loss: 0.002929408283881171\n",
      "Iteration: 6246 lambda_k: 1 Loss: 0.0029294082838812116\n",
      "Iteration: 6247 lambda_k: 1 Loss: 0.002929408283881248\n",
      "Iteration: 6248 lambda_k: 1 Loss: 0.0029294082838812858\n",
      "Iteration: 6249 lambda_k: 1 Loss: 0.002929408283881324\n",
      "Iteration: 6250 lambda_k: 1 Loss: 0.0029294082838813612\n",
      "Iteration: 6251 lambda_k: 1 Loss: 0.0029294082838813964\n",
      "Iteration: 6252 lambda_k: 1 Loss: 0.002929408283881432\n",
      "Iteration: 6253 lambda_k: 1 Loss: 0.0029294082838814688\n",
      "Iteration: 6254 lambda_k: 1 Loss: 0.002929408283881504\n",
      "Iteration: 6255 lambda_k: 1 Loss: 0.002929408283881539\n",
      "Iteration: 6256 lambda_k: 1 Loss: 0.0029294082838815755\n",
      "Iteration: 6257 lambda_k: 1 Loss: 0.00292940828388162\n",
      "Iteration: 6258 lambda_k: 1 Loss: 0.0029294082838816557\n",
      "Iteration: 6259 lambda_k: 1 Loss: 0.0029294082838816904\n",
      "Iteration: 6260 lambda_k: 1 Loss: 0.002929408283881724\n",
      "Iteration: 6261 lambda_k: 1 Loss: 0.0029294082838817606\n",
      "Iteration: 6262 lambda_k: 1 Loss: 0.002929408283881793\n",
      "Iteration: 6263 lambda_k: 1 Loss: 0.002929408283881827\n",
      "Iteration: 6264 lambda_k: 1 Loss: 0.002929408283881861\n",
      "Iteration: 6265 lambda_k: 1 Loss: 0.0029294082838818973\n",
      "Iteration: 6266 lambda_k: 1 Loss: 0.0029294082838819315\n",
      "Iteration: 6267 lambda_k: 1 Loss: 0.0029294082838819658\n",
      "Iteration: 6268 lambda_k: 1 Loss: 0.0029294082838819987\n",
      "Iteration: 6269 lambda_k: 1 Loss: 0.002929408283882032\n",
      "Iteration: 6270 lambda_k: 1 Loss: 0.0029294082838820647\n",
      "Iteration: 6271 lambda_k: 1 Loss: 0.002929408283882099\n",
      "Iteration: 6272 lambda_k: 1 Loss: 0.0029294082838821392\n",
      "Iteration: 6273 lambda_k: 1 Loss: 0.002929408283882176\n",
      "Iteration: 6274 lambda_k: 1 Loss: 0.002929408283882207\n",
      "Iteration: 6275 lambda_k: 1 Loss: 0.002929408283882237\n",
      "Iteration: 6276 lambda_k: 1 Loss: 0.0029294082838822685\n",
      "Iteration: 6277 lambda_k: 1 Loss: 0.0029294082838823\n",
      "Iteration: 6278 lambda_k: 1 Loss: 0.002929408283882337\n",
      "Iteration: 6279 lambda_k: 1 Loss: 0.0029294082838823713\n",
      "Iteration: 6280 lambda_k: 1 Loss: 0.002929408283882406\n",
      "Iteration: 6281 lambda_k: 1 Loss: 0.002929408283882438\n",
      "Iteration: 6282 lambda_k: 1 Loss: 0.0029294082838824723\n",
      "Iteration: 6283 lambda_k: 1 Loss: 0.002929408283882506\n",
      "Iteration: 6284 lambda_k: 1 Loss: 0.002929408283882539\n",
      "Iteration: 6285 lambda_k: 1 Loss: 0.002929408283882574\n",
      "Iteration: 6286 lambda_k: 1 Loss: 0.0029294082838826\n",
      "Iteration: 6287 lambda_k: 1 Loss: 0.002929408283882631\n",
      "Iteration: 6288 lambda_k: 1 Loss: 0.002929408283882661\n",
      "Iteration: 6289 lambda_k: 1 Loss: 0.002929408283882692\n",
      "Iteration: 6290 lambda_k: 1 Loss: 0.0029294082838827225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6291 lambda_k: 1 Loss: 0.002929408283882757\n",
      "Iteration: 6292 lambda_k: 1 Loss: 0.002929408283882789\n",
      "Iteration: 6293 lambda_k: 1 Loss: 0.0029294082838828214\n",
      "Iteration: 6294 lambda_k: 1 Loss: 0.002929408283882849\n",
      "Iteration: 6295 lambda_k: 1 Loss: 0.0029294082838828826\n",
      "Iteration: 6296 lambda_k: 1 Loss: 0.002929408283882911\n",
      "Iteration: 6297 lambda_k: 1 Loss: 0.0029294082838829377\n",
      "Iteration: 6298 lambda_k: 1 Loss: 0.0029294082838829693\n",
      "Iteration: 6299 lambda_k: 1 Loss: 0.0029294082838829958\n",
      "Iteration: 6300 lambda_k: 1 Loss: 0.0029294082838830253\n",
      "Iteration: 6301 lambda_k: 1 Loss: 0.002929408283883055\n",
      "Iteration: 6302 lambda_k: 1 Loss: 0.0029294082838830847\n",
      "Iteration: 6303 lambda_k: 1 Loss: 0.0029294082838831116\n",
      "Iteration: 6304 lambda_k: 1 Loss: 0.0029294082838831406\n",
      "Iteration: 6305 lambda_k: 1 Loss: 0.0029294082838831675\n",
      "Iteration: 6306 lambda_k: 1 Loss: 0.0029294082838831966\n",
      "Iteration: 6307 lambda_k: 1 Loss: 0.0029294082838832248\n",
      "Iteration: 6308 lambda_k: 1 Loss: 0.0029294082838832516\n",
      "Iteration: 6309 lambda_k: 1 Loss: 0.002929408283883275\n",
      "Iteration: 6310 lambda_k: 1 Loss: 0.002929408283883321\n",
      "Iteration: 6311 lambda_k: 1 Loss: 0.0029294082838833514\n",
      "Iteration: 6312 lambda_k: 1 Loss: 0.0029294082838833804\n",
      "Iteration: 6313 lambda_k: 1 Loss: 0.0029294082838834086\n",
      "Iteration: 6314 lambda_k: 1 Loss: 0.0029294082838834373\n",
      "Iteration: 6315 lambda_k: 1 Loss: 0.0029294082838834646\n",
      "Iteration: 6316 lambda_k: 1 Loss: 0.0029294082838834915\n",
      "Iteration: 6317 lambda_k: 1 Loss: 0.0029294082838835175\n",
      "Iteration: 6318 lambda_k: 1 Loss: 0.0029294082838835452\n",
      "Iteration: 6319 lambda_k: 1 Loss: 0.002929408283883571\n",
      "Iteration: 6320 lambda_k: 1 Loss: 0.002929408283883595\n",
      "Iteration: 6321 lambda_k: 1 Loss: 0.0029294082838836185\n",
      "Iteration: 6322 lambda_k: 1 Loss: 0.002929408283883643\n",
      "Iteration: 6323 lambda_k: 1 Loss: 0.002929408283883667\n",
      "Iteration: 6324 lambda_k: 1 Loss: 0.002929408283883692\n",
      "Iteration: 6325 lambda_k: 1 Loss: 0.002929408283883716\n",
      "Iteration: 6326 lambda_k: 1 Loss: 0.0029294082838837413\n",
      "Iteration: 6327 lambda_k: 1 Loss: 0.002929408283883768\n",
      "Iteration: 6328 lambda_k: 1 Loss: 0.002929408283883792\n",
      "Iteration: 6329 lambda_k: 1 Loss: 0.0029294082838838115\n",
      "Iteration: 6330 lambda_k: 1 Loss: 0.002929408283883834\n",
      "Iteration: 6331 lambda_k: 1 Loss: 0.0029294082838838605\n",
      "Iteration: 6332 lambda_k: 1 Loss: 0.002929408283883887\n",
      "Iteration: 6333 lambda_k: 1 Loss: 0.0029294082838839143\n",
      "Iteration: 6334 lambda_k: 1 Loss: 0.002929408283883945\n",
      "Iteration: 6335 lambda_k: 1 Loss: 0.0029294082838839694\n",
      "Iteration: 6336 lambda_k: 1 Loss: 0.0029294082838839937\n",
      "Iteration: 6337 lambda_k: 1 Loss: 0.0029294082838840145\n",
      "Iteration: 6338 lambda_k: 1 Loss: 0.0029294082838840383\n",
      "Iteration: 6339 lambda_k: 1 Loss: 0.00292940828388406\n",
      "Iteration: 6340 lambda_k: 1 Loss: 0.0029294082838840813\n",
      "Iteration: 6341 lambda_k: 1 Loss: 0.0029294082838841012\n",
      "Iteration: 6342 lambda_k: 1 Loss: 0.002929408283884125\n",
      "Iteration: 6343 lambda_k: 1 Loss: 0.0029294082838841494\n",
      "Iteration: 6344 lambda_k: 1 Loss: 0.0029294082838841723\n",
      "Iteration: 6345 lambda_k: 1 Loss: 0.0029294082838841953\n",
      "Iteration: 6346 lambda_k: 1 Loss: 0.0029294082838842174\n",
      "Iteration: 6347 lambda_k: 1 Loss: 0.00292940828388424\n",
      "Iteration: 6348 lambda_k: 1 Loss: 0.0029294082838842617\n",
      "Iteration: 6349 lambda_k: 1 Loss: 0.002929408283884283\n",
      "Iteration: 6350 lambda_k: 1 Loss: 0.0029294082838843038\n",
      "Iteration: 6351 lambda_k: 1 Loss: 0.002929408283884327\n",
      "Iteration: 6352 lambda_k: 1 Loss: 0.002929408283884345\n",
      "Iteration: 6353 lambda_k: 1 Loss: 0.0029294082838843627\n",
      "Iteration: 6354 lambda_k: 1 Loss: 0.002929408283884383\n",
      "Iteration: 6355 lambda_k: 1 Loss: 0.002929408283884403\n",
      "Iteration: 6356 lambda_k: 1 Loss: 0.002929408283884422\n",
      "Iteration: 6357 lambda_k: 1 Loss: 0.002929408283884441\n",
      "Iteration: 6358 lambda_k: 1 Loss: 0.002929408283884467\n",
      "Iteration: 6359 lambda_k: 1 Loss: 0.0029294082838844868\n",
      "Iteration: 6360 lambda_k: 1 Loss: 0.0029294082838845124\n",
      "Iteration: 6361 lambda_k: 1 Loss: 0.0029294082838845327\n",
      "Iteration: 6362 lambda_k: 1 Loss: 0.0029294082838845588\n",
      "Iteration: 6363 lambda_k: 1 Loss: 0.0029294082838845796\n",
      "Iteration: 6364 lambda_k: 1 Loss: 0.002929408283884602\n",
      "Iteration: 6365 lambda_k: 1 Loss: 0.0029294082838846255\n",
      "Iteration: 6366 lambda_k: 1 Loss: 0.00292940828388465\n",
      "Iteration: 6367 lambda_k: 1 Loss: 0.0029294082838846715\n",
      "Iteration: 6368 lambda_k: 1 Loss: 0.0029294082838846936\n",
      "Iteration: 6369 lambda_k: 1 Loss: 0.002929408283884712\n",
      "Iteration: 6370 lambda_k: 1 Loss: 0.0029294082838847322\n",
      "Iteration: 6371 lambda_k: 1 Loss: 0.002929408283884753\n",
      "Iteration: 6372 lambda_k: 1 Loss: 0.0029294082838847713\n",
      "Iteration: 6373 lambda_k: 1 Loss: 0.0029294082838847877\n",
      "Iteration: 6374 lambda_k: 1 Loss: 0.0029294082838848025\n",
      "Iteration: 6375 lambda_k: 1 Loss: 0.002929408283884822\n",
      "Iteration: 6376 lambda_k: 1 Loss: 0.002929408283884842\n",
      "Iteration: 6377 lambda_k: 1 Loss: 0.002929408283884861\n",
      "Iteration: 6378 lambda_k: 1 Loss: 0.0029294082838848797\n",
      "Iteration: 6379 lambda_k: 1 Loss: 0.002929408283884898\n",
      "Iteration: 6380 lambda_k: 1 Loss: 0.0029294082838849165\n",
      "Iteration: 6381 lambda_k: 1 Loss: 0.002929408283884938\n",
      "Iteration: 6382 lambda_k: 1 Loss: 0.00292940828388496\n",
      "Iteration: 6383 lambda_k: 1 Loss: 0.002929408283884983\n",
      "Iteration: 6384 lambda_k: 1 Loss: 0.0029294082838850046\n",
      "Iteration: 6385 lambda_k: 1 Loss: 0.0029294082838850237\n",
      "Iteration: 6386 lambda_k: 1 Loss: 0.0029294082838850427\n",
      "Iteration: 6387 lambda_k: 1 Loss: 0.0029294082838850653\n",
      "Iteration: 6388 lambda_k: 1 Loss: 0.002929408283885083\n",
      "Iteration: 6389 lambda_k: 1 Loss: 0.0029294082838851074\n",
      "Iteration: 6390 lambda_k: 1 Loss: 0.002929408283885125\n",
      "Iteration: 6391 lambda_k: 1 Loss: 0.002929408283885151\n",
      "Iteration: 6392 lambda_k: 1 Loss: 0.002929408283885171\n",
      "Iteration: 6393 lambda_k: 1 Loss: 0.0029294082838851924\n",
      "Iteration: 6394 lambda_k: 1 Loss: 0.002929408283885212\n",
      "Iteration: 6395 lambda_k: 1 Loss: 0.002929408283885231\n",
      "Iteration: 6396 lambda_k: 1 Loss: 0.0029294082838852513\n",
      "Iteration: 6397 lambda_k: 1 Loss: 0.0029294082838852674\n",
      "Iteration: 6398 lambda_k: 1 Loss: 0.002929408283885282\n",
      "Iteration: 6399 lambda_k: 1 Loss: 0.002929408283885298\n",
      "Iteration: 6400 lambda_k: 1 Loss: 0.002929408283885311\n",
      "Iteration: 6401 lambda_k: 1 Loss: 0.0029294082838853225\n",
      "Iteration: 6402 lambda_k: 1 Loss: 0.0029294082838853346\n",
      "Iteration: 6403 lambda_k: 1 Loss: 0.0029294082838853494\n",
      "Iteration: 6404 lambda_k: 1 Loss: 0.0029294082838853615\n",
      "Iteration: 6405 lambda_k: 1 Loss: 0.0029294082838853723\n",
      "Iteration: 6406 lambda_k: 1 Loss: 0.0029294082838853823\n",
      "Iteration: 6407 lambda_k: 1 Loss: 0.0029294082838853975\n",
      "Iteration: 6408 lambda_k: 1 Loss: 0.002929408283885413\n",
      "Iteration: 6409 lambda_k: 1 Loss: 0.0029294082838854257\n",
      "Iteration: 6410 lambda_k: 1 Loss: 0.0029294082838854374\n",
      "Iteration: 6411 lambda_k: 1 Loss: 0.0029294082838854534\n",
      "Iteration: 6412 lambda_k: 1 Loss: 0.002929408283885464\n",
      "Iteration: 6413 lambda_k: 1 Loss: 0.002929408283885477\n",
      "Iteration: 6414 lambda_k: 1 Loss: 0.00292940828388549\n",
      "Iteration: 6415 lambda_k: 1 Loss: 0.002929408283885504\n",
      "Iteration: 6416 lambda_k: 1 Loss: 0.0029294082838855168\n",
      "Iteration: 6417 lambda_k: 1 Loss: 0.002929408283885534\n",
      "Iteration: 6418 lambda_k: 1 Loss: 0.002929408283885551\n",
      "Iteration: 6419 lambda_k: 1 Loss: 0.0029294082838855697\n",
      "Iteration: 6420 lambda_k: 1 Loss: 0.0029294082838855844\n",
      "Iteration: 6421 lambda_k: 1 Loss: 0.0029294082838855987\n",
      "Iteration: 6422 lambda_k: 1 Loss: 0.0029294082838856135\n",
      "Iteration: 6423 lambda_k: 1 Loss: 0.0029294082838856312\n",
      "Iteration: 6424 lambda_k: 1 Loss: 0.0029294082838856464\n",
      "Iteration: 6425 lambda_k: 1 Loss: 0.0029294082838856594\n",
      "Iteration: 6426 lambda_k: 1 Loss: 0.002929408283885674\n",
      "Iteration: 6427 lambda_k: 1 Loss: 0.0029294082838856885\n",
      "Iteration: 6428 lambda_k: 1 Loss: 0.0029294082838857245\n",
      "Iteration: 6429 lambda_k: 1 Loss: 0.002929408283885739\n",
      "Iteration: 6430 lambda_k: 1 Loss: 0.002929408283885753\n",
      "Iteration: 6431 lambda_k: 1 Loss: 0.0029294082838857705\n",
      "Iteration: 6432 lambda_k: 1 Loss: 0.002929408283885785\n",
      "Iteration: 6433 lambda_k: 1 Loss: 0.0029294082838857986\n",
      "Iteration: 6434 lambda_k: 1 Loss: 0.002929408283885813\n",
      "Iteration: 6435 lambda_k: 1 Loss: 0.0029294082838858273\n",
      "Iteration: 6436 lambda_k: 1 Loss: 0.002929408283885843\n",
      "Iteration: 6437 lambda_k: 1 Loss: 0.002929408283885856\n",
      "Iteration: 6438 lambda_k: 1 Loss: 0.002929408283885868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6439 lambda_k: 1 Loss: 0.0029294082838858836\n",
      "Iteration: 6440 lambda_k: 1 Loss: 0.0029294082838859014\n",
      "Iteration: 6441 lambda_k: 1 Loss: 0.002929408283885915\n",
      "Iteration: 6442 lambda_k: 1 Loss: 0.002929408283885928\n",
      "Iteration: 6443 lambda_k: 1 Loss: 0.002929408283885941\n",
      "Iteration: 6444 lambda_k: 1 Loss: 0.0029294082838859587\n",
      "Iteration: 6445 lambda_k: 1 Loss: 0.0029294082838859747\n",
      "Iteration: 6446 lambda_k: 1 Loss: 0.002929408283885988\n",
      "Iteration: 6447 lambda_k: 1 Loss: 0.002929408283886003\n",
      "Iteration: 6448 lambda_k: 1 Loss: 0.002929408283886017\n",
      "Iteration: 6449 lambda_k: 1 Loss: 0.0029294082838860324\n",
      "Iteration: 6450 lambda_k: 1 Loss: 0.002929408283886046\n",
      "Iteration: 6451 lambda_k: 1 Loss: 0.002929408283886061\n",
      "Iteration: 6452 lambda_k: 1 Loss: 0.0029294082838860736\n",
      "Iteration: 6453 lambda_k: 1 Loss: 0.002929408283886083\n",
      "Iteration: 6454 lambda_k: 1 Loss: 0.002929408283886098\n",
      "Iteration: 6455 lambda_k: 1 Loss: 0.0029294082838861105\n",
      "Iteration: 6456 lambda_k: 1 Loss: 0.0029294082838861235\n",
      "Iteration: 6457 lambda_k: 1 Loss: 0.00292940828388614\n",
      "Iteration: 6458 lambda_k: 1 Loss: 0.0029294082838861556\n",
      "Iteration: 6459 lambda_k: 1 Loss: 0.002929408283886171\n",
      "Iteration: 6460 lambda_k: 1 Loss: 0.002929408283886188\n",
      "Iteration: 6461 lambda_k: 1 Loss: 0.002929408283886202\n",
      "Iteration: 6462 lambda_k: 1 Loss: 0.002929408283886214\n",
      "Iteration: 6463 lambda_k: 1 Loss: 0.002929408283886227\n",
      "Iteration: 6464 lambda_k: 1 Loss: 0.0029294082838862414\n",
      "Iteration: 6465 lambda_k: 1 Loss: 0.0029294082838862544\n",
      "Iteration: 6466 lambda_k: 1 Loss: 0.00292940828388627\n",
      "Iteration: 6467 lambda_k: 1 Loss: 0.0029294082838862857\n",
      "Iteration: 6468 lambda_k: 1 Loss: 0.002929408283886299\n",
      "Iteration: 6469 lambda_k: 1 Loss: 0.0029294082838863087\n",
      "Iteration: 6470 lambda_k: 1 Loss: 0.0029294082838863204\n",
      "Iteration: 6471 lambda_k: 1 Loss: 0.0029294082838863355\n",
      "Iteration: 6472 lambda_k: 1 Loss: 0.0029294082838863486\n",
      "Iteration: 6473 lambda_k: 1 Loss: 0.00292940828388636\n",
      "Iteration: 6474 lambda_k: 1 Loss: 0.002929408283886365\n",
      "Iteration: 6475 lambda_k: 1 Loss: 0.002929408283886371\n",
      "Iteration: 6476 lambda_k: 1 Loss: 0.0029294082838863776\n",
      "Iteration: 6477 lambda_k: 1 Loss: 0.002929408283886387\n",
      "Iteration: 6478 lambda_k: 1 Loss: 0.0029294082838863945\n",
      "Iteration: 6479 lambda_k: 1 Loss: 0.0029294082838864\n",
      "Iteration: 6480 lambda_k: 1 Loss: 0.002929408283886405\n",
      "Iteration: 6481 lambda_k: 1 Loss: 0.0029294082838864106\n",
      "Iteration: 6482 lambda_k: 1 Loss: 0.00292940828388642\n",
      "Iteration: 6483 lambda_k: 1 Loss: 0.0029294082838864383\n",
      "Iteration: 6484 lambda_k: 1 Loss: 0.002929408283886453\n",
      "Iteration: 6485 lambda_k: 1 Loss: 0.0029294082838864656\n",
      "Iteration: 6486 lambda_k: 1 Loss: 0.002929408283886468\n",
      "Iteration: 6487 lambda_k: 1 Loss: 0.0029294082838864726\n",
      "Iteration: 6488 lambda_k: 1 Loss: 0.0029294082838864865\n",
      "Iteration: 6489 lambda_k: 1 Loss: 0.0029294082838864995\n",
      "Iteration: 6490 lambda_k: 1 Loss: 0.002929408283886505\n",
      "Iteration: 6491 lambda_k: 1 Loss: 0.00292940828388651\n",
      "Iteration: 6492 lambda_k: 1 Loss: 0.00292940828388652\n",
      "Iteration: 6493 lambda_k: 1 Loss: 0.0029294082838865294\n",
      "Iteration: 6494 lambda_k: 1 Loss: 0.002929408283886542\n",
      "Iteration: 6495 lambda_k: 1 Loss: 0.0029294082838865537\n",
      "Iteration: 6496 lambda_k: 1 Loss: 0.0029294082838865585\n",
      "Iteration: 6497 lambda_k: 1 Loss: 0.002929408283886564\n",
      "Iteration: 6498 lambda_k: 1 Loss: 0.002929408283886576\n",
      "Iteration: 6499 lambda_k: 1 Loss: 0.0029294082838865853\n",
      "Iteration: 6500 lambda_k: 1 Loss: 0.0029294082838865966\n",
      "Iteration: 6501 lambda_k: 1 Loss: 0.0029294082838866057\n",
      "Iteration: 6502 lambda_k: 1 Loss: 0.002929408283886612\n",
      "Iteration: 6503 lambda_k: 1 Loss: 0.0029294082838866183\n",
      "Iteration: 6504 lambda_k: 1 Loss: 0.0029294082838866287\n",
      "Iteration: 6505 lambda_k: 1 Loss: 0.00292940828388664\n",
      "Iteration: 6506 lambda_k: 1 Loss: 0.002929408283886654\n",
      "Iteration: 6507 lambda_k: 1 Loss: 0.00292940828388666\n",
      "Iteration: 6508 lambda_k: 1 Loss: 0.0029294082838866656\n",
      "Iteration: 6509 lambda_k: 1 Loss: 0.0029294082838866708\n",
      "Iteration: 6510 lambda_k: 1 Loss: 0.0029294082838866747\n",
      "Iteration: 6511 lambda_k: 1 Loss: 0.0029294082838866894\n",
      "Iteration: 6512 lambda_k: 1 Loss: 0.002929408283886704\n",
      "Iteration: 6513 lambda_k: 1 Loss: 0.0029294082838867137\n",
      "Iteration: 6514 lambda_k: 1 Loss: 0.0029294082838867176\n",
      "Iteration: 6515 lambda_k: 1 Loss: 0.0029294082838867224\n",
      "Iteration: 6516 lambda_k: 1 Loss: 0.0029294082838867267\n",
      "Iteration: 6517 lambda_k: 1 Loss: 0.002929408283886739\n",
      "Iteration: 6518 lambda_k: 1 Loss: 0.0029294082838867558\n",
      "Iteration: 6519 lambda_k: 1 Loss: 0.002929408283886766\n",
      "Iteration: 6520 lambda_k: 1 Loss: 0.002929408283886774\n",
      "Iteration: 6521 lambda_k: 1 Loss: 0.0029294082838867788\n",
      "Iteration: 6522 lambda_k: 1 Loss: 0.0029294082838867913\n",
      "Iteration: 6523 lambda_k: 1 Loss: 0.002929408283886799\n",
      "Iteration: 6524 lambda_k: 1 Loss: 0.0029294082838868117\n",
      "Iteration: 6525 lambda_k: 1 Loss: 0.002929408283886822\n",
      "Iteration: 6526 lambda_k: 1 Loss: 0.002929408283886826\n",
      "Iteration: 6527 lambda_k: 1 Loss: 0.0029294082838868317\n",
      "Iteration: 6528 lambda_k: 1 Loss: 0.0029294082838868373\n",
      "Iteration: 6529 lambda_k: 1 Loss: 0.0029294082838868443\n",
      "Iteration: 6530 lambda_k: 1 Loss: 0.0029294082838868577\n",
      "Iteration: 6531 lambda_k: 1 Loss: 0.00292940828388687\n",
      "Iteration: 6532 lambda_k: 1 Loss: 0.0029294082838868776\n",
      "Iteration: 6533 lambda_k: 1 Loss: 0.00292940828388689\n",
      "Iteration: 6534 lambda_k: 1 Loss: 0.002929408283886898\n",
      "Iteration: 6535 lambda_k: 1 Loss: 0.002929408283886911\n",
      "Iteration: 6536 lambda_k: 1 Loss: 0.00292940828388692\n",
      "Iteration: 6537 lambda_k: 1 Loss: 0.00292940828388693\n",
      "Iteration: 6538 lambda_k: 1 Loss: 0.002929408283886942\n",
      "Iteration: 6539 lambda_k: 1 Loss: 0.002929408283886954\n",
      "Iteration: 6540 lambda_k: 1 Loss: 0.0029294082838869657\n",
      "Iteration: 6541 lambda_k: 1 Loss: 0.002929408283886977\n",
      "Iteration: 6542 lambda_k: 1 Loss: 0.0029294082838869874\n",
      "Iteration: 6543 lambda_k: 1 Loss: 0.0029294082838869943\n",
      "Iteration: 6544 lambda_k: 1 Loss: 0.002929408283887004\n",
      "Iteration: 6545 lambda_k: 1 Loss: 0.002929408283887013\n",
      "Iteration: 6546 lambda_k: 1 Loss: 0.0029294082838870255\n",
      "Iteration: 6547 lambda_k: 1 Loss: 0.0029294082838870307\n",
      "Iteration: 6548 lambda_k: 1 Loss: 0.0029294082838870377\n",
      "Iteration: 6549 lambda_k: 1 Loss: 0.0029294082838870468\n",
      "Iteration: 6550 lambda_k: 1 Loss: 0.002929408283887053\n",
      "Iteration: 6551 lambda_k: 1 Loss: 0.002929408283887064\n",
      "Iteration: 6552 lambda_k: 1 Loss: 0.0029294082838870776\n",
      "Iteration: 6553 lambda_k: 1 Loss: 0.002929408283887085\n",
      "Iteration: 6554 lambda_k: 1 Loss: 0.0029294082838870875\n",
      "Iteration: 6555 lambda_k: 1 Loss: 0.002929408283887091\n",
      "Iteration: 6556 lambda_k: 1 Loss: 0.0029294082838870967\n",
      "Iteration: 6557 lambda_k: 1 Loss: 0.0029294082838871027\n",
      "Iteration: 6558 lambda_k: 1 Loss: 0.002929408283887116\n",
      "Iteration: 6559 lambda_k: 1 Loss: 0.0029294082838871244\n",
      "Iteration: 6560 lambda_k: 1 Loss: 0.002929408283887135\n",
      "Iteration: 6561 lambda_k: 1 Loss: 0.0029294082838871457\n",
      "Iteration: 6562 lambda_k: 1 Loss: 0.002929408283887152\n",
      "Iteration: 6563 lambda_k: 1 Loss: 0.0029294082838871656\n",
      "Iteration: 6564 lambda_k: 1 Loss: 0.0029294082838871817\n",
      "Iteration: 6565 lambda_k: 1 Loss: 0.0029294082838871964\n",
      "Iteration: 6566 lambda_k: 1 Loss: 0.002929408283887203\n",
      "Iteration: 6567 lambda_k: 1 Loss: 0.00292940828388721\n",
      "Iteration: 6568 lambda_k: 1 Loss: 0.002929408283887214\n",
      "Iteration: 6569 lambda_k: 1 Loss: 0.002929408283887219\n",
      "Iteration: 6570 lambda_k: 1 Loss: 0.0029294082838872324\n",
      "Iteration: 6571 lambda_k: 1 Loss: 0.0029294082838872454\n",
      "Iteration: 6572 lambda_k: 1 Loss: 0.0029294082838872584\n",
      "Iteration: 6573 lambda_k: 1 Loss: 0.0029294082838872636\n",
      "Iteration: 6574 lambda_k: 1 Loss: 0.0029294082838872697\n",
      "Iteration: 6575 lambda_k: 1 Loss: 0.0029294082838872823\n",
      "Iteration: 6576 lambda_k: 1 Loss: 0.002929408283887289\n",
      "Iteration: 6577 lambda_k: 1 Loss: 0.002929408283887296\n",
      "Iteration: 6578 lambda_k: 1 Loss: 0.002929408283887304\n",
      "Iteration: 6579 lambda_k: 1 Loss: 0.002929408283887311\n",
      "Iteration: 6580 lambda_k: 1 Loss: 0.0029294082838873174\n",
      "Iteration: 6581 lambda_k: 1 Loss: 0.002929408283887325\n",
      "Iteration: 6582 lambda_k: 1 Loss: 0.0029294082838873304\n",
      "Iteration: 6583 lambda_k: 1 Loss: 0.002929408283887337\n",
      "Iteration: 6584 lambda_k: 1 Loss: 0.0029294082838873434\n",
      "Iteration: 6585 lambda_k: 1 Loss: 0.0029294082838873478\n",
      "Iteration: 6586 lambda_k: 1 Loss: 0.0029294082838873525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6587 lambda_k: 1 Loss: 0.0029294082838873577\n",
      "Iteration: 6588 lambda_k: 1 Loss: 0.002929408283887363\n",
      "Iteration: 6589 lambda_k: 1 Loss: 0.0029294082838873686\n",
      "Iteration: 6590 lambda_k: 1 Loss: 0.0029294082838873755\n",
      "Iteration: 6591 lambda_k: 1 Loss: 0.0029294082838873825\n",
      "Iteration: 6592 lambda_k: 1 Loss: 0.0029294082838873877\n",
      "Iteration: 6593 lambda_k: 1 Loss: 0.002929408283887393\n",
      "Iteration: 6594 lambda_k: 1 Loss: 0.0029294082838873963\n",
      "Iteration: 6595 lambda_k: 1 Loss: 0.0029294082838874015\n",
      "Iteration: 6596 lambda_k: 1 Loss: 0.0029294082838874063\n",
      "Iteration: 6597 lambda_k: 1 Loss: 0.002929408283887413\n",
      "Iteration: 6598 lambda_k: 1 Loss: 0.0029294082838874163\n",
      "Iteration: 6599 lambda_k: 1 Loss: 0.0029294082838874206\n",
      "Iteration: 6600 lambda_k: 1 Loss: 0.002929408283887425\n",
      "Iteration: 6601 lambda_k: 1 Loss: 0.0029294082838874297\n",
      "Iteration: 6602 lambda_k: 1 Loss: 0.002929408283887434\n",
      "Iteration: 6603 lambda_k: 1 Loss: 0.0029294082838874354\n",
      "Iteration: 6604 lambda_k: 1 Loss: 0.0029294082838874397\n",
      "Iteration: 6605 lambda_k: 1 Loss: 0.002929408283887441\n",
      "Iteration: 6606 lambda_k: 1 Loss: 0.002929408283887444\n",
      "Iteration: 6607 lambda_k: 1 Loss: 0.0029294082838874492\n",
      "Iteration: 6608 lambda_k: 1 Loss: 0.002929408283887451\n",
      "Iteration: 6609 lambda_k: 1 Loss: 0.00292940828388745\n",
      "Iteration: 6610 lambda_k: 1 Loss: 0.0029294082838874536\n",
      "Iteration: 6611 lambda_k: 1 Loss: 0.002929408283887459\n",
      "Iteration: 6612 lambda_k: 1 Loss: 0.002929408283887468\n",
      "Iteration: 6613 lambda_k: 1 Loss: 0.0029294082838874705\n",
      "Iteration: 6614 lambda_k: 1 Loss: 0.002929408283887475\n",
      "Iteration: 6615 lambda_k: 1 Loss: 0.0029294082838874796\n",
      "Iteration: 6616 lambda_k: 1 Loss: 0.002929408283887479\n",
      "Iteration: 6617 lambda_k: 1 Loss: 0.0029294082838874826\n",
      "Iteration: 6618 lambda_k: 1 Loss: 0.0029294082838875074\n",
      "Iteration: 6619 lambda_k: 1 Loss: 0.0029294082838875156\n",
      "Iteration: 6620 lambda_k: 1 Loss: 0.0029294082838875186\n",
      "Iteration: 6621 lambda_k: 1 Loss: 0.002929408283887523\n",
      "Iteration: 6622 lambda_k: 1 Loss: 0.002929408283887535\n",
      "Iteration: 6623 lambda_k: 1 Loss: 0.0029294082838875446\n",
      "Iteration: 6624 lambda_k: 1 Loss: 0.0029294082838875546\n",
      "Iteration: 6625 lambda_k: 1 Loss: 0.00292940828388756\n",
      "Iteration: 6626 lambda_k: 1 Loss: 0.0029294082838875663\n",
      "Iteration: 6627 lambda_k: 1 Loss: 0.0029294082838875724\n",
      "Iteration: 6628 lambda_k: 1 Loss: 0.0029294082838875785\n",
      "Iteration: 6629 lambda_k: 1 Loss: 0.002929408283887585\n",
      "Iteration: 6630 lambda_k: 1 Loss: 0.00292940828388759\n",
      "Iteration: 6631 lambda_k: 1 Loss: 0.00292940828388759\n",
      "Iteration: 6632 lambda_k: 1 Loss: 0.0029294082838875997\n",
      "Iteration: 6633 lambda_k: 1 Loss: 0.002929408283887604\n",
      "Iteration: 6634 lambda_k: 1 Loss: 0.0029294082838876093\n",
      "Iteration: 6635 lambda_k: 1 Loss: 0.0029294082838876127\n",
      "Iteration: 6636 lambda_k: 1 Loss: 0.0029294082838876197\n",
      "Iteration: 6637 lambda_k: 1 Loss: 0.0029294082838876288\n",
      "Iteration: 6638 lambda_k: 1 Loss: 0.002929408283887637\n",
      "Iteration: 6639 lambda_k: 1 Loss: 0.0029294082838876388\n",
      "Iteration: 6640 lambda_k: 1 Loss: 0.0029294082838876383\n",
      "Iteration: 6641 lambda_k: 1 Loss: 0.0029294082838876383\n",
      "Iteration: 6642 lambda_k: 1 Loss: 0.002929408283887639\n",
      "Iteration: 6643 lambda_k: 1 Loss: 0.002929408283887639\n",
      "Iteration: 6644 lambda_k: 1 Loss: 0.0029294082838876457\n",
      "Iteration: 6645 lambda_k: 1 Loss: 0.0029294082838876505\n",
      "Iteration: 6646 lambda_k: 1 Loss: 0.002929408283887656\n",
      "Iteration: 6647 lambda_k: 1 Loss: 0.002929408283887658\n",
      "Iteration: 6648 lambda_k: 1 Loss: 0.002929408283887664\n",
      "Iteration: 6649 lambda_k: 1 Loss: 0.00292940828388767\n",
      "Iteration: 6650 lambda_k: 1 Loss: 0.0029294082838876713\n",
      "Iteration: 6651 lambda_k: 1 Loss: 0.002929408283887676\n",
      "Iteration: 6652 lambda_k: 1 Loss: 0.0029294082838876826\n",
      "Iteration: 6653 lambda_k: 1 Loss: 0.0029294082838876856\n",
      "Iteration: 6654 lambda_k: 1 Loss: 0.00292940828388769\n",
      "Iteration: 6655 lambda_k: 1 Loss: 0.002929408283887694\n",
      "Iteration: 6656 lambda_k: 1 Loss: 0.0029294082838876973\n",
      "Iteration: 6657 lambda_k: 1 Loss: 0.0029294082838877\n",
      "Iteration: 6658 lambda_k: 1 Loss: 0.002929408283887703\n",
      "Iteration: 6659 lambda_k: 1 Loss: 0.0029294082838877086\n",
      "Iteration: 6660 lambda_k: 1 Loss: 0.0029294082838877094\n",
      "Iteration: 6661 lambda_k: 1 Loss: 0.0029294082838877073\n",
      "Iteration: 6662 lambda_k: 1 Loss: 0.002929408283887711\n",
      "Iteration: 6663 lambda_k: 1 Loss: 0.002929408283887715\n",
      "Iteration: 6664 lambda_k: 1 Loss: 0.0029294082838877147\n",
      "Iteration: 6665 lambda_k: 1 Loss: 0.0029294082838877133\n",
      "Iteration: 6666 lambda_k: 1 Loss: 0.002929408283887717\n",
      "Iteration: 6667 lambda_k: 1 Loss: 0.002929408283887719\n",
      "Iteration: 6668 lambda_k: 1 Loss: 0.002929408283887725\n",
      "Iteration: 6669 lambda_k: 1 Loss: 0.002929408283887727\n",
      "Iteration: 6670 lambda_k: 1 Loss: 0.0029294082838877316\n",
      "Iteration: 6671 lambda_k: 1 Loss: 0.0029294082838877363\n",
      "Iteration: 6672 lambda_k: 1 Loss: 0.002929408283887741\n",
      "Iteration: 6673 lambda_k: 1 Loss: 0.002929408283887744\n",
      "Iteration: 6674 lambda_k: 1 Loss: 0.002929408283887746\n",
      "Iteration: 6675 lambda_k: 1 Loss: 0.00292940828388775\n",
      "Iteration: 6676 lambda_k: 1 Loss: 0.0029294082838877532\n",
      "Iteration: 6677 lambda_k: 1 Loss: 0.0029294082838877563\n",
      "Iteration: 6678 lambda_k: 1 Loss: 0.0029294082838877598\n",
      "Iteration: 6679 lambda_k: 1 Loss: 0.0029294082838877606\n",
      "Iteration: 6680 lambda_k: 1 Loss: 0.002929408283887761\n",
      "Iteration: 6681 lambda_k: 1 Loss: 0.0029294082838877637\n",
      "Iteration: 6682 lambda_k: 1 Loss: 0.0029294082838877684\n",
      "Iteration: 6683 lambda_k: 1 Loss: 0.002929408283887771\n",
      "Iteration: 6684 lambda_k: 1 Loss: 0.0029294082838877767\n",
      "Iteration: 6685 lambda_k: 1 Loss: 0.0029294082838877784\n",
      "Iteration: 6686 lambda_k: 1 Loss: 0.0029294082838877827\n",
      "Iteration: 6687 lambda_k: 1 Loss: 0.0029294082838877875\n",
      "Iteration: 6688 lambda_k: 1 Loss: 0.0029294082838877905\n",
      "Iteration: 6689 lambda_k: 1 Loss: 0.0029294082838877944\n",
      "Iteration: 6690 lambda_k: 1 Loss: 0.0029294082838878005\n",
      "Iteration: 6691 lambda_k: 1 Loss: 0.0029294082838878027\n",
      "Iteration: 6692 lambda_k: 1 Loss: 0.0029294082838878075\n",
      "Iteration: 6693 lambda_k: 1 Loss: 0.002929408283887809\n",
      "Iteration: 6694 lambda_k: 1 Loss: 0.0029294082838878114\n",
      "Iteration: 6695 lambda_k: 1 Loss: 0.002929408283887814\n",
      "Iteration: 6696 lambda_k: 1 Loss: 0.002929408283887816\n",
      "Iteration: 6697 lambda_k: 1 Loss: 0.002929408283887819\n",
      "Iteration: 6698 lambda_k: 1 Loss: 0.0029294082838878252\n",
      "Iteration: 6699 lambda_k: 1 Loss: 0.002929408283887824\n",
      "Iteration: 6700 lambda_k: 1 Loss: 0.002929408283887828\n",
      "Iteration: 6701 lambda_k: 1 Loss: 0.0029294082838878296\n",
      "Iteration: 6702 lambda_k: 1 Loss: 0.0029294082838878326\n",
      "Iteration: 6703 lambda_k: 1 Loss: 0.0029294082838878348\n",
      "Iteration: 6704 lambda_k: 1 Loss: 0.002929408283887839\n",
      "Iteration: 6705 lambda_k: 1 Loss: 0.002929408283887842\n",
      "Iteration: 6706 lambda_k: 1 Loss: 0.0029294082838878482\n",
      "Iteration: 6707 lambda_k: 1 Loss: 0.0029294082838878504\n",
      "Iteration: 6708 lambda_k: 1 Loss: 0.0029294082838878513\n",
      "Iteration: 6709 lambda_k: 1 Loss: 0.002929408283887856\n",
      "Iteration: 6710 lambda_k: 1 Loss: 0.002929408283887859\n",
      "Iteration: 6711 lambda_k: 1 Loss: 0.0029294082838878612\n",
      "Iteration: 6712 lambda_k: 1 Loss: 0.0029294082838878625\n",
      "Iteration: 6713 lambda_k: 1 Loss: 0.002929408283887867\n",
      "Iteration: 6714 lambda_k: 1 Loss: 0.0029294082838878703\n",
      "Iteration: 6715 lambda_k: 1 Loss: 0.0029294082838878734\n",
      "Iteration: 6716 lambda_k: 1 Loss: 0.0029294082838878755\n",
      "Iteration: 6717 lambda_k: 1 Loss: 0.00292940828388788\n",
      "Iteration: 6718 lambda_k: 1 Loss: 0.0029294082838878842\n",
      "Iteration: 6719 lambda_k: 1 Loss: 0.0029294082838878873\n",
      "Iteration: 6720 lambda_k: 1 Loss: 0.0029294082838878916\n",
      "Iteration: 6721 lambda_k: 1 Loss: 0.0029294082838878964\n",
      "Iteration: 6722 lambda_k: 1 Loss: 0.0029294082838878977\n",
      "Iteration: 6723 lambda_k: 1 Loss: 0.002929408283887899\n",
      "Iteration: 6724 lambda_k: 1 Loss: 0.0029294082838879033\n",
      "Iteration: 6725 lambda_k: 1 Loss: 0.002929408283887906\n",
      "Iteration: 6726 lambda_k: 1 Loss: 0.0029294082838879094\n",
      "Iteration: 6727 lambda_k: 1 Loss: 0.002929408283887911\n",
      "Iteration: 6728 lambda_k: 1 Loss: 0.0029294082838879124\n",
      "Iteration: 6729 lambda_k: 1 Loss: 0.0029294082838879154\n",
      "Iteration: 6730 lambda_k: 1 Loss: 0.002929408283887918\n",
      "Iteration: 6731 lambda_k: 1 Loss: 0.002929408283887922\n",
      "Iteration: 6732 lambda_k: 1 Loss: 0.002929408283887922\n",
      "Iteration: 6733 lambda_k: 1 Loss: 0.0029294082838879246\n",
      "Iteration: 6734 lambda_k: 1 Loss: 0.0029294082838879276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6735 lambda_k: 1 Loss: 0.0029294082838879337\n",
      "Iteration: 6736 lambda_k: 1 Loss: 0.0029294082838879363\n",
      "Iteration: 6737 lambda_k: 1 Loss: 0.0029294082838879415\n",
      "Iteration: 6738 lambda_k: 1 Loss: 0.002929408283887944\n",
      "Iteration: 6739 lambda_k: 1 Loss: 0.002929408283887949\n",
      "Iteration: 6740 lambda_k: 1 Loss: 0.0029294082838879523\n",
      "Iteration: 6741 lambda_k: 1 Loss: 0.0029294082838879558\n",
      "Iteration: 6742 lambda_k: 1 Loss: 0.0029294082838879584\n",
      "Iteration: 6743 lambda_k: 1 Loss: 0.0029294082838879614\n",
      "Iteration: 6744 lambda_k: 1 Loss: 0.002929408283887963\n",
      "Iteration: 6745 lambda_k: 1 Loss: 0.0029294082838879653\n",
      "Iteration: 6746 lambda_k: 1 Loss: 0.002929408283887968\n",
      "Iteration: 6747 lambda_k: 1 Loss: 0.002929408283887972\n",
      "Iteration: 6748 lambda_k: 1 Loss: 0.0029294082838879727\n",
      "Iteration: 6749 lambda_k: 1 Loss: 0.0029294082838879753\n",
      "Iteration: 6750 lambda_k: 1 Loss: 0.002929408283887978\n",
      "Iteration: 6751 lambda_k: 1 Loss: 0.002929408283887979\n",
      "Iteration: 6752 lambda_k: 1 Loss: 0.0029294082838879822\n",
      "Iteration: 6753 lambda_k: 1 Loss: 0.002929408283887985\n",
      "Iteration: 6754 lambda_k: 1 Loss: 0.0029294082838879853\n",
      "Iteration: 6755 lambda_k: 1 Loss: 0.0029294082838879883\n",
      "Iteration: 6756 lambda_k: 1 Loss: 0.00292940828388799\n",
      "Iteration: 6757 lambda_k: 1 Loss: 0.0029294082838879944\n",
      "Iteration: 6758 lambda_k: 1 Loss: 0.002929408283888\n",
      "Iteration: 6759 lambda_k: 1 Loss: 0.002929408283888002\n",
      "Iteration: 6760 lambda_k: 1 Loss: 0.002929408283888003\n",
      "Iteration: 6761 lambda_k: 1 Loss: 0.002929408283888005\n",
      "Iteration: 6762 lambda_k: 1 Loss: 0.002929408283888009\n",
      "Iteration: 6763 lambda_k: 1 Loss: 0.0029294082838880104\n",
      "Iteration: 6764 lambda_k: 1 Loss: 0.0029294082838880126\n",
      "Iteration: 6765 lambda_k: 1 Loss: 0.0029294082838880135\n",
      "Iteration: 6766 lambda_k: 1 Loss: 0.0029294082838880187\n",
      "Iteration: 6767 lambda_k: 1 Loss: 0.0029294082838880226\n",
      "Iteration: 6768 lambda_k: 1 Loss: 0.0029294082838880247\n",
      "Iteration: 6769 lambda_k: 1 Loss: 0.00292940828388803\n",
      "Iteration: 6770 lambda_k: 1 Loss: 0.002929408283888033\n",
      "Iteration: 6771 lambda_k: 1 Loss: 0.002929408283888038\n",
      "Iteration: 6772 lambda_k: 1 Loss: 0.002929408283888041\n",
      "Iteration: 6773 lambda_k: 1 Loss: 0.0029294082838880416\n",
      "Iteration: 6774 lambda_k: 1 Loss: 0.0029294082838880416\n",
      "Iteration: 6775 lambda_k: 1 Loss: 0.0029294082838880486\n",
      "Iteration: 6776 lambda_k: 1 Loss: 0.0029294082838880534\n",
      "Iteration: 6777 lambda_k: 1 Loss: 0.0029294082838880547\n",
      "Iteration: 6778 lambda_k: 1 Loss: 0.002929408283888059\n",
      "Iteration: 6779 lambda_k: 1 Loss: 0.002929408283888061\n",
      "Iteration: 6780 lambda_k: 1 Loss: 0.0029294082838880655\n",
      "Iteration: 6781 lambda_k: 1 Loss: 0.0029294082838880716\n",
      "Iteration: 6782 lambda_k: 1 Loss: 0.002929408283888074\n",
      "Iteration: 6783 lambda_k: 1 Loss: 0.0029294082838880763\n",
      "Iteration: 6784 lambda_k: 1 Loss: 0.002929408283888078\n",
      "Iteration: 6785 lambda_k: 1 Loss: 0.0029294082838880846\n",
      "Iteration: 6786 lambda_k: 1 Loss: 0.002929408283888089\n",
      "Iteration: 6787 lambda_k: 1 Loss: 0.0029294082838880907\n",
      "Iteration: 6788 lambda_k: 1 Loss: 0.002929408283888094\n",
      "Iteration: 6789 lambda_k: 1 Loss: 0.0029294082838880985\n",
      "Iteration: 6790 lambda_k: 1 Loss: 0.002929408283888101\n",
      "Iteration: 6791 lambda_k: 1 Loss: 0.0029294082838881037\n",
      "Iteration: 6792 lambda_k: 1 Loss: 0.002929408283888108\n",
      "Iteration: 6793 lambda_k: 1 Loss: 0.002929408283888108\n",
      "Iteration: 6794 lambda_k: 1 Loss: 0.0029294082838881115\n",
      "Iteration: 6795 lambda_k: 1 Loss: 0.0029294082838881136\n",
      "Iteration: 6796 lambda_k: 1 Loss: 0.002929408283888115\n",
      "Iteration: 6797 lambda_k: 1 Loss: 0.002929408283888118\n",
      "Iteration: 6798 lambda_k: 1 Loss: 0.0029294082838881197\n",
      "Iteration: 6799 lambda_k: 1 Loss: 0.002929408283888124\n",
      "Iteration: 6800 lambda_k: 1 Loss: 0.0029294082838881253\n",
      "Iteration: 6801 lambda_k: 1 Loss: 0.0029294082838881284\n",
      "Iteration: 6802 lambda_k: 1 Loss: 0.0029294082838881314\n",
      "Iteration: 6803 lambda_k: 1 Loss: 0.0029294082838881345\n",
      "Iteration: 6804 lambda_k: 1 Loss: 0.0029294082838881366\n",
      "Iteration: 6805 lambda_k: 1 Loss: 0.002929408283888136\n",
      "Iteration: 6806 lambda_k: 1 Loss: 0.0029294082838881405\n",
      "Iteration: 6807 lambda_k: 1 Loss: 0.0029294082838881466\n",
      "Iteration: 6808 lambda_k: 1 Loss: 0.002929408283888151\n",
      "Iteration: 6809 lambda_k: 1 Loss: 0.0029294082838881544\n",
      "Iteration: 6810 lambda_k: 1 Loss: 0.0029294082838881553\n",
      "Iteration: 6811 lambda_k: 1 Loss: 0.0029294082838881587\n",
      "Iteration: 6812 lambda_k: 1 Loss: 0.0029294082838881618\n",
      "Iteration: 6813 lambda_k: 1 Loss: 0.002929408283888163\n",
      "Iteration: 6814 lambda_k: 1 Loss: 0.002929408283888166\n",
      "Iteration: 6815 lambda_k: 1 Loss: 0.0029294082838881696\n",
      "Iteration: 6816 lambda_k: 1 Loss: 0.0029294082838881726\n",
      "Iteration: 6817 lambda_k: 1 Loss: 0.002929408283888175\n",
      "Iteration: 6818 lambda_k: 1 Loss: 0.002929408283888177\n",
      "Iteration: 6819 lambda_k: 1 Loss: 0.0029294082838881787\n",
      "Iteration: 6820 lambda_k: 1 Loss: 0.002929408283888182\n",
      "Iteration: 6821 lambda_k: 1 Loss: 0.002929408283888186\n",
      "Iteration: 6822 lambda_k: 1 Loss: 0.002929408283888189\n",
      "Iteration: 6823 lambda_k: 1 Loss: 0.0029294082838881904\n",
      "Iteration: 6824 lambda_k: 1 Loss: 0.002929408283888194\n",
      "Iteration: 6825 lambda_k: 1 Loss: 0.002929408283888201\n",
      "Iteration: 6826 lambda_k: 1 Loss: 0.002929408283888206\n",
      "Iteration: 6827 lambda_k: 1 Loss: 0.00292940828388821\n",
      "Iteration: 6828 lambda_k: 1 Loss: 0.0029294082838882116\n",
      "Iteration: 6829 lambda_k: 1 Loss: 0.002929408283888215\n",
      "Iteration: 6830 lambda_k: 1 Loss: 0.0029294082838882164\n",
      "Iteration: 6831 lambda_k: 1 Loss: 0.0029294082838882164\n",
      "Iteration: 6832 lambda_k: 1 Loss: 0.002929408283888219\n",
      "Iteration: 6833 lambda_k: 1 Loss: 0.002929408283888223\n",
      "Iteration: 6834 lambda_k: 1 Loss: 0.002929408283888227\n",
      "Iteration: 6835 lambda_k: 1 Loss: 0.0029294082838882303\n",
      "Iteration: 6836 lambda_k: 1 Loss: 0.002929408283888234\n",
      "Iteration: 6837 lambda_k: 1 Loss: 0.002929408283888239\n",
      "Iteration: 6838 lambda_k: 1 Loss: 0.0029294082838882424\n",
      "Iteration: 6839 lambda_k: 1 Loss: 0.002929408283888244\n",
      "Iteration: 6840 lambda_k: 1 Loss: 0.002929408283888249\n",
      "Iteration: 6841 lambda_k: 1 Loss: 0.002929408283888252\n",
      "Iteration: 6842 lambda_k: 1 Loss: 0.0029294082838882576\n",
      "Iteration: 6843 lambda_k: 1 Loss: 0.0029294082838882624\n",
      "Iteration: 6844 lambda_k: 1 Loss: 0.002929408283888263\n",
      "Iteration: 6845 lambda_k: 1 Loss: 0.0029294082838882667\n",
      "Iteration: 6846 lambda_k: 1 Loss: 0.002929408283888268\n",
      "Iteration: 6847 lambda_k: 1 Loss: 0.0029294082838882706\n",
      "Iteration: 6848 lambda_k: 1 Loss: 0.0029294082838882732\n",
      "Iteration: 6849 lambda_k: 1 Loss: 0.002929408283888277\n",
      "Iteration: 6850 lambda_k: 1 Loss: 0.002929408283888279\n",
      "Iteration: 6851 lambda_k: 1 Loss: 0.0029294082838882836\n",
      "Iteration: 6852 lambda_k: 1 Loss: 0.002929408283888287\n",
      "Iteration: 6853 lambda_k: 1 Loss: 0.002929408283888289\n",
      "Iteration: 6854 lambda_k: 1 Loss: 0.0029294082838882914\n",
      "Iteration: 6855 lambda_k: 1 Loss: 0.002929408283888294\n",
      "Iteration: 6856 lambda_k: 1 Loss: 0.0029294082838882997\n",
      "Iteration: 6857 lambda_k: 1 Loss: 0.002929408283888303\n",
      "Iteration: 6858 lambda_k: 1 Loss: 0.0029294082838883053\n",
      "Iteration: 6859 lambda_k: 1 Loss: 0.002929408283888308\n",
      "Iteration: 6860 lambda_k: 1 Loss: 0.002929408283888311\n",
      "Iteration: 6861 lambda_k: 1 Loss: 0.002929408283888315\n",
      "Iteration: 6862 lambda_k: 1 Loss: 0.0029294082838883183\n",
      "Iteration: 6863 lambda_k: 1 Loss: 0.0029294082838883205\n",
      "Iteration: 6864 lambda_k: 1 Loss: 0.0029294082838883257\n",
      "Iteration: 6865 lambda_k: 1 Loss: 0.0029294082838883287\n",
      "Iteration: 6866 lambda_k: 1 Loss: 0.002929408283888333\n",
      "Iteration: 6867 lambda_k: 1 Loss: 0.002929408283888337\n",
      "Iteration: 6868 lambda_k: 1 Loss: 0.00292940828388834\n",
      "Iteration: 6869 lambda_k: 1 Loss: 0.002929408283888345\n",
      "Iteration: 6870 lambda_k: 1 Loss: 0.0029294082838883457\n",
      "Iteration: 6871 lambda_k: 1 Loss: 0.0029294082838883526\n",
      "Iteration: 6872 lambda_k: 1 Loss: 0.0029294082838883556\n",
      "Iteration: 6873 lambda_k: 1 Loss: 0.0029294082838883574\n",
      "Iteration: 6874 lambda_k: 1 Loss: 0.00292940828388836\n",
      "Iteration: 6875 lambda_k: 1 Loss: 0.002929408283888363\n",
      "Iteration: 6876 lambda_k: 1 Loss: 0.0029294082838883665\n",
      "Iteration: 6877 lambda_k: 1 Loss: 0.00292940828388837\n",
      "Iteration: 6878 lambda_k: 1 Loss: 0.0029294082838883725\n",
      "Iteration: 6879 lambda_k: 1 Loss: 0.0029294082838883773\n",
      "Iteration: 6880 lambda_k: 1 Loss: 0.0029294082838883804\n",
      "Iteration: 6881 lambda_k: 1 Loss: 0.002929408283888381\n",
      "Iteration: 6882 lambda_k: 1 Loss: 0.0029294082838883864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6883 lambda_k: 1 Loss: 0.00292940828388839\n",
      "Iteration: 6884 lambda_k: 1 Loss: 0.002929408283888395\n",
      "Iteration: 6885 lambda_k: 1 Loss: 0.002929408283888399\n",
      "Iteration: 6886 lambda_k: 1 Loss: 0.0029294082838883986\n",
      "Iteration: 6887 lambda_k: 1 Loss: 0.0029294082838884\n",
      "Iteration: 6888 lambda_k: 1 Loss: 0.002929408283888402\n",
      "Iteration: 6889 lambda_k: 1 Loss: 0.002929408283888406\n",
      "Iteration: 6890 lambda_k: 1 Loss: 0.002929408283888407\n",
      "Iteration: 6891 lambda_k: 1 Loss: 0.00292940828388841\n",
      "Iteration: 6892 lambda_k: 1 Loss: 0.002929408283888413\n",
      "Iteration: 6893 lambda_k: 1 Loss: 0.0029294082838884176\n",
      "Iteration: 6894 lambda_k: 1 Loss: 0.0029294082838884207\n",
      "Iteration: 6895 lambda_k: 1 Loss: 0.002929408283888422\n",
      "Iteration: 6896 lambda_k: 1 Loss: 0.002929408283888424\n",
      "Iteration: 6897 lambda_k: 1 Loss: 0.002929408283888428\n",
      "Iteration: 6898 lambda_k: 1 Loss: 0.002929408283888428\n",
      "Iteration: 6899 lambda_k: 1 Loss: 0.0029294082838884315\n",
      "Iteration: 6900 lambda_k: 1 Loss: 0.0029294082838884354\n",
      "Iteration: 6901 lambda_k: 1 Loss: 0.00292940828388844\n",
      "Iteration: 6902 lambda_k: 1 Loss: 0.002929408283888446\n",
      "Iteration: 6903 lambda_k: 1 Loss: 0.0029294082838884506\n",
      "Iteration: 6904 lambda_k: 1 Loss: 0.0029294082838884528\n",
      "Iteration: 6905 lambda_k: 1 Loss: 0.002929408283888456\n",
      "Iteration: 6906 lambda_k: 1 Loss: 0.0029294082838884614\n",
      "Iteration: 6907 lambda_k: 1 Loss: 0.002929408283888465\n",
      "Iteration: 6908 lambda_k: 1 Loss: 0.0029294082838884697\n",
      "Iteration: 6909 lambda_k: 1 Loss: 0.002929408283888471\n",
      "Iteration: 6910 lambda_k: 1 Loss: 0.0029294082838884745\n",
      "Iteration: 6911 lambda_k: 1 Loss: 0.0029294082838884784\n",
      "Iteration: 6912 lambda_k: 1 Loss: 0.002929408283888479\n",
      "Iteration: 6913 lambda_k: 1 Loss: 0.00292940828388848\n",
      "Iteration: 6914 lambda_k: 1 Loss: 0.0029294082838884797\n",
      "Iteration: 6915 lambda_k: 1 Loss: 0.0029294082838884797\n",
      "Iteration: 6916 lambda_k: 1 Loss: 0.0029294082838884805\n",
      "Iteration: 6917 lambda_k: 1 Loss: 0.002929408283888481\n",
      "Iteration: 6918 lambda_k: 1 Loss: 0.002929408283888482\n",
      "Iteration: 6919 lambda_k: 1 Loss: 0.002929408283888485\n",
      "Iteration: 6920 lambda_k: 1 Loss: 0.002929408283888484\n",
      "Iteration: 6921 lambda_k: 1 Loss: 0.0029294082838884836\n",
      "Iteration: 6922 lambda_k: 1 Loss: 0.002929408283888485\n",
      "Iteration: 6923 lambda_k: 1 Loss: 0.0029294082838884896\n",
      "Iteration: 6924 lambda_k: 1 Loss: 0.00292940828388849\n",
      "Iteration: 6925 lambda_k: 1 Loss: 0.0029294082838884922\n",
      "Iteration: 6926 lambda_k: 1 Loss: 0.0029294082838884922\n",
      "Iteration: 6927 lambda_k: 1 Loss: 0.0029294082838884927\n",
      "Iteration: 6928 lambda_k: 1 Loss: 0.0029294082838884927\n",
      "Iteration: 6929 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 6930 lambda_k: 1 Loss: 0.0029294082838884927\n",
      "Iteration: 6931 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 6932 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 6933 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 6934 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 6935 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 6936 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 6937 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 6938 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 6939 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 6940 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 6941 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 6942 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 6943 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 6944 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 6945 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 6946 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 6947 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 6948 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 6949 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 6950 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 6951 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 6952 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 6953 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 6954 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 6955 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 6956 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 6957 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 6958 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 6959 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 6960 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 6961 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 6962 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 6963 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 6964 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 6965 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 6966 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 6967 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 6968 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 6969 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 6970 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 6971 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 6972 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 6973 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 6974 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 6975 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 6976 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 6977 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 6978 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 6979 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 6980 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 6981 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 6982 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 6983 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 6984 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 6985 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 6986 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 6987 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 6988 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 6989 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 6990 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 6991 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 6992 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 6993 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 6994 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 6995 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 6996 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 6997 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 6998 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 6999 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7000 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7001 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7002 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7003 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7004 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7005 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7006 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7007 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7008 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7009 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7010 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7011 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7012 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7013 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7014 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7015 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7016 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7017 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7018 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7019 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7020 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7021 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7022 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7023 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7024 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7025 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7026 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7027 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7028 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7029 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7030 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7031 lambda_k: 1 Loss: 0.002929408283888494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7032 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7033 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7034 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7035 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7036 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7037 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7038 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7039 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7040 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7041 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7042 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7043 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7044 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7045 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7046 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7047 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7048 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7049 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7050 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7051 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7052 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7053 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7054 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7055 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7056 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7057 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7058 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7059 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7060 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7061 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7062 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7063 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7064 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7065 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7066 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7067 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7068 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7069 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7070 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7071 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7072 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7073 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7074 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7075 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7076 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7077 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7078 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7079 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7080 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7081 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7082 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7083 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7084 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7085 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7086 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7087 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7088 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7089 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7090 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7091 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7092 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7093 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7094 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7095 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7096 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7097 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7098 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7099 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7100 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7101 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7102 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7103 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7104 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7105 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7106 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7107 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7108 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7109 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7110 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7111 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7112 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7113 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7114 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7115 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7116 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7117 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7118 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7119 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7120 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7121 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7122 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7123 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7124 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7125 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7126 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7127 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7128 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7129 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7130 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7131 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7132 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7133 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7134 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7135 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7136 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7137 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7138 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7139 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7140 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7141 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7142 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7143 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7144 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7145 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7146 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7147 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7148 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7149 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7150 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7151 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7152 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7153 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7154 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7155 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7156 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7157 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7158 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7159 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7160 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7161 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7162 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7163 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7164 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7165 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7166 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7167 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7168 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7169 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7170 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7171 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7172 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7173 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7174 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7175 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7176 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7177 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7178 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7179 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7180 lambda_k: 1 Loss: 0.0029294082838884935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7181 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7182 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7183 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7184 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7185 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7186 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7187 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7188 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7189 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7190 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7191 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7192 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7193 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7194 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7195 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7196 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7197 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7198 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7199 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7200 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7201 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7202 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7203 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7204 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7205 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7206 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7207 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7208 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7209 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7210 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7211 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7212 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7213 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7214 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7215 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7216 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7217 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7218 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7219 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7220 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7221 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7222 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7223 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7224 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7225 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7226 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7227 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7228 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7229 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7230 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7231 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7232 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7233 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7234 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7235 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7236 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7237 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7238 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7239 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7240 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7241 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7242 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7243 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7244 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7245 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7246 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7247 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7248 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7249 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7250 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7251 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7252 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7253 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7254 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7255 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7256 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7257 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7258 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7259 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7260 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7261 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7262 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7263 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7264 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7265 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7266 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7267 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7268 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7269 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7270 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7271 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7272 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7273 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7274 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7275 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7276 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7277 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7278 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7279 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7280 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7281 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7282 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7283 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7284 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7285 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7286 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7287 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7288 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7289 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7290 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7291 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7292 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7293 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7294 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7295 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7296 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7297 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7298 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7299 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7300 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7301 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7302 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7303 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7304 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7305 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7306 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7307 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7308 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7309 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7310 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7311 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7312 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7313 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7314 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7315 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7316 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7317 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7318 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7319 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7320 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7321 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7322 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7323 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7324 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7325 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7326 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7327 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7328 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7329 lambda_k: 1 Loss: 0.002929408283888493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7330 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7331 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7332 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7333 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7334 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7335 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7336 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7337 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7338 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7339 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7340 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7341 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7342 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7343 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7344 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7345 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7346 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7347 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7348 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7349 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7350 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7351 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7352 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7353 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7354 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7355 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7356 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7357 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7358 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7359 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7360 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7361 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7362 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7363 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7364 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7365 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7366 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7367 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7368 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7369 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7370 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7371 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7372 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7373 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7374 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7375 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7376 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7377 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7378 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7379 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7380 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7381 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7382 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7383 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7384 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7385 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7386 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7387 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7388 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7389 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7390 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7391 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7392 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7393 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7394 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7395 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7396 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7397 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7398 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7399 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7400 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7401 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7402 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7403 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7404 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7405 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7406 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7407 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7408 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7409 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7410 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7411 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7412 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7413 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7414 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7415 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7416 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7417 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7418 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7419 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7420 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7421 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7422 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7423 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7424 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7425 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7426 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7427 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7428 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7429 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7430 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7431 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7432 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7433 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7434 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7435 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7436 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7437 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7438 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7439 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7440 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7441 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7442 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7443 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7444 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7445 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7446 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7447 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7448 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7449 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7450 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7451 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7452 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7453 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7454 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7455 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7456 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7457 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7458 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7459 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7460 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7461 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7462 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7463 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7464 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7465 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7466 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7467 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7468 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7469 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7470 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7471 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7472 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7473 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7474 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7475 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7476 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7477 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7478 lambda_k: 1 Loss: 0.002929408283888494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7479 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7480 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7481 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7482 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7483 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7484 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7485 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7486 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7487 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7488 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7489 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7490 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7491 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7492 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7493 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7494 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7495 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7496 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7497 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7498 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7499 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7500 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7501 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7502 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7503 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7504 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7505 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7506 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7507 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7508 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7509 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7510 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7511 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7512 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7513 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7514 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7515 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7516 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7517 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7518 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7519 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7520 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7521 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7522 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7523 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7524 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7525 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7526 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7527 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7528 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7529 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7530 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7531 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7532 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7533 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7534 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7535 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7536 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7537 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7538 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7539 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7540 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7541 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7542 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7543 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7544 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7545 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7546 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7547 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7548 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7549 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7550 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7551 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7552 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7553 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7554 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7555 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7556 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7557 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7558 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7559 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7560 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7561 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7562 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7563 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7564 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7565 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7566 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7567 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7568 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7569 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7570 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7571 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7572 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7573 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7574 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7575 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7576 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7577 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7578 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7579 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7580 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7581 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7582 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7583 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7584 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7585 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7586 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7587 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7588 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7589 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7590 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7591 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7592 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7593 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7594 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7595 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7596 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7597 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7598 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7599 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7600 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7601 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7602 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7603 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7604 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7605 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7606 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7607 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7608 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7609 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7610 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7611 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7612 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7613 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7614 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7615 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7616 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7617 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7618 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7619 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7620 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7621 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7622 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7623 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7624 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7625 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7626 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7627 lambda_k: 1 Loss: 0.0029294082838884935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7628 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7629 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7630 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7631 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7632 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7633 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7634 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7635 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7636 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7637 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7638 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7639 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7640 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7641 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7642 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7643 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7644 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7645 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7646 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7647 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7648 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7649 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7650 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7651 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7652 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7653 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7654 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7655 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7656 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7657 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7658 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7659 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7660 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7661 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7662 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7663 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7664 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7665 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7666 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7667 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7668 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7669 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7670 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7671 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7672 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7673 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7674 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7675 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7676 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7677 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7678 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7679 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7680 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7681 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7682 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7683 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7684 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7685 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7686 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7687 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7688 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7689 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7690 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7691 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7692 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7693 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7694 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7695 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7696 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7697 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7698 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7699 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7700 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7701 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7702 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7703 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7704 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7705 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7706 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7707 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7708 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7709 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7710 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7711 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7712 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7713 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7714 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7715 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7716 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7717 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7718 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7719 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7720 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7721 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7722 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7723 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7724 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7725 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7726 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7727 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7728 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7729 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7730 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7731 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7732 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7733 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7734 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7735 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7736 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7737 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7738 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7739 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7740 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7741 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7742 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7743 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7744 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7745 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7746 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7747 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7748 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7749 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7750 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7751 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7752 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7753 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7754 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7755 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7756 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7757 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7758 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7759 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7760 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7761 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7762 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7763 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7764 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7765 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7766 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7767 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7768 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7769 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7770 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7771 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7772 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7773 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7774 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7775 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7776 lambda_k: 1 Loss: 0.002929408283888493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7777 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7778 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7779 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7780 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7781 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7782 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7783 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7784 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7785 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7786 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7787 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7788 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7789 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7790 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7791 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7792 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7793 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7794 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7795 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7796 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7797 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7798 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7799 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7800 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7801 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7802 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7803 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7804 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7805 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7806 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7807 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7808 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7809 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7810 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7811 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7812 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7813 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7814 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7815 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7816 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7817 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7818 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7819 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7820 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7821 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7822 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7823 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7824 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7825 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7826 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7827 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7828 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7829 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7830 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7831 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7832 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7833 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7834 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7835 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7836 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7837 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7838 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7839 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7840 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7841 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7842 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7843 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7844 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7845 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7846 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7847 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7848 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7849 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7850 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7851 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7852 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7853 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7854 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7855 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7856 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7857 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7858 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7859 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7860 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7861 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7862 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7863 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7864 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7865 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7866 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7867 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7868 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7869 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7870 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7871 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7872 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7873 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7874 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7875 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7876 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7877 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7878 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7879 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7880 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7881 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7882 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7883 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7884 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7885 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7886 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7887 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7888 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7889 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7890 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7891 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7892 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7893 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7894 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7895 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7896 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7897 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7898 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7899 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7900 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7901 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7902 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7903 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7904 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7905 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7906 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7907 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7908 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7909 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7910 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7911 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7912 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7913 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7914 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7915 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7916 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7917 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7918 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7919 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7920 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7921 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7922 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7923 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7924 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7925 lambda_k: 1 Loss: 0.002929408283888494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7926 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7927 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7928 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7929 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7930 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7931 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7932 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7933 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7934 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7935 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7936 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7937 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7938 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7939 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7940 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7941 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7942 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7943 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7944 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7945 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7946 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7947 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7948 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7949 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7950 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7951 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7952 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7953 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7954 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7955 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7956 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7957 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7958 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7959 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7960 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7961 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7962 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7963 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7964 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7965 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7966 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7967 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7968 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7969 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7970 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7971 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7972 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7973 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7974 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7975 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7976 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7977 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7978 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7979 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7980 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7981 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7982 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7983 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7984 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7985 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7986 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7987 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7988 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7989 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7990 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7991 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7992 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7993 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7994 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7995 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7996 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 7997 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 7998 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 7999 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8000 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8001 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8002 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8003 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8004 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8005 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8006 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8007 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8008 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8009 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8010 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8011 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8012 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8013 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8014 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8015 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8016 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8017 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8018 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8019 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8020 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8021 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8022 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8023 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8024 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8025 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8026 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8027 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8028 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8029 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8030 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8031 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8032 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8033 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8034 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8035 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8036 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8037 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8038 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8039 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8040 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8041 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8042 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8043 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8044 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8045 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8046 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8047 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8048 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8049 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8050 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8051 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8052 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8053 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8054 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8055 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8056 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8057 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8058 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8059 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8060 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8061 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8062 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8063 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8064 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8065 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8066 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8067 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8068 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8069 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8070 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8071 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8072 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8073 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8074 lambda_k: 1 Loss: 0.0029294082838884935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8075 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8076 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8077 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8078 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8079 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8080 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8081 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8082 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8083 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8084 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8085 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8086 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8087 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8088 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8089 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8090 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8091 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8092 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8093 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8094 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8095 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8096 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8097 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8098 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8099 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8100 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8101 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8102 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8103 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8104 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8105 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8106 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8107 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8108 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8109 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8110 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8111 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8112 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8113 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8114 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8115 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8116 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8117 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8118 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8119 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8120 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8121 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8122 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8123 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8124 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8125 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8126 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8127 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8128 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8129 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8130 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8131 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8132 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8133 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8134 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8135 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8136 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8137 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8138 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8139 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8140 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8141 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8142 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8143 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8144 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8145 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8146 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8147 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8148 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8149 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8150 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8151 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8152 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8153 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8154 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8155 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8156 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8157 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8158 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8159 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8160 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8161 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8162 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8163 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8164 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8165 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8166 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8167 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8168 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8169 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8170 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8171 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8172 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8173 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8174 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8175 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8176 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8177 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8178 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8179 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8180 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8181 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8182 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8183 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8184 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8185 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8186 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8187 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8188 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8189 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8190 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8191 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8192 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8193 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8194 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8195 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8196 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8197 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8198 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8199 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8200 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8201 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8202 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8203 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8204 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8205 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8206 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8207 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8208 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8209 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8210 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8211 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8212 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8213 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8214 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8215 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8216 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8217 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8218 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8219 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8220 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8221 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8222 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8223 lambda_k: 1 Loss: 0.002929408283888493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8224 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8225 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8226 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8227 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8228 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8229 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8230 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8231 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8232 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8233 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8234 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8235 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8236 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8237 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8238 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8239 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8240 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8241 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8242 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8243 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8244 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8245 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8246 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8247 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8248 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8249 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8250 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8251 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8252 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8253 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8254 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8255 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8256 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8257 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8258 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8259 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8260 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8261 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8262 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8263 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8264 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8265 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8266 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8267 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8268 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8269 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8270 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8271 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8272 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8273 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8274 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8275 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8276 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8277 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8278 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8279 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8280 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8281 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8282 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8283 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8284 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8285 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8286 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8287 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8288 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8289 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8290 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8291 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8292 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8293 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8294 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8295 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8296 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8297 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8298 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8299 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8300 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8301 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8302 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8303 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8304 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8305 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8306 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8307 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8308 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8309 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8310 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8311 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8312 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8313 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8314 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8315 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8316 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8317 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8318 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8319 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8320 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8321 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8322 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8323 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8324 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8325 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8326 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8327 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8328 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8329 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8330 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8331 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8332 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8333 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8334 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8335 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8336 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8337 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8338 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8339 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8340 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8341 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8342 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8343 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8344 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8345 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8346 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8347 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8348 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8349 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8350 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8351 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8352 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8353 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8354 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8355 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8356 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8357 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8358 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8359 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8360 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8361 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8362 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8363 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8364 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8365 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8366 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8367 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8368 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8369 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8370 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8371 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8372 lambda_k: 1 Loss: 0.002929408283888494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8373 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8374 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8375 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8376 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8377 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8378 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8379 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8380 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8381 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8382 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8383 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8384 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8385 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8386 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8387 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8388 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8389 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8390 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8391 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8392 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8393 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8394 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8395 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8396 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8397 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8398 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8399 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8400 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8401 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8402 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8403 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8404 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8405 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8406 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8407 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8408 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8409 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8410 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8411 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8412 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8413 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8414 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8415 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8416 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8417 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8418 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8419 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8420 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8421 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8422 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8423 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8424 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8425 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8426 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8427 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8428 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8429 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8430 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8431 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8432 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8433 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8434 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8435 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8436 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8437 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8438 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8439 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8440 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8441 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8442 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8443 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8444 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8445 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8446 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8447 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8448 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8449 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8450 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8451 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8452 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8453 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8454 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8455 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8456 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8457 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8458 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8459 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8460 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8461 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8462 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8463 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8464 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8465 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8466 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8467 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8468 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8469 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8470 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8471 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8472 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8473 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8474 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8475 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8476 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8477 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8478 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8479 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8480 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8481 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8482 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8483 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8484 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8485 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8486 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8487 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8488 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8489 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8490 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8491 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8492 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8493 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8494 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8495 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8496 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8497 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8498 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8499 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8500 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8501 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8502 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8503 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8504 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8505 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8506 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8507 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8508 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8509 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8510 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8511 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8512 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8513 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8514 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8515 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8516 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8517 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8518 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8519 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8520 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8521 lambda_k: 1 Loss: 0.0029294082838884935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8522 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8523 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8524 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8525 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8526 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8527 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8528 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8529 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8530 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8531 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8532 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8533 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8534 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8535 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8536 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8537 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8538 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8539 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8540 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8541 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8542 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8543 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8544 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8545 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8546 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8547 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8548 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8549 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8550 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8551 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8552 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8553 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8554 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8555 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8556 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8557 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8558 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8559 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8560 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8561 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8562 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8563 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8564 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8565 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8566 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8567 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8568 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8569 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8570 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8571 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8572 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8573 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8574 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8575 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8576 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8577 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8578 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8579 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8580 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8581 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8582 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8583 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8584 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8585 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8586 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8587 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8588 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8589 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8590 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8591 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8592 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8593 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8594 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8595 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8596 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8597 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8598 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8599 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8600 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8601 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8602 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8603 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8604 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8605 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8606 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8607 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8608 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8609 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8610 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8611 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8612 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8613 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8614 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8615 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8616 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8617 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8618 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8619 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8620 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8621 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8622 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8623 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8624 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8625 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8626 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8627 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8628 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8629 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8630 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8631 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8632 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8633 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8634 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8635 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8636 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8637 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8638 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8639 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8640 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8641 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8642 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8643 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8644 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8645 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8646 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8647 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8648 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8649 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8650 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8651 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8652 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8653 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8654 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8655 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8656 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8657 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8658 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8659 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8660 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8661 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8662 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8663 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8664 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8665 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8666 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8667 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8668 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8669 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8670 lambda_k: 1 Loss: 0.002929408283888493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8671 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8672 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8673 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8674 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8675 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8676 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8677 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8678 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8679 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8680 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8681 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8682 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8683 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8684 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8685 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8686 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8687 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8688 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8689 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8690 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8691 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8692 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8693 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8694 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8695 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8696 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8697 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8698 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8699 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8700 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8701 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8702 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8703 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8704 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8705 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8706 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8707 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8708 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8709 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8710 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8711 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8712 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8713 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8714 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8715 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8716 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8717 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8718 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8719 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8720 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8721 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8722 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8723 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8724 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8725 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8726 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8727 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8728 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8729 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8730 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8731 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8732 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8733 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8734 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8735 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8736 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8737 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8738 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8739 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8740 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8741 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8742 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8743 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8744 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8745 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8746 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8747 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8748 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8749 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8750 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8751 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8752 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8753 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8754 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8755 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8756 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8757 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8758 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8759 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8760 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8761 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8762 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8763 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8764 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8765 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8766 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8767 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8768 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8769 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8770 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8771 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8772 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8773 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8774 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8775 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8776 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8777 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8778 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8779 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8780 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8781 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8782 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8783 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8784 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8785 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8786 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8787 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8788 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8789 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8790 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8791 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8792 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8793 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8794 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8795 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8796 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8797 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8798 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8799 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8800 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8801 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8802 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8803 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8804 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8805 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8806 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8807 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8808 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8809 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8810 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8811 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8812 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8813 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8814 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8815 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8816 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8817 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8818 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8819 lambda_k: 1 Loss: 0.002929408283888494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8820 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8821 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8822 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8823 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8824 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8825 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8826 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8827 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8828 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8829 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8830 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8831 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8832 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8833 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8834 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8835 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8836 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8837 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8838 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8839 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8840 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8841 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8842 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8843 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8844 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8845 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8846 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8847 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8848 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8849 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8850 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8851 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8852 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8853 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8854 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8855 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8856 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8857 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8858 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8859 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8860 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8861 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8862 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8863 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8864 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8865 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8866 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8867 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8868 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8869 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8870 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8871 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8872 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8873 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8874 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8875 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8876 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8877 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8878 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8879 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8880 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8881 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8882 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8883 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8884 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8885 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8886 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8887 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8888 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8889 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8890 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8891 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8892 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8893 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8894 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8895 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8896 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8897 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8898 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8899 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8900 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8901 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8902 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8903 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8904 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8905 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8906 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8907 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8908 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8909 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8910 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8911 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8912 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8913 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8914 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8915 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8916 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8917 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8918 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8919 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8920 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8921 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8922 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8923 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8924 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8925 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8926 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8927 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8928 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8929 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8930 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8931 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8932 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8933 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8934 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8935 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8936 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8937 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8938 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8939 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8940 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8941 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8942 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8943 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8944 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8945 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8946 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8947 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8948 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8949 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8950 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8951 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8952 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8953 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8954 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8955 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8956 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8957 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8958 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8959 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8960 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8961 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8962 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8963 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8964 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8965 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8966 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8967 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8968 lambda_k: 1 Loss: 0.0029294082838884935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8969 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8970 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8971 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8972 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8973 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8974 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8975 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8976 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8977 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8978 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8979 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8980 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8981 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8982 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8983 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8984 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8985 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8986 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8987 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8988 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8989 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8990 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8991 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8992 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8993 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8994 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8995 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8996 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 8997 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 8998 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 8999 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9000 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9001 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9002 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9003 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9004 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9005 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9006 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9007 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9008 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9009 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9010 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9011 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9012 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9013 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9014 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9015 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9016 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9017 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9018 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9019 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9020 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9021 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9022 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9023 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9024 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9025 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9026 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9027 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9028 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9029 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9030 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9031 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9032 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9033 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9034 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9035 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9036 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9037 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9038 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9039 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9040 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9041 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9042 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9043 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9044 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9045 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9046 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9047 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9048 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9049 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9050 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9051 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9052 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9053 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9054 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9055 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9056 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9057 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9058 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9059 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9060 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9061 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9062 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9063 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9064 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9065 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9066 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9067 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9068 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9069 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9070 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9071 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9072 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9073 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9074 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9075 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9076 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9077 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9078 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9079 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9080 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9081 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9082 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9083 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9084 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9085 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9086 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9087 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9088 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9089 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9090 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9091 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9092 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9093 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9094 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9095 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9096 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9097 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9098 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9099 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9100 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9101 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9102 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9103 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9104 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9105 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9106 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9107 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9108 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9109 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9110 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9111 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9112 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9113 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9114 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9115 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9116 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9117 lambda_k: 1 Loss: 0.002929408283888493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9118 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9119 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9120 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9121 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9122 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9123 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9124 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9125 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9126 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9127 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9128 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9129 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9130 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9131 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9132 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9133 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9134 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9135 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9136 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9137 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9138 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9139 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9140 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9141 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9142 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9143 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9144 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9145 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9146 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9147 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9148 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9149 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9150 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9151 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9152 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9153 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9154 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9155 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9156 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9157 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9158 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9159 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9160 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9161 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9162 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9163 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9164 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9165 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9166 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9167 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9168 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9169 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9170 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9171 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9172 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9173 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9174 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9175 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9176 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9177 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9178 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9179 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9180 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9181 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9182 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9183 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9184 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9185 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9186 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9187 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9188 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9189 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9190 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9191 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9192 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9193 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9194 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9195 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9196 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9197 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9198 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9199 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9200 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9201 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9202 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9203 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9204 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9205 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9206 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9207 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9208 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9209 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9210 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9211 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9212 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9213 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9214 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9215 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9216 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9217 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9218 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9219 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9220 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9221 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9222 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9223 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9224 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9225 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9226 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9227 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9228 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9229 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9230 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9231 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9232 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9233 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9234 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9235 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9236 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9237 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9238 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9239 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9240 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9241 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9242 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9243 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9244 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9245 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9246 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9247 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9248 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9249 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9250 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9251 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9252 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9253 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9254 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9255 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9256 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9257 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9258 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9259 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9260 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9261 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9262 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9263 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9264 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9265 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9266 lambda_k: 1 Loss: 0.002929408283888494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9267 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9268 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9269 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9270 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9271 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9272 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9273 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9274 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9275 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9276 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9277 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9278 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9279 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9280 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9281 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9282 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9283 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9284 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9285 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9286 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9287 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9288 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9289 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9290 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9291 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9292 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9293 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9294 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9295 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9296 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9297 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9298 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9299 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9300 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9301 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9302 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9303 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9304 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9305 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9306 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9307 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9308 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9309 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9310 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9311 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9312 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9313 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9314 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9315 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9316 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9317 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9318 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9319 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9320 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9321 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9322 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9323 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9324 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9325 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9326 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9327 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9328 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9329 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9330 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9331 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9332 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9333 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9334 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9335 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9336 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9337 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9338 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9339 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9340 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9341 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9342 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9343 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9344 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9345 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9346 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9347 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9348 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9349 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9350 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9351 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9352 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9353 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9354 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9355 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9356 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9357 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9358 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9359 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9360 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9361 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9362 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9363 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9364 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9365 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9366 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9367 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9368 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9369 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9370 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9371 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9372 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9373 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9374 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9375 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9376 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9377 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9378 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9379 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9380 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9381 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9382 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9383 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9384 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9385 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9386 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9387 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9388 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9389 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9390 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9391 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9392 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9393 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9394 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9395 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9396 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9397 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9398 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9399 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9400 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9401 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9402 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9403 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9404 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9405 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9406 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9407 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9408 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9409 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9410 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9411 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9412 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9413 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9414 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9415 lambda_k: 1 Loss: 0.0029294082838884935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9416 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9417 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9418 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9419 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9420 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9421 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9422 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9423 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9424 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9425 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9426 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9427 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9428 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9429 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9430 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9431 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9432 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9433 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9434 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9435 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9436 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9437 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9438 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9439 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9440 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9441 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9442 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9443 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9444 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9445 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9446 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9447 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9448 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9449 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9450 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9451 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9452 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9453 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9454 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9455 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9456 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9457 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9458 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9459 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9460 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9461 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9462 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9463 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9464 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9465 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9466 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9467 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9468 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9469 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9470 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9471 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9472 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9473 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9474 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9475 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9476 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9477 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9478 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9479 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9480 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9481 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9482 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9483 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9484 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9485 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9486 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9487 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9488 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9489 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9490 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9491 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9492 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9493 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9494 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9495 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9496 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9497 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9498 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9499 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9500 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9501 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9502 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9503 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9504 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9505 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9506 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9507 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9508 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9509 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9510 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9511 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9512 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9513 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9514 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9515 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9516 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9517 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9518 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9519 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9520 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9521 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9522 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9523 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9524 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9525 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9526 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9527 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9528 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9529 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9530 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9531 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9532 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9533 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9534 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9535 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9536 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9537 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9538 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9539 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9540 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9541 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9542 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9543 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9544 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9545 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9546 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9547 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9548 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9549 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9550 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9551 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9552 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9553 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9554 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9555 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9556 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9557 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9558 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9559 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9560 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9561 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9562 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9563 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9564 lambda_k: 1 Loss: 0.002929408283888493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9565 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9566 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9567 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9568 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9569 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9570 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9571 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9572 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9573 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9574 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9575 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9576 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9577 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9578 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9579 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9580 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9581 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9582 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9583 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9584 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9585 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9586 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9587 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9588 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9589 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9590 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9591 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9592 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9593 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9594 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9595 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9596 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9597 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9598 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9599 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9600 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9601 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9602 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9603 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9604 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9605 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9606 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9607 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9608 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9609 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9610 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9611 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9612 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9613 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9614 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9615 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9616 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9617 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9618 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9619 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9620 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9621 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9622 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9623 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9624 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9625 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9626 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9627 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9628 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9629 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9630 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9631 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9632 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9633 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9634 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9635 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9636 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9637 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9638 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9639 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9640 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9641 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9642 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9643 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9644 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9645 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9646 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9647 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9648 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9649 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9650 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9651 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9652 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9653 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9654 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9655 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9656 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9657 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9658 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9659 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9660 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9661 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9662 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9663 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9664 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9665 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9666 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9667 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9668 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9669 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9670 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9671 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9672 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9673 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9674 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9675 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9676 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9677 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9678 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9679 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9680 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9681 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9682 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9683 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9684 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9685 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9686 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9687 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9688 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9689 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9690 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9691 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9692 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9693 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9694 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9695 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9696 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9697 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9698 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9699 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9700 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9701 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9702 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9703 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9704 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9705 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9706 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9707 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9708 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9709 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9710 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9711 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9712 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9713 lambda_k: 1 Loss: 0.002929408283888494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9714 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9715 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9716 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9717 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9718 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9719 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9720 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9721 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9722 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9723 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9724 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9725 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9726 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9727 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9728 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9729 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9730 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9731 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9732 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9733 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9734 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9735 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9736 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9737 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9738 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9739 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9740 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9741 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9742 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9743 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9744 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9745 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9746 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9747 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9748 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9749 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9750 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9751 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9752 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9753 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9754 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9755 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9756 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9757 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9758 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9759 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9760 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9761 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9762 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9763 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9764 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9765 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9766 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9767 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9768 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9769 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9770 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9771 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9772 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9773 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9774 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9775 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9776 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9777 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9778 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9779 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9780 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9781 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9782 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9783 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9784 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9785 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9786 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9787 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9788 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9789 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9790 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9791 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9792 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9793 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9794 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9795 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9796 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9797 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9798 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9799 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9800 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9801 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9802 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9803 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9804 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9805 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9806 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9807 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9808 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9809 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9810 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9811 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9812 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9813 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9814 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9815 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9816 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9817 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9818 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9819 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9820 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9821 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9822 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9823 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9824 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9825 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9826 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9827 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9828 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9829 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9830 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9831 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9832 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9833 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9834 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9835 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9836 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9837 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9838 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9839 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9840 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9841 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9842 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9843 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9844 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9845 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9846 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9847 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9848 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9849 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9850 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9851 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9852 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9853 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9854 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9855 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9856 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9857 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9858 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9859 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9860 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9861 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9862 lambda_k: 1 Loss: 0.0029294082838884935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9863 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9864 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9865 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9866 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9867 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9868 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9869 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9870 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9871 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9872 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9873 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9874 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9875 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9876 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9877 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9878 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9879 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9880 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9881 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9882 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9883 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9884 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9885 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9886 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9887 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9888 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9889 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9890 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9891 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9892 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9893 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9894 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9895 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9896 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9897 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9898 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9899 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9900 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9901 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9902 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9903 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9904 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9905 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9906 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9907 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9908 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9909 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9910 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9911 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9912 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9913 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9914 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9915 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9916 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9917 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9918 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9919 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9920 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9921 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9922 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9923 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9924 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9925 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9926 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9927 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9928 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9929 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9930 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9931 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9932 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9933 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9934 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9935 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9936 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9937 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9938 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9939 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9940 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9941 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9942 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9943 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9944 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9945 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9946 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9947 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9948 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9949 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9950 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9951 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9952 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9953 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9954 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9955 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9956 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9957 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9958 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9959 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9960 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9961 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9962 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9963 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9964 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9965 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9966 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9967 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9968 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9969 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9970 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9971 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9972 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9973 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9974 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9975 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9976 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9977 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9978 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9979 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9980 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9981 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9982 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9983 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9984 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9985 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9986 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9987 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9988 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9989 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9990 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9991 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9992 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9993 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9994 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9995 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9996 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 9997 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Iteration: 9998 lambda_k: 1 Loss: 0.002929408283888494\n",
      "Iteration: 9999 lambda_k: 1 Loss: 0.002929408283888493\n",
      "Iteration: 10000 lambda_k: 1 Loss: 0.0029294082838884935\n",
      "Beta: 0.002180216632244036\n",
      "Gamma: 0.004319263595931534\n",
      "Lambda_k: 1\n"
     ]
    }
   ],
   "source": [
    "# DY\n",
    "    \n",
    "dy = time.time()\n",
    "DY_list, DY_f_list, DY_z_list, Dual_DY_list, iterations_DY   = DY.Davis_Yin(N, M, frobenius_norm, Grad_Phi, D, (x1,x2,x3), Sigma)\n",
    "fin = time.time()\n",
    "\n",
    "Times[\"DY\"] = fin - dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6753c5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:\n",
      "10000\n",
      "Primal: (x1,x2,x3)\n",
      " ((array([[1188.6],\n",
      "       [42.2],\n",
      "       [23.2]]), array([[186.6, 704.0, 833.3, 1188.6, 714.3],\n",
      "       [8.9, 30.9, 42.2, 42.2, 42.2],\n",
      "       [4.6, 15.1, 23.2, 19.2, 23.2]]), array([[0.0, 0.0, 101.3, 0.0, 720.4]])), 'infactible')\n",
      "Primal: (xf1,xf2,xf3)\n",
      " ((array([[1188.6],\n",
      "       [42.2],\n",
      "       [23.2]]), array([[186.6, 704.0, 833.3, 1188.6, 714.3],\n",
      "       [8.9, 30.9, 42.2, 42.2, 42.2],\n",
      "       [4.6, 15.1, 23.2, 19.2, 23.2]]), array([[0.0, 0.0, 101.3, 0.0, 720.4]])), 'infactible')\n",
      "Primal: (z1,z2,z3)\n",
      " ((array([[1188.6],\n",
      "       [41.9],\n",
      "       [22.8]]), array([[186.6, 704.0, 833.3, 1188.8, 714.3],\n",
      "       [8.9, 30.9, 42.8, 42.4, 42.4],\n",
      "       [4.6, 15.1, 24.0, 19.2, 24.4]]), array([[0.0, 0.0, 101.3, 0.0, 720.4]])), 'infactible')\n",
      "Dual: (Capacity, Equilibrium)\n",
      " (array([[0.0, 0.0, 0.0, 25.0, 0.0],\n",
      "       [0.0, 0.0, 144.8, 44.7, 60.5],\n",
      "       [0.0, 0.0, 191.1, 0.0, 283.9]]), array([[1865.7, 6336.1, 10000.0, 8445.5, 10000.0]]))\n"
     ]
    }
   ],
   "source": [
    "iter_ = -1\n",
    "print(f\"Iterations:\\n{iterations_DY}\")\n",
    "print(\"Primal: (x1,x2,x3)\\n\",DY_list[iter_])\n",
    "print(\"Primal: (xf1,xf2,xf3)\\n\",DY_f_list[iter_])\n",
    "print(\"Primal: (z1,z2,z3)\\n\",DY_z_list[iter_])\n",
    "print(\"Dual: (Capacity, Equilibrium)\\n\",Dual_DY_list[iter_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "999e17b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta:  0.002180216632244036\n",
      "gamma: 0.004131340922442053\n",
      "Iteration: 1 lambda_n: 0.9220333915218676 Loss: 0.5857033695247924\n",
      "Iteration: 2 lambda_n: 0.9745460840668471 Loss: 0.5283469234635656\n",
      "Iteration: 3 lambda_n: 1.012509047221805 Loss: 0.5131286758705514\n",
      "Iteration: 4 lambda_n: 1.005130860716019 Loss: 0.5070775813969565\n",
      "Iteration: 5 lambda_n: 0.9165703797144277 Loss: 0.5041250907156893\n",
      "Iteration: 6 lambda_n: 0.984417963892027 Loss: 0.5015355837510774\n",
      "Iteration: 7 lambda_n: 1.0206995580588456 Loss: 0.49889025187273545\n",
      "Iteration: 8 lambda_n: 0.9589026520970261 Loss: 0.4963062326474705\n",
      "Iteration: 9 lambda_n: 0.886134601722777 Loss: 0.4940281105709796\n",
      "Iteration: 10 lambda_n: 0.902994776774917 Loss: 0.4920250820951763\n",
      "Iteration: 11 lambda_n: 0.949247244448805 Loss: 0.490067642778301\n",
      "Iteration: 12 lambda_n: 0.9438157585977144 Loss: 0.48808903586144636\n",
      "Iteration: 13 lambda_n: 0.8764478951373134 Loss: 0.48619374580426467\n",
      "Iteration: 14 lambda_n: 0.9822161770019937 Loss: 0.48449152972139076\n",
      "Iteration: 15 lambda_n: 1.010535470795198 Loss: 0.4826389679947121\n",
      "Iteration: 16 lambda_n: 0.948404423101444 Loss: 0.4807880881193791\n",
      "Iteration: 17 lambda_n: 0.911665931617812 Loss: 0.47909667057865024\n",
      "Iteration: 18 lambda_n: 0.9903048882929741 Loss: 0.47750689541432323\n",
      "Iteration: 19 lambda_n: 0.987165501672685 Loss: 0.47581394229328217\n",
      "Iteration: 20 lambda_n: 1.019127021540012 Loss: 0.47415825978483883\n",
      "Iteration: 21 lambda_n: 0.8761414060601783 Loss: 0.47247803123247656\n",
      "Iteration: 22 lambda_n: 0.9634951317737044 Loss: 0.47105565804192095\n",
      "Iteration: 23 lambda_n: 0.9028993747380599 Loss: 0.46951100884203945\n",
      "Iteration: 24 lambda_n: 0.8810187275698621 Loss: 0.4680813215467774\n",
      "Iteration: 25 lambda_n: 0.9061086780721452 Loss: 0.46670135537226143\n",
      "Iteration: 26 lambda_n: 0.8809611110195832 Loss: 0.4652964091911691\n",
      "Iteration: 27 lambda_n: 0.9166559138013998 Loss: 0.4639439528005644\n",
      "Iteration: 28 lambda_n: 1.0054122421251528 Loss: 0.46255017109203855\n",
      "Iteration: 29 lambda_n: 0.9250189771685884 Loss: 0.4610793549594057\n",
      "Iteration: 30 lambda_n: 0.9685888017416594 Loss: 0.4598784174797892\n",
      "Iteration: 31 lambda_n: 0.9948113385948977 Loss: 0.4586400328026112\n",
      "Iteration: 32 lambda_n: 0.9849516959123917 Loss: 0.45738606198340287\n",
      "Iteration: 33 lambda_n: 0.8999805418641456 Loss: 0.4561610562211234\n",
      "Iteration: 34 lambda_n: 0.9605672860720764 Loss: 0.45505538227939846\n",
      "Iteration: 35 lambda_n: 0.9615257517360907 Loss: 0.45388760764824226\n",
      "Iteration: 36 lambda_n: 0.9112607110352442 Loss: 0.45273089555135393\n",
      "Iteration: 37 lambda_n: 0.9651187275216923 Loss: 0.4516454149308612\n",
      "Iteration: 38 lambda_n: 0.891030197499821 Loss: 0.450505880837644\n",
      "Iteration: 39 lambda_n: 0.9460300996025545 Loss: 0.4494630555151448\n",
      "Iteration: 40 lambda_n: 1.017959629946483 Loss: 0.44836436588802153\n",
      "Iteration: 41 lambda_n: 0.9341657706975943 Loss: 0.4471926596442653\n",
      "Iteration: 42 lambda_n: 0.8980419457448272 Loss: 0.4461270049996872\n",
      "Iteration: 43 lambda_n: 0.9141688118980051 Loss: 0.4451096372160268\n",
      "Iteration: 44 lambda_n: 0.9545330541701599 Loss: 0.44408047766391856\n",
      "Iteration: 45 lambda_n: 0.9097402818068646 Loss: 0.4430124088789528\n",
      "Iteration: 46 lambda_n: 0.987161389066722 Loss: 0.44200062860039885\n",
      "Iteration: 47 lambda_n: 0.8994484959102599 Loss: 0.4409088167370177\n",
      "Iteration: 48 lambda_n: 0.9501146502810569 Loss: 0.43991972684827607\n",
      "Iteration: 49 lambda_n: 0.8767811125555738 Loss: 0.4388801644114402\n",
      "Iteration: 50 lambda_n: 0.9097044544605536 Loss: 0.43792572442813915\n",
      "Iteration: 51 lambda_n: 0.9011696304193934 Loss: 0.4369399293398382\n",
      "Iteration: 52 lambda_n: 1.0032346931102323 Loss: 0.4359678143777826\n",
      "Iteration: 53 lambda_n: 0.8842527859129258 Loss: 0.4348903089388694\n",
      "Iteration: 54 lambda_n: 0.9568597175126335 Loss: 0.4339450374657513\n",
      "Iteration: 55 lambda_n: 0.9518123927793077 Loss: 0.43292623589399964\n",
      "Iteration: 56 lambda_n: 0.8868802122631989 Loss: 0.43191706311522055\n",
      "Iteration: 57 lambda_n: 0.9585267337467996 Loss: 0.4309805467961008\n",
      "Iteration: 58 lambda_n: 0.9878443041086133 Loss: 0.4299720979881142\n",
      "Iteration: 59 lambda_n: 0.9192444376455893 Loss: 0.42893683504177627\n",
      "Iteration: 60 lambda_n: 0.9732155561039045 Loss: 0.42797721568378894\n",
      "Iteration: 61 lambda_n: 0.8942294758679801 Loss: 0.42696485052383376\n",
      "Iteration: 62 lambda_n: 0.970072171942248 Loss: 0.4260380553594041\n",
      "Iteration: 63 lambda_n: 0.9594406342440317 Loss: 0.42503597099911317\n",
      "Iteration: 64 lambda_n: 0.8870135589286774 Loss: 0.42404834526517066\n",
      "Iteration: 65 lambda_n: 1.0123551452040471 Loss: 0.42313837913843627\n",
      "Iteration: 66 lambda_n: 1.0141017112324713 Loss: 0.42210304077231375\n",
      "Iteration: 67 lambda_n: 0.9859183684910267 Loss: 0.4210695160064315\n",
      "Iteration: 68 lambda_n: 0.9057363564201913 Loss: 0.4200681481414079\n",
      "Iteration: 69 lambda_n: 1.0133489861746992 Loss: 0.4191512252148439\n",
      "Iteration: 70 lambda_n: 0.9022626223119441 Loss: 0.4181284014669354\n",
      "Iteration: 71 lambda_n: 0.930182088779199 Loss: 0.41722067927399176\n",
      "Iteration: 72 lambda_n: 0.9716963161297909 Loss: 0.41628755962687297\n",
      "Iteration: 73 lambda_n: 0.87759276308007 Loss: 0.41531565308855034\n",
      "Iteration: 74 lambda_n: 0.8834217316087596 Loss: 0.41444052747638477\n",
      "Iteration: 75 lambda_n: 0.9159171410299133 Loss: 0.4135619747841722\n",
      "Iteration: 76 lambda_n: 1.0237822215715573 Loss: 0.4126535675563829\n",
      "Iteration: 77 lambda_n: 0.9875577842860035 Loss: 0.4116410032279574\n",
      "Iteration: 78 lambda_n: 0.8737514158037365 Loss: 0.41066727349285087\n",
      "Iteration: 79 lambda_n: 0.9491870009764152 Loss: 0.4098082914583527\n",
      "Iteration: 80 lambda_n: 0.9285141127840153 Loss: 0.4088775641091744\n",
      "Iteration: 81 lambda_n: 0.9392251924671055 Loss: 0.4079696488392663\n",
      "Iteration: 82 lambda_n: 0.9848321614359655 Loss: 0.40705375232844676\n",
      "Iteration: 83 lambda_n: 0.8755965882578621 Loss: 0.40609600329540296\n",
      "Iteration: 84 lambda_n: 0.9609306940456802 Loss: 0.4052469060882689\n",
      "Iteration: 85 lambda_n: 0.9320919582265977 Loss: 0.4043174039666158\n",
      "Iteration: 86 lambda_n: 0.977880294713884 Loss: 0.40341827494731913\n",
      "Iteration: 87 lambda_n: 0.909049332541794 Loss: 0.40247748148695534\n",
      "Iteration: 88 lambda_n: 0.9601275601144402 Loss: 0.40160533183633773\n",
      "Iteration: 89 lambda_n: 0.894267486126453 Loss: 0.4006865431914789\n",
      "Iteration: 90 lambda_n: 0.9876715984534126 Loss: 0.39983308948299773\n",
      "Iteration: 91 lambda_n: 0.9105797024149972 Loss: 0.3988928606128349\n",
      "Iteration: 92 lambda_n: 1.0244257394047696 Loss: 0.39802841226621355\n",
      "Iteration: 93 lambda_n: 0.9034321863381368 Loss: 0.3970583565174617\n",
      "Iteration: 94 lambda_n: 0.8964224138699505 Loss: 0.3962053059295824\n",
      "Iteration: 95 lambda_n: 1.020687701353773 Loss: 0.39536099362535887\n",
      "Iteration: 96 lambda_n: 1.015274493969168 Loss: 0.3944020261815195\n",
      "Iteration: 97 lambda_n: 0.9545123130370498 Loss: 0.39345083131970243\n",
      "Iteration: 98 lambda_n: 0.9655775003278293 Loss: 0.3925590620879845\n",
      "Iteration: 99 lambda_n: 0.9885701686205847 Loss: 0.39165932133630965\n",
      "Iteration: 100 lambda_n: 1.00086401175065 Loss: 0.3907405958766342\n",
      "Iteration: 101 lambda_n: 0.9828453910245952 Loss: 0.38981296333823734\n",
      "Iteration: 102 lambda_n: 0.892415396209037 Loss: 0.38890452309843493\n",
      "Iteration: 103 lambda_n: 0.9991491784610893 Loss: 0.3880818779695409\n",
      "Iteration: 104 lambda_n: 0.9063020515643571 Loss: 0.3871630869563355\n",
      "Iteration: 105 lambda_n: 0.9064493008786008 Loss: 0.3863319409588626\n",
      "Iteration: 106 lambda_n: 0.9849881466415986 Loss: 0.38550270887168664\n",
      "Iteration: 107 lambda_n: 0.9232901078482151 Loss: 0.38460384941549175\n",
      "Iteration: 108 lambda_n: 0.9402745851134835 Loss: 0.38376354427679593\n",
      "Iteration: 109 lambda_n: 0.9102669516399606 Loss: 0.382909924218813\n",
      "Iteration: 110 lambda_n: 0.9559496515328492 Loss: 0.3820856510051086\n",
      "Iteration: 111 lambda_n: 0.9201347274193923 Loss: 0.38122214518183395\n",
      "Iteration: 112 lambda_n: 0.9002781892575384 Loss: 0.3803931396569564\n",
      "Iteration: 113 lambda_n: 0.9905117302386907 Loss: 0.37958404146651775\n",
      "Iteration: 114 lambda_n: 1.014374693907047 Loss: 0.37869601616847537\n",
      "Iteration: 115 lambda_n: 0.9885167381560905 Loss: 0.3777890300848351\n",
      "Iteration: 116 lambda_n: 1.0017514605133615 Loss: 0.37690758356976073\n",
      "Iteration: 117 lambda_n: 1.0002058151518616 Loss: 0.37601671794787084\n",
      "Iteration: 118 lambda_n: 0.8789301226579945 Loss: 0.37512962918385895\n",
      "Iteration: 119 lambda_n: 0.927526858088071 Loss: 0.37435219912164247\n",
      "Iteration: 120 lambda_n: 0.9515596351854382 Loss: 0.37353372782628497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 121 lambda_n: 1.0260824749390653 Loss: 0.37269614719118216\n",
      "Iteration: 122 lambda_n: 0.892268783419985 Loss: 0.37179528545201634\n",
      "Iteration: 123 lambda_n: 0.9619830130096957 Loss: 0.37101406721756663\n",
      "Iteration: 124 lambda_n: 0.9040811546127379 Loss: 0.37017383456700026\n",
      "Iteration: 125 lambda_n: 1.0239465794951694 Loss: 0.3693862172963979\n",
      "Iteration: 126 lambda_n: 0.8977319804811511 Loss: 0.3684963470099603\n",
      "Iteration: 127 lambda_n: 0.9101891098025535 Loss: 0.3677183093475632\n",
      "Iteration: 128 lambda_n: 0.9682455863909679 Loss: 0.36693137884882704\n",
      "Iteration: 129 lambda_n: 0.8969852119413795 Loss: 0.36609630253077535\n",
      "Iteration: 130 lambda_n: 0.9893523986108516 Loss: 0.3653246963661287\n",
      "Iteration: 131 lambda_n: 0.9870954889033827 Loss: 0.3644756865131516\n",
      "Iteration: 132 lambda_n: 1.017079833317121 Loss: 0.3636308639282721\n",
      "Iteration: 133 lambda_n: 0.9743336821765339 Loss: 0.36276268657450045\n",
      "Iteration: 134 lambda_n: 0.9892841120153583 Loss: 0.36193327701430605\n",
      "Iteration: 135 lambda_n: 0.9775964695464336 Loss: 0.36109335882923\n",
      "Iteration: 136 lambda_n: 0.8840850004725174 Loss: 0.3602655726284836\n",
      "Iteration: 137 lambda_n: 0.9580222370265872 Loss: 0.3595189314137293\n",
      "Iteration: 138 lambda_n: 0.8892219941458548 Loss: 0.358711768572346\n",
      "Iteration: 139 lambda_n: 0.9652931192295635 Loss: 0.3579644951863334\n",
      "Iteration: 140 lambda_n: 0.9162459471083215 Loss: 0.3571552288567469\n",
      "Iteration: 141 lambda_n: 1.0174169214274442 Loss: 0.3563890666486611\n",
      "Iteration: 142 lambda_n: 0.8882958691088187 Loss: 0.35554039439207874\n",
      "Iteration: 143 lambda_n: 0.915685996846322 Loss: 0.3548014414906494\n",
      "Iteration: 144 lambda_n: 0.9046849543133254 Loss: 0.3540415127541953\n",
      "Iteration: 145 lambda_n: 0.9598875142189884 Loss: 0.3532925502745365\n",
      "Iteration: 146 lambda_n: 0.9657855396980495 Loss: 0.3524998080000486\n",
      "Iteration: 147 lambda_n: 0.9451257123084106 Loss: 0.3517042378962351\n",
      "Iteration: 148 lambda_n: 1.0246353986593273 Loss: 0.35092769118147366\n",
      "Iteration: 149 lambda_n: 0.9172535196846848 Loss: 0.35008793926363674\n",
      "Iteration: 150 lambda_n: 0.9147107470173442 Loss: 0.34933824275180836\n",
      "Iteration: 151 lambda_n: 0.9021286166099363 Loss: 0.34859244988194427\n",
      "Iteration: 152 lambda_n: 0.9167313177803487 Loss: 0.34785870527174634\n",
      "Iteration: 153 lambda_n: 0.9833780633531093 Loss: 0.3471148724074858\n",
      "Iteration: 154 lambda_n: 0.9592594564205635 Loss: 0.34631890800322707\n",
      "Iteration: 155 lambda_n: 1.0132196025805777 Loss: 0.3455444930282499\n",
      "Iteration: 156 lambda_n: 0.9632725718778808 Loss: 0.34472859986688814\n",
      "Iteration: 157 lambda_n: 1.0248441099528254 Loss: 0.34395501029206743\n",
      "Iteration: 158 lambda_n: 1.0122937823935672 Loss: 0.3431340770608503\n",
      "Iteration: 159 lambda_n: 0.98296915118684 Loss: 0.34232539882113194\n",
      "Iteration: 160 lambda_n: 0.9798421802033767 Loss: 0.34154225119601367\n",
      "Iteration: 161 lambda_n: 0.9676713562252159 Loss: 0.34076362579001646\n",
      "Iteration: 162 lambda_n: 1.0049701816787828 Loss: 0.3399966646231026\n",
      "Iteration: 163 lambda_n: 0.9986714473888012 Loss: 0.3392021796328721\n",
      "Iteration: 164 lambda_n: 1.018565560765642 Loss: 0.3384147704852093\n",
      "Iteration: 165 lambda_n: 0.9728924957017469 Loss: 0.33761379420338594\n",
      "Iteration: 166 lambda_n: 1.0271469766895889 Loss: 0.33685078984683037\n",
      "Iteration: 167 lambda_n: 0.87444864610267 Loss: 0.336047304368324\n",
      "Iteration: 168 lambda_n: 0.9001880159714634 Loss: 0.3353651162905892\n",
      "Iteration: 169 lambda_n: 0.9590335887159414 Loss: 0.33466446672396083\n",
      "Iteration: 170 lambda_n: 0.9674718021991539 Loss: 0.3339197865742776\n",
      "Iteration: 171 lambda_n: 0.9664216481682294 Loss: 0.3331704510093309\n",
      "Iteration: 172 lambda_n: 0.9900277065869522 Loss: 0.33242383417560145\n",
      "Iteration: 173 lambda_n: 0.8793628879642446 Loss: 0.3316609246954194\n",
      "Iteration: 174 lambda_n: 0.9061728681852335 Loss: 0.3309850538246604\n",
      "Iteration: 175 lambda_n: 0.9939563736623912 Loss: 0.3302901865515415\n",
      "Iteration: 176 lambda_n: 0.8809690515080683 Loss: 0.32952982122513846\n",
      "Iteration: 177 lambda_n: 0.8954904440659256 Loss: 0.3288576455744429\n",
      "Iteration: 178 lambda_n: 0.9989697959604091 Loss: 0.32817596945332433\n",
      "Iteration: 179 lambda_n: 0.90426428674703 Loss: 0.3274173095678531\n",
      "Iteration: 180 lambda_n: 0.9498431349699217 Loss: 0.32673236890131385\n",
      "Iteration: 181 lambda_n: 0.9261465127754399 Loss: 0.32601460949095834\n",
      "Iteration: 182 lambda_n: 0.9386317582745222 Loss: 0.32531649663842543\n",
      "Iteration: 183 lambda_n: 1.0121285511997624 Loss: 0.32461068775136104\n",
      "Iteration: 184 lambda_n: 0.8753732408585901 Loss: 0.323851483032794\n",
      "Iteration: 185 lambda_n: 0.9074530799496768 Loss: 0.32319659407362406\n",
      "Iteration: 186 lambda_n: 0.9469350716637043 Loss: 0.32251925906393325\n",
      "Iteration: 187 lambda_n: 0.9368945054404278 Loss: 0.321814130455448\n",
      "Iteration: 188 lambda_n: 1.0083405512388997 Loss: 0.3211182030956371\n",
      "Iteration: 189 lambda_n: 0.8890588273254721 Loss: 0.3203710385349434\n",
      "Iteration: 190 lambda_n: 1.0176020606466 Loss: 0.319713989992462\n",
      "Iteration: 191 lambda_n: 1.02130159187625 Loss: 0.31896368953545456\n",
      "Iteration: 192 lambda_n: 0.8889181921875714 Loss: 0.3182126584418814\n",
      "Iteration: 193 lambda_n: 0.9315798680090938 Loss: 0.3175607137484636\n",
      "Iteration: 194 lambda_n: 0.9857231470001694 Loss: 0.31687906262163273\n",
      "Iteration: 195 lambda_n: 0.9737146439741744 Loss: 0.3161595440049549\n",
      "Iteration: 196 lambda_n: 0.9027960978294245 Loss: 0.31545061307776184\n",
      "Iteration: 197 lambda_n: 0.9150628870030844 Loss: 0.3147949782259507\n",
      "Iteration: 198 lambda_n: 0.8987728962437035 Loss: 0.3141319943974407\n",
      "Iteration: 199 lambda_n: 0.9742008302314599 Loss: 0.31348236064701535\n",
      "Iteration: 200 lambda_n: 0.9919727395549324 Loss: 0.31277985279052023\n",
      "Iteration: 201 lambda_n: 0.9794132756273832 Loss: 0.3120663387032157\n",
      "Iteration: 202 lambda_n: 0.9924572573380464 Loss: 0.3113636713176932\n",
      "Iteration: 203 lambda_n: 1.0141533264573759 Loss: 0.3106534545760827\n",
      "Iteration: 204 lambda_n: 0.9318121223973584 Loss: 0.30992957950519806\n",
      "Iteration: 205 lambda_n: 0.9591605932433467 Loss: 0.30926622315497665\n",
      "Iteration: 206 lambda_n: 0.9229843045057825 Loss: 0.30858504602594355\n",
      "Iteration: 207 lambda_n: 0.9057089920079927 Loss: 0.30793118743604175\n",
      "Iteration: 208 lambda_n: 0.9330585960111312 Loss: 0.3072910991246163\n",
      "Iteration: 209 lambda_n: 0.960997431242838 Loss: 0.3066332277440854\n",
      "Iteration: 210 lambda_n: 1.0262737773951733 Loss: 0.30595729303455976\n",
      "Iteration: 211 lambda_n: 0.9491934245665203 Loss: 0.30523723986761964\n",
      "Iteration: 212 lambda_n: 0.9437922077668854 Loss: 0.3045730318168433\n",
      "Iteration: 213 lambda_n: 0.9293768667042518 Loss: 0.30391422223906434\n",
      "Iteration: 214 lambda_n: 0.9021464358346615 Loss: 0.3032670556223156\n",
      "Iteration: 215 lambda_n: 0.9065728398152468 Loss: 0.30264035687573787\n",
      "Iteration: 216 lambda_n: 0.9314300379145991 Loss: 0.3020120490309852\n",
      "Iteration: 217 lambda_n: 1.0141369277668837 Loss: 0.3013680235414597\n",
      "Iteration: 218 lambda_n: 1.001655162975885 Loss: 0.30066849740932855\n",
      "Iteration: 219 lambda_n: 0.9964242827667751 Loss: 0.2999793863005045\n",
      "Iteration: 220 lambda_n: 1.0254672747523381 Loss: 0.2992956426513112\n",
      "Iteration: 221 lambda_n: 0.9089176127883307 Loss: 0.2985937761836346\n",
      "Iteration: 222 lambda_n: 0.9742981830401949 Loss: 0.2979733198541458\n",
      "Iteration: 223 lambda_n: 0.8771789021051695 Loss: 0.297309789935362\n",
      "Iteration: 224 lambda_n: 1.0254604165497518 Loss: 0.2967138964954852\n",
      "Iteration: 225 lambda_n: 0.9989895115440349 Loss: 0.29601884659172306\n",
      "Iteration: 226 lambda_n: 0.8835311393050062 Loss: 0.295343522853283\n",
      "Iteration: 227 lambda_n: 0.9923063850728587 Loss: 0.294747780415871\n",
      "Iteration: 228 lambda_n: 1.0005972074361205 Loss: 0.2940802153444247\n",
      "Iteration: 229 lambda_n: 0.9814073911551658 Loss: 0.293408788568417\n",
      "Iteration: 230 lambda_n: 0.9188353070035692 Loss: 0.2927519300942776\n",
      "Iteration: 231 lambda_n: 1.003381416727372 Loss: 0.2921384989135536\n",
      "Iteration: 232 lambda_n: 1.0074398870977936 Loss: 0.2914702050653355\n",
      "Iteration: 233 lambda_n: 0.9202265231368119 Loss: 0.290800935502134\n",
      "Iteration: 234 lambda_n: 1.0197320180820395 Loss: 0.290191181479914\n",
      "Iteration: 235 lambda_n: 1.0186143996490649 Loss: 0.2895170910398543\n",
      "Iteration: 236 lambda_n: 0.8924978598201839 Loss: 0.2888454996019053\n",
      "Iteration: 237 lambda_n: 0.9191602791665227 Loss: 0.28825859209246435\n",
      "Iteration: 238 lambda_n: 0.9811068712012305 Loss: 0.28765553428839935\n",
      "Iteration: 239 lambda_n: 0.96157735271626 Loss: 0.2870133510165003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 240 lambda_n: 0.983660680033119 Loss: 0.2863855318562646\n",
      "Iteration: 241 lambda_n: 0.9969384333318538 Loss: 0.2857448764553655\n",
      "Iteration: 242 lambda_n: 0.9952980483324225 Loss: 0.28509720896518165\n",
      "Iteration: 243 lambda_n: 0.9598438792037421 Loss: 0.28445225729676554\n",
      "Iteration: 244 lambda_n: 0.978987462093004 Loss: 0.283831863322765\n",
      "Iteration: 245 lambda_n: 0.9846331644981885 Loss: 0.28320065079556744\n",
      "Iteration: 246 lambda_n: 0.9733547652324585 Loss: 0.2825673886589174\n",
      "Iteration: 247 lambda_n: 0.9249988502468888 Loss: 0.28194295689400817\n",
      "Iteration: 248 lambda_n: 0.9295430661174239 Loss: 0.28135102294254166\n",
      "Iteration: 249 lambda_n: 0.9307183160609446 Loss: 0.28075758866378203\n",
      "Iteration: 250 lambda_n: 0.9630471924968578 Loss: 0.2801648169140156\n",
      "Iteration: 251 lambda_n: 0.901885947222938 Loss: 0.27955291617901384\n",
      "Iteration: 252 lambda_n: 0.874937555134623 Loss: 0.2789812858282244\n",
      "Iteration: 253 lambda_n: 0.9559755609224453 Loss: 0.27842801426392033\n",
      "Iteration: 254 lambda_n: 0.9803834196086161 Loss: 0.2778248532615224\n",
      "Iteration: 255 lambda_n: 0.9240820220588574 Loss: 0.2772078057983188\n",
      "Iteration: 256 lambda_n: 0.9030851936740346 Loss: 0.27662765096917535\n",
      "Iteration: 257 lambda_n: 0.8999149582367164 Loss: 0.27606201800325814\n",
      "Iteration: 258 lambda_n: 0.9882576255236775 Loss: 0.2754996728852166\n",
      "Iteration: 259 lambda_n: 0.9855853230566587 Loss: 0.27488354868131676\n",
      "Iteration: 260 lambda_n: 0.9424405672538406 Loss: 0.27427064454363487\n",
      "Iteration: 261 lambda_n: 0.9027372504812937 Loss: 0.27368604790123524\n",
      "Iteration: 262 lambda_n: 0.9075667107558008 Loss: 0.2731274291209017\n",
      "Iteration: 263 lambda_n: 0.9800919420008495 Loss: 0.27256712032773656\n",
      "Iteration: 264 lambda_n: 0.9490388482550459 Loss: 0.2719634456498392\n",
      "Iteration: 265 lambda_n: 0.8740913912560022 Loss: 0.27138036466589743\n",
      "Iteration: 266 lambda_n: 0.9707182398050355 Loss: 0.2708446346498001\n",
      "Iteration: 267 lambda_n: 0.9569655406973243 Loss: 0.27025101927555245\n",
      "Iteration: 268 lambda_n: 1.0194627072169955 Loss: 0.26966727079384567\n",
      "Iteration: 269 lambda_n: 1.0196507867346434 Loss: 0.26904692909842554\n",
      "Iteration: 270 lambda_n: 0.9396487877735697 Loss: 0.26842809740867424\n",
      "Iteration: 271 lambda_n: 1.0274171954534272 Loss: 0.2678593104745394\n",
      "Iteration: 272 lambda_n: 0.9291733740980233 Loss: 0.26723890215836266\n",
      "Iteration: 273 lambda_n: 1.0195358116881876 Loss: 0.2666792980152538\n",
      "Iteration: 274 lambda_n: 1.0049663820674917 Loss: 0.266066745451187\n",
      "Iteration: 275 lambda_n: 0.8952501308985746 Loss: 0.2654645323884013\n",
      "Iteration: 276 lambda_n: 0.9099257106549008 Loss: 0.2649294516336156\n",
      "Iteration: 277 lambda_n: 0.9438866017540027 Loss: 0.26438685722548017\n",
      "Iteration: 278 lambda_n: 0.9393954029542986 Loss: 0.26382533696106863\n",
      "Iteration: 279 lambda_n: 0.8917916769246945 Loss: 0.2632678531630469\n",
      "Iteration: 280 lambda_n: 0.8979504839007278 Loss: 0.26273990549975107\n",
      "Iteration: 281 lambda_n: 1.0101103853173246 Loss: 0.26220954158743115\n",
      "Iteration: 282 lambda_n: 0.9752716616265992 Loss: 0.26161432902577236\n",
      "Iteration: 283 lambda_n: 0.9819785531023182 Loss: 0.26104115341351464\n",
      "Iteration: 284 lambda_n: 0.9338771561028726 Loss: 0.26046550284945474\n",
      "Iteration: 285 lambda_n: 1.0230059378914185 Loss: 0.25991945050407317\n",
      "Iteration: 286 lambda_n: 0.8856177288589593 Loss: 0.25932274916812464\n",
      "Iteration: 287 lambda_n: 0.9830854196142986 Loss: 0.2588075668022503\n",
      "Iteration: 288 lambda_n: 1.0127727146251284 Loss: 0.25823703875597287\n",
      "Iteration: 289 lambda_n: 0.8937836641932632 Loss: 0.2576508161097537\n",
      "Iteration: 290 lambda_n: 0.9557185023272114 Loss: 0.25713484959805855\n",
      "Iteration: 291 lambda_n: 0.9667860214576032 Loss: 0.25658444183271556\n",
      "Iteration: 292 lambda_n: 0.9602988006276877 Loss: 0.256029076249753\n",
      "Iteration: 293 lambda_n: 1.0056399750023934 Loss: 0.2554788590442187\n",
      "Iteration: 294 lambda_n: 0.92817357870111 Loss: 0.25490414655336774\n",
      "Iteration: 295 lambda_n: 0.9347386795018825 Loss: 0.25437513127538314\n",
      "Iteration: 296 lambda_n: 1.019739928009519 Loss: 0.2538437064831913\n",
      "Iteration: 297 lambda_n: 0.9450027627951321 Loss: 0.2532654286356808\n",
      "Iteration: 298 lambda_n: 0.9295144670340342 Loss: 0.2527310103956339\n",
      "Iteration: 299 lambda_n: 0.8857132965621248 Loss: 0.25220670441415904\n",
      "Iteration: 300 lambda_n: 0.8971798219639503 Loss: 0.25170837358908493\n",
      "Iteration: 301 lambda_n: 0.8966858940217608 Loss: 0.2512048231730196\n",
      "Iteration: 302 lambda_n: 0.9395704722415549 Loss: 0.25070279942804613\n",
      "Iteration: 303 lambda_n: 0.9488466815979417 Loss: 0.25017808325793967\n",
      "Iteration: 304 lambda_n: 0.8806417146622896 Loss: 0.2496495820828554\n",
      "Iteration: 305 lambda_n: 0.9326807838698304 Loss: 0.2491603757042627\n",
      "Iteration: 306 lambda_n: 0.8991073848685482 Loss: 0.24864356303787954\n",
      "Iteration: 307 lambda_n: 1.0078419923948465 Loss: 0.24814667983137104\n",
      "Iteration: 308 lambda_n: 0.8813341774725891 Loss: 0.24759116658362146\n",
      "Iteration: 309 lambda_n: 0.9576226685463762 Loss: 0.24710679349153533\n",
      "Iteration: 310 lambda_n: 0.9286989684486991 Loss: 0.24658186970365203\n",
      "Iteration: 311 lambda_n: 0.890207575107883 Loss: 0.24607424656086177\n",
      "Iteration: 312 lambda_n: 0.923547314123316 Loss: 0.24558901681597514\n",
      "Iteration: 313 lambda_n: 1.0243089038458673 Loss: 0.2450869855892983\n",
      "Iteration: 314 lambda_n: 0.9625671272512312 Loss: 0.24453179018095902\n",
      "Iteration: 315 lambda_n: 0.963882740263437 Loss: 0.2440117239380261\n",
      "Iteration: 316 lambda_n: 0.9816981681899732 Loss: 0.24349254593731437\n",
      "Iteration: 317 lambda_n: 0.8779114966123014 Loss: 0.24296543076373742\n",
      "Iteration: 318 lambda_n: 0.9543419823529895 Loss: 0.24249555032389816\n",
      "Iteration: 319 lambda_n: 0.895247148941752 Loss: 0.24198629484482484\n",
      "Iteration: 320 lambda_n: 0.9211813112424335 Loss: 0.24151012820102194\n",
      "Iteration: 321 lambda_n: 0.9469085058605174 Loss: 0.24105291920983687\n",
      "Iteration: 322 lambda_n: 0.9805087901083311 Loss: 0.24059899357969186\n",
      "Iteration: 323 lambda_n: 0.94430143729884 Loss: 0.2401350357681755\n",
      "Iteration: 324 lambda_n: 1.017947324216599 Loss: 0.239693380945897\n",
      "Iteration: 325 lambda_n: 0.8784963813718167 Loss: 0.2392225717995366\n",
      "Iteration: 326 lambda_n: 0.9848117941035909 Loss: 0.23882113016621243\n",
      "Iteration: 327 lambda_n: 0.9833976499971648 Loss: 0.23837576626537307\n",
      "Iteration: 328 lambda_n: 0.9033875054919929 Loss: 0.23793619534478022\n",
      "Iteration: 329 lambda_n: 1.022622809725207 Loss: 0.23753705046363982\n",
      "Iteration: 330 lambda_n: 0.922926499481766 Loss: 0.23709000767776622\n",
      "Iteration: 331 lambda_n: 0.9163124779878619 Loss: 0.23669136787382627\n",
      "Iteration: 332 lambda_n: 0.8904766879847673 Loss: 0.2362998417281513\n",
      "Iteration: 333 lambda_n: 0.9179618617499102 Loss: 0.235923407084206\n",
      "Iteration: 334 lambda_n: 0.9164491477751793 Loss: 0.23553936034198164\n",
      "Iteration: 335 lambda_n: 1.000635650542443 Loss: 0.23516001732475347\n",
      "Iteration: 336 lambda_n: 0.9532633994314618 Loss: 0.2347502081587766\n",
      "Iteration: 337 lambda_n: 0.9951590762350259 Loss: 0.23436429664882308\n",
      "Iteration: 338 lambda_n: 0.9343936889544024 Loss: 0.23396583511463584\n",
      "Iteration: 339 lambda_n: 0.9017952991717672 Loss: 0.23359596934194424\n",
      "Iteration: 340 lambda_n: 0.9189071150442769 Loss: 0.2332426609647302\n",
      "Iteration: 341 lambda_n: 1.0019931243188975 Loss: 0.2328859558324902\n",
      "Iteration: 342 lambda_n: 0.9371086255326011 Loss: 0.2325004370656178\n",
      "Iteration: 343 lambda_n: 0.8748514479950508 Loss: 0.2321431737252131\n",
      "Iteration: 344 lambda_n: 0.9814644724895251 Loss: 0.23181233717675392\n",
      "Iteration: 345 lambda_n: 1.0214345268974616 Loss: 0.2314438452230971\n",
      "Iteration: 346 lambda_n: 1.0008020853664097 Loss: 0.23106329482695237\n",
      "Iteration: 347 lambda_n: 0.9684588631217244 Loss: 0.2306932694718101\n",
      "Iteration: 348 lambda_n: 1.026183130482681 Loss: 0.23033774083532474\n",
      "Iteration: 349 lambda_n: 1.0053008313656333 Loss: 0.22996348407279807\n",
      "Iteration: 350 lambda_n: 0.9051201631891784 Loss: 0.22959926821998922\n",
      "Iteration: 351 lambda_n: 0.9139555043003315 Loss: 0.22927337211405985\n",
      "Iteration: 352 lambda_n: 0.9874565745770183 Loss: 0.22894604304663846\n",
      "Iteration: 353 lambda_n: 0.875485693329505 Loss: 0.22859421287630244\n",
      "Iteration: 354 lambda_n: 0.948852041681004 Loss: 0.22828394573566002\n",
      "Iteration: 355 lambda_n: 0.92808102984401 Loss: 0.22794920790199455\n",
      "Iteration: 356 lambda_n: 0.9417603978122827 Loss: 0.22762335379256518\n",
      "Iteration: 357 lambda_n: 1.0119690933095424 Loss: 0.22729417626306855\n",
      "Iteration: 358 lambda_n: 0.9816071036319579 Loss: 0.22694200733347691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 359 lambda_n: 0.9160945828499073 Loss: 0.2266019540225403\n",
      "Iteration: 360 lambda_n: 0.9635362584570424 Loss: 0.226285940367589\n",
      "Iteration: 361 lambda_n: 0.9746448974029404 Loss: 0.225954830999702\n",
      "Iteration: 362 lambda_n: 1.0238075043507113 Loss: 0.22562120754534962\n",
      "Iteration: 363 lambda_n: 0.9343407874648785 Loss: 0.2252720915980025\n",
      "Iteration: 364 lambda_n: 1.0057024555668594 Loss: 0.22495471863812316\n",
      "Iteration: 365 lambda_n: 0.9305617531957802 Loss: 0.22461427796621755\n",
      "Iteration: 366 lambda_n: 0.9363586392364216 Loss: 0.2243004032852112\n",
      "Iteration: 367 lambda_n: 0.8980827698508046 Loss: 0.22398559262370352\n",
      "Iteration: 368 lambda_n: 0.9007697838177876 Loss: 0.22368460605824958\n",
      "Iteration: 369 lambda_n: 0.9461814287288174 Loss: 0.22338361327207257\n",
      "Iteration: 370 lambda_n: 0.9686604819541279 Loss: 0.22306836498725888\n",
      "Iteration: 371 lambda_n: 0.9364807279879013 Loss: 0.22274659093667146\n",
      "Iteration: 372 lambda_n: 0.9972020916970403 Loss: 0.2224364364625298\n",
      "Iteration: 373 lambda_n: 1.0008729394322924 Loss: 0.22210710676904907\n",
      "Iteration: 374 lambda_n: 1.0173506254722393 Loss: 0.22177754135937666\n",
      "Iteration: 375 lambda_n: 1.0270578449263994 Loss: 0.22144352400147235\n",
      "Iteration: 376 lambda_n: 0.925681638977537 Loss: 0.22110729697846068\n",
      "Iteration: 377 lambda_n: 0.9186985167338321 Loss: 0.22080512691052404\n",
      "Iteration: 378 lambda_n: 1.0140509594725933 Loss: 0.2205059990618102\n",
      "Iteration: 379 lambda_n: 1.0029367271510286 Loss: 0.22017664648942095\n",
      "Iteration: 380 lambda_n: 0.9681545231062046 Loss: 0.2198517850314507\n",
      "Iteration: 381 lambda_n: 0.8823784505164527 Loss: 0.21953901636062387\n",
      "Iteration: 382 lambda_n: 0.9313710702432795 Loss: 0.21925467287495187\n",
      "Iteration: 383 lambda_n: 0.9998884788666323 Loss: 0.21895521993821965\n",
      "Iteration: 384 lambda_n: 0.9419522967644666 Loss: 0.21863449599911303\n",
      "Iteration: 385 lambda_n: 0.987927005302711 Loss: 0.21833311137140293\n",
      "Iteration: 386 lambda_n: 0.9392597016104778 Loss: 0.21801775420609135\n",
      "Iteration: 387 lambda_n: 0.9489123057363703 Loss: 0.21771865748137417\n",
      "Iteration: 388 lambda_n: 1.025562770955421 Loss: 0.21741717554033627\n",
      "Iteration: 389 lambda_n: 0.9224988262359979 Loss: 0.21709208477414688\n",
      "Iteration: 390 lambda_n: 0.9186054729562032 Loss: 0.21680037770743854\n",
      "Iteration: 391 lambda_n: 0.9178759045483946 Loss: 0.2165105346299896\n",
      "Iteration: 392 lambda_n: 1.0241206345413314 Loss: 0.2162215453423963\n",
      "Iteration: 393 lambda_n: 0.9908962308274413 Loss: 0.21589979522996855\n",
      "Iteration: 394 lambda_n: 1.016793176024598 Loss: 0.21558921987334018\n",
      "Iteration: 395 lambda_n: 0.8862910002553791 Loss: 0.21527125255437385\n",
      "Iteration: 396 lambda_n: 0.9184993953092078 Loss: 0.21499473649644585\n",
      "Iteration: 397 lambda_n: 1.008600918586073 Loss: 0.21470874711956786\n",
      "Iteration: 398 lambda_n: 0.9898955927125219 Loss: 0.21439535384170977\n",
      "Iteration: 399 lambda_n: 0.9360656753459924 Loss: 0.21408846755511643\n",
      "Iteration: 400 lambda_n: 0.8853250256808374 Loss: 0.21379890905021173\n",
      "Iteration: 401 lambda_n: 1.014536465372827 Loss: 0.21352561413412816\n",
      "Iteration: 402 lambda_n: 0.9574089296311761 Loss: 0.21321304532879304\n",
      "Iteration: 403 lambda_n: 0.9466795930790823 Loss: 0.21291873370927028\n",
      "Iteration: 404 lambda_n: 0.9866552682992291 Loss: 0.21262832933747725\n",
      "Iteration: 405 lambda_n: 0.9274661698978253 Loss: 0.21232628614531437\n",
      "Iteration: 406 lambda_n: 0.8825972410241794 Loss: 0.21204296923506535\n",
      "Iteration: 407 lambda_n: 0.903625032477858 Loss: 0.21177389819717715\n",
      "Iteration: 408 lambda_n: 0.8975511989998339 Loss: 0.21149893991139884\n",
      "Iteration: 409 lambda_n: 0.9921145564982825 Loss: 0.2112263589719068\n",
      "Iteration: 410 lambda_n: 0.9819026366756786 Loss: 0.21092563870550624\n",
      "Iteration: 411 lambda_n: 0.9532835914144795 Loss: 0.21062864282110125\n",
      "Iteration: 412 lambda_n: 0.9375420743817948 Loss: 0.21034090425651786\n",
      "Iteration: 413 lambda_n: 0.9155962109709468 Loss: 0.21005848794811846\n",
      "Iteration: 414 lambda_n: 0.9690012078406554 Loss: 0.20978322785569087\n",
      "Iteration: 415 lambda_n: 0.9431862181067481 Loss: 0.20949247413058028\n",
      "Iteration: 416 lambda_n: 0.8950158972308163 Loss: 0.20921004152002934\n",
      "Iteration: 417 lambda_n: 0.9487238883924116 Loss: 0.20894256177059325\n",
      "Iteration: 418 lambda_n: 0.905417330318217 Loss: 0.20865956124371496\n",
      "Iteration: 419 lambda_n: 0.9406162077365867 Loss: 0.20839001189704487\n",
      "Iteration: 420 lambda_n: 0.9282956632503934 Loss: 0.20811051040389208\n",
      "Iteration: 421 lambda_n: 0.9443021825553232 Loss: 0.20783520728801633\n",
      "Iteration: 422 lambda_n: 0.9571334839100073 Loss: 0.20755569454262443\n",
      "Iteration: 423 lambda_n: 0.9427631761929358 Loss: 0.2072729354518287\n",
      "Iteration: 424 lambda_n: 0.9912293953323887 Loss: 0.20699496987419874\n",
      "Iteration: 425 lambda_n: 0.9313652676859956 Loss: 0.20670328040607888\n",
      "Iteration: 426 lambda_n: 1.0174623313857187 Loss: 0.20642976288879647\n",
      "Iteration: 427 lambda_n: 0.9082668016193975 Loss: 0.20613153040089585\n",
      "Iteration: 428 lambda_n: 0.9385830985382414 Loss: 0.20586585583961228\n",
      "Iteration: 429 lambda_n: 0.9563881188191747 Loss: 0.20559182104873336\n",
      "Iteration: 430 lambda_n: 0.9545771711124202 Loss: 0.20531311999763652\n",
      "Iteration: 431 lambda_n: 0.9640219282719938 Loss: 0.20503548562460305\n",
      "Iteration: 432 lambda_n: 0.9299850587635143 Loss: 0.2047556454954815\n",
      "Iteration: 433 lambda_n: 0.9187973571134292 Loss: 0.20448621048711196\n",
      "Iteration: 434 lambda_n: 1.0203763053257968 Loss: 0.2042205151578486\n",
      "Iteration: 435 lambda_n: 0.9523245918385898 Loss: 0.2039259914795537\n",
      "Iteration: 436 lambda_n: 0.9583780741633581 Loss: 0.2036516723780996\n",
      "Iteration: 437 lambda_n: 0.9633325372032875 Loss: 0.20337613593683782\n",
      "Iteration: 438 lambda_n: 1.0003373398857236 Loss: 0.2030997055053209\n",
      "Iteration: 439 lambda_n: 0.8955029713286211 Loss: 0.20281320835827038\n",
      "Iteration: 440 lambda_n: 0.893292074534719 Loss: 0.20255724555436658\n",
      "Iteration: 441 lambda_n: 1.0127787787146205 Loss: 0.20230236895664722\n",
      "Iteration: 442 lambda_n: 0.8792635734834163 Loss: 0.20201391335408386\n",
      "Iteration: 443 lambda_n: 0.8771391590475863 Loss: 0.20176398596243378\n",
      "Iteration: 444 lambda_n: 0.9376273472185257 Loss: 0.20151509564391473\n",
      "Iteration: 445 lambda_n: 0.8986589273715795 Loss: 0.20124950269680195\n",
      "Iteration: 446 lambda_n: 0.9984318357377243 Loss: 0.20099541773758636\n",
      "Iteration: 447 lambda_n: 0.9147362931247286 Loss: 0.20071362310029803\n",
      "Iteration: 448 lambda_n: 0.9468553889434169 Loss: 0.20045595566732388\n",
      "Iteration: 449 lambda_n: 0.991012933773741 Loss: 0.20018971916581524\n",
      "Iteration: 450 lambda_n: 0.9564296252979765 Loss: 0.19991158307577514\n",
      "Iteration: 451 lambda_n: 0.9690316405866225 Loss: 0.19964367223081222\n",
      "Iteration: 452 lambda_n: 0.8913506174706757 Loss: 0.1993727375886691\n",
      "Iteration: 453 lambda_n: 0.9615714769311154 Loss: 0.19912399125136995\n",
      "Iteration: 454 lambda_n: 0.8757790260770743 Loss: 0.19885611399639472\n",
      "Iteration: 455 lambda_n: 0.9207776464837477 Loss: 0.19861259125924546\n",
      "Iteration: 456 lambda_n: 0.9355491676474182 Loss: 0.19835699063754603\n",
      "Iteration: 457 lambda_n: 0.9328523195202796 Loss: 0.1980977520052287\n",
      "Iteration: 458 lambda_n: 0.9123900330057869 Loss: 0.1978397274034856\n",
      "Iteration: 459 lambda_n: 0.9587530019262815 Loss: 0.1975878160588904\n",
      "Iteration: 460 lambda_n: 0.9765080609575851 Loss: 0.19732356899676395\n",
      "Iteration: 461 lambda_n: 0.927578281855352 Loss: 0.19705492418318832\n",
      "Iteration: 462 lambda_n: 0.8856366037973118 Loss: 0.19680021765949862\n",
      "Iteration: 463 lambda_n: 0.9506253606860422 Loss: 0.19655745943934444\n",
      "Iteration: 464 lambda_n: 1.0142968948576694 Loss: 0.19629732913879272\n",
      "Iteration: 465 lambda_n: 0.9778193905148901 Loss: 0.19602027988019186\n",
      "Iteration: 466 lambda_n: 1.0276976430911782 Loss: 0.19575370995961242\n",
      "Iteration: 467 lambda_n: 0.9009682523018635 Loss: 0.19547406396013858\n",
      "Iteration: 468 lambda_n: 1.020903077529812 Loss: 0.19522937930675735\n",
      "Iteration: 469 lambda_n: 1.0240524124476371 Loss: 0.19495259756827052\n",
      "Iteration: 470 lambda_n: 0.9491643980823085 Loss: 0.19467549839963377\n",
      "Iteration: 471 lambda_n: 0.9650108430628874 Loss: 0.19441915930198242\n",
      "Iteration: 472 lambda_n: 0.9784639517308712 Loss: 0.19415900722652782\n",
      "Iteration: 473 lambda_n: 0.9102671864629477 Loss: 0.19389570778315535\n",
      "Iteration: 474 lambda_n: 0.9021356951941579 Loss: 0.19365120953382245\n",
      "Iteration: 475 lambda_n: 1.0010897792120665 Loss: 0.1934093092223159\n",
      "Iteration: 476 lambda_n: 0.9988901421618176 Loss: 0.193141329989737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 477 lambda_n: 0.978350516040767 Loss: 0.19287444031212797\n",
      "Iteration: 478 lambda_n: 0.882021645654571 Loss: 0.19261352596439862\n",
      "Iteration: 479 lambda_n: 0.8838352614021056 Loss: 0.19237872949651225\n",
      "Iteration: 480 lambda_n: 0.9297058674203778 Loss: 0.19214383652649475\n",
      "Iteration: 481 lambda_n: 0.9096540899786401 Loss: 0.19189715908334465\n",
      "Iteration: 482 lambda_n: 1.0110114003579547 Loss: 0.1916562181794189\n",
      "Iteration: 483 lambda_n: 1.020202847988228 Loss: 0.19138888317736416\n",
      "Iteration: 484 lambda_n: 0.904994466488162 Loss: 0.19111962244616848\n",
      "Iteration: 485 lambda_n: 0.996957848894676 Loss: 0.19088121742319197\n",
      "Iteration: 486 lambda_n: 0.9964452227167633 Loss: 0.190619025516902\n",
      "Iteration: 487 lambda_n: 0.987387205835959 Loss: 0.19035744942277422\n",
      "Iteration: 488 lambda_n: 1.013807451239934 Loss: 0.19009872576207776\n",
      "Iteration: 489 lambda_n: 0.8764662511872452 Loss: 0.18983356084960645\n",
      "Iteration: 490 lambda_n: 1.0067661400975831 Loss: 0.18960474243151468\n",
      "Iteration: 491 lambda_n: 0.9777274380214058 Loss: 0.1893423294948836\n",
      "Iteration: 492 lambda_n: 0.9941150667612471 Loss: 0.18908795371423304\n",
      "Iteration: 493 lambda_n: 0.9884397314524722 Loss: 0.18882977558697223\n",
      "Iteration: 494 lambda_n: 0.893581250212538 Loss: 0.18857353583461273\n",
      "Iteration: 495 lambda_n: 0.9866596597975349 Loss: 0.18834230202016866\n",
      "Iteration: 496 lambda_n: 0.9014442592442052 Loss: 0.1880873970807949\n",
      "Iteration: 497 lambda_n: 1.0196126242375931 Loss: 0.18785492307973645\n",
      "Iteration: 498 lambda_n: 0.9516589257373589 Loss: 0.18759240464550117\n",
      "Iteration: 499 lambda_n: 1.0215068283260251 Loss: 0.18734783263441418\n",
      "Iteration: 500 lambda_n: 0.893674555509961 Loss: 0.18708576128282178\n",
      "Iteration: 501 lambda_n: 0.8839741030279508 Loss: 0.18685690609895003\n",
      "Iteration: 502 lambda_n: 0.9406266620689107 Loss: 0.1866308985132191\n",
      "Iteration: 503 lambda_n: 0.972852714217014 Loss: 0.18639078854771948\n",
      "Iteration: 504 lambda_n: 0.9112698826293631 Loss: 0.1861428713116706\n",
      "Iteration: 505 lambda_n: 0.8893988917941662 Loss: 0.18591105113942932\n",
      "Iteration: 506 lambda_n: 0.9605605053068033 Loss: 0.185685162877421\n",
      "Iteration: 507 lambda_n: 0.8950179213765219 Loss: 0.1854415889000285\n",
      "Iteration: 508 lambda_n: 0.9618505189258406 Loss: 0.18521502250409322\n",
      "Iteration: 509 lambda_n: 0.8746210075596849 Loss: 0.18497192627003065\n",
      "Iteration: 510 lambda_n: 0.9707595826924935 Loss: 0.18475125295052067\n",
      "Iteration: 511 lambda_n: 0.9882690366567523 Loss: 0.18450670404658565\n",
      "Iteration: 512 lambda_n: 0.9468345723053901 Loss: 0.18425817235110462\n",
      "Iteration: 513 lambda_n: 1.0248816524585858 Loss: 0.18402047620994988\n",
      "Iteration: 514 lambda_n: 0.9653842175440059 Loss: 0.18376361777551345\n",
      "Iteration: 515 lambda_n: 0.9529054401555419 Loss: 0.18352210706476121\n",
      "Iteration: 516 lambda_n: 0.9976228583128222 Loss: 0.18328412293239155\n",
      "Iteration: 517 lambda_n: 0.9385238741037535 Loss: 0.18303538838015534\n",
      "Iteration: 518 lambda_n: 0.8786471526988077 Loss: 0.18280179762355916\n",
      "Iteration: 519 lambda_n: 0.9125910329988988 Loss: 0.18258346848662513\n",
      "Iteration: 520 lambda_n: 0.9689507232365785 Loss: 0.1823570535986133\n",
      "Iteration: 521 lambda_n: 0.8953984190221156 Loss: 0.1821170394925124\n",
      "Iteration: 522 lambda_n: 0.9729214820416853 Loss: 0.18189561860746056\n",
      "Iteration: 523 lambda_n: 0.9158417063694373 Loss: 0.18165540311940395\n",
      "Iteration: 524 lambda_n: 0.8919562716595799 Loss: 0.18142966252586645\n",
      "Iteration: 525 lambda_n: 0.9303350908278013 Loss: 0.18121015854288633\n",
      "Iteration: 526 lambda_n: 1.0136333132443605 Loss: 0.18098156414751818\n",
      "Iteration: 527 lambda_n: 1.0021345128848682 Loss: 0.18073290438368714\n",
      "Iteration: 528 lambda_n: 0.9346067612356759 Loss: 0.18048749595307373\n",
      "Iteration: 529 lambda_n: 0.9280808951130263 Loss: 0.18025901910074113\n",
      "Iteration: 530 lambda_n: 0.8824189663827224 Loss: 0.1800325027846826\n",
      "Iteration: 531 lambda_n: 0.9345686139875724 Loss: 0.17981747450099778\n",
      "Iteration: 532 lambda_n: 0.9152243034456421 Loss: 0.17959008409295\n",
      "Iteration: 533 lambda_n: 0.9161415910309929 Loss: 0.17936775710625935\n",
      "Iteration: 534 lambda_n: 1.0262268461718929 Loss: 0.17914555614219874\n",
      "Iteration: 535 lambda_n: 0.9805642499789807 Loss: 0.1788970464006283\n",
      "Iteration: 536 lambda_n: 0.875369018808977 Loss: 0.17866000987897784\n",
      "Iteration: 537 lambda_n: 0.9025720472274384 Loss: 0.17844875541115285\n",
      "Iteration: 538 lambda_n: 0.9427416202230957 Loss: 0.17823126087254787\n",
      "Iteration: 539 lambda_n: 1.0264734605006829 Loss: 0.17800443562436297\n",
      "Iteration: 540 lambda_n: 1.0178082014059628 Loss: 0.17775786063711152\n",
      "Iteration: 541 lambda_n: 1.015575612762261 Loss: 0.17751379251982663\n",
      "Iteration: 542 lambda_n: 1.0077612966287346 Loss: 0.1772706792691691\n",
      "Iteration: 543 lambda_n: 1.019548340745604 Loss: 0.1770298505656388\n",
      "Iteration: 544 lambda_n: 0.9601407039944916 Loss: 0.17678661947361826\n",
      "Iteration: 545 lambda_n: 0.9398813679035684 Loss: 0.17655795384405598\n",
      "Iteration: 546 lambda_n: 0.9648641948252057 Loss: 0.1763344743929012\n",
      "Iteration: 547 lambda_n: 0.9910145640598068 Loss: 0.17610541708432462\n",
      "Iteration: 548 lambda_n: 0.9347489070439956 Loss: 0.1758705326989534\n",
      "Iteration: 549 lambda_n: 0.8863166097987438 Loss: 0.1756493510917508\n",
      "Iteration: 550 lambda_n: 0.9143586946789242 Loss: 0.17543995685680794\n",
      "Iteration: 551 lambda_n: 0.8963700944774519 Loss: 0.17522425757691584\n",
      "Iteration: 552 lambda_n: 1.0244026554740004 Loss: 0.17501312402100247\n",
      "Iteration: 553 lambda_n: 0.9747190505525986 Loss: 0.17477219498281224\n",
      "Iteration: 554 lambda_n: 0.9607515249641176 Loss: 0.17454334091067583\n",
      "Iteration: 555 lambda_n: 0.9912392518600692 Loss: 0.17431813112158204\n",
      "Iteration: 556 lambda_n: 0.9394595819267203 Loss: 0.17408614511274104\n",
      "Iteration: 557 lambda_n: 1.0111500458204687 Loss: 0.17386663759086857\n",
      "Iteration: 558 lambda_n: 0.9375636253879226 Loss: 0.17363074708219975\n",
      "Iteration: 559 lambda_n: 0.9541412432826722 Loss: 0.17341238768019387\n",
      "Iteration: 560 lambda_n: 0.9621406875962069 Loss: 0.1731905108538007\n",
      "Iteration: 561 lambda_n: 0.948403470082272 Loss: 0.1729671252088688\n",
      "Iteration: 562 lambda_n: 1.0211954658765732 Loss: 0.17274727698030548\n",
      "Iteration: 563 lambda_n: 1.0201055392434697 Loss: 0.17251092420075753\n",
      "Iteration: 564 lambda_n: 1.012055901918285 Loss: 0.17227521871681836\n",
      "Iteration: 565 lambda_n: 0.8938954261152862 Loss: 0.1720417633432381\n",
      "Iteration: 566 lambda_n: 0.9571006499984983 Loss: 0.17183590417320835\n",
      "Iteration: 567 lambda_n: 1.0214129028726802 Loss: 0.1716158112749428\n",
      "Iteration: 568 lambda_n: 1.0250959424060397 Loss: 0.17138129623264375\n",
      "Iteration: 569 lambda_n: 0.9304575369834065 Loss: 0.1711463266363731\n",
      "Iteration: 570 lambda_n: 0.9273048819269135 Loss: 0.17093340378978653\n",
      "Iteration: 571 lambda_n: 0.9543445100572974 Loss: 0.1707215225250657\n",
      "Iteration: 572 lambda_n: 0.9013824572363767 Loss: 0.1705037906967453\n",
      "Iteration: 573 lambda_n: 0.9407417799827703 Loss: 0.17029845885695802\n",
      "Iteration: 574 lambda_n: 0.9496342951846959 Loss: 0.17008447348361264\n",
      "Iteration: 575 lambda_n: 0.9932353349938637 Loss: 0.16986879320161943\n",
      "Iteration: 576 lambda_n: 1.0115049227487227 Loss: 0.16964355581559842\n",
      "Iteration: 577 lambda_n: 0.9867173365273172 Loss: 0.16941454204642756\n",
      "Iteration: 578 lambda_n: 0.9234000552772705 Loss: 0.16919150300686442\n",
      "Iteration: 579 lambda_n: 0.9322016825294314 Loss: 0.16898310584324988\n",
      "Iteration: 580 lambda_n: 0.9251541154617904 Loss: 0.1687730334596334\n",
      "Iteration: 581 lambda_n: 1.0266477287910538 Loss: 0.16856485995405612\n",
      "Iteration: 582 lambda_n: 0.9159830617307855 Loss: 0.1683341914464781\n",
      "Iteration: 583 lambda_n: 1.0018963553344185 Loss: 0.16812872293354433\n",
      "Iteration: 584 lambda_n: 0.9620770912556715 Loss: 0.16790431167407274\n",
      "Iteration: 585 lambda_n: 0.94913149242742 Loss: 0.16768916227356923\n",
      "Iteration: 586 lambda_n: 0.9330083266356741 Loss: 0.1674772320298107\n",
      "Iteration: 587 lambda_n: 1.0223827320991044 Loss: 0.1672692152887104\n",
      "Iteration: 588 lambda_n: 0.924060069747471 Loss: 0.1670416102129881\n",
      "Iteration: 589 lambda_n: 0.9763377367426699 Loss: 0.16683622545309754\n",
      "Iteration: 590 lambda_n: 1.01289577602584 Loss: 0.16661953858712025\n",
      "Iteration: 591 lambda_n: 0.8746364025321526 Loss: 0.1663950847032142\n",
      "Iteration: 592 lambda_n: 1.0119915892024438 Loss: 0.1662015762638217\n",
      "Iteration: 593 lambda_n: 0.9701811348525703 Loss: 0.16597798845706213\n",
      "Iteration: 594 lambda_n: 0.9580890808641744 Loss: 0.1657639784086885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 595 lambda_n: 1.0239679839374332 Loss: 0.16555295717905763\n",
      "Iteration: 596 lambda_n: 0.9786513441916002 Loss: 0.16532776527050966\n",
      "Iteration: 597 lambda_n: 0.9981911418665854 Loss: 0.16511288361479656\n",
      "Iteration: 598 lambda_n: 1.0153587030380749 Loss: 0.16489404689782933\n",
      "Iteration: 599 lambda_n: 0.9278432858726704 Loss: 0.1646717932887615\n",
      "Iteration: 600 lambda_n: 0.9359643856849196 Loss: 0.16446901627473437\n",
      "Iteration: 601 lambda_n: 0.8916288441066601 Loss: 0.16426475972321647\n",
      "Iteration: 602 lambda_n: 1.0271985352285564 Loss: 0.1640704609777765\n",
      "Iteration: 603 lambda_n: 0.881992508435291 Loss: 0.16384693102633158\n",
      "Iteration: 604 lambda_n: 0.9019624362590835 Loss: 0.16365530340925688\n",
      "Iteration: 605 lambda_n: 0.929531954032936 Loss: 0.16345960464456225\n",
      "Iteration: 606 lambda_n: 0.9937656965478594 Loss: 0.1632582055582585\n",
      "Iteration: 607 lambda_n: 0.970948824999544 Loss: 0.16304319884126808\n",
      "Iteration: 608 lambda_n: 1.0004943139772888 Loss: 0.16283345019913453\n",
      "Iteration: 609 lambda_n: 0.9747519479328165 Loss: 0.16261764242234286\n",
      "Iteration: 610 lambda_n: 1.0211370033579152 Loss: 0.16240771037332324\n",
      "Iteration: 611 lambda_n: 0.9517165452895515 Loss: 0.16218811808886813\n",
      "Iteration: 612 lambda_n: 0.9223116781888705 Loss: 0.16198377403103056\n",
      "Iteration: 613 lambda_n: 0.9597432907046909 Loss: 0.16178603172691933\n",
      "Iteration: 614 lambda_n: 0.8814370212832922 Loss: 0.1615805547770209\n",
      "Iteration: 615 lambda_n: 0.9960283528432098 Loss: 0.16139211857075836\n",
      "Iteration: 616 lambda_n: 0.9985083196410044 Loss: 0.16117947239270136\n",
      "Iteration: 617 lambda_n: 0.8902679206535709 Loss: 0.16096662023397898\n",
      "Iteration: 618 lambda_n: 0.9306382509689491 Loss: 0.16077712874006622\n",
      "Iteration: 619 lambda_n: 0.9296881561850731 Loss: 0.16057931290373587\n",
      "Iteration: 620 lambda_n: 1.016197056533946 Loss: 0.160381978068623\n",
      "Iteration: 621 lambda_n: 0.9441318600678485 Loss: 0.1601665859603242\n",
      "Iteration: 622 lambda_n: 0.9676721957619764 Loss: 0.15996677565829628\n",
      "Iteration: 623 lambda_n: 1.0170724057172527 Loss: 0.15976227598097031\n",
      "Iteration: 624 lambda_n: 1.018461880397107 Loss: 0.15954765112715286\n",
      "Iteration: 625 lambda_n: 0.9250510065249731 Loss: 0.15933306265773084\n",
      "Iteration: 626 lambda_n: 0.9458213943756778 Loss: 0.15913845350659686\n",
      "Iteration: 627 lambda_n: 0.8754848797852437 Loss: 0.15893975177891115\n",
      "Iteration: 628 lambda_n: 1.0084676874002667 Loss: 0.15875608707795025\n",
      "Iteration: 629 lambda_n: 1.0031611057178085 Loss: 0.15854480396847512\n",
      "Iteration: 630 lambda_n: 0.9008827814596331 Loss: 0.158334950127056\n",
      "Iteration: 631 lambda_n: 1.0171252419520014 Loss: 0.1581467738351835\n",
      "Iteration: 632 lambda_n: 0.8937300932860411 Loss: 0.1579346043747093\n",
      "Iteration: 633 lambda_n: 0.8902349623713022 Loss: 0.15774845638822305\n",
      "Iteration: 634 lambda_n: 1.0009908337333233 Loss: 0.15756328334634642\n",
      "Iteration: 635 lambda_n: 0.8968331184146306 Loss: 0.1573553499760927\n",
      "Iteration: 636 lambda_n: 0.9814044014261701 Loss: 0.15716932914316517\n",
      "Iteration: 637 lambda_n: 0.9258572311827369 Loss: 0.1569660387888407\n",
      "Iteration: 638 lambda_n: 0.9923929505653849 Loss: 0.15677453315590387\n",
      "Iteration: 639 lambda_n: 0.9441651368321696 Loss: 0.156569547626856\n",
      "Iteration: 640 lambda_n: 1.012952812586877 Loss: 0.15637480970480633\n",
      "Iteration: 641 lambda_n: 0.9565609979997566 Loss: 0.1561661765089322\n",
      "Iteration: 642 lambda_n: 0.9620343776846901 Loss: 0.15596945198073556\n",
      "Iteration: 643 lambda_n: 0.927200990738033 Loss: 0.1557718809022972\n",
      "Iteration: 644 lambda_n: 1.0255731825265344 Loss: 0.1555817328145386\n",
      "Iteration: 645 lambda_n: 0.965840026763792 Loss: 0.15537169893979005\n",
      "Iteration: 646 lambda_n: 0.9362000557377321 Loss: 0.1551741954594449\n",
      "Iteration: 647 lambda_n: 1.0173000098497453 Loss: 0.154983023932584\n",
      "Iteration: 648 lambda_n: 0.9602515564076854 Loss: 0.15477557790970767\n",
      "Iteration: 649 lambda_n: 1.008313676506618 Loss: 0.15458005594185378\n",
      "Iteration: 650 lambda_n: 0.9494375139344922 Loss: 0.15437503661537946\n",
      "Iteration: 651 lambda_n: 0.9912969076314982 Loss: 0.1541822719753483\n",
      "Iteration: 652 lambda_n: 0.9580726818413509 Loss: 0.15398128780073694\n",
      "Iteration: 653 lambda_n: 0.8806228125260809 Loss: 0.15378731983063448\n",
      "Iteration: 654 lambda_n: 0.9299389549597094 Loss: 0.1536092796580484\n",
      "Iteration: 655 lambda_n: 0.9015932456072092 Loss: 0.15342151015267225\n",
      "Iteration: 656 lambda_n: 0.9497342063233344 Loss: 0.15323970946790458\n",
      "Iteration: 657 lambda_n: 0.9108045710945194 Loss: 0.1530484523828519\n",
      "Iteration: 658 lambda_n: 0.979112766182452 Loss: 0.15286528672357172\n",
      "Iteration: 659 lambda_n: 0.9189715873450642 Loss: 0.15266864437551747\n",
      "Iteration: 660 lambda_n: 0.8990499048419978 Loss: 0.15248434098131833\n",
      "Iteration: 661 lambda_n: 0.8974468179615179 Loss: 0.15230427184658135\n",
      "Iteration: 662 lambda_n: 0.9381065532441887 Loss: 0.15212475672862807\n",
      "Iteration: 663 lambda_n: 1.0107692874326626 Loss: 0.15193735150741325\n",
      "Iteration: 664 lambda_n: 1.0144773268047491 Loss: 0.151735703937363\n",
      "Iteration: 665 lambda_n: 0.9347628964554049 Loss: 0.15153361059536635\n",
      "Iteration: 666 lambda_n: 0.9601259497780902 Loss: 0.15134766720906465\n",
      "Iteration: 667 lambda_n: 0.920818463009032 Loss: 0.15115693481507506\n",
      "Iteration: 668 lambda_n: 0.94756770279179 Loss: 0.15097426195989522\n",
      "Iteration: 669 lambda_n: 0.9784250140206173 Loss: 0.15078653045122153\n",
      "Iteration: 670 lambda_n: 0.8884766288825289 Loss: 0.15059294831875383\n",
      "Iteration: 671 lambda_n: 0.986009646286954 Loss: 0.15041740688722552\n",
      "Iteration: 672 lambda_n: 0.9543803675087659 Loss: 0.1502228433001845\n",
      "Iteration: 673 lambda_n: 0.8770273089984395 Loss: 0.15003478491448244\n",
      "Iteration: 674 lambda_n: 0.9160260020735993 Loss: 0.14986220240672882\n",
      "Iteration: 675 lambda_n: 1.0007262369786882 Loss: 0.14968217075416665\n",
      "Iteration: 676 lambda_n: 0.98994691718814 Loss: 0.1494857493272803\n",
      "Iteration: 677 lambda_n: 0.8852612162622762 Loss: 0.14929171916572392\n",
      "Iteration: 678 lambda_n: 0.9477469332283547 Loss: 0.14911844941931976\n",
      "Iteration: 679 lambda_n: 0.9885148828758953 Loss: 0.1489331825859595\n",
      "Iteration: 680 lambda_n: 0.9309893591167914 Loss: 0.14874020569571422\n",
      "Iteration: 681 lambda_n: 0.9193696398078726 Loss: 0.14855871172062152\n",
      "Iteration: 682 lambda_n: 0.9744061955240868 Loss: 0.14837971807309727\n",
      "Iteration: 683 lambda_n: 0.9271180138713198 Loss: 0.14819025562699134\n",
      "Iteration: 684 lambda_n: 0.97235143761995 Loss: 0.14801023435702637\n",
      "Iteration: 685 lambda_n: 0.9476023671626914 Loss: 0.14782167656960396\n",
      "Iteration: 686 lambda_n: 0.977345354650946 Loss: 0.14763816861461315\n",
      "Iteration: 687 lambda_n: 0.9843217424144441 Loss: 0.1474491526808829\n",
      "Iteration: 688 lambda_n: 0.9958459792941471 Loss: 0.14725904824809752\n",
      "Iteration: 689 lambda_n: 0.9235396498634155 Loss: 0.14706698317070047\n",
      "Iteration: 690 lambda_n: 0.8924132454689636 Loss: 0.14688911054665987\n",
      "Iteration: 691 lambda_n: 0.9554548569198432 Loss: 0.1467174540653906\n",
      "Iteration: 692 lambda_n: 0.8740739145420953 Loss: 0.14653390105765052\n",
      "Iteration: 693 lambda_n: 0.8766227710748812 Loss: 0.14636620475192627\n",
      "Iteration: 694 lambda_n: 1.0044163070045022 Loss: 0.14619822407723274\n",
      "Iteration: 695 lambda_n: 1.0203143294583383 Loss: 0.14600599168302933\n",
      "Iteration: 696 lambda_n: 0.964409231728899 Loss: 0.1458109893174683\n",
      "Iteration: 697 lambda_n: 0.9362798217484277 Loss: 0.1456269317508133\n",
      "Iteration: 698 lambda_n: 0.8986495658174736 Loss: 0.1454484811903438\n",
      "Iteration: 699 lambda_n: 0.9680451271715156 Loss: 0.1452774243631404\n",
      "Iteration: 700 lambda_n: 1.0223809809970894 Loss: 0.14509338817626935\n",
      "Iteration: 701 lambda_n: 0.9005725771868227 Loss: 0.14489928293841753\n",
      "Iteration: 702 lambda_n: 0.9125577988909225 Loss: 0.14472854355065312\n",
      "Iteration: 703 lambda_n: 0.9591042461345163 Loss: 0.14455574689996506\n",
      "Iteration: 704 lambda_n: 0.880710111681693 Loss: 0.14437436543251736\n",
      "Iteration: 705 lambda_n: 0.9651725122662582 Loss: 0.14420802839975858\n",
      "Iteration: 706 lambda_n: 0.9412584148368788 Loss: 0.144025961095009\n",
      "Iteration: 707 lambda_n: 0.9094614463585544 Loss: 0.14384863996442557\n",
      "Iteration: 708 lambda_n: 0.9253636466192808 Loss: 0.14367752986874172\n",
      "Iteration: 709 lambda_n: 1.0231863959157208 Loss: 0.14350364513587216\n",
      "Iteration: 710 lambda_n: 0.889387595981507 Loss: 0.14331162365715008\n",
      "Iteration: 711 lambda_n: 0.940281954660407 Loss: 0.1431449442008525\n",
      "Iteration: 712 lambda_n: 0.9039266657059218 Loss: 0.14296894156250403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 713 lambda_n: 0.9966092246161867 Loss: 0.1427999607151216\n",
      "Iteration: 714 lambda_n: 0.9347967346408936 Loss: 0.14261388476691322\n",
      "Iteration: 715 lambda_n: 0.8953438255464912 Loss: 0.14243958602415882\n",
      "Iteration: 716 lambda_n: 0.9826694755204345 Loss: 0.14227285550116775\n",
      "Iteration: 717 lambda_n: 0.9363491175304769 Loss: 0.14209008732362466\n",
      "Iteration: 718 lambda_n: 0.9584348525963808 Loss: 0.14191616629499473\n",
      "Iteration: 719 lambda_n: 1.0110449184643826 Loss: 0.14173836959093428\n",
      "Iteration: 720 lambda_n: 0.8778275061634369 Loss: 0.1415510579174835\n",
      "Iteration: 721 lambda_n: 0.9124735863062187 Loss: 0.1413886479580934\n",
      "Iteration: 722 lambda_n: 0.888605676107 Loss: 0.14122002906426548\n",
      "Iteration: 723 lambda_n: 0.8929818439934984 Loss: 0.1410560231414499\n",
      "Iteration: 724 lambda_n: 1.0127322841841517 Loss: 0.14089140747970222\n",
      "Iteration: 725 lambda_n: 0.937465760580831 Loss: 0.14070494338334902\n",
      "Iteration: 726 lambda_n: 0.9367627832421256 Loss: 0.14053257211752337\n",
      "Iteration: 727 lambda_n: 0.9496166688074867 Loss: 0.14036054768171682\n",
      "Iteration: 728 lambda_n: 1.0118940597033617 Loss: 0.14018638289377672\n",
      "Iteration: 729 lambda_n: 0.9767085564872426 Loss: 0.14000103405452224\n",
      "Iteration: 730 lambda_n: 1.0144562293756636 Loss: 0.13982237292307487\n",
      "Iteration: 731 lambda_n: 0.8956880575330946 Loss: 0.13963705079497576\n",
      "Iteration: 732 lambda_n: 0.8961106806290211 Loss: 0.1394736464653066\n",
      "Iteration: 733 lambda_n: 1.0052249603409817 Loss: 0.13931036123244375\n",
      "Iteration: 734 lambda_n: 0.8949434455860706 Loss: 0.13912741501899656\n",
      "Iteration: 735 lambda_n: 0.9324414965584871 Loss: 0.13896475707950276\n",
      "Iteration: 736 lambda_n: 0.9624446182383849 Loss: 0.13879548696644112\n",
      "Iteration: 737 lambda_n: 1.019641189020855 Loss: 0.13862098819033453\n",
      "Iteration: 738 lambda_n: 0.9212512223201915 Loss: 0.1384363574262938\n",
      "Iteration: 739 lambda_n: 0.8864790048663218 Loss: 0.13826976791426993\n",
      "Iteration: 740 lambda_n: 0.9507943553699235 Loss: 0.13810966242319625\n",
      "Iteration: 741 lambda_n: 0.885694239932468 Loss: 0.13793814444917096\n",
      "Iteration: 742 lambda_n: 0.973331386727834 Loss: 0.13777857124642762\n",
      "Iteration: 743 lambda_n: 0.9449865770340496 Loss: 0.1376034162088481\n",
      "Iteration: 744 lambda_n: 1.0016757404205445 Loss: 0.13743358117809498\n",
      "Iteration: 745 lambda_n: 0.9899907221083604 Loss: 0.1372537842217233\n",
      "Iteration: 746 lambda_n: 0.9957302713363908 Loss: 0.1370763202871894\n",
      "Iteration: 747 lambda_n: 0.8885273723079816 Loss: 0.13689806142703806\n",
      "Iteration: 748 lambda_n: 0.9872319799878034 Loss: 0.13673920238545614\n",
      "Iteration: 749 lambda_n: 0.9719591345625327 Loss: 0.13656290464513499\n",
      "Iteration: 750 lambda_n: 0.9524807131446354 Loss: 0.13638956025151705\n",
      "Iteration: 751 lambda_n: 0.8905059576858 Loss: 0.13621990720303254\n",
      "Iteration: 752 lambda_n: 1.0108911620626104 Loss: 0.13606149115108496\n",
      "Iteration: 753 lambda_n: 0.8972965783976116 Loss: 0.13588187187328582\n",
      "Iteration: 754 lambda_n: 0.9583507964774545 Loss: 0.1357226470370331\n",
      "Iteration: 755 lambda_n: 0.934104965285007 Loss: 0.13555278957590125\n",
      "Iteration: 756 lambda_n: 0.9889431985006638 Loss: 0.1353874374500132\n",
      "Iteration: 757 lambda_n: 0.9788376661589213 Loss: 0.13521259341381803\n",
      "Iteration: 758 lambda_n: 0.9371181723779582 Loss: 0.13503976025074438\n",
      "Iteration: 759 lambda_n: 0.9801772144368154 Loss: 0.13487450506018134\n",
      "Iteration: 760 lambda_n: 1.0208518957141488 Loss: 0.13470186929225308\n",
      "Iteration: 761 lambda_n: 0.9585740237811486 Loss: 0.13452230070210558\n",
      "Iteration: 762 lambda_n: 0.8833531454939393 Loss: 0.1343539108444332\n",
      "Iteration: 763 lambda_n: 0.9249423695864561 Loss: 0.13419892799728164\n",
      "Iteration: 764 lambda_n: 0.9826362906660633 Loss: 0.1340368359516899\n",
      "Iteration: 765 lambda_n: 0.8754135012840826 Loss: 0.13386484176197816\n",
      "Iteration: 766 lambda_n: 1.025347084124547 Loss: 0.13371180981926067\n",
      "Iteration: 767 lambda_n: 0.9519616626727377 Loss: 0.1335327743466729\n",
      "Iteration: 768 lambda_n: 0.9985163454155649 Loss: 0.1333667732438406\n",
      "Iteration: 769 lambda_n: 0.9105710964393179 Loss: 0.13319287009245964\n",
      "Iteration: 770 lambda_n: 0.9785208028526683 Loss: 0.13303448797766268\n",
      "Iteration: 771 lambda_n: 1.0054409190082225 Loss: 0.1328644889263814\n",
      "Iteration: 772 lambda_n: 0.9730109637808424 Loss: 0.13269003498396692\n",
      "Iteration: 773 lambda_n: 0.9395139286837807 Loss: 0.13252142731990205\n",
      "Iteration: 774 lambda_n: 1.017579451432185 Loss: 0.1323588286795257\n",
      "Iteration: 775 lambda_n: 0.9453444185505955 Loss: 0.13218293464599454\n",
      "Iteration: 776 lambda_n: 0.9211013185961511 Loss: 0.13201974061999283\n",
      "Iteration: 777 lambda_n: 1.0270097108371707 Loss: 0.13186092537412913\n",
      "Iteration: 778 lambda_n: 0.9484427126293035 Loss: 0.13168406159706247\n",
      "Iteration: 779 lambda_n: 1.0173135474220767 Loss: 0.13152094311755252\n",
      "Iteration: 780 lambda_n: 0.9278357943314929 Loss: 0.13134619465849245\n",
      "Iteration: 781 lambda_n: 0.9294254463357597 Loss: 0.13118702358792114\n",
      "Iteration: 782 lambda_n: 0.9748164594188636 Loss: 0.13102777007902525\n",
      "Iteration: 783 lambda_n: 0.9626349040264489 Loss: 0.13086093917150457\n",
      "Iteration: 784 lambda_n: 0.9258799595634195 Loss: 0.1306963990717298\n",
      "Iteration: 785 lambda_n: 1.0142863712137344 Loss: 0.1305383363304065\n",
      "Iteration: 786 lambda_n: 0.9567334771614388 Loss: 0.13036538813177195\n",
      "Iteration: 787 lambda_n: 0.9642862926667648 Loss: 0.1302024645588262\n",
      "Iteration: 788 lambda_n: 1.0072301198355555 Loss: 0.13003845605810416\n",
      "Iteration: 789 lambda_n: 0.9465175933838396 Loss: 0.12986735556738666\n",
      "Iteration: 790 lambda_n: 0.9604254353013423 Loss: 0.12970677459818145\n",
      "Iteration: 791 lambda_n: 0.9515100149252986 Loss: 0.1295440313240744\n",
      "Iteration: 792 lambda_n: 0.8816182313538349 Loss: 0.12938299630176864\n",
      "Iteration: 793 lambda_n: 0.9056456233912877 Loss: 0.12923397000924972\n",
      "Iteration: 794 lambda_n: 0.9780885687236924 Loss: 0.1290810545727073\n",
      "Iteration: 795 lambda_n: 0.9038112599603951 Loss: 0.12891609900841422\n",
      "Iteration: 796 lambda_n: 0.9101928886751511 Loss: 0.12876385912414226\n",
      "Iteration: 797 lambda_n: 0.9717614164647495 Loss: 0.12861072066847332\n",
      "Iteration: 798 lambda_n: 1.0224582199543115 Loss: 0.12844741353386396\n",
      "Iteration: 799 lambda_n: 0.9027889412249763 Loss: 0.12827579961461866\n",
      "Iteration: 800 lambda_n: 0.9416506137646853 Loss: 0.12812446650838868\n",
      "Iteration: 801 lambda_n: 0.9735483702674841 Loss: 0.12796680043794467\n",
      "Iteration: 802 lambda_n: 0.8972383519349206 Loss: 0.12780398863191644\n",
      "Iteration: 803 lambda_n: 1.0108715086145825 Loss: 0.1276541225600145\n",
      "Iteration: 804 lambda_n: 0.9274006064047315 Loss: 0.1274854699218605\n",
      "Iteration: 805 lambda_n: 0.9041754978505886 Loss: 0.1273309401465918\n",
      "Iteration: 806 lambda_n: 0.9837093608380304 Loss: 0.12718045662303168\n",
      "Iteration: 807 lambda_n: 0.8992939367768432 Loss: 0.12701692442972762\n",
      "Iteration: 808 lambda_n: 0.9571477431307758 Loss: 0.12686760998562224\n",
      "Iteration: 809 lambda_n: 0.9461428624777848 Loss: 0.126708871002771\n",
      "Iteration: 810 lambda_n: 1.0211140343311538 Loss: 0.1265521463777369\n",
      "Iteration: 811 lambda_n: 0.9350505495075 Loss: 0.12638320600820582\n",
      "Iteration: 812 lambda_n: 0.9976060612985373 Loss: 0.1262287022696655\n",
      "Iteration: 813 lambda_n: 0.9610700818679629 Loss: 0.12606405712612845\n",
      "Iteration: 814 lambda_n: 0.8856666499445627 Loss: 0.12590564036436813\n",
      "Iteration: 815 lambda_n: 1.019344577533548 Loss: 0.125759827921809\n",
      "Iteration: 816 lambda_n: 0.9478397283480425 Loss: 0.12559219621122264\n",
      "Iteration: 817 lambda_n: 0.9612113475944605 Loss: 0.1254365218989479\n",
      "Iteration: 818 lambda_n: 0.9479596905807895 Loss: 0.12527883936219558\n",
      "Iteration: 819 lambda_n: 0.929390208754094 Loss: 0.12512351792248957\n",
      "Iteration: 820 lambda_n: 1.01805010837333 Loss: 0.12497141966566856\n",
      "Iteration: 821 lambda_n: 0.8739407475925758 Loss: 0.12480500725198622\n",
      "Iteration: 822 lambda_n: 0.9547157351552271 Loss: 0.12466233121033032\n",
      "Iteration: 823 lambda_n: 0.9890450385912014 Loss: 0.12450663979024036\n",
      "Iteration: 824 lambda_n: 0.9561630072202693 Loss: 0.12434554312180449\n",
      "Iteration: 825 lambda_n: 0.9496327615879573 Loss: 0.12418999423919291\n",
      "Iteration: 826 lambda_n: 0.958084652241397 Loss: 0.12403569202433855\n",
      "Iteration: 827 lambda_n: 0.9623955766932885 Loss: 0.1238802010925921\n",
      "Iteration: 828 lambda_n: 0.889284672438749 Loss: 0.123724197208187\n",
      "Iteration: 829 lambda_n: 0.9011836468037295 Loss: 0.12358021640867353\n",
      "Iteration: 830 lambda_n: 0.9561437411755541 Loss: 0.12343447088148113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 831 lambda_n: 0.9319283784143254 Loss: 0.12328001117807522\n",
      "Iteration: 832 lambda_n: 0.8935643071198864 Loss: 0.12312964206309918\n",
      "Iteration: 833 lambda_n: 0.8987745574453495 Loss: 0.122985629612559\n",
      "Iteration: 834 lambda_n: 0.9907878931826233 Loss: 0.12284093838428353\n",
      "Iteration: 835 lambda_n: 0.9730514635622427 Loss: 0.12268161375900861\n",
      "Iteration: 836 lambda_n: 0.9836061365830738 Loss: 0.12252533344738853\n",
      "Iteration: 837 lambda_n: 0.9736042434275792 Loss: 0.12236754887008278\n",
      "Iteration: 838 lambda_n: 0.9773093708243868 Loss: 0.12221155908189747\n",
      "Iteration: 839 lambda_n: 0.8779519993665095 Loss: 0.1220551646664781\n",
      "Iteration: 840 lambda_n: 0.9555760039473766 Loss: 0.12191483859611142\n",
      "Iteration: 841 lambda_n: 0.925357249341134 Loss: 0.12176227284837411\n",
      "Iteration: 842 lambda_n: 0.9088490223063507 Loss: 0.12161470600284453\n",
      "Iteration: 843 lambda_n: 0.8946870765688614 Loss: 0.12146993733630236\n",
      "Iteration: 844 lambda_n: 0.942790893109778 Loss: 0.12132758438037665\n",
      "Iteration: 845 lambda_n: 0.9769810950687058 Loss: 0.12117774422418338\n",
      "Iteration: 846 lambda_n: 0.9267605540166926 Loss: 0.12102265145630814\n",
      "Iteration: 847 lambda_n: 0.9867834438588186 Loss: 0.12087570766123737\n",
      "Iteration: 848 lambda_n: 0.9735643671995785 Loss: 0.12071942670753791\n",
      "Iteration: 849 lambda_n: 0.8871633310315767 Loss: 0.12056542668391632\n",
      "Iteration: 850 lambda_n: 0.8996970003993906 Loss: 0.12042526078446716\n",
      "Iteration: 851 lambda_n: 0.973138305152973 Loss: 0.1202832701474501\n",
      "Iteration: 852 lambda_n: 0.9060866989240554 Loss: 0.12012986026438709\n",
      "Iteration: 853 lambda_n: 0.8761740757792819 Loss: 0.11998719070585209\n",
      "Iteration: 854 lambda_n: 1.025071344715952 Loss: 0.11984938437846383\n",
      "Iteration: 855 lambda_n: 0.9670559701701884 Loss: 0.11968833542181403\n",
      "Iteration: 856 lambda_n: 1.0133722383596424 Loss: 0.11953659167024054\n",
      "Iteration: 857 lambda_n: 1.0102539850766927 Loss: 0.11937776992036908\n",
      "Iteration: 858 lambda_n: 0.9757590877851685 Loss: 0.11921963376908655\n",
      "Iteration: 859 lambda_n: 0.9832932197840443 Loss: 0.11906708586674071\n",
      "Iteration: 860 lambda_n: 0.8953751076652191 Loss: 0.11891354414407874\n",
      "Iteration: 861 lambda_n: 0.8757758531017056 Loss: 0.11877389802304293\n",
      "Iteration: 862 lambda_n: 0.9252208850486695 Loss: 0.11863745819021214\n",
      "Iteration: 863 lambda_n: 0.900180866201815 Loss: 0.11849347050554043\n",
      "Iteration: 864 lambda_n: 0.9395908956327381 Loss: 0.11835353792521369\n",
      "Iteration: 865 lambda_n: 0.8987313293014981 Loss: 0.11820764055344848\n",
      "Iteration: 866 lambda_n: 0.886888495076124 Loss: 0.11806824746140265\n",
      "Iteration: 867 lambda_n: 1.0041459425376833 Loss: 0.11793084211011565\n",
      "Iteration: 868 lambda_n: 0.9835393030085168 Loss: 0.11777544068810931\n",
      "Iteration: 869 lambda_n: 0.9501261516415509 Loss: 0.11762341465501058\n",
      "Iteration: 870 lambda_n: 0.979715820693474 Loss: 0.11747672907420839\n",
      "Iteration: 871 lambda_n: 1.0174494790081985 Loss: 0.1173256511200839\n",
      "Iteration: 872 lambda_n: 0.9246246585574793 Loss: 0.11716894254098469\n",
      "Iteration: 873 lambda_n: 0.8833327812545914 Loss: 0.11702670608121696\n",
      "Iteration: 874 lambda_n: 1.0091385398370367 Loss: 0.11689097406966228\n",
      "Iteration: 875 lambda_n: 0.9163402617744097 Loss: 0.11673607989536353\n",
      "Iteration: 876 lambda_n: 0.9981537171490231 Loss: 0.11659560079326917\n",
      "Iteration: 877 lambda_n: 0.9442351102933055 Loss: 0.11644275137156376\n",
      "Iteration: 878 lambda_n: 0.9365621355608673 Loss: 0.11629833329874874\n",
      "Iteration: 879 lambda_n: 0.9047840884256786 Loss: 0.11615525311676557\n",
      "Iteration: 880 lambda_n: 0.9650095967908358 Loss: 0.1160171845587173\n",
      "Iteration: 881 lambda_n: 0.9815810937012465 Loss: 0.11587008857160862\n",
      "Iteration: 882 lambda_n: 1.0276885848068147 Loss: 0.11572064223278307\n",
      "Iteration: 883 lambda_n: 0.9950012279084713 Loss: 0.11556436327486407\n",
      "Iteration: 884 lambda_n: 0.9682135325928735 Loss: 0.11541324315346545\n",
      "Iteration: 885 lambda_n: 1.0248537022133457 Loss: 0.115266368547892\n",
      "Iteration: 886 lambda_n: 0.9789288283833125 Loss: 0.11511108535107463\n",
      "Iteration: 887 lambda_n: 0.9008312793836087 Loss: 0.11496294401656036\n",
      "Iteration: 888 lambda_n: 0.9104318155400573 Loss: 0.11482678161623819\n",
      "Iteration: 889 lambda_n: 1.0102878305147613 Loss: 0.11468931834894998\n",
      "Iteration: 890 lambda_n: 1.0148587153979864 Loss: 0.11453694803273033\n",
      "Iteration: 891 lambda_n: 0.9444392627663868 Loss: 0.11438407555355849\n",
      "Iteration: 892 lambda_n: 0.9565053078250846 Loss: 0.1142419841266478\n",
      "Iteration: 893 lambda_n: 0.9622303135750397 Loss: 0.11409824190048837\n",
      "Iteration: 894 lambda_n: 1.0161534335091533 Loss: 0.11395380661075528\n",
      "Iteration: 895 lambda_n: 0.8948537643388754 Loss: 0.1138014555237413\n",
      "Iteration: 896 lambda_n: 0.9751516906595663 Loss: 0.11366745353948766\n",
      "Iteration: 897 lambda_n: 0.9275696703835529 Loss: 0.11352158632210921\n",
      "Iteration: 898 lambda_n: 0.9670021151955879 Loss: 0.11338299914309936\n",
      "Iteration: 899 lambda_n: 1.009681184710829 Loss: 0.11323868278519154\n",
      "Iteration: 900 lambda_n: 0.9121956460096226 Loss: 0.11308817349045618\n",
      "Iteration: 901 lambda_n: 0.9329456026816161 Loss: 0.11295235994191988\n",
      "Iteration: 902 lambda_n: 0.9873685532447 Loss: 0.11281361007565816\n",
      "Iteration: 903 lambda_n: 1.016286350464576 Loss: 0.11266693234037992\n",
      "Iteration: 904 lambda_n: 0.8923103782162544 Loss: 0.11251613883684082\n",
      "Iteration: 905 lambda_n: 1.0067799830725208 Loss: 0.11238390054654905\n",
      "Iteration: 906 lambda_n: 1.0126719566647397 Loss: 0.11223486046866626\n",
      "Iteration: 907 lambda_n: 0.9194119140896656 Loss: 0.11208512990038016\n",
      "Iteration: 908 lambda_n: 0.9429995685519921 Loss: 0.1119493525331005\n",
      "Iteration: 909 lambda_n: 0.97471170224503 Loss: 0.11181024615328372\n",
      "Iteration: 910 lambda_n: 0.9307437813807129 Loss: 0.11166662533251526\n",
      "Iteration: 911 lambda_n: 0.9317176396423278 Loss: 0.11152964296589711\n",
      "Iteration: 912 lambda_n: 1.0242697853323868 Loss: 0.11139267063038424\n",
      "Iteration: 913 lambda_n: 0.9606771193196285 Loss: 0.11124226239311338\n",
      "Iteration: 914 lambda_n: 0.9476753250952561 Loss: 0.11110136477511284\n",
      "Iteration: 915 lambda_n: 0.88149376213728 Loss: 0.1109625341235235\n",
      "Iteration: 916 lambda_n: 0.9488448192211201 Loss: 0.11083354459934308\n",
      "Iteration: 917 lambda_n: 0.9657958997764715 Loss: 0.11069484755441814\n",
      "Iteration: 918 lambda_n: 0.9370220647349237 Loss: 0.11055383365483121\n",
      "Iteration: 919 lambda_n: 1.0024714791573581 Loss: 0.11041717890628326\n",
      "Iteration: 920 lambda_n: 0.9691273525933253 Loss: 0.11027114447652779\n",
      "Iteration: 921 lambda_n: 0.9187508590180841 Loss: 0.11013013644636017\n",
      "Iteration: 922 lambda_n: 1.0096563061535235 Loss: 0.10999661255456375\n",
      "Iteration: 923 lambda_n: 0.9672778532617908 Loss: 0.10985004035781604\n",
      "Iteration: 924 lambda_n: 0.9389753509228814 Loss: 0.10970978927163647\n",
      "Iteration: 925 lambda_n: 0.9246985301037887 Loss: 0.10957379908441306\n",
      "Iteration: 926 lambda_n: 0.9524564038852152 Loss: 0.10944002683640604\n",
      "Iteration: 927 lambda_n: 1.014878116238514 Loss: 0.10930239191146715\n",
      "Iteration: 928 lambda_n: 0.9395159312646352 Loss: 0.10915590499087971\n",
      "Iteration: 929 lambda_n: 0.9667549215352398 Loss: 0.10902045908078631\n",
      "Iteration: 930 lambda_n: 0.8999384322252929 Loss: 0.10888124329072264\n",
      "Iteration: 931 lambda_n: 0.9759913889138319 Loss: 0.10875179792363635\n",
      "Iteration: 932 lambda_n: 0.9877253311462428 Loss: 0.10861156561126122\n",
      "Iteration: 933 lambda_n: 0.8771143019307419 Loss: 0.10846981305150351\n",
      "Iteration: 934 lambda_n: 0.930747752246913 Loss: 0.10834408143882888\n",
      "Iteration: 935 lambda_n: 1.0229697771786501 Loss: 0.10821080232132038\n",
      "Iteration: 936 lambda_n: 0.9766442439487285 Loss: 0.10806448195893126\n",
      "Iteration: 937 lambda_n: 0.9381634934097277 Loss: 0.10792495750228298\n",
      "Iteration: 938 lambda_n: 0.9987523433191453 Loss: 0.10779108598519305\n",
      "Iteration: 939 lambda_n: 0.8905918607351772 Loss: 0.10764872939662255\n",
      "Iteration: 940 lambda_n: 0.8963810011556461 Loss: 0.10752193882435354\n",
      "Iteration: 941 lambda_n: 0.8750873686149147 Loss: 0.10739445971528694\n",
      "Iteration: 942 lambda_n: 0.9258478284500538 Loss: 0.10727014155345507\n",
      "Iteration: 943 lambda_n: 0.9226688423711121 Loss: 0.1071387502811527\n",
      "Iteration: 944 lambda_n: 0.9441265540521787 Loss: 0.10700795460189091\n",
      "Iteration: 945 lambda_n: 0.9681096671935759 Loss: 0.10687426467213113\n",
      "Iteration: 946 lambda_n: 0.9200958359961097 Loss: 0.10673733335146529\n",
      "Iteration: 947 lambda_n: 0.9389025773613767 Loss: 0.10660734245483666\n",
      "Iteration: 948 lambda_n: 0.8818796636003347 Loss: 0.10647484025357859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 949 lambda_n: 1.0167715890733784 Loss: 0.10635052358044438\n",
      "Iteration: 950 lambda_n: 0.9602503830747334 Loss: 0.1062073446055311\n",
      "Iteration: 951 lambda_n: 1.007471969352026 Loss: 0.10607228743330235\n",
      "Iteration: 952 lambda_n: 0.8894511401359234 Loss: 0.10593075151326384\n",
      "Iteration: 953 lambda_n: 0.9998980225882724 Loss: 0.10580594369628604\n",
      "Iteration: 954 lambda_n: 1.0055354305993167 Loss: 0.10566578857515586\n",
      "Iteration: 955 lambda_n: 0.987175827070197 Loss: 0.10552501103588613\n",
      "Iteration: 956 lambda_n: 0.8958464676197061 Loss: 0.10538696884598074\n",
      "Iteration: 957 lambda_n: 0.8906251011466543 Loss: 0.10526184323338571\n",
      "Iteration: 958 lambda_n: 0.9231716268910705 Loss: 0.105137579323241\n",
      "Iteration: 959 lambda_n: 0.9200498440350126 Loss: 0.10500891134573948\n",
      "Iteration: 960 lambda_n: 0.9636631630064629 Loss: 0.10488081913797692\n",
      "Iteration: 961 lambda_n: 0.968023475514667 Loss: 0.10474680247751313\n",
      "Iteration: 962 lambda_n: 1.0198129374274114 Loss: 0.10461233368683034\n",
      "Iteration: 963 lambda_n: 0.9069773089151506 Loss: 0.10447083476522383\n",
      "Iteration: 964 lambda_n: 0.9690917889754742 Loss: 0.10434514226042386\n",
      "Iteration: 965 lambda_n: 0.9234138811587748 Loss: 0.10421098755455209\n",
      "Iteration: 966 lambda_n: 1.0273839641080893 Loss: 0.10408330251866052\n",
      "Iteration: 967 lambda_n: 0.8880772263611701 Loss: 0.10394139890343466\n",
      "Iteration: 968 lambda_n: 0.9285275281039677 Loss: 0.10381888374869855\n",
      "Iteration: 969 lambda_n: 0.9547149432867894 Loss: 0.10369092405364665\n",
      "Iteration: 970 lambda_n: 0.9505938290990462 Loss: 0.10355950101467912\n",
      "Iteration: 971 lambda_n: 1.0184903137830248 Loss: 0.10342879349997268\n",
      "Iteration: 972 lambda_n: 1.0201043868638933 Loss: 0.10328890952938696\n",
      "Iteration: 973 lambda_n: 0.9301193937413712 Loss: 0.10314897326744413\n",
      "Iteration: 974 lambda_n: 0.9482172528593382 Loss: 0.10302153381013879\n",
      "Iteration: 975 lambda_n: 0.9170623979044361 Loss: 0.10289175841322236\n",
      "Iteration: 976 lambda_n: 0.9731118870112472 Loss: 0.10276638758436091\n",
      "Iteration: 977 lambda_n: 0.9601437866785102 Loss: 0.10263350008805783\n",
      "Iteration: 978 lambda_n: 0.8991043417128549 Loss: 0.10250253461410681\n",
      "Iteration: 979 lambda_n: 0.9831429157386055 Loss: 0.10238003361773712\n",
      "Iteration: 980 lambda_n: 0.9536467265107212 Loss: 0.10224622702936641\n",
      "Iteration: 981 lambda_n: 0.9576319985933224 Loss: 0.10211658566978922\n",
      "Iteration: 982 lambda_n: 0.8945755306185798 Loss: 0.10198654983780028\n",
      "Iteration: 983 lambda_n: 0.9590589406885122 Loss: 0.1018652131852186\n",
      "Iteration: 984 lambda_n: 0.8985670139706228 Loss: 0.10173526946691769\n",
      "Iteration: 985 lambda_n: 0.885043640461818 Loss: 0.10161365919673071\n",
      "Iteration: 986 lambda_n: 0.9702653058668554 Loss: 0.10149400650616053\n",
      "Iteration: 987 lambda_n: 1.0000704881154312 Loss: 0.10136297162522764\n",
      "Iteration: 988 lambda_n: 0.8865727043990347 Loss: 0.10122806747309356\n",
      "Iteration: 989 lambda_n: 0.8874874662187181 Loss: 0.10110861330738154\n",
      "Iteration: 990 lambda_n: 0.9703698248340464 Loss: 0.1009891615526259\n",
      "Iteration: 991 lambda_n: 0.8806377444162153 Loss: 0.10085869320494002\n",
      "Iteration: 992 lambda_n: 0.9025739126501864 Loss: 0.100740424102188\n",
      "Iteration: 993 lambda_n: 0.9140557783103147 Loss: 0.10061933588284304\n",
      "Iteration: 994 lambda_n: 1.0006041509543135 Loss: 0.10049683863877487\n",
      "Iteration: 995 lambda_n: 0.8936145609997164 Loss: 0.10036288960478254\n",
      "Iteration: 996 lambda_n: 0.9954885375266758 Loss: 0.1002434029678173\n",
      "Iteration: 997 lambda_n: 0.9925267305061238 Loss: 0.10011043759385978\n",
      "Iteration: 998 lambda_n: 0.9224848242975299 Loss: 0.09997802413295523\n",
      "Iteration: 999 lambda_n: 1.020601689333783 Loss: 0.09985509840716889\n",
      "Iteration: 1000 lambda_n: 0.9016649776393019 Loss: 0.09971924879925885\n",
      "Iteration: 1001 lambda_n: 0.9354692723127231 Loss: 0.09959937348110326\n",
      "Iteration: 1002 lambda_n: 1.0052964095250463 Loss: 0.09947513742041324\n",
      "Iteration: 1003 lambda_n: 0.9924136202916555 Loss: 0.09934177728819736\n",
      "Iteration: 1004 lambda_n: 0.9806665697672219 Loss: 0.09921028269077072\n",
      "Iteration: 1005 lambda_n: 0.9331901447896604 Loss: 0.09908049711057765\n",
      "Iteration: 1006 lambda_n: 0.9640741196486671 Loss: 0.09895713733592047\n",
      "Iteration: 1007 lambda_n: 0.9342873482327414 Loss: 0.09882983646652017\n",
      "Iteration: 1008 lambda_n: 0.876502900672633 Loss: 0.09870660910992007\n",
      "Iteration: 1009 lambda_n: 0.9555977397807408 Loss: 0.09859113011387347\n",
      "Iteration: 1010 lambda_n: 1.021239943647142 Loss: 0.09846536268516419\n",
      "Iteration: 1011 lambda_n: 0.9226678837056468 Loss: 0.09833110955144263\n",
      "Iteration: 1012 lambda_n: 0.9098040527438414 Loss: 0.09820995966269877\n",
      "Iteration: 1013 lambda_n: 0.8897497332726034 Loss: 0.09809062918433957\n",
      "Iteration: 1014 lambda_n: 0.9429025422590932 Loss: 0.09797405445981466\n",
      "Iteration: 1015 lambda_n: 0.9554985646686772 Loss: 0.09785064693774909\n",
      "Iteration: 1016 lambda_n: 0.9405885348021603 Loss: 0.09772573079861344\n",
      "Iteration: 1017 lambda_n: 0.9354663755666941 Loss: 0.09760290282599911\n",
      "Iteration: 1018 lambda_n: 0.9550909689231785 Loss: 0.0974808797786985\n",
      "Iteration: 1019 lambda_n: 1.0271708988135606 Loss: 0.0973564353647866\n",
      "Iteration: 1020 lambda_n: 0.8946975014582581 Loss: 0.09722275224896046\n",
      "Iteration: 1021 lambda_n: 0.9513325680232912 Loss: 0.097106449388805\n",
      "Iteration: 1022 lambda_n: 1.0262159156264492 Loss: 0.09698291670679557\n",
      "Iteration: 1023 lambda_n: 0.9202053729090116 Loss: 0.09684981210229406\n",
      "Iteration: 1024 lambda_n: 0.9521095682309444 Loss: 0.09673060063309823\n",
      "Iteration: 1025 lambda_n: 0.9864762039911739 Loss: 0.09660739117577448\n",
      "Iteration: 1026 lambda_n: 0.9120548809749193 Loss: 0.09647987921772246\n",
      "Iteration: 1027 lambda_n: 0.998691018113325 Loss: 0.0963621233834698\n",
      "Iteration: 1028 lambda_n: 0.9127266607791714 Loss: 0.09623332314866372\n",
      "Iteration: 1029 lambda_n: 0.9948784948899284 Loss: 0.09611574736092174\n",
      "Iteration: 1030 lambda_n: 1.0264079126117314 Loss: 0.09598772933137639\n",
      "Iteration: 1031 lambda_n: 0.9712741074371273 Loss: 0.09585581066999133\n",
      "Iteration: 1032 lambda_n: 1.003470598393848 Loss: 0.09573112885078613\n",
      "Iteration: 1033 lambda_n: 0.9591095617349695 Loss: 0.095602463040323\n",
      "Iteration: 1034 lambda_n: 0.9067664087643008 Loss: 0.09547963068053175\n",
      "Iteration: 1035 lambda_n: 0.982508902770183 Loss: 0.09536363294914622\n",
      "Iteration: 1036 lambda_n: 0.9915169127526476 Loss: 0.0952380826544207\n",
      "Iteration: 1037 lambda_n: 1.0191472048608063 Loss: 0.09511152912485345\n",
      "Iteration: 1038 lambda_n: 1.0085391606524698 Loss: 0.09498160257970178\n",
      "Iteration: 1039 lambda_n: 0.8810273909674237 Loss: 0.0948531836462726\n",
      "Iteration: 1040 lambda_n: 0.895737250652945 Loss: 0.09474113283022893\n",
      "Iteration: 1041 lambda_n: 0.8810573725791288 Loss: 0.09462733056627497\n",
      "Iteration: 1042 lambda_n: 0.9379008743036946 Loss: 0.09451551207266345\n",
      "Iteration: 1043 lambda_n: 0.977648034864871 Loss: 0.09439660496271511\n",
      "Iteration: 1044 lambda_n: 0.9006270994665718 Loss: 0.09427279753186071\n",
      "Iteration: 1045 lambda_n: 0.9139630148533776 Loss: 0.09415887475368445\n",
      "Iteration: 1046 lambda_n: 0.9889129562166552 Loss: 0.09404338895867476\n",
      "Iteration: 1047 lambda_n: 1.0255478676867222 Loss: 0.0939185699095758\n",
      "Iteration: 1048 lambda_n: 0.9337503391210605 Loss: 0.0937892796901758\n",
      "Iteration: 1049 lambda_n: 0.9421269824625021 Loss: 0.09367170385665376\n",
      "Iteration: 1050 lambda_n: 0.9816108537363248 Loss: 0.09355320500222049\n",
      "Iteration: 1051 lambda_n: 0.9755342981867018 Loss: 0.09342987897388351\n",
      "Iteration: 1052 lambda_n: 1.0274053606765776 Loss: 0.09330745919750598\n",
      "Iteration: 1053 lambda_n: 0.9117066822621418 Loss: 0.09317868074565176\n",
      "Iteration: 1054 lambda_n: 0.9450201754964778 Loss: 0.09306454158994566\n",
      "Iteration: 1055 lambda_n: 1.0197683492307545 Loss: 0.09294636071992865\n",
      "Iteration: 1056 lambda_n: 0.9850373526921702 Loss: 0.09281897704901607\n",
      "Iteration: 1057 lambda_n: 0.9470308578298177 Loss: 0.09269608022001004\n",
      "Iteration: 1058 lambda_n: 0.9898822710597533 Loss: 0.09257806283221214\n",
      "Iteration: 1059 lambda_n: 0.9476466467777689 Loss: 0.09245484524878766\n",
      "Iteration: 1060 lambda_n: 1.0192304085894968 Loss: 0.0923370230463396\n",
      "Iteration: 1061 lambda_n: 0.8863745142038137 Loss: 0.0922104452162288\n",
      "Iteration: 1062 lambda_n: 0.9921582350071078 Loss: 0.09210049761242448\n",
      "Iteration: 1063 lambda_n: 0.9270665052774292 Loss: 0.09197756053139335\n",
      "Iteration: 1064 lambda_n: 1.014013558239251 Loss: 0.09186282320915876\n",
      "Iteration: 1065 lambda_n: 1.0242823760869593 Loss: 0.0917374654461285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1066 lambda_n: 0.9773414228290668 Loss: 0.09161099132291788\n",
      "Iteration: 1067 lambda_n: 0.9892760512797336 Loss: 0.09149045946557184\n",
      "Iteration: 1068 lambda_n: 0.954860578505344 Loss: 0.09136859805043912\n",
      "Iteration: 1069 lambda_n: 1.0001035078912255 Loss: 0.09125111391369967\n",
      "Iteration: 1070 lambda_n: 0.9107797378176283 Loss: 0.09112820419421729\n",
      "Iteration: 1071 lambda_n: 0.917873643483193 Loss: 0.09101640369808232\n",
      "Iteration: 1072 lambda_n: 0.9917709980147633 Loss: 0.09090385487557394\n",
      "Iteration: 1073 lambda_n: 1.0244758151650417 Loss: 0.09078237954994597\n",
      "Iteration: 1074 lambda_n: 0.92922073726462 Loss: 0.09065704759021764\n",
      "Iteration: 1075 lambda_n: 0.9349280224115029 Loss: 0.09054350583578559\n",
      "Iteration: 1076 lambda_n: 1.0236103370130791 Loss: 0.0904293934639726\n",
      "Iteration: 1077 lambda_n: 1.0206386705421844 Loss: 0.09030459841688014\n",
      "Iteration: 1078 lambda_n: 1.0149601366016665 Loss: 0.09018031761508347\n",
      "Iteration: 1079 lambda_n: 1.0174907005680847 Loss: 0.09005687872821097\n",
      "Iteration: 1080 lambda_n: 0.9381901158919408 Loss: 0.0899332821034039\n",
      "Iteration: 1081 lambda_n: 0.9962289401520015 Loss: 0.08981945508393559\n",
      "Iteration: 1082 lambda_n: 0.9881193707036865 Loss: 0.08969872320537284\n",
      "Iteration: 1083 lambda_n: 0.9962458349019945 Loss: 0.08957911647662463\n",
      "Iteration: 1084 lambda_n: 0.9846146339069403 Loss: 0.08945925918307905\n",
      "Iteration: 1085 lambda_n: 0.9389765677431867 Loss: 0.08934115911320117\n",
      "Iteration: 1086 lambda_n: 0.9795410182189396 Loss: 0.08922866140304736\n",
      "Iteration: 1087 lambda_n: 0.8896729551524951 Loss: 0.08911143310666525\n",
      "Iteration: 1088 lambda_n: 0.9340090386843216 Loss: 0.0890050797541452\n",
      "Iteration: 1089 lambda_n: 0.9090322129671613 Loss: 0.08889354322413534\n",
      "Iteration: 1090 lambda_n: 0.9634118338580318 Loss: 0.08878510717465248\n",
      "Iteration: 1091 lambda_n: 0.9653843537750498 Loss: 0.08867030750333713\n",
      "Iteration: 1092 lambda_n: 0.9529247507164428 Loss: 0.08855540228119425\n",
      "Iteration: 1093 lambda_n: 0.9724044241929045 Loss: 0.08844210773896058\n",
      "Iteration: 1094 lambda_n: 0.9285873173191246 Loss: 0.08832662644394917\n",
      "Iteration: 1095 lambda_n: 0.9684691158386265 Loss: 0.08821647326043422\n",
      "Iteration: 1096 lambda_n: 1.0263433040832002 Loss: 0.08810171480184857\n",
      "Iteration: 1097 lambda_n: 0.9270356797138779 Loss: 0.08798023776261381\n",
      "Iteration: 1098 lambda_n: 0.8970450069896951 Loss: 0.08787064438994449\n",
      "Iteration: 1099 lambda_n: 0.9998873167714281 Loss: 0.08776471099956318\n",
      "Iteration: 1100 lambda_n: 0.9325406203727784 Loss: 0.0876467592830681\n",
      "Iteration: 1101 lambda_n: 0.8751726059332315 Loss: 0.0875368795799952\n",
      "Iteration: 1102 lambda_n: 0.9253013338496241 Loss: 0.08743387103572611\n",
      "Iteration: 1103 lambda_n: 0.9392851490223388 Loss: 0.08732507516950949\n",
      "Iteration: 1104 lambda_n: 0.9655318292481633 Loss: 0.08721475531208178\n",
      "Iteration: 1105 lambda_n: 0.936219492240329 Loss: 0.08710147838034926\n",
      "Iteration: 1106 lambda_n: 0.8892016709066994 Loss: 0.08699176428746905\n",
      "Iteration: 1107 lambda_n: 1.0232532327611816 Loss: 0.08688767376535662\n",
      "Iteration: 1108 lambda_n: 0.9500645758116865 Loss: 0.08676801946148453\n",
      "Iteration: 1109 lambda_n: 1.0203792532050848 Loss: 0.08665705550873472\n",
      "Iteration: 1110 lambda_n: 0.9902719598993328 Loss: 0.0865380139241843\n",
      "Iteration: 1111 lambda_n: 0.9503163203206452 Loss: 0.08642262279080874\n",
      "Iteration: 1112 lambda_n: 0.9102161587102519 Loss: 0.08631201567076348\n",
      "Iteration: 1113 lambda_n: 0.8754712784055807 Loss: 0.08620619350352303\n",
      "Iteration: 1114 lambda_n: 0.9130333543134385 Loss: 0.08610451922199502\n",
      "Iteration: 1115 lambda_n: 0.976679321302311 Loss: 0.0859985928279913\n",
      "Iteration: 1116 lambda_n: 0.9093328697652763 Loss: 0.08588540594185391\n",
      "Iteration: 1117 lambda_n: 0.8825210770497816 Loss: 0.08578014376120949\n",
      "Iteration: 1118 lambda_n: 0.9410198547614543 Loss: 0.08567809431075797\n",
      "Iteration: 1119 lambda_n: 0.9222375632104101 Loss: 0.0855693950857143\n",
      "Iteration: 1120 lambda_n: 0.9344203986473684 Loss: 0.0854629834269544\n",
      "Iteration: 1121 lambda_n: 0.941736376830457 Loss: 0.08535528381605752\n",
      "Iteration: 1122 lambda_n: 1.0169490817638682 Loss: 0.08524686103131576\n",
      "Iteration: 1123 lambda_n: 0.9952902218789328 Loss: 0.08512991116916514\n",
      "Iteration: 1124 lambda_n: 0.9538504612865002 Loss: 0.085015589291817\n",
      "Iteration: 1125 lambda_n: 0.9778085686045614 Loss: 0.08490615543330378\n",
      "Iteration: 1126 lambda_n: 0.9487073646964557 Loss: 0.08479410017362192\n",
      "Iteration: 1127 lambda_n: 0.9067259684617651 Loss: 0.08468550515015112\n",
      "Iteration: 1128 lambda_n: 0.9818050291148929 Loss: 0.08458183137718817\n",
      "Iteration: 1129 lambda_n: 0.9839931970867148 Loss: 0.08446969564074987\n",
      "Iteration: 1130 lambda_n: 0.9058211697330348 Loss: 0.08435744094108265\n",
      "Iteration: 1131 lambda_n: 0.9015078894503555 Loss: 0.08425422314855989\n",
      "Iteration: 1132 lambda_n: 0.9466984359881647 Loss: 0.08415160725018299\n",
      "Iteration: 1133 lambda_n: 1.0051529305447247 Loss: 0.08404396388859352\n",
      "Iteration: 1134 lambda_n: 1.0039065289819826 Loss: 0.0839298040364069\n",
      "Iteration: 1135 lambda_n: 0.9616980419564477 Loss: 0.08381592199313975\n",
      "Iteration: 1136 lambda_n: 0.9443539629694014 Loss: 0.08370695734578562\n",
      "Iteration: 1137 lambda_n: 0.930009442365476 Loss: 0.08360007994165244\n",
      "Iteration: 1138 lambda_n: 0.917079507911211 Loss: 0.08349494402125815\n",
      "Iteration: 1139 lambda_n: 0.9539556619314872 Loss: 0.08339138438929083\n",
      "Iteration: 1140 lambda_n: 0.9465439082478441 Loss: 0.08328377919172714\n",
      "Iteration: 1141 lambda_n: 0.968464722956782 Loss: 0.08317713132319812\n",
      "Iteration: 1142 lambda_n: 1.0180174263903827 Loss: 0.08306813736694284\n",
      "Iteration: 1143 lambda_n: 0.9250507623232156 Loss: 0.08295370028472616\n",
      "Iteration: 1144 lambda_n: 0.902152530916577 Loss: 0.08284983806936941\n",
      "Iteration: 1145 lambda_n: 0.8833379492826894 Loss: 0.08274865826031558\n",
      "Iteration: 1146 lambda_n: 0.8759241103075872 Loss: 0.08264969501429367\n",
      "Iteration: 1147 lambda_n: 0.9158094206110469 Loss: 0.08255166588030981\n",
      "Iteration: 1148 lambda_n: 1.0176686739264285 Loss: 0.08244928129977157\n",
      "Iteration: 1149 lambda_n: 0.9926732279066558 Loss: 0.08233563647035438\n",
      "Iteration: 1150 lambda_n: 0.8793615271931388 Loss: 0.08222491744192546\n",
      "Iteration: 1151 lambda_n: 0.9766963452855105 Loss: 0.08212695099361438\n",
      "Iteration: 1152 lambda_n: 0.9244858996750687 Loss: 0.08201825790716476\n",
      "Iteration: 1153 lambda_n: 0.9166290806689628 Loss: 0.0819154944878508\n",
      "Iteration: 1154 lambda_n: 0.9202119385509405 Loss: 0.08181371729151088\n",
      "Iteration: 1155 lambda_n: 0.9311799691976897 Loss: 0.08171165482449912\n",
      "Iteration: 1156 lambda_n: 0.8948077747981762 Loss: 0.08160849031083496\n",
      "Iteration: 1157 lambda_n: 0.9931299189449893 Loss: 0.08150946559274806\n",
      "Iteration: 1158 lambda_n: 0.9153420392198331 Loss: 0.0813996806754336\n",
      "Iteration: 1159 lambda_n: 0.8964114602682345 Loss: 0.08129861391763678\n",
      "Iteration: 1160 lambda_n: 0.9346554211573966 Loss: 0.08119974601535292\n",
      "Iteration: 1161 lambda_n: 0.9789792621816225 Loss: 0.08109677227685039\n",
      "Iteration: 1162 lambda_n: 0.9885994717347585 Loss: 0.08098903783317418\n",
      "Iteration: 1163 lambda_n: 1.0102685163323641 Loss: 0.08088037337975487\n",
      "Iteration: 1164 lambda_n: 1.0042090632744847 Loss: 0.08076946012457464\n",
      "Iteration: 1165 lambda_n: 0.9807230667247658 Loss: 0.08065934642442407\n",
      "Iteration: 1166 lambda_n: 0.9739264787335271 Loss: 0.08055193788378755\n",
      "Iteration: 1167 lambda_n: 0.8966248656029041 Loss: 0.08044539997101276\n",
      "Iteration: 1168 lambda_n: 0.9048758448081974 Loss: 0.0803474318594701\n",
      "Iteration: 1169 lambda_n: 0.9252780025702464 Loss: 0.08024866966933579\n",
      "Iteration: 1170 lambda_n: 0.9565059211133224 Loss: 0.08014779180202616\n",
      "Iteration: 1171 lambda_n: 0.8763978763938762 Loss: 0.0800436269624104\n",
      "Iteration: 1172 lambda_n: 0.9606688183249789 Loss: 0.07994829485883982\n",
      "Iteration: 1173 lambda_n: 0.9101666600426654 Loss: 0.07984390910648714\n",
      "Iteration: 1174 lambda_n: 0.9604723642737654 Loss: 0.07974512499418639\n",
      "Iteration: 1175 lambda_n: 1.015346420926084 Loss: 0.07964099740700206\n",
      "Iteration: 1176 lambda_n: 0.9745974691390978 Loss: 0.07953105060616214\n",
      "Iteration: 1177 lambda_n: 1.012386956614816 Loss: 0.07942564552782556\n",
      "Iteration: 1178 lambda_n: 0.9822069475298798 Loss: 0.07931628417738334\n",
      "Iteration: 1179 lambda_n: 0.9358333389438314 Loss: 0.07921031292367477\n",
      "Iteration: 1180 lambda_n: 0.9763228043217378 Loss: 0.07910946460336933\n",
      "Iteration: 1181 lambda_n: 0.9720301949130317 Loss: 0.07900437398299108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1182 lambda_n: 0.8877875052923054 Loss: 0.07889986983707656\n",
      "Iteration: 1183 lambda_n: 0.9780500027236294 Loss: 0.07880453396964969\n",
      "Iteration: 1184 lambda_n: 1.018158534438465 Loss: 0.07869962125724662\n",
      "Iteration: 1185 lambda_n: 0.9964696903616433 Loss: 0.07859053771690112\n",
      "Iteration: 1186 lambda_n: 0.9432545808329843 Loss: 0.07848391015078075\n",
      "Iteration: 1187 lambda_n: 0.9027200905368811 Loss: 0.07838309857062964\n",
      "Iteration: 1188 lambda_n: 1.002506359367777 Loss: 0.07828672956561013\n",
      "Iteration: 1189 lambda_n: 0.9555647746050231 Loss: 0.0781798288420495\n",
      "Iteration: 1190 lambda_n: 1.0120111197258659 Loss: 0.07807805763395256\n",
      "Iteration: 1191 lambda_n: 0.9689934071917192 Loss: 0.0779704024124595\n",
      "Iteration: 1192 lambda_n: 0.8860892621723054 Loss: 0.07786745017651536\n",
      "Iteration: 1193 lambda_n: 0.9922714064986594 Loss: 0.07777341627229095\n",
      "Iteration: 1194 lambda_n: 1.0078525048933935 Loss: 0.07766823140274362\n",
      "Iteration: 1195 lambda_n: 1.0098392324343823 Loss: 0.07756152555657995\n",
      "Iteration: 1196 lambda_n: 0.9830884548446042 Loss: 0.07745474192546031\n",
      "Iteration: 1197 lambda_n: 1.0178279934043741 Loss: 0.07735091549433239\n",
      "Iteration: 1198 lambda_n: 0.9228063946556285 Loss: 0.07724355118406492\n",
      "Iteration: 1199 lambda_n: 0.8784597115553309 Loss: 0.07714632988126352\n",
      "Iteration: 1200 lambda_n: 0.9492063733133054 Loss: 0.07705388494840382\n",
      "Iteration: 1201 lambda_n: 0.9129551574098247 Loss: 0.07695410499187423\n",
      "Iteration: 1202 lambda_n: 1.0187684752242463 Loss: 0.0768582473169538\n",
      "Iteration: 1203 lambda_n: 0.9810771316417282 Loss: 0.07675140301930365\n",
      "Iteration: 1204 lambda_n: 0.9257634425746509 Loss: 0.07664864022632362\n",
      "Iteration: 1205 lambda_n: 0.9419859084754396 Loss: 0.07655178757298252\n",
      "Iteration: 1206 lambda_n: 0.9146342077048284 Loss: 0.07645335106076052\n",
      "Iteration: 1207 lambda_n: 0.901220085256794 Loss: 0.0763578836123985\n",
      "Iteration: 1208 lambda_n: 0.9151902413857611 Loss: 0.07626392260119501\n",
      "Iteration: 1209 lambda_n: 0.942565226097531 Loss: 0.07616861203839854\n",
      "Iteration: 1210 lambda_n: 0.9532691857950387 Loss: 0.07607056272488258\n",
      "Iteration: 1211 lambda_n: 0.9415863284587339 Loss: 0.07597151627035378\n",
      "Iteration: 1212 lambda_n: 1.0222161263079115 Loss: 0.07587379926849802\n",
      "Iteration: 1213 lambda_n: 0.9109024666760526 Loss: 0.07576784097390883\n",
      "Iteration: 1214 lambda_n: 0.88224778853522 Loss: 0.07567353832359339\n",
      "Iteration: 1215 lambda_n: 0.9609049014706954 Loss: 0.07558230510108274\n",
      "Iteration: 1216 lambda_n: 0.9272215583286925 Loss: 0.07548304919256514\n",
      "Iteration: 1217 lambda_n: 0.9517094343775759 Loss: 0.07538738647274607\n",
      "Iteration: 1218 lambda_n: 0.9926935915449221 Loss: 0.07528931150876676\n",
      "Iteration: 1219 lambda_n: 0.9523426098206157 Loss: 0.07518713570608677\n",
      "Iteration: 1220 lambda_n: 0.9384900578900548 Loss: 0.0750892336672429\n",
      "Iteration: 1221 lambda_n: 0.8779567414329419 Loss: 0.07499287019915883\n",
      "Iteration: 1222 lambda_n: 0.9661218851039344 Loss: 0.07490282664692892\n",
      "Iteration: 1223 lambda_n: 0.8769474734745127 Loss: 0.07480385204984967\n",
      "Iteration: 1224 lambda_n: 0.9799390474038631 Loss: 0.07471411955448547\n",
      "Iteration: 1225 lambda_n: 0.9272043592149121 Loss: 0.07461396154405919\n",
      "Iteration: 1226 lambda_n: 0.9668062199060994 Loss: 0.07451930862613612\n",
      "Iteration: 1227 lambda_n: 0.8790467571145347 Loss: 0.07442072889705664\n",
      "Iteration: 1228 lambda_n: 0.9377531682874793 Loss: 0.07433120425456108\n",
      "Iteration: 1229 lambda_n: 0.977857424048358 Loss: 0.07423580781255948\n",
      "Iteration: 1230 lambda_n: 0.9143014630853945 Loss: 0.07413645002837041\n",
      "Iteration: 1231 lambda_n: 0.9431051350940195 Loss: 0.07404366278129512\n",
      "Iteration: 1232 lambda_n: 0.8980509570027255 Loss: 0.07394806339102766\n",
      "Iteration: 1233 lambda_n: 0.9304284701573297 Loss: 0.07385713810793078\n",
      "Iteration: 1234 lambda_n: 1.0046323105898236 Loss: 0.07376304223319874\n",
      "Iteration: 1235 lambda_n: 0.8906930107846632 Loss: 0.0736615633757805\n",
      "Iteration: 1236 lambda_n: 0.9284074590177792 Loss: 0.07357170503016641\n",
      "Iteration: 1237 lambda_n: 0.9465185990274093 Loss: 0.07347814826049856\n",
      "Iteration: 1238 lambda_n: 0.8841362724686634 Loss: 0.0733828789117439\n",
      "Iteration: 1239 lambda_n: 0.9481658310100338 Loss: 0.07329399360919307\n",
      "Iteration: 1240 lambda_n: 0.9548685768520639 Loss: 0.07319877964394274\n",
      "Iteration: 1241 lambda_n: 0.9641532438705732 Loss: 0.07310300804180542\n",
      "Iteration: 1242 lambda_n: 0.9353487785233697 Loss: 0.0730064226123724\n",
      "Iteration: 1243 lambda_n: 1.0250140923767068 Loss: 0.07291283667589804\n",
      "Iteration: 1244 lambda_n: 0.9154784864804201 Loss: 0.07281040376400563\n",
      "Iteration: 1245 lambda_n: 0.9458823348484632 Loss: 0.07271903355942053\n",
      "Iteration: 1246 lambda_n: 0.9865595375321252 Loss: 0.07262473966207927\n",
      "Iteration: 1247 lambda_n: 0.8899722007643175 Loss: 0.07252651030371694\n",
      "Iteration: 1248 lambda_n: 0.9398738663759539 Loss: 0.07243800694257062\n",
      "Iteration: 1249 lambda_n: 0.9811905085895218 Loss: 0.0723446485205192\n",
      "Iteration: 1250 lambda_n: 0.8750150679744354 Loss: 0.07224730415093669\n",
      "Iteration: 1251 lambda_n: 1.008748655113563 Loss: 0.07216059970785128\n",
      "Iteration: 1252 lambda_n: 0.9310635813431134 Loss: 0.07206075932983426\n",
      "Iteration: 1253 lambda_n: 1.0008878104154375 Loss: 0.07196872469644756\n",
      "Iteration: 1254 lambda_n: 0.9309061145171971 Loss: 0.07186990793218202\n",
      "Iteration: 1255 lambda_n: 0.9218784991453001 Loss: 0.07177811644723725\n",
      "Iteration: 1256 lambda_n: 1.0012357626782504 Loss: 0.07168732341562371\n",
      "Iteration: 1257 lambda_n: 0.9447722462538478 Loss: 0.07158883364619219\n",
      "Iteration: 1258 lambda_n: 0.9971909441318526 Loss: 0.07149601610859697\n",
      "Iteration: 1259 lambda_n: 1.007690237985547 Loss: 0.07139816925229321\n",
      "Iteration: 1260 lambda_n: 0.8941572711466229 Loss: 0.07129941933855669\n",
      "Iteration: 1261 lambda_n: 0.9974405890691668 Loss: 0.07121190605212162\n",
      "Iteration: 1262 lambda_n: 1.0107655503527555 Loss: 0.07111439959957687\n",
      "Iteration: 1263 lambda_n: 0.985412952912333 Loss: 0.07101571803945352\n",
      "Iteration: 1264 lambda_n: 0.9072367497667178 Loss: 0.07091963647846405\n",
      "Iteration: 1265 lambda_n: 1.0028248814638783 Loss: 0.07083128798834806\n",
      "Iteration: 1266 lambda_n: 0.9378833589911135 Loss: 0.0707337482296883\n",
      "Iteration: 1267 lambda_n: 0.9164794287477384 Loss: 0.07064264164270068\n",
      "Iteration: 1268 lambda_n: 0.955822024725833 Loss: 0.07055372181890486\n",
      "Iteration: 1269 lambda_n: 0.9044744234482135 Loss: 0.07046109608308473\n",
      "Iteration: 1270 lambda_n: 0.9164641423991465 Loss: 0.07037355362057292\n",
      "Iteration: 1271 lambda_n: 1.0009737214900098 Loss: 0.07028495516123999\n",
      "Iteration: 1272 lambda_n: 0.9380767426807627 Loss: 0.07018830445908274\n",
      "Iteration: 1273 lambda_n: 0.9373737177539778 Loss: 0.07009784307309257\n",
      "Iteration: 1274 lambda_n: 1.0143546065635456 Loss: 0.07000755981480972\n",
      "Iteration: 1275 lambda_n: 1.0002239040487952 Loss: 0.06990998366866717\n",
      "Iteration: 1276 lambda_n: 0.9580251342074666 Loss: 0.06981389364995363\n",
      "Iteration: 1277 lambda_n: 0.9011793894808464 Loss: 0.06972197654316827\n",
      "Iteration: 1278 lambda_n: 0.9363579435302373 Loss: 0.06963562014664867\n",
      "Iteration: 1279 lambda_n: 0.9587295584985492 Loss: 0.06954599936431291\n",
      "Iteration: 1280 lambda_n: 1.0005911510126413 Loss: 0.06945435035653792\n",
      "Iteration: 1281 lambda_n: 0.9442416742926109 Loss: 0.06935882089150255\n",
      "Iteration: 1282 lambda_n: 0.9116272518076509 Loss: 0.06926878791904087\n",
      "Iteration: 1283 lambda_n: 1.0069012808426436 Loss: 0.0691819714800133\n",
      "Iteration: 1284 lambda_n: 0.899848026994316 Loss: 0.06908619940227934\n",
      "Iteration: 1285 lambda_n: 0.8868866910159711 Loss: 0.06900072015372129\n",
      "Iteration: 1286 lambda_n: 0.9235549470210808 Loss: 0.06891657147245704\n",
      "Iteration: 1287 lambda_n: 1.0250207361779498 Loss: 0.06882904692075706\n",
      "Iteration: 1288 lambda_n: 0.9485394699351297 Loss: 0.0687320277379475\n",
      "Iteration: 1289 lambda_n: 0.8755483697014563 Loss: 0.06864236674394958\n",
      "Iteration: 1290 lambda_n: 0.9375392501382841 Loss: 0.06855970693722156\n",
      "Iteration: 1291 lambda_n: 1.0038760426817546 Loss: 0.06847129865541163\n",
      "Iteration: 1292 lambda_n: 0.8951171330997699 Loss: 0.06837675417812014\n",
      "Iteration: 1293 lambda_n: 0.9582880232277917 Loss: 0.0682925616080998\n",
      "Iteration: 1294 lambda_n: 0.9386292292066161 Loss: 0.06820253592952656\n",
      "Iteration: 1295 lambda_n: 0.9578279576663813 Loss: 0.06811446849235009\n",
      "Iteration: 1296 lambda_n: 1.0209730897390448 Loss: 0.06802471212880004\n",
      "Iteration: 1297 lambda_n: 1.007568666721701 Loss: 0.0679291620923657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1298 lambda_n: 1.0095377814542188 Loss: 0.06783499403879974\n",
      "Iteration: 1299 lambda_n: 0.91565511496575 Loss: 0.0677407684276148\n",
      "Iteration: 1300 lambda_n: 0.923954854143623 Loss: 0.06765541766741633\n",
      "Iteration: 1301 lambda_n: 0.9832587825855409 Loss: 0.06756939855208233\n",
      "Iteration: 1302 lambda_n: 1.0054599383725644 Loss: 0.06747797282947571\n",
      "Iteration: 1303 lambda_n: 0.9266463077344961 Loss: 0.0673846061432413\n",
      "Iteration: 1304 lambda_n: 0.9593242604999155 Loss: 0.06729867145232513\n",
      "Iteration: 1305 lambda_n: 1.0133180300110736 Loss: 0.06720981746355152\n",
      "Iteration: 1306 lambda_n: 0.8841513830410246 Loss: 0.0671160846356063\n",
      "Iteration: 1307 lambda_n: 0.9975035233878227 Loss: 0.06703440741696592\n",
      "Iteration: 1308 lambda_n: 0.9152429209019419 Loss: 0.06694237145729054\n",
      "Iteration: 1309 lambda_n: 0.951242943217852 Loss: 0.06685803615156961\n",
      "Iteration: 1310 lambda_n: 0.8811234475825539 Loss: 0.06677049240513805\n",
      "Iteration: 1311 lambda_n: 0.9874685719042012 Loss: 0.06668950365579979\n",
      "Iteration: 1312 lambda_n: 1.0080739490630446 Loss: 0.06659885095272247\n",
      "Iteration: 1313 lambda_n: 0.9085345982643975 Loss: 0.06650643032630395\n",
      "Iteration: 1314 lambda_n: 0.9851034439128636 Loss: 0.06642324596682417\n",
      "Iteration: 1315 lambda_n: 0.9023239778119078 Loss: 0.06633316392524781\n",
      "Iteration: 1316 lambda_n: 1.0185329525249303 Loss: 0.066250759106\n",
      "Iteration: 1317 lambda_n: 1.0009167399318106 Loss: 0.06615785856875635\n",
      "Iteration: 1318 lambda_n: 0.9176988159383311 Loss: 0.06606669010583731\n",
      "Iteration: 1319 lambda_n: 1.0041381174999238 Loss: 0.06598321254458699\n",
      "Iteration: 1320 lambda_n: 0.9816313672795117 Loss: 0.06589198842562517\n",
      "Iteration: 1321 lambda_n: 0.914561200706392 Loss: 0.06580292986261614\n",
      "Iteration: 1322 lambda_n: 0.9795826387480718 Loss: 0.06572006501468715\n",
      "Iteration: 1323 lambda_n: 0.9002182934226467 Loss: 0.06563142115973843\n",
      "Iteration: 1324 lambda_n: 0.9599020695667188 Loss: 0.06555006551785286\n",
      "Iteration: 1325 lambda_n: 0.9087466894146873 Loss: 0.06546342421419686\n",
      "Iteration: 1326 lambda_n: 1.0217394458136775 Loss: 0.06538150615927793\n",
      "Iteration: 1327 lambda_n: 0.9639213202380423 Loss: 0.06528952029165394\n",
      "Iteration: 1328 lambda_n: 0.8815241983257236 Loss: 0.06520285914035635\n",
      "Iteration: 1329 lambda_n: 0.9592389079746457 Loss: 0.065123708119848\n",
      "Iteration: 1330 lambda_n: 0.9040078747855533 Loss: 0.06503768542950389\n",
      "Iteration: 1331 lambda_n: 0.9542112626849454 Loss: 0.06495672075967503\n",
      "Iteration: 1332 lambda_n: 0.8916878067109044 Loss: 0.06487136722525044\n",
      "Iteration: 1333 lambda_n: 0.9535283185150008 Loss: 0.0647917091230224\n",
      "Iteration: 1334 lambda_n: 0.9145007547982349 Loss: 0.0647066327636609\n",
      "Iteration: 1335 lambda_n: 0.8864691602270275 Loss: 0.06462514445016965\n",
      "Iteration: 1336 lambda_n: 0.8778134496617032 Loss: 0.06454625262238854\n",
      "Iteration: 1337 lambda_n: 0.8779094123504434 Loss: 0.064468226308684\n",
      "Iteration: 1338 lambda_n: 0.9638007020294238 Loss: 0.06439028594867895\n",
      "Iteration: 1339 lambda_n: 0.9719288531326027 Loss: 0.0643048264992813\n",
      "Iteration: 1340 lambda_n: 0.9311081034967205 Loss: 0.06421876136433696\n",
      "Iteration: 1341 lambda_n: 0.9592221362159123 Loss: 0.06413642060352108\n",
      "Iteration: 1342 lambda_n: 0.9102457627920081 Loss: 0.06405170384609876\n",
      "Iteration: 1343 lambda_n: 0.8857284077441154 Loss: 0.06397141807532405\n",
      "Iteration: 1344 lambda_n: 0.921560998680433 Loss: 0.06389339276845077\n",
      "Iteration: 1345 lambda_n: 1.0227917995986306 Loss: 0.06381231180478114\n",
      "Iteration: 1346 lambda_n: 1.002520029328141 Loss: 0.0637224428779174\n",
      "Iteration: 1347 lambda_n: 0.8868131038249385 Loss: 0.06363447988748869\n",
      "Iteration: 1348 lambda_n: 0.8939073621275042 Loss: 0.06355677465949607\n",
      "Iteration: 1349 lambda_n: 0.9553118464593486 Loss: 0.06347854486725944\n",
      "Iteration: 1350 lambda_n: 0.896314672286375 Loss: 0.06339504746403807\n",
      "Iteration: 1351 lambda_n: 0.9761292568134458 Loss: 0.0633168094899008\n",
      "Iteration: 1352 lambda_n: 1.0069656437272394 Loss: 0.06323171389924165\n",
      "Iteration: 1353 lambda_n: 1.0024492481202756 Loss: 0.06314405104600208\n",
      "Iteration: 1354 lambda_n: 0.9406735078129608 Loss: 0.0630569043868722\n",
      "Iteration: 1355 lambda_n: 0.9899508758010666 Loss: 0.06297524128035079\n",
      "Iteration: 1356 lambda_n: 0.994509851296763 Loss: 0.06288941533148255\n",
      "Iteration: 1357 lambda_n: 0.8882762034287919 Loss: 0.06280331427980888\n",
      "Iteration: 1358 lambda_n: 0.943966235605313 Loss: 0.0627265151721192\n",
      "Iteration: 1359 lambda_n: 1.009077329513087 Loss: 0.06264500500249548\n",
      "Iteration: 1360 lambda_n: 0.8931728760432236 Loss: 0.06255799068574777\n",
      "Iteration: 1361 lambda_n: 0.9011685795419938 Loss: 0.06248107738839869\n",
      "Iteration: 1362 lambda_n: 0.9266565775790916 Loss: 0.06240357374676114\n",
      "Iteration: 1363 lambda_n: 0.9378260635575048 Loss: 0.062323980407142654\n",
      "Iteration: 1364 lambda_n: 0.9111207428699223 Loss: 0.062243533727079185\n",
      "Iteration: 1365 lambda_n: 0.9084781682469519 Loss: 0.06216548094365282\n",
      "Iteration: 1366 lambda_n: 0.9243350016838909 Loss: 0.06208775508234869\n",
      "Iteration: 1367 lambda_n: 0.9512869699239299 Loss: 0.06200877509588011\n",
      "Iteration: 1368 lambda_n: 0.9149264145013623 Loss: 0.06192759985363238\n",
      "Iteration: 1369 lambda_n: 0.9910543850997461 Loss: 0.06184963196554689\n",
      "Iteration: 1370 lambda_n: 0.967978177743932 Loss: 0.061765289179921565\n",
      "Iteration: 1371 lambda_n: 0.8904465494702432 Loss: 0.06168302600350244\n",
      "Iteration: 1372 lambda_n: 0.9498791445317212 Loss: 0.061607454169005496\n",
      "Iteration: 1373 lambda_n: 1.011879178080939 Loss: 0.06152694276501473\n",
      "Iteration: 1374 lambda_n: 0.9792190530438445 Loss: 0.061441294885886205\n",
      "Iteration: 1375 lambda_n: 0.9424712032771967 Loss: 0.061358530540602536\n",
      "Iteration: 1376 lambda_n: 0.87461415830179 Loss: 0.06127898288869486\n",
      "Iteration: 1377 lambda_n: 0.9197821045412959 Loss: 0.06120526053714402\n",
      "Iteration: 1378 lambda_n: 0.9310649576235818 Loss: 0.06112782973581069\n",
      "Iteration: 1379 lambda_n: 0.8767102424683518 Loss: 0.061049553163500914\n",
      "Iteration: 1380 lambda_n: 0.906151627867286 Loss: 0.06097594357395307\n",
      "Iteration: 1381 lambda_n: 0.9500555011415932 Loss: 0.06089995909081317\n",
      "Iteration: 1382 lambda_n: 0.9314762375981409 Loss: 0.060820398645227355\n",
      "Iteration: 1383 lambda_n: 0.9499813986621786 Loss: 0.060742500578169364\n",
      "Iteration: 1384 lambda_n: 0.953601608901549 Loss: 0.060663162558658015\n",
      "Iteration: 1385 lambda_n: 0.9789550537548769 Loss: 0.06058363185216098\n",
      "Iteration: 1386 lambda_n: 0.9111812448293054 Loss: 0.06050256010543474\n",
      "Iteration: 1387 lambda_n: 1.021567721318899 Loss: 0.06042734282406247\n",
      "Iteration: 1388 lambda_n: 0.9921758394574812 Loss: 0.060343126635755175\n",
      "Iteration: 1389 lambda_n: 0.9815165024783222 Loss: 0.06026145189674952\n",
      "Iteration: 1390 lambda_n: 0.9562097323595367 Loss: 0.060180768975103\n",
      "Iteration: 1391 lambda_n: 0.8966033535157968 Loss: 0.060102276046591165\n",
      "Iteration: 1392 lambda_n: 0.9594249811527756 Loss: 0.06002877522263805\n",
      "Iteration: 1393 lambda_n: 1.0025733065759599 Loss: 0.05995022774722979\n",
      "Iteration: 1394 lambda_n: 0.8963204859753776 Loss: 0.05986826241289776\n",
      "Iteration: 1395 lambda_n: 0.9359912176539328 Loss: 0.05979508634491604\n",
      "Iteration: 1396 lambda_n: 0.9332118535610636 Loss: 0.0597187714932543\n",
      "Iteration: 1397 lambda_n: 1.0213711934522238 Loss: 0.05964278592846996\n",
      "Iteration: 1398 lambda_n: 0.8745680424164813 Loss: 0.05955973724561991\n",
      "Iteration: 1399 lambda_n: 1.0161899500150975 Loss: 0.0594887260159812\n",
      "Iteration: 1400 lambda_n: 0.909299375259984 Loss: 0.0594063250310775\n",
      "Iteration: 1401 lambda_n: 0.9434314513849518 Loss: 0.059332696979020466\n",
      "Iteration: 1402 lambda_n: 0.9334799472183046 Loss: 0.05925640710448966\n",
      "Iteration: 1403 lambda_n: 0.8980865950788894 Loss: 0.05918102514955867\n",
      "Iteration: 1404 lambda_n: 0.9420908099909236 Loss: 0.05910859878534723\n",
      "Iteration: 1405 lambda_n: 0.9287494284362914 Loss: 0.05903272450881756\n",
      "Iteration: 1406 lambda_n: 0.9739240777383001 Loss: 0.058958027098176054\n",
      "Iteration: 1407 lambda_n: 0.9821805180850613 Loss: 0.0588798041274485\n",
      "Iteration: 1408 lambda_n: 0.9971815173376982 Loss: 0.0588010306480928\n",
      "Iteration: 1409 lambda_n: 0.9864979815784701 Loss: 0.05872116957436669\n",
      "Iteration: 1410 lambda_n: 0.9985560042567231 Loss: 0.058642279257367744\n",
      "Iteration: 1411 lambda_n: 0.8923403456743484 Loss: 0.05856254071533978\n",
      "Iteration: 1412 lambda_n: 0.9852128236672621 Loss: 0.05849138525972256\n",
      "Iteration: 1413 lambda_n: 0.907195742697464 Loss: 0.058412930424786805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1414 lambda_n: 0.9406968834383697 Loss: 0.05834079073400362\n",
      "Iteration: 1415 lambda_n: 0.8926931864940617 Loss: 0.05826608825083893\n",
      "Iteration: 1416 lambda_n: 0.9406998053844916 Loss: 0.05819529486900955\n",
      "Iteration: 1417 lambda_n: 1.0145856953379941 Loss: 0.058120794451352716\n",
      "Iteration: 1418 lambda_n: 0.9342765613371303 Loss: 0.058040557010342594\n",
      "Iteration: 1419 lambda_n: 0.910391249813571 Loss: 0.05796677930338535\n",
      "Iteration: 1420 lambda_n: 0.9734771064655215 Loss: 0.05789498673250373\n",
      "Iteration: 1421 lambda_n: 0.9210712491407077 Loss: 0.05781832527273893\n",
      "Iteration: 1422 lambda_n: 0.8828281423065515 Loss: 0.057745894207102084\n",
      "Iteration: 1423 lambda_n: 0.937387909824946 Loss: 0.057676564618564366\n",
      "Iteration: 1424 lambda_n: 1.0075706035583432 Loss: 0.057603049112677954\n",
      "Iteration: 1425 lambda_n: 0.9968958153813232 Loss: 0.0575241425632434\n",
      "Iteration: 1426 lambda_n: 1.0066007459534365 Loss: 0.0574461892892361\n",
      "Iteration: 1427 lambda_n: 0.9571781150395959 Loss: 0.05736759499812698\n",
      "Iteration: 1428 lambda_n: 0.9509900531854657 Loss: 0.057292970759554654\n",
      "Iteration: 1429 lambda_n: 0.9067793509395995 Loss: 0.05721893533282384\n",
      "Iteration: 1430 lambda_n: 0.9128368129612515 Loss: 0.05714844130384827\n",
      "Iteration: 1431 lambda_n: 0.9024738088326136 Loss: 0.057077573438554276\n",
      "Iteration: 1432 lambda_n: 0.9544469768663177 Loss: 0.05700760618804992\n",
      "Iteration: 1433 lambda_n: 0.9100433080106778 Loss: 0.05693371207278504\n",
      "Iteration: 1434 lambda_n: 0.8925336470501917 Loss: 0.05686335594209783\n",
      "Iteration: 1435 lambda_n: 0.9241466790657387 Loss: 0.05679444796803742\n",
      "Iteration: 1436 lambda_n: 0.9195935847178281 Loss: 0.05672319681466964\n",
      "Iteration: 1437 lambda_n: 1.0104694442456372 Loss: 0.0566523959467489\n",
      "Iteration: 1438 lambda_n: 0.9387263140746571 Loss: 0.056574710299939705\n",
      "Iteration: 1439 lambda_n: 0.9370830669056398 Loss: 0.056502648862385846\n",
      "Iteration: 1440 lambda_n: 0.8928292347397289 Loss: 0.056430816352556416\n",
      "Iteration: 1441 lambda_n: 0.8943012716519972 Loss: 0.056362472542658165\n",
      "Iteration: 1442 lambda_n: 0.8950559059936453 Loss: 0.056294109414523084\n",
      "Iteration: 1443 lambda_n: 0.9256342454038263 Loss: 0.056225782160451\n",
      "Iteration: 1444 lambda_n: 0.8829746085040873 Loss: 0.05615521840357789\n",
      "Iteration: 1445 lambda_n: 1.0271348245727436 Loss: 0.056088000864255624\n",
      "Iteration: 1446 lambda_n: 1.0085639884190651 Loss: 0.05600992013666002\n",
      "Iteration: 1447 lambda_n: 0.8768443727296521 Loss: 0.05593337146505184\n",
      "Iteration: 1448 lambda_n: 0.8906562443474452 Loss: 0.05586691931516802\n",
      "Iteration: 1449 lambda_n: 0.9454856752428472 Loss: 0.055799512047952866\n",
      "Iteration: 1450 lambda_n: 0.9361047328074462 Loss: 0.05572805530014739\n",
      "Iteration: 1451 lambda_n: 0.9420244334165521 Loss: 0.05565741054112068\n",
      "Iteration: 1452 lambda_n: 1.003389564648273 Loss: 0.055586422168701174\n",
      "Iteration: 1453 lambda_n: 0.9208979140855478 Loss: 0.05551092200093711\n",
      "Iteration: 1454 lambda_n: 0.9368939831510334 Loss: 0.05544173403884656\n",
      "Iteration: 1455 lambda_n: 0.921797035832675 Loss: 0.05537144547616938\n",
      "Iteration: 1456 lambda_n: 1.0125542238468717 Loss: 0.05530238978211967\n",
      "Iteration: 1457 lambda_n: 0.9073559855763352 Loss: 0.055226647254053705\n",
      "Iteration: 1458 lambda_n: 0.8971301703426106 Loss: 0.055158877643986214\n",
      "Iteration: 1459 lambda_n: 1.0250794613874343 Loss: 0.05509196638405392\n",
      "Iteration: 1460 lambda_n: 0.9480843714468626 Loss: 0.05501562414365587\n",
      "Iteration: 1461 lambda_n: 0.8828765495243991 Loss: 0.05494512675085727\n",
      "Iteration: 1462 lambda_n: 0.882661218131045 Loss: 0.05487957355914\n",
      "Iteration: 1463 lambda_n: 0.9344907833420094 Loss: 0.05481412717340947\n",
      "Iteration: 1464 lambda_n: 0.9868112130352388 Loss: 0.05474493567257934\n",
      "Iteration: 1465 lambda_n: 1.0131179889073556 Loss: 0.05467197959175161\n",
      "Iteration: 1466 lambda_n: 0.9006320111348051 Loss: 0.05459719607466444\n",
      "Iteration: 1467 lambda_n: 0.9301922193846373 Loss: 0.054530818360479565\n",
      "Iteration: 1468 lambda_n: 0.9831290324568973 Loss: 0.05446236060128333\n",
      "Iteration: 1469 lambda_n: 0.952662686586352 Loss: 0.05439011534377529\n",
      "Iteration: 1470 lambda_n: 0.8811172194833307 Loss: 0.05432021692252461\n",
      "Iteration: 1471 lambda_n: 0.9130388719882716 Loss: 0.054255663352632634\n",
      "Iteration: 1472 lambda_n: 0.9058673562951824 Loss: 0.05418886581867908\n",
      "Iteration: 1473 lambda_n: 0.9403193899823848 Loss: 0.05412268899360875\n",
      "Iteration: 1474 lambda_n: 0.9204721863486639 Loss: 0.05405409565525303\n",
      "Iteration: 1475 lambda_n: 0.9695673855639773 Loss: 0.05398705017449622\n",
      "Iteration: 1476 lambda_n: 0.9855075565243716 Loss: 0.05391653428934309\n",
      "Iteration: 1477 lambda_n: 0.9885906546864428 Loss: 0.05384497084757625\n",
      "Iteration: 1478 lambda_n: 0.8799062330640528 Loss: 0.0537732969885101\n",
      "Iteration: 1479 lambda_n: 1.0233639049344063 Loss: 0.05370960054264465\n",
      "Iteration: 1480 lambda_n: 0.9455777842729269 Loss: 0.053635629325210835\n",
      "Iteration: 1481 lambda_n: 0.9897584542148518 Loss: 0.0535673905153465\n",
      "Iteration: 1482 lambda_n: 0.9553174289797312 Loss: 0.05349607379766961\n",
      "Iteration: 1483 lambda_n: 0.8782046789958214 Loss: 0.0534273474465969\n",
      "Iteration: 1484 lambda_n: 0.9498540480379676 Loss: 0.05336426368853362\n",
      "Iteration: 1485 lambda_n: 0.9828453345555194 Loss: 0.053296132649969\n",
      "Iteration: 1486 lambda_n: 0.9571271944203866 Loss: 0.05322574492927636\n",
      "Iteration: 1487 lambda_n: 0.8946631478179635 Loss: 0.053157307466798975\n",
      "Iteration: 1488 lambda_n: 0.9841076882304252 Loss: 0.053093433806993565\n",
      "Iteration: 1489 lambda_n: 0.9167400021777538 Loss: 0.053023279906509395\n",
      "Iteration: 1490 lambda_n: 0.9757046892829415 Loss: 0.05295803095593678\n",
      "Iteration: 1491 lambda_n: 0.9141211597873586 Loss: 0.05288869124563247\n",
      "Iteration: 1492 lambda_n: 0.974488210394445 Loss: 0.052823829553948654\n",
      "Iteration: 1493 lambda_n: 0.9580864266806779 Loss: 0.05275479015830935\n",
      "Iteration: 1494 lambda_n: 1.0139615569442157 Loss: 0.05268702061864342\n",
      "Iteration: 1495 lambda_n: 0.9077944351791599 Loss: 0.05261541371934724\n",
      "Iteration: 1496 lambda_n: 1.0124959657364567 Loss: 0.05255140770931458\n",
      "Iteration: 1497 lambda_n: 0.8742690322699173 Loss: 0.052480130169597176\n",
      "Iteration: 1498 lambda_n: 1.0051741868069033 Loss: 0.05241868173427131\n",
      "Iteration: 1499 lambda_n: 0.9258774155136524 Loss: 0.05234813950444367\n",
      "Iteration: 1500 lambda_n: 1.0217561901542722 Loss: 0.05228326750723473\n",
      "Iteration: 1501 lambda_n: 1.0184857838485697 Loss: 0.05221179119249779\n",
      "Iteration: 1502 lambda_n: 0.9198521064066119 Loss: 0.05214066428973373\n",
      "Iteration: 1503 lambda_n: 0.9194158719864116 Loss: 0.05207653084006887\n",
      "Iteration: 1504 lambda_n: 0.9218843432806906 Loss: 0.05201252590156677\n",
      "Iteration: 1505 lambda_n: 0.9585220900275849 Loss: 0.05194844752585687\n",
      "Iteration: 1506 lambda_n: 0.9905320294025151 Loss: 0.051881926352927335\n",
      "Iteration: 1507 lambda_n: 1.0184657684080973 Loss: 0.051813295016708276\n",
      "Iteration: 1508 lambda_n: 0.9430446227178562 Loss: 0.05174284629947864\n",
      "Iteration: 1509 lambda_n: 0.9956967705513841 Loss: 0.05167772324412989\n",
      "Iteration: 1510 lambda_n: 0.8907859855895345 Loss: 0.05160907509633218\n",
      "Iteration: 1511 lambda_n: 0.8844973690101278 Loss: 0.051547759278721406\n",
      "Iteration: 1512 lambda_n: 1.0243153649737384 Loss: 0.05148696744741586\n",
      "Iteration: 1513 lambda_n: 0.9837398566925426 Loss: 0.05141667635008943\n",
      "Iteration: 1514 lambda_n: 0.8885220748713126 Loss: 0.05134928491975856\n",
      "Iteration: 1515 lambda_n: 1.0118103891518828 Loss: 0.0512885144825383\n",
      "Iteration: 1516 lambda_n: 0.9633797156674132 Loss: 0.05121942069557829\n",
      "Iteration: 1517 lambda_n: 1.0057546887631583 Loss: 0.05115374527394947\n",
      "Iteration: 1518 lambda_n: 0.978784219955887 Loss: 0.05108529496912136\n",
      "Iteration: 1519 lambda_n: 0.9363512970254004 Loss: 0.05101879327733624\n",
      "Iteration: 1520 lambda_n: 1.0171505812664732 Loss: 0.050955279256437114\n",
      "Iteration: 1521 lambda_n: 0.8897119496157817 Loss: 0.05088639799133924\n",
      "Iteration: 1522 lambda_n: 1.0132163827725278 Loss: 0.05082624730385286\n",
      "Iteration: 1523 lambda_n: 0.9242626869386783 Loss: 0.0507578560891106\n",
      "Iteration: 1524 lambda_n: 0.9659650076619116 Loss: 0.050695574452559455\n",
      "Iteration: 1525 lambda_n: 0.9560207032553703 Loss: 0.050630587637947144\n",
      "Iteration: 1526 lambda_n: 1.0156346910694345 Loss: 0.05056637640669851\n",
      "Iteration: 1527 lambda_n: 0.9136307389484978 Loss: 0.05049827596400942\n",
      "Iteration: 1528 lambda_n: 0.9522592324151763 Loss: 0.05043711892068246\n",
      "Iteration: 1529 lambda_n: 1.022708115053934 Loss: 0.050373478289521184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1530 lambda_n: 0.8841675674906873 Loss: 0.05030524501255122\n",
      "Iteration: 1531 lambda_n: 0.9166384433409163 Loss: 0.05024635482295827\n",
      "Iteration: 1532 lambda_n: 0.9911181011462944 Loss: 0.050185396864589484\n",
      "Iteration: 1533 lambda_n: 0.9587039777718993 Loss: 0.05011959391139683\n",
      "Iteration: 1534 lambda_n: 1.0121521059332732 Loss: 0.05005605177507684\n",
      "Iteration: 1535 lambda_n: 0.9195380600063626 Loss: 0.04998908154962116\n",
      "Iteration: 1536 lambda_n: 0.8998548031490032 Loss: 0.049928343614044884\n",
      "Iteration: 1537 lambda_n: 0.9923503734105679 Loss: 0.04986900086049191\n",
      "Iteration: 1538 lambda_n: 1.0244454821891655 Loss: 0.049803665217609594\n",
      "Iteration: 1539 lambda_n: 0.9909130372799908 Loss: 0.049736335374459005\n",
      "Iteration: 1540 lambda_n: 0.9017699304812964 Loss: 0.0496713255395838\n",
      "Iteration: 1541 lambda_n: 0.9011535528500008 Loss: 0.04961226424540206\n",
      "Iteration: 1542 lambda_n: 0.9571207790814058 Loss: 0.0495533373059033\n",
      "Iteration: 1543 lambda_n: 0.9806048430861634 Loss: 0.04949085252398407\n",
      "Iteration: 1544 lambda_n: 0.9974254632952683 Loss: 0.04942694408635736\n",
      "Iteration: 1545 lambda_n: 0.893687200879105 Loss: 0.04936205321224795\n",
      "Iteration: 1546 lambda_n: 0.9757769130687207 Loss: 0.049304010822468394\n",
      "Iteration: 1547 lambda_n: 0.9848785900072077 Loss: 0.04924074097928246\n",
      "Iteration: 1548 lambda_n: 0.9463600530793274 Loss: 0.049176992492937006\n",
      "Iteration: 1549 lambda_n: 0.9206581734850936 Loss: 0.049115843563657204\n",
      "Iteration: 1550 lambda_n: 0.9243305167446315 Loss: 0.049056455187891285\n",
      "Iteration: 1551 lambda_n: 1.0238189693324473 Loss: 0.048996928478564294\n",
      "Iteration: 1552 lambda_n: 0.980201270433521 Loss: 0.04893110823839102\n",
      "Iteration: 1553 lambda_n: 1.027535271796081 Loss: 0.048868206500202016\n",
      "Iteration: 1554 lambda_n: 0.9164516970788456 Loss: 0.04880238566246311\n",
      "Iteration: 1555 lambda_n: 1.025706532002379 Loss: 0.04874378535107383\n",
      "Iteration: 1556 lambda_n: 1.0057066754754715 Loss: 0.048678312198454796\n",
      "Iteration: 1557 lambda_n: 0.9182036202498365 Loss: 0.04861423418560789\n",
      "Iteration: 1558 lambda_n: 0.9084497687850673 Loss: 0.048555834958769827\n",
      "Iteration: 1559 lambda_n: 0.8814723815464613 Loss: 0.048498152206265\n",
      "Iteration: 1560 lambda_n: 0.9554420650759147 Loss: 0.04844227405935651\n",
      "Iteration: 1561 lambda_n: 0.8965632926125577 Loss: 0.04838180708499718\n",
      "Iteration: 1562 lambda_n: 0.9087801667282437 Loss: 0.04832516331458529\n",
      "Iteration: 1563 lambda_n: 1.0223605884032596 Loss: 0.04826784238011373\n",
      "Iteration: 1564 lambda_n: 0.930198110447283 Loss: 0.048203469559971\n",
      "Iteration: 1565 lambda_n: 0.9812762693759695 Loss: 0.048145006279737994\n",
      "Iteration: 1566 lambda_n: 0.9987482536514398 Loss: 0.04808344033176928\n",
      "Iteration: 1567 lambda_n: 0.9787004240738061 Loss: 0.04802089226017748\n",
      "Iteration: 1568 lambda_n: 0.9823705555209039 Loss: 0.04795971199900101\n",
      "Iteration: 1569 lambda_n: 0.9859048134142336 Loss: 0.04789841368995207\n",
      "Iteration: 1570 lambda_n: 0.9852494284542995 Loss: 0.04783700704810205\n",
      "Iteration: 1571 lambda_n: 0.9969872386463203 Loss: 0.047775753593172654\n",
      "Iteration: 1572 lambda_n: 0.9115418026250862 Loss: 0.047713884533969145\n",
      "Iteration: 1573 lambda_n: 0.9739285958369763 Loss: 0.04765741986544209\n",
      "Iteration: 1574 lambda_n: 0.9260777055067635 Loss: 0.04759719592310318\n",
      "Iteration: 1575 lambda_n: 0.9303138840668252 Loss: 0.04754003346731559\n",
      "Iteration: 1576 lambda_n: 0.9815655229888088 Loss: 0.04748270941232809\n",
      "Iteration: 1577 lambda_n: 0.9100462536900147 Loss: 0.04742233510404309\n",
      "Iteration: 1578 lambda_n: 0.912244632220349 Loss: 0.04736646055565675\n",
      "Iteration: 1579 lambda_n: 0.8919306670138939 Loss: 0.0473105472305573\n",
      "Iteration: 1580 lambda_n: 0.9299461701519801 Loss: 0.047255972455459636\n",
      "Iteration: 1581 lambda_n: 0.9460247332118855 Loss: 0.047199169124348274\n",
      "Iteration: 1582 lambda_n: 0.9580875374763563 Loss: 0.047141486194342296\n",
      "Iteration: 1583 lambda_n: 0.9721933453546528 Loss: 0.04708317321271291\n",
      "Iteration: 1584 lambda_n: 0.8932672148187458 Loss: 0.04702411017167948\n",
      "Iteration: 1585 lambda_n: 0.8802281907212639 Loss: 0.04696993977862077\n",
      "Iteration: 1586 lambda_n: 0.9992354794277117 Loss: 0.04691665074564143\n",
      "Iteration: 1587 lambda_n: 0.9052080725654544 Loss: 0.046856263885794586\n",
      "Iteration: 1588 lambda_n: 0.9153803495051361 Loss: 0.046801660701116826\n",
      "Iteration: 1589 lambda_n: 0.8896471819982225 Loss: 0.04674654036449058\n",
      "Iteration: 1590 lambda_n: 0.8781846461682843 Loss: 0.0466930630440584\n",
      "Iteration: 1591 lambda_n: 0.9319945188140532 Loss: 0.04664036493001881\n",
      "Iteration: 1592 lambda_n: 0.9027310035138374 Loss: 0.046584534825745\n",
      "Iteration: 1593 lambda_n: 1.0024001393447572 Loss: 0.04653055422554167\n",
      "Iteration: 1594 lambda_n: 1.0066346851091807 Loss: 0.04647072290709475\n",
      "Iteration: 1595 lambda_n: 1.023312120425059 Loss: 0.04641075614360235\n",
      "Iteration: 1596 lambda_n: 0.8876967674939571 Loss: 0.046349916198924736\n",
      "Iteration: 1597 lambda_n: 0.9700798222509781 Loss: 0.04629723954701535\n",
      "Iteration: 1598 lambda_n: 0.988757585164281 Loss: 0.04623977761245663\n",
      "Iteration: 1599 lambda_n: 0.9636349267083221 Loss: 0.04618132154273162\n",
      "Iteration: 1600 lambda_n: 0.9278874846325971 Loss: 0.046124460458076465\n",
      "Iteration: 1601 lambda_n: 0.9616197973387828 Loss: 0.04606981126247962\n",
      "Iteration: 1602 lambda_n: 1.0071316846697855 Loss: 0.046013280493277915\n",
      "Iteration: 1603 lambda_n: 0.9766263009428364 Loss: 0.04595418882377248\n",
      "Iteration: 1604 lambda_n: 0.9268579014265961 Loss: 0.04589700019523898\n",
      "Iteration: 1605 lambda_n: 0.9432598855354452 Loss: 0.04584282927441029\n",
      "Iteration: 1606 lambda_n: 0.9853045846678758 Loss: 0.045787802188593185\n",
      "Iteration: 1607 lambda_n: 1.0035183377846508 Loss: 0.04573043236766182\n",
      "Iteration: 1608 lambda_n: 0.912630005899978 Loss: 0.04567211805016912\n",
      "Iteration: 1609 lambda_n: 0.9188968872027945 Loss: 0.045619188451687734\n",
      "Iteration: 1610 lambda_n: 1.0094249053889641 Loss: 0.04556599343947111\n",
      "Iteration: 1611 lambda_n: 0.9518048795864297 Loss: 0.04550766990241242\n",
      "Iteration: 1612 lambda_n: 0.9771633386684694 Loss: 0.04545278537469068\n",
      "Iteration: 1613 lambda_n: 0.9737206246584654 Loss: 0.045396548264998185\n",
      "Iteration: 1614 lambda_n: 0.9072189169085545 Loss: 0.0453406203073476\n",
      "Iteration: 1615 lambda_n: 0.9285593365610563 Loss: 0.04528861267500112\n",
      "Iteration: 1616 lambda_n: 1.021585640450875 Loss: 0.04523548104272801\n",
      "Iteration: 1617 lambda_n: 1.0187076087816256 Loss: 0.04517714161948274\n",
      "Iteration: 1618 lambda_n: 0.8940113251208799 Loss: 0.04511908826336551\n",
      "Iteration: 1619 lambda_n: 0.9078068332175419 Loss: 0.04506824282575452\n",
      "Iteration: 1620 lambda_n: 0.8950074728873386 Loss: 0.04501670845358227\n",
      "Iteration: 1621 lambda_n: 0.9686011795862236 Loss: 0.04496599544892839\n",
      "Iteration: 1622 lambda_n: 0.9252007494149852 Loss: 0.04491121731297256\n",
      "Iteration: 1623 lambda_n: 0.9290461957425658 Loss: 0.0448589970913207\n",
      "Iteration: 1624 lambda_n: 0.9484295658725611 Loss: 0.04480666096476116\n",
      "Iteration: 1625 lambda_n: 0.9313010292317211 Loss: 0.04475333730956872\n",
      "Iteration: 1626 lambda_n: 0.949570984107611 Loss: 0.0447010798800964\n",
      "Iteration: 1627 lambda_n: 0.9519181020944153 Loss: 0.04464790215988374\n",
      "Iteration: 1628 lambda_n: 0.9129079145621439 Loss: 0.0445946995908414\n",
      "Iteration: 1629 lambda_n: 0.9586606074129134 Loss: 0.04454377817293132\n",
      "Iteration: 1630 lambda_n: 0.9695732402259449 Loss: 0.04449040991933746\n",
      "Iteration: 1631 lambda_n: 0.9751528413363071 Loss: 0.04443654440568079\n",
      "Iteration: 1632 lambda_n: 0.9613327197442543 Loss: 0.04438248091140387\n",
      "Iteration: 1633 lambda_n: 1.012485376796292 Loss: 0.04432929392887719\n",
      "Iteration: 1634 lambda_n: 0.9236193732990283 Loss: 0.04427339444724138\n",
      "Iteration: 1635 lambda_n: 1.0202045895223628 Loss: 0.04422250842067866\n",
      "Iteration: 1636 lambda_n: 0.8807947278105128 Loss: 0.044166417363616345\n",
      "Iteration: 1637 lambda_n: 0.9373156840260168 Loss: 0.04411809225820345\n",
      "Iteration: 1638 lambda_n: 0.9899056800583276 Loss: 0.044066766664417535\n",
      "Iteration: 1639 lambda_n: 0.8918469675792882 Loss: 0.04401267414229586\n",
      "Iteration: 1640 lambda_n: 0.9583756459471372 Loss: 0.04396404115576843\n",
      "Iteration: 1641 lambda_n: 0.9730142024044125 Loss: 0.043911885183594485\n",
      "Iteration: 1642 lambda_n: 1.0002587668717826 Loss: 0.04385904458232696\n",
      "Iteration: 1643 lambda_n: 0.9694147236299028 Loss: 0.04380484206896974\n",
      "Iteration: 1644 lambda_n: 0.9236083731033226 Loss: 0.04375242567474741\n",
      "Iteration: 1645 lambda_n: 1.0187897022701882 Loss: 0.043702591429345086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1646 lambda_n: 0.895092856523317 Loss: 0.04364773903899226\n",
      "Iteration: 1647 lambda_n: 0.9609081152227018 Loss: 0.04359965107350358\n",
      "Iteration: 1648 lambda_n: 0.9183170081959537 Loss: 0.04354813374996298\n",
      "Iteration: 1649 lambda_n: 0.9683768871876968 Loss: 0.043499004442484404\n",
      "Iteration: 1650 lambda_n: 0.9748703983281755 Loss: 0.04344730662904777\n",
      "Iteration: 1651 lambda_n: 1.0038880713238492 Loss: 0.04339537664067102\n",
      "Iteration: 1652 lambda_n: 0.9306369880282516 Loss: 0.04334202087620147\n",
      "Iteration: 1653 lambda_n: 0.8906317578242783 Loss: 0.043292668515370225\n",
      "Iteration: 1654 lambda_n: 0.9477970490544954 Loss: 0.0432455367459225\n",
      "Iteration: 1655 lambda_n: 1.019560633468405 Loss: 0.043195485213022175\n",
      "Iteration: 1656 lambda_n: 0.9325967640324068 Loss: 0.04314176539029863\n",
      "Iteration: 1657 lambda_n: 0.919675921772892 Loss: 0.043092740002059975\n",
      "Iteration: 1658 lambda_n: 0.9364414699392666 Loss: 0.043044498357259264\n",
      "Iteration: 1659 lambda_n: 0.9342726228003928 Loss: 0.04299548376357632\n",
      "Iteration: 1660 lambda_n: 0.9417346348821997 Loss: 0.04294669022743384\n",
      "Iteration: 1661 lambda_n: 0.8896273827847011 Loss: 0.04289761580076773\n",
      "Iteration: 1662 lambda_n: 1.0054574501797156 Loss: 0.042851357979981686\n",
      "Iteration: 1663 lambda_n: 0.9725004925449859 Loss: 0.04279919385861591\n",
      "Iteration: 1664 lambda_n: 0.9468382024793282 Loss: 0.04274885941401274\n",
      "Iteration: 1665 lambda_n: 0.8975653363706013 Loss: 0.04269996662046509\n",
      "Iteration: 1666 lambda_n: 0.8884804838991733 Loss: 0.04265372204888727\n",
      "Iteration: 1667 lambda_n: 0.921304617182903 Loss: 0.04260804493011672\n",
      "Iteration: 1668 lambda_n: 0.8751542893550835 Loss: 0.04256078451655538\n",
      "Iteration: 1669 lambda_n: 0.9117440298534862 Loss: 0.04251599088539929\n",
      "Iteration: 1670 lambda_n: 0.9724873081361863 Loss: 0.042469426794392584\n",
      "Iteration: 1671 lambda_n: 0.9251799260915247 Loss: 0.042419875635290705\n",
      "Iteration: 1672 lambda_n: 0.9985716859140451 Loss: 0.042372846891872526\n",
      "Iteration: 1673 lambda_n: 1.0033238987101227 Loss: 0.042322208895294276\n",
      "Iteration: 1674 lambda_n: 0.9457871821168915 Loss: 0.04227145818336134\n",
      "Iteration: 1675 lambda_n: 1.0108741498799896 Loss: 0.042223736703405026\n",
      "Iteration: 1676 lambda_n: 0.9162211767010247 Loss: 0.04217285762993268\n",
      "Iteration: 1677 lambda_n: 0.9914466166333015 Loss: 0.04212685773551201\n",
      "Iteration: 1678 lambda_n: 0.9606240413664762 Loss: 0.04207720282115637\n",
      "Iteration: 1679 lambda_n: 1.0197613403908472 Loss: 0.042029214073171305\n",
      "Iteration: 1680 lambda_n: 0.95549291907998 Loss: 0.04197840241784729\n",
      "Iteration: 1681 lambda_n: 0.8852736321202608 Loss: 0.041930917778388636\n",
      "Iteration: 1682 lambda_n: 0.9220573548418645 Loss: 0.04188703104595317\n",
      "Iteration: 1683 lambda_n: 0.9555414048100759 Loss: 0.04184143081005878\n",
      "Iteration: 1684 lambda_n: 1.0004023174479864 Loss: 0.04179429367387219\n",
      "Iteration: 1685 lambda_n: 0.9777989105792313 Loss: 0.04174507390163941\n",
      "Iteration: 1686 lambda_n: 0.9942296196482828 Loss: 0.04169709654422899\n",
      "Iteration: 1687 lambda_n: 0.9787404444137954 Loss: 0.04164844534816673\n",
      "Iteration: 1688 lambda_n: 0.9696315174493085 Loss: 0.04160068354782431\n",
      "Iteration: 1689 lambda_n: 0.8821060619281046 Loss: 0.041553495529767226\n",
      "Iteration: 1690 lambda_n: 0.9104364233369058 Loss: 0.04151068022980111\n",
      "Iteration: 1691 lambda_n: 0.9619958686305633 Loss: 0.04146660240388694\n",
      "Iteration: 1692 lambda_n: 0.8921652720886489 Loss: 0.04142015311787414\n",
      "Iteration: 1693 lambda_n: 0.8949892674186442 Loss: 0.04137719204596379\n",
      "Iteration: 1694 lambda_n: 0.9600063227182077 Loss: 0.04133420760962634\n",
      "Iteration: 1695 lambda_n: 0.959086337281391 Loss: 0.04128822607130177\n",
      "Iteration: 1696 lambda_n: 1.0127723009671954 Loss: 0.041242419978303975\n",
      "Iteration: 1697 lambda_n: 0.9349003636089597 Loss: 0.04119419281838072\n",
      "Iteration: 1698 lambda_n: 0.9374745817158302 Loss: 0.04114980656475666\n",
      "Iteration: 1699 lambda_n: 0.8880175736073472 Loss: 0.041105426209913885\n",
      "Iteration: 1700 lambda_n: 0.9510021844407681 Loss: 0.04106350689538905\n",
      "Iteration: 1701 lambda_n: 0.9316874821795507 Loss: 0.04101874336841219\n",
      "Iteration: 1702 lambda_n: 0.9246264394144067 Loss: 0.04097502029026129\n",
      "Iteration: 1703 lambda_n: 0.9787205694089366 Loss: 0.04093175803544739\n",
      "Iteration: 1704 lambda_n: 1.0262248841350585 Loss: 0.040886105873509555\n",
      "Iteration: 1705 lambda_n: 0.917058533734765 Loss: 0.04083839532171187\n",
      "Iteration: 1706 lambda_n: 0.8941736972995851 Loss: 0.04079589927373008\n",
      "Iteration: 1707 lambda_n: 0.9425939113358124 Loss: 0.04075459067194196\n",
      "Iteration: 1708 lambda_n: 0.9300140082581136 Loss: 0.04071118151528312\n",
      "Iteration: 1709 lambda_n: 0.9240667652445995 Loss: 0.04066849083865734\n",
      "Iteration: 1710 lambda_n: 0.8944839453657764 Loss: 0.04062621140426758\n",
      "Iteration: 1711 lambda_n: 0.9347241904557645 Loss: 0.04058541835215286\n",
      "Iteration: 1712 lambda_n: 0.8778166858724479 Loss: 0.04054293062803887\n",
      "Iteration: 1713 lambda_n: 1.010321845964339 Loss: 0.040503162672208624\n",
      "Iteration: 1714 lambda_n: 0.9615616042507378 Loss: 0.04045755121505342\n",
      "Iteration: 1715 lambda_n: 0.9202730541478832 Loss: 0.04042004211712298\n",
      "Iteration: 1716 lambda_n: 0.9851195011125193 Loss: 0.040387446572572754\n",
      "Iteration: 1717 lambda_n: 1.0069318594620098 Loss: 0.040352472760742907\n",
      "Iteration: 1718 lambda_n: 1.0248834969260083 Loss: 0.04031667692480037\n",
      "Iteration: 1719 lambda_n: 1.0085376757835294 Loss: 0.040280324014985214\n",
      "Iteration: 1720 lambda_n: 0.9025841144750899 Loss: 0.04024472508611439\n",
      "Iteration: 1721 lambda_n: 0.9362383909877174 Loss: 0.04021306974115737\n",
      "Iteration: 1722 lambda_n: 0.9619482929849005 Loss: 0.04018045584082852\n",
      "Iteration: 1723 lambda_n: 0.9312898154522945 Loss: 0.0401471981245186\n",
      "Iteration: 1724 lambda_n: 0.9800305667521809 Loss: 0.04011525855394321\n",
      "Iteration: 1725 lambda_n: 0.998022101087111 Loss: 0.04008191692986681\n",
      "Iteration: 1726 lambda_n: 1.016505321988836 Loss: 0.040048251883082996\n",
      "Iteration: 1727 lambda_n: 0.8829904969760412 Loss: 0.040014260937779746\n",
      "Iteration: 1728 lambda_n: 0.9619348179166068 Loss: 0.03998499131125877\n",
      "Iteration: 1729 lambda_n: 0.981002459498184 Loss: 0.03995334822925499\n",
      "Iteration: 1730 lambda_n: 0.9678802803118778 Loss: 0.039921342062271124\n",
      "Iteration: 1731 lambda_n: 0.9360693913222933 Loss: 0.03989002371290849\n",
      "Iteration: 1732 lambda_n: 1.0229095064873812 Loss: 0.03985997676881535\n",
      "Iteration: 1733 lambda_n: 0.8995707877883001 Loss: 0.03982739511730966\n",
      "Iteration: 1734 lambda_n: 0.9614348900529625 Loss: 0.039798975634728\n",
      "Iteration: 1735 lambda_n: 1.012209370572539 Loss: 0.039768819436126424\n",
      "Iteration: 1736 lambda_n: 0.9754188832811652 Loss: 0.03973731013360709\n",
      "Iteration: 1737 lambda_n: 0.9649639529599827 Loss: 0.03970718194626334\n",
      "Iteration: 1738 lambda_n: 0.8859799710898246 Loss: 0.03967759677258591\n",
      "Iteration: 1739 lambda_n: 0.9880124685443736 Loss: 0.039650627584041837\n",
      "Iteration: 1740 lambda_n: 0.9681277566935272 Loss: 0.03962075021809351\n",
      "Iteration: 1741 lambda_n: 0.9874814682761093 Loss: 0.03959168360320266\n",
      "Iteration: 1742 lambda_n: 0.9066507763475353 Loss: 0.03956224123301685\n",
      "Iteration: 1743 lambda_n: 0.9943114313135785 Loss: 0.03953539556339296\n",
      "Iteration: 1744 lambda_n: 0.9787715667853938 Loss: 0.03950614068179758\n",
      "Iteration: 1745 lambda_n: 0.9168379279625245 Loss: 0.03947753851179732\n",
      "Iteration: 1746 lambda_n: 0.9026843785982829 Loss: 0.03945092192894912\n",
      "Iteration: 1747 lambda_n: 1.0008571223485352 Loss: 0.039424875451608034\n",
      "Iteration: 1748 lambda_n: 0.9426092575510968 Loss: 0.03939616815707748\n",
      "Iteration: 1749 lambda_n: 1.0076305910326928 Loss: 0.03936930539094706\n",
      "Iteration: 1750 lambda_n: 1.005829076400498 Loss: 0.03934076247900416\n",
      "Iteration: 1751 lambda_n: 0.9746278339381587 Loss: 0.03931245035916578\n",
      "Iteration: 1752 lambda_n: 1.0205231708125972 Loss: 0.03928518627949221\n",
      "Iteration: 1753 lambda_n: 0.94380255169228 Loss: 0.039256807876297393\n",
      "Iteration: 1754 lambda_n: 0.9805747008540212 Loss: 0.03923072228440158\n",
      "Iteration: 1755 lambda_n: 1.0134271493972964 Loss: 0.03920377146161661\n",
      "Iteration: 1756 lambda_n: 1.0180036807744575 Loss: 0.03917607665388304\n",
      "Iteration: 1757 lambda_n: 0.9062734061319092 Loss: 0.039148418070285435\n",
      "Iteration: 1758 lambda_n: 0.9890026327616623 Loss: 0.039123935205780315\n",
      "Iteration: 1759 lambda_n: 1.0265184765522806 Loss: 0.039097352542707554\n",
      "Iteration: 1760 lambda_n: 0.9653334845943107 Loss: 0.03906991110064481\n",
      "Iteration: 1761 lambda_n: 0.9235688639257248 Loss: 0.03904424733760192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1762 lambda_n: 0.9324668163704154 Loss: 0.03901981917451965\n",
      "Iteration: 1763 lambda_n: 0.9274401454383637 Loss: 0.03899527471263202\n",
      "Iteration: 1764 lambda_n: 0.8800184493616705 Loss: 0.0389709797414615\n",
      "Iteration: 1765 lambda_n: 0.9148391393124047 Loss: 0.038948035166107946\n",
      "Iteration: 1766 lambda_n: 1.0148802701257909 Loss: 0.03892428804782578\n",
      "Iteration: 1767 lambda_n: 0.9938314101495664 Loss: 0.03889806394626541\n",
      "Iteration: 1768 lambda_n: 0.9735170456463844 Loss: 0.03887251038694817\n",
      "Iteration: 1769 lambda_n: 0.9502152687353931 Loss: 0.03884759816520179\n",
      "Iteration: 1770 lambda_n: 0.9628218434966784 Loss: 0.03882339373625434\n",
      "Iteration: 1771 lambda_n: 0.9795298700153003 Loss: 0.03879897659422273\n",
      "Iteration: 1772 lambda_n: 0.9861685925671109 Loss: 0.03877424540015924\n",
      "Iteration: 1773 lambda_n: 0.9509039281883818 Loss: 0.038749456682635644\n",
      "Iteration: 1774 lambda_n: 0.9879437782615388 Loss: 0.03872565889317397\n",
      "Iteration: 1775 lambda_n: 0.9713941909607509 Loss: 0.038701037302843494\n",
      "Iteration: 1776 lambda_n: 0.8822902858063656 Loss: 0.03867693119663497\n",
      "Iteration: 1777 lambda_n: 1.000857166454038 Loss: 0.03865512607674047\n",
      "Iteration: 1778 lambda_n: 0.8943184921046082 Loss: 0.03863048282384779\n",
      "Iteration: 1779 lambda_n: 0.9124237113870757 Loss: 0.03860855309415778\n",
      "Iteration: 1780 lambda_n: 1.0079519410592221 Loss: 0.038586260847452376\n",
      "Iteration: 1781 lambda_n: 0.9749813430970193 Loss: 0.03856172537470928\n",
      "Iteration: 1782 lambda_n: 1.0084693632180552 Loss: 0.03853808679839694\n",
      "Iteration: 1783 lambda_n: 0.8872069602106767 Loss: 0.038513729273874156\n",
      "Iteration: 1784 lambda_n: 0.9319616812194282 Loss: 0.03849238275036118\n",
      "Iteration: 1785 lambda_n: 0.918559996368789 Loss: 0.038470034757327166\n",
      "Iteration: 1786 lambda_n: 0.9606869726539927 Loss: 0.03844808447773069\n",
      "Iteration: 1787 lambda_n: 0.882082935131337 Loss: 0.03842520514289112\n",
      "Iteration: 1788 lambda_n: 0.9590659876393695 Loss: 0.03840427041913975\n",
      "Iteration: 1789 lambda_n: 0.9352904776756855 Loss: 0.03838158064703106\n",
      "Iteration: 1790 lambda_n: 1.0194800596630487 Loss: 0.03835952786631818\n",
      "Iteration: 1791 lambda_n: 0.989291019930008 Loss: 0.03833556841405821\n",
      "Iteration: 1792 lambda_n: 0.9958336851705333 Loss: 0.038312399246874705\n",
      "Iteration: 1793 lambda_n: 0.9381542181058196 Loss: 0.03828915447253819\n",
      "Iteration: 1794 lambda_n: 1.0180916740224346 Loss: 0.03826732798830856\n",
      "Iteration: 1795 lambda_n: 0.9892969984152666 Loss: 0.03824371465491717\n",
      "Iteration: 1796 lambda_n: 0.9330395841830039 Loss: 0.0382208441391358\n",
      "Iteration: 1797 lambda_n: 0.8936907639839884 Loss: 0.038199341462731656\n",
      "Iteration: 1798 lambda_n: 0.9023978557844854 Loss: 0.03817880536274107\n",
      "Iteration: 1799 lambda_n: 0.9584448772997081 Loss: 0.03815812620371184\n",
      "Iteration: 1800 lambda_n: 0.9791023171940709 Loss: 0.03813622306992599\n",
      "Iteration: 1801 lambda_n: 0.9819907558489837 Loss: 0.03811391211366913\n",
      "Iteration: 1802 lambda_n: 0.989300009151861 Loss: 0.03809159994799773\n",
      "Iteration: 1803 lambda_n: 0.9755417602729506 Loss: 0.0380691858622888\n",
      "Iteration: 1804 lambda_n: 0.9198808863271233 Loss: 0.038047146006582204\n",
      "Iteration: 1805 lambda_n: 0.9594145402871369 Loss: 0.03802642058692081\n",
      "Iteration: 1806 lambda_n: 0.8875468213936166 Loss: 0.038004859881020835\n",
      "Iteration: 1807 lambda_n: 1.0002215929644414 Loss: 0.0379849663833371\n",
      "Iteration: 1808 lambda_n: 0.9265041394184006 Loss: 0.03796260165934202\n",
      "Iteration: 1809 lambda_n: 1.0176460682599289 Loss: 0.03794194015369065\n",
      "Iteration: 1810 lambda_n: 0.9198789100309193 Loss: 0.03791930176690457\n",
      "Iteration: 1811 lambda_n: 1.0262207662671292 Loss: 0.03789889179548535\n",
      "Iteration: 1812 lambda_n: 0.9213974340248247 Loss: 0.037876176260494415\n",
      "Iteration: 1813 lambda_n: 0.9382706783118286 Loss: 0.03785583323322687\n",
      "Iteration: 1814 lambda_n: 0.9969134659635417 Loss: 0.037835165036816575\n",
      "Iteration: 1815 lambda_n: 0.9714313286820873 Loss: 0.037813255675047945\n",
      "Iteration: 1816 lambda_n: 0.9560369740234277 Loss: 0.03779195753362086\n",
      "Iteration: 1817 lambda_n: 0.9638989261923919 Loss: 0.03777104520643649\n",
      "Iteration: 1818 lambda_n: 0.8781215433696641 Loss: 0.03775000813832104\n",
      "Iteration: 1819 lambda_n: 0.9016052048497418 Loss: 0.03773088549634656\n",
      "Iteration: 1820 lambda_n: 0.8824226283874262 Loss: 0.03771129079576954\n",
      "Iteration: 1821 lambda_n: 0.9837954400418626 Loss: 0.03769215178421645\n",
      "Iteration: 1822 lambda_n: 0.923540471857477 Loss: 0.0376708562630384\n",
      "Iteration: 1823 lambda_n: 0.9872156047129704 Loss: 0.037650907876970625\n",
      "Iteration: 1824 lambda_n: 0.900564135698537 Loss: 0.0376296268889651\n",
      "Iteration: 1825 lambda_n: 0.9527714552012155 Loss: 0.03761025431627851\n",
      "Iteration: 1826 lambda_n: 1.0093871692023855 Loss: 0.037589797648111066\n",
      "Iteration: 1827 lambda_n: 0.9038461255885456 Loss: 0.037568168439698506\n",
      "Iteration: 1828 lambda_n: 0.9281161546046192 Loss: 0.03754884036463346\n",
      "Iteration: 1829 lambda_n: 0.9498377149922853 Loss: 0.03752902954382702\n",
      "Iteration: 1830 lambda_n: 0.9414232489352533 Loss: 0.037508792599713334\n",
      "Iteration: 1831 lambda_n: 0.9585489345194389 Loss: 0.03748877231515824\n",
      "Iteration: 1832 lambda_n: 0.9500437757626319 Loss: 0.03746842508570674\n",
      "Iteration: 1833 lambda_n: 0.907193135842834 Loss: 0.03744829532908168\n",
      "Iteration: 1834 lambda_n: 0.9695566657960031 Loss: 0.03742910779876404\n",
      "Iteration: 1835 lambda_n: 0.877339067431155 Loss: 0.03740863612007566\n",
      "Iteration: 1836 lambda_n: 0.9557336667676927 Loss: 0.03739014426495183\n",
      "Iteration: 1837 lambda_n: 1.0254856565315238 Loss: 0.0373700324127049\n",
      "Iteration: 1838 lambda_n: 0.8860091495630503 Loss: 0.0373484899702128\n",
      "Iteration: 1839 lambda_n: 0.8804217973820807 Loss: 0.03732991078762947\n",
      "Iteration: 1840 lambda_n: 0.8764821524149148 Loss: 0.037311477266123654\n",
      "Iteration: 1841 lambda_n: 0.9682267763545297 Loss: 0.03729315404224271\n",
      "Iteration: 1842 lambda_n: 0.9717430933910071 Loss: 0.03727294338863792\n",
      "Iteration: 1843 lambda_n: 1.0210973765046862 Loss: 0.0372526923719562\n",
      "Iteration: 1844 lambda_n: 0.9905638958840977 Loss: 0.03723144732568213\n",
      "Iteration: 1845 lambda_n: 0.9354381525428439 Loss: 0.0372108719150747\n",
      "Iteration: 1846 lambda_n: 1.0140772631696637 Loss: 0.037191472442933235\n",
      "Iteration: 1847 lambda_n: 0.9080257931127043 Loss: 0.03717047377572653\n",
      "Iteration: 1848 lambda_n: 0.8872255365082444 Loss: 0.037151700797532924\n",
      "Iteration: 1849 lambda_n: 0.8996166036577703 Loss: 0.037133383662289264\n",
      "Iteration: 1850 lambda_n: 0.9245150313298266 Loss: 0.037114836046676036\n",
      "Iteration: 1851 lambda_n: 0.9575940451019903 Loss: 0.037095801198624816\n",
      "Iteration: 1852 lambda_n: 0.9593856317424154 Loss: 0.03707611273828621\n",
      "Iteration: 1853 lambda_n: 0.9091549062069542 Loss: 0.03705641543941\n",
      "Iteration: 1854 lambda_n: 0.9789954655304339 Loss: 0.03703777546650522\n",
      "Iteration: 1855 lambda_n: 0.9408286611637051 Loss: 0.03701773021461933\n",
      "Iteration: 1856 lambda_n: 0.967071610170195 Loss: 0.036998493246671676\n",
      "Iteration: 1857 lambda_n: 0.8807257935317383 Loss: 0.036978746033420114\n",
      "Iteration: 1858 lambda_n: 0.9531530053854679 Loss: 0.03696078592836894\n",
      "Iteration: 1859 lambda_n: 0.949003656474685 Loss: 0.036941372690539084\n",
      "Iteration: 1860 lambda_n: 0.9644500352798735 Loss: 0.036922069041115245\n",
      "Iteration: 1861 lambda_n: 1.0062041636206323 Loss: 0.03690247630864417\n",
      "Iteration: 1862 lambda_n: 1.0174154141291498 Loss: 0.03688206172102649\n",
      "Iteration: 1863 lambda_n: 0.8997203300322139 Loss: 0.036861447011737235\n",
      "Iteration: 1864 lambda_n: 0.992981055379581 Loss: 0.03684324070579761\n",
      "Iteration: 1865 lambda_n: 0.8824437324183705 Loss: 0.036823170742419864\n",
      "Iteration: 1866 lambda_n: 0.8872292884554849 Loss: 0.03680535703922673\n",
      "Iteration: 1867 lambda_n: 0.9552447274970869 Loss: 0.0367874665511103\n",
      "Iteration: 1868 lambda_n: 0.9263083765153357 Loss: 0.03676822599903328\n",
      "Iteration: 1869 lambda_n: 0.900752995986913 Loss: 0.036749590049671516\n",
      "Iteration: 1870 lambda_n: 0.9426332490684455 Loss: 0.03673148851371484\n",
      "Iteration: 1871 lambda_n: 0.9860997825857998 Loss: 0.03671256597150264\n",
      "Iteration: 1872 lambda_n: 0.978377176125941 Loss: 0.036692793189381435\n",
      "Iteration: 1873 lambda_n: 0.9415257490281684 Loss: 0.03667319794738217\n",
      "Iteration: 1874 lambda_n: 0.8808301967401575 Loss: 0.036654362069527144\n",
      "Iteration: 1875 lambda_n: 0.9167325099093312 Loss: 0.03663675929864374\n",
      "Iteration: 1876 lambda_n: 0.9257537281377911 Loss: 0.03661845749593632\n",
      "Iteration: 1877 lambda_n: 0.9354468026642422 Loss: 0.03659999467662903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1878 lambda_n: 0.9154080815787023 Loss: 0.03658135780004735\n",
      "Iteration: 1879 lambda_n: 0.9318917918887701 Loss: 0.03656313888026249\n",
      "Iteration: 1880 lambda_n: 0.910167676444242 Loss: 0.03654461046048235\n",
      "Iteration: 1881 lambda_n: 0.9868736563767406 Loss: 0.03652653210086883\n",
      "Iteration: 1882 lambda_n: 1.0061461364909832 Loss: 0.03650694949771506\n",
      "Iteration: 1883 lambda_n: 1.026912566623149 Loss: 0.036487005385769156\n",
      "Iteration: 1884 lambda_n: 0.9215888417389279 Loss: 0.036466671153581945\n",
      "Iteration: 1885 lambda_n: 1.0235550093191896 Loss: 0.03644844151729165\n",
      "Iteration: 1886 lambda_n: 0.9721484881564846 Loss: 0.03642821442385791\n",
      "Iteration: 1887 lambda_n: 0.9648788531293863 Loss: 0.03640902296968701\n",
      "Iteration: 1888 lambda_n: 0.9888909767104143 Loss: 0.036389993588244206\n",
      "Iteration: 1889 lambda_n: 0.8908666707225573 Loss: 0.03637050942707129\n",
      "Iteration: 1890 lambda_n: 0.9080938640223053 Loss: 0.03635297339751005\n",
      "Iteration: 1891 lambda_n: 0.9137351099430953 Loss: 0.03633511383427021\n",
      "Iteration: 1892 lambda_n: 0.9833366835121038 Loss: 0.03631715909832109\n",
      "Iteration: 1893 lambda_n: 0.9276976359252304 Loss: 0.036297853850821835\n",
      "Iteration: 1894 lambda_n: 1.0130454410778948 Loss: 0.03627965771926968\n",
      "Iteration: 1895 lambda_n: 1.0238407953435014 Loss: 0.03625980518425197\n",
      "Iteration: 1896 lambda_n: 0.985723369503821 Loss: 0.0362397600521194\n",
      "Iteration: 1897 lambda_n: 0.8881814489670681 Loss: 0.03622047926728904\n",
      "Iteration: 1898 lambda_n: 1.0214718300394356 Loss: 0.03620312169826987\n",
      "Iteration: 1899 lambda_n: 0.9827958486296718 Loss: 0.036183175807316136\n",
      "Iteration: 1900 lambda_n: 0.8862387034894885 Loss: 0.03616400256793978\n",
      "Iteration: 1901 lambda_n: 0.9084186634282636 Loss: 0.03614672782444824\n",
      "Iteration: 1902 lambda_n: 0.970399296532461 Loss: 0.03612903463605201\n",
      "Iteration: 1903 lambda_n: 0.9453705840941725 Loss: 0.03611014947753091\n",
      "Iteration: 1904 lambda_n: 0.934815056775337 Loss: 0.03609176678509043\n",
      "Iteration: 1905 lambda_n: 0.9983498308789089 Loss: 0.03607360406526379\n",
      "Iteration: 1906 lambda_n: 0.9777329224846378 Loss: 0.036054222597678987\n",
      "Iteration: 1907 lambda_n: 0.9777301823442428 Loss: 0.036035257309522506\n",
      "Iteration: 1908 lambda_n: 0.9173963474227248 Loss: 0.036016307605525176\n",
      "Iteration: 1909 lambda_n: 0.9874235430308334 Loss: 0.03599854148130843\n",
      "Iteration: 1910 lambda_n: 1.0092241938391027 Loss: 0.03597943393714892\n",
      "Iteration: 1911 lambda_n: 0.9943898177160556 Loss: 0.035959920379659956\n",
      "Iteration: 1912 lambda_n: 0.9106697291897253 Loss: 0.035940709327527944\n",
      "Iteration: 1913 lambda_n: 0.8774561129869667 Loss: 0.03592312948158858\n",
      "Iteration: 1914 lambda_n: 0.9217899695407898 Loss: 0.03590620299764944\n",
      "Iteration: 1915 lambda_n: 0.9908702772848004 Loss: 0.035888433810005595\n",
      "Iteration: 1916 lambda_n: 0.9258460642217554 Loss: 0.035869347096344796\n",
      "Iteration: 1917 lambda_n: 1.0005414366988878 Loss: 0.035851526484546155\n",
      "Iteration: 1918 lambda_n: 0.8988855377348948 Loss: 0.035832282246596105\n",
      "Iteration: 1919 lambda_n: 1.0180801035350404 Loss: 0.035815006204542504\n",
      "Iteration: 1920 lambda_n: 0.9585661582100496 Loss: 0.03579545321567836\n",
      "Iteration: 1921 lambda_n: 0.9539819142917023 Loss: 0.03577705724233792\n",
      "Iteration: 1922 lambda_n: 0.9362806603021623 Loss: 0.035758762449013305\n",
      "Iteration: 1923 lambda_n: 1.0073093523614 Loss: 0.03574081986866226\n",
      "Iteration: 1924 lambda_n: 0.887454326972243 Loss: 0.03572152982299055\n",
      "Iteration: 1925 lambda_n: 0.9068287300212423 Loss: 0.03570454723696115\n",
      "Iteration: 1926 lambda_n: 0.9285208481229993 Loss: 0.03568720525341799\n",
      "Iteration: 1927 lambda_n: 0.886254252250828 Loss: 0.035669460245177274\n",
      "Iteration: 1928 lambda_n: 0.9843972322582313 Loss: 0.035652534239876583\n",
      "Iteration: 1929 lambda_n: 0.9724213329284405 Loss: 0.035633746230658646\n",
      "Iteration: 1930 lambda_n: 1.0075446684423754 Loss: 0.03561519981967113\n",
      "Iteration: 1931 lambda_n: 0.9402011246508554 Loss: 0.03559599694664417\n",
      "Iteration: 1932 lambda_n: 0.9348495221820096 Loss: 0.0355780900914208\n",
      "Iteration: 1933 lambda_n: 0.9811082421234728 Loss: 0.03556029689625877\n",
      "Iteration: 1934 lambda_n: 0.9531195486362045 Loss: 0.035541635612274844\n",
      "Iteration: 1935 lambda_n: 0.9737863341015885 Loss: 0.035523518932368055\n",
      "Iteration: 1936 lambda_n: 1.0148230506043157 Loss: 0.03550502167506018\n",
      "Iteration: 1937 lambda_n: 1.007378347087968 Loss: 0.03548575796104426\n",
      "Iteration: 1938 lambda_n: 0.8837897002728925 Loss: 0.03546664877622571\n",
      "Iteration: 1939 lambda_n: 0.9000062236270613 Loss: 0.035449894998720505\n",
      "Iteration: 1940 lambda_n: 1.0028494352938848 Loss: 0.035432844035161865\n",
      "Iteration: 1941 lambda_n: 0.901860045723856 Loss: 0.03541385654985446\n",
      "Iteration: 1942 lambda_n: 0.9421132444627452 Loss: 0.03539679222833662\n",
      "Iteration: 1943 lambda_n: 0.9119732918403393 Loss: 0.03537897709430124\n",
      "Iteration: 1944 lambda_n: 0.9756095705328957 Loss: 0.03536174253151596\n",
      "Iteration: 1945 lambda_n: 0.9639754837885378 Loss: 0.035343316671104755\n",
      "Iteration: 1946 lambda_n: 0.9618452799790042 Loss: 0.035325122124513844\n",
      "Iteration: 1947 lambda_n: 0.9345519085705432 Loss: 0.0353069791795953\n",
      "Iteration: 1948 lambda_n: 0.9093575552298213 Loss: 0.03528936195397474\n",
      "Iteration: 1949 lambda_n: 0.9257085629335737 Loss: 0.03527222991587666\n",
      "Iteration: 1950 lambda_n: 1.0171836437098642 Loss: 0.035254800080408634\n",
      "Iteration: 1951 lambda_n: 0.9185455142475328 Loss: 0.03523565960917203\n",
      "Iteration: 1952 lambda_n: 0.9699595389642794 Loss: 0.03521838606375563\n",
      "Iteration: 1953 lambda_n: 0.9050150896589908 Loss: 0.035200156486275176\n",
      "Iteration: 1954 lambda_n: 0.983213813138304 Loss: 0.03518315767014\n",
      "Iteration: 1955 lambda_n: 0.935417850671795 Loss: 0.03516470087494432\n",
      "Iteration: 1956 lambda_n: 0.9098986121295691 Loss: 0.03514715193822096\n",
      "Iteration: 1957 lambda_n: 0.9273027164655882 Loss: 0.03513009162514095\n",
      "Iteration: 1958 lambda_n: 1.0035974189001957 Loss: 0.03511271488808664\n",
      "Iteration: 1959 lambda_n: 0.9628139228715062 Loss: 0.03509391957240191\n",
      "Iteration: 1960 lambda_n: 0.9373552754530533 Loss: 0.03507589905489481\n",
      "Iteration: 1961 lambda_n: 0.9916999408254721 Loss: 0.03505836531938487\n",
      "Iteration: 1962 lambda_n: 0.9191729815675551 Loss: 0.03503982590398374\n",
      "Iteration: 1963 lambda_n: 1.0028544482988848 Loss: 0.03502265247794093\n",
      "Iteration: 1964 lambda_n: 0.9545714345920441 Loss: 0.03500392639688044\n",
      "Iteration: 1965 lambda_n: 0.9843254109948676 Loss: 0.03498611253978425\n",
      "Iteration: 1966 lambda_n: 0.9772288016597425 Loss: 0.034967754134181024\n",
      "Iteration: 1967 lambda_n: 0.9853530529786791 Loss: 0.03494953885706177\n",
      "Iteration: 1968 lambda_n: 0.9934043869083709 Loss: 0.03493118294809539\n",
      "Iteration: 1969 lambda_n: 0.9103265957915591 Loss: 0.034912687990992175\n",
      "Iteration: 1970 lambda_n: 0.8962803535815723 Loss: 0.03489574948380885\n",
      "Iteration: 1971 lambda_n: 0.9408976602127256 Loss: 0.03487908130739937\n",
      "Iteration: 1972 lambda_n: 0.9423595719780777 Loss: 0.03486159284871322\n",
      "Iteration: 1973 lambda_n: 0.9345731339157749 Loss: 0.03484408696577416\n",
      "Iteration: 1974 lambda_n: 1.0223717191793995 Loss: 0.03482673534413869\n",
      "Iteration: 1975 lambda_n: 0.8819957384314032 Loss: 0.03480776442454039\n",
      "Iteration: 1976 lambda_n: 1.0135105087091372 Loss: 0.034791407593817505\n",
      "Iteration: 1977 lambda_n: 1.0026046212722493 Loss: 0.03477262203725511\n",
      "Iteration: 1978 lambda_n: 1.0023471225311271 Loss: 0.03475404956890052\n",
      "Iteration: 1979 lambda_n: 0.9636090530441359 Loss: 0.034735492705344734\n",
      "Iteration: 1980 lambda_n: 0.9134198734899622 Loss: 0.03471766324247658\n",
      "Iteration: 1981 lambda_n: 0.9093890374939602 Loss: 0.03470077166110342\n",
      "Iteration: 1982 lambda_n: 0.8773593034938203 Loss: 0.03468396348185621\n",
      "Iteration: 1983 lambda_n: 0.9154891894411133 Loss: 0.03466775569232019\n",
      "Iteration: 1984 lambda_n: 0.93670988383266 Loss: 0.03465085220037305\n",
      "Iteration: 1985 lambda_n: 0.9315608416539183 Loss: 0.0346335660706332\n",
      "Iteration: 1986 lambda_n: 0.8886266147263341 Loss: 0.03461638417337602\n",
      "Iteration: 1987 lambda_n: 1.020000738141023 Loss: 0.03460000274075282\n",
      "Iteration: 1988 lambda_n: 0.8860599494315651 Loss: 0.034581209594477275\n",
      "Iteration: 1989 lambda_n: 0.9808437369629643 Loss: 0.03456489325460813\n",
      "Iteration: 1990 lambda_n: 0.9477539449023226 Loss: 0.03454684102158857\n",
      "Iteration: 1991 lambda_n: 0.9322236241417245 Loss: 0.03452940739010989\n",
      "Iteration: 1992 lambda_n: 1.0244017581387264 Loss: 0.03451226859273212\n",
      "Iteration: 1993 lambda_n: 0.9289298100475698 Loss: 0.03449344545957448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1994 lambda_n: 0.8915720057327647 Loss: 0.03447638611328431\n",
      "Iteration: 1995 lambda_n: 1.0256708608799912 Loss: 0.03446002127140075\n",
      "Iteration: 1996 lambda_n: 1.0059610020350889 Loss: 0.03444120509082195\n",
      "Iteration: 1997 lambda_n: 0.9644241423862085 Loss: 0.03442276106896806\n",
      "Iteration: 1998 lambda_n: 0.8770849354257173 Loss: 0.0344050884470728\n",
      "Iteration: 1999 lambda_n: 0.9564899887908606 Loss: 0.034389024652545286\n",
      "Iteration: 2000 lambda_n: 0.9058140781731562 Loss: 0.03437151550008353\n",
      "Iteration: 2001 lambda_n: 0.9889216612231227 Loss: 0.03435494269198131\n",
      "Iteration: 2002 lambda_n: 0.9143570839272535 Loss: 0.03433685887140303\n",
      "Iteration: 2003 lambda_n: 0.9313357516050261 Loss: 0.034320147521025274\n",
      "Iteration: 2004 lambda_n: 0.8817159442321829 Loss: 0.03430313461960987\n",
      "Iteration: 2005 lambda_n: 0.9952697882868682 Loss: 0.03428703631510519\n",
      "Iteration: 2006 lambda_n: 0.9118922610512764 Loss: 0.03426887417507253\n",
      "Iteration: 2007 lambda_n: 1.0031539235241045 Loss: 0.03425224244401162\n",
      "Iteration: 2008 lambda_n: 0.9844862853703555 Loss: 0.03423395590306748\n",
      "Iteration: 2009 lambda_n: 1.0045644975953887 Loss: 0.03421601958979803\n",
      "Iteration: 2010 lambda_n: 0.9752822631612906 Loss: 0.034197727575298036\n",
      "Iteration: 2011 lambda_n: 0.9274781594020296 Loss: 0.03417997854243906\n",
      "Iteration: 2012 lambda_n: 0.9658082180770516 Loss: 0.034163108433268384\n",
      "Iteration: 2013 lambda_n: 0.9083975539067449 Loss: 0.034145550329491736\n",
      "Iteration: 2014 lambda_n: 0.9312803323853341 Loss: 0.0341290445481438\n",
      "Iteration: 2015 lambda_n: 0.9124283594757749 Loss: 0.03411213159215497\n",
      "Iteration: 2016 lambda_n: 0.9764074837024957 Loss: 0.03409556947739743\n",
      "Iteration: 2017 lambda_n: 1.0236022264041424 Loss: 0.03407785526113333\n",
      "Iteration: 2018 lambda_n: 0.9904403794276587 Loss: 0.034059295075073784\n",
      "Iteration: 2019 lambda_n: 0.933142306118837 Loss: 0.03404134621289043\n",
      "Iteration: 2020 lambda_n: 0.9429933488222192 Loss: 0.03402444473896097\n",
      "Iteration: 2021 lambda_n: 0.905280313902664 Loss: 0.03400737369159157\n",
      "Iteration: 2022 lambda_n: 0.8937233396123441 Loss: 0.033990993755659525\n",
      "Iteration: 2023 lambda_n: 0.9866351956251221 Loss: 0.03397483097300156\n",
      "Iteration: 2024 lambda_n: 0.9774497192049627 Loss: 0.03395699712001441\n",
      "Iteration: 2025 lambda_n: 0.9739243770521917 Loss: 0.03393933889047919\n",
      "Iteration: 2026 lambda_n: 1.000757300256378 Loss: 0.03392175383498321\n",
      "Iteration: 2027 lambda_n: 0.9706773251063097 Loss: 0.0339036941305286\n",
      "Iteration: 2028 lambda_n: 0.9207553504717287 Loss: 0.03388618680505139\n",
      "Iteration: 2029 lambda_n: 0.9562895245427493 Loss: 0.03386958858188802\n",
      "Iteration: 2030 lambda_n: 0.9900698251243493 Loss: 0.0338523587153256\n",
      "Iteration: 2031 lambda_n: 1.0154068502724538 Loss: 0.033834529791760236\n",
      "Iteration: 2032 lambda_n: 1.0056587598769227 Loss: 0.03381625472698971\n",
      "Iteration: 2033 lambda_n: 0.923218219435672 Loss: 0.03379816522144321\n",
      "Iteration: 2034 lambda_n: 1.021464678584946 Loss: 0.03378156752258751\n",
      "Iteration: 2035 lambda_n: 0.9613756053308087 Loss: 0.033763213369781477\n",
      "Iteration: 2036 lambda_n: 0.997982300289866 Loss: 0.033745948424661214\n",
      "Iteration: 2037 lambda_n: 1.026722239394992 Loss: 0.03372803577495199\n",
      "Iteration: 2038 lambda_n: 0.9519043000327708 Loss: 0.03370961759065179\n",
      "Iteration: 2039 lambda_n: 0.9745334022673994 Loss: 0.03369255093616623\n",
      "Iteration: 2040 lambda_n: 0.9212450704666116 Loss: 0.03367508787930795\n",
      "Iteration: 2041 lambda_n: 1.006355262634885 Loss: 0.03365858840888371\n",
      "Iteration: 2042 lambda_n: 0.8937784009111627 Loss: 0.03364057421541751\n",
      "Iteration: 2043 lambda_n: 0.9044058900971357 Loss: 0.03362458366659978\n",
      "Iteration: 2044 lambda_n: 0.9948980688731935 Loss: 0.03360841105148822\n",
      "Iteration: 2045 lambda_n: 0.8773151749768666 Loss: 0.033590629600209636\n",
      "Iteration: 2046 lambda_n: 0.9938894575824817 Loss: 0.03357495785742284\n",
      "Iteration: 2047 lambda_n: 0.991557437399939 Loss: 0.0335572129058329\n",
      "Iteration: 2048 lambda_n: 0.9884094409159613 Loss: 0.033539519367214325\n",
      "Iteration: 2049 lambda_n: 0.8916629945653768 Loss: 0.03352189172241723\n",
      "Iteration: 2050 lambda_n: 0.9706800865616942 Loss: 0.03350599784338932\n",
      "Iteration: 2051 lambda_n: 0.9854038775283626 Loss: 0.03348870444045629\n",
      "Iteration: 2052 lambda_n: 1.0126347068984274 Loss: 0.03347115829464763\n",
      "Iteration: 2053 lambda_n: 0.9899707934511963 Loss: 0.03345313732225057\n",
      "Iteration: 2054 lambda_n: 0.8804937185948086 Loss: 0.03343552954212302\n",
      "Iteration: 2055 lambda_n: 0.8736434475357366 Loss: 0.033419877157097606\n",
      "Iteration: 2056 lambda_n: 0.9280434573947868 Loss: 0.033404354171126666\n",
      "Iteration: 2057 lambda_n: 0.9615912489745854 Loss: 0.033387872903111376\n",
      "Iteration: 2058 lambda_n: 0.9653993425710014 Loss: 0.03337080488777419\n",
      "Iteration: 2059 lambda_n: 0.9043903655236133 Loss: 0.03335367853980006\n",
      "Iteration: 2060 lambda_n: 1.0089987579869502 Loss: 0.033337642937996924\n",
      "Iteration: 2061 lambda_n: 0.8846615224527429 Loss: 0.03331976213073839\n",
      "Iteration: 2062 lambda_n: 0.8758714599134295 Loss: 0.033304093124509264\n",
      "Iteration: 2063 lambda_n: 0.8842109828946272 Loss: 0.03328858749659082\n",
      "Iteration: 2064 lambda_n: 0.9968391813017863 Loss: 0.03327294199377486\n",
      "Iteration: 2065 lambda_n: 0.9996474333889148 Loss: 0.033255312941446555\n",
      "Iteration: 2066 lambda_n: 0.9115339591472639 Loss: 0.03323764418957483\n",
      "Iteration: 2067 lambda_n: 1.01478815324396 Loss: 0.03322154156404381\n",
      "Iteration: 2068 lambda_n: 0.9843378268176756 Loss: 0.03320362465540787\n",
      "Iteration: 2069 lambda_n: 0.9233800063566119 Loss: 0.0331862552201127\n",
      "Iteration: 2070 lambda_n: 0.880130195093796 Loss: 0.033169970261917356\n",
      "Iteration: 2071 lambda_n: 0.9157338136458439 Loss: 0.03315445602320726\n",
      "Iteration: 2072 lambda_n: 1.0065728338908808 Loss: 0.03313832242236717\n",
      "Iteration: 2073 lambda_n: 0.9283220847912336 Loss: 0.03312059807817258\n",
      "Iteration: 2074 lambda_n: 0.9944746018298461 Loss: 0.033104260645014115\n",
      "Iteration: 2075 lambda_n: 0.9574078622357368 Loss: 0.033086768584310915\n",
      "Iteration: 2076 lambda_n: 0.9635134666113676 Loss: 0.033069937892217224\n",
      "Iteration: 2077 lambda_n: 0.9631497543890115 Loss: 0.03305300916738274\n",
      "Iteration: 2078 lambda_n: 0.9308730444623458 Loss: 0.03303609616533272\n",
      "Iteration: 2079 lambda_n: 0.9244609043770057 Loss: 0.03301975882422773\n",
      "Iteration: 2080 lambda_n: 0.9670696785336369 Loss: 0.033003542658792955\n",
      "Iteration: 2081 lambda_n: 1.0204028047753102 Loss: 0.03298658829526572\n",
      "Iteration: 2082 lambda_n: 0.9608620001101125 Loss: 0.03296870913128126\n",
      "Iteration: 2083 lambda_n: 0.8980082752553866 Loss: 0.03295188284190536\n",
      "Iteration: 2084 lambda_n: 1.0166132992509138 Loss: 0.03293616566822204\n",
      "Iteration: 2085 lambda_n: 0.9232087955790339 Loss: 0.03291838245869159\n",
      "Iteration: 2086 lambda_n: 0.9495587161911659 Loss: 0.0329022422129899\n",
      "Iteration: 2087 lambda_n: 0.8892551305049835 Loss: 0.03288565029897196\n",
      "Iteration: 2088 lambda_n: 1.0058873911741804 Loss: 0.03287012038283879\n",
      "Iteration: 2089 lambda_n: 0.9664983369788996 Loss: 0.03285256325650746\n",
      "Iteration: 2090 lambda_n: 0.962355296094124 Loss: 0.03283570332533596\n",
      "Iteration: 2091 lambda_n: 1.0129352690687357 Loss: 0.03281892509981966\n",
      "Iteration: 2092 lambda_n: 0.8920032837134272 Loss: 0.03280127520448322\n",
      "Iteration: 2093 lambda_n: 0.9455672953115487 Loss: 0.03278574115501776\n",
      "Iteration: 2094 lambda_n: 0.9633723728938958 Loss: 0.03276928314538088\n",
      "Iteration: 2095 lambda_n: 1.0245405642792862 Loss: 0.03275252460701045\n",
      "Iteration: 2096 lambda_n: 0.9359775596184383 Loss: 0.03273471239349804\n",
      "Iteration: 2097 lambda_n: 0.9675428687115781 Loss: 0.03271844928530226\n",
      "Iteration: 2098 lambda_n: 0.989153351855457 Loss: 0.032701647128993865\n",
      "Iteration: 2099 lambda_n: 0.9036857727812936 Loss: 0.03268447959582337\n",
      "Iteration: 2100 lambda_n: 1.0265797017042628 Loss: 0.03266880419544905\n",
      "Iteration: 2101 lambda_n: 0.966187585935018 Loss: 0.03265100721756148\n",
      "Iteration: 2102 lambda_n: 1.0186649525311464 Loss: 0.03263426710315944\n",
      "Iteration: 2103 lambda_n: 0.9158531158488805 Loss: 0.032616628156301265\n",
      "Iteration: 2104 lambda_n: 0.9548761902826096 Loss: 0.03260077860169915\n",
      "Iteration: 2105 lambda_n: 0.9778279365940964 Loss: 0.03258426291972927\n",
      "Iteration: 2106 lambda_n: 0.9532715156691672 Loss: 0.032567360003691904\n",
      "Iteration: 2107 lambda_n: 0.9682485948657318 Loss: 0.032550891079819996\n",
      "Iteration: 2108 lambda_n: 0.9450021699170462 Loss: 0.03253417302157758\n",
      "Iteration: 2109 lambda_n: 1.0239827182035395 Loss: 0.03251786569829497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2110 lambda_n: 1.0000513262605828 Loss: 0.03250020588957691\n",
      "Iteration: 2111 lambda_n: 0.9242737636450225 Loss: 0.03248296930362357\n",
      "Iteration: 2112 lambda_n: 0.8972941751482001 Loss: 0.03246704803598269\n",
      "Iteration: 2113 lambda_n: 0.9159778472824126 Loss: 0.0324516000060719\n",
      "Iteration: 2114 lambda_n: 0.9795814941192023 Loss: 0.0324358389514775\n",
      "Iteration: 2115 lambda_n: 0.9121932874037916 Loss: 0.032418993144787064\n",
      "Iteration: 2116 lambda_n: 0.9423748429226673 Loss: 0.03240331521094315\n",
      "Iteration: 2117 lambda_n: 0.8859017606723479 Loss: 0.03238712766313214\n",
      "Iteration: 2118 lambda_n: 0.9067958002963324 Loss: 0.03237191864251906\n",
      "Iteration: 2119 lambda_n: 0.9016489483445351 Loss: 0.03235635941727795\n",
      "Iteration: 2120 lambda_n: 1.0062758955850226 Loss: 0.0323408970423584\n",
      "Iteration: 2121 lambda_n: 0.9262434239890478 Loss: 0.03232365047607716\n",
      "Iteration: 2122 lambda_n: 0.8998455539451676 Loss: 0.032307784986837146\n",
      "Iteration: 2123 lambda_n: 1.008755791291498 Loss: 0.0322923802985462\n",
      "Iteration: 2124 lambda_n: 0.8995355758828079 Loss: 0.032275121267511225\n",
      "Iteration: 2125 lambda_n: 0.9855941072410838 Loss: 0.03225973995780931\n",
      "Iteration: 2126 lambda_n: 0.8877361951823513 Loss: 0.032242896909735565\n",
      "Iteration: 2127 lambda_n: 0.9270859555743276 Loss: 0.03222773496394515\n",
      "Iteration: 2128 lambda_n: 0.9199597547170886 Loss: 0.032211909840338566\n",
      "Iteration: 2129 lambda_n: 0.9483015855851845 Loss: 0.03219621535099688\n",
      "Iteration: 2130 lambda_n: 0.8859190951147613 Loss: 0.03218004673499888\n",
      "Iteration: 2131 lambda_n: 0.9465540618290966 Loss: 0.03216495036789895\n",
      "Iteration: 2132 lambda_n: 0.953822782210238 Loss: 0.03214882996746027\n",
      "Iteration: 2133 lambda_n: 0.9473814056518 Loss: 0.03213259541326155\n",
      "Iteration: 2134 lambda_n: 0.9269789504972087 Loss: 0.032116480083387446\n",
      "Iteration: 2135 lambda_n: 0.9192320752509346 Loss: 0.03210072106925734\n",
      "Iteration: 2136 lambda_n: 0.8846748619045761 Loss: 0.032085102812096596\n",
      "Iteration: 2137 lambda_n: 0.9959381665375819 Loss: 0.032070080229260944\n",
      "Iteration: 2138 lambda_n: 1.0079967312581728 Loss: 0.03205317830781151\n",
      "Iteration: 2139 lambda_n: 0.8800196975266612 Loss: 0.0320360825614571\n",
      "Iteration: 2140 lambda_n: 0.8998672482060872 Loss: 0.032021166238151086\n",
      "Iteration: 2141 lambda_n: 0.9705991883753595 Loss: 0.03200592209895351\n",
      "Iteration: 2142 lambda_n: 1.0184468115639373 Loss: 0.0319894894877649\n",
      "Iteration: 2143 lambda_n: 0.9279720974287928 Loss: 0.031972257701702984\n",
      "Iteration: 2144 lambda_n: 0.9964996315886 Loss: 0.031956566459098994\n",
      "Iteration: 2145 lambda_n: 1.0156737471387018 Loss: 0.031939726820983505\n",
      "Iteration: 2146 lambda_n: 0.8936734553387281 Loss: 0.031922574209289324\n",
      "Iteration: 2147 lambda_n: 0.9842612341566153 Loss: 0.03190749116825978\n",
      "Iteration: 2148 lambda_n: 0.9045928762612921 Loss: 0.031890889238499455\n",
      "Iteration: 2149 lambda_n: 0.969283049398185 Loss: 0.03187564038185449\n",
      "Iteration: 2150 lambda_n: 1.0070044385872452 Loss: 0.031859310899784936\n",
      "Iteration: 2151 lambda_n: 0.893470730673 Loss: 0.03184235675308655\n",
      "Iteration: 2152 lambda_n: 0.925840135596422 Loss: 0.03182732333964783\n",
      "Iteration: 2153 lambda_n: 0.975268526503342 Loss: 0.03181175447064058\n",
      "Iteration: 2154 lambda_n: 0.9470543229668156 Loss: 0.0317953645428089\n",
      "Iteration: 2155 lambda_n: 0.8968118096969984 Loss: 0.031779458728994694\n",
      "Iteration: 2156 lambda_n: 0.9477492659954121 Loss: 0.03176440579823596\n",
      "Iteration: 2157 lambda_n: 0.9038864910931033 Loss: 0.031748507470804974\n",
      "Iteration: 2158 lambda_n: 0.9095080938989764 Loss: 0.031733354125413414\n",
      "Iteration: 2159 lambda_n: 0.9254144925310037 Loss: 0.03171811560392921\n",
      "Iteration: 2160 lambda_n: 1.0214611229680648 Loss: 0.03170261992385287\n",
      "Iteration: 2161 lambda_n: 0.9793992596917709 Loss: 0.03168552694325513\n",
      "Iteration: 2162 lambda_n: 0.9210524146451238 Loss: 0.03166914863949112\n",
      "Iteration: 2163 lambda_n: 0.9515330421938134 Loss: 0.03165375573715033\n",
      "Iteration: 2164 lambda_n: 1.0092577135823773 Loss: 0.0316378632965612\n",
      "Iteration: 2165 lambda_n: 0.8768440763459807 Loss: 0.03162101771110084\n",
      "Iteration: 2166 lambda_n: 0.9138495613303339 Loss: 0.031606391433810395\n",
      "Iteration: 2167 lambda_n: 0.8970515707462142 Loss: 0.031591156978339786\n",
      "Iteration: 2168 lambda_n: 1.0200980112965736 Loss: 0.031576211597843136\n",
      "Iteration: 2169 lambda_n: 0.9152191144364853 Loss: 0.03155922709146404\n",
      "Iteration: 2170 lambda_n: 0.9700927342226529 Loss: 0.03154399869351617\n",
      "Iteration: 2171 lambda_n: 1.0006863828175065 Loss: 0.03152786746977955\n",
      "Iteration: 2172 lambda_n: 1.0161836916063232 Loss: 0.03151123855523301\n",
      "Iteration: 2173 lambda_n: 0.953280493293087 Loss: 0.03149436360100472\n",
      "Iteration: 2174 lambda_n: 1.0210664199743633 Loss: 0.0314785437696345\n",
      "Iteration: 2175 lambda_n: 0.914023792608281 Loss: 0.031461610351348215\n",
      "Iteration: 2176 lambda_n: 1.0100707309420724 Loss: 0.031446462091933525\n",
      "Iteration: 2177 lambda_n: 1.000483038022842 Loss: 0.03142973298559289\n",
      "Iteration: 2178 lambda_n: 0.9235306237479702 Loss: 0.031413174029714615\n",
      "Iteration: 2179 lambda_n: 1.0001635081741682 Loss: 0.03139789875728149\n",
      "Iteration: 2180 lambda_n: 1.0110873133904423 Loss: 0.03138136686244292\n",
      "Iteration: 2181 lambda_n: 0.9745409611115637 Loss: 0.03136466593583544\n",
      "Iteration: 2182 lambda_n: 0.9024179746565574 Loss: 0.03134857966268532\n",
      "Iteration: 2183 lambda_n: 0.9603591957011265 Loss: 0.03133369352121227\n",
      "Iteration: 2184 lambda_n: 0.9326189424900793 Loss: 0.03131786177672189\n",
      "Iteration: 2185 lambda_n: 1.0048750888606974 Loss: 0.03130249740413457\n",
      "Iteration: 2186 lambda_n: 0.9400963938542997 Loss: 0.03128595376918095\n",
      "Iteration: 2187 lambda_n: 0.8980495735524636 Loss: 0.03127048706743763\n",
      "Iteration: 2188 lambda_n: 0.9409446511511721 Loss: 0.03125572158451658\n",
      "Iteration: 2189 lambda_n: 0.9888534665575262 Loss: 0.031240260755406476\n",
      "Iteration: 2190 lambda_n: 0.9387352101059303 Loss: 0.031224023686471744\n",
      "Iteration: 2191 lambda_n: 0.8896193117001675 Loss: 0.03120861997129012\n",
      "Iteration: 2192 lambda_n: 0.9854534931857586 Loss: 0.031194031567430076\n",
      "Iteration: 2193 lambda_n: 0.9507290894219744 Loss: 0.031177882284564373\n",
      "Iteration: 2194 lambda_n: 0.9701935865559494 Loss: 0.03116231268550592\n",
      "Iteration: 2195 lambda_n: 0.935584358809828 Loss: 0.03114643510563625\n",
      "Iteration: 2196 lambda_n: 0.9911632777720084 Loss: 0.03113113424672483\n",
      "Iteration: 2197 lambda_n: 0.939206989407223 Loss: 0.03111493550889782\n",
      "Iteration: 2198 lambda_n: 0.911980908970301 Loss: 0.031099596433348253\n",
      "Iteration: 2199 lambda_n: 1.0032726909891485 Loss: 0.031084711834084133\n",
      "Iteration: 2200 lambda_n: 0.9851352298384725 Loss: 0.031068348439475132\n",
      "Iteration: 2201 lambda_n: 0.9341263602508602 Loss: 0.031052292296002718\n",
      "Iteration: 2202 lambda_n: 0.9293080618502748 Loss: 0.031037077992166373\n",
      "Iteration: 2203 lambda_n: 0.9544774104404714 Loss: 0.03102195229789407\n",
      "Iteration: 2204 lambda_n: 0.9146579859055535 Loss: 0.03100642747574291\n",
      "Iteration: 2205 lambda_n: 0.9703593365867466 Loss: 0.03099156035929066\n",
      "Iteration: 2206 lambda_n: 0.8836491375935531 Loss: 0.03097579860569082\n",
      "Iteration: 2207 lambda_n: 0.9505862713002624 Loss: 0.03096145494583037\n",
      "Iteration: 2208 lambda_n: 0.9403904763362325 Loss: 0.03094603501727\n",
      "Iteration: 2209 lambda_n: 0.9271482277804809 Loss: 0.030930790974237777\n",
      "Iteration: 2210 lambda_n: 0.8825730703571372 Loss: 0.03091577182562166\n",
      "Iteration: 2211 lambda_n: 0.9700805622388546 Loss: 0.030901484216090177\n",
      "Iteration: 2212 lambda_n: 0.9777382802306569 Loss: 0.03088579063529663\n",
      "Iteration: 2213 lambda_n: 0.9202614367053904 Loss: 0.030869984474373665\n",
      "Iteration: 2214 lambda_n: 0.9920461651575017 Loss: 0.03085511786886942\n",
      "Iteration: 2215 lambda_n: 1.0120742547448405 Loss: 0.03083910289075802\n",
      "Iteration: 2216 lambda_n: 1.0012595772537929 Loss: 0.030822776683619543\n",
      "Iteration: 2217 lambda_n: 0.9749256486019163 Loss: 0.03080663696908928\n",
      "Iteration: 2218 lambda_n: 0.9702639531492423 Loss: 0.030790933263147535\n",
      "Iteration: 2219 lambda_n: 1.0004263258197625 Loss: 0.030775315949120315\n",
      "Iteration: 2220 lambda_n: 1.0004980324969803 Loss: 0.030759224968400382\n",
      "Iteration: 2221 lambda_n: 1.014750211561643 Loss: 0.030743144860127997\n",
      "Iteration: 2222 lambda_n: 0.9913269667336194 Loss: 0.030726847992754918\n",
      "Iteration: 2223 lambda_n: 0.967566704167352 Loss: 0.030710939285403506\n",
      "Iteration: 2224 lambda_n: 0.9858455898959707 Loss: 0.030695423318469455\n",
      "Iteration: 2225 lambda_n: 0.8918808926947361 Loss: 0.030679625869401615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2226 lambda_n: 0.9591638429666123 Loss: 0.03066534427211561\n",
      "Iteration: 2227 lambda_n: 0.9221393544523433 Loss: 0.030649996042094903\n",
      "Iteration: 2228 lambda_n: 0.9759832419310198 Loss: 0.030635250796907543\n",
      "Iteration: 2229 lambda_n: 0.882824464181433 Loss: 0.030619655836191983\n",
      "Iteration: 2230 lambda_n: 0.8736241160427716 Loss: 0.030605559423542945\n",
      "Iteration: 2231 lambda_n: 0.8766344131353749 Loss: 0.030591619271472276\n",
      "Iteration: 2232 lambda_n: 0.9516499945468739 Loss: 0.03057764045179038\n",
      "Iteration: 2233 lambda_n: 0.9655929053941441 Loss: 0.030562476069534463\n",
      "Iteration: 2234 lambda_n: 0.8975575112277336 Loss: 0.03054710084304215\n",
      "Iteration: 2235 lambda_n: 0.9007741537293591 Loss: 0.03053281920312744\n",
      "Iteration: 2236 lambda_n: 0.9474727101640233 Loss: 0.030518496326702168\n",
      "Iteration: 2237 lambda_n: 0.9283612622794684 Loss: 0.03050344168076613\n",
      "Iteration: 2238 lambda_n: 0.8975563707344917 Loss: 0.030488701424152373\n",
      "Iteration: 2239 lambda_n: 0.9208307066470686 Loss: 0.030474460384059873\n",
      "Iteration: 2240 lambda_n: 0.9541819943894545 Loss: 0.030459860401948046\n",
      "Iteration: 2241 lambda_n: 1.0270712148643786 Loss: 0.030444742690720974\n",
      "Iteration: 2242 lambda_n: 0.9204669802128201 Loss: 0.030428482751344008\n",
      "Iteration: 2243 lambda_n: 0.9450964751250347 Loss: 0.03041392162048978\n",
      "Iteration: 2244 lambda_n: 0.9028110425879147 Loss: 0.030398981820277962\n",
      "Iteration: 2245 lambda_n: 0.9375679440169321 Loss: 0.03038472083272169\n",
      "Iteration: 2246 lambda_n: 1.007900869479552 Loss: 0.030369921566651425\n",
      "Iteration: 2247 lambda_n: 0.9715643764306058 Loss: 0.030354024346515764\n",
      "Iteration: 2248 lambda_n: 0.9696802815436104 Loss: 0.030338712263109417\n",
      "Iteration: 2249 lambda_n: 0.9084725795880245 Loss: 0.030323441652371677\n",
      "Iteration: 2250 lambda_n: 0.9068959580626516 Loss: 0.030309145638130745\n",
      "Iteration: 2251 lambda_n: 0.9831500534061707 Loss: 0.03029488476600113\n",
      "Iteration: 2252 lambda_n: 0.9253828271411028 Loss: 0.03027943648243785\n",
      "Iteration: 2253 lambda_n: 0.9245785330874982 Loss: 0.03026490701356427\n",
      "Iteration: 2254 lambda_n: 0.8831377286124115 Loss: 0.030250400953968647\n",
      "Iteration: 2255 lambda_n: 1.0019166139075746 Loss: 0.030236555151410608\n",
      "Iteration: 2256 lambda_n: 0.9167372841930849 Loss: 0.03022085907683436\n",
      "Iteration: 2257 lambda_n: 0.9163133747401734 Loss: 0.030206508559874768\n",
      "Iteration: 2258 lambda_n: 0.9425959640333922 Loss: 0.03019217532712039\n",
      "Iteration: 2259 lambda_n: 0.8927528723133258 Loss: 0.03017744209909054\n",
      "Iteration: 2260 lambda_n: 0.878022986409867 Loss: 0.030163498359630076\n",
      "Iteration: 2261 lambda_n: 0.9593385890813222 Loss: 0.030149794582280173\n",
      "Iteration: 2262 lambda_n: 0.9669227196589741 Loss: 0.03013483290716117\n",
      "Iteration: 2263 lambda_n: 1.0016244093063462 Loss: 0.030119764843644335\n",
      "Iteration: 2264 lambda_n: 0.8948570846864036 Loss: 0.03010416861435252\n",
      "Iteration: 2265 lambda_n: 0.9283191290450971 Loss: 0.03009024571998576\n",
      "Iteration: 2266 lambda_n: 0.9188059688560581 Loss: 0.0300758130516298\n",
      "Iteration: 2267 lambda_n: 0.9137573526665769 Loss: 0.030061539185376455\n",
      "Iteration: 2268 lambda_n: 0.9331251688753207 Loss: 0.03004735452067273\n",
      "Iteration: 2269 lambda_n: 0.9845039814620186 Loss: 0.030032880301026677\n",
      "Iteration: 2270 lambda_n: 0.8999906399785915 Loss: 0.03001762129443966\n",
      "Iteration: 2271 lambda_n: 0.915206350495899 Loss: 0.030003683131123587\n",
      "Iteration: 2272 lambda_n: 0.9994879872068471 Loss: 0.02998952006920547\n",
      "Iteration: 2273 lambda_n: 1.0052522931811414 Loss: 0.02997406512531746\n",
      "Iteration: 2274 lambda_n: 0.8762851774923609 Loss: 0.029958534124735536\n",
      "Iteration: 2275 lambda_n: 1.0236472595693877 Loss: 0.029945006360262382\n",
      "Iteration: 2276 lambda_n: 0.8945432685434355 Loss: 0.02992921633270901\n",
      "Iteration: 2277 lambda_n: 0.9989469644061583 Loss: 0.029915428950335415\n",
      "Iteration: 2278 lambda_n: 0.982678521747329 Loss: 0.029900044765383556\n",
      "Iteration: 2279 lambda_n: 0.9067810579562117 Loss: 0.029884923850052973\n",
      "Iteration: 2280 lambda_n: 0.9436684464505822 Loss: 0.029870982019023277\n",
      "Iteration: 2281 lambda_n: 0.8992759678868443 Loss: 0.02985648449018177\n",
      "Iteration: 2282 lambda_n: 1.002783999343181 Loss: 0.0298426798414981\n",
      "Iteration: 2283 lambda_n: 1.0038655137043275 Loss: 0.029827298798686112\n",
      "Iteration: 2284 lambda_n: 0.8889646699296079 Loss: 0.02981191443222133\n",
      "Iteration: 2285 lambda_n: 0.9429026333322313 Loss: 0.029798302032024365\n",
      "Iteration: 2286 lambda_n: 1.0268251642034443 Loss: 0.029783875107223728\n",
      "Iteration: 2287 lambda_n: 1.027666112367609 Loss: 0.029768177500468455\n",
      "Iteration: 2288 lambda_n: 0.8924094871040309 Loss: 0.029752481024399348\n",
      "Iteration: 2289 lambda_n: 0.9411328228940705 Loss: 0.029738861812937643\n",
      "Iteration: 2290 lambda_n: 0.8946049864243532 Loss: 0.029724510492530987\n",
      "Iteration: 2291 lambda_n: 1.014818594032937 Loss: 0.02971087960142462\n",
      "Iteration: 2292 lambda_n: 0.890091772854613 Loss: 0.02969542995629516\n",
      "Iteration: 2293 lambda_n: 1.0017738655372126 Loss: 0.029681890472015682\n",
      "Iteration: 2294 lambda_n: 0.905252652489975 Loss: 0.029666664821117697\n",
      "Iteration: 2295 lambda_n: 0.9945582508643755 Loss: 0.029652917719911638\n",
      "Iteration: 2296 lambda_n: 0.9865712791350627 Loss: 0.029637827096105296\n",
      "Iteration: 2297 lambda_n: 0.930286139353137 Loss: 0.02962287078070709\n",
      "Iteration: 2298 lambda_n: 1.0089539478773926 Loss: 0.029608779730271394\n",
      "Iteration: 2299 lambda_n: 0.9700947409929735 Loss: 0.029593510272740698\n",
      "Iteration: 2300 lambda_n: 0.8881201236582841 Loss: 0.02957884184485922\n",
      "Iteration: 2301 lambda_n: 0.9241419953165977 Loss: 0.02956542400676028\n",
      "Iteration: 2302 lambda_n: 1.0188934507876288 Loss: 0.029551473302320417\n",
      "Iteration: 2303 lambda_n: 1.0053018515559282 Loss: 0.02953610567480201\n",
      "Iteration: 2304 lambda_n: 0.9344173793208387 Loss: 0.0295209568614033\n",
      "Iteration: 2305 lambda_n: 1.0275908793584774 Loss: 0.029506888519878288\n",
      "Iteration: 2306 lambda_n: 0.9589107329890231 Loss: 0.029491431178966687\n",
      "Iteration: 2307 lambda_n: 0.8797762208426417 Loss: 0.029477019894323237\n",
      "Iteration: 2308 lambda_n: 1.0245335916283573 Loss: 0.029463808930548726\n",
      "Iteration: 2309 lambda_n: 0.962761602931886 Loss: 0.02944843757532746\n",
      "Iteration: 2310 lambda_n: 0.9456536075390191 Loss: 0.029434006090072285\n",
      "Iteration: 2311 lambda_n: 0.9848472149621474 Loss: 0.02941984341506568\n",
      "Iteration: 2312 lambda_n: 0.9467085917620189 Loss: 0.029405106803004618\n",
      "Iteration: 2313 lambda_n: 0.9539561091199541 Loss: 0.029390953442559063\n",
      "Iteration: 2314 lambda_n: 0.9997215811994027 Loss: 0.02937670421432478\n",
      "Iteration: 2315 lambda_n: 0.9454706988383138 Loss: 0.029361784855406334\n",
      "Iteration: 2316 lambda_n: 0.8840889783744577 Loss: 0.029347687809887925\n",
      "Iteration: 2317 lambda_n: 0.9101711913774287 Loss: 0.02933451715627078\n",
      "Iteration: 2318 lambda_n: 0.9279349124932587 Loss: 0.029320969254498903\n",
      "Iteration: 2319 lambda_n: 0.9502201426706045 Loss: 0.029307168768396438\n",
      "Iteration: 2320 lambda_n: 1.0092136978980226 Loss: 0.02929304924508885\n",
      "Iteration: 2321 lambda_n: 1.0003808025910117 Loss: 0.029278066878246665\n",
      "Iteration: 2322 lambda_n: 0.9831080999933166 Loss: 0.029263229644431135\n",
      "Iteration: 2323 lambda_n: 0.9538963363717256 Loss: 0.029248662195914467\n",
      "Iteration: 2324 lambda_n: 0.9858711763514045 Loss: 0.029234540509419517\n",
      "Iteration: 2325 lambda_n: 1.0026806253691374 Loss: 0.029219958842213463\n",
      "Iteration: 2326 lambda_n: 1.0154799729726354 Loss: 0.029205142525483598\n",
      "Iteration: 2327 lambda_n: 0.9377368775736772 Loss: 0.029190151460675516\n",
      "Iteration: 2328 lambda_n: 0.9490277529634075 Loss: 0.02917632095570066\n",
      "Iteration: 2329 lambda_n: 0.9401010710280872 Loss: 0.02916233652899428\n",
      "Iteration: 2330 lambda_n: 0.9676279592356019 Loss: 0.029148496161923262\n",
      "Iteration: 2331 lambda_n: 0.9660693246790207 Loss: 0.029134263571184676\n",
      "Iteration: 2332 lambda_n: 0.9047846467753716 Loss: 0.029120067114257728\n",
      "Iteration: 2333 lambda_n: 0.9948113503606154 Loss: 0.02910678322523259\n",
      "Iteration: 2334 lambda_n: 0.8762713248662464 Loss: 0.02909219098100789\n",
      "Iteration: 2335 lambda_n: 0.9436782181524022 Loss: 0.02907934916571513\n",
      "Iteration: 2336 lambda_n: 0.8998115968731738 Loss: 0.029065531711673403\n",
      "Iteration: 2337 lambda_n: 0.8774228248775782 Loss: 0.029052368370384423\n",
      "Iteration: 2338 lambda_n: 0.8865809676094653 Loss: 0.029039543674253567\n",
      "Iteration: 2339 lambda_n: 0.9526559499265246 Loss: 0.02902659628739323\n",
      "Iteration: 2340 lambda_n: 0.9103554932795808 Loss: 0.02901269648897608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2341 lambda_n: 0.9514468895108851 Loss: 0.028999426022621694\n",
      "Iteration: 2342 lambda_n: 0.9783080450318651 Loss: 0.028985569260097255\n",
      "Iteration: 2343 lambda_n: 0.8909761230885418 Loss: 0.028971334852362387\n",
      "Iteration: 2344 lambda_n: 0.9124271058754804 Loss: 0.028958383103088837\n",
      "Iteration: 2345 lambda_n: 0.9103515152827868 Loss: 0.02894513138009455\n",
      "Iteration: 2346 lambda_n: 0.9594152446579249 Loss: 0.028931921768796647\n",
      "Iteration: 2347 lambda_n: 0.9027750384967019 Loss: 0.028918013176023098\n",
      "Iteration: 2348 lambda_n: 1.0132627410693973 Loss: 0.028904937850818425\n",
      "Iteration: 2349 lambda_n: 0.9111780136763883 Loss: 0.028890276340860405\n",
      "Iteration: 2350 lambda_n: 0.949002974676776 Loss: 0.028877104673599573\n",
      "Iteration: 2351 lambda_n: 0.9646543704671402 Loss: 0.028863399044593407\n",
      "Iteration: 2352 lambda_n: 0.8915282659592149 Loss: 0.028849480803716968\n",
      "Iteration: 2353 lambda_n: 1.0037640074654754 Loss: 0.028836629695579014\n",
      "Iteration: 2354 lambda_n: 0.9856070449852793 Loss: 0.028822174621953946\n",
      "Iteration: 2355 lambda_n: 1.017620911414051 Loss: 0.028807995348026062\n",
      "Iteration: 2356 lambda_n: 0.9550739180311592 Loss: 0.0287933704268444\n",
      "Iteration: 2357 lambda_n: 0.9483056388674275 Loss: 0.028779658215755108\n",
      "Iteration: 2358 lambda_n: 0.9354305133782299 Loss: 0.02876605642361376\n",
      "Iteration: 2359 lambda_n: 0.8993212429342086 Loss: 0.028752652251775448\n",
      "Iteration: 2360 lambda_n: 0.9340983928476838 Loss: 0.028739777646990587\n",
      "Iteration: 2361 lambda_n: 0.9710819185017239 Loss: 0.02872641779640982\n",
      "Iteration: 2362 lambda_n: 0.945146368982733 Loss: 0.028712542645966066\n",
      "Iteration: 2363 lambda_n: 0.9796698637526601 Loss: 0.028699051456037322\n",
      "Iteration: 2364 lambda_n: 0.8903829313354675 Loss: 0.02868508142789972\n",
      "Iteration: 2365 lambda_n: 0.9760312252601289 Loss: 0.028672396965699162\n",
      "Iteration: 2366 lambda_n: 0.9205838880619408 Loss: 0.02865850587297642\n",
      "Iteration: 2367 lambda_n: 0.9076598840013117 Loss: 0.028645416895917732\n",
      "Iteration: 2368 lambda_n: 0.9708232002101582 Loss: 0.028632524025099547\n",
      "Iteration: 2369 lambda_n: 0.9891504093404225 Loss: 0.0286187475431736\n",
      "Iteration: 2370 lambda_n: 0.9211911229217826 Loss: 0.028604725459910958\n",
      "Iteration: 2371 lambda_n: 0.9712324103361325 Loss: 0.02859167991381475\n",
      "Iteration: 2372 lambda_n: 0.9400986896921388 Loss: 0.0285779394620152\n",
      "Iteration: 2373 lambda_n: 1.024398046261224 Loss: 0.028564652942466944\n",
      "Iteration: 2374 lambda_n: 0.9963358020710514 Loss: 0.02855019011978148\n",
      "Iteration: 2375 lambda_n: 0.9310990133754148 Loss: 0.028536138627402195\n",
      "Iteration: 2376 lambda_n: 0.9849939801228283 Loss: 0.028523020693247983\n",
      "Iteration: 2377 lambda_n: 0.9167456663358166 Loss: 0.028509157683832533\n",
      "Iteration: 2378 lambda_n: 0.9513638399986577 Loss: 0.028496268380149622\n",
      "Iteration: 2379 lambda_n: 0.9358574530165827 Loss: 0.028482905790480673\n",
      "Iteration: 2380 lambda_n: 0.9669755412922642 Loss: 0.028469774373953996\n",
      "Iteration: 2381 lambda_n: 0.9467312594808233 Loss: 0.028456220280241952\n",
      "Iteration: 2382 lambda_n: 0.91439465408202 Loss: 0.0284429637097124\n",
      "Iteration: 2383 lambda_n: 0.9652129326658668 Loss: 0.02843017287352807\n",
      "Iteration: 2384 lambda_n: 0.885456637736701 Loss: 0.028416684992661832\n",
      "Iteration: 2385 lambda_n: 0.9808471810903159 Loss: 0.028404324122501986\n",
      "Iteration: 2386 lambda_n: 1.0131643416787044 Loss: 0.028390645592427217\n",
      "Iteration: 2387 lambda_n: 0.9214628233566825 Loss: 0.028376531830483633\n",
      "Iteration: 2388 lambda_n: 1.0242405984848104 Loss: 0.028363709158487103\n",
      "Iteration: 2389 lambda_n: 0.9984670485443151 Loss: 0.028349471562934197\n",
      "Iteration: 2390 lambda_n: 0.9037175958760354 Loss: 0.028335607749720496\n",
      "Iteration: 2391 lambda_n: 0.9201562526339607 Loss: 0.028323072765485645\n",
      "Iteration: 2392 lambda_n: 0.9703300469302507 Loss: 0.028310322697974847\n",
      "Iteration: 2393 lambda_n: 0.9121169021558314 Loss: 0.02829689155277887\n",
      "Iteration: 2394 lambda_n: 0.9712118791740401 Loss: 0.02828427944752834\n",
      "Iteration: 2395 lambda_n: 0.9399310566834158 Loss: 0.028270864369008608\n",
      "Iteration: 2396 lambda_n: 0.9217590394548493 Loss: 0.028257895279189607\n",
      "Iteration: 2397 lambda_n: 0.928085147438987 Loss: 0.02824519023734306\n",
      "Iteration: 2398 lambda_n: 0.9329892941022889 Loss: 0.028232411336368727\n",
      "Iteration: 2399 lambda_n: 0.9548786140105356 Loss: 0.02821957841671324\n",
      "Iteration: 2400 lambda_n: 0.8895516735948886 Loss: 0.02820645846018808\n",
      "Iteration: 2401 lambda_n: 1.019684615375585 Loss: 0.028194248888474577\n",
      "Iteration: 2402 lambda_n: 0.9280414494850923 Loss: 0.02818026838101935\n",
      "Iteration: 2403 lambda_n: 0.8907136016139524 Loss: 0.028167558496951874\n",
      "Iteration: 2404 lambda_n: 0.9375604816854132 Loss: 0.02815537252171025\n",
      "Iteration: 2405 lambda_n: 0.8892134471163766 Loss: 0.02814255907357602\n",
      "Iteration: 2406 lambda_n: 0.9884364338085788 Loss: 0.0281304191328543\n",
      "Iteration: 2407 lambda_n: 1.0274872677095805 Loss: 0.028116939153595102\n",
      "Iteration: 2408 lambda_n: 1.0261287192027035 Loss: 0.028102942926772404\n",
      "Iteration: 2409 lambda_n: 0.9434159909638513 Loss: 0.028088981828607255\n",
      "Iteration: 2410 lambda_n: 0.9243030858586723 Loss: 0.02807616076513669\n",
      "Iteration: 2411 lambda_n: 0.9071797877788266 Loss: 0.028063613104012074\n",
      "Iteration: 2412 lambda_n: 1.0230325879921516 Loss: 0.02805131105819705\n",
      "Iteration: 2413 lambda_n: 0.9501205105810351 Loss: 0.0280374536263425\n",
      "Iteration: 2414 lambda_n: 1.0112951050996914 Loss: 0.028024598713695406\n",
      "Iteration: 2415 lambda_n: 0.8739296773127979 Loss: 0.028010931900182463\n",
      "Iteration: 2416 lambda_n: 0.9539420215907133 Loss: 0.02799913452891395\n",
      "Iteration: 2417 lambda_n: 0.8789461577953849 Loss: 0.02798627099390266\n",
      "Iteration: 2418 lambda_n: 1.0238158987210952 Loss: 0.027974431635785415\n",
      "Iteration: 2419 lambda_n: 0.8911518163578318 Loss: 0.02796065648593438\n",
      "Iteration: 2420 lambda_n: 1.0166108231892748 Loss: 0.02794867996951923\n",
      "Iteration: 2421 lambda_n: 0.9577671270827085 Loss: 0.027935032926016234\n",
      "Iteration: 2422 lambda_n: 0.9209932648182367 Loss: 0.027922190996102613\n",
      "Iteration: 2423 lambda_n: 0.9766393238903165 Loss: 0.02790985605750275\n",
      "Iteration: 2424 lambda_n: 0.8801839168670239 Loss: 0.02789679077680613\n",
      "Iteration: 2425 lambda_n: 0.9617837098127161 Loss: 0.027885029039655236\n",
      "Iteration: 2426 lambda_n: 0.9459801732636742 Loss: 0.027872191209646485\n",
      "Iteration: 2427 lambda_n: 0.9955404019423462 Loss: 0.027859578921218722\n",
      "Iteration: 2428 lambda_n: 0.8765749499483333 Loss: 0.027846321526536454\n",
      "Iteration: 2429 lambda_n: 0.9519478861074834 Loss: 0.027834661678304336\n",
      "Iteration: 2430 lambda_n: 0.9533559254739833 Loss: 0.02782201338733998\n",
      "Iteration: 2431 lambda_n: 0.904711572062765 Loss: 0.027809361160765277\n",
      "Iteration: 2432 lambda_n: 1.0243884063029167 Loss: 0.02779736819528491\n",
      "Iteration: 2433 lambda_n: 0.985084028280981 Loss: 0.02778380489594536\n",
      "Iteration: 2434 lambda_n: 0.908544438761864 Loss: 0.02777077816747378\n",
      "Iteration: 2435 lambda_n: 1.0209089931690607 Loss: 0.027758777666442774\n",
      "Iteration: 2436 lambda_n: 0.9875389312321435 Loss: 0.027745309131952164\n",
      "Iteration: 2437 lambda_n: 0.9501650332091506 Loss: 0.027732297104516437\n",
      "Iteration: 2438 lambda_n: 1.0119820319935608 Loss: 0.02771979264451824\n",
      "Iteration: 2439 lambda_n: 1.0084222531931526 Loss: 0.0277064909855991\n",
      "Iteration: 2440 lambda_n: 0.9844628140785174 Loss: 0.027693252897777146\n",
      "Iteration: 2441 lambda_n: 0.9149599280281498 Loss: 0.027680345520227103\n",
      "Iteration: 2442 lambda_n: 1.0235203662093209 Loss: 0.027668363755167534\n",
      "Iteration: 2443 lambda_n: 0.9658417964348875 Loss: 0.02765497675942022\n",
      "Iteration: 2444 lambda_n: 0.9294240514936241 Loss: 0.0276423600770031\n",
      "Iteration: 2445 lambda_n: 1.0117436922227834 Loss: 0.027630233725869525\n",
      "Iteration: 2446 lambda_n: 0.9632838766250694 Loss: 0.027617049649647996\n",
      "Iteration: 2447 lambda_n: 0.8795445200192085 Loss: 0.027604512879174564\n",
      "Iteration: 2448 lambda_n: 0.9821342066920695 Loss: 0.027593079443621302\n",
      "Iteration: 2449 lambda_n: 0.9449758131595225 Loss: 0.027580327666849674\n",
      "Iteration: 2450 lambda_n: 0.8839992968256343 Loss: 0.027568073552989483\n",
      "Iteration: 2451 lambda_n: 0.8854050617420418 Loss: 0.0275566236812402\n",
      "Iteration: 2452 lambda_n: 0.9081594237533018 Loss: 0.027545168719331742\n",
      "Iteration: 2453 lambda_n: 0.9081023595261848 Loss: 0.027533433028133786\n",
      "Iteration: 2454 lambda_n: 0.9263711432554362 Loss: 0.027521711920079628\n",
      "Iteration: 2455 lambda_n: 0.9612365842983271 Loss: 0.027509769296782206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2456 lambda_n: 0.9651256472433896 Loss: 0.027497392465866008\n",
      "Iteration: 2457 lambda_n: 0.895220962114043 Loss: 0.027484981229059523\n",
      "Iteration: 2458 lambda_n: 0.9232325527792012 Loss: 0.027473483001121894\n",
      "Iteration: 2459 lambda_n: 0.92050047599433 Loss: 0.027461639179411846\n",
      "Iteration: 2460 lambda_n: 1.0225948986346869 Loss: 0.02744984476703878\n",
      "Iteration: 2461 lambda_n: 1.0170486254850883 Loss: 0.027436759050829714\n",
      "Iteration: 2462 lambda_n: 1.0075684186121638 Loss: 0.027423761908990926\n",
      "Iteration: 2463 lambda_n: 0.9695885392751257 Loss: 0.027410903250709245\n",
      "Iteration: 2464 lambda_n: 0.9974496038437112 Loss: 0.027398545606171484\n",
      "Iteration: 2465 lambda_n: 0.9342590407296679 Loss: 0.02738584958401471\n",
      "Iteration: 2466 lambda_n: 0.9240665352651736 Loss: 0.02737397328061503\n",
      "Iteration: 2467 lambda_n: 0.9266685207651998 Loss: 0.027362241216376103\n",
      "Iteration: 2468 lambda_n: 1.022267636926651 Loss: 0.027350490789478706\n",
      "Iteration: 2469 lambda_n: 0.9323559123777935 Loss: 0.027337545206461655\n",
      "Iteration: 2470 lambda_n: 0.9867060617495012 Loss: 0.02732575386195823\n",
      "Iteration: 2471 lambda_n: 0.9014466291343277 Loss: 0.027313291426848834\n",
      "Iteration: 2472 lambda_n: 0.9973064916195434 Loss: 0.027301920487634895\n",
      "Iteration: 2473 lambda_n: 0.883112748262925 Loss: 0.027289356673251528\n",
      "Iteration: 2474 lambda_n: 0.945747939499267 Loss: 0.02727824576406908\n",
      "Iteration: 2475 lambda_n: 0.989185299429527 Loss: 0.027266361745573973\n",
      "Iteration: 2476 lambda_n: 1.0125835654362827 Loss: 0.02725394845571532\n",
      "Iteration: 2477 lambda_n: 0.977488867584387 Loss: 0.02724125909265032\n",
      "Iteration: 2478 lambda_n: 0.9937029922790133 Loss: 0.027229026392240414\n",
      "Iteration: 2479 lambda_n: 0.9600270417728548 Loss: 0.02721660779002721\n",
      "Iteration: 2480 lambda_n: 1.0246625056696352 Loss: 0.027204626354649597\n",
      "Iteration: 2481 lambda_n: 0.9855791872442469 Loss: 0.027191855955924896\n",
      "Iteration: 2482 lambda_n: 0.8938881110267048 Loss: 0.027179589928162072\n",
      "Iteration: 2483 lambda_n: 0.9964262400056209 Loss: 0.027168479709467605\n",
      "Iteration: 2484 lambda_n: 0.8759678646853186 Loss: 0.02715611150223128\n",
      "Iteration: 2485 lambda_n: 0.9553087685149209 Loss: 0.027145252849404063\n",
      "Iteration: 2486 lambda_n: 0.9695455571328032 Loss: 0.027133426007022007\n",
      "Iteration: 2487 lambda_n: 0.9418437887252937 Loss: 0.027121439290366173\n",
      "Iteration: 2488 lambda_n: 0.903199537644603 Loss: 0.027109810877490325\n",
      "Iteration: 2489 lambda_n: 0.9095838906193896 Loss: 0.027098674246970027\n",
      "Iteration: 2490 lambda_n: 0.9378429978856433 Loss: 0.027087473423820018\n",
      "Iteration: 2491 lambda_n: 1.0080134966547252 Loss: 0.027075939896357516\n",
      "Iteration: 2492 lambda_n: 0.9242383513640853 Loss: 0.02706356074364748\n",
      "Iteration: 2493 lambda_n: 0.9254241237572621 Loss: 0.027052226207451797\n",
      "Iteration: 2494 lambda_n: 1.027616266804333 Loss: 0.02704089228951652\n",
      "Iteration: 2495 lambda_n: 0.9987755231570432 Loss: 0.027028324596054615\n",
      "Iteration: 2496 lambda_n: 0.9981715843821565 Loss: 0.027016127596292846\n",
      "Iteration: 2497 lambda_n: 0.8989370745640329 Loss: 0.02700395569750057\n",
      "Iteration: 2498 lambda_n: 0.9532828166331222 Loss: 0.026993009068681368\n",
      "Iteration: 2499 lambda_n: 1.022667838756342 Loss: 0.026981416465376655\n",
      "Iteration: 2500 lambda_n: 0.9418676706288621 Loss: 0.026968998138169834\n",
      "Iteration: 2501 lambda_n: 1.022404946208383 Loss: 0.026957577479106678\n",
      "Iteration: 2502 lambda_n: 0.9720678708240212 Loss: 0.02694519822036507\n",
      "Iteration: 2503 lambda_n: 0.914075591649621 Loss: 0.026933445808597918\n",
      "Iteration: 2504 lambda_n: 0.923048333571259 Loss: 0.026922409998877433\n",
      "Iteration: 2505 lambda_n: 0.938926550423073 Loss: 0.026911281096430848\n",
      "Iteration: 2506 lambda_n: 0.8864819613343805 Loss: 0.026899976485087598\n",
      "Iteration: 2507 lambda_n: 0.8919852199473274 Loss: 0.02688931788074905\n",
      "Iteration: 2508 lambda_n: 0.9873825042664064 Loss: 0.0268786074160619\n",
      "Iteration: 2509 lambda_n: 0.9322390832846938 Loss: 0.026866768231643773\n",
      "Iteration: 2510 lambda_n: 0.9045519472025587 Loss: 0.02685560642038265\n",
      "Iteration: 2511 lambda_n: 0.9263878869300454 Loss: 0.026844791029178982\n",
      "Iteration: 2512 lambda_n: 0.9183954290463076 Loss: 0.026832381017330282\n",
      "Iteration: 2513 lambda_n: 0.8904888187831091 Loss: 0.026818696899980114\n",
      "Iteration: 2514 lambda_n: 0.9928569675611518 Loss: 0.026804849989845475\n",
      "Iteration: 2515 lambda_n: 1.0273193916341599 Loss: 0.0267891834253266\n",
      "Iteration: 2516 lambda_n: 0.9043387898005304 Loss: 0.026772928783563094\n",
      "Iteration: 2517 lambda_n: 0.9985486066463259 Loss: 0.026758691157595722\n",
      "Iteration: 2518 lambda_n: 1.0160832593897402 Loss: 0.026743097804942658\n",
      "Iteration: 2519 lambda_n: 0.9600685617835332 Loss: 0.026727395371088532\n",
      "Iteration: 2520 lambda_n: 0.9128806848071819 Loss: 0.026712727628335348\n",
      "Iteration: 2521 lambda_n: 0.9797083892639479 Loss: 0.026698937205987486\n",
      "Iteration: 2522 lambda_n: 0.9336480907681551 Loss: 0.026684300538403236\n",
      "Iteration: 2523 lambda_n: 0.8823095818451557 Loss: 0.026670514780868092\n",
      "Iteration: 2524 lambda_n: 0.9123993349926901 Loss: 0.026657631085796468\n",
      "Iteration: 2525 lambda_n: 0.9702929663683816 Loss: 0.0266444477916811\n",
      "Iteration: 2526 lambda_n: 1.010704029190711 Loss: 0.02663057872934321\n",
      "Iteration: 2527 lambda_n: 0.9873755605433319 Loss: 0.026616293521291468\n",
      "Iteration: 2528 lambda_n: 0.9189920644390656 Loss: 0.026602494591941885\n",
      "Iteration: 2529 lambda_n: 0.9808320555740516 Loss: 0.026589786500386386\n",
      "Iteration: 2530 lambda_n: 0.9635404067426603 Loss: 0.026576354968150643\n",
      "Iteration: 2531 lambda_n: 1.0124710403101747 Loss: 0.02656328969979824\n",
      "Iteration: 2532 lambda_n: 0.9928050887877893 Loss: 0.026549689347055937\n",
      "Iteration: 2533 lambda_n: 0.878047302254003 Loss: 0.026536476142935757\n",
      "Iteration: 2534 lambda_n: 0.9138882795984228 Loss: 0.02652488811796522\n",
      "Iteration: 2535 lambda_n: 0.9013551555355872 Loss: 0.02651291445175244\n",
      "Iteration: 2536 lambda_n: 0.9166902339122908 Loss: 0.026501188135837135\n",
      "Iteration: 2537 lambda_n: 0.9063040326921161 Loss: 0.02648934075533247\n",
      "Iteration: 2538 lambda_n: 0.940109226816866 Loss: 0.026477700587888932\n",
      "Iteration: 2539 lambda_n: 0.9147655542209469 Loss: 0.026465696509665397\n",
      "Iteration: 2540 lambda_n: 0.9103959616210139 Loss: 0.026454080675131508\n",
      "Iteration: 2541 lambda_n: 0.9624913969239774 Loss: 0.02644257834121098\n",
      "Iteration: 2542 lambda_n: 0.9097201023722167 Loss: 0.02643047488456391\n",
      "Iteration: 2543 lambda_n: 0.9872440278626766 Loss: 0.02641908597004959\n",
      "Iteration: 2544 lambda_n: 0.9339154788744982 Loss: 0.02640677598598823\n",
      "Iteration: 2545 lambda_n: 0.9038374684962787 Loss: 0.026395175783563072\n",
      "Iteration: 2546 lambda_n: 0.9316412975908948 Loss: 0.02638398672974868\n",
      "Iteration: 2547 lambda_n: 0.9620500038609708 Loss: 0.026372488202938592\n",
      "Iteration: 2548 lambda_n: 1.0180681655409929 Loss: 0.02636064809278657\n",
      "Iteration: 2549 lambda_n: 0.988871412973147 Loss: 0.026348152252260015\n",
      "Iteration: 2550 lambda_n: 0.93932202766866 Loss: 0.0263360454166221\n",
      "Iteration: 2551 lambda_n: 0.956242501575798 Loss: 0.02632457058231537\n",
      "Iteration: 2552 lambda_n: 0.9808713601645385 Loss: 0.02631291162951369\n",
      "Iteration: 2553 lambda_n: 0.9629037079424468 Loss: 0.026300973815139145\n",
      "Iteration: 2554 lambda_n: 0.9440309894043035 Loss: 0.026289273989284885\n",
      "Iteration: 2555 lambda_n: 0.996861063259559 Loss: 0.02627782028719542\n",
      "Iteration: 2556 lambda_n: 0.9154164001667325 Loss: 0.02626574168263246\n",
      "Iteration: 2557 lambda_n: 0.8736042300705185 Loss: 0.026254663560924534\n",
      "Iteration: 2558 lambda_n: 0.9204023793003243 Loss: 0.026244102391389846\n",
      "Iteration: 2559 lambda_n: 1.0006390228921365 Loss: 0.026232985779982256\n",
      "Iteration: 2560 lambda_n: 0.9044540320580778 Loss: 0.026220910953731497\n",
      "Iteration: 2561 lambda_n: 0.9292112761810384 Loss: 0.026210006113330516\n",
      "Iteration: 2562 lambda_n: 1.0174425976929833 Loss: 0.02619881089541608\n",
      "Iteration: 2563 lambda_n: 0.970676496233067 Loss: 0.026186561148853843\n",
      "Iteration: 2564 lambda_n: 0.876467501844467 Loss: 0.026174882283057456\n",
      "Iteration: 2565 lambda_n: 0.9991226363150638 Loss: 0.02616434298880955\n",
      "Iteration: 2566 lambda_n: 0.9853973443606838 Loss: 0.026152334867008145\n",
      "Iteration: 2567 lambda_n: 0.8821617766575507 Loss: 0.0261404977702217\n",
      "Iteration: 2568 lambda_n: 0.8746923568190447 Loss: 0.026129905608950423\n",
      "Iteration: 2569 lambda_n: 1.0036436504889577 Loss: 0.026119407170211613\n",
      "Iteration: 2570 lambda_n: 0.9927598152699209 Loss: 0.02610736541367062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2571 lambda_n: 0.8880502874399264 Loss: 0.026095458737632662\n",
      "Iteration: 2572 lambda_n: 0.9672487651767517 Loss: 0.026084811511561747\n",
      "Iteration: 2573 lambda_n: 0.9526277520293672 Loss: 0.02607321815243682\n",
      "Iteration: 2574 lambda_n: 1.012299096061854 Loss: 0.02606180339734485\n",
      "Iteration: 2575 lambda_n: 1.0221050146788524 Loss: 0.026049676973728068\n",
      "Iteration: 2576 lambda_n: 0.8834874032787613 Loss: 0.026037436408386877\n",
      "Iteration: 2577 lambda_n: 0.8750746538739947 Loss: 0.026026858563179815\n",
      "Iteration: 2578 lambda_n: 0.9775270090093054 Loss: 0.02601638364062191\n",
      "Iteration: 2579 lambda_n: 0.9814779767617774 Loss: 0.026004684691082067\n",
      "Iteration: 2580 lambda_n: 0.936551328596769 Loss: 0.025992940933399513\n",
      "Iteration: 2581 lambda_n: 0.9073698755115149 Loss: 0.025981736984103578\n",
      "Iteration: 2582 lambda_n: 0.9565120241049175 Loss: 0.025970884120302077\n",
      "Iteration: 2583 lambda_n: 0.935839193052718 Loss: 0.02595944544885158\n",
      "Iteration: 2584 lambda_n: 0.9625671354980257 Loss: 0.02594825593694237\n",
      "Iteration: 2585 lambda_n: 0.914937175763602 Loss: 0.025936748740547938\n",
      "Iteration: 2586 lambda_n: 0.9387676043707697 Loss: 0.025925812719720685\n",
      "Iteration: 2587 lambda_n: 0.9086172139395264 Loss: 0.02591459355018515\n",
      "Iteration: 2588 lambda_n: 0.9911127983553921 Loss: 0.025903736329736433\n",
      "Iteration: 2589 lambda_n: 0.9795261353660081 Loss: 0.025891895043177095\n",
      "Iteration: 2590 lambda_n: 0.9410174508805064 Loss: 0.02588019394696296\n",
      "Iteration: 2591 lambda_n: 1.0204835967218313 Loss: 0.025868954490992576\n",
      "Iteration: 2592 lambda_n: 0.9600966767890314 Loss: 0.02585676757171699\n",
      "Iteration: 2593 lambda_n: 0.9764921521149897 Loss: 0.025845303471141924\n",
      "Iteration: 2594 lambda_n: 0.9650828778423507 Loss: 0.025833645166935437\n",
      "Iteration: 2595 lambda_n: 0.9700359386704488 Loss: 0.02582212462560626\n",
      "Iteration: 2596 lambda_n: 0.9871119285813134 Loss: 0.025810546474315865\n",
      "Iteration: 2597 lambda_n: 0.9123726942333457 Loss: 0.02579876603865028\n",
      "Iteration: 2598 lambda_n: 0.8864637131549419 Loss: 0.025787878975975394\n",
      "Iteration: 2599 lambda_n: 0.909021226380263 Loss: 0.025777302338229675\n",
      "Iteration: 2600 lambda_n: 0.9661281754178676 Loss: 0.02576645780761079\n",
      "Iteration: 2601 lambda_n: 0.9286488401702665 Loss: 0.025754933345353788\n",
      "Iteration: 2602 lambda_n: 0.9735959606794309 Loss: 0.025743857316741486\n",
      "Iteration: 2603 lambda_n: 0.9300321629863901 Loss: 0.02573224656661425\n",
      "Iteration: 2604 lambda_n: 0.9212707931901662 Loss: 0.025721156695406915\n",
      "Iteration: 2605 lambda_n: 0.9831627030293162 Loss: 0.025710172569093002\n",
      "Iteration: 2606 lambda_n: 0.9254164880762799 Loss: 0.025698451857334225\n",
      "Iteration: 2607 lambda_n: 0.9622863425035713 Loss: 0.025687420897253305\n",
      "Iteration: 2608 lambda_n: 0.9892352669045716 Loss: 0.025675951749398226\n",
      "Iteration: 2609 lambda_n: 0.937910396780124 Loss: 0.02566416278971643\n",
      "Iteration: 2610 lambda_n: 0.921875248935145 Loss: 0.025652986820374875\n",
      "Iteration: 2611 lambda_n: 0.9206194028447666 Loss: 0.025642003165762827\n",
      "Iteration: 2612 lambda_n: 0.9865131917984864 Loss: 0.025631035688731273\n",
      "Iteration: 2613 lambda_n: 0.9495112900003909 Loss: 0.025619284504800065\n",
      "Iteration: 2614 lambda_n: 0.8822222697800021 Loss: 0.02560797541064581\n",
      "Iteration: 2615 lambda_n: 0.887704141724864 Loss: 0.025597468941572717\n",
      "Iteration: 2616 lambda_n: 1.0230941914235947 Loss: 0.02558689829143309\n",
      "Iteration: 2617 lambda_n: 0.9345888810240214 Loss: 0.02557471671270706\n",
      "Iteration: 2618 lambda_n: 0.957983496785903 Loss: 0.025563590267850895\n",
      "Iteration: 2619 lambda_n: 0.9200751622351782 Loss: 0.025552186553259085\n",
      "Iteration: 2620 lambda_n: 0.8807655678215687 Loss: 0.02554123531766912\n",
      "Iteration: 2621 lambda_n: 0.9916073184944003 Loss: 0.025530753086650333\n",
      "Iteration: 2622 lambda_n: 0.9695942472269429 Loss: 0.025518952902193658\n",
      "Iteration: 2623 lambda_n: 0.9628115993508283 Loss: 0.025507415994921116\n",
      "Iteration: 2624 lambda_n: 0.9464693589339357 Loss: 0.025495961069706478\n",
      "Iteration: 2625 lambda_n: 1.0183179496957682 Loss: 0.02548470181652443\n",
      "Iteration: 2626 lambda_n: 1.0054840719801863 Loss: 0.025472589158140685\n",
      "Iteration: 2627 lambda_n: 0.9136121029378335 Loss: 0.02546063054168233\n",
      "Iteration: 2628 lambda_n: 0.928404241672125 Loss: 0.025449765832907072\n",
      "Iteration: 2629 lambda_n: 1.0241285855109035 Loss: 0.02543872635491132\n",
      "Iteration: 2630 lambda_n: 0.9961687653617961 Loss: 0.025426549910028563\n",
      "Iteration: 2631 lambda_n: 0.9992226562669695 Loss: 0.025414707255829033\n",
      "Iteration: 2632 lambda_n: 0.9059573556371712 Loss: 0.025402829618618054\n",
      "Iteration: 2633 lambda_n: 0.9384290180465976 Loss: 0.025392061812161634\n",
      "Iteration: 2634 lambda_n: 0.9062678450312118 Loss: 0.025380909182492632\n",
      "Iteration: 2635 lambda_n: 0.9800994140996159 Loss: 0.025370139884505774\n",
      "Iteration: 2636 lambda_n: 0.945634699964069 Loss: 0.025358494398466786\n",
      "Iteration: 2637 lambda_n: 1.0232076621107542 Loss: 0.02534725962797686\n",
      "Iteration: 2638 lambda_n: 0.96587672268262 Loss: 0.025335104494952725\n",
      "Iteration: 2639 lambda_n: 0.8882755845947404 Loss: 0.02532363169884592\n",
      "Iteration: 2640 lambda_n: 0.9932595813825957 Loss: 0.025313081762328656\n",
      "Iteration: 2641 lambda_n: 1.01456658511614 Loss: 0.025301286075290905\n",
      "Iteration: 2642 lambda_n: 0.957107869878611 Loss: 0.025289238638744275\n",
      "Iteration: 2643 lambda_n: 0.9682510278270123 Loss: 0.02527787472828896\n",
      "Iteration: 2644 lambda_n: 0.8860690154632211 Loss: 0.025266379686139862\n",
      "Iteration: 2645 lambda_n: 0.9686987602671213 Loss: 0.02525586138718395\n",
      "Iteration: 2646 lambda_n: 0.9188063402445094 Loss: 0.025244363288810568\n",
      "Iteration: 2647 lambda_n: 0.8816728614330959 Loss: 0.025233458507522663\n",
      "Iteration: 2648 lambda_n: 1.0243445979758836 Loss: 0.025222995450030112\n",
      "Iteration: 2649 lambda_n: 1.006674542588453 Loss: 0.02521084038645089\n",
      "Iteration: 2650 lambda_n: 0.9109427806947327 Loss: 0.02519889627152707\n",
      "Iteration: 2651 lambda_n: 0.8939029587592774 Loss: 0.025188089132673404\n",
      "Iteration: 2652 lambda_n: 0.8816491575141902 Loss: 0.025177485144245924\n",
      "Iteration: 2653 lambda_n: 0.9242647587986494 Loss: 0.025167027476597447\n",
      "Iteration: 2654 lambda_n: 0.8993857843004056 Loss: 0.02515606531221554\n",
      "Iteration: 2655 lambda_n: 0.9655030458395573 Loss: 0.025145399225952058\n",
      "Iteration: 2656 lambda_n: 0.875233573311365 Loss: 0.025133950078470135\n",
      "Iteration: 2657 lambda_n: 0.9672816371146721 Loss: 0.025123572377008182\n",
      "Iteration: 2658 lambda_n: 1.0232307810903927 Loss: 0.025112104263565947\n",
      "Iteration: 2659 lambda_n: 0.9857970252156089 Loss: 0.02509997398903602\n",
      "Iteration: 2660 lambda_n: 0.937219474151089 Loss: 0.025088288677042308\n",
      "Iteration: 2661 lambda_n: 0.9430676138325813 Loss: 0.025077180271701853\n",
      "Iteration: 2662 lambda_n: 0.9238910466864911 Loss: 0.025066003583546312\n",
      "Iteration: 2663 lambda_n: 0.8844008456615057 Loss: 0.025055055177763273\n",
      "Iteration: 2664 lambda_n: 0.963152931491922 Loss: 0.025044575688896176\n",
      "Iteration: 2665 lambda_n: 0.9784691190567165 Loss: 0.02503316402761508\n",
      "Iteration: 2666 lambda_n: 0.9883143404643507 Loss: 0.02502157197693703\n",
      "Iteration: 2667 lambda_n: 0.9940236465245864 Loss: 0.025009864391796505\n",
      "Iteration: 2668 lambda_n: 0.9105464940833629 Loss: 0.024998090289212384\n",
      "Iteration: 2669 lambda_n: 0.9701402918975175 Loss: 0.02498730598657255\n",
      "Iteration: 2670 lambda_n: 0.9217053231437226 Loss: 0.024975816861178536\n",
      "Iteration: 2671 lambda_n: 0.896622439997882 Loss: 0.024964902338686354\n",
      "Iteration: 2672 lambda_n: 0.980002869285085 Loss: 0.024954285758632618\n",
      "Iteration: 2673 lambda_n: 0.8943490883533477 Loss: 0.02494268287414311\n",
      "Iteration: 2674 lambda_n: 0.8913860631119804 Loss: 0.024932095065729958\n",
      "Iteration: 2675 lambda_n: 1.0106824220962973 Loss: 0.024921543209284972\n",
      "Iteration: 2676 lambda_n: 0.9798279404391251 Loss: 0.024909580155348693\n",
      "Iteration: 2677 lambda_n: 0.9782028084431379 Loss: 0.0248979833893811\n",
      "Iteration: 2678 lambda_n: 0.9326879241472205 Loss: 0.024886406893051678\n",
      "Iteration: 2679 lambda_n: 0.9287084633555935 Loss: 0.02487537002096654\n",
      "Iteration: 2680 lambda_n: 0.9714352291885098 Loss: 0.024864381165427645\n",
      "Iteration: 2681 lambda_n: 1.0055623777758311 Loss: 0.024852887709108325\n",
      "Iteration: 2682 lambda_n: 0.9481442838006339 Loss: 0.024840991514141597\n",
      "Iteration: 2683 lambda_n: 1.018004562451903 Loss: 0.024829775601484992\n",
      "Iteration: 2684 lambda_n: 0.8922513177921345 Loss: 0.024817734299276036\n",
      "Iteration: 2685 lambda_n: 1.0048482396432008 Loss: 0.024807181395339318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2686 lambda_n: 1.0077694547972444 Loss: 0.024795297705384283\n",
      "Iteration: 2687 lambda_n: 1.0140624592226346 Loss: 0.024783380512349715\n",
      "Iteration: 2688 lambda_n: 0.8740740898963796 Loss: 0.0247713899506348\n",
      "Iteration: 2689 lambda_n: 0.9044370774915276 Loss: 0.024761055555125012\n",
      "Iteration: 2690 lambda_n: 0.9793320108237435 Loss: 0.024750362972365186\n",
      "Iteration: 2691 lambda_n: 0.9901387982248253 Loss: 0.024738785848781145\n",
      "Iteration: 2692 lambda_n: 0.8904096920069852 Loss: 0.024727081947428186\n",
      "Iteration: 2693 lambda_n: 1.0271384232843481 Loss: 0.024716557771459317\n",
      "Iteration: 2694 lambda_n: 1.0023238362899956 Loss: 0.024704418442458485\n",
      "Iteration: 2695 lambda_n: 0.9729327324780923 Loss: 0.024692573405095068\n",
      "Iteration: 2696 lambda_n: 0.9963926462137608 Loss: 0.024681076658339088\n",
      "Iteration: 2697 lambda_n: 0.9654997608169221 Loss: 0.02466930364352713\n",
      "Iteration: 2698 lambda_n: 0.9260231315903517 Loss: 0.024657896583714325\n",
      "Iteration: 2699 lambda_n: 0.9112438043974205 Loss: 0.02464695679219202\n",
      "Iteration: 2700 lambda_n: 0.9595962129188053 Loss: 0.024636192411770966\n",
      "Iteration: 2701 lambda_n: 0.9600714940180416 Loss: 0.02462485768912086\n",
      "Iteration: 2702 lambda_n: 0.9891837982161896 Loss: 0.024613518230099128\n",
      "Iteration: 2703 lambda_n: 0.9675194995642992 Loss: 0.0246018358239045\n",
      "Iteration: 2704 lambda_n: 0.9994824243803501 Loss: 0.02459041017834676\n",
      "Iteration: 2705 lambda_n: 0.9351664641399565 Loss: 0.024578607982238803\n",
      "Iteration: 2706 lambda_n: 0.9162196498584833 Loss: 0.02456756612027846\n",
      "Iteration: 2707 lambda_n: 1.007766338482899 Loss: 0.024556748764950027\n",
      "Iteration: 2708 lambda_n: 0.9153088267947626 Loss: 0.02454485141455759\n",
      "Iteration: 2709 lambda_n: 0.9398504781545972 Loss: 0.024534046432995674\n",
      "Iteration: 2710 lambda_n: 0.8868008878658153 Loss: 0.02452295252873758\n",
      "Iteration: 2711 lambda_n: 0.9874865552325282 Loss: 0.024512485573488653\n",
      "Iteration: 2712 lambda_n: 0.9549203238472909 Loss: 0.024500831011247667\n",
      "Iteration: 2713 lambda_n: 0.9636576712871614 Loss: 0.024489561651317836\n",
      "Iteration: 2714 lambda_n: 0.9263193268819411 Loss: 0.024478190001338416\n",
      "Iteration: 2715 lambda_n: 1.0228222707024002 Loss: 0.024467259756458647\n",
      "Iteration: 2716 lambda_n: 0.9105699036682998 Loss: 0.0244551916483916\n",
      "Iteration: 2717 lambda_n: 0.9655980673945747 Loss: 0.02444444880652284\n",
      "Iteration: 2718 lambda_n: 1.0042131877451754 Loss: 0.02443305751525258\n",
      "Iteration: 2719 lambda_n: 0.9693904976211616 Loss: 0.02442121152010268\n",
      "Iteration: 2720 lambda_n: 0.8803471671755004 Loss: 0.02440977714636376\n",
      "Iteration: 2721 lambda_n: 0.9431884762667612 Loss: 0.024399393811226838\n",
      "Iteration: 2722 lambda_n: 0.9213625954471922 Loss: 0.024388269999402894\n",
      "Iteration: 2723 lambda_n: 0.9771214880467222 Loss: 0.024377404338814954\n",
      "Iteration: 2724 lambda_n: 0.8932054849551369 Loss: 0.024365881874634555\n",
      "Iteration: 2725 lambda_n: 0.9758707825299199 Loss: 0.024355349705036437\n",
      "Iteration: 2726 lambda_n: 1.0112058093915388 Loss: 0.024343843524256677\n",
      "Iteration: 2727 lambda_n: 1.0206010145920632 Loss: 0.024331921542733552\n",
      "Iteration: 2728 lambda_n: 0.9489418344373661 Loss: 0.024319889649306186\n",
      "Iteration: 2729 lambda_n: 0.8999964909913986 Loss: 0.024308703346961953\n",
      "Iteration: 2730 lambda_n: 0.9649776759350538 Loss: 0.024298094721994595\n",
      "Iteration: 2731 lambda_n: 1.0094272151581891 Loss: 0.02428672084599584\n",
      "Iteration: 2732 lambda_n: 1.0091545091909109 Loss: 0.024274823848631256\n",
      "Iteration: 2733 lambda_n: 0.9562247920478985 Loss: 0.024262930887884906\n",
      "Iteration: 2734 lambda_n: 1.005640434044747 Loss: 0.02425166248255789\n",
      "Iteration: 2735 lambda_n: 0.9698898824398108 Loss: 0.024239812517975794\n",
      "Iteration: 2736 lambda_n: 0.9751600243962052 Loss: 0.02422838459454666\n",
      "Iteration: 2737 lambda_n: 0.9366302357914211 Loss: 0.024216895321405335\n",
      "Iteration: 2738 lambda_n: 0.9487094267725363 Loss: 0.024205860721040688\n",
      "Iteration: 2739 lambda_n: 0.9990352049400167 Loss: 0.02419468450766465\n",
      "Iteration: 2740 lambda_n: 0.9910915842235084 Loss: 0.02418291617088151\n",
      "Iteration: 2741 lambda_n: 1.0231978082800206 Loss: 0.024171242172725715\n",
      "Iteration: 2742 lambda_n: 0.9877180273257444 Loss: 0.024159190776868178\n",
      "Iteration: 2743 lambda_n: 0.8985638243161849 Loss: 0.024147558040120354\n",
      "Iteration: 2744 lambda_n: 0.9600120195987357 Loss: 0.02413697598117658\n",
      "Iteration: 2745 lambda_n: 0.9714259346413913 Loss: 0.024125670920980394\n",
      "Iteration: 2746 lambda_n: 0.8744923726227903 Loss: 0.024114232152073166\n",
      "Iteration: 2747 lambda_n: 0.9962995145746 Loss: 0.024103935433827488\n",
      "Iteration: 2748 lambda_n: 0.9218092283064128 Loss: 0.024092205144176492\n",
      "Iteration: 2749 lambda_n: 0.9542074552170093 Loss: 0.024081352572200003\n",
      "Iteration: 2750 lambda_n: 0.8873385695972366 Loss: 0.02407011921918357\n",
      "Iteration: 2751 lambda_n: 1.0144249697309105 Loss: 0.02405967369611066\n",
      "Iteration: 2752 lambda_n: 0.8968094104097767 Loss: 0.024047732799754508\n",
      "Iteration: 2753 lambda_n: 0.9475779366236322 Loss: 0.024037177026805894\n",
      "Iteration: 2754 lambda_n: 0.9258632521234619 Loss: 0.024026024301958358\n",
      "Iteration: 2755 lambda_n: 1.016156505438976 Loss: 0.024015127781582685\n",
      "Iteration: 2756 lambda_n: 0.9556171833943636 Loss: 0.0240031692670989\n",
      "Iteration: 2757 lambda_n: 0.9850406136498047 Loss: 0.02399192389090434\n",
      "Iteration: 2758 lambda_n: 0.8845658268234312 Loss: 0.023980332933571026\n",
      "Iteration: 2759 lambda_n: 0.8824370601719183 Loss: 0.02396992487254096\n",
      "Iteration: 2760 lambda_n: 0.9398252435897851 Loss: 0.023959542403595116\n",
      "Iteration: 2761 lambda_n: 0.983979931663296 Loss: 0.023948485299372563\n",
      "Iteration: 2762 lambda_n: 1.009208598601739 Loss: 0.023936909350977184\n",
      "Iteration: 2763 lambda_n: 0.9197726678066674 Loss: 0.023925037284149762\n",
      "Iteration: 2764 lambda_n: 0.8942974519513939 Loss: 0.02391421795233564\n",
      "Iteration: 2765 lambda_n: 0.9137107305230728 Loss: 0.023903698845415367\n",
      "Iteration: 2766 lambda_n: 0.9931345418053685 Loss: 0.023892951943360982\n",
      "Iteration: 2767 lambda_n: 0.9841991211340654 Loss: 0.023881271482691976\n",
      "Iteration: 2768 lambda_n: 0.9760394961011445 Loss: 0.02386969676707175\n",
      "Iteration: 2769 lambda_n: 0.9842384069573105 Loss: 0.02385821865194416\n",
      "Iteration: 2770 lambda_n: 0.8912313995996647 Loss: 0.02384664475390546\n",
      "Iteration: 2771 lambda_n: 0.9683343758071385 Loss: 0.023836165124782212\n",
      "Iteration: 2772 lambda_n: 1.0226303828393093 Loss: 0.02382477943748526\n",
      "Iteration: 2773 lambda_n: 0.9620067431644865 Loss: 0.023812755981559623\n",
      "Iteration: 2774 lambda_n: 0.9412846032073318 Loss: 0.023801445937692817\n",
      "Iteration: 2775 lambda_n: 0.945159105482374 Loss: 0.023790380100971936\n",
      "Iteration: 2776 lambda_n: 0.9148584409947123 Loss: 0.02377926928465487\n",
      "Iteration: 2777 lambda_n: 0.8781414627543724 Loss: 0.02376851521827296\n",
      "Iteration: 2778 lambda_n: 0.9028781894709093 Loss: 0.02375819326493994\n",
      "Iteration: 2779 lambda_n: 0.8743220964976991 Loss: 0.02374758104779108\n",
      "Iteration: 2780 lambda_n: 1.0181822310516844 Loss: 0.023737304967105154\n",
      "Iteration: 2781 lambda_n: 0.948471538222805 Loss: 0.023725338625027757\n",
      "Iteration: 2782 lambda_n: 0.8930812109833205 Loss: 0.02371419216788081\n",
      "Iteration: 2783 lambda_n: 0.9123669948167553 Loss: 0.023703697181721474\n",
      "Iteration: 2784 lambda_n: 0.88386350639466 Loss: 0.023692976060051687\n",
      "Iteration: 2785 lambda_n: 0.9008411185618396 Loss: 0.02368259037214104\n",
      "Iteration: 2786 lambda_n: 1.0052914119456788 Loss: 0.02367200567544535\n",
      "Iteration: 2787 lambda_n: 0.9637944219306963 Loss: 0.023660194256903713\n",
      "Iteration: 2788 lambda_n: 1.0248723284557733 Loss: 0.023648870979577675\n",
      "Iteration: 2789 lambda_n: 0.8981075787041111 Loss: 0.023636830710444416\n",
      "Iteration: 2790 lambda_n: 1.0012022228230275 Loss: 0.023626280229635203\n",
      "Iteration: 2791 lambda_n: 0.9426800036037969 Loss: 0.023614519180628187\n",
      "Iteration: 2792 lambda_n: 0.9631543477577483 Loss: 0.02360344614317693\n",
      "Iteration: 2793 lambda_n: 1.0171891211986557 Loss: 0.023592133138395963\n",
      "Iteration: 2794 lambda_n: 0.9003750054009564 Loss: 0.023580186022904407\n",
      "Iteration: 2795 lambda_n: 1.0120821691331972 Loss: 0.023569611445646\n",
      "Iteration: 2796 lambda_n: 0.9920973406022592 Loss: 0.02355772543277329\n",
      "Iteration: 2797 lambda_n: 0.9622979871465339 Loss: 0.023546074699018853\n",
      "Iteration: 2798 lambda_n: 0.9357365203103065 Loss: 0.023534774458660163\n",
      "Iteration: 2799 lambda_n: 0.9190478006013276 Loss: 0.02352378663870446\n",
      "Iteration: 2800 lambda_n: 0.9056347178052879 Loss: 0.023512995269094446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2801 lambda_n: 0.9630958743363663 Loss: 0.02350236186078026\n",
      "Iteration: 2802 lambda_n: 1.026665254050866 Loss: 0.023491054264751496\n",
      "Iteration: 2803 lambda_n: 0.9986531822982639 Loss: 0.023479000856209423\n",
      "Iteration: 2804 lambda_n: 0.8968740844670382 Loss: 0.02346727688446305\n",
      "Iteration: 2805 lambda_n: 0.912964860702571 Loss: 0.023456748268220966\n",
      "Iteration: 2806 lambda_n: 0.8801263296350177 Loss: 0.02344603120488448\n",
      "Iteration: 2807 lambda_n: 0.9623690538636951 Loss: 0.02343570006052198\n",
      "Iteration: 2808 lambda_n: 0.9773742712726068 Loss: 0.023424403987355617\n",
      "Iteration: 2809 lambda_n: 0.9654756445651363 Loss: 0.023412932291265738\n",
      "Iteration: 2810 lambda_n: 0.9606240047150819 Loss: 0.023401600756305888\n",
      "Iteration: 2811 lambda_n: 0.9904355394938202 Loss: 0.023390326656264192\n",
      "Iteration: 2812 lambda_n: 0.9977636105425189 Loss: 0.02337870318379971\n",
      "Iteration: 2813 lambda_n: 0.93252943500715 Loss: 0.023366994230234958\n",
      "Iteration: 2814 lambda_n: 0.9840855569791306 Loss: 0.023356051298640792\n",
      "Iteration: 2815 lambda_n: 0.9664490079988144 Loss: 0.023344503849259485\n",
      "Iteration: 2816 lambda_n: 1.0025854762531035 Loss: 0.023333163841926158\n",
      "Iteration: 2817 lambda_n: 0.9513123333446251 Loss: 0.02332140031853974\n",
      "Iteration: 2818 lambda_n: 0.9435789641181179 Loss: 0.023310238879936194\n",
      "Iteration: 2819 lambda_n: 0.9968600159716986 Loss: 0.02329916863054922\n",
      "Iteration: 2820 lambda_n: 0.9573212914272042 Loss: 0.023287473753026137\n",
      "Iteration: 2821 lambda_n: 1.0128293777860133 Loss: 0.02327624321229059\n",
      "Iteration: 2822 lambda_n: 1.0078024510592192 Loss: 0.023264361979017514\n",
      "Iteration: 2823 lambda_n: 0.8783382368059507 Loss: 0.02325254022278776\n",
      "Iteration: 2824 lambda_n: 0.9789896266136044 Loss: 0.023242237549380214\n",
      "Iteration: 2825 lambda_n: 0.9116386700338117 Loss: 0.023230754684669275\n",
      "Iteration: 2826 lambda_n: 0.8976465024116199 Loss: 0.02322006223617423\n",
      "Iteration: 2827 lambda_n: 1.01133352877949 Loss: 0.023209534297381942\n",
      "Iteration: 2828 lambda_n: 0.9894575174965528 Loss: 0.02319767343331435\n",
      "Iteration: 2829 lambda_n: 0.9735176893409445 Loss: 0.023186069611696773\n",
      "Iteration: 2830 lambda_n: 0.9992163116514364 Loss: 0.023174653184880115\n",
      "Iteration: 2831 lambda_n: 1.0088872956098995 Loss: 0.023162935853804283\n",
      "Iteration: 2832 lambda_n: 0.9454172732918481 Loss: 0.023151105593009393\n",
      "Iteration: 2833 lambda_n: 1.011006117536127 Loss: 0.02314002003377676\n",
      "Iteration: 2834 lambda_n: 0.939183835194298 Loss: 0.02312816585518739\n",
      "Iteration: 2835 lambda_n: 0.8772914202929192 Loss: 0.023117154244331004\n",
      "Iteration: 2836 lambda_n: 0.9553972489533518 Loss: 0.023106868682424158\n",
      "Iteration: 2837 lambda_n: 0.8849081604823199 Loss: 0.023095667776195775\n",
      "Iteration: 2838 lambda_n: 0.9767387270080311 Loss: 0.02308529365882086\n",
      "Iteration: 2839 lambda_n: 0.9369623416523493 Loss: 0.02307384337046138\n",
      "Iteration: 2840 lambda_n: 0.9824568545532303 Loss: 0.023062859794883445\n",
      "Iteration: 2841 lambda_n: 0.9843730269007039 Loss: 0.023051343323353565\n",
      "Iteration: 2842 lambda_n: 1.002849229148808 Loss: 0.02303980482408861\n",
      "Iteration: 2843 lambda_n: 0.8784082736403716 Loss: 0.023028050193222467\n",
      "Iteration: 2844 lambda_n: 0.8809351373610727 Loss: 0.023017754554783786\n",
      "Iteration: 2845 lambda_n: 0.9899777291645998 Loss: 0.023007429640877392\n",
      "Iteration: 2846 lambda_n: 0.8883659183831271 Loss: 0.022995827086789135\n",
      "Iteration: 2847 lambda_n: 0.9023576220516473 Loss: 0.02298541580881024\n",
      "Iteration: 2848 lambda_n: 0.9292654066103394 Loss: 0.022974840902085975\n",
      "Iteration: 2849 lambda_n: 0.878685532966315 Loss: 0.022963951020125856\n",
      "Iteration: 2850 lambda_n: 0.9346749366383831 Loss: 0.022953654225034155\n",
      "Iteration: 2851 lambda_n: 0.8767820772956597 Loss: 0.022942701674583014\n",
      "Iteration: 2852 lambda_n: 0.9415592792218075 Loss: 0.022932427863276345\n",
      "Iteration: 2853 lambda_n: 1.022409712972259 Loss: 0.022921395365946107\n",
      "Iteration: 2854 lambda_n: 0.9076063957561745 Loss: 0.022909415928396785\n",
      "Iteration: 2855 lambda_n: 0.906783150088039 Loss: 0.022898782014771826\n",
      "Iteration: 2856 lambda_n: 0.8927625100336257 Loss: 0.02288815808953737\n",
      "Iteration: 2857 lambda_n: 0.9249630146363029 Loss: 0.022877698766505656\n",
      "Iteration: 2858 lambda_n: 0.9042002784518001 Loss: 0.02286686253308242\n",
      "Iteration: 2859 lambda_n: 0.8932699539880512 Loss: 0.022856269884818953\n",
      "Iteration: 2860 lambda_n: 0.9463521683153391 Loss: 0.022845805614277608\n",
      "Iteration: 2861 lambda_n: 0.8881712723063596 Loss: 0.022834719851873745\n",
      "Iteration: 2862 lambda_n: 1.017857384700074 Loss: 0.02282431597196281\n",
      "Iteration: 2863 lambda_n: 0.880190506215062 Loss: 0.02281239333537377\n",
      "Iteration: 2864 lambda_n: 0.932861888207761 Loss: 0.02280208361316294\n",
      "Iteration: 2865 lambda_n: 0.9993431698146641 Loss: 0.02279115727446649\n",
      "Iteration: 2866 lambda_n: 0.9175948615380374 Loss: 0.02277945262875194\n",
      "Iteration: 2867 lambda_n: 1.026911251721347 Loss: 0.022768705807778025\n",
      "Iteration: 2868 lambda_n: 0.8776597664788841 Loss: 0.022756679047934436\n",
      "Iteration: 2869 lambda_n: 0.9824199041915737 Loss: 0.022746400610613348\n",
      "Iteration: 2870 lambda_n: 0.9144855420249225 Loss: 0.022734895641483753\n",
      "Iteration: 2871 lambda_n: 0.8846464248204696 Loss: 0.022724186587269504\n",
      "Iteration: 2872 lambda_n: 0.965575930136013 Loss: 0.02271382727293836\n",
      "Iteration: 2873 lambda_n: 0.9337472728890917 Loss: 0.022702520589984777\n",
      "Iteration: 2874 lambda_n: 0.9062326307590889 Loss: 0.022691586955460405\n",
      "Iteration: 2875 lambda_n: 1.0085016968276768 Loss: 0.022680975820366493\n",
      "Iteration: 2876 lambda_n: 0.9102475520167488 Loss: 0.022669167553112348\n",
      "Iteration: 2877 lambda_n: 0.9942816502790562 Loss: 0.02265851005899882\n",
      "Iteration: 2878 lambda_n: 0.9370221098677834 Loss: 0.022646869000313326\n",
      "Iteration: 2879 lambda_n: 0.9786020634845494 Loss: 0.022635898680759468\n",
      "Iteration: 2880 lambda_n: 0.9619946357925573 Loss: 0.022624441894575204\n",
      "Iteration: 2881 lambda_n: 0.9846307358729117 Loss: 0.02261317988023244\n",
      "Iteration: 2882 lambda_n: 0.9936185168139643 Loss: 0.022601653210347405\n",
      "Iteration: 2883 lambda_n: 0.9935771875122761 Loss: 0.022590021677505852\n",
      "Iteration: 2884 lambda_n: 0.8974626873266849 Loss: 0.022578390983031136\n",
      "Iteration: 2885 lambda_n: 0.9677557243470566 Loss: 0.022567885711730847\n",
      "Iteration: 2886 lambda_n: 0.8977017952542047 Loss: 0.022556557932016143\n",
      "Iteration: 2887 lambda_n: 0.9851390160375236 Loss: 0.022546050454904598\n",
      "Iteration: 2888 lambda_n: 0.9471574800211917 Loss: 0.022534519847816466\n",
      "Iteration: 2889 lambda_n: 0.897402927943008 Loss: 0.022523434123679593\n",
      "Iteration: 2890 lambda_n: 1.0136842793730199 Loss: 0.0225129310324868\n",
      "Iteration: 2891 lambda_n: 1.0056957860665416 Loss: 0.02250106731355502\n",
      "Iteration: 2892 lambda_n: 0.9839645299568522 Loss: 0.022489297439245493\n",
      "Iteration: 2893 lambda_n: 0.8842058320236215 Loss: 0.022477782229054655\n",
      "Iteration: 2894 lambda_n: 0.9241505331222926 Loss: 0.022467434778025257\n",
      "Iteration: 2895 lambda_n: 0.9141301260996213 Loss: 0.0224566201491056\n",
      "Iteration: 2896 lambda_n: 0.9468920391022785 Loss: 0.022445923065731576\n",
      "Iteration: 2897 lambda_n: 0.943806650734688 Loss: 0.022434842894710063\n",
      "Iteration: 2898 lambda_n: 0.9590118016308934 Loss: 0.022423799125443653\n",
      "Iteration: 2899 lambda_n: 0.960298589237872 Loss: 0.02241257773598645\n",
      "Iteration: 2900 lambda_n: 0.875624994142304 Loss: 0.022401341593369328\n",
      "Iteration: 2901 lambda_n: 0.9194095914675937 Loss: 0.022391096464692273\n",
      "Iteration: 2902 lambda_n: 0.900035402415352 Loss: 0.022380339302889865\n",
      "Iteration: 2903 lambda_n: 1.022175038329946 Loss: 0.02236980908902845\n",
      "Iteration: 2904 lambda_n: 0.9174174226146941 Loss: 0.022357850165538467\n",
      "Iteration: 2905 lambda_n: 0.8843637248448039 Loss: 0.022347117153525886\n",
      "Iteration: 2906 lambda_n: 1.0128593916488304 Loss: 0.022336771100993014\n",
      "Iteration: 2907 lambda_n: 0.9155307058402511 Loss: 0.022324922079391733\n",
      "Iteration: 2908 lambda_n: 0.9722259827874566 Loss: 0.02231421195873744\n",
      "Iteration: 2909 lambda_n: 0.8786252186563683 Loss: 0.02230283888140836\n",
      "Iteration: 2910 lambda_n: 0.9955461342462842 Loss: 0.022292561010709148\n",
      "Iteration: 2911 lambda_n: 0.9405222540407894 Loss: 0.02228091570928286\n",
      "Iteration: 2912 lambda_n: 0.965608768755461 Loss: 0.0222699143338502\n",
      "Iteration: 2913 lambda_n: 1.0264794210467885 Loss: 0.02225861979851114\n",
      "Iteration: 2914 lambda_n: 0.9872790548820963 Loss: 0.02224661357443967\n",
      "Iteration: 2915 lambda_n: 0.8745442582193584 Loss: 0.022235066166140325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2916 lambda_n: 0.975980393489477 Loss: 0.022224837587234456\n",
      "Iteration: 2917 lambda_n: 0.9486839532016302 Loss: 0.02221342287862241\n",
      "Iteration: 2918 lambda_n: 1.0054700664207121 Loss: 0.022202327696366736\n",
      "Iteration: 2919 lambda_n: 0.89239337784183 Loss: 0.02219056866516823\n",
      "Iteration: 2920 lambda_n: 0.9239915171065083 Loss: 0.02218013233818263\n",
      "Iteration: 2921 lambda_n: 1.000485674684965 Loss: 0.0221693267213337\n",
      "Iteration: 2922 lambda_n: 1.015327298745238 Loss: 0.022157626814575677\n",
      "Iteration: 2923 lambda_n: 0.8977887779364389 Loss: 0.022145753642581572\n",
      "Iteration: 2924 lambda_n: 0.9986062918939397 Loss: 0.022135255222698946\n",
      "Iteration: 2925 lambda_n: 0.9484474472491454 Loss: 0.0221235781373398\n",
      "Iteration: 2926 lambda_n: 1.0256774411327283 Loss: 0.02211248785008706\n",
      "Iteration: 2927 lambda_n: 0.9790596942051358 Loss: 0.02210049478242098\n",
      "Iteration: 2928 lambda_n: 0.9080882153295389 Loss: 0.022089047092699574\n",
      "Iteration: 2929 lambda_n: 0.9952826675815124 Loss: 0.02207842949010163\n",
      "Iteration: 2930 lambda_n: 1.0264738331373306 Loss: 0.0220667926407723\n",
      "Iteration: 2931 lambda_n: 1.012834899156274 Loss: 0.022054791389166207\n",
      "Iteration: 2932 lambda_n: 0.9243229414829273 Loss: 0.022042949888625405\n",
      "Iteration: 2933 lambda_n: 0.9560659783393303 Loss: 0.02203214347868813\n",
      "Iteration: 2934 lambda_n: 0.957226436683571 Loss: 0.02202096619805885\n",
      "Iteration: 2935 lambda_n: 0.9623222058131707 Loss: 0.022009775600253034\n",
      "Iteration: 2936 lambda_n: 1.0139923864252702 Loss: 0.021998525679517628\n",
      "Iteration: 2937 lambda_n: 1.0012029372403901 Loss: 0.021986671977589845\n",
      "Iteration: 2938 lambda_n: 0.8882085480643687 Loss: 0.021974968058435018\n",
      "Iteration: 2939 lambda_n: 0.9652650139605123 Loss: 0.021964585264828414\n",
      "Iteration: 2940 lambda_n: 0.965822888237103 Loss: 0.021953301940231228\n",
      "Iteration: 2941 lambda_n: 0.9530241811771867 Loss: 0.02194201234057345\n",
      "Iteration: 2942 lambda_n: 0.8932136987503368 Loss: 0.021930872587959553\n",
      "Iteration: 2943 lambda_n: 0.8763234005180126 Loss: 0.02192043217314637\n",
      "Iteration: 2944 lambda_n: 1.0240113740510928 Loss: 0.02191018938553036\n",
      "Iteration: 2945 lambda_n: 0.9900580094747852 Loss: 0.021898220598907617\n",
      "Iteration: 2946 lambda_n: 0.9941892320142667 Loss: 0.021886648924621593\n",
      "Iteration: 2947 lambda_n: 0.9197570497444608 Loss: 0.021875029216783044\n",
      "Iteration: 2948 lambda_n: 0.9783053995880309 Loss: 0.021864279676634017\n",
      "Iteration: 2949 lambda_n: 1.0178231458434648 Loss: 0.02185284608779853\n",
      "Iteration: 2950 lambda_n: 1.019441065021368 Loss: 0.02184095090010525\n",
      "Iteration: 2951 lambda_n: 0.919673415196264 Loss: 0.0218290370634999\n",
      "Iteration: 2952 lambda_n: 1.0158764969247855 Loss: 0.021818289408258387\n",
      "Iteration: 2953 lambda_n: 0.991283846612089 Loss: 0.021806417718022927\n",
      "Iteration: 2954 lambda_n: 0.9334258112521767 Loss: 0.021794833669018913\n",
      "Iteration: 2955 lambda_n: 0.9133222469486131 Loss: 0.021783925969827846\n",
      "Iteration: 2956 lambda_n: 0.9868840283917026 Loss: 0.021773253401445657\n",
      "Iteration: 2957 lambda_n: 0.9242274990085806 Loss: 0.021761721449655703\n",
      "Iteration: 2958 lambda_n: 0.9908582710561766 Loss: 0.02175092187239263\n",
      "Iteration: 2959 lambda_n: 0.9459140177192018 Loss: 0.021739343935261488\n",
      "Iteration: 2960 lambda_n: 1.015910664816931 Loss: 0.02172829138392379\n",
      "Iteration: 2961 lambda_n: 0.895268349113443 Loss: 0.021716421182881453\n",
      "Iteration: 2962 lambda_n: 0.9755576801865868 Loss: 0.021705960816574957\n",
      "Iteration: 2963 lambda_n: 0.8953208334627679 Loss: 0.0216945625496667\n",
      "Iteration: 2964 lambda_n: 0.9963265103495088 Loss: 0.021684101961307226\n",
      "Iteration: 2965 lambda_n: 1.0067999146684734 Loss: 0.021672461467683604\n",
      "Iteration: 2966 lambda_n: 0.9364977308565703 Loss: 0.021660698840196446\n",
      "Iteration: 2967 lambda_n: 0.8758568661167778 Loss: 0.0216497577821099\n",
      "Iteration: 2968 lambda_n: 0.9634233803587557 Loss: 0.02163952537529424\n",
      "Iteration: 2969 lambda_n: 1.0081114011082177 Loss: 0.02162827014309767\n",
      "Iteration: 2970 lambda_n: 0.9403336823809582 Loss: 0.021616493060423746\n",
      "Iteration: 2971 lambda_n: 0.9451653997162036 Loss: 0.021605507991630274\n",
      "Iteration: 2972 lambda_n: 0.9422675348178444 Loss: 0.021594466676537884\n",
      "Iteration: 2973 lambda_n: 0.893104302631985 Loss: 0.02158345941162013\n",
      "Iteration: 2974 lambda_n: 0.9922380049141323 Loss: 0.021573026641462004\n",
      "Iteration: 2975 lambda_n: 0.9614575758310209 Loss: 0.021561436038619215\n",
      "Iteration: 2976 lambda_n: 0.9107460611670609 Loss: 0.02155020519878209\n",
      "Iteration: 2977 lambda_n: 0.9463355203590819 Loss: 0.02153956691321962\n",
      "Iteration: 2978 lambda_n: 0.9446597097446273 Loss: 0.02152851309884598\n",
      "Iteration: 2979 lambda_n: 1.0191316212705266 Loss: 0.021517479051196282\n",
      "Iteration: 2980 lambda_n: 0.8882091341681816 Loss: 0.02150557534417753\n",
      "Iteration: 2981 lambda_n: 0.9468025982580407 Loss: 0.02149520103639698\n",
      "Iteration: 2982 lambda_n: 0.9938908654937303 Loss: 0.02148414253312393\n",
      "Iteration: 2983 lambda_n: 0.9112009397443529 Loss: 0.02147253424438857\n",
      "Iteration: 2984 lambda_n: 0.9637278889072025 Loss: 0.021461891933843753\n",
      "Iteration: 2985 lambda_n: 1.020784355673568 Loss: 0.021450636321045907\n",
      "Iteration: 2986 lambda_n: 0.9252549340186089 Loss: 0.02143871453554162\n",
      "Iteration: 2987 lambda_n: 0.9236136719571992 Loss: 0.021427908636841776\n",
      "Iteration: 2988 lambda_n: 0.9423700462066394 Loss: 0.021417122081120427\n",
      "Iteration: 2989 lambda_n: 0.9708754854993078 Loss: 0.021406116653630864\n",
      "Iteration: 2990 lambda_n: 0.8838462032389665 Loss: 0.021394778511970768\n",
      "Iteration: 2991 lambda_n: 0.9760388891922276 Loss: 0.021384456894297316\n",
      "Iteration: 2992 lambda_n: 0.9594654298148898 Loss: 0.021373058816893362\n",
      "Iteration: 2993 lambda_n: 0.9907001552569873 Loss: 0.0213618544693199\n",
      "Iteration: 2994 lambda_n: 0.9249926169896133 Loss: 0.021350285560567287\n",
      "Iteration: 2995 lambda_n: 1.011850758768339 Loss: 0.021339484132860072\n",
      "Iteration: 2996 lambda_n: 0.980076136744696 Loss: 0.021327668619087022\n",
      "Iteration: 2997 lambda_n: 1.0260662648073122 Loss: 0.021316224335239175\n",
      "Iteration: 2998 lambda_n: 0.9826443883428742 Loss: 0.021304243222829356\n",
      "Iteration: 2999 lambda_n: 0.9062954353622156 Loss: 0.021292769331060387\n",
      "Iteration: 3000 lambda_n: 0.8862025343135166 Loss: 0.021282187102137178\n",
      "Iteration: 3001 lambda_n: 1.0235141322052324 Loss: 0.021271839638310714\n",
      "Iteration: 3002 lambda_n: 0.9836545603719566 Loss: 0.021259889071152383\n",
      "Iteration: 3003 lambda_n: 0.9347525884084654 Loss: 0.02124840409510619\n",
      "Iteration: 3004 lambda_n: 0.9333547286574363 Loss: 0.021237490262437767\n",
      "Iteration: 3005 lambda_n: 1.00997517099437 Loss: 0.02122659291357117\n",
      "Iteration: 3006 lambda_n: 0.8884465328840533 Loss: 0.021214801160622417\n",
      "Iteration: 3007 lambda_n: 1.0173113352515972 Loss: 0.021204428455537657\n",
      "Iteration: 3008 lambda_n: 1.0258333252050222 Loss: 0.021192551406600577\n",
      "Iteration: 3009 lambda_n: 1.001268060641164 Loss: 0.021180575054688202\n",
      "Iteration: 3010 lambda_n: 0.9601114882798087 Loss: 0.021168885682863996\n",
      "Iteration: 3011 lambda_n: 1.0180772191898644 Loss: 0.021157676969926664\n",
      "Iteration: 3012 lambda_n: 0.947733773134515 Loss: 0.021145791718235708\n",
      "Iteration: 3013 lambda_n: 0.9232461770582048 Loss: 0.021134727843424093\n",
      "Iteration: 3014 lambda_n: 0.917469088450535 Loss: 0.021123949993058114\n",
      "Iteration: 3015 lambda_n: 0.9310001074505656 Loss: 0.021113239733325895\n",
      "Iteration: 3016 lambda_n: 0.9744222066177338 Loss: 0.021102371666683342\n",
      "Iteration: 3017 lambda_n: 0.8983018554100878 Loss: 0.021090996869100886\n",
      "Iteration: 3018 lambda_n: 0.979681302892945 Loss: 0.02108051080534019\n",
      "Iteration: 3019 lambda_n: 1.0267158570166715 Loss: 0.021069074934681416\n",
      "Iteration: 3020 lambda_n: 0.9682662431896254 Loss: 0.0210570902003789\n",
      "Iteration: 3021 lambda_n: 0.9589748449248451 Loss: 0.02104578791177674\n",
      "Iteration: 3022 lambda_n: 1.0211896309923663 Loss: 0.021034594236983936\n",
      "Iteration: 3023 lambda_n: 1.004421313630128 Loss: 0.02102267452321027\n",
      "Iteration: 3024 lambda_n: 0.8924372698056948 Loss: 0.021010950708295836\n",
      "Iteration: 3025 lambda_n: 0.9936264549038012 Loss: 0.0210005341445217\n",
      "Iteration: 3026 lambda_n: 0.8774047670488326 Loss: 0.02098893664409002\n",
      "Iteration: 3027 lambda_n: 0.987942688015698 Loss: 0.020978695814991687\n",
      "Iteration: 3028 lambda_n: 0.949358558184282 Loss: 0.020967164960175807\n",
      "Iteration: 3029 lambda_n: 0.9159353451838849 Loss: 0.020956084596908505\n",
      "Iteration: 3030 lambda_n: 0.9923379348493679 Loss: 0.020945394471716592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3031 lambda_n: 0.9727050110820125 Loss: 0.020933812778878614\n",
      "Iteration: 3032 lambda_n: 1.0070593090778253 Loss: 0.020922460379840274\n",
      "Iteration: 3033 lambda_n: 0.9688661203457389 Loss: 0.02091070719030436\n",
      "Iteration: 3034 lambda_n: 0.977270236507042 Loss: 0.020899399901485597\n",
      "Iteration: 3035 lambda_n: 0.9586062339387892 Loss: 0.020887994681367525\n",
      "Iteration: 3036 lambda_n: 0.9083273968721366 Loss: 0.020876807426982676\n",
      "Iteration: 3037 lambda_n: 0.9755447940655334 Loss: 0.020866207079937513\n",
      "Iteration: 3038 lambda_n: 0.887544402636397 Loss: 0.020854822431767336\n",
      "Iteration: 3039 lambda_n: 0.9571741937496882 Loss: 0.020844464886166345\n",
      "Iteration: 3040 lambda_n: 0.9662808171331847 Loss: 0.020833294899658298\n",
      "Iteration: 3041 lambda_n: 0.987993147585694 Loss: 0.020822018783120188\n",
      "Iteration: 3042 lambda_n: 1.0115631381159211 Loss: 0.020810489438035956\n",
      "Iteration: 3043 lambda_n: 0.9152436433634492 Loss: 0.020798685195723747\n",
      "Iteration: 3044 lambda_n: 0.9407291804823918 Loss: 0.02078800507514806\n",
      "Iteration: 3045 lambda_n: 0.8954903792687308 Loss: 0.020777027689177236\n",
      "Iteration: 3046 lambda_n: 0.9461330265837639 Loss: 0.020766578321465407\n",
      "Iteration: 3047 lambda_n: 0.9935738807064287 Loss: 0.02075553813681011\n",
      "Iteration: 3048 lambda_n: 0.9668137075829233 Loss: 0.020743944515944465\n",
      "Iteration: 3049 lambda_n: 0.9639114156824704 Loss: 0.020732663290242512\n",
      "Iteration: 3050 lambda_n: 0.9069263004515312 Loss: 0.02072141606610955\n",
      "Iteration: 3051 lambda_n: 0.8949487900490722 Loss: 0.020710833889532647\n",
      "Iteration: 3052 lambda_n: 0.9758392897314706 Loss: 0.02070039158608506\n",
      "Iteration: 3053 lambda_n: 0.9042679348197633 Loss: 0.02068900557427908\n",
      "Iteration: 3054 lambda_n: 0.9549752498439912 Loss: 0.020678454777462284\n",
      "Iteration: 3055 lambda_n: 0.9042809432320436 Loss: 0.020667312461971114\n",
      "Iteration: 3056 lambda_n: 0.9699495591711673 Loss: 0.020656761752183887\n",
      "Iteration: 3057 lambda_n: 0.9453005094114042 Loss: 0.020645444976563204\n",
      "Iteration: 3058 lambda_n: 0.8931558166067788 Loss: 0.020634415919467672\n",
      "Iteration: 3059 lambda_n: 0.9102756449430486 Loss: 0.020623995365253424\n",
      "Iteration: 3060 lambda_n: 0.9409955444462651 Loss: 0.020613375184746836\n",
      "Iteration: 3061 lambda_n: 0.9582542950720311 Loss: 0.020602396713561583\n",
      "Iteration: 3062 lambda_n: 0.9441291665043885 Loss: 0.020591217010601102\n",
      "Iteration: 3063 lambda_n: 1.016741003622122 Loss: 0.020580202225382684\n",
      "Iteration: 3064 lambda_n: 0.9739037466777741 Loss: 0.020568340436682587\n",
      "Iteration: 3065 lambda_n: 0.8983445187111296 Loss: 0.020556978541768647\n",
      "Iteration: 3066 lambda_n: 1.0109303328933448 Loss: 0.02054649826423037\n",
      "Iteration: 3067 lambda_n: 0.9397361135456336 Loss: 0.020534704658046823\n",
      "Iteration: 3068 lambda_n: 1.0104239793941006 Loss: 0.020523741736440365\n",
      "Iteration: 3069 lambda_n: 1.0271347588484607 Loss: 0.02051195429867989\n",
      "Iteration: 3070 lambda_n: 1.005828023648543 Loss: 0.020499972052183575\n",
      "Iteration: 3071 lambda_n: 0.9551522493111326 Loss: 0.02048823849871786\n",
      "Iteration: 3072 lambda_n: 0.9299239613436453 Loss: 0.020477096231703993\n",
      "Iteration: 3073 lambda_n: 0.9004898561577452 Loss: 0.020466248378429856\n",
      "Iteration: 3074 lambda_n: 1.0071229333350546 Loss: 0.020455743990801696\n",
      "Iteration: 3075 lambda_n: 0.9098088726242711 Loss: 0.020443995823733112\n",
      "Iteration: 3076 lambda_n: 0.9428802188117632 Loss: 0.02043338294927486\n",
      "Iteration: 3077 lambda_n: 0.9697848747322023 Loss: 0.02042238440773339\n",
      "Iteration: 3078 lambda_n: 0.9430164968261745 Loss: 0.02041107214289378\n",
      "Iteration: 3079 lambda_n: 0.9498592808937177 Loss: 0.020400072238032748\n",
      "Iteration: 3080 lambda_n: 0.9587288849770009 Loss: 0.0203889926263681\n",
      "Iteration: 3081 lambda_n: 0.9459817249213333 Loss: 0.020377809668163063\n",
      "Iteration: 3082 lambda_n: 0.9241806054177805 Loss: 0.02036677550910361\n",
      "Iteration: 3083 lambda_n: 0.8984465133873502 Loss: 0.02035599575064547\n",
      "Iteration: 3084 lambda_n: 0.8999027456621935 Loss: 0.0203455162590241\n",
      "Iteration: 3085 lambda_n: 0.9172610732916757 Loss: 0.020335019879881074\n",
      "Iteration: 3086 lambda_n: 1.0245113823299088 Loss: 0.02032432113440849\n",
      "Iteration: 3087 lambda_n: 0.8764344832660566 Loss: 0.02031237155602004\n",
      "Iteration: 3088 lambda_n: 0.882647483210162 Loss: 0.020302149207156683\n",
      "Iteration: 3089 lambda_n: 0.889266330570994 Loss: 0.02029185448437259\n",
      "Iteration: 3090 lambda_n: 0.9608263814263878 Loss: 0.020281482655613133\n",
      "Iteration: 3091 lambda_n: 0.8809094250153421 Loss: 0.020270276296975226\n",
      "Iteration: 3092 lambda_n: 0.9373919712774661 Loss: 0.020260002128753042\n",
      "Iteration: 3093 lambda_n: 0.9372849460173758 Loss: 0.02024906929280633\n",
      "Iteration: 3094 lambda_n: 1.0121008119831594 Loss: 0.020238137806745406\n",
      "Iteration: 3095 lambda_n: 0.9164293810624988 Loss: 0.02022633385785641\n",
      "Iteration: 3096 lambda_n: 0.9769111160429309 Loss: 0.02021564581375366\n",
      "Iteration: 3097 lambda_n: 0.9722835538974061 Loss: 0.02020425249101039\n",
      "Iteration: 3098 lambda_n: 0.8831393135696624 Loss: 0.02019291324522016\n",
      "Iteration: 3099 lambda_n: 0.9896718481005214 Loss: 0.020182613739801663\n",
      "Iteration: 3100 lambda_n: 0.9194405258804209 Loss: 0.020171071909467365\n",
      "Iteration: 3101 lambda_n: 0.9198046838787137 Loss: 0.020160349237885036\n",
      "Iteration: 3102 lambda_n: 0.9870881465791241 Loss: 0.020149622413144355\n",
      "Iteration: 3103 lambda_n: 0.8935260695382101 Loss: 0.02013811102417625\n",
      "Iteration: 3104 lambda_n: 0.9714063803991247 Loss: 0.02012769084974715\n",
      "Iteration: 3105 lambda_n: 0.9044668583124575 Loss: 0.020116362540824852\n",
      "Iteration: 3106 lambda_n: 0.9443006210217761 Loss: 0.020105814959936946\n",
      "Iteration: 3107 lambda_n: 0.9308614120363211 Loss: 0.02009480294359921\n",
      "Iteration: 3108 lambda_n: 1.0030965903056548 Loss: 0.02008394774374797\n",
      "Iteration: 3109 lambda_n: 0.9053737328608209 Loss: 0.02007225027622435\n",
      "Iteration: 3110 lambda_n: 0.9214473601305012 Loss: 0.02006169248631276\n",
      "Iteration: 3111 lambda_n: 1.0210176270284348 Loss: 0.020050947345929604\n",
      "Iteration: 3112 lambda_n: 0.9371677112108058 Loss: 0.020039041200114013\n",
      "Iteration: 3113 lambda_n: 0.9465236989598601 Loss: 0.020028112933070233\n",
      "Iteration: 3114 lambda_n: 0.9897778105373473 Loss: 0.02001707565851281\n",
      "Iteration: 3115 lambda_n: 0.9299122371221333 Loss: 0.020005534100945792\n",
      "Iteration: 3116 lambda_n: 0.9377033177457416 Loss: 0.019994690715882654\n",
      "Iteration: 3117 lambda_n: 1.0203519172636484 Loss: 0.019983756570899874\n",
      "Iteration: 3118 lambda_n: 0.9828417361139206 Loss: 0.019971858794373323\n",
      "Iteration: 3119 lambda_n: 0.9587776646277598 Loss: 0.019960398505437973\n",
      "Iteration: 3120 lambda_n: 0.8904523069997432 Loss: 0.01994921890706245\n",
      "Iteration: 3121 lambda_n: 0.8970348195651815 Loss: 0.01993883608565347\n",
      "Iteration: 3122 lambda_n: 0.8789837555445293 Loss: 0.019928376590548046\n",
      "Iteration: 3123 lambda_n: 1.005131385508818 Loss: 0.019918127650312735\n",
      "Iteration: 3124 lambda_n: 0.9889634307079855 Loss: 0.019906407917236853\n",
      "Iteration: 3125 lambda_n: 0.914525092663077 Loss: 0.01989487679836756\n",
      "Iteration: 3126 lambda_n: 1.0221048528494276 Loss: 0.019884213704020395\n",
      "Iteration: 3127 lambda_n: 0.9770313457079859 Loss: 0.01987229635182736\n",
      "Iteration: 3128 lambda_n: 0.8964109852707944 Loss: 0.01986090463580995\n",
      "Iteration: 3129 lambda_n: 0.9901069598571166 Loss: 0.019850452998425624\n",
      "Iteration: 3130 lambda_n: 0.9802346726926213 Loss: 0.019838909004400804\n",
      "Iteration: 3131 lambda_n: 0.9310199011455949 Loss: 0.01982748020680232\n",
      "Iteration: 3132 lambda_n: 1.0149623664869385 Loss: 0.019816625302445507\n",
      "Iteration: 3133 lambda_n: 0.968265414910892 Loss: 0.019804791788612747\n",
      "Iteration: 3134 lambda_n: 0.9777177920566875 Loss: 0.01979350280936167\n",
      "Iteration: 3135 lambda_n: 0.9157187892480011 Loss: 0.019782103712953365\n",
      "Iteration: 3136 lambda_n: 0.941737820252606 Loss: 0.01977142753822729\n",
      "Iteration: 3137 lambda_n: 0.91100219437963 Loss: 0.019760448092178282\n",
      "Iteration: 3138 lambda_n: 0.8908485817490271 Loss: 0.01974982706213844\n",
      "Iteration: 3139 lambda_n: 0.9730504547648615 Loss: 0.01973944106914893\n",
      "Iteration: 3140 lambda_n: 0.9841924776796358 Loss: 0.01972809680073033\n",
      "Iteration: 3141 lambda_n: 0.9805876301769555 Loss: 0.01971662271958958\n",
      "Iteration: 3142 lambda_n: 0.9868704874226308 Loss: 0.019705190751380282\n",
      "Iteration: 3143 lambda_n: 0.9235057640650925 Loss: 0.019693685621878856\n",
      "Iteration: 3144 lambda_n: 0.8801322674558378 Loss: 0.019682919291317724\n",
      "Iteration: 3145 lambda_n: 0.8767619049939195 Loss: 0.019672658685158856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3146 lambda_n: 0.9090963699918949 Loss: 0.019662437438272944\n",
      "Iteration: 3147 lambda_n: 1.0015257500024992 Loss: 0.019651839307164843\n",
      "Iteration: 3148 lambda_n: 0.8956667625635202 Loss: 0.01964016372482309\n",
      "Iteration: 3149 lambda_n: 0.9086181421864516 Loss: 0.019629722302191635\n",
      "Iteration: 3150 lambda_n: 1.0260859773330249 Loss: 0.01961912996586494\n",
      "Iteration: 3151 lambda_n: 0.8902067321098038 Loss: 0.019607168312163827\n",
      "Iteration: 3152 lambda_n: 0.9072632624694428 Loss: 0.019596790755697136\n",
      "Iteration: 3153 lambda_n: 1.0131434184549566 Loss: 0.019586214431369566\n",
      "Iteration: 3154 lambda_n: 0.9545193757607151 Loss: 0.01957440389757243\n",
      "Iteration: 3155 lambda_n: 1.0149772332520095 Loss: 0.01956327684352549\n",
      "Iteration: 3156 lambda_n: 0.9611973921202005 Loss: 0.019551445098589255\n",
      "Iteration: 3157 lambda_n: 0.9566055995031054 Loss: 0.019540240354014856\n",
      "Iteration: 3158 lambda_n: 0.8838675528426658 Loss: 0.019529089211743508\n",
      "Iteration: 3159 lambda_n: 0.889738716927213 Loss: 0.019518786045194202\n",
      "Iteration: 3160 lambda_n: 0.9754432635679557 Loss: 0.01950841450284191\n",
      "Iteration: 3161 lambda_n: 0.9966765750747058 Loss: 0.0194970439863606\n",
      "Iteration: 3162 lambda_n: 0.8850947123993914 Loss: 0.019485426036216384\n",
      "Iteration: 3163 lambda_n: 0.9006792493584261 Loss: 0.019475108831796595\n",
      "Iteration: 3164 lambda_n: 0.8935841547265252 Loss: 0.019464610027859017\n",
      "Iteration: 3165 lambda_n: 0.9038187816530638 Loss: 0.019454193991827518\n",
      "Iteration: 3166 lambda_n: 0.9090722569822632 Loss: 0.019443658719731572\n",
      "Iteration: 3167 lambda_n: 0.9980902295161731 Loss: 0.019433062275355795\n",
      "Iteration: 3168 lambda_n: 0.8840515282941523 Loss: 0.019421428278967545\n",
      "Iteration: 3169 lambda_n: 0.9327013016021036 Loss: 0.01941112361538081\n",
      "Iteration: 3170 lambda_n: 0.8894839812893829 Loss: 0.01940025194482264\n",
      "Iteration: 3171 lambda_n: 1.0181591290227525 Loss: 0.019389884083784066\n",
      "Iteration: 3172 lambda_n: 0.9095946528298952 Loss: 0.01937801644912084\n",
      "Iteration: 3173 lambda_n: 0.9558350595177262 Loss: 0.01936741430926802\n",
      "Iteration: 3174 lambda_n: 1.00259789035017 Loss: 0.01935627326168218\n",
      "Iteration: 3175 lambda_n: 0.9467018291309736 Loss: 0.019344587226587922\n",
      "Iteration: 3176 lambda_n: 1.0068141333810154 Loss: 0.019333552773134883\n",
      "Iteration: 3177 lambda_n: 0.931289416302888 Loss: 0.019321817740664923\n",
      "Iteration: 3178 lambda_n: 0.9538374072085801 Loss: 0.01931096306404606\n",
      "Iteration: 3179 lambda_n: 0.989491049662694 Loss: 0.01929984564374358\n",
      "Iteration: 3180 lambda_n: 0.9719148838299065 Loss: 0.01928831273252392\n",
      "Iteration: 3181 lambda_n: 0.9423146878102721 Loss: 0.019276984748358695\n",
      "Iteration: 3182 lambda_n: 1.0187121891593578 Loss: 0.019266001830266378\n",
      "Iteration: 3183 lambda_n: 0.9190843295776752 Loss: 0.0192541285487692\n",
      "Iteration: 3184 lambda_n: 0.9672557659724877 Loss: 0.019243416515555\n",
      "Iteration: 3185 lambda_n: 0.9295276359155868 Loss: 0.01923214310189026\n",
      "Iteration: 3186 lambda_n: 1.0202746402686143 Loss: 0.019221309474991642\n",
      "Iteration: 3187 lambda_n: 1.0093446704600466 Loss: 0.019209418260152315\n",
      "Iteration: 3188 lambda_n: 0.8826500307197896 Loss: 0.01919765450520138\n",
      "Iteration: 3189 lambda_n: 0.9106799852286527 Loss: 0.01918736741848729\n",
      "Iteration: 3190 lambda_n: 0.94120357920062 Loss: 0.01917675370455339\n",
      "Iteration: 3191 lambda_n: 1.0037080520163784 Loss: 0.019165784305849053\n",
      "Iteration: 3192 lambda_n: 0.9056496763311813 Loss: 0.019154086504012124\n",
      "Iteration: 3193 lambda_n: 0.9087459779023079 Loss: 0.019143531593826802\n",
      "Iteration: 3194 lambda_n: 0.998267338287285 Loss: 0.01913294065344552\n",
      "Iteration: 3195 lambda_n: 0.937851041453249 Loss: 0.019121306451361394\n",
      "Iteration: 3196 lambda_n: 0.9558637999489963 Loss: 0.01911037642740015\n",
      "Iteration: 3197 lambda_n: 0.9380957563136753 Loss: 0.019099236536557648\n",
      "Iteration: 3198 lambda_n: 0.9329341536867554 Loss: 0.019088303778639567\n",
      "Iteration: 3199 lambda_n: 0.8812781809141839 Loss: 0.01907743123276616\n",
      "Iteration: 3200 lambda_n: 1.021906736291246 Loss: 0.019067160746768733\n",
      "Iteration: 3201 lambda_n: 0.9851664372844802 Loss: 0.01905525142313685\n",
      "Iteration: 3202 lambda_n: 0.9388803756977402 Loss: 0.01904377033708939\n",
      "Iteration: 3203 lambda_n: 0.9810550780015114 Loss: 0.01903282872643091\n",
      "Iteration: 3204 lambda_n: 0.9232388761248677 Loss: 0.019021395675436842\n",
      "Iteration: 3205 lambda_n: 0.8907995046043142 Loss: 0.019010636462547605\n",
      "Iteration: 3206 lambda_n: 0.9804683061412028 Loss: 0.019000255342807998\n",
      "Iteration: 3207 lambda_n: 0.9492911528450789 Loss: 0.018988829303938547\n",
      "Iteration: 3208 lambda_n: 0.9073438713752675 Loss: 0.018977766651311356\n",
      "Iteration: 3209 lambda_n: 1.0162896597007236 Loss: 0.01896719288903543\n",
      "Iteration: 3210 lambda_n: 0.9567655483366636 Loss: 0.018955349580656443\n",
      "Iteration: 3211 lambda_n: 0.9228392975882651 Loss: 0.01894419999526405\n",
      "Iteration: 3212 lambda_n: 0.9426854251273896 Loss: 0.01893344582080157\n",
      "Iteration: 3213 lambda_n: 0.9035568280783789 Loss: 0.01892246042554311\n",
      "Iteration: 3214 lambda_n: 0.9166277977680853 Loss: 0.01891193105933383\n",
      "Iteration: 3215 lambda_n: 0.9265022254038017 Loss: 0.01890124942405715\n",
      "Iteration: 3216 lambda_n: 1.0031306976755723 Loss: 0.01889045277134105\n",
      "Iteration: 3217 lambda_n: 0.925860266867353 Loss: 0.018878763212572324\n",
      "Iteration: 3218 lambda_n: 0.907583886441563 Loss: 0.018867974147365265\n",
      "Iteration: 3219 lambda_n: 0.9848947239057845 Loss: 0.01885739810689312\n",
      "Iteration: 3220 lambda_n: 1.0008865580612665 Loss: 0.018845921218840125\n",
      "Iteration: 3221 lambda_n: 0.9620263165286906 Loss: 0.018834258037189203\n",
      "Iteration: 3222 lambda_n: 0.9350610409648927 Loss: 0.018823047744258456\n",
      "Iteration: 3223 lambda_n: 0.9092469639182797 Loss: 0.018812151724224808\n",
      "Iteration: 3224 lambda_n: 0.9379841502732054 Loss: 0.018801556557843855\n",
      "Iteration: 3225 lambda_n: 1.0230010365081366 Loss: 0.01879062657505404\n",
      "Iteration: 3226 lambda_n: 0.9021319112366549 Loss: 0.018778705976638606\n",
      "Iteration: 3227 lambda_n: 0.9260734086372355 Loss: 0.018768193867391845\n",
      "Iteration: 3228 lambda_n: 0.9798049046687213 Loss: 0.01875740282652768\n",
      "Iteration: 3229 lambda_n: 0.9437631327789887 Loss: 0.018745985732053685\n",
      "Iteration: 3230 lambda_n: 0.9369654192270955 Loss: 0.018734988662940093\n",
      "Iteration: 3231 lambda_n: 0.914689594829328 Loss: 0.018724070852347587\n",
      "Iteration: 3232 lambda_n: 0.919048245121041 Loss: 0.018713412653884372\n",
      "Iteration: 3233 lambda_n: 0.9528605571488672 Loss: 0.0187027037134963\n",
      "Iteration: 3234 lambda_n: 1.0109614835708922 Loss: 0.01869160083291866\n",
      "Iteration: 3235 lambda_n: 0.8838673983792452 Loss: 0.018679821003622407\n",
      "Iteration: 3236 lambda_n: 0.8785813596985237 Loss: 0.018669522136273795\n",
      "Iteration: 3237 lambda_n: 0.9692206795449172 Loss: 0.018659284903875518\n",
      "Iteration: 3238 lambda_n: 0.8863742915082439 Loss: 0.01864799158744198\n",
      "Iteration: 3239 lambda_n: 0.9007184451020166 Loss: 0.018637663639195997\n",
      "Iteration: 3240 lambda_n: 0.9517241263169378 Loss: 0.018627168596525853\n",
      "Iteration: 3241 lambda_n: 0.9326354055691397 Loss: 0.01861607928811172\n",
      "Iteration: 3242 lambda_n: 0.9223747759413714 Loss: 0.018605212444370647\n",
      "Iteration: 3243 lambda_n: 0.9234396895306175 Loss: 0.018594465199876068\n",
      "Iteration: 3244 lambda_n: 0.9842239404971316 Loss: 0.0185837055914947\n",
      "Iteration: 3245 lambda_n: 1.0092561649578318 Loss: 0.01857223779238055\n",
      "Iteration: 3246 lambda_n: 1.016268028078152 Loss: 0.018560478378394394\n",
      "Iteration: 3247 lambda_n: 0.8817396460114344 Loss: 0.01854863731758354\n",
      "Iteration: 3248 lambda_n: 0.9095606254683536 Loss: 0.018538363761534237\n",
      "Iteration: 3249 lambda_n: 0.9836209029205181 Loss: 0.018527766090869542\n",
      "Iteration: 3250 lambda_n: 0.9702800332617495 Loss: 0.01851630555790964\n",
      "Iteration: 3251 lambda_n: 0.9336527858550692 Loss: 0.018505000512060794\n",
      "Iteration: 3252 lambda_n: 0.8978518362408137 Loss: 0.01849412226709797\n",
      "Iteration: 3253 lambda_n: 0.9151859819189753 Loss: 0.018483661190246005\n",
      "Iteration: 3254 lambda_n: 0.9066702794839825 Loss: 0.01847299818968704\n",
      "Iteration: 3255 lambda_n: 0.911970707730343 Loss: 0.01846243444772116\n",
      "Iteration: 3256 lambda_n: 0.885104112676388 Loss: 0.01845180898994937\n",
      "Iteration: 3257 lambda_n: 1.0149972543541306 Loss: 0.018441496596507144\n",
      "Iteration: 3258 lambda_n: 0.8825462036143827 Loss: 0.01842967085466061\n",
      "Iteration: 3259 lambda_n: 0.9372954916080065 Loss: 0.018419388344074875\n",
      "Iteration: 3260 lambda_n: 0.8864496024961411 Loss: 0.01840846799112395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3261 lambda_n: 0.9309031389239194 Loss: 0.01839814007884063\n",
      "Iteration: 3262 lambda_n: 0.8847353499444026 Loss: 0.018387294282928464\n",
      "Iteration: 3263 lambda_n: 0.9781034644225094 Loss: 0.018376986418797703\n",
      "Iteration: 3264 lambda_n: 0.9985273593083568 Loss: 0.018365590782927748\n",
      "Iteration: 3265 lambda_n: 0.9170902653387158 Loss: 0.01835395723877767\n",
      "Iteration: 3266 lambda_n: 0.9815164780750665 Loss: 0.018343272536182016\n",
      "Iteration: 3267 lambda_n: 0.9077378947156366 Loss: 0.01833183726731864\n",
      "Iteration: 3268 lambda_n: 0.8957400694314095 Loss: 0.018321261604893155\n",
      "Iteration: 3269 lambda_n: 1.0241796679210564 Loss: 0.018310825760888154\n",
      "Iteration: 3270 lambda_n: 1.0176483707697044 Loss: 0.01829889356931417\n",
      "Iteration: 3271 lambda_n: 0.964229294149126 Loss: 0.01828703751739692\n",
      "Iteration: 3272 lambda_n: 0.8789577326351115 Loss: 0.018275803865128174\n",
      "Iteration: 3273 lambda_n: 0.884136925567422 Loss: 0.0182655636979164\n",
      "Iteration: 3274 lambda_n: 0.9080019025199413 Loss: 0.01825526322563797\n",
      "Iteration: 3275 lambda_n: 0.8845063194026033 Loss: 0.01824468475421339\n",
      "Iteration: 3276 lambda_n: 1.02537186295344 Loss: 0.01823438004793557\n",
      "Iteration: 3277 lambda_n: 1.0158497673758735 Loss: 0.01822243426449856\n",
      "Iteration: 3278 lambda_n: 0.9438958038096786 Loss: 0.018210599460476706\n",
      "Iteration: 3279 lambda_n: 0.8960755617859212 Loss: 0.01819960297236141\n",
      "Iteration: 3280 lambda_n: 0.9881508220546036 Loss: 0.018189163631460985\n",
      "Iteration: 3281 lambda_n: 0.9350062613172427 Loss: 0.01817765164527543\n",
      "Iteration: 3282 lambda_n: 1.0086808405203773 Loss: 0.018166758833952565\n",
      "Iteration: 3283 lambda_n: 0.97071016881948 Loss: 0.01815500775445338\n",
      "Iteration: 3284 lambda_n: 1.0010270058816724 Loss: 0.01814369907236725\n",
      "Iteration: 3285 lambda_n: 0.928189096429293 Loss: 0.01813203724249636\n",
      "Iteration: 3286 lambda_n: 0.9632419146643387 Loss: 0.01812122400299239\n",
      "Iteration: 3287 lambda_n: 0.9506538566659826 Loss: 0.018110002441192458\n",
      "Iteration: 3288 lambda_n: 1.0101947313509858 Loss: 0.01809892756512726\n",
      "Iteration: 3289 lambda_n: 1.0060633544223192 Loss: 0.01808715909216367\n",
      "Iteration: 3290 lambda_n: 0.9230835668820925 Loss: 0.018075438789767993\n",
      "Iteration: 3291 lambda_n: 0.9936392133128935 Loss: 0.01806468521167553\n",
      "Iteration: 3292 lambda_n: 0.8808660093767547 Loss: 0.018053109723500042\n",
      "Iteration: 3293 lambda_n: 0.9822538427214479 Loss: 0.018042848031696965\n",
      "Iteration: 3294 lambda_n: 1.0160450116825301 Loss: 0.018031405251687893\n",
      "Iteration: 3295 lambda_n: 0.9364984891389564 Loss: 0.018019568860385245\n",
      "Iteration: 3296 lambda_n: 0.8782420430548679 Loss: 0.018008659181697702\n",
      "Iteration: 3297 lambda_n: 0.9135062268522207 Loss: 0.017998428189828084\n",
      "Iteration: 3298 lambda_n: 0.9887340815642842 Loss: 0.017987786422572066\n",
      "Iteration: 3299 lambda_n: 0.9466887834663046 Loss: 0.01797626833349858\n",
      "Iteration: 3300 lambda_n: 0.9589609801761821 Loss: 0.01796524007993979\n",
      "Iteration: 3301 lambda_n: 0.8784407997647888 Loss: 0.017954068898691118\n",
      "Iteration: 3302 lambda_n: 0.9389409632953972 Loss: 0.017943835749610705\n",
      "Iteration: 3303 lambda_n: 0.9919045128510241 Loss: 0.01793289785208212\n",
      "Iteration: 3304 lambda_n: 0.938696064680074 Loss: 0.01792134300738494\n",
      "Iteration: 3305 lambda_n: 0.9266247787583796 Loss: 0.017910408030749236\n",
      "Iteration: 3306 lambda_n: 0.9646238966281531 Loss: 0.017899613706267248\n",
      "Iteration: 3307 lambda_n: 0.9705135008926257 Loss: 0.017888376760187716\n",
      "Iteration: 3308 lambda_n: 0.8749963685737712 Loss: 0.017877071240333244\n",
      "Iteration: 3309 lambda_n: 0.9315279373287261 Loss: 0.017866878431458025\n",
      "Iteration: 3310 lambda_n: 0.9527472916313356 Loss: 0.01785602711757385\n",
      "Iteration: 3311 lambda_n: 0.9800063236136083 Loss: 0.017844928652861247\n",
      "Iteration: 3312 lambda_n: 0.947307908267033 Loss: 0.01783351268388923\n",
      "Iteration: 3313 lambda_n: 0.9396564402925455 Loss: 0.0178224776478976\n",
      "Iteration: 3314 lambda_n: 1.0165691843470528 Loss: 0.017811531774366837\n",
      "Iteration: 3315 lambda_n: 0.8786566882133148 Loss: 0.01779968999326196\n",
      "Iteration: 3316 lambda_n: 1.0090434338831673 Loss: 0.017789454754695316\n",
      "Iteration: 3317 lambda_n: 1.0071734345879337 Loss: 0.017777700706644712\n",
      "Iteration: 3318 lambda_n: 0.9073871444953333 Loss: 0.01776596847714777\n",
      "Iteration: 3319 lambda_n: 0.8998177031293538 Loss: 0.017755398656797792\n",
      "Iteration: 3320 lambda_n: 1.0121266197021055 Loss: 0.01774491703828291\n",
      "Iteration: 3321 lambda_n: 0.9455503511528466 Loss: 0.01773312720914962\n",
      "Iteration: 3322 lambda_n: 1.0181293349586478 Loss: 0.017722112931132104\n",
      "Iteration: 3323 lambda_n: 0.925910257267192 Loss: 0.01771025324686261\n",
      "Iteration: 3324 lambda_n: 0.9672678240854882 Loss: 0.017699467808804645\n",
      "Iteration: 3325 lambda_n: 0.9900311628294819 Loss: 0.01768820064845435\n",
      "Iteration: 3326 lambda_n: 0.8852486922097005 Loss: 0.017676668362747977\n",
      "Iteration: 3327 lambda_n: 0.9823830509306511 Loss: 0.017666356655017743\n",
      "Iteration: 3328 lambda_n: 1.019012776159303 Loss: 0.017654913518710857\n",
      "Iteration: 3329 lambda_n: 0.874007976984907 Loss: 0.017643043739669708\n",
      "Iteration: 3330 lambda_n: 0.942592529101725 Loss: 0.01763286305094547\n",
      "Iteration: 3331 lambda_n: 0.9446242005848492 Loss: 0.01762188349701969\n",
      "Iteration: 3332 lambda_n: 0.8862225520466483 Loss: 0.017610880306491054\n",
      "Iteration: 3333 lambda_n: 0.8836391402667579 Loss: 0.017600557418162764\n",
      "Iteration: 3334 lambda_n: 1.0197060629562302 Loss: 0.01759026464697913\n",
      "Iteration: 3335 lambda_n: 1.020617112628583 Loss: 0.017578386975047586\n",
      "Iteration: 3336 lambda_n: 0.9788979257794491 Loss: 0.01756649872408009\n",
      "Iteration: 3337 lambda_n: 0.9719262744257563 Loss: 0.017555096453894286\n",
      "Iteration: 3338 lambda_n: 1.0158136716645625 Loss: 0.017543775419796383\n",
      "Iteration: 3339 lambda_n: 0.9633420254702109 Loss: 0.017531943214399753\n",
      "Iteration: 3340 lambda_n: 0.9917081488780363 Loss: 0.017520722229467843\n",
      "Iteration: 3341 lambda_n: 0.9745767188357622 Loss: 0.01750917086604746\n",
      "Iteration: 3342 lambda_n: 0.9649052314318481 Loss: 0.01749781907822891\n",
      "Iteration: 3343 lambda_n: 0.9270947560045506 Loss: 0.017486579971744097\n",
      "Iteration: 3344 lambda_n: 1.0156243711319557 Loss: 0.017475781304494453\n",
      "Iteration: 3345 lambda_n: 0.9246430310429775 Loss: 0.017463951485321545\n",
      "Iteration: 3346 lambda_n: 0.9740590628881459 Loss: 0.017453181429416525\n",
      "Iteration: 3347 lambda_n: 1.0051828138558592 Loss: 0.01744183581237349\n",
      "Iteration: 3348 lambda_n: 1.0254506510481982 Loss: 0.017430127702043087\n",
      "Iteration: 3349 lambda_n: 0.8999357810385868 Loss: 0.01741818354757439\n",
      "Iteration: 3350 lambda_n: 0.9838166531383503 Loss: 0.017407701381306092\n",
      "Iteration: 3351 lambda_n: 0.8810810753781408 Loss: 0.017396242223088294\n",
      "Iteration: 3352 lambda_n: 0.880348363674967 Loss: 0.017385979718717217\n",
      "Iteration: 3353 lambda_n: 1.0087003759559758 Loss: 0.01737572577107886\n",
      "Iteration: 3354 lambda_n: 0.9723720150188837 Loss: 0.017363976855739066\n",
      "Iteration: 3355 lambda_n: 0.898041354231606 Loss: 0.017352651105801063\n",
      "Iteration: 3356 lambda_n: 0.9161868607767244 Loss: 0.01734219115062259\n",
      "Iteration: 3357 lambda_n: 0.950756799678518 Loss: 0.01733151986852486\n",
      "Iteration: 3358 lambda_n: 0.8939477935915976 Loss: 0.017320445957710304\n",
      "Iteration: 3359 lambda_n: 0.9976124909882064 Loss: 0.01731003375182505\n",
      "Iteration: 3360 lambda_n: 0.9030787014961741 Loss: 0.0172984141420975\n",
      "Iteration: 3361 lambda_n: 0.9622331977028155 Loss: 0.017287895631864253\n",
      "Iteration: 3362 lambda_n: 0.9967424260453757 Loss: 0.017276688150027083\n",
      "Iteration: 3363 lambda_n: 0.9228564762536244 Loss: 0.01726507875284626\n",
      "Iteration: 3364 lambda_n: 0.9680748086682933 Loss: 0.01725432995543258\n",
      "Iteration: 3365 lambda_n: 1.0104783668162376 Loss: 0.017243054510175215\n",
      "Iteration: 3366 lambda_n: 0.9412442249595147 Loss: 0.01723128520484675\n",
      "Iteration: 3367 lambda_n: 0.98375852756506 Loss: 0.01722032231311831\n",
      "Iteration: 3368 lambda_n: 1.009537242758022 Loss: 0.017208864272020574\n",
      "Iteration: 3369 lambda_n: 0.904102355640407 Loss: 0.01719710600715126\n",
      "Iteration: 3370 lambda_n: 0.939177300610365 Loss: 0.017186575785737453\n",
      "Iteration: 3371 lambda_n: 0.9495936114471953 Loss: 0.017175637063201077\n",
      "Iteration: 3372 lambda_n: 0.9830512388017155 Loss: 0.017164577043753897\n",
      "Iteration: 3373 lambda_n: 1.010244081649485 Loss: 0.017153127363903516\n",
      "Iteration: 3374 lambda_n: 0.9974533906851631 Loss: 0.017141360992349582\n",
      "Iteration: 3375 lambda_n: 1.0198375947916034 Loss: 0.01712974362055544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3376 lambda_n: 1.0140558468810073 Loss: 0.017117865565149885\n",
      "Iteration: 3377 lambda_n: 0.8890229861187743 Loss: 0.017106054876020405\n",
      "Iteration: 3378 lambda_n: 0.9939488912792493 Loss: 0.017095700464962494\n",
      "Iteration: 3379 lambda_n: 1.0098039310190086 Loss: 0.017084124008714605\n",
      "Iteration: 3380 lambda_n: 1.0152413198888701 Loss: 0.017072362914908653\n",
      "Iteration: 3381 lambda_n: 0.911796461656667 Loss: 0.01706053851775513\n",
      "Iteration: 3382 lambda_n: 0.9995202193306502 Loss: 0.017049918953615406\n",
      "Iteration: 3383 lambda_n: 0.8816385028075738 Loss: 0.017038277705647153\n",
      "Iteration: 3384 lambda_n: 0.9977655128292342 Loss: 0.017028009428178333\n",
      "Iteration: 3385 lambda_n: 1.0025105144189164 Loss: 0.017016388662645168\n",
      "Iteration: 3386 lambda_n: 0.9146806790760108 Loss: 0.017004712657212\n",
      "Iteration: 3387 lambda_n: 0.9817567315468482 Loss: 0.016994059607327392\n",
      "Iteration: 3388 lambda_n: 0.9365422178920486 Loss: 0.016982625361391075\n",
      "Iteration: 3389 lambda_n: 0.9550487500976745 Loss: 0.01697171773806838\n",
      "Iteration: 3390 lambda_n: 0.9601893809582535 Loss: 0.01696059459588868\n",
      "Iteration: 3391 lambda_n: 0.96061822561902 Loss: 0.016949411603985883\n",
      "Iteration: 3392 lambda_n: 0.9265992693598776 Loss: 0.016938223639020027\n",
      "Iteration: 3393 lambda_n: 1.0249812062497123 Loss: 0.016927431900923464\n",
      "Iteration: 3394 lambda_n: 0.9154173540881878 Loss: 0.016915494368953968\n",
      "Iteration: 3395 lambda_n: 1.010621821678738 Loss: 0.016904832903396186\n",
      "Iteration: 3396 lambda_n: 0.9805962700399161 Loss: 0.016893062654181272\n",
      "Iteration: 3397 lambda_n: 0.9529308905564269 Loss: 0.01688164212133307\n",
      "Iteration: 3398 lambda_n: 0.969614444724081 Loss: 0.016870543814965597\n",
      "Iteration: 3399 lambda_n: 0.8739648187392952 Loss: 0.016859251224409254\n",
      "Iteration: 3400 lambda_n: 1.006567553248576 Loss: 0.016849072633791647\n",
      "Iteration: 3401 lambda_n: 0.8821030322395923 Loss: 0.016837349711152378\n",
      "Iteration: 3402 lambda_n: 0.960613196706909 Loss: 0.016827076375994246\n",
      "Iteration: 3403 lambda_n: 0.9501850193285591 Loss: 0.016815888697771474\n",
      "Iteration: 3404 lambda_n: 0.9047127018654844 Loss: 0.016804822490174006\n",
      "Iteration: 3405 lambda_n: 0.9395901058630546 Loss: 0.016794285888787498\n",
      "Iteration: 3406 lambda_n: 0.9170622952705463 Loss: 0.016783343111330666\n",
      "Iteration: 3407 lambda_n: 0.9427264715652314 Loss: 0.016772662718778027\n",
      "Iteration: 3408 lambda_n: 0.9751971541323319 Loss: 0.016761683451777493\n",
      "Iteration: 3409 lambda_n: 0.967939633626033 Loss: 0.016750326041352\n",
      "Iteration: 3410 lambda_n: 0.956108201811979 Loss: 0.01673905317399439\n",
      "Iteration: 3411 lambda_n: 1.0036440314116728 Loss: 0.016727918117971332\n",
      "Iteration: 3412 lambda_n: 1.0139459391696142 Loss: 0.01671622946886999\n",
      "Iteration: 3413 lambda_n: 0.9639781914175652 Loss: 0.01670442086281696\n",
      "Iteration: 3414 lambda_n: 1.0249932612343606 Loss: 0.016693194210873178\n",
      "Iteration: 3415 lambda_n: 1.0187091197549205 Loss: 0.0166812569875555\n",
      "Iteration: 3416 lambda_n: 0.9890959413788869 Loss: 0.016669392971698442\n",
      "Iteration: 3417 lambda_n: 0.880535120006494 Loss: 0.016657873855228524\n",
      "Iteration: 3418 lambda_n: 0.8920340502615512 Loss: 0.016647619067283504\n",
      "Iteration: 3419 lambda_n: 1.0197836990089986 Loss: 0.01663723037764292\n",
      "Iteration: 3420 lambda_n: 0.8999485618956562 Loss: 0.016625353925095482\n",
      "Iteration: 3421 lambda_n: 0.9697869522381266 Loss: 0.016614873096937887\n",
      "Iteration: 3422 lambda_n: 0.9872359769279766 Loss: 0.016603578946097586\n",
      "Iteration: 3423 lambda_n: 0.9964863204662895 Loss: 0.016592081602606116\n",
      "Iteration: 3424 lambda_n: 0.9513711804516156 Loss: 0.01658047654899704\n",
      "Iteration: 3425 lambda_n: 0.9231860688795588 Loss: 0.016569396923642867\n",
      "Iteration: 3426 lambda_n: 0.9094952770781974 Loss: 0.016558645557868627\n",
      "Iteration: 3427 lambda_n: 1.0262647957361077 Loss: 0.016548053650396532\n",
      "Iteration: 3428 lambda_n: 0.9803058067178957 Loss: 0.016536101872613895\n",
      "Iteration: 3429 lambda_n: 0.9544945218406575 Loss: 0.016524685347881138\n",
      "Iteration: 3430 lambda_n: 0.9169134436753024 Loss: 0.016513569436083956\n",
      "Iteration: 3431 lambda_n: 0.9123331496686594 Loss: 0.01650289120489175\n",
      "Iteration: 3432 lambda_n: 0.911937223763785 Loss: 0.016492266330817025\n",
      "Iteration: 3433 lambda_n: 0.9015576919314002 Loss: 0.01648164608318736\n",
      "Iteration: 3434 lambda_n: 0.9571861600692746 Loss: 0.016471146728910813\n",
      "Iteration: 3435 lambda_n: 0.9479951124498066 Loss: 0.016459999552982378\n",
      "Iteration: 3436 lambda_n: 0.9479059090419418 Loss: 0.016448959430650344\n",
      "Iteration: 3437 lambda_n: 0.9156017344039287 Loss: 0.01643792036361991\n",
      "Iteration: 3438 lambda_n: 1.0273528137359238 Loss: 0.016427257518464335\n",
      "Iteration: 3439 lambda_n: 1.0032058725895434 Loss: 0.016415293268121103\n",
      "Iteration: 3440 lambda_n: 0.9376846170124902 Loss: 0.016403610244572125\n",
      "Iteration: 3441 lambda_n: 0.9236770847430179 Loss: 0.016392690278046698\n",
      "Iteration: 3442 lambda_n: 0.9406345342002194 Loss: 0.01638193345406997\n",
      "Iteration: 3443 lambda_n: 1.0157285495080677 Loss: 0.016370979164903248\n",
      "Iteration: 3444 lambda_n: 1.0224839308624478 Loss: 0.016359150374877344\n",
      "Iteration: 3445 lambda_n: 0.9892877935239366 Loss: 0.01634724293245799\n",
      "Iteration: 3446 lambda_n: 0.910508483843194 Loss: 0.016335722096734645\n",
      "Iteration: 3447 lambda_n: 0.962829602221607 Loss: 0.016325118707829836\n",
      "Iteration: 3448 lambda_n: 0.9427341340954902 Loss: 0.016313906024876056\n",
      "Iteration: 3449 lambda_n: 1.014310503329914 Loss: 0.01630292738033013\n",
      "Iteration: 3450 lambda_n: 0.9097532088925143 Loss: 0.016291115206897334\n",
      "Iteration: 3451 lambda_n: 0.9974601143360947 Loss: 0.016280520673208843\n",
      "Iteration: 3452 lambda_n: 0.9417688595412379 Loss: 0.016268904763807213\n",
      "Iteration: 3453 lambda_n: 0.9459333813214407 Loss: 0.01625793742199482\n",
      "Iteration: 3454 lambda_n: 0.9040007559354366 Loss: 0.016246921597238173\n",
      "Iteration: 3455 lambda_n: 0.9889930211735688 Loss: 0.016236394111189847\n",
      "Iteration: 3456 lambda_n: 0.8822195563550173 Loss: 0.016224876867632448\n",
      "Iteration: 3457 lambda_n: 0.8792525086662967 Loss: 0.016214603060736587\n",
      "Iteration: 3458 lambda_n: 0.9267584616345647 Loss: 0.016204363818998806\n",
      "Iteration: 3459 lambda_n: 1.0159117427026525 Loss: 0.016193571364979762\n",
      "Iteration: 3460 lambda_n: 0.8769153361048683 Loss: 0.016181740702400076\n",
      "Iteration: 3461 lambda_n: 0.9440043903856007 Loss: 0.01617152871798352\n",
      "Iteration: 3462 lambda_n: 0.9187877896817204 Loss: 0.016160535471418526\n",
      "Iteration: 3463 lambda_n: 0.9500545528769153 Loss: 0.016149835894406793\n",
      "Iteration: 3464 lambda_n: 0.8968088091577056 Loss: 0.016138772219786376\n",
      "Iteration: 3465 lambda_n: 0.8779603139066632 Loss: 0.01612832862149414\n",
      "Iteration: 3466 lambda_n: 0.9217166550739166 Loss: 0.01611810453172811\n",
      "Iteration: 3467 lambda_n: 0.9466510656166653 Loss: 0.01610737089993192\n",
      "Iteration: 3468 lambda_n: 0.9993910211596403 Loss: 0.01609634691393522\n",
      "Iteration: 3469 lambda_n: 1.0210337264839067 Loss: 0.016084708772704705\n",
      "Iteration: 3470 lambda_n: 0.9530851797422073 Loss: 0.01607281861279683\n",
      "Iteration: 3471 lambda_n: 0.9057250123391152 Loss: 0.016061719743357752\n",
      "Iteration: 3472 lambda_n: 0.941142492381197 Loss: 0.01605117240573828\n",
      "Iteration: 3473 lambda_n: 1.000919878084619 Loss: 0.016040212637791745\n",
      "Iteration: 3474 lambda_n: 0.9715846039925132 Loss: 0.016028556765945987\n",
      "Iteration: 3475 lambda_n: 0.9347798047278272 Loss: 0.016017242522582537\n",
      "Iteration: 3476 lambda_n: 0.9976283614858346 Loss: 0.016006356889927974\n",
      "Iteration: 3477 lambda_n: 0.9095425789001266 Loss: 0.015994739391418256\n",
      "Iteration: 3478 lambda_n: 0.9033420780278603 Loss: 0.01598414767544681\n",
      "Iteration: 3479 lambda_n: 0.9312928275109607 Loss: 0.015973628176947835\n",
      "Iteration: 3480 lambda_n: 0.986286902368741 Loss: 0.015962783201728184\n",
      "Iteration: 3481 lambda_n: 0.9618189228390878 Loss: 0.015951297829646924\n",
      "Iteration: 3482 lambda_n: 0.8869765750405528 Loss: 0.015940097402354927\n",
      "Iteration: 3483 lambda_n: 0.970518726916152 Loss: 0.015929768529985986\n",
      "Iteration: 3484 lambda_n: 0.9720358413347738 Loss: 0.01591846681867046\n",
      "Iteration: 3485 lambda_n: 0.9211867750928198 Loss: 0.015907147453884784\n",
      "Iteration: 3486 lambda_n: 0.9013853480150633 Loss: 0.01589642023947447\n",
      "Iteration: 3487 lambda_n: 1.0237676486271823 Loss: 0.015885923624192853\n",
      "Iteration: 3488 lambda_n: 0.9141477257924517 Loss: 0.015874001882200214\n",
      "Iteration: 3489 lambda_n: 0.933289379088746 Loss: 0.015863356673727312\n",
      "Iteration: 3490 lambda_n: 0.9013573513635873 Loss: 0.01585248857333626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3491 lambda_n: 0.9360208097989956 Loss: 0.015841992396314\n",
      "Iteration: 3492 lambda_n: 0.9210621540409462 Loss: 0.0158310925513672\n",
      "Iteration: 3493 lambda_n: 0.876679806534016 Loss: 0.015820366891908747\n",
      "Iteration: 3494 lambda_n: 0.9006637457246558 Loss: 0.015810158060773943\n",
      "Iteration: 3495 lambda_n: 0.9479384494194099 Loss: 0.01579966994462787\n",
      "Iteration: 3496 lambda_n: 0.8916488555962172 Loss: 0.01578863132838044\n",
      "Iteration: 3497 lambda_n: 0.9678725523467828 Loss: 0.015778248206166308\n",
      "Iteration: 3498 lambda_n: 1.0143578780921867 Loss: 0.015766977480363042\n",
      "Iteration: 3499 lambda_n: 1.0064400588714981 Loss: 0.01575516545244639\n",
      "Iteration: 3500 lambda_n: 0.9361481564289807 Loss: 0.01574344563930673\n",
      "Iteration: 3501 lambda_n: 0.9167008356313772 Loss: 0.01573254437491687\n",
      "Iteration: 3502 lambda_n: 1.0208630475031841 Loss: 0.01572186958196353\n",
      "Iteration: 3503 lambda_n: 0.9081904638063318 Loss: 0.015709981853569772\n",
      "Iteration: 3504 lambda_n: 1.0215715256067937 Loss: 0.0156994061848931\n",
      "Iteration: 3505 lambda_n: 0.8847248629926873 Loss: 0.01568751023177619\n",
      "Iteration: 3506 lambda_n: 0.9593523257408672 Loss: 0.015677207836442\n",
      "Iteration: 3507 lambda_n: 0.9808593468495888 Loss: 0.0156660364340921\n",
      "Iteration: 3508 lambda_n: 0.9700954096090438 Loss: 0.015654614600058583\n",
      "Iteration: 3509 lambda_n: 0.9309141704246308 Loss: 0.015643318120985832\n",
      "Iteration: 3510 lambda_n: 1.0214217873132234 Loss: 0.01563247790728633\n",
      "Iteration: 3511 lambda_n: 0.990429613244292 Loss: 0.015620583771541867\n",
      "Iteration: 3512 lambda_n: 0.8803007681082551 Loss: 0.015609050542360392\n",
      "Iteration: 3513 lambda_n: 1.014865246718723 Loss: 0.015598799738242417\n",
      "Iteration: 3514 lambda_n: 0.8883947316293478 Loss: 0.015586981987223903\n",
      "Iteration: 3515 lambda_n: 0.959389076416678 Loss: 0.015576636952035351\n",
      "Iteration: 3516 lambda_n: 0.8749860237892695 Loss: 0.015565465223628512\n",
      "Iteration: 3517 lambda_n: 0.9472044357322384 Loss: 0.015555276347359969\n",
      "Iteration: 3518 lambda_n: 1.0107936070340164 Loss: 0.015544246525265336\n",
      "Iteration: 3519 lambda_n: 0.9018733235919139 Loss: 0.015532476243672187\n",
      "Iteration: 3520 lambda_n: 1.0010890559714127 Loss: 0.015521974305326841\n",
      "Iteration: 3521 lambda_n: 0.9820661179090452 Loss: 0.015510317051757698\n",
      "Iteration: 3522 lambda_n: 0.9127064655703427 Loss: 0.015498881323600592\n",
      "Iteration: 3523 lambda_n: 0.8874507803605943 Loss: 0.015488253268471367\n",
      "Iteration: 3524 lambda_n: 0.9042645647614573 Loss: 0.015477919313726099\n",
      "Iteration: 3525 lambda_n: 0.918441707342355 Loss: 0.015467389579409045\n",
      "Iteration: 3526 lambda_n: 1.014975791271847 Loss: 0.015456694768414778\n",
      "Iteration: 3527 lambda_n: 1.0113082031663707 Loss: 0.015444875875107721\n",
      "Iteration: 3528 lambda_n: 0.9923037760793297 Loss: 0.015433099700631722\n",
      "Iteration: 3529 lambda_n: 0.9146012023120524 Loss: 0.015421544834384212\n",
      "Iteration: 3530 lambda_n: 0.9178749900453139 Loss: 0.015410894784693489\n",
      "Iteration: 3531 lambda_n: 0.880545185858637 Loss: 0.01540020662277322\n",
      "Iteration: 3532 lambda_n: 0.9901870943234053 Loss: 0.015389953155312566\n",
      "Iteration: 3533 lambda_n: 0.9169478387621517 Loss: 0.015378422977314246\n",
      "Iteration: 3534 lambda_n: 0.9361197542215096 Loss: 0.015367745639625037\n",
      "Iteration: 3535 lambda_n: 1.0053783516885089 Loss: 0.01535684506516879\n",
      "Iteration: 3536 lambda_n: 0.9301983381647334 Loss: 0.015345138024467065\n",
      "Iteration: 3537 lambda_n: 0.9137728256774844 Loss: 0.015334306420935294\n",
      "Iteration: 3538 lambda_n: 0.9967645049685147 Loss: 0.015323666091755964\n",
      "Iteration: 3539 lambda_n: 0.9423552157015059 Loss: 0.015312059384590404\n",
      "Iteration: 3540 lambda_n: 0.9895360094962743 Loss: 0.015301086249930457\n",
      "Iteration: 3541 lambda_n: 0.9655414888857781 Loss: 0.015289563734359695\n",
      "Iteration: 3542 lambda_n: 0.9918434946190652 Loss: 0.015278320629664237\n",
      "Iteration: 3543 lambda_n: 1.0188900717635505 Loss: 0.015266771265120225\n",
      "Iteration: 3544 lambda_n: 0.9210412760463644 Loss: 0.015254906971440647\n",
      "Iteration: 3545 lambda_n: 0.9621902434476479 Loss: 0.015244182071217431\n",
      "Iteration: 3546 lambda_n: 0.897730636228633 Loss: 0.015232978028271392\n",
      "Iteration: 3547 lambda_n: 0.9330982819304491 Loss: 0.015222524581857903\n",
      "Iteration: 3548 lambda_n: 0.9530455083063651 Loss: 0.01521165931237892\n",
      "Iteration: 3549 lambda_n: 0.9994380908386397 Loss: 0.01520056178050767\n",
      "Iteration: 3550 lambda_n: 0.9444512217078094 Loss: 0.015188924049798873\n",
      "Iteration: 3551 lambda_n: 0.9644963267069951 Loss: 0.015177926610644286\n",
      "Iteration: 3552 lambda_n: 0.9695816574188318 Loss: 0.015166695770018087\n",
      "Iteration: 3553 lambda_n: 1.0198677749982332 Loss: 0.015155405723716327\n",
      "Iteration: 3554 lambda_n: 0.9538893142615573 Loss: 0.015143530143229405\n",
      "Iteration: 3555 lambda_n: 0.8764351652484581 Loss: 0.015132422840948833\n",
      "Iteration: 3556 lambda_n: 0.9667581287016194 Loss: 0.015122217440341696\n",
      "Iteration: 3557 lambda_n: 0.9861505167094516 Loss: 0.01511096030777547\n",
      "Iteration: 3558 lambda_n: 0.9016474590673479 Loss: 0.015099477375350324\n",
      "Iteration: 3559 lambda_n: 0.9278424301047604 Loss: 0.01508897842178247\n",
      "Iteration: 3560 lambda_n: 0.9192192961659238 Loss: 0.015078174456946607\n",
      "Iteration: 3561 lambda_n: 0.9774152456189409 Loss: 0.015067470909482817\n",
      "Iteration: 3562 lambda_n: 0.9242141482465893 Loss: 0.015056089726810258\n",
      "Iteration: 3563 lambda_n: 0.9337549450646763 Loss: 0.015045328034830125\n",
      "Iteration: 3564 lambda_n: 0.8964263091417514 Loss: 0.015034455256350499\n",
      "Iteration: 3565 lambda_n: 0.9606218160891187 Loss: 0.015024017145668032\n",
      "Iteration: 3566 lambda_n: 1.0070375816301627 Loss: 0.015012831541621065\n",
      "Iteration: 3567 lambda_n: 0.8945068719201626 Loss: 0.015001105475265801\n",
      "Iteration: 3568 lambda_n: 1.0185228863743023 Loss: 0.014990689738173838\n",
      "Iteration: 3569 lambda_n: 0.9916766610139343 Loss: 0.014978829953105595\n",
      "Iteration: 3570 lambda_n: 0.9502775129691328 Loss: 0.014967282777322058\n",
      "Iteration: 3571 lambda_n: 0.97956095247741 Loss: 0.014956217665512972\n",
      "Iteration: 3572 lambda_n: 1.0085565747991234 Loss: 0.014944811583128591\n",
      "Iteration: 3573 lambda_n: 0.900914964408262 Loss: 0.014933067882241708\n",
      "Iteration: 3574 lambda_n: 0.8979948078941933 Loss: 0.014922577575525536\n",
      "Iteration: 3575 lambda_n: 0.9801981330907974 Loss: 0.014912121278354935\n",
      "Iteration: 3576 lambda_n: 1.0053268689214703 Loss: 0.014900707809143834\n",
      "Iteration: 3577 lambda_n: 0.8909213302340787 Loss: 0.014889001748378494\n",
      "Iteration: 3578 lambda_n: 0.926441718276356 Loss: 0.014878627837361689\n",
      "Iteration: 3579 lambda_n: 0.9862429687229038 Loss: 0.014867840333089362\n",
      "Iteration: 3580 lambda_n: 0.9353892670859355 Loss: 0.014856356509743547\n",
      "Iteration: 3581 lambda_n: 0.946755597132187 Loss: 0.01484546483523841\n",
      "Iteration: 3582 lambda_n: 1.0228613244905067 Loss: 0.014834440818620714\n",
      "Iteration: 3583 lambda_n: 1.0214591746129256 Loss: 0.01482253063547181\n",
      "Iteration: 3584 lambda_n: 0.9914938243079282 Loss: 0.01481063678764157\n",
      "Iteration: 3585 lambda_n: 0.951944139848042 Loss: 0.014799091864070562\n",
      "Iteration: 3586 lambda_n: 0.9560696797157808 Loss: 0.0147880074635811\n",
      "Iteration: 3587 lambda_n: 0.8951969190253728 Loss: 0.014776875032919442\n",
      "Iteration: 3588 lambda_n: 0.9081530732141413 Loss: 0.014766451408809248\n",
      "Iteration: 3589 lambda_n: 0.9627975899596731 Loss: 0.014755876930532355\n",
      "Iteration: 3590 lambda_n: 1.0173729222318055 Loss: 0.014744666181926226\n",
      "Iteration: 3591 lambda_n: 0.9520885794880533 Loss: 0.014732819969777171\n",
      "Iteration: 3592 lambda_n: 0.9925478089800625 Loss: 0.014721733931224855\n",
      "Iteration: 3593 lambda_n: 0.8797406684866381 Loss: 0.014710176796334152\n",
      "Iteration: 3594 lambda_n: 0.8964830253693586 Loss: 0.0146999331842409\n",
      "Iteration: 3595 lambda_n: 0.9971804081383666 Loss: 0.014689494632008882\n",
      "Iteration: 3596 lambda_n: 0.9589775745234868 Loss: 0.014677883577224148\n",
      "Iteration: 3597 lambda_n: 1.0184148493725949 Loss: 0.014666717359295514\n",
      "Iteration: 3598 lambda_n: 0.9783789052338971 Loss: 0.01465485906852705\n",
      "Iteration: 3599 lambda_n: 0.8999260364431984 Loss: 0.014643466958764142\n",
      "Iteration: 3600 lambda_n: 0.883510885416676 Loss: 0.014632988350165447\n",
      "Iteration: 3601 lambda_n: 1.0036319710745694 Loss: 0.014622700883247438\n",
      "Iteration: 3602 lambda_n: 0.9106156696906305 Loss: 0.014611014751301549\n",
      "Iteration: 3603 lambda_n: 0.9611481253939337 Loss: 0.014600411693327299\n",
      "Iteration: 3604 lambda_n: 0.9414719275858489 Loss: 0.014589220250352664\n",
      "Iteration: 3605 lambda_n: 0.9717412338099723 Loss: 0.014578257920358918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3606 lambda_n: 0.9649493789484425 Loss: 0.01456694314676618\n",
      "Iteration: 3607 lambda_n: 1.018536092869866 Loss: 0.014555707463160851\n",
      "Iteration: 3608 lambda_n: 0.923619527071627 Loss: 0.014543847833430516\n",
      "Iteration: 3609 lambda_n: 0.987663521007439 Loss: 0.01453309339997567\n",
      "Iteration: 3610 lambda_n: 0.9751863852980217 Loss: 0.014521593258189018\n",
      "Iteration: 3611 lambda_n: 0.9443745667760516 Loss: 0.014510238404419422\n",
      "Iteration: 3612 lambda_n: 0.9432371710972698 Loss: 0.014499242323213691\n",
      "Iteration: 3613 lambda_n: 0.963609976337392 Loss: 0.014488259491921302\n",
      "Iteration: 3614 lambda_n: 0.9163638811031791 Loss: 0.014477039450919878\n",
      "Iteration: 3615 lambda_n: 1.0093185318860265 Loss: 0.014466369538216235\n",
      "Iteration: 3616 lambda_n: 0.9441943335788003 Loss: 0.014454617291196143\n",
      "Iteration: 3617 lambda_n: 0.9867006424978788 Loss: 0.014443623340334633\n",
      "Iteration: 3618 lambda_n: 0.9270330873045624 Loss: 0.014432134463627228\n",
      "Iteration: 3619 lambda_n: 0.9055381033472083 Loss: 0.014421340346174143\n",
      "Iteration: 3620 lambda_n: 0.9608485561064526 Loss: 0.014410796516133487\n",
      "Iteration: 3621 lambda_n: 0.9899616256975616 Loss: 0.01439960867272451\n",
      "Iteration: 3622 lambda_n: 1.0097365877953683 Loss: 0.014388081851580557\n",
      "Iteration: 3623 lambda_n: 1.0063095269514917 Loss: 0.014376324783351687\n",
      "Iteration: 3624 lambda_n: 1.0111657680841026 Loss: 0.014364607625598864\n",
      "Iteration: 3625 lambda_n: 0.9183127152945635 Loss: 0.01435283393005627\n",
      "Iteration: 3626 lambda_n: 1.0161662781709149 Loss: 0.014342141392354143\n",
      "Iteration: 3627 lambda_n: 0.9718587996217278 Loss: 0.014330309485410453\n",
      "Iteration: 3628 lambda_n: 1.0069311649077761 Loss: 0.014318993486694387\n",
      "Iteration: 3629 lambda_n: 0.9817589177075727 Loss: 0.014307269123470879\n",
      "Iteration: 3630 lambda_n: 1.0112392990963217 Loss: 0.014295837863722282\n",
      "Iteration: 3631 lambda_n: 0.8996088032819446 Loss: 0.014284063351055618\n",
      "Iteration: 3632 lambda_n: 0.9724435071219857 Loss: 0.014273588630224696\n",
      "Iteration: 3633 lambda_n: 0.9415496421276183 Loss: 0.014262265853916848\n",
      "Iteration: 3634 lambda_n: 0.8773406401866095 Loss: 0.01425130280027368\n",
      "Iteration: 3635 lambda_n: 1.0028285470696192 Loss: 0.014241087377564597\n",
      "Iteration: 3636 lambda_n: 0.9558904863299167 Loss: 0.014229410826632248\n",
      "Iteration: 3637 lambda_n: 0.9171162590857741 Loss: 0.014218280810458785\n",
      "Iteration: 3638 lambda_n: 0.9961875766774826 Loss: 0.01420760227171329\n",
      "Iteration: 3639 lambda_n: 0.9354326570399165 Loss: 0.014196003063481252\n",
      "Iteration: 3640 lambda_n: 0.9681181690343937 Loss: 0.01418511126686753\n",
      "Iteration: 3641 lambda_n: 0.9998461288830054 Loss: 0.01417383889900447\n",
      "Iteration: 3642 lambda_n: 0.8882457734751058 Loss: 0.014162197109769915\n",
      "Iteration: 3643 lambda_n: 0.9934230466800429 Loss: 0.014151854753661935\n",
      "Iteration: 3644 lambda_n: 0.9574589425509317 Loss: 0.014140287763407153\n",
      "Iteration: 3645 lambda_n: 1.0054905637961677 Loss: 0.014129139529383031\n",
      "Iteration: 3646 lambda_n: 0.8768400590723265 Loss: 0.014117432041848219\n",
      "Iteration: 3647 lambda_n: 0.9440018186045256 Loss: 0.014107222509118549\n",
      "Iteration: 3648 lambda_n: 1.020405311453718 Loss: 0.014096230979822485\n",
      "Iteration: 3649 lambda_n: 0.9764077389484319 Loss: 0.01408434984852112\n",
      "Iteration: 3650 lambda_n: 0.9958377049858975 Loss: 0.014072981010565677\n",
      "Iteration: 3651 lambda_n: 0.9019210566647028 Loss: 0.014061385944740072\n",
      "Iteration: 3652 lambda_n: 1.0053635820664353 Loss: 0.0140508844053592\n",
      "Iteration: 3653 lambda_n: 0.9034347372116177 Loss: 0.014039178435648588\n",
      "Iteration: 3654 lambda_n: 0.9431052474431824 Loss: 0.014028659281522927\n",
      "Iteration: 3655 lambda_n: 0.8907212344235327 Loss: 0.014017678228116729\n",
      "Iteration: 3656 lambda_n: 0.9878151985800965 Loss: 0.014007307113110129\n",
      "Iteration: 3657 lambda_n: 1.0080427812771433 Loss: 0.013995805489207307\n",
      "Iteration: 3658 lambda_n: 0.9808687075985073 Loss: 0.01398406835104234\n",
      "Iteration: 3659 lambda_n: 1.0233918277249687 Loss: 0.013972647619468622\n",
      "Iteration: 3660 lambda_n: 0.9763966155527004 Loss: 0.013960731776049598\n",
      "Iteration: 3661 lambda_n: 0.8992207952472232 Loss: 0.013949363125971009\n",
      "Iteration: 3662 lambda_n: 0.8811636764284675 Loss: 0.013938893075448803\n",
      "Iteration: 3663 lambda_n: 0.9980561363714268 Loss: 0.013928633276743872\n",
      "Iteration: 3664 lambda_n: 0.8742959405616318 Loss: 0.013917012449476058\n",
      "Iteration: 3665 lambda_n: 0.9975234066508565 Loss: 0.013906832623847872\n",
      "Iteration: 3666 lambda_n: 0.8808416929824855 Loss: 0.013895218009388161\n",
      "Iteration: 3667 lambda_n: 1.0131909658414033 Loss: 0.013884961977354652\n",
      "Iteration: 3668 lambda_n: 0.9107938331235839 Loss: 0.013873164948516979\n",
      "Iteration: 3669 lambda_n: 1.0171129125254186 Loss: 0.013862560179458078\n",
      "Iteration: 3670 lambda_n: 0.958592885770571 Loss: 0.013850717495914273\n",
      "Iteration: 3671 lambda_n: 0.9931019612380024 Loss: 0.013839556191300801\n",
      "Iteration: 3672 lambda_n: 1.016549337994582 Loss: 0.013827993087762553\n",
      "Iteration: 3673 lambda_n: 0.9003240010547023 Loss: 0.01381615698174669\n",
      "Iteration: 3674 lambda_n: 1.0036006081599373 Loss: 0.01380567414026009\n",
      "Iteration: 3675 lambda_n: 0.9740794951102251 Loss: 0.013793988811480713\n",
      "Iteration: 3676 lambda_n: 0.9965646865648257 Loss: 0.013782647213928483\n",
      "Iteration: 3677 lambda_n: 0.9740760312843354 Loss: 0.013771043817166283\n",
      "Iteration: 3678 lambda_n: 0.9378636336332677 Loss: 0.013759702269560884\n",
      "Iteration: 3679 lambda_n: 0.9077682279945387 Loss: 0.013748782361580508\n",
      "Iteration: 3680 lambda_n: 0.9144718187675432 Loss: 0.013738212870220671\n",
      "Iteration: 3681 lambda_n: 0.9927632284170412 Loss: 0.013727565330478784\n",
      "Iteration: 3682 lambda_n: 0.9052856767570109 Loss: 0.01371600621881141\n",
      "Iteration: 3683 lambda_n: 0.9593786822152653 Loss: 0.01370546564519911\n",
      "Iteration: 3684 lambda_n: 1.0262113500025403 Loss: 0.013694295251058622\n",
      "Iteration: 3685 lambda_n: 0.9257113574517513 Loss: 0.013682346704652138\n",
      "Iteration: 3686 lambda_n: 1.000881435730415 Loss: 0.013671568320211394\n",
      "Iteration: 3687 lambda_n: 1.0169631116573223 Loss: 0.01365991470839608\n",
      "Iteration: 3688 lambda_n: 0.9649646292552396 Loss: 0.013648073856834171\n",
      "Iteration: 3689 lambda_n: 0.9412089669145708 Loss: 0.013636838446120302\n",
      "Iteration: 3690 lambda_n: 0.9456325394274622 Loss: 0.013625879634896624\n",
      "Iteration: 3691 lambda_n: 1.0164937943228418 Loss: 0.013614869322679081\n",
      "Iteration: 3692 lambda_n: 0.9465222644677425 Loss: 0.013603033953898938\n",
      "Iteration: 3693 lambda_n: 1.0258150397737276 Loss: 0.01359201329089275\n",
      "Iteration: 3694 lambda_n: 0.8769638531315315 Loss: 0.01358006940105865\n",
      "Iteration: 3695 lambda_n: 0.9728918935908055 Loss: 0.013569858636893892\n",
      "Iteration: 3696 lambda_n: 0.9623785367352691 Loss: 0.013558530956442527\n",
      "Iteration: 3697 lambda_n: 0.9835460795920518 Loss: 0.01354732569046653\n",
      "Iteration: 3698 lambda_n: 0.9784248842666163 Loss: 0.01353587396859264\n",
      "Iteration: 3699 lambda_n: 0.9521044611440274 Loss: 0.013524481878619515\n",
      "Iteration: 3700 lambda_n: 0.9309519125348927 Loss: 0.013513396249236465\n",
      "Iteration: 3701 lambda_n: 0.8978279187515426 Loss: 0.013502556909040177\n",
      "Iteration: 3702 lambda_n: 0.9227038769102388 Loss: 0.013492103244670085\n",
      "Iteration: 3703 lambda_n: 0.9004708708207555 Loss: 0.013481359946096196\n",
      "Iteration: 3704 lambda_n: 0.9204813944552472 Loss: 0.013470875516217257\n",
      "Iteration: 3705 lambda_n: 0.962830051517516 Loss: 0.013460158101888\n",
      "Iteration: 3706 lambda_n: 1.0091137451651873 Loss: 0.013448947614473256\n",
      "Iteration: 3707 lambda_n: 0.9144711565882547 Loss: 0.013437198237803325\n",
      "Iteration: 3708 lambda_n: 0.9036868194165556 Loss: 0.013426550813597615\n",
      "Iteration: 3709 lambda_n: 0.907370324652179 Loss: 0.013416028957709658\n",
      "Iteration: 3710 lambda_n: 0.9262974951890633 Loss: 0.013405464217277431\n",
      "Iteration: 3711 lambda_n: 0.9479222699796731 Loss: 0.013394679106561991\n",
      "Iteration: 3712 lambda_n: 0.9769058132835088 Loss: 0.01338364221690022\n",
      "Iteration: 3713 lambda_n: 0.8859235131437947 Loss: 0.013372267868622327\n",
      "Iteration: 3714 lambda_n: 1.0195065749587247 Loss: 0.013361952852610073\n",
      "Iteration: 3715 lambda_n: 0.956680199844114 Loss: 0.013350082501259939\n",
      "Iteration: 3716 lambda_n: 0.9896046645733292 Loss: 0.013338943655928942\n",
      "Iteration: 3717 lambda_n: 0.920907380255084 Loss: 0.013327421467393937\n",
      "Iteration: 3718 lambda_n: 0.8931214499511573 Loss: 0.013316699140381392\n",
      "Iteration: 3719 lambda_n: 0.9080455224814346 Loss: 0.013306300334355516\n",
      "Iteration: 3720 lambda_n: 1.026473020184519 Loss: 0.013295727767359256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3721 lambda_n: 0.9719816608722899 Loss: 0.013283776327533551\n",
      "Iteration: 3722 lambda_n: 0.9609240130915561 Loss: 0.013272459345923126\n",
      "Iteration: 3723 lambda_n: 1.0253258443030082 Loss: 0.01326127111443309\n",
      "Iteration: 3724 lambda_n: 1.0016388863013168 Loss: 0.013249333043315124\n",
      "Iteration: 3725 lambda_n: 1.0171727824437458 Loss: 0.013237670768105699\n",
      "Iteration: 3726 lambda_n: 0.9547352839791012 Loss: 0.013225827632668385\n",
      "Iteration: 3727 lambda_n: 0.8902241169092759 Loss: 0.013214711472579268\n",
      "Iteration: 3728 lambda_n: 0.9852847018844676 Loss: 0.01320434643121993\n",
      "Iteration: 3729 lambda_n: 1.0071620831571644 Loss: 0.013192874585499932\n",
      "Iteration: 3730 lambda_n: 0.9667176050855132 Loss: 0.013181148021266584\n",
      "Iteration: 3731 lambda_n: 0.9441889656925461 Loss: 0.0131698923628154\n",
      "Iteration: 3732 lambda_n: 0.9262523007543714 Loss: 0.013158899012571161\n",
      "Iteration: 3733 lambda_n: 0.9502537730128285 Loss: 0.013148114505164767\n",
      "Iteration: 3734 lambda_n: 0.9996479939440204 Loss: 0.01313705054792928\n",
      "Iteration: 3735 lambda_n: 0.877999548283026 Loss: 0.013125411489336379\n",
      "Iteration: 3736 lambda_n: 0.9047721822330743 Loss: 0.013115188805906114\n",
      "Iteration: 3737 lambda_n: 1.0069989502547363 Loss: 0.013104654407475587\n",
      "Iteration: 3738 lambda_n: 1.0005987462562154 Loss: 0.01309292977071993\n",
      "Iteration: 3739 lambda_n: 0.8857760423629806 Loss: 0.01308127965610114\n",
      "Iteration: 3740 lambda_n: 1.0108739752425435 Loss: 0.01307096644184855\n",
      "Iteration: 3741 lambda_n: 0.8923402889300664 Loss: 0.013059196698062095\n",
      "Iteration: 3742 lambda_n: 0.9541538328503657 Loss: 0.01304880706136716\n",
      "Iteration: 3743 lambda_n: 0.9187362380579159 Loss: 0.013037697724350256\n",
      "Iteration: 3744 lambda_n: 1.0042190485643612 Loss: 0.013027000762056262\n",
      "Iteration: 3745 lambda_n: 0.8803857907272217 Loss: 0.01301530851579059\n",
      "Iteration: 3746 lambda_n: 0.9883140931934001 Loss: 0.013005058078482077\n",
      "Iteration: 3747 lambda_n: 0.9765678829047482 Loss: 0.01299355102200295\n",
      "Iteration: 3748 lambda_n: 0.976897345275573 Loss: 0.012982180731327722\n",
      "Iteration: 3749 lambda_n: 0.9853856987745643 Loss: 0.012970806607933205\n",
      "Iteration: 3750 lambda_n: 0.9741211918242829 Loss: 0.01295933365696364\n",
      "Iteration: 3751 lambda_n: 0.9912373204673635 Loss: 0.012947991863084608\n",
      "Iteration: 3752 lambda_n: 0.988359921262183 Loss: 0.012936450787581596\n",
      "Iteration: 3753 lambda_n: 1.0075808522861478 Loss: 0.012924943217188945\n",
      "Iteration: 3754 lambda_n: 1.0048049541224353 Loss: 0.012913211858930988\n",
      "Iteration: 3755 lambda_n: 0.9821058675732119 Loss: 0.012901512824049792\n",
      "Iteration: 3756 lambda_n: 0.9676406454463109 Loss: 0.012890078079916125\n",
      "Iteration: 3757 lambda_n: 1.0216681514922654 Loss: 0.012878811758717698\n",
      "Iteration: 3758 lambda_n: 0.9245551578459508 Loss: 0.01286691639393595\n",
      "Iteration: 3759 lambda_n: 0.9250972430862405 Loss: 0.012856151726622595\n",
      "Iteration: 3760 lambda_n: 0.9510348675583172 Loss: 0.012845380750507434\n",
      "Iteration: 3761 lambda_n: 0.9529102036976672 Loss: 0.012834307783500719\n",
      "Iteration: 3762 lambda_n: 0.9747799372758407 Loss: 0.012823212984692732\n",
      "Iteration: 3763 lambda_n: 1.0226129854592518 Loss: 0.012811863558005367\n",
      "Iteration: 3764 lambda_n: 0.952901966893045 Loss: 0.012799957211128213\n",
      "Iteration: 3765 lambda_n: 0.880147693285298 Loss: 0.01278886251703321\n",
      "Iteration: 3766 lambda_n: 0.91261279679969 Loss: 0.012778614907858702\n",
      "Iteration: 3767 lambda_n: 0.9116796550112324 Loss: 0.012767989308131335\n",
      "Iteration: 3768 lambda_n: 0.9204296156469164 Loss: 0.012757374575576061\n",
      "Iteration: 3769 lambda_n: 0.9725171923137318 Loss: 0.012746657969345257\n",
      "Iteration: 3770 lambda_n: 0.9262555324396583 Loss: 0.012735334907674777\n",
      "Iteration: 3771 lambda_n: 0.8943921352927459 Loss: 0.012724550475323934\n",
      "Iteration: 3772 lambda_n: 0.9745490696310287 Loss: 0.012714137032342478\n",
      "Iteration: 3773 lambda_n: 0.9385892704139527 Loss: 0.012702790321610823\n",
      "Iteration: 3774 lambda_n: 0.8854355275868626 Loss: 0.012691862294865996\n",
      "Iteration: 3775 lambda_n: 0.970764393597585 Loss: 0.012681553141410864\n",
      "Iteration: 3776 lambda_n: 0.9143426787496126 Loss: 0.012670250503836196\n",
      "Iteration: 3777 lambda_n: 0.8859530147376553 Loss: 0.012659604788497189\n",
      "Iteration: 3778 lambda_n: 0.9189966453601854 Loss: 0.012649289617104407\n",
      "Iteration: 3779 lambda_n: 0.9962477375040288 Loss: 0.012638589720315108\n",
      "Iteration: 3780 lambda_n: 0.9558505721491088 Loss: 0.012626990390113172\n",
      "Iteration: 3781 lambda_n: 0.974309426537167 Loss: 0.012615861407558054\n",
      "Iteration: 3782 lambda_n: 0.9074942622171294 Loss: 0.01260451751093828\n",
      "Iteration: 3783 lambda_n: 0.8888211931817969 Loss: 0.012593951546603414\n",
      "Iteration: 3784 lambda_n: 0.9511164931750038 Loss: 0.012583602995262759\n",
      "Iteration: 3785 lambda_n: 0.8865707454015449 Loss: 0.012572529141681303\n",
      "Iteration: 3786 lambda_n: 0.8898873874042468 Loss: 0.012562206796882585\n",
      "Iteration: 3787 lambda_n: 1.011646760984554 Loss: 0.012551845838604626\n",
      "Iteration: 3788 lambda_n: 1.014187980007559 Loss: 0.01254006723850459\n",
      "Iteration: 3789 lambda_n: 0.9672532468611116 Loss: 0.012528259053818424\n",
      "Iteration: 3790 lambda_n: 0.9554714356488663 Loss: 0.01251699733263421\n",
      "Iteration: 3791 lambda_n: 0.9547772502144306 Loss: 0.012505872789486385\n",
      "Iteration: 3792 lambda_n: 1.0272429958501252 Loss: 0.012494756331196242\n",
      "Iteration: 3793 lambda_n: 1.0122695178372894 Loss: 0.012482796157869451\n",
      "Iteration: 3794 lambda_n: 1.0235133621995056 Loss: 0.01247101032328017\n",
      "Iteration: 3795 lambda_n: 0.9108098885462013 Loss: 0.012459093579579394\n",
      "Iteration: 3796 lambda_n: 0.9658395553321244 Loss: 0.012448489042421325\n",
      "Iteration: 3797 lambda_n: 0.995961448737791 Loss: 0.01243724379851527\n",
      "Iteration: 3798 lambda_n: 0.8951436471382025 Loss: 0.012425647848729966\n",
      "Iteration: 3799 lambda_n: 0.9932861209298676 Loss: 0.012415225719953397\n",
      "Iteration: 3800 lambda_n: 0.9831289880878776 Loss: 0.012403660923799648\n",
      "Iteration: 3801 lambda_n: 0.9981894516581299 Loss: 0.01239221438930522\n",
      "Iteration: 3802 lambda_n: 0.9246426248545432 Loss: 0.012380592508898884\n",
      "Iteration: 3803 lambda_n: 0.8754727001787923 Loss: 0.012369826933642322\n",
      "Iteration: 3804 lambda_n: 0.9974264679365021 Loss: 0.01235963384380087\n",
      "Iteration: 3805 lambda_n: 0.9265359560078529 Loss: 0.012348020853843487\n",
      "Iteration: 3806 lambda_n: 0.9259925291057736 Loss: 0.01233723324113096\n",
      "Iteration: 3807 lambda_n: 0.9078128032099034 Loss: 0.012326451957646395\n",
      "Iteration: 3808 lambda_n: 0.9980117634324416 Loss: 0.012315882341858373\n",
      "Iteration: 3809 lambda_n: 0.9637143396788744 Loss: 0.012304262546644478\n",
      "Iteration: 3810 lambda_n: 1.0180063437972249 Loss: 0.012293042076776053\n",
      "Iteration: 3811 lambda_n: 1.0037503948871485 Loss: 0.012281189490659334\n",
      "Iteration: 3812 lambda_n: 1.0185822573585257 Loss: 0.012269502888167095\n",
      "Iteration: 3813 lambda_n: 1.0014772095173825 Loss: 0.012257643601699437\n",
      "Iteration: 3814 lambda_n: 0.9311362705272377 Loss: 0.012245983470626439\n",
      "Iteration: 3815 lambda_n: 0.9312088330392485 Loss: 0.012235142316545591\n",
      "Iteration: 3816 lambda_n: 0.94774632324034 Loss: 0.01222430031967806\n",
      "Iteration: 3817 lambda_n: 0.937852736937661 Loss: 0.012213265780091409\n",
      "Iteration: 3818 lambda_n: 0.9419645735837552 Loss: 0.012202346432873459\n",
      "Iteration: 3819 lambda_n: 0.8928970041139286 Loss: 0.012191379213917235\n",
      "Iteration: 3820 lambda_n: 0.9489557368052022 Loss: 0.012180983286737931\n",
      "Iteration: 3821 lambda_n: 0.8865723750219952 Loss: 0.012169934674248652\n",
      "Iteration: 3822 lambda_n: 1.0123497612650296 Loss: 0.012159612387967693\n",
      "Iteration: 3823 lambda_n: 0.9472098448734378 Loss: 0.01214782568839255\n",
      "Iteration: 3824 lambda_n: 1.0098513996631724 Loss: 0.01213679740933455\n",
      "Iteration: 3825 lambda_n: 0.9813597338269847 Loss: 0.012125039802446355\n",
      "Iteration: 3826 lambda_n: 1.0138490584855566 Loss: 0.01211361392362666\n",
      "Iteration: 3827 lambda_n: 0.949291696838806 Loss: 0.012101809776886309\n",
      "Iteration: 3828 lambda_n: 0.9483373690066592 Loss: 0.012090757267414579\n",
      "Iteration: 3829 lambda_n: 0.976955359327777 Loss: 0.012079715871075465\n",
      "Iteration: 3830 lambda_n: 0.9544459762699649 Loss: 0.012068341280396327\n",
      "Iteration: 3831 lambda_n: 0.9355031317647436 Loss: 0.012057228766184012\n",
      "Iteration: 3832 lambda_n: 0.8889559971258193 Loss: 0.012046336803459705\n",
      "Iteration: 3833 lambda_n: 0.9302176640285074 Loss: 0.012035986785833543\n",
      "Iteration: 3834 lambda_n: 0.9816217044371963 Loss: 0.012025156364901845\n",
      "Iteration: 3835 lambda_n: 0.9336457709512335 Loss: 0.012013727454390727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3836 lambda_n: 1.0188153226943346 Loss: 0.012002857124198275\n",
      "Iteration: 3837 lambda_n: 0.9711154955726753 Loss: 0.011990995176735523\n",
      "Iteration: 3838 lambda_n: 1.0148065043698398 Loss: 0.011979688594854798\n",
      "Iteration: 3839 lambda_n: 0.8953692808278327 Loss: 0.011967873325832207\n",
      "Iteration: 3840 lambda_n: 0.8802452831654967 Loss: 0.011957448651805043\n",
      "Iteration: 3841 lambda_n: 0.9583688884023318 Loss: 0.011947200066246742\n",
      "Iteration: 3842 lambda_n: 0.8739324831645685 Loss: 0.011936041899076351\n",
      "Iteration: 3843 lambda_n: 1.0034970058541024 Loss: 0.011925866815956962\n",
      "Iteration: 3844 lambda_n: 1.0244001815989714 Loss: 0.011914183231607114\n",
      "Iteration: 3845 lambda_n: 0.9716643715418031 Loss: 0.011902256276400918\n",
      "Iteration: 3846 lambda_n: 0.9804626088516785 Loss: 0.011890943319230945\n",
      "Iteration: 3847 lambda_n: 1.019148508065066 Loss: 0.011879527927280488\n",
      "Iteration: 3848 lambda_n: 0.9572877896584122 Loss: 0.011867662122692498\n",
      "Iteration: 3849 lambda_n: 1.0007603640611917 Loss: 0.011856516555794496\n",
      "Iteration: 3850 lambda_n: 0.997341263827588 Loss: 0.011844864845724048\n",
      "Iteration: 3851 lambda_n: 0.9814703330549961 Loss: 0.011833252945706044\n",
      "Iteration: 3852 lambda_n: 0.8928226574445649 Loss: 0.011821825830546388\n",
      "Iteration: 3853 lambda_n: 0.8887687149615038 Loss: 0.011811430828972479\n",
      "Iteration: 3854 lambda_n: 1.0247919369624279 Loss: 0.011801083028378147\n",
      "Iteration: 3855 lambda_n: 1.0174844728520724 Loss: 0.011789151531596733\n",
      "Iteration: 3856 lambda_n: 0.9868162217081748 Loss: 0.01177730511649904\n",
      "Iteration: 3857 lambda_n: 0.8763721736079523 Loss: 0.011765815769033527\n",
      "Iteration: 3858 lambda_n: 0.9740708173515925 Loss: 0.011755612306037604\n",
      "Iteration: 3859 lambda_n: 0.9646374414171348 Loss: 0.011744271354742809\n",
      "Iteration: 3860 lambda_n: 0.9565805200896217 Loss: 0.011733040236494434\n",
      "Iteration: 3861 lambda_n: 0.9698269032909012 Loss: 0.011721902925395251\n",
      "Iteration: 3862 lambda_n: 0.8785406300190123 Loss: 0.011710611390527904\n",
      "Iteration: 3863 lambda_n: 0.9443256173870574 Loss: 0.01170038268826635\n",
      "Iteration: 3864 lambda_n: 0.9672212050012998 Loss: 0.011689388063866317\n",
      "Iteration: 3865 lambda_n: 0.8857501395281062 Loss: 0.011678126871648589\n",
      "Iteration: 3866 lambda_n: 0.9461564475003641 Loss: 0.011667814234757611\n",
      "Iteration: 3867 lambda_n: 1.0169443025837372 Loss: 0.011656798299094289\n",
      "Iteration: 3868 lambda_n: 0.8919811053289085 Loss: 0.01164495819441416\n",
      "Iteration: 3869 lambda_n: 0.8984692739562 Loss: 0.011634573016000835\n",
      "Iteration: 3870 lambda_n: 0.9090730869633511 Loss: 0.011624112298404052\n",
      "Iteration: 3871 lambda_n: 0.8840198294875946 Loss: 0.011613528123940089\n",
      "Iteration: 3872 lambda_n: 0.878476382859908 Loss: 0.011603235641454499\n",
      "Iteration: 3873 lambda_n: 0.9230800804061254 Loss: 0.01159300770166505\n",
      "Iteration: 3874 lambda_n: 0.9902683246012508 Loss: 0.01158226045059183\n",
      "Iteration: 3875 lambda_n: 0.9478300020188319 Loss: 0.011570730940763859\n",
      "Iteration: 3876 lambda_n: 0.9100806782458518 Loss: 0.01155969553403893\n",
      "Iteration: 3877 lambda_n: 1.0256582918252746 Loss: 0.011549099637073672\n",
      "Iteration: 3878 lambda_n: 0.9292188913376638 Loss: 0.011537158093428253\n",
      "Iteration: 3879 lambda_n: 0.9634721574334199 Loss: 0.011526339376916998\n",
      "Iteration: 3880 lambda_n: 0.8977895458756294 Loss: 0.011515121857731262\n",
      "Iteration: 3881 lambda_n: 0.9232895595131471 Loss: 0.011504669069878152\n",
      "Iteration: 3882 lambda_n: 0.9831398750027539 Loss: 0.011493919391736948\n",
      "Iteration: 3883 lambda_n: 0.8925908771096576 Loss: 0.011482472889683074\n",
      "Iteration: 3884 lambda_n: 0.9765884619022227 Loss: 0.011472080633047171\n",
      "Iteration: 3885 lambda_n: 0.9313731548178869 Loss: 0.011460710410796196\n",
      "Iteration: 3886 lambda_n: 0.9913074364373782 Loss: 0.01144986662271416\n",
      "Iteration: 3887 lambda_n: 1.006501612461062 Loss: 0.011438325033482789\n",
      "Iteration: 3888 lambda_n: 1.0137818650978214 Loss: 0.011426606543180555\n",
      "Iteration: 3889 lambda_n: 1.0015895074144114 Loss: 0.011414803292033674\n",
      "Iteration: 3890 lambda_n: 0.9158467861908371 Loss: 0.011403141995583696\n",
      "Iteration: 3891 lambda_n: 1.0201404183210296 Loss: 0.011392478985094625\n",
      "Iteration: 3892 lambda_n: 0.9933678924085687 Loss: 0.011380601707367488\n",
      "Iteration: 3893 lambda_n: 0.9705607373158094 Loss: 0.01136903613804592\n",
      "Iteration: 3894 lambda_n: 0.9396144724604415 Loss: 0.011357736109039451\n",
      "Iteration: 3895 lambda_n: 0.9634655693439566 Loss: 0.011346796382120524\n",
      "Iteration: 3896 lambda_n: 1.0208735251079746 Loss: 0.011335578963461424\n",
      "Iteration: 3897 lambda_n: 0.9949827723157177 Loss: 0.011323693158048872\n",
      "Iteration: 3898 lambda_n: 0.9336400387849013 Loss: 0.011312108794509734\n",
      "Iteration: 3899 lambda_n: 0.9873411874608032 Loss: 0.011301238632205205\n",
      "Iteration: 3900 lambda_n: 1.0139317440168518 Loss: 0.011289743240815886\n",
      "Iteration: 3901 lambda_n: 0.9020389661412405 Loss: 0.011277938263055386\n",
      "Iteration: 3902 lambda_n: 0.8864122388758711 Loss: 0.011267436028938671\n",
      "Iteration: 3903 lambda_n: 0.9178240765471013 Loss: 0.011257115734429692\n",
      "Iteration: 3904 lambda_n: 0.9065079728629329 Loss: 0.011246429720293077\n",
      "Iteration: 3905 lambda_n: 1.0191627286602827 Loss: 0.01123587545816193\n",
      "Iteration: 3906 lambda_n: 0.8783100356049063 Loss: 0.011224009584177502\n",
      "Iteration: 3907 lambda_n: 1.0137656201061307 Loss: 0.011213783626549944\n",
      "Iteration: 3908 lambda_n: 0.8964286847276068 Loss: 0.011201980592623752\n",
      "Iteration: 3909 lambda_n: 0.9795522156878802 Loss: 0.011191543686255318\n",
      "Iteration: 3910 lambda_n: 0.9762429985796783 Loss: 0.011180138993601563\n",
      "Iteration: 3911 lambda_n: 1.0146557307661102 Loss: 0.011168772830729912\n",
      "Iteration: 3912 lambda_n: 0.9531975458609087 Loss: 0.01115695943903695\n",
      "Iteration: 3913 lambda_n: 1.0180732079777715 Loss: 0.01114586159150975\n",
      "Iteration: 3914 lambda_n: 0.8816977789240521 Loss: 0.011134008413758912\n",
      "Iteration: 3915 lambda_n: 0.8886772721409072 Loss: 0.01112374302307213\n",
      "Iteration: 3916 lambda_n: 0.9356095685121562 Loss: 0.011113396372941408\n",
      "Iteration: 3917 lambda_n: 0.9033404257824978 Loss: 0.011102503302809937\n",
      "Iteration: 3918 lambda_n: 0.9294298576420538 Loss: 0.011091985935457791\n",
      "Iteration: 3919 lambda_n: 0.8793706714934484 Loss: 0.011081164816496394\n",
      "Iteration: 3920 lambda_n: 0.9411024038638675 Loss: 0.011070926525202019\n",
      "Iteration: 3921 lambda_n: 1.0062119927974662 Loss: 0.01105996950800548\n",
      "Iteration: 3922 lambda_n: 0.9506350763517669 Loss: 0.01104825443760465\n",
      "Iteration: 3923 lambda_n: 0.9983752734121611 Loss: 0.011037186436384946\n",
      "Iteration: 3924 lambda_n: 0.9719062676665502 Loss: 0.01102556260950927\n",
      "Iteration: 3925 lambda_n: 1.024927249991789 Loss: 0.011014246955746534\n",
      "Iteration: 3926 lambda_n: 1.0042270246315195 Loss: 0.011002313993678946\n",
      "Iteration: 3927 lambda_n: 0.9552644320843245 Loss: 0.010990622040305023\n",
      "Iteration: 3928 lambda_n: 0.9449649947113744 Loss: 0.01097950014686998\n",
      "Iteration: 3929 lambda_n: 0.9365747790681626 Loss: 0.010968498168247076\n",
      "Iteration: 3930 lambda_n: 0.9412716521091907 Loss: 0.010957593875828868\n",
      "Iteration: 3931 lambda_n: 0.9368255569363496 Loss: 0.010946634900082887\n",
      "Iteration: 3932 lambda_n: 0.9157472070707482 Loss: 0.01093572769015871\n",
      "Iteration: 3933 lambda_n: 0.8758793116648625 Loss: 0.010925065890917268\n",
      "Iteration: 3934 lambda_n: 1.0195172471852412 Loss: 0.0109148682638997\n",
      "Iteration: 3935 lambda_n: 0.9000694292315268 Loss: 0.010902998300183176\n",
      "Iteration: 3936 lambda_n: 0.8997261063436776 Loss: 0.010892519036260933\n",
      "Iteration: 3937 lambda_n: 0.9874997213628428 Loss: 0.010882043770552841\n",
      "Iteration: 3938 lambda_n: 1.013320282660101 Loss: 0.010870546581670719\n",
      "Iteration: 3939 lambda_n: 0.9352017074321156 Loss: 0.010858748772284925\n",
      "Iteration: 3940 lambda_n: 0.8804735967919449 Loss: 0.010847860477138514\n",
      "Iteration: 3941 lambda_n: 0.9441754476723211 Loss: 0.010837609367260798\n",
      "Iteration: 3942 lambda_n: 0.9300723384052768 Loss: 0.010826616595400155\n",
      "Iteration: 3943 lambda_n: 0.9974547054829773 Loss: 0.010815788023163608\n",
      "Iteration: 3944 lambda_n: 0.9086453475746321 Loss: 0.010804174937966513\n",
      "Iteration: 3945 lambda_n: 0.8850262757974158 Loss: 0.01079359583627064\n",
      "Iteration: 3946 lambda_n: 0.9046389623502722 Loss: 0.01078329172571639\n",
      "Iteration: 3947 lambda_n: 0.8740789836393327 Loss: 0.010772759271137647\n",
      "Iteration: 3948 lambda_n: 1.0241735173263782 Loss: 0.010762582618634307\n",
      "Iteration: 3949 lambda_n: 1.0217538525210954 Loss: 0.010750658459287254\n",
      "Iteration: 3950 lambda_n: 0.9446449862224641 Loss: 0.010738762472611326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3951 lambda_n: 0.9010020101274364 Loss: 0.010727764243427269\n",
      "Iteration: 3952 lambda_n: 0.904733092980832 Loss: 0.010717274137798815\n",
      "Iteration: 3953 lambda_n: 0.9410854350740842 Loss: 0.010706740593176157\n",
      "Iteration: 3954 lambda_n: 0.9185004966142528 Loss: 0.010695783809752763\n",
      "Iteration: 3955 lambda_n: 0.9360401140367732 Loss: 0.010685089977154432\n",
      "Iteration: 3956 lambda_n: 0.8787601415172661 Loss: 0.010674191936880325\n",
      "Iteration: 3957 lambda_n: 0.8947801743987165 Loss: 0.010663960791424596\n",
      "Iteration: 3958 lambda_n: 0.9346520678062215 Loss: 0.010653543130313334\n",
      "Iteration: 3959 lambda_n: 0.8999595637568217 Loss: 0.010642661253450198\n",
      "Iteration: 3960 lambda_n: 0.9515360466623449 Loss: 0.010632183292043258\n",
      "Iteration: 3961 lambda_n: 1.0230588766198738 Loss: 0.010621104841923999\n",
      "Iteration: 3962 lambda_n: 1.0164650458982087 Loss: 0.01060919367390093\n",
      "Iteration: 3963 lambda_n: 0.9782224110358523 Loss: 0.01059735927699237\n",
      "Iteration: 3964 lambda_n: 0.9559385615777889 Loss: 0.010585970128646609\n",
      "Iteration: 3965 lambda_n: 1.006829978838948 Loss: 0.01057484042543026\n",
      "Iteration: 3966 lambda_n: 0.9642780630048254 Loss: 0.01056311820987044\n",
      "Iteration: 3967 lambda_n: 1.0144029009132962 Loss: 0.010551891414360299\n",
      "Iteration: 3968 lambda_n: 0.9803011728828324 Loss: 0.010540081031666938\n",
      "Iteration: 3969 lambda_n: 1.0168507171256445 Loss: 0.010528667685996546\n",
      "Iteration: 3970 lambda_n: 1.0234451735995795 Loss: 0.010516828806231754\n",
      "Iteration: 3971 lambda_n: 1.0245212696650001 Loss: 0.010504913150311384\n",
      "Iteration: 3972 lambda_n: 0.8835422977312875 Loss: 0.010492984966809355\n",
      "Iteration: 3973 lambda_n: 1.0072294929322845 Loss: 0.01048269815868909\n",
      "Iteration: 3974 lambda_n: 0.8976433086872732 Loss: 0.010470971299931318\n",
      "Iteration: 3975 lambda_n: 0.9121966976850773 Loss: 0.010460520319838218\n",
      "Iteration: 3976 lambda_n: 0.9400507865394453 Loss: 0.010449899900010965\n",
      "Iteration: 3977 lambda_n: 0.9048869601223648 Loss: 0.01043895518462344\n",
      "Iteration: 3978 lambda_n: 0.8878498956780222 Loss: 0.010428419871439719\n",
      "Iteration: 3979 lambda_n: 0.8969630606636549 Loss: 0.010418082916209477\n",
      "Iteration: 3980 lambda_n: 0.9619879719893918 Loss: 0.010407639860064856\n",
      "Iteration: 3981 lambda_n: 0.8992159568427556 Loss: 0.010396439740363085\n",
      "Iteration: 3982 lambda_n: 0.8968020205748781 Loss: 0.010385970456088703\n",
      "Iteration: 3983 lambda_n: 0.880514438233401 Loss: 0.01037552927728141\n",
      "Iteration: 3984 lambda_n: 1.0250371614321379 Loss: 0.010365277730338563\n",
      "Iteration: 3985 lambda_n: 0.942678231391006 Loss: 0.010353343552585801\n",
      "Iteration: 3986 lambda_n: 1.0252520536715255 Loss: 0.010342368254265603\n",
      "Iteration: 3987 lambda_n: 1.0192457638979646 Loss: 0.010330431576503124\n",
      "Iteration: 3988 lambda_n: 0.9884340207885349 Loss: 0.010318564829009967\n",
      "Iteration: 3989 lambda_n: 0.9577551340141087 Loss: 0.010307056813578819\n",
      "Iteration: 3990 lambda_n: 1.0165148506676285 Loss: 0.01029590598331539\n",
      "Iteration: 3991 lambda_n: 0.9599657170636691 Loss: 0.01028407103375861\n",
      "Iteration: 3992 lambda_n: 0.9683907852436835 Loss: 0.010272894468144254\n",
      "Iteration: 3993 lambda_n: 0.9127956370296866 Loss: 0.010261619813076743\n",
      "Iteration: 3994 lambda_n: 0.9727103683596033 Loss: 0.010250992434818059\n",
      "Iteration: 3995 lambda_n: 0.9353582772351124 Loss: 0.010239667489924463\n",
      "Iteration: 3996 lambda_n: 0.9536947026924387 Loss: 0.010228777423883659\n",
      "Iteration: 3997 lambda_n: 0.8782748110763868 Loss: 0.010217673873723255\n",
      "Iteration: 3998 lambda_n: 0.9010085116726357 Loss: 0.010207448413015976\n",
      "Iteration: 3999 lambda_n: 0.9634462623343559 Loss: 0.010196958272114549\n",
      "Iteration: 4000 lambda_n: 0.903753107239163 Loss: 0.010185741190118518\n",
      "Iteration: 4001 lambda_n: 1.0067353459910366 Loss: 0.010175219096287338\n",
      "Iteration: 4002 lambda_n: 0.9835353317221076 Loss: 0.010163498015637316\n",
      "Iteration: 4003 lambda_n: 0.9541311628002599 Loss: 0.010152047045799448\n",
      "Iteration: 4004 lambda_n: 0.9477562980744471 Loss: 0.010140938419582208\n",
      "Iteration: 4005 lambda_n: 0.9322804371735472 Loss: 0.010129904014532159\n",
      "Iteration: 4006 lambda_n: 0.9789262695945152 Loss: 0.010119049790433213\n",
      "Iteration: 4007 lambda_n: 0.9753675378299395 Loss: 0.010107652485543205\n",
      "Iteration: 4008 lambda_n: 0.8797226777027689 Loss: 0.010096296614559075\n",
      "Iteration: 4009 lambda_n: 0.9590571522737772 Loss: 0.010086054304721311\n",
      "Iteration: 4010 lambda_n: 0.8812039814181825 Loss: 0.010074888331471073\n",
      "Iteration: 4011 lambda_n: 0.9240412300446949 Loss: 0.010064628776668996\n",
      "Iteration: 4012 lambda_n: 0.997999223199787 Loss: 0.01005387048319001\n",
      "Iteration: 4013 lambda_n: 0.8850202450000708 Loss: 0.010042251123050829\n",
      "Iteration: 4014 lambda_n: 0.8763749810067343 Loss: 0.010031947138841115\n",
      "Iteration: 4015 lambda_n: 1.014807722853272 Loss: 0.010021743809075973\n",
      "Iteration: 4016 lambda_n: 0.967916189858547 Loss: 0.010009928755658131\n",
      "Iteration: 4017 lambda_n: 0.9247426738253638 Loss: 0.009998659644842294\n",
      "Iteration: 4018 lambda_n: 0.9847371335817786 Loss: 0.00998789318894043\n",
      "Iteration: 4019 lambda_n: 0.8847469351083037 Loss: 0.009976428239226169\n",
      "Iteration: 4020 lambda_n: 0.9678249005596744 Loss: 0.009966127441077829\n",
      "Iteration: 4021 lambda_n: 0.9971034157381333 Loss: 0.009954859395999566\n",
      "Iteration: 4022 lambda_n: 0.879419717078046 Loss: 0.00994325047221725\n",
      "Iteration: 4023 lambda_n: 0.8901701189566644 Loss: 0.009933011698960154\n",
      "Iteration: 4024 lambda_n: 0.9793257590636123 Loss: 0.009922647763175719\n",
      "Iteration: 4025 lambda_n: 0.912912522433682 Loss: 0.009911245820485083\n",
      "Iteration: 4026 lambda_n: 0.943040235531298 Loss: 0.009900617104253481\n",
      "Iteration: 4027 lambda_n: 0.9564177738894853 Loss: 0.009889637622415794\n",
      "Iteration: 4028 lambda_n: 0.9994880705684268 Loss: 0.00987850239134385\n",
      "Iteration: 4029 lambda_n: 0.952748890875282 Loss: 0.009866865708887961\n",
      "Iteration: 4030 lambda_n: 0.93480921887414 Loss: 0.009855773194714087\n",
      "Iteration: 4031 lambda_n: 0.9925166537520396 Loss: 0.009844889546381727\n",
      "Iteration: 4032 lambda_n: 0.9800685244828365 Loss: 0.009833334031778416\n",
      "Iteration: 4033 lambda_n: 0.9337413065410116 Loss: 0.00982192344698707\n",
      "Iteration: 4034 lambda_n: 0.900639391593279 Loss: 0.009811052233977829\n",
      "Iteration: 4035 lambda_n: 0.8866818635495459 Loss: 0.009800566415230296\n",
      "Iteration: 4036 lambda_n: 1.0140612079878715 Loss: 0.009790243099514163\n",
      "Iteration: 4037 lambda_n: 0.9625926438970999 Loss: 0.00977843675288626\n",
      "Iteration: 4038 lambda_n: 0.926613332985969 Loss: 0.009767229636772888\n",
      "Iteration: 4039 lambda_n: 0.932908394817829 Loss: 0.009756441415330176\n",
      "Iteration: 4040 lambda_n: 0.8849369481440458 Loss: 0.00974557990339035\n",
      "Iteration: 4041 lambda_n: 0.9038565373134374 Loss: 0.009735276906078708\n",
      "Iteration: 4042 lambda_n: 0.8941659022711498 Loss: 0.009724753635470741\n",
      "Iteration: 4043 lambda_n: 0.9291862047768886 Loss: 0.00971434318994268\n",
      "Iteration: 4044 lambda_n: 0.9888455859818891 Loss: 0.009703525016459963\n",
      "Iteration: 4045 lambda_n: 0.9938789067395465 Loss: 0.009692012251364608\n",
      "Iteration: 4046 lambda_n: 0.9055499558438265 Loss: 0.009680440885845912\n",
      "Iteration: 4047 lambda_n: 1.0074686214862498 Loss: 0.00966989790233899\n",
      "Iteration: 4048 lambda_n: 0.8955905113322087 Loss: 0.009658168393082794\n",
      "Iteration: 4049 lambda_n: 0.9795309199784374 Loss: 0.009647741398831042\n",
      "Iteration: 4050 lambda_n: 1.021933452232094 Loss: 0.009636337103993448\n",
      "Iteration: 4051 lambda_n: 0.916712988035069 Loss: 0.00962443912194387\n",
      "Iteration: 4052 lambda_n: 0.9071006538112679 Loss: 0.009613766176219885\n",
      "Iteration: 4053 lambda_n: 0.8991563476310994 Loss: 0.009603205141068945\n",
      "Iteration: 4054 lambda_n: 0.9005077225501861 Loss: 0.009592736597589465\n",
      "Iteration: 4055 lambda_n: 0.9955083090933168 Loss: 0.00958225232036042\n",
      "Iteration: 4056 lambda_n: 0.9085976555319659 Loss: 0.009570661986809406\n",
      "Iteration: 4057 lambda_n: 0.9494320036804634 Loss: 0.00956008352224221\n",
      "Iteration: 4058 lambda_n: 0.9939395736914325 Loss: 0.009549029639199106\n",
      "Iteration: 4059 lambda_n: 0.9865997791389506 Loss: 0.00953745757188549\n",
      "Iteration: 4060 lambda_n: 0.988204223755404 Loss: 0.009525970959902211\n",
      "Iteration: 4061 lambda_n: 0.9692204783657143 Loss: 0.009514465668811485\n",
      "Iteration: 4062 lambda_n: 0.9257377420924455 Loss: 0.009503181399160138\n",
      "Iteration: 4063 lambda_n: 0.9195337094778453 Loss: 0.009492403383402643\n",
      "Iteration: 4064 lambda_n: 0.8912631572711167 Loss: 0.009481697599544604\n",
      "Iteration: 4065 lambda_n: 0.9156543735388959 Loss: 0.009471320959695618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4066 lambda_n: 1.024939174971721 Loss: 0.009460660342763213\n",
      "Iteration: 4067 lambda_n: 0.9747078246869607 Loss: 0.00944872736499405\n",
      "Iteration: 4068 lambda_n: 1.0151951999915327 Loss: 0.009437379212496322\n",
      "Iteration: 4069 lambda_n: 0.8762837643912366 Loss: 0.009425559681589287\n",
      "Iteration: 4070 lambda_n: 0.9748715200256395 Loss: 0.00941535744421873\n",
      "Iteration: 4071 lambda_n: 0.9370493221072748 Loss: 0.00940400738782619\n",
      "Iteration: 4072 lambda_n: 0.937037186902245 Loss: 0.009393097681438678\n",
      "Iteration: 4073 lambda_n: 0.9430398061570628 Loss: 0.009382188116918905\n",
      "Iteration: 4074 lambda_n: 1.0099157112519082 Loss: 0.00937120866678406\n",
      "Iteration: 4075 lambda_n: 0.9768996156980674 Loss: 0.009359450606796585\n",
      "Iteration: 4076 lambda_n: 0.9183926827105742 Loss: 0.009348076941135092\n",
      "Iteration: 4077 lambda_n: 0.9091603876867939 Loss: 0.009337384449709003\n",
      "Iteration: 4078 lambda_n: 0.975798425562424 Loss: 0.009326799446852002\n",
      "Iteration: 4079 lambda_n: 0.9854084681603127 Loss: 0.009315438603636672\n",
      "Iteration: 4080 lambda_n: 0.9570881293757971 Loss: 0.009303965875008084\n",
      "Iteration: 4081 lambda_n: 0.9687444371598903 Loss: 0.009292822869675801\n",
      "Iteration: 4082 lambda_n: 1.0241937508689065 Loss: 0.009281544155040304\n",
      "Iteration: 4083 lambda_n: 0.9178137378683544 Loss: 0.009269619866210726\n",
      "Iteration: 4084 lambda_n: 0.8931032641874107 Loss: 0.009258934118988055\n",
      "Iteration: 4085 lambda_n: 0.9025817919797678 Loss: 0.009248536066652789\n",
      "Iteration: 4086 lambda_n: 1.0227945954237916 Loss: 0.009238027659995\n",
      "Iteration: 4087 lambda_n: 0.9938345270061893 Loss: 0.009226119663239593\n",
      "Iteration: 4088 lambda_n: 0.8875209369520042 Loss: 0.009214548837803175\n",
      "Iteration: 4089 lambda_n: 0.9903006353000277 Loss: 0.009204215780287451\n",
      "Iteration: 4090 lambda_n: 0.926888962436606 Loss: 0.009192686099633172\n",
      "Iteration: 4091 lambda_n: 0.8817184988871064 Loss: 0.009181894696656985\n",
      "Iteration: 4092 lambda_n: 0.9131768357117108 Loss: 0.009171629196050453\n",
      "Iteration: 4093 lambda_n: 0.9191182633955511 Loss: 0.00916099743889091\n",
      "Iteration: 4094 lambda_n: 0.9910530175479709 Loss: 0.009150296508503285\n",
      "Iteration: 4095 lambda_n: 0.9799097123783185 Loss: 0.009138758070730668\n",
      "Iteration: 4096 lambda_n: 1.0111099736081937 Loss: 0.009127349370580998\n",
      "Iteration: 4097 lambda_n: 1.0144135228068503 Loss: 0.00911557741870538\n",
      "Iteration: 4098 lambda_n: 1.0209681861633668 Loss: 0.009103767005475426\n",
      "Iteration: 4099 lambda_n: 0.9566019038850593 Loss: 0.009091880279465522\n",
      "Iteration: 4100 lambda_n: 0.9910240394530141 Loss: 0.009080742944971031\n",
      "Iteration: 4101 lambda_n: 0.971028869926366 Loss: 0.009069204847783854\n",
      "Iteration: 4102 lambda_n: 0.9908622277464777 Loss: 0.00905789954688102\n",
      "Iteration: 4103 lambda_n: 1.0126930921726742 Loss: 0.00904636333462887\n",
      "Iteration: 4104 lambda_n: 0.8818969766788455 Loss: 0.009034572954886246\n",
      "Iteration: 4105 lambda_n: 0.9107605991546076 Loss: 0.009024305382351822\n",
      "Iteration: 4106 lambda_n: 0.8884103745566034 Loss: 0.009013701762660557\n",
      "Iteration: 4107 lambda_n: 0.9032959898459048 Loss: 0.009003358358068907\n",
      "Iteration: 4108 lambda_n: 0.8979924555036503 Loss: 0.008992841646656523\n",
      "Iteration: 4109 lambda_n: 0.8924266452744688 Loss: 0.00898238668257042\n",
      "Iteration: 4110 lambda_n: 0.9574297933692255 Loss: 0.008971996519373306\n",
      "Iteration: 4111 lambda_n: 0.9592041907847049 Loss: 0.008960849551190943\n",
      "Iteration: 4112 lambda_n: 0.8966546802503614 Loss: 0.008949681924875134\n",
      "Iteration: 4113 lambda_n: 0.8893207155247289 Loss: 0.008939242537631028\n",
      "Iteration: 4114 lambda_n: 1.0214804753243578 Loss: 0.008928888537161101\n",
      "Iteration: 4115 lambda_n: 0.9312225176979256 Loss: 0.00891699585464472\n",
      "Iteration: 4116 lambda_n: 0.9494587852876183 Loss: 0.008906154009354227\n",
      "Iteration: 4117 lambda_n: 1.0265818369676614 Loss: 0.008895099847041017\n",
      "Iteration: 4118 lambda_n: 0.9946067570637024 Loss: 0.00888314777288777\n",
      "Iteration: 4119 lambda_n: 0.9576012755487104 Loss: 0.00887156797205466\n",
      "Iteration: 4120 lambda_n: 0.8927804307029406 Loss: 0.008860419011404514\n",
      "Iteration: 4121 lambda_n: 0.8922915774183302 Loss: 0.008850024733788755\n",
      "Iteration: 4122 lambda_n: 1.023407677908369 Loss: 0.00883963614806721\n",
      "Iteration: 4123 lambda_n: 0.8969945769479709 Loss: 0.008827721031666503\n",
      "Iteration: 4124 lambda_n: 0.8981894493457496 Loss: 0.008817277691648558\n",
      "Iteration: 4125 lambda_n: 0.9547504394050075 Loss: 0.00880682044059441\n",
      "Iteration: 4126 lambda_n: 1.0082798818667902 Loss: 0.008795704673546137\n",
      "Iteration: 4127 lambda_n: 0.9945345882031411 Loss: 0.008783965685636141\n",
      "Iteration: 4128 lambda_n: 0.948013301211693 Loss: 0.008772386728982928\n",
      "Iteration: 4129 lambda_n: 0.8833294730635014 Loss: 0.008761349400943167\n",
      "Iteration: 4130 lambda_n: 0.9625838574318808 Loss: 0.008751065160424969\n",
      "Iteration: 4131 lambda_n: 1.0063866119254663 Loss: 0.008739858194197163\n",
      "Iteration: 4132 lambda_n: 0.8794171394377576 Loss: 0.008728141251024038\n",
      "Iteration: 4133 lambda_n: 0.905982649947439 Loss: 0.008717902561307797\n",
      "Iteration: 4134 lambda_n: 1.005880474247709 Loss: 0.00870735458070009\n",
      "Iteration: 4135 lambda_n: 0.9165301281145041 Loss: 0.008695643531514578\n",
      "Iteration: 4136 lambda_n: 0.969350656061612 Loss: 0.008684972751753442\n",
      "Iteration: 4137 lambda_n: 0.8864938382566319 Loss: 0.008673687004899547\n",
      "Iteration: 4138 lambda_n: 0.8984137808125727 Loss: 0.008663365925912145\n",
      "Iteration: 4139 lambda_n: 0.9985377665017052 Loss: 0.008652906068332853\n",
      "Iteration: 4140 lambda_n: 0.9468184525972894 Loss: 0.008641280509258348\n",
      "Iteration: 4141 lambda_n: 0.959349503704695 Loss: 0.008630257097001784\n",
      "Iteration: 4142 lambda_n: 0.9324714546479449 Loss: 0.008619087791326962\n",
      "Iteration: 4143 lambda_n: 0.9316807517706605 Loss: 0.008608231415928081\n",
      "Iteration: 4144 lambda_n: 0.9465060017780313 Loss: 0.008597384246713769\n",
      "Iteration: 4145 lambda_n: 1.0182253313798397 Loss: 0.008586364473680743\n",
      "Iteration: 4146 lambda_n: 0.9783550482782947 Loss: 0.008574509702921887\n",
      "Iteration: 4147 lambda_n: 0.8737066155994991 Loss: 0.00856311912556571\n",
      "Iteration: 4148 lambda_n: 0.9274644301627903 Loss: 0.008552946926353363\n",
      "Iteration: 4149 lambda_n: 1.0098824146290315 Loss: 0.008542148847809274\n",
      "Iteration: 4150 lambda_n: 0.9250143881528159 Loss: 0.008530391211688745\n",
      "Iteration: 4151 lambda_n: 0.8911672885452043 Loss: 0.008519621658670207\n",
      "Iteration: 4152 lambda_n: 0.9122829700811725 Loss: 0.008509246173506995\n",
      "Iteration: 4153 lambda_n: 0.9374455971184167 Loss: 0.008498624847683808\n",
      "Iteration: 4154 lambda_n: 0.9548639964239651 Loss: 0.008487710564343743\n",
      "Iteration: 4155 lambda_n: 0.9001493481801413 Loss: 0.008476593486284047\n",
      "Iteration: 4156 lambda_n: 0.9601334561817997 Loss: 0.008466113428091808\n",
      "Iteration: 4157 lambda_n: 0.9454916142653976 Loss: 0.008454935000641513\n",
      "Iteration: 4158 lambda_n: 0.8984295334170492 Loss: 0.008443927042312354\n",
      "Iteration: 4159 lambda_n: 0.8923008823494092 Loss: 0.008433467008178826\n",
      "Iteration: 4160 lambda_n: 0.986495546038289 Loss: 0.00842307832763503\n",
      "Iteration: 4161 lambda_n: 0.964037712438092 Loss: 0.008411592978922857\n",
      "Iteration: 4162 lambda_n: 0.8908448846115602 Loss: 0.008400369097586303\n",
      "Iteration: 4163 lambda_n: 1.0257346819769182 Loss: 0.008389997369554592\n",
      "Iteration: 4164 lambda_n: 0.9147635164617215 Loss: 0.008378055177332169\n",
      "Iteration: 4165 lambda_n: 0.9308901344002898 Loss: 0.008367404975493027\n",
      "Iteration: 4166 lambda_n: 0.9903197203872744 Loss: 0.008356567018625444\n",
      "Iteration: 4167 lambda_n: 0.9652359377774583 Loss: 0.008345037148768112\n",
      "Iteration: 4168 lambda_n: 0.9764431945861053 Loss: 0.00833379931903404\n",
      "Iteration: 4169 lambda_n: 0.9409656991093854 Loss: 0.008322431008434534\n",
      "Iteration: 4170 lambda_n: 0.9245070012792037 Loss: 0.008311475747629317\n",
      "Iteration: 4171 lambda_n: 1.0193588188931608 Loss: 0.008300712108695013\n",
      "Iteration: 4172 lambda_n: 0.9337466156452828 Loss: 0.008288844150972299\n",
      "Iteration: 4173 lambda_n: 0.8778001001779209 Loss: 0.008277972939715815\n",
      "Iteration: 4174 lambda_n: 1.0100071358610563 Loss: 0.008267753089990419\n",
      "Iteration: 4175 lambda_n: 1.0173803460489617 Loss: 0.00825599401078624\n",
      "Iteration: 4176 lambda_n: 0.9853145838673301 Loss: 0.008244149088794707\n",
      "Iteration: 4177 lambda_n: 0.8797028688795888 Loss: 0.008232677495015989\n",
      "Iteration: 4178 lambda_n: 0.9160860033390967 Loss: 0.008222435493285298\n",
      "Iteration: 4179 lambda_n: 0.8786627731601053 Loss: 0.008211769898673841\n",
      "Iteration: 4180 lambda_n: 0.9666461490952964 Loss: 0.008201540006884023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4181 lambda_n: 0.8979746883431887 Loss: 0.008190285762872992\n",
      "Iteration: 4182 lambda_n: 0.8955110356658144 Loss: 0.008179831031336916\n",
      "Iteration: 4183 lambda_n: 0.9232712511384032 Loss: 0.008169404983310402\n",
      "Iteration: 4184 lambda_n: 0.8926497922370348 Loss: 0.008158655735361864\n",
      "Iteration: 4185 lambda_n: 0.9154689995000842 Loss: 0.00814826300008751\n",
      "Iteration: 4186 lambda_n: 0.9848208869761269 Loss: 0.008137604590922862\n",
      "Iteration: 4187 lambda_n: 0.9344008113831985 Loss: 0.008126138748053784\n",
      "Iteration: 4188 lambda_n: 0.9354581018716557 Loss: 0.008115259924575133\n",
      "Iteration: 4189 lambda_n: 0.9563246847793568 Loss: 0.008104368791802857\n",
      "Iteration: 4190 lambda_n: 0.9979618747029196 Loss: 0.008093234718746534\n",
      "Iteration: 4191 lambda_n: 1.0043839281583724 Loss: 0.008081615882271596\n",
      "Iteration: 4192 lambda_n: 0.8902948176899466 Loss: 0.008069922276936868\n",
      "Iteration: 4193 lambda_n: 0.940290224826641 Loss: 0.008059556961787789\n",
      "Iteration: 4194 lambda_n: 0.8764609372163988 Loss: 0.008048609572132036\n",
      "Iteration: 4195 lambda_n: 0.922800886223844 Loss: 0.008038405319336353\n",
      "Iteration: 4196 lambda_n: 1.0268596432645483 Loss: 0.008027661550957542\n",
      "Iteration: 4197 lambda_n: 0.8985538468747644 Loss: 0.00801570627213335\n",
      "Iteration: 4198 lambda_n: 0.9217450106364389 Loss: 0.008005244802003049\n",
      "Iteration: 4199 lambda_n: 1.0086691862655008 Loss: 0.00799451332753213\n",
      "Iteration: 4200 lambda_n: 1.0111963619839004 Loss: 0.007982769833142854\n",
      "Iteration: 4201 lambda_n: 0.9239773237518054 Loss: 0.007970996916260825\n",
      "Iteration: 4202 lambda_n: 0.9361131426940154 Loss: 0.007960239452767666\n",
      "Iteration: 4203 lambda_n: 0.9880470408362358 Loss: 0.007949340697506638\n",
      "Iteration: 4204 lambda_n: 1.018166391879649 Loss: 0.007937837298894884\n",
      "Iteration: 4205 lambda_n: 1.0243947561228501 Loss: 0.007925983234179524\n",
      "Iteration: 4206 lambda_n: 0.9271995068012209 Loss: 0.007914056655658955\n",
      "Iteration: 4207 lambda_n: 0.9756420395089578 Loss: 0.007903261679044246\n",
      "Iteration: 4208 lambda_n: 0.9517328595170031 Loss: 0.007891902707561987\n",
      "Iteration: 4209 lambda_n: 1.025329654329295 Loss: 0.00788082210042403\n",
      "Iteration: 4210 lambda_n: 0.9251512978315687 Loss: 0.007868884638451517\n",
      "Iteration: 4211 lambda_n: 0.929103295682991 Loss: 0.007858113509272843\n",
      "Iteration: 4212 lambda_n: 1.00899249261839 Loss: 0.007847296368968706\n",
      "Iteration: 4213 lambda_n: 0.943302046372361 Loss: 0.00783554911421034\n",
      "Iteration: 4214 lambda_n: 0.9148428324649122 Loss: 0.007824566664625547\n",
      "Iteration: 4215 lambda_n: 0.8910340533519306 Loss: 0.007813915553355673\n",
      "Iteration: 4216 lambda_n: 0.8802136001847027 Loss: 0.007803541637418999\n",
      "Iteration: 4217 lambda_n: 1.0098310662176766 Loss: 0.007793293699457321\n",
      "Iteration: 4218 lambda_n: 0.9597180900632692 Loss: 0.007781536682865044\n",
      "Iteration: 4219 lambda_n: 0.9909510721791722 Loss: 0.007770363109762449\n",
      "Iteration: 4220 lambda_n: 0.8984947273711857 Loss: 0.007758825905127892\n",
      "Iteration: 4221 lambda_n: 1.0226659124794975 Loss: 0.007748365129027185\n",
      "Iteration: 4222 lambda_n: 0.9672628458987005 Loss: 0.0077364586830713615\n",
      "Iteration: 4223 lambda_n: 0.9075582349854925 Loss: 0.007725197270730251\n",
      "Iteration: 4224 lambda_n: 0.8945613541586811 Loss: 0.007714630972930219\n",
      "Iteration: 4225 lambda_n: 0.908150688399013 Loss: 0.0077042159922622\n",
      "Iteration: 4226 lambda_n: 0.9534401500598371 Loss: 0.007693642797224128\n",
      "Iteration: 4227 lambda_n: 0.9384530498816007 Loss: 0.007682542317330778\n",
      "Iteration: 4228 lambda_n: 0.8970833967661908 Loss: 0.007671616325817808\n",
      "Iteration: 4229 lambda_n: 0.9553738079905493 Loss: 0.007661171982997502\n",
      "Iteration: 4230 lambda_n: 0.9800685259461128 Loss: 0.007650048991078371\n",
      "Iteration: 4231 lambda_n: 1.0002064287360979 Loss: 0.007638638489793345\n",
      "Iteration: 4232 lambda_n: 1.024881392563805 Loss: 0.00762699353212764\n",
      "Iteration: 4233 lambda_n: 0.9617869895945409 Loss: 0.007615061295116817\n",
      "Iteration: 4234 lambda_n: 0.9843158613751855 Loss: 0.007603863638352806\n",
      "Iteration: 4235 lambda_n: 0.9026184647738495 Loss: 0.007592403688225724\n",
      "Iteration: 4236 lambda_n: 0.8853302378231841 Loss: 0.007581894904636176\n",
      "Iteration: 4237 lambda_n: 0.9015814849619784 Loss: 0.007571587400349522\n",
      "Iteration: 4238 lambda_n: 0.8747789216262798 Loss: 0.007561090690253314\n",
      "Iteration: 4239 lambda_n: 1.0116409106880817 Loss: 0.007550906030612052\n",
      "Iteration: 4240 lambda_n: 1.0060047846747813 Loss: 0.007539127948276195\n",
      "Iteration: 4241 lambda_n: 0.9292332905263642 Loss: 0.0075274154850813796\n",
      "Iteration: 4242 lambda_n: 0.9111142524843228 Loss: 0.007516596838239695\n",
      "Iteration: 4243 lambda_n: 0.9675369257473893 Loss: 0.007505989143446002\n",
      "Iteration: 4244 lambda_n: 0.9546976768658061 Loss: 0.007494724544974651\n",
      "Iteration: 4245 lambda_n: 0.9197974881055101 Loss: 0.007483609428340867\n",
      "Iteration: 4246 lambda_n: 0.9066047888937407 Loss: 0.007472900639152318\n",
      "Iteration: 4247 lambda_n: 0.9881752952280682 Loss: 0.007462345446834591\n",
      "Iteration: 4248 lambda_n: 0.9642638211669001 Loss: 0.007450840565962865\n",
      "Iteration: 4249 lambda_n: 1.0027803119928056 Loss: 0.007439614075860305\n",
      "Iteration: 4250 lambda_n: 0.9974811108146143 Loss: 0.007427939155798017\n",
      "Iteration: 4251 lambda_n: 0.8874617867168129 Loss: 0.00741632593218216\n",
      "Iteration: 4252 lambda_n: 1.001942845094502 Loss: 0.0074059936142413345\n",
      "Iteration: 4253 lambda_n: 0.9868473914371737 Loss: 0.007394328445093952\n",
      "Iteration: 4254 lambda_n: 1.0091309833078705 Loss: 0.0073828390257372825\n",
      "Iteration: 4255 lambda_n: 0.9024427103834336 Loss: 0.007371090168789755\n",
      "Iteration: 4256 lambda_n: 0.8934004250501654 Loss: 0.007360583435494896\n",
      "Iteration: 4257 lambda_n: 0.9496786239110434 Loss: 0.007350181977628377\n",
      "Iteration: 4258 lambda_n: 0.8831808127335683 Loss: 0.007339125298285095\n",
      "Iteration: 4259 lambda_n: 0.95058838398143 Loss: 0.007328842823112375\n",
      "Iteration: 4260 lambda_n: 0.9240195057033408 Loss: 0.007317775552227631\n",
      "Iteration: 4261 lambda_n: 0.9788394924729416 Loss: 0.007307017610975628\n",
      "Iteration: 4262 lambda_n: 0.9755666547706754 Loss: 0.007295621425604746\n",
      "Iteration: 4263 lambda_n: 0.9046995598974336 Loss: 0.007284263344609259\n",
      "Iteration: 4264 lambda_n: 1.002422128367042 Loss: 0.007273730337313165\n",
      "Iteration: 4265 lambda_n: 0.9958822161155113 Loss: 0.007262059590598734\n",
      "Iteration: 4266 lambda_n: 0.9710049112185931 Loss: 0.007250464985332491\n",
      "Iteration: 4267 lambda_n: 0.9239539512475223 Loss: 0.0072391600154564925\n",
      "Iteration: 4268 lambda_n: 0.8967813624137045 Loss: 0.007228402838760964\n",
      "Iteration: 4269 lambda_n: 0.975198274123342 Loss: 0.007217962020363694\n",
      "Iteration: 4270 lambda_n: 0.9530501324499486 Loss: 0.007206608229635201\n",
      "Iteration: 4271 lambda_n: 0.9750089766961292 Loss: 0.007195512299856741\n",
      "Iteration: 4272 lambda_n: 0.8793823275008849 Loss: 0.007184160713420821\n",
      "Iteration: 4273 lambda_n: 0.9874066555331941 Loss: 0.007173922464780999\n",
      "Iteration: 4274 lambda_n: 1.0041939956594568 Loss: 0.0071624265381787155\n",
      "Iteration: 4275 lambda_n: 1.0189668174810398 Loss: 0.0071507351644116484\n",
      "Iteration: 4276 lambda_n: 1.003873577745736 Loss: 0.007138871797608359\n",
      "Iteration: 4277 lambda_n: 0.9680666532870744 Loss: 0.007127184154729752\n",
      "Iteration: 4278 lambda_n: 0.8981465752695489 Loss: 0.00711591339575918\n",
      "Iteration: 4279 lambda_n: 0.9060672025731984 Loss: 0.007105456684572367\n",
      "Iteration: 4280 lambda_n: 0.9417125404116781 Loss: 0.007094907757292816\n",
      "Iteration: 4281 lambda_n: 0.9714794630302881 Loss: 0.007083943827766286\n",
      "Iteration: 4282 lambda_n: 0.9911384092298879 Loss: 0.007072633335717102\n",
      "Iteration: 4283 lambda_n: 0.9119206736900966 Loss: 0.0070610939637157\n",
      "Iteration: 4284 lambda_n: 0.918751354254548 Loss: 0.007050476887817412\n",
      "Iteration: 4285 lambda_n: 0.9593753580783381 Loss: 0.00703978028558599\n",
      "Iteration: 4286 lambda_n: 1.012990105939888 Loss: 0.007028610716807874\n",
      "Iteration: 4287 lambda_n: 0.9264398808971309 Loss: 0.007016816936211559\n",
      "Iteration: 4288 lambda_n: 1.0052666665892915 Loss: 0.007006030820486159\n",
      "Iteration: 4289 lambda_n: 0.8955010901654306 Loss: 0.006994326960729924\n",
      "Iteration: 4290 lambda_n: 0.9400653850843574 Loss: 0.006983901051515928\n",
      "Iteration: 4291 lambda_n: 0.9822224502416207 Loss: 0.006972956300775096\n",
      "Iteration: 4292 lambda_n: 0.9915795717424107 Loss: 0.0069615207348027435\n",
      "Iteration: 4293 lambda_n: 0.9817637159645005 Loss: 0.00694997622833095\n",
      "Iteration: 4294 lambda_n: 0.8931337148549068 Loss: 0.006938546003547011\n",
      "Iteration: 4295 lambda_n: 0.9122476541362899 Loss: 0.006928147657386574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4296 lambda_n: 0.8988040488931194 Loss: 0.006917526776546026\n",
      "Iteration: 4297 lambda_n: 0.8833626801846396 Loss: 0.006907062413580116\n",
      "Iteration: 4298 lambda_n: 1.0201439608197764 Loss: 0.006896777827524195\n",
      "Iteration: 4299 lambda_n: 1.001660648677949 Loss: 0.006884900759986656\n",
      "Iteration: 4300 lambda_n: 0.9448557495669552 Loss: 0.006873238885343252\n",
      "Iteration: 4301 lambda_n: 0.9115097987030236 Loss: 0.006862238364203629\n",
      "Iteration: 4302 lambda_n: 0.935786739448276 Loss: 0.0068516260747915735\n",
      "Iteration: 4303 lambda_n: 0.8955673706202765 Loss: 0.006840731140271263\n",
      "Iteration: 4304 lambda_n: 0.9928303271817753 Loss: 0.006830304461505511\n",
      "Iteration: 4305 lambda_n: 0.9269320285487068 Loss: 0.006818745395049198\n",
      "Iteration: 4306 lambda_n: 0.9302890715334748 Loss: 0.006807953552305929\n",
      "Iteration: 4307 lambda_n: 0.880895406815815 Loss: 0.006797122625204279\n",
      "Iteration: 4308 lambda_n: 0.8758956553633414 Loss: 0.00678686676592573\n",
      "Iteration: 4309 lambda_n: 0.9706414375016218 Loss: 0.006776669116579574\n",
      "Iteration: 4310 lambda_n: 0.8982927800428978 Loss: 0.006765368385910598\n",
      "Iteration: 4311 lambda_n: 0.9740993153632918 Loss: 0.006754909977444327\n",
      "Iteration: 4312 lambda_n: 0.9093342243150729 Loss: 0.006743568988601466\n",
      "Iteration: 4313 lambda_n: 0.9944705459071727 Loss: 0.006732982029975158\n",
      "Iteration: 4314 lambda_n: 1.0170947819701253 Loss: 0.006721403868604097\n",
      "Iteration: 4315 lambda_n: 0.9717741206925256 Loss: 0.006709562303864789\n",
      "Iteration: 4316 lambda_n: 0.9373132395840049 Loss: 0.006698248386811492\n",
      "Iteration: 4317 lambda_n: 0.9675097611374589 Loss: 0.00668733568202075\n",
      "Iteration: 4318 lambda_n: 0.9150712116164732 Loss: 0.00667607141323333\n",
      "Iteration: 4319 lambda_n: 0.9247629333967624 Loss: 0.006665417662372922\n",
      "Iteration: 4320 lambda_n: 0.9014891153254319 Loss: 0.006654651075413356\n",
      "Iteration: 4321 lambda_n: 0.9900380697361729 Loss: 0.006644155454881162\n",
      "Iteration: 4322 lambda_n: 0.8955464168943564 Loss: 0.0066326288999966525\n",
      "Iteration: 4323 lambda_n: 0.9399974125140456 Loss: 0.006622202467818004\n",
      "Iteration: 4324 lambda_n: 1.0209737570743276 Loss: 0.006611258513415115\n",
      "Iteration: 4325 lambda_n: 0.9893515058836927 Loss: 0.006599371789095149\n",
      "Iteration: 4326 lambda_n: 0.994720693197203 Loss: 0.006587853228147404\n",
      "Iteration: 4327 lambda_n: 0.9323855272646475 Loss: 0.00657627215639171\n",
      "Iteration: 4328 lambda_n: 0.9737221578602905 Loss: 0.006565416824208188\n",
      "Iteration: 4329 lambda_n: 0.9062761448773085 Loss: 0.006554080228947366\n",
      "Iteration: 4330 lambda_n: 1.0162068534599904 Loss: 0.006543528876452792\n",
      "Iteration: 4331 lambda_n: 0.8743173668100398 Loss: 0.00653169765188222\n",
      "Iteration: 4332 lambda_n: 0.9098408386958562 Loss: 0.006521518380858137\n",
      "Iteration: 4333 lambda_n: 0.9952967253323948 Loss: 0.006510925526667866\n",
      "Iteration: 4334 lambda_n: 1.0078453680847823 Loss: 0.0064993377494244504\n",
      "Iteration: 4335 lambda_n: 0.9308945120203206 Loss: 0.0064876038743124165\n",
      "Iteration: 4336 lambda_n: 0.910901377495775 Loss: 0.006476765902383154\n",
      "Iteration: 4337 lambda_n: 0.992512614853109 Loss: 0.006466160701345377\n",
      "Iteration: 4338 lambda_n: 0.8930233023719187 Loss: 0.006454605338762727\n",
      "Iteration: 4339 lambda_n: 0.9417583002765548 Loss: 0.006444208284082797\n",
      "Iteration: 4340 lambda_n: 0.9531047970799235 Loss: 0.006433243830624698\n",
      "Iteration: 4341 lambda_n: 0.8803607042534184 Loss: 0.006422147275314352\n",
      "Iteration: 4342 lambda_n: 0.9448997909472288 Loss: 0.006411897645720066\n",
      "Iteration: 4343 lambda_n: 0.9722737811973544 Loss: 0.006400896617719818\n",
      "Iteration: 4344 lambda_n: 0.9818676550799199 Loss: 0.006389576887234378\n",
      "Iteration: 4345 lambda_n: 1.0172175556332839 Loss: 0.006378145459879787\n",
      "Iteration: 4346 lambda_n: 0.9325414416462158 Loss: 0.006366302470251825\n",
      "Iteration: 4347 lambda_n: 1.0118200296783135 Loss: 0.006355445325261647\n",
      "Iteration: 4348 lambda_n: 0.9553856288836142 Loss: 0.006343665176782438\n",
      "Iteration: 4349 lambda_n: 0.9976604323999944 Loss: 0.00633254206783055\n",
      "Iteration: 4350 lambda_n: 0.9098355546840613 Loss: 0.0063209267732022515\n",
      "Iteration: 4351 lambda_n: 0.9404788042559448 Loss: 0.006310333982744026\n",
      "Iteration: 4352 lambda_n: 0.874667776150601 Loss: 0.006299384427356536\n",
      "Iteration: 4353 lambda_n: 0.9914505745842725 Loss: 0.006289201079136381\n",
      "Iteration: 4354 lambda_n: 0.9693530215088556 Loss: 0.00627765808347709\n",
      "Iteration: 4355 lambda_n: 1.0141325625763022 Loss: 0.00626637235942729\n",
      "Iteration: 4356 lambda_n: 0.8916849266609902 Loss: 0.0062545652882446835\n",
      "Iteration: 4357 lambda_n: 0.9943399761385715 Loss: 0.006244183817744353\n",
      "Iteration: 4358 lambda_n: 0.8911647786608572 Loss: 0.006232607182636744\n",
      "Iteration: 4359 lambda_n: 0.8921749266019943 Loss: 0.006222231768193944\n",
      "Iteration: 4360 lambda_n: 0.8906040219614991 Loss: 0.006211844593173575\n",
      "Iteration: 4361 lambda_n: 0.9275776945963172 Loss: 0.006201475707562128\n",
      "Iteration: 4362 lambda_n: 1.0209958857284311 Loss: 0.006190676354897837\n",
      "Iteration: 4363 lambda_n: 0.9561077471347225 Loss: 0.00617878937810674\n",
      "Iteration: 4364 lambda_n: 1.014107271669695 Loss: 0.006167657863638033\n",
      "Iteration: 4365 lambda_n: 1.0096460497932214 Loss: 0.00615585108800506\n",
      "Iteration: 4366 lambda_n: 0.9132815525526549 Loss: 0.006144096252414832\n",
      "Iteration: 4367 lambda_n: 1.0191928744114873 Loss: 0.0061334633436001115\n",
      "Iteration: 4368 lambda_n: 0.9263609270935534 Loss: 0.00612159735904882\n",
      "Iteration: 4369 lambda_n: 0.9332086883581826 Loss: 0.00611081217342993\n",
      "Iteration: 4370 lambda_n: 1.0049460200048579 Loss: 0.006099947262644929\n",
      "Iteration: 4371 lambda_n: 0.8911900464534154 Loss: 0.006088247147888955\n",
      "Iteration: 4372 lambda_n: 1.0229695191545445 Loss: 0.006077871440640135\n",
      "Iteration: 4373 lambda_n: 1.009694899543818 Loss: 0.00606596148697774\n",
      "Iteration: 4374 lambda_n: 0.9253511919205188 Loss: 0.006054206083600305\n",
      "Iteration: 4375 lambda_n: 0.91713532863007 Loss: 0.0060434326544983015\n",
      "Iteration: 4376 lambda_n: 0.9005245485412626 Loss: 0.006032754878931966\n",
      "Iteration: 4377 lambda_n: 0.9346308308738281 Loss: 0.006022270494967757\n",
      "Iteration: 4378 lambda_n: 1.0038915307028529 Loss: 0.006011389027690538\n",
      "Iteration: 4379 lambda_n: 0.9562803016526611 Loss: 0.0059997011907621444\n",
      "Iteration: 4380 lambda_n: 0.939759264943556 Loss: 0.005988567669091206\n",
      "Iteration: 4381 lambda_n: 0.8936874558619893 Loss: 0.005977626494181445\n",
      "Iteration: 4382 lambda_n: 0.8805205932163305 Loss: 0.005967221711761173\n",
      "Iteration: 4383 lambda_n: 0.8781234595446797 Loss: 0.005956970225014789\n",
      "Iteration: 4384 lambda_n: 0.9880080222029727 Loss: 0.005946746647054653\n",
      "Iteration: 4385 lambda_n: 0.9110926602998483 Loss: 0.005935243734970111\n",
      "Iteration: 4386 lambda_n: 0.9699764139484003 Loss: 0.005924636312320352\n",
      "Iteration: 4387 lambda_n: 0.8850628853419392 Loss: 0.005913343333963896\n",
      "Iteration: 4388 lambda_n: 0.9008380874483242 Loss: 0.00590303896390658\n",
      "Iteration: 4389 lambda_n: 0.9474077844768567 Loss: 0.005892550930693439\n",
      "Iteration: 4390 lambda_n: 0.9911893223186835 Loss: 0.0058815207085433285\n",
      "Iteration: 4391 lambda_n: 0.8889123196894324 Loss: 0.005869980758692527\n",
      "Iteration: 4392 lambda_n: 0.8984847665975343 Loss: 0.005859631571846551\n",
      "Iteration: 4393 lambda_n: 0.9504204861119615 Loss: 0.005849170937602494\n",
      "Iteration: 4394 lambda_n: 0.9830709201351232 Loss: 0.005838105640370219\n",
      "Iteration: 4395 lambda_n: 0.963136866868304 Loss: 0.005826660209641436\n",
      "Iteration: 4396 lambda_n: 0.9388786994883371 Loss: 0.005815446861786696\n",
      "Iteration: 4397 lambda_n: 1.0071046821713627 Loss: 0.0058045159404181024\n",
      "Iteration: 4398 lambda_n: 1.0107237083005938 Loss: 0.005792790696247992\n",
      "Iteration: 4399 lambda_n: 0.8966200333410858 Loss: 0.0057810233175721055\n",
      "Iteration: 4400 lambda_n: 0.9188573700930811 Loss: 0.005770584394175893\n",
      "Iteration: 4401 lambda_n: 0.9004492337405787 Loss: 0.005759886572059996\n",
      "Iteration: 4402 lambda_n: 0.8977868831711207 Loss: 0.005749403067261065\n",
      "Iteration: 4403 lambda_n: 0.9633034106092695 Loss: 0.005738950559033917\n",
      "Iteration: 4404 lambda_n: 0.9854481971145423 Loss: 0.005727735272940441\n",
      "Iteration: 4405 lambda_n: 0.9371353395676179 Loss: 0.005716262165665238\n",
      "Iteration: 4406 lambda_n: 0.8930948591162744 Loss: 0.005705351542232961\n",
      "Iteration: 4407 lambda_n: 0.8975304479952282 Loss: 0.005694953661364266\n",
      "Iteration: 4408 lambda_n: 0.9888341530146116 Loss: 0.005684504139112719\n",
      "Iteration: 4409 lambda_n: 0.920540904901593 Loss: 0.005672991611120782\n",
      "Iteration: 4410 lambda_n: 0.9249595742532669 Loss: 0.005662274189180537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4411 lambda_n: 0.9280003776934974 Loss: 0.00565150532284837\n",
      "Iteration: 4412 lambda_n: 0.8959749998170748 Loss: 0.005640701053965288\n",
      "Iteration: 4413 lambda_n: 0.9235374522420811 Loss: 0.005630269641469717\n",
      "Iteration: 4414 lambda_n: 0.937997034422789 Loss: 0.005619517332476838\n",
      "Iteration: 4415 lambda_n: 1.0227493549031927 Loss: 0.00560859667750253\n",
      "Iteration: 4416 lambda_n: 0.918393850323505 Loss: 0.00559668929151482\n",
      "Iteration: 4417 lambda_n: 1.0196455732029908 Loss: 0.005585996867294838\n",
      "Iteration: 4418 lambda_n: 0.907518886997815 Loss: 0.005574125617353767\n",
      "Iteration: 4419 lambda_n: 0.8879222581711725 Loss: 0.005563559805341911\n",
      "Iteration: 4420 lambda_n: 1.0144184434490595 Loss: 0.00555322214766106\n",
      "Iteration: 4421 lambda_n: 0.8849871049863658 Loss: 0.005541411754980047\n",
      "Iteration: 4422 lambda_n: 0.9697552429496861 Loss: 0.005531108270051475\n",
      "Iteration: 4423 lambda_n: 0.9019425174260909 Loss: 0.005519817869994764\n",
      "Iteration: 4424 lambda_n: 1.003856183251158 Loss: 0.005509316981398031\n",
      "Iteration: 4425 lambda_n: 0.9352459106574708 Loss: 0.005497629560448804\n",
      "Iteration: 4426 lambda_n: 0.8820735047663646 Loss: 0.005486740936414835\n",
      "Iteration: 4427 lambda_n: 1.019447448327799 Loss: 0.005476471373527843\n",
      "Iteration: 4428 lambda_n: 0.9190532767056643 Loss: 0.00546460243114039\n",
      "Iteration: 4429 lambda_n: 0.9071056033629762 Loss: 0.005453902330487795\n",
      "Iteration: 4430 lambda_n: 0.9055348395853985 Loss: 0.005443341330993381\n",
      "Iteration: 4431 lambda_n: 1.00593022570764 Loss: 0.005432798619227606\n",
      "Iteration: 4432 lambda_n: 0.898840497329865 Loss: 0.005421087051775779\n",
      "Iteration: 4433 lambda_n: 0.94750592331362 Loss: 0.005410622279205548\n",
      "Iteration: 4434 lambda_n: 0.9033740557732238 Loss: 0.005399590918291917\n",
      "Iteration: 4435 lambda_n: 0.9562948812844488 Loss: 0.005389073363800816\n",
      "Iteration: 4436 lambda_n: 0.9876308567140265 Loss: 0.0053779396773814625\n",
      "Iteration: 4437 lambda_n: 0.8804135614459574 Loss: 0.0053664411611866565\n",
      "Iteration: 4438 lambda_n: 0.9449424746303354 Loss: 0.005356190925025501\n",
      "Iteration: 4439 lambda_n: 0.9095840782921081 Loss: 0.005345189409503839\n",
      "Iteration: 4440 lambda_n: 0.9273448277412238 Loss: 0.005334599555035571\n",
      "Iteration: 4441 lambda_n: 0.9586483585241683 Loss: 0.005323802920686068\n",
      "Iteration: 4442 lambda_n: 0.9973596534277156 Loss: 0.0053126418343043364\n",
      "Iteration: 4443 lambda_n: 0.9442534868773094 Loss: 0.005301030050827493\n",
      "Iteration: 4444 lambda_n: 0.8914597892626913 Loss: 0.005290036557234877\n",
      "Iteration: 4445 lambda_n: 0.894331547935384 Loss: 0.005279657715586541\n",
      "Iteration: 4446 lambda_n: 0.9644864280707194 Loss: 0.005269245439485023\n",
      "Iteration: 4447 lambda_n: 0.9873771186299106 Loss: 0.005258016383609875\n",
      "Iteration: 4448 lambda_n: 0.9791491725976386 Loss: 0.005246520822409517\n",
      "Iteration: 4449 lambda_n: 0.986108192567266 Loss: 0.005235121055340911\n",
      "Iteration: 4450 lambda_n: 0.884450047760493 Loss: 0.005223640267797157\n",
      "Iteration: 4451 lambda_n: 0.9353247239276273 Loss: 0.005213343037635498\n",
      "Iteration: 4452 lambda_n: 0.9823539520369751 Loss: 0.005202453497943817\n",
      "Iteration: 4453 lambda_n: 0.9732421467697373 Loss: 0.005191016419450315\n",
      "Iteration: 4454 lambda_n: 0.9493469560636093 Loss: 0.005179685425434027\n",
      "Iteration: 4455 lambda_n: 1.013346119962208 Loss: 0.005168632631795418\n",
      "Iteration: 4456 lambda_n: 0.9211730458034131 Loss: 0.005156834726502405\n",
      "Iteration: 4457 lambda_n: 0.8802452532215745 Loss: 0.005146109948397414\n",
      "Iteration: 4458 lambda_n: 0.9515168326194241 Loss: 0.005135861673108982\n",
      "Iteration: 4459 lambda_n: 0.9509033432772164 Loss: 0.0051247836169076\n",
      "Iteration: 4460 lambda_n: 0.9581670545957647 Loss: 0.00511371270333821\n",
      "Iteration: 4461 lambda_n: 0.9169476710811965 Loss: 0.0051025572219149514\n",
      "Iteration: 4462 lambda_n: 0.9639775468105912 Loss: 0.00509188163815453\n",
      "Iteration: 4463 lambda_n: 0.9667208307299376 Loss: 0.005080658508080319\n",
      "Iteration: 4464 lambda_n: 1.014058099865395 Loss: 0.005069403439331007\n",
      "Iteration: 4465 lambda_n: 0.8963346301726813 Loss: 0.005057597245446318\n",
      "Iteration: 4466 lambda_n: 0.8789710216072016 Loss: 0.005047161649731209\n",
      "Iteration: 4467 lambda_n: 0.9023914546151746 Loss: 0.005036928210269493\n",
      "Iteration: 4468 lambda_n: 1.0115903582291563 Loss: 0.005026422097958624\n",
      "Iteration: 4469 lambda_n: 0.9361035835266847 Loss: 0.005014644635082224\n",
      "Iteration: 4470 lambda_n: 0.9732417406801458 Loss: 0.005003746028699742\n",
      "Iteration: 4471 lambda_n: 0.9991852440117066 Loss: 0.004992415040577869\n",
      "Iteration: 4472 lambda_n: 0.9174052505511583 Loss: 0.004980782004723588\n",
      "Iteration: 4473 lambda_n: 0.9666825413119725 Loss: 0.004970101094279966\n",
      "Iteration: 4474 lambda_n: 0.9734380789746765 Loss: 0.004958846471976684\n",
      "Iteration: 4475 lambda_n: 0.8785662491719777 Loss: 0.004947513198246473\n",
      "Iteration: 4476 lambda_n: 0.9992688791458227 Loss: 0.004937284471890703\n",
      "Iteration: 4477 lambda_n: 0.9646723947703209 Loss: 0.004925650462643344\n",
      "Iteration: 4478 lambda_n: 0.9644344874901098 Loss: 0.004914419243769869\n",
      "Iteration: 4479 lambda_n: 0.9758856505366674 Loss: 0.0049031907948006315\n",
      "Iteration: 4480 lambda_n: 0.9519527749056859 Loss: 0.004891829025486891\n",
      "Iteration: 4481 lambda_n: 0.9928238574724816 Loss: 0.004880745895246433\n",
      "Iteration: 4482 lambda_n: 0.996830945889117 Loss: 0.004869186922629665\n",
      "Iteration: 4483 lambda_n: 0.9156767258578636 Loss: 0.0048575812974681115\n",
      "Iteration: 4484 lambda_n: 0.9473355499193329 Loss: 0.004846920512073474\n",
      "Iteration: 4485 lambda_n: 1.013492197611747 Loss: 0.0048358911382163205\n",
      "Iteration: 4486 lambda_n: 0.9807170378012255 Loss: 0.00482409153427477\n",
      "Iteration: 4487 lambda_n: 0.8797292754187472 Loss: 0.004812673515876569\n",
      "Iteration: 4488 lambda_n: 0.9296429106702433 Loss: 0.004802431249647049\n",
      "Iteration: 4489 lambda_n: 0.8915529813518809 Loss: 0.004791607862942917\n",
      "Iteration: 4490 lambda_n: 0.9085425155534977 Loss: 0.004781227939077607\n",
      "Iteration: 4491 lambda_n: 0.9098503112194327 Loss: 0.004770650214264034\n",
      "Iteration: 4492 lambda_n: 0.9953795481451787 Loss: 0.004760057263465486\n",
      "Iteration: 4493 lambda_n: 0.9701132650357722 Loss: 0.0047484685368398396\n",
      "Iteration: 4494 lambda_n: 0.9981242971684657 Loss: 0.004737173973490038\n",
      "Iteration: 4495 lambda_n: 0.9721185189929259 Loss: 0.004725553291191842\n",
      "Iteration: 4496 lambda_n: 0.9292264131760918 Loss: 0.004714235381752224\n",
      "Iteration: 4497 lambda_n: 0.881348022566459 Loss: 0.004703416844574816\n",
      "Iteration: 4498 lambda_n: 0.9512425027182947 Loss: 0.004693155732572336\n",
      "Iteration: 4499 lambda_n: 0.9471000304097084 Loss: 0.004682080872736618\n",
      "Iteration: 4500 lambda_n: 0.9423793889030309 Loss: 0.004671054241772704\n",
      "Iteration: 4501 lambda_n: 0.9039689538594119 Loss: 0.004660082571025946\n",
      "Iteration: 4502 lambda_n: 1.0219734882881335 Loss: 0.004649558094583488\n",
      "Iteration: 4503 lambda_n: 0.8948235863350638 Loss: 0.004637659748068393\n",
      "Iteration: 4504 lambda_n: 1.0050077934295465 Loss: 0.0046272417468521095\n",
      "Iteration: 4505 lambda_n: 0.9904457505636548 Loss: 0.004615540923889741\n",
      "Iteration: 4506 lambda_n: 0.9909700715649369 Loss: 0.0046040096398564\n",
      "Iteration: 4507 lambda_n: 1.0207859458398092 Loss: 0.004592472251463388\n",
      "Iteration: 4508 lambda_n: 0.990156667929539 Loss: 0.004580587731231305\n",
      "Iteration: 4509 lambda_n: 0.9354209928480466 Loss: 0.00456905981302196\n",
      "Iteration: 4510 lambda_n: 1.0062525171212178 Loss: 0.004558169156025271\n",
      "Iteration: 4511 lambda_n: 0.9911508636831571 Loss: 0.004546453841692131\n",
      "Iteration: 4512 lambda_n: 0.8878836190002105 Loss: 0.004534914348707767\n",
      "Iteration: 4513 lambda_n: 1.0264272793172076 Loss: 0.0045245771466551985\n",
      "Iteration: 4514 lambda_n: 0.9764031611671126 Loss: 0.004512626947433762\n",
      "Iteration: 4515 lambda_n: 0.9020176252235136 Loss: 0.004501259155020924\n",
      "Iteration: 4516 lambda_n: 0.9060097267501315 Loss: 0.004490757397677583\n",
      "Iteration: 4517 lambda_n: 0.96166179546852 Loss: 0.0044802091622621314\n",
      "Iteration: 4518 lambda_n: 0.9198593061582133 Loss: 0.004469012996630383\n",
      "Iteration: 4519 lambda_n: 1.0274847035134413 Loss: 0.004458303517319666\n",
      "Iteration: 4520 lambda_n: 0.9895032804010279 Loss: 0.004446341007343702\n",
      "Iteration: 4521 lambda_n: 0.9735407745625204 Loss: 0.004434820696856137\n",
      "Iteration: 4522 lambda_n: 0.8981672550430848 Loss: 0.004423486230194509\n",
      "Iteration: 4523 lambda_n: 0.8975358156994436 Loss: 0.0044130293011899425\n",
      "Iteration: 4524 lambda_n: 0.9157068071486966 Loss: 0.004402579723773242\n",
      "Iteration: 4525 lambda_n: 0.9451133927891152 Loss: 0.004391918590295018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4526 lambda_n: 0.8867026523745937 Loss: 0.004380915090144586\n",
      "Iteration: 4527 lambda_n: 0.9837571030805682 Loss: 0.004370591638164004\n",
      "Iteration: 4528 lambda_n: 1.0095188470089567 Loss: 0.004359138227997645\n",
      "Iteration: 4529 lambda_n: 0.9773016854311863 Loss: 0.004347384886306066\n",
      "Iteration: 4530 lambda_n: 0.958338786676886 Loss: 0.00433600663356074\n",
      "Iteration: 4531 lambda_n: 0.9115338689721777 Loss: 0.00432484915675977\n",
      "Iteration: 4532 lambda_n: 0.9617613592628839 Loss: 0.004314236607112019\n",
      "Iteration: 4533 lambda_n: 1.0038846249642168 Loss: 0.004303039283040285\n",
      "Iteration: 4534 lambda_n: 0.9111801033516077 Loss: 0.004291351538133049\n",
      "Iteration: 4535 lambda_n: 0.9000874397336307 Loss: 0.0042807431073424536\n",
      "Iteration: 4536 lambda_n: 0.8972214291043478 Loss: 0.004270263823129798\n",
      "Iteration: 4537 lambda_n: 0.8739348548674897 Loss: 0.004259817906538255\n",
      "Iteration: 4538 lambda_n: 0.9449905991336955 Loss: 0.004249643104344623\n",
      "Iteration: 4539 lambda_n: 1.0029025380857444 Loss: 0.004238641034417601\n",
      "Iteration: 4540 lambda_n: 0.9930392435414463 Loss: 0.004226964723754289\n",
      "Iteration: 4541 lambda_n: 0.8915228913823733 Loss: 0.004215403246722755\n",
      "Iteration: 4542 lambda_n: 0.9072991772250831 Loss: 0.004205023675668005\n",
      "Iteration: 4543 lambda_n: 0.9594888559380649 Loss: 0.004194460428965531\n",
      "Iteration: 4544 lambda_n: 0.929271568288649 Loss: 0.004183289563048951\n",
      "Iteration: 4545 lambda_n: 0.9265627295504058 Loss: 0.004172470502479691\n",
      "Iteration: 4546 lambda_n: 0.8915282124030731 Loss: 0.004161682979654639\n",
      "Iteration: 4547 lambda_n: 0.9625332432562209 Loss: 0.004151303346849466\n",
      "Iteration: 4548 lambda_n: 0.9909059858935895 Loss: 0.004140097036771788\n",
      "Iteration: 4549 lambda_n: 0.9138930255452504 Loss: 0.00412856039658834\n",
      "Iteration: 4550 lambda_n: 0.9075547538841776 Loss: 0.00411792038117871\n",
      "Iteration: 4551 lambda_n: 0.9993124831859855 Loss: 0.0041073541592459495\n",
      "Iteration: 4552 lambda_n: 0.9060067233110416 Loss: 0.004095719646403884\n",
      "Iteration: 4553 lambda_n: 0.9568278660406779 Loss: 0.004085171447524648\n",
      "Iteration: 4554 lambda_n: 0.9999954522586151 Loss: 0.004074031562656158\n",
      "Iteration: 4555 lambda_n: 0.9641673515227529 Loss: 0.004062389098467688\n",
      "Iteration: 4556 lambda_n: 1.0141132528064993 Loss: 0.004051163763600255\n",
      "Iteration: 4557 lambda_n: 1.0185907977884758 Loss: 0.004039356932768232\n",
      "Iteration: 4558 lambda_n: 0.9382222012176257 Loss: 0.004027497972089393\n",
      "Iteration: 4559 lambda_n: 0.9459186292985773 Loss: 0.0040165747042055725\n",
      "Iteration: 4560 lambda_n: 0.9310588116113038 Loss: 0.0040055618305672195\n",
      "Iteration: 4561 lambda_n: 0.943326658311186 Loss: 0.003994721962646993\n",
      "Iteration: 4562 lambda_n: 0.9295742608618348 Loss: 0.003983739266153721\n",
      "Iteration: 4563 lambda_n: 0.8823594251389097 Loss: 0.003972916682215005\n",
      "Iteration: 4564 lambda_n: 0.9083646947395857 Loss: 0.003962643797822354\n",
      "Iteration: 4565 lambda_n: 0.9350909898455083 Loss: 0.003952068146673892\n",
      "Iteration: 4566 lambda_n: 0.8999146124502306 Loss: 0.003941181334222878\n",
      "Iteration: 4567 lambda_n: 1.013215866223374 Loss: 0.003930704063368565\n",
      "Iteration: 4568 lambda_n: 0.9699604733582876 Loss: 0.003918907680820978\n",
      "Iteration: 4569 lambda_n: 0.9225143667590067 Loss: 0.00390761489994607\n",
      "Iteration: 4570 lambda_n: 0.91497522162508 Loss: 0.0038968745111921713\n",
      "Iteration: 4571 lambda_n: 0.8742769899230505 Loss: 0.0038862218970968747\n",
      "Iteration: 4572 lambda_n: 0.9850225138619668 Loss: 0.0038760431128695783\n",
      "Iteration: 4573 lambda_n: 0.9744713751976014 Loss: 0.0038645749720965083\n",
      "Iteration: 4574 lambda_n: 0.9710577621972442 Loss: 0.003853229673169677\n",
      "Iteration: 4575 lambda_n: 0.9493426562892713 Loss: 0.003841924117328448\n",
      "Iteration: 4576 lambda_n: 0.908119448996065 Loss: 0.003830871380001211\n",
      "Iteration: 4577 lambda_n: 0.9921587624940404 Loss: 0.0038202985845721665\n",
      "Iteration: 4578 lambda_n: 1.020499406617368 Loss: 0.003808747360109043\n",
      "Iteration: 4579 lambda_n: 0.9867332230397344 Loss: 0.0037968661792798283\n",
      "Iteration: 4580 lambda_n: 0.8878530610804576 Loss: 0.003785378121830149\n",
      "Iteration: 4581 lambda_n: 0.9046432069706418 Loss: 0.003775041278295485\n",
      "Iteration: 4582 lambda_n: 0.88003736118501 Loss: 0.003764508955250249\n",
      "Iteration: 4583 lambda_n: 0.8792702838809405 Loss: 0.003754263106191544\n",
      "Iteration: 4584 lambda_n: 1.007175169770875 Loss: 0.0037440261878739214\n",
      "Iteration: 4585 lambda_n: 0.9658103103236333 Loss: 0.0037323001349159717\n",
      "Iteration: 4586 lambda_n: 0.9563611176361803 Loss: 0.003721055673031974\n",
      "Iteration: 4587 lambda_n: 1.0140479936632003 Loss: 0.0037099212235607313\n",
      "Iteration: 4588 lambda_n: 1.0043559847456016 Loss: 0.003698115153759338\n",
      "Iteration: 4589 lambda_n: 0.9805286377701774 Loss: 0.003686421923365097\n",
      "Iteration: 4590 lambda_n: 0.9959630096794292 Loss: 0.0036750061032723574\n",
      "Iteration: 4591 lambda_n: 1.0235001727837278 Loss: 0.0036634105882997967\n",
      "Iteration: 4592 lambda_n: 0.967872297050353 Loss: 0.0036514944715130148\n",
      "Iteration: 4593 lambda_n: 0.8990020016767363 Loss: 0.003640226003178513\n",
      "Iteration: 4594 lambda_n: 0.905063904797427 Loss: 0.003629759358366772\n",
      "Iteration: 4595 lambda_n: 1.0140257177953889 Loss: 0.0036192221377844246\n",
      "Iteration: 4596 lambda_n: 0.9913986635352159 Loss: 0.0036074163276350007\n",
      "Iteration: 4597 lambda_n: 0.9169979870616912 Loss: 0.0035958739533536676\n",
      "Iteration: 4598 lambda_n: 0.8952977101873397 Loss: 0.003585197790133492\n",
      "Iteration: 4599 lambda_n: 0.9544503309148535 Loss: 0.0035747742727524107\n",
      "Iteration: 4600 lambda_n: 0.9496052322076252 Loss: 0.003563662070105054\n",
      "Iteration: 4601 lambda_n: 0.9052373538960232 Loss: 0.0035526062766272986\n",
      "Iteration: 4602 lambda_n: 0.9669315710513902 Loss: 0.003542067036884672\n",
      "Iteration: 4603 lambda_n: 0.9597069767940912 Loss: 0.0035308095213068907\n",
      "Iteration: 4604 lambda_n: 0.8764063994892224 Loss: 0.0035196361182117703\n",
      "Iteration: 4605 lambda_n: 0.8869666906258852 Loss: 0.0035094325433871225\n",
      "Iteration: 4606 lambda_n: 0.9762186738267145 Loss: 0.003499106020240392\n",
      "Iteration: 4607 lambda_n: 0.9909279418968863 Loss: 0.0034877403795567476\n",
      "Iteration: 4608 lambda_n: 0.9210999362857165 Loss: 0.0034762034860314557\n",
      "Iteration: 4609 lambda_n: 0.945554766496222 Loss: 0.0034654795661481917\n",
      "Iteration: 4610 lambda_n: 1.0137994656192906 Loss: 0.003454470930566566\n",
      "Iteration: 4611 lambda_n: 0.8806073777269742 Loss: 0.0034426677550832775\n",
      "Iteration: 4612 lambda_n: 1.014824128219422 Loss: 0.00343241527051207\n",
      "Iteration: 4613 lambda_n: 1.0039205268443128 Loss: 0.003420600165446152\n",
      "Iteration: 4614 lambda_n: 0.9818779600526725 Loss: 0.0034089120057573126\n",
      "Iteration: 4615 lambda_n: 0.9833099629795767 Loss: 0.0033974804770146583\n",
      "Iteration: 4616 lambda_n: 1.0104957467205347 Loss: 0.0033860322761897384\n",
      "Iteration: 4617 lambda_n: 0.9232061299395743 Loss: 0.0033742675645083973\n",
      "Iteration: 4618 lambda_n: 1.0250398797277762 Loss: 0.003363519123513507\n",
      "Iteration: 4619 lambda_n: 0.8865630228562346 Loss: 0.0033515850816155574\n",
      "Iteration: 4620 lambda_n: 0.9339561191839891 Loss: 0.003341263258593939\n",
      "Iteration: 4621 lambda_n: 1.012234266820813 Loss: 0.003330389660779564\n",
      "Iteration: 4622 lambda_n: 0.8977419008596204 Loss: 0.0033186047085148503\n",
      "Iteration: 4623 lambda_n: 0.9066768439655951 Loss: 0.003308152735326449\n",
      "Iteration: 4624 lambda_n: 0.9227641055511363 Loss: 0.003297596736959316\n",
      "Iteration: 4625 lambda_n: 0.965103847829894 Loss: 0.003286853442442244\n",
      "Iteration: 4626 lambda_n: 0.9369856800359563 Loss: 0.0032756172068891132\n",
      "Iteration: 4627 lambda_n: 0.8825432294810956 Loss: 0.003264708337541976\n",
      "Iteration: 4628 lambda_n: 0.9627960783361881 Loss: 0.003254433315240511\n",
      "Iteration: 4629 lambda_n: 0.9242102613243907 Loss: 0.0032432239480140457\n",
      "Iteration: 4630 lambda_n: 0.8760911043007159 Loss: 0.003232463816747139\n",
      "Iteration: 4631 lambda_n: 0.8911277734865932 Loss: 0.0032222639134841776\n",
      "Iteration: 4632 lambda_n: 0.9897496469199567 Loss: 0.0032118889456091058\n",
      "Iteration: 4633 lambda_n: 0.9932759738796104 Loss: 0.0032003657711870154\n",
      "Iteration: 4634 lambda_n: 0.9063908146095878 Loss: 0.003188801541483855\n",
      "Iteration: 4635 lambda_n: 0.9979448857890061 Loss: 0.0031782488735158606\n",
      "Iteration: 4636 lambda_n: 1.002417914950799 Loss: 0.0031666302859985045\n",
      "Iteration: 4637 lambda_n: 0.8883545007386933 Loss: 0.0031549596212066522\n",
      "Iteration: 4638 lambda_n: 0.9975880746489779 Loss: 0.0031446169413600227\n",
      "Iteration: 4639 lambda_n: 0.9908063606737688 Loss: 0.0031330025081106132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4640 lambda_n: 0.9558441373124762 Loss: 0.003121467031092243\n",
      "Iteration: 4641 lambda_n: 0.99263534841496 Loss: 0.0031103386022809966\n",
      "Iteration: 4642 lambda_n: 1.0157816861664113 Loss: 0.003098781831305893\n",
      "Iteration: 4643 lambda_n: 0.8947438549141941 Loss: 0.003086955578799149\n",
      "Iteration: 4644 lambda_n: 1.0248780672744247 Loss: 0.003076538510965152\n",
      "Iteration: 4645 lambda_n: 0.9395834330074706 Loss: 0.003064606353772716\n",
      "Iteration: 4646 lambda_n: 0.8855287813919942 Loss: 0.003053667240578066\n",
      "Iteration: 4647 lambda_n: 1.0170295329279333 Loss: 0.00304335745944178\n",
      "Iteration: 4648 lambda_n: 1.022899037858463 Loss: 0.00303151667900594\n",
      "Iteration: 4649 lambda_n: 0.9944035830005354 Loss: 0.003019607562808171\n",
      "Iteration: 4650 lambda_n: 0.9954847846272648 Loss: 0.003008030205367527\n",
      "Iteration: 4651 lambda_n: 0.9234837948617095 Loss: 0.0029964402600508315\n",
      "Iteration: 4652 lambda_n: 0.8841281536031795 Loss: 0.002985688587275861\n",
      "Iteration: 4653 lambda_n: 0.9633792495905988 Loss: 0.002975395113118721\n",
      "Iteration: 4654 lambda_n: 0.9601302994435676 Loss: 0.0029641789570138168\n",
      "Iteration: 4655 lambda_n: 0.9380495768164042 Loss: 0.00295300062688194\n",
      "Iteration: 4656 lambda_n: 0.9147016117687504 Loss: 0.002942079371890646\n",
      "Iteration: 4657 lambda_n: 0.9794715136714882 Loss: 0.002931429945925139\n",
      "Iteration: 4658 lambda_n: 0.9900740872545354 Loss: 0.002920026435520404\n",
      "Iteration: 4659 lambda_n: 1.0193458733456322 Loss: 0.0029084994845359376\n",
      "Iteration: 4660 lambda_n: 0.920337201674211 Loss: 0.002896631736413259\n",
      "Iteration: 4661 lambda_n: 0.9636352371209134 Loss: 0.0028859166981162754\n",
      "Iteration: 4662 lambda_n: 0.9626426713637052 Loss: 0.0028746975618815667\n",
      "Iteration: 4663 lambda_n: 0.9199495365986518 Loss: 0.0028634899816326535\n",
      "Iteration: 4664 lambda_n: 0.8757130785934323 Loss: 0.0028527794568037985\n",
      "Iteration: 4665 lambda_n: 0.9439259837686215 Loss: 0.0028425839555530955\n",
      "Iteration: 4666 lambda_n: 0.8769522632508705 Loss: 0.0028315942846624757\n",
      "Iteration: 4667 lambda_n: 0.9756227116623154 Loss: 0.002821384356231173\n",
      "Iteration: 4668 lambda_n: 0.9116921753860509 Loss: 0.0028100256558102254\n",
      "Iteration: 4669 lambda_n: 0.8943305467666025 Loss: 0.002799411267533687\n",
      "Iteration: 4670 lambda_n: 0.9980385684018673 Loss: 0.0027889990122710816\n",
      "Iteration: 4671 lambda_n: 0.9870993480924224 Loss: 0.00277737933501375\n",
      "Iteration: 4672 lambda_n: 0.9562555334253785 Loss: 0.0027658870177999927\n",
      "Iteration: 4673 lambda_n: 0.9790841031698785 Loss: 0.0027547538001313453\n",
      "Iteration: 4674 lambda_n: 0.9611331938869634 Loss: 0.002743354800562835\n",
      "Iteration: 4675 lambda_n: 0.9585551449619244 Loss: 0.002732164794716788\n",
      "Iteration: 4676 lambda_n: 0.9666490625984889 Loss: 0.0027210048038633144\n",
      "Iteration: 4677 lambda_n: 1.0228575378026594 Loss: 0.0027097505794916534\n",
      "Iteration: 4678 lambda_n: 1.0075407730079242 Loss: 0.0026978419472368674\n",
      "Iteration: 4679 lambda_n: 0.964930035825 Loss: 0.0026861116406430266\n",
      "Iteration: 4680 lambda_n: 1.0216711804633178 Loss: 0.002674877430137466\n",
      "Iteration: 4681 lambda_n: 0.9564907520257259 Loss: 0.0026629826101417027\n",
      "Iteration: 4682 lambda_n: 0.965403392428376 Loss: 0.0026518466541559397\n",
      "Iteration: 4683 lambda_n: 0.9913278212904456 Loss: 0.00264060693266216\n",
      "Iteration: 4684 lambda_n: 1.003073896857626 Loss: 0.002629065385694007\n",
      "Iteration: 4685 lambda_n: 0.9530349651647586 Loss: 0.002617387084914442\n",
      "Iteration: 4686 lambda_n: 0.9235488910687832 Loss: 0.0026062913630664807\n",
      "Iteration: 4687 lambda_n: 0.9473912826144618 Loss: 0.0025955389332377645\n",
      "Iteration: 4688 lambda_n: 0.9393002880417812 Loss: 0.002584508918081026\n",
      "Iteration: 4689 lambda_n: 1.0007523433545533 Loss: 0.0025735731024545327\n",
      "Iteration: 4690 lambda_n: 0.9034169914695561 Loss: 0.0025619218305107908\n",
      "Iteration: 4691 lambda_n: 0.9347722171862884 Loss: 0.0025514037866673145\n",
      "Iteration: 4692 lambda_n: 1.0067204912979728 Loss: 0.002540520689229342\n",
      "Iteration: 4693 lambda_n: 0.970714320115613 Loss: 0.002528799933117214\n",
      "Iteration: 4694 lambda_n: 0.9732199968615186 Loss: 0.0025174983793344635\n",
      "Iteration: 4695 lambda_n: 0.9736255415568339 Loss: 0.002506167653201378\n",
      "Iteration: 4696 lambda_n: 0.9073374989836843 Loss: 0.0024948322055319888\n",
      "Iteration: 4697 lambda_n: 0.9400348488704112 Loss: 0.0024842685172571065\n",
      "Iteration: 4698 lambda_n: 0.953887814730895 Loss: 0.0024733241496942967\n",
      "Iteration: 4699 lambda_n: 0.992767245486054 Loss: 0.0024622184988235085\n",
      "Iteration: 4700 lambda_n: 0.9062523401947058 Loss: 0.0024506601937141458\n",
      "Iteration: 4701 lambda_n: 1.0256840888144771 Loss: 0.002440109139496288\n",
      "Iteration: 4702 lambda_n: 0.9410331497982778 Loss: 0.0024281675996717534\n",
      "Iteration: 4703 lambda_n: 1.016231804266863 Loss: 0.002417211609485546\n",
      "Iteration: 4704 lambda_n: 0.9281861885872159 Loss: 0.002405380118049058\n",
      "Iteration: 4705 lambda_n: 0.9753450174012231 Loss: 0.0023945736988100776\n",
      "Iteration: 4706 lambda_n: 0.9099486132431389 Loss: 0.0023832182323357495\n",
      "Iteration: 4707 lambda_n: 0.8968966263314901 Loss: 0.0023726241443263888\n",
      "Iteration: 4708 lambda_n: 0.9085952223501073 Loss: 0.002362182014254106\n",
      "Iteration: 4709 lambda_n: 0.9968201979546999 Loss: 0.0023516036831512997\n",
      "Iteration: 4710 lambda_n: 0.8851618131475394 Loss: 0.0023399981917008312\n",
      "Iteration: 4711 lambda_n: 0.9333986038700927 Loss: 0.002329692684393385\n",
      "Iteration: 4712 lambda_n: 1.0125354451617632 Loss: 0.0023188255796754566\n",
      "Iteration: 4713 lambda_n: 0.9935113859652347 Loss: 0.0023070371233323153\n",
      "Iteration: 4714 lambda_n: 1.0164909661566386 Loss: 0.0022954701548553538\n",
      "Iteration: 4715 lambda_n: 1.003688682445377 Loss: 0.002283635646358175\n",
      "Iteration: 4716 lambda_n: 1.0008628235233903 Loss: 0.002271950188629311\n",
      "Iteration: 4717 lambda_n: 0.8764289012957506 Loss: 0.002260297631020664\n",
      "Iteration: 4718 lambda_n: 0.8972091247911337 Loss: 0.0022500937968867683\n",
      "Iteration: 4719 lambda_n: 0.916040382853908 Loss: 0.002239648028766331\n",
      "Iteration: 4720 lambda_n: 0.9176733333581415 Loss: 0.002228983017514206\n",
      "Iteration: 4721 lambda_n: 0.9429571774279369 Loss: 0.002218298994635219\n",
      "Iteration: 4722 lambda_n: 0.9011073523571635 Loss: 0.00220732060431559\n",
      "Iteration: 4723 lambda_n: 1.010479109662168 Loss: 0.0021968294511086977\n",
      "Iteration: 4724 lambda_n: 0.9208848521242443 Loss: 0.0021850649359201673\n",
      "Iteration: 4725 lambda_n: 0.9633360769486871 Loss: 0.0021743435229736006\n",
      "Iteration: 4726 lambda_n: 1.020185353736298 Loss: 0.002163127871152264\n",
      "Iteration: 4727 lambda_n: 1.027465291988051 Loss: 0.0021512503509665886\n",
      "Iteration: 4728 lambda_n: 0.9025789635511394 Loss: 0.0021392880740361327\n",
      "Iteration: 4729 lambda_n: 0.9105270377987646 Loss: 0.002128779787696328\n",
      "Iteration: 4730 lambda_n: 0.9810505239308603 Loss: 0.002118178965825634\n",
      "Iteration: 4731 lambda_n: 0.9579272562740468 Loss: 0.002106757073448402\n",
      "Iteration: 4732 lambda_n: 0.899042460997155 Loss: 0.0020956043940103166\n",
      "Iteration: 4733 lambda_n: 0.9646166481203373 Loss: 0.0020851372815184335\n",
      "Iteration: 4734 lambda_n: 0.9026967742409787 Loss: 0.0020739067207961085\n",
      "Iteration: 4735 lambda_n: 1.0107164122801153 Loss: 0.0020633970629562706\n",
      "Iteration: 4736 lambda_n: 1.0094281155652318 Loss: 0.00205162978522688\n",
      "Iteration: 4737 lambda_n: 0.9113570036656382 Loss: 0.0020398775065298114\n",
      "Iteration: 4738 lambda_n: 0.9619625416993168 Loss: 0.002029267021925934\n",
      "Iteration: 4739 lambda_n: 0.9554617733857071 Loss: 0.002018067361771588\n",
      "Iteration: 4740 lambda_n: 0.9608829877044922 Loss: 0.0020069433869091572\n",
      "Iteration: 4741 lambda_n: 0.9623694376508296 Loss: 0.0019957562955165878\n",
      "Iteration: 4742 lambda_n: 0.9530726214139906 Loss: 0.0019845518981345786\n",
      "Iteration: 4743 lambda_n: 0.9879953924960391 Loss: 0.0019734557390655807\n",
      "Iteration: 4744 lambda_n: 0.9119535791003217 Loss: 0.0019619529912516523\n",
      "Iteration: 4745 lambda_n: 0.9231347451003585 Loss: 0.0019513355611555827\n",
      "Iteration: 4746 lambda_n: 0.9019737609591266 Loss: 0.0019405879542170876\n",
      "Iteration: 4747 lambda_n: 0.9474969977677833 Loss: 0.001930086714278105\n",
      "Iteration: 4748 lambda_n: 0.9407628700138775 Loss: 0.0019190554694095271\n",
      "Iteration: 4749 lambda_n: 0.9961034840755697 Loss: 0.0019081026267366054\n",
      "Iteration: 4750 lambda_n: 0.8859363922207333 Loss: 0.001896505480380193\n",
      "Iteration: 4751 lambda_n: 1.0173131081831022 Loss: 0.0018861909557170511\n",
      "Iteration: 4752 lambda_n: 0.8977733444049146 Loss: 0.0018743468761642024\n",
      "Iteration: 4753 lambda_n: 0.9564778831250643 Loss: 0.0018638945397197954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4754 lambda_n: 0.9735772685791386 Loss: 0.0018527587350385057\n",
      "Iteration: 4755 lambda_n: 0.9844079856649852 Loss: 0.0018414238505932757\n",
      "Iteration: 4756 lambda_n: 0.9451640945340142 Loss: 0.0018299628694256863\n",
      "Iteration: 4757 lambda_n: 1.0243416963463652 Loss: 0.001818958785729216\n",
      "Iteration: 4758 lambda_n: 1.0014256175044025 Loss: 0.0018070328759272223\n",
      "Iteration: 4759 lambda_n: 1.0100789146386864 Loss: 0.0017953737668553393\n",
      "Iteration: 4760 lambda_n: 0.8739101584354331 Loss: 0.0017836139116955382\n",
      "Iteration: 4761 lambda_n: 0.9093327097796547 Loss: 0.0017734394028348496\n",
      "Iteration: 4762 lambda_n: 1.006650299643733 Loss: 0.0017628524865379584\n",
      "Iteration: 4763 lambda_n: 0.9638690553154353 Loss: 0.0017511325491261405\n",
      "Iteration: 4764 lambda_n: 0.9401858292561881 Loss: 0.0017399106928518353\n",
      "Iteration: 4765 lambda_n: 1.0228471751442056 Loss: 0.0017289645688211123\n",
      "Iteration: 4766 lambda_n: 0.8845244018961737 Loss: 0.001717056059166587\n",
      "Iteration: 4767 lambda_n: 0.8758175486132793 Loss: 0.0017067579739625137\n",
      "Iteration: 4768 lambda_n: 0.8761725221372306 Loss: 0.00169656125841093\n",
      "Iteration: 4769 lambda_n: 0.9665996255588978 Loss: 0.0016863604100921417\n",
      "Iteration: 4770 lambda_n: 0.9826088463391454 Loss: 0.0016751067632352716\n",
      "Iteration: 4771 lambda_n: 0.9456033879975654 Loss: 0.0016636667288678132\n",
      "Iteration: 4772 lambda_n: 0.9031609885315105 Loss: 0.0016526575309780498\n",
      "Iteration: 4773 lambda_n: 0.9633910499331044 Loss: 0.0016421424692107532\n",
      "Iteration: 4774 lambda_n: 0.9523579027618059 Loss: 0.0016309261783077145\n",
      "Iteration: 4775 lambda_n: 0.9370382499390885 Loss: 0.0016198383409613201\n",
      "Iteration: 4776 lambda_n: 1.0017046592855172 Loss: 0.0016089288628599722\n",
      "Iteration: 4777 lambda_n: 0.973562689659983 Loss: 0.001597266505398408\n",
      "Iteration: 4778 lambda_n: 0.9163346720237743 Loss: 0.0015859317911468167\n",
      "Iteration: 4779 lambda_n: 1.0234534949447858 Loss: 0.0015752633547346739\n",
      "Iteration: 4780 lambda_n: 0.8739003413307562 Loss: 0.001563347786278374\n",
      "Iteration: 4781 lambda_n: 0.8966895669474833 Loss: 0.00155317339206\n",
      "Iteration: 4782 lambda_n: 0.9566746978445632 Loss: 0.0015427336740494063\n",
      "Iteration: 4783 lambda_n: 1.0224833344611761 Loss: 0.0015315955785133156\n",
      "Iteration: 4784 lambda_n: 0.947810731003095 Loss: 0.001519691305228662\n",
      "Iteration: 4785 lambda_n: 0.9355341448395592 Loss: 0.001508656408555927\n",
      "Iteration: 4786 lambda_n: 0.9190210183939979 Loss: 0.0014977644421880437\n",
      "Iteration: 4787 lambda_n: 0.9935709930057954 Loss: 0.0014870647300907864\n",
      "Iteration: 4788 lambda_n: 1.0178004341705063 Loss: 0.0014754970691312618\n",
      "Iteration: 4789 lambda_n: 1.007651536767651 Loss: 0.0014636473166645386\n",
      "Iteration: 4790 lambda_n: 0.9065653736620048 Loss: 0.0014519157228669666\n",
      "Iteration: 4791 lambda_n: 0.9134969550208636 Loss: 0.0014413610258248895\n",
      "Iteration: 4792 lambda_n: 0.9896845201540868 Loss: 0.0014307256277911018\n",
      "Iteration: 4793 lambda_n: 0.969269891269989 Loss: 0.0014192032152383755\n",
      "Iteration: 4794 lambda_n: 0.9862703302148575 Loss: 0.0014079184802409045\n",
      "Iteration: 4795 lambda_n: 0.9618874934698666 Loss: 0.0013964358174739149\n",
      "Iteration: 4796 lambda_n: 0.9373021378379455 Loss: 0.0013852370321632973\n",
      "Iteration: 4797 lambda_n: 0.8906679738432323 Loss: 0.0013743244821358116\n",
      "Iteration: 4798 lambda_n: 0.9436281283325223 Loss: 0.001363954870873484\n",
      "Iteration: 4799 lambda_n: 0.9165867088446595 Loss: 0.0013529686704740879\n",
      "Iteration: 4800 lambda_n: 0.9801973479012639 Loss: 0.0013422973001046244\n",
      "Iteration: 4801 lambda_n: 1.0216310802104762 Loss: 0.0013308853422214757\n",
      "Iteration: 4802 lambda_n: 0.925984610597464 Loss: 0.001318990991697971\n",
      "Iteration: 4803 lambda_n: 1.0011203396129102 Loss: 0.0013082102062173294\n",
      "Iteration: 4804 lambda_n: 0.9880102887321695 Loss: 0.0012965546522498267\n",
      "Iteration: 4805 lambda_n: 1.0271227736338984 Loss: 0.0012850517322091804\n",
      "Iteration: 4806 lambda_n: 0.878200556273117 Loss: 0.0012730934446811557\n",
      "Iteration: 4807 lambda_n: 0.9308539669586262 Loss: 0.0012628689856323913\n",
      "Iteration: 4808 lambda_n: 0.9408955288102475 Loss: 0.0012520315087255578\n",
      "Iteration: 4809 lambda_n: 0.9330263664485694 Loss: 0.0012410771228526514\n",
      "Iteration: 4810 lambda_n: 0.966102272789782 Loss: 0.0012302143538050457\n",
      "Iteration: 4811 lambda_n: 0.9514466737164957 Loss: 0.0012189664982007313\n",
      "Iteration: 4812 lambda_n: 1.0127132529866665 Loss: 0.0012078892705816608\n",
      "Iteration: 4813 lambda_n: 0.9389619911015843 Loss: 0.0011960987462118658\n",
      "Iteration: 4814 lambda_n: 0.9343076954593557 Loss: 0.001185166871684459\n",
      "Iteration: 4815 lambda_n: 0.8753908355640083 Loss: 0.0011742891848639842\n",
      "Iteration: 4816 lambda_n: 0.969581778591743 Loss: 0.0011640974382036552\n",
      "Iteration: 4817 lambda_n: 0.9097281978915348 Loss: 0.0011528090725584752\n",
      "Iteration: 4818 lambda_n: 0.9480534025957666 Loss: 0.0011422175528542355\n",
      "Iteration: 4819 lambda_n: 0.889001041528412 Loss: 0.0011311798315915235\n",
      "Iteration: 4820 lambda_n: 0.9994845762673671 Loss: 0.001120829628058635\n",
      "Iteration: 4821 lambda_n: 0.8974535087184909 Loss: 0.0011091931188901242\n",
      "Iteration: 4822 lambda_n: 1.0021874616398287 Loss: 0.001098744507470308\n",
      "Iteration: 4823 lambda_n: 0.9158526758729751 Loss: 0.0010870765299852393\n",
      "Iteration: 4824 lambda_n: 0.8939679691246547 Loss: 0.0010764137061280397\n",
      "Iteration: 4825 lambda_n: 0.9677237154593522 Loss: 0.0010660056752099165\n",
      "Iteration: 4826 lambda_n: 1.0190142952427013 Loss: 0.0010547389423115343\n",
      "Iteration: 4827 lambda_n: 0.9856470371368828 Loss: 0.0010428750583618958\n",
      "Iteration: 4828 lambda_n: 0.9845644592840711 Loss: 0.0010313996530733587\n",
      "Iteration: 4829 lambda_n: 0.901873728131879 Loss: 0.0010199368517381934\n",
      "Iteration: 4830 lambda_n: 1.0151294957016 Loss: 0.0010094367780762923\n",
      "Iteration: 4831 lambda_n: 1.0113822685366498 Loss: 0.0009976181230671158\n",
      "Iteration: 4832 lambda_n: 0.9103207714256452 Loss: 0.000985843095220192\n",
      "Iteration: 4833 lambda_n: 0.896027507169711 Loss: 0.000975244676861949\n",
      "Iteration: 4834 lambda_n: 0.9786249122858082 Loss: 0.0009648126679984702\n",
      "Iteration: 4835 lambda_n: 0.9093953189336103 Loss: 0.0009534190180826807\n",
      "Iteration: 4836 lambda_n: 1.016438956853331 Loss: 0.0009428313744022464\n",
      "Iteration: 4837 lambda_n: 1.0221029397938717 Loss: 0.0009309974741792894\n",
      "Iteration: 4838 lambda_n: 0.9932677024524653 Loss: 0.0009190976310196403\n",
      "Iteration: 4839 lambda_n: 0.873959120665601 Loss: 0.0009075335024226048\n",
      "Iteration: 4840 lambda_n: 1.0135127737158527 Loss: 0.0008973584251485993\n",
      "Iteration: 4841 lambda_n: 1.009958807987869 Loss: 0.0008855585931933542\n",
      "Iteration: 4842 lambda_n: 0.9262234135998617 Loss: 0.0008738001383594814\n",
      "Iteration: 4843 lambda_n: 0.9705660488461625 Loss: 0.0008630165736747478\n",
      "Iteration: 4844 lambda_n: 0.9321185599701806 Loss: 0.0008517167494871246\n",
      "Iteration: 4845 lambda_n: 0.8777519349457562 Loss: 0.0008408645505844952\n",
      "Iteration: 4846 lambda_n: 0.9973099959779972 Loss: 0.0008306453156514178\n",
      "Iteration: 4847 lambda_n: 1.0139880731923254 Loss: 0.0008190341249409075\n",
      "Iteration: 4848 lambda_n: 0.8785632391225999 Loss: 0.0008072287596145066\n",
      "Iteration: 4849 lambda_n: 0.9106981602745029 Loss: 0.0007970000791909655\n",
      "Iteration: 4850 lambda_n: 0.940179702233842 Loss: 0.0007863972677011181\n",
      "Iteration: 4851 lambda_n: 0.9514339835037701 Loss: 0.0007754512171421098\n",
      "Iteration: 4852 lambda_n: 0.8986431980141836 Loss: 0.000764374138564354\n",
      "Iteration: 4853 lambda_n: 0.9839790023321248 Loss: 0.000753911677225069\n",
      "Iteration: 4854 lambda_n: 1.0127191288248583 Loss: 0.0007424556930882616\n",
      "Iteration: 4855 lambda_n: 0.9789571733067722 Loss: 0.0007306651018422023\n",
      "Iteration: 4856 lambda_n: 0.9736780270202287 Loss: 0.000719267584523837\n",
      "Iteration: 4857 lambda_n: 0.8987810854594903 Loss: 0.0007079315297800797\n",
      "Iteration: 4858 lambda_n: 0.8943473492102627 Loss: 0.0006974674633843287\n",
      "Iteration: 4859 lambda_n: 0.8817447545313856 Loss: 0.0006870550168648786\n",
      "Iteration: 4860 lambda_n: 1.027459381385138 Loss: 0.0006767892962265582\n",
      "Iteration: 4861 lambda_n: 0.8759227800658131 Loss: 0.0006648270919144569\n",
      "Iteration: 4862 lambda_n: 0.9993618984536892 Loss: 0.0006546291538172489\n",
      "Iteration: 4863 lambda_n: 0.9145749717287209 Loss: 0.0006429940748692023\n",
      "Iteration: 4864 lambda_n: 0.9919646990508998 Loss: 0.0006323461284983066\n",
      "Iteration: 4865 lambda_n: 0.9961182716176795 Loss: 0.0006207971717204488\n",
      "Iteration: 4866 lambda_n: 0.9390735234360813 Loss: 0.0006091998570687561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4867 lambda_n: 0.9152571587525808 Loss: 0.0005982666864694822\n",
      "Iteration: 4868 lambda_n: 1.0238299693880943 Loss: 0.0005876107982105879\n",
      "Iteration: 4869 lambda_n: 0.9957013325421088 Loss: 0.0005756908503610719\n",
      "Iteration: 4870 lambda_n: 0.917261007563448 Loss: 0.0005640983905672141\n",
      "Iteration: 4871 lambda_n: 1.0046544217269457 Loss: 0.0005534191730125836\n",
      "Iteration: 4872 lambda_n: 0.9970365201859398 Loss: 0.0005417224772712084\n",
      "Iteration: 4873 lambda_n: 0.8767776700453029 Loss: 0.0005301144732954473\n",
      "Iteration: 4874 lambda_n: 0.9890955576599954 Loss: 0.0005199065840520492\n",
      "Iteration: 4875 lambda_n: 1.0012215630854957 Loss: 0.000508391033513721\n",
      "Iteration: 4876 lambda_n: 0.9673555883268281 Loss: 0.0004967343064265467\n",
      "Iteration: 4877 lambda_n: 0.9120666600293044 Loss: 0.0004854718648206729\n",
      "Iteration: 4878 lambda_n: 1.0030108144177419 Loss: 0.0004748531257186875\n",
      "Iteration: 4879 lambda_n: 0.8905129316690368 Loss: 0.0004631755705233133\n",
      "Iteration: 4880 lambda_n: 1.0180497127415395 Loss: 0.00045280783922378124\n",
      "Iteration: 4881 lambda_n: 0.9946808841082055 Loss: 0.00044095590624640787\n",
      "Iteration: 4882 lambda_n: 0.9847032619405421 Loss: 0.0004294940079508857\n",
      "Iteration: 4883 lambda_n: 1.0054899768978216 Loss: 0.00041838532022529996\n",
      "Iteration: 4884 lambda_n: 0.926460883716727 Loss: 0.00040733540213495553\n",
      "Iteration: 4885 lambda_n: 0.9950410153486168 Loss: 0.0003974474410441031\n",
      "Iteration: 4886 lambda_n: 0.9215799225364378 Loss: 0.0003871222544764014\n",
      "Iteration: 4887 lambda_n: 0.9365863798466736 Loss: 0.0003778483500930632\n",
      "Iteration: 4888 lambda_n: 1.0126364250772013 Loss: 0.00036868983958768205\n",
      "Iteration: 4889 lambda_n: 0.8771637439599677 Loss: 0.00035907326945327687\n",
      "Iteration: 4890 lambda_n: 0.97040925962743 Loss: 0.0003510017012748312\n",
      "Iteration: 4891 lambda_n: 0.8839122195148789 Loss: 0.0003423133006039407\n",
      "Iteration: 4892 lambda_n: 0.9948138894298504 Loss: 0.0003346342910680704\n",
      "Iteration: 4893 lambda_n: 0.9000047838505385 Loss: 0.00032622657146988106\n",
      "Iteration: 4894 lambda_n: 1.0270496124509017 Loss: 0.0003188508473697881\n",
      "Iteration: 4895 lambda_n: 1.0097078234383587 Loss: 0.000310666286242484\n",
      "Iteration: 4896 lambda_n: 0.885262086687829 Loss: 0.0003028718469191505\n",
      "Iteration: 4897 lambda_n: 0.9899431623676 Loss: 0.00029624740093624734\n",
      "Iteration: 4898 lambda_n: 0.9611289830163513 Loss: 0.00028904007830551107\n",
      "Iteration: 4899 lambda_n: 0.9946509532627711 Loss: 0.0002822530791689957\n",
      "Iteration: 4900 lambda_n: 0.9787186943488013 Loss: 0.00027543494002436766\n",
      "Iteration: 4901 lambda_n: 1.016683555001485 Loss: 0.00026892877519235734\n",
      "Iteration: 4902 lambda_n: 0.8818964140168197 Loss: 0.000262371623779009\n",
      "Iteration: 4903 lambda_n: 0.8962829673626355 Loss: 0.000256858597373858\n",
      "Iteration: 4904 lambda_n: 0.9452082026296855 Loss: 0.0002514059076541188\n",
      "Iteration: 4905 lambda_n: 1.0105954999188418 Loss: 0.000245812570659809\n",
      "Iteration: 4906 lambda_n: 0.88213176258505 Loss: 0.00024000460186958828\n",
      "Iteration: 4907 lambda_n: 0.9339894329656182 Loss: 0.0002350897241704672\n",
      "Iteration: 4908 lambda_n: 1.0095332054636819 Loss: 0.00023002575075107312\n",
      "Iteration: 4909 lambda_n: 0.9726489886342832 Loss: 0.0002247080485344815\n",
      "Iteration: 4910 lambda_n: 0.9333603102837693 Loss: 0.00021974141193457255\n",
      "Iteration: 4911 lambda_n: 0.9489133975865254 Loss: 0.0002151158091691679\n",
      "Iteration: 4912 lambda_n: 0.922349488825552 Loss: 0.0002105463583474156\n",
      "Iteration: 4913 lambda_n: 0.8802159785127938 Loss: 0.0002062323822688548\n",
      "Iteration: 4914 lambda_n: 0.9465920977833068 Loss: 0.00020223019240224729\n",
      "Iteration: 4915 lambda_n: 0.8756728577997682 Loss: 0.00019804122027703864\n",
      "Iteration: 4916 lambda_n: 0.880463806669234 Loss: 0.00019427650069275202\n",
      "Iteration: 4917 lambda_n: 0.8999103521374722 Loss: 0.00019059121950665143\n",
      "Iteration: 4918 lambda_n: 0.8883854963501006 Loss: 0.00018692456558739686\n",
      "Iteration: 4919 lambda_n: 0.9685804000035345 Loss: 0.0001834027794303409\n",
      "Iteration: 4920 lambda_n: 1.0216158672589315 Loss: 0.0001796659494153812\n",
      "Iteration: 4921 lambda_n: 0.9051436970445672 Loss: 0.0001758391861758687\n",
      "Iteration: 4922 lambda_n: 0.9165785783620933 Loss: 0.00017255163556514867\n",
      "Iteration: 4923 lambda_n: 0.9993447718052517 Loss: 0.00016931244054957902\n",
      "Iteration: 4924 lambda_n: 0.94216767861625 Loss: 0.0001658774315822129\n",
      "Iteration: 4925 lambda_n: 1.0053876791334084 Loss: 0.00016273463984842407\n",
      "Iteration: 4926 lambda_n: 1.0048130091850975 Loss: 0.00015947466465694827\n",
      "Iteration: 4927 lambda_n: 0.982349322873309 Loss: 0.0001563130524412399\n",
      "Iteration: 4928 lambda_n: 0.9519462250953594 Loss: 0.00015331319917046403\n",
      "Iteration: 4929 lambda_n: 0.8777435729935391 Loss: 0.00015048959494882508\n",
      "Iteration: 4930 lambda_n: 0.9236978049628434 Loss: 0.00014795804857144467\n",
      "Iteration: 4931 lambda_n: 0.9083155753897755 Loss: 0.00014536202719369446\n",
      "Iteration: 4932 lambda_n: 0.9096831771980635 Loss: 0.00014287739955727314\n",
      "Iteration: 4933 lambda_n: 0.9841037859622422 Loss: 0.00014045416820728394\n",
      "Iteration: 4934 lambda_n: 1.007766994158359 Loss: 0.00013790139000859818\n",
      "Iteration: 4935 lambda_n: 0.8847995952119954 Loss: 0.000135360831100822\n",
      "Iteration: 4936 lambda_n: 0.9753680206846511 Loss: 0.00013319388833932954\n",
      "Iteration: 4937 lambda_n: 0.9377815462743757 Loss: 0.00013086526392026257\n",
      "Iteration: 4938 lambda_n: 0.9984320337083102 Loss: 0.0001286878552173331\n",
      "Iteration: 4939 lambda_n: 0.945829119722903 Loss: 0.00012643080449371453\n",
      "Iteration: 4940 lambda_n: 0.9628692826908326 Loss: 0.000124352173751311\n",
      "Iteration: 4941 lambda_n: 0.9178173225977034 Loss: 0.0001222917686203841\n",
      "Iteration: 4942 lambda_n: 0.9520901483749882 Loss: 0.00012037998606219782\n",
      "Iteration: 4943 lambda_n: 0.9315013162056857 Loss: 0.00011844699161083427\n",
      "Iteration: 4944 lambda_n: 0.95320709054377 Loss: 0.00011660508341416452\n",
      "Iteration: 4945 lambda_n: 0.91576187084564 Loss: 0.00011476815187716557\n",
      "Iteration: 4946 lambda_n: 0.8895323280204075 Loss: 0.00011304893970497837\n",
      "Iteration: 4947 lambda_n: 0.9206286032680412 Loss: 0.00011142019532961824\n",
      "Iteration: 4948 lambda_n: 1.0042449073503734 Loss: 0.00010977482364321354\n",
      "Iteration: 4949 lambda_n: 0.9922796730478933 Loss: 0.00010802429034747772\n",
      "Iteration: 4950 lambda_n: 0.9320006001626224 Loss: 0.00010634075860076654\n",
      "Iteration: 4951 lambda_n: 0.9819505526678797 Loss: 0.00010480088559991018\n",
      "Iteration: 4952 lambda_n: 0.9142474885436758 Loss: 0.00010321829346809493\n",
      "Iteration: 4953 lambda_n: 0.880550657808362 Loss: 0.00010178256411963075\n",
      "Iteration: 4954 lambda_n: 0.9370106021750337 Loss: 0.00010043258739226196\n",
      "Iteration: 4955 lambda_n: 0.9957643950200454 Loss: 9.902883060756889e-05\n",
      "Iteration: 4956 lambda_n: 1.0052544092607283 Loss: 9.757308277401627e-05\n",
      "Iteration: 4957 lambda_n: 0.9345829265042628 Loss: 9.614091423629036e-05\n",
      "Iteration: 4958 lambda_n: 1.0027382374983538 Loss: 9.484341404194751e-05\n",
      "Iteration: 4959 lambda_n: 1.001925632084913 Loss: 9.348425794838021e-05\n",
      "Iteration: 4960 lambda_n: 0.9160037798800388 Loss: 9.216044560414538e-05\n",
      "Iteration: 4961 lambda_n: 0.87455773260585 Loss: 9.098040572894602e-05\n",
      "Iteration: 4962 lambda_n: 0.9817179688526928 Loss: 8.987937588181394e-05\n",
      "Iteration: 4963 lambda_n: 0.9128331184005638 Loss: 8.867022446789631e-05\n",
      "Iteration: 4964 lambda_n: 1.0153856375768653 Loss: 8.757299968200025e-05\n",
      "Iteration: 4965 lambda_n: 0.879833709288198 Loss: 8.637979303328771e-05\n",
      "Iteration: 4966 lambda_n: 1.0025397144021742 Loss: 8.537130363039897e-05\n",
      "Iteration: 4967 lambda_n: 0.9706948625144255 Loss: 8.42466442256705e-05\n",
      "Iteration: 4968 lambda_n: 0.9344727042770886 Loss: 8.318389578212143e-05\n",
      "Iteration: 4969 lambda_n: 0.912973718047546 Loss: 8.218447511866769e-05\n",
      "Iteration: 4970 lambda_n: 0.8889915120371936 Loss: 8.122966909430525e-05\n",
      "Iteration: 4971 lambda_n: 0.9173601163383994 Loss: 8.031993714553574e-05\n",
      "Iteration: 4972 lambda_n: 0.9430073973132694 Loss: 7.940073769477792e-05\n",
      "Iteration: 4973 lambda_n: 0.9800784389892409 Loss: 7.847603865305836e-05\n",
      "Iteration: 4974 lambda_n: 0.9754606286495165 Loss: 7.753598317485565e-05\n",
      "Iteration: 4975 lambda_n: 0.935478121824417 Loss: 7.662145063027157e-05\n",
      "Iteration: 4976 lambda_n: 1.0121657049792352 Loss: 7.57639435664834e-05\n",
      "Iteration: 4977 lambda_n: 0.8746544161052404 Loss: 7.485587471451343e-05\n",
      "Iteration: 4978 lambda_n: 0.9554870842121582 Loss: 7.40890566225315e-05\n",
      "Iteration: 4979 lambda_n: 0.8957207900094427 Loss: 7.326781383083631e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4980 lambda_n: 0.9575943742564361 Loss: 7.251430940078203e-05\n",
      "Iteration: 4981 lambda_n: 0.8971006155524227 Loss: 7.172473684088604e-05\n",
      "Iteration: 4982 lambda_n: 0.9411153042489737 Loss: 7.100060425977624e-05\n",
      "Iteration: 4983 lambda_n: 0.9892090595935865 Loss: 7.025583746404438e-05\n",
      "Iteration: 4984 lambda_n: 0.9653398440768881 Loss: 6.948900902857155e-05\n",
      "Iteration: 4985 lambda_n: 0.9855488048104679 Loss: 6.875663379968204e-05\n",
      "Iteration: 4986 lambda_n: 1.0088106956870475 Loss: 6.802437694217958e-05\n",
      "Iteration: 4987 lambda_n: 0.9088446379894846 Loss: 6.729053798501692e-05\n",
      "Iteration: 4988 lambda_n: 1.0255251861851242 Loss: 6.664347026776111e-05\n",
      "Iteration: 4989 lambda_n: 1.0136082644574713 Loss: 6.592724498689188e-05\n",
      "Iteration: 4990 lambda_n: 0.9456826903943168 Loss: 6.523443685868156e-05\n",
      "Iteration: 4991 lambda_n: 0.9783806054525225 Loss: 6.460156564508137e-05\n",
      "Iteration: 4992 lambda_n: 0.9163757623798398 Loss: 6.395949546721656e-05\n",
      "Iteration: 4993 lambda_n: 0.9732998171438048 Loss: 6.337006881180384e-05\n",
      "Iteration: 4994 lambda_n: 1.0069871339039198 Loss: 6.275560848786576e-05\n",
      "Iteration: 4995 lambda_n: 1.0268907467117214 Loss: 6.21322778088998e-05\n",
      "Iteration: 4996 lambda_n: 1.0155339494645126 Loss: 6.1509350694241e-05\n",
      "Iteration: 4997 lambda_n: 0.8881202374998665 Loss: 6.090578359795259e-05\n",
      "Iteration: 4998 lambda_n: 0.921596330114313 Loss: 6.038841285732943e-05\n",
      "Iteration: 4999 lambda_n: 0.9325808970821028 Loss: 5.986079122346184e-05\n",
      "Iteration: 5000 lambda_n: 1.0204673654083922 Loss: 5.933635561753777e-05\n",
      "Iteration: 5001 lambda_n: 0.9757149183938161 Loss: 5.877273088709625e-05\n",
      "Iteration: 5002 lambda_n: 0.9771681699928648 Loss: 5.8244246670190154e-05\n",
      "Iteration: 5003 lambda_n: 0.9670019964250709 Loss: 5.7724683161261636e-05\n",
      "Iteration: 5004 lambda_n: 0.9245087647649831 Loss: 5.72198913118824e-05\n",
      "Iteration: 5005 lambda_n: 0.9966788778068264 Loss: 5.6745907635359164e-05\n",
      "Iteration: 5006 lambda_n: 0.9729250598775374 Loss: 5.624359047059293e-05\n",
      "Iteration: 5007 lambda_n: 0.8879168663162464 Loss: 5.576213334341862e-05\n",
      "Iteration: 5008 lambda_n: 0.9313290196140629 Loss: 5.533044747750161e-05\n",
      "Iteration: 5009 lambda_n: 0.8848023475720466 Loss: 5.4884846369131686e-05\n",
      "Iteration: 5010 lambda_n: 0.9526049547204697 Loss: 5.44684984125137e-05\n",
      "Iteration: 5011 lambda_n: 0.9248644103960406 Loss: 5.402722681030221e-05\n",
      "Iteration: 5012 lambda_n: 0.9544844242378732 Loss: 5.360592748415566e-05\n",
      "Iteration: 5013 lambda_n: 0.9388001405647756 Loss: 5.3178095481213754e-05\n",
      "Iteration: 5014 lambda_n: 0.9969972706992 Loss: 5.2764185299137374e-05\n",
      "Iteration: 5015 lambda_n: 0.9080250360203006 Loss: 5.233163852990827e-05\n",
      "Iteration: 5016 lambda_n: 0.9110952815183194 Loss: 5.194431271329964e-05\n",
      "Iteration: 5017 lambda_n: 1.0183776276755898 Loss: 5.156157389126893e-05\n",
      "Iteration: 5018 lambda_n: 0.9050632237279714 Loss: 5.1140228848205904e-05\n",
      "Iteration: 5019 lambda_n: 1.0084615914913697 Loss: 5.0772027502441914e-05\n",
      "Iteration: 5020 lambda_n: 0.9308073273652586 Loss: 5.036780578776376e-05\n",
      "Iteration: 5021 lambda_n: 0.9369167054037485 Loss: 5.000077713762215e-05\n",
      "Iteration: 5022 lambda_n: 1.0174331180829694 Loss: 4.963683337237624e-05\n",
      "Iteration: 5023 lambda_n: 0.8881392439981536 Loss: 4.924747834747601e-05\n",
      "Iteration: 5024 lambda_n: 0.8793551729284863 Loss: 4.891302499367546e-05\n",
      "Iteration: 5025 lambda_n: 0.9010150437184851 Loss: 4.858644887218745e-05\n",
      "Iteration: 5026 lambda_n: 1.0034580567482214 Loss: 4.825636154909668e-05\n",
      "Iteration: 5027 lambda_n: 0.9468026105948272 Loss: 4.789380464573743e-05\n",
      "Iteration: 5028 lambda_n: 0.9235270398159734 Loss: 4.7556912422470115e-05\n",
      "Iteration: 5029 lambda_n: 0.9555475602252868 Loss: 4.7232964550542476e-05\n",
      "Iteration: 5030 lambda_n: 0.976609459930194 Loss: 4.690238108666974e-05\n",
      "Iteration: 5031 lambda_n: 0.9639064991560429 Loss: 4.6569260684070034e-05\n",
      "Iteration: 5032 lambda_n: 1.015646523744449 Loss: 4.624515159457116e-05\n",
      "Iteration: 5033 lambda_n: 0.905684233528997 Loss: 4.5908395488615925e-05\n",
      "Iteration: 5034 lambda_n: 0.9003679566225515 Loss: 4.561245585315534e-05\n",
      "Iteration: 5035 lambda_n: 1.0133273623106696 Loss: 4.532202170605232e-05\n",
      "Iteration: 5036 lambda_n: 1.0275206283930183 Loss: 4.4999275712231245e-05\n",
      "Iteration: 5037 lambda_n: 0.941424064178943 Loss: 4.467661412688453e-05\n",
      "Iteration: 5038 lambda_n: 0.9262588405412037 Loss: 4.4385161996974906e-05\n",
      "Iteration: 5039 lambda_n: 0.9019592000537693 Loss: 4.4102075791875325e-05\n",
      "Iteration: 5040 lambda_n: 0.9267328256971001 Loss: 4.382985475240702e-05\n",
      "Iteration: 5041 lambda_n: 1.0063977489412108 Loss: 4.3553522542944205e-05\n",
      "Iteration: 5042 lambda_n: 0.8873744229812711 Loss: 4.325711246495908e-05\n",
      "Iteration: 5043 lambda_n: 0.9824083402672183 Loss: 4.299920026070874e-05\n",
      "Iteration: 5044 lambda_n: 0.9199455732736149 Loss: 4.271695039305345e-05\n",
      "Iteration: 5045 lambda_n: 0.9671755065398169 Loss: 4.245597922421112e-05\n",
      "Iteration: 5046 lambda_n: 0.9423750922509767 Loss: 4.218481769469791e-05\n",
      "Iteration: 5047 lambda_n: 0.8848537036553544 Loss: 4.192382561032069e-05\n",
      "Iteration: 5048 lambda_n: 0.9443213349326549 Loss: 4.1681641649277035e-05\n",
      "Iteration: 5049 lambda_n: 0.9841174113326885 Loss: 4.142600403396482e-05\n",
      "Iteration: 5050 lambda_n: 0.9240160520206139 Loss: 4.1162669600929286e-05\n",
      "Iteration: 5051 lambda_n: 0.933717542846925 Loss: 4.0918363199422235e-05\n",
      "Iteration: 5052 lambda_n: 1.0274554099116269 Loss: 4.06742257337725e-05\n",
      "Iteration: 5053 lambda_n: 0.9012900319830058 Loss: 4.040855692714286e-05\n",
      "Iteration: 5054 lambda_n: 0.947375189979162 Loss: 4.017832495859709e-05\n",
      "Iteration: 5055 lambda_n: 0.915115331215665 Loss: 3.9938857868352374e-05\n",
      "Iteration: 5056 lambda_n: 0.8880584801989894 Loss: 3.971007013525546e-05\n",
      "Iteration: 5057 lambda_n: 0.9533328323046548 Loss: 3.94903651919449e-05\n",
      "Iteration: 5058 lambda_n: 0.9298908407957737 Loss: 3.925687923207012e-05\n",
      "Iteration: 5059 lambda_n: 0.9784916576581416 Loss: 3.9031566272223746e-05\n",
      "Iteration: 5060 lambda_n: 0.8895067786431102 Loss: 3.8796922664174886e-05\n",
      "Iteration: 5061 lambda_n: 0.9423430276897556 Loss: 3.858591027191627e-05\n",
      "Iteration: 5062 lambda_n: 0.9293495015441493 Loss: 3.836452657969143e-05\n",
      "Iteration: 5063 lambda_n: 0.8793640983165064 Loss: 3.8148412680403604e-05\n",
      "Iteration: 5064 lambda_n: 0.8869078989628489 Loss: 3.794595087005908e-05\n",
      "Iteration: 5065 lambda_n: 0.9828697602074606 Loss: 3.774365079737594e-05\n",
      "Iteration: 5066 lambda_n: 1.0102497880929364 Loss: 3.752154611971128e-05\n",
      "Iteration: 5067 lambda_n: 0.916846767305435 Loss: 3.7295584604620295e-05\n",
      "Iteration: 5068 lambda_n: 0.9899547082437543 Loss: 3.7092644545377543e-05\n",
      "Iteration: 5069 lambda_n: 0.9086721467960402 Loss: 3.6875567480201e-05\n",
      "Iteration: 5070 lambda_n: 0.9135473465247926 Loss: 3.667830329692353e-05\n",
      "Iteration: 5071 lambda_n: 0.9816685888741505 Loss: 3.648178044232211e-05\n",
      "Iteration: 5072 lambda_n: 1.0149044464151165 Loss: 3.62725127394437e-05\n",
      "Iteration: 5073 lambda_n: 1.0082057852250748 Loss: 3.605824305866226e-05\n",
      "Iteration: 5074 lambda_n: 1.0016897482463614 Loss: 3.584748593246109e-05\n",
      "Iteration: 5075 lambda_n: 1.0265824139388955 Loss: 3.5640121183586086e-05\n",
      "Iteration: 5076 lambda_n: 0.8809221439057648 Loss: 3.5429630502747826e-05\n",
      "Iteration: 5077 lambda_n: 0.9797772789782359 Loss: 3.5250754514426506e-05\n",
      "Iteration: 5078 lambda_n: 0.9395379462500018 Loss: 3.505344212108649e-05\n",
      "Iteration: 5079 lambda_n: 0.8915095228935045 Loss: 3.4865949768529813e-05\n",
      "Iteration: 5080 lambda_n: 0.9963697100185376 Loss: 3.4689575119480034e-05\n",
      "Iteration: 5081 lambda_n: 0.8948640785657234 Loss: 3.44940530156011e-05\n",
      "Iteration: 5082 lambda_n: 0.9273996921926199 Loss: 3.432002718578859e-05\n",
      "Iteration: 5083 lambda_n: 0.9132935732440964 Loss: 3.4141115726096414e-05\n",
      "Iteration: 5084 lambda_n: 0.9414870344267872 Loss: 3.396637301172428e-05\n",
      "Iteration: 5085 lambda_n: 0.9706986318541903 Loss: 3.378768094358459e-05\n",
      "Iteration: 5086 lambda_n: 0.9006394974532833 Loss: 3.3604955357856016e-05\n",
      "Iteration: 5087 lambda_n: 1.0236310906120503 Loss: 3.343683885436943e-05\n",
      "Iteration: 5088 lambda_n: 0.9869645021932713 Loss: 3.324723750233728e-05\n",
      "Iteration: 5089 lambda_n: 0.9515854257811861 Loss: 3.306601660134497e-05\n",
      "Iteration: 5090 lambda_n: 0.8924457039082063 Loss: 3.289274276192156e-05\n",
      "Iteration: 5091 lambda_n: 0.9154946739587072 Loss: 3.273152745975893e-05\n",
      "Iteration: 5092 lambda_n: 0.8883105780874423 Loss: 3.256736938550022e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5093 lambda_n: 0.9955553573749778 Loss: 3.2409282653872395e-05\n",
      "Iteration: 5094 lambda_n: 0.931507537093834 Loss: 3.223339207748582e-05\n",
      "Iteration: 5095 lambda_n: 0.9280133315812648 Loss: 3.2070141426310105e-05\n",
      "Iteration: 5096 lambda_n: 0.8830278144185246 Loss: 3.190871736571434e-05\n",
      "Iteration: 5097 lambda_n: 0.9626936041496132 Loss: 3.17562518826197e-05\n",
      "Iteration: 5098 lambda_n: 1.0087451264373433 Loss: 3.159118946536208e-05\n",
      "Iteration: 5099 lambda_n: 0.9844505259268737 Loss: 3.141953560447611e-05\n",
      "Iteration: 5100 lambda_n: 0.8811031066958978 Loss: 3.1253329393246756e-05\n",
      "Iteration: 5101 lambda_n: 0.9790117581305506 Loss: 3.110570055278646e-05\n",
      "Iteration: 5102 lambda_n: 1.0259621991385781 Loss: 3.0942772799909944e-05\n",
      "Iteration: 5103 lambda_n: 0.9069430974430206 Loss: 3.0773301425533484e-05\n",
      "Iteration: 5104 lambda_n: 0.9924871526934808 Loss: 3.062464867045011e-05\n",
      "Iteration: 5105 lambda_n: 1.0079314849297982 Loss: 3.0463078306172964e-05\n",
      "Iteration: 5106 lambda_n: 0.8821536995215619 Loss: 3.0300203338332776e-05\n",
      "Iteration: 5107 lambda_n: 0.9692332177675616 Loss: 3.0158712427616925e-05\n",
      "Iteration: 5108 lambda_n: 0.886723743088806 Loss: 3.000425806241098e-05\n",
      "Iteration: 5109 lambda_n: 0.9441137815978398 Loss: 2.986394777531844e-05\n",
      "Iteration: 5110 lambda_n: 0.8959524226007011 Loss: 2.971551261091282e-05\n",
      "Iteration: 5111 lambda_n: 0.8986414732503152 Loss: 2.9575603286401836e-05\n",
      "Iteration: 5112 lambda_n: 0.882726107440587 Loss: 2.9436169751106052e-05\n",
      "Iteration: 5113 lambda_n: 0.9876162390011314 Loss: 2.9300076978230682e-05\n",
      "Iteration: 5114 lambda_n: 0.9818930615468622 Loss: 2.9148758552448935e-05\n",
      "Iteration: 5115 lambda_n: 0.9113696382878649 Loss: 2.8999355948992687e-05\n",
      "Iteration: 5116 lambda_n: 0.9262888985143766 Loss: 2.8861629748105416e-05\n",
      "Iteration: 5117 lambda_n: 0.9374350347766391 Loss: 2.872252922093041e-05\n",
      "Iteration: 5118 lambda_n: 1.0013480144301232 Loss: 2.8582649221676014e-05\n",
      "Iteration: 5119 lambda_n: 0.8857080319928738 Loss: 2.843418727931682e-05\n",
      "Iteration: 5120 lambda_n: 0.9113054188381425 Loss: 2.8303761341928317e-05\n",
      "Iteration: 5121 lambda_n: 0.9430248944383919 Loss: 2.817036628907841e-05\n",
      "Iteration: 5122 lambda_n: 0.9907743304896212 Loss: 2.8033170469545612e-05\n",
      "Iteration: 5123 lambda_n: 0.9874076971163972 Loss: 2.7889932832458156e-05\n",
      "Iteration: 5124 lambda_n: 0.9431578099414628 Loss: 2.774811813174722e-05\n",
      "Iteration: 5125 lambda_n: 0.9709384362861655 Loss: 2.761353883551817e-05\n",
      "Iteration: 5126 lambda_n: 0.9543992835183704 Loss: 2.7475850282361847e-05\n",
      "Iteration: 5127 lambda_n: 1.024315026503272 Loss: 2.7341362015396987e-05\n",
      "Iteration: 5128 lambda_n: 0.9001608283035979 Loss: 2.7197912808818638e-05\n",
      "Iteration: 5129 lambda_n: 0.9577946969463774 Loss: 2.707268144810148e-05\n",
      "Iteration: 5130 lambda_n: 0.9524090112302459 Loss: 2.6940199316578222e-05\n",
      "Iteration: 5131 lambda_n: 1.0098555621187402 Loss: 2.6809265308213855e-05\n",
      "Iteration: 5132 lambda_n: 0.8789230003286388 Loss: 2.6671271042499696e-05\n",
      "Iteration: 5133 lambda_n: 0.9606579992157297 Loss: 2.655193250213747e-05\n",
      "Iteration: 5134 lambda_n: 0.9588879527888845 Loss: 2.64222145785248e-05\n",
      "Iteration: 5135 lambda_n: 0.9287810174621701 Loss: 2.6293511604395255e-05\n",
      "Iteration: 5136 lambda_n: 1.0214725016648172 Loss: 2.6169591655946973e-05\n",
      "Iteration: 5137 lambda_n: 0.9243582158684255 Loss: 2.603408660523892e-05\n",
      "Iteration: 5138 lambda_n: 0.9774571369797218 Loss: 2.5912234644122612e-05\n",
      "Iteration: 5139 lambda_n: 0.8867157000977034 Loss: 2.578411179805128e-05\n",
      "Iteration: 5140 lambda_n: 1.0184518788903125 Loss: 2.5668575210253068e-05\n",
      "Iteration: 5141 lambda_n: 1.0055261614154127 Loss: 2.5536587292045413e-05\n",
      "Iteration: 5142 lambda_n: 0.973756831126837 Loss: 2.540707597348149e-05\n",
      "Iteration: 5143 lambda_n: 0.9127437678356549 Loss: 2.5282414578889105e-05\n",
      "Iteration: 5144 lambda_n: 0.9631999681791751 Loss: 2.5166245004436385e-05\n",
      "Iteration: 5145 lambda_n: 0.9971867601875388 Loss: 2.5044320305800542e-05\n",
      "Iteration: 5146 lambda_n: 0.8804592601111327 Loss: 2.4918815013823998e-05\n",
      "Iteration: 5147 lambda_n: 0.9245211971294524 Loss: 2.4808654080710205e-05\n",
      "Iteration: 5148 lambda_n: 0.950762586119747 Loss: 2.469357967045289e-05\n",
      "Iteration: 5149 lambda_n: 0.9460983239562005 Loss: 2.4575880644818913e-05\n",
      "Iteration: 5150 lambda_n: 1.0020092294851317 Loss: 2.445940963300475e-05\n",
      "Iteration: 5151 lambda_n: 0.9326231574733215 Loss: 2.4336734907404123e-05\n",
      "Iteration: 5152 lambda_n: 0.9181699329494954 Loss: 2.4223218497213786e-05\n",
      "Iteration: 5153 lambda_n: 0.8848870870077572 Loss: 2.411206339455491e-05\n",
      "Iteration: 5154 lambda_n: 0.9297979391748521 Loss: 2.400550377473237e-05\n",
      "Iteration: 5155 lambda_n: 0.980458139140886 Loss: 2.389410432821315e-05\n",
      "Iteration: 5156 lambda_n: 0.9457608859114509 Loss: 2.3777259858028493e-05\n",
      "Iteration: 5157 lambda_n: 0.9295327484609002 Loss: 2.3665180212552984e-05\n",
      "Iteration: 5158 lambda_n: 0.9548468309369763 Loss: 2.3555615463547155e-05\n",
      "Iteration: 5159 lambda_n: 0.9234718242894471 Loss: 2.3443659206485896e-05\n",
      "Iteration: 5160 lambda_n: 0.9597943034521688 Loss: 2.333596516639682e-05\n",
      "Iteration: 5161 lambda_n: 0.8786124946498255 Loss: 2.3224616742976357e-05\n",
      "Iteration: 5162 lambda_n: 0.9772881632460471 Loss: 2.3123235162304077e-05\n",
      "Iteration: 5163 lambda_n: 0.8920974534615449 Loss: 2.3011021568600652e-05\n",
      "Iteration: 5164 lambda_n: 0.9959048619968185 Loss: 2.2909147883892307e-05\n",
      "Iteration: 5165 lambda_n: 0.9903863838174597 Loss: 2.2795983861445098e-05\n",
      "Iteration: 5166 lambda_n: 0.9667555796898666 Loss: 2.268406827177253e-05\n",
      "Iteration: 5167 lambda_n: 0.8741129513115943 Loss: 2.2575421083921092e-05\n",
      "Iteration: 5168 lambda_n: 0.8863831949035138 Loss: 2.247770883222081e-05\n",
      "Iteration: 5169 lambda_n: 0.9655990312469922 Loss: 2.2379101015116214e-05\n",
      "Iteration: 5170 lambda_n: 0.894306979316997 Loss: 2.2272202716410464e-05\n",
      "Iteration: 5171 lambda_n: 0.9152726262775738 Loss: 2.217371982502927e-05\n",
      "Iteration: 5172 lambda_n: 0.8802955182896599 Loss: 2.207341989389615e-05\n",
      "Iteration: 5173 lambda_n: 1.0255834043355587 Loss: 2.1977433450953874e-05\n",
      "Iteration: 5174 lambda_n: 0.9596341127216449 Loss: 2.1866139465858116e-05\n",
      "Iteration: 5175 lambda_n: 0.9237757286997241 Loss: 2.1762580714711855e-05\n",
      "Iteration: 5176 lambda_n: 0.8858726211071986 Loss: 2.1663408517769315e-05\n",
      "Iteration: 5177 lambda_n: 0.9793744850710149 Loss: 2.1568778996615384e-05\n",
      "Iteration: 5178 lambda_n: 0.9271161034694431 Loss: 2.1464660011323308e-05\n",
      "Iteration: 5179 lambda_n: 0.9290669749925095 Loss: 2.1366614806736657e-05\n",
      "Iteration: 5180 lambda_n: 0.9599950221568333 Loss: 2.1268851086362376e-05\n",
      "Iteration: 5181 lambda_n: 0.9313259395443917 Loss: 2.116833440090411e-05\n",
      "Iteration: 5182 lambda_n: 0.922148357507022 Loss: 2.107131873761229e-05\n",
      "Iteration: 5183 lambda_n: 1.0236040996204816 Loss: 2.097573517429597e-05\n",
      "Iteration: 5184 lambda_n: 0.9828688223164733 Loss: 2.0870155022433287e-05\n",
      "Iteration: 5185 lambda_n: 0.9858481816203308 Loss: 2.076932658200196e-05\n",
      "Iteration: 5186 lambda_n: 0.9726558045402359 Loss: 2.0668718278679342e-05\n",
      "Iteration: 5187 lambda_n: 1.005196410441458 Loss: 2.0569972874523403e-05\n",
      "Iteration: 5188 lambda_n: 1.0051762199804921 Loss: 2.046844686376283e-05\n",
      "Iteration: 5189 lambda_n: 0.8988502312792968 Loss: 2.0367459559627833e-05\n",
      "Iteration: 5190 lambda_n: 0.9054097929561974 Loss: 2.0277630981630514e-05\n",
      "Iteration: 5191 lambda_n: 1.0224585634259515 Loss: 2.0187572958095267e-05\n",
      "Iteration: 5192 lambda_n: 1.0105468385164784 Loss: 2.0086354121008193e-05\n",
      "Iteration: 5193 lambda_n: 0.9285572485631506 Loss: 1.998684864089251e-05\n",
      "Iteration: 5194 lambda_n: 0.9988160409027745 Loss: 1.9895898071203783e-05\n",
      "Iteration: 5195 lambda_n: 0.986407231196433 Loss: 1.9798538506235462e-05\n",
      "Iteration: 5196 lambda_n: 0.9931102962453652 Loss: 1.9702887473265356e-05\n",
      "Iteration: 5197 lambda_n: 0.9280911123781046 Loss: 1.960707920422924e-05\n",
      "Iteration: 5198 lambda_n: 0.9741223768790136 Loss: 1.9518004051077343e-05\n",
      "Iteration: 5199 lambda_n: 1.0226892607804128 Loss: 1.9424959666645957e-05\n",
      "Iteration: 5200 lambda_n: 0.9934049977747882 Loss: 1.932776772581653e-05\n",
      "Iteration: 5201 lambda_n: 0.9918146359684845 Loss: 1.923385667658103e-05\n",
      "Iteration: 5202 lambda_n: 0.9772984216676667 Loss: 1.9140575518982087e-05\n",
      "Iteration: 5203 lambda_n: 0.9156466824073675 Loss: 1.9049128317913698e-05\n",
      "Iteration: 5204 lambda_n: 1.0071867707544897 Loss: 1.8963879851488807e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5205 lambda_n: 0.8994879185794267 Loss: 1.8870549055576246e-05\n",
      "Iteration: 5206 lambda_n: 0.8999036392599769 Loss: 1.8787628057338607e-05\n",
      "Iteration: 5207 lambda_n: 1.001745827755361 Loss: 1.870505035830363e-05\n",
      "Iteration: 5208 lambda_n: 0.9583573529122801 Loss: 1.8613549897478514e-05\n",
      "Iteration: 5209 lambda_n: 0.939471276831066 Loss: 1.8526460020510246e-05\n",
      "Iteration: 5210 lambda_n: 0.9186028131509542 Loss: 1.844150336091985e-05\n",
      "Iteration: 5211 lambda_n: 1.0245263899408221 Loss: 1.8358831097123948e-05\n",
      "Iteration: 5212 lambda_n: 0.9932888025765163 Loss: 1.8267056612106936e-05\n",
      "Iteration: 5213 lambda_n: 0.9987519622108835 Loss: 1.8178543331349118e-05\n",
      "Iteration: 5214 lambda_n: 0.9323472620444667 Loss: 1.808999172423713e-05\n",
      "Iteration: 5215 lambda_n: 0.9808369558793247 Loss: 1.8007746113019348e-05\n",
      "Iteration: 5216 lambda_n: 0.9563364961805944 Loss: 1.792163143610516e-05\n",
      "Iteration: 5217 lambda_n: 1.0200950880644712 Loss: 1.783808432963694e-05\n",
      "Iteration: 5218 lambda_n: 0.9970838176105057 Loss: 1.7749397754770322e-05\n",
      "Iteration: 5219 lambda_n: 0.9605760988204433 Loss: 1.766315809999545e-05\n",
      "Iteration: 5220 lambda_n: 0.9959058577078445 Loss: 1.758049377057542e-05\n",
      "Iteration: 5221 lambda_n: 0.9014835743492431 Loss: 1.749520378068664e-05\n",
      "Iteration: 5222 lambda_n: 0.9877229567548181 Loss: 1.741838714046873e-05\n",
      "Iteration: 5223 lambda_n: 0.896470649177386 Loss: 1.7334603438806228e-05\n",
      "Iteration: 5224 lambda_n: 1.0187288205929959 Loss: 1.7258937588866554e-05\n",
      "Iteration: 5225 lambda_n: 1.0201011274021032 Loss: 1.7173339583292075e-05\n",
      "Iteration: 5226 lambda_n: 0.9921096553323141 Loss: 1.7088064254826902e-05\n",
      "Iteration: 5227 lambda_n: 0.9793303921360772 Loss: 1.7005552867579426e-05\n",
      "Iteration: 5228 lambda_n: 0.9166775283180748 Loss: 1.692450892792443e-05\n",
      "Iteration: 5229 lambda_n: 0.9745276099227133 Loss: 1.6849021498353046e-05\n",
      "Iteration: 5230 lambda_n: 0.9191275522672752 Loss: 1.6769137963576972e-05\n",
      "Iteration: 5231 lambda_n: 0.9345376966812111 Loss: 1.669416247912388e-05\n",
      "Iteration: 5232 lambda_n: 0.9410618758767597 Loss: 1.6618279751020562e-05\n",
      "Iteration: 5233 lambda_n: 1.0108988303055289 Loss: 1.6542223534165467e-05\n",
      "Iteration: 5234 lambda_n: 0.989391805012227 Loss: 1.646090644214082e-05\n",
      "Iteration: 5235 lambda_n: 0.9062489424562018 Loss: 1.6381720229815547e-05\n",
      "Iteration: 5236 lambda_n: 0.9424442220189946 Loss: 1.630954566711999e-05\n",
      "Iteration: 5237 lambda_n: 0.9318727710146811 Loss: 1.6234826910935746e-05\n",
      "Iteration: 5238 lambda_n: 0.9957556347871473 Loss: 1.6161292506872985e-05\n",
      "Iteration: 5239 lambda_n: 0.8893447523614337 Loss: 1.6083080960918218e-05\n",
      "Iteration: 5240 lambda_n: 0.9360461280600205 Loss: 1.6013572908485047e-05\n",
      "Iteration: 5241 lambda_n: 0.9072104615926053 Loss: 1.5940737780691154e-05\n",
      "Iteration: 5242 lambda_n: 0.9663491123841187 Loss: 1.5870474195592148e-05\n",
      "Iteration: 5243 lambda_n: 0.8909106700391111 Loss: 1.579596696458019e-05\n",
      "Iteration: 5244 lambda_n: 0.9542774360391475 Loss: 1.5727605114534584e-05\n",
      "Iteration: 5245 lambda_n: 0.8758785411311965 Loss: 1.5654704072696754e-05\n",
      "Iteration: 5246 lambda_n: 1.0169183203093504 Loss: 1.5588108326619688e-05\n",
      "Iteration: 5247 lambda_n: 0.9238385397514939 Loss: 1.5511123968924364e-05\n",
      "Iteration: 5248 lambda_n: 0.9444792942212829 Loss: 1.544153781442062e-05\n",
      "Iteration: 5249 lambda_n: 0.9266155097573425 Loss: 1.5370721800567385e-05\n",
      "Iteration: 5250 lambda_n: 0.8863028760710814 Loss: 1.530156939164583e-05\n",
      "Iteration: 5251 lambda_n: 0.8792044355566668 Loss: 1.5235728142647013e-05\n",
      "Iteration: 5252 lambda_n: 0.92939408882191 Loss: 1.5170699958143018e-05\n",
      "Iteration: 5253 lambda_n: 1.02443264179577 Loss: 1.510225781512035e-05\n",
      "Iteration: 5254 lambda_n: 0.906085868061573 Loss: 1.502716267656906e-05\n",
      "Iteration: 5255 lambda_n: 0.9386329035514046 Loss: 1.4961078288555202e-05\n",
      "Iteration: 5256 lambda_n: 0.9134281116552296 Loss: 1.4892925775673069e-05\n",
      "Iteration: 5257 lambda_n: 0.9561461014622132 Loss: 1.482690997745765e-05\n",
      "Iteration: 5258 lambda_n: 0.8804552830758523 Loss: 1.4758117631856617e-05\n",
      "Iteration: 5259 lambda_n: 1.0060372542733798 Loss: 1.4695069165519168e-05\n",
      "Iteration: 5260 lambda_n: 0.9377804646535699 Loss: 1.4623339977974147e-05\n",
      "Iteration: 5261 lambda_n: 0.9712680142795962 Loss: 1.4556808246502186e-05\n",
      "Iteration: 5262 lambda_n: 0.9168564684368689 Loss: 1.4488218403356951e-05\n",
      "Iteration: 5263 lambda_n: 0.9696325847109669 Loss: 1.442378010567332e-05\n",
      "Iteration: 5264 lambda_n: 0.9071697368872053 Loss: 1.4355939568964661e-05\n",
      "Iteration: 5265 lambda_n: 0.8950833732427291 Loss: 1.429277150613519e-05\n",
      "Iteration: 5266 lambda_n: 0.8901679666100585 Loss: 1.4230722627241526e-05\n",
      "Iteration: 5267 lambda_n: 0.8788272744961703 Loss: 1.4169285578846039e-05\n",
      "Iteration: 5268 lambda_n: 0.9156216116163592 Loss: 1.4108896147254603e-05\n",
      "Iteration: 5269 lambda_n: 0.8881581946512437 Loss: 1.4046249575375856e-05\n",
      "Iteration: 5270 lambda_n: 0.9674113313854261 Loss: 1.3985754879581711e-05\n",
      "Iteration: 5271 lambda_n: 0.8907269324016115 Loss: 1.3920148943825171e-05\n",
      "Iteration: 5272 lambda_n: 0.9520939773840612 Loss: 1.3860029823733548e-05\n",
      "Iteration: 5273 lambda_n: 1.0099690875356766 Loss: 1.3796049204346055e-05\n",
      "Iteration: 5274 lambda_n: 1.0185111944388343 Loss: 1.372849588905462e-05\n",
      "Iteration: 5275 lambda_n: 0.9270158203448443 Loss: 1.3660708134241076e-05\n",
      "Iteration: 5276 lambda_n: 0.9299639441746971 Loss: 1.3599317542795415e-05\n",
      "Iteration: 5277 lambda_n: 0.9431473631369136 Loss: 1.3538011113474912e-05\n",
      "Iteration: 5278 lambda_n: 0.9912813286534952 Loss: 1.3476118487142033e-05\n",
      "Iteration: 5279 lambda_n: 0.9446848647724809 Loss: 1.3411367247995272e-05\n",
      "Iteration: 5280 lambda_n: 1.017465235762404 Loss: 1.3349958860114015e-05\n",
      "Iteration: 5281 lambda_n: 1.0042869471899132 Loss: 1.3284124921241357e-05\n",
      "Iteration: 5282 lambda_n: 0.9844178947762322 Loss: 1.3219466837192921e-05\n",
      "Iteration: 5283 lambda_n: 0.9175665029557322 Loss: 1.3156399004318732e-05\n",
      "Iteration: 5284 lambda_n: 0.955868801568387 Loss: 1.3097896800495886e-05\n",
      "Iteration: 5285 lambda_n: 0.9023330363290882 Loss: 1.3037225657117648e-05\n",
      "Iteration: 5286 lambda_n: 0.9746696598215526 Loss: 1.2980219893672159e-05\n",
      "Iteration: 5287 lambda_n: 0.9398073732674507 Loss: 1.2918915467125843e-05\n",
      "Iteration: 5288 lambda_n: 0.9473966480958579 Loss: 1.2860085036067779e-05\n",
      "Iteration: 5289 lambda_n: 0.9936728021298936 Loss: 1.2801051542162956e-05\n",
      "Iteration: 5290 lambda_n: 0.9618107275084415 Loss: 1.2739420751536683e-05\n",
      "Iteration: 5291 lambda_n: 1.0137594094128293 Loss: 1.268005533526001e-05\n",
      "Iteration: 5292 lambda_n: 0.9005401557738641 Loss: 1.2617777057361734e-05\n",
      "Iteration: 5293 lambda_n: 0.9572841303128596 Loss: 1.2562727681470698e-05\n",
      "Iteration: 5294 lambda_n: 0.976059960671394 Loss: 1.250446652865674e-05\n",
      "Iteration: 5295 lambda_n: 0.8785905061911363 Loss: 1.2445339882696073e-05\n",
      "Iteration: 5296 lambda_n: 0.8908731234853651 Loss: 1.2392370830618646e-05\n",
      "Iteration: 5297 lambda_n: 1.0055117700618144 Loss: 1.233889123922268e-05\n",
      "Iteration: 5298 lambda_n: 1.0164319321160886 Loss: 1.2278791845967016e-05\n",
      "Iteration: 5299 lambda_n: 0.8866956597050829 Loss: 1.2218337359051248e-05\n",
      "Iteration: 5300 lambda_n: 0.949689212041212 Loss: 1.2165860326346301e-05\n",
      "Iteration: 5301 lambda_n: 0.8741114963930805 Loss: 1.210989788195996e-05\n",
      "Iteration: 5302 lambda_n: 0.8996045846917661 Loss: 1.2058627219327932e-05\n",
      "Iteration: 5303 lambda_n: 1.0029322681172372 Loss: 1.2006085836270602e-05\n",
      "Iteration: 5304 lambda_n: 0.8865073684118582 Loss: 1.1947766134004412e-05\n",
      "Iteration: 5305 lambda_n: 0.9138170133591265 Loss: 1.1896468102169666e-05\n",
      "Iteration: 5306 lambda_n: 0.8997622400443012 Loss: 1.1843817932430104e-05\n",
      "Iteration: 5307 lambda_n: 0.9294468733559378 Loss: 1.1792208064646258e-05\n",
      "Iteration: 5308 lambda_n: 0.9397497446825612 Loss: 1.1739128900576574e-05\n",
      "Iteration: 5309 lambda_n: 1.0210218449086712 Loss: 1.168570403093052e-05\n",
      "Iteration: 5310 lambda_n: 0.8740286098550467 Loss: 1.1627924180321002e-05\n",
      "Iteration: 5311 lambda_n: 0.9718677412298998 Loss: 1.1578708340822414e-05\n",
      "Iteration: 5312 lambda_n: 0.9096933006873822 Loss: 1.1524215875006373e-05\n",
      "Iteration: 5313 lambda_n: 0.9201225526546833 Loss: 1.1473450572463537e-05\n",
      "Iteration: 5314 lambda_n: 0.8806757080499859 Loss: 1.1422330380219893e-05\n",
      "Iteration: 5315 lambda_n: 1.0188214599139527 Loss: 1.1373620649688123e-05\n",
      "Iteration: 5316 lambda_n: 0.8894056063057444 Loss: 1.1317511388394613e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5317 lambda_n: 1.020419877521168 Loss: 1.1268771974928751e-05\n",
      "Iteration: 5318 lambda_n: 0.9873873675026961 Loss: 1.1213094697652546e-05\n",
      "Iteration: 5319 lambda_n: 0.9550003927322332 Loss: 1.1159486936388581e-05\n",
      "Iteration: 5320 lambda_n: 0.9525905848021181 Loss: 1.1107886311538544e-05\n",
      "Iteration: 5321 lambda_n: 0.8978119376381047 Loss: 1.1056654716721617e-05\n",
      "Iteration: 5322 lambda_n: 0.8890247160365589 Loss: 1.1008592649420799e-05\n",
      "Iteration: 5323 lambda_n: 0.8853497248199805 Loss: 1.0961208546568805e-05\n",
      "Iteration: 5324 lambda_n: 1.021758056257534 Loss: 1.0914224088766009e-05\n",
      "Iteration: 5325 lambda_n: 0.9468576688653092 Loss: 1.0860233767696492e-05\n",
      "Iteration: 5326 lambda_n: 0.9036363092576188 Loss: 1.081044950010352e-05\n",
      "Iteration: 5327 lambda_n: 0.8901397490308812 Loss: 1.0763156202695095e-05\n",
      "Iteration: 5328 lambda_n: 0.9726097775267353 Loss: 1.0716773682835903e-05\n",
      "Iteration: 5329 lambda_n: 0.8858871109861512 Loss: 1.0666312929660858e-05\n",
      "Iteration: 5330 lambda_n: 0.9519709259481356 Loss: 1.0620568533747988e-05\n",
      "Iteration: 5331 lambda_n: 0.9724891143341837 Loss: 1.0571623183028985e-05\n",
      "Iteration: 5332 lambda_n: 0.9722945595933057 Loss: 1.052185395007422e-05\n",
      "Iteration: 5333 lambda_n: 0.9976432628329239 Loss: 1.0472329554130752e-05\n",
      "Iteration: 5334 lambda_n: 0.8881316674219162 Loss: 1.0421753807537604e-05\n",
      "Iteration: 5335 lambda_n: 0.902375271068858 Loss: 1.0376947769231562e-05\n",
      "Iteration: 5336 lambda_n: 0.9101382260333267 Loss: 1.0331619352309627e-05\n",
      "Iteration: 5337 lambda_n: 0.8870851902748741 Loss: 1.02861011744966e-05\n",
      "Iteration: 5338 lambda_n: 0.9246447751417709 Loss: 1.0241931858245279e-05\n",
      "Iteration: 5339 lambda_n: 0.9935082815620893 Loss: 1.0196090549307654e-05\n",
      "Iteration: 5340 lambda_n: 0.9273388824417231 Loss: 1.0147056140218764e-05\n",
      "Iteration: 5341 lambda_n: 0.9018795909466847 Loss: 1.0101508105915863e-05\n",
      "Iteration: 5342 lambda_n: 0.8807975529975647 Loss: 1.0057409827782937e-05\n",
      "Iteration: 5343 lambda_n: 0.951930358857945 Loss: 1.0014530786953941e-05\n",
      "Iteration: 5344 lambda_n: 0.8738123067501444 Loss: 9.968386840321046e-06\n",
      "Iteration: 5345 lambda_n: 1.0180572297676078 Loss: 9.926225160596817e-06\n",
      "Iteration: 5346 lambda_n: 0.8773947307026666 Loss: 9.877311798822933e-06\n",
      "Iteration: 5347 lambda_n: 0.8793766157546925 Loss: 9.83536480936653e-06\n",
      "Iteration: 5348 lambda_n: 1.0036565464630007 Loss: 9.793501950913491e-06\n",
      "Iteration: 5349 lambda_n: 0.8905800470550242 Loss: 9.745926471232491e-06\n",
      "Iteration: 5350 lambda_n: 0.9308891847465479 Loss: 9.703916511040238e-06\n",
      "Iteration: 5351 lambda_n: 1.0276994753741404 Loss: 9.66019472730981e-06\n",
      "Iteration: 5352 lambda_n: 0.9263632335415884 Loss: 9.612143837992929e-06\n",
      "Iteration: 5353 lambda_n: 0.9736350142564866 Loss: 9.56904681320765e-06\n",
      "Iteration: 5354 lambda_n: 0.9674203658375393 Loss: 9.5239539997803e-06\n",
      "Iteration: 5355 lambda_n: 0.9133964325080899 Loss: 9.479360491182026e-06\n",
      "Iteration: 5356 lambda_n: 0.9846615825212957 Loss: 9.437454681462445e-06\n",
      "Iteration: 5357 lambda_n: 0.9180591384052742 Loss: 9.392479309041e-06\n",
      "Iteration: 5358 lambda_n: 0.9244878742467911 Loss: 9.350746209210236e-06\n",
      "Iteration: 5359 lambda_n: 1.013873383594628 Loss: 9.30890787786097e-06\n",
      "Iteration: 5360 lambda_n: 0.9698737975557185 Loss: 9.263229939956233e-06\n",
      "Iteration: 5361 lambda_n: 0.8997141921242778 Loss: 9.219749025604268e-06\n",
      "Iteration: 5362 lambda_n: 0.9085464363722697 Loss: 9.179603067137023e-06\n",
      "Iteration: 5363 lambda_n: 0.9958134118029536 Loss: 9.139239771176053e-06\n",
      "Iteration: 5364 lambda_n: 1.0153267183894072 Loss: 9.09519431701224e-06\n",
      "Iteration: 5365 lambda_n: 0.9677646229173191 Loss: 9.05050248881282e-06\n",
      "Iteration: 5366 lambda_n: 0.9184845996251981 Loss: 9.008113794147114e-06\n",
      "Iteration: 5367 lambda_n: 0.9929797612898391 Loss: 8.968072249225494e-06\n",
      "Iteration: 5368 lambda_n: 0.9097951686995343 Loss: 8.924975725867949e-06\n",
      "Iteration: 5369 lambda_n: 0.8815471793390348 Loss: 8.885679491725351e-06\n",
      "Iteration: 5370 lambda_n: 0.9110842509146856 Loss: 8.8477711968335e-06\n",
      "Iteration: 5371 lambda_n: 0.9737105967943257 Loss: 8.808760081533477e-06\n",
      "Iteration: 5372 lambda_n: 0.988298960674944 Loss: 8.767251442526388e-06\n",
      "Iteration: 5373 lambda_n: 1.0148310443778594 Loss: 8.725319653678405e-06\n",
      "Iteration: 5374 lambda_n: 0.9532092505781935 Loss: 8.682468307366767e-06\n",
      "Iteration: 5375 lambda_n: 1.0212897933818827 Loss: 8.6424168226603e-06\n",
      "Iteration: 5376 lambda_n: 0.958919363497553 Loss: 8.599702912135395e-06\n",
      "Iteration: 5377 lambda_n: 1.0068048701993586 Loss: 8.559795961746535e-06\n",
      "Iteration: 5378 lambda_n: 0.9693744190359238 Loss: 8.518090803448674e-06\n",
      "Iteration: 5379 lambda_n: 0.9991665973039904 Loss: 8.478131963570433e-06\n",
      "Iteration: 5380 lambda_n: 0.921549749159959 Loss: 8.437138440252377e-06\n",
      "Iteration: 5381 lambda_n: 0.8810287669599235 Loss: 8.399512338062251e-06\n",
      "Iteration: 5382 lambda_n: 0.909704993238172 Loss: 8.363701233006574e-06\n",
      "Iteration: 5383 lambda_n: 0.8815179924943366 Loss: 8.32688231245928e-06\n",
      "Iteration: 5384 lambda_n: 0.9611503255723359 Loss: 8.291361413125934e-06\n",
      "Iteration: 5385 lambda_n: 0.9937693821122368 Loss: 8.25279706676915e-06\n",
      "Iteration: 5386 lambda_n: 0.8804532471683876 Loss: 8.213109547783331e-06\n",
      "Iteration: 5387 lambda_n: 0.9047045019818829 Loss: 8.178116688557006e-06\n",
      "Iteration: 5388 lambda_n: 0.9154330538758105 Loss: 8.142313299711559e-06\n",
      "Iteration: 5389 lambda_n: 0.943807340537071 Loss: 8.106244056057536e-06\n",
      "Iteration: 5390 lambda_n: 0.9443684053891567 Loss: 8.06922168477048e-06\n",
      "Iteration: 5391 lambda_n: 0.972452557615739 Loss: 8.032346614407185e-06\n",
      "Iteration: 5392 lambda_n: 0.9684948696575734 Loss: 7.994548580349919e-06\n",
      "Iteration: 5393 lambda_n: 0.9656843152056702 Loss: 7.957081643042001e-06\n",
      "Iteration: 5394 lambda_n: 0.9911916570583041 Loss: 7.919898634378036e-06\n",
      "Iteration: 5395 lambda_n: 0.9067454227434958 Loss: 7.881911945095935e-06\n",
      "Iteration: 5396 lambda_n: 0.9201595953955557 Loss: 7.847328378778436e-06\n",
      "Iteration: 5397 lambda_n: 0.9493973620700522 Loss: 7.81238727680335e-06\n",
      "Iteration: 5398 lambda_n: 1.0211445195298674 Loss: 7.776496555181899e-06\n",
      "Iteration: 5399 lambda_n: 0.9330938602038883 Loss: 7.738070979603896e-06\n",
      "Iteration: 5400 lambda_n: 0.9142143208853716 Loss: 7.703132343748776e-06\n",
      "Iteration: 5401 lambda_n: 0.9774607735567689 Loss: 7.669055282250112e-06\n",
      "Iteration: 5402 lambda_n: 0.8955230201599226 Loss: 7.632781999064843e-06\n",
      "Iteration: 5403 lambda_n: 0.9506649749532922 Loss: 7.5997066738593965e-06\n",
      "Iteration: 5404 lambda_n: 0.9422602727392634 Loss: 7.564746965292637e-06\n",
      "Iteration: 5405 lambda_n: 0.9079681368397933 Loss: 7.530255813725387e-06\n",
      "Iteration: 5406 lambda_n: 1.0092399193685875 Loss: 7.49717153229736e-06\n",
      "Iteration: 5407 lambda_n: 0.9114261594205734 Loss: 7.460558790300742e-06\n",
      "Iteration: 5408 lambda_n: 1.0045949610891272 Loss: 7.427656042721534e-06\n",
      "Iteration: 5409 lambda_n: 0.8906396313843078 Loss: 7.391549894867875e-06\n",
      "Iteration: 5410 lambda_n: 0.907264694010048 Loss: 7.359695094527026e-06\n",
      "Iteration: 5411 lambda_n: 0.9682853382183917 Loss: 7.3273855892152095e-06\n",
      "Iteration: 5412 lambda_n: 0.933052348429838 Loss: 7.293054467282457e-06\n",
      "Iteration: 5413 lambda_n: 0.9764153717248756 Loss: 7.260127620725161e-06\n",
      "Iteration: 5414 lambda_n: 0.9497933932433335 Loss: 7.225826155946435e-06\n",
      "Iteration: 5415 lambda_n: 0.9003174641542406 Loss: 7.192617632551702e-06\n",
      "Iteration: 5416 lambda_n: 0.9926506280856449 Loss: 7.161283713855205e-06\n",
      "Iteration: 5417 lambda_n: 0.9588520404539895 Loss: 7.12688687129264e-06\n",
      "Iteration: 5418 lambda_n: 0.9856422121691067 Loss: 7.093820854061484e-06\n",
      "Iteration: 5419 lambda_n: 0.9219221744155556 Loss: 7.059988740259453e-06\n",
      "Iteration: 5420 lambda_n: 0.9562570020474682 Loss: 7.028494794039197e-06\n",
      "Iteration: 5421 lambda_n: 0.8833512237264047 Loss: 6.995973709012807e-06\n",
      "Iteration: 5422 lambda_n: 0.945840809966228 Loss: 6.966071112411354e-06\n",
      "Iteration: 5423 lambda_n: 0.9121746614395985 Loss: 6.934190064435687e-06\n",
      "Iteration: 5424 lambda_n: 0.9551069035954783 Loss: 6.9035845512133615e-06\n",
      "Iteration: 5425 lambda_n: 0.9702000251693338 Loss: 6.871680056063004e-06\n",
      "Iteration: 5426 lambda_n: 0.9058641810373621 Loss: 6.839421214894386e-06\n",
      "Iteration: 5427 lambda_n: 0.9695647939326069 Loss: 6.809442963475165e-06\n",
      "Iteration: 5428 lambda_n: 1.026353746673702 Loss: 6.777497319258662e-06\n",
      "Iteration: 5429 lambda_n: 0.9065790896628201 Loss: 6.743839266275848e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5430 lambda_n: 1.0169569754347094 Loss: 6.71425677238264e-06\n",
      "Iteration: 5431 lambda_n: 0.8883672588740038 Loss: 6.68121815946388e-06\n",
      "Iteration: 5432 lambda_n: 0.8883757436020883 Loss: 6.652499191929633e-06\n",
      "Iteration: 5433 lambda_n: 0.9840179155191129 Loss: 6.623903435534112e-06\n",
      "Iteration: 5434 lambda_n: 1.0154896044719244 Loss: 6.592365263698349e-06\n",
      "Iteration: 5435 lambda_n: 0.9835756523892852 Loss: 6.559973420330362e-06\n",
      "Iteration: 5436 lambda_n: 0.9061542931671615 Loss: 6.528753760643455e-06\n",
      "Iteration: 5437 lambda_n: 0.9477813640806847 Loss: 6.500128451607785e-06\n",
      "Iteration: 5438 lambda_n: 0.9709514120253219 Loss: 6.470319456921126e-06\n",
      "Iteration: 5439 lambda_n: 1.0100238698325936 Loss: 6.439921813524437e-06\n",
      "Iteration: 5440 lambda_n: 1.009092322342115 Loss: 6.408449519978571e-06\n",
      "Iteration: 5441 lambda_n: 0.989812942420755 Loss: 6.3771599577953496e-06\n",
      "Iteration: 5442 lambda_n: 0.9345455913362821 Loss: 6.346618094939668e-06\n",
      "Iteration: 5443 lambda_n: 0.9477694534024897 Loss: 6.3179197117177166e-06\n",
      "Iteration: 5444 lambda_n: 0.9339635228726443 Loss: 6.288946882404181e-06\n",
      "Iteration: 5445 lambda_n: 1.027335790707781 Loss: 6.260527052954107e-06\n",
      "Iteration: 5446 lambda_n: 0.960936440767674 Loss: 6.229407275322781e-06\n",
      "Iteration: 5447 lambda_n: 0.8800178460124046 Loss: 6.200443573645966e-06\n",
      "Iteration: 5448 lambda_n: 0.9714298174573502 Loss: 6.174042203591573e-06\n",
      "Iteration: 5449 lambda_n: 0.9537193553408065 Loss: 6.145022508633812e-06\n",
      "Iteration: 5450 lambda_n: 0.9227068950751225 Loss: 6.116665824001775e-06\n",
      "Iteration: 5451 lambda_n: 0.9711134115840445 Loss: 6.089357850436174e-06\n",
      "Iteration: 5452 lambda_n: 0.9319413304828851 Loss: 6.060745601198361e-06\n",
      "Iteration: 5453 lambda_n: 0.9341069794709014 Loss: 6.03341653698545e-06\n",
      "Iteration: 5454 lambda_n: 0.928106824739409 Loss: 6.006147508426795e-06\n",
      "Iteration: 5455 lambda_n: 0.9425245937038483 Loss: 5.9791761193934e-06\n",
      "Iteration: 5456 lambda_n: 0.946995804634233 Loss: 5.9519087644117365e-06\n",
      "Iteration: 5457 lambda_n: 0.9590700120672456 Loss: 5.924637020001354e-06\n",
      "Iteration: 5458 lambda_n: 0.8897227166738522 Loss: 5.897144136950381e-06\n",
      "Iteration: 5459 lambda_n: 1.0049909101406527 Loss: 5.871757552391662e-06\n",
      "Iteration: 5460 lambda_n: 0.9526606025951321 Loss: 5.84320547155999e-06\n",
      "Iteration: 5461 lambda_n: 0.9851501347806514 Loss: 5.816271741504861e-06\n",
      "Iteration: 5462 lambda_n: 0.9453242202127647 Loss: 5.7885478685865925e-06\n",
      "Iteration: 5463 lambda_n: 0.8767123553065647 Loss: 5.762071595781642e-06\n",
      "Iteration: 5464 lambda_n: 0.9106477395460366 Loss: 5.7376293067863245e-06\n",
      "Iteration: 5465 lambda_n: 0.9459228507618571 Loss: 5.712348630281816e-06\n",
      "Iteration: 5466 lambda_n: 0.9472754185803078 Loss: 5.686204397629623e-06\n",
      "Iteration: 5467 lambda_n: 0.9344614026294156 Loss: 5.660142628599978e-06\n",
      "Iteration: 5468 lambda_n: 0.9483148974079761 Loss: 5.634551255772991e-06\n",
      "Iteration: 5469 lambda_n: 0.9840700651701317 Loss: 5.6086979286592325e-06\n",
      "Iteration: 5470 lambda_n: 0.978113765442077 Loss: 5.581992946058835e-06\n",
      "Iteration: 5471 lambda_n: 0.9447072596046524 Loss: 5.5555760024669834e-06\n",
      "Iteration: 5472 lambda_n: 1.0180934041476166 Loss: 5.53018207035672e-06\n",
      "Iteration: 5473 lambda_n: 0.885762082767224 Loss: 5.502940610948196e-06\n",
      "Iteration: 5474 lambda_n: 0.8814960009472702 Loss: 5.479356749282471e-06\n",
      "Iteration: 5475 lambda_n: 0.996346469056974 Loss: 5.455987074603901e-06\n",
      "Iteration: 5476 lambda_n: 0.9927929073218933 Loss: 5.429685230326566e-06\n",
      "Iteration: 5477 lambda_n: 1.0157107674716979 Loss: 5.403603553296264e-06\n",
      "Iteration: 5478 lambda_n: 0.9201646717518843 Loss: 5.377047994527768e-06\n",
      "Iteration: 5479 lambda_n: 0.9391630655802061 Loss: 5.353108714177548e-06\n",
      "Iteration: 5480 lambda_n: 0.9645095887912969 Loss: 5.328783961394583e-06\n",
      "Iteration: 5481 lambda_n: 1.0238935901106256 Loss: 5.303916252633754e-06\n",
      "Iteration: 5482 lambda_n: 1.0012685512311328 Loss: 5.277640671443135e-06\n",
      "Iteration: 5483 lambda_n: 0.9850092466715717 Loss: 5.252073012237787e-06\n",
      "Iteration: 5484 lambda_n: 0.9293662725724366 Loss: 5.2270424056491335e-06\n",
      "Iteration: 5485 lambda_n: 0.8852587082533314 Loss: 5.2035383401849005e-06\n",
      "Iteration: 5486 lambda_n: 0.9505417934007282 Loss: 5.181250458901755e-06\n",
      "Iteration: 5487 lambda_n: 0.9740483220305477 Loss: 5.15742148184465e-06\n",
      "Iteration: 5488 lambda_n: 0.9099582611989792 Loss: 5.133115538219907e-06\n",
      "Iteration: 5489 lambda_n: 0.9096248194043136 Loss: 5.11051589251799e-06\n",
      "Iteration: 5490 lambda_n: 0.8862670294735249 Loss: 5.088024002970478e-06\n",
      "Iteration: 5491 lambda_n: 0.9589281427917276 Loss: 5.0662061292537156e-06\n",
      "Iteration: 5492 lambda_n: 0.9775342580656501 Loss: 5.0427007431652145e-06\n",
      "Iteration: 5493 lambda_n: 0.9955074916837617 Loss: 5.0188504662921485e-06\n",
      "Iteration: 5494 lambda_n: 0.8978683176168851 Loss: 4.994676561652603e-06\n",
      "Iteration: 5495 lambda_n: 0.9916675654749022 Loss: 4.972978656621956e-06\n",
      "Iteration: 5496 lambda_n: 0.9703738239233174 Loss: 4.949118115555488e-06\n",
      "Iteration: 5497 lambda_n: 0.9056772509104004 Loss: 4.925881961219911e-06\n",
      "Iteration: 5498 lambda_n: 0.9810753259486223 Loss: 4.90429683439052e-06\n",
      "Iteration: 5499 lambda_n: 0.9040790903493222 Loss: 4.8810172057666e-06\n",
      "Iteration: 5500 lambda_n: 0.9461339731995826 Loss: 4.8596664380033964e-06\n",
      "Iteration: 5501 lambda_n: 0.9100237404771168 Loss: 4.837420248296128e-06\n",
      "Iteration: 5502 lambda_n: 1.0053238118037089 Loss: 4.816121068611014e-06\n",
      "Iteration: 5503 lambda_n: 0.9754501501471291 Loss: 4.792694994654556e-06\n",
      "Iteration: 5504 lambda_n: 1.0196842857124029 Loss: 4.770075608937015e-06\n",
      "Iteration: 5505 lambda_n: 1.014465927149353 Loss: 4.746542098071017e-06\n",
      "Iteration: 5506 lambda_n: 0.9618506978807848 Loss: 4.7232445441251175e-06\n",
      "Iteration: 5507 lambda_n: 1.0011587465860252 Loss: 4.701263748197354e-06\n",
      "Iteration: 5508 lambda_n: 0.9603132046983477 Loss: 4.678491144562372e-06\n",
      "Iteration: 5509 lambda_n: 0.9799280968690169 Loss: 4.65675344220957e-06\n",
      "Iteration: 5510 lambda_n: 0.9990416622561706 Loss: 4.634674808730983e-06\n",
      "Iteration: 5511 lambda_n: 0.9249373241548816 Loss: 4.612272261018566e-06\n",
      "Iteration: 5512 lambda_n: 0.9672166492918487 Loss: 4.591631695569485e-06\n",
      "Iteration: 5513 lambda_n: 0.9456729148397605 Loss: 4.570144240564193e-06\n",
      "Iteration: 5514 lambda_n: 0.9360616655188106 Loss: 4.549233719990136e-06\n",
      "Iteration: 5515 lambda_n: 1.0225608618549116 Loss: 4.528630432746404e-06\n",
      "Iteration: 5516 lambda_n: 1.0019983136750599 Loss: 4.50622518872627e-06\n",
      "Iteration: 5517 lambda_n: 0.994712104986954 Loss: 4.484379118680868e-06\n",
      "Iteration: 5518 lambda_n: 0.8904684507136237 Loss: 4.462797054521172e-06\n",
      "Iteration: 5519 lambda_n: 0.926245364146874 Loss: 4.443569734881996e-06\n",
      "Iteration: 5520 lambda_n: 1.000155915870468 Loss: 4.423656080918982e-06\n",
      "Iteration: 5521 lambda_n: 0.9395528864023561 Loss: 4.402249771263394e-06\n",
      "Iteration: 5522 lambda_n: 0.9191565962822665 Loss: 4.382237864539567e-06\n",
      "Iteration: 5523 lambda_n: 0.8962954367064587 Loss: 4.36274939001187e-06\n",
      "Iteration: 5524 lambda_n: 0.9861067383798197 Loss: 4.343830150368766e-06\n",
      "Iteration: 5525 lambda_n: 0.9469415317670564 Loss: 4.323105422850854e-06\n",
      "Iteration: 5526 lambda_n: 1.0044156183295376 Loss: 4.303298779385723e-06\n",
      "Iteration: 5527 lambda_n: 1.015133792050641 Loss: 4.2823862442237585e-06\n",
      "Iteration: 5528 lambda_n: 0.9902863909110333 Loss: 4.261353270867099e-06\n",
      "Iteration: 5529 lambda_n: 0.9133338921196833 Loss: 4.240935904369697e-06\n",
      "Iteration: 5530 lambda_n: 0.96980440934415 Loss: 4.222195347751152e-06\n",
      "Iteration: 5531 lambda_n: 0.9476149866275457 Loss: 4.202384022719803e-06\n",
      "Iteration: 5532 lambda_n: 0.9450923790649711 Loss: 4.183116825754428e-06\n",
      "Iteration: 5533 lambda_n: 0.9202960588752784 Loss: 4.163989027915157e-06\n",
      "Iteration: 5534 lambda_n: 0.9739858133630684 Loss: 4.1454482609091834e-06\n",
      "Iteration: 5535 lambda_n: 0.9042654946194499 Loss: 4.125913210702196e-06\n",
      "Iteration: 5536 lambda_n: 0.9824612040099053 Loss: 4.107862002108288e-06\n",
      "Iteration: 5537 lambda_n: 0.9755692591946381 Loss: 4.0883356399200655e-06\n",
      "Iteration: 5538 lambda_n: 0.9602267381531804 Loss: 4.069038427821348e-06\n",
      "Iteration: 5539 lambda_n: 0.9695365123365695 Loss: 4.050134356641437e-06\n",
      "Iteration: 5540 lambda_n: 0.957561429003747 Loss: 4.0311356867278074e-06\n",
      "Iteration: 5541 lambda_n: 0.9402174662988741 Loss: 4.012459702526065e-06\n",
      "Iteration: 5542 lambda_n: 0.9949195802361088 Loss: 3.994206953734896e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5543 lambda_n: 0.9772040250683777 Loss: 3.97498012470521e-06\n",
      "Iteration: 5544 lambda_n: 0.9786976985770343 Loss: 3.956186559807536e-06\n",
      "Iteration: 5545 lambda_n: 0.9221134137391639 Loss: 3.937453266664486e-06\n",
      "Iteration: 5546 lambda_n: 0.9100998800783622 Loss: 3.919886639250405e-06\n",
      "Iteration: 5547 lambda_n: 0.8961491137431891 Loss: 3.902626231445681e-06\n",
      "Iteration: 5548 lambda_n: 1.0141330942495774 Loss: 3.885705248894597e-06\n",
      "Iteration: 5549 lambda_n: 0.9404457289984542 Loss: 3.866639538057373e-06\n",
      "Iteration: 5550 lambda_n: 1.0009325481249385 Loss: 3.84904590807763e-06\n",
      "Iteration: 5551 lambda_n: 0.9846896311619182 Loss: 3.830405913672353e-06\n",
      "Iteration: 5552 lambda_n: 0.899827268604962 Loss: 3.8121572159943367e-06\n",
      "Iteration: 5553 lambda_n: 0.9776456145749818 Loss: 3.795560677944685e-06\n",
      "Iteration: 5554 lambda_n: 0.9476942259752131 Loss: 3.7776073566883486e-06\n",
      "Iteration: 5555 lambda_n: 0.8938267320813361 Loss: 3.760286383110016e-06\n",
      "Iteration: 5556 lambda_n: 0.8812155408492541 Loss: 3.7440248548853534e-06\n",
      "Iteration: 5557 lambda_n: 1.0045594166187233 Loss: 3.728062100831882e-06\n",
      "Iteration: 5558 lambda_n: 0.9142188390068183 Loss: 3.7099426272332e-06\n",
      "Iteration: 5559 lambda_n: 0.9249815487079088 Loss: 3.6935328000630017e-06\n",
      "Iteration: 5560 lambda_n: 0.9686012240094849 Loss: 3.6770032310790825e-06\n",
      "Iteration: 5561 lambda_n: 0.8965252636223409 Loss: 3.6597716401620333e-06\n",
      "Iteration: 5562 lambda_n: 0.9838892747905097 Loss: 3.6438970429773007e-06\n",
      "Iteration: 5563 lambda_n: 1.0146189773819798 Loss: 3.6265510816362298e-06\n",
      "Iteration: 5564 lambda_n: 0.8871866319711842 Loss: 3.608748512894528e-06\n",
      "Iteration: 5565 lambda_n: 1.0007010939063252 Loss: 3.593258301843877e-06\n",
      "Iteration: 5566 lambda_n: 0.9687284640384781 Loss: 3.575861140272455e-06\n",
      "Iteration: 5567 lambda_n: 1.0252463806172323 Loss: 3.559101367364543e-06\n",
      "Iteration: 5568 lambda_n: 0.9265806375323062 Loss: 3.5414469305150065e-06\n",
      "Iteration: 5569 lambda_n: 1.0198675148953948 Loss: 3.5255706391953877e-06\n",
      "Iteration: 5570 lambda_n: 0.9939374398527002 Loss: 3.5081742894436957e-06\n",
      "Iteration: 5571 lambda_n: 1.0029160311363976 Loss: 3.4913039042937376e-06\n",
      "Iteration: 5572 lambda_n: 0.9719066620457903 Loss: 3.4743629897140386e-06\n",
      "Iteration: 5573 lambda_n: 0.9899516984471035 Loss: 3.45802554196264e-06\n",
      "Iteration: 5574 lambda_n: 1.0107285836033046 Loss: 3.4414630185139003e-06\n",
      "Iteration: 5575 lambda_n: 0.9631169998907496 Loss: 3.4246338833585533e-06\n",
      "Iteration: 5576 lambda_n: 0.9345866834637396 Loss: 3.40867593053366e-06\n",
      "Iteration: 5577 lambda_n: 0.9617718269037546 Loss: 3.393262861459255e-06\n",
      "Iteration: 5578 lambda_n: 1.0111213595132844 Loss: 3.3774731852457373e-06\n",
      "Iteration: 5579 lambda_n: 0.8797099723451816 Loss: 3.360950573172635e-06\n",
      "Iteration: 5580 lambda_n: 0.9798353408322978 Loss: 3.3466456678666613e-06\n",
      "Iteration: 5581 lambda_n: 0.8986182359914153 Loss: 3.3307804502011745e-06\n",
      "Iteration: 5582 lambda_n: 0.9878091307384107 Loss: 3.3162992593890226e-06\n",
      "Iteration: 5583 lambda_n: 0.8875272544916273 Loss: 3.3004499755369832e-06\n",
      "Iteration: 5584 lambda_n: 0.8982088596830179 Loss: 3.286277765391329e-06\n",
      "Iteration: 5585 lambda_n: 0.8952929170661634 Loss: 3.2719965823117952e-06\n",
      "Iteration: 5586 lambda_n: 0.8940712461659193 Loss: 3.257823626761847e-06\n",
      "Iteration: 5587 lambda_n: 1.0160327688817548 Loss: 3.243731323424027e-06\n",
      "Iteration: 5588 lambda_n: 0.984605738694205 Loss: 3.227785948796867e-06\n",
      "Iteration: 5589 lambda_n: 1.0090910621875264 Loss: 3.2124097474397247e-06\n",
      "Iteration: 5590 lambda_n: 0.8942890512871265 Loss: 3.196726243461933e-06\n",
      "Iteration: 5591 lambda_n: 1.021559805081805 Loss: 3.1828948799730285e-06\n",
      "Iteration: 5592 lambda_n: 0.8780007888117624 Loss: 3.167163472630987e-06\n",
      "Iteration: 5593 lambda_n: 0.9412583357021747 Loss: 3.1537096189991058e-06\n",
      "Iteration: 5594 lambda_n: 0.9760634277275263 Loss: 3.139347725481869e-06\n",
      "Iteration: 5595 lambda_n: 0.9617929852806678 Loss: 3.1245225969287895e-06\n",
      "Iteration: 5596 lambda_n: 0.9434578302678349 Loss: 3.109983209319476e-06\n",
      "Iteration: 5597 lambda_n: 0.9812426479528094 Loss: 3.0957873654599756e-06\n",
      "Iteration: 5598 lambda_n: 0.9286033336169861 Loss: 3.081090387108397e-06\n",
      "Iteration: 5599 lambda_n: 0.9542965274696826 Loss: 3.0672478714089985e-06\n",
      "Iteration: 5600 lambda_n: 0.8866140188206796 Loss: 3.053086268742263e-06\n",
      "Iteration: 5601 lambda_n: 0.960371691703197 Loss: 3.0399898155961313e-06\n",
      "Iteration: 5602 lambda_n: 0.9453770821911535 Loss: 3.02586472149645e-06\n",
      "Iteration: 5603 lambda_n: 0.9261264685172054 Loss: 3.0120247791046153e-06\n",
      "Iteration: 5604 lambda_n: 0.9479332954698281 Loss: 2.9985286763180092e-06\n",
      "Iteration: 5605 lambda_n: 0.9581165284248944 Loss: 2.984776692158221e-06\n",
      "Iteration: 5606 lambda_n: 0.8807381643671881 Loss: 2.9709407292146378e-06\n",
      "Iteration: 5607 lambda_n: 1.01282189251034 Loss: 2.95828113312426e-06\n",
      "Iteration: 5608 lambda_n: 0.978105572473043 Loss: 2.9437850251710535e-06\n",
      "Iteration: 5609 lambda_n: 0.9828926828159491 Loss: 2.929854402241971e-06\n",
      "Iteration: 5610 lambda_n: 1.0093380983330678 Loss: 2.9159218500171975e-06\n",
      "Iteration: 5611 lambda_n: 0.8817670774341891 Loss: 2.9016824755626062e-06\n",
      "Iteration: 5612 lambda_n: 0.9059644262212737 Loss: 2.8893035785126037e-06\n",
      "Iteration: 5613 lambda_n: 1.0208968406985237 Loss: 2.876639244777351e-06\n",
      "Iteration: 5614 lambda_n: 0.9476199755881077 Loss: 2.862430846421619e-06\n",
      "Iteration: 5615 lambda_n: 0.9255168936961842 Loss: 2.849307430576589e-06\n",
      "Iteration: 5616 lambda_n: 0.996806237604252 Loss: 2.8365488849806805e-06\n",
      "Iteration: 5617 lambda_n: 0.9897845985785615 Loss: 2.8228691286697247e-06\n",
      "Iteration: 5618 lambda_n: 1.0026888783579384 Loss: 2.80935124839596e-06\n",
      "Iteration: 5619 lambda_n: 0.9369459544989202 Loss: 2.795722712005711e-06\n",
      "Iteration: 5620 lambda_n: 1.0226980981281728 Loss: 2.7830495370774153e-06\n",
      "Iteration: 5621 lambda_n: 0.9294522458666721 Loss: 2.7692791861679556e-06\n",
      "Iteration: 5622 lambda_n: 0.9901724602472101 Loss: 2.7568262931773852e-06\n",
      "Iteration: 5623 lambda_n: 0.912177007712154 Loss: 2.74361952647843e-06\n",
      "Iteration: 5624 lambda_n: 0.9980332694390934 Loss: 2.7315113405143265e-06\n",
      "Iteration: 5625 lambda_n: 0.9729390084998543 Loss: 2.71832197412957e-06\n",
      "Iteration: 5626 lambda_n: 0.886702576228081 Loss: 2.705526327733495e-06\n",
      "Iteration: 5627 lambda_n: 0.9465142487098943 Loss: 2.6939197210094605e-06\n",
      "Iteration: 5628 lambda_n: 0.9440174347080451 Loss: 2.681583357082361e-06\n",
      "Iteration: 5629 lambda_n: 0.9759860452742631 Loss: 2.6693358836985267e-06\n",
      "Iteration: 5630 lambda_n: 0.9244821492214537 Loss: 2.656731493477226e-06\n",
      "Iteration: 5631 lambda_n: 0.9951608804556197 Loss: 2.6448486325479036e-06\n",
      "Iteration: 5632 lambda_n: 0.8914271291635144 Loss: 2.632114517767268e-06\n",
      "Iteration: 5633 lambda_n: 0.9520850683088762 Loss: 2.6207627086997074e-06\n",
      "Iteration: 5634 lambda_n: 0.9726481117122896 Loss: 2.6086907502216687e-06\n",
      "Iteration: 5635 lambda_n: 0.9533746464558981 Loss: 2.5964148757493795e-06\n",
      "Iteration: 5636 lambda_n: 0.9691692913540872 Loss: 2.5844388812015956e-06\n",
      "Iteration: 5637 lambda_n: 1.0077885384198342 Loss: 2.5723206390416985e-06\n",
      "Iteration: 5638 lambda_n: 0.9104269709664069 Loss: 2.5597786030743983e-06\n",
      "Iteration: 5639 lambda_n: 0.9522128523999792 Loss: 2.5485034915871796e-06\n",
      "Iteration: 5640 lambda_n: 0.8872098753348221 Loss: 2.536762834152843e-06\n",
      "Iteration: 5641 lambda_n: 1.0131888467678658 Loss: 2.5258740550291284e-06\n",
      "Iteration: 5642 lambda_n: 0.9949698781764371 Loss: 2.5134925091959174e-06\n",
      "Iteration: 5643 lambda_n: 0.8910180066890774 Loss: 2.5013932131898672e-06\n",
      "Iteration: 5644 lambda_n: 0.9664424079101037 Loss: 2.4906101830292723e-06\n",
      "Iteration: 5645 lambda_n: 1.013515519444088 Loss: 2.4789647959346075e-06\n",
      "Iteration: 5646 lambda_n: 0.9109775360599845 Loss: 2.4668092979155926e-06\n",
      "Iteration: 5647 lambda_n: 1.0223081914940502 Loss: 2.4559371580290485e-06\n",
      "Iteration: 5648 lambda_n: 0.8842876954727008 Loss: 2.4437901116366524e-06\n",
      "Iteration: 5649 lambda_n: 1.0088908705140178 Loss: 2.433334995202221e-06\n",
      "Iteration: 5650 lambda_n: 0.9575647482197525 Loss: 2.4214577068663357e-06\n",
      "Iteration: 5651 lambda_n: 0.968996348879013 Loss: 2.410239691440182e-06\n",
      "Iteration: 5652 lambda_n: 0.939744348916663 Loss: 2.3989403490119535e-06\n",
      "Iteration: 5653 lambda_n: 0.876941851650393 Loss: 2.38803348838379e-06\n",
      "Iteration: 5654 lambda_n: 0.9325660167932595 Loss: 2.3779018051604588e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5655 lambda_n: 0.9362908514907377 Loss: 2.3671731889482726e-06\n",
      "Iteration: 5656 lambda_n: 0.9914773943558731 Loss: 2.356450324352581e-06\n",
      "Iteration: 5657 lambda_n: 1.0234524505373412 Loss: 2.345146877037074e-06\n",
      "Iteration: 5658 lambda_n: 0.9271936470697248 Loss: 2.333534869373484e-06\n",
      "Iteration: 5659 lambda_n: 0.9572865633219458 Loss: 2.3230671006768796e-06\n",
      "Iteration: 5660 lambda_n: 0.9160045898064992 Loss: 2.312308076286838e-06\n",
      "Iteration: 5661 lambda_n: 1.0116021613852255 Loss: 2.3020607087817353e-06\n",
      "Iteration: 5662 lambda_n: 0.9340856381139341 Loss: 2.2907940462404815e-06\n",
      "Iteration: 5663 lambda_n: 0.9604856569923693 Loss: 2.280441640514866e-06\n",
      "Iteration: 5664 lambda_n: 0.8963927084922535 Loss: 2.2698447564399e-06\n",
      "Iteration: 5665 lambda_n: 0.9402058404371855 Loss: 2.2600009606644555e-06\n",
      "Iteration: 5666 lambda_n: 0.9755279899540855 Loss: 2.2497208096748195e-06\n",
      "Iteration: 5667 lambda_n: 0.8745628331022992 Loss: 2.239102972174165e-06\n",
      "Iteration: 5668 lambda_n: 0.9049574413238481 Loss: 2.2296289896520136e-06\n",
      "Iteration: 5669 lambda_n: 0.9301853331049325 Loss: 2.219867231207375e-06\n",
      "Iteration: 5670 lambda_n: 0.9837119265754386 Loss: 2.2098772750748867e-06\n",
      "Iteration: 5671 lambda_n: 0.9028861121223669 Loss: 2.199360006254624e-06\n",
      "Iteration: 5672 lambda_n: 0.9192872239286071 Loss: 2.1897528256857097e-06\n",
      "Iteration: 5673 lambda_n: 0.885347970797 Loss: 2.1800138614809365e-06\n",
      "Iteration: 5674 lambda_n: 1.0065290657234183 Loss: 2.1706761706918997e-06\n",
      "Iteration: 5675 lambda_n: 0.9825397719786785 Loss: 2.1601058687940917e-06\n",
      "Iteration: 5676 lambda_n: 1.0136494220721777 Loss: 2.149837747887523e-06\n",
      "Iteration: 5677 lambda_n: 0.8892186749213056 Loss: 2.139294873569411e-06\n",
      "Iteration: 5678 lambda_n: 0.9311517768998236 Loss: 2.1300915528664427e-06\n",
      "Iteration: 5679 lambda_n: 0.9524049380518305 Loss: 2.120495693675997e-06\n",
      "Iteration: 5680 lambda_n: 0.8754768921550724 Loss: 2.11072503296538e-06\n",
      "Iteration: 5681 lambda_n: 0.9436527819228034 Loss: 2.1017849608641307e-06\n",
      "Iteration: 5682 lambda_n: 0.9326410525701034 Loss: 2.092189519236335e-06\n",
      "Iteration: 5683 lambda_n: 0.8921700291673392 Loss: 2.082749349859534e-06\n",
      "Iteration: 5684 lambda_n: 0.9111354557495874 Loss: 2.0737595784539555e-06\n",
      "Iteration: 5685 lambda_n: 0.8944990643100988 Loss: 2.064618337709634e-06\n",
      "Iteration: 5686 lambda_n: 0.929710169064184 Loss: 2.0556835704644696e-06\n",
      "Iteration: 5687 lambda_n: 0.9306600483009846 Loss: 2.046437287084356e-06\n",
      "Iteration: 5688 lambda_n: 1.0055834013898164 Loss: 2.0372231931237192e-06\n",
      "Iteration: 5689 lambda_n: 0.916089009093699 Loss: 2.0273121445830023e-06\n",
      "Iteration: 5690 lambda_n: 0.977098909082485 Loss: 2.018327085439987e-06\n",
      "Iteration: 5691 lambda_n: 0.9196268626747833 Loss: 2.008786116245523e-06\n",
      "Iteration: 5692 lambda_n: 0.9390480226059633 Loss: 1.9998487918850333e-06\n",
      "Iteration: 5693 lambda_n: 1.0125009545933392 Loss: 1.9907633322376457e-06\n",
      "Iteration: 5694 lambda_n: 0.9586090790142912 Loss: 1.9810117120573643e-06\n",
      "Iteration: 5695 lambda_n: 1.0108992996955266 Loss: 1.9718243670158165e-06\n",
      "Iteration: 5696 lambda_n: 0.8929163050325207 Loss: 1.962180808373545e-06\n",
      "Iteration: 5697 lambda_n: 0.9197606327301422 Loss: 1.9537044224978595e-06\n",
      "Iteration: 5698 lambda_n: 0.8835523580420117 Loss: 1.9450109278939212e-06\n",
      "Iteration: 5699 lambda_n: 1.0102247844825607 Loss: 1.936696836194539e-06\n",
      "Iteration: 5700 lambda_n: 0.9832248148320912 Loss: 1.927231415829393e-06\n",
      "Iteration: 5701 lambda_n: 0.8854878651229736 Loss: 1.9180640053599302e-06\n",
      "Iteration: 5702 lambda_n: 1.02266470283147 Loss: 1.9098471539926415e-06\n",
      "Iteration: 5703 lambda_n: 0.9194840078449147 Loss: 1.9003980339540973e-06\n",
      "Iteration: 5704 lambda_n: 1.0158218234883971 Loss: 1.8919443118110827e-06\n",
      "Iteration: 5705 lambda_n: 1.0028773432412645 Loss: 1.8826464120560156e-06\n",
      "Iteration: 5706 lambda_n: 1.0230160540910878 Loss: 1.873512111865951e-06\n",
      "Iteration: 5707 lambda_n: 0.946603259857117 Loss: 1.864239600303002e-06\n",
      "Iteration: 5708 lambda_n: 0.9333675356647004 Loss: 1.8557021561158983e-06\n",
      "Iteration: 5709 lambda_n: 1.0137193617252989 Loss: 1.847322641572884e-06\n",
      "Iteration: 5710 lambda_n: 0.951166981271536 Loss: 1.8382628514919227e-06\n",
      "Iteration: 5711 lambda_n: 0.9880277377587118 Loss: 1.82980379862121e-06\n",
      "Iteration: 5712 lambda_n: 0.9304724071193987 Loss: 1.8210573698907234e-06\n",
      "Iteration: 5713 lambda_n: 0.9574704285450469 Loss: 1.8128598221760041e-06\n",
      "Iteration: 5714 lambda_n: 1.0130466907289482 Loss: 1.8044623966648141e-06\n",
      "Iteration: 5715 lambda_n: 0.9370427178184756 Loss: 1.7956187048978855e-06\n",
      "Iteration: 5716 lambda_n: 0.8742164450947675 Loss: 1.7874786089430953e-06\n",
      "Iteration: 5717 lambda_n: 0.9142330598530342 Loss: 1.7799187172366931e-06\n",
      "Iteration: 5718 lambda_n: 0.8994204216061917 Loss: 1.772046218637854e-06\n",
      "Iteration: 5719 lambda_n: 0.9571967590307546 Loss: 1.7643355324406656e-06\n",
      "Iteration: 5720 lambda_n: 0.9523037821699007 Loss: 1.7561652441276535e-06\n",
      "Iteration: 5721 lambda_n: 0.8762681935465629 Loss: 1.7480743672224841e-06\n",
      "Iteration: 5722 lambda_n: 0.8753151942874224 Loss: 1.7406638011373023e-06\n",
      "Iteration: 5723 lambda_n: 0.9128690269756078 Loss: 1.7332926801202627e-06\n",
      "Iteration: 5724 lambda_n: 1.0082822727754606 Loss: 1.725637872231608e-06\n",
      "Iteration: 5725 lambda_n: 0.8962665096312514 Loss: 1.717220327227481e-06\n",
      "Iteration: 5726 lambda_n: 1.007114688966308 Loss: 1.7097744384335114e-06\n",
      "Iteration: 5727 lambda_n: 0.8973266507226898 Loss: 1.7014439428121408e-06\n",
      "Iteration: 5728 lambda_n: 0.915611536090318 Loss: 1.694057743845544e-06\n",
      "Iteration: 5729 lambda_n: 0.8832655462846513 Loss: 1.6865537583097056e-06\n",
      "Iteration: 5730 lambda_n: 1.0126871152645438 Loss: 1.6793469374472536e-06\n",
      "Iteration: 5731 lambda_n: 0.9098154046364871 Loss: 1.6711194412368883e-06\n",
      "Iteration: 5732 lambda_n: 1.0138821263806557 Loss: 1.663763936980855e-06\n",
      "Iteration: 5733 lambda_n: 0.975463399945952 Loss: 1.655603177669932e-06\n",
      "Iteration: 5734 lambda_n: 0.8862528081979392 Loss: 1.6477901688373753e-06\n",
      "Iteration: 5735 lambda_n: 0.9178833248383207 Loss: 1.640725198911172e-06\n",
      "Iteration: 5736 lambda_n: 0.929575859962171 Loss: 1.6334394560429938e-06\n",
      "Iteration: 5737 lambda_n: 0.9660746088224396 Loss: 1.626093672877038e-06\n",
      "Iteration: 5738 lambda_n: 0.9917277692853644 Loss: 1.6184938028861017e-06\n",
      "Iteration: 5739 lambda_n: 1.027141983699494 Loss: 1.6107285939248697e-06\n",
      "Iteration: 5740 lambda_n: 0.8839809653334687 Loss: 1.6027246844342013e-06\n",
      "Iteration: 5741 lambda_n: 1.0269679179925568 Loss: 1.5958705781746775e-06\n",
      "Iteration: 5742 lambda_n: 0.8770364949032281 Loss: 1.587941854971418e-06\n",
      "Iteration: 5743 lambda_n: 0.984032267954153 Loss: 1.5812043260049914e-06\n",
      "Iteration: 5744 lambda_n: 0.8788971218755348 Loss: 1.5736769183799629e-06\n",
      "Iteration: 5745 lambda_n: 0.9647443484602052 Loss: 1.5669857585891682e-06\n",
      "Iteration: 5746 lambda_n: 0.9056604792404658 Loss: 1.5596722665906217e-06\n",
      "Iteration: 5747 lambda_n: 0.9101237831941537 Loss: 1.5528387233202525e-06\n",
      "Iteration: 5748 lambda_n: 0.9588601379432209 Loss: 1.546001595449035e-06\n",
      "Iteration: 5749 lambda_n: 0.9776557236584562 Loss: 1.5388300659521803e-06\n",
      "Iteration: 5750 lambda_n: 1.0238599109704454 Loss: 1.5315518844923616e-06\n",
      "Iteration: 5751 lambda_n: 1.0147000398253885 Loss: 1.5239657909148868e-06\n",
      "Iteration: 5752 lambda_n: 0.8998995545794067 Loss: 1.516484810876776e-06\n",
      "Iteration: 5753 lambda_n: 0.9506958851336575 Loss: 1.5098827828851583e-06\n",
      "Iteration: 5754 lambda_n: 0.9538578235507358 Loss: 1.5029384616450797e-06\n",
      "Iteration: 5755 lambda_n: 0.903127362127804 Loss: 1.496003094180622e-06\n",
      "Iteration: 5756 lambda_n: 0.9797249074604906 Loss: 1.4894668871324102e-06\n",
      "Iteration: 5757 lambda_n: 1.0034453269641948 Loss: 1.4824073049363388e-06\n",
      "Iteration: 5758 lambda_n: 0.9889259395786395 Loss: 1.475211076837408e-06\n",
      "Iteration: 5759 lambda_n: 0.8993454408873207 Loss: 1.468153408501822e-06\n",
      "Iteration: 5760 lambda_n: 1.0149142241458187 Loss: 1.4617657609716904e-06\n",
      "Iteration: 5761 lambda_n: 0.9994131011780865 Loss: 1.4545886480593774e-06\n",
      "Iteration: 5762 lambda_n: 1.010757723426863 Loss: 1.4475558600264769e-06\n",
      "Iteration: 5763 lambda_n: 1.0146914205922146 Loss: 1.440477635350227e-06\n",
      "Iteration: 5764 lambda_n: 1.0073344547673864 Loss: 1.4334066149097324e-06\n",
      "Iteration: 5765 lambda_n: 0.9868285233268467 Loss: 1.4264213269334514e-06\n",
      "Iteration: 5766 lambda_n: 0.9367716872439875 Loss: 1.4196115893289511e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5767 lambda_n: 1.0204485603612399 Loss: 1.4131781412899666e-06\n",
      "Iteration: 5768 lambda_n: 0.9698647228084208 Loss: 1.4062017923888842e-06\n",
      "Iteration: 5769 lambda_n: 0.9572206897493304 Loss: 1.3996040006633664e-06\n",
      "Iteration: 5770 lambda_n: 0.9839330224254785 Loss: 1.393122781770604e-06\n",
      "Iteration: 5771 lambda_n: 0.907347012694647 Loss: 1.3864915529271723e-06\n",
      "Iteration: 5772 lambda_n: 0.9391869148642799 Loss: 1.3804055891468581e-06\n",
      "Iteration: 5773 lambda_n: 0.9103809928234535 Loss: 1.3741337180263773e-06\n",
      "Iteration: 5774 lambda_n: 0.9921035132014595 Loss: 1.3680818393839778e-06\n",
      "Iteration: 5775 lambda_n: 0.8811248974031783 Loss: 1.3615157505282421e-06\n",
      "Iteration: 5776 lambda_n: 0.8911354823193637 Loss: 1.3557121506843296e-06\n",
      "Iteration: 5777 lambda_n: 1.0036793146573075 Loss: 1.3498676393216472e-06\n",
      "Iteration: 5778 lambda_n: 0.9160215982595588 Loss: 1.3433133923642845e-06\n",
      "Iteration: 5779 lambda_n: 0.9944172935191975 Loss: 1.3373606194228114e-06\n",
      "Iteration: 5780 lambda_n: 0.9252818434631543 Loss: 1.3309270335195543e-06\n",
      "Iteration: 5781 lambda_n: 1.0033567032787378 Loss: 1.3249695368495781e-06\n",
      "Iteration: 5782 lambda_n: 1.024306795010958 Loss: 1.3185382717641052e-06\n",
      "Iteration: 5783 lambda_n: 0.9224425183099693 Loss: 1.3120045962379689e-06\n",
      "Iteration: 5784 lambda_n: 1.0084960596715515 Loss: 1.3061498370514695e-06\n",
      "Iteration: 5785 lambda_n: 1.0274568852024781 Loss: 1.2997774637549117e-06\n",
      "Iteration: 5786 lambda_n: 0.8832992498504907 Loss: 1.2933169625467476e-06\n",
      "Iteration: 5787 lambda_n: 0.904961208474645 Loss: 1.28779051530168e-06\n",
      "Iteration: 5788 lambda_n: 0.8906658749465862 Loss: 1.2821527366002449e-06\n",
      "Iteration: 5789 lambda_n: 0.9569903447443602 Loss: 1.2766283119803166e-06\n",
      "Iteration: 5790 lambda_n: 0.9497827573351426 Loss: 1.2707180852520141e-06\n",
      "Iteration: 5791 lambda_n: 0.8993338087413443 Loss: 1.2648795323724699e-06\n",
      "Iteration: 5792 lambda_n: 0.9983831047558961 Loss: 1.2593765081928496e-06\n",
      "Iteration: 5793 lambda_n: 0.919461923101779 Loss: 1.2532939849351925e-06\n",
      "Iteration: 5794 lambda_n: 1.0080409716316074 Loss: 1.2477193393738093e-06\n",
      "Iteration: 5795 lambda_n: 0.9433923393882668 Loss: 1.2416348342004733e-06\n",
      "Iteration: 5796 lambda_n: 0.8998342250403487 Loss: 1.2359683199176927e-06\n",
      "Iteration: 5797 lambda_n: 1.0273954277199036 Loss: 1.2305881101930227e-06\n",
      "Iteration: 5798 lambda_n: 0.9817575218460447 Loss: 1.2244719433756528e-06\n",
      "Iteration: 5799 lambda_n: 1.0072984452035523 Loss: 1.2186565162131147e-06\n",
      "Iteration: 5800 lambda_n: 0.9142058038163186 Loss: 1.2127181414135017e-06\n",
      "Iteration: 5801 lambda_n: 0.9203620204794717 Loss: 1.207354848147466e-06\n",
      "Iteration: 5802 lambda_n: 0.96093064731711 Loss: 1.2019793226846204e-06\n",
      "Iteration: 5803 lambda_n: 1.009251339106012 Loss: 1.1963918431268153e-06\n",
      "Iteration: 5804 lambda_n: 0.9496296572174775 Loss: 1.1905506809370966e-06\n",
      "Iteration: 5805 lambda_n: 0.9769864847258816 Loss: 1.1850814255306707e-06\n",
      "Iteration: 5806 lambda_n: 1.0227209572463034 Loss: 1.1794804667238038e-06\n",
      "Iteration: 5807 lambda_n: 0.8960016662555881 Loss: 1.1736450334343279e-06\n",
      "Iteration: 5808 lambda_n: 0.9564520491074798 Loss: 1.168557932789418e-06\n",
      "Iteration: 5809 lambda_n: 0.9199669571751794 Loss: 1.1631511639924223e-06\n",
      "Iteration: 5810 lambda_n: 1.0254615340333684 Loss: 1.1579747105831213e-06\n",
      "Iteration: 5811 lambda_n: 0.8940211007502822 Loss: 1.1522303465477615e-06\n",
      "Iteration: 5812 lambda_n: 0.9757617930425898 Loss: 1.1472471258552504e-06\n",
      "Iteration: 5813 lambda_n: 0.927410530175327 Loss: 1.1418318146102464e-06\n",
      "Iteration: 5814 lambda_n: 0.9325593233635209 Loss: 1.136709144922539e-06\n",
      "Iteration: 5815 lambda_n: 1.0272406445602966 Loss: 1.1315811499922637e-06\n",
      "Iteration: 5816 lambda_n: 1.013386838531265 Loss: 1.1259580056399399e-06\n",
      "Iteration: 5817 lambda_n: 0.9931976406076073 Loss: 1.1204382695954505e-06\n",
      "Iteration: 5818 lambda_n: 0.892113430580648 Loss: 1.1150550264120233e-06\n",
      "Iteration: 5819 lambda_n: 0.9860594959936986 Loss: 1.1102429080924996e-06\n",
      "Iteration: 5820 lambda_n: 0.9509517123365744 Loss: 1.1049469977223725e-06\n",
      "Iteration: 5821 lambda_n: 0.9747271308092375 Loss: 1.0998640113681123e-06\n",
      "Iteration: 5822 lambda_n: 0.9950885802478658 Loss: 1.0946779144743092e-06\n",
      "Iteration: 5823 lambda_n: 0.9378449801891925 Loss: 1.0894084532612181e-06\n",
      "Iteration: 5824 lambda_n: 0.8992279173706551 Loss: 1.0844660357076095e-06\n",
      "Iteration: 5825 lambda_n: 0.9637665906204378 Loss: 1.079748633342895e-06\n",
      "Iteration: 5826 lambda_n: 0.9160284492204385 Loss: 1.0747146557622765e-06\n",
      "Iteration: 5827 lambda_n: 0.9830347246439691 Loss: 1.0699523375865878e-06\n",
      "Iteration: 5828 lambda_n: 0.963576642179707 Loss: 1.064864313996625e-06\n",
      "Iteration: 5829 lambda_n: 0.9751103462858618 Loss: 1.0599007242564253e-06\n",
      "Iteration: 5830 lambda_n: 0.9697489583216541 Loss: 1.0549011408709463e-06\n",
      "Iteration: 5831 lambda_n: 0.9615688482130251 Loss: 1.0499525054332833e-06\n",
      "Iteration: 5832 lambda_n: 0.9920752608552997 Loss: 1.0450686372736124e-06\n",
      "Iteration: 5833 lambda_n: 0.9742649892739471 Loss: 1.04005326884736e-06\n",
      "Iteration: 5834 lambda_n: 0.9961728234424914 Loss: 1.0351515817362597e-06\n",
      "Iteration: 5835 lambda_n: 0.9574055917703079 Loss: 1.0301632991325461e-06\n",
      "Iteration: 5836 lambda_n: 0.9070965859275458 Loss: 1.0253922494958051e-06\n",
      "Iteration: 5837 lambda_n: 0.9358535415725241 Loss: 1.0208928456885468e-06\n",
      "Iteration: 5838 lambda_n: 0.9506981418404666 Loss: 1.0162711750801836e-06\n",
      "Iteration: 5839 lambda_n: 0.9577163633196242 Loss: 1.0115974548967805e-06\n",
      "Iteration: 5840 lambda_n: 0.8873814025557099 Loss: 1.0069108904388403e-06\n",
      "Iteration: 5841 lambda_n: 0.9018665957640714 Loss: 1.0025886311225564e-06\n",
      "Iteration: 5842 lambda_n: 0.8915359242474739 Loss: 9.98214678584401e-07\n",
      "Iteration: 5843 lambda_n: 0.8876127715142623 Loss: 9.939096968607536e-07\n",
      "Iteration: 5844 lambda_n: 1.0034351654830367 Loss: 9.896421479346936e-07\n",
      "Iteration: 5845 lambda_n: 1.0041122150171768 Loss: 9.848384569325319e-07\n",
      "Iteration: 5846 lambda_n: 0.9965104130667741 Loss: 9.800548633711878e-07\n",
      "Iteration: 5847 lambda_n: 1.0040279970259434 Loss: 9.75330549873036e-07\n",
      "Iteration: 5848 lambda_n: 0.8908755638105768 Loss: 9.705935476351287e-07\n",
      "Iteration: 5849 lambda_n: 0.9397842432458705 Loss: 9.66410817559213e-07\n",
      "Iteration: 5850 lambda_n: 0.98685212599108 Loss: 9.620174772494824e-07\n",
      "Iteration: 5851 lambda_n: 0.921655547262691 Loss: 9.574250802278888e-07\n",
      "Iteration: 5852 lambda_n: 0.9283819808184955 Loss: 9.531565606462357e-07\n",
      "Iteration: 5853 lambda_n: 0.9981687379545607 Loss: 9.488760629546193e-07\n",
      "Iteration: 5854 lambda_n: 0.9296344534950728 Loss: 9.442944725683936e-07\n",
      "Iteration: 5855 lambda_n: 0.8748688427157798 Loss: 9.400480627970052e-07\n",
      "Iteration: 5856 lambda_n: 0.9870129806695647 Loss: 9.360697884712981e-07\n",
      "Iteration: 5857 lambda_n: 0.9377744149583384 Loss: 9.316005625303804e-07\n",
      "Iteration: 5858 lambda_n: 0.9773402541565219 Loss: 9.273745695037377e-07\n",
      "Iteration: 5859 lambda_n: 0.9026465463820801 Loss: 9.22990261213615e-07\n",
      "Iteration: 5860 lambda_n: 0.959684999079181 Loss: 9.189601744479546e-07\n",
      "Iteration: 5861 lambda_n: 1.0228440957668603 Loss: 9.146941393535311e-07\n",
      "Iteration: 5862 lambda_n: 0.8873623129541727 Loss: 9.101684597453161e-07\n",
      "Iteration: 5863 lambda_n: 0.9287337966210104 Loss: 9.062616646794562e-07\n",
      "Iteration: 5864 lambda_n: 0.9088462963813465 Loss: 9.021902793886477e-07\n",
      "Iteration: 5865 lambda_n: 1.0118034474076283 Loss: 8.982239810215713e-07\n",
      "Iteration: 5866 lambda_n: 0.9626145316537806 Loss: 8.938277849543987e-07\n",
      "Iteration: 5867 lambda_n: 0.9973566201169353 Loss: 8.89665786521776e-07\n",
      "Iteration: 5868 lambda_n: 0.9683546939701961 Loss: 8.853736608372674e-07\n",
      "Iteration: 5869 lambda_n: 0.947959881123687 Loss: 8.812264556451473e-07\n",
      "Iteration: 5870 lambda_n: 0.9180479471855371 Loss: 8.771856183891794e-07\n",
      "Iteration: 5871 lambda_n: 0.8884245618375238 Loss: 8.732902353873462e-07\n",
      "Iteration: 5872 lambda_n: 0.9914161589971354 Loss: 8.695372930232311e-07\n",
      "Iteration: 5873 lambda_n: 0.9734737500133309 Loss: 8.653672897932931e-07\n",
      "Iteration: 5874 lambda_n: 0.9033978920448056 Loss: 8.612923960338542e-07\n",
      "Iteration: 5875 lambda_n: 1.013073897993174 Loss: 8.575286470628463e-07\n",
      "Iteration: 5876 lambda_n: 0.9030006998396762 Loss: 8.53326413776322e-07\n",
      "Iteration: 5877 lambda_n: 0.9026065236333215 Loss: 8.495991250658979e-07\n",
      "Iteration: 5878 lambda_n: 0.9927804832708663 Loss: 8.458897417714999e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5879 lambda_n: 1.0041019668456768 Loss: 8.418275951169322e-07\n",
      "Iteration: 5880 lambda_n: 1.0056212314898099 Loss: 8.377388602565703e-07\n",
      "Iteration: 5881 lambda_n: 0.9853708504145149 Loss: 8.336638338890248e-07\n",
      "Iteration: 5882 lambda_n: 1.0116074432897513 Loss: 8.296902960852598e-07\n",
      "Iteration: 5883 lambda_n: 0.9757201500577359 Loss: 8.256304079914294e-07\n",
      "Iteration: 5884 lambda_n: 0.9800263158193482 Loss: 8.217337137683519e-07\n",
      "Iteration: 5885 lambda_n: 0.9811946595470876 Loss: 8.178383001599148e-07\n",
      "Iteration: 5886 lambda_n: 0.8907512444939429 Loss: 8.139567365576263e-07\n",
      "Iteration: 5887 lambda_n: 0.9917777225454513 Loss: 8.104496927226295e-07\n",
      "Iteration: 5888 lambda_n: 0.987059305633093 Loss: 8.065617196325355e-07\n",
      "Iteration: 5889 lambda_n: 0.8817737508222528 Loss: 8.027108126841219e-07\n",
      "Iteration: 5890 lambda_n: 0.87839185346696 Loss: 7.992870962609291e-07\n",
      "Iteration: 5891 lambda_n: 1.0082931056817683 Loss: 7.958910624162049e-07\n",
      "Iteration: 5892 lambda_n: 1.016382724583375 Loss: 7.920093734440384e-07\n",
      "Iteration: 5893 lambda_n: 0.9075723670582376 Loss: 7.881156310837064e-07\n",
      "Iteration: 5894 lambda_n: 0.8984035575861508 Loss: 7.846558380234018e-07\n",
      "Iteration: 5895 lambda_n: 0.884411284185321 Loss: 7.812460375685905e-07\n",
      "Iteration: 5896 lambda_n: 0.9913812936651702 Loss: 7.779039350504296e-07\n",
      "Iteration: 5897 lambda_n: 0.9501526991060233 Loss: 7.741736352974853e-07\n",
      "Iteration: 5898 lambda_n: 1.0151796306774659 Loss: 7.706156173768755e-07\n",
      "Iteration: 5899 lambda_n: 0.989775744640489 Loss: 7.668315716543623e-07\n",
      "Iteration: 5900 lambda_n: 1.0270349120438744 Loss: 7.631603403735537e-07\n",
      "Iteration: 5901 lambda_n: 0.920092134974696 Loss: 7.59369153047575e-07\n",
      "Iteration: 5902 lambda_n: 0.979809221681819 Loss: 7.559896116021947e-07\n",
      "Iteration: 5903 lambda_n: 0.932105739113049 Loss: 7.524067486961539e-07\n",
      "Iteration: 5904 lambda_n: 0.9737782923187864 Loss: 7.490144819467678e-07\n",
      "Iteration: 5905 lambda_n: 0.9024743142185535 Loss: 7.454865373838611e-07\n",
      "Iteration: 5906 lambda_n: 1.0274216378780723 Loss: 7.422323287618716e-07\n",
      "Iteration: 5907 lambda_n: 1.01811211006029 Loss: 7.385437535655058e-07\n",
      "Iteration: 5908 lambda_n: 0.9003725464480914 Loss: 7.349067716883925e-07\n",
      "Iteration: 5909 lambda_n: 0.8780319412913842 Loss: 7.3170623329586e-07\n",
      "Iteration: 5910 lambda_n: 0.9926064196336247 Loss: 7.285987060666436e-07\n",
      "Iteration: 5911 lambda_n: 0.9320771228201289 Loss: 7.251006023308648e-07\n",
      "Iteration: 5912 lambda_n: 0.9348658864559699 Loss: 7.218315898977273e-07\n",
      "Iteration: 5913 lambda_n: 0.9928079996951702 Loss: 7.185675839247721e-07\n",
      "Iteration: 5914 lambda_n: 1.020670272173043 Loss: 7.151169576946421e-07\n",
      "Iteration: 5915 lambda_n: 0.9229256248559666 Loss: 7.115865341541673e-07\n",
      "Iteration: 5916 lambda_n: 0.9262381906756549 Loss: 7.084099679939987e-07\n",
      "Iteration: 5917 lambda_n: 0.9575470036789736 Loss: 7.052362370372399e-07\n",
      "Iteration: 5918 lambda_n: 0.9063399082641935 Loss: 7.019699318752452e-07\n",
      "Iteration: 5919 lambda_n: 0.9061037304397949 Loss: 6.988926243522389e-07\n",
      "Iteration: 5920 lambda_n: 1.0226755633781484 Loss: 6.958296106152025e-07\n",
      "Iteration: 5921 lambda_n: 0.9861197163810833 Loss: 6.923876917478628e-07\n",
      "Iteration: 5922 lambda_n: 0.8804809947135379 Loss: 6.890852283738766e-07\n",
      "Iteration: 5923 lambda_n: 0.960327387167063 Loss: 6.861506131418473e-07\n",
      "Iteration: 5924 lambda_n: 0.9124330595543322 Loss: 6.829635086778869e-07\n",
      "Iteration: 5925 lambda_n: 0.9644320726101685 Loss: 6.79949425303328e-07\n",
      "Iteration: 5926 lambda_n: 0.9292547335802095 Loss: 6.767776365159808e-07\n",
      "Iteration: 5927 lambda_n: 0.9449056851035692 Loss: 6.737357991161479e-07\n",
      "Iteration: 5928 lambda_n: 1.0160545595468864 Loss: 6.70656637162887e-07\n",
      "Iteration: 5929 lambda_n: 0.9316937472668873 Loss: 6.673607607575221e-07\n",
      "Iteration: 5930 lambda_n: 0.929960963530338 Loss: 6.643533921018039e-07\n",
      "Iteration: 5931 lambda_n: 0.919717195735945 Loss: 6.613651490688578e-07\n",
      "Iteration: 5932 lambda_n: 1.01922472653133 Loss: 6.584231206095543e-07\n",
      "Iteration: 5933 lambda_n: 1.0166131961006515 Loss: 6.551772926420995e-07\n",
      "Iteration: 5934 lambda_n: 0.9030225270222361 Loss: 6.51955747747279e-07\n",
      "Iteration: 5935 lambda_n: 0.9631131878295727 Loss: 6.49108236519073e-07\n",
      "Iteration: 5936 lambda_n: 0.9347932697466594 Loss: 6.460845106834413e-07\n",
      "Iteration: 5937 lambda_n: 0.9594848686839338 Loss: 6.431633729364634e-07\n",
      "Iteration: 5938 lambda_n: 0.9556363360273874 Loss: 6.401786381279214e-07\n",
      "Iteration: 5939 lambda_n: 0.9365332688348746 Loss: 6.372196766514253e-07\n",
      "Iteration: 5940 lambda_n: 0.9239015913489423 Loss: 6.34333273244631e-07\n",
      "Iteration: 5941 lambda_n: 0.971740088050544 Loss: 6.314987043277991e-07\n",
      "Iteration: 5942 lambda_n: 0.9248093432391613 Loss: 6.285306928708698e-07\n",
      "Iteration: 5943 lambda_n: 0.96462402450903 Loss: 6.257193046218609e-07\n",
      "Iteration: 5944 lambda_n: 0.9235265375090661 Loss: 6.228000033201123e-07\n",
      "Iteration: 5945 lambda_n: 0.9436190434256035 Loss: 6.200181232008193e-07\n",
      "Iteration: 5946 lambda_n: 0.9982977551087473 Loss: 6.171884214120709e-07\n",
      "Iteration: 5947 lambda_n: 0.909721117795086 Loss: 6.142084191889146e-07\n",
      "Iteration: 5948 lambda_n: 0.9664965989367187 Loss: 6.115059431463498e-07\n",
      "Iteration: 5949 lambda_n: 0.9472940464742294 Loss: 6.086474445511281e-07\n",
      "Iteration: 5950 lambda_n: 0.9860103966351571 Loss: 6.058588415585162e-07\n",
      "Iteration: 5951 lambda_n: 0.9003192272727292 Loss: 6.029695715005944e-07\n",
      "Iteration: 5952 lambda_n: 0.9575106610066788 Loss: 6.003439858253957e-07\n",
      "Iteration: 5953 lambda_n: 0.8744546170440773 Loss: 5.975637783762006e-07\n",
      "Iteration: 5954 lambda_n: 0.9926055703549194 Loss: 5.950364943198107e-07\n",
      "Iteration: 5955 lambda_n: 0.9417901661408845 Loss: 5.921798774706633e-07\n",
      "Iteration: 5956 lambda_n: 1.0259437412389008 Loss: 5.89482519842756e-07\n",
      "Iteration: 5957 lambda_n: 0.8851151995272544 Loss: 5.865575303909983e-07\n",
      "Iteration: 5958 lambda_n: 0.9894865676775101 Loss: 5.840465735251315e-07\n",
      "Iteration: 5959 lambda_n: 0.9350753096814953 Loss: 5.812515507084039e-07\n",
      "Iteration: 5960 lambda_n: 1.0097527594464422 Loss: 5.786228706840315e-07\n",
      "Iteration: 5961 lambda_n: 0.8915613613089018 Loss: 5.757971011911046e-07\n",
      "Iteration: 5962 lambda_n: 0.899760918884506 Loss: 5.73314277922934e-07\n",
      "Iteration: 5963 lambda_n: 0.9736584424803061 Loss: 5.708194299249063e-07\n",
      "Iteration: 5964 lambda_n: 0.9145600740389229 Loss: 5.681314334909608e-07\n",
      "Iteration: 5965 lambda_n: 0.9456672477658454 Loss: 5.656184861550281e-07\n",
      "Iteration: 5966 lambda_n: 0.9384552833848586 Loss: 5.630315640613331e-07\n",
      "Iteration: 5967 lambda_n: 0.9527641404619025 Loss: 5.604761176726052e-07\n",
      "Iteration: 5968 lambda_n: 0.9686517013766144 Loss: 5.578934887996898e-07\n",
      "Iteration: 5969 lambda_n: 1.018005832551607 Loss: 5.552798988701159e-07\n",
      "Iteration: 5970 lambda_n: 1.0034862251812762 Loss: 5.525460170975418e-07\n",
      "Iteration: 5971 lambda_n: 0.8877313434508353 Loss: 5.498644027338805e-07\n",
      "Iteration: 5972 lambda_n: 0.8784995258052294 Loss: 5.475036387460065e-07\n",
      "Iteration: 5973 lambda_n: 0.9772567612783408 Loss: 5.451774603215736e-07\n",
      "Iteration: 5974 lambda_n: 1.0085906636876865 Loss: 5.426007824399584e-07\n",
      "Iteration: 5975 lambda_n: 0.9662624198952499 Loss: 5.39954063197894e-07\n",
      "Iteration: 5976 lambda_n: 0.908742244798951 Loss: 5.374307954049134e-07\n",
      "Iteration: 5977 lambda_n: 0.9299411807911312 Loss: 5.350688292146321e-07\n",
      "Iteration: 5978 lambda_n: 0.9127972385406579 Loss: 5.326623918509969e-07\n",
      "Iteration: 5979 lambda_n: 0.9502130049137607 Loss: 5.303109471242866e-07\n",
      "Iteration: 5980 lambda_n: 0.9621409545035875 Loss: 5.278739277133751e-07\n",
      "Iteration: 5981 lambda_n: 0.9124568285492015 Loss: 5.254176622505842e-07\n",
      "Iteration: 5982 lambda_n: 0.9650492487274162 Loss: 5.230990809811375e-07\n",
      "Iteration: 5983 lambda_n: 0.9831053547325876 Loss: 5.206576876696032e-07\n",
      "Iteration: 5984 lambda_n: 0.9527316173393235 Loss: 5.181822295261061e-07\n",
      "Iteration: 5985 lambda_n: 0.988153368529287 Loss: 5.157946643376967e-07\n",
      "Iteration: 5986 lambda_n: 0.9596898069561285 Loss: 5.133297475228872e-07\n",
      "Iteration: 5987 lambda_n: 0.9464525549028868 Loss: 5.109472784679703e-07\n",
      "Iteration: 5988 lambda_n: 0.9983810089019103 Loss: 5.086085823196235e-07\n",
      "Iteration: 5989 lambda_n: 1.027534887885372 Loss: 5.061528683985696e-07\n",
      "Iteration: 5990 lambda_n: 0.9500967678951469 Loss: 5.03637654609395e-07\n",
      "Iteration: 5991 lambda_n: 1.0226864649208762 Loss: 5.013235580514735e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5992 lambda_n: 0.9895100006565195 Loss: 4.98844110334829e-07\n",
      "Iteration: 5993 lambda_n: 0.9392110049327239 Loss: 4.96456968776268e-07\n",
      "Iteration: 5994 lambda_n: 1.0076295089908998 Loss: 4.942020195975814e-07\n",
      "Iteration: 5995 lambda_n: 0.9075129126749543 Loss: 4.917937990794477e-07\n",
      "Iteration: 5996 lambda_n: 0.9288619295890089 Loss: 4.896354309153531e-07\n",
      "Iteration: 5997 lambda_n: 0.9154414451104289 Loss: 4.874359885897892e-07\n",
      "Iteration: 5998 lambda_n: 0.9298056825244839 Loss: 4.852780671878984e-07\n",
      "Iteration: 5999 lambda_n: 0.8800160573492284 Loss: 4.830959945108511e-07\n",
      "Iteration: 6000 lambda_n: 0.8937326803824893 Loss: 4.810400600837535e-07\n",
      "Iteration: 6001 lambda_n: 1.0061426823464374 Loss: 4.78960971323093e-07\n",
      "Iteration: 6002 lambda_n: 0.9764703523509484 Loss: 4.766305054838787e-07\n",
      "Iteration: 6003 lambda_n: 0.9587200640064131 Loss: 4.74379779110249e-07\n",
      "Iteration: 6004 lambda_n: 0.9527017187918924 Loss: 4.721804077159375e-07\n",
      "Iteration: 6005 lambda_n: 0.8767675268712497 Loss: 4.700049817891611e-07\n",
      "Iteration: 6006 lambda_n: 0.9557751435364468 Loss: 4.680121753757144e-07\n",
      "Iteration: 6007 lambda_n: 0.9293854138032032 Loss: 4.6584900878206743e-07\n",
      "Iteration: 6008 lambda_n: 0.8773613703040595 Loss: 4.6375529698850757e-07\n",
      "Iteration: 6009 lambda_n: 1.0045151734998474 Loss: 4.61787673119151e-07\n",
      "Iteration: 6010 lambda_n: 0.9490894790592945 Loss: 4.595444504073054e-07\n",
      "Iteration: 6011 lambda_n: 0.9823113502623555 Loss: 4.5743530293864285e-07\n",
      "Iteration: 6012 lambda_n: 0.9452338478237154 Loss: 4.552623522494338e-07\n",
      "Iteration: 6013 lambda_n: 0.904569001180445 Loss: 4.531813586284635e-07\n",
      "Iteration: 6014 lambda_n: 0.9348275982973192 Loss: 4.511989999032665e-07\n",
      "Iteration: 6015 lambda_n: 0.9537028517056552 Loss: 4.4915929675749e-07\n",
      "Iteration: 6016 lambda_n: 0.88216791467734 Loss: 4.4708782248037224e-07\n",
      "Iteration: 6017 lambda_n: 0.8859877301310005 Loss: 4.45180566866098e-07\n",
      "Iteration: 6018 lambda_n: 0.9696036134981564 Loss: 4.432732294684307e-07\n",
      "Iteration: 6019 lambda_n: 1.0265992282888834 Loss: 4.4119483418894436e-07\n",
      "Iteration: 6020 lambda_n: 0.9511721886585939 Loss: 4.3900459042925467e-07\n",
      "Iteration: 6021 lambda_n: 0.9347242878968531 Loss: 4.369853505786312e-07\n",
      "Iteration: 6022 lambda_n: 1.0001081589345278 Loss: 4.350101609129738e-07\n",
      "Iteration: 6023 lambda_n: 0.9628445656722151 Loss: 4.329063656275028e-07\n",
      "Iteration: 6024 lambda_n: 0.9355845699917906 Loss: 4.3089075854683273e-07\n",
      "Iteration: 6025 lambda_n: 0.934450857474421 Loss: 4.2894134216944203e-07\n",
      "Iteration: 6026 lambda_n: 0.8991277024695862 Loss: 4.2700310267260507e-07\n",
      "Iteration: 6027 lambda_n: 1.0183966666728745 Loss: 4.251465633222822e-07\n",
      "Iteration: 6028 lambda_n: 0.880513077266097 Loss: 4.230529035509135e-07\n",
      "Iteration: 6029 lambda_n: 0.9101366052750196 Loss: 4.2125163066676933e-07\n",
      "Iteration: 6030 lambda_n: 0.9075734379266663 Loss: 4.1939768956861496e-07\n",
      "Iteration: 6031 lambda_n: 0.9745525693825906 Loss: 4.1755711145875446e-07\n",
      "Iteration: 6032 lambda_n: 0.9972899047561576 Loss: 4.155893779620515e-07\n",
      "Iteration: 6033 lambda_n: 0.9566342958458732 Loss: 4.1358523100912773e-07\n",
      "Iteration: 6034 lambda_n: 0.8817570162980468 Loss: 4.1167206255829873e-07\n",
      "Iteration: 6035 lambda_n: 0.9203740916761768 Loss: 4.0991680377512854e-07\n",
      "Iteration: 6036 lambda_n: 0.8991548811192577 Loss: 4.0809248960656905e-07\n",
      "Iteration: 6037 lambda_n: 0.8748321408546897 Loss: 4.063181723985235e-07\n",
      "Iteration: 6038 lambda_n: 0.9600244258866235 Loss: 4.045993627282302e-07\n",
      "Iteration: 6039 lambda_n: 1.0251546569146701 Loss: 4.027211578656574e-07\n",
      "Iteration: 6040 lambda_n: 0.9711814875587385 Loss: 4.007248484254237e-07\n",
      "Iteration: 6041 lambda_n: 0.9290415332946361 Loss: 3.9884302385553716e-07\n",
      "Iteration: 6042 lambda_n: 1.0274766466219154 Loss: 3.9705131224028733e-07\n",
      "Iteration: 6043 lambda_n: 0.9517948214332382 Loss: 3.9507867088493715e-07\n",
      "Iteration: 6044 lambda_n: 0.9563359624092534 Loss: 3.9326041554063745e-07\n",
      "Iteration: 6045 lambda_n: 0.9664353565436258 Loss: 3.9144189928115297e-07\n",
      "Iteration: 6046 lambda_n: 1.026073781424526 Loss: 3.8961268288475365e-07\n",
      "Iteration: 6047 lambda_n: 0.9610624459925738 Loss: 3.8767966838761625e-07\n",
      "Iteration: 6048 lambda_n: 0.9936409486552513 Loss: 3.858781178627848e-07\n",
      "Iteration: 6049 lambda_n: 0.9253875301676658 Loss: 3.840241597986712e-07\n",
      "Iteration: 6050 lambda_n: 0.9121286431870872 Loss: 3.823058523526661e-07\n",
      "Iteration: 6051 lambda_n: 0.883793087838341 Loss: 3.8061974885555946e-07\n",
      "Iteration: 6052 lambda_n: 0.9161583014992805 Loss: 3.7899323552039006e-07\n",
      "Iteration: 6053 lambda_n: 0.9958107019961028 Loss: 3.773143687066719e-07\n",
      "Iteration: 6054 lambda_n: 0.9497254524412978 Loss: 3.7549762824833954e-07\n",
      "Iteration: 6055 lambda_n: 0.9779326047343793 Loss: 3.7377331411392434e-07\n",
      "Iteration: 6056 lambda_n: 0.9041966274031108 Loss: 3.720059470753824e-07\n",
      "Iteration: 6057 lambda_n: 0.9631310211074011 Loss: 3.703795721414464e-07\n",
      "Iteration: 6058 lambda_n: 0.9381864108651958 Loss: 3.6865477198990523e-07\n",
      "Iteration: 6059 lambda_n: 0.9829042238912785 Loss: 3.669824736558838e-07\n",
      "Iteration: 6060 lambda_n: 0.9040862997236051 Loss: 3.6523842063208176e-07\n",
      "Iteration: 6061 lambda_n: 0.8893210576731378 Loss: 3.6364185110302285e-07\n",
      "Iteration: 6062 lambda_n: 1.0107599359443853 Loss: 3.620782269451319e-07\n",
      "Iteration: 6063 lambda_n: 0.8947575527959757 Loss: 3.6030873407511244e-07\n",
      "Iteration: 6064 lambda_n: 0.8907899428993891 Loss: 3.5874998285512417e-07\n",
      "Iteration: 6065 lambda_n: 0.9022947373649463 Loss: 3.572048626383633e-07\n",
      "Iteration: 6066 lambda_n: 0.9296741236602899 Loss: 3.556465331140275e-07\n",
      "Iteration: 6067 lambda_n: 0.897158932133841 Loss: 3.5404792784309337e-07\n",
      "Iteration: 6068 lambda_n: 1.0228269126415657 Loss: 3.525121736388103e-07\n",
      "Iteration: 6069 lambda_n: 0.9973662492991777 Loss: 3.507689025730193e-07\n",
      "Iteration: 6070 lambda_n: 0.9085798134425996 Loss: 3.490774393227882e-07\n",
      "Iteration: 6071 lambda_n: 0.9918779535648197 Loss: 3.4754398837607555e-07\n",
      "Iteration: 6072 lambda_n: 0.9977160628621797 Loss: 3.458773116181404e-07\n",
      "Iteration: 6073 lambda_n: 0.9578019017859324 Loss: 3.4420887164366787e-07\n",
      "Iteration: 6074 lambda_n: 0.9256171217694356 Loss: 3.426149114464105e-07\n",
      "Iteration: 6075 lambda_n: 1.0022675559727423 Loss: 3.410816521994711e-07\n",
      "Iteration: 6076 lambda_n: 1.0224293284930879 Loss: 3.394288600174219e-07\n",
      "Iteration: 6077 lambda_n: 0.88956840521797 Loss: 3.377509973804329e-07\n",
      "Iteration: 6078 lambda_n: 0.9470349293431989 Loss: 3.3629838942722494e-07\n",
      "Iteration: 6079 lambda_n: 0.9066384523529646 Loss: 3.3475859933024805e-07\n",
      "Iteration: 6080 lambda_n: 0.9990988171695718 Loss: 3.3329124562262703e-07\n",
      "Iteration: 6081 lambda_n: 0.9923519095897997 Loss: 3.316813432525044e-07\n",
      "Iteration: 6082 lambda_n: 0.8963824097969214 Loss: 3.3009004342225456e-07\n",
      "Iteration: 6083 lambda_n: 0.9484995902196447 Loss: 3.286595393414631e-07\n",
      "Iteration: 6084 lambda_n: 0.9532478034473306 Loss: 3.2715242921428953e-07\n",
      "Iteration: 6085 lambda_n: 0.9840535942181138 Loss: 3.256447265621831e-07\n",
      "Iteration: 6086 lambda_n: 0.8944282329569222 Loss: 3.2409547960656507e-07\n",
      "Iteration: 6087 lambda_n: 0.8790300501291782 Loss: 3.226940400133917e-07\n",
      "Iteration: 6088 lambda_n: 0.9709590004498091 Loss: 3.2132268848521814e-07\n",
      "Iteration: 6089 lambda_n: 0.9959086906124364 Loss: 3.1981436450069786e-07\n",
      "Iteration: 6090 lambda_n: 0.891572134946374 Loss: 3.182745518666106e-07\n",
      "Iteration: 6091 lambda_n: 0.9853681517238478 Loss: 3.1690270136771356e-07\n",
      "Iteration: 6092 lambda_n: 0.9830536093005481 Loss: 3.153930696487929e-07\n",
      "Iteration: 6093 lambda_n: 0.8996617931248653 Loss: 3.1389416545736e-07\n",
      "Iteration: 6094 lambda_n: 0.8875536911888612 Loss: 3.1252893796129485e-07\n",
      "Iteration: 6095 lambda_n: 0.8824335777332005 Loss: 3.1118794806319775e-07\n",
      "Iteration: 6096 lambda_n: 0.929347433810619 Loss: 3.0986042040378086e-07\n",
      "Iteration: 6097 lambda_n: 0.9178036328502556 Loss: 3.0846828610198265e-07\n",
      "Iteration: 6098 lambda_n: 0.8738656234332985 Loss: 3.070996271105724e-07\n",
      "Iteration: 6099 lambda_n: 0.9956610486731451 Loss: 3.058022776975655e-07\n",
      "Iteration: 6100 lambda_n: 0.9992503801826775 Loss: 3.043303605475248e-07\n",
      "Iteration: 6101 lambda_n: 0.9731895474595276 Loss: 3.028602547406901e-07\n",
      "Iteration: 6102 lambda_n: 0.8979867650428691 Loss: 3.014354132456062e-07\n",
      "Iteration: 6103 lambda_n: 0.8953295444327598 Loss: 3.0012686740920023e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6104 lambda_n: 0.8858615163051617 Loss: 2.98827863204613e-07\n",
      "Iteration: 6105 lambda_n: 0.9739441180802335 Loss: 2.9754816450538694e-07\n",
      "Iteration: 6106 lambda_n: 1.0200639675803205 Loss: 2.9614725480875686e-07\n",
      "Iteration: 6107 lambda_n: 0.9317370242503545 Loss: 2.9468692224918937e-07\n",
      "Iteration: 6108 lambda_n: 0.9894025500393654 Loss: 2.9335962375618616e-07\n",
      "Iteration: 6109 lambda_n: 0.9735408870164135 Loss: 2.9195653339607536e-07\n",
      "Iteration: 6110 lambda_n: 0.9456245875578065 Loss: 2.9058254701049757e-07\n",
      "Iteration: 6111 lambda_n: 0.9773296607298356 Loss: 2.8925424722262707e-07\n",
      "Iteration: 6112 lambda_n: 1.0140946208727568 Loss: 2.8788769425780334e-07\n",
      "Iteration: 6113 lambda_n: 0.9547805025015276 Loss: 2.86476440969145e-07\n",
      "Iteration: 6114 lambda_n: 0.896179969132374 Loss: 2.851542521032528e-07\n",
      "Iteration: 6115 lambda_n: 0.9576599790501293 Loss: 2.8391894794453174e-07\n",
      "Iteration: 6116 lambda_n: 0.9442370075762584 Loss: 2.8260462400392906e-07\n",
      "Iteration: 6117 lambda_n: 0.8738780650683456 Loss: 2.813147279092701e-07\n",
      "Iteration: 6118 lambda_n: 0.915052892614504 Loss: 2.801264021275469e-07\n",
      "Iteration: 6119 lambda_n: 1.0217053978998776 Loss: 2.7888734777433586e-07\n",
      "Iteration: 6120 lambda_n: 0.9041742461207457 Loss: 2.7751000385063465e-07\n",
      "Iteration: 6121 lambda_n: 0.9111643509871984 Loss: 2.7629712834857843e-07\n",
      "Iteration: 6122 lambda_n: 1.0263194692832358 Loss: 2.7508022428793563e-07\n",
      "Iteration: 6123 lambda_n: 0.8884272033132813 Loss: 2.737155690968402e-07\n",
      "Iteration: 6124 lambda_n: 0.918412795435976 Loss: 2.7254013077527324e-07\n",
      "Iteration: 6125 lambda_n: 1.0068066600112473 Loss: 2.713302441480709e-07\n",
      "Iteration: 6126 lambda_n: 1.0097602726033006 Loss: 2.700098053656554e-07\n",
      "Iteration: 6127 lambda_n: 0.9615377364089993 Loss: 2.686919453560539e-07\n",
      "Iteration: 6128 lambda_n: 0.8789074910704331 Loss: 2.674431539425634e-07\n",
      "Iteration: 6129 lambda_n: 0.9175782397573081 Loss: 2.6630698961490695e-07\n",
      "Iteration: 6130 lambda_n: 0.9403350627884344 Loss: 2.651258807674171e-07\n",
      "Iteration: 6131 lambda_n: 0.9626502145074266 Loss: 2.6392085413433347e-07\n",
      "Iteration: 6132 lambda_n: 0.8902199736460121 Loss: 2.6269284476326697e-07\n",
      "Iteration: 6133 lambda_n: 1.0259576434393725 Loss: 2.6156252178444707e-07\n",
      "Iteration: 6134 lambda_n: 0.919373933548608 Loss: 2.60265463265002e-07\n",
      "Iteration: 6135 lambda_n: 0.9605659385484269 Loss: 2.591089232225257e-07\n",
      "Iteration: 6136 lambda_n: 0.8989593725761754 Loss: 2.579059414177197e-07\n",
      "Iteration: 6137 lambda_n: 0.8894690935985855 Loss: 2.567853472165696e-07\n",
      "Iteration: 6138 lambda_n: 0.8933496078052634 Loss: 2.5568140672764835e-07\n",
      "Iteration: 6139 lambda_n: 0.9798619981059388 Loss: 2.54577422763039e-07\n",
      "Iteration: 6140 lambda_n: 0.9247532258239702 Loss: 2.5337176365029376e-07\n",
      "Iteration: 6141 lambda_n: 0.8864414218226158 Loss: 2.522393081480051e-07\n",
      "Iteration: 6142 lambda_n: 0.9299532828967414 Loss: 2.5115862754640717e-07\n",
      "Iteration: 6143 lambda_n: 1.00203945360478 Loss: 2.500297643019722e-07\n",
      "Iteration: 6144 lambda_n: 0.9076961477589691 Loss: 2.4881887054105776e-07\n",
      "Iteration: 6145 lambda_n: 1.0107235821004283 Loss: 2.4772730318780475e-07\n",
      "Iteration: 6146 lambda_n: 0.8737787987785207 Loss: 2.465171776260589e-07\n",
      "Iteration: 6147 lambda_n: 0.9711239138559815 Loss: 2.4547613135508785e-07\n",
      "Iteration: 6148 lambda_n: 0.9758547628016249 Loss: 2.4432399795870185e-07\n",
      "Iteration: 6149 lambda_n: 0.8981524077353908 Loss: 2.431716931306535e-07\n",
      "Iteration: 6150 lambda_n: 0.9840680373866624 Loss: 2.421161491367696e-07\n",
      "Iteration: 6151 lambda_n: 1.0136500048475032 Loss: 2.409646607780351e-07\n",
      "Iteration: 6152 lambda_n: 0.8958073048257227 Loss: 2.397842064665042e-07\n",
      "Iteration: 6153 lambda_n: 1.0002020122121495 Loss: 2.387461044471885e-07\n",
      "Iteration: 6154 lambda_n: 0.8930204886237214 Loss: 2.3759205021546715e-07\n",
      "Iteration: 6155 lambda_n: 0.9633351513922355 Loss: 2.3656665192960377e-07\n",
      "Iteration: 6156 lambda_n: 0.9890765406972168 Loss: 2.3546529646669214e-07\n",
      "Iteration: 6157 lambda_n: 0.909211158631988 Loss: 2.3433978351816256e-07\n",
      "Iteration: 6158 lambda_n: 0.9863036493124755 Loss: 2.3331010535521134e-07\n",
      "Iteration: 6159 lambda_n: 0.963651882285736 Loss: 2.3219803532336409e-07\n",
      "Iteration: 6160 lambda_n: 0.96408225494387 Loss: 2.311166918837536e-07\n",
      "Iteration: 6161 lambda_n: 0.9424506040285179 Loss: 2.3003991089775683e-07\n",
      "Iteration: 6162 lambda_n: 1.026172551183709 Loss: 2.2899220161956466e-07\n",
      "Iteration: 6163 lambda_n: 0.8816120979237035 Loss: 2.2785662318219402e-07\n",
      "Iteration: 6164 lambda_n: 0.9432015188770434 Loss: 2.268858627376652e-07\n",
      "Iteration: 6165 lambda_n: 0.9968717741542102 Loss: 2.258517163799862e-07\n",
      "Iteration: 6166 lambda_n: 0.9032177495839946 Loss: 2.2476371418866315e-07\n",
      "Iteration: 6167 lambda_n: 0.9379088609427788 Loss: 2.2378268351106329e-07\n",
      "Iteration: 6168 lambda_n: 0.9866120203229957 Loss: 2.2276842624172386e-07\n",
      "Iteration: 6169 lambda_n: 0.9275960427153613 Loss: 2.2170634434613807e-07\n",
      "Iteration: 6170 lambda_n: 1.0260545385319995 Loss: 2.207125608283499e-07\n",
      "Iteration: 6171 lambda_n: 0.8841116190950907 Loss: 2.1961822853250927e-07\n",
      "Iteration: 6172 lambda_n: 0.9853983560225775 Loss: 2.1867996708874243e-07\n",
      "Iteration: 6173 lambda_n: 1.0100359527540674 Loss: 2.1763869010275264e-07\n",
      "Iteration: 6174 lambda_n: 0.9287843882440051 Loss: 2.165764686089937e-07\n",
      "Iteration: 6175 lambda_n: 1.025788171749403 Loss: 2.1560447152848126e-07\n",
      "Iteration: 6176 lambda_n: 0.9462004882221713 Loss: 2.145357831698384e-07\n",
      "Iteration: 6177 lambda_n: 0.888942910445467 Loss: 2.1355490498723427e-07\n",
      "Iteration: 6178 lambda_n: 0.9824474035457467 Loss: 2.126376029023936e-07\n",
      "Iteration: 6179 lambda_n: 1.02464705067827 Loss: 2.116281751294414e-07\n",
      "Iteration: 6180 lambda_n: 0.9202614590445071 Loss: 2.1058039477833193e-07\n",
      "Iteration: 6181 lambda_n: 1.0055180666858294 Loss: 2.096440234539595e-07\n",
      "Iteration: 6182 lambda_n: 0.9659014508041466 Loss: 2.0862546010615454e-07\n",
      "Iteration: 6183 lambda_n: 0.8890901840158055 Loss: 2.07651789017542e-07\n",
      "Iteration: 6184 lambda_n: 0.8866605787906681 Loss: 2.0675973687451578e-07\n",
      "Iteration: 6185 lambda_n: 1.0196098436720609 Loss: 2.0587395058903781e-07\n",
      "Iteration: 6186 lambda_n: 0.9138897817812568 Loss: 2.048597174988622e-07\n",
      "Iteration: 6187 lambda_n: 1.0180538715491152 Loss: 2.0395513308757144e-07\n",
      "Iteration: 6188 lambda_n: 1.0091127311703665 Loss: 2.029519025314183e-07\n",
      "Iteration: 6189 lambda_n: 0.8757013504495802 Loss: 2.0196238283836685e-07\n",
      "Iteration: 6190 lambda_n: 0.919396733543103 Loss: 2.011078781182157e-07\n",
      "Iteration: 6191 lambda_n: 0.907051017240728 Loss: 2.0021453819214904e-07\n",
      "Iteration: 6192 lambda_n: 1.0058161247994581 Loss: 1.9933711602150363e-07\n",
      "Iteration: 6193 lambda_n: 1.0099380675610854 Loss: 1.983684264739178e-07\n",
      "Iteration: 6194 lambda_n: 1.014609950381469 Loss: 1.974005022913608e-07\n",
      "Iteration: 6195 lambda_n: 0.8947517971456523 Loss: 1.964328538827636e-07\n",
      "Iteration: 6196 lambda_n: 0.9967036165805228 Loss: 1.955837064818532e-07\n",
      "Iteration: 6197 lambda_n: 0.8874524375808985 Loss: 1.9464190014203963e-07\n",
      "Iteration: 6198 lambda_n: 0.9219198745075832 Loss: 1.9380737292964204e-07\n",
      "Iteration: 6199 lambda_n: 0.8857252965578123 Loss: 1.9294415772192147e-07\n",
      "Iteration: 6200 lambda_n: 0.8930138965299166 Loss: 1.921185329854958e-07\n",
      "Iteration: 6201 lambda_n: 0.94420781293382 Loss: 1.9128968288429258e-07\n",
      "Iteration: 6202 lambda_n: 0.9288202913879773 Loss: 1.90417105216421e-07\n",
      "Iteration: 6203 lambda_n: 0.9231042949449333 Loss: 1.8956267057060224e-07\n",
      "Iteration: 6204 lambda_n: 0.965083513657013 Loss: 1.88717311804176e-07\n",
      "Iteration: 6205 lambda_n: 0.8740734044271617 Loss: 1.8783745829825235e-07\n",
      "Iteration: 6206 lambda_n: 0.8938892316709484 Loss: 1.87044299856986e-07\n",
      "Iteration: 6207 lambda_n: 0.9892641673828044 Loss: 1.862365917509702e-07\n",
      "Iteration: 6208 lambda_n: 0.9798097702130205 Loss: 1.8534657159750859e-07\n",
      "Iteration: 6209 lambda_n: 1.0094834332945004 Loss: 1.8446927836440828e-07\n",
      "Iteration: 6210 lambda_n: 0.9663855078522163 Loss: 1.8356970290097132e-07\n",
      "Iteration: 6211 lambda_n: 0.8872781752473902 Loss: 1.8271274091951624e-07\n",
      "Iteration: 6212 lambda_n: 0.965452375419919 Loss: 1.8192960935501695e-07\n",
      "Iteration: 6213 lambda_n: 0.9668765677758595 Loss: 1.8108113925882477e-07\n",
      "Iteration: 6214 lambda_n: 0.901561828089851 Loss: 1.8023538844846914e-07\n",
      "Iteration: 6215 lambda_n: 0.9229368802126978 Loss: 1.7945046081085692e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6216 lambda_n: 0.9168713956261909 Loss: 1.7865043001277635e-07\n",
      "Iteration: 6217 lambda_n: 0.9155505232112746 Loss: 1.7785920755825137e-07\n",
      "Iteration: 6218 lambda_n: 0.9971540016190888 Loss: 1.770726314305914e-07\n",
      "Iteration: 6219 lambda_n: 1.0257866747734967 Loss: 1.7621974402330807e-07\n",
      "Iteration: 6220 lambda_n: 0.9500170308921796 Loss: 1.7534660137146082e-07\n",
      "Iteration: 6221 lambda_n: 1.0035386880867552 Loss: 1.7454196848507302e-07\n",
      "Iteration: 6222 lambda_n: 0.9605885645017653 Loss: 1.7369591323667776e-07\n",
      "Iteration: 6223 lambda_n: 0.9448584723133915 Loss: 1.728900019614482e-07\n",
      "Iteration: 6224 lambda_n: 0.8785325607700901 Loss: 1.7210097383154513e-07\n",
      "Iteration: 6225 lambda_n: 0.9727241900490308 Loss: 1.7137068824541648e-07\n",
      "Iteration: 6226 lambda_n: 1.0035090210694773 Loss: 1.7056554398401724e-07\n",
      "Iteration: 6227 lambda_n: 0.9513301913890491 Loss: 1.697388295880806e-07\n",
      "Iteration: 6228 lambda_n: 0.9210900420103107 Loss: 1.6895890837864465e-07\n",
      "Iteration: 6229 lambda_n: 0.8802682108271904 Loss: 1.6820725611689348e-07\n",
      "Iteration: 6230 lambda_n: 0.9096840838306112 Loss: 1.6749211920532977e-07\n",
      "Iteration: 6231 lambda_n: 0.9019041472676372 Loss: 1.6675623376516775e-07\n",
      "Iteration: 6232 lambda_n: 0.9539049634739887 Loss: 1.6602985464881216e-07\n",
      "Iteration: 6233 lambda_n: 0.9133482336906962 Loss: 1.652649490993712e-07\n",
      "Iteration: 6234 lambda_n: 0.9721211559884043 Loss: 1.6453594654722532e-07\n",
      "Iteration: 6235 lambda_n: 0.9588689857514243 Loss: 1.637634641157271e-07\n",
      "Iteration: 6236 lambda_n: 0.8840839667009053 Loss: 1.6300509796442706e-07\n",
      "Iteration: 6237 lambda_n: 0.9279843536999873 Loss: 1.623091245624527e-07\n",
      "Iteration: 6238 lambda_n: 0.9708648890618565 Loss: 1.6158171817086116e-07\n",
      "Iteration: 6239 lambda_n: 0.9848918766523131 Loss: 1.608241183336862e-07\n",
      "Iteration: 6240 lambda_n: 0.8777047575825311 Loss: 1.6005918482503245e-07\n",
      "Iteration: 6241 lambda_n: 0.8965730748215089 Loss: 1.593807501018047e-07\n",
      "Iteration: 6242 lambda_n: 1.0226859511131885 Loss: 1.586906754461174e-07\n",
      "Iteration: 6243 lambda_n: 0.9509753321545593 Loss: 1.579069507212852e-07\n",
      "Iteration: 6244 lambda_n: 0.9183936237831997 Loss: 1.571817886127315e-07\n",
      "Iteration: 6245 lambda_n: 0.8970561909034772 Loss: 1.564846955596514e-07\n",
      "Iteration: 6246 lambda_n: 0.9119814742489404 Loss: 1.5580682559986104e-07\n",
      "Iteration: 6247 lambda_n: 0.992798027701176 Loss: 1.5512066996653886e-07\n",
      "Iteration: 6248 lambda_n: 1.0170481910409659 Loss: 1.5437700755466876e-07\n",
      "Iteration: 6249 lambda_n: 0.9790206944285065 Loss: 1.5361884193209546e-07\n",
      "Iteration: 6250 lambda_n: 0.9210931120679219 Loss: 1.5289261749416882e-07\n",
      "Iteration: 6251 lambda_n: 0.8902000965664355 Loss: 1.52212601256455e-07\n",
      "Iteration: 6252 lambda_n: 1.0196869885960704 Loss: 1.5155832302658804e-07\n",
      "Iteration: 6253 lambda_n: 0.9956328472878104 Loss: 1.508121046409316e-07\n",
      "Iteration: 6254 lambda_n: 0.8892194725482511 Loss: 1.5008708614869508e-07\n",
      "Iteration: 6255 lambda_n: 0.9434590854306557 Loss: 1.494426787825673e-07\n",
      "Iteration: 6256 lambda_n: 0.947767021186796 Loss: 1.4876190800814494e-07\n",
      "Iteration: 6257 lambda_n: 1.0055273992963394 Loss: 1.4808115241260405e-07\n",
      "Iteration: 6258 lambda_n: 0.9991634045715873 Loss: 1.4736222311755704e-07\n",
      "Iteration: 6259 lambda_n: 0.9499327898742169 Loss: 1.4665132160072737e-07\n",
      "Iteration: 6260 lambda_n: 0.9370854418795823 Loss: 1.4597871687127727e-07\n",
      "Iteration: 6261 lambda_n: 0.9343797252557927 Loss: 1.4531826024416648e-07\n",
      "Iteration: 6262 lambda_n: 0.9787717580946889 Loss: 1.4466269832651812e-07\n",
      "Iteration: 6263 lambda_n: 1.013539841459179 Loss: 1.439790974387646e-07\n",
      "Iteration: 6264 lambda_n: 1.0209679576683703 Loss: 1.432745680659134e-07\n",
      "Iteration: 6265 lambda_n: 0.9294149184943766 Loss: 1.4256835778250212e-07\n",
      "Iteration: 6266 lambda_n: 0.9473806004140418 Loss: 1.4192865305971684e-07\n",
      "Iteration: 6267 lambda_n: 1.013112928629068 Loss: 1.4127951699277158e-07\n",
      "Iteration: 6268 lambda_n: 0.9106827387242229 Loss: 1.4058852589977492e-07\n",
      "Iteration: 6269 lambda_n: 0.9340512266239998 Loss: 1.3997044366384348e-07\n",
      "Iteration: 6270 lambda_n: 1.0020508659531486 Loss: 1.3933929638833704e-07\n",
      "Iteration: 6271 lambda_n: 0.9369998725759267 Loss: 1.3866526325837465e-07\n",
      "Iteration: 6272 lambda_n: 0.8747682221071356 Loss: 1.3803804471147976e-07\n",
      "Iteration: 6273 lambda_n: 0.9643418577662147 Loss: 1.3745513986843743e-07\n",
      "Iteration: 6274 lambda_n: 0.8812314446046658 Loss: 1.3681526906707616e-07\n",
      "Iteration: 6275 lambda_n: 1.0247187520929006 Loss: 1.3623327468831672e-07\n",
      "Iteration: 6276 lambda_n: 1.0224712921979575 Loss: 1.3555940429251959e-07\n",
      "Iteration: 6277 lambda_n: 0.8927154524590238 Loss: 1.3489034792427587e-07\n",
      "Iteration: 6278 lambda_n: 0.9112780788657264 Loss: 1.343090893921793e-07\n",
      "Iteration: 6279 lambda_n: 1.0228293674697944 Loss: 1.3371830921809276e-07\n",
      "Iteration: 6280 lambda_n: 0.9324469957279012 Loss: 1.3305813646961226e-07\n",
      "Iteration: 6281 lambda_n: 0.9524858053710378 Loss: 1.3245928040953854e-07\n",
      "Iteration: 6282 lambda_n: 0.9974844057398785 Loss: 1.318503165053098e-07\n",
      "Iteration: 6283 lambda_n: 1.0121286663731912 Loss: 1.3121552436969843e-07\n",
      "Iteration: 6284 lambda_n: 0.9543652144777518 Loss: 1.305745237369832e-07\n",
      "Iteration: 6285 lambda_n: 0.9879677320108037 Loss: 1.2997306788477602e-07\n",
      "Iteration: 6286 lambda_n: 0.9618279902824951 Loss: 1.2935331252066745e-07\n",
      "Iteration: 6287 lambda_n: 0.9876958404033096 Loss: 1.2875284107954698e-07\n",
      "Iteration: 6288 lambda_n: 0.9377528044213727 Loss: 1.2813909211872074e-07\n",
      "Iteration: 6289 lambda_n: 0.9240978495405358 Loss: 1.2755916437541305e-07\n",
      "Iteration: 6290 lambda_n: 0.950287763088599 Loss: 1.2699027614911573e-07\n",
      "Iteration: 6291 lambda_n: 0.9367331465028869 Loss: 1.2640788283832113e-07\n",
      "Iteration: 6292 lambda_n: 0.9784884716549838 Loss: 1.258364383107464e-07\n",
      "Iteration: 6293 lambda_n: 0.896458543552162 Loss: 1.2524222904219448e-07\n",
      "Iteration: 6294 lambda_n: 1.0204868448591904 Loss: 1.2470041367729907e-07\n",
      "Iteration: 6295 lambda_n: 0.8962204364511303 Loss: 1.2408631378694301e-07\n",
      "Iteration: 6296 lambda_n: 0.885183454028316 Loss: 1.2354965888556254e-07\n",
      "Iteration: 6297 lambda_n: 1.0026591550922288 Loss: 1.23021913266695e-07\n",
      "Iteration: 6298 lambda_n: 0.9455505445899979 Loss: 1.224266912938448e-07\n",
      "Iteration: 6299 lambda_n: 0.8745777668513386 Loss: 1.21868096850859e-07\n",
      "Iteration: 6300 lambda_n: 0.8786976453730923 Loss: 1.2135379606266703e-07\n",
      "Iteration: 6301 lambda_n: 0.9969245104610179 Loss: 1.2083926102273855e-07\n",
      "Iteration: 6302 lambda_n: 0.9073864628507622 Loss: 1.2025798061297874e-07\n",
      "Iteration: 6303 lambda_n: 0.8843234582285364 Loss: 1.1973146164669706e-07\n",
      "Iteration: 6304 lambda_n: 0.9529510567121279 Loss: 1.1922058000780146e-07\n",
      "Iteration: 6305 lambda_n: 0.9135484114211555 Loss: 1.1867240935151677e-07\n",
      "Iteration: 6306 lambda_n: 0.9918982971311849 Loss: 1.1814932961538146e-07\n",
      "Iteration: 6307 lambda_n: 0.9971530272493629 Loss: 1.1758390105112504e-07\n",
      "Iteration: 6308 lambda_n: 0.9803766478130234 Loss: 1.1701820756952811e-07\n",
      "Iteration: 6309 lambda_n: 0.9995154475099154 Loss: 1.1646471729729824e-07\n",
      "Iteration: 6310 lambda_n: 0.9339278386746409 Loss: 1.1590310112635942e-07\n",
      "Iteration: 6311 lambda_n: 1.019563723705941 Loss: 1.153808779914658e-07\n",
      "Iteration: 6312 lambda_n: 0.9782988980663848 Loss: 1.1481334871010381e-07\n",
      "Iteration: 6313 lambda_n: 0.8864988643406186 Loss: 1.1427147794999305e-07\n",
      "Iteration: 6314 lambda_n: 0.9802525141322207 Loss: 1.1378278077682112e-07\n",
      "Iteration: 6315 lambda_n: 0.9323885025176145 Loss: 1.1324472056134799e-07\n",
      "Iteration: 6316 lambda_n: 0.9872118373325585 Loss: 1.1273536259028221e-07\n",
      "Iteration: 6317 lambda_n: 0.8964985660715061 Loss: 1.1219849044567936e-07\n",
      "Iteration: 6318 lambda_n: 0.9052919502348876 Loss: 1.1171328160003243e-07\n",
      "Iteration: 6319 lambda_n: 0.9112668089818966 Loss: 1.1122544102048078e-07\n",
      "Iteration: 6320 lambda_n: 0.9492593593955053 Loss: 1.1073653386996836e-07\n",
      "Iteration: 6321 lambda_n: 1.021318960305246 Loss: 1.1022949107941656e-07\n",
      "Iteration: 6322 lambda_n: 0.9104583280892885 Loss: 1.0968646627508878e-07\n",
      "Iteration: 6323 lambda_n: 1.0008109712000195 Loss: 1.092047794207967e-07\n",
      "Iteration: 6324 lambda_n: 1.011721863506915 Loss: 1.0867762571462844e-07\n",
      "Iteration: 6325 lambda_n: 0.957404768729734 Loss: 1.0814730822415651e-07\n",
      "Iteration: 6326 lambda_n: 0.9018912945975323 Loss: 1.0764792148597325e-07\n",
      "Iteration: 6327 lambda_n: 0.9997445816924229 Loss: 1.0717967233524986e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6328 lambda_n: 0.9476293260773494 Loss: 1.0666288677886795e-07\n",
      "Iteration: 6329 lambda_n: 1.0158994136635189 Loss: 1.0617541255673733e-07\n",
      "Iteration: 6330 lambda_n: 0.9301068310382633 Loss: 1.056552181009215e-07\n",
      "Iteration: 6331 lambda_n: 0.9479581372215762 Loss: 1.0518129753323292e-07\n",
      "Iteration: 6332 lambda_n: 0.8837528307417039 Loss: 1.0470045732020788e-07\n",
      "Iteration: 6333 lambda_n: 0.9603627632990964 Loss: 1.042542428180409e-07\n",
      "Iteration: 6334 lambda_n: 0.9982651691745644 Loss: 1.0377142318742041e-07\n",
      "Iteration: 6335 lambda_n: 0.9198006347597453 Loss: 1.0327188303848802e-07\n",
      "Iteration: 6336 lambda_n: 0.9231998782863776 Loss: 1.0281383284821929e-07\n",
      "Iteration: 6337 lambda_n: 0.8826005644774717 Loss: 1.023561383513756e-07\n",
      "Iteration: 6338 lambda_n: 0.9501085216953596 Loss: 1.0192052859583562e-07\n",
      "Iteration: 6339 lambda_n: 0.9040592028229555 Loss: 1.0145360512792932e-07\n",
      "Iteration: 6340 lambda_n: 0.9249529409118734 Loss: 1.0101135710658327e-07\n",
      "Iteration: 6341 lambda_n: 0.9803036391092771 Loss: 1.0056086992436214e-07\n",
      "Iteration: 6342 lambda_n: 0.8741380353911459 Loss: 1.0008556429522891e-07\n",
      "Iteration: 6343 lambda_n: 0.9423581151778926 Loss: 9.966374630965484e-08\n",
      "Iteration: 6344 lambda_n: 0.9095870469451235 Loss: 9.921093432583994e-08\n",
      "Iteration: 6345 lambda_n: 0.9335059410373696 Loss: 9.877586443875832e-08\n",
      "Iteration: 6346 lambda_n: 0.9233984650084328 Loss: 9.833132140321898e-08\n",
      "Iteration: 6347 lambda_n: 0.9440795299878577 Loss: 9.789358031850195e-08\n",
      "Iteration: 6348 lambda_n: 0.9466330340633201 Loss: 9.744803744517069e-08\n",
      "Iteration: 6349 lambda_n: 1.0259445089331742 Loss: 9.700333285997557e-08\n",
      "Iteration: 6350 lambda_n: 0.9249848548369342 Loss: 9.652358024738785e-08\n",
      "Iteration: 6351 lambda_n: 0.8752245824322107 Loss: 9.609318828857029e-08\n",
      "Iteration: 6352 lambda_n: 0.9294561570598044 Loss: 9.568777457151944e-08\n",
      "Iteration: 6353 lambda_n: 0.8871817403395738 Loss: 9.525906592117114e-08\n",
      "Iteration: 6354 lambda_n: 0.9368579486491291 Loss: 9.485169894447853e-08\n",
      "Iteration: 6355 lambda_n: 0.9334812896977175 Loss: 9.442337132789636e-08\n",
      "Iteration: 6356 lambda_n: 0.94364139649255 Loss: 9.399852478074055e-08\n",
      "Iteration: 6357 lambda_n: 0.9834540974122756 Loss: 9.357099663300741e-08\n",
      "Iteration: 6358 lambda_n: 0.9309920803611266 Loss: 9.312746813980337e-08\n",
      "Iteration: 6359 lambda_n: 1.0126381055020977 Loss: 9.270960018650611e-08\n",
      "Iteration: 6360 lambda_n: 0.9034663970214668 Loss: 9.225713655768766e-08\n",
      "Iteration: 6361 lambda_n: 0.8972305830140521 Loss: 9.185543327488863e-08\n",
      "Iteration: 6362 lambda_n: 0.9027272833414492 Loss: 9.14582490058791e-08\n",
      "Iteration: 6363 lambda_n: 0.9268143430541824 Loss: 9.106036885508893e-08\n",
      "Iteration: 6364 lambda_n: 0.9418417464954545 Loss: 9.065365917136998e-08\n",
      "Iteration: 6365 lambda_n: 0.9025468599373639 Loss: 9.024221127246408e-08\n",
      "Iteration: 6366 lambda_n: 0.9971863640506339 Loss: 8.984972895773987e-08\n",
      "Iteration: 6367 lambda_n: 0.9143652527942185 Loss: 8.94179883225876e-08\n",
      "Iteration: 6368 lambda_n: 0.9359961552226146 Loss: 8.902401868789501e-08\n",
      "Iteration: 6369 lambda_n: 0.9767408481326774 Loss: 8.862251601818657e-08\n",
      "Iteration: 6370 lambda_n: 0.9825810545619608 Loss: 8.820543609709667e-08\n",
      "Iteration: 6371 lambda_n: 0.9688184649753293 Loss: 8.778784834338817e-08\n",
      "Iteration: 6372 lambda_n: 0.9201833333970323 Loss: 8.737807013064084e-08\n",
      "Iteration: 6373 lambda_n: 0.9898728871314358 Loss: 8.699069024962456e-08\n",
      "Iteration: 6374 lambda_n: 1.0117612536011442 Loss: 8.657583084332865e-08\n",
      "Iteration: 6375 lambda_n: 0.8778432163482535 Loss: 8.61538321706889e-08\n",
      "Iteration: 6376 lambda_n: 0.9313664011193113 Loss: 8.578948494760903e-08\n",
      "Iteration: 6377 lambda_n: 0.8982247149991001 Loss: 8.540456771998859e-08\n",
      "Iteration: 6378 lambda_n: 0.9150807479944897 Loss: 8.503502296801582e-08\n",
      "Iteration: 6379 lambda_n: 0.9968659171779066 Loss: 8.466018234163555e-08\n",
      "Iteration: 6380 lambda_n: 0.9498727278663219 Loss: 8.425365156221664e-08\n",
      "Iteration: 6381 lambda_n: 0.9908812609471571 Loss: 8.386815654780497e-08\n",
      "Iteration: 6382 lambda_n: 0.9741398298357904 Loss: 8.346787014215746e-08\n",
      "Iteration: 6383 lambda_n: 0.8783786914392167 Loss: 8.307623671981629e-08\n",
      "Iteration: 6384 lambda_n: 1.0024683883649645 Loss: 8.272476937890319e-08\n",
      "Iteration: 6385 lambda_n: 0.9309365895439071 Loss: 8.232535779036836e-08\n",
      "Iteration: 6386 lambda_n: 0.9360521152107909 Loss: 8.195624865862938e-08\n",
      "Iteration: 6387 lambda_n: 0.9206047148017034 Loss: 8.158678599398959e-08\n",
      "Iteration: 6388 lambda_n: 0.904031665126889 Loss: 8.122506913732216e-08\n",
      "Iteration: 6389 lambda_n: 0.903082374042939 Loss: 8.087144910887745e-08\n",
      "Iteration: 6390 lambda_n: 0.8822383139546747 Loss: 8.051974841263012e-08\n",
      "Iteration: 6391 lambda_n: 0.9806986504684873 Loss: 8.017766939170363e-08\n",
      "Iteration: 6392 lambda_n: 1.0042653976580107 Loss: 7.979903978259617e-08\n",
      "Iteration: 6393 lambda_n: 1.0092921043134289 Loss: 7.941315481579797e-08\n",
      "Iteration: 6394 lambda_n: 0.974112598585495 Loss: 7.90272264112452e-08\n",
      "Iteration: 6395 lambda_n: 0.904879321438229 Loss: 7.865657218244708e-08\n",
      "Iteration: 6396 lambda_n: 0.8967965266568259 Loss: 7.831388739459543e-08\n",
      "Iteration: 6397 lambda_n: 0.9645723948106578 Loss: 7.797575347683681e-08\n",
      "Iteration: 6398 lambda_n: 0.9329164574136949 Loss: 7.761364623464011e-08\n",
      "Iteration: 6399 lambda_n: 0.9172214793371893 Loss: 7.726506056759731e-08\n",
      "Iteration: 6400 lambda_n: 0.8908985928028732 Loss: 7.692388945080477e-08\n",
      "Iteration: 6401 lambda_n: 0.9624845932879296 Loss: 7.659398302715021e-08\n",
      "Iteration: 6402 lambda_n: 0.8849904071068795 Loss: 7.623910739501526e-08\n",
      "Iteration: 6403 lambda_n: 0.8983707947848231 Loss: 7.591432704595204e-08\n",
      "Iteration: 6404 lambda_n: 0.97927843563254 Loss: 7.558605096293913e-08\n",
      "Iteration: 6405 lambda_n: 0.9034241072427022 Loss: 7.522976904194163e-08\n",
      "Iteration: 6406 lambda_n: 1.0124320599171754 Loss: 7.49026450543069e-08\n",
      "Iteration: 6407 lambda_n: 0.9567194352985791 Loss: 7.453765606047932e-08\n",
      "Iteration: 6408 lambda_n: 0.9865679055749249 Loss: 7.41944449571521e-08\n",
      "Iteration: 6409 lambda_n: 1.0044321277276393 Loss: 7.38421680152107e-08\n",
      "Iteration: 6410 lambda_n: 0.9162811513501333 Loss: 7.348522805130335e-08\n",
      "Iteration: 6411 lambda_n: 0.9055529861952177 Loss: 7.316119964884762e-08\n",
      "Iteration: 6412 lambda_n: 0.8889852881680791 Loss: 7.284238794697528e-08\n",
      "Iteration: 6413 lambda_n: 0.930544773312376 Loss: 7.253078348853895e-08\n",
      "Iteration: 6414 lambda_n: 0.9852885601018196 Loss: 7.220601794006287e-08\n",
      "Iteration: 6415 lambda_n: 0.9359556365162498 Loss: 7.186369836753219e-08\n",
      "Iteration: 6416 lambda_n: 0.8837411315097194 Loss: 7.15400722639603e-08\n",
      "Iteration: 6417 lambda_n: 0.8957321875658262 Loss: 7.123588734409794e-08\n",
      "Iteration: 6418 lambda_n: 1.017765366053251 Loss: 7.092889651073783e-08\n",
      "Iteration: 6419 lambda_n: 0.9753377636786635 Loss: 7.058159726589329e-08\n",
      "Iteration: 6420 lambda_n: 1.0073270929987377 Loss: 7.025041867175108e-08\n",
      "Iteration: 6421 lambda_n: 0.9183554584502029 Loss: 6.990999608422176e-08\n",
      "Iteration: 6422 lambda_n: 0.9912385269285218 Loss: 6.960115726964401e-08\n",
      "Iteration: 6423 lambda_n: 0.9363907457720858 Loss: 6.926929318212501e-08\n",
      "Iteration: 6424 lambda_n: 0.8955625479890277 Loss: 6.895729915558019e-08\n",
      "Iteration: 6425 lambda_n: 0.8753751627019754 Loss: 6.866026378387948e-08\n",
      "Iteration: 6426 lambda_n: 0.9450178636297143 Loss: 6.837118525282313e-08\n",
      "Iteration: 6427 lambda_n: 0.9685128107604832 Loss: 6.806043359120916e-08\n",
      "Iteration: 6428 lambda_n: 0.9899149613647513 Loss: 6.774341601406076e-08\n",
      "Iteration: 6429 lambda_n: 0.8989264957273704 Loss: 6.742091534185149e-08\n",
      "Iteration: 6430 lambda_n: 0.9793477086459484 Loss: 6.712946360021032e-08\n",
      "Iteration: 6431 lambda_n: 0.8743809993717411 Loss: 6.681332234996416e-08\n",
      "Iteration: 6432 lambda_n: 1.0170510662441288 Loss: 6.653240601024172e-08\n",
      "Iteration: 6433 lambda_n: 0.9899508079831988 Loss: 6.620703972941399e-08\n",
      "Iteration: 6434 lambda_n: 0.9315808467805625 Loss: 6.58919056890705e-08\n",
      "Iteration: 6435 lambda_n: 0.9834390048776223 Loss: 6.559677690194821e-08\n",
      "Iteration: 6436 lambda_n: 0.9975494098011504 Loss: 6.52866274650369e-08\n",
      "Iteration: 6437 lambda_n: 0.9171946809476618 Loss: 6.497352908726693e-08\n",
      "Iteration: 6438 lambda_n: 0.9330043511004062 Loss: 6.468704461515776e-08\n",
      "Iteration: 6439 lambda_n: 0.9075313700931087 Loss: 6.43969189151832e-08\n",
      "Iteration: 6440 lambda_n: 0.9470201761628406 Loss: 6.411599175305849e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6441 lambda_n: 1.018076258770857 Loss: 6.382413175080852e-08\n",
      "Iteration: 6442 lambda_n: 1.0019917755777963 Loss: 6.351181506435245e-08\n",
      "Iteration: 6443 lambda_n: 0.9272212003568036 Loss: 6.320595111866725e-08\n",
      "Iteration: 6444 lambda_n: 0.9411651891444567 Loss: 6.292428736868333e-08\n",
      "Iteration: 6445 lambda_n: 0.9298357274172808 Loss: 6.263967424464993e-08\n",
      "Iteration: 6446 lambda_n: 0.8842081652185081 Loss: 6.235977143536703e-08\n",
      "Iteration: 6447 lambda_n: 0.8749865427140976 Loss: 6.209480455775328e-08\n",
      "Iteration: 6448 lambda_n: 0.9619259989830956 Loss: 6.183372620475143e-08\n",
      "Iteration: 6449 lambda_n: 0.9504745352614427 Loss: 6.154792584026833e-08\n",
      "Iteration: 6450 lambda_n: 0.9232373103075304 Loss: 6.126684617678106e-08\n",
      "Iteration: 6451 lambda_n: 0.8878205895442021 Loss: 6.099508064693901e-08\n",
      "Iteration: 6452 lambda_n: 0.9783646754798228 Loss: 6.073491141400465e-08\n",
      "Iteration: 6453 lambda_n: 1.0041462935702754 Loss: 6.044944452376846e-08\n",
      "Iteration: 6454 lambda_n: 0.8997449486086981 Loss: 6.015784644837109e-08\n",
      "Iteration: 6455 lambda_n: 0.8768684873693984 Loss: 5.989783914729712e-08\n",
      "Iteration: 6456 lambda_n: 0.9678087717912203 Loss: 5.96455492734184e-08\n",
      "Iteration: 6457 lambda_n: 0.9846571503186989 Loss: 5.936827976050082e-08\n",
      "Iteration: 6458 lambda_n: 0.9711077506575784 Loss: 5.908750862324101e-08\n",
      "Iteration: 6459 lambda_n: 0.8800504289956976 Loss: 5.8811924578203834e-08\n",
      "Iteration: 6460 lambda_n: 0.9409551263186469 Loss: 5.8563358227458265e-08\n",
      "Iteration: 6461 lambda_n: 0.9008116570331024 Loss: 5.829872516883663e-08\n",
      "Iteration: 6462 lambda_n: 0.9865083037395139 Loss: 5.804653921081782e-08\n",
      "Iteration: 6463 lambda_n: 0.9343106692845085 Loss: 5.7771570128365055e-08\n",
      "Iteration: 6464 lambda_n: 0.8833431675574144 Loss: 5.751239722746155e-08\n",
      "Iteration: 6465 lambda_n: 0.9808830049366046 Loss: 5.726847384943585e-08\n",
      "Iteration: 6466 lambda_n: 0.9044384406273125 Loss: 5.6998778059256905e-08\n",
      "Iteration: 6467 lambda_n: 0.9782873903487259 Loss: 5.675128503896956e-08\n",
      "Iteration: 6468 lambda_n: 0.9835398515885375 Loss: 5.648475955631581e-08\n",
      "Iteration: 6469 lambda_n: 0.903030837224113 Loss: 5.6218075944851e-08\n",
      "Iteration: 6470 lambda_n: 0.9380758016751453 Loss: 5.59743912922528e-08\n",
      "Iteration: 6471 lambda_n: 0.9261279318601598 Loss: 5.5722359795245154e-08\n",
      "Iteration: 6472 lambda_n: 0.8752979358540369 Loss: 5.547467175623122e-08\n",
      "Iteration: 6473 lambda_n: 0.9600239245589118 Loss: 5.5241630646262985e-08\n",
      "Iteration: 6474 lambda_n: 1.0050266764403784 Loss: 5.498711859145723e-08\n",
      "Iteration: 6475 lambda_n: 1.0216131188258215 Loss: 5.4721918208316886e-08\n",
      "Iteration: 6476 lambda_n: 0.9637715534177872 Loss: 5.4453656934921975e-08\n",
      "Iteration: 6477 lambda_n: 0.9868777777247338 Loss: 5.4201839599423435e-08\n",
      "Iteration: 6478 lambda_n: 0.9189337817179142 Loss: 5.3945192049377765e-08\n",
      "Iteration: 6479 lambda_n: 0.9977985378546715 Loss: 5.370735937703436e-08\n",
      "Iteration: 6480 lambda_n: 0.8926866161176057 Loss: 5.345026827901275e-08\n",
      "Iteration: 6481 lambda_n: 0.8758669677312723 Loss: 5.322137468072584e-08\n",
      "Iteration: 6482 lambda_n: 1.0249953217791494 Loss: 5.299776762115738e-08\n",
      "Iteration: 6483 lambda_n: 0.9441768457583457 Loss: 5.2737202121183425e-08\n",
      "Iteration: 6484 lambda_n: 0.9528513596127094 Loss: 5.249837652869991e-08\n",
      "Iteration: 6485 lambda_n: 0.8765708225152625 Loss: 5.225846227726272e-08\n",
      "Iteration: 6486 lambda_n: 0.9020604552537362 Loss: 5.20387758962038e-08\n",
      "Iteration: 6487 lambda_n: 1.003232584218244 Loss: 5.181366410435625e-08\n",
      "Iteration: 6488 lambda_n: 0.9390514712499028 Loss: 5.1564402005144845e-08\n",
      "Iteration: 6489 lambda_n: 1.016106888619808 Loss: 5.1332223368435146e-08\n",
      "Iteration: 6490 lambda_n: 1.0240956509232975 Loss: 5.1082139399614106e-08\n",
      "Iteration: 6491 lambda_n: 0.9396288246609262 Loss: 5.083133366953479e-08\n",
      "Iteration: 6492 lambda_n: 0.9673064980929178 Loss: 5.06023591695079e-08\n",
      "Iteration: 6493 lambda_n: 0.9319975401118892 Loss: 5.036771635450387e-08\n",
      "Iteration: 6494 lambda_n: 0.920327614132452 Loss: 5.014270115040654e-08\n",
      "Iteration: 6495 lambda_n: 0.9360258872671027 Loss: 4.992150979360812e-08\n",
      "Iteration: 6496 lambda_n: 1.0250040452857765 Loss: 4.969755173317872e-08\n",
      "Iteration: 6497 lambda_n: 0.9014052152407989 Loss: 4.945342018195937e-08\n",
      "Iteration: 6498 lambda_n: 0.9843721125094274 Loss: 4.923979617013081e-08\n",
      "Iteration: 6499 lambda_n: 1.0056435622635598 Loss: 4.9007532090291205e-08\n",
      "Iteration: 6500 lambda_n: 1.00783609323419 Loss: 4.877138430429881e-08\n",
      "Iteration: 6501 lambda_n: 1.00640540838603 Loss: 4.853587848836457e-08\n",
      "Iteration: 6502 lambda_n: 0.926394602523932 Loss: 4.8301859060674084e-08\n",
      "Iteration: 6503 lambda_n: 0.91259334363688 Loss: 4.808749818428443e-08\n",
      "Iteration: 6504 lambda_n: 0.892122502996246 Loss: 4.787728175639745e-08\n",
      "Iteration: 6505 lambda_n: 0.8872423164096488 Loss: 4.767269243451925e-08\n",
      "Iteration: 6506 lambda_n: 1.0234275513380962 Loss: 4.7470104724800694e-08\n",
      "Iteration: 6507 lambda_n: 0.9475190305508265 Loss: 4.723742963949404e-08\n",
      "Iteration: 6508 lambda_n: 0.9850526555023507 Loss: 4.7023083946193864e-08\n",
      "Iteration: 6509 lambda_n: 0.9254968267148541 Loss: 4.6801274161792866e-08\n",
      "Iteration: 6510 lambda_n: 1.0151869946144068 Loss: 4.6593872881903215e-08\n",
      "Iteration: 6511 lambda_n: 0.9043340836021928 Loss: 4.6367396320808906e-08\n",
      "Iteration: 6512 lambda_n: 0.9917705620181143 Loss: 4.616664540954992e-08\n",
      "Iteration: 6513 lambda_n: 0.915266156384828 Loss: 4.594745310382195e-08\n",
      "Iteration: 6514 lambda_n: 0.9671424172706906 Loss: 4.574614452632843e-08\n",
      "Iteration: 6515 lambda_n: 1.0220771862934377 Loss: 4.5534372990824124e-08\n",
      "Iteration: 6516 lambda_n: 1.0151872322244673 Loss: 4.5311625398104756e-08\n",
      "Iteration: 6517 lambda_n: 0.9425393730428209 Loss: 4.509147916330941e-08\n",
      "Iteration: 6518 lambda_n: 0.9493765634097558 Loss: 4.4888095840336264e-08\n",
      "Iteration: 6519 lambda_n: 0.9309825351564696 Loss: 4.4684176366650184e-08\n",
      "Iteration: 6520 lambda_n: 0.9139003112000159 Loss: 4.448513119440432e-08\n",
      "Iteration: 6521 lambda_n: 1.0114293715337268 Loss: 4.429062303401718e-08\n",
      "Iteration: 6522 lambda_n: 0.8810289527590461 Loss: 4.4076314761854975e-08\n",
      "Iteration: 6523 lambda_n: 0.9028468065122203 Loss: 4.3890554788559486e-08\n",
      "Iteration: 6524 lambda_n: 0.8868860438193785 Loss: 4.370101061178674e-08\n",
      "Iteration: 6525 lambda_n: 0.8957725395570567 Loss: 4.3515635063179774e-08\n",
      "Iteration: 6526 lambda_n: 0.9843851119626502 Loss: 4.332920999149269e-08\n",
      "Iteration: 6527 lambda_n: 0.8966999880261721 Loss: 4.312523631987368e-08\n",
      "Iteration: 6528 lambda_n: 0.8956865366860609 Loss: 4.294032155208528e-08\n",
      "Iteration: 6529 lambda_n: 0.9266107238461242 Loss: 4.2756421688184783e-08\n",
      "Iteration: 6530 lambda_n: 0.9859402544832808 Loss: 4.256700185120157e-08\n",
      "Iteration: 6531 lambda_n: 0.9643733170392711 Loss: 4.2366362714335336e-08\n",
      "Iteration: 6532 lambda_n: 0.9226569357328699 Loss: 4.217105403138948e-08\n",
      "Iteration: 6533 lambda_n: 0.8807842362205331 Loss: 4.1985070803976275e-08\n",
      "Iteration: 6534 lambda_n: 0.8938586700181707 Loss: 4.1808325161614436e-08\n",
      "Iteration: 6535 lambda_n: 0.8822841785895147 Loss: 4.1629724874498614e-08\n",
      "Iteration: 6536 lambda_n: 0.9268551842137179 Loss: 4.145420421406961e-08\n",
      "Iteration: 6537 lambda_n: 0.9883573164617273 Loss: 4.1270608627896564e-08\n",
      "Iteration: 6538 lambda_n: 1.0184509488537794 Loss: 4.107571390301783e-08\n",
      "Iteration: 6539 lambda_n: 0.8767110207376166 Loss: 4.0875851339446644e-08\n",
      "Iteration: 6540 lambda_n: 1.00354963639635 Loss: 4.070465671520824e-08\n",
      "Iteration: 6541 lambda_n: 0.9642635826595296 Loss: 4.050953121138494e-08\n",
      "Iteration: 6542 lambda_n: 0.8878617865370635 Loss: 4.0322960243139005e-08\n",
      "Iteration: 6543 lambda_n: 0.8882595725008438 Loss: 4.015197824067799e-08\n",
      "Iteration: 6544 lambda_n: 0.9417030285180714 Loss: 3.998165914409757e-08\n",
      "Iteration: 6545 lambda_n: 0.9648142021811549 Loss: 3.980187368318824e-08\n",
      "Iteration: 6546 lambda_n: 0.9954098444700383 Loss: 3.961852069047377e-08\n",
      "Iteration: 6547 lambda_n: 0.9022852080937597 Loss: 3.9430242182159604e-08\n",
      "Iteration: 6548 lambda_n: 0.9573040781872321 Loss: 3.9260404962388616e-08\n",
      "Iteration: 6549 lambda_n: 0.9526577533814715 Loss: 3.908100353333499e-08\n",
      "Iteration: 6550 lambda_n: 1.0135148935523461 Loss: 3.890330522231482e-08\n",
      "Iteration: 6551 lambda_n: 0.9748877025651692 Loss: 3.8715132696076094e-08\n",
      "Iteration: 6552 lambda_n: 0.9352891013513939 Loss: 3.853502527235081e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6553 lambda_n: 0.9045621659532299 Loss: 3.83630540156091e-08\n",
      "Iteration: 6554 lambda_n: 0.9425272745406642 Loss: 3.819749022584561e-08\n",
      "Iteration: 6555 lambda_n: 0.9740344682656071 Loss: 3.8025737923306965e-08\n",
      "Iteration: 6556 lambda_n: 0.9482396365130054 Loss: 3.7849059345978064e-08\n",
      "Iteration: 6557 lambda_n: 1.0012456711127797 Loss: 3.7677875822935884e-08\n",
      "Iteration: 6558 lambda_n: 0.9326444187332827 Loss: 3.749795852859695e-08\n",
      "Iteration: 6559 lambda_n: 0.9777618576347795 Loss: 3.7331185854633335e-08\n",
      "Iteration: 6560 lambda_n: 0.9578508587569465 Loss: 3.715714014681476e-08\n",
      "Iteration: 6561 lambda_n: 0.9009411575631291 Loss: 3.698745102567565e-08\n",
      "Iteration: 6562 lambda_n: 0.9029580937202772 Loss: 3.682858868338056e-08\n",
      "Iteration: 6563 lambda_n: 0.9733053335335691 Loss: 3.66700698087807e-08\n",
      "Iteration: 6564 lambda_n: 0.9742875891520159 Loss: 3.6499953313321985e-08\n",
      "Iteration: 6565 lambda_n: 0.8980961706434457 Loss: 3.633047299222373e-08\n",
      "Iteration: 6566 lambda_n: 0.8763690938578845 Loss: 3.617498811626336e-08\n",
      "Iteration: 6567 lambda_n: 0.8773206744600102 Loss: 3.6023928954037695e-08\n",
      "Iteration: 6568 lambda_n: 0.9119969576052142 Loss: 3.587335183755527e-08\n",
      "Iteration: 6569 lambda_n: 1.0041671804456787 Loss: 3.571749269873958e-08\n",
      "Iteration: 6570 lambda_n: 0.9924337779189322 Loss: 3.554664512069159e-08\n",
      "Iteration: 6571 lambda_n: 0.986153821338919 Loss: 3.537862049480493e-08\n",
      "Iteration: 6572 lambda_n: 0.9420398204871376 Loss: 3.52124670091309e-08\n",
      "Iteration: 6573 lambda_n: 1.0149607798137916 Loss: 3.505450922833942e-08\n",
      "Iteration: 6574 lambda_n: 0.9830433943808021 Loss: 3.4885106365332747e-08\n",
      "Iteration: 6575 lambda_n: 0.9780119761626709 Loss: 3.4721842742941274e-08\n",
      "Iteration: 6576 lambda_n: 0.8757930663168232 Loss: 3.456019342935345e-08\n",
      "Iteration: 6577 lambda_n: 1.0255153293202424 Loss: 3.441612941917966e-08\n",
      "Iteration: 6578 lambda_n: 0.9449845515702314 Loss: 3.4248157869592807e-08\n",
      "Iteration: 6579 lambda_n: 0.9545128002166574 Loss: 3.4094150661483494e-08\n",
      "Iteration: 6580 lambda_n: 0.9628054775519341 Loss: 3.393930771740395e-08\n",
      "Iteration: 6581 lambda_n: 0.8898922252811923 Loss: 3.3783846833252857e-08\n",
      "Iteration: 6582 lambda_n: 1.0186536550615601 Loss: 3.3640833713594275e-08\n",
      "Iteration: 6583 lambda_n: 0.9511278140426888 Loss: 3.3477838743887414e-08\n",
      "Iteration: 6584 lambda_n: 0.9139618064656608 Loss: 3.332640480072394e-08\n",
      "Iteration: 6585 lambda_n: 0.9119145832778972 Loss: 3.318156348891578e-08\n",
      "Iteration: 6586 lambda_n: 0.909516581309195 Loss: 3.303769113601305e-08\n",
      "Iteration: 6587 lambda_n: 0.9878873441680247 Loss: 3.2894835679134746e-08\n",
      "Iteration: 6588 lambda_n: 0.9377146141432907 Loss: 3.274035971018605e-08\n",
      "Iteration: 6589 lambda_n: 1.010008147186428 Loss: 3.259443607687001e-08\n",
      "Iteration: 6590 lambda_n: 0.9526493290135178 Loss: 3.243798198775248e-08\n",
      "Iteration: 6591 lambda_n: 1.0209797328829466 Loss: 3.229114034293674e-08\n",
      "Iteration: 6592 lambda_n: 0.8933985289401178 Loss: 3.213449829951765e-08\n",
      "Iteration: 6593 lambda_n: 1.006891143263805 Loss: 3.1998112969035334e-08\n",
      "Iteration: 6594 lambda_n: 0.8999611766170628 Loss: 3.184507278161085e-08\n",
      "Iteration: 6595 lambda_n: 0.9544204276391766 Loss: 3.1708957294264784e-08\n",
      "Iteration: 6596 lambda_n: 0.9955420794278038 Loss: 3.156523954888463e-08\n",
      "Iteration: 6597 lambda_n: 0.9484650612721733 Loss: 3.1416028430175334e-08\n",
      "Iteration: 6598 lambda_n: 0.9980171358739631 Loss: 3.127456410967715e-08\n",
      "Iteration: 6599 lambda_n: 0.9624831187081887 Loss: 3.1126398705380654e-08\n",
      "Iteration: 6600 lambda_n: 0.9731006701753875 Loss: 3.098420502005644e-08\n",
      "Iteration: 6601 lambda_n: 0.9968914016557144 Loss: 3.084111857452237e-08\n",
      "Iteration: 6602 lambda_n: 0.9328268938960387 Loss: 3.069523071137193e-08\n",
      "Iteration: 6603 lambda_n: 0.9939567625412008 Loss: 3.05593827772883e-08\n",
      "Iteration: 6604 lambda_n: 0.9180931970126114 Loss: 3.041529229649839e-08\n",
      "Iteration: 6605 lambda_n: 0.9251129090149246 Loss: 3.0282845534018785e-08\n",
      "Iteration: 6606 lambda_n: 0.9857215215836798 Loss: 3.0149984750199813e-08\n",
      "Iteration: 6607 lambda_n: 1.0101194542488643 Loss: 3.000905970758895e-08\n",
      "Iteration: 6608 lambda_n: 1.007592313408027 Loss: 2.986534223274775e-08\n",
      "Iteration: 6609 lambda_n: 1.0172477316345083 Loss: 2.972269192601178e-08\n",
      "Iteration: 6610 lambda_n: 0.9956592166286538 Loss: 2.957938382587862e-08\n",
      "Iteration: 6611 lambda_n: 0.9209714788461222 Loss: 2.943981435236597e-08\n",
      "Iteration: 6612 lambda_n: 1.0225553236651146 Loss: 2.9311342461168253e-08\n",
      "Iteration: 6613 lambda_n: 0.9311570472387756 Loss: 2.916934250055046e-08\n",
      "Iteration: 6614 lambda_n: 0.8788445059526904 Loss: 2.9040680867419746e-08\n",
      "Iteration: 6615 lambda_n: 0.9671545205310599 Loss: 2.8919800067739706e-08\n",
      "Iteration: 6616 lambda_n: 0.9798974790906007 Loss: 2.8787344517363127e-08\n",
      "Iteration: 6617 lambda_n: 1.025345230363823 Loss: 2.8653778380760975e-08\n",
      "Iteration: 6618 lambda_n: 0.9055516307515739 Loss: 2.8514687212831914e-08\n",
      "Iteration: 6619 lambda_n: 0.9346002790048535 Loss: 2.8392461939003006e-08\n",
      "Iteration: 6620 lambda_n: 1.0000602441641764 Loss: 2.8266874568375413e-08\n",
      "Iteration: 6621 lambda_n: 0.9982456826764703 Loss: 2.8133105431071632e-08\n",
      "Iteration: 6622 lambda_n: 1.0010465672884086 Loss: 2.800023209090171e-08\n",
      "Iteration: 6623 lambda_n: 1.002907024287515 Loss: 2.7867636516234978e-08\n",
      "Iteration: 6624 lambda_n: 0.9120202430886594 Loss: 2.773544500905115e-08\n",
      "Iteration: 6625 lambda_n: 0.973575449781839 Loss: 2.761582260507951e-08\n",
      "Iteration: 6626 lambda_n: 1.0244597712885655 Loss: 2.7488696449288782e-08\n",
      "Iteration: 6627 lambda_n: 0.9642344152744236 Loss: 2.735556335811779e-08\n",
      "Iteration: 6628 lambda_n: 0.9404056491577149 Loss: 2.723088471807083e-08\n",
      "Iteration: 6629 lambda_n: 0.9614750995790775 Loss: 2.7109860839727835e-08\n",
      "Iteration: 6630 lambda_n: 0.8770367159746045 Loss: 2.6986694951449617e-08\n",
      "Iteration: 6631 lambda_n: 0.91518486314815 Loss: 2.6874874080881016e-08\n",
      "Iteration: 6632 lambda_n: 1.0125785250577086 Loss: 2.675869034208913e-08\n",
      "Iteration: 6633 lambda_n: 0.9120735437662362 Loss: 2.6630718538829956e-08\n",
      "Iteration: 6634 lambda_n: 1.023076002316897 Loss: 2.651601975577906e-08\n",
      "Iteration: 6635 lambda_n: 0.8952594032531517 Loss: 2.638793656131783e-08\n",
      "Iteration: 6636 lambda_n: 0.9714069626756714 Loss: 2.6276416223534318e-08\n",
      "Iteration: 6637 lambda_n: 0.9370996542552321 Loss: 2.6155941004260543e-08\n",
      "Iteration: 6638 lambda_n: 0.9082905124595911 Loss: 2.6040273264371342e-08\n",
      "Iteration: 6639 lambda_n: 0.9711559694933469 Loss: 2.592867581023982e-08\n",
      "Iteration: 6640 lambda_n: 1.0048155399872887 Loss: 2.5809885301268977e-08\n",
      "Iteration: 6641 lambda_n: 1.017491088347757 Loss: 2.5687562258637005e-08\n",
      "Iteration: 6642 lambda_n: 0.9098415037813511 Loss: 2.5564305747781995e-08\n",
      "Iteration: 6643 lambda_n: 0.8908432977357907 Loss: 2.5454638564907236e-08\n",
      "Iteration: 6644 lambda_n: 0.9824086926301433 Loss: 2.534773980020308e-08\n",
      "Iteration: 6645 lambda_n: 0.9236278330339337 Loss: 2.523036819473444e-08\n",
      "Iteration: 6646 lambda_n: 0.9986056529443657 Loss: 2.5120550215351035e-08\n",
      "Iteration: 6647 lambda_n: 0.9756720364561376 Loss: 2.5002355027807273e-08\n",
      "Iteration: 6648 lambda_n: 1.0119437984190534 Loss: 2.48874392017663e-08\n",
      "Iteration: 6649 lambda_n: 0.8954454069768311 Loss: 2.476882119227825e-08\n",
      "Iteration: 6650 lambda_n: 0.8994887056526183 Loss: 2.4664379005786674e-08\n",
      "Iteration: 6651 lambda_n: 0.8986090489276365 Loss: 2.4559925614610262e-08\n",
      "Iteration: 6652 lambda_n: 0.913144950191154 Loss: 2.4456034402147177e-08\n",
      "Iteration: 6653 lambda_n: 1.0125024357210661 Loss: 2.4350927664691232e-08\n",
      "Iteration: 6654 lambda_n: 0.9930567091900514 Loss: 2.4234906487426805e-08\n",
      "Iteration: 6655 lambda_n: 0.896355843958783 Loss: 2.412167827774324e-08\n",
      "Iteration: 6656 lambda_n: 0.9906949573573078 Loss: 2.4019973129542278e-08\n",
      "Iteration: 6657 lambda_n: 0.9153310001473294 Loss: 2.3908058111105494e-08\n",
      "Iteration: 6658 lambda_n: 0.9359314374951643 Loss: 2.380515868415622e-08\n",
      "Iteration: 6659 lambda_n: 0.9164935179640477 Loss: 2.370041571030955e-08\n",
      "Iteration: 6660 lambda_n: 0.9326057559793487 Loss: 2.359831877755618e-08\n",
      "Iteration: 6661 lambda_n: 0.9195026612088416 Loss: 2.3494893950555715e-08\n",
      "Iteration: 6662 lambda_n: 1.0016978350301144 Loss: 2.3393388608635803e-08\n",
      "Iteration: 6663 lambda_n: 1.003987357843855 Loss: 2.328330863615071e-08\n",
      "Iteration: 6664 lambda_n: 0.9911772676396369 Loss: 2.31735191721505e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6665 lambda_n: 0.8834421134545458 Loss: 2.3065664324425105e-08\n",
      "Iteration: 6666 lambda_n: 0.9704867144386278 Loss: 2.2969999757124413e-08\n",
      "Iteration: 6667 lambda_n: 0.9105002310437408 Loss: 2.286536528519696e-08\n",
      "Iteration: 6668 lambda_n: 0.9648131968896022 Loss: 2.276766559840323e-08\n",
      "Iteration: 6669 lambda_n: 0.9310918483690211 Loss: 2.2664600677596665e-08\n",
      "Iteration: 6670 lambda_n: 0.9081578637107821 Loss: 2.256560881021744e-08\n",
      "Iteration: 6671 lambda_n: 0.9354605370129393 Loss: 2.246949635891031e-08\n",
      "Iteration: 6672 lambda_n: 0.9420741076572604 Loss: 2.23709357683709e-08\n",
      "Iteration: 6673 lambda_n: 0.9316896025931601 Loss: 2.2272134147056992e-08\n",
      "Iteration: 6674 lambda_n: 0.8809061720829346 Loss: 2.217487344316445e-08\n",
      "Iteration: 6675 lambda_n: 0.9538752353956025 Loss: 2.208333453251571e-08\n",
      "Iteration: 6676 lambda_n: 0.9567289466662126 Loss: 2.1984642020541736e-08\n",
      "Iteration: 6677 lambda_n: 0.9224216844859512 Loss: 2.1886117846008778e-08\n",
      "Iteration: 6678 lambda_n: 0.8837998938489672 Loss: 2.1791572763297735e-08\n",
      "Iteration: 6679 lambda_n: 1.0161828441798695 Loss: 2.170139646928757e-08\n",
      "Iteration: 6680 lambda_n: 1.0050613177091925 Loss: 2.1598163324962964e-08\n",
      "Iteration: 6681 lambda_n: 1.0094089418834657 Loss: 2.149656952683906e-08\n",
      "Iteration: 6682 lambda_n: 0.8973751752729039 Loss: 2.139503995581815e-08\n",
      "Iteration: 6683 lambda_n: 1.0088829782617108 Loss: 2.1305226252102506e-08\n",
      "Iteration: 6684 lambda_n: 0.9802330645612265 Loss: 2.1204697832254068e-08\n",
      "Iteration: 6685 lambda_n: 0.9055665614887215 Loss: 2.1107508190785276e-08\n",
      "Iteration: 6686 lambda_n: 0.9165486399655969 Loss: 2.1018153841836098e-08\n",
      "Iteration: 6687 lambda_n: 0.889085381958395 Loss: 2.0928118294513814e-08\n",
      "Iteration: 6688 lambda_n: 0.8936446537884773 Loss: 2.084117380793759e-08\n",
      "Iteration: 6689 lambda_n: 0.9526789817643386 Loss: 2.075416529769354e-08\n",
      "Iteration: 6690 lambda_n: 0.9711221882481513 Loss: 2.0661816559901422e-08\n",
      "Iteration: 6691 lambda_n: 1.0123947116893828 Loss: 2.056812083048739e-08\n",
      "Iteration: 6692 lambda_n: 0.8779252294286098 Loss: 2.047090945564087e-08\n",
      "Iteration: 6693 lambda_n: 0.9773308208845262 Loss: 2.038702907029629e-08\n",
      "Iteration: 6694 lambda_n: 0.8762450615753671 Loss: 2.029405445397578e-08\n",
      "Iteration: 6695 lambda_n: 1.0183595250598534 Loss: 2.0211096432888756e-08\n",
      "Iteration: 6696 lambda_n: 0.9500121485040105 Loss: 2.0115099700316918e-08\n",
      "Iteration: 6697 lambda_n: 1.0215654462281938 Loss: 2.002599398471624e-08\n",
      "Iteration: 6698 lambda_n: 0.8851007299829127 Loss: 1.9930624839845308e-08\n",
      "Iteration: 6699 lambda_n: 0.9543660646082952 Loss: 1.9848410139743226e-08\n",
      "Iteration: 6700 lambda_n: 0.9920666875766951 Loss: 1.9760147701675043e-08\n",
      "Iteration: 6701 lambda_n: 0.9140223936597694 Loss: 1.9668829394273926e-08\n",
      "Iteration: 6702 lambda_n: 1.0159627812441525 Loss: 1.9585105247237433e-08\n",
      "Iteration: 6703 lambda_n: 0.9626264226134558 Loss: 1.9492462198324567e-08\n",
      "Iteration: 6704 lambda_n: 0.9038142286915004 Loss: 1.9405121282955134e-08\n",
      "Iteration: 6705 lambda_n: 0.8935731655415736 Loss: 1.9323504678830067e-08\n",
      "Iteration: 6706 lambda_n: 0.8792839963059996 Loss: 1.924317166505282e-08\n",
      "Iteration: 6707 lambda_n: 0.9537907281672874 Loss: 1.9164470777634145e-08\n",
      "Iteration: 6708 lambda_n: 1.0230348614086684 Loss: 1.907947075078156e-08\n",
      "Iteration: 6709 lambda_n: 1.0129616734365705 Loss: 1.8988728027050316e-08\n",
      "Iteration: 6710 lambda_n: 0.8907067846964358 Loss: 1.8899331127882476e-08\n",
      "Iteration: 6711 lambda_n: 0.8948573655278808 Loss: 1.882111510013132e-08\n",
      "Iteration: 6712 lambda_n: 0.9151561108656878 Loss: 1.8742879134651877e-08\n",
      "Iteration: 6713 lambda_n: 1.022963760253092 Loss: 1.866322100354811e-08\n",
      "Iteration: 6714 lambda_n: 0.8963362041913746 Loss: 1.8574580503359415e-08\n",
      "Iteration: 6715 lambda_n: 0.947351775440895 Loss: 1.8497303117392477e-08\n",
      "Iteration: 6716 lambda_n: 0.9969516767882209 Loss: 1.8415988063733496e-08\n",
      "Iteration: 6717 lambda_n: 0.9815960595576187 Loss: 1.8330815007874787e-08\n",
      "Iteration: 6718 lambda_n: 0.9834050865225525 Loss: 1.8247365479355597e-08\n",
      "Iteration: 6719 lambda_n: 0.9755853288592143 Loss: 1.816416630264738e-08\n",
      "Iteration: 6720 lambda_n: 0.9288677376965224 Loss: 1.8082028434257495e-08\n",
      "Iteration: 6721 lambda_n: 0.9925463034415123 Loss: 1.8004199504335448e-08\n",
      "Iteration: 6722 lambda_n: 0.9856006171799117 Loss: 1.792141574654006e-08\n",
      "Iteration: 6723 lambda_n: 0.928416989453386 Loss: 1.7839613192093198e-08\n",
      "Iteration: 6724 lambda_n: 0.8762010377366027 Loss: 1.776293071408181e-08\n",
      "Iteration: 6725 lambda_n: 1.0069467344390564 Loss: 1.7690891864197082e-08\n",
      "Iteration: 6726 lambda_n: 0.8944143018075188 Loss: 1.7608461338634712e-08\n",
      "Iteration: 6727 lambda_n: 0.9404726368236076 Loss: 1.7535605852624717e-08\n",
      "Iteration: 6728 lambda_n: 1.0006255329833087 Loss: 1.745933644314438e-08\n",
      "Iteration: 6729 lambda_n: 1.0121620468438586 Loss: 1.7378565126600453e-08\n",
      "Iteration: 6730 lambda_n: 1.005222903504386 Loss: 1.729726554266772e-08\n",
      "Iteration: 6731 lambda_n: 0.9071598076816709 Loss: 1.7216926099565064e-08\n",
      "Iteration: 6732 lambda_n: 0.8770318990659446 Loss: 1.7144782971551647e-08\n",
      "Iteration: 6733 lambda_n: 0.9220415309437742 Loss: 1.7075347615820323e-08\n",
      "Iteration: 6734 lambda_n: 1.004236608708703 Loss: 1.7002664584583427e-08\n",
      "Iteration: 6735 lambda_n: 0.9310478030000601 Loss: 1.692386240184668e-08\n",
      "Iteration: 6736 lambda_n: 0.9587914181598036 Loss: 1.6851164808877947e-08\n",
      "Iteration: 6737 lambda_n: 0.9891612052209358 Loss: 1.6776644726739245e-08\n",
      "Iteration: 6738 lambda_n: 0.9868680821795359 Loss: 1.67001278203677e-08\n",
      "Iteration: 6739 lambda_n: 0.9292191527188337 Loss: 1.6624160671078937e-08\n",
      "Iteration: 6740 lambda_n: 1.0126560437843133 Loss: 1.6552979162707488e-08\n",
      "Iteration: 6741 lambda_n: 0.9273380550978315 Loss: 1.647576190919487e-08\n",
      "Iteration: 6742 lambda_n: 0.9032287616204161 Loss: 1.640540324827684e-08\n",
      "Iteration: 6743 lambda_n: 0.9458938357830945 Loss: 1.633718720308308e-08\n",
      "Iteration: 6744 lambda_n: 0.9803025352017524 Loss: 1.6266067330453804e-08\n",
      "Iteration: 6745 lambda_n: 0.9966225855538585 Loss: 1.6192704406724803e-08\n",
      "Iteration: 6746 lambda_n: 0.9300141260848585 Loss: 1.6118480931023517e-08\n",
      "Iteration: 6747 lambda_n: 0.9821087201717952 Loss: 1.6049558490382627e-08\n",
      "Iteration: 6748 lambda_n: 1.0077489070068186 Loss: 1.5977109543761605e-08\n",
      "Iteration: 6749 lambda_n: 0.9907674423098437 Loss: 1.5903129523802033e-08\n",
      "Iteration: 6750 lambda_n: 0.9966504132980015 Loss: 1.5830757799525394e-08\n",
      "Iteration: 6751 lambda_n: 1.0068084610879862 Loss: 1.575831233712496e-08\n",
      "Iteration: 6752 lambda_n: 1.017207826501986 Loss: 1.568548851978909e-08\n",
      "Iteration: 6753 lambda_n: 0.9040855066985597 Loss: 1.5612278184116273e-08\n",
      "Iteration: 6754 lambda_n: 0.9114971786505693 Loss: 1.554753583136158e-08\n",
      "Iteration: 6755 lambda_n: 0.9415871119427466 Loss: 1.5482554059213966e-08\n",
      "Iteration: 6756 lambda_n: 0.9402780054874105 Loss: 1.5415729304909782e-08\n",
      "Iteration: 6757 lambda_n: 1.0237858781999398 Loss: 1.5349307669227135e-08\n",
      "Iteration: 6758 lambda_n: 0.9541187812074429 Loss: 1.5277323050791615e-08\n",
      "Iteration: 6759 lambda_n: 0.9145648446707981 Loss: 1.5210575767437403e-08\n",
      "Iteration: 6760 lambda_n: 0.9320654556170194 Loss: 1.5146896890728733e-08\n",
      "Iteration: 6761 lambda_n: 1.0075167357633334 Loss: 1.508229263453784e-08\n",
      "Iteration: 6762 lambda_n: 0.9721379554830103 Loss: 1.5012780344805923e-08\n",
      "Iteration: 6763 lambda_n: 0.9508216455827657 Loss: 1.4946042582885495e-08\n",
      "Iteration: 6764 lambda_n: 0.8792843403265224 Loss: 1.4881081544799401e-08\n",
      "Iteration: 6765 lambda_n: 1.0100276394674164 Loss: 1.4821289927830406e-08\n",
      "Iteration: 6766 lambda_n: 1.014946952046938 Loss: 1.4752906489865353e-08\n",
      "Iteration: 6767 lambda_n: 0.9597387532042265 Loss: 1.4684532867164158e-08\n",
      "Iteration: 6768 lambda_n: 1.0199217741369524 Loss: 1.4620202432467965e-08\n",
      "Iteration: 6769 lambda_n: 0.8836505063579829 Loss: 1.4552162363311255e-08\n",
      "Iteration: 6770 lambda_n: 1.0060139966633124 Loss: 1.4493509744581536e-08\n",
      "Iteration: 6771 lambda_n: 0.8802894803207638 Loss: 1.4427027175164545e-08\n",
      "Iteration: 6772 lambda_n: 0.984387891469953 Loss: 1.4369141930704203e-08\n",
      "Iteration: 6773 lambda_n: 0.8770065106378674 Loss: 1.430469341401589e-08\n",
      "Iteration: 6774 lambda_n: 1.0158697222534478 Loss: 1.4247554229375655e-08\n",
      "Iteration: 6775 lambda_n: 0.9900940583576746 Loss: 1.4181655098065815e-08\n",
      "Iteration: 6776 lambda_n: 0.9299063904315537 Loss: 1.4117750413502125e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6777 lambda_n: 1.000107228895814 Loss: 1.4058024017723366e-08\n",
      "Iteration: 6778 lambda_n: 0.8949993529784743 Loss: 1.3994084215226535e-08\n",
      "Iteration: 6779 lambda_n: 0.9421673010816884 Loss: 1.3937146835963758e-08\n",
      "Iteration: 6780 lambda_n: 0.9718003344409086 Loss: 1.387747410328658e-08\n",
      "Iteration: 6781 lambda_n: 0.879543210585393 Loss: 1.3816211314606055e-08\n",
      "Iteration: 6782 lambda_n: 0.8849871861742129 Loss: 1.3761030583479107e-08\n",
      "Iteration: 6783 lambda_n: 1.0055569207254458 Loss: 1.3705749769313045e-08\n",
      "Iteration: 6784 lambda_n: 0.9025277163577139 Loss: 1.364321280555389e-08\n",
      "Iteration: 6785 lambda_n: 1.0249986709375598 Loss: 1.3587362137096287e-08\n",
      "Iteration: 6786 lambda_n: 0.9463046641936483 Loss: 1.3524216152247954e-08\n",
      "Iteration: 6787 lambda_n: 0.8995668853799242 Loss: 1.3466213422995095e-08\n",
      "Iteration: 6788 lambda_n: 0.906243208292652 Loss: 1.3411333335096554e-08\n",
      "Iteration: 6789 lambda_n: 0.9653332077056828 Loss: 1.3356291927787074e-08\n",
      "Iteration: 6790 lambda_n: 1.004697951017017 Loss: 1.3297924604148688e-08\n",
      "Iteration: 6791 lambda_n: 1.0018111928147204 Loss: 1.3237467322893621e-08\n",
      "Iteration: 6792 lambda_n: 0.9258184545918899 Loss: 1.3177483306529517e-08\n",
      "Iteration: 6793 lambda_n: 1.0146494415081038 Loss: 1.3122323865017667e-08\n",
      "Iteration: 6794 lambda_n: 0.9564019602522087 Loss: 1.3062149093523193e-08\n",
      "Iteration: 6795 lambda_n: 0.9471255749205275 Loss: 1.3005713256053903e-08\n",
      "Iteration: 6796 lambda_n: 0.9406160306188901 Loss: 1.2950089206302675e-08\n",
      "Iteration: 6797 lambda_n: 1.0269368983960787 Loss: 1.2895106285521915e-08\n",
      "Iteration: 6798 lambda_n: 0.9151558813909727 Loss: 1.283535718374931e-08\n",
      "Iteration: 6799 lambda_n: 0.9931235843188626 Loss: 1.278238191769164e-08\n",
      "Iteration: 6800 lambda_n: 0.8988935101529079 Loss: 1.2725153923154252e-08\n",
      "Iteration: 6801 lambda_n: 0.877459209011283 Loss: 1.267361013947455e-08\n",
      "Iteration: 6802 lambda_n: 0.9709580949703317 Loss: 1.2623519166126228e-08\n",
      "Iteration: 6803 lambda_n: 1.0259376727911549 Loss: 1.2568331628100275e-08\n",
      "Iteration: 6804 lambda_n: 0.9226504373989574 Loss: 1.2510299519136797e-08\n",
      "Iteration: 6805 lambda_n: 0.8913478906630335 Loss: 1.245837451008808e-08\n",
      "Iteration: 6806 lambda_n: 0.9532084089896277 Loss: 1.2408440122598936e-08\n",
      "Iteration: 6807 lambda_n: 0.9644921643189456 Loss: 1.235527599012486e-08\n",
      "Iteration: 6808 lambda_n: 1.0079347854141911 Loss: 1.230173633424097e-08\n",
      "Iteration: 6809 lambda_n: 0.9291880505778651 Loss: 1.224605238004411e-08\n",
      "Iteration: 6810 lambda_n: 1.005521252854466 Loss: 1.2194974706703607e-08\n",
      "Iteration: 6811 lambda_n: 1.0276355169979343 Loss: 1.213995544069576e-08\n",
      "Iteration: 6812 lambda_n: 0.9905837059246105 Loss: 1.2084006086416481e-08\n",
      "Iteration: 6813 lambda_n: 0.899815757207403 Loss: 1.2030348249377927e-08\n",
      "Iteration: 6814 lambda_n: 0.9717786529090413 Loss: 1.1981845891445903e-08\n",
      "Iteration: 6815 lambda_n: 0.9415990450333501 Loss: 1.1929698091235234e-08\n",
      "Iteration: 6816 lambda_n: 0.9804030167173282 Loss: 1.1879412796775436e-08\n",
      "Iteration: 6817 lambda_n: 0.976589399609009 Loss: 1.1827299406567497e-08\n",
      "Iteration: 6818 lambda_n: 0.8872366698186989 Loss: 1.1775640681061449e-08\n",
      "Iteration: 6819 lambda_n: 0.9127921729672042 Loss: 1.172893514472007e-08\n",
      "Iteration: 6820 lambda_n: 0.9744247820231222 Loss: 1.1681095489927656e-08\n",
      "Iteration: 6821 lambda_n: 0.8908467958012544 Loss: 1.1630256644830539e-08\n",
      "Iteration: 6822 lambda_n: 0.9621079770771782 Loss: 1.1584002360694029e-08\n",
      "Iteration: 6823 lambda_n: 0.9196838612071598 Loss: 1.1534268626530474e-08\n",
      "Iteration: 6824 lambda_n: 0.9875828963948644 Loss: 1.1486954285555528e-08\n",
      "Iteration: 6825 lambda_n: 0.8885612257806118 Loss: 1.1436378360408956e-08\n",
      "Iteration: 6826 lambda_n: 0.9091356955336382 Loss: 1.1391095824426412e-08\n",
      "Iteration: 6827 lambda_n: 1.0160334728326932 Loss: 1.1344968696117978e-08\n",
      "Iteration: 6828 lambda_n: 0.9059991225250551 Loss: 1.1293650270179506e-08\n",
      "Iteration: 6829 lambda_n: 0.9009911632631715 Loss: 1.124811948880951e-08\n",
      "Iteration: 6830 lambda_n: 0.9989935414671316 Loss: 1.1203043545075118e-08\n",
      "Iteration: 6831 lambda_n: 1.0217767157672992 Loss: 1.1153287896749992e-08\n",
      "Iteration: 6832 lambda_n: 0.9312784719774574 Loss: 1.1102649387171073e-08\n",
      "Iteration: 6833 lambda_n: 0.9502657630137663 Loss: 1.1056729228471134e-08\n",
      "Iteration: 6834 lambda_n: 0.9406095628032327 Loss: 1.1010089015628133e-08\n",
      "Iteration: 6835 lambda_n: 0.8910824228646521 Loss: 1.0964139994765634e-08\n",
      "Iteration: 6836 lambda_n: 1.0187259822763803 Loss: 1.0920813087186343e-08\n",
      "Iteration: 6837 lambda_n: 0.8900520299080423 Loss: 1.0871498780897694e-08\n",
      "Iteration: 6838 lambda_n: 0.9163979111861816 Loss: 1.0828630384454896e-08\n",
      "Iteration: 6839 lambda_n: 0.9432128788409581 Loss: 1.078468772178438e-08\n",
      "Iteration: 6840 lambda_n: 0.9282250833137773 Loss: 1.0739664625557817e-08\n",
      "Iteration: 6841 lambda_n: 0.9714751888002099 Loss: 1.069556393829487e-08\n",
      "Iteration: 6842 lambda_n: 0.8803386698048254 Loss: 1.0649620755540863e-08\n",
      "Iteration: 6843 lambda_n: 0.94292115211414 Loss: 1.0608187750701061e-08\n",
      "Iteration: 6844 lambda_n: 0.9166643749610051 Loss: 1.0564003006528763e-08\n",
      "Iteration: 6845 lambda_n: 0.9011358869234007 Loss: 1.052124920891074e-08\n",
      "Iteration: 6846 lambda_n: 0.9500970662713926 Loss: 1.0479410507648685e-08\n",
      "Iteration: 6847 lambda_n: 1.0148252708937004 Loss: 1.0435495636326472e-08\n",
      "Iteration: 6848 lambda_n: 0.9101207044347875 Loss: 1.0388809900073338e-08\n",
      "Iteration: 6849 lambda_n: 0.9200146172252174 Loss: 1.0347151203013062e-08\n",
      "Iteration: 6850 lambda_n: 0.9849584553918536 Loss: 1.0305229531588764e-08\n",
      "Iteration: 6851 lambda_n: 0.9813172456286557 Loss: 1.0260553348648522e-08\n",
      "Iteration: 6852 lambda_n: 0.9826090177754699 Loss: 1.0216259521845612e-08\n",
      "Iteration: 6853 lambda_n: 0.9712704058876229 Loss: 1.0172123022114662e-08\n",
      "Iteration: 6854 lambda_n: 0.9389598641193944 Loss: 1.012870818450637e-08\n",
      "Iteration: 6855 lambda_n: 0.9640516203171924 Loss: 1.0086939492184766e-08\n",
      "Iteration: 6856 lambda_n: 0.9440851353948471 Loss: 1.0044254177166745e-08\n",
      "Iteration: 6857 lambda_n: 0.8820034009926873 Loss: 1.0002652526216714e-08\n",
      "Iteration: 6858 lambda_n: 0.933323666491528 Loss: 9.963968210439149e-09\n",
      "Iteration: 6859 lambda_n: 0.8897090571344484 Loss: 9.923212007977544e-09\n",
      "Iteration: 6860 lambda_n: 1.0180783690176987 Loss: 9.884539930741689e-09\n",
      "Iteration: 6861 lambda_n: 0.9159778498649058 Loss: 9.840483548858339e-09\n",
      "Iteration: 6862 lambda_n: 0.9185223827603254 Loss: 9.801045182574056e-09\n",
      "Iteration: 6863 lambda_n: 1.020635059706945 Loss: 9.761676744069438e-09\n",
      "Iteration: 6864 lambda_n: 0.9491841290075002 Loss: 9.718131033859332e-09\n",
      "Iteration: 6865 lambda_n: 0.9454582468644239 Loss: 9.677838431802116e-09\n",
      "Iteration: 6866 lambda_n: 0.9251554899841157 Loss: 9.637892727814941e-09\n",
      "Iteration: 6867 lambda_n: 0.9551133451969771 Loss: 9.598987881825537e-09\n",
      "Iteration: 6868 lambda_n: 1.0251580249541536 Loss: 9.559007422616295e-09\n",
      "Iteration: 6869 lambda_n: 0.9149480952521487 Loss: 9.5162981894164e-09\n",
      "Iteration: 6870 lambda_n: 1.0248599506989449 Loss: 9.478373801317966e-09\n",
      "Iteration: 6871 lambda_n: 1.0263596431647757 Loss: 9.436086434629384e-09\n",
      "Iteration: 6872 lambda_n: 0.9704909885221195 Loss: 9.393952244140276e-09\n",
      "Iteration: 6873 lambda_n: 1.0196457699996058 Loss: 9.354314046633822e-09\n",
      "Iteration: 6874 lambda_n: 0.892258074227643 Loss: 9.312868582315727e-09\n",
      "Iteration: 6875 lambda_n: 0.8994915131529659 Loss: 9.276783982935684e-09\n",
      "Iteration: 6876 lambda_n: 0.9646447116193462 Loss: 9.240567691866582e-09\n",
      "Iteration: 6877 lambda_n: 0.962225347539026 Loss: 9.201901375853362e-09\n",
      "Iteration: 6878 lambda_n: 0.9192602557136729 Loss: 9.163516370556556e-09\n",
      "Iteration: 6879 lambda_n: 0.9704654008477219 Loss: 9.127020058770555e-09\n",
      "Iteration: 6880 lambda_n: 0.9437006881795227 Loss: 9.088666411197445e-09\n",
      "Iteration: 6881 lambda_n: 0.8780430142444537 Loss: 9.051549799550516e-09\n",
      "Iteration: 6882 lambda_n: 1.0165779160917023 Loss: 9.017176906470512e-09\n",
      "Iteration: 6883 lambda_n: 1.0212391204047413 Loss: 8.977554209534874e-09\n",
      "Iteration: 6884 lambda_n: 0.8741111668118441 Loss: 8.93795032772565e-09\n",
      "Iteration: 6885 lambda_n: 0.937764304684094 Loss: 8.904223320647301e-09\n",
      "Iteration: 6886 lambda_n: 0.9155355174958619 Loss: 8.868197119242726e-09\n",
      "Iteration: 6887 lambda_n: 0.909101444525843 Loss: 8.833188262878923e-09\n",
      "Iteration: 6888 lambda_n: 1.0276762929420908 Loss: 8.798583094128264e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6889 lambda_n: 0.9079746480109995 Loss: 8.75964081039243e-09\n",
      "Iteration: 6890 lambda_n: 0.9486482270881543 Loss: 8.725409377636516e-09\n",
      "Iteration: 6891 lambda_n: 0.961838534378696 Loss: 8.689805476263668e-09\n",
      "Iteration: 6892 lambda_n: 1.0011179116284756 Loss: 8.653876209290953e-09\n",
      "Iteration: 6893 lambda_n: 1.011984855298942 Loss: 8.616657948704666e-09\n",
      "Iteration: 6894 lambda_n: 0.8854472304292457 Loss: 8.579222303100592e-09\n",
      "Iteration: 6895 lambda_n: 0.955503765227087 Loss: 8.546631542649372e-09\n",
      "Iteration: 6896 lambda_n: 0.9228382575354106 Loss: 8.511616618774683e-09\n",
      "Iteration: 6897 lambda_n: 0.992225205342742 Loss: 8.477958736781805e-09\n",
      "Iteration: 6898 lambda_n: 0.9321156854208251 Loss: 8.44193575136905e-09\n",
      "Iteration: 6899 lambda_n: 0.9247466694920284 Loss: 8.408261272897853e-09\n",
      "Iteration: 6900 lambda_n: 1.0273188141529612 Loss: 8.375007259257468e-09\n",
      "Iteration: 6901 lambda_n: 1.021585486551874 Loss: 8.338234174628563e-09\n",
      "Iteration: 6902 lambda_n: 0.8760570087026418 Loss: 8.301852389740771e-09\n",
      "Iteration: 6903 lambda_n: 0.9847707007139124 Loss: 8.27081092286574e-09\n",
      "Iteration: 6904 lambda_n: 0.9004867378575067 Loss: 8.236069031500345e-09\n",
      "Iteration: 6905 lambda_n: 0.9962845724313439 Loss: 8.204455410724142e-09\n",
      "Iteration: 6906 lambda_n: 0.9038924360729965 Loss: 8.169634776628108e-09\n",
      "Iteration: 6907 lambda_n: 1.0174603763525865 Loss: 8.138199017905713e-09\n",
      "Iteration: 6908 lambda_n: 0.8874360135310211 Loss: 8.1029722154972e-09\n",
      "Iteration: 6909 lambda_n: 0.9592921055580872 Loss: 8.072401746935935e-09\n",
      "Iteration: 6910 lambda_n: 0.9730897633686783 Loss: 8.039501356047809e-09\n",
      "Iteration: 6911 lambda_n: 1.011015362022227 Loss: 8.006286337889606e-09\n",
      "Iteration: 6912 lambda_n: 0.9619406732079009 Loss: 7.971943164830838e-09\n",
      "Iteration: 6913 lambda_n: 1.0160848027424674 Loss: 7.939430520777934e-09\n",
      "Iteration: 6914 lambda_n: 0.950550326621746 Loss: 7.90525156882997e-09\n",
      "Iteration: 6915 lambda_n: 0.8872800634205271 Loss: 7.873437833570499e-09\n",
      "Iteration: 6916 lambda_n: 0.9431510148916289 Loss: 7.843881345440193e-09\n",
      "Iteration: 6917 lambda_n: 0.8865133697315429 Loss: 7.812601855637514e-09\n",
      "Iteration: 6918 lambda_n: 0.9217840474638409 Loss: 7.783337970648402e-09\n",
      "Iteration: 6919 lambda_n: 0.930708064188945 Loss: 7.753043421744554e-09\n",
      "Iteration: 6920 lambda_n: 0.9973930270166972 Loss: 7.722595215330956e-09\n",
      "Iteration: 6921 lambda_n: 0.9957333600843025 Loss: 7.690115886987397e-09\n",
      "Iteration: 6922 lambda_n: 0.9579580179007672 Loss: 7.65785073807413e-09\n",
      "Iteration: 6923 lambda_n: 0.9921369294501848 Loss: 7.626962611172215e-09\n",
      "Iteration: 6924 lambda_n: 0.947923781620785 Loss: 7.595124202617268e-09\n",
      "Iteration: 6925 lambda_n: 0.9307971093200077 Loss: 7.564853975917214e-09\n",
      "Iteration: 6926 lambda_n: 0.9631704474552673 Loss: 7.535270116495395e-09\n",
      "Iteration: 6927 lambda_n: 0.9554209222245474 Loss: 7.504798443825334e-09\n",
      "Iteration: 6928 lambda_n: 0.9351726583203104 Loss: 7.474716060135408e-09\n",
      "Iteration: 6929 lambda_n: 0.9976647317094661 Loss: 7.4454104420405364e-09\n",
      "Iteration: 6930 lambda_n: 0.9972822123327737 Loss: 7.414291339926201e-09\n",
      "Iteration: 6931 lambda_n: 1.0000394373359425 Loss: 7.383337785779725e-09\n",
      "Iteration: 6932 lambda_n: 0.8897802600200726 Loss: 7.352451882158229e-09\n",
      "Iteration: 6933 lambda_n: 0.9609341453253923 Loss: 7.325107202867058e-09\n",
      "Iteration: 6934 lambda_n: 0.8822125240301236 Loss: 7.295706011503602e-09\n",
      "Iteration: 6935 lambda_n: 0.9059103301046267 Loss: 7.268841682208948e-09\n",
      "Iteration: 6936 lambda_n: 0.9567591313358064 Loss: 7.241376166168284e-09\n",
      "Iteration: 6937 lambda_n: 0.9502117519657283 Loss: 7.21249914148327e-09\n",
      "Iteration: 6938 lambda_n: 0.9066426781445892 Loss: 7.183955524020331e-09\n",
      "Iteration: 6939 lambda_n: 0.9203080852047647 Loss: 7.156848689366105e-09\n",
      "Iteration: 6940 lambda_n: 1.0197220515525243 Loss: 7.1294567456297655e-09\n",
      "Iteration: 6941 lambda_n: 1.0046317596404146 Loss: 7.099244257233044e-09\n",
      "Iteration: 6942 lambda_n: 1.0205347101756388 Loss: 7.06962902536585e-09\n",
      "Iteration: 6943 lambda_n: 0.9801545899625023 Loss: 7.039694582225812e-09\n",
      "Iteration: 6944 lambda_n: 1.0006764525657157 Loss: 7.011089683124725e-09\n",
      "Iteration: 6945 lambda_n: 0.8855034528145841 Loss: 6.982027531796543e-09\n",
      "Iteration: 6946 lambda_n: 0.902911800229526 Loss: 6.956437454564039e-09\n",
      "Iteration: 6947 lambda_n: 0.9796990747905849 Loss: 6.930458634492627e-09\n",
      "Iteration: 6948 lambda_n: 0.9459088400079227 Loss: 6.902396499237303e-09\n",
      "Iteration: 6949 lambda_n: 0.953131381631553 Loss: 6.875433520571873e-09\n",
      "Iteration: 6950 lambda_n: 0.9133351412737859 Loss: 6.848391800098312e-09\n",
      "Iteration: 6951 lambda_n: 0.874322253761344 Loss: 6.822601271357061e-09\n",
      "Iteration: 6952 lambda_n: 0.9013510479658775 Loss: 6.798023872479477e-09\n",
      "Iteration: 6953 lambda_n: 0.8881209930297167 Loss: 6.772796295799823e-09\n",
      "Iteration: 6954 lambda_n: 0.9141942030993592 Loss: 6.748049829426114e-09\n",
      "Iteration: 6955 lambda_n: 0.9448289963091475 Loss: 6.722688796158107e-09\n",
      "Iteration: 6956 lambda_n: 0.8807876403060022 Loss: 6.696596457702667e-09\n",
      "Iteration: 6957 lambda_n: 0.9871095514338795 Loss: 6.672386260737018e-09\n",
      "Iteration: 6958 lambda_n: 0.8964828696514139 Loss: 6.645371953032117e-09\n",
      "Iteration: 6959 lambda_n: 0.8911441128946799 Loss: 6.620957469692544e-09\n",
      "Iteration: 6960 lambda_n: 0.8863464124083211 Loss: 6.5967959693533105e-09\n",
      "Iteration: 6961 lambda_n: 0.9579655696684823 Loss: 6.57287043174669e-09\n",
      "Iteration: 6962 lambda_n: 0.8888048622114844 Loss: 6.547125101966308e-09\n",
      "Iteration: 6963 lambda_n: 0.9019652409633243 Loss: 6.523351511884688e-09\n",
      "Iteration: 6964 lambda_n: 0.9440215289266097 Loss: 6.499331954695023e-09\n",
      "Iteration: 6965 lambda_n: 0.9242157857135926 Loss: 6.474304597022378e-09\n",
      "Iteration: 6966 lambda_n: 0.909517475387379 Loss: 6.449916641629486e-09\n",
      "Iteration: 6967 lambda_n: 0.9277552995788839 Loss: 6.426026192503748e-09\n",
      "Iteration: 6968 lambda_n: 0.9621861183989565 Loss: 6.401766285772384e-09\n",
      "Iteration: 6969 lambda_n: 0.998903731009068 Loss: 6.376721483291261e-09\n",
      "Iteration: 6970 lambda_n: 0.9555473474096067 Loss: 6.350844675541077e-09\n",
      "Iteration: 6971 lambda_n: 1.0221812075973034 Loss: 6.326213238305401e-09\n",
      "Iteration: 6972 lambda_n: 0.8782996719029542 Loss: 6.299988660918095e-09\n",
      "Iteration: 6973 lambda_n: 0.970701333490087 Loss: 6.2775691156728475e-09\n",
      "Iteration: 6974 lambda_n: 0.9701161346723092 Loss: 6.252898597504943e-09\n",
      "Iteration: 6975 lambda_n: 0.9620313499271138 Loss: 6.228361228719378e-09\n",
      "Iteration: 6976 lambda_n: 0.9470303126787925 Loss: 6.204144997850443e-09\n",
      "Iteration: 6977 lambda_n: 0.9106655893131635 Loss: 6.1804196877564195e-09\n",
      "Iteration: 6978 lambda_n: 1.0110492004673222 Loss: 6.1577121180346645e-09\n",
      "Iteration: 6979 lambda_n: 0.8922046523367587 Loss: 6.132615058688079e-09\n",
      "Iteration: 6980 lambda_n: 0.899798520809675 Loss: 6.11057856082007e-09\n",
      "Iteration: 6981 lambda_n: 0.9886494822970308 Loss: 6.088452480013581e-09\n",
      "Iteration: 6982 lambda_n: 0.8917765737741097 Loss: 6.064249736487596e-09\n",
      "Iteration: 6983 lambda_n: 0.8972092844474512 Loss: 6.042525032251656e-09\n",
      "Iteration: 6984 lambda_n: 0.9655580252607117 Loss: 6.020764277405141e-09\n",
      "Iteration: 6985 lambda_n: 0.9181453370947495 Loss: 5.997449687172297e-09\n",
      "Iteration: 6986 lambda_n: 1.0274458253186713 Loss: 5.975385627959743e-09\n",
      "Iteration: 6987 lambda_n: 1.0197325452556067 Loss: 5.950807083501985e-09\n",
      "Iteration: 6988 lambda_n: 0.9688730880880966 Loss: 5.9265368431654925e-09\n",
      "Iteration: 6989 lambda_n: 0.9047907845439294 Loss: 5.903593171458686e-09\n",
      "Iteration: 6990 lambda_n: 0.9879578179461439 Loss: 5.882269476267777e-09\n",
      "Iteration: 6991 lambda_n: 1.0037915578233216 Loss: 5.859089888098473e-09\n",
      "Iteration: 6992 lambda_n: 0.8940618367859022 Loss: 5.835653733674745e-09\n",
      "Iteration: 6993 lambda_n: 1.0108147812625152 Loss: 5.814882869886558e-09\n",
      "Iteration: 6994 lambda_n: 1.0215142254109542 Loss: 5.791503411594946e-09\n",
      "Iteration: 6995 lambda_n: 0.9753226457865511 Loss: 5.767994426084869e-09\n",
      "Iteration: 6996 lambda_n: 1.0257032087054254 Loss: 5.745661640787124e-09\n",
      "Iteration: 6997 lambda_n: 0.8748328287232884 Loss: 5.722288411673948e-09\n",
      "Iteration: 6998 lambda_n: 0.8811749481853149 Loss: 5.702453950147993e-09\n",
      "Iteration: 6999 lambda_n: 0.9146327998491162 Loss: 5.682561989028609e-09\n",
      "Iteration: 7000 lambda_n: 0.9547430981618196 Loss: 5.66200460199821e-09\n",
      "Iteration: 7001 lambda_n: 0.9824642715024932 Loss: 5.640642611327092e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7002 lambda_n: 1.0230456296622397 Loss: 5.618764011487263e-09\n",
      "Iteration: 7003 lambda_n: 0.9664334143700466 Loss: 5.596092218417249e-09\n",
      "Iteration: 7004 lambda_n: 1.0141661299571356 Loss: 5.5747831042667165e-09\n",
      "Iteration: 7005 lambda_n: 0.983265356440643 Loss: 5.552528241883271e-09\n",
      "Iteration: 7006 lambda_n: 0.9702311975822266 Loss: 5.531059424002215e-09\n",
      "Iteration: 7007 lambda_n: 0.9363151783306641 Loss: 5.509977985749437e-09\n",
      "Iteration: 7008 lambda_n: 0.9185236788423642 Loss: 5.489730857664098e-09\n",
      "Iteration: 7009 lambda_n: 0.9606892604506861 Loss: 5.4699602221556084e-09\n",
      "Iteration: 7010 lambda_n: 0.9984533979625094 Loss: 5.449375753336716e-09\n",
      "Iteration: 7011 lambda_n: 0.9819869045717559 Loss: 5.428083573064029e-09\n",
      "Iteration: 7012 lambda_n: 1.0057733160875535 Loss: 5.407245682210652e-09\n",
      "Iteration: 7013 lambda_n: 1.0169901436110695 Loss: 5.38600646699559e-09\n",
      "Iteration: 7014 lambda_n: 0.9680210939253177 Loss: 5.36463696228433e-09\n",
      "Iteration: 7015 lambda_n: 0.8997398433957672 Loss: 5.344398420596823e-09\n",
      "Iteration: 7016 lambda_n: 0.91537766725258 Loss: 5.325677206452268e-09\n",
      "Iteration: 7017 lambda_n: 0.9551905708801954 Loss: 5.306715154483826e-09\n",
      "Iteration: 7018 lambda_n: 0.9810717347391915 Loss: 5.287017754815743e-09\n",
      "Iteration: 7019 lambda_n: 0.9194315721152743 Loss: 5.266881990521105e-09\n",
      "Iteration: 7020 lambda_n: 0.8840153346402481 Loss: 5.248102602293961e-09\n",
      "Iteration: 7021 lambda_n: 1.0108710489704922 Loss: 5.2301284348321206e-09\n",
      "Iteration: 7022 lambda_n: 0.9252662723006122 Loss: 5.209664702407355e-09\n",
      "Iteration: 7023 lambda_n: 0.9067225681209609 Loss: 5.191027232039235e-09\n",
      "Iteration: 7024 lambda_n: 1.0102688335551682 Loss: 5.17284659657174e-09\n",
      "Iteration: 7025 lambda_n: 0.9389934566997578 Loss: 5.15268043171339e-09\n",
      "Iteration: 7026 lambda_n: 0.9082960604335341 Loss: 5.134030302825265e-09\n",
      "Iteration: 7027 lambda_n: 1.0276423955485243 Loss: 5.116073379738645e-09\n",
      "Iteration: 7028 lambda_n: 0.9627839229866733 Loss: 5.0958480799717465e-09\n",
      "Iteration: 7029 lambda_n: 0.9776639081638177 Loss: 5.076995214238346e-09\n",
      "Iteration: 7030 lambda_n: 0.9637919998985239 Loss: 5.0579418479482515e-09\n",
      "Iteration: 7031 lambda_n: 0.9603337027867186 Loss: 5.039249335884063e-09\n",
      "Iteration: 7032 lambda_n: 0.9044006903411991 Loss: 5.020712378052961e-09\n",
      "Iteration: 7033 lambda_n: 0.895803575875745 Loss: 5.003337660098223e-09\n",
      "Iteration: 7034 lambda_n: 0.9751098265192444 Loss: 4.986204813638974e-09\n",
      "Iteration: 7035 lambda_n: 0.9491530820867219 Loss: 4.967637596888701e-09\n",
      "Iteration: 7036 lambda_n: 0.9267700072903094 Loss: 4.9496514443427275e-09\n",
      "Iteration: 7037 lambda_n: 1.0137247506544946 Loss: 4.9321715652242335e-09\n",
      "Iteration: 7038 lambda_n: 1.0033773509114532 Loss: 4.913139024337093e-09\n",
      "Iteration: 7039 lambda_n: 0.9839722035159602 Loss: 4.894394851743107e-09\n",
      "Iteration: 7040 lambda_n: 0.920569792666263 Loss: 4.876104044055218e-09\n",
      "Iteration: 7041 lambda_n: 0.9170505886383045 Loss: 4.859074720076913e-09\n",
      "Iteration: 7042 lambda_n: 0.9840819010730028 Loss: 4.842187443620996e-09\n",
      "Iteration: 7043 lambda_n: 0.9215560592509939 Loss: 4.824147718565062e-09\n",
      "Iteration: 7044 lambda_n: 0.9057185958846409 Loss: 4.807336032582288e-09\n",
      "Iteration: 7045 lambda_n: 0.933071551639622 Loss: 4.790888263159362e-09\n",
      "Iteration: 7046 lambda_n: 1.0033054097029774 Loss: 4.774019391036992e-09\n",
      "Iteration: 7047 lambda_n: 0.9978074539836084 Loss: 4.755964185473963e-09\n",
      "Iteration: 7048 lambda_n: 0.8844901840153727 Loss: 4.7380966468616314e-09\n",
      "Iteration: 7049 lambda_n: 1.016755433433332 Loss: 4.722336018021033e-09\n",
      "Iteration: 7050 lambda_n: 0.9624857024725668 Loss: 4.704297592888519e-09\n",
      "Iteration: 7051 lambda_n: 0.952010160299763 Loss: 4.687307435064626e-09\n",
      "Iteration: 7052 lambda_n: 0.8998052155889875 Loss: 4.670581838893058e-09\n",
      "Iteration: 7053 lambda_n: 0.8865037145146203 Loss: 4.654847483213661e-09\n",
      "Iteration: 7054 lambda_n: 0.8890627927513303 Loss: 4.639414399065904e-09\n",
      "Iteration: 7055 lambda_n: 0.887205190404866 Loss: 4.624004324673304e-09\n",
      "Iteration: 7056 lambda_n: 0.8802194754633457 Loss: 4.608693762172614e-09\n",
      "Iteration: 7057 lambda_n: 0.9640330884634755 Loss: 4.5935700950330045e-09\n",
      "Iteration: 7058 lambda_n: 0.8814587666922427 Loss: 4.5770782132595575e-09\n",
      "Iteration: 7059 lambda_n: 0.8974498794276276 Loss: 4.56207044323518e-09\n",
      "Iteration: 7060 lambda_n: 0.9197599484124044 Loss: 4.54685671921978e-09\n",
      "Iteration: 7061 lambda_n: 0.9450674721819281 Loss: 4.531333692596452e-09\n",
      "Iteration: 7062 lambda_n: 0.9434102308647653 Loss: 4.515455765642835e-09\n",
      "Iteration: 7063 lambda_n: 0.9633426549348925 Loss: 4.49967940737861e-09\n",
      "Iteration: 7064 lambda_n: 0.9836204793127075 Loss: 4.483644530301579e-09\n",
      "Iteration: 7065 lambda_n: 1.0181740387904896 Loss: 4.467349766599922e-09\n",
      "Iteration: 7066 lambda_n: 0.9813171404479794 Loss: 4.450564256513965e-09\n",
      "Iteration: 7067 lambda_n: 0.9570225961048381 Loss: 4.434467400535505e-09\n",
      "Iteration: 7068 lambda_n: 0.9679716695330612 Loss: 4.418844837827263e-09\n",
      "Iteration: 7069 lambda_n: 0.9771114345604209 Loss: 4.403117952259197e-09\n",
      "Iteration: 7070 lambda_n: 1.0243266308653778 Loss: 4.387318189280888e-09\n",
      "Iteration: 7071 lambda_n: 0.9242974132856631 Loss: 4.370834624687998e-09\n",
      "Iteration: 7072 lambda_n: 0.9705561658216162 Loss: 4.3560356352195746e-09\n",
      "Iteration: 7073 lambda_n: 1.027172838306077 Loss: 4.3405666771098526e-09\n",
      "Iteration: 7074 lambda_n: 1.0172263773441554 Loss: 4.324273561469291e-09\n",
      "Iteration: 7075 lambda_n: 0.9291465140942432 Loss: 4.30821973292588e-09\n",
      "Iteration: 7076 lambda_n: 0.9115167723963046 Loss: 4.293629290914537e-09\n",
      "Iteration: 7077 lambda_n: 0.8874740458156811 Loss: 4.27938109636887e-09\n",
      "Iteration: 7078 lambda_n: 0.9809128444409883 Loss: 4.265570901313412e-09\n",
      "Iteration: 7079 lambda_n: 0.9792102367596937 Loss: 4.250373370766262e-09\n",
      "Iteration: 7080 lambda_n: 0.9132726016902739 Loss: 4.2352753935114485e-09\n",
      "Iteration: 7081 lambda_n: 0.9236693988110494 Loss: 4.221261845538959e-09\n",
      "Iteration: 7082 lambda_n: 0.9627988862275113 Loss: 4.207152431389454e-09\n",
      "Iteration: 7083 lambda_n: 0.9603119021631036 Loss: 4.192512112422875e-09\n",
      "Iteration: 7084 lambda_n: 0.946469925362038 Loss: 4.177978736822714e-09\n",
      "Iteration: 7085 lambda_n: 1.0055855064610173 Loss: 4.163722479230876e-09\n",
      "Iteration: 7086 lambda_n: 0.9506573002314235 Loss: 4.148646296561458e-09\n",
      "Iteration: 7087 lambda_n: 1.0046794078982952 Loss: 4.1344640575310285e-09\n",
      "Iteration: 7088 lambda_n: 0.9286736840232372 Loss: 4.119545972684935e-09\n",
      "Iteration: 7089 lambda_n: 0.879133773534542 Loss: 4.105824533681088e-09\n",
      "Iteration: 7090 lambda_n: 1.0082300956332213 Loss: 4.092894325510344e-09\n",
      "Iteration: 7091 lambda_n: 0.9747078507232885 Loss: 4.0781295444667855e-09\n",
      "Iteration: 7092 lambda_n: 0.9231591297761734 Loss: 4.063926385686059e-09\n",
      "Iteration: 7093 lambda_n: 1.0191113751632126 Loss: 4.050538801257316e-09\n",
      "Iteration: 7094 lambda_n: 0.9977385553530254 Loss: 4.035826836417692e-09\n",
      "Iteration: 7095 lambda_n: 1.0065529846682046 Loss: 4.021495537294902e-09\n",
      "Iteration: 7096 lambda_n: 0.9899283397498194 Loss: 4.007108517399794e-09\n",
      "Iteration: 7097 lambda_n: 1.0180079419800379 Loss: 3.993029094235622e-09\n",
      "Iteration: 7098 lambda_n: 1.0265277713485832 Loss: 3.978620751935302e-09\n",
      "Iteration: 7099 lambda_n: 0.9479531686825898 Loss: 3.9641645008491475e-09\n",
      "Iteration: 7100 lambda_n: 0.9645233256923043 Loss: 3.950882080867706e-09\n",
      "Iteration: 7101 lambda_n: 0.8984996918031161 Loss: 3.9374304344038246e-09\n",
      "Iteration: 7102 lambda_n: 0.9510862088628796 Loss: 3.924958931023974e-09\n",
      "Iteration: 7103 lambda_n: 1.0133091149158875 Loss: 3.911815807877381e-09\n",
      "Iteration: 7104 lambda_n: 0.9638174235584784 Loss: 3.8978782744902045e-09\n",
      "Iteration: 7105 lambda_n: 0.9426168689370772 Loss: 3.884687439319213e-09\n",
      "Iteration: 7106 lambda_n: 0.8759742951288872 Loss: 3.871847825250889e-09\n",
      "Iteration: 7107 lambda_n: 0.9802793876671665 Loss: 3.8599711757824564e-09\n",
      "Iteration: 7108 lambda_n: 0.885185323116117 Loss: 3.84673757625099e-09\n",
      "Iteration: 7109 lambda_n: 0.9158455168117902 Loss: 3.8348452145169366e-09\n",
      "Iteration: 7110 lambda_n: 1.0062098346546966 Loss: 3.8225944415735925e-09\n",
      "Iteration: 7111 lambda_n: 1.026318790839878 Loss: 3.809195486454894e-09\n",
      "Iteration: 7112 lambda_n: 0.955944118787897 Loss: 3.795596309237832e-09\n",
      "Iteration: 7113 lambda_n: 0.8914009121521559 Loss: 3.782993431186473e-09\n",
      "Iteration: 7114 lambda_n: 0.9610359985316436 Loss: 3.77129659853303e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7115 lambda_n: 0.9864242705938935 Loss: 3.758741260541664e-09\n",
      "Iteration: 7116 lambda_n: 0.9512860616168201 Loss: 3.745915055810921e-09\n",
      "Iteration: 7117 lambda_n: 0.8908406846287602 Loss: 3.733605632332171e-09\n",
      "Iteration: 7118 lambda_n: 0.9450072077773286 Loss: 3.7221321776497007e-09\n",
      "Iteration: 7119 lambda_n: 1.0122286827586855 Loss: 3.710014347174843e-09\n",
      "Iteration: 7120 lambda_n: 0.9350710039009514 Loss: 3.697094775169372e-09\n",
      "Iteration: 7121 lambda_n: 0.8946616543039939 Loss: 3.6852192695958285e-09\n",
      "Iteration: 7122 lambda_n: 0.8952792786699482 Loss: 3.6739091067774588e-09\n",
      "Iteration: 7123 lambda_n: 0.8770682584340547 Loss: 3.6626408466424263e-09\n",
      "Iteration: 7124 lambda_n: 0.8808403903497045 Loss: 3.6516502951286846e-09\n",
      "Iteration: 7125 lambda_n: 0.9775964282036437 Loss: 3.6406599884124653e-09\n",
      "Iteration: 7126 lambda_n: 0.9569271577454275 Loss: 3.6285152267879616e-09\n",
      "Iteration: 7127 lambda_n: 1.019691383218748 Loss: 3.616684266120933e-09\n",
      "Iteration: 7128 lambda_n: 0.935387423898975 Loss: 3.604136557888428e-09\n",
      "Iteration: 7129 lambda_n: 0.96789683451969 Loss: 3.592683809622244e-09\n",
      "Iteration: 7130 lambda_n: 0.926628007730475 Loss: 3.580887420199587e-09\n",
      "Iteration: 7131 lambda_n: 1.0180278278347115 Loss: 3.569647631385924e-09\n",
      "Iteration: 7132 lambda_n: 0.9688239884298355 Loss: 3.5573553590194905e-09\n",
      "Iteration: 7133 lambda_n: 1.0264099176841404 Loss: 3.5457156182939445e-09\n",
      "Iteration: 7134 lambda_n: 0.920114766405915 Loss: 3.5334426690643974e-09\n",
      "Iteration: 7135 lambda_n: 0.9014872492575202 Loss: 3.522496065246872e-09\n",
      "Iteration: 7136 lambda_n: 0.903626389419703 Loss: 3.5118194763868202e-09\n",
      "Iteration: 7137 lambda_n: 1.0067893086738804 Loss: 3.501164889090443e-09\n",
      "Iteration: 7138 lambda_n: 0.9154359602280052 Loss: 3.4893465865709204e-09\n",
      "Iteration: 7139 lambda_n: 0.8757947140587539 Loss: 3.4786536788108704e-09\n",
      "Iteration: 7140 lambda_n: 0.9766154251785726 Loss: 3.4684697328001424e-09\n",
      "Iteration: 7141 lambda_n: 0.9216903457365967 Loss: 3.4571622398870747e-09\n",
      "Iteration: 7142 lambda_n: 0.9767597334794726 Loss: 3.446541787883952e-09\n",
      "Iteration: 7143 lambda_n: 0.9618737744117558 Loss: 3.435337678077956e-09\n",
      "Iteration: 7144 lambda_n: 1.0117717741351868 Loss: 3.424357171947614e-09\n",
      "Iteration: 7145 lambda_n: 0.8749002551906253 Loss: 3.4128615393483996e-09\n",
      "Iteration: 7146 lambda_n: 0.9657532371401819 Loss: 3.4029703193666647e-09\n",
      "Iteration: 7147 lambda_n: 0.9383177834960564 Loss: 3.3920988281686666e-09\n",
      "Iteration: 7148 lambda_n: 0.9988680332504428 Loss: 3.3815861935154356e-09\n",
      "Iteration: 7149 lambda_n: 0.9665940456720173 Loss: 3.370446686050931e-09\n",
      "Iteration: 7150 lambda_n: 0.9891319440165303 Loss: 3.3597198793791525e-09\n",
      "Iteration: 7151 lambda_n: 0.9276253699069653 Loss: 3.3487950022559324e-09\n",
      "Iteration: 7152 lambda_n: 0.9092806697022279 Loss: 3.3385991239377232e-09\n",
      "Iteration: 7153 lambda_n: 0.9203940402242446 Loss: 3.3286503276205845e-09\n",
      "Iteration: 7154 lambda_n: 1.0068638920970272 Loss: 3.318624831715567e-09\n",
      "Iteration: 7155 lambda_n: 1.0031927432767889 Loss: 3.3077069749143887e-09\n",
      "Iteration: 7156 lambda_n: 0.9659948843290576 Loss: 3.296882610692904e-09\n",
      "Iteration: 7157 lambda_n: 0.8912601649310062 Loss: 3.286510861171847e-09\n",
      "Iteration: 7158 lambda_n: 1.017255998491587 Loss: 3.2769868180365195e-09\n",
      "Iteration: 7159 lambda_n: 0.9299820121155011 Loss: 3.266163914942809e-09\n",
      "Iteration: 7160 lambda_n: 0.9708347587806196 Loss: 3.256318855698234e-09\n",
      "Iteration: 7161 lambda_n: 0.9394433556934204 Loss: 3.2460881770073073e-09\n",
      "Iteration: 7162 lambda_n: 0.9940082740977938 Loss: 3.2362354046175957e-09\n",
      "Iteration: 7163 lambda_n: 0.8911279540526034 Loss: 3.2258583790115077e-09\n",
      "Iteration: 7164 lambda_n: 0.8974269260700847 Loss: 3.216600680435636e-09\n",
      "Iteration: 7165 lambda_n: 1.0004057714656993 Loss: 3.207318258508395e-09\n",
      "Iteration: 7166 lambda_n: 0.9918309943044938 Loss: 3.197016226232738e-09\n",
      "Iteration: 7167 lambda_n: 1.0230484572635616 Loss: 3.1868525663868177e-09\n",
      "Iteration: 7168 lambda_n: 0.9659491846223577 Loss: 3.176419980288515e-09\n",
      "Iteration: 7169 lambda_n: 1.0223962641204722 Loss: 3.1666190277686766e-09\n",
      "Iteration: 7170 lambda_n: 0.9812494537810443 Loss: 3.156294464404748e-09\n",
      "Iteration: 7171 lambda_n: 1.0124168975722483 Loss: 3.1464350464117416e-09\n",
      "Iteration: 7172 lambda_n: 0.9849789660921294 Loss: 3.1363113835242773e-09\n",
      "Iteration: 7173 lambda_n: 0.9630955318395825 Loss: 3.126510938373655e-09\n",
      "Iteration: 7174 lambda_n: 0.9019447971318374 Loss: 3.1169744700157285e-09\n",
      "Iteration: 7175 lambda_n: 0.9804746308480994 Loss: 3.1080856328718213e-09\n",
      "Iteration: 7176 lambda_n: 0.9274674375730956 Loss: 3.098465589809452e-09\n",
      "Iteration: 7177 lambda_n: 1.022739322438703 Loss: 3.089409331621466e-09\n",
      "Iteration: 7178 lambda_n: 1.0016675310176122 Loss: 3.079468189139279e-09\n",
      "Iteration: 7179 lambda_n: 0.9530557884263938 Loss: 3.0697806414929374e-09\n",
      "Iteration: 7180 lambda_n: 0.9428764882247798 Loss: 3.0606084544743942e-09\n",
      "Iteration: 7181 lambda_n: 0.8804728219786387 Loss: 3.0515766021757134e-09\n",
      "Iteration: 7182 lambda_n: 0.879631683087676 Loss: 3.0431814454276122e-09\n",
      "Iteration: 7183 lambda_n: 0.9270813577320338 Loss: 3.034830477293351e-09\n",
      "Iteration: 7184 lambda_n: 0.9508820519874459 Loss: 3.0260669868879394e-09\n",
      "Iteration: 7185 lambda_n: 0.8987292093611596 Loss: 3.0171193236675942e-09\n",
      "Iteration: 7186 lambda_n: 0.9794075671725773 Loss: 3.0087017824991494e-09\n",
      "Iteration: 7187 lambda_n: 0.9113953035991929 Loss: 2.9995690092686616e-09\n",
      "Iteration: 7188 lambda_n: 0.8833090935224271 Loss: 2.9911111865494435e-09\n",
      "Iteration: 7189 lambda_n: 0.9066050439446188 Loss: 2.98295058562805e-09\n",
      "Iteration: 7190 lambda_n: 0.9315660289858632 Loss: 2.974611001938218e-09\n",
      "Iteration: 7191 lambda_n: 0.9980408286978304 Loss: 2.966079861481476e-09\n",
      "Iteration: 7192 lambda_n: 1.0156388657649467 Loss: 2.9569816601047982e-09\n",
      "Iteration: 7193 lambda_n: 1.013082412359724 Loss: 2.947768296374576e-09\n",
      "Iteration: 7194 lambda_n: 0.9654777333941982 Loss: 2.9386238285054023e-09\n",
      "Iteration: 7195 lambda_n: 0.879091191828638 Loss: 2.9299522730794687e-09\n",
      "Iteration: 7196 lambda_n: 0.9819316784000048 Loss: 2.922093922356704e-09\n",
      "Iteration: 7197 lambda_n: 0.9472991113278926 Loss: 2.913354071205819e-09\n",
      "Iteration: 7198 lambda_n: 0.9255672456973716 Loss: 2.904963008309506e-09\n",
      "Iteration: 7199 lambda_n: 1.0056217114102837 Loss: 2.896802463504208e-09\n",
      "Iteration: 7200 lambda_n: 1.0205598077094422 Loss: 2.8879762971386845e-09\n",
      "Iteration: 7201 lambda_n: 0.9081018204068022 Loss: 2.8790631207013578e-09\n",
      "Iteration: 7202 lambda_n: 1.0169584368036066 Loss: 2.8711717139244675e-09\n",
      "Iteration: 7203 lambda_n: 0.8830197199274885 Loss: 2.8623736629822993e-09\n",
      "Iteration: 7204 lambda_n: 0.9221511314045854 Loss: 2.854772362614014e-09\n",
      "Iteration: 7205 lambda_n: 0.983228147516772 Loss: 2.846868540119285e-09\n",
      "Iteration: 7206 lambda_n: 0.9098113255367086 Loss: 2.8384792749758634e-09\n",
      "Iteration: 7207 lambda_n: 0.9195662238099669 Loss: 2.8307537796516422e-09\n",
      "Iteration: 7208 lambda_n: 0.9672912416112038 Loss: 2.8229802280144304e-09\n",
      "Iteration: 7209 lambda_n: 0.9848416254120683 Loss: 2.814840048128401e-09\n",
      "Iteration: 7210 lambda_n: 0.9481674260580364 Loss: 2.806591426774057e-09\n",
      "Iteration: 7211 lambda_n: 0.890396412331945 Loss: 2.7986882548898726e-09\n",
      "Iteration: 7212 lambda_n: 0.971982117043729 Loss: 2.791301040176477e-09\n",
      "Iteration: 7213 lambda_n: 0.9632568340136834 Loss: 2.7832721148937797e-09\n",
      "Iteration: 7214 lambda_n: 0.8912323139236311 Loss: 2.775353111749735e-09\n",
      "Iteration: 7215 lambda_n: 0.9159491252932171 Loss: 2.768060759424145e-09\n",
      "Iteration: 7216 lambda_n: 0.9889189319848893 Loss: 2.760598876993338e-09\n",
      "Iteration: 7217 lambda_n: 0.9914211442070932 Loss: 2.752578653218819e-09\n",
      "Iteration: 7218 lambda_n: 0.9505909085080702 Loss: 2.7445770653554557e-09\n",
      "Iteration: 7219 lambda_n: 0.9821940008442236 Loss: 2.7369422176687654e-09\n",
      "Iteration: 7220 lambda_n: 0.9610952116728281 Loss: 2.729090252654947e-09\n",
      "Iteration: 7221 lambda_n: 0.9098225786102807 Loss: 2.721443887710954e-09\n",
      "Iteration: 7222 lambda_n: 0.9557594014499707 Loss: 2.71423948654796e-09\n",
      "Iteration: 7223 lambda_n: 0.9471607489105406 Loss: 2.7067050385824505e-09\n",
      "Iteration: 7224 lambda_n: 1.0138836237697015 Loss: 2.6992733009942204e-09\n",
      "Iteration: 7225 lambda_n: 0.899203968316319 Loss: 2.6913549205766882e-09\n",
      "Iteration: 7226 lambda_n: 0.8946329768901773 Loss: 2.6843669948288355e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7227 lambda_n: 0.8958712585732866 Loss: 2.6774451887854135e-09\n",
      "Iteration: 7228 lambda_n: 1.025923919822977 Loss: 2.670544151183479e-09\n",
      "Iteration: 7229 lambda_n: 0.8808242086857138 Loss: 2.662675972551122e-09\n",
      "Iteration: 7230 lambda_n: 0.904486772813568 Loss: 2.655954503151793e-09\n",
      "Iteration: 7231 lambda_n: 0.9084593183468007 Loss: 2.6490822171538598e-09\n",
      "Iteration: 7232 lambda_n: 0.9061603394349992 Loss: 2.642210303262618e-09\n",
      "Iteration: 7233 lambda_n: 0.9996785142228982 Loss: 2.6353862512720247e-09\n",
      "Iteration: 7234 lambda_n: 0.9006536056713619 Loss: 2.6278913282681113e-09\n",
      "Iteration: 7235 lambda_n: 0.87994908189076 Loss: 2.6211718448287883e-09\n",
      "Iteration: 7236 lambda_n: 0.9651449393013088 Loss: 2.6146357582342305e-09\n",
      "Iteration: 7237 lambda_n: 0.9965169100949907 Loss: 2.6074977354234547e-09\n",
      "Iteration: 7238 lambda_n: 0.9138573457170047 Loss: 2.600162491186535e-09\n",
      "Iteration: 7239 lambda_n: 0.9001035250386593 Loss: 2.5934684859574966e-09\n",
      "Iteration: 7240 lambda_n: 0.9145462156263172 Loss: 2.586904706834223e-09\n",
      "Iteration: 7241 lambda_n: 0.9704304814530522 Loss: 2.580264984509892e-09\n",
      "Iteration: 7242 lambda_n: 0.9923292060901924 Loss: 2.5732510745547364e-09\n",
      "Iteration: 7243 lambda_n: 0.9596016942522507 Loss: 2.5661129387511043e-09\n",
      "Iteration: 7244 lambda_n: 0.9403954884391204 Loss: 2.5592437314429452e-09\n",
      "Iteration: 7245 lambda_n: 0.9571069386015657 Loss: 2.5525436134448273e-09\n",
      "Iteration: 7246 lambda_n: 1.0075025950106036 Loss: 2.5457558019290336e-09\n",
      "Iteration: 7247 lambda_n: 0.924458637662225 Loss: 2.5386440504445917e-09\n",
      "Iteration: 7248 lambda_n: 0.9834304895908261 Loss: 2.5321506437444818e-09\n",
      "Iteration: 7249 lambda_n: 0.921199212777772 Loss: 2.5252742669854665e-09\n",
      "Iteration: 7250 lambda_n: 0.9263644125381648 Loss: 2.518864008030209e-09\n",
      "Iteration: 7251 lambda_n: 0.995847614188299 Loss: 2.5124468585949394e-09\n",
      "Iteration: 7252 lambda_n: 0.9468287852858461 Loss: 2.505579666773114e-09\n",
      "Iteration: 7253 lambda_n: 0.9650782882792455 Loss: 2.499082294951552e-09\n",
      "Iteration: 7254 lambda_n: 0.9252151625096887 Loss: 2.4924903673102623e-09\n",
      "Iteration: 7255 lambda_n: 0.9220857191277971 Loss: 2.486200557888074e-09\n",
      "Iteration: 7256 lambda_n: 1.016093453370041 Loss: 2.479960395546105e-09\n",
      "Iteration: 7257 lambda_n: 0.9927349280184504 Loss: 2.4731150690772502e-09\n",
      "Iteration: 7258 lambda_n: 0.9532360473623104 Loss: 2.4664603535601203e-09\n",
      "Iteration: 7259 lambda_n: 0.8982317076900777 Loss: 2.4601014446173962e-09\n",
      "Iteration: 7260 lambda_n: 0.9718618951320206 Loss: 2.4541373954686206e-09\n",
      "Iteration: 7261 lambda_n: 0.9094983292554202 Loss: 2.447712821562916e-09\n",
      "Iteration: 7262 lambda_n: 0.9702460005103865 Loss: 2.4417290851648595e-09\n",
      "Iteration: 7263 lambda_n: 0.9193791175962521 Loss: 2.4353740935236503e-09\n",
      "Iteration: 7264 lambda_n: 1.0106821269786717 Loss: 2.429380841597487e-09\n",
      "Iteration: 7265 lambda_n: 0.9976796720102238 Loss: 2.4228220600877823e-09\n",
      "Iteration: 7266 lambda_n: 0.9254959455143215 Loss: 2.416379668025302e-09\n",
      "Iteration: 7267 lambda_n: 0.9002033965219696 Loss: 2.410432545790687e-09\n",
      "Iteration: 7268 lambda_n: 0.9349417651576022 Loss: 2.404674136982995e-09\n",
      "Iteration: 7269 lambda_n: 0.9604556124092233 Loss: 2.398719854665889e-09\n",
      "Iteration: 7270 lambda_n: 1.017798048949235 Loss: 2.392631063221986e-09\n",
      "Iteration: 7271 lambda_n: 1.0237430690354472 Loss: 2.386209080430561e-09\n",
      "Iteration: 7272 lambda_n: 1.021704695325244 Loss: 2.379781758917406e-09\n",
      "Iteration: 7273 lambda_n: 0.8889031790464081 Loss: 2.373399344274898e-09\n",
      "Iteration: 7274 lambda_n: 0.9884461839987878 Loss: 2.367874262788988e-09\n",
      "Iteration: 7275 lambda_n: 0.8812176678246704 Loss: 2.3617571835228615e-09\n",
      "Iteration: 7276 lambda_n: 0.9209035406901253 Loss: 2.356330053523631e-09\n",
      "Iteration: 7277 lambda_n: 0.9620367821676742 Loss: 2.350682966147031e-09\n",
      "Iteration: 7278 lambda_n: 0.9605671127468894 Loss: 2.3448102243124065e-09\n",
      "Iteration: 7279 lambda_n: 0.9196233115112942 Loss: 2.3389740649438117e-09\n",
      "Iteration: 7280 lambda_n: 0.9133600730274759 Loss: 2.333412916484262e-09\n",
      "Iteration: 7281 lambda_n: 0.9585088647264102 Loss: 2.327914488180773e-09\n",
      "Iteration: 7282 lambda_n: 0.993176710840556 Loss: 2.3221700566883546e-09\n",
      "Iteration: 7283 lambda_n: 0.9736389097331485 Loss: 2.316245771201331e-09\n",
      "Iteration: 7284 lambda_n: 0.9201128882630505 Loss: 2.310466242140865e-09\n",
      "Iteration: 7285 lambda_n: 0.9292655680495435 Loss: 2.3050304442041183e-09\n",
      "Iteration: 7286 lambda_n: 0.8928140626594244 Loss: 2.299565298843693e-09\n",
      "Iteration: 7287 lambda_n: 1.0031148759202067 Loss: 2.294338398523104e-09\n",
      "Iteration: 7288 lambda_n: 0.9707405908298462 Loss: 2.2884914199550394e-09\n",
      "Iteration: 7289 lambda_n: 0.8828049430426952 Loss: 2.2828609125133703e-09\n",
      "Iteration: 7290 lambda_n: 0.9495114181660994 Loss: 2.277764749813253e-09\n",
      "Iteration: 7291 lambda_n: 0.9473075119961127 Loss: 2.2723071970865197e-09\n",
      "Iteration: 7292 lambda_n: 1.0052890723851235 Loss: 2.266887603213316e-09\n",
      "Iteration: 7293 lambda_n: 0.9415361062908639 Loss: 2.2611629700612534e-09\n",
      "Iteration: 7294 lambda_n: 0.8915732271237886 Loss: 2.2558277357490005e-09\n",
      "Iteration: 7295 lambda_n: 0.9109681950577858 Loss: 2.2507988848157265e-09\n",
      "Iteration: 7296 lambda_n: 0.9808970897932281 Loss: 2.245683047923746e-09\n",
      "Iteration: 7297 lambda_n: 0.9357662115277097 Loss: 2.240199074506935e-09\n",
      "Iteration: 7298 lambda_n: 0.9791785076869423 Loss: 2.2349925132064695e-09\n",
      "Iteration: 7299 lambda_n: 0.875449497570884 Loss: 2.229569361519728e-09\n",
      "Iteration: 7300 lambda_n: 0.9004128970662636 Loss: 2.2247439300918037e-09\n",
      "Iteration: 7301 lambda_n: 0.9391697141880448 Loss: 2.219802160719139e-09\n",
      "Iteration: 7302 lambda_n: 0.9208090867631036 Loss: 2.2146703929397797e-09\n",
      "Iteration: 7303 lambda_n: 0.8947571069516792 Loss: 2.20966207452016e-09\n",
      "Iteration: 7304 lambda_n: 0.9748552720371073 Loss: 2.2048173794706947e-09\n",
      "Iteration: 7305 lambda_n: 0.9343002202323051 Loss: 2.1995621192185577e-09\n",
      "Iteration: 7306 lambda_n: 0.9720281333303785 Loss: 2.194549488403569e-09\n",
      "Iteration: 7307 lambda_n: 0.963138722097037 Loss: 2.189358297290306e-09\n",
      "Iteration: 7308 lambda_n: 0.9772683761873913 Loss: 2.1842390419523933e-09\n",
      "Iteration: 7309 lambda_n: 1.009140102235537 Loss: 2.1790691723943494e-09\n",
      "Iteration: 7310 lambda_n: 0.9321510985825203 Loss: 2.1737562323992387e-09\n",
      "Iteration: 7311 lambda_n: 1.0254053260903262 Loss: 2.1688728529768086e-09\n",
      "Iteration: 7312 lambda_n: 0.8796398966942923 Loss: 2.1635254582001223e-09\n",
      "Iteration: 7313 lambda_n: 0.9984725409005688 Loss: 2.158961216771782e-09\n",
      "Iteration: 7314 lambda_n: 0.9598305593258428 Loss: 2.153802708337907e-09\n",
      "Iteration: 7315 lambda_n: 0.9016533284631923 Loss: 2.1488680639714015e-09\n",
      "Iteration: 7316 lambda_n: 0.9622251320266246 Loss: 2.144254285175718e-09\n",
      "Iteration: 7317 lambda_n: 0.9946450404247259 Loss: 2.1393522927127836e-09\n",
      "Iteration: 7318 lambda_n: 1.0156811256783582 Loss: 2.134309017058466e-09\n",
      "Iteration: 7319 lambda_n: 1.0130726658912825 Loss: 2.129184149797869e-09\n",
      "Iteration: 7320 lambda_n: 0.8951873183110696 Loss: 2.124097856168695e-09\n",
      "Iteration: 7321 lambda_n: 0.9769908339419675 Loss: 2.1196256967810377e-09\n",
      "Iteration: 7322 lambda_n: 0.9285039666295741 Loss: 2.1147662720611826e-09\n",
      "Iteration: 7323 lambda_n: 0.9707343818260699 Loss: 2.1101700863811966e-09\n",
      "Iteration: 7324 lambda_n: 0.9259586596486498 Loss: 2.105386708915093e-09\n",
      "Iteration: 7325 lambda_n: 0.9404794319114615 Loss: 2.1008456449860444e-09\n",
      "Iteration: 7326 lambda_n: 1.0112248621680213 Loss: 2.0962542773674474e-09\n",
      "Iteration: 7327 lambda_n: 1.0214764351494003 Loss: 2.091340277717811e-09\n",
      "Iteration: 7328 lambda_n: 0.913920956659515 Loss: 2.086401032907145e-09\n",
      "Iteration: 7329 lambda_n: 0.9442383849433471 Loss: 2.0820039546994567e-09\n",
      "Iteration: 7330 lambda_n: 0.8856728430144389 Loss: 2.0774813411606493e-09\n",
      "Iteration: 7331 lambda_n: 0.8951273788965901 Loss: 2.0732588376450682e-09\n",
      "Iteration: 7332 lambda_n: 0.9405972765463468 Loss: 2.069009774049362e-09\n",
      "Iteration: 7333 lambda_n: 0.9632171542847177 Loss: 2.0645644469513914e-09\n",
      "Iteration: 7334 lambda_n: 0.8792097329958153 Loss: 2.0600331959960564e-09\n",
      "Iteration: 7335 lambda_n: 0.9341672509525931 Loss: 2.0559166243346443e-09\n",
      "Iteration: 7336 lambda_n: 0.9537446352228567 Loss: 2.0515615819197546e-09\n",
      "Iteration: 7337 lambda_n: 0.9919823241848431 Loss: 2.047135609848852e-09\n",
      "Iteration: 7338 lambda_n: 0.9588477809445007 Loss: 2.0425537020985027e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7339 lambda_n: 0.9809560238722713 Loss: 2.038146349606632e-09\n",
      "Iteration: 7340 lambda_n: 0.9981599850484976 Loss: 2.0336585599539087e-09\n",
      "Iteration: 7341 lambda_n: 0.9434770885638278 Loss: 2.0291140036790474e-09\n",
      "Iteration: 7342 lambda_n: 0.9725873903731276 Loss: 2.024839412856933e-09\n",
      "Iteration: 7343 lambda_n: 1.0201448985056276 Loss: 2.0204532930183866e-09\n",
      "Iteration: 7344 lambda_n: 0.949024863419652 Loss: 2.015874630315718e-09\n",
      "Iteration: 7345 lambda_n: 1.003103314198013 Loss: 2.011636456349201e-09\n",
      "Iteration: 7346 lambda_n: 1.0164337112936828 Loss: 2.0071776046738175e-09\n",
      "Iteration: 7347 lambda_n: 0.9939414694021874 Loss: 2.0026817002370456e-09\n",
      "Iteration: 7348 lambda_n: 0.9266079294526245 Loss: 1.9983071734938003e-09\n",
      "Iteration: 7349 lambda_n: 0.9027573359593768 Loss: 1.9942488506600035e-09\n",
      "Iteration: 7350 lambda_n: 0.9122066897286397 Loss: 1.9903129281774125e-09\n",
      "Iteration: 7351 lambda_n: 0.9494952644933542 Loss: 1.986353409083354e-09\n",
      "Iteration: 7352 lambda_n: 0.9482366659496281 Loss: 1.982250466868459e-09\n",
      "Iteration: 7353 lambda_n: 0.979211310312215 Loss: 1.9781720226909665e-09\n",
      "Iteration: 7354 lambda_n: 0.9355010311002516 Loss: 1.973979929318636e-09\n",
      "Iteration: 7355 lambda_n: 0.9426980007822194 Loss: 1.9699941829208808e-09\n",
      "Iteration: 7356 lambda_n: 0.9140389152354433 Loss: 1.965996192958075e-09\n",
      "Iteration: 7357 lambda_n: 0.9963161758888309 Loss: 1.9621376434483855e-09\n",
      "Iteration: 7358 lambda_n: 0.8943930023851665 Loss: 1.957950618317417e-09\n",
      "Iteration: 7359 lambda_n: 0.9909748845395387 Loss: 1.9542102755209357e-09\n",
      "Iteration: 7360 lambda_n: 0.9607013767273301 Loss: 1.950084203817588e-09\n",
      "Iteration: 7361 lambda_n: 0.9583778501394687 Loss: 1.9461036127499643e-09\n",
      "Iteration: 7362 lambda_n: 1.0265784120846233 Loss: 1.9421513525627242e-09\n",
      "Iteration: 7363 lambda_n: 0.9713933850482983 Loss: 1.937937744615843e-09\n",
      "Iteration: 7364 lambda_n: 0.9655249863014942 Loss: 1.933970708647102e-09\n",
      "Iteration: 7365 lambda_n: 1.0179664044101728 Loss: 1.9300464189417235e-09\n",
      "Iteration: 7366 lambda_n: 0.9152101104883122 Loss: 1.9259285782429113e-09\n",
      "Iteration: 7367 lambda_n: 0.9611581072500484 Loss: 1.9222448745888433e-09\n",
      "Iteration: 7368 lambda_n: 1.0013699862692806 Loss: 1.918393600443258e-09\n",
      "Iteration: 7369 lambda_n: 0.9661523201194094 Loss: 1.9144001141492403e-09\n",
      "Iteration: 7370 lambda_n: 0.8842988426402548 Loss: 1.910565989973398e-09\n",
      "Iteration: 7371 lambda_n: 0.9624667778565531 Loss: 1.907073320368938e-09\n",
      "Iteration: 7372 lambda_n: 0.9839004783221671 Loss: 1.903288421367274e-09\n",
      "Iteration: 7373 lambda_n: 0.9035499017890107 Loss: 1.8994375026107514e-09\n",
      "Iteration: 7374 lambda_n: 0.9480260066263754 Loss: 1.8959181297442452e-09\n",
      "Iteration: 7375 lambda_n: 0.8947976296757474 Loss: 1.892241896742537e-09\n",
      "Iteration: 7376 lambda_n: 1.0052292624609513 Loss: 1.8887882068534733e-09\n",
      "Iteration: 7377 lambda_n: 1.0029893631201567 Loss: 1.884925330760708e-09\n",
      "Iteration: 7378 lambda_n: 0.8775869381396743 Loss: 1.881090069512225e-09\n",
      "Iteration: 7379 lambda_n: 0.9848847527415521 Loss: 1.877750835198926e-09\n",
      "Iteration: 7380 lambda_n: 0.9984428666992368 Loss: 1.8740194791221292e-09\n",
      "Iteration: 7381 lambda_n: 1.0190280921454746 Loss: 1.8702550480968615e-09\n",
      "Iteration: 7382 lambda_n: 0.8774151985217634 Loss: 1.8664318320725707e-09\n",
      "Iteration: 7383 lambda_n: 0.9382520604592666 Loss: 1.863156372619363e-09\n",
      "Iteration: 7384 lambda_n: 0.8979726622702675 Loss: 1.8596688931819629e-09\n",
      "Iteration: 7385 lambda_n: 0.8864972519805171 Loss: 1.856346511009755e-09\n",
      "Iteration: 7386 lambda_n: 0.9072944375588203 Loss: 1.853081041276852e-09\n",
      "Iteration: 7387 lambda_n: 0.8884375338757056 Loss: 1.8497535127630422e-09\n",
      "Iteration: 7388 lambda_n: 1.0204278315449709 Loss: 1.8465096571290778e-09\n",
      "Iteration: 7389 lambda_n: 0.8928405297193456 Loss: 1.842800152987056e-09\n",
      "Iteration: 7390 lambda_n: 0.9192106868938541 Loss: 1.8395707152944005e-09\n",
      "Iteration: 7391 lambda_n: 0.8740321506883121 Loss: 1.8362604724314358e-09\n",
      "Iteration: 7392 lambda_n: 0.9941930266850052 Loss: 1.8331271354166585e-09\n",
      "Iteration: 7393 lambda_n: 0.9208253062483097 Loss: 1.82957834418384e-09\n",
      "Iteration: 7394 lambda_n: 0.884787311152995 Loss: 1.8263074880257476e-09\n",
      "Iteration: 7395 lambda_n: 0.9545630190565623 Loss: 1.8231788557522503e-09\n",
      "Iteration: 7396 lambda_n: 0.9586083675426806 Loss: 1.8198181758614437e-09\n",
      "Iteration: 7397 lambda_n: 1.0192407399824883 Loss: 1.8164590850930799e-09\n",
      "Iteration: 7398 lambda_n: 1.0122041199843639 Loss: 1.812904363796866e-09\n",
      "Iteration: 7399 lambda_n: 0.9100866195434242 Loss: 1.8093918653459892e-09\n",
      "Iteration: 7400 lambda_n: 0.9635438726778834 Loss: 1.806249429668803e-09\n",
      "Iteration: 7401 lambda_n: 1.0131801723840814 Loss: 1.8029372953834373e-09\n",
      "Iteration: 7402 lambda_n: 1.0254989295101824 Loss: 1.7994710450074256e-09\n",
      "Iteration: 7403 lambda_n: 0.9357560882777762 Loss: 1.7959801236417532e-09\n",
      "Iteration: 7404 lambda_n: 1.0097745480484686 Loss: 1.7928107506916537e-09\n",
      "Iteration: 7405 lambda_n: 0.9989562166083601 Loss: 1.789406427936887e-09\n",
      "Iteration: 7406 lambda_n: 0.8817627420874843 Loss: 1.7860552980154904e-09\n",
      "Iteration: 7407 lambda_n: 0.9136131434715294 Loss: 1.783111831567558e-09\n",
      "Iteration: 7408 lambda_n: 0.9889924426256885 Loss: 1.7800752753986813e-09\n",
      "Iteration: 7409 lambda_n: 0.9518363437315935 Loss: 1.776802966296257e-09\n",
      "Iteration: 7410 lambda_n: 0.9580603159793577 Loss: 1.773668907588606e-09\n",
      "Iteration: 7411 lambda_n: 1.0062970816451209 Loss: 1.7705291266360599e-09\n",
      "Iteration: 7412 lambda_n: 0.9804532327486303 Loss: 1.767246813357479e-09\n",
      "Iteration: 7413 lambda_n: 0.9901527460598653 Loss: 1.764064635767693e-09\n",
      "Iteration: 7414 lambda_n: 0.9615801318408682 Loss: 1.7608664866408754e-09\n",
      "Iteration: 7415 lambda_n: 0.9984145000681562 Loss: 1.7577757574953366e-09\n",
      "Iteration: 7416 lambda_n: 0.8852454403597075 Loss: 1.754581828625803e-09\n",
      "Iteration: 7417 lambda_n: 0.8946103881911286 Loss: 1.751763828287077e-09\n",
      "Iteration: 7418 lambda_n: 0.9388424270330972 Loss: 1.7489284333932306e-09\n",
      "Iteration: 7419 lambda_n: 0.8971309766718634 Loss: 1.745965957054836e-09\n",
      "Iteration: 7420 lambda_n: 1.0239235444630181 Loss: 1.7431481846628612e-09\n",
      "Iteration: 7421 lambda_n: 0.9900577613571097 Loss: 1.7399464044646466e-09\n",
      "Iteration: 7422 lambda_n: 1.0094778006729506 Loss: 1.736866118801388e-09\n",
      "Iteration: 7423 lambda_n: 0.889876950570873 Loss: 1.7337407452646132e-09\n",
      "Iteration: 7424 lambda_n: 1.0140870378884304 Loss: 1.7309993393167915e-09\n",
      "Iteration: 7425 lambda_n: 0.927824447129589 Loss: 1.727889003668438e-09\n",
      "Iteration: 7426 lambda_n: 0.9382118102768356 Loss: 1.7250574459435045e-09\n",
      "Iteration: 7427 lambda_n: 0.981053745868217 Loss: 1.7222072920980184e-09\n",
      "Iteration: 7428 lambda_n: 0.8896718452629796 Loss: 1.719240775877252e-09\n",
      "Iteration: 7429 lambda_n: 0.8833605413895409 Loss: 1.7165635799130213e-09\n",
      "Iteration: 7430 lambda_n: 0.9076494950863944 Loss: 1.7139170297865444e-09\n",
      "Iteration: 7431 lambda_n: 0.9980615656331804 Loss: 1.7112095603271458e-09\n",
      "Iteration: 7432 lambda_n: 0.935948085547657 Loss: 1.7082457319351975e-09\n",
      "Iteration: 7433 lambda_n: 1.0191796319785547 Loss: 1.7054800241428603e-09\n",
      "Iteration: 7434 lambda_n: 1.023942376621378 Loss: 1.7024822816255916e-09\n",
      "Iteration: 7435 lambda_n: 1.0190626590227085 Loss: 1.6994856838336063e-09\n",
      "Iteration: 7436 lambda_n: 0.9573979505384658 Loss: 1.6965184230913742e-09\n",
      "Iteration: 7437 lambda_n: 0.999895417551103 Loss: 1.6937447232190326e-09\n",
      "Iteration: 7438 lambda_n: 0.9136554050314507 Loss: 1.6908615967409547e-09\n",
      "Iteration: 7439 lambda_n: 0.9976477767864633 Loss: 1.6882401217243493e-09\n",
      "Iteration: 7440 lambda_n: 1.0253124545135242 Loss: 1.6853905775847575e-09\n",
      "Iteration: 7441 lambda_n: 0.9816761782510905 Loss: 1.6824764424939585e-09\n",
      "Iteration: 7442 lambda_n: 0.8963172250698936 Loss: 1.6797004474872462e-09\n",
      "Iteration: 7443 lambda_n: 0.906699849620376 Loss: 1.6771781044050106e-09\n",
      "Iteration: 7444 lambda_n: 0.9093591561561462 Loss: 1.6746378385049496e-09\n",
      "Iteration: 7445 lambda_n: 0.9416699290116562 Loss: 1.672101534677567e-09\n",
      "Iteration: 7446 lambda_n: 0.9421763147274779 Loss: 1.6694869161695484e-09\n",
      "Iteration: 7447 lambda_n: 0.9134634038358823 Loss: 1.6668830575903814e-09\n",
      "Iteration: 7448 lambda_n: 0.9211328682081321 Loss: 1.6643703090811912e-09\n",
      "Iteration: 7449 lambda_n: 0.9534169588475482 Loss: 1.661847904120967e-09\n",
      "Iteration: 7450 lambda_n: 1.0021102229037269 Loss: 1.6592489824467316e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7451 lambda_n: 0.9999869339817246 Loss: 1.6565302105249414e-09\n",
      "Iteration: 7452 lambda_n: 1.0005973314624983 Loss: 1.6538306321862332e-09\n",
      "Iteration: 7453 lambda_n: 0.9928720658825775 Loss: 1.6511427699177769e-09\n",
      "Iteration: 7454 lambda_n: 1.0131918466347245 Loss: 1.6484888529439379e-09\n",
      "Iteration: 7455 lambda_n: 0.9514600212715602 Loss: 1.6457939287362085e-09\n",
      "Iteration: 7456 lambda_n: 1.0062437177346018 Loss: 1.6432758770773364e-09\n",
      "Iteration: 7457 lambda_n: 0.9407642607478246 Loss: 1.6406253774694854e-09\n",
      "Iteration: 7458 lambda_n: 0.8878163669941055 Loss: 1.6381596894035181e-09\n",
      "Iteration: 7459 lambda_n: 0.9322236771250021 Loss: 1.635843601456409e-09\n",
      "Iteration: 7460 lambda_n: 1.0019402633420003 Loss: 1.6334223542736744e-09\n",
      "Iteration: 7461 lambda_n: 0.9822174872175903 Loss: 1.6308320591873793e-09\n",
      "Iteration: 7462 lambda_n: 0.9327105628291753 Loss: 1.6283053532969952e-09\n",
      "Iteration: 7463 lambda_n: 1.0252114174557652 Loss: 1.6259176636507664e-09\n",
      "Iteration: 7464 lambda_n: 1.0078346764336084 Loss: 1.6233053196005684e-09\n",
      "Iteration: 7465 lambda_n: 0.9773550671439081 Loss: 1.6207502923118665e-09\n",
      "Iteration: 7466 lambda_n: 0.9539542376670466 Loss: 1.618284906259009e-09\n",
      "Iteration: 7467 lambda_n: 0.9646652275659373 Loss: 1.6158901974271018e-09\n",
      "Iteration: 7468 lambda_n: 1.023948065419257 Loss: 1.6134800559149369e-09\n",
      "Iteration: 7469 lambda_n: 0.9700262265277194 Loss: 1.6109340513555244e-09\n",
      "Iteration: 7470 lambda_n: 0.8840711150611239 Loss: 1.6085343503598405e-09\n",
      "Iteration: 7471 lambda_n: 0.9663963854985719 Loss: 1.6063578019147992e-09\n",
      "Iteration: 7472 lambda_n: 1.0249169428522142 Loss: 1.6039890171280483e-09\n",
      "Iteration: 7473 lambda_n: 0.9303714906307722 Loss: 1.6014888470830076e-09\n",
      "Iteration: 7474 lambda_n: 0.9575146151576265 Loss: 1.599230838159065e-09\n",
      "Iteration: 7475 lambda_n: 0.9556734341431059 Loss: 1.5969176886879347e-09\n",
      "Iteration: 7476 lambda_n: 0.995833448591502 Loss: 1.5946199648656532e-09\n",
      "Iteration: 7477 lambda_n: 0.8806699940335353 Loss: 1.592237040189838e-09\n",
      "Iteration: 7478 lambda_n: 0.887714473414992 Loss: 1.590140093315911e-09\n",
      "Iteration: 7479 lambda_n: 0.9546559762679991 Loss: 1.588035618755112e-09\n",
      "Iteration: 7480 lambda_n: 0.9585026033133143 Loss: 1.585782442931845e-09\n",
      "Iteration: 7481 lambda_n: 0.981866135194886 Loss: 1.5835309147282675e-09\n",
      "Iteration: 7482 lambda_n: 0.8779317408250058 Loss: 1.5812354976031832e-09\n",
      "Iteration: 7483 lambda_n: 0.8887494031153534 Loss: 1.579193068008232e-09\n",
      "Iteration: 7484 lambda_n: 0.9038487193545299 Loss: 1.5771344931812358e-09\n",
      "Iteration: 7485 lambda_n: 0.9501577469250642 Loss: 1.5750501998496059e-09\n",
      "Iteration: 7486 lambda_n: 0.9073861864965502 Loss: 1.5728689714668019e-09\n",
      "Iteration: 7487 lambda_n: 0.9205208063519199 Loss: 1.570795771697814e-09\n",
      "Iteration: 7488 lambda_n: 0.8738684191644993 Loss: 1.5687020553754295e-09\n",
      "Iteration: 7489 lambda_n: 0.9013880982678107 Loss: 1.5667235472710443e-09\n",
      "Iteration: 7490 lambda_n: 0.9680737900097499 Loss: 1.56469161391555e-09\n",
      "Iteration: 7491 lambda_n: 0.9048153877043601 Loss: 1.5625191536556075e-09\n",
      "Iteration: 7492 lambda_n: 0.9888042411928962 Loss: 1.5604984335275847e-09\n",
      "Iteration: 7493 lambda_n: 1.0180675363806089 Loss: 1.5583001035236206e-09\n",
      "Iteration: 7494 lambda_n: 0.9567599894965158 Loss: 1.55604785932144e-09\n",
      "Iteration: 7495 lambda_n: 0.9582014346485616 Loss: 1.553941978934823e-09\n",
      "Iteration: 7496 lambda_n: 0.887771281203995 Loss: 1.5518429855910475e-09\n",
      "Iteration: 7497 lambda_n: 0.9760381818973864 Loss: 1.5499075451874679e-09\n",
      "Iteration: 7498 lambda_n: 0.9145444239284704 Loss: 1.5477891066301746e-09\n",
      "Iteration: 7499 lambda_n: 1.0229476124745116 Loss: 1.5458137901140324e-09\n",
      "Iteration: 7500 lambda_n: 1.0130518167399705 Loss: 1.5436144246876212e-09\n",
      "Iteration: 7501 lambda_n: 1.0131656435543481 Loss: 1.54144746242377e-09\n",
      "Iteration: 7502 lambda_n: 0.8743014115616595 Loss: 1.5392912061606945e-09\n",
      "Iteration: 7503 lambda_n: 0.9943958628942651 Loss: 1.5374398690668644e-09\n",
      "Iteration: 7504 lambda_n: 0.930367635100546 Loss: 1.535343452730644e-09\n",
      "Iteration: 7505 lambda_n: 0.9561890005397407 Loss: 1.533391750185032e-09\n",
      "Iteration: 7506 lambda_n: 0.9793550121741951 Loss: 1.5313952094700656e-09\n",
      "Iteration: 7507 lambda_n: 0.9986274928750885 Loss: 1.5293600599655831e-09\n",
      "Iteration: 7508 lambda_n: 0.9017883970315461 Loss: 1.5272950200765442e-09\n",
      "Iteration: 7509 lambda_n: 0.9070714315260876 Loss: 1.5254395299882204e-09\n",
      "Iteration: 7510 lambda_n: 1.01173875692115 Loss: 1.523581590189993e-09\n",
      "Iteration: 7511 lambda_n: 0.9680186184500583 Loss: 1.5215186798289999e-09\n",
      "Iteration: 7512 lambda_n: 1.019345710409116 Loss: 1.5195548933744844e-09\n",
      "Iteration: 7513 lambda_n: 0.9510643571254503 Loss: 1.5174970006514848e-09\n",
      "Iteration: 7514 lambda_n: 0.9939383351053392 Loss: 1.5155867468639833e-09\n",
      "Iteration: 7515 lambda_n: 0.9519242271109737 Loss: 1.5135998908510404e-09\n",
      "Iteration: 7516 lambda_n: 1.0178060744602435 Loss: 1.5117064764865243e-09\n",
      "Iteration: 7517 lambda_n: 0.9617304355623157 Loss: 1.5096916908246772e-09\n",
      "Iteration: 7518 lambda_n: 0.8771821699484799 Loss: 1.5077976058376075e-09\n",
      "Iteration: 7519 lambda_n: 0.9188647971006769 Loss: 1.5060783518230808e-09\n",
      "Iteration: 7520 lambda_n: 0.8802949633555985 Loss: 1.5042853209221091e-09\n",
      "Iteration: 7521 lambda_n: 0.9812224653679633 Loss: 1.5025754618214395e-09\n",
      "Iteration: 7522 lambda_n: 0.9340123591962144 Loss: 1.5006779907236775e-09\n",
      "Iteration: 7523 lambda_n: 0.9586213071228024 Loss: 1.4988807058589508e-09\n",
      "Iteration: 7524 lambda_n: 0.9830109482685127 Loss: 1.4970447107294642e-09\n",
      "Iteration: 7525 lambda_n: 1.0162439565330195 Loss: 1.4951710689505203e-09\n",
      "Iteration: 7526 lambda_n: 0.9126282264394366 Loss: 1.4932436473936927e-09\n",
      "Iteration: 7527 lambda_n: 0.9518191374976774 Loss: 1.4915215676637433e-09\n",
      "Iteration: 7528 lambda_n: 0.8792804514021573 Loss: 1.4897337753513557e-09\n",
      "Iteration: 7529 lambda_n: 0.9993981836480175 Loss: 1.488090117409642e-09\n",
      "Iteration: 7530 lambda_n: 0.8962421787086977 Loss: 1.4862302052611396e-09\n",
      "Iteration: 7531 lambda_n: 1.0171530523200842 Loss: 1.4845706303404693e-09\n",
      "Iteration: 7532 lambda_n: 1.0236871506288552 Loss: 1.4826956767284236e-09\n",
      "Iteration: 7533 lambda_n: 0.8860363678538054 Loss: 1.4808183305930547e-09\n",
      "Iteration: 7534 lambda_n: 1.0151383682670234 Loss: 1.4792017737078706e-09\n",
      "Iteration: 7535 lambda_n: 0.9724079718537925 Loss: 1.4773579548591076e-09\n",
      "Iteration: 7536 lambda_n: 0.9005985016569834 Loss: 1.4756007695494147e-09\n",
      "Iteration: 7537 lambda_n: 0.9046377221932616 Loss: 1.4739813064356855e-09\n",
      "Iteration: 7538 lambda_n: 1.023217735587507 Loss: 1.472361961263023e-09\n",
      "Iteration: 7539 lambda_n: 0.9889216521978483 Loss: 1.470538726473194e-09\n",
      "Iteration: 7540 lambda_n: 0.898876426398775 Loss: 1.4687856856614748e-09\n",
      "Iteration: 7541 lambda_n: 0.9260921589325765 Loss: 1.4672001986602877e-09\n",
      "Iteration: 7542 lambda_n: 0.9730677836233026 Loss: 1.4655741212479081e-09\n",
      "Iteration: 7543 lambda_n: 1.0237628462602069 Loss: 1.4638735519628205e-09\n",
      "Iteration: 7544 lambda_n: 1.0219432529486105 Loss: 1.4620931859320225e-09\n",
      "Iteration: 7545 lambda_n: 0.9398570498890328 Loss: 1.460325168487926e-09\n",
      "Iteration: 7546 lambda_n: 0.9923593213102724 Loss: 1.4587075498938548e-09\n",
      "Iteration: 7547 lambda_n: 0.9302574898239555 Loss: 1.4570076888419548e-09\n",
      "Iteration: 7548 lambda_n: 0.9783308666051692 Loss: 1.4554221763155598e-09\n",
      "Iteration: 7549 lambda_n: 0.9920699195485054 Loss: 1.4537625842350918e-09\n",
      "Iteration: 7550 lambda_n: 0.9723641271306419 Loss: 1.4520880139597702e-09\n",
      "Iteration: 7551 lambda_n: 0.9886846968507997 Loss: 1.4504549499937215e-09\n",
      "Iteration: 7552 lambda_n: 0.9527027721081682 Loss: 1.448802650250341e-09\n",
      "Iteration: 7553 lambda_n: 0.9572025708980062 Loss: 1.4472184448486496e-09\n",
      "Iteration: 7554 lambda_n: 0.9587311750055136 Loss: 1.4456344348776136e-09\n",
      "Iteration: 7555 lambda_n: 0.9694521067925241 Loss: 1.4440555929264716e-09\n",
      "Iteration: 7556 lambda_n: 0.9693958014564225 Loss: 1.4424668589989944e-09\n",
      "Iteration: 7557 lambda_n: 0.999854495027154 Loss: 1.4408860292258972e-09\n",
      "Iteration: 7558 lambda_n: 1.0224298636108176 Loss: 1.4392635459417913e-09\n",
      "Iteration: 7559 lambda_n: 0.9123085612155623 Loss: 1.4376128535839972e-09\n",
      "Iteration: 7560 lambda_n: 1.0107454962245432 Loss: 1.4361475741484026e-09\n",
      "Iteration: 7561 lambda_n: 0.9035168291998216 Loss: 1.4345317318969403e-09\n",
      "Iteration: 7562 lambda_n: 0.9206598465060795 Loss: 1.4330947079839228e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7563 lambda_n: 0.9410824740932492 Loss: 1.4316371428696826e-09\n",
      "Iteration: 7564 lambda_n: 0.9611408772214254 Loss: 1.4301542246342745e-09\n",
      "Iteration: 7565 lambda_n: 0.8738400117398829 Loss: 1.428646952333294e-09\n",
      "Iteration: 7566 lambda_n: 0.9671191302362858 Loss: 1.4272832725130625e-09\n",
      "Iteration: 7567 lambda_n: 0.9272504553413495 Loss: 1.4257807489055932e-09\n",
      "Iteration: 7568 lambda_n: 0.9235397134070308 Loss: 1.4243472568675542e-09\n",
      "Iteration: 7569 lambda_n: 0.8771801445432613 Loss: 1.4229262356464105e-09\n",
      "Iteration: 7570 lambda_n: 0.962054981587572 Loss: 1.4215828954177375e-09\n",
      "Iteration: 7571 lambda_n: 0.9938039448994844 Loss: 1.4201161716018673e-09\n",
      "Iteration: 7572 lambda_n: 0.9995642822017896 Loss: 1.4186084860650268e-09\n",
      "Iteration: 7573 lambda_n: 1.018115447320198 Loss: 1.417099753498712e-09\n",
      "Iteration: 7574 lambda_n: 0.8743469041699614 Loss: 1.4155708551275402e-09\n",
      "Iteration: 7575 lambda_n: 0.9501496271532528 Loss: 1.4142646601462293e-09\n",
      "Iteration: 7576 lambda_n: 0.8831369017714686 Loss: 1.4128515628546864e-09\n",
      "Iteration: 7577 lambda_n: 0.9942928618744231 Loss: 1.411544497322841e-09\n",
      "Iteration: 7578 lambda_n: 0.8871713450468692 Loss: 1.410079582455174e-09\n",
      "Iteration: 7579 lambda_n: 0.9883548354881003 Loss: 1.4087791221284765e-09\n",
      "Iteration: 7580 lambda_n: 0.9376892460788299 Loss: 1.4073369348568656e-09\n",
      "Iteration: 7581 lambda_n: 0.9548015622362721 Loss: 1.4059755925885618e-09\n",
      "Iteration: 7582 lambda_n: 0.9660476422777502 Loss: 1.404596053778327e-09\n",
      "Iteration: 7583 lambda_n: 1.0205388987187944 Loss: 1.4032070971043255e-09\n",
      "Iteration: 7584 lambda_n: 0.9824926369631948 Loss: 1.401747071009736e-09\n",
      "Iteration: 7585 lambda_n: 0.9881648424336209 Loss: 1.4003488191434813e-09\n",
      "Iteration: 7586 lambda_n: 0.9313917803254158 Loss: 1.3989495900031578e-09\n",
      "Iteration: 7587 lambda_n: 0.9207485717320483 Loss: 1.3976374124414407e-09\n",
      "Iteration: 7588 lambda_n: 0.9745942151741059 Loss: 1.3963464345301463e-09\n",
      "Iteration: 7589 lambda_n: 1.0091127481293447 Loss: 1.3949864335249206e-09\n",
      "Iteration: 7590 lambda_n: 0.9700368055703615 Loss: 1.3935853195780163e-09\n",
      "Iteration: 7591 lambda_n: 0.971516588509228 Loss: 1.3922454368579668e-09\n",
      "Iteration: 7592 lambda_n: 0.9494007695388309 Loss: 1.3909101992110386e-09\n",
      "Iteration: 7593 lambda_n: 0.9910118682201902 Loss: 1.389611875994217e-09\n",
      "Iteration: 7594 lambda_n: 0.9488809806453649 Loss: 1.3882632840922309e-09\n",
      "Iteration: 7595 lambda_n: 1.0199439122065215 Loss: 1.3869785985819002e-09\n",
      "Iteration: 7596 lambda_n: 0.9839100095858226 Loss: 1.3856044687395588e-09\n",
      "Iteration: 7597 lambda_n: 1.0156053941094147 Loss: 1.3842858431738813e-09\n",
      "Iteration: 7598 lambda_n: 0.9237187112012244 Loss: 1.3829316561342426e-09\n",
      "Iteration: 7599 lambda_n: 0.9320815443442044 Loss: 1.3817064231683486e-09\n",
      "Iteration: 7600 lambda_n: 0.9006011599548723 Loss: 1.3804759957774168e-09\n",
      "Iteration: 7601 lambda_n: 0.9782685656793971 Loss: 1.3792928416545775e-09\n",
      "Iteration: 7602 lambda_n: 0.9172759525865049 Loss: 1.3780136450641444e-09\n",
      "Iteration: 7603 lambda_n: 0.8900910429748158 Loss: 1.3768202524594457e-09\n",
      "Iteration: 7604 lambda_n: 0.9317919393731345 Loss: 1.375667718935293e-09\n",
      "Iteration: 7605 lambda_n: 0.9243994569847691 Loss: 1.374466752011552e-09\n",
      "Iteration: 7606 lambda_n: 0.9484845990728632 Loss: 1.3732810582488582e-09\n",
      "Iteration: 7607 lambda_n: 0.9075680927816296 Loss: 1.3720702929215925e-09\n",
      "Iteration: 7608 lambda_n: 0.993861820050342 Loss: 1.37091744596688e-09\n",
      "Iteration: 7609 lambda_n: 0.9697103011873429 Loss: 1.3696609453429799e-09\n",
      "Iteration: 7610 lambda_n: 0.978624881363625 Loss: 1.3684412880894955e-09\n",
      "Iteration: 7611 lambda_n: 1.0210152071482286 Loss: 1.367216613602088e-09\n",
      "Iteration: 7612 lambda_n: 1.0082018634530612 Loss: 1.3659453872370585e-09\n",
      "Iteration: 7613 lambda_n: 1.0079724550011182 Loss: 1.3646967686068913e-09\n",
      "Iteration: 7614 lambda_n: 0.9706833921185184 Loss: 1.3634549765635103e-09\n",
      "Iteration: 7615 lambda_n: 0.9696977628108989 Loss: 1.3622653874305192e-09\n",
      "Iteration: 7616 lambda_n: 0.9574252686562065 Loss: 1.3610830018906385e-09\n",
      "Iteration: 7617 lambda_n: 1.0143688446084904 Loss: 1.359921464516058e-09\n",
      "Iteration: 7618 lambda_n: 0.9136594479389498 Loss: 1.3586969929038846e-09\n",
      "Iteration: 7619 lambda_n: 0.9951374899408012 Loss: 1.3575999003774588e-09\n",
      "Iteration: 7620 lambda_n: 0.9533794368867846 Loss: 1.3564106808804362e-09\n",
      "Iteration: 7621 lambda_n: 0.9732514996655514 Loss: 1.3552772630605967e-09\n",
      "Iteration: 7622 lambda_n: 0.9474792667985307 Loss: 1.354125982717008e-09\n",
      "Iteration: 7623 lambda_n: 0.9869251626830828 Loss: 1.3530108774847146e-09\n",
      "Iteration: 7624 lambda_n: 0.9254775913766222 Loss: 1.3518551045874883e-09\n",
      "Iteration: 7625 lambda_n: 0.8737394026958473 Loss: 1.3507768724761808e-09\n",
      "Iteration: 7626 lambda_n: 0.8810376879128229 Loss: 1.3497638371902154e-09\n",
      "Iteration: 7627 lambda_n: 0.9280194992782844 Loss: 1.3487470111514539e-09\n",
      "Iteration: 7628 lambda_n: 1.0227723812609268 Loss: 1.347680911119871e-09\n",
      "Iteration: 7629 lambda_n: 0.9922863991150559 Loss: 1.346511694161303e-09\n",
      "Iteration: 7630 lambda_n: 0.8985618654192934 Loss: 1.3453834000444601e-09\n",
      "Iteration: 7631 lambda_n: 0.8751376898159535 Loss: 1.34436697453156e-09\n",
      "Iteration: 7632 lambda_n: 0.9552114009690031 Loss: 1.3433817031287268e-09\n",
      "Iteration: 7633 lambda_n: 0.9168102192430972 Loss: 1.3423112387259014e-09\n",
      "Iteration: 7634 lambda_n: 0.9696446186065034 Loss: 1.3412889609314177e-09\n",
      "Iteration: 7635 lambda_n: 0.9077691138586323 Loss: 1.3402129950508067e-09\n",
      "Iteration: 7636 lambda_n: 1.0022839943489832 Loss: 1.3392107978117754e-09\n",
      "Iteration: 7637 lambda_n: 0.9306529920151425 Loss: 1.3381095720399655e-09\n",
      "Iteration: 7638 lambda_n: 0.8737195346530416 Loss: 1.3370924241945192e-09\n",
      "Iteration: 7639 lambda_n: 0.9128555684412849 Loss: 1.336142172568128e-09\n",
      "Iteration: 7640 lambda_n: 0.9513862013329394 Loss: 1.3351539350110784e-09\n",
      "Iteration: 7641 lambda_n: 0.9903610692198278 Loss: 1.3341289506029838e-09\n",
      "Iteration: 7642 lambda_n: 1.0180606161036632 Loss: 1.3330673363922153e-09\n",
      "Iteration: 7643 lambda_n: 0.9256732093935488 Loss: 1.3319817430178114e-09\n",
      "Iteration: 7644 lambda_n: 0.9008898393351595 Loss: 1.330999956218299e-09\n",
      "Iteration: 7645 lambda_n: 0.9724392759163452 Loss: 1.3300491221498385e-09\n",
      "Iteration: 7646 lambda_n: 0.9009189647363687 Loss: 1.329027681633814e-09\n",
      "Iteration: 7647 lambda_n: 0.9045411685697254 Loss: 1.3280862152156738e-09\n",
      "Iteration: 7648 lambda_n: 0.9314131456613084 Loss: 1.3271454772297509e-09\n",
      "Iteration: 7649 lambda_n: 0.9449420662518948 Loss: 1.3261814325604427e-09\n",
      "Iteration: 7650 lambda_n: 0.9263783989540199 Loss: 1.3252082166189499e-09\n",
      "Iteration: 7651 lambda_n: 0.9934789942779155 Loss: 1.3242589063397002e-09\n",
      "Iteration: 7652 lambda_n: 0.9944785565880584 Loss: 1.3232458489496898e-09\n",
      "Iteration: 7653 lambda_n: 0.9198398208491806 Loss: 1.3222371212588648e-09\n",
      "Iteration: 7654 lambda_n: 0.9822563628482668 Loss: 1.321309016726713e-09\n",
      "Iteration: 7655 lambda_n: 0.9201142068882748 Loss: 1.320322798099045e-09\n",
      "Iteration: 7656 lambda_n: 0.998693545824349 Loss: 1.3194037891320426e-09\n",
      "Iteration: 7657 lambda_n: 0.9323957051537596 Loss: 1.3184111851668558e-09\n",
      "Iteration: 7658 lambda_n: 1.0223753698654237 Loss: 1.317489399788269e-09\n",
      "Iteration: 7659 lambda_n: 0.9312609473921902 Loss: 1.3164837017353876e-09\n",
      "Iteration: 7660 lambda_n: 0.9699642080923571 Loss: 1.315572599388592e-09\n",
      "Iteration: 7661 lambda_n: 0.9888720343103365 Loss: 1.3146283611404339e-09\n",
      "Iteration: 7662 lambda_n: 0.9941977187768386 Loss: 1.3136707099850391e-09\n",
      "Iteration: 7663 lambda_n: 0.9061811510925148 Loss: 1.3127129938679596e-09\n",
      "Iteration: 7664 lambda_n: 0.8781961301807047 Loss: 1.3118446739822741e-09\n",
      "Iteration: 7665 lambda_n: 0.9391979932775953 Loss: 1.3110072480633679e-09\n",
      "Iteration: 7666 lambda_n: 0.8788402791443269 Loss: 1.310115874491095e-09\n",
      "Iteration: 7667 lambda_n: 0.9939153966926466 Loss: 1.3092859717234829e-09\n",
      "Iteration: 7668 lambda_n: 0.9991957132350954 Loss: 1.3083518426733124e-09\n",
      "Iteration: 7669 lambda_n: 0.8984671402179509 Loss: 1.3074177612941586e-09\n",
      "Iteration: 7670 lambda_n: 0.8843458826782128 Loss: 1.3065823322564014e-09\n",
      "Iteration: 7671 lambda_n: 1.0211699876733453 Loss: 1.3057639986180303e-09\n",
      "Iteration: 7672 lambda_n: 1.0086353763406308 Loss: 1.3048235740033338e-09\n",
      "Iteration: 7673 lambda_n: 0.9155069649532825 Loss: 1.303899792246147e-09\n",
      "Iteration: 7674 lambda_n: 0.9957137497846064 Loss: 1.3030658376758547e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7675 lambda_n: 0.9870062965497542 Loss: 1.3021633082816487e-09\n",
      "Iteration: 7676 lambda_n: 0.9216122426801235 Loss: 1.3012734699155181e-09\n",
      "Iteration: 7677 lambda_n: 0.8848548110223644 Loss: 1.3004469903927396e-09\n",
      "Iteration: 7678 lambda_n: 0.909369550018953 Loss: 1.2996574208029687e-09\n",
      "Iteration: 7679 lambda_n: 0.9381107484576949 Loss: 1.2988498501456157e-09\n",
      "Iteration: 7680 lambda_n: 0.9894808220607776 Loss: 1.2980208549063034e-09\n",
      "Iteration: 7681 lambda_n: 0.9113752374075345 Loss: 1.2971509055748872e-09\n",
      "Iteration: 7682 lambda_n: 0.9468072070341718 Loss: 1.2963538998670586e-09\n",
      "Iteration: 7683 lambda_n: 1.0160304285450965 Loss: 1.2955300052429306e-09\n",
      "Iteration: 7684 lambda_n: 0.894110298291774 Loss: 1.2946504196836242e-09\n",
      "Iteration: 7685 lambda_n: 0.8795912066564923 Loss: 1.2938806206556028e-09\n",
      "Iteration: 7686 lambda_n: 0.8877962281865686 Loss: 1.2931269861761857e-09\n",
      "Iteration: 7687 lambda_n: 0.9356449202488668 Loss: 1.2923699591425948e-09\n",
      "Iteration: 7688 lambda_n: 0.9443059641099967 Loss: 1.2915759889861435e-09\n",
      "Iteration: 7689 lambda_n: 1.005891710611047 Loss: 1.2907787461596231e-09\n",
      "Iteration: 7690 lambda_n: 0.9639776402550484 Loss: 1.2899338796240067e-09\n",
      "Iteration: 7691 lambda_n: 0.9288789166066873 Loss: 1.289128646587525e-09\n",
      "Iteration: 7692 lambda_n: 0.8910424160299661 Loss: 1.2883567869910621e-09\n",
      "Iteration: 7693 lambda_n: 1.0220857971629713 Loss: 1.2876201139222024e-09\n",
      "Iteration: 7694 lambda_n: 0.8928070891693534 Loss: 1.2867792401450227e-09\n",
      "Iteration: 7695 lambda_n: 0.9466943968036977 Loss: 1.2860487898883808e-09\n",
      "Iteration: 7696 lambda_n: 0.9530275318679821 Loss: 1.2852780387877346e-09\n",
      "Iteration: 7697 lambda_n: 0.9402499127937128 Loss: 1.2845061421044843e-09\n",
      "Iteration: 7698 lambda_n: 0.8937271017684839 Loss: 1.2837485672601459e-09\n",
      "Iteration: 7699 lambda_n: 0.963493109754854 Loss: 1.2830321677059962e-09\n",
      "Iteration: 7700 lambda_n: 0.9691367548786951 Loss: 1.2822636356207342e-09\n",
      "Iteration: 7701 lambda_n: 1.0185595925417812 Loss: 1.2814946891711025e-09\n",
      "Iteration: 7702 lambda_n: 0.9091075476965251 Loss: 1.2806908345519954e-09\n",
      "Iteration: 7703 lambda_n: 0.8988761022090241 Loss: 1.2799773408437382e-09\n",
      "Iteration: 7704 lambda_n: 0.9421909024584636 Loss: 1.2792754003132567e-09\n",
      "Iteration: 7705 lambda_n: 0.9889158257376285 Loss: 1.27854328095029e-09\n",
      "Iteration: 7706 lambda_n: 0.8990564541670923 Loss: 1.2777788435651436e-09\n",
      "Iteration: 7707 lambda_n: 0.8797314228555304 Loss: 1.2770876242995704e-09\n",
      "Iteration: 7708 lambda_n: 0.9356629356859809 Loss: 1.2764146109289505e-09\n",
      "Iteration: 7709 lambda_n: 0.9469613569071711 Loss: 1.2757022915883556e-09\n",
      "Iteration: 7710 lambda_n: 0.9937905672417808 Loss: 1.2749850895749874e-09\n",
      "Iteration: 7711 lambda_n: 0.9036097346554774 Loss: 1.2742363628772587e-09\n",
      "Iteration: 7712 lambda_n: 0.9364757886165063 Loss: 1.273559290173478e-09\n",
      "Iteration: 7713 lambda_n: 0.9000767829020867 Loss: 1.2728611060479973e-09\n",
      "Iteration: 7714 lambda_n: 0.8854456635566403 Loss: 1.2721935267308732e-09\n",
      "Iteration: 7715 lambda_n: 0.8972394170146848 Loss: 1.271540063516622e-09\n",
      "Iteration: 7716 lambda_n: 0.9133490157861055 Loss: 1.2708811455751162e-09\n",
      "Iteration: 7717 lambda_n: 0.9062476575873469 Loss: 1.2702137340637245e-09\n",
      "Iteration: 7718 lambda_n: 0.9061129673049994 Loss: 1.26955486180121e-09\n",
      "Iteration: 7719 lambda_n: 0.8807621385567648 Loss: 1.2688993996181766e-09\n",
      "Iteration: 7720 lambda_n: 0.9009698144587444 Loss: 1.2682654771456877e-09\n",
      "Iteration: 7721 lambda_n: 0.9146185947178593 Loss: 1.267620178922516e-09\n",
      "Iteration: 7722 lambda_n: 0.89802082925983 Loss: 1.2669683875480806e-09\n",
      "Iteration: 7723 lambda_n: 0.939748313590307 Loss: 1.2663316858474838e-09\n",
      "Iteration: 7724 lambda_n: 0.9860835898686803 Loss: 1.2656687360494297e-09\n",
      "Iteration: 7725 lambda_n: 0.9583032792790002 Loss: 1.2649767421143428e-09\n",
      "Iteration: 7726 lambda_n: 0.9405034382248254 Loss: 1.2643079384728433e-09\n",
      "Iteration: 7727 lambda_n: 0.9631622996200623 Loss: 1.26365505802117e-09\n",
      "Iteration: 7728 lambda_n: 1.0111973957863523 Loss: 1.262989965658484e-09\n",
      "Iteration: 7729 lambda_n: 0.9515446165859208 Loss: 1.2622954696516333e-09\n",
      "Iteration: 7730 lambda_n: 0.9133144772679465 Loss: 1.261645620355141e-09\n",
      "Iteration: 7731 lambda_n: 0.9036356071198747 Loss: 1.2610251945284145e-09\n",
      "Iteration: 7732 lambda_n: 1.0243263093347366 Loss: 1.2604144854165016e-09\n",
      "Iteration: 7733 lambda_n: 0.9644175960537186 Loss: 1.2597257438723792e-09\n",
      "Iteration: 7734 lambda_n: 0.986245470289416 Loss: 1.2590809846226074e-09\n",
      "Iteration: 7735 lambda_n: 0.9903910585332684 Loss: 1.258425209414855e-09\n",
      "Iteration: 7736 lambda_n: 0.9801712987160347 Loss: 1.257770320120429e-09\n",
      "Iteration: 7737 lambda_n: 0.9726876038253647 Loss: 1.2571257986103615e-09\n",
      "Iteration: 7738 lambda_n: 0.9479725098990444 Loss: 1.2564897211988284e-09\n",
      "Iteration: 7739 lambda_n: 0.9514464445670564 Loss: 1.255873189078501e-09\n",
      "Iteration: 7740 lambda_n: 0.9691164239472633 Loss: 1.255257697255486e-09\n",
      "Iteration: 7741 lambda_n: 0.9145867311568018 Loss: 1.254634135523183e-09\n",
      "Iteration: 7742 lambda_n: 0.9968493307974744 Loss: 1.2540488616470369e-09\n",
      "Iteration: 7743 lambda_n: 1.0145363999889812 Loss: 1.2534142601840467e-09\n",
      "Iteration: 7744 lambda_n: 0.9490888057668303 Loss: 1.252772035892852e-09\n",
      "Iteration: 7745 lambda_n: 0.8850506859642981 Loss: 1.2521746680706924e-09\n",
      "Iteration: 7746 lambda_n: 0.9158188187358263 Loss: 1.2516205812855766e-09\n",
      "Iteration: 7747 lambda_n: 0.8961913852795889 Loss: 1.2510501124750917e-09\n",
      "Iteration: 7748 lambda_n: 0.8967461564318238 Loss: 1.2504947583458644e-09\n",
      "Iteration: 7749 lambda_n: 0.9185612134783576 Loss: 1.2499418792218754e-09\n",
      "Iteration: 7750 lambda_n: 0.9269622155187566 Loss: 1.2493784384907734e-09\n",
      "Iteration: 7751 lambda_n: 0.9795533809547594 Loss: 1.248812802723241e-09\n",
      "Iteration: 7752 lambda_n: 0.9743498963917065 Loss: 1.2482182340593845e-09\n",
      "Iteration: 7753 lambda_n: 1.007347766406121 Loss: 1.2476301186231429e-09\n",
      "Iteration: 7754 lambda_n: 0.9807005035378359 Loss: 1.2470254560567949e-09\n",
      "Iteration: 7755 lambda_n: 1.0094714723124882 Loss: 1.24644016129111e-09\n",
      "Iteration: 7756 lambda_n: 0.90742366458902 Loss: 1.2458410667427585e-09\n",
      "Iteration: 7757 lambda_n: 1.0117988273156073 Loss: 1.2453056118100095e-09\n",
      "Iteration: 7758 lambda_n: 0.9169269996426805 Loss: 1.2447116770748718e-09\n",
      "Iteration: 7759 lambda_n: 0.8911737821587367 Loss: 1.2441765153497103e-09\n",
      "Iteration: 7760 lambda_n: 0.8997921429040229 Loss: 1.243659100074091e-09\n",
      "Iteration: 7761 lambda_n: 1.0069135518015733 Loss: 1.2431393426545945e-09\n",
      "Iteration: 7762 lambda_n: 0.8759369300268322 Loss: 1.2425607277294006e-09\n",
      "Iteration: 7763 lambda_n: 1.01558986085808 Loss: 1.2420602429286729e-09\n",
      "Iteration: 7764 lambda_n: 0.947054547329579 Loss: 1.241482906278923e-09\n",
      "Iteration: 7765 lambda_n: 1.0223164164482543 Loss: 1.2409476474834157e-09\n",
      "Iteration: 7766 lambda_n: 1.0122760432460467 Loss: 1.2403730085173626e-09\n",
      "Iteration: 7767 lambda_n: 0.9134051350738193 Loss: 1.2398073445090879e-09\n",
      "Iteration: 7768 lambda_n: 0.8792164493102802 Loss: 1.2392998736447039e-09\n",
      "Iteration: 7769 lambda_n: 1.006455508702319 Loss: 1.2388139524919602e-09\n",
      "Iteration: 7770 lambda_n: 0.9695301149487181 Loss: 1.2382605524417172e-09\n",
      "Iteration: 7771 lambda_n: 0.9361496175383712 Loss: 1.2377305327815874e-09\n",
      "Iteration: 7772 lambda_n: 0.9215125153443325 Loss: 1.2372216142758326e-09\n",
      "Iteration: 7773 lambda_n: 0.9548600935448087 Loss: 1.2367233439762657e-09\n",
      "Iteration: 7774 lambda_n: 0.9917840545768919 Loss: 1.2362097883035151e-09\n",
      "Iteration: 7775 lambda_n: 0.9362246886741382 Loss: 1.2356793197728406e-09\n",
      "Iteration: 7776 lambda_n: 0.9043695538057663 Loss: 1.2351814232891745e-09\n",
      "Iteration: 7777 lambda_n: 0.9289755307213837 Loss: 1.2347030629851673e-09\n",
      "Iteration: 7778 lambda_n: 0.8777819303329911 Loss: 1.2342142603685379e-09\n",
      "Iteration: 7779 lambda_n: 1.0059408775047283 Loss: 1.2337548632531065e-09\n",
      "Iteration: 7780 lambda_n: 0.8959070758942229 Loss: 1.2332310983060733e-09\n",
      "Iteration: 7781 lambda_n: 0.9282751701287366 Loss: 1.2327673158971289e-09\n",
      "Iteration: 7782 lambda_n: 0.9790782725363218 Loss: 1.2322892749154788e-09\n",
      "Iteration: 7783 lambda_n: 1.0206623917663302 Loss: 1.2317877983184945e-09\n",
      "Iteration: 7784 lambda_n: 0.9004162650542452 Loss: 1.2312680006251633e-09\n",
      "Iteration: 7785 lambda_n: 0.9946086795211855 Loss: 1.2308121273065053e-09\n",
      "Iteration: 7786 lambda_n: 1.0254980989087792 Loss: 1.2303112166077823e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7787 lambda_n: 0.9196555594732865 Loss: 1.2297977461476457e-09\n",
      "Iteration: 7788 lambda_n: 0.8800025071094068 Loss: 1.2293399931500659e-09\n",
      "Iteration: 7789 lambda_n: 0.9055954015709954 Loss: 1.2289043118289879e-09\n",
      "Iteration: 7790 lambda_n: 0.943425230685108 Loss: 1.228458255397412e-09\n",
      "Iteration: 7791 lambda_n: 0.9944337800396114 Loss: 1.227996028391558e-09\n",
      "Iteration: 7792 lambda_n: 0.9226522189187134 Loss: 1.2275115046910466e-09\n",
      "Iteration: 7793 lambda_n: 0.9511586135681608 Loss: 1.2270645447888822e-09\n",
      "Iteration: 7794 lambda_n: 0.9724014248621202 Loss: 1.226606258922115e-09\n",
      "Iteration: 7795 lambda_n: 0.9351594027126494 Loss: 1.2261403423316165e-09\n",
      "Iteration: 7796 lambda_n: 0.9100355246341257 Loss: 1.2256948073223864e-09\n",
      "Iteration: 7797 lambda_n: 0.9802526559265322 Loss: 1.2252636102475939e-09\n",
      "Iteration: 7798 lambda_n: 0.8980312484951606 Loss: 1.2248016287047931e-09\n",
      "Iteration: 7799 lambda_n: 1.0100158852189758 Loss: 1.2243808083743998e-09\n",
      "Iteration: 7800 lambda_n: 0.8777757850402967 Loss: 1.2239100299621715e-09\n",
      "Iteration: 7801 lambda_n: 0.8935263446716759 Loss: 1.2235032815124159e-09\n",
      "Iteration: 7802 lambda_n: 0.9208686026859416 Loss: 1.2230913620100177e-09\n",
      "Iteration: 7803 lambda_n: 0.9045101391763988 Loss: 1.2226690688117752e-09\n",
      "Iteration: 7804 lambda_n: 0.9805675382012912 Loss: 1.2222565159939119e-09\n",
      "Iteration: 7805 lambda_n: 1.0199591952958074 Loss: 1.2218116634418054e-09\n",
      "Iteration: 7806 lambda_n: 0.9284964564991361 Loss: 1.2213516203929775e-09\n",
      "Iteration: 7807 lambda_n: 0.8804064868020446 Loss: 1.2209353183356394e-09\n",
      "Iteration: 7808 lambda_n: 0.9605026678669258 Loss: 1.2205427232423912e-09\n",
      "Iteration: 7809 lambda_n: 0.8851519196735618 Loss: 1.2201166422148235e-09\n",
      "Iteration: 7810 lambda_n: 0.9875782879568062 Loss: 1.2197261955749482e-09\n",
      "Iteration: 7811 lambda_n: 0.955507935707659 Loss: 1.2192928630680683e-09\n",
      "Iteration: 7812 lambda_n: 0.9884289141375896 Loss: 1.2188760341112545e-09\n",
      "Iteration: 7813 lambda_n: 0.9408024652938539 Loss: 1.2184472825462248e-09\n",
      "Iteration: 7814 lambda_n: 0.9078574578475028 Loss: 1.2180415601033165e-09\n",
      "Iteration: 7815 lambda_n: 0.9295173366820451 Loss: 1.2176522114584377e-09\n",
      "Iteration: 7816 lambda_n: 1.022025166123444 Loss: 1.2172557162041413e-09\n",
      "Iteration: 7817 lambda_n: 1.025786500832436 Loss: 1.2168221681183454e-09\n",
      "Iteration: 7818 lambda_n: 1.0140069585618232 Loss: 1.2163896513560463e-09\n",
      "Iteration: 7819 lambda_n: 0.9244839530166047 Loss: 1.215964706051245e-09\n",
      "Iteration: 7820 lambda_n: 0.9496181779898297 Loss: 1.2155795794126654e-09\n",
      "Iteration: 7821 lambda_n: 0.9305165351602632 Loss: 1.2151861537938997e-09\n",
      "Iteration: 7822 lambda_n: 0.939437573624642 Loss: 1.2148028053357145e-09\n",
      "Iteration: 7823 lambda_n: 1.023417929708198 Loss: 1.2144179170177956e-09\n",
      "Iteration: 7824 lambda_n: 0.8861098339787223 Loss: 1.2140009754233976e-09\n",
      "Iteration: 7825 lambda_n: 0.9516701256584243 Loss: 1.2136421408007615e-09\n",
      "Iteration: 7826 lambda_n: 1.0125527610263227 Loss: 1.213258799499312e-09\n",
      "Iteration: 7827 lambda_n: 0.9466749149487944 Loss: 1.2128532511701243e-09\n",
      "Iteration: 7828 lambda_n: 0.9492275502007752 Loss: 1.212476350754954e-09\n",
      "Iteration: 7829 lambda_n: 0.9290791041385775 Loss: 1.2121005602606128e-09\n",
      "Iteration: 7830 lambda_n: 0.9679293990754569 Loss: 1.2117348300049463e-09\n",
      "Iteration: 7831 lambda_n: 0.9175479913005995 Loss: 1.2113559126849507e-09\n",
      "Iteration: 7832 lambda_n: 0.9488612603479043 Loss: 1.2109987811291536e-09\n",
      "Iteration: 7833 lambda_n: 0.9560608664919837 Loss: 1.2106314833506648e-09\n",
      "Iteration: 7834 lambda_n: 0.9158303368506591 Loss: 1.2102634864125573e-09\n",
      "Iteration: 7835 lambda_n: 0.9269486528543244 Loss: 1.2099129779042718e-09\n",
      "Iteration: 7836 lambda_n: 0.8817021689871026 Loss: 1.209560154277593e-09\n",
      "Iteration: 7837 lambda_n: 0.9449549092441958 Loss: 1.2092263995014227e-09\n",
      "Iteration: 7838 lambda_n: 0.9842579120989449 Loss: 1.2088705879891702e-09\n",
      "Iteration: 7839 lambda_n: 0.9947096541665881 Loss: 1.2085020788161766e-09\n",
      "Iteration: 7840 lambda_n: 1.0257403664607005 Loss: 1.208131855704054e-09\n",
      "Iteration: 7841 lambda_n: 0.9134302948501524 Loss: 1.207752356265724e-09\n",
      "Iteration: 7842 lambda_n: 0.896676601094509 Loss: 1.2074164547525657e-09\n",
      "Iteration: 7843 lambda_n: 0.900840445367476 Loss: 1.2070885160992328e-09\n",
      "Iteration: 7844 lambda_n: 0.9292257621113664 Loss: 1.2067608189047436e-09\n",
      "Iteration: 7845 lambda_n: 0.9882294183895893 Loss: 1.2064246243887752e-09\n",
      "Iteration: 7846 lambda_n: 0.9778165923983568 Loss: 1.2060690864985038e-09\n",
      "Iteration: 7847 lambda_n: 0.9977768167470951 Loss: 1.2057193688157075e-09\n",
      "Iteration: 7848 lambda_n: 0.8864932609808757 Loss: 1.205364608428999e-09\n",
      "Iteration: 7849 lambda_n: 0.9569878515758579 Loss: 1.205051277241611e-09\n",
      "Iteration: 7850 lambda_n: 0.8797564643711745 Loss: 1.204714845577013e-09\n",
      "Iteration: 7851 lambda_n: 0.9397801472459705 Loss: 1.2044073216576448e-09\n",
      "Iteration: 7852 lambda_n: 0.9265010856869181 Loss: 1.2040805615343649e-09\n",
      "Iteration: 7853 lambda_n: 0.9262793756173044 Loss: 1.2037602306162266e-09\n",
      "Iteration: 7854 lambda_n: 0.9900326763494274 Loss: 1.2034417588349024e-09\n",
      "Iteration: 7855 lambda_n: 0.9809480006260219 Loss: 1.2031032727943991e-09\n",
      "Iteration: 7856 lambda_n: 0.9120249522173411 Loss: 1.2027698890686084e-09\n",
      "Iteration: 7857 lambda_n: 0.9904972181834164 Loss: 1.2024617468196172e-09\n",
      "Iteration: 7858 lambda_n: 0.9879434769311187 Loss: 1.202128933820137e-09\n",
      "Iteration: 7859 lambda_n: 0.8881241024647518 Loss: 1.201798956238156e-09\n",
      "Iteration: 7860 lambda_n: 0.9535353169908578 Loss: 1.201504067029362e-09\n",
      "Iteration: 7861 lambda_n: 0.8755990220760532 Loss: 1.2011891599801578e-09\n",
      "Iteration: 7862 lambda_n: 0.9642325301846968 Loss: 1.200901633714899e-09\n",
      "Iteration: 7863 lambda_n: 0.9129427965055794 Loss: 1.2005866922270943e-09\n",
      "Iteration: 7864 lambda_n: 0.9527023219925385 Loss: 1.2002902223060711e-09\n",
      "Iteration: 7865 lambda_n: 0.9709910774290174 Loss: 1.1999825489487395e-09\n",
      "Iteration: 7866 lambda_n: 0.8743919275090891 Loss: 1.1996707786655662e-09\n",
      "Iteration: 7867 lambda_n: 1.0033515834218654 Loss: 1.199391647966318e-09\n",
      "Iteration: 7868 lambda_n: 0.984281621038391 Loss: 1.1990730589639491e-09\n",
      "Iteration: 7869 lambda_n: 0.9505467056383233 Loss: 1.1987624178703746e-09\n",
      "Iteration: 7870 lambda_n: 0.901850849284272 Loss: 1.198464197889472e-09\n",
      "Iteration: 7871 lambda_n: 0.9535766523232679 Loss: 1.1981828653240486e-09\n",
      "Iteration: 7872 lambda_n: 0.8784800545119106 Loss: 1.1978870298723893e-09\n",
      "Iteration: 7873 lambda_n: 1.0213682520582912 Loss: 1.1976160427770626e-09\n",
      "Iteration: 7874 lambda_n: 0.932614988017217 Loss: 1.1973026789857262e-09\n",
      "Iteration: 7875 lambda_n: 0.9060494585064948 Loss: 1.1970182940929725e-09\n",
      "Iteration: 7876 lambda_n: 0.9934363303012037 Loss: 1.196743563595455e-09\n",
      "Iteration: 7877 lambda_n: 0.9773794422574059 Loss: 1.1964439979642662e-09\n",
      "Iteration: 7878 lambda_n: 1.0149734357409028 Loss: 1.1961510396213839e-09\n",
      "Iteration: 7879 lambda_n: 1.0066890754198499 Loss: 1.1958486250194815e-09\n",
      "Iteration: 7880 lambda_n: 0.9565790891749817 Loss: 1.1955505071610633e-09\n",
      "Iteration: 7881 lambda_n: 0.892257776780737 Loss: 1.1952689538126393e-09\n",
      "Iteration: 7882 lambda_n: 0.9317033440787983 Loss: 1.1950078397662614e-09\n",
      "Iteration: 7883 lambda_n: 0.8763047706269742 Loss: 1.1947366571039062e-09\n",
      "Iteration: 7884 lambda_n: 0.9735498368140778 Loss: 1.194483027906286e-09\n",
      "Iteration: 7885 lambda_n: 0.9019083694620305 Loss: 1.1942027610201937e-09\n",
      "Iteration: 7886 lambda_n: 0.9820800125817534 Loss: 1.1939446366274582e-09\n",
      "Iteration: 7887 lambda_n: 0.9164231541336988 Loss: 1.1936651171012455e-09\n",
      "Iteration: 7888 lambda_n: 0.8880925524830209 Loss: 1.1934058238174198e-09\n",
      "Iteration: 7889 lambda_n: 1.014458973447874 Loss: 1.193155938571306e-09\n",
      "Iteration: 7890 lambda_n: 0.9908367278086654 Loss: 1.1928720478465942e-09\n",
      "Iteration: 7891 lambda_n: 1.0157944963307521 Loss: 1.1925964712342467e-09\n",
      "Iteration: 7892 lambda_n: 0.9003450765303797 Loss: 1.1923156488024085e-09\n",
      "Iteration: 7893 lambda_n: 0.9923756150468382 Loss: 1.1920682569426624e-09\n",
      "Iteration: 7894 lambda_n: 0.9832506010225376 Loss: 1.1917970792136025e-09\n",
      "Iteration: 7895 lambda_n: 0.9319192238944308 Loss: 1.1915300086288662e-09\n",
      "Iteration: 7896 lambda_n: 0.8809093279551515 Loss: 1.191278384115682e-09\n",
      "Iteration: 7897 lambda_n: 0.9848412338459973 Loss: 1.1910418674735713e-09\n",
      "Iteration: 7898 lambda_n: 0.9934583434274699 Loss: 1.1907788778447442e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7899 lambda_n: 0.9435075187398875 Loss: 1.1905151723045898e-09\n",
      "Iteration: 7900 lambda_n: 0.96658403978772 Loss: 1.1902662194036946e-09\n",
      "Iteration: 7901 lambda_n: 0.9305478252510837 Loss: 1.1900126450147888e-09\n",
      "Iteration: 7902 lambda_n: 1.0163856752653162 Loss: 1.1897699476020248e-09\n",
      "Iteration: 7903 lambda_n: 0.9014426070990034 Loss: 1.1895063733535924e-09\n",
      "Iteration: 7904 lambda_n: 0.9654811480609913 Loss: 1.1892740235577976e-09\n",
      "Iteration: 7905 lambda_n: 0.8946589352681833 Loss: 1.1890265429488905e-09\n",
      "Iteration: 7906 lambda_n: 0.9946703207335751 Loss: 1.1887985525844386e-09\n",
      "Iteration: 7907 lambda_n: 0.9485568694509587 Loss: 1.1885464631084965e-09\n",
      "Iteration: 7908 lambda_n: 1.027517445121603 Loss: 1.1883075043543931e-09\n",
      "Iteration: 7909 lambda_n: 0.8929172903083803 Loss: 1.1880501610427283e-09\n",
      "Iteration: 7910 lambda_n: 0.8863237136042538 Loss: 1.187827898126191e-09\n",
      "Iteration: 7911 lambda_n: 0.9430061915660851 Loss: 1.1876084719609954e-09\n",
      "Iteration: 7912 lambda_n: 0.9808265379237845 Loss: 1.1873762785768863e-09\n",
      "Iteration: 7913 lambda_n: 0.9057559582201126 Loss: 1.1871361588624769e-09\n",
      "Iteration: 7914 lambda_n: 0.8825221392715207 Loss: 1.186915729390979e-09\n",
      "Iteration: 7915 lambda_n: 1.016821342573299 Loss: 1.1867021229357634e-09\n",
      "Iteration: 7916 lambda_n: 0.9243517021356777 Loss: 1.1864964440750032e-09\n",
      "Iteration: 7917 lambda_n: 0.9114101684820153 Loss: 1.186573151982729e-09\n",
      "Iteration: 7918 lambda_n: 0.9078951587084304 Loss: 1.1866486940080803e-09\n",
      "Iteration: 7919 lambda_n: 0.9414223069533584 Loss: 1.1867238540526976e-09\n",
      "Iteration: 7920 lambda_n: 0.9701939885582068 Loss: 1.1868016988947717e-09\n",
      "Iteration: 7921 lambda_n: 0.8849273327505964 Loss: 1.1868818228603634e-09\n",
      "Iteration: 7922 lambda_n: 0.9873215276852797 Loss: 1.1869548018798924e-09\n",
      "Iteration: 7923 lambda_n: 0.8781817639998822 Loss: 1.1870361359684667e-09\n",
      "Iteration: 7924 lambda_n: 0.9648647654458375 Loss: 1.1871083716259418e-09\n",
      "Iteration: 7925 lambda_n: 0.9533316325286785 Loss: 1.187187645888616e-09\n",
      "Iteration: 7926 lambda_n: 0.9533047927501795 Loss: 1.1872658692135777e-09\n",
      "Iteration: 7927 lambda_n: 1.0069809732733859 Loss: 1.1873439782324256e-09\n",
      "Iteration: 7928 lambda_n: 0.8774021575179454 Loss: 1.1874263774057299e-09\n",
      "Iteration: 7929 lambda_n: 0.9557491381624545 Loss: 1.187498053218594e-09\n",
      "Iteration: 7930 lambda_n: 0.913810432855045 Loss: 1.1875760374198823e-09\n",
      "Iteration: 7931 lambda_n: 0.8992901180469989 Loss: 1.1876504855501832e-09\n",
      "Iteration: 7932 lambda_n: 0.8878778864146406 Loss: 1.1877236512488329e-09\n",
      "Iteration: 7933 lambda_n: 0.9400408323242726 Loss: 1.1877957842809165e-09\n",
      "Iteration: 7934 lambda_n: 0.9415104394586595 Loss: 1.1878720612633792e-09\n",
      "Iteration: 7935 lambda_n: 0.9776091487579566 Loss: 1.1879483394799766e-09\n",
      "Iteration: 7936 lambda_n: 0.894267031871438 Loss: 1.1880274314307993e-09\n",
      "Iteration: 7937 lambda_n: 0.9712728889984729 Loss: 1.1880996569973077e-09\n",
      "Iteration: 7938 lambda_n: 0.9053818120341431 Loss: 1.1881780009424066e-09\n",
      "Iteration: 7939 lambda_n: 0.8960030439003299 Loss: 1.1882509077087352e-09\n",
      "Iteration: 7940 lambda_n: 0.9741782355903168 Loss: 1.1883229497551957e-09\n",
      "Iteration: 7941 lambda_n: 0.9668066857465677 Loss: 1.1884011727518316e-09\n",
      "Iteration: 7942 lambda_n: 0.9495545877973907 Loss: 1.188478680617706e-09\n",
      "Iteration: 7943 lambda_n: 0.9393976552723465 Loss: 1.1885546761878988e-09\n",
      "Iteration: 7944 lambda_n: 0.9575006130508242 Loss: 1.1886297332050353e-09\n",
      "Iteration: 7945 lambda_n: 0.9993648924238479 Loss: 1.188706122133468e-09\n",
      "Iteration: 7946 lambda_n: 0.9444621462687096 Loss: 1.1887857205413173e-09\n",
      "Iteration: 7947 lambda_n: 0.9447591539696436 Loss: 1.1888608091618682e-09\n",
      "Iteration: 7948 lambda_n: 0.9911822526331626 Loss: 1.1889357927613767e-09\n",
      "Iteration: 7949 lambda_n: 0.9714923388916828 Loss: 1.1890143405020677e-09\n",
      "Iteration: 7950 lambda_n: 0.9769818593891653 Loss: 1.1890911925801496e-09\n",
      "Iteration: 7951 lambda_n: 0.9806404409051618 Loss: 1.1891683350881018e-09\n",
      "Iteration: 7952 lambda_n: 0.9864984410025643 Loss: 1.189245635322735e-09\n",
      "Iteration: 7953 lambda_n: 1.0065276969570007 Loss: 1.1893232532220705e-09\n",
      "Iteration: 7954 lambda_n: 1.0031732100384279 Loss: 1.1894023103447869e-09\n",
      "Iteration: 7955 lambda_n: 0.9858143212240423 Loss: 1.1894809551815925e-09\n",
      "Iteration: 7956 lambda_n: 0.9731258048119051 Loss: 1.1895580856891025e-09\n",
      "Iteration: 7957 lambda_n: 0.8942940384685284 Loss: 1.1896340843252622e-09\n",
      "Iteration: 7958 lambda_n: 0.9178452961918282 Loss: 1.1897037944889306e-09\n",
      "Iteration: 7959 lambda_n: 1.0123240368310937 Loss: 1.1897752086562835e-09\n",
      "Iteration: 7960 lambda_n: 0.907055538335264 Loss: 1.1898538542650607e-09\n",
      "Iteration: 7961 lambda_n: 0.967119404411058 Loss: 1.189924161157997e-09\n",
      "Iteration: 7962 lambda_n: 0.9154111933827038 Loss: 1.189999009150773e-09\n",
      "Iteration: 7963 lambda_n: 0.8755377164191734 Loss: 1.1900697053595912e-09\n",
      "Iteration: 7964 lambda_n: 0.9232094599865713 Loss: 1.1901372022488606e-09\n",
      "Iteration: 7965 lambda_n: 0.8900230709765315 Loss: 1.1902082549631025e-09\n",
      "Iteration: 7966 lambda_n: 0.8896411859002638 Loss: 1.1902766190558364e-09\n",
      "Iteration: 7967 lambda_n: 1.0148354958962886 Loss: 1.1903448301267787e-09\n",
      "Iteration: 7968 lambda_n: 0.9361154242122379 Loss: 1.190422511323416e-09\n",
      "Iteration: 7969 lambda_n: 1.0166150093429778 Loss: 1.1904940082712336e-09\n",
      "Iteration: 7970 lambda_n: 0.9598930425886354 Loss: 1.1905715140055535e-09\n",
      "Iteration: 7971 lambda_n: 0.927251695284606 Loss: 1.1906445350666209e-09\n",
      "Iteration: 7972 lambda_n: 0.952323009074702 Loss: 1.1907149254951179e-09\n",
      "Iteration: 7973 lambda_n: 0.9818993493190759 Loss: 1.1907870854436223e-09\n",
      "Iteration: 7974 lambda_n: 0.9177979884830965 Loss: 1.1908613327363551e-09\n",
      "Iteration: 7975 lambda_n: 0.942311430097258 Loss: 1.1909305820911257e-09\n",
      "Iteration: 7976 lambda_n: 0.986946174005309 Loss: 1.191001541825551e-09\n",
      "Iteration: 7977 lambda_n: 0.9736083393660316 Loss: 1.1910757175833345e-09\n",
      "Iteration: 7978 lambda_n: 1.0068808283788193 Loss: 1.1911487309492155e-09\n",
      "Iteration: 7979 lambda_n: 0.9527237480082122 Loss: 1.1912240811186775e-09\n",
      "Iteration: 7980 lambda_n: 0.9286218773855791 Loss: 1.1912952168391868e-09\n",
      "Iteration: 7981 lambda_n: 0.9182840755768722 Loss: 1.1913644031845083e-09\n",
      "Iteration: 7982 lambda_n: 0.9464099698417096 Loss: 1.1914326740183483e-09\n",
      "Iteration: 7983 lambda_n: 0.8829388616286153 Loss: 1.1915028943669879e-09\n",
      "Iteration: 7984 lambda_n: 0.8921989243275514 Loss: 1.1915682581068077e-09\n",
      "Iteration: 7985 lambda_n: 0.9354463980465306 Loss: 1.1916341791802172e-09\n",
      "Iteration: 7986 lambda_n: 0.8804967284724539 Loss: 1.1917031582797337e-09\n",
      "Iteration: 7987 lambda_n: 0.92382474051578 Loss: 1.1917679437456616e-09\n",
      "Iteration: 7988 lambda_n: 1.0081676714011125 Loss: 1.1918357807010208e-09\n",
      "Iteration: 7989 lambda_n: 0.9846444075269143 Loss: 1.1919096594453839e-09\n",
      "Iteration: 7990 lambda_n: 0.946183451499157 Loss: 1.1919816422634262e-09\n",
      "Iteration: 7991 lambda_n: 0.912704945467457 Loss: 1.1920506542702e-09\n",
      "Iteration: 7992 lambda_n: 1.0165522909382545 Loss: 1.1921170705019479e-09\n",
      "Iteration: 7993 lambda_n: 1.0146306785826118 Loss: 1.192190891962452e-09\n",
      "Iteration: 7994 lambda_n: 0.8834616219702036 Loss: 1.1922643926117598e-09\n",
      "Iteration: 7995 lambda_n: 0.8876506347976885 Loss: 1.19232822791257e-09\n",
      "Iteration: 7996 lambda_n: 1.019633098563383 Loss: 1.1923922301453812e-09\n",
      "Iteration: 7997 lambda_n: 0.9770105237091133 Loss: 1.1924656028870785e-09\n",
      "Iteration: 7998 lambda_n: 0.8910654494637253 Loss: 1.1925357286658024e-09\n",
      "Iteration: 7999 lambda_n: 0.9784684001672197 Loss: 1.1925995244790063e-09\n",
      "Iteration: 8000 lambda_n: 1.0075043754174615 Loss: 1.1926694387358163e-09\n",
      "Iteration: 8001 lambda_n: 0.895993274115789 Loss: 1.1927412495493602e-09\n",
      "Iteration: 8002 lambda_n: 0.8829627429823341 Loss: 1.1928049457063807e-09\n",
      "Iteration: 8003 lambda_n: 1.0038294289491154 Loss: 1.1928675716151948e-09\n",
      "Iteration: 8004 lambda_n: 0.8968583030604743 Loss: 1.1929386282293501e-09\n",
      "Iteration: 8005 lambda_n: 0.8845439304055214 Loss: 1.1930019406277346e-09\n",
      "Iteration: 8006 lambda_n: 0.987760515732647 Loss: 1.193064243307433e-09\n",
      "Iteration: 8007 lambda_n: 0.9544165366342542 Loss: 1.1931336663379844e-09\n",
      "Iteration: 8008 lambda_n: 0.8954801226839042 Loss: 1.193200575862138e-09\n",
      "Iteration: 8009 lambda_n: 1.0272094440049004 Loss: 1.1932631951566144e-09\n",
      "Iteration: 8010 lambda_n: 0.9710484701648598 Loss: 1.1933348743875461e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8011 lambda_n: 0.9205498869700395 Loss: 1.1934024414871456e-09\n",
      "Iteration: 8012 lambda_n: 0.8937718382278581 Loss: 1.1934663326021605e-09\n",
      "Iteration: 8013 lambda_n: 0.9204142058445536 Loss: 1.193528218838724e-09\n",
      "Iteration: 8014 lambda_n: 0.95146019430901 Loss: 1.1935918007910892e-09\n",
      "Iteration: 8015 lambda_n: 0.8928819635437654 Loss: 1.1936573691469982e-09\n",
      "Iteration: 8016 lambda_n: 0.9176689379590893 Loss: 1.193718742664425e-09\n",
      "Iteration: 8017 lambda_n: 0.9262652662102455 Loss: 1.1937816698277822e-09\n",
      "Iteration: 8018 lambda_n: 0.8888179119994586 Loss: 1.1938450357696352e-09\n",
      "Iteration: 8019 lambda_n: 1.0044636336128097 Loss: 1.1939056822731023e-09\n",
      "Iteration: 8020 lambda_n: 0.9926562387718113 Loss: 1.1939740664769277e-09\n",
      "Iteration: 8021 lambda_n: 0.992259788053076 Loss: 1.1940414643700639e-09\n",
      "Iteration: 8022 lambda_n: 0.9107758597688587 Loss: 1.194108655667031e-09\n",
      "Iteration: 8023 lambda_n: 0.9666095837377789 Loss: 1.1941701526695625e-09\n",
      "Iteration: 8024 lambda_n: 0.9062096331132763 Loss: 1.1942352676810877e-09\n",
      "Iteration: 8025 lambda_n: 0.941326954157818 Loss: 1.194296142408846e-09\n",
      "Iteration: 8026 lambda_n: 0.8744131110086559 Loss: 1.1943592242091179e-09\n",
      "Iteration: 8027 lambda_n: 1.0137919342433779 Loss: 1.1944176657128609e-09\n",
      "Iteration: 8028 lambda_n: 1.0146896983760854 Loss: 1.1944852677726778e-09\n",
      "Iteration: 8029 lambda_n: 1.0119685007631576 Loss: 1.194552735889566e-09\n",
      "Iteration: 8030 lambda_n: 0.9220699033204606 Loss: 1.1946198359973206e-09\n",
      "Iteration: 8031 lambda_n: 0.9090395480611693 Loss: 1.1946807969281501e-09\n",
      "Iteration: 8032 lambda_n: 1.022044121262746 Loss: 1.1947407388857817e-09\n",
      "Iteration: 8033 lambda_n: 0.9217143855666493 Loss: 1.1948079663486788e-09\n",
      "Iteration: 8034 lambda_n: 0.8788806002689888 Loss: 1.1948684085302638e-09\n",
      "Iteration: 8035 lambda_n: 1.001614608481572 Loss: 1.1949258920232242e-09\n",
      "Iteration: 8036 lambda_n: 0.9702698495597375 Loss: 1.1949912399642697e-09\n",
      "Iteration: 8037 lambda_n: 0.9008752562136094 Loss: 1.1950543664715413e-09\n",
      "Iteration: 8038 lambda_n: 0.9813687035303931 Loss: 1.1951128051347709e-09\n",
      "Iteration: 8039 lambda_n: 0.8793436413391634 Loss: 1.195176309059757e-09\n",
      "Iteration: 8040 lambda_n: 0.9576297186826607 Loss: 1.1952330402848073e-09\n",
      "Iteration: 8041 lambda_n: 0.897404306498836 Loss: 1.195294669786331e-09\n",
      "Iteration: 8042 lambda_n: 0.8818294812220571 Loss: 1.1953522588913184e-09\n",
      "Iteration: 8043 lambda_n: 0.9932145871272992 Loss: 1.1954087025341378e-09\n",
      "Iteration: 8044 lambda_n: 0.9979514621323002 Loss: 1.1954721139124846e-09\n",
      "Iteration: 8045 lambda_n: 0.9552317755731583 Loss: 1.1955356406031044e-09\n",
      "Iteration: 8046 lambda_n: 0.982197667037073 Loss: 1.1955962667714382e-09\n",
      "Iteration: 8047 lambda_n: 0.9559094268692393 Loss: 1.1956584305441606e-09\n",
      "Iteration: 8048 lambda_n: 0.9697501718241324 Loss: 1.1957187487452666e-09\n",
      "Iteration: 8049 lambda_n: 1.0049139801482827 Loss: 1.195779775660109e-09\n",
      "Iteration: 8050 lambda_n: 0.9874130289984583 Loss: 1.1958428261107428e-09\n",
      "Iteration: 8051 lambda_n: 0.9649924516735177 Loss: 1.195904588603871e-09\n",
      "Iteration: 8052 lambda_n: 0.986117282670565 Loss: 1.1959647690619313e-09\n",
      "Iteration: 8053 lambda_n: 0.9938250128054466 Loss: 1.196026093900956e-09\n",
      "Iteration: 8054 lambda_n: 0.9531294427275425 Loss: 1.196087705944743e-09\n",
      "Iteration: 8055 lambda_n: 1.0007226620378875 Loss: 1.1961466192754178e-09\n",
      "Iteration: 8056 lambda_n: 0.9876466486176164 Loss: 1.1962082993431297e-09\n",
      "Iteration: 8057 lambda_n: 0.9749933467758691 Loss: 1.196268978519731e-09\n",
      "Iteration: 8058 lambda_n: 0.9124296067480792 Loss: 1.1963286993115967e-09\n",
      "Iteration: 8059 lambda_n: 0.9570743436974711 Loss: 1.1963844181270333e-09\n",
      "Iteration: 8060 lambda_n: 0.9504172409988364 Loss: 1.196442704766256e-09\n",
      "Iteration: 8061 lambda_n: 0.9516408038507631 Loss: 1.1965004088702406e-09\n",
      "Iteration: 8062 lambda_n: 0.9296707820538337 Loss: 1.1965580132488537e-09\n",
      "Iteration: 8063 lambda_n: 0.9216610335740872 Loss: 1.1966141234600006e-09\n",
      "Iteration: 8064 lambda_n: 1.005403603241191 Loss: 1.1966695901998102e-09\n",
      "Iteration: 8065 lambda_n: 0.9137704118438583 Loss: 1.1967299225864061e-09\n",
      "Iteration: 8066 lambda_n: 0.9816210423214793 Loss: 1.1967845730498389e-09\n",
      "Iteration: 8067 lambda_n: 0.9836625442262527 Loss: 1.1968431256626241e-09\n",
      "Iteration: 8068 lambda_n: 0.9764325221663518 Loss: 1.1969016170256612e-09\n",
      "Iteration: 8069 lambda_n: 0.9288454758762296 Loss: 1.1969594955125981e-09\n",
      "Iteration: 8070 lambda_n: 0.9678909627117869 Loss: 1.1970143807840802e-09\n",
      "Iteration: 8071 lambda_n: 0.9383838398612159 Loss: 1.1970714009043815e-09\n",
      "Iteration: 8072 lambda_n: 0.9782914153028734 Loss: 1.1971265154844982e-09\n",
      "Iteration: 8073 lambda_n: 0.910115169786236 Loss: 1.197183798131065e-09\n",
      "Iteration: 8074 lambda_n: 1.004188217484932 Loss: 1.1972369202107534e-09\n",
      "Iteration: 8075 lambda_n: 0.9334476074869625 Loss: 1.1972953669881058e-09\n",
      "Iteration: 8076 lambda_n: 0.969333570811414 Loss: 1.197349516386463e-09\n",
      "Iteration: 8077 lambda_n: 0.9209583625740358 Loss: 1.1974055751267565e-09\n",
      "Iteration: 8078 lambda_n: 0.9403967922252294 Loss: 1.1974586701923189e-09\n",
      "Iteration: 8079 lambda_n: 0.9222967569695836 Loss: 1.1975127265402123e-09\n",
      "Iteration: 8080 lambda_n: 0.914033566000664 Loss: 1.1975655725889746e-09\n",
      "Iteration: 8081 lambda_n: 0.9259672923803539 Loss: 1.1976177927750623e-09\n",
      "Iteration: 8082 lambda_n: 0.8744735429807621 Loss: 1.1976705307099509e-09\n",
      "Iteration: 8083 lambda_n: 0.8770323282713063 Loss: 1.1977201806997084e-09\n",
      "Iteration: 8084 lambda_n: 0.9858061909537441 Loss: 1.1977698400609578e-09\n",
      "Iteration: 8085 lambda_n: 0.9107327543370023 Loss: 1.1978254960028197e-09\n",
      "Iteration: 8086 lambda_n: 1.0079023147776078 Loss: 1.1978767454401297e-09\n",
      "Iteration: 8087 lambda_n: 0.9156661054553925 Loss: 1.1979332938482663e-09\n",
      "Iteration: 8088 lambda_n: 1.0008553172582386 Loss: 1.1979844878316912e-09\n",
      "Iteration: 8089 lambda_n: 0.8903512859226128 Loss: 1.1980402802149436e-09\n",
      "Iteration: 8090 lambda_n: 0.9976166804119904 Loss: 1.1980897403778996e-09\n",
      "Iteration: 8091 lambda_n: 0.9561769884027942 Loss: 1.1981450015080118e-09\n",
      "Iteration: 8092 lambda_n: 0.9322679102462663 Loss: 1.1981977899103325e-09\n",
      "Iteration: 8093 lambda_n: 0.9316021769687375 Loss: 1.1982490868964743e-09\n",
      "Iteration: 8094 lambda_n: 0.92281318200684 Loss: 1.1983001917263577e-09\n",
      "Iteration: 8095 lambda_n: 0.9317743825152435 Loss: 1.1983506494214654e-09\n",
      "Iteration: 8096 lambda_n: 0.8957089288561481 Loss: 1.198401434763635e-09\n",
      "Iteration: 8097 lambda_n: 1.0004382801583263 Loss: 1.1984501029290455e-09\n",
      "Iteration: 8098 lambda_n: 0.9551335018228528 Loss: 1.1985043005382347e-09\n",
      "Iteration: 8099 lambda_n: 0.9979445868392635 Loss: 1.1985558636525678e-09\n",
      "Iteration: 8100 lambda_n: 0.9339288983699586 Loss: 1.1986095655552866e-09\n",
      "Iteration: 8101 lambda_n: 0.8806848663515344 Loss: 1.1986596494386134e-09\n",
      "Iteration: 8102 lambda_n: 0.9894631139783543 Loss: 1.1987067225836053e-09\n",
      "Iteration: 8103 lambda_n: 0.9313520285066489 Loss: 1.1987594563377486e-09\n",
      "Iteration: 8104 lambda_n: 0.8919802058088844 Loss: 1.1988089170033675e-09\n",
      "Iteration: 8105 lambda_n: 0.8941477525834665 Loss: 1.1988561385698836e-09\n",
      "Iteration: 8106 lambda_n: 0.8942867549129606 Loss: 1.1989033261836151e-09\n",
      "Iteration: 8107 lambda_n: 0.980889784918397 Loss: 1.1989503712532817e-09\n",
      "Iteration: 8108 lambda_n: 0.9155294372680195 Loss: 1.1990018201511083e-09\n",
      "Iteration: 8109 lambda_n: 0.9773160545975843 Loss: 1.199049670622859e-09\n",
      "Iteration: 8110 lambda_n: 0.8769014836530956 Loss: 1.199100592825873e-09\n",
      "Iteration: 8111 lambda_n: 0.9871258738115178 Loss: 1.1991461227240826e-09\n",
      "Iteration: 8112 lambda_n: 0.8748380135427074 Loss: 1.1991972203994476e-09\n",
      "Iteration: 8113 lambda_n: 0.8903028549025338 Loss: 1.1992423452581934e-09\n",
      "Iteration: 8114 lambda_n: 0.9352578203542088 Loss: 1.1992881307179275e-09\n",
      "Iteration: 8115 lambda_n: 0.8897596885645581 Loss: 1.1993360762398655e-09\n",
      "Iteration: 8116 lambda_n: 0.8791711250997547 Loss: 1.199381535290998e-09\n",
      "Iteration: 8117 lambda_n: 1.0182203793813673 Loss: 1.1994263135262544e-09\n",
      "Iteration: 8118 lambda_n: 0.9130022524030837 Loss: 1.199478015775332e-09\n",
      "Iteration: 8119 lambda_n: 0.9188733621004141 Loss: 1.1995242073018108e-09\n",
      "Iteration: 8120 lambda_n: 0.9577594659103063 Loss: 1.199570539879915e-09\n",
      "Iteration: 8121 lambda_n: 0.9850448537082617 Loss: 1.1996186800000223e-09\n",
      "Iteration: 8122 lambda_n: 0.9865726865042288 Loss: 1.1996680201657688e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8123 lambda_n: 1.0116171645733647 Loss: 1.1997172643782242e-09\n",
      "Iteration: 8124 lambda_n: 1.0149003910720003 Loss: 1.1997675804824213e-09\n",
      "Iteration: 8125 lambda_n: 0.9165427527885649 Loss: 1.1998178819212357e-09\n",
      "Iteration: 8126 lambda_n: 1.014162129466821 Loss: 1.199863135959175e-09\n",
      "Iteration: 8127 lambda_n: 0.9750343037314484 Loss: 1.199913048137316e-09\n",
      "Iteration: 8128 lambda_n: 0.9877530463737362 Loss: 1.1999608570907904e-09\n",
      "Iteration: 8129 lambda_n: 1.0136164583990583 Loss: 1.2000091209748388e-09\n",
      "Iteration: 8130 lambda_n: 0.9119599328748673 Loss: 1.2000584646531636e-09\n",
      "Iteration: 8131 lambda_n: 0.9956267801467638 Loss: 1.2001026990288884e-09\n",
      "Iteration: 8132 lambda_n: 0.8805825297819817 Loss: 1.2001508335584214e-09\n",
      "Iteration: 8133 lambda_n: 0.9950854543528509 Loss: 1.2001932471419447e-09\n",
      "Iteration: 8134 lambda_n: 0.8887872218331497 Loss: 1.200241021659709e-09\n",
      "Iteration: 8135 lambda_n: 1.0257440018996813 Loss: 1.2002835398689464e-09\n",
      "Iteration: 8136 lambda_n: 0.9238640495455072 Loss: 1.200332451094977e-09\n",
      "Iteration: 8137 lambda_n: 1.0092399957426816 Loss: 1.2003763398572363e-09\n",
      "Iteration: 8138 lambda_n: 0.9153690183855577 Loss: 1.2004241198993373e-09\n",
      "Iteration: 8139 lambda_n: 0.8986067599933381 Loss: 1.2004672947732652e-09\n",
      "Iteration: 8140 lambda_n: 0.9939164294467394 Loss: 1.2005095346776615e-09\n",
      "Iteration: 8141 lambda_n: 0.9031108979806659 Loss: 1.2005561005272861e-09\n",
      "Iteration: 8142 lambda_n: 0.8952933373865978 Loss: 1.2005982599095086e-09\n",
      "Iteration: 8143 lambda_n: 0.9703671392791858 Loss: 1.2006399108502982e-09\n",
      "Iteration: 8144 lambda_n: 0.8926903610260699 Loss: 1.2006849075544008e-09\n",
      "Iteration: 8145 lambda_n: 1.0133139189435494 Loss: 1.2007261499092002e-09\n",
      "Iteration: 8146 lambda_n: 0.9595676534050822 Loss: 1.2007728136064933e-09\n",
      "Iteration: 8147 lambda_n: 0.9716602843186583 Loss: 1.2008168316756046e-09\n",
      "Iteration: 8148 lambda_n: 0.9622643854758165 Loss: 1.2008612487741069e-09\n",
      "Iteration: 8149 lambda_n: 0.8912867870615874 Loss: 1.2009050705251944e-09\n",
      "Iteration: 8150 lambda_n: 0.9789353114228811 Loss: 1.2009455186086874e-09\n",
      "Iteration: 8151 lambda_n: 0.97686664153098 Loss: 1.2009897943720246e-09\n",
      "Iteration: 8152 lambda_n: 0.9540250753665596 Loss: 1.2010338134078135e-09\n",
      "Iteration: 8153 lambda_n: 0.9673653420372316 Loss: 1.2010766468468592e-09\n",
      "Iteration: 8154 lambda_n: 0.953817786632738 Loss: 1.2011199209112686e-09\n",
      "Iteration: 8155 lambda_n: 0.9250590790763872 Loss: 1.2011624334702932e-09\n",
      "Iteration: 8156 lambda_n: 0.958303428403653 Loss: 1.2012035144250799e-09\n",
      "Iteration: 8157 lambda_n: 0.8933107740140888 Loss: 1.2012459251304264e-09\n",
      "Iteration: 8158 lambda_n: 0.9542414407435793 Loss: 1.201285312177669e-09\n",
      "Iteration: 8159 lambda_n: 0.87999092803165 Loss: 1.2013272424963301e-09\n",
      "Iteration: 8160 lambda_n: 1.0224076249788174 Loss: 1.2013657721775895e-09\n",
      "Iteration: 8161 lambda_n: 0.9810016595310308 Loss: 1.2014103909134137e-09\n",
      "Iteration: 8162 lambda_n: 0.9506814985917539 Loss: 1.201453035278399e-09\n",
      "Iteration: 8163 lambda_n: 0.8847357922961798 Loss: 1.2014942055371032e-09\n",
      "Iteration: 8164 lambda_n: 0.9501042402246258 Loss: 1.2015323787763967e-09\n",
      "Iteration: 8165 lambda_n: 0.9809954414882861 Loss: 1.201573235010034e-09\n",
      "Iteration: 8166 lambda_n: 0.9274912505689216 Loss: 1.201615263527552e-09\n",
      "Iteration: 8167 lambda_n: 0.8913720940925384 Loss: 1.2016548514383856e-09\n",
      "Iteration: 8168 lambda_n: 0.9368920290157068 Loss: 1.2016927618773382e-09\n",
      "Iteration: 8169 lambda_n: 0.8786877154298903 Loss: 1.2017324738000419e-09\n",
      "Iteration: 8170 lambda_n: 0.8948830983258882 Loss: 1.2017695818726394e-09\n",
      "Iteration: 8171 lambda_n: 0.9323815373068162 Loss: 1.2018072424385356e-09\n",
      "Iteration: 8172 lambda_n: 1.022786211469445 Loss: 1.2018463528321752e-09\n",
      "Iteration: 8173 lambda_n: 0.9016855543946333 Loss: 1.201889098791171e-09\n",
      "Iteration: 8174 lambda_n: 0.930526371908269 Loss: 1.2019266323975291e-09\n",
      "Iteration: 8175 lambda_n: 0.8844520638988191 Loss: 1.201965231817429e-09\n",
      "Iteration: 8176 lambda_n: 0.9866457352260005 Loss: 1.2020017862894604e-09\n",
      "Iteration: 8177 lambda_n: 0.9287346559596094 Loss: 1.2020424291007926e-09\n",
      "Iteration: 8178 lambda_n: 1.012152337585178 Loss: 1.2020805337701864e-09\n",
      "Iteration: 8179 lambda_n: 0.9184612609157744 Loss: 1.202121914643399e-09\n",
      "Iteration: 8180 lambda_n: 0.9723012044479235 Loss: 1.2021593189087972e-09\n",
      "Iteration: 8181 lambda_n: 0.9257364321141326 Loss: 1.2021987730285532e-09\n",
      "Iteration: 8182 lambda_n: 0.9268121975126847 Loss: 1.20223618965424e-09\n",
      "Iteration: 8183 lambda_n: 0.8939119560555534 Loss: 1.2022735160033919e-09\n",
      "Iteration: 8184 lambda_n: 0.883557689656837 Loss: 1.2023093857016787e-09\n",
      "Iteration: 8185 lambda_n: 1.0219262890743759 Loss: 1.2023447202958973e-09\n",
      "Iteration: 8186 lambda_n: 0.9747193740355761 Loss: 1.20238544609655e-09\n",
      "Iteration: 8187 lambda_n: 0.895081874935257 Loss: 1.202424129801003e-09\n",
      "Iteration: 8188 lambda_n: 1.0182712177064233 Loss: 1.202459518676922e-09\n",
      "Iteration: 8189 lambda_n: 0.9525272420941399 Loss: 1.2024996391015877e-09\n",
      "Iteration: 8190 lambda_n: 1.0143057822243593 Loss: 1.2025370184625846e-09\n",
      "Iteration: 8191 lambda_n: 0.8827385665638093 Loss: 1.2025766709266345e-09\n",
      "Iteration: 8192 lambda_n: 0.9921561132482675 Loss: 1.202611037420759e-09\n",
      "Iteration: 8193 lambda_n: 1.0103657773043015 Loss: 1.2026495305510327e-09\n",
      "Iteration: 8194 lambda_n: 0.9169891703052571 Loss: 1.2026885748712058e-09\n",
      "Iteration: 8195 lambda_n: 0.9553851787516314 Loss: 1.202723871593819e-09\n",
      "Iteration: 8196 lambda_n: 0.9126358754609059 Loss: 1.2027605119993062e-09\n",
      "Iteration: 8197 lambda_n: 0.9404582529328539 Loss: 1.2027953781416717e-09\n",
      "Iteration: 8198 lambda_n: 1.012685281349901 Loss: 1.2028311763110823e-09\n",
      "Iteration: 8199 lambda_n: 1.024636964570281 Loss: 1.2028695809073017e-09\n",
      "Iteration: 8200 lambda_n: 0.8747828591463113 Loss: 1.2029082807748512e-09\n",
      "Iteration: 8201 lambda_n: 1.0031439858330584 Loss: 1.202941183207448e-09\n",
      "Iteration: 8202 lambda_n: 1.0143504559026666 Loss: 1.2029787845157387e-09\n",
      "Iteration: 8203 lambda_n: 1.0114908829267144 Loss: 1.2030166523847475e-09\n",
      "Iteration: 8204 lambda_n: 0.889970473895872 Loss: 1.2030542589099156e-09\n",
      "Iteration: 8205 lambda_n: 0.8863479229642034 Loss: 1.203087212191582e-09\n",
      "Iteration: 8206 lambda_n: 0.9187938178453451 Loss: 1.203119910267771e-09\n",
      "Iteration: 8207 lambda_n: 0.9050430691113945 Loss: 1.2031536916245368e-09\n",
      "Iteration: 8208 lambda_n: 0.9384153653166589 Loss: 1.203186841507785e-09\n",
      "Iteration: 8209 lambda_n: 0.8909033219773517 Loss: 1.203221089511179e-09\n",
      "Iteration: 8210 lambda_n: 0.9063478098111647 Loss: 1.2032534779399592e-09\n",
      "Iteration: 8211 lambda_n: 0.9193340305907255 Loss: 1.2032863089866855e-09\n",
      "Iteration: 8212 lambda_n: 1.0269839132978529 Loss: 1.203319491711415e-09\n",
      "Iteration: 8213 lambda_n: 0.921115717477799 Loss: 1.2033564241915062e-09\n",
      "Iteration: 8214 lambda_n: 1.0147024316293018 Loss: 1.2033894072558616e-09\n",
      "Iteration: 8215 lambda_n: 0.8831266582403415 Loss: 1.2034256094650531e-09\n",
      "Iteration: 8216 lambda_n: 0.9366415158933822 Loss: 1.2034569855332414e-09\n",
      "Iteration: 8217 lambda_n: 0.9902270976750093 Loss: 1.2034901437159265e-09\n",
      "Iteration: 8218 lambda_n: 0.9161849213594773 Loss: 1.2035250663638131e-09\n",
      "Iteration: 8219 lambda_n: 0.9474471664166769 Loss: 1.20355724248436e-09\n",
      "Iteration: 8220 lambda_n: 0.8970733520656584 Loss: 1.203590395967905e-09\n",
      "Iteration: 8221 lambda_n: 0.9367842507023599 Loss: 1.2036216683878786e-09\n",
      "Iteration: 8222 lambda_n: 0.9800669838518842 Loss: 1.203654201497698e-09\n",
      "Iteration: 8223 lambda_n: 0.9557935498885103 Loss: 1.203688110554792e-09\n",
      "Iteration: 8224 lambda_n: 0.998904691465287 Loss: 1.2037210480670009e-09\n",
      "Iteration: 8225 lambda_n: 0.9433089837475969 Loss: 1.2037553322648464e-09\n",
      "Iteration: 8226 lambda_n: 0.8885231694382824 Loss: 1.2037875785166946e-09\n",
      "Iteration: 8227 lambda_n: 0.9009558866693541 Loss: 1.2038178311345456e-09\n",
      "Iteration: 8228 lambda_n: 0.902335537787104 Loss: 1.2038483946567053e-09\n",
      "Iteration: 8229 lambda_n: 0.8867272318318479 Loss: 1.203878896166635e-09\n",
      "Iteration: 8230 lambda_n: 0.9686424661708586 Loss: 1.2039087543274124e-09\n",
      "Iteration: 8231 lambda_n: 0.9261225690001005 Loss: 1.2039412578749216e-09\n",
      "Iteration: 8232 lambda_n: 1.0267605561965825 Loss: 1.2039722063991641e-09\n",
      "Iteration: 8233 lambda_n: 1.0208829193529791 Loss: 1.2040063900851253e-09\n",
      "Iteration: 8234 lambda_n: 0.9817107912373131 Loss: 1.2040402329935465e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8235 lambda_n: 1.023569207006433 Loss: 1.2040726393909322e-09\n",
      "Iteration: 8236 lambda_n: 1.0083251928071404 Loss: 1.2041062908117184e-09\n",
      "Iteration: 8237 lambda_n: 0.9434161366243816 Loss: 1.2041392995057355e-09\n",
      "Iteration: 8238 lambda_n: 1.0038858816955725 Loss: 1.2041700554652886e-09\n",
      "Iteration: 8239 lambda_n: 0.9332817035737639 Loss: 1.2042026554999219e-09\n",
      "Iteration: 8240 lambda_n: 0.9511250058737779 Loss: 1.2042328358396702e-09\n",
      "Iteration: 8241 lambda_n: 0.8809565517967378 Loss: 1.2042634731857967e-09\n",
      "Iteration: 8242 lambda_n: 0.9188268090617774 Loss: 1.204291738716252e-09\n",
      "Iteration: 8243 lambda_n: 0.9761442965593132 Loss: 1.2043211073296375e-09\n",
      "Iteration: 8244 lambda_n: 1.01758883267079 Loss: 1.2043521943148312e-09\n",
      "Iteration: 8245 lambda_n: 1.0244968631608278 Loss: 1.2043844657464822e-09\n",
      "Iteration: 8246 lambda_n: 1.0018081588273278 Loss: 1.2044168222848738e-09\n",
      "Iteration: 8247 lambda_n: 0.8941489089582102 Loss: 1.2044483219145293e-09\n",
      "Iteration: 8248 lambda_n: 1.0007509681380178 Loss: 1.2044763226138193e-09\n",
      "Iteration: 8249 lambda_n: 0.9676878972263254 Loss: 1.2045075429850403e-09\n",
      "Iteration: 8250 lambda_n: 1.0257103900364288 Loss: 1.204537606158036e-09\n",
      "Iteration: 8251 lambda_n: 0.9990566912663587 Loss: 1.2045693423000754e-09\n",
      "Iteration: 8252 lambda_n: 0.9230468360242815 Loss: 1.2046001216729704e-09\n",
      "Iteration: 8253 lambda_n: 1.0063035447702828 Loss: 1.2046284363159374e-09\n",
      "Iteration: 8254 lambda_n: 0.9768191100154129 Loss: 1.20465918817769e-09\n",
      "Iteration: 8255 lambda_n: 0.933070294349078 Loss: 1.2046889110165783e-09\n",
      "Iteration: 8256 lambda_n: 0.9284762096455185 Loss: 1.204717187554546e-09\n",
      "Iteration: 8257 lambda_n: 0.9416336804332975 Loss: 1.2047452169811169e-09\n",
      "Iteration: 8258 lambda_n: 0.9116313439426045 Loss: 1.2047735309982223e-09\n",
      "Iteration: 8259 lambda_n: 0.9778448512699183 Loss: 1.2048008320560955e-09\n",
      "Iteration: 8260 lambda_n: 0.9174966174917093 Loss: 1.2048300069131287e-09\n",
      "Iteration: 8261 lambda_n: 0.9946003169207684 Loss: 1.2048572659078328e-09\n",
      "Iteration: 8262 lambda_n: 0.9881758966865645 Loss: 1.204886702925879e-09\n",
      "Iteration: 8263 lambda_n: 0.9124505761661748 Loss: 1.2049158229690935e-09\n",
      "Iteration: 8264 lambda_n: 0.9550469150228089 Loss: 1.2049426047148732e-09\n",
      "Iteration: 8265 lambda_n: 0.9292252789252896 Loss: 1.2049705247198707e-09\n",
      "Iteration: 8266 lambda_n: 0.9611363080414425 Loss: 1.2049975810034187e-09\n",
      "Iteration: 8267 lambda_n: 0.9336835665153154 Loss: 1.2050254576486222e-09\n",
      "Iteration: 8268 lambda_n: 0.9093068103342022 Loss: 1.205052426938434e-09\n",
      "Iteration: 8269 lambda_n: 0.9005469466113538 Loss: 1.205078589110086e-09\n",
      "Iteration: 8270 lambda_n: 0.9508274647730628 Loss: 1.2051043974490143e-09\n",
      "Iteration: 8271 lambda_n: 0.8885849350277482 Loss: 1.2051315420026731e-09\n",
      "Iteration: 8272 lambda_n: 0.8757010912821136 Loss: 1.2051568057627976e-09\n",
      "Iteration: 8273 lambda_n: 0.9980332364027654 Loss: 1.2051816078507187e-09\n",
      "Iteration: 8274 lambda_n: 0.8980917402550734 Loss: 1.2052097699154955e-09\n",
      "Iteration: 8275 lambda_n: 0.9967571310820994 Loss: 1.205235003136048e-09\n",
      "Iteration: 8276 lambda_n: 0.9015681073546287 Loss: 1.205262907559467e-09\n",
      "Iteration: 8277 lambda_n: 0.9740141184874814 Loss: 1.2052880336801685e-09\n",
      "Iteration: 8278 lambda_n: 1.0232354892788906 Loss: 1.2053150800251229e-09\n",
      "Iteration: 8279 lambda_n: 1.0178789179160124 Loss: 1.2053433742966834e-09\n",
      "Iteration: 8280 lambda_n: 0.9420857877311342 Loss: 1.2053713935875098e-09\n",
      "Iteration: 8281 lambda_n: 0.9393187598060019 Loss: 1.2053972135813469e-09\n",
      "Iteration: 8282 lambda_n: 1.00601211661626 Loss: 1.2054228549105848e-09\n",
      "Iteration: 8283 lambda_n: 0.9639644273212291 Loss: 1.2054502078028328e-09\n",
      "Iteration: 8284 lambda_n: 0.9535322299784759 Loss: 1.2054763057818007e-09\n",
      "Iteration: 8285 lambda_n: 0.9265062535067854 Loss: 1.2055020114763551e-09\n",
      "Iteration: 8286 lambda_n: 0.9520232274050955 Loss: 1.2055268871217827e-09\n",
      "Iteration: 8287 lambda_n: 0.9223517614499637 Loss: 1.2055523464919408e-09\n",
      "Iteration: 8288 lambda_n: 0.9411312048317277 Loss: 1.2055769100541253e-09\n",
      "Iteration: 8289 lambda_n: 0.979828811383582 Loss: 1.205601875743735e-09\n",
      "Iteration: 8290 lambda_n: 0.8918248555043805 Loss: 1.205627764247708e-09\n",
      "Iteration: 8291 lambda_n: 0.9658522607204263 Loss: 1.2056512220916512e-09\n",
      "Iteration: 8292 lambda_n: 0.8736887363088839 Loss: 1.2056765361817664e-09\n",
      "Iteration: 8293 lambda_n: 0.9034745029142421 Loss: 1.2056993388474634e-09\n",
      "Iteration: 8294 lambda_n: 0.9505595770019886 Loss: 1.2057228288280094e-09\n",
      "Iteration: 8295 lambda_n: 1.0194269537883747 Loss: 1.2057474457651342e-09\n",
      "Iteration: 8296 lambda_n: 1.0274430916539306 Loss: 1.2057737380879854e-09\n",
      "Iteration: 8297 lambda_n: 0.9998581546397506 Loss: 1.2058001226977871e-09\n",
      "Iteration: 8298 lambda_n: 0.9288846580100619 Loss: 1.2058256847636804e-09\n",
      "Iteration: 8299 lambda_n: 0.9418780987101365 Loss: 1.2058493281778405e-09\n",
      "Iteration: 8300 lambda_n: 0.9905713505551034 Loss: 1.2058732050443892e-09\n",
      "Iteration: 8301 lambda_n: 0.8858400782691157 Loss: 1.205898216146435e-09\n",
      "Iteration: 8302 lambda_n: 0.9492152845993939 Loss: 1.2059204829688132e-09\n",
      "Iteration: 8303 lambda_n: 0.951472207687348 Loss: 1.205944254795905e-09\n",
      "Iteration: 8304 lambda_n: 1.0038091939981881 Loss: 1.2059679842963273e-09\n",
      "Iteration: 8305 lambda_n: 1.011155684228184 Loss: 1.205992915486752e-09\n",
      "Iteration: 8306 lambda_n: 0.9427830970029907 Loss: 1.2060179183743702e-09\n",
      "Iteration: 8307 lambda_n: 0.935155454554139 Loss: 1.2060411289271823e-09\n",
      "Iteration: 8308 lambda_n: 0.9992096640605576 Loss: 1.2060640585019968e-09\n",
      "Iteration: 8309 lambda_n: 0.9595682639768737 Loss: 1.2060884586457404e-09\n",
      "Iteration: 8310 lambda_n: 0.8921939078299453 Loss: 1.2061117861500996e-09\n",
      "Iteration: 8311 lambda_n: 0.9996778762184005 Loss: 1.2061333852833287e-09\n",
      "Iteration: 8312 lambda_n: 0.9546662762640538 Loss: 1.2061574955070264e-09\n",
      "Iteration: 8313 lambda_n: 0.907662433453311 Loss: 1.2061804169638394e-09\n",
      "Iteration: 8314 lambda_n: 1.0016886173865183 Loss: 1.2062021234618979e-09\n",
      "Iteration: 8315 lambda_n: 0.9507519127993638 Loss: 1.2062259795263574e-09\n",
      "Iteration: 8316 lambda_n: 0.9164538616731516 Loss: 1.2062485238870583e-09\n",
      "Iteration: 8317 lambda_n: 0.8768050764242975 Loss: 1.2062701633630745e-09\n",
      "Iteration: 8318 lambda_n: 0.9965454627294845 Loss: 1.2062907858753189e-09\n",
      "Iteration: 8319 lambda_n: 0.9502281838101138 Loss: 1.2063141329863544e-09\n",
      "Iteration: 8320 lambda_n: 0.8804690063295721 Loss: 1.2063362997075729e-09\n",
      "Iteration: 8321 lambda_n: 1.0136538768289527 Loss: 1.2063567514614885e-09\n",
      "Iteration: 8322 lambda_n: 1.0200698357059337 Loss: 1.206380207034997e-09\n",
      "Iteration: 8323 lambda_n: 0.9428248484647912 Loss: 1.2064037073745435e-09\n",
      "Iteration: 8324 lambda_n: 0.9026679071441605 Loss: 1.2064253308694297e-09\n",
      "Iteration: 8325 lambda_n: 0.9979041209402301 Loss: 1.2064459423647042e-09\n",
      "Iteration: 8326 lambda_n: 0.9492903277873855 Loss: 1.2064686440576346e-09\n",
      "Iteration: 8327 lambda_n: 1.0011585024432885 Loss: 1.2064901445805114e-09\n",
      "Iteration: 8328 lambda_n: 0.9816305245449778 Loss: 1.2065127227684678e-09\n",
      "Iteration: 8329 lambda_n: 0.9232513607952013 Loss: 1.2065347636682926e-09\n",
      "Iteration: 8330 lambda_n: 0.9115772191663821 Loss: 1.2065554037864277e-09\n",
      "Iteration: 8331 lambda_n: 0.9959756918599758 Loss: 1.206575698055796e-09\n",
      "Iteration: 8332 lambda_n: 0.9576470821293346 Loss: 1.2065977851664981e-09\n",
      "Iteration: 8333 lambda_n: 0.9229744880606692 Loss: 1.2066189325293162e-09\n",
      "Iteration: 8334 lambda_n: 1.0023763598666882 Loss: 1.2066392227410746e-09\n",
      "Iteration: 8335 lambda_n: 1.0062974771720254 Loss: 1.2066611749687733e-09\n",
      "Iteration: 8336 lambda_n: 0.9661937168354747 Loss: 1.2066831130429154e-09\n",
      "Iteration: 8337 lambda_n: 0.8827455186518434 Loss: 1.2067040848856304e-09\n",
      "Iteration: 8338 lambda_n: 0.9373453193631907 Loss: 1.2067231593578948e-09\n",
      "Iteration: 8339 lambda_n: 0.9482553731718855 Loss: 1.2067433377821328e-09\n",
      "Iteration: 8340 lambda_n: 1.0178468549643767 Loss: 1.2067636653605411e-09\n",
      "Iteration: 8341 lambda_n: 0.9544431881328247 Loss: 1.2067853924671188e-09\n",
      "Iteration: 8342 lambda_n: 0.959412874920486 Loss: 1.2068056756164977e-09\n",
      "Iteration: 8343 lambda_n: 0.9732396348672288 Loss: 1.2068259789353924e-09\n",
      "Iteration: 8344 lambda_n: 1.0149546236219655 Loss: 1.2068464858343702e-09\n",
      "Iteration: 8345 lambda_n: 0.9252395840290334 Loss: 1.2068677805009758e-09\n",
      "Iteration: 8346 lambda_n: 1.0091998842994094 Loss: 1.206887104799928e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8347 lambda_n: 0.9193840159630787 Loss: 1.2069080959391337e-09\n",
      "Iteration: 8348 lambda_n: 0.9794418911287408 Loss: 1.2069271338071045e-09\n",
      "Iteration: 8349 lambda_n: 0.9418693038403106 Loss: 1.2069473320323742e-09\n",
      "Iteration: 8350 lambda_n: 0.9523425727401889 Loss: 1.2069666719397506e-09\n",
      "Iteration: 8351 lambda_n: 1.0253487505598438 Loss: 1.20698614141248e-09\n",
      "Iteration: 8352 lambda_n: 0.9488490988542041 Loss: 1.2070070175791872e-09\n",
      "Iteration: 8353 lambda_n: 1.0252878262713025 Loss: 1.2070262482544448e-09\n",
      "Iteration: 8354 lambda_n: 0.9632809161537934 Loss: 1.2070469406327885e-09\n",
      "Iteration: 8355 lambda_n: 0.8901393739832972 Loss: 1.2070662934455345e-09\n",
      "Iteration: 8356 lambda_n: 0.9523883502810003 Loss: 1.207084098403155e-09\n",
      "Iteration: 8357 lambda_n: 1.009902770760083 Loss: 1.2071030751050964e-09\n",
      "Iteration: 8358 lambda_n: 0.9598728091463641 Loss: 1.207123111627668e-09\n",
      "Iteration: 8359 lambda_n: 0.9382127420179502 Loss: 1.2071420685341528e-09\n",
      "Iteration: 8360 lambda_n: 0.906410635228246 Loss: 1.2071605167338627e-09\n",
      "Iteration: 8361 lambda_n: 0.9083303740288866 Loss: 1.2071782673962624e-09\n",
      "Iteration: 8362 lambda_n: 0.9910911302988111 Loss: 1.2071959825876298e-09\n",
      "Iteration: 8363 lambda_n: 0.8967809403350715 Loss: 1.2072152351266278e-09\n",
      "Iteration: 8364 lambda_n: 0.8987784143322777 Loss: 1.2072325781692091e-09\n",
      "Iteration: 8365 lambda_n: 0.910419172515118 Loss: 1.2072498891483957e-09\n",
      "Iteration: 8366 lambda_n: 0.9075416789267257 Loss: 1.207267354484907e-09\n",
      "Iteration: 8367 lambda_n: 0.9140653606831995 Loss: 1.2072846928046977e-09\n",
      "Iteration: 8368 lambda_n: 0.9635302857894339 Loss: 1.2073020848689724e-09\n",
      "Iteration: 8369 lambda_n: 0.9079953776564126 Loss: 1.2073203427772905e-09\n",
      "Iteration: 8370 lambda_n: 0.9268512660095808 Loss: 1.2073374746247157e-09\n",
      "Iteration: 8371 lambda_n: 0.8995973492589875 Loss: 1.2073548915516951e-09\n",
      "Iteration: 8372 lambda_n: 0.9354658699624537 Loss: 1.207371726497434e-09\n",
      "Iteration: 8373 lambda_n: 0.9784157336107057 Loss: 1.207389162547752e-09\n",
      "Iteration: 8374 lambda_n: 0.8893751722572033 Loss: 1.2074073215718898e-09\n",
      "Iteration: 8375 lambda_n: 0.9855905208083626 Loss: 1.20742375501002e-09\n",
      "Iteration: 8376 lambda_n: 0.9780499864946234 Loss: 1.2074418929560394e-09\n",
      "Iteration: 8377 lambda_n: 0.8935797243355093 Loss: 1.207459811796203e-09\n",
      "Iteration: 8378 lambda_n: 0.8995953846819024 Loss: 1.2074761177678063e-09\n",
      "Iteration: 8379 lambda_n: 0.9373018796355844 Loss: 1.2074924621617593e-09\n",
      "Iteration: 8380 lambda_n: 0.9158309900608957 Loss: 1.2075094227365918e-09\n",
      "Iteration: 8381 lambda_n: 0.8816066338532761 Loss: 1.2075259258144968e-09\n",
      "Iteration: 8382 lambda_n: 0.9910167848104507 Loss: 1.2075417450761723e-09\n",
      "Iteration: 8383 lambda_n: 1.0129688815171636 Loss: 1.2075594611408673e-09\n",
      "Iteration: 8384 lambda_n: 0.9252127345628905 Loss: 1.2075774844261482e-09\n",
      "Iteration: 8385 lambda_n: 0.8824665309488433 Loss: 1.207593871700913e-09\n",
      "Iteration: 8386 lambda_n: 1.0018335040968303 Loss: 1.2076094373692943e-09\n",
      "Iteration: 8387 lambda_n: 0.9116686169243436 Loss: 1.2076270384474963e-09\n",
      "Iteration: 8388 lambda_n: 1.0106025936047869 Loss: 1.207642982733738e-09\n",
      "Iteration: 8389 lambda_n: 0.8788586775201688 Loss: 1.2076605837295186e-09\n",
      "Iteration: 8390 lambda_n: 0.9932539668340495 Loss: 1.207675818353865e-09\n",
      "Iteration: 8391 lambda_n: 0.8814444823142049 Loss: 1.2076929694685799e-09\n",
      "Iteration: 8392 lambda_n: 0.9513361419169182 Loss: 1.2077081222567576e-09\n",
      "Iteration: 8393 lambda_n: 0.8962138904904833 Loss: 1.2077244122093378e-09\n",
      "Iteration: 8394 lambda_n: 0.8934854498389452 Loss: 1.2077396909922952e-09\n",
      "Iteration: 8395 lambda_n: 0.970118181212873 Loss: 1.207754864608458e-09\n",
      "Iteration: 8396 lambda_n: 0.9561445085674513 Loss: 1.2077712715736267e-09\n",
      "Iteration: 8397 lambda_n: 0.8954024197473653 Loss: 1.2077873688943022e-09\n",
      "Iteration: 8398 lambda_n: 0.9079245402938061 Loss: 1.207802382476295e-09\n",
      "Iteration: 8399 lambda_n: 1.0031602665315944 Loss: 1.2078175402387552e-09\n",
      "Iteration: 8400 lambda_n: 0.9629980157259972 Loss: 1.2078342251331532e-09\n",
      "Iteration: 8401 lambda_n: 0.9290275433893908 Loss: 1.207850163813344e-09\n",
      "Iteration: 8402 lambda_n: 0.8992633872271806 Loss: 1.2078654766932779e-09\n",
      "Iteration: 8403 lambda_n: 0.9302996936101768 Loss: 1.2078802335522713e-09\n",
      "Iteration: 8404 lambda_n: 0.8840486428214954 Loss: 1.2078954400182372e-09\n",
      "Iteration: 8405 lambda_n: 0.9152590804017617 Loss: 1.2079098299866943e-09\n",
      "Iteration: 8406 lambda_n: 0.9165662917916517 Loss: 1.2079246644559688e-09\n",
      "Iteration: 8407 lambda_n: 0.8856949376154373 Loss: 1.2079394598645187e-09\n",
      "Iteration: 8408 lambda_n: 0.9319118855916948 Loss: 1.2079536984835206e-09\n",
      "Iteration: 8409 lambda_n: 0.9774266016371553 Loss: 1.207968616427752e-09\n",
      "Iteration: 8410 lambda_n: 0.9930319904960798 Loss: 1.2079841999000343e-09\n",
      "Iteration: 8411 lambda_n: 0.9107786653620514 Loss: 1.207999961915263e-09\n",
      "Iteration: 8412 lambda_n: 0.8823654266158959 Loss: 1.2080143498936545e-09\n",
      "Iteration: 8413 lambda_n: 0.9790694949196602 Loss: 1.2080282344823796e-09\n",
      "Iteration: 8414 lambda_n: 0.8996467480020386 Loss: 1.2080435787239717e-09\n",
      "Iteration: 8415 lambda_n: 0.9298362251883932 Loss: 1.2080576149684603e-09\n",
      "Iteration: 8416 lambda_n: 0.897048040542965 Loss: 1.2080720635470218e-09\n",
      "Iteration: 8417 lambda_n: 0.9183817098496443 Loss: 1.2080859434252846e-09\n",
      "Iteration: 8418 lambda_n: 0.9988943913393695 Loss: 1.2081000936145485e-09\n",
      "Iteration: 8419 lambda_n: 0.8952088047990618 Loss: 1.2081154186304352e-09\n",
      "Iteration: 8420 lambda_n: 0.9740560301150322 Loss: 1.208129092013072e-09\n",
      "Iteration: 8421 lambda_n: 1.0183709840626904 Loss: 1.2081439103062779e-09\n",
      "Iteration: 8422 lambda_n: 0.9369440812299871 Loss: 1.2081593346575296e-09\n",
      "Iteration: 8423 lambda_n: 1.0069160652909572 Loss: 1.2081734578442983e-09\n",
      "Iteration: 8424 lambda_n: 0.9534026504978316 Loss: 1.208188572542653e-09\n",
      "Iteration: 8425 lambda_n: 0.9803798644018474 Loss: 1.2082028186700478e-09\n",
      "Iteration: 8426 lambda_n: 0.9658793272454275 Loss: 1.208217399379996e-09\n",
      "Iteration: 8427 lambda_n: 0.9395196026471971 Loss: 1.2082317045718167e-09\n",
      "Iteration: 8428 lambda_n: 0.9992528579959062 Loss: 1.2082455578827732e-09\n",
      "Iteration: 8429 lambda_n: 1.0133630936375606 Loss: 1.2082602275805944e-09\n",
      "Iteration: 8430 lambda_n: 1.0245511050359564 Loss: 1.208275035995209e-09\n",
      "Iteration: 8431 lambda_n: 0.9750059292605298 Loss: 1.2082899382649795e-09\n",
      "Iteration: 8432 lambda_n: 0.9840375240021202 Loss: 1.2083040491181896e-09\n",
      "Iteration: 8433 lambda_n: 0.9510713992723396 Loss: 1.208318235674357e-09\n",
      "Iteration: 8434 lambda_n: 0.898758173875761 Loss: 1.20833188130436e-09\n",
      "Iteration: 8435 lambda_n: 0.9280027097647221 Loss: 1.2083447200761667e-09\n",
      "Iteration: 8436 lambda_n: 0.9527050231364471 Loss: 1.2083579230457328e-09\n",
      "Iteration: 8437 lambda_n: 0.9715615933590226 Loss: 1.2083714173555217e-09\n",
      "Iteration: 8438 lambda_n: 0.9682679474384014 Loss: 1.208385122900502e-09\n",
      "Iteration: 8439 lambda_n: 0.8839156945978659 Loss: 1.2083987196295213e-09\n",
      "Iteration: 8440 lambda_n: 0.940488637085009 Loss: 1.2084110779770171e-09\n",
      "Iteration: 8441 lambda_n: 0.9537086514539367 Loss: 1.2084241733088678e-09\n",
      "Iteration: 8442 lambda_n: 0.987575523131899 Loss: 1.208437392987332e-09\n",
      "Iteration: 8443 lambda_n: 0.9111656880754342 Loss: 1.2084510237976694e-09\n",
      "Iteration: 8444 lambda_n: 0.9912010208492995 Loss: 1.208463542533635e-09\n",
      "Iteration: 8445 lambda_n: 0.8909870239445494 Loss: 1.2084771077578005e-09\n",
      "Iteration: 8446 lambda_n: 0.9300015976731736 Loss: 1.208489240717624e-09\n",
      "Iteration: 8447 lambda_n: 0.9066441357521041 Loss: 1.2085018539325154e-09\n",
      "Iteration: 8448 lambda_n: 0.8863866608109968 Loss: 1.2085140959247943e-09\n",
      "Iteration: 8449 lambda_n: 0.9366312510906045 Loss: 1.208526020393442e-09\n",
      "Iteration: 8450 lambda_n: 0.9695105668954875 Loss: 1.2085385653925063e-09\n",
      "Iteration: 8451 lambda_n: 0.8931904464407283 Loss: 1.208551497276606e-09\n",
      "Iteration: 8452 lambda_n: 0.8813245878777017 Loss: 1.2085633580157e-09\n",
      "Iteration: 8453 lambda_n: 0.9606714264631347 Loss: 1.208575010588048e-09\n",
      "Iteration: 8454 lambda_n: 0.9804148429909656 Loss: 1.2085876653843827e-09\n",
      "Iteration: 8455 lambda_n: 0.9719653063634031 Loss: 1.208600518923101e-09\n",
      "Iteration: 8456 lambda_n: 0.9791358339571397 Loss: 1.2086132047322168e-09\n",
      "Iteration: 8457 lambda_n: 0.985447357017107 Loss: 1.2086259283379746e-09\n",
      "Iteration: 8458 lambda_n: 0.9130944630785331 Loss: 1.2086386782529205e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8459 lambda_n: 0.9586327569959403 Loss: 1.208650439586926e-09\n",
      "Iteration: 8460 lambda_n: 0.9924097022878302 Loss: 1.2086627313189343e-09\n",
      "Iteration: 8461 lambda_n: 0.9958854012991701 Loss: 1.2086754009217782e-09\n",
      "Iteration: 8462 lambda_n: 0.8746332588798761 Loss: 1.208688059260301e-09\n",
      "Iteration: 8463 lambda_n: 0.9622744489713161 Loss: 1.2086991217809686e-09\n",
      "Iteration: 8464 lambda_n: 0.9315744159333317 Loss: 1.2087112431998026e-09\n",
      "Iteration: 8465 lambda_n: 1.026647310927991 Loss: 1.2087229299216966e-09\n",
      "Iteration: 8466 lambda_n: 1.0181299093069076 Loss: 1.2087357534294373e-09\n",
      "Iteration: 8467 lambda_n: 0.9751655645555908 Loss: 1.20874840707726e-09\n",
      "Iteration: 8468 lambda_n: 0.9739146928712512 Loss: 1.2087604714189412e-09\n",
      "Iteration: 8469 lambda_n: 0.9040796240545536 Loss: 1.2087724691751599e-09\n",
      "Iteration: 8470 lambda_n: 1.0173796369614316 Loss: 1.2087835547243283e-09\n",
      "Iteration: 8471 lambda_n: 1.0069790281022104 Loss: 1.2087959758824058e-09\n",
      "Iteration: 8472 lambda_n: 0.9305104327588857 Loss: 1.2088082119866508e-09\n",
      "Iteration: 8473 lambda_n: 0.9286098296808543 Loss: 1.2088194710870862e-09\n",
      "Iteration: 8474 lambda_n: 0.9081966095621102 Loss: 1.2088306544841017e-09\n",
      "Iteration: 8475 lambda_n: 1.004769301558828 Loss: 1.2088415445665804e-09\n",
      "Iteration: 8476 lambda_n: 0.9617697475308102 Loss: 1.2088535407884155e-09\n",
      "Iteration: 8477 lambda_n: 0.9551627998707501 Loss: 1.208864974470043e-09\n",
      "Iteration: 8478 lambda_n: 0.9004679539427867 Loss: 1.2088762742916285e-09\n",
      "Iteration: 8479 lambda_n: 1.0177907077096744 Loss: 1.208886883872524e-09\n",
      "Iteration: 8480 lambda_n: 0.9613933589133323 Loss: 1.2088988243175794e-09\n",
      "Iteration: 8481 lambda_n: 0.9538494997274984 Loss: 1.20891004913059e-09\n",
      "Iteration: 8482 lambda_n: 1.0092156783999993 Loss: 1.2089211383609763e-09\n",
      "Iteration: 8483 lambda_n: 0.8970339553234624 Loss: 1.2089328186635072e-09\n",
      "Iteration: 8484 lambda_n: 1.0274667297747617 Loss: 1.2089431485693013e-09\n",
      "Iteration: 8485 lambda_n: 0.9520299635944038 Loss: 1.208954934255205e-09\n",
      "Iteration: 8486 lambda_n: 0.8746198954916308 Loss: 1.2089658028478494e-09\n",
      "Iteration: 8487 lambda_n: 1.0255938594191707 Loss: 1.208975742955528e-09\n",
      "Iteration: 8488 lambda_n: 0.8947762038583542 Loss: 1.2089873536029479e-09\n",
      "Iteration: 8489 lambda_n: 0.9061901104435093 Loss: 1.2089974326135158e-09\n",
      "Iteration: 8490 lambda_n: 0.8811013705657977 Loss: 1.2090075996102598e-09\n",
      "Iteration: 8491 lambda_n: 0.9346778828738764 Loss: 1.209017445063885e-09\n",
      "Iteration: 8492 lambda_n: 1.0195529665344187 Loss: 1.2090278460985423e-09\n",
      "Iteration: 8493 lambda_n: 0.8912681391125575 Loss: 1.2090391413515344e-09\n",
      "Iteration: 8494 lambda_n: 1.0258623961406343 Loss: 1.209048967814295e-09\n",
      "Iteration: 8495 lambda_n: 0.9204614123643975 Loss: 1.2090602300120565e-09\n",
      "Iteration: 8496 lambda_n: 0.9024579320114753 Loss: 1.2090702881410147e-09\n",
      "Iteration: 8497 lambda_n: 0.9221117806858817 Loss: 1.2090801090193575e-09\n",
      "Iteration: 8498 lambda_n: 0.8755730944705721 Loss: 1.2090900993494002e-09\n",
      "Iteration: 8499 lambda_n: 0.9750062329408755 Loss: 1.209099543731524e-09\n",
      "Iteration: 8500 lambda_n: 0.8905199464918787 Loss: 1.2091100210707022e-09\n",
      "Iteration: 8501 lambda_n: 0.8868680577467167 Loss: 1.209119545095475e-09\n",
      "Iteration: 8502 lambda_n: 1.024709535724806 Loss: 1.2091289913468589e-09\n",
      "Iteration: 8503 lambda_n: 0.9202143344929842 Loss: 1.2091398621497845e-09\n",
      "Iteration: 8504 lambda_n: 0.9284275062528617 Loss: 1.2091495777061507e-09\n",
      "Iteration: 8505 lambda_n: 0.9935846255045329 Loss: 1.2091593388709973e-09\n",
      "Iteration: 8506 lambda_n: 0.8787662915597289 Loss: 1.209169739011242e-09\n",
      "Iteration: 8507 lambda_n: 0.8755882374644818 Loss: 1.2091788940160987e-09\n",
      "Iteration: 8508 lambda_n: 0.958953750156911 Loss: 1.2091879775826583e-09\n",
      "Iteration: 8509 lambda_n: 0.928174597650487 Loss: 1.2091978883524887e-09\n",
      "Iteration: 8510 lambda_n: 0.9846236155727214 Loss: 1.2092074359089154e-09\n",
      "Iteration: 8511 lambda_n: 1.0000366993320218 Loss: 1.2092175216097454e-09\n",
      "Iteration: 8512 lambda_n: 0.9807422587096695 Loss: 1.2092277139428785e-09\n",
      "Iteration: 8513 lambda_n: 1.0098290358086082 Loss: 1.209237666723349e-09\n",
      "Iteration: 8514 lambda_n: 0.9677479289954841 Loss: 1.2092478709081363e-09\n",
      "Iteration: 8515 lambda_n: 0.926999218812957 Loss: 1.209257602616766e-09\n",
      "Iteration: 8516 lambda_n: 0.9397401752593922 Loss: 1.209266880230611e-09\n",
      "Iteration: 8517 lambda_n: 0.9444456948441468 Loss: 1.2092762505399593e-09\n",
      "Iteration: 8518 lambda_n: 0.9764312663824392 Loss: 1.2092856214832765e-09\n",
      "Iteration: 8519 lambda_n: 0.9009034502487557 Loss: 1.2092952682720273e-09\n",
      "Iteration: 8520 lambda_n: 1.0115353246458014 Loss: 1.2093041269862282e-09\n",
      "Iteration: 8521 lambda_n: 0.9804078555474642 Loss: 1.2093140336467355e-09\n",
      "Iteration: 8522 lambda_n: 0.956639506095192 Loss: 1.2093235892937614e-09\n",
      "Iteration: 8523 lambda_n: 0.8764758276347094 Loss: 1.2093328722191454e-09\n",
      "Iteration: 8524 lambda_n: 0.9149596946827339 Loss: 1.209341339514787e-09\n",
      "Iteration: 8525 lambda_n: 0.9565346378234182 Loss: 1.2093501415892848e-09\n",
      "Iteration: 8526 lambda_n: 0.9948312371528265 Loss: 1.2093593031848414e-09\n",
      "Iteration: 8527 lambda_n: 0.899059805489849 Loss: 1.2093687907972535e-09\n",
      "Iteration: 8528 lambda_n: 0.9898553763621613 Loss: 1.2093773239604672e-09\n",
      "Iteration: 8529 lambda_n: 1.0050372872243212 Loss: 1.2093866794850712e-09\n",
      "Iteration: 8530 lambda_n: 0.9068637028502073 Loss: 1.2093961297922446e-09\n",
      "Iteration: 8531 lambda_n: 0.8973866002948996 Loss: 1.2094046238301722e-09\n",
      "Iteration: 8532 lambda_n: 0.9230307564767226 Loss: 1.2094129883974802e-09\n",
      "Iteration: 8533 lambda_n: 0.982457626160724 Loss: 1.209421558464174e-09\n",
      "Iteration: 8534 lambda_n: 0.962068276892541 Loss: 1.2094306407791655e-09\n",
      "Iteration: 8535 lambda_n: 0.9499220307089162 Loss: 1.2094394938974522e-09\n",
      "Iteration: 8536 lambda_n: 1.0186802389017042 Loss: 1.2094481949467022e-09\n",
      "Iteration: 8537 lambda_n: 0.8894802575579049 Loss: 1.2094574846100962e-09\n",
      "Iteration: 8538 lambda_n: 0.9411454952432857 Loss: 1.2094655577632623e-09\n",
      "Iteration: 8539 lambda_n: 0.9767611969631895 Loss: 1.2094740637846297e-09\n",
      "Iteration: 8540 lambda_n: 0.9349715087295832 Loss: 1.2094828492662392e-09\n",
      "Iteration: 8541 lambda_n: 0.9629818833715167 Loss: 1.2094912234103703e-09\n",
      "Iteration: 8542 lambda_n: 1.0180454533298786 Loss: 1.2094998136976834e-09\n",
      "Iteration: 8543 lambda_n: 0.9649649556268656 Loss: 1.2095088498090492e-09\n",
      "Iteration: 8544 lambda_n: 0.9795163740937244 Loss: 1.2095173733343148e-09\n",
      "Iteration: 8545 lambda_n: 0.8775550664763553 Loss: 1.2095259864080244e-09\n",
      "Iteration: 8546 lambda_n: 0.9195600857435682 Loss: 1.2095336695452515e-09\n",
      "Iteration: 8547 lambda_n: 1.0196154095109242 Loss: 1.209541687545895e-09\n",
      "Iteration: 8548 lambda_n: 0.9393808096754446 Loss: 1.2095505442117087e-09\n",
      "Iteration: 8549 lambda_n: 1.0274002739379484 Loss: 1.2095586603820387e-09\n",
      "Iteration: 8550 lambda_n: 0.9123509820306721 Loss: 1.2095674984687636e-09\n",
      "Iteration: 8551 lambda_n: 1.0106062010870118 Loss: 1.2095753095786235e-09\n",
      "Iteration: 8552 lambda_n: 0.887424477225142 Loss: 1.2095839263691197e-09\n",
      "Iteration: 8553 lambda_n: 0.9920657910406009 Loss: 1.2095914565369826e-09\n",
      "Iteration: 8554 lambda_n: 1.0031638806427643 Loss: 1.209599836855218e-09\n",
      "Iteration: 8555 lambda_n: 0.888643605412262 Loss: 1.2096082755026245e-09\n",
      "Iteration: 8556 lambda_n: 0.8935613137547718 Loss: 1.209615713299662e-09\n",
      "Iteration: 8557 lambda_n: 0.9553880895276282 Loss: 1.2096231585604086e-09\n",
      "Iteration: 8558 lambda_n: 0.8984687487297555 Loss: 1.2096310874911752e-09\n",
      "Iteration: 8559 lambda_n: 0.915663090529053 Loss: 1.2096385101579786e-09\n",
      "Iteration: 8560 lambda_n: 0.8949975327805919 Loss: 1.2096460441887802e-09\n",
      "Iteration: 8561 lambda_n: 0.9811882096945276 Loss: 1.2096533769080342e-09\n",
      "Iteration: 8562 lambda_n: 0.8823694295964626 Loss: 1.2096613823814365e-09\n",
      "Iteration: 8563 lambda_n: 1.023697090143392 Loss: 1.2096685474670739e-09\n",
      "Iteration: 8564 lambda_n: 0.9308990254793692 Loss: 1.2096768271998934e-09\n",
      "Iteration: 8565 lambda_n: 0.9725525475131758 Loss: 1.209684317689806e-09\n",
      "Iteration: 8566 lambda_n: 0.9723361780477331 Loss: 1.209692111061216e-09\n",
      "Iteration: 8567 lambda_n: 0.887211112176523 Loss: 1.2096998646941506e-09\n",
      "Iteration: 8568 lambda_n: 1.0205204469473008 Loss: 1.2097069095004746e-09\n",
      "Iteration: 8569 lambda_n: 0.895279254825317 Loss: 1.2097149771444695e-09\n",
      "Iteration: 8570 lambda_n: 1.0275159319638343 Loss: 1.209722022567062e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8571 lambda_n: 0.8974249596094719 Loss: 1.2097300731219955e-09\n",
      "Iteration: 8572 lambda_n: 0.8829270641912739 Loss: 1.2097370696496009e-09\n",
      "Iteration: 8573 lambda_n: 1.0071495686548042 Loss: 1.209743922583825e-09\n",
      "Iteration: 8574 lambda_n: 0.8805244690683847 Loss: 1.2097517106067002e-09\n",
      "Iteration: 8575 lambda_n: 0.8760159494636183 Loss: 1.209758488775963e-09\n",
      "Iteration: 8576 lambda_n: 0.935905201577414 Loss: 1.209765203457478e-09\n",
      "Iteration: 8577 lambda_n: 0.9796219459212591 Loss: 1.2097723463195742e-09\n",
      "Iteration: 8578 lambda_n: 1.0245437786709306 Loss: 1.2097797894071994e-09\n",
      "Iteration: 8579 lambda_n: 0.9306152851381325 Loss: 1.209787536224333e-09\n",
      "Iteration: 8580 lambda_n: 0.9277792664839475 Loss: 1.2097945445869863e-09\n",
      "Iteration: 8581 lambda_n: 0.9711823138016004 Loss: 1.2098014987997745e-09\n",
      "Iteration: 8582 lambda_n: 0.9617202685501015 Loss: 1.2098087462346821e-09\n",
      "Iteration: 8583 lambda_n: 0.9487812487735474 Loss: 1.20981588898044e-09\n",
      "Iteration: 8584 lambda_n: 1.0092978852737617 Loss: 1.209822905194628e-09\n",
      "Iteration: 8585 lambda_n: 1.0152850450604345 Loss: 1.209830334802778e-09\n",
      "Iteration: 8586 lambda_n: 1.003930752738042 Loss: 1.209837773152747e-09\n",
      "Iteration: 8587 lambda_n: 1.025087370767878 Loss: 1.209845089438387e-09\n",
      "Iteration: 8588 lambda_n: 0.9371569002748391 Loss: 1.2098525270956148e-09\n",
      "Iteration: 8589 lambda_n: 0.9521780031357324 Loss: 1.2098592954391964e-09\n",
      "Iteration: 8590 lambda_n: 0.9981770230348479 Loss: 1.2098661392069808e-09\n",
      "Iteration: 8591 lambda_n: 0.9077736823184288 Loss: 1.2098732812450764e-09\n",
      "Iteration: 8592 lambda_n: 0.9695474935851772 Loss: 1.2098797495022754e-09\n",
      "Iteration: 8593 lambda_n: 1.0148043152453055 Loss: 1.2098866271267982e-09\n",
      "Iteration: 8594 lambda_n: 0.8813070133794709 Loss: 1.2098937904995484e-09\n",
      "Iteration: 8595 lambda_n: 0.8921499312461245 Loss: 1.2098999812101148e-09\n",
      "Iteration: 8596 lambda_n: 1.0233318774628624 Loss: 1.2099062237519853e-09\n",
      "Iteration: 8597 lambda_n: 0.9563430631497487 Loss: 1.2099133555148533e-09\n",
      "Iteration: 8598 lambda_n: 0.9162241585384284 Loss: 1.2099199845847373e-09\n",
      "Iteration: 8599 lambda_n: 0.9814740306355149 Loss: 1.2099263061022446e-09\n",
      "Iteration: 8600 lambda_n: 0.8971726212801079 Loss: 1.2099330505573418e-09\n",
      "Iteration: 8601 lambda_n: 0.8856705717845096 Loss: 1.2099391880415961e-09\n",
      "Iteration: 8602 lambda_n: 0.9050019495211488 Loss: 1.2099452227929377e-09\n",
      "Iteration: 8603 lambda_n: 0.9264552054754948 Loss: 1.2099513595115172e-09\n",
      "Iteration: 8604 lambda_n: 0.9378717717704622 Loss: 1.2099576156885185e-09\n",
      "Iteration: 8605 lambda_n: 0.9785254572789979 Loss: 1.2099639254909685e-09\n",
      "Iteration: 8606 lambda_n: 0.9793000887788144 Loss: 1.2099704768167836e-09\n",
      "Iteration: 8607 lambda_n: 0.9404614550256657 Loss: 1.2099770014427577e-09\n",
      "Iteration: 8608 lambda_n: 0.8909927363902543 Loss: 1.2099832421202175e-09\n",
      "Iteration: 8609 lambda_n: 0.8977979653150764 Loss: 1.209989125888548e-09\n",
      "Iteration: 8610 lambda_n: 0.976617919894006 Loss: 1.209995028748154e-09\n",
      "Iteration: 8611 lambda_n: 0.9251575389768936 Loss: 1.2100014245610672e-09\n",
      "Iteration: 8612 lambda_n: 0.8992841078662598 Loss: 1.2100074568724724e-09\n",
      "Iteration: 8613 lambda_n: 0.9939748936221698 Loss: 1.2100132892385042e-09\n",
      "Iteration: 8614 lambda_n: 0.9749946885025751 Loss: 1.2100197135653969e-09\n",
      "Iteration: 8615 lambda_n: 1.0215626452989168 Loss: 1.210025986245481e-09\n",
      "Iteration: 8616 lambda_n: 0.9807630471320267 Loss: 1.2100325243557109e-09\n",
      "Iteration: 8617 lambda_n: 0.9583950856646719 Loss: 1.2100387737646005e-09\n",
      "Iteration: 8618 lambda_n: 0.9794490321970495 Loss: 1.2100448505142838e-09\n",
      "Iteration: 8619 lambda_n: 1.027385096453138 Loss: 1.2100510348716079e-09\n",
      "Iteration: 8620 lambda_n: 0.897927776843041 Loss: 1.2100574899002797e-09\n",
      "Iteration: 8621 lambda_n: 0.9945434716686583 Loss: 1.2100631037142476e-09\n",
      "Iteration: 8622 lambda_n: 0.9998936566732117 Loss: 1.2100692982941335e-09\n",
      "Iteration: 8623 lambda_n: 1.0246673724803295 Loss: 1.210075492534577e-09\n",
      "Iteration: 8624 lambda_n: 0.9153077250859448 Loss: 1.2100818157322343e-09\n",
      "Iteration: 8625 lambda_n: 1.0015877531242874 Loss: 1.2100874324124746e-09\n",
      "Iteration: 8626 lambda_n: 0.9604065732236094 Loss: 1.210093555528735e-09\n",
      "Iteration: 8627 lambda_n: 0.8918065206035071 Loss: 1.2100993946264997e-09\n",
      "Iteration: 8628 lambda_n: 0.9629425820662266 Loss: 1.210104795484919e-09\n",
      "Iteration: 8629 lambda_n: 0.9604407218556602 Loss: 1.2101106030279561e-09\n",
      "Iteration: 8630 lambda_n: 0.9822168556687929 Loss: 1.2101163665810877e-09\n",
      "Iteration: 8631 lambda_n: 0.8749673353908349 Loss: 1.2101222344478411e-09\n",
      "Iteration: 8632 lambda_n: 0.9770489960430172 Loss: 1.2101274353683695e-09\n",
      "Iteration: 8633 lambda_n: 0.9847817018845163 Loss: 1.2101332217173e-09\n",
      "Iteration: 8634 lambda_n: 0.9075668733649326 Loss: 1.2101390247819416e-09\n",
      "Iteration: 8635 lambda_n: 0.9950806893726294 Loss: 1.21014434974263e-09\n",
      "Iteration: 8636 lambda_n: 0.9038415644275394 Loss: 1.210150164013225e-09\n",
      "Iteration: 8637 lambda_n: 0.9211056640899371 Loss: 1.21015541893362e-09\n",
      "Iteration: 8638 lambda_n: 1.0237318661686572 Loss: 1.2101607522634715e-09\n",
      "Iteration: 8639 lambda_n: 0.898916902890829 Loss: 1.2101666514586018e-09\n",
      "Iteration: 8640 lambda_n: 0.9655277420685795 Loss: 1.2101718105355467e-09\n",
      "Iteration: 8641 lambda_n: 0.9275713185518001 Loss: 1.2101773230990221e-09\n",
      "Iteration: 8642 lambda_n: 0.9282004035236078 Loss: 1.2101825976747976e-09\n",
      "Iteration: 8643 lambda_n: 1.0198144680138597 Loss: 1.210187851581603e-09\n",
      "Iteration: 8644 lambda_n: 0.9788123226976243 Loss: 1.2101935980629565e-09\n",
      "Iteration: 8645 lambda_n: 1.021271036247192 Loss: 1.2101990856348084e-09\n",
      "Iteration: 8646 lambda_n: 0.9847735387911861 Loss: 1.2102047881743463e-09\n",
      "Iteration: 8647 lambda_n: 0.9891756652946329 Loss: 1.210210256126156e-09\n",
      "Iteration: 8648 lambda_n: 0.946698812567097 Loss: 1.2102157252185625e-09\n",
      "Iteration: 8649 lambda_n: 0.886094823058172 Loss: 1.2102209383389225e-09\n",
      "Iteration: 8650 lambda_n: 0.915767301575231 Loss: 1.2102257921281834e-09\n",
      "Iteration: 8651 lambda_n: 0.8926844758141609 Loss: 1.210230788600485e-09\n",
      "Iteration: 8652 lambda_n: 0.9784253177983513 Loss: 1.2102356383028307e-09\n",
      "Iteration: 8653 lambda_n: 0.969109112934596 Loss: 1.210240925355627e-09\n",
      "Iteration: 8654 lambda_n: 0.9510500417095736 Loss: 1.2102461460530319e-09\n",
      "Iteration: 8655 lambda_n: 0.9373775003502928 Loss: 1.2102512399756912e-09\n",
      "Iteration: 8656 lambda_n: 0.9219644082020223 Loss: 1.2102562429637756e-09\n",
      "Iteration: 8657 lambda_n: 0.940905199404262 Loss: 1.2102611380803328e-09\n",
      "Iteration: 8658 lambda_n: 0.9850189063382002 Loss: 1.2102661116657793e-09\n",
      "Iteration: 8659 lambda_n: 1.018561890126944 Loss: 1.210271298835149e-09\n",
      "Iteration: 8660 lambda_n: 1.0061770811893207 Loss: 1.2102766332901555e-09\n",
      "Iteration: 8661 lambda_n: 0.9012153381898064 Loss: 1.2102818824588204e-09\n",
      "Iteration: 8662 lambda_n: 0.9021079152150894 Loss: 1.2102865571822127e-09\n",
      "Iteration: 8663 lambda_n: 1.0026218506737758 Loss: 1.2102912176471795e-09\n",
      "Iteration: 8664 lambda_n: 0.8933199851870613 Loss: 1.2102963758010381e-09\n",
      "Iteration: 8665 lambda_n: 0.9408154432594205 Loss: 1.2103009486189644e-09\n",
      "Iteration: 8666 lambda_n: 0.9977497501598432 Loss: 1.2103057473640184e-09\n",
      "Iteration: 8667 lambda_n: 0.9885298907684954 Loss: 1.2103108109680729e-09\n",
      "Iteration: 8668 lambda_n: 1.0149602264455757 Loss: 1.210315804059418e-09\n",
      "Iteration: 8669 lambda_n: 1.0085216791299172 Loss: 1.2103209084617761e-09\n",
      "Iteration: 8670 lambda_n: 0.9921742657188394 Loss: 1.2103259546841216e-09\n",
      "Iteration: 8671 lambda_n: 0.8773199381362822 Loss: 1.21033089622721e-09\n",
      "Iteration: 8672 lambda_n: 0.8810555614983164 Loss: 1.2103352430902337e-09\n",
      "Iteration: 8673 lambda_n: 0.9577468112033503 Loss: 1.2103395907706207e-09\n",
      "Iteration: 8674 lambda_n: 0.9556021648035022 Loss: 1.2103442969047186e-09\n",
      "Iteration: 8675 lambda_n: 0.8836252564887438 Loss: 1.2103489703026828e-09\n",
      "Iteration: 8676 lambda_n: 0.9968332127826166 Loss: 1.210353275623814e-09\n",
      "Iteration: 8677 lambda_n: 0.9999767367653979 Loss: 1.2103581089688503e-09\n",
      "Iteration: 8678 lambda_n: 0.9569027167419039 Loss: 1.2103629355109994e-09\n",
      "Iteration: 8679 lambda_n: 1.0108765405738964 Loss: 1.2103675309518038e-09\n",
      "Iteration: 8680 lambda_n: 1.0087290987849533 Loss: 1.2103723644240566e-09\n",
      "Iteration: 8681 lambda_n: 0.9792147669070216 Loss: 1.210377168650525e-09\n",
      "Iteration: 8682 lambda_n: 0.9366881086181503 Loss: 1.210381804844274e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8683 lambda_n: 0.8806896691360677 Loss: 1.2103862191940425e-09\n",
      "Iteration: 8684 lambda_n: 0.9549972893037499 Loss: 1.21039035548554e-09\n",
      "Iteration: 8685 lambda_n: 0.9312032844423803 Loss: 1.2103948193217298e-09\n",
      "Iteration: 8686 lambda_n: 0.977619570181532 Loss: 1.2103991519957887e-09\n",
      "Iteration: 8687 lambda_n: 0.9035073505735263 Loss: 1.2104036787590692e-09\n",
      "Iteration: 8688 lambda_n: 0.9330386363375636 Loss: 1.2104078449482369e-09\n",
      "Iteration: 8689 lambda_n: 0.9842782528773533 Loss: 1.2104121290375372e-09\n",
      "Iteration: 8690 lambda_n: 0.9356420693650066 Loss: 1.210416627073447e-09\n",
      "Iteration: 8691 lambda_n: 0.9619420932403694 Loss: 1.2104208837194745e-09\n",
      "Iteration: 8692 lambda_n: 0.8860868378083132 Loss: 1.210425240194175e-09\n",
      "Iteration: 8693 lambda_n: 0.9709598222618289 Loss: 1.2104292333865937e-09\n",
      "Iteration: 8694 lambda_n: 1.0093995676840268 Loss: 1.2104335908768625e-09\n",
      "Iteration: 8695 lambda_n: 1.0045707735532186 Loss: 1.2104381029755838e-09\n",
      "Iteration: 8696 lambda_n: 0.9832845892938709 Loss: 1.2104425682478595e-09\n",
      "Iteration: 8697 lambda_n: 1.0034990203788727 Loss: 1.2104469183956836e-09\n",
      "Iteration: 8698 lambda_n: 0.918796944787769 Loss: 1.2104513368179246e-09\n",
      "Iteration: 8699 lambda_n: 0.9651181825147501 Loss: 1.2104553648427264e-09\n",
      "Iteration: 8700 lambda_n: 0.9813267836252239 Loss: 1.2104595759848266e-09\n",
      "Iteration: 8701 lambda_n: 0.9506677773409531 Loss: 1.2104638364383577e-09\n",
      "Iteration: 8702 lambda_n: 0.9174056601090651 Loss: 1.2104679456337718e-09\n",
      "Iteration: 8703 lambda_n: 0.9564514709344951 Loss: 1.21047189615628e-09\n",
      "Iteration: 8704 lambda_n: 0.9672388666370564 Loss: 1.2104759936449415e-09\n",
      "Iteration: 8705 lambda_n: 0.8870053414727178 Loss: 1.2104801190648312e-09\n",
      "Iteration: 8706 lambda_n: 0.9585067275219504 Loss: 1.210483885708174e-09\n",
      "Iteration: 8707 lambda_n: 0.9646585050205555 Loss: 1.2104879377308659e-09\n",
      "Iteration: 8708 lambda_n: 0.903478310188075 Loss: 1.2104919992005066e-09\n",
      "Iteration: 8709 lambda_n: 1.023430898889814 Loss: 1.2104957813674492e-09\n",
      "Iteration: 8710 lambda_n: 0.9531203003107144 Loss: 1.210500048070082e-09\n",
      "Iteration: 8711 lambda_n: 0.9844926544107547 Loss: 1.2105040059564963e-09\n",
      "Iteration: 8712 lambda_n: 0.9366892827447272 Loss: 1.2105080743893302e-09\n",
      "Iteration: 8713 lambda_n: 0.8918077794276454 Loss: 1.2105119275786905e-09\n",
      "Iteration: 8714 lambda_n: 0.8763717026148007 Loss: 1.210515578503963e-09\n",
      "Iteration: 8715 lambda_n: 0.9075610832502921 Loss: 1.210519154681606e-09\n",
      "Iteration: 8716 lambda_n: 0.9365812574432197 Loss: 1.2105228401508702e-09\n",
      "Iteration: 8717 lambda_n: 1.0019616688621051 Loss: 1.210526624713503e-09\n",
      "Iteration: 8718 lambda_n: 0.953136908931338 Loss: 1.2105306578827181e-09\n",
      "Iteration: 8719 lambda_n: 1.0113150895533098 Loss: 1.210534471511377e-09\n",
      "Iteration: 8720 lambda_n: 0.9622802008498971 Loss: 1.210538502355488e-09\n",
      "Iteration: 8721 lambda_n: 0.913817726040104 Loss: 1.2105423195472438e-09\n",
      "Iteration: 8722 lambda_n: 0.9740579899428577 Loss: 1.2105459262213575e-09\n",
      "Iteration: 8723 lambda_n: 0.8829061248324864 Loss: 1.2105497556792358e-09\n",
      "Iteration: 8724 lambda_n: 0.9663139502596099 Loss: 1.2105532126300683e-09\n",
      "Iteration: 8725 lambda_n: 0.9848416344601532 Loss: 1.2105569810623381e-09\n",
      "Iteration: 8726 lambda_n: 0.8918485700270519 Loss: 1.210560801148552e-09\n",
      "Iteration: 8727 lambda_n: 0.8978246428537183 Loss: 1.2105642439558261e-09\n",
      "Iteration: 8728 lambda_n: 0.9009645589220121 Loss: 1.210567693756773e-09\n",
      "Iteration: 8729 lambda_n: 0.9188836656345326 Loss: 1.210571144894406e-09\n",
      "Iteration: 8730 lambda_n: 1.0050238962095328 Loss: 1.2105746458787164e-09\n",
      "Iteration: 8731 lambda_n: 1.0162022024949169 Loss: 1.2105784580169343e-09\n",
      "Iteration: 8732 lambda_n: 0.8808999510205853 Loss: 1.2105822971569557e-09\n",
      "Iteration: 8733 lambda_n: 0.9548491923524758 Loss: 1.2105856095174164e-09\n",
      "Iteration: 8734 lambda_n: 0.9201611333852773 Loss: 1.210589186659325e-09\n",
      "Iteration: 8735 lambda_n: 0.9058458840778681 Loss: 1.2105926150356834e-09\n",
      "Iteration: 8736 lambda_n: 0.9833861910541268 Loss: 1.2105959760298108e-09\n",
      "Iteration: 8737 lambda_n: 0.9535344461756035 Loss: 1.2105996074679518e-09\n",
      "Iteration: 8738 lambda_n: 0.9987315646330966 Loss: 1.2106031124434669e-09\n",
      "Iteration: 8739 lambda_n: 0.9548336957470589 Loss: 1.2106067684738643e-09\n",
      "Iteration: 8740 lambda_n: 0.8744340737665329 Loss: 1.2106102455836144e-09\n",
      "Iteration: 8741 lambda_n: 0.9594799152640097 Loss: 1.2106134180109755e-09\n",
      "Iteration: 8742 lambda_n: 1.015301958783985 Loss: 1.2106168811692273e-09\n",
      "Iteration: 8743 lambda_n: 0.924720009193268 Loss: 1.2106205277273099e-09\n",
      "Iteration: 8744 lambda_n: 0.9000334629180337 Loss: 1.2106238359593921e-09\n",
      "Iteration: 8745 lambda_n: 1.0189932449819554 Loss: 1.2106270385986583e-09\n",
      "Iteration: 8746 lambda_n: 1.0060450125758598 Loss: 1.210630649620859e-09\n",
      "Iteration: 8747 lambda_n: 0.9246991259912609 Loss: 1.2106342022082268e-09\n",
      "Iteration: 8748 lambda_n: 0.9952632590145472 Loss: 1.2106374478272582e-09\n",
      "Iteration: 8749 lambda_n: 1.026550054555556 Loss: 1.2106409258545025e-09\n",
      "Iteration: 8750 lambda_n: 1.0155915332039949 Loss: 1.2106444994319123e-09\n",
      "Iteration: 8751 lambda_n: 0.8787474116628853 Loss: 1.2106480156476393e-09\n",
      "Iteration: 8752 lambda_n: 1.024257096528962 Loss: 1.2106510428083876e-09\n",
      "Iteration: 8753 lambda_n: 0.9458040913003758 Loss: 1.2106545556162926e-09\n",
      "Iteration: 8754 lambda_n: 0.9707492401658094 Loss: 1.210657782271594e-09\n",
      "Iteration: 8755 lambda_n: 0.909408479662068 Loss: 1.2106610836253195e-09\n",
      "Iteration: 8756 lambda_n: 0.9541997192856428 Loss: 1.2106641584920263e-09\n",
      "Iteration: 8757 lambda_n: 0.9467163437495524 Loss: 1.2106673695900292e-09\n",
      "Iteration: 8758 lambda_n: 0.8914434998847952 Loss: 1.2106705406881847e-09\n",
      "Iteration: 8759 lambda_n: 1.0059381880622196 Loss: 1.210673517301859e-09\n",
      "Iteration: 8760 lambda_n: 0.9097483830922947 Loss: 1.2106768578044758e-09\n",
      "Iteration: 8761 lambda_n: 0.9192939199181639 Loss: 1.210679866541327e-09\n",
      "Iteration: 8762 lambda_n: 0.9341009595830224 Loss: 1.2106828953354207e-09\n",
      "Iteration: 8763 lambda_n: 0.9811729101069615 Loss: 1.2106859562938029e-09\n",
      "Iteration: 8764 lambda_n: 0.9837941533238201 Loss: 1.2106891561219426e-09\n",
      "Iteration: 8765 lambda_n: 0.9270851680439348 Loss: 1.210692354753487e-09\n",
      "Iteration: 8766 lambda_n: 0.9102387010009448 Loss: 1.2106953500819131e-09\n",
      "Iteration: 8767 lambda_n: 0.9358685772482105 Loss: 1.210698281517818e-09\n",
      "Iteration: 8768 lambda_n: 0.9374523810944388 Loss: 1.210701278122632e-09\n",
      "Iteration: 8769 lambda_n: 0.8904469980232018 Loss: 1.2107042665489871e-09\n",
      "Iteration: 8770 lambda_n: 0.9511510473125434 Loss: 1.2107070933321332e-09\n",
      "Iteration: 8771 lambda_n: 0.9743787732884985 Loss: 1.2107101040780745e-09\n",
      "Iteration: 8772 lambda_n: 1.009864371451524 Loss: 1.210713166715225e-09\n",
      "Iteration: 8773 lambda_n: 0.8831097397860862 Loss: 1.2107163339878257e-09\n",
      "Iteration: 8774 lambda_n: 0.9653519621300027 Loss: 1.2107190861899809e-09\n",
      "Iteration: 8775 lambda_n: 0.8827704768159322 Loss: 1.210722084580121e-09\n",
      "Iteration: 8776 lambda_n: 0.942551983022056 Loss: 1.21072481186581e-09\n",
      "Iteration: 8777 lambda_n: 0.9245338673872376 Loss: 1.2107277132913332e-09\n",
      "Iteration: 8778 lambda_n: 0.9017513076166255 Loss: 1.2107305437840607e-09\n",
      "Iteration: 8779 lambda_n: 0.9776645648499324 Loss: 1.2107332936119348e-09\n",
      "Iteration: 8780 lambda_n: 0.9153330639582975 Loss: 1.2107362650267185e-09\n",
      "Iteration: 8781 lambda_n: 0.9642366264979981 Loss: 1.2107390332005559e-09\n",
      "Iteration: 8782 lambda_n: 0.9954496481883445 Loss: 1.2107419346885189e-09\n",
      "Iteration: 8783 lambda_n: 0.8816137859118155 Loss: 1.2107449145706746e-09\n",
      "Iteration: 8784 lambda_n: 1.0057945591473358 Loss: 1.2107475438644679e-09\n",
      "Iteration: 8785 lambda_n: 1.0015585914440173 Loss: 1.2107505289749415e-09\n",
      "Iteration: 8786 lambda_n: 1.022490186169097 Loss: 1.2107534896872288e-09\n",
      "Iteration: 8787 lambda_n: 0.9448823699069743 Loss: 1.210756495388273e-09\n",
      "Iteration: 8788 lambda_n: 0.9356688365369856 Loss: 1.2107592598476086e-09\n",
      "Iteration: 8789 lambda_n: 1.0059658853907014 Loss: 1.2107619843860973e-09\n",
      "Iteration: 8790 lambda_n: 0.9204782756943745 Loss: 1.2107649027368344e-09\n",
      "Iteration: 8791 lambda_n: 0.9771715541570503 Loss: 1.2107675568351892e-09\n",
      "Iteration: 8792 lambda_n: 0.8912524003982333 Loss: 1.210770363220061e-09\n",
      "Iteration: 8793 lambda_n: 0.8757447579616877 Loss: 1.2107729108630245e-09\n",
      "Iteration: 8794 lambda_n: 0.9737205350098183 Loss: 1.2107754024586951e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8795 lambda_n: 0.9948544343099534 Loss: 1.210778163839839e-09\n",
      "Iteration: 8796 lambda_n: 1.0169667827572448 Loss: 1.2107809710773955e-09\n",
      "Iteration: 8797 lambda_n: 1.0146548898184604 Loss: 1.210783827047526e-09\n",
      "Iteration: 8798 lambda_n: 0.9031382233795109 Loss: 1.2107866613600596e-09\n",
      "Iteration: 8799 lambda_n: 0.9292389856989332 Loss: 1.2107891723594816e-09\n",
      "Iteration: 8800 lambda_n: 0.9217045481510828 Loss: 1.210791744896837e-09\n",
      "Iteration: 8801 lambda_n: 0.9436010027775886 Loss: 1.210794283936159e-09\n",
      "Iteration: 8802 lambda_n: 0.8931083122959401 Loss: 1.2107968745921144e-09\n",
      "Iteration: 8803 lambda_n: 1.0230953612205034 Loss: 1.210799314588913e-09\n",
      "Iteration: 8804 lambda_n: 0.9919542045724059 Loss: 1.2108020977959655e-09\n",
      "Iteration: 8805 lambda_n: 0.8875302385718123 Loss: 1.210804785175723e-09\n",
      "Iteration: 8806 lambda_n: 1.0242826318744127 Loss: 1.2108071757692546e-09\n",
      "Iteration: 8807 lambda_n: 0.892009219001873 Loss: 1.2108099235877352e-09\n",
      "Iteration: 8808 lambda_n: 0.8825932601861929 Loss: 1.2108123044014313e-09\n",
      "Iteration: 8809 lambda_n: 0.9422850502035008 Loss: 1.210814651172348e-09\n",
      "Iteration: 8810 lambda_n: 0.928554569209895 Loss: 1.2108171423476127e-09\n",
      "Iteration: 8811 lambda_n: 0.8777597325131523 Loss: 1.2108195881970937e-09\n",
      "Iteration: 8812 lambda_n: 0.8933750265758746 Loss: 1.2108218907641829e-09\n",
      "Iteration: 8813 lambda_n: 0.9585725838323113 Loss: 1.210824224750492e-09\n",
      "Iteration: 8814 lambda_n: 0.9674991957702669 Loss: 1.2108267170612725e-09\n",
      "Iteration: 8815 lambda_n: 0.917574178448413 Loss: 1.210829221435339e-09\n",
      "Iteration: 8816 lambda_n: 0.9305539581443583 Loss: 1.2108315865630278e-09\n",
      "Iteration: 8817 lambda_n: 1.0239118013565183 Loss: 1.2108339734361055e-09\n",
      "Iteration: 8818 lambda_n: 0.9384854985549985 Loss: 1.2108365899272603e-09\n",
      "Iteration: 8819 lambda_n: 0.9242731321204567 Loss: 1.210838974965973e-09\n",
      "Iteration: 8820 lambda_n: 0.9317901233016809 Loss: 1.2108413135089022e-09\n",
      "Iteration: 8821 lambda_n: 0.9976376728054694 Loss: 1.2108436621349338e-09\n",
      "Iteration: 8822 lambda_n: 0.9652498063584047 Loss: 1.2108461649022212e-09\n",
      "Iteration: 8823 lambda_n: 0.9281265434936351 Loss: 1.2108485734046868e-09\n",
      "Iteration: 8824 lambda_n: 0.8746968354826191 Loss: 1.2108508775374924e-09\n",
      "Iteration: 8825 lambda_n: 1.0129934232691757 Loss: 1.2108530415454436e-09\n",
      "Iteration: 8826 lambda_n: 0.9957193144485352 Loss: 1.2108555365763177e-09\n",
      "Iteration: 8827 lambda_n: 0.995906096049226 Loss: 1.2108579745408065e-09\n",
      "Iteration: 8828 lambda_n: 0.9708191253300897 Loss: 1.210860404873844e-09\n",
      "Iteration: 8829 lambda_n: 0.96400887984538 Loss: 1.210862760846978e-09\n",
      "Iteration: 8830 lambda_n: 0.9996736575428656 Loss: 1.210865089296562e-09\n",
      "Iteration: 8831 lambda_n: 0.9699202243921902 Loss: 1.2108674940953358e-09\n",
      "Iteration: 8832 lambda_n: 0.8972405945380246 Loss: 1.2108698159408244e-09\n",
      "Iteration: 8833 lambda_n: 0.9907367372150498 Loss: 1.2108719543150567e-09\n",
      "Iteration: 8834 lambda_n: 0.9074243206451029 Loss: 1.2108743026290148e-09\n",
      "Iteration: 8835 lambda_n: 0.971972003231668 Loss: 1.2108764451830977e-09\n",
      "Iteration: 8836 lambda_n: 1.0105260604341506 Loss: 1.210878729805227e-09\n",
      "Iteration: 8837 lambda_n: 0.9941172146903328 Loss: 1.2108810951429057e-09\n",
      "Iteration: 8838 lambda_n: 0.9603183640383005 Loss: 1.2108834081634855e-09\n",
      "Iteration: 8839 lambda_n: 0.990437725055621 Loss: 1.210885633696996e-09\n",
      "Iteration: 8840 lambda_n: 1.0233707441792272 Loss: 1.2108879170257823e-09\n",
      "Iteration: 8841 lambda_n: 0.9826033900024166 Loss: 1.2108902681558053e-09\n",
      "Iteration: 8842 lambda_n: 0.9679929384074488 Loss: 1.2108925139176322e-09\n",
      "Iteration: 8843 lambda_n: 0.927403252652394 Loss: 1.2108947137790452e-09\n",
      "Iteration: 8844 lambda_n: 0.9299501303078991 Loss: 1.2108968122209482e-09\n",
      "Iteration: 8845 lambda_n: 1.007198899606236 Loss: 1.2108989086229307e-09\n",
      "Iteration: 8846 lambda_n: 1.0162977249245175 Loss: 1.210901164347892e-09\n",
      "Iteration: 8847 lambda_n: 0.9905639608070744 Loss: 1.2109034343750559e-09\n",
      "Iteration: 8848 lambda_n: 0.947619220545337 Loss: 1.2109056347048926e-09\n",
      "Iteration: 8849 lambda_n: 0.990399729975585 Loss: 1.2109077286715654e-09\n",
      "Iteration: 8850 lambda_n: 0.8872763224019348 Loss: 1.210909910274354e-09\n",
      "Iteration: 8851 lambda_n: 1.009078297812382 Loss: 1.21091184998477e-09\n",
      "Iteration: 8852 lambda_n: 1.0209473092427759 Loss: 1.2109140509915705e-09\n",
      "Iteration: 8853 lambda_n: 0.9927138971938385 Loss: 1.2109162646988655e-09\n",
      "Iteration: 8854 lambda_n: 0.9279453709817647 Loss: 1.2109184043722324e-09\n",
      "Iteration: 8855 lambda_n: 0.9951460550701026 Loss: 1.2109204008187983e-09\n",
      "Iteration: 8856 lambda_n: 1.021369992766234 Loss: 1.210922530087131e-09\n",
      "Iteration: 8857 lambda_n: 0.8893322532730098 Loss: 1.2109247046840914e-09\n",
      "Iteration: 8858 lambda_n: 0.972786765693688 Loss: 1.2109265871207429e-09\n",
      "Iteration: 8859 lambda_n: 0.964149683897278 Loss: 1.2109286378149381e-09\n",
      "Iteration: 8860 lambda_n: 0.8984814874254966 Loss: 1.2109306606170818e-09\n",
      "Iteration: 8861 lambda_n: 0.9936381071991721 Loss: 1.2109325402295594e-09\n",
      "Iteration: 8862 lambda_n: 0.9384385474971361 Loss: 1.210934606743709e-09\n",
      "Iteration: 8863 lambda_n: 0.8785080473615369 Loss: 1.2109365502033403e-09\n",
      "Iteration: 8864 lambda_n: 0.951505218458666 Loss: 1.2109383646145566e-09\n",
      "Iteration: 8865 lambda_n: 0.9056614917014474 Loss: 1.2109403163975713e-09\n",
      "Iteration: 8866 lambda_n: 0.897423559291203 Loss: 1.2109421675509994e-09\n",
      "Iteration: 8867 lambda_n: 0.9578238805447 Loss: 1.2109439950124491e-09\n",
      "Iteration: 8868 lambda_n: 0.9430793627179012 Loss: 1.2109459362544794e-09\n",
      "Iteration: 8869 lambda_n: 0.9790038554246452 Loss: 1.2109478364684183e-09\n",
      "Iteration: 8870 lambda_n: 0.9733335734780587 Loss: 1.2109498010758968e-09\n",
      "Iteration: 8871 lambda_n: 1.0222113017572005 Loss: 1.2109517482746973e-09\n",
      "Iteration: 8872 lambda_n: 0.9927436217808084 Loss: 1.2109537798042129e-09\n",
      "Iteration: 8873 lambda_n: 0.985704900719735 Loss: 1.2109557422454219e-09\n",
      "Iteration: 8874 lambda_n: 0.9680297735021592 Loss: 1.2109576878013254e-09\n",
      "Iteration: 8875 lambda_n: 0.8990706468355105 Loss: 1.210959583192044e-09\n",
      "Iteration: 8876 lambda_n: 1.0021633465644935 Loss: 1.2109613365407029e-09\n",
      "Iteration: 8877 lambda_n: 0.9007784639643655 Loss: 1.2109632866805094e-09\n",
      "Iteration: 8878 lambda_n: 0.9900622887582335 Loss: 1.210965028871068e-09\n",
      "Iteration: 8879 lambda_n: 0.9321705312575154 Loss: 1.2109669346017497e-09\n",
      "Iteration: 8880 lambda_n: 0.9963836982742712 Loss: 1.210968719573226e-09\n",
      "Iteration: 8881 lambda_n: 0.9046300306253049 Loss: 1.2109706186687257e-09\n",
      "Iteration: 8882 lambda_n: 0.9984520383412588 Loss: 1.210972335658147e-09\n",
      "Iteration: 8883 lambda_n: 1.0131036423256483 Loss: 1.210974221632527e-09\n",
      "Iteration: 8884 lambda_n: 0.9370724389972104 Loss: 1.2109761285419166e-09\n",
      "Iteration: 8885 lambda_n: 0.9499938866359832 Loss: 1.210977883101809e-09\n",
      "Iteration: 8886 lambda_n: 0.8878695643646491 Loss: 1.21097965133583e-09\n",
      "Iteration: 8887 lambda_n: 0.9325178248096694 Loss: 1.2109812999040269e-09\n",
      "Iteration: 8888 lambda_n: 0.9100898998801408 Loss: 1.210983022587655e-09\n",
      "Iteration: 8889 lambda_n: 0.8887963466358598 Loss: 1.2109846956814843e-09\n",
      "Iteration: 8890 lambda_n: 0.9858004200092015 Loss: 1.2109863191384466e-09\n",
      "Iteration: 8891 lambda_n: 0.9739597543297122 Loss: 1.210988115332349e-09\n",
      "Iteration: 8892 lambda_n: 0.8828037577758231 Loss: 1.2109898843790456e-09\n",
      "Iteration: 8893 lambda_n: 0.9755299142972248 Loss: 1.2109914800595565e-09\n",
      "Iteration: 8894 lambda_n: 0.9448356233698939 Loss: 1.2109932339370117e-09\n",
      "Iteration: 8895 lambda_n: 0.9211692529858144 Loss: 1.210994924444007e-09\n",
      "Iteration: 8896 lambda_n: 0.9745634157718801 Loss: 1.2109965644726494e-09\n",
      "Iteration: 8897 lambda_n: 0.934939192974012 Loss: 1.2109982946617065e-09\n",
      "Iteration: 8898 lambda_n: 0.9015881805547417 Loss: 1.2109999443408377e-09\n",
      "Iteration: 8899 lambda_n: 0.8737342912665058 Loss: 1.2110015270515427e-09\n",
      "Iteration: 8900 lambda_n: 1.0270236067228662 Loss: 1.2110030554565532e-09\n",
      "Iteration: 8901 lambda_n: 0.9323534852098645 Loss: 1.2110048417491213e-09\n",
      "Iteration: 8902 lambda_n: 1.0187747290129634 Loss: 1.2110064568985768e-09\n",
      "Iteration: 8903 lambda_n: 0.9882135171089867 Loss: 1.211008213037968e-09\n",
      "Iteration: 8904 lambda_n: 1.007127200310625 Loss: 1.211009910944979e-09\n",
      "Iteration: 8905 lambda_n: 1.0257806095187356 Loss: 1.211011631608805e-09\n",
      "Iteration: 8906 lambda_n: 1.014601892520033 Loss: 1.2110133753541795e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8907 lambda_n: 0.9498297778939564 Loss: 1.211015093754563e-09\n",
      "Iteration: 8908 lambda_n: 0.9255301945221435 Loss: 1.211016693220079e-09\n",
      "Iteration: 8909 lambda_n: 0.9957964554978403 Loss: 1.2110182479218029e-09\n",
      "Iteration: 8910 lambda_n: 0.8979296226818868 Loss: 1.2110199096125507e-09\n",
      "Iteration: 8911 lambda_n: 0.883810421605631 Loss: 1.2110214001878631e-09\n",
      "Iteration: 8912 lambda_n: 0.980668613763581 Loss: 1.211022863469591e-09\n",
      "Iteration: 8913 lambda_n: 1.0211705074114759 Loss: 1.2110244784728607e-09\n",
      "Iteration: 8914 lambda_n: 0.9282204313174108 Loss: 1.211026155411624e-09\n",
      "Iteration: 8915 lambda_n: 1.0014045109661927 Loss: 1.2110276679874318e-09\n",
      "Iteration: 8916 lambda_n: 0.9994251146609229 Loss: 1.2110292938560493e-09\n",
      "Iteration: 8917 lambda_n: 1.0200051038437137 Loss: 1.2110309087455453e-09\n",
      "Iteration: 8918 lambda_n: 0.8899771164177712 Loss: 1.2110325517326848e-09\n",
      "Iteration: 8919 lambda_n: 0.8918625167587786 Loss: 1.2110339753643058e-09\n",
      "Iteration: 8920 lambda_n: 0.9616804935257854 Loss: 1.2110353956650954e-09\n",
      "Iteration: 8921 lambda_n: 0.9399809574034991 Loss: 1.2110369221538072e-09\n",
      "Iteration: 8922 lambda_n: 0.8775673616043754 Loss: 1.2110384069589376e-09\n",
      "Iteration: 8923 lambda_n: 0.9740488896998334 Loss: 1.211039784720499e-09\n",
      "Iteration: 8924 lambda_n: 0.9136157822592937 Loss: 1.211041310018947e-09\n",
      "Iteration: 8925 lambda_n: 0.9638028533513583 Loss: 1.2110427359609856e-09\n",
      "Iteration: 8926 lambda_n: 1.024141086735924 Loss: 1.2110442302724535e-09\n",
      "Iteration: 8927 lambda_n: 0.8810252561405991 Loss: 1.2110458106580954e-09\n",
      "Iteration: 8928 lambda_n: 0.9605540347539134 Loss: 1.2110471652598114e-09\n",
      "Iteration: 8929 lambda_n: 0.8888953664011576 Loss: 1.2110486348208789e-09\n",
      "Iteration: 8930 lambda_n: 0.8841937801722577 Loss: 1.2110499908552556e-09\n",
      "Iteration: 8931 lambda_n: 0.8958115653250183 Loss: 1.2110513314015215e-09\n",
      "Iteration: 8932 lambda_n: 0.9452650582238035 Loss: 1.2110526825001195e-09\n",
      "Iteration: 8933 lambda_n: 0.9098656774679735 Loss: 1.2110541073829198e-09\n",
      "Iteration: 8934 lambda_n: 0.8864761942342682 Loss: 1.2110554692092445e-09\n",
      "Iteration: 8935 lambda_n: 0.9086961369856137 Loss: 1.211056792603204e-09\n",
      "Iteration: 8936 lambda_n: 0.9775518714419537 Loss: 1.2110581390391873e-09\n",
      "Iteration: 8937 lambda_n: 0.8768559658968222 Loss: 1.2110595863232279e-09\n",
      "Iteration: 8938 lambda_n: 0.9999108338953351 Loss: 1.2110608748195522e-09\n",
      "Iteration: 8939 lambda_n: 0.9337938124576807 Loss: 1.2110623376298894e-09\n",
      "Iteration: 8940 lambda_n: 0.9199440673214297 Loss: 1.211063698675441e-09\n",
      "Iteration: 8941 lambda_n: 0.8772106251503211 Loss: 1.2110650365351431e-09\n",
      "Iteration: 8942 lambda_n: 1.016762387365211 Loss: 1.2110663030963505e-09\n",
      "Iteration: 8943 lambda_n: 0.9469171553885154 Loss: 1.2110677685015483e-09\n",
      "Iteration: 8944 lambda_n: 1.0221721416177205 Loss: 1.2110691228006634e-09\n",
      "Iteration: 8945 lambda_n: 0.9771031178703641 Loss: 1.2110705807898348e-09\n",
      "Iteration: 8946 lambda_n: 1.026656322223028 Loss: 1.2110719650059416e-09\n",
      "Iteration: 8947 lambda_n: 0.9250570675060149 Loss: 1.2110734118947814e-09\n",
      "Iteration: 8948 lambda_n: 0.9039469712077919 Loss: 1.211074711872714e-09\n",
      "Iteration: 8949 lambda_n: 1.019953193794739 Loss: 1.2110759747675077e-09\n",
      "Iteration: 8950 lambda_n: 0.9491579808597619 Loss: 1.2110773956581756e-09\n",
      "Iteration: 8951 lambda_n: 0.9049577571698919 Loss: 1.2110787067179948e-09\n",
      "Iteration: 8952 lambda_n: 0.9068260003895078 Loss: 1.2110799507183775e-09\n",
      "Iteration: 8953 lambda_n: 0.8924729570456208 Loss: 1.211081193341644e-09\n",
      "Iteration: 8954 lambda_n: 0.883489815087558 Loss: 1.2110824139890918e-09\n",
      "Iteration: 8955 lambda_n: 1.018573632309652 Loss: 1.2110836201409784e-09\n",
      "Iteration: 8956 lambda_n: 0.973303275833009 Loss: 1.211085000173889e-09\n",
      "Iteration: 8957 lambda_n: 0.9493851436121435 Loss: 1.2110863095787596e-09\n",
      "Iteration: 8958 lambda_n: 0.9145216999941375 Loss: 1.2110875850297904e-09\n",
      "Iteration: 8959 lambda_n: 1.0246017162931618 Loss: 1.2110888044054496e-09\n",
      "Iteration: 8960 lambda_n: 0.945201755855212 Loss: 1.2110901665300395e-09\n",
      "Iteration: 8961 lambda_n: 0.9120444992965887 Loss: 1.2110914194457281e-09\n",
      "Iteration: 8962 lambda_n: 0.9084141161842512 Loss: 1.2110926191222862e-09\n",
      "Iteration: 8963 lambda_n: 0.985088046814039 Loss: 1.2110938116113392e-09\n",
      "Iteration: 8964 lambda_n: 0.9333671936090725 Loss: 1.2110950947562781e-09\n",
      "Iteration: 8965 lambda_n: 1.0197023535767518 Loss: 1.2110963095897095e-09\n",
      "Iteration: 8966 lambda_n: 0.9611156913333245 Loss: 1.2110976269668102e-09\n",
      "Iteration: 8967 lambda_n: 0.8977737637641133 Loss: 1.211098863495721e-09\n",
      "Iteration: 8968 lambda_n: 1.009931866048331 Loss: 1.2111000141756385e-09\n",
      "Iteration: 8969 lambda_n: 0.9166857836999157 Loss: 1.211101301965187e-09\n",
      "Iteration: 8970 lambda_n: 1.0181512690667036 Loss: 1.211102466956792e-09\n",
      "Iteration: 8971 lambda_n: 1.0149313748474578 Loss: 1.2111037514562908e-09\n",
      "Iteration: 8972 lambda_n: 0.9456403194709024 Loss: 1.2111050279820703e-09\n",
      "Iteration: 8973 lambda_n: 0.935403327550702 Loss: 1.2111062097958603e-09\n",
      "Iteration: 8974 lambda_n: 0.9711152994364295 Loss: 1.211107376704437e-09\n",
      "Iteration: 8975 lambda_n: 0.8940236447374177 Loss: 1.2111085772448149e-09\n",
      "Iteration: 8976 lambda_n: 0.9206833321410489 Loss: 1.2111096822072134e-09\n",
      "Iteration: 8977 lambda_n: 0.9769030510933107 Loss: 1.2111108170061264e-09\n",
      "Iteration: 8978 lambda_n: 0.9605116315098525 Loss: 1.2111120107945209e-09\n",
      "Iteration: 8979 lambda_n: 1.0023666216126301 Loss: 1.2111131811931477e-09\n",
      "Iteration: 8980 lambda_n: 0.9506311728104603 Loss: 1.2111143967720217e-09\n",
      "Iteration: 8981 lambda_n: 0.9553322047207845 Loss: 1.2111155467448845e-09\n",
      "Iteration: 8982 lambda_n: 0.9517996978245279 Loss: 1.2111166928885285e-09\n",
      "Iteration: 8983 lambda_n: 0.8752003016849443 Loss: 1.2111178294291173e-09\n",
      "Iteration: 8984 lambda_n: 0.9220396003858713 Loss: 1.2111188712796368e-09\n",
      "Iteration: 8985 lambda_n: 1.027149484537406 Loss: 1.211119962665126e-09\n",
      "Iteration: 8986 lambda_n: 0.996494866378769 Loss: 1.2111211733141445e-09\n",
      "Iteration: 8987 lambda_n: 1.0127303856986327 Loss: 1.2111223435648665e-09\n",
      "Iteration: 8988 lambda_n: 0.8913830105635759 Loss: 1.2111235238088146e-09\n",
      "Iteration: 8989 lambda_n: 0.9481006272789722 Loss: 1.2111245572976116e-09\n",
      "Iteration: 8990 lambda_n: 0.9425584137071991 Loss: 1.2111256558661953e-09\n",
      "Iteration: 8991 lambda_n: 0.8841990706442323 Loss: 1.2111267384217744e-09\n",
      "Iteration: 8992 lambda_n: 0.936654369039041 Loss: 1.2111277498051252e-09\n",
      "Iteration: 8993 lambda_n: 0.8871863307276479 Loss: 1.2111288192991866e-09\n",
      "Iteration: 8994 lambda_n: 0.9722014244690598 Loss: 1.2111298269381908e-09\n",
      "Iteration: 8995 lambda_n: 0.9807503268638174 Loss: 1.2111309231395209e-09\n",
      "Iteration: 8996 lambda_n: 0.9462731367491383 Loss: 1.2111320299893956e-09\n",
      "Iteration: 8997 lambda_n: 0.9971235571761724 Loss: 1.2111330886164693e-09\n",
      "Iteration: 8998 lambda_n: 0.9119619211277872 Loss: 1.2111342037113725e-09\n",
      "Iteration: 8999 lambda_n: 0.9250023647973641 Loss: 1.2111352123053894e-09\n",
      "Iteration: 9000 lambda_n: 0.9859760754466179 Loss: 1.2111362385499445e-09\n",
      "Iteration: 9001 lambda_n: 0.8819939713334447 Loss: 1.2111373258473762e-09\n",
      "Iteration: 9002 lambda_n: 1.0063238045020872 Loss: 1.211138290857734e-09\n",
      "Iteration: 9003 lambda_n: 0.9390389229154693 Loss: 1.2111393877239246e-09\n",
      "Iteration: 9004 lambda_n: 1.0072941520287566 Loss: 1.2111404071190635e-09\n",
      "Iteration: 9005 lambda_n: 1.0009828341935347 Loss: 1.211141498212354e-09\n",
      "Iteration: 9006 lambda_n: 0.9617007752137026 Loss: 1.2111425744259145e-09\n",
      "Iteration: 9007 lambda_n: 0.9778229254596514 Loss: 1.211143601799651e-09\n",
      "Iteration: 9008 lambda_n: 0.9893329036250937 Loss: 1.2111446434662964e-09\n",
      "Iteration: 9009 lambda_n: 0.9039406395668566 Loss: 1.211145691078937e-09\n",
      "Iteration: 9010 lambda_n: 0.9943997969590397 Loss: 1.2111466452948352e-09\n",
      "Iteration: 9011 lambda_n: 1.0231519416832227 Loss: 1.2111476925498096e-09\n",
      "Iteration: 9012 lambda_n: 0.9896700157080262 Loss: 1.2111487596928388e-09\n",
      "Iteration: 9013 lambda_n: 0.9133151799203764 Loss: 1.211149789369161e-09\n",
      "Iteration: 9014 lambda_n: 1.0249856176546719 Loss: 1.211150738705027e-09\n",
      "Iteration: 9015 lambda_n: 0.9860182363705777 Loss: 1.2111517960440708e-09\n",
      "Iteration: 9016 lambda_n: 0.9029843998534729 Loss: 1.2111528048555998e-09\n",
      "Iteration: 9017 lambda_n: 1.0236426121360358 Loss: 1.2111537261520573e-09\n",
      "Iteration: 9018 lambda_n: 0.9435762808144726 Loss: 1.2111547658067361e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9019 lambda_n: 0.9961730141083964 Loss: 1.2111557210148999e-09\n",
      "Iteration: 9020 lambda_n: 0.8889654792809301 Loss: 1.211156722528795e-09\n",
      "Iteration: 9021 lambda_n: 0.9605329272543844 Loss: 1.2111576141517267e-09\n",
      "Iteration: 9022 lambda_n: 0.954995864067613 Loss: 1.211158572089603e-09\n",
      "Iteration: 9023 lambda_n: 0.8822292276501318 Loss: 1.2111595208283074e-09\n",
      "Iteration: 9024 lambda_n: 0.9760968718714642 Loss: 1.2111603936533539e-09\n",
      "Iteration: 9025 lambda_n: 1.0152202560061323 Loss: 1.211161354587738e-09\n",
      "Iteration: 9026 lambda_n: 0.8742021059794081 Loss: 1.2111623502576125e-09\n",
      "Iteration: 9027 lambda_n: 1.0126027593766775 Loss: 1.2111632035646112e-09\n",
      "Iteration: 9028 lambda_n: 1.021123413456478 Loss: 1.2111641887596948e-09\n",
      "Iteration: 9029 lambda_n: 0.9962763154384857 Loss: 1.2111651746169208e-09\n",
      "Iteration: 9030 lambda_n: 0.9251891553411057 Loss: 1.211166133582327e-09\n",
      "Iteration: 9031 lambda_n: 1.0152170572591512 Loss: 1.2111670181420455e-09\n",
      "Iteration: 9032 lambda_n: 0.9630631846931949 Loss: 1.211167986685265e-09\n",
      "Iteration: 9033 lambda_n: 0.9806451639255774 Loss: 1.2111688961615787e-09\n",
      "Iteration: 9034 lambda_n: 0.8792511890392782 Loss: 1.2111698212505346e-09\n",
      "Iteration: 9035 lambda_n: 1.0079948886640555 Loss: 1.2111706463537722e-09\n",
      "Iteration: 9036 lambda_n: 0.920730531489684 Loss: 1.211171583825374e-09\n",
      "Iteration: 9037 lambda_n: 1.0155488228025116 Loss: 1.2111724413067046e-09\n",
      "Iteration: 9038 lambda_n: 0.9560825225263625 Loss: 1.2111733806966387e-09\n",
      "Iteration: 9039 lambda_n: 0.902541821123747 Loss: 1.2111742585239347e-09\n",
      "Iteration: 9040 lambda_n: 0.9693197834789067 Loss: 1.2111750867713294e-09\n",
      "Iteration: 9041 lambda_n: 0.9548974077027601 Loss: 1.2111759710452542e-09\n",
      "Iteration: 9042 lambda_n: 1.0064498031291145 Loss: 1.2111768390956533e-09\n",
      "Iteration: 9043 lambda_n: 0.9890026482376033 Loss: 1.2111777492219877e-09\n",
      "Iteration: 9044 lambda_n: 0.9407562685706142 Loss: 1.21117863987176e-09\n",
      "Iteration: 9045 lambda_n: 0.8907693358896613 Loss: 1.211179481206124e-09\n",
      "Iteration: 9046 lambda_n: 0.9372765676860202 Loss: 1.21118027507102e-09\n",
      "Iteration: 9047 lambda_n: 0.9356197977539952 Loss: 1.2111811060557517e-09\n",
      "Iteration: 9048 lambda_n: 1.0050327451496701 Loss: 1.2111819378419686e-09\n",
      "Iteration: 9049 lambda_n: 0.8778955842207229 Loss: 1.2111828205807295e-09\n",
      "Iteration: 9050 lambda_n: 0.8856734812555821 Loss: 1.211183589922567e-09\n",
      "Iteration: 9051 lambda_n: 0.9808419972646198 Loss: 1.211184363146777e-09\n",
      "Iteration: 9052 lambda_n: 0.9135150664147833 Loss: 1.2111852168197207e-09\n",
      "Iteration: 9053 lambda_n: 0.9448210997044194 Loss: 1.2111860040385609e-09\n",
      "Iteration: 9054 lambda_n: 0.9862339614080983 Loss: 1.2111868188972309e-09\n",
      "Iteration: 9055 lambda_n: 0.9392434652138919 Loss: 1.2111876637056001e-09\n",
      "Iteration: 9056 lambda_n: 0.958412485565052 Loss: 1.2111884661245656e-09\n",
      "Iteration: 9057 lambda_n: 0.9759709301661821 Loss: 1.2111892773388056e-09\n",
      "Iteration: 9058 lambda_n: 0.9949715569216628 Loss: 1.2111901022405573e-09\n",
      "Iteration: 9059 lambda_n: 0.8779868483716482 Loss: 1.2111909397558656e-09\n",
      "Iteration: 9060 lambda_n: 1.006706849614751 Loss: 1.2111916735591607e-09\n",
      "Iteration: 9061 lambda_n: 0.9757701421123799 Loss: 1.2111925137126677e-09\n",
      "Iteration: 9062 lambda_n: 0.9998165547088987 Loss: 1.211193320262259e-09\n",
      "Iteration: 9063 lambda_n: 0.9735681267478485 Loss: 1.2111941473556013e-09\n",
      "Iteration: 9064 lambda_n: 0.9386482119103757 Loss: 1.2111949471831005e-09\n",
      "Iteration: 9065 lambda_n: 0.9206316097041954 Loss: 1.2111957131333953e-09\n",
      "Iteration: 9066 lambda_n: 0.8916149986989261 Loss: 1.2111964628162649e-09\n",
      "Iteration: 9067 lambda_n: 0.9139607244463575 Loss: 1.2111971856323482e-09\n",
      "Iteration: 9068 lambda_n: 0.9083716002943034 Loss: 1.2111979235558927e-09\n",
      "Iteration: 9069 lambda_n: 0.933402757136155 Loss: 1.2111986503522829e-09\n",
      "Iteration: 9070 lambda_n: 1.010126480455211 Loss: 1.2111993953214157e-09\n",
      "Iteration: 9071 lambda_n: 0.8742155317908537 Loss: 1.211200197944847e-09\n",
      "Iteration: 9072 lambda_n: 0.9589004286135585 Loss: 1.2112008928414762e-09\n",
      "Iteration: 9073 lambda_n: 0.9276946975696454 Loss: 1.211201650172336e-09\n",
      "Iteration: 9074 lambda_n: 1.0244483447010273 Loss: 1.2112023804600795e-09\n",
      "Iteration: 9075 lambda_n: 0.936985834008662 Loss: 1.2112031787272613e-09\n",
      "Iteration: 9076 lambda_n: 0.9628571514753506 Loss: 1.211203909047665e-09\n",
      "Iteration: 9077 lambda_n: 0.8843273997879864 Loss: 1.2112046538849472e-09\n",
      "Iteration: 9078 lambda_n: 0.9814976305450693 Loss: 1.2112053393771226e-09\n",
      "Iteration: 9079 lambda_n: 0.9244848110654278 Loss: 1.2112060887571365e-09\n",
      "Iteration: 9080 lambda_n: 0.9190906984771321 Loss: 1.2112067965237385e-09\n",
      "Iteration: 9081 lambda_n: 1.0103013791134872 Loss: 1.2112074980291742e-09\n",
      "Iteration: 9082 lambda_n: 0.9334784892493686 Loss: 1.2112082606763057e-09\n",
      "Iteration: 9083 lambda_n: 0.9863267888853776 Loss: 1.2112089651678404e-09\n",
      "Iteration: 9084 lambda_n: 0.9666951519101301 Loss: 1.2112097058831289e-09\n",
      "Iteration: 9085 lambda_n: 0.8854170566418788 Loss: 1.2112104253749431e-09\n",
      "Iteration: 9086 lambda_n: 0.970904258244049 Loss: 1.2112110825247081e-09\n",
      "Iteration: 9087 lambda_n: 1.0083167811523142 Loss: 1.2112117998937918e-09\n",
      "Iteration: 9088 lambda_n: 1.0194133183639698 Loss: 1.211212545256615e-09\n",
      "Iteration: 9089 lambda_n: 1.0248856867078084 Loss: 1.2112132906001561e-09\n",
      "Iteration: 9090 lambda_n: 1.027462229424323 Loss: 1.2112140380266835e-09\n",
      "Iteration: 9091 lambda_n: 0.8862794799145051 Loss: 1.2112147862134863e-09\n",
      "Iteration: 9092 lambda_n: 0.9084960193856719 Loss: 1.211215426637192e-09\n",
      "Iteration: 9093 lambda_n: 0.9323844415547781 Loss: 1.2112160819177051e-09\n",
      "Iteration: 9094 lambda_n: 0.9139838242604422 Loss: 1.211216747842938e-09\n",
      "Iteration: 9095 lambda_n: 0.9157572273024109 Loss: 1.2112174007208288e-09\n",
      "Iteration: 9096 lambda_n: 0.9199958416061408 Loss: 1.211218050868938e-09\n",
      "Iteration: 9097 lambda_n: 0.9057610873685855 Loss: 1.2112186996600311e-09\n",
      "Iteration: 9098 lambda_n: 0.9483387128282719 Loss: 1.211219339831679e-09\n",
      "Iteration: 9099 lambda_n: 0.9990767872183349 Loss: 1.2112200050247836e-09\n",
      "Iteration: 9100 lambda_n: 0.916993806906702 Loss: 1.2112206997489e-09\n",
      "Iteration: 9101 lambda_n: 0.9632017730283877 Loss: 1.2112213366484036e-09\n",
      "Iteration: 9102 lambda_n: 0.8902092366351603 Loss: 1.2112220003441906e-09\n",
      "Iteration: 9103 lambda_n: 0.883755970304467 Loss: 1.211222613326398e-09\n",
      "Iteration: 9104 lambda_n: 1.0062423889761718 Loss: 1.2112232169029598e-09\n",
      "Iteration: 9105 lambda_n: 0.905652008562901 Loss: 1.2112239009131792e-09\n",
      "Iteration: 9106 lambda_n: 0.9313701965102267 Loss: 1.211224516739318e-09\n",
      "Iteration: 9107 lambda_n: 0.9917315941430853 Loss: 1.2112251431724031e-09\n",
      "Iteration: 9108 lambda_n: 1.0080928577612764 Loss: 1.2112258099240606e-09\n",
      "Iteration: 9109 lambda_n: 0.8771719147961541 Loss: 1.2112264846378081e-09\n",
      "Iteration: 9110 lambda_n: 0.9209843284982802 Loss: 1.2112270715275366e-09\n",
      "Iteration: 9111 lambda_n: 0.913452578500228 Loss: 1.211227679556688e-09\n",
      "Iteration: 9112 lambda_n: 0.971978095496693 Loss: 1.2112282844122173e-09\n",
      "Iteration: 9113 lambda_n: 0.8908037432062359 Loss: 1.2112289197301457e-09\n",
      "Iteration: 9114 lambda_n: 0.9761899651963231 Loss: 1.211229503338259e-09\n",
      "Iteration: 9115 lambda_n: 0.960393911114425 Loss: 1.2112301419034114e-09\n",
      "Iteration: 9116 lambda_n: 0.8962166082288826 Loss: 1.2112307644704178e-09\n",
      "Iteration: 9117 lambda_n: 0.9382109243152774 Loss: 1.2112313419206771e-09\n",
      "Iteration: 9118 lambda_n: 0.967518812205764 Loss: 1.211231942910862e-09\n",
      "Iteration: 9119 lambda_n: 0.8756477645275134 Loss: 1.2112325606853665e-09\n",
      "Iteration: 9120 lambda_n: 0.9067224210647996 Loss: 1.21123311679495e-09\n",
      "Iteration: 9121 lambda_n: 0.9292038993430188 Loss: 1.2112336922120289e-09\n",
      "Iteration: 9122 lambda_n: 0.9830416829844897 Loss: 1.2112342750418887e-09\n",
      "Iteration: 9123 lambda_n: 0.9722483730687776 Loss: 1.2112348927936313e-09\n",
      "Iteration: 9124 lambda_n: 0.9710710569642029 Loss: 1.2112354999946536e-09\n",
      "Iteration: 9125 lambda_n: 1.013317392860673 Loss: 1.2112361027178862e-09\n",
      "Iteration: 9126 lambda_n: 1.0217267730948478 Loss: 1.211236725087391e-09\n",
      "Iteration: 9127 lambda_n: 0.9680912616523636 Loss: 1.2112373540847873e-09\n",
      "Iteration: 9128 lambda_n: 0.8868631690159821 Loss: 1.2112379471323948e-09\n",
      "Iteration: 9129 lambda_n: 0.9733856161450339 Loss: 1.211238489671273e-09\n",
      "Iteration: 9130 lambda_n: 0.9321777849648661 Loss: 1.2112390829188388e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9131 lambda_n: 0.9908828125732445 Loss: 1.2112396443200205e-09\n",
      "Iteration: 9132 lambda_n: 0.9864032034296476 Loss: 1.2112402402262573e-09\n",
      "Iteration: 9133 lambda_n: 1.0103153363884247 Loss: 1.2112408311741314e-09\n",
      "Iteration: 9134 lambda_n: 0.9911120922855189 Loss: 1.211241431764445e-09\n",
      "Iteration: 9135 lambda_n: 0.996891314093055 Loss: 1.2112420191117968e-09\n",
      "Iteration: 9136 lambda_n: 0.8922644159177412 Loss: 1.2112426082388765e-09\n",
      "Iteration: 9137 lambda_n: 0.9676574269714129 Loss: 1.211243134032687e-09\n",
      "Iteration: 9138 lambda_n: 0.8790520069838444 Loss: 1.2112436979184173e-09\n",
      "Iteration: 9139 lambda_n: 0.8868946972000366 Loss: 1.2112442099232945e-09\n",
      "Iteration: 9140 lambda_n: 0.9488779866041458 Loss: 1.2112447239343553e-09\n",
      "Iteration: 9141 lambda_n: 0.9474796345172122 Loss: 1.2112452710012396e-09\n",
      "Iteration: 9142 lambda_n: 0.898736870633441 Loss: 1.2112458156755102e-09\n",
      "Iteration: 9143 lambda_n: 1.0021887868018897 Loss: 1.2112463318599755e-09\n",
      "Iteration: 9144 lambda_n: 0.9518986051293958 Loss: 1.2112469031571844e-09\n",
      "Iteration: 9145 lambda_n: 0.9238342668716 Loss: 1.211247442368926e-09\n",
      "Iteration: 9146 lambda_n: 0.9223296019813886 Loss: 1.2112479631856696e-09\n",
      "Iteration: 9147 lambda_n: 0.9533991028213741 Loss: 1.2112484777675326e-09\n",
      "Iteration: 9148 lambda_n: 0.9584818999389569 Loss: 1.2112490119328254e-09\n",
      "Iteration: 9149 lambda_n: 0.990315621838173 Loss: 1.211249546412737e-09\n",
      "Iteration: 9150 lambda_n: 0.9823741292558429 Loss: 1.2112500926774394e-09\n",
      "Iteration: 9151 lambda_n: 0.9862491051571297 Loss: 1.2112506358187332e-09\n",
      "Iteration: 9152 lambda_n: 1.0018982659077584 Loss: 1.2112511796091764e-09\n",
      "Iteration: 9153 lambda_n: 0.9850195288577658 Loss: 1.2112517262376029e-09\n",
      "Iteration: 9154 lambda_n: 0.9196139106712841 Loss: 1.2112522627681954e-09\n",
      "Iteration: 9155 lambda_n: 0.9742022174523126 Loss: 1.211252760090567e-09\n",
      "Iteration: 9156 lambda_n: 0.9381822099034844 Loss: 1.2112532854392701e-09\n",
      "Iteration: 9157 lambda_n: 0.9157246404109248 Loss: 1.211253788954613e-09\n",
      "Iteration: 9158 lambda_n: 0.8932173850554159 Loss: 1.2112542759954596e-09\n",
      "Iteration: 9159 lambda_n: 0.9010092804242367 Loss: 1.2112547521209812e-09\n",
      "Iteration: 9160 lambda_n: 0.944690199737856 Loss: 1.2112552284721356e-09\n",
      "Iteration: 9161 lambda_n: 0.9958881049985598 Loss: 1.211255723452923e-09\n",
      "Iteration: 9162 lambda_n: 0.8814270396937157 Loss: 1.211256246523739e-09\n",
      "Iteration: 9163 lambda_n: 0.9710019737306081 Loss: 1.211256708231034e-09\n",
      "Iteration: 9164 lambda_n: 0.9474356623386808 Loss: 1.2112572112210513e-09\n",
      "Iteration: 9165 lambda_n: 0.8969574011420263 Loss: 1.211257700889356e-09\n",
      "Iteration: 9166 lambda_n: 1.0185606632865423 Loss: 1.2112581631060685e-09\n",
      "Iteration: 9167 lambda_n: 0.919388168778897 Loss: 1.2112586841217617e-09\n",
      "Iteration: 9168 lambda_n: 0.9294818275013702 Loss: 1.2112591520678348e-09\n",
      "Iteration: 9169 lambda_n: 0.939830802424345 Loss: 1.2112596247570785e-09\n",
      "Iteration: 9170 lambda_n: 0.9081572093216166 Loss: 1.2112600993303467e-09\n",
      "Iteration: 9171 lambda_n: 0.9636538139254702 Loss: 1.2112605576002932e-09\n",
      "Iteration: 9172 lambda_n: 0.9436064531734967 Loss: 1.2112610402167214e-09\n",
      "Iteration: 9173 lambda_n: 0.8967853135726744 Loss: 1.2112615108802673e-09\n",
      "Iteration: 9174 lambda_n: 0.9823178204630584 Loss: 1.2112619549715772e-09\n",
      "Iteration: 9175 lambda_n: 0.8755042180240038 Loss: 1.2112624391843213e-09\n",
      "Iteration: 9176 lambda_n: 0.9957682360606416 Loss: 1.2112628696421184e-09\n",
      "Iteration: 9177 lambda_n: 0.9749220220335302 Loss: 1.2112633563396316e-09\n",
      "Iteration: 9178 lambda_n: 0.9393943742232319 Loss: 1.2112638312466616e-09\n",
      "Iteration: 9179 lambda_n: 0.8821291196027782 Loss: 1.2112642879908175e-09\n",
      "Iteration: 9180 lambda_n: 0.8990848259027605 Loss: 1.2112647144308729e-09\n",
      "Iteration: 9181 lambda_n: 0.8778755783518577 Loss: 1.211265146494511e-09\n",
      "Iteration: 9182 lambda_n: 0.9913308379838729 Loss: 1.211265570156277e-09\n",
      "Iteration: 9183 lambda_n: 0.994433927869052 Loss: 1.2112660373879928e-09\n",
      "Iteration: 9184 lambda_n: 1.0238772094861395 Loss: 1.211266508717856e-09\n",
      "Iteration: 9185 lambda_n: 0.9309807774432025 Loss: 1.2112669945755013e-09\n",
      "Iteration: 9186 lambda_n: 1.0269509109240165 Loss: 1.2112674315543582e-09\n",
      "Iteration: 9187 lambda_n: 0.8938767759648022 Loss: 1.2112679134993955e-09\n",
      "Iteration: 9188 lambda_n: 0.8842650532074027 Loss: 1.2112683276078339e-09\n",
      "Iteration: 9189 lambda_n: 0.8760928035971687 Loss: 1.2112687393640001e-09\n",
      "Iteration: 9190 lambda_n: 1.0274015314196936 Loss: 1.2112691425198492e-09\n",
      "Iteration: 9191 lambda_n: 0.8785724407109622 Loss: 1.21126961847645e-09\n",
      "Iteration: 9192 lambda_n: 0.9407360008144057 Loss: 1.2112700181438452e-09\n",
      "Iteration: 9193 lambda_n: 0.9363003703156934 Loss: 1.2112704482461308e-09\n",
      "Iteration: 9194 lambda_n: 0.8972445003021996 Loss: 1.2112708711687814e-09\n",
      "Iteration: 9195 lambda_n: 0.8945646990671443 Loss: 1.2112712786445035e-09\n",
      "Iteration: 9196 lambda_n: 0.9186906668660455 Loss: 1.2112716822566611e-09\n",
      "Iteration: 9197 lambda_n: 0.9445998105707796 Loss: 1.211272091292361e-09\n",
      "Iteration: 9198 lambda_n: 0.8906113816036958 Loss: 1.2112725149297867e-09\n",
      "Iteration: 9199 lambda_n: 0.9221840089685315 Loss: 1.211272906858726e-09\n",
      "Iteration: 9200 lambda_n: 0.9546961028598651 Loss: 1.2112733145729968e-09\n",
      "Iteration: 9201 lambda_n: 0.9619595908150836 Loss: 1.2112737370999929e-09\n",
      "Iteration: 9202 lambda_n: 0.881022573437265 Loss: 1.211274158031098e-09\n",
      "Iteration: 9203 lambda_n: 0.9675611356234648 Loss: 1.2112745474895667e-09\n",
      "Iteration: 9204 lambda_n: 0.9786619809940025 Loss: 1.2112749651913405e-09\n",
      "Iteration: 9205 lambda_n: 0.9953350240091471 Loss: 1.21127539282242e-09\n",
      "Iteration: 9206 lambda_n: 0.8782341418792835 Loss: 1.2112758217555026e-09\n",
      "Iteration: 9207 lambda_n: 0.9531023376704607 Loss: 1.2112761967927255e-09\n",
      "Iteration: 9208 lambda_n: 0.9424050143947186 Loss: 1.2112766025521249e-09\n",
      "Iteration: 9209 lambda_n: 1.027620097711482 Loss: 1.2112770035340126e-09\n",
      "Iteration: 9210 lambda_n: 0.9299354866365095 Loss: 1.2112774344602943e-09\n",
      "Iteration: 9211 lambda_n: 0.9095407376317148 Loss: 1.2112778259563725e-09\n",
      "Iteration: 9212 lambda_n: 0.9586320750324423 Loss: 1.211278204863655e-09\n",
      "Iteration: 9213 lambda_n: 0.905240866978954 Loss: 1.2112786058599294e-09\n",
      "Iteration: 9214 lambda_n: 0.9422957837774825 Loss: 1.2112789824918268e-09\n",
      "Iteration: 9215 lambda_n: 0.9495043001849777 Loss: 1.2112793664609945e-09\n",
      "Iteration: 9216 lambda_n: 0.9644224307783839 Loss: 1.2112797575908314e-09\n",
      "Iteration: 9217 lambda_n: 0.9530983168317713 Loss: 1.2112801512761945e-09\n",
      "Iteration: 9218 lambda_n: 0.877587638146795 Loss: 1.211280535104993e-09\n",
      "Iteration: 9219 lambda_n: 0.8777847120558367 Loss: 1.2112808907443904e-09\n",
      "Iteration: 9220 lambda_n: 0.979981625005322 Loss: 1.2112812453346161e-09\n",
      "Iteration: 9221 lambda_n: 0.920500396219864 Loss: 1.2112816352041942e-09\n",
      "Iteration: 9222 lambda_n: 0.8989756064069766 Loss: 1.211282004768654e-09\n",
      "Iteration: 9223 lambda_n: 0.8911914392060171 Loss: 1.2112823620013692e-09\n",
      "Iteration: 9224 lambda_n: 0.8840756088400342 Loss: 1.2112827170611482e-09\n",
      "Iteration: 9225 lambda_n: 0.9898140436706868 Loss: 1.211283063145067e-09\n",
      "Iteration: 9226 lambda_n: 0.9270849635729785 Loss: 1.2112834501847903e-09\n",
      "Iteration: 9227 lambda_n: 0.8794804986959115 Loss: 1.2112838129699798e-09\n",
      "Iteration: 9228 lambda_n: 0.9213075042122677 Loss: 1.2112841530514474e-09\n",
      "Iteration: 9229 lambda_n: 0.9879207787173095 Loss: 1.211284508369602e-09\n",
      "Iteration: 9230 lambda_n: 0.9723934468243008 Loss: 1.2112848894273288e-09\n",
      "Iteration: 9231 lambda_n: 0.9879774771254962 Loss: 1.211285261339354e-09\n",
      "Iteration: 9232 lambda_n: 1.0076914764155374 Loss: 1.211285638820752e-09\n",
      "Iteration: 9233 lambda_n: 0.9132520990840992 Loss: 1.2112860179131986e-09\n",
      "Iteration: 9234 lambda_n: 0.9239625890797629 Loss: 1.2112863635584094e-09\n",
      "Iteration: 9235 lambda_n: 0.9787681323376635 Loss: 1.211286711910838e-09\n",
      "Iteration: 9236 lambda_n: 0.9276500392252187 Loss: 1.2112870768591765e-09\n",
      "Iteration: 9237 lambda_n: 0.9257415566520688 Loss: 1.2112874211175896e-09\n",
      "Iteration: 9238 lambda_n: 0.9923684857833429 Loss: 1.211287764956864e-09\n",
      "Iteration: 9239 lambda_n: 0.9023501655624916 Loss: 1.2112881324754401e-09\n",
      "Iteration: 9240 lambda_n: 1.0181175172214492 Loss: 1.211288466224394e-09\n",
      "Iteration: 9241 lambda_n: 0.8800415109859987 Loss: 1.2112888357258404e-09\n",
      "Iteration: 9242 lambda_n: 0.9664774260010401 Loss: 1.2112891571826216e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9243 lambda_n: 0.8907024980619023 Loss: 1.2112895097863667e-09\n",
      "Iteration: 9244 lambda_n: 0.9403265111468956 Loss: 1.2112898317549277e-09\n",
      "Iteration: 9245 lambda_n: 0.8828071372136032 Loss: 1.2112901719665756e-09\n",
      "Iteration: 9246 lambda_n: 1.0115221094898121 Loss: 1.211290486573521e-09\n",
      "Iteration: 9247 lambda_n: 0.9725129806550278 Loss: 1.2112908476531266e-09\n",
      "Iteration: 9248 lambda_n: 0.9060879198777665 Loss: 1.2112911945579763e-09\n",
      "Iteration: 9249 lambda_n: 0.8794738696133542 Loss: 1.2112915166455862e-09\n",
      "Iteration: 9250 lambda_n: 1.0141232430075011 Loss: 1.211291824124323e-09\n",
      "Iteration: 9251 lambda_n: 1.0198634654325947 Loss: 1.2112921790464318e-09\n",
      "Iteration: 9252 lambda_n: 0.9548120399199003 Loss: 1.2112925356236097e-09\n",
      "Iteration: 9253 lambda_n: 0.9031186580734736 Loss: 1.2112928685346973e-09\n",
      "Iteration: 9254 lambda_n: 0.9967730450768081 Loss: 1.2112931798488412e-09\n",
      "Iteration: 9255 lambda_n: 0.9173022844829984 Loss: 1.2112935223115064e-09\n",
      "Iteration: 9256 lambda_n: 0.9264198620628809 Loss: 1.2112938388181872e-09\n",
      "Iteration: 9257 lambda_n: 0.9522276641341493 Loss: 1.2112941503583486e-09\n",
      "Iteration: 9258 lambda_n: 1.008044118404773 Loss: 1.2112944750747708e-09\n",
      "Iteration: 9259 lambda_n: 0.8936759561404123 Loss: 1.211294811009159e-09\n",
      "Iteration: 9260 lambda_n: 0.9857338823490774 Loss: 1.21129511358015e-09\n",
      "Iteration: 9261 lambda_n: 0.9022077883464411 Loss: 1.211295438406303e-09\n",
      "Iteration: 9262 lambda_n: 1.0199677068131647 Loss: 1.2112957403778584e-09\n",
      "Iteration: 9263 lambda_n: 1.01664496949947 Loss: 1.2112960756334528e-09\n",
      "Iteration: 9264 lambda_n: 0.9537781535252408 Loss: 1.2112964103890984e-09\n",
      "Iteration: 9265 lambda_n: 0.9760261114966822 Loss: 1.2112967224707639e-09\n",
      "Iteration: 9266 lambda_n: 0.9276686336684837 Loss: 1.2112970394464482e-09\n",
      "Iteration: 9267 lambda_n: 0.9859424973685119 Loss: 1.211297343070222e-09\n",
      "Iteration: 9268 lambda_n: 0.9244924832553656 Loss: 1.211297659259034e-09\n",
      "Iteration: 9269 lambda_n: 1.0187143585816039 Loss: 1.2112979597089465e-09\n",
      "Iteration: 9270 lambda_n: 0.9050301839794006 Loss: 1.211298282260505e-09\n",
      "Iteration: 9271 lambda_n: 0.8865644145671115 Loss: 1.2112985704105206e-09\n",
      "Iteration: 9272 lambda_n: 0.9619440100141498 Loss: 1.2112988517362786e-09\n",
      "Iteration: 9273 lambda_n: 0.9669289250826592 Loss: 1.2112991578909762e-09\n",
      "Iteration: 9274 lambda_n: 1.010798863571593 Loss: 1.2112994591115909e-09\n",
      "Iteration: 9275 lambda_n: 0.9554900736995445 Loss: 1.211299774568491e-09\n",
      "Iteration: 9276 lambda_n: 0.8964264459992876 Loss: 1.2113000722361056e-09\n",
      "Iteration: 9277 lambda_n: 0.8866425006869322 Loss: 1.2113003523382422e-09\n",
      "Iteration: 9278 lambda_n: 0.9820933200149897 Loss: 1.2113006271967466e-09\n",
      "Iteration: 9279 lambda_n: 0.8961988324016616 Loss: 1.211300926860587e-09\n",
      "Iteration: 9280 lambda_n: 0.9909963682207342 Loss: 1.2113011997344106e-09\n",
      "Iteration: 9281 lambda_n: 0.8942572752595863 Loss: 1.2113015015646686e-09\n",
      "Iteration: 9282 lambda_n: 1.0016499424684575 Loss: 1.2113017715983609e-09\n",
      "Iteration: 9283 lambda_n: 1.0073346765920863 Loss: 1.211302076446684e-09\n",
      "Iteration: 9284 lambda_n: 1.0072371724553504 Loss: 1.2113023770468322e-09\n",
      "Iteration: 9285 lambda_n: 0.9631288872309203 Loss: 1.2113026788249278e-09\n",
      "Iteration: 9286 lambda_n: 0.9476531708480852 Loss: 1.2113029646773011e-09\n",
      "Iteration: 9287 lambda_n: 0.903502637008067 Loss: 1.2113032448693948e-09\n",
      "Iteration: 9288 lambda_n: 0.9477156245639703 Loss: 1.2113035138936217e-09\n",
      "Iteration: 9289 lambda_n: 0.9815111082360526 Loss: 1.2113037872517175e-09\n",
      "Iteration: 9290 lambda_n: 1.026897412304816 Loss: 1.2113040748113027e-09\n",
      "Iteration: 9291 lambda_n: 0.956068563175863 Loss: 1.2113043717814174e-09\n",
      "Iteration: 9292 lambda_n: 1.0180606175994085 Loss: 1.2113046507500075e-09\n",
      "Iteration: 9293 lambda_n: 0.9581707256152455 Loss: 1.2113049406670792e-09\n",
      "Iteration: 9294 lambda_n: 0.8911200143874393 Loss: 1.2113052159613136e-09\n",
      "Iteration: 9295 lambda_n: 1.00706179366336 Loss: 1.211305469498442e-09\n",
      "Iteration: 9296 lambda_n: 0.8766299864979598 Loss: 1.211305755438659e-09\n",
      "Iteration: 9297 lambda_n: 0.940831911694251 Loss: 1.2113060034794575e-09\n",
      "Iteration: 9298 lambda_n: 0.9684987769404828 Loss: 1.2113062684520346e-09\n",
      "Iteration: 9299 lambda_n: 0.9769626338985238 Loss: 1.2113065386014995e-09\n",
      "Iteration: 9300 lambda_n: 0.9120363362722823 Loss: 1.2113068116244252e-09\n",
      "Iteration: 9301 lambda_n: 0.9628961279061407 Loss: 1.21130706499036e-09\n",
      "Iteration: 9302 lambda_n: 0.9242922808343872 Loss: 1.211307331331278e-09\n",
      "Iteration: 9303 lambda_n: 0.9117557380547813 Loss: 1.2113075873424604e-09\n",
      "Iteration: 9304 lambda_n: 0.9761360101576069 Loss: 1.211307833708975e-09\n",
      "Iteration: 9305 lambda_n: 0.9784164550923139 Loss: 1.2113081021789991e-09\n",
      "Iteration: 9306 lambda_n: 0.9612168905237971 Loss: 1.2113083654102703e-09\n",
      "Iteration: 9307 lambda_n: 0.975932859962774 Loss: 1.2113086245866432e-09\n",
      "Iteration: 9308 lambda_n: 0.984456970798642 Loss: 1.2113088853137044e-09\n",
      "Iteration: 9309 lambda_n: 1.0175554708669778 Loss: 1.2113091456540317e-09\n",
      "Iteration: 9310 lambda_n: 0.8805461621392275 Loss: 1.211309415698504e-09\n",
      "Iteration: 9311 lambda_n: 1.0221677763946466 Loss: 1.2113096499646282e-09\n",
      "Iteration: 9312 lambda_n: 0.8979682356773273 Loss: 1.2113099178285608e-09\n",
      "Iteration: 9313 lambda_n: 0.9286265008618185 Loss: 1.2113101539593168e-09\n",
      "Iteration: 9314 lambda_n: 0.8963547270905794 Loss: 1.211310395118014e-09\n",
      "Iteration: 9315 lambda_n: 0.9167484141807791 Loss: 1.2113106299788477e-09\n",
      "Iteration: 9316 lambda_n: 1.0197620750026242 Loss: 1.2113108670179613e-09\n",
      "Iteration: 9317 lambda_n: 0.9031763136605656 Loss: 1.2113111266405382e-09\n",
      "Iteration: 9318 lambda_n: 0.9745524453964185 Loss: 1.2113113590646838e-09\n",
      "Iteration: 9319 lambda_n: 0.9555225067659207 Loss: 1.2113116060821676e-09\n",
      "Iteration: 9320 lambda_n: 0.9909337793234321 Loss: 1.211311850426941e-09\n",
      "Iteration: 9321 lambda_n: 0.9105501162475754 Loss: 1.2113121001758804e-09\n",
      "Iteration: 9322 lambda_n: 0.9541402674850807 Loss: 1.2113123319849755e-09\n",
      "Iteration: 9323 lambda_n: 0.9380845506098369 Loss: 1.2113125705660257e-09\n",
      "Iteration: 9324 lambda_n: 0.8839522043828218 Loss: 1.2113128015159219e-09\n",
      "Iteration: 9325 lambda_n: 1.0195003598069245 Loss: 1.2113130222029828e-09\n",
      "Iteration: 9326 lambda_n: 0.8868431286922592 Loss: 1.2113132754910804e-09\n",
      "Iteration: 9327 lambda_n: 0.9119411489290387 Loss: 1.21131349264393e-09\n",
      "Iteration: 9328 lambda_n: 1.0088728077057296 Loss: 1.2113137180459581e-09\n",
      "Iteration: 9329 lambda_n: 1.0036440789195975 Loss: 1.2113139665370825e-09\n",
      "Iteration: 9330 lambda_n: 0.9112543937405716 Loss: 1.2113142094237965e-09\n",
      "Iteration: 9331 lambda_n: 0.9652836383059596 Loss: 1.2113144312106703e-09\n",
      "Iteration: 9332 lambda_n: 0.9126917148939045 Loss: 1.2113146654080577e-09\n",
      "Iteration: 9333 lambda_n: 0.9996684923761627 Loss: 1.2113148845698768e-09\n",
      "Iteration: 9334 lambda_n: 0.9632266189063993 Loss: 1.2113151249707918e-09\n",
      "Iteration: 9335 lambda_n: 0.9813604425670904 Loss: 1.2113153523913952e-09\n",
      "Iteration: 9336 lambda_n: 0.9847642600505586 Loss: 1.2113155865692408e-09\n",
      "Iteration: 9337 lambda_n: 0.9499508818544965 Loss: 1.2113158158826402e-09\n",
      "Iteration: 9338 lambda_n: 0.8936110410809418 Loss: 1.2113160396847646e-09\n",
      "Iteration: 9339 lambda_n: 0.9327134940861654 Loss: 1.211316249798367e-09\n",
      "Iteration: 9340 lambda_n: 0.897999790194334 Loss: 1.2113164682437015e-09\n",
      "Iteration: 9341 lambda_n: 0.8829978362901244 Loss: 1.2113166766668031e-09\n",
      "Iteration: 9342 lambda_n: 0.9815820030516539 Loss: 1.2113168778082157e-09\n",
      "Iteration: 9343 lambda_n: 0.9287688452980083 Loss: 1.2113171024765089e-09\n",
      "Iteration: 9344 lambda_n: 0.9395523906364585 Loss: 1.211317313289178e-09\n",
      "Iteration: 9345 lambda_n: 0.9609820348724368 Loss: 1.2113175303296806e-09\n",
      "Iteration: 9346 lambda_n: 0.8814036430259189 Loss: 1.2113177433312559e-09\n",
      "Iteration: 9347 lambda_n: 1.0108026980081954 Loss: 1.2113179435152581e-09\n",
      "Iteration: 9348 lambda_n: 0.9692208093462198 Loss: 1.2113181706112824e-09\n",
      "Iteration: 9349 lambda_n: 0.9137029839233548 Loss: 1.2113183874622134e-09\n",
      "Iteration: 9350 lambda_n: 0.9345439816740634 Loss: 1.2113185878291222e-09\n",
      "Iteration: 9351 lambda_n: 0.9755881725863538 Loss: 1.2113187913137273e-09\n",
      "Iteration: 9352 lambda_n: 0.9195409519339356 Loss: 1.2113190049685443e-09\n",
      "Iteration: 9353 lambda_n: 1.0089130027881783 Loss: 1.2113192052520063e-09\n",
      "Iteration: 9354 lambda_n: 0.9256992865487336 Loss: 1.2113194243104793e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9355 lambda_n: 0.9032738251558825 Loss: 1.2113196242358501e-09\n",
      "Iteration: 9356 lambda_n: 0.9140177776341017 Loss: 1.2113198203523896e-09\n",
      "Iteration: 9357 lambda_n: 0.9970160686808206 Loss: 1.2113200133411198e-09\n",
      "Iteration: 9358 lambda_n: 0.9686931987005828 Loss: 1.2113202269832666e-09\n",
      "Iteration: 9359 lambda_n: 0.9746117218122475 Loss: 1.2113204307378064e-09\n",
      "Iteration: 9360 lambda_n: 0.9451465936199466 Loss: 1.2113206396427418e-09\n",
      "Iteration: 9361 lambda_n: 0.9989479855460937 Loss: 1.2113208380577006e-09\n",
      "Iteration: 9362 lambda_n: 0.9835907490493601 Loss: 1.2113210493912098e-09\n",
      "Iteration: 9363 lambda_n: 0.894687536512342 Loss: 1.2113212518234627e-09\n",
      "Iteration: 9364 lambda_n: 0.937463903833382 Loss: 1.2113214403243885e-09\n",
      "Iteration: 9365 lambda_n: 0.9286732546928448 Loss: 1.2113216329448043e-09\n",
      "Iteration: 9366 lambda_n: 0.8736631187304099 Loss: 1.211321823678592e-09\n",
      "Iteration: 9367 lambda_n: 0.9936060908976524 Loss: 1.2113220010706345e-09\n",
      "Iteration: 9368 lambda_n: 0.8808805688422835 Loss: 1.2113222061315255e-09\n",
      "Iteration: 9369 lambda_n: 0.9109278056634637 Loss: 1.211322384998139e-09\n",
      "Iteration: 9370 lambda_n: 0.9037403058197835 Loss: 1.211322569876986e-09\n",
      "Iteration: 9371 lambda_n: 0.9619074043422424 Loss: 1.2113227510510436e-09\n",
      "Iteration: 9372 lambda_n: 0.9848098474846702 Loss: 1.211322945218176e-09\n",
      "Iteration: 9373 lambda_n: 0.9325184528196301 Loss: 1.2113231420703884e-09\n",
      "Iteration: 9374 lambda_n: 0.948340058386798 Loss: 1.2113233243337175e-09\n",
      "Iteration: 9375 lambda_n: 0.9612077666485626 Loss: 1.2113235137008077e-09\n",
      "Iteration: 9376 lambda_n: 0.9185848839123585 Loss: 1.2113237031460266e-09\n",
      "Iteration: 9377 lambda_n: 0.9482430410790418 Loss: 1.2113238834127636e-09\n",
      "Iteration: 9378 lambda_n: 0.9576076384851777 Loss: 1.2113240685997897e-09\n",
      "Iteration: 9379 lambda_n: 0.8824079813263485 Loss: 1.211324254960758e-09\n",
      "Iteration: 9380 lambda_n: 0.9949315016636918 Loss: 1.2113244278317621e-09\n",
      "Iteration: 9381 lambda_n: 0.9775158062633393 Loss: 1.2113246224317913e-09\n",
      "Iteration: 9382 lambda_n: 0.9837570025988894 Loss: 1.2113248080984169e-09\n",
      "Iteration: 9383 lambda_n: 0.9697540462282233 Loss: 1.2113249941249049e-09\n",
      "Iteration: 9384 lambda_n: 0.9329126267525202 Loss: 1.2113251799248205e-09\n",
      "Iteration: 9385 lambda_n: 0.8764914363694373 Loss: 1.2113253587338233e-09\n",
      "Iteration: 9386 lambda_n: 0.887479758989355 Loss: 1.2113255230919152e-09\n",
      "Iteration: 9387 lambda_n: 0.9779684611883849 Loss: 1.2113256877390946e-09\n",
      "Iteration: 9388 lambda_n: 0.9155614755208322 Loss: 1.211325872199302e-09\n",
      "Iteration: 9389 lambda_n: 0.9373948325648369 Loss: 1.2113260412135306e-09\n",
      "Iteration: 9390 lambda_n: 0.9389133519884261 Loss: 1.2113262133639132e-09\n",
      "Iteration: 9391 lambda_n: 0.9226622267511313 Loss: 1.2113263856719061e-09\n",
      "Iteration: 9392 lambda_n: 0.9072831980790098 Loss: 1.211326552979496e-09\n",
      "Iteration: 9393 lambda_n: 1.016039617756149 Loss: 1.2113267210188177e-09\n",
      "Iteration: 9394 lambda_n: 0.9505897845443598 Loss: 1.2113269040401478e-09\n",
      "Iteration: 9395 lambda_n: 0.8880362642378141 Loss: 1.2113270773565326e-09\n",
      "Iteration: 9396 lambda_n: 0.9073006869499505 Loss: 1.211327235336598e-09\n",
      "Iteration: 9397 lambda_n: 0.9416958664308619 Loss: 1.2113273994266881e-09\n",
      "Iteration: 9398 lambda_n: 0.9883704161878781 Loss: 1.2113275657647111e-09\n",
      "Iteration: 9399 lambda_n: 0.9513144054069488 Loss: 1.2113277379014943e-09\n",
      "Iteration: 9400 lambda_n: 0.9217577952465178 Loss: 1.2113279081173485e-09\n",
      "Iteration: 9401 lambda_n: 0.9239342416834033 Loss: 1.211328067080014e-09\n",
      "Iteration: 9402 lambda_n: 0.95516350689772 Loss: 1.2113282300445374e-09\n",
      "Iteration: 9403 lambda_n: 0.9602229027125427 Loss: 1.2113283933834597e-09\n",
      "Iteration: 9404 lambda_n: 0.908671639809939 Loss: 1.2113285602474753e-09\n",
      "Iteration: 9405 lambda_n: 1.024398031910455 Loss: 1.2113287145212419e-09\n",
      "Iteration: 9406 lambda_n: 0.9138756996317163 Loss: 1.2113288928736509e-09\n",
      "Iteration: 9407 lambda_n: 0.9789498372172873 Loss: 1.2113290458498245e-09\n",
      "Iteration: 9408 lambda_n: 1.0093121770886722 Loss: 1.2113292140204504e-09\n",
      "Iteration: 9409 lambda_n: 1.006458563683091 Loss: 1.2113293866172095e-09\n",
      "Iteration: 9410 lambda_n: 1.0126640475344466 Loss: 1.2113295531321623e-09\n",
      "Iteration: 9411 lambda_n: 1.0252005606860495 Loss: 1.2113297232683695e-09\n",
      "Iteration: 9412 lambda_n: 0.9085518549042484 Loss: 1.211329892779691e-09\n",
      "Iteration: 9413 lambda_n: 0.9761471978533272 Loss: 1.2113300437375984e-09\n",
      "Iteration: 9414 lambda_n: 0.9197716916571425 Loss: 1.2113302023775246e-09\n",
      "Iteration: 9415 lambda_n: 0.9162018299088256 Loss: 1.211330354771234e-09\n",
      "Iteration: 9416 lambda_n: 0.9019716508642911 Loss: 1.2113305018884136e-09\n",
      "Iteration: 9417 lambda_n: 1.025775487201408 Loss: 1.211330649853366e-09\n",
      "Iteration: 9418 lambda_n: 1.0245143810733033 Loss: 1.2113308198847445e-09\n",
      "Iteration: 9419 lambda_n: 0.9429711761927048 Loss: 1.2113309817681912e-09\n",
      "Iteration: 9420 lambda_n: 1.0009576885864464 Loss: 1.2113311354262358e-09\n",
      "Iteration: 9421 lambda_n: 0.9892064259245078 Loss: 1.2113312947573384e-09\n",
      "Iteration: 9422 lambda_n: 0.9194899688363012 Loss: 1.2113314521374455e-09\n",
      "Iteration: 9423 lambda_n: 1.024600330960125 Loss: 1.2113315956518625e-09\n",
      "Iteration: 9424 lambda_n: 0.909745204186537 Loss: 1.21133176196565e-09\n",
      "Iteration: 9425 lambda_n: 0.9222693901259854 Loss: 1.2113319025905433e-09\n",
      "Iteration: 9426 lambda_n: 0.9939287817073401 Loss: 1.2113320472137067e-09\n",
      "Iteration: 9427 lambda_n: 0.8936343017594819 Loss: 1.2113322026426111e-09\n",
      "Iteration: 9428 lambda_n: 0.9652425735682574 Loss: 1.2113323396887455e-09\n",
      "Iteration: 9429 lambda_n: 0.9798628265001401 Loss: 1.2113324911155802e-09\n",
      "Iteration: 9430 lambda_n: 1.0203810480938602 Loss: 1.2113326384057512e-09\n",
      "Iteration: 9431 lambda_n: 0.8736859350614969 Loss: 1.2113327946425773e-09\n",
      "Iteration: 9432 lambda_n: 0.9120613416455663 Loss: 1.2113329277803136e-09\n",
      "Iteration: 9433 lambda_n: 0.9180874875734336 Loss: 1.2113330674101718e-09\n",
      "Iteration: 9434 lambda_n: 1.0032463735962012 Loss: 1.2113332047446114e-09\n",
      "Iteration: 9435 lambda_n: 0.9577696249852374 Loss: 1.2113333553310686e-09\n",
      "Iteration: 9436 lambda_n: 0.9689143588323775 Loss: 1.2113334956592124e-09\n",
      "Iteration: 9437 lambda_n: 0.9659893370042044 Loss: 1.2113336411533883e-09\n",
      "Iteration: 9438 lambda_n: 1.008300798820751 Loss: 1.2113337807222776e-09\n",
      "Iteration: 9439 lambda_n: 1.0041920609882384 Loss: 1.211333929836543e-09\n",
      "Iteration: 9440 lambda_n: 0.8894194385890962 Loss: 1.2113340762225616e-09\n",
      "Iteration: 9441 lambda_n: 0.9286223435489936 Loss: 1.2113342019209778e-09\n",
      "Iteration: 9442 lambda_n: 0.9013984592304696 Loss: 1.2113343370102091e-09\n",
      "Iteration: 9443 lambda_n: 0.9972727209127971 Loss: 1.2113344671770605e-09\n",
      "Iteration: 9444 lambda_n: 0.973822657969897 Loss: 1.211334615816396e-09\n",
      "Iteration: 9445 lambda_n: 1.0090732236969604 Loss: 1.2113347532268176e-09\n",
      "Iteration: 9446 lambda_n: 0.9058196542844259 Loss: 1.211334897047626e-09\n",
      "Iteration: 9447 lambda_n: 0.9041274348397493 Loss: 1.21133502593306e-09\n",
      "Iteration: 9448 lambda_n: 0.9786496871162769 Loss: 1.2113351513947545e-09\n",
      "Iteration: 9449 lambda_n: 0.8769006561244846 Loss: 1.2113352904351555e-09\n",
      "Iteration: 9450 lambda_n: 0.8922590300650709 Loss: 1.211335413445465e-09\n",
      "Iteration: 9451 lambda_n: 0.9283491921711984 Loss: 1.2113355352625874e-09\n",
      "Iteration: 9452 lambda_n: 0.9243830158299817 Loss: 1.211335664736915e-09\n",
      "Iteration: 9453 lambda_n: 0.9052180969172108 Loss: 1.2113357904690653e-09\n",
      "Iteration: 9454 lambda_n: 0.9257171026594982 Loss: 1.2113359176067195e-09\n",
      "Iteration: 9455 lambda_n: 1.0013087478693705 Loss: 1.2113360439257378e-09\n",
      "Iteration: 9456 lambda_n: 0.9559492464770875 Loss: 1.211336178921261e-09\n",
      "Iteration: 9457 lambda_n: 0.959643379222945 Loss: 1.2113363108782567e-09\n",
      "Iteration: 9458 lambda_n: 0.9513786025154665 Loss: 1.2113364410017407e-09\n",
      "Iteration: 9459 lambda_n: 0.9736530403861492 Loss: 1.2113365675264894e-09\n",
      "Iteration: 9460 lambda_n: 0.9618005300423988 Loss: 1.2113366966916585e-09\n",
      "Iteration: 9461 lambda_n: 0.9383916961496463 Loss: 1.2113368219284698e-09\n",
      "Iteration: 9462 lambda_n: 0.9372425331152545 Loss: 1.2113369478034e-09\n",
      "Iteration: 9463 lambda_n: 0.94479876439353 Loss: 1.2113370710462779e-09\n",
      "Iteration: 9464 lambda_n: 0.8742866078585824 Loss: 1.2113371957528542e-09\n",
      "Iteration: 9465 lambda_n: 0.9583929921639541 Loss: 1.211337310259999e-09\n",
      "Iteration: 9466 lambda_n: 0.8909024982733247 Loss: 1.2113374337109727e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9467 lambda_n: 0.891229330014793 Loss: 1.21133754700375e-09\n",
      "Iteration: 9468 lambda_n: 0.8859692156385512 Loss: 1.2113376670694369e-09\n",
      "Iteration: 9469 lambda_n: 0.9036133555969202 Loss: 1.2113377782387275e-09\n",
      "Iteration: 9470 lambda_n: 0.9190923687752451 Loss: 1.2113378917259427e-09\n",
      "Iteration: 9471 lambda_n: 0.8959833941995342 Loss: 1.211338011683625e-09\n",
      "Iteration: 9472 lambda_n: 0.8766068947946594 Loss: 1.21133812170558e-09\n",
      "Iteration: 9473 lambda_n: 0.987111942284773 Loss: 1.2113382307257789e-09\n",
      "Iteration: 9474 lambda_n: 1.026101415086269 Loss: 1.2113383574819059e-09\n",
      "Iteration: 9475 lambda_n: 1.009271947339262 Loss: 1.211338484851691e-09\n",
      "Iteration: 9476 lambda_n: 0.9124713738204997 Loss: 1.211338612111996e-09\n",
      "Iteration: 9477 lambda_n: 0.9180712766825653 Loss: 1.2113387223274443e-09\n",
      "Iteration: 9478 lambda_n: 0.974106813664785 Loss: 1.2113388380837016e-09\n",
      "Iteration: 9479 lambda_n: 0.932039634845053 Loss: 1.2113389551271826e-09\n",
      "Iteration: 9480 lambda_n: 1.008341887273276 Loss: 1.2113390715954168e-09\n",
      "Iteration: 9481 lambda_n: 0.9267282031558851 Loss: 1.2113391902280973e-09\n",
      "Iteration: 9482 lambda_n: 0.969241021434434 Loss: 1.2113393016886388e-09\n",
      "Iteration: 9483 lambda_n: 0.8979729533295476 Loss: 1.2113394197669695e-09\n",
      "Iteration: 9484 lambda_n: 0.9843181125609493 Loss: 1.2113395256037413e-09\n",
      "Iteration: 9485 lambda_n: 0.8938002600066409 Loss: 1.2113396450781765e-09\n",
      "Iteration: 9486 lambda_n: 0.9424772258442934 Loss: 1.2113397498282998e-09\n",
      "Iteration: 9487 lambda_n: 0.9205376126733595 Loss: 1.2113398604956138e-09\n",
      "Iteration: 9488 lambda_n: 0.9181196593030287 Loss: 1.2113399713860407e-09\n",
      "Iteration: 9489 lambda_n: 0.9884956234678256 Loss: 1.2113400764527737e-09\n",
      "Iteration: 9490 lambda_n: 1.0227749835969318 Loss: 1.2113401911438445e-09\n",
      "Iteration: 9491 lambda_n: 0.8757699352784059 Loss: 1.2113403122684947e-09\n",
      "Iteration: 9492 lambda_n: 0.9633685323766622 Loss: 1.2113404112150746e-09\n",
      "Iteration: 9493 lambda_n: 0.956553902639041 Loss: 1.2113405195296402e-09\n",
      "Iteration: 9494 lambda_n: 0.936448811876415 Loss: 1.2113406305766675e-09\n",
      "Iteration: 9495 lambda_n: 0.9128356673961929 Loss: 1.2113407378014371e-09\n",
      "Iteration: 9496 lambda_n: 0.8958160483506146 Loss: 1.2113408395386858e-09\n",
      "Iteration: 9497 lambda_n: 0.9308784468826745 Loss: 1.2113409435292055e-09\n",
      "Iteration: 9498 lambda_n: 1.0119328994305032 Loss: 1.211341046128011e-09\n",
      "Iteration: 9499 lambda_n: 0.8801028148190766 Loss: 1.2113411607141103e-09\n",
      "Iteration: 9500 lambda_n: 0.9988088578659178 Loss: 1.2113412584148538e-09\n",
      "Iteration: 9501 lambda_n: 0.9475506003000307 Loss: 1.2113413673703918e-09\n",
      "Iteration: 9502 lambda_n: 1.0061634047961039 Loss: 1.2113414707175602e-09\n",
      "Iteration: 9503 lambda_n: 0.8887748582868321 Loss: 1.2113415816705596e-09\n",
      "Iteration: 9504 lambda_n: 0.9370676203957768 Loss: 1.2113416764377638e-09\n",
      "Iteration: 9505 lambda_n: 0.9856818723793519 Loss: 1.2113417817314707e-09\n",
      "Iteration: 9506 lambda_n: 0.9234058061508484 Loss: 1.2113418844072607e-09\n",
      "Iteration: 9507 lambda_n: 0.9017201101909916 Loss: 1.2113419866091903e-09\n",
      "Iteration: 9508 lambda_n: 0.9255859249047718 Loss: 1.2113420803672629e-09\n",
      "Iteration: 9509 lambda_n: 0.8910466607897897 Loss: 1.2113421790799767e-09\n",
      "Iteration: 9510 lambda_n: 0.9918297731414022 Loss: 1.21134227573661e-09\n",
      "Iteration: 9511 lambda_n: 0.9104519696735783 Loss: 1.211342377079071e-09\n",
      "Iteration: 9512 lambda_n: 0.9993409630171919 Loss: 1.2113424744894203e-09\n",
      "Iteration: 9513 lambda_n: 0.9656258491486482 Loss: 1.2113425809146446e-09\n",
      "Iteration: 9514 lambda_n: 0.9159823715000278 Loss: 1.2113426798023107e-09\n",
      "Iteration: 9515 lambda_n: 0.9681421739095103 Loss: 1.2113427785993056e-09\n",
      "Iteration: 9516 lambda_n: 0.9026775606641065 Loss: 1.2113428785672786e-09\n",
      "Iteration: 9517 lambda_n: 0.9309479520635005 Loss: 1.2113429717035692e-09\n",
      "Iteration: 9518 lambda_n: 1.0154621594330662 Loss: 1.2113430686625262e-09\n",
      "Iteration: 9519 lambda_n: 0.9466644108956381 Loss: 1.2113431725453809e-09\n",
      "Iteration: 9520 lambda_n: 0.9584212005152436 Loss: 1.2113432678437746e-09\n",
      "Iteration: 9521 lambda_n: 0.8817662455737234 Loss: 1.2113433667744212e-09\n",
      "Iteration: 9522 lambda_n: 1.0141202893772525 Loss: 1.2113434522002441e-09\n",
      "Iteration: 9523 lambda_n: 0.9892886789723956 Loss: 1.2113435538426747e-09\n",
      "Iteration: 9524 lambda_n: 0.9942459828056928 Loss: 1.2113436491378272e-09\n",
      "Iteration: 9525 lambda_n: 1.0244140853928556 Loss: 1.2113437483400558e-09\n",
      "Iteration: 9526 lambda_n: 0.9357852359806804 Loss: 1.2113438513607333e-09\n",
      "Iteration: 9527 lambda_n: 0.9285482734809355 Loss: 1.2113439397380164e-09\n",
      "Iteration: 9528 lambda_n: 1.0148345806633556 Loss: 1.2113440321589278e-09\n",
      "Iteration: 9529 lambda_n: 0.9107356671807699 Loss: 1.2113441317410476e-09\n",
      "Iteration: 9530 lambda_n: 0.964674277462757 Loss: 1.2113442184268897e-09\n",
      "Iteration: 9531 lambda_n: 0.9265158257258774 Loss: 1.2113443107540103e-09\n",
      "Iteration: 9532 lambda_n: 0.9327449397944524 Loss: 1.2113444009806791e-09\n",
      "Iteration: 9533 lambda_n: 0.878154274736298 Loss: 1.211344488896561e-09\n",
      "Iteration: 9534 lambda_n: 0.9153728109839773 Loss: 1.2113445719748813e-09\n",
      "Iteration: 9535 lambda_n: 0.8782492843941745 Loss: 1.2113446596162543e-09\n",
      "Iteration: 9536 lambda_n: 0.9824109949284804 Loss: 1.2113447419986577e-09\n",
      "Iteration: 9537 lambda_n: 0.9379866559594247 Loss: 1.2113448314875696e-09\n",
      "Iteration: 9538 lambda_n: 0.9379808313145227 Loss: 1.2113449213516486e-09\n",
      "Iteration: 9539 lambda_n: 0.9650109346210245 Loss: 1.2113450095833825e-09\n",
      "Iteration: 9540 lambda_n: 0.9712328258412927 Loss: 1.211345098075747e-09\n",
      "Iteration: 9541 lambda_n: 0.9780785114123285 Loss: 1.2113451898245664e-09\n",
      "Iteration: 9542 lambda_n: 0.9916644700178813 Loss: 1.211345277587837e-09\n",
      "Iteration: 9543 lambda_n: 0.9087632860692522 Loss: 1.2113453698531939e-09\n",
      "Iteration: 9544 lambda_n: 0.9971110733684252 Loss: 1.2113454540786712e-09\n",
      "Iteration: 9545 lambda_n: 0.9050084056122587 Loss: 1.211345544376739e-09\n",
      "Iteration: 9546 lambda_n: 0.9315762376462363 Loss: 1.2113456286014499e-09\n",
      "Iteration: 9547 lambda_n: 0.8990792560282853 Loss: 1.21134571154516e-09\n",
      "Iteration: 9548 lambda_n: 0.9744064033407734 Loss: 1.2113457920800925e-09\n",
      "Iteration: 9549 lambda_n: 1.020345880239212 Loss: 1.2113458812667297e-09\n",
      "Iteration: 9550 lambda_n: 0.9130713593823241 Loss: 1.211345970795237e-09\n",
      "Iteration: 9551 lambda_n: 0.9920238812946183 Loss: 1.2113460531048292e-09\n",
      "Iteration: 9552 lambda_n: 0.9163820374007557 Loss: 1.2113461380291354e-09\n",
      "Iteration: 9553 lambda_n: 0.9921273881008967 Loss: 1.2113462195783623e-09\n",
      "Iteration: 9554 lambda_n: 1.0024015205092685 Loss: 1.211346306654959e-09\n",
      "Iteration: 9555 lambda_n: 0.9916774885073583 Loss: 1.211346395054834e-09\n",
      "Iteration: 9556 lambda_n: 0.8859298379532159 Loss: 1.211346479055395e-09\n",
      "Iteration: 9557 lambda_n: 0.9491229725176232 Loss: 1.211346555514138e-09\n",
      "Iteration: 9558 lambda_n: 0.889503886276093 Loss: 1.2113466392487309e-09\n",
      "Iteration: 9559 lambda_n: 0.8844915755246413 Loss: 1.2113467150013147e-09\n",
      "Iteration: 9560 lambda_n: 0.9556770186831929 Loss: 1.2113467909720447e-09\n",
      "Iteration: 9561 lambda_n: 0.9808510481566287 Loss: 1.211346870798633e-09\n",
      "Iteration: 9562 lambda_n: 0.9295809781039934 Loss: 1.2113469532422184e-09\n",
      "Iteration: 9563 lambda_n: 1.0135324863279258 Loss: 1.2113470328855912e-09\n",
      "Iteration: 9564 lambda_n: 1.0019297067406818 Loss: 1.21134711338396e-09\n",
      "Iteration: 9565 lambda_n: 1.0092017383376888 Loss: 1.211347196676103e-09\n",
      "Iteration: 9566 lambda_n: 0.9493555579465048 Loss: 1.2113472778092939e-09\n",
      "Iteration: 9567 lambda_n: 0.94593421683907 Loss: 1.2113473571514048e-09\n",
      "Iteration: 9568 lambda_n: 1.0038535938146234 Loss: 1.2113474357875765e-09\n",
      "Iteration: 9569 lambda_n: 0.9615230038406907 Loss: 1.2113475140901466e-09\n",
      "Iteration: 9570 lambda_n: 0.9312504405765293 Loss: 1.211347592483863e-09\n",
      "Iteration: 9571 lambda_n: 0.9365072580653242 Loss: 1.2113476664686817e-09\n",
      "Iteration: 9572 lambda_n: 0.9204982978729604 Loss: 1.2113477403017357e-09\n",
      "Iteration: 9573 lambda_n: 0.9659947238459996 Loss: 1.2113478130221008e-09\n",
      "Iteration: 9574 lambda_n: 1.0195894077525387 Loss: 1.2113478890738516e-09\n",
      "Iteration: 9575 lambda_n: 0.9529479227879748 Loss: 1.2113479699683376e-09\n",
      "Iteration: 9576 lambda_n: 0.966355036850139 Loss: 1.2113480437332515e-09\n",
      "Iteration: 9577 lambda_n: 0.9011797583278467 Loss: 1.211348116294903e-09\n",
      "Iteration: 9578 lambda_n: 0.8800893220440223 Loss: 1.2113481862098665e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9579 lambda_n: 0.9133729026410545 Loss: 1.2113482521109534e-09\n",
      "Iteration: 9580 lambda_n: 1.0167653166477415 Loss: 1.2113483207683293e-09\n",
      "Iteration: 9581 lambda_n: 0.8976207948477801 Loss: 1.2113484017490987e-09\n",
      "Iteration: 9582 lambda_n: 0.9418453454349871 Loss: 1.2113484666195655e-09\n",
      "Iteration: 9583 lambda_n: 0.9222611577306383 Loss: 1.211348538264911e-09\n",
      "Iteration: 9584 lambda_n: 0.9439572429986739 Loss: 1.2113486106816535e-09\n",
      "Iteration: 9585 lambda_n: 0.9212315332430763 Loss: 1.2113486814308422e-09\n",
      "Iteration: 9586 lambda_n: 0.8915692701825358 Loss: 1.2113487481371493e-09\n",
      "Iteration: 9587 lambda_n: 0.9116307859593823 Loss: 1.2113488156161823e-09\n",
      "Iteration: 9588 lambda_n: 0.9438269234185636 Loss: 1.2113488810476748e-09\n",
      "Iteration: 9589 lambda_n: 0.9469206396025601 Loss: 1.2113489502772476e-09\n",
      "Iteration: 9590 lambda_n: 0.89966365380628 Loss: 1.211349021650848e-09\n",
      "Iteration: 9591 lambda_n: 1.0122113134621826 Loss: 1.2113490881914492e-09\n",
      "Iteration: 9592 lambda_n: 0.8758220955026126 Loss: 1.211349160622653e-09\n",
      "Iteration: 9593 lambda_n: 0.9203356569246626 Loss: 1.2113492248295343e-09\n",
      "Iteration: 9594 lambda_n: 0.9912983030921283 Loss: 1.2113492933138908e-09\n",
      "Iteration: 9595 lambda_n: 0.976313048116016 Loss: 1.2113493634422486e-09\n",
      "Iteration: 9596 lambda_n: 0.9830232194721645 Loss: 1.2113494339782777e-09\n",
      "Iteration: 9597 lambda_n: 0.9421621375927208 Loss: 1.211349503202127e-09\n",
      "Iteration: 9598 lambda_n: 0.8835923324666305 Loss: 1.2113495703069873e-09\n",
      "Iteration: 9599 lambda_n: 0.9454038499509991 Loss: 1.211349634530528e-09\n",
      "Iteration: 9600 lambda_n: 0.8960404564936615 Loss: 1.2113496984571891e-09\n",
      "Iteration: 9601 lambda_n: 0.9698018489705619 Loss: 1.2113497610854404e-09\n",
      "Iteration: 9602 lambda_n: 0.8785331204251469 Loss: 1.211349829790368e-09\n",
      "Iteration: 9603 lambda_n: 1.0023822612549582 Loss: 1.211349888427838e-09\n",
      "Iteration: 9604 lambda_n: 0.9597738977752504 Loss: 1.2113499593042302e-09\n",
      "Iteration: 9605 lambda_n: 0.9664619459134022 Loss: 1.2113500273670902e-09\n",
      "Iteration: 9606 lambda_n: 1.012354656715629 Loss: 1.2113500937949146e-09\n",
      "Iteration: 9607 lambda_n: 0.8906736190802932 Loss: 1.211350164912579e-09\n",
      "Iteration: 9608 lambda_n: 0.9191863494691102 Loss: 1.2113502263437547e-09\n",
      "Iteration: 9609 lambda_n: 1.0172207413306693 Loss: 1.211350286839566e-09\n",
      "Iteration: 9610 lambda_n: 0.9391317002031909 Loss: 1.211350356235723e-09\n",
      "Iteration: 9611 lambda_n: 0.9272951854362013 Loss: 1.2113504183184878e-09\n",
      "Iteration: 9612 lambda_n: 1.016423362863322 Loss: 1.211350479844955e-09\n",
      "Iteration: 9613 lambda_n: 0.9134712978556317 Loss: 1.2113505502183975e-09\n",
      "Iteration: 9614 lambda_n: 0.893218283424645 Loss: 1.2113506087637872e-09\n",
      "Iteration: 9615 lambda_n: 0.9498933176627333 Loss: 1.2113506691354857e-09\n",
      "Iteration: 9616 lambda_n: 0.9454841452320675 Loss: 1.2113507294232875e-09\n",
      "Iteration: 9617 lambda_n: 0.9960958609065534 Loss: 1.211350793342625e-09\n",
      "Iteration: 9618 lambda_n: 0.928670828012503 Loss: 1.2113508551774436e-09\n",
      "Iteration: 9619 lambda_n: 0.8872696412884374 Loss: 1.2113509164391119e-09\n",
      "Iteration: 9620 lambda_n: 0.9000300780222292 Loss: 1.2113509747190579e-09\n",
      "Iteration: 9621 lambda_n: 0.8845819760996284 Loss: 1.2113510295127182e-09\n",
      "Iteration: 9622 lambda_n: 0.9065904096991919 Loss: 1.2113510892831606e-09\n",
      "Iteration: 9623 lambda_n: 0.9636088639813566 Loss: 1.2113511438397923e-09\n",
      "Iteration: 9624 lambda_n: 1.0255284424534827 Loss: 1.211351208316808e-09\n",
      "Iteration: 9625 lambda_n: 0.9705599063116859 Loss: 1.2113512716247157e-09\n",
      "Iteration: 9626 lambda_n: 0.9584705357376642 Loss: 1.211351331275069e-09\n",
      "Iteration: 9627 lambda_n: 0.9509778249732959 Loss: 1.2113513928278463e-09\n",
      "Iteration: 9628 lambda_n: 0.9607623390266444 Loss: 1.2113514541935995e-09\n",
      "Iteration: 9629 lambda_n: 0.8778807815065238 Loss: 1.2113515099179948e-09\n",
      "Iteration: 9630 lambda_n: 0.9647994160331407 Loss: 1.2113515666297694e-09\n",
      "Iteration: 9631 lambda_n: 0.9194085567777441 Loss: 1.2113516233827764e-09\n",
      "Iteration: 9632 lambda_n: 0.8990931343372911 Loss: 1.211351678390293e-09\n",
      "Iteration: 9633 lambda_n: 0.9420096789188003 Loss: 1.2113517341521969e-09\n",
      "Iteration: 9634 lambda_n: 0.9875767443340528 Loss: 1.2113517905314843e-09\n",
      "Iteration: 9635 lambda_n: 0.9656681847382236 Loss: 1.2113518511897233e-09\n",
      "Iteration: 9636 lambda_n: 0.9640365987348197 Loss: 1.2113519070838632e-09\n",
      "Iteration: 9637 lambda_n: 0.8829642693218611 Loss: 1.2113519618685882e-09\n",
      "Iteration: 9638 lambda_n: 1.0040694881055376 Loss: 1.2113520166999529e-09\n",
      "Iteration: 9639 lambda_n: 0.9159923595122315 Loss: 1.211352073940933e-09\n",
      "Iteration: 9640 lambda_n: 0.9948639115707288 Loss: 1.2113521304358919e-09\n",
      "Iteration: 9641 lambda_n: 0.9098814190556179 Loss: 1.21135218662585e-09\n",
      "Iteration: 9642 lambda_n: 0.9456360162328262 Loss: 1.2113522429135905e-09\n",
      "Iteration: 9643 lambda_n: 0.8914335720499501 Loss: 1.2113522945576064e-09\n",
      "Iteration: 9644 lambda_n: 0.9719332757288568 Loss: 1.2113523472887818e-09\n",
      "Iteration: 9645 lambda_n: 0.9587242837142418 Loss: 1.21135240049358e-09\n",
      "Iteration: 9646 lambda_n: 0.9601048096176791 Loss: 1.211352454941912e-09\n",
      "Iteration: 9647 lambda_n: 0.8756401888442252 Loss: 1.2113525112719128e-09\n",
      "Iteration: 9648 lambda_n: 0.9455814542585224 Loss: 1.2113525595087614e-09\n",
      "Iteration: 9649 lambda_n: 0.9487131830688612 Loss: 1.2113526160039995e-09\n",
      "Iteration: 9650 lambda_n: 0.9418725444646361 Loss: 1.2113526703290653e-09\n",
      "Iteration: 9651 lambda_n: 0.982768436175178 Loss: 1.2113527239671815e-09\n",
      "Iteration: 9652 lambda_n: 0.8784989700111389 Loss: 1.2113527795077648e-09\n",
      "Iteration: 9653 lambda_n: 0.9317814851067181 Loss: 1.211352830458723e-09\n",
      "Iteration: 9654 lambda_n: 0.9885037327017768 Loss: 1.2113528812205573e-09\n",
      "Iteration: 9655 lambda_n: 0.9133227714009904 Loss: 1.2113529367523121e-09\n",
      "Iteration: 9656 lambda_n: 0.8988884342824635 Loss: 1.2113529863841121e-09\n",
      "Iteration: 9657 lambda_n: 0.9019293447728355 Loss: 1.2113530335591723e-09\n",
      "Iteration: 9658 lambda_n: 1.010613847544797 Loss: 1.2113530854804217e-09\n",
      "Iteration: 9659 lambda_n: 0.9799952812786747 Loss: 1.2113531394070948e-09\n",
      "Iteration: 9660 lambda_n: 1.0052813961422002 Loss: 1.2113531931298973e-09\n",
      "Iteration: 9661 lambda_n: 1.0107788205468586 Loss: 1.2113532428206577e-09\n",
      "Iteration: 9662 lambda_n: 0.9793290176664325 Loss: 1.2113533003018899e-09\n",
      "Iteration: 9663 lambda_n: 0.92142566388937 Loss: 1.2113533501680955e-09\n",
      "Iteration: 9664 lambda_n: 0.9258231354585565 Loss: 1.2113534013859043e-09\n",
      "Iteration: 9665 lambda_n: 0.991145596574698 Loss: 1.2113534464257724e-09\n",
      "Iteration: 9666 lambda_n: 0.9618802695224233 Loss: 1.2113534994493902e-09\n",
      "Iteration: 9667 lambda_n: 1.0052486888650796 Loss: 1.2113535457708905e-09\n",
      "Iteration: 9668 lambda_n: 0.9276900615574577 Loss: 1.2113535944191184e-09\n",
      "Iteration: 9669 lambda_n: 0.9135890841363832 Loss: 1.2113536444678585e-09\n",
      "Iteration: 9670 lambda_n: 0.896324148017636 Loss: 1.2113536875934318e-09\n",
      "Iteration: 9671 lambda_n: 0.9292710461571335 Loss: 1.2113537351290118e-09\n",
      "Iteration: 9672 lambda_n: 0.966662229604075 Loss: 1.2113537792148888e-09\n",
      "Iteration: 9673 lambda_n: 0.886768845806259 Loss: 1.2113538311823325e-09\n",
      "Iteration: 9674 lambda_n: 0.9565025736823338 Loss: 1.2113538732151675e-09\n",
      "Iteration: 9675 lambda_n: 1.0209922890359817 Loss: 1.211353924712607e-09\n",
      "Iteration: 9676 lambda_n: 0.8979017707919693 Loss: 1.2113539722318337e-09\n",
      "Iteration: 9677 lambda_n: 1.0170323475017111 Loss: 1.2113540187871717e-09\n",
      "Iteration: 9678 lambda_n: 0.8840841580294801 Loss: 1.211354065837177e-09\n",
      "Iteration: 9679 lambda_n: 0.9196138480213535 Loss: 1.211354110807635e-09\n",
      "Iteration: 9680 lambda_n: 0.948116882995717 Loss: 1.211354156053208e-09\n",
      "Iteration: 9681 lambda_n: 0.8817005695953021 Loss: 1.2113541997469655e-09\n",
      "Iteration: 9682 lambda_n: 1.0035661891186283 Loss: 1.211354242504858e-09\n",
      "Iteration: 9683 lambda_n: 1.0135201155235005 Loss: 1.2113542886452178e-09\n",
      "Iteration: 9684 lambda_n: 0.963118888158977 Loss: 1.2113543352277407e-09\n",
      "Iteration: 9685 lambda_n: 0.8985061267071975 Loss: 1.211354381171917e-09\n",
      "Iteration: 9686 lambda_n: 0.9922942295379176 Loss: 1.2113544264214118e-09\n",
      "Iteration: 9687 lambda_n: 1.024225924986201 Loss: 1.211354471388199e-09\n",
      "Iteration: 9688 lambda_n: 0.9232292782854329 Loss: 1.2113545210160295e-09\n",
      "Iteration: 9689 lambda_n: 0.9202956748545994 Loss: 1.2113545643786529e-09\n",
      "Iteration: 9690 lambda_n: 0.9885532319944369 Loss: 1.2113546049892408e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9691 lambda_n: 0.891834626000469 Loss: 1.2113546534620821e-09\n",
      "Iteration: 9692 lambda_n: 1.0174338819408806 Loss: 1.211354690866856e-09\n",
      "Iteration: 9693 lambda_n: 0.9523901489618006 Loss: 1.2113547402943166e-09\n",
      "Iteration: 9694 lambda_n: 0.9838599028300907 Loss: 1.2113547802809469e-09\n",
      "Iteration: 9695 lambda_n: 0.8940073302185125 Loss: 1.211354826150355e-09\n",
      "Iteration: 9696 lambda_n: 0.9960218809883432 Loss: 1.2113548655259363e-09\n",
      "Iteration: 9697 lambda_n: 0.9468307253696203 Loss: 1.2113549096835692e-09\n",
      "Iteration: 9698 lambda_n: 1.0112769206065004 Loss: 1.2113549506378078e-09\n",
      "Iteration: 9699 lambda_n: 0.9478998479644485 Loss: 1.2113549992308704e-09\n",
      "Iteration: 9700 lambda_n: 0.9827020154559405 Loss: 1.2113550379029407e-09\n",
      "Iteration: 9701 lambda_n: 0.9673509481693683 Loss: 1.2113550799924305e-09\n",
      "Iteration: 9702 lambda_n: 0.8978937737068492 Loss: 1.211355123558395e-09\n",
      "Iteration: 9703 lambda_n: 1.0010011011253883 Loss: 1.2113551596526023e-09\n",
      "Iteration: 9704 lambda_n: 0.885914576951464 Loss: 1.2113552051829466e-09\n",
      "Iteration: 9705 lambda_n: 0.8957795534513248 Loss: 1.21135524116222e-09\n",
      "Iteration: 9706 lambda_n: 0.9912515124642781 Loss: 1.211355279544091e-09\n",
      "Iteration: 9707 lambda_n: 0.8773175531701801 Loss: 1.2113553249764216e-09\n",
      "Iteration: 9708 lambda_n: 0.9107790543359018 Loss: 1.2113553618167544e-09\n",
      "Iteration: 9709 lambda_n: 0.8936859150566302 Loss: 1.2113554020386985e-09\n",
      "Iteration: 9710 lambda_n: 1.0227388431323456 Loss: 1.2113554403140116e-09\n",
      "Iteration: 9711 lambda_n: 0.8827854106360024 Loss: 1.2113554858100806e-09\n",
      "Iteration: 9712 lambda_n: 0.9974360399757065 Loss: 1.2113555214920702e-09\n",
      "Iteration: 9713 lambda_n: 0.9387627001633506 Loss: 1.21135556438225e-09\n",
      "Iteration: 9714 lambda_n: 0.88371270702999 Loss: 1.2113556003217427e-09\n",
      "Iteration: 9715 lambda_n: 0.9032023021157193 Loss: 1.211355638636127e-09\n",
      "Iteration: 9716 lambda_n: 0.9991690046617531 Loss: 1.2113556721358515e-09\n",
      "Iteration: 9717 lambda_n: 0.9761549093819493 Loss: 1.2113557163041361e-09\n",
      "Iteration: 9718 lambda_n: 0.9606007661454533 Loss: 1.211355753295299e-09\n",
      "Iteration: 9719 lambda_n: 1.011345183731058 Loss: 1.211355793697536e-09\n",
      "Iteration: 9720 lambda_n: 1.0004838779547036 Loss: 1.2113558333245752e-09\n",
      "Iteration: 9721 lambda_n: 0.9409835429406775 Loss: 1.2113558753832021e-09\n",
      "Iteration: 9722 lambda_n: 0.9678978497682229 Loss: 1.2113559097309961e-09\n",
      "Iteration: 9723 lambda_n: 1.0074201092897184 Loss: 1.2113559490666125e-09\n",
      "Iteration: 9724 lambda_n: 1.002407491981621 Loss: 1.2113559903718975e-09\n",
      "Iteration: 9725 lambda_n: 0.87525953212954 Loss: 1.2113560314247356e-09\n",
      "Iteration: 9726 lambda_n: 0.8991323356438129 Loss: 1.2113560674670722e-09\n",
      "Iteration: 9727 lambda_n: 0.9943769867601954 Loss: 1.2113561031309494e-09\n",
      "Iteration: 9728 lambda_n: 0.9141138254632755 Loss: 1.2113561433183994e-09\n",
      "Iteration: 9729 lambda_n: 0.966899910355285 Loss: 1.2113561786342431e-09\n",
      "Iteration: 9730 lambda_n: 0.879682576058045 Loss: 1.211356213222163e-09\n",
      "Iteration: 9731 lambda_n: 0.915071925736048 Loss: 1.211356247629155e-09\n",
      "Iteration: 9732 lambda_n: 0.9289836634248971 Loss: 1.211356277817803e-09\n",
      "Iteration: 9733 lambda_n: 0.9664970429686184 Loss: 1.2113563125984849e-09\n",
      "Iteration: 9734 lambda_n: 1.013018477397507 Loss: 1.2113563516002433e-09\n",
      "Iteration: 9735 lambda_n: 0.8842392318640414 Loss: 1.2113563857764974e-09\n",
      "Iteration: 9736 lambda_n: 0.9164519810136027 Loss: 1.2113564192202063e-09\n",
      "Iteration: 9737 lambda_n: 0.9139538866824278 Loss: 1.2113564543795946e-09\n",
      "Iteration: 9738 lambda_n: 0.9534732987144474 Loss: 1.2113564881395775e-09\n",
      "Iteration: 9739 lambda_n: 0.9347651544882823 Loss: 1.2113565215840133e-09\n",
      "Iteration: 9740 lambda_n: 0.9744234716962683 Loss: 1.211356557061771e-09\n",
      "Iteration: 9741 lambda_n: 0.9728628131483228 Loss: 1.2113565910113043e-09\n",
      "Iteration: 9742 lambda_n: 0.8743764273673784 Loss: 1.2113566298411618e-09\n",
      "Iteration: 9743 lambda_n: 0.9705357711730902 Loss: 1.2113566604897625e-09\n",
      "Iteration: 9744 lambda_n: 0.8747549313769858 Loss: 1.2113566973079668e-09\n",
      "Iteration: 9745 lambda_n: 0.9389543271254022 Loss: 1.2113567296176978e-09\n",
      "Iteration: 9746 lambda_n: 0.9509642366779575 Loss: 1.2113567630703338e-09\n",
      "Iteration: 9747 lambda_n: 0.9271323301571586 Loss: 1.2113567996313184e-09\n",
      "Iteration: 9748 lambda_n: 0.9357248857049183 Loss: 1.2113568317108062e-09\n",
      "Iteration: 9749 lambda_n: 0.9544105762435664 Loss: 1.2113568686898706e-09\n",
      "Iteration: 9750 lambda_n: 1.013317001386551 Loss: 1.2113569039598365e-09\n",
      "Iteration: 9751 lambda_n: 0.9566029577652639 Loss: 1.211356937450888e-09\n",
      "Iteration: 9752 lambda_n: 0.9638607583249195 Loss: 1.2113569716194676e-09\n",
      "Iteration: 9753 lambda_n: 0.8995049396946739 Loss: 1.2113570070617725e-09\n",
      "Iteration: 9754 lambda_n: 0.912065319297952 Loss: 1.2113570349140774e-09\n",
      "Iteration: 9755 lambda_n: 0.9163829560398185 Loss: 1.2113570669991457e-09\n",
      "Iteration: 9756 lambda_n: 1.0035409789030707 Loss: 1.2113570996392963e-09\n",
      "Iteration: 9757 lambda_n: 0.9907101244508834 Loss: 1.2113571355305346e-09\n",
      "Iteration: 9758 lambda_n: 0.9979818845836762 Loss: 1.2113571686245172e-09\n",
      "Iteration: 9759 lambda_n: 0.9847301249067312 Loss: 1.2113572047473051e-09\n",
      "Iteration: 9760 lambda_n: 0.9304022773798124 Loss: 1.2113572380124307e-09\n",
      "Iteration: 9761 lambda_n: 0.9822688348073058 Loss: 1.2113572693278794e-09\n",
      "Iteration: 9762 lambda_n: 0.9864582369038448 Loss: 1.211357301697914e-09\n",
      "Iteration: 9763 lambda_n: 1.0195990118364333 Loss: 1.2113573358652912e-09\n",
      "Iteration: 9764 lambda_n: 1.0097182344060038 Loss: 1.2113573700271976e-09\n",
      "Iteration: 9765 lambda_n: 0.9403495104219244 Loss: 1.2113574064968108e-09\n",
      "Iteration: 9766 lambda_n: 0.9053265863955512 Loss: 1.2113574361550888e-09\n",
      "Iteration: 9767 lambda_n: 1.006462886401316 Loss: 1.2113574662474459e-09\n",
      "Iteration: 9768 lambda_n: 0.967587657029697 Loss: 1.211357500015377e-09\n",
      "Iteration: 9769 lambda_n: 0.8826946138326934 Loss: 1.2113575315453349e-09\n",
      "Iteration: 9770 lambda_n: 0.9237161356153981 Loss: 1.2113575629141514e-09\n",
      "Iteration: 9771 lambda_n: 0.9540847729881903 Loss: 1.2113575923638064e-09\n",
      "Iteration: 9772 lambda_n: 0.8965936989895609 Loss: 1.2113576233181386e-09\n",
      "Iteration: 9773 lambda_n: 0.8750173933197085 Loss: 1.2113576506881548e-09\n",
      "Iteration: 9774 lambda_n: 0.9801986146504493 Loss: 1.211357676670208e-09\n",
      "Iteration: 9775 lambda_n: 0.9649472647240702 Loss: 1.211357705941164e-09\n",
      "Iteration: 9776 lambda_n: 0.9914657245806143 Loss: 1.2113577366424295e-09\n",
      "Iteration: 9777 lambda_n: 0.9697079975705547 Loss: 1.211357770055108e-09\n",
      "Iteration: 9778 lambda_n: 1.0049888774955067 Loss: 1.2113577967523958e-09\n",
      "Iteration: 9779 lambda_n: 0.9052022549071755 Loss: 1.2113578276882038e-09\n",
      "Iteration: 9780 lambda_n: 0.962940699120867 Loss: 1.2113578529110218e-09\n",
      "Iteration: 9781 lambda_n: 0.9862299568109664 Loss: 1.2113578841498803e-09\n",
      "Iteration: 9782 lambda_n: 1.0059931043192825 Loss: 1.2113579148821393e-09\n",
      "Iteration: 9783 lambda_n: 0.9119833484343635 Loss: 1.211357943317656e-09\n",
      "Iteration: 9784 lambda_n: 0.9409199054116639 Loss: 1.2113579701267118e-09\n",
      "Iteration: 9785 lambda_n: 0.9347115974114774 Loss: 1.2113580000873697e-09\n",
      "Iteration: 9786 lambda_n: 1.0133849944727742 Loss: 1.2113580306382236e-09\n",
      "Iteration: 9787 lambda_n: 0.8934274776731232 Loss: 1.211358059150629e-09\n",
      "Iteration: 9788 lambda_n: 1.0164623065194163 Loss: 1.2113580852749578e-09\n",
      "Iteration: 9789 lambda_n: 0.9624771184326189 Loss: 1.211358116015923e-09\n",
      "Iteration: 9790 lambda_n: 0.9552146298518795 Loss: 1.2113581487095634e-09\n",
      "Iteration: 9791 lambda_n: 0.8927295869035784 Loss: 1.2113581750801998e-09\n",
      "Iteration: 9792 lambda_n: 0.8986270606980384 Loss: 1.211358201903337e-09\n",
      "Iteration: 9793 lambda_n: 0.8870344171267869 Loss: 1.2113582289832594e-09\n",
      "Iteration: 9794 lambda_n: 0.8822348606118171 Loss: 1.2113582582266885e-09\n",
      "Iteration: 9795 lambda_n: 1.019045669109064 Loss: 1.2113582810834437e-09\n",
      "Iteration: 9796 lambda_n: 0.9574218150157695 Loss: 1.2113583125262232e-09\n",
      "Iteration: 9797 lambda_n: 0.9793809146400554 Loss: 1.2113583415896899e-09\n",
      "Iteration: 9798 lambda_n: 0.9139523247807404 Loss: 1.2113583703397583e-09\n",
      "Iteration: 9799 lambda_n: 0.9589423001920201 Loss: 1.2113583933267065e-09\n",
      "Iteration: 9800 lambda_n: 0.9549171649650227 Loss: 1.2113584222559865e-09\n",
      "Iteration: 9801 lambda_n: 0.9356215963101361 Loss: 1.2113584499459644e-09\n",
      "Iteration: 9802 lambda_n: 0.961165360080215 Loss: 1.2113584764974504e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9803 lambda_n: 0.8736350315956025 Loss: 1.2113585034485883e-09\n",
      "Iteration: 9804 lambda_n: 0.8817497615529816 Loss: 1.2113585271492346e-09\n",
      "Iteration: 9805 lambda_n: 0.8770318006060442 Loss: 1.2113585533718254e-09\n",
      "Iteration: 9806 lambda_n: 1.0013659040379113 Loss: 1.2113585808495773e-09\n",
      "Iteration: 9807 lambda_n: 0.9693746957191252 Loss: 1.211358606762688e-09\n",
      "Iteration: 9808 lambda_n: 1.000994701542496 Loss: 1.211358635329489e-09\n",
      "Iteration: 9809 lambda_n: 0.8767167292070941 Loss: 1.2113586621796113e-09\n",
      "Iteration: 9810 lambda_n: 0.8780210813754883 Loss: 1.2113586850618883e-09\n",
      "Iteration: 9811 lambda_n: 0.9506116047287637 Loss: 1.211358710888552e-09\n",
      "Iteration: 9812 lambda_n: 0.8871868454417537 Loss: 1.2113587388232611e-09\n",
      "Iteration: 9813 lambda_n: 0.9711981441675465 Loss: 1.2113587614869406e-09\n",
      "Iteration: 9814 lambda_n: 0.9259141169982987 Loss: 1.2113587868523197e-09\n",
      "Iteration: 9815 lambda_n: 0.9801401479217613 Loss: 1.2113588146000702e-09\n",
      "Iteration: 9816 lambda_n: 0.9596718336394501 Loss: 1.211358840031955e-09\n",
      "Iteration: 9817 lambda_n: 0.8865568611981365 Loss: 1.211358869617947e-09\n",
      "Iteration: 9818 lambda_n: 0.9995349671776684 Loss: 1.2113588927208115e-09\n",
      "Iteration: 9819 lambda_n: 0.9315794671721616 Loss: 1.2113589196335722e-09\n",
      "Iteration: 9820 lambda_n: 0.992202133469104 Loss: 1.2113589414363566e-09\n",
      "Iteration: 9821 lambda_n: 0.9149009452299118 Loss: 1.211358967260833e-09\n",
      "Iteration: 9822 lambda_n: 0.967801666941045 Loss: 1.2113589893254887e-09\n",
      "Iteration: 9823 lambda_n: 1.0201392032748438 Loss: 1.2113590135641856e-09\n",
      "Iteration: 9824 lambda_n: 0.9783098338249999 Loss: 1.2113590376818763e-09\n",
      "Iteration: 9825 lambda_n: 0.9982060919339447 Loss: 1.2113590615990168e-09\n",
      "Iteration: 9826 lambda_n: 0.9456093817966011 Loss: 1.2113590867457316e-09\n",
      "Iteration: 9827 lambda_n: 0.875989859705834 Loss: 1.2113591079541909e-09\n",
      "Iteration: 9828 lambda_n: 0.9132165173130307 Loss: 1.2113591289382153e-09\n",
      "Iteration: 9829 lambda_n: 0.8883752372925543 Loss: 1.2113591503969733e-09\n",
      "Iteration: 9830 lambda_n: 0.8979375543968379 Loss: 1.2113591715027221e-09\n",
      "Iteration: 9831 lambda_n: 0.8762100496710679 Loss: 1.2113591930343476e-09\n",
      "Iteration: 9832 lambda_n: 0.9363181467399568 Loss: 1.2113592177541402e-09\n",
      "Iteration: 9833 lambda_n: 0.9474291947422674 Loss: 1.2113592402254274e-09\n",
      "Iteration: 9834 lambda_n: 0.9155162083662877 Loss: 1.2113592657899755e-09\n",
      "Iteration: 9835 lambda_n: 0.9362836041219061 Loss: 1.211359286772533e-09\n",
      "Iteration: 9836 lambda_n: 0.9204346679568738 Loss: 1.2113593113348218e-09\n",
      "Iteration: 9837 lambda_n: 0.9801821580094341 Loss: 1.2113593311435042e-09\n",
      "Iteration: 9838 lambda_n: 0.9202567168010529 Loss: 1.211359355692869e-09\n",
      "Iteration: 9839 lambda_n: 0.9720252153876401 Loss: 1.2113593756518597e-09\n",
      "Iteration: 9840 lambda_n: 0.9401461244384262 Loss: 1.2113594011569356e-09\n",
      "Iteration: 9841 lambda_n: 0.9626460206524943 Loss: 1.2113594238447195e-09\n",
      "Iteration: 9842 lambda_n: 0.9766121409296817 Loss: 1.2113594465763977e-09\n",
      "Iteration: 9843 lambda_n: 1.0131931868251078 Loss: 1.2113594698324121e-09\n",
      "Iteration: 9844 lambda_n: 0.9385200043915007 Loss: 1.2113594924270044e-09\n",
      "Iteration: 9845 lambda_n: 0.888524434242648 Loss: 1.2113595188927827e-09\n",
      "Iteration: 9846 lambda_n: 0.8933992603491058 Loss: 1.211359539309349e-09\n",
      "Iteration: 9847 lambda_n: 0.9125020461687877 Loss: 1.2113595613341474e-09\n",
      "Iteration: 9848 lambda_n: 0.8923029694233365 Loss: 1.2113595800047946e-09\n",
      "Iteration: 9849 lambda_n: 0.9703440762073109 Loss: 1.2113596030983676e-09\n",
      "Iteration: 9850 lambda_n: 0.9837281245255092 Loss: 1.2113596220383844e-09\n",
      "Iteration: 9851 lambda_n: 0.9878755597935674 Loss: 1.2113596427507592e-09\n",
      "Iteration: 9852 lambda_n: 0.9031018995699931 Loss: 1.2113596677015619e-09\n",
      "Iteration: 9853 lambda_n: 0.9328447100301058 Loss: 1.2113596861381862e-09\n",
      "Iteration: 9854 lambda_n: 1.0048679970091758 Loss: 1.2113597064326682e-09\n",
      "Iteration: 9855 lambda_n: 0.9517766913156739 Loss: 1.211359729028727e-09\n",
      "Iteration: 9856 lambda_n: 0.996863187114011 Loss: 1.21135974771901e-09\n",
      "Iteration: 9857 lambda_n: 0.9869054735718022 Loss: 1.211359769563943e-09\n",
      "Iteration: 9858 lambda_n: 1.0201449994988427 Loss: 1.2113597891036714e-09\n",
      "Iteration: 9859 lambda_n: 0.9643993202254904 Loss: 1.2113598150924818e-09\n",
      "Iteration: 9860 lambda_n: 0.9973678792663222 Loss: 1.2113598325085796e-09\n",
      "Iteration: 9861 lambda_n: 0.8977102987941239 Loss: 1.211359851597364e-09\n",
      "Iteration: 9862 lambda_n: 0.944890888605355 Loss: 1.2113598714191833e-09\n",
      "Iteration: 9863 lambda_n: 0.9184174812495491 Loss: 1.2113598916924045e-09\n",
      "Iteration: 9864 lambda_n: 0.902938049254997 Loss: 1.2113599094165753e-09\n",
      "Iteration: 9865 lambda_n: 0.897700601962198 Loss: 1.2113599277191915e-09\n",
      "Iteration: 9866 lambda_n: 0.9529008323537199 Loss: 1.2113599486698833e-09\n",
      "Iteration: 9867 lambda_n: 0.9201483331818407 Loss: 1.2113599687544167e-09\n",
      "Iteration: 9868 lambda_n: 0.9981482201824199 Loss: 1.2113599873198831e-09\n",
      "Iteration: 9869 lambda_n: 0.8955252576091081 Loss: 1.211360011842687e-09\n",
      "Iteration: 9870 lambda_n: 0.8992086667604428 Loss: 1.211360027691567e-09\n",
      "Iteration: 9871 lambda_n: 0.9781166302706344 Loss: 1.2113600498392764e-09\n",
      "Iteration: 9872 lambda_n: 0.8797538713781202 Loss: 1.2113600689994501e-09\n",
      "Iteration: 9873 lambda_n: 1.016902401636496 Loss: 1.2113600872645694e-09\n",
      "Iteration: 9874 lambda_n: 0.9551041957982302 Loss: 1.2113601109956789e-09\n",
      "Iteration: 9875 lambda_n: 0.9741803938435833 Loss: 1.2113601319857848e-09\n",
      "Iteration: 9876 lambda_n: 1.0050473581776 Loss: 1.2113601494295761e-09\n",
      "Iteration: 9877 lambda_n: 1.023116945425323 Loss: 1.211360168596908e-09\n",
      "Iteration: 9878 lambda_n: 0.9188374633333883 Loss: 1.2113601910479616e-09\n",
      "Iteration: 9879 lambda_n: 0.9675008883803934 Loss: 1.2113602123519815e-09\n",
      "Iteration: 9880 lambda_n: 0.938266682762765 Loss: 1.2113602323566845e-09\n",
      "Iteration: 9881 lambda_n: 1.0193842534870745 Loss: 1.2113602495305644e-09\n",
      "Iteration: 9882 lambda_n: 0.9622365823995628 Loss: 1.2113602703471896e-09\n",
      "Iteration: 9883 lambda_n: 0.934481475981157 Loss: 1.211360289740137e-09\n",
      "Iteration: 9884 lambda_n: 0.9809524301024375 Loss: 1.211360307079342e-09\n",
      "Iteration: 9885 lambda_n: 1.0123201033031708 Loss: 1.2113603232607427e-09\n",
      "Iteration: 9886 lambda_n: 0.9975862016615149 Loss: 1.2113603396067436e-09\n",
      "Iteration: 9887 lambda_n: 0.9382988213951421 Loss: 1.2113603577932505e-09\n",
      "Iteration: 9888 lambda_n: 0.8856970916669085 Loss: 1.2113603769378318e-09\n",
      "Iteration: 9889 lambda_n: 0.9681178330212092 Loss: 1.2113603918656305e-09\n",
      "Iteration: 9890 lambda_n: 0.9054716819894164 Loss: 1.211360411581507e-09\n",
      "Iteration: 9891 lambda_n: 0.9968494104834524 Loss: 1.211360426652193e-09\n",
      "Iteration: 9892 lambda_n: 0.9318794890722627 Loss: 1.211360445165236e-09\n",
      "Iteration: 9893 lambda_n: 0.9848333577676828 Loss: 1.211360460234076e-09\n",
      "Iteration: 9894 lambda_n: 0.9275533044817685 Loss: 1.2113604771301609e-09\n",
      "Iteration: 9895 lambda_n: 0.9620633056988759 Loss: 1.2113604933830545e-09\n",
      "Iteration: 9896 lambda_n: 0.901445059045327 Loss: 1.2113605120181445e-09\n",
      "Iteration: 9897 lambda_n: 0.9823000849972919 Loss: 1.2113605293427725e-09\n",
      "Iteration: 9898 lambda_n: 0.9457494236928633 Loss: 1.2113605458924252e-09\n",
      "Iteration: 9899 lambda_n: 0.8825484174211119 Loss: 1.21136055975007e-09\n",
      "Iteration: 9900 lambda_n: 0.9228906954205354 Loss: 1.2113605747426333e-09\n",
      "Iteration: 9901 lambda_n: 1.0093438337787084 Loss: 1.2113605933149902e-09\n",
      "Iteration: 9902 lambda_n: 0.954042925065127 Loss: 1.2113606085005007e-09\n",
      "Iteration: 9903 lambda_n: 0.903438926738935 Loss: 1.2113606252175628e-09\n",
      "Iteration: 9904 lambda_n: 0.9236714869104141 Loss: 1.211360642389983e-09\n",
      "Iteration: 9905 lambda_n: 0.8918914449942724 Loss: 1.2113606566527122e-09\n",
      "Iteration: 9906 lambda_n: 0.8866995414640564 Loss: 1.2113606719235127e-09\n",
      "Iteration: 9907 lambda_n: 0.9492433579207045 Loss: 1.2113606873477633e-09\n",
      "Iteration: 9908 lambda_n: 0.9434020613500675 Loss: 1.2113607073567728e-09\n",
      "Iteration: 9909 lambda_n: 0.9135022609423661 Loss: 1.2113607254355444e-09\n",
      "Iteration: 9910 lambda_n: 1.013947396994612 Loss: 1.211360737799523e-09\n",
      "Iteration: 9911 lambda_n: 0.8738974880612315 Loss: 1.211360756149038e-09\n",
      "Iteration: 9912 lambda_n: 0.9631508088653594 Loss: 1.2113607709821425e-09\n",
      "Iteration: 9913 lambda_n: 0.8937378767741525 Loss: 1.2113607874697728e-09\n",
      "Iteration: 9914 lambda_n: 0.9564664404455608 Loss: 1.211360803212248e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9915 lambda_n: 1.0258169597161464 Loss: 1.2113608232080163e-09\n",
      "Iteration: 9916 lambda_n: 0.8747047161030792 Loss: 1.2113608390305055e-09\n",
      "Iteration: 9917 lambda_n: 0.9712079733415658 Loss: 1.2113608563899844e-09\n",
      "Iteration: 9918 lambda_n: 0.8764371486720063 Loss: 1.211360873152545e-09\n",
      "Iteration: 9919 lambda_n: 0.9354089446437834 Loss: 1.2113608909389804e-09\n",
      "Iteration: 9920 lambda_n: 0.8934855802421438 Loss: 1.2113609080134775e-09\n",
      "Iteration: 9921 lambda_n: 0.9291008155349458 Loss: 1.2113609221813745e-09\n",
      "Iteration: 9922 lambda_n: 1.0004868335060457 Loss: 1.2113609393525625e-09\n",
      "Iteration: 9923 lambda_n: 1.0145775324357866 Loss: 1.2113609546073507e-09\n",
      "Iteration: 9924 lambda_n: 0.8802677841582175 Loss: 1.2113609690201845e-09\n",
      "Iteration: 9925 lambda_n: 0.9983326879753106 Loss: 1.2113609842681662e-09\n",
      "Iteration: 9926 lambda_n: 1.010306618121948 Loss: 1.2113609982177427e-09\n",
      "Iteration: 9927 lambda_n: 0.8898795309417012 Loss: 1.2113610142917375e-09\n",
      "Iteration: 9928 lambda_n: 1.022657604246192 Loss: 1.2113610272663125e-09\n",
      "Iteration: 9929 lambda_n: 1.0034564150988787 Loss: 1.2113610392061913e-09\n",
      "Iteration: 9930 lambda_n: 0.8761292065551686 Loss: 1.2113610552428117e-09\n",
      "Iteration: 9931 lambda_n: 0.9785917284656895 Loss: 1.2113610656084883e-09\n",
      "Iteration: 9932 lambda_n: 0.9940318028705766 Loss: 1.211361082897109e-09\n",
      "Iteration: 9933 lambda_n: 0.9421665831042877 Loss: 1.2113610933524054e-09\n",
      "Iteration: 9934 lambda_n: 0.9777864653920116 Loss: 1.2113611070828008e-09\n",
      "Iteration: 9935 lambda_n: 0.8935462772377383 Loss: 1.2113611215577044e-09\n",
      "Iteration: 9936 lambda_n: 0.9243334969036374 Loss: 1.2113611339146483e-09\n",
      "Iteration: 9937 lambda_n: 0.9108147117715056 Loss: 1.211361149779203e-09\n",
      "Iteration: 9938 lambda_n: 0.9737969796037258 Loss: 1.2113611599239871e-09\n",
      "Iteration: 9939 lambda_n: 0.9776715214632086 Loss: 1.2113611735801916e-09\n",
      "Iteration: 9940 lambda_n: 0.9481549224184013 Loss: 1.2113611898164002e-09\n",
      "Iteration: 9941 lambda_n: 0.9067795895032398 Loss: 1.2113612022583654e-09\n",
      "Iteration: 9942 lambda_n: 0.8814028983499519 Loss: 1.211361213460261e-09\n",
      "Iteration: 9943 lambda_n: 1.0135095375498646 Loss: 1.2113612301893383e-09\n",
      "Iteration: 9944 lambda_n: 1.0253369300877704 Loss: 1.2113612422021667e-09\n",
      "Iteration: 9945 lambda_n: 0.982594094461999 Loss: 1.2113612582557566e-09\n",
      "Iteration: 9946 lambda_n: 1.0107723290219834 Loss: 1.2113612736883255e-09\n",
      "Iteration: 9947 lambda_n: 0.8779366743202845 Loss: 1.2113612844062101e-09\n",
      "Iteration: 9948 lambda_n: 0.998093780113175 Loss: 1.2113612952145344e-09\n",
      "Iteration: 9949 lambda_n: 0.9310877671699134 Loss: 1.2113613119058323e-09\n",
      "Iteration: 9950 lambda_n: 0.8898080305924653 Loss: 1.2113613249904858e-09\n",
      "Iteration: 9951 lambda_n: 0.8816078481559095 Loss: 1.2113613350011366e-09\n",
      "Iteration: 9952 lambda_n: 0.9528484536879517 Loss: 1.2113613482794557e-09\n",
      "Iteration: 9953 lambda_n: 0.9226088518109796 Loss: 1.21136136112196e-09\n",
      "Iteration: 9954 lambda_n: 0.8995902949260142 Loss: 1.2113613737462521e-09\n",
      "Iteration: 9955 lambda_n: 1.02723021921582 Loss: 1.2113613877077115e-09\n",
      "Iteration: 9956 lambda_n: 0.8979243827808899 Loss: 1.211361398241245e-09\n",
      "Iteration: 9957 lambda_n: 1.0135968358805116 Loss: 1.2113614130151797e-09\n",
      "Iteration: 9958 lambda_n: 0.9122992188923422 Loss: 1.211361423398681e-09\n",
      "Iteration: 9959 lambda_n: 0.8937618304790593 Loss: 1.2113614381591247e-09\n",
      "Iteration: 9960 lambda_n: 0.975582291260436 Loss: 1.2113614518233889e-09\n",
      "Iteration: 9961 lambda_n: 0.9429206671782013 Loss: 1.211361465141462e-09\n",
      "Iteration: 9962 lambda_n: 0.9243316577656158 Loss: 1.2113614790296944e-09\n",
      "Iteration: 9963 lambda_n: 1.0167214017387363 Loss: 1.2113614934299462e-09\n",
      "Iteration: 9964 lambda_n: 0.9929890835303468 Loss: 1.211361508557631e-09\n",
      "Iteration: 9965 lambda_n: 0.9081863296954529 Loss: 1.2113615235499982e-09\n",
      "Iteration: 9966 lambda_n: 0.9805347794199624 Loss: 1.211361537165912e-09\n",
      "Iteration: 9967 lambda_n: 0.8797853401823039 Loss: 1.21136155026338e-09\n",
      "Iteration: 9968 lambda_n: 0.913412617015326 Loss: 1.2113615626335619e-09\n",
      "Iteration: 9969 lambda_n: 0.9172604745589733 Loss: 1.2113615754939537e-09\n",
      "Iteration: 9970 lambda_n: 0.9089064161661783 Loss: 1.211361588595677e-09\n",
      "Iteration: 9971 lambda_n: 0.8967163783621542 Loss: 1.2113616004917487e-09\n",
      "Iteration: 9972 lambda_n: 0.9853129807663918 Loss: 1.2113616118123934e-09\n",
      "Iteration: 9973 lambda_n: 0.8924873445188577 Loss: 1.211361624241212e-09\n",
      "Iteration: 9974 lambda_n: 0.9689619822137144 Loss: 1.21136163705041e-09\n",
      "Iteration: 9975 lambda_n: 1.012518918789854 Loss: 1.2113616450179472e-09\n",
      "Iteration: 9976 lambda_n: 0.9616289075081141 Loss: 1.2113616619043116e-09\n",
      "Iteration: 9977 lambda_n: 0.8986749705272773 Loss: 1.2113616740715735e-09\n",
      "Iteration: 9978 lambda_n: 0.9840755298243021 Loss: 1.2113616837114379e-09\n",
      "Iteration: 9979 lambda_n: 0.9980730197935589 Loss: 1.211361693993722e-09\n",
      "Iteration: 9980 lambda_n: 0.9915207724597147 Loss: 1.211361707546172e-09\n",
      "Iteration: 9981 lambda_n: 0.9864819298587788 Loss: 1.2113617150856768e-09\n",
      "Iteration: 9982 lambda_n: 1.0170096784898817 Loss: 1.2113617273432397e-09\n",
      "Iteration: 9983 lambda_n: 0.8787302809084765 Loss: 1.2113617386279319e-09\n",
      "Iteration: 9984 lambda_n: 0.9107258543254511 Loss: 1.2113617471732298e-09\n",
      "Iteration: 9985 lambda_n: 1.0014391909906504 Loss: 1.2113617561014405e-09\n",
      "Iteration: 9986 lambda_n: 0.9859641212928931 Loss: 1.21136176653878e-09\n",
      "Iteration: 9987 lambda_n: 0.9570248029954587 Loss: 1.2113617741648547e-09\n",
      "Iteration: 9988 lambda_n: 0.9131795130786775 Loss: 1.2113617860454362e-09\n",
      "Iteration: 9989 lambda_n: 0.9375138159373505 Loss: 1.211361795799187e-09\n",
      "Iteration: 9990 lambda_n: 0.9850207550496902 Loss: 1.2113618059634221e-09\n",
      "Iteration: 9991 lambda_n: 0.9940189673667453 Loss: 1.2113618155052218e-09\n",
      "Iteration: 9992 lambda_n: 0.9812115512162374 Loss: 1.2113618241095643e-09\n",
      "Iteration: 9993 lambda_n: 0.9182459762881022 Loss: 1.2113618346417477e-09\n",
      "Iteration: 9994 lambda_n: 1.0139136673494173 Loss: 1.2113618461650466e-09\n",
      "Iteration: 9995 lambda_n: 0.9960779576006101 Loss: 1.2113618566720858e-09\n",
      "Iteration: 9996 lambda_n: 0.9028517712823797 Loss: 1.211361867348819e-09\n",
      "Iteration: 9997 lambda_n: 0.9107521907898696 Loss: 1.2113618776514921e-09\n",
      "Iteration: 9998 lambda_n: 0.9213069524119051 Loss: 1.2113618859938931e-09\n",
      "Iteration: 9999 lambda_n: 0.9732062679050612 Loss: 1.2113618958275488e-09\n",
      "Iteration: 10000 lambda_n: 0.9605787937918335 Loss: 1.2113619058780735e-09\n",
      "beta:  0.002180216632244036\n",
      "gamma: 0.004131340922442053\n"
     ]
    }
   ],
   "source": [
    "# BA    \n",
    "ba = time.time()\n",
    "x_BA_list, z_BA_list, dual_BA_list, iterations_BA   = BA.Briceno_Arias(N, M, frobenius_norm, Grad_Phi_NA, Sigma, D, (x1,x2,x3),gamma=1e-3, lambdan=1e-3)\n",
    "fin = time.time()\n",
    "\n",
    "Times[\"BA\"] = fin - ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56a5dbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:\n",
      " 10000\n",
      "Primal: (x1,x2,x3)\n",
      " ((array([[1188.3, 1188.3, 1188.3, 1188.3, 1188.3],\n",
      "       [44.3, 44.3, 44.3, 44.3, 44.3],\n",
      "       [25.7, 25.7, 25.7, 25.7, 25.7]]), array([[186.6, 704.0, 833.3, 1188.3, 714.3],\n",
      "       [8.9, 30.9, 44.3, 42.8, 43.5],\n",
      "       [4.6, 15.1, 25.6, 19.0, 25.7]]), array([[0.0, 0.0, 96.7, 0.0, 716.5]])), 'infactible')\n",
      "Primal: (z1,z2,z3)\n",
      " (array([[1188.2, 1188.2, 1188.2, 1188.3, 1188.2],\n",
      "       [44.1, 44.1, 45.1, 44.1, 44.1],\n",
      "       [25.4, 25.4, 25.4, 25.4, 27.3]]), array([[186.6, 704.0, 833.3, 1188.3, 714.3],\n",
      "       [8.9, 30.9, 44.3, 42.8, 43.5],\n",
      "       [4.6, 15.1, 25.6, 19.0, 25.7]]), array([[0.0, 0.0, 96.7, 0.0, 716.5]]))\n",
      "Dual: (Capacity, Equilibrium)\n",
      " (array([[0.0, 0.0, 0.0, 25.0, 0.0],\n",
      "       [0.0, 0.0, 250.0, 0.0, 0.0],\n",
      "       [0.0, 0.0, 0.0, 0.0, 475.0]]), array([[1865.7, 6336.1, 10000.0, 8342.8, 10000.0]]))\n"
     ]
    }
   ],
   "source": [
    "iter_ = -1\n",
    "print(\"Iterations:\\n\",iterations_BA)\n",
    "print(\"Primal: (x1,x2,x3)\\n\", x_BA_list[iter_])\n",
    "print(\"Primal: (z1,z2,z3)\\n\", z_BA_list[iter_])\n",
    "print(\"Dual: (Capacity, Equilibrium)\\n\",dual_BA_list[iter_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9a848c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r: 1\n",
      "Iteration: 1 Loss: 3.4040428718878912\n",
      "Iteration: 2 Loss: 2.8228770933971834\n",
      "Iteration: 3 Loss: 2.3657117523399593\n",
      "Iteration: 4 Loss: 2.0006165730498244\n",
      "Iteration: 5 Loss: 1.7077220821521906\n",
      "Iteration: 6 Loss: 1.472425401869873\n",
      "Iteration: 7 Loss: 1.2832605605490934\n",
      "Iteration: 8 Loss: 1.1310591285182194\n",
      "Iteration: 9 Loss: 1.008267336779426\n",
      "Iteration: 10 Loss: 0.9089119555618903\n",
      "Iteration: 11 Loss: 0.8283183552029256\n",
      "Iteration: 12 Loss: 0.7627607231845481\n",
      "Iteration: 13 Loss: 0.709263386279433\n",
      "Iteration: 14 Loss: 0.6654486439879979\n",
      "Iteration: 15 Loss: 0.6294156325571457\n",
      "Iteration: 16 Loss: 0.5996436918382062\n",
      "Iteration: 17 Loss: 0.5749153090414474\n",
      "Iteration: 18 Loss: 0.554254708713121\n",
      "Iteration: 19 Loss: 0.536878940063906\n",
      "Iteration: 20 Loss: 0.5221589389887958\n",
      "Iteration: 21 Loss: 0.5095885445607371\n",
      "Iteration: 22 Loss: 0.49875985303299836\n",
      "Iteration: 23 Loss: 0.4893436160572594\n",
      "Iteration: 24 Loss: 0.48107364960837434\n",
      "Iteration: 25 Loss: 0.4737344285716483\n",
      "Iteration: 26 Loss: 0.4671512091690071\n",
      "Iteration: 27 Loss: 0.4611821554585864\n",
      "Iteration: 28 Loss: 0.45571205352713484\n",
      "Iteration: 29 Loss: 0.4506472829214161\n",
      "Iteration: 30 Loss: 0.44591178352878147\n",
      "Iteration: 31 Loss: 0.4414438108964572\n",
      "Iteration: 32 Loss: 0.43719331659652994\n",
      "Iteration: 33 Loss: 0.43311982489519507\n",
      "Iteration: 34 Loss: 0.4291907044431619\n",
      "Iteration: 35 Loss: 0.4253797554004872\n",
      "Iteration: 36 Loss: 0.42166604949950726\n",
      "Iteration: 37 Loss: 0.41803297396728534\n",
      "Iteration: 38 Loss: 0.414467440727298\n",
      "Iteration: 39 Loss: 0.4109592304859368\n",
      "Iteration: 40 Loss: 0.40750044767135457\n",
      "Iteration: 41 Loss: 0.4040850671222723\n",
      "Iteration: 42 Loss: 0.400708557236503\n",
      "Iteration: 43 Loss: 0.3973646412830686\n",
      "Iteration: 44 Loss: 0.3940509224888434\n",
      "Iteration: 45 Loss: 0.39076897297875623\n",
      "Iteration: 46 Loss: 0.3875181572674849\n",
      "Iteration: 47 Loss: 0.384297595541542\n",
      "Iteration: 48 Loss: 0.3811065997135975\n",
      "Iteration: 49 Loss: 0.37794469331895914\n",
      "Iteration: 50 Loss: 0.37481155578533004\n",
      "Iteration: 51 Loss: 0.37170696434660017\n",
      "Iteration: 52 Loss: 0.3686307477086936\n",
      "Iteration: 53 Loss: 0.36558275226057946\n",
      "Iteration: 54 Loss: 0.36256281891976716\n",
      "Iteration: 55 Loss: 0.35957076843442565\n",
      "Iteration: 56 Loss: 0.35660639320842585\n",
      "Iteration: 57 Loss: 0.3536694540271355\n",
      "Iteration: 58 Loss: 0.3507596803474599\n",
      "Iteration: 59 Loss: 0.34787677306261633\n",
      "Iteration: 60 Loss: 0.3450204088637915\n",
      "Iteration: 61 Loss: 0.3421902455022804\n",
      "Iteration: 62 Loss: 0.33938592741118656\n",
      "Iteration: 63 Loss: 0.3366070912784217\n",
      "Iteration: 64 Loss: 0.3338533712750282\n",
      "Iteration: 65 Loss: 0.33112440373683\n",
      "Iteration: 66 Loss: 0.3284198311748179\n",
      "Iteration: 67 Loss: 0.32573930555224306\n",
      "Iteration: 68 Loss: 0.32308249081565477\n",
      "Iteration: 69 Loss: 0.3204490647046909\n",
      "Iteration: 70 Loss: 0.3178387198928026\n",
      "Iteration: 71 Loss: 0.31525116452971413\n",
      "Iteration: 72 Loss: 0.31268612226772474\n",
      "Iteration: 73 Loss: 0.3101433318591987\n",
      "Iteration: 74 Loss: 0.30762254641302844\n",
      "Iteration: 75 Loss: 0.3051235323945346\n",
      "Iteration: 76 Loss: 0.3026453353761836\n",
      "Iteration: 77 Loss: 0.3001849271026265\n",
      "Iteration: 78 Loss: 0.2977450057335209\n",
      "Iteration: 79 Loss: 0.2953264202373536\n",
      "Iteration: 80 Loss: 0.2929293092920702\n",
      "Iteration: 81 Loss: 0.2905535131051727\n",
      "Iteration: 82 Loss: 0.2881987646598992\n",
      "Iteration: 83 Loss: 0.28586477348432526\n",
      "Iteration: 84 Loss: 0.2835512585724323\n",
      "Iteration: 85 Loss: 0.28125795862348013\n",
      "Iteration: 86 Loss: 0.27898463288772857\n",
      "Iteration: 87 Loss: 0.27673105860848474\n",
      "Iteration: 88 Loss: 0.2744970276298804\n",
      "Iteration: 89 Loss: 0.2722823431881058\n",
      "Iteration: 90 Loss: 0.2700868172224298\n",
      "Iteration: 91 Loss: 0.2679102682572087\n",
      "Iteration: 92 Loss: 0.265749396751001\n",
      "Iteration: 93 Loss: 0.263605684649443\n",
      "Iteration: 94 Loss: 0.2614810606413931\n",
      "Iteration: 95 Loss: 0.25937557670618105\n",
      "Iteration: 96 Loss: 0.2572889163691235\n",
      "Iteration: 97 Loss: 0.2552207423599898\n",
      "Iteration: 98 Loss: 0.25317075533510774\n",
      "Iteration: 99 Loss: 0.25113869400257066\n",
      "Iteration: 100 Loss: 0.2491243263219762\n",
      "Iteration: 101 Loss: 0.24712744158549405\n",
      "Iteration: 102 Loss: 0.24514784462682282\n",
      "Iteration: 103 Loss: 0.24318535178837009\n",
      "Iteration: 104 Loss: 0.24123978813318253\n",
      "Iteration: 105 Loss: 0.23931098550380492\n",
      "Iteration: 106 Loss: 0.23739878115341645\n",
      "Iteration: 107 Loss: 0.23550301676546181\n",
      "Iteration: 108 Loss: 0.23362353773948683\n",
      "Iteration: 109 Loss: 0.23176019266151068\n",
      "Iteration: 110 Loss: 0.22991283290400108\n",
      "Iteration: 111 Loss: 0.22808131231817513\n",
      "Iteration: 112 Loss: 0.22626548699310564\n",
      "Iteration: 113 Loss: 0.22446521506400213\n",
      "Iteration: 114 Loss: 0.22268035655736798\n",
      "Iteration: 115 Loss: 0.2209107732643931\n",
      "Iteration: 116 Loss: 0.2191563286364618\n",
      "Iteration: 117 Loss: 0.2174168876984182\n",
      "Iteration: 118 Loss: 0.21569210704169955\n",
      "Iteration: 119 Loss: 0.21397928373009403\n",
      "Iteration: 120 Loss: 0.21228013355896622\n",
      "Iteration: 121 Loss: 0.21059532215478757\n",
      "Iteration: 122 Loss: 0.20892517865034077\n",
      "Iteration: 123 Loss: 0.2072697938219443\n",
      "Iteration: 124 Loss: 0.20562910637019927\n",
      "Iteration: 125 Loss: 0.2040029690645441\n",
      "Iteration: 126 Loss: 0.20239119342239187\n",
      "Iteration: 127 Loss: 0.20079357700922426\n",
      "Iteration: 128 Loss: 0.19920991862229687\n",
      "Iteration: 129 Loss: 0.19764002586381427\n",
      "Iteration: 130 Loss: 0.1960837183416791\n",
      "Iteration: 131 Loss: 0.19454082859819363\n",
      "Iteration: 132 Loss: 0.1930112020673153\n",
      "Iteration: 133 Loss: 0.19149469692235582\n",
      "Iteration: 134 Loss: 0.18999118460400508\n",
      "Iteration: 135 Loss: 0.18850055227433188\n",
      "Iteration: 136 Loss: 0.18702271017354136\n",
      "Iteration: 137 Loss: 0.1855576130296091\n",
      "Iteration: 138 Loss: 0.18410533141223717\n",
      "Iteration: 139 Loss: 0.18266637456527993\n",
      "Iteration: 140 Loss: 0.18124458217907186\n",
      "Iteration: 141 Loss: 0.1799574977658156\n",
      "Iteration: 142 Loss: 0.1780190144956559\n",
      "Iteration: 143 Loss: 0.1739795981246595\n",
      "Iteration: 144 Loss: 0.16856616320140902\n",
      "Iteration: 145 Loss: 0.16238569260042876\n",
      "Iteration: 146 Loss: 0.15591858839269757\n",
      "Iteration: 147 Loss: 0.1495360563907627\n",
      "Iteration: 148 Loss: 0.1435043770848041\n",
      "Iteration: 149 Loss: 0.13800063236566099\n",
      "Iteration: 150 Loss: 0.1331277183455429\n",
      "Iteration: 151 Loss: 0.1289288827338541\n",
      "Iteration: 152 Loss: 0.12835431613989892\n",
      "Iteration: 153 Loss: 0.12895187710474268\n",
      "Iteration: 154 Loss: 0.1290077369933957\n",
      "Iteration: 155 Loss: 0.1285987913670645\n",
      "Iteration: 156 Loss: 0.12780830385872463\n",
      "Iteration: 157 Loss: 0.12671850802210333\n",
      "Iteration: 158 Loss: 0.1254034668180947\n",
      "Iteration: 159 Loss: 0.12392847444428927\n",
      "Iteration: 160 Loss: 0.1223521757820631\n",
      "Iteration: 161 Loss: 0.1207256822060015\n",
      "Iteration: 162 Loss: 0.11909128742658366\n",
      "Iteration: 163 Loss: 0.11748216096546685\n",
      "Iteration: 164 Loss: 0.11592277885018172\n",
      "Iteration: 165 Loss: 0.1144297700373778\n",
      "Iteration: 166 Loss: 0.1130129768063387\n",
      "Iteration: 167 Loss: 0.11167660169484686\n",
      "Iteration: 168 Loss: 0.11042035657233006\n",
      "Iteration: 169 Loss: 0.1093646360601495\n",
      "Iteration: 170 Loss: 0.10858209453653409\n",
      "Iteration: 171 Loss: 0.10775446701385853\n",
      "Iteration: 172 Loss: 0.10688980237235138\n",
      "Iteration: 173 Loss: 0.10599630071919185\n",
      "Iteration: 174 Loss: 0.10508188904244198\n",
      "Iteration: 175 Loss: 0.10415391070843599\n",
      "Iteration: 176 Loss: 0.10321891603786844\n",
      "Iteration: 177 Loss: 0.10228254025680707\n",
      "Iteration: 178 Loss: 0.10134945432866875\n",
      "Iteration: 179 Loss: 0.10042337394568965\n",
      "Iteration: 180 Loss: 0.0995071124449244\n",
      "Iteration: 181 Loss: 0.09860266453040441\n",
      "Iteration: 182 Loss: 0.09771130924282374\n",
      "Iteration: 183 Loss: 0.09683372242471502\n",
      "Iteration: 184 Loss: 0.09599351792402849\n",
      "Iteration: 185 Loss: 0.09519202606829613\n",
      "Iteration: 186 Loss: 0.09439199494436902\n",
      "Iteration: 187 Loss: 0.0935939477685308\n",
      "Iteration: 188 Loss: 0.09279852140981458\n",
      "Iteration: 189 Loss: 0.09200640391786304\n",
      "Iteration: 190 Loss: 0.09121828513852749\n",
      "Iteration: 191 Loss: 0.09043481955422857\n",
      "Iteration: 192 Loss: 0.08965660011482629\n",
      "Iteration: 193 Loss: 0.08888414160153314\n",
      "Iteration: 194 Loss: 0.08811787196650717\n",
      "Iteration: 195 Loss: 0.08735813008959663\n",
      "Iteration: 196 Loss: 0.08660516846773113\n",
      "Iteration: 197 Loss: 0.08585915948007834\n",
      "Iteration: 198 Loss: 0.08512020403417225\n",
      "Iteration: 199 Loss: 0.0843883415785096\n",
      "Iteration: 200 Loss: 0.0836635606523594\n",
      "Iteration: 201 Loss: 0.08294580932341829\n",
      "Iteration: 202 Loss: 0.08223500503098057\n",
      "Iteration: 203 Loss: 0.08153104350142201\n",
      "Iteration: 204 Loss: 0.08083380653114117\n",
      "Iteration: 205 Loss: 0.08014316853850638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 206 Loss: 0.07945900187106297\n",
      "Iteration: 207 Loss: 0.07878118091851051\n",
      "Iteration: 208 Loss: 0.07810958512771535\n",
      "Iteration: 209 Loss: 0.077444101045664\n",
      "Iteration: 210 Loss: 0.07678462353235509\n",
      "Iteration: 211 Loss: 0.07613105629077196\n",
      "Iteration: 212 Loss: 0.07548331185775721\n",
      "Iteration: 213 Loss: 0.07484131119010295\n",
      "Iteration: 214 Loss: 0.07420498296647607\n",
      "Iteration: 215 Loss: 0.07357426270963806\n",
      "Iteration: 216 Loss: 0.07294909181618815\n",
      "Iteration: 217 Loss: 0.07232941656386854\n",
      "Iteration: 218 Loss: 0.07171518715014644\n",
      "Iteration: 219 Loss: 0.07110635680090191\n",
      "Iteration: 220 Loss: 0.07050288097498884\n",
      "Iteration: 221 Loss: 0.06990431589007547\n",
      "Iteration: 222 Loss: 0.06931015829719504\n",
      "Iteration: 223 Loss: 0.06872078887762181\n",
      "Iteration: 224 Loss: 0.06813636233068317\n",
      "Iteration: 225 Loss: 0.06755700600761588\n",
      "Iteration: 226 Loss: 0.06698280772274791\n",
      "Iteration: 227 Loss: 0.06641381267207024\n",
      "Iteration: 228 Loss: 0.06585002734939313\n",
      "Iteration: 229 Loss: 0.06529142730065908\n",
      "Iteration: 230 Loss: 0.06473796615818779\n",
      "Iteration: 231 Loss: 0.06418958418383491\n",
      "Iteration: 232 Loss: 0.06364621529093778\n",
      "Iteration: 233 Loss: 0.06310779212392953\n",
      "Iteration: 234 Loss: 0.06257424921682335\n",
      "Iteration: 235 Loss: 0.062045524526892624\n",
      "Iteration: 236 Loss: 0.06152155977006043\n",
      "Iteration: 237 Loss: 0.06100230000395129\n",
      "Iteration: 238 Loss: 0.06048769285064607\n",
      "Iteration: 239 Loss: 0.05997768765827097\n",
      "Iteration: 240 Loss: 0.059472234796232835\n",
      "Iteration: 241 Loss: 0.058971285182605415\n",
      "Iteration: 242 Loss: 0.058474643160612294\n",
      "Iteration: 243 Loss: 0.057978952281518004\n",
      "Iteration: 244 Loss: 0.05748698502418166\n",
      "Iteration: 245 Loss: 0.05699946691891381\n",
      "Iteration: 246 Loss: 0.05651674736211456\n",
      "Iteration: 247 Loss: 0.05603890660057292\n",
      "Iteration: 248 Loss: 0.05556587073689295\n",
      "Iteration: 249 Loss: 0.055097498553413254\n",
      "Iteration: 250 Loss: 0.05463363453585799\n",
      "Iteration: 251 Loss: 0.05417413521717855\n",
      "Iteration: 252 Loss: 0.053718878506685976\n",
      "Iteration: 253 Loss: 0.0532677639892272\n",
      "Iteration: 254 Loss: 0.05282070947304367\n",
      "Iteration: 255 Loss: 0.05237764671264515\n",
      "Iteration: 256 Loss: 0.05193851762357573\n",
      "Iteration: 257 Loss: 0.05150327138178881\n",
      "Iteration: 258 Loss: 0.05107186236214536\n",
      "Iteration: 259 Loss: 0.050644248720671664\n",
      "Iteration: 260 Loss: 0.050220391418002296\n",
      "Iteration: 261 Loss: 0.04980025352751691\n",
      "Iteration: 262 Loss: 0.04938379972507255\n",
      "Iteration: 263 Loss: 0.048970995899484525\n",
      "Iteration: 264 Loss: 0.048561808850284806\n",
      "Iteration: 265 Loss: 0.04815620605438213\n",
      "Iteration: 266 Loss: 0.04775415549024442\n",
      "Iteration: 267 Loss: 0.04735562551080323\n",
      "Iteration: 268 Loss: 0.046960584756972554\n",
      "Iteration: 269 Loss: 0.046569002103886564\n",
      "Iteration: 270 Loss: 0.04618084663233561\n",
      "Iteration: 271 Loss: 0.04579608761857505\n",
      "Iteration: 272 Loss: 0.045414694536633146\n",
      "Iteration: 273 Loss: 0.04503663706832718\n",
      "Iteration: 274 Loss: 0.04466188511726907\n",
      "Iteration: 275 Loss: 0.0442904088241283\n",
      "Iteration: 276 Loss: 0.04392217858126299\n",
      "Iteration: 277 Loss: 0.043557165045509584\n",
      "Iteration: 278 Loss: 0.04319533914845836\n",
      "Iteration: 279 Loss: 0.042836672103933854\n",
      "Iteration: 280 Loss: 0.04248113541268185\n",
      "Iteration: 281 Loss: 0.042128700864452244\n",
      "Iteration: 282 Loss: 0.041779340537786015\n",
      "Iteration: 283 Loss: 0.041433026797874244\n",
      "Iteration: 284 Loss: 0.0410897322928834\n",
      "Iteration: 285 Loss: 0.040749429949128\n",
      "Iteration: 286 Loss: 0.04041209296545188\n",
      "Iteration: 287 Loss: 0.04007769480713485\n",
      "Iteration: 288 Loss: 0.03974620919959899\n",
      "Iteration: 289 Loss: 0.03941761012213861\n",
      "Iteration: 290 Loss: 0.03909187180185192\n",
      "Iteration: 291 Loss: 0.038768968707902175\n",
      "Iteration: 292 Loss: 0.03844887554620476\n",
      "Iteration: 293 Loss: 0.03813156725458815\n",
      "Iteration: 294 Loss: 0.03781701899845724\n",
      "Iteration: 295 Loss: 0.03750520616695636\n",
      "Iteration: 296 Loss: 0.037196104369611016\n",
      "Iteration: 297 Loss: 0.036889689433415564\n",
      "Iteration: 298 Loss: 0.03658593740031767\n",
      "Iteration: 299 Loss: 0.03628482452505179\n",
      "Iteration: 300 Loss: 0.03598632727326378\n",
      "Iteration: 301 Loss: 0.03569042231987399\n",
      "Iteration: 302 Loss: 0.035397086547624024\n",
      "Iteration: 303 Loss: 0.0351062970457572\n",
      "Iteration: 304 Loss: 0.03481803110878955\n",
      "Iteration: 305 Loss: 0.034532266235330185\n",
      "Iteration: 306 Loss: 0.03424898012691705\n",
      "Iteration: 307 Loss: 0.03396815068684426\n",
      "Iteration: 308 Loss: 0.03368975601895355\n",
      "Iteration: 309 Loss: 0.03341377442638062\n",
      "Iteration: 310 Loss: 0.03314018441024156\n",
      "Iteration: 311 Loss: 0.03286896466825608\n",
      "Iteration: 312 Loss: 0.032600050288019496\n",
      "Iteration: 313 Loss: 0.03233338763709222\n",
      "Iteration: 314 Loss: 0.03206897581049963\n",
      "Iteration: 315 Loss: 0.031806811764880316\n",
      "Iteration: 316 Loss: 0.031546888803498596\n",
      "Iteration: 317 Loss: 0.03128919693032785\n",
      "Iteration: 318 Loss: 0.03103372336333663\n",
      "Iteration: 319 Loss: 0.030780453128570705\n",
      "Iteration: 320 Loss: 0.03052936963134061\n",
      "Iteration: 321 Loss: 0.030280455151602877\n",
      "Iteration: 322 Loss: 0.03003369124557704\n",
      "Iteration: 323 Loss: 0.029789059055247348\n",
      "Iteration: 324 Loss: 0.029546539536809677\n",
      "Iteration: 325 Loss: 0.02930611362242286\n",
      "Iteration: 326 Loss: 0.029067762329641162\n",
      "Iteration: 327 Loss: 0.028831466831361587\n",
      "Iteration: 328 Loss: 0.0285972084970169\n",
      "Iteration: 329 Loss: 0.028364968913597365\n",
      "Iteration: 330 Loss: 0.028134729893126553\n",
      "Iteration: 331 Loss: 0.027906473471553145\n",
      "Iteration: 332 Loss: 0.027680181902648444\n",
      "Iteration: 333 Loss: 0.027455837649417815\n",
      "Iteration: 334 Loss: 0.027233423374692766\n",
      "Iteration: 335 Loss: 0.027012921931944277\n",
      "Iteration: 336 Loss: 0.026794316356907252\n",
      "Iteration: 337 Loss: 0.0265775898602839\n",
      "Iteration: 338 Loss: 0.02636272582159644\n",
      "Iteration: 339 Loss: 0.026149707784122157\n",
      "Iteration: 340 Loss: 0.02593851945078773\n",
      "Iteration: 341 Loss: 0.025729144680863136\n",
      "Iteration: 342 Loss: 0.02552156748730008\n",
      "Iteration: 343 Loss: 0.025315772034572782\n",
      "Iteration: 344 Loss: 0.025111742636900032\n",
      "Iteration: 345 Loss: 0.024909463756752733\n",
      "Iteration: 346 Loss: 0.024708920003577425\n",
      "Iteration: 347 Loss: 0.02451009613268383\n",
      "Iteration: 348 Loss: 0.024312977044265244\n",
      "Iteration: 349 Loss: 0.02411754778253502\n",
      "Iteration: 350 Loss: 0.023923793534972544\n",
      "Iteration: 351 Loss: 0.023731699631681577\n",
      "Iteration: 352 Loss: 0.023541251544871227\n",
      "Iteration: 353 Loss: 0.023352434888472554\n",
      "Iteration: 354 Loss: 0.023165235417911823\n",
      "Iteration: 355 Loss: 0.022979639030061196\n",
      "Iteration: 356 Loss: 0.02279563176339676\n",
      "Iteration: 357 Loss: 0.022613199798396894\n",
      "Iteration: 358 Loss: 0.022432329458218127\n",
      "Iteration: 359 Loss: 0.022253007209700283\n",
      "Iteration: 360 Loss: 0.022075219664756788\n",
      "Iteration: 361 Loss: 0.021898953582223445\n",
      "Iteration: 362 Loss: 0.02172419587025646\n",
      "Iteration: 363 Loss: 0.021550933589390097\n",
      "Iteration: 364 Loss: 0.021379153956397983\n",
      "Iteration: 365 Loss: 0.021208844349136006\n",
      "Iteration: 366 Loss: 0.021039992312593566\n",
      "Iteration: 367 Loss: 0.02087258556644764\n",
      "Iteration: 368 Loss: 0.02070661201449473\n",
      "Iteration: 369 Loss: 0.020542059756453002\n",
      "Iteration: 370 Loss: 0.020378917102776967\n",
      "Iteration: 371 Loss: 0.02021717259333957\n",
      "Iteration: 372 Loss: 0.020056815021118547\n",
      "Iteration: 373 Loss: 0.019897833462431878\n",
      "Iteration: 374 Loss: 0.019740217315827868\n",
      "Iteration: 375 Loss: 0.01958395635255005\n",
      "Iteration: 376 Loss: 0.01942904078268\n",
      "Iteration: 377 Loss: 0.019275461342796785\n",
      "Iteration: 378 Loss: 0.01912320941362145\n",
      "Iteration: 379 Loss: 0.018972277180124574\n",
      "Iteration: 380 Loss: 0.018822657852861756\n",
      "Iteration: 381 Loss: 0.018674345979364623\n",
      "Iteration: 382 Loss: 0.018527337890934102\n",
      "Iteration: 383 Loss: 0.01838163235810809\n",
      "Iteration: 384 Loss: 0.01823723157674373\n",
      "Iteration: 385 Loss: 0.018094142694561336\n",
      "Iteration: 386 Loss: 0.01795238025318206\n",
      "Iteration: 387 Loss: 0.017811970245300148\n",
      "Iteration: 388 Loss: 0.017672957157556544\n",
      "Iteration: 389 Loss: 0.0175354168373411\n",
      "Iteration: 390 Loss: 0.01739948143883612\n",
      "Iteration: 391 Loss: 0.017265391188906107\n",
      "Iteration: 392 Loss: 0.017133609923499625\n",
      "Iteration: 393 Loss: 0.017005098775912948\n",
      "Iteration: 394 Loss: 0.01688194540171217\n",
      "Iteration: 395 Loss: 0.016768244363396634\n",
      "Iteration: 396 Loss: 0.016668421006829397\n",
      "Iteration: 397 Loss: 0.01639637550140808\n",
      "Iteration: 398 Loss: 0.01587082612811561\n",
      "Iteration: 399 Loss: 0.015182097907580296\n",
      "Iteration: 400 Loss: 0.01440609436883867\n",
      "Iteration: 401 Loss: 0.013603305753109874\n",
      "Iteration: 402 Loss: 0.012819601305376245\n",
      "Iteration: 403 Loss: 0.012087742989951345\n",
      "Iteration: 404 Loss: 0.011429189640520754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 405 Loss: 0.010855983589803699\n",
      "Iteration: 406 Loss: 0.010372595020227094\n",
      "Iteration: 407 Loss: 0.010351796697910875\n",
      "Iteration: 408 Loss: 0.010475856918931476\n",
      "Iteration: 409 Loss: 0.010529281303325824\n",
      "Iteration: 410 Loss: 0.010522064463527239\n",
      "Iteration: 411 Loss: 0.010464799076940085\n",
      "Iteration: 412 Loss: 0.010368013636570455\n",
      "Iteration: 413 Loss: 0.010241678209857762\n",
      "Iteration: 414 Loss: 0.010094853643302872\n",
      "Iteration: 415 Loss: 0.009935465983551469\n",
      "Iteration: 416 Loss: 0.009770189042440265\n",
      "Iteration: 417 Loss: 0.009604416743619597\n",
      "Iteration: 418 Loss: 0.009442305953128564\n",
      "Iteration: 419 Loss: 0.009286870908056835\n",
      "Iteration: 420 Loss: 0.009140111958009619\n",
      "Iteration: 421 Loss: 0.00900316365513642\n",
      "Iteration: 422 Loss: 0.008877543985578697\n",
      "Iteration: 423 Loss: 0.008835729615441644\n",
      "Iteration: 424 Loss: 0.008785835693583082\n",
      "Iteration: 425 Loss: 0.008728682440526454\n",
      "Iteration: 426 Loss: 0.008665265400383926\n",
      "Iteration: 427 Loss: 0.008596664705059338\n",
      "Iteration: 428 Loss: 0.008523972505790338\n",
      "Iteration: 429 Loss: 0.008448237597391954\n",
      "Iteration: 430 Loss: 0.00837042568616543\n",
      "Iteration: 431 Loss: 0.008291393385586507\n",
      "Iteration: 432 Loss: 0.008211873833381405\n",
      "Iteration: 433 Loss: 0.008132471777814045\n",
      "Iteration: 434 Loss: 0.008053666048813355\n",
      "Iteration: 435 Loss: 0.00797581748135547\n",
      "Iteration: 436 Loss: 0.007899180567203479\n",
      "Iteration: 437 Loss: 0.007823917352989802\n",
      "Iteration: 438 Loss: 0.0077501123578644845\n",
      "Iteration: 439 Loss: 0.0076777875366583846\n",
      "Iteration: 440 Loss: 0.007606916552811868\n",
      "Iteration: 441 Loss: 0.007537437840904588\n",
      "Iteration: 442 Loss: 0.007469266126371256\n",
      "Iteration: 443 Loss: 0.0074023022274728205\n",
      "Iteration: 444 Loss: 0.007336441091536794\n",
      "Iteration: 445 Loss: 0.0072715781151940495\n",
      "Iteration: 446 Loss: 0.0072076138693104586\n",
      "Iteration: 447 Loss: 0.007144457396717175\n",
      "Iteration: 448 Loss: 0.007082028278256316\n",
      "Iteration: 449 Loss: 0.00702025767372929\n",
      "Iteration: 450 Loss: 0.006959088542611463\n",
      "Iteration: 451 Loss: 0.006898475238155769\n",
      "Iteration: 452 Loss: 0.006838382650683335\n",
      "Iteration: 453 Loss: 0.006778785053936268\n",
      "Iteration: 454 Loss: 0.006719664784465998\n",
      "Iteration: 455 Loss: 0.006661010859761258\n",
      "Iteration: 456 Loss: 0.006602817617484937\n",
      "Iteration: 457 Loss: 0.006545083436660315\n",
      "Iteration: 458 Loss: 0.00648780958253219\n",
      "Iteration: 459 Loss: 0.006430999200440906\n",
      "Iteration: 460 Loss: 0.0063746564705123195\n",
      "Iteration: 461 Loss: 0.006318785924220537\n",
      "Iteration: 462 Loss: 0.006263391915762118\n",
      "Iteration: 463 Loss: 0.006208478235413563\n",
      "Iteration: 464 Loss: 0.006154047848342092\n",
      "Iteration: 465 Loss: 0.006100102740359081\n",
      "Iteration: 466 Loss: 0.006046643851509269\n",
      "Iteration: 467 Loss: 0.005993671078877985\n",
      "Iteration: 468 Loss: 0.005941183331261611\n",
      "Iteration: 469 Loss: 0.005889178620144682\n",
      "Iteration: 470 Loss: 0.005837654173529391\n",
      "Iteration: 471 Loss: 0.00578660656139814\n",
      "Iteration: 472 Loss: 0.005736031823812602\n",
      "Iteration: 473 Loss: 0.005685925594761994\n",
      "Iteration: 474 Loss: 0.005636283216790488\n",
      "Iteration: 475 Loss: 0.005587099843116952\n",
      "Iteration: 476 Loss: 0.00553837052538236\n",
      "Iteration: 477 Loss: 0.005490090286326297\n",
      "Iteration: 478 Loss: 0.005442254177591198\n",
      "Iteration: 479 Loss: 0.005394857323532807\n",
      "Iteration: 480 Loss: 0.005347894952372351\n",
      "Iteration: 481 Loss: 0.0053013624163086946\n",
      "Iteration: 482 Loss: 0.005255255202341785\n",
      "Iteration: 483 Loss: 0.005209568935578908\n",
      "Iteration: 484 Loss: 0.005164299376713102\n",
      "Iteration: 485 Loss: 0.005119442415226697\n",
      "Iteration: 486 Loss: 0.0050749940597037205\n",
      "Iteration: 487 Loss: 0.005030950426419318\n",
      "Iteration: 488 Loss: 0.0049873077271733795\n",
      "Iteration: 489 Loss: 0.0049440622571340674\n",
      "Iteration: 490 Loss: 0.004901210383261226\n",
      "Iteration: 491 Loss: 0.004858748533712854\n",
      "Iteration: 492 Loss: 0.00481667318849045\n",
      "Iteration: 493 Loss: 0.004774980871454658\n",
      "Iteration: 494 Loss: 0.0047336681437413364\n",
      "Iteration: 495 Loss: 0.004692731598537981\n",
      "Iteration: 496 Loss: 0.004652167857118992\n",
      "Iteration: 497 Loss: 0.0046119735660077035\n",
      "Iteration: 498 Loss: 0.004572145395105347\n",
      "Iteration: 499 Loss: 0.004532680036627466\n",
      "Iteration: 500 Loss: 0.004493574204680699\n",
      "Iteration: 501 Loss: 0.004454824635331744\n",
      "Iteration: 502 Loss: 0.004416428087025957\n",
      "Iteration: 503 Loss: 0.004378381341237053\n",
      "Iteration: 504 Loss: 0.004340681203243895\n",
      "Iteration: 505 Loss: 0.004303324502953051\n",
      "Iteration: 506 Loss: 0.004266308095704677\n",
      "Iteration: 507 Loss: 0.004229628863007863\n",
      "Iteration: 508 Loss: 0.0041932837131821105\n",
      "Iteration: 509 Loss: 0.004157269581878152\n",
      "Iteration: 510 Loss: 0.0041215834324742585\n",
      "Iteration: 511 Loss: 0.004086222256345094\n",
      "Iteration: 512 Loss: 0.004051183073010444\n",
      "Iteration: 513 Loss: 0.00401646293017447\n",
      "Iteration: 514 Loss: 0.00398205890366745\n",
      "Iteration: 515 Loss: 0.003947968097306679\n",
      "Iteration: 516 Loss: 0.003914187642688851\n",
      "Iteration: 517 Loss: 0.003880714698934195\n",
      "Iteration: 518 Loss: 0.0038475464523889257\n",
      "Iteration: 519 Loss: 0.0038146801163045202\n",
      "Iteration: 520 Loss: 0.003782112930499814\n",
      "Iteration: 521 Loss: 0.0037498421610181773\n",
      "Iteration: 522 Loss: 0.0037178650997850646\n",
      "Iteration: 523 Loss: 0.0036861790642700666\n",
      "Iteration: 524 Loss: 0.0036547813971616327\n",
      "Iteration: 525 Loss: 0.003623669466050859\n",
      "Iteration: 526 Loss: 0.003592840663131256\n",
      "Iteration: 527 Loss: 0.0035622924049121263\n",
      "Iteration: 528 Loss: 0.0035320221319447955\n",
      "Iteration: 529 Loss: 0.0035020273085645463\n",
      "Iteration: 530 Loss: 0.0034723054226411267\n",
      "Iteration: 531 Loss: 0.003442853985343125\n",
      "Iteration: 532 Loss: 0.003413670530910601\n",
      "Iteration: 533 Loss: 0.0033847526164350033\n",
      "Iteration: 534 Loss: 0.0033560978216460456\n",
      "Iteration: 535 Loss: 0.0033277037487046404\n",
      "Iteration: 536 Loss: 0.003299568021999322\n",
      "Iteration: 537 Loss: 0.003271688287944955\n",
      "Iteration: 538 Loss: 0.003244062214786394\n",
      "Iteration: 539 Loss: 0.0032166874924015455\n",
      "Iteration: 540 Loss: 0.0031895618321077495\n",
      "Iteration: 541 Loss: 0.0031626829664672534\n",
      "Iteration: 542 Loss: 0.0031360486490960913\n",
      "Iteration: 543 Loss: 0.0031096566544704927\n",
      "Iteration: 544 Loss: 0.003083504777737093\n",
      "Iteration: 545 Loss: 0.0030575908345221235\n",
      "Iteration: 546 Loss: 0.003031912660741563\n",
      "Iteration: 547 Loss: 0.0030064681124125853\n",
      "Iteration: 548 Loss: 0.0029812550654659525\n",
      "Iteration: 549 Loss: 0.0029562714155583567\n",
      "Iteration: 550 Loss: 0.002931515077888921\n",
      "Iteration: 551 Loss: 0.0029069839870130066\n",
      "Iteration: 552 Loss: 0.0028826760966610917\n",
      "Iteration: 553 Loss: 0.00285858937955598\n",
      "Iteration: 554 Loss: 0.00283472182723463\n",
      "Iteration: 555 Loss: 0.002811071449869145\n",
      "Iteration: 556 Loss: 0.0027876362760905224\n",
      "Iteration: 557 Loss: 0.0027644143528136747\n",
      "Iteration: 558 Loss: 0.0027414037450649675\n",
      "Iteration: 559 Loss: 0.0027186025358099013\n",
      "Iteration: 560 Loss: 0.002696008825784066\n",
      "Iteration: 561 Loss: 0.002673620733324551\n",
      "Iteration: 562 Loss: 0.0026514363942033493\n",
      "Iteration: 563 Loss: 0.002629453961462507\n",
      "Iteration: 564 Loss: 0.002607671605250813\n",
      "Iteration: 565 Loss: 0.0025860875126623257\n",
      "Iteration: 566 Loss: 0.0025646998875750245\n",
      "Iteration: 567 Loss: 0.0025435069504932527\n",
      "Iteration: 568 Loss: 0.0025225069383897247\n",
      "Iteration: 569 Loss: 0.002501698104549483\n",
      "Iteration: 570 Loss: 0.002481078718415557\n",
      "Iteration: 571 Loss: 0.0024606470654365683\n",
      "Iteration: 572 Loss: 0.0024404014469138293\n",
      "Iteration: 573 Loss: 0.002420340179852291\n",
      "Iteration: 574 Loss: 0.0024004615968106022\n",
      "Iteration: 575 Loss: 0.0023807640457538175\n",
      "Iteration: 576 Loss: 0.0023612458899073424\n",
      "Iteration: 577 Loss: 0.0023419055076118116\n",
      "Iteration: 578 Loss: 0.0023227412921795433\n",
      "Iteration: 579 Loss: 0.0023037516517518852\n",
      "Iteration: 580 Loss: 0.002284935009158536\n",
      "Iteration: 581 Loss: 0.0022662898017767616\n",
      "Iteration: 582 Loss: 0.0022478144813938483\n",
      "Iteration: 583 Loss: 0.002229507514069571\n",
      "Iteration: 584 Loss: 0.0022113673799997813\n",
      "Iteration: 585 Loss: 0.0021933925733812628\n",
      "Iteration: 586 Loss: 0.0021755816022791187\n",
      "Iteration: 587 Loss: 0.002157932988493154\n",
      "Iteration: 588 Loss: 0.002140445267427346\n",
      "Iteration: 589 Loss: 0.0021231169879598003\n",
      "Iteration: 590 Loss: 0.0021059467123136677\n",
      "Iteration: 591 Loss: 0.0020889330159291303\n",
      "Iteration: 592 Loss: 0.002072074487337013\n",
      "Iteration: 593 Loss: 0.002055369728033911\n",
      "Iteration: 594 Loss: 0.0020388173523576323\n",
      "Iteration: 595 Loss: 0.002022415987363511\n",
      "Iteration: 596 Loss: 0.0020061642727030394\n",
      "Iteration: 597 Loss: 0.001990060860502765\n",
      "Iteration: 598 Loss: 0.0019741044152445466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 599 Loss: 0.0019582936136466208\n",
      "Iteration: 600 Loss: 0.0019426271445460157\n",
      "Iteration: 601 Loss: 0.001927103708781977\n",
      "Iteration: 602 Loss: 0.0019117220190804382\n",
      "Iteration: 603 Loss: 0.0018964807999417436\n",
      "Iteration: 604 Loss: 0.0018813787875228516\n",
      "Iteration: 605 Loss: 0.0018664147295285158\n",
      "Iteration: 606 Loss: 0.001851587385098471\n",
      "Iteration: 607 Loss: 0.0018368955246982652\n",
      "Iteration: 608 Loss: 0.0018223379300088753\n",
      "Iteration: 609 Loss: 0.0018079133938188295\n",
      "Iteration: 610 Loss: 0.0017936207199178678\n",
      "Iteration: 611 Loss: 0.0017794587229894103\n",
      "Iteration: 612 Loss: 0.0017654262285061333\n",
      "Iteration: 613 Loss: 0.001751522072625531\n",
      "Iteration: 614 Loss: 0.0017377451020862801\n",
      "Iteration: 615 Loss: 0.0017240941741063502\n",
      "Iteration: 616 Loss: 0.0017105681562819928\n",
      "Iteration: 617 Loss: 0.0016971659264865464\n",
      "Iteration: 618 Loss: 0.0016838863727714423\n",
      "Iteration: 619 Loss: 0.0016707283932680668\n",
      "Iteration: 620 Loss: 0.0016576908960897465\n",
      "Iteration: 621 Loss: 0.0016447727992349745\n",
      "Iteration: 622 Loss: 0.0016319730304924813\n",
      "Iteration: 623 Loss: 0.001619290527345764\n",
      "Iteration: 624 Loss: 0.001606724236880259\n",
      "Iteration: 625 Loss: 0.0015942731156887758\n",
      "Iteration: 626 Loss: 0.0015819361297812547\n",
      "Iteration: 627 Loss: 0.0015697122544925152\n",
      "Iteration: 628 Loss: 0.001557600474392333\n",
      "Iteration: 629 Loss: 0.0015455997831964005\n",
      "Iteration: 630 Loss: 0.0015337091836775086\n",
      "Iteration: 631 Loss: 0.001521927687578279\n",
      "Iteration: 632 Loss: 0.001510254315524746\n",
      "Iteration: 633 Loss: 0.0014986880969398073\n",
      "Iteration: 634 Loss: 0.0014872280699588032\n",
      "Iteration: 635 Loss: 0.0014758732813453276\n",
      "Iteration: 636 Loss: 0.0014646227864078892\n",
      "Iteration: 637 Loss: 0.0014534756489175977\n",
      "Iteration: 638 Loss: 0.0014424309410269091\n",
      "Iteration: 639 Loss: 0.0014314877431882832\n",
      "Iteration: 640 Loss: 0.0014206451440750435\n",
      "Iteration: 641 Loss: 0.0014099022405021306\n",
      "Iteration: 642 Loss: 0.0013992581373473815\n",
      "Iteration: 643 Loss: 0.0013887119474750244\n",
      "Iteration: 644 Loss: 0.0013782627916585502\n",
      "Iteration: 645 Loss: 0.0013679097985048933\n",
      "Iteration: 646 Loss: 0.0013576521043796197\n",
      "Iteration: 647 Loss: 0.0013474888533324483\n",
      "Iteration: 648 Loss: 0.0013374191970250168\n",
      "Iteration: 649 Loss: 0.0013274422946571139\n",
      "Iteration: 650 Loss: 0.0013175573128951295\n",
      "Iteration: 651 Loss: 0.00130776342580169\n",
      "Iteration: 652 Loss: 0.001298059814763869\n",
      "Iteration: 653 Loss: 0.0012884456684258909\n",
      "Iteration: 654 Loss: 0.0012789201826184189\n",
      "Iteration: 655 Loss: 0.001269482560292116\n",
      "Iteration: 656 Loss: 0.001260132011448859\n",
      "Iteration: 657 Loss: 0.0012508677530768794\n",
      "Iteration: 658 Loss: 0.0012416890090842757\n",
      "Iteration: 659 Loss: 0.0012325950102337262\n",
      "Iteration: 660 Loss: 0.0012235849940785638\n",
      "Iteration: 661 Loss: 0.0012146582048991298\n",
      "Iteration: 662 Loss: 0.0012058138936399522\n",
      "Iteration: 663 Loss: 0.0011970513178477023\n",
      "Iteration: 664 Loss: 0.0011883697416098602\n",
      "Iteration: 665 Loss: 0.0011797684354932236\n",
      "Iteration: 666 Loss: 0.0011712466764847989\n",
      "Iteration: 667 Loss: 0.0011628037479314438\n",
      "Iteration: 668 Loss: 0.0011544389394824277\n",
      "Iteration: 669 Loss: 0.0011461515470298001\n",
      "Iteration: 670 Loss: 0.001137940872652523\n",
      "Iteration: 671 Loss: 0.0011298062245589085\n",
      "Iteration: 672 Loss: 0.0011217469170301713\n",
      "Iteration: 673 Loss: 0.0011137622703657126\n",
      "Iteration: 674 Loss: 0.0011058516108279493\n",
      "Iteration: 675 Loss: 0.0010980142705880086\n",
      "Iteration: 676 Loss: 0.0010902495876727737\n",
      "Iteration: 677 Loss: 0.0010825569059105884\n",
      "Iteration: 678 Loss: 0.0010749355748796827\n",
      "Iteration: 679 Loss: 0.001067384949856433\n",
      "Iteration: 680 Loss: 0.0010599043917638967\n",
      "Iteration: 681 Loss: 0.0010524932671213147\n",
      "Iteration: 682 Loss: 0.001045150947993003\n",
      "Iteration: 683 Loss: 0.0010378768119406316\n",
      "Iteration: 684 Loss: 0.0010306702419728307\n",
      "Iteration: 685 Loss: 0.0010235306264972496\n",
      "Iteration: 686 Loss: 0.0010164573592719927\n",
      "Iteration: 687 Loss: 0.0010094498393587307\n",
      "Iteration: 688 Loss: 0.001002507471075659\n",
      "Iteration: 689 Loss: 0.0009956296639516392\n",
      "Iteration: 690 Loss: 0.0009888158326789318\n",
      "Iteration: 691 Loss: 0.0009820653970691404\n",
      "Iteration: 692 Loss: 0.000975377782007025\n",
      "Iteration: 693 Loss: 0.0009687524174074046\n",
      "Iteration: 694 Loss: 0.0009621887381698988\n",
      "Iteration: 695 Loss: 0.0009556861841356764\n",
      "Iteration: 696 Loss: 0.0009492442000447312\n",
      "Iteration: 697 Loss: 0.0009428622354926589\n",
      "Iteration: 698 Loss: 0.0009365397448888214\n",
      "Iteration: 699 Loss: 0.0009302761874137184\n",
      "Iteration: 700 Loss: 0.0009240710269783568\n",
      "Iteration: 701 Loss: 0.0009179237321827168\n",
      "Iteration: 702 Loss: 0.0009118337762753464\n",
      "Iteration: 703 Loss: 0.0009058006371130027\n",
      "Iteration: 704 Loss: 0.0008998237971204786\n",
      "Iteration: 705 Loss: 0.0008939027432509104\n",
      "Iteration: 706 Loss: 0.0008880369669474133\n",
      "Iteration: 707 Loss: 0.0008822259641030227\n",
      "Iteration: 708 Loss: 0.0008764692350237705\n",
      "Iteration: 709 Loss: 0.0008707662843883305\n",
      "Iteration: 710 Loss: 0.0008651166212128818\n",
      "Iteration: 711 Loss: 0.0008595197588110818\n",
      "Iteration: 712 Loss: 0.0008539752147578757\n",
      "Iteration: 713 Loss: 0.0008484825108533296\n",
      "Iteration: 714 Loss: 0.0008430411730846849\n",
      "Iteration: 715 Loss: 0.0008376507315909662\n",
      "Iteration: 716 Loss: 0.000832310720626529\n",
      "Iteration: 717 Loss: 0.0008270206785247471\n",
      "Iteration: 718 Loss: 0.0008217801476644637\n",
      "Iteration: 719 Loss: 0.0008165886744326169\n",
      "Iteration: 720 Loss: 0.0008114458091900287\n",
      "Iteration: 721 Loss: 0.000806351106237262\n",
      "Iteration: 722 Loss: 0.0008013041237794886\n",
      "Iteration: 723 Loss: 0.0007963044238926927\n",
      "Iteration: 724 Loss: 0.0007913515724893536\n",
      "Iteration: 725 Loss: 0.0007864451392852028\n",
      "Iteration: 726 Loss: 0.0007815846977650147\n",
      "Iteration: 727 Loss: 0.0007767698251499977\n",
      "Iteration: 728 Loss: 0.0007720001023646339\n",
      "Iteration: 729 Loss: 0.0007672751140034258\n",
      "Iteration: 730 Loss: 0.0007625944482984442\n",
      "Iteration: 731 Loss: 0.0007579576970872423\n",
      "Iteration: 732 Loss: 0.0007533644557807103\n",
      "Iteration: 733 Loss: 0.0007488143233303314\n",
      "Iteration: 734 Loss: 0.0007443069021970503\n",
      "Iteration: 735 Loss: 0.000739841798319821\n",
      "Iteration: 736 Loss: 0.0007354186210831894\n",
      "Iteration: 737 Loss: 0.0007310369832876875\n",
      "Iteration: 738 Loss: 0.0007266965011169183\n",
      "Iteration: 739 Loss: 0.0007223967941087318\n",
      "Iteration: 740 Loss: 0.0007181374851235957\n",
      "Iteration: 741 Loss: 0.0007139182003139202\n",
      "Iteration: 742 Loss: 0.0007097385690939778\n",
      "Iteration: 743 Loss: 0.0007055982241102706\n",
      "Iteration: 744 Loss: 0.0007014968012116797\n",
      "Iteration: 745 Loss: 0.0006974339394192076\n",
      "Iteration: 746 Loss: 0.0006934092808969694\n",
      "Iteration: 747 Loss: 0.000689422470922555\n",
      "Iteration: 748 Loss: 0.0006854731578584581\n",
      "Iteration: 749 Loss: 0.0006815609931224907\n",
      "Iteration: 750 Loss: 0.0006776856311598503\n",
      "Iteration: 751 Loss: 0.0006738467294136388\n",
      "Iteration: 752 Loss: 0.0006700439482972715\n",
      "Iteration: 753 Loss: 0.0006662769511663429\n",
      "Iteration: 754 Loss: 0.0006625454042898189\n",
      "Iteration: 755 Loss: 0.0006588489768229042\n",
      "Iteration: 756 Loss: 0.0006551873407798643\n",
      "Iteration: 757 Loss: 0.0006515601710057727\n",
      "Iteration: 758 Loss: 0.0006479671451496481\n",
      "Iteration: 759 Loss: 0.0006444079436384183\n",
      "Iteration: 760 Loss: 0.0006408822496488856\n",
      "Iteration: 761 Loss: 0.0006373897490812129\n",
      "Iteration: 762 Loss: 0.0006339301305336754\n",
      "Iteration: 763 Loss: 0.0006305030852760969\n",
      "Iteration: 764 Loss: 0.0006271083072226056\n",
      "Iteration: 765 Loss: 0.0006237454929083983\n",
      "Iteration: 766 Loss: 0.00062041434146227\n",
      "Iteration: 767 Loss: 0.0006171145545822832\n",
      "Iteration: 768 Loss: 0.0006138458365101584\n",
      "Iteration: 769 Loss: 0.0006106078940071118\n",
      "Iteration: 770 Loss: 0.0006074004363291979\n",
      "Iteration: 771 Loss: 0.0006042231752026748\n",
      "Iteration: 772 Loss: 0.0006010758247999231\n",
      "Iteration: 773 Loss: 0.0005979581017159363\n",
      "Iteration: 774 Loss: 0.0005948697249447352\n",
      "Iteration: 775 Loss: 0.0005918104158551007\n",
      "Iteration: 776 Loss: 0.000588779898169122\n",
      "Iteration: 777 Loss: 0.0005857778979376716\n",
      "Iteration: 778 Loss: 0.0005828041435192137\n",
      "Iteration: 779 Loss: 0.0005798583655563827\n",
      "Iteration: 780 Loss: 0.0005769402969548707\n",
      "Iteration: 781 Loss: 0.0005740496728603333\n",
      "Iteration: 782 Loss: 0.0005711862306386892\n",
      "Iteration: 783 Loss: 0.0005683497098529621\n",
      "Iteration: 784 Loss: 0.0005655398522436229\n",
      "Iteration: 785 Loss: 0.0005627564017074048\n",
      "Iteration: 786 Loss: 0.0005599991042767922\n",
      "Iteration: 787 Loss: 0.0005572677080999155\n",
      "Iteration: 788 Loss: 0.0005545619634210627\n",
      "Iteration: 789 Loss: 0.0005518816225604501\n",
      "Iteration: 790 Loss: 0.0005492264398950677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 791 Loss: 0.0005465961718402278\n",
      "Iteration: 792 Loss: 0.0005439905768302484\n",
      "Iteration: 793 Loss: 0.0005414094152998539\n",
      "Iteration: 794 Loss: 0.0005388524496669892\n",
      "Iteration: 795 Loss: 0.0005363194443139018\n",
      "Iteration: 796 Loss: 0.0005338101655702483\n",
      "Iteration: 797 Loss: 0.0005313243816953653\n",
      "Iteration: 798 Loss: 0.0005288618628614111\n",
      "Iteration: 799 Loss: 0.0005264223811371598\n",
      "Iteration: 800 Loss: 0.000524005710470778\n",
      "Iteration: 801 Loss: 0.0005216116266742706\n",
      "Iteration: 802 Loss: 0.0005192399074075891\n",
      "Iteration: 803 Loss: 0.0005168903321632314\n",
      "Iteration: 804 Loss: 0.0005145626822504313\n",
      "Iteration: 805 Loss: 0.0005122567407809635\n",
      "Iteration: 806 Loss: 0.0005099722926535215\n",
      "Iteration: 807 Loss: 0.0005077091245406567\n",
      "Iteration: 808 Loss: 0.0005054670248731286\n",
      "Iteration: 809 Loss: 0.0005032457838279512\n",
      "Iteration: 810 Loss: 0.0005010451933126387\n",
      "Iteration: 811 Loss: 0.0004988650469544408\n",
      "Iteration: 812 Loss: 0.0004967051400846261\n",
      "Iteration: 813 Loss: 0.0004945652697277235\n",
      "Iteration: 814 Loss: 0.0004924452345885441\n",
      "Iteration: 815 Loss: 0.0004903448350396338\n",
      "Iteration: 816 Loss: 0.0004882638731098944\n",
      "Iteration: 817 Loss: 0.00048620215247198635\n",
      "Iteration: 818 Loss: 0.00048415947843206654\n",
      "Iteration: 819 Loss: 0.00048213565791751075\n",
      "Iteration: 820 Loss: 0.0004801304994665485\n",
      "Iteration: 821 Loss: 0.0004781438132172197\n",
      "Iteration: 822 Loss: 0.0004761754108968452\n",
      "Iteration: 823 Loss: 0.0004742251058124406\n",
      "Iteration: 824 Loss: 0.0004722927128395678\n",
      "Iteration: 825 Loss: 0.0004703780484133557\n",
      "Iteration: 826 Loss: 0.00046848093051816227\n",
      "Iteration: 827 Loss: 0.0004666011786784855\n",
      "Iteration: 828 Loss: 0.00046473861394954135\n",
      "Iteration: 829 Loss: 0.0004628930589085175\n",
      "Iteration: 830 Loss: 0.0004610643376453897\n",
      "Iteration: 831 Loss: 0.0004592522757542533\n",
      "Iteration: 832 Loss: 0.00045745670032423607\n",
      "Iteration: 833 Loss: 0.0004556774399326002\n",
      "Iteration: 834 Loss: 0.00045391432463444924\n",
      "Iteration: 835 Loss: 0.0004521671859565835\n",
      "Iteration: 836 Loss: 0.000450435856887926\n",
      "Iteration: 837 Loss: 0.0004487201718732351\n",
      "Iteration: 838 Loss: 0.00044701996680358136\n",
      "Iteration: 839 Loss: 0.0004453350790110665\n",
      "Iteration: 840 Loss: 0.00044366534725899244\n",
      "Iteration: 841 Loss: 0.0004420106117362783\n",
      "Iteration: 842 Loss: 0.00044037071404908287\n",
      "Iteration: 843 Loss: 0.00043874549721452285\n",
      "Iteration: 844 Loss: 0.0004371348056526717\n",
      "Iteration: 845 Loss: 0.00043553848518063006\n",
      "Iteration: 846 Loss: 0.00043395638300442656\n",
      "Iteration: 847 Loss: 0.00043238834771338117\n",
      "Iteration: 848 Loss: 0.0004308342292726186\n",
      "Iteration: 849 Loss: 0.00042929387901605143\n",
      "Iteration: 850 Loss: 0.00042776714964020025\n",
      "Iteration: 851 Loss: 0.0004262538951970864\n",
      "Iteration: 852 Loss: 0.00042475397108879915\n",
      "Iteration: 853 Loss: 0.000423267234059351\n",
      "Iteration: 854 Loss: 0.00042179354218872193\n",
      "Iteration: 855 Loss: 0.00042033275488690254\n",
      "Iteration: 856 Loss: 0.0004188847328866924\n",
      "Iteration: 857 Loss: 0.000417449338237273\n",
      "Iteration: 858 Loss: 0.00041602643429760824\n",
      "Iteration: 859 Loss: 0.00041461588573075666\n",
      "Iteration: 860 Loss: 0.00041321755849556404\n",
      "Iteration: 861 Loss: 0.00041183131984295175\n",
      "Iteration: 862 Loss: 0.0004104570383063558\n",
      "Iteration: 863 Loss: 0.00040909458369751343\n",
      "Iteration: 864 Loss: 0.0004077438270984126\n",
      "Iteration: 865 Loss: 0.000406404640855159\n",
      "Iteration: 866 Loss: 0.0004050768985716017\n",
      "Iteration: 867 Loss: 0.0004037604751026577\n",
      "Iteration: 868 Loss: 0.00040245524654738935\n",
      "Iteration: 869 Loss: 0.00040116109024237366\n",
      "Iteration: 870 Loss: 0.00039987788475468\n",
      "Iteration: 871 Loss: 0.00039860550987548413\n",
      "Iteration: 872 Loss: 0.00039734384661320904\n",
      "Iteration: 873 Loss: 0.0003960927771860232\n",
      "Iteration: 874 Loss: 0.0003948521850161934\n",
      "Iteration: 875 Loss: 0.0003936219547214283\n",
      "Iteration: 876 Loss: 0.00039240197210912194\n",
      "Iteration: 877 Loss: 0.00039119212416892547\n",
      "Iteration: 878 Loss: 0.00038999229906515955\n",
      "Iteration: 879 Loss: 0.0003888023861304158\n",
      "Iteration: 880 Loss: 0.0003876222758576307\n",
      "Iteration: 881 Loss: 0.0003864518598931496\n",
      "Iteration: 882 Loss: 0.00038529103102927725\n",
      "Iteration: 883 Loss: 0.00038413968319675934\n",
      "Iteration: 884 Loss: 0.00038299771145799277\n",
      "Iteration: 885 Loss: 0.00038186501199856057\n",
      "Iteration: 886 Loss: 0.0003807414821199079\n",
      "Iteration: 887 Loss: 0.00037962702023253055\n",
      "Iteration: 888 Loss: 0.00037852152584731414\n",
      "Iteration: 889 Loss: 0.000377424899568285\n",
      "Iteration: 890 Loss: 0.0003763370430848769\n",
      "Iteration: 891 Loss: 0.00037525785916423177\n",
      "Iteration: 892 Loss: 0.0003741872516427725\n",
      "Iteration: 893 Loss: 0.0003731251254187447\n",
      "Iteration: 894 Loss: 0.0003720713864444964\n",
      "Iteration: 895 Loss: 0.0003710259417183543\n",
      "Iteration: 896 Loss: 0.0003699886992759503\n",
      "Iteration: 897 Loss: 0.0003689595681825605\n",
      "Iteration: 898 Loss: 0.00036793845852545827\n",
      "Iteration: 899 Loss: 0.00036692528140565023\n",
      "Iteration: 900 Loss: 0.0003659199489289732\n",
      "Iteration: 901 Loss: 0.0003649223741986932\n",
      "Iteration: 902 Loss: 0.0003639324713064305\n",
      "Iteration: 903 Loss: 0.00036295015532515455\n",
      "Iteration: 904 Loss: 0.0003619753422994451\n",
      "Iteration: 905 Loss: 0.00036100794923827424\n",
      "Iteration: 906 Loss: 0.00036004789410616183\n",
      "Iteration: 907 Loss: 0.0003590950958148924\n",
      "Iteration: 908 Loss: 0.0003581494742150387\n",
      "Iteration: 909 Loss: 0.0003572109500877055\n",
      "Iteration: 910 Loss: 0.00035627944513647085\n",
      "Iteration: 911 Loss: 0.00035535488197839093\n",
      "Iteration: 912 Loss: 0.0003544371841355765\n",
      "Iteration: 913 Loss: 0.0003535262760272854\n",
      "Iteration: 914 Loss: 0.00035262208296117924\n",
      "Iteration: 915 Loss: 0.00035172453112436225\n",
      "Iteration: 916 Loss: 0.0003508335475760161\n",
      "Iteration: 917 Loss: 0.0003499490602385927\n",
      "Iteration: 918 Loss: 0.0003490709978890409\n",
      "Iteration: 919 Loss: 0.00034819929015036325\n",
      "Iteration: 920 Loss: 0.0003473338674835824\n",
      "Iteration: 921 Loss: 0.0003464746611795178\n",
      "Iteration: 922 Loss: 0.0003456216033492112\n",
      "Iteration: 923 Loss: 0.00034477462691721834\n",
      "Iteration: 924 Loss: 0.0003439336656122624\n",
      "Iteration: 925 Loss: 0.0003430986539593119\n",
      "Iteration: 926 Loss: 0.000342269527270412\n",
      "Iteration: 927 Loss: 0.0003414462216378192\n",
      "Iteration: 928 Loss: 0.0003406286739245306\n",
      "Iteration: 929 Loss: 0.000339816821756948\n",
      "Iteration: 930 Loss: 0.0003390106035155429\n",
      "Iteration: 931 Loss: 0.00033820995832830025\n",
      "Iteration: 932 Loss: 0.00033741482606068304\n",
      "Iteration: 933 Loss: 0.0003366251473096837\n",
      "Iteration: 934 Loss: 0.0003358408633935699\n",
      "Iteration: 935 Loss: 0.00033506191634549324\n",
      "Iteration: 936 Loss: 0.00033428824890473625\n",
      "Iteration: 937 Loss: 0.00033351980450911547\n",
      "Iteration: 938 Loss: 0.00033275652728639664\n",
      "Iteration: 939 Loss: 0.00033199836204749573\n",
      "Iteration: 940 Loss: 0.00033124525427799474\n",
      "Iteration: 941 Loss: 0.00033049715013057547\n",
      "Iteration: 942 Loss: 0.0003297539964171658\n",
      "Iteration: 943 Loss: 0.0003290157406011522\n",
      "Iteration: 944 Loss: 0.0003282823307898123\n",
      "Iteration: 945 Loss: 0.00032755371572791347\n",
      "Iteration: 946 Loss: 0.00032682984478840936\n",
      "Iteration: 947 Loss: 0.0003261106679654842\n",
      "Iteration: 948 Loss: 0.00032539613586764286\n",
      "Iteration: 949 Loss: 0.00032468619971069826\n",
      "Iteration: 950 Loss: 0.0003239808113090577\n",
      "Iteration: 951 Loss: 0.0003232799230700363\n",
      "Iteration: 952 Loss: 0.0003225834879855774\n",
      "Iteration: 953 Loss: 0.00032189145962583515\n",
      "Iteration: 954 Loss: 0.00032120379213128103\n",
      "Iteration: 955 Loss: 0.0003205204402069019\n",
      "Iteration: 956 Loss: 0.00031984135911383\n",
      "Iteration: 957 Loss: 0.00031916650466385217\n",
      "Iteration: 958 Loss: 0.00031849583321168814\n",
      "Iteration: 959 Loss: 0.0003178293016485743\n",
      "Iteration: 960 Loss: 0.00031716686739566465\n",
      "Iteration: 961 Loss: 0.000316508488396736\n",
      "Iteration: 962 Loss: 0.000315854123112272\n",
      "Iteration: 963 Loss: 0.0003152037305133658\n",
      "Iteration: 964 Loss: 0.00031455727007392495\n",
      "Iteration: 965 Loss: 0.00031391470176530916\n",
      "Iteration: 966 Loss: 0.0003132759860497915\n",
      "Iteration: 967 Loss: 0.0003126410838745407\n",
      "Iteration: 968 Loss: 0.0003120099566646917\n",
      "Iteration: 969 Loss: 0.00031138256631801085\n",
      "Iteration: 970 Loss: 0.00031075887519829926\n",
      "Iteration: 971 Loss: 0.0003101388461299124\n",
      "Iteration: 972 Loss: 0.0003095224423911903\n",
      "Iteration: 973 Loss: 0.0003089096277091149\n",
      "Iteration: 974 Loss: 0.0003083003662531213\n",
      "Iteration: 975 Loss: 0.0003076946226296439\n",
      "Iteration: 976 Loss: 0.0003070923618767346\n",
      "Iteration: 977 Loss: 0.00030649354945701577\n",
      "Iteration: 978 Loss: 0.000305898151254194\n",
      "Iteration: 979 Loss: 0.00030530613356620533\n",
      "Iteration: 980 Loss: 0.000304717463100262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 981 Loss: 0.0003041321069669449\n",
      "Iteration: 982 Loss: 0.0003035500326755229\n",
      "Iteration: 983 Loss: 0.00030297120812860135\n",
      "Iteration: 984 Loss: 0.00030239560161673853\n",
      "Iteration: 985 Loss: 0.00030182318181365273\n",
      "Iteration: 986 Loss: 0.0003012539177705723\n",
      "Iteration: 987 Loss: 0.0003006877789121563\n",
      "Iteration: 988 Loss: 0.00030012473503057604\n",
      "Iteration: 989 Loss: 0.00029956475628154476\n",
      "Iteration: 990 Loss: 0.00029900781317875334\n",
      "Iteration: 991 Loss: 0.0002984538765898215\n",
      "Iteration: 992 Loss: 0.0002979029177303047\n",
      "Iteration: 993 Loss: 0.0002973549081614561\n",
      "Iteration: 994 Loss: 0.00029680981978298074\n",
      "Iteration: 995 Loss: 0.0002962676248304738\n",
      "Iteration: 996 Loss: 0.0002957282958699581\n",
      "Iteration: 997 Loss: 0.0002951918057933897\n",
      "Iteration: 998 Loss: 0.0002946581278150947\n",
      "Iteration: 999 Loss: 0.0002941272354670607\n",
      "Iteration: 1000 Loss: 0.00029359910259442294\n",
      "Iteration: 1001 Loss: 0.0002930737033521007\n",
      "Iteration: 1002 Loss: 0.000292551012199625\n",
      "Iteration: 1003 Loss: 0.0002920310038976089\n",
      "Iteration: 1004 Loss: 0.0002915136535042105\n",
      "Iteration: 1005 Loss: 0.00029099893637031944\n",
      "Iteration: 1006 Loss: 0.00029048682813594183\n",
      "Iteration: 1007 Loss: 0.0002899773047263638\n",
      "Iteration: 1008 Loss: 0.0002894703423489257\n",
      "Iteration: 1009 Loss: 0.0002889659174883683\n",
      "Iteration: 1010 Loss: 0.0002884640069031775\n",
      "Iteration: 1011 Loss: 0.00028796458762260583\n",
      "Iteration: 1012 Loss: 0.0002874676369427208\n",
      "Iteration: 1013 Loss: 0.00028697313242303303\n",
      "Iteration: 1014 Loss: 0.0002864810518825782\n",
      "Iteration: 1015 Loss: 0.00028599137339677463\n",
      "Iteration: 1016 Loss: 0.0002855040752935485\n",
      "Iteration: 1017 Loss: 0.00028501913615102087\n",
      "Iteration: 1018 Loss: 0.00028453653479282727\n",
      "Iteration: 1019 Loss: 0.00028405625028613587\n",
      "Iteration: 1020 Loss: 0.00028357826193744096\n",
      "Iteration: 1021 Loss: 0.00028310254928971866\n",
      "Iteration: 1022 Loss: 0.0002826290921199235\n",
      "Iteration: 1023 Loss: 0.00028215787043474214\n",
      "Iteration: 1024 Loss: 0.00028168886446830146\n",
      "Iteration: 1025 Loss: 0.00028122205467850394\n",
      "Iteration: 1026 Loss: 0.0002807574217456455\n",
      "Iteration: 1027 Loss: 0.00028029494656718954\n",
      "Iteration: 1028 Loss: 0.00027983461025665745\n",
      "Iteration: 1029 Loss: 0.0002793763941401689\n",
      "Iteration: 1030 Loss: 0.0002789202797537823\n",
      "Iteration: 1031 Loss: 0.00027846624884026756\n",
      "Iteration: 1032 Loss: 0.00027801428334726274\n",
      "Iteration: 1033 Loss: 0.0002775643654232882\n",
      "Iteration: 1034 Loss: 0.00027711647741649414\n",
      "Iteration: 1035 Loss: 0.0002766706018712296\n",
      "Iteration: 1036 Loss: 0.0002762267215258857\n",
      "Iteration: 1037 Loss: 0.0002757848193096911\n",
      "Iteration: 1038 Loss: 0.0002753448783408149\n",
      "Iteration: 1039 Loss: 0.00027490688192423194\n",
      "Iteration: 1040 Loss: 0.00027447081354809\n",
      "Iteration: 1041 Loss: 0.0002740366568828212\n",
      "Iteration: 1042 Loss: 0.000273604395777299\n",
      "Iteration: 1043 Loss: 0.0002731740142575364\n",
      "Iteration: 1044 Loss: 0.0002727454965242435\n",
      "Iteration: 1045 Loss: 0.0002723188269502387\n",
      "Iteration: 1046 Loss: 0.00027189399007812213\n",
      "Iteration: 1047 Loss: 0.0002714709706188346\n",
      "Iteration: 1048 Loss: 0.0002710497534483011\n",
      "Iteration: 1049 Loss: 0.0002706303236066737\n",
      "Iteration: 1050 Loss: 0.00027021266629534304\n",
      "Iteration: 1051 Loss: 0.00026979676687517056\n",
      "Iteration: 1052 Loss: 0.00026938261086398025\n",
      "Iteration: 1053 Loss: 0.00026897018393502053\n",
      "Iteration: 1054 Loss: 0.0002685594719152497\n",
      "Iteration: 1055 Loss: 0.0002681504607824427\n",
      "Iteration: 1056 Loss: 0.00026774313666426733\n",
      "Iteration: 1057 Loss: 0.00026733748583552755\n",
      "Iteration: 1058 Loss: 0.0002669334947168336\n",
      "Iteration: 1059 Loss: 0.00026653114987310527\n",
      "Iteration: 1060 Loss: 0.0002661304380105894\n",
      "Iteration: 1061 Loss: 0.00026573134597601\n",
      "Iteration: 1062 Loss: 0.00026533386075471927\n",
      "Iteration: 1063 Loss: 0.0002649379694682511\n",
      "Iteration: 1064 Loss: 0.00026454365937329664\n",
      "Iteration: 1065 Loss: 0.00026415091786032084\n",
      "Iteration: 1066 Loss: 0.00026375973245083487\n",
      "Iteration: 1067 Loss: 0.0002633700907963432\n",
      "Iteration: 1068 Loss: 0.00026298198067712536\n",
      "Iteration: 1069 Loss: 0.0002625953899998476\n",
      "Iteration: 1070 Loss: 0.0002622103067963049\n",
      "Iteration: 1071 Loss: 0.00026182671922203934\n",
      "Iteration: 1072 Loss: 0.0002614446155549119\n",
      "Iteration: 1073 Loss: 0.00026106398419263274\n",
      "Iteration: 1074 Loss: 0.0002606848136529297\n",
      "Iteration: 1075 Loss: 0.0002603070925705012\n",
      "Iteration: 1076 Loss: 0.0002599308096961375\n",
      "Iteration: 1077 Loss: 0.00025955595389564473\n",
      "Iteration: 1078 Loss: 0.0002591825141481066\n",
      "Iteration: 1079 Loss: 0.00025881047954491907\n",
      "Iteration: 1080 Loss: 0.00025843983928745134\n",
      "Iteration: 1081 Loss: 0.0002580705826869609\n",
      "Iteration: 1082 Loss: 0.00025770269916174007\n",
      "Iteration: 1083 Loss: 0.0002573361782376377\n",
      "Iteration: 1084 Loss: 0.00025697100954477387\n",
      "Iteration: 1085 Loss: 0.00025660718281835876\n",
      "Iteration: 1086 Loss: 0.00025624468789568923\n",
      "Iteration: 1087 Loss: 0.00025588351471614437\n",
      "Iteration: 1088 Loss: 0.0002555236533187617\n",
      "Iteration: 1089 Loss: 0.00025516509384259576\n",
      "Iteration: 1090 Loss: 0.00025480782652375435\n",
      "Iteration: 1091 Loss: 0.00025445184169555217\n",
      "Iteration: 1092 Loss: 0.00025409712978683363\n",
      "Iteration: 1093 Loss: 0.0002537436813211154\n",
      "Iteration: 1094 Loss: 0.0002533914869147478\n",
      "Iteration: 1095 Loss: 0.0002530405372771488\n",
      "Iteration: 1096 Loss: 0.0002526908232082867\n",
      "Iteration: 1097 Loss: 0.0002523423355985013\n",
      "Iteration: 1098 Loss: 0.0002519950654271543\n",
      "Iteration: 1099 Loss: 0.0002516490037611276\n",
      "Iteration: 1100 Loss: 0.00025130414175515546\n",
      "Iteration: 1101 Loss: 0.0002509604706490907\n",
      "Iteration: 1102 Loss: 0.00025061798176817464\n",
      "Iteration: 1103 Loss: 0.0002502766665213588\n",
      "Iteration: 1104 Loss: 0.0002499365164010003\n",
      "Iteration: 1105 Loss: 0.00024959752298111686\n",
      "Iteration: 1106 Loss: 0.00024925967791658544\n",
      "Iteration: 1107 Loss: 0.00024892297294281605\n",
      "Iteration: 1108 Loss: 0.0002485873998744535\n",
      "Iteration: 1109 Loss: 0.00024825295060469345\n",
      "Iteration: 1110 Loss: 0.00024791961710316515\n",
      "Iteration: 1111 Loss: 0.00024758739141663924\n",
      "Iteration: 1112 Loss: 0.0002472562656676397\n",
      "Iteration: 1113 Loss: 0.00024692623205391733\n",
      "Iteration: 1114 Loss: 0.00024659728284612183\n",
      "Iteration: 1115 Loss: 0.0002462694103886414\n",
      "Iteration: 1116 Loss: 0.00024594260709789065\n",
      "Iteration: 1117 Loss: 0.000245616865461985\n",
      "Iteration: 1118 Loss: 0.00024529217803939755\n",
      "Iteration: 1119 Loss: 0.0002449685374589622\n",
      "Iteration: 1120 Loss: 0.0002446459364181556\n",
      "Iteration: 1121 Loss: 0.00024432436768273113\n",
      "Iteration: 1122 Loss: 0.00024400382408624818\n",
      "Iteration: 1123 Loss: 0.00024368429852894522\n",
      "Iteration: 1124 Loss: 0.00024336578397713356\n",
      "Iteration: 1125 Loss: 0.00024304827346259268\n",
      "Iteration: 1126 Loss: 0.00024273176008152573\n",
      "Iteration: 1127 Loss: 0.0002424162369937372\n",
      "Iteration: 1128 Loss: 0.00024210169742300202\n",
      "Iteration: 1129 Loss: 0.00024178813465525675\n",
      "Iteration: 1130 Loss: 0.00024147554203753866\n",
      "Iteration: 1131 Loss: 0.00024116391297885115\n",
      "Iteration: 1132 Loss: 0.00024085324094831883\n",
      "Iteration: 1133 Loss: 0.0002405435194749982\n",
      "Iteration: 1134 Loss: 0.0002402347421472855\n",
      "Iteration: 1135 Loss: 0.0002399269026115549\n",
      "Iteration: 1136 Loss: 0.00023961999457244512\n",
      "Iteration: 1137 Loss: 0.00023931401179165833\n",
      "Iteration: 1138 Loss: 0.00023900894808793924\n",
      "Iteration: 1139 Loss: 0.0002387047973356744\n",
      "Iteration: 1140 Loss: 0.0002384015534648103\n",
      "Iteration: 1141 Loss: 0.0002380992104601544\n",
      "Iteration: 1142 Loss: 0.0002377977623604347\n",
      "Iteration: 1143 Loss: 0.0002374972032586459\n",
      "Iteration: 1144 Loss: 0.00023719752730046634\n",
      "Iteration: 1145 Loss: 0.00023689872868432946\n",
      "Iteration: 1146 Loss: 0.00023660080166027902\n",
      "Iteration: 1147 Loss: 0.00023630374053046113\n",
      "Iteration: 1148 Loss: 0.00023600753964714207\n",
      "Iteration: 1149 Loss: 0.00023571219341374076\n",
      "Iteration: 1150 Loss: 0.00023541769628304272\n",
      "Iteration: 1151 Loss: 0.00023512404275719917\n",
      "Iteration: 1152 Loss: 0.00023483122738740094\n",
      "Iteration: 1153 Loss: 0.00023453924477291027\n",
      "Iteration: 1154 Loss: 0.00023424808956058125\n",
      "Iteration: 1155 Loss: 0.00023395775644455592\n",
      "Iteration: 1156 Loss: 0.00023366824016648433\n",
      "Iteration: 1157 Loss: 0.00023337953551354646\n",
      "Iteration: 1158 Loss: 0.00023309163731907725\n",
      "Iteration: 1159 Loss: 0.00023280454046213804\n",
      "Iteration: 1160 Loss: 0.0002325182398658537\n",
      "Iteration: 1161 Loss: 0.00023223273049867847\n",
      "Iteration: 1162 Loss: 0.00023194800737266403\n",
      "Iteration: 1163 Loss: 0.00023166406554294436\n",
      "Iteration: 1164 Loss: 0.00023138090010857394\n",
      "Iteration: 1165 Loss: 0.0002310985062107566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1166 Loss: 0.00023081687903309667\n",
      "Iteration: 1167 Loss: 0.00023053601380108163\n",
      "Iteration: 1168 Loss: 0.0002302559057814464\n",
      "Iteration: 1169 Loss: 0.00022997655028229684\n",
      "Iteration: 1170 Loss: 0.00022969794265133658\n",
      "Iteration: 1171 Loss: 0.00022942007827741264\n",
      "Iteration: 1172 Loss: 0.00022914295258868314\n",
      "Iteration: 1173 Loss: 0.00022886656105263928\n",
      "Iteration: 1174 Loss: 0.00022859089917566798\n",
      "Iteration: 1175 Loss: 0.00022831596250289093\n",
      "Iteration: 1176 Loss: 0.00022804174661807082\n",
      "Iteration: 1177 Loss: 0.00022776824714194606\n",
      "Iteration: 1178 Loss: 0.00022749545973288024\n",
      "Iteration: 1179 Loss: 0.00022722338008679158\n",
      "Iteration: 1180 Loss: 0.00022695200393588752\n",
      "Iteration: 1181 Loss: 0.00022668132704861156\n",
      "Iteration: 1182 Loss: 0.0002264113452297152\n",
      "Iteration: 1183 Loss: 0.00022614205431940092\n",
      "Iteration: 1184 Loss: 0.00022587345019313431\n",
      "Iteration: 1185 Loss: 0.00022560552876206224\n",
      "Iteration: 1186 Loss: 0.00022533828597080108\n",
      "Iteration: 1187 Loss: 0.0002250717177987803\n",
      "Iteration: 1188 Loss: 0.00022480582025911273\n",
      "Iteration: 1189 Loss: 0.00022454058939905424\n",
      "Iteration: 1190 Loss: 0.00022427602129869258\n",
      "Iteration: 1191 Loss: 0.00022401211207107537\n",
      "Iteration: 1192 Loss: 0.00022374885786232598\n",
      "Iteration: 1193 Loss: 0.00022348625485071267\n",
      "Iteration: 1194 Loss: 0.00022322429924600027\n",
      "Iteration: 1195 Loss: 0.00022296298729030075\n",
      "Iteration: 1196 Loss: 0.00022270231525712878\n",
      "Iteration: 1197 Loss: 0.00022244227945058693\n",
      "Iteration: 1198 Loss: 0.00022218287620618464\n",
      "Iteration: 1199 Loss: 0.00022192410188999096\n",
      "Iteration: 1200 Loss: 0.00022166595289773922\n",
      "Iteration: 1201 Loss: 0.00022140842565557964\n",
      "Iteration: 1202 Loss: 0.00022115151661916018\n",
      "Iteration: 1203 Loss: 0.0002208952222733367\n",
      "Iteration: 1204 Loss: 0.00022063953913253052\n",
      "Iteration: 1205 Loss: 0.0002203844637396767\n",
      "Iteration: 1206 Loss: 0.0002201299926665861\n",
      "Iteration: 1207 Loss: 0.00021987612251340438\n",
      "Iteration: 1208 Loss: 0.0002196228499080454\n",
      "Iteration: 1209 Loss: 0.00021937017150619586\n",
      "Iteration: 1210 Loss: 0.00021911808399125224\n",
      "Iteration: 1211 Loss: 0.00021886658407388343\n",
      "Iteration: 1212 Loss: 0.00021861566849164314\n",
      "Iteration: 1213 Loss: 0.0002183653340086441\n",
      "Iteration: 1214 Loss: 0.00021811557741606733\n",
      "Iteration: 1215 Loss: 0.00021786639553129767\n",
      "Iteration: 1216 Loss: 0.00021761778519739376\n",
      "Iteration: 1217 Loss: 0.0002173697432835941\n",
      "Iteration: 1218 Loss: 0.00021712226668425317\n",
      "Iteration: 1219 Loss: 0.00021687535231975212\n",
      "Iteration: 1220 Loss: 0.0002166289971349373\n",
      "Iteration: 1221 Loss: 0.0002163831980997554\n",
      "Iteration: 1222 Loss: 0.00021613795220887702\n",
      "Iteration: 1223 Loss: 0.00021589325648125308\n",
      "Iteration: 1224 Loss: 0.0002156491079600276\n",
      "Iteration: 1225 Loss: 0.00021540550371246733\n",
      "Iteration: 1226 Loss: 0.00021516244082972068\n",
      "Iteration: 1227 Loss: 0.0002149199164262332\n",
      "Iteration: 1228 Loss: 0.0002146779276397764\n",
      "Iteration: 1229 Loss: 0.00021443647163167873\n",
      "Iteration: 1230 Loss: 0.00021419554558554056\n",
      "Iteration: 1231 Loss: 0.0002139551467082954\n",
      "Iteration: 1232 Loss: 0.00021371527222900424\n",
      "Iteration: 1233 Loss: 0.00021347591939941223\n",
      "Iteration: 1234 Loss: 0.00021323708549297402\n",
      "Iteration: 1235 Loss: 0.00021299876780524324\n",
      "Iteration: 1236 Loss: 0.00021276096365386646\n",
      "Iteration: 1237 Loss: 0.00021252367037739379\n",
      "Iteration: 1238 Loss: 0.0002122868853362357\n",
      "Iteration: 1239 Loss: 0.00021205060591137314\n",
      "Iteration: 1240 Loss: 0.0002118148295055692\n",
      "Iteration: 1241 Loss: 0.00021157955354138578\n",
      "Iteration: 1242 Loss: 0.0002113447754629784\n",
      "Iteration: 1243 Loss: 0.00021111049273467032\n",
      "Iteration: 1244 Loss: 0.00021087670284054437\n",
      "Iteration: 1245 Loss: 0.00021064340328546649\n",
      "Iteration: 1246 Loss: 0.00021041059159330642\n",
      "Iteration: 1247 Loss: 0.0002101782653085812\n",
      "Iteration: 1248 Loss: 0.00020994642199511225\n",
      "Iteration: 1249 Loss: 0.0002097150592354607\n",
      "Iteration: 1250 Loss: 0.00020948417463201077\n",
      "Iteration: 1251 Loss: 0.0002092537658059893\n",
      "Iteration: 1252 Loss: 0.0002090238303979544\n",
      "Iteration: 1253 Loss: 0.00020879436606644028\n",
      "Iteration: 1254 Loss: 0.00020856537048927288\n",
      "Iteration: 1255 Loss: 0.00020833684136227436\n",
      "Iteration: 1256 Loss: 0.0002081087763996507\n",
      "Iteration: 1257 Loss: 0.00020788117333332977\n",
      "Iteration: 1258 Loss: 0.00020765402991364517\n",
      "Iteration: 1259 Loss: 0.00020742734390862547\n",
      "Iteration: 1260 Loss: 0.00020720111310368515\n",
      "Iteration: 1261 Loss: 0.0002069753353018862\n",
      "Iteration: 1262 Loss: 0.00020675000832378742\n",
      "Iteration: 1263 Loss: 0.00020652513000676598\n",
      "Iteration: 1264 Loss: 0.0002063006982054702\n",
      "Iteration: 1265 Loss: 0.00020607671079142628\n",
      "Iteration: 1266 Loss: 0.00020585316565301145\n",
      "Iteration: 1267 Loss: 0.00020563006069502645\n",
      "Iteration: 1268 Loss: 0.0002054073938388984\n",
      "Iteration: 1269 Loss: 0.00020518516302195816\n",
      "Iteration: 1270 Loss: 0.0002049633661983812\n",
      "Iteration: 1271 Loss: 0.0002047420013381666\n",
      "Iteration: 1272 Loss: 0.0002045210664271299\n",
      "Iteration: 1273 Loss: 0.00020430055946683276\n",
      "Iteration: 1274 Loss: 0.00020408047847433544\n",
      "Iteration: 1275 Loss: 0.00020386082148303054\n",
      "Iteration: 1276 Loss: 0.00020364158654094815\n",
      "Iteration: 1277 Loss: 0.0002034227717116537\n",
      "Iteration: 1278 Loss: 0.0002032043750738654\n",
      "Iteration: 1279 Loss: 0.00020298639472096639\n",
      "Iteration: 1280 Loss: 0.00020276882876206135\n",
      "Iteration: 1281 Loss: 0.00020255167532040063\n",
      "Iteration: 1282 Loss: 0.00020233493253387272\n",
      "Iteration: 1283 Loss: 0.00020211859855505133\n",
      "Iteration: 1284 Loss: 0.0002019026715511396\n",
      "Iteration: 1285 Loss: 0.00020168714970307902\n",
      "Iteration: 1286 Loss: 0.00020147203120680319\n",
      "Iteration: 1287 Loss: 0.0002012573142714437\n",
      "Iteration: 1288 Loss: 0.00020104299712058103\n",
      "Iteration: 1289 Loss: 0.0002008290779918777\n",
      "Iteration: 1290 Loss: 0.00020061555513621398\n",
      "Iteration: 1291 Loss: 0.00020040242681797357\n",
      "Iteration: 1292 Loss: 0.0002001896913157009\n",
      "Iteration: 1293 Loss: 0.00019997734692065312\n",
      "Iteration: 1294 Loss: 0.0001997653919380759\n",
      "Iteration: 1295 Loss: 0.00019955382468562295\n",
      "Iteration: 1296 Loss: 0.00019934264349471233\n",
      "Iteration: 1297 Loss: 0.00019913184670912478\n",
      "Iteration: 1298 Loss: 0.0001989214326857944\n",
      "Iteration: 1299 Loss: 0.00019871139979458317\n",
      "Iteration: 1300 Loss: 0.0001985017464180871\n",
      "Iteration: 1301 Loss: 0.00019829247095097663\n",
      "Iteration: 1302 Loss: 0.00019808357180049331\n",
      "Iteration: 1303 Loss: 0.00019787504738638388\n",
      "Iteration: 1304 Loss: 0.0001976668961408896\n",
      "Iteration: 1305 Loss: 0.0001974591165077675\n",
      "Iteration: 1306 Loss: 0.00019725170694354427\n",
      "Iteration: 1307 Loss: 0.0001970446659162412\n",
      "Iteration: 1308 Loss: 0.00019683799190619216\n",
      "Iteration: 1309 Loss: 0.00019663168340527197\n",
      "Iteration: 1310 Loss: 0.00019642573891638965\n",
      "Iteration: 1311 Loss: 0.00019622015695541374\n",
      "Iteration: 1312 Loss: 0.0001960149360487986\n",
      "Iteration: 1313 Loss: 0.00019581007473461345\n",
      "Iteration: 1314 Loss: 0.00019560557156214156\n",
      "Iteration: 1315 Loss: 0.00019540142509207699\n",
      "Iteration: 1316 Loss: 0.00019519763389616216\n",
      "Iteration: 1317 Loss: 0.0001949941965572151\n",
      "Iteration: 1318 Loss: 0.0001947911116692406\n",
      "Iteration: 1319 Loss: 0.0001945883778365762\n",
      "Iteration: 1320 Loss: 0.00019438599367513235\n",
      "Iteration: 1321 Loss: 0.00019418395781083662\n",
      "Iteration: 1322 Loss: 0.00019398226888101466\n",
      "Iteration: 1323 Loss: 0.0001937809255328232\n",
      "Iteration: 1324 Loss: 0.00019357992642442308\n",
      "Iteration: 1325 Loss: 0.00019337927022403112\n",
      "Iteration: 1326 Loss: 0.0001931789556101282\n",
      "Iteration: 1327 Loss: 0.0001929789812720179\n",
      "Iteration: 1328 Loss: 0.00019277934590858259\n",
      "Iteration: 1329 Loss: 0.00019258004822872066\n",
      "Iteration: 1330 Loss: 0.00019238108695202654\n",
      "Iteration: 1331 Loss: 0.00019218246080736213\n",
      "Iteration: 1332 Loss: 0.00019198416853377642\n",
      "Iteration: 1333 Loss: 0.00019178620887975702\n",
      "Iteration: 1334 Loss: 0.00019158858060384007\n",
      "Iteration: 1335 Loss: 0.00019139128247422311\n",
      "Iteration: 1336 Loss: 0.00019119431326817502\n",
      "Iteration: 1337 Loss: 0.000190997671773613\n",
      "Iteration: 1338 Loss: 0.00019080135678629034\n",
      "Iteration: 1339 Loss: 0.0001906053671125875\n",
      "Iteration: 1340 Loss: 0.00019040970156735716\n",
      "Iteration: 1341 Loss: 0.00019021435897502364\n",
      "Iteration: 1342 Loss: 0.00019001933816935757\n",
      "Iteration: 1343 Loss: 0.0001898246379925896\n",
      "Iteration: 1344 Loss: 0.00018963025729648323\n",
      "Iteration: 1345 Loss: 0.00018943619494186123\n",
      "Iteration: 1346 Loss: 0.0001892424497978666\n",
      "Iteration: 1347 Loss: 0.00018904902074303606\n",
      "Iteration: 1348 Loss: 0.00018885590666391607\n",
      "Iteration: 1349 Loss: 0.00018866310645645535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1350 Loss: 0.00018847061902460546\n",
      "Iteration: 1351 Loss: 0.00018827844328148403\n",
      "Iteration: 1352 Loss: 0.0001880865781480228\n",
      "Iteration: 1353 Loss: 0.0001878950225544983\n",
      "Iteration: 1354 Loss: 0.00018770377543859512\n",
      "Iteration: 1355 Loss: 0.0001875128357469132\n",
      "Iteration: 1356 Loss: 0.00018732220243371448\n",
      "Iteration: 1357 Loss: 0.00018713187446243028\n",
      "Iteration: 1358 Loss: 0.0001869418508037604\n",
      "Iteration: 1359 Loss: 0.0001867521304367519\n",
      "Iteration: 1360 Loss: 0.00018656271234831833\n",
      "Iteration: 1361 Loss: 0.00018637359553352949\n",
      "Iteration: 1362 Loss: 0.0001861847789953154\n",
      "Iteration: 1363 Loss: 0.00018599626174460007\n",
      "Iteration: 1364 Loss: 0.00018580804279952082\n",
      "Iteration: 1365 Loss: 0.00018562012118650704\n",
      "Iteration: 1366 Loss: 0.0001854324959395392\n",
      "Iteration: 1367 Loss: 0.00018524516610027284\n",
      "Iteration: 1368 Loss: 0.00018505813071763568\n",
      "Iteration: 1369 Loss: 0.0001848713888481177\n",
      "Iteration: 1370 Loss: 0.00018468493955608537\n",
      "Iteration: 1371 Loss: 0.00018449878191273983\n",
      "Iteration: 1372 Loss: 0.00018431291499714636\n",
      "Iteration: 1373 Loss: 0.00018412733789537126\n",
      "Iteration: 1374 Loss: 0.00018394204970107315\n",
      "Iteration: 1375 Loss: 0.00018375704951449417\n",
      "Iteration: 1376 Loss: 0.00018357233644358437\n",
      "Iteration: 1377 Loss: 0.00018338790960326697\n",
      "Iteration: 1378 Loss: 0.00018320376811533016\n",
      "Iteration: 1379 Loss: 0.00018301991110871663\n",
      "Iteration: 1380 Loss: 0.00018283633771937034\n",
      "Iteration: 1381 Loss: 0.00018265304709029479\n",
      "Iteration: 1382 Loss: 0.00018247003837095407\n",
      "Iteration: 1383 Loss: 0.00018228731071788154\n",
      "Iteration: 1384 Loss: 0.00018210486329431387\n",
      "Iteration: 1385 Loss: 0.00018192269527039591\n",
      "Iteration: 1386 Loss: 0.0001817408058229953\n",
      "Iteration: 1387 Loss: 0.00018155919413502525\n",
      "Iteration: 1388 Loss: 0.00018137785939651907\n",
      "Iteration: 1389 Loss: 0.00018119680080388713\n",
      "Iteration: 1390 Loss: 0.00018101601756028552\n",
      "Iteration: 1391 Loss: 0.00018083550887504127\n",
      "Iteration: 1392 Loss: 0.00018065527396415615\n",
      "Iteration: 1393 Loss: 0.0001804753120497148\n",
      "Iteration: 1394 Loss: 0.00018029562236035387\n",
      "Iteration: 1395 Loss: 0.00018011620413097956\n",
      "Iteration: 1396 Loss: 0.00017993705660284944\n",
      "Iteration: 1397 Loss: 0.00017975817902312\n",
      "Iteration: 1398 Loss: 0.00017957957064535426\n",
      "Iteration: 1399 Loss: 0.00017940123072915384\n",
      "Iteration: 1400 Loss: 0.00017922315854043214\n",
      "Iteration: 1401 Loss: 0.00017904535335066353\n",
      "Iteration: 1402 Loss: 0.0001788678144377232\n",
      "Iteration: 1403 Loss: 0.00017869054108565088\n",
      "Iteration: 1404 Loss: 0.00017851353258415027\n",
      "Iteration: 1405 Loss: 0.00017833678822881327\n",
      "Iteration: 1406 Loss: 0.0001781603073210281\n",
      "Iteration: 1407 Loss: 0.00017798408916852785\n",
      "Iteration: 1408 Loss: 0.0001778081330838631\n",
      "Iteration: 1409 Loss: 0.0001776324383864055\n",
      "Iteration: 1410 Loss: 0.00017745700440062435\n",
      "Iteration: 1411 Loss: 0.0001772818304570019\n",
      "Iteration: 1412 Loss: 0.0001771069158914009\n",
      "Iteration: 1413 Loss: 0.0001769322600452851\n",
      "Iteration: 1414 Loss: 0.00017675786226590737\n",
      "Iteration: 1415 Loss: 0.00017658372190587103\n",
      "Iteration: 1416 Loss: 0.00017640983832334356\n",
      "Iteration: 1417 Loss: 0.00017623621088236352\n",
      "Iteration: 1418 Loss: 0.00017606283895200462\n",
      "Iteration: 1419 Loss: 0.00017588972190655112\n",
      "Iteration: 1420 Loss: 0.00017571685912610744\n",
      "Iteration: 1421 Loss: 0.00017554424999602507\n",
      "Iteration: 1422 Loss: 0.00017537189390716672\n",
      "Iteration: 1423 Loss: 0.00017519979025547351\n",
      "Iteration: 1424 Loss: 0.00017502793844199007\n",
      "Iteration: 1425 Loss: 0.00017485633787340477\n",
      "Iteration: 1426 Loss: 0.00017468498796129498\n",
      "Iteration: 1427 Loss: 0.0001745138881222699\n",
      "Iteration: 1428 Loss: 0.00017434303777840582\n",
      "Iteration: 1429 Loss: 0.00017417243635703362\n",
      "Iteration: 1430 Loss: 0.0001740020832898477\n",
      "Iteration: 1431 Loss: 0.00017383197801445387\n",
      "Iteration: 1432 Loss: 0.00017366211997347827\n",
      "Iteration: 1433 Loss: 0.00017349250861364073\n",
      "Iteration: 1434 Loss: 0.00017332314338729435\n",
      "Iteration: 1435 Loss: 0.0001731540237519368\n",
      "Iteration: 1436 Loss: 0.00017298514916927577\n",
      "Iteration: 1437 Loss: 0.00017281651910654974\n",
      "Iteration: 1438 Loss: 0.0001726481330357975\n",
      "Iteration: 1439 Loss: 0.00017247999043337758\n",
      "Iteration: 1440 Loss: 0.00017231209078112306\n",
      "Iteration: 1441 Loss: 0.00017214443356522555\n",
      "Iteration: 1442 Loss: 0.00017197701827698455\n",
      "Iteration: 1443 Loss: 0.00017180984441219165\n",
      "Iteration: 1444 Loss: 0.0001716429114718302\n",
      "Iteration: 1445 Loss: 0.00017147621896056072\n",
      "Iteration: 1446 Loss: 0.0001713097663887491\n",
      "Iteration: 1447 Loss: 0.00017114355327086373\n",
      "Iteration: 1448 Loss: 0.0001709775791262903\n",
      "Iteration: 1449 Loss: 0.0001708118434788866\n",
      "Iteration: 1450 Loss: 0.00017064634585708526\n",
      "Iteration: 1451 Loss: 0.0001704810857939869\n",
      "Iteration: 1452 Loss: 0.00017031606282693452\n",
      "Iteration: 1453 Loss: 0.0001701512764982073\n",
      "Iteration: 1454 Loss: 0.00016998672635438358\n",
      "Iteration: 1455 Loss: 0.0001698224119463457\n",
      "Iteration: 1456 Loss: 0.00016965833283008738\n",
      "Iteration: 1457 Loss: 0.00016949448856512633\n",
      "Iteration: 1458 Loss: 0.00016933087871599857\n",
      "Iteration: 1459 Loss: 0.0001691675028511785\n",
      "Iteration: 1460 Loss: 0.00016900436054414152\n",
      "Iteration: 1461 Loss: 0.0001688414513723017\n",
      "Iteration: 1462 Loss: 0.00016867877491751252\n",
      "Iteration: 1463 Loss: 0.00016851633076621697\n",
      "Iteration: 1464 Loss: 0.00016835411850873677\n",
      "Iteration: 1465 Loss: 0.00016819213773966866\n",
      "Iteration: 1466 Loss: 0.0001680303880582968\n",
      "Iteration: 1467 Loss: 0.00016786886906791447\n",
      "Iteration: 1468 Loss: 0.0001677075803760706\n",
      "Iteration: 1469 Loss: 0.00016754652159435289\n",
      "Iteration: 1470 Loss: 0.00016738569233920136\n",
      "Iteration: 1471 Loss: 0.00016722509223061582\n",
      "Iteration: 1472 Loss: 0.00016706472089277744\n",
      "Iteration: 1473 Loss: 0.00016690457795402028\n",
      "Iteration: 1474 Loss: 0.00016674466304750777\n",
      "Iteration: 1475 Loss: 0.0001665849758097631\n",
      "Iteration: 1476 Loss: 0.00016642551588192704\n",
      "Iteration: 1477 Loss: 0.00016626628290897818\n",
      "Iteration: 1478 Loss: 0.0001661072765397387\n",
      "Iteration: 1479 Loss: 0.00016594849642739403\n",
      "Iteration: 1480 Loss: 0.00016578994222955437\n",
      "Iteration: 1481 Loss: 0.00016563161360731355\n",
      "Iteration: 1482 Loss: 0.0001654735102258564\n",
      "Iteration: 1483 Loss: 0.00016531563175471977\n",
      "Iteration: 1484 Loss: 0.00016515797786713254\n",
      "Iteration: 1485 Loss: 0.00016500054824083462\n",
      "Iteration: 1486 Loss: 0.0001648433425571399\n",
      "Iteration: 1487 Loss: 0.0001646863605010922\n",
      "Iteration: 1488 Loss: 0.00016452960176222258\n",
      "Iteration: 1489 Loss: 0.00016437306603369418\n",
      "Iteration: 1490 Loss: 0.00016421675301285657\n",
      "Iteration: 1491 Loss: 0.0001640606624007363\n",
      "Iteration: 1492 Loss: 0.00016390479390287682\n",
      "Iteration: 1493 Loss: 0.00016374914722787083\n",
      "Iteration: 1494 Loss: 0.0001635937220888435\n",
      "Iteration: 1495 Loss: 0.00016343851820266623\n",
      "Iteration: 1496 Loss: 0.00016328353529013572\n",
      "Iteration: 1497 Loss: 0.00016312877307585947\n",
      "Iteration: 1498 Loss: 0.00016297423128835547\n",
      "Iteration: 1499 Loss: 0.00016281990966000067\n",
      "Iteration: 1500 Loss: 0.00016266580792703163\n",
      "Iteration: 1501 Loss: 0.00016251192582974463\n",
      "Iteration: 1502 Loss: 0.00016235826311222619\n",
      "Iteration: 1503 Loss: 0.0001622048195221788\n",
      "Iteration: 1504 Loss: 0.00016205159481121788\n",
      "Iteration: 1505 Loss: 0.00016189858873522515\n",
      "Iteration: 1506 Loss: 0.00016174580105356314\n",
      "Iteration: 1507 Loss: 0.00016159323152947243\n",
      "Iteration: 1508 Loss: 0.0001614408799298999\n",
      "Iteration: 1509 Loss: 0.00016128874602568967\n",
      "Iteration: 1510 Loss: 0.00016113682959158503\n",
      "Iteration: 1511 Loss: 0.00016098513040607102\n",
      "Iteration: 1512 Loss: 0.00016083364825182273\n",
      "Iteration: 1513 Loss: 0.0001606823829146512\n",
      "Iteration: 1514 Loss: 0.0001605313341847857\n",
      "Iteration: 1515 Loss: 0.00016038050185603184\n",
      "Iteration: 1516 Loss: 0.00016022988572618894\n",
      "Iteration: 1517 Loss: 0.00016007948559641327\n",
      "Iteration: 1518 Loss: 0.00015992930127204605\n",
      "Iteration: 1519 Loss: 0.00015977933256231805\n",
      "Iteration: 1520 Loss: 0.0001596295792800159\n",
      "Iteration: 1521 Loss: 0.00015948004124215464\n",
      "Iteration: 1522 Loss: 0.00015933071826905156\n",
      "Iteration: 1523 Loss: 0.00015918161018497527\n",
      "Iteration: 1524 Loss: 0.00015903271681825694\n",
      "Iteration: 1525 Loss: 0.00015888403800084523\n",
      "Iteration: 1526 Loss: 0.00015873557356877078\n",
      "Iteration: 1527 Loss: 0.00015858732336161764\n",
      "Iteration: 1528 Loss: 0.000158439287222779\n",
      "Iteration: 1529 Loss: 0.00015829146500030613\n",
      "Iteration: 1530 Loss: 0.0001581438565445775\n",
      "Iteration: 1531 Loss: 0.00015799646171141577\n",
      "Iteration: 1532 Loss: 0.00015784928035938209\n",
      "Iteration: 1533 Loss: 0.00015770231235148215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1534 Loss: 0.0001575555575543044\n",
      "Iteration: 1535 Loss: 0.00015740901583843412\n",
      "Iteration: 1536 Loss: 0.00015726268707869914\n",
      "Iteration: 1537 Loss: 0.00015711657115325396\n",
      "Iteration: 1538 Loss: 0.00015697066794420996\n",
      "Iteration: 1539 Loss: 0.00015682497733808656\n",
      "Iteration: 1540 Loss: 0.00015667949922488954\n",
      "Iteration: 1541 Loss: 0.000156534233498608\n",
      "Iteration: 1542 Loss: 0.00015638918005759635\n",
      "Iteration: 1543 Loss: 0.00015624433880360986\n",
      "Iteration: 1544 Loss: 0.00015609970964299463\n",
      "Iteration: 1545 Loss: 0.00015595529248546266\n",
      "Iteration: 1546 Loss: 0.00015581108724504537\n",
      "Iteration: 1547 Loss: 0.00015566709383991388\n",
      "Iteration: 1548 Loss: 0.00015552331219216526\n",
      "Iteration: 1549 Loss: 0.000155379742227992\n",
      "Iteration: 1550 Loss: 0.00015523638387757573\n",
      "Iteration: 1551 Loss: 0.00015509323707492507\n",
      "Iteration: 1552 Loss: 0.00015495030175846682\n",
      "Iteration: 1553 Loss: 0.00015480757787096698\n",
      "Iteration: 1554 Loss: 0.0001546650653590205\n",
      "Iteration: 1555 Loss: 0.00015452276417306864\n",
      "Iteration: 1556 Loss: 0.0001543806742680098\n",
      "Iteration: 1557 Loss: 0.0001542387956035371\n",
      "Iteration: 1558 Loss: 0.00015409712814278519\n",
      "Iteration: 1559 Loss: 0.00015395567185338987\n",
      "Iteration: 1560 Loss: 0.00015381442670730992\n",
      "Iteration: 1561 Loss: 0.00015367339268090055\n",
      "Iteration: 1562 Loss: 0.00015353256975441225\n",
      "Iteration: 1563 Loss: 0.0001533919579126669\n",
      "Iteration: 1564 Loss: 0.00015325155714499442\n",
      "Iteration: 1565 Loss: 0.00015311136744468836\n",
      "Iteration: 1566 Loss: 0.00015297138881035596\n",
      "Iteration: 1567 Loss: 0.00015283162124404435\n",
      "Iteration: 1568 Loss: 0.00015269206475274064\n",
      "Iteration: 1569 Loss: 0.00015255271934759316\n",
      "Iteration: 1570 Loss: 0.0001524135850453233\n",
      "Iteration: 1571 Loss: 0.0001522746618658845\n",
      "Iteration: 1572 Loss: 0.00015213594983456077\n",
      "Iteration: 1573 Loss: 0.0001519974489815844\n",
      "Iteration: 1574 Loss: 0.00015185915934114565\n",
      "Iteration: 1575 Loss: 0.00015172108095221694\n",
      "Iteration: 1576 Loss: 0.00015158321385893157\n",
      "Iteration: 1577 Loss: 0.00015144555811004301\n",
      "Iteration: 1578 Loss: 0.0001513081137589513\n",
      "Iteration: 1579 Loss: 0.00015117088086431506\n",
      "Iteration: 1580 Loss: 0.00015103385948939924\n",
      "Iteration: 1581 Loss: 0.0001508970497022381\n",
      "Iteration: 1582 Loss: 0.0001507604515762157\n",
      "Iteration: 1583 Loss: 0.00015062406518938408\n",
      "Iteration: 1584 Loss: 0.00015048789062503524\n",
      "Iteration: 1585 Loss: 0.0001503519279718606\n",
      "Iteration: 1586 Loss: 0.00015021617732343754\n",
      "Iteration: 1587 Loss: 0.0001500806387787455\n",
      "Iteration: 1588 Loss: 0.00014994531244176594\n",
      "Iteration: 1589 Loss: 0.00014981019842181585\n",
      "Iteration: 1590 Loss: 0.00014967529683399377\n",
      "Iteration: 1591 Loss: 0.00014954060779837606\n",
      "Iteration: 1592 Loss: 0.00014940613144081265\n",
      "Iteration: 1593 Loss: 0.00014927186789249154\n",
      "Iteration: 1594 Loss: 0.00014913781729022931\n",
      "Iteration: 1595 Loss: 0.00014900397977683118\n",
      "Iteration: 1596 Loss: 0.00014887035550031005\n",
      "Iteration: 1597 Loss: 0.0001487369446146096\n",
      "Iteration: 1598 Loss: 0.00014860374727948943\n",
      "Iteration: 1599 Loss: 0.00014847076366097096\n",
      "Iteration: 1600 Loss: 0.0001483379939306588\n",
      "Iteration: 1601 Loss: 0.00014820543826617836\n",
      "Iteration: 1602 Loss: 0.00014807309685178205\n",
      "Iteration: 1603 Loss: 0.00014794096987731631\n",
      "Iteration: 1604 Loss: 0.0001478090575392663\n",
      "Iteration: 1605 Loss: 0.0001476773600403999\n",
      "Iteration: 1606 Loss: 0.00014754587758981078\n",
      "Iteration: 1607 Loss: 0.0001474146104028772\n",
      "Iteration: 1608 Loss: 0.00014728355870174636\n",
      "Iteration: 1609 Loss: 0.0001471527227155491\n",
      "Iteration: 1610 Loss: 0.00014702210267964486\n",
      "Iteration: 1611 Loss: 0.000146891698836581\n",
      "Iteration: 1612 Loss: 0.00014676151143513293\n",
      "Iteration: 1613 Loss: 0.00014663154073188035\n",
      "Iteration: 1614 Loss: 0.00014650178699035014\n",
      "Iteration: 1615 Loss: 0.00014637225048105717\n",
      "Iteration: 1616 Loss: 0.00014624293148181273\n",
      "Iteration: 1617 Loss: 0.00014611383027759406\n",
      "Iteration: 1618 Loss: 0.0001459849471615774\n",
      "Iteration: 1619 Loss: 0.00014585628243362942\n",
      "Iteration: 1620 Loss: 0.00014572783640198514\n",
      "Iteration: 1621 Loss: 0.00014559960938273218\n",
      "Iteration: 1622 Loss: 0.0001454716016991298\n",
      "Iteration: 1623 Loss: 0.00014534381368316786\n",
      "Iteration: 1624 Loss: 0.0001452162456745325\n",
      "Iteration: 1625 Loss: 0.00014508889802150457\n",
      "Iteration: 1626 Loss: 0.00014496177108061888\n",
      "Iteration: 1627 Loss: 0.00014483486521686486\n",
      "Iteration: 1628 Loss: 0.00014470818080354394\n",
      "Iteration: 1629 Loss: 0.0001445817182234345\n",
      "Iteration: 1630 Loss: 0.00014445547786761493\n",
      "Iteration: 1631 Loss: 0.0001443294601363949\n",
      "Iteration: 1632 Loss: 0.00014420366543933019\n",
      "Iteration: 1633 Loss: 0.00014407809419486717\n",
      "Iteration: 1634 Loss: 0.0001439527468314192\n",
      "Iteration: 1635 Loss: 0.0001438276237865022\n",
      "Iteration: 1636 Loss: 0.0001437027255075795\n",
      "Iteration: 1637 Loss: 0.0001435780524521344\n",
      "Iteration: 1638 Loss: 0.0001434536050875696\n",
      "Iteration: 1639 Loss: 0.00014332938389179927\n",
      "Iteration: 1640 Loss: 0.00014320538935215916\n",
      "Iteration: 1641 Loss: 0.00014308162196737925\n",
      "Iteration: 1642 Loss: 0.0001429580822463347\n",
      "Iteration: 1643 Loss: 0.00014283477070900453\n",
      "Iteration: 1644 Loss: 0.0001427116878868205\n",
      "Iteration: 1645 Loss: 0.0001425888343217489\n",
      "Iteration: 1646 Loss: 0.00014246621056704422\n",
      "Iteration: 1647 Loss: 0.00014234381718829532\n",
      "Iteration: 1648 Loss: 0.000142221654762713\n",
      "Iteration: 1649 Loss: 0.00014209972387862308\n",
      "Iteration: 1650 Loss: 0.00014197802513753085\n",
      "Iteration: 1651 Loss: 0.00014185655915253535\n",
      "Iteration: 1652 Loss: 0.0001417353265498593\n",
      "Iteration: 1653 Loss: 0.00014161432796831657\n",
      "Iteration: 1654 Loss: 0.00014149356405933027\n",
      "Iteration: 1655 Loss: 0.0001413730354882021\n",
      "Iteration: 1656 Loss: 0.00014125274293337681\n",
      "Iteration: 1657 Loss: 0.00014113268708671384\n",
      "Iteration: 1658 Loss: 0.00014101286865411664\n",
      "Iteration: 1659 Loss: 0.0001408932883556585\n",
      "Iteration: 1660 Loss: 0.00014077394692572705\n",
      "Iteration: 1661 Loss: 0.00014065484511277272\n",
      "Iteration: 1662 Loss: 0.00014053598368090033\n",
      "Iteration: 1663 Loss: 0.00014041736340926285\n",
      "Iteration: 1664 Loss: 0.0001402989850917651\n",
      "Iteration: 1665 Loss: 0.00014018084953838653\n",
      "Iteration: 1666 Loss: 0.00014006295757463633\n",
      "Iteration: 1667 Loss: 0.00013994531004247798\n",
      "Iteration: 1668 Loss: 0.00013982790779997267\n",
      "Iteration: 1669 Loss: 0.0001397107517220331\n",
      "Iteration: 1670 Loss: 0.00013959384270078557\n",
      "Iteration: 1671 Loss: 0.0001394771816450269\n",
      "Iteration: 1672 Loss: 0.00013936076948194392\n",
      "Iteration: 1673 Loss: 0.00013924460715582246\n",
      "Iteration: 1674 Loss: 0.00013912869562917964\n",
      "Iteration: 1675 Loss: 0.0001390130358832679\n",
      "Iteration: 1676 Loss: 0.00013889762891783686\n",
      "Iteration: 1677 Loss: 0.0001387824757519031\n",
      "Iteration: 1678 Loss: 0.0001386675774238182\n",
      "Iteration: 1679 Loss: 0.0001385529349915332\n",
      "Iteration: 1680 Loss: 0.00013843854953352947\n",
      "Iteration: 1681 Loss: 0.00013832442214804406\n",
      "Iteration: 1682 Loss: 0.00013821055395429841\n",
      "Iteration: 1683 Loss: 0.00013809694609267722\n",
      "Iteration: 1684 Loss: 0.0001379835997245197\n",
      "Iteration: 1685 Loss: 0.0001378705160333978\n",
      "Iteration: 1686 Loss: 0.0001377576962249929\n",
      "Iteration: 1687 Loss: 0.00013764514152709266\n",
      "Iteration: 1688 Loss: 0.0001375328531904205\n",
      "Iteration: 1689 Loss: 0.00013742083248867917\n",
      "Iteration: 1690 Loss: 0.00013730908071945465\n",
      "Iteration: 1691 Loss: 0.00013719759920436734\n",
      "Iteration: 1692 Loss: 0.0001370863892886968\n",
      "Iteration: 1693 Loss: 0.0001369754523429144\n",
      "Iteration: 1694 Loss: 0.00013686478976222108\n",
      "Iteration: 1695 Loss: 0.00013675440296748645\n",
      "Iteration: 1696 Loss: 0.00013666555177379731\n",
      "Iteration: 1697 Loss: 0.00013659529885952961\n",
      "Iteration: 1698 Loss: 0.00013652528908556313\n",
      "Iteration: 1699 Loss: 0.00013645552401180317\n",
      "Iteration: 1700 Loss: 0.00013638600522551458\n",
      "Iteration: 1701 Loss: 0.0001363167343417504\n",
      "Iteration: 1702 Loss: 0.00013624771300370354\n",
      "Iteration: 1703 Loss: 0.00013617894288275252\n",
      "Iteration: 1704 Loss: 0.00013611042567913186\n",
      "Iteration: 1705 Loss: 0.00013604216312242066\n",
      "Iteration: 1706 Loss: 0.00013597415697185858\n",
      "Iteration: 1707 Loss: 0.00013592507697581581\n",
      "Iteration: 1708 Loss: 0.00013589908218868707\n",
      "Iteration: 1709 Loss: 0.00013587331062041516\n",
      "Iteration: 1710 Loss: 0.00013584776418753994\n",
      "Iteration: 1711 Loss: 0.00013582244483789145\n",
      "Iteration: 1712 Loss: 0.00013579735455058076\n",
      "Iteration: 1713 Loss: 0.00013577249533667035\n",
      "Iteration: 1714 Loss: 0.00013574786924025508\n",
      "Iteration: 1715 Loss: 0.0001357234783371531\n",
      "Iteration: 1716 Loss: 0.0001356993247373984\n",
      "Iteration: 1717 Loss: 0.00013567541058323004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1718 Loss: 0.00013565173805113397\n",
      "Iteration: 1719 Loss: 0.0001356283093510718\n",
      "Iteration: 1720 Loss: 0.00013560512672732832\n",
      "Iteration: 1721 Loss: 0.0001355821924586896\n",
      "Iteration: 1722 Loss: 0.00013555950885816567\n",
      "Iteration: 1723 Loss: 0.0001355370782738058\n",
      "Iteration: 1724 Loss: 0.00013551490308838892\n",
      "Iteration: 1725 Loss: 0.00013549298571979344\n",
      "Iteration: 1726 Loss: 0.00013547132862151348\n",
      "Iteration: 1727 Loss: 0.0001354499342818884\n",
      "Iteration: 1728 Loss: 0.0001354288052254615\n",
      "Iteration: 1729 Loss: 0.00013540794401160103\n",
      "Iteration: 1730 Loss: 0.00013538735323551955\n",
      "Iteration: 1731 Loss: 0.0001353670355282038\n",
      "Iteration: 1732 Loss: 0.0001353469935555798\n",
      "Iteration: 1733 Loss: 0.00013532723001967976\n",
      "Iteration: 1734 Loss: 0.00013530774765715855\n",
      "Iteration: 1735 Loss: 0.00013528854924049345\n",
      "Iteration: 1736 Loss: 0.00013526963757633578\n",
      "Iteration: 1737 Loss: 0.00013525101550681404\n",
      "Iteration: 1738 Loss: 0.00013523268590806878\n",
      "Iteration: 1739 Loss: 0.00013521465169092717\n",
      "Iteration: 1740 Loss: 0.00013519691579922318\n",
      "Iteration: 1741 Loss: 0.000135179481210127\n",
      "Iteration: 1742 Loss: 0.00013516235093447257\n",
      "Iteration: 1743 Loss: 0.00013514552801429245\n",
      "Iteration: 1744 Loss: 0.00013512901552380395\n",
      "Iteration: 1745 Loss: 0.000135112816568053\n",
      "Iteration: 1746 Loss: 0.00013509693428228898\n",
      "Iteration: 1747 Loss: 0.00013508137183132908\n",
      "Iteration: 1748 Loss: 0.00013506613240808706\n",
      "Iteration: 1749 Loss: 0.0001350512192333565\n",
      "Iteration: 1750 Loss: 0.0001350366355542393\n",
      "Iteration: 1751 Loss: 0.0001350223846434897\n",
      "Iteration: 1752 Loss: 0.00013500846979774684\n",
      "Iteration: 1753 Loss: 0.00013499489433679504\n",
      "Iteration: 1754 Loss: 0.0001349816616021356\n",
      "Iteration: 1755 Loss: 0.00013496877495490712\n",
      "Iteration: 1756 Loss: 0.0001349562377754605\n",
      "Iteration: 1757 Loss: 0.0001349440534608777\n",
      "Iteration: 1758 Loss: 0.0001349322254235191\n",
      "Iteration: 1759 Loss: 0.0001349207570889036\n",
      "Iteration: 1760 Loss: 0.0001349096518947905\n",
      "Iteration: 1761 Loss: 0.00013489891328729968\n",
      "Iteration: 1762 Loss: 0.0001348885447214407\n",
      "Iteration: 1763 Loss: 0.00013487854965668977\n",
      "Iteration: 1764 Loss: 0.0001348689315554664\n",
      "Iteration: 1765 Loss: 0.00013485969388037585\n",
      "Iteration: 1766 Loss: 0.00013485084009284135\n",
      "Iteration: 1767 Loss: 0.00013484237364928502\n",
      "Iteration: 1768 Loss: 0.0001348342979990774\n",
      "Iteration: 1769 Loss: 0.0001348266165814842\n",
      "Iteration: 1770 Loss: 0.00013481933282273397\n",
      "Iteration: 1771 Loss: 0.00013481245013341364\n",
      "Iteration: 1772 Loss: 0.00013480597190463218\n",
      "Iteration: 1773 Loss: 0.00013479990150621433\n",
      "Iteration: 1774 Loss: 0.00013479424228195915\n",
      "Iteration: 1775 Loss: 0.0001347889975470262\n",
      "Iteration: 1776 Loss: 0.0001347841705845951\n",
      "Iteration: 1777 Loss: 0.0001347797646422266\n",
      "Iteration: 1778 Loss: 0.00013477578292824379\n",
      "Iteration: 1779 Loss: 0.00013477222860841103\n",
      "Iteration: 1780 Loss: 0.00013476910480230802\n",
      "Iteration: 1781 Loss: 0.00013476641457891894\n",
      "Iteration: 1782 Loss: 0.00013476416095319004\n",
      "Iteration: 1783 Loss: 0.00013476234688272683\n",
      "Iteration: 1784 Loss: 0.00013476097526336222\n",
      "Iteration: 1785 Loss: 0.00013476004892581678\n",
      "Iteration: 1786 Loss: 0.00013475957063118692\n",
      "Iteration: 1787 Loss: 0.00013475954306752118\n",
      "Iteration: 1788 Loss: 0.00013475996884616167\n",
      "Iteration: 1789 Loss: 0.00013476085049709605\n",
      "Iteration: 1790 Loss: 0.0001347621904657111\n",
      "Iteration: 1791 Loss: 0.0001347639911092252\n",
      "Iteration: 1792 Loss: 0.00013476625469259392\n",
      "Iteration: 1793 Loss: 0.00013476898338398588\n",
      "Iteration: 1794 Loss: 0.00013477217925287154\n",
      "Iteration: 1795 Loss: 0.00013477584426549268\n",
      "Iteration: 1796 Loss: 0.00013477998028127582\n",
      "Iteration: 1797 Loss: 0.00013478458904983985\n",
      "Iteration: 1798 Loss: 0.00013478967220791594\n",
      "Iteration: 1799 Loss: 0.00013479523127535022\n",
      "Iteration: 1800 Loss: 0.0001348012676533035\n",
      "Iteration: 1801 Loss: 0.00013480778262044498\n",
      "Iteration: 1802 Loss: 0.00013481477733087518\n",
      "Iteration: 1803 Loss: 0.00013482225281136808\n",
      "Iteration: 1804 Loss: 0.0001348302099587747\n",
      "Iteration: 1805 Loss: 0.00013483864953817373\n",
      "Iteration: 1806 Loss: 0.00013484757218092634\n",
      "Iteration: 1807 Loss: 0.00013166393384899666\n",
      "Iteration: 1808 Loss: 0.00012455278717458785\n",
      "Iteration: 1809 Loss: 0.00011459337730658465\n",
      "Iteration: 1810 Loss: 0.00010276179609639233\n",
      "Iteration: 1811 Loss: 8.992138889045541e-05\n",
      "Iteration: 1812 Loss: 7.681569516307754e-05\n",
      "Iteration: 1813 Loss: 6.407358327993625e-05\n",
      "Iteration: 1814 Loss: 5.22326498228977e-05\n",
      "Iteration: 1815 Loss: 4.180285967172255e-05\n",
      "Iteration: 1816 Loss: 3.343723345263936e-05\n",
      "Iteration: 1817 Loss: 2.822817165850845e-05\n",
      "Iteration: 1818 Loss: 2.7043346834506392e-05\n",
      "Iteration: 1819 Loss: 2.8835258617116604e-05\n",
      "Iteration: 1820 Loss: 3.144873482533207e-05\n",
      "Iteration: 1821 Loss: 3.3592124623098326e-05\n",
      "Iteration: 1822 Loss: 3.4946976393371964e-05\n",
      "Iteration: 1823 Loss: 3.551759066081141e-05\n",
      "Iteration: 1824 Loss: 3.540985527691434e-05\n",
      "Iteration: 1825 Loss: 3.476529649228738e-05\n",
      "Iteration: 1826 Loss: 3.373561893250921e-05\n",
      "Iteration: 1827 Loss: 3.247040347842621e-05\n",
      "Iteration: 1828 Loss: 3.1109916571203745e-05\n",
      "Iteration: 1829 Loss: 2.9779666348262745e-05\n",
      "Iteration: 1830 Loss: 2.8584627881982714e-05\n",
      "Iteration: 1831 Loss: 2.760197471102597e-05\n",
      "Iteration: 1832 Loss: 2.6873159930758536e-05\n",
      "Iteration: 1833 Loss: 2.6399308660147804e-05\n",
      "Iteration: 1834 Loss: 2.6145243509136633e-05\n",
      "Iteration: 1835 Loss: 2.60533576145677e-05\n",
      "Iteration: 1836 Loss: 2.6061392314529116e-05\n",
      "Iteration: 1837 Loss: 2.6116069144130812e-05\n",
      "Iteration: 1838 Loss: 2.617918346896033e-05\n",
      "Iteration: 1839 Loss: 2.6227745555472043e-05\n",
      "Iteration: 1840 Loss: 2.625116786253172e-05\n",
      "Iteration: 1841 Loss: 2.6247657554340077e-05\n",
      "Iteration: 1842 Loss: 2.622086891953867e-05\n",
      "Iteration: 1843 Loss: 2.6177191458283628e-05\n",
      "Iteration: 1844 Loss: 2.6123742592288012e-05\n",
      "Iteration: 1845 Loss: 2.606701949394737e-05\n",
      "Iteration: 1846 Loss: 2.6012124362466405e-05\n",
      "Iteration: 1847 Loss: 2.5962458643270287e-05\n",
      "Iteration: 1848 Loss: 2.5919768995131526e-05\n",
      "Iteration: 1849 Loss: 2.5884422481344554e-05\n",
      "Iteration: 1850 Loss: 2.5855795079962594e-05\n",
      "Iteration: 1851 Loss: 2.5832677252599263e-05\n",
      "Iteration: 1852 Loss: 2.581362898238143e-05\n",
      "Iteration: 1853 Loss: 2.5797247571080722e-05\n",
      "Iteration: 1854 Loss: 2.578233820136972e-05\n",
      "Iteration: 1855 Loss: 2.5767996186854225e-05\n",
      "Iteration: 1856 Loss: 2.5753620253926046e-05\n",
      "Iteration: 1857 Loss: 2.5738879553519894e-05\n",
      "Iteration: 1858 Loss: 2.5723655619417514e-05\n",
      "Iteration: 1859 Loss: 2.5707976360906373e-05\n",
      "Iteration: 1860 Loss: 2.5691954101145544e-05\n",
      "Iteration: 1861 Loss: 2.5675734769423447e-05\n",
      "Iteration: 1862 Loss: 2.565946125112042e-05\n",
      "Iteration: 1863 Loss: 2.5643250866260015e-05\n",
      "Iteration: 1864 Loss: 2.5627184995680725e-05\n",
      "Iteration: 1865 Loss: 2.5611307908139482e-05\n",
      "Iteration: 1866 Loss: 2.5595631602877087e-05\n",
      "Iteration: 1867 Loss: 2.5580143792483094e-05\n",
      "Iteration: 1868 Loss: 2.556481673525387e-05\n",
      "Iteration: 1869 Loss: 2.5549615338441776e-05\n",
      "Iteration: 1870 Loss: 2.5534503618665488e-05\n",
      "Iteration: 1871 Loss: 2.5519449172775798e-05\n",
      "Iteration: 1872 Loss: 2.550442571892243e-05\n",
      "Iteration: 1873 Loss: 2.5489414022004725e-05\n",
      "Iteration: 1874 Loss: 2.5474401630331684e-05\n",
      "Iteration: 1875 Loss: 2.545938187017911e-05\n",
      "Iteration: 1876 Loss: 2.5444352490028847e-05\n",
      "Iteration: 1877 Loss: 2.542931425064344e-05\n",
      "Iteration: 1878 Loss: 2.541426966800616e-05\n",
      "Iteration: 1879 Loss: 2.5399222015145155e-05\n",
      "Iteration: 1880 Loss: 2.5384174614540806e-05\n",
      "Iteration: 1881 Loss: 2.536913041130584e-05\n",
      "Iteration: 1882 Loss: 2.535409176932441e-05\n",
      "Iteration: 1883 Loss: 2.533906043640507e-05\n",
      "Iteration: 1884 Loss: 2.5324037610312448e-05\n",
      "Iteration: 1885 Loss: 2.530902405872658e-05\n",
      "Iteration: 1886 Loss: 2.5294020249557438e-05\n",
      "Iteration: 1887 Loss: 2.5279026468839327e-05\n",
      "Iteration: 1888 Loss: 2.526404290974192e-05\n",
      "Iteration: 1889 Loss: 2.5249069731314556e-05\n",
      "Iteration: 1890 Loss: 2.5234107088665527e-05\n",
      "Iteration: 1891 Loss: 2.5219155141580006e-05\n",
      "Iteration: 1892 Loss: 2.5204214047191512e-05\n",
      "Iteration: 1893 Loss: 2.518928394720118e-05\n",
      "Iteration: 1894 Loss: 2.5174364952653086e-05\n",
      "Iteration: 1895 Loss: 2.5159457131307286e-05\n",
      "Iteration: 1896 Loss: 2.5144560501565173e-05\n",
      "Iteration: 1897 Loss: 2.512967502978413e-05\n",
      "Iteration: 1898 Loss: 2.511480063470377e-05\n",
      "Iteration: 1899 Loss: 2.5099937194235146e-05\n",
      "Iteration: 1900 Loss: 2.5085084555846633e-05\n",
      "Iteration: 1901 Loss: 2.5070242547480034e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1902 Loss: 2.505541098860226e-05\n",
      "Iteration: 1903 Loss: 2.5040589699113282e-05\n",
      "Iteration: 1904 Loss: 2.502577850834496e-05\n",
      "Iteration: 1905 Loss: 2.501097725946995e-05\n",
      "Iteration: 1906 Loss: 2.4996185814583263e-05\n",
      "Iteration: 1907 Loss: 2.4981404055770237e-05\n",
      "Iteration: 1908 Loss: 2.4966631886085012e-05\n",
      "Iteration: 1909 Loss: 2.4951869227919018e-05\n",
      "Iteration: 1910 Loss: 2.4937116022773778e-05\n",
      "Iteration: 1911 Loss: 2.4922372227347415e-05\n",
      "Iteration: 1912 Loss: 2.490763781237816e-05\n",
      "Iteration: 1913 Loss: 2.489291275936652e-05\n",
      "Iteration: 1914 Loss: 2.487819705908188e-05\n",
      "Iteration: 1915 Loss: 2.486349070804726e-05\n",
      "Iteration: 1916 Loss: 2.484879370744003e-05\n",
      "Iteration: 1917 Loss: 2.483410606090213e-05\n",
      "Iteration: 1918 Loss: 2.4819427774170616e-05\n",
      "Iteration: 1919 Loss: 2.4804758852019144e-05\n",
      "Iteration: 1920 Loss: 2.4790099299496666e-05\n",
      "Iteration: 1921 Loss: 2.477544911954245e-05\n",
      "Iteration: 1922 Loss: 2.476080831331733e-05\n",
      "Iteration: 1923 Loss: 2.4746176879645323e-05\n",
      "Iteration: 1924 Loss: 2.4731554814757814e-05\n",
      "Iteration: 1925 Loss: 2.4716942112799366e-05\n",
      "Iteration: 1926 Loss: 2.4702338765292858e-05\n",
      "Iteration: 1927 Loss: 2.4687744761720698e-05\n",
      "Iteration: 1928 Loss: 2.4673160089093013e-05\n",
      "Iteration: 1929 Loss: 2.4658584732948642e-05\n",
      "Iteration: 1930 Loss: 2.4644018676961185e-05\n",
      "Iteration: 1931 Loss: 2.4629461904542102e-05\n",
      "Iteration: 1932 Loss: 2.4614914396864164e-05\n",
      "Iteration: 1933 Loss: 2.460037613528583e-05\n",
      "Iteration: 1934 Loss: 2.458584710041396e-05\n",
      "Iteration: 1935 Loss: 2.457132727248385e-05\n",
      "Iteration: 1936 Loss: 2.4556816632092315e-05\n",
      "Iteration: 1937 Loss: 2.45423151599164e-05\n",
      "Iteration: 1938 Loss: 2.452782283635217e-05\n",
      "Iteration: 1939 Loss: 2.4513339642854847e-05\n",
      "Iteration: 1940 Loss: 2.4498865560958036e-05\n",
      "Iteration: 1941 Loss: 2.4484400572485685e-05\n",
      "Iteration: 1942 Loss: 2.4469944660289438e-05\n",
      "Iteration: 1943 Loss: 2.445549780689755e-05\n",
      "Iteration: 1944 Loss: 2.4441059995921553e-05\n",
      "Iteration: 1945 Loss: 2.4426631211200614e-05\n",
      "Iteration: 1946 Loss: 2.4412211436756212e-05\n",
      "Iteration: 1947 Loss: 2.4397800656961375e-05\n",
      "Iteration: 1948 Loss: 2.4383398857080302e-05\n",
      "Iteration: 1949 Loss: 2.436900602197754e-05\n",
      "Iteration: 1950 Loss: 2.435462213673584e-05\n",
      "Iteration: 1951 Loss: 2.4340247187096313e-05\n",
      "Iteration: 1952 Loss: 2.4325881158343585e-05\n",
      "Iteration: 1953 Loss: 2.4311524036390367e-05\n",
      "Iteration: 1954 Loss: 2.429717580709087e-05\n",
      "Iteration: 1955 Loss: 2.4282836456080858e-05\n",
      "Iteration: 1956 Loss: 2.426850596938718e-05\n",
      "Iteration: 1957 Loss: 2.425418433282666e-05\n",
      "Iteration: 1958 Loss: 2.423987153243747e-05\n",
      "Iteration: 1959 Loss: 2.4225567554246584e-05\n",
      "Iteration: 1960 Loss: 2.42112723844278e-05\n",
      "Iteration: 1961 Loss: 2.419698600883459e-05\n",
      "Iteration: 1962 Loss: 2.4182708413597016e-05\n",
      "Iteration: 1963 Loss: 2.416843958476613e-05\n",
      "Iteration: 1964 Loss: 2.4154179508707415e-05\n",
      "Iteration: 1965 Loss: 2.4139928171275343e-05\n",
      "Iteration: 1966 Loss: 2.4125685558900535e-05\n",
      "Iteration: 1967 Loss: 2.4111451657916823e-05\n",
      "Iteration: 1968 Loss: 2.4097226454784415e-05\n",
      "Iteration: 1969 Loss: 2.4083009935664033e-05\n",
      "Iteration: 1970 Loss: 2.4068802087272584e-05\n",
      "Iteration: 1971 Loss: 2.4054602896022745e-05\n",
      "Iteration: 1972 Loss: 2.404041234869215e-05\n",
      "Iteration: 1973 Loss: 2.4026230431692797e-05\n",
      "Iteration: 1974 Loss: 2.4012057131983968e-05\n",
      "Iteration: 1975 Loss: 2.3997892436371053e-05\n",
      "Iteration: 1976 Loss: 2.3983736331966683e-05\n",
      "Iteration: 1977 Loss: 2.3969588805765382e-05\n",
      "Iteration: 1978 Loss: 2.3955449844698665e-05\n",
      "Iteration: 1979 Loss: 2.3941319435948253e-05\n",
      "Iteration: 1980 Loss: 2.3927197566888627e-05\n",
      "Iteration: 1981 Loss: 2.391308422470324e-05\n",
      "Iteration: 1982 Loss: 2.3898979396757143e-05\n",
      "Iteration: 1983 Loss: 2.388488307054803e-05\n",
      "Iteration: 1984 Loss: 2.387079523345674e-05\n",
      "Iteration: 1985 Loss: 2.3856715873188866e-05\n",
      "Iteration: 1986 Loss: 2.3842644977513217e-05\n",
      "Iteration: 1987 Loss: 2.3828582534026113e-05\n",
      "Iteration: 1988 Loss: 2.3814528530714423e-05\n",
      "Iteration: 1989 Loss: 2.3800482955126445e-05\n",
      "Iteration: 1990 Loss: 2.378644579556163e-05\n",
      "Iteration: 1991 Loss: 2.3772417039666105e-05\n",
      "Iteration: 1992 Loss: 2.37583966757169e-05\n",
      "Iteration: 1993 Loss: 2.3744384691558192e-05\n",
      "Iteration: 1994 Loss: 2.3730381075639467e-05\n",
      "Iteration: 1995 Loss: 2.3716385816122538e-05\n",
      "Iteration: 1996 Loss: 2.3702398900947275e-05\n",
      "Iteration: 1997 Loss: 2.368842031899341e-05\n",
      "Iteration: 1998 Loss: 2.3674450058125385e-05\n",
      "Iteration: 1999 Loss: 2.366048810721096e-05\n",
      "Iteration: 2000 Loss: 2.3646534454447035e-05\n",
      "Iteration: 2001 Loss: 2.3632589088715142e-05\n",
      "Iteration: 2002 Loss: 2.36186519983643e-05\n",
      "Iteration: 2003 Loss: 2.3604723172262643e-05\n",
      "Iteration: 2004 Loss: 2.3590802598916963e-05\n",
      "Iteration: 2005 Loss: 2.3576890267355284e-05\n",
      "Iteration: 2006 Loss: 2.3562986166630598e-05\n",
      "Iteration: 2007 Loss: 2.3549090285376158e-05\n",
      "Iteration: 2008 Loss: 2.3535202612458706e-05\n",
      "Iteration: 2009 Loss: 2.352132313700523e-05\n",
      "Iteration: 2010 Loss: 2.3507451848503977e-05\n",
      "Iteration: 2011 Loss: 2.349358873560826e-05\n",
      "Iteration: 2012 Loss: 2.347973378774007e-05\n",
      "Iteration: 2013 Loss: 2.346588699385234e-05\n",
      "Iteration: 2014 Loss: 2.3452048343609205e-05\n",
      "Iteration: 2015 Loss: 2.3438217826061487e-05\n",
      "Iteration: 2016 Loss: 2.3424395430762266e-05\n",
      "Iteration: 2017 Loss: 2.3410581147198203e-05\n",
      "Iteration: 2018 Loss: 2.339677496480867e-05\n",
      "Iteration: 2019 Loss: 2.338297687310874e-05\n",
      "Iteration: 2020 Loss: 2.336918686167889e-05\n",
      "Iteration: 2021 Loss: 2.335540492017003e-05\n",
      "Iteration: 2022 Loss: 2.334163103857258e-05\n",
      "Iteration: 2023 Loss: 2.3327865206554993e-05\n",
      "Iteration: 2024 Loss: 2.3314107413690912e-05\n",
      "Iteration: 2025 Loss: 2.3300357650078424e-05\n",
      "Iteration: 2026 Loss: 2.3286615905571644e-05\n",
      "Iteration: 2027 Loss: 2.3272882170056785e-05\n",
      "Iteration: 2028 Loss: 2.325915643338292e-05\n",
      "Iteration: 2029 Loss: 2.3245438685815306e-05\n",
      "Iteration: 2030 Loss: 2.3231728917307527e-05\n",
      "Iteration: 2031 Loss: 2.3218027118174026e-05\n",
      "Iteration: 2032 Loss: 2.320433327870614e-05\n",
      "Iteration: 2033 Loss: 2.319064738867092e-05\n",
      "Iteration: 2034 Loss: 2.3176969438727895e-05\n",
      "Iteration: 2035 Loss: 2.3163299419309687e-05\n",
      "Iteration: 2036 Loss: 2.3149637320547426e-05\n",
      "Iteration: 2037 Loss: 2.313598313309351e-05\n",
      "Iteration: 2038 Loss: 2.312233684715553e-05\n",
      "Iteration: 2039 Loss: 2.3108698453463285e-05\n",
      "Iteration: 2040 Loss: 2.3095067942477045e-05\n",
      "Iteration: 2041 Loss: 2.3081445305088022e-05\n",
      "Iteration: 2042 Loss: 2.306783053123887e-05\n",
      "Iteration: 2043 Loss: 2.3054223612219026e-05\n",
      "Iteration: 2044 Loss: 2.3040624538533268e-05\n",
      "Iteration: 2045 Loss: 2.3027033300951574e-05\n",
      "Iteration: 2046 Loss: 2.301344989012955e-05\n",
      "Iteration: 2047 Loss: 2.2999874297181488e-05\n",
      "Iteration: 2048 Loss: 2.2986306513275464e-05\n",
      "Iteration: 2049 Loss: 2.297274652876826e-05\n",
      "Iteration: 2050 Loss: 2.2959194334912676e-05\n",
      "Iteration: 2051 Loss: 2.294564992278314e-05\n",
      "Iteration: 2052 Loss: 2.2932113283377663e-05\n",
      "Iteration: 2053 Loss: 2.291858440789373e-05\n",
      "Iteration: 2054 Loss: 2.2905063287363355e-05\n",
      "Iteration: 2055 Loss: 2.2891549912905466e-05\n",
      "Iteration: 2056 Loss: 2.2878044275542417e-05\n",
      "Iteration: 2057 Loss: 2.286454636708143e-05\n",
      "Iteration: 2058 Loss: 2.2851056178319424e-05\n",
      "Iteration: 2059 Loss: 2.2837573701147124e-05\n",
      "Iteration: 2060 Loss: 2.2824098926547998e-05\n",
      "Iteration: 2061 Loss: 2.281063184604031e-05\n",
      "Iteration: 2062 Loss: 2.2797172451115435e-05\n",
      "Iteration: 2063 Loss: 2.2783720733071737e-05\n",
      "Iteration: 2064 Loss: 2.2770276683405273e-05\n",
      "Iteration: 2065 Loss: 2.275684029396459e-05\n",
      "Iteration: 2066 Loss: 2.2743411556382065e-05\n",
      "Iteration: 2067 Loss: 2.2729990462018892e-05\n",
      "Iteration: 2068 Loss: 2.2716577002575064e-05\n",
      "Iteration: 2069 Loss: 2.2703171169991785e-05\n",
      "Iteration: 2070 Loss: 2.26897729557602e-05\n",
      "Iteration: 2071 Loss: 2.2676382351756482e-05\n",
      "Iteration: 2072 Loss: 2.2662999350029786e-05\n",
      "Iteration: 2073 Loss: 2.264962394223871e-05\n",
      "Iteration: 2074 Loss: 2.2636256120354524e-05\n",
      "Iteration: 2075 Loss: 2.2622895876158283e-05\n",
      "Iteration: 2076 Loss: 2.260954320166689e-05\n",
      "Iteration: 2077 Loss: 2.2596198088827776e-05\n",
      "Iteration: 2078 Loss: 2.258286052959771e-05\n",
      "Iteration: 2079 Loss: 2.2569530516333538e-05\n",
      "Iteration: 2080 Loss: 2.2556208040867233e-05\n",
      "Iteration: 2081 Loss: 2.2542893095443332e-05\n",
      "Iteration: 2082 Loss: 2.2529585672214977e-05\n",
      "Iteration: 2083 Loss: 2.2516285763284274e-05\n",
      "Iteration: 2084 Loss: 2.2502993360887337e-05\n",
      "Iteration: 2085 Loss: 2.248970845749245e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2086 Loss: 2.247643104503669e-05\n",
      "Iteration: 2087 Loss: 2.2463161116199715e-05\n",
      "Iteration: 2088 Loss: 2.24498986628949e-05\n",
      "Iteration: 2089 Loss: 2.2436643677899362e-05\n",
      "Iteration: 2090 Loss: 2.2423396153473956e-05\n",
      "Iteration: 2091 Loss: 2.241015608208136e-05\n",
      "Iteration: 2092 Loss: 2.2396923456061037e-05\n",
      "Iteration: 2093 Loss: 2.238369826797025e-05\n",
      "Iteration: 2094 Loss: 2.2370480510369905e-05\n",
      "Iteration: 2095 Loss: 2.2357270176113398e-05\n",
      "Iteration: 2096 Loss: 2.2344067257395533e-05\n",
      "Iteration: 2097 Loss: 2.2330871747011276e-05\n",
      "Iteration: 2098 Loss: 2.231768363783953e-05\n",
      "Iteration: 2099 Loss: 2.2304502921913077e-05\n",
      "Iteration: 2100 Loss: 2.2291329592386705e-05\n",
      "Iteration: 2101 Loss: 2.2278163641985107e-05\n",
      "Iteration: 2102 Loss: 2.2265005063641165e-05\n",
      "Iteration: 2103 Loss: 2.2251853849841984e-05\n",
      "Iteration: 2104 Loss: 2.2238709993660394e-05\n",
      "Iteration: 2105 Loss: 2.2225573487872057e-05\n",
      "Iteration: 2106 Loss: 2.221244432530228e-05\n",
      "Iteration: 2107 Loss: 2.2199322498896598e-05\n",
      "Iteration: 2108 Loss: 2.218620800140664e-05\n",
      "Iteration: 2109 Loss: 2.2173100825969494e-05\n",
      "Iteration: 2110 Loss: 2.2160000965497035e-05\n",
      "Iteration: 2111 Loss: 2.2146908413162985e-05\n",
      "Iteration: 2112 Loss: 2.2133823162031522e-05\n",
      "Iteration: 2113 Loss: 2.2120745204902203e-05\n",
      "Iteration: 2114 Loss: 2.210767453518114e-05\n",
      "Iteration: 2115 Loss: 2.2094611145893687e-05\n",
      "Iteration: 2116 Loss: 2.2081555029957023e-05\n",
      "Iteration: 2117 Loss: 2.2068506180950155e-05\n",
      "Iteration: 2118 Loss: 2.2055464591712383e-05\n",
      "Iteration: 2119 Loss: 2.2042430255778122e-05\n",
      "Iteration: 2120 Loss: 2.2029403165892217e-05\n",
      "Iteration: 2121 Loss: 2.201638331576695e-05\n",
      "Iteration: 2122 Loss: 2.200337069874106e-05\n",
      "Iteration: 2123 Loss: 2.1990365307984096e-05\n",
      "Iteration: 2124 Loss: 2.1977367136918475e-05\n",
      "Iteration: 2125 Loss: 2.1964376178914172e-05\n",
      "Iteration: 2126 Loss: 2.1951392427248405e-05\n",
      "Iteration: 2127 Loss: 2.1938415875415982e-05\n",
      "Iteration: 2128 Loss: 2.1925446517042515e-05\n",
      "Iteration: 2129 Loss: 2.1912484345477143e-05\n",
      "Iteration: 2130 Loss: 2.1899529353965713e-05\n",
      "Iteration: 2131 Loss: 2.188658153609327e-05\n",
      "Iteration: 2132 Loss: 2.18736408856381e-05\n",
      "Iteration: 2133 Loss: 2.186070739607528e-05\n",
      "Iteration: 2134 Loss: 2.184778106090387e-05\n",
      "Iteration: 2135 Loss: 2.1834861873884264e-05\n",
      "Iteration: 2136 Loss: 2.1821949828708947e-05\n",
      "Iteration: 2137 Loss: 2.1809044918842643e-05\n",
      "Iteration: 2138 Loss: 2.1796147137994493e-05\n",
      "Iteration: 2139 Loss: 2.1783256479741557e-05\n",
      "Iteration: 2140 Loss: 2.1770372938284824e-05\n",
      "Iteration: 2141 Loss: 2.1757496507058234e-05\n",
      "Iteration: 2142 Loss: 2.1744627179605058e-05\n",
      "Iteration: 2143 Loss: 2.1731764950197426e-05\n",
      "Iteration: 2144 Loss: 2.171890981224355e-05\n",
      "Iteration: 2145 Loss: 2.170606175973408e-05\n",
      "Iteration: 2146 Loss: 2.169322078628544e-05\n",
      "Iteration: 2147 Loss: 2.1680386886062957e-05\n",
      "Iteration: 2148 Loss: 2.1667560052792195e-05\n",
      "Iteration: 2149 Loss: 2.1654740280779727e-05\n",
      "Iteration: 2150 Loss: 2.1641927563616574e-05\n",
      "Iteration: 2151 Loss: 2.1629121895073366e-05\n",
      "Iteration: 2152 Loss: 2.161632326954833e-05\n",
      "Iteration: 2153 Loss: 2.1603531680444862e-05\n",
      "Iteration: 2154 Loss: 2.159074712257921e-05\n",
      "Iteration: 2155 Loss: 2.1577969589613313e-05\n",
      "Iteration: 2156 Loss: 2.1565199075501348e-05\n",
      "Iteration: 2157 Loss: 2.1552435574114374e-05\n",
      "Iteration: 2158 Loss: 2.1539679080054662e-05\n",
      "Iteration: 2159 Loss: 2.15269295869642e-05\n",
      "Iteration: 2160 Loss: 2.1514187089243055e-05\n",
      "Iteration: 2161 Loss: 2.150145158101924e-05\n",
      "Iteration: 2162 Loss: 2.1488723056275395e-05\n",
      "Iteration: 2163 Loss: 2.1476001509592072e-05\n",
      "Iteration: 2164 Loss: 2.146328693482859e-05\n",
      "Iteration: 2165 Loss: 2.1450579326325416e-05\n",
      "Iteration: 2166 Loss: 2.1437878678405547e-05\n",
      "Iteration: 2167 Loss: 2.1425184985289265e-05\n",
      "Iteration: 2168 Loss: 2.141249824110811e-05\n",
      "Iteration: 2169 Loss: 2.1399818440302482e-05\n",
      "Iteration: 2170 Loss: 2.138714557718242e-05\n",
      "Iteration: 2171 Loss: 2.1374479646170843e-05\n",
      "Iteration: 2172 Loss: 2.1361820641399166e-05\n",
      "Iteration: 2173 Loss: 2.134916855757807e-05\n",
      "Iteration: 2174 Loss: 2.133652338866583e-05\n",
      "Iteration: 2175 Loss: 2.1323885129408345e-05\n",
      "Iteration: 2176 Loss: 2.1311253774160092e-05\n",
      "Iteration: 2177 Loss: 2.1298629317228275e-05\n",
      "Iteration: 2178 Loss: 2.1286011753193578e-05\n",
      "Iteration: 2179 Loss: 2.127340107648471e-05\n",
      "Iteration: 2180 Loss: 2.1260797281749718e-05\n",
      "Iteration: 2181 Loss: 2.1248200363316603e-05\n",
      "Iteration: 2182 Loss: 2.1235610315663224e-05\n",
      "Iteration: 2183 Loss: 2.1223027133502696e-05\n",
      "Iteration: 2184 Loss: 2.1210450811364423e-05\n",
      "Iteration: 2185 Loss: 2.119788134363375e-05\n",
      "Iteration: 2186 Loss: 2.1185318725118002e-05\n",
      "Iteration: 2187 Loss: 2.1172762950131915e-05\n",
      "Iteration: 2188 Loss: 2.116021401368089e-05\n",
      "Iteration: 2189 Loss: 2.1147671910127284e-05\n",
      "Iteration: 2190 Loss: 2.113513663425803e-05\n",
      "Iteration: 2191 Loss: 2.1122608180786825e-05\n",
      "Iteration: 2192 Loss: 2.1110086544308337e-05\n",
      "Iteration: 2193 Loss: 2.1097571719389307e-05\n",
      "Iteration: 2194 Loss: 2.108506370086784e-05\n",
      "Iteration: 2195 Loss: 2.1072562483436795e-05\n",
      "Iteration: 2196 Loss: 2.106006806202869e-05\n",
      "Iteration: 2197 Loss: 2.1047580431274018e-05\n",
      "Iteration: 2198 Loss: 2.1035099586176932e-05\n",
      "Iteration: 2199 Loss: 2.102262552105673e-05\n",
      "Iteration: 2200 Loss: 2.1010158231169717e-05\n",
      "Iteration: 2201 Loss: 2.0997697711057958e-05\n",
      "Iteration: 2202 Loss: 2.0985243955543358e-05\n",
      "Iteration: 2203 Loss: 2.097279695966494e-05\n",
      "Iteration: 2204 Loss: 2.09603567181782e-05\n",
      "Iteration: 2205 Loss: 2.0947923226030172e-05\n",
      "Iteration: 2206 Loss: 2.0935496478002544e-05\n",
      "Iteration: 2207 Loss: 2.0923076469128602e-05\n",
      "Iteration: 2208 Loss: 2.0910663194193408e-05\n",
      "Iteration: 2209 Loss: 2.0898256648023694e-05\n",
      "Iteration: 2210 Loss: 2.088585682600267e-05\n",
      "Iteration: 2211 Loss: 2.0873463722955673e-05\n",
      "Iteration: 2212 Loss: 2.0861077333547097e-05\n",
      "Iteration: 2213 Loss: 2.0848697652960563e-05\n",
      "Iteration: 2214 Loss: 2.0836324676059297e-05\n",
      "Iteration: 2215 Loss: 2.0823958398330062e-05\n",
      "Iteration: 2216 Loss: 2.0811598814117606e-05\n",
      "Iteration: 2217 Loss: 2.079924591897319e-05\n",
      "Iteration: 2218 Loss: 2.078689970771689e-05\n",
      "Iteration: 2219 Loss: 2.0774560175478274e-05\n",
      "Iteration: 2220 Loss: 2.076222731754563e-05\n",
      "Iteration: 2221 Loss: 2.074990112874885e-05\n",
      "Iteration: 2222 Loss: 2.0737581604162632e-05\n",
      "Iteration: 2223 Loss: 2.072526873898286e-05\n",
      "Iteration: 2224 Loss: 2.071296252856606e-05\n",
      "Iteration: 2225 Loss: 2.0700662967708877e-05\n",
      "Iteration: 2226 Loss: 2.0688370051838956e-05\n",
      "Iteration: 2227 Loss: 2.067608377593685e-05\n",
      "Iteration: 2228 Loss: 2.0663804135480793e-05\n",
      "Iteration: 2229 Loss: 2.0651531125196957e-05\n",
      "Iteration: 2230 Loss: 2.0639264740600515e-05\n",
      "Iteration: 2231 Loss: 2.062700497701465e-05\n",
      "Iteration: 2232 Loss: 2.0614751829422025e-05\n",
      "Iteration: 2233 Loss: 2.060250529310383e-05\n",
      "Iteration: 2234 Loss: 2.0590265363428152e-05\n",
      "Iteration: 2235 Loss: 2.0578032035712907e-05\n",
      "Iteration: 2236 Loss: 2.056580530501311e-05\n",
      "Iteration: 2237 Loss: 2.0553585166704685e-05\n",
      "Iteration: 2238 Loss: 2.054137161603671e-05\n",
      "Iteration: 2239 Loss: 2.0529164648491814e-05\n",
      "Iteration: 2240 Loss: 2.051696425922031e-05\n",
      "Iteration: 2241 Loss: 2.0504770443646918e-05\n",
      "Iteration: 2242 Loss: 2.0492583197207804e-05\n",
      "Iteration: 2243 Loss: 2.0480402515333097e-05\n",
      "Iteration: 2244 Loss: 2.0468228392821936e-05\n",
      "Iteration: 2245 Loss: 2.045606082567454e-05\n",
      "Iteration: 2246 Loss: 2.0443899809061195e-05\n",
      "Iteration: 2247 Loss: 2.0431745338486508e-05\n",
      "Iteration: 2248 Loss: 2.0419597409166247e-05\n",
      "Iteration: 2249 Loss: 2.040745601656431e-05\n",
      "Iteration: 2250 Loss: 2.039532115638897e-05\n",
      "Iteration: 2251 Loss: 2.03831928236989e-05\n",
      "Iteration: 2252 Loss: 2.0371071014238152e-05\n",
      "Iteration: 2253 Loss: 2.0358955723507784e-05\n",
      "Iteration: 2254 Loss: 2.0346846946629503e-05\n",
      "Iteration: 2255 Loss: 2.033474467924739e-05\n",
      "Iteration: 2256 Loss: 2.0322648917293784e-05\n",
      "Iteration: 2257 Loss: 2.0310559655525818e-05\n",
      "Iteration: 2258 Loss: 2.0298476889970016e-05\n",
      "Iteration: 2259 Loss: 2.028640061580632e-05\n",
      "Iteration: 2260 Loss: 2.0274330829000448e-05\n",
      "Iteration: 2261 Loss: 2.026226752476252e-05\n",
      "Iteration: 2262 Loss: 2.0250210698806314e-05\n",
      "Iteration: 2263 Loss: 2.023816034686979e-05\n",
      "Iteration: 2264 Loss: 2.022611646429327e-05\n",
      "Iteration: 2265 Loss: 2.0214079046704783e-05\n",
      "Iteration: 2266 Loss: 2.0202048089686195e-05\n",
      "Iteration: 2267 Loss: 2.0190023588787985e-05\n",
      "Iteration: 2268 Loss: 2.0178005539785548e-05\n",
      "Iteration: 2269 Loss: 2.0165993938072635e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2270 Loss: 2.0153988779446157e-05\n",
      "Iteration: 2271 Loss: 2.01419900595926e-05\n",
      "Iteration: 2272 Loss: 2.0129997774019643e-05\n",
      "Iteration: 2273 Loss: 2.0118011918602074e-05\n",
      "Iteration: 2274 Loss: 2.0106032488983248e-05\n",
      "Iteration: 2275 Loss: 2.0094059480647862e-05\n",
      "Iteration: 2276 Loss: 2.008209288947765e-05\n",
      "Iteration: 2277 Loss: 2.0070132711139667e-05\n",
      "Iteration: 2278 Loss: 2.0058178941056935e-05\n",
      "Iteration: 2279 Loss: 2.004623157529714e-05\n",
      "Iteration: 2280 Loss: 2.0034290609423717e-05\n",
      "Iteration: 2281 Loss: 2.0022356039535815e-05\n",
      "Iteration: 2282 Loss: 2.001042786083287e-05\n",
      "Iteration: 2283 Loss: 1.9998506069509897e-05\n",
      "Iteration: 2284 Loss: 1.998659066107116e-05\n",
      "Iteration: 2285 Loss: 1.9974681631273996e-05\n",
      "Iteration: 2286 Loss: 1.9962778976107962e-05\n",
      "Iteration: 2287 Loss: 1.9950882691193765e-05\n",
      "Iteration: 2288 Loss: 1.993899277235999e-05\n",
      "Iteration: 2289 Loss: 1.992710921527852e-05\n",
      "Iteration: 2290 Loss: 1.991523201600273e-05\n",
      "Iteration: 2291 Loss: 1.9903361170197593e-05\n",
      "Iteration: 2292 Loss: 1.9891496673719513e-05\n",
      "Iteration: 2293 Loss: 1.9879638522400227e-05\n",
      "Iteration: 2294 Loss: 1.9867786712184694e-05\n",
      "Iteration: 2295 Loss: 1.9855941238885646e-05\n",
      "Iteration: 2296 Loss: 1.9844102098158973e-05\n",
      "Iteration: 2297 Loss: 1.9832269286129947e-05\n",
      "Iteration: 2298 Loss: 1.9820442798569668e-05\n",
      "Iteration: 2299 Loss: 1.9808622631515533e-05\n",
      "Iteration: 2300 Loss: 1.979680878087757e-05\n",
      "Iteration: 2301 Loss: 1.978500124240645e-05\n",
      "Iteration: 2302 Loss: 1.9773200012106936e-05\n",
      "Iteration: 2303 Loss: 1.9761405085838472e-05\n",
      "Iteration: 2304 Loss: 1.9749616459516477e-05\n",
      "Iteration: 2305 Loss: 1.9737834129031736e-05\n",
      "Iteration: 2306 Loss: 1.9726058090427993e-05\n",
      "Iteration: 2307 Loss: 1.9714288339497066e-05\n",
      "Iteration: 2308 Loss: 1.970252487237791e-05\n",
      "Iteration: 2309 Loss: 1.969076768479903e-05\n",
      "Iteration: 2310 Loss: 1.9679016773016738e-05\n",
      "Iteration: 2311 Loss: 1.966727213282734e-05\n",
      "Iteration: 2312 Loss: 1.965553376046859e-05\n",
      "Iteration: 2313 Loss: 1.964380165162937e-05\n",
      "Iteration: 2314 Loss: 1.9632075802605716e-05\n",
      "Iteration: 2315 Loss: 1.9620356209067653e-05\n",
      "Iteration: 2316 Loss: 1.9608642867198788e-05\n",
      "Iteration: 2317 Loss: 1.9596935773013768e-05\n",
      "Iteration: 2318 Loss: 1.9585234922727694e-05\n",
      "Iteration: 2319 Loss: 1.9573540312124496e-05\n",
      "Iteration: 2320 Loss: 1.9561851937060704e-05\n",
      "Iteration: 2321 Loss: 1.955016979386488e-05\n",
      "Iteration: 2322 Loss: 1.9538493878867288e-05\n",
      "Iteration: 2323 Loss: 1.9526824187892887e-05\n",
      "Iteration: 2324 Loss: 1.9515160716971383e-05\n",
      "Iteration: 2325 Loss: 1.95035034620481e-05\n",
      "Iteration: 2326 Loss: 1.9491852419322987e-05\n",
      "Iteration: 2327 Loss: 1.94802075849412e-05\n",
      "Iteration: 2328 Loss: 1.9468568954958515e-05\n",
      "Iteration: 2329 Loss: 1.945693652529066e-05\n",
      "Iteration: 2330 Loss: 1.9445310292624343e-05\n",
      "Iteration: 2331 Loss: 1.943369025260058e-05\n",
      "Iteration: 2332 Loss: 1.942207640130061e-05\n",
      "Iteration: 2333 Loss: 1.9410468735032887e-05\n",
      "Iteration: 2334 Loss: 1.939886724997045e-05\n",
      "Iteration: 2335 Loss: 1.938727194216268e-05\n",
      "Iteration: 2336 Loss: 1.9375682808045876e-05\n",
      "Iteration: 2337 Loss: 1.936409984339374e-05\n",
      "Iteration: 2338 Loss: 1.935252304429003e-05\n",
      "Iteration: 2339 Loss: 1.9340952407219975e-05\n",
      "Iteration: 2340 Loss: 1.9329387928387195e-05\n",
      "Iteration: 2341 Loss: 1.9317829603819575e-05\n",
      "Iteration: 2342 Loss: 1.9306277429797733e-05\n",
      "Iteration: 2343 Loss: 1.9294731402419974e-05\n",
      "Iteration: 2344 Loss: 1.928319151783236e-05\n",
      "Iteration: 2345 Loss: 1.9271657772456893e-05\n",
      "Iteration: 2346 Loss: 1.9260130162204052e-05\n",
      "Iteration: 2347 Loss: 1.9248608683495275e-05\n",
      "Iteration: 2348 Loss: 1.9237093332648812e-05\n",
      "Iteration: 2349 Loss: 1.922558410581931e-05\n",
      "Iteration: 2350 Loss: 1.9214080999292623e-05\n",
      "Iteration: 2351 Loss: 1.9202584009192938e-05\n",
      "Iteration: 2352 Loss: 1.9191093131766266e-05\n",
      "Iteration: 2353 Loss: 1.9179608363633142e-05\n",
      "Iteration: 2354 Loss: 1.9168129700576573e-05\n",
      "Iteration: 2355 Loss: 1.9156657138976817e-05\n",
      "Iteration: 2356 Loss: 1.9145190675483497e-05\n",
      "Iteration: 2357 Loss: 1.9133730305846794e-05\n",
      "Iteration: 2358 Loss: 1.9122276026386627e-05\n",
      "Iteration: 2359 Loss: 1.911082783372289e-05\n",
      "Iteration: 2360 Loss: 1.909938572389756e-05\n",
      "Iteration: 2361 Loss: 1.9087949693364333e-05\n",
      "Iteration: 2362 Loss: 1.9076519738388654e-05\n",
      "Iteration: 2363 Loss: 1.906509585520038e-05\n",
      "Iteration: 2364 Loss: 1.9053678040567766e-05\n",
      "Iteration: 2365 Loss: 1.9042266290217512e-05\n",
      "Iteration: 2366 Loss: 1.903086060064892e-05\n",
      "Iteration: 2367 Loss: 1.9019460968238633e-05\n",
      "Iteration: 2368 Loss: 1.9008067389457375e-05\n",
      "Iteration: 2369 Loss: 1.8996679860382068e-05\n",
      "Iteration: 2370 Loss: 1.8985298377469273e-05\n",
      "Iteration: 2371 Loss: 1.8973922937177837e-05\n",
      "Iteration: 2372 Loss: 1.8962553535829263e-05\n",
      "Iteration: 2373 Loss: 1.8951190169896983e-05\n",
      "Iteration: 2374 Loss: 1.8939832835661888e-05\n",
      "Iteration: 2375 Loss: 1.892848152939921e-05\n",
      "Iteration: 2376 Loss: 1.8917136247578536e-05\n",
      "Iteration: 2377 Loss: 1.890579698677027e-05\n",
      "Iteration: 2378 Loss: 1.8894463743047024e-05\n",
      "Iteration: 2379 Loss: 1.888313651298338e-05\n",
      "Iteration: 2380 Loss: 1.8871815293154416e-05\n",
      "Iteration: 2381 Loss: 1.886050007939019e-05\n",
      "Iteration: 2382 Loss: 1.884919086864104e-05\n",
      "Iteration: 2383 Loss: 1.883788765721472e-05\n",
      "Iteration: 2384 Loss: 1.88265904414842e-05\n",
      "Iteration: 2385 Loss: 1.8815299217824226e-05\n",
      "Iteration: 2386 Loss: 1.8804013982778092e-05\n",
      "Iteration: 2387 Loss: 1.879273473262062e-05\n",
      "Iteration: 2388 Loss: 1.87814614642163e-05\n",
      "Iteration: 2389 Loss: 1.877019417352817e-05\n",
      "Iteration: 2390 Loss: 1.875893285705248e-05\n",
      "Iteration: 2391 Loss: 1.8747677511647393e-05\n",
      "Iteration: 2392 Loss: 1.8736428133321948e-05\n",
      "Iteration: 2393 Loss: 1.8725184718644797e-05\n",
      "Iteration: 2394 Loss: 1.8713947264336956e-05\n",
      "Iteration: 2395 Loss: 1.8702715766473135e-05\n",
      "Iteration: 2396 Loss: 1.8691490221873587e-05\n",
      "Iteration: 2397 Loss: 1.868027062712674e-05\n",
      "Iteration: 2398 Loss: 1.8669056978569886e-05\n",
      "Iteration: 2399 Loss: 1.8657849272398772e-05\n",
      "Iteration: 2400 Loss: 1.8646647505436656e-05\n",
      "Iteration: 2401 Loss: 1.8635451674091866e-05\n",
      "Iteration: 2402 Loss: 1.862426177481619e-05\n",
      "Iteration: 2403 Loss: 1.8613077804330408e-05\n",
      "Iteration: 2404 Loss: 1.860189975889116e-05\n",
      "Iteration: 2405 Loss: 1.8590727635221546e-05\n",
      "Iteration: 2406 Loss: 1.857956142983337e-05\n",
      "Iteration: 2407 Loss: 1.8568401139100746e-05\n",
      "Iteration: 2408 Loss: 1.855724675957964e-05\n",
      "Iteration: 2409 Loss: 1.8546098287843483e-05\n",
      "Iteration: 2410 Loss: 1.8534955720640905e-05\n",
      "Iteration: 2411 Loss: 1.8523819054285118e-05\n",
      "Iteration: 2412 Loss: 1.8512688285594204e-05\n",
      "Iteration: 2413 Loss: 1.850156341064423e-05\n",
      "Iteration: 2414 Loss: 1.8490444426592327e-05\n",
      "Iteration: 2415 Loss: 1.8479331329779734e-05\n",
      "Iteration: 2416 Loss: 1.8468224116781438e-05\n",
      "Iteration: 2417 Loss: 1.8457122784074255e-05\n",
      "Iteration: 2418 Loss: 1.8446027328325972e-05\n",
      "Iteration: 2419 Loss: 1.8434937745986463e-05\n",
      "Iteration: 2420 Loss: 1.8423854034076978e-05\n",
      "Iteration: 2421 Loss: 1.841277618869478e-05\n",
      "Iteration: 2422 Loss: 1.8401704206331505e-05\n",
      "Iteration: 2423 Loss: 1.8390638083788303e-05\n",
      "Iteration: 2424 Loss: 1.8379577817902552e-05\n",
      "Iteration: 2425 Loss: 1.836852340475177e-05\n",
      "Iteration: 2426 Loss: 1.8357474841583063e-05\n",
      "Iteration: 2427 Loss: 1.834643212493865e-05\n",
      "Iteration: 2428 Loss: 1.8335395250975016e-05\n",
      "Iteration: 2429 Loss: 1.8324364216661682e-05\n",
      "Iteration: 2430 Loss: 1.8313339018544322e-05\n",
      "Iteration: 2431 Loss: 1.8302319653240728e-05\n",
      "Iteration: 2432 Loss: 1.8291306117296928e-05\n",
      "Iteration: 2433 Loss: 1.8280298407430877e-05\n",
      "Iteration: 2434 Loss: 1.8269296520433496e-05\n",
      "Iteration: 2435 Loss: 1.825830045270175e-05\n",
      "Iteration: 2436 Loss: 1.824731020133657e-05\n",
      "Iteration: 2437 Loss: 1.823632576246654e-05\n",
      "Iteration: 2438 Loss: 1.822534713294194e-05\n",
      "Iteration: 2439 Loss: 1.821437430966731e-05\n",
      "Iteration: 2440 Loss: 1.820340728883492e-05\n",
      "Iteration: 2441 Loss: 1.8192446067596e-05\n",
      "Iteration: 2442 Loss: 1.8181490642267772e-05\n",
      "Iteration: 2443 Loss: 1.8170541009734408e-05\n",
      "Iteration: 2444 Loss: 1.8159597166494104e-05\n",
      "Iteration: 2445 Loss: 1.8148659109397e-05\n",
      "Iteration: 2446 Loss: 1.813772683506065e-05\n",
      "Iteration: 2447 Loss: 1.8126800340163337e-05\n",
      "Iteration: 2448 Loss: 1.811587962150494e-05\n",
      "Iteration: 2449 Loss: 1.8104964675444315e-05\n",
      "Iteration: 2450 Loss: 1.8094055499295975e-05\n",
      "Iteration: 2451 Loss: 1.808315208949825e-05\n",
      "Iteration: 2452 Loss: 1.8072254442594336e-05\n",
      "Iteration: 2453 Loss: 1.8061362555649997e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2454 Loss: 1.8050476424936765e-05\n",
      "Iteration: 2455 Loss: 1.80395960472792e-05\n",
      "Iteration: 2456 Loss: 1.8028721419371013e-05\n",
      "Iteration: 2457 Loss: 1.8017852538147822e-05\n",
      "Iteration: 2458 Loss: 1.800698940056643e-05\n",
      "Iteration: 2459 Loss: 1.7996132003010663e-05\n",
      "Iteration: 2460 Loss: 1.7985280342104323e-05\n",
      "Iteration: 2461 Loss: 1.7974434414906013e-05\n",
      "Iteration: 2462 Loss: 1.796359421804516e-05\n",
      "Iteration: 2463 Loss: 1.7952759748336502e-05\n",
      "Iteration: 2464 Loss: 1.794193100235078e-05\n",
      "Iteration: 2465 Loss: 1.7931107977080383e-05\n",
      "Iteration: 2466 Loss: 1.7920290669121044e-05\n",
      "Iteration: 2467 Loss: 1.790947907508847e-05\n",
      "Iteration: 2468 Loss: 1.7898673192079898e-05\n",
      "Iteration: 2469 Loss: 1.7887873016716786e-05\n",
      "Iteration: 2470 Loss: 1.7877078545715967e-05\n",
      "Iteration: 2471 Loss: 1.7866289775916314e-05\n",
      "Iteration: 2472 Loss: 1.7855506704024036e-05\n",
      "Iteration: 2473 Loss: 1.7844729327061066e-05\n",
      "Iteration: 2474 Loss: 1.783395764161613e-05\n",
      "Iteration: 2475 Loss: 1.782319164453064e-05\n",
      "Iteration: 2476 Loss: 1.781243133241839e-05\n",
      "Iteration: 2477 Loss: 1.7801676702508547e-05\n",
      "Iteration: 2478 Loss: 1.7790927751358512e-05\n",
      "Iteration: 2479 Loss: 1.7780184475776203e-05\n",
      "Iteration: 2480 Loss: 1.7769446872595445e-05\n",
      "Iteration: 2481 Loss: 1.7758714938548164e-05\n",
      "Iteration: 2482 Loss: 1.7747988670598916e-05\n",
      "Iteration: 2483 Loss: 1.7737268065465126e-05\n",
      "Iteration: 2484 Loss: 1.7726553120011154e-05\n",
      "Iteration: 2485 Loss: 1.771584383094588e-05\n",
      "Iteration: 2486 Loss: 1.770514019508955e-05\n",
      "Iteration: 2487 Loss: 1.769444220931765e-05\n",
      "Iteration: 2488 Loss: 1.7683749870427708e-05\n",
      "Iteration: 2489 Loss: 1.7673063175147532e-05\n",
      "Iteration: 2490 Loss: 1.766238212070294e-05\n",
      "Iteration: 2491 Loss: 1.765170670348325e-05\n",
      "Iteration: 2492 Loss: 1.7641036920651526e-05\n",
      "Iteration: 2493 Loss: 1.763037276916857e-05\n",
      "Iteration: 2494 Loss: 1.7619714245673363e-05\n",
      "Iteration: 2495 Loss: 1.7609061346997395e-05\n",
      "Iteration: 2496 Loss: 1.7598414070049556e-05\n",
      "Iteration: 2497 Loss: 1.7587772411782602e-05\n",
      "Iteration: 2498 Loss: 1.757713636882343e-05\n",
      "Iteration: 2499 Loss: 1.7566505938062185e-05\n",
      "Iteration: 2500 Loss: 1.7555881116197596e-05\n",
      "Iteration: 2501 Loss: 1.754526190076372e-05\n",
      "Iteration: 2502 Loss: 1.7534648288154454e-05\n",
      "Iteration: 2503 Loss: 1.7524040275345735e-05\n",
      "Iteration: 2504 Loss: 1.7513437859265477e-05\n",
      "Iteration: 2505 Loss: 1.750284103679516e-05\n",
      "Iteration: 2506 Loss: 1.749224980497719e-05\n",
      "Iteration: 2507 Loss: 1.7481664160436162e-05\n",
      "Iteration: 2508 Loss: 1.7471084099928786e-05\n",
      "Iteration: 2509 Loss: 1.7460509620339674e-05\n",
      "Iteration: 2510 Loss: 1.7449940719024917e-05\n",
      "Iteration: 2511 Loss: 1.7439377392844152e-05\n",
      "Iteration: 2512 Loss: 1.74288196381726e-05\n",
      "Iteration: 2513 Loss: 1.741826745253516e-05\n",
      "Iteration: 2514 Loss: 1.740772083222095e-05\n",
      "Iteration: 2515 Loss: 1.739717977472557e-05\n",
      "Iteration: 2516 Loss: 1.738664427682645e-05\n",
      "Iteration: 2517 Loss: 1.7376114335168277e-05\n",
      "Iteration: 2518 Loss: 1.7365589946945964e-05\n",
      "Iteration: 2519 Loss: 1.7355071108942294e-05\n",
      "Iteration: 2520 Loss: 1.7344557818141628e-05\n",
      "Iteration: 2521 Loss: 1.733405007130873e-05\n",
      "Iteration: 2522 Loss: 1.7323547865750776e-05\n",
      "Iteration: 2523 Loss: 1.7313051197972038e-05\n",
      "Iteration: 2524 Loss: 1.730256006522623e-05\n",
      "Iteration: 2525 Loss: 1.7292074464037434e-05\n",
      "Iteration: 2526 Loss: 1.7281594391812516e-05\n",
      "Iteration: 2527 Loss: 1.7271119845399373e-05\n",
      "Iteration: 2528 Loss: 1.7260650821551742e-05\n",
      "Iteration: 2529 Loss: 1.7250187317567825e-05\n",
      "Iteration: 2530 Loss: 1.723972933011565e-05\n",
      "Iteration: 2531 Loss: 1.7229276856227716e-05\n",
      "Iteration: 2532 Loss: 1.721882989282798e-05\n",
      "Iteration: 2533 Loss: 1.720838843659707e-05\n",
      "Iteration: 2534 Loss: 1.719795248498617e-05\n",
      "Iteration: 2535 Loss: 1.718752203487145e-05\n",
      "Iteration: 2536 Loss: 1.7177097082900673e-05\n",
      "Iteration: 2537 Loss: 1.7166677626478775e-05\n",
      "Iteration: 2538 Loss: 1.7156263662542974e-05\n",
      "Iteration: 2539 Loss: 1.714585518782036e-05\n",
      "Iteration: 2540 Loss: 1.7135452199316e-05\n",
      "Iteration: 2541 Loss: 1.7125054694083053e-05\n",
      "Iteration: 2542 Loss: 1.711466266930556e-05\n",
      "Iteration: 2543 Loss: 1.7104276121578114e-05\n",
      "Iteration: 2544 Loss: 1.7093895047779992e-05\n",
      "Iteration: 2545 Loss: 1.7083519445601762e-05\n",
      "Iteration: 2546 Loss: 1.707314931164393e-05\n",
      "Iteration: 2547 Loss: 1.706278464274341e-05\n",
      "Iteration: 2548 Loss: 1.7052425436087304e-05\n",
      "Iteration: 2549 Loss: 1.7042071688475997e-05\n",
      "Iteration: 2550 Loss: 1.7031723397288442e-05\n",
      "Iteration: 2551 Loss: 1.702138055944962e-05\n",
      "Iteration: 2552 Loss: 1.7011043171731337e-05\n",
      "Iteration: 2553 Loss: 1.700071123127539e-05\n",
      "Iteration: 2554 Loss: 1.6990384735142608e-05\n",
      "Iteration: 2555 Loss: 1.6980063680271136e-05\n",
      "Iteration: 2556 Loss: 1.6969748063443993e-05\n",
      "Iteration: 2557 Loss: 1.695943788229486e-05\n",
      "Iteration: 2558 Loss: 1.6949133133424098e-05\n",
      "Iteration: 2559 Loss: 1.6938833814070895e-05\n",
      "Iteration: 2560 Loss: 1.6928539921010683e-05\n",
      "Iteration: 2561 Loss: 1.6918251451192428e-05\n",
      "Iteration: 2562 Loss: 1.6907968402227832e-05\n",
      "Iteration: 2563 Loss: 1.689769077054681e-05\n",
      "Iteration: 2564 Loss: 1.688741855344546e-05\n",
      "Iteration: 2565 Loss: 1.687715174789765e-05\n",
      "Iteration: 2566 Loss: 1.6866890351071272e-05\n",
      "Iteration: 2567 Loss: 1.685663436008872e-05\n",
      "Iteration: 2568 Loss: 1.684638377155887e-05\n",
      "Iteration: 2569 Loss: 1.6836138583121557e-05\n",
      "Iteration: 2570 Loss: 1.6825898791464278e-05\n",
      "Iteration: 2571 Loss: 1.681566439353535e-05\n",
      "Iteration: 2572 Loss: 1.6805435386660416e-05\n",
      "Iteration: 2573 Loss: 1.679521176780952e-05\n",
      "Iteration: 2574 Loss: 1.678499353388766e-05\n",
      "Iteration: 2575 Loss: 1.6774780682362142e-05\n",
      "Iteration: 2576 Loss: 1.6764573209972503e-05\n",
      "Iteration: 2577 Loss: 1.6754371114099523e-05\n",
      "Iteration: 2578 Loss: 1.674417439145122e-05\n",
      "Iteration: 2579 Loss: 1.673398303910191e-05\n",
      "Iteration: 2580 Loss: 1.6723797054352294e-05\n",
      "Iteration: 2581 Loss: 1.671361643425777e-05\n",
      "Iteration: 2582 Loss: 1.6703441175784367e-05\n",
      "Iteration: 2583 Loss: 1.6693271276166912e-05\n",
      "Iteration: 2584 Loss: 1.668310673219164e-05\n",
      "Iteration: 2585 Loss: 1.667294754108355e-05\n",
      "Iteration: 2586 Loss: 1.666279370015516e-05\n",
      "Iteration: 2587 Loss: 1.6652645206327865e-05\n",
      "Iteration: 2588 Loss: 1.6642502056812232e-05\n",
      "Iteration: 2589 Loss: 1.663236424875097e-05\n",
      "Iteration: 2590 Loss: 1.662223177894935e-05\n",
      "Iteration: 2591 Loss: 1.6612104644701493e-05\n",
      "Iteration: 2592 Loss: 1.6601982842976027e-05\n",
      "Iteration: 2593 Loss: 1.659186637095676e-05\n",
      "Iteration: 2594 Loss: 1.6581755225822983e-05\n",
      "Iteration: 2595 Loss: 1.6571649404558236e-05\n",
      "Iteration: 2596 Loss: 1.6561548904328846e-05\n",
      "Iteration: 2597 Loss: 1.6551453722358368e-05\n",
      "Iteration: 2598 Loss: 1.6541363855581708e-05\n",
      "Iteration: 2599 Loss: 1.6531279301327175e-05\n",
      "Iteration: 2600 Loss: 1.652120005642512e-05\n",
      "Iteration: 2601 Loss: 1.6511126118233243e-05\n",
      "Iteration: 2602 Loss: 1.6501057483882763e-05\n",
      "Iteration: 2603 Loss: 1.649099415054189e-05\n",
      "Iteration: 2604 Loss: 1.648093611522168e-05\n",
      "Iteration: 2605 Loss: 1.6470883374986644e-05\n",
      "Iteration: 2606 Loss: 1.6460835926993077e-05\n",
      "Iteration: 2607 Loss: 1.6450793768495804e-05\n",
      "Iteration: 2608 Loss: 1.644075689651558e-05\n",
      "Iteration: 2609 Loss: 1.6430725308316605e-05\n",
      "Iteration: 2610 Loss: 1.6420699000961698e-05\n",
      "Iteration: 2611 Loss: 1.6410677971598777e-05\n",
      "Iteration: 2612 Loss: 1.640066221753141e-05\n",
      "Iteration: 2613 Loss: 1.6390651735851587e-05\n",
      "Iteration: 2614 Loss: 1.6380646523234304e-05\n",
      "Iteration: 2615 Loss: 1.6370646577266023e-05\n",
      "Iteration: 2616 Loss: 1.636065189519365e-05\n",
      "Iteration: 2617 Loss: 1.6350662474117905e-05\n",
      "Iteration: 2618 Loss: 1.6340678310953016e-05\n",
      "Iteration: 2619 Loss: 1.6330699402994234e-05\n",
      "Iteration: 2620 Loss: 1.6320725747450302e-05\n",
      "Iteration: 2621 Loss: 1.631075734162607e-05\n",
      "Iteration: 2622 Loss: 1.6300794182445122e-05\n",
      "Iteration: 2623 Loss: 1.6290836267257636e-05\n",
      "Iteration: 2624 Loss: 1.6280883592933717e-05\n",
      "Iteration: 2625 Loss: 1.6270936156884358e-05\n",
      "Iteration: 2626 Loss: 1.626099395636523e-05\n",
      "Iteration: 2627 Loss: 1.625105698847443e-05\n",
      "Iteration: 2628 Loss: 1.6241125250278203e-05\n",
      "Iteration: 2629 Loss: 1.623119873905889e-05\n",
      "Iteration: 2630 Loss: 1.6221277452010127e-05\n",
      "Iteration: 2631 Loss: 1.6211361386555915e-05\n",
      "Iteration: 2632 Loss: 1.6201450539415596e-05\n",
      "Iteration: 2633 Loss: 1.619154490803015e-05\n",
      "Iteration: 2634 Loss: 1.618164448946513e-05\n",
      "Iteration: 2635 Loss: 1.617174928085043e-05\n",
      "Iteration: 2636 Loss: 1.616185927945283e-05\n",
      "Iteration: 2637 Loss: 1.6151974482544942e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2638 Loss: 1.6142094887469036e-05\n",
      "Iteration: 2639 Loss: 1.613222049134319e-05\n",
      "Iteration: 2640 Loss: 1.612235129107079e-05\n",
      "Iteration: 2641 Loss: 1.6112487284153482e-05\n",
      "Iteration: 2642 Loss: 1.6102628467959825e-05\n",
      "Iteration: 2643 Loss: 1.6092774839318908e-05\n",
      "Iteration: 2644 Loss: 1.6082926395396812e-05\n",
      "Iteration: 2645 Loss: 1.607308313381424e-05\n",
      "Iteration: 2646 Loss: 1.6063245051623618e-05\n",
      "Iteration: 2647 Loss: 1.605341214584209e-05\n",
      "Iteration: 2648 Loss: 1.6043584413896862e-05\n",
      "Iteration: 2649 Loss: 1.6033761853021093e-05\n",
      "Iteration: 2650 Loss: 1.6023944460533593e-05\n",
      "Iteration: 2651 Loss: 1.601413223331587e-05\n",
      "Iteration: 2652 Loss: 1.600432516894476e-05\n",
      "Iteration: 2653 Loss: 1.5994523264335178e-05\n",
      "Iteration: 2654 Loss: 1.5984726516807345e-05\n",
      "Iteration: 2655 Loss: 1.5974934923676176e-05\n",
      "Iteration: 2656 Loss: 1.5965148482213887e-05\n",
      "Iteration: 2657 Loss: 1.595536718958369e-05\n",
      "Iteration: 2658 Loss: 1.5945591042857605e-05\n",
      "Iteration: 2659 Loss: 1.5935820039641554e-05\n",
      "Iteration: 2660 Loss: 1.5926054176887358e-05\n",
      "Iteration: 2661 Loss: 1.5916293451799605e-05\n",
      "Iteration: 2662 Loss: 1.5906537861819956e-05\n",
      "Iteration: 2663 Loss: 1.589678740421447e-05\n",
      "Iteration: 2664 Loss: 1.5887042076061824e-05\n",
      "Iteration: 2665 Loss: 1.5877301874843083e-05\n",
      "Iteration: 2666 Loss: 1.5867566797564855e-05\n",
      "Iteration: 2667 Loss: 1.585783684147617e-05\n",
      "Iteration: 2668 Loss: 1.584811200396313e-05\n",
      "Iteration: 2669 Loss: 1.5838392282413485e-05\n",
      "Iteration: 2670 Loss: 1.582867767387888e-05\n",
      "Iteration: 2671 Loss: 1.5818968175628715e-05\n",
      "Iteration: 2672 Loss: 1.5809263785088764e-05\n",
      "Iteration: 2673 Loss: 1.5799564499405152e-05\n",
      "Iteration: 2674 Loss: 1.5789870315750075e-05\n",
      "Iteration: 2675 Loss: 1.5780181231650036e-05\n",
      "Iteration: 2676 Loss: 1.577049724409061e-05\n",
      "Iteration: 2677 Loss: 1.576081835046878e-05\n",
      "Iteration: 2678 Loss: 1.5751144547976763e-05\n",
      "Iteration: 2679 Loss: 1.574147583384298e-05\n",
      "Iteration: 2680 Loss: 1.573181220557793e-05\n",
      "Iteration: 2681 Loss: 1.572215366039685e-05\n",
      "Iteration: 2682 Loss: 1.5712500195563217e-05\n",
      "Iteration: 2683 Loss: 1.570285180825348e-05\n",
      "Iteration: 2684 Loss: 1.5693208495938332e-05\n",
      "Iteration: 2685 Loss: 1.5683570255642046e-05\n",
      "Iteration: 2686 Loss: 1.5673937085154442e-05\n",
      "Iteration: 2687 Loss: 1.566430898127298e-05\n",
      "Iteration: 2688 Loss: 1.565468594140649e-05\n",
      "Iteration: 2689 Loss: 1.5645067962861382e-05\n",
      "Iteration: 2690 Loss: 1.5635455043082806e-05\n",
      "Iteration: 2691 Loss: 1.562584717907452e-05\n",
      "Iteration: 2692 Loss: 1.5616244368483325e-05\n",
      "Iteration: 2693 Loss: 1.5606646608394635e-05\n",
      "Iteration: 2694 Loss: 1.5597053896066846e-05\n",
      "Iteration: 2695 Loss: 1.55874662288323e-05\n",
      "Iteration: 2696 Loss: 1.557788360408047e-05\n",
      "Iteration: 2697 Loss: 1.5568306019184288e-05\n",
      "Iteration: 2698 Loss: 1.5558733471452854e-05\n",
      "Iteration: 2699 Loss: 1.554916595809665e-05\n",
      "Iteration: 2700 Loss: 1.553960347641566e-05\n",
      "Iteration: 2701 Loss: 1.5530046023766624e-05\n",
      "Iteration: 2702 Loss: 1.5520493597423192e-05\n",
      "Iteration: 2703 Loss: 1.5510946194318306e-05\n",
      "Iteration: 2704 Loss: 1.5501403812398172e-05\n",
      "Iteration: 2705 Loss: 1.5491866449006863e-05\n",
      "Iteration: 2706 Loss: 1.5482334101049745e-05\n",
      "Iteration: 2707 Loss: 1.5472806765870726e-05\n",
      "Iteration: 2708 Loss: 1.546328444110168e-05\n",
      "Iteration: 2709 Loss: 1.54537671237471e-05\n",
      "Iteration: 2710 Loss: 1.5444254811291533e-05\n",
      "Iteration: 2711 Loss: 1.543474750137129e-05\n",
      "Iteration: 2712 Loss: 1.5425245190899203e-05\n",
      "Iteration: 2713 Loss: 1.5415747877389863e-05\n",
      "Iteration: 2714 Loss: 1.5406255558241542e-05\n",
      "Iteration: 2715 Loss: 1.5396768230754645e-05\n",
      "Iteration: 2716 Loss: 1.538728589189576e-05\n",
      "Iteration: 2717 Loss: 1.5377808539309095e-05\n",
      "Iteration: 2718 Loss: 1.536833617020673e-05\n",
      "Iteration: 2719 Loss: 1.535886878239223e-05\n",
      "Iteration: 2720 Loss: 1.534940637284644e-05\n",
      "Iteration: 2721 Loss: 1.533994893900806e-05\n",
      "Iteration: 2722 Loss: 1.5330496478165512e-05\n",
      "Iteration: 2723 Loss: 1.532104898771875e-05\n",
      "Iteration: 2724 Loss: 1.531160646502979e-05\n",
      "Iteration: 2725 Loss: 1.530216890737697e-05\n",
      "Iteration: 2726 Loss: 1.5292736312094133e-05\n",
      "Iteration: 2727 Loss: 1.5283308676685347e-05\n",
      "Iteration: 2728 Loss: 1.5273885998272644e-05\n",
      "Iteration: 2729 Loss: 1.5264468274251706e-05\n",
      "Iteration: 2730 Loss: 1.5255055502477431e-05\n",
      "Iteration: 2731 Loss: 1.5245647679861902e-05\n",
      "Iteration: 2732 Loss: 1.5236244803708552e-05\n",
      "Iteration: 2733 Loss: 1.5226846871556388e-05\n",
      "Iteration: 2734 Loss: 1.5217453880666157e-05\n",
      "Iteration: 2735 Loss: 1.5208065828611797e-05\n",
      "Iteration: 2736 Loss: 1.5198682712530383e-05\n",
      "Iteration: 2737 Loss: 1.5189304530067128e-05\n",
      "Iteration: 2738 Loss: 1.5179931278540852e-05\n",
      "Iteration: 2739 Loss: 1.5170562955087339e-05\n",
      "Iteration: 2740 Loss: 1.5161199557258585e-05\n",
      "Iteration: 2741 Loss: 1.5151841082321272e-05\n",
      "Iteration: 2742 Loss: 1.5142487527881472e-05\n",
      "Iteration: 2743 Loss: 1.5133138891172986e-05\n",
      "Iteration: 2744 Loss: 1.512379516959722e-05\n",
      "Iteration: 2745 Loss: 1.5114456360399e-05\n",
      "Iteration: 2746 Loss: 1.5105122461230825e-05\n",
      "Iteration: 2747 Loss: 1.5095793469441017e-05\n",
      "Iteration: 2748 Loss: 1.5086469382428634e-05\n",
      "Iteration: 2749 Loss: 1.507715019749369e-05\n",
      "Iteration: 2750 Loss: 1.5067835912164794e-05\n",
      "Iteration: 2751 Loss: 1.5058526523667594e-05\n",
      "Iteration: 2752 Loss: 1.5049222029356493e-05\n",
      "Iteration: 2753 Loss: 1.5039922426670236e-05\n",
      "Iteration: 2754 Loss: 1.5030627713077845e-05\n",
      "Iteration: 2755 Loss: 1.5021337886127539e-05\n",
      "Iteration: 2756 Loss: 1.5012052943066743e-05\n",
      "Iteration: 2757 Loss: 1.500277288126299e-05\n",
      "Iteration: 2758 Loss: 1.4993497698358261e-05\n",
      "Iteration: 2759 Loss: 1.4984227391613796e-05\n",
      "Iteration: 2760 Loss: 1.4974961958358284e-05\n",
      "Iteration: 2761 Loss: 1.4965701396087657e-05\n",
      "Iteration: 2762 Loss: 1.4956445702242985e-05\n",
      "Iteration: 2763 Loss: 1.4947194874212535e-05\n",
      "Iteration: 2764 Loss: 1.4937948909403747e-05\n",
      "Iteration: 2765 Loss: 1.4928707805330682e-05\n",
      "Iteration: 2766 Loss: 1.4919471559346246e-05\n",
      "Iteration: 2767 Loss: 1.4910240168719183e-05\n",
      "Iteration: 2768 Loss: 1.4901013631127648e-05\n",
      "Iteration: 2769 Loss: 1.4891791943703397e-05\n",
      "Iteration: 2770 Loss: 1.4882575104344363e-05\n",
      "Iteration: 2771 Loss: 1.4873363110138387e-05\n",
      "Iteration: 2772 Loss: 1.4864155958618567e-05\n",
      "Iteration: 2773 Loss: 1.4854953647288972e-05\n",
      "Iteration: 2774 Loss: 1.4845756173540604e-05\n",
      "Iteration: 2775 Loss: 1.4836563534729296e-05\n",
      "Iteration: 2776 Loss: 1.4827375728250127e-05\n",
      "Iteration: 2777 Loss: 1.4818192751672385e-05\n",
      "Iteration: 2778 Loss: 1.4809014602699534e-05\n",
      "Iteration: 2779 Loss: 1.4799841278419995e-05\n",
      "Iteration: 2780 Loss: 1.4790672776537489e-05\n",
      "Iteration: 2781 Loss: 1.4781509094287334e-05\n",
      "Iteration: 2782 Loss: 1.4772350228917912e-05\n",
      "Iteration: 2783 Loss: 1.4763196178181656e-05\n",
      "Iteration: 2784 Loss: 1.4754046939537634e-05\n",
      "Iteration: 2785 Loss: 1.4744902510740846e-05\n",
      "Iteration: 2786 Loss: 1.4735762889058426e-05\n",
      "Iteration: 2787 Loss: 1.4726628071635774e-05\n",
      "Iteration: 2788 Loss: 1.4717498056329868e-05\n",
      "Iteration: 2789 Loss: 1.4708372840735238e-05\n",
      "Iteration: 2790 Loss: 1.4699252422307909e-05\n",
      "Iteration: 2791 Loss: 1.4690136798166243e-05\n",
      "Iteration: 2792 Loss: 1.4681025966252708e-05\n",
      "Iteration: 2793 Loss: 1.4671919923823737e-05\n",
      "Iteration: 2794 Loss: 1.4662818668384451e-05\n",
      "Iteration: 2795 Loss: 1.4653722197368052e-05\n",
      "Iteration: 2796 Loss: 1.4644630508617385e-05\n",
      "Iteration: 2797 Loss: 1.463554359956212e-05\n",
      "Iteration: 2798 Loss: 1.4626461467663466e-05\n",
      "Iteration: 2799 Loss: 1.4617384110995947e-05\n",
      "Iteration: 2800 Loss: 1.4608311526872852e-05\n",
      "Iteration: 2801 Loss: 1.4599243712593838e-05\n",
      "Iteration: 2802 Loss: 1.459018066644275e-05\n",
      "Iteration: 2803 Loss: 1.4581122385898751e-05\n",
      "Iteration: 2804 Loss: 1.4572068868762768e-05\n",
      "Iteration: 2805 Loss: 1.4563020113186235e-05\n",
      "Iteration: 2806 Loss: 1.4553976116812002e-05\n",
      "Iteration: 2807 Loss: 1.454493687816745e-05\n",
      "Iteration: 2808 Loss: 1.4535902395022129e-05\n",
      "Iteration: 2809 Loss: 1.4526872666019963e-05\n",
      "Iteration: 2810 Loss: 1.4517847690083599e-05\n",
      "Iteration: 2811 Loss: 1.450882746637922e-05\n",
      "Iteration: 2812 Loss: 1.449981199525343e-05\n",
      "Iteration: 2813 Loss: 1.4490801277316979e-05\n",
      "Iteration: 2814 Loss: 1.4481795315480081e-05\n",
      "Iteration: 2815 Loss: 1.4472794114292836e-05\n",
      "Iteration: 2816 Loss: 1.4463797684007545e-05\n",
      "Iteration: 2817 Loss: 1.4454806042728864e-05\n",
      "Iteration: 2818 Loss: 1.4445819226487532e-05\n",
      "Iteration: 2819 Loss: 1.4436837311707418e-05\n",
      "Iteration: 2820 Loss: 1.442786049107111e-05\n",
      "Iteration: 2821 Loss: 1.4418889399605306e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2822 Loss: 1.4409928065246318e-05\n",
      "Iteration: 2823 Loss: 1.4256792609835707e-05\n",
      "Iteration: 2824 Loss: 1.3574759770423325e-05\n",
      "Iteration: 2825 Loss: 1.2487862337175585e-05\n",
      "Iteration: 2826 Loss: 1.1120415019042104e-05\n",
      "Iteration: 2827 Loss: 9.583041035425202e-06\n",
      "Iteration: 2828 Loss: 7.971016507991355e-06\n",
      "Iteration: 2829 Loss: 6.363561566962442e-06\n",
      "Iteration: 2830 Loss: 4.823841249981002e-06\n",
      "Iteration: 2831 Loss: 3.399693041479925e-06\n",
      "Iteration: 2832 Loss: 2.124789868654993e-06\n",
      "Iteration: 2833 Loss: 1.0201209039955709e-06\n",
      "Iteration: 2834 Loss: 3.236524683124801e-07\n",
      "Iteration: 2835 Loss: 6.69162335656813e-07\n",
      "Iteration: 2836 Loss: 1.2336712488269003e-06\n",
      "Iteration: 2837 Loss: 1.6360383589772957e-06\n",
      "Iteration: 2838 Loss: 1.8926386757218621e-06\n",
      "Iteration: 2839 Loss: 2.02239007177435e-06\n",
      "Iteration: 2840 Loss: 2.0456483364000337e-06\n",
      "Iteration: 2841 Loss: 1.9833663577461286e-06\n",
      "Iteration: 2842 Loss: 1.855398422410907e-06\n",
      "Iteration: 2843 Loss: 1.679416172670959e-06\n",
      "Iteration: 2844 Loss: 1.4716546795099577e-06\n",
      "Iteration: 2845 Loss: 1.2467543858746366e-06\n",
      "Iteration: 2846 Loss: 1.0171127803374077e-06\n",
      "Iteration: 2847 Loss: 7.928343279706386e-07\n",
      "Iteration: 2848 Loss: 5.817904983692063e-07\n",
      "Iteration: 2849 Loss: 3.897511746933943e-07\n",
      "Iteration: 2850 Loss: 2.2057456373216758e-07\n",
      "Iteration: 2851 Loss: 7.643558837565073e-08\n",
      "Iteration: 2852 Loss: 4.602614058453724e-08\n",
      "Iteration: 2853 Loss: 1.382841695516662e-07\n",
      "Iteration: 2854 Loss: 2.0657259588411385e-07\n",
      "Iteration: 2855 Loss: 2.5296109472300473e-07\n",
      "Iteration: 2856 Loss: 2.79979323377771e-07\n",
      "Iteration: 2857 Loss: 2.9044731562792306e-07\n",
      "Iteration: 2858 Loss: 2.872637699534269e-07\n",
      "Iteration: 2859 Loss: 2.7340140603150647e-07\n",
      "Iteration: 2860 Loss: 2.515771134441864e-07\n",
      "Iteration: 2861 Loss: 2.240846906431463e-07\n",
      "Iteration: 2862 Loss: 1.931539287484693e-07\n",
      "Iteration: 2863 Loss: 1.6070601760558633e-07\n",
      "Iteration: 2864 Loss: 1.2833850150085414e-07\n",
      "Iteration: 2865 Loss: 9.732626309812782e-08\n",
      "Iteration: 2866 Loss: 6.863519063727694e-08\n",
      "Iteration: 2867 Loss: 4.2945441161458605e-08\n",
      "Iteration: 2868 Loss: 2.0681449699043264e-08\n",
      "Iteration: 2869 Loss: 2.0461877782161916e-09\n",
      "Iteration: 2870 Loss: 1.35758303742978e-08\n",
      "Iteration: 2871 Loss: 2.4931229711565258e-08\n",
      "Iteration: 2872 Loss: 3.302249992672469e-08\n",
      "Iteration: 2873 Loss: 3.81797780671374e-08\n",
      "Iteration: 2874 Loss: 4.078644446745125e-08\n",
      "Iteration: 2875 Loss: 4.124942076920046e-08\n",
      "Iteration: 2876 Loss: 3.998559441545045e-08\n",
      "Iteration: 2877 Loss: 3.74170309887514e-08\n",
      "Iteration: 2878 Loss: 3.3876610903031924e-08\n",
      "Iteration: 2879 Loss: 2.969478087520073e-08\n",
      "Iteration: 2880 Loss: 2.5166560549972912e-08\n",
      "Iteration: 2881 Loss: 2.0541822628442697e-08\n",
      "Iteration: 2882 Loss: 1.6024329552664614e-08\n",
      "Iteration: 2883 Loss: 1.177282038161597e-08\n",
      "Iteration: 2884 Loss: 7.903683060759136e-09\n",
      "Iteration: 2885 Loss: 4.494781502012965e-09\n",
      "Iteration: 2886 Loss: 1.5900767052961193e-09\n",
      "Iteration: 2887 Loss: 9.55230487679481e-10\n",
      "Iteration: 2888 Loss: 2.8123314079465517e-09\n",
      "Iteration: 2889 Loss: 4.186878011512714e-09\n",
      "Iteration: 2890 Loss: 5.120507406383723e-09\n",
      "Iteration: 2891 Loss: 5.664187423120456e-09\n",
      "Iteration: 2892 Loss: 5.874409087272092e-09\n",
      "Iteration: 2893 Loss: 5.810283639883345e-09\n",
      "Iteration: 2894 Loss: 5.532511235391733e-09\n",
      "Iteration: 2895 Loss: 5.09256750396366e-09\n",
      "Iteration: 2896 Loss: 4.538541377038357e-09\n",
      "Iteration: 2897 Loss: 3.9153170416167155e-09\n",
      "Iteration: 2898 Loss: 3.261572107549859e-09\n",
      "Iteration: 2899 Loss: 2.60947538684697e-09\n",
      "Iteration: 2900 Loss: 1.984704029259084e-09\n",
      "Iteration: 2901 Loss: 1.4067166202467053e-09\n",
      "Iteration: 2902 Loss: 8.892248943895057e-10\n",
      "Iteration: 2903 Loss: 4.4083559650392046e-10\n",
      "Iteration: 2904 Loss: 6.865981238995124e-11\n",
      "Iteration: 2905 Loss: 3.147733479017183e-10\n",
      "Iteration: 2906 Loss: 5.433132277493219e-10\n",
      "Iteration: 2907 Loss: 7.062275527772004e-10\n",
      "Iteration: 2908 Loss: 8.100725246977537e-10\n",
      "Iteration: 2909 Loss: 8.625005546716597e-10\n",
      "Iteration: 2910 Loss: 8.716926953375207e-10\n",
      "Iteration: 2911 Loss: 8.459210406760619e-10\n",
      "Iteration: 2912 Loss: 7.931996170192353e-10\n",
      "Iteration: 2913 Loss: 7.215957370095095e-10\n",
      "Iteration: 2914 Loss: 6.371486203667278e-10\n",
      "Iteration: 2915 Loss: 5.457640272891265e-10\n",
      "Iteration: 2916 Loss: 4.5247463309257673e-10\n",
      "Iteration: 2917 Loss: 3.6138755848643956e-10\n",
      "Iteration: 2918 Loss: 2.757125700538495e-10\n",
      "Iteration: 2919 Loss: 1.9783118170542025e-10\n",
      "Iteration: 2920 Loss: 1.2944043056951729e-10\n",
      "Iteration: 2921 Loss: 7.218259428650158e-11\n",
      "Iteration: 2922 Loss: 4.6785333379288556e-11\n",
      "Iteration: 2923 Loss: 7.861315297920667e-11\n",
      "Iteration: 2924 Loss: 1.0555299275215889e-10\n",
      "Iteration: 2925 Loss: 1.2422800601616869e-10\n",
      "Iteration: 2926 Loss: 1.351666041530881e-10\n",
      "Iteration: 2927 Loss: 1.3942208715099083e-10\n",
      "Iteration: 2928 Loss: 1.3814280573344635e-10\n",
      "Iteration: 2929 Loss: 1.3248205134278188e-10\n",
      "Iteration: 2930 Loss: 1.2354658380538024e-10\n",
      "Iteration: 2931 Loss: 1.123617255579037e-10\n",
      "Iteration: 2932 Loss: 9.985103234421638e-11\n",
      "Iteration: 2933 Loss: 8.683045165517812e-11\n",
      "Iteration: 2934 Loss: 7.401957073514894e-11\n",
      "Iteration: 2935 Loss: 6.209120359309413e-11\n",
      "Iteration: 2936 Loss: 5.1817518448771445e-11\n",
      "Iteration: 2937 Loss: 4.4468384766423505e-11\n",
      "Iteration: 2938 Loss: 4.199170218852522e-11\n",
      "Iteration: 2939 Loss: 4.409724107544883e-11\n",
      "Iteration: 2940 Loss: 4.7744593246385416e-11\n",
      "Iteration: 2941 Loss: 5.1139370777829035e-11\n",
      "Iteration: 2942 Loss: 5.371964109714998e-11\n",
      "Iteration: 2943 Loss: 5.5372622131812975e-11\n",
      "Iteration: 2944 Loss: 5.614895140690308e-11\n",
      "Iteration: 2945 Loss: 5.616419419847998e-11\n",
      "Iteration: 2946 Loss: 5.5560304595402473e-11\n",
      "Iteration: 2947 Loss: 5.4486396847867635e-11\n",
      "Iteration: 2948 Loss: 5.308702530432344e-11\n",
      "Iteration: 2949 Loss: 5.149533194552041e-11\n",
      "Iteration: 2950 Loss: 4.982836087277125e-11\n",
      "Iteration: 2951 Loss: 4.81837797558244e-11\n",
      "Iteration: 2952 Loss: 4.663728388281756e-11\n",
      "Iteration: 2953 Loss: 4.524185217500872e-11\n",
      "Iteration: 2954 Loss: 4.4028288233760694e-11\n",
      "Iteration: 2955 Loss: 4.3006851495900975e-11\n",
      "Iteration: 2956 Loss: 4.217140211938368e-11\n",
      "Iteration: 2957 Loss: 4.150399832017409e-11\n",
      "Iteration: 2958 Loss: 4.098045721092612e-11\n",
      "Iteration: 2959 Loss: 4.057649107415591e-11\n",
      "Iteration: 2960 Loss: 4.0268717066809216e-11\n",
      "Iteration: 2961 Loss: 4.0037935247005264e-11\n",
      "Iteration: 2962 Loss: 3.9868998102248083e-11\n",
      "Iteration: 2963 Loss: 3.974953250364246e-11\n",
      "Iteration: 2964 Loss: 3.9669538193863485e-11\n",
      "Iteration: 2965 Loss: 3.9620719800440625e-11\n",
      "Iteration: 2966 Loss: 3.959523995473178e-11\n",
      "Iteration: 2967 Loss: 3.958621752415297e-11\n",
      "Iteration: 2968 Loss: 3.958700124992376e-11\n",
      "Iteration: 2969 Loss: 3.9591401724491183e-11\n",
      "Iteration: 2970 Loss: 3.959417720381812e-11\n",
      "Iteration: 2971 Loss: 3.959039339489881e-11\n",
      "Iteration: 2972 Loss: 3.9577086950474156e-11\n",
      "Iteration: 2973 Loss: 3.955121021714142e-11\n",
      "Iteration: 2974 Loss: 3.951149698492757e-11\n",
      "Iteration: 2975 Loss: 3.945704775579199e-11\n",
      "Iteration: 2976 Loss: 3.9388213071459333e-11\n",
      "Iteration: 2977 Loss: 3.9305618210497156e-11\n",
      "Iteration: 2978 Loss: 3.9210467926142056e-11\n",
      "Iteration: 2979 Loss: 3.910471167241063e-11\n",
      "Iteration: 2980 Loss: 3.899008868807543e-11\n",
      "Iteration: 2981 Loss: 3.886828016016452e-11\n",
      "Iteration: 2982 Loss: 3.8741129398399954e-11\n",
      "Iteration: 2983 Loss: 3.8610519091842134e-11\n",
      "Iteration: 2984 Loss: 3.847793686218602e-11\n",
      "Iteration: 2985 Loss: 3.834455729066046e-11\n",
      "Iteration: 2986 Loss: 3.821173855721197e-11\n",
      "Iteration: 2987 Loss: 3.8080282294337714e-11\n",
      "Iteration: 2988 Loss: 3.795077263764663e-11\n",
      "Iteration: 2989 Loss: 3.7823908328060916e-11\n",
      "Iteration: 2990 Loss: 3.7699957232486624e-11\n",
      "Iteration: 2991 Loss: 3.757908495696105e-11\n",
      "Iteration: 2992 Loss: 3.746126233307856e-11\n",
      "Iteration: 2993 Loss: 3.7346563088133347e-11\n",
      "Iteration: 2994 Loss: 3.723478710177915e-11\n",
      "Iteration: 2995 Loss: 3.712563546023983e-11\n",
      "Iteration: 2996 Loss: 3.7019074597647635e-11\n",
      "Iteration: 2997 Loss: 3.6914862586593975e-11\n",
      "Iteration: 2998 Loss: 3.6812728451910034e-11\n",
      "Iteration: 2999 Loss: 3.671208652165323e-11\n",
      "Iteration: 3000 Loss: 3.6612882973835896e-11\n",
      "Iteration: 3001 Loss: 3.651490417886905e-11\n",
      "Iteration: 3002 Loss: 3.6417953873655015e-11\n",
      "Iteration: 3003 Loss: 3.632185423965784e-11\n",
      "Iteration: 3004 Loss: 3.6226399396677277e-11\n",
      "Iteration: 3005 Loss: 3.6131562131186456e-11\n",
      "Iteration: 3006 Loss: 3.603720322522407e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3007 Loss: 3.594327420845773e-11\n",
      "Iteration: 3008 Loss: 3.5849698950952667e-11\n",
      "Iteration: 3009 Loss: 3.575651243696858e-11\n",
      "Iteration: 3010 Loss: 3.566365939495602e-11\n",
      "Iteration: 3011 Loss: 3.5571175490413994e-11\n",
      "Iteration: 3012 Loss: 3.547903949316216e-11\n",
      "Iteration: 3013 Loss: 3.538730407013714e-11\n",
      "Iteration: 3014 Loss: 3.5296023456304926e-11\n",
      "Iteration: 3015 Loss: 3.520518814427959e-11\n",
      "Iteration: 3016 Loss: 3.511483392997472e-11\n",
      "Iteration: 3017 Loss: 3.502499516047954e-11\n",
      "Iteration: 3018 Loss: 3.493572351145527e-11\n",
      "Iteration: 3019 Loss: 3.4847092843552e-11\n",
      "Iteration: 3020 Loss: 3.475897052157044e-11\n",
      "Iteration: 3021 Loss: 3.467148235468832e-11\n",
      "Iteration: 3022 Loss: 3.458465081970752e-11\n",
      "Iteration: 3023 Loss: 3.4498452958657e-11\n",
      "Iteration: 3024 Loss: 3.441291847646174e-11\n",
      "Iteration: 3025 Loss: 3.4327993168702036e-11\n",
      "Iteration: 3026 Loss: 3.4243733495929156e-11\n",
      "Iteration: 3027 Loss: 3.4160124717823615e-11\n",
      "Iteration: 3028 Loss: 3.407711579611833e-11\n",
      "Iteration: 3029 Loss: 3.3994759202523545e-11\n",
      "Iteration: 3030 Loss: 3.3913013268512e-11\n",
      "Iteration: 3031 Loss: 3.383190943187774e-11\n",
      "Iteration: 3032 Loss: 3.3751399584682457e-11\n",
      "Iteration: 3033 Loss: 3.3671364221047065e-11\n",
      "Iteration: 3034 Loss: 3.3592108659423065e-11\n",
      "Iteration: 3035 Loss: 3.351334174142413e-11\n",
      "Iteration: 3036 Loss: 3.3435240468731286e-11\n",
      "Iteration: 3037 Loss: 3.335767139107637e-11\n",
      "Iteration: 3038 Loss: 3.328066637101511e-11\n",
      "Iteration: 3039 Loss: 3.3204158000613886e-11\n",
      "Iteration: 3040 Loss: 3.312822563166292e-11\n",
      "Iteration: 3041 Loss: 3.3052908077764985e-11\n",
      "Iteration: 3042 Loss: 3.2978045022590336e-11\n",
      "Iteration: 3043 Loss: 3.29037197475187e-11\n",
      "Iteration: 3044 Loss: 3.2829879763800663e-11\n",
      "Iteration: 3045 Loss: 3.2756569207709707e-11\n",
      "Iteration: 3046 Loss: 3.2683769946305996e-11\n",
      "Iteration: 3047 Loss: 3.2611458429130425e-11\n",
      "Iteration: 3048 Loss: 3.2539755591768984e-11\n",
      "Iteration: 3049 Loss: 3.246853320967673e-11\n",
      "Iteration: 3050 Loss: 3.239775146126184e-11\n",
      "Iteration: 3051 Loss: 3.232750711481406e-11\n",
      "Iteration: 3052 Loss: 3.225775638198165e-11\n",
      "Iteration: 3053 Loss: 3.2188481530198574e-11\n",
      "Iteration: 3054 Loss: 3.211969643788284e-11\n",
      "Iteration: 3055 Loss: 3.205140486102015e-11\n",
      "Iteration: 3056 Loss: 3.19836246784219e-11\n",
      "Iteration: 3057 Loss: 3.191631060041237e-11\n",
      "Iteration: 3058 Loss: 3.184948931331085e-11\n",
      "Iteration: 3059 Loss: 3.1783098889767893e-11\n",
      "Iteration: 3060 Loss: 3.171722914134849e-11\n",
      "Iteration: 3061 Loss: 3.16518677490184e-11\n",
      "Iteration: 3062 Loss: 3.158695644244566e-11\n",
      "Iteration: 3063 Loss: 3.152245550821889e-11\n",
      "Iteration: 3064 Loss: 3.145847614564877e-11\n",
      "Iteration: 3065 Loss: 3.139490739277914e-11\n",
      "Iteration: 3066 Loss: 3.1331804360062464e-11\n",
      "Iteration: 3067 Loss: 3.126915936882731e-11\n",
      "Iteration: 3068 Loss: 3.120693552745791e-11\n",
      "Iteration: 3069 Loss: 3.1145175444408326e-11\n",
      "Iteration: 3070 Loss: 3.108386509128846e-11\n",
      "Iteration: 3071 Loss: 3.102295682529896e-11\n",
      "Iteration: 3072 Loss: 3.0962414902938505e-11\n",
      "Iteration: 3073 Loss: 3.0902491498431515e-11\n",
      "Iteration: 3074 Loss: 3.084287290404362e-11\n",
      "Iteration: 3075 Loss: 3.078366951540535e-11\n",
      "Iteration: 3076 Loss: 3.07249739699579e-11\n",
      "Iteration: 3077 Loss: 3.066661507021075e-11\n",
      "Iteration: 3078 Loss: 3.0608686018915445e-11\n",
      "Iteration: 3079 Loss: 3.055117815840098e-11\n",
      "Iteration: 3080 Loss: 3.049409294385749e-11\n",
      "Iteration: 3081 Loss: 3.043739874181664e-11\n",
      "Iteration: 3082 Loss: 3.038111474203846e-11\n",
      "Iteration: 3083 Loss: 3.032524570589139e-11\n",
      "Iteration: 3084 Loss: 3.02697690196815e-11\n",
      "Iteration: 3085 Loss: 3.0214688098048565e-11\n",
      "Iteration: 3086 Loss: 3.015999580674596e-11\n",
      "Iteration: 3087 Loss: 3.01057056033135e-11\n",
      "Iteration: 3088 Loss: 3.005176992882301e-11\n",
      "Iteration: 3089 Loss: 2.9998252005562226e-11\n",
      "Iteration: 3090 Loss: 2.9945075963461046e-11\n",
      "Iteration: 3091 Loss: 2.98922935435499e-11\n",
      "Iteration: 3092 Loss: 2.9839883662286895e-11\n",
      "Iteration: 3093 Loss: 2.978787107664261e-11\n",
      "Iteration: 3094 Loss: 2.973623317820214e-11\n",
      "Iteration: 3095 Loss: 2.968497004101846e-11\n",
      "Iteration: 3096 Loss: 2.9634020501155535e-11\n",
      "Iteration: 3097 Loss: 2.958344936491833e-11\n",
      "Iteration: 3098 Loss: 2.9533306445710217e-11\n",
      "Iteration: 3099 Loss: 2.948343787154641e-11\n",
      "Iteration: 3100 Loss: 2.943394781285146e-11\n",
      "Iteration: 3101 Loss: 2.938481774146979e-11\n",
      "Iteration: 3102 Loss: 2.9336029994083385e-11\n",
      "Iteration: 3103 Loss: 2.9287602121888335e-11\n",
      "Iteration: 3104 Loss: 2.923949270578261e-11\n",
      "Iteration: 3105 Loss: 2.919164015927939e-11\n",
      "Iteration: 3106 Loss: 2.914424305060728e-11\n",
      "Iteration: 3107 Loss: 2.909713850062932e-11\n",
      "Iteration: 3108 Loss: 2.9050428010784817e-11\n",
      "Iteration: 3109 Loss: 2.900411333126305e-11\n",
      "Iteration: 3110 Loss: 2.895802006463708e-11\n",
      "Iteration: 3111 Loss: 2.8912235186648925e-11\n",
      "Iteration: 3112 Loss: 2.886679984121632e-11\n",
      "Iteration: 3113 Loss: 2.8821691584113742e-11\n",
      "Iteration: 3114 Loss: 2.8776894568883356e-11\n",
      "Iteration: 3115 Loss: 2.873243404475452e-11\n",
      "Iteration: 3116 Loss: 2.868828706254796e-11\n",
      "Iteration: 3117 Loss: 2.864456017240456e-11\n",
      "Iteration: 3118 Loss: 2.860109526179455e-11\n",
      "Iteration: 3119 Loss: 2.8557850346613947e-11\n",
      "Iteration: 3120 Loss: 2.8515013157618428e-11\n",
      "Iteration: 3121 Loss: 2.8472386557254994e-11\n",
      "Iteration: 3122 Loss: 2.843009929667289e-11\n",
      "Iteration: 3123 Loss: 2.838810713586288e-11\n",
      "Iteration: 3124 Loss: 2.83464274048389e-11\n",
      "Iteration: 3125 Loss: 2.830508620653676e-11\n",
      "Iteration: 3126 Loss: 2.8263951484717e-11\n",
      "Iteration: 3127 Loss: 2.822309878747763e-11\n",
      "Iteration: 3128 Loss: 2.818257376368801e-11\n",
      "Iteration: 3129 Loss: 2.814235758402951e-11\n",
      "Iteration: 3130 Loss: 2.810243072229707e-11\n",
      "Iteration: 3131 Loss: 2.8062777903794157e-11\n",
      "Iteration: 3132 Loss: 2.8023420817351862e-11\n",
      "Iteration: 3133 Loss: 2.7984444948386357e-11\n",
      "Iteration: 3134 Loss: 2.7945616721122074e-11\n",
      "Iteration: 3135 Loss: 2.7907051245046564e-11\n",
      "Iteration: 3136 Loss: 2.7868798556177374e-11\n",
      "Iteration: 3137 Loss: 2.783079732286846e-11\n",
      "Iteration: 3138 Loss: 2.7793061932211795e-11\n",
      "Iteration: 3139 Loss: 2.775558333762233e-11\n",
      "Iteration: 3140 Loss: 2.771838182540863e-11\n",
      "Iteration: 3141 Loss: 2.7681447439601143e-11\n",
      "Iteration: 3142 Loss: 2.7644778996679878e-11\n",
      "Iteration: 3143 Loss: 2.7608373015064138e-11\n",
      "Iteration: 3144 Loss: 2.757232542365911e-11\n",
      "Iteration: 3145 Loss: 2.7536408676886332e-11\n",
      "Iteration: 3146 Loss: 2.7500703016218347e-11\n",
      "Iteration: 3147 Loss: 2.7465321728805874e-11\n",
      "Iteration: 3148 Loss: 2.7430192496448856e-11\n",
      "Iteration: 3149 Loss: 2.739537358274855e-11\n",
      "Iteration: 3150 Loss: 2.7360723064296714e-11\n",
      "Iteration: 3151 Loss: 2.7326345716315477e-11\n",
      "Iteration: 3152 Loss: 2.729220463494774e-11\n",
      "Iteration: 3153 Loss: 2.7258217003084947e-11\n",
      "Iteration: 3154 Loss: 2.722456068341104e-11\n",
      "Iteration: 3155 Loss: 2.7191158829774707e-11\n",
      "Iteration: 3156 Loss: 2.715799594746226e-11\n",
      "Iteration: 3157 Loss: 2.7125064510905955e-11\n",
      "Iteration: 3158 Loss: 2.709236542742665e-11\n",
      "Iteration: 3159 Loss: 2.7059914827223455e-11\n",
      "Iteration: 3160 Loss: 2.702778119380179e-11\n",
      "Iteration: 3161 Loss: 2.6995778625452913e-11\n",
      "Iteration: 3162 Loss: 2.6964056360593392e-11\n",
      "Iteration: 3163 Loss: 2.6932486805693234e-11\n",
      "Iteration: 3164 Loss: 2.6901174639349155e-11\n",
      "Iteration: 3165 Loss: 2.6870067764154313e-11\n",
      "Iteration: 3166 Loss: 2.6839189204144248e-11\n",
      "Iteration: 3167 Loss: 2.680854570874911e-11\n",
      "Iteration: 3168 Loss: 2.677807453820923e-11\n",
      "Iteration: 3169 Loss: 2.6747826966444598e-11\n",
      "Iteration: 3170 Loss: 2.6717810312837393e-11\n",
      "Iteration: 3171 Loss: 2.6688014942936873e-11\n",
      "Iteration: 3172 Loss: 2.6658305321367464e-11\n",
      "Iteration: 3173 Loss: 2.6628890199413098e-11\n",
      "Iteration: 3174 Loss: 2.659959738963777e-11\n",
      "Iteration: 3175 Loss: 2.657072344925175e-11\n",
      "Iteration: 3176 Loss: 2.6541968096658315e-11\n",
      "Iteration: 3177 Loss: 2.6513410306637347e-11\n",
      "Iteration: 3178 Loss: 2.648504764515354e-11\n",
      "Iteration: 3179 Loss: 2.645688760439738e-11\n",
      "Iteration: 3180 Loss: 2.6428933628309598e-11\n",
      "Iteration: 3181 Loss: 2.640128903227159e-11\n",
      "Iteration: 3182 Loss: 2.637372569891774e-11\n",
      "Iteration: 3183 Loss: 2.634636433194676e-11\n",
      "Iteration: 3184 Loss: 2.6319206142241868e-11\n",
      "Iteration: 3185 Loss: 2.629222336061004e-11\n",
      "Iteration: 3186 Loss: 2.6265445652825293e-11\n",
      "Iteration: 3187 Loss: 2.6238854257034926e-11\n",
      "Iteration: 3188 Loss: 2.6212453470484464e-11\n",
      "Iteration: 3189 Loss: 2.6186235269105157e-11\n",
      "Iteration: 3190 Loss: 2.616021878845518e-11\n",
      "Iteration: 3191 Loss: 2.613436876253737e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3192 Loss: 2.6108712339705436e-11\n",
      "Iteration: 3193 Loss: 2.6083241462163834e-11\n",
      "Iteration: 3194 Loss: 2.6058009595654826e-11\n",
      "Iteration: 3195 Loss: 2.6032922562354952e-11\n",
      "Iteration: 3196 Loss: 2.600797367060438e-11\n",
      "Iteration: 3197 Loss: 2.5983221997912347e-11\n",
      "Iteration: 3198 Loss: 2.595864265277288e-11\n",
      "Iteration: 3199 Loss: 2.5934238413328738e-11\n",
      "Iteration: 3200 Loss: 2.591017303631403e-11\n",
      "Iteration: 3201 Loss: 2.588609479578993e-11\n",
      "Iteration: 3202 Loss: 2.586220384178617e-11\n",
      "Iteration: 3203 Loss: 2.5838492263398295e-11\n",
      "Iteration: 3204 Loss: 2.5814910583139798e-11\n",
      "Iteration: 3205 Loss: 2.5791497806001545e-11\n",
      "Iteration: 3206 Loss: 2.5768293668671028e-11\n",
      "Iteration: 3207 Loss: 2.574524977852446e-11\n",
      "Iteration: 3208 Loss: 2.572237336477907e-11\n",
      "Iteration: 3209 Loss: 2.56996568048988e-11\n",
      "Iteration: 3210 Loss: 2.567710341444105e-11\n",
      "Iteration: 3211 Loss: 2.5654710207752138e-11\n",
      "Iteration: 3212 Loss: 2.563248618351738e-11\n",
      "Iteration: 3213 Loss: 2.5610410019320102e-11\n",
      "Iteration: 3214 Loss: 2.558849465410078e-11\n",
      "Iteration: 3215 Loss: 2.5566701189211588e-11\n",
      "Iteration: 3216 Loss: 2.5545100258780602e-11\n",
      "Iteration: 3217 Loss: 2.5523646099564577e-11\n",
      "Iteration: 3218 Loss: 2.5502323831343233e-11\n",
      "Iteration: 3219 Loss: 2.5481159062303156e-11\n",
      "Iteration: 3220 Loss: 2.546013611475695e-11\n",
      "Iteration: 3221 Loss: 2.5439291818026815e-11\n",
      "Iteration: 3222 Loss: 2.541854224974752e-11\n",
      "Iteration: 3223 Loss: 2.5397959725869152e-11\n",
      "Iteration: 3224 Loss: 2.5377588142663127e-11\n",
      "Iteration: 3225 Loss: 2.535727235871452e-11\n",
      "Iteration: 3226 Loss: 2.533713920365245e-11\n",
      "Iteration: 3227 Loss: 2.531714402945212e-11\n",
      "Iteration: 3228 Loss: 2.5297294865801314e-11\n",
      "Iteration: 3229 Loss: 2.5277580392377328e-11\n",
      "Iteration: 3230 Loss: 2.5258013539755844e-11\n",
      "Iteration: 3231 Loss: 2.5238537733419522e-11\n",
      "Iteration: 3232 Loss: 2.52192610184556e-11\n",
      "Iteration: 3233 Loss: 2.520010185509154e-11\n",
      "Iteration: 3234 Loss: 2.5181111567382552e-11\n",
      "Iteration: 3235 Loss: 2.516222677629073e-11\n",
      "Iteration: 3236 Loss: 2.51434798237031e-11\n",
      "Iteration: 3237 Loss: 2.5124857951279287e-11\n",
      "Iteration: 3238 Loss: 2.510626951747325e-11\n",
      "Iteration: 3239 Loss: 2.5087923542341577e-11\n",
      "Iteration: 3240 Loss: 2.506970309675457e-11\n",
      "Iteration: 3241 Loss: 2.505150768268746e-11\n",
      "Iteration: 3242 Loss: 2.5033542863450456e-11\n",
      "Iteration: 3243 Loss: 2.501571491571926e-11\n",
      "Iteration: 3244 Loss: 2.49979963948373e-11\n",
      "Iteration: 3245 Loss: 2.498041481036897e-11\n",
      "Iteration: 3246 Loss: 2.496306723084342e-11\n",
      "Iteration: 3247 Loss: 2.494573139732786e-11\n",
      "Iteration: 3248 Loss: 2.4928528512452408e-11\n",
      "Iteration: 3249 Loss: 2.4911435754435835e-11\n",
      "Iteration: 3250 Loss: 2.4894472468098336e-11\n",
      "Iteration: 3251 Loss: 2.4877731973203082e-11\n",
      "Iteration: 3252 Loss: 2.4861017135947535e-11\n",
      "Iteration: 3253 Loss: 2.4844465744607257e-11\n",
      "Iteration: 3254 Loss: 2.4827950405347308e-11\n",
      "Iteration: 3255 Loss: 2.481157947547698e-11\n",
      "Iteration: 3256 Loss: 2.479532322932191e-11\n",
      "Iteration: 3257 Loss: 2.4779186062088254e-11\n",
      "Iteration: 3258 Loss: 2.4763165439799577e-11\n",
      "Iteration: 3259 Loss: 2.4747262929557685e-11\n",
      "Iteration: 3260 Loss: 2.4731460753693187e-11\n",
      "Iteration: 3261 Loss: 2.471578885898734e-11\n",
      "Iteration: 3262 Loss: 2.4700266740627113e-11\n",
      "Iteration: 3263 Loss: 2.4684780425155463e-11\n",
      "Iteration: 3264 Loss: 2.4669426740653842e-11\n",
      "Iteration: 3265 Loss: 2.4654192471599575e-11\n",
      "Iteration: 3266 Loss: 2.4639062420188674e-11\n",
      "Iteration: 3267 Loss: 2.462404356638458e-11\n",
      "Iteration: 3268 Loss: 2.460912037767209e-11\n",
      "Iteration: 3269 Loss: 2.4594283908970596e-11\n",
      "Iteration: 3270 Loss: 2.4579579660154058e-11\n",
      "Iteration: 3271 Loss: 2.4564986133769745e-11\n",
      "Iteration: 3272 Loss: 2.4550512962403967e-11\n",
      "Iteration: 3273 Loss: 2.453611798199879e-11\n",
      "Iteration: 3274 Loss: 2.4521836309387608e-11\n",
      "Iteration: 3275 Loss: 2.4507645089231542e-11\n",
      "Iteration: 3276 Loss: 2.4493556218587687e-11\n",
      "Iteration: 3277 Loss: 2.4479560770495538e-11\n",
      "Iteration: 3278 Loss: 2.4465679063385174e-11\n",
      "Iteration: 3279 Loss: 2.445188412993592e-11\n",
      "Iteration: 3280 Loss: 2.443820177191548e-11\n",
      "Iteration: 3281 Loss: 2.4424661588892836e-11\n",
      "Iteration: 3282 Loss: 2.441113719277588e-11\n",
      "Iteration: 3283 Loss: 2.4397739734092464e-11\n",
      "Iteration: 3284 Loss: 2.4384433288134302e-11\n",
      "Iteration: 3285 Loss: 2.437122325486057e-11\n",
      "Iteration: 3286 Loss: 2.435811682974847e-11\n",
      "Iteration: 3287 Loss: 2.4345086063227646e-11\n",
      "Iteration: 3288 Loss: 2.4332152909262236e-11\n",
      "Iteration: 3289 Loss: 2.431932826504413e-11\n",
      "Iteration: 3290 Loss: 2.430662656550549e-11\n",
      "Iteration: 3291 Loss: 2.429394941376493e-11\n",
      "Iteration: 3292 Loss: 2.4281379248913103e-11\n",
      "Iteration: 3293 Loss: 2.426890388734194e-11\n",
      "Iteration: 3294 Loss: 2.425651753234447e-11\n",
      "Iteration: 3295 Loss: 2.4244221558082615e-11\n",
      "Iteration: 3296 Loss: 2.4232008427076135e-11\n",
      "Iteration: 3297 Loss: 2.4219882536317492e-11\n",
      "Iteration: 3298 Loss: 2.4207848671124587e-11\n",
      "Iteration: 3299 Loss: 2.4195944577483804e-11\n",
      "Iteration: 3300 Loss: 2.418404499599508e-11\n",
      "Iteration: 3301 Loss: 2.417227314967231e-11\n",
      "Iteration: 3302 Loss: 2.4160571524175466e-11\n",
      "Iteration: 3303 Loss: 2.41489473325034e-11\n",
      "Iteration: 3304 Loss: 2.413742075389906e-11\n",
      "Iteration: 3305 Loss: 2.4125939118508567e-11\n",
      "Iteration: 3306 Loss: 2.411457412467989e-11\n",
      "Iteration: 3307 Loss: 2.410336044032135e-11\n",
      "Iteration: 3308 Loss: 2.4092146334556655e-11\n",
      "Iteration: 3309 Loss: 2.4080924736349987e-11\n",
      "Iteration: 3310 Loss: 2.4069879463259092e-11\n",
      "Iteration: 3311 Loss: 2.405892073951321e-11\n",
      "Iteration: 3312 Loss: 2.404798535854825e-11\n",
      "Iteration: 3313 Loss: 2.4037175118786163e-11\n",
      "Iteration: 3314 Loss: 2.4026434247534658e-11\n",
      "Iteration: 3315 Loss: 2.401587625055463e-11\n",
      "Iteration: 3316 Loss: 2.400529313649846e-11\n",
      "Iteration: 3317 Loss: 2.3994684410443038e-11\n",
      "Iteration: 3318 Loss: 2.398421744440673e-11\n",
      "Iteration: 3319 Loss: 2.397386892594645e-11\n",
      "Iteration: 3320 Loss: 2.3963524971782875e-11\n",
      "Iteration: 3321 Loss: 2.3953290151270924e-11\n",
      "Iteration: 3322 Loss: 2.3943120846727864e-11\n",
      "Iteration: 3323 Loss: 2.3933019523985976e-11\n",
      "Iteration: 3324 Loss: 2.3922997035744747e-11\n",
      "Iteration: 3325 Loss: 2.391304508858488e-11\n",
      "Iteration: 3326 Loss: 2.3903213160972484e-11\n",
      "Iteration: 3327 Loss: 2.3893428099231655e-11\n",
      "Iteration: 3328 Loss: 2.3883748611456175e-11\n",
      "Iteration: 3329 Loss: 2.387420353269553e-11\n",
      "Iteration: 3330 Loss: 2.3864605848154854e-11\n",
      "Iteration: 3331 Loss: 2.3855047512790025e-11\n",
      "Iteration: 3332 Loss: 2.3845632839495812e-11\n",
      "Iteration: 3333 Loss: 2.383619549001664e-11\n",
      "Iteration: 3334 Loss: 2.3826822587948532e-11\n",
      "Iteration: 3335 Loss: 2.3817584738029638e-11\n",
      "Iteration: 3336 Loss: 2.380835436538881e-11\n",
      "Iteration: 3337 Loss: 2.3799214859577484e-11\n",
      "Iteration: 3338 Loss: 2.3790048964637342e-11\n",
      "Iteration: 3339 Loss: 2.3781038493940872e-11\n",
      "Iteration: 3340 Loss: 2.3772100061326146e-11\n",
      "Iteration: 3341 Loss: 2.3763225910215918e-11\n",
      "Iteration: 3342 Loss: 2.3754402574779348e-11\n",
      "Iteration: 3343 Loss: 2.3745653864312485e-11\n",
      "Iteration: 3344 Loss: 2.3736964844357663e-11\n",
      "Iteration: 3345 Loss: 2.3728330004133144e-11\n",
      "Iteration: 3346 Loss: 2.3719769388732814e-11\n",
      "Iteration: 3347 Loss: 2.3711260982170538e-11\n",
      "Iteration: 3348 Loss: 2.3702811466777955e-11\n",
      "Iteration: 3349 Loss: 2.3694431913380888e-11\n",
      "Iteration: 3350 Loss: 2.3686098304531015e-11\n",
      "Iteration: 3351 Loss: 2.3677826111012247e-11\n",
      "Iteration: 3352 Loss: 2.366961608155772e-11\n",
      "Iteration: 3353 Loss: 2.3661572065948298e-11\n",
      "Iteration: 3354 Loss: 2.365348273211751e-11\n",
      "Iteration: 3355 Loss: 2.364544699473412e-11\n",
      "Iteration: 3356 Loss: 2.363745889968066e-11\n",
      "Iteration: 3357 Loss: 2.3629610052120128e-11\n",
      "Iteration: 3358 Loss: 2.3621705935463263e-11\n",
      "Iteration: 3359 Loss: 2.3613894241703492e-11\n",
      "Iteration: 3360 Loss: 2.3606245465902427e-11\n",
      "Iteration: 3361 Loss: 2.359854972664296e-11\n",
      "Iteration: 3362 Loss: 2.3590909611739904e-11\n",
      "Iteration: 3363 Loss: 2.3583308761860736e-11\n",
      "Iteration: 3364 Loss: 2.3575833103077034e-11\n",
      "Iteration: 3365 Loss: 2.3568313643601225e-11\n",
      "Iteration: 3366 Loss: 2.356089627981591e-11\n",
      "Iteration: 3367 Loss: 2.3553515060392956e-11\n",
      "Iteration: 3368 Loss: 2.3546192440314548e-11\n",
      "Iteration: 3369 Loss: 2.3538923666300202e-11\n",
      "Iteration: 3370 Loss: 2.3531695027071845e-11\n",
      "Iteration: 3371 Loss: 2.3524528226875004e-11\n",
      "Iteration: 3372 Loss: 2.35174102739139e-11\n",
      "Iteration: 3373 Loss: 2.3510348185867104e-11\n",
      "Iteration: 3374 Loss: 2.3503330756974475e-11\n",
      "Iteration: 3375 Loss: 2.3496367908649925e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3376 Loss: 2.3489440316291843e-11\n",
      "Iteration: 3377 Loss: 2.3482576991196734e-11\n",
      "Iteration: 3378 Loss: 2.3475816529663276e-11\n",
      "Iteration: 3379 Loss: 2.3469062590473664e-11\n",
      "Iteration: 3380 Loss: 2.346231692717524e-11\n",
      "Iteration: 3381 Loss: 2.345563487127029e-11\n",
      "Iteration: 3382 Loss: 2.34490036208791e-11\n",
      "Iteration: 3383 Loss: 2.3442428769943047e-11\n",
      "Iteration: 3384 Loss: 2.3435950611360402e-11\n",
      "Iteration: 3385 Loss: 2.342948512039933e-11\n",
      "Iteration: 3386 Loss: 2.3423006556986643e-11\n",
      "Iteration: 3387 Loss: 2.3416617975835212e-11\n",
      "Iteration: 3388 Loss: 2.3410264824871842e-11\n",
      "Iteration: 3389 Loss: 2.3403962878438398e-11\n",
      "Iteration: 3390 Loss: 2.3397696580815836e-11\n",
      "Iteration: 3391 Loss: 2.3391481129260604e-11\n",
      "Iteration: 3392 Loss: 2.3385304296583695e-11\n",
      "Iteration: 3393 Loss: 2.337915961620606e-11\n",
      "Iteration: 3394 Loss: 2.3373066208154306e-11\n",
      "Iteration: 3395 Loss: 2.3367025455094666e-11\n",
      "Iteration: 3396 Loss: 2.3361023782897182e-11\n",
      "Iteration: 3397 Loss: 2.335507642157115e-11\n",
      "Iteration: 3398 Loss: 2.3349156608914328e-11\n",
      "Iteration: 3399 Loss: 2.3343276510766804e-11\n",
      "Iteration: 3400 Loss: 2.333745604299931e-11\n",
      "Iteration: 3401 Loss: 2.3331667736222706e-11\n",
      "Iteration: 3402 Loss: 2.332591809746332e-11\n",
      "Iteration: 3403 Loss: 2.3320209529538098e-11\n",
      "Iteration: 3404 Loss: 2.331453813497519e-11\n",
      "Iteration: 3405 Loss: 2.3308904762962228e-11\n",
      "Iteration: 3406 Loss: 2.3303325305378525e-11\n",
      "Iteration: 3407 Loss: 2.329778062131313e-11\n",
      "Iteration: 3408 Loss: 2.3292269379535053e-11\n",
      "Iteration: 3409 Loss: 2.3286696245229206e-11\n",
      "Iteration: 3410 Loss: 2.3281271333173642e-11\n",
      "Iteration: 3411 Loss: 2.3275874134690697e-11\n",
      "Iteration: 3412 Loss: 2.3270525792129106e-11\n",
      "Iteration: 3413 Loss: 2.3265207097191667e-11\n",
      "Iteration: 3414 Loss: 2.3259928954289172e-11\n",
      "Iteration: 3415 Loss: 2.3254687540489818e-11\n",
      "Iteration: 3416 Loss: 2.324949390557445e-11\n",
      "Iteration: 3417 Loss: 2.3244429248257797e-11\n",
      "Iteration: 3418 Loss: 2.3239305793583885e-11\n",
      "Iteration: 3419 Loss: 2.32342062998468e-11\n",
      "Iteration: 3420 Loss: 2.322916181453046e-11\n",
      "Iteration: 3421 Loss: 2.32241403890593e-11\n",
      "Iteration: 3422 Loss: 2.3219139352553746e-11\n",
      "Iteration: 3423 Loss: 2.321419599076626e-11\n",
      "Iteration: 3424 Loss: 2.3209287113196576e-11\n",
      "Iteration: 3425 Loss: 2.3204406367846706e-11\n",
      "Iteration: 3426 Loss: 2.3199558059435982e-11\n",
      "Iteration: 3427 Loss: 2.319474183110729e-11\n",
      "Iteration: 3428 Loss: 2.318997841331394e-11\n",
      "Iteration: 3429 Loss: 2.3185234080243904e-11\n",
      "Iteration: 3430 Loss: 2.3180517237944586e-11\n",
      "Iteration: 3431 Loss: 2.3175842189105584e-11\n",
      "Iteration: 3432 Loss: 2.317120409039892e-11\n",
      "Iteration: 3433 Loss: 2.3166584659160072e-11\n",
      "Iteration: 3434 Loss: 2.316201912315875e-11\n",
      "Iteration: 3435 Loss: 2.31574751219894e-11\n",
      "Iteration: 3436 Loss: 2.31529616503118e-11\n",
      "Iteration: 3437 Loss: 2.3148481030119424e-11\n",
      "Iteration: 3438 Loss: 2.3144030697606983e-11\n",
      "Iteration: 3439 Loss: 2.3139609042867964e-11\n",
      "Iteration: 3440 Loss: 2.3135222670088996e-11\n",
      "Iteration: 3441 Loss: 2.3130863249970907e-11\n",
      "Iteration: 3442 Loss: 2.3126541188376657e-11\n",
      "Iteration: 3443 Loss: 2.3122256193526404e-11\n",
      "Iteration: 3444 Loss: 2.311798825527725e-11\n",
      "Iteration: 3445 Loss: 2.311376230994346e-11\n",
      "Iteration: 3446 Loss: 2.310955905505066e-11\n",
      "Iteration: 3447 Loss: 2.310538422010831e-11\n",
      "Iteration: 3448 Loss: 2.310124651478582e-11\n",
      "Iteration: 3449 Loss: 2.309713134451054e-11\n",
      "Iteration: 3450 Loss: 2.309304518782364e-11\n",
      "Iteration: 3451 Loss: 2.308898930353628e-11\n",
      "Iteration: 3452 Loss: 2.308495880456534e-11\n",
      "Iteration: 3453 Loss: 2.3080969054175683e-11\n",
      "Iteration: 3454 Loss: 2.3076991807728973e-11\n",
      "Iteration: 3455 Loss: 2.30730497713093e-11\n",
      "Iteration: 3456 Loss: 2.3069144692377994e-11\n",
      "Iteration: 3457 Loss: 2.3065250797072753e-11\n",
      "Iteration: 3458 Loss: 2.3061392478039468e-11\n",
      "Iteration: 3459 Loss: 2.305756743859487e-11\n",
      "Iteration: 3460 Loss: 2.305375565938538e-11\n",
      "Iteration: 3461 Loss: 2.304999020598332e-11\n",
      "Iteration: 3462 Loss: 2.3046240278731666e-11\n",
      "Iteration: 3463 Loss: 2.304251042090618e-11\n",
      "Iteration: 3464 Loss: 2.3038810587789254e-11\n",
      "Iteration: 3465 Loss: 2.3035146336574093e-11\n",
      "Iteration: 3466 Loss: 2.3031496418270663e-11\n",
      "Iteration: 3467 Loss: 2.302787879171564e-11\n",
      "Iteration: 3468 Loss: 2.3024292487893114e-11\n",
      "Iteration: 3469 Loss: 2.3020720432089174e-11\n",
      "Iteration: 3470 Loss: 2.3017180950484836e-11\n",
      "Iteration: 3471 Loss: 2.301365645376852e-11\n",
      "Iteration: 3472 Loss: 2.3010169945740242e-11\n",
      "Iteration: 3473 Loss: 2.30067010868653e-11\n",
      "Iteration: 3474 Loss: 2.3003258804397465e-11\n",
      "Iteration: 3475 Loss: 2.2999841673342225e-11\n",
      "Iteration: 3476 Loss: 2.299644321317927e-11\n",
      "Iteration: 3477 Loss: 2.299306984721006e-11\n",
      "Iteration: 3478 Loss: 2.2989720437427732e-11\n",
      "Iteration: 3479 Loss: 2.2986402866579035e-11\n",
      "Iteration: 3480 Loss: 2.298310877123456e-11\n",
      "Iteration: 3481 Loss: 2.2979828019866935e-11\n",
      "Iteration: 3482 Loss: 2.297667371372322e-11\n",
      "Iteration: 3483 Loss: 2.2973454978954125e-11\n",
      "Iteration: 3484 Loss: 2.2970133024697124e-11\n",
      "Iteration: 3485 Loss: 2.296698361772714e-11\n",
      "Iteration: 3486 Loss: 2.2963828038729653e-11\n",
      "Iteration: 3487 Loss: 2.2960686636991688e-11\n",
      "Iteration: 3488 Loss: 2.295756565399928e-11\n",
      "Iteration: 3489 Loss: 2.2954476369586186e-11\n",
      "Iteration: 3490 Loss: 2.2951405561672615e-11\n",
      "Iteration: 3491 Loss: 2.2948255028884772e-11\n",
      "Iteration: 3492 Loss: 2.2945211510797335e-11\n",
      "Iteration: 3493 Loss: 2.2942211060747915e-11\n",
      "Iteration: 3494 Loss: 2.2939222665845116e-11\n",
      "Iteration: 3495 Loss: 2.2936260998979127e-11\n",
      "Iteration: 3496 Loss: 2.2933317155249943e-11\n",
      "Iteration: 3497 Loss: 2.293040501971815e-11\n",
      "Iteration: 3498 Loss: 2.2927495021401237e-11\n",
      "Iteration: 3499 Loss: 2.292461372910463e-11\n",
      "Iteration: 3500 Loss: 2.2921809683867643e-11\n",
      "Iteration: 3501 Loss: 2.2918935983391276e-11\n",
      "Iteration: 3502 Loss: 2.2916122334061663e-11\n",
      "Iteration: 3503 Loss: 2.2913322634224608e-11\n",
      "Iteration: 3504 Loss: 2.291054269617532e-11\n",
      "Iteration: 3505 Loss: 2.2907776826438577e-11\n",
      "Iteration: 3506 Loss: 2.2905038161724133e-11\n",
      "Iteration: 3507 Loss: 2.2902215928692543e-11\n",
      "Iteration: 3508 Loss: 2.2899508290409452e-11\n",
      "Iteration: 3509 Loss: 2.2896928379106662e-11\n",
      "Iteration: 3510 Loss: 2.2894259140661935e-11\n",
      "Iteration: 3511 Loss: 2.2891618270732116e-11\n",
      "Iteration: 3512 Loss: 2.288899682581482e-11\n",
      "Iteration: 3513 Loss: 2.2886383608955304e-11\n",
      "Iteration: 3514 Loss: 2.2883696763064674e-11\n",
      "Iteration: 3515 Loss: 2.288111806483887e-11\n",
      "Iteration: 3516 Loss: 2.287858078088905e-11\n",
      "Iteration: 3517 Loss: 2.287594181885749e-11\n",
      "Iteration: 3518 Loss: 2.287340013589835e-11\n",
      "Iteration: 3519 Loss: 2.2870887636848014e-11\n",
      "Iteration: 3520 Loss: 2.2868409368112362e-11\n",
      "Iteration: 3521 Loss: 2.286593700577835e-11\n",
      "Iteration: 3522 Loss: 2.2863494294850326e-11\n",
      "Iteration: 3523 Loss: 2.2861068272045866e-11\n",
      "Iteration: 3524 Loss: 2.2858655430409277e-11\n",
      "Iteration: 3525 Loss: 2.2856259209259312e-11\n",
      "Iteration: 3526 Loss: 2.2853984101076088e-11\n",
      "Iteration: 3527 Loss: 2.2851633491569792e-11\n",
      "Iteration: 3528 Loss: 2.2849385193027712e-11\n",
      "Iteration: 3529 Loss: 2.2847058931415013e-11\n",
      "Iteration: 3530 Loss: 2.2844742095006627e-11\n",
      "Iteration: 3531 Loss: 2.2842448597583253e-11\n",
      "Iteration: 3532 Loss: 2.2840203999827727e-11\n",
      "Iteration: 3533 Loss: 2.2837934162259305e-11\n",
      "Iteration: 3534 Loss: 2.283568871053338e-11\n",
      "Iteration: 3535 Loss: 2.283346389594091e-11\n",
      "Iteration: 3536 Loss: 2.2831252884900987e-11\n",
      "Iteration: 3537 Loss: 2.282904855953632e-11\n",
      "Iteration: 3538 Loss: 2.2826862621990043e-11\n",
      "Iteration: 3539 Loss: 2.2824713244301265e-11\n",
      "Iteration: 3540 Loss: 2.2822523417880305e-11\n",
      "Iteration: 3541 Loss: 2.2820383468090568e-11\n",
      "Iteration: 3542 Loss: 2.2818259528545924e-11\n",
      "Iteration: 3543 Loss: 2.281614824664887e-11\n",
      "Iteration: 3544 Loss: 2.2814166539856836e-11\n",
      "Iteration: 3545 Loss: 2.2812085231887924e-11\n",
      "Iteration: 3546 Loss: 2.2810028721889358e-11\n",
      "Iteration: 3547 Loss: 2.2807981968900754e-11\n",
      "Iteration: 3548 Loss: 2.280594422242709e-11\n",
      "Iteration: 3549 Loss: 2.2803925154869196e-11\n",
      "Iteration: 3550 Loss: 2.280191872556766e-11\n",
      "Iteration: 3551 Loss: 2.2799955595898097e-11\n",
      "Iteration: 3552 Loss: 2.279797965407982e-11\n",
      "Iteration: 3553 Loss: 2.279601573316263e-11\n",
      "Iteration: 3554 Loss: 2.2794073765162535e-11\n",
      "Iteration: 3555 Loss: 2.2792131587867318e-11\n",
      "Iteration: 3556 Loss: 2.2790213143726344e-11\n",
      "Iteration: 3557 Loss: 2.278830280984975e-11\n",
      "Iteration: 3558 Loss: 2.278641743792944e-11\n",
      "Iteration: 3559 Loss: 2.2784530269473574e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3560 Loss: 2.2782669385118223e-11\n",
      "Iteration: 3561 Loss: 2.2780713599942313e-11\n",
      "Iteration: 3562 Loss: 2.2778868398125386e-11\n",
      "Iteration: 3563 Loss: 2.2777144151496656e-11\n",
      "Iteration: 3564 Loss: 2.2775337440712477e-11\n",
      "Iteration: 3565 Loss: 2.277353334971008e-11\n",
      "Iteration: 3566 Loss: 2.2771738957196478e-11\n",
      "Iteration: 3567 Loss: 2.2769966534393972e-11\n",
      "Iteration: 3568 Loss: 2.2768200750867534e-11\n",
      "Iteration: 3569 Loss: 2.2766451276244335e-11\n",
      "Iteration: 3570 Loss: 2.2764708169342116e-11\n",
      "Iteration: 3571 Loss: 2.27629871814778e-11\n",
      "Iteration: 3572 Loss: 2.2761268895193353e-11\n",
      "Iteration: 3573 Loss: 2.2759546110757772e-11\n",
      "Iteration: 3574 Loss: 2.2757850712641343e-11\n",
      "Iteration: 3575 Loss: 2.2756173423404757e-11\n",
      "Iteration: 3576 Loss: 2.275450990463604e-11\n",
      "Iteration: 3577 Loss: 2.275285815482894e-11\n",
      "Iteration: 3578 Loss: 2.2751216119789924e-11\n",
      "Iteration: 3579 Loss: 2.2749482608060766e-11\n",
      "Iteration: 3580 Loss: 2.2747862631272e-11\n",
      "Iteration: 3581 Loss: 2.274636510785556e-11\n",
      "Iteration: 3582 Loss: 2.2744753759762175e-11\n",
      "Iteration: 3583 Loss: 2.2743066601595037e-11\n",
      "Iteration: 3584 Loss: 2.2741496669716173e-11\n",
      "Iteration: 3585 Loss: 2.2739939602622732e-11\n",
      "Iteration: 3586 Loss: 2.2738486762884047e-11\n",
      "Iteration: 3587 Loss: 2.2736968422099928e-11\n",
      "Iteration: 3588 Loss: 2.2735438330449546e-11\n",
      "Iteration: 3589 Loss: 2.2733921489259128e-11\n",
      "Iteration: 3590 Loss: 2.27324220389328e-11\n",
      "Iteration: 3591 Loss: 2.273091971999901e-11\n",
      "Iteration: 3592 Loss: 2.2729429849805177e-11\n",
      "Iteration: 3593 Loss: 2.2727954716366738e-11\n",
      "Iteration: 3594 Loss: 2.2726487018702612e-11\n",
      "Iteration: 3595 Loss: 2.2725034608908918e-11\n",
      "Iteration: 3596 Loss: 2.272369386384479e-11\n",
      "Iteration: 3597 Loss: 2.272215165506604e-11\n",
      "Iteration: 3598 Loss: 2.2720727798345668e-11\n",
      "Iteration: 3599 Loss: 2.271931703824735e-11\n",
      "Iteration: 3600 Loss: 2.2717903677627215e-11\n",
      "Iteration: 3601 Loss: 2.2716518941107138e-11\n",
      "Iteration: 3602 Loss: 2.2715130499561756e-11\n",
      "Iteration: 3603 Loss: 2.27137614593526e-11\n",
      "Iteration: 3604 Loss: 2.271238731701901e-11\n",
      "Iteration: 3605 Loss: 2.2710937970602702e-11\n",
      "Iteration: 3606 Loss: 2.2709590337030628e-11\n",
      "Iteration: 3607 Loss: 2.2708252550489736e-11\n",
      "Iteration: 3608 Loss: 2.2707078876809732e-11\n",
      "Iteration: 3609 Loss: 2.27057963077102e-11\n",
      "Iteration: 3610 Loss: 2.270451270983448e-11\n",
      "Iteration: 3611 Loss: 2.270319119132532e-11\n",
      "Iteration: 3612 Loss: 2.2701897494190785e-11\n",
      "Iteration: 3613 Loss: 2.2700620825396606e-11\n",
      "Iteration: 3614 Loss: 2.269934373465519e-11\n",
      "Iteration: 3615 Loss: 2.269809033971051e-11\n",
      "Iteration: 3616 Loss: 2.269682621974273e-11\n",
      "Iteration: 3617 Loss: 2.2695588161399616e-11\n",
      "Iteration: 3618 Loss: 2.2694324376761685e-11\n",
      "Iteration: 3619 Loss: 2.269307236492736e-11\n",
      "Iteration: 3620 Loss: 2.2691855416638424e-11\n",
      "Iteration: 3621 Loss: 2.2690647064204534e-11\n",
      "Iteration: 3622 Loss: 2.2689451027064785e-11\n",
      "Iteration: 3623 Loss: 2.26882616439299e-11\n",
      "Iteration: 3624 Loss: 2.2687069196378018e-11\n",
      "Iteration: 3625 Loss: 2.2685899114388092e-11\n",
      "Iteration: 3626 Loss: 2.2684731021037623e-11\n",
      "Iteration: 3627 Loss: 2.268357043898572e-11\n",
      "Iteration: 3628 Loss: 2.2682422245134388e-11\n",
      "Iteration: 3629 Loss: 2.2681280076993627e-11\n",
      "Iteration: 3630 Loss: 2.2680145396230902e-11\n",
      "Iteration: 3631 Loss: 2.2679016917028076e-11\n",
      "Iteration: 3632 Loss: 2.2677895256333057e-11\n",
      "Iteration: 3633 Loss: 2.2676791275408013e-11\n",
      "Iteration: 3634 Loss: 2.2675693224575453e-11\n",
      "Iteration: 3635 Loss: 2.2674593780444817e-11\n",
      "Iteration: 3636 Loss: 2.26735122720363e-11\n",
      "Iteration: 3637 Loss: 2.2672437256144857e-11\n",
      "Iteration: 3638 Loss: 2.2671357173422198e-11\n",
      "Iteration: 3639 Loss: 2.267029164573211e-11\n",
      "Iteration: 3640 Loss: 2.2669246700591763e-11\n",
      "Iteration: 3641 Loss: 2.2668201841632178e-11\n",
      "Iteration: 3642 Loss: 2.266715714880549e-11\n",
      "Iteration: 3643 Loss: 2.2666122061125036e-11\n",
      "Iteration: 3644 Loss: 2.2665095020047368e-11\n",
      "Iteration: 3645 Loss: 2.266407318731555e-11\n",
      "Iteration: 3646 Loss: 2.266306685133714e-11\n",
      "Iteration: 3647 Loss: 2.266195637735546e-11\n",
      "Iteration: 3648 Loss: 2.266095496232159e-11\n",
      "Iteration: 3649 Loss: 2.2659969612531115e-11\n",
      "Iteration: 3650 Loss: 2.265898574475132e-11\n",
      "Iteration: 3651 Loss: 2.265811305603826e-11\n",
      "Iteration: 3652 Loss: 2.265703017262124e-11\n",
      "Iteration: 3653 Loss: 2.265608043385828e-11\n",
      "Iteration: 3654 Loss: 2.265512097277983e-11\n",
      "Iteration: 3655 Loss: 2.26541757946758e-11\n",
      "Iteration: 3656 Loss: 2.2653232227088264e-11\n",
      "Iteration: 3657 Loss: 2.265229069620056e-11\n",
      "Iteration: 3658 Loss: 2.265125559274017e-11\n",
      "Iteration: 3659 Loss: 2.2650339364440242e-11\n",
      "Iteration: 3660 Loss: 2.2649425071095022e-11\n",
      "Iteration: 3661 Loss: 2.2648516972573235e-11\n",
      "Iteration: 3662 Loss: 2.2647618204326344e-11\n",
      "Iteration: 3663 Loss: 2.2646716980392006e-11\n",
      "Iteration: 3664 Loss: 2.2645819440485093e-11\n",
      "Iteration: 3665 Loss: 2.2644948928460127e-11\n",
      "Iteration: 3666 Loss: 2.2644062762258833e-11\n",
      "Iteration: 3667 Loss: 2.2643201599051585e-11\n",
      "Iteration: 3668 Loss: 2.2642439358029248e-11\n",
      "Iteration: 3669 Loss: 2.2641577367043895e-11\n",
      "Iteration: 3670 Loss: 2.264072536468564e-11\n",
      "Iteration: 3671 Loss: 2.2639875727385703e-11\n",
      "Iteration: 3672 Loss: 2.26390319162161e-11\n",
      "Iteration: 3673 Loss: 2.2638196439388946e-11\n",
      "Iteration: 3674 Loss: 2.263737748317021e-11\n",
      "Iteration: 3675 Loss: 2.2636553227708288e-11\n",
      "Iteration: 3676 Loss: 2.2635737332202523e-11\n",
      "Iteration: 3677 Loss: 2.263492223068783e-11\n",
      "Iteration: 3678 Loss: 2.263411551730631e-11\n",
      "Iteration: 3679 Loss: 2.2633318003790028e-11\n",
      "Iteration: 3680 Loss: 2.263252432598011e-11\n",
      "Iteration: 3681 Loss: 2.2631734959324117e-11\n",
      "Iteration: 3682 Loss: 2.2630962075032322e-11\n",
      "Iteration: 3683 Loss: 2.2630179509360993e-11\n",
      "Iteration: 3684 Loss: 2.2629412325553477e-11\n",
      "Iteration: 3685 Loss: 2.2628636738369815e-11\n",
      "Iteration: 3686 Loss: 2.26278797296763e-11\n",
      "Iteration: 3687 Loss: 2.2627127456621408e-11\n",
      "Iteration: 3688 Loss: 2.2626376634102917e-11\n",
      "Iteration: 3689 Loss: 2.2625632786436454e-11\n",
      "Iteration: 3690 Loss: 2.2624895093978836e-11\n",
      "Iteration: 3691 Loss: 2.2624165406572582e-11\n",
      "Iteration: 3692 Loss: 2.2623430515112504e-11\n",
      "Iteration: 3693 Loss: 2.2622707592241056e-11\n",
      "Iteration: 3694 Loss: 2.2621989198070637e-11\n",
      "Iteration: 3695 Loss: 2.2621273968883702e-11\n",
      "Iteration: 3696 Loss: 2.2620564033407364e-11\n",
      "Iteration: 3697 Loss: 2.2619863259400486e-11\n",
      "Iteration: 3698 Loss: 2.2619165772105552e-11\n",
      "Iteration: 3699 Loss: 2.2618572406945634e-11\n",
      "Iteration: 3700 Loss: 2.2617786608530572e-11\n",
      "Iteration: 3701 Loss: 2.2617106747177806e-11\n",
      "Iteration: 3702 Loss: 2.2616415984920605e-11\n",
      "Iteration: 3703 Loss: 2.2615647937642696e-11\n",
      "Iteration: 3704 Loss: 2.2614979538958984e-11\n",
      "Iteration: 3705 Loss: 2.2614314131987426e-11\n",
      "Iteration: 3706 Loss: 2.2613652122455163e-11\n",
      "Iteration: 3707 Loss: 2.2613001399955465e-11\n",
      "Iteration: 3708 Loss: 2.2612358631417164e-11\n",
      "Iteration: 3709 Loss: 2.2611707496770845e-11\n",
      "Iteration: 3710 Loss: 2.261106637215137e-11\n",
      "Iteration: 3711 Loss: 2.261053120377663e-11\n",
      "Iteration: 3712 Loss: 2.2609908833749414e-11\n",
      "Iteration: 3713 Loss: 2.260927739073752e-11\n",
      "Iteration: 3714 Loss: 2.2608649431761844e-11\n",
      "Iteration: 3715 Loss: 2.2608032069420827e-11\n",
      "Iteration: 3716 Loss: 2.260730803920661e-11\n",
      "Iteration: 3717 Loss: 2.2606810369455584e-11\n",
      "Iteration: 3718 Loss: 2.260620812342861e-11\n",
      "Iteration: 3719 Loss: 2.2605612220284543e-11\n",
      "Iteration: 3720 Loss: 2.2605005820194974e-11\n",
      "Iteration: 3721 Loss: 2.2604517433039734e-11\n",
      "Iteration: 3722 Loss: 2.260383182279374e-11\n",
      "Iteration: 3723 Loss: 2.260324998829869e-11\n",
      "Iteration: 3724 Loss: 2.260266518802419e-11\n",
      "Iteration: 3725 Loss: 2.2602096231478432e-11\n",
      "Iteration: 3726 Loss: 2.26015244265848e-11\n",
      "Iteration: 3727 Loss: 2.2600950993405947e-11\n",
      "Iteration: 3728 Loss: 2.2600391566898492e-11\n",
      "Iteration: 3729 Loss: 2.2599829254269168e-11\n",
      "Iteration: 3730 Loss: 2.2599273492577846e-11\n",
      "Iteration: 3731 Loss: 2.2598722085794812e-11\n",
      "Iteration: 3732 Loss: 2.2598171512223215e-11\n",
      "Iteration: 3733 Loss: 2.2597634140600124e-11\n",
      "Iteration: 3734 Loss: 2.259710245262683e-11\n",
      "Iteration: 3735 Loss: 2.259656600586328e-11\n",
      "Iteration: 3736 Loss: 2.2596026084935724e-11\n",
      "Iteration: 3737 Loss: 2.2595508565670253e-11\n",
      "Iteration: 3738 Loss: 2.2594972420223077e-11\n",
      "Iteration: 3739 Loss: 2.2594454774457576e-11\n",
      "Iteration: 3740 Loss: 2.2593950294779455e-11\n",
      "Iteration: 3741 Loss: 2.2593423337050475e-11\n",
      "Iteration: 3742 Loss: 2.2592920437669038e-11\n",
      "Iteration: 3743 Loss: 2.2592409904824196e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3744 Loss: 2.2591904971325148e-11\n",
      "Iteration: 3745 Loss: 2.2591411926166156e-11\n",
      "Iteration: 3746 Loss: 2.2590918989787396e-11\n",
      "Iteration: 3747 Loss: 2.2590421149451184e-11\n",
      "Iteration: 3748 Loss: 2.258993112273409e-11\n",
      "Iteration: 3749 Loss: 2.258944419934915e-11\n",
      "Iteration: 3750 Loss: 2.2588968739815204e-11\n",
      "Iteration: 3751 Loss: 2.258848544763125e-11\n",
      "Iteration: 3752 Loss: 2.258801431302051e-11\n",
      "Iteration: 3753 Loss: 2.2587551635743826e-11\n",
      "Iteration: 3754 Loss: 2.25870860043864e-11\n",
      "Iteration: 3755 Loss: 2.25866152979235e-11\n",
      "Iteration: 3756 Loss: 2.2586152976713682e-11\n",
      "Iteration: 3757 Loss: 2.258569994109327e-11\n",
      "Iteration: 3758 Loss: 2.258524824794602e-11\n",
      "Iteration: 3759 Loss: 2.2584797896561373e-11\n",
      "Iteration: 3760 Loss: 2.258434756754736e-11\n",
      "Iteration: 3761 Loss: 2.2583906982607967e-11\n",
      "Iteration: 3762 Loss: 2.2583462540255646e-11\n",
      "Iteration: 3763 Loss: 2.258301903549413e-11\n",
      "Iteration: 3764 Loss: 2.2582591931076886e-11\n",
      "Iteration: 3765 Loss: 2.2582162514412603e-11\n",
      "Iteration: 3766 Loss: 2.258173054559371e-11\n",
      "Iteration: 3767 Loss: 2.258129886087324e-11\n",
      "Iteration: 3768 Loss: 2.2580881454482658e-11\n",
      "Iteration: 3769 Loss: 2.2580461204399014e-11\n",
      "Iteration: 3770 Loss: 2.25800416484504e-11\n",
      "Iteration: 3771 Loss: 2.25796247046576e-11\n",
      "Iteration: 3772 Loss: 2.257921414729468e-11\n",
      "Iteration: 3773 Loss: 2.257881092567325e-11\n",
      "Iteration: 3774 Loss: 2.2578401028544344e-11\n",
      "Iteration: 3775 Loss: 2.257799659110377e-11\n",
      "Iteration: 3776 Loss: 2.2577608095234845e-11\n",
      "Iteration: 3777 Loss: 2.2577220760613694e-11\n",
      "Iteration: 3778 Loss: 2.2576809806287233e-11\n",
      "Iteration: 3779 Loss: 2.2576419822993862e-11\n",
      "Iteration: 3780 Loss: 2.2576036102858032e-11\n",
      "Iteration: 3781 Loss: 2.2575653415704686e-11\n",
      "Iteration: 3782 Loss: 2.2575274288703792e-11\n",
      "Iteration: 3783 Loss: 2.2574884541838228e-11\n",
      "Iteration: 3784 Loss: 2.257451821757858e-11\n",
      "Iteration: 3785 Loss: 2.257413487404664e-11\n",
      "Iteration: 3786 Loss: 2.257376678034534e-11\n",
      "Iteration: 3787 Loss: 2.257340090179437e-11\n",
      "Iteration: 3788 Loss: 2.2573021791131928e-11\n",
      "Iteration: 3789 Loss: 2.257266534716733e-11\n",
      "Iteration: 3790 Loss: 2.2572310513711458e-11\n",
      "Iteration: 3791 Loss: 2.2571950640433656e-11\n",
      "Iteration: 3792 Loss: 2.2571584842194845e-11\n",
      "Iteration: 3793 Loss: 2.2571237773169088e-11\n",
      "Iteration: 3794 Loss: 2.257088615696168e-11\n",
      "Iteration: 3795 Loss: 2.257053303256154e-11\n",
      "Iteration: 3796 Loss: 2.2570193427855444e-11\n",
      "Iteration: 3797 Loss: 2.256985341350738e-11\n",
      "Iteration: 3798 Loss: 2.2569515070326518e-11\n",
      "Iteration: 3799 Loss: 2.2569176211796906e-11\n",
      "Iteration: 3800 Loss: 2.256873785365641e-11\n",
      "Iteration: 3801 Loss: 2.2568509837134977e-11\n",
      "Iteration: 3802 Loss: 2.2568170778630028e-11\n",
      "Iteration: 3803 Loss: 2.2567840675449428e-11\n",
      "Iteration: 3804 Loss: 2.2567620282042302e-11\n",
      "Iteration: 3805 Loss: 2.2567292367769218e-11\n",
      "Iteration: 3806 Loss: 2.256698267037323e-11\n",
      "Iteration: 3807 Loss: 2.256665052269421e-11\n",
      "Iteration: 3808 Loss: 2.2566337504715406e-11\n",
      "Iteration: 3809 Loss: 2.2566029724947378e-11\n",
      "Iteration: 3810 Loss: 2.2565612455598925e-11\n",
      "Iteration: 3811 Loss: 2.256529853231328e-11\n",
      "Iteration: 3812 Loss: 2.2564990399604155e-11\n",
      "Iteration: 3813 Loss: 2.2564677565340292e-11\n",
      "Iteration: 3814 Loss: 2.256437710475668e-11\n",
      "Iteration: 3815 Loss: 2.2564075124247524e-11\n",
      "Iteration: 3816 Loss: 2.2563785717975273e-11\n",
      "Iteration: 3817 Loss: 2.2563477777469263e-11\n",
      "Iteration: 3818 Loss: 2.2563185397394647e-11\n",
      "Iteration: 3819 Loss: 2.2562896413113742e-11\n",
      "Iteration: 3820 Loss: 2.2562503643260895e-11\n",
      "Iteration: 3821 Loss: 2.256221588659928e-11\n",
      "Iteration: 3822 Loss: 2.2561931297401474e-11\n",
      "Iteration: 3823 Loss: 2.2561741955627003e-11\n",
      "Iteration: 3824 Loss: 2.2561462844458986e-11\n",
      "Iteration: 3825 Loss: 2.25611867439201e-11\n",
      "Iteration: 3826 Loss: 2.2560913743137897e-11\n",
      "Iteration: 3827 Loss: 2.256071674965091e-11\n",
      "Iteration: 3828 Loss: 2.2560451933076293e-11\n",
      "Iteration: 3829 Loss: 2.2560076271510943e-11\n",
      "Iteration: 3830 Loss: 2.255981283211139e-11\n",
      "Iteration: 3831 Loss: 2.2559553580922952e-11\n",
      "Iteration: 3832 Loss: 2.2559275005818168e-11\n",
      "Iteration: 3833 Loss: 2.2559011304860687e-11\n",
      "Iteration: 3834 Loss: 2.255874852406981e-11\n",
      "Iteration: 3835 Loss: 2.2558486708262225e-11\n",
      "Iteration: 3836 Loss: 2.25582241887849e-11\n",
      "Iteration: 3837 Loss: 2.255795414386526e-11\n",
      "Iteration: 3838 Loss: 2.2557706496262344e-11\n",
      "Iteration: 3839 Loss: 2.255745625425387e-11\n",
      "Iteration: 3840 Loss: 2.2557205408432502e-11\n",
      "Iteration: 3841 Loss: 2.2556950767268016e-11\n",
      "Iteration: 3842 Loss: 2.255670660693478e-11\n",
      "Iteration: 3843 Loss: 2.255646888840506e-11\n",
      "Iteration: 3844 Loss: 2.2556220974652815e-11\n",
      "Iteration: 3845 Loss: 2.2555972810443584e-11\n",
      "Iteration: 3846 Loss: 2.2555623045413774e-11\n",
      "Iteration: 3847 Loss: 2.2555416920124152e-11\n",
      "Iteration: 3848 Loss: 2.2555208611295886e-11\n",
      "Iteration: 3849 Loss: 2.2554971266895398e-11\n",
      "Iteration: 3850 Loss: 2.255472926983344e-11\n",
      "Iteration: 3851 Loss: 2.2554507447546658e-11\n",
      "Iteration: 3852 Loss: 2.255427301895683e-11\n",
      "Iteration: 3853 Loss: 2.2554038559716544e-11\n",
      "Iteration: 3854 Loss: 2.2553809179127e-11\n",
      "Iteration: 3855 Loss: 2.2553589244284277e-11\n",
      "Iteration: 3856 Loss: 2.2553468251065027e-11\n",
      "Iteration: 3857 Loss: 2.2553249459410093e-11\n",
      "Iteration: 3858 Loss: 2.255302857315343e-11\n",
      "Iteration: 3859 Loss: 2.2552804040089252e-11\n",
      "Iteration: 3860 Loss: 2.255258838108564e-11\n",
      "Iteration: 3861 Loss: 2.2552369441339986e-11\n",
      "Iteration: 3862 Loss: 2.2552158056041313e-11\n",
      "Iteration: 3863 Loss: 2.255194174021145e-11\n",
      "Iteration: 3864 Loss: 2.2551720641895807e-11\n",
      "Iteration: 3865 Loss: 2.255150455681871e-11\n",
      "Iteration: 3866 Loss: 2.255131274652236e-11\n",
      "Iteration: 3867 Loss: 2.255100392608346e-11\n",
      "Iteration: 3868 Loss: 2.255079227070464e-11\n",
      "Iteration: 3869 Loss: 2.255059141440551e-11\n",
      "Iteration: 3870 Loss: 2.255039043132127e-11\n",
      "Iteration: 3871 Loss: 2.2550190218464564e-11\n",
      "Iteration: 3872 Loss: 2.2549991062370385e-11\n",
      "Iteration: 3873 Loss: 2.254979391506764e-11\n",
      "Iteration: 3874 Loss: 2.254956945347158e-11\n",
      "Iteration: 3875 Loss: 2.2549351237256712e-11\n",
      "Iteration: 3876 Loss: 2.2549143588613877e-11\n",
      "Iteration: 3877 Loss: 2.2548950407791134e-11\n",
      "Iteration: 3878 Loss: 2.2548764003512532e-11\n",
      "Iteration: 3879 Loss: 2.2548565717199645e-11\n",
      "Iteration: 3880 Loss: 2.254838043740915e-11\n",
      "Iteration: 3881 Loss: 2.2548203569602404e-11\n",
      "Iteration: 3882 Loss: 2.254799833297174e-11\n",
      "Iteration: 3883 Loss: 2.2547832329463535e-11\n",
      "Iteration: 3884 Loss: 2.2547642469641614e-11\n",
      "Iteration: 3885 Loss: 2.2547572654200448e-11\n",
      "Iteration: 3886 Loss: 2.2547388599883536e-11\n",
      "Iteration: 3887 Loss: 2.2547208389562955e-11\n",
      "Iteration: 3888 Loss: 2.2547135821682333e-11\n",
      "Iteration: 3889 Loss: 2.254686101199051e-11\n",
      "Iteration: 3890 Loss: 2.2546683948402255e-11\n",
      "Iteration: 3891 Loss: 2.2546512577874817e-11\n",
      "Iteration: 3892 Loss: 2.2546338094671417e-11\n",
      "Iteration: 3893 Loss: 2.25461629734991e-11\n",
      "Iteration: 3894 Loss: 2.2546014054012288e-11\n",
      "Iteration: 3895 Loss: 2.2545878110929716e-11\n",
      "Iteration: 3896 Loss: 2.254570301756273e-11\n",
      "Iteration: 3897 Loss: 2.254554078737733e-11\n",
      "Iteration: 3898 Loss: 2.2545377691757953e-11\n",
      "Iteration: 3899 Loss: 2.2545205730208717e-11\n",
      "Iteration: 3900 Loss: 2.2544944360418424e-11\n",
      "Iteration: 3901 Loss: 2.2544880144953948e-11\n",
      "Iteration: 3902 Loss: 2.2544717941854337e-11\n",
      "Iteration: 3903 Loss: 2.2544549999133942e-11\n",
      "Iteration: 3904 Loss: 2.2544399349979594e-11\n",
      "Iteration: 3905 Loss: 2.2544246603913656e-11\n",
      "Iteration: 3906 Loss: 2.2544089917236595e-11\n",
      "Iteration: 3907 Loss: 2.2543932421087944e-11\n",
      "Iteration: 3908 Loss: 2.2543781307049067e-11\n",
      "Iteration: 3909 Loss: 2.2543624072549513e-11\n",
      "Iteration: 3910 Loss: 2.254348171810791e-11\n",
      "Iteration: 3911 Loss: 2.2543325859880336e-11\n",
      "Iteration: 3912 Loss: 2.254318111187138e-11\n",
      "Iteration: 3913 Loss: 2.2543031499207373e-11\n",
      "Iteration: 3914 Loss: 2.2542872853015596e-11\n",
      "Iteration: 3915 Loss: 2.2542734439224946e-11\n",
      "Iteration: 3916 Loss: 2.2542586442768366e-11\n",
      "Iteration: 3917 Loss: 2.254243571778044e-11\n",
      "Iteration: 3918 Loss: 2.2542299468186217e-11\n",
      "Iteration: 3919 Loss: 2.2542155167816432e-11\n",
      "Iteration: 3920 Loss: 2.2542004438990807e-11\n",
      "Iteration: 3921 Loss: 2.25418647066225e-11\n",
      "Iteration: 3922 Loss: 2.2541731724199545e-11\n",
      "Iteration: 3923 Loss: 2.254159047904456e-11\n",
      "Iteration: 3924 Loss: 2.2541454703014466e-11\n",
      "Iteration: 3925 Loss: 2.254120826708607e-11\n",
      "Iteration: 3926 Loss: 2.2541085582945614e-11\n",
      "Iteration: 3927 Loss: 2.2540940895061994e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3928 Loss: 2.2540817928857357e-11\n",
      "Iteration: 3929 Loss: 2.2540682684207953e-11\n",
      "Iteration: 3930 Loss: 2.254055085016365e-11\n",
      "Iteration: 3931 Loss: 2.254042667617911e-11\n",
      "Iteration: 3932 Loss: 2.2540299229843897e-11\n",
      "Iteration: 3933 Loss: 2.2540266573721628e-11\n",
      "Iteration: 3934 Loss: 2.254014230016143e-11\n",
      "Iteration: 3935 Loss: 2.2540017724192226e-11\n",
      "Iteration: 3936 Loss: 2.2539892106165923e-11\n",
      "Iteration: 3937 Loss: 2.253977255911314e-11\n",
      "Iteration: 3938 Loss: 2.2539638987328955e-11\n",
      "Iteration: 3939 Loss: 2.253951953773325e-11\n",
      "Iteration: 3940 Loss: 2.253939882887707e-11\n",
      "Iteration: 3941 Loss: 2.2539276283047434e-11\n",
      "Iteration: 3942 Loss: 2.253915104471773e-11\n",
      "Iteration: 3943 Loss: 2.2539031563857067e-11\n",
      "Iteration: 3944 Loss: 2.2538919477734063e-11\n",
      "Iteration: 3945 Loss: 2.2538794344118937e-11\n",
      "Iteration: 3946 Loss: 2.2538680215870195e-11\n",
      "Iteration: 3947 Loss: 2.2538566721916383e-11\n",
      "Iteration: 3948 Loss: 2.2538436436690374e-11\n",
      "Iteration: 3949 Loss: 2.2538323564877713e-11\n",
      "Iteration: 3950 Loss: 2.2538215811428393e-11\n",
      "Iteration: 3951 Loss: 2.25380956354044e-11\n",
      "Iteration: 3952 Loss: 2.253788572679095e-11\n",
      "Iteration: 3953 Loss: 2.2537879410219546e-11\n",
      "Iteration: 3954 Loss: 2.2537876676966696e-11\n",
      "Iteration: 3955 Loss: 2.253776562613791e-11\n",
      "Iteration: 3956 Loss: 2.2537655984544083e-11\n",
      "Iteration: 3957 Loss: 2.2537548444732768e-11\n",
      "Iteration: 3958 Loss: 2.2537452390789606e-11\n",
      "Iteration: 3959 Loss: 2.2537234006657356e-11\n",
      "Iteration: 3960 Loss: 2.2537131639979606e-11\n",
      "Iteration: 3961 Loss: 2.2537016647444224e-11\n",
      "Iteration: 3962 Loss: 2.2536925187239362e-11\n",
      "Iteration: 3963 Loss: 2.2536821263081466e-11\n",
      "Iteration: 3964 Loss: 2.2536710776418346e-11\n",
      "Iteration: 3965 Loss: 2.2536610212700766e-11\n",
      "Iteration: 3966 Loss: 2.2536511246582055e-11\n",
      "Iteration: 3967 Loss: 2.2536414273635943e-11\n",
      "Iteration: 3968 Loss: 2.2536316765226145e-11\n",
      "Iteration: 3969 Loss: 2.2536212198630972e-11\n",
      "Iteration: 3970 Loss: 2.253611057330081e-11\n",
      "Iteration: 3971 Loss: 2.2536012970633346e-11\n",
      "Iteration: 3972 Loss: 2.2535919259257135e-11\n",
      "Iteration: 3973 Loss: 2.253581785761668e-11\n",
      "Iteration: 3974 Loss: 2.2535720870678786e-11\n",
      "Iteration: 3975 Loss: 2.2535628121209767e-11\n",
      "Iteration: 3976 Loss: 2.2535530260499423e-11\n",
      "Iteration: 3977 Loss: 2.253543788254869e-11\n",
      "Iteration: 3978 Loss: 2.253533833928924e-11\n",
      "Iteration: 3979 Loss: 2.253525572659113e-11\n",
      "Iteration: 3980 Loss: 2.253515800376789e-11\n",
      "Iteration: 3981 Loss: 2.2535071284308702e-11\n",
      "Iteration: 3982 Loss: 2.2534985857669812e-11\n",
      "Iteration: 3983 Loss: 2.25348872459954e-11\n",
      "Iteration: 3984 Loss: 2.2534703054794036e-11\n",
      "Iteration: 3985 Loss: 2.2534710204851447e-11\n",
      "Iteration: 3986 Loss: 2.253462823473181e-11\n",
      "Iteration: 3987 Loss: 2.253454226993322e-11\n",
      "Iteration: 3988 Loss: 2.2534466328059238e-11\n",
      "Iteration: 3989 Loss: 2.2534276116801258e-11\n",
      "Iteration: 3990 Loss: 2.253419025549843e-11\n",
      "Iteration: 3991 Loss: 2.2534109979739574e-11\n",
      "Iteration: 3992 Loss: 2.253402012362393e-11\n",
      "Iteration: 3993 Loss: 2.2534042401656703e-11\n",
      "Iteration: 3994 Loss: 2.25339651050848e-11\n",
      "Iteration: 3995 Loss: 2.253387943302748e-11\n",
      "Iteration: 3996 Loss: 2.2533802358256786e-11\n",
      "Iteration: 3997 Loss: 2.2533723504299516e-11\n",
      "Iteration: 3998 Loss: 2.2533644181879606e-11\n",
      "Iteration: 3999 Loss: 2.2533555731340367e-11\n",
      "Iteration: 4000 Loss: 2.2533383736549664e-11\n",
      "Iteration: 4001 Loss: 2.2533305744702912e-11\n",
      "Iteration: 4002 Loss: 2.2533224910618536e-11\n",
      "Iteration: 4003 Loss: 2.253315056495963e-11\n",
      "Iteration: 4004 Loss: 2.25330702223164e-11\n",
      "Iteration: 4005 Loss: 2.253309417345116e-11\n",
      "Iteration: 4006 Loss: 2.2533021235794245e-11\n",
      "Iteration: 4007 Loss: 2.253284068495639e-11\n",
      "Iteration: 4008 Loss: 2.253276237934832e-11\n",
      "Iteration: 4009 Loss: 2.2532690542320946e-11\n",
      "Iteration: 4010 Loss: 2.253261438952535e-11\n",
      "Iteration: 4011 Loss: 2.2532544257555867e-11\n",
      "Iteration: 4012 Loss: 2.2532575828530548e-11\n",
      "Iteration: 4013 Loss: 2.2532494756029164e-11\n",
      "Iteration: 4014 Loss: 2.2532327254459253e-11\n",
      "Iteration: 4015 Loss: 2.2532355537206563e-11\n",
      "Iteration: 4016 Loss: 2.2532284122305545e-11\n",
      "Iteration: 4017 Loss: 2.253211112989512e-11\n",
      "Iteration: 4018 Loss: 2.2532040295974933e-11\n",
      "Iteration: 4019 Loss: 2.2532069779001488e-11\n",
      "Iteration: 4020 Loss: 2.253210858564133e-11\n",
      "Iteration: 4021 Loss: 2.253203073778186e-11\n",
      "Iteration: 4022 Loss: 2.2531959355235934e-11\n",
      "Iteration: 4023 Loss: 2.25318994956386e-11\n",
      "Iteration: 4024 Loss: 2.2531822777671016e-11\n",
      "Iteration: 4025 Loss: 2.2531768324654873e-11\n",
      "Iteration: 4026 Loss: 2.253169486403987e-11\n",
      "Iteration: 4027 Loss: 2.2531539476271515e-11\n",
      "Iteration: 4028 Loss: 2.2531465766592196e-11\n",
      "Iteration: 4029 Loss: 2.2531504156328513e-11\n",
      "Iteration: 4030 Loss: 2.2531440511779908e-11\n",
      "Iteration: 4031 Loss: 2.2531390804273432e-11\n",
      "Iteration: 4032 Loss: 2.2531321153780322e-11\n",
      "Iteration: 4033 Loss: 2.2531265847528996e-11\n",
      "Iteration: 4034 Loss: 2.2531203600819202e-11\n",
      "Iteration: 4035 Loss: 2.2531142415139543e-11\n",
      "Iteration: 4036 Loss: 2.2531083604212964e-11\n",
      "Iteration: 4037 Loss: 2.253102212995217e-11\n",
      "Iteration: 4038 Loss: 2.2530972644102134e-11\n",
      "Iteration: 4039 Loss: 2.25309093204326e-11\n",
      "Iteration: 4040 Loss: 2.253085453832354e-11\n",
      "Iteration: 4041 Loss: 2.2530793978807282e-11\n",
      "Iteration: 4042 Loss: 2.2530734722914996e-11\n",
      "Iteration: 4043 Loss: 2.2530669715318636e-11\n",
      "Iteration: 4044 Loss: 2.2530625172230372e-11\n",
      "Iteration: 4045 Loss: 2.253056777985963e-11\n",
      "Iteration: 4046 Loss: 2.2530508634507282e-11\n",
      "Iteration: 4047 Loss: 2.2530455690154505e-11\n",
      "Iteration: 4048 Loss: 2.253039137700462e-11\n",
      "Iteration: 4049 Loss: 2.2530243488009765e-11\n",
      "Iteration: 4050 Loss: 2.2530188201583557e-11\n",
      "Iteration: 4051 Loss: 2.2530129164165553e-11\n",
      "Iteration: 4052 Loss: 2.2530076189661028e-11\n",
      "Iteration: 4053 Loss: 2.2530020423118933e-11\n",
      "Iteration: 4054 Loss: 2.252997904325695e-11\n",
      "Iteration: 4055 Loss: 2.2529923180694735e-11\n",
      "Iteration: 4056 Loss: 2.2529870479184567e-11\n",
      "Iteration: 4057 Loss: 2.252992383961642e-11\n",
      "Iteration: 4058 Loss: 2.2529864562684866e-11\n",
      "Iteration: 4059 Loss: 2.252981420828393e-11\n",
      "Iteration: 4060 Loss: 2.2529757610253066e-11\n",
      "Iteration: 4061 Loss: 2.2529704856293178e-11\n",
      "Iteration: 4062 Loss: 2.2529555302975056e-11\n",
      "Iteration: 4063 Loss: 2.252950978632293e-11\n",
      "Iteration: 4064 Loss: 2.2529458494665447e-11\n",
      "Iteration: 4065 Loss: 2.2529408046714747e-11\n",
      "Iteration: 4066 Loss: 2.2529362268823962e-11\n",
      "Iteration: 4067 Loss: 2.2529307388875794e-11\n",
      "Iteration: 4068 Loss: 2.2529262076147568e-11\n",
      "Iteration: 4069 Loss: 2.2529211165192643e-11\n",
      "Iteration: 4070 Loss: 2.2529061842919242e-11\n",
      "Iteration: 4071 Loss: 2.252911111470036e-11\n",
      "Iteration: 4072 Loss: 2.2529066847506536e-11\n",
      "Iteration: 4073 Loss: 2.252902298961604e-11\n",
      "Iteration: 4074 Loss: 2.252897365858746e-11\n",
      "Iteration: 4075 Loss: 2.2528931868979847e-11\n",
      "Iteration: 4076 Loss: 2.2528878374875437e-11\n",
      "Iteration: 4077 Loss: 2.2528836401801e-11\n",
      "Iteration: 4078 Loss: 2.2528783027338103e-11\n",
      "Iteration: 4079 Loss: 2.2528740852831184e-11\n",
      "Iteration: 4080 Loss: 2.2528697367247386e-11\n",
      "Iteration: 4081 Loss: 2.2528652597328326e-11\n",
      "Iteration: 4082 Loss: 2.2528606613077954e-11\n",
      "Iteration: 4083 Loss: 2.252856763415923e-11\n",
      "Iteration: 4084 Loss: 2.2528534137511393e-11\n",
      "Iteration: 4085 Loss: 2.2528481021164716e-11\n",
      "Iteration: 4086 Loss: 2.252843322628844e-11\n",
      "Iteration: 4087 Loss: 2.2528399765048587e-11\n",
      "Iteration: 4088 Loss: 2.252834797696185e-11\n",
      "Iteration: 4089 Loss: 2.252831977228301e-11\n",
      "Iteration: 4090 Loss: 2.2528282086874388e-11\n",
      "Iteration: 4091 Loss: 2.252823943394448e-11\n",
      "Iteration: 4092 Loss: 2.2528187782613762e-11\n",
      "Iteration: 4093 Loss: 2.2528060078847955e-11\n",
      "Iteration: 4094 Loss: 2.2528012737935377e-11\n",
      "Iteration: 4095 Loss: 2.2528086679759284e-11\n",
      "Iteration: 4096 Loss: 2.2528135256046887e-11\n",
      "Iteration: 4097 Loss: 2.252810889017933e-11\n",
      "Iteration: 4098 Loss: 2.252806407900361e-11\n",
      "Iteration: 4099 Loss: 2.2528028498536864e-11\n",
      "Iteration: 4100 Loss: 2.252799215073252e-11\n",
      "Iteration: 4101 Loss: 2.2527955184753498e-11\n",
      "Iteration: 4102 Loss: 2.252792482488215e-11\n",
      "Iteration: 4103 Loss: 2.252788375488848e-11\n",
      "Iteration: 4104 Loss: 2.252784624346472e-11\n",
      "Iteration: 4105 Loss: 2.252781021602236e-11\n",
      "Iteration: 4106 Loss: 2.252778145043107e-11\n",
      "Iteration: 4107 Loss: 2.252774698752317e-11\n",
      "Iteration: 4108 Loss: 2.252760332958216e-11\n",
      "Iteration: 4109 Loss: 2.2527568993462117e-11\n",
      "Iteration: 4110 Loss: 2.2527537582342207e-11\n",
      "Iteration: 4111 Loss: 2.2527503083076022e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4112 Loss: 2.2527467114338427e-11\n",
      "Iteration: 4113 Loss: 2.2527437606861614e-11\n",
      "Iteration: 4114 Loss: 2.2527403265771876e-11\n",
      "Iteration: 4115 Loss: 2.252737443773297e-11\n",
      "Iteration: 4116 Loss: 2.2527343169698813e-11\n",
      "Iteration: 4117 Loss: 2.2527305654309087e-11\n",
      "Iteration: 4118 Loss: 2.2527269612717336e-11\n",
      "Iteration: 4119 Loss: 2.2527233162739938e-11\n",
      "Iteration: 4120 Loss: 2.252731952635692e-11\n",
      "Iteration: 4121 Loss: 2.2527381919924965e-11\n",
      "Iteration: 4122 Loss: 2.2527243432617493e-11\n",
      "Iteration: 4123 Loss: 2.2527214599753258e-11\n",
      "Iteration: 4124 Loss: 2.2527192298071587e-11\n",
      "Iteration: 4125 Loss: 2.2527152781203093e-11\n",
      "Iteration: 4126 Loss: 2.252712311668924e-11\n",
      "Iteration: 4127 Loss: 2.2527097452814568e-11\n",
      "Iteration: 4128 Loss: 2.252706509849675e-11\n",
      "Iteration: 4129 Loss: 2.252703573116598e-11\n",
      "Iteration: 4130 Loss: 2.2527003081850036e-11\n",
      "Iteration: 4131 Loss: 2.252697502920413e-11\n",
      "Iteration: 4132 Loss: 2.252694604070832e-11\n",
      "Iteration: 4133 Loss: 2.2526921993884213e-11\n",
      "Iteration: 4134 Loss: 2.252688533770554e-11\n",
      "Iteration: 4135 Loss: 2.2526861304349978e-11\n",
      "Iteration: 4136 Loss: 2.2526832130015808e-11\n",
      "Iteration: 4137 Loss: 2.252679979285992e-11\n",
      "Iteration: 4138 Loss: 2.2526773263935204e-11\n",
      "Iteration: 4139 Loss: 2.252674505185511e-11\n",
      "Iteration: 4140 Loss: 2.252672003432284e-11\n",
      "Iteration: 4141 Loss: 2.252668674517896e-11\n",
      "Iteration: 4142 Loss: 2.252666274824048e-11\n",
      "Iteration: 4143 Loss: 2.2526629679271828e-11\n",
      "Iteration: 4144 Loss: 2.2526600068960966e-11\n",
      "Iteration: 4145 Loss: 2.2526682140106927e-11\n",
      "Iteration: 4146 Loss: 2.2526657347163765e-11\n",
      "Iteration: 4147 Loss: 2.252663318806969e-11\n",
      "Iteration: 4148 Loss: 2.252659877820401e-11\n",
      "Iteration: 4149 Loss: 2.2526574046443863e-11\n",
      "Iteration: 4150 Loss: 2.2526550367805812e-11\n",
      "Iteration: 4151 Loss: 2.2526522470054192e-11\n",
      "Iteration: 4152 Loss: 2.2526497738801882e-11\n",
      "Iteration: 4153 Loss: 2.2526363652679033e-11\n",
      "Iteration: 4154 Loss: 2.2526341189260265e-11\n",
      "Iteration: 4155 Loss: 2.252631347501461e-11\n",
      "Iteration: 4156 Loss: 2.2526294279409896e-11\n",
      "Iteration: 4157 Loss: 2.2526269834441362e-11\n",
      "Iteration: 4158 Loss: 2.252624343386215e-11\n",
      "Iteration: 4159 Loss: 2.252621583007614e-11\n",
      "Iteration: 4160 Loss: 2.252619446193322e-11\n",
      "Iteration: 4161 Loss: 2.2526171986142804e-11\n",
      "Iteration: 4162 Loss: 2.2526147275877847e-11\n",
      "Iteration: 4163 Loss: 2.2526117340700876e-11\n",
      "Iteration: 4164 Loss: 2.252609816440646e-11\n",
      "Iteration: 4165 Loss: 2.2526070482591172e-11\n",
      "Iteration: 4166 Loss: 2.2526054706504347e-11\n",
      "Iteration: 4167 Loss: 2.252602306067843e-11\n",
      "Iteration: 4168 Loss: 2.2525998557850345e-11\n",
      "Iteration: 4169 Loss: 2.2525973998986957e-11\n",
      "Iteration: 4170 Loss: 2.2525954948950535e-11\n",
      "Iteration: 4171 Loss: 2.252593883665303e-11\n",
      "Iteration: 4172 Loss: 2.252580992463747e-11\n",
      "Iteration: 4173 Loss: 2.252578331006127e-11\n",
      "Iteration: 4174 Loss: 2.2525760217463354e-11\n",
      "Iteration: 4175 Loss: 2.2525741137719684e-11\n",
      "Iteration: 4176 Loss: 2.252571673662387e-11\n",
      "Iteration: 4177 Loss: 2.2525693912077872e-11\n",
      "Iteration: 4178 Loss: 2.2525663755019673e-11\n",
      "Iteration: 4179 Loss: 2.2525647847365664e-11\n",
      "Iteration: 4180 Loss: 2.2525626246918057e-11\n",
      "Iteration: 4181 Loss: 2.2525604851144598e-11\n",
      "Iteration: 4182 Loss: 2.252558190922076e-11\n",
      "Iteration: 4183 Loss: 2.2525560742543897e-11\n",
      "Iteration: 4184 Loss: 2.2525543430621436e-11\n",
      "Iteration: 4185 Loss: 2.252551951270981e-11\n",
      "Iteration: 4186 Loss: 2.2525500483617965e-11\n",
      "Iteration: 4187 Loss: 2.2525482974283658e-11\n",
      "Iteration: 4188 Loss: 2.2525452576374785e-11\n",
      "Iteration: 4189 Loss: 2.2525438448689168e-11\n",
      "Iteration: 4190 Loss: 2.252541165352188e-11\n",
      "Iteration: 4191 Loss: 2.25253899716306e-11\n",
      "Iteration: 4192 Loss: 2.2525375815978414e-11\n",
      "Iteration: 4193 Loss: 2.2525354643775334e-11\n",
      "Iteration: 4194 Loss: 2.2525333177344902e-11\n",
      "Iteration: 4195 Loss: 2.2525311819472976e-11\n",
      "Iteration: 4196 Loss: 2.252529240331207e-11\n",
      "Iteration: 4197 Loss: 2.2525275551343464e-11\n",
      "Iteration: 4198 Loss: 2.2525269564298124e-11\n",
      "Iteration: 4199 Loss: 2.2525243059569766e-11\n",
      "Iteration: 4200 Loss: 2.2525216590121955e-11\n",
      "Iteration: 4201 Loss: 2.2525196733065435e-11\n",
      "Iteration: 4202 Loss: 2.2525175580352304e-11\n",
      "Iteration: 4203 Loss: 2.252516059874257e-11\n",
      "Iteration: 4204 Loss: 2.2525149576220335e-11\n",
      "Iteration: 4205 Loss: 2.2525123023104406e-11\n",
      "Iteration: 4206 Loss: 2.2525209780383014e-11\n",
      "Iteration: 4207 Loss: 2.252518462117903e-11\n",
      "Iteration: 4208 Loss: 2.2525177069921364e-11\n",
      "Iteration: 4209 Loss: 2.2525059967618913e-11\n",
      "Iteration: 4210 Loss: 2.2525041876047314e-11\n",
      "Iteration: 4211 Loss: 2.252502307435726e-11\n",
      "Iteration: 4212 Loss: 2.2525003541740438e-11\n",
      "Iteration: 4213 Loss: 2.252498487579269e-11\n",
      "Iteration: 4214 Loss: 2.2524975680058785e-11\n",
      "Iteration: 4215 Loss: 2.252495054012156e-11\n",
      "Iteration: 4216 Loss: 2.2524938031004423e-11\n",
      "Iteration: 4217 Loss: 2.2524919456787778e-11\n",
      "Iteration: 4218 Loss: 2.252501649743922e-11\n",
      "Iteration: 4219 Loss: 2.2524991132849222e-11\n",
      "Iteration: 4220 Loss: 2.2524972732248872e-11\n",
      "Iteration: 4221 Loss: 2.252495469552367e-11\n",
      "Iteration: 4222 Loss: 2.2524941886319454e-11\n",
      "Iteration: 4223 Loss: 2.252492991703229e-11\n",
      "Iteration: 4224 Loss: 2.252502008703664e-11\n",
      "Iteration: 4225 Loss: 2.252489902819996e-11\n",
      "Iteration: 4226 Loss: 2.252488027745651e-11\n",
      "Iteration: 4227 Loss: 2.2524860102711703e-11\n",
      "Iteration: 4228 Loss: 2.2524851078189628e-11\n",
      "Iteration: 4229 Loss: 2.252483254564797e-11\n",
      "Iteration: 4230 Loss: 2.2524822967350195e-11\n",
      "Iteration: 4231 Loss: 2.2524912462802902e-11\n",
      "Iteration: 4232 Loss: 2.252489071493202e-11\n",
      "Iteration: 4233 Loss: 2.2524884682430557e-11\n",
      "Iteration: 4234 Loss: 2.2524875164471643e-11\n",
      "Iteration: 4235 Loss: 2.252486032891394e-11\n",
      "Iteration: 4236 Loss: 2.252483938889469e-11\n",
      "Iteration: 4237 Loss: 2.2524832087914837e-11\n",
      "Iteration: 4238 Loss: 2.2524822207954713e-11\n",
      "Iteration: 4239 Loss: 2.2524697431874133e-11\n",
      "Iteration: 4240 Loss: 2.2524682307879207e-11\n",
      "Iteration: 4241 Loss: 2.2524677573330724e-11\n",
      "Iteration: 4242 Loss: 2.2524664495317406e-11\n",
      "Iteration: 4243 Loss: 2.252465171143454e-11\n",
      "Iteration: 4244 Loss: 2.2524641819212292e-11\n",
      "Iteration: 4245 Loss: 2.252452942988457e-11\n",
      "Iteration: 4246 Loss: 2.252451646368494e-11\n",
      "Iteration: 4247 Loss: 2.252450805910468e-11\n",
      "Iteration: 4248 Loss: 2.2524495008381392e-11\n",
      "Iteration: 4249 Loss: 2.252448813433659e-11\n",
      "Iteration: 4250 Loss: 2.2524577548971386e-11\n",
      "Iteration: 4251 Loss: 2.2524568174061032e-11\n",
      "Iteration: 4252 Loss: 2.2524555889768386e-11\n",
      "Iteration: 4253 Loss: 2.252454358964601e-11\n",
      "Iteration: 4254 Loss: 2.2524535051187753e-11\n",
      "Iteration: 4255 Loss: 2.25245221538298e-11\n",
      "Iteration: 4256 Loss: 2.2524513690193388e-11\n",
      "Iteration: 4257 Loss: 2.2524503671658016e-11\n",
      "Iteration: 4258 Loss: 2.2524494580508277e-11\n",
      "Iteration: 4259 Loss: 2.2524485163732426e-11\n",
      "Iteration: 4260 Loss: 2.2524472272672938e-11\n",
      "Iteration: 4261 Loss: 2.252436107077198e-11\n",
      "Iteration: 4262 Loss: 2.2524351262450215e-11\n",
      "Iteration: 4263 Loss: 2.25243416277675e-11\n",
      "Iteration: 4264 Loss: 2.2524435697872642e-11\n",
      "Iteration: 4265 Loss: 2.252432447712468e-11\n",
      "Iteration: 4266 Loss: 2.2524316056028084e-11\n",
      "Iteration: 4267 Loss: 2.252440838213192e-11\n",
      "Iteration: 4268 Loss: 2.2524400069872118e-11\n",
      "Iteration: 4269 Loss: 2.2524390100415575e-11\n",
      "Iteration: 4270 Loss: 2.252438135919557e-11\n",
      "Iteration: 4271 Loss: 2.252437337889183e-11\n",
      "Iteration: 4272 Loss: 2.2524364827709064e-11\n",
      "Iteration: 4273 Loss: 2.2524354515359956e-11\n",
      "Iteration: 4274 Loss: 2.2524346417480047e-11\n",
      "Iteration: 4275 Loss: 2.252433929957699e-11\n",
      "Iteration: 4276 Loss: 2.2524330774826282e-11\n",
      "Iteration: 4277 Loss: 2.252431932970576e-11\n",
      "Iteration: 4278 Loss: 2.2524312583885854e-11\n",
      "Iteration: 4279 Loss: 2.2524298970998735e-11\n",
      "Iteration: 4280 Loss: 2.2524290576097064e-11\n",
      "Iteration: 4281 Loss: 2.2524177657301607e-11\n",
      "Iteration: 4282 Loss: 2.2524175243989814e-11\n",
      "Iteration: 4283 Loss: 2.252416439835834e-11\n",
      "Iteration: 4284 Loss: 2.2524155660107446e-11\n",
      "Iteration: 4285 Loss: 2.2524144607037955e-11\n",
      "Iteration: 4286 Loss: 2.2524138829142088e-11\n",
      "Iteration: 4287 Loss: 2.2524133469097277e-11\n",
      "Iteration: 4288 Loss: 2.2524125139415605e-11\n",
      "Iteration: 4289 Loss: 2.252412456534506e-11\n",
      "Iteration: 4290 Loss: 2.252411092941047e-11\n",
      "Iteration: 4291 Loss: 2.2524099912863113e-11\n",
      "Iteration: 4292 Loss: 2.2524083614956387e-11\n",
      "Iteration: 4293 Loss: 2.252407463186055e-11\n",
      "Iteration: 4294 Loss: 2.2524067035812987e-11\n",
      "Iteration: 4295 Loss: 2.252406131129568e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4296 Loss: 2.2524150358455376e-11\n",
      "Iteration: 4297 Loss: 2.252414668894048e-11\n",
      "Iteration: 4298 Loss: 2.2524141785554108e-11\n",
      "Iteration: 4299 Loss: 2.2524133034051395e-11\n",
      "Iteration: 4300 Loss: 2.252412786281346e-11\n",
      "Iteration: 4301 Loss: 2.2524019864165338e-11\n",
      "Iteration: 4302 Loss: 2.2523997891937698e-11\n",
      "Iteration: 4303 Loss: 2.252409528331207e-11\n",
      "Iteration: 4304 Loss: 2.2523992247381908e-11\n",
      "Iteration: 4305 Loss: 2.2524085927614197e-11\n",
      "Iteration: 4306 Loss: 2.2524083396061536e-11\n",
      "Iteration: 4307 Loss: 2.2524074020108854e-11\n",
      "Iteration: 4308 Loss: 2.2524059949438047e-11\n",
      "Iteration: 4309 Loss: 2.2524060199487114e-11\n",
      "Iteration: 4310 Loss: 2.2524053313539942e-11\n",
      "Iteration: 4311 Loss: 2.252415005554704e-11\n",
      "Iteration: 4312 Loss: 2.2524136862809563e-11\n",
      "Iteration: 4313 Loss: 2.2524023070341907e-11\n",
      "Iteration: 4314 Loss: 2.2524023260800867e-11\n",
      "Iteration: 4315 Loss: 2.252401647824957e-11\n",
      "Iteration: 4316 Loss: 2.252401094279673e-11\n",
      "Iteration: 4317 Loss: 2.2524002640225794e-11\n",
      "Iteration: 4318 Loss: 2.2523997025474478e-11\n",
      "Iteration: 4319 Loss: 2.2523990203545722e-11\n",
      "Iteration: 4320 Loss: 2.2523984680543384e-11\n",
      "Iteration: 4321 Loss: 2.2523978851476344e-11\n",
      "Iteration: 4322 Loss: 2.252397197490612e-11\n",
      "Iteration: 4323 Loss: 2.2523963915167016e-11\n",
      "Iteration: 4324 Loss: 2.2523959581752956e-11\n",
      "Iteration: 4325 Loss: 2.2523952551165587e-11\n",
      "Iteration: 4326 Loss: 2.2523945906576254e-11\n",
      "Iteration: 4327 Loss: 2.252394021888352e-11\n",
      "Iteration: 4328 Loss: 2.2523934992105363e-11\n",
      "Iteration: 4329 Loss: 2.2523930769885887e-11\n",
      "Iteration: 4330 Loss: 2.252392663825658e-11\n",
      "Iteration: 4331 Loss: 2.2523918457211644e-11\n",
      "Iteration: 4332 Loss: 2.2523914146908302e-11\n",
      "Iteration: 4333 Loss: 2.2523908847227972e-11\n",
      "Iteration: 4334 Loss: 2.2523904977272255e-11\n",
      "Iteration: 4335 Loss: 2.2523897790272096e-11\n",
      "Iteration: 4336 Loss: 2.252389236666383e-11\n",
      "Iteration: 4337 Loss: 2.2523886793894013e-11\n",
      "Iteration: 4338 Loss: 2.252387978329561e-11\n",
      "Iteration: 4339 Loss: 2.2523875579144898e-11\n",
      "Iteration: 4340 Loss: 2.2523871635521975e-11\n",
      "Iteration: 4341 Loss: 2.2523967432744493e-11\n",
      "Iteration: 4342 Loss: 2.2523963398221436e-11\n",
      "Iteration: 4343 Loss: 2.2523956546597255e-11\n",
      "Iteration: 4344 Loss: 2.252384801443547e-11\n",
      "Iteration: 4345 Loss: 2.2523740297080787e-11\n",
      "Iteration: 4346 Loss: 2.2523736486208685e-11\n",
      "Iteration: 4347 Loss: 2.252373228275565e-11\n",
      "Iteration: 4348 Loss: 2.2523725168798523e-11\n",
      "Iteration: 4349 Loss: 2.2523822193456125e-11\n",
      "Iteration: 4350 Loss: 2.2523921048864756e-11\n",
      "Iteration: 4351 Loss: 2.252391557611174e-11\n",
      "Iteration: 4352 Loss: 2.2523911656126427e-11\n",
      "Iteration: 4353 Loss: 2.2523804497823532e-11\n",
      "Iteration: 4354 Loss: 2.2523796501785587e-11\n",
      "Iteration: 4355 Loss: 2.252379251260159e-11\n",
      "Iteration: 4356 Loss: 2.2523788552972985e-11\n",
      "Iteration: 4357 Loss: 2.252378311508472e-11\n",
      "Iteration: 4358 Loss: 2.252377762264023e-11\n",
      "Iteration: 4359 Loss: 2.2523774800170737e-11\n",
      "Iteration: 4360 Loss: 2.2523769493340677e-11\n",
      "Iteration: 4361 Loss: 2.2523765324976317e-11\n",
      "Iteration: 4362 Loss: 2.2523762596669474e-11\n",
      "Iteration: 4363 Loss: 2.2523757317330178e-11\n",
      "Iteration: 4364 Loss: 2.2523753268900412e-11\n",
      "Iteration: 4365 Loss: 2.25237486511245e-11\n",
      "Iteration: 4366 Loss: 2.252374376180482e-11\n",
      "Iteration: 4367 Loss: 2.2523740945192342e-11\n",
      "Iteration: 4368 Loss: 2.2523735452728694e-11\n",
      "Iteration: 4369 Loss: 2.252373089674664e-11\n",
      "Iteration: 4370 Loss: 2.252372730642331e-11\n",
      "Iteration: 4371 Loss: 2.252372180294929e-11\n",
      "Iteration: 4372 Loss: 2.252371950801636e-11\n",
      "Iteration: 4373 Loss: 2.2523713857528423e-11\n",
      "Iteration: 4374 Loss: 2.2523712776330735e-11\n",
      "Iteration: 4375 Loss: 2.2523708664973944e-11\n",
      "Iteration: 4376 Loss: 2.2523701648494492e-11\n",
      "Iteration: 4377 Loss: 2.2523697620463614e-11\n",
      "Iteration: 4378 Loss: 2.2523695228378945e-11\n",
      "Iteration: 4379 Loss: 2.2523691070192878e-11\n",
      "Iteration: 4380 Loss: 2.2523687063229905e-11\n",
      "Iteration: 4381 Loss: 2.2523680059579214e-11\n",
      "Iteration: 4382 Loss: 2.252367573329108e-11\n",
      "Iteration: 4383 Loss: 2.2523673102287514e-11\n",
      "Iteration: 4384 Loss: 2.252366946627073e-11\n",
      "Iteration: 4385 Loss: 2.2523665360303767e-11\n",
      "Iteration: 4386 Loss: 2.2523661427297904e-11\n",
      "Iteration: 4387 Loss: 2.2523657404012034e-11\n",
      "Iteration: 4388 Loss: 2.2523552171228748e-11\n",
      "Iteration: 4389 Loss: 2.2523546325752303e-11\n",
      "Iteration: 4390 Loss: 2.2523542299898576e-11\n",
      "Iteration: 4391 Loss: 2.252354132297454e-11\n",
      "Iteration: 4392 Loss: 2.2523533039271082e-11\n",
      "Iteration: 4393 Loss: 2.2523533494987665e-11\n",
      "Iteration: 4394 Loss: 2.252352903374325e-11\n",
      "Iteration: 4395 Loss: 2.2523525191741516e-11\n",
      "Iteration: 4396 Loss: 2.2523519891895068e-11\n",
      "Iteration: 4397 Loss: 2.2523520085633666e-11\n",
      "Iteration: 4398 Loss: 2.2523513066598627e-11\n",
      "Iteration: 4399 Loss: 2.25235115284761e-11\n",
      "Iteration: 4400 Loss: 2.2523610141528183e-11\n",
      "Iteration: 4401 Loss: 2.252360617054801e-11\n",
      "Iteration: 4402 Loss: 2.252360524795133e-11\n",
      "Iteration: 4403 Loss: 2.25236022717581e-11\n",
      "Iteration: 4404 Loss: 2.252349578223649e-11\n",
      "Iteration: 4405 Loss: 2.2523491892552847e-11\n",
      "Iteration: 4406 Loss: 2.252349063250383e-11\n",
      "Iteration: 4407 Loss: 2.252347727164474e-11\n",
      "Iteration: 4408 Loss: 2.2523471835401105e-11\n",
      "Iteration: 4409 Loss: 2.25233682484417e-11\n",
      "Iteration: 4410 Loss: 2.2523365513290532e-11\n",
      "Iteration: 4411 Loss: 2.2523361474978347e-11\n",
      "Iteration: 4412 Loss: 2.2523357231714327e-11\n",
      "Iteration: 4413 Loss: 2.2523455400964633e-11\n",
      "Iteration: 4414 Loss: 2.2523454884942254e-11\n",
      "Iteration: 4415 Loss: 2.252355310841934e-11\n",
      "Iteration: 4416 Loss: 2.252355210675311e-11\n",
      "Iteration: 4417 Loss: 2.2523548241125207e-11\n",
      "Iteration: 4418 Loss: 2.252354548356682e-11\n",
      "Iteration: 4419 Loss: 2.2523541560541833e-11\n",
      "Iteration: 4420 Loss: 2.2523540046677962e-11\n",
      "Iteration: 4421 Loss: 2.2523433643665784e-11\n",
      "Iteration: 4422 Loss: 2.252353225042455e-11\n",
      "Iteration: 4423 Loss: 2.2523530967538902e-11\n",
      "Iteration: 4424 Loss: 2.2523526889434222e-11\n",
      "Iteration: 4425 Loss: 2.2523522404367935e-11\n",
      "Iteration: 4426 Loss: 2.2523520123159067e-11\n",
      "Iteration: 4427 Loss: 2.252351643902822e-11\n",
      "Iteration: 4428 Loss: 2.2523514833892175e-11\n",
      "Iteration: 4429 Loss: 2.252351077676656e-11\n",
      "Iteration: 4430 Loss: 2.2523506842241492e-11\n",
      "Iteration: 4431 Loss: 2.2523507053359178e-11\n",
      "Iteration: 4432 Loss: 2.2523501231756338e-11\n",
      "Iteration: 4433 Loss: 2.2523498977342092e-11\n",
      "Iteration: 4434 Loss: 2.2523497344501993e-11\n",
      "Iteration: 4435 Loss: 2.2523494615220522e-11\n",
      "Iteration: 4436 Loss: 2.2523489653953555e-11\n",
      "Iteration: 4437 Loss: 2.2523489646545658e-11\n",
      "Iteration: 4438 Loss: 2.2523486001097403e-11\n",
      "Iteration: 4439 Loss: 2.2523481784169654e-11\n",
      "Iteration: 4440 Loss: 2.2523478000180227e-11\n",
      "Iteration: 4441 Loss: 2.2523480349005594e-11\n",
      "Iteration: 4442 Loss: 2.252347398094385e-11\n",
      "Iteration: 4443 Loss: 2.2523472339440422e-11\n",
      "Iteration: 4444 Loss: 2.252346870310984e-11\n",
      "Iteration: 4445 Loss: 2.2523467226348253e-11\n",
      "Iteration: 4446 Loss: 2.2523459217256057e-11\n",
      "Iteration: 4447 Loss: 2.2523360352873735e-11\n",
      "Iteration: 4448 Loss: 2.252335849177635e-11\n",
      "Iteration: 4449 Loss: 2.2523356937394053e-11\n",
      "Iteration: 4450 Loss: 2.2523352806437126e-11\n",
      "Iteration: 4451 Loss: 2.252335021091355e-11\n",
      "Iteration: 4452 Loss: 2.252334875707657e-11\n",
      "Iteration: 4453 Loss: 2.2523450374311872e-11\n",
      "Iteration: 4454 Loss: 2.2523442118479234e-11\n",
      "Iteration: 4455 Loss: 2.252344098098096e-11\n",
      "Iteration: 4456 Loss: 2.2523437706411484e-11\n",
      "Iteration: 4457 Loss: 2.252343708719671e-11\n",
      "Iteration: 4458 Loss: 2.2523439899978783e-11\n",
      "Iteration: 4459 Loss: 2.2523438618920055e-11\n",
      "Iteration: 4460 Loss: 2.2523435630518793e-11\n",
      "Iteration: 4461 Loss: 2.2523429117369047e-11\n",
      "Iteration: 4462 Loss: 2.2523431849173103e-11\n",
      "Iteration: 4463 Loss: 2.2523326449468578e-11\n",
      "Iteration: 4464 Loss: 2.252332354197579e-11\n",
      "Iteration: 4465 Loss: 2.252331834052804e-11\n",
      "Iteration: 4466 Loss: 2.252331438999546e-11\n",
      "Iteration: 4467 Loss: 2.2523314414254817e-11\n",
      "Iteration: 4468 Loss: 2.252331297018588e-11\n",
      "Iteration: 4469 Loss: 2.2523206807785702e-11\n",
      "Iteration: 4470 Loss: 2.2523209248748472e-11\n",
      "Iteration: 4471 Loss: 2.2523203207369282e-11\n",
      "Iteration: 4472 Loss: 2.252319998882952e-11\n",
      "Iteration: 4473 Loss: 2.2523197293653167e-11\n",
      "Iteration: 4474 Loss: 2.252319741206284e-11\n",
      "Iteration: 4475 Loss: 2.252320025845358e-11\n",
      "Iteration: 4476 Loss: 2.2523182628608864e-11\n",
      "Iteration: 4477 Loss: 2.252319125059707e-11\n",
      "Iteration: 4478 Loss: 2.2523189615349037e-11\n",
      "Iteration: 4479 Loss: 2.25231856740352e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4480 Loss: 2.2523178435650576e-11\n",
      "Iteration: 4481 Loss: 2.2523178294776435e-11\n",
      "Iteration: 4482 Loss: 2.2523076608169215e-11\n",
      "Iteration: 4483 Loss: 2.252307031928539e-11\n",
      "Iteration: 4484 Loss: 2.252307448499411e-11\n",
      "Iteration: 4485 Loss: 2.252306620946125e-11\n",
      "Iteration: 4486 Loss: 2.2523070238692735e-11\n",
      "Iteration: 4487 Loss: 2.2523067414751564e-11\n",
      "Iteration: 4488 Loss: 2.2523170612920808e-11\n",
      "Iteration: 4489 Loss: 2.252316662249799e-11\n",
      "Iteration: 4490 Loss: 2.252316019252086e-11\n",
      "Iteration: 4491 Loss: 2.2523162524417793e-11\n",
      "Iteration: 4492 Loss: 2.2523256030174243e-11\n",
      "Iteration: 4493 Loss: 2.2523260678215254e-11\n",
      "Iteration: 4494 Loss: 2.2523259912040562e-11\n",
      "Iteration: 4495 Loss: 2.2523250825103774e-11\n",
      "Iteration: 4496 Loss: 2.252325598279695e-11\n",
      "Iteration: 4497 Loss: 2.2523250809974844e-11\n",
      "Iteration: 4498 Loss: 2.2523252095198747e-11\n",
      "Iteration: 4499 Loss: 2.2523145866128308e-11\n",
      "Iteration: 4500 Loss: 2.2523241123455647e-11\n",
      "Iteration: 4501 Loss: 2.2523241539147512e-11\n",
      "Iteration: 4502 Loss: 2.2523244211643416e-11\n",
      "Iteration: 4503 Loss: 2.2523137077700757e-11\n",
      "Iteration: 4504 Loss: 2.2523134804550673e-11\n",
      "Iteration: 4505 Loss: 2.2523137899324483e-11\n",
      "Iteration: 4506 Loss: 2.2523135296474983e-11\n",
      "Iteration: 4507 Loss: 2.2523229628771545e-11\n",
      "Iteration: 4508 Loss: 2.2523230063641154e-11\n",
      "Iteration: 4509 Loss: 2.2523235155034798e-11\n",
      "Iteration: 4510 Loss: 2.2523226202117955e-11\n",
      "Iteration: 4511 Loss: 2.252322991222055e-11\n",
      "Iteration: 4512 Loss: 2.2523224918804608e-11\n",
      "Iteration: 4513 Loss: 2.2523227343079164e-11\n",
      "Iteration: 4514 Loss: 2.2523226057851915e-11\n",
      "Iteration: 4515 Loss: 2.2523218338610697e-11\n",
      "Iteration: 4516 Loss: 2.2523221826876574e-11\n",
      "Iteration: 4517 Loss: 2.2523216213093473e-11\n",
      "Iteration: 4518 Loss: 2.252322228061557e-11\n",
      "Iteration: 4519 Loss: 2.252321295032521e-11\n",
      "Iteration: 4520 Loss: 2.2523216672195224e-11\n",
      "Iteration: 4521 Loss: 2.252321722581454e-11\n",
      "Iteration: 4522 Loss: 2.2523217028279592e-11\n",
      "Iteration: 4523 Loss: 2.2523209891337608e-11\n",
      "Iteration: 4524 Loss: 2.2523210389651003e-11\n",
      "Iteration: 4525 Loss: 2.2523210296223583e-11\n",
      "Iteration: 4526 Loss: 2.2523208956089952e-11\n",
      "Iteration: 4527 Loss: 2.2523209249425755e-11\n",
      "Iteration: 4528 Loss: 2.2523206620239688e-11\n",
      "Iteration: 4529 Loss: 2.252310294417157e-11\n",
      "Iteration: 4530 Loss: 2.2523097046863253e-11\n",
      "Iteration: 4531 Loss: 2.25231005084582e-11\n",
      "Iteration: 4532 Loss: 2.252310049617913e-11\n",
      "Iteration: 4533 Loss: 2.2523098751969686e-11\n",
      "Iteration: 4534 Loss: 2.2523198902476797e-11\n",
      "Iteration: 4535 Loss: 2.2523094985255428e-11\n",
      "Iteration: 4536 Loss: 2.2523094723022123e-11\n",
      "Iteration: 4537 Loss: 2.2522990942973785e-11\n",
      "Iteration: 4538 Loss: 2.252309339119212e-11\n",
      "Iteration: 4539 Loss: 2.2523094068065096e-11\n",
      "Iteration: 4540 Loss: 2.2523188329573192e-11\n",
      "Iteration: 4541 Loss: 2.2523189627305613e-11\n",
      "Iteration: 4542 Loss: 2.2523189745570863e-11\n",
      "Iteration: 4543 Loss: 2.2523189745570863e-11\n",
      "Iteration: 4544 Loss: 2.252318452492995e-11\n",
      "Iteration: 4545 Loss: 2.2523188442216767e-11\n",
      "Iteration: 4546 Loss: 2.252318582605069e-11\n",
      "Iteration: 4547 Loss: 2.2523185634874575e-11\n",
      "Iteration: 4548 Loss: 2.252318457241565e-11\n",
      "Iteration: 4549 Loss: 2.252318465030392e-11\n",
      "Iteration: 4550 Loss: 2.252318205717423e-11\n",
      "Iteration: 4551 Loss: 2.2523176429238432e-11\n",
      "Iteration: 4552 Loss: 2.2523175674716486e-11\n",
      "Iteration: 4553 Loss: 2.252317538081657e-11\n",
      "Iteration: 4554 Loss: 2.2523280607061735e-11\n",
      "Iteration: 4555 Loss: 2.2523274230692572e-11\n",
      "Iteration: 4556 Loss: 2.2523274457541575e-11\n",
      "Iteration: 4557 Loss: 2.2523279843564834e-11\n",
      "Iteration: 4558 Loss: 2.2523273246225266e-11\n",
      "Iteration: 4559 Loss: 2.2523273167664892e-11\n",
      "Iteration: 4560 Loss: 2.252327036375389e-11\n",
      "Iteration: 4561 Loss: 2.2523269094724927e-11\n",
      "Iteration: 4562 Loss: 2.2523269231627426e-11\n",
      "Iteration: 4563 Loss: 2.2523269296071842e-11\n",
      "Iteration: 4564 Loss: 2.2523266644840956e-11\n",
      "Iteration: 4565 Loss: 2.2523265406350845e-11\n",
      "Iteration: 4566 Loss: 2.2523265268421115e-11\n",
      "Iteration: 4567 Loss: 2.252315999944859e-11\n",
      "Iteration: 4568 Loss: 2.2523158981991094e-11\n",
      "Iteration: 4569 Loss: 2.252326685692221e-11\n",
      "Iteration: 4570 Loss: 2.2523261623929565e-11\n",
      "Iteration: 4571 Loss: 2.252325867942793e-11\n",
      "Iteration: 4572 Loss: 2.2523257442167205e-11\n",
      "Iteration: 4573 Loss: 2.2523257016839985e-11\n",
      "Iteration: 4574 Loss: 2.252325746519961e-11\n",
      "Iteration: 4575 Loss: 2.2523254858585505e-11\n",
      "Iteration: 4576 Loss: 2.252326040352876e-11\n",
      "Iteration: 4577 Loss: 2.2523253040812627e-11\n",
      "Iteration: 4578 Loss: 2.2523252350096924e-11\n",
      "Iteration: 4579 Loss: 2.2523354528945852e-11\n",
      "Iteration: 4580 Loss: 2.2523355135853098e-11\n",
      "Iteration: 4581 Loss: 2.2523351801192668e-11\n",
      "Iteration: 4582 Loss: 2.2523350453340065e-11\n",
      "Iteration: 4583 Loss: 2.2523351102846328e-11\n",
      "Iteration: 4584 Loss: 2.25233485965572e-11\n",
      "Iteration: 4585 Loss: 2.2523347330224815e-11\n",
      "Iteration: 4586 Loss: 2.252334703247741e-11\n",
      "Iteration: 4587 Loss: 2.252324216341949e-11\n",
      "Iteration: 4588 Loss: 2.252324225383448e-11\n",
      "Iteration: 4589 Loss: 2.252324203022274e-11\n",
      "Iteration: 4590 Loss: 2.2523240982976597e-11\n",
      "Iteration: 4591 Loss: 2.2523237698599262e-11\n",
      "Iteration: 4592 Loss: 2.252323696851453e-11\n",
      "Iteration: 4593 Loss: 2.2523236308984245e-11\n",
      "Iteration: 4594 Loss: 2.2523134313306034e-11\n",
      "Iteration: 4595 Loss: 2.2523134280288422e-11\n",
      "Iteration: 4596 Loss: 2.252312623333517e-11\n",
      "Iteration: 4597 Loss: 2.2523130860788567e-11\n",
      "Iteration: 4598 Loss: 2.2523129020899297e-11\n",
      "Iteration: 4599 Loss: 2.2523129082009174e-11\n",
      "Iteration: 4600 Loss: 2.2523134387190782e-11\n",
      "Iteration: 4601 Loss: 2.252312127372781e-11\n",
      "Iteration: 4602 Loss: 2.252312151574929e-11\n",
      "Iteration: 4603 Loss: 2.2523120490146177e-11\n",
      "Iteration: 4604 Loss: 2.2523117389765973e-11\n",
      "Iteration: 4605 Loss: 2.2523220128892806e-11\n",
      "Iteration: 4606 Loss: 2.2523224552453956e-11\n",
      "Iteration: 4607 Loss: 2.2523219051310886e-11\n",
      "Iteration: 4608 Loss: 2.2523216165285952e-11\n",
      "Iteration: 4609 Loss: 2.252332402980566e-11\n",
      "Iteration: 4610 Loss: 2.2523317622397795e-11\n",
      "Iteration: 4611 Loss: 2.252331492662648e-11\n",
      "Iteration: 4612 Loss: 2.252332038423601e-11\n",
      "Iteration: 4613 Loss: 2.2523217983118953e-11\n",
      "Iteration: 4614 Loss: 2.2523216411024627e-11\n",
      "Iteration: 4615 Loss: 2.2523216636718802e-11\n",
      "Iteration: 4616 Loss: 2.2523214063279547e-11\n",
      "Iteration: 4617 Loss: 2.252320718054777e-11\n",
      "Iteration: 4618 Loss: 2.2523211370857402e-11\n",
      "Iteration: 4619 Loss: 2.2523211766419278e-11\n",
      "Iteration: 4620 Loss: 2.2523211485884304e-11\n",
      "Iteration: 4621 Loss: 2.2523208847320742e-11\n",
      "Iteration: 4622 Loss: 2.252310091588933e-11\n",
      "Iteration: 4623 Loss: 2.2523099835398732e-11\n",
      "Iteration: 4624 Loss: 2.2523099899176357e-11\n",
      "Iteration: 4625 Loss: 2.2523102633181343e-11\n",
      "Iteration: 4626 Loss: 2.252310016955803e-11\n",
      "Iteration: 4627 Loss: 2.2523097307619692e-11\n",
      "Iteration: 4628 Loss: 2.2523198382109878e-11\n",
      "Iteration: 4629 Loss: 2.2523201265977887e-11\n",
      "Iteration: 4630 Loss: 2.2523200869104452e-11\n",
      "Iteration: 4631 Loss: 2.2523199564266207e-11\n",
      "Iteration: 4632 Loss: 2.25232002019356e-11\n",
      "Iteration: 4633 Loss: 2.2523300064151678e-11\n",
      "Iteration: 4634 Loss: 2.2523302275716278e-11\n",
      "Iteration: 4635 Loss: 2.252330024341068e-11\n",
      "Iteration: 4636 Loss: 2.2523298961689115e-11\n",
      "Iteration: 4637 Loss: 2.252329343543916e-11\n",
      "Iteration: 4638 Loss: 2.25230886126476e-11\n",
      "Iteration: 4639 Loss: 2.2523085433397386e-11\n",
      "Iteration: 4640 Loss: 2.252308541093692e-11\n",
      "Iteration: 4641 Loss: 2.252308937335742e-11\n",
      "Iteration: 4642 Loss: 2.252319166293214e-11\n",
      "Iteration: 4643 Loss: 2.252308971540305e-11\n",
      "Iteration: 4644 Loss: 2.2523088870243793e-11\n",
      "Iteration: 4645 Loss: 2.252308971098751e-11\n",
      "Iteration: 4646 Loss: 2.2523080387613682e-11\n",
      "Iteration: 4647 Loss: 2.252308071643327e-11\n",
      "Iteration: 4648 Loss: 2.2523079184400673e-11\n",
      "Iteration: 4649 Loss: 2.252308454459499e-11\n",
      "Iteration: 4650 Loss: 2.252308496931711e-11\n",
      "Iteration: 4651 Loss: 2.252308217753601e-11\n",
      "Iteration: 4652 Loss: 2.2523084274769318e-11\n",
      "Iteration: 4653 Loss: 2.252308482051183e-11\n",
      "Iteration: 4654 Loss: 2.2523076901321892e-11\n",
      "Iteration: 4655 Loss: 2.2523075580241193e-11\n",
      "Iteration: 4656 Loss: 2.2523075520744855e-11\n",
      "Iteration: 4657 Loss: 2.2523075807283482e-11\n",
      "Iteration: 4658 Loss: 2.2523073146625464e-11\n",
      "Iteration: 4659 Loss: 2.2523071479496764e-11\n",
      "Iteration: 4660 Loss: 2.252307794281337e-11\n",
      "Iteration: 4661 Loss: 2.252307737088548e-11\n",
      "Iteration: 4662 Loss: 2.2523073280927855e-11\n",
      "Iteration: 4663 Loss: 2.25230661258236e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4664 Loss: 2.2523071844698846e-11\n",
      "Iteration: 4665 Loss: 2.2523071890393822e-11\n",
      "Iteration: 4666 Loss: 2.2523071921336458e-11\n",
      "Iteration: 4667 Loss: 2.2523074528833737e-11\n",
      "Iteration: 4668 Loss: 2.2523073221355526e-11\n",
      "Iteration: 4669 Loss: 2.2523074631120106e-11\n",
      "Iteration: 4670 Loss: 2.252307467289158e-11\n",
      "Iteration: 4671 Loss: 2.2523067759841364e-11\n",
      "Iteration: 4672 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4673 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4674 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4675 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4676 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4677 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4678 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4679 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4680 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4681 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4682 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4683 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4684 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4685 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4686 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4687 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4688 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4689 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4690 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4691 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4692 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4693 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4694 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4695 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4696 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4697 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4698 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4699 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4700 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4701 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4702 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4703 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4704 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4705 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4706 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4707 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4708 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4709 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4710 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4711 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4712 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4713 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4714 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4715 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4716 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4717 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4718 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4719 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4720 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4721 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4722 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4723 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4724 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4725 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4726 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4727 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4728 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4729 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4730 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4731 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4732 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4733 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4734 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4735 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4736 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4737 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4738 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4739 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4740 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4741 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4742 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4743 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4744 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4745 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4746 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4747 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4748 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4749 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4750 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4751 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4752 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4753 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4754 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4755 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4756 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4757 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4758 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4759 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4760 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4761 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4762 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4763 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4764 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4765 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4766 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4767 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4768 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4769 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4770 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4771 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4772 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4773 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4774 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4775 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4776 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4777 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4778 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4779 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4780 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4781 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4782 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4783 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4784 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4785 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4786 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4787 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4788 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4789 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4790 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4791 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4792 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4793 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4794 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4795 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4796 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4797 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4798 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4799 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4800 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4801 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4802 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4803 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4804 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4805 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4806 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4807 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4808 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4809 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4810 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4811 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4812 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4813 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4814 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4815 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4816 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4817 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4818 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4819 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4820 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4821 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4822 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4823 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4824 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4825 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4826 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4827 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4828 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4829 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4830 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4831 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4832 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4833 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4834 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4835 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4836 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4837 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4838 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4839 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4840 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4841 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4842 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4843 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4844 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4845 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4846 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4847 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4848 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4849 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4850 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4851 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4852 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4853 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4854 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4855 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4856 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4857 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4858 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4859 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4860 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4861 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4862 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4863 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4864 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4865 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4866 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4867 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4868 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4869 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4870 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4871 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4872 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4873 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4874 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4875 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4876 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4877 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4878 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4879 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4880 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4881 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4882 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4883 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4884 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4885 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4886 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4887 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4888 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4889 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4890 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4891 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4892 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4893 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4894 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4895 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4896 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4897 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4898 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4899 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4900 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4901 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4902 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4903 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4904 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4905 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4906 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4907 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4908 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4909 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4910 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4911 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4912 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4913 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4914 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4915 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4916 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4917 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4918 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4919 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4920 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4921 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4922 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4923 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4924 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4925 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4926 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4927 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4928 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4929 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4930 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4931 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4932 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4933 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4934 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4935 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4936 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4937 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4938 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4939 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4940 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4941 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4942 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4943 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4944 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4945 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4946 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4947 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4948 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4949 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4950 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4951 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4952 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4953 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4954 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4955 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4956 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4957 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4958 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4959 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4960 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4961 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4962 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4963 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4964 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4965 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4966 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4967 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4968 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4969 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4970 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4971 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4972 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4973 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4974 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4975 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4976 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4977 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4978 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4979 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4980 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4981 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4982 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4983 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4984 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4985 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4986 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4987 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4988 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4989 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4990 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4991 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4992 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4993 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4994 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4995 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4996 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4997 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4998 Loss: 2.2523072727373513e-11\n",
      "Iteration: 4999 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5000 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5001 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5002 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5003 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5004 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5005 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5006 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5007 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5008 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5009 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5010 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5011 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5012 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5013 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5014 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5015 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5016 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5017 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5018 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5019 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5020 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5021 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5022 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5023 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5024 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5025 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5026 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5027 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5028 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5029 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5030 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5031 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5032 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5033 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5034 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5035 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5036 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5037 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5038 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5039 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5040 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5041 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5042 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5043 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5044 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5045 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5046 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5047 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5048 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5049 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5050 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5051 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5052 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5053 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5054 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5055 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5056 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5057 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5058 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5059 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5060 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5061 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5062 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5063 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5064 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5065 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5066 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5067 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5068 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5069 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5070 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5071 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5072 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5073 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5074 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5075 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5076 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5077 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5078 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5079 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5080 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5081 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5082 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5083 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5084 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5085 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5086 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5087 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5088 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5089 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5090 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5091 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5092 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5093 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5094 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5095 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5096 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5097 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5098 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5099 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5100 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5101 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5102 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5103 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5104 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5105 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5106 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5107 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5108 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5109 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5110 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5111 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5112 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5113 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5114 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5115 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5116 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5117 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5118 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5119 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5120 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5121 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5122 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5123 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5124 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5125 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5126 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5127 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5128 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5129 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5130 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5131 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5132 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5133 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5134 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5135 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5136 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5137 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5138 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5139 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5140 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5141 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5142 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5143 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5144 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5145 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5146 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5147 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5148 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5149 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5150 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5151 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5152 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5153 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5154 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5155 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5156 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5157 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5158 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5159 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5160 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5161 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5162 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5163 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5164 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5165 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5166 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5167 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5168 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5169 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5170 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5171 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5172 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5173 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5174 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5175 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5176 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5177 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5178 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5179 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5180 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5181 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5182 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5183 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5184 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5185 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5186 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5187 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5188 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5189 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5190 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5191 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5192 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5193 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5194 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5195 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5196 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5197 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5198 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5199 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5200 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5201 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5202 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5203 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5204 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5205 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5206 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5207 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5208 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5209 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5210 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5211 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5212 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5213 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5214 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5215 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5216 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5217 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5218 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5219 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5220 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5221 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5222 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5223 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5224 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5225 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5226 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5227 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5228 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5229 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5230 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5231 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5232 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5233 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5234 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5235 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5236 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5237 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5238 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5239 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5240 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5241 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5242 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5243 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5244 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5245 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5246 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5247 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5248 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5249 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5250 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5251 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5252 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5253 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5254 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5255 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5256 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5257 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5258 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5259 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5260 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5261 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5262 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5263 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5264 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5265 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5266 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5267 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5268 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5269 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5270 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5271 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5272 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5273 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5274 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5275 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5276 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5277 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5278 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5279 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5280 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5281 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5282 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5283 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5284 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5285 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5286 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5287 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5288 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5289 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5290 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5291 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5292 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5293 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5294 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5295 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5296 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5297 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5298 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5299 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5300 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5301 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5302 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5303 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5304 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5305 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5306 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5307 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5308 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5309 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5310 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5311 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5312 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5313 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5314 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5315 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5316 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5317 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5318 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5319 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5320 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5321 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5322 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5323 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5324 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5325 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5326 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5327 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5328 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5329 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5330 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5331 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5332 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5333 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5334 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5335 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5336 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5337 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5338 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5339 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5340 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5341 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5342 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5343 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5344 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5345 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5346 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5347 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5348 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5349 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5350 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5351 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5352 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5353 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5354 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5355 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5356 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5357 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5358 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5359 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5360 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5361 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5362 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5363 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5364 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5365 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5366 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5367 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5368 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5369 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5370 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5371 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5372 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5373 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5374 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5375 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5376 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5377 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5378 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5379 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5380 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5381 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5382 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5383 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5384 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5385 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5386 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5387 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5388 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5389 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5390 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5391 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5392 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5393 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5394 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5395 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5396 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5397 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5398 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5399 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5400 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5401 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5402 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5403 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5404 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5405 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5406 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5407 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5408 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5409 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5410 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5411 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5412 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5413 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5414 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5415 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5416 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5417 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5418 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5419 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5420 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5421 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5422 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5423 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5424 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5425 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5426 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5427 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5428 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5429 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5430 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5431 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5432 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5433 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5434 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5435 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5436 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5437 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5438 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5439 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5440 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5441 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5442 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5443 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5444 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5445 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5446 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5447 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5448 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5449 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5450 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5451 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5452 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5453 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5454 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5455 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5456 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5457 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5458 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5459 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5460 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5461 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5462 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5463 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5464 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5465 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5466 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5467 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5468 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5469 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5470 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5471 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5472 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5473 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5474 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5475 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5476 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5477 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5478 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5479 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5480 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5481 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5482 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5483 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5484 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5485 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5486 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5487 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5488 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5489 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5490 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5491 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5492 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5493 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5494 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5495 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5496 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5497 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5498 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5499 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5500 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5501 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5502 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5503 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5504 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5505 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5506 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5507 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5508 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5509 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5510 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5511 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5512 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5513 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5514 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5515 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5516 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5517 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5518 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5519 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5520 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5521 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5522 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5523 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5524 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5525 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5526 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5527 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5528 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5529 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5530 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5531 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5532 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5533 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5534 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5535 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5536 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5537 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5538 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5539 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5540 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5541 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5542 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5543 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5544 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5545 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5546 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5547 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5548 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5549 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5550 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5551 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5552 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5553 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5554 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5555 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5556 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5557 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5558 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5559 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5560 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5561 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5562 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5563 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5564 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5565 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5566 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5567 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5568 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5569 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5570 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5571 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5572 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5573 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5574 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5575 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5576 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5577 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5578 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5579 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5580 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5581 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5582 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5583 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5584 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5585 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5586 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5587 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5588 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5589 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5590 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5591 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5592 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5593 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5594 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5595 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5596 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5597 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5598 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5599 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5600 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5601 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5602 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5603 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5604 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5605 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5606 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5607 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5608 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5609 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5610 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5611 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5612 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5613 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5614 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5615 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5616 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5617 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5618 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5619 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5620 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5621 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5622 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5623 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5624 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5625 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5626 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5627 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5628 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5629 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5630 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5631 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5632 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5633 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5634 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5635 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5636 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5637 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5638 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5639 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5640 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5641 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5642 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5643 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5644 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5645 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5646 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5647 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5648 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5649 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5650 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5651 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5652 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5653 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5654 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5655 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5656 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5657 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5658 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5659 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5660 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5661 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5662 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5663 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5664 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5665 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5666 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5667 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5668 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5669 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5670 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5671 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5672 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5673 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5674 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5675 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5676 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5677 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5678 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5679 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5680 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5681 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5682 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5683 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5684 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5685 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5686 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5687 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5688 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5689 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5690 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5691 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5692 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5693 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5694 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5695 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5696 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5697 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5698 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5699 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5700 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5701 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5702 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5703 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5704 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5705 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5706 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5707 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5708 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5709 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5710 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5711 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5712 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5713 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5714 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5715 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5716 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5717 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5718 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5719 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5720 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5721 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5722 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5723 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5724 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5725 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5726 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5727 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5728 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5729 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5730 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5731 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5732 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5733 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5734 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5735 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5736 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5737 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5738 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5739 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5740 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5741 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5742 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5743 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5744 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5745 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5746 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5747 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5748 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5749 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5750 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5751 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5752 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5753 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5754 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5755 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5756 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5757 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5758 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5759 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5760 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5761 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5762 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5763 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5764 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5765 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5766 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5767 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5768 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5769 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5770 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5771 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5772 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5773 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5774 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5775 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5776 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5777 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5778 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5779 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5780 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5781 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5782 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5783 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5784 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5785 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5786 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5787 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5788 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5789 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5790 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5791 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5792 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5793 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5794 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5795 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5796 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5797 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5798 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5799 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5800 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5801 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5802 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5803 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5804 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5805 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5806 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5807 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5808 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5809 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5810 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5811 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5812 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5813 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5814 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5815 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5816 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5817 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5818 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5819 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5820 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5821 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5822 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5823 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5824 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5825 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5826 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5827 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5828 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5829 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5830 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5831 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5832 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5833 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5834 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5835 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5836 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5837 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5838 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5839 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5840 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5841 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5842 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5843 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5844 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5845 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5846 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5847 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5848 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5849 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5850 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5851 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5852 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5853 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5854 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5855 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5856 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5857 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5858 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5859 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5860 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5861 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5862 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5863 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5864 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5865 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5866 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5867 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5868 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5869 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5870 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5871 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5872 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5873 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5874 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5875 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5876 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5877 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5878 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5879 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5880 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5881 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5882 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5883 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5884 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5885 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5886 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5887 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5888 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5889 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5890 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5891 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5892 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5893 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5894 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5895 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5896 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5897 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5898 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5899 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5900 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5901 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5902 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5903 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5904 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5905 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5906 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5907 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5908 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5909 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5910 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5911 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5912 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5913 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5914 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5915 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5916 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5917 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5918 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5919 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5920 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5921 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5922 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5923 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5924 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5925 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5926 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5927 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5928 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5929 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5930 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5931 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5932 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5933 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5934 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5935 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5936 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5937 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5938 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5939 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5940 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5941 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5942 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5943 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5944 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5945 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5946 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5947 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5948 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5949 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5950 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5951 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5952 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5953 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5954 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5955 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5956 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5957 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5958 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5959 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5960 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5961 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5962 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5963 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5964 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5965 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5966 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5967 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5968 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5969 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5970 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5971 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5972 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5973 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5974 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5975 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5976 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5977 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5978 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5979 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5980 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5981 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5982 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5983 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5984 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5985 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5986 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5987 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5988 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5989 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5990 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5991 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5992 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5993 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5994 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5995 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5996 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5997 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5998 Loss: 2.2523072727373513e-11\n",
      "Iteration: 5999 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6000 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6001 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6002 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6003 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6004 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6005 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6006 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6007 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6008 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6009 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6010 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6011 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6012 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6013 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6014 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6015 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6016 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6017 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6018 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6019 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6020 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6021 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6022 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6023 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6024 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6025 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6026 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6027 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6028 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6029 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6030 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6031 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6032 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6033 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6034 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6035 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6036 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6037 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6038 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6039 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6040 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6041 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6042 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6043 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6044 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6045 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6046 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6047 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6048 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6049 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6050 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6051 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6052 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6053 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6054 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6055 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6056 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6057 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6058 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6059 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6060 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6061 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6062 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6063 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6064 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6065 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6066 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6067 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6068 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6069 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6070 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6071 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6072 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6073 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6074 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6075 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6076 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6077 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6078 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6079 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6080 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6081 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6082 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6083 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6084 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6085 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6086 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6087 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6088 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6089 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6090 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6091 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6092 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6093 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6094 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6095 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6096 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6097 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6098 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6099 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6100 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6101 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6102 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6103 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6104 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6105 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6106 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6107 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6108 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6109 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6110 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6111 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6112 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6113 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6114 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6115 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6116 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6117 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6118 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6119 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6120 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6121 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6122 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6123 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6124 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6125 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6126 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6127 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6128 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6129 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6130 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6131 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6132 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6133 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6134 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6135 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6136 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6137 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6138 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6139 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6140 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6141 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6142 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6143 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6144 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6145 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6146 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6147 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6148 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6149 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6150 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6151 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6152 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6153 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6154 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6155 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6156 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6157 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6158 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6159 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6160 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6161 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6162 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6163 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6164 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6165 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6166 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6167 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6168 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6169 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6170 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6171 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6172 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6173 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6174 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6175 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6176 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6177 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6178 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6179 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6180 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6181 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6182 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6183 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6184 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6185 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6186 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6187 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6188 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6189 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6190 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6191 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6192 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6193 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6194 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6195 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6196 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6197 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6198 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6199 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6200 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6201 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6202 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6203 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6204 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6205 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6206 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6207 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6208 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6209 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6210 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6211 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6212 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6213 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6214 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6215 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6216 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6217 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6218 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6219 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6220 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6221 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6222 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6223 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6224 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6225 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6226 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6227 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6228 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6229 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6230 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6231 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6232 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6233 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6234 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6235 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6236 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6237 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6238 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6239 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6240 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6241 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6242 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6243 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6244 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6245 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6246 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6247 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6248 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6249 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6250 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6251 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6252 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6253 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6254 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6255 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6256 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6257 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6258 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6259 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6260 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6261 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6262 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6263 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6264 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6265 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6266 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6267 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6268 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6269 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6270 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6271 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6272 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6273 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6274 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6275 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6276 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6277 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6278 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6279 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6280 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6281 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6282 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6283 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6284 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6285 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6286 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6287 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6288 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6289 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6290 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6291 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6292 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6293 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6294 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6295 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6296 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6297 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6298 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6299 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6300 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6301 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6302 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6303 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6304 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6305 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6306 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6307 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6308 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6309 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6310 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6311 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6312 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6313 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6314 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6315 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6316 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6317 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6318 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6319 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6320 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6321 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6322 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6323 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6324 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6325 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6326 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6327 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6328 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6329 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6330 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6331 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6332 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6333 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6334 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6335 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6336 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6337 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6338 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6339 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6340 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6341 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6342 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6343 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6344 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6345 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6346 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6347 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6348 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6349 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6350 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6351 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6352 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6353 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6354 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6355 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6356 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6357 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6358 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6359 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6360 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6361 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6362 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6363 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6364 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6365 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6366 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6367 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6368 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6369 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6370 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6371 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6372 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6373 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6374 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6375 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6376 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6377 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6378 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6379 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6380 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6381 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6382 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6383 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6384 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6385 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6386 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6387 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6388 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6389 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6390 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6391 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6392 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6393 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6394 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6395 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6396 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6397 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6398 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6399 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6400 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6401 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6402 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6403 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6404 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6405 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6406 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6407 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6408 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6409 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6410 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6411 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6412 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6413 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6414 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6415 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6416 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6417 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6418 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6419 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6420 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6421 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6422 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6423 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6424 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6425 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6426 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6427 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6428 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6429 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6430 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6431 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6432 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6433 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6434 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6435 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6436 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6437 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6438 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6439 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6440 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6441 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6442 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6443 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6444 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6445 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6446 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6447 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6448 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6449 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6450 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6451 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6452 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6453 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6454 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6455 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6456 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6457 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6458 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6459 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6460 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6461 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6462 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6463 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6464 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6465 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6466 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6467 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6468 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6469 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6470 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6471 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6472 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6473 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6474 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6475 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6476 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6477 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6478 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6479 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6480 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6481 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6482 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6483 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6484 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6485 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6486 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6487 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6488 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6489 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6490 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6491 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6492 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6493 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6494 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6495 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6496 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6497 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6498 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6499 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6500 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6501 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6502 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6503 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6504 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6505 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6506 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6507 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6508 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6509 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6510 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6511 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6512 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6513 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6514 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6515 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6516 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6517 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6518 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6519 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6520 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6521 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6522 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6523 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6524 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6525 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6526 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6527 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6528 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6529 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6530 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6531 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6532 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6533 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6534 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6535 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6536 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6537 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6538 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6539 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6540 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6541 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6542 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6543 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6544 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6545 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6546 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6547 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6548 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6549 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6550 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6551 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6552 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6553 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6554 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6555 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6556 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6557 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6558 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6559 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6560 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6561 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6562 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6563 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6564 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6565 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6566 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6567 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6568 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6569 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6570 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6571 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6572 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6573 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6574 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6575 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6576 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6577 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6578 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6579 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6580 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6581 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6582 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6583 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6584 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6585 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6586 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6587 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6588 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6589 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6590 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6591 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6592 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6593 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6594 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6595 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6596 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6597 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6598 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6599 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6600 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6601 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6602 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6603 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6604 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6605 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6606 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6607 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6608 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6609 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6610 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6611 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6612 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6613 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6614 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6615 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6616 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6617 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6618 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6619 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6620 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6621 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6622 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6623 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6624 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6625 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6626 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6627 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6628 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6629 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6630 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6631 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6632 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6633 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6634 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6635 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6636 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6637 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6638 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6639 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6640 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6641 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6642 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6643 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6644 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6645 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6646 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6647 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6648 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6649 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6650 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6651 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6652 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6653 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6654 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6655 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6656 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6657 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6658 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6659 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6660 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6661 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6662 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6663 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6664 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6665 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6666 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6667 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6668 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6669 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6670 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6671 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6672 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6673 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6674 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6675 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6676 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6677 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6678 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6679 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6680 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6681 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6682 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6683 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6684 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6685 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6686 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6687 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6688 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6689 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6690 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6691 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6692 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6693 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6694 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6695 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6696 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6697 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6698 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6699 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6700 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6701 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6702 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6703 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6704 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6705 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6706 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6707 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6708 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6709 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6710 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6711 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6712 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6713 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6714 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6715 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6716 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6717 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6718 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6719 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6720 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6721 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6722 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6723 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6724 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6725 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6726 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6727 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6728 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6729 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6730 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6731 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6732 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6733 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6734 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6735 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6736 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6737 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6738 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6739 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6740 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6741 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6742 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6743 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6744 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6745 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6746 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6747 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6748 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6749 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6750 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6751 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6752 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6753 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6754 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6755 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6756 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6757 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6758 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6759 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6760 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6761 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6762 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6763 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6764 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6765 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6766 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6767 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6768 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6769 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6770 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6771 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6772 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6773 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6774 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6775 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6776 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6777 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6778 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6779 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6780 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6781 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6782 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6783 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6784 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6785 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6786 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6787 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6788 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6789 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6790 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6791 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6792 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6793 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6794 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6795 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6796 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6797 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6798 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6799 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6800 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6801 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6802 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6803 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6804 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6805 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6806 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6807 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6808 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6809 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6810 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6811 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6812 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6813 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6814 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6815 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6816 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6817 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6818 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6819 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6820 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6821 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6822 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6823 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6824 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6825 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6826 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6827 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6828 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6829 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6830 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6831 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6832 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6833 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6834 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6835 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6836 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6837 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6838 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6839 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6840 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6841 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6842 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6843 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6844 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6845 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6846 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6847 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6848 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6849 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6850 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6851 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6852 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6853 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6854 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6855 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6856 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6857 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6858 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6859 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6860 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6861 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6862 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6863 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6864 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6865 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6866 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6867 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6868 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6869 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6870 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6871 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6872 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6873 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6874 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6875 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6876 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6877 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6878 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6879 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6880 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6881 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6882 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6883 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6884 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6885 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6886 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6887 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6888 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6889 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6890 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6891 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6892 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6893 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6894 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6895 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6896 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6897 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6898 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6899 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6900 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6901 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6902 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6903 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6904 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6905 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6906 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6907 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6908 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6909 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6910 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6911 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6912 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6913 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6914 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6915 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6916 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6917 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6918 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6919 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6920 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6921 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6922 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6923 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6924 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6925 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6926 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6927 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6928 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6929 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6930 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6931 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6932 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6933 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6934 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6935 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6936 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6937 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6938 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6939 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6940 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6941 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6942 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6943 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6944 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6945 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6946 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6947 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6948 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6949 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6950 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6951 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6952 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6953 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6954 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6955 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6956 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6957 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6958 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6959 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6960 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6961 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6962 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6963 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6964 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6965 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6966 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6967 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6968 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6969 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6970 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6971 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6972 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6973 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6974 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6975 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6976 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6977 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6978 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6979 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6980 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6981 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6982 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6983 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6984 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6985 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6986 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6987 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6988 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6989 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6990 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6991 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6992 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6993 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6994 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6995 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6996 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6997 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6998 Loss: 2.2523072727373513e-11\n",
      "Iteration: 6999 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7000 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7001 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7002 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7003 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7004 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7005 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7006 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7007 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7008 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7009 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7010 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7011 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7012 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7013 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7014 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7015 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7016 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7017 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7018 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7019 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7020 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7021 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7022 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7023 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7024 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7025 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7026 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7027 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7028 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7029 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7030 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7031 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7032 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7033 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7034 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7035 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7036 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7037 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7038 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7039 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7040 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7041 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7042 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7043 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7044 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7045 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7046 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7047 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7048 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7049 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7050 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7051 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7052 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7053 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7054 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7055 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7056 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7057 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7058 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7059 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7060 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7061 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7062 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7063 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7064 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7065 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7066 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7067 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7068 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7069 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7070 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7071 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7072 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7073 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7074 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7075 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7076 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7077 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7078 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7079 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7080 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7081 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7082 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7083 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7084 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7085 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7086 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7087 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7088 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7089 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7090 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7091 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7092 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7093 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7094 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7095 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7096 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7097 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7098 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7099 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7100 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7101 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7102 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7103 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7104 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7105 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7106 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7107 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7108 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7109 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7110 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7111 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7112 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7113 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7114 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7115 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7116 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7117 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7118 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7119 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7120 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7121 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7122 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7123 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7124 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7125 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7126 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7127 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7128 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7129 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7130 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7131 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7132 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7133 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7134 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7135 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7136 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7137 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7138 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7139 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7140 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7141 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7142 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7143 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7144 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7145 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7146 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7147 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7148 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7149 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7150 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7151 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7152 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7153 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7154 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7155 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7156 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7157 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7158 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7159 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7160 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7161 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7162 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7163 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7164 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7165 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7166 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7167 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7168 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7169 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7170 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7171 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7172 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7173 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7174 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7175 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7176 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7177 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7178 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7179 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7180 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7181 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7182 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7183 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7184 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7185 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7186 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7187 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7188 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7189 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7190 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7191 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7192 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7193 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7194 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7195 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7196 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7197 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7198 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7199 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7200 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7201 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7202 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7203 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7204 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7205 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7206 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7207 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7208 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7209 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7210 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7211 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7212 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7213 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7214 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7215 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7216 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7217 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7218 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7219 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7220 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7221 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7222 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7223 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7224 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7225 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7226 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7227 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7228 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7229 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7230 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7231 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7232 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7233 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7234 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7235 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7236 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7237 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7238 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7239 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7240 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7241 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7242 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7243 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7244 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7245 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7246 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7247 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7248 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7249 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7250 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7251 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7252 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7253 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7254 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7255 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7256 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7257 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7258 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7259 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7260 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7261 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7262 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7263 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7264 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7265 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7266 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7267 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7268 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7269 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7270 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7271 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7272 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7273 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7274 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7275 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7276 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7277 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7278 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7279 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7280 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7281 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7282 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7283 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7284 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7285 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7286 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7287 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7288 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7289 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7290 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7291 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7292 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7293 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7294 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7295 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7296 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7297 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7298 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7299 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7300 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7301 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7302 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7303 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7304 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7305 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7306 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7307 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7308 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7309 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7310 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7311 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7312 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7313 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7314 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7315 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7316 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7317 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7318 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7319 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7320 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7321 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7322 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7323 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7324 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7325 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7326 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7327 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7328 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7329 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7330 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7331 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7332 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7333 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7334 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7335 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7336 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7337 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7338 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7339 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7340 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7341 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7342 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7343 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7344 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7345 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7346 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7347 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7348 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7349 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7350 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7351 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7352 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7353 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7354 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7355 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7356 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7357 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7358 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7359 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7360 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7361 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7362 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7363 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7364 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7365 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7366 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7367 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7368 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7369 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7370 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7371 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7372 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7373 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7374 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7375 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7376 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7377 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7378 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7379 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7380 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7381 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7382 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7383 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7384 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7385 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7386 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7387 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7388 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7389 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7390 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7391 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7392 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7393 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7394 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7395 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7396 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7397 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7398 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7399 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7400 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7401 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7402 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7403 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7404 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7405 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7406 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7407 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7408 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7409 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7410 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7411 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7412 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7413 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7414 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7415 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7416 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7417 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7418 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7419 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7420 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7421 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7422 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7423 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7424 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7425 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7426 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7427 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7428 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7429 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7430 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7431 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7432 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7433 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7434 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7435 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7436 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7437 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7438 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7439 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7440 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7441 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7442 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7443 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7444 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7445 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7446 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7447 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7448 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7449 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7450 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7451 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7452 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7453 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7454 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7455 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7456 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7457 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7458 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7459 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7460 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7461 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7462 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7463 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7464 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7465 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7466 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7467 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7468 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7469 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7470 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7471 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7472 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7473 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7474 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7475 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7476 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7477 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7478 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7479 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7480 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7481 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7482 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7483 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7484 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7485 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7486 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7487 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7488 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7489 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7490 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7491 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7492 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7493 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7494 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7495 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7496 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7497 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7498 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7499 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7500 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7501 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7502 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7503 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7504 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7505 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7506 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7507 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7508 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7509 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7510 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7511 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7512 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7513 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7514 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7515 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7516 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7517 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7518 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7519 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7520 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7521 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7522 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7523 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7524 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7525 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7526 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7527 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7528 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7529 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7530 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7531 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7532 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7533 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7534 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7535 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7536 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7537 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7538 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7539 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7540 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7541 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7542 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7543 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7544 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7545 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7546 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7547 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7548 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7549 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7550 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7551 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7552 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7553 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7554 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7555 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7556 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7557 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7558 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7559 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7560 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7561 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7562 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7563 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7564 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7565 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7566 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7567 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7568 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7569 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7570 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7571 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7572 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7573 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7574 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7575 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7576 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7577 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7578 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7579 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7580 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7581 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7582 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7583 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7584 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7585 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7586 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7587 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7588 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7589 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7590 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7591 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7592 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7593 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7594 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7595 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7596 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7597 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7598 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7599 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7600 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7601 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7602 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7603 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7604 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7605 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7606 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7607 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7608 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7609 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7610 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7611 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7612 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7613 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7614 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7615 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7616 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7617 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7618 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7619 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7620 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7621 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7622 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7623 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7624 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7625 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7626 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7627 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7628 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7629 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7630 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7631 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7632 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7633 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7634 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7635 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7636 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7637 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7638 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7639 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7640 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7641 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7642 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7643 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7644 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7645 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7646 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7647 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7648 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7649 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7650 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7651 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7652 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7653 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7654 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7655 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7656 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7657 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7658 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7659 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7660 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7661 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7662 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7663 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7664 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7665 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7666 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7667 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7668 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7669 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7670 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7671 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7672 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7673 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7674 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7675 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7676 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7677 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7678 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7679 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7680 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7681 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7682 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7683 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7684 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7685 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7686 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7687 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7688 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7689 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7690 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7691 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7692 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7693 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7694 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7695 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7696 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7697 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7698 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7699 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7700 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7701 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7702 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7703 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7704 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7705 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7706 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7707 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7708 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7709 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7710 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7711 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7712 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7713 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7714 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7715 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7716 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7717 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7718 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7719 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7720 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7721 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7722 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7723 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7724 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7725 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7726 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7727 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7728 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7729 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7730 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7731 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7732 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7733 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7734 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7735 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7736 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7737 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7738 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7739 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7740 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7741 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7742 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7743 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7744 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7745 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7746 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7747 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7748 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7749 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7750 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7751 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7752 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7753 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7754 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7755 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7756 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7757 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7758 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7759 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7760 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7761 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7762 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7763 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7764 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7765 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7766 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7767 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7768 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7769 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7770 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7771 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7772 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7773 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7774 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7775 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7776 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7777 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7778 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7779 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7780 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7781 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7782 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7783 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7784 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7785 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7786 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7787 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7788 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7789 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7790 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7791 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7792 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7793 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7794 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7795 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7796 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7797 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7798 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7799 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7800 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7801 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7802 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7803 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7804 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7805 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7806 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7807 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7808 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7809 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7810 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7811 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7812 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7813 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7814 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7815 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7816 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7817 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7818 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7819 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7820 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7821 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7822 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7823 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7824 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7825 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7826 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7827 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7828 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7829 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7830 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7831 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7832 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7833 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7834 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7835 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7836 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7837 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7838 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7839 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7840 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7841 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7842 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7843 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7844 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7845 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7846 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7847 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7848 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7849 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7850 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7851 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7852 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7853 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7854 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7855 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7856 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7857 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7858 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7859 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7860 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7861 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7862 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7863 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7864 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7865 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7866 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7867 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7868 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7869 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7870 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7871 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7872 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7873 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7874 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7875 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7876 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7877 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7878 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7879 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7880 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7881 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7882 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7883 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7884 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7885 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7886 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7887 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7888 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7889 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7890 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7891 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7892 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7893 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7894 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7895 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7896 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7897 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7898 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7899 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7900 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7901 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7902 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7903 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7904 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7905 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7906 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7907 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7908 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7909 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7910 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7911 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7912 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7913 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7914 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7915 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7916 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7917 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7918 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7919 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7920 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7921 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7922 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7923 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7924 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7925 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7926 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7927 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7928 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7929 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7930 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7931 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7932 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7933 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7934 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7935 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7936 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7937 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7938 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7939 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7940 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7941 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7942 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7943 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7944 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7945 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7946 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7947 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7948 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7949 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7950 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7951 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7952 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7953 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7954 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7955 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7956 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7957 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7958 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7959 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7960 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7961 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7962 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7963 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7964 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7965 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7966 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7967 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7968 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7969 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7970 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7971 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7972 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7973 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7974 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7975 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7976 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7977 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7978 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7979 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7980 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7981 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7982 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7983 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7984 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7985 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7986 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7987 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7988 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7989 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7990 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7991 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7992 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7993 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7994 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7995 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7996 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7997 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7998 Loss: 2.2523072727373513e-11\n",
      "Iteration: 7999 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8000 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8001 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8002 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8003 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8004 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8005 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8006 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8007 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8008 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8009 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8010 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8011 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8012 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8013 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8014 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8015 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8016 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8017 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8018 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8019 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8020 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8021 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8022 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8023 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8024 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8025 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8026 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8027 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8028 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8029 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8030 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8031 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8032 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8033 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8034 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8035 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8036 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8037 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8038 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8039 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8040 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8041 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8042 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8043 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8044 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8045 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8046 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8047 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8048 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8049 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8050 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8051 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8052 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8053 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8054 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8055 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8056 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8057 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8058 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8059 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8060 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8061 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8062 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8063 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8064 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8065 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8066 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8067 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8068 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8069 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8070 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8071 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8072 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8073 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8074 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8075 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8076 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8077 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8078 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8079 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8080 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8081 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8082 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8083 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8084 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8085 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8086 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8087 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8088 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8089 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8090 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8091 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8092 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8093 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8094 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8095 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8096 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8097 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8098 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8099 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8100 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8101 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8102 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8103 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8104 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8105 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8106 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8107 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8108 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8109 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8110 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8111 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8112 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8113 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8114 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8115 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8116 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8117 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8118 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8119 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8120 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8121 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8122 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8123 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8124 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8125 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8126 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8127 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8128 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8129 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8130 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8131 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8132 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8133 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8134 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8135 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8136 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8137 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8138 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8139 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8140 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8141 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8142 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8143 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8144 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8145 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8146 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8147 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8148 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8149 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8150 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8151 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8152 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8153 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8154 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8155 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8156 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8157 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8158 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8159 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8160 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8161 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8162 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8163 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8164 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8165 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8166 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8167 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8168 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8169 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8170 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8171 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8172 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8173 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8174 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8175 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8176 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8177 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8178 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8179 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8180 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8181 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8182 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8183 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8184 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8185 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8186 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8187 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8188 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8189 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8190 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8191 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8192 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8193 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8194 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8195 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8196 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8197 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8198 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8199 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8200 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8201 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8202 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8203 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8204 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8205 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8206 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8207 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8208 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8209 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8210 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8211 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8212 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8213 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8214 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8215 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8216 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8217 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8218 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8219 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8220 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8221 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8222 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8223 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8224 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8225 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8226 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8227 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8228 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8229 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8230 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8231 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8232 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8233 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8234 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8235 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8236 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8237 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8238 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8239 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8240 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8241 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8242 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8243 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8244 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8245 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8246 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8247 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8248 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8249 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8250 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8251 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8252 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8253 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8254 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8255 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8256 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8257 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8258 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8259 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8260 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8261 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8262 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8263 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8264 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8265 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8266 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8267 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8268 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8269 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8270 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8271 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8272 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8273 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8274 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8275 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8276 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8277 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8278 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8279 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8280 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8281 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8282 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8283 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8284 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8285 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8286 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8287 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8288 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8289 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8290 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8291 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8292 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8293 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8294 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8295 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8296 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8297 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8298 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8299 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8300 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8301 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8302 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8303 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8304 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8305 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8306 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8307 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8308 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8309 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8310 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8311 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8312 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8313 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8314 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8315 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8316 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8317 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8318 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8319 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8320 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8321 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8322 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8323 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8324 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8325 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8326 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8327 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8328 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8329 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8330 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8331 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8332 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8333 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8334 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8335 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8336 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8337 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8338 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8339 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8340 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8341 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8342 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8343 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8344 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8345 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8346 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8347 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8348 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8349 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8350 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8351 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8352 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8353 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8354 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8355 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8356 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8357 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8358 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8359 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8360 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8361 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8362 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8363 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8364 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8365 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8366 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8367 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8368 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8369 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8370 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8371 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8372 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8373 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8374 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8375 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8376 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8377 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8378 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8379 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8380 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8381 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8382 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8383 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8384 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8385 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8386 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8387 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8388 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8389 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8390 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8391 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8392 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8393 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8394 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8395 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8396 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8397 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8398 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8399 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8400 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8401 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8402 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8403 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8404 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8405 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8406 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8407 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8408 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8409 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8410 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8411 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8412 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8413 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8414 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8415 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8416 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8417 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8418 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8419 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8420 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8421 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8422 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8423 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8424 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8425 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8426 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8427 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8428 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8429 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8430 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8431 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8432 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8433 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8434 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8435 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8436 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8437 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8438 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8439 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8440 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8441 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8442 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8443 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8444 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8445 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8446 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8447 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8448 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8449 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8450 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8451 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8452 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8453 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8454 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8455 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8456 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8457 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8458 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8459 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8460 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8461 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8462 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8463 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8464 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8465 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8466 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8467 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8468 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8469 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8470 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8471 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8472 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8473 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8474 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8475 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8476 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8477 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8478 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8479 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8480 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8481 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8482 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8483 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8484 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8485 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8486 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8487 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8488 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8489 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8490 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8491 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8492 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8493 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8494 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8495 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8496 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8497 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8498 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8499 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8500 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8501 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8502 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8503 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8504 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8505 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8506 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8507 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8508 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8509 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8510 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8511 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8512 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8513 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8514 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8515 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8516 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8517 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8518 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8519 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8520 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8521 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8522 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8523 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8524 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8525 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8526 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8527 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8528 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8529 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8530 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8531 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8532 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8533 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8534 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8535 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8536 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8537 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8538 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8539 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8540 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8541 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8542 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8543 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8544 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8545 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8546 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8547 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8548 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8549 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8550 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8551 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8552 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8553 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8554 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8555 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8556 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8557 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8558 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8559 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8560 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8561 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8562 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8563 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8564 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8565 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8566 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8567 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8568 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8569 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8570 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8571 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8572 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8573 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8574 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8575 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8576 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8577 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8578 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8579 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8580 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8581 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8582 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8583 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8584 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8585 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8586 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8587 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8588 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8589 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8590 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8591 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8592 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8593 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8594 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8595 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8596 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8597 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8598 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8599 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8600 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8601 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8602 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8603 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8604 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8605 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8606 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8607 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8608 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8609 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8610 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8611 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8612 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8613 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8614 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8615 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8616 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8617 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8618 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8619 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8620 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8621 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8622 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8623 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8624 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8625 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8626 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8627 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8628 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8629 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8630 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8631 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8632 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8633 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8634 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8635 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8636 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8637 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8638 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8639 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8640 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8641 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8642 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8643 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8644 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8645 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8646 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8647 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8648 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8649 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8650 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8651 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8652 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8653 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8654 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8655 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8656 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8657 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8658 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8659 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8660 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8661 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8662 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8663 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8664 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8665 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8666 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8667 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8668 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8669 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8670 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8671 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8672 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8673 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8674 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8675 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8676 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8677 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8678 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8679 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8680 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8681 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8682 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8683 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8684 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8685 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8686 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8687 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8688 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8689 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8690 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8691 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8692 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8693 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8694 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8695 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8696 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8697 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8698 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8699 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8700 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8701 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8702 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8703 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8704 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8705 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8706 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8707 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8708 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8709 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8710 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8711 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8712 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8713 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8714 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8715 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8716 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8717 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8718 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8719 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8720 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8721 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8722 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8723 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8724 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8725 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8726 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8727 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8728 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8729 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8730 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8731 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8732 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8733 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8734 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8735 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8736 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8737 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8738 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8739 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8740 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8741 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8742 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8743 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8744 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8745 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8746 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8747 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8748 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8749 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8750 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8751 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8752 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8753 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8754 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8755 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8756 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8757 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8758 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8759 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8760 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8761 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8762 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8763 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8764 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8765 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8766 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8767 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8768 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8769 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8770 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8771 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8772 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8773 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8774 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8775 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8776 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8777 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8778 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8779 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8780 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8781 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8782 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8783 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8784 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8785 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8786 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8787 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8788 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8789 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8790 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8791 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8792 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8793 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8794 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8795 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8796 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8797 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8798 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8799 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8800 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8801 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8802 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8803 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8804 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8805 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8806 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8807 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8808 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8809 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8810 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8811 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8812 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8813 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8814 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8815 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8816 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8817 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8818 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8819 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8820 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8821 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8822 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8823 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8824 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8825 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8826 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8827 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8828 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8829 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8830 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8831 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8832 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8833 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8834 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8835 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8836 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8837 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8838 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8839 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8840 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8841 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8842 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8843 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8844 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8845 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8846 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8847 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8848 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8849 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8850 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8851 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8852 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8853 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8854 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8855 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8856 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8857 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8858 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8859 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8860 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8861 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8862 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8863 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8864 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8865 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8866 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8867 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8868 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8869 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8870 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8871 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8872 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8873 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8874 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8875 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8876 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8877 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8878 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8879 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8880 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8881 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8882 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8883 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8884 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8885 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8886 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8887 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8888 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8889 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8890 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8891 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8892 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8893 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8894 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8895 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8896 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8897 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8898 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8899 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8900 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8901 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8902 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8903 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8904 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8905 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8906 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8907 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8908 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8909 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8910 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8911 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8912 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8913 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8914 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8915 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8916 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8917 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8918 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8919 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8920 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8921 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8922 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8923 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8924 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8925 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8926 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8927 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8928 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8929 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8930 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8931 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8932 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8933 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8934 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8935 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8936 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8937 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8938 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8939 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8940 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8941 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8942 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8943 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8944 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8945 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8946 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8947 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8948 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8949 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8950 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8951 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8952 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8953 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8954 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8955 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8956 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8957 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8958 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8959 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8960 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8961 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8962 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8963 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8964 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8965 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8966 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8967 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8968 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8969 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8970 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8971 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8972 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8973 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8974 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8975 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8976 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8977 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8978 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8979 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8980 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8981 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8982 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8983 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8984 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8985 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8986 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8987 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8988 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8989 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8990 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8991 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8992 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8993 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8994 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8995 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8996 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8997 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8998 Loss: 2.2523072727373513e-11\n",
      "Iteration: 8999 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9000 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9001 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9002 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9003 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9004 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9005 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9006 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9007 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9008 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9009 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9010 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9011 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9012 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9013 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9014 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9015 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9016 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9017 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9018 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9019 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9020 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9021 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9022 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9023 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9024 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9025 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9026 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9027 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9028 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9029 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9030 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9031 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9032 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9033 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9034 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9035 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9036 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9037 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9038 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9039 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9040 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9041 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9042 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9043 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9044 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9045 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9046 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9047 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9048 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9049 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9050 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9051 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9052 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9053 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9054 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9055 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9056 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9057 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9058 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9059 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9060 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9061 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9062 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9063 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9064 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9065 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9066 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9067 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9068 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9069 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9070 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9071 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9072 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9073 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9074 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9075 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9076 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9077 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9078 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9079 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9080 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9081 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9082 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9083 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9084 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9085 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9086 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9087 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9088 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9089 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9090 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9091 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9092 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9093 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9094 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9095 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9096 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9097 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9098 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9099 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9100 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9101 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9102 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9103 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9104 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9105 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9106 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9107 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9108 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9109 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9110 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9111 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9112 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9113 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9114 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9115 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9116 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9117 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9118 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9119 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9120 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9121 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9122 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9123 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9124 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9125 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9126 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9127 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9128 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9129 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9130 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9131 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9132 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9133 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9134 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9135 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9136 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9137 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9138 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9139 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9140 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9141 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9142 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9143 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9144 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9145 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9146 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9147 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9148 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9149 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9150 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9151 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9152 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9153 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9154 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9155 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9156 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9157 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9158 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9159 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9160 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9161 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9162 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9163 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9164 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9165 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9166 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9167 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9168 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9169 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9170 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9171 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9172 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9173 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9174 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9175 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9176 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9177 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9178 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9179 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9180 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9181 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9182 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9183 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9184 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9185 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9186 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9187 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9188 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9189 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9190 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9191 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9192 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9193 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9194 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9195 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9196 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9197 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9198 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9199 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9200 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9201 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9202 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9203 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9204 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9205 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9206 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9207 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9208 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9209 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9210 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9211 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9212 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9213 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9214 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9215 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9216 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9217 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9218 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9219 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9220 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9221 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9222 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9223 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9224 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9225 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9226 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9227 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9228 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9229 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9230 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9231 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9232 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9233 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9234 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9235 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9236 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9237 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9238 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9239 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9240 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9241 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9242 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9243 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9244 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9245 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9246 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9247 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9248 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9249 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9250 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9251 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9252 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9253 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9254 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9255 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9256 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9257 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9258 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9259 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9260 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9261 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9262 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9263 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9264 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9265 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9266 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9267 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9268 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9269 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9270 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9271 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9272 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9273 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9274 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9275 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9276 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9277 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9278 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9279 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9280 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9281 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9282 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9283 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9284 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9285 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9286 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9287 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9288 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9289 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9290 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9291 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9292 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9293 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9294 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9295 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9296 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9297 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9298 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9299 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9300 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9301 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9302 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9303 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9304 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9305 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9306 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9307 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9308 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9309 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9310 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9311 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9312 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9313 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9314 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9315 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9316 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9317 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9318 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9319 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9320 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9321 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9322 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9323 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9324 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9325 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9326 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9327 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9328 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9329 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9330 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9331 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9332 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9333 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9334 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9335 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9336 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9337 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9338 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9339 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9340 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9341 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9342 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9343 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9344 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9345 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9346 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9347 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9348 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9349 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9350 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9351 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9352 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9353 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9354 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9355 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9356 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9357 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9358 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9359 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9360 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9361 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9362 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9363 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9364 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9365 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9366 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9367 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9368 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9369 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9370 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9371 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9372 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9373 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9374 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9375 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9376 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9377 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9378 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9379 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9380 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9381 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9382 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9383 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9384 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9385 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9386 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9387 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9388 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9389 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9390 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9391 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9392 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9393 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9394 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9395 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9396 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9397 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9398 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9399 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9400 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9401 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9402 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9403 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9404 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9405 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9406 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9407 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9408 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9409 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9410 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9411 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9412 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9413 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9414 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9415 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9416 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9417 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9418 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9419 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9420 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9421 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9422 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9423 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9424 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9425 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9426 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9427 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9428 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9429 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9430 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9431 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9432 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9433 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9434 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9435 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9436 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9437 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9438 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9439 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9440 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9441 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9442 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9443 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9444 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9445 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9446 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9447 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9448 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9449 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9450 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9451 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9452 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9453 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9454 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9455 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9456 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9457 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9458 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9459 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9460 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9461 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9462 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9463 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9464 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9465 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9466 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9467 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9468 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9469 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9470 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9471 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9472 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9473 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9474 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9475 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9476 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9477 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9478 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9479 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9480 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9481 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9482 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9483 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9484 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9485 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9486 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9487 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9488 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9489 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9490 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9491 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9492 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9493 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9494 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9495 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9496 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9497 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9498 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9499 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9500 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9501 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9502 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9503 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9504 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9505 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9506 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9507 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9508 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9509 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9510 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9511 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9512 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9513 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9514 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9515 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9516 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9517 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9518 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9519 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9520 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9521 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9522 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9523 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9524 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9525 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9526 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9527 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9528 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9529 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9530 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9531 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9532 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9533 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9534 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9535 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9536 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9537 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9538 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9539 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9540 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9541 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9542 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9543 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9544 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9545 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9546 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9547 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9548 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9549 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9550 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9551 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9552 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9553 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9554 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9555 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9556 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9557 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9558 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9559 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9560 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9561 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9562 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9563 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9564 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9565 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9566 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9567 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9568 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9569 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9570 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9571 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9572 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9573 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9574 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9575 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9576 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9577 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9578 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9579 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9580 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9581 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9582 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9583 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9584 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9585 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9586 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9587 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9588 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9589 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9590 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9591 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9592 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9593 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9594 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9595 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9596 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9597 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9598 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9599 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9600 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9601 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9602 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9603 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9604 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9605 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9606 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9607 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9608 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9609 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9610 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9611 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9612 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9613 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9614 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9615 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9616 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9617 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9618 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9619 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9620 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9621 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9622 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9623 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9624 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9625 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9626 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9627 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9628 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9629 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9630 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9631 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9632 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9633 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9634 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9635 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9636 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9637 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9638 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9639 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9640 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9641 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9642 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9643 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9644 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9645 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9646 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9647 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9648 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9649 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9650 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9651 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9652 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9653 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9654 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9655 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9656 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9657 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9658 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9659 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9660 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9661 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9662 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9663 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9664 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9665 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9666 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9667 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9668 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9669 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9670 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9671 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9672 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9673 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9674 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9675 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9676 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9677 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9678 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9679 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9680 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9681 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9682 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9683 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9684 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9685 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9686 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9687 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9688 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9689 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9690 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9691 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9692 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9693 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9694 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9695 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9696 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9697 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9698 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9699 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9700 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9701 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9702 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9703 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9704 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9705 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9706 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9707 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9708 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9709 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9710 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9711 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9712 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9713 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9714 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9715 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9716 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9717 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9718 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9719 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9720 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9721 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9722 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9723 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9724 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9725 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9726 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9727 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9728 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9729 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9730 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9731 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9732 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9733 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9734 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9735 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9736 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9737 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9738 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9739 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9740 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9741 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9742 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9743 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9744 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9745 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9746 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9747 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9748 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9749 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9750 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9751 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9752 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9753 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9754 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9755 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9756 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9757 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9758 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9759 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9760 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9761 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9762 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9763 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9764 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9765 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9766 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9767 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9768 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9769 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9770 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9771 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9772 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9773 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9774 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9775 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9776 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9777 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9778 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9779 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9780 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9781 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9782 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9783 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9784 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9785 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9786 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9787 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9788 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9789 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9790 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9791 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9792 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9793 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9794 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9795 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9796 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9797 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9798 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9799 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9800 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9801 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9802 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9803 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9804 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9805 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9806 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9807 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9808 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9809 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9810 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9811 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9812 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9813 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9814 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9815 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9816 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9817 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9818 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9819 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9820 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9821 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9822 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9823 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9824 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9825 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9826 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9827 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9828 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9829 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9830 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9831 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9832 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9833 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9834 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9835 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9836 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9837 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9838 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9839 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9840 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9841 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9842 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9843 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9844 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9845 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9846 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9847 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9848 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9849 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9850 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9851 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9852 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9853 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9854 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9855 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9856 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9857 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9858 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9859 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9860 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9861 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9862 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9863 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9864 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9865 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9866 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9867 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9868 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9869 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9870 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9871 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9872 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9873 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9874 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9875 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9876 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9877 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9878 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9879 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9880 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9881 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9882 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9883 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9884 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9885 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9886 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9887 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9888 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9889 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9890 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9891 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9892 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9893 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9894 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9895 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9896 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9897 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9898 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9899 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9900 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9901 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9902 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9903 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9904 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9905 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9906 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9907 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9908 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9909 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9910 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9911 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9912 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9913 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9914 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9915 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9916 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9917 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9918 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9919 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9920 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9921 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9922 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9923 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9924 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9925 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9926 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9927 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9928 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9929 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9930 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9931 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9932 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9933 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9934 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9935 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9936 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9937 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9938 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9939 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9940 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9941 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9942 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9943 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9944 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9945 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9946 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9947 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9948 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9949 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9950 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9951 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9952 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9953 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9954 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9955 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9956 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9957 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9958 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9959 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9960 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9961 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9962 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9963 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9964 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9965 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9966 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9967 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9968 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9969 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9970 Loss: 2.2523072727373513e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9971 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9972 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9973 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9974 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9975 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9976 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9977 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9978 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9979 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9980 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9981 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9982 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9983 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9984 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9985 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9986 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9987 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9988 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9989 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9990 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9991 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9992 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9993 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9994 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9995 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9996 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9997 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9998 Loss: 2.2523072727373513e-11\n",
      "Iteration: 9999 Loss: 2.2523072727373513e-11\n",
      "Iteration: 10000 Loss: 2.2523072727373513e-11\n",
      "r: 1\n"
     ]
    }
   ],
   "source": [
    "adm = time.time()\n",
    "ADMM_list, dual_ADMM_list, iterations_ADMM = admm.ADMM(N, M, (Q1,B1), (Q2,B2), (Q3,B3), Sigma, D, e1, e2, e31, e32, (x1,x2,x3), 1)\n",
    "fin = time.time()\n",
    "\n",
    "Times[\"ADMM\"] = fin - adm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33f4c36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 10000\n",
      "Primal: (x1,x2,x3),(y1,y2,y3),(z1,z2,z3)\n",
      " ((array([[1188.3, 1188.3, 1188.3, 1188.3, 1188.3],\n",
      "       [44.3, 44.3, 44.3, 44.3, 44.3],\n",
      "       [25.7, 25.7, 25.7, 25.7, 25.7]]), array([[186.6, 704.0, 833.3, 1188.3, 714.3],\n",
      "       [8.9, 30.9, 44.3, 42.8, 43.5],\n",
      "       [4.6, 15.1, 25.6, 19.0, 25.7]]), array([[-0.0, -0.0, 96.7, -0.0, 716.5]])), (array([[1188.3, 1188.3, 1188.3, 1188.3, 1188.3],\n",
      "       [44.3, 44.3, 44.3, 44.3, 44.3],\n",
      "       [25.7, 25.7, 25.7, 25.7, 25.7]]), array([[186.6, 704.0, 833.3, 1188.3, 714.3],\n",
      "       [8.9, 30.9, 44.3, 42.8, 43.5],\n",
      "       [4.6, 15.1, 25.6, 19.0, 25.7]]), array([[0.0, 0.0, 96.7, 0.0, 716.5]])), (array([[-5.0, -5.0, -5.0, 20.0, -5.0],\n",
      "       [-50.0, -50.0, 200.0, -50.0, -50.0],\n",
      "       [-95.0, -95.0, -95.0, -95.0, 380.0]]), array([[-1865.7, -6336.1, -10000.0, -8342.8, -10000.0],\n",
      "       [-1865.7, -6336.1, -10000.0, -8342.8, -10000.0],\n",
      "       [-1865.7, -6336.1, -10000.0, -8342.8, -10000.0]]), array([[-10000.0, -10000.0, -10000.0, -10000.0, -10000.0]])), 'infactible')\n",
      "Dual: (Equilibrium)\n",
      " [[1865.7 6336.1 10000.0 8342.8 10000.0]]\n"
     ]
    }
   ],
   "source": [
    "iter_ = -1\n",
    "print(\"Iterations:\",iterations_ADMM)\n",
    "print(\"Primal: (x1,x2,x3),(y1,y2,y3),(z1,z2,z3)\\n\",ADMM_list[iter_])\n",
    "print(\"Dual: (Equilibrium)\\n\",dual_ADMM_list[iter_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1d7890b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_iters_list = max([iterations_DY,iterations_BA,iterations_ADMM])\n",
    "max_iters_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8240e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_adjusted(x_sol, x_teo, sigma):\n",
    "    x_sol_1, x_sol_2, x_sol_3 = x_sol\n",
    "    x_teo_1, x_teo_2, x_teo_3 = x_teo\n",
    "    sigma=sigma[0]\n",
    "    \n",
    "    return LA.norm(x_sol_1-x_teo_1) + sum([sigma[xi]*LA.norm(x_sol_2[:,xi]-x_teo_2[:,xi]) for xi in range(M)]) + sum([sigma[xi]*LA.norm(x_sol_3[:,xi]-x_teo_3[:,xi]) for xi in range(M)])\n",
    "\n",
    "\n",
    "def norm_adjusted_N(x_sol, x_teo, sigma):\n",
    "    x_sol_1, x_sol_2, x_sol_3 = x_sol\n",
    "    x_teo_1, x_teo_2, x_teo_3 = x_teo\n",
    "    sigma=sigma[0]\n",
    "    \n",
    "    return sum([sigma[xi]*LA.norm(x_sol_1[:,xi][:,np.newaxis]-x_teo_1) for xi in range(M)]) + sum([sigma[xi]*LA.norm(x_sol_2[:,xi]-x_teo_2[:,xi]) for xi in range(M)]) + sum([sigma[xi]*LA.norm(x_sol_3[:,xi]-x_teo_3[:,xi]) for xi in range(M)])\n",
    "\n",
    "\n",
    "\n",
    "def generate_list(lista, lista_2, algoritmo, solution, objective_function, Demanda, max_iterations, P):\n",
    "\n",
    "    # unpack solution\n",
    "    x1, x2, x3 = solution\n",
    "\n",
    "    # create a list with index of graphics\n",
    "    iterations = list(range(max_iterations))\n",
    "\n",
    "    # create list to return\n",
    "    x_solution     = []\n",
    "    Fx_solution    = []\n",
    "    Non_anti_sol   = []\n",
    "    equili_solut   = []\n",
    "    capacity_solut = []\n",
    "    demand_solu    = []\n",
    "    dual_solut     = []\n",
    "    \n",
    "    zero_1 = np.zeros((N,1))\n",
    "    zero_1_N = np.zeros((N,M))\n",
    "    zero_2 = np.zeros((N,M))\n",
    "    zero_3 = np.zeros((1,M))\n",
    "    zeroo = (zero_1, zero_2, zero_3)\n",
    "    zeroo_N = (zero_1_N, zero_2, zero_3)\n",
    "    \n",
    "    # create arrays for each graph\n",
    "\n",
    "    if algoritmo == \"DY\":\n",
    "        for x_algo, x_fact in lista:\n",
    "            x1_algo, x2_algo, x3_algo = x_algo\n",
    "            x_solution.append( norm_adjusted(x_algo, (x1,x2,x3), P)/norm_adjusted((x1,x2,x3), zeroo, P) )\n",
    "            Fx_solution.append( abs(objective_function(x1_algo, x2_algo, x3_algo)[0][0] - objective_function(x1, x2, x3)[0][0] )/abs(objective_function(x1, x2, x3)[0][0]) )\n",
    "            Non_anti_sol.append( LA.norm(x1_algo - np.roll(x1_algo, 1, axis=1)) ) #No aplica a DY\n",
    "            equili_solut.append( norm_adjusted((zero_1,zero_2,x2_algo.sum(axis=0) - (D - x3_algo)), zeroo, P) )\n",
    "            capacity_solut.append( norm_adjusted((zero_1,x1_algo - x2_algo,zero_3), zeroo, P) )\n",
    "            demand_solu.append(norm_adjusted((zero_1,zero_2,D - x3_algo), zeroo, P))\n",
    "\n",
    "        for lambda1, lambda2 in lista_2:\n",
    "            dual_solut.append(norm_adjusted((zero_1,zero_2,rho - lambda2),zeroo,P)/norm_adjusted((zero_1,zero_2,rho),zeroo,P))\n",
    "    \n",
    "\n",
    "    elif algoritmo == \"ADMM\":\n",
    "        for elemento in lista:\n",
    "            x1_algo, x2_algo, x3_algo = elemento[0]\n",
    "            x_solution.append( norm_adjusted_N((x1_algo, x2_algo, x3_algo), (x1,x2,x3), P)/norm_adjusted((x1,x2,x3), zeroo, P) )\n",
    "            Fx_solution.append( abs(objective_function(x1_algo, x2_algo, x3_algo)[0][0] - objective_function(x1, x2, x3)[0][0] )/abs(objective_function(x1, x2, x3)[0][0]) )\n",
    "            Non_anti_sol.append( norm_adjusted_N((x1_algo - np.roll(x1_algo, 1, axis=1),zero_2, zero_3), zeroo, P) ) #No aplica a DY\n",
    "            equili_solut.append( norm_adjusted_N((zero_1_N,zero_2,x2_algo.sum(axis=0) - (D - x3_algo)), zeroo, P) )\n",
    "            capacity_solut.append( norm_adjusted_N((zero_1_N,x1_algo - x2_algo,zero_3), zeroo, P) )\n",
    "            demand_solu.append(norm_adjusted_N((zero_1_N,zero_2,D - x3_algo), zeroo, P))\n",
    "\n",
    "        for lambda2 in lista_2:\n",
    "            dual_solut.append(norm_adjusted_N((zero_1_N,zero_2,rho - lambda2),zeroo,P)/norm_adjusted((zero_1,zero_2,rho),zeroo,P))\n",
    "\n",
    "\n",
    "    elif algoritmo == \"BA\":\n",
    "        for x_algo, x_fact in lista:\n",
    "            x1_algo, x2_algo, x3_algo = x_algo\n",
    "            x_solution.append( norm_adjusted_N((x1_algo, x2_algo, x3_algo), (x1,x2,x3), P)/norm_adjusted((x1,x2,x3), zeroo, P) )\n",
    "            Fx_solution.append( abs(objective_function(x1_algo, x2_algo, x3_algo)[0][0] - objective_function(x1, x2, x3)[0][0] )/abs(objective_function(x1, x2, x3)[0][0]) )\n",
    "            Non_anti_sol.append( norm_adjusted_N((x1_algo - np.roll(x1_algo, 1, axis=1),zero_2, zero_3), zeroo, P) ) #No aplica a DY\n",
    "            equili_solut.append( norm_adjusted_N((zero_1_N,zero_2,x2_algo.sum(axis=0) - (D - x3_algo)), zeroo, P) )\n",
    "            capacity_solut.append( norm_adjusted_N((zero_1_N,x1_algo - x2_algo,zero_3), zeroo, P) )\n",
    "            demand_solu.append(norm_adjusted_N((zero_1_N,zero_2,D - x3_algo), zeroo, P))\n",
    "            \n",
    "        \n",
    "        for lambda1, lambda2 in lista_2:\n",
    "            dual_solut.append(norm_adjusted_N((zero_1_N,zero_2,rho - lambda2),zeroo,P)/norm_adjusted((zero_1,zero_2,rho),zeroo,P))\n",
    "            \n",
    "    \n",
    "    print(\"Completando listas\")\n",
    "    \n",
    "    x_solution     = x_solution     + [None]*(max_iterations-len(lista))\n",
    "    Fx_solution    = Fx_solution    + [None]*(max_iterations-len(lista))\n",
    "    Non_anti_sol   = Non_anti_sol   + [None]*(max_iterations-len(lista))\n",
    "    equili_solut   = equili_solut   + [None]*(max_iterations-len(lista))\n",
    "    capacity_solut = capacity_solut + [None]*(max_iterations-len(lista))\n",
    "    demand_solu    = demand_solu    + [None]*(max_iterations-len(lista))\n",
    "    dual_solut     = dual_solut     + [None]*(max_iterations-len(lista))\n",
    "\n",
    "    k = 0\n",
    "\n",
    "    return iterations[k:], x_solution[k:], Fx_solution[k:], Non_anti_sol[k:], equili_solut[k:], capacity_solut[k:], demand_solu[k:], dual_solut[k:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9094918c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DY\n",
      "Completando listas\n",
      "ADMM\n",
      "Completando listas\n",
      "BA\n",
      "Completando listas\n"
     ]
    }
   ],
   "source": [
    "print(\"DY\")\n",
    "iter_DY, x_DY_sol, Fx_DY_sol, Non_anti_DY, equili_DY_solu, capacity_DY_solu, demand_DY_sol, dual_DY_sol = generate_list(DY_list, Dual_DY_list, \"DY\", (x1, x2, x3), objective_function, D, max_iters_list, Sigma)\n",
    "print(\"ADMM\")\n",
    "iter_ADMM, x_ADMM_sol, Fx_ADMM_sol, Non_anti_ADMM, equili_ADMM_solu, capacity_ADMM_solu, demand_ADMM_sol, dual_ADMM_sol  = generate_list(ADMM_list, dual_ADMM_list, \"ADMM\", (x1, x2, x3), objective_function, D, max_iters_list,Sigma)\n",
    "print(\"BA\")\n",
    "iter_BA, x_BA_sol, Fx_BA_sol, Non_anti_BA, equili_BA_solu, capacity_BA_solu, demand_BA_sol, BA_dual_sol = generate_list(x_BA_list, dual_BA_list, \"BA\", (x1, x2, x3), objective_function, D, max_iters_list, Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56958ac3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1IAAAE6CAYAAAAcHmMZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABIS0lEQVR4nO3de3ycdZ3+/2tyaNo0TaZTaEshhd7hpLhAJ0k5eQA6AdZd0KWTVllRl7UZEfXnYskQ/LqKorMJddfDqiRFUUS0mcED6rJLpiBntM0goBzN3UKRQ0snk/RAU9rM74+7M80kM0kmp7mTeT0fj3Fm7sPc70xvbK5+To54PB4XAAAAAGDUCnJdAAAAAABMNwQpAAAAAMgSQQoAAAAAskSQAgAAAIAsEaQAAAAAIEsEKQAAAADIEkEKAAAAALJEkAIAAACALBGkAACYQnV1dYpEIrkuAwAwTgQpAACmiN/vl9PplNvtznUpAIBxIkgBADAFEq1QwWAwx5UAACaCIx6Px3NdBAAAM51pmjIMI9dlAAAmCEEKAPKIaZpqbW1VS0uLDMOQz+eTJO3atUuSVFVVpYaGhlyWaGuRSER+v1+maaqrq2tU58RiMQUCAdXW1kqSotGoJPE9A8A0R5ACgDxUV1cnwzDU2tqast3n8ykajaZ0P0sEh9F2SWtra5vykJBtjeMRDofl8/lGFaQSwSsYDMrpdCa3h0Ihtba2qqOjY0w1pPuOp/I7AAAwRgoAMEBra6tisZja2tqS2+rq6rRmzZpRf8ZYw8F4ZFvjeLhcrlEfu3LlyuQEEwN5vd6UFsFspfuOp/I7AABIRbkuAABgL/X19fL7/ckWD4/HM+pz29raZJrmZJWWUTY1ThW/3y/DMDLW5vf7VVVVlTxutDJ9x3b8DgBgJiNIAQBSrF69Wj6fLznLXLoxQW1tbTIMQ7FYTKZpyul0yjAMdXR0yDRNtbS0SJIaGxslKdnKlTjG5/MlpwAPh8Py+/2SpA0bNsg0TZmmqV27dqm5uTmltoEtZZI1zijTuKXhrjmcsZ43WCgUGva8RHgKhUJqbGxMfg8ul0v19fXJWgZ+D+FwOO13nO47SPe9RqNRdXZ2qrW1VW1tbXK5XNq4caOampqG1JoYRydZY+sSf5YAgMPiAIC84/F44g0NDRn3S4q3trbG4/F4vLOzM24YRnJfMBhM7ovH4/Gurq7k+46Ojrjb7R7yeY2NjfGurq7ke8Mw4t3d3cn3HR0dccMw4h0dHSnHdHZ2Jt83NzfHGxsbU+oIBoNpaxzNNTMZ6bx010pHUry5uXnYYwzDiHu93uT7YDAYl5Ry/cbGxpQ/q0zfcbq6Mn2vg7/HwZ/n9XpTzunq6op7PJ5hfxYAyDeMkQIAZC0YDCoWi0myWlZqamqGPd40TYXD4eR7wzBS3rtcLpmmmdI9zTCMZBe2WCwmv9+vpqam5P6NGzcO241wpGtO9HnpJGZDHK3EYr0Du/o1NTWNuctkpu91ILfbnfLZkUhE4XB4yDnRaHTM3wMAzER07QMApBgYkNLxer1qbW3V/Pnz5Xa7tWbNmhG7fSVmkkt0BYxGo8lpwBMGX8/pdCaP2bJli5xOZ8qkDSPNTjeaa07keYMNDIKZmKY54oQTiZ87EomMaR2qdN9rVVVVxuO3bNmS9jqJro6MxQIACy1SAIAUW7ZskaRhW5k6OjrU2dmpNWvWJNelSicRJCKRiOrr69Xe3i7DMLIOBIlwl42xXnO8tSZ4PJ7kOLNM10kcNx4TPbnHWL5rAMhHBCkAQIrW1lY1NzcPmbI7ITHhg9vtVmNjozo7O7Vx48a0x0YiEcViMa1cuVJNTU1qaGiQ0+lM/rI+2hDgdrvT/oKf6Zf+sV5zImpNaG5uVjQaVSgUSrs/MTPiSBNZxGIxxWKxjMcNF9bGwuPxpP1ZTdNMLioMACBIAQAGaGlpUSwWG7ar3uB1pqQj3ccGdmczTTM5/mZwEEh0lRsuBAwMSYZhyOv1prR8xWIxtbe3pz13rNcc63npOJ1OBYNBBQKBIcEk8f0NnpUwcZ2BP3sgEFBDQ8Ow33E2Rmpxcrvd8ng8KeOhEj+71+vN6loAMJM54vF4PNdFAACmhmmaya54AxeE3bVrl2KxmKqqqlJCVCQSUSAQUCgUUnNzsxobG5MhILEwrWmaydYbSckpt6uqqpJrUSW21dXVSbLCgN/v15o1a2QYxpBrtLS0KBAIyDAMNTU1JX+B9/v9WrBgQXLyg8T054PPH+mawwWC0dba2NiYNggNFovFFAgEhoxLSnw3AyWmLG9qakqOi5I0JNgO/o7TfQfptiW+15qamuRU6wN/nsR1E9dI1NzV1TWqnxUA8glBCgAAm0gEqc7OzlyXAgAYAV37AAAAACBLBCkAAAAAyBJBCgAAGwiHw2publYkEsk4nTwAwD5sPUYqEolo7dq1I/YVN01ToVAoOZPRwEHPAAAAADDRbBukEsGourpaI5VYXV2dDFumacrv94+44j0AAAAAjJVtg1SCw+EYNkiZpqn6+vqUVqv58+eru7t7KsoDAAAAkIeKcl3AeIXD4eRaJgkul0uRSCTjIoV9fX3q6+tLvu/v71c0GtWCBQvkcDgmtV4AAAAA9hWPx7V7924tWbJEBQWZp5SY9kEq0wrtiZXo0wkEArrxxhsnqSIAAAAA09327dt13HHHZdw/7YNUJpkCliQ1NTXp2muvTb7v6enR0qVLtX37dpWXl09BdZntd1Vo9iHpnfqzbr2nUueem9NyAAAAgLzS29uryspKzZs3b9jjpn2QcjqdQ1qfotHosLP2lZSUqKSkZMj28vLynAepYoc0R1KhyjRnTrlyXA4AAACQl0Ya8jPt15HyeDxpt9fU1ExxJROj//Cfl0Nx2XsaEAAAACB/TYsgNbibXiQSkWmakiTDMFL2maapmpqaabuOVPxwkCpQv/r7c1sLAAAAgPRsG6TC4bD8fr8ka3KIUCiU3Df4fTAYlN/vVygUUmtr67ReQyrRCEWLFAAAAGBftl9Hair09vaqoqJCPT09OR8j1TvbofI+6US9qO/fe6Lq6nJaDgAAACZRf3+/Dhw4kOsy8kpxcbEKCwsz7h9tNpj2k03MNHTtAwAAyA8HDhzQ1q1b1c8vfVPO6XRq8eLF41pDliBlM3E5Dv8vXfsAAABmqng8rtdee02FhYWqrKwcduFXTJx4PK59+/Zpx44dkqRjjjlmzJ9FkLKZ+IBZ+/jHCQAAgJnp4MGD2rdvn5YsWaLS0tJcl5NX5syZI0nasWOHFi5cOGw3v+EQfW0mzvTnAAAAM96hQ4ckSbNmzcpxJfkpEV7ffvvtMX8GQcpm+hkjBQAAkDfGM0YHYzcR3ztBymYGTn9OkAIAAADsiTFSdjOga9/hFl8AAADAVlpaWuR0OuVyuWSapgzDkNfrTe6PRCJqbW1VW1ubGhsbVVVVpa6uLpmmKZ/PJ4/HI0kyTVOhUEhOp1OSZBiGTNNUQ0PDqPbnEkHKZhKz9hWoXwcP5roaAAAAIFV1dbU2bNggt9ud3Ob3+7V582Y1NzdLktxut5qbm9XW1qampqZkEIrFYpo/f746OzvldrtVX1+vzs7O5Oe0tLRo165dyfcj7c8lgpTN9B/ubEmLFAAAQP6Ix+Pa9/a+nFy7tLh01GOG/H6/DMNICVGS1NzcrPnz52vNmjVD9g3kdDplGIY2btyYDFcDNTY2qqWlRZLVGjXc/lwjSNnMwDFStEgBAADkh31v71NZoCwn197TtEdzZ80d1bEtLS1qbW1Nu8/j8SgQCCgYDA77GdFoVFVVVcluem1tbSld9RKvR9qfa0w2YTeMkQIAAIANJVqIampq0u43DEORSCTj+bFYTH6/Xx6PJxmGNmzYIJ/PJ4fDobq6OoXD4ZSWqpH25xItUjYTP9ysyhgpAACA/FFaXKo9TXtydu1sRKPRrI5va2uTYRiSJJ/Pl3wtSV6vV11dXQqHw+ro6FBdXZ2CwWBy4oqR9ucSQcpm+mmRAgAAyDsOh2PU3etyJRGA0o1dkqyZ+tKNj2poaEjbihSLxZJjphoaGtTQ0KC2tjYFAgF5vd4R9+caXftsijFSAAAAsJvGxsaMY6C2bNkin8836s8yTXNIV8DVq1crFouNan+uEaRsZmDXPlqkAAAAYCfNzc2KRqMKh8Mp230+n1avXp1cH2qg4boC+v3+lPfhcDiltWmk/blE1z6biQ/o2keLFAAAAOyms7NTfr9fpmkmF+Stq6sbsiDvxo0bJVnhy+fzpe32V19fn1zcV5K6urqSa1GNZn8uOeLxeHzkw2a23t5eVVRUqKenR+Xl5Tmt5aWji3X8mwd1rh7RP7Wcq+uuy2k5AAAAmAT79+/X1q1btWzZMs2ePTvX5eSd4b7/0WYDuvbZTKJrHy1SAAAAgH0RpGzmwKG3JTFGCgAAALAzgpTNMEYKAAAAsD+ClM2wjhQAAABgfwQpm0nM/FGoQ7RIAQAAADZFkLKZ+JwSSdJs7adFCgAAALApgpTNzKpYIEmaq720SAEAAAA2RZCymYOl1jz2ZdpDkAIAAABsiiBlM4dK50iyghRd+wAAAAB7IkjZTP+8MklShXpokQIAAIBtRSIR+f3+tNt9Pp8cDof8fr/a2trU0tIin8+nUCiU8di2tra016mvr9f8+fPV0tIy5nMmgyMej8dHPmxm6+3tVUVFhXp6elReXp7TWiIfv0TuH/+fvqXP6ul//ZZuvTWn5QAAAGAS7N+/X1u3btWyZcs0e/bsXJczJj6fT+3t7eru7h6yLxaLaf78+eru7pbT6Uxur6+vV21trRobG1OOXbt2rUzTVGdn55DP8fv9Mk1THR0d4zpnoOG+/9FmA1qkbKb/KGuyiaO1kxYpAACAPBGPS3v35uYx1mYVp9OpWCymcDg86nM2bNggv9+vWCyWsn3NmjUyTVOmaaZs37Jli6qrq9N+1ljOmUgEKZspWnSMJGmhdjBGCgAAIE/s2yeVleXmsW9f9vWGw2GtWbNGHo9HwWBw1Oc5nU653e4hXfKcTqdWr149pOvfSJ+V7TkTiSBlM2XHGZJokQIAAIB9RSIRud3uZPe+bBiGoc2bNw/Z7vP51NramnKNmpqaYT9rLOdMFIKUzbiOP0WS1SLVd6A/x9UAAABgKpSWSnv25OZRWjr2ur1eb9bd+yQN6donSW63W5IVhiQpGo2mjK9KZyznTJSiKbkKRs1ZeZIk6Si9qT1735I0N7cFAQAAYNI5HNLcafJrXzgcVldXV7J7nmEYCgaD8ng8ozrfNM2Mx3q9XrW2tqa0Mo1kLOdMBIKUzRQsWqx+ScU6KEfP65Kqcl0SAAAAkBSJRFJCi8vl0tq1a0cdZEzTlM/nS7vP5/Opurpa9fX1ow5mYzlnItC1z25mzdKbpdY/R8zr2ZbbWgAAAIARZNO9z+fzqaGhQYZhpGxPdPUzDEOGYWSctny850wkW7dImaapUCgkwzBkmqYaGhoy9nk0TVPhcFgul0umacrr9Q75A5ou3ix3auG+vZq/55VclwIAAABIsrr0NTc3KxqNyuPxJMcntbW1yel0yu/3y+fzqaamJtk6FQgEVFVVpVgspq6uLtXV1cnr9SY/MxKJKBAIJKcw93q98vl8yd/jQ6GQgsGgtmzZora2NjU0NIzpnMlg6wV5q6urkwtsmaYpv9+fcXrFlpaWlIW9Bs/gMRw7LcgrSY+98wyd8+xTuv6or+k/dt6Q63IAAAAwwWbCgrzT2YxekHfwwlqGYQzbXLhx48bJLmnKvHX0YknS0X2v5bgSAAAAAOnYNkgluukN5HK5klMbDuZyuVRdXZ3s4ldXV5fxs/v6+tTb25vysJP4kuMlSYsPEKQAAAAAO7JtkEo3t7xkzQ2fTqLLX1VVlYLBYErfy8ECgYAqKiqSj8rKynHXO5HmHH+yJGnJwVdzXAkAAACAdGwbpDLJFLASg99aW1vV1taWcUpFSWpqalJPT0/ysX379kmqdmycp5wpSVrW/4p69vfkthgAAAAAQ9g2SDmdziGtT5lWKjZNU5s3b5bH41FDQ4O6urrU3t4+ZJxVQklJicrLy1MedjLnHX8nSVoaf0Xma8/kuBoAAAAAg9k2SGVaTKumpmbItkgkotra2uR7wzDU1NSUsfXK7oqWLFSv5qlAcb3x9OO5LgcAAADAILYNUoPXgDJNUzU1NckWqUgkkmxxcrvd2rx5c8rxu3btSs5tP92UzHboRZ0kSdr9dPrJNQAAAADkjq0X5A0Gg/L7/aqtrdXmzZtT1pAKBAKqra1VY2OjDMNQXV2dWlpakkFruDFSdldSIr2ok1StiA4+/9dclwMAAABgEFsHKcMw1NzcLElDZuEbvDCvx+PJ2B1wuikpkf6qEyVJs82/5bgaAAAAwOoRlpjYrbGxUVVVVYrFYurq6lJbW5u6u7tlmuaQY7q6umSapnw+nzwej8LhsILBYPKYuro6eTwemaapUCiUbBgxDEOmaaqhoSG3P3gGjng8Hs91Ebk22tWLp0o8Ln2s4Hbdro/pkeNLdd62vbkuCQAAABNo//792rp1q5YtW6bZs2fnupxRM01TVVVV6u7uTpkErq2tTTU1NXK73YrFYpo/f37KMYltnZ2dcrvdaT+nurpanZ2dyc9saWnRrl27kg0rE2m473+02cDWLVL5yuGQumafJu2XTn1jv6L7dslVuiDXZQEAAGCyxOPSvn25uXZpqfUL6Ci4XK6021evXq0tW7ZkPM/pdMowDG3cuFFut3vI56SbbbuxsVEtLS2jqisXCFI2tX3eaTq0v0AL9vdr818elqv2A7kuCQAAAJNl3z6prCw3196zR5o7d0ynRiIRGYaRDErDiUajqqqqSrsv0Y2vra0tpSufXbv1STaetS/fFZXN1vM6RZIU/cPvc1sMAAAAkMbGjRuTrzMFqVgsJr/fn1zzNZMNGzbI5/PJ4XCorq5O4XA47RqydkGLlE2VlUlP6XS9U8/q4J+YAh0AAGBGKy21WoZyde0stbW1SZLC4bCampoyHpMIVz6fb8QWK6/Xq66uLoXDYXV0dKiurk7BYHDIpHN2QZCyqblzpaf1d/qQNqr0ua5clwMAAIDJ5HCMuXtdLjQ0NMjpdA67bmvimNGIxWLJ7oENDQ1qaGhQW1ubAoGAbYMUXftsKtEiJUnHmDtzXA0AAAAwlMfjmZDud6ZpKhJJ7YW1evVqxWKxcX/2ZCFI2dTcuUeClLHjgPbv681xRQAAAMh30Wh0Qo5Nt8/v96e8D4fDtm2NkujaZ1tz50ova6l6ikpUcbBPL/yxQyefvyrXZQEAACBPJRbklazQU1dXNyToRCKR5AQUzc3N8vl8Q7r/JRbklaRAIKA1a9ZIkurr69XS0pJs4erq6pqUNaQmCgvyyn4L8kqSzye1tUl/cJ2kFdG/6vH/+LTO9n8n12UBAABgAkzXBXlniolYkJeufTaVGGtoVlhToB968okcVgMAAABgIIKUTSXWY9teYY2Tmvvs0NWeAQAAAOQGQcqmEi1SL5fXSpKO2cbMfQAAAIBdEKRsKhGkXi07W5K0KHZQb+94PYcVAQAAAEggSNlUomvfvv7F2jrfIUl65eF7clgRAAAAJhrzvuVGf3//uD+D6c9tKjFByO7dDr18glPLursVe+x+6fJ/yW1hAAAAGLfi4mI5HA7t3LlTRx99tBwOR65LygvxeFwHDhzQzp07VVBQoFmzZo35swhSNpUIUj09UveZVdITW1QwaLVnAAAATE+FhYU67rjj9Morr2jbtm25LifvlJaWaunSpSooGHsHPYKUTSWCVG+v5Kiple7YoqOe2ZbTmgAAADBxysrKdNJJJ+ntt9/OdSl5pbCwUEVFReNuBSRI2dTAILXg3RdJ+r6OfX2v1URVUZHT2gAAADAxCgsLVVhYmOsyMAZMNmFTiazU2yudcuq7tdVpvX/rD4/krCYAAAAAFoKUTSVapPr7pVIdpT8vLZEk7XyQmfsAAACAXCNI2VRpqZQY+9bTI71xaqUk6eAfH89hVQAAAAAkgpRtORyp46TePvN0SVL50y/msCoAAAAAEkHK1gYGqfJzL5AkHfV6jxSN5rAqAAAAAAQpGxs44cTJJ56lv84/vIP1pAAAAICcIkjZ2MAWqdMWnqYtS6z3ux/5fc5qAgAAAECQsrVEkOrpkUqLS7XtxKMkSfseeyCHVQEAAAAgSNnYwBYpSepb/i5J0uwn/5yjigAAAABIBClbGzhGSpLmnf0+a/vrMWnnztwUBQAAAIAgZWeDW6ROqzpHzy04vLOzMyc1AQAAACBI2drgILX8mOXqPDzhRN8fH8tNUQAAAAAIUnY2cLIJSVo4d6FeXGb199vz6P05qgoAAAAAQcrGBo+RkqS3znynJKkk8nQOKgIAAAAgEaRsbXDXPkmad9Z7ddAhle2MSdu356QuAAAAIN8RpGwsXZA6bdlZ+tPiw28eeWTKawIAAAAgFeW6gOGYpqlQKCTDMGSaphoaGuR0OjMeHw6HZZqmDMOQJHk8nimqdHKkC1LLj1muXy2Val6TDj38oAo/9KHcFAcAAADkMVsHqfr6enUenubbNE2tXbtWwWAw7bHhcFjBYFCtra0yTVN1dXXq6uqaynInXGKMVGKyCUk6vuJ4PVk1V/rDXvU9cJ9Kc1MaAAAAkNdsG6RM00x5bxiGwuFwxuN9Pl8ydBmGoY6OjkmtbyoMbJGKxyWHQ3I4HNpTe4Z056Oa/cwLVspKJC4AAAAAU8K2Y6TC4bBcLlfKNpfLpUgkMuRY0zQVjUbldDoViUQUi8WS3fums0Qvxv5+affuI9uXvuNsdc2XCvrj0uOP56Q2AAAAIJ/ZNkjFYrG026PR6JBtkUhELpcrOZ6qra1NoVAo42f39fWpt7c35WFHc+ZIJSXW6+7uI9trltTo4aWH3zDhBAAAADDlbBukMkkXsKLRqEzTlMfjkdPpVENDg+rr6zN+RiAQUEVFRfJRWVk5iRWPz/z51vPAILXi2BV65HDJ/Q89NPVFAQAAAHluUoJUYWHhuD/D6XQOaX1KdN8bzDAMOZ3O5L7Ec7pugJLU1NSknp6e5GO7jddjShekjPmG/nzy4XFRjz8mvf321BcGAAAA5LGsg9TgLnGDHz09PYrH4+MuLNPU5TU1NUO2ZTseqqSkROXl5SkPu0oXpBwOhyrOOEu75kgF+/ukJ57ITXEAAABAnsp61r7hZsNzOByKx+NyOBzjKkoaGo5M01RNTU1Ka5PT6ZRhGDIMQzU1NYrFYnI6ncm1pNxu97jryLV0QUqSaivP0qOV9+rSF2SNk1qxYsprAwAAAPJV1kFq1apVk1FHWsFgUH6/X7W1tdq8eXPKGlKBQEC1tbVqbGxMOba6ulqdnZ0zYvpzKXOQWnHsCj20VFaQevhh6d/+bcprAwAAAPJV1kHq5ptvHrbFaSK69SUYhqHm5mZJktfrTdk3eGFep9Op1tbWCbu2XSSGhA1pkVpSq8Dhmfv6H35IBYmFpgAAAABMuqyD1HXXXTfiMddff/2YisFQmVqkFpUt0s5TK9VXuF0lO3ZKXV3SiSdOfYEAAABAHpp205/nm0SQSres1hknnKXNSw6/efjhqSoJAAAAyHsEKZvL1CIlSSuWrDiyMC9BCgAAAJgyWQepTZs2af369dq2bdsklIPBhg1SxxKkAAAAgFzIOkiZpql7770342K3mFjDBanqJdV6fOnhCSaef17auXPqCgMAAADyWNZByjAM1dfXa/ny5ZNRDwYZLkiVzSrTkuPfpacXHt5w//1TVhcAAACQz7KetW/lypVauXLlqI7t7e0d8Zjy8vJsS8grA4NUuhnOz6s8T2Hjaf3dDkmbNkmrV095jQAAAEC+yTpIJdx1113aunWr1q1bpyeeeCJtC9VIi+I6HA5dfvnlYy0hLyTWkTp4UNq7VyorS91/3tLz9PNlt+jfHpcUDk91eQAAAEBeGnOQko4skmsYhm699VZ94hOfSNm/atWq8Xw8JM2dKxUVWUGquztNkKo8T1efIB0skIpMU9q6VVq2LCe1AgAAAPlizEHK6XTK6/XqQx/6kDwej7rTDOK5+eab5RjcF+2weDwuh8OhdevWjbWEvOBwWN37du60glRlZer+E5wnaN6CY/SHY1/Tedtlde8bFGgBAAAATKwxB6knnnhCmzZt0pYtW3TLLbfooosuGnLMddddN67iYBkYpAZzOBw6b+l5ChshghQAAAAwRca8IO+yZcvU2dmplStXyu/3KxqNTmRdGGC4mfskq3vfpkRvvk2bpP7+KakLAAAAyFdjDlKrVq3SssNjcWKxWPI1Jt6CBdZzpqx6XuV5evw4aW+xrKarP/95ymoDAAAA8tGYg5SkZHhavnx5xinRN23apPXr12vbtm3juVReO+oo6/nNN9PvP3PxmSqeU6oHjz+8gdn7AAAAgEk1riAlacSAZJqm7r33XkUikfFeKm+NFKSKC4t11rFnKWwc3rBp05TUBQAAAOSrcQepUCg07H7DMFRfX592nSmMzkhBSho0TuqBB6QDBya9LgAAACBfjTtIxePxYfevXLlSa9euZQzVOIwmSL3n+PfoqUXSzrICa+XeRx6ZmuIAAACAPDTuIJVpnaiB7rrrLq1fv16SNW06sjOaIHVu5bkqLCrSPcbhGfvuuWfyCwMAAADy1LiD1Gh5vV5JVle/W2+9daouOyOMJkiVzSrTimNX6J6TDm8gSAEAAACTZtK79kmS0+mU1+vV+vXrtXXrVnVnWhAJaY0mSEnSBSdcoHurpP4ChzUF+vbtk18cAAAAkIfGHaQMwxjxmCeeeEKbNm3S8uXLdcstt6iqqmq8l80riSDV3S0dPJj5uPNPOF/RUumJymJrw//+7+QXBwAAAOShcQepVatWjXjMsmXL1NnZqZUrV8rv9yuaaWVZpDV/vpQYijbcV3du5bkqLijWr5cdnrGPIAUAAABMiikZI7Vq1arkrH2xWIwZ/LJUVGSFKWn47n2lxaU6+7iz9T+JcVL33iv19U16fQAAAEC+yTpIJSaKGGkh3sES4Wn58uVauXJltpfNe6MdJ3X+CecrcowUdc2R9uyR7rtv8osDAAAA8kzWQSoRiILB4IjH9vb2jvjA6GQz4US8QPrNKYf7Av7615NbGAAAAJCHikZ74Pr16+V2u7VgwQKtX79edXV1I57T0dEx7H6Hw6HLL798tCXktdEGqXMqz1FJYYl+ZuzTxx6TdPfd0ve+JxVM2Uz3AAAAwIw36iC1bNkydXd3q729XVu2bNGuXbt05plnDnvOaCaiwOiMNkjNLpqtcyrP0f19v9eB0tma9dpr0pYt0ooVk18kAAAAkCdGHaQSoSibcHTzzTfLkZhubpB4PC6Hw6F169aN+vPy2WiDlCRdZFyk32/7vf5wukvvefxV6Ve/IkgBAAAAE2jUQWqwu+66S1u3btW6dev0xBNPaPny5UOOue6668ZVHI7IJkhdfOLFuuG+G3Tb0l16z+Oyxkl9/euTWh8AAACQT8Y1cMbr9UqyFuVNzOaHyZEIUjt3jnzsmYvP1MK5C/WLZX3qLyqUnnlGevHFyS0QAAAAyCNjDlJOp1Ner1fr16/X1q1b1d3dPZF1YZCFC63nHTtGPrbAUaCLqi5Szxyp6/RKa+Ndd01ecQAAAECeGXOQeuKJJ7Rp0yYtX75ct9xyi6qqqiayLgyyeLH1/Prrozv+4qqLJUl3ntZvbbjjDiken4TKAAAAgPwz5iC1bNkydXZ2auXKlfL7/YpGoxNZFwZJBKkdO6T+/pGPv6jqIknSfy15WfGSEukvf5GefHISKwQAAADyx5iD1KpVq5KL88ZiseRrTI5E176DB6XRZNaFcxdq+eLl6pkjvfze062Nd9wxeQUCAAAAeWRck00kwtPy5cu1cuXKCSkI6RUXSwsWWK+z7d73i+pSa8Odd0qHDk1CdQAAAEB+yTpI9fb2Dvvo6emZsOJM01RLS4tCoZBaWloUi8VGdZ7f7x/1sdNJtuOkLjnxEknSzeV/Vtzlkl57Tbr//kmqDgAAAMgfWa8j1dHRkXGfw+FQfAInNKivr1dnZ6ckK1StXbtWwWBw2HMikYhaWlrU1NQ0YXXYxeLF1lCn0Qap85aep/mz5+u1/bv02t9fpiU/vdvq3ufxTG6hAAAAwAyXdZBatWrVZNQxhGmaKe8Nw1A4HB7VeYZhTFZZOZVti1RRQZEuPeVS3f7k7Qq5Z+uzP5U1Dfr3vieVlk5anQAAAMBMl3WQuvnmm+VwODLun6gWqXA4LJfLlbLN5XIpEonI7XanPScUCsnr9crv909IDXazaJH1/MYboz/ng6d8ULc/ebu+WfBHfWbZMjm2brXC1JVXTk6RAAAAQB7IOkhdd911Ix4zEUEm0xinTNOsx2IxOZ3OUX12X1+f+vr6ku97e3uzLS8nsm2Rkqxp0GcXzdbWnm16ffU1Oqb5u1IgIF1xhVRYODmFAgAAADPcuGbty6R/NAsdjVGmgNXe3i7PKMf+BAIBVVRUJB+VlZUTWOHkGUuQmjtrbnL2vu+fXSg5ndKzz0rt7RNfIAAAAJAnJiVITQSn0zmk9SkajaZtdQqHw1q9evWoP7upqUk9PT3Jx/bt28db7pQYS5CSpKuWXyVJ+v6Ld+rgv/1/1savfIWp0AEAAIAxsm2QytS6VFNTk3Z7e3u72tra1NbWJtM0FQgEFIlE0h5bUlKi8vLylMd0MNYg9f6T3q8l85bozX1v6u6LT5Dmz5eee076+c8nvEYAAAAgHzjiEzlf+QSrrq5Omf7c5/Mlp1+PRCJyOp1pZ+hzOBzq6uoa9ex9vb29qqioUE9Pj61D1Y4d1oQTDofU12ct0jta/37/v+urD35V51Wep4dff7/0hS9IS5da3fyYwQ8AAACQNPpsYNsWKUkKBoPy+/0KhUJqbW1NWUMqEAgoFAqlHB+LxdTS0iJJam5uztgiNV0tWGDNDxGPW6EqG5+s+aSKC4r1yPZH9Lj3bKmyUnr5Zenw9wUAAABg9GzdIjVVpkuLlCQdd5z0t79Jf/yjVFub3bn/+ut/1Q//9EN98NQP6pcFV0irV0uzZ1vd/I4/fnIKBgAAAKaRGdEihaGOO856fuWV7M+97jxr6vpfP/drPXf+u6Tzz5f275fWrZu4AgEAAIA8QJCaZsYTpE496lR94JQPKK64bn50vfTtb0sFBVIoJN1338QWCgAAAMxgBKlpZjxBSpKuf/f1kqQfP/ljdR1bKn3qU9aOz3xGOnBgAioEAAAAZj6C1DSTCFJjXfrq7OPO1iUnXqJD8UO66aGbpBtvlI4+WnrmGSaeAAAAAEaJIDXNVFZaz2NtkZKkG8+/UZJ0+5O368X4Lulb37J2fPWr1sQTAAAAAIZFkJpmxtu1T5JWHLtC/3DSP6g/3q+vPvhV6UMfkt7/fqtr39q10qFDE1MsAAAAMEMRpKaZRJD629+k/v6xf06iVeqnT/9Uz+96Qfr+96WyMunhh6Wbb56ASgEAAICZiyA1zSxZIjkcVuPRm2+O/XOql1TrA6d8QP3xfn3hvi9IS5dK3/mOtfOLX5Q2b56YggEAAIAZiCA1zRQXS4sXW6/HOuFEwk0X3qQCR4HuevYuPbr9UeljH7MW6T14ULriCmnPnvEXDAAAAMxABKlpaCLGSUnSuxa+S1edeZUkad296xSXpFtusWa0+Otfpc9+dnwXAAAAAGYogtQ0NFFBSpK+csFXVFpcqsdeeUx3PXuXNH++dMcd1kK9t90mtbeP/yIAAADADEOQmobGu5bUQMfMO0brzlknSbo+fL0OHDogvfe9UlOTdUBDg/Tyy+O/EAAAADCDEKSmoaVLreeJCFKSdN1512nR3EXq6u7Sd//4XWvjl74knXWW1NMjfeQjTIkOAAAADECQmoYSQeqllybm88pmlemmC2+SJH35gS/rtd2vWbNa/PSn1pToDz0k/cd/TMzFAAAAgBmAIDUNHX+89TyRPe6uWn6VVhy7Qr19vWoMN1obq6qk7w5ooXrssYm7IAAAADCNEaSmoUSL1N/+Zs1UPhEKHAX67vu/K4ccuuOpO/TgSw9aO668Uvrwh62ufatWWRcFAAAA8hxBaho66ijrub9/Ypd6qllSI1+1T5J0zf9co7cPvW2t/tvaKp12mvTaa9IHPyi99dbEXRQAAACYhghS01BRkTU7uTTxmeZrK7+mBXMW6M87/qxv/+Hb1sZ586S775ZcLmnLFukTn5Di8Ym9MAAAADCNEKSmIYdDmj3ber1//8R+tmuOS82eZknSF+//orqiXdYOw5BCISvF3Xmn1NIysRcGAAAAphGC1DQ1WUFKsiaeuHDZhXrr4Fv6xG8+of54v7Xjggukbx9upWpqkn7zm4m/OAAAADANEKSmqTlzrOfJGK7kcDi04dINKi0u1e+3/V4bOjcc2Xn11dInP2l17bviCukvf5n4AgAAAACbI0hNU5PZIiVJxnxDX7/w65Kk6zqu08s9A+Za/9a3pPe9z5rp4rLLpF27JqcIAAAAwKYIUtPUZAcpSfr0ik/r3MpztfvAbq39zdojXfxmzZKCQemEEyTTlFavlg4cmLxCAAAAAJshSE1Tk9m1L6GwoFA/uOwHml00W/d23avv/OE7R3YefbT0619Lc+dK990nffSj1lpTAAAAQB4gSE1TiRapyV7S6dSjTtU3LvqGJKkx3Kin3njqyM7TT5fuuksqLpY2bpSuuYZp0QEAAJAXCFLTVCJI9fVN/rWurrlal558qQ4cOqAP3/VhvfX2gPR28cXSHXccWbj3C1+Y/IIAAACAHCNITVNT0bUvweFw6AeX/UCLyxbrmZ3P6LqO61IPWL3aClGSFAhIN988+UUBAAAAOUSQmqamYrKJgY6ee7R+/MEfS5K+u/m7Cv4lmHrA2rVSs7WQrxobpf/+76kpDAAAAMgBgtQ0NZUtUgkXVV0k/3l+SdK//Ppf9OzOZ1MPaGyUbrjBev2Zz1jTpAMAAAAzEEFqmprqFqmEmy68SReccIH2vr1Xl7dfrt19uwcdcNORcVKf+5z0n/85tQUCAAAAU4AgNU3lokVKkooKivRz78917Lxj9dybz+lf7/5XxQfO1OdwSF/9qvTv/269//znGTMFAACAGYcgNU3lqkVKkhbOXahgfVDFBcUKPhPUzY8OCkoOh3TjjdKXv2y9b2yUvvIVpkYHAADAjEGQmqZy1SKVcE7lOfrWJdYYqOvD1+vu5+8eetCXviR97WtHXq9bR5gCAADAjECQmqZy2SKVcHXt1bq65mrFFdc//+Kf9fQbTw896IYbpG9/23r9n/8pNTRIBw9ObaEAAADABCNITVOJhp0f/SinZehbl3xLK5et1J4De3Tpzy7Vjr07hh70mc9It90mFRRIt94qXX65tHfv1BcLAAAATBCC1DS1cOGR1wcO5K6O4sJitde360TXiXqp5yVdvvFy9R3sG3rgxz8uhUJWU9pvfiNdeKG0I03oAgAAAKYBRzxu30ErpmkqFArJMAyZpqmGhgY5nc60x0YiEYXDYUnS5s2btWHDhozHDtbb26uKigr19PSovLx8gqqfXAcPSsXF1uu//lWqqsptPc+9+ZzOvvVs9fT16Iq/u0I/+aefqMCRJqc/+qh06aVSNCoZhnTPPdLJJ099wQAAAEAao80Gtm6Rqq+vV2Njo7xer7xer9auXZvx2HA4rMbGRjU2Nqq2tlYrV66cwkqnXlGR9I53WK+3bctpKZKkU486VcH6oIoKinTn03fK3+FPf+C551phatkyyTSlc86R7r9/aosFAAAAxsm2Qco0zZT3hmEkW5wGi0QiCgQCyfder1eRSGTIZ8w0y5ZZz3b5Meuq6vSDy34gSVr/2Hp98/Fvpj/wlFOkxx6TVqywWqYuuki65ZapKxQAAAAYJ9sGqXA4LJfLlbLN5XIpEokMOdbtdmvDhg3J97FYLHl8On19fert7U15TEeJILV1a27rGOijZ3xUgZVWqL32/65V+1/a0x+4aJH0+99LV1xh9VO8+mrp05+W3n576ooFAAAAxsi2QSoRhgaLRqNpt3u93uTrjRs3yuPxZBwjFQgEVFFRkXxUVlaOt9ycMAzr2S4tUgn+8/z6dO2nFVdcV/7ySt2/NUPXvTlzpDvukL7+dev9d78rrVwpvf761BULAAAAjIFtg1QmmQLWwP2hUEjBYDDjMU1NTerp6Uk+tm/fPsFVTg27BimHw6FvXvJNrXrHKh04dECX/fwy/fFvf8x0sNTUJN19t1ReLj30kFRdbXX9AwAAAGzKtkHK6XQOaX2KRqMjzsTn9/vV0dEx7HElJSUqLy9PeUxHdg1SklRYUKg7Lr9DF5xwgfYc2KOL77hYT77+ZOYTLr1U2rxZeuc7pVdfld73PmvclH0nlQQAAEAes22Q8ng8abfX1NRkPKelpUV+v1+GYSgWi43YejXdGYbVoLNrl/TGG7muZqjZRbN194fv1rmV5yq2P6a6n9Tp2Z3PZj7h5JOlP/xB8nqtsVJXX22NoerpmbqiAQAAgFGwbZAyEs0th5mmqZqammRL0+BZ+UKhkNxudzJEtbe3j3odqemqrEw67TTrtV17wpXNKtP/XPE/ch/j1s59O7Xy9pXqinYNc0KZ1N4utbRIhYXSz38unXmm9PjjU1YzAAAAMBLbBilJCgaD8vv9CoVCam1tTRn3FAgEFAqFJFkhq76+XnV1dXI4HJo/f778/gzrGM0w555rPT/6aG7rGE7F7Ard+5F79a6F79Jre17TyttXaltsW+YTHA7puuukhx+WTjjBWijr3e+WAgGpv3+KqgYAAAAyc8TjDEIZ7erFdvSjH0n/8i9WznjooVxXM7zX97yu9/3ofXph1wtaWrFU9330PlW5qoY/qadH+uQnrZYpyRo79eMfS8cfP/kFAwAAIO+MNhvYukUKI0u0SG3eLB04kNtaRrK4bLHu/9j9OmXBKXq552W990fv1fNvPj/8SRUV0p13Sj/8oTR3rvTAA9Lpp0s/+QkTUQAAACBnCFLT3EknSQsWSH190p/+lOtqRrZk3hI98PEHdNrRp+nV3a/qfT96n57Z+czwJzkcVrPbn/4knX221NsrffSj0j/8g71WIwYAAEDeIEhNcw6HdM451ms7j5MaaFHZIt3/sft1xqIz9MbeN/S+H71v+KnRE0480eq/eNNN0qxZ0j33WLNttLRYs/wBAAAAU4QgNQNMhwknBjt67tG672P3qWZJjd7c96bO//H5euilUQzyKiqSvvAF6amnpPPPl956S/L7WcQXAAAAU4ogNQMkWqSmW45wzXEpfGVY51Wep9j+mC664yL9+rlfj+7kU06R7rvPmm1jwQLp6ael886TPvUpaYavHwYAAIDcI0jNALW11pJLr7wibd+e62qyUzG7Qh1XduiyUy7T/oP7dXn75bo1cuvoTnY4pI99THruOenjH7cmn/j+96V3vMNai4rJKAAAADBJCFIzwNy50hlnWK+nW6uUJM0pnqO7Vt+lq868Sv3xfq39zVrd9OBNGvXM/EcdJd12m9VCdfLJ0uuvS2vWMBkFAAAAJg1BaoaYjuOkBioqKNKtl92qG959gyTpi/d/Ub7f+vT2oSwmkbjgAmvs1Je/fGQyine+0xpDtWvX5BQOAACAvESQmiESQeqRR3Jbx3g4HA59beXX9O1Lvi2HHNoQ2aBLfnqJom9FR/8hJSXSl75kBaoLLpD277dm9TMM6StfkXbvnrwfAAAAAHmDIDVDnH++9dzZKe3YkdNSxu0zZ31Gd3/4bpXNKtN9W+/T2beerRd2vZDdh5xyirRpk/Tb31r9Hnt7rYBlGNI3vmHN9gcAAACMEUFqhjjmGMnttuZXuOeeXFczfv948j/qkase0dKKpXox+qLOvvVs3b/1/uw+xOGwxklFItLGjdb4qTfflNats9akam1l/SkAAACMCUFqBvnHf7Sef/vb3NYxUU5fdLr++Ik/6uzjzlb3/m7V/aRO//nYf45+EoqEggJp9WrpL3+RfvADqbJSevVV6ZOflE49VbrjDunQocn5IQAAADAjEaRmkESQ+r//kw4cyG0tE2VR2SLd/7H79ZHTP6JD8UP6/L2f15rQGu3uG8NYp6Ii6aqrpBdflL79bWnhQsk0pSuvtLr/hUIEKgAAAIwKQWoGqa6WFi2y5lO4775cVzNxZhfN1u0fvF3//ff/reKCYgWfCWrFrSv0zM5nxvaBJSXSZz5jhaivf11yOq3Wqvp6qapKuvlmKZrFBBcAAADIOwSpGaSgQFq1ynr9s5/ltpaJ5nA4dM2Ka/TAxx/QsfOO1XNvPqcVG1boJ0/+JPuufglz50pNTdZaU//v/0kul/TSS1Jjo3TccVJDg/T00xP7gwAAAGBGIEjNMP/8z9bzL34h7duX21omwzmV5yjii+jCZRdq79t79dFffVRX/OIKxfbHxv6hTqf01a9Kr7xijaE64wxrVr8NG6TTT5cuvFD61a/o9gcAAIAkgtQMc8450gknSHv2zJxJJwZbOHeh7v3IvbrpgptU6CjUz//8c51xyxl68KUHx/fBc+ZYY6ieeEJ68EHJ65UKC6X775f+6Z+sbn8tLdJrr03MDwIAAIBpiyA1wzgc0hVXWK9vuy23tUymwoJCfeG9X9AjVz2iqvlVernnZZ3/o/PVFG7S/oP7x/fhDof0nvdIwaDV7a+pSVqwwOr25/db3f4uukj68Y9Z4BcAACBPOeJjHmAyc/T29qqiokI9PT0qLy/PdTnj1tUlnXSStabUiy9aSybNZLv7dutz//s5/fBPP5QkveOod+i2D9yms447a+Iu8tZb1sCzH/5QeuSRI9vnzJEuu0z6yEekiy+Wiosn7poAAACYcqPNBrRIzUBVVdL732+9/t73clvLVJhXMk8/+MAP9Ms1v9SiuYv07JvP6twfnqt1967TW2+/NTEXSXT7e/hhK6l+9avSKadYAWvjRunSS6UlS6RrrpF+/3sW+gUAAJjhaJHSzGuRkqT//V/p7/9eqqiweqRVVOS6oqkRfSuqz/3v5/STp34iSTrJdZJ++IEf6t1L3z3xF4vHpUjEWtD3Zz+T3njjyL7ycqv73z/8g3TJJdLixRN/fQAAAEy40WYDgpRmZpDq75fe9S7p2Welr3xF+uIXc13R1PrtC7+V77c+vbr7VUlSg7tBAU9ArjmuybngwYPW4l133in97nfSm2+m7q+utpoJL77Yej179uTUAQAAgHEhSGVhJgYpyepx9qEPWa1R27ZZs3znk9j+mD7/f59Pjp06uvRorb9ova48/Uo5HI7Ju/ChQ1JnpxWo/ud/pC1bUvcXF0vLl0tutzWYLfFYtsxaLBgAAAA5Q5DKwkwNUv391jJIf/mLdP31UiCQ64py48GXHtTVv7taz+x8RpJ0/gnn63vv/57ecfQ7pqaAN96w+lr+7nfW+KmdO9Mf53BYXQCPOy71ceyxqa9pzQIAAJg0BKkszNQgJUl33y194ANWI8hTT0mnnprrinLjwKED+q/H/ks3PnCj3jr4looLivX5cz6vG95zg+aVzJu6QuJxa0r1xx6TnnnGmlbxxRelv/7VWvxrNBYssALXSA+XSypgPhkAAIBsEKSyMJODVDxuTSj3u99JF14odXTk9+/W22Lb9Nl7PqvfvPAbSdLissUKrAzoo2d8VAWOHH4x8bi0Y4f0yitHHn/7W+r7V16xZgkcraIiadGiI8HqmGPSB66jj5bmzbNaxAAAAPIcQSoLMzlISZJpSqedJu3fL33jG9K11+a6oty7+/m79fl7P6+/Rv8qSao+plrfvOSbkzO730SJx6XubitgvfGG9PrrmR+7dmX32cXF0lFHWa1dRx018mPBAmnuXMIXAACYcQhSWZjpQUqSbrlFuvpq6/flhx+WVqzIdUW513ewT//9x//WVx78inr7eiVJq09brWZPs05wnpDb4sbrwAGrhWu4sJV47N07tmuUlKQGK6dTmj/feh7p9ezZhDAAAGBLBKks5EOQisel+nrprrus3l6PPWZNEgdpx94d+vf7/10bIhvUH+/XrMJZurrmat3wnhu0cO7CXJc3+d56y2rBevPN0T/6+sZ3zVmzsgteA19XVFj/IgAAADAJCFJZyIcgJUk9PdJ732tNOnHyydayR8cem+uq7OOpN57Stf93rTZt3SRJKptVpmvPvlafP/fzKi+ZufdF1uJxad++1GC1a5cUi1mP7u7hX/f3j7+GuXPHHsTmzcvvgYIAAGBYBKks5EuQkqRXX5XOOUd6+WXphBOsySdOPDHXVdlHPB5X2AyraVOTOl/rlCQtmLNAN7znBn2q9lOaXcTU4+MSj1uzE2YKWyMFsd27x1+Dw2G1ajmdR54zvc60jRYxAABmLIJUFvIpSEnSSy9JHo814/aiRdI991jrw+KIeDyuXzz7C33hvi/o+V3PS5KOnXesrn/39fqE+xMEqlw5eFDq7R251SvTvv37J6aO0tLhA9hIr+fMYYwYAAA2RZDKQr4FKcma9O3ii6Unn7TmCdi2TSory3VV9nOw/6B+/Kcf68sPfFmv9L4iyZoyvfHcRvlqfCotLs1xhcjK/v1WqOrpORKwsnk92rW+RlJcPPYQ5nTSPREAgElEkMpCPgYpyfrd8PTTrW5+v/yl9MEP5roi+9p/cL9ue+I2/ccj/6GXe16WJB1derTWnbtOV9dcPbWL+iJ3Ei1i4wljEzFGzOGQysvHHsYqKqwJPwAAwBAEqSzka5CSpM9+VvrOd6zXDz4ovfvd9DgazoFDB3T7k7cr8HBAZrcpSXLNcelTNZ/SNSuu0eKyxTmuELaWGCM21hAWi41/xsSE0tKxt4g5nXRPBADMWDMiSJmmqVAoJMMwZJqmGhoa5HQ6x33sYPkcpJ54QnK7j7w/4wxrmvQPflB65zv5PSmTg/0HdefTd+prD31NL+x6QZI0q3CWPvJ3H9G151yr0xaeluMKMWPt338kWA1+Hs3riZiwQ5KKirKfqGPg6/JyuicCAGxpRgSp6upqdXZaM6eZpim/369gMDjuYwfL5yAlWTNX+/3SHXek/mP30UdbLVQrVlih6p3vtNaeKizMXa12c6j/kH713K/0jce+ocdeeSy5/ZITL9HnzvqcPIZHhQV8YbCRQ4eOdE8caxfFQ4fGX4fDYY31Gs+kHXRPBABMgmkfpEzTVH19fTIcSdL8+fPV3d09rmPTyfcglbBrl/SrX1mL9t53X/oeRIWF0uLF0pIl1mPhwiNDLsrLreeyMqmkxHrMmmU9Br4uKrL+IbqgwPpdKvF68Pt0rwe2kCVep9s2Efuz9dj2x/SNx76hXz73S/XHrXEwi8sW67zK87Ro7iIVFxar0FGouOKKx+PjepY05D2mjk3/b3NqxOMq2X9QpXsPqHTf29bzoNdzDr+eO+B16b4DKt37tkr3HdCsAxMQxCT1zSrUvrmz9FZpsfbNnaW9A17vmztL+1Jez9K+ucV6q3TW4WOLdaCkiGZ3ALCRs449S9esuCbXZYw6GxRNYU1ZCYfDcrlcKdtcLpcikYjcA/uiZXmsJPX19alvQEro6emRZH1p+ay42OrWV19vhag//Ul69FHpz3+Wnn9eeuEFa/vf/mY98tHwQew0ORw/VIF+oHj8kOLxQ3pd0l0Dg45j4C/g8czbUraP9fzhfxYgJwqk4pI+lWu3KuK9qlCvKuK7VaFelcd3qyLeowrtVnl8t5zqVUW8V+UDj9FuVejw7IkHDqnwwFsq635LY5l09KAKdEiFissx5CFZ/zWl2z7VcnVdS379zHn8TySALTyw+DX1Pn9lrstIZoKR/uHUtkEqFoul3R6NRsd1rCQFAgHdeOONQ7ZXVlaOuj7kp4H/PeVzowQwHm9L2nX4kVv9hx8AAFt4PayrKipyXUXS7t27VTFMPbYNUplkCk3ZHNvU1KRrr702+b6/v1/RaFQLFiyQI8fdPHp7e1VZWant27fndTdDjB73DLLFPYNscc8gW9wzyJad7pl4PK7du3dryZIlwx5n2yDldDqHtChFo9G0M/Flc6wklZSUqKSkZMhn2El5eXnObyJML9wzyBb3DLLFPYNscc8gW3a5Z4ZriUqw7dyzHo8n7faamppxHQsAAAAA42XbIGUYRsp70zRVU1OTbDmKRCIyTXNUxwIAAADARLJt1z5JCgaD8vv9qq2t1ebNm1PWhQoEAqqtrVVjY+OIx04nJSUl+tKXvjSk6yGQCfcMssU9g2xxzyBb3DPI1nS8Z2y7jhQAAAAA2JVtu/YBAAAAgF0RpAAAAAAgSwQpAAAAAMiSrSebyDemaSoUCskwDJmmqYaGBmYezEORSEThcFiStHnzZm3YsCF5Hwx3j4x1H2YWv9+vpqYm7hmMKBwOyzTN5My3iaVEuGeQjmmaCofDcrlcMk1TXq83ee9wz0Cyfn9Zu3atOjs7U7ZPxv1hm3snDttwu93J111dXXGv15vDapArzc3NKa8H3hfD3SNj3YeZo7OzMy4p3t3dndzGPYN0Ojo64g0NDfF43PrzNQwjuY97BukM/LspHo8n7594nHsG8XgwGEz+HTTYZNwfdrl36NpnE4k1sRIMw0i2SiB/RCIRBQKB5Huv15tcM224e2Ss+zCzDGxdSLwfiHsGCT6fT83NzZKsP9+Ojg5J3DPIbOPGjWm3c89Asn5fcbvdQ7ZPxv1hp3uHIGUTiebygVwulyKRSI4qQi643W5t2LAh+T4Wi0my7oXh7pGx7sPMEQqF5PV6U7ZxzyAd0zQVjUbldDoViUQUi8WSAZx7Bpm4XC5VV1cnu/jV1dVJ4p7B8Cbj/rDTvUOQsonEL8yDRaPRqS0EOTfwl+GNGzfK4/HI6XQOe4+MdR9mhlgslrZvOPcM0olEInK5XMnxBW1tbQqFQpK4Z5BZMBiUJFVVVSkYDCb/ruKewXAm4/6w073DZBM2l+lmwcwXi8UUCoWGDNpMd9xE78P00t7eroaGhlEfzz2T36LRqEzTTP4jTUNDg+bPn694PJ7xHO4ZhMNhNTc3yzRN+Xw+SVJra2vG47lnMJzJuD9yce/QImUTTqdzSJJOdL1AfvL7/ero6EjeA8PdI2Pdh+kvHA5r9erVafdxzyAdwzCSf86Sks+RSIR7BmmZpqnNmzfL4/GooaFBXV1dam9vl2ma3DMY1mTcH3a6dwhSNpGYdnawmpqaKa4EdtDS0iK/3y/DMBSLxRSLxYa9R8a6DzNDe3u72tra1NbWJtM0FQgEFIlEuGeQ1sAJSQbjnkE6kUhEtbW1yfeGYaipqYm/mzCiybg/7HTv0LXPJgb/xWaapmpqaviXmTwUCoXkdruTISrRbWvwvTDwHhnrPkx/g/9C8fl88vl8aX9Z5p6BZP19U1NTkxxbl5jtMdOMW9wzcLvdam1tTRnDu2vXLu4ZpDVw3O5wv9/OhN9rHPHhOkVjSpmmqdbWVtXW1mrz5s0pi2oiP5imqaqqqpRtTqdT3d3dyf2Z7pGx7sPMEIvF1NbWJr/fr4aGBvl8Prndbu4ZpBWLxeT3+1VdXa3Ozs5kC7jE/88gvXA4nOz+KVn/iMM9g4RwOKyOjg61tLSosbFRtbW1yeA9GfeHXe4dghQAAAAAZIkxUgAAAACQJYIUAAAAAGSJIAUAAAAAWSJIAQAAAECWCFIAAAAAkCWCFAAAAABkiSAFALCNcDgsn88nh8Mhv9+vcDics1qqq6sVCoVydn0AgL2xjhQAwFYSC1N3d3enLLAYi8WmdMHFcDismpoaFggFAKRFixQAwFZcLteQbaZpqr29fUrr8Hg8hCgAQEYEKQCA7TU3N+e6BAAAUhTlugAAAIYTDoe1ZcsWRaNRSVZLkWEYCofDikQiMgxDmzdvVnNzc3KMld/vlyS1traqs7NToVBITqdTpmmqq6srJZiZpqnW1lbV1tYqGo1q9erVMk1Ta9eulc/nU0NDgyQpEokoHA7LMAyZpimv15usw+/3y+fzJfd1dHQoGAym/AyDa43FYmpvb5dhGIrFYsntAIDpgSAFALA1j8cjj8ejqqqqZKgxTVN+v1+dnZ2SpGg0qpaWFjU2Nsrj8aizs1Otra3JboL19fXq6uqSx+ORz+dTKBSS1+tVLBZTXV2dOjs75XQ65ff71dbWpsbGRq1ZsyZZQ+J6HR0dyW3V1dXatGlTsr6B4SkYDCoSicjtdmesVZLcbrc8Hk9yOwBg+iBIAQCmnURIGjir3+bNmyVJTqdTCxYskCR5vV5JSk5cYZqmotGoTNOUpGSLUGIsVFNTU8brud3ulG2GYai9vV0NDQ1asGBB8pqJGhLBKFOtzc3Nqq6ulmEYWrNmTTIkAgCmB4IUAGBaicViklJbcySlBBHDMFLOCQQCWrBgQbI73sDPGjihxGRNLpGu1lgspu7ubkUiEW3cuFH19fUpLV4AAHtjsgkAgK2M1MUtHA5rzZo1Q9aYGvh+4Gckxic1NjYmxyMltnu9XkUikYyfkzg23fUikYhWr1494s+TqdZAICDTNOV2u9Xc3MwMgQAwzbCOFADANsLhsILBYMo4pcQ4o0RXuIGTTXR0dKi2tlaSNZZqy5Yt8vv9crlc8vv98ng8isViyYkjElpbW7VmzRp5vd60n5OYbMLlcqm1tTXt5BaJ2iKRiNauXStJ2rBhQ3JMVCIgZaq1ra1NTqdTLpdL0WhULpcr2RURAGB/BCkAAAAAyBJd+wAAAAAgSwQpAAAAAMgSQQoAAAAAskSQAgAAAIAsEaQAAAAAIEsEKQAAAADIEkEKAAAAALJEkAIAAACALBGkAAAAACBLBCkAAAAAyBJBCgAAAACyRJACAAAAgCz9/1vtabXBkc+BAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1UAAAE2CAYAAACJALgbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABE70lEQVR4nO3de3Qb6X3f/w94FymRQ3CvtrW2huv4knrTBUE723Qb1wK9zaVO4oBi3TiNG4fAsdu0yZ4uYTbJz9nTJAy5jn+pe9oakH3iNslxRCCb2LGd34aQ7cRuk5riOPZ2na67GK0t23sVOIQkipRI4vcHhVkMCF5BLgbi+3UOj4T5zgwePPNohC+fywSKxWJRAAAAAIA9aap3AQAAAACgkZFUAQAAAEANSKoAAAAAoAYkVQAAAABQA5IqAAAAAKgBSRUAAAAA1ICkCgAAAABqQFIFAAAAADUgqQIAAACAGrTUuwAA0Ihs21YymdTU1JRM01Q8HpckXbx4UZLU39+vWCy2o3Nls1ml02lJ0tDQkKLR6MEU2ofi8bimp6eVTqcViUR2ffx+Xge/qbVuKk1NTbl/v3jxouLxuDKZjMbGxmo+927sV3u3LEuJREL5fF5zc3P7WUQA2LVAsVgs1rsQANCohoaGZJqmksmkZ3s8Hlc+n3e/PEpSIpGQbduebZIUCAQ0Pz+vc+fOSdK+fIHeT5uVe78MDAxocnKyps+9m+vQSPajbqT1eojH4wqFQu624eFhSdpV3aRSqQ1J6m7bx36292w2q3g8rlwut+dzAMB+YPgfAByAZDIpx3GUSqXcbUNDQxoZGfHsZ1mWTNOUYRiKRCK+S6ik6uVuFNWuw2E0PT3tSagk6fTp07s+z8zMzIZtu2kf+93eg8FgTccDwH5h+B8AHJDh4WElEgn3N/ubfYE0DONlLNXu+THR243K63AYOY4j27Zlmqa7zTAMDQ4O7vgcqVRKtm1v2L7b9uH39g4Ae0FSBQAH5NSpU4rH47IsS9JLw6RKQ5Usy1IymZRt2+6coNL8ktLr0hfhaDSqbDarRCLhzh0q9RpMTk5ue4y03jNh27Zs29bFixfd40oqe3NisZg7b6W83JLc3h/TNDUzM7NhaNlWpqamZBjGlr0M1T7LXpVfh1IZd1pXpfk6yWRSqVRKwWBQZ86c0fj4uOfzblUfu7kG29XNXus9FAppaGhIyWTSkwSVz6fa7jPMzMy4bbV07Gbto3SeUjJnGIbC4fCu2nv5ucpVS44ty9q0XmtpqwCwY0UAwJ5FIpFiLBbbNC6pmEwmi8VisTg3N1c0TdMTr7YtGo0W0+m05z3m5uaKxWKxmE6ni6FQqDgzM1Ocm5srjo2NbXvMzMxM0TTN4szMjBs3TdONF4vF4uTkpHuu0vuUzletjGNjY8VcLuc53/z8/Kb1UH5cqT6KxWJxfn6+KMlTtq0+y2Z2cx32UleVdRMKhTZ8rq3qYyfXYCd1s9d6z+VyRdM0i5KKkoqRSMRz3p1+hsrPXSxubB/pdNrzOXK53Jb/Bra6Hlu1y9L5DMPYtl73UmcAsBvMqQIAH7FtW5lMxvOb+uHhYXcBBsMwZFmWIpGIQqGQJicntz0mGAzKtm1PD0WpV0Ba/01+IpHQ+Pi4Gz9z5kzVoV7l5cxms57zlb+uxnEcTU1NeXoaDMPw9Bps91lqtde6KhcKhTbUzXb1sZNrsF3d7OR9NmOapnK5nGZmZjQ2NqZ8Pq+hoSFlMpmaz11NOp2W4zjuecLhcNX9troeO22XjuNsWq/7/bkAYDMM/wOAA1L+pXKnstmsDMPwfOnL5XKeL4mV59vLMYZhKJ/PS5LOnTsnwzA8c122W8mtFC8N78rn8+75tvtsO9lnq8+yW+XXYa911d/fv+V77KQ+troGO6mbnb7PVsoXh0gkEhodHXUTmlrPXRKNRpVMJtXb26tQKKSRkZFNl23f6nrstF1uVa/7+bkAYCskVQBwQEpLRm/2W/pqHMeRaZqe37xXLgRQ+eV7J8ds9567ZVmWJiYmNDQ0pFOnTu04cdxutbZaP0s15dehtPrcfp5f2nt9lNvJSnZ7eR/HcZTNZjfMS5ucnNTU1JQcx3F7QHdz7sqFL8rNzMzIsixls1m3F7BaYrXV9S7vRavFflwbANgOw/8A4IAkk0lNTk7uarWzakPLpK0Tn70cU3l8tX03O95xHJ08eVLj4+OKxWIyDMPdd6sepc3KuZN99pL4lZRfh4M4/17ro9xO6qaW95mdna26vbS8+V7OXVqApVJpYYlQKKSxsTHNzc3pzJkzVffd6nrstl1Wsx/XBgB2gqQKAA5AqQdgs2FPm4lEIgqHwxt+Sz89Pb2vx5R/MS2ttlZa1a0U3+x427bdL70lpeFUm33RLr1PLBbzrObmOI4sy3LLs5fPspXK61BrXVWz1/qovAbb1c1e30daT3Qq5xGV917t5Nzlc5Vs2950Bb1qzwXbrHdoq+ux23ZZvk9JLXUGALsRKBaLxXoXAgAajW3bSiaT7lLQ8XhcknTx4kU5jqP+/n5PQlUagpTJZDQ5OekuR13aNjY2ppGREffLXyKRUH9/vzskrLTk9+TkpM6dO6fx8XFFo1HPl9Vqx1R736mpKU1MTMg0Tfc8peP7+vpkmqby+by7pHrl8aV9pfUHv0rrX5oTiYRGRka2Xf68VGelHrzSuSYnJz3zfSo/y35ch73WVTgcViKRUDAY9Fyz8fFxGYaxZX2Yprnja7Bd3eyl3kuJiGmaGxKJ8rrZyblL+/T392/aPkoJValubdtWLBaTbdu7au/l5apsl1L1f1PV6rWWtgoAO0VSBQAAAAA1YPgfAAAAANSApAoAAAAAauCrJdVLD/krjSUvPRSwNDm2tHJPLTEAAAAA2E++mVNlWZYGBgY0Pz/vJkADAwOam5uTtJ4oJRIJ9yF+e40BAAAAwH7yzfC/yocIVj4/wjRNdznYvcYAAAAAYL/5YvhfJpPxLHsqrT8/o/Lp8sFgUJZl6dy5c3uKVT5TY3l5WcvLy+7rtbU15fN59fX1KRAI7NfHAwAAANBgisWiLl26pFe84hVqatq6L6ruSZXjOFXnO232sMV8Pr/nWKWJiQk9/PDDOywpAAAAgMPmwoULetWrXrXlPnVPqqanp90H+e3EVk+2321sfHxcDz74oPt6YWFBd911ly5cuKDu7u4dl+kgfOEL0k/+pPSGN0h/8zd1LQoAAABw6BQKBR0/flzHjh3bdt+6JlXZbFanTp2qGjMMY0PvUj6fl2EYe45Vam9vV3t7+4bt3d3ddU+qjh5d/7O5WapzUQAAAIBDayfTguq+UMX09LRSqZRSqZRs29bExIQsy1IkEqm6fzgc3nOsEfljbUYAAAAAm6lrT1VlAhSPxxWPxz2rAJbYtq1wOOz2Ru0l1khYJwMAAABoDHWfUyWtz3dKpVKSpMnJScXjcYVCIaXTaSUSCQ0ODmp2dtbzrKm9xhoNPVUAAACAv/nm4b9+UCgU1NPTo4WFhbrPqfrCF6S3vU164xulJ56oa1EAAADwMlhdXdX169frXYxDo7W1Vc3NzZvGd5Mb+KKnCpsj5QUAALi5FYtFPfvss1uuZI2DYRiG7rjjjpqfUUtS5VPMqQIAADgcSgnVbbfdps7Ozpq/4GN7xWJRi4uLev755yVJd955Z03nI6nyOXqqAAAAbl6rq6tuQtXX11fv4hwqR44ckSQ9//zzuu2227YcCridui+pjupKv6AgqQIAALh5leZQdXZ21rkkh1Op3mudy0ZS5VP0+gIAABweDPmrj/2qd5Iqn6OnCgAAAPA35lT5FL+sAAAAgN9NTU3JMAwFg0HZti3TNBWNRt24ZVlKJpNKpVIaGxtTf3+/crmcbNtWPB5XJBKRJNm2rUwmI8MwJEmmacq2bcVisR3F642kyufoqQIAAIAfDQwM6PTp0wqFQu62RCKh2dlZTU5OSpJCoZAmJyeVSqU0Pj7uJkWO46i3t1dzc3MKhUIaHh7W3Nyce56pqSldvHjRfb1dvN5IqnyKnioAAIDDp1gsavH6Yl3eu7N158u5JxIJmabpSagkaXJyUr29vRoZGdkQK2cYhkzT1JkzZ9xEq9zY2JimpqYkrfdSbRX3A5Iqn6OnCgAA4PBYvL6ooxNH6/Lel8cvq6uta0f7Tk1NKZlMVo1FIhFNTEwonU5veY58Pq/+/n53KF8qlfIM5yv9fbu4H7BQhU/RUwUAAAA/KvUchcPhqnHTNGVZ1qbHO46jRCKhSCTiJkanT59WPB5XIBDQ0NCQstmspwdru3i90VPlc/RUAQAAHB6drZ26PH65bu+9G/l8flf7p1IpmaYpSYrH4+7fJSkajSqXyymbzWpmZkZDQ0NKp9PuohfbxestUCzytb2kUCiop6dHCwsL6u7urmtZ/uf/lH7ohyTTlHK5uhYFAAAAB2RpaUnnz5/XiRMn1NHRUe/i7FggEFAymaw6BG9oaEiGYbjD/0qLUszPz1ftXXIcZ8P2VCqlZDKpubm5beO12Kr+d5MbMPzPpxj+BwAAAL8aGxvbdM7UuXPnFI/Hd3wu27Y3DBc8deqUHMfZUdwPSKp8jn5EAAAA+M3k5KTy+byy2axnezwe16lTp9znT5XbarhgIpHwvM5ms56hfdvF6405VT5FTxUAAAD8bG5uTolEQrZtuw//HRoa2vDw3zNnzkhaT8Ti8XjVpdaHh4fdBwlLUi6Xc591tZN4vTGnqoyf5lT9zd9I990nveY10vnzdS0KAAAADkijzqm6WTCn6iZHTxUAAADQGOo+/K80DtNxHM3OznqevlyakBYKhWTbthzHcWO2bSuTybgPA4vFYm534FaxRkM/IgAAAOBvdU+qhoeHdfbsWUUiEeXzeQ0PDyt3Yw3xZDKpVColaf3JzOUrjAwPD7tLKNq2rdHRUTe+VaxR0FMFAAAANIa6J1XpdNozWa28R2lgYEDz8/Mbtpee4lximqbb47VVrBHRUwUAAAD4W92TqvLlFtPp9IY17asN28tmswoGg55twWBQlmXp3Llzm8YqVxpZXl7W8vKy+7pQKOz1Y+w7eqoAAACAxlD3pEp6aanFoaEhz1OZHcdRJpORJM3Ozioej8s0zU0f9JXP57eMVZqYmNDDDz9cc/kPEj1VAAAAgL/5IqkKhUIyTVOJREKZTMZd2758gQnTNDU0NOTOt6pmq6cqV4uNj4/rwQcfdF8XCgUdP358T59hv5V6qkiqAAAAAH/zzZLqhmFoeHhYw8PDbgJUPj+qtJKfbdsyDGNDz1M+n5dhGFvGKrW3t6u7u9vz4xcM/wMAAAAaQ12Tqmw2q97eXve1aZqS1pMpy7J08uTJDccEg0HPPKxy4XB4y1gjoqcKAAAAfmZZlhKJRNXt8XhcgUBAiURCqVRKU1NTisfj7hSfavuWVv+uNDw8rN7eXk1NTe35mINS1+F/lQmSZVkyDEOhUEiO42hyctKNZbNZRaNRtzeqnG3bCofD28YaCT1VAAAAaATJZFLT09Oe7+7S+hSfyclJpVIpjY+Pe76PDw8Py7ZtjY2NefbN5/NKJpOedRak9ak8wWBQ4XC4pmMOSl2TqlAopJGRETeznJmZcZ8vZRiGwuGwpqamZBiGcrmc51lT6XRaiURCg4ODmp2d3XGs0dBTBQAAAD8zDEOO4yibzW46aqzS6dOn1dvb61lDQZJGRkY0Ojoq27bdUWySdO7cOQ0MDGx4fNJej9lvdV+oorQohaQN2WUoFNqwDHqJaZpuNlx+ju1ijYKeKgAAgMOnWJQWF+vz3p2du/8Oms1mNTIyIsuylE6nd5xUlUanpVIpTy+SYRg6deqUMpnMjnuX9nLMfvPNQhWojp4qAACAw2NxUTp6tD4/e0nmSs+Cjcfjmp6e3tWxpmlqdnZ2w/Z4PK5kMul5j+3WR9jLMfuJpMqn6KkCAABAo4hGo+4QwN2o9tij0kg1y7Ikbb6Sd63H7Ke6D//D1uipAgAAODw6O6XLl+v33ruRzWaVy+Xc9RFM09zVEEDbtjfdNxqNKplMenqftrOXY/YLSZVP8fBfAACAwycQkLq66l2KnbEsy5PABINBjY6O7jipsW1b8Xi8aiwej2tgYEDDw8M7TtL2csx+YfifTzH8DwAAAI1kN0MA4/G4YrGYZ7U+6aXhgKZpyjRNzczMbHuuvRyz3+ip8jl6qgAAAOAn2WzWfT5UJBJx5zOlUikZhqFEIqF4PK5wOOz2Wk1MTKi/v1+O4yiXy2loaMizSrdlWZqYmHCXP49Go4rH427SlclklE6nde7cOaVSKcVisT0dc1ACxSJf20sKhYJ6enq0sLCg7u7uupbl8cele+6RbrtNeu65uhYFAAAAB2RpaUnnz5/XiRMn1NHRUe/iHDpb1f9ucgOG//kcKS8AAADgbyRVPsWcKgAAAKAxkFT5HD1VAAAAgL+RVPkUPVUAAABAYyCp8jl6qgAAAAB/I6nyKXqqAAAAgMZAUuVz9FQBAAAA/kZS5VOlniqSKgAAAMDfSKp8iuF/AAAAQGNoqXcBsDV6qgAAAOAnlmUpmUwqlUppbGxM/f39chxHuVxOqVRK8/Pzsm17wz65XE62bSsejysSiSibzSqdTrv7DA0NKRKJyLZtZTIZGYYhSTJNU7ZtKxaL1feDbyFQLNb3a3s2m5UkOY6j2dlZjYyMKBQKSZJboeUVWarcvca2UigU1NPTo4WFBXV3dx/Ex92xJ5+UXv96yTCk+fm6FgUAAAAHZGlpSefPn9eJEyfU0dFR7+LsmG3b6u/v1/z8vOd7diqVUjgcVigUkuM46u3t9exT2jY3N6dQKFT1PAMDA5qbm3PPOTU1pYsXL2pycnLfP8dW9b+b3KDuPVXDw8M6e/asIpGI8vm8hoeHlcvl3FipQm3b1ujoqNLpdE2xRkNPFQAAwCFSLEqLi/V5787OHc9BCQaDVbefOnVK586d2/Q4wzBkmqbOnDmjUCi04Ty2bW84ZmxsTFNTUzsqV73UPalKp9Nuz5QkT29TOdM03V6tvcYaCXOqAAAADqHFReno0fq89+XLUlfXng61LEumabpJ01by+bz6+/urxkojzVKplGe4n5+H/kk+WKgiEom4f0+n04rH45LWhwVWZq7BYFCWZe051ojoqQIAAIDfnTlzxv37ZkmV4zhKJBKKRCJbJkmnT59WPB5XIBDQ0NCQstnsjqby1FPde6qk9cz2zJkzGhoacivYcZyq++bz+T3HKi0vL2t5edl9XSgUdlXug0RPFQAAwCHU2bneY1Sv996lVColab1DZHx8fNN9SolWPB7fticrGo0ql8spm81qZmZGQ0NDSqfTikajuy7fy8UXSVUoFJJpmkokEspkMltW2GZJ015iExMTevjhh3dR0pcfPVUAAACHSCCw5yF49VBaEK58Os9m++yE4zjuEMJYLKZYLKZUKqWJiQlfJ1V1H/5XYhiGhoeHNTw87FZmZe9SPp+XYRh7jlUaHx/XwsKC+3PhwoV9/1x7xcN/AQAA0Cgikci+DNGzbXvDtJ1Tp05t2XniB3VNqrLZrHp7e93Xpa5A27Y9c63KhcPhPccqtbe3q7u72/PjFwz/AwAAgF9Vm1qzl32rxRKJhOd1Npv1dS+VVOfhf8Fg0JMEWZa1afehbdsKh8Nub9ReYo2InioAAAD4Senhv9J6AjQ0NLQh6SmtmSBJk5OTisfjG77jlx7+K61PyxkZGZG0/nikqakp9/t7Lpc7kGdU7ae6P/w3k8m4GerMzIwmJyc9PVbJZFKDg4OanZ3V+Pi4Z8n1vcS24qeH/9q21N+/PqS2XnMVAQAAcLAa9eG/N4v9evhv3ZMqP/FjUtXZKV25UteiAAAA4ICQVNXXfiVVvlmoAl7MqQIAAAAaA0mVz9GPCAAAAPgbSZVP0VMFAAAANAaSKp+jpwoAAODmt7a2Vu8iHEr7Ve91XVIdm6OnCgAA4ObX1tampqYmfe9739Ott96qtrY2BfgieOCKxaKuXbumF154QU1NTWpra6vpfCRVPkdPFQAAwM2rqalJJ06c0DPPPKPvfe979S7OodPZ2am77rpLTU21DeAjqfKp0i8oSKoAAABubm1tbbrrrru0srKi1dXVehfn0GhublZLS8u+9AySVPkUvb4AAACHRyAQUGtrq1pbW+tdFOwBC1X4HD1VAAAAgL+RVPkUPVUAAABAYyCp8jl6qgAAAAB/I6nyKXqqAAAAgMZAUuVz9FQBAAAA/kZS5VP0VAEAAACNgaTK5+ipAgAAAPyNpMqnePgvAAAA0BhIqnyKpAoAAABoDCRVPsWcKgAAAKAxtNS7AJZlKZvNSpJmZ2d1+vRpGYbhxiQpFArJtm05jqNQKCRJsm1bmUxGpmnKtm3FYjH3uK1ijaI8qSoWSbIAAAAAv6p7UpXNZjU2NiZJmpqa0smTJzU3NydJSiaTSqVSkqRIJKJ0Ou0eNzw87O5n27ZGR0fd+FaxRtFU1odIUgUAAAD4V12H/1mWpYmJCfd1NBqVZVmybVuSNDAwoPn5ec3Pz2tmZsbTE1XONE23t2urWCMpT6LW1upXDgAAAABbq2tSFQqFdPr0afe14ziSpGAw6G4zDGPD0L1sNuvZp3RMaSjhZrFGUjn8DwAAAIA/1X34XzQadf9+5swZRSIRN4lyHEeZTEbS+nyreDwu0zTd5KtSPp/fMlZpeXlZy8vL7utCobC3D3EASKoAAACAxlD3pKqklECV5kJJ8iwwYZqmhoaGlMvltjzHbmITExN6+OGH91rkA1U5pwoAAACAP/lmSfVEIuGZNyV550eVVvKzbVuGYWzoecrn8+5Qwc1ilcbHx7WwsOD+XLhwYV8/Uy2YUwUAAAA0Bl8kVVNTU0okEu7QPsdxZFmWTp48uWHfYDCoSCRS9TzhcHjLWKX29nZ1d3d7fvyC4X8AAABAY6h7UpXJZBQKhdyEanp6WoZhyDRNTU5Ouvtls1lFo1E3Vs62bYXD4W1jjYThfwAAAEBjCBSLO/vKvrCwoFQqpUAgoB0eokAgoFgstmkPkG3b6u/v92wzDEPz8/OSXnowsGEYyuVyniTLtm0lk0kNDg5qdnZW4+PjniXXN4ttpVAoqKenRwsLC3XvtVpclLq61v9+6ZJ09GhdiwMAAAAcKrvJDXacVB0Gfkqqrl6VOjvX/76wIPloZCIAAABw09tNbrDj1f8WFhZ09uzZXRcmEonUPUFpRMypAgAAABrDjpOqnp4e3Xvvvbt+AxKqvWFOFQAAANAYdvWcqhMnThxUOVCBnioAAACgMdR99T9Ux3OqAAAAgMawq6Tqq1/9qh599FFJ0vnz51UoFA6kUGD4HwAAANAodpVU5fN5d2nyEydOaHp6+iDKBDH8DwAAAGgUu0qqwuGwgsGgvvrVryocDiuXyx1UuQ49kioAAACgMexooYq7775b/f39GhoakmmampmZ0blz5w66bLiBOVUAAACAf+2op2pmZkaPPfaY7r33Xn3lK19RLpfTAw88oA996EMHXb5DrTSvip4qAAAAwL921FNVWkr95MmTOnnypLv9/PnzB1MqSHppCCBJFQAAAOBfNS2pHiif+IN9V6pehv8BAAAA/lVTUpVOpxUMBjUyMqKPfexjevrppz1xllyvDT1VAAAAgP/VlFSZpqnz588rFovpqaeeUiQSUV9fn5tkJRKJ/SrnocScKgAAAMD/ah7+19PTo5MnT+q3f/u39dRTT+kDH/iAm2Rls9n9KuehRE8VAAAA4H87WqhiM7lcTh/72Mf0C7/wC+62/v5+d0GL/v7+mgt4mDGnCgAAAPC/mnqqHnroIT311FPq6+vTAw88oPe9732anZ1146OjozUX8DBj+B8AAADgf4Fisfav7OfPn5dlWZKkn/7pn665UPVSKBTU09OjhYUFdXd317s4OnZMunxZeuopiU4/AAAA4OWzm9ygpuF/JSdOnHCfZYX9w5wqAAAAwP/2JamqhWVZ7oIWs7OzOn36tAzDkCTZtq1MJiPTNGXbtmKxWM2xRsKcKgAAAMD/6p5UZbNZjY2NSZKmpqZ08uRJzc3NSZKGh4fdv9u2rdHRUaXT6ZpijYQ5VQAAAID/1bRQRcnnP//5PR1nWZYmJibc19FoVJZlybZt2bbt2dc0TbdHa6+xRsPwPwAAAMD/9iWpmpmZ2dNxoVBIp0+fdl87jiNJCgaDymazCgaDnv2DwaA7XHAvsUZDUgUAAAD4374M/6tlAcFoNOr+/cyZM4pEIjIMw02wKuXz+T3HKi0vL2t5edl9XSgUdlzulwNzqgAAAAD/25eeqkDp238NHMdRJpPZdu7TZknTXmITExPq6elxf44fP77D0r48mFMFAAAA+N++JFX7IZFIaGZmxl2lzzCMDb1L+XxehmHsOVZpfHxcCwsL7s+FCxf29TPViuF/AAAAgP/5IqmamppSIpGQaZpyHEeO4ygSiVTdNxwO7zlWqb29Xd3d3Z4fPyGpAgAAAPyv7klVJpNRKBRyE6rp6WkZhiHTND372batcDhcU6zRlIb/MacKAAAA8K+6LlRh27aGh4c92wzDUCwWkySl02klEgkNDg5qdnbWM99qr7FGQk8VAAAA4H+BYi1L991w+vRpjY6O7kd56qpQKKinp0cLCwu+GAr4qldJ3/2uNDcnhUL1Lg0AAABweOwmN9iX4X83Q0LlR/RUAQAAAP5X9zlV2BxzqgAAAAD/I6nyMXqqAAAAAP8jqfIxkioAAADA/0iqfKw0/I+kCgAAAPCvXSdVjzzyiILBoJqbm9Xc3KwHHnhAf/Inf3IQZTv0Sj1VzKkCAAAA/GtXSdUjjzyiXC6ns2fPKp/P6y/+4i8UiUT00EMP6YEHHlChUDioch5KDP8DAAAA/G9XSVUul9NHP/pR3Xvvverp6dHJkyf10EMP6amnntLo6ChLq+8zkioAAADA/3aVVPX3928ai0aj+sAHPqAPfehDNRcK61hSHQAAAPC/XSVVvb29W8bvvfdevfjiizUVCC+hpwoAAADwv10lVXNzc9vu09fXt+fCwIukCgAAAPC/XSVVyWRSzc3Neu1rX6v3ve99evTRRzcsTrFdbxZ2jqQKAAAA8L9dJVWTk5PK5/P66Ec/qp6eHv3Wb/2WDMNQX1+fRkZG9LGPfWxHvVnYGeZUAQAAAP4XKBZr7wc5e/asLMvSzMyMzp49q9XV1f0o28uuUCiop6dHCwsL6u7urndxdM890uOPSzMzUiRS79IAAAAAh8ducoOW/XjDkydPusurP/LII/txSojhfwAAAEAj2NXwv52IRqP7fcpDi+F/AAAAgP/te1J14sSJ/T7lodXcvP5ng46mBAAAAA6FfU+qsH9IqgAAAAD/2/GcqoWFBaVSKQUCAe10bYtAIKBYLLblxC7LsjQ6Orph1UDLsiRJoVBItm3LcRyFQiFJkm3bymQyMk1Ttm0rFovJMIxtY42GpAoAAADwvx0nVT09PXrooYf29c1LyU8pgSqXTCaVSqUkSZFIROl02o0NDw+7SZht2xodHXXjW8UaDUkVAAAA4H+76qk6e/bsrt8gEols2lO11aIWAwMDmp+flyRPT5Nt2579TNNUNpvdNtaISKoAAAAA/9tVT9W999676zeo5XlP1YbtZbNZBYNBz7ZgMCjLsnTu3LlNY6Whg+WWl5e1vLzsvi4UCnsu60FouXF1VlbqWw4AAAAAm9vVc6pezpX9HMdRJpORJM3Ozioej8s0TTmOU3X/fD6/ZayaiYkJPfzww/tR3ANBTxUAAADgf/vy8N+DUL7AhGmaGhoaUi6X23T/zRKqrWLj4+N68MEH3deFQkHHjx/fS3EPBEkVAAAA4H++XVK9fH5UaSU/27ZlGMaGnqd8Pi/DMLaMVdPe3q7u7m7Pj5+QVAEAAAD+58ukyrIsnTx5csP2YDCoSCRS9ZhwOLxlrBGRVAEAAAD+55vhf47jeIb7TU5OurFsNqtoNOr2RpWzbVvhcHjbWCMiqQIAAAD8r65JVTab1czMjKT1RSMGBwfd5CkcDmtqakqGYSiXy3meNZVOp5VIJDQ4OKjZ2dkdxxoNSRUAAADgf4FisVisdyH8olAoqKenRwsLC76YX/Wud0l/9EfS7/6u9G//bb1LAwAAABweu8kNfDmnCuvoqQIAAAD8j6TKx0iqAAAAAP8jqfIxkioAAADA/0iqfIykCgAAAPA/kiofI6kCAAAA/I+kysdIqgAAAAD/I6nysVJStbJS33IAAAAA2BxJlY+13Hg0Mz1VAAAAgH+RVPkYw/8AAAAA/yOp8jGG/wEAAAD+R1LlY+3t639eu1bfcgAAAADYHEmVj5WSquXl+pYDAAAAwOZIqnyso2P9z6Wl+pYDAAAAwOZIqnyMnioAAADA/0iqfIyeKgAAAMD/SKp8jJ4qAAAAwP9IqnyMnioAAADA/0iqfIyeKgAAAMD/WupdAMuyNDo6qrm5Oc9227aVyWRkmqZs21YsFpNhGDXFGg09VQAAAID/1TWpKiU/lmVtiA0PD7uJlm3bGh0dVTqdrinWaLq61v+8fLm+5QAAAACwubomVdFotOp227Y9r03TVDabrSnWiILB9T/z+fqWAwAAAMDm6j78r5psNqtgKaO4IRgMyrIsnTt3bk+xUCi04X2Wl5e1XDZhqVAo7OOnqF3poziOtLoqNTfXtTgAAAAAqvDlQhWO41Tdns/n9xyrZmJiQj09Pe7P8ePH91Dag1OeH87P168cAAAAADbny6RqM5slTXuNjY+Pa2Fhwf25cOFCbQXcZy0t0q23rv/96afrWhQAAAAAm/Dl8D/DMDb0LuXzeRmGsedYNe3t7WovrVvuM9/+6hd1+Xvf0lv636bPvHBcf/iH0sqK1NQkBQLrPyW33iq9+tX1KysAAABwmPmypyoSiVTdHg6H9xxrNN/5V/9Cb/zx9+gnj3xIkvS7vyvdd5/0lrdIb36zNDj40o9pShUr0gMAAAB4mfimp8pxHLdHyTRNT8y2bYXDYbc3ai+xhtO03hVl3vMdPXiv9Od/vv68qmJRWlt7abdvf3v99eysNDBQp7ICAAAAh1hdk6psNquZmRlJ64tGDA4Oususp9NpJRIJDQ4OanZ21vOsqb3GGknxxvi+Jq3pd35H+p3fqb5fLCadPi298MLLWDgAAAAArkCxWCzWuxB+USgU1NPTo4WFBXV3d9e1LP/jh0/oh/7qaX3x37xDb/2Pn9p0v1/9Vek3f1P61/9a+k//6WUsIAAAAHAT201u4Ms5VdBLK1GUj/WrorQ6ID1VAAAAQH2QVPlUMXDj0myTVN122/qfzz9/wAUCAAAAUBVJlV/d6Kkq0lMFAAAA+BpJlV81lYb/bT3lrZRUPf/8eqcWM+QAAACAl5dvllRHhdKcquLOh/+1tKwnVU1NUnPz+uuWlu3/XvozGJRSqfXnXgEAAADYGZKqKp599llduXLFfd3R0aHe3l6trKzohSrj7O68805J0osvvqjr1697YoZh6MiRI7py5YoKhYIn1tbWpr6+Pq2trem5557zxArH+rTa1CStrSmfz2t5edkTP3bsmI4ePaqurqu6807HE7t+vUUvvnirrl+X7rjjGTc/k9Z7s7773Vu0stKqnh5HnZ1Xyz639Ja3dOnEiW6trCzr6NG8pPI1M5p06dLtkqTu7ucUCHgTvitXglpZaVdHR0Ht7Vc8sWvXjujqVUNNTdd17NiLG+pwYeHOG5/rBTU1rXhii4uGrl8/ovb2y+rouOSJray068qVoAKBVXV3b5xYtrBwu6QmdXVdVEvLNU/s6tVuXbvWpdbWq+rs9Nbh6mqrLl++RZJk9HxXbWtLalm7qta1RbWuLWk136y2lSV1djrqaL2s9rUltawtqa24pPbL6z9tLVfV2nNdbWvLal1bUruW1XF9ScEX8uosXtXyre1qCax63vfYxYJar69o8Vinlro6PLH2xSV1FRa10tKswi09nligWFTvc/Prn/mWHq22NHviR+cvqW35uq52dejqsU5PrG3pmo46l7Xa1KSF24wNddj7bF4BSZeCx3S9rdUT61q4ovary1o+0q4rPV2eWOu16zqWv6SipPk7gmWFXW9Q3c87al5b02XjqK53tHmO7bi0qCNXlnStvVVXeo95Yk0rq+p5cUGSNH97rzwNXNKxFxfUsrKqK92dutZZUYdXltR5aVHXW1t0uc+7ik9gbU3G846k9Tpcq6zD/CW1Xruuq0ePaOnoEe9nvbqsowtXtNrcpMKthidWlNT77Pq1uRQ8ppU27223y7mitqVrWu5s12K399q0LK/o2PwlFQMBObd7zytJPc87alor3qhD77U5cumqOq4s6VpHm64Y3mvTfH1V3RfX70frdeg9b/eLBTWvrOpKd5eudVZcmytLOnLpqq63tehysOLarBbV/cL6tVm4tUdrzd7BEOt1uKKrx45saN9ti8vqKixqtaVZhVsqVlgqym3fhb5urbZ6r02Xc1ltS9e11NWhq8cqrs3SdR11LmutKVC1fRvPOQoUi7rUe0wr7d5r01lYVPvictU6bLm2cqN9BzR/R++G83a/sKDm1TVd6enStSMVdXj5qo5cXqpah80rq+p+cf3aLNxmaK2pon1fvKSW6ytaPHZEyxvuEcvqvHGPuFRRh4FiUcZzjiSpcEv3hntE1/xltS1vVofXdNS5orWmJi3c1qNiRYMxnp137xGV7btz4Yrar17T8pE2LfZsVoeSU6UOe55fUNPamq4YXbpWcY9w23d7q670HvXEyuvQud1wH1FScuzFglpWVrXY3anlznZPbP0ecVUrrS261Fe6NuvHB9bW1PP8evsu3NK94R7RdaN9Lx3tqHKPuKauG/eIS7d679/Seh1K63W4WlmHZfeIqxvuEdd1dP6yioGAFqrcI7rde0SXVqrcZzuuLOtaR6sWDW8dNl1fUffF9f9vnduNKvfZ9XvEYnenrlWpw9I94kpF+w6srqmn7B5RrLhHdJXdIyrbd9uN9r1apX2rvH33HdNaa2Udlu4R7Vqq+D+wxW3fARWq3CN6btwjLvce1Up7xX227B6xWHmfvdG+pert+1jZPeJ6lXtEx417RGUdNnnuET0qNlXcZ2/cI7aqw5WWZl3eqg6rtO/OsnvEUpV7RNeNe0Thto3tu8e9RxzVasX3iCNl94irZfeIlUCTfsD6U/X19alYLOrZZ5/dcN7bbrtNzc3Nmp+f19LSkidW+p68tLSk+fl5T6ylpUW33hjq9eyzz6pyIfRbbrlFra2tWlhY2PD9fCskVVX83u/9njo6XmqIb3rTm/TOd75ThUJBqVRqw/4f/OAHJUmf+tSn9J3vfMcT+6mf+indc889euKJJ/Tnf/7nnlh/f7/e/e536/r16xvP+wMP6Icf+5qKxTU99thj+uY3v+kJv/3tb9d9992nCxdsxeMZT+yWW+7QT/xEXKur0n//7x/X2pr3i/v9979PR47cpscf/ys988xXPbEvfemHdPZsRK95zTP6iZ/4b55YoXBMH/7wg5KkBx/8Q3V3exOcT3zi5/T006/RyZNf0f33/w9PzLLu1ac//Q7deuu8/tW/8n7WlZVm/cZv/KokKR5/VHfe6f2HMz0d1Te+8f26777H9ba3/YUn9uST36dPfvJd6uxc0tjYxmvzZxM/qq7lRQ28e049d3vL2/rZZfV+bV7Fe6QXfvwOT+yWC88r+vGMjuqyPvTrYxvO+4sf+YiC+bwefec79fg993hiP/zFL+qtX/yinurv1x/+zM96Yr35vH76I5+RJD3yzx/SYpf3JvzzH/uYjn/nO3rszQ/ob+67zxMLf+Ur+rHPfU7P3Hqn/vgX/rkn1ra8rPGJCUnSf3nn+/VCqQvzhn/2yU/qdU/m9KU3/UN9PhLxxN74xBMaTqdV6O5W5hd+ZsNn/ZX/8B/UsrqqT/zoe/St17zGE/unn/60Xm99Q9ZrQ5p5x495Yq9++mm95xOf0Epzs37zF7z1IEm//OEPq7tQUPqtw/rG93+/J/a2bFahL39ZT5qv05++60c9sVuff17v/y//RZI08bPjutbu/Q89lkzqzmee0Wff8qM69+Y3e2I/+Nd/rQcee0wXbn+V/uS9/8wT67xyRQ898ogk6SPD/0bzwaAn/jO///u6O5fTF3/grfrLt77VE3vT17+udz76qPLHgvrj93qvjSR98Nd/XZL08R9/r75z/Lgn9lOPPqo3fv3/6ivfN6jsj3k/a/9TT+ndf/AHWm5v129XOe+/m5pS1/KiPhl5l775utd5Ym9/7DGF/vpxPfHaNyp7ynveO555RvFkUpL0G+/5Va22eP8reN9//s+67YUX9Okfeoe+Ggp5Yj/0pS9p4OxZPf3K1+hP3zPiiR0rFPTghz8sSfrwP3tQlyqWn/25T3xCr3n6aWXvPan/cf/9nti9lqV3fPrTet64VX/yXu95m1dW9Ku/8RuSpORPxPXsjV9klUSnp/X93/im/voN9+nsAw94Yt/35JN61yc/qSudnfrQe9+lSh+YmFD7tWX9wdvfrdzdd3tiP/LZzyo0+7i+/oZ7dPadP+KJverCBb334x+XJD383l/fcF73HvGPqt8jBr/4RT11V78+/bPDnlhvPq9/85GPSNrmHjG4xT3iljv1qfee8sQ894if2uwe8aS+9Pe2vkf8vxXXRiq7R/xI9XvEm6yvy3pTSF94h/faeO4R7/21Defd7h4x+OW/1ZPm6/SZd3nPW+s94i037hGffm/UE9uve8R/eq/3mkvb3yPu+frf6SvfN6gv/NjbPTHvPWJ8w3m3u0fc89d/qyde+0Z99pT3vLXeI95y1tLTr3yNPvOen/bE9use8V8rrs3O7hF/p79+w336wgPez+q9R2z8/367e8Q9s3+rr7/hHn32nd7z1nqPePMXv6qn7urXZ37WW4e13iN+8MY9IlVRh7u5R3xhy3vExva93T3iHuvvbtwjvHX47T/7M73nPe/R6upq1e/fv/zLv6zu7m5ls1l94xvf8MTe9ra36f7779e3vvUt/dEf/ZEnduutt+r973+/pPXv/NeueX/pHovFdOedd+rLX/6yvvzlL294383wnKoypbXon3zySR079tJvBurRU/XVfx3VA3/6N/rLf3G//v7vPLppT9XVq1f17LOOLlyQXvva9Vh5Bv7MM89sKG8pA3ccR1evvtRT9d3vSpbVpTvu6FZb27KWlvIVc7Sa1NW13lN15cpzKlYMTTxyJKjm5nYtLxd0/bq3p6q19Yja2w2trl7X1auVPVVFNbf3amF5QfnCBV1ZWtDylYtaK7yo5st5NTsX1ea8qLblS2pbuabW5atqX76q9uVlHb1ySbe9+KKOra5ptadXXSurOlI2D+32Z59VU7Goi8Hghv9YuxcW1LW4qKtHjsgxDG95r13TLRcvrtdh2c35WkBaam5Rp5PXaiCgF4N9Wuzq0rXmFl1vbdNqS6uaVlYVKAZ07UiXFnsMrbV1aK2tQ8XWDqmtQ22BI7re3qHFjhatNLd6egla1prUpIBWtKa1gPefZqAoNRebVFRRK00bJ9C1rK3/VmmlaVUvRdb/1rzWpKZiQCuBNa01eY9rKgbUvHbjvM0bh5u2rDYpoIBWmtZUrChT6bxrgaJWm7zJe6AYUMsOzrvatPGzNq0F1FxsunFe77Gl80rS9Wbve3rOW+2zls6rolabNz/vStOapIrPuhpQkwJaDRSr1KHK6nDjLbV19aXzFit6hZrXAmV1uPGat2xx3pbVQNm18cbWP2v5eV86vnReSbpe9doEyq7NJudVUauVZSpKraXzNq1t6AHbrzqsVKrDauetpQ53cm2kretwy2tzow4DFW2t9FmrnXfrOgyoeS2wbR1WvTb7VYeV/272pX0XN9x7NrbvTT7rlu27uPHec6MO3fZddp8NVJy38l1byuqwskxNxYBablyb61U+a1upfTcXVXnmWs7bWlaHlZ+1uawOVyqvjQJqXQ1s+lm3PG+pDqued4d12FTUapVrs9ln9Z63uKEd1qMOmxRQy406vFalHe792mxTh6V7xBZ1uFbl31y967DyvIGmJo38SswXPVWve93rdvScKpKqMn56+O+XfuxNuv9z/1uf/7l/pLd94i/rWpa9uL56Xc9deU7PXHpGz1z6nvLfzWnxW/9Xqxe+rcALL6jpYl6t8wV1FK6oa+GqeheL6luUblmU+q5K7Ru/K+/aUluTrhxp0WJnq5Y623Stq13XOzu00tmh1c4jWjvaJXV2SkePKnDsmJqOdavpWLdajhlq7TbU2tOrtp6gOnr61NEd1JFjQbW1HVGgYhgEAAAAbj67yQ0Y/udXO3z4bz0srSzpwsIFfXvh27rgfEvOU/9bS+f/r1a/c0Ftzz6vrucd9eav6pUF6RWXpL93STqysv15K11rbdKV7iO62t2p5d5jWuntUbHXUKC3Vy09QbUGb1F78DYdCd6uI7fcoebeoNTdLfX0SN3d6mhtVYekvn2vAQAAAOAlJFV+1VT/pOqFKy/oiRee0P954e/07JOWVh7/WzXnzqv3uxfVn5fuzkv/cH7nvUqXj7Xr8q09utbXq2JfUIFbb1Prrber/Y5XqvOO42q//ZUK3HKL1Ncn3XKL2jo71RYIaOP0TgAAAMA/SKr8KnBjNZeXaXTm9y59T1/57lf0v+3/pfm5L6n58Sd0/FuO3vScdOo5Kbi0+bErLeurGS3ffouKr3yFWl51l468ul+dr3mtml51XHrlK6VXvEJHOzp0dPPTAAAAAA2JpMqvDrin6vz8eX3x6S/qiXOf09qX/kr9/+d5/YML0j99Tqoyx1CrTQEVXn2HVl/bryOvf5M633CPAnffLd19t1qOH1dvc/PGgwAAAIBDgKTKr270VO3nOiKPP/e4fv9r/13f+f+mdf+Xvq2hnPQv5zfud6WvW9e+//XqGvhBtf39Aemee9T8hjeot2L1PAAAAAAkVf7V9NLDBmv15ItP6tc///9oLT2tX/kr6Z6yZ+SuNjfp0ve/VkfeGlH7/W+V7rtPXa98pbo2PRsAAACAciRVPlXchzlVL1x5Qb9y9t/r+T/6uB7+fFE/cONRWCsdbVo9Naz26Iia3/pWGceObX0iAAAAAJsiqfKpQA1Lqq+ureqj5z6qvzid0K987ore/L0b248dVfNDY2r5xV9US8XDbgEAAADsDUmVXzXtrafqa89+TeO/9zP6+d9/Qp/6u/Vtq0c61PxLv6zmf/fvpGBwnwsKAAAAHG6+Tqosy5IkhUIh2bYtx3EUCoUkSbZtK5PJyDRN2batWCwm40bvy1axhlFKqnbYU7WytqJH/nJCzm//utJfWFPXdWmtuUl63/vV/Gu/Jt122wEWFgAAADi8fJ1UJZNJpVIpSVIkElE6nXZjw8PDmpubk7SeRI2OjrrxrWINI1BaqGL7nqqnnaf17//jO/RLqcfdoX7X/sEPqu2jKelNbzrIUgIAAACHnq+TqoGBAc3Pr6/5Xd7TZNu2Zz/TNJXNZreNNZJi6/qlaVpZ3XK/z33zs/rC2LA+8WdX1bYmXTvaqdbf/Yjafv7n3cQMAAAAwMHxdVIlqeqwvWw2q2DF3KBgMCjLsnTu3LlNY6WhgyXLy8taXl52XxcKhf0reI2Wjh2RJHVeXqoaXyuu6Tcf+1Xd9YEJPfK19W2L/ySizo//N+kVr3i5igkAAAAcek31LsBWHMdRJpNRJpNRIpFwe6Ecx6m6fz6f3zJWaWJiQj09Pe7P8ePH96voNVs+1ilJOnJpeUNsaWVJv/Rff0I//i8n9HNfk1abAlqZ+m11fu4vSKgAAACAl5mve6rKF5gwTVNDQ0PK5XKb7r9ZQrVZbHx8XA8++KD7ulAo+CaxWu5eT6o6r3iTqouLF/Wbv/bD+uB/fUJ9V6Wrvcd05I8/Jf3jf1yPYgIAAACHnq+TKtu23SF7pZX8bNuWYRgbep7y+bwMw9gyVqm9vV3t7e0HVv5aLBlHJUndzlV327fy5/Xoz71ZH/rMi2qSVLjnder+zIzkk0QQAAAAOIx8O/zPsiydPHlyw/ZgMKhIJFL1mHA4vGWskTivvl2SdMsLV6RLl/TN8+f0dz/8Rv3yjYQq/+6ouv/X35JQAQAAAHXm254q0zQ1OTnpvs5ms4pGo25vVDnbthUOh7eNNRK7aUHfPSa98pL0vV96r1o+9cf6JxfXdK0loMsfnlTwFx+qdxEBAAAASAoUi8XtH4RUJ5ZlKZvNyjAM5XI5T5Jl27aSyaQGBwc1Ozur8fFxz8N/N4ttpVAoqKenRwsLC+ru7j6gT7UzT774pM7+yOv1/nMvbXsm2KYjf/JnMv7R2+tXMAAAAOAQ2E1u4Ouk6uXmp6RKkj7xud/S62O/ouMF6Wv3v1b3f2xGx+58db2LBQAAANz0SKr2yG9JlSR9e+HburZ6TXcH7653UQAAAIBDYze5gW/nVGHdXT131bsIAAAAALbg29X/AAAAAKARkFQBAAAAQA1IqgAAAACgBiRVAAAAAFADkioAAAAAqAFJFQAAAADUgKQKAAAAAGpAUgUAAAAANSCpAgAAAIAakFQBAAAAQA1IqgAAAACgBiRVAAAAAFADkioAAAAAqAFJFQAAAADUgKQKAAAAAGpAUgUAAAAANWipdwEOgm3bymQyMk1Ttm0rFovJMIx6FwsAAADATeimTKqGh4c1NzcnaT3BGh0dVTqdrnOpAAAAANyMbrrhf7Zte16bpqlsNlun0gAAAAC42d10PVXZbFbBYNCzLRgMyrIshUIhz/bl5WUtLy+7rxcWFiRJhULh4AsKAAAAwLdKOUGxWNx235suqXIcp+r2fD6/YdvExIQefvjhDduPHz++38UCAAAA0IAuXbqknp6eLfe56ZKqzVRLtsbHx/Xggw+6r9fW1pTP59XX16dAIPAylm6jQqGg48eP68KFC+ru7q5rWdAYaDPYLdoMdos2g92izWC3/NRmisWiLl26pFe84hXb7nvTJVWGYWzolcrn81VX/2tvb1d7e/uG4/2ku7u77g0KjYU2g92izWC3aDPYLdoMdssvbWa7HqqSm26hikgkUnV7OBx+mUsCAAAA4DC46ZIq0zQ9r23bVjgc9l0PFAAAAICbw003/E+S0um0EomEBgcHNTs725DPqGpvb9cHP/jBDcMTgc3QZrBbtBnsFm0Gu0WbwW41apsJFHeyRiAAAAAAoKqbbvgfAAAAALycSKoAAAAAoAYkVQAAAABQg5tyoYpGZ9u2MpmMTNOUbduKxWKsXngIWZalbDYrSZqdndXp06fddrBVG9lrDDeXRCKh8fFx2gy2lc1mZdu2u3pu6dEktBlUY9u2stmsgsGgbNtWNBp12w5tBiWWZWl0dFRzc3Oe7QfRRnzTforwnVAo5P49l8sVo9FoHUuDepmcnPT8vbxdbNVG9hrDzWNubq4oqTg/P+9uo82gmpmZmWIsFisWi+vX1zRNN0abQTXl/zcVi0W3/RSLtBmsS6fT7v9DlQ6ijfil/TD8z2ds2/a8Nk3T7a3A4WFZliYmJtzX0WhUlmXJtu0t28heY7i5lPc6lF6Xo82gJB6Pa3JyUtL69Z2ZmZFEm8Hmzpw5U3U7bQYl0WhUoVBow/aDaCN+aj8kVT5T6lIvFwwGZVlWnUqEegiFQjp9+rT72nEcSettYas2stcYbh6ZTEbRaNSzjTaDamzbVj6fl2EYsixLjuO4yThtBpsJBoMaGBhwhwEODQ1Jos1gewfRRvzUfkiqfKb05blSPp9/eQuCuiv/YnzmzBlFIhEZhrFlG9lrDDcHx3GqjiOnzaAay7IUDAbduQipVEqZTEYSbQabS6fTkqT+/n6l02n3/yraDLZzEG3ET+2HhSoaxGaNBjc/x3GUyWQ2TPastt9+x9BYpqenFYvFdrw/beZwy+fzsm3b/YVNLBZTb2+visXipsfQZpDNZjU5OSnbthWPxyVJyWRy0/1pM9jOQbSRerQfeqp8xjCMDdl1aXgGDqdEIqGZmRm3DWzVRvYaQ+PLZrM6depU1RhtBtWYpuleZ0nun5Zl0WZQlW3bmp2dVSQSUSwWUy6X0/T0tGzbps1gWwfRRvzUfkiqfKa0lG2lcDj8MpcEfjA1NaVEIiHTNOU4jhzH2bKN7DWGm8P09LRSqZRSqZRs29bExIQsy6LNoKryxUwq0WZQjWVZGhwcdF+bpqnx8XH+b8KOHEQb8VP7Yfifz1T+J2fbtsLhML+xOYQymYxCoZCbUJWGdlW2hfI2stcYGl/lfyzxeFzxeLzqF2faDKT1/2/C4bA7F6+0auRmq3bRZhAKhZRMJj1zfi9evEibwabK5/pu9R33ZvhuEyhuNXgadWHbtpLJpAYHBzU7O+t5gCcOB9u21d/f79lmGIbm5+fd+GZtZK8x3Bwcx1EqlVIikVAsFlM8HlcoFKLNoCrHcZRIJDQwMKC5uTm3Z1ziPoPqstmsO0RUWv+FDm0G5bLZrGZmZjQ1NaWxsTENDg66ifhBtBG/tB+SKgAAAACoAXOqAAAAAKAGJFUAAAAAUAOSKgAAAACoAUkVAAAAANSApAoAAAAAakBSBQAAAAA1IKkCAPhONptVPB5XIBBQIpFQNputW1kGBgaUyWTq9v4AAP/jOVUAAF8qPQR7fn7e8yBHx3Fe1gc7ZrNZhcNhHkYKANgUPVUAAF8KBoMbttm2renp6Ze1HJFIhIQKALAlkioAQMOYnJysdxEAANigpd4FAABgJ7LZrM6dO6d8Pi9pvQfJNE1ls1lZliXTNDU7O6vJyUl3TlYikZAkJZNJzc3NKZPJyDAM2batXC7nSdJs21YymdTg4KDy+bxOnTol27Y1OjqqeDyuWCwmSbIsS9lsVqZpyrZtRaNRtxyJRELxeNyNzczMKJ1Oez5DZVkdx9H09LRM05TjOO52AEDjIKkCADSESCSiSCSi/v5+N8GxbVuJREJzc3OSpHw+r6mpKY2NjSkSiWhubk7JZNIdSjg8PKxcLqdIJKJ4PK5MJqNoNCrHcTQ0NKS5uTkZhqFEIqFUKqWxsTGNjIy4ZSi938zMjLttYGBAZ8+edctXnkil02lZlqVQKLRpWSUpFAopEom42wEAjYWkCgDQsEoJU/nqgLOzs5IkwzDU19cnSYpGo5LkLnph27by+bxs25Ykt6eoNHdqfHx80/cLhUKebaZpanp6WrFYTH19fe57lspQSpI2K+vk5KQGBgZkmqZGRkbchBEA0DhIqgAADclxHEneXh5JnqTENE3PMRMTE+rr63OH7JWfq3wxioNamKJaWR3H0fz8vCzL0pkzZzQ8POzpCQMA+B8LVQAAfGm7YXDZbFYjIyMbnmFV/rr8HKX5TGNjY+78pdL2aDQqy7I2PU9p32rvZ1mWTp06te3n2aysExMTsm1boVBIk5OTrDQIAA2I51QBAHwnm80qnU575jWV5iWVhsuVL1QxMzOjwcFBSetzr86dO6dEIqFgMKhEIqFIJCLHcdxFJ0qSyaRGRkYUjUarnqe0UEUwGFQymay6MEapbJZlaXR0VJJ0+vRpdw5VKVnarKypVEqGYSgYDCqfzysYDLrDFQEAjYGkCgAAAABqwPA/AAAAAKgBSRUAAAAA1ICkCgAAAABqQFIFAAAAADUgqQIAAACAGpBUAQAAAEANSKoAAAAAoAYkVQAAAABQA5IqAAAAAKgBSRUAAAAA1ICkCgAAAABqQFIFAAAAADX4/wEKpyX5obLBOwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Crear un rango de iteraciones para las gráficas\n",
    "k = 0\n",
    "l = int(max_iters_list / 1) + 1\n",
    "\n",
    "zero_1 = np.zeros((N,1))\n",
    "zero_2 = np.zeros((N,M))\n",
    "zero_3 = np.zeros((1,M))\n",
    "zeroo = (zero_1, zero_2, zero_3)\n",
    "\n",
    "# Definir los niveles teóricos según tu solución teórica\n",
    "nivel_teorico_equilibrio = norm_adjusted((zero_1,zero_2,x2.sum(axis=0) - (D - x3)), zeroo, Sigma)\n",
    "nivel_teorico_capacidad = norm_adjusted((zero_1,x1 - x2,zero_3), zeroo, Sigma)\n",
    "nivel_teorico_demanda = norm_adjusted((zero_1,zero_2,D - x3), zeroo, Sigma)\n",
    "\n",
    "# Variable de sufijo para los nombres de archivo\n",
    "suffix = \"_1\"\n",
    "\n",
    "# Función para configurar y guardar cada gráfico\n",
    "def configurar_grafico(ax, x_data, y_data, labels, colors, title, y_label, width, height, y_lim=None, nivel_teorico=None):\n",
    "    for x, y, label, color in zip(x_data, y_data, labels, colors):\n",
    "        ax.plot(x[k:l], y[k:l], '-', linewidth=1.5, label=label, color=color)\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_xlabel('Iteraciones')\n",
    "    ax.legend()\n",
    "    if y_lim:\n",
    "        ax.set_ylim(y_lim)\n",
    "    if nivel_teorico is not None:\n",
    "        ax.axhline(y=nivel_teorico, color='gray', linestyle='--', linewidth=1)\n",
    "    ax.figure.set_figwidth(width)\n",
    "    ax.figure.set_figheight(height)\n",
    "\n",
    "# Datos para los gráficos\n",
    "iter_data = [iter_DY, iter_ADMM, iter_BA]\n",
    "labels = ['TOPS', 'ADMM', 'FPIS']\n",
    "colors = ['green', 'blue', 'red']\n",
    "\n",
    "# Definir la altura y el ancho deseado para cada gráfico\n",
    "width = 10  # Ajusta esta variable según sea necesario\n",
    "height = 3  # Ajusta esta variable según sea necesario\n",
    "\n",
    "# Crear y guardar cada figura individualmente\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "configurar_grafico(ax, iter_data, [x_DY_sol, x_ADMM_sol, x_BA_sol], labels, colors, 'Distancia al Óptimo', \n",
    "                   r'$\\frac{\\|x^{k} - x^{*}\\|}{\\|x^{*}\\|}$', width, height, y_lim=(0, 1))\n",
    "plt.savefig(f'images/caso{suffix}/distancia_al_optimo{suffix}.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "configurar_grafico(ax, iter_data, [Fx_DY_sol, Fx_ADMM_sol, Fx_BA_sol], labels, colors, 'Diferencia Valor Función Objetivo', \n",
    "                   r'$\\|f(x^{k})-f(x^{*})\\|$', width, height, y_lim=(0, 1))\n",
    "plt.savefig(f'images/caso{suffix}/diferencia_valor_funcion_objetivo{suffix}.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "configurar_grafico(ax, iter_data[1:], [Non_anti_DY, Non_anti_ADMM, Non_anti_BA][1:], labels[1:], colors[1:], 'No-Anticipatividad', \n",
    "                   r'$\\|c^{k}-P_{\\mathcal{N}}(c^{k})\\|$', width, height)#, y_lim=(0, 25))\n",
    "plt.savefig(f'images/caso{suffix}/no_anticipatividad{suffix}.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "configurar_grafico(ax, iter_data, [equili_DY_solu, equili_ADMM_solu, equili_BA_solu], labels, colors, 'Restricción de Equilibrio', \n",
    "                   r'$\\|\\textbf{1}^{T}g^{k}-(D-q^{k})\\|$', width, height, y_lim=(-0.1, 1.0), nivel_teorico=nivel_teorico_equilibrio)\n",
    "plt.savefig(f'images/caso{suffix}/restriccion_de_equilibrio{suffix}.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "configurar_grafico(ax, iter_data, [capacity_DY_solu, capacity_ADMM_solu, capacity_BA_solu], labels, colors, 'Restricción de Capacidad de Producción', \n",
    "                   r'$\\|c^{k} - g^{k}\\|$', width, height, nivel_teorico=nivel_teorico_capacidad)\n",
    "plt.savefig(f'images/caso{suffix}/restriccion_de_capacidad_de_produccion{suffix}.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "configurar_grafico(ax, iter_data, [demand_DY_sol, demand_ADMM_sol, demand_BA_sol], labels, colors, 'Diferencia de Demanda Satisfecha', \n",
    "                   r'$\\|D-q^{k}\\|$', width, height, y_lim=(0, 4000), nivel_teorico=nivel_teorico_demanda)\n",
    "plt.savefig(f'images/caso{suffix}/diferencia_de_demanda_satisfecha{suffix}.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "configurar_grafico(ax, iter_data, [dual_DY_sol, dual_ADMM_sol, BA_dual_sol], labels, colors, 'Diferencia al Precio Óptimo', \n",
    "                   r'$\\frac{\\|\\rho^{k}-\\rho^{*}\\|}{\\|\\rho^{*}\\|}$', width, height, y_lim=(0, 1))\n",
    "plt.savefig(f'images/caso{suffix}/diferencia_al_precio_optimo{suffix}.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0041b318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CP': 0.08634495735168457,\n",
       " 'DY': 1201.1167607307434,\n",
       " 'BA': 631.8935613632202,\n",
       " 'ADMM': 617.9680731296539}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e36f3d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9999,\n",
       " 0.0030972216179836532,\n",
       " 0.00013854712893720264,\n",
       " 0.0,\n",
       " 9.391778644385341e-06,\n",
       " 464.25960487047945,\n",
       " 775.6655535911405,\n",
       " 0.0028104328863389418)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_DY[-1], x_DY_sol[-1], Fx_DY_sol[-1], Non_anti_DY[-1], equili_DY_solu[-1], capacity_DY_solu[-1], demand_DY_sol[-1], dual_DY_sol[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb25dc82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9999,\n",
       " 2.0682174098825933e-09,\n",
       " 1.0091982322219152e-11,\n",
       " 0.0,\n",
       " 9.391546973347432e-06,\n",
       " 464.58783447779405,\n",
       " 777.3599568792234,\n",
       " 2.5962500716804377e-11)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_BA[-1], x_BA_sol[-1], Fx_BA_sol[-1], Non_anti_BA[-1], equili_BA_solu[-1], capacity_BA_solu[-1], demand_BA_sol[-1], BA_dual_sol[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1a81cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9999,\n",
       " 3.8454743304016674e-11,\n",
       " 1.6132027363332004e-12,\n",
       " 4.5474735088646414e-14,\n",
       " 9.394005041940546e-06,\n",
       " 464.58783155365506,\n",
       " 777.3599568381936,\n",
       " 1.8358130691611741e-13)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_ADMM[-1], x_ADMM_sol[-1], Fx_ADMM_sol[-1], Non_anti_ADMM[-1], equili_ADMM_solu[-1], capacity_ADMM_solu[-1], demand_ADMM_sol[-1], dual_ADMM_sol[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30c264f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9.393999260964846e-06, 464.5878315116832, 777.3599568385031)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nivel_teorico_equilibrio, nivel_teorico_capacidad, nivel_teorico_demanda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9557bad2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
